{"cell_type":{"90740ca1":"code","7d951710":"code","0ae0478c":"code","caef151d":"code","a5c5aa38":"code","f5889529":"code","11686085":"code","1bcf842a":"code","56209d75":"code","ce4b52fb":"code","70eb82f5":"code","2e82504f":"code","0e46dc36":"code","e4fc2425":"code","c056b037":"code","a3a087f4":"code","d152e272":"code","13f6f630":"code","ad97cdbc":"code","c32876af":"code","924efd37":"code","9613a085":"code","f0b9763b":"code","9c6e1119":"code","cb6a2aa4":"code","2ef1768f":"code","550bf28b":"code","b2165fb2":"code","d1b3c956":"code","a2424f91":"code","bc1ced30":"code","2b9c873d":"code","20469d2e":"code","63305a83":"code","7f1d0ccc":"code","7ab747b6":"code","83d82678":"code","84e74435":"code","f2b555b0":"code","28298ee7":"code","fe83953a":"code","ac70319a":"code","62e1ed3a":"code","23c2df12":"code","9ee4aef2":"code","bb252014":"code","21dfe853":"code","ea47be3e":"code","fcd7b6b9":"code","8fbea985":"code","66d2d2fe":"code","48ab6afc":"code","04555621":"code","3d3ddc78":"markdown","fbaa81f0":"markdown","ab8308bd":"markdown","e5b36c98":"markdown","aaa3df35":"markdown","a5052c6e":"markdown","ace24997":"markdown","0db34e55":"markdown","8dc2394d":"markdown","42e4bd03":"markdown","c72bb0de":"markdown","318426cf":"markdown","8a20a377":"markdown","01b5ead6":"markdown","4980267e":"markdown","10f5325d":"markdown","ce49eb9a":"markdown","29782e46":"markdown","dd6abce9":"markdown","ca7f5b8c":"markdown","c3e18f6c":"markdown","967dcd6e":"markdown","16c7e692":"markdown","0d29c364":"markdown","c8e6ca12":"markdown"},"source":{"90740ca1":"                                                \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport time \n","7d951710":"import pandas\nimport os\nimport glob\nimport tensorflow as tf\nfrom keras.applications import ResNet50V2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nimport keras\nimport os\nimport seaborn as sns\nimport cv2\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.metrics import classification_report\nfrom keras import regularizers\n\nimport seaborn as sns\nimport glob","0ae0478c":"main_path = \"..\/input\/chest-xray-pneumonia\/chest_xray\/\"\ntrain_path = os.path.join(main_path,\"train\")\ntest_path=os.path.join(main_path,\"test\")\nval_path=os.path.join(main_path,\"val\")\n\npneumonia_train_images = glob.glob(train_path+\"\/PNEUMONIA\/*.jpeg\")\nnormal_train_images = glob.glob(train_path+\"\/NORMAL\/*.jpeg\")\n\npneumonia_val_images = glob.glob(val_path+\"\/PNEUMONIA\/*.jpeg\")\nnormal_val_images = glob.glob(val_path+\"\/NORMAL\/*.jpeg\")\n\npneumonia_test_images = glob.glob(test_path+\"\/PNEUMONIA\/*.jpeg\")\nnormal_test_images = glob.glob(test_path+\"\/NORMAL\/*.jpeg\")\n\n#CATEGORIES =[\"NORMAL\",\"PNEUMONIA\"]\n#img_size=150","caef151d":"fig, axs = plt.subplots(1, 3, figsize=(13,13))\nplt.style.use(\"ggplot\")\naxs[0].pie(x=np.array([len(pneumonia_train_images), len(normal_train_images)]), autopct=\"%.1f%%\", explode=[0.2,0], labels=[\"pneumonia\", \"normal\"], pctdistance=0.5)\naxs[0].title.set_text(\"Distribution of train\")\n\n\naxs[1].pie(x=np.array([len(pneumonia_test_images), len(normal_test_images)]), autopct=\"%.1f%%\", explode=[0.2,0], labels=[\"pneumonia\", \"normal\"], pctdistance=0.5)\naxs[1].title.set_text(\"Distribution of test \")\n\naxs[2].pie(x=np.array([len(pneumonia_val_images), len(normal_val_images)]), autopct=\"%.1f%%\", explode=[0.2,0], labels=[\"pneumonia\", \"normal\"], pctdistance=1)\naxs[2].title.set_text(\"Distribution of val \")\n\n\n\n","a5c5aa38":"\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(15, 10))\nplt.pie(x=np.array([len(pneumonia_train_images)+len(pneumonia_test_images)+len(pneumonia_val_images), len(normal_train_images)+len(normal_test_images)+len(normal_val_images)]), autopct=\"%.1f%%\", explode=[0.2,0], labels=[\"pneumonia\", \"normal\"], pctdistance=0.5)\nplt.title(\"Type of images and total share in full dataset\", fontsize=14);","f5889529":"fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(15,10), subplot_kw={'xticks':[], 'yticks':[]})\nfor i, ax in enumerate(axes.flat):\n    img = cv2.imread(pneumonia_train_images[i])\n    img = cv2.resize(img, (500,500))\n    ax.imshow(img)\n    ax.set_title(\"Pneumonia\")\n    \nplt.show()\n\nfig, axes = plt.subplots(nrows=1, ncols=6, figsize=(15,10), subplot_kw={'xticks':[], 'yticks':[]})\nfor i, ax in enumerate(axes.flat):\n    img = cv2.imread(normal_train_images[i])\n    img = cv2.resize(img, (220,220))\n    ax.imshow(img)\n    ax.set_title(\"Normal\")\nfig.tight_layout()    \nplt.show()","11686085":"train_Pneumonia = len(os.listdir(train_path+'\/PNEUMONIA'))\ntrain_Normal =len(os.listdir(train_path+'\/NORMAL'))\nprint(f'len(train_Normal) = {train_Normal},len(train_Pneumonia)={train_Pneumonia}')","1bcf842a":"test_Pneumonia = len(os.listdir(test_path+'\/PNEUMONIA'))\ntest_Normal =len(os.listdir(test_path+'\/NORMAL'))\nprint(f'len(test_Normal) = {test_Normal},len(test_Pneumonia)={test_Pneumonia}')","56209d75":"val_Pneumonia = len(os.listdir(val_path+'\/PNEUMONIA'))\nval_Normal =len(os.listdir(val_path+'\/NORMAL'))\nprint(f'len(val_Normal) = {val_Normal},len(val_Pneumonia)={val_Pneumonia}')","ce4b52fb":"from distutils.dir_util import copy_tree\n\n# This will let us transfer our file from input dir to kaggles working\/temp\ncopy_tree(main_path,'temp')\n\npath = '.\/temp\/chest_xray'\n\ntrain_path = os.path.join(path,\"train\")\ntest_path=os.path.join(path,\"test\")\nval_path=os.path.join(path,\"val\")\n\nval_Pneumonia = len(os.listdir(val_path+'\/PNEUMONIA'))\nval_Normal =len(os.listdir(val_path+'\/NORMAL'))\nprint(f'len(val_Normal) = {val_Normal},len(val_Pneumonia)={val_Pneumonia}')","70eb82f5":"for i in ['\/NORMAL\/','\/PNEUMONIA\/']:\n    for img in os.listdir(train_path+i)[:191]:\n        os.replace(train_path+i+img, val_path+i+img)","2e82504f":"val_Pneumonia = len(os.listdir(val_path+'\/PNEUMONIA'))\nval_Normal =len(os.listdir(val_path+'\/NORMAL'))\nprint(f'len(val_Normal) = {val_Normal},len(val_Pneumonia)={val_Pneumonia}')","0e46dc36":"#Deleting variables which will not be used in furture ,to avoid Memory issues\ndel pneumonia_train_images\ndel pneumonia_test_images\ndel pneumonia_val_images\ndel normal_train_images\ndel normal_test_images\ndel normal_val_images","e4fc2425":"pneumonia_train_img = glob.glob(train_path+\"\/PNEUMONIA\/*.jpeg\")\nnormal_train_img = glob.glob(train_path+\"\/NORMAL\/*.jpeg\")\n\npneumonia_val_img = glob.glob(val_path+\"\/PNEUMONIA\/*.jpeg\")\nnormal_val_img = glob.glob(val_path+\"\/NORMAL\/*.jpeg\")\n\npneumonia_test_img = glob.glob(test_path+\"\/PNEUMONIA\/*.jpeg\")\nnormal_test_img = glob.glob(test_path+\"\/NORMAL\/*.jpeg\")\n","c056b037":"#prepare train dataset for phumonia positive\ndf_train1 = pd.DataFrame(columns=['image_name','category'])\ndf_train1['image_name'] = normal_train_img\ndf_train1['category'] = 1\n\n#prepare validation dataset for phumonia positive\ndf_val1 = pd.DataFrame(columns=['image_name','category'])\ndf_val1['image_name'] = normal_val_img\ndf_val1['category'] = 1\n\n#prepare test dataset for phumonia positive\ndf_test1 = pd.DataFrame(columns=['image_name','category'])\ndf_test1['image_name'] = normal_test_img\ndf_test1['category'] = 1\n\n#prepare train dataset for phumonia negative\ndf_train2 = pd.DataFrame(columns=['image_name','category'])\ndf_train2['image_name'] = pneumonia_train_img\ndf_train2['category'] = 0\n\n#prepare validation dataset for phumonia negative\ndf_val2 = pd.DataFrame(columns=['image_name','category'])\ndf_val2['image_name'] = pneumonia_val_img\ndf_val2['category'] = 0\n\n#prepare test dataset for phumonia negative\ndf_test2 = pd.DataFrame(columns=['image_name','category'])\ndf_test2['image_name'] = pneumonia_test_img\ndf_test2['category'] = 0","a3a087f4":"#combine both dataset\ndf_train = df_train1.append(df_train2)\ndf_val = df_val1.append(df_val2)\ndf_test = df_test1.append(df_test2)","d152e272":"print('Train dataset shape: %s' %str(df_train.shape))\nprint('Validation dataset shape: %s' %str(df_val.shape))\nprint('Test dataset shape: %s' %str(df_test.shape))","13f6f630":"#facing some memory issue , So removing unsed dataframes \ndel df_test2\ndel df_test1\ndel df_train1\ndel df_train2\ndel df_val1\ndel df_val2","ad97cdbc":"import cv2\nimg_size = 128\nx_train = []\ny_train = []\nx_val = []\ny_val = []\nx_test = []\ny_test = []\ntime_s = time.time()\n\n#read train image files\nfor i,c in zip(df_train.image_name,df_train.category):\n    img = cv2.imread(i, cv2.IMREAD_GRAYSCALE)\n    r_siz = cv2.resize(img, (img_size, img_size))\n    x_train.append(r_siz)\n    y_train.append(c)\n    \n#read validation image files\nfor i,c in zip(df_val.image_name,df_val.category):\n    img = cv2.imread(i, cv2.IMREAD_GRAYSCALE)\n    r_siz = cv2.resize(img, (img_size, img_size))\n    x_val.append(r_siz)\n    y_val.append(c)\n    \n#read test image files\nfor i,c in zip(df_test.image_name,df_test.category):\n    img = cv2.imread(i, cv2.IMREAD_GRAYSCALE)\n    r_siz = cv2.resize(img, (img_size, img_size))\n    x_test.append(r_siz)\n    y_test.append(c)\n\nprint(\"Time taken: %s seconds\" %(time.time() - time_s))","c32876af":"time_s = time.time()\n\n# training images\nx_train = np.array(x_train) \/ 255.\nx_train = x_train.reshape(-1, img_size, img_size, 1)\ny_train = np.array(y_train)\n\n# validation images\nx_val = np.array(x_val) \/ 255.\nx_val = x_val.reshape(-1, img_size, img_size, 1)\ny_val = np.array(y_val)\n\n# test images\nx_test = np.array(x_test) \/ 255.\nx_test = x_test.reshape(-1, img_size, img_size, 1)\ny_test = np.array(y_test)\n\nprint(\"Time taken: %s seconds\" %(time.time() - time_s))","924efd37":"time_s = time.time()\n\ndatagen = ImageDataGenerator(horizontal_flip=True)\nx_train_aug1 = np.copy(x_train)\ny_train_aug1 = np.copy(y_train)\nfor i, img in enumerate(x_train):\n    tmp = img.reshape(-1, img_size, img_size, 1)\n    t_iter = datagen.flow(tmp,  batch_size=1)\n    x_train_aug1[i] = next(t_iter)[0]\n\nprint(\"Time taken: %s seconds\" %(time.time() - time_s))","9613a085":"time_s = time.time()\n\ndatagen = ImageDataGenerator(width_shift_range=.2)\nx_train_aug2 = np.copy(x_train)\ny_train_aug2 = np.copy(y_train)\nfor i, img in enumerate(x_train):\n    tmp = img.reshape(-1, img_size, img_size, 1)\n    t_iter = datagen.flow(tmp,  batch_size=1)\n    x_train_aug2[i] = next(t_iter)[0]\n\nprint(\"Time taken: %s seconds\" %(time.time() - time_s))","f0b9763b":"fig, axes = plt.subplots(constrained_layout = True,\n                         nrows=6, ncols=3, figsize=(10,14))\nindex = [0, 165, 350, 4717, 4550, 4822]\n\nfor i in range(6):     \n    sample_results = []\n    sample_results.append(x_train[index[i]])\n    sample_results.append(x_train_aug1[index[i]])\n    sample_results.append(x_train_aug2[index[i]])\n    for s in range(3):\n        axes[i][s].imshow(sample_results[s])\n        axes[i][s].axis(\"off\")\n        if(s == 0):\n            t = \"Original (Pneumonia)\" if y_train[index[i]] == 1 else \"Original (Normal)\"\n            axes[i][s].title.set_text(t)\n\naxes[0][1].title.set_text(\"Horizontal Flip\")\naxes[0][2].title.set_text(\"Width Shift\")","9c6e1119":"x_train_f = np.concatenate([x_train,x_train_aug1,x_train_aug2])\ny_train_f = np.concatenate([y_train,y_train_aug1,y_train_aug2])","cb6a2aa4":"x_train_f.shape, y_train_f.shape","2ef1768f":"print('Train dataset shape: %s' %str(x_train_f.shape))\nprint('Validation dataset shape: %s' %str(x_val.shape))\nprint('Test dataset shape: %s' %str(x_test.shape))","550bf28b":"from keras import backend as K\nK.clear_session()\nmodel = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = (128,128,1)),\\\n\ntf.keras.layers.Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.001)),\\\ntf.keras.layers.Dense(64, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.001)),\\\ntf.keras.layers.Dropout(.5),\\\ntf.keras.layers.Dense(32, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.001)),\\\ntf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])","b2165fb2":"model.summary()","d1b3c956":"\n\nmodel.compile(optimizer = tf.optimizers.Adam(learning_rate=0.0001),\\\nloss = 'binary_crossentropy',\\\nmetrics=['accuracy', tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])","a2424f91":"time_s = time.time()\nepochs = 10\nhistory = model.fit( x=x_train_f, y=y_train_f,shuffle=True,batch_size=32,\\\n  validation_data=(x_val,y_val),validation_batch_size=16,\\\n  epochs = epochs\n)\nt1 = (time.time() - time_s)\nprint(\"Time taken to build model: %s seconds\" %(t1))","bc1ced30":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","2b9c873d":"print('Final training accuracy %s' %acc[-1])\nprint('Final training loss %s' %loss[-1])\nprint('Final validation accuracy %s' %val_acc[-1])\nprint('Final validation loss %s' %val_loss[-1])\n","20469d2e":"pred = model.predict(x_test, batch_size=10)\ny_pred = np.copy(y_test)\nfor i,x in enumerate(pred):\n    y_pred[i] = 1 if x[0] >= 0.5 else 0","63305a83":"score = model.evaluate(x_test, y_test) \nac1 = score[1]\npc1 = score[2]\nre1 = score[3]\nprint('Test loss:', score[0]) \nprint('Test accuracy:', score[1])\nprint('Test precision:', score[2])\nprint('Test recall:', score[3])","7f1d0ccc":"sv_cmatrix = pd.DataFrame({'test': y_test,\n                          'pred': y_pred})\n\n#Confusion matrix\nprint()\nprint()\nprint('*******Confusion Matrix******')\npd.crosstab(sv_cmatrix['test'], sv_cmatrix['pred'])","7ab747b6":"#Printing misclassified images\n\nfig, axes = plt.subplots(constrained_layout = True,\n                         nrows=1, ncols=2, figsize=(10,14))\ncl = 1\nfor i,c in enumerate(y_test):\n    if (c != pred[i] and c == cl):\n        axes[cl].imshow(x_train[i])\n        axes[cl].axis(\"off\")\n        cl -= 1\n    elif cl == -1:\n        break\n\naxes[0].title.set_text(\"Original: Normal Classified: Pneumonia\")\naxes[1].title.set_text(\"Original: Pneumonia Classified: Normal\")","83d82678":"K.clear_session()\nmodel1 = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = (128,128,1)),\\\ntf.keras.layers.Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.1)),\\\ntf.keras.layers.Dense(64, activation=tf.nn.relu,kernel_regularizer=regularizers.l2(0.1)),\\\ntf.keras.layers.Dropout(.2),\\\ntf.keras.layers.Dense(32, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.1)),\\\ntf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])","84e74435":"model1.summary()","f2b555b0":"model1.compile(optimizer = tf.optimizers.Adam(learning_rate=0.0001),\\\nloss = 'binary_crossentropy',\\\nmetrics=['accuracy', tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])","28298ee7":"# changed batch size to 128\ntime_s = time.time()\nepochs = 15\nhistory1 = model1.fit( x=x_train_f, y=y_train_f,shuffle=True,batch_size=64,\\\n  validation_data=(x_val,y_val),validation_batch_size=16,\\\n  epochs = epochs\n)\nt2 = (time.time() - time_s)\nprint(\"Time taken to build model: %s seconds\" %(t2))","fe83953a":"acc1 = history1.history['accuracy']\nval_acc1 = history1.history['val_accuracy']\n\nloss1 = history1.history['loss']\nval_loss1 = history1.history['val_loss']\n\nprint('Final training accuracy %s' %acc[-1])\nprint('Final training loss %s' %loss[-1])\nprint('Final validation accuracy %s' %val_acc[-1])\nprint('Final validation loss %s' %val_loss[-1])","ac70319a":"pred1 = model1.predict(x_test, batch_size=10)\ny_pred1 = np.copy(y_test)\nfor i,x in enumerate(pred1):\n    y_pred1[i] = 1 if x[0] >= 0.5 else 0","62e1ed3a":"score1 = model1.evaluate(x_test, y_test) \nac2 = score1[1]\npc2 = score1[2]\nre2 = score1[3]\nprint('Test loss:', score1[0]) \nprint('Test accuracy:', score1[1])\nprint('Test precision:', score1[2])\nprint('Test recall:', score1[3])","23c2df12":"sv_cmatrix1 = pd.DataFrame({'test': y_test,\n                          'pred': y_pred1})\n\n#Confusion matrix\nprint()\nprint()\nprint('*******Confusion Matrix******')\npd.crosstab(sv_cmatrix1['test'], sv_cmatrix1['pred'])","9ee4aef2":"K.clear_session()\nmodel2 = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = (128,128,1)),\\\ntf.keras.layers.Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.1)),\\\ntf.keras.layers.Dense(64, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.1)),\\\ntf.keras.layers.Dense(32, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.1)),\\\ntf.keras.layers.Dropout(.2),\\\ntf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])","bb252014":"model2.summary()","21dfe853":"model2.compile(optimizer = tf.optimizers.Adam(learning_rate=0.0001),\\\nloss = 'binary_crossentropy',\\\nmetrics=['accuracy', tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])","ea47be3e":"time_s = time.time()\nepochs = 10\nhistory2 = model2.fit( x=x_train_f, y=y_train_f,shuffle=True,batch_size=32,\\\n  validation_data=(x_val,y_val),validation_batch_size=16,\\\n  epochs = epochs\n)\nt3 = (time.time() - time_s)\nprint(\"Time taken to build model: %s seconds\" %(t3))","fcd7b6b9":"acc2 = history.history['accuracy']\nval_acc2 = history.history['val_accuracy']\n\nloss2 = history.history['loss']\nval_loss2 = history.history['val_loss']\n\nprint('Final training accuracy %s' %acc2[-1])\nprint('Final training loss %s' %loss2[-1])\nprint('Final validation accuracy %s' %val_acc2[-1])\nprint('Final validation loss %s' %val_loss2[-1])","8fbea985":"pred2 = model2.predict(x_test, batch_size=10)\ny_pred2 = np.copy(y_test)\nfor i,x in enumerate(pred2):\n    y_pred2[i] = 1 if x[0] >= 0.5 else 0","66d2d2fe":"score2 = model2.evaluate(x_test, y_test) \nac3 = score2[1]\npc3 = score2[2]\nre3 = score2[3]\nprint('Test loss:', score2[0]) \nprint('Test accuracy:', score2[1])\nprint('Test precision:', score2[2])\nprint('Test recall:', score2[3])","48ab6afc":"sv_cmatrix2 = pd.DataFrame({'test': y_test,\n                          'pred': y_pred2})\n\n#Confusion matrix\nprint()\nprint()\nprint('*******Confusion Matrix******')\npd.crosstab(sv_cmatrix2['test'], sv_cmatrix2['pred'])","04555621":"results = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'Build Time'])\nresults.loc[0] = ['3 Layer Fully Connected',ac1,pc1,re1,t1]\nresults.loc[1] = ['With Changed Batch Size',ac2,pc2,re2,t2]\nresults.loc[2] = ['With Changed Dropout Layer',ac3,pc3,re3,t3]\nresults","3d3ddc78":"# Lets print some sample image and its augmentations","fbaa81f0":"### As we can see from above Validation data is quite small so we need to add more information to Validation data, so as the reduce the variance in the Validation Accuracy","ab8308bd":"### So we can see that there is clear class imbalance in Test and Train data set . Val data set have 50-50 distribution","e5b36c98":"### Change batch size","aaa3df35":"We saw earlier in this notebook that the data was imbalanced, with more images classified as pneumonia than normal. We will correct for that in this following section. So we will do this data augumentation\n\nSince we got a imbalanced data, lets augment the data and create more images by rotating, zooming and change the width and height. Doing that will help in reducing the bias of model and also generalize it\n\nIn our case we are just applying horizontal flip and width shift augmentation\n","a5052c6e":" # Model Building","ace24997":"#### Shifting some training data to Validation data\u00b6\n","0db34e55":"## choosen Dropout layer in middle as if we apply Droupout early then it may be possible that we can loose important information\/paterrn which can helpful in good accuracy. So used to apply at middle and performance also increased with the same","8dc2394d":"# Model Prediction and Evaluation","42e4bd03":"## Used ADM optimizer as this is very fast and converge rapidly . We have choosed learning rate to 0.0001 as Upon testing with values 0.01 and 0.1, Data was not coverging well . Val dataset performance was not increasing , Accuracy was only 50% and it was not increasing in any ephocs . So best was 0.0001 ","c72bb0de":"# Comparison between models","318426cf":"# Augment images of training Dataset","8a20a377":"# Hyperparameter Tuning","01b5ead6":"## Found by running models 2-3 times that other posistions of ","4980267e":"#### Augmentation with width_shift_range=.2","10f5325d":"# Normalize and Reshape images","ce49eb9a":"#### Combine original and augmented data","29782e46":"#### We see that we have an overall imbalanced dataset. Hence, we will be using image augmentation techniques to compensate for this.","dd6abce9":"## Read image files and load","ca7f5b8c":"### Print Misclassified Sample","c3e18f6c":"## Dealing with Data Imbalance","967dcd6e":"### Change dropout layer","16c7e692":"#### Augmentation with horizontal_flip ","0d29c364":"### we are using 3 hidden layers only this model has only 2 features and if we increase the hidden layer s then it will increase the complxicity of model that is not needed in this case . So choosing idle configuration that for simple model 2-4 layers is enogh for the model","c8e6ca12":"# Above result shows doubling batch size higher improved build time, Model has more accuracy then others"}}