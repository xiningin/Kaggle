{"cell_type":{"3e91b8e8":"code","e760a94e":"code","00be6914":"code","adfaf4ae":"code","d5a6bdb4":"code","4836bc14":"code","b3be9a7e":"code","c684e407":"code","28926be5":"code","42c6d276":"code","9ef83d89":"code","52830be9":"code","c1aebb9b":"code","4ebf096b":"code","c190b47c":"code","e586b2f5":"code","330b3814":"code","4ea83558":"code","01e60501":"code","7dd639c7":"code","0f3d7a70":"code","b2c02a7a":"code","6006ee03":"code","184b0fd1":"code","2f00fbd2":"code","b1f0bed2":"code","80738736":"code","530d083f":"code","9310be05":"code","600d9d55":"code","4c9864d9":"code","1d02605e":"code","26718289":"code","9a373a62":"code","bc6dbe8f":"code","677502aa":"code","3b679426":"code","500a00a3":"code","091ff138":"code","b74f7009":"code","da4b10c4":"code","712f1236":"code","c3d4ed4c":"code","8dd11b1c":"code","d4b8e24f":"code","8a0e83d5":"code","09c28e06":"code","f1278856":"code","e22fc0f1":"code","829356e6":"code","cd100358":"code","2c0d0bf1":"code","d362ee6c":"code","10b9235f":"code","0f6023c9":"code","2a44061c":"code","8448993e":"code","277eadbb":"code","41bd372d":"code","ccf175d4":"code","4ac10ae1":"code","548f17d8":"code","99a35379":"code","79eef824":"code","d976bf6a":"code","1f318935":"code","e16994d9":"code","01b03b80":"code","5c39d028":"code","c797b22c":"code","5b9e7467":"code","86b6bfed":"code","b4920c07":"code","135196b2":"code","768d096c":"code","c14ea7dd":"code","98bef49d":"code","abb1e58d":"code","78742258":"code","a1bca8cf":"code","b43beb5f":"code","067c84bd":"markdown","063d75f6":"markdown","4206c4ae":"markdown","fbf41e9f":"markdown","961dbc9a":"markdown"},"source":{"3e91b8e8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","e760a94e":"data = pd.read_csv('..\/input\/water-potability\/water_potability.csv')","00be6914":"data.head()","adfaf4ae":"data.sample(10)","d5a6bdb4":"data.info()","4836bc14":"data.describe().T","b3be9a7e":"data.isnull().sum()","c684e407":"data.Potability.value_counts()","28926be5":"fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(26, 14))\nsns.histplot(data=data, x=\"ph\", kde=True,ax=axes[0][0])\nsns.histplot(data=data, x=\"Hardness\", kde=True,ax=axes[1][0])\nsns.histplot(data=data, x=\"Solids\", kde=True,ax=axes[0][1])\nsns.histplot(data=data, x=\"Chloramines\", kde=True,ax=axes[1][1])\nsns.histplot(data=data, x=\"Sulfate\", kde=True,ax=axes[0][2])\nsns.histplot(data=data, x=\"Conductivity\", kde=True,ax=axes[1][2])\nsns.histplot(data=data, x=\"Organic_carbon\", kde=True,ax=axes[0][3])\nsns.histplot(data=data, x=\"Trihalomethanes\", kde=True,ax=axes[1][3])\nsns.histplot(data=data, x=\"Turbidity\", kde=True,ax=axes[0][4])\nsns.countplot(data=data, x=\"Potability\",ax=axes[1][4])\n","42c6d276":"def distp(x):\n    if not x == 'Potability':\n        plt.figure(figsize=(12,12))\n        ax = sns.distplot(data[x][data.Potability == 1],color=\"darkturquoise\", rug=True)\n        sns.distplot(data[x][data.Potability == 0], color=\"lightcoral\", rug=True);\n        plt.legend(['Potable', 'Not Potable']) \n        fig.tight_layout()\nfor column in data.columns:\n    distp(column)  ","9ef83d89":"g = sns.pairplot(data, diag_kind=\"kde\",hue=\"Potability\")\ng.map_lower(sns.kdeplot, levels=4, color=\".2\")","52830be9":"plt.figure(figsize=(16,8))\nsns.heatmap(data.corr(),annot=True)","c1aebb9b":"X= data[[\"ph\",\"Sulfate\",'Trihalomethanes']]\ny=data[\"Potability\"]\nX.shape","4ebf096b":"X.isnull().sum()","c190b47c":"tree = DecisionTreeClassifier()\nresultado = []\ntipos = ['mean', 'median', 'most_frequent', 'constant']\n\nfor t in tipos:\n    imputer=SimpleImputer(strategy=t)\n    imputer.fit(X)\n    X_trans= imputer.transform(X)\n    tree = DecisionTreeClassifier(max_depth=10,random_state=42)\n    tree.fit(X_trans,y)\n    y_pred = tree.predict(X_trans)\n    f1sc=f1_score(y, y_pred, average='weighted')\n    rauc=(y, y_pred)\n    resultado.append(f1sc)\n    print(\"El escalado Utilizado--->\",t)\n    print(\"f1 segun el tipo de estrategia:\",f1sc)\n    print(\"----------------------------------------\")\n   ","e586b2f5":"vecinos = [1,3,6,9,12]\nfor v in vecinos:\n    KNN_imputer=KNNImputer(n_neighbors=v)\n    KNN_imputer.fit(X)\n    X_knn= KNN_imputer.transform(X)\n    tree = DecisionTreeClassifier(max_depth=10,random_state=42)\n    tree.fit(X_knn,y)\n    y_pred = tree.predict(X_knn)\n    f1sc=f1_score(y, y_pred, average='weighted')\n    rauc=(y, y_pred)\n    resultado.append(f1sc)\n    print(\"El escalado Utilizado--->\",t)\n    print(\"f1 segun el tipo de estrategia:\",f1sc)\n    print(\"----------------------------------------\")\n   ","330b3814":"tipos2 = ['ascending', 'descending', 'roman', 'arabic', 'random']\nfor t in tipos2:\n    iter_imputer=IterativeImputer(imputation_order=t)\n    iter_imputer.fit(X)\n    X_iter= iter_imputer.transform(X)\n    tree = DecisionTreeClassifier(max_depth=10,random_state=42)\n    tree.fit(X_iter,y)\n    f1sc=f1_score(y, y_pred, average='weighted')\n    rauc=(y, y_pred)\n    resultado.append(f1sc)\n    print(\"El escalado Utilizado--->\",t)\n    print(\"f1 segun el tipo de estrategia:\",f1sc)\n    print(\"----------------------------------------\")\n   ","4ea83558":"data.columns","01e60501":"imputer=SimpleImputer(strategy=\"constant\")\nimputer=imputer.fit(data[[\"ph\",\"Sulfate\",'Trihalomethanes']])\ndata[[\"ph\",\"Sulfate\",'Trihalomethanes']]=imputer.transform(data[[\"ph\",\"Sulfate\",'Trihalomethanes']])\n","7dd639c7":"data.isnull().sum()","0f3d7a70":"plt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"ph\"],color='lightblue')\ndata=data[(np.abs(stats.zscore(data[\"ph\"])) < 3)]\nplt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"ph\"],color='lightblue')","b2c02a7a":"plt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Hardness\"],color='lightblue')\ndata=data[(np.abs(stats.zscore(data[\"Hardness\"])) < 3)]\nplt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Hardness\"],color='lightblue')","6006ee03":"plt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Solids\"],color='lightblue')\ndata=data[(np.abs(stats.zscore(data[\"Solids\"])) < 3)]\nplt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Solids\"],color='lightblue')","184b0fd1":"plt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Chloramines\"],color='lightblue')\ndata=data[(np.abs(stats.zscore(data[\"Chloramines\"])) < 3)]\nplt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Chloramines\"],color='lightblue')","2f00fbd2":"plt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Sulfate\"],color='lightblue')\ndata=data[(np.abs(stats.zscore(data[\"Sulfate\"])) < 3)]\nplt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Sulfate\"],color='lightblue')","b1f0bed2":"plt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Conductivity\"],color='lightblue')\ndata=data[(np.abs(stats.zscore(data[\"Conductivity\"])) < 3)]\nplt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Conductivity\"],color='lightblue')","80738736":"plt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Organic_carbon\"],color='lightblue')\ndata=data[(np.abs(stats.zscore(data[\"Organic_carbon\"])) < 3)]\nplt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Organic_carbon\"],color='lightblue')","530d083f":"plt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Trihalomethanes\"],color='lightblue')\ndata=data[(np.abs(stats.zscore(data[\"Trihalomethanes\"])) < 3)]\nplt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Trihalomethanes\"],color='lightblue')","9310be05":"plt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Turbidity\"],color='lightblue')\ndata=data[(np.abs(stats.zscore(data[\"Turbidity\"])) < 3)]\nplt.figure(figsize=(8,4))\nsns.boxplot(data=data,x=data[\"Turbidity\"],color='lightblue')","600d9d55":"plt.figure(figsize=(16,8))\nsns.heatmap(data.corr(),annot=True)","4c9864d9":"robust_sc = preprocessing.RobustScaler()\nstandard_sc = preprocessing.StandardScaler() \nminmax_sc = preprocessing.MinMaxScaler()","1d02605e":"X = data.drop(['Potability'],axis=1)\ny = data[\"Potability\"]","26718289":"for x in [robust_sc,standard_sc,minmax_sc]:\n    %time\n    resultado = []\n    scaler = x.fit(X)\n    X_new = x.transform(X)\n    tree = DecisionTreeClassifier(max_depth=15,random_state=42)\n    tree.fit(X_new,y)\n    y_pred = tree.predict(X_new)\n    f1sc=f1_score(y, y_pred, average='weighted')\n    rauc=(y, y_pred)\n    resultado.append(f1sc)\n    print(\"El escalado Utilizado--->\",x)\n    print(\"f1 segun el tipo de estrategia:\",f1sc)\n    print(\"----------------------------------------\")","9a373a62":"Counter(data['Potability'])","bc6dbe8f":"over = SMOTE()\nunder = RandomUnderSampler()\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)","677502aa":"X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)","3b679426":"X_train, y_train = pipeline.fit_resample(X_train, y_train)","500a00a3":"Counter(y_train)","091ff138":"Counter(y_test)","b74f7009":"X_train=robust_sc.fit_transform(X_train)\nX_test=robust_sc.transform(X_test)","da4b10c4":"from sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV","712f1236":"def confusion(y_test,y_test_pred,X):\n    names=['non_potable','potable']\n    cm=confusion_matrix(y_test,y_test_pred)\n    f,ax=plt.subplots(figsize=(10,10))\n    sns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\n    plt.title(X, size = 25)\n    plt.xlabel(\"y_pred\")\n    plt.ylabel(\"y_true\")\n    ax.set_xticklabels(names)\n    ax.set_yticklabels(names)\n    plt.show()\n\n    return","c3d4ed4c":"gbm = XGBClassifier(verbosity=1)\nparams_xgb = {\n        \"n_estimators\":[500,1000,1500],\n        \"learning_rate\":[0.1,0.3,0.6],\n        'gpu_id': [0],\n        \"predictor\":[\"gpu_predictor\"],\n        'tree_method': ['gpu_hist'],\n        \"updater\":[\"grow_gpu_hist\"],\n        \"sampling_method\":[\"gradient_based\"],\n        \"updater\":[\"grow_gpu_hist\"]\n}","8dd11b1c":"model_xgb = GridSearchCV(gbm,param_grid=params_xgb, cv=3,n_jobs=-1)\nmodel_xgb.fit(X_train,y_train)","d4b8e24f":"print(\"Best params: \"+str(model_xgb.best_params_))\nprint(\"Best Score: \"+str(model_xgb.best_score_)+'\\n')","8a0e83d5":"scores = pd.DataFrame(model_xgb.cv_results_)\nscores.sort_values(by=\"rank_test_score\")","09c28e06":"y_train_pred_xgb = model_xgb.predict(X_train)\ny_test_pred_xgb = model_xgb.predict(X_test)","f1278856":"print(classification_report(y_test, y_test_pred_xgb))","e22fc0f1":"confusion(y_test,y_test_pred_xgb,\"XGB\")","829356e6":"clf = MLPClassifier(random_state=42)\nparams_MLP = {\n        \"hidden_layer_sizes\":[64,128,256],\n        \"activation\":[\"identity\", \"logistic\", \"tanh\", \"relu\"],\n        'solver': [\"lbfgs\", \"sgd\", \"adam\"],\n        \"learning_rate\":[\"constant\", \"invscaling\", \"adaptive\"],\n        'max_iter': [100,200],\n        \"warm_start\":[True]\n}","cd100358":"model_MLP = GridSearchCV(clf,param_grid=params_MLP, cv=3,n_jobs=-1)\nmodel_MLP.fit(X_train,y_train)\n","2c0d0bf1":"\nprint(\"Best params: \"+str(model_MLP.best_params_))\nprint(\"Best Score: \"+str(model_MLP.best_score_)+'\\n')","d362ee6c":"scores = pd.DataFrame(model_MLP.cv_results_)\nscores.sort_values(by=\"rank_test_score\")","10b9235f":"y_train_pred_MLP = model_MLP.predict(X_train)\ny_test_pred_MLP = model_MLP.predict(X_test)","0f6023c9":"print(classification_report(y_test, y_test_pred_MLP))","2a44061c":"confusion(y_test,y_test_pred_MLP,\"MLP\")","8448993e":"clf = RandomForestClassifier(random_state=42)\nparams_RF = {\n        \"max_depth\":[250,500,1000],\n        \"criterion\":[\"gini\", \"entropy\"],\n        'min_samples_split': [2,4,6],\n        \"min_samples_leaf\":[1,2,3],\n        \"max_features\":['auto', 'sqrt', 'log2'],\n        'warm_start':[True],\n        'class_weight':['balanced', 'balanced_subsample']\n}","277eadbb":"model_RF = GridSearchCV(clf,param_grid=params_RF, cv=3,n_jobs=-1)\nmodel_RF.fit(X_train,y_train)","41bd372d":"print(\"Best params: \"+str(model_RF.best_params_))\nprint(\"Best Score: \"+str(model_RF.best_score_)+'\\n')","ccf175d4":"scores = pd.DataFrame(model_RF.cv_results_)\nscores.sort_values(by=\"rank_test_score\")","4ac10ae1":"y_train_pred_RF = model_RF.predict(X_train)\ny_test_pred_RF = model_RF.predict(X_test)","548f17d8":"print(classification_report(y_test, y_test_pred_RF))","99a35379":"confusion(y_test,y_test_pred_RF,\"RF\")","79eef824":"import tensorflow as tf\nimport keras\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Dropout","d976bf6a":"model = Sequential()\n\nmodel.add(Dense(64, activation='relu', kernel_initializer='random_normal', input_dim=9))\n\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(32, activation='relu', kernel_initializer='random_normal'))\n\nmodel.add(Dense(16, activation='relu', kernel_initializer='random_normal'))\n\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))","1f318935":"model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])","e16994d9":"model.fit(X_train,y_train, batch_size=32, epochs=50, validation_data =(X_test,y_test))","01b03b80":"eval_model=model.evaluate(X_train, y_train)\neval_model","5c39d028":"y_pred=model.predict(X_test)\ny_pred =(y_pred>0.5)","c797b22c":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","5b9e7467":"print(classification_report(y_test, y_pred)) ","86b6bfed":"confusion(y_test,y_pred,\"NN\")","b4920c07":"class LRA(keras.callbacks.Callback):\n    best_weights=model.get_weights() # set a class vaiable so weights can be loaded after training is completed\n    def __init__(self, patience=2, threshold=.95, factor=.5):\n        super(LRA, self).__init__()\n        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n        self.factor=factor # factor by which to reduce the learning rate\n        self.lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it in self.lr\n        self.highest_tracc=0.0 # set highest training accuracy to 0\n        self.lowest_vloss=np.inf # set lowest validation loss to infinity\n        self.count=0\n        msg='\\n Starting Training - Initializing Custom Callback'\n        print_in_color (msg, (244, 252, 3), (55,65,80))\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        acc=logs.get('accuracy')  # get training accuracy        \n        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n            if acc>self.highest_tracc: # training accuracy improved in the epoch\n                msg= f'\\n training accuracy improved from  {self.highest_tracc:7.2f} to {acc:7.2f} learning rate held at {lr:9.6f}'\n                print_in_color(msg, (0,255,0), (55,65,80))\n                self.highest_tracc=acc # set new highest training accuracy\n                LRA.best_weights=model.get_weights() # traing accuracy improved so save the weights\n                count=0 # set count to 0 since training accuracy improved\n                if v_loss<self.lowest_vloss:\n                    self.lowest_vloss=v_loss                    \n            else:  # training accuracy did not improve check if this has happened for patience number of epochs if so adjust learning rate\n                if self.count>=self.patience -1:\n                    self.lr= lr* self.factor # adjust the learning by factor\n                    tf.keras.backend.set_value(model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    self.count=0 # reset the count to 0\n                    if v_loss<self.lowest_vloss:\n                        self.lowest_vloss=v_loss\n                    msg=f'\\nfor epoch {epoch +1} training accuracy did not improve for {self.patience } consecutive epochs, learning rate adjusted to {lr:9.6f}'\n                    print_in_color(msg, (255,0,0), (55,65,80))\n                else:\n                    self.count=self.count +1\n                    msg=f'\\nfor  epoch {epoch +1} training accuracy did not improve, patience count incremented to {self.count}'\n                    print_in_color(msg, (255,255,0), (55,65,80))\n        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n            if v_loss< self.lowest_vloss: # check if the validation loss improved\n                msg=f'\\n for epoch {epoch+1} validation loss improved from  {self.lowest_vloss:7.4f} to {v_loss:7.4}, saving best weights'\n                print_in_color(msg, (0,255,0), (55,65,80))\n                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n                LRA.best_weights=model.get_weights() # validation loss improved so save the weights\n                self.count=0 # reset count since validation loss improved               \n            else: # validation loss did not improve\n                if self.count>=self.patience-1:\n                    self.lr=self.lr * self.factor\n                    msg=f' \\nfor epoch {epoch+1} validation loss failed to improve for {self.patience} consecutive epochs, learning rate adjusted to {self.lr:9.6f}'\n                    self.count=0 # reset counter\n                    print_in_color(msg, (255,0,0), (55,65,80))\n                    tf.keras.backend.set_value(model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                else: \n                    self.count =self.count +1 # increment the count\n                    msg=f' \\nfor epoch {epoch+1} validation loss did not improve patience count incremented to {self.count}'\n                    print_in_color(msg, (255,255,0), (55,65,80))\n                    ","135196b2":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat))\n    print('\\33[0m') # returns default print color to back to black\n    return","768d096c":"epochs=50","c14ea7dd":"callbacks=[LRA()]\nmodel.fit(X_train, y_train,epochs=epochs,\n                              verbose=1,\n                              validation_data =(X_train,y_train),callbacks=callbacks,shuffle=True)\n","98bef49d":"eval_model=model.evaluate(X_train, y_train)\neval_model","abb1e58d":"y_pred=model.predict(X_test)\ny_pred =(y_pred>0.5)","78742258":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)","a1bca8cf":"print(classification_report(y_test, y_pred)) ","b43beb5f":"confusion(y_test,y_pred,\"NN with LRA\")","067c84bd":"### Preprocessing","063d75f6":"### EDA","4206c4ae":"## Deep Learning","fbf41e9f":"### WIth LRA","961dbc9a":"### Model"}}