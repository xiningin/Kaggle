{"cell_type":{"f1654108":"code","d0d6bc70":"code","f6b26a93":"code","9aeab902":"code","29629c97":"code","7e034dba":"code","221d201e":"markdown","b7381794":"markdown"},"source":{"f1654108":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","d0d6bc70":"!git clone https:\/\/github.com\/lezwon\/pytorch-lightning.git","f6b26a93":"!cd pytorch-lightning && git pull origin 1246_tpu_tests && git checkout 1246_tpu_tests","9aeab902":"!pip install -r pytorch-lightning\/tests\/requirements.txt","29629c97":"!pytest pytorch-lightning\/tests\/models\/test_tpu.py","7e034dba":"!pytest pytorch-lightning\/tests\/trainer\/test_trainer.py::test_tpu_choice","221d201e":"# Parallel KFold training on TPU using Pytorch Lightning\nThis kernel demonstrates training K instances of a model parallely on each TPU core.","b7381794":"### Install XLA\nXLA powers the TPU support for PyTorch"}}