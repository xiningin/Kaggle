{"cell_type":{"f6608a1b":"code","a626d6ad":"code","15aea77f":"code","464a1e12":"code","e76bd3c5":"code","02333b2a":"code","670ae046":"code","fe6cf1fa":"code","1e4679a4":"code","d793b0b5":"code","2c1cb465":"code","f66917a7":"code","72e34367":"code","81b47c39":"code","afa823e8":"code","f2733363":"code","5fe810d6":"code","99428238":"code","a31308ff":"code","2009e0fd":"code","0b22d3ed":"code","70ba29f5":"code","307132de":"code","89352330":"code","dbcc9d09":"code","e8ee3a7b":"code","9290a916":"code","5205d25a":"code","457e7344":"markdown","b9b006e7":"markdown","4c60383c":"markdown","601a1e37":"markdown","da33b699":"markdown","97afbfac":"markdown","90626115":"markdown","aaa7924e":"markdown","69516ad6":"markdown","4c7e3d22":"markdown","db9457b5":"markdown"},"source":{"f6608a1b":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords, gutenberg\nfrom nltk.stem.porter import *\nimport gensim\nfrom gensim.models.phrases import Phraser, Phrases\nfrom gensim.models.word2vec import Word2Vec\nimport string\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom bokeh.io import output_notebook, output_file\nfrom bokeh.plotting import show, figure\n# nltk.download('gutenberg') # Need to download resource the first time\n# nltk.download('stopwords') # Need to download resource the first time","a626d6ad":"gutenberg.fileids()","15aea77f":"alice = gutenberg.raw('carroll-alice.txt')","464a1e12":"sentences_v1 = sent_tokenize(gutenberg.raw()) # Break text into sentences\nsentences_v1[0:5]","e76bd3c5":"print(word_tokenize(sentences_v1[1])) # Break a sentence into words","02333b2a":"sentences_v2 = gutenberg.sents('carroll-alice.txt') # Gutenberg library comes with built in sents() method\nsentences_v2","670ae046":"def lowercase(corpus):\n    sentences_lower = []\n\n    for sentence in corpus:\n        sentences_lower.append([w.lower() for w in sentence])\n\n    return sentences_lower\n\nsentences_lower = lowercase(sentences_v2)\nsentences_lower[0:2]","fe6cf1fa":"stopwords = stopwords.words('english')\nstopwords[0:10]","1e4679a4":"def remove_stopwords(corpus):\n    no_stopwords = []\n\n    for sentence in corpus:\n        no_stopwords.append([w for w in sentence if w not in stopwords])\n    \n    return no_stopwords\n\nno_stopwords = remove_stopwords(sentences_v2)\nno_stopwords[0:2]","d793b0b5":"punctuation = string.punctuation\npunctuation","2c1cb465":"# Slow\ndef remove_punctuation(corpus):\n    no_punctuation = []\n\n    for sentence in corpus:\n        no_punctuation.append([c for c in sentence if c not in list(punctuation)])\n\n    return no_punctuation\n\nno_punctuation = remove_punctuation(sentences_v2)\nno_punctuation[0:2]","f66917a7":"# Faster\ndef remove_punctuation(corpus):\n    no_punctuation = []\n\n    for sentence in corpus:\n        no_punctuation.append([word.translate(str.maketrans(\"\",\"\",punctuation)) for word in sentence if word.translate(str.maketrans(\"\",\"\",punctuation)) != \"\"])\n\n    return no_punctuation\n\nno_punctuation = remove_punctuation(sentences_v2)\nno_punctuation[0:2]","72e34367":"def apply_stemming(corpus):\n    stemmer = PorterStemmer()\n    stems = []\n\n    for sentence in corpus:\n        stems.append([stemmer.stem(w) for w in sentence])\n        \n    return stems\n\nstems = apply_stemming(sentences_v2)\nstems[0:2]","81b47c39":"# Train the detector on the corpus; apply a min_count and threshold to refine\nphrases = Phrases(sentences_lower)\n# Create a dictionary for parsing bi-grams\nbigram = Phraser(phrases)","afa823e8":"# Print all bigrams (long list)\n# bigram.phrasegrams ","f2733363":"# Sort bigrams by score\nsorted_bigrams = {k:v for k,v in sorted(bigram.phrasegrams.items(), key=lambda item: item[1], reverse=True)}","5fe810d6":"# Print top 10 bigrams by score (notice that none of these are actual bi-grams, so we'd want to tweak the parameters of Phrases() to cut out the noise)\nfor i, (k, v) in enumerate(sorted_bigrams.items()):\n    if i < 10:\n        print(k,v)","99428238":"def apply_bigrams(corpus):\n    bigram = Phraser(Phrases(corpus))\n    sentences_bigrams = []\n\n    for sentence in corpus:\n        sentences_bigrams.append(bigram[sentence])\n        \n    return sentences_bigrams\n\nsentences_bigrams = apply_bigrams(sentences_v2)\nsentences_bigrams[0:2]","a31308ff":"# Prepare corpus\ntokenized_corpus = gutenberg.sents()\nclean_corpus = remove_punctuation(remove_stopwords(lowercase(tokenized_corpus)))","2009e0fd":"# size = number of dimensions\n# sg = skip-gram or CBOW architecture\n# window = number of context words to consider\n# iter = number of epochs\n# min-count = number of times word must appear in corpus in order to fit into word vector space\n# workers = number of processing cores\n\n# Run Word2Vec\nmodel = Word2Vec(sentences=clean_corpus, size=64, sg=1, window=10, iter=5, min_count=10, workers=4)","0b22d3ed":"# Number of words in our corpus (after min count threshold applied)\nlen(model.wv.vocab)","70ba29f5":"# View entire vocabulary (long list)\n# model.wv.vocab","307132de":"# View a word's location in n-dimensional space (here, 64-dimensional)\nmodel.wv['queen']","89352330":"# View similar words\nmodel.wv.most_similar(\"queen\", topn=3)","dbcc9d09":"# Extract a 2D represention of the word vector space\ntsne = TSNE(n_components=2, n_iter=1000)\nwv_2d = tsne.fit_transform(model.wv[model.wv.vocab])","e8ee3a7b":"# Compile a dataframe with x,y coords\nword_coords = pd.DataFrame(wv_2d, columns=[\"x\", \"y\"])\nword_coords[\"token\"] = model.wv.vocab.keys()\nword_coords.head()","9290a916":"# Plot 2D vectorspace (not very helpful, but still cool)\nsns.scatterplot(x=\"x\", y=\"y\", data=word_coords)\nplt.title(\"2D Representation of Entire Vector Space\")\nplt.show()","5205d25a":"# Plot an interactive chart of a sample of the word vectors within the vectorspace (for speed & clarity)\noutput_notebook()\ncorpus_sample = word_coords.sample(1000)\np = figure(plot_width=800, plot_height=800)\n_ = p.text(x=corpus_sample[\"x\"], y=corpus_sample[\"y\"], text=corpus_sample[\"token\"])\nshow(p)","457e7344":"### Tokenize\n\nTokenization is used to break the corpus into sentences and individual words within these sentences.","b9b006e7":"### Remove Punctuation\n\nPunctuation is almost always removed, unless specific punctuation is important for representation learning (ex: \"?\" fo question answering tasks).","4c60383c":"### Convert to Lowercase\n\nFor small datasets, all characters are converted to lowercase. For larger datasets, where there are many examples of words with differing capitalizations, it can be helpful to skip this step.","601a1e37":"# NLP Preprocessing Pipeline\n\nGreetings! In this notebook we will review common preprocessing techniques that applied to natural language data.\n\nFor this exploration, we will use the Gutenberg corpus, a collection of classic, free-to-use books.","da33b699":"### List Available Texts","97afbfac":"### Conclusion\n\nI hope you enjoyed this quick review of NLP preprocessing techniques (especially the interactive word vectorspace plot at the end - that's my favorite!). \n\nPlease leave a comment below if this helped you out or if you have suggestions for improvement.\n\nAnd until next time, happy coding :)","90626115":"### Load a Text","aaa7924e":"### Apply Word2Vec\n\nWord2Vec produces word embeddings, i.e. it converts text into vectors within high dimensional space, which can then be passed into a machine learning model.\n\nThe larger the corpus, the more accurate are the numerical representations of word features in vectorspace.\n\nBelow, the entirety of the Gutenberg corpus is cleaned and passed into the Word2Vec model.","69516ad6":"### Stemming\n\nTruncating words down to their stems is often used with smaller datasets because words with similar meanings are grouped into a single token.","4c7e3d22":"### Handle Bi-Grams\n\nSome words that co-occur frequently are better interpreted as one token, rather than two.","db9457b5":"### Remove Stopwords\n\nThe choice to remove stopwords is context dependent. Sentiment analysis often benefits from including stopwords such as \"not\", in order to differentiate phrases like \"not good\"."}}