{"cell_type":{"c0c45e31":"code","f3f7bc2f":"code","0b14f334":"code","ae1317c3":"code","bde495e5":"code","30017bcf":"code","82547765":"code","21d71ee6":"code","c799c018":"code","4b1410d3":"code","786469b6":"code","4aecd68f":"code","e438f9cc":"code","1f3928c5":"code","1ded65cf":"code","6e16f992":"code","f31efc65":"code","724f6905":"code","dcd3ffe4":"code","89dc881f":"code","88bbeb62":"code","3129ff31":"code","75cef00a":"code","1802a06b":"code","bb3702c4":"code","9f82a75e":"code","4c628bd6":"code","b304fc5e":"code","e896dcff":"code","742da951":"code","09da676c":"markdown","b84a2ad8":"markdown","b1778d61":"markdown","383b36f6":"markdown","aa492b51":"markdown","4a73b076":"markdown","f3c15b16":"markdown","b8d93635":"markdown"},"source":{"c0c45e31":"import numpy as np\n\nimport pandas as pd\nfrom pandas import DataFrame\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\n\nimport re\n\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport sys\nsys.path = [\n    '..\/input\/readability-package',\n] + sys.path\nimport readability\nimport spacy\n\nfrom sklearn import model_selection\n\nimport transformers\nimport torch\nimport pytorch_lightning as pl\nfrom transformers import BertModel, BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import DataLoader, Dataset\n\nimport random\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import KFold\n\nimport lightgbm as lgb\n\nfrom fastprogress.fastprogress import  progress_bar","f3f7bc2f":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\n\ntrain_df['excerpt'] = train_df['excerpt'].apply(lambda e: e.replace('\\n', ''))\ntest_df['excerpt'] = test_df['excerpt'].apply(lambda e: e.replace('\\n', ''))","0b14f334":"def preprocess(df):\n    excerpt_processed=[]\n    for e in progress_bar(df['excerpt']):\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        excerpt_processed.append(e)\n        \n    return excerpt_processed","ae1317c3":"train_df['excerpt_preprocessed'] = preprocess(train_df)\ntest_df[\"excerpt_preprocessed\"] = preprocess(test_df)","bde495e5":"#source: https:\/\/www.kaggle.com\/ravishah1\/readability-feature-engineering-non-nn-baseline\/data\n\ndef readability_measurements(passage: str):\n    \"\"\"\n    This function uses the readability library for feature engineering.\n    It includes textual statistics, readability scales and metric, and some pos stats\n    \"\"\"\n    results = readability.getmeasures(passage, lang='en')\n    \n    chars_per_word = results['sentence info']['characters_per_word']\n    syll_per_word = results['sentence info']['syll_per_word']\n    words_per_sent = results['sentence info']['words_per_sentence']\n    \n    kincaid = results['readability grades']['Kincaid']\n    ari = results['readability grades']['ARI']\n    coleman_liau = results['readability grades']['Coleman-Liau']\n    flesch = results['readability grades']['FleschReadingEase']\n    gunning_fog = results['readability grades']['GunningFogIndex']\n    lix = results['readability grades']['LIX']\n    smog = results['readability grades']['SMOGIndex']\n    rix = results['readability grades']['RIX']\n    dale_chall = results['readability grades']['DaleChallIndex']\n    \n    tobeverb = results['word usage']['tobeverb']\n    auxverb = results['word usage']['auxverb']\n    conjunction = results['word usage']['conjunction']\n    pronoun = results['word usage']['pronoun']\n    preposition = results['word usage']['preposition']\n    nominalization = results['word usage']['nominalization']\n    \n    pronoun_b = results['sentence beginnings']['pronoun']\n    interrogative = results['sentence beginnings']['interrogative']\n    article = results['sentence beginnings']['article']\n    subordination = results['sentence beginnings']['subordination']\n    conjunction_b = results['sentence beginnings']['conjunction']\n    preposition_b = results['sentence beginnings']['preposition']\n\n    \n    return [chars_per_word, syll_per_word, words_per_sent,\n            kincaid, ari, coleman_liau, flesch, gunning_fog, lix, smog, rix, dale_chall,\n            tobeverb, auxverb, conjunction, pronoun, preposition, nominalization,\n            pronoun_b, interrogative, article, subordination, conjunction_b, preposition_b]","30017bcf":"def spacy_features(df: pd.DataFrame):\n    \"\"\"\n    This function generates features using spacy en_core_wb_lg\n    I learned about this from these resources:\n    https:\/\/www.kaggle.com\/konradb\/linear-baseline-with-cv\n    https:\/\/www.kaggle.com\/anaverageengineer\/comlrp-baseline-for-complete-beginners\n    \"\"\"\n    \n    nlp = spacy.load('en_core_web_lg')\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(text).vector for text in df.excerpt])\n        \n    return vectors\n\ndef get_spacy_col_names():\n    names = list()\n    for i in range(300):\n        names.append(f\"spacy_{i}\")\n        \n    return names","82547765":"def pos_tag_features(passage: str):\n    \"\"\"\n    This function counts the number of times different parts of speech occur in an excerpt\n    \"\"\"\n    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n    \n    tags = pos_tag(word_tokenize(passage))\n    tag_list= list()\n    \n    for tag in pos_tags:\n        tag_list.append(len([i[0] for i in tags if i[1] == tag]))\n    \n    return tag_list","21d71ee6":"def generate_other_features(passage: str):\n    \"\"\"\n    This function is where I test miscellaneous features\n    This is experimental\n    \"\"\"\n    # punctuation count\n    periods = passage.count(\".\")\n    commas = passage.count(\",\")\n    semis = passage.count(\";\")\n    exclaims = passage.count(\"!\")\n    questions = passage.count(\"?\")\n    \n    # Some other stats\n    num_char = len(passage)\n    num_words = len(passage.split(\" \"))\n    unique_words = len(set(passage.split(\" \") ))\n    word_diversity = unique_words\/num_words\n    \n    word_len = [len(w) for w in passage.split(\" \")]\n    longest_word = np.max(word_len)\n    avg_len_word = np.mean(word_len)\n    \n    return [periods, commas, semis, exclaims, questions,\n            num_char, num_words, unique_words, word_diversity,\n            longest_word, avg_len_word]","c799c018":"def create_folds(data: pd.DataFrame, num_splits: int):\n    \"\"\" \n    This function creates a kfold cross validation system based on this reference: \n    https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\n    \"\"\"\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","4b1410d3":"class CLRDataset:\n    \"\"\"\n    This is my CommonLit Readability Dataset.\n    By calling the get_df method on an object of this class,\n    you will have a fully feature engineered dataframe\n    \"\"\"\n    def __init__(self, df: pd.DataFrame, train: bool, n_folds=2):\n        self.df = df\n        self.excerpts = df[\"excerpt_preprocessed\"]\n        \n        self._extract_features()\n        \n        if train:\n            self.df = create_folds(self.df, n_folds)\n        \n    def _extract_features(self):\n        scores_df = pd.DataFrame(self.df[\"excerpt_preprocessed\"].apply(lambda p : readability_measurements(p)).tolist(), \n                                 columns=[\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\n                                          \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n                                          \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\",\n                                          \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\"])\n        self.df = pd.merge(self.df, scores_df, left_index=True, right_index=True)\n        \n        spacy_df = pd.DataFrame(spacy_features(self.df), columns=get_spacy_col_names())\n        self.df = pd.merge(self.df, spacy_df, left_index=True, right_index=True)\n        \n        pos_df = pd.DataFrame(self.df[\"excerpt_preprocessed\"].apply(lambda p : pos_tag_features(p)).tolist(),\n                              columns=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                                       \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                                       \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"])\n        self.df = pd.merge(self.df, pos_df, left_index=True, right_index=True)\n        \n        other_df = pd.DataFrame(self.df[\"excerpt_preprocessed\"].apply(lambda p : generate_other_features(p)).tolist(),\n                                columns=[\"periods\", \"commas\", \"semis\", \"exclaims\", \"questions\",\n                                         \"num_char\", \"num_words\", \"unique_words\", \"word_diversity\",\n                                         \"longest_word\", \"avg_len_word\"])\n        self.df = pd.merge(self.df, other_df, left_index=True, right_index=True)\n        \n    def get_df(self):\n        return self.df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx: int):\n        pass","786469b6":"dataset = CLRDataset(train_df, train=True)\ntrain_df = dataset.get_df()\n\ntrain_df","4aecd68f":"test_dataset = CLRDataset(test_df, train=False)\ntest_df = test_dataset.get_df()\n\ntest_df","e438f9cc":"MODEL_PATH = '..\/input\/huggingface-bert\/bert-base-uncased'\n\ntokenizer = BertTokenizer.from_pretrained(MODEL_PATH)","1f3928c5":"class BertForSequenceClassification_pl(pl.LightningModule):\n    def __init__(self, model_name, num_labels, lr):\n        super().__init__()\n        \n        self.save_hyperparameters()\n        \n        self.bert_sc = BertForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=num_labels\n        )\n        \n    def training_step(self, batch, batch_idx):\n        output = self.bert_sc(**batch)\n        loss = output.loss\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        output = self.bert_sc(**batch)\n        val_loss = output.loss\n        self.log('val_loss', val_loss)\n        \n    def test_step(self, batch, batch_idx):\n        labels = batch.pop('labels')\n        output = self.bert_sc(**batch)\n        labels_predicted = output.logits.argmax(-1)\n        num_correct = (labels_predicted == labels).sum().item()\n        accuracy = num_correct \/ labels.size(0)\n        self.log('accuracy', accuracy)\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)","1ded65cf":"checkpoint = pl.callbacks.ModelCheckpoint(\n    monitor='val_loss',\n    mode='min',\n    save_top_k=1,\n    save_weights_only=True,\n    dirpath='model\/'\n)\n\ntrainer = pl.Trainer(\n    gpus=1,\n    max_epochs=10,\n    callbacks=[checkpoint]\n)","6e16f992":"def createBertFineDataSet(excerpts, targets):\n    data = []    \n    for excerpt, target in zip(excerpts, targets):\n        encoding = tokenizer.encode_plus(\n            excerpt,\n            max_length = 314,\n            padding='max_length',\n            truncation=True\n        )\n\n        encoding['labels'] = target\n        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n\n        data.append(encoding)\n\n    return data","f31efc65":"# update kfold values for fine tune\nkfolds = []\n\nfor i in progress_bar(train_df.index):\n    kfolds.append(i % 5)\n    \ntrain_df['kfold'] = kfolds","724f6905":"model = BertForSequenceClassification_pl(\n    MODEL_PATH,\n    num_labels=1,\n    lr=1e-5\n)","dcd3ffe4":"for i in progress_bar(train_df['kfold'].unique()):\n    train_df_for_fine_tune = train_df[train_df['kfold'] != i]\n    test_df_for_fine_tune = train_df[train_df['kfold'] == i]\n    \n    dataset_train = createBertFineDataSet(\n        train_df_for_fine_tune['excerpt'],\n        train_df_for_fine_tune['target']\n    )\n    \n    dataset_val = createBertFineDataSet(\n        test_df_for_fine_tune['excerpt'],\n        test_df_for_fine_tune['target']\n    )\n\n    train_dataloader = DataLoader(\n        dataset_train,\n        batch_size=32,\n        shuffle=True\n    )\n    val_dataloader = DataLoader(\n        dataset_val, \n        batch_size=256\n    )\n\n    trainer.fit(model, train_dataloader, val_dataloader)","89dc881f":"best_model_path = checkpoint.best_model_path\n\nmodel = BertForSequenceClassification_pl.load_from_checkpoint(\n    best_model_path\n)\n\nFINE_TUNED_MODEL_PATH = '\/kaggle\/working\/model_transformers'\n\nmodel.bert_sc.save_pretrained(FINE_TUNED_MODEL_PATH)","88bbeb62":"class BertDataset(nn.Module):\n    def __init__(self, df, tokenizer, max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer.encode_plus(\n            self.excerpt[idx],\n            return_tensors='pt',\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True\n        )\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \n\ndef get_embeddings(df, path, plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    MODEL_PATH = path\n    model = BertModel.from_pretrained(MODEL_PATH, num_labels=1)\n    model.to(device)\n    model.eval()\n    \n    ds = BertDataset(df, tokenizer, config['max_len'])\n    dl = DataLoader(\n        ds,\n        batch_size=config[\"batch_size\"],\n        shuffle=False,\n        num_workers = 4,\n        pin_memory=True,\n        drop_last=False\n    )\n\n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in progress_bar(list(enumerate(dl))):\n            inputs = {key:val.reshape(val.shape[0], -1).to(device) for key, val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs[0][:, 0].detach().cpu().numpy()\n            embeddings.extend(outputs)\n            \n    return np.array(embeddings)","3129ff31":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","75cef00a":"config = {\n    'batch_size': 128,\n    'max_len': 256,\n    'seed': 42,\n}\nseed_everything(seed=config['seed'])\n\ntrain_embeddings =  get_embeddings(train_df, FINE_TUNED_MODEL_PATH)\ntest_embeddings = get_embeddings(test_df, FINE_TUNED_MODEL_PATH)","1802a06b":"pd.set_option('display.max_rows', 500)\n\ntrain_df.filter(regex='^(?!.*spacy_).*$').corr().query('target < -0.5 | 0.5 < target')['target']","bb3702c4":"columns = [\n    'chars_per_word', \n    'syll_per_word',\n    'coleman_liau',\n    'smog', \n    'rix',\n    'dale_chall',\n    'avg_len_word'\n]","9f82a75e":"X_train = pd.DataFrame(train_embeddings)\nX_train = pd.concat([X_train, train_df[columns]], axis=1)\n\nX_test = pd.DataFrame(test_embeddings)\nX_test = pd.concat([X_test, test_df[columns]], axis=1)","4c628bd6":"y_train = train_df[['target']]","b304fc5e":"kf = KFold(n_splits=5, shuffle=True, random_state=71)\n\ncv = list(kf.split(X_train, y_train))","e896dcff":"params = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'verbose': -1,\n    'learning_rate': 0.5,\n    'max_depth': 3,\n    'feature_pre_filter': False,\n    'lambda_l1': 2.215942517163985,\n    'lambda_l2': 0.0015606472088872934,\n    'num_leaves': 2,\n    'feature_fraction': 0.8999999999999999,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'min_child_samples': 20,\n}\n\npred = np.zeros(X_test.shape[0])\nrmses = []\n\nfor tr_idx, val_idx in progress_bar(cv):\n    x_tr, x_va = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n\n    train_set = lgb.Dataset(x_tr, y_tr)\n    val_set = lgb.Dataset(x_va, y_va, reference=train_set)\n\n    model = lgb.train(\n        params,\n        train_set, \n        num_boost_round=10000,\n        early_stopping_rounds=100,\n        valid_sets=[train_set, val_set], \n        verbose_eval=-1\n    )\n\n    y_pred = model.predict(x_va)\n    rmse = np.sqrt(mse(y_va, y_pred))\n    rmses.append(rmse)\n    \n    tmp_pred = model.predict(X_test)\n    pred += tmp_pred \/ 5\n    \nprint(\"\\n\", \"Mean Fold RMSE:\", np.mean(rmses))","742da951":"predictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = pred\npredictions.to_csv(\"submission.csv\", index=False)\n\npredictions","09da676c":"## Fine Tuning","b84a2ad8":"# Fetch some features","b1778d61":"# Light GBM","383b36f6":"# Cleaning Texts Function","aa492b51":"# Preparation","4a73b076":"# Vectorize By BERT Function","f3c15b16":"## Bert interface","b8d93635":"# Prepare train and test data"}}