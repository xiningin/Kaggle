{"cell_type":{"0362f0d3":"code","3f79c13f":"code","1aaf5d13":"code","abe766f6":"code","d5b8a09e":"code","e14bd843":"code","f40406b4":"code","4c62335c":"code","7c27a643":"code","f30c0f34":"code","8d084fe1":"code","62c7483e":"code","98d80f70":"code","d8dceab7":"code","5f270cff":"code","b7991e34":"code","7d0c10db":"code","bbf96e7c":"code","3ca285fd":"code","4a2b35e8":"markdown","b97fd4d1":"markdown","71e7be3b":"markdown","c49615b5":"markdown","b7a3ad63":"markdown","3676b00f":"markdown","8cdbb293":"markdown","113902b8":"markdown","49666dfe":"markdown","28d5f94b":"markdown","40004acb":"markdown","b0943fe7":"markdown","5980c7a0":"markdown","f37c68c7":"markdown","1cb07a52":"markdown","5fd6143a":"markdown","444bc6af":"markdown","3f3cf2d3":"markdown","43476d0b":"markdown","a401960b":"markdown","587ae0b1":"markdown"},"source":{"0362f0d3":"!pip install seaborn --upgrade #Update Seaborn for Plotting","3f79c13f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno #Visualize null\n\n#Plotting Functions\nimport matplotlib.pyplot as plt\n\n#Aesthetics\nimport seaborn as sns\nsns.set_style('ticks') #No grid with ticks\nprint(sns.__version__)","1aaf5d13":"#Data Import\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","abe766f6":"fetal=pd.read_csv('\/kaggle\/input\/fetal-health-classification\/fetal_health.csv')\nfetal.info()\nfetal.head()","d5b8a09e":"#Dropping duplicates\nfetal_dup=fetal.copy()\nfetal_dup.drop_duplicates(inplace=True)\nprint('Total number of replicates are:', fetal.shape[0] - fetal_dup.shape[0])\nfetal=fetal_dup.copy()\nfetal","e14bd843":"def Plotter(plot, x_label, y_label, x_rot=None, y_rot=None,  fontsize=12, fontweight=None, legend=None, save=False,save_name=None):\n    \"\"\"\n    Helper function to make a quick consistent plot with few easy changes for aesthetics.\n    Input:\n    plot: sns or matplot plotting function\n    x_label: x_label as string\n    y_label: y_label as string\n    x_rot: x-tick rotation, default=None, can be int 0-360\n    y_rot: y-tick rotation, default=None, can be int 0-360\n    fontsize: size of plot font on axis, defaul=12, can be int\/float\n    fontweight: Adding character to font, default=None, can be 'bold'\n    legend: Choice of including legend, default=None, bool, True:False\n    save: Saves image output, default=False, bool\n    save_name: Name of output image file as .png. Requires Save to be True.\n               default=None, string: 'Insert Name.png'\n    Output: A customized plot based on given parameters and an output file\n    \n    \"\"\"\n    #Ticks\n    ax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\n    plt.xticks(fontsize=fontsize, fontweight=fontweight, rotation=x_rot)\n    plt.yticks(fontsize=fontsize, fontweight=fontweight, rotation=y_rot)\n\n    #Legend\n    if legend==None:\n        pass\n    elif legend==True:\n        \n        plt.legend()\n        ax.legend()\n        pass\n    else:\n        ax.legend().remove()\n        \n    #Labels\n    plt.xlabel(x_label, fontsize=fontsize, fontweight=fontweight, color='k')\n    plt.ylabel(y_label, fontsize=fontsize, fontweight=fontweight, color='k')\n\n    #Removing Spines and setting up remianing, preset prior to use.\n    ax.spines['top'].set_color(None)\n    ax.spines['right'].set_color(None)\n    ax.spines['bottom'].set_color('k')\n    ax.spines['bottom'].set_linewidth(3)\n    ax.spines['left'].set_color('k')\n    ax.spines['left'].set_linewidth(3)\n    \n    if save==True:\n        plt.savefig(save_name)","f40406b4":"fig, ax=plt.subplots()#Required outside of function. This needs to be activated first when plotting in every code block\nplot=sns.countplot(data=fetal, x='fetal_health', hue='fetal_health', palette=['b','r','g'])#count plot\nPlotter(plot, 'fetal_health level', 'count', legend=True, save=True, save_name='fetal health count.png')#Plotter function for aesthetics\nplot","4c62335c":"fig, ax=plt.subplots(figsize=(12,12))#Required outside of function. This needs to be activated first when plotting in every code block\nplot=sns.heatmap(fetal.corr(),annot=True, cmap='Blues', linewidths=1)\nPlotter(plot, None, None, 90,legend=False, save=True, save_name='Corr.png')","7c27a643":"from sklearn.feature_selection import SelectKBest #Feature Selector\nfrom sklearn.feature_selection import f_classif #F-ratio statistic for categorical values","f30c0f34":"#Feature Selection\nX=fetal.drop(['fetal_health'], axis=1)\nY=fetal['fetal_health']\nbestfeatures = SelectKBest(score_func=f_classif, k='all')\nfit = bestfeatures.fit(X,Y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\n\n#Visualize the feature scores\nfig, ax=plt.subplots(figsize=(7,7))\nplot=sns.barplot(data=featureScores, x='Score', y='Feature', palette='viridis',linewidth=0.5, saturation=2, orient='h')\nPlotter(plot, 'Score', 'Feature', legend=False, save=True, save_name='Feature Importance.png')#Plotter function for aesthetics\nplot","8d084fe1":"#Selection method\nselection=featureScores[featureScores['Score']>=200]#Selects features that scored more than 200\nselection=list(selection['Feature'])#Generates the features into a list\nselection.append('fetal_health')#Adding the Level string to be used to make new data frame\nnew_fetal=fetal[selection] #New dataframe with selected features\nnew_fetal.head() #Lets take a look at the first 5","62c7483e":"new_name_fetal=new_fetal.rename(columns = {'percentage_of_time_with_abnormal_long_term_variability':'%_ab_long_var', \n                                           'abnormal_short_term_variability': 'short_var'}) #Reduce the size of names for plotting\nsns.pairplot(new_name_fetal, hue='fetal_health')","98d80f70":"#Splitting\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(fetal.drop(['fetal_health'], axis=1), fetal['fetal_health'],test_size=0.30, random_state=0, \n                                                 stratify=fetal['fetal_health'])\n\n#Checking the shapes\nprint(\"X_train shape :\",X_train.shape)\nprint(\"Y_train shape :\",y_train.shape)\nprint(\"X_test shape :\",X_test.shape)\nprint(\"Y_test shape :\",y_test.shape)\n\n#Scaling\nfrom sklearn import preprocessing\nscaler=preprocessing.StandardScaler()\n\nX_train_scaled=scaler.fit_transform(X_train) #Scaling and fitting the training set to a model\nX_test_scaled=scaler.transform(X_test) #Transformation of testing set based off of trained scaler model","d8dceab7":"#Packages for metrics and search\n\"\"\"These packages are required for the functions below\n\"\"\"\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV #Paramterizers\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix #Accuracy metrics\nimport itertools #Used for iterations","5f270cff":"def Searcher(estimator, param_grid, search, train_x, train_y, test_x, test_y,label=None,cv=10):\n    \"\"\"\n    This is a helper function for tuning hyperparameters using the two search methods.\n    Methods must be GridSearchCV or RandomizedSearchCV.\n    Inputs:\n        estimator: Any Classifier\n        param_grid: Range of parameters to search\n        search: Grid search or Randomized search\n        train_x: input variable of your X_train variables \n        train_y: input variable of your y_train variables\n        test_x: input variable of your X_test variables\n        test_y: input variable of your y_test variables\n        label: str to print estimator, default=None\n        cv: cross-validation replicates, int, default=10\n    Output:\n        Returns the estimator instance, clf\n        \n    Modified from: https:\/\/www.kaggle.com\/crawford\/hyperparameter-search-comparison-grid-vs-random#To-standardize-or-not-to-standardize\n    \n    \"\"\"   \n    \n    try:\n        if search == \"grid\":\n            clf = GridSearchCV(\n                estimator=estimator, \n                param_grid=param_grid, \n                scoring=None,\n                n_jobs=-1, \n                cv=cv, #Cross-validation at 10 replicates\n                verbose=0,\n                return_train_score=True\n            )\n        elif search == \"random\":           \n            clf = RandomizedSearchCV(\n                estimator=estimator,\n                param_distributions=param_grid,\n                n_iter=10,\n                n_jobs=-1,\n                cv=cv,\n                verbose=0,\n                random_state=1,\n                return_train_score=True\n            )\n    except:\n        print('Search argument has to be \"grid\" or \"random\"')\n        sys.exit(0) #Exits program if not grid or random\n        \n    # Fit the model\n    print('Start model fitting for', label)\n    clf.fit(X=train_x, y=train_y)\n    \n    #Testing the model\n    \n    try:\n        if search=='grid':\n            cfmatrix=confusion_matrix(\n            y_true=test_y, y_pred=clf.predict(test_x))\n        \n            #Defining prints for accuracy metrics of grid\n            print(\"**Grid search results of\", label,\"**\")\n            print(\"The best parameters are:\",clf.best_params_)\n            print(\"Best training accuracy:\\t\", clf.best_score_)\n            print('Classification Report:')\n            print(classification_report(y_true=test_y, y_pred=clf.predict(test_x))\n             )\n        elif search == 'random':\n            cfmatrix=confusion_matrix(\n            y_true=test_y, y_pred=clf.predict(test_x))\n\n            #Defining prints for accuracy metrics of grid\n          \n            print(\"**Random search results of\", label,\"**\")\n            print(\"The best parameters are:\",clf.best_params_)\n            print(\"Best training accuracy:\\t\", clf.best_score_)\n            print('Classification Report:')\n            print(classification_report(y_true=test_y, y_pred=clf.predict(test_x))\n             )\n    except:\n        print('Search argument has to be \"grid\" or \"random\"')\n        sys.exit(0) #Exits program if not grid or random\n        \n    return clf, cfmatrix; #Returns a trained classifier with best parameters","b7991e34":"def plot_confusion_matrix(cm, label,color=None,title=None):\n    \"\"\"\n    Plot for Confusion Matrix:\n    Inputs:\n        cm: sklearn confusion_matrix function for y_true and y_pred as seen in https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html\n        title: title of confusion matrix as a 'string', default=None\n        label: the unique label that represents classes for prediction can be done as sorted(dataframe['labels'].unique()).\n        color: confusion matrix color, default=None, set as a plt.cm.color, based on matplot lib color gradients\n    \"\"\"\n    \n    classes=sorted(label)\n    plt.imshow(cm, interpolation='nearest', cmap=color)\n    plt.title(title)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    thresh = cm.mean()\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j]), \n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] < thresh else \"black\")","7d0c10db":"from sklearn.svm import SVC #Support Vector Classifier\n\n#Grid Search SVM Parameters\nsvm_param = {\n    \"C\": [.01, .1, 1, 5, 10, 100], #Specific parameters to be tested at all combinations\n    \"gamma\": [0, .01, .1, 1],\n    \"kernel\": [\"rbf\",\"linear\",\"poly\"],\n    \"degree\": [3,4],\n    \"random_state\": [1]}\n\n#Randomized Search SVM Parameters\nsvm_dist = {\n    \"C\": np.arange(0.01,100, 0.01),   #By using np.arange it will select from randomized values\n    \"gamma\": np.arange(0,1, 0.01),\n    \"kernel\": [\"rbf\",\"linear\",\"poly\"],\n    \"degree\": [3,4],\n    \"random_state\": [1]}\n\n\"\"\"\nFollowing the code above, we can set the parameters for both grid search and randomized search. The grid search will evaluate all specified \nparameters while the randomized search will look at the parameters labeled in random order at the best training accuracy. The np.arange function\nallows for a multitude of points to be looked at between the set start and end values of 0.01 to 1. \"\"\"\n\n#Grid Search SVM\nsvm_grid, cfmatrix_grid= Searcher(SVC(), svm_param, \"grid\", X_train_scaled, y_train, X_test_scaled, y_test,label='SVC Grid')\n\nprint('_____'*20)#Spacer\n\n#Random Search SVM\nsvm_rand, cfmatrix_rand= Searcher(SVC(), svm_dist, \"random\", X_train_scaled, y_train, X_test_scaled, y_test,label='SVC Random')\n\n#Plotting the confusion matrices\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title='Random Search Confusion Matrix',label=fetal['fetal_health'].unique(), color=plt.cm.cividis) #grid matrix function\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title='Grid Search Confusion Matrix', label=fetal['fetal_health'].unique(), color=plt.cm.cividis) #randomized matrix function","bbf96e7c":"from sklearn.ensemble import RandomForestClassifier as RFC\n\n#Grid Search RFC Parameters\nrfc_param = {\n    \"n_estimators\": [10, 50, 75, 100, 150,200], #Specific parameters to be tested at all combinations\n    \"criterion\": ['entropy','gini'],\n    \"random_state\": [1],\n    \"max_depth\":np.arange(1,16,1)}\n\n#Randomized Search RFC Parameters\nrfc_dist = {\n    \"n_estimators\": np.arange(10,200, 10),   #By using np.arange it will select from randomized values\n    \"criterion\": ['entropy','gini'],\n    \"random_state\": [1],\n    \"max_depth\":np.arange(1,16,1)}\n\n#Grid Search RFC\nrfc_grid, cfmatrix_grid= Searcher(RFC(), rfc_param, \"grid\", X_train_scaled, y_train, X_test_scaled, y_test,label='RFC Grid')\n\nprint('_____'*20)#Spacer\n\n#Random Search RFC\nrfc_rand, cfmatrix_rand= Searcher(RFC(), rfc_dist, \"random\", X_train_scaled, y_train, X_test_scaled, y_test,label='RFC Random')\n\n#Plotting the confusion matrices\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title='Random Search Confusion Matrix',label=fetal['fetal_health'].unique(), color=plt.cm.cividis) #grid matrix function\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title='Grid Search Confusion Matrix', label=fetal['fetal_health'].unique(), color=plt.cm.cividis) #randomized matrix function","3ca285fd":"from sklearn.neural_network import MLPClassifier as MLP\n\n#Grid Search MLP Parameters\nmlp_param = {\n    \"hidden_layer_sizes\": [(6,),(6,4),(6,4,2)], #Specific parameters to be tested at all combinations\n    \"activation\": ['identity', 'logistic', 'tanh', 'relu'],\n    \"max_iter\":[200,400,600,800,1000],\n    \"solver\":['lbfgs', 'sgd', 'adam'],\n    \"learning_rate_init\":[0.001],\n    \"learning_rate\":['constant','adaptive'],\n    \"random_state\": [1]}\n\n#Randomized Search MLP Parameters\nmlp_dist = {\n    \"hidden_layer_sizes\": [(6,),(6,4),(6,4,2)], #Specific parameters to be tested at all combinations\n    \"activation\": ['identity', 'logistic', 'tanh', 'relu'],\n    \"max_iter\":np.arange(100,1000, 100),\n    \"solver\":['lbfgs', 'sgd', 'adam'],\n    \"learning_rate_init\":[0.001],\n    \"learning_rate\":['constant','adaptive'],\n    \"random_state\": [1]}\n\n\n#Grid Search SVM\nrfc_grid, cfmatrix_grid= Searcher(MLP(), mlp_param, \"grid\", X_train_scaled, y_train, X_test_scaled, y_test,label='MLP Grid')\n\nprint('_____'*20)#Spacer\n\n#Random Search SVM\nrfc_rand, cfmatrix_rand= Searcher(MLP(), mlp_dist, \"random\", X_train_scaled, y_train, X_test_scaled, y_test,label='MLP Random')\n\n#Plotting the confusion matrices\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title='Random Search Confusion Matrix',label=fetal['fetal_health'].unique(), color=plt.cm.cividis) #grid matrix function\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title='Grid Search Confusion Matrix', label=fetal['fetal_health'].unique(), color=plt.cm.cividis) #randomized matrix function","4a2b35e8":"First I will find the most important features using the KBest Alogirthm with f_classif. The correlation coefficients can aid in explaining the importances in the features importance.","b97fd4d1":"## Search Function\n(Read red indents below def function for understanding of code)","71e7be3b":"## Confusion Matrix Function \n(Read red indents below def function for understanding of code)","c49615b5":"## Importing the Data","b7a3ad63":"We will take all the features that scored more than 200 as they show the least redundancy.","3676b00f":"There are no null values and all the data besides fetal_health which is our target are floats. Therefore, we will quickly asses if we have any replicates then move into brief EDA.","8cdbb293":"## Modeling\nWe will start with support vector machines. Using the support vector machines classifiers (SVC) it can handle higher dimensional data and generate hyperplanes for separation and score on a yes (1) no (1) basis. The rulings are decided for where a data point lands within a decision boundary. F-1 score provides us with a method to monitor the precision and recall of our values.","113902b8":"## Multi-Layer Perceptron\nFeed-forward neural network. Very simple compared to using tensor flow or keras,however, may not be as powerful. The number of nodes are determined by (2\/3 * input feature count) + (number of output + 2). The number of layers were decided by 2\/3 of the first and 1\/2 the second layer. We can paramterize plenty of activator functions and set this up with the search function above. Running the searches with this code may take about 30-60min.","49666dfe":"Interestingly, even with imbalanced data all models scored relatively well with random forest performing the best.","28d5f94b":"We were left with 6 features that were selected as the most important. Since we have a reduced feature amount lets plot a quick pairplot to spot some differences. ","40004acb":"Replicates were removed lets do brief EDA.\n# Exploratory Data Analysis\nThere are a ton of variables so lets not get caught up looking at all combinations. Instead lets just make sure our data is relatively balanced. First I will set a plotting function that makes publication ready figures then I will plot a count plot.","b0943fe7":"# Feature Selection","5980c7a0":"## Plotter","f37c68c7":"# Splitting the data and Scaling\nFirst, the data will be split so we can train a scaler model to apply to an unknwon (test) data set. We will save 25% of the data for testing. The data will then be split by standard scaler using the formula $z=\\frac{X_{o}-\\mu}{\\sigma}$. This can help reduce the effect of outliers when modeling later. As per the task, stratify will be used. Spoiler: Highest achieved accuracy without stratify was 95% with RFC.","1cb07a52":"Looks like 1 and 3 can be well distinguished from eachother at least and 2 may be tougher. Lets start the process for machine learning.","5fd6143a":"## Random Forest\nAn ensemble method that estimates several weak decision trees and combines the mean to create an uncorrelated forest at the end. The uncorrelated forest should be able to predict more accurately than an individual tree.","444bc6af":"Clearly, the data is imbalanced and I do not plan on performing an upsample till intial modeling is complete. Instead of plotting a pair plot, we can plot a correlation matrix to observe the pearson correlation coefficients. Remember though that correlation does not imply causation. This will also guide us to predicting what the feature selection (KBest) will decide are the most important features as well later.","3f3cf2d3":"# Importing Relevant General Packages","43476d0b":"# <u>Fetal Health: A Quick Guide to High Accuracy! <\/u>\n## <br> Author: Christopher W. Smith <\/br>\n<br>Unfinished, Updated:10\/14\/2020 <\/br>\n\n<br> <i>linkdin<\/i>: www.linkedin.com\/in\/christopher-w-smith022 <\/br>\n\n<br> The purpose of this notebook is to classify the differences of fetal health based on different test results values. I skipped over most EDA as there is a lot of values and moved directly too feature importance and selected features by ANOVA F-ratio. Then I used, support vector machines (SVM), Random Forest Classier and Multi-Layer Perceptron with hyper parameterization by grid and randomized searchers for the most accurately built models. From here I plan to see if I can reach 100% by applying either linear discriminant analysis or principal component analysis as another pre-processing steps after feature selection. Current highest score is 94% accuracy.<\/br>\n\nKaggle link: https:\/\/www.kaggle.com\/andrewmvd\/fetal-health-classification\n![image.png](attachment:image.png)","a401960b":"Now that the data is split and scaled we can begin modeling.\n# Machine Learning with Classifiers and Grid\/Randomization Search\nFor modeling we will use a few different classifiers.  We can evalute multiple parameters at one using Grid or Randomization Search functions. Grid Search evalutes several input parameters at all combinations input while randomized search looks for the best. Cross-validation is the models self assemessment when trying to find the best parameters on the training data and can be done in \"n\" amount of replicates. We will set up two functions: one for the searches and the other for the confusion matrices.","587ae0b1":"# Conclusion\nWe observed that:\n* Using KBestSelection we were able to use the most important features from the data set\n* The 6 selected features provided 100% accuracy when modeled with either Grid\/Randomized Searches on SVM, RFC or MLP classifiers.\n* RFC impressingly on an imbalanced data set performed at nearly 95% testing accuracy.\n* Maybe using reduction techniques as a preprocessing step may aid in separating the data.\n\n# Next Steps\n* Compare PCA and LDA dimension reduction techniques in attempt to further separate the data.\n* Re-train models on dimensionally reduced data for comparison.\n* Re-apply upsampling if neccessary.\n\n<b>Suggestions?\n\nIf you stuck around to the end please leave a comment for feedback or upvote!<\/b>\n\nLike what I have done? Check out my other notebooks here: https:\/\/www.kaggle.com\/christopherwsmith\n\n# Selected Notebooks That Are Helpful!\n* https:\/\/www.kaggle.com\/christopherwsmith\/classify-that-penguin-100-accuracy\n\n* https:\/\/www.kaggle.com\/christopherwsmith\/tutorial-quick-custom-and-helpful-functions\n\n* https:\/\/www.kaggle.com\/christopherwsmith\/how-to-predict-lung-cancer-levels-100-accuracy"}}