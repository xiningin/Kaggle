{"cell_type":{"3ce890f8":"code","2a03affb":"code","a7c8aa87":"code","c59489e5":"code","c5017302":"code","9b00dc2e":"code","c209d298":"code","babc6f65":"code","3a6526d4":"code","de99e98d":"code","05a7dd5f":"code","4b907cbb":"code","83cab7c3":"code","78a65139":"markdown","9a0330fa":"markdown","b41e9694":"markdown","e97ab365":"markdown","32940c81":"markdown","acd67a98":"markdown","c2e4daf0":"markdown","dfab22bd":"markdown","3033bf3a":"markdown"},"source":{"3ce890f8":"from kaggle_secrets import UserSecretsClient\nproject_id = UserSecretsClient().get_secret(\"project_id\")\nauth_key = UserSecretsClient().get_secret(\"auth_key\")\ngcs_service_account = UserSecretsClient().get_secret(\"gcs_service_account\")\nauth_key_location = UserSecretsClient().get_secret(\"auth_key_location\")\nprint(auth_key,file=open(auth_key_location, \"w\"))","2a03affb":"import os\nos.environ['project_id'] =  project_id\nos.environ['gcs_service_account'] = gcs_service_account\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = auth_key_location","a7c8aa87":"%%bash --err null\ngcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS\ngcloud config set project $project_id","c59489e5":"!gsutil cp gs:\/\/gauntlet-gcp-practice\/fraud_ap\/click_log.csv click_log.csv","c5017302":"!bq show 'bigquery-public-data:ethereum_blockchain.blocks'","9b00dc2e":"import pandas as pd\ndata = pd.read_csv(\"click_log.csv\")","c209d298":"\"\"\"Essential Columns\n<client id>: Advertiser ID\npubclient id: Publisher ID\nclickIp: IP Address\nclmbuser id : unique user id\nimpr id: Unique Key for every served impression\nsite id: Publisher wesite\n<goal id>: Conversion`s goal type identification id\nCity id \/ State id \/ CountryDim id: Geo Details\nbrowser id: browser used for accessing publisher on any device on web.\n<adslot id>: slot id where advertisement is displayed on any site (unqiue for all sites)\ncrtd: timestamp of the action\nitmclmb id: Image\/Creative shown\nispDimId: Internet Service Provider\ndevTypeDimId: Device Id\nosVerDimId: OS Version\"\"\"","babc6f65":"data.columns","3a6526d4":"!pip3 install apache-beam[gcp]","de99e98d":"%%writefile preprocess.py\n\nimport argparse\nimport logging\nimport re\n\nimport apache_beam as beam\nfrom apache_beam.io import ReadFromText\nfrom apache_beam.io import WriteToText\nfrom apache_beam.options.pipeline_options import PipelineOptions\nfrom apache_beam.options.pipeline_options import SetupOptions\nfrom apache_beam.dataframe.io import read_csv\n\n\ndef run(argv=None, save_main_session=True):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n      '--input',\n      dest='input',\n      default='gs:\/\/gauntlet-gcp-practice\/fraud_ap\/click_log.csv',\n      help='Input file to process.')\n    parser.add_argument(\n      '--output',\n      dest='output',\n      required=True,\n      help='Output file to write results to.')\n    known_args, pipeline_args = parser.parse_known_args(argv)\n    pipeline_options = PipelineOptions(pipeline_args)\n    pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n\n    with beam.Pipeline(options=pipeline_options) as p:\n        df = p | 'Read CSV' >> read_csv(known_args.input)\n        df = df.loc[:,[\"pubClientId\",\"clickIp\",\"clmbUserId\",\"imprId\",\"siteId\",\"cityDimId\",\"stateDimId\",\"countryDimId\",\"browserDimId\",\"crtd\",\"itmClmbLId\",\"ispDimId\",\"devTypeDimId\"]]\n        df = df.applymap(str)\n        df.to_csv(known_args.output,index = False)\n\n\nif __name__ == '__main__':\n    logging.getLogger().setLevel(logging.INFO)\n    run()","05a7dd5f":"!gsutil cp preprocess.py gs:\/\/gauntlet-gcp-practice\/fraud_app\/preprocess.py","4b907cbb":"%%bash\n\npython3 -m preprocess \\\n    --output .\/tech_gig_dataframe.csv","83cab7c3":"%%bash\n\npython3 -m preprocess \\\n    --project gauntlet-260920 \\\n    --runner DataflowRunner \\\n    --temp_location gs:\/\/gauntlet-gcp-practice\/fraud_ap\/beam-temp \\\n    --output gs:\/\/gauntlet-gcp-practice\/fraud_ap\/preprocess.csv \\\n    --job_name dataflow-intro-4 \\\n    --region us-central1","78a65139":"# Cloud Data Flow from Kaggle\n\nBig query serves as an excellent tool for preprocessing huge quantities of data. Even though it is very powerful , it is still only capable of tackling problems existing in the SQL world. Hoever we often come across cases when our data is not in SQL format but we want to do some pre processing on it. May be load a csv file , format its structure and then dump it into a big query database. All this can be easily done using cloud dataflow , which is based on apache beam.","9a0330fa":"## Setting up GCP\n\nWe will set up GCP as deonstrated in [this](https:\/\/www.kaggle.com\/ezzzio\/setting-up-gcp) notebook.","b41e9694":"Here we download the data so that we can run our sample jobs in kaggle and once we are satisfied with our pipeline we can run the same code in our runner in dataflow","e97ab365":"![In depth data pre processing pipeline on GCP](https:\/\/cloudx-bricks-prod-bucket.storage.googleapis.com\/d7c57ac02f4941ae23beb551ddc60af79868051e21d64a19c0726099a31c49ac.svg)","32940c81":"![Use of Cloud Data Flow](https:\/\/cloud.google.com\/dataflow\/images\/molecules-1.png)","acd67a98":"The above code will execute our dataflow pipeline as a GCS Dataflow job and the result preprocessed data will be stored in out GCS bucket. Just to try it out we can run the pipeline on kaggle and when we are satisfied with it we can run the whole pipeline on Dataflow.","c2e4daf0":"## Conclusion\n\nThe execution of the job as a dataflow pipeline is brutally fast. We can perform many operations on it. Here we read the data into a dataframe , since it had multiple columns of different datatypes we standardised all of them to string and dumped it into a gcs bucket.\nPrevious this data was not importable into big query but now we can pretty much do it quite effevtively. I have imported the data manually to bigquery here , however we can also do it directly from Beam. Also we have saved the runner file in our GCP bucket to run the pipeline dirctly from the console.\n\nNote:-\nWe have to assign the appropriate roles to our service account to run the code as a GCP Dataflow pipeline. These are not under the scope of the notebooks and may be discussed later. ","dfab22bd":"To demonstrate the capabilities of cloud dataflow , we will work on an experimental csv file, whose format is such thaat it cannot be loaded into bigquery. So we will format the file according to its needs and dup it into bigquery.","3033bf3a":"## Exploring the dataset\n\nThe data is located at a secure location is GCS. We will use the gsutil command to dump the data in our local system and then perform eda for writing our pre processing pipeline."}}