{"cell_type":{"7657c36a":"code","aa82eff5":"code","fd17c951":"code","7fca7746":"code","8bd10a9c":"code","f719aeb5":"code","93ac36d0":"code","1c0a7a18":"code","6d333153":"code","d4695f01":"code","ff1652dd":"code","78f5a2c2":"code","ae645583":"code","4f7ee784":"code","cfafbbc0":"code","4156dd35":"code","bd4d4ecb":"code","ee2b3ac9":"code","b900e9d1":"code","092861a2":"code","eead3370":"code","f6ca2452":"code","a804fb2d":"code","e1c570e1":"code","a38c0cf4":"code","61f0b35b":"code","e23fe199":"code","c53fd6cd":"code","3ff53597":"code","f711125e":"code","e50feb7a":"markdown","5cbc5dc3":"markdown","ed9ead58":"markdown","2d63008c":"markdown","ebac86e2":"markdown","986641f8":"markdown","fd58ddd1":"markdown","6434fcdd":"markdown","026464ef":"markdown","429132a3":"markdown","aae6c420":"markdown","d78cc8c6":"markdown","7f61b8c2":"markdown","436a23b1":"markdown","9531e43a":"markdown","8bae386f":"markdown","64d7614f":"markdown","4de9aab8":"markdown","6ea37dbf":"markdown","240c8241":"markdown","d46aa000":"markdown","8d01681a":"markdown","352f79e1":"markdown","a9d44971":"markdown","b9cdc57e":"markdown","d9b4c49b":"markdown","c9e93907":"markdown","7da56e34":"markdown","5b790929":"markdown","a0b73c29":"markdown","6f3b7889":"markdown","51a9897d":"markdown"},"source":{"7657c36a":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport tensorflow as tf\nimport math\nfrom scipy import special,stats #comb, factorial\nfrom keras import backend as K\nfrom scipy.stats import uniform\nfrom matplotlib import pyplot as plt\nfrom sklearn import tree\nfrom scipy import sparse,stats\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler,LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import classification_report, roc_auc_score, recall_score, make_scorer, plot_confusion_matrix, confusion_matrix, accuracy_score,f1_score\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aa82eff5":"df = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ndf.head()","fd17c951":"print(f'Shape of the dataset: {df.shape}')","7fca7746":"df.info()","8bd10a9c":"df.isnull().sum()","f719aeb5":"ser = df.isnull().sum()\nser = ser[ser > 0]\ncolumns = ser.index.values\ndf[columns]","93ac36d0":"val = 'Unknown'\ntaken = False\nfor col in columns:\n    if val in df[col].unique():\n        taken = True\nif taken:\n    print(\"value `Unknown` is already taken\")\nelse: print('There is no column that contains unique value `Unknown`')","1c0a7a18":"ser = df.isnull().sum()\nser = ser[ser > 0]\ncolumns = ser.index.values\n\nfor col in columns:\n    df[col].fillna(value='Unknown',inplace=True)","6d333153":"df.isnull().sum()","d4695f01":"df.head()","ff1652dd":"df['enrollee_id'].astype(str).describe()","78f5a2c2":"df.drop(['enrollee_id'],axis=1,inplace=True)","ae645583":"df.head()","4f7ee784":"df['target'].value_counts()","cfafbbc0":"df['target'].value_counts(normalize=True)","4156dd35":"cont_feat = ['city_development_index', 'training_hours']\nround(df[cont_feat].describe(),2)","bd4d4ecb":"cont_features = ['city_development_index', 'training_hours']\nWIDTH = 10\nLENGTH = 6\n\nrows = math.ceil(len(cont_features)\/3)\nfig, ax = plt.subplots(1,2,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    ax[i].hist(df[feature],alpha=0.6)\n    ax[i].set_title(f'Distribution of a feature `{feature}`')","ee2b3ac9":"cont_features = ['city_development_index', 'training_hours']\ncat_variable = 'target'\nWIDTH = 10\nLENGTH = 6\n\nrows = math.ceil(len(cont_features)\/3)\nfig, ax = plt.subplots(1,2,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    sns.boxplot(x=cat_variable, y=feature, data=df,ax=ax[i])\n    ax[i].set_title(f'Cond. dist. of feature `{feature}`')","b900e9d1":"plt.figure(figsize=(10,6))\ncont_features = ['city_development_index', 'training_hours']\nsns.scatterplot(data=df, x=cont_features[0], y=cont_features[1], hue='target',alpha=0.6)\nplt.show()","092861a2":"cat_features = ['city', 'gender', 'relevent_experience',\n       'enrolled_university', 'education_level', 'major_discipline',\n       'experience', 'company_size', 'company_type', 'last_new_job',]\n\ncount = np.array([df[feature].unique().size for feature in cat_features])\n\nto_sort = np.argsort(count)[::-1]\ncat_features = np.array(cat_features)[to_sort]\ncount = count[to_sort]\n\nplt.figure(figsize=(11,6))\ngraph = sns.barplot(cat_features,count)\nfor p in graph.patches:\n    graph.annotate(p.get_height(), (p.get_x()+0.4, p.get_height()),\n                   ha='center', va='bottom',\n                   color= 'black')\n\n\nplt.title(\"Number of unique values per each feature\")\nplt.xticks(rotation=45)\nplt.ylabel('Count')\nplt.xlabel('Feature')\nplt.show()","eead3370":"cat_features = ['city', 'gender', 'relevent_experience',\n       'enrolled_university', 'education_level', 'major_discipline',\n       'experience', 'company_size', 'company_type', 'last_new_job',]\n\nplt.figure(figsize=(10,30))\nfor feature in cat_features[1:]:\n    dataframe = df\n    feature_1 = feature # FEATURE\n    feature_2 = 'target' # LABEL\n    to_sort = True # `True` would be useful if label is binary\n\n\n\n    cs = pd.crosstab(dataframe[feature_1],\n                     dataframe[feature_2],\n                     normalize='index')\n    if to_sort == True:\n        cs.sort_values(by=[cs.columns[0]],inplace=True)\n    cs.plot.bar(stacked=True,figsize=(10,6))\n    plt.xlabel(feature)\n    plt.xticks(rotation=45)\n    plt.title(f'Conditional distributions of `{feature_2}`')\nplt.show()","f6ca2452":"cat_features = ['city', 'gender', 'relevent_experience',\n       'enrolled_university', 'education_level', 'major_discipline',\n       'experience', 'company_size', 'company_type', 'last_new_job',]\n\n\ncat_feat_df = df[cat_features].copy()\ncat_feat_df = OneHotEncoder().fit_transform(cat_feat_df)","a804fb2d":"cont_feat = ['city_development_index','training_hours']\ncont_feat_df = df[cont_feat].copy()\ncont_feat_df = sparse.csr_matrix(cont_feat_df.values)","e1c570e1":"X,y = sparse.hstack((cat_feat_df,cont_feat_df)), df['target']","a38c0cf4":"X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=11)\n\n\nsc = StandardScaler()\n\nleft = X_train[:,:-2]\nright = sparse.csr_matrix(sc.fit_transform(X_train[:,-2:].todense()))\nX_train = sparse.hstack((left,right)).tocsr()\n\n\nleft = X_test[:,:-2]\nright = sparse.csr_matrix(sc.transform(X_test[:,-2:].todense()))\nX_test = sparse.hstack((left,right)).tocsr()","61f0b35b":"log_random_state = None\nlog_clf = LogisticRegression(random_state=log_random_state,max_iter=500).fit(X_train, y_train)\nprint(classification_report(y_true=y_test, y_pred=log_clf.predict(X_test)))\nplot_confusion_matrix(log_clf, X_test, y_test)","e23fe199":"knn_clf = KNeighborsClassifier(n_neighbors=25).fit(X_train,y_train)\nprint(classification_report(y_true=y_test, y_pred=knn_clf.predict(X_test)))\nplot_confusion_matrix(knn_clf, X_test, y_test)","c53fd6cd":"rf_clf = RandomForestClassifier(bootstrap=False, \n                                max_depth=20, \n                                n_estimators=700,\n                                random_state=13).fit(X_train, y_train)\n\nprint(classification_report(y_true=y_test, y_pred=rf_clf.predict(X_test)))\nplot_confusion_matrix(rf_clf, X_test, y_test)","3ff53597":"svm_clf = SVC(gamma=0.0870736086175949).fit(X_train,y_train)\nprint(classification_report(y_true=y_test, y_pred=svm_clf.predict(X_test)))\nplot_confusion_matrix(svm_clf, X_test, y_test)","f711125e":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train,label=y_train)\nparam = {'max_depth': 7, \n         'eta': 0.047895, 'objective': \n         'binary:hinge'}\nbst = xgb.train(params=param,dtrain=dtrain, num_boost_round=30)\n\n\ndtest = xgb.DMatrix(X_test)\nprint(classification_report(y_true=y_test,y_pred=bst.predict(dtest)))","e50feb7a":"The first observation suggests that `enrollee_id` will not be of much use.","5cbc5dc3":"For each feature (besides `city`), let's visualize the distribution (conditional on `target`)","ed9ead58":"Indeed, every single value in the column is unique. We will remove the column.","2d63008c":"Couple of observations can be made here: \n1. Gender doesn\u2019t seem to be a good predictor of people who want to switch jobs.\n2. If one didn\u2019t have a previous relevant experience, one is more likely to look for a new job.\n3.  Those who signed up for a full time course are more likely to look for a new job (especially when we compare with the candidates who didn\u2019t sign up for any course)\n4. Those with little or no working experience (i.e., working experience less than 1 year) are the most likely to look for a new job (roughly 50% probability). Furthermore, based on the graph, we see that the experience is (roughly) negatively correlated with the proportion of people who look for a new job, in other words: the more experience you have, the less likely it is that you will be looking for a new job.\n5.  The number of previous jobs is negatively correlated with the probability of looking for a new job (as the last graph suggests): the more jobs you have had previously, the less likely it is that you will be looking for a new job.","ebac86e2":"# Evaluate models\n\nSince we deal with the imbalanced target variable (i.e., there are way more entries with label $0$ than with label $1$), we would expect our models to incorrectly predict a lot of entries with label $1$. Hence the metric that we will be closely looking at is f1 score (where positive label is $1$)","986641f8":"We see that all features with nulls are categorical. For each column, we will replace all `NaN` values with `Unknown`. But first let's verify that neither of the columns with nulls contain unique value `Unknown` (if some column already has value `Unknown`, then it clearly wouldn't make sense to replace `NaN` values with it)","fd58ddd1":"We see that while precision for label $1$ drops by roughly $3\\%$, the recall increases significantly, thus giving us the best f1-score (0.62) out of all models used.","6434fcdd":"# Logistic Regression","026464ef":"Since no columns contains value `Unknown`, we will replace all nulls in the columns with it.","429132a3":"# Feature preprocessing","aae6c420":"Concatenating matrices containing cat. and cont. features.","d78cc8c6":"Preprosessing cat. features","7f61b8c2":"Our dataset contains only two numeric features, namely `city_development_index` and `training_hours`. Let's check the statistics summary and the distribution.","436a23b1":"We see that while `training_hours` doesn't seem to be doing good job at discerning those who will move to a new job, but `city_development_index` does give us some insights: the smaller the `city_development_index` (generally speaking), the more likely it is that he will be looking for a new job.","9531e43a":"We see that we will mostly be dealing with categorical features. Furhermore, it seems that there are quite a lot of nulls. Let's see where the nulls are:","8bae386f":"Let's have a look at the distribution of our label, namely `target`:","64d7614f":"# Random Forest\nWhere the hyperparameters are:\n1. `max_depth` = 20\n2. `n_estimators` = 700\n3. `bootstrap` = False","4de9aab8":"We see that the label is pretty disbalanced.","6ea37dbf":"# KNN (25 neighbors)","240c8241":"Let's have a closer look at the columns with nulls:","d46aa000":"Let's check whether we have nulls now:","8d01681a":"Besides what we have already mentioned (i.e., smaller `city_development_index` implies higher chance of a candidate looking for a new job), there doesn't seem to be any significant pattern.","352f79e1":"In this notebook we will only be working with `aug_train.csv`. The ultimate goal is to try to accurately predict whether particular candidate will be looking for a new job.","a9d44971":"Importing relevant libraries","b9cdc57e":"Now let's check the bivariate conditional distribution","d9b4c49b":"# XGBoost\n\nAfter using grid search (in a separate notebook), the optimal (i.e., those that maximize f1-score (where positive label is $1$)) hyperparameters found are:\n\n1. `max_depth` = 7\n\n2. `eta` = 0.047895\n\n3. `ojbective` = 'binary:hinge'\n","c9e93907":"As expected, we see that the model misclassifies a lot of people who chose to look for a new job (i.e., entries where the value in the `target` is $1$)","7da56e34":"Now let's have a look at the categorical features:","5b790929":"We see that even without any hyperparameter tuning, SVM performs way better than the previous models (mainly signified by the fact that SVM gives us the highest `f1 score` (where $1$ is positive label))\nIn a separate notebook, I have tested different sets of hyperparameters. After trying dozens of combinations, I haven't found any set that would give us better f1 score (positive label: 1) than the default set (by the \"default\" set of hyperparameters, I mean $C=1$, gamma $=$ 'scale' $\\approx  0.087$, and kernel $=$ 'rbf')","a0b73c29":"# SVM: Default hyperparameters","6f3b7889":"Split our dataset into training and test sets.","51a9897d":"No, we don't"}}