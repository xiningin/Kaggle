{"cell_type":{"2487472b":"code","77b0e410":"code","437280e2":"code","e0266c8f":"code","8b9f778e":"code","6ea75ac8":"code","2d6b0bcb":"code","30821852":"code","a975d51a":"code","3852ff15":"code","39ff7b6f":"code","d3214ddf":"code","11829201":"code","b1868bd4":"code","0a31e516":"code","c3d94675":"code","a4f69585":"code","335a679d":"code","819becc9":"code","9c9f67ec":"code","03d66288":"code","d8da9ad6":"code","15281a3d":"code","6cacf336":"code","33edb734":"code","a743096e":"code","016bc677":"code","6c600c89":"code","8e87e7b8":"code","603d0300":"code","2f666b30":"code","90955d13":"code","11ff0850":"code","4c9f7c7a":"code","9ebe074e":"code","dcf54b7c":"code","13704aaf":"code","4d0af570":"code","e357f633":"code","11593fad":"code","330c0630":"code","d7d5ced2":"code","988e0d4b":"code","811993ab":"code","95f6447f":"code","76197a19":"code","dd976b62":"code","fa206178":"code","fc7f549d":"code","71dfc2d3":"code","ee817e3f":"markdown","705feec5":"markdown","c92d790c":"markdown","769e0c8c":"markdown","393bd69e":"markdown","c817e43e":"markdown","56aeeabc":"markdown","db1bb71f":"markdown","fd28d75c":"markdown","7217542b":"markdown","ed9f0567":"markdown","bcd03f55":"markdown","fc0bd6c9":"markdown","6cc9c317":"markdown","a26cb461":"markdown","7aa82cdb":"markdown","821a231f":"markdown","ef1eec2f":"markdown","36c17a27":"markdown","e108f083":"markdown","8d483654":"markdown","4a932f32":"markdown","1a5530bb":"markdown","3670f4bf":"markdown","f8a4f4d2":"markdown","db8335e7":"markdown","2d66769c":"markdown","b2ee899d":"markdown","0f34d7ca":"markdown","8efe61fd":"markdown","362b1a6c":"markdown","fb6d84f0":"markdown","47e00eb6":"markdown","4a0edf61":"markdown","3ee46840":"markdown","efbcef83":"markdown","bc67847b":"markdown","fc8f7393":"markdown","6cf7360f":"markdown","49923c0f":"markdown","3529d8bf":"markdown"},"source":{"2487472b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77b0e410":"train_data = pd.read_csv(\"\/kaggle\/input\/av-guided-hackathon\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/av-guided-hackathon\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/av-guided-hackathon\/sample_submission_cxCGjdN.csv\")\n\n\n%matplotlib inline\nplt.style.use(\"seaborn-dark\")\n\n\nimport warnings\nwarnings.simplefilter('ignore')","437280e2":"train_data.head(3)","e0266c8f":"Id_col, Target_col = 'video_id', 'likes'\nprint(f'\\n Train set contains  {train_data.shape[0]} samples and  {train_data.shape[1]} variables' )\nprint(f'\\n Test set contains  {test_data.shape[0]} samples and  {test_data.shape[1]} variables' )\n\nfeatures = [col for col in train_data.columns if col not in [Id_col, Target_col]]\n\nprint(f'\\n Data set contains  {len(features)} features' )","8b9f778e":"# This is the regression problem sowe can plot PDE to find the distribution of the data.( Target destribution)\n\n_ = train_data[Target_col].plot(kind='density',title='Likes Distribution', fontsize = 14, figsize = (8,5))","6ea75ac8":"_ = pd.Series(np.log1p(train_data[Target_col])).plot(kind='density',title = \"Likes Distribution\", fontsize=14,figsize= (8,6))","2d6b0bcb":"#original target\n\n_  = train_data[Target_col].plot(kind='box',figsize=(8,6),vert=False, fontsize=14,title='Likes boxplot')","30821852":"#log transformed data\n\n_= pd.Series(np.log1p(train_data[Target_col])).plot(kind='box',vert=False, figsize=(8,6), fontsize=14,title='Log likes boxplot')","a975d51a":"#Checking independent variables\n\ntrain_data.info()","3852ff15":"\nNull_values_per_variable = 100*(train_data[features].isnull().sum()\/train_data.shape[0])\nNull_values_per_variable.sort_values(ascending=False) ","39ff7b6f":"train_data.nunique()","d3214ddf":"train_data.columns","11829201":"Category =  ['category_id','country_code']\nnumerical = ['views','dislikes','comment_count']\ndatetime= ['publish_date']\ntext =['title','tags','description']","b1868bd4":"fig, axes = plt.subplots(3,1, figsize=(10,8))\n\nfor i,c in enumerate(numerical):\n   _= train_data[[c]].boxplot(vert=False,ax=axes[i])","0a31e516":"sns.set(font_scale=1.3)\nfig, axes = plt.subplots(2,2, figsize=(10,8))\nnumerical = ['views','dislikes','comment_count']\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i,c in enumerate(numerical):\n    plot = sns.kdeplot(data = train_data[c],ax=axes[i])\nplt.tight_layout()","c3d94675":"for col in numerical+ [Target_col]:\n    train_data[col] = pd.Series(np.log1p(train_data[col]))","a4f69585":"#checking the boxl[lot and PDE again for m=numerical data \nfig, axes = plt.subplots(3,1, figsize=(10,8))\n\nfor i,c in enumerate(numerical):\n   _= train_data[[c]].boxplot(vert=False,ax=axes[i])","335a679d":"#checking PDE for transformed data\nsns.set(font_scale=1.3)\nfig, axes = plt.subplots(2,2, figsize=(10,8))\nnumerical = ['views','dislikes','comment_count']\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i,c in enumerate(numerical):\n    plot = sns.kdeplot(data = train_data[c],ax=axes[i])\nplt.tight_layout()","819becc9":"plt.figure(figsize = (10,8))\n_ = sns.heatmap(train_data[numerical+['likes']].corr(), annot=True)","9c9f67ec":"\n_ = sns.pairplot(train_data[numerical+['likes']],height=5,aspect=24\/16 )","03d66288":"figure, axes = plt.subplots(1,2,figsize=(15,10))\n\n\nfor i,c in enumerate(['category_id','country_code']):\n    _ = train_data[c].value_counts()[::-1].plot(kind='pie',title=c,ax=axes[i],fontsize=18,autopct='%.0f')\n    _ = axes[i].set_ylabel(\"\")\n_ = plt.tight_layout()","d8da9ad6":"#Top 20 channles with highest number of videos\ntop_20_channels = train_data['channel_title'].value_counts()[:20].reset_index()\ntop_20_channels.columns = ['channel_title','num_videos']\nplt.figure(figsize=(15,10))\n_ = sns.barplot(data= top_20_channels,y ='channel_title',x='num_videos')\n_ = plt.title(\"Top 20 channels with highest number of videos\")\n","15281a3d":"countrywise = train_data.groupby(['country_code','channel_title']).size().reset_index()\ncountrywise.columns = ['country_code','channel_title','num_videos']\ncountrywise = countrywise.sort_values(by='num_videos',ascending=False)","6cacf336":"fig,axes= plt.subplots(4,1,figsize=(10,20))\n\nfor i, c in enumerate(train_data['country_code'].unique()):\n    country = countrywise[countrywise['country_code']==c][:10]\n    _ =sns.barplot(data=country,x='num_videos',y='channel_title',ax=axes[i])\n    _ = axes[i].set_title(f\"Country code {c}\")\n\nplt.tight_layout()","33edb734":"\ncountrywise_likes = train_data.groupby(['country_code','channel_title'])['likes'].max().reset_index()\ncountrywise_likes = countrywise_likes.sort_values(by=['likes'],ascending=False)","a743096e":"fig,axes= plt.subplots(4,1,figsize=(10,20))\n\nfor i, c in enumerate(train_data['country_code'].unique()):\n    country = countrywise_likes[countrywise_likes['country_code']==c][:10]\n    _ =sns.barplot(data=country,x='likes',y='channel_title',ax=axes[i])\n    _ = axes[i].set_title(f\"Country code {c}\")\n\nplt.tight_layout()","016bc677":"sns.catplot(data=train_data,x='category_id',y='likes',height =5, aspect=24\/8)","6c600c89":"sns.catplot(data=train_data,x='country_code',y='likes',height =5, aspect=24\/8)","8e87e7b8":"#countrywise distribution of likes;\n\n_=train_data.groupby(['country_code'])['likes'].mean().sort_values().plot(kind='barh')","603d0300":"train_data['publish_date'] = train_data['publish_date'].astype('datetime64')\ntrain_data['publish_date'].max(), train_data['publish_date'].min()\ntest_data['publish_date'] = test_data['publish_date'].astype('datetime64')\n","2f666b30":"train_data['publish_date'].dt.year.value_counts()","90955d13":"latest_train = train_data[train_data['publish_date']>'2017-11']\nlatest_test = test_data[test_data['publish_date']>'2017-11']\n\n_ = latest_train.sort_values(by='publish_date').groupby('publish_date').size().rename('train').plot(figsize=(14,8),title=\"Number of videos \")\n_ = latest_test.sort_values(by='publish_date').groupby('publish_date').size().rename('test').plot(figsize=(14,8),title=\"Number of videos\")\n_ = plt.legend()","11ff0850":"#Number of likes sorted by data\nlatest_train = train_data[train_data['publish_date']>'2017-11']\n_ = latest_train.sort_values(by='publish_date').groupby('publish_date')['likes'].mean().plot(figsize=(14,8),title=\"Number of videos \")\n","4c9f7c7a":"\ncountry = latest_train.groupby(['country_code','publish_date']).size().reset_index()\n_ = country.pivot_table(index='publish_date',columns='country_code',values=0).plot(subplots=True,figsize=(18,18),\n                                                                                 title = 'Number of videos countrywise',\n                                                                                 sharex=False,\n                                                                                 fontsize=18)\n","9ebe074e":"#NO. OF LIKES COUNTRYWISE.\n\ncountry = latest_train.groupby(['country_code','publish_date'])['likes'].mean().reset_index()\n_ = country.pivot_table(index='publish_date',columns='country_code',values='likes').plot(subplots=True,figsize=(18,18),\n                                                                                 title = 'Number of videos countrywise',\n                                                                                 sharex=False,\n                                                                                 fontsize=18)","dcf54b7c":"#Do people post more videos on weekdays or weekends?\ntrain_data['dayofweek'] = train_data['publish_date'].dt.dayofweek","13704aaf":"videos_per_Day_of_week = train_data['dayofweek'].value_counts().sort_index().reset_index()\nvideos_per_Day_of_week.columns = ['dayofweek','num_videos']\nvideos_per_Day_of_week['dayofweek'] = ['Mon','Tue', 'Wed', 'Thru','Fri', 'Sat', 'Sun']\n_ = sns.catplot(data=videos_per_Day_of_week,x='dayofweek',y='num_videos',kind='point',aspect=24\/6)\n_=  plt.title(\"Number of videos posted per days of week\",fontsize=14)","4d0af570":"from wordcloud import WordCloud,STOPWORDS\ntext =['title','tags','description']\nwc = WordCloud(stopwords = set(list(STOPWORDS)+['|']),random_state=42)\ntrain_data['likes'].describe()","e357f633":"100* ((train_data['likes']>10).sum())\/train_data.shape[0]","11593fad":"def plot_countrywise(country_code='IN'):\n    country = train_data[train_data['country_code']==country_code]\n    country =country[country['likes']>10]\n    fig, axes = plt.subplots(2,2, figsize = (20,12))\n    axes = [ax for row_axes in axes for ax in row_axes]\n    \n    for i,c in enumerate(text):\n        op =  wc.generate(str(country[c]))\n        _ = axes[i].imshow(op)\n        _ = axes[i].set_title(c.upper(),fontsize=20)\n        _ = axes[i].axis('off')\n        \n    fig.delaxes(axes[3])\n    plt.suptitle(f\"Country code : '{country_code}'\",fontsize=30)\n    ","330c0630":"plot_countrywise('IN')","d7d5ced2":"plot_countrywise('US')","988e0d4b":"plot_countrywise('GB')","811993ab":"plot_countrywise('CA')","95f6447f":"train_data['description_len'] = train_data['description'].apply(lambda x:len(x))\ntrain_data['title_len'] = train_data['title'].apply(lambda x:len(x))\ntrain_data['tags_len'] = train_data['tags'].apply(lambda x:len(x))\ntrain_data['channel_len'] = train_data['channel_title'].apply(lambda x:len(x))\n\n_ = sns.heatmap(train_data[['description_len','title_len','channel_len','tags_len','likes']].corr(),annot=True)","76197a19":"#data load\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom catboost import CatBoostRegressor\nimport pickle\nfrom sklearn.model_selection import train_test_split\n\n\ntrain = pd.read_csv(\"\/kaggle\/input\/av-guided-hackathon\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/av-guided-hackathon\/test.csv\")\n\ncat_cols =  ['category_id','country_code','channel_title']\nnumerical = ['views','dislikes','comment_count']\ndatetime= ['publish_date']\ntext =['title','tags','description']\nmain_features = list(test.columns)","dd976b62":"#metric selection\n\n\ndef rmlse(pred,orig):\n    return np.sqrt(mean_squared_log_error(orig,pred))\n\ndef av_metric(pred,orig):\n    return 1000*np.sqrt(mean_squared_error(orig,pred))\n    \n    \ndef download_data(pred_test,filename):\n    pred_test  = pd.Series(np.expm1(pred_test))\n    pred_test = pred_test.round()\n    improved = pd.concat([test['video_id'],pred_test],axis=1,).reset_index(drop=True)\n    improved.set_index('video_id',inplace=True)\n    improved.rename(columns={0:'likes'},inplace=True)\n    improved.to_csv(filename)\n    \n\ndef join_df(train,test):\n    df = pd.concat([train,test],axis=0).reset_index(drop=True)\n    df[cat_cols] = df[cat_cols].apply(lambda x: pd.factorize(x)[0])    \n    features  = [c for c in df.columns if c not in ['likes','video_id']]\n    df[numerical+['likes']] = df[numerical+['likes']].apply(lambda x: np.log1p(x))\n    \n    return df,features\n\ndef get_split_features(df,train_nrows):\n    \n    train_proc,test_proc = df[:train_nrows],df[train_nrows:].reset_index(drop=True)\n    cols = [i for i in train_proc.columns if i not in ['likes','video_id']]\n    return train_proc,test_proc, cols\n\n\n\n\ndef feature_engineering(df):\n    ## datetime columns\n\n    df['publish_date'] = pd.to_datetime(df['publish_date'],format =\"%Y-%m-%d\")\n\n    df['publish_date_days_since_start'] = (df['publish_date']-df['publish_date'].min()).dt.days\n    df['publish_date_days_of_weeks'] = df['publish_date'].dt.dayofweek\n    df['publish_date_year'] = df['publish_date'].dt.year\n    df['publish_date_month'] = df['publish_date'].dt.month\n    \n    #channel title\n    df['channel_title_num_videos']= df['channel_title'].map(df['channel_title'].value_counts())\n    df['publish_date_num_videos'] = df['publish_date'].map(df['publish_date'].value_counts())\n    #creating more colum\n    df['Channel_in_n_countries']=df.groupby(['channel_title'])['country_code'].transform('nunique')\n    \n    # Grouping features\n\n    df['mean_views_in_channel'] = df.groupby(['channel_title'])['views'].transform('mean')\n    df['max_views_in_channel'] = df.groupby(['channel_title'])['views'].transform('max')\n    df['min_views_in_channel'] = df.groupby(['channel_title'])['views'].transform('min')\n\n\n    df['mean_comments_in_channel'] =df.groupby(['channel_title'])['comment_count'].transform('mean')\n    df['max_comments_in_channel'] = df.groupby(['channel_title'])['comment_count'].transform('max')\n    df['min_comments_in_channel'] = df.groupby(['channel_title'])['comment_count'].transform('min')\n\n\n    df['mean_dislikes_in_channel'] =df.groupby(['channel_title'])['dislikes'].transform('mean')\n    df['max_dislikes_in_channel'] = df.groupby(['channel_title'])['dislikes'].transform('max')\n    df['min_dislikes_in_channel'] = df.groupby(['channel_title'])['dislikes'].transform('min')\n    \n    # Length of text columns\n\n    df['len_of_title_columns'] = df['title'].apply(lambda x:len(x))\n    df['len_of_tags_columns'] = df['tags'].apply(lambda x:len(x))\n    df['len_of_description_columns'] =df['description'].apply(lambda x:len(x))\n    \n    #text\n    TOP_N_WORDS=50\n\n    #for description\n    vect = CountVectorizer(max_features=TOP_N_WORDS)\n    txt_to_fts_desc = vect.fit_transform(df['description']).toarray()\n\n    c = 'description'\n    txt_fts_names = [c + f\"_word_{i}_count\" for i in range(TOP_N_WORDS)]\n    descrip = pd.DataFrame(txt_to_fts_desc,columns=txt_fts_names)\n\n    #for tags\n    vect1 = CountVectorizer(max_features=TOP_N_WORDS)\n    txt_to_fts_tags = vect1.fit_transform(df['tags']).toarray()\n\n    c = 'tags'\n    txt_fts_names = [c + f\"_word_{i}_count\" for i in range(TOP_N_WORDS)]\n    tags = pd.DataFrame(txt_to_fts_tags,columns=txt_fts_names)\n\n    #for titles\n    vect2 = CountVectorizer(max_features=TOP_N_WORDS)\n    txt_to_fts_titles = vect2.fit_transform(df['title']).toarray()\n\n    c = 'title'\n    txt_fts_names = [c + f\"_word_{i}_count\" for i in range(TOP_N_WORDS)]\n    title = pd.DataFrame(txt_to_fts_titles,columns=txt_fts_names)\n    \n    #merge\n    \n\n    df = pd.concat([df,title,tags,descrip],axis=1).reset_index(drop=True)\n     \n    return df\n\n\ndef feature_selection(df):\n    features = [c for c in df.columns if c not in ['likes','video_id']]\n    cat_num_columns = [c for c in features if c not in ['title','tags','description','publish_date']]\n\n    train_proc, test_proc, features = get_split_features(df,train.shape[0])\n    \n\n    model = ExtraTreesRegressor()\n    model.fit(train_proc[cat_num_columns],train['likes'])\n    imp = pd.Series(model.feature_importances_,index=cat_num_columns)\n    imp_features  =imp.nlargest(int(0.80*len(cat_num_columns))).index.tolist()\n    \n    return train_proc,test_proc,imp_features","fa206178":"#function for gradient boosting:\ndef run_gradient_boosting(clf, fit_params, train, test, features):\n\n    N_splits = 5\n    oofs = np.zeros(len(train))\n    preds = np.zeros((len(test)))\n    \n    target = train['likes']\n    fold = StratifiedKFold(n_splits= N_splits)\n    stratified_target = pd.qcut(train['likes'],10,labels=False,duplicates='drop')\n    \n    feature_importance= pd.DataFrame()\n\n    for fold_, (trn_idx, val_idx) in enumerate(fold.split(train, stratified_target)):\n        print(f'\\n------------- Fold {fold_ + 1} -------------')\n\n        ### Training Set\n        X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n        ### Validation Set\n        X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n        ### Test Set\n        X_test = test[features]\n\n        scaler = StandardScaler()\n        _ = scaler.fit(X_trn)\n\n        X_trn = scaler.transform(X_trn)\n        X_val = scaler.transform(X_val)\n        X_test = scaler.transform(X_test)\n\n        _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n        \n        fold_importance = pd.DataFrame({'fold':fold_+1,'features':features,'importance':clf.feature_importances_})\n        feature_importance = pd.concat([feature_importance,fold_importance],axis=0)\n        ### Instead of directly predicting the classes we will obtain the probability of positive class.\n        preds_val = clf.predict(X_val)\n        preds_test = clf.predict(X_test)\n\n        fold_score = av_metric(preds_val, y_val)\n        print(f'\\nAV score for validation set is {fold_score}')\n\n        oofs[val_idx] = preds_val\n        preds += preds_test \/ N_splits\n\n\n    oofs_score = av_metric(oofs,target)\n    print(f'\\n\\n AV score for oofs is {oofs_score}')\n    \n    \n    feature_importance = feature_importance.reset_index(drop=True)\n    fi = feature_importance.groupby('features')['importance'].mean().sort_values(ascending=False)[:20][::-1]\n    #fi.plot(kind='barh',figsize=(12,6))\n    \n    return oofs, preds,fi,clf\n\n\n#preprocessing\ndef preprocessing(train,test):\n    \n    df,features = join_df(train,test)\n    new_df = feature_engineering(df)\n    train_proc,test_proc,imp_features = feature_selection(new_df)\n    return train_proc,test_proc,imp_features","fc7f549d":"  \n#catboost final  model\n\ndef model(train,test):\n    \n    train_proc,test_proc,imp_features = preprocessing(train,test)\n    \n    clf = CatBoostRegressor(learning_rate=0.01,iterations=3000,random_state=2054,task_type='GPU')\n    params = {'verbose':False,'early_stopping_rounds':200}\n    cat_oofs, cat_preds, fi,model = run_gradient_boosting(clf, params, train_proc, test_proc, imp_features)\n    \n    download_data(cat_preds,\"final_catboost_fe_fs.csv\")\n    \n    return model\n\n","71dfc2d3":"YLP_model = model(train,test)\n","ee817e3f":"* ### Bivariate analysis\n\nNow we need to check the above analysis countrywise which can give us better idea about the approach.","705feec5":"The distribution is similar in both train and test data is similar.\n#### Number of likes sorted by date\n","c92d790c":"#### Observation:\n1. Views are highly positively correlated with dislikes and likes.\n2. Also the comment count is positively correlated with the likes.\n3. There is no negative correlation it means dislikes not affect the likes inversely.\n\n","769e0c8c":"#### likes distribution per country","393bd69e":"#### Observation:\n1. We observer that in category id more that 50% of data belongs to category 24 and 25.\n2. Also in Cpuntry code around 80% of the videos are uploaded from india and canada only.Rest are from other countries.\n3. In channel title feature there are around 5700 unique values and that was inaprropriate to plot as a pie chart.\n4. But we can find top 10 channels or top 20 channels name from the channel title feature in terms of videos.\n\n#### Horizontal bar plot","c817e43e":"## Unique values in each variables\n","56aeeabc":"#### Density plots","db1bb71f":"#### Log transforming the numerical columns along with target variable","fd28d75c":"Some videos can be repeated too as many peoples can upload same videos. Thats why there is a little difference between unique values in video_id and title. Other variables have a high number of categories.","7217542b":"### Bivariate analysis","ed9f0567":"# Hypothesis question\n1. Do videos with more views get more likes?\n3. Do videos with more comments get more likes?\n4. Do country of origin affects likes in videos?\n5. Do people post more videos on weekends then weekdays?\n6. Does channel affects likes on videos?\n7. Do videos with description  get more likes?\n8. Does legthy videos have more likes?\n9. Does videos with more tags get more likes?","bcd03f55":"From this we can observe that our data is highly right skewed hence it is confirm that we need to do log transform in these three columns.","fc0bd6c9":"### Question: Do descriptive videos get more number of likes?","6cc9c317":"### Univariate analysis","a26cb461":"#### likes distribution per category\n\n","7aa82cdb":"#### Pie charts","821a231f":"We can see that all features have high number of outliers so we can make log transform to transform them into normal distribution.","ef1eec2f":"Now due to the distribution one can see that the target variables have lots of outliers and for this we can visualize it using box plot","36c17a27":"> **We observed that our target distribution is highly right skewed so we can do log transformation to normalize this target data.**","e108f083":"## Numerical data\nwe have to check boxplots, PDE and correlation for these features","8d483654":"Q. Do more videos uploadedd on weekends than weekdays?\n- Ans: No, our hypothesis was wrong. As from the graph more number of videos are uploaded on friday then compare to weekdays.","4a932f32":"## Handling datetime variables\n'datetime= ['publish_date']\n### univariate analysis","1a5530bb":"### Univariate analysis\n#### 1. Boxplot","3670f4bf":"### Multivariate analysis\n\n1.Number of videos by countrywise.","f8a4f4d2":"## Categorical data:\nCategory =  ['category_id','country_code','channel_title']","db8335e7":"## Text Data\ntext =['title','tags','description']\n> To analyze the textual data we used word clouds which gives us more insights of highl occuring words.\n","2d66769c":"1. Video_id can be ignores as it is unique.\n2. Title and channel is basically strings.\n3. category_id is category type data determines the category of a particular video.\n4. Publish date should be datetime object.\n5. Tags,and desription are both string type data.\n6. views, dislikes and comment_count are numeric continous data hence float.\n7. Country code is again categorical data and unique for each country.\n8. Likes is basically our target variable.","b2ee899d":"#### Answering hypothesis:\n\n\n1. Do videos with more views get more likes?\n> Ans-  Yes, views are positively correlated with likes and this hypothesis is true.\n2. Do videos with more comments get more likes?\n> Ans- Yes, also both are positively correlated, this hypothesis is also true.\n\n**For other hypothesis we need to explore the data more. SO let's do that.**\n","0f34d7ca":"# Load Data","8efe61fd":"### Multivariate analysis\nAnalysis more than 2 variable together.\n1. countrywise likes for each channel?\n","362b1a6c":"## Percentage of null values.\n","fb6d84f0":"# Model Building","47e00eb6":"## Analyzing each variable and their relationships","4a0edf61":"#### Correlation heatmaps","3ee46840":"# EDA\n\n1. Brief view on data set and submission file.\n2. Details about shape of the data.\n3. Details about distribution of the target variable.\n4. Details about the independent variables.\n5. Missing values in a dataset.\n6. Unique values per columns in dataset.\n\n","efbcef83":"Now the numerical variables are normalised and have normal distribution as well.\n","bc67847b":"> This is because the indian videos are also popular offshore.\n1. Does country affect the number of likes?\n2. Ans - Yes it affect the number of likes.","fc8f7393":"#### Observation:\n1. In this we can observe that some channels from one country are shown in other country as well which means people are viewing there videos there as well.\n","6cf7360f":"There are 4 types of features we have in this dataset.\n1. Categorical\n2. Numerical\n3. Text\n4. Datetime\n\nFor each feature we perform 2 analysis:\n1. univariate \n2. Bivariate","49923c0f":"1. If title lenght is short then chances of more numbert of likes in videos are large.\n2. Likes are less positively correlated with the description lenght.\n3. More tags fet more likes.\n","3529d8bf":"#### Pair plots"}}