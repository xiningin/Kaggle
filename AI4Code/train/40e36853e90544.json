{"cell_type":{"88eb58c9":"code","4693bbab":"code","8cbe76de":"code","866ee0f4":"code","9eb4d5ce":"code","ce40ad0a":"code","8fbc6056":"code","89c8c9bd":"code","f1e167dc":"code","dde66547":"code","33f9c1b5":"code","2d96c0e5":"code","bec0158e":"code","450219c5":"code","8ecdd483":"code","e3b74c83":"code","908b3bbf":"code","09b9f8b7":"code","bb073ad4":"code","8bb0ab55":"code","fcea7283":"code","db348746":"code","5ad85492":"code","f7a6dfd7":"code","fff3c664":"code","e9a150fc":"code","0bfd4de1":"code","35d19597":"code","29a3eb64":"code","937827c3":"code","0257ff19":"code","7249e58b":"code","0c2d76cc":"code","72eac4f5":"code","51679f7e":"code","2c59e0b6":"code","5e649e12":"code","c21233a8":"code","0832af0e":"code","3891953b":"code","b0a105c0":"code","f7c2ab38":"code","80433d40":"code","972dd7a6":"code","30ea5249":"code","f32d4013":"code","cfe64391":"code","592ab1b8":"code","a3e4fc0f":"code","e317d818":"code","dd98b998":"code","85796da6":"code","21f98fc3":"code","bd7e0cba":"code","348709bb":"code","5adca883":"code","c2b75c96":"code","79de3c82":"code","c41f47ec":"code","45acb599":"code","e56732d2":"code","862c878a":"code","6359072a":"code","5aee72cb":"code","d75dcc13":"code","8cfd912d":"code","b942305b":"code","60e67a23":"code","b4ea48ea":"markdown","7b3c0ed3":"markdown","26ad6e04":"markdown"},"source":{"88eb58c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4693bbab":"# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nimport os       ","8cbe76de":"from sklearn.metrics import r2_score","866ee0f4":"# reading the dataset\nhousing = pd.read_csv(\"..\/input\/train.csv\")","9eb4d5ce":"housing.head()","ce40ad0a":"housing.shape","8fbc6056":"housing.columns","89c8c9bd":"# summary of the dataset: 1460 rows, 81 columns, no null values\nprint(housing.info())","f1e167dc":"housing.describe()","dde66547":"# Checking the percentage of missing values\nround(100*(housing.isnull().sum()\/len(housing.index)), 2)","33f9c1b5":"# Write your code for column-wise null count here\nhousing.isnull().sum()","2d96c0e5":"#Replacing NA values per data dictionary\nhousing.PoolQC.replace(np.NaN, 'No Pool', inplace=True)\nhousing.Alley.replace(np.NaN, 'No alley access', inplace=True)\nhousing.BsmtQual.replace(np.NaN, 'No Basement', inplace=True)\nhousing.BsmtCond.replace(np.NaN, 'No Basement', inplace=True)\nhousing.BsmtExposure.replace(np.NaN, 'No Basement', inplace=True)\nhousing.BsmtFinType1.replace(np.NaN, 'No Basement', inplace=True)\nhousing.BsmtFinType2.replace(np.NaN, 'No Basement', inplace=True)\nhousing.FireplaceQu.replace(np.NaN, 'No Fireplace', inplace=True)\nhousing.GarageType.replace(np.NaN, 'No Garage', inplace=True)\nhousing.GarageFinish.replace(np.NaN, 'No Garage', inplace=True)\nhousing.GarageFinish.replace(np.NaN, 'No Garage', inplace=True)\nhousing.GarageQual.replace(np.NaN, 'No Garage', inplace=True)\nhousing.GarageCond.replace(np.NaN, 'No Garage', inplace=True)\nhousing.Fence.replace(np.NaN, 'No Fence', inplace=True)\nhousing.MiscFeature.replace(np.NaN, 'None', inplace=True)\nhousing.MasVnrType.replace(np.NaN, 'None', inplace=True)","bec0158e":"# Checking the percentage of missing values\nround(100*(housing.isnull().sum()\/len(housing.index)), 2)","450219c5":"#checking if any duplicate values in the df\nprint(any(housing.duplicated()))  ","8ecdd483":"housing.LotFrontage.describe(percentiles=[.25,.5,.75,.90,.95,.99])","e3b74c83":"housing.LotFrontage.replace(np.NaN, 70.049958, inplace=True)","908b3bbf":"housing = housing.drop('GarageYrBlt', axis=1)","09b9f8b7":"# Checking the percentage of missing values\nround(100*(housing.isnull().sum()\/len(housing.index) ), 2)","bb073ad4":"# Write your code for dropping the rows here\nhousing[housing.columns].isnull().sum().value_counts()","8bb0ab55":"housing.columns[housing.isna().any()].tolist()","fcea7283":"# Write your code for dropping the rows here\nhousing[housing.columns].isnull().sum().value_counts()\n","db348746":"housing = housing.dropna(axis=0, subset=['MasVnrArea'])","5ad85492":"housing = housing.dropna(axis=0, subset=['Electrical'])","f7a6dfd7":"housing.info()","fff3c664":"# all numeric (float and int) variables in the dataset\nhousing_numeric = housing.select_dtypes(include=['float64', 'int64'])\nhousing_numeric.head()","e9a150fc":"housing_numeric = housing_numeric.drop(['MSSubClass'], axis=1)","0bfd4de1":"# correlation matrix\ncor = housing_numeric.corr()\ncor","35d19597":"# plotting correlations on a heatmap\n\n# figure size\nplt.figure(figsize=(25,18))\n\n# heatmap\nsns.heatmap(cor, cmap=\"YlGnBu\", annot=True)\nplt.show()\n","29a3eb64":"#histogram\nsns.distplot(housing['SalePrice']);","937827c3":"#scatter plot grlivarea\/saleprice\ndata = pd.concat([housing['SalePrice'], housing['GrLivArea']], axis=1)\ndata.plot.scatter(x=\"GrLivArea\", y='SalePrice', ylim=(0,800000));","0257ff19":"#scatter plot totalbsmtsf\/saleprice\ndata = pd.concat([housing['SalePrice'], housing['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x=\"TotalBsmtSF\", y='SalePrice', ylim=(0,800000));","7249e58b":"#box plot overallqual\/saleprice\ndata = pd.concat([housing['SalePrice'], housing['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","0c2d76cc":"data = pd.concat([housing['SalePrice'], housing['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=\"YearBuilt\", y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","72eac4f5":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = housing.corr().nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(housing[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","51679f7e":"# converting MSSubClass to categorical\nhousing['MSSubClass'] = housing['MSSubClass'].astype('object')\nhousing.info()\n\n","2c59e0b6":"# creating dummy variables for categorical variables\n# subset all categorical variables\nhousing_categorical = housing.select_dtypes(include=['object'])\nhousing_categorical.head()\n","5e649e12":"# convert into dummies\nhousing_dummies = pd.get_dummies(housing_categorical, drop_first=True)\nhousing_dummies.head()","c21233a8":" # drop categorical variables \nhousing_final = housing.drop(list(housing_categorical.columns), axis=1)","0832af0e":"# concat dummy variables with X\nhousing_final = pd.concat([housing_final, housing_dummies], axis=1)","3891953b":"housing_final.shape","b0a105c0":"housing_final","f7c2ab38":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n#housing_final = pd.DataFrame(scaler.fit_transform(housing_final), columns=housing_final.columns)\n#housing_final","80433d40":"from sklearn.model_selection import train_test_split\n\n# Putting feature variable to X\nX = housing_final.drop(['SalePrice','Id'], axis=1)\n\nX.head()","972dd7a6":"# scaling the features\nfrom sklearn.preprocessing import scale\n\n# storing column names in cols, since column names are (annoyingly) lost after \n# scaling (the df is converted to a numpy array)\ncols = X.columns\nX = pd.DataFrame(scale(X))\nX.columns = cols\nX.columns","30ea5249":"# Putting response variable to y\ny = housing_final['SalePrice']\ny.head()","f32d4013":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=200)","cfe64391":"# linear regression\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\n# predict\ny_train_pred = lm.predict(X_train)\nr2_score(y_true=y_train, y_pred=y_train_pred)","592ab1b8":"y_test_pred = lm.predict(X_test)\n\nr2_score(y_true=y_test, y_pred=y_test_pred)","a3e4fc0f":"# model coefficients\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))","e317d818":"# lasso regression\nlm = Lasso(alpha=0.001)\nlm.fit(X_train, y_train)\n\n# predict\ny_train_pred = lm.predict(X_train)\nprint(r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lm.predict(X_test)\nprint(r2_score(y_true=y_test, y_pred=y_test_pred))","dd98b998":"# lasso model parameters\nmodel_parameters = list(lm.coef_)\nmodel_parameters.insert(0, lm.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))\n# grid search CV\n\n# set up cross validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# specify range of hyperparameters\nparams = {'alpha': [0.001, 0.01, 1.0, 5.0, 10.0]}\n\n# grid search\n# lasso model\nmodel = Lasso()\nmodel_cv = GridSearchCV(estimator = model, param_grid = params, \n                        scoring= 'r2', \n                        cv = folds, \n                        return_train_score=True, verbose = 1)            \nmodel_cv.fit(X_train, y_train) ","85796da6":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","21f98fc3":"# plot\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('r2 score')\nplt.xscale('log')\nplt.show()","bd7e0cba":"# model with optimal alpha\n# lasso regression\nlm = Lasso(alpha=500)\nlm.fit(X_train, y_train)\n\n# predict\ny_train_pred = lm.predict(X_train)\nprint(r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lm.predict(X_test)\nprint(r2_score(y_true=y_test, y_pred=y_test_pred))","348709bb":"# lasso model parameters\nmodel_parameters = list(lm.coef_)\nmodel_parameters.insert(0, lm.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))","5adca883":"# set up cross validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 4)","c2b75c96":"# specify range of hyperparameters\nparams = {'alpha': [0.001, 0.01, 1.0, 5.0, 10.0,50.0,100.0,200.0,300.0,350.0,400.0,450.0,500.0,550.0,600.0,650.0,700.0,750.0,800.0,850.0]}\n# grid search\n# lasso model\nmodel = Lasso()\nmodel_cv = GridSearchCV(estimator = model, param_grid = params, \n                        scoring= 'r2', \n                        cv = folds, \n                        return_train_score=True, verbose = 1)            \nmodel_cv.fit(X_train, y_train) ","79de3c82":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","c41f47ec":"# plot\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('r2 score')\nplt.xscale('log')\nplt.show()","45acb599":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\nfolds = 5\n\nlasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","e56732d2":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results = cv_results[cv_results['param_alpha']<=200]\ncv_results.head()","862c878a":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","6359072a":"alpha =100\n\nlasso = Lasso(alpha=alpha)\n        \nlasso.fit(X_train, y_train) ","5aee72cb":"lasso.coef_","d75dcc13":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.5, 1.0, 2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0 ,10.0, 11.0,12.0,13.0,14.0, 15.0,16.0,17.0,18.0,19.0, 20.0,25.0, 30.0, 35.0, 40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0 ]}\n\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train) ","8cfd912d":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results = cv_results[cv_results['param_alpha']<=100]\ncv_results.head()","b942305b":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","60e67a23":"# model with optimal alpha\n# Ridge regression\nlm = Ridge(alpha=90.0)\nlm.fit(X_train, y_train)\n\n# predict\ny_train_pred = lm.predict(X_train)\nprint(r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lm.predict(X_test)\nprint(r2_score(y_true=y_test, y_pred=y_test_pred))","b4ea48ea":"**3  Scaling and Split the dataset**","7b3c0ed3":"**5 Ridge**","26ad6e04":"**2  Data Cleaning**"}}