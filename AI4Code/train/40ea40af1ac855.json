{"cell_type":{"b584f8fe":"code","fcc4a2ae":"code","636188bb":"code","71036f4f":"code","475a1b73":"code","2f213597":"code","4053c40e":"code","b8588381":"code","3f95e336":"code","1b897be0":"code","bf116822":"code","b6f8bd55":"code","4bdf381c":"code","3a732e6b":"code","25dd135a":"code","7f7e4b43":"code","b8e6f29e":"code","555c3a15":"code","3912e9dd":"code","9d382b55":"code","eefe640d":"code","8f488235":"markdown","71b8bf36":"markdown","74ddae63":"markdown","114ebed6":"markdown","416f9393":"markdown","40b3418c":"markdown","a23ea777":"markdown","7c34c9ad":"markdown","f8eee89f":"markdown","ddcb8441":"markdown","8c74c132":"markdown","604198d7":"markdown","42735153":"markdown","3b6d90d6":"markdown","7aa458c8":"markdown","7406eec2":"markdown","b1c2cd3f":"markdown","4b7ada82":"markdown"},"source":{"b584f8fe":"import keras\nfrom keras.layers import Input\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.preprocessing.image import load_img\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.models import Sequential\nimport cv2\nfrom keras.layers import Dropout, Flatten,Activation\nfrom keras.layers import Conv2D, MaxPooling2D,Dense\nfrom pathlib import Path\nfrom keras.preprocessing import image","fcc4a2ae":"import numpy as np\nmy_seed = 0\nnp.random.seed(my_seed)\nimport random \nrandom.seed(my_seed)\nimport tensorflow as tf\ntf.random.set_seed(my_seed)","636188bb":"data_dir_train='..\/input\/flowers-recognition\/flowers\/flowers'\nDaisy_flower_dir = '..\/input\/flowers-recognition\/flowers\/flowers\/daisy\/'\nSunflower_flower_dir = '..\/input\/flowers-recognition\/flowers\/flowers\/sunflower\/'\nTulip_flower_dir = '..\/input\/flowers-recognition\/flowers\/flowers\/tulip\/'\nDandelion_flower_dir = '..\/input\/flowers-recognition\/flowers\/flowers\/dandelion\/'\nRose_flower_dir = '..\/input\/flowers-recognition\/flowers\/flowers\/rose\/'","71036f4f":"batch_size = 32\nimg_height = 180\nimg_width = 180","475a1b73":"#train_ds = ##todo\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir_train,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","2f213597":"val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir_train,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","4053c40e":"class_names = train_ds.class_names\nprint(class_names)","b8588381":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nn=0\nlabels_taken=[]\nfor images, labels in train_ds.take(2):\n  print(labels)\n  for i in range(32):\n    if (labels[i] not in labels_taken):\n      ax = plt.subplot(3,3,n+1)\n      plt.imshow(images[i].numpy().astype(\"uint8\"))\n      plt.title(class_names[labels[i]])\n      plt.axis(\"off\")\n      n+=1\n      labels_taken.append(labels[i])","3f95e336":"AUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","1b897be0":"num_classes = 5\n\nmodel = Sequential()\nmodel.add(Conv2D( filters=16, kernel_size=3, activation='relu',input_shape=(180, 180, 3)))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Conv2D(filters=64, kernel_size=3, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Flatten())\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dense(num_classes,activation='softmax'))","bf116822":"model.compile(optimizer='adam',\n              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","b6f8bd55":"# # View the summary of all layers\nmodel.build((32,180,180,3))\nmodel.summary()","4bdf381c":"epochs = 20\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","3a732e6b":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","25dd135a":"train_datagen = ImageDataGenerator(\n        rescale=1 \/ 255.0,\n        rotation_range=20,\n        zoom_range=0.05,\n        width_shift_range=0.05,\n        height_shift_range=0.05,\n        shear_range=0.05,\n        horizontal_flip=True,\n        fill_mode=\"nearest\",\n        validation_split=0.20)\n\n#val_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntraining_set = train_datagen.flow_from_directory(data_dir_train,\n                                                 target_size = (img_height, img_width),\n                                                 batch_size = batch_size,\n                                                 class_mode = 'categorical',subset='training')\n\nval_set = train_datagen.flow_from_directory(data_dir_train,\n                                          target_size = (img_height, img_width),\n                                          batch_size = batch_size,\n                                          class_mode = 'categorical',subset='validation')","7f7e4b43":"num_classes = 5\n\nmodel = Sequential()\nmodel.add(Conv2D( filters=16, kernel_size=3, activation='relu',input_shape=(180, 180, 3)))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Dropout(0.20))\nmodel.add(Conv2D(filters=64, kernel_size=3, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(units=256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes,activation='softmax'))","b8e6f29e":"# Compling the model\nmodel.compile(optimizer=Adam(lr=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","555c3a15":"# # View the summary of all layers\nmodel.summary()","3912e9dd":"## Traing the model using call back\nepochs = 50\nmy_callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\nReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=0.00003, patience=1, cooldown=2, verbose=1)]\nhistory = model.fit(\n  training_set,\n  validation_data=val_set,\n  epochs=epochs,callbacks=my_callbacks)","9d382b55":"# Plotting train loss and validation loss\nplt.figure()\nplt.plot(history.history['loss'],color=\"olive\", label='loss')\nplt.plot(history.history['val_loss'], color=\"plum\", label='validation loss')\nplt.legend(loc='best')\nplt.title(label=\"Loss and Validation Loss\")","eefe640d":"# Plotting train accuracy and validation accuracy\n\nplt.figure()\nplt.plot(history.history['accuracy'],color=\"mediumspringgreen\", label='accuracy')\nplt.plot(history.history['val_accuracy'], color=\"black\", label='validation accuracy')\nplt.legend(loc='best')\nplt.title(label=\"Accuracy and Validation Accuracy\")","8f488235":"### Model 1","71b8bf36":"### Visualizing training results","74ddae63":"## After Data Augmentation Model is not overfitting, Model gives train accuracy 74%, and val accuracy 70%","114ebed6":"### Create the model","416f9393":"## Using Image Data Generator to deal Overfitting","40b3418c":"### Train the model","a23ea777":"### Visualize the data","7c34c9ad":"## Model 2","f8eee89f":"## Observations\n- we can see that model is overfitting. ","ddcb8441":"### Creating Model 2","8c74c132":"### Visualizing training results","604198d7":"## Compile the Model","42735153":"`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch.\n\n`Dataset.prefetch()` overlaps data preprocessing and model execution while training.","3b6d90d6":"## Introduction\nwe will go through the training conv nets, ,preprocessing, data augmentation, model building etc.","7aa458c8":"The `image_batch` is a tensor of the shape `(32, 180, 180, 3)`. This is a batch of 32 images of shape `180x180x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.","7406eec2":"### Compile the model","b1c2cd3f":"### Loading using keras.preprocessing\n\nLet's load these images off disk using the helpful image_dataset_from_directory utility.","4b7ada82":"### Train the Model"}}