{"cell_type":{"f0cbd79c":"code","cca19707":"code","27b6a12b":"code","baae6e4a":"code","30f6c5e0":"code","dc313b91":"code","5a60eb24":"code","b3ddf7b2":"code","7faf8a4b":"code","8160620f":"code","ee0a1b8d":"code","ea399c0b":"code","9a842191":"code","45c65920":"code","60713f9e":"code","19df2803":"code","0e02825c":"code","0d93a3c9":"code","4aa8f2b8":"code","d57f99a6":"code","e7e23c51":"code","bd97b249":"code","9dad2388":"code","a2827f28":"code","f3f44a93":"code","14481647":"code","4f828e66":"code","088c34f7":"markdown","c033de9d":"markdown","61132696":"markdown","17e4f168":"markdown","90eccc99":"markdown","59c0ea16":"markdown","4ee77a9f":"markdown","2ca871a8":"markdown","608b281d":"markdown","d4e44645":"markdown","505d9fb4":"markdown","38107b08":"markdown","8521814d":"markdown","b75de303":"markdown","6da25d62":"markdown","6e8eb68e":"markdown","6dd902a2":"markdown","1456a8d3":"markdown","a546091a":"markdown","d8c7d84c":"markdown","b6e6293f":"markdown","6519053e":"markdown","a343f08e":"markdown"},"source":{"f0cbd79c":"#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","cca19707":"from __future__ import absolute_import, division, print_function, unicode_literals","27b6a12b":"# try:\n#   # %tensorflow_version only exists in Colab.\n#   %tensorflow_version 2.x\n# except Exception:\n#   pass\n","baae6e4a":"import tensorflow as tf","30f6c5e0":"tf.__version__","dc313b91":"# To generate GIFs\n!pip install -q imageio","5a60eb24":"import glob\nimport imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL\nfrom tensorflow.keras import layers\nimport time, os, math, shutil\nimport pickle\n\nimport IPython\nfrom IPython import display","b3ddf7b2":"BUFFER_SIZE = 60000","7faf8a4b":"reduce_ = False\nquarter = True\nassert not (quarter and reduce_)\nfile_ending = (\"_quarter_64\" if quarter else \"\") + (\"_reduced\" if reduce_ else \"\")\nside_length = 32 if reduce_ else 64","8160620f":"# Load data (each row corresponds to one sample)\nx_train = np.loadtxt(\"..\/input\/quarter\/x_train{}.csv\".format(file_ending), dtype=np.float64, delimiter=',')\nx_test  = np.loadtxt(\"..\/input\/quarter\/x_test{}.csv\".format(file_ending), dtype=np.float64, delimiter=',')\n\n# Reshape x to recover its 2D content\nx_train = x_train.reshape(x_train.shape[0], side_length, side_length, 1)\nx_test = x_test.reshape(x_test.shape[0], side_length, side_length, 1)\nprint(x_train.shape)\nprint(x_test.shape)","ee0a1b8d":"# Load labels:\ny_train = np.loadtxt(\"..\/input\/quarter\/y_test.csv\", dtype=np.float64, delimiter=',')\ny_test = np.loadtxt(\"..\/input\/quarter\/y_test.csv\", dtype=np.float64, delimiter=',')\n\n# Transform the labels y so that min(y) == 0 and max(y) == 1. Importantly, y_train and y_test must be considered jointly.\ndef up_scale(*args):\n    \"\"\" Up-scale features from ~10^-9 to ~0\n    \"\"\"\n    for x in args:\n        yield x * 10**9\n        \ny_train, y_test = up_scale(y_train, y_test)\n#y_train, y_test = standardize_zero_one(y_train, y_test)\nprint(\"Mean and standard deviation of y_train\", np.mean(y_train), np.std(y_train))\nprint(\"Mean and standard deviation of y_test\", np.mean(y_test), np.std(y_test))","ea399c0b":"x = np.concatenate((x_train, x_test), axis=0)\ny = np.concatenate((y_train, y_test), axis=0)","9a842191":"BATCH_SIZE = 128\n\n# Batch and shuffle the data\ndataset = tf.data.Dataset.from_tensor_slices(x).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","45c65920":"def make_generator_model(noise_dim):\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(side_length*side_length*16, use_bias=False, input_shape=(noise_dim,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((side_length\/\/4, side_length\/\/4, 256)))\n    assert model.output_shape == (None, side_length\/\/4, side_length\/\/4, 256) # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    assert model.output_shape == (None, side_length\/\/4, side_length\/\/4, 128)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, side_length\/\/2, side_length\/\/2, 64)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='sigmoid'))\n    assert model.output_shape == (None, side_length, side_length, 1)\n\n    return model","60713f9e":"def make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[side_length, side_length, 1]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model","19df2803":"# load json and create model\nwith open('..\/input\/psi2-predictor-191212\/model.json', 'r') as json_file:\n    loaded_model_json = json_file.read()\npsi2_model = tf.keras.models.model_from_json(loaded_model_json)\n# load weights into new model\npsi2_model.load_weights(\"..\/input\/psi2-predictor-191212\/model.h5\")\nfor l in psi2_model.layers:\n    l.trainable = False\npsi2_model.compile(loss='mean_squared_error')\nprint(\"Loaded model from disk\")","0e02825c":"print(psi2_model.summary())","0d93a3c9":"# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","4aa8f2b8":"def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","d57f99a6":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","e7e23c51":"class Images:\n    \"\"\"This class defines only class variables and class methods. It needn't be initiated.\n    \n    It was created to tackle the file generation limit of 500 files on kaggle. Unfortunately,\n    it seems that the kaggle kernel does not keep track of how many files exist, but rather\n    counts how many files were created (regardless of later destruction).\n    \"\"\"\n    zipzip_count = 0\n    zip_count = 0\n    image_count = 0\n    zip_dir = \"\/kaggle\/working\/archives\"\n    MAX_ZIP_COUNT = 70\n    \n    def cleanup():\n        \"\"\"Create a zip archive with all of the figures in TestRun.base_dir, and delete the figures.\n        \"\"\"\n        zip_name = \"output_figures_{}\".format(Images.zip_count + Images.MAX_ZIP_COUNT * Images.zipzip_count)\n        shutil.make_archive(os.path.join(Images.zip_dir, zip_name), 'zip', TestRun.base_dir)\n#         !zip -r {os.path.join(Images.zip_dir, zip_name)} {TestRun.base_dir}\/\n        !rm -rf  {TestRun.base_dir}\/*\n    \n    def archive_zips():\n        \"\"\"Create a zip archive with all of the zip archives created by the Images.cleanup() function,\n        and subsequently delete them.\n        \"\"\"\n        zipzip_name = \"\/kaggle\/working\/output_archives_{}\".format(Images.zipzip_count)\n        shutil.make_archive(zipzip_name, 'zip', Images.zip_dir)\n#         !zip -r {zipzip_name} {Images.zip_dir}\/\n        !rm -rf  {Images.zip_dir}\/*\n        if not os.path.isdir(Images.zip_dir):\n            os.makedirs(Images.zip_dir)\n        Images.zipzip_count += 1\n        Images.zip_count = 0\n        \n    def prepare(target_dir):\n        \"\"\"This function is called whenever a figure will be created.\n        It keeps track of the current number of figures in the folder, and ensures that\n        the target directory exists.\n        \"\"\"\n        Images.increment_image_count()\n        if not os.path.isdir(target_dir):\n            os.makedirs(target_dir)\n        \n    def increment_image_count():\n        \"\"\"Increment the image counter. If it exceeds the specified limit,\n        create a zip archive of the figures and subsequently remove them.\n        \"\"\"\n        Images.image_count += 1\n        if Images.image_count + Images.zip_count + Images.zipzip_count > 400 :\n            if Images.zip_count > MAX_ZIP_COUNT :\n                Images.archive_zips()\n            else:\n                Images.cleanup()\n                Images.zip_count += 1\n    \n    def generate_and_save(model, epoch, test_input, figure_dir):\n        \"\"\"Use the given model to generate 16 invisibility cloaks and plot them in a figure.\n        \"\"\"\n        Images.prepare(figure_dir)\n        # Notice `training` is set to False.\n        # This is so all layers run in inference mode (batchnorm).\n        predictions = model(test_input, training=False)\n\n        fig = plt.figure(figsize=(4,4))\n\n        for i in range(predictions.shape[0]):\n            plt.subplot(4, 4, i+1)\n            plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n            plt.axis('off')\n\n        plt.savefig(os.path.join(figure_dir, 'image_at_epoch_{:04d}.png'.format(epoch)))\n#         plt.show()\n        \n    def make_gif(figure_dir):\n        \"\"\"Use the images in the directory figure_dir to generate a gif\n        showing how the generator improves with each epoch.\n        \"\"\"\n        Images.prepare(figure_dir)\n        anim_file = os.path.join(figure_dir, 'dcgan.gif')\n\n        with imageio.get_writer(anim_file, mode='I') as writer:\n            filenames = glob.glob(os.path.join(figure_dir, 'image*.png'))\n            filenames = sorted(filenames)\n            last = -1\n            for i,filename in enumerate(filenames):\n                frame = 2*(i**0.5)\n                if round(frame) > round(last):\n                    last = frame\n                else:\n                    continue\n                image = imageio.imread(filename)\n                writer.append_data(image)\n            image = imageio.imread(filename)\n            writer.append_data(image)\n#         if IPython.version_info > (6,2,0,''):\n#             display.Image(filename=anim_file)\n            \n    def plot_and_save(lossRecorder, figure_dir):\n        \"\"\"Plot the evolution of generator loss, discriminator loss, and Psi2,\n        at each epoch of one training loop.\n        Takes as input a LossRecorder instance.\n        \"\"\"\n        Images.prepare(figure_dir)\n        \n        fig = plt.figure(figsize=(4,4))\n        plt.plot(lossRecorder.psi2_losses, linestyle='--')\n        ax = plt.gca()\n        ax.set_ylabel(\"Psi2\")\n        ax.legend([\"Psi2\"])\n        ax = ax.twinx()\n        ax.plot(lossRecorder.gen_losses)\n        ax.plot(lossRecorder.disc_losses)\n        plt.legend([\"Generator loss\", \"Discriminator loss\"])\n        plt.xlabel(\"Epoch\")\n        ax.set_ylabel(\"Cross-entropy\")\n        plt.title(\"Evolution of the GAN\")\n        plt.savefig(os.path.join(figure_dir, 'Losses_vs_epochs'))\n#         plt.show()","bd97b249":"class LossRecorder:\n    def __init__(self):\n        self.psi2_losses = []\n        self.gen_losses = []\n        self.disc_losses = []\n    \n    def append(self, losses):\n        self.psi2_losses.append(losses[0])\n        self.gen_losses.append(losses[1])\n        self.disc_losses.append(losses[2])\n        \n    def append_mean_of(self, loss_recorder):\n        psi2_loss, gen_loss, disc_loss = loss_recorder.mean()\n        self.psi2_losses.append(psi2_loss)\n        self.gen_losses.append(gen_loss)\n        self.disc_losses.append(disc_loss)\n    \n    def mean(self):\n        return np.mean(self.psi2_losses), np.mean(self.gen_losses), np.mean(self.disc_losses)\n    \n    def last(self):\n        return self.psi2_losses[-1], self.gen_losses[-1], self.disc_losses[-1]\n    ","9dad2388":"class UninitializedException(Exception):\n    def __init__(self):\n        self.message = \"TestRun must always be reinitialized before training!\"","a2827f28":"class TestRun:\n    \"\"\"Generate and train the models of the GAN for a given set of parameters\"\"\"\n    num_examples_to_generate = 16 # Number of images to generate for the gif.\n    base_dir = '\/kaggle\/working\/figures'\n    \n    def __init__(self, noise_dim):\n        \"\"\"A new TestRun instance must be created for each change in noise_dim.\n        The same seed is then used to generate 16 invisibility cloaks (for show) for all psi2_factors.\n        \"\"\"\n        self.noise_dim = noise_dim\n        self.seed = tf.random.normal([self.num_examples_to_generate, self.noise_dim])\n        self.initialized = False\n    \n    def reinitialize(self, psi2_scale_factor):\n        \"\"\"Each time the psi2_scale_factor is changed, the TestRun class must be reinitialized.\n        Configure the path where images are saved,\n        create a new generator and a new discriminator,\n        and create new optimizers for the two models.\n        \"\"\"\n        self.initialized = True;\n        self.psi2_scale_factor = psi2_scale_factor\n        \n        self.figure_dir = os.path.join(self.base_dir, \"noise_dim_{}\".format(self.noise_dim), \"psi2_factor_{}\".format(self.psi2_scale_factor))\n        \n        self.generator = make_generator_model(self.noise_dim)\n        self.discriminator = make_discriminator_model()\n        \n        self.generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n        self.discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n    \n    def train_step(self, images):\n        \"\"\"Perform one training step (one batch of images).\n        Start by generating a random noise vector.\n        Then, generate images and compute the losses.\n        Use tf.GradientTape to compute and apply gradients.\n        Return the losses.\n        \"\"\"\n        noise = tf.random.normal([BATCH_SIZE, self.noise_dim])\n\n        with tf.GradientTape(watch_accessed_variables=False) as gen_tape, tf.GradientTape(watch_accessed_variables=False) as disc_tape:\n            gen_tape.watch(self.generator.trainable_variables)\n            disc_tape.watch(self.discriminator.trainable_variables)\n\n            generated_images = self.generator(noise, training=True)\n\n            real_output = self.discriminator(images, training=True)\n            fake_output = self.discriminator(generated_images, training=True)\n\n            psi2_loss = np.mean(psi2_model.predict(generated_images))\n            gen_loss = generator_loss(fake_output)\n            disc_loss = discriminator_loss(real_output, fake_output)\n#             print(\"Generator loss:\", gen_loss)\n#             print(\"Psi2 loss:\", psi2_loss.shape)\n\n            total_generator_loss = (gen_loss + self.psi2_scale_factor * psi2_loss) \/ (1 + self.psi2_scale_factor)\n\n        gradients_of_generator = gen_tape.gradient(total_generator_loss, self.generator.trainable_variables)\n        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n\n        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n\n        return psi2_loss, gen_loss, disc_loss\n    \n    def train(self, dataset, epochs):\n        \"\"\"Train the GAN for the given number of epochs on the given dataset.\n        Loop over the epochs and train batchwise.\n        \"\"\"\n        if not self.initialized:\n            raise UninitializedException()\n        self.initialized = False\n        \n        losses_by_epoch = LossRecorder()\n        for epoch in range(epochs):\n            start = time.time()\n\n            losses_by_image = LossRecorder()\n            for image_batch in dataset:\n                losses_by_image.append(\n                    self.train_step(image_batch)\n                )\n            losses_by_epoch.append_mean_of(losses_by_image)\n\n            # We can't produce beautiful gifs because kaggle won't let us create more than 500 files even if we regularly archive & delete them :(\n            # Produce images for the GIF as we go\n    #         display.clear_output(wait=True)\n#             Images.generate_and_save(self.generator,\n#                                      epoch + 1,\n#                                      self.seed,\n#                                      self.figure_dir)\n#             # Save the model every 15 epochs\n#             if (epoch + 1) % 15 == 0:\n#                 checkpoint.save(file_prefix = checkpoint_prefix)\n\n            print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n        # Generate after the final epoch\n#         display.clear_output(wait=True)\n        Images.generate_and_save(self.generator,\n                                 epochs,\n                                 self.seed,\n                                 self.figure_dir)\n        Images.plot_and_save(losses_by_epoch, self.figure_dir)\n#         Images.make_gif(self.figure_dir)\n        \n        return losses_by_epoch.last()\n        ","f3f44a93":"class DcganGridsearch:\n    \"\"\"Find the best parameters noise_dim and psi2_scale_factor\n    \"\"\"\n    pickle_filepath = '\/kaggle\/working\/grid_search_results.pickle'\n    MAX_RUN_TIME = 28800 # Kernel is killed after 32400s (9h) -> 28800 (8h) leaves 1h\n    \n    def __init__(self, epochs, noise_dims, psi2_factors):\n        self.start_time = time.time()\n        self.cleanup_done = False\n        \n        self.epochs = epochs\n        self.noise_dims = noise_dims\n        self.psi2_factors = psi2_factors\n        \n        self.grid_shape = (len(noise_dims), len(psi2_factors))\n        self.psi2_losses = np.zeros(self.grid_shape)\n        self.gen_losses = np.zeros(self.grid_shape)\n        self.disc_losses = np.zeros(self.grid_shape)\n    \n    def search(self):\n        \"\"\"Perform a grid search (two nested for loops).\n        Return the generator (for generating and showing images).\"\"\"\n        for i, noise_dim in enumerate(self.noise_dims):\n            test_run = TestRun(noise_dim)\n\n            for j, psi2_scale_factor in enumerate(self.psi2_factors):\n                if self.check_remaining_time():\n                    break\n                \n                test_run.reinitialize(psi2_scale_factor)\n                psi2_loss, gen_loss, disc_loss = test_run.train(dataset, self.epochs)\n                self.psi2_losses[i,j] = psi2_loss\n                self.gen_losses[i,j] = gen_loss\n                self.disc_losses[i,j] = disc_loss\n        generator = test_run.generator\n        del test_run\n        return generator\n    \n    def pickle(self):\n        \"\"\"Serialize the grid search results\"\"\"\n        pickle_dict = {\n            'noise_dims': self.noise_dims,\n            'psi2_factors': self.psi2_factors,\n            'psi2_losses': self.psi2_losses,\n            'gen_losses': self.gen_losses,\n            'disc_losses': self.disc_losses,\n        }\n        with open(self.pickle_filepath, 'wb') as dumpfile:\n            pickle.dump(pickle_dict, dumpfile)\n    \n    def check_remaining_time(self):\n        if time.time() > self.start_time + self.MAX_RUN_TIME :\n            self.cleanup()\n            return True\n        return False\n    \n    def cleanup(self):\n        if not self.cleanup_done:\n            Images.cleanup()\n            Images.archive_zips()\n            self.cleanup_done = True\n    \n    def get_best_psi2(self):\n        \"\"\"Zip and remove figures etc.\"\"\"\n        best_psi2_by_factor = self.psi2_losses.min(axis=0)\n        assert len(best_psi2_by_factor.shape) == 1\n        assert best_psi2_by_factor.shape[0] == len(self.psi2_factors)\n        best_factor_ind = np.argmin(best_psi2_by_factor)\n        best_noisedims_ind = np.argmin(self.psi2_losses[:,best_factor])\n        best_psi2 = self.psi2_losses[best_noisedims_ind, best_factor_ind]\n        assert np.isclose(best_psi2, self.psi2_losses.min())\n        return best_psi2, self.noise_dims[best_noisedims_ind], self.psi2_factors[best_factor_ind]\n        \n        ","14481647":"#if you get error messages, make sure you have the latest version of tensorflow (then it should work, not tested though)\n#if still not working please fork and run the latest version published on kaggle that you can find here:\n# https:\/\/www.kaggle.com\/froehlichbergbier\/dc-gan-for-invisibility-cloak\n\ndcgan_gs = DcganGridsearch(\n    epochs= 40,\n    noise_dims= [64], # np.power(2, range(4, 10)),\n    psi2_factors= [2.5] # [0.1, 0.8, 2.5, 10]\n)\n\nlast_generator = dcgan_gs.search()\ndcgan_gs.pickle()\ndcgan_gs.cleanup()","4f828e66":"# Generate the invisibility cloaks (without running out of memory...)\nNUM_BATCHES = 50\nBATCH_SIZE = 1000\npsi2_of_images = []\nseed = tf.random.normal([BATCH_SIZE*NUM_BATCHES, dcgan_gs.noise_dims[-1]])\nfor batch in range(NUM_BATCHES):\n    generated_images = last_generator(seed[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE], training=False)\n    psi2_of_images = np.concatenate((psi2_of_images, psi2_model.predict(generated_images).flat), axis=0)\n    del generated_images\nassert len(psi2_of_images.shape) == 1\nassert psi2_of_images.shape[0] == NUM_BATCHES * BATCH_SIZE\n\n# Visualize the predicted Psi2 values\nplt.boxplot(psi2_of_images)\nplt.ylabel(\"Psi 2\")\nplt.title(\"Box-plot of the generated images' Psi 2 metric\")\nplt.savefig(os.path.join(TestRun.base_dir, \"generator_box_plot\"))\n\n# Find the best invisibility cloaks (after we had to delete them...) and plot them\nbest_indices = np.argsort(psi2_of_images)[:16]\n\nbest_images = np.concatenate([last_generator(seed[ind:ind+1,:], training=False) for ind in best_indices], axis=0)\n\nfig = plt.figure(figsize=(8,2))\n\nfor i in range(16):\n    plt.subplot(2,8, i+1)\n    plt.imshow(best_images[i,:, :, 0] * 127.5 + 127.5, cmap='gray')\n    plt.axis('off')\n\nplt.savefig(os.path.join(TestRun.base_dir, \"16 best images\"))\n\n# Save the best invisibility cloaks to CSV format\nnp.savetxt(\"\/kaggle\/working\/best16images.csv\", best_images.reshape((16*64, 64)), delimiter=',', fmt='%i')\n\nprint(psi2_of_images[best_indices])","088c34f7":"### The Generator\n\nThe generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). Start with a `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image size of 64x64x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses sigmoid.","c033de9d":"# Generate invisibility cloaks and evaluate them\nGenerate 50000 images, record their psi2 loss, and show the best few images","61132696":"## Define the loss and optimizers\n\nDefine loss functions and optimizers for both models.\n","17e4f168":"Choose which version of the data set should be used\n- reduce: set this option equal true, if you use a dataset containing full shells in 64x64 resolution and you want to use just the upper left quarter\n- quarter: set this value to true, if you use a dataset containing just the upper left quarter of the matrice in higher resolution (64x64)","90eccc99":"### The Psi2 predictor\n\nThe Psi2 predictor is a CNN that was trained using the inputs and outputs of simulations. It predicts the loss metric Psi2, which corresponds to the amount of light scattered by the shell.","59c0ea16":"### What are GANs?\n[Generative Adversarial Networks](https:\/\/arxiv.org\/abs\/1406.2661) (GANs) are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A *generator* (\"the artist\") learns to create images that look real, while a *discriminator* (\"the art critic\") learns to tell real images apart from fakes.\n\nDuring training, the *generator* progressively becomes better at creating images that look real, while the *discriminator* becomes better at telling them apart. The process reaches equilibrium when the *discriminator* can no longer distinguish real images from fakes.\n\nTo learn more about GANs, we recommend MIT's [Intro to Deep Learning](http:\/\/introtodeeplearning.com\/) course.","4ee77a9f":"### Import TensorFlow and other libraries","2ca871a8":"## Run the grid search","608b281d":"## Define a class to record the losses during training","d4e44645":"##### Copyright 2019 The TensorFlow Authors.","505d9fb4":"## Table of contents\n\n*Highlighted* sections were partially adapted by us, **strongly highlighted** ones are our contribution, though they incorporate code fragments from the tutorial.\n- *Import TensorFlow and other libraries*\n- **Load the dataset**\n- Create the models\n  - *The generator*\n  - The discriminator\n  - **The Psi2 predictor**\n- Define the loss functions\n- **Define a class to generate and archive images**\n- **Define a class to record the losses during training**\n- **Define a class to train one GAN for a given set of parameters**\n- **Define a class for hyper-parameter tuning**\n- **Run the grid search**\n- **Generate invisibility cloaks and evaluate them**","38107b08":"### The Discriminator\n\nThe discriminator is a CNN-based image classifier.","8521814d":"## Define a class for hyper-parameter tuning","b75de303":"The full dataset is used to train the GAN, without a test\/train split","6da25d62":"### Generator loss\nThe generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s.","6e8eb68e":"This notebook generates invisibility cloaks using a [Deep Convolutional Generative Adversarial Network](https:\/\/arxiv.org\/pdf\/1511.06434.pdf) (DCGAN). The code is written using the [Keras Sequential API](https:\/\/www.tensorflow.org\/guide\/keras) with a `tf.GradientTape` training loop.","6dd902a2":"### Discriminator loss\n\nThis method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s.","1456a8d3":"# Deep Convolutional Generative Adversarial Network","a546091a":"## License agreement of the TensorFlow tutorial from which this notebook was adapted","d8c7d84c":"## Define a class to generate and archive images","b6e6293f":"## Create the models\n\nBoth the generator and discriminator are defined using the [Keras Sequential API](https:\/\/www.tensorflow.org\/guide\/keras#sequential_model).","6519053e":"### Load the dataset","a343f08e":"## Define a class to train one GAN for a given set of parameters\nThis class is used repeatdely by the DcganGridsearch class. The parameters are:\n- `noise_dim`: the dimension of the random noise input vector\n- `psi2_scale_factor`: the relative weight of psi2 in the total generator loss\n\nThe training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator.\n\nThe generator is trained to minimize the **total generator loss**, which is a weighted average of the generator loss and Psi2.\n\nThe TestRun class uses a custom Exception defined here:"}}