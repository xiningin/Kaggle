{"cell_type":{"b8a93834":"code","ad758ea2":"code","0b0d5dde":"code","97571672":"code","db6109db":"code","b4dd8e2e":"code","a6fa0cb5":"code","d08f56c7":"code","273e4b14":"code","2242f1c3":"code","9b53c665":"code","83aebd97":"code","ce924e7f":"code","af8fad3c":"code","8376a6eb":"code","88b024f7":"code","dd27c017":"code","1ef8a35e":"code","43e570b8":"code","c608da19":"code","5bf1ac92":"code","6d1e5469":"markdown","716fedeb":"markdown","2eda632d":"markdown","edf4128a":"markdown","e7492efb":"markdown","0567b39b":"markdown","3a03bb1a":"markdown"},"source":{"b8a93834":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","ad758ea2":"def read_train():\n    train=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","0b0d5dde":"def jaccard_improve(str1, str2): \n    str1=str1.lower()\n    str2=str2.lower()    \n    index=str1.find(str2) \n    text1=str1[:index]\n    #print(text1)\n    text2=str1[index:].replace(str2,'')\n    words1=text1.split()\n    words2=text2.split()\n    #print(words1[-3:])\n\n    if len(words1)>len(words2):\n        words1=words1[-3:]\n        mod_text=\" \".join(words1)+\" \"+ str2\n    else:\n        words2=words2[0:2]\n        mod_text=str2+\" \"+\" \".join(words2)\n    return mod_text ","97571672":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split())  \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","db6109db":"print(len(train_df))\n#train_df1=train_df","b4dd8e2e":"train_df['selected_text_mod']=train_df['selected_text']\ntrain_df['mod']=0","a6fa0cb5":"MAX_LEN = 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","d08f56c7":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')","273e4b14":"print(input_ids.shape)\nprint(attention_mask.shape)\nprint(token_type_ids.shape)\nprint(start_tokens.shape)\nprint(end_tokens.shape)","2242f1c3":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","9b53c665":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","83aebd97":"def scheduler(epoch):\n    return 3e-5 * 0.2**epoch","ce924e7f":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    #x1 = tf.keras.layers.ReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    #x2 = tf.keras.layers.ReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)    \n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model\n","af8fad3c":"n_splits = 5","8376a6eb":"preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(5):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    #model.load_weights('..\/input\/m6aprila\/v6-roberta-%i.h5'%i)\n    model.load_weights('..\/input\/model8\/v8-roberta-%i.h5'%i)\n\n    #model.load_weights('v5-roberta-%i.h5'%i)\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/n_splits\n    preds_end += preds[1]\/n_splits","88b024f7":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_df.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])          \n        st1=st.strip()\n        if st1=='****' or  st1=='****!' or st1=='****!' or st1=='****!' or st1=='****,' or  st1=='****,' or  st1=='****.' or st1=='****.':\n            #print(st1.strip())\n            #print(text1)\n            st=text1\n        elif st1=='(good':   \n            st='good'\n        elif st1=='__joy':   \n            st='joy'           \n    all.append(st)","dd27c017":"test_df['selected_text'] = all","1ef8a35e":"test_df.head()","43e570b8":"from nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\nstop = set(stopwords.words('english'))\nsentence = \"this is a foo bar sentence\"\n\nlist = []\n\nfor sentence, sentiment  in zip(test_df['selected_text'], test_df['sentiment']):\n    if sentiment == 'positive' or sentiment == 'negative':\n        filtered_sentence = [i for i in word_tokenize(sentence.lower()) if i not in stop]\n        list.append(' '.join(filtered_sentence))\n    else:\n        filtered_sentence = sentence\n        list.append(filtered_sentence)","c608da19":"test_df['selected_text'] = pd.Series(list)","5bf1ac92":"test_df[['textID','selected_text']].to_csv('submission.csv',index=False)\n ","6d1e5469":"# Model","716fedeb":"# Data preproccesing","2eda632d":"# TensorFlow roBERTa + CNN head - LB   v2","edf4128a":"# Load  data and libraries","e7492efb":"# Inference","0567b39b":"# Train\nWe will skip this stage and load already trained model","3a03bb1a":"Hello everyone! \n\n1. 1. 1. This kernel is based on [Al-Kharba Kiram](https:\/\/www.kaggle.com\/al0kharba\/tensorflow-roberta-0-712\/output).  \n\n "}}