{"cell_type":{"37146f65":"code","13a5c399":"code","e9a5070e":"code","20143484":"code","4f39723b":"code","d26bc897":"code","8ab74774":"code","e5eb4de5":"code","e34eb0de":"code","d25056cf":"code","7779fb8b":"code","7881e260":"code","cc3d0ba6":"code","cd241382":"code","c11dc0f7":"code","1b5ca95f":"code","e010bd52":"code","aef5b4e0":"code","820afb8a":"code","46db0b12":"code","d98fcc09":"code","e12417f5":"code","6e970824":"code","4fffe0d0":"code","febc1c23":"code","52947a57":"code","f6d97981":"code","0b32c9c5":"code","878bc691":"code","5f86169d":"code","2831fa27":"code","8fee8d78":"code","6be01cad":"code","4f88c7c0":"code","8f58b41d":"code","b6bd690d":"code","880ade90":"code","c7c765a5":"code","c323230f":"code","28e17e41":"code","bd9b0015":"code","c942749e":"code","3e6e352c":"code","10116b09":"code","b2e8fe72":"code","50a8634a":"code","a66d015d":"code","9eb86d72":"code","0c92da8c":"code","216bf3c4":"code","0dad7d98":"code","9628e767":"code","768ecfea":"code","fec47091":"code","0a342ea6":"code","56536091":"code","b6dcdf08":"code","0e5c6cca":"code","20055b57":"code","1715dd3d":"code","c4527a17":"code","80822ca3":"code","51f05ddc":"code","1dc623f2":"code","28037263":"code","103ec2ae":"code","76204140":"code","a0db15b0":"code","7de0fbb6":"code","b2f8f8a9":"code","b6f0c835":"code","eec5f46f":"code","e2132fee":"code","ec1438bf":"code","6a37de57":"code","6b5dfe13":"code","52f3a112":"code","af38a66a":"code","9996d6a8":"code","f50abba3":"code","e2eef850":"code","aac3ea0c":"code","d83df8a1":"code","8ca036e2":"code","1f68735f":"code","080a674b":"code","96843838":"code","b9a8ab4e":"code","f474f393":"code","87ca5013":"code","39cf839d":"code","993777e5":"code","860e3e3a":"code","ec002a7f":"code","e8d45663":"code","391d66c6":"code","b69c083b":"code","7686e7c6":"code","03eda502":"code","cde24feb":"code","ce024614":"code","9801423e":"code","89cde882":"code","eb116a2a":"code","c1fc340c":"code","2ade7acd":"code","7e9c9a14":"code","8a3834b0":"code","45bac6f2":"code","30e86d00":"code","dd679bcf":"code","14fb3e35":"code","cae63cf6":"code","14247161":"code","00bfc57e":"code","e2124930":"code","5a2ff3d8":"code","5437e0a9":"code","d253ba7e":"code","246c19c3":"code","678205b5":"code","0c3d0480":"code","181f99f0":"code","23c9e1c8":"code","0a6d7977":"markdown","67a04ca1":"markdown","52ee701e":"markdown","108c9416":"markdown","8c57ced5":"markdown","e733cb55":"markdown","b40b7c27":"markdown","5d2d9917":"markdown","84577af8":"markdown","c7f0639f":"markdown","363fcc12":"markdown","fad41143":"markdown","be2166a0":"markdown","735869ee":"markdown","c35face8":"markdown","a3f16955":"markdown","2b1e0895":"markdown","26840bc3":"markdown","0edea101":"markdown","b591ea88":"markdown","baacdaab":"markdown","58502c28":"markdown","0d24fe99":"markdown","cc9edcb0":"markdown","80d727fb":"markdown","217be54a":"markdown","8b72b169":"markdown","6edb4351":"markdown","2ec3ee3c":"markdown","ce821fae":"markdown","aaa1c6bd":"markdown","50de403f":"markdown","3cf71c7c":"markdown","41bbce43":"markdown","e730caed":"markdown","07a6919f":"markdown","8cac801a":"markdown","be4f1727":"markdown","2133fdd0":"markdown","4b4ff8dc":"markdown","fcbd3996":"markdown","efb119d3":"markdown","95f7cb07":"markdown","3a562b04":"markdown","9178ab6b":"markdown","150a0925":"markdown","6162670c":"markdown","627588c2":"markdown","4df1d379":"markdown","c4d1eea4":"markdown","70114a38":"markdown","a97ffb89":"markdown","357d99a3":"markdown","f5447507":"markdown","84c506bb":"markdown","f5436709":"markdown","8e0216f2":"markdown","105230de":"markdown","9e4877c9":"markdown","8a9e74e3":"markdown","ce09e3cf":"markdown","226f0cf1":"markdown","a6427ea2":"markdown","43a7edc8":"markdown","fc57d7ce":"markdown","0c235507":"markdown","d9c13d41":"markdown","b798d25b":"markdown","a4ec3a25":"markdown","5ec855aa":"markdown","fa85ef79":"markdown","4f346385":"markdown","b24d0e44":"markdown","706430ec":"markdown","57f62662":"markdown","e0f84431":"markdown"},"source":{"37146f65":"import torch \nimport numpy as np","13a5c399":"torch.cuda.is_available()","e9a5070e":"data = [[1 , 2 ] , [3 , 4]]\nprint(data)\ntype(data)","20143484":"np.array(data)","4f39723b":"data = torch.tensor(data)\ntype(data)","d26bc897":"data.dtype","8ab74774":"array =np.random.rand(3 ,4 )\narray","e5eb4de5":"torch.from_numpy(array)","e34eb0de":"torch.tensor(array)","d25056cf":"torch.ones(3 ,4)","7779fb8b":"torch.zeros(3 ,4)","7881e260":"my_tensor = torch.rand(3 , 4)","cc3d0ba6":"my_tensor.dtype","cd241382":"my_tensor.device","c11dc0f7":"my_tensor  = my_tensor.to(\"cuda\")","1b5ca95f":"my_tensor","e010bd52":"my_tensor[0]","aef5b4e0":"my_tensor[: , 1:3]","820afb8a":"print(my_tensor.mul(my_tensor))\n\nprint(\"\\n\",my_tensor * my_tensor)","46db0b12":"my_tensor.matmul(my_tensor.T)","d98fcc09":"torch.matmul(my_tensor , my_tensor.T)","e12417f5":"my_tensor @ my_tensor.T","6e970824":"my_tensor.sum()  , my_tensor.sum(axis = 0) , my_tensor.sum(axis = 1)","4fffe0d0":"my_tensor.max() , my_tensor.min()","febc1c23":"torch.cat([my_tensor , my_tensor], axis = 1)","52947a57":"torch.cat([my_tensor , my_tensor ] ,axis = 0)","f6d97981":"my_tensor.shape","0b32c9c5":"my_tensor.size()","878bc691":"my_tensor.clip(0.2 ,0.8)","5f86169d":"my_tensor.cpu().detach().numpy()","2831fa27":"empty_ = torch.empty(size =(3,2))\nempty_","8fee8d78":"torch.ones(size = (3, 3))","6be01cad":"torch.ones(3,3)","4f88c7c0":"torch.zeros(size = (3 ,4))","8f58b41d":"torch.zeros(3 ,3)","b6bd690d":"arr = torch.linspace(start = 1 , end = 10 , steps = 20) \na = torch.tensor(arr , dtype = torch.float32)\ntype(a)","880ade90":"a.numpy()","c7c765a5":"torch.eye(3)","c323230f":"torch.eye(4)","28e17e41":"a = torch.arange(5)\na","bd9b0015":"diag_ = torch.diag(a)\ndiag_","c942749e":"diag_.diag()","3e6e352c":"a = torch.ones(size = (3,4))\nb = torch.ones(size = (3,4)) * 4 \n\nprint(f'a : \\n{a} \\n\\n\\n  b:\\n {b}')","10116b09":"z = torch.empty(size = (3,4))\n\ntorch.add(a , b , out = z)","b2e8fe72":"z = torch.add(a,b)\nz","50a8634a":"z = a + b\nz","a66d015d":"a.add_(b)","9eb86d72":"x = torch.ones(size = (3,4))\ny = torch.ones(size = (4 , 5)) * 5\n\nprint(f'x: \\n {x} \\n\\n y: \\n {y}')","0c92da8c":"torch.matmul(x , y)","216bf3c4":"torch.mm(x , y)","0dad7d98":"x@y","9628e767":"torch.rand(3 , 2 , 4) \n\n# 3 matrix in one tensor with size 2 X 4 ","768ecfea":"batch = 10 \nm = 30\nn = 20\nk = 10 \n\nx = torch.rand(size =(batch , m , n ))\ny = torch.rand(size =(batch , n , k ))\ntorch.bmm(x,y).shape #bmm= batch multiplication","fec47091":"t = torch.arange(15)\nt","0a342ea6":"t.shape","56536091":"t.unsqueeze(axis = 0)","b6dcdf08":"t.unsqueeze(axis = 1)","0e5c6cca":"t.view(3, 5)","20055b57":"t1 = torch.linspace(0 , 15 , 24)\nt1","1715dd3d":"t1.view(2 , 3 ,4)","c4527a17":"t1.reshape(2 , 3 , 4)","80822ca3":"import torch ","51f05ddc":"a = torch.tensor([5.0] , requires_grad = True)\nb = torch.tensor([6.0] , requires_grad = True)","1dc623f2":"a","28037263":"b","103ec2ae":"y = a**3 - b**2\ny","76204140":"print(a.grad) , print(b.grad)","a0db15b0":"y.backward()","7de0fbb6":"a.grad , b.grad","b2f8f8a9":"Weight = torch.randn(10 , 1, requires_grad = True)\nbias = torch.randn(1 , requires_grad = True) ","b6f0c835":"Weight","eec5f46f":"bias","e2132fee":"features = torch.rand(1, 10)","ec1438bf":"features","6a37de57":"output = torch.matmul(features , Weight)+bias","6b5dfe13":"loss = 1-output","52f3a112":"output","af38a66a":"loss.backward()","9996d6a8":"Weight.grad","f50abba3":"learning_rate = 0.001\nwith torch.no_grad():\n    Weight = Weight - learning_rate * Weight.grad.data","e2eef850":"Weight","aac3ea0c":"import numpy as np\nimport torch ","d83df8a1":"inputs = np.array([[73 , 67 , 43],\n                  [91 , 88 , 64],\n                   [87 , 134 , 58],\n                   [102 , 43 , 37],\n                   [69 ,96 , 70]] , dtype = \"float32\")","8ca036e2":"targets = np.array([[56, 70],\n                  [81 ,101],\n                  [119, 133],\n                  [22 ,37],\n                  [103 , 119]] , dtype = \"float32\")","1f68735f":"inputs = torch.from_numpy(inputs)\ntarget = torch.from_numpy(targets)","080a674b":"inputs","96843838":"w = torch.randn(2 , 3 , requires_grad = True)\nb = torch.randn(2 , requires_grad = True)","b9a8ab4e":"w , b ","f474f393":"def model(x):\n    return x @ w.T + b\n# @ for vector multiplication","87ca5013":"# Predicted values\npred = model(inputs)\nprint(pred)","39cf839d":"# actual target \nprint(targets)","993777e5":"def mse(t1 , t2):\n    diff = t1 - t2\n    return torch.sum(diff * diff) \/ diff.numel()\n# numel will give the total number of elements\n# * gives the element by element multiplication","860e3e3a":"# Compute the loss\nloss = mse(pred , target)\nloss","ec002a7f":"import math\nmath.sqrt(loss)","e8d45663":"loss.backward()","391d66c6":"print(w)\nprint(w.grad)","b69c083b":"print(b)\nprint(b.grad)","7686e7c6":"w.grad.zero_()\nb.grad.zero_()\nw.grad , b.grad","03eda502":"# making predictions\npreds = model(inputs)\npreds","cde24feb":"with torch.no_grad():\n    w-= w.grad * 0.0001\n    b-= b.grad * 0.0001\n    w.grad.zero_()\n    b.grad.zero_()","ce024614":"w , b","9801423e":"preds = model(inputs)\nloss = mse(preds , target)\nloss","89cde882":"# for 50 epoches\nfor i in range(50):\n    preds = model(inputs)\n    loss = mse(preds , target)\n    loss.backward()\n    with torch.no_grad():\n        w -= w.grad * 0.0001\n        b -= b.grad * 0.0001\n        w.grad.zero_()\n        b.grad.zero_()","eb116a2a":"preds = model(inputs)\nloss = mse(preds , target)\nloss","c1fc340c":"preds","2ade7acd":"target","7e9c9a14":"import torch.nn as nn\nfrom torch.utils.data import TensorDataset","8a3834b0":"inputs = np.array([[73, 67, 43], \n                   [91, 88, 64], \n                   [87, 134, 58], \n                   [102, 43, 37], \n                   [69, 96, 70], \n                   [74, 66, 43], \n                   [91, 87, 65], \n                   [88, 134, 59], \n                   [101, 44, 37], \n                   [68, 96, 71], \n                   [73, 66, 44], \n                   [92, 87, 64], \n                   [87, 135, 57], \n                   [103, 43, 36], \n                   [68, 97, 70]], \n                  dtype='float32')\n\n# Targets (apples, oranges)\n\ntargets = np.array([[56, 70], \n                    [81, 101], \n                    [119, 133], \n                    [22, 37], \n                    [103, 119],\n                    [57, 69], \n                    [80, 102], \n                    [118, 132], \n                    [21, 38], \n                    [104, 118], \n                    [57, 69], \n                    [82, 100], \n                    [118, 134], \n                    [20, 38], \n                    [102, 120]], \n                   dtype='float32')\n\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)","45bac6f2":"inputs[:5]","30e86d00":"train_data = TensorDataset(inputs, targets)\ntrain_data[0:3]","dd679bcf":"from torch.utils.data import DataLoader","14fb3e35":"batch_size = 5\ntrain_loader = DataLoader(train_data, batch_size, shuffle=True)\n#shuffle as the name suggest shuffles the data and gives us random values","cae63cf6":"for x , y in train_loader:\n    print(\"Batche 1 : \")\n    print(x)\n    print(y)\n    # To check all the batches comment out the break statement\n    break\n","14247161":"model = nn.Linear(3 , 2)\nprint(model.weight)\nprint(model.bias)","00bfc57e":"list(model.parameters())","e2124930":"preds = model(inputs)\npreds[:4]","5a2ff3d8":"import torch.nn.functional as F","5437e0a9":"# ?nn.Linear","d253ba7e":"loss_fn = F.mse_loss\nloss = loss_fn(model(inputs) ,targets )\nloss","246c19c3":"opt = torch.optim.SGD(model.parameters() , lr = 0.00001)","678205b5":"# Utility function to train the model\ndef fit(num_epochs, model, loss_fn, opt, train_data):\n    \n    # Repeat for given number of epochs\n    for epoch in range(num_epochs):\n        \n        # Train with batches of data\n        for x,y in train_data:\n            \n            # 1. Generate predictions\n            pred = model(x)\n            \n            # 2. Calculate loss\n            loss = loss_fn(pred, y)\n            \n            # 3. Compute gradients\n            loss.backward()\n            \n            # 4. Update parameters using gradients\n            opt.step()\n            \n            # 5. Reset the gradients to zero\n            opt.zero_grad()\n        \n        # Print the progress\n        if (epoch+1) % 10 == 0:\n            print('Epoch [{}\/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))","0c3d0480":"fit(100, model, loss_fn, opt, train_data)","181f99f0":"preds = model(inputs)\npreds","23c9e1c8":"targets","0a6d7977":"# Introduction","67a04ca1":"# Linear Regression using Back-propogation \n\nFor this you should have some basic knowledge of maths which is working behind Linear Regression and Gradient Descent \n\n[Check out this You_tube Video for understanding the Gradient Descent concept](https:\/\/www.youtube.com\/watch?v=vsWrXfO3wWw)","52ee701e":"# Linear Regression using PyTorch Bulit-In Library","108c9416":"## Train the model\n![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1628643\/2676548\/grad%20steps.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211004%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211004T190914Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=7655b98b732f2caafec13aa8a31ed206914d58bcf2a396698053a2507f13e0b83e395eb80ccdcd706ab8689f8dc6447abe65ebe3c480de65f3be75729273592c7488b012efdd503851425b3ec63e0c59145857157e9269bb7ce41c566ecd6e1f68d7d636e6f02d511f50fca35002e1951e88ea5d245844b264b14768135eb095d54c286d8648989c4398601723ede47da367c837d141a6eb36bb22a69f0eac7d5039995dd4efc0707500000fd8d958e16cf7a12bcc9dc47a27515e162f919b56ea99cce2dcf4a37936313d0d55b7827269c24159dd101316eb2621c0e046758a704aa3e60e650e7d325dbaeff3875a751b177b237dce38243ea1d77e6199cc44)","8c57ced5":"#### **DataLoader can split the data in batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data**","e733cb55":"## **NOTE: For this Notebook you should have a little bit knowledge of Numpy \ud83d\udcda\ud83d\udcd9**\n\n[Check out this link for getting a basic Idea of Numpy ](https:\/\/www.kaggle.com\/colinmorris\/working-with-external-libraries)\n\n","b40b7c27":"## **To check  if torch can access GPU or not**\n\nFor this you have to select accelator (GPU) from right --->\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F4127694%2Ffbbf954abb41e8bbbb6f5a668ec44849%2Fgpu.PNG?generation=1582892394479926&alt=media)","5d2d9917":"Here Y is the predicted value and x vector is features and beta one is weights and epislon one is the bias \n\n**torch.randn** creates a tensor with the given shape with elements picked randomly from a normal distribution with mean = 0 and standard deviation 1","84577af8":"## Intiate the back-propoagtion","c7f0639f":"## Matmul (Matrix Multlipication)\n**Matmul method is used for matrix multiplication of two tensors** \n![](https:\/\/i1.faceprep.in\/Companies-1\/matrix-multiplication-in-python.png)\n\nThree different way that you can perform this operations are shown below \ud83d\udd3d\n","363fcc12":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTyHkcnQcqBu-GYtVbFaocaV33_bW4EyLe7Jw&usqp=CAU)","fad41143":"**Clips are alias for Clamps**\n\nClamps all elements in input into the range [ min, max ]. Letting min_value and max_value be min and max, respectively, this returns:\n\nyi=min\u2061(max\u2061(xi,min_valuei),max_valuei)\n\n\nIf min is None, there is no lower bound. Or, if max is None there is no upper bound.\n\n**NOTE : If min is greater than max torch.clamp(..., min, max) sets all elements in input to the value of max.**","be2166a0":"![](https:\/\/i.imgflip.com\/1cjwpr.jpg)","735869ee":"We can see that there are huge difference between our Actual targets and Our Predicted Targets\n \n## Loss function\n\nBefore we improve our model, we need a way to evaluate how well our model is performing. We can compare the model's predictions with the actual targets using the following method:\n\n* Calculate the difference between the two matrices (`preds` and `targets`).\n* Square all elements of the difference matrix to remove negative values.\n* Calculate the average of the elements in the resulting matrix.\n\nThe result is a single number, known as the **mean squared error** (MSE).","c35face8":"At first the grad will be none for both tensors","a3f16955":"The gradients are stored in the .grad property of the respective tensors\n\n**NOTE :** The Derivative of the loss w.r.t the weights is itself a matrix , with the same dimensions ","2b1e0895":"### **Switching between CPU and GPU**\nA torch.device is an object representing the device on which a torch.Tensor is or will be allocated.\n\nThe torch.device contains a device type ('cpu' or 'cuda') and optional \ndevice ordinal for the device type.\n\nFor switching from CPU to GPU use \".to('cuda')\n\n![](https:\/\/naadispeaks.files.wordpress.com\/2020\/10\/download.jpeg?w=300)","26840bc3":"## Creating some random features ","0edea101":"# Optimizer\n\nInstead of changing the weights and biased manually we will take the help of inbuilt Optimizers in Pytorch. \n\nSGD :  stochastic gradient descent\n\nTo understand this SGD [click here](https:\/\/www.youtube.com\/watch?v=IU5fuoYBTAM&t=885s)\n\nlr is the learning rate and we have to provide model.parameters() as and argument so that the optimizer knows which matrices should be modified druing the update step","b591ea88":"\n#### **Imagine TensorDataset as a Class in which you pass your data as arguments and use the methods which are provided by the class**\n\nTensor Dataset allows us to access rows from the inputs and targets as tuples and provide us standard API's for working with many different types of datasets in PyTorch","baacdaab":"### Way to create tensors to normal Numpy Arrays","58502c28":"### Creating a tensor with ones and zero","0d24fe99":"## Tensor Multiplication\n\n![](https:\/\/d138zd1ktt9iqe.cloudfront.net\/media\/seo_landing_files\/multiplication-of-matrices-in-linear-algebra-1627878775.png)\n\nAs we Already know to multiply two matrix n should be same for two matrix ","cc9edcb0":"## Converting Normal arrays in tensors","80d727fb":"Again we take the random values for bias and weights using the randn method","217be54a":"## **Train the model for multiple epochs**\n epoches simply means the no of iterations","8b72b169":"Modified Weights are : ","6edb4351":"converting a back to tensor","2ec3ee3c":"### Creating a Identity Tensor","ce821fae":"## Example 2 (Linear Reggression)\n\n","aaa1c6bd":"### Simple list ","50de403f":"As we can see loss is pretty low now ","3cf71c7c":"#### Model.parameters() method returns a list containing all the weights and bias matrices present in the model","41bbce43":"Don't worry if you are not getting the matrix concept.\nJust Understand this here in model function we are returning the predicted values and the predicted values is in the form of tensors( matrix )\n\n\n**.T** use to transpose the matrix\n\nwe transpose our weight vector because we have created our weights in 2 dimensions and for vector muliplication purpose we have to transpose it\n\n![](https:\/\/i.imgur.com\/WGXLFvA.pnghttps:\/\/i.imgur.com\/WGXLFvA.png)","e730caed":"## **Creating Tensors using Torch** \n\n![](https:\/\/media.makeameme.org\/created\/what-is-tensor.jpg)\n\nA tensor is a dimensional data structure. Vectors are one-dimensional data structures and matrices are two-dimensional data structures. Tensors are superficially similar to these other data structures, but the difference is that they can exist in dimensions ranging from zero to n \n\nThere are different way we can create tensors for example creating tensors from simple list and from numpy array as shown below \ud83d\udd3d","07a6919f":"## Difference between Numpy and Pytorch ?\nThe most important difference between the two frameworks is naming. Numpy calls tensors (high dimensional matrices or vectors) arrays while in PyTorch there\u2019s just called tensors. Everything else is quite similar.\n\n## Why PyTorch?\n\nEven if you already know Numpy, there are still a couple of reasons to switch to PyTorch for tensor computation. The main reason is the GPU acceleration. As you\u2019ll see, using a GPU with PyTorch is super easy and super fast. If you do large computations, this is beneficial because it speeds things up a lot.\n\n![](https:\/\/tensorflownet.readthedocs.io\/en\/latest\/_static\/tensor-naming.png)\n\n## Why Numpy?\n\nNumpy is the most commonly used computing framework for linear algebra. A good use case of Numpy is quick experimentation and small projects because Numpy is a light weight framework compared to PyTorch.","8cac801a":"**Learning rate** gives the rate of speed where the gradient moves during gradient descent. Setting it too high would make your path instable, too low would make convergence slow. Put it to zero means your model isn't learning anything from the gradients.","be4f1727":"![image](https:\/\/analyticsindiamag.com\/wp-content\/uploads\/2020\/02\/Pytorch.png)","2133fdd0":"## Batch Multplication ","4b4ff8dc":"Compare both predictions and Target, there is little bit difference. Now try for More epoches!","fcbd3996":"To back the values of the diagonal elements ","efb119d3":"**torch.no_grad** :\nDisabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True.","95f7cb07":"### Changing the Shape and Size of the Tensor","3a562b04":"### Creating a diagonal Tensor","9178ab6b":"## dy\/da = 3*a**2  = 75\n## dy\/db = -2b = -12","150a0925":"The nn.functional package contains many useful loss funcations and several other utilities\n\nInstead of defining a loss function manually, we can use the built-in loss function mse_loss.","6162670c":"# Operations on Tensors \n\n.mul for item by item multiplication \n\n![](https:\/\/miro.medium.com\/max\/1400\/1*54rq3_-FZaJxKLdOYN8qjA.png)\n\nThere are 2 ways in which we can perform this operation ","627588c2":"#### **max and min will help you to get the max and min value from your tensors**","4df1d379":"It tells us the our model is off by this value we have to minimize this error.(Low error better the model)","c4d1eea4":"### Creating a Empty Tensor","70114a38":"## What is Pytorch\n\nPyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab","a97ffb89":"###  This Notebook will give you the basic idea about Pytorch and give you step by step instructions that how to implement some basic operation in tensors using Pytorch.\n\n#### This Notebook will cover the following topics:\n#### **1. What are tensors**\n#### **2. Difference between the basic arrays and tensors**\n#### **3. How to create Tensors**\n#### **4. Basic matrix operations on Tensors**\n#### **5. What is AutoGrad**\n#### **6. Creating a neural network from scratch using Autograd**\n#### **7. Linear Regression from Scratch**\n#### **8. What are Data Loaders and DataClass in Pytorch**\n#### **9. Performing Linear Regression using Pytorch Libraries**\n\n     NOTE: This Notebook is Just for giving you a basic Idea about Pytorch. Pytorch is very deep and I really Don't want this Notebook to be too lengthy so you have to explore rest of it on your own! but Don't worry this Notebook will give you a headstart and will explain you some basics that will be very helpfull in your future journey\n\n![](https:\/\/i.pinimg.com\/originals\/d5\/85\/e9\/d585e948d82cbae60bb087175b9f70e1.jpg)\n","357d99a3":"## nn.Linear\nNow we just have to provide number of features and no of targets. We don't have to calculate weights and bias manually nn.Linear will calculate that for us\n\n**Our data only have 1 feature and 1 output**","f5447507":"# Defining a Model","84c506bb":"### Creating tensor with specific datatype \n\nUsing linspace methods you create a tensor within a range with user defined steps","f5436709":"# AUTO GRAD\n\ntorch.autograd is PyTorch\u2019s automatic differentiation engine that powers neural network training\n\nTraining a Neural Networks(NN) happens in two steps:\n\nForward Propagation: In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n\nBackward Propagation: In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent.","8e0216f2":"## We create another tensor Y from a and b.","105230de":"When Data is too large In Pytorch we divide our data in different batches and because of working in different  batches there will be less load on memory.\n\n**NOTE: Here Our Data is not that much big but still for the demonstration purpose we will work with batches** \n\n#### So here is the one batch of data","9e4877c9":"## Loss Function","8a9e74e3":"## Resetting the gradient to zero \n\n.zero_() resets the gradient to zero ","ce09e3cf":"# FEW METHODS \n\nMethods in Tensorflow are very much similar to methods in Numpy","226f0cf1":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTyHkcnQcqBu-GYtVbFaocaV33_bW4EyLe7Jw&usqp=CAU)","a6427ea2":"# DAY 2","43a7edc8":"![image.png](attachment:b95df6d6-6fb7-45d9-8cb1-8912713a9c3b.png)","fc57d7ce":"#### batch means no of matrix in one tensor m,n,k are the dimension of each matrix","0c235507":"## **Matrix Addition**\n![](https:\/\/i1.faceprep.in\/Companies-1\/matrix-addition-in-python.png)\n\nyou can provide axis as parameter to perform sum column-wise and row-wise","d9c13d41":"# Creating Tensor","b798d25b":"Our data is small and right now we dont have any csv so I created my data in numpy. You can also create csv and import it here in Kaggle and using iloc methods of pandas you can get the same array ","a4ec3a25":"### **cat will help you cocatinate two tesnor together with axis parameter you can specify the orentation of concatinating** \n\n![](https:\/\/static.javatpoint.com\/tutorial\/numpy\/images\/numpy-concatenate.png)","5ec855aa":"We create two tensors a and b with requires_grad=True. This signals to autograd that every operation on them should be tracked.","fa85ef79":"# Tensor Operations (Depth)\n\nNow we know how to create a tensor and few methods associated with tensor, Now we will perform some operation on tensors. We already disscussed basic arthematic operations before but this time we will try to go in little bit in depth.","4f346385":"### Tensor Addition","b24d0e44":"w = Weights ||  b = Bias\n\nIn Linear Regression we find the value of bias and weights and using that values we predict new set of output  ","706430ec":"Creating new variable for storing results can take lot of space especially when we have tensors with high dimensions so instead of saving them in new variable we can save them in one of the operand using add_ method","57f62662":"### We reduce loss little bit ","e0f84431":"![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1628643\/2676548\/grad%20steps.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211004%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211004T190914Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=7655b98b732f2caafec13aa8a31ed206914d58bcf2a396698053a2507f13e0b83e395eb80ccdcd706ab8689f8dc6447abe65ebe3c480de65f3be75729273592c7488b012efdd503851425b3ec63e0c59145857157e9269bb7ce41c566ecd6e1f68d7d636e6f02d511f50fca35002e1951e88ea5d245844b264b14768135eb095d54c286d8648989c4398601723ede47da367c837d141a6eb36bb22a69f0eac7d5039995dd4efc0707500000fd8d958e16cf7a12bcc9dc47a27515e162f919b56ea99cce2dcf4a37936313d0d55b7827269c24159dd101316eb2621c0e046758a704aa3e60e650e7d325dbaeff3875a751b177b237dce38243ea1d77e6199cc44)"}}