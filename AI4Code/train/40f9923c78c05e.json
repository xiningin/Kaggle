{"cell_type":{"2999bd2f":"code","2c502303":"code","4b575673":"code","cd347321":"code","3f5b7a44":"code","040d9efe":"code","e0fda6cd":"code","5f90863e":"code","1041df83":"code","6ac15815":"code","362e3615":"code","71796e05":"code","1b23a53b":"code","20f9877d":"code","3f627901":"code","c2a66cb0":"code","f2f38716":"code","e5b3dc9e":"code","4890387f":"code","85258a47":"code","281b61f5":"code","500cccc3":"code","78a19c32":"code","983a7e53":"code","e34c4ec9":"code","5775f63e":"code","3a4eab7a":"code","be1e0ee8":"code","4a9d14b1":"code","a639b1da":"code","83480454":"code","8e0a3dbf":"code","b657ed29":"code","beff2603":"code","7f418e16":"code","00358b2e":"code","f1db1486":"code","d09c39f6":"code","05bda14d":"code","c46e4f88":"code","9c2a57b6":"code","3bac5459":"code","8517816d":"code","7113b8f6":"code","3b2c3a5c":"code","e308b62c":"code","e6d974ad":"code","a219dec9":"code","29e5d1bf":"code","578b0e6d":"code","f6d862ac":"code","ad62b231":"code","cc49764b":"code","559527bb":"code","530a0e96":"code","459b4f1f":"markdown","7226730d":"markdown","1064e689":"markdown","e08bd82e":"markdown","6cee8f58":"markdown","c8597439":"markdown","a188b16b":"markdown","ff9e4bfb":"markdown","5ac08778":"markdown","0ad6e2a7":"markdown","454b1075":"markdown","7d88a69e":"markdown","e81ade00":"markdown"},"source":{"2999bd2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom plotly import tools\nimport plotly as py\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, mean_squared_error, roc_auc_score\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler,  OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\n\nimport keras\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, MaxPooling1D , GlobalMaxPool1D , GlobalMaxPooling1D , GlobalAveragePooling1D , MaxPooling1D\nfrom keras import backend as K\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils.np_utils import to_categorical \nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nimport math\nimport itertools\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2c502303":"crime = pd.read_csv('\/kaggle\/input\/crimes-in-boston\/crime.csv' , encoding='latin-1')\ncode = pd.read_csv('\/kaggle\/input\/crimes-in-boston\/offense_codes.csv' , encoding='latin-1')\ndata = pd.read_csv('\/kaggle\/input\/crimes-in-boston\/crime.csv' , encoding='latin-1')\ncrime.info()\ncode.info()","4b575673":"crime.head()","cd347321":"code.head()","3f5b7a44":"crime = crime.drop(['INCIDENT_NUMBER' ,  'REPORTING_AREA' , 'OFFENSE_DESCRIPTION' , 'OCCURRED_ON_DATE' , 'UCR_PART' ,'Location'] , axis=1)\ncrime.head()","040d9efe":"crime.isnull().sum()","e0fda6cd":"code.isnull().sum()","5f90863e":"crime.SHOOTING = [1 if each == 'Y' else 0 for each in crime.SHOOTING]","1041df83":"crime = crime.dropna(how=\"any\")\ncrime.isnull().sum()","6ac15815":"crime.head(19)","362e3615":"le = LabelEncoder()\ncrime['DAY_OF_WEEK'] = le.fit_transform(crime['DAY_OF_WEEK'] ) \ncrime['STREET'] = le.fit_transform(crime['STREET'])\ncrime['OFFENSE_CODE_GROUP'] = le.fit_transform(crime['OFFENSE_CODE_GROUP'])\ncrime['DISTRICT'] = le.fit_transform(crime['DISTRICT'])\ncrime.head()","71796e05":"def off_code_size(df):\n    codes = pd.unique(df.OFFENSE_CODE)\n    size = []\n    for i in codes:\n        size.append(len(df[df['OFFENSE_CODE'] == i]))\n    return size\n\ndef code_name(df):\n    codes = code.values\n    names = []\n    off_codes = pd.unique(df.OFFENSE_CODE)\n    iterr = 0\n    for i in off_codes:\n        if(i in codes):\n            iterr += 1\n            names.append(codes[iterr][1])\n    return names","1b23a53b":"from wordcloud import WordCloud\nplt.figure(figsize=(25,15))\nwordcloud = WordCloud(\n                          background_color='black',\n                          width=1920,\n                          height=1080\n                         ).generate(\" \".join(data.OFFENSE_CODE_GROUP))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.savefig('graph.png')\nplt.show()","20f9877d":"f,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(crime.corr(), annot=True, linewidths=.5, fmt= '.3f',ax=ax)\nplt.savefig(\"corr.png\")\nplt.show()","3f627901":"crime_2015 = crime[crime.YEAR == 2015] \ncrime_2016 = crime[crime.YEAR == 2016]\ncrime_2017 = crime[crime.YEAR == 2017]\ncrime_2018 = crime[crime.YEAR == 2018]","c2a66cb0":"fig, ax1 = plt.subplots(2, 2, figsize= (140, 70) )\n\nax1[0,0].bar(code_name(crime_2015) , off_code_size(crime_2015))\nax1[0,0].tick_params(labelrotation=90)\nax1[0,1].bar(code_name(crime_2016) , off_code_size(crime_2016))  \nax1[0,1].tick_params(labelrotation=90)\nax1[1,0].bar(code_name(crime_2017) , off_code_size(crime_2017)) \nax1[1,0].tick_params(labelrotation=90)\nax1[1,1].bar(code_name(crime_2018) , off_code_size(crime_2018)) \nax1[1,1].tick_params(labelrotation=90)\n\nax1[0,0].grid()\nax1[0,1].grid()\nax1[1,0].grid()\nax1[1,1].grid()\n\nplt.show()","f2f38716":"sns.countplot(data=crime, x='DISTRICT')","e5b3dc9e":"sns.countplot(data=crime, x='MONTH')","4890387f":"sns.countplot(data=crime, x='YEAR')","85258a47":"sns.countplot(data=crime, x='HOUR')","281b61f5":"trace1 = go.Scatter3d(\n    x=crime_2018.OFFENSE_CODE,\n    y=off_code_size(crime_2018),\n    z=crime_2018.DISTRICT,\n    name='2018',\n    mode='markers',\n    marker=dict(\n        size=12,\n        line=dict(\n            color='rgb(120,120,120)',\n            width=0.5\n        ),\n        opacity=0.7\n    )\n)\n\ntrace2 = go.Scatter3d(\n    x=crime_2017.OFFENSE_CODE,\n    y=off_code_size(crime_2017),\n    z=crime_2017.DISTRICT,\n    name='2017',\n    mode='markers',\n    marker=dict(\n        size=12,\n        symbol='circle',\n        line=dict(\n            color='rgb(160,160,160)',\n            width=0.5\n        ),\n        opacity=0.7\n    )\n)\ntrace3 = go.Scatter3d(\n    x=crime_2016.OFFENSE_CODE,\n    y=off_code_size(crime_2016),\n    z=crime_2016.DISTRICT,\n    name='2016',\n    mode='markers',\n    marker=dict(\n        size=12,\n        line=dict(\n            color='rgb(200,220,220)',\n            width=0.5\n        ),\n        opacity=0.7\n    )\n)\n\ntrace4 = go.Scatter3d(\n    x=crime_2015.OFFENSE_CODE,\n    y=off_code_size(crime_2015),\n    z=crime_2015.DISTRICT,\n    name='2015',\n    mode='markers',\n    marker=dict(\n        color='rgb(127, 127, 127)',\n        size=12,\n        symbol='circle',\n        line=dict(\n            width=0.5\n        ),\n        opacity=0.7\n       \n    )\n)\ndata = [trace1, trace2 , trace3 , trace4]\nlayout = go.Layout(\n    \n    margin=dict(\n        l=100,\n        r=100,\n        b=100,\n        t=100\n    )\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='simple-3d-scatter')","500cccc3":"labels_dis = le.inverse_transform(crime['DISTRICT'])\nld = sorted(pd.unique(labels_dis))\nprint(ld)","78a19c32":"X = crime.drop(['DISTRICT'] , axis=1)\nY = crime.DISTRICT\n\nx_train , x_test , y_train , y_test = train_test_split(X,Y,test_size=0.33, random_state=0)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test = sc.fit_transform(x_test)\n","983a7e53":"knn = KNeighborsClassifier(n_neighbors=77 , metric='minkowski')\nknn.fit(X_train , y_train)\ny_pred_knn = knn.predict(X_test)\n\ncm_knn = confusion_matrix(y_test , y_pred_knn)\nprint('KNN')\nprint(cm_knn)","e34c4ec9":"f,ax = plt.subplots(figsize=(18, 18))\nax = sns.heatmap(cm_knn, xticklabels=ld, yticklabels=ld, linewidths=.5 , cbar=False , fmt=\"d\" , annot=True)\nplt.savefig(\"knn.png\")\nplt.show()","5775f63e":"label_error = 1 - np.diag(cm_knn) \/ np.sum(cm_knn, axis=1)\nplt.figure(figsize=(25,10))\nplt.bar(ld,label_error)\nplt.xlabel('True Label')\nplt.ylabel('Classified incorrectly')\nplt.savefig(\"knn_bar.png\")\nplt.show","3a4eab7a":"print(classification_report(y_test, y_pred_knn))","be1e0ee8":"dtc = DecisionTreeClassifier(criterion='entropy')\ndtc.fit(X_train , y_train)\ny_pred_dtc = dtc.predict(X_test)\n\ncm_dtc = confusion_matrix(y_test,y_pred_dtc)\nprint('DTC')\nprint(cm_dtc)","4a9d14b1":"f,ax = plt.subplots(figsize=(18, 18))\nax = sns.heatmap(cm_dtc, xticklabels=ld, yticklabels=ld, linewidths=.5 , cbar=False , fmt=\"d\" , annot=True)\nplt.savefig(\"dtc.png\")\nplt.show()","a639b1da":"label_error = 1 - np.diag(cm_dtc) \/ np.sum(cm_dtc, axis=1)\nplt.figure(figsize=(25,10))\nplt.bar(ld,label_error)\nplt.xlabel('True Label')\nplt.ylabel('Classified incorrectly')\nplt.savefig(\"dtc_bar.png\")\nplt.show()","83480454":"print(classification_report(y_test, y_pred_dtc))","8e0a3dbf":"rfc = RandomForestClassifier(criterion='entropy' , n_estimators=33)\nrfc.fit(X_train , y_train)\ny_pred_rfc = rfc.predict(X_test)\n\ncm_rfc = confusion_matrix(y_test,y_pred_rfc)\nprint('RFC')\nprint(cm_rfc)","b657ed29":"f,ax = plt.subplots(figsize=(18, 18))\nax = sns.heatmap(cm_rfc, xticklabels=ld, yticklabels=ld, linewidths=.5 , cbar=False , fmt=\"d\" , annot=True)\nplt.savefig(\"rfc.png\")\nplt.show()","beff2603":"label_error = 1 - np.diag(cm_rfc) \/ np.sum(cm_rfc, axis=1)\nplt.figure(figsize=(25,10))\nplt.bar(ld,label_error)\nplt.xlabel('True Label')\nplt.ylabel('Classified incorrectly')\nplt.savefig(\"rfc_bar.png\")\nplt.show()","7f418e16":"print(classification_report(y_test, y_pred_rfc))","00358b2e":"gnb = GaussianNB()\ngnb.fit(X_train , y_train)\ny_pred_gnb = gnb.predict(X_test)\n\ncm_gnb = confusion_matrix(y_test,y_pred_gnb)\nprint('GNB')\nprint(cm_gnb)","f1db1486":"f,ax = plt.subplots(figsize=(18, 18))\nax = sns.heatmap(cm_gnb, xticklabels=ld, yticklabels=ld, linewidths=.5 , cbar=False , fmt=\"d\" , annot=True)\nplt.savefig(\"gbn.png\")\nplt.show()","d09c39f6":"label_error = 1 - np.diag(cm_gnb) \/ np.sum(cm_gnb, axis=1)\nplt.figure(figsize=(25,10))\nplt.bar(ld,label_error)\nplt.xlabel('True Label')\nplt.ylabel('Classified incorrectly')\nplt.savefig(\"gbn_bar.png\")\nplt.show()","05bda14d":"print(classification_report(y_test, y_pred_gnb))","c46e4f88":"X = crime.drop(['DISTRICT'] , axis=1)\nY = crime['DISTRICT']\n\nx_orjinal_train , x_orjinal_test, y_orjinal_train, y_orjinal_test = train_test_split(X, Y, test_size=0.33,random_state=21)\n\ny_train = to_categorical(y_orjinal_train, num_classes = 12)\ny_test = to_categorical(y_orjinal_test, num_classes = 12)\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x_orjinal_train)\nx_train = pd.DataFrame(x_scaled)\nx_scaled1 = min_max_scaler.fit_transform(x_orjinal_test)\nx_test = pd.DataFrame(x_scaled1)\n","9c2a57b6":"x_train = x_train.values\nx_test = x_test.values","3bac5459":"x_train = np.asarray(x_train)\nx_test = np.asarray(x_test)\n\nx_train_mean = np.mean(x_train)\nx_train_std = np.std(x_train)\n\nx_test_mean = np.mean(x_test)\nx_test_std = np.std(x_test)\n\nx_train = (x_train - x_train_mean)\/x_train_std\nx_test = (x_test - x_test_mean)\/x_test_std","8517816d":"x_train.shape","7113b8f6":"x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\nx_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))","3b2c3a5c":"x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size = 0.33, random_state = 21)\n","e308b62c":"x_train.shape","e6d974ad":"type(x_train)\nprint(x_train.shape)","a219dec9":"from keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation, Dense\ndef swish(x):\n    return (K.sigmoid(x) * x)\n\nget_custom_objects().update({'swish': Activation(swish)})","29e5d1bf":"get_custom_objects().update({'swish': Activation(swish )})\nclassifier = Sequential()\nclassifier.add(Conv1D(110, 10, activation= 'swish', padding= 'Same', input_shape = (1, 10)))\nclassifier.add(Conv1D(110, 10, activation= 'swish', padding= 'Same'))\nclassifier.add(MaxPooling1D(1))\nclassifier.add(Dropout(0.25))\nclassifier.add(Conv1D(130, 10, activation= 'swish', padding= 'Same'))\nclassifier.add(Conv1D(130, 10, activation= 'swish', padding= 'Same'))\nclassifier.add(MaxPooling1D(1))\nclassifier.add(Dropout(0.25))\nclassifier.add(Conv1D(150, 10, activation= 'swish', padding= 'Same'))\nclassifier.add(Conv1D(150, 10, activation= 'swish', padding= 'Same'))\nclassifier.add(GlobalAveragePooling1D())\nclassifier.add(Dropout(0.35))\n\nclassifier.add(Dense(12, activation= 'softmax'))\nclassifier.summary()\n\nclassifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n\nepochs = 145\nbatch_size = 2000\nhistory = classifier.fit(x_train , y_train , verbose=1 , batch_size=batch_size , epochs=epochs ,validation_data=(x_test, y_test) )","578b0e6d":"from keras.utils import plot_model\nplot_model(classifier)","f6d862ac":"loss, accuracy = classifier.evaluate(x_test, y_test, verbose=1)\nloss_v, accuracy_v = classifier.evaluate(x_validate, y_validate, verbose=1)\nprint(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\nprint(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n","ad62b231":"fig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"acc.png\")\nplt.show()\n\nfig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"loss.png\")\nplt.show()\n\n","cc49764b":"Y_pred = classifier.predict(x_validate)\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(y_validate,axis = 1) \ncm = confusion_matrix(Y_true, Y_pred_classes)\nprint(cm)","559527bb":"f,ax = plt.subplots(figsize=(18, 18))\nax = sns.heatmap(cm, xticklabels=ld, yticklabels=ld, linewidths=.5 , cbar=False , fmt=\"d\" , annot=True)\nplt.savefig(\"corr_cnn.png\")\nplt.show()","530a0e96":"label_error = 1 - np.diag(cm) \/ np.sum(cm, axis=1)\nplt.figure(figsize=(25,10))\nplt.bar(ld,label_error)\nplt.xlabel('True Label')\nplt.ylabel('Classified incorrectly')\nplt.savefig(\"cnn_bar.png\")","459b4f1f":"# Classifications","7226730d":"# Data Visualizations","1064e689":"--------------","e08bd82e":"---------------","6cee8f58":"--------------------------","c8597439":"# Introduction\n\nHello, welcome to our kernel. This is our first data mining project. \nOur aim in this project is to use machine learning and deep learning algorithms and compare their results.\n\nThe algorithms we use for machine learning\n    \n    K Nearest Neighborhood\n    Decision Tree Classifier\n    Random Forest Classifier\n    Naive Bayes\n\n\nThe algorithms we use for deep learning\n     \n     Convolutional Neural Network\n","a188b16b":"---------------------","ff9e4bfb":"--------------------------------------","5ac08778":"# Conclusion\n\nAs a result of our study, we have not achieved very successful results. As for the comparison of algorithms, we obtained high accuracy in the decision tree and random forest algorithms, but very low accuracy values in the results of deep learning and other classification algorithms. This may be because the dataset we selected does not have sufficient properties.","0ad6e2a7":"# Data Read","454b1075":"# Data Preprocessing","7d88a69e":"-----------------","e81ade00":"# CNN"}}