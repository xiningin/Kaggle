{"cell_type":{"42af1c77":"code","d26d37a1":"code","1c8c564c":"code","2d68a2da":"code","43458d50":"code","ebcb5a39":"code","58f776a7":"code","95a4bac7":"code","d8ef41d0":"code","1efa38c3":"code","d4cbedc4":"code","4f29cec1":"code","28174d83":"code","e7da3509":"code","ba3d360c":"code","903d3f4c":"code","78d8a7ee":"code","601b8652":"code","e1c7d69c":"code","6125997e":"code","f8160326":"code","8ef64a47":"code","ba9782be":"code","863b16e9":"code","a8167566":"code","c4d66858":"code","92f60eff":"code","f97a9455":"code","6a79de3b":"code","35ee0b40":"code","9264bf8a":"code","9c493e04":"code","03f47726":"code","e6dab5b0":"code","45768d76":"code","a78c617a":"code","2f814ce3":"code","10e38bf3":"code","8fa7f213":"code","4351c4e5":"code","15845094":"code","7edb4e74":"markdown","2829f7f4":"markdown","db90b908":"markdown","b8749515":"markdown","582adabf":"markdown","779517bf":"markdown","90d9fd33":"markdown","c55aaf00":"markdown","435ff598":"markdown","5064b8b7":"markdown","0ec07248":"markdown","7dd79f10":"markdown","c2932e23":"markdown","b2f8177a":"markdown","82fda379":"markdown"},"source":{"42af1c77":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d26d37a1":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nmissing_values = [\"n\/a\", \"na\", \"--\",\"-\"]\nnewsdat=pd.read_csv(\"\/kaggle\/input\/online-news-popularity\/OnlineNewsPopularity.csv\",na_values = missing_values)","1c8c564c":"newsdat.shape","2d68a2da":"newsdat.head(5)","43458d50":"lifestyledf=newsdat[newsdat[' data_channel_is_lifestyle']==1]","ebcb5a39":"lifestyledf['dat_ch']='lifestyle'","58f776a7":"lifestyledf.shape","95a4bac7":"entertainmentdf=newsdat[newsdat[' data_channel_is_entertainment']==1]","d8ef41d0":"entertainmentdf['dat_ch']='entertainment'","1efa38c3":"entertainmentdf.shape","d4cbedc4":"busdf=newsdat[newsdat[' data_channel_is_bus']==1]","4f29cec1":"busdf['dat_ch']='bus'","28174d83":"busdf.shape","e7da3509":"socmeddf=newsdat[newsdat[' data_channel_is_socmed']==1]","ba3d360c":"socmeddf['dat_ch']='socmed'","903d3f4c":"socmeddf.shape","78d8a7ee":"techdf=newsdat[newsdat[' data_channel_is_tech']==1]","601b8652":"techdf['dat_ch']='tech'","e1c7d69c":"techdf.shape","6125997e":"worlddf=newsdat[newsdat[' data_channel_is_world']==1]","f8160326":"worlddf['dat_ch']='world'","8ef64a47":"worlddf.shape","ba9782be":"nonclasdf=newsdat[(newsdat[' data_channel_is_world']==0) & (newsdat[' data_channel_is_tech']==0) & (newsdat[' data_channel_is_socmed']==0) & (newsdat[' data_channel_is_bus']==0) & (newsdat[' data_channel_is_lifestyle']==0) & (newsdat[' data_channel_is_entertainment']==0)]","863b16e9":"nonclasdf['dat_ch']='other'","a8167566":"nonclasdf.shape","c4d66858":"frames=[nonclasdf,worlddf,techdf,socmeddf,busdf,lifestyledf,entertainmentdf]\nnewsdatcr=pd.concat(frames,ignore_index=True)\nnewsdatcr=newsdatcr.drop(['url',' data_channel_is_lifestyle',' data_channel_is_entertainment',' data_channel_is_bus',' data_channel_is_socmed',' data_channel_is_tech',' data_channel_is_world'],axis=1)","92f60eff":"plt.figure(figsize=(40,30))\ncor = newsdatcr.corr(method ='pearson')\nsns.heatmap(cor, cmap=\"RdYlGn\")\nplt.show()","f97a9455":"newsdatcr=newsdatcr.drop([' n_non_stop_words',' n_unique_tokens',' kw_avg_min',' kw_avg_avg',' self_reference_avg_sharess'],axis=1)","6a79de3b":"newsdatcr.to_csv(r'Updatednewspopularity.csv')\nX1= newsdatcr.iloc[:,0:49]\ny1= newsdatcr.iloc[:,49]\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX1 = sc.fit_transform(X1)\nY1=y1.values\n","35ee0b40":"encoder = LabelEncoder()","9264bf8a":"newsdatcr.isnull().any()","9c493e04":"encoder.fit(Y1)","03f47726":"encoded_Y1 = encoder.transform(Y1)\ntransf_y1 = np_utils.to_categorical(encoded_Y1)\ntransf_y1","e6dab5b0":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1, transf_y1, test_size=0.3)","45768d76":"model = Sequential()\nmodel.add(Dense(800, input_dim=49, activation='relu'))\nmodel.add(Dense(600, activation='relu'))\nmodel.add(Dense(400, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(7, activation='softmax'))","a78c617a":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","2f814ce3":"model.fit(X_train,y_train,epochs=80, batch_size=32, verbose=1)","10e38bf3":"eval_model=model.evaluate(X_train, y_train)\neval_model","8fa7f213":"y_pred=model.predict(X_test)\n#Converting predictions to label\npred = list()\nfor i in range(len(y_pred)):\n    pred.append(np.argmax(y_pred[i]))\n#Converting one hot encoded test label to label\ntest = list()\nfor i in range(len(y_test)):\n    test.append(np.argmax(y_test[i]))","4351c4e5":"from sklearn.metrics import accuracy_score\na = accuracy_score(pred,test)\nprint('Accuracy is:', a*100)","15845094":"from keras.utils import plot_model\nplot_model(model, to_file='model.png')","7edb4e74":"Token length Vs Shares:\n\n ![image.jpeg](attachment:image.jpeg)\nPlotted this graph is mainly to analyze at what token length or at what length of articles the people are liking the articles and shared most. By observing the above graph we can actually say that people are reading the short articles most and shared a lot and the other one are very length article who has more 2000 tokens has been read and shared a lot. By this we can understood that users are of two types who are concentrating on the content and the other are who are just observing what their need is presented in article in a straight-forward manner.\n","2829f7f4":"#Objective\n\u2022\tClassifying the articles into different categories?\n\u2022\tWhich category of article should be published maximum for higher number of shares?\n\u2022\tOn What week-day What type of article should Mashable post more?\n\u2022\tFor different categories of articles what should be their min and max content length?\n","db90b908":"#Feature Engineering:\nStandardization has been performed so that all attributes have a mean 0 and deviation 1.","b8749515":"#Neural Network model:","582adabf":"*  By the below figure Mashable can actually interpret that it needs to publish very less articles on Lifestyle and publish more on social media and technology articles\n![image.jpeg](attachment:image.jpeg)","779517bf":"By the above Pearson correlation graph we can say that number of unique words and number of non-stop-words and number of non-stop-unique tokens are strongly correlated which implies that they are strongly linearly dependent on each other. Same as the above case Kw-avg- min and kw-max-min are also strongly corelated.\n","90d9fd33":"#Pearson Coorelation Analysis:","c55aaf00":"Tokens Vs Images:\n\n ![image.jpeg](attachment:image.jpeg)\nFrom the above Analysis we are able to Analyze that for what token lengths the number of images should be more. By observing the above graph we can say that till min of 2000 tokens or words we need to have minimum of 30 images and gradually the increase in tokens we need to increase in images so that people who are reading these articles have more understanding in a visualizing way.\n","435ff598":"#Data PreProcessing:\nwe need to classify the articles into different categories we have choosed neural networks and  need to change the every different categories into one category and needed to perform one hot encoded which will finally placed into a vector of one column.\nThen we performed cross-tabulation of every category in our data set and found out the distribution of categories in our data set. The below table shows the number of articles placed in each category:\n\nCategory\tNumber of articles in that category\nWorld\t        8427\ntechnology\t    7346\nSocial media\t2323\nEntertainment\t7057\nLifestyle\t    2099\nBusiness\t    6258\n\nAs we can observe the data classification if categories with Social media and life style has less variety of assumptions. We are dropping time delta and Url which is metadata of articles which won\u2019t be any use in our regression model.\n\n","5064b8b7":"#Classification:","0ec07248":"#Exploratory Analysis:","7dd79f10":"* By the below plot we can actually decide for maximum popularity of news what should be the minimum token lengths at each sector . Because many of users doesn\u2019t like to read long articles and in some cases they like to if we observe in other category the users like to read short articles as we go for technology there averagely people are okay to read even long articles. This helps the Mashable Organization to decide for which sectors what should be their max and min token lengths for more shares or more popularity\n![image.jpeg](attachment:image.jpeg)","c2932e23":"#Feature Selection\n\nBy the above correlation graph we can clearly say that these features:\n\u2022\tnumber of unique words and number of non-stop-words and number of non-stop-unique tokens\n\u2022\tKw-avg- min and kw-max-min\nThese are strongly correlated and linearly dependent which makes us to assume that these features are so linearly dependent that any one of the strong correlated feature can be used and excluding the other features won\u2019t affect the model and will be indirectly helpful in our model by not allowing to do overfitting.\n","b2f8177a":"Density Distribution of Shares:\n ![image.jpeg](attachment:image.jpeg)\nThe above graph represents the density distribution of shares in our whole dataset. As we can see we have maximum examples of shares between 0-10,000 shares and gradually decreased with the number of shares increasing. We can actually consider that the shares which are above 20,000 as outliers but in our project these are considered as anomalies. As our project says about the maximum of shares of an article if the article has been shared the most of the time then the features of article has been so peculiar that it has reached maximum of shares. So considering anomality condition we are not classifying these peculiar data  as outliers but as anamolies.\n","82fda379":"#Interpreting the Results:\n\n* helps the digital platforms organizations to categorize their articles when they extracted it from different sources and have no clue, in which sector does this article belong, So by our model it will be very helpful for them to categorize these articles saving around a lot of manual labor time which can be put for more efficient purposes."}}