{"cell_type":{"c29907db":"code","68bb755b":"code","6ec398f5":"code","fd45b021":"code","8653b061":"code","b60bd90d":"code","5d376551":"code","886673e5":"code","6c4cb38b":"code","1517c02a":"code","c5ea5530":"code","dc44f8fa":"code","2340d12b":"code","8fefde75":"code","e8ba0dee":"code","590d7610":"code","ce028b19":"code","04e6b478":"code","d01e05de":"code","6c7c04b0":"code","a9efb20d":"code","bd9b0bac":"code","dd45827d":"code","6ffcf6a7":"code","471937f4":"code","8461f780":"code","49d12004":"markdown","dd44ebe9":"markdown","1e12da53":"markdown","c438b3e1":"markdown","9b9ff62a":"markdown","39ff5f66":"markdown","7b6db9cf":"markdown","0399cbc2":"markdown","59c19d8e":"markdown","4a5d6883":"markdown","9728e7ec":"markdown","04590001":"markdown","e09c282e":"markdown","8b966757":"markdown","c34b3b79":"markdown","ea503daa":"markdown","74439dff":"markdown","14d754f2":"markdown","88f9a667":"markdown","a351f261":"markdown"},"source":{"c29907db":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","68bb755b":"dfm = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\ndfm","6ec398f5":"#fill the blanks with a placehonder character\ndfm = dfm.fillna(\"x\")","fd45b021":"df2 = dfm[['title', 'abstract']]\n#make sure both coulms are strings\ndf2.title = df2.title.astype(str)\ndf2.abstract = df2.abstract.astype(str)\ndf2","8653b061":"import csv\ndf2.to_csv('metadata_out.csv', index=False, quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)","b60bd90d":"DATA_DIR = \"\/kaggle\/input\/cord19-elasticbert-query-results\/\"","5d376551":"vdf = pd.read_csv(DATA_DIR+\"BERT_vaccines.csv\")\ntdf = pd.read_csv(DATA_DIR+\"BERT_therapeutics.csv\")\nvdf2 = pd.read_csv(DATA_DIR+\"BERT_coronavirus vaccine.csv\")\ntdf2 = pd.read_csv(DATA_DIR+\"BERT_coronavirus therapeutics.csv\")","886673e5":"def cleanup(df):\n  df[['abstract','title']] = df._source.str.split(\"'title':\",expand=True)\n  df[\"title\"] = df.title.str[2:-2]\n  df[\"abstract\"] = df.abstract.str[14:-3]","6c4cb38b":"cleanup(vdf)\ncleanup(vdf2)\ncleanup(tdf)\ncleanup(tdf2)\nvdf","1517c02a":"metadf_title= dfm[[\"title\"]]\n#metadf_title = metadf_title.drop_duplicates()\nmetadf_title","c5ea5530":"\nmerged_vdf = vdf.merge(vdf2, on=['title'], \n                   how='inner', indicator=True, suffixes=('', '_y'))\nmerged_vdf[\"score\"] = merged_vdf[[\"_score\", \"_score_y\"]].values.max(1)\nmerged_vdf.drop(list(merged_vdf.filter(regex='_y$')), axis=1, inplace=True)\nmerged_vdf = merged_vdf[[\"title\", \"score\"]]\nmerged_vdf.drop_duplicates(inplace=True)\nmerged_vdf","dc44f8fa":"vdf_concat = pd.concat([vdf, vdf2])\n#vdf_concat = vdf_concat[[\"title\"]]\nvdf_concat.drop_duplicates(subset=[\"title\"], inplace=True)\nvdf_concat[\"score\"] = vdf_concat[\"_score\"]\nvdf_concat = vdf_concat[[\"title\", \"score\"]]\nvdf_concat","2340d12b":"vdf_concat2 = vdf_concat.merge(merged_vdf, on =[\"title\"], how = 'outer' ,indicator=True).loc[lambda x : x['_merge']=='left_only']\nvdf_concat2[\"score\"] = vdf_concat2[\"score_x\"]\nvdf_concat2 = vdf_concat2[[\"title\", \"score\"]]\nvdf_concat2","8fefde75":"vdf_final = pd.concat([vdf_concat2, merged_vdf])\nvdf_final.drop_duplicates(subset=[\"title\"],inplace=True)\nvdf_final","e8ba0dee":"merged_tdf = tdf.merge(tdf2, on=['title'], \n                   how='inner', indicator=True, suffixes=('', '_y'))\nmerged_tdf[\"score\"] = merged_tdf[[\"_score\", \"_score_y\"]].values.max(1)\nmerged_tdf.drop(list(merged_tdf.filter(regex='_y$')), axis=1, inplace=True)\nmerged_tdf = merged_tdf[[\"title\", \"score\"]]\nmerged_tdf.drop_duplicates(inplace=True)\nmerged_tdf","590d7610":"tdf_concat = pd.concat([tdf, tdf2])\n#tdf_concat = tdf_concat[[\"title\"]]\ntdf_concat.drop_duplicates(subset=[\"title\"], inplace=True)\ntdf_concat[\"score\"] = tdf_concat[\"_score\"]\ntdf_concat = tdf_concat[[\"title\", \"score\"]]\ntdf_concat","ce028b19":"tdf_concat2 = tdf_concat.merge(merged_tdf, on =[\"title\"], how = 'outer' ,indicator=True).loc[lambda x : x['_merge']=='left_only']\ntdf_concat2[\"score\"] = tdf_concat2[\"score_x\"]\ntdf_concat2 = tdf_concat2[[\"title\", \"score\"]]\ntdf_concat2","04e6b478":"tdf_final = pd.concat([tdf_concat2, merged_tdf])\ntdf_final.drop_duplicates(subset=[\"title\"],inplace=True)\ntdf_final","d01e05de":"metadf_final = metadf_title\nmetadf_final[\"class\"] = 0\nmetadf_final","6c7c04b0":"#the common ones between VACNNIES AND THERPEUTICS\nmerged_all = tdf_final.merge(vdf_final, on=['title'], \n                   how='inner', indicator=True, suffixes=('', '_y'))\nmerged_all.drop(list(merged_all.filter(regex='_y$')), axis=1, inplace=True)\n#merged_all.drop_duplicates(inplace=True)\nmerged_all","a9efb20d":"x = y = 0\nfor i, row in metadf_final.iterrows():\n  if (row[\"title\"] in vdf_final.values) and (row[\"title\"] in tdf_final.values):\n    #vdf_final.loc[df['title'] == row[\"title\"]]\n    vi = vdf_final.index[vdf_final['title'] == row[\"title\"]].tolist()[0]\n    ti = tdf_final.index[tdf_final['title'] == row[\"title\"]].tolist()[0]\n    #print(vdf_final.iloc[vi].score, tdf_final.iloc[ti].score)\n    if vdf_final.iloc[vi].score >= tdf_final.iloc[ti].score:\n      metadf_final.loc[i,'class'] = 1\n      x = x+1\n    else:\n      metadf_final.loc[i,'class'] = 2\n      y = y+1\n  elif row[\"title\"] in vdf_final.values:\n    metadf_final.loc[i,'class'] = 1\n  elif row[\"title\"] in tdf_final.values:\n    metadf_final.loc[i,'class'] = 2","bd9b0bac":"print(\"common articles split into virus and therapeutics respectively\")\nprint(x,y)","dd45827d":"metadf_final","6ffcf6a7":"#rename column to support query syntax\nmetadf_final = metadf_final.rename(columns={\"class\": \"classif\"}, errors=\"raise\")","471937f4":"metadf_final.query(\"`classif` == 1\")","8461f780":"metadf_final.query(\"`classif` == 2\")","49d12004":"OUtput the final file to be used for elasticBERT","dd44ebe9":"# Introduction\nThis repository deals with the \"cord19-vaccines-and-therapeutics\" dataset which is based on the [\"What do we know about vaccines and therapeutics?\"](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks?taskId=561) task of the COVID-19 Open Research Dataset Challenge (CORD-19).","1e12da53":"There is an overlap of articles between the \"vaccines\" and \"therapeutics\", our current classificiation ensemble bythe whole team does not cater to 2 samples having the same class (the \"both\" class) yet, so we split the 1268 samples into the 2 classes based on the maximum cosine similarity score.","c438b3e1":"get the titles from the metadata file","9b9ff62a":"merge the 2 tdf's and take max score","39ff5f66":"merge the 2 vdf's and take max score","7b6db9cf":"The output of elasticBERT needs to be cleaned up and the title and abstract need to be separated","0399cbc2":"The final output:","59c19d8e":"(optinal) query the metadata fot see the final classes****","4a5d6883":"# Part 3: converting elasticBERT scores to classes","9728e7ec":"### part 3-c: For therpeutics class","04590001":"# Using Novel Language Models and elasticsearch to Effectively Identify Articles related to Therapeutics and Vaccines\n * Team: MD-Lab, ASU\n * Author: Jitesh Pabla, Email: jpabla1@asu.edu, Kaggle ID: jiteshpabla\n * Team Members: Rishab Banerjee, Hong Guan, Ashwin Karthik Ambalavanan, Mihir Parmar, Murthy Devarakonda\n * Email ID: loccapollo@gmail.com, hguan6@asu.edu, aambalav@asu.edu, mparmar3@asu.edu, Murthy.Devarakonda@asu.edu\n * Kaggle ID: loccapollo, hongguan, ashwinambal96, mihir3031, murthydevarakonda\n * This is a Team Submission\n * Here are the links to our teams Kernels:\n     - https:\/\/www.kaggle.com\/jiteshpabla\/scoring-cord-19-using-google-training-on-scibert\/\n     - https:\/\/www.kaggle.com\/ashwinambal96\/scibert-based-article-identification\n     - https:\/\/www.kaggle.com\/hongguan\/micro-scorers-for-covid-19-open-challenge\/\n     - https:\/\/www.kaggle.com\/mihir3031\/bert-sts-for-searching-relevant-research-papers\n     - https:\/\/www.kaggle.com\/loccapollo\/lexicon-based-similarity-scoring-with-bert-biobert\n     - The final ensembling that combines everything together: http:\/\/https:\/\/www.kaggle.com\/hongguan\/ensemble-model-for-covid-19-open-challenge\/\n ","e09c282e":"# Part 1: prepare a csv for elasticBERT","8b966757":"### part 3-d: Merging the 2 extracted classes with metadata","c34b3b79":"### part 3-b: For vaccine class","ea503daa":"### part 3-a: loading and cleanup","74439dff":"The goal is to find the most relevant articles related to \"vaccines\" and \"therapeutics\". So In the elasticBERT code, the queries I used are:\n* vaccines\n* coronavirus vaccine\n* coronavirus therapeutics\n* therapeutics\n\nThe first 1000 relevant articles are taken from each query.","14d754f2":"ElasticBERT is basically uses any BERT model (here, BERT-cased-768 is used) to generate an output vector for each title+abstract combination for each sample in the metadata file.\n\nThen, it uses the same BERT model to generate a vector for the query and then calculates the cosine similarity between the query vector and the title+abstract vector for each sample to get the most relevant articles\/samples.\n\nThis method helps run queries extremely fast and its main advantage is running many different queries to answer different questions (like the many questions in all of the CORD-19 tasks) is a streamlined fashion.\nBut currently, the focus is only on a few queries (as a part of only one CORD-19 task).","88f9a667":"Take only the title and abstract (only these 2 will be used for elasticBERT for now. A combination of title+journal and only abstacts were tried but by a manual analysis, title+abstract seems to work the best.","a351f261":"# Part 2: elasticBERT\n\nThe code for elasticBERT and the instructions for running it can be found here: [repository for the full team](https:\/\/github.com\/md-labs\/covid19-kaggle)\nor you could easily clone it form [elasticBERT standalone repository](https:\/\/github.com\/jiteshpabla\/elasticbert) and run it locally with the instructions given.\n\nThe code is based on docker and takes significant time to run, hence it has not been put into the kernel yet.\n\n\n\n"}}