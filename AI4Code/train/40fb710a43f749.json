{"cell_type":{"01e75913":"code","8a59577b":"code","e9290f0c":"code","d563e358":"code","47f2b501":"code","14c62909":"code","93736954":"code","cde2c776":"code","4cd96328":"code","a02f39ec":"code","8809506d":"code","3dabf172":"code","983c4f71":"code","35640053":"code","908f7c67":"code","a52d8259":"code","20ff5d08":"code","e1c69c97":"code","2c8b3b78":"code","2915db98":"code","f6fc4758":"code","251dc88e":"code","9eaac3fd":"code","5b8a9273":"code","91bda210":"code","fbfcefd8":"code","77f96ad9":"code","43026f8a":"code","6a164552":"code","a95ebab0":"code","60ab5ad0":"code","0e2c475c":"code","b010ce89":"code","e363fcb8":"code","1866958e":"code","83331bd9":"code","9b62224b":"code","fde7d935":"code","fe449563":"code","05ac0e77":"code","c1e9c756":"code","ba87c86f":"code","3a787927":"code","3d1fd0e8":"code","c9e5d4ec":"code","f383b4bd":"code","74924f55":"code","712aaa99":"code","930030a7":"code","9723cfc6":"code","e923ca4a":"code","600c7eff":"code","768953c3":"code","37c197c5":"code","4900da5e":"code","92cca36e":"code","952892e7":"markdown","d6ba06e0":"markdown","99f7a161":"markdown","3579cb0c":"markdown","69cc19fe":"markdown","bbf9603f":"markdown","801e0da4":"markdown","2641196f":"markdown","273abb60":"markdown","d5cbc7a7":"markdown"},"source":{"01e75913":"import requests\nimport IPython.display as Disp\nurl = 'https:\/\/kaggle-examples-work-bucket.s3.us-east-2.amazonaws.com\/Football.DecisionTree.v3.jpg'\nDisp.Image(requests.get(url).content,height=800,width=800)","8a59577b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","e9290f0c":"#import libraries \nimport pandas as pd ","d563e358":"#read in the plays data \nplays = pd.read_csv(\"..\/input\/nfl-big-data-bowl-2021\/plays.csv\")\nplays.head()","47f2b501":"#read in the games data \ngames = pd.read_csv(\"..\/input\/nfl-big-data-bowl-2021\/games.csv\")\ngames.head()","14c62909":"#merge the two datasets and save as df \ndf = plays.merge(games,how=\"left\",on=\"gameId\")\ndf.head()","93736954":"#check the size of the dataset\ndf.shape","cde2c776":"#check the types on each column \ndf.dtypes","4cd96328":"#create a defense dataframe \ndefense = df.personnelD.str.split(',', expand=True)\ndefense.head()","a02f39ec":"#delete columns\ndefense.drop(defense.columns[[3, 4, 5]], axis = 1, inplace = True) \ndefense.head()","8809506d":"#rename columns \ndefense.rename(columns = {0:'DL',1:'LB',2:'DB'}, inplace = True) \ndefense.head()","3dabf172":"defense = defense.fillna(0)\ndefense['DL'] = defense['DL'].str.strip()\ndefense['LB'] = defense['LB'].str.strip()\ndefense['DB'] = defense['DB'].str.strip()","983c4f71":"#keep only the numeric values\ndefense[\"DL_num\"] = pd.to_numeric(defense['DL'].str[:1])\ndefense[\"LB_num\"] = pd.to_numeric(defense['LB'].str[:1])\ndefense[\"DB_num\"] = pd.to_numeric(defense['DB'].str[:1])\ndefense.head()","35640053":"#create a offense dataframe \noffense = df.personnelO.str.split(',', expand=True)\noffense.head()","908f7c67":"#delete columns\noffense.drop(offense.columns[[3, 4, 5, 6, 7]], axis = 1, inplace = True) \noffense.head()","a52d8259":"#rename columns \noffense.rename(columns = {0:'RB',1:'TE',2:'WR'}, inplace = True) \noffense.head()","20ff5d08":"offense = offense.fillna(0)\noffense['RB'] = offense['RB'].str.strip()\noffense['TE'] = offense['TE'].str.strip()\noffense['WR'] = offense['WR'].str.strip()","e1c69c97":"#keep only the numeric values\noffense['RB_num'] = pd.to_numeric(offense['RB'].str[:1])\noffense['TE_num'] = pd.to_numeric(offense['TE'].str[:1])\noffense['WR_num'] = pd.to_numeric(offense['WR'].str[:1])","2c8b3b78":"#concatenate the data frames, offense and defense, back to df \ndf = pd.concat((df,defense), axis=1)\ndf = pd.concat((df,offense), axis=1)\ndf.head()","2915db98":"#investigate duplicate indices \ndf[df.index.duplicated()]","f6fc4758":"#remove duplicated indices \ndf = df[~df.index.duplicated()]","251dc88e":"import numpy as np\nimport matplotlib.pyplot as plt  \nimport seaborn as sns \n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})","9eaac3fd":"#select only numeric colummns\nnum_df = df.loc[:,['quarter','down','yardsToGo','yardlineNumber','defendersInTheBox','numberOfPassRushers','preSnapVisitorScore',\n          'preSnapHomeScore','absoluteYardlineNumber','offensePlayResult','playResult','epa','week','DL_num','LB_num',\n           'DB_num','RB_num','TE_num','WR_num']]\nnum_df.head()","5b8a9273":"#save the column names \ncol_names = num_df.columns","91bda210":"#check for nulls in the data \nnum_df.isnull().sum()","fbfcefd8":"#standardize the data for processing \nfrom sklearn.preprocessing import StandardScaler\nx = StandardScaler().fit_transform(num_df)\nx = pd.DataFrame(x)\nx.head()","77f96ad9":"x.columns = col_names\nx.head()","43026f8a":"#see size of x\nx.shape","6a164552":"#check for nulls in the data \nx.isnull().sum()","a95ebab0":"#drop the NA rows \nx = x.dropna()\nx.shape","60ab5ad0":"from sklearn.decomposition import PCA\npcamodel = PCA(n_components=5)\npca = pcamodel.fit_transform(x)\npca.shape","0e2c475c":"plt.bar(range(1,len(pcamodel.explained_variance_ )+1),pcamodel.explained_variance_ )\nplt.ylabel('Explained variance')\nplt.xlabel('Components')\nplt.plot(range(1,len(pcamodel.explained_variance_ )+1),\n         np.cumsum(pcamodel.explained_variance_),\n         c='red',\n         label=\"Cumulative Explained Variance\")\nplt.legend(loc='upper left')","b010ce89":"plt.plot(pcamodel.explained_variance_ratio_)\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()\n\n#PCA1 is at 0 in xscale","e363fcb8":"plt.plot(pcamodel.explained_variance_)\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","1866958e":"plt.scatter(pca[:, 0], pca[:, 1])","83331bd9":"ax = sns.heatmap(pcamodel.components_,\n                 cmap='YlGnBu',\n                 yticklabels=[ \"PCA\"+str(x) for x in range(1,pcamodel.n_components_+1)],\n                 xticklabels=list(x.columns),\n                 cbar_kws={\"orientation\": \"horizontal\"})\nax.set_aspect(\"equal\")","9b62224b":"def myplot(score,coeff,labels=None):\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0\/(xs.max() - xs.min())\n    scaley = 1.0\/(ys.max() - ys.min())\n    plt.scatter(xs * scalex,ys * scaley,s=5)\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n        if labels is None:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'green', ha = 'center', va = 'center')\n        else:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'black', ha = 'center', va = 'center')\n \n    plt.xlabel(\"PC{}\".format(1))\n    plt.ylabel(\"PC{}\".format(2))\n    plt.grid()\n\nmyplot(pca[:,0:2],np.transpose(pcamodel.components_[0:2, :]),list(x.columns))\nplt.show()","fde7d935":"import seaborn as sn\ncorrMatrix = x.corr()\nsn.heatmap(corrMatrix, annot=True)\nplt.show()","fe449563":"#https:\/\/stackoverflow.com\/questions\/17778394\/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(x, 20))","05ac0e77":"from sklearn import metrics\nfrom scipy.spatial.distance import cdist\nfrom sklearn.cluster import KMeans\n\n## \n#This code runs slowly\n## \n\n# run kmeans with many different k\ndistortions = []\nK = range(2, 50)\nfor k in K:\n    k_means = KMeans(n_clusters=k, random_state=42).fit(x)\n    k_means.fit(x)\n    distortions.append(sum(np.min(cdist(x, k_means.cluster_centers_, 'euclidean'), axis=1)) \/ x.shape[0])\n    #print('Found distortion for {} clusters'.format(k))","c1e9c756":"#Estimate the size of K \n\nX_line = [K[0], K[-1]]\nY_line = [distortions[0], distortions[-1]]\n\n# Plot the elbow\nplt.plot(K, distortions, 'b-')\nplt.plot(X_line, Y_line, 'r')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","ba87c86f":"k = 15\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(x)\n#x['y'] = y_pred","3a787927":"from sklearn.manifold import TSNE\n\ntsne = TSNE(verbose=1, perplexity=100, random_state=42)\nX_embedded = tsne.fit_transform(x)","3d1fd0e8":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", 1)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\nplt.title('t-SNE with no Labels')\nplt.savefig(\"t-sne_covid19.png\")\nplt.show()","c9e5d4ec":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.hls_palette(k, l=.4, s=.9)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title('t-SNE with Kmeans Labels')\nplt.savefig(\"improved_cluster_tsne.png\")\nplt.show()","f383b4bd":"#drop the NA rows \nnum_df = num_df.dropna()\nnum_df.shape","74924f55":"#show data and check for NA's \nnum_df.head()","712aaa99":"#check for nulls in the data \nnum_df.isnull().sum()","930030a7":"#drop all missing \nnum_df = num_df.dropna()\nnum_df.shape","9723cfc6":"#add a label column which will be whether there was progress on the play \nnum_df['label'] = np.where(num_df['offensePlayResult']<=0, 1, 0)\nnum_df.dtypes","e923ca4a":"#Load Libraries \nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation","600c7eff":"feature_cols = ['quarter', 'down', 'yardsToGo', 'yardlineNumber','defendersInTheBox','numberOfPassRushers',\n                'preSnapVisitorScore','preSnapHomeScore','absoluteYardlineNumber','week','DL_num','LB_num',\n               'DB_num','RB_num','TE_num','WR_num']\nX = num_df[feature_cols] # Features\ny = num_df.label # Target variable\n\nX.head()","768953c3":"# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test","37c197c5":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(max_depth=4)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)","4900da5e":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","92cca36e":"from sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\nplt.figure(edgecolor='white',facecolor='black', linewidth=10,figsize=(20,20))\n \nplot_tree(clf,filled=True,rounded=True, fontsize=8, class_names=[\"Offense\",\"Defense\"],feature_names=X.columns);\n","952892e7":"## Investigate Relationships within the Numeric Columns ","d6ba06e0":"### Begin Modeling with Decision Trees","99f7a161":"## Dimensionality Reduction with t-SNE","3579cb0c":"# Appendix: Research Walkthrough\nThe data has a few interesting details. First, it is entirely passing plays so any analysis derived comes from the assumption that the passing play is known, thus the analysis should be under the assumption you are on the offensive side of the ball. Second, much of the provided data is low level position data per play, this might be extraneous to derive general principles or performance indicators, so we will focus more on play meta-data. Last, we separate our data into control information (knowns) and possible dependent variables we want to predict or understand. The control information is going to be down, distance, score, teams and the types of players on the field.","69cc19fe":"## PCA Analysis  \n\"PCA is dimension reduction technique which takes set of possibly correlated variables and tranforms into linearly uncorrelated principal components. It is used to emphasize variations and bring out strong patterns in a dataset.\n\nIn simple words, principal component analysis is a method of extracting important variables from a large set of variables available in a data set. It extracts low dimensional set of features from a high dimensional data set with a motive to capture as much information as possible.\" - https:\/\/ostwalprasad.github.io\/machine-learning\/PCA-using-python.html","bbf9603f":"## Create New Columns for Analysis","801e0da4":"### Clean up the main data file\nWe want to select columns relevant to our analysis, get rid of nulls and recode the penalty column","2641196f":"# Summary of Research\n\nFocused on the results of 18,000+ plays as characterized by the following:\n* quarter \n* down\n* distance\n* yard line \n* defenders in box\n* number of pass rushers \n* score\n* the quantity of players for each position on the field\n\n## Steps \nFirst I created the database and ran both a principled components analysis and correlation matrix. The results of these experiments are intuitively obvious as a dynamic game with a fixed number of players on the field, positions and outcomes. Next I ran a decision tree on these variables on whether the offense had zero or negative yardage on a play. This led to a fairly transparent but still convoluted decision tree. I cleaned up the tree by hand considering what are the main decision points leading to better outcomes for defenses and created an infographic to summarize. \n\n## Future Research\nFuture research would focus on game theoretic, mixed equilibrium outcomes (given knowledge of play strategies offenses would counter - how do we choose what to do with probability). **I believe you could use the lower level data to determine speed and spacial coverage of the defending backs and wide receivers to make an improved decision tree.**","273abb60":"## Create Main Data Table\nFor this analysis, we're going to focus on the plays and games data table which requires a merge on gameid. \n","d5cbc7a7":"## Decision Tree Modeling "}}