{"cell_type":{"021edf8c":"code","0eabccab":"code","7d34ec5c":"code","3f79e94b":"code","5755f683":"code","7efbe1ae":"code","10206794":"code","4cb69679":"code","294259c7":"code","a714b55a":"code","bae4608c":"code","55acb4c4":"code","7042670c":"code","f516b37d":"markdown","bcbfef09":"markdown","ec05e366":"markdown","0e07ec98":"markdown","9f30ecb5":"markdown","57df37a8":"markdown","a44e1040":"markdown","757af26a":"markdown","6d49291a":"markdown","b8ae789f":"markdown","8c81d4f4":"markdown","59c360ac":"markdown","db17a947":"markdown","563a0ba7":"markdown"},"source":{"021edf8c":"import re\nfrom collections import defaultdict\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk import tokenize\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nsns.set()","0eabccab":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')","7d34ec5c":"fig, ax = plt.subplots(figsize=(12, 8))\nsns.histplot(\n    data=train,\n    x='target',\n    stat='probability',\n    ax=ax\n)\n\nax.set_title('Target\\'s distribution')\nplt.show()","3f79e94b":"fig, ax = plt.subplots(figsize=(12, 8))\nsns.scatterplot(\n    data=train,\n    x='target',\n    y='standard_error',\n    ax=ax\n)\n\nax.set_title('How standard error changes with target values')\nplt.show()","5755f683":"train['license_type_cnt'] = train.groupby('license').transform('count')['id']\nlicense_data = train[train['license_type_cnt'] >= 10]\n\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.pointplot(\n    data=license_data,\n    x='license',\n    y='target',\n    ci='sd',\n    join=False\n)\n\nplt.show()","7efbe1ae":"def get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\nlemmatizer = WordNetLemmatizer()\ndef clean_text(text):\n    text = re.sub('[^A-Za-z0-9]+', ' ', text.lower())\n    words = nltk.word_tokenize(text)\n    tagged = nltk.pos_tag(words)\n    words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n    words = [word for word in words if word not in stopwords.words('english')]\n    return words\n\ndef get_ngrams(words, n):\n    return [tuple(words[i:i+n]) for i in range(len(words)-n+1)]","10206794":"corpus = []\nfor text, target in train[['excerpt', 'target']].itertuples(index=False):\n    sentences = []\n    for sentence in tokenize.sent_tokenize(text):\n        words = clean_text(sentence)\n        unigrams = get_ngrams(words, n=1)\n        bigrams = get_ngrams(words, n=2)\n        trigrams = get_ngrams(words, n=3)\n        sentences.append(words)\n    corpus.append({\n        'target' : target,\n        'text' : text,\n        'sentences' : sentences,\n        'unigrams' : unigrams,\n        'bigrams' : bigrams,\n        'trigrams' : trigrams,\n    })\n\ncorpus = sorted(corpus, key=lambda x: x['target'])","4cb69679":"def plot_grams_target(gram_type):\n    gram_cnt = defaultdict(lambda: 0)\n    gram_sum = defaultdict(lambda: 0.)\n    gram_avg = {}\n\n    for datapoint in corpus:\n        for gram in datapoint[gram_type]:\n            gram_cnt[gram] += 1\n            gram_sum[gram] += datapoint['target']\n\n    for gram in gram_cnt:\n        if gram_cnt[gram] >= 5:\n            gram_avg[gram] = gram_sum[gram] \/ gram_cnt[gram]\n    \n    fig, ax = plt.subplots(1, 2, figsize=(12, 8))\n    \n    top_lowest = sorted(gram_avg.items(), key=lambda x: x[1])[:10]\n    ngrams, avg_target = zip(*top_lowest)\n    ax[0].bar(\n        range(len(ngrams)),\n        avg_target\n    )\n\n    ax[0].set_title(f'{gram_type} with lowest readability')\n    ax[0].set_xlabel(gram_type)\n    ax[0].set_ylabel('Average readability')\n    ax[0].set_xticks(range(len(ngrams)))\n    ax[0].set_xticklabels([' '.join(x) for x in ngrams], rotation='vertical')\n    \n    top_highest = sorted(gram_avg.items(), key=lambda x: x[1])[-10:]\n    ngrams, avg_target = zip(*top_highest)\n    ax[1].bar(\n        range(len(ngrams)),\n        avg_target\n    )\n\n    ax[1].set_title(f'{gram_type} with highest readability')\n    ax[1].set_xlabel(gram_type)\n    ax[1].set_ylabel('Average readability')\n    ax[1].set_xticks(range(len(ngrams)))\n    ax[1].set_xticklabels([' '.join(x) for x in ngrams], rotation='vertical')\n\n    plt.show()","294259c7":"plot_grams_target('unigrams')","a714b55a":"plot_grams_target('bigrams')","bae4608c":"top_lowest = corpus[:500]\nlowest_target_sentence_lengths = [ \\\n    np.mean([len(sentence) for sentence in datapoint['sentences']]) \\\n    for datapoint in top_lowest \\\n]\n\ntop_highest = corpus[-500:]\nhighest_target_sentence_lengths = [ \\\n    np.mean([len(sentence) for sentence in datapoint['sentences']]) \\\n    for datapoint in top_highest \\\n]\n\ntop_lowest_mean = np.mean(lowest_target_sentence_lengths)\ntop_lowest_std = np.std(lowest_target_sentence_lengths)\ntop_highest_mean = np.mean(highest_target_sentence_lengths)\ntop_highest_std = np.std(highest_target_sentence_lengths)\n\nfig, ax = plt.subplots(figsize=(6, 8))\nax.errorbar(\n    x=[0, 1],\n    y=[top_lowest_mean, top_highest_mean],\n    yerr=[top_lowest_std, top_highest_std],\n    fmt='o'\n)\n\nax.set_title('Average sentence length and Readability')\nax.set_ylabel('Sentence length')\nax.set_xticks([0, 1])\nax.set_xticklabels(['Top lowest readability', 'Top highest readability'])\n\nplt.show()","55acb4c4":"!pip install wordfreq\n\nfrom wordfreq import word_frequency\nlowest_target_word_freq = [\n    [word_frequency(word[0], 'en') for word in datapoint['unigrams']]\n    for datapoint in top_lowest\n]\nhighest_target_word_freq = [\n    [word_frequency(word[0], 'en') for word in datapoint['unigrams']]\n    for datapoint in top_highest\n]","7042670c":"lowest_min_freq = [np.min(datapoint) for datapoint in lowest_target_word_freq]\nhighest_min_freq = [np.min(datapoint) for datapoint in highest_target_word_freq]\n\nfig, ax = plt.subplots(figsize=(6, 8))\nax.errorbar(\n    x=[0, 1],\n    y=[np.mean(lowest_min_freq), np.mean(highest_min_freq)],\n    yerr=[np.std(lowest_min_freq), np.std(highest_min_freq)],\n    fmt='o'\n)\n\nax.set_ylabel('Min word frequency in general English')\nax.set_xticks([0, 1])\nax.set_xticklabels(['Top lowest readability', 'Top highest readability'])\n\nplt.show()","f516b37d":"- In general, the standard error is lowest when the readability is around -1. It tends to get higher for more extreme values of readability.\n- There is one data point with 0 readability and 0 standard error.","bcbfef09":"In this competition, you\u2019ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.","ec05e366":"Texts with shorter sentence lengths are often easier to read.","0e07ec98":"Hard-to-read texts often contain some less-usual words.","9f30ecb5":"# Clean and standardize texts","57df37a8":"# Target and Standard Error","a44e1040":"# Target and sentence length","757af26a":"![Image of Yaktocat](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25914\/logos\/header.png)","6d49291a":"### Data Description\n\n#### Files\n* **train.csv** - the training set\n* **test.csv** - the test set\n* **sample_submission.csv** - a sample submission file in the correct format\n\n#### Columns\n* `id` - unique ID for excerpt\n* `url_legal` - URL of source - this is blank in the test set.\n* `license` - license of source material - this is blank in the test set.\n* `excerpt` - text to predict reading ease of\n* `target` - reading ease\n* `standard_error` - measure of spread of scores among multiple raters for each excerpt. Not included for test data.","b8ae789f":"# EDA","8c81d4f4":"# License and Target","59c360ac":"Note that we only examine grams with at least 5 occurrences.","db17a947":"# N-grams","563a0ba7":"Different licenses have different average readability, but the standard deviation is quite high.\n\nNote that in test data, the license field is always blank."}}