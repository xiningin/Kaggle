{"cell_type":{"0d415e2c":"code","b6112c61":"code","23d9c1c8":"code","0d0744e6":"code","131cf9fc":"code","8a397a8d":"code","21ff3aef":"code","24d9d475":"code","0df2b3d8":"code","50194cfa":"code","1a051ae7":"code","aff24a12":"code","6d8ccabc":"code","9a4893ed":"code","32092d13":"code","baae1a62":"markdown","6ad696f0":"markdown"},"source":{"0d415e2c":"#All the imports for this program\nimport numpy as np # linear algebra\nimport pandas as pd \nimport os\nimport random\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import  accuracy_score,recall_score,precision_score,f1_score,roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n#Check for files present\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","b6112c61":"random.seed(30)\n#read review data \nreviews = pd.read_csv(\"..\/input\/amazon-music-reviews\/Musical_instruments_reviews.csv\")\n\nreviews.head()\n","23d9c1c8":"#remove unncessary features\ndel reviews['reviewerID']\ndel reviews['asin']\ndel reviews['unixReviewTime']\ndel reviews['reviewTime']\ndel reviews['reviewerName']\ndel reviews['helpful']","0d0744e6":"#check for missing value\nreviews.isna().sum()\n\n#fill in missing values with \"\"\nreviews.reviewText.fillna(\"\",inplace = True)\n\n#combine the summary with review text and delete the summary and reviewText fiels\nreviews['review'] = reviews['reviewText'] + ' ' + reviews['summary']\ndel reviews['reviewText']\ndel reviews['summary']\n","131cf9fc":"#for ratings 4 & 5 consider then good 1 otherwise bad 0\nreviews['overall'] = (reviews['overall'] >3).astype(int)","8a397a8d":"#At this point we have the overall field which is binary and review which is a text field\nreviews.head()","21ff3aef":"#lets check the distribution of good and bad reviews\nreviews.overall.value_counts()","24d9d475":"#lets prep the data for modeling\n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n\nfeatureCounts = tfidf.fit_transform(reviews.review)\n\ntfidf_transformer = TfidfTransformer()\n\nfeatures = tfidf_transformer.fit_transform(featureCounts)\n\nlabels = reviews.overall\nfeatures.shape\n","0df2b3d8":"#use chi square test to list out the words used for good and bad reviews\nN = 2\n\nfor i in range(2):\n    features_chi2 = chi2(features, labels == i)\n    indices = np.argsort(features_chi2[i])\n    feature_names = np.array(tfidf.get_feature_names())[indices]\n    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n    \n    print(\"# '{}':\".format(i))\n    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))\n","50194cfa":"#split the data for training and testing\nX_train, X_test, y_train, y_test = train_test_split(features, labels, random_state = 0)\n","1a051ae7":"#I am planning to run multiple model and then choose the best, in order to do so let me write a function \n# which can train model, test the prediction, cross validate etc\n#please note that I am using stratified KFold because the distribution of classes is not normal\ndef performClassification(name, estimator, X, y, X_train, y_train, X_test, y_test):\n    \n    model = estimator.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    kFold = StratifiedKFold(n_splits=8)\n    \n    cv_score = round((cross_val_score(estimator, X ,y.values.ravel(), cv=kFold, scoring='roc_auc').mean())*100,3)\n    \n    accuracy = round((accuracy_score(y_test, y_pred))*100,3)\n    \n    recall = round((recall_score(y_test, y_pred))*100,3)\n    \n    precision = round((precision_score(y_test, y_pred))*100,3)\n    f1 = round((f1_score(y_test, y_pred))*100,3)\n    \n    roc_auc = round((roc_auc_score(y_test, y_pred))*100,3)\n\n    returnArray = pd.array([name,cv_score,accuracy,recall,precision,f1,roc_auc])\n    \n    return returnArray\n\n#reate a data frame to store the scores\nmodelScores = pd.DataFrame(columns =['Name','CV','Accuracy','Recall','Precision','F1','Roc_Auc'])","aff24a12":"#lets try different models\n#execute models one by one\nlm = LogisticRegression()\nmodelScores = modelScores.\\\n    append(pd.Series(performClassification('Logistic Regression',lm,features,labels, X_train, y_train, X_test, y_test),\\\n                     index=modelScores.columns), ignore_index=True)\n\nMNB = MultinomialNB()\nmodelScores = modelScores.\\\n    append(pd.Series(performClassification('Multinomial Naive Bayes',MNB,features,labels, X_train, y_train, X_test, y_test),\\\n                     index=modelScores.columns), ignore_index=True)\n        \nSVC = LinearSVC()\nmodelScores = modelScores.\\\n    append(pd.Series(performClassification('Linear SVM',SVC,features,labels, X_train, y_train, X_test, y_test),\\\n                     index=modelScores.columns), ignore_index=True)\n","6d8ccabc":"#lets check the outcome\nprint(modelScores)\n#we can choose model based on multiple scores here, I would go ahead and choose Linear SVM base don ROc_AUC score","9a4893ed":"#lets first check the classifucation report\ny_pred = SVC.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_pred, target_names=reviews['overall'].unique().astype(str)))","32092d13":"#now lets buld the confusion matrix and plot the heatmap\nconf_mat = confusion_matrix(y_test, y_pred)\ntn, fp, fn, tp = conf_mat.ravel()\n(tn, fp, fn, tp)","baae1a62":"I have tried to achieve few things in this notebook\n* run models and validate them using stratified Kfold cross validation\n* compare multiple models using f1 score, AUC and accuracy\n\nFew this I will try later,\n* use more models, use hyper parameter tuning for some of these models\n* clean up of data and use other ways to generate vectors from the text","6ad696f0":"As we can see, the best model is Linear SVM with 91% accuracy"}}