{"cell_type":{"23393d91":"code","198da6b2":"code","779145cd":"code","72dd899b":"code","44d7c9ac":"code","de45305e":"code","c75d1430":"code","bbd4cb61":"code","cb98620d":"code","bbf21611":"code","40f8b225":"code","1d8ed5c9":"code","ca2e0dfd":"code","1abfde74":"code","5db28301":"code","fba8cd37":"code","c5940ed7":"code","74ed2366":"code","c578ab05":"code","ca75d7a0":"code","269e99d8":"code","548c16a6":"code","5ef3f704":"code","1bc6bd0d":"code","abdf128f":"code","3a302c83":"code","d3910c9c":"code","eb72824e":"code","62a77dc6":"code","7e28ed49":"code","3242ea4c":"code","bf2501ac":"code","3aa5538b":"code","04725d54":"code","8194fed0":"code","cf3ab106":"code","cf7a6b44":"code","ecb7fb25":"code","169cdddd":"code","25693224":"code","64ec3996":"code","e556ee05":"markdown","21507d05":"markdown","b33f5908":"markdown","5fcdc024":"markdown","a3e8d409":"markdown","0f28aed7":"markdown","930795df":"markdown","fff11068":"markdown","567c5881":"markdown","476191bf":"markdown","6b664f43":"markdown","1f0aabd8":"markdown"},"source":{"23393d91":"#Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix","198da6b2":"#Make data frames\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","779145cd":"#Training data frame with 143 features\ntrain.head()","72dd899b":"#Testing without target feature\ntest.head()","44d7c9ac":"#See what values are missing >%50\ntrain.isnull().sum().sort_values(ascending=False)","de45305e":"#Droped the features that have and missing values and\/or were redundant\ntrain.drop(labels=(['v2a1', 'v18q1', 'rez_esc', 'tamviv', 'r4h3', 'r4h3', 'r4t1', 'r4t2', 'r4t3', 'meaneduc', 'SQBescolari',\n                   'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', \n                   'SQBmeaned', 'agesq']), axis=1, inplace=True)","c75d1430":"#Sanity check #001\ntrain.shape","bbd4cb61":"#Get the value counts of our target variable\ntrain.Target.value_counts()","cb98620d":"#Some visualization for the target variable\nplt.hist(train.Target, edgecolor='black')\nplt.xticks([1, 2, 3, 4])\nplt.xlabel(\"Poverty Level\")\nplt.ylabel(\"Value Counts\")\nplt.title(\"Value Counts for Poverty in Costa Rica\")","bbf21611":"#Create a categorical version of the target column\nmapped = {1: 'extreme poverty', 2:'moderate poverty', 3: 'vulnerable', 4: 'non_vulnerable'}\ntrain['Target_cat'] = train['Target']\ntrain.Target_cat = train.Target_cat.replace(mapped)","40f8b225":"#sanity check #002\ntrain.Target_cat.value_counts()","1d8ed5c9":"#Create a graph to show the levels of poverty\ntrain.Target_cat.value_counts().plot.barh(color='blue', edgecolor='black')\nplt.ylabel('Poverty level')\nplt.xlabel('Number of People')\nplt.title('Poverty levels by person')","ca2e0dfd":"print(\"Non-vulnerable cases make up\", round((train.Target_cat.value_counts()[0] \/ train.shape[0]) * 100, 2), \n      \"% of all cases.\")","1abfde74":"#Replace numbers with each person's gender\ngender_map = {1: 'Female', 0: 'Male'}\ntrain['Gender'] = train.female.replace(gender_map)\ntrain.drop(['male'], axis=1, inplace=True)","5db28301":"#See the beakdown of gender when it comes to poverty\nprint(train.groupby(['Target_cat']).Gender.value_counts())\ntrain.groupby(['Target_cat']).Gender.value_counts().plot.barh(color='blue', edgecolor='black')\nplt.ylabel('Poverty Level by Gender')\nplt.xlabel('Number of people')\nplt.title('Number of people in poverty by gender')","fba8cd37":"#See the breakdown of poverty by whether someone has a disability\ndis_map = {1: 'Handicapped', 0: 'Non_Handicapped'}\ntrain['Disability'] = train.dis.replace(dis_map)\ntrain.drop('dis', axis=1, inplace=True)\nprint(train.groupby(['Target_cat']).Disability.value_counts())\ntrain.groupby(['Target_cat']).Disability.value_counts().plot.barh(color='blue', edgecolor='black')","c5940ed7":"#Hmmmm it was hard to tell from that graph. Let's try by percentages\ndis_counts = train.groupby(['Target_cat']).Disability.value_counts()\nprint(dis_counts)\ndis_counts = [round(53 \/ (702 + 53) * 100, 2), round(135 \/ (1462 + 135) * 100, 2), round(285 \/ (5711 + 285) * 100, 2), round(77 \/ (1132 + 77) * 100, 2)]\nprint(\"Percentage of impoverished by disability:\", dis_counts)","74ed2366":"#Get a list of feature types\ntrain.info(verbose=True)","c578ab05":"#See what the 8 object categories are\ntrain.select_dtypes('object').head(10)","ca75d7a0":"#Replace yes with 1 and no with 0\nmap_resp = {'yes': 1, 'no': 0}\ncats = ['dependency', 'edjefe', 'edjefa']\nfor x in cats:\n    train[x] = train[x].replace(map_resp)\n    test[x] = test[x].replace(map_resp)","269e99d8":"#Sanity check #003\ntrain.select_dtypes('object').head(10)","548c16a6":"#Let's check how many of these columns are Boolean\nprint(train.select_dtypes('int64').nunique().value_counts().sort_index())\ntrain.select_dtypes('int64').nunique().value_counts().sort_index().plot.bar(color='blue', edgecolor='black')\nplt.xlabel('Unique Values')\nplt.ylabel('Number of features with unique value')\nplt.title('Number of Unique Values per feature')","5ef3f704":"#Checking the column with only one unique value\nOne_val = []\nfor x in train.columns:\n    if train[x].nunique() == 1:\n        One_val.append(x)\n    else:\n        pass\ntrain_one = train[One_val]\ntrain_one.head()","1bc6bd0d":"#Sanity check #004\ntrain_one.elimbasu5.value_counts()","abdf128f":"#Alright let's drop it\ntrain = train.drop(['elimbasu5'], axis=1)\ntrain.shape","3a302c83":"#A seperate dataframe is created to analyize the boolean columns\nbool_col = []\nfor x in train.columns:\n    if train[x].nunique() == 2:\n        bool_col.append(x)\n    else:\n        pass\ntrain_bool = train[bool_col]\ntrain_bool.head()","d3910c9c":"#Create sub dataframes depending on each level of poverty\ntrain_bool['Target'] = train['Target']\nextreme_mask = train_bool.Target == 1\nextreme = train_bool[extreme_mask]\nmoderate_mask = train_bool.Target == 2\nmoderate = train_bool[moderate_mask]\nvunlnerable_mask = train_bool.Target == 3\nvunlnerable = train_bool[vunlnerable_mask]\nnon_vunlnerable_mask = train_bool.Target == 4\nnon_vunlnerable = train_bool[non_vunlnerable_mask]\n#note: I'm aware I spelled vulnerable wrong","eb72824e":"#See how many cases are in each level\nprint(extreme.shape)\nprint(moderate.shape)\nprint(vunlnerable.shape)\nprint(non_vunlnerable.shape)","62a77dc6":"#Generate a report of how much each column makes up when it comes to extreme poverty\nname = []\nmean = []\nstd = []\ntrain_bool = train_bool.drop(['Target'], axis=1)\nfor x in train_bool.columns:\n    print(x + \":\")\n    mask = extreme[x] == 1\n    extreme1 = extreme[mask]\n    print(x, \"makes up\", round(extreme1.shape[0] \/ extreme.shape[0] * 100, 2), \"% of the extreme category.\")\n    mask = moderate[x] == 1\n    moderate1 = moderate[mask]\n    print(x, \"makes up\", round(moderate1.shape[0] \/ moderate.shape[0] * 100, 2), \"% of the moderate category.\")\n    mask = vunlnerable[x] == 1\n    vunlnerable1 = vunlnerable[mask]\n    print(x, \"makes up\", round(vunlnerable1.shape[0] \/ vunlnerable.shape[0] * 100, 2), \"% of the vunlnerable category.\")\n    mask = non_vunlnerable[x] == 1\n    non_vunlnerable1 = non_vunlnerable[mask]\n    print(x, \"makes up\", round(non_vunlnerable1.shape[0] \/ non_vunlnerable.shape[0] * 100, 2), \"% of the non_vunlnerable category.\")\n    combined = np.array([extreme1.shape[0] \/ extreme.shape[0] * 100, moderate1.shape[0] \/ moderate.shape[0] * 100, vunlnerable1.shape[0] \/ vunlnerable.shape[0] * 100, non_vunlnerable1.shape[0] \/ non_vunlnerable.shape[0] * 100])\n    print(x, \"Mean:\", round(combined.mean(), 2), \"%\")\n    print(x, \"Std:\", round(combined.std(), 2), \"%\")\n    name.append(x)\n    mean.append(combined.mean())\n    std.append(combined.std())\n    print(\"\\n\")","7e28ed49":"#Generate a data frame to determine priority of columns\nname = np.array(name)\nmean = np.array(mean)\nstd = np.array(std)\nBoolean_df = pd.DataFrame({'Column': name, 'Mean': mean, 'Standard Deviation': std})\nBoolean_df.sort_values(by='Standard Deviation', ascending=False).reset_index().head(10)\n","3242ea4c":"#Generate a data frame for the rest of the columns\nint_col = []\nfor x in train.columns:\n    if train[x].nunique() > 2:\n        int_col.append(x)\n    else:\n        pass\ntrain_int = train[int_col]\ntrain_int.head()","bf2501ac":"#We should drop the Id column from the data frame. Let's check what else needs to be dropped\ntrain_int.columns","3aa5538b":"#Looks like we can drop Id and idhogar as these are identifiers. Let's also drop hogar total since it's redundent with\n#the other breakdowns by day. Let's also drop hogar_total from the main data frame as well\ntrain = train.drop(['hogar_total'], axis=1)","04725d54":"#Let's see which featuers would be important in a model\nx_list = train.columns\nx_list = x_list.drop(['Id', 'Target', 'Target_cat', 'idhogar'])\nX = train[x_list]\nX = X.drop(['Disability', 'Gender'], axis=1)\ny = train.Target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","8194fed0":"#Let's see how the model scores\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nrf.score(X_test, y_test)","cf3ab106":"#A Confusion Matrix shows where the inaccurate predictions are\nvalues = rf.predict(X_test)\nprint(np.unique(values, return_counts=True))\nconfusion_matrix(y_test, values)","cf7a6b44":"#See the importance of the features\nfeatures = rf.feature_importances_\nsummation = []\nvar_name = []\nfor feature in zip(x_list, features):\n    if feature[1] > .01:\n        print(feature)\n        summation.append(feature[1])\n        var_name.append(feature[0])\n    else:\n        pass\nsummation = np.array(summation)\nprint(str(len(summation)) + \" variables account for \" + str(round(summation.sum() * 100, 2)) + \"% of the variation\")","ecb7fb25":"#Use train as the training data and test as the test data\nX_train = train[var_name]\ny_train = train['Target']\nX_test = test[var_name]","169cdddd":"#Fit the random forest and predict\nrf.fit(X_train, y_train)\ny_test = rf.predict(X_test)\ntest['Target'] = y_test","25693224":"#sanity check #005\ntest.Target.value_counts()","64ec3996":"#Submission process\nsub_var = ['Id', 'Target']\nsubmission = test[sub_var]\nsubmission.to_csv('Submission.csv', index=False)","e556ee05":"It appears that the majority of people are doing fine. This will create an issue of imbalanced classes where a model that predicts a non-vulnerable case will be about 62% accurate. This will be considered the baseline for all other models. ","21507d05":"## Building a Random Forest Classifier","b33f5908":"As we can see Gender and Disability have now been converted to objects along with Target category. It also makes sense that Id and idhogar(household identifier) are objects since they identify people and households. What's odd is that dependency, edjefe(years of education for male head of house), and edjefa(years of education for the female head of house) are a mix of numbers and words. Let's replace yes with 1 and no with 0 as it is in the description.","5fcdc024":"## Preping the data\n\nIt's important to remove variables that will not add value to the analysis or that have too many missing values","a3e8d409":"# Costa Rican Household Poverty Level Prediction\n\nA Kaggle Competition. The task is using the data to build a model which predicts different levels of poverty so social services can target those households that need help.","0f28aed7":"## Exploratory Data Analysis\n\nLet's see if we can get some interesting insights","930795df":"Let's see if we can pull out any meaningful insights about these features.","fff11068":"## Feature Engineering","567c5881":"The three columns are still considered an object but for now they can all be converted to integers at least.","476191bf":"So it appears that 99 features are made of boolean features. We can see which poverty group contains the most true values for many given features. First I'll split the data set by the level of poverty and check which group has the highest percentage for of true values for each true value. Also one column only has one unique value?","6b664f43":"Using a large number of features I decided to use a random forest classifier to predict the outcome.","1f0aabd8":"Strange. According to the Kaggle website a value of one indicates \"=1 if rubbish disposal mainly by throwing in river,  creek or sea\". It seems no one is doing this so it will add nothing to the model."}}