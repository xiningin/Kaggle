{"cell_type":{"f508c557":"code","023695ee":"code","592a31e2":"code","d7107518":"code","685d313a":"code","f0113715":"code","62d6cf1a":"code","89ae0cb7":"code","876368dc":"code","7013cb76":"code","306ba773":"code","763ace78":"code","dea97e99":"code","7f76b5f0":"code","ad5c669c":"code","2cca1b9f":"code","b1c5385e":"code","a9167df8":"code","c0bc1cc7":"code","124a24a3":"code","529182a7":"code","fcf58186":"code","1295b269":"markdown","f20cee01":"markdown","d5c024e2":"markdown","35bb0dce":"markdown","71c9b4bd":"markdown","d77b6db0":"markdown","51637269":"markdown","69212ea6":"markdown","95f1ef68":"markdown","4f299f9f":"markdown","f894c0fa":"markdown","709d7e0f":"markdown","16c03b98":"markdown","b112f100":"markdown","6e95f7aa":"markdown","4f850e4e":"markdown","3ef24db2":"markdown","b9f1d105":"markdown","3aa5229e":"markdown","29a3c105":"markdown","b8698159":"markdown","908172a3":"markdown","144e8e72":"markdown","f1d262e0":"markdown","4f0ed979":"markdown","283c7906":"markdown","0d9ceb2c":"markdown"},"source":{"f508c557":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","023695ee":"### Call the dataset","592a31e2":"df_iris = pd.read_csv('..\/input\/iris\/Iris.csv')\ndf_iris.head()","d7107518":"print('Unique targets are ', df_iris['Species'].unique())\nprint('Number of row: ', df_iris['Id'].count())\nprint('Number of columns: ', len(df_iris.columns))","685d313a":"display(df_iris.isna().sum())\nprint('-' * 40)\nprint(df_iris['Id'].nunique(), 'unique flowers')\nprint('-' * 40)\nprint(df_iris.shape)","f0113715":"#drop the Id column now. We don't need it for the ML algorithm\ndf_iris.drop('Id', axis=1, inplace=True)","62d6cf1a":"df_iris.groupby('Species').agg({'Species': ['count']})","89ae0cb7":"plt.figure()\nsns.pairplot(data = df_iris, hue = 'Species')\nplt.show()","876368dc":"plt.figure(figsize = (15,10))\nplt.subplot(2,2,1)\nsns.violinplot(x = 'Species', y = 'SepalLengthCm', data = df_iris)\nplt.subplot(2,2,2)\nsns.violinplot(x = 'Species', y = 'SepalWidthCm', data = df_iris)\nplt.subplot(2,2,3)\nsns.violinplot(x = 'Species', y = 'PetalLengthCm', data = df_iris)\nplt.subplot(2,2,4)\nsns.violinplot(x = 'Species', y = 'PetalWidthCm', data = df_iris)\nplt.show()","7013cb76":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_selection import SelectKBest, chi2 #for feature selection\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.metrics import mean_absolute_error, confusion_matrix, classification_report\nfrom sklearn.model_selection import GridSearchCV","306ba773":"#correlation between features\nplt.figure()\nsns.heatmap(df_iris.corr(), annot = True, cmap = 'flare')\nplt.show()","763ace78":"X = df_iris.drop('Species', axis = 1)\nY = df_iris['Species']\n\nfor k in np.arange(1, 5, 1):\n    X_new = SelectKBest(score_func = chi2, k = k)\n    X_new.fit_transform(X, Y)\n    features = X.columns[X_new.get_support()]\n    print('---------------For k =', k, '---------------')\n    display(features)","dea97e99":"X = df_iris[['PetalLengthCm', 'PetalWidthCm']] #features\ny = df_iris['Species'] #target\n\n#X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 1) #split train and test sets, with train size at 80%","7f76b5f0":"t = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n#k = range(1,20)\n\nknn = KNeighborsClassifier(n_neighbors = 1)\n\n\nplt.figure()\n#can run an additional for loop to test the k value as well. Depends on your CPU lol\n#for k in k_range:\n    #knn = KNeighborsClassifier(n_neighbors = k)\nfor s in t:\n    scores = []\n    for i in range(1,1000):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s)\n        knn.fit(X_train, y_train)\n        scores.append(knn.score(X_test, y_test))\n    plt.plot(s, np.mean(scores), 'bo')\n\nplt.xlabel('Training set proportion (%)')\nplt.ylabel('accuracy');","ad5c669c":"X = df_iris[['PetalLengthCm', 'PetalWidthCm']] #features\ny = df_iris['Species'] #target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 1) #split train and test sets, with train size at 80%","2cca1b9f":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\npredictions_knn = knn.predict(X_test)\naccuracy_knn = metrics.accuracy_score(y_test, predictions_knn)\n\nprint('The model Accuracy is', accuracy_knn)","b1c5385e":"k_range = range(1,20) #range of k from 1 to 19\nscores = []\nscores_test = []\nscores_train = []\n\nfor k in k_range:\n    knn_2 = KNeighborsClassifier(n_neighbors = k)\n    knn_2.fit(X_train, y_train)\n    predictions_knn_2 = knn_2.predict(X_test)\n    scores.append(metrics.accuracy_score(y_test, predictions_knn_2))\n    scores_train.append(knn_2.score(X_train, y_train))\n\nplt.figure()\nplt.xlabel('k')\nplt.ylabel('accuracy')\nplt.plot(k_range, scores)\nplt.plot(k_range, scores_train)\nplt.legend(['Accuracy Prediction (Test Set)', 'Accuracy Train Set'])\nplt.xticks([0,5,10,15,20]);","a9167df8":"print('--------------------KNN--------------------')\nprint()\n#ACCURACY\nprint('Training accuracy is', knn.score(X_train, y_train))\nprint('Test accuracy is', knn.score(X_test, y_test))\nprint('-' * 40)\n\n#CROSS VALIDATION (K-Fold) -> run our modeling process on different subsets of the data to get multiple measures of model quality\n#using cross validation is a good idea because our data set is small (only 150 entries)\nscore_accuracy = cross_val_score(knn, X_train, y_train, cv = 10, scoring = 'accuracy')\nprint('Average accuracy is', score_accuracy.mean())\nprint('-' * 40)\n\n#CONFUSION MATRIX\nconfusion = confusion_matrix(y_test, predictions_knn)\nprint('The confusion matrix is')\nprint(confusion)\n\n#CLASSIFICATION REPORT -> performance evaluation metric\nprint('-' * 40)\nprint('The classification report is')\nprint(classification_report(y_test, predictions_knn))\n","c0bc1cc7":"svm = SVC()\nsvm.fit(X_train, y_train)\npredictions_svm = svm.predict(X_test)\n\naccuracy_svm = metrics.accuracy_score(y_test, predictions_svm)\nprint('The accuracy of the model is', accuracy_svm)","124a24a3":"#grid search\n\n#define the hyperparameters first\n#like i said, it might take long to run the cell. After trying to run the cell multiple time and seeing it running forever, I decided to keep it really simple\n#just to show the idea of grid search\nc = [0.1, 1, 10, 100]\ngamma = [0.1, 1, 10, 100] #Kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019\n#degree = [1, 2, 3, 4, 5] #Degree of the polynomial kernel function (\u2018poly\u2019). Ignored by all other kernels.\n#kernel = ['linear', 'poly', 'rbf', 'sigmoid']\nkernel = ['linear', 'rbf'] #Specifies the kernel type to be used in the algorithm. \n\n#define a dictionary to store values\n#grid_dict = dict(C = c, gamma = gamma, degree = degree, kernel = kernel)\ngrid_dict = dict(C = c, gamma = gamma, kernel = kernel) #attention !!! for the c parameter it is a capital c\ngrid_search = GridSearchCV(estimator = svm, param_grid = grid_dict, cv = 10, scoring = 'accuracy', error_score = 0)\ngrid_result = grid_search.fit(X, y)\n\n#print the optimal hyperparameters and accuracy\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","529182a7":"svm.get_params().keys()","fcf58186":"print('--------------------SVM--------------------')\nprint()\n#ACCURACY\nprint('Training accuracy is', svm.score(X_train, y_train))\nprint('Test accuracy is', svm.score(X_test, y_test))\nprint('-' * 40)\n\n#CROSS VALIDATION (K-Fold) -> run our modeling process on different subsets of the data to get multiple measures of model quality\n#using cross validation is a good idea because our data set is small (only 150 entries)\nscore_accuracy = cross_val_score(svm, X_train, y_train, cv = 10, scoring = 'accuracy')\nprint('Average accuracy is', score_accuracy.mean())\nprint('-' * 40)\n\n#CONFUSION MATRIX\nconfusion = confusion_matrix(y_test, predictions_svm)\nprint('The confusion matrix is')\nprint(confusion)\n\n#CLASSIFICATION REPORT -> performance evaluation metric\nprint('-' * 40)\nprint('The classification report is')\nprint(classification_report(y_test, predictions_svm))","1295b269":"# Project 1\/52\nIris dataset KNN vs SVM.\n<br>\nWe'll first go through some <b>data explanatory<\/b>, then <b>feature selection<\/b>, and finally ML modeling with <b>hyperparameter tuning<\/b>, and <b>model evaluation<\/b> (<b>confusion matrix + cross validation<\/b>).\n<br>\n<br>\nThe goal is not to have the best accuracy. Even without touching the data set (data cleaning or feature engineering), the accuracy is pretty good. But to discover the different step of building a ML model.\n<br>\n<br>\nEnjoy","f20cee01":"# Conclusion","d5c024e2":"Set up the features and the target. Then split the train and test sets.","35bb0dce":"After stating the problem, it can clearly be categorized as a classification problem -> predictive modeling problem where a class label is predicted for a given example of input data. Here, the target is the species classified in 3 categories.\n<br>\n<br>\nSince there are 3 categories, it is a multi-class classification problem. For this classification problem, the most popular algorithm are:\n<br>\nk-Nearest Neighbors, Decision Trees, Naive Bayes, Random Forest, Gradient Boosting.\n<br>\nWe are here to learn, so we're gonna try them all.","71c9b4bd":"Seems to have no positive or negative skewness (when the histogram has its peak is not centered -> more on the right or left).\nWe can easily observe some correlation among the species. By observing the clusters, a simple classification model will do the work. However, certain variables (Sepal Length vs Sepal Width) might be challenging to predict. Let's see how our model can perform.","d77b6db0":"#### KNN\nIt assumes that similar things exist in close proximity. In other words, similar things are near to each other.","51637269":"#### Model Evalution\nEvaluating the model is important to find potential error not seen previously and to understand the model accuracy better. This part allow the right correction for our model (if needed of course).","69212ea6":"How to interpret a 3x3 confusion matrix?\n<br>\nColumns -> Predicted label\n<br>\nRows -> Actual label (true)\n<br>\n1st column and 1st row -> Iris-setosa\n<br>\n2nd column and 2nd row -> Iris-versicolor\n<br>\n3rd column and 3rd row -> Iris-virginica\n<br>\n<br>\nSo? With this confusion matrix?\n<br>\n1st column -> 14 predicted values, for 14 true setosa and 0 versicolor and virginica\n<br>\n2nd column -> 17 predicted values, for 16 true versicolor and 0 setosa and 1 virginica\n<br>\n3rd column -> 12 predicted values, for 1 versicolor, 11 virginica and 0 setosa","95f1ef68":"### Some Data Discovery first","4f299f9f":"______________________________________________________________\n<br>\nI hope you enjoyed this notebook and that you grab interesting information. Don't hesistate to correct me in the comment section.\n<br>\n<br>\n---wst---","f894c0fa":"How to interpret the classification report?\n<br>\n<ul>\n    <li>Precision -> what fraction of positive predicitions are correct<\/li>\n    <li>Recall -> what fraction of all positive instances does the classifier correctly indentify as positive<\/li>\n    <li>F1 score -> convey the balance between the precision and the recall<\/li>\n    <li>Support -> number of actual occurances of the class in the specified dataset<\/li>\n<\/ul>\nThinking about it, if a completely new dataset had to be implemented in this model, the accuracy of the model would be closer to the F1 score, since it takes consideration of false\/true positive\/negative predictions. Hence the importance of the model evaluation with the confusion matrix and the classification report (precision and recall above all).","709d7e0f":"Feature selection is an important part of the design of a model. The model's performance will depand on the number of feature we will use. Many methods exist, let's review some.","16c03b98":"### Feature Selection","b112f100":"Re set up the model environment so the previous code block doesn't mess with the rest","6e95f7aa":"### Set up ML environment","4f850e4e":"### Prediciton","3ef24db2":"#### SVM\nLet's compare KNN with SVM\n<br>\nWhy? SVM take cares of outliers better than KNN.\n<br> \nSVM outperform KNN when there are large features and lesser training data. Which is not the case here but let's compare anyway.","b9f1d105":"We saw: <b>feature selection<\/b>, <b>model evaluation<\/b>, and <b>hyperparameter tuning<\/b>. And compared the best model between KNN and SVM.\n<br>\n<br>\nRegarding feature selection, I decided to stick with the 4 features, but we can go further by choosing less features. Or, with feature engineering, creating new features (like the areas of petal or sepal). We can have a lot of fun with the dataset.\n<br>\n<br>\nThe iris data set is pretty simple to use because it doesn't involve data cleaning and feature engineering, which is normally the most crucial part of building a model (especially data cleaning).","3aa5229e":"Overfitting and underfitting is an important aspect of Machine Learning. To prevent either one, we can check at the accuracy (or Root Square) or the train and test sets.\n<br>\nThe plot line Orange and Blue represent respectively the Train set and Test set accuracy. ","29a3c105":"##### Correlation between features","b8698159":"#### Hyperparameter tuning \nChoice of the k parameter (n_neighbors) for best accuracy.\n<br>\nThe hyperparameter k make the accuracy of the model varies between 1 and 0.966 (approx) for k values from 1 to 19.\n<br>\nThe KNN algorithm is pretty efficient and accurate for this problem.","908172a3":"Observation show that the Iris-Setosa shows large disparity in its data. While the 2 other species present less disparity which means that it is more easily recognizable.","144e8e72":"#### Hyperparameter tuning\nFor this part, unlike for the KNN model, I'm going to use the Grid Search process to find the best hyperparameter.\n<br>\nThe Grid Search process build a modfel on each parameter combination possible to find the optimal hyperparameter of the model which results in the most accurate prediction.\n<br>\nAttention -> for certain model it might take long to run.","f1d262e0":"#### Find the best training\/testing size\nWhile testing a train_size = 0.8 then 0.7 I noticed that the accuracy of the model was different. So the code below allow the visualize the best size of the test\/train set.","4f0ed979":"Here, we can use both the correlation heatmap and the SelectKBest to conclude on how many feature we should use. \n<br>\n<br>\nThe straight forward but annoying solution is to try the model with 1, 2, 3, and 4 features separatly.\n<br>\n<br>\nBUT, the heatmap tell us a lot, PetalLengthCm and PetalWidthCm are the most correlated features. The same conclusion is drawn with the violinplot above. So, let's start with these 2 features only.","283c7906":"##### Using SelectKBest\nSelectKBest is efficient and straight forward. The result is the same as the conclusion given for the correlation between features. Clearly, the Petal Length and Width have the best correlations. Having these 2 features is a must for the model.\n<br>\n<br>\nWe have a classification problem -> For classification: chi2, f_classif, mutual_info_classif","0d9ceb2c":"#### Model evaluation"}}