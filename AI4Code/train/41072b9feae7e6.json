{"cell_type":{"435b17e8":"code","b832bae9":"code","a2a9913a":"code","0db2d712":"code","87d2b167":"code","ee221905":"code","59d2dbc5":"code","3801f79b":"code","befec40f":"code","a20732d4":"code","072ffb3f":"code","94c40832":"code","7b04976b":"code","27b10208":"code","52d36f67":"code","12ed8c9f":"code","4b8bf55c":"code","5e5132de":"code","c09f9ac7":"code","297a21ee":"code","763b1d9a":"code","5d2b0997":"code","6c59e20c":"code","ba97e43f":"code","3aac1397":"code","8ed968f3":"code","6e758004":"code","45e8ebe5":"code","5ae95904":"code","9dda6f45":"code","048aafdc":"code","f4889957":"code","fc566aa9":"code","b8efbc30":"code","42bf992c":"code","55507621":"code","10d21c92":"code","cf6dc916":"code","19bb45ed":"code","8e9ecdfb":"code","e4408cd5":"code","80a90267":"markdown","6f74743b":"markdown","9c5675c6":"markdown","116948a9":"markdown","ae5c9100":"markdown","ce2714b5":"markdown","372a011b":"markdown","b60a9c8c":"markdown","1e73f538":"markdown","e1958825":"markdown","31c70fcf":"markdown","606ea8db":"markdown","ae209f20":"markdown","e20f5cf1":"markdown","04981935":"markdown","4f98793e":"markdown","2dfabce4":"markdown"},"source":{"435b17e8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt","b832bae9":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","a2a9913a":"#Print all rows and columns. Dont hide any\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\ntrain.head(3)","0db2d712":"print(train.shape, test.shape)","87d2b167":"train.describe()","ee221905":"#Checking missing values\nprint('Is the training data contains any missing values? ' + str(train.isnull().any().any()) + '\\n'\n     + 'Is the testing data contains any missing values? ' + str(test.isnull().any().any()))","59d2dbc5":"train_list_name = list(train.columns.values)\ntrain_list_name.pop() #pop out the loss column\ntest_list_name = list(test.columns.values)\nprint('Are the columns identical to each other for both train & test dataset? ' + str(train_list_name == test_list_name))","3801f79b":"#Check column values for each categorical columns\ndef showunique(df):\n    list_name = list(train.columns.values)\n    for i, col_name in enumerate(list_name):\n        if col_name[:3] == 'cat':\n            print(df.groupby('cat' + str(i))['id'].nunique())","befec40f":"showunique(train)","a20732d4":"#Separate the dataset, starting from column index 117\ntrain_cont = train.iloc[:, 117:]\ntest_cont = test.iloc[:, 117:]","072ffb3f":"train_cont.head(3)","94c40832":"test_cont.head(3)","7b04976b":"#Checking the skewness of the remaining dataset, the ones close to 0 are less skewed data\nprint(train_cont.skew())","27b10208":"#Heatmap to check the correlation\ncor = train_cont.corr()\nf, ax = plt.subplots(figsize = (12, 8))\nsns.heatmap(cor, vmax = 0.9, annot = True, square = True, fmt = '.2f')","52d36f67":"#Let us apply PCA\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\nloss = train_cont.loc[:, train_cont.columns == 'loss']\ntrain_cont_phase = train_cont.loc[:, train_cont.columns != 'loss']\n\nscaled_train_cont = StandardScaler().fit_transform(train_cont_phase)\nscaled_test_cont = StandardScaler().fit_transform(test_cont)","12ed8c9f":"#Do the PCA\npca = PCA()\npca.fit(scaled_train_cont)\npca.data = pca.transform(scaled_train_cont)","4b8bf55c":"#Percentage variance of each pca component stands for\nper_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n#Create labels for the scree plot\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]","5e5132de":"#Plot the data\nplt.bar(x=range(1, len(per_var)+1), height=per_var, tick_label = labels)\nplt.ylabel('percentage of Explained Variance')\nplt.xlabel('Principle Component')\nplt.title('Scree plot')\nplt.show()","c09f9ac7":"variance = 0\ncount = 0\nfor i in pca.explained_variance_ratio_:\n    if variance <= 95:\n        variance += i * 100\n        count+=1\nprint(str(np.round(variance, 2)) + '% of the variance is explained by ' + str(count) + ' of Principle Components')","297a21ee":"#Extract the PC1 through PC9 information\ntrain_append = pd.DataFrame(data=pca.data[:,:9], columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7'\n                                                           , 'PC8', 'PC9'])","763b1d9a":"train_append.head(3)","5d2b0997":"#Glue the PC data back to the training dataset\nnew_train = pd.concat((train.iloc[:, :117], train_append), axis = 1)\nnew_train.head(3)","6c59e20c":"#Now performing the same action for the testing dataset\npca.fit(scaled_test_cont)\npca.data = pca.transform(scaled_test_cont)\ntest_append = pd.DataFrame(data=pca.data[:,:9], columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7'\n                                                           , 'PC8', 'PC9'])\nnew_test = pd.concat((test.iloc[:, :117], test_append), axis = 1)\nnew_test.head(3)","ba97e43f":"#Check the distributions of the catevariables:\n# Count of each label in each category\n\n#names of all the columns\ncols = new_train.columns\n\n#Plot count plot for all attributes in a 29x4 grid\nn_cols = 4\nn_rows = 29\nfor i in range(n_rows):\n    fg,ax = plt.subplots(nrows=1,ncols=n_cols,sharey=True,figsize=(12, 8))\n    for j in range(n_cols):\n        sns.countplot(x=cols[i*n_cols+j+1], data=new_train, ax=ax[j])","3aac1397":"#Show dominanting percentage less than 2%, and drop them during the process\ndef show_and_drop_percentage(df, df2):\n    for i in range(1, 117):\n        A = df['cat' + str(i)].value_counts()\n        per = sum(A[1:]) \/ sum(A) * 100\n        if per < 2:\n            print('cat' + str(i) + ': ' + 'Dominating percentage is: ' + str(np.round(per, 2)) + '%')\n            df = df.drop(['cat' + str(i)], axis = 1)\n            df2 = df2.drop(['cat' + str(i)], axis = 1)\n    print('-' * 80 + '\\n')\n    print('Cleaning complete for columns cat1 to cat 116, The above categories had been dropped\\n')\n    return df, df2","8ed968f3":"#Operate on the training set\nremoved_train, removed_test = show_and_drop_percentage(new_train, new_test)","6e758004":"removed_train.head(3)","45e8ebe5":"removed_test.head(3)","5ae95904":"#Check if the same procedure was done on the train & test columns\nany(removed_train.columns == removed_test.columns)","9dda6f45":"#Remove the id columns\nremoved_train = removed_train.iloc[:, 1:]\nremoved_test = removed_test.iloc[:, 1:]","048aafdc":"removed_train.head(3)","f4889957":"#range of features considered\nsplit = 78\n\n#Grab out the categorical variables\ncat_train = removed_train.iloc[:, :split]\ncat_test = removed_test.iloc[:, :split]\n\n#List the column names\ncols = cat_train.columns\n\n#Variable to hold the list of variables for an attribute in the train and test data\nlabels = []\n\nfor i in range(0,split):\n    train = cat_train[cols[i]].unique()\n    test = cat_test[cols[i]].unique()\n    labels.append(list(set(train) | set(test))) ","fc566aa9":"#One hot encode all categorical attributes \n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ncats = []\nfor i in range(0, split):\n    #Label encode\n    label_encoder = LabelEncoder()\n    label_encoder.fit(labels[i])\n    feature = label_encoder.transform(cat_train.iloc[:,i])\n    feature = feature.reshape(cat_train.shape[0], 1)\n    #One hot encode\n    onehot_encoder = OneHotEncoder(sparse=False,categories= [range(len(labels[i]))])\n    feature = onehot_encoder.fit_transform(feature)\n    cats.append(feature)\n    \n# Make a 2D array from a list of 1D arrays\nencoded_cats = np.column_stack(cats)\n\n# Print the shape of the encoded data\nprint(encoded_cats.shape)\n\n#Concatenate encoded attributes with continuous attributes\ntrain_encoded = np.concatenate((encoded_cats,removed_train.iloc[:,split:].values),axis=1)\n\n#Transfer it back into pandas dataframe\ntrain_encoded = pd.DataFrame(data=train_encoded)\ntrain_encoded.head(3)","b8efbc30":"#One hot encode all categorical attributes\ncats = []\nfor i in range(0, split):\n    #Label encode\n    label_encoder = LabelEncoder()\n    label_encoder.fit(labels[i])\n    feature = label_encoder.transform(cat_test.iloc[:,i])\n    feature = feature.reshape(cat_test.shape[0], 1)\n    #One hot encode\n    onehot_encoder = OneHotEncoder(sparse=False,categories= [range(len(labels[i]))])\n    feature = onehot_encoder.fit_transform(feature)\n    cats.append(feature)\n    \n# Make a 2D array from a list of 1D arrays\nencoded_cats = np.column_stack(cats)\n\n# Print the shape of the encoded data\nprint(encoded_cats.shape)\n\n#Concatenate encoded attributes with continuous attributes\ntest_encoded = np.concatenate((encoded_cats,removed_test.iloc[:,split:].values),axis=1)\n\ntest_encoded = pd.DataFrame(data=test_encoded)\ntest_encoded.head(3)","42bf992c":"#First of all, we do train test split of our dataset\nfrom sklearn.model_selection import train_test_split\n\n#Set our random seed to ensure productive result\nseed = 2019\n\nX_train, X_test, y_train, y_test = train_test_split(\n     train_encoded, loss, test_size=0.25, random_state=seed)","55507621":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train) \nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","10d21c92":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_scaled, y_train)\n\n#Calculating the MAE\nlin_pred = lin_reg.predict(X_test_scaled)\nlin_result = mean_absolute_error(y_test, lin_pred)\nlin_result","cf6dc916":"from sklearn.linear_model import ElasticNet\n\nela = ElasticNet(random_state=seed)\nela.fit(X_train_scaled, y_train)\n\nela_pred = ela.predict(X_test_scaled)\nela_result = mean_absolute_error(y_test, ela_pred)\nela_result","19bb45ed":"#Make predictions using the model\n#Write it to the file\nTest_scaled = scaler.transform(test_encoded)\n\npredictions = ela.predict(Test_scaled)\n\npd.DataFrame(predictions, columns = ['loss']).to_csv('submission.csv')","8e9ecdfb":"from sklearn.linear_model import SGDRegressor\n\nsgd = SGDRegressor(max_iter = 1500, eta0=1e-14,\n                  learning_rate = 'adaptive',\n                  penalty = 'elasticnet')\nsgd.fit(X_train_scaled, y_train)\n\nsgd_pred = sgd.predict(X_test_scaled)\nsgd_result = mean_absolute_error(y_test, sgd_pred)\nsgd_result","e4408cd5":"sgd_pred","80a90267":"### __Stochastic Gradient Descent (SGD)__","6f74743b":"According to the graph we plotted: Cat14, 15, 17, 18, 19, 20, 21, 22, 29, 30, 32, 33, 34, 35, 42, 43, 45, 46, 47, 48, 51, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 74, 77, 78, 85 can be categorized as noisy columns\n\nBut we can not just rely on our eyes, we are going to remove them using calculations","9c5675c6":"### __Elastic Net__","116948a9":"__Data Cleaning__","ae5c9100":"__Thus, we are going to use 9 Principle components to preserve 96.8% of the entire 14 constant part of variance__","ce2714b5":"__For Training Data Set__","372a011b":"## __First of all, Let us take care of the constant part of the dataset__","b60a9c8c":"This data set contains 116 categorical columns plus 14 constant columns and one predictor (loss) column","1e73f538":"__Takeaways from the heatmap__ <br \/>\n\n- Most of the continous variables are somewhat correlated with the loss (lowest cont13)\n- Some of the variables are strongly correlated with each other (eg: cont1 and cont9 correlation is 0.93)\n- PCA or SVD could be applied to extract the most important features from these variables","e1958825":"__For Testing Data Set__","31c70fcf":"### __Now we dummy coding the remaining categorical variables__\n\nThanks to: https:\/\/www.kaggle.com\/sharmasanthosh\/exploratory-study-on-ml-algorithms\nprovide the method to achieve one-hot encoding considering value difference in values in train & test dataset\n(pandas get dummies would result in unevenly columns due to column value differences)","606ea8db":"### __Linear Regression__","ae209f20":"Elastic Net is doing much better, but still with quite large error rate","e20f5cf1":"__Looking good, now we try to handle the 116 categorical variables__\n\n__Strategy:__\n - Remove the noisy columns that has very unevenly distributed columns\n - One hot encoding the columns (Since the columns are more like the nominal variables)\n - We are not considering using label encoding since the variables does not look like ordinal","04981935":"## __Now we can prepare our data for modeling__\n\nModels to consider:\n   - linear Regression\n   - Lasso\n   - Ridge\n   - Elastic Net\n   - Stochastic Gradient Descent (SGD)\n   - RandomForest\n   - Xgboost\n   - LightGBM","4f98793e":"__the MAE of linear regession probably told us why we should NEVER use simple linear regression for our model__\n\n__We should go ahead and try some more complex model for our prediction__","2dfabce4":"- The takeaways from this column unique value check shows that from cat1 to cat73, we only have 2 selection choices A and B.<br \/>\n\nFor these columns some of them have uneven distribution of As and Bs, which makes the values not important, __for example__: <br \/>\n\nIn cat 70 there are 188295 As (99.98%) and 23Bs (0.02%). These columns can be removed to reduce the dimensionality of our model<br \/> \n\n- As for cat73 to cat76 we have A, B and C. <br \/>\n\n- From cat77 to cat87 we have A, B, C and D.<br \/>\n\n- Cat88 has value A, B, D, E. <br \/>\n\n- Starting from cat89 we have more than 4 unique values for each categorical features. <br \/>\n"}}