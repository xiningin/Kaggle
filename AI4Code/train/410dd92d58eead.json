{"cell_type":{"6a6b617a":"code","312f97b3":"code","94bd5211":"code","0c578c7c":"code","6e9fff8c":"code","dfed11fa":"code","22740cb7":"code","0ba99140":"code","12e8f5d5":"code","4796c96d":"code","6982ce39":"code","ef1839ab":"code","cbe405cd":"code","0dd169dc":"code","3d147fc1":"code","f0282fb7":"code","9bb2dde2":"code","624f8fda":"code","e0d7de7d":"code","648e2083":"code","5d0b1b23":"code","fa6f47bf":"code","64c483bb":"code","41248702":"code","d0082b4f":"code","67bae554":"code","b61c9254":"code","5d4169b5":"code","6ab41c52":"code","367ec208":"code","dd082a2f":"code","5596b2b3":"code","9a054908":"code","30bd53c4":"code","bb36ce9d":"code","cdce2102":"code","e5034350":"code","69afcfbb":"code","c73bb589":"code","862c6333":"code","6e1fe16b":"code","cc7fdf25":"code","9dd7916b":"code","e1a0eea4":"code","c9c6d6d0":"code","dcf348ee":"code","8a5bb2cc":"code","071766d0":"code","bcc460ec":"code","610ec42b":"code","b71831b3":"code","794d952f":"code","9e239b09":"code","f79af2e5":"code","bcccd3bb":"code","011935c9":"code","16bc1d31":"code","dd5e3c23":"code","77dde30c":"code","8753dba8":"code","b3ecd9d5":"code","9f088009":"code","19df9cb3":"code","6e172714":"code","f5b37164":"code","fe0747a3":"code","d6d92333":"code","e6343cae":"code","16dbd95e":"code","55c49856":"code","8b7e8f80":"code","ceb90161":"code","79371f95":"code","895d74e7":"code","b12ffc47":"code","4ac79c04":"code","826f56ca":"code","5b6e9476":"code","df5b12a4":"code","8f2cf920":"code","7acb0f68":"code","116763a1":"code","5742a482":"code","3cad7238":"markdown","15f1e119":"markdown","db4f80aa":"markdown","4e08ed95":"markdown","72422aa6":"markdown","77c4eeb9":"markdown","0ac274e7":"markdown","63ed19a4":"markdown","2b1aa061":"markdown","cfbe9dd1":"markdown","c16201a7":"markdown","bcf1aae3":"markdown","57902d87":"markdown","1dfe7378":"markdown","295e1c8a":"markdown","49f2c7d9":"markdown","50da91d2":"markdown","c0e9ecc7":"markdown","2b5dc724":"markdown","00054fdf":"markdown","02b4cc85":"markdown","1965dbf3":"markdown","7b18c827":"markdown","ba15adc8":"markdown","2235803d":"markdown","8c56e4d3":"markdown","a7bef2b0":"markdown","1effde7c":"markdown","34916b69":"markdown","b598b452":"markdown","bfb0ae30":"markdown","040ec06a":"markdown","559bbebf":"markdown","66ad1e18":"markdown","fcd23a34":"markdown","2aadb932":"markdown","7b078ea0":"markdown","3ae5f619":"markdown","eae1c650":"markdown","a6c86c48":"markdown","95454c18":"markdown","a218ee11":"markdown","fbd78fff":"markdown","371d0ffd":"markdown","d4409f1c":"markdown","4fe0bf51":"markdown","f39dcf11":"markdown","3f9c388c":"markdown","7e1914b7":"markdown","1aca9ef3":"markdown","18754ffd":"markdown","fa80cb77":"markdown","7b1812ad":"markdown","7a243ff7":"markdown","858d5177":"markdown","0dea07ba":"markdown","fad70243":"markdown","87c34dcc":"markdown","8c8e8f8f":"markdown","ee8ddca1":"markdown","dd883ea8":"markdown","75a22b8f":"markdown","eac0fe61":"markdown","5b64a118":"markdown","00b7c5b0":"markdown","4c4ecf33":"markdown","2f050862":"markdown","d59b4d30":"markdown","567b02c6":"markdown","fdaacf62":"markdown","fb88d870":"markdown","c32fe855":"markdown","ebd26893":"markdown","a59d92ea":"markdown","9dd0fc0f":"markdown","2acf55ab":"markdown","49ef88e3":"markdown","64f39d24":"markdown","5da41592":"markdown","4afc5881":"markdown","51aa2022":"markdown","25cc13b9":"markdown"},"source":{"6a6b617a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","312f97b3":"#Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","94bd5211":"#import required modules\nfrom pandas_profiling import ProfileReport\nimport missingno as msno\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n#Pre-Processing libraries\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import FeatureUnion, Pipeline\n\n#Sk-Learn Models\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n#Sk-Learn Model Selection\nfrom sklearn.model_selection import cross_val_score, KFold, GridSearchCV, RandomizedSearchCV, train_test_split, StratifiedKFold\nfrom sklearn.feature_selection import RFECV, RFE\nfrom sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom scipy import stats","0c578c7c":"#Set Seaborn Theme\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", rc=custom_params)\nsns.set_style(\"darkgrid\")","6e9fff8c":"#Setting to display Pipeline\nfrom sklearn import set_config\nset_config(display=\"diagram\")","dfed11fa":"df = pd.read_csv('..\/input\/hr-dataset\/HR_Dataset.csv')","22740cb7":"#Visualize the dataframe\ndf.head()","0ba99140":"# profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n# profile.to_notebook_iframe()","12e8f5d5":"# Visualize missing values as a matrix\nmsno.matrix(df)","4796c96d":"# Renaming columns with a better description\ndf = df.rename(columns={'satisfaction_level': 'satisfaction', \n                        'last_evaluation': 'evaluation',\n                        'number_project': 'projectCount',\n                        'average_montly_hours': 'averageMonthlyHours',\n                        'time_spend_company': 'yearsAtCompany',\n                        'Work_accident': 'workAccident',\n                        'promotion_last_5years': 'promotion',\n                        'Departments ' : 'department',\n                        'left' : 'turnover'\n                        })","6982ce39":"# Moving the reponse variable \"turnover\" to the front of the table\nfront = df['turnover']\ndf.drop(labels=['turnover'], axis=1,inplace = True)\ndf.insert(0, 'turnover', front)\ndf.head()","ef1839ab":"# Check for numbe of duplicates in the dataset\nprint(df.duplicated().sum())","cbe405cd":"#Remove duplicates\ndf.drop_duplicates(inplace=True)","0dd169dc":"# Check for numbe of duplicates in the dataset\nprint(df.duplicated().sum())","3d147fc1":"# Turnover Percentage across the firm\n'{:.2%}'.format(df['turnover'].sum()\/df['turnover'].count())","f0282fb7":"#Calculate attrition rate by number of years spent at the firm\nattritionRate = df.groupby('yearsAtCompany')['turnover'].sum()\/df.groupby('yearsAtCompany')['turnover'].count()","9bb2dde2":"#View attrition rate\nattritionRate","624f8fda":"#Attrition rate by years spent at the firm\nsns.barplot(x=attritionRate.index, y=attritionRate)","e0d7de7d":"#Average satisfaction levels by years spent at the firm\navgSatisfactionLevels = df.groupby('yearsAtCompany')['satisfaction'].mean()\navgSatisfactionLevels","648e2083":"#Create a Dataframe to visualize the trend between Emp Satisfaction levels and attrition rate\ntempDf = pd.DataFrame({\"avgSatisfactionLevels\": avgSatisfactionLevels, \n        \"attritionRate\": attritionRate})","5d0b1b23":"#View attrition rate and average satisfaction levels by years spent at the firm\nsns.lineplot(data=tempDf)","fa6f47bf":"sns.kdeplot(data=df, x=\"satisfaction\", hue=\"turnover\", fill=True, common_norm=False,\n   alpha=.5, linewidth=0)\nplt.axvline(x=df.satisfaction.mean(),\n            color='red')","64c483bb":"#Calculate attrition rate by salary\nattritionRateBySalary = df.groupby('salary')['turnover'].sum()\/df.groupby('salary')['turnover'].count()","41248702":"attritionRateBySalary","d0082b4f":"#Plot attrition rate by salary\nsns.barplot(x=attritionRateBySalary.index, y=attritionRateBySalary, order=[\"low\",\"medium\",\"high\"])","67bae554":"f, ax = plt.subplots(figsize=(15, 4))\nsns.countplot(y=\"salary\", hue='turnover', data=df).set_title('Employee Salary Turnover Distribution');","b61c9254":"sns.displot(data=df, x=\"averageMonthlyHours\", binwidth=10, kde=True, hue=\"turnover\", col=\"salary\")","5d4169b5":"#Visualize scatter plot of satisfaction and evaluation \n\n#We create this scatterplot to understand how the combination of rating\/evaluation effects employee turnover\nsns.relplot(x=\"satisfaction\", y=\"evaluation\", col=\"turnover\", hue=\"salary\", data=df, kind=\"scatter\")\nplt.axvline(x=df.satisfaction.mean(),\n            color='red')\nplt.axhline(y=df.evaluation.mean(),\n            color='blue')","6ab41c52":"ax = sns.boxplot(x=\"turnover\", y=\"evaluation\", data=df)","367ec208":"#Average working hours in the firm\ndf['averageMonthlyHours'].mean()","dd082a2f":"#Percentage of leaving employees who worked more than the average monthly hours\ndf.loc[(df[\"turnover\"] == 1) & (df[\"averageMonthlyHours\"] > 200), [\"turnover\"]].count()\/df.loc[df[\"turnover\"]==1,[\"turnover\"]].count()","5596b2b3":"df.head()","9a054908":"sns.kdeplot(data=df, x=\"averageMonthlyHours\", hue=\"turnover\", fill=True, common_norm=False,\n   alpha=.5, linewidth=0)\nplt.axvline(x=df.averageMonthlyHours.mean(),\n            color='red')\nplt.show","30bd53c4":"#Visualize scatter plot of satisfaction and evaluation \n\n#We create this scatterplot to understand how the combination of rating\/evaluation effects employee turnover\nsns.relplot(x=\"satisfaction\", y=\"averageMonthlyHours\", col=\"turnover\", data=df, kind=\"scatter\")\nplt.axvline(x=df.satisfaction.mean(),\n            color='red')\nplt.axhline(y=df.averageMonthlyHours.mean(),\n            color='blue')","bb36ce9d":"#Calculate average attrition by department\navgAttrition = (df.groupby(['department'])['turnover'].sum()\/df.groupby(['department'])['turnover'].count()).sort_values(ascending=False)","cdce2102":"avgAttrition","e5034350":"#Plot average attrition by department\nsns.barplot(y=avgAttrition.index, x=avgAttrition)\nplt.axvline(x=avgAttrition.mean(),\n            color='red')","69afcfbb":"avgMonthlyHours = df.groupby(['department'])['averageMonthlyHours'].mean()\navgMonthlyHours","c73bb589":"# Create scatterplot for avgMonthlyHours and avgAttrition\nsns.scatterplot(x=avgMonthlyHours, y=avgAttrition, hue=avgMonthlyHours.index)\n\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","862c6333":"from scipy.stats import pearsonr\n#Calculate correlation coefficient between turnover and averageMonthlyHours\ncorr, _ = pearsonr(avgMonthlyHours, avgAttrition)\nprint('Pearsons correlation: %.3f' % corr)","6e1fe16b":"temp = df.loc[df[\"turnover\"] == 1,:].groupby('department')['averageMonthlyHours', 'satisfaction'].mean()","cc7fdf25":"temp","9dd7916b":"sns.scatterplot(x=temp[\"averageMonthlyHours\"], y=temp[\"satisfaction\"], hue=[0,0,0,1,0,0,0,1,1,1])","e1a0eea4":"#Distribution of average working hours by department\nax = sns.pointplot(y=\"department\", x=\"averageMonthlyHours\", data=df)","c9c6d6d0":"sns.kdeplot(\n   data=df, x=\"projectCount\", hue=\"turnover\",\n   fill=True, common_norm=False, palette=\"crest\",\n   alpha=.5, linewidth=0,\n)","dcf348ee":"f, ax = plt.subplots(figsize=(15, 4))\nsns.countplot(y=\"projectCount\", hue='turnover', data=df).set_title('Employee Salary Turnover Distribution');","8a5bb2cc":"#Split the train and test data\ntrain, test =  train_test_split(df,test_size=0.2, random_state=123, stratify=df['turnover'])\nprint(train.shape)\nprint(test.shape)","071766d0":"#Get the list of categorical columns for Dummy Encoding \ndummyColList = ['department', 'salary']","bcc460ec":"#Get the list of numerical columns for Scaling\nnumCols = ['satisfaction', 'evaluation', 'projectCount', 'averageMonthlyHours', 'yearsAtCompany']","610ec42b":"#Custom Transformer that transforms Categorical columns\nclass DummyEncoding( BaseEstimator, TransformerMixin):\n    \n    #Class Constructor\n    def __init__(self, dummyColList):\n        self.dummyColList = dummyColList\n        \n    #Return self nothing else to do here\n    def fit( self, X, y = None  ):\n        return self\n    \n    #Transformer method we wrote for this transformer \n    def transform(self, X , y = None ):\n        \n        #One-Hot Encoding of categorical columns\n        #Get dummy variables\n        for each_col in self.dummyColList:\n            X[each_col] = pd.factorize(X[each_col])[0]\n        \n        return X","b71831b3":"#Custom Transformer that scales Numerical columns\nclass CustomScaler( BaseEstimator, TransformerMixin):\n    \n    #Class Constructor\n    def __init__(self, numCols):\n        self.numCols = numCols\n        self.scaler = MinMaxScaler()\n        \n    #Return self nothing else to do here\n    def fit( self, X, y = None  ):\n        self.scaler.fit(X[numCols])\n        return self\n    \n    #Transformer method we wrote for this transformer \n    def transform(self, X , y = None ):\n        \n        X[numCols] = self.scaler.transform(X[numCols])\n        \n        return X","794d952f":"#Defining the steps in the categorical pipeline \ncategorical_pipeline = Pipeline( steps = [('dummyEncodingTransformer', DummyEncoding(dummyColList))] )","9e239b09":"#Defining the steps in the numerical pipeline     \nnumerical_pipeline = Pipeline( steps = [ ( 'min_max_scaler', CustomScaler(numCols)) ] )","f79af2e5":"#Combining numerical and categorical piepline into one full big pipeline horizontally \n#using FeatureUnion\npreProcessingPipeline = Pipeline( steps = [ ( 'categorical_pipeline', categorical_pipeline ), \n                                                  \n                                                  ( 'numerical_pipeline', numerical_pipeline ) ] )\n\n","bcccd3bb":"preProcessingPipeline","011935c9":"#Fit_transform the pipeline on training data\ntrain_transform = preProcessingPipeline.fit_transform(train)","16bc1d31":"#Transform the pipeline on test data\ntest_transform = preProcessingPipeline.transform(test)","dd5e3c23":"# Putting response variable to y\ny_train = train_transform.pop('turnover')\nX_train = train_transform","77dde30c":"# Putting response variable to y\ny_test = test_transform.pop('turnover')\nX_test = test_transform","8753dba8":"sm = SMOTE(random_state=2)\nX_train, y_train = sm.fit_resample(X_train, y_train.ravel())","b3ecd9d5":"#Hyper Parameters for different models \nparamsLogReg = {\n                'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n                'C' : [100, 10, 1.0, 0.1, 0.01],\n                'solver' : ['lbfgs','newton-cg','liblinear','sag','saga']\n                }\n\nparamsRidge = {\"alpha\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\nparamsSGD = {}\nparamsNB = {'var_smoothing': np.logspace(0,-9, num=100)}\nparamsKNN = {\n            'n_neighbors' : range(1,21,2),\n            'weights' : ['uniform','distance'],\n            'metric' : ['minkowski','euclidean','manhattan']\n            }\n\nparamsDT = {\n            'max_depth': [2, 3, 5, 10, 20],\n            'min_samples_leaf': [5, 10, 20, 50, 100],\n            'criterion': [\"gini\", \"entropy\"]\n            }\n\nparamsRF = {'n_estimators': [25, 50, 100, 150, 200], \n            'max_depth': [3, 5, 7, 9], \n            'max_features': [\"auto\", \"sqrt\", \"log2\"], \n            'random_state': [42]\n           }\n\nparamsBC = {\n            \"n_estimators\": [10, 100, 1000]\n            }\n\nparamsGBC = {\n            \"n_estimators\": [10, 100, 1000],\n            \"learning_rate\": [0.001, 0.01, 0.1],\n            \"subsample\": [0.5, 0.7, 1.0],\n            \"max_depth\": [3, 7, 9]\n            }\n\n","9f088009":"#List of models to evaluate\nmodels = [{\"modelName\": \"LogisticRegression\", \n           \"model\": LogisticRegression(), \n           \"modelAvgCVScore\": 10, \n           \"modelParams\": paramsLogReg, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n          \n          {\"modelName\": \"Ridge\", \n           \"model\": RidgeClassifier(), \n           \"modelAvgCVScore\": 11, \n           \"modelParams\": paramsRidge, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []},\n          \n          {\"modelName\": \"SGD\", \n           \"model\": SGDClassifier(), \n           \"modelAvgCVScore\": 9, \n           \"modelParams\": paramsSGD, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n          \n          {\"modelName\": \"NaiveBayes\", \n           \"model\": CategoricalNB(), \n           \"modelAvgCVScore\": 0, \n           \"modelParams\": paramsNB, \n           \"cvResults\": [], \n           \"rfeStatus\": False, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n          \n          {\"modelName\": \"KNN\", \n           \"model\": KNeighborsClassifier(), \n           \"modelAvgCVScore\": 0, \n           \"modelParams\": paramsKNN, \n           \"cvResults\": [], \n           \"rfeStatus\": False, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n          \n          {\"modelName\": \"DecisionTree\", \n           \"model\": DecisionTreeClassifier(), \n           \"modelAvgCVScore\": 0, \n           \"modelParams\": paramsDT, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n        \n          {\"modelName\": \"RandomForest\", \n           \"model\": RandomForestClassifier(n_estimators = 100), \n           \"modelAvgCVScore\": 0, \n           \"modelParams\": paramsRF, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n          \n          {\"modelName\": \"BaggingClassifier\", \n           \"model\": BaggingClassifier(), \n           \"modelAvgCVScore\": 0, \n           \"modelParams\": paramsBC, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n          \n          {\"modelName\": \"BoostingClassifier\", \n           \"model\": GradientBoostingClassifier(), \n           \"modelAvgCVScore\": 0, \n           \"modelParams\": paramsGBC, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          }\n          \n         ]","19df9cb3":"#Minimun number of features for RFE\nmin_features = 2\n\n#Maximum number of features for RFE\nmax_features = 9\n\n#Step size in RFECV\nrfecvStep = 1\n\n#Perform manual RFE\nrfeStatus = False\n\n#Perform Auto RFE\nrfeCVStatus = True","6e172714":"#Scoring parameter\nscoring = \"roc_auc\"\n\n#Number of splits in K-Fold Cross Validation\nn_splits = 5\n\n#Random state\nrandom_state = 23\n\n#Shuffle in K-Fold cross validation\nshuffle = True","f5b37164":"class Models:\n    \n    #Init Function\n    def __init__(self, models, min_features, max_features, rfecvStep, scoring, n_splits, random_state, shuffle, rfeStatus, rfeCVStatus):\n        \n        self.models = models\n        self.min_features = min_features\n        self.max_features = max_features\n        self.scoring = scoring\n        self.n_splits = n_splits\n        self.random_state = random_state\n        self.shuffle = shuffle\n        self.rfecvStep = rfecvStep\n        self.rfeStatus = rfeStatus\n        self.rfeCVStatus = rfeCVStatus\n    \n    #K-Fold Cross validation\n    def crossValidation(self, model, X, y):\n        #Instantiate KFold\n        kfold = KFold(n_splits=self.n_splits, random_state=self.random_state, shuffle=self.shuffle)\n        return cross_val_score(model, X, y, cv=kfold, scoring=self.scoring)\n    \n    #Function to runModels\n    def runModels(self, X, y):\n        \n        print(\"Starting to execute and compare various models...\")\n        #Loop over each model and do K-Fold Cross validation to select the best model\n        for each_model in self.models:\n            print(\"Running {}\".format(each_model[\"modelName\"]))\n            \n            #List of CV results of each model\n            each_model[\"cvResults\"] = self.crossValidation(each_model[\"model\"], X, y)\n            \n            #Average CV score\n            each_model[\"modelAvgCVScore\"] = each_model[\"cvResults\"].mean()\n\n        print(\"All models successfully executed\")\n    \n    #Function to compare Model results        \n    def compareModels(self, results, names):\n        \n        #Print results of various models\n        for each_result, modelName in zip(results, names):\n            print(\"Model Name: %s:\" % (modelName))\n            print(\"Model Average CV Score: %f\" % (each_result.mean()))\n            print(\"Model CV Std (%f)\" % (each_result.std())) \n            print(\"CV_Results: \", (each_result))\n            print(\"\\n\")\n        \n        # Box Plot of Model Results\n        fig = plt.figure()\n        fig.suptitle('Algorithm Comparison')\n        ax = fig.add_subplot(111)\n        plt.boxplot(results)\n        ax.set_xticklabels(names)\n        plt.xticks(rotation=45)\n        plt.show()\n        \n    #Get shortlisted Model\n    def getShortlistedModel(self):\n        \n        #Get shortlisted model based on highest average CV Score\n        self.shortlistedModel = max(self.models, key=lambda x:x['modelAvgCVScore'])\n        \n        print(\"Shortlisted model is : \", self.shortlistedModel)\n            \n            \n    #Function to run RFE models and return the CV results\n    def runRFEModels(self, X, y):\n        \n        results = []\n        \n        print(\"These are the models: \", self.shortlistedModel[\"rfeModels\"])\n        \n        if ((self.rfeStatus == True) and (self.shortlistedModel[\"rfeStatus\"] == True)):\n        \n            #Loop over the rfe models for the shortlisted model\n            for each_model in self.shortlistedModel[\"rfeModels\"]:\n            \n                #CV results of each model\n                cvResults = self.crossValidation(each_model, X, y)\n                print(\"Average score is : \", cvResults.mean())\n            \n                results.append(cvResults)\n            \n        return results\n            \n            \n    #Get the list of RFE Models\n    def getRfeModels(self):\n\n        #Perform RFE on the model\n        for i in range(self.min_features, self.max_features):\n            rfe = RFE(estimator=self.shortlistedModel[\"model\"], n_features_to_select=i)\n            self.shortlistedModel[\"rfeModels\"].append(Pipeline(steps=[('selection',rfe),('model',self.shortlistedModel[\"model\"])]))\n            self.shortlistedModel[\"rfeInputFeaturesCount\"].append(i)\n\n                    \n    #Set optimial number of features based on RFE Model\n    def getOptimalNumFeatures(self):\n#         self.shortlistedModel[]\n\n        #It is an array of arrays with each nested array containing CV results\n        results = modelObj.shortlistedModel[\"RFECVResults\"]\n        meanResults = [x.mean() for x in results]\n        maxpos = meanResults.index(max(meanResults)) \n\n        self.optimalFeatures =  list(range(self.min_features, self.max_features))[maxpos]\n\n    \n    #Run shortlisted RFE Model with optimal number of features to actually get feature names\n    def getBestFeatures(self, X, y):\n        \n        \n        rfe = RFE(estimator=self.shortlistedModel[\"model\"], n_features_to_select=self.optimalFeatures)\n        self.pipe = Pipeline(steps=[('selection',rfe), ('model', self.shortlistedModel[\"model\"])])\n        self.pipe.fit(X, y)\n\n        # summarize all features\n        print(\"Summarizing the results of RFE\")\n        for each_col, col_num in zip(X.columns, list(range(X.shape[1]))):\n            print('ColumnName: %s | Selected %s | Rank: %.3f' % (each_col, rfe.support_[col_num], rfe.ranking_[col_num]))\n\n\n        print(\"\\n\")\n        print(\"List of selected columns\")\n        print(list(zip(X.columns,rfe.support_,rfe.ranking_)))\n\n        #Visualize the selected columns\n        print(\"\\n\")\n        print(\"Visualize selected columns\")\n        print(X.columns[rfe.support_])\n\n        return X.columns[rfe.support_]\n        \n    \n    #Run RFECV Model to find the names of optimal features\n    def getBestFeaturesCV(self, X, y):\n\n        rfecv = RFECV(estimator=self.shortlistedModel[\"model\"], step=self.rfecvStep, cv=self.n_splits)\n\n        self.pipe = Pipeline(steps=[('selection',rfecv), ('model', self.shortlistedModel[\"model\"])])\n        self.pipe.fit(X, y)\n\n        # summarize all features\n        print(\"Summarizing the results of RFE\")\n        for each_col, col_num in zip(X.columns, list(range(X.shape[1]))):\n            print('ColumnName: %s | Selected %s | Rank: %.3f' % (each_col, rfecv.support_[col_num], rfecv.ranking_[col_num]))\n\n\n        print(\"\\n\")\n        print(\"List of selected columns\")\n        print(\"this is rfecv support: \", rfecv.support_)\n        print(list(zip(X.columns,rfecv.support_,rfecv.ranking_)))\n\n        #Visualize the selected columns\n        print(\"\\n\")\n        print(\"Visualize selected columns\")\n        print(X.columns[rfecv.support_])\n\n        return X.columns[rfecv.support_]\n    \n        \n    def getParams(self, modelName):\n        \n        for each_param in self.params:\n            \n            if each_param[\"modelName\"] == modelName:\n                return each_param[\"modelParams\"]\n            \n            else:\n                return 0\n               \n    def hyperParameterOptimization(self):\n        # Instantiate the grid search model\n        grid_search = RandomizedSearchCV(estimator=self.shortlistedModel[\"model\"], \n                           param_distributions=self.shortlistedModel[\"modelParams\"], \n                           cv=self.n_splits, n_jobs=-1, verbose=1, scoring = self.scoring)\n        \n        grid_search.fit(X_train, y_train)\n        \n        return grid_search.best_estimator_\n    \n    def runFinalModel(self):\n        pass\n    \n    def evaluateModel(self):\n        pass","fe0747a3":"#Instantiate Models class\nmodelObj = Models(models, min_features, max_features, rfecvStep, scoring, n_splits, random_state, shuffle, rfeStatus, rfeCVStatus)","d6d92333":"#Run the models and print score\nmodelObj.runModels(X_train, y_train)","e6343cae":"#Store CV results and names of various models\nresults = [x[\"cvResults\"] for x in modelObj.models]\nnames = [x[\"modelName\"] for x in modelObj.models]","16dbd95e":"#Plot the results\nmodelObj.compareModels(results, names)","55c49856":"#Set the shortlisted Model\nmodelObj.getShortlistedModel()","8b7e8f80":"#Get the List of RFE Models based on number of features and the shortlisted model\n\nif ((modelObj.rfeStatus == True) and (modelObj.shortlistedModel[\"rfeStatus\"] == True)):\n   \n    #Get RFE Models\n    modelObj.getRfeModels()\n    \n    #Run RFE Models and get rfeCVResults\n    modelObj.shortlistedModel[\"RFECVResults\"] = modelObj.runRFEModels(X_train, y_train)\n    \n    #Compare RFE Model results\n    modelObj.compareModels(modelObj.shortlistedModel[\"RFECVResults\"], list(range(modelObj.min_features, modelObj.max_features)))\n    \n    #Select the best performing RFE Model and number of features\n    modelObj.getOptimalNumFeatures()\n    \n    #Get the names of selected columns by RFE\n    selectColumns = modelObj.getBestFeatures(X_train, y_train)","ceb90161":"#View the shortlisted Model\nmodelObj.shortlistedModel","79371f95":"#Check if rfeCVStatus == True\nif ((modelObj.rfeCVStatus == True) and (modelObj.shortlistedModel[\"rfeStatus\"] == True)):\n    selectColumns = modelObj.getBestFeaturesCV(X_train, y_train)","895d74e7":"#If we are doing Recursive Feature Elimination\nif (((modelObj.rfeStatus == True) or (modelObj.rfeCVStatus == True)) and (modelObj.shortlistedModel[\"rfeStatus\"] == True)):\n    X_train_rfe = X_train[selectColumns]\n    X_test_rfe = X_test[selectColumns]\n    \nelse:\n    X_train_rfe = X_train\n    X_test_rfe = X_test","b12ffc47":"X_train_rfe","4ac79c04":"params = {\n    'max_depth': [2, 3, 5, 10, 20],\n    'min_samples_leaf': [5, 10, 20, 50, 100],\n    'criterion': [\"gini\", \"entropy\"]\n}","826f56ca":"modelObj.shortlistedModel[\"model\"]","5b6e9476":"finalModel = modelObj.hyperParameterOptimization()","df5b12a4":"#Visualize the Final Model\nfinalModel","8f2cf920":"#Fit the final Model\nfinalModel.fit(X_train_rfe, y_train)","7acb0f68":"#Predictions on Final Model\ny_test_pred = finalModel.predict(X_test_rfe)","116763a1":"print(accuracy_score(y_test, y_test_pred))\nconfusion_matrix(y_test, y_test_pred)","5742a482":"cm = confusion_matrix(y_test, y_test_pred, labels=finalModel.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=finalModel.classes_)\ndisp.plot()\n\nplt.show()","3cad7238":"## End of notebook","15f1e119":"-------","db4f80aa":"# 12. Models Class","4e08ed95":"### 10.3 Pre-Processing pipeline","72422aa6":"-------","77c4eeb9":"**From Pearson Correlation coefficient of -0.111, we note that there is not a \nsignificant correlation between working long hours and attrition rate**","0ac274e7":"### Key takeaways for the HR Department\n- Why a group of hard working and highly satisfied employees left their job?","63ed19a4":"### 9.4 ->  As expected, attrition rate is high for employees with low salaries","2b1aa061":"Let's explore the dataset and try to find some insights in the dataset. Some initial questions that can be explored \n- Turnover percentage across the firm\n- Employee Attrition by years at company","cfbe9dd1":"### 12.1 Models Class\nThe Models Class provides the following functionalities\n- Initialize various Classification models along with respective hyper-parameters\n- Run and compare model performance\n- Select the best performing model\n- Perform RFE on the best performing model\n- Select the best features\n- Run the final model","c16201a7":"### Key takeaways for the HR Department\n- Why average satisfaction levels are low for employees in the 2-4 years range?\n- Could low satisfaction levels be the reason for high attrition in the 2-4 years range?\n- Why are employees with high satisfaction leaving the job? (May be better salary?)","bcf1aae3":"### 10.1 Create train-test split","57902d87":"------","1dfe7378":"--------","295e1c8a":"# 17. Model Tuning - Hyperparameter optimization","49f2c7d9":"### 9.2 ->  Attrition rate is maximum for employees with 5 years of experience at the firm","50da91d2":"### 8.3 Check and remove duplicates","c0e9ecc7":"### 9.5 -> We note that there are 3 clusters of employees leaving the firm\n- Top rated, Less satisfaction\n- Top rated, Highly satisfaction\n- Bottom rated, below average satisfaction","2b5dc724":"### Key findings\n- Average satisfaction levels dip for employees during 2 - 4 years stay at the firm\n- Low satisfaction levels could be the reason for higher attrition rate in the employees with 2 - 5 years of stay bracket.\n- Attrition distribution is tri-modal for employee satisfaction levels.\n    - Certain Employees with `low` satisfaction\n    - Majority of the Employees with `medium` satisfaction\n    - Certain Employees with `high` satisfaction","00054fdf":"# 2. Goal","02b4cc85":"# HR Analytics - Predicting Employee Turnover","1965dbf3":"### 7.1 Check for missing values","7b18c827":"-------","ba15adc8":"-------","2235803d":"## 5. Load the required libraries","8c56e4d3":"# 7. Overview of the dataset","a7bef2b0":"# 3. Key Findings","1effde7c":"-----","34916b69":"# 6. Load the dataset","b598b452":"### Key takeaways for the HR Department\n- Why were certain `Top rated` employees not satisfied with the firm?\n    - Were they overworked?\n    - Were they not satisfied with their job?\n    - Was salary an issue?\n    \n- Why are `Top Rated` and `Highly satisfied employees` leaving the firm?\n    - Are they looking for better opportunities?\n    - Is salary or growth an issue for these set of employees?\n    - Are they overworked?\n    \n- How can we help underperforming and unsatisfied employees?\n    - Can we identify training gaps?\n    - Is there some issue with our hiring policy, did we hire people with skills mismatch?","bfb0ae30":"### 10.2 Custom Pre-Processing Transformers","040ec06a":"### 10.4 Extract X and y","559bbebf":"# 13. Shortlist top performing model","66ad1e18":"# 9. Exploratory Data Analysis","fcd23a34":"\n<blockquote>\n    There is <b>16.60% Employee attrition<\/b> at the firm.\n<\/blockquote>","2aadb932":"### Key takeaways for the HR Department\n- Benchmark salaries of low salaries staff to check whether salary can be improved for certain staff\n- Why are certain high salaried employees leaving the firm?","7b078ea0":"# 8. Data manipulation","3ae5f619":"# 19. Evaluate Final Model","eae1c650":"---------","a6c86c48":"**Project Name**: HR Analytics - Predicting Employee Turnover\n\n**Author**: Ankur Dhamija\n\n**Connect on Linkedin**: https:\/\/www.linkedin.com\/in\/ankurdhamija\/","95454c18":"# 18. Final Model","a218ee11":"-------","fbd78fff":"*The goal is to understand what factors contribute most to employee turnover and create a model that can predict if a certain employee will leave the company or not.*","371d0ffd":"## 4. Approach\n- Load the required libraries\n- Load the dataset\n- Overview of the dataset\n- Data Manipulation\n- Exploratory Data Analysis\n- Data Pre-Processing\n- Model Selection\n- Recursive Feature Elimination\n- Select Features\n- Model Tuning - Hyperparameter optimization\n- Final Model\n- Evaluate Final Model","d4409f1c":"---------","4fe0bf51":"### 8.2 Move target variable to the beginning of the dataset","f39dcf11":"### 10.5 Class Imbalance treatment - SMOTE","3f9c388c":"### 9.1 Employee Attrition","7e1914b7":"--------","1aca9ef3":"With the evolution of Data Science, HR Analytics is having a significant impact on the HR policies of various organizations.\n\nIt is very important that companies recognize and understand what factors are associated with employee turnover. This will allow pro-active action and limit employee attrition at various firms.\n\nPredictive analytics give managers the opportunity to take corrective steps to build and preserve their successful business.","18754ffd":"### 12.4 Compare model performance","fa80cb77":"# 1. HR Analytics","7b1812ad":"**Key Findings**","7a243ff7":"------","858d5177":"### Key Findings\n- We see that maximum attrition is when employees have spent 5 years with the firm. \n- Another interesting thing to note is that attrition rises gradually from 2 years duration and peaks at 5 years.\n- Attrition falls sharply after has employee has spent more than 5 years with the firm\n\nMay be its the case that there is some `retention bonus paid at 5 years employment` and few employees\n`leave immediately after taking the the bonus`.\n\nOr it could also be the case that the retention bonus is `conditional`. \nEmployees who don't get the `bonus leave` and those who get the bonus `continue to stay` with the firm.","0dea07ba":"### 9.6 -> 54% of the employees who left the firm worked more than the average monthly hours (200 hours)\n- Those employees worked hard and were highly satisfied with their job","fad70243":"### 11.3 RFE\/RFECV Inputs","87c34dcc":"### 9.8 -> Attrition rate is high for employees in the under-worked and over-worked segment","8c8e8f8f":"### Key takeaways for the HR Department\n- How can the workload be balance across employees?","ee8ddca1":"# 15. RFECV","dd883ea8":"### 11.2 Models List","75a22b8f":"### 11.4 Cross-Validation Inputs","eac0fe61":"### Key findings\n- Nearly 20% attrition rate in employees with low salaries\n- Certain high salaries staff is also leaving the company","5b64a118":"### 12.3 Run multiple models","00b7c5b0":"-------","4c4ecf33":"### 8.1 Rename columns","2f050862":"------","d59b4d30":"- There is 16.60% Employee attrition at the firm\n- Attrition rate is maximum for employees with `5 years of experience` at the firm\n- Employees with `3 to 5 years of experience` reported `lower satisfaction levels` on average\n- As expected, attrition rate is high for employees with `low salaries`\n- We note that there are `3 clusters of employees` leaving the firm\n    - Top rated, Less satisfaction\n    - Top rated, Highly satisfaction\n    - Bottom rated, below average satisfaction\n- `54% of the employees` who left the firm worked more than the average monthly hours (200 hours)\n- Attrition rate is high for employees in the `under-worked` and `over-worked segment`\n    -  Employees left when they are underworked (less than 150hr\/month or 6hr\/day)\n    -  Employees left when they are overworked (more than 250hr\/month or 10hr\/day)","567b02c6":"## 11. Model Inputs","fdaacf62":"# 10. Data Pre-Processing (Dummy variables, Test-Train split and Feature Scaling)","fb88d870":"**We note that there is no specific department which is overburdened and working very long hours**","c32fe855":"# 14. Recursive Feature Elimination","ebd26893":"### 11.1 Hyper Parameters for the model","a59d92ea":"### 9.3 ->  Employees with 3 to 5 years of experience reported lower satisfaction levels on average","9dd0fc0f":"-----","2acf55ab":"### 12.2 Instantiate Models class","49ef88e3":"# 16. Feature Selection","64f39d24":"### Key takeaways for the HR Department\n- Why attrition is rising gradually for employees in 2-5 years range?\n- 70% of the total attrition comes from employees in 4-5 years range\n- Can we use some retention policies that we have for employees in >6 years range to lower retention in 4-5 years range?","5da41592":"-------","4afc5881":"### 9.7 -> Attrition by department","51aa2022":"--------","25cc13b9":"-----"}}