{"cell_type":{"157a9016":"code","36b315d9":"code","20b8110f":"code","ac19f4e7":"code","820dc1eb":"code","c5cced2f":"code","bbe50526":"code","e0bdaa0e":"code","1997ac79":"code","8eb9519f":"code","af79c34d":"code","5ba85107":"code","db5a96b6":"code","c89f5b16":"markdown","a70fb63a":"markdown","30d05eb6":"markdown","fca94f29":"markdown","521c6cc8":"markdown","c9f56cfd":"markdown","4b097025":"markdown","cad28368":"markdown","3e6b3b32":"markdown","e330514e":"markdown","56c47701":"markdown","73778113":"markdown","18ec929a":"markdown","977f9a39":"markdown","917ff767":"markdown","10de76e9":"markdown","4b9eeef5":"markdown","449f7850":"markdown","488a4c50":"markdown"},"source":{"157a9016":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport json\nimport glob\nprint(tf.__version__)","36b315d9":"d_raw = np.load('..\/input\/g2net-gravitational-wave-detection\/test\/0\/0\/2\/0021f9dd71.npy')\nd = d_raw[0,:]\/np.max(d_raw[0,:])\nplt.plot(d);","20b8110f":"%%writefile tf_cwt.py\n# Based on Alexander Neergaard Olesen's CWT Python implementation. https:\/\/github.com\/neergaard\/CWT\n# Adapted to Keras by Geir Drange\n# MIT License\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport math\n\n# calculate CWT of input signal\nclass Wavelet1D(keras.layers.Layer):\n    def __init__(self, nv=12, sr=1., flow=0., fhigh=0.5, batch_size=None, trainable=False):\n        super(Wavelet1D, self).__init__()\n        assert fhigh > flow, 'fhigh parameters must be > flow!'\n        assert batch_size != None, 'batch size must be set!'\n        \n        self.batch_size = batch_size \n        self.nv = nv # number of voices\n        self.sr = sr # sample rate (Hz)\n        self.flow = flow # lowest frequency of interest (Hz)\n        self.fhigh = fhigh # highest frequency of interest (Hz)\n        self.trainable = trainable # True to train the wavelet filter bank\n\n    def build(self, input_shape):\n        assert len(input_shape) == 2, 'Input dimension must be 2! Dimension is {}'.format(len(input_shape))\n        \n        max_scale = input_shape[-1] \/\/ (np.sqrt(2) * 2)\n        if max_scale <= 1:\n            max_scale = input_shape[-1] \/\/ 2\n        max_scale = np.floor(self.nv * np.log2(max_scale))\n        scales = 2 * (2**(1\/self.nv)) ** np.arange(0, max_scale + 1)\n        frequencies = self.sr * (6 \/ (2 * np.pi)) \/ scales\n        frequencies = frequencies[frequencies >= self.flow] # remove low frequencies\n        scales = scales[0:len(frequencies)]\n        frequencies = frequencies[frequencies <= self.fhigh] # remove high frequencies\n        scales = scales[len(scales)-len(frequencies):len(scales)]\n        # wavft\n        padvalue = input_shape[-1] \/\/ 2\n        n = padvalue*2+input_shape[-1]\n        omega = np.arange(1, math.floor(n \/ 2) + 1, dtype=np.float64)\n        omega = omega*(2 * np.pi) \/ n\n        omega = np.concatenate((np.array([0]), omega, -omega[np.arange(math.floor((n - 1) \/ 2), 0, -1, dtype=int) - 1]))\n        _wft = np.zeros([scales.size, omega.size])\n        for jj, scale in enumerate(scales):\n            expnt = -(scale * omega - 6) ** 2 \/ 2 * (omega > 0)\n            _wft[jj, ] = 2 * np.exp(expnt) * (omega > 0)\n        # parameters we want to use during call():\n        self.wft = tf.Variable(_wft, trainable=self.trainable) # yes, the wavelets can be trainable if desired\n        self.padvalue = padvalue\n        self.num_scales = scales.shape[-1]\n        self.n = n\n    \n    def call(self, inputs):\n        x = tf.concat((tf.reverse(inputs[:,0:self.padvalue], axis=[1]), inputs, tf.reverse(inputs[:,-self.padvalue:], axis=[1])), axis=1)\n        f = tf.signal.fft(tf.cast(x, tf.complex64))\n        fr = tf.repeat(f, self.num_scales, axis=0)\n        fr = tf.reshape(fr, (self.batch_size, self.num_scales, self.n))\n        cwtcfs = tf.signal.ifft(fr * tf.cast(self.wft, tf.complex64))\n        cfs = cwtcfs[:, :, self.padvalue:self.padvalue + inputs.shape[-1]]\n        return tf.math.log(tf.math.abs(cfs))\n    \n# scale input to range 0.0 - upper\nclass Scaler(keras.layers.Layer):\n    def __init__(self, upper=1.0):\n        super(Scaler, self).__init__()\n        self.upper = tf.cast(upper, dtype=tf.float32) # upper value (typically 1.0 or 255.0 for image CNNs)\n    \n    def call(self, inputs):\n        min_val = tf.math.reduce_min(inputs)\n        max_val = tf.math.reduce_max(tf.math.subtract(inputs, min_val))\n        return tf.math.multiply(tf.math.subtract(inputs, min_val), self.upper\/max_val)\n    \n# Stack three channels into RGB image\nclass RGBStack(keras.layers.Layer):\n    def __init__(self):\n        super(RGBStack, self).__init__()\n        \n    def call(self, inputs):\n        return tf.stack(inputs, axis = 3)","ac19f4e7":"from tf_cwt import Wavelet1D, Scaler, RGBStack","820dc1eb":"fig = plt.figure(figsize=(16,22))\nidx = 0\nfor nv in [8,16]:\n    for flow in [8,32]:\n        for fhigh in [500, 1000]:\n            y = Wavelet1D(nv=nv, sr=2048., flow=flow, fhigh=fhigh, batch_size=1)(tf.expand_dims(d, axis=0))\n            y = Scaler(upper=1)(y)\n            ax = plt.subplot(4, 2, 1+idx)\n            plt.xlabel('Sample')\n            plt.ylabel('Scale')\n            plt.title('Voices: {}, Flow: {}Hz, Fhigh: {}Hz'.format(nv, flow, fhigh))\n            plt.imshow(np.squeeze(y.numpy()), cmap='magma', aspect='auto') \n            idx += 1","c5cced2f":"TARGET_IMG_SIZE = 224\nSTRIDE = int(np.ceil(4096\/TARGET_IMG_SIZE))\n\ny1 = Wavelet1D(nv=16, sr=2048., flow=20, fhigh=750, batch_size=1)(tf.expand_dims(d, axis=0))\ny1 = Scaler(upper=1)(y1)\ny1 = tf.reshape(y1, [y1.shape[0],y1.shape[1],y1.shape[2],1])\nfig = plt.figure(figsize=(10,12))\nax = plt.subplot(3, 1, 1)\ny2a = MaxPool2D(pool_size=(1,STRIDE), strides=(1,STRIDE))(y1)\ny2a = Scaler(upper=1)(y2a)\nplt.title('Max Pooling')\nplt.imshow(tf.reshape(y2a, [y2a.shape[1],y2a.shape[2]]).numpy(), cmap='magma', aspect='auto')\nax = plt.subplot(3, 1, 2)\ny2b = AvgPool2D(pool_size=(1,STRIDE), strides=(1,STRIDE))(y1)\ny2b = Scaler(upper=1)(y2b)\nplt.title('Average Pooling')\nplt.imshow(tf.reshape(y2b, [y2b.shape[1],y2b.shape[2]]).numpy(), cmap='magma', aspect='auto') \nax = plt.subplot(3, 1, 3)\ny2c = Conv2D(filters=1, kernel_size=(1,STRIDE), strides=(1,STRIDE))(y1)\ny2c = Scaler(upper=1)(y2c)\nplt.title('2D Convolution (untrained)')\nplt.imshow(tf.reshape(y2c, [y2c.shape[1],y2c.shape[2]]).numpy(), cmap='magma', aspect='auto');","bbe50526":"r = Wavelet1D(nv=16, sr=2048., flow=20, fhigh=750, batch_size=1)(tf.expand_dims(d_raw[0,:], axis=0))\nr = AvgPool2D(pool_size=(1,STRIDE), strides=(1,STRIDE))(tf.reshape(r, [r.shape[0],r.shape[1],r.shape[2],1]))\nr = Scaler(upper=1)(r)\nr = tf.reshape(r, [r.shape[1],r.shape[2]])\ng = Wavelet1D(nv=16, sr=2048., flow=20, fhigh=750, batch_size=1)(tf.expand_dims(d_raw[1,:], axis=0))\ng = AvgPool2D(pool_size=(1,STRIDE), strides=(1,STRIDE))(tf.reshape(g, [g.shape[0],g.shape[1],g.shape[2],1]))\ng = Scaler(upper=1)(g)\ng = tf.reshape(g, [g.shape[1],g.shape[2]])\nb = Wavelet1D(nv=16, sr=2048., flow=20, fhigh=750, batch_size=1)(tf.expand_dims(d_raw[2,:], axis=0))\nb = AvgPool2D(pool_size=(1,STRIDE), strides=(1,STRIDE))(tf.reshape(b, [b.shape[0],b.shape[1],b.shape[2],1]))\nb = Scaler(upper=1)(b)\nb = tf.reshape(b, [b.shape[1],b.shape[2]])\nrgb = tf.stack([r, g, b], axis = 2)\nfig = plt.figure(figsize=(10,5))\nplt.title('RGB image constructed from the 3 detectors')\nplt.imshow(rgb.numpy(), cmap='magma', aspect='auto');","e0bdaa0e":"fig = plt.figure(figsize=(16,7))\nax = plt.subplot(1, 2, 1)\nrgb3 = tf.image.resize(rgb, [224,224])\nplt.title('Stretched')\nplt.imshow(rgb3.numpy(), cmap='magma', aspect='auto')\nax = plt.subplot(1, 2, 2)\nrgb4 = tf.image.resize_with_pad(rgb, 224, 224)\nplt.title('Zero padded')\nplt.imshow(rgb4.numpy(), cmap='magma', aspect='auto');","1997ac79":"from scipy.signal import butter, sosfiltfilt\n\ndef butter_bandpass(lowcut, highcut, fs, order=5):\n    nyq = 0.5 * fs\n    low = lowcut \/ nyq\n    high = highcut \/ nyq\n    sos = butter(order, [low, high], btype='band', output='sos')\n    return sos\n\nFILT_SOS = butter_bandpass(20., 500., 2048, order=7)\n\ndef butter_bandpass_filter(data):\n    y = sosfiltfilt(FILT_SOS, data, padlen=1024)\n    return y","8eb9519f":"fig = plt.figure(figsize=(16,16))\n# un-filtered\nax = plt.subplot(2, 2, 1)\ny = Wavelet1D(nv=16, sr=2048., flow=0, fhigh=1024, batch_size=1)(tf.expand_dims(d, axis=0))\nplt.xlabel('Sample')\nplt.ylabel('Scale')\nplt.title('Unfiltered - Wavelet1D(Flow=0Hz, Fhigh=1024Hz)')\nplt.imshow(np.squeeze(y.numpy()), cmap='magma', aspect='auto') \n# un-filtered \nax = plt.subplot(2, 2, 2)\ny = Wavelet1D(nv=16, sr=2048., flow=20, fhigh=500, batch_size=1)(tf.expand_dims(d, axis=0))\nplt.xlabel('Sample')\nplt.ylabel('Scale')\nplt.title('Unfiltered - Wavelet1D(Flow=20Hz, Fhigh=500Hz)')\nplt.imshow(np.squeeze(y.numpy()), cmap='magma', aspect='auto')  \n# filtered\ndf = butter_bandpass_filter(d)\nax = plt.subplot(2, 2, 3)\ny = Wavelet1D(nv=16, sr=2048., flow=0, fhigh=1024, batch_size=1)(tf.expand_dims(df, axis=0))\nplt.xlabel('Sample')\nplt.ylabel('Scale')\nplt.title('BP Filtered -  Wavelet1D(Flow=0Hz, Fhigh=1024Hz)')\nplt.imshow(np.squeeze(y.numpy()), cmap='magma', aspect='auto') \nax = plt.subplot(2, 2, 4)\ny = Wavelet1D(nv=16, sr=2048., flow=20, fhigh=500, batch_size=1)(tf.expand_dims(df, axis=0))\nplt.xlabel('Sample')\nplt.ylabel('Scale')\nplt.title('BP Filtered - Wavelet1D(Flow=20Hz, Fhigh=500Hz)')\nplt.imshow(np.squeeze(y.numpy()), cmap='magma', aspect='auto');","af79c34d":"test_files = glob.glob('..\/input\/g2net-gravitational-wave-detection\/test\/*\/*\/*\/*.npy')\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 16\n\n# read mean & std values\nwith open('..\/input\/mean-and-std-calculations-for-the-entire-dataset\/train_stats.json') as fs:\n    stats = json.load(fs)\nmean = np.zeros((3,1), dtype=np.float64)\nstd = mean\nfor i in range(3):\n    mean[i] = stats['detector'][i]['mean']\n    std[i] = stats['detector'][i]['std']\n    \n# wrap numpy-based function for use with TF\n@tf.function(input_signature=[tf.TensorSpec(None, tf.float64)])\ndef tf_bp_filter(input):\n    y = tf.numpy_function(butter_bandpass_filter, [input], tf.float64)\n    return y\n\ndef _parse_function1(filename):\n    np_data = tf.io.read_file(filename)\n    np_data = tf.strings.substr(np_data, 128, 98304) # header is 128 bytes (skip)\n    np_data = tf.reshape(tf.io.decode_raw(np_data, tf.float64), (3, 4096))\n    np_data = tf.math.divide(tf.math.subtract(np_data, mean), std) # standardize\n    np_data = tf_bp_filter(np_data) # bandpass filter\n    return tf.reshape(tf.cast(np_data, tf.float32), (3, 4096))\n\ntest_ds = tf.data.Dataset.from_tensor_slices(test_files)\ntest_ds = test_ds.map(_parse_function1, num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.batch(BATCH_SIZE, drop_remainder=True)","5ba85107":"TARGET_IMG_SIZE = 224 # image size expected by the CNN model\n# skip start to make img_size integer multiple of 4096\nOFFSET = 4096-int(np.floor(4096\/224))*TARGET_IMG_SIZE\nSTRIDE = (4096-OFFSET)\/\/TARGET_IMG_SIZE\n\n# CTW model\ndef build_cwt_model(nv=16, flow=20, fhigh=700, batch_size=BATCH_SIZE):\n    inputs = Input(shape=(3, 4096))\n    # channels\n    r = Wavelet1D(nv=nv, sr=2048., flow=flow, fhigh=fhigh, batch_size=batch_size)(inputs[:,0,OFFSET:])  \n    g = Wavelet1D(nv=nv, sr=2048., flow=flow, fhigh=fhigh, batch_size=batch_size)(inputs[:,1,OFFSET:]) \n    b = Wavelet1D(nv=nv, sr=2048., flow=flow, fhigh=fhigh, batch_size=batch_size)(inputs[:,2, OFFSET:]) \n    # combine into rgb\n    rgb = RGBStack()([r, g, b])\n    # downsample\n    rgb = AvgPool2D(pool_size=(1,STRIDE), strides=(1,STRIDE))(rgb) \n    rgb = Scaler(upper=255.)(rgb) # adjust 'upper' according to CNN \n    \n    return tf.keras.Model(inputs, rgb, name=\"Wavlet\")\n    \ncwt_model = build_cwt_model()\ncwt_model.summary()","db5a96b6":"cwt_model.compile()\npred = cwt_model.predict(test_ds.take(1))\n\nfig = plt.figure(figsize=(16,16))\nfor i in range(BATCH_SIZE):\n    ax = plt.subplot(4, 4, 1+i)\n    plt.imshow(pred[i,:,:]\/255, aspect='auto') ","c89f5b16":"# Convert batches\nNext, we can make a model that converts a batch of numpy files to CWT images. The output from this model can be used as input for a CNN classifier model.","a70fb63a":"# Wavelet transform on GPU\nA popular way to classify 1D signals is to transform the 1D signal into one of several types of spectrograms and then convert this spectrogram to an image. The image is then fed to a pretrained CNN which is fine-tuned during training. Transformation to the frequency domain is quite compute intensive, so we would really like to perform this function on the GPU. Tensorflow currently has no continous wavelet transform (CWT), so we have to make our own. Thankfully, this is a quite simple task using Keras custom layer.   \n\nThe benefits of doing the CWT on the GPU are:\n  * Much faster than CPU (leave the CPU for data loading)\n  * Avoid quantization to 8-bit integer between float inputs and float ouputs to CNN\n  * The wavelets can be trained(!)\n  \nIn this notebook we will create a custom Keras layer called Wavelet1D, ready for use in Keras models.","30d05eb6":"That's it! Coming up: Training a model with these layers.","fca94f29":"# Downsampling\nThe scaleogram has the same width as the input signal. This needs to be reduced to our target image size (here we choose 224). Rather than doing an image resize type operation, we can use several types of layers:  \n  * Max pooling\n  * Average pooling\n  * 2D convolution (this can also be trained!)\n  \nThe trick is to set the pool\/filter size to (1,n), where n is the reduction factor, and stride to the same size. Let's try it out:","521c6cc8":"# Color images\nSo far we only use data from a single detector, but we can combine the 3 detectors into a RGB image.","c9f56cfd":"## Dataset\nFirst, we create a tf.dataset from numpy files. TF dataset has no direct support for reading numpy files, but the numpy format is very simple, and since all the files are of identical size, we can just skip the 128 byte numpy header and read raw data. We also normalize the data using the mean and stddev calculated in [this notebook](https:\/\/www.kaggle.com\/mistag\/mean-and-std-calculations-for-the-entire-dataset). We also throw in a bandpass filter. We can use a scipy function by decorating it with @tf_function:","4b097025":"![wiki](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/a8\/Continuous_wavelet_transform.svg\/1920px-Continuous_wavelet_transform.svg.png)","cad28368":"# Continous wavelet transform\nThe wavelet transform is computed by convolving multiple wavelets with the input signal. There are many types of wavelets, here we use the Morlet wavelet. Those who work with signal processing know that a convolution in the time domain is equivalent to a multiplication in the frequency domain. The CWT implementation here is based on [Alexander Neergaard Olesen's Python code](https:\/\/github.com\/neergaard\/CWT). The CWT is explored and compared with FFT in [this notebook](https:\/\/www.kaggle.com\/mistag\/extracting-bird-song-signatures-with-wavelets).  \n\nThe CWT is computed with the following steps:  \n  * Take FFT of input signal\n  * Multiply FFT spectrum with wavelet filterbank\n  * Perform inverse FFT of the product\n  \nFFT+multiply+IFFT might not be the fastest implementation though, since it always computes with maximum resolution. A 2D convolution implementation can benefit from the stride parameter to reduce computations.\n  \nThe code has been modified with additional parameters for lower and upper frequency of intereset.    \n\nThe custom Keras layers code is saved to file here, for easy use in other notebooks. We also define a custom layer for scaling logarithmic data to image data range called Scaler, and a layer to stack 3 channels into a RGB image.","3e6b3b32":"# Zero padding\nThe final step before feeding the scaleogram image to a CNN is to either strech or zero-pad to square format. Again, testing is required to find the best approach. Stretching does not give more detail, but more weights in the CNN are activated.","e330514e":"## Load sample GW signal\nStart by loading in a sample gravitational wave signal that has a strong target signal (chirp).","56c47701":"The chirp does visibly stand out better when applying a BP-filter prior to the Wavelet transform, so this is probably a good thing to do.","73778113":"## CWT to image model\nHere we build a model that takes a batch of numpy arrays as input and converts this into CWT images.","18ec929a":"We clearly see that a lot of low frequency noise can be omitted from the scaleogram by setting the flow parameter.","977f9a39":"Then simply convert a batch by calling .predict:","917ff767":"# CWT parameters\nThe CWT layer has several parameters that are to be considered hyperparameters:  \n  * Number of voices per octave (nv): Scales per octave\n  * Low frequency (flow): Lowest frequency of interest (Hz)\n  * High frequency (fhigh): Highest frequency of interest (Hz)\n  \nBelow we explore how these parameters affect the scaleogram (spectrogram).","10de76e9":"Import the cusomt layers:","4b9eeef5":"Even if the Conv2D layer is initialized with random weights, the untrained layer performs rather well.","449f7850":"## Normalization of input data\nThere is actually no need to normalize the input data to the Wavelet1D layer, as the Scaler1D layer will do the normalization after it. The float64 format has plenty of dynamic range even for GW signals. If the CWT wavelet weights are to be trained, scaling the input first might be a good idea though. ","488a4c50":"# Bandpass filtering\nSo there is plenty of low frequency noise in these signals below 20-30Hz. And also high frequency noise. We can choose which scales (frequencies) to use with the Wavelet1D layer - but should we apply a bandpass filter too? Let's check the difference with and without using scipy.signal, applying a bandpass filter from 20 to 500Hz (we also apply a Tukey window function to the signal to avoid ringing at the start and end): "}}