{"cell_type":{"eda22a62":"code","9aa390aa":"code","f5fa8da5":"code","e58dc1e2":"code","c186b0d9":"code","9e898edf":"code","f56eaa8a":"code","99b0506b":"code","012fdd51":"code","b50d11a7":"code","944a223d":"code","9946b115":"code","a0a72d0f":"code","d927f241":"code","672b15c4":"code","3740eeb1":"code","5c901d45":"code","ece7bd09":"code","0be92c7e":"code","856ed85e":"code","3b6a856c":"code","e817080c":"code","fedcd67f":"code","8f572388":"code","a08e183b":"code","f86f4fec":"code","5aa9ff13":"code","048d90ed":"code","09fa3656":"code","8b485e44":"code","364f2e87":"code","a0d6a167":"code","1c821753":"code","c863c52a":"code","361b0282":"code","091284f9":"code","cc2c7608":"code","5c1bcb5e":"code","7f686f7b":"code","fb24e285":"code","2787e678":"code","711317ee":"code","12462541":"code","75fa1174":"code","5a1c1d20":"code","c7206068":"code","351a8c17":"code","8746958c":"code","9076514c":"code","d61f33ef":"code","be67a32c":"code","52076168":"code","e856437c":"code","9286c210":"code","e6264b0d":"markdown","e0488868":"markdown","ce3d692e":"markdown","7eae30c0":"markdown","b7698e4c":"markdown","a39abef7":"markdown","b7f35ff3":"markdown","b15913e9":"markdown","53c5eb3e":"markdown","33eac250":"markdown","0b6196e9":"markdown","c496d433":"markdown","325602fa":"markdown","a7dba9cd":"markdown","601b8406":"markdown","818fb02d":"markdown","bb2b957b":"markdown","c0b88c18":"markdown"},"source":{"eda22a62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm_notebook as tqdm\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/jigsaw-toxic-comment-classification-challenge\"))\n\n# Any results you write to the current directory are saved as output.","9aa390aa":"# Load the dataset\ntrain = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\ntarget = train['toxic']\ntest = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\n\ntrain.head(10)","f5fa8da5":"sns.distplot(target, kde=False)\nprint(target.mean())\nprint('Minimum accuracy:', max(target.mean(), 1 - target.mean()))","e58dc1e2":"# define preprocessing function\nimport string\n\ndef preprocess(doc):\n    # lowercasing\n    doc = doc.lower()\n    # remove punctuation and different kinds of whitespaces e.g. newlines and tabs\n    for p in string.punctuation + string.whitespace:\n        doc = doc.replace(p, ' ')\n    # remove unneeded spaces\n    doc = doc.strip()\n    doc = ' '.join([w for w in doc.split(' ') if w != ''])\n    return doc","c186b0d9":"# Preprocessed text corpus\ncorpus = train['comment_text'].map(preprocess)\ncorpus.head(10)","9e898edf":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nvectorizer = TfidfVectorizer(max_features=30000)\nsvd = TruncatedSVD(n_components=100)\n\nX_tfidf = vectorizer.fit_transform(corpus)\nprint(X_tfidf.shape)\nX_svd = svd.fit_transform(X_tfidf)\nprint(X_svd.shape)","f56eaa8a":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.utils.testing import ignore_warnings\n\ndef eval_on_trainset(X, y, model_names=None):\n    models = {\n        'SVM_rbf': SVC(C=100, kernel='rbf'),\n        'SVM_linear': SVC(C=100, kernel='linear'),\n        'Log regression': LogisticRegression(),\n        'naive bayes': GaussianNB(),\n        'random forest': RandomForestClassifier(),\n        'KNN': KNeighborsClassifier(),\n    }\n    scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n    \n    print('Dummy accuracy:', max(y.mean(), 1 - y.mean()))\n    print()\n    \n    try:\n        X = X.toarray()\n    except:\n        pass\n    \n    for name in model_names or sorted(models):\n        model = models[name]\n        with ignore_warnings():\n            scores_svc = cross_validate(model, X, y, cv=3, scoring=scoring)\n        for sc in scoring:\n            mean = scores_svc['test_' + sc].mean()\n            std = scores_svc['test_' + sc].std()\n            print(name, sc, '{:.03} +- {:.03}'.format(mean, std))\n        print()","99b0506b":"# Small part of tfidf, because of time and memory usage\n%time eval_on_trainset(X_tfidf[:3000], target[:3000], ['Log regression', 'naive bayes', 'random forest'])","012fdd51":"# Part of data, because of time usage\n%time eval_on_trainset(X_svd[:10000], target[:10000])","b50d11a7":"# Eval on full data, for fastest algorythms\n%time eval_on_trainset(X_svd, target, ['Log regression', 'naive bayes', 'random forest'])","944a223d":"n_components = 5\n\nplot_data = pd.DataFrame(X_svd[:1000, :n_components], columns=['f{}'.format(i) for i in range(n_components)])\nplot_data['target'] = target[:1000]\nsns.pairplot(plot_data, hue='target')","9946b115":"# tokenize\nimport nltk\n\ntok_corpus = []\nfor sent in tqdm(corpus):\n#     tok_corpus.append(nltk.word_tokenize(sent))\n    tok_corpus.append(sent.split())\n\ntok_corpus[:5]","a0a72d0f":"# count words\ncounter = {}\nfor sent in tqdm(tok_corpus):\n    for word in sent:\n        counter[word] = counter.get(word, 0) + 1","d927f241":"\nplt.plot(sorted([np.log10(v) for v in counter.values()], reverse=True))\nplt.xlabel('Word id')\nplt.ylabel('log(frequency)')","672b15c4":"# Make vocab\nvocab = sorted(list(counter), key=counter.get,reverse=True)\nprint('Length:', len(vocab))\n# Take only frequent words\nmin_count = 10\nvocab = [word for word in vocab if counter[word] >= min_count]\nprint('Length:', len(vocab))\n# Add <UNK> token\nvocab.append('<UNK>')\nprint(vocab[:5])\n\n# Make word index\nword2idx = {word: idx for (idx, word) in enumerate(vocab)}\nprint(word2idx['the'], word2idx['to'], word2idx['hello'])","3740eeb1":"encoded_corpus = [[word2idx[word] for word in sent if word in word2idx] for sent in tqdm(tok_corpus)]\nencoded_corpus[:5]","5c901d45":"import gensim\n\nw2v_google = gensim.models.KeyedVectors.load_word2vec_format(\"..\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin\", binary=True)","ece7bd09":"vec = w2v_google['hello']\nprint(type(vec))\nprint(vec.shape)","0be92c7e":"w2v_google.most_similar([vec])","856ed85e":"w2v_google.most_similar(['hello'])","3b6a856c":"w2v_google.most_similar(positive=['woman', 'king'], negative=['man'])","e817080c":"w2v_google.most_similar(positive=['father', 'woman'], negative=['man'])","fedcd67f":"# YOUR CODE HERE\n# w2v_google.most_similar(positive=['...', '...'], negative=['...'])","8f572388":"w2v_google.most_similar(['fuck'])","a08e183b":"w2v_google.most_similar(['ass'])","f86f4fec":"# Bag of words\nX_w2v = [np.sum([np.zeros(vec.shape)] + [w2v_google[w] for w in sent if w in w2v_google], axis=0) for sent in tqdm(tok_corpus)]\nnormalize = lambda x: x \/ np.sqrt(np.sum(x**2) + 1e-8)\nX_w2v = [normalize(x) for x in tqdm(X_w2v)]\nX_w2v = np.array(X_w2v)\nX_w2v.shape","5aa9ff13":"%time eval_on_trainset(X_w2v[:1000], target[:1000])","048d90ed":"%time eval_on_trainset(X_w2v[:10000], target[:10000], ['Log regression', 'random forest'])","09fa3656":"n_components = 5\n\nplot_data = pd.DataFrame(X_w2v[:1000, :n_components], columns=['f{}'.format(i) for i in range(n_components)])\nplot_data['target'] = target[:1000]\nsns.pairplot(plot_data, hue='target')","8b485e44":"n_components = 5\n\nplot_data = pd.DataFrame(TruncatedSVD(n_components=n_components).fit_transform(X_w2v)[:1000], columns=['f{}'.format(i) for i in range(n_components)])\nplot_data['target'] = target[:1000]\nsns.pairplot(plot_data, hue='target')","364f2e87":"from sklearn.cluster import MiniBatchKMeans, MeanShift, AgglomerativeClustering, DBSCAN\nfrom sklearn.mixture import GaussianMixture\n\nfrom sklearn.metrics import silhouette_score","a0d6a167":"def distance(point1, point2):\n    return np.sqrt(np.sum((point1 - point2)**2))\n\ndists = []\nscores = []\n\nfor k in tqdm(range(2, 21)):\n    kmeans = MiniBatchKMeans(k)\n    kmeans.fit(X_w2v)\n    centers = kmeans.cluster_centers_\n    labels = kmeans.predict(X_w2v)\n    # Mean squared distance\n    mean_dist = np.sum([distance(x, centers[label])**2 for x, label in zip(X_w2v, labels)])\n    dists.append(mean_dist)\n    # Silhouette\n    score = silhouette_score(X_w2v[:2000], labels[:2000], metric='euclidean')\n    scores.append(score)","1c821753":"plt.figure(figsize=(14, 8))\nplt.plot(np.arange(2, 21), dists)\nplt.ylabel('Sum of squared distance')\nplt.xlabel('Number of clusters k')\nplt.show()","c863c52a":"plt.figure(figsize=(14, 8))\nplt.plot(np.arange(2, 21), scores)\nplt.ylabel('Silhouette score')\nplt.xlabel('Number of clusters k')\nplt.show()","361b0282":"k = 7\nkmeans = MiniBatchKMeans(k, random_state=2)\nkmeans.fit(X_w2v)\ntrain['KMeans'] = kmeans.predict(X_w2v)","091284f9":"kmeans.predict(X_w2v)","cc2c7608":"plt.subplots(figsize=(10,6))\nsns.barplot(x='KMeans' , y='toxic' , data=train)\nplt.ylabel(\"Toxic\")\nplt.title(\"Toxic as function of KMeans\")\nplt.show()","5c1bcb5e":"# Find the most toxic sentence\nam = np.argmin(np.sum((X_w2v - kmeans.cluster_centers_[0].reshape([1, 300]))**2, axis=1))\ncorpus[am]","7f686f7b":"# Find the least toxic sentence\nam = np.argmin(np.sum((X_w2v[target == 1] - kmeans.cluster_centers_[1].reshape([1, 300]))**2, 1))\ncorpus[target == 1].iloc[am]","fb24e285":"k = 7\nkmeans = MiniBatchKMeans(k, random_state=0)\nkmeans.fit(X_svd)\ntrain['KMeans_SVD'] = kmeans.predict(X_svd)","2787e678":"plt.subplots(figsize=(10,6))\nsns.barplot(x='KMeans_SVD' , y='toxic' , data=train)\nplt.ylabel(\"Toxic\")\nplt.title(\"Toxic as function of KMeans\")\nplt.show()","711317ee":"# Find the most toxic sentence\nam = np.argmin(np.sum((X_svd - kmeans.cluster_centers_[0].reshape([1, 100]))**2, 1))\ncorpus[am]","12462541":"preds = (train['KMeans'] == 0).astype('int')\nprint('accuracy', accuracy_score(preds, target))\nprint('precision', precision_score(target, preds))\nprint('recall', recall_score(target, preds))\nprint('f1', f1_score(target, preds))","75fa1174":"train.head(20)","5a1c1d20":"%%time\nmeanshift = MeanShift(bandwidth=0.9)\nmeanshift.fit(X_w2v[:1000])","c7206068":"train['MeanShift'] = meanshift.predict(X_w2v)\nprint(len(train['MeanShift'].unique()))","351a8c17":"plt.subplots(figsize=(10,6))\nsns.barplot(x='MeanShift' , y='toxic' , data=train)\nplt.ylabel(\"Toxic\")\nplt.title(\"Toxic as function of Mean Shift\")\nplt.show()","8746958c":"preds = (train['MeanShift'] == 10).astype('int')\nprint('accuracy', accuracy_score(preds, target))\nprint('precision', precision_score(target, preds))\nprint('recall', recall_score(target, preds))\nprint('f1', f1_score(target, preds))","9076514c":"k = 7\ngmix = GaussianMixture(k, random_state=2)\n%time gmix.fit(X_w2v[:10000])\ntrain['GMixture'] = gmix.predict(X_w2v)","d61f33ef":"plt.subplots(figsize=(10,6))\nsns.barplot(x='GMixture' , y='toxic' , data=train)\nplt.ylabel(\"Toxic\")\nplt.title(\"Toxic as function of GMixture\")\nplt.show()","be67a32c":"preds = (train['GMixture'] == 1).astype('int')\nprint('accuracy', accuracy_score(preds, target))\nprint('precision', precision_score(target, preds))\nprint('recall', recall_score(target, preds))\nprint('f1', f1_score(target, preds))","52076168":"train.head()","e856437c":"n_cluterizers = 4\n\ntrain_cl = train.iloc[:, -n_cluterizers:]\nfor cl in train_cl.columns:\n    print(cl, train_cl[cl].unique())\n    for i in sorted(train_cl[cl].unique()):\n        train_cl[cl + '_%i' % i] = (train_cl[cl] == i).astype(int)\ntrain_cl = train_cl.iloc[:, n_cluterizers:]\ntrain_cl.head()","9286c210":"%time eval_on_trainset(train_cl.iloc[:10000, :], target[:10000])","e6264b0d":"## Train on word2vec bag of words","e0488868":"# Text preprocessing low-level code","ce3d692e":"## Encode corpus","7eae30c0":"1. # Word2vec visualization","b7698e4c":"## Gaussian Mixture","a39abef7":"Prediction based entirely on one cluster:","b7f35ff3":"# Create dictionary","b15913e9":"# Toxic comments detection\nThe adjective 'toxic' is the [word of the year 2018 by Oxford](https:\/\/languages.oup.com\/word-of-the-year\/2018\/).\n> The Oxford Word of the Year is a word or expression that is judged to reflect the ethos, mood, or preoccupations of the passing year, and have lasting potential as a term of cultural significance.\n> \n>  \n> \n> In 2018, toxic added many strings to its poisoned bow becoming an intoxicating descriptor for the year\u2019s most talked about topics. It is the sheer scope of its application, as found by our research, that made toxic the stand-out choice for the Word of the Year title.\n\nToxic means: `extremely harsh, malicious, or harmful`.  \nexample: toxic sarcasm","53c5eb3e":"Elbow can be considered at k value from 3 to 6.","33eac250":"# TF-IDF","0b6196e9":"**Task:** try to get word \"moscow\" using the same principle","c496d433":"## Load word2vec","325602fa":"# MeanShift","a7dba9cd":"More than half of the words have frequency 1","601b8406":"# Try to train models on clusters","818fb02d":"Check consistency with our corpus","bb2b957b":"# KMeans Clustering","c0b88c18":"## SVD Visualization"}}