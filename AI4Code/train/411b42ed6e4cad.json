{"cell_type":{"5fd3f008":"code","ce23ed2d":"code","f6d48d78":"code","1a9bb96a":"code","2100afd3":"code","a9da7d86":"code","fdbfe9af":"code","f75411ce":"code","75be6fec":"code","7125ec77":"code","babee699":"code","d7133db3":"code","5089ccc1":"code","4fc52611":"code","4d015c43":"code","72c3f8f4":"code","956c6c72":"code","f1c2224d":"code","0853f364":"code","dddff21e":"code","7ae7c348":"code","a5b4873f":"code","6e8be285":"markdown","c7df545e":"markdown","8ddaccff":"markdown","11de999b":"markdown","f606e5f3":"markdown","d897ed1c":"markdown","551fcc3f":"markdown","37f3036b":"markdown","f3b10bd6":"markdown","fe202ac2":"markdown","8d44332d":"markdown","b94c85bf":"markdown","5e879a20":"markdown","7ba380a6":"markdown","904b0495":"markdown","562eb119":"markdown","faec8abf":"markdown"},"source":{"5fd3f008":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ce23ed2d":"sample_submission = pd.read_csv(\"\/kaggle\/input\/analytics-vidhya-ltfs-2\/sample_submission.csv\", parse_dates=['application_date'])\ntrain = pd.read_csv(\"\/kaggle\/input\/analytics-vidhya-ltfs-2\/train.csv\",parse_dates=['application_date'])\ntest = pd.read_csv(\"\/kaggle\/input\/ltfs-2\/test_1eLl9Yf.csv\", parse_dates=['application_date'])\n","f6d48d78":"# CHECKING THE SHAPES OF THE FILES\n\ntrain.shape,test.shape,sample_submission.shape","1a9bb96a":"# LOOKING AT FIRST 5 ROWS OF TRAIN AND TEST FILES\n\ntrain.head() , test.head() ","2100afd3":"# CHECKING FOR MISSING VALUES AND COLUMN TYPES\n\ntrain.info(), test.info()","a9da7d86":"# We can also visualize the missing values using the following command: \n\ntrain.isna().sum(), test.isna().sum()\n\n","fdbfe9af":"# CHECKING FOR UNIQUE COLUMN VALUES\n\ntrain.nunique(), test.nunique()","f75411ce":"import holidays","75be6fec":"test.drop(['id'], axis=1, inplace=True)\ntrain = train.sort_values('application_date').reset_index(drop = True)\ntest = test.sort_values('application_date').reset_index(drop = True)","7125ec77":"train.application_date.min(), train.application_date.max()","babee699":"test.application_date.min(), test.application_date.max()","d7133db3":"train.groupby(['application_date','segment','branch_id']).mean().reset_index()","5089ccc1":"train_1=train[train['segment']==1].groupby(['application_date']).sum().reset_index()[['application_date','case_count']].sort_values('application_date').set_index('application_date')\ntrain_2=train[train['segment']==2].groupby(['application_date']).sum().reset_index()[['application_date','case_count']].sort_values('application_date').set_index('application_date')\ntest_1=test[test['segment']==1][['application_date']].sort_values('application_date').set_index('application_date')\ntest_2=test[test['segment']==2][['application_date']].sort_values('application_date').set_index('application_date')","4fc52611":"train_1.tail(10)","4d015c43":"train_2.tail(10)","72c3f8f4":"train_1.plot(style='.', figsize=(15,5), title='Train_data_segment 1')","956c6c72":"#train_1.boxplot(by ='day', column =['total_bill'], grid = False)\n\ntrain_1.boxplot( column =['case_count'], grid = True)","f1c2224d":"train_1.describe()","0853f364":"train_1.loc[train_1.case_count>=8700,'case_count']=np.nan\n","dddff21e":"train_2.plot(style='.', figsize=(15,5), title='Train_data_segment 2')","7ae7c348":"# PLOTTING BOXPLOT FOR TRAIN DATA SEGMENT 2 \n\ntrain_2.boxplot( column =['case_count'], grid = True)","a5b4873f":"    train_2.describe()","6e8be285":"\nTHIS CLEARLY SHOWS THAT THERE ARE 13804 MISSING VALUES IN THE 'ZONE' AND \"BRANCH_ID\" COLUMN","c7df545e":"TRAIN SET WITH SEGMENT-1 HAS VALUES TILL DATE  5 JULY 2019","8ddaccff":"# LTFS Data Science FinHack 2\nLTFS receives a lot of requests for its various finance offerings that include housing loan, two-wheeler loan, real estate financing and micro loans. The number of applications received is something that varies a lot with season. Going through these applications is a manual process and is tedious. Accurately forecasting the number of cases received can help with resource and manpower management resulting into quick response on applications and more efficient processing.\n    \n    \n # Problem Statement\n \n You have been appointed with the task of forecasting daily cases for next 3 months for 2 different business segments aggregated at the country level keeping in consideration the following major Indian festivals (inclusive but not exhaustive list): Diwali, Dussehra, Ganesh Chaturthi, Navratri, Holi etc. (You are free to use any publicly available open source external datasets). Some other examples could be:\n     Weather Macroeconomic variables Note that the external dataset must belong to a reliable source.\n    Data Dictionary The train data has been provided in the following way:\n    For business segment 1, historical data has been made available at branch ID level For business segment 2, historical data has been made available at State level.\n    Train File Variable Definition application_date Date of application segment Business Segment (1\/2) branch_id Anonymised id for branch at which application was received state State in which application was received (Karnataka, MP etc.) zone Zone of state in which application was received (Central, East etc.) case_count (Target) Number of cases\/applications received\n    Test File Forecasting needs to be done at country level for the dates provided in test set for each segment.\n    Variable Definition id Unique id for each sample in test set application_date Date of application segment Business Segment (1\/2)\n    \n    \n# Evaluation\n\n Evaluation Metric The evaluation metric for scoring the forecasts is *MAPE (Mean Absolute Percentage Error) M with the formula:\n Where At is the actual value and Ft is the forecast value.\n The Final score is calculated using MAPE for both the segments using the formula\n    \n","11de999b":"CLEARLY TEST HAS FEWER COLUMNS THAN THE TRAIN FILE.","f606e5f3":"TRAIN DATASET IS BETWEEN 1ST APRIL 2017 TO 23 JULY 2019","d897ed1c":"*NOW WE ARE READY FOR FEATURE GENERATION. FEATURE GENERATION WILL BE COVERED IN NEXT NOTEBOOK \nANALYTICS VIDHYA LTFS HACKATHON JAN 2020 PART 2 *","551fcc3f":"TRAIN DATA WITH SEGMENT-2 HAS VALUES TILL DATE 23 JULY 2019\n","37f3036b":"TEST DATASET IS BETWEEN 6 JULY 2019 TO 24 OCTOBER 2019. THERE MAY BE SOME OVERLAP OF TRAIN AND TEST DATASET.","f3b10bd6":"**SEGMENT 2 DATA **","fe202ac2":"TRAIN FILE HAS MISSING VALUES IN 'BRANCH_ID' AND 'ZONE' COLUMN\n\nTEST FILE HAS NO MISSING VALUES\n","8d44332d":"   THERE ARE NO OULIERS IN THE TRAIN DATA OF SEGMENT 2 ","b94c85bf":"*THIS NOTEBOOK IS FIRST IN SERIES OF THREE NOTEBOOKS *\n*     *FIRST ONE IS FOR DATA VISUALIZATION *\n*     *SECOND ONE IS FOR FEATURE GENERATION *\n*     *THIRD ONE IS FOR MODELLING *\n    \n\n ","5e879a20":"CLEARLY THERE ARE OUTLIERS IN THE DATASET. WE WILL SELECT ONLY DATA WITHOUT OUTLIERS.","7ba380a6":"I AM USING 3 STD DEVIATION FOR OUTLIER CONSIDERATION AS IT CONTAIN 95% OF THE DATA.\n\n    We can see that outliers are mostly in the upper boundaries of the dataset. Dataset above 8700(mean + 3 std) are outliers.  ","904b0495":"THERE ARE ONLY TWO SEGMENTS : 1 AND 2\n\nWE HAVE ID COLUMN IN THE TEST DATASET , WE CAN DROP IT FOR OUR PREDICTION","562eb119":"## Visualizing the dataset","faec8abf":"CREDITS :\n1. https:\/\/github.com\/rajat5ranjan\/AV-LTFS-Data-Science-FinHack-2\n1. https:\/\/github.com\/KrishnaPriyaIITR\/LTFS-Data-Science-FinHack-2\/blob\/master\/LTFS_baseline_V31_br.ipynb "}}