{"cell_type":{"c6512550":"code","a1979425":"code","7dd4a41e":"code","2f2e6b22":"code","bf555119":"code","de8696cb":"markdown","4e41bf4a":"markdown","3ecaa59a":"markdown","29a5f17a":"markdown","bd14d1a9":"markdown"},"source":{"c6512550":"!pip install BorutaShap","a1979425":"import pandas  as pd\n\n#===========================================================================\n# read in the House Prices data\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n#===========================================================================\n# select some features (These are all 'integer' fields for today).\n#===========================================================================\nfeatures = ['LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', \n            'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', \n            '1stFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', \n            'BsmtHalfBath', 'HalfBath', 'BedroomAbvGr',  'Fireplaces', \n            'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n            'EnclosedPorch',  'PoolArea', 'YrSold']\n\n#===========================================================================\n#===========================================================================\nX_train       = train_data[features]\ny_train       = train_data[\"SalePrice\"]","7dd4a41e":"from BorutaShap import BorutaShap\n\n# If no model is selected default is the Random Forest\n# If classification is True it is a classification problem\nFeature_Selector = BorutaShap(importance_measure='shap', classification=False)\n\nFeature_Selector.fit(X=X_train, y=y_train, n_trials=50, random_state=0)","2f2e6b22":"Feature_Selector.plot(which_features='all', figsize=(16,12))","bf555119":"# Return a subset of the original data with the selected features\nFeature_Selector.Subset()","de8696cb":"Now return a [box-plot](https:\/\/en.wikipedia.org\/wiki\/Box_plot) of the features. The `which_features` parameter can be: `all`, `accepted`, `rejected` and `tentative`. ","4e41bf4a":"These results compare well with those obtained both via [recursive feature elimination (RFE)](https:\/\/www.kaggle.com\/carlmcbrideellis\/recursive-feature-elimination-rfe-example) and via [permutation importance](https:\/\/www.kaggle.com\/carlmcbrideellis\/house-prices-permutation-importance-example), both applied to the very same dataset.\n### Links:\n* [Boruta-Shap](https:\/\/github.com\/Ekeany\/Boruta-Shap) (GitHub)\n* [SHAP (SHapley Additive exPlanations)](https:\/\/github.com\/slundberg\/shap) by Scott Lundberg (GitHub)\n* [Christoph Molnar \"SHAP (SHapley Additive exPlanations)\" in \"Interpretable Machine Learning: A Guide for Making Black Box Models Explainable\"](https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html)\n* Miron B. Kursa, Witold R. Rudnicki \"Feature Selection with the Boruta Package\", Journal of Statistical Software Volume 36, Issue 11 (2010) [doi: 10.18637\/jss.v036.i11](https:\/\/www.jstatsoft.org\/article\/view\/v036i11) ([pdf](https:\/\/www.jstatsoft.org\/article\/view\/v036i11\/v36i11.pdf))\n\n**Related notebooks:**\n\n* [Automated feature selection with boruta](https:\/\/www.kaggle.com\/residentmario\/automated-feature-selection-with-boruta) by [Aleksey Bilogur](https:\/\/www.kaggle.com\/residentmario)\n* [Boruta Beats 'em all-New look at Feature Selection](https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection) by [Ajay Sampath](https:\/\/www.kaggle.com\/ajaysamp)\n* [SHAP Values](https:\/\/www.kaggle.com\/dansbecker\/shap-values) by [DanB](https:\/\/www.kaggle.com\/dansbecker)\n* [Advanced Uses of SHAP values](https:\/\/www.kaggle.com\/dansbecker\/advanced-uses-of-shap-values) by [DanB](https:\/\/www.kaggle.com\/dansbecker)","3ecaa59a":"# Feature selection using the Boruta-SHAP package\n\nFeature selection (taken from [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Feature_selection)):\n\n> *In machine learning and statistics feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons:*\n> *  *simplification of models to make them easier to interpret*\n> *  *shorter training times,*\n> *  *to avoid the curse of dimensionality,*\n> *  *enhanced generalization by reducing overfitting (reduction of variance)* \n\n        \nThe BorutaShap package, as the name suggests, combines the [Boruta feature selection algorithm](https:\/\/www.jstatsoft.org\/article\/view\/v036i11) with the [SHAP (SHapley Additive exPlanations) technique](https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html). The Boruta algorithm (named after a god of the forest in Slavic mythology) is tasked with [finding a minimal optimal feature set](https:\/\/dl.acm.org\/doi\/10.5555\/1314498.1314519) rather than finding all the features relevant to the target variable. This leads to an unbiased and stable selection of important and non-important attributes.\nThe BorutaShap package was written by Eoghan Keany, who has also written an introductory article [\"Is this the Best Feature Selection Algorithm 'BorutaShap'?\"](https:\/\/medium.com\/analytics-vidhya\/is-this-the-best-feature-selection-algorithm-borutashap-8bc238aa1677) providing an overview of how BorutaShap works.\n\nThis is a simple example script to perform Boruta-SHAP on the kaggle [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) competition data. \n\nFirst we shall install [BorutaShap](https:\/\/github.com\/Ekeany\/Boruta-Shap):","29a5f17a":"Now for the `BorutaShap`, here using 50 trials. Other options are:\n* `importance_measure`: can be `shap`, `gain` or `permutation`.\n\nThe default `model` is the Random Forest. However other  tree based models can be used instead, such as  `DecisionTreeClassifier`, `RandomForestClassifier`, `XGBClassifier` and `CatBoostClassifier` ","bd14d1a9":"Now load in the kaggle House Prices data:"}}