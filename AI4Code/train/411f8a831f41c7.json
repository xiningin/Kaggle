{"cell_type":{"1dd2845b":"code","d9d23bb9":"code","a298a4da":"code","a08c9b39":"code","40adb1a2":"code","1184d326":"code","c097edfe":"code","24155056":"code","32577c31":"code","ec3be0cc":"code","2f7923e2":"code","b7fafbd8":"code","bb889100":"code","2f35914c":"code","b296a5d0":"markdown","0b512954":"markdown","86bf5ed4":"markdown","931fb585":"markdown","96ce557d":"markdown","210c3ba9":"markdown"},"source":{"1dd2845b":"import os\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.model_selection import KFold\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig","d9d23bb9":"class config:\n    sample=False\n    batch_size = 2\n    acc_steps = 8\n    epochs = 5\n    max_len = 1024\n    lr = 2e-5\n    weight_decay=1e-3\n    model_name = 'allenai\/longformer-base-4096'\n    device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )","a298a4da":"class2label={\n    'B-Lead': 0,\n    'I-Lead': 1,\n    \n    'B-Position': 2,\n    'I-Position': 3,\n    \n    'B-Evidence': 4,\n    'I-Evidence': 5,\n    \n    'B-Claim': 6,\n    'I-Claim': 7,\n    \n    'B-Concluding Statement' : 8,\n    'I-Concluding Statement': 9,\n    \n    'B-Counterclaim': 10,\n    'I-Counterclaim': 11,\n    \n    'B-Rebuttal': 12,\n    'I-Rebuttal': 13,\n    \n    'O': 14,\n    'PAD': -100\n}","a08c9b39":"label2class={\n    0: 'B-Lead',\n    1: 'I-Lead',\n    \n    2: 'B-Position',\n    3: 'I-Position',\n    \n    4: 'B-Evidence',\n    5: 'I-Evidence',\n    \n    6: 'B-Claim',\n    7: 'I-Claim',\n    \n    8: 'B-Concluding Statement',\n    9: 'I-Concluding Statement',\n    \n    10: 'B-Counterclaim',\n    11: 'I-Counterclaim',\n    \n    12: 'B-Rebuttal',\n    13: 'I-Rebuttal',\n    \n    14: 'O'\n}","40adb1a2":"label2segment={\n    0: 'Lead',\n    1: 'Lead',\n    \n    2: 'Position',\n    3: 'Position',\n    \n    4: 'Evidence',\n    5: 'Evidence',\n    \n    6: 'Claim',\n    7: 'Claim',\n    \n    8: 'Concluding Statement',\n    9: 'Concluding Statement',\n    \n    10: 'Counterclaim',\n    11: 'Counterclaim',\n    \n    12: 'Rebuttal',\n    13: 'Rebuttal'\n}","1184d326":"def read_essay(filename):\n    essay_folder='..\/input\/feedback-prize-2021\/train'\n    filepath = os.path.join(essay_folder, filename+\".txt\")\n    essay = ''\n    with open(filepath) as file:\n        essay = file.read()\n    return essay\n\n\ndef get_labels(row):\n    discourse_type = row.discourse_type\n    predictionstring = row.predictionstring\n    content = row.content\n    labels = ['O']*len(content)\n    \n    for i, cls_label in enumerate(discourse_type):\n        token_ids = [int(x) for x in predictionstring[i].split()]\n        \n        for j, token_id in enumerate(token_ids):\n            label=''\n            if j == 0:\n                label = 'B-'+cls_label\n            else:\n                label = 'I-'+cls_label\n            labels[token_id] = label\n    return labels","c097edfe":"train_df = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\nif config.sample:\n    train_df = train_df.head(100)\n\ntrain_df = train_df.groupby('id')[['discourse_type', 'predictionstring']].agg(list).reset_index()\ntrain_df['content'] = train_df.id.apply(read_essay)\ntrain_df['content'] = train_df['content'].apply(lambda content: content.split())\ntrain_df['labels'] = train_df.apply(get_labels, axis=1)\n\ntrain_df.head()","24155056":"class FeedbackDataset( torch.utils.data.Dataset ):\n    def __init__(self, df, tokenizer):\n        self.tokenizer=tokenizer\n        df=df.copy()\n        self.content = df.content.values\n        self.labels = df.labels.values\n    \n    def get_tokenized_inputs(self, essay, labels):\n        tokenized_inputs = self.tokenizer(essay, is_split_into_words=True)\n        word_ids = tokenized_inputs.word_ids()\n        labelids = []\n        prv_word_idx=None\n        for word_id in word_ids:\n            if (word_id is None) or (prv_word_idx == word_id):\n                labelids.append( class2label['PAD'] )\n            elif prv_word_idx != word_id:\n                labelids.append( class2label[ labels[word_id] ] )\n            prv_word_idx = word_id\n        return (tokenized_inputs, labelids, word_ids)\n    \n    \n    def __getitem__(self, idx):\n        essay  = self.content[idx]\n        labels = self.labels[idx]\n        (tokenized_inputs, labelids, word_ids) = self.get_tokenized_inputs(essay, labels)\n        word_ids[0] = -100\n        word_ids[-1] = -100\n        \n        input_ids = tokenized_inputs['input_ids'][:config.max_len]\n        attn_mask = tokenized_inputs['attention_mask'][:config.max_len]\n        labelids  = labelids[:config.max_len]\n        word_ids = word_ids[:config.max_len]\n        seq_len = len(input_ids)\n        \n        if seq_len < config.max_len:\n            len_diff = config.max_len - seq_len\n            attn_mask += [0] * len_diff\n            labelids  += [-100] * len_diff\n            input_ids += [self.tokenizer.pad_token_id] * len_diff\n            word_ids += [-100] * len_diff\n        \n        rpercentile = ((1 + np.arange(0, config.max_len))\/seq_len) - 0.5\n        \n        input_ids=torch.tensor(input_ids, dtype=torch.long)\n        attn_mask = torch.tensor(attn_mask, dtype=torch.long)\n        y = torch.tensor(labelids, dtype=torch.long)\n        seq_len = torch.tensor(seq_len, dtype=torch.long)\n        word_ids= torch.tensor(word_ids, dtype=torch.long)\n        rpercentile = torch.tensor(rpercentile, dtype=torch.float32)\n        \n        return {\n            'input_ids': input_ids,\n            'attn_mask': attn_mask,\n            'y': y,\n            'word_ids': word_ids,\n            'seq_len': seq_len,\n            'rpercentile': rpercentile\n        }\n    def __len__(self):\n        return len(self.labels)","32577c31":"class FeedbackModel(nn.Module):\n    def __init__(self, num_labels):\n        super(FeedbackModel, self).__init__()\n        modelconfig = AutoConfig.from_pretrained(config.model_name)\n\n        self.backbone = AutoModel.from_pretrained(config.model_name)\n        self.output = nn.Linear( 1+modelconfig.hidden_size, num_labels)\n    \n    def forward(self, input_ids, attn_mask, rpercentile):\n        attn_outputs = self.backbone(input_ids, attn_mask)\n        x=torch.cat([attn_outputs.last_hidden_state, rpercentile.unsqueeze(-1)], dim=-1)\n        y=self.output(x)\n        return y","ec3be0cc":"def postprocess( y, word_ids):\n    seq_len = len(y)\n    prv_word_id=None\n    predSegment=[]\n    predTokens=[]\n    \n    preds=[]\n    for i in range(seq_len):\n        word_id = word_ids[i]\n        if  (word_id== -100) or (prv_word_id == word_id):\n            continue\n        prv_word_id = word_id\n        \n        if y[i] not in label2segment:\n            continue\n        \n        segment = label2segment[ y[i] ]\n        predSegment.append(segment)\n        predTokens.append( word_id )\n    \n    if len(predSegment) == 0:\n        return []\n    \n    if len(predSegment) == 1:\n        preds.append({\n            'segment': predSegment[0],\n            'word_ids': [predTokens[0]]\n        })\n        return preds\n    else:\n        num_tokens=len(predTokens)\n        prv_id=0\n        cur_id=0\n        prv_segment=predSegment[0]\n        \n        for i in range(1, num_tokens+1):\n            cur_id=i\n            if (i!=num_tokens) and (predTokens[i] == 1+predTokens[i-1]) and (predSegment[i] == predSegment[i-1]):\n                continue\n            \n            pred_token_list=[]\n            for j in range(prv_id, cur_id):\n                pred_token_list.append(predTokens[j])\n            \n            preds.append({\n                'segment': prv_segment,\n                'word_ids': pred_token_list\n            })\n            if i!=num_tokens:\n                prv_segment = predSegment[i]\n                prv_id=cur_id\n    return preds","2f7923e2":"def is_positive(true_token, pred_token):\n    true_segment = true_token['segment']\n    pred_segment = pred_token['segment']\n    \n    true_wordids = set(true_token['word_ids'])\n    pred_wordids = set(pred_token['word_ids'])\n    \n    if true_segment!=pred_segment:\n        return False\n    \n    num_true_tokens = len(true_wordids)\n    num_pred_tokens = len(pred_wordids)\n    \n    num_common_wordids = len( true_wordids.intersection(pred_wordids) )\n    \n    p1 = num_common_wordids\/num_true_tokens\n    p2 = num_common_wordids\/num_pred_tokens\n    \n    if p1>=0.5 and p2 >=0.5:\n        return True\n    return False","b7fafbd8":"def evaluate(val_dataloader, model):\n    true_positives=0\n    false_positives=0\n    false_negatives=0\n    \n    model.eval()\n    for inputs in val_dataloader:\n        input_ids = inputs['input_ids']\n        attn_mask = inputs['attn_mask']\n        y = inputs['y']\n        rpercentile = inputs['rpercentile']\n        word_ids = inputs['word_ids']\n        seq_len = inputs['seq_len']\n        batch_max_seqlen = torch.max(seq_len).item()\n        \n        \n        input_ids = input_ids[:, :batch_max_seqlen].to(config.device)\n        attn_mask = attn_mask[:, :batch_max_seqlen].to(config.device)\n        rpercentile = rpercentile[:, :batch_max_seqlen].to(config.device)\n        y = y[:, :batch_max_seqlen]\n        \n        with torch.no_grad():\n            yhat = model(input_ids, attn_mask, rpercentile).softmax(dim=-1).argmax(dim=-1).cpu()\n        \n        bsize = y.shape[0]\n        for i in range(bsize):\n            yi = y[i].numpy()\n            yhati = yhat[i].numpy()\n            word_ids_i = word_ids[i].numpy()\n            \n            true_tokens = postprocess(yi, word_ids_i)\n            pred_tokens = postprocess(yhati, word_ids_i)\n            \n            num_true_tokens = len(true_tokens)\n            num_pred_tokens = len(pred_tokens)\n            \n            pos=0\n            for j in range(num_true_tokens):\n                for k in range(num_pred_tokens):\n                    if is_positive(true_tokens[j], pred_tokens[k]):\n                        pos+=1\n                        break\n            \n            \n            true_positives+=pos\n            false_positives += (num_pred_tokens - pos)\n            false_negatives += (num_true_tokens - pos)\n    \n    precision = true_positives\/(true_positives + false_positives)\n    recall = true_positives\/(true_positives + false_negatives)\n    fscore = 0\n    if precision!=0 and recall!=0:\n        fscore = 2*precision*recall\/(precision + recall)\n    \n    return precision, recall, fscore","bb889100":"def train_epoch(model, train_dataloader, optimizer, schedular, criterion):\n    epoch_losses = []\n    model.train()\n    model.zero_grad(set_to_none=True)\n    \n    for i, inputs in enumerate(train_dataloader):\n        input_ids = inputs['input_ids']\n        attn_mask = inputs['attn_mask']\n        y = inputs['y']\n        seq_len = inputs['seq_len']\n        rpercentile = inputs['rpercentile']\n        batch_max_seqlen = torch.max(seq_len).item()\n\n        if i%100==0:\n            print('iteratin:', i, batch_max_seqlen)\n            gc.collect()\n            \n        input_ids = input_ids[:, :batch_max_seqlen].to(config.device)\n        attn_mask = attn_mask[:, :batch_max_seqlen].to(config.device)\n        rpercentile = rpercentile[:, :batch_max_seqlen].to(config.device)\n        y = y[:, :batch_max_seqlen].to(config.device)\n        \n        yhat=model(input_ids, attn_mask, rpercentile)\n        loss=criterion(yhat.transpose(1, 2), y)\/config.acc_steps\n        loss.backward()\n        \n        if ((i+1)%config.acc_steps==0) or (i == len(train_dataloader)-1):\n            optimizer.step()\n            schedular.step()\n            model.zero_grad(set_to_none=True)\n        \n        epoch_losses.append( loss.item() )\n        \n        del yhat\n        del input_ids\n        del attn_mask\n        del y\n        \n    return epoch_losses\n        \ndef train_model(fold_train_df, fold_val_df):\n    tokenizer=AutoTokenizer.from_pretrained(config.model_name, add_prefix_space=True, truncate=True, padding=True)\n    train_dataset = FeedbackDataset(fold_train_df, tokenizer)\n    val_dataset   = FeedbackDataset(fold_val_df, tokenizer)\n    \n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2*config.batch_size, shuffle=False)\n    \n    num_labels = len(class2label)-1\n    model = FeedbackModel(num_labels).to(config.device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr,\n                                  weight_decay=config.weight_decay)\n    schedular = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr = config.lr,\n        epochs = config.epochs,\n        steps_per_epoch = 3+(len(train_dataloader)\/\/config.acc_steps)\n    )\n    criterion = nn.CrossEntropyLoss(ignore_index = -100).to(config.device)\n    \n    print('batch size:', config.batch_size)\n    print(len(train_dataloader), len(val_dataloader))\n    \n    best_fscore=None\n    for e in range(config.epochs):\n        print(\"started training of epoch:\" ,e)\n        epoch_losses = train_epoch(model, train_dataloader, optimizer, schedular, criterion)\n        print('epoch loss:{:.4f}'.format(np.mean(epoch_losses)))\n        print(\"evaluating at epoch:\", e)\n        precision, recall, fscore = evaluate(val_dataloader, model)\n        if (best_fscore is None) or (best_fscore<fscore):\n            torch.save(model, 'model.pt')\n            best_fscore = fscore\n        \n        print('precision:{:.4f}| recall:{:.4f} | fscore:{:.4f}|best fscore:{:.4f}'.format(precision, recall, fscore, best_fscore))\n        \n        epoch_losses=np.array(epoch_losses)\n        epoch_losses = np.cumsum(epoch_losses)\n        epoch_losses = (epoch_losses[8:] - epoch_losses[:-8])\/8\n        if epoch_losses.shape[0] !=0:\n            plt.title(\"epoch losses\")\n            plt.plot(epoch_losses)\n            plt.show()","2f35914c":"kfold = KFold(n_splits=5,shuffle=True, random_state=2040)\n\nfor train_index, val_index in kfold.split(train_df.id):\n    fold_train_df = train_df.iloc[train_index].copy()\n    fold_val_df   = train_df.iloc[val_index].copy()\n    \n    print(fold_train_df.shape, fold_val_df.shape)\n    train_model(fold_train_df, fold_val_df)\n    break","b296a5d0":"# evaluate","0b512954":"# helper functions","86bf5ed4":"# model","931fb585":"# train epochs","96ce557d":"Version 6:\n\n    1. Include the relative percentile of the token.\n    2. Shown from analysis that  https:\/\/www.kaggle.com\/narendra\/eda-feedback-prize this will give the direct cue of where to find the class elements","210c3ba9":"# dataset"}}