{"cell_type":{"ba86e4d4":"code","d4ee57ef":"code","c24b96b6":"code","0083c644":"code","9cd457ab":"code","6802b882":"code","30ecd5dd":"code","9d0672e2":"code","ea7eacd7":"code","451dd4ed":"code","263b463a":"code","33f5849f":"code","499301d4":"code","2524412b":"code","287905e7":"code","231ade02":"code","37023629":"code","fc70248e":"code","a85a302d":"code","0652aee8":"code","c80a5a25":"code","d20f48a9":"code","c63920b4":"code","8c06f55b":"code","45ab7de2":"code","2c45ceea":"code","c0fe7c5e":"code","fa6da18e":"code","2d0a6650":"code","5cee8868":"code","1eefcfb5":"code","a4164644":"code","9400bdb8":"code","f75f6627":"code","a50317f5":"code","8102d038":"code","701e1f29":"code","81d2cdac":"code","735f6ef9":"code","a470628f":"code","1db6c74d":"code","8f4879c9":"code","6d1600b8":"code","ed78f8b9":"code","42777b48":"code","8b749b42":"code","5e2fa9c3":"code","461f1d19":"code","92a74007":"code","7121a3b6":"code","f996dc3a":"code","5e93eb60":"code","b8557e59":"code","36e40900":"code","54131571":"code","06a478e6":"code","4c50712a":"code","619846c0":"code","dc38ce45":"code","c1157c9f":"code","4030c273":"code","addaa5b0":"code","046e3439":"code","564fb67d":"code","5e63a944":"code","4035901a":"code","886a295c":"code","ee4f6ba1":"code","60a70791":"code","af3e7936":"code","1d204b62":"code","987f7bc0":"code","617d442a":"code","aafdb891":"code","edcc52e6":"code","20873fe8":"code","49608d41":"code","4e538600":"code","05d3a7e0":"code","3bab5c1d":"markdown","2ad1d29c":"markdown","55099b8d":"markdown","0b6fbf9f":"markdown","85e0db7a":"markdown","3aab9473":"markdown","1c14af55":"markdown","2b5746d5":"markdown","988029ac":"markdown","93939edd":"markdown","79139ff2":"markdown","738f50a8":"markdown","436060e7":"markdown","1df6ef78":"markdown","4e225104":"markdown","928cb7ff":"markdown","7464e4e4":"markdown","00a5c368":"markdown","815540f5":"markdown","f2f8350c":"markdown","a975421c":"markdown","e604ed00":"markdown","cd601979":"markdown","e84da0ea":"markdown","542d5b86":"markdown","04ddf12c":"markdown","1036ad17":"markdown","9ea387bf":"markdown","22426380":"markdown","e78f4147":"markdown","9c0c368a":"markdown","7f717a0f":"markdown","47c3459f":"markdown","1d641b39":"markdown","2353ff31":"markdown","7ef64b0d":"markdown","cc7b25b4":"markdown","68f97f4f":"markdown","96036065":"markdown","68669f48":"markdown","89b04c82":"markdown","25e10b13":"markdown","469d8e8a":"markdown","2315360e":"markdown","965f2927":"markdown","cf8c34a5":"markdown","74bd3ded":"markdown","ccd6dd9e":"markdown","ec377ab5":"markdown","ea1c63a1":"markdown","2aa9ba31":"markdown","b1d6fd32":"markdown","f6d9fa04":"markdown","b779596c":"markdown","efe18373":"markdown","34f14d0b":"markdown","d969e6b4":"markdown","fa4720fb":"markdown","faa50905":"markdown","dda67b17":"markdown","bb30e4aa":"markdown","1dea58b7":"markdown","04cf8d06":"markdown","cfe53d90":"markdown","edcd062e":"markdown","0243ba47":"markdown","e4cd9c7d":"markdown","0cbd17f8":"markdown","22af9ba6":"markdown","bce22c52":"markdown","301ab20e":"markdown","30ebf76e":"markdown","eedd42fd":"markdown","4e83d22a":"markdown","91bda9a1":"markdown","48328a26":"markdown","48046ece":"markdown","2599e3ac":"markdown","00835e11":"markdown","ee2e6e29":"markdown","f83d6b7d":"markdown","61e11172":"markdown","5b3d13f0":"markdown","056a14c4":"markdown","7d7b6b58":"markdown"},"source":{"ba86e4d4":"# linear algebra\nimport numpy as np\n# data processing\nimport pandas as pd\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algortithmic packages\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","d4ee57ef":"print(os.listdir(\"..\/input\/titanic\/\"))","c24b96b6":"# loading Datasets\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#printing first 5 rows of data\ntrain_df.head()","0083c644":"# let's display the columns names\ntrain_df.columns","9cd457ab":"# printing shape of Train Dataframe\ntrain_df.shape","6802b882":"train_df.info()","30ecd5dd":"test_df.info()","9d0672e2":"train_df.describe()","ea7eacd7":"train_df.head()","451dd4ed":"# count of null values in each column\ncount  = train_df.isnull().sum().sort_values(ascending = False)\n\n# percentage of Null Values in each column\npercent = train_df.isnull().sum()\/len(train_df)*100\n\n# rounding and arranging the percentage\npercent = round(percent,2).sort_values(ascending = False)\n\n# concatenating count and percentage into one\nmissing_data = pd.concat([count,percent], axis = 1)\nmissing_data.columns = ['Count', 'Percent']\n# printing top 5 rows\nmissing_data.head()","263b463a":"sns.barplot(x = 'Pclass', y = 'Survived' , data = train_df)\nplt.show()","33f5849f":"f, ax = plt.subplots(1,2, figsize=(20,8))\n\ncolors = [\"#FA5858\", \"#64FE2E\"]\nlabels =\"Not Survived\", \"Survived\"\n\nplt.suptitle('Information on Survival', fontsize=20)\n\ntrain_df[\"Survived\"].value_counts().plot.pie(explode=[0,0.05], autopct='%1.2f%%', ax=ax[0], shadow=True, colors=colors, \n                                             labels=labels, fontsize=15, startangle=45)\n\n\n\nax[0].set_xlabel('Survived vs Non Survived percentage', fontsize=14)\nax[0].set_ylabel('')\n\npalette = [\"#64FE2E\", \"#FA5858\"]\n\nsns.barplot(x = 'Pclass', y = 'Survived' , hue = 'Sex' , data = train_df, palette=palette)\nax[1].set(ylabel=\"(Percentage of Passengers)\")\nax[1].set_xticklabels(train_df[\"Pclass\"].unique(), rotation=0, rotation_mode=\"anchor\")\nplt.show()","499301d4":"train_df[(train_df.Pclass == 3) & (train_df.Survived == 1)].Sex.value_counts()\/len(train_df[(train_df.Pclass == 3) & (train_df.Survived == 1)].Sex)*100","2524412b":"survived = 'survided'\nnot_survived = 'not_survived'\nfig, axes = plt.subplots(nrows = 1 , ncols = 2 , figsize = (18,8))\nwomen = train_df[train_df['Sex'] == 'female']\nmen = train_df[train_df['Sex'] == 'male']\n\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=8, label = survived , ax=axes[0], kde = False)\nax = sns.distplot(women[women['Survived'] == 0].Age.dropna(), bins = 40, label=not_survived, ax = axes[0], kde =False)\nax.set_title('Female')\n\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=8, label = survived , ax=axes[1], kde = False)\nax = sns.distplot(men[men['Survived'] == 0].Age.dropna(), bins = 40, label=not_survived, ax = axes[1], kde =False)\n_ = ax.set_title('Male')\nax.legend()\nplt.show()","287905e7":"facetgrid = sns.FacetGrid(train_df , row = 'Embarked', height = 4.5 , aspect =1.8)\nfacetgrid.map(sns.pointplot, 'Pclass','Survived', 'Sex', order=None, hue_order=None)\nfacetgrid.add_legend()\nplt.show()","231ade02":"grid = sns.FacetGrid(train_df, row = 'Pclass', col='Survived', hue_order=None, height = 3, aspect=2)\ngrid.map(plt.hist, 'Age', alpha=0.7, bins = 20)\nplt.show()\n","37023629":"data = [train_df , test_df]\nfor dataset in data:\n  dataset['Relatives']=dataset['Parch']+dataset['SibSp']\n  dataset.loc[dataset['Relatives']>0,'Alone']=0\n  dataset.loc[dataset['Relatives']==0,'Alone']=1\n  dataset['Alone']=dataset['Alone'].astype(int)","fc70248e":"train_df.head()","a85a302d":"train_df.Alone.value_counts()","0652aee8":"f, ax = plt.subplots(1,3, figsize=(20,8 ))\nplt.suptitle('Information on Survival for Alone vs With Family', fontsize=20)\n\ntrain_df[\"Alone\"].value_counts().plot.pie(explode=[0,0.05], autopct='%1.2f%%', ax=ax[0], shadow=True, colors=colors, \n                                             labels=['Alone', 'With Family'], fontsize=15, startangle=45)\nsns.barplot(x = 'Alone', y = 'Survived',data = train_df , ax = ax[1])\nsns.barplot(x = 'Alone', y = 'Survived', hue = 'Sex',data = train_df,  ax = ax[2])\nplt.show()","c80a5a25":"plt.figure(figsize=(16,7))\nsns.pointplot(x='Relatives', y = 'Survived',hue = 'Sex', data= train_df )\nplt.show()","d20f48a9":"train_df = train_df.drop(['PassengerId'], axis = 1)\ntrain_df.head()","c63920b4":"import re\n\ndeck =  {'A':1  , 'B': 2 , 'C': 3, 'D':4 ,'E' : 5 , 'F':6 , 'G':7 , 'U':8}\ndata = [train_df , test_df]\nfor dataset in data:\n  dataset['Cabin']=dataset['Cabin'].fillna('U0')\n  dataset['Deck']=dataset['Cabin'].map(lambda x : re.compile(\"([a-zA-Z]+)\").search(x).group())\n  dataset['Deck']=dataset['Deck'].map(deck)\n  dataset['Deck']=dataset['Deck'].fillna(0)\n  dataset['Deck']=dataset['Deck'].astype(int)","8c06f55b":"train_df.Deck.value_counts()","45ab7de2":"# Same code as above, Regular Expression Simplified\n\nimport re\n\ndeck =  {'A':1  , 'B': 2 , 'C': 3, 'D':4 ,'E' : 5 , 'F':6 , 'G':7 , 'U':8}\ndata = [train_df , test_df]\n\nfor dataset in data:\n  dataset['Cabin']=dataset['Cabin'].fillna('U0')\n  dataset['Deck']=dataset['Cabin'].map(lambda x : x[0])\n  dataset['Deck']=dataset['Deck'].map(deck)\n  dataset['Deck']=dataset['Deck'].fillna(0)\n  dataset['Deck']=dataset['Deck'].astype(int)\n\n  \n  \ntrain_df.Deck.value_counts()","2c45ceea":"data = [train_df,test_df]\nfor dataset in data:\n  dataset=dataset.drop(['Cabin'], axis = 1)","c0fe7c5e":"train_df=train_df.drop('Cabin', axis = 1)\ntest_df=test_df.drop('Cabin', axis = 1)","fa6da18e":"data = [train_df , test_df]\nmean = train_df['Age'].mean()\nstd  = test_df['Age'].std()\n\n\nfor dataset in data:\n  count_of_null = dataset['Age'].isnull().sum()\n  \n  rand_age = np.random.randint(mean-std,mean+std, size = count_of_null)\n  \n  age_slice = dataset['Age'].copy()\n  age_slice[np.isnan(age_slice)]= rand_age\n  \n  dataset['Age']=age_slice\n  dataset['Age']=dataset['Age'].astype(int)","2d0a6650":"train_df['Embarked'].describe()","5cee8868":"train_df['Embarked'] = train_df['Embarked'].fillna('S')\ntest_df['Embarked'] = test_df['Embarked'].fillna('S')","1eefcfb5":"train_df.info()","a4164644":"test_df.info()","9400bdb8":"test_df.Fare = test_df.Fare.fillna(mean)\ntest_df.Fare.isna().sum()","f75f6627":"test_df.info()","a50317f5":"train_df.Name.head()","8102d038":"data = [train_df,test_df]\n\nfor dataset in data:\n  dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\\.', expand = False)","701e1f29":"train_df.Title.value_counts()","81d2cdac":"data = [train_df,test_df]\n\nfor dataset in data:\n  dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don','Dr',\\\n                                              'Major','Rev','Sir','Jonkheer','Dona'], 'Rare')\n  dataset['Title'] = dataset['Title'].replace('Mlle' , 'Miss')\n  dataset['Title'] = dataset['Title'].replace('Ms' , 'Miss')\n  dataset['Title'] = dataset['Title'].replace('Mme' , 'Mrs')\n  ","735f6ef9":"titles={'Mr':1,'Miss':2,'Mrs':3, 'Master':4, 'Rare':5}\n\nfor dataset in data:\n  dataset['Title']=dataset['Title'].map(titles)","a470628f":"print(train_df.Title.isna().sum())\nprint(test_df.Title.isna().sum())","1db6c74d":"train_df = train_df.drop(['Name'], axis = 1)\ntest_df = test_df.drop(['Name'], axis = 1)","8f4879c9":"train_df.Sex.value_counts()","6d1600b8":"gender = {'male':0 , 'female':1}\ndata = [train_df,test_df]\n\nfor dataset in data:\n  dataset['Sex']=dataset['Sex'].map(gender)","ed78f8b9":"train_df.Ticket.head()","42777b48":"train_df=train_df.drop('Ticket', axis = 1)\ntest_df=test_df.drop('Ticket', axis = 1)","8b749b42":"ports = {'S':0,'C':1, 'Q':2}\ndata = [train_df,test_df]\n\nfor dataset in data:\n  dataset['Embarked']=dataset['Embarked'].map(ports)","5e2fa9c3":"data = [train_df,test_df]\nfor dataset in data:\n  dataset['Age']=dataset['Age'].astype(int)\n  dataset.loc[dataset['Age']<=11, 'Age']=0\n  dataset.loc[(dataset['Age']>11) & (dataset['Age']<=18), 'Age']=1\n  dataset.loc[(dataset['Age']>18) & (dataset['Age']<=22), 'Age']=2\n  dataset.loc[(dataset['Age']>22) & (dataset['Age']<=27), 'Age']=3\n  dataset.loc[(dataset['Age']>27) & (dataset['Age']<=33), 'Age']=4\n  dataset.loc[(dataset['Age']>33) & (dataset['Age']<=40), 'Age']=5\n  dataset.loc[(dataset['Age']>40) & (dataset['Age']<=66), 'Age']=6\n  dataset.loc[(dataset['Age']>66), 'Age']=6","461f1d19":"train_df.Age.value_counts()","92a74007":"train_df.head()","7121a3b6":"data = [train_df,test_df]\n\nfor dataset in data:\n  dataset.loc[dataset['Fare']<=7.91, 'Fare']=0\n  dataset.loc[(dataset['Fare']>7.91) & (dataset['Fare']<=14.454), 'Fare']=1\n  dataset.loc[(dataset['Fare']>14.454) & (dataset['Fare']<=31), 'Fare']=2\n  dataset.loc[(dataset['Fare']>31) & (dataset['Fare']<=99), 'Fare']=3\n  dataset.loc[(dataset['Fare']>99) & (dataset['Fare']<=250), 'Fare']=4\n  dataset.loc[(dataset['Fare']>250) , 'Fare']=5\n  dataset['Fare']=dataset['Fare'].fillna(0)\n  dataset['Fare']=dataset['Fare'].astype(int)\n\ntrain_df.Fare.value_counts()","f996dc3a":"# test_df[test_df.Fare.isna()==True]","5e93eb60":"data = [train_df, test_df]\n\n\nfor dataset in data:\n  dataset['age_class'] = dataset['Age']*dataset['Pclass']\n  \n  \ntrain_df.head()","b8557e59":"# Let's see the top 5 row of processed dataset\ntrain_df.head()","36e40900":"X_train = train_df.drop('Survived', axis = 1)\ny_train = train_df['Survived']\n\nX_test = test_df.drop('PassengerId' ,  axis = 1)","54131571":"# creating model object\nsgd = linear_model.SGDClassifier(max_iter = 5, tol = None)\n\n# Fitting model on Data\nsgd.fit(X_train,y_train)\n\n#using model to predict\ny_pred = sgd.predict(X_test)\n\n# Storing prediction accuracy\nacc_sgd = round(sgd.score(X_train,y_train)*100,2)\nprint(acc_sgd)","06a478e6":"logreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\n\n\nacc_log=round(logreg.score(X_train,y_train)*100,2)\nprint(acc_log)","4c50712a":"# Creating a dictionary with Name of models as keys and Model Objects as values\ndict_model = {'sgd':linear_model.SGDClassifier(max_iter = 5, tol = None), \n             'log_reg':LogisticRegression(),\n             'decision_tree':DecisionTreeClassifier(),\n             'random_forest':RandomForestClassifier(n_estimators = 100),\n             'knn_classifier': KNeighborsClassifier(n_neighbors= 3),\n             'gaussian':GaussianNB(),\n             'perceptron':Perceptron(max_iter=5),\n             'linear_svc':LinearSVC()\n             }","619846c0":"# dictionary to store the name of model as key and respective accuracy as value\ndict_accuracies={}\n\nfor name,classifier in dict_model.items():\n  dict_model[name].fit(X_train,y_train)\n  score = dict_model[name].score(X_train,y_train)\n  dict_accuracies[name]=round(score*100,2)\n  \n","dc38ce45":"result_df=pd.DataFrame.from_dict(dict_accuracies,orient = 'index',columns = ['Score'])\nresult_df= result_df.sort_values(by = 'Score', ascending = False)\nresult_df","c1157c9f":"rf_final = RandomForestClassifier(n_estimators = 100 , oob_score = True)\nrf_final.fit(X_train , y_train)\n\nrf_final_score = rf_final.score(X_train,y_train)*100\nprint(round(rf_final_score,2,), \"%\")","4030c273":"print(\"OOB Score: \", round(rf_final.oob_score_ *100, 2))","addaa5b0":"#Importing cross_val_score\nfrom sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators =  100, random_state = 42)\n\n# Passing our RF model, training data, number of folds and evaluation metric to cross_val_score object\ncv_score = cross_val_score(rf, X_train, y_train, cv = 5,  scoring = 'accuracy')","046e3439":"print(\"The results of cross validation are:\")\nprint(\"Scores on each fold:\",cv_score)\nprint(\"Mean:\",cv_score.mean())\nprint(\"Standard Deviation:\", cv_score.std())","564fb67d":"# Parameter Grid to look for best Parameters\nparam_grid = { \"n_estimators\": [100, 200,500,1000],\n              \"criterion\" : [\"gini\", \"entropy\"], \n              \"min_samples_leaf\" : [1, 5, 10], \n              \"min_samples_split\" : [2, 5, 10]\n              }","5e63a944":"from sklearn.model_selection import GridSearchCV, cross_val_score\nrf = RandomForestClassifier(max_features='auto', oob_score=True, random_state = 42, n_jobs=-1)\n\ngs_clf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1,\n                           scoring = 'accuracy')\n\ngs_clf.fit(X_train, y_train)\n\n# Displaying the best parameters found by gridSearch\nprint(gs_clf.best_params_)","4035901a":"# Passing the dictionary of best params \nrandom_forest_final = RandomForestClassifier(**gs_clf.best_params_, oob_score = True , n_jobs = -1, random_state = 42)\nrandom_forest_final.fit(X_train,y_train)\n\n# Let's see the training scores now\nprint(\"Training Score: \", round(random_forest_final.score(X_train,y_train)*100,2),\"%\")","886a295c":"print(\"OOB Score for baseline Model: \", round(rf_final.oob_score_ *100, 2))\nprint(\"OOB Score for tuned Model: \", round(random_forest_final.oob_score_ *100, 2))","ee4f6ba1":"from sklearn.metrics import confusion_matrix\n\n# Calulating predicted values for train data\ny_pred = random_forest_final.predict(X_train)\n\n# Calculating and displaying the confusion matrix\nconfusion_matrix(y_train, y_pred)","60a70791":"from sklearn.metrics import precision_score , recall_score\nprint(\"Precision Score:\" , round(precision_score(y_train,y_pred)*100,2),\"%\" )\nprint(\"Recall Score:\", round(recall_score(y_train,y_pred)*100,2),\"%\")\n","af3e7936":"from sklearn.metrics import f1_score\nf1_score(y_train,y_pred)","1d204b62":"## TODO","987f7bc0":"## TODO","617d442a":"feature_importance = pd.DataFrame({'Feature':X_train.columns, 'Importance Score':random_forest_final.feature_importances_})\nfeature_importance = feature_importance.sort_values(by='Importance Score',  ascending = False).set_index('Feature')\n\n#printing feature importance score of all the features\nfeature_importance","aafdb891":"plt.figure(figsize = (16,8))\nsns.barplot(x=feature_importance.index , y = 'Importance Score' ,data  = feature_importance)\nplt.show()","edcc52e6":"from sklearn.feature_selection import SelectFromModel\nsfm = SelectFromModel(random_forest_final, threshold=0.05 )\n\n# Train the selector\nsfm.fit(X_train, y_train)\n\n# Printing the names of the important features\nfor feature_list_index in sfm.get_support(indices=True):\n    print(X_train.columns[feature_list_index])","20873fe8":"# Transform the data to create a new dataset containing only the most important features\n# Note: We have to apply the transform to both the training X and test X data.\nX_important_train = sfm.transform(X_train)\nX_important_test = sfm.transform(X_test)","49608d41":"# Create a new random forest classifier for the most important features\nrf_important = RandomForestClassifier( random_state = 42)\nrf = RandomForestClassifier(random_state = 42)\n\n# Train the new classifier on the new dataset containing the most important features\nrf_important.fit(X_important_train, y_train)\nrf.fit(X_train, y_train)\n","4e538600":"rf.score(X_train,y_train), rf_important.score(X_important_train,y_train)","05d3a7e0":"# To-Do","3bab5c1d":"#### Missing Data in cabin :\n As a reminder, we have to deal with Cabin (687), Embarked (2) and Age (177). First I thought of dropping the 'Cabin' variable but then I found something interesting.\n\n`A cabin number looks like \u2018C123\u2019 where the first letter refers to the deck.`\n \n- Therefore we\u2019re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. \n\n- The missing values will be replaced with to 'U0'. In the picture below you can see the actual decks of the titanic, ranging from A to G.\n\n- For Passengers with missing Data in Cabin, we will put them in Deck U representing Unknown Deck","2ad1d29c":"### Feature Importance ","55099b8d":"#### More Features\n\n\nWe will now create categories within the following features:\n\n#### Age Category Binning:\nNow we will create bins fro the 'age' feature. First we will convert it from float into integer. Then we will create the new 'AgeGroup\" variable, by categorizing every age into a group. Note that it is important to place attention on how you form these groups, since you don't want for example that 80% of your data falls into group 1.","0b6fbf9f":"Above you can clearly see that the recall is falling of rapidly at a precision of around 85%. Because of that you may want to select the precision\/recall tradeoff before that - maybe at around 75 %.\n\nYou are now able to choose a threshold, that gives you the best precision\/recall tradeoff for your current machine learning problem. If you want for example a precision of 80%, you can easily look at the plots and see that you would need a threshold of around 0.4. Then you could train a model with exactly that threshold and would get the desired accuracy.\n\nAnother way is to plot the precision and recall against each other","85e0db7a":"#### The final Model: Random Forest\nRandom forest is an ensemble based modeling algorithm which takes a subset of observations and a subset of variables to build a decision trees. It builds multiple such decision tree and amalgamate them together to get a more accurate and stable prediction. \n\nThis is direct consequence of the fact that by maximum voting from a panel of independent judges, we get the final prediction better than the best judge.","3aab9473":"#### 1. Let's plot a couple of graphs to qunatify the survival percentage and Survival Percentage in Each PClass for each Gender","1c14af55":"#### Missing data in Age\nNow we can tackle the issue with the age features missing values:  \n\nI will create an array of size n that contains random numbers, which are computed based on the mean age value in regards to the standard deviation where n is equal to number of missing Values.","2b5746d5":"- from the output of describe Above, we can see that 38% out of the training-set survived the Titanic. \n- We can also see that the passenger ages range from 0.4 to 80. \n- On top of that we can already detect some features, that contain missing values, like the 'Age' feature. \n- Also we can find outliers if any by visual observation of quartiles.\n\nLet's look at head of the data","988029ac":"Another great advantage of random forest is that they make it very easy to measure the relative importance of each feature.\n\nThe reason is because the tree-based strategies used by random forests naturally rank the features by measuring how well they improve the purity of the node. This mean decrease in impurity over all trees (called gini impurity). Nodes with the greatest decrease in impurity happen at the start of the trees, while notes with the least decrease in impurity occur at the end of trees.\n\nIt computes this score automatically for each feature after training and scales the results so that the sum of all importances is equal to 1. \n\nLet's see the feature importance scores for our dataset:","93939edd":"- Title and Sex are the most important features in deciding the chances or survival\n- not_alone and Parch doesn't play a significant role in our random forest classifiers prediction process.\n- Infact we can drop them and simplify our model further if it doesn't impact our evaluation metric i.e accurcacy score a lot. Because of that I will drop them from the dataset and train the classifier again. \n\nSee the code below to select features based on a certain threshold score from feature importance","79139ff2":"#### Using best params for modeling.","738f50a8":"#### All the missing values in our TrainDF and Test_DF dataset has been dealt.\n#### Let's check info of on both the Dataframes to verify this\n","436060e7":"#### Using Name to Extract useful Information\n We will use the Name feature to extract the Titles from the Name, so that we can build a new feature out of that.","1df6ef78":"#### GridSearchCV:\nGridSearchCV helps us in finding the best parameter by performing exhaustive search all the combination of parameters passed using param_grid dictionary and picks the optimal parameter from all the possible sets of parameters.","4e225104":"### Evaluation Metrics","928cb7ff":"#### 4. Assuming, Being alone and survival rate has high dependency. let's Create a new binary feature - 'Alone' ","7464e4e4":"#### Let's check info of the dataset to get an idea of it's datatype and how it's looking","00a5c368":"we can now drop the cabin feature now","815540f5":"As can be seen by the accuracy scores, our original model which contained all the features is 92.2% accurate while the our \u2018limited\u2019 model which contained only half features is 87.8% accurate. \n\nThus, for a small cost in accuracy we halved the number of features in the model and simplified it resulting in reduced training time and more generalizable result.","f2f8350c":"#### Sex is categorical but we have to convert 'Sex' feature into numeric.","a975421c":"### Basic Pre-Processing","e604ed00":"You can see the Distribution of survived passenger from different age Groups in Each Class","cd601979":"#### 1. Precision Score tells us that 83.61% of passengers predicted by our model as survived are Correct.\n\n#### 2. The recall tells us that it predicted the survival of 73.1 % of the people who actually survived.\n\n","e84da0ea":"Very interesting to note that survival rate slowly increases and then falls down with Increase in number of Relatives","542d5b86":"80.25 % accuracy.\n\nGreat right.\n\nLet's perform Cross Validation Check now to verify model performance\n\n","04ddf12c":"#### Let's take a look at description of columns having numerical Values in Dataset","1036ad17":"Training Score of our model reduced to 84.18%.\n\nBut we thought it would Increase the Model performance Right?\n\nYes, As we know trainig score is not the right criteria to Judge thee Model Performance.\nWe should be looking at cross Validation Score or OOB score to evaluate the model performance.\n\nLet's check the Out of Bag Error and compare it with our baseline RandomForest Model","9ea387bf":"#### 3. Embarked, Pclass and Sex:","22426380":"### Loading Data and Diagnostics\nLet's import all required packages Modules","e78f4147":"#### Embarked\nLet's convert Embark into a numerical Feature","9c0c368a":"#### Creating Data Subset With Only The Most Important Features","7f717a0f":"- As you can see the decision tree and Random Forest are best performers on our dataset.\n\n- From here Onwards, we will pick RandomForest for further analysis and Optimization since it uses bagging technique and is less prone to Overfitting.\n\n- It also provides OOB(Out of Bag error estimation) and Feature ranking\/importance.\n\n- Read more about advantages of RandomForest from sklearn's official Documentation.\n\nYou can pick and try the steps on any one of the above algorithms as well.","47c3459f":"Great 78.23 % accuracy with our first Model.\n\nNow Let's try the classical Logistic Regression Model","1d641b39":"Another way to evaluate and compare your binary classifier is provided by the ROC AUC Curve. This curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances), instead of plotting the precision versus the reca\n\nThe ROC AUC Score is the corresponding score to the ROC AUC Curve. It is simply computed by measuring the area under the curve, which is called AUC.\n\nA classifiers that is 100% correct, would have a ROC AUC Score of 1 and a completely random classiffier would have a score of 0.5.","2353ff31":"## Modeling","7ef64b0d":"#### 2. Age vs Sex\nTo check the relationship between survival vs age\/sex, let's plot the distribution of data.","cc7b25b4":"- Females who embarked from `port S` have a higher percentage of Survival Compared to Male Counterparts from Same port in each P Class\n- Males who embarked from `port C` have a higher percentage of Survival Compared to FeMale Counterparts from Same port in each P Class\n- Females who embarked from `port O` have a higher percentage of Survival Compared to Male Counterparts from Same port in each P Class","68f97f4f":"### Visualization\nIn this section, we will derive relationships and insight from our Training Dataset by plotting them.\n<br>\n<br>\n\n","96036065":"#### Let's divide our data into Feature or Preictor Variables i.e X and Label or Predicted variable i.e y","68669f48":"### Also let's plot the bars for this","89b04c82":"#### From the table above, we can note following things:\n- First of all, that we need to convert a lot of features into numeric ones later on, so that the machine learning algorithms can process them.\n- Furthermore, we can see that the features have widely different ranges, that we will need to convert into roughly the same scale.\n- We can also spot some more features, that contain missing values (NaN = not a number), that wee need to deal with.\n\nLet's take a more detailed look at what data is actually missing:","25e10b13":"- Now that we have a proper model, we can start evaluating it's performace in a more accurate way. \n- Previously we only used accuracy and the oob score, which is just another form of accuracy. \n- The problem is just, that it's more complicated to evaluate a classification model than a regression model.\n\n- Classification accuracy alone can be misleading if you have an unequal number of observations in each class or if you have more than two classes in your dataset.\n\n- There are many Evaluation Criterias catering to different kind of Problem statements and situtions\n\nWe will discuss some of the important ones in the following section:","469d8e8a":"\nThe first row contains passengers who did not survive:\n\n- \u201cTrue negative\u201d:  500 passengers were correctly classified as not survived.\n- 49 were wrongly classified as survived (false Positives).\n\nThe second row is about the passengers who survived: \n- 92 passengers were wrongly classified as Not-survived (False Negtives) \n- 250 were correctly classified as survived and they actually survived (true positives).\n\n","2315360e":"#### Now let's train our first Model: A Stochastic Gradient Descent Classifier","965f2927":"### Cross Validation Evaluation\nK-Fold Cross Validation randomly splits the training data into K subsets called folds. Let's image we would split our data into 4 folds (K = 4). Our random forest model would be trained and evaluated 4 times, using a different fold for evaluation everytime, while it would be trained on the remaining 3 folds.\n\n\nThe image below shows the process, using 4 folds (K = 4). Every row represents one training + evaluation process. In the first row, the model get's trained on the first, second and third subset and evaluated on the fourth. In the second row, the model get's trained on the second, third and fourth subset and evaluated on the first. K-Fold Cross Validation repeats this process till every fold acted once as an evaluation fold.\n\n\nThe result of our K-Fold Cross Validation example would be an array that contains 4 different scores. We then need to compute the mean and the standard deviation for these scores.\n\n\nThe code below perform K-Fold Cross Validation on our `random forest model`, using 10 folds (K = 10). Therefore it outputs an array with 10 different scores.","cf8c34a5":"#### All the models have been fitted and respective accuracies are stored in Dictionary.\nLet's convert the dictionary into dataframe and display the Values","74bd3ded":"#### Uh oh! 1 value is missing in Fare Column in test data set. Let's deal with it before moving forward","ccd6dd9e":"#### Let's plot the bargraph to indicate chances of survival in Each PClass first: ","ec377ab5":"#### 5. No_of_Relatives vs Survived","ea1c63a1":"#### There are some missing values in Age and Cabin Column in Train Data while Age, Cabin and Fare in test data.\n we will be dealing with these missing values in coming sections","2aa9ba31":"#### From 80.47 to 82.15% by just optimizing and tuning the Parameters.\nIsn't that Cool?\n\nyou can also try and play with other parameters to tune the performance of the Model further.","b1d6fd32":"- You can notice, Cabin has the highest percentage of missing values.\n- The Embarked feature has only 2 missing values, which can easily be filled. \n- It will be much more tricky, to deal with the 'Age' feature, which has 177 missing values. \n-The 'Cabin' feature needs further investigation, but it looks like that we might need to drop it from the dataset, since 77 % of it are missing.\n\n","f6d9fa04":"#### F1-Score: 2 * (precision * recall) \/ (precision + recall)\n- You can combine precision and recall into one score, which is called the F-score. \n- The F-score is computed with the harmonic mean of precision and recall. Note that it assigns much more weight to low values. \n- As a result of that, the classifier will only get a high F-score, if both recall and precision are high.\n- Therefore, this score takes both false positives and false negatives into account. \n\nIntuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution.\n","b779596c":"#### Fare binning\nSimilarly for the fare , we do same approach, For the 'Fare' feature, we need to do the same as with the 'Age' feature. But it isn't that easy, because if we cut the range of the fare values into a few equally big categories, 80% of the values would fall into the first category. Fortunately, we can use sklearn \"qcut()\" function, that we can use to see, how we can form the categories.\n\nCurrent dataframe looks as follows\n","efe18373":"### Feature Engineering ","34f14d0b":"we can derive:-\n- 61.62% Passengers did not survived while 38.38% survived the Titanic Disaster\n- Females had a higher survival percentage in each class compared to male counterparts\n- PClass 3 has highest percentage of People surviving both in Male and Female category, followed by PClass 1 and PClass 2","d969e6b4":"So No Null values anymore. Let's Move ahead","fa4720fb":"- n_estimators is number of trees to be fitted and OOB is Out of Bag error.\n- Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging) to sub-sample data samples used for training.\n- Using the out-of-bag error estimate removes the need for a set aside test set.\n- Infact, It is cosidered as good as Cross Validation technqiue for measuring performance of a model.\n\nYou will also learn about cross Validation Technique in next section but before that let's see the OOB score of our model.","faa50905":"Since the Ticket attribute has 681 unique tickets, it will be a bit tricky to convert them into useful information. So we will drop it from the dataset.","dda67b17":"#### 3. Let's create another plot below for distribution of P Class:\n","bb30e4aa":"The above grouping can also be achieved in single line with Pd.Cut. You can give a try.\n\nNow Let's see the Groups.","1dea58b7":"#### Let's Train A New Random Forest Classifier Using Only Most Important Features","04cf8d06":"There we have it, a 78 % F-score. The score is not that high, because we have a recall of 73.1%.\n\nBut unfortunately the F-score is not perfect, because it favors classifiers that have a similar precision and recall. This is a problem, because you sometimes want a high precision and sometimes a high recall. The thing is that an increasing precision, sometimes results in an decreasing recall and vice versa (depending on the threshold). \n\nThis is called the precision\/recall tradeoff. We will discuss this in the following section.","cfe53d90":"we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1.\n\nLet's go Deeper into it\n","edcd062e":"Another importannt criteria for performance Evaluation is precision and Recall score.\n\nPrecision and Recall can be Calculated using confusion Matrix.\n- Precision: Percentage of People that were correctly classified as Survived out of total predicted survived\n- Recall: Percentage of people correctly classified as Survived out of total passenger who survived","0243ba47":"#### Let's Load the dataset in a pandas Dataframe","e4cd9c7d":"In first step,we will see if any features are irrelevant and drop them. \nAs you can see passenger ID is irrelevant.  I will drop 'PassengerId' from the train set, because it does not contribute to a persons survival probability. \n\nI will not drop it from the test set, since it is required there for the submission","0cbd17f8":"### Missing Value Treatment","22af9ba6":"#### Let's Create a new Feature A combination of Age and Class","bce22c52":"#### Missing data in Embarked:\nSince the Embarked feature has only 2 missing values, we will just fill these with the most common one.\n\n","301ab20e":"### 1. Confusion Matrix:\n- A confusion matrix is a summary of prediction results on a classification problem.\n\n- The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.\n\n- The confusion matrix shows the ways in which your classification model\nis confused when it makes predictions.\n\n- It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made.\n\n- It is this breakdown that overcomes the limitation of using classification accuracy alone","30ebf76e":"*Mean-while how about checking cross validation scores for the rest of the models?\nTO-DO: Write a function\/loop which takes dictionary containing model names and objects and returns a dataframe with Modelname as Index, mean_cross_val_score as column sorted by mean_cross_val_score descending.*","eedd42fd":"### Hyper Parameter Tuning:\n\nSo we are getting our accuracies in range of 80%-82% till now.\n\nCan we improve our model further?<br>\nYes! by performing hyperparameter Tuning of our Final model, we can improve the scores.\n\n\n\nIn the upcoming section you will see how to tune Hyper parameters and how to use them in our final model using GridSearch.\n\nWe will optinimize following parameters:\n- n_estimators: The number of trees in the forest\n- criterion: The function to measure the quality of a split\n- min_samples_split: The minimum number of samples required to split an internal \nnode.\n- min_samples_leaf: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. \n\n\nYou can read more about random forest hyper parameter by clicking [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html).","4e83d22a":"## Problem Statement\n\n### Introduction\n\nIn this kernel I will go through the whole process of creating a machine learning model on the famous Titanic dataset, which is used by many people all over the world. It provides information on the fate of passengers on the Titanic, summarized according to economic status (class), sex, age and survival. In this challenge, I will predict whether a passenger on the titanic would have been survived or not.\n\n### About Titanic \n\nRMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after it collided with an iceberg during its maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard the ship, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history. The RMS Titanic was the largest ship afloat at the time it entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. The Titanic was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, her architect, died in the disaster.\n\n### About the Data Set\n\nWe have data of all passengers and the problem statement is to predict the survival  of a passenger given his current data inputs. The complete data can be downloaded here - www.kaggle.com\/c\/titanic\n\n\n### Titanic - The sad story\n- Exploratory Data Analysis I\n- Exploratory Data Analysis II\n- EDA - Visualization\/Plotting\n- Preprocessing\n- Preprocessing II\n- Feature Engineering\n- Modelling\n- Advanced Modelling\n- Final Model\n- Model Evaluation\n- Feature Importance\n- Feedback\n\n","91bda9a1":"#### Let's Perform Fare binning Now:\n","48328a26":"- You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40.\n\n- For men the probability of survival is very low between the age of 5 and 18, but that isn't true for women. Another thing to note is that infants also have a little bit higher probability of survival.\n\n- Since there seem to be certain ages, which have increased odds of survival and because I want every feature to be roughly on the same scale, We will create age groups later on.","48046ece":"## Conclusion","2599e3ac":"#### Description of Columns in Dataset\nThe training set has 891 examples and 11 features along with the Target variable(survived). Datatypes for 2 of the features are floats, 5 are integers and 5 are objects. Below I have listed the features with a short description:\n\n      survival:   Survival\n      PassengerId: Unique Id of a passenger.\n      pclass:     Ticket class   \n      sex   :     Sex \n      Age   :     Age in years    \n      sibsp :     # of siblings \/ spouses aboard the Titanic  \n      parch :     # of Childs \/ children aboard the Titanic  \n      ticket:     Ticket number   \n      fare  :     Passenger fare  \n      cabin :     Cabin number    \n      embarked:   Port of Embarkation","00835e11":"- 60.27% of Passengers are alone while 39.73% passengers are with Family\n- As we can see people who were alone had a lower rate of survival in both Females as well as Males.\n- Females who were alone had a slightly higher chance of survival compared to Females with Family.","ee2e6e29":"This looks much more realistic than before. Our model has a average accuracy of 82% with a standard deviation of approx 4 %. The standard deviation shows us, how precise the estimates are . This means in our case that the accuracy of our model can differ + - 4%.\n\n\nI think the accuracy is still really good and since random forest is an easy to use model, we will try to increase it's performance even further in the upcoming section.\n\n\n","f83d6b7d":"#### Let's Compare The Accuracy Of Our Full Feature Classifier To Our Limited Feature Classifier","61e11172":"#### We will write a loop which will fit and predict accuracy for different classes of models\n\nand return a dataframe with respective accuracies","5b3d13f0":"#### So, both the dataframes have been processed and ready to be Modeled.\n\nlet's go to the most exciting part of this: Modeling","056a14c4":"#### Ticket & Fare","7d7b6b58":"Dropping name Column now as we have already extracted useful information from it and full name is of no use to us anymore"}}