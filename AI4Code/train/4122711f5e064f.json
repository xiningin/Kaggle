{"cell_type":{"9bf936cc":"code","762ef607":"code","103557cf":"code","8d857fe5":"code","e3fda6c6":"code","57a4af05":"code","8dfbeb32":"code","7dbaef93":"code","44f90c4d":"code","deceee32":"code","4925aac4":"code","9cf735c3":"code","01e6a4ef":"code","d097c667":"code","a07b3f7f":"code","351ce4eb":"markdown","827f81e1":"markdown","fdd4ca13":"markdown","74f56f2e":"markdown","8f5e5519":"markdown","eed375fd":"markdown","26b5626c":"markdown"},"source":{"9bf936cc":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tokenizers\nimport transformers\nfrom tqdm import tqdm\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nimport re","762ef607":"MAX_LEN = 64\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 8\nLEARNING_RATE = 3e-5\nEPOCHS = 3\nTRAINING_FILE = \"..\/input\/nlp-getting-started\/train.csv\"\nTEST_FILE = \"..\/input\/nlp-getting-started\/test.csv\"\nROBERTA_PATH = \"..\/input\/tf-roberta\"\n# TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n#         vocab_file=f\"{ROBERTA_PATH}\/vocab-roberta-base.json\", \n#         merges_file=f\"{ROBERTA_PATH}\/merges-roberta-base.txt\", \n#         lowercase=True,\n#         add_prefix_space=True\n#     )\nroberta_tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base', lower_case=True)","103557cf":"train = pd.read_csv(TRAINING_FILE).fillna('')\ntest = pd.read_csv(TEST_FILE).fillna('')\nprint(\"Training samples: {}\".format(train.shape[0]))\nprint(\"Test samples: {}\".format(test.shape[0]))\ntrain.head(5)","8d857fe5":"# remove urls\nurl = \"Great paper by Kalchbrenner https:\/\/arxiv.org\/pdf\/1404.2188.pdf?utm_medium=App.net&utm_source=PourOver\"\n\ndef remove_urls(text):\n    re_url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return re_url.sub('', text).strip()\n\nprint(remove_urls(url))\ntrain['text'] = train['text'].apply(lambda x : remove_urls(x))\ntest['text'] = test['text'].apply(lambda x : remove_urls(x))","e3fda6c6":"html = \"\"\"<div>\n<h1>Hey<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">removed tags<\/a>\n<\/div>\"\"\"\n# remove html tags\ndef remove_html(text):\n    re_html = re.compile(r'<.*?>')\n    return re_html.sub('', text)\n\nprint(remove_html(html))\ntrain['text'] = train['text'].apply(lambda x : remove_html(x)) \ntest['text'] = test['text'].apply(lambda x : remove_html(x)) ","57a4af05":"# remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text).strip()\n\nprint(remove_emoji(\"Difficult kernel \ud83d\ude14\ud83d\ude14\"))\ntrain['text'] = train['text'].apply(lambda x: remove_emoji(x))\ntest['text'] = test['text'].apply(lambda x: remove_emoji(x))","8dfbeb32":"# remove punctuations\npunct = 'Cristiano. is #king .l'\nimport string\ndef remove_puncts(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table).strip()\n\nprint(remove_puncts(punct))\ntrain['text'] = train['text'].apply(lambda x: remove_puncts(x))\ntest['text'] = test['text'].apply(lambda x: remove_puncts(x))","7dbaef93":"num_classes = train.target.nunique()\nnum_classes","44f90c4d":"n_train = train.shape[0]\ninput_ids = np.ones((n_train, MAX_LEN), dtype='int32')\nmask = np.zeros((n_train, MAX_LEN), dtype='int32')\n\n# roberta tokenizer\nfor k in range(train.shape[0]):\n    text = train.loc[k, 'text']\n    output = roberta_tokenizer.encode_plus(text, max_length=MAX_LEN, pad_to_max_length=True)\n    input_ids[k] = output['input_ids']\n    mask[k] = output[\"attention_mask\"]","deceee32":"n_test = test.shape[0]\ninput_ids_t = np.ones((n_test, MAX_LEN), dtype='int32')\nmask_t = np.zeros((n_test, MAX_LEN), dtype='int32')\n\n# roberta tokenizer\nfor k in range(test.shape[0]):\n    text = test.loc[k, 'text']\n    output = roberta_tokenizer.encode_plus(text, max_length=MAX_LEN, pad_to_max_length=True)\n    input_ids_t[k] = output['input_ids']\n    mask_t[k] = output[\"attention_mask\"]","4925aac4":"def create_dataset():\n    xtrain = [input_ids, mask]\n    xtest = [input_ids_t, mask_t]\n    \n    ytrain = tf.keras.utils.to_categorical(train['target'].values.reshape(-1, 1))\n    return xtrain, ytrain, xtest","9cf735c3":"xtrain, ytrain, xtest = create_dataset()\nprint(\"X train : {0}\".format(len(xtrain[0])))\nprint(\"Y train : {0}\".format(len(ytrain)))\nprint(\"X test : {0}\".format(len(xtest[0])))","01e6a4ef":"def build_model():\n    roberta = transformers.TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n    optim = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipnorm=2.0)\n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    roberta.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])\n    return roberta","d097c667":"Folds = 5\nkfold = KFold(n_splits=Folds)\npredictions = list()\n\nfor i, (train_idx, test_idx) in enumerate(kfold.split(xtrain[0])):\n    xtrain_fold = [xtrain[i][train_idx] for i in range(len(xtrain))]\n    xvalid_fold = [xtrain[i][test_idx] for i in range(len(xtrain))]\n    \n    ytrain_fold = ytrain[train_idx]\n    yvalid_fold = ytrain[test_idx]\n    \n    # class weights to deal with class imbalance\n    positive = train.iloc[train_idx, :].target.value_counts()[0]\n    negative = train.iloc[train_idx, :].target.value_counts()[1]\n    pos_weight = positive \/ (positive + negative)\n    neg_weight = negative \/ (positive + negative)\n\n    class_weight = [{0:pos_weight, 1:neg_weight}, {0:neg_weight, 1:pos_weight}]\n    \n    tf.keras.backend.clear_session()\n    \n    roberta = build_model()\n    roberta.fit(xtrain_fold, ytrain_fold, \n                batch_size=TRAIN_BATCH_SIZE, \n                epochs=EPOCHS, \n                class_weight=class_weight,\n                validation_data=(xvalid_fold, yvalid_fold))\n    val_preds = roberta.predict(xvalid_fold, batch_size=VALID_BATCH_SIZE, verbose=1)\n    val_preds = np.argmax(val_preds, axis=1).flatten()\n    print(metrics.accuracy_score(train.iloc[test_idx, :].target.values, val_preds))\n\n    preds = roberta.predict(xtest, batch_size=TRAIN_BATCH_SIZE, verbose=1)\n    predictions.append(preds)\n","a07b3f7f":"sample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n\npredictions = np.average(predictions, axis=0)\npredictions = np.argmax(predictions, axis=1).flatten()\nsample_submission['target'] = predictions\nsample_submission['target'].value_counts()\nsample_submission.to_csv('submission.csv', index=False)","351ce4eb":"### Remove emojis","827f81e1":"# Preprocessing","fdd4ca13":"### Remove punctuations","74f56f2e":"# Config","8f5e5519":"# Training","eed375fd":"### Remove urls","26b5626c":"### Remove Html "}}