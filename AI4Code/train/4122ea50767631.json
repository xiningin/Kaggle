{"cell_type":{"a1166086":"code","fae7e1a0":"code","cc86593d":"code","278b70a1":"code","7a53ab94":"code","bab79e4f":"code","7fe7fa82":"code","0566feb2":"code","c45b65f0":"code","bb6f1e27":"code","2d9288ec":"code","79d5f849":"code","c7d4344d":"code","93d71f82":"code","db5c6347":"code","a40d8a03":"code","cc14cb8f":"code","b5edc4af":"code","cf2e1fdc":"code","42373ebd":"code","911b63eb":"code","4063a068":"code","b31f0670":"code","3a27287d":"code","0a70af9e":"code","9d95f350":"code","4c4373b8":"code","e92d9fd8":"code","67e2e08e":"code","41888a4e":"markdown","b6dc9a89":"markdown","857ddc8f":"markdown","ab3bcc86":"markdown","299d03d6":"markdown","7672e2ca":"markdown","b3cdf8ae":"markdown","89b97dd8":"markdown"},"source":{"a1166086":"# Some interesting Lecture and Concepts\n# https:\/\/www.coursera.org\/learn\/ml-foundations\/notebook\/et8PR\/predicting-house-prices-assignment\n# https:\/\/www.kaggle.com\/mattcarter865\/boston-house-prices\n# http:\/\/www.neural.cz\/dataset-exploration-boston-house-pricing.html\n# https:\/\/jovianlin.io\/data-visualization-seaborn-part-3\/\n# http:\/\/www.bigendiandata.com\/2017-06-27-Mapping_in_Jupyter\/\n# https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n# https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n# https:\/\/www.kaggle.com\/darquesm\/house-prices-ensemble-regressors","fae7e1a0":"# Importing the Required Librairies\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns ; sns.set()\nimport sklearn\nimport math\nimport warnings\n\nfrom sklearn import preprocessing\nfrom scipy.stats import norm, skew #for some statistics\nfrom sklearn.preprocessing import QuantileTransformer, quantile_transform\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn import linear_model\nfrom sklearn.linear_model import RidgeCV, Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import ensemble\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\nfrom sklearn.metrics import r2_score\nfrom mpl_toolkits.mplot3d import Axes3D\n\npd.options.display.float_format = '{:.3f}'.format\n%matplotlib inline\nprint('The scikit-learn version is {}.'.format(sklearn.__version__))","cc86593d":"# Scaling data with a Standard Scaler\nUSE_SCALED_DATA = False                       # Bool : Scale or not the data                  \n\n# Filtering data parameters\nSUPPRESS_TOP_OUTLIERS = 10                     # Supressing top 2N outliers (Low & High)\nFILTER_ZIPCODE_EXCLUDE_TOP_DEVIANCE = 2        # Exclude zipcode with most deviance in price\nFILTER_SQFT_LIVING = {'min':20, 'max':7000}    # Filter by living surface\nFILTER_PRICE = {'min':2000, 'max':6000000}     # Filter by house price\nFILTER_YEAR_BUILT = {'min':1950, 'max':2012}   # Filter by built year\nFILTER_GRADE = {'min':-1, 'max':15}            # Filter by grade\n\n# Dataset Testing Ratio\nRATIO_TRAIN_TEST = 0.20                        # Exemple : 0.20 => 80% Training \/ 20\u00f9 Testing\n\n# xgboost parameters\nXGBOOST_PERFORM_GRID = False                   # Perform grid Search CV optimisation\nXGBOOST_CROSS_VALIDATION = False               # Perform cross validation of the model","278b70a1":"def compute_score(y_pred, y_test): \n    mse = mean_squared_error(y_test, y_pred)\n    print(\"Mean Squarred Error: %.4f\" % mse)\n    print(\"Root Mean Squarred Error: %.4f\" % math.sqrt(mse))\n    print(\"Mean Absolute Error: %.4f\" % mean_absolute_error(y_test, y_pred))\n    print(\"Variance Score (Best possible score is 1): %.4f\" % explained_variance_score(y_test, y_pred))\n    print(\"R2Score (Best possible score is 1): %.4f\" % r2_score(y_test, y_pred))\n    \ndef plot_prediction(y_pred, y_test):\n    fig, ax = plt.subplots()\n    ax.scatter(y_test, y_pred, edgecolors=(0, 0, 0))\n    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n    ax.set_xlabel('Measured Price')\n    ax.set_ylabel('Predicted Price')\n    ax.set_title('Price Prediction')\n    plt.show()\n    \ndef train_and_evaluate(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    compute_score(y_pred, y_test)\n    plot_prediction(y_pred, y_test)\n    try: \n        print(model.feature_importances_)\n    except AttributeError: \n        print(\"No feature_importances_ for this model\")","7a53ab94":"dataset = pd.read_csv(\"..\/input\/home_data(3).csv\", delimiter=\",\")        # Read dataset\ndataset = dataset.drop(columns=['id', 'date'])                           # Suppress unused columns\nprint(\"The Dataset shape is (row, col): {}\".format(dataset.shape))       # print the shape of the Dataset\ndataset.head()                                                           # Showing some lines of original datas","bab79e4f":"# Plotting all the features in histogram\nax = dataset.hist(figsize=(18,20))","7fe7fa82":"# Price stats\ny = dataset['price']       # Get the target price (y) \ny.describe().astype(int)   # Print some stats about price","0566feb2":"# Plotting price distribution\nwith warnings.catch_warnings():\n    ax = sns.distplot(dataset['price'], bins=200, fit=norm)\n    sns.set(rc={'figure.figsize':(7,5)})\n    ax.set_title('Price Target distribution')\n    ax.set(xlim=(50000, 1800000))","c45b65f0":"# find correlation between atributes\npearson = dataset.corr(method='pearson')\n\n# Draw a heatmap\nsns.set(rc={'figure.figsize':(30,12)})\nmask = np.zeros_like(pearson)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    ax = sns.heatmap(data=pearson, cmap=\"YlGnBu\", mask=mask, vmax=1, annot=True, square=True)","bb6f1e27":"# Plotting between year of built and number of floors\nax = sns.jointplot(dataset['yr_built'], dataset['floors'], kind='kde', \n                   joint_kws={'alpha':0.5}, \n                   xlim=(FILTER_YEAR_BUILT['min'], FILTER_YEAR_BUILT['max']), \n                   ylim=(0,4), \n                   height=6)","2d9288ec":"# Plotting between price of built and surface\n# NB: Max price & max sqft are fixed for better plotting\nax = sns.jointplot(dataset['price'], dataset['sqft_living'], kind='kde', \n                   joint_kws={'alpha':0.5}, \n                   xlim=(FILTER_PRICE['min'],1000000), \n                   ylim=(FILTER_SQFT_LIVING['min'], 4000),\n                   height=6)","79d5f849":"# Plot price repartition\nax = sns.jointplot(dataset['price'], dataset['sqft_living'], kind='scatter', \n              joint_kws={'alpha':0.5}, xlim=(0,6000000), ylim=(0,9000), height=6)","c7d4344d":"# Print top outliers\nsaleprice = dataset['price'][:,np.newaxis]\nlow_range = saleprice[saleprice[:,0].argsort()][:SUPPRESS_TOP_OUTLIERS]\nhigh_range= saleprice[saleprice[:,0].argsort()][-SUPPRESS_TOP_OUTLIERS:]\nprint('outer range (low) of the distribution:\\n {}'.format(low_range))\nprint('\\nouter range (high) of the distribution:\\n {}'.format(high_range))","93d71f82":"# Suppress top outliers\nif SUPPRESS_TOP_OUTLIERS > 0:\n    print(\"Suppressing Outliers...\")\n    dataset=dataset.loc[~dataset['price'].isin(list(high_range))]\n    dataset=dataset.loc[~dataset['price'].isin(list(low_range))]\n    print(\"New Shape : {} \".format(dataset.shape))\n    print(\"Done\")","db5c6347":"# Filtering on different criteria\n# Surface\nif len(FILTER_SQFT_LIVING) > 0:\n    print(\"Filtering by Sqft min {}, max {} ..\".format(FILTER_SQFT_LIVING['min'], FILTER_SQFT_LIVING['max']))\n    filtered_sqft = dataset.loc[(dataset['sqft_living'] > FILTER_SQFT_LIVING['min']) & \n                                (dataset['sqft_living'] < FILTER_SQFT_LIVING['max'])]\n    ratio = len(filtered_sqft) \/ len(dataset)\n    print(\"New Shape : {} , Ratio of Selected Sqft {}\".format(filtered_sqft.shape, ratio))\n    dataset = filtered_sqft.copy()\n\n# Zipcode with deviance\nif FILTER_ZIPCODE_EXCLUDE_TOP_DEVIANCE > 0:\n    print(\"Filtering by Zipcode Deviance : Top {}\".format(FILTER_ZIPCODE_EXCLUDE_TOP_DEVIANCE))\n    ds = dataset.groupby(['zipcode']).std()\n    zipcode_exclude_list = ds['price'].astype(int).sort_values(ascending=False).nlargest(FILTER_ZIPCODE_EXCLUDE_TOP_DEVIANCE)\n    zipcode_exclude_list = zipcode_exclude_list.index.tolist()\n    dataset = dataset.loc[(~dataset.zipcode.isin(list(zipcode_exclude_list)))]\n    print(\"New Shape : {} \".format(dataset.shape))\n\n# Grade of House\nif len(FILTER_GRADE) > 0:\n    print(\"Filtering by Grade : min {} , max {}\".format(FILTER_GRADE['min'], FILTER_GRADE['max']))\n    \n    filtered_dataset = dataset[(dataset.grade < FILTER_GRADE['max']) & \n                               (dataset.grade > FILTER_GRADE['min'])]\n        \n    print(\"Filtering Grade : We exclude {} rows of data\".format(len(dataset) - len(filtered_dataset)))    \n    print(\"New Shape : {} \".format(filtered_dataset.shape))    \n    dataset = filtered_dataset.copy()\n\n# Built year\nif len(FILTER_YEAR_BUILT) > 0:\n    print(\"Filtering by Year built : min {} , max {}\".format(FILTER_YEAR_BUILT['min'], FILTER_YEAR_BUILT['max']))\n\n    filtered_dataset = dataset[(dataset.yr_built < FILTER_YEAR_BUILT['max']) & \n                               (dataset.yr_built > FILTER_YEAR_BUILT['min'])]    \n    \n    print(\"Filtering Yr_built : We exclude {} rows of data\".format(len(dataset) - len(filtered_dataset)))\n    print(\"New Shape : {} \".format(filtered_dataset.shape))        \n    dataset = filtered_dataset.copy()\n\n# House Price\nif len(FILTER_PRICE) > 0:\n\n    print(\"Filtering by Price : min {} , max {}\".format(FILTER_PRICE['min'], FILTER_PRICE['max']))\n    filtered_dataset = dataset[(dataset.price < FILTER_PRICE['max']) & \n                               (dataset.price > FILTER_PRICE['min'])]\n    print(\"Filtering Price : We exclude {} rows of data\".format(len(dataset) - len(filtered_dataset)))\n    print(\"New Shape : {} \".format(filtered_dataset.shape)) \n    dataset = filtered_dataset.copy()      ","a40d8a03":"# Plot Infos by ZipCode\ndataset.astype('int').groupby(['zipcode'])['price'].describe().plot()","cc14cb8f":"zip_mean_max = dataset.astype('int').groupby(['zipcode'])['price'].mean().idxmax()         # zipcode with the max of mean price\nzip_mean = dataset.loc[(dataset['zipcode'] == zip_mean_max)]['price'].mean()               # mean price\nprint(\"Zipcode with maximum mean is {} with value: {}\".format(zip_mean_max, int(zip_mean)))","b5edc4af":"# Finding the most expensive zipcode regarding price\/m\u00b2\nmoy = dataset.groupby(['zipcode']).mean()\nmoy['sqm2_living'] = moy['sqft_living'] \/ 3.28\nmoy['m2'] = moy['price'] \/ moy['sqm2_living']\nmoy[['price','sqft_living', 'sqm2_living', 'm2']].nlargest(5, columns='m2')","cf2e1fdc":"ax = sns.jointplot(dataset['long'], dataset['lat'], kind='scatter', \n                   joint_kws={'alpha':0.03}, xlim=(-122.6, -121.6), ylim=(47.2,47.8), height=7)","42373ebd":"fig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nxs = dataset['lat']\nys = dataset['long']\nzs = dataset['price']\nax.scatter(xs, ys, zs, s=50, alpha=0.1, edgecolors='w')\nax.set_xlabel('Lattitude')\nax.set_ylabel('Longitude')\nax.set_zlabel('Price')\nax.set(zlim=(FILTER_PRICE['min'], 2500000))\nplt.show()","911b63eb":"# Preparation for Scaling\nscaler = preprocessing.StandardScaler()                                               # Init Data scaler\nnumpy_scaled = scaler.fit_transform(dataset.astype(\"float64\"))                        # return a numpy array\ndf_scaled = pd.DataFrame.from_records(data=numpy_scaled, columns=dataset.columns)     # get a dataframe\ndf_scaled.head()                                                                      # show some lines\n\nif USE_SCALED_DATA:\n    print(\"Using Scaled Data\")\n    print(\"Shape : {} \".format(df_scaled.shape))\n    dataset = df_scaled.copy()\n    \ndataset.isnull().sum()","4063a068":"# Dataset X, y and splitting test \/ train\ny = dataset.price   \nX = dataset.loc[:, ~dataset.columns.isin(['price', 'y_trans'])]\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=RATIO_TRAIN_TEST, \n                                                                        random_state=5, shuffle=True) ","b31f0670":"# Gradient Booster & Features Importance\nparams = {'n_estimators': 650, 'max_depth': 6, 'min_samples_split': 2, 'learning_rate': 0.01, 'loss': 'ls'}\nclf = ensemble.GradientBoostingRegressor(**params)\ntrain_and_evaluate(clf, X_train, X_test, y_train, y_test)","3a27287d":"test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\nfeatureslist = np.array(X.columns.tolist())\n\nfor i, y_pred in enumerate(clf.staged_predict(X_test)):\n    test_score[i] = clf.loss_(y_test, y_pred)\n\n# Plotting Deviance\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title('Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-', label='Training Set Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-', label='Test Set Deviance')\nplt.legend(loc='upper right')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Deviance')\n\n# Feature importance\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, featureslist[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","0a70af9e":"# Gradient Booster with regularised y distribution\nparams = {'n_estimators': 700, 'max_depth': 7, 'min_samples_split': 2, 'learning_rate': 0.01, 'loss': 'ls'}\ngradboost = TransformedTargetRegressor(\n    regressor=ensemble.GradientBoostingRegressor(**params),\n    transformer=QuantileTransformer(output_distribution='normal'))\ntrain_and_evaluate(gradboost, X_train, X_test, y_train, y_test)","9d95f350":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV","4c4373b8":"# Regressor parameters\nxg_reg = xgb.XGBRegressor(booster=\"gbtree\", objective=\"reg:linear\",colsample_bytree=0.8, min_child_weight=1, \n          learning_rate=0.1, max_depth=6, alpha=2, n_estimators=350, subsample=1, reg_alpha=1e-05)\ntrain_and_evaluate(xg_reg, X_train, X_test, y_train, y_test)","e92d9fd8":"if XGBOOST_PERFORM_GRID:\n\n    # Grid Search for optimizing\n    param_test1 = {\n     'max_depth': [5,6,7],\n     'n_estimators': [250,350,450],\n     'min_child_weight': [20],\n     'reg_alpha':[1e-5],\n     'colsample_bytree': [0.7,0.8]\n    }\n\n    gsearch1 = GridSearchCV(estimator = xgb.XGBRegressor(\n                            learning_rate =0.1, alpha=1, objective='reg:linear'), \n               param_grid = param_test1, scoring='r2', cv=3, verbose=3)\n\n    gsearch1.fit(X_train, y_train)\n    gsearch1.best_params_, gsearch1.best_score_","67e2e08e":"# Cross Validation of the defined Model\nif XGBOOST_CROSS_VALIDATION: \n    \n    params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.8, 'min_child_weight': 20, 'reg_alpha': 1e-05,\n              'learning_rate': 0.1, 'max_depth': 6, 'alpha': 2, 'n_estimators': 350}\n\n    data_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n\n    cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=5,\n                        num_boost_round=100,early_stopping_rounds=10,metrics=\"rmse\", \n                        as_pandas=True, seed=123)\n\n    print((cv_results[\"test-rmse-mean\"]).tail(1))","41888a4e":"**XgBoost Regressor**","b6dc9a89":"**Dataset Exploration**","857ddc8f":"**Plotting Lattitude and Longitude**","ab3bcc86":"**Configuration for Filtering & Transformation**","299d03d6":"**House Price Predictions with scikit and xgboost**","7672e2ca":"**Transforming Data : Outliers \/ Filters \/ Scaling**","b3cdf8ae":"**Linear Regressions**","89b97dd8":"**Common functions for Plotting and stats about the regressors performance**"}}