{"cell_type":{"8a615dd4":"code","c570bd61":"code","967c1290":"code","0b4d2ced":"code","ce01bdb2":"code","4a4a1540":"code","20213848":"code","41858034":"code","e416fabe":"code","0a74d653":"code","64c5da24":"code","9c2dcfff":"code","bf4ab4d3":"code","1d27e40a":"code","9f466abd":"code","dbc2270e":"code","7fc0e37f":"code","b1d402a6":"code","c2b1f4f5":"code","4874ec9a":"code","f9bd64a3":"code","b50cc0f1":"code","0174b9af":"code","22c88f54":"code","7c7385dd":"code","2facfc76":"code","19ed1fa0":"code","e1e0bf12":"code","637ec079":"code","ead479b9":"code","b0faf3f2":"code","86a30c46":"code","4002de72":"code","c83651e3":"code","1302f6a4":"code","f4719dd1":"code","98b9f0b0":"code","88b862b3":"code","5fc17d7d":"code","0b8481a8":"code","6cc43569":"code","359e00de":"code","f769ad2e":"code","0e568337":"code","66f5c584":"code","564e6452":"code","4bfc1d9c":"code","9e1fa3a4":"code","b2dd7067":"code","46b9db19":"code","260e80c1":"code","082acecd":"code","a4788564":"code","9d02f27e":"code","45bf5869":"code","812e4116":"code","5c2f05e0":"code","6bd2926c":"code","1ac50f85":"code","7d1a5c2c":"code","3e6b9d7a":"code","92f421b9":"code","3be58802":"code","c58b322d":"code","d1756a01":"code","d240374a":"code","ad33a28f":"code","9b8b9e0c":"code","c0639634":"code","05d5e54a":"code","c1562ad8":"code","d7760a1c":"code","1ca28a69":"code","3b651e2e":"code","85a2706c":"code","8a15fcb3":"code","6045e41a":"code","2f636f34":"code","8a99e126":"code","9b7806a1":"code","c2d79e96":"code","59b82e7a":"code","fbc78fe4":"code","9b268165":"code","156b28c9":"code","15ee623c":"code","f6542eea":"code","38b28277":"code","b8b709f3":"code","c4ae2769":"code","395c0a1a":"code","ce539ab6":"code","77a9159b":"code","78c3e9a2":"markdown","8ed13925":"markdown","66bb470e":"markdown","a9760988":"markdown","80f03fb0":"markdown","81eb110f":"markdown","7888f314":"markdown","4966377f":"markdown","9e3111bb":"markdown","08952ffa":"markdown","f8c2db0f":"markdown","ed7a207d":"markdown","163f6c1f":"markdown","8d946eae":"markdown","06e532a4":"markdown","96e962da":"markdown","147e2df1":"markdown","54c2b923":"markdown","64b0de80":"markdown","19279d25":"markdown","ae676fbe":"markdown","3a7a557d":"markdown","ed86d301":"markdown","21e7b848":"markdown","5326323b":"markdown","dc18e765":"markdown","45b41837":"markdown","72ac243f":"markdown","11ed0533":"markdown","c77fbb30":"markdown","945d897d":"markdown","0b8d685e":"markdown","bc87da19":"markdown","95ceabed":"markdown","d2f14445":"markdown","6dfef367":"markdown","d628ef9e":"markdown","ca43f194":"markdown","2e45433f":"markdown","7affe896":"markdown","ea20b333":"markdown","a63e805d":"markdown","79a1b95e":"markdown","adf8944d":"markdown"},"source":{"8a615dd4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c570bd61":"ttrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nttrain.head()","967c1290":"ttest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nttest.head()","0b4d2ced":"print(\"Shape of Training set\")\nprint(ttrain.shape)\nprint(\"\\nShape of Testing set\")\nprint(ttest.shape)","ce01bdb2":"print(\"Information of training set\\n\")\nprint(ttrain.info())\nprint(\"\\nInformation of testing set\\n\")\nprint(ttest.info())","4a4a1540":"print(\"Missing value in training set\")\ntrain = round(((ttrain.isnull().sum()*100)\/(ttrain.shape[0])),3).sort_values(ascending=False).head(5)\nprint(train)\n\nprint(\"\\nMissing value in test set\")\ntest = round(((ttest.isnull().sum()*100)\/(ttrain.shape[0])),3).sort_values(ascending=False).head(5)\nprint(test)","20213848":"#Age\ntrain_mean = round(ttrain['Age'].mean(),0) \nttrain['Age'] = ttrain['Age'].fillna(train_mean)\n\n#Cabin\nttrain.drop(['Cabin'],inplace=True,axis=1)\nttrain.head()","41858034":"#Embarked\nprint(ttrain['Embarked'].value_counts())\n\nvalue = ttrain['Embarked'].value_counts().index[0]\n\n#Since the frequency of 'S' is the highest we substitute the missing values with 'S'\nttrain['Embarked'] = ttrain['Embarked'].fillna(value) ","e416fabe":"#Age\ntest_mean = round(ttest['Age'].mean(),0) \nttest['Age'] = ttest['Age'].fillna(test_mean)\n\n#Cabin\nttest.drop(['Cabin'],inplace=True,axis=1)\nttest.head()","0a74d653":"#Fare\nprint(ttest['Fare'].mode())\ntest = ttest['Fare'].mode()[0]\n\n#Since the frequency of '7.7500' is the highest we substitute the missing values with '7.7500'\nttest['Fare'] = ttest['Fare'].fillna(test) ","64c5da24":"print(\"Missing value in training set\")\nprint(ttrain.isnull().sum())\nprint(\"\\nMissing value in test set\")\nprint(ttest.isnull().sum())","9c2dcfff":"ttrain.head()","bf4ab4d3":"print(ttrain.groupby(['Survived']).count()['PassengerId'])\nsns.countplot(x='Survived',data=ttrain)","1d27e40a":"sns.countplot(x='Sex',data=ttrain)","9f466abd":"print(ttrain.groupby(['Pclass']).mean()['Survived'],\"\\n\")\nsns.barplot(x='Pclass',y='Survived',data=ttrain)","dbc2270e":"print(ttrain.groupby(['Pclass','Survived']).count()['PassengerId'],\"\\n\")\nsns.countplot(x='Pclass',hue='Survived',data=ttrain)","7fc0e37f":"print(ttrain.groupby(['Pclass','Sex']).count()['PassengerId'],\"\\n\")\nsns.countplot(x='Pclass',hue='Sex',data=ttrain)","b1d402a6":"print(ttrain.groupby(['Sex','Survived']).count()['PassengerId'],\"\\n\")\nsns.countplot(x='Sex',hue='Survived',data=ttrain)","c2b1f4f5":"print(ttrain.groupby(['Pclass','Sex']).mean()['Survived'],\"\\n\")\nsns.barplot(x='Pclass',y='Survived',hue='Sex',data=ttrain)","4874ec9a":"print(ttrain.groupby(['SibSp','Survived']).count()['PassengerId'],\"\\n\")\nsns.countplot(x='SibSp',hue='Survived',data=ttrain)","f9bd64a3":"sns.stripplot(x='Survived',y='Age',data=ttrain,jitter=True)","b50cc0f1":"tfare = sns.FacetGrid(data=ttrain,hue='Survived')\ntfare.map(sns.kdeplot,'Fare')","0174b9af":"sns.jointplot(x='Age',y='Fare',data=ttrain,kind='reg')","22c88f54":"sns.countplot(x='Embarked',hue='Survived',data=ttrain)","7c7385dd":"sns.countplot(x='Parch',hue='Survived',data=ttrain)","2facfc76":"target = ttrain.groupby(['Embarked','Pclass','Survived'])\n\nplt.figure(figsize=(8,8))\ntarget.count()['PassengerId'].plot.bar()\nplt.xticks(rotation=50)\nplt.xlabel(\"Grouped Conditions\")\nplt.ylabel(\"Total Count\")\n\nplt.show()","19ed1fa0":"correlate = ttrain.corr()\ncorrelate","e1e0bf12":"sns.heatmap(correlate)","637ec079":"print(\"Columns of training set\")\nprint(ttrain.columns)\nprint(\"\\nColumns of testing set\")\nprint(ttest.columns)","ead479b9":"data = [ttrain, ttest]\nfor dataset in data:\n    dataset.drop(['PassengerId','Ticket'],axis=1,inplace=True)","b0faf3f2":"for dataset in data:\n    dataset['Relation'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['Relation'] > 0, 'Travelled_alone'] = 'No'\n    dataset.loc[dataset['Relation'] == 0, 'Travelled_alone'] = 'Yes'","86a30c46":"print(\"Information of training set\\n\")\nprint(ttrain.info())\nprint(\"\\nInformation of testing set\\n\")\nprint(ttest.info())","4002de72":"for dataset in data:\n    dataset['Travelled_alone'] = dataset['Travelled_alone'].map({'No':0,'Yes':1})\n    dataset['Sex'] = dataset['Sex'].map({'male':0,'female':1})\n    dataset['Embarked'] = dataset['Embarked'].map({'S':0,'C':1,'Q':2})","c83651e3":"def age(num): \n    \n    if num <= 11: \n        return 0\n  \n    elif num > 11 and num <= 18:\n        return 1\n    \n    elif num > 18 and num <= 22:\n        return 2\n    \n    elif num > 22 and num <= 27:\n        return 3\n    \n    elif num > 27 and num <= 33:\n        return 4\n    \n    elif num > 33 and num <= 40:\n        return 5\n    \n    elif num > 40 and num <= 66:\n        return 6\n    \n    else: \n        return 7\n    \n    \ndef fare(num): \n    \n    if num <= 7.91: \n        return 0\n  \n    elif num > 7.91 and num <= 33:\n        return 1\n    \n    elif num > 33 and num <= 66:\n        return 2\n    \n    elif num > 66 and num <= 99:\n        return 3\n    \n    elif num > 99 and num <= 250:\n        return 4\n    \n    elif num > 250 and num <= 360:\n        return 5\n   \n    else: \n        return 6\n","1302f6a4":"for dataset in data:\n    dataset['Age'] = dataset['Age'].apply(age)\n    dataset['Fare'] = dataset['Fare'].apply(fare)","f4719dd1":"for dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Age'] = dataset['Age'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    dataset['Age'] = dataset['Age'].astype(int)","98b9f0b0":"print(\"Information of training set\\n\")\nprint(ttrain.info())\nprint(\"\\nInformation of testing set\\n\")\nprint(ttest.info())","88b862b3":"titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    \n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    \n    dataset['Title'] = dataset['Title'].map(titles)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    dataset['Title'] = dataset['Title'].astype(int)","5fc17d7d":"for dataset in data:\n    dataset.drop(['Name'],axis=1,inplace=True)","0b8481a8":"print(\"Information of training set\\n\")\nprint(ttrain.info())\nprint(\"\\nInformation of testing set\\n\")\nprint(ttest.info())","6cc43569":"from sklearn.model_selection import train_test_split \nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix","359e00de":"X = ttrain.drop('Survived',axis=1)\ny = ttrain['Survived']","f769ad2e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","0e568337":"#Import Packages \nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV","66f5c584":"#Object creation and fitting of training set\nmodel1 = LogisticRegression()\nmodel1.fit(X_train,y_train)","564e6452":"#Creation of a prediction variable\npredict1 = model1.predict(X_test)","4bfc1d9c":"#Accuracy Matrix\nprint(\"\\nClassification Matrix\")\nprint(classification_report(y_test,predict1))\nprint(\"\\nConfusion Matrix\")\nprint(confusion_matrix(y_test,predict1))","9e1fa3a4":"#Accuracy Percentage\npredict11 = round((model1.score(X_test, y_test)*100),0)\nprint(\"Precision of Logistic Regression is: \",predict11,\"%\") ","b2dd7067":"#Import Packages \nfrom sklearn.neighbors import KNeighborsClassifier","46b9db19":"#Object creation and fitting of training set\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train,y_train)","260e80c1":"#Creation of a prediction variable\npredictionsknn = knn.predict(X_test)","082acecd":"#Accuracy Matrix\nprint(\"\\nClassification Matrix\")\nprint(classification_report(y_test,predictionsknn))\nprint(\"\\nConfusion Matrix\")\nprint(confusion_matrix(y_test,predictionsknn))","a4788564":"#Accuracy Percentage\nknnp = round((knn.score(X_test, y_test)*100),0)\nprint(\"Precision of K Nearest Neighbors is: \",knnp,\"%\") ","9d02f27e":"#Import Packages \nfrom sklearn.tree import DecisionTreeClassifier","45bf5869":"#Object creation and fitting of training set\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)","812e4116":"#Creation of a prediction variable\npredictiondt = dtree.predict(X_test)","5c2f05e0":"#Accuracy Matrix\nprint(\"\\nClassification Matrix\")\nprint(classification_report(y_test,predictiondt))\nprint(\"\\nConfusion Matrix\")\nprint(confusion_matrix(y_test,predictiondt))","6bd2926c":"#Accuracy Percentage\ndtt = round((dtree.score(X_test, y_test)*100),0)\nprint(\"Precision of Decision Tree is: \",dtt,\"%\") ","1ac50f85":"#Import Packages \nfrom sklearn.ensemble import RandomForestClassifier","7d1a5c2c":"#Object creation and fitting of training set\nrandfc = RandomForestClassifier(n_estimators=100)\nrandfc.fit(X_train,y_train)","3e6b9d7a":"#Creation of a prediction variable\npredictionrf = randfc.predict(X_test)","92f421b9":"#Accuracy Matrix\nprint(\"\\nClassification Matrix\")\nprint(classification_report(y_test,predictionrf))\nprint(\"\\nConfusion Matrix\")\nprint(confusion_matrix(y_test,predictionrf))","3be58802":"#Accuracy Percentage\nrandom = round((randfc.score(X_test, y_test)*100),0)\nprint(\"Precision of Random Forest is: \",random,\"%\") ","c58b322d":"#Import Packages \nfrom sklearn.svm import SVC","d1756a01":"#Object creation and fitting of training set\nsvcm = SVC()\nsvcm.fit(X_train,y_train)","d240374a":"#Creation of a prediction variable\npredictionsvc = svcm.predict(X_test)","ad33a28f":"#Accuracy Matrix\nprint(\"\\nClassification Matrix\")\nprint(classification_report(y_test,predictionsvc))\nprint(\"\\nConfusion Matrix\")\nprint(confusion_matrix(y_test,predictionsvc))","9b8b9e0c":"#Accuracy Percentage\nsvc = round((svcm.score(X_test, y_test)*100),0)\nprint(\"Precision of Support Vector Classifier: \",svc,\"%\")\n","c0639634":"#Import Packages \nfrom sklearn.naive_bayes import GaussianNB","05d5e54a":"#Object creation and fitting of training set\ngaus = GaussianNB()\ngaus.fit(X_train,y_train)","c1562ad8":"#Creation of a prediction variable\npredictiongus = gaus.predict(X_test)","d7760a1c":"#Accuracy Matrix\nprint(\"\\nClassification Matrix\")\nprint(classification_report(y_test,predictiongus))\nprint(\"\\nConfusion Matrix\")\nprint(confusion_matrix(y_test,predictiongus))","1ca28a69":"#Accuracy Percentage\nrig = round((gaus.score(X_test, y_test)*100),0)\nprint(\"Precision of Gaussian Naive Bayes is: \",rig,\"%\") ","3b651e2e":"#Object creation and fitting of training set\nlgrcv = LogisticRegressionCV()\nlgrcv.fit(X_train,y_train)","85a2706c":"#Creation of a prediction variable\npredictionlgcv = lgrcv.predict(X_test)","8a15fcb3":"#Accuracy Matrix\nprint(\"\\nClassification Matrix\")\nprint(classification_report(y_test,predictionlgcv))\nprint(\"\\nConfusion Matrix\")\nprint(confusion_matrix(y_test,predictionlgcv))","6045e41a":"#Accuracy Percentage\nlgcv = round((lgrcv.score(X_test, y_test)*100),0)\nprint(\"Precision of  Logistic Regression CV is: \",lgcv,\"%\") ","2f636f34":"#Import Packages \nfrom sklearn.ensemble import GradientBoostingClassifier","8a99e126":"#Object creation and fitting of training set\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train,y_train)","9b7806a1":"#Creation of a prediction variable\npredictiongbc = gbc.predict(X_test)","c2d79e96":"#Accuracy Matrix\nprint(\"\\nClassification Matrix\")\nprint(classification_report(y_test,predictiongbc))\nprint(\"\\nConfusion Matrix\")\nprint(confusion_matrix(y_test,predictiongbc))","59b82e7a":"#Accuracy Percentage\ngbcp = round((gbc.score(X_test, y_test)*100),0)\nprint(\"Precision of Gradient Boosting Classifier is: \",gbcp,\"%\") ","fbc78fe4":"#Import Packages \nfrom sklearn.linear_model import Perceptron","9b268165":"#Object creation and fitting of training set\nper = Perceptron(max_iter=6)\nper.fit(X_train,y_train)","156b28c9":"#Creation of a prediction variable\npredictionper = per.predict(X_test)","15ee623c":"#Accuracy Matrix\nprint(\"\\nClassification Matrix\")\nprint(classification_report(y_test,predictionper))\nprint(\"\\nConfusion Matrix\")\nprint(confusion_matrix(y_test,predictionper))","f6542eea":"#Accuracy Percentage\nperr = round((per.score(X_test, y_test)*100),0)\nprint(\"Precision of Perceptron is: \",perr,\"%\") ","38b28277":"#Import Packages \nfrom sklearn.linear_model import SGDClassifier","b8b709f3":"#Object creation and fitting of training set\nmodel2 = SGDClassifier()\nmodel2.fit(X_train,y_train)","c4ae2769":"#Creation of a prediction variable\npredict2 = model2.predict(X_test)","395c0a1a":"#Accuracy Matrix\nprint(\"\\nClassification Matrix\")\nprint(classification_report(y_test,predict2))\nprint(\"\\nConfusion Matrix\")\nprint(confusion_matrix(y_test,predict2))","ce539ab6":"linr = round((model2.score(X_test, y_test)*100),0)\nprint(\"Precision of Stochastic Gradient Descent is: \",linr,\"%\") ","77a9159b":"results = pd.DataFrame({'Model': ['Logistic Regression', 'KNN', 'Decision Tree', 'Random Forest', 'Support Vector Machines',  \n                                  'Gausian Naive Baye', 'Logistic Regression CV', 'Stochastic Gradient Decent', 'Perceptron','Stochastic Gradient Descent'],\n                        'Score': [predict11, knnp, dtt, random, svc, rig, lgcv, gbcp, perr,linr]\n                      })\n\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(10)","78c3e9a2":"# **8. Gradient Boosting Classifier**","8ed13925":"# 6. Gaussian Naive Bayes","66bb470e":"It is observed that there are more survivors from passanger class 1.","a9760988":"Passengers between the age of 0-65 have survived with an exception of 1 passenger of age 80.","80f03fb0":"Passengers from the 3rd passanger class were not able to survive.","81eb110f":"# **9. Perceptron**","7888f314":"It is observed that there are many male passangers and the maximum number of passangers belonged to the 3rd passanger class.","4966377f":"# **Data Cleaning**","9e3111bb":"A large number of passengers who had been travelling alone were not able to survive.","08952ffa":"# **Data Loading**","f8c2db0f":"# **Conclusion**","ed7a207d":"The relationship between each and every feature can be represented by identifying the postive and negative correalation between them and then visualizing them into a heatmap ","163f6c1f":"Before preparing any model it is essential to process the dataset and create a dataset with numeric values.","8d946eae":"Loading the datasets ie: train.csv and test.csv ","06e532a4":"The survivors include those who have paid a higher conveyance fare.","96e962da":"All the missing values within the datasets have been dealt with and the datasets are clean. Thus we now perform Exploratory Data Analysis (EDA).","147e2df1":"The training set consists of 891 values and testing set consists of 418. \nFrom the above output we know that values from 'Age and Cabin' are missing from the ttrain as well as ttest dataset.\nAlong with 'Age and Cabin' the values from features 'Embarked' and 'Fare' are missing from the ttrain and ttest dataset respectively.\nThe output below shows the percentage of missing values from each of the dataset.","54c2b923":"We observe the with the precision of the Decision Tree algorithm is the highest.Thus we can use Decision Tree for the future analysis of our dataset.   ","64b0de80":"There is no relation between Age and Fare paid for conveyance.\nTitanic has many passengers between the age 20-40.\nMost of the tickets purchased costs less comparatively. ","19279d25":"The majority survivers include those who board the titanic from Southampton (S) and belonged to the 1-2 class. Unfortunately the passengers from the 3 class who board from the same port did not surive","ae676fbe":"# 5. Support Vector Machine","3a7a557d":"# **Summary of Observation**","ed86d301":"Passangers that had board the ship from Southampton are unlikely to survive.","21e7b848":"# 3. Decision Tree","5326323b":"# **7. Logistic Regression CV**","dc18e765":"*  Less than half of the total number of passenger had survived the titanic accident.\n*  The number of male passengers were more than the female passengers.\n*  Maximum number of passengers traveled in the 3rd passenger class as the fare of the same was economically feasible.\n*  Many passengers were travelling alone.\n*  There were 3 ports from where the passenger could board.\n*  All the passengers travelling were below the age of 75. Although there was a single exception of a passenger of age 80.\n*  The maximum cost of the ticket for the titanic was above 500$.\n*  The survivors majorly included:\n>     1. Female passangers.\n>     2. Passangers from the 1st passenger class.\n>     3. Passanger with sibling, spouse, parent or children. ","45b41837":"For the purpose of Data Modeling we need to split our data into training and test set.\nOnce the split is done we can put our data into various models and check each the precision of each model.\nWe select the model with the highest precision score.","72ac243f":"# 2. K-Nearest Neighbour","11ed0533":"Passengers ID and Ticket are unique values for each individual, we can not use them to build a model or predict any future outcomes. Thus it is desirable to drop these columns from the training and the testing set.","c77fbb30":"# 10. Stochastic Gradient Descent (SGD)","945d897d":"# **Data Modeling**","0b8d685e":"We notice that sibling, spouse, parents and children are relations for a passenger, therefore we can club them into 1 single feature. ","bc87da19":"The number of survivors are very few.","95ceabed":"# **Feature Engineering**","d2f14445":"Data cleaning has to be done by identifying number of missing values within the datasets.","6dfef367":"The above plot shows that many survivors are females among Pclass 1 and 2 .","d628ef9e":"The testing set consists of 418.\nThe missing values of the features from the testing set are handled as follows:\n* Age = Since the percentage of missing values is low, the mean of the column is substitued with the missings values\n* Cabin = Since the percentage of missing values is very high the column is dropped\n* Fare = Since the percentage of missing values is very low, the higgest frequency value is substitued with the missings values","ca43f194":"The training and testing set includes object and floating values. It is a good practice to always feed in numerical (Integer) values into a predictive data model. Hence we know convert the two objects into numerical value. We also notice that we do not have any categorical value for the features Age and Fare, thus we seprate them into categories such that they can be easily used as an input to a model. ","2e45433f":"# 1. Logistic Regression","7affe896":"The training set consists of 891 values.\nThe missing values of the features from the training set are handelled as follows:\n* Age = Since the percentage of missing values is less, the mean of the column is substitued with the missings values\n* Cabin = Since the percentage of missing values is very high the column is dropped\n* Embarked = Since the percentage of missing values is very low, the higgest frequency value is substitued with the missings values","ea20b333":"# 4. Random Forest","a63e805d":"# **Exploratory Data Analysis (Data Visualization)**","79a1b95e":"Parch can not help us to determine any pattern of survivors.","adf8944d":"There are a large number of female passangers who were able to survive."}}