{"cell_type":{"e943b049":"code","7b0c3511":"code","b2175dae":"code","2bec61c3":"code","52fbe3bc":"code","d83ba8a8":"code","9f0f856b":"code","91d88399":"code","33ca5e4e":"code","7f600edf":"code","06bcd81d":"code","9f64245b":"code","51a7dac4":"code","0a8aea4f":"code","79e1c163":"code","015bcf45":"code","7253a885":"code","259545a5":"code","ca4d3095":"code","b2ed8e67":"code","7667f2e5":"code","7d4a85d0":"code","2c6ca436":"code","a0d645d3":"code","7ca12960":"code","e936366e":"code","cf89b5fb":"code","112dc975":"code","d17fe241":"code","d470e2de":"markdown","61a6e1ba":"markdown","5170829a":"markdown","5cdb073e":"markdown","f0790908":"markdown","53d40983":"markdown","65209f35":"markdown","0dcc3a47":"markdown","c39a56f0":"markdown","4763ed1f":"markdown","2f80e633":"markdown"},"source":{"e943b049":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore')","7b0c3511":"data = pd.read_csv('..\/input\/entity-annotated-corpus\/ner_dataset.csv', encoding = 'latin1')\ndata.fillna(method = 'ffill', inplace = True)\ndata.head().append(data.tail())","b2175dae":"data.shape","2bec61c3":"data.drop(['POS'], axis = 1, inplace = True)","52fbe3bc":"print(f'No.of Unique Words:{data.Word.nunique()}')\nprint(f'No.of Unique Tags:{data.Tag.nunique()}')","d83ba8a8":"plt.figure(figsize = (10, 5))\nplt.style.use('seaborn')\nplt.hist(data.Tag, log = True, bins = 40, label = 'Tags', color = 'red')\nplt.title('UNIQUE TAGS', fontsize = 13)\nplt.xlabel('TAGS', fontsize = 13)\nplt.ylabel('COUNT', fontsize = 13)\nplt.legend(fontsize = 13)\nplt.show()","9f0f856b":"words = list(set(data['Word'].values))\nnum_words = len(words)\ntags = list(set(data['Tag'].values))\nn_tags = len(tags)\nprint(num_words, n_tags)","91d88399":"class Grouping:\n    \n    def __init__(self, data):\n        self.data = data\n        self.empty = False\n        agg_fun = lambda S : [(w, t) for w, t in zip(S['Word'].values.tolist(),\n                                                     S['Tag'].values.tolist())]\n        self.group = self.data.groupby('Sentence #').apply(agg_fun)\n        self.sentence = [sen for sen in self.group]","33ca5e4e":"getter = Grouping(data)\nsentences = getter.sentence","7f600edf":"sentences[0]","06bcd81d":"word2idx = {w : i + 1 for i, w in enumerate(words)}\ntag2idx = {t : i for i, t in enumerate(tags)}","9f64245b":"word2idx","51a7dac4":"tag2idx","0a8aea4f":"plt.figure(figsize = (10, 5))\nplt.hist([len(s) for s in sentences], color = 'red', label = 'max_len', bins = 50)\nplt.title('MAX LENGTH OF SENTENCE',fontsize = 15)\nplt.xlabel('LENGTH', fontsize = 15)\nplt.legend(fontsize = 15)\nplt.show()","79e1c163":"from tensorflow import keras\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","015bcf45":"max_len = 60\nX = [[word2idx[w[0]] for w in s] for s in sentences]\nX = pad_sequences(X, maxlen = max_len, padding = 'post', value = num_words - 1)\ny = [[tag2idx[t[1]] for t in s] for s in sentences]\ny = pad_sequences(y, maxlen = max_len, padding = 'post', value = tag2idx['O'])\ny = [to_categorical(i, num_classes = n_tags) for i in y]","7253a885":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 44)","259545a5":"X_train.shape, X_test.shape","ca4d3095":"print(len(y_train), len(y_test))","b2ed8e67":"input_dim = num_words + 1 #model.fit() iterates through the input with the range(0:35178). Thus we are adding 1 to fullfil the range(0:35179).\noutput_dim = max_len\ninput_length = max_len","7667f2e5":"input_dim, output_dim, input_length","7d4a85d0":"from tensorflow.keras import Model, Sequential, Input\nfrom tensorflow.keras.layers import Embedding, Dense, LSTM\nfrom tensorflow.keras.layers import Bidirectional, TimeDistributed, SpatialDropout1D","2c6ca436":"model = Sequential()\n\nmodel.add(Embedding(input_dim = input_dim, output_dim = output_dim, input_length = input_length))\n\nmodel.add(SpatialDropout1D(0.1))\n\nmodel.add(Bidirectional(LSTM(units = 100, recurrent_dropout = 0.1, return_sequences = True)))\n\nmodel.add(TimeDistributed(Dense(units = n_tags, activation = 'softmax')))\n\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\nmodel.summary()","a0d645d3":"from tensorflow.keras.callbacks import EarlyStopping\n\ncallbacks = EarlyStopping(monitor = 'val_accuracy', patience = 2, verbose = 1, mode = 'max', restore_best_weights = True)\n\nhistory = model.fit(X_train, np.array(y_train), batch_size = 32, validation_split = 0.1, epochs = 20, callbacks = callbacks, verbose = 1)","7ca12960":"model.save('.\/NER_MODEL_KAGGLE.h5')","e936366e":"model.evaluate(X_test, np.array(y_test))","cf89b5fb":"plt.figure(figsize=[8,6])\nplt.plot(history.history['accuracy'],'r',linewidth=3.0)\nplt.plot(history.history['val_accuracy'],'g',linewidth=3.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves',fontsize=16)","112dc975":"plt.figure(figsize=[8,6])\nplt.plot(history.history['loss'],'r',linewidth=3.0)\nplt.plot(history.history['val_loss'],'g',linewidth=3.0)\nplt.legend(['Training loss', 'Validation Loss'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves',fontsize=16)","d17fe241":"i = np.random.randint(0,X_test.shape[0])\np = model.predict(np.array([X_test[i]]))\np = np.argmax(p, axis =-1)\ny_true = np.argmax(np.array(y_test), axis =-1)[i]\nprint(\"{:15}{:5}\\t {} \\n\".format(\"Word\",\"True\",\"Pred\"))\nprint(\"-\"*30)\nfor w,true,pred in zip(X_test[i],y_true,p[0]):\n    print(\"{:15}{}\\t{}\".format(words[w-1],tags[true],tags[pred]))","d470e2de":"### PADDING WORDS AND TAGS:","61a6e1ba":"# NAMED ENTITY RECOGNITION USING BI-LSTM","5170829a":"### SPLITTING THE DATA FOR TRAINING AND TESTING:","5cdb073e":"### GROUPING WORDS AND TAGS:","f0790908":"- HERE WE CAN SEE THAT THE MAX LENGTH REACHES 60","53d40983":"### IMPORTING DATA:","65209f35":"### IMPORTING LIBRARIES:","0dcc3a47":"### MAPPING WORDS AND TAGS TO INDEX:","c39a56f0":"### DEFINING BI-LSTM:","4763ed1f":"### TESTING THE NER MODEL:","2f80e633":"### ACCURACY AND LOSS OF THE MODEL:"}}