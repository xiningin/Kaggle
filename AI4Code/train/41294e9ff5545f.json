{"cell_type":{"d3cf42ad":"code","b53bbe8b":"code","3c9f5d02":"code","db72bc08":"code","70349dfd":"code","2a7840f4":"code","af9c412c":"code","d42fdd4a":"code","c59e2fbb":"code","0a218ae6":"code","acc44b06":"code","cb738b4b":"code","34835076":"code","01252784":"code","84b84c83":"code","b54d7680":"code","7dc9fcf5":"code","09d28c89":"code","451bebe0":"code","ddacedcd":"code","c7d95c6e":"code","f7246dfd":"code","c139bdae":"code","f8d1b349":"code","f13dd8b3":"code","c0ec8127":"code","291e8252":"code","2408f0c9":"code","afb4a4e9":"code","32409aa9":"code","8b67942a":"code","9363946d":"code","de8865e5":"code","844a3cc4":"code","1bf5ae63":"code","6ba5a52c":"code","9b5308bf":"code","7659e7ea":"code","67a998bf":"code","21c0d75d":"code","7d433264":"code","4a3f8861":"code","3cc64271":"code","b6d2f36e":"code","e732106a":"code","dd8e9a55":"code","a5ff907b":"code","63a85589":"code","86c8eba1":"code","c66b27cc":"code","fba11343":"code","ec880cfe":"code","fa05975b":"code","6cafc92e":"code","05dc48fd":"code","92d58d2e":"code","53aa6fc5":"code","f66763c0":"code","16f02056":"code","95de3a3e":"code","239b2d36":"code","c86f67ef":"code","03e96392":"code","042b0bbc":"code","f7f94bc2":"code","fbf2bc5d":"code","a78ac082":"code","c20006e7":"code","5cf1cda1":"code","d70aa81c":"code","0a1c31fb":"code","53ff953d":"code","7986e56d":"code","125f1b4b":"code","584df149":"code","0bf91b4a":"code","73309623":"code","4acd1a09":"code","bffbe43d":"code","be32a0e4":"code","89097660":"code","f21b526d":"code","b52773e4":"code","d5d1a1d8":"code","357c3896":"code","31a6fa87":"code","a6e795a6":"code","a3b9190d":"code","e6322054":"code","d32f5c7a":"code","d6f5ce8f":"code","66e255fb":"code","dcf21585":"code","d8a2a45e":"code","394d25d0":"code","b0c69f7b":"code","f0c89e6c":"code","a2b491ed":"code","ab536863":"code","82dfcc62":"code","fb9a4ff0":"code","63b02cd7":"code","e7a5c472":"code","a7679e33":"code","24b18262":"code","6d23a5fb":"code","135b9b0e":"code","565df3c6":"code","aa339164":"code","9e150610":"code","f76a6b97":"code","8941d788":"code","73aa3f79":"code","1a37e228":"code","d2f35a96":"code","096723f3":"code","c4986fb0":"code","537294a0":"code","d263cf14":"code","02be398d":"code","301af6a2":"code","be96ea3d":"code","a0ac6a3b":"code","c1128744":"code","5fae2e0f":"code","91eb298a":"code","b7fccf1e":"code","7b4682d9":"code","d55f2dee":"code","9fcb1b75":"code","1a42e3f7":"code","86d7d4cf":"code","a9c60de7":"code","318aef94":"code","e8e85246":"code","90e352bf":"code","ae57f509":"code","c0fc46da":"code","9a11a134":"code","70c0b0e4":"code","de441503":"code","9d78e43f":"code","09d25dae":"code","52e1ee2e":"code","d176d5f0":"code","f1ef7f09":"code","b3a9eae4":"code","b2ec597a":"code","1dc53d7d":"code","b15fd3b7":"code","224dc59a":"code","e158cbc6":"code","021773f4":"code","08263c44":"code","661861fd":"code","81d54086":"code","70107fa3":"code","3f24b690":"code","813748a0":"code","e9df20fa":"code","a2ec8ade":"code","ee498ab1":"code","740f755a":"code","feb33005":"code","e5e84d2a":"code","6bfb5c26":"code","25941c86":"code","1c8d812a":"code","95adfefe":"code","df714ca7":"code","4ac6d376":"code","58ddb81f":"code","0fef896f":"code","6bae4709":"code","c54e59ab":"code","35c3576b":"code","8ad1c810":"code","1af2e78b":"code","e4b610fa":"code","3fe63ee4":"code","1d0494c8":"code","997a8281":"code","eb259bb4":"code","53a6fa48":"code","839f16b0":"code","3d9afd03":"code","3cc1fe41":"code","924ab37a":"code","9e716e71":"code","0a642be1":"code","99183b76":"code","23466b3a":"code","0a917270":"code","2e56b68a":"code","867c64fa":"code","365b804e":"code","f7e0fc3e":"code","4807d90c":"code","4329a2cb":"code","4f57a043":"code","2419939d":"code","49013756":"code","84475c0d":"code","de5a4474":"code","d90c218f":"code","2307fb75":"code","d908160b":"code","6fde3521":"code","7a848044":"code","bd3435bb":"code","818f42fc":"code","4226df01":"code","485ef1dc":"code","88644a40":"code","05d0a8c0":"code","e7785ae5":"code","8f60528b":"code","635ae88e":"code","aa4a43e7":"code","220c493c":"code","8a07c455":"code","ff6f14cc":"code","320582d5":"code","afdd258a":"code","70549dff":"code","07eb452c":"code","98a7873a":"code","34e0090f":"markdown","5fc49dd2":"markdown","443ef9be":"markdown","532b0c47":"markdown","ea1d8828":"markdown","5bec5c47":"markdown","4beea348":"markdown","fd174aa2":"markdown","f7b58e00":"markdown","5dbb5bca":"markdown","05aad668":"markdown","65b52025":"markdown","fe4e3631":"markdown","3dcdfaa6":"markdown","55a1ab1f":"markdown","d9086d32":"markdown","822249c9":"markdown","f866ed76":"markdown","6c73b6c7":"markdown","99b994e6":"markdown","ab08b7fb":"markdown","122cec5d":"markdown","36ea3695":"markdown","3ae5ec27":"markdown","84797fda":"markdown","c40b8a52":"markdown","dea89fdd":"markdown","be20ad0c":"markdown","3f2cf07c":"markdown","83f10982":"markdown","58c8b708":"markdown","f37272d6":"markdown","7a05f21c":"markdown","bd9c4164":"markdown","f5f39066":"markdown","2c6fdc90":"markdown","592f2009":"markdown","a01b3a62":"markdown","cd57e930":"markdown","1ad8f8f9":"markdown","047842be":"markdown","4b7f6622":"markdown","cf9afdbd":"markdown","8d8185da":"markdown","0599af0e":"markdown","f4b01a07":"markdown","6c2dcc47":"markdown","0b4c2224":"markdown","3c3704d6":"markdown","01a66510":"markdown","fc89b169":"markdown","3148c659":"markdown","7ac87d46":"markdown","1088c491":"markdown","ed249538":"markdown","3ab8786c":"markdown","e9f6ecf9":"markdown","d8d28752":"markdown","41728b8a":"markdown","0948b6cb":"markdown","836c3f19":"markdown","bef9bf89":"markdown","4ea0fc6a":"markdown","6e18115b":"markdown","c4a0df3d":"markdown","1a4c0abf":"markdown","677b404e":"markdown","2f7e6c4a":"markdown","9936152c":"markdown","2a01969a":"markdown","9d3c3143":"markdown","ab9fc89d":"markdown","18795fab":"markdown","4b628ef7":"markdown","1f73a9b2":"markdown","7eef0d4a":"markdown","8da1b1b2":"markdown","01458c9c":"markdown","097590f8":"markdown","3d40282b":"markdown","2d10689f":"markdown","41182660":"markdown","f91e4e15":"markdown","61e0ce3a":"markdown","9b314e2a":"markdown","1509af14":"markdown","91bb20fd":"markdown","75fc9925":"markdown","4837723c":"markdown","cd79f3b4":"markdown","27737659":"markdown","66ebbdbd":"markdown","cd456b75":"markdown","2fe2845a":"markdown","95612e0a":"markdown","f560b23d":"markdown","97c02504":"markdown","676af5f3":"markdown","966434cc":"markdown","9d349a07":"markdown","c6895da5":"markdown","fdf2732a":"markdown","ca844eaa":"markdown","f93de938":"markdown","383071e8":"markdown","9ad81107":"markdown","e629eb53":"markdown","1c206bad":"markdown","08097ee2":"markdown","d2f02d66":"markdown","09f76810":"markdown","a8d8c413":"markdown","54b0a218":"markdown","9e78a23b":"markdown","7827f679":"markdown","668c7933":"markdown"},"source":{"d3cf42ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b53bbe8b":"import requests\nurl = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_confirmed_US.csv'\nr = requests.get(url, allow_redirects=True)\nopen('.\/time_series_covid19_confirmed_US.csv', 'wb').write(r.content)\n#\nurl = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_confirmed_global.csv'\nr = requests.get(url, allow_redirects=True)\nopen('.\/time_series_covid19_confirmed_global.csv', 'wb').write(r.content)\n#\nurl = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_deaths_US.csv'\nr = requests.get(url, allow_redirects=True)\nopen('.\/time_series_covid19_deaths_US.csv', 'wb').write(r.content)\n#\nurl = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_deaths_global.csv'\nr = requests.get(url, allow_redirects=True)\nopen('.\/time_series_covid19_deaths_global.csv', 'wb').write(r.content)\n#\nurl = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_recovered_global.csv'\nr = requests.get(url, allow_redirects=True)\nopen('.\/time_series_covid19_recovered_global.csv', 'wb').write(r.content)\n#\nfrom datetime import datetime, timedelta\nurl=datetime.strftime(datetime.today() - timedelta(1), 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_daily_reports\/%m-%d-%Y.csv')\nr = requests.get(url, allow_redirects=True)\nopen('.\/csse_covid_19_daily_reports.csv', 'wb').write(r.content)\n","3c9f5d02":"import shutil\n\noriginal = r'..\/input\/input1\/covid19_by_country.csv'\ntarget = r'.\/covid19_by_country.csv'\nshutil.copyfile(original, target)\noriginal = r'..\/input\/input1\/GlobalLandTemperaturesByCountry.csv'\ntarget = r'.\/GlobalLandTemperaturesByCountry.csv'\nshutil.copyfile(original, target)\noriginal = r'..\/input\/input1\/GlobalLandTemperaturesByMajorCity.csv'\ntarget = r'.\/GlobalLandTemperaturesByMajorCity.csv'\nshutil.copyfile(original, target)\noriginal = r'..\/input\/input1\/API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'\ntarget = r'.\/API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'\nshutil.copyfile(original, target)\noriginal = r'..\/input\/input1\/shift.JPG'\ntarget = r'.\/shift.JPG'\nshutil.copyfile(original, target)\n\nprint(os.listdir(\".\/\"))\n","db72bc08":"# This Python 3 environment comes with many helpful analytics libraries installed\n# Loading datasets required for analysis\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"white\", color_codes=True)\nimport warnings # current version of seaborn generates a bunch of warnings that we'll ignore\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nprint(os.listdir(\".\/\"))\n#print(os.listdir(\"..\/input\/input\"))\n%load_ext autoreload\n%autoreload 2\nimport plotly.io as pio\npio.renderers.default = \"browser\"","70349dfd":"codiv_country=pd.read_csv('.\/covid19_by_country.csv')\ncodiv_country.sample(10)","2a7840f4":"codiv_country.describe()","af9c412c":"#nomalize some datas, also change the population in real size\ncodiv_country['Population 2020']=codiv_country['Population 2020']*1000\ncodiv_country['Tests_per_10kp']=codiv_country['Tests']*10000.0\/codiv_country['Population 2020']\ncodiv_country['Tests_per_10kp_log1p']=np.log1p(codiv_country['Tests_per_10kp'])\ncodiv_country['GDP 2018 per_1p_log1p']=np.log1p((codiv_country['GDP 2018']\/codiv_country['Population 2020']))","d42fdd4a":"codiv_country=codiv_country.drop([\n    'Sex Ratio','Crime Index','Density','Tests_per_10kp','Test Pop','sex0','sex14','sex25','sex54','sex64','sex65plus','Total Infected','Total Deaths','Total Recovered','Tests','GDP 2018','Population 2020'\n], axis=1)\ncodiv_country.sample(10)","c59e2fbb":"Health_expenditure=pd.read_csv(os.path.join('.\/', 'API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'))\nHealth_expenditure=Health_expenditure.set_index('Country Name')\nHealth_expenditure.sample(3)","0a218ae6":"codiv_country['Health expenditure Ratio'] = 0.0\nfor countryindex in Health_expenditure.index:\n    # If the country exists in the other table\n    if not codiv_country[codiv_country['Country']==countryindex].empty :\n        codiv_country.at[codiv_country['Country']==countryindex,'Health expenditure Ratio']=Health_expenditure[Health_expenditure.index==countryindex]['2017'][0]\n","acc44b06":"codiv_country.hist(figsize=(12, 12))\nplt.show()","cb738b4b":"codiv_country.dtypes","34835076":"codiv_country.isna().sum()","01252784":"codiv_country[codiv_country['Health expenditure Ratio'].isnull()]","84b84c83":"codiv_country['Health expenditure Ratio'] = codiv_country['Health expenditure Ratio'].fillna(codiv_country['Health expenditure Ratio'].mean())","b54d7680":"codiv_country['Tests_per_10kp_log1p'] = codiv_country['Tests_per_10kp_log1p'].fillna(0) ","7dc9fcf5":"codiv_country.Country[codiv_country.Country == \"United States\"] = \"US\"","09d28c89":"z_scores_Females= (codiv_country['Females 2018']- codiv_country['Females 2018'].mean()) \/codiv_country['Females 2018'].std()\nz_scores_Females.plot()\nplt.hlines(-1,0,100,colors=\"red\")\nplt.hlines(-2,0,100,colors=\"yellow\")\nplt.hlines(-3,0,100,colors=\"green\")\nplt.hlines(1,0,100,colors=\"red\")\nplt.hlines(2,0,100,colors=\"yellow\")\nplt.hlines(3,0,100,colors=\"green\")\nplt.ylabel(\"z_scores_Females\")\nplt.xlabel(\"country index\")","451bebe0":"codiv_country['Females 2018'].plot()\nplt.ylabel(\"Females %\")\nplt.xlabel(\"country index\")","ddacedcd":"idx_Females = (np.abs(z_scores_Females) > 2)\ncodiv_country['Females 2018'][idx_Females]=codiv_country['Females 2018'].mean()\nprint('z-scores_Females:', z_scores_Females.shape)\ncodiv_country['Females 2018'].plot()","c7d95c6e":"z_scores_Female_Lung= (codiv_country['Female Lung']- codiv_country['Female Lung'].mean()) \/codiv_country['Female Lung'].std()\nz_scores_Female_Lung.plot()\nplt.hlines(-1,0,100,colors=\"red\")\nplt.hlines(-2,0,100,colors=\"yellow\")\nplt.hlines(-3,0,100,colors=\"green\")\nplt.hlines(1,0,100,colors=\"red\")\nplt.hlines(2,0,100,colors=\"yellow\")\nplt.hlines(3,0,100,colors=\"green\")\nplt.ylabel(\"z_scores_Female_Lung\")\nplt.xlabel(\"country index\")","f7246dfd":"idx_Female_Lung = (np.abs(z_scores_Female_Lung) > 2)\ncodiv_country['Female Lung'][idx_Female_Lung]=codiv_country['Female Lung'].mean()\nprint('z-scores_Female_Lung:', z_scores_Female_Lung.shape)","c139bdae":"z_scores_Male_Lung= (codiv_country['Male Lung']- codiv_country['Male Lung'].mean()) \/codiv_country['Male Lung'].std()\nz_scores_Male_Lung.plot()\nplt.hlines(-1,0,100,colors=\"red\")\nplt.hlines(-2,0,100,colors=\"yellow\")\nplt.hlines(-3,0,100,colors=\"green\")\nplt.hlines(1,0,100,colors=\"red\")\nplt.hlines(2,0,100,colors=\"yellow\")\nplt.hlines(3,0,100,colors=\"green\")\nplt.ylabel(\"z_scores_Male_Lung\")\nplt.xlabel(\"country index\")","f8d1b349":"idx_Male_Lung = (np.abs(z_scores_Male_Lung) > 2)\ncodiv_country['Male Lung'][idx_Male_Lung]=codiv_country['Male Lung'].mean()\nprint('z-scores_Male_Lung:', z_scores_Male_Lung.shape)","f13dd8b3":"z_scores_Lung= (codiv_country['lung']- codiv_country['lung'].mean()) \/codiv_country['lung'].std()\nz_scores_Lung.plot()\nplt.hlines(-1,0,100,colors=\"red\",label=\"np.abs(z_scores_Females) > 1)\")\nplt.hlines(-2,0,100,colors=\"yellow\",label=\"np.abs(z_scores_Females) > 2)\")\nplt.hlines(-3,0,100,colors=\"green\",label=\"np.abs(z_scores_Females) > 3)\")\nplt.hlines(1,0,100,colors=\"red\",label=\"np.abs(z_scores_Females) > 1)\")\nplt.hlines(2,0,100,colors=\"yellow\",label=\"np.abs(z_scores_Females) > 2)\")\nplt.hlines(3,0,100,colors=\"green\",label=\"np.abs(z_scores_Females) > 3)\")\nplt.ylabel(\"z_scores_Lung\")\nplt.xlabel(\"country index\")","c0ec8127":"idx_Lung = (np.abs(z_scores_Lung) > 2)\ncodiv_country['lung'][idx_Lung]=codiv_country['lung'].mean()\nprint('z-scores_Lung:', z_scores_Lung.shape)","291e8252":"codiv_csse=pd.read_csv(os.path.join('.\/', 'csse_covid_19_daily_reports.csv'))\ncodiv_csse=codiv_csse.drop(['FIPS','Admin2','Last_Update','Combined_Key','Long_','Lat'], axis=1)\ncodiv_csse.sample(10)","2408f0c9":"codiv_csse = codiv_csse.rename(columns={'Confirmed': 'Infected'})","afb4a4e9":"codiv_csse=codiv_csse.groupby('Country_Region').sum().reset_index()\ncodiv_csse.sample(10)","32409aa9":"codiv_csse[\"Deaths Ratio\"]=codiv_csse[\"Deaths\"]*1000\/codiv_csse[\"Infected\"]\ncodiv_csse.sample(10)","8b67942a":"set(codiv_country['Country'].unique()) - set(codiv_csse['Country_Region'].unique())","9363946d":"codiv_csse.Country_Region[codiv_csse.Country_Region == \"Korea, South\"] = \"South Korea\"\ncodiv_csse.Country_Region[codiv_csse.Country_Region == \"Czechia\"] = \"Czech Republic\"","de8865e5":"codiv_csse=codiv_csse.set_index('Country_Region')\ncodiv_csse.sample(10)","844a3cc4":"codiv_csse.describe()","1bf5ae63":"codiv_csse.dtypes","6ba5a52c":"codiv_csse.isna().sum()","9b5308bf":"codiv_country['Total Infected'] = 0.0\ncodiv_country['Total Deaths'] = 0.0\ncodiv_country['Total Recovered'] = 0.0\ncodiv_country['Total Active'] = 0.0\ncodiv_country['Deaths Ratio'] = 0.0\nfor countryindex in codiv_csse.index:\n    # If the country exists in the other table\n    if not codiv_country[codiv_country['Country']==countryindex].empty :\n        codiv_country.at[codiv_country['Country']==countryindex,'Total Infected Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Infected'][0])\n        codiv_country.at[codiv_country['Country']==countryindex,'Total Infected']=codiv_csse[codiv_csse.index==countryindex]['Infected'][0]\n        codiv_country.at[codiv_country['Country']==countryindex,'Total Deaths Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Deaths'][0])\n        codiv_country.at[codiv_country['Country']==countryindex,'Total Deaths']=codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]\n        codiv_country.at[codiv_country['Country']==countryindex,'Total Recovered Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Recovered'][0])\n        codiv_country.at[codiv_country['Country']==countryindex,'Total Recovered']=codiv_csse[codiv_csse.index==countryindex]['Recovered'][0]\n        codiv_country.at[codiv_country['Country']==countryindex,'Deaths Ratio']=codiv_csse[codiv_csse.index==countryindex]['Deaths Ratio'][0]\n        if codiv_csse[codiv_csse.index==countryindex]['Active'][0] ==0.0:\n            codiv_country.at[codiv_country['Country']==countryindex,'Total Active Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Infected'][0]-codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]-codiv_csse[codiv_csse.index==countryindex]['Recovered'][0])\n            codiv_country.at[codiv_country['Country']==countryindex,'Total Active']=codiv_csse[codiv_csse.index==countryindex]['Infected'][0]-codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]-codiv_csse[codiv_csse.index==countryindex]['Recovered'][0]\n        else:\n            codiv_country.at[codiv_country['Country']==countryindex,'Total Active Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Active'][0])\n            codiv_country.at[codiv_country['Country']==countryindex,'Total Active']=codiv_csse[codiv_csse.index==countryindex]['Active'][0]","7659e7ea":"codiv_country.isnull()[['Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active', 'Deaths Ratio']].sum()","67a998bf":"codiv_country_qurantine=codiv_country[codiv_country['Quarantine'].notnull()]\ncodiv_country_Restrictions=codiv_country[codiv_country['Restrictions'].notnull()]\ncodiv_country_without_Restrictions_qurantine=codiv_country[codiv_country['Restrictions'].isnull() & codiv_country['Quarantine'].isnull()]\n\nprint(\"codiv_country_qurantine shape\" , codiv_country_qurantine.shape)\nprint(\"codiv_country_Restrictionse shape\" , codiv_country_Restrictions.shape)\nprint(\"codiv_country_without_Restrictions_quarantine\" , codiv_country_without_Restrictions_qurantine.shape)","21c0d75d":"codiv_time_confirmed=pd.read_csv(os.path.join('.\/', 'time_series_covid19_confirmed_global.csv'))\ncodiv_time_confirmed=codiv_time_confirmed.drop(['Lat','Long'], axis=1)\n\ncodiv_time_deaths=pd.read_csv(os.path.join('.\/', 'time_series_covid19_deaths_global.csv'))\ncodiv_time_deaths=codiv_time_deaths.drop(['Lat','Long'], axis=1)\n\ncodiv_time_recovered=pd.read_csv(os.path.join('.\/', 'time_series_covid19_recovered_global.csv'))\ncodiv_time_recovered=codiv_time_recovered.drop(['Lat','Long'], axis=1)\n\ncodiv_time_confirmed.sample(10)","7d433264":"codiv_country_temp=pd.read_csv('.\/\/GlobalLandTemperaturesByCountry.csv')\n#codiv_country_temp.loc['2020-01-01':'2020-04-01']\ncodiv_country_temp=codiv_country_temp.drop(['AverageTemperatureUncertainty'], axis=1)\ncodiv_country_temp=codiv_country_temp[((codiv_country_temp['dt'] > '2013-01-01') & (codiv_country_temp['dt'] < '2013-04-01')) | \n                                      ((codiv_country_temp['dt'] > '2012-01-01') & (codiv_country_temp['dt'] < '2012-04-01')) | \n                                      ((codiv_country_temp['dt'] > '2011-01-01') & (codiv_country_temp['dt'] < '2011-04-01'))]\ncodiv_country_temp = codiv_country_temp.groupby(['Country'])['AverageTemperature'].mean()\nprint(codiv_country_temp.sample(10))\nprint(\"codiv_country_temp\",codiv_country_temp.shape)","4a3f8861":"print(codiv_country_temp.describe())","3cc64271":"codiv_country_tempMajorCity=pd.read_csv('.\/\/GlobalLandTemperaturesByMajorCity.csv')\n\ncodiv_country_tempMajorCity=codiv_country_tempMajorCity.drop(['AverageTemperatureUncertainty','Latitude','Longitude'], axis=1)\ncodiv_country_tempMajorCity=codiv_country_tempMajorCity[((codiv_country_tempMajorCity['dt'] > '2013-01-01') & (codiv_country_tempMajorCity['dt'] < '2013-04-01')) | \n                                      ((codiv_country_tempMajorCity['dt'] > '2012-01-01') & (codiv_country_tempMajorCity['dt'] < '2012-04-01')) | \n                                      ((codiv_country_tempMajorCity['dt'] > '2011-01-01') & (codiv_country_tempMajorCity['dt'] < '2011-04-01'))]\ncodiv_country_tempMajorCity= codiv_country_tempMajorCity.groupby(['Country'])['AverageTemperature'].mean()\nprint(codiv_country_tempMajorCity.sample(10))\nprint(\"codiv_country_tempMajorCity\",codiv_country_tempMajorCity.shape)\nprint(\"243-49=194 countries dont have temperature for big city\")","b6d2f36e":"Temperature_difference=(codiv_country_tempMajorCity-codiv_country_temp).dropna()\nTemperature_difference=Temperature_difference.sort_values( ascending=False)\nx=Temperature_difference.index\ny=Temperature_difference\nfig, ax = plt.subplots(figsize=(20,10))\nax.scatter(x, y, alpha=0.5)\nplt.xticks(rotation=45)\nplt.ylabel(\"Delta Temp AvrMaincity-AvrCountry\")\nplt.grid(True)\nplt.show()","e732106a":"codiv_country['Temp_mean_jan_apr'] = 0.0\nfor countryindex in codiv_country_tempMajorCity.index:\n    if not codiv_country[codiv_country['Country']==countryindex].empty :\n        codiv_country.at[codiv_country['Country']==countryindex,'Temp_mean_jan_apr']=codiv_country_tempMajorCity.loc[countryindex]\ncodiv_country['Temp_mean_jan_apr'].sample(10)","dd8e9a55":"for countryindex in codiv_country_temp.index:\n    if not codiv_country[codiv_country['Country']==countryindex].empty:\n        if codiv_country[codiv_country['Country']==countryindex]['Temp_mean_jan_apr'].values[0] == 0:\n            codiv_country.at[codiv_country['Country']==countryindex,'Temp_mean_jan_apr']=codiv_country_temp.loc[countryindex]\ncodiv_country.head()","a5ff907b":"codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)]","63a85589":"transit=codiv_country.sort_values(by='Total Infected', ascending=False)\ntransit = transit.reset_index(drop=True)\ntransit.head(11).style.background_gradient(cmap='Blues')","86c8eba1":"transit=codiv_country.sort_values(by='Total Active', ascending=False)\ntransit = transit.reset_index(drop=True)\ntransit.head(11).style.background_gradient(cmap='Greens')","c66b27cc":"transit=codiv_country.sort_values(by='Total Deaths', ascending=False)\ntransit = transit.reset_index(drop=True)\ntransit.head(11).style.background_gradient(cmap='Reds')","fba11343":"transit=codiv_country.sort_values(by='Deaths Ratio', ascending=False)\ntransit = transit.reset_index(drop=True)\ntransit.head(11).style.background_gradient(cmap='Oranges')","ec880cfe":"codiv_country_short=codiv_country.sort_values(by='Total Infected', ascending=False).head(n=60).reset_index(drop=True)\ncodiv_country_short","fa05975b":"codiv_country_short.describe()","6cafc92e":"sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\\n   .map(plt.scatter, \"Tests_per_10kp_log1p\", \"Total Infected Log10\") \\\n   .add_legend()\n\nplt.grid(True)","05dc48fd":"#histogram of \"total infected\" for countries that have Tests_per_10kp_log1p == 0\ncodiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] == 0][\"Total Infected Log10\"].hist(figsize=(6, 6), alpha=0.5, density=True)\nplt.xlabel('Total Infected Log1p')\nplt.ylabel('Nr Country density')\nplt.title('Histogram Tests_per_10kp_log1p==0')","92d58d2e":"#histogram of \"total infected\" for countries that have Tests_per_10kp_log1p != 0\ncodiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] != 0][\"Total Infected Log10\"].hist(figsize=(6,6), alpha=0.5, density=True)\nplt.xlabel('Total Infected Log1p')\nplt.ylabel('Nr Country density')\nplt.title('Histogram Tests_per_10kp_log1p not 0')","53aa6fc5":"sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\\n   .map(plt.scatter, \"Tests_per_10kp_log1p\", \"Total Active Log10\") \\\n   .add_legend()\nplt.grid(True)","f66763c0":"codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] == 0][\"Total Active Log10\"].hist(figsize=(6, 6), alpha=0.5, density=True)\nplt.xlabel('Total Active Log10')\nplt.ylabel('Nr Country density')\nplt.title('Histogram Tests_per_10kp_log1p==0')","16f02056":"codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] != 0][\"Total Active Log10\"].hist(figsize=(6,6), alpha=0.5, density=True)\nplt.xlabel('Total Active Log10')\nplt.ylabel('Nr Country density')\nplt.title('Histogram Tests_per_10kp_log1p not 0')","95de3a3e":"codiv_country_short.drop([   'Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active'], axis=1)","239b2d36":"import seaborn as sns\nimport numpy as np\nfrom scipy.stats import pearsonr\n#I use a temp table to hide some columns not needed for the sns.\ncodiv_country_short_tmp=codiv_country_short.drop([ 'Deaths Ratio','Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active','Total Recovered Log10','Total Infected Log10','Total Deaths Log10','Total Active Log10'], axis=1)\nfor col in codiv_country_short_tmp.loc[:, codiv_country_short_tmp.dtypes == np.number].keys():\n    sns.jointplot(codiv_country_short['Total Infected Log10'], col, data=codiv_country_short, height=6, s=1, color=\"blue\")\n    corr, _ = pearsonr(codiv_country_short['Total Infected Log10'], codiv_country_short[col])\n    print(\"Pearsons correlation Total Infected Log10 <-> %s : %.3f\" %(col, corr))\n    sns.jointplot(codiv_country_short['Total Deaths Log10'], col, data=codiv_country_short, height=6, s=1, color=\"red\")\n    corr, _ = pearsonr(codiv_country_short['Total Deaths Log10'], codiv_country_short[col])\n    print(\"Pearsons correlation Total Deaths Log10 <-> %s : %.3f\" %(col, corr))\n    sns.jointplot(codiv_country_short['Total Active Log10'], col, data=codiv_country_short, height=6, s=1, color=\"green\")\n    corr, _ = pearsonr(codiv_country_short['Total Active Log10'], codiv_country_short[col])\n    print(\"Pearsons correlation Total Active Log10 <-> %s : %.3f\" %(col, corr))\n    sns.jointplot(codiv_country_short['Deaths Ratio'], col, data=codiv_country_short, height=6, s=1, color=\"orange\")\n    corr, _ = pearsonr(codiv_country_short['Deaths Ratio'], codiv_country_short[col])\n    print(\"Pearsons correlation Deaths Ratio <-> %s : %.3f\" %(col, corr))\nplt.show()","c86f67ef":"coefs_poly3 = np.polyfit(codiv_country_short[\"Temp_mean_jan_apr\"], codiv_country_short[\"Total Infected Log10\"], deg=8) \nsns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\\n   .map(plt.scatter, \"Temp_mean_jan_apr\", \"Total Infected Log10\") \\\n   .add_legend()\nfor indexq in range(-80,300,1):\n    plt.plot(indexq\/10,np.polyval(coefs_poly3, indexq\/10), '.', color='black')\nplt.vlines(7,7,14,colors=\"red\",linestyles='dashed')\nplt.vlines(24,7,14,colors=\"red\",linestyles='dashed')\nplt.grid(True)","03e96392":"m,b = np.polyfit(codiv_country_short[\"Health expenditure Ratio\"], codiv_country_short[\"Total Infected Log10\"], 1) \n\nsns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\\n   .map(plt.scatter, \"Health expenditure Ratio\", \"Total Infected Log10\") \\\n   .add_legend()\nplt.plot(codiv_country_short[\"Health expenditure Ratio\"], m*codiv_country_short[\"Health expenditure Ratio\"]+b, '--k') \nplt.grid(True)","042b0bbc":"# Working on a copy\ncodiv_country_analyze = codiv_country.copy()\n\n# Creating categorical variables\ncodiv_country_analyze['Quarantine_cat'] = codiv_country_analyze['Quarantine'].notnull().astype(int)\ncodiv_country_analyze['Restrictions_cat'] = codiv_country_analyze['Restrictions'].notnull().astype(int)\ncodiv_country_analyze['Schools_cat'] = codiv_country_analyze['Schools'].notnull().astype(int)\n\n# \ncodiv_country_analyze=codiv_country_analyze.drop([\n    'Quarantine','Schools','Restrictions', # now categorical\n    'Country', # not helpful\n    \n    # Only keep \"Total Infected Log10\"\n    'Total Deaths','Total Infected','Total Active','Total Recovered', \n    \"Total Deaths Log10\",\"Total Recovered Log10\",\"Total Active Log10\",\"Deaths Ratio\"\n], axis=1)\n\ncodiv_country_analyze.sample(10)","f7f94bc2":"codiv_country_analyze[codiv_country_analyze.isnull().any(axis=1)]","fbf2bc5d":"codiv_country_analyze = codiv_country_analyze[codiv_country_analyze.notnull().all(axis=1)]","a78ac082":"columns = ['method','Cross-validation']\nindex=range(8)\ndf_resultat = pd.DataFrame(index=index, columns=columns)","c20006e7":"codiv_country_analyze['category_balanced'], bin_edges_balanced=pd.qcut(codiv_country_analyze['Total Infected Log10'], q=6,labels=False, retbins=True)\nprint(bin_edges_balanced)","5cf1cda1":"#explanation of these values\npd.qcut(codiv_country_analyze['Total Infected Log10'], q=6)","d70aa81c":"#balanced bins deliver category for 0-5, euqidistant from 1-6, so i will add 1 to the category numbers\ncodiv_country_analyze['category_balanced'] = codiv_country_analyze['category_balanced'] +1","0a1c31fb":"ylevel0_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][0]\nylevel1_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][1]\nylevel2_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][2]\nylevel3_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][3]\nylevel4_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][4]\nylevel5_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][5]\nylevel6_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][6]\nLevel1_balanced='%.3f<Infected<%.3f'%(ylevel0_balanced,ylevel1_balanced,)\nLevel2_balanced='%.3f<Infected<%.3f'%(ylevel1_balanced,ylevel2_balanced,)\nLevel3_balanced='%.3f<Infected<%.3f'%(ylevel2_balanced,ylevel3_balanced,)\nLevel4_balanced='%.3f<Infected<%.3f'%(ylevel3_balanced,ylevel4_balanced,)\nLevel5_balanced='%.3f<Infected<%.3f'%(ylevel4_balanced,ylevel5_balanced,)\nLevel6_balanced='%.3f<Infected<%.3f'%(ylevel5_balanced,ylevel6_balanced,)\nprint(\"Level1_balanced= \",Level1_balanced)\nprint(\"Level2_balanced= \",Level2_balanced)\nprint(\"Level3_balanced= \",Level3_balanced)\nprint(\"Level4_balanced= \",Level4_balanced)\nprint(\"Level5_balanced= \",Level5_balanced)\nprint(\"Level6_balanced= \",Level6_balanced)","53ff953d":"list(codiv_country_analyze['category_balanced'].value_counts())","7986e56d":"ymax=codiv_country_analyze['Total Infected Log10'].values.max()\nymax=ymax+ymax*0.001\nprint(\"ymax = \",ymax)\nymin=(codiv_country_analyze['Total Infected Log10'].values.min())\nymin=ymin-ymin*0.001\nprint(\"ymin = \",ymin)\nyinterval=(ymax-ymin)\/6\n# \nylevel0_equidistant = ymin\nylevel1_equidistant = ymin+yinterval*1\nylevel2_equidistant = ymin+yinterval*2\nylevel3_equidistant = ymin+yinterval*3\nylevel4_equidistant = ymin+yinterval*4\nylevel5_equidistant = ymin+yinterval*5\nylevel6_equidistant = ymin+yinterval*6\n\n#\nLevel1_equidistant='%.3f<Infected<%.3f'%(ylevel0_equidistant,ylevel1_equidistant,)\nLevel2_equidistant='%.3f<Infected<%.3f'%(ylevel1_equidistant,ylevel2_equidistant,)\nLevel3_equidistant='%.3f<Infected<%.3f'%(ylevel2_equidistant,ylevel3_equidistant,)\nLevel4_equidistant='%.3f<Infected<%.3f'%(ylevel3_equidistant,ylevel4_equidistant,)\nLevel5_equidistant='%.3f<Infected<%.3f'%(ylevel4_equidistant,ylevel5_equidistant,)\nLevel6_equidistant='%.3f<Infected<%.3f'%(ylevel5_equidistant,ylevel6_equidistant,)\ncodiv_country_analyze['category_equidistant']=np.digitize(codiv_country_analyze['Total Infected Log10'].values,\n                          bins=[ylevel0_equidistant,ylevel1_equidistant,ylevel2_equidistant,ylevel3_equidistant,ylevel4_equidistant,ylevel5_equidistant,ylevel6_equidistant])\nprint(\"Level1_equidistant= \",Level1_equidistant)\nprint(\"Level2_equidistant= \",Level2_equidistant)\nprint(\"Level3_equidistant= \",Level3_equidistant)\nprint(\"Level4_equidistant= \",Level4_equidistant)\nprint(\"Level5_equidistant= \",Level5_equidistant)\nprint(\"Level6_equidistant= \",Level6_equidistant)","125f1b4b":"y_equidistant = codiv_country_analyze['category_equidistant']\ny_balanced = codiv_country_analyze['category_balanced']\nX = codiv_country_analyze.drop(['Total Infected Log10','category_balanced','category_equidistant'], axis=1) \nsns.countplot(y_balanced)","584df149":"sns.countplot(y_equidistant)","0bf91b4a":"X.sample(10)","73309623":"from sklearn.model_selection import train_test_split\nX_tr_equid, X_te_equid, y_tr_equid, y_te_equid = train_test_split(X, y_equidistant, test_size=0.15, random_state=1)\nprint('Train set equidistant:', X_tr_equid.shape, y_tr_equid.shape)\nprint('Testn set equidistant:', X_te_equid.shape, y_te_equid.shape)\nX_tr_balan, X_te_balan, y_tr_balan, y_te_balan = train_test_split(X, y_balanced, test_size=0.15, random_state=1)\nprint('Train set balanced:', X_tr_balan.shape, y_tr_balan.shape)\nprint('Testn set balanced:', X_te_balan.shape, y_te_balan.shape)","4acd1a09":"sns.countplot(y_te_equid)","bffbe43d":"sns.countplot(y_te_balan)","be32a0e4":"from sklearn.model_selection import KFold\ncv_strategy = KFold(n_splits=5, shuffle=False, random_state=None)","89097660":"from sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X, y_equidistant)\ndummy_clf.predict(X)\nprint(\"scores baseline mostfrequent classifier equidistant = \", dummy_clf.score(X, y_equidistant))","f21b526d":"df_resultat.at[df_resultat.index==0,\"method\"]=\"baseline_equidistant\"\nBaseline_training_scores = cross_validate(dummy_clf, X, y_equidistant, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==0,\"Cross-validation\"]=np.mean(Baseline_training_scores['test_score'])\nprint(\"crossvalidation baseline mostfrequent classifier equidistant = \", np.mean(Baseline_training_scores['test_score']) )","b52773e4":"from sklearn.metrics import confusion_matrix\ndummy_clf= DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_tr_equid, y_tr_equid)\ny_pred_equid=dummy_clf.predict(X_te_equid)\nmatrix = confusion_matrix(y_true=y_te_equid, y_pred=y_pred_equid)\nprint(matrix)","d5d1a1d8":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(dummy_clf, X_te_equid, y_te_equid, cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])","357c3896":"df_resultat.at[df_resultat.index==1,\"method\"]=\"baseline balanced bins\"\nBaseline_training_scores = cross_validate(dummy_clf, X, y_balanced, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==1,\"Cross-validation\"]=np.mean(Baseline_training_scores['test_score'])\nprint(\"crossvalidation baseline mostfrequent classifier balanced = \", np.mean(Baseline_training_scores['test_score']) )","31a6fa87":"dummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_tr_balan, y_tr_balan)\ny_pred_balan=dummy_clf.predict(X_te_balan)","a6e795a6":"matrix = confusion_matrix(y_true=y_te_balan, y_pred=y_pred_balan)\nprint(matrix)","a3b9190d":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(dummy_clf, X_te_balan, y_te_balan, cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])","e6322054":"from sklearn.tree import DecisionTreeClassifier","d32f5c7a":"depths=range(1,15)\nscoring='accuracy'\ncv_val_scores_list = []\ncv_val_scores_std = []\ncv_val_scores_mean = []\naccuracy_scores = []\nfor depth in depths:\n    DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)\n    DecisionTree.fit(X, y_equidistant)\n    cv_val_scores = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)\n    cv_val_scores_list.append(cv_val_scores)\n    cv_val_scores_mean.append(cv_val_scores.mean())\n    cv_val_scores_std.append(cv_val_scores.std())\n    accuracy_scores.append(DecisionTree.score(X, y_equidistant))\n    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_equidistant))\n    print(Text)\ncv_val_scores_mean = np.array(cv_val_scores_mean)\ncv_val_scores_std = np.array(cv_val_scores_std)\naccuracy_scores = np.array(accuracy_scores)","d6f5ce8f":"print(\"The maximum is by depth = \",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)\nOptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1","66e255fb":"fig, ax = plt.subplots(1,1, figsize=(15,5))\nax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\nax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\nylim = plt.ylim()\nax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)\ntitle=\"decision tree accurency\"\nax.set_title(title, fontsize=16)\nax.set_xlabel('Tree depth', fontsize=14)\nax.set_ylabel('accuracy', fontsize=14)\nax.set_xticks(depths)\nax.legend()","dcf21585":"DecisionTree = DecisionTreeClassifier(\n    criterion='gini',  random_state=0, max_depth=OptDepth)\n# Fit decision tree\nDecisionTree.fit(X, y_equidistant)\nTree_scores = cross_validate(DecisionTree, X, y_equidistant, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==2,\"method\"]=\"decision-tree equidistant\"\ndf_resultat.at[df_resultat.index==2,\"Cross-validation\"]=np.mean(Tree_scores['test_score'])\n                                                              \nprint(\"score DecistionTree equidistant =\",DecisionTree.score(X, y_equidistant))\nprint(\"cross validation DecisionTree equidistant =\",np.mean(Tree_scores['test_score']))","d8a2a45e":"from sklearn.metrics import confusion_matrix\nDecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)\nDecisionTree_confusion.fit(X_tr_equid, y_tr_equid)\nconfusion_matrix(y_te_equid, DecisionTree_confusion.predict(X_te_equid))","394d25d0":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(DecisionTree_confusion, X_te_equid, y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])","b0c69f7b":"from sklearn.tree import export_graphviz\nimport os\nos.environ[\"PATH\"] += os.pathsep + 'D:\/Program Files (x86)\/Graphviz2.38\/bin\/'\n# Export decision tree\ndot_data = export_graphviz(\n    DecisionTree, out_file=None,\n    feature_names=X.columns, class_names=[Level1_equidistant,Level2_equidistant, Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant],\n    filled=True, rounded=True, proportion=True   \n)\nimport graphviz\n\n# Display decision tree\ngraphviz.Source(dot_data)","f0c89e6c":"depths=range(1,15)\nscoring='accuracy'\ncv_val_scores_list = []\ncv_val_scores_std = []\ncv_val_scores_mean = []\naccuracy_scores = []\nfor depth in depths:\n    DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)\n    DecisionTree.fit(X, y_balanced)\n    cv_val_scores = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring)\n    cv_val_scores_list.append(cv_val_scores)\n    cv_val_scores_mean.append(cv_val_scores.mean())\n    cv_val_scores_std.append(cv_val_scores.std())\n    accuracy_scores.append(DecisionTree.score(X, y_balanced))\n    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced))\n    print(Text)\ncv_val_scores_mean = np.array(cv_val_scores_mean)\ncv_val_scores_std = np.array(cv_val_scores_std)\naccuracy_scores = np.array(accuracy_scores)","a2b491ed":"print(\"The maximum is by depth_balanced = \",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)\nOptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1","ab536863":"fig, ax = plt.subplots(1,1, figsize=(15,5))\nax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\nax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\nylim = plt.ylim()\nax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)\ntitle=\"decision tree accurency - _balanced bins\"\nax.set_title(title, fontsize=16)\nax.set_xlabel('Tree depth', fontsize=14)\nax.set_ylabel('accuracy', fontsize=14)\nax.set_xticks(depths)\nax.legend()","82dfcc62":"DecisionTree = DecisionTreeClassifier(\n    criterion='gini',  random_state=0, max_depth=OptDepth)\n# Fit decision tree\nDecisionTree.fit(X, y_balanced)\n# Get score\nTree_scores = cross_validate(DecisionTree, X, y_balanced, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==3,\"method\"]=\"decision-tree _balanced bins\"\ndf_resultat.at[df_resultat.index==3,\"Cross-validation\"]=np.mean(Tree_scores['test_score'])\nprint(\"score DecisionTree _balanced bins =\",DecisionTree.score(X, y_balanced))\nprint(\"cross validation DecisionTree balanced bins=\",np.mean(Tree_scores['test_score']))","fb9a4ff0":"from sklearn.metrics import confusion_matrix\nDecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)\nDecisionTree_confusion.fit(X_tr_balan, y_tr_balan)\nconfusion_matrix(y_te_balan, DecisionTree_confusion.predict(X_te_balan))","63b02cd7":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(DecisionTree_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])","e7a5c472":"from sklearn.tree import export_graphviz\nimport os\nos.environ[\"PATH\"] += os.pathsep + 'D:\/Program Files (x86)\/Graphviz2.38\/bin\/'\n# Export decision tree\ndot_data = export_graphviz(\n    DecisionTree, out_file=None,\n    feature_names=X.columns, class_names=[Level1_balanced,Level2_balanced, Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced],\n    filled=True, rounded=True, proportion=True   \n)\nimport graphviz\n\n# Display decision tree\ngraphviz.Source(dot_data)","a7679e33":"from sklearn.ensemble import RandomForestClassifier\nEstimators=range(100,2000,100)\nscoring='accuracy'\ncv_val_scores_list = []\ncv_val_scores_std = []\ncv_val_scores_mean = []\naccuracy_scores = []\nfor Estimator in Estimators:\n    forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)\n    forest.fit(X, y_equidistant)\n    cv_val_scores = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)\n    cv_val_scores_list.append(cv_val_scores)\n    cv_val_scores_mean.append(cv_val_scores.mean())\n    cv_val_scores_std.append(cv_val_scores.std())\n    accuracy_scores.append(forest.fit(X, y_equidistant).score(X, y_equidistant))\n    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))\n    print(Text)\ncv_val_scores_mean = np.array(cv_val_scores_mean)\ncv_val_scores_std = np.array(cv_val_scores_std)\naccuracy_scores = np.array(accuracy_scores)","24b18262":"print(\"The maximum is by Estimator_balanced = \",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)\nOptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100","6d23a5fb":"fig, ax = plt.subplots(1,1, figsize=(15,5))\nax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\nax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\nylim = plt.ylim()\nax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)\ntitle=\"Forest accurency - _balanced bins\"\nax.set_title(title, fontsize=16)\nax.set_xlabel('Forest Estimator', fontsize=14)\nax.set_ylabel('accuracy', fontsize=14)\nax.set_xticks(Estimators)\nax.legend()","135b9b0e":"# Build a forest and compute the feature importances\nforest = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\nforest.fit(X, y_equidistant)","565df3c6":"# Mean test score of a 250x decision tree \nForest_scores = cross_validate(forest, X, y_equidistant, cv=cv_strategy)\nprint('Forest by equidistant interval score {:.3f}'.format(forest.score(X, y_equidistant)))\ndf_resultat.at[df_resultat.index==4,\"method\"]=\"Forest equidistant\"\ndf_resultat.at[df_resultat.index==4,\"Cross-validation\"]=np.mean(Forest_scores['test_score'])\nprint('Forest by equidistant interval cross validation {:.3f}'.format(np.mean(Forest_scores['test_score'])))","aa339164":"forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\nforest_confusion.fit(X_tr_equid, y_tr_equid)\nconfusion_matrix(y_te_equid, forest_confusion.predict(X_te_equid))","9e150610":"plot_confusion_matrix(forest_confusion, X_te_equid,y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])","f76a6b97":"importances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\nindices = np.argsort(importances)[::-1]","8941d788":"# Print the feature ranking\nprint(\"Feature ranking by equidistand interval:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure(figsize=(20,10))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)\nplt.xlim([-1, X.shape[1]])\nplt.show()","73aa3f79":"Estimators=range(100,2000,100)\nscoring='accuracy'\ncv_val_scores_list = []\ncv_val_scores_std = []\ncv_val_scores_mean = []\naccuracy_scores = []\nfor Estimator in Estimators:\n    forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)\n    forest.fit(X, y_balanced)\n    cv_val_scores = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring)\n    cv_val_scores_list.append(cv_val_scores)\n    cv_val_scores_mean.append(cv_val_scores.mean())\n    cv_val_scores_std.append(cv_val_scores.std())\n    accuracy_scores.append(forest.fit(X, y_balanced).score(X, y_balanced))\n    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced))\n    print(Text)\ncv_val_scores_mean = np.array(cv_val_scores_mean)\ncv_val_scores_std = np.array(cv_val_scores_std)\naccuracy_scores = np.array(accuracy_scores)","1a37e228":"print(\"The maximum is by Estimator_balanced = \",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)\nOptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100","d2f35a96":"fig, ax = plt.subplots(1,1, figsize=(15,5))\nax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\nax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\nylim = plt.ylim()\nax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)\ntitle=\"Forest accurency - _balanced bins\"\nax.set_title(title, fontsize=16)\nax.set_xlabel('Forest Estimator', fontsize=14)\nax.set_ylabel('accuracy', fontsize=14)\nax.set_xticks(Estimators)\nax.legend()","096723f3":"from sklearn.model_selection import cross_validate\nForest_scores = cross_validate(forest, X, y_balanced, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==5,\"method\"]=\"Forest balanced bins\"\ndf_resultat.at[df_resultat.index==5,\"Cross-validation\"]=np.mean(Forest_scores['test_score'])\nprint('Forest by balanced bins of element score {:.3f}'.format(forest.score(X, y_balanced)))\nprint('Forest by balanced bins of element - mean test {:.3f}'.format(np.mean(Forest_scores['test_score'])))","c4986fb0":"forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\nforest_confusion.fit(X_tr_balan, y_tr_balan)\nconfusion_matrix(y_te_balan, forest_confusion.predict(X_te_balan))","537294a0":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(forest_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])","d263cf14":"importances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]","02be398d":"# Print the feature ranking\nprint(\"Feature ranking by balanced bins:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure(figsize=(20,10))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)\nplt.xlim([-1, X.shape[1]])\nplt.show()","301af6a2":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\n# Apply PCA\npca.fit(X, y=None); # Unsupervised learning, no y variable\nfeature_2= pca.transform(X)\nfeature_2_df=pd.DataFrame(feature_2)\nlen_cluster, col=feature_2_df.shape","be96ea3d":"from sklearn.model_selection import train_test_split\nX_fea_2_tr_equid, X_fea_2_te_equid, y_fea_2_tr_equid, y_fea_2_te_equid = train_test_split(feature_2, y_equidistant, test_size=0.15, random_state=1)\nprint('Train set equidistant:', X_fea_2_tr_equid.shape, y_fea_2_tr_equid.shape)\nprint('Testn set equidistant:', X_fea_2_te_equid.shape, y_fea_2_te_equid.shape)\nX_fea_2_tr_balan, X_fea_2_te_balan, y_fea_2_tr_balan, y_fea_2_te_balan = train_test_split(feature_2, y_balanced, test_size=0.15, random_state=1)\nprint('Train set balanced:', X_fea_2_tr_balan.shape, y_fea_2_tr_balan.shape)\nprint('Testn set balanced:', X_fea_2_te_balan.shape, y_fea_2_te_balan.shape)","a0ac6a3b":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Create k-NN classifier\npipe = Pipeline([\n    #('scaler', StandardScaler()), # With standardization\n    ('scaler', None), # Better performance without standardization!\n    ('knn', KNeighborsClassifier(\n        n_jobs=-1 # As many parallel jobs as possible\n    ))\n])\n","c1128744":"from matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nneighbors = list(range(1,35))\nCV_scores = []\n\n\nfor n_neighborss in neighbors:\n    weights='distance'\n    feature1=pd.DataFrame(feature_2).loc[:][0]\n    feature2=pd.DataFrame(feature_2).loc[:][1]\n    h = .02  # step size in the mesh\n    # we create an instance of Neighbours Classifier and fit the data.\n    # Create a k-NN pipeline\n    knn_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights)) ])\n    # Fit estimator\n    knn_pipe.fit(feature_2, y_equidistant)\n    scores = cross_val_score(knn_pipe,feature_2,y_equidistant,cv=cv_strategy )\n    CV_scores.append(scores.mean())\nMissclassification_Error = [1-x for x in CV_scores]\nprint(\"the optimal k is \",format(Missclassification_Error.index(min(Missclassification_Error))))\nk_Optimal=Missclassification_Error.index(min(Missclassification_Error))","5fae2e0f":"from matplotlib.legend_handler import HandlerLine2D\nplt.plot(Missclassification_Error)\nplt.xlabel(\"number of neighbors K\")\nplt.ylabel(\"Missclassification Error\")\nplt.show()","91eb298a":"from matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\nfrom sklearn.metrics import accuracy_score\n\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])\n\nfeature1=pd.DataFrame(feature_2).loc[:][0]\nfeature2=pd.DataFrame(feature_2).loc[:][1]\n\nweights='distance'\nh = .02  # step size in the mesh\n# we create an instance of Neighbours Classifier and fit the data.\n# Create a k-NN pipeline\nknn_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal))\n    #, weights=weights))\n])\n# Fit estimator\nknn_pipe.fit(feature_2, y_equidistant)\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nfeature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1\nfeature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1\nxx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))\nZ = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])\nZo = Z.reshape(xx.shape)","b7fccf1e":"# Put the result into a color plot\nplt.figure(figsize=(18, 10))\nplt.pcolormesh(xx, yy, Zo, cmap=cmap_light)\n#Label1=pd.DataFrame(list(class_index.items()))\nLabel1=pd.DataFrame([1,2,3,4,5])\n# Plot also the training points\nplt.scatter(feature1, feature2, c=y_equidistant, cmap=cmap_bold)\n#color bar with label\ncbar = plt.colorbar() \ncbar.ax.set_yticklabels([Level1_equidistant,Level2_equidistant,Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant])\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xlabel(\"feature 1\")\nplt.ylabel(\"feature 2\")\nplt.title(\"6-Class classification,   equidistant bins -  total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))\nplt.show()","7b4682d9":"from sklearn.model_selection import cross_validate\nknn_scores = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==6,\"method\"]=\"knn equidistant bins\"\ndf_resultat.at[df_resultat.index==6,\"Cross-validation\"]=np.mean(knn_scores['test_score'])\nprint('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant)))\nprint('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score'])))","d55f2dee":"knn_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n])\n# Fit estimator\nknn_confusion=knn_pipe.fit(X_fea_2_tr_equid, y_fea_2_tr_equid)\nknn_predict = knn_pipe.predict(X_fea_2_te_equid)\nconfusion_matrix(y_fea_2_te_equid, knn_predict)","9fcb1b75":"plot_confusion_matrix(knn_confusion, X_fea_2_te_equid, y_fea_2_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])","1a42e3f7":"from sklearn.metrics import classification_report\nprint(classification_report(y_true=y_fea_2_te_equid, y_pred=knn_predict))","86d7d4cf":"neighbors = list(range(1,35))\nCV_scores = []\n\nfor n_neighborss in neighbors:\n    weights='distance'\n    feature1=pd.DataFrame(feature_2).loc[:][0]\n    feature2=pd.DataFrame(feature_2).loc[:][1]\n    h = .02  # step size in the mesh\n    # we create an instance of Neighbours Classifier and fit the data.\n    # Create a k-NN pipeline\n    knn_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights))\n    ])\n    # Fit estimator\n    knn_pipe.fit(feature_2, y_balanced)\n    #\n    scores = cross_val_score(knn_pipe,feature_2,y_balanced,cv=cv_strategy )\n    CV_scores.append(scores.mean())\n    \nMissclassification_Error = [1-x for x in CV_scores]\nprint(\"the optimal k is \",format(Missclassification_Error.index(min(Missclassification_Error))))\nk_Optimal=Missclassification_Error.index(min(Missclassification_Error))\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nplt.plot( Missclassification_Error)\nplt.xlabel(\"number of neighbors K\")\nplt.ylabel(\"Missclassification Error\")\nplt.show()","a9c60de7":"# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])\n#\nfeature1=pd.DataFrame(feature_2).loc[:][0]\nfeature2=pd.DataFrame(feature_2).loc[:][1]\n#\nweights='distance'\nh = .02  # step size in the mesh\n# Create a k-NN pipeline\nknn_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n])\n# Fit estimator\nknn_pipe.fit(feature_2, y_balanced)\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nfeature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1\nfeature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1\nxx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))\nZ = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])\nZo = Z.reshape(xx.shape)","318aef94":"# Put the result into a color plot\nplt.figure(figsize=(18, 10))\nplt.pcolormesh(xx, yy, Zo, cmap=cmap_light)\n#Label1=pd.DataFrame(list(class_index.items()))\nLabel1=pd.DataFrame([1,2,3,4,5])\n# Plot also the training points\nplt.scatter(feature1, feature2, c=y_balanced, cmap=cmap_bold)\ncbar = plt.colorbar() \ncbar.ax.set_yticklabels([Level1_balanced,Level2_balanced,Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced])\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xlabel(\"feature 1\")\nplt.ylabel(\"feature 2\")\nplt.title(\"6-Class classification,   balanced bins -  total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))\n\nplt.show()","e8e85246":"from sklearn.model_selection import cross_validate\nknn_scores = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==7,\"method\"]=\"knn balanced bins\"\ndf_resultat.at[df_resultat.index==7,\"Cross-validation\"]=np.mean(knn_scores['test_score'])\nprint('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced)))\nprint('knn by equidistant bins of element - mean test_score {:.3f}'.format(np.mean(knn_scores['test_score'])))","90e352bf":"knn_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n])\n# Fit estimator\nknn_confusion=knn_pipe.fit(X_fea_2_tr_balan, y_fea_2_tr_balan)\nknn_predict = knn_pipe.predict(X_fea_2_te_balan)","ae57f509":"confusion_matrix(y_fea_2_te_balan, knn_predict)","c0fc46da":"plot_confusion_matrix(knn_confusion, X_fea_2_te_balan, y_fea_2_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])","9a11a134":"df_resultat","70c0b0e4":"sns.barplot(x=df_resultat[\"Cross-validation\"],y=df_resultat['method'])\nplt.show()","de441503":"# Working on a copy\ncodiv_country_analyze = codiv_country.copy()\n\n# Creating categorical variables\ncodiv_country_analyze['Quarantine_cat'] = codiv_country_analyze['Quarantine'].notnull().astype(int)\ncodiv_country_analyze['Restrictions_cat'] = codiv_country_analyze['Restrictions'].notnull().astype(int)\ncodiv_country_analyze['Schools_cat'] = codiv_country_analyze['Schools'].notnull().astype(int)\n\n# \ncodiv_country_analyze=codiv_country_analyze.drop([\n    'Quarantine','Schools','Restrictions', # now categorical\n    'Country', # not helpful\n    'Total Deaths','Total Infected','Total Active','Total Recovered', \n    \"Total Deaths Log10\",\"Total Recovered Log10\",\"Total Active Log10\",\"Total Infected Log10\"\n], axis=1)\n\ncodiv_country_analyze.sample(10)","9d78e43f":"columns = ['method','Cross-validation']\nindex=range(8)\ndf_resultat = pd.DataFrame(index=index, columns=columns)","09d25dae":"codiv_country_analyze['Deaths Ratio'].describe()","52e1ee2e":"codiv_country_analyze['category_balanced'], bin_edges_balanced=pd.qcut(codiv_country_analyze['Deaths Ratio'], q=6,labels=False, retbins=True)\n#balanced bins deliver category for 0-5, euqidistant from 1-6, so i will add 1 to the category numbers\ncodiv_country_analyze['category_balanced'] = codiv_country_analyze['category_balanced'] +1","d176d5f0":"ylevel0_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][0]\nylevel1_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][1]\nylevel2_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][2]\nylevel3_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][3]\nylevel4_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][4]\nylevel5_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][5]\nylevel6_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][6]\nLevel1_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel0_balanced,ylevel1_balanced,)\nLevel2_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel1_balanced,ylevel2_balanced,)\nLevel3_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel2_balanced,ylevel3_balanced,)\nLevel4_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel3_balanced,ylevel4_balanced,)\nLevel5_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel4_balanced,ylevel5_balanced,)\nLevel6_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel5_balanced,ylevel6_balanced,)\nprint(\"Level1_balanced= \",Level1_balanced)\nprint(\"Level2_balanced= \",Level2_balanced)\nprint(\"Level3_balanced= \",Level3_balanced)\nprint(\"Level4_balanced= \",Level4_balanced)\nprint(\"Level5_balanced= \",Level5_balanced)\nprint(\"Level6_balanced= \",Level6_balanced)","f1ef7f09":"ymax=codiv_country_analyze['Deaths Ratio'].values.max()\nprint(ymax)\nymax=ymax+ymax*0.001\nprint(\"ymax = \",ymax)\nymin=(codiv_country_analyze['Deaths Ratio'].values.min())\nymin=ymin-ymin*0.001\nprint(\"ymin = \",ymin)\nyinterval=(ymax-ymin)\/6\nprint(ymin,ymax)\n# \nylevel0_equidistant = ymin\nylevel1_equidistant = ymin+yinterval*1\nylevel2_equidistant = ymin+yinterval*2\nylevel3_equidistant = ymin+yinterval*3\nylevel4_equidistant = ymin+yinterval*4\nylevel5_equidistant = ymin+yinterval*5\nylevel6_equidistant = ymin+yinterval*6\n\n#\nLevel1_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel0_equidistant,ylevel1_equidistant,)\nLevel2_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel1_equidistant,ylevel2_equidistant,)\nLevel3_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel2_equidistant,ylevel3_equidistant,)\nLevel4_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel3_equidistant,ylevel4_equidistant,)\nLevel5_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel4_equidistant,ylevel5_equidistant,)\nLevel6_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel5_equidistant,ylevel6_equidistant,)\ncodiv_country_analyze['category_equidistant']=np.digitize(codiv_country_analyze['Deaths Ratio'].values,\n                          bins=[ylevel0_equidistant,ylevel1_equidistant,ylevel2_equidistant,ylevel3_equidistant,ylevel4_equidistant,ylevel5_equidistant,ylevel6_equidistant])\nprint(\"Level1_equidistant= \",Level1_equidistant)\nprint(\"Level2_equidistant= \",Level2_equidistant)\nprint(\"Level3_equidistant= \",Level3_equidistant)\nprint(\"Level4_equidistant= \",Level4_equidistant)\nprint(\"Level5_equidistant= \",Level5_equidistant)\nprint(\"Level6_equidistant= \",Level6_equidistant)","b3a9eae4":"y_equidistant = codiv_country_analyze['category_equidistant']\ny_balanced = codiv_country_analyze['category_balanced']\nX = codiv_country_analyze.drop(['Deaths Ratio','category_balanced','category_equidistant'], axis=1) \nsns.countplot(y_balanced)","b2ec597a":"sns.countplot(y_equidistant)","1dc53d7d":"from sklearn.model_selection import train_test_split\nX_tr_equid, X_te_equid, y_tr_equid, y_te_equid = train_test_split(X, y_equidistant, test_size=0.15, random_state=1)\nprint('Train set equidistant:', X_tr_equid.shape, y_tr_equid.shape)\nprint('Testn set equidistant:', X_te_equid.shape, y_te_equid.shape)\nX_tr_balan, X_te_balan, y_tr_balan, y_te_balan = train_test_split(X, y_balanced, test_size=0.15, random_state=1)\nprint('Train set balanced:', X_tr_balan.shape, y_tr_balan.shape)\nprint('Testn set balanced:', X_te_balan.shape, y_te_balan.shape)","b15fd3b7":"sns.countplot(y_te_equid)","224dc59a":"sns.countplot(y_te_balan)","e158cbc6":"dummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X, y_equidistant)\ndummy_clf.predict(X)\nprint(\"scores baseline mostfrequent classifier equidistant = \", dummy_clf.score(X, y_equidistant))","021773f4":"df_resultat.at[df_resultat.index==0,\"method\"]=\"baseline_equidistant\"\nBaseline_training_scores = cross_validate(dummy_clf, X, y_equidistant, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==0,\"Cross-validation\"]=np.mean(Baseline_training_scores['test_score'])\nprint(\"crossvalidation baseline mostfrequent classifier equidistant = \", np.mean(Baseline_training_scores['test_score']) )","08263c44":"from sklearn.metrics import confusion_matrix\ndummy_clf= DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_tr_equid, y_tr_equid)\ny_pred_equid=dummy_clf.predict(X_te_equid)\nmatrix = confusion_matrix(y_true=y_te_equid, y_pred=y_pred_equid)\nprint(matrix)","661861fd":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(dummy_clf, X_te_equid, y_te_equid, cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])","81d54086":"dummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X, y_balanced)\ndummy_clf.predict(X)\nprint(\"training_scores baseline mostfrequent classifier balanced = \", dummy_clf.score(X, y_balanced))","70107fa3":"df_resultat.at[df_resultat.index==1,\"method\"]=\"baseline balanced bins\"\nBaseline_training_scores = cross_validate(dummy_clf, X, y_balanced, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==1,\"Cross-validation\"]=np.mean(Baseline_training_scores['test_score'])\nprint(\"crossvalidation baseline mostfrequent classifier balanced = \", np.mean(Baseline_training_scores['test_score']) )","3f24b690":"dummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_tr_balan, y_tr_balan)\ny_pred_balan=dummy_clf.predict(X_te_balan)","813748a0":"matrix = confusion_matrix(y_true=y_te_balan, y_pred=y_pred_balan)\nprint(matrix)","e9df20fa":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(dummy_clf, X_te_balan, y_te_balan, cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])","a2ec8ade":"depths=range(1,15)\nscoring='accuracy'\ncv_val_scores_list = []\ncv_val_scores_std = []\ncv_val_scores_mean = []\naccuracy_scores = []\nfor depth in depths:\n    DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)\n    DecisionTree.fit(X, y_equidistant)\n    cv_val_scores = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)\n    cv_val_scores_list.append(cv_val_scores)\n    cv_val_scores_mean.append(cv_val_scores.mean())\n    cv_val_scores_std.append(cv_val_scores.std())\n    accuracy_scores.append(DecisionTree.score(X, y_equidistant))\n    Text=\"depth %d, cv_val_scores_mean %f std %f score %f\"%(depth,cv_val_scores.mean(),cv_val_scores.std(),DecisionTree.score(X, y_equidistant))\n    print(Text)\ncv_val_scores_mean = np.array(cv_val_scores_mean)\ncv_val_scores_std = np.array(cv_val_scores_std)\naccuracy_scores = np.array(accuracy_scores)","ee498ab1":"print(\"The maximum is by depth = \",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)\nOptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1\n","740f755a":"fig, ax = plt.subplots(1,1, figsize=(15,5))\nax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\nax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\nylim = plt.ylim()\nax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)\ntitle=\"decision tree accurency\"\nax.set_title(title, fontsize=16)\nax.set_xlabel('Tree depth', fontsize=14)\nax.set_ylabel('accuracy', fontsize=14)\nax.set_xticks(depths)\nax.legend()","feb33005":"DecisionTree = DecisionTreeClassifier(\n    criterion='gini',  random_state=0, max_depth=OptDepth)\n# Fit decision tree\nDecisionTree.fit(X, y_equidistant)\nTree_scores = cross_validate(DecisionTree, X, y_equidistant, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==2,\"method\"]=\"decision-tree equidistant\"\ndf_resultat.at[df_resultat.index==2,\"Cross-validation\"]=np.mean(Tree_scores['test_score'])\n                                                              \nprint(\"score DecistionTree equidistant =\",DecisionTree.score(X, y_equidistant))\nprint(\"cross validation DecisionTree equidistant =\",np.mean(Tree_scores['test_score']))","e5e84d2a":"from sklearn.metrics import confusion_matrix\nDecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)\nDecisionTree_confusion.fit(X_tr_equid, y_tr_equid)\nconfusion_matrix(y_te_equid, DecisionTree_confusion.predict(X_te_equid))","6bfb5c26":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(DecisionTree_confusion, X_te_equid, y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])","25941c86":"os.environ[\"PATH\"] += os.pathsep + 'D:\/Program Files (x86)\/Graphviz2.38\/bin\/'\n# Export decision tree\ndot_data = export_graphviz(\n    DecisionTree, out_file=None,\n    feature_names=X.columns, class_names=[Level1_equidistant,Level2_equidistant, Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant],\n    filled=True, rounded=True, proportion=True   \n)\nimport graphviz\n\n# Display decision tree\ngraphviz.Source(dot_data)","1c8d812a":"depths=range(1,15)\nscoring='accuracy'\ncv_val_scores_list = []\ncv_val_scores_std = []\ncv_val_scores_mean = []\naccuracy_scores = []\nfor depth in depths:\n    DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)\n    DecisionTree.fit(X, y_balanced)\n    cv_val_scores = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring)\n    cv_val_scores_list.append(cv_val_scores)\n    cv_val_scores_mean.append(cv_val_scores.mean())\n    cv_val_scores_std.append(cv_val_scores.std())\n    accuracy_scores.append(DecisionTree.score(X, y_balanced))\n    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced))\n    print(Text)\ncv_val_scores_mean = np.array(cv_val_scores_mean)\ncv_val_scores_std = np.array(cv_val_scores_std)\naccuracy_scores = np.array(accuracy_scores)","95adfefe":"print(\"The maximum is by depth_balanced = \",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)\nOptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1","df714ca7":"fig, ax = plt.subplots(1,1, figsize=(15,5))\nax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\nax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\nylim = plt.ylim()\nax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)\ntitle=\"decision tree accurency - _balanced bins\"\nax.set_title(title, fontsize=16)\nax.set_xlabel('Tree depth', fontsize=14)\nax.set_ylabel('accuracy', fontsize=14)\nax.set_xticks(depths)\nax.legend()","4ac6d376":"DecisionTree = DecisionTreeClassifier(\n    criterion='gini',  random_state=0, max_depth=OptDepth)\n# Fit decision tree\nDecisionTree.fit(X, y_balanced)\n# Get score\nTree_scores = cross_validate(DecisionTree, X, y_balanced, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==3,\"method\"]=\"decision-tree _balanced bins\"\ndf_resultat.at[df_resultat.index==3,\"Cross-validation\"]=np.mean(Tree_scores['test_score'])\nprint(\"score DecisionTree _balanced bins =\",DecisionTree.score(X, y_balanced))\nprint(\"cross validation DecisionTree balanced bins=\",np.mean(Tree_scores['test_score']))","58ddb81f":"from sklearn.metrics import confusion_matrix\nDecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)\nDecisionTree_confusion.fit(X_tr_balan, y_tr_balan)\nconfusion_matrix(y_te_balan, DecisionTree_confusion.predict(X_te_balan))","0fef896f":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(DecisionTree_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])","6bae4709":"from sklearn.tree import export_graphviz\nimport os\nos.environ[\"PATH\"] += os.pathsep + 'D:\/Program Files (x86)\/Graphviz2.38\/bin\/'\n# Export decision tree\ndot_data = export_graphviz(\n    DecisionTree, out_file=None,\n    feature_names=X.columns, class_names=[Level1_balanced,Level2_balanced, Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced],\n    filled=True, rounded=True, proportion=True   \n)\nimport graphviz\n\n# Display decision tree\ngraphviz.Source(dot_data)","c54e59ab":"Estimators=range(100,2000,100)\nscoring='accuracy'\ncv_val_scores_list = []\ncv_val_scores_std = []\ncv_val_scores_mean = []\naccuracy_scores = []\nfor Estimator in Estimators:\n    forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)\n    forest.fit(X, y_equidistant)\n    cv_val_scores = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)\n    cv_val_scores_list.append(cv_val_scores)\n    cv_val_scores_mean.append(cv_val_scores.mean())\n    cv_val_scores_std.append(cv_val_scores.std())\n    accuracy_scores.append(forest.fit(X, y_equidistant).score(X, y_equidistant))\n    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))\n    print(Text)\ncv_val_scores_mean = np.array(cv_val_scores_mean)\ncv_val_scores_std = np.array(cv_val_scores_std)\naccuracy_scores = np.array(accuracy_scores)","35c3576b":"print(\"The maximum is by Estimator_balanced = \",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)\nOptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100","8ad1c810":"fig, ax = plt.subplots(1,1, figsize=(15,5))\nax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\nax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\nylim = plt.ylim()\nax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)\ntitle=\"Forest accurency - _balanced bins\"\nax.set_title(title, fontsize=16)\nax.set_xlabel('Forest Estimator', fontsize=14)\nax.set_ylabel('accuracy', fontsize=14)\nax.set_xticks(Estimators)\nax.legend()","1af2e78b":"# Build a forest and compute the feature importances\nforest = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\nforest.fit(X, y_equidistant)","e4b610fa":"# Mean test score of a 250x decision tree \nForest_scores = cross_validate(forest, X, y_equidistant, cv=cv_strategy)\nprint('Forest by equidistant interval score {:.3f}'.format(forest.score(X, y_equidistant)))\ndf_resultat.at[df_resultat.index==4,\"method\"]=\"Forest equidistant\"\ndf_resultat.at[df_resultat.index==4,\"Cross-validation\"]=np.mean(Forest_scores['test_score'])\nprint('Forest by equidistant interval cross validation {:.3f}'.format(np.mean(Forest_scores['test_score'])))","3fe63ee4":"forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\nforest_confusion.fit(X_tr_equid, y_tr_equid)\nconfusion_matrix(y_te_equid, forest_confusion.predict(X_te_equid))","1d0494c8":"plot_confusion_matrix(forest_confusion, X_te_equid,y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])","997a8281":"importances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]","eb259bb4":"# Print the feature ranking\nprint(\"Feature ranking by equidistand interval:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure(figsize=(20,10))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)\nplt.xlim([-1, X.shape[1]])\nplt.show()","53a6fa48":"Estimators=range(100,2000,100)\nscoring='accuracy'\ncv_val_scores_list = []\ncv_val_scores_std = []\ncv_val_scores_mean = []\naccuracy_scores = []\nfor Estimator in Estimators:\n    forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)\n    forest.fit(X, y_balanced)\n    cv_val_scores = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring)\n    cv_val_scores_list.append(cv_val_scores)\n    cv_val_scores_mean.append(cv_val_scores.mean())\n    cv_val_scores_std.append(cv_val_scores.std())\n    accuracy_scores.append(forest.fit(X, y_balanced).score(X, y_balanced))\n    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced))\n    print(Text)\ncv_val_scores_mean = np.array(cv_val_scores_mean)\ncv_val_scores_std = np.array(cv_val_scores_std)\naccuracy_scores = np.array(accuracy_scores)","839f16b0":"print(\"The maximum is by Estimator_balanced = \",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)\nOptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100","3d9afd03":"fig, ax = plt.subplots(1,1, figsize=(15,5))\nax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\nax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\nylim = plt.ylim()\nax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)\ntitle=\"Forest accurency - _balanced bins\"\nax.set_title(title, fontsize=16)\nax.set_xlabel('Forest Estimator', fontsize=14)\nax.set_ylabel('accuracy', fontsize=14)\nax.set_xticks(Estimators)\nax.legend()","3cc1fe41":"from sklearn.model_selection import cross_validate\nForest_scores = cross_validate(forest, X, y_balanced, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==5,\"method\"]=\"Forest balanced bins\"\ndf_resultat.at[df_resultat.index==5,\"Cross-validation\"]=np.mean(Forest_scores['test_score'])\nprint('Forest by balanced bins of element score {:.3f}'.format(forest.score(X, y_balanced)))\nprint('Forest by balanced bins of element - mean test {:.3f}'.format(np.mean(Forest_scores['test_score'])))","924ab37a":"forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\nforest_confusion.fit(X_tr_balan, y_tr_balan)\nconfusion_matrix(y_te_balan, forest_confusion.predict(X_te_balan))","9e716e71":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(forest_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])","0a642be1":"importances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]","99183b76":"# Print the feature ranking\nprint(\"Feature ranking by balanced bins:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure(figsize=(20,10))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)\nplt.xlim([-1, X.shape[1]])\nplt.show()","23466b3a":"pca = PCA(n_components=2)\n# Apply PCA\npca.fit(X, y=None); # Unsupervised learning, no y variable\nfeature_2= pca.transform(X)\nfeature_2_df=pd.DataFrame(feature_2)\nlen_cluster, col=feature_2_df.shape","0a917270":"X_fea_2_tr_equid, X_fea_2_te_equid, y_fea_2_tr_equid, y_fea_2_te_equid = train_test_split(feature_2, y_equidistant, test_size=0.15, random_state=1)\nprint('Train set equidistant:', X_fea_2_tr_equid.shape, y_fea_2_tr_equid.shape)\nprint('Testn set equidistant:', X_fea_2_te_equid.shape, y_fea_2_te_equid.shape)\nX_fea_2_tr_balan, X_fea_2_te_balan, y_fea_2_tr_balan, y_fea_2_te_balan = train_test_split(feature_2, y_balanced, test_size=0.15, random_state=1)\nprint('Train set balanced:', X_fea_2_tr_balan.shape, y_fea_2_tr_balan.shape)\nprint('Testn set balanced:', X_fea_2_te_balan.shape, y_fea_2_te_balan.shape)","2e56b68a":"neighbors = list(range(1,25))\nCV_scores = []\n\nfor n_neighborss in neighbors:\n    weights='distance'\n    feature1=pd.DataFrame(feature_2).loc[:][0]\n    feature2=pd.DataFrame(feature_2).loc[:][1]\n    h = .02  # step size in the mesh\n    # we create an instance of Neighbours Classifier and fit the data.\n    # Create a k-NN pipeline\n    knn_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights))\n    ])\n    # Fit estimator\n    knn_pipe.fit(feature_2, y_equidistant)\n    #\n    Z = knn_pipe.predict(feature_2)\n    scores = cross_val_score(knn_pipe,feature_2,y_equidistant,cv=cv_strategy )\n    CV_scores.append(scores.mean())\n    \nMissclassification_Error = [1-x for x in CV_scores]\nprint(\"the optimal k is \",format(Missclassification_Error.index(min(Missclassification_Error))))\nk_Optimal=Missclassification_Error.index(min(Missclassification_Error))\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nplt.plot( Missclassification_Error)\nplt.xlabel(\"number of neighbors K\")\nplt.ylabel(\"Missclassification Error\")\nplt.show()","867c64fa":"# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])\n\nfeature1=pd.DataFrame(feature_2).loc[:][0]\nfeature2=pd.DataFrame(feature_2).loc[:][1]\n\nweights='distance'\nh = .02  # step size in the mesh\n# we create an instance of Neighbours Classifier and fit the data.\n# Create a k-NN pipeline\nknn_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n])\n# Fit estimator\nknn_pipe.fit(feature_2, y_equidistant)\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nfeature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1\nfeature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1\nxx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))\nZ = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])\nZo = Z.reshape(xx.shape)","365b804e":"# Put the result into a color plot\nplt.figure(figsize=(18, 10))\nplt.pcolormesh(xx, yy, Zo, cmap=cmap_light)\n#Label1=pd.DataFrame(list(class_index.items()))\nLabel1=pd.DataFrame([1,2,3,4,5])\n# Plot also the training points\nplt.scatter(feature1, feature2, c=y_equidistant, cmap=cmap_bold)\ncbar = plt.colorbar() \ncbar.ax.set_yticklabels([Level1_equidistant,Level2_equidistant,Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant])\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xlabel(\"feature 1\")\nplt.ylabel(\"feature 2\")\nplt.title(\"6-Class classification,   equidistant bins -  total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))\n\nplt.show()","f7e0fc3e":"knn_scores = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==6,\"method\"]=\"knn equidistant bins\"\ndf_resultat.at[df_resultat.index==6,\"Cross-validation\"]=np.mean(knn_scores['test_score'])\nprint('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant)))\nprint('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score'])))\n","4807d90c":"knn_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n])\n# Fit estimator\nknn_confusion=knn_pipe.fit(X_fea_2_tr_equid, y_fea_2_tr_equid)\nknn_predict = knn_pipe.predict(X_fea_2_te_equid)\nconfusion_matrix(y_fea_2_te_equid, knn_predict)","4329a2cb":"plot_confusion_matrix(knn_confusion, X_fea_2_te_equid, y_fea_2_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])","4f57a043":"neighbors = list(range(1,25))\nCV_scores = []\n\nfor n_neighborss in neighbors:\n    weights='distance'\n    feature1=pd.DataFrame(feature_2).loc[:][0]\n    feature2=pd.DataFrame(feature_2).loc[:][1]\n    h = .02  # step size in the mesh\n    # we create an instance of Neighbours Classifier and fit the data.\n    # Create a k-NN pipeline\n    knn_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights))\n    ])\n    # Fit estimator\n    knn_pipe.fit(feature_2, y_balanced)\n    #\n    Z = knn_pipe.predict(feature_2)\n    scores = cross_val_score(knn_pipe,feature_2,y_balanced,cv=cv_strategy )\n    CV_scores.append(scores.mean())\n    \nMissclassification_Error = [1-x for x in CV_scores]\nprint(\"the optimal k is \",format(Missclassification_Error.index(min(Missclassification_Error))))\nk_Optimal=Missclassification_Error.index(min(Missclassification_Error))\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nplt.plot( Missclassification_Error)\nplt.xlabel(\"number of neighbors K\")\nplt.ylabel(\"Missclassification Error\")\nplt.show()","2419939d":"# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])\n\nfeature1=pd.DataFrame(feature_2).loc[:][0]\nfeature2=pd.DataFrame(feature_2).loc[:][1]\n\nweights='distance'\nh = .02  # step size in the mesh\n# we create an instance of Neighbours Classifier and fit the data.\n# Create a k-NN pipeline\nknn_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n])\n# Fit estimator\nknn_pipe.fit(feature_2, y_balanced)\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nfeature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1\nfeature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1\nxx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))\nZ = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])\nZo = Z.reshape(xx.shape)","49013756":"# Put the result into a color plot\nplt.figure(figsize=(18, 10))\nplt.pcolormesh(xx, yy, Zo, cmap=cmap_light)\n#Label1=pd.DataFrame(list(class_index.items()))\nLabel1=pd.DataFrame([1,2,3,4,5])\n# Plot also the training points\nplt.scatter(feature1, feature2, c=y_balanced, cmap=cmap_bold)\ncbar = plt.colorbar() \ncbar.ax.set_yticklabels([Level1_balanced,Level2_balanced,Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced])\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xlabel(\"feature 1\")\nplt.ylabel(\"feature 2\")\nplt.title(\"6-Class classification,   balanced bins -  total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))\n\nplt.show()","84475c0d":"\nknn_scores = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy)\ndf_resultat.at[df_resultat.index==7,\"method\"]=\"knn balanced bins\"\ndf_resultat.at[df_resultat.index==7,\"Cross-validation\"]=np.mean(knn_scores['test_score'])\nprint('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced)))\nprint('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score'])))\n","de5a4474":"knn_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n])\n# Fit estimator\nknn_confusion=knn_pipe.fit(X_fea_2_tr_balan, y_fea_2_tr_balan)\nknn_predict = knn_pipe.predict(X_fea_2_te_balan)","d90c218f":"confusion_matrix(y_fea_2_te_balan, knn_predict)","2307fb75":"plot_confusion_matrix(knn_confusion, X_fea_2_te_balan, y_fea_2_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])","d908160b":"df_resultat","6fde3521":"sns.barplot(x=df_resultat[\"Cross-validation\"],y=df_resultat['method'])\nplt.show()","7a848044":"codiv_time_confirmed","bd3435bb":"codiv_time_confirmed_grouped=codiv_time_confirmed.groupby('Country\/Region').sum().reset_index()\ndfsum=codiv_time_confirmed_grouped.agg(['sum'])\ndfsum['Country\/Region']=\"total_confirmed\"\ncodiv_time_confirmed_grouped_sum=codiv_time_confirmed_grouped.append(dfsum)\n#\ncodiv_time_recovered_grouped=codiv_time_recovered.groupby('Country\/Region').sum().reset_index()\ndfsum=codiv_time_recovered_grouped.agg(['sum'])\ndfsum['Country\/Region']=\"total_recovered\"\ncodiv_time_recovered_grouped_sum=codiv_time_recovered_grouped.append(dfsum)\n#\ncodiv_time_deaths_grouped=codiv_time_deaths.groupby('Country\/Region').sum().reset_index()\ndfsum=codiv_time_deaths_grouped.agg(['sum'])\ndfsum['Country\/Region']=\"total_deaths\"\ncodiv_time_deaths_grouped_sum=codiv_time_deaths_grouped.append(dfsum)\ncodiv_time_deaths_grouped_sum","818f42fc":"#Recovered\nStart=1\nfor countryInd in codiv_country_qurantine['Country']:\n    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n        if Start==1:\n            codiv_country_qurantine_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country\/Region']==countryInd]\n            Start=0\n        else:\n            codiv_country_qurantine_recovered=codiv_country_qurantine_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country\/Region']==countryInd])\n#\nStart=1\nfor countryInd in codiv_country_Restrictions['Country']:\n    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n        if Start==1:\n            codiv_country_Restrictions_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country\/Region']==countryInd]\n            Start=0\n        else:\n            codiv_country_Restrictions_recovered=codiv_country_Restrictions_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country\/Region']==countryInd])\n#\nStart=1\nfor countryInd in codiv_country_without_Restrictions_qurantine['Country']:\n    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n        if Start==1:\n            codiv_country_without_Restrictions_qurantine_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country\/Region']==countryInd]\n            Start=0\n        else:\n            codiv_country_without_Restrictions_qurantine_recovered=codiv_country_without_Restrictions_qurantine_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country\/Region']==countryInd]) \n#deaths   \nStart=1\nfor countryInd in codiv_country_qurantine['Country']:\n    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n        if Start==1:\n            codiv_country_qurantine_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country\/Region']==countryInd]\n            Start=0\n        else:\n            codiv_country_qurantine_deaths=codiv_country_qurantine_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country\/Region']==countryInd])\n#\nStart=1\nfor countryInd in codiv_country_Restrictions['Country']:\n    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n        if Start==1:\n            codiv_country_Restrictions_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country\/Region']==countryInd]\n            Start=0\n        else:\n            codiv_country_Restrictions_deaths=codiv_country_Restrictions_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country\/Region']==countryInd])\n#\nStart=1\nfor countryInd in codiv_country_without_Restrictions_qurantine['Country']:\n    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n        if Start==1:\n            codiv_country_without_Restrictions_qurantine_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country\/Region']==countryInd]\n            Start=0\n        else:\n            codiv_country_without_Restrictions_qurantine_deaths=codiv_country_without_Restrictions_qurantine_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country\/Region']==countryInd]) \n#confirmed\n\nStart=1\nfor countryInd in codiv_country_qurantine['Country']:\n    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n        if Start==1:\n            codiv_country_qurantine_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country\/Region']==countryInd]\n            Start=0\n        else:\n            codiv_country_qurantine_confirmed=codiv_country_qurantine_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country\/Region']==countryInd])\n\nStart=1\nfor countryInd in codiv_country_Restrictions['Country']:\n    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n        if Start==1:\n            codiv_country_Restrictions_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country\/Region']==countryInd]\n            Start=0\n        else:\n            codiv_country_Restrictions_confirmed=codiv_country_Restrictions_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country\/Region']==countryInd])\n\n#codiv_country_Restrictions_confirmed=codiv_country_Restrictions_confirmed.set_index('Country\/Region')\n#\nStart=1\nfor countryInd in codiv_country_without_Restrictions_qurantine['Country']:\n    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n        if Start==1:\n            codiv_country_without_Restrictions_qurantine_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country\/Region']==countryInd]\n            Start=0\n        else:\n            codiv_country_without_Restrictions_qurantine_confirmed=codiv_country_without_Restrictions_qurantine_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country\/Region']==countryInd]) \n\n","4226df01":"codiv_country_without_Restrictions_qurantine_active= codiv_country_without_Restrictions_qurantine_confirmed.set_index('Country\/Region')-codiv_country_without_Restrictions_qurantine_deaths.set_index('Country\/Region')-codiv_country_without_Restrictions_qurantine_recovered.set_index('Country\/Region')\ncodiv_country_qurantine_active= codiv_country_qurantine_confirmed.set_index('Country\/Region')-codiv_country_qurantine_deaths.set_index('Country\/Region')-codiv_country_qurantine_recovered.set_index('Country\/Region')\ncodiv_country_Restrictions_active= codiv_country_Restrictions_confirmed.set_index('Country\/Region')- codiv_country_Restrictions_deaths.set_index('Country\/Region')-codiv_country_Restrictions_recovered.set_index('Country\/Region')","485ef1dc":"import datetime\nfrom matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n                               AutoMinorLocator)\nrow, col= codiv_country_qurantine_confirmed.shape\nfig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))\nts = pd.Series(\n    np.random.randn(col-1),\n    index=pd.date_range('20\/1\/2020', periods=col-1))\ndf_qua_conf = pd.DataFrame((np.log1p(codiv_country_qurantine_confirmed.set_index('Country\/Region').T).to_numpy()),\n    index=ts.index,\n    columns=list(codiv_country_qurantine_confirmed.set_index('Country\/Region').index))\ndf_res_conf = pd.DataFrame((np.log1p(codiv_country_Restrictions_confirmed.set_index('Country\/Region').T).to_numpy()),\n    index=ts.index,\n    columns=list(codiv_country_Restrictions_confirmed.set_index('Country\/Region').index))\ndf_whi_conf = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_confirmed.set_index('Country\/Region').T).to_numpy()),\n    index=ts.index,\n    columns=list(codiv_country_without_Restrictions_qurantine_confirmed.set_index('Country\/Region').index))\n\nax0.plot(df_qua_conf.index,df_qua_conf)\nax1.plot(df_res_conf.index, df_res_conf)\nax2.plot(df_whi_conf.index, df_whi_conf)\nax0.set_title(\"quarantine_confirmed\")\nax1.set_title(\"Restrictions_confirmed\")\nax2.set_title(\"without_Restrictions_quarantine_confirmed\")\nax0.legend(codiv_country_qurantine_confirmed['Country\/Region'].tolist(),loc='upper left')\nax1.legend(codiv_country_Restrictions_confirmed['Country\/Region'].tolist(),loc='upper left')\nax2.legend(codiv_country_without_Restrictions_qurantine_confirmed['Country\/Region'].tolist(),loc='upper left')\nax0.set_ylabel(\"quarantine_confirmed\")\nax1.set_ylabel(\"Restrictions_confirmed\")\nax2.set_ylabel(\"without_Restrictions_quarantine_confirmed\")\n\nax0.grid(True)\nax0.xaxis.set_minor_locator(AutoMinorLocator())\nax1.grid(True)\nax1.xaxis.set_minor_locator(AutoMinorLocator())\nax2.grid(True)\nax2.xaxis.set_minor_locator(AutoMinorLocator())\nax0.tick_params(axis=\"x\", rotation=45)\nax1.tick_params(axis=\"x\", rotation=45)\nax2.tick_params(axis=\"x\", rotation=45)\n####\nfor countryindex in codiv_country_qurantine_confirmed['Country\/Region']:\n    if not codiv_country[codiv_country['Country']==countryindex].empty :\n        if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=\"2000-01-01\" and countryindex!=\"New Zealand\":\n            QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]\n            QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), \"%m\/%d\/%Y\")\n            Centervalue=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]\n            ax0.annotate('Quarantine', (QuarantineTimeD, Centervalue),\n                xytext=(0.2, 0.95), textcoords='axes fraction',\n                arrowprops=dict(facecolor='red', shrink=0.001),\n                fontsize=16,\n                horizontalalignment='right', verticalalignment='top')\n####\nfor countryindex in codiv_country_Restrictions_confirmed['Country\/Region']:\n    if not codiv_country[codiv_country['Country']==countryindex].empty :\n        if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=\"2000-01-01\":\n            RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]\n            RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), \"%m\/%d\/%Y\")\n            Centervalue=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]\n            ax1.annotate('Restrictions', (RestrictionsTimeD, Centervalue),\n                xytext=(0.2, 0.95), textcoords='axes fraction',\n                arrowprops=dict(facecolor='red', shrink=0.001),\n                fontsize=16,\n                horizontalalignment='right', verticalalignment='top')","88644a40":"import datetime\nfrom matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n                               AutoMinorLocator)\nrow, col= codiv_country_qurantine_deaths.shape\nfig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))\nts = pd.Series(\n    np.random.randn(col-1),\n    index=pd.date_range('20\/1\/2020', periods=col-1))\ndf_qua_conf = pd.DataFrame((np.log1p(codiv_country_qurantine_deaths.set_index('Country\/Region').T).to_numpy()),\n    index=ts.index,\n    columns=list(codiv_country_qurantine_deaths.set_index('Country\/Region').index))\ndf_res_conf = pd.DataFrame((np.log1p(codiv_country_Restrictions_deaths.set_index('Country\/Region').T).to_numpy()),\n    index=ts.index,\n    columns=list(codiv_country_Restrictions_deaths.set_index('Country\/Region').index))\ndf_whi_conf = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_deaths.set_index('Country\/Region').T).to_numpy()),\n    index=ts.index,\n    columns=list(codiv_country_without_Restrictions_qurantine_deaths.set_index('Country\/Region').index))\n\nax0.plot(df_qua_conf.index,df_qua_conf)\nax1.plot(df_res_conf.index, df_res_conf)\nax2.plot(df_whi_conf.index, df_whi_conf)\nax0.set_title(\"quarantine_deaths\")\nax1.set_title(\"Restrictions_deaths\")\nax2.set_title(\"without_Restrictions_quarantine_deaths\")\nax0.legend(codiv_country_qurantine_deaths['Country\/Region'].tolist(),loc='upper left')\nax1.legend(codiv_country_Restrictions_deaths['Country\/Region'].tolist(),loc='upper left')\nax2.legend(codiv_country_without_Restrictions_qurantine_deaths['Country\/Region'].tolist(),loc='upper left')\nax0.set_ylabel(\"quarantine_deaths\")\nax1.set_ylabel(\"Restrictions_deaths\")\nax2.set_ylabel(\"without_Restrictions_quarantine_deaths\")\n\nax0.grid(True)\nax0.xaxis.set_minor_locator(AutoMinorLocator())\nax1.grid(True)\nax1.xaxis.set_minor_locator(AutoMinorLocator())\nax2.grid(True)\nax2.xaxis.set_minor_locator(AutoMinorLocator())\nax0.tick_params(axis=\"x\", rotation=45)\nax1.tick_params(axis=\"x\", rotation=45)\nax2.tick_params(axis=\"x\", rotation=45)\n####\nfor countryindex in codiv_country_qurantine_deaths['Country\/Region']:\n    if not codiv_country[codiv_country['Country']==countryindex].empty :\n        if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=\"2000-01-01\" and countryindex!=\"New Zealand\":\n            QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]\n            QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), \"%m\/%d\/%Y\")\n            Centervalue=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]\n            ax0.annotate('Quarantine', (QuarantineTimeD, Centervalue),\n                xytext=(0.2, 0.95), textcoords='axes fraction',\n                arrowprops=dict(facecolor='red', shrink=0.001),\n                fontsize=16,\n                horizontalalignment='right', verticalalignment='top')\n####\nfor countryindex in codiv_country_Restrictions_deaths['Country\/Region']:\n    if not codiv_country[codiv_country['Country']==countryindex].empty :\n        if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=\"2000-01-01\":\n            RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]\n            RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), \"%m\/%d\/%Y\")\n            Centervalue=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]\n            ax1.annotate('Restrictions', (RestrictionsTimeD, Centervalue),\n                xytext=(0.2, 0.95), textcoords='axes fraction',\n                arrowprops=dict(facecolor='red', shrink=0.001),\n                fontsize=16,\n                horizontalalignment='right', verticalalignment='top')","05d0a8c0":"import datetime\nfrom matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n                               AutoMinorLocator)\nrow, col= codiv_country_qurantine_active.shape\nfig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))\nts = pd.Series(\n    np.random.randn(col),\n    index=pd.date_range('20\/1\/2020', periods=col))\ndf_qua_conf = pd.DataFrame((np.log1p(codiv_country_qurantine_active.T).to_numpy()),\n    index=ts.index,\n    columns=list(codiv_country_qurantine_active.index))\ndf_res_conf = pd.DataFrame((np.log1p(codiv_country_Restrictions_active.T).to_numpy()),\n    index=ts.index,\n    columns=list(codiv_country_Restrictions_active.index))\ndf_whi_conf = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_active.T).to_numpy()),\n    index=ts.index,\n    columns=list(codiv_country_without_Restrictions_qurantine_active.index))\n\nax0.plot(df_qua_conf.index,df_qua_conf)\nax1.plot(df_res_conf.index, df_res_conf)\nax2.plot(df_whi_conf.index, df_whi_conf)\nax0.set_title(\"quarantine_active\")\nax1.set_title(\"Restrictions_active\")\nax2.set_title(\"without_Restrictions_quarantine_active\")\nax0.legend(codiv_country_qurantine_deaths['Country\/Region'].tolist(),loc='upper left')\nax1.legend(codiv_country_Restrictions_deaths['Country\/Region'].tolist(),loc='upper left')\nax2.legend(codiv_country_without_Restrictions_qurantine_deaths['Country\/Region'].tolist(),loc='upper left')\nax0.set_ylabel(\"quarantine_active\")\nax1.set_ylabel(\"Restrictions_active\")\nax2.set_ylabel(\"without_Restrictions_qurantine_active\")\n\nax0.grid(True)\nax0.xaxis.set_minor_locator(AutoMinorLocator())\nax1.grid(True)\nax1.xaxis.set_minor_locator(AutoMinorLocator())\nax2.grid(True)\nax2.xaxis.set_minor_locator(AutoMinorLocator())\nax0.tick_params(axis=\"x\", rotation=45)\nax1.tick_params(axis=\"x\", rotation=45)\nax2.tick_params(axis=\"x\", rotation=45)\n####\nfor countryindex in codiv_country_qurantine_deaths['Country\/Region']:\n    if not codiv_country[codiv_country['Country']==countryindex].empty :\n        if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=\"2000-01-01\" and countryindex!=\"New Zealand\":\n            QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]\n            QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), \"%m\/%d\/%Y\")\n            Centervalue=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]\n            ax0.annotate('Quarantine', (QuarantineTimeD, Centervalue),\n                xytext=(0.2, 0.95), textcoords='axes fraction',\n                arrowprops=dict(facecolor='red', shrink=0.001),\n                fontsize=16,\n                horizontalalignment='right', verticalalignment='top')\n####\nfor countryindex in codiv_country_Restrictions_deaths['Country\/Region']:\n    if not codiv_country[codiv_country['Country']==countryindex].empty :\n        if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=\"2000-01-01\":\n            RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]\n            RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), \"%m\/%d\/%Y\")\n            Centervalue=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]\n            ax1.annotate('Restrictions', (RestrictionsTimeD, Centervalue),\n                xytext=(0.2, 0.95), textcoords='axes fraction',\n                arrowprops=dict(facecolor='red', shrink=0.001),\n                fontsize=16,\n                horizontalalignment='right', verticalalignment='top')","e7785ae5":"from IPython.display import Image\nImage(filename=os.path.join('.\/\/', 'shift.JPG'))","8f60528b":"# evalue the stadt date of the stats\nStartDate=codiv_country_Restrictions_active.T.index[0]\nprint(\" Epidemy StartDate = \",StartDate)\ncolumn_names = [\"date\"]\ndfdate = pd.DataFrame(columns = column_names)\nfor Dateindex in range(20):\n    df_date1 = pd.DataFrame ([[str(pd.Timestamp(StartDate,unit=\"D\")+ pd.Timedelta(days=Dateindex))]], columns = [\"date\"])\n    dfdate=dfdate.append(df_date1, ignore_index = True)\n","635ae88e":"# Root mean squared error (RMSE)\ndef rmse(y, y_pred):\n    return np.sqrt(np.mean(np.square(y - y_pred)))","aa4a43e7":"import scipy.stats as stats\nimport datetime\ndef Polifq(Country,Version=\"restrictions\",shift=0,trigger=40):\n    # Version= works for restrictions, quarantin and without_Restrictions_qurantine.\n    if Version==\"restrictions\":\n        df_codiv=codiv_country_Restrictions_active.T[Country]\n    else:\n        if Version==\"quarantine\":\n            df_codiv=codiv_country_qurantine_active.T[Country]\n        else:\n            df_codiv=codiv_country_without_Restrictions_qurantine_active.T[Country]\n    #######################################\n    ######################## parameter setup\n    StartCountryIndex=0\n    ShiftIndex=0\n    x_tr1_China_model=0\n    y_tr1_China_model=0\n    highfactor=1.0\n    LargeFactor=1.0\n    ##############################################################################\n    ####################################### fitting the Country ##################\n    ######################## filter outlier for the Country\n    filter0 = np.abs(df_codiv - df_codiv.mean()) > (3 * df_codiv.std())\n    outliers = df_codiv.loc[filter0]\n    df_codiv = df_codiv.drop(outliers.index, axis=0)\n    ###########\n    # max actual value of the country\n    maxCountry=max(df_codiv)\n    #######################################\n    ########### Build the dataframe for the country\n    df_country = pd.DataFrame(columns = [Country,\"Value\",\"time\"], dtype='int')\n    for Index in range(0,len(df_codiv)):\n        df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [Country,\"Value\",\"time\"])\n        df_country=df_country.append(df_codiv1, ignore_index = True)\n    #######################################\n    #################### polyfit 10gr for the country value\n    x_tr1_Country=df_country[\"Value\"]\n    y_tr1_Country=df_country[Country]\n    #\n    # * Polyfit with degree 10\n    coefs_Country_poly10 = np.polyfit(x_tr1_Country, y_tr1_Country, deg=10) # Fit to train data\n    ##############################################################################\n    ##############################################################################\n    coefs_predict_poly10=0\n    #######################################\n    ############### Now shifting the china model to fitt to the country\n    if shift==1:\n        Start=1\n        ####### for the country\n        maxChina=max(codiv_country_qurantine_active.T[\"China\"])\n        for Index in range(0,len(df_codiv)):\n            #look for the start position of the country epidemy, it should be higher then the trigger value, it will be setup only by start of procedure ( STart=1)\n            if df_codiv[Index]>trigger*(maxCountry*1.0\/maxChina) and Start==1 and shift==1:\n                \n                if maxCountry>maxChina:\n                    trigger=trigger*(maxCountry*1.5\/maxChina)\n                StartCountryIndex=Index\n                Start=0\n            # register the index of the coutry max position\n            if df_codiv[Index]==maxCountry:\n                maxCountryIndex=Index\n        # deliver the actual distance between start and maximum for the country\n        LargeCountry=maxCountryIndex-StartCountryIndex\n        #################################\n        #######################################\n        ####### for china\n        df_codiv_China_ref=codiv_country_qurantine_active.T[\"China\"]\n        #######################################\n        ###### outiler\n        filter0 = np.abs(df_codiv_China_ref - df_codiv_China_ref.mean()) > (3 * df_codiv_China_ref.std())\n        outliers = df_codiv_China_ref.loc[filter0]\n        df_codiv = df_codiv_China_ref.drop(outliers.index, axis=0)\n        #######################################\n        ###### check for the start point by 40 infected\n        maxChina=max(df_codiv_China_ref)\n        Start=1\n        for Index in range(0,len(df_codiv_China_ref)):\n            if df_codiv_China_ref[Index]>60 and Start==1 and shift==1:\n                StartChinaIndex=Index\n                Start=0\n            if df_codiv_China_ref[Index]==maxChina:\n                maxChinaIndex=Index\n        #######################################\n        ###### start value for the fitting\n        highfactor=maxCountry*1.0\/maxChina\n        LargeChina=maxChinaIndex-StartChinaIndex\n        LargeFactor=LargeCountry*1.0\/LargeChina\n        ShiftIndex=StartCountryIndex-StartChinaIndex\n        ##########OPTIMISATION \n        step=0.02\n        ############# the target\n        # Predictions with the current a,b values\n        ################# generate the function country is china\n        df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n        for Index in range(0,len(df_codiv)):\n            df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n            df_country=df_country.append(df_codiv1, ignore_index = True)\n        x_tr1_China_model=df_country[\"Value\"]\n        y_tr1_China_model=df_country[\"China\"]      \n        error0 = rmse(y_tr1_China_model,y_tr1_Country)\n        error1=0\n        for runNr in range(1,60,1):\n            if error1<error0:\n                LargeFactor=LargeFactor+step\n                error0=error1\n                ################# generate the function\n                df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n                for Index in range(0,len(df_codiv)):\n                    df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n                    df_country=df_country.append(df_codiv1, ignore_index = True)\n                x_tr1_China_model=df_country[\"Value\"]\n                y_tr1_China_model=df_country[\"China\"] \n                error1 = rmse(y_tr1_China_model,y_tr1_Country)\n            if error1<error0:\n                LargeFactor=LargeFactor+step\n                error0=error1\n                ################# generate the function\n                df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n                for Index in range(0,len(df_codiv)):\n                    df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n                    df_country=df_country.append(df_codiv1, ignore_index = True)\n                x_tr1_China_model=df_country[\"Value\"]\n                y_tr1_China_model=df_country[\"China\"] \n                error1 = rmse(y_tr1_China_model,y_tr1_Country)\n            ##############\n            if error1<error0:\n                highfactor=highfactor+step\n                error0=error1\n                ################# generate the function\n                df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n                for Index in range(0,len(df_codiv)):\n                    df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n                    df_country=df_country.append(df_codiv1, ignore_index = True)\n                x_tr1_China_model=df_country[\"Value\"]\n                y_tr1_China_model=df_country[\"China\"] \n                error1 = rmse(y_tr1_China_model,y_tr1_Country)\n            #################\n            if error1<error0:\n                highfactor=highfactor+step\n                error0=error1\n                ################# generate the function\n                df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n                for Index in range(0,len(df_codiv)):\n                    df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n                    df_country=df_country.append(df_codiv1, ignore_index = True)\n                x_tr1_China_model=df_country[\"Value\"]\n                y_tr1_China_model=df_country[\"China\"] \n                error1 = rmse(y_tr1_China_model,y_tr1_Country)\n        #after having this fitting, we can fit the modified china data  on a 10grade polynome\n        #\n        # * Polyfit with degree 10\n        coefs_predict_poly10 = np.polyfit(x_tr1_China_model, y_tr1_China_model, deg=10) # Fit to train data\n    return coefs_Country_poly10,x_tr1_Country,y_tr1_Country,coefs_predict_poly10,x_tr1_China_model,y_tr1_China_model,StartCountryIndex","220c493c":"# how much data are available for a country\nnumber,lendata=codiv_country_Restrictions_active.shape\nx_values = np.linspace(0, lendata+40, num=lendata+30)","8a07c455":"#compute the ymax china\ncoefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(\"China\",shift=0,Version=\"quarantine\",trigger=70)\nlendataChina=max(x_tr_China)-7\ny_China = np.polyval(coefs_China, x_values)\nymaxchina=int(max(y_tr_China)*1.5)","ff6f14cc":"StartDate=codiv_country_Restrictions_active.T.index[0]\ncolumn_names = [\"date\"]\ndfdate = pd.DataFrame(columns = column_names)\nfor Dateindex in range(0,lendata*20,10):\n    df_date1 = pd.DataFrame ([[str(pd.Timestamp(StartDate,unit=\"D\")+ pd.Timedelta(days=Dateindex))]], columns = [\"date\"])\n    dfdate=dfdate.append(df_date1, ignore_index = True)","320582d5":"for countryindex in codiv_country_qurantine_active.T:\n#codiv_country_quarantine_active.T:\n    if countryindex==\"China\":\n        coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=0,Version=\"quarantine\",trigger=70)\n        y_Country = np.polyval(coefs_Country, x_values)\n        ymax=0\n    else:\n        coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=\"quarantine\",trigger=170)\n        y_Country = np.polyval(coefs_Country, x_values)\n        y_Country_prev = np.polyval(coefs_Country_prev, x_values)\n        #adjust the ymax of the graphic\n        ymax=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40)\n    if ymaxchina>ymax:\n        ymax=ymaxchina\n    #\n    fig = plt.figure(figsize= (18, 10))\n    #china reference country\n    plt.scatter(x_tr_China, y_tr_China, s=10)\n    plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=\"blue\")\n    #plot the second country\n    if countryindex!=\"China\":\n        plt.scatter(x_tr_Country, y_tr_Country, s=10)\n        plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=\"magenta\")\n        plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=\"black\")\n        PredictionName='%s-prediction'%(countryindex,)\n        plt.plot(x_values, y_Country_prev, label=PredictionName,c=\"brown\",ls='dashed')\n    plt.ylim(-5,ymax)\n    plt.ylabel(\"Number\")\n    plt.grid(True)\n    tickvalues = range(0,len(dfdate),10)\n    plt.xticks(ticks = tickvalues ,labels = dfdate[\"date\"], rotation = 75)\n    if countryindex!=\"China\":\n        Title='%s-prediction - Quarantine'%(countryindex,)\n        plt.title(Title)\n    else:\n        plt.title(\"China\")\n    Texte=\"\"\n    if countryindex==\"Belgium\":\n        Texte=\"Belgium only success to slow the infection. The model wait for the peak to fit better\"\n    if countryindex==\"France\":\n        Texte=\"France was relaxing the quarantine? the model need the next days data to fit better. the datas are not clear\"      \n    if countryindex==\"Germany\" or countryindex==\"Austria\":\n        Texte=\"Germany was relaxing the quarantine. but the model fit mostly good\"      \n    if countryindex==\"India\" or countryindex==\"Argentina\" or countryindex==\"Peru\" or countryindex==\"Colombia\":\n        Texte=\" The model wait for the peak to fit better\"       \n    if countryindex==\"Spain\":\n        Texte=\" Spain had no clear data, the model need the next days data to fit better\"  \n    if countryindex==\"China\":\n        Texte=\" China is the reference country, but we see they have to face to some epidemic restart ( imported case).\"  \n    plt.text(1, ymax-10000, Texte, fontsize=15)\n    plt.legend()\n    plt.show()","afdd258a":"number,lendata=codiv_country_Restrictions_active.shape\nx_values = np.linspace(0, lendata+40, num=lendata+30)\ncoefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(\"China\",shift=0,Version=\"quarantine\",trigger=70)\nlendataChina=max(x_tr_China)-7\ny_China = np.polyval(coefs_China, x_values)\nymaxchina=int(max(y_tr_China)*1.5)","70549dff":"for countryindex in codiv_country_Restrictions_active.T:\n#codiv_country_quarantine_active.T:\n    # China is only in quarantine group\n    if countryindex==\"Japan\":\n        trigger=4500\n    else:\n        trigger=250\n    coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=\"restrictions\",trigger=trigger)\n    y_Country = np.polyval(coefs_Country, x_values)\n    y_Country_prev = np.polyval(coefs_Country_prev, x_values)\n    ymax=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40)\n    #adjust the ymax limit\n    if ymaxchina>ymax:\n        ymax=ymaxchina\n    #\n    fig = plt.figure(figsize= (18, 10))\n    plt.scatter(x_tr_China, y_tr_China, s=10)\n    plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=\"blue\")\n    if countryindex!=\"China\":\n        plt.scatter(x_tr_Country, y_tr_Country, s=10)\n        plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=\"magenta\")\n        plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=\"black\")\n        PredictionName='%s-prediction'%(countryindex,)\n        plt.plot(x_values, y_Country_prev, label=PredictionName,c=\"brown\",ls='dashed')\n    plt.ylim(-5,ymax)\n    plt.ylabel(\"Number\")\n    plt.grid(True)\n    tickvalues = range(0,len(dfdate),10)\n    plt.xticks(ticks = tickvalues ,labels = dfdate[\"date\"], rotation = 75)\n    if countryindex!=\"China\":\n        Title='%s-prediction - Restrictions'%(countryindex,)\n        plt.title(Title)\n    else:\n        plt.title(\"China\")\n    ##\n    Texte=\"\"\n    if countryindex==\"Ireland\":\n        Texte=\"Ireland had no clear data, the model need the next days data to fit better. Are the information trustfull?\"\n    if countryindex==\"Japan\":\n        Texte=\"Japan tried long time to contain the epidemy, after it gone up. the start threshold should be higher\" \n    if countryindex==\"Canada\" or countryindex==\"Netherlands\" or countryindex==\"Peru\" or countryindex==\"Colombia\"or countryindex==\"Portugal\"or countryindex==\"Sweden\"or countryindex==\"United Kingdom\":\n        Texte=\" The model wait for the peak to fit better\"     \n    if countryindex==\"Norway\" or countryindex==\"Poland\":\n        Texte=\" The model wait for the peak to fit better,they take long time to reach it\" \n    plt.text(1, ymax-10000, Texte, fontsize=15)\n    ##\n    plt.legend()\n    plt.show()","07eb452c":"number,lendata=codiv_country_Restrictions_active.shape\nx_values = np.linspace(0, lendata+40, num=lendata+30)\ncoefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(\"China\",shift=0,Version=\"quarantine\",trigger=70)\nlendataChina=max(x_tr_China)-7\ny_China = np.polyval(coefs_China, x_values)\nymaxchina=int(max(y_tr_China)*1.5)","98a7873a":"for countryindex in codiv_country_without_Restrictions_qurantine_active.T:\n#codiv_country_quarantine_active.T:\n    # china is only in quarntine group\n    if countryindex==\"Singapore\" or countryindex==\"Qatar\" or countryindex==\"Kuwait\":\n        trigger=2000\n    else:\n        trigger=300\n    coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=\"NoQuaNoRES\",trigger=trigger)\n    y_Country = np.polyval(coefs_Country, x_values)\n    y_Country_prev = np.polyval(coefs_Country_prev, x_values)\n    # adjust the ymax graph limit\n    if countryindex==\"US\":\n        ymax=1500000\n    else:\n        if countryindex==\"Brazil\":\n            ymax=180000\n        else:\n            ymax=60000\n    fig = plt.figure(figsize= (18, 10))\n    plt.scatter(x_tr_China, y_tr_China, s=10)\n    plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=\"blue\")\n    if countryindex!=\"China\":\n        plt.scatter(x_tr_Country, y_tr_Country, s=10)\n        plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=\"magenta\")\n        plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=\"black\")\n        PredictionName='%s-prediction'%(countryindex,)\n        plt.plot(x_values, y_Country_prev, label=PredictionName,c=\"brown\",ls='dashed')\n    plt.ylim(-5,ymax)\n    plt.ylabel(\"Number\")\n    plt.grid(True)\n    tickvalues = range(0,len(dfdate),10)\n    plt.xticks(ticks = tickvalues ,labels = dfdate[\"date\"], rotation = 75)\n    if countryindex!=\"China\":\n        Title='%s-prediction - No-Restrictions No-quarantine'%(countryindex,)\n        plt.title(Title)\n    else:\n        plt.title(\"China\")\n    ##\n    Texte=\"\"\n    if countryindex==\"Armenia\" or countryindex==\"Bahrain\" or countryindex==\"Bangladesh\" or countryindex==\"Belarus\"or countryindex==\"Brazil\"or countryindex==\"Chile\"or countryindex==\"United Kingdom\"or countryindex==\"US\":\n        Texte=\" The model wait for the peak to fit better\"  \n    if countryindex==\"Ecuador\" or countryindex==\"Indonesia\" or countryindex==\"Kuwait\" or countryindex==\"Mexico\" or countryindex==\"Oman\" or countryindex==\"Pakistan\" or countryindex==\"Quatar\" or countryindex==\"South Africa\":\n        Texte=\" The model wait for the peak to fit better\"\n    if countryindex==\"Dominican Republic\" or countryindex==\"Ghana\":\n        Texte=\" The model wait for more data to fit better\"\n    plt.text(1, ymax-10000, Texte, fontsize=15)\n    plt.legend()\n    plt.show()","34e0090f":"---\nFeature reduction to 2 and kneighbour classifier -  - Deaths Ratio feature\n---\nfeature reduction","5fc49dd2":"Summary  - Deaths Ratio feature\n---","443ef9be":"---\nsummary - Total Infected log10 feature\n---","532b0c47":"Create the total infected, deaths, recovered and infected columns for the `codiv_country` DataFrame. I however have to check that the country is in the `codiv_csse` DataFrame which will not be used anymore later in the analysis. I will create this column with the following fomula\n\n$$\\text{Total Active} = \\text{Confirmed} - \\text{Deaths} - \\text{Recovered}$$","ea1d8828":"Addind a new information : Health expenditure\/GDP in %\n---\nHealth expenditure (% of GDP) : https:\/\/data.worldbank.org\/indicator\/SH.XPD.CHEX.GD.ZS\nthe file dont have informations for 2018-2019, so i will use the 2017 data","5bec5c47":"equidistant bins","4beea348":"Read actual Data \n---","fd174aa2":"confusion matrix  with split data","f7b58e00":"Create equidistant and balanced categories for the all 4 models\n---\n\ni think there is 2 different approche from the categories : \n- equidistant category\n- category with same number of elements (_balanced)\nwe have a log10 number tables, classical outliers are here value full elements like US,  if i want to know why element are making that a country go high, i dont think that i can mix ihm with lower avlue country, because he will push others with wrong informations and other elements f the category will weak his importance\n\nbut i will try both for every models\n\n.define bin_balanced same number in each category","5dbb5bca":"the highest class in test datas are in class 4, 6\/15 he recognized good 0.40\n\nBaseline  most frequent - balanced bins - Total Infected log10 feature\n---","05aad668":"confusion matrix  with split data","65b52025":"---\nRandom forest - Balanced bins - Total Infected log10 feature\n---","fe4e3631":"Balanced bins","3dcdfaa6":"Check data types","55a1ab1f":"---\nPart 4 : Fitting the data to a model and predictions\n---\n\nexplanations:\n---\nI dont target the total number of infected, but only the active number of infected. \nThe goal is to predict when we will arrive under a low level of infected.\nOnly active infected represent a danger, not the recovered persone.\n```\n```\nfirst i will fit the China active  courve to a model\nThe epidemic curve is a standard epidemical curve.\nAfter i will transpose theis epidemic model to the other countries.\nFor the transposition it will be 3 fitting parameter : 1. start point 2. The high point 3. The large \nThe Model can not absorbe the suddently adjustment of measuring method of a country, but global it fitt very well","d9086d32":"---\nDecision Tree - equidistant bins - Total Infected log10 feature\n---","822249c9":"There seems to be a correlation between \"Health Expenditure Ratio\" and Deaths\/Infected which is hard to explain. Maybe it comes from other variables that are not part of the data. (confounding factors)","f866ed76":"---\nRandom forests - equidistant bins - Total Infected log10 feature\n---\n\nI will use the same methodology as above. I use 250 for the number of `n_estimators` as this seems to be a good default value in practice from my research on online forums.\n","6c73b6c7":"For the **factor analysis**, I'm using the decision tree model to analyze which factors are the most important. For the random forest, I'm plotting the feature importance.For the KNeighbors am also plotting the decision boundaries for each class. I compare all models to the \"most-frequent\" baseline to know how much information are captured by them. I'm using those models because they can be easily interpreted.\n\u200b\nThe input data will be the external factors and the target are the total number of infected and the deaths ratio. To make this a classification problem, I'm creating 6 categories from low to high and explore two manners to achieve this\n\u200b\n- Equidistant bins\n- \"Quantile bins\" to have a balanced target\n\u200b\nSince the number of data points is low, I always report the cross-validated mean test score on the entire data set and only split the data into train\/test sets for reporting purpose with the classification matrix.\n\u200b\nFor the **epidemic modelling**, I'm fitting the values from the China epidemic to other countries. To achieve this, I'm finding the epidemic peak and duration from start to peak, and fitting this to the other countries peak and duration using RMSE as a metric. To model the evolution of the numbers for each countrie, I'm also fitting a polynomial to the current points.","99b994e6":"---\nK neightbour classifier - balanced bins  - Deaths Ratio feature\n---","ab08b7fb":"It look like having 2 temperature peaks by 7c and 24c","122cec5d":"---\nRandom forests - equidistant bins  - Deaths Ratio feature\n---\n\nI will use the same methodology as above. I use 250 for the number of `n_estimators` as this seems to be a good default value in practice from my research on online forums.\n","36ea3695":"Till now are 4-5 main virus variant\n\nin china was 2 version (A) , one agressiv in wuhan, an other one less in the rest of the country (B).\n\nIn rest of asia was also an other mutation\n\nin france, spain, italy is also an different the number of deaths is different then in germany\/and other eastern countries.\nIn france was discovered the first virus  in december\n\nthat all means it is not possible on the graphic to learn something due of these mutations.\n\nactive Log10\n---","3ae5ec27":"Population 2020 was used to normalize some data,  Density will  also not be relevant,  but we have Urban Pop","84797fda":"Baseline  most frequent - balanced bins  - Deaths Ratio feature\n---","c40b8a52":"```\na low correlation Pearsons correlation Total Deaths Log10 <-> Health expenditure Ratio : 0.580 very strange\n```\nno correlation on number of smoking\/male\/female\/lung (-0.5<Pearsons correlation<0.5), but look like an information on the avg temperature country having number of infect >4.0 (log) have an avr temp 2.5-7c over this temperature  the number of infected is lower., in cold temperature the same. the death show the same","dea89fdd":"Compute feature importance","be20ad0c":"Later, I will only analyze the top 60 countries where the epidemic is the worse.","3f2cf07c":"Tuning the decision tree on the complete data X, y with cross-validation.","83f10982":"`covid19_by_country.csv`\n---\n\nSource: [COVID-19 Predictors](https:\/\/www.kaggle.com\/nightranger77\/covid19-demographic-predictors?select=covid19_by_country.csv) version 19 (12.mai.2020)\n\nGives meta-information about policies and how the epidemic is handled in the different countries\n\nNote that COVID-19 testing data will not be updated; however, COVID-19 infections and deaths from the Johns Hopkins dataset will be updated every few days.\n\nCombines the Johns Hopkins COVID-19 data with several other public datasets\n\n2018 GDP in Million USD in covid https:\/\/data.worldbank.org\/indicator\/NY.GDP.MKTP.CD\n\nCrime and Population https:\/\/worldpopulationreview.com\/countries\/crime-rate-by-country\/\n\nSmoking rate https:\/\/ourworldindata.org\/smoking#prevalence-of-smoking-across-the-world\n\nSex (% Female) https:\/\/data.worldbank.org\/indicator\/SP.POP.TOTL.FE.ZS\n\nMedian Age https:\/\/worldpopulationreview.com\/countries\/median-age\/\n\nAlso includes COVID-19 specific data from @koryto https:\/\/www.kaggle.com\/koryto\/countryinfo\n\nHealth expenditure (% of GDP) : https:\/\/data.worldbank.org\/indicator\/SH.XPD.CHEX.GD.ZS\n\nfrom https:\/\/www.kaggle.com\/koryto\/countryinfo\n\n```\npop             : Population in 1000\ntests           : The number of corona detection tests made per day (as for 3\/24\/2020)\ntestpop         : pop \/ tests (as for 3\/24\/2020)\ndensity         : The amount of citizens per square meter\nmedianage       : Median age\nurbanpop        : The % of people who lives in urban locations.\nquarantine      : Initial date of quarantine policy\nschools         : The initial date of school closure policy\npublicplace     : The initial date of public places (restaurents, bars, etc.) restrictions\nhospibed        : The amount of hospital bed per 1K people\nsmokers         : The % of smokers within the population.\nsex0            : Amount of males per female at birth (2020)\nsex14           : Amount of males per female in the age group 0-14\nsex25           : Amount of males per female in the age group 14-25\nsex54           : Amount of males per female in the age group 26-54\nsex64           : Amount of males per female in the age group 55-64\nsex65plus       : Amount of males per female in the age group 65+\nsexratio        : Amount of males per female (not separated by age)\nlungDeath       : rate from lung diseases per 100k people\nfemalelungDeath : rate from lung diseases per 100k people for female\nmalelungDeath   : rate from lung diseases per 100k people for male\ngdp2019Nominal  : GDP for 2019 (in 1 million USD)\nspme explanations\n```","58c8b708":"The number of \"comfirmed\" is referred to in the first DataFrame as \"Infected\". I will rename it","f37272d6":"tabelle available: stats\n    codiv_country_without_Restrictions_qurantine_confirmed\n    codiv_country_without_Restrictions_qurantine_deaths\n    codiv_country_without_Restrictions_qurantine_recovered\n    \n    codiv_country_qurantine_confirmed\n    codiv_country_qurantine_deaths\n    codiv_country_qurantine_recovered\n    \n    codiv_country_Restrictions_confirmed\n    codiv_country_Restrictions_deaths\n    codiv_country_Restrictions_recovered\n    \n    \n    Build active-infected table","7a05f21c":"---\nDecision Tree - balanced bins - Total Infected log10 feature\n---","bd9c4164":"features importances","f5f39066":"---\nK neightbour classifier - balanced bins - Total Infected log10 feature\n---\noptimize the n_neighbors, over 35 i am getting cpu problem\nhigher is the n_neighbors more is he isolating every point, it means high n_neighbors would only means there is no big area available with same label elements","2c6fdc90":"---\nDecision Tree - equidistant bins  - Deaths Ratio feature\n---","592f2009":"Later, I will join the tables together, and hence need to make sure they have the same country names.","a01b3a62":"computing the tree again with the optimal depth","cd57e930":"Computing it again with optimal estimator","1ad8f8f9":"Because of the joining that I will do later, I also need to normalize the country names.","047842be":"codiv_country[codiv_country['Health expenditure Ratio'].isnull()]","4b7f6622":"i will use the weight distance   because i cant explain the result default=\u2019uniform\u2019\n\n\u2018uniform\u2019 : uniform weights. All points in each neighborhood are weighted equally.\n\n\u2018distance\u2019 : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away","cf9afdbd":"---\nK neightbour classifier - equidistant bins  - Deaths Ratio feature\n---","8d8185da":"---\ncheck some correlation possibilities\n---\nthe goal is to check which parapemer could have an impact, the GDP\/population size\/\/density are take out, hmore population you have more it you will have infected., more poeple you have higher is the GDP, etc..\nI will also check the pearsons correlation factor.","0599af0e":"At the beginning of the COVID 2020 epidemic, many people were discussing the factors that could affect the disease such as smoking, age for example. I would like to know wether those are the part of the important factors. I was also interested to use China epidemic values as reference to model the evaluation of the epidemic in the other countries. I believe that China can really be taken as a reference as its policy (quarantine) was consistent throughout the epidemic (from beginning to end) without considering the economic loss.\n\nThe project will address two questions\n\nThe effect of external factors (tests, temperature, health and other descriptors of the population, policy: quarantine, school closing, restrictions) on the epidemic\nModel the evolution of the epidemic value based on China numbers as reference\nThis was a very iterative project where the workflow evolved a lot after each finding and results.\n\nthe data mostly will be get from internet to be actual","f4b01a07":"data cleaning","6c2dcc47":"There is a peak of \"total active\" for mean temperature around 5\u00b0C. This similarity with worse countries cannot be explained by the latitude as seen below\n\n- USA : 39,91N\n- China : 39,55N\n- Germany: 55N\n- Italy:44N\n- Spain 40N","0b4c2224":"---\nBaseline classifier - most frequent - Deaths Ratio feature\n---","3c3704d6":"Two countries have missing values for the temperatures. I will keep them in the data and see if this produces issues later in the analysis.\n\nThe table have a heatmap per collumn, the higher value have dark red, smaller light red\n\nnow let show the data with total infected ascending=false\nFIRST for the Total Infected","01a66510":"buiding new table for the 3 cathegory\ncodiv_country_qurantine\ncodiv_country_Restrictions\ncodiv_country_without_Restrictions_qurantine","fc89b169":"Finaly let show the data with total Deaths ascending=false for the Deaths","3148c659":"New feature : deaths_ratio\n---\n                create deaths_ratio = number of deaths*1000 \/ number of infected\n\n\ndeaths_ratio means how much from 1000 infected persons was dying","7ac87d46":"Handling data types\n---\nWe have information for 95 countries and 26 attributes left","1088c491":"corona daten analyze and predictions\n---","ed249538":"confusion matrix with the optimal depth","3ab8786c":"Some are skewed, apply the log-transform in such cases NB: Density is difficult to use ie canada , usa, russia have high density in big city and 0 outside, epidemy is mostly in big city\nI will drop all `sex` and `Crime Index` which are not part of the analysis. I will also drop the `Total` columns since I collect this information from the real time data set `csse_covid_19_daily_reports.csv`\n\nTestpop = Population\/test, this is less interesing, better would be test*10000\/population = number of test by 10000 person\/day\nAlso the drop of density can be explain that the density in belgium is higher then in china, this is on country level, so not interesting","e9f6ecf9":"data source : https:\/\/github.com\/CSSEGISandData\/COVID-19\/tree\/master\/csse_covid_19_data\/csse_covid_19_time_series\n              https:\/\/github.com\/CSSEGISandData\/COVID-19\/tree\/master\/csse_covid_19_data\/csse_covid_19_daily_reports\n","d8d28752":"confusion matrix","41728b8a":"Historgram of \"total infected\" for countries that have Tests_per_10kp_log1p  == 0","0948b6cb":"The issue highlighted in this article [Forecasting s-curves is hard](https:\/\/constancecrozier.com\/2020\/04\/16\/forecasting-s-curves-is-hard\/) is that forecasting the numbers isn't really accurate at the beginning of the cruve until it starts to decrease. For this reason, the models below only provide good insights from China as a model after the epidemic \"peak\".\n\nAlso, the \"S curve\" model (or Normal curve) only works to model for a single epidemic event which might not be the case due to change in policy ex. end of quarantine or for some countries where the data was changed. The data is not clean in the sense that some countries such as Germany stopped recording during weekends. As another example, the policy can suddenly change as in Wuhan where it was decided to systematically test the population (15th May).\n\nI decided to use a polymial curve to highlight those sudden changes even if the model will only work within the range of data and shouldn't be use to forecast future due to boundary effects as explained here: https:\/\/twitter.com\/laurabronner\/status\/1257743918919151616 - the curves in the plots below are limited to the start\/end of data points I have.","836c3f19":"The confusion_matrix with split","bef9bf89":"score and cross validation","4ea0fc6a":"****This notebook provide the covid-19 predictions for around 60 countries about the number of active infected persons\n---","6e18115b":"Equidistant bins deliver better resultat\n---\n---\nlets analyze  the deaths ratio feature \n---\ncodiv_csse[\"Deaths Ratio\"]=codiv_csse[\"Deaths\"]*1000\/codiv_csse[\"Infected\"]","c4a0df3d":"Restrictions\n---","1a4c0abf":"Now, I'm aggregating the information by country. Two countries will be affected: US and China.","677b404e":"complete missing value with mean country value","2f7e6c4a":"deaths Log10\n---","9936152c":"Depending on the seed, the results can vary, but the optimal models always have a large depth (usually between 7-10).\n\nThe decision tree performs better than the baseline.","2a01969a":"More you invest in health more infected and death you have? or less you invest in health, less poeple are going to hospital and so less poeple are officialy registrated as infected?\n\nChina gave covid  test and medicine for free to everyone in the country, rich and poor. What about USA\/europa?  if you have symptome please stay at home... that means they are not registrated as infected, if you are seriously sick, come over for a test (germany , belgium, switzerland and you have to pay for.)","9d3c3143":"Missing values\n---\nChecking for missing values. It seems that 15 countries have decided to put in place a quarantine and 67 not. But we should be careful about how quarantine is defined. For instance, the quarantine in France is not the same as in Wuhan.","ab9fc89d":"Preparing the time series data\n---\nI'm dropping the Latitude and Longitude information which are not part of the analysis.","18795fab":"---\ncodiv_country_temp and codiv_country_tempMajorCity\n---\nThe temperature table deliver 1 value\/month from 1700 till 2013, \n- the epidemic started in january, till april,\n- The actual temperature value are not available, so i till build the data based on the last 3 years available to protect against temperature fluctuations\n","4b628ef7":"you can see the adjustment for some country like russia and canada, the avr temp big city is slightly different for the avr country temperature, the same for the hot countries\n\ncomplete the tabelle first  with the main city temp if they exists","1f73a9b2":"No Quarantine  and  No Restrictions\n---","7eef0d4a":"---\nRandom forest - Balanced bins  - Deaths Ratio feature\n---","8da1b1b2":"checking outlier by some columns\n---","01458c9c":"---\nscatter plot for total infected\n---\nChecking the relation between Tests_per_10kp and total infected.","097590f8":"To tune  the optimal depth of the tree, I will use cross validation with the default `cv=5` value from Scikit-learn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html","3d40282b":"---\nK neightbour classifier - equidistant bins - Total Infected log10 feature\n---","2d10689f":"let show the data with total active ascending=false for the active","41182660":"Setting the index to Countries","f91e4e15":"working on the time serie file csse_covid_19_daily_reports.csv\n---\nRemoving columns that I won't use in the analysis","61e0ce3a":"Remark: \n\n1. The informations like number of bed reflect the political decision : \n        ie in belgium aged poeple in hospice have no right for hospital, so the number of needed bed is not huge\n        ie, in wuhan the was building extra new hospital to give youg and aged poeple a chance to get healty\nthats why i will not considere parameter depending of the political impact\n\n2. The number of test, there is missing the time information about the test number, ie, if the test capacity increased, then when","9b314e2a":"Checking for missing values","1509af14":"a few surprized that the tree is high by deapth 1, the forest deliver the awaiting factor importance\n\n---\n---\nPART 3 : daily value trend and prediction\n---\n\nwe have these tabel\ncodiv_country_qurantine\ncodiv_country_Restrictions\ncodiv_country_without_Restrictions_qurantine\ncodiv_time_recovered\ncodiv_time_deaths\ncodiv_time_confirmed\n\nneed to consolidate the country name\ngroup row having same country name","91bb20fd":"Section 0: Data Preperation","75fc9925":"The countries without restrictions and quarantine definitely have an increasing epidemic.\n\nThe countries with restrictions show a slowdown in development.\n\nThe countries with the quarantine have been successful in achieving a better result.\n\nQuarantine and restrictions are political decisions. How they are used differs from country to country, also how violations of the law are punished or monitor is dependent on country","4837723c":"Female  = % female, this is the invers von sex ratio, we will drop sex ratio","cd79f3b4":"---\nscatter plot for total active\n---","27737659":"At the beginning of the COVID 2020 epidemic, many people were discussing the factors that could affect the disease such as smoking, age for example. I would like to know wether those are the part of the important factors. I was also interested to use China epidemic values as reference to model the evaluation of the epidemic in the other countries. I believe that China can really be taken as a reference as its policy (quarantine) was consistent throughout the epidemic (from beginning to end) without considering the economic loss.\n\nThe project will address two questions\n\nThe effect of external factors (tests, temperature, health and other descriptors of the population, policy: quarantine, school closing, restrictions) on the epidemic\nModel the evolution of the epidemic value based on China numbers as reference","66ebbdbd":"the country density and incted porsone is slightly higher then the country making test\n\nhistogram of \"Total Active\" for countries that have Tests_per_10kp_per_100ps_log1p > 0","cd456b75":"define bin_equidistants category","2fe2845a":"compute for the optimal estimator","95612e0a":"confirmed Log10\n---","f560b23d":"The temperature table deliver 1 value\/month till 2013, so i use the last 3 Years periode jan to april and make the mean.\n\nthe temperature is mainly done from big city information. epidemie is mostly also in big cities","97c02504":"For the second and 3rd part i will build new table, covid without restriction \/ with quarantine \/ with rectriction ","676af5f3":"For the confusion matrix i use the split data","966434cc":"confusion matrix with the optimal depth","9d349a07":"---\n---\nSection 2: Analyzing if there are any external factors that affect the epidemic value\n---\n\nIn this part of the analysis, I will fit, tune and evaluate a Decision Tree model, Forest model, and dummyclassifier to analyze the effect of external factors on the epidemic value.","c6895da5":"Those results make sense as the \"Urban Population\" is directly affecting how much the epidemic is spreading. Also, if the population is older, then it will also affect the number of people that are detected.\n\nI was maybe expecting the \"Smoking\" to be higher, but it's difficult to know its effect on the disease.\n\nRegarding the quarantine measures, the quarantine\/schools\/restrictions are at the end of the results, but I think that this is because it highly depends on how this is applied. For instance, in China or Germany, people going to the supermarket have to wear masks which is not the case for instance in Switzerland or other countries ex. in Europe. Distance measures are also not the same.","fdf2732a":"If the number of Tests_per_10kp is missing, I will assume that the country is not testing and I will replace by zero.","ca844eaa":"Checking data type","f93de938":"The epidemy happen mostly in big city\nbig country like usa\/china\/Russia have lot of unmanned area which have completely different temperature\nThat means, the representative temp, should be the one of the big cities ( if they exists)","383071e8":"---\nBaseline classifier most frequent - equidistant bins -  Total Infected log10 feature\n---\n\nI will use the \"most-frequent\" baseline.\n\nFor the evaluation, I will not split into train\/test sets as the number of data points is very low. Instead, I will report the cross-validated score.","9ad81107":"The deaths ratio deliver complete new tops. I think it is in relation to the land registration policy, ie how they register a codiv death\n---\n---\nSection 1: data analyze\n---\nthe interest is to check the worst country, so i limit the number  to 60 country  sorted on the higest infected","e629eb53":"Later, I will evaluate the decision tree and random forest estimators. I will store the results in a DataFrame.","1c206bad":"Input matrix\n---","08097ee2":"I think that this highlight an issue with the \"balanced\" approach: we are mixing together countries that have a very different number of infected people, in particular with the top bins ex. category 5 and 6.","d2f02d66":"Display the tree","09f76810":"with split for the confusion matrix","a8d8c413":"splitting in training and test, these will be used for the confusion matrix","54b0a218":"Decision Tree - balanced bins  - Deaths Ratio feature\n---","9e78a23b":"Quarantine\n---","7827f679":"Checking the distribution of the variables","668c7933":"---\nFeature reduction to 2 and kneighbour classifier - equidistant bins - Total Infected log10 feature\n---\nfeature reduction"}}