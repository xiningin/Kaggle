{"cell_type":{"d7387c80":"code","6ca6ffaa":"code","3a3faecd":"code","7a85590e":"code","905f7be1":"code","09fc68d7":"code","37175a2b":"code","0288b9dd":"code","82a56270":"code","c2ea441c":"code","28bb0612":"code","49ec791d":"code","ddba477b":"code","468b8355":"code","8ccd2995":"code","a9663b1b":"code","c29e1f22":"code","9d9ec203":"code","ab457eb7":"code","6e102598":"code","8a62db9a":"code","84cbe12f":"code","af13b4ec":"code","4f96812d":"code","7bc23eeb":"code","8b7977ae":"code","24549980":"code","040ef0f1":"code","b067afc0":"code","2f5bb49a":"code","3ba19b2e":"code","13d63878":"code","c1d1dbb7":"markdown","113807b7":"markdown","a728b9ad":"markdown","2e11eaff":"markdown","99d0b57b":"markdown","3d2c4b82":"markdown","b5809988":"markdown","2e0a1c0c":"markdown","9f9e6a50":"markdown","1e1840cf":"markdown","92d4df7d":"markdown","4b16e15d":"markdown","18eeacff":"markdown","db115285":"markdown","74dec259":"markdown","781d3aa7":"markdown","9e3e2ffa":"markdown","5e234e55":"markdown","72579b58":"markdown","b8925aa9":"markdown","0b2a4f66":"markdown","ecab8145":"markdown","ba5ad1c6":"markdown","63e3a09b":"markdown","7172c21f":"markdown","81ceacba":"markdown","f75cc677":"markdown","92d220e5":"markdown","9556b719":"markdown","4995ddcd":"markdown"},"source":{"d7387c80":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ca6ffaa":"import pandas as pd\nimport numpy as np\nfrom wordcloud import WordCloud, STOPWORDS\nimport seaborn as sns\nfrom os import path\nsns.set()\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\nimport datetime as dt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport calendar\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n%matplotlib inline\nimport time","3a3faecd":"# Importing the csv data files \nsarcasm_df = pd.read_csv('..\/input\/sarcasm\/train-balanced-sarcasm.csv')\n ","7a85590e":"# Data Pre-Processing\n# Removing the null comments\nsarcasm_df.dropna(subset=['comment'], inplace=True)\nsarcasm_df['comment'] = sarcasm_df['comment'].str.lower()\nsarcasm_df['comment'] = sarcasm_df['comment'].str.replace('[^\\w\\s]','')","905f7be1":"# Converting the timestamp into DateTime object\nsarcasm_df.created_utc = pd.to_datetime(sarcasm_df.created_utc)\nsarcasm_df.info()","09fc68d7":"plt.figure(figsize=(5,5))\nax = sns.countplot(x='label',  data= sarcasm_df)\nax.set(title = \"Distribution of Classes\", xlabel=\"Sarcasm Status\", ylabel = \"Total Count\")\ntotal = float(len(sarcasm_df ))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.1f}%'.format((height\/total)*100),\n            ha=\"center\") \nplt.show()","37175a2b":"# Distribution of the lenth of Sarcastic comments\nsns.boxplot(x= sarcasm_df.loc[sarcasm_df['label'] == 1, 'comment'].str.len()).set(title = 'Length of Sarcastic Comments', xlabel = 'Length')\nsns.despine(offset=10, trim=True)\nplt.show()","0288b9dd":"# Distribution of the lenth of Neutral comments\nsns.boxplot(x= sarcasm_df.loc[sarcasm_df['label'] == 0, 'comment'].str.len()).set(title = 'Length of Neutral Comments', xlabel = 'Length')\nsns.despine(offset=10, trim=True)\nplt.show()","82a56270":"sarcasm_df['log_comment'] = sarcasm_df['comment'].apply(lambda text: np.log1p(len(text)))\nsarcasm_df[sarcasm_df['label']==1]['log_comment'].hist(alpha=0.6,label='Sarcastic', color = 'blue')\nsarcasm_df[sarcasm_df['label']==0]['log_comment'].hist(alpha=0.6,label='Non-Sarcastic', color = 'red')\nplt.legend()\nplt.title('Natural Log Length of Comments')\nplt.show()","c2ea441c":"wordcloud = WordCloud(background_color='black', stopwords = STOPWORDS,\n                max_words = 200, max_font_size = 100, \n                random_state = 17, width=800, height=400)\n\nplt.figure(figsize=(12, 12))\nwordcloud.generate(str(sarcasm_df.loc[sarcasm_df['label'] == 1, 'comment']))\nplt.grid(b= False)\nplt.imshow(wordcloud);","28bb0612":"# Converting the scores into numpy array\nsarcasm_score = np.array(sarcasm_df.loc[sarcasm_df['label'] == 1]['score'])\nneutral_score = np.array(sarcasm_df.loc[sarcasm_df['label'] == 0]['score'])","49ec791d":"# Displaying the distribution of Marital Status in a Pie chart\nlabels = ['Sarcastic Score', 'Neutral Score']\nsizes = [3235069, 3725113]\n#colors\ncolors = ['#F21F3B', '#1FF257']\n \nplt.rcParams.update({'font.size': 14})\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, colors = colors, labels=labels, autopct='%1.1f%%', startangle=30)\nax1.set_title(\"Scores of Subreddits\")\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal') \nplt.tight_layout()\nplt.show()","ddba477b":"sarcasm_comm_len = np.array(sarcasm_df.loc[sarcasm_df['label'] == 1]['comment'].str.len())\nparent_comm_len = np.array(sarcasm_df.loc[sarcasm_df['label'] == 1]['parent_comment'].str.len())\nratio_len = np.array((sarcasm_df.loc[sarcasm_df['label'] == 1]['comment'].str.len())\/(sarcasm_df.loc[sarcasm_df['label'] == 1]['parent_comment'].str.len()))","468b8355":"dataset = pd.DataFrame({'Comment Length': sarcasm_comm_len, 'Parent Comment Length': parent_comm_len, 'Ratio Length': ratio_len}, columns=['Comment Length', 'Parent Comment Length', 'Ratio Length'])","8ccd2995":"ax = plt.axes()\nsns.scatterplot(data=dataset, x=\"Comment Length\", y=\"Parent Comment Length\",  size=ratio_len)\nax.set_title(\"Comparing Sarcastic Comment Length with Parent Comment\")\n# control x and y limits\nplt.ylim(0, 12000)\nplt.xlim(0, 800)\nplt.show()","a9663b1b":"# Getting the top 5 popular subreddits\nsarcasm_df['subreddit'].value_counts()[:5]","c29e1f22":"top_reddits =['AskReddit', 'politics', 'worldnews', 'leagueoflegends', 'pcmasterrace']","9d9ec203":"subreddit = pd.DataFrame()\nsubreddit['subreddit'] = top_reddits\nsubreddit['sarcastic'] = np.nan\nsubreddit['natural'] = np.nan\nsubreddit['total'] = np.nan","ab457eb7":"# Calculating the count of Sarcastic and Natural comments for the top 5 subreddits \nfor i in range(len(top_reddits)):\n    temp = sarcasm_df.loc[sarcasm_df['subreddit'] == subreddit.subreddit.iloc[i]]\n    length = len(temp)\n    count_sarcastic = len(temp.loc[temp['label'] == 1])\n    subreddit.sarcastic.iloc[i] = count_sarcastic\n    subreddit.natural.iloc[i] = length - count_sarcastic\n    subreddit.total.iloc[i] = length","6e102598":"subreddit","8a62db9a":"# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 5))\n# Plot the total comments for the subreddits\nsns.barplot(x=\"total\", y=\"subreddit\", data=subreddit,\n            label=\"Total\", color=\"b\")\n# Plot the total sarcastic comments for the subreddits\nsns.barplot(x=\"sarcastic\", y=\"subreddit\", data=subreddit,\n            label=\"Sarcastic Comments\", color=\"r\")\nax.legend(ncol=2, loc=\"lower right\", frameon=True)\nax.set( ylabel=\"Subreddits\",\n       xlabel=\"Total number of comments\")\nsns.despine(left=True, bottom=True)","84cbe12f":"# Feature Engineering- Extracting the day of a week\nsarcasm_df['created_utc'] = pd.to_datetime(sarcasm_df['created_utc'], format = '%d\/%m\/%Y %H:%M:%S')\nsarcasm_df['Day of Week'] = sarcasm_df['created_utc'].dt.day_name()","af13b4ec":"# Visualization of Column- label\nplt.figure(figsize=(10,5))\nax = sns.countplot(x='Day of Week',  data= sarcasm_df.loc[sarcasm_df['label']==1])\nax.set(title = \"Count of sarcastic comments per day\", xlabel=\"Days of the week\", ylabel = \"Total Count\")\ntotal = float(len(sarcasm_df ))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 7,\n            '{:1.1f}%'.format((height\/total)*100*2),\n            ha=\"center\") \nplt.show()","4f96812d":"tf_idf = TfidfVectorizer(ngram_range=(1, 1), stop_words= 'english', max_features=50000, min_df=2)","7bc23eeb":"x_train, x_test, y_train, y_test = train_test_split(sarcasm_df['comment'], sarcasm_df['label'], test_size= 0.3, random_state=42)","8b7977ae":"parameters = {'solver': [ 'lbfgs', 'liblinear', 'saga'], 'verbose': [0, 1, 2]}\n\n# multinomial logistic regression a.k.a softmax classifier\nlogit = LogisticRegression(random_state= 42)\n\nlogit_classifier = GridSearchCV(logit, parameters, cv = 5, n_jobs= 4)\n\n# sklearn's pipeline\ntfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf),  ('logit_classifier', logit_classifier)])","24549980":"start = time.time()\ntfidf_logit_pipeline.fit(x_train, y_train)\nend = time.time()\nprint(end - start)  ","040ef0f1":"print(logit_classifier.best_params_)\nprint(logit_classifier.best_score_)","b067afc0":"valid_pred = tfidf_logit_pipeline.predict(x_test)","2f5bb49a":"accuracy_score(y_test, valid_pred)","3ba19b2e":"# Getting the accuracy metric\nacc = accuracy_score(valid_pred, y_test)\npre = precision_score(valid_pred, y_test)\nrec = recall_score(valid_pred, y_test)\nf1 = f1_score(valid_pred, y_test)\n\nprint ('Model Performance Statistic Suite-1: ')\nprint ('Accuracy: ', acc)\nprint ('Precision: ', pre)\nprint ('Recall: ',rec)\nprint ('F1 Score: ', f1)\n\ncm = confusion_matrix(y_test,valid_pred)\n\nsensitivity = cm[0,0]\/(cm[0,0]+cm[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = cm[1,1]\/(cm[0,1]+cm[1,1])\nprint('Specificity : ', specificity)","13d63878":"cm =  pd.DataFrame(cm, index=['Natural','Sarcastic'],columns=['Natural','Sarcastic'])\nfig = plt.figure(figsize=(8,6))\nax = sns.heatmap(cm,annot=True,cbar=False, cmap='Greens',linewidths=0.5,fmt='.0f')\nax.set_title('Confusion Matrix',fontsize=16,y=1.25)\nax.set_ylabel('Ground Truth',fontsize=14)\nax.set_xlabel('Predicted',fontsize=14)\nax.xaxis.set_ticks_position('top')\nax.xaxis.set_label_position('top')\nax.tick_params(labelsize=12)","c1d1dbb7":"# Exploratory Data Analysis","113807b7":"## Distribution of the classes in the dataset","a728b9ad":"## Feature Engineering","2e11eaff":"## Model traning- Using Logistic Regression","99d0b57b":"## Top Five popular subreddits & Sarcastic comments","3d2c4b82":"## Length of Sarcastic comment compared to the parent comment","b5809988":"With the help of the score of the comments, we can determine whether the sarcastic comments are more popular in Reddit discussions.","2e0a1c0c":"Parameters of the best performing model and its accuracy with the training dataset","9f9e6a50":"## Being sarcastic on a specific day of the week","1e1840cf":"In this EDA we are tying to figure out wehther the user of Reddit tend to be more sarcastic on a specific day of the week.","92d4df7d":"According to the graph above the lenght of the sarcastic comments is notmally distributed where as the non-sarcastic comments is slightly negatively skewed.","4b16e15d":"Lets see if there is any relation between the length of the comment and the comment being sarcastic","18eeacff":"## Plotting the Confusion Matrix","db115285":"Creating the training and test dataset","74dec259":"### Importing all the necessary libraries","781d3aa7":"According to the dataset sarcastic comments tend to be less popular due to having lower overall scores.","9e3e2ffa":"## Wordcloud of  Sarcastic comments","5e234e55":"Test accuracy of the model","72579b58":"Validating the accuracy of the model with the test datset","b8925aa9":"The figure above ensures that the dataset is balanced as the proportion of the sarcastic and non-sarcastic comments are same i.e.- 50%","0b2a4f66":"According to the above visual, we can conclude that in most of the cases; the length of the sarcastic comments is longer than its corresponding parent comment.","ecab8145":"Now, lets determine whether the length of the sarcastic comments is more than its parent comment.","ba5ad1c6":"Using unigram for the tokens","63e3a09b":"Training the model with the traning dataset","7172c21f":"According to the visual above we can see that the the count of the sarcastic comments decreases during the weekends. One of the reason for this issue could be due to the reduced number of traffic in Reddit during the weekends ","81ceacba":"## Popularity of the comments according to being sarcastic","f75cc677":"# **Creating the Classifier Model**","92d220e5":"## Length of the comments","9556b719":"Since the dataset is skewed log transformations are being made\n\nNatural Log Length of Comments for Sarcastic and Non-Sarcastic Comments","4995ddcd":"In this EDA we will analyse the proportion of sarcastic comments for top 5 Subreddits in the dataset. "}}