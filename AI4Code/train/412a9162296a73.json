{"cell_type":{"6dd0ce72":"code","1fc29e46":"code","f3b17e26":"code","d7821797":"code","294e3e2b":"code","7165aae1":"code","650edddb":"code","bd80943e":"code","40148bac":"code","895b0b91":"code","b0a943e7":"code","80c010cd":"code","16f66d8d":"code","2b9b0d79":"code","b248175f":"code","708daa10":"code","76a73d74":"markdown","f7c5f6dd":"markdown","d40763aa":"markdown","2822e53b":"markdown","450a352c":"markdown","d4730f36":"markdown","54598cf1":"markdown","b934e879":"markdown","42339d70":"markdown","0f2fa248":"markdown","d3814d0a":"markdown","dd68f822":"markdown","ec2c5f00":"markdown","ea228566":"markdown"},"source":{"6dd0ce72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1fc29e46":"# read csv file\npath = '\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv'\n\ndf = pd.read_csv(path)\nprint('The shape of our data is:' , df.shape)\ndf.head()\n","f3b17e26":"print('Numeric columns')\ndf.describe(include = np.number)","d7821797":"print('Categorical columns')\ndf.describe(include = np.object)","294e3e2b":"# processing\ncolumn_names = list(df.columns)\n\n# extract features and the target\ndata = df.iloc[:, 1 :-2]\ntarget = df.iloc[:,-2:]\n\n# separate between categorical and numeric columns\nnumeric_columns = data.select_dtypes(include=['int64' , 'float64'])\ncategorical_columns = data.select_dtypes(exclude=['int64' , 'float64'])","7165aae1":"# data visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('darkgrid')\n\n# seaborn pairplot\nsns.pairplot(df.iloc[:, 1:], hue=\"status\")\nplt.show()\n\n#from pandas.plotting import corr_matrix\ncorr_matrix = df.corr()\nprint('Correlation matrix:')\ncorr_matrix[\"salary\"].sort_values()","650edddb":"# Bar charts\nfig, ax = plt.subplots(4,2,figsize=(15,15))\n\nfor i in range(len(categorical_columns.columns)): \n    sns.countplot(x = categorical_columns[categorical_columns.columns[i]], \n                  hue = target['status'], \n                  ax = ax[i\/\/2,i%2])","bd80943e":"# label encode the binary data, as this would increase the prediction accuracy\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# get column names with binary data i.e. two classes\nc = categorical_columns.nunique() == 2\nbinary_cats = c[c].index\n\nlabel_categorical_columns = categorical_columns.copy()\nlabel_data = data.copy()\n\nfor c in binary_cats: \n    label_categorical_columns[c] = label_encoder.fit_transform(categorical_columns[c])\n    label_data[c] = label_encoder.fit_transform(categorical_columns[c])\n\n# make sure to keep track which are the positive and negative classes\nprint(label_categorical_columns.head())\nprint('---')\nprint(categorical_columns)","40148bac":"# label encode the status column from target data\nlabel_target = target.copy()\nlabel_placed = label_encoder.fit_transform(target['status'])\nlabel_target['status'] = label_placed\n\n# one hot encode the categorical columns, except those with binary values\ncategorical_columns_onehot = pd.get_dummies(label_categorical_columns)\n\n#t = label_encoder.fit_transform(data['gender'])\ndata_onehot = pd.get_dummies(label_data)\ndata_onehot","895b0b91":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX = data_onehot\ny = label_placed\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\nforest_model = RandomForestClassifier(random_state = 1)\nforest_model.fit(train_X, train_y)\n\n# evaluate model\nprint('F1 Score:', forest_model.score(val_X, val_y))\n","b0a943e7":"# evaluate each feature's importance\nimportances = forest_model.feature_importances_\n\n# sort by descending order of importances\nindices = np.argsort(importances)[::-1]\n\n#create sorted dictionary\nsorted_importances = {}\n\nprint(\"Feature ranking:\")\nfor f in range(X.shape[1]):\n    sorted_importances[X.columns[indices[f]]] = importances[indices[f]]\n    print(\"%d. %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))","80c010cd":"# Obtain average percentages for the DataFrame\npercent_df = pd.DataFrame(numeric_columns.agg('mean', axis = 1)).join(df['status'])\n\npercent = percent_df.groupby(['status']).agg(['mean','var', 'count'])\npercent.columns = percent.columns.droplevel()\npercent","16f66d8d":"# one tailed t-test\n# H0: mu_placed - mu_notplaced = 0\n# H1: mu_placed - mu_notplaced > 0\n\n# unequal sample sizes, similar variance\n\n# extract variables from table\nplaced_mu, notplaced_mu = percent['mean']['Placed'], percent['mean']['Not Placed']\nplaced_var, notplaced_var = percent['var']['Placed'], percent['var']['Not Placed']\nn1, n2 = percent['count']['Placed'], percent['count']['Not Placed']\n\n# calculate the t statistic\nsp = np.sqrt(((n1 - 1) * placed_var + (n2 - 1) * notplaced_var)\/ (n1 + n2 - 2))\nt_stat = (placed_mu - notplaced_mu) \/ (sp * np.sqrt(1\/ n1 + 1\/ n2))\n\nprint('The t statistic is', t_stat)\nfrom scipy.stats import t\nprint('The p-value is,', t.cdf(-np.abs(t_stat), df = n1 + n2- 2))\n","2b9b0d79":"data_joined = data.join(df['status'])\n\n# obtain table for placed students in each specialisation\np = data_joined.groupby(['specialisation'])['status'].agg([lambda z: np.mean(z=='Placed'), \"size\"])\np.columns = [\"Placed\", 'Total']\nprint(p)\n","b248175f":"# We want to test whether it is finance students find it easier to get placed \n# H0 is pfin - phr > 0, as we want to do a one-tailed test \n# H1: pfin - phr <= 0 \n\n# calculate pool proportion\np_us = len(df[df['status']=='Placed']) \/ len(df)\n\n# obtaining individual proportions and total counts from table above\npfin, phr = p['Placed']['Mkt&Fin'], p['Placed']['Mkt&HR']\nn1, n2 = p['Total']['Mkt&Fin'], p['Total']['Mkt&HR']\n\n# calculate standard error\nse = np.sqrt(p_us*(1- p_us)*(1\/n1 + 1\/n2))\n\n# Calculate the best estimate of the proportion distribution\nbe = pfin - phr\n\n# Calculate the hypothesized estimate, which is no difference\nhe = 0\n\n#Calculate the test statistic\ntest_statistic = (be - he)\/se\n\n# Obtain one tailed p-value\nfrom scipy.stats import norm\npvalue = norm.cdf(-np.abs(test_statistic))\n\nprint('The p-value is {0:.6f}'.format(pvalue))","708daa10":"onehot_target = data_onehot.join(label_target)\ncorr_matrix = onehot_target.corr()\nfig = plt.figure(figsize = (10,10))\nsns.heatmap(corr_matrix, square=True, cmap=\"YlGnBu\")","76a73d74":"## 4. Conclusion\n\n\nA Random Forest classifier was used to model the factors responsible for successful Campus Placements of students studying for an MBA, achieving an F1 accuracy of 90%. Using this classifier, it was noted that secondary school percentages was the best predictor for whether a person could get a placement. I digress, this seems weird but gives some insight into the question of whether an individual's success is already predetermined during childhood. The worst predictors were degree types.\n\nUsing a t-test, we found that percentage indeed matters with a high degree of significance. A proportion z-test gave the result that those who study the Marketing and Finance specialisation are more demanded by corporates than those who study Marketing and HR, with a p-value of about 0.1%. However, we also spot a slight correlation that those who study the Finance specialisation tend to have higher overall percentages. Thus, this may be the underlying cause of the two deductions we made.","f7c5f6dd":"---\n## 3. Statistical tests","d40763aa":"# 1. Exploratory Data Analysis","2822e53b":"### What factor influenced a candidate in getting placed?\n\nThus, we notice that the most useful predictors for this is the secondary education percentage (0.343) the degree percentage (0.168), followed by the higher secondary (0.157) and MBA percentages (0.110). \n\nThe least influential predictors indeed are the high school degree specialisations and undergraduate degree types. \n\n### Does percentage matters for one to get placed?\n\nEvidently, percentage matters to a large extent in determining whether a particular student gets placed, accounting for the majority (0.779) of the random forest importance score. Some possible explanation of this would be how percentages are correlated with performance abilities, which would shine be at play in interview scenarios, if they exist, or would stand out to employers very well on paper. ","450a352c":"If you're unconvinced about the influence of percentages, we can perform a hypothesis test about whether those who are placed have higher percentages than those who are not placed. ","d4730f36":"### 2.2 Model training","54598cf1":"### Does percentage matter for one to get placed (part 2)?\nWe can thus establish, using the t-test that **those with placements tend to have higher percentages**, because that the p-value is extremely small. ","b934e879":"### 2.1 Some more processing for categorical data","42339d70":"# 2. Modelling\n\nWe will now employ a Random Forest Classifier to understand the different factors at play which influence whether a candidate gets placed. ","0f2fa248":"Here are some subplots indicating how different categorical data influenced whether an individual gets placed or not. What is important to notice is not the absolute number of the number of people who get placed but should be the **relative fraction**, given that the individual is in that category. \n\nBy observation, we can see that those who are female, have work experience or study Marketing and Finance do indeed have an advantage over their counterparts. However, we would like to know whether this advantage is significant or not. ","d3814d0a":"# Factors affecting Campus Placements\n\n## Campus placements\nA campus placement is a type of recruitment programme usually done at higher educational institutions, with the aim of providing graduate jobs to students at companies. Typically, these placement positions are quite competitive, since their numbers are limited. Thus, career-oriented students may want to understand what choices they can make to maximise their success in landing their first job.\n\n## Dataset\nKindly provided by Ben Roshan D (MBA in Business Analytics at Jain University Bangalore), this data set consists of placement data of students in some campus. It includes secondary and higher secondary school percentage and specialization. It also includes degree specialization, type and work experience and salary offers to the placed students.\n\n## Questions\nWe aim to solve these set of questions included with the dataset.\n1. Which factor influenced a candidate in getting placed?\n2. Does percentage matters for one to get placed?\n3. Which degree specialization is much demanded by corporate?\n4. Play with the data conducting all statistical tests.\n","dd68f822":"## Exploratory Analysis\nWe want to distinguish between those who got placed and those who did not, depending on the various factors at play. Using the pairplot and grouping the data by whether the person got placed or not allows us to recognise patterns at play in the job market. From the plots we see a general positive correlation between the various degree percentages of each student in question, and that those who gained placements generally do better on these tests. \n\nOne exception is with the MBA percentages, seeing as they do","ec2c5f00":"###  Which degree specialization is much demanded by corporate?\n\nThough degree specialisations are not the determining factors crucial to whether a student gets placed or not, we can still perform tests to see if there is a statistical difference to the two groups. Notice that the proportion of people who do the Marketing and Finance specialisation who get placed is higher than those who do Marketing and Human Resources (0.792 vs 0.558). Our one-tailed proportion difference hypothesis test allows us to figure out whether this difference is due to randomness or not.\n\nThe p-value of the test was 0.000119, which means that there is a 0.1% chance that the difference in proportions was due to randomness. Thus, we can conclude that those in the finance specialisation perform better than those in HR. \n\nThough in the correlation matrix below, we can see that those in Finance tend to have had higher percentages, so that may be the main reason why this is true.","ea228566":"### 2.3 Feature importance ranking"}}