{"cell_type":{"48ad4d1f":"code","9bcf186b":"code","e8a3a4fb":"code","b45d1e1f":"code","5e3712be":"code","34b16723":"code","41548291":"code","4b962c56":"code","3120e304":"code","f6e0c3f3":"code","9bc9bde4":"code","63b2443e":"code","2ad27ad0":"code","7a359102":"code","2af5aea8":"code","5969de28":"code","920152c1":"code","06c1c722":"markdown","af3d5917":"markdown","f692c895":"markdown","0797c075":"markdown"},"source":{"48ad4d1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\n\nimport plotly\nplotly.offline.init_notebook_mode(connected=True)\n\nimport matplotlib\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9bcf186b":"!pip install openpyxl","e8a3a4fb":"df = pd.read_excel('\/kaggle\/input\/arabic-cyberbullying\/dataset\/AJCommentsClassification-CF (2).xlsx')\ndf.head(2)","b45d1e1f":"df1 = pd.read_excel('\/kaggle\/input\/arabic-cyberbullying\/dataset\/Hamdy-TweetClassification-Summary.xlsx')\ndf1.head()","5e3712be":"df.isnull().sum()","34b16723":"df1['annotator1'].value_counts()","41548291":"import tensorflow as tf\nimport sklearn\nfrom tqdm import tqdm","4b962c56":"#Code by Lucas Abrah\u00e3o https:\/\/www.kaggle.com\/lucasabrahao\/trabalho-manufatura-an-lise-de-dados-no-brasil\n\ndf[\"_trusted_judgments\"].value_counts().plot.bar(color=['blue', '#f5005a', 'green'], title='Trusted Judgments');","3120e304":"#Code by Lucas Abrah\u00e3o https:\/\/www.kaggle.com\/lucasabrahao\/trabalho-manufatura-an-lise-de-dados-no-brasil\n\ndf[\"languagecomment\"].value_counts().plot.bar(color=['green', '#f5005a', 'blue'], title='Language comment');","f6e0c3f3":"#Code by Lucas Abrah\u00e3o https:\/\/www.kaggle.com\/lucasabrahao\/trabalho-manufatura-an-lise-de-dados-no-brasil\n\ndf1[\"aggregatedAnnotation\"].value_counts().plot.bar(color=['blue', '#f5005a', 'green'], title='Aggregated Annotation');","9bc9bde4":"#Code by Lucas Abrah\u00e3o https:\/\/www.kaggle.com\/lucasabrahao\/trabalho-manufatura-an-lise-de-dados-no-brasil\n\ndf1[\"annotator1\"].value_counts().plot.bar(color=['#DC143C', '#7FFF00', '#8B008B'], title='Annotator 1');","63b2443e":"#Code by Satyam Prasad Tiwari https:\/\/www.kaggle.com\/satyampd\/imdb-sentiment-analysis-using-bert-w-huggingface\/notebook\n\n# Loading the BERT Classifier and Tokenizer along with Input module\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom transformers import InputExample, InputFeatures\n\nmodel = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","2ad27ad0":"#Code by Satyam Prasad Tiwari https:\/\/www.kaggle.com\/satyampd\/imdb-sentiment-analysis-using-bert-w-huggingface\/notebook\n\n# But first see BERT tokenizer exmaples and other required stuff!\n\nexample='\u062a\u0635\u0641\u064a\u0629 \u0627\u0644\u0645\u0639\u0627\u0631\u0636\u064a\u0646 \u0628\u0645\u0635\u0631.. \u062a\u0635\u0631\u0641\u0627\u062a \u0641\u0631\u062f\u064a\u0629 \u0623\u0645 \u0645\u0645\u0646\u0647\u062c\u0629\u061f'  #I've no clue of this means I copied from articletitle 2nd row\ntokens=tokenizer.tokenize(example)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(tokens)\nprint(token_ids)","7a359102":"train = df[:45000]\ntest = df[45000:]","2af5aea8":"#Code by Satyam Prasad Tiwari https:\/\/www.kaggle.com\/satyampd\/imdb-sentiment-analysis-using-bert-w-huggingface\/notebook\n\ndef convert_data_to_examples(train, test, review, sentiment): \n    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n                                                          text_a = x['\u0628\u062f\u0644 \u0645\u0627 \u0627\u0646\u062a \u0642\u0627\u0639\u062f \u0628\u0631\u0647 \u0643\u062f\u0647 \u062a\u0639\u0627\u0644\u064a \u0627\u0632\u0631\u0639 \u0627\u0644\u0635\u062d\u0631\u0627'],                                                 \n                                                          label = x[languagecomment]), axis = 1)\n\n    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n                                                          text_a = x['\u064a\u0627\u062f \u0645\u062f\u0646\u064a: \u0644\u062f\u064a\u0646\u0627 \u0645\u0628\u0627\u062f\u0631\u0627\u062a \u0644\u062d\u0644 \u0623\u0632\u0645\u0627\u062a \u0627\u0644\u0639\u0627\u0644\u0645 \u0627\u0644\u0625\u0633...'], \n                                                          label = x[languagecomment]), axis = 1,)\n  \n    return train_InputExamples, validation_InputExamples\n\ntrain_InputExamples, validation_InputExamples = convert_data_to_examples(train,  test, '\u0627\u0646\u062a\u062e\u0627\u0628\u0627\u062a \u0627\u0644\u0645\u063a\u0631\u0628 \u0648\u0645\u0633\u0627\u0631 \u0627\u0644\u0625\u0635\u0644\u0627\u062d',  'languagecomment')","5969de28":"pred_sentences = ['\u0628\u062f\u0644 \u0645\u0627 \u0627\u0646\u062a \u0642\u0627\u0639\u062f \u0628\u0631\u0647 \u0643\u062f\u0647 \u062a\u0639\u0627\u0644\u064a \u0627\u0632\u0631\u0639 \u0627\u0644\u0635\u062d\u0631\u0627', '\u064a\u0627\u062f \u0645\u062f\u0646\u064a: \u0644\u062f\u064a\u0646\u0627 \u0645\u0628\u0627\u062f\u0631\u0627\u062a \u0644\u062d\u0644 \u0623\u0632\u0645\u0627\u062a \u0627\u0644\u0639\u0627\u0644\u0645 \u0627\u0644\u0625\u0633'] ","920152c1":"#tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')   # we are tokenizing before sending into our trained model\n#tf_outputs = model(tf_batch)                                  \n#tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)       # axis=-1, this means that the index that will be returned by argmax will be taken from the *last* axis.\nlabels = ['-1', '0', '-2']\n#label = tf.argmax(tf_predictions, axis=1)\n#label = label.numpy()\n#for i in range(len(pred_sentences)):\n    # print(pred_sentences[i], \": \", labels[label[i]])\nprint(pred_sentences, \": \", labels)","06c1c722":"#I think the programm didn't recognize the language. Because when I change to english, the error goes to another line.","af3d5917":"#Even to select the sentences it's hard since the pointer run in the opposite direction.\n\n#Hence I'm gonna stop here, since I'm NOT getting the whole thing.","f692c895":"#We found 31630 (3) and only 44 (4), 18 (6). That explains the barplot above.","0797c075":"#I wrote -1 (minus one) though it returned  -2- ?"}}