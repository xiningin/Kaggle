{"cell_type":{"7027efa1":"code","15a664aa":"code","b674e077":"code","dcc6373b":"code","fc793f70":"code","513db6b4":"code","45a8fc5d":"code","5d9e0be9":"code","0bf38500":"code","95874f1f":"code","735ad05e":"code","427f4cba":"code","ba460afa":"code","d0ec8df7":"code","dac70a93":"code","77b099d5":"code","e78a81db":"code","644b22a9":"markdown","e36bcaeb":"markdown","c17560ab":"markdown","fb90d83b":"markdown","b56ff815":"markdown"},"source":{"7027efa1":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","15a664aa":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tokenization\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n#from wordcloud import WordCloud\n\nimport re\nimport string\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b674e077":"## Load csv files \ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","dcc6373b":"print('Missing location:', train[train.location != train.location].shape[0])\nprint('Missing keyword:', train[train.keyword != train.keyword].shape[0])","fc793f70":"disasterCount = train[train.target == 1].shape[0]\nnonDisasCount = train[train.target == 0].shape[0]\n\nplt.rcParams['figure.figsize'] = (5,3)\nplt.bar(2, disasterCount, width=1, label='Disaster', color='blue')\nplt.bar(4, nonDisasCount, width=1, label='Non Disaster', color='red')\nplt.legend()\nplt.ylabel('# cases')\nplt.title('Disaster \/ Non Disaster Count')\nplt.show()","513db6b4":"## To Do\n# 1.World Cloud","45a8fc5d":"# Clean URL: https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert\/data\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ntrain['text']=train['text'].apply(lambda x : remove_URL(x))\ntrain['text']=train['text'].apply(lambda x : remove_html(x))\ntrain['text']=train['text'].apply(lambda x : remove_punct(x))\n\ntest['text']=test['text'].apply(lambda x : remove_URL(x))\ntest['text']=test['text'].apply(lambda x : remove_html(x))\ntest['text']=test['text'].apply(lambda x : remove_punct(x))","5d9e0be9":"# Is this really useful?\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","0bf38500":"## Targeted Cor\n# https:\/\/www.kaggle.com\/wrrosa\/keras-bert-using-tfhub-modified-train-data\/comments\nids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain[train['id'].isin(ids_with_target_error)]\ntrain.at[train['id'].isin(ids_with_target_error),'target'] = 0\ntrain[train['id'].isin(ids_with_target_error)]","95874f1f":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        # Token embedding [CLS],[SEP]    \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        \n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","735ad05e":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n## Load BERT from the Tensorflow Hub","427f4cba":"## Load tokenizer from the bert layer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","ba460afa":"## Encode the text into tokens, masks, and segment flags\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","d0ec8df7":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","dac70a93":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=5,\n    callbacks=[checkpoint],\n    batch_size=16\n)","77b099d5":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","e78a81db":"# Save output\nfrom datetime import datetime\ntimestamp = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission_BERT_' + timestamp + '.csv', index=False)\nprint(\"Your submission was successfully saved on \" + timestamp)","644b22a9":"Let's count the number of cases between disaster tweets and non disaster tweets","e36bcaeb":"<h3>Explore Data<\/h3>","c17560ab":"<h3>Load Dependencies and Import Data<\/h3>","fb90d83b":"This is a self-study notebook based on Notebook: https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub <br>\nReference: https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270","b56ff815":"<h3>Data Cleaning<\/h3>"}}