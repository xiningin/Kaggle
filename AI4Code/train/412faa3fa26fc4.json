{"cell_type":{"ff977bbf":"code","118379da":"code","6ddb794a":"code","7c30f2b0":"code","4fc3d5fa":"code","26f138f0":"code","8928cada":"code","f96cb926":"code","e61ed9bb":"code","88d0b77d":"code","486c72d1":"code","f0a3ee99":"code","bb691728":"code","f139c719":"code","74130e8c":"code","52b46b23":"markdown","22c5ebe2":"markdown","2666e55a":"markdown","56c4b0a8":"markdown","8f67bcc6":"markdown","51c23339":"markdown","256a70c8":"markdown","6be15ff5":"markdown","5f3d6917":"markdown","07a95bb8":"markdown","23ea964c":"markdown","8de85c80":"markdown","1cf3102a":"markdown","1bbfec5d":"markdown","eff57060":"markdown"},"source":{"ff977bbf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time","118379da":"df=pd.read_csv('..\/input\/company-bankruptcy-prediction\/data.csv')\nprint(df.shape)\ndf.head(5)","6ddb794a":"df['Bankrupt?'].value_counts()","7c30f2b0":"#get the dimensions without 'Bankrupt'\nno_features=['Bankrupt?']\nfeatures=[feat for feat in df.columns.values if feat not in no_features]\nprint(\"diminsion:\",len(features))","4fc3d5fa":"#The features are all numerical features, which are all processed as rank features\nfor feat in list(df[features].columns.values):\n    df[feat]=df[feat].rank()\/float(df.shape[0]) # sort and normalize","26f138f0":"#Visually observe the result of sort and normalize\nprint(df[' ROA(C) before interest and depreciation before interest'].plot(kind='kde'))","8928cada":"#to get the X and y\nX=df[features].values\ny=df['Bankrupt?'].values.astype(int)\nprint('X shape:',X.shape)\nprint('y shape:',y.shape)","f96cb926":"from math import e\nimport math\nk=np.log(X.shape[0]) #\u9ed8\u8ba4\u4ee5e\u4e3a\u5e95\nprint('k:{}'.format(k))\nif (X.shape[0])\/k>3*(X.shape[1]):\n    print(\"meet the condition\")\nprint(\"the final k value is :\",round(k))","e61ed9bb":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb","88d0b77d":"print(\"start\uff1a********************************\")\nstart = time.time()\n\nK = 9 \nskf = StratifiedKFold(n_splits=K,shuffle=True,random_state=2018)\n\nauc_cv = []\npred_cv = []\n\nfor k,(train_in,test_in) in enumerate(skf.split(X,y)):\n    X_train,X_test,y_train,y_test = X[train_in],X[test_in],\\\n                                    y[train_in],y[test_in]\n    \n    # The data structure\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n    # Set the parameters\n    params = {\n                'boosting': 'gbdt',\n                'objective':'binary',\n                'verbosity': -1,\n                'learning_rate': 0.01,\n                'metric': 'auc',\n                'num_leaves':17 ,\n                'min_data_in_leaf': 26, \n                'min_child_weight': 1.12,\n                'max_depth': 9,\n                \"feature_fraction\": 0.91,\n                \"bagging_fraction\": 0.82,\n                \"bagging_freq\": 2,\n                }\n\n    print('................Start training..........................')\n    # train\n    gbm = lgb.train(params,\n                    lgb_train,\n                    num_boost_round=2000,\n                    valid_sets=lgb_eval,\n                    early_stopping_rounds=100,\n                    verbose_eval=100)\n\n    print('................Start predict .........................')\n    # Predict\n    y_pred = gbm.predict(X_test,num_iteration=gbm.best_iteration)\n    # Evaluate\n    tmp_auc = roc_auc_score(y_test,y_pred)\n    auc_cv.append(tmp_auc)\n    print(\"valid auc:\",tmp_auc)\n    # Test\n    pred = gbm.predict(X, num_iteration = gbm.best_iteration)\n    pred_cv.append(pred) \n    \n# the mean auc score of StratifiedKFold\nprint('the cv information:')\nprint(auc_cv)\nlgb_mean_auc = np.mean(auc_cv)\nprint('cv mean score',lgb_mean_auc)\n\nend = time.time()\nlgb_practice_time=end-start\nprint(\"......................run with time: {} s\".format(lgb_practice_time)  )\nprint(\"over:*********************************\")\n\n# turn into array\nres =  np.array(pred_cv)\nprint(\"rusult\uff1a\",res.shape)\n# mean the result\nr = res.mean(axis = 0)\nprint('result shape:',r.shape)\nresult = pd.DataFrame()\nresult['company_id'] = range(1,df.shape[0]+1)\nresult['pred_prob'] = r","486c72d1":"# Displays the features of the Top30\nlgb.plot_importance(gbm,max_num_features = 30,figsize=(20,10))\nplt.show()","f0a3ee99":"# Rank features by importance\ndf1 = pd.DataFrame({'feature': features,'importance': gbm.feature_importance()}).sort_values(by='importance',ascending = False) \nuse = df1.loc[df1['importance']!=0,'feature'].tolist()\nprint('Number of useful features:',len(use))","bb691728":"df1.head(10)","f139c719":"result","74130e8c":"total_auc = roc_auc_score(y,result['pred_prob'])\nprint(total_auc)","52b46b23":"### 5.Model training and evaluation","22c5ebe2":"Tree models (which belong to probabilistic models) do not need normalization because they do not care about the values of variables, but about the distribution of variables and the conditional probabilities between variables. Rank processing of numerical features is carried out here to ensure the robustness of the model to abnormal data, improve the stability of the model, reduce the risk of overfitting, and improve the prediction accuracy of enterprises in different industries from the perspective of economics.","2666e55a":"1. Train set\/test set  \n  StratifiedKfold method is used for data division (train_test_split can also be used)\n2. Model selection   \n  LGB model was adopted\n3. Evaluation indicators   \n  AUC&F1","56c4b0a8":"4.Feature selection\n\nThe embedded method based on tree model was adopted. Firstly, all the features were trained, and then the topK features were selected for training and analysis according to the importance of the features obtained from the model.\n\nSpecifically, the parameter FEATURE_FRaction is set in the LightTGBM model. If the FEATURE_FRaction is less than 1.0, LightTGBM will randomly select some features in each iteration. For example, if set to 0.8, 80% of the features will be selected before each tree is trained.\n\n5.Feature extraction\n\nFeature extraction is not done here, because feature extraction in economic and management research will make the feature space lose its original connotation and greatly reduce the interpretability. Generally, feature extraction is used in computer competitions to improve the index value.","8f67bcc6":"1. Missing value analysis\n\n  According to the statistics, there are no missing values in 95 feature columns, which have relatively complete features and little interference to model prediction. So instead of removing any of the original features, use automatic model selection, as the model can automatically assign features of the weight.\n\n2. Deal with outliers","51c23339":"### 4.Feature engineering","256a70c8":"5.Put in the parameters after Bayesian parameter adjustment to calculate, K-fold cross validation combined with LGB","6be15ff5":"  4.  K fold cross validation -- determine the value of K  \n\nReasons for choosing cross validation:  \n   Root cause: due to limited data, it is easy to overfit if the data is used solely for training the model.\n\n   Theory: With cross-validation, the model variance \"should\" be reduced and the generalization ability of the model can be improved. We expect the model to perform well on multiple sub-data sets of the training set, rather than on the whole training data set alone.\n\n   From the perspective of variance deviation: K=1, cross validation is not used at all, so the data are used for training, and the model is prone to over-fitting, which is characterized by low deviation and high variance. If K=n, leave one method, the overall deviation of the model increases and the variance decreases.\n\nA 2017 study suggested an alternative empirical choice, suggesting k=log(n) and guaranteed n\/ k >3d, where n represents the amount of data and d represents the number of features","5f3d6917":"### 1.Data import and call packages","07a95bb8":"The top10 features of importance are: \n1. Interest-bearing debt interest rate \n2. Borrowing dependency \n3. Persistent EPS in the Last Four Seasons \n4. Accounts Receivable Turnove \n5. Net Value Growth Rate \n6. Total debt\/Total net worth \n7. Non-industry income and expenditure\/revenue \n8. Inventory\/Working Capital \n9. Cash\/Total Assets \n10. Quick Ratio     \n\nWe could draw a conclusion that in the economic sense, the above ten points and corporate bankruptcy can be explained very strongly.","23ea964c":"1.Design feature Altman Z score according to Edward Altman in 1968\n\n2.One-hot coding of category variables\n   \n  For category variables, the general model (LR,SVM...) do one-hot coding, however, for integrated tree models such as LGB and XGB, one-hot coding can be done without one-hot processing. According to LGB official website documents, one-hot coding is not a good solution because of the category characteristics with large cardinality. The LGB learning tree grows very imbalanced and requires a very deep depth to achieve good accuracy. And because of the category variables has been changed into numerical variables in the data, we don't handle with this part.\n  \n3.Rank processing of numerical features\n\nRank processing is carried out on numerical features to ensure the robustness of the model to abnormal data, improve the stability of the model and reduce the risk of overfitting. In the sense of economics, the post-Rank normalization treatment is also beneficial to unify the standards of bankrupt enterprises in different industries.","8de85c80":"### 6.Results display and analysis","1cf3102a":"### 2.Data analysis","1bbfec5d":"### 3.Data preprocessing","eff57060":"1. data.csv, is a training dataset with label, whose length is  6819.\n\n2. There are 95 dimensions in training data, and the leaved column 'Bankrupt?' is the target to predict, regarded as the y of data.\n\n3. The 'Bankrupt?' column consists of label '1' and '0', and the label '1' points to the bankrupt condition of company, the label '0' means not bankrupt. The number of bankrupt company : not bankrupt company =1:30, as we turn it into binary classifier problem.m"}}