{"cell_type":{"19caa93e":"code","a1135af4":"code","194df341":"code","063dbd9f":"code","de352bed":"code","6c3d1415":"code","dbdf5674":"code","257a7d11":"markdown","35c2de1a":"markdown","d2164c9a":"markdown","7bf3f6c9":"markdown","4d428414":"markdown","d840c20b":"markdown","96778964":"markdown"},"source":{"19caa93e":"# Install:\n# GFootball environment (https:\/\/github.com\/google-research\/football\/),\n# SEED RL for training an agent (https:\/\/github.com\/google-research\/seed_rl\/),\n# Tensorflow 2.2, which is needed by SEED RL.\n\n!apt-get update\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n!pip3 install tensorflow==2.2\n!pip3 install tensorflow_probability==0.9.0\n\n# Update kaggle-environments to the newest version.\n!pip3 install kaggle-environments -U\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -b v2.3 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n\n!wget https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.3.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .\n\n!git clone https:\/\/github.com\/google-research\/seed_rl.git\n!cd seed_rl && git checkout 34fb2874d41241eb4d5a03344619fb4e34dd9be6\n!mkdir \/kaggle_simulations\/agent","a1135af4":"%%writefile train.sh\n# Training launcher script.\n\n# Make SEED RL visible to Python.\nexport PYTHONPATH=$PYTHONPATH:$(pwd)\nENVIRONMENT=$1\nAGENT=$2\nNUM_ACTORS=$3\nshift 3\n\n# Start actor tasks which run environment loop.\nactor=0\nwhile [ \"$actor\" -lt ${NUM_ACTORS} ]; do\n  python3 seed_rl\/${ENVIRONMENT}\/${AGENT}_main.py --run_mode=actor --logtostderr $@ --num_actors=${NUM_ACTORS} --task=${actor} 2>\/dev\/null >\/dev\/null &\n  actor=$(( actor + 1 ))\ndone\n# Start learner task which performs training of the agent.\npython3 seed_rl\/${ENVIRONMENT}\/${AGENT}_main.py --run_mode=learner --logtostderr $@ --num_actors=\"${NUM_ACTORS}\"\n","194df341":"!bash train.sh football vtrace 4 '--total_environment_frames=10000 --game=11_vs_11_kaggle --reward_experiment=scoring,checkpoints --logdir=\/kaggle_simulations\/agent\/'","063dbd9f":"!ls -la \/kaggle_simulations\/agent\/saved_model","de352bed":"%%writefile \/kaggle_simulations\/agent\/main.py\n\nimport collections\nimport gym\nimport numpy as np\nimport os\nimport sys\nimport tensorflow as tf\n\nfrom gfootball.env import observation_preprocessing\nfrom gfootball.env import wrappers\n\nEnvOutput = collections.namedtuple(\n    'EnvOutput', 'reward done observation abandoned episode_step')\n\ndef prepare_agent_input(observation, prev_action, state):\n    # SEED RL agent accepts input in a form of EnvOutput. When not training\n    # only observation is used for generating action, so we use a dummy values\n    # for the rest.\n    env_output = EnvOutput(reward=tf.zeros(shape=[], dtype=tf.float32),\n        done=tf.zeros(shape=[], dtype=tf.bool),\n        observation=observation, abandoned=False,\n        episode_step=tf.zeros(shape=[], dtype=tf.int32))\n    # add batch dimension\n    prev_action, env_output = tf.nest.map_structure(\n        lambda t: tf.expand_dims(t, 0), (prev_action, env_output))\n\n    return (prev_action, env_output, state)\n\n# Previously executed action\nprevious_action = tf.constant(0, dtype=tf.int64)\n# Queue of recent observations (SEED agent we trained uses frame stacking).\nobservations = collections.deque([], maxlen=4)\n# Current state of the agent (used by recurrent agents).\nstate = ()\n\n# Load previously trained Tensorflow model.\npolicy = tf.compat.v2.saved_model.load('\/kaggle_simulations\/agent\/saved_model')\n\ndef agent(obs):\n    global step\n    global previous_action\n    global observations\n    global state\n    global policy\n    # Get observations for the first (and only one) player we control.\n    obs = obs['players_raw'][0]\n    # Agent we trained uses Super Mini Map (SMM) representation.\n    # See https:\/\/github.com\/google-research\/seed_rl\/blob\/master\/football\/env.py for details.\n    obs = observation_preprocessing.generate_smm([obs])[0]\n    if not observations:\n        observations.extend([obs] * 4)\n    else:\n        observations.append(obs)\n    \n    # SEED packs observations to reduce transfer times.\n    # See PackedBitsObservation in https:\/\/github.com\/google-research\/seed_rl\/blob\/master\/football\/observation.py\n    obs = np.packbits(obs, axis=-1)\n    if obs.shape[-1] % 2 == 1:\n        obs = np.pad(obs, [(0, 0)] * (obs.ndim - 1) + [(0, 1)], 'constant')\n    obs = obs.view(np.uint16)\n\n    # Execute our agent to obtain action to take.\n    agent_output, state = policy.get_action(*prepare_agent_input(obs, previous_action, state))\n    previous_action = agent_output.action[0]\n    return [int(previous_action)]","6c3d1415":"from kaggle_environments import make\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"11_vs_11_kaggle\", \"running_in_notebook\": True})\nenv.run([\"\/kaggle_simulations\/agent\/main.py\", \"run_right\"])\nenv.render(mode=\"human\", width=800, height=600)","dbdf5674":"# Prepare a submision package containing trained model and the main execution logic.\n!cd \/kaggle_simulations\/agent && tar -czvf \/kaggle\/working\/submit.tar.gz main.py saved_model","257a7d11":"Now we can easily visualize behavior of our agent in action:","35c2de1a":"# Submit to Competition\n1. \"Save & Run All\" (commit) this Notebook\n1. Go to the notebook viewer\n1. Go to \"Data\" section and find submit.tar.gz file.\n1. Click \"Submit to Competition\"\n1. Go to [My Submissions](https:\/\/www.kaggle.com\/c\/football\/submissions) to view your score and episodes being played.","d2164c9a":"Lets first try to visualize a game played by our trained agent. For that we need to implement a wrapper which loads Tensorflow model and converts observations provided by Kaggle environment to observations accepted by the SEED agent:","7bf3f6c9":"SEED RL provides scripts for running training on [local machine inside Docker](https:\/\/github.com\/google-research\/seed_rl\/#local-machine-training-on-a-single-level) and [distributed training](https:\/\/github.com\/google-research\/seed_rl\/#distributed-training-using-ai-platform) at scale using [AI Platform](https:\/\/cloud.google.com\/ai-platform). To make it run inside a notebook we need to create a launcher script (based on [SEED's docker launcher script](https:\/\/github.com\/google-research\/seed_rl\/blob\/master\/docker\/run.sh)):","4d428414":"Now we can run the training for the Kaggle competition scenario (11_vs_11_kaggle). As this example is meant to be interactive, we train for  10000 steps, which doesn't provide a good quality agent, but training should take only a few minutes.","d840c20b":"# SEED RL agent to play GFootball\nIn this notebook we present a way to train V-trace off-policy actor-critic agent introduced in [IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures](https:\/\/arxiv.org\/pdf\/1802.01561.pdf) using [SEED RL](https:\/\/github.com\/google-research\/seed_rl\/) framework.\nAgent trained in this notebook can serve as a starting point, but as the main goal is to provide an interactive introduction into applying Deep-RL to GFootball, obtained agent is far from comprehensive. See the [Google Research Football: A Novel Reinforcement Learning Environment](https:\/\/arxiv.org\/abs\/1907.11180) paper for details on aplying V-trace to GFootball.\n\nThe first step is to install required tools:","96778964":"At the end of the training Tensorflow model is saved for later use."}}