{"cell_type":{"ccf802a3":"code","5f83bd1e":"code","b3bedb44":"code","c31dc76f":"code","03acf21e":"code","2fd6e624":"code","7aa4d152":"code","0683798e":"code","a96613fd":"code","5ddafad5":"code","3cb48e14":"code","a4e25319":"code","6520da6e":"code","d5966901":"code","7de44e67":"code","983df9ee":"code","05e47fd6":"code","2cc45304":"code","fa4277e9":"code","29bbd6f9":"code","ab21c247":"code","cab30664":"code","0f0654d9":"code","00305be7":"code","72efc821":"code","3b6e32f0":"code","0d38223f":"code","5e23dd6e":"code","587555a4":"code","ef6ca2e6":"code","f7222c68":"code","208a9361":"code","a08fce40":"code","628b4fb5":"code","ac0fdab8":"code","ff3889ed":"code","d8a36737":"code","b25098f4":"code","404d74d9":"code","dd5a3f08":"code","67098a12":"code","1e42684a":"code","d7876e7f":"code","47d86e17":"code","d9d46792":"code","2f823f1d":"code","9ed5a0b4":"code","42a9809f":"code","ac76ef40":"code","c3244215":"code","1dc8b22d":"code","e7c91684":"code","f02941e3":"code","3a35d774":"code","5eb4d7f8":"code","e836342c":"code","4906370b":"code","0d028c01":"code","4c9ac11a":"code","1c9fac4d":"code","a39afcd6":"code","e3181f6f":"code","178706b5":"code","defffb04":"code","8215ccbc":"code","1c0a37fe":"code","185ad44b":"code","f5a0fd55":"code","58443d66":"code","0467e01f":"code","d04f6fdf":"code","e6c9c00a":"code","8ea3c329":"code","0c43ffad":"code","9fb75e20":"markdown","de3b1747":"markdown","9381bfc2":"markdown","2cedf713":"markdown","e03d5508":"markdown","30abee27":"markdown","4d832134":"markdown","c16d9f6f":"markdown","5e0d9caf":"markdown","fe69ca33":"markdown","c45186c7":"markdown","61435a78":"markdown","e3bacb9f":"markdown","31e5a4ff":"markdown","a73c130b":"markdown","97be0ced":"markdown","6d13ffad":"markdown","30da5035":"markdown","6741326e":"markdown","36a20ba5":"markdown","d73b67ce":"markdown","6afa4148":"markdown","49bfe360":"markdown","10903e41":"markdown","4b12a086":"markdown","f1ffcc41":"markdown","29894ee6":"markdown","f1c77771":"markdown","faac3cd0":"markdown","67d36846":"markdown","9eb2d64a":"markdown","062c54f9":"markdown","96209b1a":"markdown","8a7c4da5":"markdown","15d22785":"markdown","a3f9d10d":"markdown","677eca01":"markdown","2cc91429":"markdown","969fd1ba":"markdown","a49fdfe3":"markdown","c5017dd6":"markdown","86baba65":"markdown","d52af2e7":"markdown","dd6c65a5":"markdown","03df5566":"markdown"},"source":{"ccf802a3":"import sqlite3\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns',None)","5f83bd1e":"path = \"..\/input\/soccer\/null\"  #Insert path here\ndatabase = path + 'database.sqlite'\n\n\n\nconn = sqlite3.connect('..\/input\/soccer\/database.sqlite')\n\ntables = pd.read_sql(\"\"\"SELECT *\n                        FROM sqlite_master\n                        WHERE type='table';\"\"\", conn)\n\n","b3bedb44":"df = pd.read_sql_query(\"SELECT * FROM Player_Attributes\", conn)","c31dc76f":"df.head()","03acf21e":"print('Total Rows present: ',df.shape[0])\nprint('Total Columns present: ',df.shape[1])","2fd6e624":"# Total 47301 Null values are present in different columns\ndf.isnull().sum().sum()","7aa4d152":"# Missing values in each columns\ndf.isnull().sum()","0683798e":"# Null values in percentage\ndf.isnull().sum()\/df.shape[0]*100","a96613fd":"df1 = df[['overall_rating','attacking_work_rate','crossing','heading_accuracy','vision','dribbling','sliding_tackle']].isnull().sum().T\/df.shape[0]*100\nplt.figure(figsize=(15,5))\nplt.bar(df1.index, df1)\nplt.title('Missing Values in %')","5ddafad5":"df = df.dropna(axis=0 , subset=['overall_rating'])\ndf.shape  # Current dimensions of Dataset","3cb48e14":"print('Total null values present in the dataset: ',df.isnull().sum().sum())","a4e25319":"df = df.dropna(axis=0, subset=['volleys'])  \n# Dropping all the rows contains Null values in 'volleys' columns\n# since most of the columns contains null in same records","6520da6e":"df.isnull().sum()","d5966901":"df['attacking_work_rate'].value_counts()","7de44e67":"df['attacking_work_rate'].mode()[0]","983df9ee":"# Imputing attacking_work_rate with mode i.e. 'Medium'\ndf.loc[ df['attacking_work_rate'].isnull() , 'attacking_work_rate' ] = df['attacking_work_rate'].mode()[0]","05e47fd6":"print('Total Missing values present now: ',df.isnull().sum().sum())","2cc45304":"# Shape of the Data Set\ndf.shape","fa4277e9":"df = df.reset_index(drop=True)","29bbd6f9":"df = df.drop(columns=['id','player_fifa_api_id','player_api_id','date'])\ndf.head()","ab21c247":"df.shape","cab30664":"df['attacking_work_rate'].value_counts()","0f0654d9":"df['defensive_work_rate'].value_counts()","00305be7":"pd.crosstab(df['attacking_work_rate'], df['defensive_work_rate'])","72efc821":"# Changing Classes name in attacking_work_rate\ndf['attacking_work_rate'].replace({'le':'lean','norm':'normal','stoc':'stocky','y':'yes'}, inplace=True)","3b6e32f0":"# Changing Classes name in attacking_work_rate\ndf['defensive_work_rate'].replace({'ean':'lean','ormal':'normal','tocky':'stocky','es':'yes','o':'None','_0':'medium'}, inplace=True)","0d38223f":"# Imputing numeric classes to None\ndf.loc[ df['defensive_work_rate'].isin(list('0123456789')) , 'defensive_work_rate' ] = 'None'","5e23dd6e":"df['attacking_work_rate'].value_counts()","587555a4":"# All the classes have same name.\ndf['defensive_work_rate'].value_counts()","ef6ca2e6":"df = pd.get_dummies(df, drop_first=True)\ndf.head()","f7222c68":"x = df.drop(columns='overall_rating')\ny = df[['overall_rating']]","208a9361":"x_num = x.iloc[:,:-15]\nx_cat = x.iloc[:,-15:]","a08fce40":"for i in x_num.columns[:3]:\n    sns.boxplot(x_num[i])\n    plt.show()","628b4fb5":"for i in x_num.columns:\n    q1 = x_num[i].quantile(0.25)\n    q3 = x_num[i].quantile(0.75)\n    \n    iqr = q3-q1\n    \n    ub = q3 + iqr*1.5\n    lb = q1 - iqr*1.5\n    \n#     x_num.loc[ (x_num[i] > ub) , i] = x_num[i].quantile(0.99)\n    x_num.loc[ (x_num[i] < lb) , i] = x_num[i].quantile(0.01)","ac0fdab8":"for i in x_num.columns[:3]:\n    sns.boxplot(x_num[i])\n    plt.show()","ff3889ed":"x = pd.concat([x_num, x_cat], axis=1)","d8a36737":"import statsmodels.api as sm\n\nxc = sm.add_constant(x)\nols1 = sm.OLS(y, xc)\nols_mod = ols1.fit()","b25098f4":"ols_mod.summary()","404d74d9":"# Removing categorical features and their respective labels\nx1 = x.iloc[:,:-15]","dd5a3f08":"# Creating model again\nxc = sm.add_constant(x1)\nols1 = sm.OLS(y, xc)\nols_mod = ols1.fit()","67098a12":"ols_mod.summary()","1e42684a":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score","d7876e7f":"lr = LinearRegression()\nkf = KFold(n_splits= 5, shuffle = True)\nmse = cross_val_score(lr, x, y, cv=5, scoring='neg_mean_squared_error')\nrmse = np.sqrt(abs(mse))\nnp.mean(rmse), np.std(rmse)","47d86e17":"lr = LinearRegression()\nkf = KFold(n_splits= 5, shuffle = True)\nmse = cross_val_score(lr, x1, y, cv=5, scoring='neg_mean_squared_error')\nrmse = np.sqrt(abs(mse))\nnp.mean(rmse), np.std(rmse)","d9d46792":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error","2f823f1d":"xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=42)","9ed5a0b4":"lr = LinearRegression()\nlr.fit(xtrain, ytrain)\n\nytrain_pred = lr.predict(xtrain)\nytest_pred = lr.predict(xtest)\n\nprint('Train R2: ',r2_score(ytrain, ytrain_pred))\nprint('Test R2: ',r2_score(ytest, ytest_pred))\n\nprint('Train RMSE: ',mean_squared_error(ytrain, ytrain_pred)**0.5)\nprint('Test R2: ',mean_squared_error(ytest, ytest_pred)**0.5)","42a9809f":"xtrain, xtest, ytrain, ytest = train_test_split(x1, y, test_size=0.3, random_state=42)","ac76ef40":"lr = LinearRegression()\nlr.fit(xtrain, ytrain)\n\nytrain_pred = lr.predict(xtrain)\nytest_pred = lr.predict(xtest)\n\nprint('Train R2: ',r2_score(ytrain, ytrain_pred))\nprint('Test R2: ',r2_score(ytest, ytest_pred))\n\nprint('Train RMSE: ',mean_squared_error(ytrain, ytrain_pred)**0.5)\nprint('Test R2: ',mean_squared_error(ytest, ytest_pred)**0.5)","c3244215":"from sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import GridSearchCV","1dc8b22d":"from sklearn.preprocessing import StandardScaler","e7c91684":"st = StandardScaler()\nx1 = pd.DataFrame(st.fit_transform(x1) , columns=x1.columns)","f02941e3":"rid = Ridge()\nparam = {'alpha':[0.00005,0.00008,0.0001,0.001,0.01,0.1,0.5,1,2,3,4,5,6,7,8,9,10,20,30]}\n\ngrid = GridSearchCV(rid, param_grid=param, cv=5, scoring='neg_mean_squared_error' )\n\nrid_mod=grid.fit(x1,y)\n\nprint(rid_mod.best_params_)\nprint(abs(rid_mod.best_score_))","3a35d774":"rid = Ridge(**rid_mod.best_params_)\nmse = cross_val_score(rid, x, y, cv=5, scoring='neg_mean_squared_error')\nrmse = np.sqrt(abs(mse))\nnp.mean(rmse), np.std(rmse)","5eb4d7f8":"las = Lasso()\nparam = {'alpha':[0.00005,0.0008,0.0001,0.001,0.01,0.1,0.5,1,2,3,4,5,6,7,8,9,10,20,30]}\n\ngrid = GridSearchCV(las, param_grid=param, cv=5, scoring='neg_mean_squared_error' )\n\nlas_mod=grid.fit(x1,y)\n\nprint(las_mod.best_params_)\nprint(abs(las_mod.best_score_))","e836342c":"las = Lasso(**las_mod.best_params_)\nmse = cross_val_score(las, x, y, cv=5, scoring='neg_mean_squared_error')\nrmse = np.sqrt(abs(mse))\nnp.mean(rmse), np.std(rmse)","4906370b":"from sklearn.model_selection import train_test_split","0d028c01":"xtrain, xtest, ytrain, ytest = train_test_split(x1, y, test_size=0.3, random_state=42)\nxtrain.shape, xtest.shape, ytrain.shape, ytest.shape","4c9ac11a":"st = StandardScaler()\n\nxtrain = pd.DataFrame(st.fit_transform(xtrain), columns = xtrain.columns)\n\nxtest = pd.DataFrame(st.transform(xtest), columns = xtest.columns)\n\nxtrain.head()","1c9fac4d":"cov_matrix =  np.cov(xtrain.T)  # We use transpose, because np.cov function will by default take rows for finding covariance\ncov_matrix.shape","a39afcd6":"eig_vals, eig_vecs = np.linalg.eig(cov_matrix) \neig_vals.shape, eig_vecs.shape","e3181f6f":"# Array of eigen values. But this is not in sorted order.\neig_vals","178706b5":"# Fetching eigen values and their cooresponding eigen vectors so that we can later sort them on the basis of eigen values\n\n# Fetching eigen vector. they are columns\neigen_pairs = [(np.abs(eig_vals[i]), eig_vecs[ :, i]) for i in range(eig_vecs.shape[0])]","defffb04":"eigen_pairs_sorted = sorted(eigen_pairs,reverse=True)\neigen_pairs_sorted[:2]  # Showing first 2 eigen pairs","8215ccbc":"# Total sum of eigen value\ntot = sum(eig_vals)\n\n# List contain percenatage wise eigen value contain\nvar_exp = [round(( i \/tot )*100,3) for i in sorted(eig_vals, reverse=True)]\nprint('Variance of each eigen vector:\\n',var_exp)\n\n# List contains cumulative eigen value for each eigen vector starting from the first eigen vector's eigen value\ncum_var_exp = np.cumsum(var_exp)\nprint(\"\\nCumulative Variance Explained\", cum_var_exp)\n# The last 4 features are contributing very less amount, similar to 1.4% of total variance","1c0a37fe":"plt.figure(figsize=(15,6))\nsns.barplot(np.arange(1,cum_var_exp.shape[0]+1) , cum_var_exp)","185ad44b":"# Extract the eigenvalues and eigenvectors from the eigen pair (sorted)\neigvalues_sort = [eigen_pairs_sorted[j][0] for j in range(len(eig_vals))]\neigvectors_sort = [eigen_pairs_sorted[j][1] for j in range(len(eig_vals))]\n\n# Here the aigen vectors are row wise. we have to again change them to columns-wise","f5a0fd55":"# Eigen vectors columns wise\neig_vect =  np.array(eigvectors_sort).transpose()","58443d66":"feat_importance =  pd.DataFrame({'feature':xtrain.columns , 'PCA1_weight':eig_vect[:,0], 'PCA1_abs':abs(eig_vect[:,0])})\nfeat_importance.sort_values(by='PCA1_abs', ascending=False).head()","0467e01f":"xtrain_pca = np.dot(xtrain, eig_vect)\n\nxtest_pca = np.dot(xtest, eig_vect)\n\nxtrain_pca[0]  # Values of 1st record on PCA scale.","d04f6fdf":"from sklearn.decomposition import PCA","e6c9c00a":"pc = PCA()\nxtr_pca = pc.fit_transform(xtrain)\nxtt_pca = pc.transform(xtest)\nxtr_pca[0]","8ea3c329":"lr = LinearRegression()\nlr.fit(xtrain_pca, ytrain)\n\nytrain_pred = lr.predict(xtrain_pca)\nytest_pred = lr.predict(xtest_pca)\n\nprint('Train R2: ',r2_score(ytrain, ytrain_pred))\nprint('Test R2: ',r2_score(ytest, ytest_pred))\n\nprint('Train RMSE: ',mean_squared_error(ytrain, ytrain_pred)**0.5)\nprint('Test R2: ',mean_squared_error(ytest, ytest_pred)**0.5)","0c43ffad":"lr = LinearRegression()\nlr.fit(xtrain_pca[:,:18], ytrain)\n\nytrain_pred = lr.predict(xtrain_pca[:,:18])\nytest_pred = lr.predict(xtest_pca[:,:18])\n\nprint('Train R2: ',r2_score(ytrain, ytrain_pred))\nprint('Test R2: ',r2_score(ytest, ytest_pred))\n\nprint('Train RMSE: ',mean_squared_error(ytrain, ytrain_pred)**0.5)\nprint('Test R2: ',mean_squared_error(ytest, ytest_pred)**0.5)","9fb75e20":"Around 18 PCA are able to explain 95% of the total variance.","de3b1747":"## Removing Redundant Features","9381bfc2":"### Extracting Eigen values and eigen vector back","2cedf713":"### Importing PCA","e03d5508":"Target variable contains 836 null values, its better to remove those.","30abee27":"### LR model without categorical features","4d832134":"As we can see that p-value of mostly all the categorical features are more than 0.05 and value of condition no is around 10^16, which is showing high multicollinearity. Lets remove all these categorical features and their label encoding.","c16d9f6f":"`As we can see that`, values of the record using manual calculation and using PCA are exactly same.","5e0d9caf":"## Treating Null Values","fe69ca33":"### Removing rows which contains Null Values in Target Column (Overall Rating)","c45186c7":"### Cross validation without categorical features","61435a78":"* Multicollineairy is decreased from 10^16 to 4650. Suggesting the categorical feature we removed have lower variance among them with higher correlation.\n\n* All the features now have less than 0.05 p-value suggesting all these features are statistically significant.","e3bacb9f":"### Conclusion\n#### After using regularization and PCA the result is not getting improve, since the already the model is very rightly fit.","31e5a4ff":"#### Using all PCA Model","a73c130b":"## Principal Component Analysis","97be0ced":"### Transforming data in PCA space.","6d13ffad":"### Creating covariance matrix","30da5035":"### Top 5 features impacting PCA1","6741326e":"We have 3 categorical variable in the data set namely\n1. preferred_foot : It consists of 2 category i.e. Right and Left. After encoding, right will be termed as 1 and left as 0.\n2. attacking_work_rate and defensive_work_rate will create a new columns for each category in it.","36a20ba5":"Since our main aim is to predict the overall rating of the players, based upon their performances in different aspects of athletics, We will remove, some unique features like is, api_id as well as date.","d73b67ce":"<b> There is very little to no sign of overfitting <b>","6afa4148":"## Regularization","49bfe360":"### Cross validation with all the features including categorical","10903e41":"### LR model with all features","4b12a086":"## Cross Validation of Linear Regression Model","f1ffcc41":"### Scaling the dat before regularization","29894ee6":"### Lasso Regularization","f1c77771":"### Scaling of Original Features","faac3cd0":"### Eigen Values and Eigen Vectors","67d36846":"### Total number of rows and columns present in the data set","9eb2d64a":"### Removing Null values from Other columns","062c54f9":"## Creating a Base model","96209b1a":"## Fitting a Base Linear regression Model","8a7c4da5":"### Splitting in test and train","15d22785":"## Creating Dummy columns for Categorical Variable","a3f9d10d":"Assuming some mistakes while taking data, it is assumed that some of the cataegories from attacking_work_rate is present in the defensive_work_rate like\n* stoc - tockey --> stockey (a person who has short height but borad chest and shoulders)\n* norm - ormal --> normal\n* le - ean --> lean\n* None - o --> None\n","677eca01":"### Cumulative sum of eigen values","2cc91429":"Since we got our eigen vectors for PCA, we can check that which original feature is given most importance for PCA1","969fd1ba":"#### Using only 18 PCA components","a49fdfe3":"<b>There is very limited effect of excluding categorical features. Bias error is increased by 0.02 and variance error decreases by 0.001. This may be due to the fact that the columns we have excluded are having higher amount of multicollinearity. <b>","c5017dd6":"## Importing Data Set","86baba65":"### Ridge Regularization","d52af2e7":"There are some rows which contains null values in most number of columns, so its better to drop them.","dd6c65a5":"### Creating LR model","03df5566":"<b> R2- score and RMSE of train and test data are almost equal inferencing that the model is not overfit. <b>"}}