{"cell_type":{"c6494958":"code","ea4f9bda":"code","71363ad1":"code","34c98b2c":"code","43638318":"code","786a1a94":"code","efe4c882":"code","5a9fe5e2":"code","840ac456":"markdown","7d7acf6e":"markdown","bfda7afa":"markdown","d674dac5":"markdown","4fa4cecd":"markdown","858bc06d":"markdown","52b50982":"markdown","3328096a":"markdown"},"source":{"c6494958":"import numpy as np # linear algebra\nimport pandas as pd #\nimport numpy as np\nimport os\nimport re\nimport gensim\nimport spacy\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\nbiorxiv = pd.read_csv(\"\/kaggle\/input\/clean-csv\/biorxiv_clean.csv\")\nbiorxiv.shape\nbiorxiv.head()\n\nbiorxiv = biorxiv[['paper_id','title','text']].dropna().drop_duplicates()\npmc = pd.read_csv('\/kaggle\/input\/clean-csv-new\/clean_pmc.csv')\npmc = pmc[['paper_id','title','text']].dropna().drop_duplicates()\n\nbiorxiv = pd.concat([biorxiv,pmc]).drop_duplicates()\n\n\n","ea4f9bda":"! python -m spacy download en_core_web_sm\nimport spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()","71363ad1":"\nstemmer = SnowballStemmer(\"english\")\n\ndef text_clean_tokenize(article_data):\n    \n    review_lines = list()\n\n    lines = article_data['text'].values.astype(str).tolist()\n\n    for line in lines:\n        tokens = word_tokenize(line)\n        tokens = [w.lower() for w in tokens]\n        table = str.maketrans('','',string.punctuation)\n        stripped = [w.translate(table) for w in tokens]\n        # remove remaining tokens that are not alphabetic\n        words = [word for word in stripped if word.isalpha()]\n        stop_words = set(stopwords.words('english'))\n        words = [w for w in words if not w in stop_words]\n        words = [stemmer.stem(w) for w in words]\n\n        review_lines.append(words)\n    return(review_lines)\n    \n    \nreview_lines = text_clean_tokenize(biorxiv)","34c98b2c":"model =  gensim.models.Word2Vec(sentences = review_lines,\n                               size=100,\n                               window=2,\n                               workers=4,\n                               min_count=2,\n                               seed=42,\n                               iter= 50)\n\nmodel.save(\"word2vec.model\")","43638318":"import spacy\n\ndef tokenize(sent):\n    doc = nlp.tokenizer(sent)\n    return [token.lower_ for token in doc if not token.is_punct]\n\nnew_df = (biorxiv['text'].apply(tokenize).apply(pd.Series))\n\nnew_df = new_df.stack()\nnew_df = (new_df.reset_index(level=0)\n                .set_index('level_0')\n                .rename(columns={0: 'word'}))\n\nnew_df = new_df.join(biorxiv.drop('text', 1), how='left')\n\nnew_df = new_df[['word','paper_id','title']]\nword_list = list(model.wv.vocab)\nvectors = model.wv[word_list]\nvectors_df = pd.DataFrame(vectors)\nvectors_df['word'] = word_list\nmerged_frame = pd.merge(vectors_df, new_df, on='word')\nmerged_frame_rolled_up = merged_frame.drop('word',axis=1).groupby(['paper_id','title']).mean().reset_index()\ndel merged_frame\ndel new_df\ndel vectors","786a1a94":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\ndata_subset = merged_frame_rolled_up.drop(['paper_id','title'],axis=1).values\n\nfrom sklearn.cluster import KMeans\nnumber_of_clusters = range(1,20,5)\nkmeans = [KMeans(n_clusters=i,max_iter=50,init='k-means++') for i in number_of_clusters]\nscore = [kmeans[i].fit(data_subset).inertia_ for i in range(len(kmeans))]\ntmp =0\nbest_k = 0\nvalue_all = 0\ndiff1 = []\nfor i in range(len(score)-1):\n    \n    scores = (score[i] - score[i+1])\n    diff1.append(scores)\n    \ndiff2=[]\nfor i in range(len(diff1)-1):\n    difference = diff1[i] - diff1[i+1]\n    diff2.append(difference)\ndiff2.insert(0, 0) \ndiff3 = [i-j for i,j in zip(diff2,diff1)]\n\nm = max(i for i in diff3)\nbest_k = number_of_clusters[diff3.index(m)]\nprint(best_k)\n\nkmeans = KMeans(n_clusters=best_k,init='k-means++',max_iter=50).fit(data_subset)\nlabels = kmeans.labels_\nmerged_frame_rolled_up['labels'] = labels","efe4c882":"tsne = TSNE(n_components=2, verbose=1, perplexity=10, n_iter=3000,init='pca',random_state=42)\ntsne_results = tsne.fit_transform(data_subset)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\nmerged_frame_rolled_up['tsne-2d-one'] = tsne_results[:,0]\nmerged_frame_rolled_up['tsne-2d-two'] = tsne_results[:,1]\nplt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n    hue = 'labels',\n    palette=sns.color_palette(\"hls\", len(merged_frame_rolled_up['labels'].unique())),\n    data=merged_frame_rolled_up,\n    legend=\"full\",\n    alpha=0.3\n)","5a9fe5e2":"from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap\nfrom bokeh.io import output_file, show\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= merged_frame_rolled_up['tsne-2d-one'].values, \n    y= merged_frame_rolled_up['tsne-2d-two'].values, \n    desc= merged_frame_rolled_up['labels'].values, \n    titles= merged_frame_rolled_up['title'].values\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles\")\n])\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[len(merged_frame_rolled_up['labels'].unique())],\n                     low=min(merged_frame_rolled_up['labels'].values) ,high=max(merged_frame_rolled_up['labels'].values))\n#prepare the figure\np = figure(plot_width=1000, plot_height=1000, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"Clustering Medical Papers\", \n           toolbar_location=\"right\")\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\")\n\nshow(p)","840ac456":"## K Means Clustering\nAs the data set is un-labelled, its our job to find structure and label it accordingly. For this purpose, the K - Means clustering algorithm is used. 'K' or the number of clusters is chosen dynamically by looking K for which the difference between the first and second differential of change in distortion is maximum. This idea of dynamically choosing K can be found in this article. https:\/\/www.datasciencecentral.com\/profiles\/blogs\/how-to-automatically-determine-the-number-of-clusters-in-your-dat","7d7acf6e":"## Interactive Visualization","bfda7afa":"Before feeding the data into the word2vector (skip-gram) model, the text data is converted to a list object that is passed. The following code snippet removes stopwords, punctuations and stems words so as to remove noise.","d674dac5":"To ease text processing for english words, the spacy's english module library is used.","4fa4cecd":"After the numeric vector representation of each word is obtained, these are used to create numeric representations of papers. For each paper, the word2vec representations of each constituent words is found and averaged.","858bc06d":"\n## Introduction\n\nI imported the clean biorxiv file from the data conversion and cleaning kernel by xhlulu. https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv. The aim of the kernel is to use this data to cluster similar papers together.\n\n## Word2Vec\nWord2Vec is a method to represent words in a numerical - vector format such that words that are closely related to each other are close to each other in numeric vector space. This method was developed by Thomas Mikolov in 2013 at Google.\n\nEach word in the corpus is modeled against surrounding words, in such a way that the surrounding words get maximum probabilities of occurence. The mapping that allows this to happen , becomes the word2vec representation of the word. The number of surrounding words can be chosen through a model parameter called \"window size\". The length of the vector representation is chosen using the parameter 'size'.\n\nIn this notebook, the library gensim is used to construct the word2vec models","52b50982":"## Visualizing Data\nAfter each data point is assigned a cluster, the next natural step is to visualize it. But there is a problem. The data we have has over 100 columns. We will need to reduce the number of columns to at most 3 to visualize it. Here to do that, we use t-SNE, which helps keep closer points together and farther points far from each other in lower dimensions. These visualizations were made possible by using this the ideas from the kernel : https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering#Dimensionality-Reduction-with-t-SNE","3328096a":"The resulting list is then passed to the `gensim.models.Word2Vec()` function. Each word is represented by a vector that is 100 elements long.And at a time, four words surrounding the context word is used to train the model."}}