{"cell_type":{"b7f64ebf":"code","ecc434c7":"code","628f7f9e":"code","9cceb71e":"code","32847687":"code","3f101822":"code","08bce60f":"code","680f8fa5":"code","65c773b8":"code","85231cc4":"code","2e157b65":"code","b732a739":"code","9b689beb":"code","fbde111b":"code","3f91d9ad":"code","17825b5b":"code","06cf79fc":"code","7dc9c7b2":"code","b37cb361":"code","131a0cc1":"code","19ac9e65":"code","8f529258":"code","8ed1a366":"code","dc687a18":"code","1c7284c4":"markdown","46f71b30":"markdown","83f4e942":"markdown","bc8be4ca":"markdown"},"source":{"b7f64ebf":"%reload_ext autoreload\n%autoreload 2\n\nfrom fastai.tabular import *\nfrom fastai.callbacks import *\nimport numpy as np\nimport pandas as pd\nimport os\nimport math\nimport matplotlib.pyplot as plt\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ecc434c7":"path = Path(os.getcwd())","628f7f9e":"df = pd.read_csv('\/kaggle\/input\/bitcoin-historical-data\/coinbaseUSD_1-min_data_2014-12-01_to_2019-01-09.csv')\ndf['date'] = pd.to_datetime(df['Timestamp'],unit='s').dt.date\ndf = df.groupby('date')\ndf = df['Weighted_Price'].mean(); df = df.to_frame() #convert series to df\n\n#testing to see if logging makes the model better: https:\/\/stats.stackexchange.com\/questions\/298\/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va\ndf['Weighted_Price'] = df['Weighted_Price'].apply(lambda x: math.log(x))\nprint(df.shape); df.plot()","9cceb71e":"#What we will train the model on\ndf.iloc[:int(len(df)*.9)].plot()","32847687":"#What we will want the model to predict\ndf.iloc[int(len(df)*.9):].plot()","3f101822":"def prep_df(df,lag=1):\n  \"Extend df sideways to allow a longer more timesteps while taking advantage of fastai's dataset\/loader\"\n  df_org = df.copy(deep=True)\n\n  for i in range(lag):\n    df_lag = df_org.shift(i+1)\n    df_lag = df_lag.add_suffix('_M' + str(i+1))\n    #not taking last column of lagged values because cannot use for competition\n    df = df.merge(df_lag, left_index=True, right_index=True ,suffixes=(False, False))\n    \n    df.dropna(inplace=True)\n  \n  return df","08bce60f":"df_com = prep_df(df,5); df_com.reset_index(drop=True,inplace=True); df_com.head(2)","680f8fa5":"dep_var = 'Weighted_Price'; cat_names =[]","65c773b8":"procs = [FillMissing, Categorify, Normalize]\nvalid_idx = range(int(len(df_com)*.9), len(df_com))","85231cc4":"data = TabularDataBunch.from_df(path, df_com, dep_var, valid_idx=valid_idx, procs=procs, \n                                cat_names=cat_names,bs=64)\n\n#making shuffling false so that there is no data leakage\ndata.train_dl = data.train_dl.new(shuffle=False)","2e157b65":"#preview of data for sanity checks\nx,y = next(iter(data.train_dl))\n(cat_x,cont_x),y = next(iter(data.train_dl))\nfor o in (cat_x, cont_x, y): print(to_np(o[:5]))","b732a739":"class gen_block(nn.Module):\n  def __init__(self,n_in, n_hidden, theta_dim, n_out, bn:bool=True, ps:float=0., actn:Optional[nn.Module]=None):\n    super().__init__()\n    self.FC1 = nn.Sequential(*bn_drop_lin(n_in,n_hidden,bn,ps,actn))\n    self.FC2 = nn.Sequential(*bn_drop_lin(n_hidden,n_hidden,bn,ps,actn))\n    self.FC3 = nn.Sequential(*bn_drop_lin(n_hidden,n_hidden,bn,ps,actn))\n    self.FC4 = nn.Sequential(*bn_drop_lin(n_hidden,n_hidden,bn,ps,actn))\n    self.Fcst = nn.Sequential(*(bn_drop_lin(n_hidden,theta_dim,bn,ps,actn)+bn_drop_lin(theta_dim,n_out,bn,ps))) #forecast output shouldnt have relu\n    self.Bcst = nn.Sequential(*(bn_drop_lin(n_hidden,theta_dim,bn,ps,actn)+bn_drop_lin(theta_dim,n_in,bn,ps))) #same for backcast\n\n  def forward(self, x):\n    x1 = self.FC1(x)\n    x1 = self.FC2(x1)\n    x1 = self.FC3(x1)\n    x1 = self.FC4(x1)\n    x2 = self.Fcst(x1)\n    x3 = self.Bcst(x1)\n\n    return (x-x3, x2)","9b689beb":"class FFModel(Module):\n    \"Modified tabular model from fastai for embedding projections, if needed\"\n    def __init__(self, n_hidden, theta_dim, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n                 emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False):\n        super().__init__()\n        self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(emb_drop)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n        self.n_in  = self.n_emb + self.n_cont\n        self.n_out = out_sz\n        self.n_hidden = n_hidden\n        self.theta_dim = theta_dim\n\n        self.blk1 = gen_block(n_in=self.n_in,n_hidden=self.n_hidden,theta_dim=self.theta_dim,n_out=self.n_out,ps=ps,actn=nn.ReLU(inplace=True))\n        self.blk2 = gen_block(n_in=self.n_in,n_hidden=self.n_hidden,theta_dim=self.theta_dim,n_out=self.n_out,ps=ps,actn=nn.ReLU(inplace=True))\n        self.blk3 = gen_block(n_in=self.n_in,n_hidden=self.n_hidden,theta_dim=self.theta_dim,n_out=self.n_out,ps=ps,actn=nn.ReLU(inplace=True))\n        self.blk4 = gen_block(n_in=self.n_in,n_hidden=self.n_hidden,theta_dim=self.theta_dim,n_out=self.n_out,ps=ps,actn=nn.ReLU(inplace=True))\n        self.blk5 = gen_block(n_in=self.n_in,n_hidden=self.n_hidden,theta_dim=self.theta_dim,n_out=self.n_out,ps=ps,actn=nn.ReLU(inplace=True))\n        self.blk6 = gen_block(n_in=self.n_in,n_hidden=self.n_hidden,theta_dim=self.theta_dim,n_out=self.n_out,ps=ps,actn=nn.ReLU(inplace=True))\n\n\n    def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n\n        x, f1 = self.blk1(x)\n        x, f2 = self.blk2(x)\n        x, f3 = self.blk3(x)\n        x, f4 = self.blk4(x)\n        x, f5 = self.blk5(x)\n        x, f6 = self.blk6(x)        \n\n        x = f1+f2+f3+f4+f5+f6\n\n        if self.y_range is not None:\n            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n\n        return x","fbde111b":"def FFLearner(data:DataBunch, n_hidden, theta_dim, layers:Collection[int], emb_szs:Dict[str,int]=None, metrics=None,\n        ps:Collection[float]=None, emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, **learn_kwargs):\n    \"Get a `Learner` using `data`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n    emb_szs = data.get_emb_szs(ifnone(emb_szs, {}))\n    model = FFModel(n_hidden, theta_dim, emb_szs, len(data.cont_names), out_sz=data.c, layers=layers, ps=ps, emb_drop=emb_drop,\n                    y_range=y_range, use_bn=use_bn)\n    return Learner(data, model, metrics=metrics, **learn_kwargs)","3f91d9ad":"y_range=None\n#y_range = (df_com['Weighted_Price'].min(), df_com['Weighted_Price'].max()); print(y_range)\n#del df_com; gc.collect()","17825b5b":"learn = FFLearner(data, n_hidden=512,theta_dim=8,layers=[0], metrics=mean_absolute_error,\n            emb_drop=0, y_range=y_range, \n                  callback_fns=[ShowGraph,partial(CSVLogger, append=True)], ps=0.1)","06cf79fc":"learn.model","7dc9c7b2":"learn.lr_find(); learn.recorder.plot()","b37cb361":"learn.fit_one_cycle(4, 1e-2)\n#, callbacks=[SaveModelCallback(learn, every='epoch', monitor='mean_absolute_error')])","131a0cc1":"preds, y = learn.get_preds()\nplt.plot(preds, label = 'Predictions'); plt.plot(y, label = 'Actuals')\nplt.legend(); plt.show()","19ac9e65":"learn.lr_find(); learn.recorder.plot()","8f529258":"learn.fit_one_cycle(4, 1e-2)","8ed1a366":"preds, y = learn.get_preds()\nplt.plot(preds, label = 'Predictions'); plt.plot(y, label = 'Actuals')\nplt.legend(); plt.show()","dc687a18":"learn.save('NBEATS_LAG5_LOG')","1c7284c4":"We can see the model getting more expressive as we train more.","46f71b30":"## References\n[0] Oreshkin et al. N-BEATS: Neural basis expansion analysis for interpretable time series forecasting","83f4e942":"# Forecasting BTC using N-BEATS \nIn this kernel, I implement the N-BEATS[0] architecture that is the current SOTA for time series forecasting as far as I know. For more details on what the architecture is about, please read my medium blog on it.\n\nI modified [fastai](https:\/\/www.fast.ai\/)'s tabular model\/learner and use their one cycle learning to speed up training among other things!","bc8be4ca":"Pretty muted predictions at first"}}