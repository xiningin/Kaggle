{"cell_type":{"d8c04ead":"code","993fd76b":"code","3ff0677d":"code","d6299076":"code","31109f2a":"code","d8403ca9":"code","dd721fd0":"code","fd312416":"code","93f92054":"code","a9d439da":"code","b888ff18":"code","770698a8":"code","aa063dcc":"code","e7cd43f8":"markdown","ce1f5aca":"markdown","534571b6":"markdown"},"source":{"d8c04ead":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport zipfile\nimport sys\nimport time\n\n# Any results you write to the current directory are saved as output.","993fd76b":"!conda remove -y greenlet\n!pip install allennlp","3ff0677d":"!wget https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-development.tsv\n!wget https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-validation.tsv\n!wget https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-test.tsv\n!ls","d6299076":"from allennlp.commands.elmo import ElmoEmbedder\nfrom allennlp.data.tokenizers import word_tokenizer\nfrom sklearn.preprocessing import OneHotEncoder","31109f2a":"def get_elmo_fea(data, op, wg):\n\tdef get_nearest(slot, target):\n\t\tfor i in range(target, -1, -1):\n\t\t\tif i in slot:\n\t\t\t\treturn i\n    # op = 'models\/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n    # wg = 'models\/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n\n\telmo = ElmoEmbedder(options_file=op, weight_file=wg, cuda_device=0)\n\n\ttk = word_tokenizer.WordTokenizer()\n\ttokens = tk.batch_tokenize(data.Text)\n\tidx = []\n\n\tfor i in range(len(tokens)):\n\t\tidx.append([x.idx for x in tokens[i]])\n\t\ttokens[i] = [x.text for x in tokens[i]]\n\n\tvectors = elmo.embed_sentences(tokens)\n\n\tans = []\n\tfor i, vector in enumerate([v for v in vectors]):\n\t\tP_l = data.iloc[i].Pronoun\n\t\tA_l = data.iloc[i].A.split()\n\t\tB_l = data.iloc[i].B.split()\n\n\t\tP_offset = data.iloc[i]['Pronoun-offset']\n\t\tA_offset = data.iloc[i]['A-offset']\n\t\tB_offset = data.iloc[i]['B-offset']\n\n\t\tif P_offset not in idx[i]:\n\t\t\tP_offset = get_nearest(idx[i], P_offset)\n\t\tif A_offset not in idx[i]:\n\t\t\tA_offset = get_nearest(idx[i], A_offset)\n\t\tif B_offset not in idx[i]:\n\t\t\tB_offset = get_nearest(idx[i], B_offset)\n\n\t\temb_P = np.mean(vector[1:3, idx[i].index(P_offset), :], axis=0, keepdims=True)\n\n\t\temb_A = np.mean(vector[1:3, idx[i].index(A_offset):idx[i].index(A_offset) + len(A_l), :], axis=(1, 0),\n                        keepdims=True)\n\t\temb_A = np.squeeze(emb_A, axis=0)\n\n\t\temb_B = np.mean(vector[1:3, idx[i].index(B_offset):idx[i].index(B_offset) + len(B_l), :], axis=(1, 0),\n                        keepdims=True)\n\t\temb_B = np.squeeze(emb_B, axis=0)\n        \n\t\tans.append(np.concatenate([emb_A, emb_B, emb_P], axis=1))\n\n\temb = np.concatenate(ans, axis=0)  \n\treturn emb","d8403ca9":"def _row_to_y(row):\n\tif row.loc['A-coref']:\n\t\treturn 0\n\tif row.loc['B-coref']:\n\t\treturn 1\n\treturn 2","dd721fd0":"enc = OneHotEncoder(handle_unknown='ignore')\nop = \"https:\/\/s3-us-west-2.amazonaws.com\/allennlp\/models\/elmo\/2x4096_512_2048cnn_2xhighway\/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\nwg = \"https:\/\/s3-us-west-2.amazonaws.com\/allennlp\/models\/elmo\/2x4096_512_2048cnn_2xhighway\/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n\nprint(\"Started at \", time.ctime())\ntest_data = pd.read_csv(\"gap-test.tsv\", sep = '\\t')\nX_test = get_elmo_fea(test_data, op, wg)\nY_test = test_data.apply(_row_to_y, axis=1)\n\nvalidation_data = pd.read_csv(\"gap-validation.tsv\", sep = '\\t')\nX_validation = get_elmo_fea(validation_data, op, wg)\nY_validation = validation_data.apply(_row_to_y, axis=1)\n\ndevelopment_data = pd.read_csv(\"gap-development.tsv\", sep = '\\t')\nX_development = get_elmo_fea(development_data, op, wg)\nY_development = development_data.apply(_row_to_y, axis=1)\n\nprint(\"Finished at \", time.ctime())","fd312416":"from keras import backend, models, layers, initializers, regularizers, constraints, optimizers\nfrom keras import callbacks as kc\nfrom keras import optimizers as ko\n\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom sklearn.metrics import log_loss\nimport time\n\n\ndense_layer_sizes = [37]\ndropout_rate = 0.6\nlearning_rate = 0.001\nn_fold = 5\nbatch_size = 32\nepochs = 1000\npatience = 100\n# n_test = 100\nlambd = 0.1 # L2 regularization","93f92054":"def build_mlp_model(input_shape):\n\tX_input = layers.Input(input_shape)\n\n\t# First dense layer\n\tX = layers.Dense(dense_layer_sizes[0], name = 'dense0')(X_input)\n\tX = layers.BatchNormalization(name = 'bn0')(X)\n\tX = layers.Activation('relu')(X)\n\tX = layers.Dropout(dropout_rate, seed = 7)(X)\n\n\t# Second dense layer\n# \tX = layers.Dense(dense_layer_sizes[0], name = 'dense1')(X)\n# \tX = layers.BatchNormalization(name = 'bn1')(X)\n# \tX = layers.Activation('relu')(X)\n# \tX = layers.Dropout(dropout_rate, seed = 9)(X)\n\n\t# Output layer\n\tX = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n\tX = layers.Activation('softmax')(X)\n\n\t# Create model\n\tmodel = models.Model(input = X_input, output = X, name = \"classif_model\")\n\treturn model","a9d439da":"# There may be a few NaN values, where the offset of a target word is greater than the max_seq_length of BERT.\n# They are very few, so I'm just dropping the rows.\nremove_test = [row for row in range(len(X_test)) if np.sum(np.isnan(X_test[row]))]\nX_test = np.delete(X_test, remove_test, 0)\nY_test = np.delete(Y_test, remove_test, 0)\n\nremove_validation = [row for row in range(len(X_validation)) if np.sum(np.isnan(X_validation[row]))]\nX_validation = np.delete(X_validation, remove_validation, 0)\nY_validation = np.delete(Y_validation, remove_validation, 0)\n\n# We want predictions for all development rows. So instead of removing rows, make them 0\nremove_development = [row for row in range(len(X_development)) if np.sum(np.isnan(X_development[row]))]\nX_development[remove_development] = np.zeros(3*1024)","b888ff18":"# Will train on data from the gap-test and gap-validation files, in total 2454 rows\nX_train = np.concatenate((X_test, X_validation), axis = 0)\nY_train = np.concatenate((Y_test, Y_validation), axis = 0)\n\n# Will predict probabilities for data from the gap-development file; initializing the predictions\nprediction = np.zeros((len(X_development),3)) # testing predictions","770698a8":"# Training and cross-validation\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=3)\nscores = []\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_train)):\n\t# split training and validation data\n\tprint('Fold', fold_n, 'started at', time.ctime())\n\tX_tr, X_val = X_train[train_index], X_train[valid_index]\n\tY_tr, Y_val = Y_train[train_index], Y_train[valid_index]\n\n\t# Define the model, re-initializing for each fold\n\tclassif_model = build_mlp_model([X_train.shape[1]])\n\tclassif_model.compile(optimizer = optimizers.Adam(lr = learning_rate), loss = \"sparse_categorical_crossentropy\")\n\tcallbacks = [kc.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights = True)]\n\n\t# train the model\n\tclassif_model.fit(x = X_tr, y = Y_tr, epochs = epochs, batch_size = batch_size, callbacks = callbacks, validation_data = (X_val, Y_val), verbose = 0)\n\n\t# make predictions on validation and test data\n\tpred_valid = classif_model.predict(x = X_val, verbose = 0)\n\tpred = classif_model.predict(x = X_development, verbose = 0)\n\n\t# oof[valid_index] = pred_valid.reshape(-1,)\n\tscores.append(log_loss(Y_val, pred_valid))\n\tprediction += pred\nprediction \/= n_fold\n\n# Print CV scores, as well as score on the test data\nprint('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\nprint(scores)\nprint(\"Test score:\", log_loss(Y_development,prediction))","aa063dcc":"# Write the prediction to file for submission\nsubmission = pd.read_csv(\"..\/input\/sample_submission_stage_1.csv\", index_col = \"ID\")\nsubmission[\"A\"] = prediction[:,0]\nsubmission[\"B\"] = prediction[:,1]\nsubmission[\"NEITHER\"] = prediction[:,2]\nsubmission.to_csv(\"submission_bert.csv\")","e7cd43f8":"We use the method defined above to parse the contextual embeddings, for each of the 3 GAP data files. The variable names here may be a bit counter-intuitive. Keep in mind that we will use X_test and X_validation for training, and then make predictions on X_development.","ce1f5aca":"This kernel is base on the [Ceshine Lee's kernel](https:\/\/www.kaggle.com\/ceshine\/pytorch-bert-endpointspanextractor-kfold\/notebook) and [Matei lonita's kernel](https:\/\/www.kaggle.com\/mateiionita\/taming-the-bert-a-baseline). Thanks for providing a good starting point for this competition.","534571b6":"We define a model with two hidden layers and one output layer in Keras."}}