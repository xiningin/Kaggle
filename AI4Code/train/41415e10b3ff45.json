{"cell_type":{"c8ca9c0a":"code","b456e0e7":"code","f366d13c":"code","cd628dbb":"code","b3bdd95b":"code","83336f9b":"code","f87fc20f":"code","d1fb7d4e":"code","c14f2c38":"code","cc7b9603":"code","35133b71":"code","a9cf0fe0":"code","7db1b74d":"code","0b41cb0a":"code","4a3a0954":"markdown"},"source":{"c8ca9c0a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b456e0e7":"# AI is a big umbrella that covers everything which makes Machine intelligent\n# AI -> ML -> Deep Learning\n\n# AI is giving artificial human int. to m\/ch\n# like, doing the whole task, recognizing if person dance, move or anything or drive a car, anything\n\n# It can be narrow and broad.\n# Broad have a lot scope, like all the task (driving car) done by the system\n# narrow is limited to some scope\n\n\n","f366d13c":"# MACHINE LEARNING:\n\n# comes under AI\n# given data to machine, it is expected to learn and work on own, provided some training.\n# Training in machine learning entails giving a lot of data to the algorithm \n# and allowing it to learn more about the processed information.\n","cd628dbb":"# 3 different learning styles:\n# supervised\n# unsupervised\n# semi-supervised\n\n# reinforcement","b3bdd95b":"# 1. Supervised Learning\n# Input data is called training data and has a known label or result such as spam\/not-spam or a stock price at a time.\n\n# A model is prepared through a training process in which it is required to make \n# predictions and is corrected when those predictions are wrong. \n# The training process continues until the model achieves a desired level of accuracy on the training data.\n\n# Example problems are classification and regression.\n\n# Example algorithms include: Logistic Regression and the Back Propagation Neural Network.","83336f9b":"# Linear Regression\n\n# Method 1: Machine Learning from Scratch (Math behind it)\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# Generating some 'random' data\nnp.random.seed(0)\nX = np.random.randint(low=0, high=50, size=(10))         # Array of 100 values \n\n# Generate 100 dependent terms\ny = np.random.randint(low=0, high=50, size=(10)) * 0.98        # Actual values of Y\n\n# Create pandas dataframe to store our X and y values\ndf = pd.DataFrame(\n    {'X': X,\n     'y': y}\n)\n\n# Show the first five rows of our dataframe\ndf.head()","f87fc20f":"# Now, we need mean of x and y values:\n\nmean_x = np.mean(X)\nmean_y = np.mean(y)\nprint(mean_x, mean_y)","d1fb7d4e":"# Now, for slope, 'm'\nnumerator = 0\ndenomenator = 0\n\nfor i in range(len(df)):\n  numerator += (X[i]-mean_x)*(y[i]-mean_y)\n  denomenator += (X[i]-mean_x)**2\n\nslope = numerator\/denomenator\nprint(slope)\n# line_equation:\n# y = slope * X + c\n# that is, mean_y = slope * mean_x + c\n# finding c \n\nc = mean_y - (slope*mean_x)\nprint(c)\n","c14f2c38":"# for plotting from start to end for y pred\n\nmax_x = np.max(X)+10\nmin_x = np.min(X) - 10\n\nx = np.linspace(min_x, max_x)\ny_line = (slope * x) + c\n# print(y_line)\n\nplt.plot(x,y_line, color=\"red\", label=\"Regression line\")\nplt.scatter(X,y, color=\"green\", label=\"Plot of data\" )\n\nplt.xlabel(\"x values\")\nplt.ylabel(\"y values\")\nplt.legend()\nplt.show()","cc7b9603":"# get y_pred only for our x values\n\nmax_x = np.max(X)+10\nmin_x = np.min(X) - 10\n\ny_line = (slope * X) + c\n# print(y_line)\nplt.plot(X,y_line, color=\"red\", label=\"Regression line\")\nplt.scatter(X,y, color=\"green\", label=\"Plot of data\" )\n\nplt.xlabel(\"x values\")\nplt.ylabel(\"y values\")\nplt.legend()\nplt.show()","35133b71":"# R Square\n\n# ss_t = total sum of Square\n# ss_r = total sum of sq of residuals\n\nss_t=0\nss_r=0\n\nfor i in range(len(df)):\n  ss_t += (y_line[i] - mean_y) ** 2\n  ss_r += (y[i] - mean_y) ** 2\n\naccuracy = 1 - (ss_t \/ ss_r)\nprint(accuracy)","a9cf0fe0":"#  R value close to 1 depicts that our regression \n#  model is close enough to the actual value and good\n","7db1b74d":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nX = X.reshape((len(df),1))\n\n# model creation:\nreg = LinearRegression()\n\n# Fitting the training data\nreg = reg.fit(X,y)\n\n# Predicting y values using model \n# (here, y_pred is same as our y_line)\nY_pred = reg.predict(X)\n\n# R sq value calculation\naccuracy = 1 - reg.score(X,y)\nprint(accuracy)","0b41cb0a":"# # For more than two independent variables\n\n# predictors = ['TV', 'Radio']\n# X = advert[predictors]\n# y = advert['Sales']\n\n# # Initialise and fit model\n# lm = LinearRegression()\n# model = lm.fit(X, y)","4a3a0954":"# **Linear Regression using scikit learn**"}}