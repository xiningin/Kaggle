{"cell_type":{"44d21325":"code","19252c12":"code","fb410a1e":"code","c5c8cb2d":"code","58ba492f":"code","d52541c9":"code","45a80e51":"code","8d5ca227":"code","caf62562":"code","2edd6c94":"code","8a6bd6db":"code","a8e3c500":"code","d82ad967":"code","346e4275":"code","68905f63":"code","2a70a2f9":"code","ed7b6035":"code","53231b86":"code","0ad5ff7c":"code","b7d7de58":"code","9c1feba4":"code","d06af476":"code","9e070d0d":"code","05d3927c":"code","121d1737":"code","fd36f45d":"code","993fa0d0":"code","80fc05c7":"code","99ab4a45":"code","58d54ae0":"code","c8a5ef92":"code","a0245727":"code","04560adc":"code","39c51872":"code","b615d53c":"code","33852e6e":"code","6ed65314":"code","2d101adb":"code","8c695617":"code","f32382f7":"code","bc5884d4":"code","13384fad":"code","f29ac966":"code","c3992c85":"code","3054551b":"code","6a4136c3":"code","69a7bb91":"code","1d488289":"code","afe1ed37":"code","681d24ff":"code","f157d74b":"code","157972d3":"code","f15cce67":"code","d4ae5006":"code","83cab243":"code","167a6acf":"code","b4608407":"code","433a5a7f":"code","6a8abd00":"code","39b296d8":"code","ac4067e5":"code","5e47297d":"code","177c03c0":"code","86ae5662":"code","ed39bf29":"code","a9da974f":"code","7a41bf2f":"code","da99f53c":"code","38cb126c":"code","3c9af8ff":"code","d5d60ace":"code","ccbe061f":"code","9efcf543":"code","0f189e28":"code","0564cad4":"code","3408a750":"code","4f406937":"code","b1ea86af":"code","b01a4d6e":"code","27a02e3f":"code","c43c2296":"code","d4db6171":"code","85ca1ed6":"code","e34c933c":"code","cde6c28d":"code","44679a4a":"code","3a72f8cc":"code","82284dfa":"code","20d60041":"code","8e8d7d98":"code","0787627f":"code","b7ca3fc0":"code","d62d1293":"code","e62112ee":"code","b60db7db":"code","dc4d22e8":"code","8034e935":"code","a707daa5":"code","0d9a9e94":"code","f169710e":"code","256d62dd":"code","2a2cae70":"code","e42ceec2":"code","acf7792e":"code","74e781a0":"code","a9de464d":"code","79c3f663":"code","fdc3f348":"code","de43ba90":"code","40bd35d4":"code","c2e7e5e9":"code","ab10748a":"code","9e6d57d3":"code","7b0d015b":"code","e0380a4d":"code","3babc121":"code","40ccec5d":"code","33d8c430":"code","35d648db":"code","0a6e3891":"code","be7be686":"code","bb1f56eb":"code","3d137837":"code","ca152d85":"markdown","61189098":"markdown","f91a05bf":"markdown","ceb1e1d4":"markdown","d7fe7472":"markdown","9a800651":"markdown","7a7c92aa":"markdown","c2bc8aa2":"markdown","bf10cd07":"markdown","ca62e422":"markdown","0fc19987":"markdown","ae876ca9":"markdown","c662d213":"markdown","4c70242e":"markdown","14a98451":"markdown","9c270edd":"markdown","cdbed785":"markdown","f692743e":"markdown","50c84e05":"markdown","86caf137":"markdown","01b638cd":"markdown","26a47a05":"markdown","779b9b8c":"markdown","4ac882e8":"markdown","b566426a":"markdown","8223fc63":"markdown","924332e5":"markdown","d49e1d77":"markdown","6454b561":"markdown","1fbe54bd":"markdown","1c78a8b0":"markdown","5884270f":"markdown","852b3b9f":"markdown","8f9af906":"markdown","00be1efb":"markdown","e73b12b6":"markdown","e72ff944":"markdown","6c75f73f":"markdown","6023add5":"markdown","7208b098":"markdown","686e37f9":"markdown","3f1805b6":"markdown","ee60c7dd":"markdown","a25ec161":"markdown","4c482a6f":"markdown","59ccf1c9":"markdown","3256bcab":"markdown","5e16b28b":"markdown","ffdc761e":"markdown","343ac67c":"markdown","771f612d":"markdown","3cb7f286":"markdown","a3f67d43":"markdown","32da603c":"markdown","3bb2952e":"markdown","2bda7b27":"markdown","ed6be4db":"markdown"},"source":{"44d21325":"# Importing libraries\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#sns.set_style('darkgrid')","19252c12":"import warnings\nwarnings.filterwarnings('ignore')","fb410a1e":"# Loading the train data\ndf = pd.read_csv('\/kaggle\/input\/customer\/Train.csv')\n\n# Looking top 10 rows\ndf.head(10)","c5c8cb2d":"# Looking the bigger picture\ndf.info()","58ba492f":"# Checking the number of missing values in each column\ndf.isnull().sum()","d52541c9":"# Removing all those rows that have 3 or more missing values\ndf = df.loc[df.isnull().sum(axis=1)<3]","45a80e51":"# Looking random 10 rows of the data\ndf.sample(10)","8d5ca227":"print('The count of each category\\n',df.Var_1.value_counts())","caf62562":"# Checking for null values\ndf.Var_1.isnull().sum()","2edd6c94":"# Filling the missing values w.r.t other attributes underlying pattern \ndf.loc[ (pd.isnull(df['Var_1'])) & (df['Graduated'] == 'Yes'),\"Var_1\"] = 'Cat_6'\ndf.loc[ (pd.isnull(df['Var_1'])) & (df['Graduated'] == 'No'),\"Var_1\"] = 'Cat_4'\ndf.loc[ (pd.isnull(df[\"Var_1\"])) & ((df['Profession'] == 'Lawyer') | (df['Profession'] == 'Artist')),\"Var_1\"] = 'Cat_6'\ndf.loc[ (pd.isnull(df[\"Var_1\"])) & (df['Age'] > 40),\"Var_1\"] = 'Cat_6'","8a6bd6db":"# Counting Var_1 in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Var_1\"].value_counts().unstack().round(3)\n\n# Percentage of category of Var_1 in each segment\nax2 = df.pivot_table(columns='Var_1',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","a8e3c500":"print('The count of gender\\n',df.Gender.value_counts())","d82ad967":"# Checking the count of missing values\ndf.Gender.isnull().sum()","346e4275":"# Counting male-female in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Gender\"].value_counts().unstack().round(3)\n\n# Percentage of male-female in each segment\nax2 = df.pivot_table(columns='Gender',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","68905f63":"print('Count of married vs not married\\n',df.Ever_Married.value_counts())","2a70a2f9":"# Checking the count of missing values\ndf.Ever_Married.isnull().sum()","ed7b6035":"# Filling the missing values w.r.t other attributes underlying pattern\ndf.loc[ (pd.isnull(df[\"Ever_Married\"])) & ((df['Spending_Score'] == 'Average') | (df['Spending_Score'] == 'High')),\"Ever_Married\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Ever_Married\"])) & (df['Spending_Score'] == 'Low'),\"Ever_Married\"] = 'No'\ndf.loc[ (pd.isnull(df[\"Ever_Married\"])) & (df['Age'] > 40),\"Ever_Married\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Ever_Married\"])) & (df['Profession'] == 'Healthcare'),\"Ever_Married\"] = 'No'","53231b86":"# Counting married and non-married in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Ever_Married\"].value_counts().unstack().round(3)\n\n# Percentage of married and non-married in each segment\nax2 = df.pivot_table(columns='Ever_Married',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","0ad5ff7c":"df.Age.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","b7d7de58":"# Checking the count of missing values\ndf.Age.isnull().sum()","9c1feba4":"# Looking the distribution of column Age\nplt.figure(figsize=(10,5))\n\nskewness = round(df.Age.skew(),2)\nkurtosis = round(df.Age.kurtosis(),2)\nmean = round(np.mean(df.Age),0)\nmedian = np.median(df.Age)\n\nplt.subplot(1,2,1)\nsns.boxplot(y=df.Age)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(1,2,2)\nsns.distplot(df.Age)\nplt.title('Distribution Plot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.show()","d06af476":"# Looking the distribution of column Age w.r.t to each segment\na = df[df.Segmentation =='A'][\"Age\"]\nb = df[df.Segmentation =='B'][\"Age\"]\nc = df[df.Segmentation =='C'][\"Age\"]\nd = df[df.Segmentation =='D'][\"Age\"]\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.boxplot(data = df, x = \"Segmentation\", y=\"Age\")\nplt.title('Boxplot')\n\nplt.subplot(1,2,2)\nsns.kdeplot(a,shade= False, label = 'A')\nsns.kdeplot(b,shade= False, label = 'B')\nsns.kdeplot(c,shade= False, label = 'C')\nsns.kdeplot(d,shade= False, label = 'D')\nplt.xlabel('Age')\nplt.ylabel('Density')\nplt.title(\"Mean\\n A: {}\\n B: {}\\n C: {}\\n D: {}\".format(round(a.mean(),0),round(b.mean(),0),round(c.mean(),0),round(d.mean(),0)))\n\nplt.show()","9e070d0d":"# Converting the datatype from float to int\ndf['Age'] = df['Age'].astype(int)","05d3927c":"df.Age.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","121d1737":"# Divide people in the 4 age group\ndf['Age_Bin'] = pd.cut(df.Age,bins=[17,30,45,60,90],labels=['17-30','31-45','46-60','60+'])","fd36f45d":"# Counting different age group in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Age_Bin\"].value_counts().unstack().round(3)\n\n# Percentage of age bins in each segment\nax2 = df.pivot_table(columns='Age_Bin',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","993fa0d0":"print('Count of each graduate and non-graduate\\n',df.Graduated.value_counts())","80fc05c7":"# Checking the count of missing values\ndf.Graduated.isnull().sum()","99ab4a45":"# Filling the missing values w.r.t other attributes underlying pattern\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Spending_Score'] == 'Average'),\"Graduated\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Profession'] == 'Artist'),\"Graduated\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Age'] > 49),\"Graduated\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Var_1'] == 'Cat_4'),\"Graduated\"] = 'No'\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Ever_Married'] == 'Yes'),\"Graduated\"] = 'Yes'\n\n# Replacing remaining NaN with previous values\ndf['Graduated'] = df['Graduated'].fillna(method='pad')","58d54ae0":"# Counting graduate and non-graduate in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Graduated\"].value_counts().unstack().round(3)\n\n# Percentage of graduate and non-graduate in each segment\nax2 = df.pivot_table(columns='Graduated',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","c8a5ef92":"print('Count of each profession\\n',df.Profession.value_counts())","a0245727":"# Checking the count of missing values\ndf.Profession.isnull().sum()","04560adc":"# Filling the missing values w.r.t other attributes underlying pattern\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Work_Experience'] > 8),\"Profession\"] = 'Homemaker'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Age'] > 70),\"Profession\"] = 'Lawyer'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Family_Size'] < 3),\"Profession\"] = 'Lawyer'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Spending_Score'] == 'Average'),\"Profession\"] = 'Artist'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Graduated'] == 'Yes'),\"Profession\"] = 'Artist'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Ever_Married'] == 'Yes'),\"Profession\"] = 'Artist'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Ever_Married'] == 'No'),\"Profession\"] = 'Healthcare'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Spending_Score'] == 'High'),\"Profession\"] = 'Executives'","39c51872":"# Count of segments in each profession\nax1 = df.groupby([\"Profession\"])[\"Segmentation\"].value_counts().unstack().round(3)\n\n# Percentage of segments in each profession\nax2 = df.pivot_table(columns='Segmentation',index='Profession',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (16,5))\nlabel = ['Artist','Doctor','Engineer','Entertainment','Executives','Healthcare','Homemaker','Lawyer','Marketing']\nax[0].set_xticklabels(labels = label,rotation = 45)\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (16,5))\nax[1].set_xticklabels(labels = label,rotation = 45)\n\nplt.show()","b615d53c":"df.Work_Experience.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","33852e6e":"# Checking the count of missing values\ndf.Work_Experience.isnull().sum()","6ed65314":"# Replacing NaN with previous values\ndf['Work_Experience'] = df['Work_Experience'].fillna(method='pad')","2d101adb":"# Looking the distribution of column Work Experience\nplt.figure(figsize=(15,10))\n\nskewness = round(df.Work_Experience.skew(),2)\nkurtosis = round(df.Work_Experience.kurtosis(),2)\nmean = round(np.mean(df.Work_Experience),0)\nmedian = np.median(df.Work_Experience)\n\nplt.subplot(1,2,1)\nsns.boxplot(y=df.Work_Experience)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(2,2,2)\nsns.distplot(df.Work_Experience)\nplt.title('Distribution Plot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.show()","8c695617":"# Looking the distribution of column Work_Experience w.r.t to each segment\na = df[df.Segmentation =='A'][\"Work_Experience\"]\nb = df[df.Segmentation =='B'][\"Work_Experience\"]\nc = df[df.Segmentation =='C'][\"Work_Experience\"]\nd = df[df.Segmentation =='D'][\"Work_Experience\"]\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.boxplot(data = df, x = \"Segmentation\", y=\"Work_Experience\")\nplt.title('Boxplot')\n\nplt.subplot(1,2,2)\nsns.kdeplot(a,shade= False, label = 'A')\nsns.kdeplot(b,shade= False, label = 'B')\nsns.kdeplot(c,shade= False, label = 'C')\nsns.kdeplot(d,shade= False, label = 'D')\nplt.xlabel('Work Experience')\nplt.ylabel('Density')\nplt.title(\"Mean\\n A: {}\\n B: {}\\n C: {}\\n D: {}\".format(round(a.mean(),0),round(b.mean(),0),round(c.mean(),0),round(d.mean(),0)))\n\nplt.show()","f32382f7":"# Changing the data type\ndf['Work_Experience'] = df['Work_Experience'].astype(int)","bc5884d4":"df.Work_Experience.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","13384fad":"# Dividing the people into 3 category of work experience \ndf['Work_Exp_Category'] = pd.cut(df.Work_Experience,bins=[-1,1,7,15],labels=['Low Experience','Medium Experience','High Experience'])","f29ac966":"# Counting different category of work experience in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Work_Exp_Category\"].value_counts().unstack().round(3)\n\n# Percentage of work experience in each segment\nax2 = df.pivot_table(columns='Work_Exp_Category',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","c3992c85":"print('Count of spending score\\n',df.Spending_Score.value_counts())","3054551b":"# Checking the count of missing values\ndf.Spending_Score.isnull().sum()","6a4136c3":"# Counting different category of spending score in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Spending_Score\"].value_counts().unstack().round(3)\n\n# Percentage of spending score in each segment\nax2 = df.pivot_table(columns='Spending_Score',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","69a7bb91":"df.Family_Size.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","1d488289":"# Checking the count of missing values\ndf.Family_Size.isnull().sum()","afe1ed37":"# Filling the missing values w.r.t other attributes underlying pattern\ndf.loc[ (pd.isnull(df[\"Family_Size\"])) & (df['Ever_Married'] == 'Yes'),\"Family_Size\"] = 2.0\ndf.loc[ (pd.isnull(df[\"Family_Size\"])) & (df['Var_1'] == 'Cat_6'),\"Family_Size\"] = 2.0\ndf.loc[ (pd.isnull(df[\"Family_Size\"])) & (df['Graduated'] == 'Yes'),\"Family_Size\"] = 2.0\n\n# Fill remaining NaN with previous values\ndf['Family_Size'] = df['Family_Size'].fillna(method='pad')","681d24ff":"# Looking the distribution of column Work Experience\nplt.figure(figsize=(15,10))\n\nskewness = round(df.Family_Size.skew(),2)\nkurtosis = round(df.Family_Size.kurtosis(),2)\nmean = round(np.mean(df.Family_Size),0)\nmedian = np.median(df.Family_Size)\n\nplt.subplot(1,2,1)\nsns.boxplot(y=df.Family_Size)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(2,2,2)\nsns.distplot(df.Family_Size)\nplt.title('Distribution Plot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.show()","f157d74b":"# Looking the distribution of column Family Size w.r.t to each segment\na = df[df.Segmentation =='A'][\"Family_Size\"]\nb = df[df.Segmentation =='B'][\"Family_Size\"]\nc = df[df.Segmentation =='C'][\"Family_Size\"]\nd = df[df.Segmentation =='D'][\"Family_Size\"]\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.boxplot(data = df, x = \"Segmentation\", y=\"Family_Size\")\nplt.title('Boxplot')\n\nplt.subplot(1,2,2)\nsns.kdeplot(a,shade= False, label = 'A')\nsns.kdeplot(b,shade= False, label = 'B')\nsns.kdeplot(c,shade= False, label = 'C')\nsns.kdeplot(d,shade= False, label = 'D')\nplt.xlabel('Family Size')\nplt.ylabel('Density')\nplt.title(\"Mean\\n A: {}\\n B: {}\\n C: {}\\n D: {}\".format(round(a.mean(),0),round(b.mean(),0),round(c.mean(),0),round(d.mean(),0)))\n\nplt.show()","157972d3":"# Changing the data type\ndf['Family_Size'] = df['Family_Size'].astype(int)","f15cce67":"df.Family_Size.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","d4ae5006":"# Divide family size into 3 category\ndf['Family_Size_Category'] = pd.cut(df.Family_Size,bins=[0,4,6,10],labels=['Small Family','Big Family','Joint Family'])","83cab243":"# Counting different category of family size in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Family_Size_Category\"].value_counts().unstack().round(3)\n\n# Percentage of family size in each segment\nax2 = df.pivot_table(columns='Family_Size_Category',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(3)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","167a6acf":"print('Count of each category of segmentation\\n',df.Segmentation.value_counts())","b4608407":"segments = df.loc[:,\"Segmentation\"].value_counts()\nplt.xlabel(\"Segment\")\nplt.ylabel('Count')\nsns.barplot(segments.index , segments.values).set_title('Segments')\nplt.show()","433a5a7f":"df.reset_index(drop=True, inplace=True)\ndf.info()","6a8abd00":"# number of unique ids\ndf.ID.nunique()","39b296d8":"df.describe(include='all')","ac4067e5":"df = df[['ID','Gender', 'Ever_Married', 'Age', 'Age_Bin', 'Graduated', 'Profession', 'Work_Experience', 'Work_Exp_Category',\n         'Spending_Score', 'Family_Size', 'Family_Size_Category','Var_1', 'Segmentation']]\ndf.head(10)","5e47297d":"df1 = df.copy()\ndf1.head()","177c03c0":"# Separating dependent-independent variables\nX = df1.drop('Segmentation',axis=1)\ny = df1['Segmentation']","86ae5662":"# import the train-test split\nfrom sklearn.model_selection import train_test_split\n\n# divide into train and test sets\ndf1_trainX, df1_testX, df1_trainY, df1_testY = train_test_split(X,y, train_size = 0.7, random_state = 101, stratify=y)","ed39bf29":"# converting binary variables to numeric\ndf1_trainX['Gender'] = df1_trainX['Gender'].replace(('Male','Female'),(1,0))\ndf1_trainX['Ever_Married'] = df1_trainX['Ever_Married'].replace(('Yes','No'),(1,0))\ndf1_trainX['Graduated'] = df1_trainX['Graduated'].replace(('Yes','No'),(1,0))\ndf1_trainX['Spending_Score'] = df1_trainX['Spending_Score'].replace(('High','Average','Low'),(3,2,1))\n\n# converting nominal variables into dummy variables\npf = pd.get_dummies(df1_trainX.Profession,prefix='Profession')\ndf1_trainX = pd.concat([df1_trainX,pf],axis=1)\n\nvr = pd.get_dummies(df1_trainX.Var_1,prefix='Var_1')\ndf1_trainX = pd.concat([df1_trainX,vr],axis=1)\n\n# scaling continuous variables\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf1_trainX[['Age','Work_Experience','Family_Size']] = scaler.fit_transform(df1_trainX[['Age','Work_Experience','Family_Size']])\n\ndf1_trainX.drop(['ID','Age_Bin','Profession','Work_Exp_Category','Family_Size_Category','Var_1'], axis=1, inplace=True)","a9da974f":"# converting binary variables to numeric\ndf1_testX['Gender'] = df1_testX['Gender'].replace(('Male','Female'),(1,0))\ndf1_testX['Ever_Married'] = df1_testX['Ever_Married'].replace(('Yes','No'),(1,0))\ndf1_testX['Graduated'] = df1_testX['Graduated'].replace(('Yes','No'),(1,0))\ndf1_testX['Spending_Score'] = df1_testX['Spending_Score'].replace(('High','Average','Low'),(3,2,1))\n\n# converting nominal variables into dummy variables\npf = pd.get_dummies(df1_testX.Profession,prefix='Profession')\ndf1_testX = pd.concat([df1_testX,pf],axis=1)\n\nvr = pd.get_dummies(df1_testX.Var_1,prefix='Var_1')\ndf1_testX = pd.concat([df1_testX,vr],axis=1)\n\n# scaling continuous variables\ndf1_testX[['Age','Work_Experience','Family_Size']] = scaler.transform(df1_testX[['Age','Work_Experience','Family_Size']])\n\ndf1_testX.drop(['ID','Age_Bin','Profession','Work_Exp_Category','Family_Size_Category','Var_1'], axis=1, inplace=True)","7a41bf2f":"df1_trainX.shape, df1_trainY.shape, df1_testX.shape, df1_testY.shape","da99f53c":"# Correlation matrix\nplt.figure(figsize=(17,10))\nsns.heatmap(df1_trainX.corr(method='spearman').round(2),linewidth = 0.5,annot=True,cmap=\"YlGnBu\")\nplt.show()","38cb126c":"df2 = df.copy()\ndf2.head()","3c9af8ff":"# Separating dependent-independent variables\nX = df2.drop('Segmentation',axis=1)\ny = df2['Segmentation']","d5d60ace":"# import the train-test split\nfrom sklearn.model_selection import train_test_split\n\n# divide into train and test sets\ndf2_trainX, df2_testX, df2_trainY, df2_testY = train_test_split(X,y, train_size = 0.7, random_state = 101, stratify=y)","ccbe061f":"# Converting binary to numeric\ndf2_trainX['Gender'] = df2_trainX['Gender'].replace(('Male','Female'),(1,0))\ndf2_trainX['Ever_Married'] = df2_trainX['Ever_Married'].replace(('Yes','No'),(1,0))\ndf2_trainX['Graduated'] = df2_trainX['Graduated'].replace(('Yes','No'),(1,0))\n\n# Converting nominal variables to dummy variables\nab = pd.get_dummies(df2_trainX.Age_Bin,prefix='Age_Bin')\ndf2_trainX = pd.concat([df2_trainX,ab],axis=1)\n\npf = pd.get_dummies(df2_trainX.Profession,prefix='Profession')\ndf2_trainX = pd.concat([df2_trainX,pf],axis=1)\n\nwe = pd.get_dummies(df2_trainX.Work_Exp_Category,prefix='WorkExp')\ndf2_trainX = pd.concat([df2_trainX,we],axis=1)\n\nsc = pd.get_dummies(df2_trainX.Spending_Score,prefix='Spending')\ndf2_trainX = pd.concat([df2_trainX,sc],axis=1)\n\nfs = pd.get_dummies(df2_trainX.Family_Size_Category,prefix='FamilySize')\ndf2_trainX = pd.concat([df2_trainX,fs],axis=1)\n\nvr = pd.get_dummies(df2_trainX.Var_1,prefix='Var_1')\ndf2_trainX = pd.concat([df2_trainX,vr],axis=1)\n\ndf2_trainX.drop(['ID','Age','Age_Bin','Profession','Work_Experience','Work_Exp_Category','Spending_Score',\n               'Family_Size','Family_Size_Category','Var_1'],axis=1,inplace=True)","9efcf543":"# Converting binary to numeric\ndf2_testX['Gender'] = df2_testX['Gender'].replace(('Male','Female'),(1,0))\ndf2_testX['Ever_Married'] = df2_testX['Ever_Married'].replace(('Yes','No'),(1,0))\ndf2_testX['Graduated'] = df2_testX['Graduated'].replace(('Yes','No'),(1,0))\n\n# Converting nominal variables to dummy variables\nab = pd.get_dummies(df2_testX.Age_Bin,prefix='Age_Bin')\ndf2_testX = pd.concat([df2_testX,ab],axis=1)\n\npf = pd.get_dummies(df2_testX.Profession,prefix='Profession')\ndf2_testX = pd.concat([df2_testX,pf],axis=1)\n\nwe = pd.get_dummies(df2_testX.Work_Exp_Category,prefix='WorkExp')\ndf2_testX = pd.concat([df2_testX,we],axis=1)\n\nsc = pd.get_dummies(df2_testX.Spending_Score,prefix='Spending')\ndf2_testX = pd.concat([df2_testX,sc],axis=1)\n\nfs = pd.get_dummies(df2_testX.Family_Size_Category,prefix='FamilySize')\ndf2_testX = pd.concat([df2_testX,fs],axis=1)\n\nvr = pd.get_dummies(df2_testX.Var_1,prefix='Var_1')\ndf2_testX = pd.concat([df2_testX,vr],axis=1)\n\ndf2_testX.drop(['ID','Age','Age_Bin','Profession','Work_Experience','Work_Exp_Category','Spending_Score',\n               'Family_Size','Family_Size_Category','Var_1'],axis=1,inplace=True)","0f189e28":"df2_trainX.shape, df2_trainY.shape, df2_testX.shape, df2_testY.shape","0564cad4":"# Correlation matrix\nplt.figure(figsize=(17,10))\nsns.heatmap(df2_trainX.corr(method='spearman').round(2),linewidth = 0.5,annot=True,cmap=\"YlGnBu\")\nplt.show()","3408a750":"train_dt1_x = df1_trainX.copy()\ntrain_dt1_x.head()","4f406937":"train_dt1_y = df1_trainY.copy()\ntrain_dt1_y.head()","b1ea86af":"# importing decision tree classifier \nfrom sklearn.tree import DecisionTreeClassifier\n\n# creating the decision tree function\nmodel_dt1 = DecisionTreeClassifier(random_state=10,criterion='gini')\n\n#fitting the model\nmodel_dt1.fit(train_dt1_x, train_dt1_y)\n\n# depth of the decision tree\nprint('Depth of the Decision Tree: ', model_dt1.get_depth())\n\n#checking the training score\nprint('Accuracy on training: ',model_dt1.score(train_dt1_x, train_dt1_y))\n\n# predict the target on the train dataset\nyhat1 = model_dt1.predict(train_dt1_x)\n\nfrom sklearn.metrics import confusion_matrix\ncm1 = confusion_matrix(train_dt1_y.values, yhat1, labels=[\"A\",\"B\",\"C\",\"D\"])\nprint('-------The confusion matrix for this model is-------')\nprint(cm1)\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the whole report of the model-------')\nprint(classification_report(train_dt1_y.values, yhat1))","b01a4d6e":"X1 = train_dt1_x.copy()\ny1 = pd.DataFrame({'Seg':train_dt1_y})\ny1['Seg'] = y1['Seg'].replace(('A','B','C','D'),(1,2,3,4))","27a02e3f":"# Implementing grid search\n\nparameter_grid = {\n    'max_depth' : [24,25,26,27,28,29,30],\n    'max_features': [0.3, 0.5, 0.7]\n    }\n\nfrom sklearn.model_selection import GridSearchCV\ngridsearch = GridSearchCV(estimator=model_dt1, param_grid=parameter_grid, scoring='neg_mean_squared_error', cv=5)\n\ngridsearch.fit(X1, y1)\n\nprint(gridsearch.best_params_)","c43c2296":"# Implementing random search\n\nparameter_grid = {\n    'max_depth' : [24,25,26,27,28,29,30],\n    'max_features': [0.3, 0.5, 0.7,0.9]\n    }\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrandomsearch = RandomizedSearchCV(estimator=model_dt1, param_distributions=parameter_grid, n_iter= 10, cv=5)\nrandomsearch.fit(X1, y1)\n\nprint(randomsearch.best_params_)","d4db6171":"# final model\nmodel_dt1 = DecisionTreeClassifier(max_depth=26, max_features=0.9 ,random_state=10)\n\n# fitting the model\nmodel_dt1.fit(train_dt1_x, train_dt1_y)\n\n# Training score\nprint(model_dt1.score(train_dt1_x, train_dt1_y).round(4))","85ca1ed6":"from sklearn import tree\n\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(model_dt1, feature_names=train_dt1_x.columns, max_depth=2, filled=True)","e34c933c":"test_dt1_x = df1_testX.copy()\ntest_dt1_x.head()","cde6c28d":"test_dt1_y = df1_testY.copy()\ntest_dt1_y.head()","44679a4a":"y_dt1 = model_dt1.predict(test_dt1_x)\ny_dt1","3a72f8cc":"from sklearn.metrics import confusion_matrix\nprint('-------The confusion matrix for test data is-------\\n')\nprint(confusion_matrix(test_dt1_y.values, y_dt1, labels=[\"A\",\"B\",\"C\",\"D\"]))\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of test data-------\\n')\nprint(classification_report(test_dt1_y.values, y_dt1))","82284dfa":"pd.Series(y_dt1).value_counts()","20d60041":"train_dt2_x = df2_trainX.copy()\ntrain_dt2_x.head()","8e8d7d98":"train_dt2_y = df2_trainY.copy()\ntrain_dt2_y.head()","0787627f":"# importing decision tree classifier \nfrom sklearn.tree import DecisionTreeClassifier\n\n# creating the decision tree function\nmodel_dt2 = DecisionTreeClassifier(random_state=10,criterion='gini')\n\n#fitting the model\nmodel_dt2.fit(train_dt2_x, train_dt2_y)\n\n# depth of the decision tree\nprint('Depth of the Decision Tree: ', model_dt2.get_depth())\n\n#checking the training score\nprint('Accuracy on training: ',model_dt2.score(train_dt2_x, train_dt2_y))\n\n# predict the target on the train dataset\nyhat2 = model_dt2.predict(train_dt2_x)\n\nfrom sklearn.metrics import confusion_matrix\ncm2 = confusion_matrix(train_dt2_y.values, yhat2, labels=[\"A\",\"B\",\"C\",\"D\"])\nprint('-------The confusion matrix for this model is-------')\nprint(cm2)\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the whole report of the model-------')\nprint(classification_report(train_dt2_y.values, yhat2))","b7ca3fc0":"X2 = train_dt2_x.copy()\ny2 = pd.DataFrame({'Seg':train_dt2_y})\ny2['Seg'] = y2['Seg'].replace(('A','B','C','D'),(1,2,3,4))","d62d1293":"# Implementing grid search\n\nparameter_grid = {\n    'max_depth' : [24,25,26,27,28,29,30],\n    'max_features': [0.3, 0.5, 0.7]\n    }\n\nfrom sklearn.model_selection import GridSearchCV\ngridsearch = GridSearchCV(estimator=model_dt2, param_grid=parameter_grid, scoring='neg_mean_squared_error', cv=5)\n\ngridsearch.fit(X2, y2)\n\nprint(gridsearch.best_params_)","e62112ee":"# Implementing random search\n\nparameter_grid = {\n    'max_depth' : [24,25,26,27,28,29,30],\n    'max_features': [0.3, 0.5, 0.7,0.9]\n    }\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrandomsearch = RandomizedSearchCV(estimator=model_dt2, param_distributions=parameter_grid, n_iter= 10, cv=5)\nrandomsearch.fit(X2, y2)\n\nprint(randomsearch.best_params_)","b60db7db":"# final model\nmodel_dt2 = DecisionTreeClassifier(max_depth=25, max_features=0.7, random_state=10)\n\n#fitting the model\nmodel_dt2.fit(train_dt2_x, train_dt2_y)\n\n#Training score\nprint(model_dt2.score(train_dt2_x, train_dt2_y).round(4))","dc4d22e8":"from sklearn import tree\n\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(model_dt2, feature_names=train_dt2_x.columns, max_depth=2, filled=True)","8034e935":"test_dt2_x = df2_testX.copy()\ntest_dt2_x.head()","a707daa5":"test_dt2_y = df2_testY.copy()\ntest_dt2_y.head()","0d9a9e94":"y_dt2 = model_dt2.predict(test_dt2_x)\ny_dt2","f169710e":"from sklearn.metrics import confusion_matrix\nprint('-------The confusion matrix for test data is-------')\nprint(confusion_matrix(test_dt2_y.values, y_dt2, labels=[\"A\",\"B\",\"C\",\"D\"]))\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of test data-------')\nprint(classification_report(test_dt2_y.values, y_dt2))","256d62dd":"pd.Series(y_dt2).value_counts()","2a2cae70":"print('************************  MODEL-1 REPORT  *********************************\\n')\nprint('Train data')\nprint(classification_report(train_dt1_y.values, yhat1))\nprint('\\nTest data')\nprint(classification_report(test_dt1_y.values, y_dt1))","e42ceec2":"print('************************  MODEL-2 REPORT  *********************************\\n')\nprint('Train data')\nprint(classification_report(train_dt2_y.values, yhat2))\nprint('\\nTest data')\nprint(classification_report(test_dt2_y.values, y_dt2))","acf7792e":"train_rf1_x = df1_trainX.copy()\ntrain_rf1_x.head()","74e781a0":"train_rf1_y = df1_trainY.copy()\ntrain_rf1_y.head()","a9de464d":"# Importing the library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Instantiate the classifier with 20 decision tree\nrfc1 = RandomForestClassifier(random_state=0,n_estimators=20)\n\n# Train model\nmodel_rfc1 = rfc1.fit(train_rf1_x, train_rf1_y)\n\n# Predicting the classes\nyhat3 = rfc1.predict(train_rf1_x)\n\n# view the feature scores\nfeature_scores = pd.Series(rfc1.feature_importances_, index=train_rf1_x.columns).sort_values(ascending=False)\nprint('The importance of features ranked from high to low:\\n',feature_scores)\n\nfrom sklearn.metrics import confusion_matrix\ncm3 = confusion_matrix(train_rf1_y.values, yhat3, labels=[\"A\",\"B\",\"C\",\"D\"])\nprint('\\n\\n-------The confusion matrix for this model is-------')\nprint(cm3)\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the whole report of the model-------')\nprint(classification_report(train_rf1_y.values, yhat3))","79c3f663":"# Creating bar plot of scores of variables importance\nplt.figure(figsize=(10,8))\nsns.barplot(x=feature_scores, y=feature_scores.index)\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.show()","fdc3f348":"test_rf1_x = df1_testX.copy()\ntest_rf1_x.head()","de43ba90":"test_rf1_y = df1_testY.copy()\ntest_rf1_y.head()","40bd35d4":"y_rf1 = rfc1.predict(test_rf1_x)\ny_rf1","c2e7e5e9":"from sklearn.metrics import confusion_matrix\nprint('-------The confusion matrix for test data is-------\\n')\nprint(confusion_matrix(test_rf1_y.values, y_rf1, labels=[\"A\",\"B\",\"C\",\"D\"]))\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of test data-------\\n')\nprint(classification_report(test_rf1_y.values, y_rf1))","ab10748a":"pd.Series(y_rf1).value_counts()","9e6d57d3":"train_rf2_x = df2_trainX.copy()\ntrain_rf2_x.head()","7b0d015b":"train_rf2_y = df2_trainY.copy()\ntrain_rf2_y.head()","e0380a4d":"# Importing the library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Instantiate the classifier with 20 decision tree\nrfc2 = RandomForestClassifier(random_state=0,n_estimators=20)\n\n# Train model\nmodel_rfc2 = rfc2.fit(train_rf2_x, train_rf2_y)\n\n# Predicting the classes\nyhat4 = rfc2.predict(train_rf2_x)\n\n# view the feature scores\nfeature_scores = pd.Series(rfc2.feature_importances_, index=train_rf2_x.columns).sort_values(ascending=False)\nprint('The importance of features ranked from high to low:\\n',feature_scores)\n\nfrom sklearn.metrics import confusion_matrix\ncm4 = confusion_matrix(train_rf2_y.values, yhat4, labels=[\"A\",\"B\",\"C\",\"D\"])\nprint('\\n\\n-------The confusion matrix for this model is-------')\nprint(cm4)\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the whole report of the model-------')\nprint(classification_report(train_rf2_y.values, yhat4))","3babc121":"# Creating bar plot of scores of variables importance\nplt.figure(figsize=(10,8))\nsns.barplot(x=feature_scores, y=feature_scores.index)\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.show()","40ccec5d":"test_rf2_x = df2_testX.copy()\ntest_rf2_x.head()","33d8c430":"test_rf2_y = df2_testY.copy()\ntest_rf2_y.head()","35d648db":"y_rf2 = rfc2.predict(test_rf2_x)\ny_rf2","0a6e3891":"from sklearn.metrics import confusion_matrix\nprint('-------The confusion matrix for test data is-------\\n')\nprint(confusion_matrix(test_rf2_y.values, y_rf2, labels=[\"A\",\"B\",\"C\",\"D\"]))\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of test data-------\\n')\nprint(classification_report(test_rf2_y.values, y_rf2))","be7be686":"pd.Series(y_rf2).value_counts()","bb1f56eb":"print('************************  MODEL-1 REPORT  *********************************\\n')\nprint('Train data')\nprint(classification_report(train_rf1_y.values, yhat3))\nprint('\\nTest data')\nprint(classification_report(test_rf1_y.values, y_rf1))","3d137837":"print('************************  MODEL-2 REPORT  *********************************\\n')\nprint('Train data')\nprint(classification_report(train_rf2_y.values, yhat4))\nprint('\\nTest data')\nprint(classification_report(test_rf2_y.values, y_rf2))","ca152d85":"- Decision tree is a type of supervised learning algorithm (having a predefined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter \/ differentiator in input variables.\n- A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.\n- Decision Tree is a white box type of ML algorithm. It shares internal decision-making logic, which is not available in the black box type of algorithms such as Neural Network. \n- Its training time is faster compared to the neural network algorithm. The time complexity of decision trees is a function of the number of records and number of attributes in the given data.\n- The decision tree is a distribution-free or non-parametric method, which does not depend upon probability distribution assumptions. Decision trees can handle high dimensional data with good accuracy.\n\n#### How does the Decision Tree algorithm work?\nThe basic idea behind any decision tree algorithm is as follows:<br>\n\n1. Select the best attribute using Attribute Selection Measures(ASM) to split the records.\n2. Make that attribute a decision node and breaks the dataset into smaller subsets.\n3. Starts tree building by repeating this process recursively for each child until one of the condition will match:\n    1. All the tuples belong to the same attribute value.\n    2. There are no more remaining attributes.\n    3. There are no more instances.\n\n**Further Reading**\n- https:\/\/www.analyticsvidhya.com\/blog\/2016\/04\/tree-based-algorithms-complete-tutorial-scratch-in-python\/\n- https:\/\/www.datacamp.com\/community\/tutorials\/decision-tree-classification-python","61189098":"### <font color = 'blue'>Topics Covered in this notebook<\/font>\n1. Basic cleaning and EDA\n2. Decision Tree\n    - Model Building with two different dataframes\n    - Model Evaluation\n    - Final comment on which dataframe is good for this algorithm\n3. Random Forest\n    - Model Building with two different dataframes\n    - Model Evaluation\n    - Final comment on which dataframe is good for this algorithm","f91a05bf":"<font color='blue'>1. It is clearly seen that model-1 is far better than model-2.<br>\n<font color='blue'>2. But the point here to note that there are 2327 entries which are common in train-test data i.e. 2327 test entries(out of total 2627) are already in train data and when we see the result on test set model-1 gave 69% accuracy while model-2 gave 64%.<br>\n<font color='blue'>3. A drop from 95% train accuracy to 69% test accuracy in model-1 shows that it overfit the train data while in model-2 69% of train accuracy and 64% of test accuracy gives a balaced trade-off.<br>\n<font color='blue'>4. So we can conclude that `df_type2 is a better data` for model building in Random Forest technique.","ceb1e1d4":"###### Work Experience","d7fe7472":"###### Family Size","9a800651":"<font color='blue'>Segment D has maximum number of people with low spending score while in Segment C average spending people are more.","7a7c92aa":"---\n## <font color='orange'>Step III: Model Evaluation","c2bc8aa2":"<font color='blue'>In each of the segment the count of cat_6 or proportion of cat_6 is very high i.e. most of the entries in the given data belongs to cat_6.","bf10cd07":"---\n## <font color='orange'>Step II: Model Building","ca62e422":"##### Observation:\n1. `Age` and `Ever_Married` has a positive correlation of 0.6 which means that people who are married have more age as compared to those who are unmarried.\n2. `Age` and `Profession_Healthcare` has a negative correlation of 0.5 which means all those people whose profession is healthcare are younger in age to those who of other professions people.\n3. `Profession_Healthcare` and `Ever_Married` has negative correlation of 0.42 which means all those peoples whose profession is healthcare are unmarried.(only 13% of healthcare professionals are married).\n4. `Age` and `Profession_Lawyer` has a positive correlation of 0.42 which means all those people whose profession is lawyer are older in age to those of other professions people.\n5. `Ever_Married` and `Spending_Average` has a positive correlation which means those who are married spend averagely.(around 42% married people spent averagely)\n6. `Ever_Married` and `Spending_High` has a little positive correlation which means those who are married spend high.(around 25% of married people spent high)\n7. `Ever_Married` and `Spending_Low` has a negative correlation of 0.67 which means those who are unmarried spent low.(round 99% of unmarried people spent low )\n8. `Age` and `Spending_Score` has a positive correlation of 0.42 which means as age increase the spending power also increase.\n9. `Profession_Executives` and `Spending_High` has positive correlation of 0.40 which means all those peoples whose profession is executive spent high.(around 66% of executives spent high).","0fc19987":"###### Segmentation","ae876ca9":"**Ways to treat missing values**<br>\nCheck here:https:\/\/www.datasciencenovice.com\/2020\/08\/5-ways-to-treat-missing-values.html","c662d213":"###### Gender","4c70242e":"![image.png](attachment:image.png)","14a98451":"###### Preprocessing in test data","9c270edd":"#### Building the model with `first type` of dataframe(df_type1)","cdbed785":"#### Predicting on test set","f692743e":"### Variables Description\n\n           \n| Variable\t            | Definition                                                        |\n|---------------------- |-------------------------------------------------------------------|\n| ID\t                | Unique ID                                                         |\n| Gender\t            | Gender of the customer                                            |\n| Ever_Married\t        | Marital status of the customer                                    |\n| Age\t                | Age of the customer                                               |\n| Graduated\t            | Is the customer a graduate?                                       |\n| Profession\t        | Profession of the customer                                        |\n| Work_Experience\t    | Work Experience in years                                          |\n| Spending_Score\t    | Spending score of the customer                                    |\n| Family_Size\t        | Number of family members for the customer(including the customer) |\n| Var_1\t                | Anonymised Category for the customer                              |\n| Segmentation(target)  | Customer Segment of the customer                                  |","50c84e05":"<font color='blue'>Segment C has most number of customers who are graduated while segment D has lowest number of graduate customers.","86caf137":"## <font color='orange'>Step II: Model Building","01b638cd":"#### Building the model with `first_type` of dataframe(df_type1)","26a47a05":"#### Predicting on test set","779b9b8c":"##### Why Spearman?\nCheck this: https:\/\/idkwhoneedstohearthis.blogspot.com\/2020\/05\/correlation-why-spearmans.html","4ac882e8":"###### Var_1","b566426a":"###### Preprocessing on test data","8223fc63":"<font color='blue'>Segment D has people with relatively more experienced than other segments while Segment C has people with low experience","924332e5":"###### Age","d49e1d77":"<font color='blue'>We seen that most of the customers in segment C are married while segment D has least number of married customers. It means segment D is a group of customers that are singles and maybe younger in age. ","6454b561":"<font color='blue'>All the 4 segments have around same number of male-female distribution. In all segment male are more than female. <br> \n<font color='blue'>But segment D has highest male percentage as compared to other segments.","1fbe54bd":"### III. Decision Tree","1c78a8b0":"###### Ever Married","5884270f":"## Algorithms Covered\n\n1. In this notebook, we are going to use **Decision Tree** and **Random Forest** algorithms to solve the same problem.\n\n2. In first notebook, we used **One vs Rest(OvR)** and **One vs One(OvO)** algorithms to solve the problem, [click here to see](https:\/\/www.kaggle.com\/mittalvasu95\/multi-class-classification-c101)\n\n3. In the last notebook, we are going to use **k-NN** and **Naive Bayes** algorithms to solve the same problem, [click here to see](https:\/\/www.kaggle.com\/mittalvasu95\/multi-class-classification-c103?scriptVersionId=43468336)\n\n**Note**:The EDA process is same in all the three notebooks. The only change is in algorithm to solve the problem.","852b3b9f":"###### Preprocessing on train data","8f9af906":"#### Training data","00be1efb":"<font color='blue'>In the given data it is observed that most of the people have family size of 1 or 2 (i.e. they have small family).<br> But Segment D has more number of  big families as compared to other segments.","e73b12b6":"## Multi Class Classification \n- A classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multi-class classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time. \n- Common examples include image classification (is it a cat, dog, human, etc) or handwritten digit recognition (classifying an image of a handwritten number into a digit from 0 to 9).\n- In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification).\n- Multiclass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance.","e72ff944":"![image.png](attachment:image.png)","6c75f73f":"- Random forest is a supervised learning algorithm. It has two variations \u2013 one is used for classification problems and other is used for regression problems. It is one of the most flexible and easy to use algorithm. It creates decision trees on the given data samples, gets prediction from each tree and selects the best solution by means of voting. It is also a pretty good indicator of feature importance.\n- Random forest algorithm combines multiple decision-trees, resulting in a forest of trees, hence the name Random Forest. In the random forest classifier, the higher the number of trees in the forest results in higher accuracy.\n\n#### How does the algorithm work?\nIt works in four steps:\n1. Select random samples from a given dataset.\n2. Construct a decision tree for each sample and get a prediction result from each decision tree.\n3. Perform a vote for each predicted result.\n4. Select the prediction result with the most votes as the final prediction.\n\n#### Difference between Decision Tree and Random Forest\nI will compare random forests with decision-trees. Some salient features of comparison are as follows:-\n1. Random forests is a set of multiple decision-trees.\n2. Decision-trees are computationally faster as compared to random forests.\n3. Deep decision-trees may suffer from overfitting. Random forest prevents overfitting by creating trees on random forests.\n4. Random forest is difficult to interpret. But, a decision-tree is easily interpretable and can be converted to rules.\n\n#### Relationship to nearest neighbour\nA relationship between random forests and the k-nearest neighbours algorithm was pointed out by Lin and Jeon in 2002. It turns out that both can be viewed as so-called weighted neighbourhoods schemes. These are models built from a training set that make predictions for new points by looking at the neighbourhood of the point, formalized by a weight function.<br>\n**Further Reading:**  https:\/\/www.datacamp.com\/community\/tutorials\/random-forests-classifier-python","6023add5":"#### Is ACCURACY everything? \nIn general, there is no general best measure. The best measure is derived from your needs. `In a sense, it is not a machine learning question, but a business question`. It is common that two people will use the same data set but will choose different metrics due to different goals.\n<br><br>\nAccuracy is a great metric. Actually, most metrics are great and I like to evaluate many metrics. However, at some point you will need to decide between using model A or B. There you should use a single metric that best fits your need.<br><br>\nRead more: https:\/\/towardsdatascience.com\/is-accuracy-everything-96da9afd540d","7208b098":"<font color='blue'>The mean age of segment D is 33 and we can say that people in this segment are belong to 30s i.e. they are younger and also from 'ever_married' distribution it is seen that segment D has maximum number of customers who are singles indicating they are younger.<br>\n<font color='blue'>Also segment C has mean age of 49 and we also seen that most cutomers in this segment are married. ","686e37f9":"###### Preprocessing in train data","3f1805b6":"#### Building the model with `second type` of dataframe(df_type2)","ee60c7dd":"### Problem Statement\nAn automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4 and P5). After intensive market research, they\u2019ve deduced that the behavior of new market is similar to their existing market. \n<br><br>\nIn their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for different segment of customers. This strategy has work exceptionally well for them. They plan to use the same strategy on new markets and have identified 2627 new potential customers. \n<br><br>\nYou are required to help the manager to predict the right group of the new customers.<br><br>\nYou can check this link: https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-customer-segmentation\/","a25ec161":"<font color='blue'>1. We have seen that there are `missing values` in the dataset. So we will work on data cleaning.<br>\n<font color='blue'>2. `Create some new attributes` based upon given data\/domain knowledge\/prior experience.<br>\n<font color='blue'>3. Create graphs and `performs EDA` and write observations.","4c482a6f":"#### Predicting on test set","59ccf1c9":"<font color='blue'>1. It is clearly seen that model-1 is far better than model-2.<br>\n<font color='blue'>2. But the point here to note that there are 2327 entries which are common in train-test data i.e. 2327 test entries(out of total 2627) are already in train data and when we see the result on test set model-1 gave 57% accuracy while model-2 gave 63%. Plus other metrices precision, recall and f1-score of model-2 are much better than model-1.<br>\n<font color='blue'>3. So we can conclude that `df_type2 is a better data` for model building in Decision Tree technique.","3256bcab":"#### Predicting on test set","5e16b28b":"###### Spending Score","ffdc761e":"<font color='blue'>Now all the data has been cleaned. There is no missing value and columns are in right format. <br>\n<font color='blue'>All the ids are unique that is there is no duplicate entry.<br>\n<font color='blue'>Created new column: 'Age_Bin', 'Work_Exp_Category' and 'Family_Size_Category'. <br> \n<font color='blue'>Delete only 0.2% of rows. ","343ac67c":"---\n---\n### IV. Random Forest","771f612d":"###### Graduated","3cb7f286":"#### <font color='red'>Making two different dataframes\n<font color='red'>Now we consider\/make two different dataframes apart from the above main dataframe (namely df) <br>\n- `df1`: Spending Score(ranking), Age(normalise), Work_Experience(normalise), Family Size(normalise)\n- `df2`: Spending Score(dummy variables), Age Bin(dummy variables), Work_Exp_Category(dummy variables), Family_Size_Category(dummy variables)","a3f67d43":"###### Profession","32da603c":"---\n## <font color='orange'>Step I: Importing, Cleaning and EDA","3bb2952e":"#### Building the model with `second type` of dataframe(df_type2)","2bda7b27":"<font color='blue'>Segment A,B and C have major customers from profession:**Artist** while Segment D have major customers from profession:**Healthcare** <br>\n**Homemaker** is least in all the four segment","ed6be4db":"---\n## <font color='orange'>Step III: Model Evaluation"}}