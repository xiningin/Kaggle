{"cell_type":{"20492797":"code","628b1400":"code","febe7f34":"code","f27f4cf0":"code","22ff4f8d":"code","8c4462b7":"code","c6a3526f":"code","05707499":"code","73596ae3":"code","bf3bf381":"code","f1085f4e":"code","25be5050":"code","f9ff7416":"code","6994628a":"code","2bc754ac":"code","6939cf67":"code","ca32f6fa":"code","579b9e14":"code","84a48f07":"code","ed45c670":"code","4e8d32ba":"code","03f57e33":"code","2db770b3":"code","85a354de":"markdown"},"source":{"20492797":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","628b1400":"train = pd.read_csv('\/kaggle\/input\/train.tsv',sep = '\\t')\ntest = pd.read_csv('\/kaggle\/input\/test.tsv',sep = '\\t')","febe7f34":"train.head(10)","f27f4cf0":"train['Phrase'].str.len().mean()","22ff4f8d":"train['Sentiment'].value_counts()","8c4462b7":"\nsns.barplot( x = ['2','3','1','4','0'],y=train['Sentiment'].value_counts())\nplt.show()","c6a3526f":"train['Phrase'].str.len().max()","05707499":"\"\"\"\n    Convert data to proper format.\n    1) Shuffle\n    2) Lowercase\n    3) Sentiments to Categorical\n    4) Tokenize and Fit\n    5) Convert to sequence (format accepted by the network)\n    6) Pad\n    7) Voila!\n    \"\"\"\n\n#This method will be using the tokenized values\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\ndef format_data(train,test, max_features, maxlen):\n    #This is to shuffle the data \n    train = train.sample(frac=1).reset_index(drop=True)\n    \n    train['Phrase'] = train['Phrase'].apply(lambda x: x.lower())\n    test['Phrase'] = test['Phrase'].apply(lambda x: x.lower())\n    \n    X = train['Phrase']\n    print(X[1])\n    test_X = test['Phrase']\n    Y = to_categorical(train['Sentiment'].values)\n    \n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(X))\n    \n    X = tokenizer.texts_to_sequences(X)\n    print(X[1])\n    X = pad_sequences(X, maxlen=maxlen)\n    test_X = tokenizer.texts_to_sequences(test_X)\n    test_X = pad_sequences(test_X, maxlen=maxlen)\n\n    return X,Y,test_X","73596ae3":"\n#This method is just for tokenizing .\nX,Y,test_X = format_data(train,test,max_features=10000,maxlen=125)","bf3bf381":"import re\nimport spacy\ndef cleanup_text_word2vec(docs, logging=False):\n    sentences = []\n    counter = 1\n    nlp = spacy.load(\"en_core_web_sm\")\n    for doc in docs:\n        if counter % 1000 == 0 and logging:\n            print(\"Processed %d out of %d documents\" % (counter, len(docs)))\n        # Disable tagger so that lemma_ of personal pronouns (I, me, etc) don't getted marked as \"-PRON-\"\n        doc = nlp(doc, disable=['tagger'])\n        # Grab lemmatized form of words and make lowercase\n        doc = \" \".join([tok.lemma_.lower() for tok in doc])\n        # Split into sentences based on punctuation\n        doc = re.split(\"[\\.?!;] \", doc)\n        # Remove commas, periods, and other punctuation (mostly commas)\n        doc = [re.sub(\"[\\.,;:!?]\", \"\", sent) for sent in doc]\n        # Split into words\n        doc = [sent.split() for sent in doc]\n        sentences += doc\n        counter += 1\n    return sentences","f1085f4e":"\nall_text = np.concatenate((train['Phrase'], test['Phrase']), axis=0)\nall_text = pd.DataFrame(all_text, columns=['Phrase'])\nprint('Number of total text documents:', len(all_text))","25be5050":"length = []\nfor i in range(len(train)):\n    length.append((len(train['Phrase'][i].split())))\n\nprint(pd.Series(length).value_counts())\n","f9ff7416":"train_cleaned_word2vec = cleanup_text_word2vec(all_text['Phrase'], logging=True)","6994628a":"from gensim.models.word2vec import Word2Vec\ndef Word2Vec_(train):\n    #train = train.sample(frac=1).reset_index(drop=True)\n    \n    #train['Phrase'] = train['Phrase'].apply(lambda x: x.lower())\n    #test['Phrase'] = test['Phrase'].apply(lambda x: x.lower())\n    \n    \n    #X = train['Phrase']\n    #print(type(X))\n    #test_X = test['Phrase']\n    #Y = to_categorical(train['Sentiment'].values)\n    \n    #words = pd.concat([X,test_X],axis = 0)\n    #print(len(words))\n    wordvec_model = Word2Vec(train,size = 300, window=5, min_count=3, workers=4,sg = 1)\n    print(\"Word to vec model created\")\n    print(\"%d unique words represented by %d dimensional vectors\" % (len(wordvec_model.wv.vocab), 300))\n    \n\n    return wordvec_model ","2bc754ac":"wordvec_model = Word2Vec_(train_cleaned_word2vec)\n","6939cf67":"print(wordvec_model.wv.most_similar(positive=['boy', 'girl'], negative=['man']))","ca32f6fa":"def create_average_vec(doc,wordvec_model):\n    average = np.zeros((300), dtype='float32')\n    num_words = 0\n    for word in doc.split():\n        if word in wordvec_model.wv.vocab:\n            average = np.add(average, wordvec_model[word])\n            num_words += 1\n    if num_words != 0:\n        average = np.divide(average, num_words)\n    return average","579b9e14":"print(all_text['Phrase'][3],\"\\t\",all_text['Phrase'][0])\nprint(train['Phrase'][3],\"\\t\",train['Phrase'][0])","84a48f07":"train_cleaned_vec = np.zeros((train.shape[0],300),dtype=\"float32\")\nprint(len(train))\nfor i in range(len(train)):\n    if i % 1000 == 0 :\n            print(\"Processed %d out of %d documents\" % (i, len(train)))\n    train_cleaned_vec[i] = create_average_vec(all_text['Phrase'][i],wordvec_model)\n\nprint(\"Train word vector shape :\" , train_cleaned_vec.shape)","ed45c670":"train_cleaned_vec","4e8d32ba":"from sklearn.model_selection import train_test_split\nY = to_categorical(train['Sentiment'].values)\nX_train,X_test,Y_train,Y_test = train_test_split(train_cleaned_vec,Y,test_size = 0.25,random_state = 42)\nprint(X_train.shape,\" and \",Y_train.shape)\nprint(X_test.shape,\" and \",Y_test.shape)","03f57e33":"from keras.layers import Dense, Embedding,LSTM\nfrom keras.models import Sequential \nmodel = Sequential()\n\n# Input \/ Embdedding\nmodel.add(Embedding(300,100,mask_zero=True))\nmodel.add(LSTM(64,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\nmodel.add(LSTM(32,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n\n# Output layer\nmodel.add(Dense(5, activation='sigmoid'))\n\nmodel.summary()","2db770b3":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=32, verbose=1)","85a354de":"Here i am actually trying out some stuff which i think should improve this word to vec model"}}