{"cell_type":{"2e0d2a92":"code","38befbd9":"code","7c9ac17d":"code","7f30d896":"code","dd41715b":"code","d9378926":"code","e7bd04be":"code","fc1077f1":"code","e3d834d1":"code","b877ed45":"code","1b7ac428":"code","b0139c0e":"code","b31b0388":"code","6721c1ad":"code","812c4c1f":"code","27b7773c":"code","fc5d17ce":"code","d1bbe69d":"code","07f47036":"code","b3166989":"code","8600db4f":"code","ce807e1b":"code","161f1666":"code","c48b22ad":"code","5b8cbf0b":"code","ffad143d":"code","07b4f41b":"code","eab4466a":"code","48a30ee0":"code","8f0395f0":"code","732bca82":"code","bbf3fd2d":"code","3589b268":"code","412e69d9":"code","622f6cf4":"code","6dc31bae":"code","37e0ad33":"code","4fb7ba6c":"code","fcbb7ac3":"code","ed63427b":"code","716165f6":"code","94313d75":"code","21ba78d0":"code","20a54c92":"code","7eaca604":"code","25ab9de6":"code","0deee10b":"code","b660b74e":"code","b8c64bbf":"code","41ab8707":"code","12f399a7":"code","4f32f837":"code","39c70ff9":"code","e5babd69":"code","6759664f":"code","3e3b8999":"code","098bebcf":"code","a971b1cf":"code","786582ae":"code","64495030":"code","d41f3b75":"code","125dc900":"code","9f323f55":"code","d6c34786":"code","2cda6543":"code","6f991dc0":"code","4ec9a44c":"markdown","e4bafd79":"markdown","6d6cf985":"markdown","90971f54":"markdown","f1e76f25":"markdown","94300bdd":"markdown","4b589db9":"markdown","2f3ec791":"markdown","e33fa876":"markdown","0c3303de":"markdown","09f27085":"markdown","56e66121":"markdown","c5de3002":"markdown","154da6ad":"markdown","60ad0cf5":"markdown","dfea4129":"markdown","e3d98fb2":"markdown","f7ad41b2":"markdown","f3e1963f":"markdown","4299e004":"markdown","85fc5aba":"markdown","fdbd1864":"markdown","9fa386a7":"markdown","704b2625":"markdown","1b6eb11b":"markdown","95e0797c":"markdown","78a112a9":"markdown","c8ff0256":"markdown","18920ad1":"markdown","7a87cacd":"markdown","6f723c2b":"markdown","6fd8ea76":"markdown","d455815d":"markdown","82addc2b":"markdown","285d9106":"markdown","7f06ce2c":"markdown","6bd19ab4":"markdown","d90b8026":"markdown","fc8eff48":"markdown","1c21bbd2":"markdown","26a9fe17":"markdown","f3dc48ef":"markdown","52a39143":"markdown","24cf015b":"markdown","2eac4f18":"markdown","424bcaee":"markdown","c6cac2ea":"markdown"},"source":{"2e0d2a92":"import numpy as np\nimport pandas as pd\nimport os","38befbd9":"modality = {'01':'full_av','02':'video_only','03':'audio_only'}\nvocal_channel = {'01':'speech','02':'song'}\nemotion = {'01':'neutral','02':'calm','03':'happy','04':'sad','05':'angry','06':'fearful','07':'disgust','08':'surprised'}\nemotional_intensity = {'01':'normal','02':'strong'}\nstatement = {'01':'Kids are talking by the door','02':'Dogs are sitting by the door'}\nreptition = {'01':'first_repitition','02':'second_repetition'}\ndef actor_f(num):\n    if int(num)%2==0: return('female')\n    else: return('male')","7c9ac17d":"actors = sorted(os.listdir('..\/input\/ravdess-emotional-speech-audio'))\nactors.pop()\nactors","7f30d896":"audio_file_dict = {}\nfor actor in actors:\n    actor_dir = os.path.join('..\/input\/ravdess-emotional-speech-audio',actor)\n    actor_files = os.listdir(actor_dir)\n    actor_dict = [i.replace(\".wav\",\"\").split(\"-\") for i in actor_files]\n    dict_entry = {os.path.join(actor_dir,i):j for i,j in zip(actor_files,actor_dict)}\n    audio_file_dict.update(dict_entry)","dd41715b":"audio_file_dict = pd.DataFrame(audio_file_dict).T\naudio_file_dict.columns = ['modality','vocal_channel','emotion','emotional_intensity','statement','repetition','actor']\naudio_file_dict","d9378926":"audio_file_dict.modality = audio_file_dict.modality.map(modality)\naudio_file_dict.vocal_channel = audio_file_dict.vocal_channel.map(vocal_channel)\naudio_file_dict.emotion = audio_file_dict.emotion.map(emotion)\naudio_file_dict.emotional_intensity = audio_file_dict.emotional_intensity.map(emotional_intensity)\naudio_file_dict.statement = audio_file_dict.statement.map(statement)\naudio_file_dict.repetition = audio_file_dict.repetition.map(reptition)\naudio_file_dict['actor_sex'] = audio_file_dict.actor.apply(actor_f)","e7bd04be":"audio_file_dict","fc1077f1":"import matplotlib.pyplot as plt\nimport seaborn as sns","e3d834d1":"fig, (ax1,ax2) = plt.subplots(2, 2,figsize=(12,8))\nax1[0].barh(y=audio_file_dict.emotion.value_counts().index,width=audio_file_dict.emotion.value_counts().values)\nax1[0].set_title('Emotion')\nax1[1].bar(x=audio_file_dict.actor_sex.value_counts().index,height=audio_file_dict.actor_sex.value_counts().values)\nax1[1].set_title('Actor Sex')\nax2[0].bar(x=audio_file_dict.emotional_intensity.value_counts().index,height=audio_file_dict.emotional_intensity.value_counts().values)\nax2[0].set_title('Emotional Intensity')\nax2[1].bar(x=audio_file_dict.statement.value_counts().index,height=audio_file_dict.statement.value_counts().values)\nplt.xticks(rotation=45)\nax2[1].set_title('Statement')\nfig.tight_layout() ","b877ed45":"import torchaudio","1b7ac428":"sample1, sample_rate1 = torchaudio.load('..\/input\/ravdess-emotional-speech-audio\/Actor_01\/03-01-01-01-01-01-01.wav')\nsample1, sample_rate1","b0139c0e":"sample2, sample_rate2 = torchaudio.load('..\/input\/ravdess-emotional-speech-audio\/Actor_01\/03-01-01-01-01-02-01.wav')\nsample2, sample_rate2","b31b0388":"sample1.shape","6721c1ad":"sample2.shape","812c4c1f":"import torch","27b7773c":"torch.mean(sample1), torch.std(sample1), torch.min(sample1), torch.max(sample1)","fc5d17ce":"torch.mean(sample2), torch.std(sample2), torch.min(sample2), torch.max(sample2)","d1bbe69d":"plt.plot(sample1.t().numpy())","07f47036":"plt.plot(sample2.t().numpy())","b3166989":"audio_files = []\nfor i in list(audio_file_dict.index):\n    i, _ = torchaudio.load(i)\n    audio_files.append(i)","8600db4f":"maxlen = 0\nminlen = np.Inf\nfor i in audio_files:\n    if i.shape[1]>maxlen:\n        maxlen = i.shape[1]\n    if i.shape[1]<minlen:\n        minlen = i.shape[1]","ce807e1b":"minlen, maxlen","161f1666":"specgram = torchaudio.transforms.Spectrogram()(sample1)\n\nprint(\"Shape of spectrogram: {}\".format(specgram.size()))\n\nplt.figure()\nplt.imshow(specgram.log2()[0,:,:].numpy(), cmap='gray')","c48b22ad":"spectrograms = []\nfor i in audio_files:\n    specgram = torchaudio.transforms.Spectrogram()(i)\n    spectrograms.append(specgram)","5b8cbf0b":"spectrograms[0].shape,spectrograms[1].shape,spectrograms[2].shape,","ffad143d":"max_width, max_height = max([i.shape[2] for i in spectrograms]), max([i.shape[1] for i in spectrograms])","07b4f41b":"import torch.nn.functional as F","eab4466a":"image_batch = [\n    # The needed padding is the difference between the\n    # max width\/height and the image's actual width\/height.\n    F.pad(img, [0, max_width - img.size(2), 0, max_height - img.size(1)])\n    for img in spectrograms\n]","48a30ee0":"image_batch[0].shape, image_batch[1].shape, image_batch[2].shape,","8f0395f0":"plt.imshow(image_batch[0][0].log2())","732bca82":"image_batch = torch.cat(image_batch,0)","bbf3fd2d":"del audio_files, spectrograms","3589b268":"y = pd.get_dummies(audio_file_dict.actor_sex,drop_first=True)\ny.plot.hist()","412e69d9":"y = torch.from_numpy(np.array(y))\ny.shape","622f6cf4":"from torch.utils.data.dataset import Dataset","6dc31bae":"\n\nclass MyCustomDataset(Dataset):\n    def __init__(self, audio_file_dict):\n        self.audio_fie_dict = audio_file_dict\n        \n    def __getitem__(self, index):\n        img = list(audio_file_dict.index)[index]\n        img, _ = torchaudio.load(img)\n        img = torch.mean(img, dim=0).unsqueeze(0)\n        img = torchaudio.transforms.Spectrogram()(img)\n        img = F.pad(img, [0, max_width - img.size(2), 0, max_height - img.size(1)])\n        \n        def labeler(name):\n            if name == 'male':\n                return(1)\n            else:\n                return(0)\n        \n        label = list(audio_file_dict.actor_sex)[index]\n        label = np.array(labeler(label))\n        label = torch.from_numpy(label)\n        return (img, label)\n\n    def __len__(self):\n        count = len(audio_file_dict)\n        return count","37e0ad33":"from sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(audio_file_dict,test_size=0.3)","4fb7ba6c":"train_data = MyCustomDataset(audio_file_dict=X_train)\ntest_data = MyCustomDataset(audio_file_dict=X_test)","fcbb7ac3":"del image_batch, y","ed63427b":"num_epochs = 50\nnum_classes = 2\nbatch_size = 16\nlearning_rate = 0.000001","716165f6":"from torch.utils.data import DataLoader","94313d75":"train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)","21ba78d0":"import torch.nn as nn","20a54c92":"class ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.drop_out = nn.Dropout()\n        self.fc1 = nn.Linear(242688, 1000)\n        self.fc2 = nn.Linear(1000, 2)\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.drop_out(out)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out\n\nmodel = ConvNet()\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","7eaca604":"model.cuda()","25ab9de6":"# Train the model\ntotal_step = len(train_loader)\n\nfor epoch in range(num_epochs):\n    loss_list = []\n    acc_list = []\n    for i, (images, labels) in enumerate(train_loader):\n        # Run the forward pass\n        images = images.cuda()\n        labels = labels.cuda()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss_list.append(loss.item())\n\n        # Backprop and perform Adam optimisation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Track the accuracy\n        total = labels.size(0)\n        _, predicted = torch.max(outputs.data, 1)\n        correct = (predicted == labels).sum().item()\n        acc_list.append(correct \/ total)\n    print(f'epoch: {epoch}: acc:',np.mean(acc_list),'loss: ',np.mean(loss_list))","0deee10b":"model.eval()","b660b74e":"preds = []\noutcome = []\nlabs = []\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data\n        images = images.cuda()\n        labels = labels.cuda()\n        labs.append(labels)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        preds.append(predicted)\n        c = (predicted == labels).squeeze()\n        outcome.append(c)","b8c64bbf":"outcome = torch.stack(outcome).view(-1).cpu().numpy()","41ab8707":"print('Accuracy on test set after 50 epochs: ',100*round(outcome.sum()\/len(outcome),2),'%')","12f399a7":"class EmotionDataset(Dataset):\n    def __init__(self, audio_file_dict):\n        self.audio_fie_dict = audio_file_dict\n        \n    def __getitem__(self, index):\n        img = list(audio_file_dict.index)[index]\n        img, _ = torchaudio.load(img)\n        img = torch.mean(img, dim=0).unsqueeze(0)\n        img = torchaudio.transforms.Spectrogram()(img)\n        img = F.pad(img, [0, max_width - img.size(2), 0, max_height - img.size(1)])\n        \n        label = pd.get_dummies(audio_file_dict.emotion)[index]\n        label = np.array(label)\n        label = torch.from_numpy(label)\n        return (img, label)\n\n    def __len__(self):\n        count = len(audio_file_dict)\n        return count","4f32f837":"train_data = EmotionDataset(audio_file_dict=X_train)\ntest_data = EmotionDataset(audio_file_dict=X_test)","39c70ff9":"class ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.drop_out = nn.Dropout()\n        self.fc1 = nn.Linear(242688, 1000)\n        self.fc2 = nn.Linear(1000, 8)\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.drop_out(out)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out\n\nmodel = ConvNet()\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","e5babd69":"model.cuda()","6759664f":"# Train the model\ntotal_step = len(train_loader)\n\nfor epoch in range(num_epochs):\n    loss_list = []\n    acc_list = []\n    for i, (images, labels) in enumerate(train_loader):\n        # Run the forward pass\n        images = images.cuda()\n        labels = labels.cuda()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss_list.append(loss.item())\n\n        # Backprop and perform Adam optimisation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Track the accuracy\n        total = labels.size(0)\n        _, predicted = torch.max(outputs.data, 1)\n        correct = (predicted == labels).sum().item()\n        acc_list.append(correct \/ total)\n    print(f'epoch: {epoch}: acc:',np.mean(acc_list),'loss: ',np.mean(loss_list))","3e3b8999":"model.eval()","098bebcf":"preds = []\noutcome = []\nlabs = []\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data\n        images = images.cuda()\n        labels = labels.cuda()\n        labs.append(labels)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        preds.append(predicted)\n        c = (predicted == labels).squeeze()\n        outcome.append(c)","a971b1cf":"outcome = torch.stack(outcome).view(-1).cpu().numpy()","786582ae":"print('Accuracy on test set after 50 epochs: ',100*round(outcome.sum()\/len(outcome),2),'%')","64495030":"from torchvision import models","d41f3b75":"class MultiOutputModel(nn.Module):\n    def __init__(self, n_actor_sex_classes, n_emotion_classes, n_emotional_intensity_classes, n_statement_classes):\n        super().__init__()\n        arch = models.AlexNet().features  # take the model without classifier\n        arch = list(arch.children())\n        w = arch[0].weight\n        arch[0] = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=2, bias=False)\n        arch[0].weight = nn.Parameter(torch.mean(w, dim=1, keepdim=True))\n        self.base_model = nn.Sequential(*arch)\n        \n        last_channel = 256 # size of the layer before the classifier\n\n        # the input for the classifier should be two-dimensional, but we will have\n        # [<batch_size>, <channels>, <width>, <height>]\n        # so, let's do the spatial averaging: reduce <width> and <height> to 1\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n\n        # create separate classifiers for our outputs\n        self.actor_sex = nn.Sequential(\n            nn.Dropout(p=0.2),\n            nn.Linear(in_features=last_channel, out_features=n_actor_sex_classes )\n        )\n        self.emotion = nn.Sequential(\n            nn.Dropout(p=0.2),\n            nn.Linear(in_features=last_channel, out_features=n_emotion_classes)\n        )\n        self.emotional_intensity = nn.Sequential(\n            nn.Dropout(p=0.2),\n            nn.Linear(in_features=last_channel, out_features=n_emotional_intensity_classes)\n        )\n        self.statement = nn.Sequential(\n            nn.Dropout(p=0.2),\n            nn.Linear(in_features=last_channel, out_features=n_statement_classes)\n        )\n    def forward(self, x):\n        x = self.base_model(x)\n        x = self.pool(x)\n\n        # reshape from [batch, channels, 1, 1] to [batch, channels] to put it into classifier\n        #x = torch.flatten(x, start_dim=1)\n        x = x.view(-1)\n        return {\n            'actor_sex': self.actor_sex(x),\n            'emotion': self.emotion(x),\n            'emotional_intensity': self.emotional_intensity(x),\n            'statement': self.statement(x)\n        }\n    def get_loss(self, net_output, ground_truth):\n        actor_sex_loss = F.cross_entropy(net_output['actor_sex'].unsqueeze(0), torch.argmax(ground_truth['actor_sex']).unsqueeze(0))\n        emotion_loss = F.cross_entropy(net_output['emotion'].unsqueeze(0), torch.argmax(ground_truth['emotion']).unsqueeze(0))\n        emotional_intensity_loss = F.cross_entropy(net_output['emotional_intensity'].unsqueeze(0),\n                                                   torch.argmax(ground_truth['emotional_intensity']).unsqueeze(0))\n        statement_loss = F.cross_entropy(net_output['statement'].unsqueeze(0), torch.argmax(ground_truth['statement']).unsqueeze(0))\n        loss = actor_sex_loss + emotion_loss + emotional_intensity_loss + statement_loss\n        return loss, {'actor_sex': actor_sex_loss, 'emotion': emotion_loss, 'emotional_intensity': emotional_intensity_loss,\n                     'statement': statement_loss}","125dc900":"N_epochs = 50\nbatch_size = 1\n\nmodel = MultiOutputModel(n_actor_sex_classes=2,n_emotion_classes=8,n_emotional_intensity_classes=2,n_statement_classes=2).cuda()\n\n\noptimizer = torch.optim.Adam(model.parameters(),lr=0.01)","9f323f55":"import torch","d6c34786":"class MultiLabelDataset(Dataset):\n    def __init__(self, audio_file_dict):\n        self.audio_file_dict = audio_file_dict\n        \n    def __getitem__(self, index):\n        img = list(audio_file_dict.index)[index]\n        img, _ = torchaudio.load(img)\n        img = torch.mean(img, dim=0).unsqueeze(0)\n        img = torchaudio.transforms.Spectrogram()(img)\n        img = F.pad(img, [0, max_width - img.size(2), 0, max_height - img.size(1)])\n        \n        def labeler(name):\n            if name == 'male':\n                return([1,0])\n            else:\n                return([0,1])\n        \n        actor_sex_label = list(audio_file_dict.actor_sex)[index]\n        actor_sex_label = np.array(labeler(actor_sex_label))\n        actor_sex_label = torch.from_numpy(actor_sex_label)\n        \n        emotion_label = pd.get_dummies(audio_file_dict.emotion).iloc[index,:]\n        emotion_label = torch.from_numpy(np.array(emotion_label))\n        \n        emotional_intensity_label = pd.get_dummies(audio_file_dict.emotional_intensity).iloc[index,:]\n        emotional_intensity_label = torch.from_numpy(np.array(emotional_intensity_label))\n        \n        statement_label =  pd.get_dummies(audio_file_dict.statement).iloc[index,:]\n        statement_label = torch.from_numpy(np.array(statement_label))\n        \n        label = {'actor_sex': actor_sex_label.cuda(),\n                'emotion': emotion_label.cuda(),\n                'emotional_intensity':emotional_intensity_label.cuda(),\n                'statement': statement_label.cuda()}\n        \n        return (img, label)\n\n    def __len__(self):\n        count = len(audio_file_dict)\n        return count","2cda6543":"train_data = MultiLabelDataset(audio_file_dict=X_train)\ntest_data = MultiLabelDataset(audio_file_dict=X_test)\ntrain_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)","6f991dc0":"for epoch in range(0, N_epochs + 1):\n    total_loss = []\n    total_accuracy = []\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n        img, labels = batch\n        img = img.cuda()\n        output = model(img)\n        \n        loss_train, losses_train = model.get_loss(output, labels)\n        total_loss.append(loss_train.item())\n        \n        acc = 100*(torch.sum(torch.tensor([torch.argmax(output['actor_sex'])==torch.argmax(labels['actor_sex']),\n            torch.argmax(output['emotion'])==torch.argmax(labels['emotion']),\n            torch.argmax(output['emotional_intensity'])==torch.argmax(labels['emotional_intensity']),\n            torch.argmax(output['statement'])==torch.argmax(labels['statement'])]))\/4.)\n        total_accuracy.append(acc)\n        \n        \n    print(f'Epoch: {epoch}: Loss: ',np.mean(total_loss),' Accuracy: ',np.mean(total_accuracy))","4ec9a44c":"# Classifying emotion\n\nNext, we'll step up our game a little and try to predict emotions, rather than just the binary case of male\/ female. Before starting with this, an initial guess is that this might be a bit harder to get good accuracy with, but let's see what we can come up with:","e4bafd79":"Now, we evaluate how well our model does on the unseen data in the test set:","6d6cf985":"Now, we grab the maximum heights and widths for all the spectrograms:","90971f54":"Now, for each of the actors, we obtain the label information from the filenames:","f1e76f25":"We need the **Torch Audio** library to be able to load the .wav files in this dataset:","94300bdd":"# Classifying male vs female voices\n\nThis should hopefully be easy, but you never know...\n\nFirst, we one-hot encode the actor sex column and plot the result, confirming that we have similar numbers of male and female actors, making accuracy an easy to implement metric to evaluate the success of our model:","4b589db9":"Let's plot an example waveform:","2f3ec791":"Again, move model onto the GPU device:","e33fa876":"We then get the minimum and maximum lengths for all the files in our dataset:","0c3303de":"Now, we can go ahead and delete these fairly large objects, to free up some more memory:","09f27085":"We define a convolutional neural network, with 4 convolutional layers, with increasing number of filters, each with a kernel size of 5x5, followed by maxpooling downsampling with a window of 2x2 and lastly two fully connected layers.\n\nWe will use CrossEntropy as our loss function and Adam as our optimizer:","56e66121":"Next, we loop through all of our loaded waveforms and transform each one into a spectrogram:","c5de3002":"# Classifying multiple attributes with a single model, using transfer learning:\n\nLastly, we want to use transfer learning to take a pretrained neural network, chop off its head and give it four new heads (shameless GOT reference: how many heads does the Dragon have? - It's not 4...)\n\nWe then want to use this single model to predict 4 different outcomes:\n* Actor Sex\n* Emotion\n* Emotional Intensity\n* Statement","154da6ad":"Look at a couple of examples of their shapes:","60ad0cf5":"We import the torch library and have a look at some very basic descriptive about the examples we have loaded:","dfea4129":"But really, in order to be able to properly train on this dataset, we need to define our own custom PyTorch Dataset class, in order to get the relevant files from disk only as they're needed, thus freeing up memory:","e3d98fb2":"Plotting an example, showing the padded space on the right:","f7ad41b2":"We collapse this list of tensors, which are all the same size now, along the first dimension:","f3e1963f":"We transform one of our example waveforms into a spectrogram and visualize the result:","4299e004":"We cast this as a pandas dataframe, but need to transpose it so that the labels appear in the columns and the filenames become the row indices, we also give our columns appropriate names:","85fc5aba":"And we use the pad function from torch.nn.functional, to pad images that are smaller than these maximum sizes with zeros, to make them all the same shape:","fdbd1864":"We create various dictionaries and a function to decipher this encoded information in the filename, in order to be able to train our Deep Learning models to predict them.","9fa386a7":"Train for 50 epochs:","704b2625":"We just delete some objects in our workspace which are chowing our limited available RAM:","1b6eb11b":"Import plotting libraries:","95e0797c":"The following information is given in the dataset description. Essentially, the name of the file is an encoded representation of each of the following aspects:","78a112a9":"Similarly, we use the same model architecture, only changing the number of neurons in the output layer from 2 to 8, representing the 8 possible emotions in the set of labels we've been given:","c8ff0256":"Here, we just get a full list of all the actors, removing a piece of irrelevant information via the .pop() method:","18920ad1":"Unfortunately, the files do not have the same lengths, which will require us to do some extra processing later:","7a87cacd":"Now, we can see that they are all the same shape:","6f723c2b":"Import some packages to get us started:","6fd8ea76":"We set up some hyperparameters before training. We will see what 50 epochs can achieve, at a batch size of 16, with a relatively low learning rate of 1e-5","d455815d":"We have access to a GPU on Kaggle, thankfully, so we move our model to the CUDA device:","82addc2b":"Here, we train our model, printing accuracy and loss metrics after each of 50 epochs:","285d9106":"Accuracy on test set:","7f06ce2c":"**Filename identifiers**\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n\n* Vocal channel (01 = speech, 02 = song).\n\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).","6bd19ab4":"Next, we encode our audio data into a spectrogram:\n\nFrom : [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Spectrogram#:~:text=A%20spectrogram%20is%20a%20visual,sonographs%2C%20voiceprints%2C%20or%20voicegrams.)\n> A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. When applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams. ","d90b8026":"We plot a few of the labels we might want to predict in this exercise, below. Mostly there is an even class distribution. We shouldn't have to use any sampling techniques to account for class imbalances","fc8eff48":"Now, in order to get around the fact that we have audio files of differing lengths, we first load all the files into a python list:","1c21bbd2":"# Various classifications of speech data, via Deep Learning methods, built using PyTorch","26a9fe17":"We use the same data loader, but just adjust it to load the emotion associated with each file, instead of the sex of the actor, as the labels to be predicted:","f3dc48ef":"Next, we use a dataloader each to load our training and test sets, shuffling the training, but not the test set:","52a39143":"Now, we use the dictionaries and function created above to transform the digit-encoded labels into human-readable format:","24cf015b":"Having a look at what loading an audio file returns, we see a 2D-tensor as well as the sampling rate gets returned for each file","2eac4f18":"We split our data into training- (70%) and testing- (30%) sets:","424bcaee":"We put our model into evaluation mode, disabling features only needed during training, such as Dropout:","c6cac2ea":"So, we get just under 80% accuracy on our training dataset after 50 epochs, let's see how we do on unseen data:"}}