{"cell_type":{"8a847f5e":"code","b60d594e":"code","9d8cc3a5":"code","72a2c104":"code","380518aa":"code","7cfe8132":"code","7679537f":"code","595a7eda":"code","9c60c77b":"code","9a483601":"code","7ddf475f":"code","f135b293":"code","e2f59991":"code","1df6626c":"code","3e9d5ad8":"code","1244a23e":"code","71013a8f":"code","67204e54":"code","6183e471":"code","e07f56e8":"code","3da01e3e":"code","a0eed4e7":"code","26043e81":"code","7b059409":"code","8eb4467e":"code","37fd18e1":"code","35f0512f":"code","99cd5bca":"code","3cd62450":"code","9a176869":"code","ee6d5d9a":"code","127971df":"code","a0c12648":"code","193ec2e9":"code","8492799a":"code","2df715f0":"code","246e1e46":"code","cdffcbe9":"code","3943da5c":"code","28066e7a":"markdown","f16cd669":"markdown","d5fa5aa3":"markdown","6dc83da4":"markdown","e36f59fc":"markdown","7d356dcc":"markdown","9ac4e8f7":"markdown","d4e0c231":"markdown","42706718":"markdown","d6c28d77":"markdown"},"source":{"8a847f5e":"import numpy as np\nfrom os import path\nimport matplotlib.pyplot as plt\n\nimport sklearn\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim","b60d594e":"!pip install captum","9d8cc3a5":"from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients","72a2c104":"from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation","380518aa":"boston = load_boston()","7cfe8132":"feature_names = boston.feature_names\nX = boston.data\ny = boston.target","7679537f":"torch.manual_seed(12134)","595a7eda":"np.random.seed(1234)","9c60c77b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","9a483601":"fig, axs = plt.subplots(nrows=3, ncols=5, figsize=(30, 20))\nfor i, (ax, col) in enumerate(zip(axs.flat, feature_names)):\n    x = X[:, i]\n    pf = np.polyfit(x, y, 1)\n    p = np.poly1d(pf)\n    ax.plot(x, y, 'o')\n    ax.plot(x, p(x), 'r--')\n    ax.set_title(col + ' vs Prices')\n    ax.set_xlabel(col)\n    ax.set_ylabel('Prices')","7ddf475f":"X_train = torch.tensor(X_train).float()\ny_train = torch.tensor(y_train).view(-1, 1).float()\nX_test = torch.tensor(X_test).float()\ny_test = torch.tensor(y_test).view(-1, 1).float()\ndatasets = torch.utils.data.TensorDataset(X_train, y_train)","f135b293":"train_iter = torch.utils.data.DataLoader(datasets, batch_size = 10, shuffle=True)","e2f59991":"batch_size= 50\nnum_epochs = 200\nlearning_rate = 0.0001\nsize_hidden1 = 100\nsize_hidden2 = 50\nsize_hidden3 = 10\nsize_hidden4 = 1","1df6626c":"class BostonModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin1 = nn.Linear(13, size_hidden1)\n        self.relu1 = nn.ReLU()\n        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n        self.relu2 = nn.ReLU()\n        self.lin3 = nn.Linear(size_hidden2, size_hidden3)\n        self.relu3 = nn.ReLU()\n        self.lin4 = nn.Linear(size_hidden3, size_hidden4)\n    def forward(self, input):\n        return self.lin4(self.relu3(self.lin3(self.relu2(self.lin2(self.relu1(self.lin1(input)))))))","3e9d5ad8":"model = BostonModel()\nmodel.train()","1244a23e":"!pip install torchviz","71013a8f":"dummy_input = torch.zeros([1, 13])","67204e54":"dummy_input","6183e471":"dummy_out = model(dummy_input)","e07f56e8":"from torchviz import make_dot\nmake_dot(dummy_out)","3da01e3e":"criterion = nn.MSELoss(reduction='sum')\ndef train(model_inp, num_epochs = num_epochs):\n    optimizer = optim.RMSprop(model_inp.parameters(), lr=learning_rate)\n    for epoch in range(num_epochs):\n        running_loss = 0.\n        for inputs , labels in train_iter:\n            outputs = model_inp(inputs)\n            loss= criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            running_loss += loss.item()\n            optimizer.step()\n        if epoch % 20 == 0:\n            print(f'Epoch {epoch+1}\/{num_epochs} running accumulative loss {running_loss:.3f}')","a0eed4e7":"def train_load_save_model(model_obj, model_path):\n    if path.isfile(model_path):\n        print(\"Loading pretrain model  from {}\".format(model_path))\n        model_obj.load_state_dict(torch.load(model_path))\n    else:\n        train(model_obj)\n        print('Finished training the model. Sasving teh model to the path: {}'.format(model_path))\n        torch.save(model_obj.state_dict(), model_path)","26043e81":"SAVED_MODEL_PATH = 'boston_model.pt'\ntrain_load_save_model(model, SAVED_MODEL_PATH)","7b059409":"model.eval()\noutputs = model(X_test)\nerr = np.sqrt(mean_squared_error(outputs.detach().numpy(), y_test.detach().numpy()))\nprint('model err: ', err)","8eb4467e":"%%time\nig = IntegratedGradients(model)\nig_attr_test = ig.attribute(X_test, n_steps = 50)","37fd18e1":"%%time\nig_nt = NoiseTunnel(ig)\nig_nt_attr_test = ig_nt.attribute(X_test)","35f0512f":"%%time\ndl = DeepLift(model)\ndl_attr_test = dl.attribute(X_test)","99cd5bca":"%%time\ngs = GradientShap(model)\ngs_attr_test = gs.attribute(X_test, X_train)","3cd62450":"%%time\nfa = FeatureAblation(model)\nfa_attr_test = fa.attribute(X_test)","9a176869":"x_axis_data = np.arange(X_test.shape[1])\nx_axis_data_labels = list(map(lambda idx: feature_names[idx], x_axis_data))\n\nig_attr_test_sum = ig_attr_test.detach().numpy().sum(0)\nig_attr_test_norm_sum = ig_attr_test_sum \/ np.linalg.norm(ig_attr_test_sum, ord=1)\n\nig_nt_attr_test_sum = ig_nt_attr_test.detach().numpy().sum(0)\nig_nt_attr_test_norm_sum = ig_nt_attr_test_sum \/ np.linalg.norm(ig_nt_attr_test_sum, ord=1)\n\ndl_attr_test_sum = dl_attr_test.detach().numpy().sum(0)\ndl_attr_test_norm_sum = dl_attr_test_sum \/ np.linalg.norm(dl_attr_test_sum, ord=1)\n\ngs_attr_test_sum = gs_attr_test.detach().numpy().sum(0)\ngs_attr_test_norm_sum = gs_attr_test_sum \/ np.linalg.norm(gs_attr_test_sum, ord=1)\n\nfa_attr_test_sum = fa_attr_test.detach().numpy().sum(0)\nfa_attr_test_norm_sum = fa_attr_test_sum \/ np.linalg.norm(fa_attr_test_sum, ord=1)\n\nlin_weight = model.lin1.weight.detach().numpy().sum(0)\ny_axis_lin_weight = lin_weight \/ np.linalg.norm(lin_weight, ord=1)","ee6d5d9a":"model.lin1.weight.shape","127971df":"width = 0.14\nlegends = ['Int Grads', 'Int Grads w\/SmoothGrad','DeepLift', 'GradientSHAP', 'Feature Ablation', 'Weights']\n\nplt.figure(figsize=(20, 10))\n\nax = plt.subplot()\nax.set_title('Comparing input feature importances across multiple algorithms and learned weights')\nax.set_ylabel('Attributions')\n\nFONT_SIZE = 16\nplt.rc('font', size=FONT_SIZE)            # fontsize of the text sizes\nplt.rc('axes', titlesize=FONT_SIZE)       # fontsize of the axes title\nplt.rc('axes', labelsize=FONT_SIZE)       # fontsize of the x and y labels\nplt.rc('legend', fontsize=FONT_SIZE - 4)  # fontsize of the legend\n\nax.bar(x_axis_data, ig_attr_test_norm_sum, width, align='center', alpha=0.8, color='#eb5e7c')\nax.bar(x_axis_data + width, ig_nt_attr_test_norm_sum, width, align='center', alpha=0.7, color='#A90000')\nax.bar(x_axis_data + 2 * width, dl_attr_test_norm_sum, width, align='center', alpha=0.6, color='#34b8e0')\nax.bar(x_axis_data + 3 * width, gs_attr_test_norm_sum, width, align='center',  alpha=0.8, color='#4260f5')\nax.bar(x_axis_data + 4 * width, fa_attr_test_norm_sum, width, align='center', alpha=1.0, color='#49ba81')\nax.bar(x_axis_data + 5 * width, y_axis_lin_weight, width, align='center', alpha=1.0, color='grey')\nax.autoscale_view()\nplt.tight_layout()\n\nax.set_xticks(x_axis_data + 0.5)\nax.set_xticklabels(x_axis_data_labels)\n\nplt.legend(legends, loc=3)\nplt.show()","a0c12648":"# Compute the attributions of the output with respect to teh inpus of teh fourth linear layer.\nlc = LayerConductance(model, model.lin4)\nlc_attr_test = lc.attribute(X_test, n_steps= 100, attribute_to_layer_input = True)","193ec2e9":"# Shape\nlc_attr_test","8492799a":"lc_attr_test = lc_attr_test[0]","2df715f0":"lc_attr_test","246e1e46":"# weights fromo forth linear layer\nlin4_weight = model.lin4.weight","cdffcbe9":"lin4_weight","3943da5c":"plt.figure(figsize=(15, 8))\nx_axis_data = np.arange(lc_attr_test.shape[1])\n\ny_axis_lc_attr_test = lc_attr_test.mean(0).detach().numpy()\ny_axis_lc_attr_test = y_axis_lc_attr_test\/np.linalg.norm(y_axis_lc_attr_test, ord=1)\ny_axis_lin4_weight = lin4_weight[0].detach().numpy()\ny_axis_lin4_weight = y_axis_lin4_weight\/np.linalg.norm(y_axis_lin4_weight, ord=1)\n\n\nx_axis_labels = [f'Neuron {i}' for i in range(len(y_axis_lin4_weight))]\n\nax = plt.subplot()\nax.set_title('Aggreggated neuron importances and learned weights in the last linear layer of the model')\nax.bar(x_axis_data + width, y_axis_lc_attr_test, width, align='center', alpha = 0.5, color='red')\nax.bar(x_axis_data + 2 * width, y_axis_lin4_weight, width, align='center', alpha=0.5, color='green')\nplt.legend(legends, loc=2, prop={'size': 20})\nax.autoscale_view()\nplt.tight_layout()\nax.set_xticks(x_axis_data + 0.25)\nax.set_xticklabels(x_axis_labels)\n","28066e7a":"# Attributing to the layers and comparing with model weights\nNow let's bedie attributing to the inputs of the model, also attribute to the layers of the model and understand which neurons appear to be more importantj.\n\nIn the cell below we will attribute to teh inputs of teh second linear layer of our model. Similar to the previous case, the attribution is performed on the test datase.","f16cd669":"# Data Exploration","d5fa5aa3":"# Data loading and pre-processing\nLet's load boston house prices dataset and correpsonding labels from scikit-learn library.","6dc83da4":"# Interpret regression models using Boston House Prices Dataset\nThis notebook demonstrates how to apply Captum liibrary on a regression model and understand important features, layers\/neurons that contribute to the prediction. It compares ","e36f59fc":"# Tensorizing inputs and creating batches","7d356dcc":"It is interesting to observe that the weights and attribution scores are well  aligned for all 10 neurons in teh last layer. Meaning that the neurons with negative weights also have negative attribution scores and we can observe the same for the positive weights and attributions.\n\nWe also observe that the neurons five and nine have very small attributions but relatively larger weights. Another interesting thing to observe is that the weights do noot fluctuate much whereas attributions do fluctuate more relative to that and spike in neuron 4.","9ac4e8f7":"Now let's visualize attribution scores  with respect to inputs (using test dataset) for our simple model in one plot. This wll help us to understand how similar or different the attribution scores assigned fromo different attribution algorithms are. Apart from that we will also comare attribution scores with the learned model weights.\n\nIt is iimportant to note that we aggregate the attributiions across the entire test dataset in order to rettain a global view of feature importance. This, however, is not ideal since the attribtion can cancel out each other whebn we aggregate then across multiple samples.","d4e0c231":"# Train Boston Model","42706718":"# Comparing different attribution algorithms\nLet's compute the attributions witth respect to the inputs of teh model iusing different attribution algorithms from core `Captum` library and visualize those attributions. WE use test dataset defined in the cell above for this purpose.\n\nWe use mainly default settings, such as default baselines, number of steps etc., for all algorithms, however  you are welcome to play with the settings. Fro GradientSHAP specifically we use the entire treaining dataset as the distribution of baselines.","d6c28d77":"The magnitudes of learned model weights tell us about the correlations between the dependent variable `Price` and each independent variable. Zero weight means no correlation whereas positive weights indiicate positive correlations and negative weights the opposite. Sicne the network has more than one layer these weights might not be directly correlated with the price.\n\n\nFrom the plot above we can see that the attribution algorithms sometimes disagree on assigning importance scores and that they are not always aligned with weights. However, we can still observe that the top importance features are `LSAT`, `RM` and `PTRATIO` are also considered to be important based on both most attribution algorithms and the weight scores.\n\nIt is interesting to observe that the feature `B` has high positive attribution score bsed on some of teh attribution algorithms. This can be related, for example, to the choice of the baseline. In this tutorial we use zero-valued baselines for all features, however, if we were to choose those values more carefully for each feature the picture will change. Similar arguments apply also when teh signs of the weights and attributions mismatches or when one algirthm assigns higher or lower attribution scores compare to the oothers.\n\nIn terms of least important features, we can bserve that `CHAS` and `RAD` are voted to be least important both based n most attribution algorithms and learned coefficients.\n\nAnother interesting observation is that both IG and DeepLift return similar attribution scores across all features. This is associated with the fact that algthough we have non-linearities in our model, their effects aren't significant and DeepLift is close o `(input-baselines)*gradients`. And because the gradients do not change significantly along the stright line from baseline input , we observe similar situation with Integrated Gradient as well."}}