{"cell_type":{"0b56aea8":"code","976e54d9":"code","fc9e1e9e":"code","35030b18":"code","de9a6759":"code","78eab720":"code","4e377507":"code","fe74e2ec":"code","3bf76b68":"code","4ca22da9":"code","53bedb48":"code","564d5bc4":"markdown","7603c5c8":"markdown","e62e1b0a":"markdown","ec331840":"markdown","3fe6cd6a":"markdown","d07d5bf3":"markdown","c2bc0f04":"markdown","2fc88ee4":"markdown","ccf1840c":"markdown","89ba5158":"markdown"},"source":{"0b56aea8":"#imports \nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.datasets\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport optuna \nimport lightgbm as lgb\nimport xgboost as xgb\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt","976e54d9":"train = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest  = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')\n\n#separating training data, all the columns and target columns \nfeatures = [f'cont{x}'for x in range(1,15)]\n\ndata = train[features]\nX_test = test[features]\ntarget = train['target']\ntrain.head()","fc9e1e9e":"# some part of this code is adapted from this great notebook by Lavanya for beginners\n#https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition\n#we will be analyzing the target value first\n\n#setting up the platform\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n\nsns.distplot(target, color=\"g\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Values\")\nax.set(xlabel=\"Target\")\nax.set(title=\"Target distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","35030b18":"# we will look into the features distribution now, to get insight into the data\ni = 1\nplt.figure()\nfig, ax = plt.subplots(5, 3,figsize=(14, 24))\nfor feature in features:\n    plt.subplot(5, 3,i)\n    sns.distplot(train[feature],color=\"blue\", kde=True,bins=120, label='train')\n    sns.distplot(test[feature],color=\"red\", kde=True,bins=120, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","de9a6759":"corr = train.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"inferno\", square=True)","78eab720":"#normalising the training data\ntrain_data = (data-data.mean())\/data.std()\ntest_data = (X_test - X_test.mean())\/X_test.std()","4e377507":"# code and parameters adapted from this great notebook by Hamza, \n#https:\/\/www.kaggle.com\/hamzaghanmi\/xgboost-hyperparameter-tuning-using-optuna\n\nBest_trial = {'lambda': 0.0030282073258141168, 'alpha': 0.01563845128469084, 'colsample_bytree': 0.5,\n             'subsample': 0.7,'n_estimators': 4000, 'learning_rate': 0.01,'max_depth': 15,\n             'random_state': 2020, 'min_child_weight': 257,'tree_method':'gpu_hist'\n             ,'predictor': 'gpu_predictor'}\n\n#you might want to turn on the GPU from the Accelerator or just remove 'tree_method':'gpu_hist'\n#from the parameters list\ntrain = train_data\ntest = test_data\n#train on 7 folds cross validation\npreds = np.zeros(test.shape[0])\n#creating 7 folds\nkf = KFold(n_splits=7,random_state=48,shuffle=True)\nrmse=[]\nn=0\nfor trn_idx, test_idx in kf.split(train[features],target):\n    #separating training and validation data from training columns\n    X_tr,X_val=train[features].iloc[trn_idx],train[features].iloc[test_idx]\n    #separating training and validation data from target values\n    y_tr,y_val=target.iloc[trn_idx],target.iloc[test_idx]\n    \n    #xgboost regressor with optimized parameters \n    model = xgb.XGBRegressor(**Best_trial)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    \n    #predicting on test data provided in separate file(actual test data not validation)\n    preds+=model.predict(test[features])\/kf.n_splits\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    print(n+1,rmse[n])\n    n+=1\nprint(f\"mean RMSE for all the folds is {np.mean(rmse)}\")","fe74e2ec":"#ensemble with notebook submission found at great kernal\n#https:\/\/www.kaggle.com\/somayyehgholami\/results-driven-tabular-playground-series-20\nsub1 = pd.read_csv('..\/input\/results-driven-tabular-playground-series-201\/submission.csv')\npredictions1 = sub1['target'].tolist()       #will give about 0.69673 with 75,25 ratio \n\nresults = [x*0.66 + y*0.34 for x, y in zip(predictions1, preds)]\n# sub['target']=results\n# sub.to_csv('submission.csv', index=False)","3bf76b68":"# parameters from this awesome kernal \n# https:\/\/www.kaggle.com\/hamditarek\/tabular-playground-series-xgboost-lightgbm\nBest_trial ={'random_state': 33,'n_estimators':5000,\n 'min_data_per_group': 5,\n 'boosting_type': 'gbdt',\n 'device_type' : 'gpu',\n 'num_leaves': 256,\n 'num_iterations' : 5000,\n 'max_dept': -1,\n 'learning_rate': 0.005,\n 'subsample_for_bin': 200000,\n 'lambda_l1': 1.074622455507616e-05,\n 'lambda_l2': 2.0521330798729704e-06,\n 'n_jobs': -1,\n 'cat_smooth': 1.0,\n 'silent': True,\n 'importance_type': 'split',\n 'metric': 'rmse',\n 'feature_pre_filter': False,\n 'bagging_fraction': 0.8206341150202605,\n 'min_data_in_leaf': 100,\n 'min_sum_hessian_in_leaf': 0.001,\n 'bagging_freq': 6,\n 'feature_fraction': 0.5,\n 'min_gain_to_split': 0.0,\n 'min_child_samples': 20}","4ca22da9":"preds = np.zeros(test.shape[0])\n#creating 10 folds\nkf = KFold(n_splits=10,random_state=48,shuffle=True)\nrmse=[]\nn=0\nfor trn_idx, test_idx in kf.split(train[features],target):\n    #separating training and validation data from training columns\n    X_tr,X_val=train[features].iloc[trn_idx],train[features].iloc[test_idx]\n    #separating training and validation data from target values\n    y_tr,y_val=target.iloc[trn_idx],target.iloc[test_idx]\n    \n    #xgboost regressor with optimized parameters \n    model = lgb.LGBMRegressor(**Best_trial)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=50,verbose=False)\n    \n    #predicting on test data provided in separate file(actual test data, not the validation)\n    preds+=model.predict(test[features])\/kf.n_splits\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    print(n+1,rmse[n])\n    n+=1\nprint(f\"mean RMSE for all the folds is {np.mean(rmse)}\")","53bedb48":"#ensembling old results with light GBM model\n\n# sub1 = pd.read_csv('..\/input\/submission-result-driven-notebook\/submission(9).csv')\n# fin_res  = sub1['target'].tolist() \n\nfinal_results = [x*0.72 + y*0.28 for x, y in zip(results, preds)]\nsub['target']=final_results\nsub.to_csv('submission.csv', index=False)","564d5bc4":"### Importing data and getting a peak into it","7603c5c8":"## Part 3 LightGBM Model","e62e1b0a":"## Part 1 - Some EDA ","ec331840":"## Part 2 - training XGBoostRegressor (tuned using Optuna)\n\nXGBoost works pretty well with regression tasks of these types.\nHere, we are training the XGBoost model which is tuned using optuna. You can find how to optimize the hyperparameters [here](https:\/\/www.kaggle.com\/hamzaghanmi\/xgboost-hyperparameter-tuning-using-optuna) Great work by hamza. \nI have found these sets of hyper parameters works pretty well and performs better than normal parameters, I have found it by running it with multiple sets, so You may be interested in tuning those with GPUs enabled and multiple tries)","3fe6cd6a":"We can see that test and train data are almost identical, means the data distribution is looking same in the graph.\n\nAlso features, cont6, cont12, cont11 and cont13 looks similar in some fashion, Let's look at what the correlation data tells us about that.","d07d5bf3":"Here we are building LightGBM model which is very power tool when tuned. We have tuned it using optuna(in another notebook), but make sure you run this on gpu as it will take a lot time in CPU iterations. The hyper-parameters are adapted from [this kernal](https:\/\/www.kaggle.com\/hamditarek\/tabular-playground-series-xgboost-lightgbm) but works with Normalized dataset pretty well. We are having normalized dataset which works slightly better than normal dataset (for XGBoost marginal difference) so we will stick to that for the next model.","c2bc0f04":"We will be looking at the features distribution, its correlation and Train, Test data distribution correspondance(if it doesn't match the exact distribution we can try altering it) and finally The target data distribution. ","2fc88ee4":"We notice two things here:\n\n1) The distribution is skewed as the left part is streached and              \n2) It is made of multiple normal distribution (atleast two) as we can see the distribution splits around 8.","ccf1840c":"### Starting with Training Data Visualization","89ba5158":"Somehow the features cont6 to cont13 seem to be correlated!!\n\n(I have tried only training XGBoost on them, but other features are also seem to play an important role in predicting the target, so the they are not the only ones impacting the regression results)"}}