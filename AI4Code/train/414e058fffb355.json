{"cell_type":{"fad1e6b3":"code","33552a10":"code","076c8ac0":"code","230751a5":"code","e4f17422":"code","e7deea00":"code","358b4a56":"code","a34a9842":"code","9a0450c5":"code","f8fd94e4":"code","111fa318":"code","ff0cdd7a":"code","6289f991":"code","9f7f0ce3":"code","af815d37":"code","41e44639":"code","53bc0b47":"code","512ddcf9":"code","8b4b3f71":"code","50615e1a":"code","a9f902d8":"code","9e10c5a2":"code","e5fa1370":"code","cc90d0da":"code","d7d512a5":"code","589be445":"code","73e29819":"code","695a7206":"code","20243732":"code","7302e8cb":"code","12365634":"code","8e03d3b2":"code","cd174b16":"code","e1fc1513":"code","382b3557":"code","4abd4b23":"code","403cd5a3":"code","f8016bec":"code","083df27d":"code","1a27c52e":"code","17fa7790":"code","2c5c3e0f":"code","9c0c9141":"code","845ebbd0":"code","53c2439f":"code","0dd0e94f":"code","6093dc8e":"code","942b9900":"code","106fdd51":"code","a5e1d5d8":"code","da3bbf54":"code","d7ea2b07":"code","6d2c3af7":"code","c23894ba":"code","f58f9fd0":"code","df8885e7":"code","23b38dd7":"code","c4d304be":"code","11bbf8c1":"code","dfcb4a5b":"code","f7ffc29a":"code","aa22c41a":"code","217a4695":"code","37c359b8":"code","2f8039c5":"code","68c590cb":"code","492f4f01":"code","fcb55a85":"code","e83e5cc4":"code","270a7d39":"code","fc08509b":"code","4b3a2854":"code","c3f9b1eb":"code","2a829cbc":"code","0f27b94e":"code","901ce107":"code","16d1b876":"code","768bf908":"code","a512def7":"code","77330cc6":"code","54272863":"code","6391cddd":"code","e7a39a6e":"code","ebf5e0a4":"code","cf71544d":"code","a8316a4f":"code","9bb0d30c":"code","7801f2fb":"code","b813a8d7":"code","b74d5a1b":"markdown","7b9a36a8":"markdown","e5ceb3e9":"markdown","5ffc71af":"markdown","4b48bd53":"markdown","bb7bfec2":"markdown","075a9133":"markdown","117fd8dd":"markdown","16bf6dc0":"markdown","abecda9e":"markdown","0eda3e43":"markdown","03fa2768":"markdown","13053b38":"markdown","f38e6206":"markdown","1fc283a7":"markdown","c2324b28":"markdown","0dd91624":"markdown","5c8d6cc0":"markdown","bc66e2f5":"markdown","fddae246":"markdown","cb525657":"markdown","097ef07e":"markdown","84fd064b":"markdown","346dfd1b":"markdown","122ee3f4":"markdown","a70c448e":"markdown","1a87be1e":"markdown","ceaa1c68":"markdown","ed6772b7":"markdown","c6c8c235":"markdown","bab1568c":"markdown","39527b38":"markdown","a95b8754":"markdown","662facfb":"markdown","3931c52b":"markdown","b456977f":"markdown","1fb753e4":"markdown","81b339df":"markdown","81493124":"markdown","4e8a62be":"markdown","b8f781fa":"markdown","bac4b7b3":"markdown","2d86ff85":"markdown","08090da7":"markdown","0769845b":"markdown","ddecb14d":"markdown","54aa4149":"markdown","a8af3df8":"markdown","08f31c91":"markdown","ec9f676b":"markdown","7fb026b5":"markdown","2460035d":"markdown","5df5354d":"markdown","7f8cf21e":"markdown","df347f8e":"markdown","4aa75d10":"markdown","18d271da":"markdown","fe4758dc":"markdown","2eeace6d":"markdown","b1170643":"markdown","f076b5fb":"markdown","b880fff1":"markdown","aa8f4294":"markdown","50a0d469":"markdown","07b9c1df":"markdown","3e03860d":"markdown","8c2a8193":"markdown","8a3d4716":"markdown","d98526db":"markdown","bcb03279":"markdown","e1ae5904":"markdown","dfe8f3c6":"markdown","27e84e7c":"markdown","1bea3652":"markdown","217843fa":"markdown","fc38d626":"markdown","f9a1195e":"markdown","51922426":"markdown","1aeb9461":"markdown","a7302864":"markdown","e45c679e":"markdown","3dff241a":"markdown","6db35b32":"markdown","f5c84424":"markdown","d97be3f2":"markdown","7006e065":"markdown","493459dc":"markdown","734020d2":"markdown","cc47032f":"markdown","e13498fc":"markdown","ddb7a128":"markdown","af15b503":"markdown","79243d8b":"markdown","9614c357":"markdown","c3212c8a":"markdown","905d18a0":"markdown","a5b73bb4":"markdown","67ab421f":"markdown","6fdbfd53":"markdown","ea0e5b0a":"markdown","d24b9486":"markdown","ba66efc0":"markdown","212b6d84":"markdown","173755be":"markdown","63682ad5":"markdown","1a4051f9":"markdown","c24efafb":"markdown","52de6e41":"markdown","d93d7089":"markdown","bc158b3e":"markdown"},"source":{"fad1e6b3":"import collections\n\nimport numpy as np\nimport pandas as pd\n\n# Visualisation\nfrom pprint import pprint\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# display options\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n# sklearn\n# from sklearn.model_selection import StratifiedKFold, KFold, LeaveOneOut, cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve\nfrom sklearn import tree\n\n# Save model\nimport pickle\n","33552a10":"def diff(li1, li2):\n    '''\n    This function returns different elements between 2 lists\n    '''\n    return (list(set(li1) - set(li2)))\n\ndef plot_stats(df, feature, target_ftr, label_rotation=False, horizontal_layout=True):\n    '''\n    This function plot the categorical feature distribution according to target variable\n    '''\n    temp = df[feature].value_counts()\n    df1 = pd.DataFrame({feature: temp.index,'Number of repaid-loans': temp.values})\n\n    # Calculate the percentage of target=1 per category value\n    cat_perc = df[[feature, target_ftr]].groupby([feature],as_index=False).mean()\n    cat_perc.sort_values(by=target_ftr, ascending=False, inplace=True)\n    \n    sns.set_color_codes(\"pastel\")\n    s = sns.barplot(x = feature, y=\"Number of repaid-loans\",data=df1)\n    if(label_rotation):\n        s.set_xticklabels(s.get_xticklabels(),rotation=60)\n\n    plt.tick_params(axis='both', which='major', labelsize=10)\n\n    plt.show();\n\n\ndef get_rocauc(model, xTest, yTest): \n    '''\n    This function produces the Area under the curve for the model. \n    The 'auto' method calculates this metric by using the roc_auc_score function from sklearn.\n    Range: 0 to 1 (0 being the worst predictive model, 0.5 being the random and 1 being the best)\n    '''\n    predictions = model.predict_proba(xTest)[:, 1]\n    roc_auc = roc_auc_score(yTest, predictions)\n    print('Model Performance:')\n    print('--'*5)\n    print('--'*5)\n    print('ROC = {:0.2f}%'.format(roc_auc))\n    \n    return roc_auc\n\ndef plot_roc(yTest, yPred):\n    '''\n    This function plots the ROC and gives the AUC.\n    Range for Area under the curve : 0 to 1 (0 being the worst and 1 being best for predictive model)\n    '''\n    fpr, tpr, thresholds = roc_curve(yTest, yPred)\n    roc_auc = roc_auc_score(yTest, yPred)\n    plt.figure(figsize=(10,10))\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--', label='random')\n    plt.plot([0,0,1,1],[0,1,1,1],'g-',label='perfect')\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\ndef perf_measure(y_actual, y_pred):\n    '''\n    Function for calculating TP, FP, TN and FN\n    '''\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_pred)): \n        if y_actual[i]==y_pred[i]==1:\n            TP += 1\n        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n            FP += 1\n        if y_actual[i]==y_pred[i]==0:\n            TN += 1\n        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n            FN += 1\n\n    return(TP, FP, TN, FN)\n\ndef capture_curve(test_df, y_test, preds, roc, title):\n    '''\n    Function for to plot capture curve for risky or non-repaid loans\n    This is similar to gain and lift chart in statistics.\n    x-axis: Population % of granted loans\n    y-axis: Risk \/ Non-repaid loan %\n    '''\n    fpr, tpr, threshold = roc_curve(y_test, preds)\n    roc_auc = auc(fpr, tpr)\n    rate = []\n    for i in threshold:\n        T = perf_measure(list(y_test),[1 if j >= i else 0 for j in preds])\n        rate.append(T[0]+T[1])\n    rate2 = [i\/len(test_df) for i in rate]\n    plt.figure(figsize=[12,12])\n    plt.plot(rate2, tpr, label='ROC_AUC {}'.format(roc) % roc_auc, linewidth=4)\n    plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n    plt.xlim([-0.05, 1.0])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('Granted Loans', fontsize=18)\n    plt.ylabel('Captured out of total non-repaid proportion', fontsize=18)\n    plt.title('Capture plot for {}'.format(title), fontsize=18)\n    plt.legend(loc=\"lower right\",prop={'size':30})\n    plt.show()\n\ndef decile_cutoff_risk_detected(df_pred):\n    '''\n    Function to find the decile-wise risky loan application %\n    '''\n    pop_perc = list(np.arange(0.1,1.1,0.1))\n    perc_list = []\n    risk_perc_list = []\n    risk_num_list = []\n    avg_pred_prob_list = []\n    min_pred_prob_list = []\n    max_pred_prob_list = []\n    df_pop_risk = pd.DataFrame(columns=['loan_percentage','non-repaid_percentage', '#non-repaid_loans', 'avg_pred_prob'])\n    total_risk_count = df_pred[df_pred['loan_quality']==1]['loan_id'].count()\n    start = 0\n    for perc in pop_perc:\n        split_len = int(perc*len(df_pred))\n        sorted_results_final = df_pred.iloc[start:split_len]\n        risk_count = sorted_results_final[sorted_results_final['loan_quality']==1]['loan_id'].count()\n        min_pred_prob = sorted_results_final['preds'].min()\n        max_pred_prob = sorted_results_final['preds'].max()\n        avg_pred_prob = sorted_results_final['preds'].mean()\n        perc_list.append(int(perc*100))\n        risk_perc_list.append(round(((risk_count\/total_risk_count)*100),2))\n        risk_num_list.append(round(risk_count,2))\n        min_pred_prob_list.append(round(min_pred_prob, 2))\n        max_pred_prob_list.append(round(max_pred_prob, 2))\n        avg_pred_prob_list.append(round(avg_pred_prob, 2))\n        start = split_len\n    df_pop_risk['loan_percentage'] = perc_list\n    df_pop_risk['non-repaid_percentage'] = risk_perc_list  \n    df_pop_risk['#non-repaid_loans'] = risk_num_list\n    df_pop_risk['min_pred_prob'] = min_pred_prob_list\n    df_pop_risk['max_pred_prob'] = max_pred_prob_list\n    df_pop_risk['avg_pred_prob'] = avg_pred_prob_list\n    return df_pop_risk\n\ndef cum_decile_cutoff_risk_detected(df_pred):\n    '''\n    Function to find the decile-wise risky loan application %\n    '''\n    pop_perc = list(np.arange(0.1,1.1,0.1))\n    perc_list = []\n    risk_perc_list = []\n    risk_num_list = []\n    avg_pred_prob_list = []\n    min_pred_prob_list = []\n    max_pred_prob_list = []\n    df_pop_risk = pd.DataFrame(columns=['loan_percentage','non-repaid_percentage', '#non-repaid_loans', 'avg_pred_prob'])\n    total_risk_count = df_pred[df_pred['loan_quality']==1]['loan_id'].count()\n    for perc in pop_perc:\n        split_len = int(perc*len(df_pred))\n        sorted_results_final = df_pred.iloc[:split_len]\n        risk_count = sorted_results_final[sorted_results_final['loan_quality']==1]['loan_id'].count()\n        min_pred_prob = sorted_results_final['preds'].min()\n        max_pred_prob = sorted_results_final['preds'].max()\n        avg_pred_prob = sorted_results_final['preds'].mean()\n        perc_list.append(int(perc*100))\n        risk_perc_list.append(round(((risk_count\/total_risk_count)*100),2))\n        risk_num_list.append(round(risk_count,2))\n        min_pred_prob_list.append(round(min_pred_prob, 2))\n        max_pred_prob_list.append(round(max_pred_prob, 2))\n        avg_pred_prob_list.append(round(avg_pred_prob, 2))\n    df_pop_risk['loan_percentage'] = perc_list\n    df_pop_risk['non-repaid_percentage'] = risk_perc_list  \n    df_pop_risk['#non-repaid_loans'] = risk_num_list\n    df_pop_risk['min_pred_prob'] = min_pred_prob_list\n    df_pop_risk['max_pred_prob'] = max_pred_prob_list\n    df_pop_risk['avg_pred_prob'] = avg_pred_prob_list\n    return df_pop_risk\n\ndef profitability_diff(score1_name, score2_name, score1, score2):\n    '''\n    This function takes 2 scores as input and gives Absolute Difference between scores as output\n    '''\n    if score1 > score2:\n        print('%s is %d point better than %s' % (score1_name, abs(score1 - score2), score2_name))\n        print('--'*30)\n        print('%s is %f times better than %s' % (score1_name, (score1\/score2), score2_name))\n    else:\n        print('%s is %d point better than %s' % (score2_name, abs(score1 - score2), score1_name))\n        print('--'*30)\n        print('%s is %f times better than %s' % (score2_name, (score2\/score1), score1_name))\n","076c8ac0":"# Directory Path\ndir_path = '..\/input\/bank-loan-data\/'","230751a5":"borrower = pd.read_csv(dir_path + 'borrower_table.csv')\nprint(borrower.shape)\nborrower.head(3)","e4f17422":"loan = pd.read_csv(dir_path + 'loan_table.csv')\nprint(loan.shape)\nloan.head(3)","e7deea00":"print(f'There are {loan.shape[0] - loan.loan_id.nunique()} duplicates for loan id in the loan table')\nprint(f'There are {borrower.shape[0] - borrower.loan_id.nunique()} duplicates for loan id in the borrower table')","358b4a56":"borrower_loan_id = list(borrower['loan_id'])\nloan_loan_id = list(loan['loan_id'])\n\nif collections.Counter(borrower_loan_id) == collections.Counter(loan_loan_id): \n    print (\"All the loan ids are same\") \nelse : \n    print (\"Different loan ids are present in both the datasets\") ","a34a9842":"print(diff(borrower_loan_id, loan_loan_id))","9a0450c5":"# Merge Dataframes\n\ndf_loan = loan.merge(borrower, how='left', on='loan_id')\nprint(df_loan.shape)\ndf_loan.head(3)","f8fd94e4":"print(df_loan.info())","111fa318":"# Numerical features\n\ndf_loan.describe().transpose()","ff0cdd7a":"# Calculate missing value count and percentage\n\nmissing_value_df_loan = pd.DataFrame(index = df_loan.keys(), data =df_loan.isnull().sum(), columns = ['Missing_Value_Count'])\nmissing_value_df_loan['Missing_Value_Percentage'] = np.round(((df_loan.isnull().mean())*100),2)\nmissing_value_df_loan.sort_values('Missing_Value_Count',ascending= False)","6289f991":"# Make correlation table according to spearman's correlation\n\ncorr_spearman = df_loan.corr()\ncorr_spearman","9f7f0ce3":"# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr_spearman, dtype=np.bool))\n\n# Visualisation of heatmap matrix\nfig, ax = plt.subplots(figsize=(10,10))\nax = sns.heatmap(corr_spearman, mask=mask, annot = True)","af815d37":"# granted & ungranted\n\ngranted   = df_loan.loc[(df_loan['loan_granted'] == 1)]\nungranted = df_loan.loc[(df_loan['loan_granted'] == 0)]","41e44639":"# repaid & non-repaid\n\nrepaid     = granted.loc[(granted['loan_repaid'] == 1)]\nnon_repaid = granted.loc[(granted['loan_repaid'] == 0)]","53bc0b47":"# Class Imbalance check\n\ntemp_ln_rpd = granted['loan_repaid'].value_counts()\ndf_ln_rpd = pd.DataFrame({'labels': temp_ln_rpd.index,\n                   'values': temp_ln_rpd.values})\nplt.figure(figsize = (6,6))\nplt.title('Application loans repaid')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'labels', y=\"values\", data=df_ln_rpd)\nlocs, labels = plt.xticks()\nplt.show()","512ddcf9":"plot_stats(granted, 'dependent_number', 'loan_repaid', label_rotation=False, horizontal_layout=True)","8b4b3f71":"plot_stats(granted, 'loan_purpose', 'loan_repaid', label_rotation=True, horizontal_layout=True)","50615e1a":"plot_stats(granted, 'is_employed', 'loan_repaid', label_rotation=False, horizontal_layout=True)","a9f902d8":"# How many of employed repay their loans\nprint('Employed who repay their loans (%) :')\nprint(round(((granted[(granted['is_employed']==1) & (granted['loan_repaid']==1)].shape[0])\/granted[(granted['is_employed']==1)].shape[0])*100,1))\n\nprint('--'*20)\nprint('--'*20)\n\n# How many of un-employed repay their loans\nprint('Unemployed who repay their loans (%) :')\nprint(round(((granted[(granted['is_employed']==0) & (granted['loan_repaid']==1)].shape[0])\/granted[(granted['is_employed']==0)].shape[0])*100,1))\n","9e10c5a2":"plot_stats(granted, 'currently_repaying_other_loans', 'loan_repaid', label_rotation=False, horizontal_layout=True)","e5fa1370":"# How many repay the loan while paying for another loan\nprint('People who repay their loans having existing loan payment (%) :')\nprint(round(((granted[(granted['currently_repaying_other_loans']==1) & (granted['loan_repaid']==1)].shape[0])\/granted[(granted['currently_repaying_other_loans']==1)].shape[0])*100,2))\n\nprint('--'*40)\nprint('--'*40)\n\n# How many repay the loan when not paying for other loans\nprint('People who repay their loans not having any other loan payment (%) :')\nprint(round(((granted[(granted['currently_repaying_other_loans']==0) & (granted['loan_repaid']==1)].shape[0])\/granted[(granted['currently_repaying_other_loans']==0)].shape[0])*100,2))\n","cc90d0da":"plot_stats(granted, 'fully_repaid_previous_loans', 'loan_repaid', label_rotation=False, horizontal_layout=True)","d7d512a5":"# How many of people repay their loans who have fully repaid previous loans\nprint('People who repay their loans who have fully repaid previous loans (%) :')\nprint(round(((granted[(granted['fully_repaid_previous_loans']==1) & (granted['loan_repaid']==1)].shape[0])\/granted[(granted['fully_repaid_previous_loans']==1)].shape[0])*100,2))\n\nprint('--'*40)\nprint('--'*40)\n\n# How many of people repay their loans who haven't fully repaid previous loans\nprint('People who repay their loans who have not fully repaid previous loans (%) :')\nprint(round(((granted[(granted['fully_repaid_previous_loans']==0) & (granted['loan_repaid']==1)].shape[0])\/granted[(granted['fully_repaid_previous_loans']==0)].shape[0])*100,2))\n","589be445":"fig, axes = plt.subplots(1, 3)\n\nfig.set_size_inches(12, 4)\n\nrepaid.hist('saving_amount', bins=100, ax=axes[0])\naxes[0].set_xlabel('repaid')\nnon_repaid.hist('saving_amount', bins=100, ax=axes[1])\naxes[1].set_xlabel('non_repaid')\nungranted.hist('saving_amount', bins=100, ax=axes[2])\naxes[2].set_xlabel('ungranted')\n\nplt.show()","73e29819":"print(f'Average saving_amount by repaid group: {repaid[\"saving_amount\"].median()}')\nprint(f'Average saving_amount by non-repaid group: {non_repaid[\"saving_amount\"].median()}')\nprint(f'Average saving_amount by ungranted group: {ungranted[\"saving_amount\"].median()}')","695a7206":"fig, axes = plt.subplots(1, 3)\n\nfig.set_size_inches(12, 4)\n\nrepaid.hist('checking_amount', bins=100, ax=axes[0])\naxes[0].set_xlabel('repaid')\nnon_repaid.hist('checking_amount', bins=100, ax=axes[1])\naxes[1].set_xlabel('non_repaid')\nungranted.hist('checking_amount', bins=100, ax=axes[2])\naxes[2].set_xlabel('ungranted')\n\nplt.show()","20243732":"print(f'Average checking_amount by repaid group: {repaid[\"checking_amount\"].median()}')\nprint(f'Average checking_amount by non-repaid group: {non_repaid[\"checking_amount\"].median()}')\nprint(f'Average checking_amount by ungranted group: {ungranted[\"checking_amount\"].median()}')","7302e8cb":"fig, axes = plt.subplots(1, 3)\n\nfig.set_size_inches(12, 4)\n\nrepaid.hist('yearly_salary', bins=100, ax=axes[0])\naxes[0].set_xlabel('repaid')\nnon_repaid.hist('yearly_salary', bins=100, ax=axes[1])\naxes[1].set_xlabel('non_repaid')\nungranted.hist('yearly_salary', bins=100, ax=axes[2])\naxes[2].set_xlabel('ungranted')\n\nplt.show()","12365634":"print(f'Average yearly_salary by repaid group: {repaid[\"yearly_salary\"].median()}')\nprint(f'Average yearly_salary by non-repaid group: {non_repaid[\"yearly_salary\"].median()}')\nprint(f'Average yearly_salary by ungranted group: {ungranted[\"yearly_salary\"].median()}')","8e03d3b2":"fig, axes = plt.subplots(1, 3)\n\nfig.set_size_inches(12, 4)\n\nrepaid.hist('total_credit_card_limit', bins=100, ax=axes[0])\naxes[0].set_xlabel('repaid')\nnon_repaid.hist('total_credit_card_limit', bins=100, ax=axes[1])\naxes[1].set_xlabel('non_repaid')\nungranted.hist('total_credit_card_limit', bins=100, ax=axes[2])\naxes[2].set_xlabel('ungranted')\n\nplt.show()","cd174b16":"print(f'Average total_credit_card_limit by repaid group: {repaid[\"total_credit_card_limit\"].median()}')\nprint(f'Average total_credit_card_limit by non-repaid group: {non_repaid[\"total_credit_card_limit\"].median()}')\nprint(f'Average total_credit_card_limit by ungranted group: {ungranted[\"total_credit_card_limit\"].median()}')","e1fc1513":"fig, axes = plt.subplots(1, 3)\n\nfig.set_size_inches(12, 4)\n\nrepaid.hist('avg_percentage_credit_card_limit_used_last_year', bins=100, ax=axes[0])\naxes[0].set_xlabel('repaid')\nnon_repaid.hist('avg_percentage_credit_card_limit_used_last_year', bins=100, ax=axes[1])\naxes[1].set_xlabel('non_repaid')\nungranted.hist('avg_percentage_credit_card_limit_used_last_year', bins=100, ax=axes[2])\naxes[2].set_xlabel('ungranted')\n\nplt.show()","382b3557":"print(repaid['avg_percentage_credit_card_limit_used_last_year'].median())\nprint(non_repaid['avg_percentage_credit_card_limit_used_last_year'].median())\nprint(ungranted['avg_percentage_credit_card_limit_used_last_year'].median())","4abd4b23":"print(df_loan['loan_granted'].unique())\nprint(df_loan['loan_repaid'].unique())","403cd5a3":"print(df_loan.loc[df_loan.loan_granted ==0 ]['loan_repaid'].unique())\nprint(df_loan.loc[df_loan.loan_granted ==1 ]['loan_repaid'].unique())","f8016bec":"df_loan.loc[((df_loan['loan_granted'] == 1) & (df_loan['loan_repaid'] == 1)), 'loan_quality'] = 0\ndf_loan.loc[((df_loan['loan_granted'] == 1) & (df_loan['loan_repaid'] == 0)), 'loan_quality'] = 1\ndf_loan.loc[(df_loan['loan_granted'] == 0), 'loan_quality'] = -1","083df27d":"print(df_loan['loan_quality'].unique())\nprint('--'*10)\nprint(df_loan.loc[(df_loan['loan_granted']==1)&(df_loan['loan_repaid']==1)]['loan_quality'].unique())\nprint(df_loan.loc[(df_loan['loan_granted']==1)&(df_loan['loan_repaid']==0)]['loan_quality'].unique())\nprint(df_loan.loc[(df_loan['loan_granted']==0)]['loan_quality'].unique())","1a27c52e":"# Create a copy of the loan dataset for feature transformation\n\ndf_loan_tr = df_loan.copy()","17fa7790":"# Cast the datatype of \"date\" field\ndf_loan_tr['date']= pd.to_datetime(df_loan_tr['date'])\ndf_loan_tr['date'].dtypes","2c5c3e0f":"# Year from date\ndf_loan_tr['year'] = df_loan_tr['date'].dt.year\n\n# Month from date\ndf_loan_tr['month'] = df_loan_tr['date'].dt.month\n\n# Day from date\ndf_loan_tr['day'] = df_loan_tr['date'].dt.day\n\n# Quarter from date\ndf_loan_tr['quarter'] = df_loan_tr['date'].dt.quarter\n\n# Semester from date\ndf_loan_tr['semester'] = np.where(df_loan_tr.quarter.isin([1,2]),1,2)\n\n# Day of the week from date\ndf_loan_tr['dayofweek'] = df_loan_tr['date'].dt.dayofweek\n\ndf_loan_tr[['date', 'year', 'month', 'day', 'quarter', 'semester', 'dayofweek']].head()","9c0c9141":"# Check the unique day of week\ndf_loan_tr['dayofweek'].unique()","845ebbd0":"# Unique values in loan_purpose\n\ndf_loan_tr[\"loan_purpose\"].unique()","53c2439f":"# Need to convert the datatypes of the feature to 'category' before Label encoding.\ndf_loan_tr[\"loan_purpose\"] = df_loan_tr[\"loan_purpose\"].astype('category')\n\n# Label Encoding\ndf_loan_tr[\"loan_purpose_cat\"] = df_loan_tr[\"loan_purpose\"].cat.codes\n\ndf_loan_tr[[\"loan_purpose\", \"loan_purpose_cat\"]].head(3)","0dd0e94f":"# Calculate missing value count and percentage\n\nmissing_value_df_loan_tr = pd.DataFrame(index = df_loan_tr.keys(), data =df_loan_tr.isnull().sum(), columns = ['Missing_Value_Count'])\nmissing_value_df_loan_tr['Missing_Value_Percentage'] = np.round(((df_loan_tr.isnull().mean())*100),2)\nmissing_value_df_loan_tr.sort_values('Missing_Value_Count',ascending= False)","6093dc8e":"print(df_loan_tr['fully_repaid_previous_loans'].unique())\nprint(df_loan_tr.loc[df_loan_tr['fully_repaid_previous_loans'].isnull()]['is_first_loan'].unique())\n\n# Impute fully_repaid_previous_loans with some numerical values for all the first loans\n\n","942b9900":"print(df_loan_tr['currently_repaying_other_loans'].unique())\nprint(df_loan_tr.loc[df_loan_tr['currently_repaying_other_loans'].isnull()]['is_first_loan'].unique())\n\n# Impute currently_repaying_other_loans with some numerical values for all the first loans","106fdd51":"# print(df_loan_tr['avg_percentage_credit_card_limit_used_last_year'].unique())\nprint(df_loan_tr.loc[df_loan_tr['avg_percentage_credit_card_limit_used_last_year'].isnull()]['total_credit_card_limit'].unique())\nprint(df_loan_tr.loc[df_loan_tr['total_credit_card_limit']==0]['avg_percentage_credit_card_limit_used_last_year'].unique())","a5e1d5d8":"# replace null with -1\n\ndf_loan_tr['fully_repaid_previous_loans'].fillna(-1, inplace=True)\ndf_loan_tr['currently_repaying_other_loans'].fillna(-1, inplace=True)\ndf_loan_tr['avg_percentage_credit_card_limit_used_last_year'].fillna(-1, inplace=True)\n","da3bbf54":"feature_set = ['is_first_loan', \n'fully_repaid_previous_loans', \n'currently_repaying_other_loans', \n'total_credit_card_limit', \n'avg_percentage_credit_card_limit_used_last_year', \n'saving_amount', \n'checking_amount', \n'is_employed', \n'yearly_salary',\n'age', \n'dependent_number',\n# 'month', \n# 'day', \n# 'quarter', \n# 'semester', \n# 'dayofweek', \n'loan_purpose_cat']","d7ea2b07":"## Train and test on granted loans\n\ngranted_loan = df_loan_tr.loc[(df_loan_tr['loan_quality'] == 1) | (df_loan_tr['loan_quality'] == 0)]\nprint(f'#loan_id: {granted_loan.shape[0]} ')","6d2c3af7":"print('cross check the target values')\nprint('--'*15)\nprint(granted_loan['loan_quality'].unique())\nprint(granted_loan['loan_granted'].unique())\nprint(granted_loan['loan_repaid'].unique())","c23894ba":"## Predict \/ Score Population on ungranted loans\n\nungranted_loan = df_loan_tr.loc[(df_loan_tr['loan_quality'] == -1)]\nprint(f'#loan_id: {ungranted_loan.shape[0]} ')","f58f9fd0":"print('cross check the target values')\nprint('--'*15)\nprint(ungranted_loan['loan_quality'].unique())\nprint(ungranted_loan['loan_granted'].unique())\nprint(ungranted_loan['loan_repaid'].unique())","df8885e7":"train_granted = granted_loan.loc[~(granted_loan['month'].isin([1,2,3]))]\nexog_train_granted = train_granted[feature_set]\nendog_train_granted = train_granted['loan_quality']\n\nprint(train_granted.shape)","23b38dd7":"# Insample train and validation split\n\nx_train, x_val, y_train, y_val = train_test_split(exog_train_granted, endog_train_granted, test_size=0.1, random_state=42)\n","c4d304be":"test_granted = granted_loan.loc[(granted_loan['month'].isin([1,2,3]))]\nexog_test = test_granted[feature_set]\n\nprint(test_granted.shape)","11bbf8c1":"test_granted['loan_quality'].unique()","dfcb4a5b":"# Random Forest Model\n\nrf = RandomForestClassifier(random_state = 42)","f7ffc29a":"# Parameters used by the current forest\n\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","aa22c41a":"base_model = RandomForestClassifier(n_estimators = 100, max_depth=5, random_state = 42)\nbase_model.fit(x_train, y_train)\nbase_accuracy = get_rocauc(base_model, x_val, y_val)","217a4695":"# Compute cross-validated AUC scores for the training set: cv_auc\n\ncv_auc = cross_val_score(base_model, x_train, y_train, cv=5, scoring = 'roc_auc')\n\n# Print list of AUC scores\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))","37c359b8":"features = exog_train_granted.columns\nimportances = base_model.feature_importances_\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='blue', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","2f8039c5":"# save the model to disk\nfilename = 'randomforest_basemodel'\npickle.dump(base_model, open(filename, 'wb'))","68c590cb":"# # Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)]\n\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(1, 50, num = 15)]\n# max_depth.append(None)\n\n# # Minimum number of samples required to split a node\n# min_samples_split = [2, 5, 10]\n\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 4]\n\n# # Method of selecting samples for training each tree\n# bootstrap = [True, False]\n","492f4f01":"# # Create the random grid\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n# pprint(random_grid)","fcb55a85":"# rf1 = RandomForestClassifier()\n\n# # Random search of parameters, using 3 fold cross validation\n# # Search across 100 different combinations, and by using all available cores\n# rf_random = RandomizedSearchCV(estimator = rf1, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# # Fit the random search model\n# rf_random.fit(x_train, y_train)\n","e83e5cc4":"granted_model = pickle.load(open('randomforest_basemodel', 'rb'))\npred_test_granted = granted_model.predict_proba(exog_test)[:, 1]\n\nprint(pred_test_granted)","270a7d39":"roc_test_granted = roc_auc_score(list(test_granted['loan_quality']), pred_test_granted)\nprint(round(roc_test_granted,2))","fc08509b":"plot_roc(test_granted['loan_quality'], pred_test_granted)","4b3a2854":"capture_curve(test_granted, test_granted['loan_quality'], pred_test_granted, round(roc_test_granted,2), 'Granted Loans in 2012 Q1')","c3f9b1eb":"# Create a copy of test data\ntest_granted_preds=test_granted.copy()","2a829cbc":"test_granted_preds['preds']=list(pred_test_granted)","0f27b94e":"sorted_test_granted_preds = test_granted_preds.sort_values(by ='preds' , ascending=False)\nsorted_test_granted_preds.shape","901ce107":"test_granted_preds.shape","16d1b876":"df_decile_cutoff_test_granted = decile_cutoff_risk_detected(sorted_test_granted_preds)\ndf_decile_cutoff_test_granted","768bf908":"df_cum_decile_cutoff_test_granted = cum_decile_cutoff_risk_detected(sorted_test_granted_preds)\ndf_cum_decile_cutoff_test_granted","a512def7":"# independent variables of ungranted loans\nexog_ungranted = ungranted_loan[feature_set]","77330cc6":"# predictions for ungranted loans\npred_ungranted = granted_model.predict_proba(exog_ungranted)[:, 1]\nprint(pred_ungranted)","54272863":"# create copy of ungranted loans\nungranted_preds=ungranted_loan.copy()\n\n# Attaching predictions with ungranted dataframes\nungranted_preds['preds']=list(pred_ungranted)","6391cddd":"# cross checking shape of both the dataframes\n\nprint(ungranted_loan.shape)\nprint(ungranted_preds.shape)","e7a39a6e":"ungranted_lt_1_risk = ungranted_preds.loc[ungranted_preds['preds'] < 0.15]\nprint(ungranted_lt_1_risk.shape)\nungranted_lt_1_risk.head()","ebf5e0a4":"ungranted_lt_10_risk = ungranted_preds.loc[ungranted_preds['preds'] < 0.4]\nprint(ungranted_lt_10_risk.shape)\nungranted_lt_10_risk.head()","cf71544d":"# Export loan_id with < 1% risk\nungranted_lt_1_risk['loan_id'].to_csv('loan_preds_lt_1_risk.csv')\n\n# Export loan_id with < 10% risk\nungranted_lt_10_risk['loan_id'].to_csv('loan_preds_lt_10_risk.csv')","a8316a4f":"granted_2012Q1 = granted_loan.loc[granted_loan['month'].isin([1,2,3])]\nprint('Maximum Possible Score:')\nprint('--'*12)\nprint((granted_2012Q1.shape[0]) + (ungranted_loan.shape[0]))","9bb0d30c":"# Bank Profitability Calculation\n\nbank_score = (granted_2012Q1.loc[(granted_2012Q1['loan_repaid']==1)].shape[0]) - (granted_2012Q1.loc[(granted_2012Q1['loan_repaid']==0)].shape[0])\n\nprint('Bank score:')\nprint('--'*12)\nprint(bank_score)\n\n# print((granted_2012Q1.loc[(granted_2012Q1['loan_repaid']==1)].shape[0]))\n# print((granted_2012Q1.loc[(granted_2012Q1['loan_repaid']==0)].shape[0]))\n","7801f2fb":"# ML Model Profitability Calculation with < 1% risk\n\ngranted_2012Q1_lt_1_risk      = test_granted_preds[test_granted_preds['preds'] < 0.15]\nloss_granted_2012Q1_lt_1_risk = granted_2012Q1_lt_1_risk[granted_2012Q1_lt_1_risk['loan_repaid']==0]\nml_model_score = ((granted_2012Q1_lt_1_risk.shape[0]) - (loss_granted_2012Q1_lt_1_risk.shape[0])) + (ungranted_lt_1_risk.shape[0] - int((ungranted_lt_1_risk.shape[0])\/100))\n\nprint('ML Model score:')\nprint('--'*10)\nprint(ml_model_score)","b813a8d7":"profitability_diff('bank_score', 'ml_model_score', bank_score, ml_model_score)","b74d5a1b":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import Data](#1)  \n1. [Data Quality Check](#2)\n1. [Exploratory Data Analysis](#3)\n1. [Model Framework Design](#4)\n1. [Data Preprocessing](#5)\n1. [Model Build](#6)\n1. [Optimization](#7)\n1. [Model Evaluation](#8)\n1. [Score - Ungranted Loans](#9)\n1. [Model Profitability Compariso](#10)","7b9a36a8":"#### Libraries","e5ceb3e9":"## 3. Exploratory Data Analysis <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","5ffc71af":"##### 8.3.1.1 Decile-wise statistics","4b48bd53":"** Optimisation is saved for future**","bb7bfec2":"### 7.1 Hyperparameter Optimization","075a9133":"### 2.3 Datatype","117fd8dd":"#### 7.1.1 Define Hyperparameters for Chosing a Better Model","16bf6dc0":"## 6. Model Build <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","abecda9e":"### 6.3 Feature importance","0eda3e43":"### 8.2 Capture Plot","03fa2768":"### 5.4 Train & Test Data Preparation","13053b38":"#### 5.4.2 Test Data Preparation","f38e6206":"### 6.4 Save the model into a pickle file","1fc283a7":"### 8.1 ROC","c2324b28":"### 5.3 Feature Selection","0dd91624":"* Granted Loans","5c8d6cc0":"#### 7.1.2 Random Hyperparameter Grid","bc66e2f5":"**The above is a capture plot for all the granted loans which are not going to repay**\n\n* This shows the risk factors with the model developed.    \n   All though the model is having 0.97 ROC_AUC, but not efficient of capturing the loans which are risky (not going to pay).   \n\n\n* **Note:** ROC is here is just indicator, this is not a ROC curve.","fddae246":"Most important variables\n\n* Below are the top 7 variables from the model (feature importance in descending order, 1 is most imp).  \n\n\n     1. saving_amount\n     2. checking_amount\n     3. yearly_salary\n     4. total_credit_card_limit: <5000 \n     5. currently_repaying_other_loans\n     6. is_employed\n     7. avg_percentage_credit_card_limit_used_last_year\n     \n     (Visualisations can be referred for detailed and intuitive observation)  \n      \n     1. saving_amount: People with < 2000 saving_amount are mostly risky\n     2. checking_amount: People with < 4000 checking_amount seems to be risky\n     3. yearly_salary: < 35000 yearly salary can be an important indicator for risk\n     4. total_credit_card_limit: < 5000 credit card limit can be risky for loan approval\n     5. currently_repaying_other_loans: 80% people, not having any other loans are repaying the loan while less 30% repay their loan having existing loan payment\n     6. is_employed: 70% of the employed people repay their loans, where less than 20% unemployed repay their loans \n     7. avg_percentage_credit_card_limit_used_last_year: Less balance used is less riskier. cut-off boundary is not that intuitive.    \n     \n          \n** `is_employed` is one of the important factor while repaying loans. 70% employed repay their loans, where it is < 20% in case of an unemployed. **  \n     \n** 30% people with existing loan payment repays, while repayment is 80% when no existing loans. **  \n     \n** While all the above observations are quite intuitive, we may not follow any strict rule based approach for maximum output.**   \n** A Machine Learning approach makes use of the combination of features and finds the best pattern to minimise the risk and we gain most out of it while taking decision for approving a loan **\n     \n","cb525657":"### 10.3 Bank Score","097ef07e":"* Class imbalance is not significant","84fd064b":"## 1. Import Data <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","346dfd1b":"### 2.2 ***loan_id*** comparison","122ee3f4":"### 1.2 Borrower Details","a70c448e":"### 1.3 Loan Details","1a87be1e":"#### total_credit_card_limit","ceaa1c68":"* fully_repaid_previous_loans","ed6772b7":"### 6.2 Cross - Validation","c6c8c235":"### 10.6 Profitablity Difference - Bank Model vs ML Model","bab1568c":"#### yearly_salary","39527b38":"#### 8.1.1 ROC AUC","a95b8754":"*********************************************\n*********************************************\n*********************************************","662facfb":"* All the loan data is in weekdays (0 to 4, i.e. Monday to Friday)","3931c52b":"#### 9.1.2 For < 10% risk, consider loan-id s with predicted probability of  < 0.4","b456977f":"##### 8.3.1.2 Cumulative decile-wise statistics","1fb753e4":"#### Create the target: loan_quality","81b339df":"#### Population crop for loan applications can be granted according to **risk-appetite**","81493124":"* 3 variation of feature set\n\n  **1. All the features (including newly created and transformed)**   \n  ROC around 0.97 and good predicted probability range\n  \n  **2. All without `checking_amount` **   \n  ROC around 0.94.   \n  Although checking_amount looks multicollinear, but gives better accuracy when used and capture non-repaid loans more accurately\n  \n  **3. All without `newly created timestamp variables from date` **   \n  ROC around 0.97   \n  Best among all. better predicted probability range than the model using all the features   \n  \n    ***Option 3 is used while building the final model***","4e8a62be":"* Cross check with set difference option","b8f781fa":"* **loan_repaid** can be null because of non-grant of loan itself.\n* **fully_repaid_previous_loans, currently_repaying_other_loans** can also be null in case of first loan.\n* **avg_percentage_credit_card_limit_used_last_year** needs to be analysed further before any treatment (imputation).","bac4b7b3":"* Create different datasets for granted & ungranted and repaid & non-repaid segments","2d86ff85":"* Target is created according to a risk framework for model building.\n   1. If loan is granted and not paid -> risk, mark it 1\n   2. If loan is granted and paid -> risk, mark it 0\n   \n   3. for all the ungranted loans, -1 is just an indicator. Mathematical calculation will be different for business situation","08090da7":"#### Others (For future scope)\n\n  1. time period for which data is available\n  2. How on each quarter, half-yearly, yearly loan trends\n  3. Outlier Analysis","0769845b":"### 10.1 Loan Population for Profitability Comparison","ddecb14d":"##### Gradient Boosting can be tried as next step for optimisation","54aa4149":"#### 8.1.2 ROC Curve","a8af3df8":"### 3.1 Correlation","08f31c91":"* Random Forest base version","ec9f676b":"* Predictions","7fb026b5":"#### User Defined Functions","2460035d":"### 3.2 Feature Analysis","5df5354d":"#### 3.2.1 Target class Distribution","7f8cf21e":"## 5. Data Preprocessing <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","df347f8e":"* avg_percentage_credit_card_limit_used_last_year is null when total_credit_card_limit is 0","4aa75d10":"### 5.2 Null Value Treatment: Imputation","18d271da":"#### 9.1.1 For < 1% risk, consider loan-id s with predicted probability of  < 0.15","fe4758dc":"### 1.1 Source Directory Path","2eeace6d":"#### 8.3.1 Statistics Decile-wise","b1170643":"### 10.2 Possible Maximum score ","f076b5fb":"### 6.1 Base Model training and accuracy","b880fff1":"### 4.2 Features from sub-models\n\n* Build 2 sub-models taking loan_granted & loan_repaid as targets  \n* Use the scores for each loan_ids as 2 independent features in the final models\n\n*** These 2 features are not used for now, due to time constraint, may be followed in future***\n\n","aa8f4294":"### 10.5 Assumptions ","50a0d469":"## Important Findings","07b9c1df":"* avg_percentage_credit_card_limit_used_last_year","3e03860d":"#### 5.1.1 New features from date variables","8c2a8193":"## 2. Data Quality Check <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","8a3d4716":"* currently_repaying_other_loans","d98526db":"### 2.1 Duplicate Check","bcb03279":"* xgboost and lightgbm can be used to check accuracy","e1ae5904":"## 10. Model Profitability Comparison <a class=\"anchor\" id=\"10\"><\/a>\n\n[Back to Table of Contents](#0.1)","dfe8f3c6":"1. ***Maximum Possible Score:***   \nCalculated assuming each and every loan applications will be granted and repaid.     \nThis might not be the best score, as there are some bad applications in real-time.    \n\n\n2. ***Bank Profitability Score:***  \nBank gains from all the repaid loans, but lose from all the bad loans.  \nNo gain from ungranted loans\n\n\n3. ***ML Model Profitability Score:***   \nML Model scores from both the crops. Granted and Ungranted.   \nGains when a loan is repaid and lose when it is not repaid.    \nWhile calculation of loss from ungranted bank loans, maximum 1% of loss has been deducted from gains (as labels now are not present)\n","27e84e7c":"### 9.1 Filter out loans with 1% and 10% risk","1bea3652":"* As only 5 unique values are there, it can be label encoded.","217843fa":"![image.png](attachment:image.png)","fc38d626":"### 2.4 Statistics","f9a1195e":"* ROC from all the samples are around 0.96. Variation is negligible. We may move forward with this base model.","51922426":"##### Simulation 1:\n\n**Risk < 1%** or **loan will be repaid with 99% success confidence** , go for loan_id s with **avg_pred_prob < 0.15**   \n\n\n##### Simulation 2:\n\n**Risk < 10%** or **loan will be repaid with 90% success confidence** , go for loan_id s with **avg_pred_prob < 0.4**","1aeb9461":"* saving_amount & checking_amount distribution looks similar","a7302864":"### 2.5 Missing Value Analysis","e45c679e":"#### Check the values of loan_granted and loan_repaid","3dff241a":"## 7. Optimization <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","6db35b32":"### 9.2 Export predicted loan_id to csv file","f5c84424":"### 8.4 Simulation for selecting loan population by setting the threshold","d97be3f2":"#### 6.2.2 Accuracy comparison - Different feature sets","7006e065":"* Categoricl Feature Distribution (repaid-loans)","493459dc":"## 4. Model Framework Design <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","734020d2":"###### Below is the way to interpret the above table  \n\n* **Cumulative Table**\n\n**non-repaid_percentage**\n* If we take top 10% of loans, approx 28% of risk is there.\n* For top 20% of loans, risk is approx 55%     \n* If we consider bottom 50% of the loans, less than 1% risk is there.   \n\n**#non-repaid_loans**\n* If we take top 10% of loans, 1172 risky loan applications are there.\n* For top 20% of loans, risky applications are around 2300.  \n\n**min_pred_prob**\n* This column shows the decile wise minimum predicted probability, which can be used for probability cut-off for decision","cc47032f":"* **Replace null with -1**","e13498fc":"### <font color='orange'>Please Upvote<\/font> if you find this kernel useful.","ddb7a128":"For fair and apple-to-apple comparison considering the below conditions  for score comparison\n\n   a) Granted Loans of 2012 Q1   \n   b) All the ungranted loans","af15b503":"#### checking_amount","79243d8b":"Compare bank profitability vs your model profitability  \n\n* Machine Learning Model's output is **4.7 times** more profitable than the bank's scoring method","9614c357":"## 9. Score - Ungranted Loans <a class=\"anchor\" id=\"9\"><\/a>\n\n[Back to Table of Contents](#0.1)","c3212c8a":"#### 5.4.1 Training and Validation Data Preparation","905d18a0":"### 10.4 ML Model Score","a5b73bb4":"#### 7.1.3 Random Search Training","67ab421f":"### 8.3 Decile-wise risky applications calculation","6fdbfd53":"### 4.1 Target Definition and Model framework preparation\n\n* loan_quality\n   1. 1:  (loan_granted==1) & (loan_repaid==1)\n   2. 0:  (loan_granted==1) & (loan_repaid==0)\n* Train the model on loan_quality = 0 or 1\n* Score on all the non-granted `loan_id`s\n* Create decile based on predicted-probability. Check till which decile it is safe to grant loan.    \nNeeds to decide the risk threshold (how much risk we want to take). \n* Based on the decided risk threshold and the decile, we choose the crop to gain more on ungranted loans.","ea0e5b0a":"* All the loan ids are same in both the dataframes\n* Now both the datasets can be merged and analyzed.","d24b9486":"#### saving_amount","ba66efc0":"#### 6.2.1 5-fold cross validation","212b6d84":"### 5.1 New feature Creation","173755be":"#### Granted and Ungranted Loans","63682ad5":"### Purpose of this Model:\n\nBuild a strategy to grant loans.     \nSpecifically, we should build a model which is better than the bank's loan grant model.","1a4051f9":"## 8. Model Evaluation <a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","c24efafb":"* Ungranted Loans","52de6e41":"#### 3.2.2 Uni-variate Analysis","d93d7089":"* Continuous Feature Distribution (repaid-loans)","bc158b3e":"#### 5.1.2 Categorical feature Encoding"}}