{"cell_type":{"01935222":"code","020207bc":"code","264f24da":"code","5e7ed75d":"code","6591473b":"code","c47fa20d":"code","52ce8b00":"code","f76e4b48":"code","5e1d9dfe":"code","015527e2":"code","8f19bfdd":"code","9173e4c2":"code","b8ec7819":"code","3712b763":"code","03a94d94":"code","903c2e3a":"code","28f91a77":"code","1889d3a0":"code","a6f7bdd6":"code","be3f279a":"code","26e1eb90":"code","d77f2637":"code","47463153":"code","66bf6c2e":"code","52804a87":"code","e43362ce":"code","dc3a80a1":"code","5874b7fa":"code","1988e822":"code","0d6dddc8":"code","164024f2":"code","3787cdc1":"code","a690c403":"code","5103a445":"code","0d3957de":"code","947194a6":"code","4f7c0a9e":"code","d81a7963":"code","f6b4bd6b":"code","50d4c2a1":"code","adc29665":"code","7c2d2e5b":"code","8c1b0783":"code","b22bb0fd":"code","331cfd01":"code","70f6336c":"code","c4cc0196":"code","1beb4c79":"code","57002baf":"code","440bc90f":"markdown","0a799100":"markdown","839d5a98":"markdown","da0a7b1a":"markdown","2b47146c":"markdown","45cf1c06":"markdown","607074fc":"markdown","cbc089ed":"markdown","47c00539":"markdown","c5cfb739":"markdown","493a7e1e":"markdown","81387601":"markdown","68f46c84":"markdown","da702ca6":"markdown"},"source":{"01935222":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import norm, skew, kurtosis\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","020207bc":"#Load the data \ndf_train=pd.read_csv('\/kaggle\/input\/house-prices-dataset\/train.csv')\ny_train = df_train.SalePrice.values\ndf_train.head()","264f24da":"df_test = pd.read_csv('\/kaggle\/input\/house-prices-dataset\/test.csv')\ndf_test.head()","5e7ed75d":"df_train.info()","6591473b":"df_test.info()","c47fa20d":"#Size of the data\ndf_train.shape, df_test.shape","52ce8b00":"df_train.columns.shape","f76e4b48":"df_test.columns.shape","5e1d9dfe":"#since we are determining houseprices, lets explore the sale price\ndf_train['SalePrice'].describe()","015527e2":"from plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode()\nimport cufflinks as cf\nimport plotly.offline as pyo\ncf.go_offline()\npyo.init_notebook_mode()\nprint(__version__)\ndf_train['SalePrice'].iplot(kind='histogram', xTitle='price',yTitle='frequency',colors='darkred')","8f19bfdd":"# Heatmap to show relationship between different variables\ncorr = df_train.corr()\nplt.style.use('classic')\nplt.subplots(figsize=(12,9))\nsns.heatmap(corr, annot=False, vmax=0.9, square=True)","9173e4c2":"ad=12\ncols=corr.nlargest(ad,'SalePrice')['SalePrice'].index  \ncm=np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.30)\nfig, ax = plt.subplots(figsize=(12, 9))\nfinal_plot = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},\n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","b8ec7819":"# Getting the skewness and kurtosis of the target variable\nskew(df_train['SalePrice']), kurtosis(df_train['SalePrice'])","3712b763":"plt.figure(figsize=(6,3))\nsns.distplot(df_train['SalePrice'])\n\nfig = plt.figure(figsize=(6,3))\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","03a94d94":"print(\"Skewness: %f\" %df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" %df_train['SalePrice'].kurt())","903c2e3a":"#Using the numpy fuction log1p which  applies log(1+x) to all to normalize the saleprice\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\n#new distribution\nplt.figure(figsize=(6,4))\nsns.distplot(df_train['SalePrice'] , fit=norm);\n\n#fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#QQ-plot\nfig = plt.figure(figsize=(4,3))\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","28f91a77":"#Scatter plot for GrLivArea\/saleprice\nvariable ='GrLivArea'\ndf_train.iplot(kind='scatter', x='GrLivArea', y='SalePrice',mode='markers',size=6, color='blue')","1889d3a0":"#scatter plot for TotalBsmtSF\/saleprice\nvariable ='TotalBsmtSF'\ndf_train.iplot(kind='scatter', x='TotalBsmtSF', y='SalePrice',mode='markers',size=6, color='cyan')","a6f7bdd6":"#scatter plot for LotFrontage\/saleprice\nvariable ='LotFrontage'\ndf_train.iplot(kind='scatter', x='LotFrontage', y='SalePrice',mode='markers',size=6, color='brown')","be3f279a":"#Scatter plot for 1stFlrSF\/saleprice\nvariable ='1stFlrSF'\ndf_train.iplot(kind='scatter', x='1stFlrSF', y='SalePrice',mode='markers',size=6, color='green')","26e1eb90":"variable ='SaleCondition'\nfig, ax = plt.subplots(figsize=(14, 8))\nfig = sns.boxplot(x=df_train.SaleCondition, y=\"SalePrice\", data=df_train)\nplt.xticks(rotation=60)","d77f2637":"variable ='OverallQual'\nfig, ax = plt.subplots(figsize=(14, 8))\nfig = sns.boxplot(x=df_train.OverallQual, y=\"SalePrice\", data=df_train)\nplt.xticks(rotation=80)\n#fig.axis(ymin=0, ymax=8000)","47463153":"#scatter plots between SalePrice and correlated variables\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath',\n        'YearBuilt','Fireplaces','YearRemodAdd']\nsns.pairplot(df_train[cols], size = 3.5)\nplt.show()","66bf6c2e":"df_train.isna().sum()","52804a87":"df_train.isna().sum().value_counts()","e43362ce":"df_train[\"PoolQC\"] = df_train[\"PoolQC\"].fillna(\"None\")\n\ndf_train[\"MiscFeature\"] = df_train[\"MiscFeature\"].fillna(\"None\")\n\ndf_train[\"Alley\"] = df_train[\"Alley\"].fillna(\"None\")\n\ndf_train[\"Fence\"] = df_train[\"Fence\"].fillna(\"None\")\n\ndf_train[\"FireplaceQu\"] = df_train[\"FireplaceQu\"].fillna(\"None\")\n\ndf_train[\"LotFrontage\"] = df_train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    df_train[col] = df_train[col].fillna('None')\n\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    df_train[col] = df_train[col].fillna(0)\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    df_train[col] = df_train[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df_train[col] = df_train[col].fillna('None')\n\ndf_train[\"MasVnrType\"] = df_train[\"MasVnrType\"].fillna(\"None\")\ndf_train[\"MasVnrArea\"] = df_train[\"MasVnrArea\"].fillna(0)\n\ndf_train['MSZoning'] = df_train['MSZoning'].fillna(df_train['MSZoning'].mode()[0])\n\ndf_train = df_train.drop(['Utilities'], axis=1)\n\ndf_train[\"Functional\"] = df_train[\"Functional\"].fillna(\"Typ\")\n\ndf_train['Electrical'] = df_train['Electrical'].fillna(df_train['Electrical'].mode()[0])\n\ndf_train['KitchenQual'] = df_train['KitchenQual'].fillna(df_train['KitchenQual'].mode()[0])\n\ndf_train['Exterior1st'] = df_train['Exterior1st'].fillna(df_train['Exterior1st'].mode()[0])\ndf_train['Exterior2nd'] = df_train['Exterior2nd'].fillna(df_train['Exterior2nd'].mode()[0])\n\ndf_train['SaleType'] = df_train['SaleType'].fillna(df_train['SaleType'].mode()[0])\n\ndf_train['MSSubClass'] = df_train['MSSubClass'].fillna(\"None\")\n","dc3a80a1":"df_test[\"PoolQC\"] = df_test[\"PoolQC\"].fillna(\"None\")\n\ndf_test[\"MiscFeature\"] = df_test[\"MiscFeature\"].fillna(\"None\")\n\ndf_test[\"Alley\"] = df_test[\"Alley\"].fillna(\"None\")\n\ndf_test[\"Fence\"] = df_test[\"Fence\"].fillna(\"None\")\n\ndf_test[\"FireplaceQu\"] = df_test[\"FireplaceQu\"].fillna(\"None\")\n\ndf_test[\"LotFrontage\"] = df_test.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    df_test[col] = df_test[col].fillna('None')\n\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    df_test[col] = df_test[col].fillna(0)\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    df_test[col] = df_test[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df_test[col] = df_test[col].fillna('None')\n\ndf_test[\"MasVnrType\"] = df_test[\"MasVnrType\"].fillna(\"None\")\ndf_test[\"MasVnrArea\"] = df_test[\"MasVnrArea\"].fillna(0)\n\ndf_test['MSZoning'] = df_test['MSZoning'].fillna(df_test['MSZoning'].mode()[0])\n\ndf_test = df_test.drop(['Utilities'], axis=1)\n\ndf_test[\"Functional\"] = df_test[\"Functional\"].fillna(\"Typ\")\n\ndf_test['Electrical'] = df_test['Electrical'].fillna(df_test['Electrical'].mode()[0])\n\ndf_test['KitchenQual'] = df_test['KitchenQual'].fillna(df_test['KitchenQual'].mode()[0])\n\ndf_test['Exterior1st'] = df_test['Exterior1st'].fillna(df_test['Exterior1st'].mode()[0])\ndf_test['Exterior2nd'] = df_test['Exterior2nd'].fillna(df_test['Exterior2nd'].mode()[0])\n\ndf_test['SaleType'] = df_test['SaleType'].fillna(df_test['SaleType'].mode()[0])\n\ndf_test['MSSubClass'] = df_test['MSSubClass'].fillna(\"None\")","5874b7fa":"#More feature engineering to transform some numerical variables that are really categorical\nadrian = ['MSSubClass', 'OverallCond', 'YrSold', 'MoSold']\nfor i in adrian:\n    df_train[i] = df_train[i].apply(str)\n    \nfor j in adrian:\n    df_test[j] = df_test[j].apply(str)","1988e822":"#Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\nfor j in cols:\n    encoder = LabelEncoder() \n    encoder.fit(list(df_train[j].values)) \n    df_train[j] = encoder.transform(list(df_train[j].values))\n    \nfor k in cols: \n    encoder = LabelEncoder()\n    encoder.fit(list(df_test[k].values)) \n    df_test[k] = encoder.transform(list(df_test[k].values))","0d6dddc8":"#Shape of the data\ndf_train.shape, df_test.shape","164024f2":"numerical_features = df_train.dtypes[df_train.dtypes != \"object\"].index\n\n# Checking the skewness of all numerical features\nskewed_features = df_train[numerical_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_features})\nprint(skewness.head())\n\nnumerical_featurez = df_test.dtypes[df_test.dtypes != \"object\"].index\n\n# Checking the skewness of all numerical features\nskewed_featurez = df_test[numerical_featurez].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical featurez: \\n\")\nskewnes = pd.DataFrame({'Skew' :skewed_featurez})\nprint(skewnes.head())","3787cdc1":"skewness = skewness[abs(skewness) > 1.0]\nprint(\"There are {} skewed numerical features to boxcox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlmbda = 0.15\nfor i in skewed_features:\n    df_train[i] = boxcox1p(df_train[i], lmbda)\n    \n    \nskewnes = skewnes[abs(skewnes) > 1.0]\nprint(\"There are {} skewed numerical features to boxcox transform\".format(skewnes.shape[0]))\n\nskewed_featurez = skewnes.index\nlmbda = 0.15\nfor i in skewed_featurez:\n    df_test[i] = boxcox1p(df_test[i], lmbda)","a690c403":"train_set = df_train.shape[0]\ntest_set = df_test.shape[0]\ndata = pd.concat((df_train, df_test)).reset_index(drop=True)\ndata.drop(['SalePrice', 'Id'], axis=1, inplace=True)\nprint(\"data size is : {}\".format(data.shape))","5103a445":"data = pd.get_dummies(data)\nprint(data.shape)","0d3957de":"#data = data.sample(frac=1.0)\n#data","947194a6":"train = data[:train_set]\ntest = data[train_set:]","4f7c0a9e":"from sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","d81a7963":"Enet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)).fit(train, y_train)\nEnet","f6b4bd6b":"Ridge = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5).fit(train,y_train)\nRidge","50d4c2a1":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1)).fit(train, y_train)\nlasso","adc29665":"Xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                            learning_rate=0.05, max_depth=3,\n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1).fit(train, y_train)\nXgb","7c2d2e5b":"Lgbm = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=700,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11).fit(train, y_train)\nLgbm","8c1b0783":"Gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5).fit(train,y_train)\nGboost","b22bb0fd":"#Scores for different models\nprint('LGBM score : {:.4f}'.format(Lgbm.score(train, y_train)))\nprint('Xgb score : {:.4f}'.format(Xgb.score(train, y_train)))\nprint('Enet score : {:.4f}'.format(Enet.score(train, y_train)))\nprint('Lasso score : {:.4f}'.format(lasso.score(train, y_train)))\nprint('Ridge score : {:.4f}'.format(Ridge.score(train, y_train)))\nprint('Gboost score : {:.4f}'.format(Gboost.score(train, y_train)))","331cfd01":"lasso_pred = (lasso.predict(test))\nRidge_pred = (Ridge.predict(test))\nXgb_pred = (Xgb.predict(test))\nLgbm_pred = (Lgbm.predict(test))\nEnet_pred = (Enet.predict(test))\nGboost_pred = (Gboost.predict(test))","70f6336c":"# Getting final predictions \nfinal_prediction = Xgb_pred*0.35 + Lgbm_pred*0.20 + Gboost_pred*0.20 + lasso_pred*0.10 + Ridge_pred*0.10 + Enet_pred*0.05\nfinal_prediction ","c4cc0196":"test_data = pd.read_csv('\/kaggle\/input\/house-prices-dataset\/test.csv')\ntest_data","1beb4c79":"#Final submission\nsubmission = pd.DataFrame()\nsubmission['Id'] = test_data['Id']\nsubmission['SalePrice'] = final_prediction\nsubmission","57002baf":"submission.to_csv('submission.csv', index=False)","440bc90f":"Kindly upvote if you found this notebook useful..thanks","0a799100":"1. PoolQC : NA means no pool\n2. MiscFeature : NA means no miscfeature\n3. Alley : NA means no Alley access\n4. MSSubClass : NA most likely means No building class. We can replace missing values with None\n5. SaleType : Fill in again with most frequent which is \"WD\"\n6. Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n7. KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual\n8. Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value\n9. Functional : data description says NA means typical\n10. Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it\n11. MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'\n12. MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type\n13. BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement\n14. BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement    \n15. GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage)    \n16. GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\n17. LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.\n18. Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood    \n19. FireplaceQu : data description says NA means \"no fireplace\"\n20. Fence : data description says NA means \"no fence\"\n    ","839d5a98":"Despite having several outliers in my data, i think i'll choose not to delet them and i'll use the RobustScaler to scale features using statistics that are robust to outliers","da0a7b1a":"Relationship of the Saleprice with OverallQual and YearBuilt using box plots","2b47146c":"From the heatmap you can tell the variables that are highly correlated together and those that are very similar, and its from there that some variables are dropped before modelling","45cf1c06":"Since we need to get dummies for easier modelling, we concatenate the train data and the test data and plan to split them after getting dummies so as to have equal shapes for predicting purposes.\nGetting the dummies separately, predicting becomes tricky because of imbalanced shapes.\nAlso we drop the target variable from our train data as well as the 'Id' because they are totally not needed to train our model","607074fc":"From the above histogram \nIt deviates from the normal distribution hence shows that our target variable is positively skewed\nIt also shows peakedness and its leptokurtic indicating a serious outlier since its a large kurtosis value","cbc089ed":"Robust scaler is used to make the models robust to outliers for the ElasticNet, KernelRidge and Lasso..\nGradient Boosting Regression with a huber loss that makes it robust to outliers, Xgb and Lgb are mostly robust to outliers","47c00539":"# HOUSE PRICES","c5cfb739":"Box Cox Transformation of highly skewed features to normalize the data for better results after modelling","493a7e1e":"Skewness and Kurtosis\nSkewness = measure of symmetry or the lack of it, a distribution or data set is symmetric if it looks \nthe same to the left and right\n\nKurtosis = measure of whether data is heavily tailed or lightly tailed relative to a normal distribution","81387601":"# MODELLING","68f46c84":"Predictions for all models","da702ca6":"Checking the relationship of the Saleprice with numerical variables using scatter plots\n\nGrLivArea,TotalBsmtSF,LotFrontage,LotArea,MasVnrArea,1stFlrSF,2ndFlrSF,TotRmsAbvGrd,GarageCars,PoolArea\n\nIn this case lets use GrLivArea,TotalBsmtSF,LotFrontage,1stFlrSF"}}