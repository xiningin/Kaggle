{"cell_type":{"ee146d43":"code","3e6e6051":"code","369b7b5b":"code","c79e89fe":"code","e2f89eea":"code","10262cee":"code","eaf85e85":"code","cb94d25d":"code","9f0504f4":"code","b86255b5":"code","b24804f8":"code","6d262e51":"code","2dd7ce5b":"code","6bcb6eb2":"code","c16352e3":"code","949cae22":"code","71083075":"code","eb8190aa":"code","5d092e7e":"code","e28c6b5c":"code","ee83f202":"code","a764427a":"code","75f3a06e":"code","8699a4e9":"code","f8f81882":"code","74bc2a4a":"code","c06e39f4":"code","14f9d407":"code","5e1dc06c":"code","483259bc":"code","b037e709":"code","5ac70022":"code","e8357758":"code","f422d206":"code","aa151e30":"code","260a83c5":"code","ef46010b":"code","3cd72ec3":"code","8a9547a8":"code","ba26fefe":"code","0ccc51f9":"code","a43ca03f":"code","fbef1747":"code","f83f7b48":"code","e304b014":"code","e57dfd84":"code","43252514":"code","366750d9":"code","73be87ce":"code","7468b730":"code","b78b6788":"code","a9bccd02":"code","841ed010":"code","3b6934d1":"markdown","56fd4cd8":"markdown","967a35bb":"markdown","07e20970":"markdown","73833132":"markdown","1c409434":"markdown","20c9d9ae":"markdown","e94394cd":"markdown","12e9b4ba":"markdown","123740ab":"markdown","0f741704":"markdown","dd0cfa68":"markdown","5dc07bb0":"markdown","1ccb76f6":"markdown","71f540dd":"markdown","8c080eda":"markdown","66161520":"markdown","9f87782e":"markdown","9d2b1eee":"markdown","2330186f":"markdown","0f2b6311":"markdown","658538ee":"markdown","93927860":"markdown"},"source":{"ee146d43":"import geopandas as gpd\nimport matplotlib.pyplot as plt\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns # visualization\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom eli5 import show_weights\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\nimport re\nfrom collections import Counter\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import datasets\nfrom IPython.display import Image  \nfrom sklearn import tree\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# Any results you write to the current directory are saved as output.","3e6e6051":"#creates the bike dataframe from the csv file\nbike_file_path = '..\/input\/austin-bike\/austin_bikeshare_trips.csv'\nbike_data = pd.read_csv(bike_file_path)\n#creates the station dataframe from the csv file\nstation_file_path = '..\/input\/austin-bike\/austin_bikeshare_stations.csv'\nstation_data = pd.read_csv(station_file_path)\n#creates copies of the dataframes\ncbd = bike_data.copy()\ncsd = station_data.copy()","369b7b5b":"cbd.head()","c79e89fe":"csd.head()","e2f89eea":"#prints out the station dataframe\ncsd","10262cee":"#prints out bike dataframe\ncbd","eaf85e85":"cbd.dtypes","cb94d25d":"print(\"Columns with Number of Missing Entries(bike_data):\")\nprint(cbd.isnull().sum())\nprint(\"Columns with Number of Missing Entries(station_data):\")\nprint(csd.isnull().sum())","9f0504f4":"x = 0\nDate=[]\nwhile x<len(cbd):\n    Date.append(cbd.start_time[x][8:10])\n    x = x+1","b86255b5":"cbd['date'] = Date\ncbd[\"date\"] = cbd[\"date\"].astype('int8')\ncbd","b24804f8":"#dropping start\/end IDs\n#Since there are no missing start\/end names, will encode those to fill previously missing IDs\ndel cbd['end_station_id']\ndel cbd['start_station_id']\n\ncbd = cbd[np.isfinite(cbd['month'])]\ncbd","6d262e51":"for category in cbd:\n    cbd[category]=cbd[category].astype('category')\ncbd.dtypes","2dd7ce5b":"cbd[\"start_code\"] = cbd[\"start_station_name\"].cat.codes\ncbd[\"end_code\"] = cbd[\"end_station_name\"].cat.codes\ncbd[\"sub_code\"] = cbd[\"subscriber_type\"].cat.codes\nprint(cbd.isnull().sum())","6bcb6eb2":"#drops .4%\ncbd = cbd.dropna(axis = 0)\ncbd","c16352e3":"cbd.checkout_time\n#for time in cbd.checkout_time:\ntime_list = [time[:2] for time in cbd.checkout_time]\ntime_list","949cae22":"new_times = [time.replace(\":\",\"\") for time in time_list]\nnew_times","71083075":"counted_times = Counter(new_times)\ncounted_times.most_common()","eb8190aa":"cbd[\"start_hour\"] = new_times\ncbd","5d092e7e":"cbd['month_name']=cbd['month'].astype('str')\n\n\n\ncbd['month_name'] = cbd['month_name'].map({'1.0':'January','2.0':'February','3.0':'March','4.0':'April','5.0':'May','6.0':'June','7.0':'July','8.0':'August','9.0':'September','10.0':'October','11.0':'November','12.0':'December'})\ncbd.month_name","e28c6b5c":"cbd = cbd[['bikeid','trip_id','start_station_name','end_station_name','start_time','checkout_time','start_hour','date','month','month_name','year','subscriber_type','duration_minutes','start_code','end_code','sub_code']]\ncbd.head()","ee83f202":"sns.set(font_scale = 1.5)\ng=sns.catplot(x=\"start_hour\",\n              kind=\"count\",\n              palette=\"twilight\",\n              data = cbd,\n              order = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\n                       \"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\n                       \"19\",\"20\",\"21\",\"22\",\"23\",\"24\"])\n\ng.fig.set_size_inches(30,10)\ng.fig.suptitle(\"Start Times\", fontsize=40)\nplt.ylabel(\"Count\", fontsize = 30)\nplt.xlabel(\"Time\", fontsize = 30)\n\nplt.show()","a764427a":"sns.set(font_scale = 1.5)\ng=sns.catplot(x=\"start_station_name\",kind=\"count\", palette=\"Spectral\",data = cbd,order=pd.value_counts(cbd['start_station_name']).iloc[:100].index)\ng.set_xticklabels(rotation=90)\n\ng.fig.set_size_inches(30,10)\ng.fig.suptitle('Top Starting Stations', fontsize=40)\nplt.ylabel(\"Count\", fontsize = 30)\nplt.xlabel(\"Stations\", fontsize = 30)\n\n\nplt.show()","75f3a06e":"sns.set(font_scale = 1.5)\ng=sns.catplot(x=\"end_station_name\",kind=\"count\", palette=\"Spectral\",data = cbd,order=pd.value_counts(cbd['end_station_name']).iloc[:100].index)\ng.set_xticklabels(rotation=90)\n\ng.fig.set_size_inches(30,10)\ng.fig.suptitle('Top Ending Stations', fontsize=40)\nplt.ylabel(\"Count\", fontsize = 30)\nplt.xlabel(\"Stations\", fontsize = 30)\n\n\nplt.show()","8699a4e9":"g = sns.catplot(x=\"start_station_name\",\n                hue=\"month_name\",\n                kind=\"count\",\n                palette=\"Spectral\",\n                edgecolor=\".2\",\n                data=cbd,\n                height=7,\n                aspect =2,\n                order=pd.value_counts(cbd['start_station_name']).iloc[:15].index,\n                hue_order =[\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\n                            \"November\",\"December\"] )\nplt.ylabel('Count',fontsize=30)\nplt.xlabel('Stations',fontsize=30)\nplt.yticks(fontsize=20)\nplt.xticks(fontsize=20)\nplt.title('Top 15 Stations by month',fontsize=50)\n\ng.set_xticklabels(rotation=90)","f8f81882":"top_cbd = cbd[cbd['start_station_name']=='Riverside @ S. Lamar']\nsns.set_style(\"dark\")\nplt.figure(figsize=(20, 10))\nplt.rc('xtick', labelsize=20)\nplt.rc('ytick', labelsize=20)\ng = sns.countplot(x=\"month_name\", data=top_cbd,palette=\"Spectral\",order =[\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\n                            \"November\",\"December\"])\nplt.ylabel(\"Count\", fontsize=30)\nplt.xlabel(\"Month\", fontsize=30)\n\nplt.title(\"Top Station(Riverside @ S. Lamar) Popularity by Month\", fontsize=40)\nplt.xticks(rotation =90)\nplt.show()","74bc2a4a":"sns.set(font_scale = 1.5)\ng=sns.catplot(x=\"subscriber_type\",kind=\"count\", palette=\"Spectral\",data = cbd,order=pd.value_counts(cbd['subscriber_type']).iloc[:6].index)\ng.set_xticklabels(rotation=90)\n\ng.fig.set_size_inches(30,10)\ng.fig.suptitle('Top Subscriber Types', fontsize=40)\nplt.ylabel(\"Count\", fontsize = 30)\nplt.xlabel(\"Subscriber Type\", fontsize = 30)\n\n\nplt.show()","c06e39f4":"list_of_months = ['January','February','March','April','May','June',\n                  'July','August','September','October','November','December']","14f9d407":"index = 0\ndata1=pd.DataFrame()\ndata2=pd.DataFrame()\ndata3=pd.DataFrame()\ndata4=pd.DataFrame()\ndata5=pd.DataFrame()\ndata6=pd.DataFrame()\ndata7=pd.DataFrame()\ndata8=pd.DataFrame()\ndata9=pd.DataFrame()\ndata10=pd.DataFrame()\ndata11=pd.DataFrame()\ndata12=pd.DataFrame()\ndata13=pd.DataFrame()\ndfs = [data1,data2, data3, data4, data5, data6, data7, data8, data9, data10, data11, data12, data13]\n\nwhile index < len(dfs):\n    dfs[index] = cbd[cbd['month'] == index]\n    index = index + 1","5e1dc06c":"sns.set_style(\"dark\")\nfig, axes = plt.subplots(nrows=4, \n                         ncols=3, \n                         figsize=(30,30),\n                         )\nindex2 = 0\nindex = 1\nrow = 0\ncol = 0\nwhile( index<len(dfs)):\n    y = dfs[index]['date'].value_counts()\n    \n    \n    x = y.index\n    \n    axes[row,col].bar(x,y)\n    axes[row,col].set_title('Bike Trips by Day of The Month: ' + list_of_months[index2])\n    index = index + 1\n    index2 = index2 +1\n    if col == 2:\n        col = 0\n        row = row + 1\n        \n    elif col != 2:\n        col = col + 1","483259bc":"index = 0\ndata1=pd.DataFrame()\ndata2=pd.DataFrame()\ndata3=pd.DataFrame()\ndata4=pd.DataFrame()\ndata5=pd.DataFrame()\n\nyear = 2013\ndfs = [data1,data2, data3, data4, data5]\n\nwhile index < len(dfs):\n    dfs[index] = cbd[cbd['year'] == year]\n    index = index + 1\n    year = year + 1","b037e709":"sns.set_style(\"dark\")\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(30,30))\nyear = 2013\nindex = 0\nrow = 0\ncol = 0\nwhile( index<len(dfs)):\n    y = dfs[index]['month'].value_counts()\n    x = y.index\n    axes[row,col].bar(x,y)\n    axes[row,col].set_title(\"{}{}\".format(\"Month Spread in Year: \",year))\n    index = index + 1\n    year = year + 1\n    if col == 1:\n        col = 0\n        row = row + 1\n        \n    elif col != 1:\n        col = col + 1","5ac70022":"def embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')","e8357758":"# Create a map\n# Create a base map\nm_4 = folium.Map(location=[30.2672,-97.7431], tiles='cartodbpositron', zoom_start=13)\n\ndef color_producer(STATUS):\n    if STATUS == \"active\":\n        return 'forestgreen'\n    else:\n        return 'darkred'\n\n# Add a bubble map to the base map\nfor i in range(0,len(station_data)):\n    Circle(\n        location=[station_data.iloc[i]['latitude'], station_data.iloc[i]['longitude']],\n        popup=(station_data.iloc[i]['name'],station_data.iloc[i]['status']),\n        radius=20,\n        color=color_producer(station_data.iloc[i]['status'])).add_to(m_4)\n\n# Display the map\nembed_map(m_4, 'm_4.html')","f422d206":"# Create a map\nm_2 = folium.Map(location=[30.2672,-97.7431], tiles='cartodbpositron', zoom_start=13)\n\n\n# Add points to the map\nfor i in range(0,len(station_data)):\n    Marker(location=[station_data.iloc[i]['latitude'], station_data.iloc[i]['longitude']],\n        tooltip=(station_data.iloc[i]['name'],station_data.iloc[i]['status']),\n        radius=20).add_to(m_2)\n\n# Display the map\nembed_map(m_2, 'm_2.html')","aa151e30":"sns.set(font_scale = 1.5)\ng=sns.catplot(x='duration_minutes',kind=\"count\", palette=\"Spectral\",data = cbd, order=pd.value_counts(cbd['duration_minutes']).iloc[:100].index)\ng.set_xticklabels(rotation=90)\n\ng.fig.set_size_inches(30,10)\ng.fig.suptitle('Duration of Trips', fontsize=40)\nplt.ylabel(\"Count\", fontsize = 30)\nplt.xlabel(\"Duration in Minutes\", fontsize = 30)\n\n\n\nplt.show()","260a83c5":"sns.set(font_scale = 1.5)\ng=sns.catplot(x=\"bikeid\",kind=\"count\", palette=\"icefire\",data = cbd,order=pd.value_counts(cbd['bikeid']).iloc[:100].index)\ng.set_xticklabels(rotation=90)\n\ng.fig.set_size_inches(30,10)\ng.fig.suptitle('Most used Bikes', fontsize=40)\nplt.ylabel(\"Count\", fontsize = 30)\nplt.xlabel(\"Bike ID Number\", fontsize = 30)\n\n\nplt.show()","ef46010b":"cbd[\"year\"] = cbd[\"year\"].astype('category')\ncbd[\"month\"] = cbd[\"month\"].astype('category')\ncbd[\"trip_id\"] = cbd[\"trip_id\"].astype('category')\n\ncbd[\"date\"] = cbd[\"date\"].astype('int8')\n\ncbd[\"year_code\"] = cbd[\"year\"].cat.codes\ncbd[\"month_code\"] = cbd[\"month\"].cat.codes\ncbd[\"trip_id_code\"] = cbd[\"trip_id\"].cat.codes","3cd72ec3":"y = cbd.date\nfeatures = ['year_code','month_code','trip_id_code']\nx = cbd[features]\ntrain_x, val_x, train_y, val_y = train_test_split(x, y, random_state = 0)\nbasic_model = DecisionTreeRegressor()\nbasic_model.fit(train_x, train_y)\nval_predictions = basic_model.predict(val_x)\nprint(\"Printing MAE for Basic Decision Tree Regressor:\\n\", mean_absolute_error(val_y, val_predictions))","8a9547a8":"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    leaf_model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    leaf_model.fit(train_x, train_y)\n    preds_val = leaf_model.predict(val_x)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\nprint(\"Decision Tree with Leaves\\n\")\nfor max_leaf_nodes in [5, 50, 500, 5000,50000]:\n    my_mae = get_mae(max_leaf_nodes, train_x, val_x, train_y, val_y)\n    \n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %f\" %(max_leaf_nodes, my_mae))","ba26fefe":"forest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(train_x, train_y)\nforest_preds = forest_model.predict(val_x)\nprint(\"Printing MAE for Random Forest Model:\\n\",mean_absolute_error(val_y, forest_preds))","0ccc51f9":"perm = PermutationImportance(basic_model, random_state=1).fit(val_x, val_y)\neli5.show_weights(perm, feature_names = val_x.columns.tolist())","a43ca03f":"y = cbd.date\n\n#choosing features\ntrip_features = ['year_code','month_code','trip_id_code']\nX = cbd[trip_features]\n\n#testing\n#X.describe()\nX.head()","fbef1747":"model = DecisionTreeRegressor(random_state=1)\nmodel.fit(X,y)","f83f7b48":"print(\"Making date predictions for the following 10 trips:\")\nprint(X.head(10))\nprint(\"The date predictions are\")\nprint(model.predict(X.head(10)))\n\nprint('\\nOriginal dates')\nprint(cbd['date'].head(10))","e304b014":"predicted_trip_month = model.predict(X)\nprint(\"Printing the mean absolute error\", mean_absolute_error(y, predicted_trip_month))","e57dfd84":"train_X, val_X, train_y, val_y = train_test_split(X,y, random_state = 0)\n\nmodel = DecisionTreeRegressor()\n\nmodel.fit(train_X,train_y)\n\n#getting predicted points\nval_predictions = model.predict(val_X)\nprint(\"Using the DecisionTreeRegressor.. Now\\nPrinting the mean absolute value \",mean_absolute_error(val_y, val_predictions))","43252514":"print(\"Making date predictions for the following 10 Trips:\")\nprint(X.head(10))\nprint(\"The date predictions are\")\nprint(model.predict(X.head(10)))\n\nprint('\\nOriginal Dates')\nprint(cbd['date'].head(10))","366750d9":"my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n                              ('model',\n                               RandomForestRegressor(n_estimators=50,random_state=0))])","73be87ce":"points_CV = -1 * cross_val_score(my_pipeline, X, y, cv=5, \n                               scoring = 'neg_mean_absolute_error')\nprint(\"Using Cross Validation..\\nNow Printing Mean Absolute Error points:\\n\",\n       points_CV)","7468b730":"print(\"Using Cross Validation..\\nNow Printing Average Mean Absolute Error points across all experiments: \\n\", points_CV.mean())","b78b6788":"pipe_data = cbd\n\npipe_data.dropna(axis=0, inplace=True)\ny = pipe_data.date\n\n\n \n\n\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(pipe_data, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n# Select categorical columns\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10000 and \n                    X_train_full[cname].dtype == \"object\"]\n\n \n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\n \n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n\n\n \n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n \n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n \n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n \n\n# Define model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\n \n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\n \n\n# Preprocessing of training data, fit model \nclf.fit(X_train, y_train)\n\n \n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid)\n\nprint('MAE Using Pipeline:', mean_absolute_error(y_valid, preds))","a9bccd02":"xgbr_model = XGBRegressor(n_estimators=5000, learning_rate=0.05, n_jobs=10)\nxgbr_model.fit(train_X, train_y, \n             early_stopping_rounds=5, \n             eval_set=[(val_X, val_y)], \n             verbose=False)","841ed010":"predictionsXBGR = xgbr_model.predict(val_X)\nprint(\"Mean Absolute Error using XGBR: \" + str(mean_absolute_error(predictionsXBGR, val_y)))","3b6934d1":"Now the data will be split into training and validation portions. ","56fd4cd8":"# Bike Share Traffic Over Each Year\nHere we look at the bike share traffic over across each year on a given month.","967a35bb":"Since we have split the data into validation and training portions, we predict point values again.","07e20970":"# Data Visualization\nWe will now look at various relationships between different aspects of the Austin bike share dataset.","73833132":"# Extracting Hour\nWe extract the hours from checkout_time for our use in later visuals.","1c409434":"# Bike Share Traffic Each Month\nHere we will look at the bike share traffic over the course of every month over the time span of the data.","20c9d9ae":"# Model Visualization\nWe have looked at various graphs and data visualizations over this dataset, now we will work with that data and those relationships to build amodels and make predictions based on that data. First we will create a basic model to look at the Mean Absolute Error.","e94394cd":"# Cleaning Up Missing Data\nHere we will drop the end and start IDs and encode them using the names since thse columns do not have missing data. this will create ID columns with zero missing data. We also drop rows with missing month entries by looking at the dataset containing finite values in the column. This will drop the rows with missing year entries (due to them being the rows identical to those with missing month values).","12e9b4ba":"# Exploring and Manipulation the Austin Bike Trip Dataset\nIn this notebook we will be exploring the Austin Bike Share Trip dataset to create visuals that will allow us to show the relationship between different aspects of the data as well as create models to help predict proper employee levels at stations to keep up with usage of bikes throughout the year.","123740ab":"We encode the station as well as subscriber information to obtain new ID codes.","0f741704":"We now drop the remaining 0.4% of data with missing values to have a nice clean dataset.","dd0cfa68":"**Using a Pipeline**","5dc07bb0":"There seems to be some missing data. Lets take a look at that information with missing entries.","1ccb76f6":"Now we will look at the Random Forest Regressor model.","71f540dd":"# Extracting Date\nHere we will extract the date from the time stamp of the start_time column for further visual and predictive uses.","8c080eda":"**Using Cross-Validation\n**\n\n\nCross-Validation is a way to get a more accurate measure of the model's quality. First  we need to define a pipeline, which will fill in the missing values. A randomforest model will make the predictions.","66161520":"We first take a look at the two datasets.","9f87782e":"**XCBRegressor\n**\n\n\nWe will be working with extreme gradient boosting.","9d2b1eee":"We make a column for months using their respective name, rather than number value for later visuals.","2330186f":"The following is a prediction of mean absolute error using the entire dataset. Recall that [error = actual - predicted] ","0f2b6311":"Now we will be using the basic Decision Tree add more and more Leaf Nodes to try and focus the data.","658538ee":"# Geospatial Mapping\nHere we will be able to see the geolocation of each station using the station dataset, one showing the status using a color code for ease of interpretation as well as one with markers to easy of identifying location.","93927860":"Cross validation used 5 different splits of the data to compute MAE. Now we will look at the whole model to be able to determine the model's quality and compare various models."}}