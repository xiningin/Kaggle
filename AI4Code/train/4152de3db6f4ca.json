{"cell_type":{"f42be29a":"code","0eedace6":"code","f9ff4312":"code","b09e3f77":"code","6e5a3637":"code","fb08cf17":"code","7c28623e":"code","932bdbfa":"code","e9c76143":"code","78fc4e31":"code","a69a6fb3":"code","91be51ad":"code","7ef55526":"code","a7803e77":"code","885a71fc":"code","bbb47f06":"code","3177d6c8":"code","ee61ddbb":"code","289a2c1d":"code","8d30026a":"code","287ea1e5":"code","11ab0fd5":"code","edd93a1c":"code","8bb7849d":"code","51de12fa":"code","ac751c6f":"code","0a12ce17":"code","cdfa4edb":"code","dde7b5fa":"code","41342cab":"code","9ab9609c":"code","c386b9cd":"code","64f19b43":"markdown","1da1447a":"markdown","a4593702":"markdown","b0a98fae":"markdown","2a34cf32":"markdown","87be084e":"markdown","f620fe92":"markdown","21dff0c2":"markdown","e9f78c3d":"markdown","a7befd14":"markdown","3b1fe344":"markdown"},"source":{"f42be29a":"import os\nimport collections\nimport pandas as pd\nimport numpy as np\nimport functools\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom sklearn import preprocessing \n\n\nimport xml.etree.ElementTree as ET\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data import SequentialSampler","0eedace6":"BASE_PATH = \"..\/input\/pascal-voc-2012\/VOC2012\"\nXML_PATH = os.path.join(BASE_PATH, \"Annotations\")\nIMG_PATH = os.path.join(BASE_PATH, \"JPEGImages\")\nXML_FILES = [os.path.join(XML_PATH, f) for f in os.listdir(XML_PATH)]\n","f9ff4312":"class XmlParser(object):\n\n    def __init__(self,xml_file):\n\n        self.xml_file = xml_file\n        self._root = ET.parse(self.xml_file).getroot()\n        self._objects = self._root.findall(\"object\")\n        # path to the image file as describe in the xml file\n        self.img_path = os.path.join(IMG_PATH, self._root.find('filename').text)\n        # image id \n        self.image_id = self._root.find(\"filename\").text\n        # names of the classes contained in the xml file\n        self.names = self._get_names()\n        # coordinates of the bounding boxes\n        self.boxes = self._get_bndbox()\n\n    def parse_xml(self):\n        \"\"\"\"Parse the xml file returning the root.\"\"\"\n    \n        tree = ET.parse(self.xml_file)\n        return tree.getroot()\n\n    def _get_names(self):\n\n        names = []\n        for obj in self._objects:\n            name = obj.find(\"name\")\n            names.append(name.text)\n\n        return np.array(names)\n\n    def _get_bndbox(self):\n\n        boxes = []\n        for obj in self._objects:\n            coordinates = []\n            bndbox = obj.find(\"bndbox\")\n            coordinates.append(np.int32(bndbox.find(\"xmin\").text))\n            coordinates.append(np.int32(np.float32(bndbox.find(\"ymin\").text)))\n            coordinates.append(np.int32(bndbox.find(\"xmax\").text))\n            coordinates.append(np.int32(bndbox.find(\"ymax\").text))\n            boxes.append(coordinates)\n\n        return np.array(boxes)","b09e3f77":"def xml_files_to_df(xml_files):\n    \n    \"\"\"\"Return pandas dataframe from list of XML files.\"\"\"\n    \n    names = []\n    boxes = []\n    image_id = []\n    xml_path = []\n    img_path = []\n    for file in xml_files:\n        xml = XmlParser(file)\n        names.extend(xml.names)\n        boxes.extend(xml.boxes)\n        image_id.extend([xml.image_id] * len(xml.names))\n        xml_path.extend([xml.xml_file] * len(xml.names))\n        img_path.extend([xml.img_path] * len(xml.names))\n    a = {\"image_id\": image_id,\n         \"names\": names,\n         \"boxes\": boxes,\n         \"xml_path\":xml_path,\n         \"img_path\":img_path}\n    \n    df = pd.DataFrame.from_dict(a, orient='index')\n    df = df.transpose()\n    \n    return df\n\ndf = xml_files_to_df(XML_FILES)\ndf.head()","6e5a3637":"# check values for per class\ndf['names'].value_counts()","fb08cf17":"# remove .jpg extension from image_id \ndf['img_id'] = df['image_id'].apply(lambda x:x.split('.')).map(lambda x:x[0])\ndf.drop(columns=['image_id'], inplace=True)\ndf.head()","7c28623e":"# classes need to be in int form so we use LabelEncoder for this task\nenc = preprocessing.LabelEncoder()\ndf['labels'] = enc.fit_transform(df['names'])\ndf['labels'] = np.stack(df['labels'][i]+1 for i in range(len(df['labels']))) ","932bdbfa":"classes = df[['names','labels']].value_counts()\nclasses","e9c76143":"# make dictionary for class objects so we can call objects by their keys.\nclasses= {1:'aeroplane',2:'bicycle',3:'bird',4:'boat',5:'bottle',6:'bus',7:'car',8:'cat',9:'chair',10:'cow',11:'diningtable',12:'dog',13:'horse',14:'motorbike',15:'person',16:'pottedplant',17:'sheep',18:'sofa',19:'train',20:'tvmonitor'}","78fc4e31":"# bounding box coordinates point need to be in separate columns\n\ndf['xmin'] = -1\ndf['ymin'] = -1\ndf['xmax'] = -1\ndf['ymax'] = -1\n\ndf[['xmin','ymin','xmax','ymax']]=np.stack(df['boxes'][i] for i in range(len(df['boxes'])))\n\ndf.drop(columns=['boxes'], inplace=True)\ndf['xmin'] = df['xmin'].astype(np.float)\ndf['ymin'] = df['ymin'].astype(np.float)\ndf['xmax'] = df['xmax'].astype(np.float)\ndf['ymax'] = df['ymax'].astype(np.float)","a69a6fb3":"# drop names column since we dont need it anymore\ndf.drop(columns=['names'], inplace=True)\ndf.head()","91be51ad":"len(df['img_id'].unique())","7ef55526":"image_ids = df['img_id'].unique()\nvalid_ids = image_ids[-4000:]\ntrain_ids = image_ids[:-4000]\nlen(train_ids)","a7803e77":"valid_df = df[df['img_id'].isin(valid_ids)]\ntrain_df = df[df['img_id'].isin(train_ids)]\nvalid_df.shape, train_df.shape","885a71fc":"!pip install -q albumentations\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport os\nfrom albumentations import RandomRotate90\nfrom tensorflow.keras import mixed_precision\nimport gc","bbb47f06":"def func(image):\n    Trgb2lms =np.array( [\n          np.array([17.8824, 43.5161, 4.1194]),\n          np.array([3.4557,27.1154, 3.8671]),\n          np.array([0.0300, 0.1843, 1.4671]) \n      ])\n    \n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    x,y,z = image.shape\n#     print(image.shape)\n    cvd_due = np.array([\n                     np.array([1 ,0, 0]),   \n                     np.array([0.494207, 0, 1.24827]),   \n                     np.array([0, 0, 1]),   \n    ])\n    INV_Trgb2lms = np.linalg.inv(Trgb2lms) \n\n#     print(image.transpose(2, 0, 1).shape)\n    out = np.dot(INV_Trgb2lms, cvd_due)\n    out = np.dot(out, Trgb2lms)\n    out = np.dot(out, image.transpose(2, 0, 1).reshape(3,-1)) \n    out = out.reshape(3,x,y).transpose(1, 2, 0)\n    out = cv2.cvtColor(np.float32(out), cv2.COLOR_RGB2BGR)\n\n    return out\n  ","3177d6c8":"class VOCDataset(Dataset):\n    \n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n        \n        self.image_ids = dataframe['img_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n    \n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['img_id'] == image_id]\n        \n        image = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = func(image)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        rows, cols = image.shape[:2]\n        \n        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n        \n       \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        label = records['labels'].values\n        labels = torch.as_tensor(label, dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        # target['masks'] = None\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        \n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1,0)\n            \n            return image, target\n        \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","ee61ddbb":"def get_transform_train():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.2),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format':'pascal_voc', 'label_fields': ['labels']})\n\ndef get_transform_valid():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})","289a2c1d":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = VOCDataset(train_df, IMG_PATH , get_transform_train())\nvalid_dataset = VOCDataset(valid_df, IMG_PATH, get_transform_valid())\n\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\n","8d30026a":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","287ea1e5":"images, targets= next(iter(train_data_loader))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nplt.figure(figsize=(20,20))\nfor i, (image, target) in enumerate(zip(images, targets)):\n    plt.subplot(2,2, i+1)\n    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n    sample = images[i].permute(1,2,0).cpu().numpy()\n    names = targets[i]['labels'].cpu().numpy().astype(np.int64)\n    for i,box in enumerate(boxes):\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (0, 0, 220), 2)\n        cv2.putText(sample, classes[names[i]], (box[0],box[1]+15),cv2.FONT_HERSHEY_COMPLEX ,0.5,(0,220,0),1,cv2.LINE_AA)  \n\n    plt.axis('off')\n    plt.imshow(sample)\n    \n    \n    ","11ab0fd5":"# load a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","edd93a1c":"num_classes = 21  \n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","8bb7849d":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)","51de12fa":"!pip install -U 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'","ac751c6f":"!git clone https:\/\/github.com\/pytorch\/vision.git\n!cd vision;cp references\/detection\/utils.py ..\/;cp references\/detection\/transforms.py ..\/;cp references\/detection\/coco_eval.py ..\/;cp references\/detection\/engine.py ..\/;cp references\/detection\/coco_utils.py ..\/","0a12ce17":"from engine import train_one_epoch, evaluate\nimport utils","cdfa4edb":"%%time\n# let's train it for 2 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, valid_data_loader, device=device)","dde7b5fa":"torch.save(model.state_dict(), 'faster_rcnn_state.pth')","41342cab":"# load  a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n\nWEIGHTS_FILE = \".\/faster_rcnn_state.pth\"\n\nnum_classes = 21\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Load the traines weights\nmodel.load_state_dict(torch.load(WEIGHTS_FILE))\n\nmodel = model.to(device)","9ab9609c":"def obj_detector(img):\n    img = cv2.imread(img, cv2.IMREAD_COLOR)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n\n\n    img \/= 255.0\n    img = torch.from_numpy(img)\n    img = img.unsqueeze(0)\n    img = img.permute(0,3,1,2)\n    \n    model.eval()\n\n    detection_threshold = 0.70\n    \n    img = list(im.to(device) for im in img)\n    output = model(img)\n\n    for i , im in enumerate(img):\n        boxes = output[i]['boxes'].data.cpu().numpy()\n        scores = output[i]['scores'].data.cpu().numpy()\n        labels = output[i]['labels'].data.cpu().numpy()\n\n        labels = labels[scores >= detection_threshold]\n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n    \n    sample = img[0].permute(1,2,0).cpu().numpy()\n    sample = np.array(sample)\n    boxes = output[0]['boxes'].data.cpu().numpy()\n    name = output[0]['labels'].data.cpu().numpy()\n    scores = output[0]['scores'].data.cpu().numpy()\n    boxes = boxes[scores >= detection_threshold].astype(np.int32)\n    names = name.tolist()\n    \n    return names, boxes, sample","c386b9cd":"pred_path = \"..\/input\/data-images\"\npred_files = [os.path.join(pred_path,f) for f in os.listdir(pred_path)]\n\nplt.figure(figsize=(20,60))\nfor i, images in enumerate(pred_files):\n    if i > 19:break\n    plt.subplot(10,2,i+1)\n    names,boxes,sample = obj_detector(images)\n    for i,box in enumerate(boxes):\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (0, 220, 0), 2)\n        cv2.putText(sample, classes[names[i]], (box[0],box[1]-5),cv2.FONT_HERSHEY_COMPLEX ,0.7,(220,0,0),1,cv2.LINE_AA)  \n\n    plt.axis('off')\n    plt.imshow(sample)\n#     plt.savefig('save_image.png', bbox_inches='tight')  # if you want to save result","64f19b43":"# **Test model**","1da1447a":"## **Make dataframe from extracted information**","a4593702":"# Download pretrained model","b0a98fae":"## **Make dataset by Dataset Module** ","2a34cf32":"# **Thats all folks,please consider uplvote this notebook, Thanks for your time.**","87be084e":"## **Separate train and validation data**","f620fe92":"# **View sample**","21dff0c2":"## **Download modules for model training**","e9f78c3d":"## **Extract info. from xml files**","a7befd14":"---","3b1fe344":"## **Train object detection model**"}}