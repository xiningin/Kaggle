{"cell_type":{"12c6eb42":"code","728a7901":"code","1924bf2a":"code","35b38708":"code","2d6a458d":"code","56ce474b":"code","d808bdfd":"code","1c9d82f2":"code","6f753f70":"code","d070f4e4":"code","7f6c3200":"code","f4bd77c2":"code","8e1c94a8":"code","f69b84cb":"code","0ed0bbd6":"code","e211e560":"code","a8568111":"code","a570d871":"code","3096fd40":"markdown","dfa942ef":"markdown","44659f20":"markdown","05c7a299":"markdown"},"source":{"12c6eb42":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))","728a7901":"train = pd.read_excel('..\/input\/Train_Data.xlsx')","1924bf2a":"import fastai\nfrom fastai.text import *\nfrom fastai.callbacks import *","35b38708":"# check the contents of the dat set\ntrain.head()","2d6a458d":"train['question_text'][2]","56ce474b":"train['question_text'][9]","d808bdfd":"train['question_text'][19]","1c9d82f2":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(train)","6f753f70":"# Classifier model data\ndata_clas  = TextClasDataBunch.from_df('.', train,valid_df=val,text_cols='question_text',label_cols='target')","d070f4e4":"data_clas.show_batch()","7f6c3200":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)","f4bd77c2":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","8e1c94a8":"learn.fit_one_cycle(2,1.74E-01)","f69b84cb":"learn.unfreeze()","0ed0bbd6":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","e211e560":"learn.fit_one_cycle(3, 1e-05)","a8568111":"txt_ci = TextClassificationInterpretation.from_learner(learn)","a570d871":"import matplotlib.cm as cm\ntest_text = \"Bangalore was perhaps the best place i have ever seen!\"\ntxt_ci.show_intrinsic_attention(test_text,cmap=cm.Purples)","3096fd40":"we dont find a huge difference between AWD LSTM and Language models because the text corpus is generic, its similar to the corpus which the embeddings are trained on like Wiki103 corpus, but Language models give a huge lift in accuracy\/F1 when the corpus is domain specific and also when we have a large amount of domain corpus which need not be labelled","dfa942ef":"Use Regularized version of Bi LSTM model called AWD LSTM to classify the text documents, before Language models became SOTA, BiLstm were used extensively for Text classification Tasks.\n\nwe have used the Fastai library for this notbook","44659f20":"**Some Cool Visualization to interpret the model working**\n\nProvides an interpretation of classification based on input sensitivity.\n\nThe darker the word-shading in the below example, the more it contributes to the classification. Results here are without any fitting. After fitting to acceptable accuracy, this class can show you what is being used to produce the classification of a particular case.","05c7a299":"we see that the first 4 tokens are the most influential for this sentence"}}