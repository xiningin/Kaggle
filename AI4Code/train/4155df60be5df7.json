{"cell_type":{"77204388":"code","1fff49e2":"code","89114027":"code","15c59354":"code","72998fdd":"code","3d658dcd":"code","aa297b8b":"code","030fa0a0":"code","1ac192bb":"markdown","241ce2b4":"markdown","1a6c48f3":"markdown","d83d1812":"markdown","22ac0df5":"markdown","4e8c2762":"markdown","23d54c6a":"markdown","cbe400bc":"markdown","47340c0f":"markdown","df764811":"markdown","03950d98":"markdown","c93ed47d":"markdown","b376ec60":"markdown","edf9e707":"markdown","83e812b2":"markdown"},"source":{"77204388":"import pandas as pd\n\ndf = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv')\ndf.dropna(subset=['pubmed_id'], inplace=True)\ndf.drop_duplicates(subset=\"pubmed_id\", keep=False, inplace=True)\ndf.info()","1fff49e2":"from Bio import Entrez\n\nEntrez.api_key = '14e49850c0c543c6475d9a78943173bc8508'\nEntrez.email = 'pubmed.query@gmail.com'","89114027":"refs = []\nallids = [int(id) for id in df['pubmed_id'].tolist()]\nidbatches = [allids[x:x+100] for x in range(0, len(allids), 100)]\nfor i, ids in enumerate(idbatches):\n    print('Processing batch %i' % i)\n    handle = Entrez.elink(dbfrom='pubmed', id=ids, linkname='pubmed_pubmed_refs')\n    results = Entrez.read(handle)\n    \n    for res in results:\n        if res[\"LinkSetDb\"] == []:\n            pmids = []\n        else:\n            pmids = [int(link[\"Id\"]) for link in res[\"LinkSetDb\"][0][\"Link\"]]\n        refs.append(pmids)\n        \ndf['refs'] = refs\nallrefs = [ref for reflist in df['refs'].tolist() for ref in reflist] # might contain duplicates","15c59354":"seen = {}\ncommonrefs = []\n\nfor x in allrefs:\n    if x not in seen:\n        seen[x] = 1\n    else:\n        if seen[x] == 1:\n            commonrefs.append(x)\n        seen[x] += 1\nprint('There are %i refs that occur more than once.' % len(commonrefs))","72998fdd":"shared = []\ncross = {id:1 for id in allids}\nfor index, row in df.iterrows():\n    sharedrefs = [ref for ref in row['refs'] if seen[ref] > 1 or ref in cross]\n    shared.append(sharedrefs)","3d658dcd":"df['sharedrefs'] = shared\ndf['nsharedrefs'] = df['sharedrefs'].apply(lambda x: len(x))\ndf = df[df['nsharedrefs'] > 0]","aa297b8b":"nodes = allids + commonrefs\nnode_types = [1 for pub in allids] + [0 for ref in commonrefs]\nnode_titles = [title for title in df['title'].tolist()] + ['' for ref in commonrefs]\ndf_nodes = pd.DataFrame(list(zip(nodes, node_types, node_titles)), columns =['Id', 'Type', 'Label'])\ndf_nodes.to_csv('nodes.csv', index=False)  ","030fa0a0":"sources = []\ntargets = []\nfor _,row in df.iterrows():\n    source = int(row['pubmed_id'])\n    for ref in row['sharedrefs']:\n        target = ref\n        sources.append(source)\n        targets.append(target)\n        \ndf_edges = pd.DataFrame(list(zip(sources, targets)), columns =['Source', 'Target'])\ndf_edges.to_csv('edges.csv', index=False)  ","1ac192bb":"Alright, we got the references. There is a little more processing to do before drawing, in order to keep the number of vertices in the final graph as low as possible. If there are too many, layout and drawing will take forever.","241ce2b4":"## A comment on the final map\nThe final map only shows the references that are in the dataset. However, both layout and partioning have included the common references as well. This should give a robust clustering. A different variation of this analysis would be to create this map only based on crossreferences within this dataset. Another intersting thing might be to introduce a publication date somehow and turn this into more of a familiy tree.","1a6c48f3":"## Connect to PubMed\nI use the Bio API and an account I made for this purpose to connect to PubMed. This will allow me to query citation data.","d83d1812":"This is the data part done. The rest is done with Gephi. I pretty much follow this tutorial [here](http:\/\/blog.miz.space\/tutorial\/2020\/01\/05\/gephi-tutorial-sigma-js-plugin-publishing-interactive-graph-online\/), and publish the result here: https:\/\/moritzmoeller.github.io\/covid-19-literature-atlas\/. The main steps are:\n* Use Gephi's Modularity feature to partion nodes into communities (tighly connected subgraphs). This gives a partion.\n* Use this partion as first and number of incoming edges as second feature for the Circle Pack layout to obtain a nice map.\n* Make nodes and edges of shared references invisible (we only care about the present dataset for now)\n* Use number of citations to determine the size of the nodes (i.e. articles that are referenced often will be big)\n* Export a SigmaJS template using the respective plugin, and publish through GitHub.","22ac0df5":"I then add the lists of shared references to the dataframe. I drop articles that are not connected to any of them.","4e8c2762":"![graph_v2.jpg](attachment:graph_v2.jpg)","23d54c6a":"Then, get a list of shared or cross references for each article in our data set (we throw away the references that are only cited in one article, since they won't help us with the atlas).","cbe400bc":"## Export for Gephi\nTo visualise the atlas of publications and the references between them, I'll use Gephi (https:\/\/gephi.org). It allows to run some graph analyses useful for finding clusters, as well as layouts and drawing. To do all this, I need to export a list of nodes (those will be the publications, both from the dataset and from the shared references) and a list of edges (those are the references, i.e. there should be an edge between two publications if one of them references the other).","47340c0f":"## Find shared references\nOut of all references, I am only interested in two types:\n1. Crossreferences between articles in our data set\n2. References that occur in more than one reference list, which I refer to as shared references\n\nNext, I'll find those shared references. First, get a list of them:","df764811":"Then, I create the edge list, and also save it as *csv. It is important to have one column called Source and one called Target. In Gephi, the Id of nodes will be identified with the entries in Source and Target.","03950d98":"First, create the node list. For each node, I supply a PubMed ID, the title of the publication and whether or not is is in the dataset. I create a table containing those things and save it as *.csv. It is important to have one column called Id and one called Label.","c93ed47d":"Now, we have our reference data. The last bit of this will be to visualise it. ","b376ec60":"## Load data\nFor now, I'll only use metadata for this. I get my reference data from PubMed, so I discard all articles without Pubmed ID. I'll get rid of duplicates, too.","edf9e707":"## Get citation data\nFor each article, I retrieve the PubMed IDs of the articles on its reference list. This is done in batches of 100 articles at a time.","83e812b2":"# A references-based atlas of COVID-19 research\nWe have a massive set of articles. I want to create a map of them. One way to organise this map is through references: these articles will cite each other and will have common references as well. References will correlate strongly with the content. Hence, structuring the map using the web of references might help to organise of this huge body of literature.\n\nThe interactive map (the final result) can be found here: https:\/\/moritzmoeller.github.io\/covid-19-literature-atlas\/\n\nYou can:\n* get an idea of the global picture of the dataset\n* look at article titles by hovering over nodes\n* explore the publications referenced in articles by clicking on nodes\n* search for articles by name\n\nBelow, I'll show how to get and process the data to the point where all is put together in a graph. The layout and visualisition is done in Gephi. I describe the steps I have taken there, but there is no code to provide as I used the GUI to arrive at the final result."}}