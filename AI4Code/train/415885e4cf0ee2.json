{"cell_type":{"45f06882":"code","ad5fd752":"code","add2061e":"code","3ec5933b":"code","e48ed63c":"code","fcb889e9":"code","0cafee9b":"code","cf439da9":"code","eace75bb":"code","d733c79b":"code","f50259c7":"code","2ea19057":"code","eda11bbf":"code","68971bcd":"code","2f289ce4":"code","f8f1207b":"code","ad53a632":"code","28c03e46":"code","553d9937":"code","153740b2":"code","a1c1216f":"code","c114d4e9":"code","dbfdcc96":"code","5039de5d":"code","ac6f90ea":"code","2f64178a":"code","e1bdff43":"code","0602b402":"code","7ab896fb":"code","44c795e2":"code","9c60db10":"code","4ad81326":"code","dde1963d":"code","a23c5b1c":"code","73fe9f88":"code","7bf1f8a6":"code","01958bd0":"code","dd093ff2":"code","ea4f9382":"code","3e9c8289":"code","272794d1":"code","f001598c":"code","55a1cb16":"code","2449961c":"code","c1e66074":"code","7b2189fa":"code","4b969ba0":"code","d389ab53":"code","40168447":"code","3dc1c02e":"code","854be948":"code","4ec8965c":"code","b87198ee":"code","9208fc88":"code","4693c4fb":"code","c0ba549e":"code","ffb91738":"code","a1052253":"code","8fbb9db8":"code","81e89315":"code","3c9e866e":"code","f0a28d50":"code","1c24a3df":"code","f06c99c1":"code","6d6db7fe":"code","a6fb8fad":"code","c28c12c9":"code","fa90291f":"code","3f26193d":"code","1044865d":"code","73c0b2ba":"code","edb9e08a":"code","02dbc0ec":"code","c1e9e15f":"code","c967ecd6":"code","81ea1377":"code","6225823f":"code","fddedb39":"code","84b18753":"code","a8247938":"code","507d441a":"code","c569f963":"code","c13aac29":"markdown","497fb2ff":"markdown","164e238f":"markdown","447b7ded":"markdown","62417500":"markdown","23f0ecec":"markdown","8a409a15":"markdown","0b027003":"markdown","67f18652":"markdown","66047c57":"markdown","fcb38d01":"markdown","976b6831":"markdown","9240cf7c":"markdown","3b7f5873":"markdown","1ae2643d":"markdown","80feb85b":"markdown","aed38b5c":"markdown","fedfbb8c":"markdown","4611d8c9":"markdown","f941d452":"markdown","841de912":"markdown","5a06f744":"markdown","10ffb341":"markdown","c711c1ec":"markdown","4339f77e":"markdown","5af1f825":"markdown","db1917aa":"markdown","1f15c7e8":"markdown","30d9991c":"markdown","e4d7a2c4":"markdown","10b52f0c":"markdown","f6bc9b39":"markdown","f1f8b450":"markdown","8af1cc4a":"markdown","80e97aac":"markdown","d549c532":"markdown","f7cf490a":"markdown","5aea0e31":"markdown","a7c45eea":"markdown","d68e28d8":"markdown","d8c47f0b":"markdown","29d3b660":"markdown","c7cd42df":"markdown","55e763da":"markdown","6c9da807":"markdown","a19d2049":"markdown","585f6947":"markdown","8d2910b2":"markdown","a5ed3845":"markdown","08189eb6":"markdown","f85604d1":"markdown","7958d163":"markdown","f3c0d7e6":"markdown","6a6b7ead":"markdown","24aea002":"markdown","376896fd":"markdown","63d3f02a":"markdown","4ad3e98f":"markdown","ae032cd4":"markdown","3a84a45d":"markdown","f481c93e":"markdown","5f771517":"markdown","60ef5415":"markdown","62454adc":"markdown","f23b009b":"markdown","02bdae40":"markdown"},"source":{"45f06882":"from typing import List\nimport random\n\nimport glob\nfrom nltk import tokenize, download\nimport numpy as np\nimport pandas as pd","ad5fd752":"download('punkt')","add2061e":"def split_text(filepath: str, min_char: int = 5) -> List[str]:\n    \n    text = str()\n    with open(filepath, 'r', encoding='utf8') as file:\n        text = file.read().replace('\\n', '. ')\n        text = text.replace('.\u201d', '\u201d.').replace('.\"', '\".').replace('?\u201d', '\u201d?').replace('!\u201d', '\u201d!')\n        text = text.replace('--', ' ').replace('. . .', '').replace('_', '')\n    \n    sentences = tokenize.sent_tokenize(text)    \n    sentences = [sentence for sentence in sentences if len(sentence) >= min_char]\n\n    return list(sentences)","3ec5933b":"chekhov = []\nfor path in glob.glob('..\/input\/russian-literature\/prose\/Chekhov\/*.txt'):\n    chekhov += split_text(path)\n    \ndostoevsky = []\nfor path in glob.glob('..\/input\/russian-literature\/prose\/Dostoevsky\/*.txt'):\n    dostoevsky += split_text(path)\n\ntolstoy = []\nfor path in glob.glob('..\/input\/russian-literature\/prose\/Tolstoy\/*.txt'):\n    tolstoy += split_text(path)","e48ed63c":"text_dict = { 'Chekhov': chekhov, 'Dostoevsky': dostoevsky, 'Tolstoy': tolstoy }\n\nfor key in text_dict.keys():\n    print(key, ':', len(text_dict[key]), ' sentences')","fcb889e9":"np.random.seed(1)\n\nmax_len = 20_000\n\nnames = [chekhov, dostoevsky, tolstoy]\n\ncombined = []\nfor name in names:\n    name = np.random.choice(name, max_len, replace = False)\n    combined += list(name)\n\nprint('Length of combo and internally shuffled list:', len(combined))","0cafee9b":"labels = ['Chekhov'] * max_len + ['Dostoevsky'] * max_len + ['Tolstoy'] * max_len\n\nprint('Length of marked list:', len(labels))","cf439da9":"len(combined) == len(labels)","eace75bb":"random.seed(3)\n\nzipped = list(zip(combined, labels))\nrandom.shuffle(zipped)\ncombined, labels = zip(*zipped)","d733c79b":"out_data = pd.DataFrame()\nout_data['text'] = combined\nout_data['author'] = labels","f50259c7":"print(out_data.head())\nprint(out_data.tail())","2ea19057":"out_data.to_csv('author_data.csv', index=False)","eda11bbf":"import string\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.stem.porter import PorterStemmer","68971bcd":"data = pd.read_csv('author_data.csv', encoding='utf8')\nprint(data.head())","2f289ce4":"text = list(data['text'].values)\nauthor = list(data['author'].values)\n\nprint('Dataset contains {} notes.'.format(len(text)))","f8f1207b":"authors = Counter(author)\nauthors","ad53a632":"author_names = list(authors.keys())\nauthor_names","28c03e46":"np.random.seed(73)\nn = len(text)\n\nfor _ in range(5):\n    print(text[np.random.randint(0, n)])","553d9937":"word_count = np.array([len(sent.split()) for sent in text])\nchar_count = np.array([len(sent) for sent in text])\nave_length = char_count \/ word_count","153740b2":"def get_stats(var):    \n    print('\\t Min: ', np.min(var))\n    print('\\t Max: ', np.max(var))\n    print('\\t Average: ', np.mean(var))\n    print('\\t Median: ', np.median(var))\n    print('\\t Percentile 1%: ', np.percentile(var, 1))\n    print('\\t Percentile 95%: ', np.percentile(var, 95))\n    print('\\t Percentile 99%: ', np.percentile(var, 99))\n    print('\\t Percentile 99.5%: ', np.percentile(var, 99.5))\n    print('\\t Percentile 99.9%: ', np.percentile(var, 99.9))","a1c1216f":"print('Word count statistics:')\nget_stats(word_count)","c114d4e9":"sns.distplot(word_count, kde=True, bins=80, color='green').set_title('Distribution of word count')\nplt.xlabel('Sentence length in words')\nplt.ylabel('Number of offers')\nplt.xlim(0, 100)\nplt.savefig('word_count.png')","dbfdcc96":"print('Character count statistics:')\nget_stats(char_count)","5039de5d":"sns.distplot(char_count, kde=True, bins=80, color='green').set_title('Distribution of characters')\nplt.xlabel('Sentence length in characters')\nplt.ylabel('Number of sentences')\nplt.xlim(0, 400)\nplt.savefig('char_count.png')","ac6f90ea":"print('Average length statistics:')\nget_stats(ave_length)","2f64178a":"sns.distplot(ave_length, kde=True, bins=80, color='green').set_title('Distribution of average word length')\nplt.xlabel('Average word length in characters')\nplt.ylabel('Number of sentences')\nplt.xlim(0, 10)\nplt.savefig('ave_length.png')","e1bdff43":"word_outliers = np.where(word_count > 150)\n\nfor i in word_outliers[0][:5]:\n    print('Author: {}, Sentence length: {}'.format(author[i], word_count[i]))\n    print(text[i], '\\n')","0602b402":"max_authors = {author : 0 for author in author_names}\n\nfor i in word_outliers[0]:\n    max_authors[author[i]] += 1\n\nCounter(max_authors)","7ab896fb":"word_outliers = np.where(word_count < 2)\n\nfor i in word_outliers[0][:10]:\n    print('Sentence length: {}'.format(word_count[i]))\n    print(text[i], '\\n')","44c795e2":"text_string = ''\nfor sents in text:\n    text_string += sents.lower()\n\nchar_cnt = Counter(text_string)\nprint(char_cnt)\nprint(len(char_cnt), 'unusual symbols in data.')","9c60db10":"print(list(char_cnt.keys()))","4ad81326":"accented_chars = ['f', 'u', 'r', 's', 'i', 'c', 'h', '\u0301', 'n', 'd', 'p', 'e', 'a', 't', 'o', 'l', 'x', 'm', 'j', '\u00e9', '\u00f4', 'v', 'q', '\u00ea', 'g', 'b', 'k', 'y', '\u00e0', '\u0456', 'z', 'w', '\u00e8', '\u00f3', '\u00f6', '\u00b0', '\u00e7', '\u00ef', '\u00e1', '\u00fc', '\u00f9', '\u00fb', '\u00ee', '\u0463', '\u00e2']\n\naccented_text = []\nfor i in range(len(text)):\n    for j in text[i]:\n        if j in accented_chars:\n            accented_text.append(i)\n        \naccented_text = list(set(accented_text))\n \nprint(len(accented_text), 'sentences contains unusual symbols.')","dde1963d":"for i in accented_text[:10]:\n    print('Sentence number {}: '.format(i))\n    print(text[i], '\\n')","a23c5b1c":"text = [excerpt.replace('\\xa0', '').replace('\\x7f', '') for excerpt in text]","73fe9f88":"ctr = 0\nfor excerpt in text:\n    if '  ' in excerpt:\n        ctr += 1\n\nprint(ctr, 'occurrences of large blocks of indentation.')","7bf1f8a6":"new_text = []\nfor excerpt in text:\n    while '  ' in excerpt:\n        excerpt = excerpt.replace('  ',' ')\n    new_text.append(excerpt)\n\ntext = new_text\nprint(len(text))","01958bd0":"normed_text = []\n\nfor sent in text:\n    new = sent.lower()\n    new = new.translate(str.maketrans('','', string.punctuation))\n    new = new.replace('\u201c', '').replace('\u201d', '') # english quotes\n    new = new.replace('\u201f', '').replace('\u201d', '') # french quotes\n    new = new.replace('\u00ab', '').replace('\u00bb', '') # christmas tree quotes\n    new = new.replace('\u2014', '').replace('\u2013', '') # em dash\n    new = new.replace('(', '').replace(')', '')\n    new = new.replace('\u2026', '') # ellipsis as one character\n    \n    normed_text.append(new)\n    \nprint(normed_text[0:5])\nprint(len(normed_text))","dd093ff2":"data['text'] = normed_text\n\ndata.to_csv('preprocessed_data.csv', index=False)","ea4f9382":"from typing import List\n\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.preprocessing import LabelBinarizer\n\nimport keras\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Flatten, Dropout, Embedding\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.text import one_hot\nfrom keras.callbacks import ModelCheckpoint \n\nfrom scipy import stats","3e9c8289":"data = pd.read_csv(\"preprocessed_data.csv\", encoding='utf8')\nprint(data.head())","272794d1":"normed_text = list(data['text'])\nauthor = list(data['author'])\n\nauthors_names = list(Counter(author).keys())\nauthors_count = len(authors_names)\n\nnormed_text = [str(i) for i in normed_text]","f001598c":"def plot_confusion_matrix(cm, classes: List[str],\n                          normalize: bool = False,\n                          title: str = 'Confusion matrix',\n                          cmap = plt.cm.Greens):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print('Normalized confusion matrix')\n    else:\n        print('Unnormalized confusion matrix')\n\n    print(cm)\n       \n    df_cm = pd.DataFrame(cm, index = classes,\n                  columns = classes)\n    sns.heatmap(df_cm, annot=True, cmap = cmap)\n    plt.ylabel('Right author')\n    plt.xlabel('Predicted author')\n    plt.title(title)","55a1cb16":"def plot_history_of_accurancy(history):\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model\\'s accurancy')\n    plt.ylabel('accurancy')\n    plt.xlabel('epochs')\n    plt.legend(['teaching data', 'test data'], loc='upper left')","2449961c":"def plot_history_of_loss(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model\\'s error')\n    plt.ylabel('error')\n    plt.xlabel('epochs')\n    plt.legend(['teaching data', 'test data'], loc='upper left')","c1e66074":"text_train, text_test, author_train, author_test = train_test_split(normed_text, author, test_size=0.2, random_state=5)","7b2189fa":"print(np.shape(text_train))\nprint(np.shape(text_test))\nprint(np.shape(author_train))\nprint(np.shape(author_test))","4b969ba0":"def create_n_grams(excerpt_list: List[str], n: int, vocab_size: int, seq_size: int):\n    n_gram_list = []\n\n    for excerpt in excerpt_list:\n        excerpt = excerpt.replace(\" \", \"\")\n\n        n_grams = [excerpt[i:i + n] for i in range(len(excerpt) - n + 1)]\n\n        new_string = \" \".join(n_grams)\n\n        hot = one_hot(new_string, round(vocab_size * 1.3))\n\n        hot_len = len(hot)\n        if hot_len >= seq_size:\n            hot = hot[0:seq_size]\n        else:\n            diff = seq_size - hot_len\n            extra = [0]*diff\n            hot = hot + extra\n\n        n_gram_list.append(hot)\n    \n    n_gram_array = np.array(n_gram_list)\n    \n    return n_gram_array","d389ab53":"def get_vocab_size(excerpt_list: List[str], n: int, seq_size: int) -> int:\n    n_gram_list = []\n\n    for excerpt in excerpt_list:\n        excerpt = excerpt.replace(\" \", \"\")\n   \n        n_grams = [excerpt[i:i + n] for i in range(len(excerpt) - n + 1)]\n\n        gram_len = len(n_grams)\n        if gram_len >= seq_size:\n            n_grams = n_grams[0:seq_size]\n        else:\n            diff = seq_size - gram_len\n            extra = [0]*diff\n            n_grams = n_grams + extra\n        \n        n_gram_list.append(n_grams)\n    \n    n_gram_list = list(np.array(n_gram_list).flat)\n    \n    n_gram_cnt = Counter(n_gram_list)\n    vocab_size = len(n_gram_cnt)\n    \n    return vocab_size","40168447":"vocab_sizes = []\nfor i in range(1, 4):\n    vocab_sizes.append(get_vocab_size(text_train, i, 350))\n    print('Size for n =', i, 'is:', vocab_sizes[i - 1])","3dc1c02e":"gram1_train = create_n_grams(text_train, 1, vocab_sizes[0], 350)\ngram2_train = create_n_grams(text_train, 2, vocab_sizes[1], 350)\ngram3_train = create_n_grams(text_train, 3, vocab_sizes[2], 350)","854be948":"gram1_test = create_n_grams(text_test, 1, vocab_sizes[0], 350)\ngram2_test = create_n_grams(text_test, 2, vocab_sizes[1], 350)\ngram3_test = create_n_grams(text_test, 3, vocab_sizes[2], 350)","4ec8965c":"print(np.shape(gram1_train))\nprint(np.shape(gram2_train))\nprint(np.shape(gram3_train))\n\nprint(np.shape(gram1_test))\nprint(np.shape(gram2_test))\nprint(np.shape(gram3_test))","b87198ee":"max_1gram = np.max(gram1_train)\nmax_2gram = np.max(gram2_train)\nmax_3gram = np.max(gram3_train)\n\nprint('Max value for 1-gramms: ', max_1gram)\nprint('Max value for bigramms: ', max_2gram)\nprint('Max value for trigramms: ', max_3gram)","9208fc88":"processed_train = text_train\nprocessed_test = text_test\n\nprint(processed_train[0:5])","4693c4fb":"vectorizer = TfidfVectorizer(strip_accents = 'unicode', min_df = 6)\nvectorizer.fit(processed_train)\n\nprint('Dictionary size: ', len(vectorizer.vocabulary_))\n\nwords_train = vectorizer.transform(processed_train)\nwords_test = vectorizer.transform(processed_test)","c0ba549e":"author_lb = LabelBinarizer()\n\nauthor_lb.fit(author_train)\nauthor_train_hot = author_lb.transform(author_train)\nauthor_test_hot = author_lb.transform(author_test)","ffb91738":"def define_model(input_len: int, output_size: int, vocab_size : int, embedding_dim: int, verbose: bool = True,\n                drop_out_pct: float = 0.25, conv_filters: int = 500, activation_fn: str = 'relu', pool_size: int = 2, learning: float = 0.0001):\n    inputs1 = Input(shape=(input_len,))\n    embedding1 = Embedding(vocab_size, embedding_dim)(inputs1)\n    drop1 = Dropout(drop_out_pct)(embedding1)\n    conv1 = Conv1D(filters=conv_filters, kernel_size=3, activation=activation_fn)(drop1)\n    pool1 = MaxPooling1D(pool_size=pool_size)(conv1)\n    flat1 = Flatten()(pool1)\n    \n    inputs2 = Input(shape=(input_len,))\n    embedding2 = Embedding(vocab_size, embedding_dim)(inputs2)\n    drop2 = Dropout(drop_out_pct)(embedding2)\n    conv2 = Conv1D(filters=conv_filters, kernel_size=4, activation=activation_fn)(drop2)\n    pool2 = MaxPooling1D(pool_size=pool_size)(conv2)\n    flat2 = Flatten()(pool2)\n    \n    inputs3 = Input(shape=(input_len,))\n    embedding3= Embedding(vocab_size, embedding_dim)(inputs3)\n    drop3 = Dropout(drop_out_pct)(embedding3)\n    conv3 = Conv1D(filters=conv_filters, kernel_size=5, activation=activation_fn)(drop3)\n    pool3 = MaxPooling1D(pool_size=pool_size)(conv3)\n    flat3 = Flatten()(pool3)\n    \n    merged = concatenate([flat1, flat2, flat3])\n    \n    output = Dense(output_size, activation='softmax')(merged)\n    \n    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n    \n    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning), metrics=['accuracy'])\n    \n    if verbose:\n        print(model.summary())\n        \n    return model","a1052253":"gram1_model = define_model(350, authors_count, max_1gram + 1, 26)","8fbb9db8":"gram1_model_history = gram1_model.fit([gram1_train, gram1_train, gram1_train], author_train_hot, epochs=10, batch_size=32, \n                verbose = 1, validation_split = 0.2)","81e89315":"gram2_model = define_model(350, authors_count, max_2gram + 1, 300)","3c9e866e":"gram2_model_history = gram2_model.fit([gram2_train, gram2_train, gram2_train], author_train_hot, epochs=10, batch_size=32, \n                verbose = 1, validation_split = 0.2)","f0a28d50":"t0 = time.time()\ngram3_model = define_model(350, authors_count, max_3gram + 1, 600)","1c24a3df":"gram3_model_history = gram3_model.fit([gram3_train, gram3_train, gram3_train], author_train_hot, epochs=10, batch_size=32, \n                verbose=1, validation_split=0.2)\nt1 = time.time()","f06c99c1":"author_pred1 = gram3_model.predict([gram3_test, gram3_test, gram3_test])\n\nt2 = time.time()\n\nauthor_pred1 = author_lb.inverse_transform(author_pred1)\n\naccuracy = accuracy_score(author_test, author_pred1)\nprecision, recall, f1, support = score(author_test, author_pred1)\nave_precision = np.average(precision, weights = support\/np.sum(support))\nave_recall = np.average(recall, weights = support\/np.sum(support))\nave_f1 = np.average(f1, weights = support\/np.sum(support))\nconfusion = confusion_matrix(author_test, author_pred1, labels=authors_names)\n    \nprint('Accurancy:', accuracy)\nprint('Average Precision:', ave_precision)\nprint('Average Recall:', ave_recall)\nprint('Average F1 Score:', ave_f1)\nprint('Learning time:', (t1 - t0), 'seconds')\nprint('Prediction time:', (t2 - t1), 'seconds')\nprint('Confusion matrix:\\n', confusion)","6d6db7fe":"plot_confusion_matrix(confusion, classes=authors_names, \\\n                      normalize=True, title='Normalized confusion matrix - Model 1')\n\nplt.savefig('confusion_model1.png')","a6fb8fad":"plot_history_of_accurancy(gram3_model_history)\nplt.savefig('accurancy_model1.png')","c28c12c9":"plot_history_of_loss(gram3_model_history)\nplt.savefig('loss_model1.png')","fa90291f":"keras.utils.plot_model(gram3_model, 'gram3_model1_arh.png')","3f26193d":"def define_model2(input_len: int, output_size: int, vocab_size: int, embedding_dim: int, verbose: bool = True,\n                drop_out_pct: float = 0.25, conv_filters: int = 500, activation_fn: str = 'relu', pool_size:int = 2, learning: float = 0.0001):\n    \n    inputs1 = Input(shape=(input_len,))\n    embedding1 = Embedding(vocab_size, embedding_dim)(inputs1)\n    drop1 = Dropout(drop_out_pct)(embedding1)\n    conv1 = Conv1D(filters=conv_filters, kernel_size=3, activation=activation_fn)(drop1)\n    pool1 = MaxPooling1D(pool_size=pool_size)(conv1)\n    flat1 = Flatten()(pool1)\n    \n    inputs2 = Input(shape=(input_len,))\n    embedding2 = Embedding(vocab_size, embedding_dim)(inputs2)\n    drop2 = Dropout(drop_out_pct)(embedding2)\n    conv2 = Conv1D(filters=conv_filters, kernel_size=4, activation=activation_fn)(drop2)\n    pool2 = MaxPooling1D(pool_size=pool_size)(conv2)\n    flat2 = Flatten()(pool2)\n\n    inputs3 = Input(shape=(input_len,))\n    embedding3= Embedding(vocab_size, embedding_dim)(inputs3)\n    drop3 = Dropout(drop_out_pct)(embedding3)\n    conv3 = Conv1D(filters=conv_filters, kernel_size=5, activation=activation_fn)(drop3)\n    pool3 = MaxPooling1D(pool_size=pool_size)(conv3)\n    flat3 = Flatten()(pool3)\n    \n    inputs4 = Input(shape=(input_len,))\n    embedding4 = Embedding(vocab_size, embedding_dim)(inputs4)\n    drop4 = Dropout(drop_out_pct)(embedding4)\n    conv4 = Conv1D(filters=conv_filters, kernel_size=6, activation=activation_fn)(drop4)\n    pool4 = MaxPooling1D(pool_size=pool_size)(conv4)\n    flat4 = Flatten()(pool4)\n    \n    merged = concatenate([flat1, flat2, flat3, flat4])\n    \n    output = Dense(output_size, activation='softmax')(merged)\n    \n    model = Model(inputs = [inputs1, inputs2, inputs3, inputs4], outputs = output)\n    \n    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning), metrics=['accuracy'])\n    \n    if verbose:\n        print(model.summary())\n        \n    return model","1044865d":"t0 = time.time()\ngram3_model2 = define_model2(350, authors_count, max_3gram + 1, 600)","73c0b2ba":"gram3_model2_history = gram3_model2.fit([gram3_train, gram3_train, gram3_train, gram3_train], author_train_hot, epochs=5, batch_size=32, \n                verbose=1, validation_split=0.2)\nt1 = time.time()","edb9e08a":"author_pred2 = gram3_model2.predict([gram3_test, gram3_test, gram3_test, gram3_test])\n\nt2 = time.time()\n\nauthor_pred2 = author_lb.inverse_transform(author_pred2)\n\naccuracy = accuracy_score(author_test, author_pred2)\nprecision, recall, f1, support=score(author_test, author_pred2)\nave_precision = np.average(precision, weights=support\/np.sum(support))\nave_recall = np.average(recall, weights=support\/np.sum(support))\nave_f1 = np.average(f1, weights=support\/np.sum(support))\nconfusion = confusion_matrix(author_test, author_pred2, labels=authors_names)\n    \nprint('Accurancy:', accuracy)\nprint('Average Precision:', ave_precision)\nprint('Average Recall:', ave_recall)\nprint('Average F1 Score:', ave_f1)\nprint('Learning time:', (t1 - t0), 'seconds')\nprint('Predict time:', (t2 - t1), 'seconds')\nprint('Confusion matrix:\\n', confusion)","02dbc0ec":"plot_confusion_matrix(confusion, classes=authors_names, \\\n                      normalize=True, title='Normalized confusion matrix - Model 2')\n\nplt.savefig('confusion_model2.png')","c1e9e15f":"plot_history_of_accurancy(gram3_model2_history)\nplt.savefig('accurancy_model2.png')","c967ecd6":"plot_history_of_loss(gram3_model2_history)\nplt.savefig('loss_model2.png')","81ea1377":"keras.utils.plot_model(gram3_model2, 'gram3_model2_arh.png')","6225823f":"accuracy_list = []\nprec_list = []\nrecall_list = []\nf1_list = []\n\nfor i in range(10):\n    author_pred3 = np.random.choice(authors_names, len(author_test))\n\n    accuracy = accuracy_score(author_test, author_pred3)\n    precision, recall, f1, support = score(author_test, author_pred3)\n    ave_precision = np.average(precision, weights = support\/np.sum(support))\n    ave_recall = np.average(recall, weights = support\/np.sum(support))\n    ave_f1 = np.average(f1, weights = support\/np.sum(support))\n    \n    accuracy_list.append(accuracy)\n    prec_list.append(ave_precision)\n    recall_list.append(ave_recall)\n    f1_list.append(ave_f1)\n\nprint('Accurancy:', accuracy_list, np.mean(accuracy_list), np.std(accuracy_list))\nprint('Average Precision:', prec_list, np.mean(prec_list), np.std(prec_list))\nprint('Average Recall:', recall_list, np.mean(recall_list), np.std(recall_list))\nprint('Average F1 Score:', f1_list, np.mean(f1_list), np.std(f1_list))","fddedb39":"for i in range(100):\n    print('Sentence', i, '- Right answer =', author_test[i],  'Model\\'s 1 predict =', author_pred1[i], \n         'Model\\'s 2 predict =', author_pred2[i])\n    print(text_test[i], '\\n')","84b18753":"def calculate_averages(true, pred, text):\n    \n    correct_len_chars = []\n    incorrect_len_chars = []\n    correct_len_words = []\n    incorrect_len_words = []\n\n    \n    for i in range(len(true)):\n        if true[i] == pred[i]:\n            correct_len_chars.append(len(text[i]))\n            correct_len_words.append(len(text[i].split()))\n        else:\n            incorrect_len_chars.append(len(text[i]))\n            incorrect_len_words.append(len(text[i].split()))\n    \n    correct_ave_chars = np.mean(correct_len_chars)\n    correct_ave_words = np.mean(correct_len_words)\n    incorrect_ave_chars = np.mean(incorrect_len_chars)\n    incorrect_ave_words = np.mean(incorrect_len_words)\n    \n    print('t-test for characters')\n    print(stats.ttest_ind(correct_len_chars, incorrect_len_chars, equal_var = False))\n    \n    print('\\nt-test for words')\n    print(stats.ttest_ind(correct_len_words, incorrect_len_words, equal_var = False))\n    \n    return correct_ave_chars, correct_ave_words, incorrect_ave_chars, incorrect_ave_words","a8247938":"correct_ave_chars1, correct_ave_words1, incorrect_ave_chars1, incorrect_ave_words1\\\n= calculate_averages(author_test, author_pred1, text_test)","507d441a":"correct_ave_chars2, correct_ave_words2, incorrect_ave_chars2, incorrect_ave_words2\\\n= calculate_averages(author_test, author_pred2, text_test)","c569f963":"print('Model 1 - Average length correct predicted sentences by characters =', correct_ave_chars1, \n        ', incorrect =', incorrect_ave_chars1)\nprint('Model 2 - Average length correct predicted sentences by characters =', correct_ave_chars2, \n      ', incorrect =', incorrect_ave_chars2)\n\nprint('\\nModel 1 - Average length correct predicted sentences by words =', correct_ave_words1, \n        ', incorrect =', incorrect_ave_words1)\nprint('Model 2 - Average length correct predicted sentences by words =', correct_ave_words2, \n      ', incorrect =', incorrect_ave_words2)","c13aac29":"If you are only interested in the classification model, then skip directly to step 3.","497fb2ff":"### Extremely short","164e238f":"## Preparing data for direct use","447b7ded":"# 2) Dataset preprocessing","62417500":"## Calculating statistics by words:","23f0ecec":"Let's look at some sample sentences:","8a409a15":"And create lists of n-grams:","0b027003":"## Section with statistics and output functions","67f18652":"And big blocks of indentation.","66047c57":"### Function for loading and preprocessing text","fcb38d01":"This notebook is a completely finished text classification work in the sense that this laptop contains all the steps necessary for analyzing and training the model, except for collecting raw data, and produces the result.","976b6831":"This part is needed to create a raw dataset for the task of classifying sentences by authorship from the original texts. The output will be a csv file in the format: \"sentence\", \"author\".","9240cf7c":"### Extremely long sentences","3b7f5873":"### Exporting the resulting dataset","1ae2643d":"Determine the size of the dictionary for n from 1 to 3 inclusive:","80feb85b":"### Packages import","aed38b5c":"## Data exploration","fedfbb8c":"### Combining sentences","4611d8c9":"## Exploring symbols","f941d452":"### Average length","841de912":"Number of sentences for each author:","5a06f744":"## Examining outliers in data","10ffb341":"Let's create a list of sentences, the length of which is more than 5 characters, since shorter ones, most likely, do not carry information useful for attribution. Generally speaking, these sentences can express, and express quite vividly, the writing style of a particular author; however, this is not used in the model.","c711c1ec":"## Overview","4339f77e":"### Word count","5af1f825":"This is exactly where we remove the unacceptable uninformative characters.","db1917aa":"To create a dataset, works are used (at start of this work):","1f15c7e8":"## Importing packages and loading pre-prepared data","30d9991c":"# 4*) Benchmarks and comparation","e4d7a2c4":"It is necessary to tokenize the offer, it is enough to call it on the working machine once:","10b52f0c":"### Create a sentence| set for each author","f6bc9b39":"### Create n-gram sequences","f1f8b450":"Teaching and learning models.","8af1cc4a":"Remove punctuation and convert all letters of the sentence to lowercase.","80e97aac":"## Vectorization","d549c532":"## Data preparation","f7cf490a":"### Character count","5aea0e31":"Each list contains 21'860 to 117'861 sentences. In order to have an even distribution of authors in our set, we will limit the set for each, for example, to 20'000 sentences.","a7c45eea":"## Calculations","d68e28d8":"## Importing packages and loading data","d8c47f0b":"Let's create a dictionary showing the number of dataset inclusions for each character.","29d3b660":"## Improvement","c7cd42df":"# NLP: classification task","55e763da":"## 3-gramm first model statistics","6c9da807":"The output of the quantity was needed for additional control over the data and their labels. Equality means that every sentence in our dataset will have a label (correct or incorrect - it should have been controlled before).","a19d2049":"\n|Author|Works|\n|---------  |-------|\n|\u0410.\u041f. \u0427\u0435\u0445\u043e\u0432 | Collection of stories |\n|\u0424.\u041c. \u0414\u043e\u0441\u0442\u043e\u0435\u0432\u0441\u043a\u0438\u0439| Collection of selected works |\n|\u041b.\u041d. \u0422\u043e\u043b\u0441\u0442\u043e\u0439| Most Popular Writings |","585f6947":"Retraining the trigram model with the addition of an additional channel.","8d2910b2":"### We select the training and test set","a5ed3845":"Each numbered part is essentially a stand-alone notebook. Thus, it is possible to divide this notebook into three parts.","08189eb6":"The trigram model showed the best results in terms of accuracy, so you should choose it as the main one.\n\nThe improved version should only be trained for 5 epochs, because the graph shows a plateau and even a decline in model accuracy after this point.","f85604d1":"## Model implementation","7958d163":"## Save the prepared data","f3c0d7e6":"# Analysys","6a6b7ead":"At this point, it is important to indicate the labels of the authors (their names) in the same order as in the previous step, otherwise the data will simply turn out to be incorrect. So far, a simple regulating mechanism does not come to mind.","24aea002":"All symbols used:","376896fd":"Preparing data for use in model training and explore it.","63d3f02a":"### Create a marked list","4ad3e98f":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 n-\u0433\u0440\u0430\u043c\u043c, \u0447\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u043e \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0441\u0435\u0442\u0438.","ae032cd4":"To improve the performance of the offer tokenizer, some character combinations are replaced. So, the replicas will be separate from the speech of the author in sentences, and the problem with quotes should be solved.","3a84a45d":"### Randomly shuffle the data","f481c93e":"https:\/\/machinelearningmastery.com\/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis\/","5f771517":"Based on the above research proposals, we can say that our data is quite suitable for analysis. The only thing is that you need to remove the indented blocks and some invalid characters that are artifacts of the original text.","60ef5415":"## Notes","62454adc":"Among them there are many that do not belong to the standard ones, such as punctuation or Cyrillic characters. Let's highlight those sentences in which they occur.","f23b009b":"# 1) Dataset creation","02bdae40":"## File creation"}}