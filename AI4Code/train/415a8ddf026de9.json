{"cell_type":{"9f9082f0":"code","d77652cf":"code","3e4c6f55":"code","bb98c05a":"code","08d86df0":"code","63ed09e0":"code","11fc2e8e":"code","513422f0":"code","c000c51c":"code","07b1dfa6":"code","b59cd1a4":"code","66ece2d7":"code","199c46fb":"code","0899c76d":"code","7075023f":"code","0eb155d0":"code","bf805269":"code","bb2572da":"code","3a543e53":"code","8d35e0d3":"code","cfc6b00d":"code","42547060":"code","dcf49dbc":"code","54ec8729":"code","c872bdbd":"code","73d393ce":"code","1bdcb1e6":"code","24dd9f68":"code","864dc0df":"code","baad0760":"code","3cc7c326":"code","2d811c6c":"code","fe9d8aad":"code","6dca28ab":"code","3a2b0da0":"code","ac546474":"markdown","7e4cca4f":"markdown","27eb31bd":"markdown","39881dd0":"markdown","7a3401fc":"markdown","641a49a5":"markdown","c5aabbd4":"markdown","c6acc3e3":"markdown","06198349":"markdown","2e58192c":"markdown","7cc47171":"markdown","1ef7d0b4":"markdown","4a3d4f1d":"markdown","9b4921e2":"markdown","eb085d2c":"markdown","0e536e9d":"markdown","57696713":"markdown"},"source":{"9f9082f0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","d77652cf":"train=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntrain","3e4c6f55":"test=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\ntest","bb98c05a":"train.info()","08d86df0":"train.isna().sum()","63ed09e0":"#% of null values\ndata_percentage=print('percentage of missing values'.upper())\ntrain.isna().sum()\/train.shape[0]*100","11fc2e8e":"for i in range(118):\n    train['f'+str(i+1)].fillna(train['f'+str(i+1)].mean(), inplace = True)","513422f0":"train.isna().sum()","c000c51c":"def skew_kurt(column, data = train):\n    sns.displot(x = column, data = data, kde = True)\n    skewness=str(data[column].skew())\n    kurtosis=str(data[column].kurt())\n    plt.legend([skewness,kurtosis],title=(\"skewness and kurtosis\"))\n    plt.show()\nskew_kurt('f1')    ","07b1dfa6":"def skew_kurt(column, data = train):\n    sns.displot(x = column, data = data, kde = True)\n    skewness=str(data[column].skew())\n    kurtosis=str(data[column].kurt())\n    plt.legend([skewness,kurtosis],title=(\"skewness and kurtosis\"))\n    plt.show()\nskew_kurt('f100')    ","b59cd1a4":"corr = train.corr()\nplt.figure(figsize=(30, 30))\nsns.heatmap(corr)\nplt.show()","66ece2d7":"x, y = train.drop('claim', axis=1) , train['claim']","199c46fb":"def finding_correlation(data, threshold):\n    correlated_columns = set()\n    correlation_matrix = X.corr()\n    for i in range(correlation_matrix.shape[0]):\n        for j in range(i):\n            if abs(correlation_matrix.iloc[i,j]) > threshold:\n                column_name = correlation_matrix.columns[i]\n                correlated_columns.add(column_name)\n    return correlated_columns","0899c76d":"# copy of datasets\nfrom sklearn.preprocessing import StandardScaler\n\nx_train_stand =  train.iloc[:,1:119]\nx_test_stand  = test.drop (['id'], axis=1)\nnum_cols = x_train_stand.columns\nfor i in num_cols:\n    scale = StandardScaler()\n    x_train_stand[i] = scale.fit_transform(x_train_stand[[i]])\n    x_test_stand[i] = scale.transform(x_test_stand[[i]])\n    \n# adding claim column back \nx_train_stand[\"claim\"] = train[\"claim\"]","7075023f":"X = train.drop('claim', axis=1)\ny = train['claim']","0eb155d0":"#splitting the dataset into train and test set.\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 31)","bf805269":"len(x_train), len(x_test), len(y_train), len(y_test)","bb2572da":"x_train.shape, y_train.shape","3a543e53":"from sklearn.feature_selection import VarianceThreshold, mutual_info_classif, SelectKBest\nfrom sklearn.ensemble import ExtraTreesClassifier","8d35e0d3":"var_threshold = VarianceThreshold(threshold=0)\nvar_threshold.fit(x_train)","cfc6b00d":"mutual_info = mutual_info_classif(x_train, y_train)\n\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = x_train.columns\nmutual_info.sort_values(ascending=False)","42547060":"select_k_features  = SelectKBest(mutual_info_classif, k=50)\nselect_k_features.fit(x_train, y_train)\nselect_k_features","dcf49dbc":"x_train.columns[select_k_features.get_support()]","54ec8729":"from sklearn.ensemble import ExtraTreesClassifier","c872bdbd":"from sklearn.ensemble import ExtraTreesClassifier\ntree_classifier = ExtraTreesClassifier()\ntree_classifier.fit(x_train, y_train)","73d393ce":"tree_classifier.feature_importances_","1bdcb1e6":"feature_importance = pd.Series(tree_classifier.feature_importances_)\nfeature_importance","24dd9f68":"from sklearn.naive_bayes import GaussianNB\nmodel_nb = GaussianNB()\nmodel_nb.fit(x_train,y_train)\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\ny_pred = model_nb.predict(x_test)\nprint(accuracy_score(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","864dc0df":"from sklearn.linear_model import LogisticRegression\nsimple_regressor = LogisticRegression(random_state=0)\nsimple_regressor.get_params()","baad0760":"%%time\nsimple_regressor.fit(x_train,y_train)","3cc7c326":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\ny_pred =simple_regressor.predict(x_test)\nprint(accuracy_score(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","2d811c6c":"predictes_for_test = simple_regressor.predict_proba(x_test)","fe9d8aad":"import pandas as pd\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub.claim = predictes_for_test\nsub","6dca28ab":"sub.to_csv('submission.csv', index=False)","3a2b0da0":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions[\"claim\"] =predictes_for_test\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","ac546474":"# Load The Dataset","7e4cca4f":"Correlation","27eb31bd":"# Logistic Regression","39881dd0":"# Import the libraries","7a3401fc":"Feature Selection","641a49a5":"FEATURE SELECTION USING MUTUAL INFO CLASSIFIER","c5aabbd4":"Removing columns with low varience","c6acc3e3":"# GAUSSIAN NB","06198349":"Null values","2e58192c":"# Feature Transformation","7cc47171":"# Splitting the dataset","1ef7d0b4":"Feature selection by Extratreeclassifier","4a3d4f1d":"# SUBMISSION","9b4921e2":"# EDA","eb085d2c":"Skewness and Kurtosis","0e536e9d":"# Model Building Algorithms","57696713":"from tqdm import tqdm\nplt.figure(figsize=(24, 6*(118\/4)))\nfor i in tqdm(range(len(train.columns.tolist())-1)):\n    plt.subplot(30, 4, i+1)\n    sns.histplot(train[f'f{i+1}'], kde=True)\n    sns.histplot(test[f'f{i+1}'], kde=True, color='red')\nplt.show()"}}