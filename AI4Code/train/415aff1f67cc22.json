{"cell_type":{"acf1cc6c":"code","f51c64cb":"code","47323dbd":"code","da7156a2":"code","d78e914f":"code","0c5fd966":"code","0d534e35":"code","b4fd7e72":"code","9b448f76":"code","2a219a45":"code","25468f86":"code","a96e9719":"code","c70e3684":"code","d506a57c":"code","5886122b":"code","198d15ad":"code","68c77cae":"code","cdfb57fd":"code","50c8405d":"code","4cac6b0a":"code","0d97ab5f":"code","03b59eb2":"code","0ed66187":"code","84da3122":"code","f174e9d3":"code","75944e16":"code","52aa6267":"code","5362402e":"code","04b1cae5":"code","f968bf52":"code","740b68de":"code","d20f0b69":"code","a43b3741":"code","637aed58":"code","cdc639fb":"code","0fd76ec4":"code","92039761":"code","c9d07237":"code","7c07289a":"code","f157017c":"code","3fe1982e":"code","85bb8f85":"code","e2743d98":"code","88cd1162":"code","831ea2d8":"code","9cb751c4":"code","8e8589c2":"code","81afd3dd":"code","cddb31f2":"code","c0bc84d9":"code","40cd5dca":"code","da8ef6de":"code","8a24efa0":"code","c4ba8c56":"code","31ad4186":"code","ed42ec9d":"code","4f98d0c2":"code","6d15ddd9":"code","4ca64780":"code","d08537ad":"code","f0fd0f6a":"code","8dfaee52":"code","381d1a8c":"code","87b67abe":"code","6343423e":"code","f4bef829":"code","b93f07c7":"code","4eec1933":"code","36400a51":"code","351545b3":"code","4e189a2b":"code","22a6d5de":"code","c458fe01":"code","ef2d9284":"code","e892b37f":"code","e1be6308":"code","135446a3":"code","5e0b1d93":"code","5d599327":"code","f9332346":"code","f54ee154":"code","b710050c":"code","37b8e2fd":"code","5ebbda2e":"code","d15af411":"code","2690734a":"code","165514a7":"code","c8deac94":"code","6befef62":"code","1073d2e9":"code","0e82e922":"code","ca005484":"code","7bf75b75":"code","8700491a":"code","05786085":"code","29f0aba6":"code","86f6a674":"code","76ef4fa4":"code","203118dc":"code","c125adbe":"code","d1ae7116":"code","c7ca7b8b":"code","b70e9421":"code","f526bb3b":"code","37d9033a":"code","d6663c72":"code","d42234ee":"code","6c3a5090":"code","067259ec":"code","ebc3f38f":"code","84acfe21":"code","d1bd3664":"code","f2afed33":"code","01853eab":"code","7a74cc5c":"code","e3243ca3":"code","0dc57be0":"code","db2da4e9":"code","3fa677e1":"code","9d3c15ba":"code","5b1df5ae":"code","ca9c2993":"code","143c641a":"code","d318704c":"code","ca88e705":"code","5fb1864c":"code","5260aa27":"code","28c9b46c":"code","6879ee87":"code","0e345cb8":"code","d59c83a8":"code","0c7daed2":"code","51bbb08a":"code","297b876d":"code","1e8b281d":"code","971b8a8f":"code","9203e15e":"code","2b18f626":"code","79c5aa73":"code","f0fa405a":"code","958f7a14":"code","2938d425":"code","f227a82f":"code","bbab08ec":"code","8f07c334":"code","c967652d":"code","68e44a1a":"code","c847f8fd":"code","0a0aa3d5":"code","8f616cd8":"code","dd12a4d0":"code","73c046b4":"code","51afbe3f":"code","44e47b08":"code","a82081f5":"code","12060b6c":"code","2deadf81":"code","10f73c4d":"code","e4c96252":"code","ac9369d6":"code","df77da4a":"code","e0ff84aa":"code","86d60a6c":"code","9bafe4c0":"code","e9bbf117":"code","21fbb889":"code","1f906a85":"code","41e41d78":"code","4fb9cf70":"code","20f469f4":"code","e17a7a0d":"code","7c9dd3f2":"code","4769f2b1":"code","05a4be94":"code","8be0e4fc":"code","b94963eb":"code","a720712c":"code","eb3ab7fc":"code","717c4a94":"code","e36d965d":"code","8cfa9ded":"code","9cc272a5":"code","31811f76":"code","855d6c3f":"code","e6943e7b":"code","3bb7e2d7":"code","b0cec841":"code","f3e78187":"code","9bab8b06":"code","7325e184":"code","a0b69577":"code","f6286e2b":"code","3cc9e93d":"code","8b9a5ce3":"code","2bde8dd3":"code","826bc42a":"code","5e9baf01":"code","89fd0fb7":"code","be17eed2":"code","98e97745":"code","1c9cbba0":"code","c033e9c5":"code","85cc9fed":"code","5c9ad03d":"code","e121610b":"code","5e357497":"code","c6231504":"code","ff933b9e":"markdown","3ba03a9b":"markdown","6f0cfcdd":"markdown","acbc3e75":"markdown","80461da3":"markdown","c84018de":"markdown","ef1e6e7e":"markdown","6405ba66":"markdown","01dd559a":"markdown","c45faa86":"markdown","bd96172e":"markdown","69465a17":"markdown","6172fa91":"markdown","f21a67b0":"markdown","7ff75507":"markdown","14d6deb5":"markdown","f44c1bc4":"markdown","6611d27a":"markdown","9808c382":"markdown","db827bb3":"markdown","409a2994":"markdown","4286f62a":"markdown","02f67c1b":"markdown","c90dde52":"markdown","9302abb0":"markdown","a67e416f":"markdown","4ac52491":"markdown","64445ce8":"markdown","0ef6c6c7":"markdown","d8fdee6e":"markdown","92316a66":"markdown","7fb87921":"markdown","1b8e9a62":"markdown","2d0c2319":"markdown","8ac125b0":"markdown","688449b4":"markdown","598b0758":"markdown","18e5b91a":"markdown","7bfe21a4":"markdown","99ccf63b":"markdown","e1324477":"markdown","46174b4b":"markdown"},"source":{"acf1cc6c":"# Install libraries\n!pip install boostaroota\n!pip install h2o\n## !pip3 install fastf1 --user","f51c64cb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nfrom sklearn import model_selection as sk_model_selection\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.metrics import mean_squared_error,roc_auc_score,precision_score\nfrom sklearn import metrics\nimport optuna\nfrom boostaroota import BoostARoota\nfrom sklearn.metrics import log_loss\nfrom optuna.samplers import TPESampler\nimport functools\nfrom functools import partial\nimport xgboost as xgb\nimport joblib\nfrom matplotlib_venn import venn2, venn2_circles, venn2_unweighted\nfrom matplotlib_venn import venn3, venn3_circles\nimport statsmodels.api as sm\nimport pylab\nfrom xgboost import plot_tree\nimport shap\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import LabelEncoder\nimport h2o\nfrom h2o.automl import H2OAutoML\nfrom catboost import Pool, CatBoostRegressor, cv\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, LSTM, GRU\nfrom tensorflow.keras.layers import Bidirectional, Multiply\n\nSEED = 42","47323dbd":"train_data = pd.read_csv('..\/input\/envision-racing\/train.csv')\ntrain_weather_data = pd.read_csv('..\/input\/envision-racing\/train_weather.csv')\ntest_data = pd.read_csv('..\/input\/envision-racing\/test.csv')\ntest_weather_data = pd.read_csv('..\/input\/envision-racing\/test_weather.csv')\nsubmission_data = pd.read_csv('..\/input\/envision-racing\/submission.csv')","da7156a2":"pd.set_option('display.max_columns',None)\nprint(train_data.shape)\nprint(train_data.columns)\ntrain_data.head()","d78e914f":"train_data.dtypes","0c5fd966":"print(train_weather_data.shape)\nprint(train_weather_data.columns)\ntrain_weather_data.head()","0d534e35":"print(test_data.shape)\nprint(test_data.columns)\ntest_data.head()","b4fd7e72":"set_numbers_train = set(train_data[['NUMBER']].drop_duplicates().sort_values(by = 'NUMBER')['NUMBER'].tolist())\nset_numbers_test = set(test_data[['NUMBER']].drop_duplicates().sort_values(by = 'NUMBER')['NUMBER'].tolist())\nvenn2((set_numbers_train, set_numbers_test), set_labels = ('Train numbers', 'Test numbers'))","9b448f76":"display(train_data[[' LAP_NUMBER']].drop_duplicates())\ntest_data[[' LAP_NUMBER']].drop_duplicates()","2a219a45":"display(train_data.groupby([' LAP_NUMBER']).agg({'LOCATION':'nunique'}).reset_index().head())\ndisplay(test_data.groupby([' LAP_NUMBER']).agg({'LOCATION':'nunique'}).reset_index())","25468f86":"display(train_data[['GROUP']].drop_duplicates())\ntest_data[['GROUP']].drop_duplicates()","a96e9719":"set_teams_train = set(train_data[['TEAM']].drop_duplicates().sort_values(by = 'TEAM')['TEAM'].tolist())\nset_teams_test = set(test_data[['TEAM']].drop_duplicates().sort_values(by = 'TEAM')['TEAM'].tolist())\nvenn2((set_teams_train, set_teams_test), set_labels = ('Train teams', 'Test teams'))\nset_teams_train - set_teams_test","c70e3684":"display(train_data[[' DRIVER_NUMBER']].drop_duplicates())\ntest_data[[' DRIVER_NUMBER']].drop_duplicates()","d506a57c":"set_DRIVER_NAMEs_train = set(train_data[['DRIVER_NAME']].drop_duplicates().sort_values(by = 'DRIVER_NAME')['DRIVER_NAME'].tolist())\nset_DRIVER_NAMEs_test = set(test_data[['DRIVER_NAME']].drop_duplicates().sort_values(by = 'DRIVER_NAME')['DRIVER_NAME'].tolist())\nvenn2((set_DRIVER_NAMEs_train, set_DRIVER_NAMEs_test), set_labels = ('Train DRIVER_NAMEs', 'Test DRIVER_NAMEs'))\nset_DRIVER_NAMEs_train - set_DRIVER_NAMEs_test","5886122b":"display(train_data.groupby(['DRIVER_NAME']).agg({'TEAM':'nunique'}).reset_index())\ntest_data.groupby(['DRIVER_NAME']).agg({'TEAM':'nunique'}).reset_index()","198d15ad":"display(train_data[['LOCATION']].drop_duplicates())\ntest_data[['LOCATION']].drop_duplicates()","68c77cae":"display(train_data[['EVENT']].drop_duplicates())\ntest_data[['EVENT']].drop_duplicates()","cdfb57fd":"print(test_weather_data.shape)\nprint(test_weather_data.columns)\ntest_weather_data.head()","50c8405d":"display(train_weather_data[['RAIN','LOCATION']].drop_duplicates())\ntest_weather_data[['RAIN','LOCATION']].drop_duplicates()","4cac6b0a":"test_weather_data = test_weather_data.rename(columns = {'EVENTS':'EVENT'})\ndisplay(train_weather_data[['EVENT']].drop_duplicates())\ntest_weather_data[['EVENT']].drop_duplicates()","0d97ab5f":"print(submission_data.shape)\nprint(submission_data.columns)\nsubmission_data.head()","03b59eb2":"print('Number of rows:',train_data.shape[0])\n(train_data.isnull().sum().sort_values(ascending = False)\/train_data.shape[0] * 100)","0ed66187":"print('Number of rows:',test_data.shape[0])\n(test_data.isnull().sum().sort_values(ascending = False)\/test_data.shape[0] * 100)","84da3122":"print('Number of rows:',train_weather_data.shape[0])\n(train_weather_data.isnull().sum().sort_values(ascending = False)\/train_weather_data.shape[0] * 100)","f174e9d3":"print('Number of rows:',test_weather_data.shape[0])\n(test_weather_data.isnull().sum().sort_values(ascending = False)\/test_weather_data.shape[0] * 100)","75944e16":"for col in [' ELAPSED', ' HOUR', 'S1_LARGE', 'S2_LARGE', 'S3_LARGE','PIT_TIME']:\n    train_data[col + '_seconds'] = train_data[col].apply(lambda x: float(x.split(':')[0]) * 60 + float(x.split(':')[1]) if not pd.isnull(x) else x)\n    test_data[col + '_seconds'] = test_data[col].apply(lambda x: float(x.split(':')[0]) * 60 + float(x.split(':')[1]) if not pd.isnull(x) else x)\n    \ntrain_data[[' ELAPSED', ' HOUR', 'S1_LARGE', 'S2_LARGE', 'S3_LARGE', 'PIT_TIME', ' ELAPSED' + '_seconds', ' HOUR' + '_seconds', 'S1_LARGE' + '_seconds', 'S2_LARGE' + '_seconds', 'S3_LARGE' + '_seconds', 'PIT_TIME' + '_seconds']].head()","52aa6267":"train_data.dtypes","5362402e":"def box_plot_by_metadata(metadata, numerical_col):\n    print('\\n# Unique values in',numerical_col,':',train_data[numerical_col].nunique(),'\\n')\n    display(train_data[numerical_col].describe())\n    if metadata == 'All':\n        fig = px.box(train_data, y=numerical_col)\n    else:\n        fig = px.box(train_data, y=numerical_col, x = metadata)\n    fig.show()\n    \nmetadata_list = ['All'] + ['GROUP','TEAM','LOCATION','EVENT','DRIVER_NAME','TEAM']\nnumerical_col_list = ['LAP_TIME','NUMBER',' LAP_NUMBER',' LAP_IMPROVEMENT','POWER']\n\nw = widgets.interactive(box_plot_by_metadata, metadata = metadata_list, numerical_col = numerical_col_list)\ndisplay(w)","04b1cae5":"print('# Records with 0 lap time:',train_data[(train_data['LAP_TIME']==0)].shape[0])\ndisplay(train_data[(train_data['LAP_TIME']==0)].groupby(['EVENT']).size().reset_index().rename(columns = {0:'# Records with 0 lap time'}))\nprint('\\nQualifying event with 0 lap time')\ntrain_data[(train_data['LAP_TIME']==0) & (train_data['EVENT'].str.contains('Qualifying'))]","f968bf52":"train_data[[' LAP_NUMBER', 'LAP_TIME',' LAP_IMPROVEMENT']].head()","740b68de":"train_data[train_data[' LAP_IMPROVEMENT']>0][[' LAP_NUMBER', 'LAP_TIME',' LAP_IMPROVEMENT']][' LAP_NUMBER'].unique()","d20f0b69":"train_data[['NUMBER',' LAP_NUMBER', 'TEAM', 'LOCATION', 'EVENT',]].drop_duplicates().shape, train_data.shape","a43b3741":"train_data.groupby(['NUMBER',' LAP_NUMBER', 'TEAM', 'LOCATION', 'EVENT',]).size().sort_values(ascending = False).head()","637aed58":"train_data[(train_data['NUMBER']==29) & (train_data[' LAP_NUMBER']==4) & (train_data['LOCATION']=='Location 8') & (train_data['EVENT']=='Free Practice 2')]","cdc639fb":"train_data['Key'] = train_data['NUMBER'].astype(str) + '_' + train_data[' LAP_NUMBER'].astype(str) + '_' + train_data['LOCATION'] + '_' + train_data['EVENT']\ntemp = train_data.groupby(['Key']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False)\nlist_keys_with_multiple_records = temp[temp['# Records']>1]['Key'].unique().tolist()\ntrain_data[(train_data['Key'].isin(list_keys_with_multiple_records)) & (train_data['EVENT'].str.contains('Qualifying'))].sort_values(by = 'Key').head()","0fd76ec4":"temp.head()","92039761":"# Check level of test data\n\ntest_data['Key'] = test_data['NUMBER'].astype(str) + '_' + test_data[' LAP_NUMBER'].astype(str) + '_' + test_data['LOCATION'] + '_' + test_data['EVENT']\ntemp = test_data.groupby(['Key']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False)\nlist_keys_with_multiple_records = temp[temp['# Records']>1]['Key'].unique().tolist()\ntest_data[(test_data['Key'].isin(list_keys_with_multiple_records)) & (test_data['EVENT'].str.contains('Qualifying'))].sort_values(by = 'Key').head()","c9d07237":"train_data.groupby(['DRIVER_NAME']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False).head(10)","7c07289a":"test_data.groupby(['DRIVER_NAME']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False).head(10)","f157017c":"train_data.groupby(['TEAM']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False).head(10)","3fe1982e":"test_data.groupby(['TEAM']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False).head(10)","85bb8f85":"train_data.groupby(['EVENT']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False).head(10)","e2743d98":"test_data.groupby(['EVENT']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False).head(10)","88cd1162":"train_data.groupby(['GROUP']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False).head(10)","831ea2d8":"test_data.groupby(['GROUP']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False).head(10)","9cb751c4":"train_data.columns","8e8589c2":"## Sectors in a race\ntrain_data[[' S1', ' S1_IMPROVEMENT', ' S2', ' S2_IMPROVEMENT', ' S3', ' S3_IMPROVEMENT', 'S1_LARGE', 'S2_LARGE', 'S3_LARGE', ' ELAPSED', ' HOUR']].head()","81afd3dd":"test_data[(test_data['NUMBER']==48) & (test_data['EVENT']=='Qualifying Group 2') & (test_data['LOCATION']=='Location 8')][['LAP_TIME',' ELAPSED',' HOUR','S1_LARGE','S2_LARGE','S3_LARGE','PIT_TIME']]","cddb31f2":"train_data[(train_data['NUMBER']==48) & (train_data['EVENT']=='Free Practice 2') & (train_data['LOCATION']=='Location 2')][['LAP_TIME',' ELAPSED',' HOUR','S1_LARGE','S2_LARGE','S3_LARGE','PIT_TIME']]","c0bc84d9":"train_data[(train_data[' CROSSING_FINISH_LINE_IN_PIT'].notna()) & (train_data[' LAP_NUMBER']==1)].groupby(['Key']).size()","40cd5dca":"test_data[(test_data[' CROSSING_FINISH_LINE_IN_PIT'].notna())].groupby(['Key']).size()","da8ef6de":"train_data[(train_data['NUMBER']==4) & (train_data['EVENT']=='Qualifying Group 1') & (train_data['LOCATION']=='Location 2')][['LAP_TIME',' LAP_NUMBER',' LAP_IMPROVEMENT',' ELAPSED',' HOUR','S1_LARGE','S2_LARGE','S3_LARGE','PIT_TIME']]","8a24efa0":"train_data[train_data[' LAP_IMPROVEMENT']>=4].head()","c4ba8c56":"train_weather_data['Date'] = train_weather_data['TIME_UTC_STR'].apply(lambda x: x.split(' ')[0])\ntrain_weather_data['Time'] = train_weather_data['TIME_UTC_STR'].apply(lambda x: x.split(' ')[1])\ntrain_weather_data['Hour'] = train_weather_data['Time'].apply(lambda x: x.split(':')[0])\ntrain_weather_data['Minute'] = train_weather_data['Time'].apply(lambda x: x.split(':')[1])\ntrain_weather_data['Hour'].unique()","31ad4186":"test_weather_data['Date'] = test_weather_data['TIME_UTC_STR'].apply(lambda x: x.split(' ')[0])\ntest_weather_data['Time'] = test_weather_data['TIME_UTC_STR'].apply(lambda x: x.split(' ')[1])\ntest_weather_data['Hour'] = test_weather_data['Time'].apply(lambda x: x.split(':')[0])\ntest_weather_data['Minute'] = test_weather_data['Time'].apply(lambda x: x.split(':')[1])\ntest_weather_data['Hour'].unique()","ed42ec9d":"train_weather_data.head()","4f98d0c2":"train_weather_data.groupby(['LOCATION','EVENT']).agg({'Date':['nunique','unique']})","6d15ddd9":"test_weather_data.groupby(['LOCATION','EVENT']).agg({'Date':['nunique','unique']})","4ca64780":"# Commas and vales of str type found\ntrain_weather_data.iloc[223:225,:]","d08537ad":"def fix_commas(x):\n    if x.count(',')==1:\n        x = x.replace(',','.')\n    if x.count(',')==2:\n        x = np.NaN\n#         x = x.replace(\",\", \"\", 1)\n#         x = x.replace(',','.')\n    return x\n    \nfor col in ['AIR_TEMP','TRACK_TEMP','HUMIDITY','PRESSURE','WIND_SPEED','WIND_DIRECTION','RAIN']:\n    train_weather_data[col] = train_weather_data[col].apply(lambda x: fix_commas(x) if type(x) == str else x)\n    train_weather_data[col] = train_weather_data[col].astype(float)\n    test_weather_data[col] = test_weather_data[col].apply(lambda x: fix_commas(x) if type(x) == str else x)\n    test_weather_data[col] = test_weather_data[col].astype(float)","f0fd0f6a":"master_weather_data = pd.concat([train_weather_data, test_weather_data], axis = 0).reset_index(drop = True)\nprint(master_weather_data.shape)","8dfaee52":"master_weather_data[['EVENT','Date']].drop_duplicates().sort_values(by = ['EVENT','Date'], ascending = True)","381d1a8c":"master_agg_weather_data = master_weather_data.groupby(['LOCATION','EVENT','Hour']).agg({'AIR_TEMP':'mean','TRACK_TEMP':'mean','HUMIDITY':'mean','PRESSURE':'mean','WIND_SPEED':'mean','WIND_DIRECTION':'mean','RAIN':'mean'}).reset_index()\nmaster_agg_weather_data = master_agg_weather_data.groupby(['LOCATION','EVENT']).agg({'AIR_TEMP':'mean','TRACK_TEMP':'mean','HUMIDITY':'mean','PRESSURE':'mean','WIND_SPEED':'mean','WIND_DIRECTION':'mean','RAIN':'mean'}).reset_index()\nmaster_agg_weather_data.head()","87b67abe":"master_agg_weather_data_min = master_weather_data.groupby(['LOCATION','EVENT','Hour']).agg({'AIR_TEMP':'min','TRACK_TEMP':'min','HUMIDITY':'min','PRESSURE':'min','WIND_SPEED':'min','WIND_DIRECTION':'min','RAIN':'min'}).reset_index()\nmaster_agg_weather_data_min = master_agg_weather_data_min.groupby(['LOCATION','EVENT']).agg({'AIR_TEMP':'min','TRACK_TEMP':'min','HUMIDITY':'min','PRESSURE':'min','WIND_SPEED':'min','WIND_DIRECTION':'min','RAIN':'min'}).reset_index()\nfor col in master_agg_weather_data_min.columns:\n    if col not in ['LOCATION','EVENT']:\n        master_agg_weather_data_min = master_agg_weather_data_min.rename(columns = {col:col+\"_min\"})\nmaster_agg_weather_data_min.head()","6343423e":"master_agg_weather_data_max = master_weather_data.groupby(['LOCATION','EVENT','Hour']).agg({'AIR_TEMP':'max','TRACK_TEMP':'max','HUMIDITY':'max','PRESSURE':'max','WIND_SPEED':'max','WIND_DIRECTION':'max','RAIN':'max'}).reset_index()\nmaster_agg_weather_data_max = master_agg_weather_data_max.groupby(['LOCATION','EVENT']).agg({'AIR_TEMP':'max','TRACK_TEMP':'max','HUMIDITY':'max','PRESSURE':'max','WIND_SPEED':'max','WIND_DIRECTION':'max','RAIN':'max'}).reset_index()\nfor col in master_agg_weather_data_max.columns:\n    if col not in ['LOCATION','EVENT']:\n        master_agg_weather_data_max = master_agg_weather_data_max.rename(columns = {col:col+\"_max\"})\nmaster_agg_weather_data_max.head()","f4bef829":"print(master_agg_weather_data.shape)\nmaster_agg_weather_data = master_agg_weather_data.merge(master_agg_weather_data_min, how = 'left', on = ['LOCATION','EVENT'])\nmaster_agg_weather_data = master_agg_weather_data.merge(master_agg_weather_data_max, how = 'left', on = ['LOCATION','EVENT'])\nprint(master_agg_weather_data.shape)","b93f07c7":"master_agg_weather_data.columns","4eec1933":"master_agg_weather_data.head()","36400a51":"print(train_data.shape)\nprint(test_data.shape)\ntrain_data = train_data.merge(master_agg_weather_data, how = 'left', on = ['LOCATION','EVENT'])\ntest_data = test_data.merge(master_agg_weather_data, how = 'left', on = ['LOCATION','EVENT'])\nprint(train_data.shape)\nprint(test_data.shape)","351545b3":"metadata_list = ['AIR_TEMP',\n 'TRACK_TEMP',\n 'HUMIDITY',\n 'PRESSURE',\n 'WIND_SPEED',\n 'WIND_DIRECTION',\n 'RAIN']\n\n# % Nulls\ntrain_data[metadata_list].isnull().sum()\/train_data.shape[0]*10","4e189a2b":"train_data[metadata_list].describe()","22a6d5de":"def box_plot_by_weather_data(metadata, numerical_col):\n    print('\\n# Unique values in',numerical_col,':',train_data[numerical_col].nunique(),'\\n')\n    display(train_data[numerical_col].describe())\n    if metadata == 'All':\n        fig = px.box(train_data, y=numerical_col)\n    else:\n        train_data['Weather metadata category'] = pd.qcut(train_data[metadata],3, labels = ['Low','Medium','High'])\n        fig = px.box(train_data, y=numerical_col, x = 'Weather metadata category')\n    fig.show()\n    \nmetadata_list = ['All'] + ['AIR_TEMP',\n 'TRACK_TEMP',\n 'HUMIDITY',\n 'PRESSURE',\n 'WIND_SPEED',\n 'WIND_DIRECTION',\n 'RAIN']\n\nnumerical_col_list = ['LAP_TIME','NUMBER',' LAP_NUMBER',' LAP_IMPROVEMENT','POWER']\n\nw = widgets.interactive(box_plot_by_weather_data, metadata = metadata_list, numerical_col = numerical_col_list)\ndisplay(w)","c458fe01":"corr = train_data[numerical_col_list + [' ELAPSED_seconds',\n       ' HOUR_seconds', 'S1_LARGE_seconds', 'S2_LARGE_seconds',\n       'S3_LARGE_seconds']].corr()\ncorr.style.background_gradient(cmap='coolwarm', axis=None)","ef2d9284":"sm.qqplot(train_data['LAP_TIME'], line='45')\npylab.show()","e892b37f":"test_data['EVENT'].unique()","e1be6308":"train_data['EVENT'].unique()","135446a3":"test_data['RAIN'].unique()","5e0b1d93":"test_data['LOCATION'].unique()","5d599327":"# remove specific rows\ntrain_data['filter'] = 1\n# train_data['filter'] = np.where(train_data['LOCATION'].isin(['Location 7', 'Location 6', 'Location 8']), 1, 0)\ntrain_data['filter'] = np.where(((train_data['LAP_TIME']==0) & (train_data['EVENT'].str.contains('Qualifying'))), 0, train_data['filter'])\n# Remove very small lap times\ntrain_data['filter'] = np.where(train_data['LAP_TIME']<30, 0, train_data['filter'])\ntrain_data['filter'].sum(), train_data['filter'].sum()\/train_data['filter'].shape[0] * 100","f9332346":"train_data[train_data['filter']==1]['LAP_TIME'].min()","f54ee154":"train_data[train_data['filter']==1]['EVENT'].unique()","b710050c":"train_data[train_data['filter']==1]['LOCATION'].unique()","37b8e2fd":"test_data['LOCATION'].unique()","5ebbda2e":"def objective_regressor(X_train, y_train, X_val, y_val, target_value, trial):\n    \"\"\"It tries to find the best hyper-parameters for XGBOOST model for given task\n\n        Details:\n            It uses OPTUNA library which is based on Baseian-optimization to tune the hyper-params.\n\n        Args:\n            X_train: training data\n            X_test: testing data\n            y_tain: training label\n            y_val: validation label\n            trail: object of optuna for optimizing the task in hand\n\n        Returns:\n            best score till now\n\n    \"\"\"\n    if ((target_value)):\n        tree_methods = ['approx', 'hist', 'exact']\n#         tree_methods = ['gpu_hist']\n        boosting_lists = ['gbtree', 'gblinear']\n        objective_list_reg = ['reg:squarederror']  # 'reg:gamma', 'reg:tweedie'\n        boosting = trial.suggest_categorical('boosting', boosting_lists),\n        tree_method = trial.suggest_categorical('tree_method', tree_methods),\n        n_estimator = trial.suggest_int('n_estimators',20, 300, 10),\n        max_depth = trial.suggest_int('max_depth', 2, 100),\n        reg_alpha = trial.suggest_int('reg_alpha', 1,10),\n        reg_lambda = trial.suggest_int('reg_lambda', 1,10),\n        min_child_weight = trial.suggest_int('min_child_weight', 1,5),\n        gamma = trial.suggest_int('gamma', 1, 5),\n        learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n        objective = trial.suggest_categorical('objective', objective_list_reg),\n        colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree', 0.8, 1, 0.05),\n        colsample_bynode = trial.suggest_discrete_uniform('colsample_bynode', 0.8, 1, 0.05),\n        colsample_bylevel = trial.suggest_discrete_uniform('colsample_bylevel', 0.8, 1, 0.05),\n        subsample = trial.suggest_discrete_uniform('subsample', 0.7, 1, 0.05),\n        nthread = -1\n        \n        \n    xgboost_tune = xgb.XGBRegressor(\n        tree_method=tree_method[0],\n        boosting=boosting[0],\n        reg_alpha=reg_alpha[0],\n        reg_lambda=reg_lambda[0],\n        gamma=gamma[0],\n        objective=objective[0],\n        colsample_bynode=colsample_bynode[0],\n        colsample_bylevel=colsample_bylevel[0],\n        n_estimators=n_estimator[0],\n        max_depth=max_depth[0],\n        min_child_weight=min_child_weight[0],\n        learning_rate=learning_rate[0],\n        subsample=subsample[0],\n        colsample_bytree=colsample_bytree[0],\n        eval_metric='rmsle',\n        n_jobs=nthread,\n        random_state=SEED)\n    \n    xgboost_tune.fit(X_train, y_train)\n    pred_val = xgboost_tune.predict(X_val)\n    \n    return np.sqrt(mean_squared_log_error(y_val, pred_val))","d15af411":"y_true = [3, 5, 2.5, 7]\ny_pred = [2.5, 5, 4, 8]\nnp.sqrt(mean_squared_log_error(y_true, y_pred))","2690734a":"display(train_data['LAP_TIME'].describe())\nlabels = [1,2,3,4,5]\ntrain_data['LAP_TIME bin'] = pd.qcut(train_data['LAP_TIME'], 5, labels=labels)\ntrain_data['LOCATION 6_7_8'] = np.where(train_data['LOCATION'].isin(['Location 7', 'Location 6', 'Location 8']), 1, 0)\ntrain_data['LAP_TIME bin_loc678'] = train_data['LAP_TIME bin'].astype(str) + '_' + train_data['LOCATION 6_7_8'].astype(str)\n\nprint('Train + Valid')\ndisplay(train_data.groupby(['LAP_TIME bin_loc678']).size().reset_index())\n\ndf_train, df_valid = sk_model_selection.train_test_split(\n    train_data[train_data['filter']==1], \n    test_size=0.1, \n    random_state=42,\n    stratify = train_data[train_data['filter']==1]['LAP_TIME bin_loc678'])\n\nprint('Train')\ndisplay(df_train.groupby(['LAP_TIME bin_loc678']).size().reset_index())\nprint('Validation')\ndisplay(df_valid.groupby(['LAP_TIME bin_loc678']).size().reset_index())","165514a7":"train_indices = list(df_train.index)\nvalid_indices = list(df_valid.index)\nlen(train_indices) + len(valid_indices)","c8deac94":"train_data.head()","6befef62":"train_data['# Times in same location for driver'] = np.NaN\ntrain_data['# Times in same location and event for driver'] = np.NaN\n# train_data['# Times in same location and event and lap for driver'] = np.NaN\n\ntrain_data['Average lap time in same location for driver'] = np.NaN\ntrain_data['Average lap time in same location and event for driver'] = np.NaN\n# train_data['Average lap time in same location and event and lap for driver'] = np.NaN\ntrain_data['SL_NO'] = train_data.index + 1\n\ntest_data['# Times in same location for driver'] = np.NaN\ntest_data['# Times in same location and event for driver'] = np.NaN\n# test_data['# Times in same location and event and lap for driver'] = np.NaN\n\ntest_data['Average lap time in same location for driver'] = np.NaN\ntest_data['Average lap time in same location and event for driver'] = np.NaN\n# test_data['Average lap time in same location and event and lap for driver'] = np.NaN\ntest_data['SL_NO'] = test_data.index + 1\n\nfor i in tqdm(range(0, train_data.shape[0])):\n    driver_name = train_data['DRIVER_NAME'].iloc[i]\n    loc_name = train_data['LOCATION'].iloc[i]\n    event = train_data['EVENT'].iloc[i]\n    lap_num = train_data[' LAP_NUMBER'].iloc[i]\n    sl_no = train_data['SL_NO'].iloc[i]\n    \n    temp1 = train_data[(train_data.index.isin(train_indices)) & (train_data['DRIVER_NAME']==driver_name) & (train_data['LOCATION']==loc_name) & (train_data['SL_NO']!=sl_no)].reset_index(drop = True)\n    temp2 = train_data[(train_data.index.isin(train_indices)) & (train_data['DRIVER_NAME']==driver_name) & (train_data['LOCATION']==loc_name) & (train_data['EVENT']==event) & (train_data['SL_NO']!=sl_no)].reset_index(drop = True)\n#     temp3 = train_data[(train_data.index.isin(train_indices)) & (train_data['DRIVER_NAME']==driver_name) & (train_data['LOCATION']==loc_name) & (train_data['EVENT']==event) & (train_data[' LAP_NUMBER']==lap_num) & (train_data['SL_NO']!=sl_no)].reset_index(drop = True)\n    \n    if temp1.shape[0] != 0:\n        train_data['# Times in same location for driver'].iloc[i] = temp1.shape[0]\n        train_data['Average lap time in same location for driver'].iloc[i] = temp1['LAP_TIME'].mean()\n        \n    if temp2.shape[0] != 0:\n        train_data['# Times in same location and event for driver'].iloc[i] = temp2.shape[0]\n        train_data['Average lap time in same location and event for driver'].iloc[i] = temp2['LAP_TIME'].mean()\n        \n#     if temp3.shape[0] != 0:\n#         train_data['# Times in same location and event and lap for driver'].iloc[i] = temp3.shape[0]\n#         train_data['Average lap time in same location and event and lap for driver'].iloc[i] = temp3['LAP_TIME'].mean()\n\nfor i in tqdm(range(0, test_data.shape[0])):\n    driver_name = test_data['DRIVER_NAME'].iloc[i]\n    loc_name = test_data['LOCATION'].iloc[i]\n    event = test_data['EVENT'].iloc[i]\n    lap_num = test_data[' LAP_NUMBER'].iloc[i]\n\n    temp1 = train_data[(train_data.index.isin(train_indices)) & (train_data['DRIVER_NAME']==driver_name) & (train_data['LOCATION']==loc_name)].reset_index(drop = True)\n    temp2 = train_data[(train_data.index.isin(train_indices)) & (train_data['DRIVER_NAME']==driver_name) & (train_data['LOCATION']==loc_name) & (train_data['EVENT']==event)].reset_index(drop = True)\n#     temp3 = train_data[(train_data.index.isin(train_indices)) & (train_data['DRIVER_NAME']==driver_name) & (train_data['LOCATION']==loc_name) & (train_data['EVENT']==event) & (train_data[' LAP_NUMBER']==lap_num)].reset_index(drop = True)\n    \n    if temp1.shape[0] != 0:\n        test_data['# Times in same location for driver'].iloc[i] = temp1.shape[0]\n        test_data['Average lap time in same location for driver'].iloc[i] = temp1['LAP_TIME'].mean()\n        \n    if temp2.shape[0] != 0:\n        test_data['# Times in same location and event for driver'].iloc[i] = temp2.shape[0]\n        test_data['Average lap time in same location and event for driver'].iloc[i] = temp2['LAP_TIME'].mean()\n        \n#     if temp3.shape[0] != 0:\n#         test_data['# Times in same location and event and lap for driver'].iloc[i] = temp3.shape[0]\n#         test_data['Average lap time in same location and event and lap for driver'].iloc[i] = temp3['LAP_TIME'].mean()","1073d2e9":"train_data[(train_data['DRIVER_NAME']=='SB') & (train_data['LOCATION']=='Location 2') & (train_data['EVENT']=='Qualifying Group 1')].head()","0e82e922":"test_data[(test_data['DRIVER_NAME']=='SB') & (test_data['LOCATION']=='Location 8') & (test_data['EVENT']=='Qualifying Group 1')].head()","ca005484":"train_data.isnull().sum()\/train_data.shape[0] * 100","7bf75b75":"test_data.isnull().sum()\/train_data.shape[0] * 100","8700491a":"# Additional features\ntrain_data['S1 to S2_LARGE_seconds'] = train_data['S2_LARGE_seconds'] - train_data['S1_LARGE_seconds']\ntrain_data['S2 to S3_LARGE_seconds'] = train_data['S3_LARGE_seconds'] - train_data['S2_LARGE_seconds']\ntrain_data['S1 to S3_LARGE_seconds'] = train_data['S3_LARGE_seconds'] - train_data['S1_LARGE_seconds']\ntrain_data['S1 + S2 + S3'] = train_data['S3_LARGE_seconds'] + train_data['S2_LARGE_seconds'] + train_data['S1_LARGE_seconds']\ntest_data['S1 to S2_LARGE_seconds'] = test_data['S2_LARGE_seconds'] - test_data['S1_LARGE_seconds']\ntest_data['S2 to S3_LARGE_seconds'] = test_data['S3_LARGE_seconds'] - test_data['S2_LARGE_seconds']\ntest_data['S1 to S3_LARGE_seconds'] = test_data['S3_LARGE_seconds'] - test_data['S1_LARGE_seconds']\ntest_data['S1 + S2 + S3'] = test_data['S3_LARGE_seconds'] + train_data['S2_LARGE_seconds'] + train_data['S1_LARGE_seconds']\n\ntrain_data['CROSSING_FINISH_LINE_IN_PIT_B'] = np.where(train_data[' CROSSING_FINISH_LINE_IN_PIT']=='B',1,0)\ntest_data['CROSSING_FINISH_LINE_IN_PIT_B'] = np.where(test_data[' CROSSING_FINISH_LINE_IN_PIT']=='B',1,0)\n\nEVENT_GROUP = {\n  'Free Practice 1': 'Free Practice',\n  'Free Practice 2': 'Free Practice',\n  'Free Practice 3': 'Free Practice',\n  'Qualifying Group 3': 'Qualification',\n  'Qualifying Group 4': 'Qualification',\n  'Qualifying Group 2': 'Qualification',\n  'Qualifying Group 1': 'Qualification'\n}\n\nfor k, v in EVENT_GROUP.items():\n    train_data.loc[train_data['EVENT'] == k, 'EVENT_GROUP'] = v\n    test_data.loc[test_data['EVENT'] == k, 'EVENT_GROUP'] = v\n    \nle = LabelEncoder()\ntrain_data['driver_name'] = le.fit_transform(train_data['DRIVER_NAME'])\ntest_data['driver_name'] = le.transform(test_data['DRIVER_NAME'])\n\ntrain_data['team'] = le.fit_transform(train_data['TEAM'])\ntest_data['team'] = le.transform(test_data['TEAM'])\n\ntrain_data['location'] = le.fit_transform(train_data['LOCATION'])\ntest_data['location'] = le.transform(test_data['LOCATION'])\n\ntrain_data['event'] = le.fit_transform(train_data['EVENT_GROUP'])\ntest_data['event'] = le.transform(test_data['EVENT_GROUP'])","05786085":"train_data.groupby(['GROUP']).size()","29f0aba6":"train_data['POWER'].unique(), train_data['POWER'].mean()","86f6a674":"test_data['POWER'].unique()","76ef4fa4":"train_data.groupby(['POWER']).size()","203118dc":"# train_data['POWER flag'] = np.where(train_data['POWER']==235,0,np.NaN)\n# train_data['POWER flag'] = np.where(train_data['POWER']==250,1,train_data['POWER flag'])\ntrain_data['POWER flag'] = np.where(train_data['POWER']==235,0,1)\n\ntest_data['POWER flag'] = np.where(test_data['POWER']==235,0,1)\ntrain_data['POWER flag'].sum(), test_data['POWER flag'].sum()","c125adbe":"train_data['GROUP'].isnull().sum(), train_data.shape","d1ae7116":"print(train_data.shape)\n\nencoding = 'Label'\n\n# HOUR_seconds, GROUP\n\nif encoding == 'One-hot':\n    train_data = pd.concat([train_data, pd.get_dummies(train_data[['DRIVER_NAME','TEAM','EVENT_GROUP','LOCATION']])], axis =1).reset_index(drop = True)\n    train_data_for_modelling = pd.concat([train_data[train_data['filter']==1].reset_index(drop = True)[['LAP_TIME', 'NUMBER',' LAP_NUMBER',' LAP_IMPROVEMENT', ' KPH','POWER flag',\n   'AIR_TEMP', 'TRACK_TEMP', 'HUMIDITY', 'PRESSURE', 'WIND_SPEED', 'WIND_DIRECTION', 'RAIN', \n   'AIR_TEMP_min', 'TRACK_TEMP_min', 'HUMIDITY_min', 'PRESSURE_min', 'WIND_SPEED_min', 'WIND_DIRECTION_min', 'RAIN_min', \n   'AIR_TEMP_max', 'TRACK_TEMP_max', 'HUMIDITY_max', 'PRESSURE_max', 'WIND_SPEED_max', 'WIND_DIRECTION_max', 'RAIN_max',\n   'CROSSING_FINISH_LINE_IN_PIT_B', ' ELAPSED_seconds', 'S1_LARGE_seconds', 'S2_LARGE_seconds', 'S3_LARGE_seconds', 'PIT_TIME_seconds', 'S1 to S2_LARGE_seconds', 'S2 to S3_LARGE_seconds', 'S1 to S3_LARGE_seconds', 'S1 + S2 + S3',\n   '# Times in same location for driver', '# Times in same location and event for driver', 'Average lap time in same location for driver', 'Average lap time in same location and event for driver']],pd.get_dummies(train_data[train_data['filter']==1].reset_index(drop = True)[['DRIVER_NAME','TEAM','EVENT_GROUP','LOCATION']])], axis = 1)\n    print(train_data_for_modelling.shape)\n\n    test_data = pd.concat([test_data, pd.get_dummies(test_data[['DRIVER_NAME','TEAM','EVENT_GROUP','LOCATION']])], axis =1).reset_index(drop = True)\n    print(test_data.shape)\n    test_data_for_modelling = pd.concat([test_data[['LAP_TIME', 'NUMBER',' LAP_NUMBER',' LAP_IMPROVEMENT', ' KPH', 'POWER flag',\n   'AIR_TEMP', 'TRACK_TEMP', 'HUMIDITY', 'PRESSURE', 'WIND_SPEED', 'WIND_DIRECTION', 'RAIN', \n   'AIR_TEMP_min', 'TRACK_TEMP_min', 'HUMIDITY_min', 'PRESSURE_min', 'WIND_SPEED_min', 'WIND_DIRECTION_min', 'RAIN_min', \n   'AIR_TEMP_max', 'TRACK_TEMP_max', 'HUMIDITY_max', 'PRESSURE_max', 'WIND_SPEED_max', 'WIND_DIRECTION_max', 'RAIN_max',\n   'CROSSING_FINISH_LINE_IN_PIT_B', ' ELAPSED_seconds', 'S1_LARGE_seconds', 'S2_LARGE_seconds', 'S3_LARGE_seconds', 'PIT_TIME_seconds', 'S1 to S2_LARGE_seconds', 'S2 to S3_LARGE_seconds', 'S1 to S3_LARGE_seconds', 'S1 + S2 + S3',\n   '# Times in same location for driver', '# Times in same location and event for driver', 'Average lap time in same location for driver', 'Average lap time in same location and event for driver']],pd.get_dummies(test_data[['DRIVER_NAME','TEAM','EVENT','LOCATION']])], axis = 1)\n    print(test_data_for_modelling.shape)\n    \nelif encoding == 'Label':\n    train_data_for_modelling = train_data[train_data['filter']==1].reset_index(drop = True)[['LAP_TIME', 'NUMBER',' LAP_NUMBER',' LAP_IMPROVEMENT', ' KPH','POWER flag',\n   'AIR_TEMP', 'TRACK_TEMP', 'HUMIDITY', 'PRESSURE', 'WIND_SPEED', 'WIND_DIRECTION', 'RAIN', \n   'AIR_TEMP_min', 'TRACK_TEMP_min', 'HUMIDITY_min', 'PRESSURE_min', 'WIND_SPEED_min', 'WIND_DIRECTION_min', 'RAIN_min', \n   'AIR_TEMP_max', 'TRACK_TEMP_max', 'HUMIDITY_max', 'PRESSURE_max', 'WIND_SPEED_max', 'WIND_DIRECTION_max', 'RAIN_max',\n   'CROSSING_FINISH_LINE_IN_PIT_B', ' ELAPSED_seconds', 'S1_LARGE_seconds', 'S2_LARGE_seconds', 'S3_LARGE_seconds', 'PIT_TIME_seconds', 'S1 to S2_LARGE_seconds', 'S2 to S3_LARGE_seconds', 'S1 to S3_LARGE_seconds', 'S1 + S2 + S3',\n   '# Times in same location for driver', '# Times in same location and event for driver', 'Average lap time in same location for driver', 'Average lap time in same location and event for driver',\n   'driver_name','team','location','event']]\n    print(train_data_for_modelling.shape)\n\n    print(test_data.shape)\n    test_data_for_modelling = test_data[['LAP_TIME', 'NUMBER',' LAP_NUMBER',' LAP_IMPROVEMENT', ' KPH','POWER flag',\n   'AIR_TEMP', 'TRACK_TEMP', 'HUMIDITY', 'PRESSURE', 'WIND_SPEED', 'WIND_DIRECTION', 'RAIN', \n   'AIR_TEMP_min', 'TRACK_TEMP_min', 'HUMIDITY_min', 'PRESSURE_min', 'WIND_SPEED_min', 'WIND_DIRECTION_min', 'RAIN_min', \n   'AIR_TEMP_max', 'TRACK_TEMP_max', 'HUMIDITY_max', 'PRESSURE_max', 'WIND_SPEED_max', 'WIND_DIRECTION_max', 'RAIN_max',\n   'CROSSING_FINISH_LINE_IN_PIT_B', ' ELAPSED_seconds', 'S1_LARGE_seconds', 'S2_LARGE_seconds', 'S3_LARGE_seconds', 'PIT_TIME_seconds', 'S1 to S2_LARGE_seconds', 'S2 to S3_LARGE_seconds', 'S1 to S3_LARGE_seconds', 'S1 + S2 + S3',\n   '# Times in same location for driver', '# Times in same location and event for driver', 'Average lap time in same location for driver', 'Average lap time in same location and event for driver',\n   'driver_name','team','location','event']]\n    print(test_data_for_modelling.shape)\n    \nif encoding == 'One-hot':\n    if 'EVENT_GROUP_Qualification' not in train_data_for_modelling.columns.tolist():\n        train_data_for_modelling.drop(columns = ['EVENT_GROUP_Free Practice'], inplace = True)\n    else:\n        train_data_for_modelling['EVENT_Qualifying sum'] = train_data_for_modelling['EVENT_GROUP_Qualification']\n        train_data_for_modelling.drop(columns = ['EVENT_GROUP_Free Practice'], inplace = True)\n    #     train_data_for_modelling['EVENT_Qualifying sum'] = train_data_for_modelling[['EVENT_Qualifying Group 1','EVENT_Qualifying Group 2','EVENT_Qualifying Group 3','EVENT_Qualifying Group 4']].sum(axis = 1)\n        train_data_for_modelling['EVENT_Qualifying sum'] = np.where(train_data_for_modelling['EVENT_Qualifying sum']>0,1,0)\n        print(train_data_for_modelling[(train_data_for_modelling['EVENT_Qualifying sum']!=0)].shape[0])\nelif encoding == 'Label':\n    train_data_for_modelling['EVENT_Qualifying sum'] = train_data_for_modelling['event']\n    train_data_for_modelling['EVENT_Qualifying sum'] = np.where(train_data_for_modelling['EVENT_Qualifying sum']>0,1,0)\n    print(train_data_for_modelling[(train_data_for_modelling['EVENT_Qualifying sum']!=0)].shape[0])","c7ca7b8b":"train_data[['EVENT','event']].drop_duplicates()","b70e9421":"train_data_for_modelling.head()","f526bb3b":"# XGBoost model\ndf_train = train_data_for_modelling[train_data_for_modelling.index.isin(train_indices)].reset_index(drop = True) \ndf_valid = train_data_for_modelling[train_data_for_modelling.index.isin(valid_indices)].reset_index(drop = True) \n\n# Drop columns that are all NaNs\ncols_full_nans = []\nfor col in df_train.columns.tolist():\n    if ((df_train[col].isnull().sum()==df_train.shape[0]) | (df_valid[col].isnull().sum()==df_valid.shape[0])):\n        cols_full_nans += [col]\n        \nfeature_cols = [x for x in train_data_for_modelling.columns if x not in ['LAP_TIME','EVENT_Qualifying sum','LAP_TIME bin','LAP_TIME bin_loc678'] + cols_full_nans]\ndisplay(np.array(feature_cols))\n\nprint(df_train.shape[0], df_valid.shape[0])\n\n# print(df_train['EVENT_Qualifying sum'].sum(), df_valid['EVENT_Qualifying sum'].sum())","37d9033a":"# XGBoost model\nX_train = df_train[feature_cols]\ny_train = df_train[['LAP_TIME']]\nX_valid = df_valid[feature_cols]\ny_valid = df_valid[['LAP_TIME']]\n\nprint(X_train.shape)\nX_train.isnull().sum().reset_index().sort_values(by = 0, ascending = False)","d6663c72":"# XGBoost model\nX_train = df_train[feature_cols]\ny_train = df_train[['LAP_TIME']]\nX_valid = df_valid[feature_cols]\ny_valid = df_valid[['LAP_TIME']]\n\n# # Convert label to log of label since evaluation metric is rmsle\n# y_train = np.log1p(y_train)\n# y_valid = np.log1p(y_valid)\n\nprint('Total # features for modelling before boostaroota:', X_train.shape[1])\n\nbr = BoostARoota(metric='rmsle', silent = True)\nbr.fit(X_train,y_train)\nX_train=X_train[br.keep_vars_.tolist()]\nX_valid=X_valid[br.keep_vars_.tolist()]\nprint('Total # features for modelling after boostaroota:', len(br.keep_vars_.tolist()))\n\nstudy = optuna.create_study(direction='minimize', sampler=TPESampler(seed=SEED))\nstudy.optimize(\n    functools.partial(objective_regressor, X_train, y_train, X_valid, y_valid,'trial'),\n            timeout=500)\n\nmodel_xgb = xgb.XGBRegressor(**study.best_params, random_state=SEED)\nmodel_xgb.fit(X_train,y_train)","d42234ee":"print('\\nTrain data performance:')\ny_predicted = model_xgb.predict(X_train)\n# print('RMSE: ', mean_squared_error(y_train, y_predicted, squared=False))\n# print('RMSLE: ', np.sqrt(mean_squared_log_error(df_train['LAP_TIME'], np.expm1(model_xgb.predict(X_train)))))\nprint('RMSLE: ', np.sqrt(mean_squared_log_error(df_train['LAP_TIME'], model_xgb.predict(X_train))))\nprint('\\nValidation data performance:')\ny_predicted = model_xgb.predict(X_valid)\n# print('RMSE: ', mean_squared_error(y_valid, y_predicted, squared=False))\n# print('RMSLE: ', np.sqrt(mean_squared_log_error(df_valid['LAP_TIME'], np.expm1(model_xgb.predict(X_valid)))))\nprint('RMSLE: ', np.sqrt(mean_squared_log_error(df_valid['LAP_TIME'], model_xgb.predict(X_valid))))\n\nprint(\"Saving model .. \",end=\" \")\njoblib.dump(model_xgb,\"XGBoost_model.pkl\")","6c3a5090":"y_predicted = model_xgb.predict(pd.concat([X_train, X_valid], axis = 0))\nprint('RMSLE: ', np.sqrt(mean_squared_log_error(pd.concat([df_train['LAP_TIME'], df_valid['LAP_TIME']], axis = 0), y_predicted)))","067259ec":"all_lap_times = np.sort(train_data_for_modelling['LAP_TIME'].unique())\nP_MIN = all_lap_times[0].item()\nP_MAX = all_lap_times[-1].item()\nP_STEP = (all_lap_times[1] - all_lap_times[0]).item()\nP_MIN, P_MAX, P_STEP","ebc3f38f":"y_predicted = model_xgb.predict(X_train)\ny_predicted_v1 = np.round(y_predicted)\ny_predicted_v1 = np.clip(y_predicted_v1, P_MIN, P_MAX)\nprint('\\Train data performance:')\nprint('RMSLE: ', np.sqrt(mean_squared_log_error(df_train['LAP_TIME'], y_predicted_v1)))","84acfe21":"y_predicted = model_xgb.predict(X_valid)\ny_predicted_v1 = np.round(y_predicted)\ny_predicted_v1 = np.clip(y_predicted_v1, P_MIN, P_MAX)\nprint('\\nValidation data performance:')\nprint('RMSLE: ', np.sqrt(mean_squared_log_error(df_valid['LAP_TIME'], y_predicted_v1)))","d1bd3664":"model_xgb.n_estimators, model_xgb.max_depth","f2afed33":"# explainer = shap.TreeExplainer(model_xgb)\n# shap_values = explainer.shap_values(X_train)\n# shap.initjs()\n# plt.clf()\n# shap.summary_plot(shap_values,features=X_train)","01853eab":"# explainer.expected_value","7a74cc5c":"# np.mean(model_xgb.predict(X_train))","e3243ca3":"# class ShapObject:\n    \n#     def __init__(self, base_values, data, values, feature_names):\n#         self.base_values = base_values # Single value\n#         self.data = data # Raw feature values for 1 row of data\n#         self.values = values # SHAP values for the same row of data\n#         self.feature_names = feature_names # Column names\n        \n\n# shap_object = ShapObject(base_values = explainer.expected_value,\n#                          values = shap_values[0,:],\n#                          feature_names = X_train.columns,\n#                          data = X_train.iloc[0,:])\n\n# shap.waterfall_plot(shap_object)","0dc57be0":"# shap.plots.force(explainer.expected_value,shap_values[0])","db2da4e9":"X_train.head()","3fa677e1":"y_train","9d3c15ba":"y_predicted","5b1df5ae":"fig = plt.figure(dpi=180)\nax = plt.subplot(1,1,1)\n\nplot_tree(model_xgb, num_trees=0, ax = ax)\nplt.show()","ca9c2993":"temp = pd.DataFrame(model_xgb.feature_importances_)\ntemp['Feature'] = X_train.columns.tolist()\ntemp.columns = ['Feature importance','Feature']\ntemp[temp['Feature importance']>0][['Feature importance','Feature']].sort_values(by = 'Feature importance', ascending = False).reset_index(drop = True)","143c641a":"model_xgb_full_data = xgb.XGBRegressor(**study.best_params, random_state=SEED)\nmodel_xgb_full_data.fit(pd.concat([X_train,X_valid], axis = 0), pd.concat([y_train,y_valid], axis = 0))\n\ny_predicted = model_xgb_full_data.predict(pd.concat([X_train,X_valid], axis = 0))\n# print('RMSE: ', mean_squared_error(pd.concat([y_train,y_valid], axis = 0), y_predicted, squared=False))\nprint('RMSLE: ', np.sqrt(mean_squared_log_error(pd.concat([y_train,y_valid], axis = 0), model_xgb_full_data.predict(pd.concat([X_train,X_valid], axis = 0)))))\n\nprint(\"Saving model .. \",end=\" \")\njoblib.dump(model_xgb_full_data,\"XGBoost_model_full_data.pkl\")","d318704c":"display(list(set(X_train.columns.tolist()) - set(test_data_for_modelling.columns.tolist())))\ntest_data_for_modelling[list(set(X_train.columns.tolist()) - set(test_data_for_modelling.columns.tolist()))] = 0\n\nxgboost_pred = model_xgb.predict(test_data_for_modelling[X_train.columns.tolist()])\n# xgboost_pred = np.round(xgboost_pred)\n# xgboost_pred = np.clip(xgboost_pred, P_MIN, P_MAX)\n\nsubmission_data['LAP_TIME'] = xgboost_pred\nsubmission_data.to_csv('my_submission_file_xgboost.csv', index=False)\nprint(submission_data.shape)\ndisplay(submission_data.head())\n\nxgboost_pred = model_xgb_full_data.predict(test_data_for_modelling[X_train.columns.tolist()])\n# xgboost_pred = np.round(xgboost_pred)\n# xgboost_pred = np.clip(xgboost_pred, P_MIN, P_MAX)\n\nsubmission_data['LAP_TIME'] = xgboost_pred\nsubmission_data.to_csv('my_submission_file_xgboost_full_data_train.csv', index=False)\nprint(submission_data.shape)\ndisplay(submission_data.head())","ca88e705":"# # Nested cross validation\n# from sklearn.model_selection import StratifiedKFold\n# skf = StratifiedKFold(n_splits=5)\n# X = train_data[train_data['filter']==1].reset_index(drop = True)\n# y = X[['LAP_TIME bin']]\n# for train_index, test_index in skf.split(X, y):\n#     df_train = X[X.index.isin(train_index)].reset_index(drop = True)\n#     df_test = X[X.index.isin(test_index)].reset_index(drop = True)\n#     skf_v1 = StratifiedKFold(n_splits=5)\n#     for train_index_v1, test_index_v1 in skf_v1.split(df_train, df_train[['LAP_TIME bin']]):\n#         df_train_v1 = df_train[df_train.index.isin(train_index_v1)].reset_index(drop = True)\n#         df_test_v1 = df_valid[df_valid.index.isin(test_index_v1)].reset_index(drop = True)","5fb1864c":"from sklearn.linear_model import RidgeCV\n\ndf_train_v1 = df_train.copy()\ndf_valid_v1 = df_valid.copy()\n\nfor col in feature_cols:\n    df_valid_v1[col] = df_valid_v1[col].fillna(df_train_v1[col].mean())\n    df_train_v1[col] = df_train_v1[col].fillna(df_train_v1[col].mean())\n    \nmodel_ridgecv = RidgeCV(fit_intercept=True, \n                alphas=[5.0], normalize=False,\n                cv = 2, scoring='neg_mean_squared_error')\n\nprint(\"Fitting Model\")\nmodel_ridgecv.fit(df_train_v1[feature_cols], df_train_v1['LAP_TIME'])\npreds = model_ridgecv.predict(df_train_v1[feature_cols])\nprint('RMSLE: ', np.sqrt(mean_squared_log_error(df_train_v1['LAP_TIME'], preds)))\npreds = model_ridgecv.predict(df_valid_v1[feature_cols])\nprint('RMSLE: ', np.sqrt(mean_squared_log_error(df_valid_v1['LAP_TIME'], preds)))","5260aa27":"for col in feature_cols:\n    if col not in test_data_for_modelling.columns:\n        test_data_for_modelling[col] = 0\nX_test = test_data_for_modelling[feature_cols]\n\nfor col in feature_cols:\n    X_test[col] = X_test[col].fillna(df_train[col].mean())\npreds = model_ridgecv.predict(X_test[feature_cols])\nsubmission_data['LAP_TIME'] = preds\nsubmission_data.to_csv('my_submission_ridgecv.csv', index=False)\nprint(submission_data.shape)\ndisplay(submission_data.head())","28c9b46c":"h2o.init(\n    nthreads=-1,     # number of threads when launching a new H2O server\n    max_mem_size=12  # in gigabytes\n)","6879ee87":"def _convert_h2oframe_to_numeric(h2o_frame, training_columns):\n    for column in training_columns:\n        h2o_frame[column] = h2o_frame[column].asnumeric()\n    return h2o_frame","0e345cb8":"# Convert label to log of label since evaluation metric is rmsle\ndf_train['LAP_TIME_log'] = np.log1p(df_train['LAP_TIME'])\ndf_valid['LAP_TIME_log'] = np.log1p(df_valid['LAP_TIME'])\n\nX_y_train_h = h2o.H2OFrame(\n    pd.concat(\n        [pd.concat([df_train[feature_cols], df_valid[feature_cols]], axis = 0), pd.concat([df_train['LAP_TIME_log'], df_valid['LAP_TIME_log']], axis = 0)],\n        axis='columns'\n    )\n)\n\nX_y_train_h = _convert_h2oframe_to_numeric(X_y_train_h, feature_cols)\n\naml = H2OAutoML(\n    max_runtime_secs=(int(3600 * 1)),  # hours\n    max_models=None,  # no limit\n    seed=SEED,\n)\n\naml.train(\n    x=feature_cols,\n    y='LAP_TIME_log',\n    training_frame=X_y_train_h\n)\n\nlb = aml.leaderboard\nmodel_ids = list(lb['model_id'].as_data_frame().iloc[:,0])\nout_path = \".\"\n\nfor m_id in model_ids:\n    mdl = h2o.get_model(m_id)\n    h2o.save_model(model=mdl, path=out_path, force=True)\n\nh2o.export_file(lb, os.path.join(out_path, 'aml_leaderboard.h2o'), force=True)","d59c83a8":"models_path = \".\"\nlb = h2o.import_file(path=os.path.join(models_path, \"aml_leaderboard.h2o\"))\nlb.head(rows=10)","0c7daed2":"test_data_for_h2o = _convert_h2oframe_to_numeric(h2o.H2OFrame(test_data[feature_cols]), feature_cols)\nh20ai_best_model_pred = aml.leader.predict(test_data_for_h2o)","51bbb08a":"submission_data['LAP_TIME'] = np.expm1(list(h20ai_best_model_pred.as_data_frame()['predict']))\nsubmission_data.to_csv('my_submission_h20_ai_best_model.csv', index=False)\nprint(submission_data.shape)\ndisplay(submission_data.head())","297b876d":"print(len(feature_cols))\nfeature_cols","1e8b281d":"cat_features=[0, 2, 4, 26, 40, 41, 42, 43]\ncat_col_list = [feature_cols[x] for x in range(0, len(feature_cols)) if x in cat_features]\n\ndf_train_for_cat_boost = pd.concat([df_train,df_valid], axis = 0).copy()\n\ncb_model = CatBoostRegressor(\n    eval_metric='RMSE',\n    use_best_model=True,\n    random_seed=17\n)\n\nfor col in df_train_for_cat_boost.columns.tolist():\n    if col in cat_col_list:\n        val = df_train_for_cat_boost.groupby(col).size().reset_index().sort_values(by = 0, ascending = False)[col].iloc[0]\n        df_train_for_cat_boost[col].fillna(val, inplace = True)\n        df_train_for_cat_boost[col] = df_train_for_cat_boost[col].astype(int)\n    else:\n        df_train_for_cat_boost[col] = df_train_for_cat_boost[col].fillna(df_train_for_cat_boost[col].mean())\n        \ncv_data = cv(\n    Pool(df_train_for_cat_boost[feature_cols], df_train_for_cat_boost['LAP_TIME_log'], cat_features=cat_features),\n    cb_model.get_params(),\n    fold_count=6,\n    verbose=False,\n    return_models=True\n)","971b8a8f":"cv_data[1][2].predict(df_train_for_cat_boost[feature_cols].head())","9203e15e":"cv_data[0].head()","2b18f626":"print(\"CatBoostRegressor: the best cv rmse is\", np.min(cv_data[0]['test-RMSE-mean']), 'Iteration no:', cv_data[0][cv_data[0]['test-RMSE-mean']==np.min(cv_data[0]['test-RMSE-mean'])]['iterations'].iloc[0])","79c5aa73":"df_test_for_cat_boost = test_data[feature_cols].copy()\n\nfor col in df_test_for_cat_boost.columns.tolist():\n    if col in cat_col_list:\n        val = df_test_for_cat_boost.groupby(col).size().reset_index().sort_values(by = 0, ascending = False)[col].iloc[0]\n        df_test_for_cat_boost[col].fillna(val, inplace = True)\n        df_test_for_cat_boost[col] = df_test_for_cat_boost[col].astype(int)\n    else:\n        df_test_for_cat_boost[col] = df_test_for_cat_boost[col].fillna(df_test_for_cat_boost[col].mean())","f0fa405a":"for i in range(0,6):\n    cat_boost_pred = cv_data[1][i].predict(df_test_for_cat_boost[feature_cols])\n    submission_data['LAP_TIME'] = np.expm1(cat_boost_pred)\n    submission_data.to_csv('my_submission_' + str(i) + '_catboost.csv', index=False)\n    print(submission_data.shape)\n    display(submission_data.head())","958f7a14":"print(train_data.shape)\n\nencoding = 'One-hot'\n\n# HOUR_seconds, GROUP\n\nif encoding == 'One-hot':\n    train_data = pd.concat([train_data, pd.get_dummies(train_data[['DRIVER_NAME','TEAM','EVENT_GROUP','LOCATION']])], axis =1).reset_index(drop = True)\n    train_data_for_modelling = pd.concat([train_data[train_data['filter']==1].reset_index(drop = True)[['LAP_TIME', 'NUMBER',' LAP_NUMBER',' LAP_IMPROVEMENT', ' KPH','POWER flag',\n   'AIR_TEMP', 'TRACK_TEMP', 'HUMIDITY', 'PRESSURE', 'WIND_SPEED', 'WIND_DIRECTION', 'RAIN', \n   'AIR_TEMP_min', 'TRACK_TEMP_min', 'HUMIDITY_min', 'PRESSURE_min', 'WIND_SPEED_min', 'WIND_DIRECTION_min', 'RAIN_min', \n   'AIR_TEMP_max', 'TRACK_TEMP_max', 'HUMIDITY_max', 'PRESSURE_max', 'WIND_SPEED_max', 'WIND_DIRECTION_max', 'RAIN_max',\n   'CROSSING_FINISH_LINE_IN_PIT_B', ' ELAPSED_seconds', 'S1_LARGE_seconds', 'S2_LARGE_seconds', 'S3_LARGE_seconds', 'PIT_TIME_seconds', 'S1 to S2_LARGE_seconds', 'S2 to S3_LARGE_seconds', 'S1 to S3_LARGE_seconds', 'S1 + S2 + S3',\n   '# Times in same location for driver', '# Times in same location and event for driver', 'Average lap time in same location for driver', 'Average lap time in same location and event for driver']],pd.get_dummies(train_data[train_data['filter']==1].reset_index(drop = True)[['DRIVER_NAME','TEAM','EVENT_GROUP','LOCATION']])], axis = 1)\n    print(train_data_for_modelling.shape)\n\n    test_data = pd.concat([test_data, pd.get_dummies(test_data[['DRIVER_NAME','TEAM','EVENT_GROUP','LOCATION']])], axis =1).reset_index(drop = True)\n    print(test_data.shape)\n    test_data_for_modelling = pd.concat([test_data[['LAP_TIME', 'NUMBER',' LAP_NUMBER',' LAP_IMPROVEMENT', ' KPH', 'POWER flag',\n   'AIR_TEMP', 'TRACK_TEMP', 'HUMIDITY', 'PRESSURE', 'WIND_SPEED', 'WIND_DIRECTION', 'RAIN', \n   'AIR_TEMP_min', 'TRACK_TEMP_min', 'HUMIDITY_min', 'PRESSURE_min', 'WIND_SPEED_min', 'WIND_DIRECTION_min', 'RAIN_min', \n   'AIR_TEMP_max', 'TRACK_TEMP_max', 'HUMIDITY_max', 'PRESSURE_max', 'WIND_SPEED_max', 'WIND_DIRECTION_max', 'RAIN_max',\n   'CROSSING_FINISH_LINE_IN_PIT_B', ' ELAPSED_seconds', 'S1_LARGE_seconds', 'S2_LARGE_seconds', 'S3_LARGE_seconds', 'PIT_TIME_seconds', 'S1 to S2_LARGE_seconds', 'S2 to S3_LARGE_seconds', 'S1 to S3_LARGE_seconds', 'S1 + S2 + S3',\n   '# Times in same location for driver', '# Times in same location and event for driver', 'Average lap time in same location for driver', 'Average lap time in same location and event for driver']],pd.get_dummies(test_data[['DRIVER_NAME','TEAM','EVENT','LOCATION']])], axis = 1)\n    print(test_data_for_modelling.shape)\n    \nelif encoding == 'Label':\n    train_data_for_modelling = train_data[train_data['filter']==1].reset_index(drop = True)[['LAP_TIME', 'NUMBER',' LAP_NUMBER',' LAP_IMPROVEMENT', ' KPH','POWER flag',\n   'AIR_TEMP', 'TRACK_TEMP', 'HUMIDITY', 'PRESSURE', 'WIND_SPEED', 'WIND_DIRECTION', 'RAIN', \n   'AIR_TEMP_min', 'TRACK_TEMP_min', 'HUMIDITY_min', 'PRESSURE_min', 'WIND_SPEED_min', 'WIND_DIRECTION_min', 'RAIN_min', \n   'AIR_TEMP_max', 'TRACK_TEMP_max', 'HUMIDITY_max', 'PRESSURE_max', 'WIND_SPEED_max', 'WIND_DIRECTION_max', 'RAIN_max',\n   'CROSSING_FINISH_LINE_IN_PIT_B', ' ELAPSED_seconds', 'S1_LARGE_seconds', 'S2_LARGE_seconds', 'S3_LARGE_seconds', 'PIT_TIME_seconds', 'S1 to S2_LARGE_seconds', 'S2 to S3_LARGE_seconds', 'S1 to S3_LARGE_seconds', 'S1 + S2 + S3',\n   '# Times in same location for driver', '# Times in same location and event for driver', 'Average lap time in same location for driver', 'Average lap time in same location and event for driver',\n   'driver_name','team','location','event']]\n    print(train_data_for_modelling.shape)\n\n    print(test_data.shape)\n    test_data_for_modelling = test_data[['LAP_TIME', 'NUMBER',' LAP_NUMBER',' LAP_IMPROVEMENT', ' KPH','POWER flag',\n   'AIR_TEMP', 'TRACK_TEMP', 'HUMIDITY', 'PRESSURE', 'WIND_SPEED', 'WIND_DIRECTION', 'RAIN', \n   'AIR_TEMP_min', 'TRACK_TEMP_min', 'HUMIDITY_min', 'PRESSURE_min', 'WIND_SPEED_min', 'WIND_DIRECTION_min', 'RAIN_min', \n   'AIR_TEMP_max', 'TRACK_TEMP_max', 'HUMIDITY_max', 'PRESSURE_max', 'WIND_SPEED_max', 'WIND_DIRECTION_max', 'RAIN_max',\n   'CROSSING_FINISH_LINE_IN_PIT_B', ' ELAPSED_seconds', 'S1_LARGE_seconds', 'S2_LARGE_seconds', 'S3_LARGE_seconds', 'PIT_TIME_seconds', 'S1 to S2_LARGE_seconds', 'S2 to S3_LARGE_seconds', 'S1 to S3_LARGE_seconds', 'S1 + S2 + S3',\n   '# Times in same location for driver', '# Times in same location and event for driver', 'Average lap time in same location for driver', 'Average lap time in same location and event for driver',\n   'driver_name','team','location','event']]\n    print(test_data_for_modelling.shape)\n    \nif encoding == 'One-hot':\n    if 'EVENT_GROUP_Qualification' not in train_data_for_modelling.columns.tolist():\n        train_data_for_modelling.drop(columns = ['EVENT_GROUP_Free Practice'], inplace = True)\n    else:\n        train_data_for_modelling['EVENT_Qualifying sum'] = train_data_for_modelling['EVENT_GROUP_Qualification']\n        train_data_for_modelling.drop(columns = ['EVENT_GROUP_Free Practice'], inplace = True)\n    #     train_data_for_modelling['EVENT_Qualifying sum'] = train_data_for_modelling[['EVENT_Qualifying Group 1','EVENT_Qualifying Group 2','EVENT_Qualifying Group 3','EVENT_Qualifying Group 4']].sum(axis = 1)\n        train_data_for_modelling['EVENT_Qualifying sum'] = np.where(train_data_for_modelling['EVENT_Qualifying sum']>0,1,0)\n        print(train_data_for_modelling[(train_data_for_modelling['EVENT_Qualifying sum']!=0)].shape[0])\nelif encoding == 'Label':\n    train_data_for_modelling['EVENT_Qualifying sum'] = train_data_for_modelling['event']\n    train_data_for_modelling['EVENT_Qualifying sum'] = np.where(train_data_for_modelling['EVENT_Qualifying sum']>0,1,0)\n    print(train_data_for_modelling[(train_data_for_modelling['EVENT_Qualifying sum']!=0)].shape[0])","2938d425":"df_train = train_data_for_modelling[train_data_for_modelling.index.isin(train_indices)].reset_index(drop = True) \ndf_valid = train_data_for_modelling[train_data_for_modelling.index.isin(valid_indices)].reset_index(drop = True) \n\n# Drop columns that are all NaNs\ncols_full_nans = []\nfor col in df_train.columns.tolist():\n    if ((df_train[col].isnull().sum()==df_train.shape[0]) | (df_valid[col].isnull().sum()==df_valid.shape[0])):\n        cols_full_nans += [col]\n        \nfeature_cols = [x for x in train_data_for_modelling.columns if x not in ['LAP_TIME','EVENT_Qualifying sum','LAP_TIME bin','LAP_TIME bin_loc678'] + cols_full_nans]\ndisplay(np.array(feature_cols))\n\nprint(df_train.shape[0], df_valid.shape[0])\n\nX_train = df_train[feature_cols]\ny_train = df_train[['LAP_TIME']]\nX_valid = df_valid[feature_cols]\ny_valid = df_valid[['LAP_TIME']]","f227a82f":"BATCH_SIZE = 16\nEPOCHS = 5000","bbab08ec":"df_train_v1 = df_train.copy()\ndf_valid_v1 = df_valid.copy()\n\nfor col in feature_cols:\n    df_valid_v1[col] = df_valid_v1[col].fillna(df_train_v1[col].mean())\n    df_train_v1[col] = df_train_v1[col].fillna(df_train_v1[col].mean())\n    \nX_train = df_train_v1[feature_cols]\ny_train = df_train_v1['LAP_TIME']\nX_valid = df_valid_v1[feature_cols]\ny_valid = df_valid_v1['LAP_TIME']","8f07c334":"from sklearn.preprocessing import RobustScaler, normalize\nscaler = RobustScaler()\n# transform data\nX_train_v1 = scaler.fit_transform(X_train)\nX_valid_v1 = scaler.transform(X_valid)","c967652d":"X_train_v1[0]","68e44a1a":"from tensorflow.keras import regularizers\ndef simple_nn():\n    x_input = Input(shape=(X_train_v1.shape[1]))\n    \n    x = Dense(units=128, activation='relu',\n    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(x_input)\n    x1 = BatchNormalization()(x)\n    \n    x2 = Dense(units=256, activation='relu',\n    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(x1)\n    x3 = BatchNormalization()(x2)\n    \n    x4 = Dense(units=256, activation='relu',\n    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(x3)\n    x5 = BatchNormalization()(x4)\n    \n    x6 = Dense(units=128, activation='relu',\n    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(x5)\n    x7 = BatchNormalization()(x6)\n    \n    x_output = Dense(units=1, activation='relu',\n    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(x7)\n    \n    model = Model(inputs=x_input, outputs=x_output, \n                  name='NN_Model')\n    return model","c847f8fd":"model = simple_nn()\nmodel.summary()","0a0aa3d5":"plot_model(\n    model, \n    to_file='Keras_Model.png', \n    show_shapes=True,\n    show_layer_names=True\n)","8f616cd8":"msle = tf.keras.losses.MeanSquaredLogarithmicError()\nmodel.compile(optimizer=\"adam\", loss=msle, metrics=[msle])\n\nes = EarlyStopping(monitor=\"val_loss\", patience=1000,\n                   verbose=2, mode=\"min\", \n                   restore_best_weights=True)\n\nsave_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\nchk_point = ModelCheckpoint(f'.\/nn_model.h5', options=save_locally, \n                            monitor='val_loss', verbose=2, \n                            save_best_only=True, mode='min')\n\nhistory = model.fit(X_train_v1, y_train, \n          validation_data=(X_valid_v1, y_valid), \n          epochs=EPOCHS,\n          verbose=2,\n          batch_size=BATCH_SIZE, \n          callbacks=[chk_point, es])","dd12a4d0":"from tensorflow.keras.models import load_model\nmodel = load_model(f'.\/nn_model.h5', compile = False)","73c046b4":"y_pred = model.predict(X_valid_v1, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1)","51afbe3f":"plt.plot(history.history['loss'][50:])\nplt.plot(history.history['val_loss'][50:])\nplt.title('model loss')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()","44e47b08":"y_pred.mean(), y_pred.min(), y_pred.max()","a82081f5":"print('RMSLE: ', np.sqrt(mean_squared_log_error(df_train['LAP_TIME'], model.predict(X_train_v1))))","12060b6c":"print('RMSLE: ', np.sqrt(mean_squared_log_error(df_valid['LAP_TIME'], model.predict(X_valid_v1))))","2deadf81":"print('RMSLE: ', np.sqrt(mean_squared_log_error(df_valid['LAP_TIME'], np.floor(model.predict(X_valid_v1)))))","10f73c4d":"for col in feature_cols:\n    if col not in test_data_for_modelling.columns:\n        test_data_for_modelling[col] = 0\nX_test = test_data_for_modelling[feature_cols]\n\nfor col in feature_cols:\n    X_test[col] = X_test[col].fillna(df_train[col].mean())\n    \nX_test = scaler.transform(X_test)\npreds = model.predict(X_test, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1)\nsubmission_data['LAP_TIME'] = preds\nsubmission_data.to_csv('my_submission_nn.csv', index=False)\nprint(submission_data.shape)\ndisplay(submission_data.head())","e4c96252":"train_data.head()","ac9369d6":"train_data['Key'] = train_data['NUMBER'].astype(str) + '_' + train_data[' LAP_NUMBER'].astype(str) + '_' + train_data['LOCATION'] + '_' + train_data['EVENT']\ntemp = train_data.groupby(['Key']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False)\nlist_keys_with_multiple_records = temp[temp['# Records']>1]['Key'].unique().tolist()\ntrain_data[(train_data['Key'].isin(list_keys_with_multiple_records)) & (train_data['EVENT'].str.contains('Qualifying'))].sort_values(by = 'Key').head()","df77da4a":"test_data['Key_without_lap_num'] = test_data['NUMBER'].astype(str) + '_' + test_data['LOCATION'] + '_' + test_data['EVENT']\ntemp = test_data.groupby(['Key_without_lap_num']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False)\ntemp.head()","e0ff84aa":"train_data['Key_without_lap_num'] = train_data['NUMBER'].astype(str) + '_' + train_data['LOCATION'] + '_' + train_data['EVENT']\ntemp = train_data.groupby(['Key_without_lap_num']).size().reset_index().rename(columns = {0:'# Records'}).sort_values(by = '# Records', ascending = False)\nlist_keys_with_multiple_records = temp[temp['# Records']>1]['Key_without_lap_num'].unique().tolist()\ntrain_data[(train_data['Key_without_lap_num'].isin(list_keys_with_multiple_records)) & (train_data['EVENT'].str.contains('Qualifying'))].sort_values(by = 'Key_without_lap_num').head(10)","86d60a6c":"train_data['Key_without_lap_num'].nunique(), len(list_keys_with_multiple_records)","9bafe4c0":"temp","e9bbf117":"timesteps = temp['# Records'].max()\ntimesteps","21fbb889":"train_data['Sl no.'] = train_data.index + 1\ntest_data['Sl no.'] = test_data.index + 1\ntrain_data = train_data.sort_values(by = ['Key_without_lap_num',' LAP_NUMBER'], ascending = True).reset_index(drop = True)\ntest_data = test_data.sort_values(by = ['Key_without_lap_num',' LAP_NUMBER'], ascending = True).reset_index(drop = True)\n\ntrain_data['Key count'] = np.NaN\nfor i in tqdm(range(0, train_data.shape[0])):\n    train_data['Key count'].iloc[i] = train_data[(train_data.index<i) & (train_data['Key']==train_data['Key'].iloc[i])].shape[0]+1\n\ntest_data['Key count'] = np.NaN\nfor i in tqdm(range(0, test_data.shape[0])):\n    test_data['Key count'].iloc[i] = test_data[(test_data.index<i) & (test_data['Key']==test_data['Key'].iloc[i])].shape[0]+1","1f906a85":"train_data = train_data.sort_values(by = ['Key_without_lap_num','Key count'], ascending = True).reset_index(drop = True)\ntest_data = test_data.sort_values(by = ['Key_without_lap_num','Key count'], ascending = True).reset_index(drop = True)","41e41d78":"print(train_data['Key_without_lap_num'].isnull().sum())\ntrain_data[(train_data['Key_without_lap_num']=='17_Location 8_Free Practice 1')][['Key','Key_without_lap_num',' LAP_NUMBER','Key count']]","4fb9cf70":"test_data[['NUMBER','LOCATION','EVENT','TEAM']].drop_duplicates().shape, test_data[['NUMBER','LOCATION','EVENT']].drop_duplicates().shape","20f469f4":"train_data[['NUMBER','LOCATION','EVENT','TEAM']].drop_duplicates().shape, train_data[['NUMBER','LOCATION','EVENT']].drop_duplicates().shape","e17a7a0d":"train_data[feature_cols].head()","7c9dd3f2":"train_data[(train_data['Key_without_lap_num']=='17_Location 8_Free Practice 1')][['Key count','LAP_TIME',' LAP_NUMBER']].groupby(['Key count']).agg({'LAP_TIME':lambda x: list(x), ' LAP_NUMBER':lambda x: list(x)}).reset_index()","4769f2b1":"train_keys, valid_keys = sk_model_selection.train_test_split(\n    train_data[train_data['filter']==1][['Key_without_lap_num']].drop_duplicates().reset_index(drop = True), \n    test_size=0.1, \n    random_state=42)\n\nprint(len(train_keys), len(valid_keys))","05a4be94":"train_keys = train_keys['Key_without_lap_num'].tolist()\nvalid_keys = valid_keys['Key_without_lap_num'].tolist()","8be0e4fc":"df_train_v1 = train_data[train_data['Key_without_lap_num'].isin(train_keys)].reset_index(drop = True)\ndf_valid_v1 = train_data[train_data['Key_without_lap_num'].isin(valid_keys)].reset_index(drop = True)\nprint(df_train_v1.shape, df_valid_v1.shape)\n\nfor col in feature_cols:\n    if col not in test_data.columns.tolist():\n        test_data[col] = 0\n        \ntest_data_v1 = test_data.copy()\n\nfor col in feature_cols:\n    df_valid_v1[col] = df_valid_v1[col].fillna(df_train_v1[col].mean())\n    test_data_v1[col] = test_data_v1[col].fillna(df_train_v1[col].mean())\n    df_train_v1[col] = df_train_v1[col].fillna(df_train_v1[col].mean())\n\nother_cols = list(set([x for x in df_train_v1.columns.tolist() if x not in feature_cols]))\nother_cols_in_test = [x for x in test_data.columns.tolist() if x in other_cols]\n        \nscaler = RobustScaler()\n# transform data\ntemp = pd.DataFrame(scaler.fit_transform(df_train_v1[feature_cols]))\ntemp.columns = feature_cols\ntrain_data_for_lstm = pd.concat([df_train_v1[other_cols], temp], axis = 1)\ntemp = pd.DataFrame(scaler.transform(df_valid_v1[feature_cols]))\ntemp.columns = feature_cols\nvalid_data_for_lstm = pd.concat([df_valid_v1[other_cols], temp], axis = 1)\ntemp = pd.DataFrame(scaler.transform(test_data_v1[feature_cols]))\ntemp.columns = feature_cols\ntest_data_for_lstm = pd.concat([test_data_v1[other_cols_in_test], temp], axis = 1)","b94963eb":"print(train_data_for_lstm.shape)\ntrain_data_for_lstm.head()","a720712c":"agg_dict = {}\nfor col in feature_cols + ['LAP_TIME'] + ['Sl no.']:\n    agg_dict[col] = lambda x:list(x)\ntrain_data_for_lstm = train_data_for_lstm.reset_index(drop = True).groupby(['Key_without_lap_num','Key count']).agg(agg_dict).reset_index()\nvalid_data_for_lstm = valid_data_for_lstm.reset_index(drop = True).groupby(['Key_without_lap_num','Key count']).agg(agg_dict).reset_index()\ntest_data_for_lstm = test_data_for_lstm.groupby(['Key_without_lap_num','Key count']).agg(agg_dict).reset_index()\nprint(train_data_for_lstm.shape)\nprint(valid_data_for_lstm.shape)\nprint(test_data_for_lstm.shape)","eb3ab7fc":"# Pad timesteps\nfor i in tqdm(range(0, train_data_for_lstm.shape[0])):\n    for j in range(0, train_data_for_lstm.shape[1]):\n        col = train_data_for_lstm.columns.tolist()[j]\n        if col not in ['Key_without_lap_num','Key count']:\n            value = train_data_for_lstm.iloc[i,j]\n            if len(value)<timesteps:\n                pad_len = timesteps - len(value)\n                train_data_for_lstm.at[i, col] = value + [0]*pad_len\n                          \n# Pad timesteps\nfor i in tqdm(range(0, valid_data_for_lstm.shape[0])):\n    for j in range(0, valid_data_for_lstm.shape[1]):\n        col = valid_data_for_lstm.columns.tolist()[j]\n        if col not in ['Key_without_lap_num','Key count']:\n            value = valid_data_for_lstm.iloc[i,j]\n            if len(value)<timesteps:\n                pad_len = timesteps - len(value)\n                valid_data_for_lstm.at[i, col] = value + [0]*pad_len\n                \n# Pad timesteps\nfor i in tqdm(range(0, test_data_for_lstm.shape[0])):\n    for j in range(0, test_data_for_lstm.shape[1]):\n        col = test_data_for_lstm.columns.tolist()[j]\n        if col not in ['Key_without_lap_num','Key count']:\n            value = test_data_for_lstm.iloc[i,j]\n            if len(value)<timesteps:\n                pad_len = timesteps - len(value)\n                test_data_for_lstm.at[i, col] = value + [0]*pad_len","717c4a94":"train_data_for_lstm.head()","e36d965d":"train_data_for_lstm.iloc[10,:]","8cfa9ded":"feature_cols","9cc272a5":"cols_for_lstm = [x for x in feature_cols if x not in [' LAP_NUMBER']]\ncols_for_lstm","31811f76":"X_train = train_data_for_lstm[cols_for_lstm]\narray_temp = np.array(list(X_train.iloc[0].to_numpy())).reshape(timesteps, X_train.shape[1])\nfor i in range(1, X_train.shape[0]):\n    array_temp_1 = np.array(list(X_train.iloc[i].to_numpy())).reshape(timesteps, X_train.shape[1])\n    array_temp = np.dstack((array_temp,array_temp_1))\narray_temp = array_temp.reshape(X_train.shape[0], timesteps, X_train.shape[1])\nX_train = array_temp.copy()\n\nX_valid = valid_data_for_lstm[cols_for_lstm]\narray_temp = list(np.array(list(X_valid.iloc[0,:].to_numpy())).reshape(1,timesteps, X_valid.shape[1]))\nfor i in range(1, X_valid.shape[0]):\n    array_temp_1 = list(np.array(list(X_valid.iloc[i].to_numpy())).reshape(1,timesteps, X_valid.shape[1]))\n    array_temp += array_temp_1\nX_valid = np.array(array_temp).copy()\n\ny_train = train_data_for_lstm[['LAP_TIME']]\narray_temp = list(np.array(list(y_train.iloc[0,:].to_numpy())).reshape(1,timesteps, y_train.shape[1]))\nfor i in range(1, y_train.shape[0]):\n    array_temp_1 = list(np.array(list(y_train.iloc[i].to_numpy())).reshape(1,timesteps, y_train.shape[1]))\n    array_temp += array_temp_1\ny_train = np.array(array_temp).copy()\n\ny_valid = valid_data_for_lstm[['LAP_TIME']]\narray_temp = list(np.array(list(y_valid.iloc[0,:].to_numpy())).reshape(1,timesteps, y_valid.shape[1]))\nfor i in range(1, y_valid.shape[0]):\n    array_temp_1 = list(np.array(list(y_valid.iloc[i].to_numpy())).reshape(1,timesteps, y_valid.shape[1]))\n    array_temp += array_temp_1\ny_valid = np.array(array_temp).copy()","855d6c3f":"train_indices = train_data_for_lstm[['Sl no.']]\narray_temp = list(np.array(list(train_indices.iloc[0,:].to_numpy())).reshape(1,timesteps, train_indices.shape[1]))\nfor i in range(1, train_indices.shape[0]):\n    array_temp_1 = list(np.array(list(train_indices.iloc[i].to_numpy())).reshape(1,timesteps, train_indices.shape[1]))\n    array_temp += array_temp_1\ntrain_indices = np.array(array_temp).copy()\n\nvalid_indices = valid_data_for_lstm[['Sl no.']]\narray_temp = list(np.array(list(valid_indices.iloc[0,:].to_numpy())).reshape(1,timesteps, valid_indices.shape[1]))\nfor i in range(1, valid_indices.shape[0]):\n    array_temp_1 = list(np.array(list(valid_indices.iloc[i].to_numpy())).reshape(1,timesteps, valid_indices.shape[1]))\n    array_temp += array_temp_1\nvalid_indices = np.array(array_temp).copy()","e6943e7b":"def lstm_model():\n    \n    x_input = Input(shape=(X_train.shape[-2:]))\n    \n    x1 = Bidirectional(LSTM(units=64, return_sequences=True))(x_input)\n    x2 = Bidirectional(GRU(units=64, return_sequences=True))(x_input)\n    z31 = Multiply()([x1, x2])\n    z3 = BatchNormalization()(z31)\n    \n    x = Concatenate(axis=2)([x_input, z3])\n    \n    x = Dense(units=512, activation='selu')(x)\n    x = BatchNormalization()(x)\n    \n    x = Dense(units=256, activation='selu')(x)\n    x = BatchNormalization()(x)\n    \n    x_output = Dense(units=1, activation = 'relu')(x)\n    \n    model = Model(inputs=x_input, outputs=x_output, \n                  name='LSTM_Model_v1')\n    return model","3bb7e2d7":"model_lstm = lstm_model()\nmodel_lstm.summary()","b0cec841":"plot_model(\n    model_lstm, \n    to_file='Keras_LSTM_Model.png', \n    show_shapes=True,\n    show_layer_names=True\n)","f3e78187":"X_train.shape, y_train.shape, X_valid.shape, y_valid.shape","9bab8b06":"BATCH_SIZE = 8\nEPOCHS = 200\nmsle = tf.keras.losses.MeanSquaredLogarithmicError()\nmodel_lstm.compile(optimizer=\"adam\", loss=msle, metrics=[msle])\n\nes = EarlyStopping(monitor=\"val_loss\", patience=1000,\n                   verbose=2, mode=\"min\", \n                   restore_best_weights=True)\n\nsave_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\nchk_point = ModelCheckpoint(f'.\/lstm_model.h5', options=save_locally, \n                            monitor='val_loss', verbose=2, \n                            save_best_only=True, mode='min')\n\nhistory = model_lstm.fit(X_train, y_train, \n          validation_data=(X_valid, y_valid), \n          epochs=EPOCHS,\n          verbose=2,\n          batch_size=BATCH_SIZE, \n          callbacks=[chk_point, es])","7325e184":"model_lstm = load_model(f'.\/lstm_model.h5', compile = False)","a0b69577":"plt.plot(history.history['loss'][0:])\nplt.plot(history.history['val_loss'][0:])\nplt.title('model loss')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()","f6286e2b":"y_pred = model_lstm.predict(X_train, batch_size=BATCH_SIZE)","3cc9e93d":"y_train[0,:,0]","8b9a5ce3":"y_pred[0,:,0]","2bde8dd3":"train_indices[0,:,0]","826bc42a":"pred_df = pd.DataFrame(data = None, columns = ['Sl no.','LSTM model predictions', 'Actual lap time'])\nfor i in range(0, train_data_for_lstm.shape[0]):\n    temp = pd.concat([pd.DataFrame(train_indices[i,:,0]), pd.DataFrame(y_pred[i,:,0]), pd.DataFrame(y_train[i,:,0])], axis = 1)\n    temp.columns = ['Sl no.','LSTM model predictions','Actual lap time']\n    temp = temp[temp['Sl no.']!=0].reset_index(drop = True)\n    pred_df = pd.concat([pred_df, temp], axis = 0).reset_index(drop = True)\nprint(pred_df.shape)\npred_df.head()","5e9baf01":"print('RMSLE: ', np.sqrt(mean_squared_log_error(pred_df['Actual lap time'], pred_df['LSTM model predictions'])))","89fd0fb7":"y_pred = model_lstm.predict(X_valid, batch_size=BATCH_SIZE)\npred_df = pd.DataFrame(data = None, columns = ['Sl no.','LSTM model predictions', 'Actual lap time'])\nfor i in range(0, valid_data_for_lstm.shape[0]):\n    temp = pd.concat([pd.DataFrame(valid_indices[i,:,0]), pd.DataFrame(y_pred[i,:,0]), pd.DataFrame(y_valid[i,:,0])], axis = 1)\n    temp.columns = ['Sl no.','LSTM model predictions','Actual lap time']\n    temp = temp[temp['Sl no.']!=0].reset_index(drop = True)\n    pred_df = pd.concat([pred_df, temp], axis = 0).reset_index(drop = True)\nprint(pred_df.shape)\npred_df.head()","be17eed2":"print('RMSLE: ', np.sqrt(mean_squared_log_error(pred_df['Actual lap time'], pred_df['LSTM model predictions'])))","98e97745":"X_test = test_data_for_lstm[cols_for_lstm]\narray_temp = np.array(list(X_test.iloc[0].to_numpy())).reshape(timesteps, X_test.shape[1])\nfor i in range(1, X_test.shape[0]):\n    array_temp_1 = np.array(list(X_test.iloc[i].to_numpy())).reshape(timesteps, X_test.shape[1])\n    array_temp = np.dstack((array_temp,array_temp_1))\narray_temp = array_temp.reshape(X_test.shape[0], timesteps, X_test.shape[1])\nX_test = array_temp.copy()\n\ntest_indices = test_data_for_lstm[['Sl no.']]\narray_temp = list(np.array(list(test_indices.iloc[0,:].to_numpy())).reshape(1,timesteps, test_indices.shape[1]))\nfor i in range(1, test_indices.shape[0]):\n    array_temp_1 = list(np.array(list(test_indices.iloc[i].to_numpy())).reshape(1,timesteps, test_indices.shape[1]))\n    array_temp += array_temp_1\ntest_indices = np.array(array_temp).copy()","1c9cbba0":"y_pred = model_lstm.predict(X_test, batch_size=BATCH_SIZE)\npred_df = pd.DataFrame(data = None, columns = ['Sl no.','LSTM model predictions'])\nfor i in range(0, test_data_for_lstm.shape[0]):\n    temp = pd.concat([pd.DataFrame(test_indices[i,:,0]), pd.DataFrame(y_pred[i,:,0])], axis = 1)\n    temp.columns = ['Sl no.','LSTM model predictions']\n    temp = temp[temp['Sl no.']!=0].reset_index(drop = True)\n    pred_df = pd.concat([pred_df, temp], axis = 0).reset_index(drop = True)\nprint(pred_df.shape)\npred_df.head()","c033e9c5":"print(test_data.shape)\ntest_data = test_data.merge(pred_df, how = 'left', on = 'Sl no.')\nprint(test_data.shape)\nprint(test_data['LSTM model predictions'].isnull().sum())\ntest_data[['Sl no.','LSTM model predictions']].head()","85cc9fed":"test_data = test_data.sort_values(by = ['Sl no.'], ascending = True).reset_index(drop = True)","5c9ad03d":"submission_data['LAP_TIME'] = test_data['LSTM model predictions']\nsubmission_data.to_csv('my_submission_lstm.csv', index=False)\nprint(submission_data.shape)\ndisplay(submission_data.head())","e121610b":"print(submission_data.shape)\nsubmission_data.head()","5e357497":"plt.figure(figsize=(5, 5))\nplt.hist(train_data_for_modelling[\"LAP_TIME\"]);","c6231504":"plt.figure(figsize=(5, 5))\nplt.hist(submission_data[\"LAP_TIME\"]);","ff933b9e":"## Sectors in a race","3ba03a9b":"## Catboost","6f0cfcdd":"## Distribution of numerical features","acbc3e75":"# EDA","80461da3":"## XGBoost","c84018de":"## H20.ai","ef1e6e7e":"CROSSING_FINISH_LINE_IN_PIT is 'B' when a lap is finiised just at pit","6405ba66":"Level of data is not a single column. The same driver gets at max 2 chances to complete a lap.","01dd559a":"Driver 'N\u00ac\u00faL' not in test data","c45faa86":"Only 1 driver number (driver number: 1) in train and test data","bd96172e":"## Distribution by team","69465a17":"## Distribution by drivers","6172fa91":"Free practice events are not in test data","f21a67b0":"## Lap improvement","7ff75507":"## Fill rate across columns","14d6deb5":"# Dare in reality hackathon","f44c1bc4":"## RidgeCV","6611d27a":"## Distriubtion by event","9808c382":"## Combine weather data with data containing target (lap time)","db827bb3":"High % nulls in 3 columns:\n>- CROSSING_FINISH_LINE_IN_PIT:    87.991436 %\n>- PIT_TIME                    :    81.529778 %\n>- POWER                        :   71.749708 %","409a2994":"## Level of data","4286f62a":"Reference: https:\/\/www.liveabout.com\/formula1-timing-screens-explained-1347397","02f67c1b":"Team 'DRAGON\u221a\u00c9' not in test data","c90dde52":"## Perceptron","9302abb0":"## Distribution by group","a67e416f":"# Load data","4ac52491":"## Challenge details\n### Hackathon Starts\n> Datasets will be made live on 08th November, at 06:00 PM IST\n\n### Dataset Description\n>- train.csv - 10276 rows x 25 columns (Includes target column as LAP_TIME)\nAttributes\n>>- NUMBER: Number in sequence\n>>- DRIVER_NUMBER: Driver number\n>>- LAP_NUMBER: lap number\n>>- LAP_TIME: Lap time in seconds\n>>- LAP_IMPROVEMENT: Number of Lap Improvement\n>>- CROSSING_FINISH_LINE_IN_PIT\n>>- S1: Sector 1 in [min:sec.microseconds]\n>>- S1_IMPROVEMENT: Improvement in sector 1\n>>- S2: Sector 2 in [min:sec.microseconds]\n>>- S2_IMPROVEMENT: Improvement in sector 2\n>>- S3: Sector 3 in [min:sec.microseconds]\n>>- S3_IMPROVEMENT: Improvement in sector 3\n>>- KPH: speed in kilometer\/hour\n>>- ELAPSED: Time elapsed in [min:sec.microseconds]\n>>- HOUR: in [min:sec.microseconds]\n>>- S1_LARGE: in [min:sec.microseconds]\n>>- S2_LARGE: in [min:sec.microseconds]\n>>- S3_LARGE: in [min:sec.microseconds]\n>>- DRIVER_NAME: Name of the driver\n>>- PIT_TIME: time taken to car stops in the pits for fuel and other consumables to be renewed or replenished\n>>- GROUP: Group of driver\n>>- TEAM: Team name\n>>- POWER: Brake Horsepower(bhp)\n>>- LOCATION: Location of the event\n>>- EVENT: Free practice or qualifying\n\n>- test.csv - 420 rows x 25 columns(Includes target column as LAP_TIME)\n\n>- submission.csv -Please check the Evaluation section for more details on how to generate a valid submission.\n\n>- Sectors: For timing purposes the lap is split into three sections, each of which is roughly a third of the lap. These sections are officially known as Sector 1, Sector 2 and Sector 3\n\n\n### The challenge is to predict the LAP_TIME for the qualifying groups of location 6, 7 and 8.\n\n### Knowledge and Skills\n>- Multivariate Regression\n>- Big dataset, underfitting vs overfitting\n>- Optimizing RMSLE to generalize well on unseen data\n\n### Final winners will be notified via an email, based on an aggregate score of their private leaderboard rankings.","64445ce8":"52 is the max number of time steps","0ef6c6c7":"Reference for online datasets: https:\/\/theoehrly.github.io\/Fast-F1\/core.html","d8fdee6e":"Only lap numbers 1-5 are in test data. Lap number is based on the round completed specific to a location (lap number 1 can be in multiple locations).","92316a66":"S1 + S2 + S3 is the elapsed time. PIT_TIME is not included in elapsed time. Hour is for the entire race.","7fb87921":"## Insights from EDA:\n>- Only lap numbers 1-5 are in test data. Lap number is based on the round completed specific to a location (lap number 1 can be in multiple locations).\n>- Only 1 driver number (driver number: 1) in train and test data\n>- Driver 'N\u00ac\u00faL' not in test data\n>- Only driver of name 'J' has more than 1 team\n>- Only 3 locations (6, 7 and 8) in test data\n>- Free practice events are not in test data\n>- Location 8 is not in train data. All records in test data is when rain column is 0.\n>- High % nulls in 3 columns:\n>>- CROSSING_FINISH_LINE_IN_PIT:    87.991436 %\n>>- PIT_TIME                    :    81.529778 %\n>>- POWER                        :   71.749708 %\n>- Only 1 qualifying event with 0 as lap time (might be a cancelled race in train data)\n>- Level of data is not a single column. The same driver gets at max 2 chances to complete a lap.\n>- S1 + S2 + S3 is the elapsed time. PIT_TIME is not included in elapsed time. Hour is for the entire race.\n>- CROSSING_FINISH_LINE_IN_PIT is 'B' when a lap is finiised just at pit","1b8e9a62":"## Correlation analysis","2d0c2319":"# Import packages","8ac125b0":"## LSTM","688449b4":"## Looking at the data","598b0758":"Only driver of name 'J' has more than 1 team","18e5b91a":"Location 8 is not in train weather data. For all records in test data: rain column is 0.","7bfe21a4":"# Modelling","99ccf63b":"Only 1 qualifying event with 0 as lap time (might be a cancelled race in train data)","e1324477":"# Predictions","46174b4b":"Only 3 locations (6, 7 and 8) in test data"}}