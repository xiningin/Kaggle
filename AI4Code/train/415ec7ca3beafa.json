{"cell_type":{"d0fc889d":"code","85483c35":"code","f6d96c9a":"code","e65c8485":"code","75333bc9":"code","1e7c0e7e":"code","fb2c8089":"code","f1807efc":"code","e211e75e":"code","363735fc":"code","42a6aeac":"code","54cfdd13":"code","f164be93":"code","361d99ea":"code","42a9761e":"code","8ee936fb":"code","03a6a8a7":"code","71749673":"code","6404ba71":"code","88212fab":"code","09891ab2":"code","1b22a2fe":"code","27b40761":"code","ecd4ba3f":"code","15fac219":"code","862d7b16":"code","1dd24409":"code","85baa944":"code","562075ea":"code","f56068d9":"code","5d104a24":"code","727301ad":"code","96763b76":"code","ad0b3c16":"code","d720d99d":"code","5a4192a5":"code","5320b8c8":"code","e2d99f5c":"code","a3545444":"code","97b8270a":"code","d87ed697":"code","9096acbf":"code","36a793ff":"code","acb41bd9":"code","5c377e73":"markdown","86bbee4a":"markdown","cae953e2":"markdown","d533f950":"markdown","acc0a6e2":"markdown","39ce77b8":"markdown","4aebcc84":"markdown","2f355538":"markdown","c154dbe1":"markdown","2b530aa9":"markdown","00c3ed25":"markdown","1fa8668e":"markdown","337b93d7":"markdown","c30b718e":"markdown","b6411cd2":"markdown","a6ef3b5d":"markdown","426c69f7":"markdown","d2f6aeec":"markdown","f72661e3":"markdown","014fea88":"markdown","9484bc13":"markdown","3c8a7c46":"markdown","8129eae9":"markdown","00b99322":"markdown","94f5311a":"markdown","02cc224d":"markdown","fd4bcc04":"markdown"},"source":{"d0fc889d":"import os, sys\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","85483c35":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","f6d96c9a":"df.head(2)","e65c8485":"df.isnull().sum()","75333bc9":"df.dropna(subset=['Embarked'], inplace=True)","1e7c0e7e":"df.isnull().sum()","fb2c8089":"df.drop(columns=['PassengerId', 'Name', 'Ticket'], inplace=True)","f1807efc":"df.info()","e211e75e":"df['Cabin'] = df['Cabin'].isnull().astype(int)\ndf['Cabin'].value_counts()","363735fc":"df.head(2)","42a6aeac":"def summarize_categoricals(df, show_levels=False):\n    \"\"\"\n        Display uniqueness in each column\n    \"\"\"\n    data = [[df[c].unique(), len(df[c].unique()), df[c].isnull().sum()] for c in df.columns]\n    df_temp = pd.DataFrame(data, index=df.columns,\n                           columns=['Levels', 'No. of Levels', 'No. of Missing Values'])\n    return df_temp.iloc[:, 0 if show_levels else 1:]\n\n\ndef to_categorical(columns, df):\n    \"\"\"\n        Converts the columns passed in `columns` to categorical datatype\n    \"\"\"\n    for col in columns:\n        df[col] = df[col].astype('category')\n    return df","54cfdd13":"summarize_categoricals(df, show_levels=True)","f164be93":"df = to_categorical(['Survived', 'Pclass', 'Sex',\n                     'SibSp', 'Parch', 'Cabin', 'Embarked'], df)","361d99ea":"df.info()","42a9761e":"x = df.iloc[:, 1:]\ny = df['Survived']\n\ncategorical_columns = list(x.select_dtypes(include='category').columns)\nnumeric_columns = list(x.select_dtypes(exclude='category').columns)","8ee936fb":"categorical_columns, numeric_columns","03a6a8a7":"from sklearn.model_selection import train_test_split\n\ndata_splits = train_test_split(x, y, test_size=0.25, random_state=0,\n                               shuffle=True, stratify=y)\nx_train, x_test, y_train, y_test = data_splits\n\nlist(map(lambda x: x.shape, [x, y, x_train, x_test, y_train, y_test]))","71749673":"pd.Series(y_test).value_counts()","6404ba71":"sns.countplot(x=y_test);","88212fab":"df.isnull().sum()","09891ab2":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline \n\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(drop='first', dtype=np.int))\n])\n\n## Column Transformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_columns),\n        ('cat', categorical_transformer, categorical_columns)\n    ])\n\n\n## Applying Column Transformer\nx_train = preprocessor.fit_transform(x_train)\nx_test = preprocessor.transform(x_test)\n\n\n## Label encoding\ny_trans = LabelEncoder()\ny_train = y_trans.fit_transform(y_train)\ny_test = y_trans.transform(y_test)\n\n\n## Save feature names after one-hot encoding for feature importances plots\nfeature_names = list(preprocessor.named_transformers_['cat'].named_steps['onehot'] \\\n                            .get_feature_names(input_features=categorical_columns))\nfeature_names = feature_names + numeric_columns","1b22a2fe":"import timeit\nimport pickle\nimport sys\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, \\\n                            precision_recall_curve, roc_curve, accuracy_score\nfrom sklearn.exceptions import NotFittedError","27b40761":"def confusion_plot(matrix, labels=None):\n    \"\"\" Display binary confusion matrix as a Seaborn heatmap \"\"\"\n    \n    labels = labels if labels else ['Negative (0)', 'Positive (1)']\n    \n    fig, ax = plt.subplots(nrows=1, ncols=1)\n    sns.heatmap(data=matrix, cmap='Blues', annot=True, fmt='d',\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel('PREDICTED')\n    ax.set_ylabel('ACTUAL')\n    ax.set_title('Confusion Matrix')\n    plt.close()\n    \n    return fig","ecd4ba3f":"def roc_plot(y_true, y_probs, label, compare=False, ax=None):\n    \"\"\" Plot Receiver Operating Characteristic (ROC) curve \n        Set `compare=True` to use this function to compare classifiers. \"\"\"\n    \n    fpr, tpr, thresh = roc_curve(y_true, y_probs,\n                                 drop_intermediate=False)\n    auc = round(roc_auc_score(y_true, y_probs), 2)\n    \n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1)\n    label = ' '.join([label, f'({auc})']) if compare else None\n    sns.lineplot(x=fpr, y=tpr, ax=axis, label=label)\n    \n    if compare:\n        axis.legend(title='Classifier (AUC)', loc='lower right')\n    else:\n        axis.text(0.72, 0.05, f'AUC = { auc }', fontsize=12,\n                  bbox=dict(facecolor='green', alpha=0.4, pad=5))\n            \n        # Plot No-Info classifier\n        axis.fill_between(fpr, fpr, tpr, alpha=0.3, edgecolor='g',\n                          linestyle='--', linewidth=2)\n        \n    axis.set_xlim(0, 1)\n    axis.set_ylim(0, 1)\n    axis.set_title('ROC Curve')\n    axis.set_xlabel('False Positive Rate [FPR]\\n(1 - Specificity)')\n    axis.set_ylabel('True Positive Rate [TPR]\\n(Sensitivity or Recall)')\n    \n    plt.close()\n    \n    return axis if ax else fig","15fac219":"def precision_recall_plot(y_true, y_probs, label, compare=False, ax=None):\n    \"\"\" Plot Precision-Recall curve.\n        Set `compare=True` to use this function to compare classifiers. \"\"\"\n    \n    p, r, thresh = precision_recall_curve(y_true, y_probs)\n    p, r, thresh = list(p), list(r), list(thresh)\n    p.pop()\n    r.pop()\n    \n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1)\n    \n    if compare:\n        sns.lineplot(r, p, ax=axis, label=label)\n        axis.set_xlabel('Recall')\n        axis.set_ylabel('Precision')\n        axis.legend(loc='lower left')\n    else:\n        sns.lineplot(thresh, p, label='Precision', ax=axis)\n        axis.set_xlabel('Threshold')\n        axis.set_ylabel('Precision')\n        axis.legend(loc='lower left')\n\n        axis_twin = axis.twinx()\n        sns.lineplot(thresh, r, color='limegreen', label='Recall', ax=axis_twin)\n        axis_twin.set_ylabel('Recall')\n        axis_twin.set_ylim(0, 1)\n        axis_twin.legend(bbox_to_anchor=(0.24, 0.18))\n    \n    axis.set_xlim(0, 1)\n    axis.set_ylim(0, 1)\n    axis.set_title('Precision Vs Recall')\n    \n    plt.close()\n    \n    return axis if ax else fig","862d7b16":"def feature_importance_plot(importances, feature_labels, ax=None):\n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1, figsize=(5, 10))\n    sns.barplot(x=importances, y=feature_labels, ax=axis)\n    axis.set_title('Feature Importance Measures')\n    \n    plt.close()\n    \n    return axis if ax else fig","1dd24409":"def train_clf(clf, x_train, y_train, sample_weight=None, refit=False):\n    train_time = 0\n    \n    try:\n        if refit:\n            raise NotFittedError\n        y_pred_train = clf.predict(x_train)\n    except NotFittedError:\n        start = timeit.default_timer()\n        \n        if sample_weight is not None:\n            clf.fit(x_train, y_train, sample_weight=sample_weight)\n        else:\n            clf.fit(x_train, y_train)\n        \n        end = timeit.default_timer()\n        train_time = end - start\n        \n        y_pred_train = clf.predict(x_train)\n    \n    train_acc = accuracy_score(y_train, y_pred_train)\n    return clf, y_pred_train, train_acc, train_time","85baa944":"def model_memory_size(clf):\n    return sys.getsizeof(pickle.dumps(clf))","562075ea":"def report(clf, x_train, y_train, x_test, y_test, display_scores=[],\n           sample_weight=None, refit=False, importance_plot=False,\n           confusion_labels=None, feature_labels=None, verbose=True):\n    \"\"\" Trains the passed classifier if not already trained and reports\n        various metrics of the trained classifier \"\"\"\n    \n    dump = dict()\n    \n    ## Train if not already trained\n    clf, train_predictions, \\\n    train_acc, train_time = train_clf(clf, x_train, y_train,\n                                      sample_weight=sample_weight,\n                                      refit=refit)\n    ## Testing\n    start = timeit.default_timer()\n    test_predictions = clf.predict(x_test)\n    end = timeit.default_timer()\n    test_time = end - start\n    \n    test_acc = accuracy_score(y_test, test_predictions)\n    y_probs = clf.predict_proba(x_test)[:, 1]\n    \n    roc_auc = roc_auc_score(y_test, y_probs)\n        \n    ## Additional scores\n    scores_dict = dict()\n    for func in display_scores:\n        scores_dict[func.__name__] = [func(y_train, train_predictions),\n                                      func(y_test, test_predictions)]\n        \n    ## Model Memory\n    model_mem = round(model_memory_size(clf) \/ 1024, 2)\n    \n    print(clf)\n    print(\"\\n=============================> TRAIN-TEST DETAILS <======================================\")\n    \n    ## Metrics\n    print(f\"Train Size: {x_train.shape[0]} samples\")\n    print(f\" Test Size: {x_test.shape[0]} samples\")\n    print(\"---------------------------------------------\")\n    print(f\"Training Time: {round(train_time, 3)} seconds\")\n    print(f\" Testing Time: {round(test_time, 3)} seconds\")\n    print(\"---------------------------------------------\")\n    print(\"Train Accuracy: \", train_acc)\n    print(\" Test Accuracy: \", test_acc)\n    print(\"---------------------------------------------\")\n    \n    if display_scores:\n        for k, v in scores_dict.items():\n            score_name = ' '.join(map(lambda x: x.title(), k.split('_')))\n            print(f'Train {score_name}: ', v[0])\n            print(f' Test {score_name}: ', v[1])\n            print()\n        print(\"---------------------------------------------\")\n    \n    print(\" Area Under ROC (test): \", roc_auc)\n    print(\"---------------------------------------------\")\n    print(f\"Model Memory Size: {model_mem} kB\")\n    print(\"\\n=============================> CLASSIFICATION REPORT <===================================\")\n    \n    ## Classification Report\n    clf_rep = classification_report(y_test, test_predictions, output_dict=True)\n    \n    print(classification_report(y_test, test_predictions,\n                                target_names=confusion_labels))\n    \n    \n    if verbose:\n        print(\"\\n================================> CONFUSION MATRIX <=====================================\")\n    \n        ## Confusion Matrix HeatMap\n        display(confusion_plot(confusion_matrix(y_test, test_predictions),\n                               labels=confusion_labels))\n        print(\"\\n=======================================> PLOTS <=========================================\")\n\n\n        ## Variable importance plot\n        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\n        roc_axes = axes[0, 0]\n        pr_axes = axes[0, 1]\n        importances = None\n\n        if importance_plot:\n            if not feature_labels:\n                raise RuntimeError(\"'feature_labels' argument not passed \"\n                                   \"when 'importance_plot' is True\")\n\n            try:\n                importances = pd.Series(clf.feature_importances_,\n                                        index=feature_labels) \\\n                                .sort_values(ascending=False)\n            except AttributeError:\n                try:\n                    importances = pd.Series(clf.coef_.ravel(),\n                                            index=feature_labels) \\\n                                    .sort_values(ascending=False)\n                except AttributeError:\n                    pass\n\n            if importances is not None:\n                # Modifying grid\n                grid_spec = axes[0, 0].get_gridspec()\n                for ax in axes[:, 0]:\n                    ax.remove()   # remove first column axes\n                large_axs = fig.add_subplot(grid_spec[0:, 0])\n\n                # Plot importance curve\n                feature_importance_plot(importances=importances.values,\n                                        feature_labels=importances.index,\n                                        ax=large_axs)\n                large_axs.axvline(x=0)\n\n                # Axis for ROC and PR curve\n                roc_axes = axes[0, 1]\n                pr_axes = axes[1, 1]\n            else:\n                # remove second row axes\n                for ax in axes[1, :]:\n                    ax.remove()\n        else:\n            # remove second row axes\n            for ax in axes[1, :]:\n                ax.remove()\n\n\n        ## ROC and Precision-Recall curves\n        clf_name = clf.__class__.__name__\n        roc_plot(y_test, y_probs, clf_name, ax=roc_axes)\n        precision_recall_plot(y_test, y_probs, clf_name, ax=pr_axes)\n\n        fig.subplots_adjust(wspace=5)\n        fig.tight_layout()\n        display(fig)\n    \n    ## Dump to report_dict\n    dump = dict(clf=clf, accuracy=[train_acc, test_acc], **scores_dict,\n                train_time=train_time, train_predictions=train_predictions,\n                test_time=test_time, test_predictions=test_predictions,\n                test_probs=y_probs, report=clf_rep, roc_auc=roc_auc,\n                model_memory=model_mem)\n    \n    return clf, dump","f56068d9":"def compare_models(y_test=None, clf_reports=[], labels=[], score='accuracy'):\n    \"\"\" Compare evaluation metrics for the True Positive class [1] of \n        binary classifiers passed in the argument and plot ROC and PR curves.\n        \n        Arguments:\n        ---------\n        y_test: to plot ROC and Precision-Recall curves\n         score: is the name corresponding to the sklearn metrics\n        \n        Returns:\n        -------\n        compare_table: pandas DataFrame containing evaluated metrics\n                  fig: `matplotlib` figure object with ROC and PR curves \"\"\"\n\n    \n    ## Classifier Labels\n    default_names = [rep['clf'].__class__.__name__ for rep in clf_reports]\n    clf_names =  labels if len(labels) == len(clf_reports) else default_names\n    \n    \n    ## Compare Table\n    table = dict()\n    index = ['Train ' + score, 'Test ' + score, 'Overfitting', 'ROC Area',\n             'Precision', 'Recall', 'F1-score', 'Support']\n    for i in range(len(clf_reports)):\n        scores = [round(i, 3) for i in clf_reports[i][score]]\n        \n        roc_auc = clf_reports[i]['roc_auc']\n        \n        # Get metrics of True Positive class from sklearn classification_report\n        true_positive_metrics = list(clf_reports[i]['report'][\"1\"].values())\n        \n        table[clf_names[i]] = scores + [scores[1] < scores[0], roc_auc] + \\\n                              true_positive_metrics\n    \n    table = pd.DataFrame(data=table, index=index)\n    \n    \n    ## Compare Plots\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n    \n    # ROC and Precision-Recall\n    for i in range(len(clf_reports)):\n        clf_probs = clf_reports[i]['test_probs']\n        roc_plot(y_test, clf_probs, label=clf_names[i],\n                 compare=True, ax=axes[0])\n        precision_recall_plot(y_test, clf_probs, label=clf_names[i],\n                              compare=True, ax=axes[1])\n    # Plot No-Info classifier\n    axes[0].plot([0,1], [0,1], linestyle='--', color='green')\n        \n    fig.tight_layout()\n    plt.close()\n    \n    return table.T, fig","5d104a24":"from xgboost import XGBClassifier\nfrom sklearn.utils import class_weight\nfrom sklearn import metrics","727301ad":"primary_eval_metric = metrics.f1_score\nconfusion_lbs = ['Not Survived', 'Survived']\n\n## Compute `class_weights` using sklearn\ncls_weight = (y_train.shape[0] - np.sum(y_train)) \/ np.sum(y_train)","96763b76":"xgb_clf_default = XGBClassifier(scale_pos_weight=cls_weight,\n                                random_state=0, n_jobs=-1)\nxgb_clf_default.fit(x_train, y_train);\n\nxgb_clf_default, xgb_report_default = report(xgb_clf_default, x_train, y_train,\n                                             x_test, y_test,\n                                             display_scores=[primary_eval_metric],\n                                             importance_plot=True,\n                                             feature_labels=feature_names,\n                                             confusion_labels=confusion_lbs)","ad0b3c16":"import xgboost as xgb\nfrom optuna import create_study, logging\nfrom optuna.pruners import MedianPruner\nfrom optuna.integration import XGBoostPruningCallback\n\n\ndef objective(trial, X, y, group, score, params=dict()):\n    dtrain = xgb.DMatrix(X, label=y)\n    class_weight = (y.shape[0] - np.sum(y)) \/ np.sum(y)\n    \n    ## Initial Learning Parameters\n    params['learning_rate'] = 0.1\n    params['num_boost_round'] = 1000\n\n    if group == '1':\n        params['max_depth'] = trial.suggest_int('max_depth', 2, 10)\n        params['min_child_weight'] = trial.suggest_loguniform('min_child_weight',\n                                                              1e-10, 1e10)\n    \n    if group == '2':\n        params['subsample'] = trial.suggest_uniform('subsample', 0, 1)\n        params['colsample_bytree'] = trial.suggest_uniform('colsample_bytree', 0, 1)\n    \n    if group == '3':\n        params['learning_rate'] = trial.suggest_uniform('learning_rate', 0, 0.1)\n        params['num_boost_round'] = trial.suggest_int('num_boost_round', 100, 1000)\n\n    pruning_callback = XGBoostPruningCallback(trial, \"test-\" + score.__name__)\n    cv_scores = xgb.cv(params, dtrain, nfold=5,\n                       stratified=True,\n                       feval=score,\n                       early_stopping_rounds=10,\n                       callbacks=[pruning_callback],\n                       seed=0)\n\n    return cv_scores['test-' + score.__name__ + '-mean'].values[-1]\n\n\ndef execute_optimization(study_name, group, score, trials,\n                         params=dict(), direction='maximize'):\n    logging.set_verbosity(logging.ERROR)\n    \n    ## We use pruner to skip trials that are NOT fruitful\n    pruner = MedianPruner(n_warmup_steps=5)\n    \n    study = create_study(direction=direction,\n                         study_name=study_name,\n                         storage='sqlite:\/\/\/optuna.db',\n                         load_if_exists=True,\n                         pruner=pruner)\n\n    study.optimize(lambda trial: objective(trial, x_train, y_train,\n                                           group, score, params),\n                   n_trials=trials,\n                   n_jobs=-1)\n    \n    \n    print(\"STUDY NAME: \", study_name)\n    print('------------------------------------------------')\n    print(\"EVALUATION METRIC: \", score.__name__)\n    print('------------------------------------------------')\n    print(\"BEST CV SCORE\", study.best_value)\n    print('------------------------------------------------')\n    print(f\"OPTIMAL GROUP - {group} PARAMS: \", study.best_params)\n    print('------------------------------------------------')\n    print(\"BEST TRIAL\", study.best_trial)\n    print('------------------------------------------------')\n    \n    \n    return study.best_params","d720d99d":"score_func = metrics.f1_score\ndef score_function(y_pred, dtrain):\n    y_pred = (y_pred > 0.5).astype(int)\n    y_true = (dtrain.get_label() > 0.5).astype(int)\n    return score_func.__name__, score_func(y_true, y_pred)\n\nscore_function.__name__ = score_func.__name__","5a4192a5":"def stepwise_optimization(trials=10):\n    final_params = dict()\n    for g in ['1', '2', '3']:\n        print(f\"=========================== Optimizing Group - {g} ============================\")\n        update_params = execute_optimization('xgboost', g, score_function, trials,\n                                             params=final_params, direction='maximize')\n        final_params.update(update_params)\n        print(f\"PARAMS after optimizing GROUP - {g}: \", final_params)\n        print()\n        print()\n\n    print(\"=========================== FINAL OPTIMAL PARAMETERS ============================\")\n    print(final_params)\n    \n    return final_params","5320b8c8":"params = stepwise_optimization()","e2d99f5c":"params","a3545444":"xgb_clf_tuned_1 = XGBClassifier(**params, scale_pos_weight=cls_weight,\n                                random_state=0, n_jobs=-1)\nxgb_clf_tuned_1.fit(x_train, y_train);\n\nxgb_clf_tuned_1, xgb_report_tuned_1 = report(xgb_clf_tuned_1, x_train, y_train,\n                                             x_test, y_test,\n                                             display_scores=[primary_eval_metric],\n                                             importance_plot=True,\n                                             feature_labels=feature_names,\n                                             confusion_labels=confusion_lbs)","97b8270a":"params = stepwise_optimization(trials=50)","d87ed697":"params","9096acbf":"xgb_clf_tuned_2 = XGBClassifier(**params, scale_pos_weight=cls_weight,\n                                random_state=0, n_jobs=-1)\nxgb_clf_tuned_2.fit(x_train, y_train);\n\nxgb_clf_tuned_2, xgb_report_tuned_2 = report(xgb_clf_tuned_2, x_train, y_train,\n                                             x_test, y_test,\n                                             display_scores=[primary_eval_metric],\n                                             importance_plot=True,\n                                             feature_labels=feature_names,\n                                             confusion_labels=confusion_lbs)","36a793ff":"report_list = [xgb_report_default, xgb_report_tuned_1, xgb_report_tuned_2]\nclf_labels = ['XGBoost with default params',\n              'XGBoost after 10 trials',\n              'XGBoost after 50 more trials',]","acb41bd9":"compare_table, compare_plot = compare_models(y_test, clf_reports=report_list,\n                                             labels=clf_labels,\n                                             score=primary_eval_metric.__name__)\n\ncompare_table","5c377e73":"<a id=\"modifying-%60cabin%60-feature\"><\/a>\n## 1.3.  Modifying `Cabin` feature\nSince there are several null values in the `cabin` feature, we will modify the features values as missing (1) and not missing (0).","86bbee4a":"Let's run the XGB classifier using the obtained optimal hyperparameters.","cae953e2":"<a id=\"what-is-optuna%3F\"><\/a>\n# 6. What is Optuna?\nOptuna is one of the optimization libraries that use Sequential Model Based Optimization (SMBO). Conventional optimization approaches like Grid search and Random search do not make use of the information available regarding the previously explored hyperparameter search space. Unlike those techniques, SMBO techniques use the historical exploration information to make a more informed decision regarding the direction in which the optimizer should explore next in the search space. Such a mechanism gives SMBO techniques an edge over the conventional approaches in terms of convergence speed. \n\n**Advantages of SMBO:** \n> 1. Requires fewer iterations\/ trials to converge, even if the search space is very huge.\n>\n> 2. Search is more focused in the region of the search space that is closer to the true optimal point.\n>\n> 3. Facilitates pausing and continuing the optimization process. \n\nSome examples of SMBO based optimization libraries:- Skopt, Hyperopt, Optuna \n\n**Advantages of Optuna over Hyperopt and Skopt:**\n> 1. Provides support storing the optimization information in a database. Hence, facilitates saving and resuming the optimization process, unlike Hyperopt and Skopt.\n>\n> 2. The interface and documentation is more mature than Skopt and Hyperopt.","d533f950":"<a id=\"stepwise-hyperparameter-tuning\"><\/a>\n# 7. Stepwise Hyperparameter Tuning\nThe stepwise algorithm for XGBoost hyperparameter tuning is inspired by a similar algorithm for LightGBM explained in [this post](https:\/\/medium.com\/optuna\/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258). \n\nThe most commonly used and the most effective XGBoost parameters are split into 3 groups: \n\n> **GROUP 1:** max_depth , min_child_weight \n>\n> **GROUP 2:** subsample, colsample_bytree \n>\n> **GROUP 3:** learning_rate, num_boost_round \n\nInitially, `learning_rate` and `num_boost_round` are fixed at 0.1 and 1000 respectively.\n\nEach of these groups of hyperparameters are tuned sequentially. While tuning a particular group, all the subsequent groups are fixed at default or initial values and all the preceding groups are fixed at the values obtained after the tuning process. For example, by the time execution reachs GROUP 2, GROUP 1 is already tuned so we will fix GROUP 1 at the optimal values obtained, while the parameters in the subsequent groups (only GROUP 3 in this case) are left default or at the intialized values (0.1 and 1000 in this case) since they still need to be tuned. \n\nThe benefit of stepwise tuning is that the hyperparameter space is narrowed down to the group being tuned. In conventional tuning methods, we tune all the hyperparameters togeather which requires searching through a larger space. For instance, in this case we have 6 hyperparameters, tuning all of them together will involve searching through a 6 dimensional space. However, if stepwise algorithm is used, we will have to search a space of only 2 dimensions at once which is way more efficient and faster than searching through a larger space. ","acc0a6e2":"<a id=\"deleting-features\"><\/a>\n## 1.2.   Deleting features","39ce77b8":"Finding categorical features and converting their pandas *dtype* to `categorical` will ease visualization","4aebcc84":"<a id=\"resuming-the-optimization-process-in-optuna\"><\/a>\n## 7.1. Resuming the optimization process in Optuna\nSince the classifier is still overfitting, we can ***continue\/ resume*** the optimization process just by running the optimization process again on the same study by passing the study name and storage location to the **study_name** and **storage** arguments of the `create_study` method (see `execute_optimization` function).\n\nSince I am calling the from the `execute_optimization` function from the `stepwise_optimization` function, we just have to execute the `stepwise_optimization` function once more but with more trails.","2f355538":"<a id=\"import-and-clean-data\"><\/a>\n# 1.   Import and Clean Data","c154dbe1":"**Highlights:**\n> 1. Demonstrate hyperparameter tuning using Optuna with an example.\n>\n> 2. Stepwise hyperparameter tuning of XGBoost classifier.\n>\n> 3. Demonstrate save and resume feature of Optuna\n>\n> 4. Hyperparameter tuning based on F1-score","2b530aa9":"<a id=\"xgboost-with-default-parameters\"><\/a>\n# 5. XGBoost with default parameters","00c3ed25":"Let's run the XGB classifier using the obtained optimal hyperparameters and check if it is outside the overfitting region.","1fa8668e":"Since `PassengerId`, `Name`, and `Ticket` columns do not provide any relevant information in predicting the survival of a passenger, we can delete the columns.","337b93d7":"As we can see, the XGBoost classifier in its default configuration is overfitting in terms of F1-score.","c30b718e":"Since XGBoost does not have F1-score as one of the evaluation metrics, we will have a pass a custom F1-score function in the `feval=` parameter of the `xgb.cv` method.","b6411cd2":"We need to impute values in `Age`.","a6ef3b5d":"<a id=\"find-categorical-columns-and-change-their-%2Adtype%2A-from-%60object%60-to-%60categorical%60\"><\/a>\n## 1.4.   Find categorical columns and change their *Dtype* from `object` to `Categorical`","426c69f7":"# Stepwise Hyperparameter Tuning of XGBoost Classifier using Optuna","d2f6aeec":"<a id=\"train-test-split\"><\/a>\n## 2.1.   Train-Test split","f72661e3":"Since the dataset is imbalanced we will be using class-weighted\/ cost-sensitive learning. In cost-sensitive learning, a weighted cost function is used. Therefore, misclassifying a sample from the minority class will cost the classifiers more than misclassifying a sample from the majority class. In most of the Sklearn classifiers, cost-sensitive learning can be enabled by setting `class_weight='balanced'`.","014fea88":"<a id=\"evaluation-metric\"><\/a>\n# 4. Evaluation metric\nSince the titanic dataset is imbalanced, F1-score is a more appropriate metric than accuracy. Therefore, we will use F1-score as the evaluation metric while performing hyperparameter tuning of XGBoost classifier using Optuna.","9484bc13":"<a id=\"data-preprocessing\"><\/a>\n# 2.   Data Preprocessing","3c8a7c46":"<a id=\"model-comparison\"><\/a>\n# 8. Model Comparison","8129eae9":"Thank You!!","00b99322":"* [1.   Import and Clean Data](#import-and-clean-data)\n    * [1.1.   Missing values](#missing-values)\n    * [1.2.   Deleting features](#deleting-features)\n    * [1.3.  Modifying `Cabin` feature](#modifying-%60cabin%60-feature)\n    * [1.4.   Find categorical columns and change their *Dtype* from `object` to `Categorical`](#find-categorical-columns-and-change-their-%2Adtype%2A-from-%60object%60-to-%60categorical%60)\n* [2.   Data Preprocessing](#data-preprocessing)\n    * [2.1.   Train-Test split](#train-test-split)\n    * [2.2.   One-hot Encoding and Standardization](#one-hot-encoding-and-standardization)\n* [3.   Utility Functions](#utility-functions)\n* [4. Evaluation metric](#evaluation-metric)\n* [5. XGBoost with default parameters](#xgboost-with-default-parameters)\n* [6. What is Optuna?](#what-is-optuna%3F)\n* [7. Stepwise Hyperparameter Tuning](#stepwise-hyperparameter-tuning)\n    * [7.1. Resuming the optimization process in Optuna](#resuming-the-optimization-process-in-optuna)\n* [8. Model Comparison](#model-comparison)","94f5311a":"<a id=\"utility-functions\"><\/a>\n# 3.   Utility Functions","02cc224d":"<a id=\"missing-values\"><\/a>\n## 1.1.   Missing values","fd4bcc04":"<a id=\"one-hot-encoding-and-standardization\"><\/a>\n## 2.2.   One-hot Encoding and Standardization\nWe need to standardize the continuous or quantitative variables\/ features before applying Machine Learning models. This is important because if we don't standardize the features, features with high variance that are orders of magnitude larger that others might dominate the model fitting process and causing the model unable to learn from other features (with lower variance) correctly as expected. <br\/>\nThere is no need to standardize categorical variables.\n\n***Also we need to standardize the data only after performing train-test split because if we standardize before splitting then there is a chance for some information leak from the test set into the train set. We always want the test set to be completely new to the ML models. [Read more](https:\/\/scikit-learn.org\/stable\/modules\/compose.html#columntransformer-for-heterogeneous-data)***"}}