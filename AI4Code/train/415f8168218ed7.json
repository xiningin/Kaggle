{"cell_type":{"ffd3dc97":"code","17fa96c7":"code","1ad8b457":"code","170645c5":"code","903be229":"code","b1d041cc":"code","aaa3f60d":"code","0a588e36":"code","7ac60bd1":"code","8a455b33":"code","59949dbb":"code","cc8e0bb0":"code","0c84fe82":"code","fc52b498":"code","c0ed18d7":"code","b7e8637d":"code","6443842b":"code","a38425f5":"code","f6a1ff9f":"code","885ef2c4":"code","2ab6ab00":"code","ece23bcc":"code","d0ef2672":"code","f6cafaae":"code","5e00eb9a":"code","95426a74":"code","6d8dc143":"code","2bc17c30":"code","1ac14378":"code","b41ceeab":"code","7df36af2":"code","bc6c9d7e":"code","568fe12c":"code","eb7412c1":"code","f733d60a":"code","6857bee6":"code","f1a87456":"code","42a1c339":"markdown","0039d58d":"markdown","760ecc78":"markdown","5d3b915e":"markdown","b39f38dc":"markdown","3b8119fd":"markdown","07cdf59d":"markdown","e2ac2006":"markdown","06617e46":"markdown","4951f41b":"markdown","1feb8961":"markdown","2d3aa7d2":"markdown","6da461d7":"markdown","f84fbe98":"markdown","369150b0":"markdown","e2abc788":"markdown","6216992e":"markdown","997ddbe7":"markdown","f0989bfb":"markdown","1b3c9d10":"markdown","e30194c2":"markdown","d3fb96f7":"markdown","321bb059":"markdown"},"source":{"ffd3dc97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","17fa96c7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\n\nimport lightgbm as lgb\n\nfrom fastai import *\nfrom fastai.tabular import *","1ad8b457":"# lgb.__version__","170645c5":"data = pd.read_csv('\/kaggle\/input\/data co-lab engineering department test\/data_co_lab_engineering_dataset.csv')","903be229":"# data = data.head(2000)\ndata.head()","b1d041cc":"data.info()","aaa3f60d":"# These are some categorical features\nprint(data.nunique())\nprint(data['Browser'].unique())\n# print(data['OperatingSystems'].unique())\n# print(data['Region'].unique())\n# print(data['TrafficType'].unique())","0a588e36":"data.describe()","7ac60bd1":"_ = sns.countplot(data['Revenue'], palette='Set3')","8a455b33":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(4,3,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(4,3,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","59949dbb":"t0 = data.loc[data['Revenue'] == True]\nt1 = data.loc[data['Revenue'] == False]\n## keep only numerical columns\ncat_columns =  ['Month', 'Browser', 'OperatingSystems', 'Region', 'TrafficType', 'VisitorType', 'Weekend']\ncon_columns = [col for col in data.columns.values if col not in cat_columns]\nplot_feature_distribution(t0, t1, 'True', 'False', con_columns)","cc8e0bb0":"correlations = data[con_columns].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\n## the least correlated columns\ncorrelations.head(10)","0c84fe82":"## the most correlated columns\ncorrelations.tail(10)","fc52b498":"# from sklearn.decomposition import PCA\n\n# ## Use PCA to reduce the dimensionality : keep only 10 columns\n# pca = PCA(n_components=10)\n# X = pca.fit_transform(X)\n# print 'Shape of X after PCA to the first 10 dimensions: ', X.shape\n# Shape of X after PCA to the first 10 dimensions:  (1000, 10)\n# In [6]:\n# ## PCA can also be used to visualize the data\n# plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.7)\n# plt.show()","c0ed18d7":"## determine the list of the columns that contains missing values\nmissing_cols = data.columns[data.isna().any()].tolist()\n## show the dtype of this columns\ndata[missing_cols].info()","b7e8637d":"## Imputation techniques: https:\/\/towardsdatascience.com\/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779\n\ndata_to_impute = data[missing_cols]\nimp_mean = SimpleImputer( strategy='mean') #for median imputation replace 'mean' with 'median'\nimputed_data = imp_mean.fit_transform(data_to_impute)\ndata[missing_cols] = pd.DataFrame(imputed_data, columns = missing_cols)\ndata.info()","6443842b":"## one hot encoding\ny = pd.DataFrame()\ny['Revenue'] = data['Revenue']\ndata_copy = data.drop(['Revenue'], axis = 1)\n## convert the type of Weekend column since get_dummies converts only columns with object or category dtype\n# data_copy['Weekend'] = data_copy['Weekend'].astype(str)\n# print(data_copy.info())\nX = pd.get_dummies(data_copy, columns = cat_columns)\nprint(X.shape)\nX.columns","a38425f5":"# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# X_normalized = pd.DataFrame(scaler.fit_transform(X))\n# X_normalized.columns = X.columns\n# X_normalized.head()","f6a1ff9f":"# label encoding of the target column so we convert False to 0 and True to 1\n\nle = LabelEncoder()\ny['Revenue'] = le.fit_transform(y['Revenue'])\ny.head()","885ef2c4":"### Split the data to train and test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nX_train.shape","2ab6ab00":"### Train a logistic regression\nlr = LogisticRegression(solver = 'lbfgs')\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\n# evaluate the model\n## accuracy\nprint('The accuracy of our logistic regression model is {}'.format(accuracy_score(y_test, y_pred)))\n## confusion matrix\ncm = confusion_matrix(y_true= y_test, y_pred=y_pred)\ncm_df = pd.DataFrame(cm,\n                     index = ['False','True'], \n                     columns = ['False','True'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('Logistic regression \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","ece23bcc":"## just used some frequently used parameters for gradient boosting.\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary',  ## binary classification problem\n    'verbosity': 1,\n    'is_unbalance ': True,\n}","d0ef2672":"## We use cross validation with 10 folds\nfolds = StratifiedKFold(n_splits=10, shuffle=False, random_state=44000)\noof = np.zeros(len(X_train))\npredictions = np.zeros(len(X_test))\nfeatures = X_train.columns.values\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train.values, y_train.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(X_train.iloc[trn_idx][features], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(X_train.iloc[val_idx][features], label=y_train.iloc[val_idx])\n\n    num_round = 1000000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(X_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(X_test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y_train, oof)))","f6cafaae":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:10].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,5))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()","5e00eb9a":"y_pred = np.round(predictions)\nprint('The accuracy of our gradient boosting model is {}'.format(accuracy_score(y_test, y_pred)))\n## confusion matrix\ncm = confusion_matrix(y_true= y_test, y_pred=y_pred)\ncm_df = pd.DataFrame(cm,\n                     index = ['False','True'], \n                     columns = ['False','True'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('Gradient Boosting \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","95426a74":"### maybe we have to use a different threshhold other than 0.5\nthresh = 0.4\ny_pred = np.array([1 if x >thresh else 0 for x in list(predictions)])\nprint('The accuracy of our gradient boosting model is {}'.format(accuracy_score(y_test, y_pred)))\n## confusion matrix\ncm = confusion_matrix(y_true= y_test, y_pred=y_pred)\ncm_df = pd.DataFrame(cm,\n                     index = ['False','True'], \n                     columns = ['False','True'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('Gradient Boosting \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","6d8dc143":"data_copy = data.copy()\nle = LabelEncoder()\ndata_copy['Revenue'] = le.fit_transform(data_copy['Revenue'])\nX_train, X_test, y_train, y_test = train_test_split(data_copy, y, test_size = 0.2, random_state = 0)\nX_test.drop(['Revenue'], axis = 1, inplace = True)\n\n## the dependent variable\ndep_var = 'Revenue'\n## catergorical column names\ncat_names = cat_columns\n## continuous column names\ncont_names = [col for col in con_columns if col != 'Revenue']\n\n## here we specify the preprocessing that must be done\n## FillMissing to fill the null values\n## Categorify to one hot encode categorical columns\n## Normalize which is very important in neural network to get a faster gradient decent.\nprocs = [FillMissing, Categorify, Normalize]\n\nprint(\"Categorical columns are : \", cat_names)\nprint('Continuous numerical columns are :', cont_names)","2bc17c30":"X_train.head()","1ac14378":"## construct the test and train TabularList instances\ntest = TabularList.from_df(X_test, cat_names=cat_names, cont_names=cont_names)\n\ntrain = (TabularList.from_df(X_train, path='.', cat_names=cat_names, cont_names=cont_names, procs=procs)\n                        .random_split_by_pct(valid_pct=0.2, seed=43)\n                        .label_from_df(cols=dep_var)\n                        .add_test(test, label=0)\n                        .databunch())","b41ceeab":"train.show_batch(rows=10)","7df36af2":"learn = tabular_learner(train, layers=[1000,500], metrics=accuracy)","bc6c9d7e":"learn.fit_one_cycle(5, 2.5e-2)","568fe12c":"learn.lr_find()\nlearn.recorder.plot()","eb7412c1":"learn.unfreeze()\nlearn.fit_one_cycle(20, slice(3e-3))","f733d60a":"preds, _ = learn.get_preds(ds_type=DatasetType.Test)\npred_prob, pred_class = preds.max(1)","6857bee6":"y_pred = pred_class.numpy()\nprint('The accuracy of our neural network model is {}'.format(accuracy_score(y_test, y_pred)))\n## confusion matrix\ncm = confusion_matrix(y_true= y_test, y_pred=y_pred)\ncm_df = pd.DataFrame(cm,\n                     index = ['False','True'], \n                     columns = ['False','True'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('Neural network \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","f1a87456":"## version of fastai\nprint(__version__)","42a1c339":"### Data cleaning\n\nSince we have some missing values we will replace them with other values: Imputation.","0039d58d":"Maybe we can do some data augmentation to add more features that may better the results.","760ecc78":"### Read the data","5d3b915e":"### Feature extraction (dimensionality reduction)","b39f38dc":"We can notice that the best value of learning rate that we best choose is 3e-06","3b8119fd":"We can see that they are all numeric columns so we can use sklearn Imputer to repalce missing values with the mean which the most used replacement values.","07cdf59d":"### Density plots of features\n\nWe plot the density of variables in our data. We compare the disribution of variables in both target values 0 and 1.","e2ac2006":"Standarization of the data : When we have features with different scaling and magniture we standardize to prevent getting a feature \nmore importance just for having bigger values. But we loose in terms of coefficients interpretation if we use logistic regression. \n\nI will use it only in the neural network model ","06617e46":"### Grandient Boosting\n\nThis is the most used and accurate algorithm for tabular problems.","4951f41b":"As we can see here that there are columns that have different distributions for both classes like Informational and the ExitRates. These ones will be very helpful in classifying and in building a model that separates the two classes.","1feb8961":"### Calculate the correlation between variables","2d3aa7d2":"### Logistic regression","6da461d7":"We can see that the neural network gave an accuracy that is comparable to the gradient boosting model. In this neural network I have chosen a random architecture composed only of 2 hidden layers.\n\nI must fine tune the number of hidden layers layers and the number of nodes.\n\nOther things that must be done are regularisation and data augmentation.\n\nAn other technique is embedding of categorical features as explained here https:\/\/www.fast.ai\/2018\/04\/29\/categorical-embeddings\/","f84fbe98":"### EDA","369150b0":"So now we have no more missing values.","e2abc788":"We notice here a lot (270) of false negative predictions and this is due to our imbalanced data. Maybe we can try Random Over-Sampling to sample more data with the class True but may cause overfitting. And we have to use auc metric as it is the most significant with imbalanced data.","6216992e":"### Use Neural Networks","997ddbe7":"### Data preprocessing\n\nIn this section we will prepare the data for training. We will convert categorical data to numerical ones. And for this we will use one hot encoding since it is the most accurate technique and don't add noise to our data.","f0989bfb":"### Data augmentation","1b3c9d10":"So here we have null values that we have to either delete their corresponding rows or replace them.","e30194c2":"We can see that there are few columns which aree too correlated. ","d3fb96f7":"As we can see here the data is imbalanced, we have more rows with False revenue.","321bb059":"I have to do some parameter tuning to find the optimal parameters of gradient boosting algorithm."}}