{"cell_type":{"385a3657":"code","54d950ef":"code","7198a6d0":"code","3a5670fb":"code","c44c615d":"code","520a745c":"code","22cc44d9":"code","ea4bb019":"code","469212e3":"code","77eb26da":"code","c3b07b71":"code","975f817e":"code","f7fe3f80":"code","39a81b3c":"code","b9b8ed5f":"code","eb43de46":"code","fb4e74be":"code","e1673bf1":"code","1afc1ef9":"code","8d6d24a8":"code","33c39a1d":"code","39f1f299":"code","c56bf63e":"code","f7fe1771":"code","7f97c277":"markdown"},"source":{"385a3657":"import pickle\nimport pandas as pd\nhpa_df = pd.read_pickle(\"..\/input\/gtdataframes\/dataset_224.pkl\")","54d950ef":"hpa_df","7198a6d0":"LABELS= {\n0: \"Nucleoplasm\",\n1: \"Nuclear membrane\",\n2: \"Nucleoli\",\n3: \"Nucleoli fibrillar center\",\n4: \"Nuclear speckles\",\n5: \"Nuclear bodies\",\n6: \"Endoplasmic reticulum\",\n7: \"Golgi apparatus\",\n8: \"Intermediate filaments\",\n9: \"Actin filaments\",\n10: \"Microtubules\",\n11: \"Mitotic spindle\",\n12: \"Centrosome\",\n13: \"Plasma membrane\",\n14: \"Mitochondria\",\n15: \"Aggresome\",\n16: \"Cytosol\",\n17: \"Vesicles and punctate cytosolic patterns\",\n18: \"Negative\"\n}","3a5670fb":"import tensorflow as tf\nprint(tf.__version__)\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nimport tensorflow_addons as tfa\n\nimport os\nimport re\nimport cv2\nimport glob\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom functools import partial\nimport matplotlib.pyplot as plt","c44c615d":"IMG_WIDTH = 224\nIMG_HEIGHT = 224\nBATCH_SIZE = 16\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","520a745c":"#%%script echo skipping\n#set an amount of folds to split dataframe into --> k-fold cross validation\n# explanation: https:\/\/towardsdatascience.com\/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85\nN_FOLDS=5\n#choose which one of the 5 folds will be used as validation set this time\ni_VAL_FOLD=1\nhpa_df=np.array_split(hpa_df, N_FOLDS+1) #add one extra part for testing set\ndf_test_split=hpa_df[-1]\nhpa_df=hpa_df[:-1]\ni_training = [i for i in range(N_FOLDS)]\ni_training.pop(i_VAL_FOLD-1)\ni_validation=i_VAL_FOLD-1\ndf_train_split=list()\nfor i in i_training:\n    df_train_split.append(hpa_df[i])\ndf_train_split=pd.concat(df_train_split)\ndf_val_split=hpa_df[i_validation]","22cc44d9":"print(len(df_train_split))\nprint(len(df_val_split))\nprint(len(df_test_split))","ea4bb019":"#%%script echo skipping\n#analyze class imbalance and set up class weights here\n#https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/improve-class-imbalance-class-weights\/\ny_train=df_train_split[\"Label\"].apply(lambda x:list(map(int, x.split(\"|\"))))\ny_train=y_train.values\ny_train=np.concatenate(y_train)\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)","469212e3":"#%%script echo skipping\ntmp_dict={}\nfor i in range(len(LABELS)):\n    tmp_dict[i]=class_weights[i]\nclass_weights=tmp_dict\nclass_weights","77eb26da":"#%%script echo skipping\n@tf.function\ndef multiple_one_hot(cat_tensor, depth_list):\n    \"\"\"Creates one-hot-encodings for multiple categorical attributes and\n    concatenates the resulting encodings\n\n    Args:\n        cat_tensor (tf.Tensor): tensor with mutiple columns containing categorical features\n        depth_list (list): list of the no. of values (depth) for each categorical\n\n    Returns:\n        one_hot_enc_tensor (tf.Tensor): concatenated one-hot-encodings of cat_tensor\n    \"\"\"\n    one_hot_enc_tensor = tf.one_hot(cat_int_tensor[:,0], depth_list[0], axis=1)\n    for col in range(1, len(depth_list)):\n        add = tf.one_hot(cat_int_tensor[:,col], depth_list[col], axis=1)\n        one_hot_enc_tensor = tf.concat([one_hot_enc_tensor, add], axis=1)\n    return one_hot_enc_tensor\n\n@tf.function\ndef load_image(df_dict):\n    # Load image\n    rgb = tf.io.read_file(df_dict['path'])\n    image = tf.image.decode_png(rgb, channels=3)\n    #https:\/\/medium.com\/@kyawsawhtoon\/a-tutorial-to-histogram-equalization-497600f270e2\n    image=tf.image.per_image_standardization(image)\n    \n    # Parse label\n    label = tf.strings.split(df_dict['Label'], sep='|')\n    label = tf.strings.to_number(label, out_type=tf.int32)\n    label = tf.reduce_sum(tf.one_hot(indices=label, depth=19), axis=0)\n    \n    return image, label","c3b07b71":"#%%script echo skipping\ntrain_ds = tf.data.Dataset.from_tensor_slices(dict(df_train_split))\nval_ds = tf.data.Dataset.from_tensor_slices(dict(df_val_split))\n\n# Training Dataset\ntrain_ds = (\n    train_ds\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n# Validation Dataset\nval_ds = (\n    val_ds\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)","975f817e":"#%%script echo skipping\ndef get_label_name(labels):\n    l = np.where(labels == 1.)[0]\n    label_names = []\n    for label in l:\n        label_names.append(LABELS[label])\n        \n    return '-'.join(str(label_name) for label_name in label_names)\n\ndef show_batch(image_batch, label_batch):\n  plt.figure(figsize=(20,20))\n  for n in range(10):\n      ax = plt.subplot(5,5,n+1)\n      plt.imshow(image_batch[n])\n      plt.title(get_label_name(label_batch[n].numpy()))\n      plt.axis('off')","f7fe3f80":"#%%script echo skipping\n# Training batch\nimage_batch, label_batch = next(iter(train_ds))\nshow_batch(image_batch, label_batch)\n#print(label_batch)","39a81b3c":"#%%script echo skipping\n\ndef get_model():\n    base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet')\n    base_model.trainable = True\n\n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\n    x = base_model(inputs, training=True)\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(0.4)(x)\n    outputs = Dense(len(LABELS), activation='sigmoid')(x)\n    \n    return Model(inputs, outputs)\n\ntf.keras.backend.clear_session()\nmodel = get_model()\nmodel.summary()","b9b8ed5f":"#%%script echo skipping\ntime_stopping_callback = tfa.callbacks.TimeStopping(seconds=int(round(60*60*8)), verbose=1) #8h\n\nearlystopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=10, verbose=0, mode='min',\n    restore_best_weights=True\n)\n\n#https:\/\/keras.io\/api\/optimizers\/learning_rate_schedules\/exponential_decay\/\ninitial_learning_rate = 0.1\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=1000,\n    decay_rate=0.96,\n    staircase=True)","eb43de46":"#%%script echo skipping\n#set up checkpoint save\n#source:https:\/\/www.tensorflow.org\/tutorials\/keras\/save_and_load\n!pip install -q pyyaml h5py\nimport os\ncheckpoint_path = \".\/cp.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)","fb4e74be":"#https:\/\/mmuratarat.github.io\/2020-01-25\/multilabel_classification_metrics\n#define metrics\n\nimport sklearn.metrics\n\nemr=sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n\nhl=sklearn.metrics.hamming_loss(y_true, y_pred)\n\n#\"samples\" applies only to multilabel problems. It does not calculate a per-class measure, instead calculating the metric over the true and predicted classes \n#for each sample in the evaluation data, and returning their (sample_weight-weighted) average.\n\nr=sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, average='samples')\n\np=sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, average='samples')\nf=sklearn.metrics.f1_score(y_true=y_true, y_pred=y_pred, average='samples')","e1673bf1":"#%%script echo skipping\nimport keras.backend as K\nK_epsilon = K.epsilon()\ndef f1(y_true, y_pred):\n    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), 0.5), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K_epsilon)\n    r = tp \/ (tp + fn + K_epsilon)\n\n    f1 = 2*p*r \/ (p+r+K_epsilon)\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\ndef f1_loss(y_true, y_pred):\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K_epsilon)\n    r = tp \/ (tp + fn + K_epsilon)\n\n    f1 = 2*p*r \/ (p+r+K_epsilon)\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1-K.mean(f1)","1afc1ef9":"#%%script echo skipping\nimport tensorflow as tf\nimport timeit\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","8d6d24a8":"#%%script echo skipping\n# Initialize model\ntf.keras.backend.clear_session()\nmodel = get_model()\n\n#model.load_weights(checkpoint_path)\n\n# Compile model\nmodel.compile(optimizer='adam', loss=f1_loss, metrics=f1)\n\n\nmodel.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),\n              loss=f1_loss,\n              metrics=['accuracy',f1,'AUC',emr,hl,r,p,f])\n\n# Train\nhistory=model.fit(train_ds,\n                  epochs=1000,\n                  validation_data=val_ds,\n                  class_weight=class_weights,\n                  callbacks=[cp_callback,earlystopper,time_stopping_callback])","33c39a1d":"#%%script echo skipping\nhistory.history","39f1f299":"#%%script echo skipping\n#source: https:\/\/machinelearningmastery.com\/display-deep-learning-model-training-history-in-keras\/\n# list all data in history\nprint(history.history.keys())\n# summarize history for f1\nplt.plot(history.history['f1'])\nplt.plot(history.history['val_f1'])\nplt.title('f1')\nplt.ylabel('f1')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for auc\nplt.plot(history.history['auc'])\nplt.plot(history.history['val_auc'])\nplt.title('AUC')\nplt.ylabel('AUC')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","c56bf63e":"with open('.\/historyhistory.pkl', 'wb') as handle:\n    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)","f7fe1771":"with open('.\/history.pkl', 'wb') as handle:\n    pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)","7f97c277":"#### this notebook is part of the documentation on my HPA approach  \n    -> main notebook: https:\/\/www.kaggle.com\/philipjamessullivan\/0-hpa-approach-summary\n\n\n## 7: network training\n    -> https:\/\/www.kaggle.com\/philipjamessullivan\/7-train-effnetb0-version-a-part-1  \n    -> https:\/\/www.kaggle.com\/philipjamessullivan\/7-train-effnetb1-version-a-part-1  \n    -> https:\/\/www.kaggle.com\/philipjamessullivan\/7-train-effnetb2-version-a-part-1  \n    -> https:\/\/www.kaggle.com\/philipjamessullivan\/7-train-effnetb3-version-a-part-1  \n    \n##### Network Architecture: Efficientnet (B0 to B3) with a Dropout layer of 0.3  \n\n#### Goal: further refine model  \n\n##### INPUT: \n**resized cropped RGB cell images**:  \n\nEfficientnetB0 -> 224x224  \nEfficientnetB1 -> 240x240  \nEfficientnetB2 -> 260x260  \nEfficientnetB3 -> 300x300  \n\n**specific subset of ground truth dataframe**:  \n\nColums: img_id, Label, Cell#, bbox coords, path  \n\n##### OUTPUT:\n**model checkpoints**:  \n\none after each epoch\n\n**training history**:  \n\nshown as printed array on screen with f1 loss and f1 score per step"}}