{"cell_type":{"96804a7f":"code","f6cf541c":"code","0e3e3db8":"code","846dc31e":"code","1573d8c8":"code","7e8dc6a5":"code","f2488fa4":"code","a4bbe49e":"code","1ace1dd9":"code","c2d97405":"code","207b5362":"code","5a5b6c29":"code","89f66710":"code","2a232100":"code","752f6234":"code","8304c7cc":"code","34f15dce":"code","37ed29d4":"code","42f77365":"code","77d07284":"code","16799e26":"code","9a0daa2f":"code","a886b86f":"code","69f9688d":"code","4e83fd66":"code","1cd52319":"code","72712a7f":"code","992748dd":"code","da22fb65":"code","4b072c0c":"code","5d5de363":"code","d2f5666f":"code","c727f7d5":"code","490fe3f4":"code","1f429490":"code","6224ab39":"code","deeb0fee":"code","0fb42556":"code","7789ebeb":"code","f8271183":"code","243b1a8b":"code","c622985f":"code","77d8eee2":"code","620f04b5":"code","1544c77f":"code","38e0a2de":"code","8754d0f1":"code","59f069e9":"code","e4880de0":"code","b9ae9fe7":"code","605ab2e0":"code","f837cc0f":"code","9cadf1a8":"code","8a615a58":"code","b67fdba9":"code","b5f75cf6":"code","e4749730":"markdown","820b8693":"markdown","7ff8e6cc":"markdown","014293a9":"markdown","385a9206":"markdown","9eac3307":"markdown","f1419ecb":"markdown","dd530ed0":"markdown","f526d76e":"markdown","3da6dea3":"markdown","1eb38135":"markdown","dff83978":"markdown","0d02d078":"markdown","0416a994":"markdown","a96435d4":"markdown","6ce7d727":"markdown","f4822931":"markdown","d192fdfb":"markdown","a59ba4c2":"markdown","85f9fd78":"markdown","c4dbe965":"markdown","cf034648":"markdown","f1cfc969":"markdown","f2039751":"markdown","357db155":"markdown","a90d06c9":"markdown","51e673e8":"markdown","ab126dac":"markdown","d60cba69":"markdown","dad92ad8":"markdown","250787f0":"markdown","02a17b97":"markdown","2e4c04aa":"markdown","29b64bfd":"markdown","a1e45c7d":"markdown","7fb082c1":"markdown","513af0d9":"markdown","251ebd23":"markdown"},"source":{"96804a7f":"import math\nimport calendar\n\nimport pandas as pd\nimport missingno as msno\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nfrom pylab import rcParams","f6cf541c":"google = pd.read_csv('..\/input\/stock-time-series-20050101-to-20171231\/GOOGL_2006-01-01_to_2018-01-01.csv')\ngoogle","0e3e3db8":"google = pd.read_csv('..\/input\/stock-time-series-20050101-to-20171231\/GOOGL_2006-01-01_to_2018-01-01.csv', \n                     index_col = 'Date',\n                    )\ngoogle.head()","846dc31e":"google.index.dtype","1573d8c8":"google = pd.read_csv('..\/input\/stock-time-series-20050101-to-20171231\/GOOGL_2006-01-01_to_2018-01-01.csv', \n                     index_col = 'Date',\n                     parse_dates=['Date'], # parse 'Date' column as a datetime type\n                    )\ngoogle.head()","7e8dc6a5":"google.index.dtype","f2488fa4":"humidity = pd.read_csv('..\/input\/historical-hourly-weather-data\/humidity.csv')\nhumidity","a4bbe49e":"humidity = pd.read_csv('..\/input\/historical-hourly-weather-data\/humidity.csv', \n                       index_col = 'datetime')\nhumidity.head()","1ace1dd9":"humidity.index.dtype","c2d97405":"humidity = pd.read_csv('..\/input\/historical-hourly-weather-data\/humidity.csv', \n                       index_col = 'datetime',\n                       parse_dates = ['datetime'],\n                      )\nhumidity.head()","207b5362":"humidity.index.dtype","5a5b6c29":"google.isna().sum()","89f66710":"humidity.isna().sum()","2a232100":"total = humidity.index.size\nnull_sum = humidity.isna().sum()[humidity.isna().sum() != 0]\n\npd.DataFrame(data={'Amount of null values': null_sum,\n                   'Percentage of null values': (null_sum \/ total*100).round(2).astype(str) + '%'},\n            ).sort_values(by=['Percentage of null values'])","752f6234":"msno.bar(humidity)","8304c7cc":"msno.heatmap(humidity, cmap='coolwarm')","34f15dce":"msno.matrix(humidity)","37ed29d4":"plt.figure(figsize=(20,10))\n# 13:70 chosen to see humidity for 2 full days\ny = np.array(humidity[['Houston', 'Los Angeles']])[13:70] \nx = np.array(humidity.index)[13:70]\nplt.plot(x, y)","42f77365":"plt.figure(figsize=(20,10))\n# 0:700 chosen to see humidity for 1 full month\ny = np.array(humidity[['Houston', 'Los Angeles']])[0:700]\nx = np.array(humidity.index)[0:700]\nplt.plot(x, y)","77d07284":"plt.figure(figsize=(20,10))\n# 0:2100 chosen to see humidity for 3 full months\ny = np.array(humidity[['Houston', 'San Francisco']])[0:2100]\nx = np.array(humidity.index)[0:2100]\nplt.plot(x, y)","16799e26":"humidity.iloc[0] # humidity for first date","9a0daa2f":"humidity.iloc[-1] # humidity for last date","a886b86f":"humidity = humidity.fillna(method='bfill')\nhumidity.head()","69f9688d":"humidity.iloc[-1]","4e83fd66":"humidity = humidity.fillna(method='ffill')\nhumidity.head()","1cd52319":"humidity.isna().sum()","72712a7f":"google.duplicated().sum()","992748dd":"humidity.duplicated().sum()","da22fb65":"plt.figure(figsize=(20,10))\nplt.plot(google.index, google['High'], 'r-.')\nplt.title(\"Google\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"High stock prices\")","4b072c0c":"google_144_months = google[['High']].groupby(pd.Grouper(freq='M')).sum()\ngoogle_144_months = google_144_months.rename(columns={'High': \"Total monthly prices\"})\ngoogle_144_months","5d5de363":"google_january = google_144_months.groupby(google_144_months.index.month).groups[1]\ngoogle_january","d2f5666f":"months = [i for i in range(1,13)]\nmonth_names = []\nfor i in months:\n    month_names.append(calendar.month_name[i])\nmonth_names","c727f7d5":"dicts = {}\nkeys = month_names\nvalues = [\"Hi\", \"I\", \"am\", \"John\"]\nfor i in keys:\n        dicts[i] = values[i]\nprint(dicts)","490fe3f4":"google_january[1].date().strftime('%Y-%m-%d')","1f429490":"jan_google_dates = []\nfor i in range(12):\n    jan_google_dates.append((google_january[i].date().strftime('%Y-%m-%d')))\njan_google_dates","6224ab39":"jan_google_prices = []\nfor i in jan_google_dates:\n    jan_google_prices.append(google_144_months['Total monthly prices'][google_144_months.index==i])\njan_google_prices[0].values[0]","deeb0fee":"prices = []\nfor i in range(12):\n    prices.append(jan_google_prices[i].values[0])\nprices","0fb42556":"def index_slice(part):\n    full_size = google.index.size\n    years = 12\n    days_in_year = full_size \/\/ years\n    if part == 1:\n        start = 0\n        end = days_in_year\n    if part != 1:\n        start = index_slice(part-1)[1]\n        end = index_slice(part-1)[1] + days_in_year\n    return [start, end]","7789ebeb":"# example of above function\nindex_slice(1) ","f8271183":"def years_slice(part):\n    years = range(2006, 2019) # Google prices starts from 2006 to 2018\n    return [years[part-1], years[part]] # part-1 because part constists of 1...12","243b1a8b":"# example of above function\nyears_slice(1)","c622985f":"def axes_slice(i):\n    if i%2 == 0:\n        y = 1\n        x = i\/\/2-1\n    else:\n        y = 0\n        x = math.floor(i\/\/2)\n    return [x, y]","77d8eee2":"axes_slice(6)","620f04b5":"fig, axs = plt.subplots(6,2, figsize=(15, 10))\nfor i in range(1,13):\n    axs[int(axes_slice(i)[0]),\n        int(axes_slice(i)[1])].plot(google.index[\n                                        int(index_slice(i)[0]):\n                                        int(index_slice(i)[1])], \n                                    google['High'][\n                                        int(index_slice(i)[0]):\n                                        int(index_slice(i)[1])],\n                                   )\n    axs[int(axes_slice(i)[0]),\n        int(axes_slice(i)[1])].set_title(f'Google in {years_slice(i)[0]}')\n\n\n\nfor ax in axs.flat:\n    ax.set(xlabel='Time', ylabel='High stock prices')\n\n# hide x labels and tick labels for top plots and y ticks for right plots\nfor ax in axs.flat:\n    ax.label_outer()","1544c77f":"def three_point_MA(data):\n    list_ = [np.nan] # np.nan because we aren't able to calculate MA for first price\n    for i in range(1, data.index.size-1):\n        ma = (data.iloc[i-1] + data.iloc[i] + data.iloc[i+1])\/3\n        list_.append(ma)\n    list_.append(np.nan) # np.nan because we aren't able to calculate MA for last price\n    return list_","38e0a2de":"google_high_trend = pd.DataFrame(data = {\n                        'High': google['High'],\n                        'Trend': three_point_MA(google['High']),\n})\n\ngoogle_high_trend","8754d0f1":"google_high_and_var = pd.DataFrame(data = {\n    'High': google['High'],\n    'Variance': google['High'].var(),\n})\ngoogle_high_and_var","59f069e9":"fig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(google_high_trend['High'], c='b', ls='--', label='High')\nax.plot(google_high_trend['Trend'], c='r', label = 'Trend')\nplt.legend(loc=2)\nplt.show()","e4880de0":"def seas_var_around_trend(data):\n    list_ = [np.nan] # np.nan because we aren't able to find diff between first item and previous one\n    for i in range(1, data.index.size):\n        var_around_trend = (data.iloc[i] - google_high_trend['Trend'].iloc[i])\n        list_.append(var_around_trend)\n    return list_","b9ae9fe7":"google_statistics = pd.DataFrame(data = {\n                        'High': google['High'],\n                        'Trend': three_point_MA(google['High']),\n                        'Seasonal variation around trend' : seas_var_around_trend(google['High']),\n})\n\ngoogle_statistics","605ab2e0":"fig, axs = plt.subplots(6,2, figsize=(15, 10))\nfor i in range(1,13):\n    axs[int(axes_slice(i)[0]),\n        int(axes_slice(i)[1])].plot(google.index[\n                                        int(index_slice(i)[0]):\n                                        int(index_slice(i)[1])], \n                                    google_statistics['Seasonal variation around trend'][\n                                        int(index_slice(i)[0]):\n                                        int(index_slice(i)[1])],\n                                   )\n    axs[int(axes_slice(i)[0]),\n        int(axes_slice(i)[1])].set_title(f'Seasonal variation around trend of Google\\'s prices in {years_slice(i)[0]}')\n\n\n\nfor ax in axs.flat:\n    ax.set(xlabel='Time', ylabel='Seasonal var around trend')\n\n# hide x labels and tick labels for top plots and y ticks for right plots\nfor ax in axs.flat:\n    ax.label_outer()","f837cc0f":"def daily_perc_change(data):\n    list_ = [np.nan] # np.nan because we aren't able to find diff between first item and previous one\n    for i in range(1, data.index.size):\n        daily_perc_change = (data.iloc[i]-data.iloc[i-1])\/data.iloc[i-1]\n        list_.append(daily_perc_change)\n    return list_","9cadf1a8":"google_statistics = pd.DataFrame(data = {\n                        'High': google['High'],\n                        'Daily % change': daily_perc_change(google['High']),\n})\n\ngoogle_statistics","8a615a58":"fig, axs = plt.subplots(6,2, figsize=(15, 10))\nfor i in range(1,13):\n    axs[int(axes_slice(i)[0]),\n        int(axes_slice(i)[1])].plot(google.index[\n                                        int(index_slice(i)[0]):\n                                        int(index_slice(i)[1])], \n                                    google_statistics['Daily % change'][\n                                        int(index_slice(i)[0]):\n                                        int(index_slice(i)[1])],\n                                   )\n    axs[int(axes_slice(i)[0]),\n        int(axes_slice(i)[1])].set_title(f'Daily percent change of Google\\'s prices in {years_slice(i)[0]}')\n\n\n\nfor ax in axs.flat:\n    ax.set(xlabel='Time', ylabel='% Change')\n\n# hide x labels and tick labels for top plots and y ticks for right plots\nfor ax in axs.flat:\n    ax.label_outer()","b67fdba9":"rcParams['figure.figsize'] = 11, 9\ndecomposed_google_volume = sm.tsa.seasonal_decompose(google[\"High\"], \n                                                     period=360,\n                                                     model='additive',\n                                                    ) \nfigure = decomposed_google_volume.plot()\nplt.show()","b5f75cf6":"rcParams['figure.figsize'] = 11, 9\ndecomposed_google_volume = sm.tsa.seasonal_decompose(google[\"High\"], \n                                                     period=360,\n                                                     model='multiplicative',\n                                                    ) \nfigure = decomposed_google_volume.plot()\nplt.show()","e4749730":"It's hard to see season variations here, so above data has been divided into 12 months.","820b8693":"#### 4.1.1.1 [Run-sequence plot](https:\/\/en.wikipedia.org\/wiki\/Seasonality#Detection)","7ff8e6cc":"## 2. Data inputs and first transformations","014293a9":"### Trend and seasonality\nThe main difference between trend and seasonality is repeting over time. Both are linear \/ nonlinear component that changes over time [but seasonality is repeating and trend is not](https:\/\/towardsdatascience.com\/trend-seasonality-moving-average-auto-regressive-model-my-journey-to-time-series-data-with-edc4c0c8284b).\n\n\n\nTo find trend and seasonality, we will perfom [**time-series decomposition**](https:\/\/machinelearningmastery.com\/decompose-time-series-data-trend-seasonality\/).","385a9206":"First data input are Google Stocks data.","9eac3307":"We need to convert index column to datatime type as well, as we did it in Google dataset.","f1419ecb":"We can notice that seasonal variation doesn't increase \/ decrease over time and **additive type can be prooved**.","dd530ed0":"### Imputation methods for time-series\n\nBasic imputation methods for time-series are:\n* last observation carried forward ([can introduce bias when we have visible trend](https:\/\/towardsdatascience.com\/how-to-handle-missing-data-8646b18db0d4)) --> [ffil](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.fillna.html)\n* next observation carried backward (can introduce bias when we have visible trend) --> bfill\n* interpolation (when we have trend without seasonality)\n* interpolation with seasonal adjustment (when we have trend and seasonality)","f526d76e":"## 4. Time-series decomposition\n[**Components** of time-series are](https:\/\/machinelearningmastery.com\/decompose-time-series-data-trend-seasonality\/):\n* **systematic** (have consistency and can be modeled):\n    * **trend** - increasing \/ decreasing \/ constant tendency in the series\n    * **seasonality** - repeating short-term cycle in the series. Seasonality are marked by **frequency** (frequency of seasons are width of cycles) and **amplitude** (heigth of cycles).\n* **non-systematic** (cannot be directly modeled):\n    * **noise** - the remainder component.\n\nNoise is a characteristc of every time-series, in contrast to trend and seasonality.\n\nDepending on how components are linked to each other [we distinguish](https:\/\/otexts.com\/fpp2\/components.html):\n* **additive model** (linear)   \n* * $y_t =  trend + seasonality + noise$\n* * If the magnitude of the seasonal fluctuations or variation around the trend, **doesn't vary with the level** of time series (are relatively constant over time). ([Level is the average value in the series.](https:\/\/medium.com\/@sigmundojr\/seasonality-in-python-additive-or-multiplicative-model-d4b9cf1f48a7))\n* **multiplicative model** (non-linear)   \n* * $y_t = trend \\cdot seasonality \\cdot noise$\n* * If mentioned variation and seasonal fluctuation **are proportional to** the **level** of time series. [It means that seasonal variation increases over time](https:\/\/online.stat.psu.edu\/stat510\/book\/export\/html\/667).\n* **pseudo-additive \/ mixed model** ([combining the elements of above both models](https:\/\/digitalcommons.wayne.edu\/cgi\/viewcontent.cgi?article=2030&context=jmasm))\n* * $y_t = trend \\cdot seasonality + noise$","3da6dea3":"### 4.1 [Seasonality](https:\/\/digitalcommons.wayne.edu\/cgi\/viewcontent.cgi?article=2030&context=jmasm)\n### 4.1.1 Graphical techniques to detect seasonality:\n1. Run-sequence plot\n2. Seasonal sub-series plot \n - assuming that seasonal periods are known. \n  - On vertical axis there is a response variable and on the horizontal axis is time ordered by season. [For example](https:\/\/www.itl.nist.gov\/div898\/handbook\/pmc\/section4\/pmc4431.htm) with monthly data, all the January values are plotted (in chronological order), then all the February values, and so on. A **reference line** is drawn at the group means.\n3. Multiple box plots\n - assuming that seasonal periods are known \n - [preferable](https:\/\/www.itl.nist.gov\/div898\/handbook\/pmc\/section4\/pmc4431.htm) for for large number of observations).\n4. Autocorrelation plot\n - significant seasonality --> plot should show spikes at multiples of lags equal to the period \n - For example, for quarterly data, we'd expect to see significant spikes at lag 4,8,12, and so on*.","1eb38135":"To choose proper method for dealing with missing values, it's important to understand reasons why missing values occurs in the dataset. [Missing values can be](https:\/\/www.researchgate.net\/post\/What_the_difference_between_MAR_MCAR_and_MNAR_in_missing_data_mechanism):\n1. Missing not at Random (MNAR) where missing values **depend on hipothetical value or other variable's value**. For example, people don't want to share their incomes when their salaries are high (income depends on hypothetical high salaries) or females don't want to reveal their age (age depends on gender).\n2. Missing at Random (MAR) where **missingness is conditional on another variable.**\n3. Missing Completely at Random (MCAR) where missing value is not associated with other variables from data set or the missing value itself. **The reason for missing values is e.g. losing by chance.** \n\nTo see dependency between other variables, we will use a [heatmap](https:\/\/towardsdatascience.com\/visualize-missing-values-with-missingno-ad4d938b00a1).","dff83978":"Second data input is humidity around the world.","0d02d078":"### 3.1 Missing values\n[As isnull() is an alias of isna()](https:\/\/github.com\/pandas-dev\/pandas\/blob\/0409521665bd436a10aea7e06336066bf07ff057\/pandas\/core\/dtypes\/missing.py#L109), it is preferred to use isna().","0416a994":"To fill last NaN values in, we will use foward filling now.","a96435d4":"**<M8[ns]** is a specific datatype of [DatetimeArray](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.24.0rc1\/api\/generated\/pandas.arrays.DatetimeArray.html\/).","6ce7d727":"#### Trend\n\nIn order to see the trend in a time series better we can smooth the data by taking averages of a number of observed values at adjacent time periods. This is called the [**moving averages**](https:\/\/iase-web.org\/islp\/apps\/gov_stats_priceindices\/timeseries.htm#). The simplest moving average is a **3-point moving average** where we replace each value in the original series by the mean of the value and the two points either side of it.","f4822931":"As we can see the percentage of blanks is from 0.27% to 4.04%. **Bar chart** below gives a graphical overview of the completeness of the Humidity dataset and **blanks matrix** lets us find patterns.","d192fdfb":"Above we can see humidity for Houston and Los Angeles for 1 month.","a59ba4c2":"In our case, it's hard to conclude what is the reason for the missingness. Because we don't have any other variable except for the humidity, we cannot prove neither MNAR, MAR nor MCAR.","85f9fd78":"Above a humidity for 3 months for Houston and San Francisco. It's hard to notice a trend and seasonability. So the best option seems to be last observation carried forward or next observation carried backward because we don't have visible trends.","c4dbe965":"Let's start with the cities with the lowest number of missing values - Houston and Los Angeles.","cf034648":"#### 4.1.2 [Moving averages](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.seasonal.seasonal_decompose.html#statsmodels.tsa.seasonal.seasonal_decompose)\n\n","f1cfc969":"Since missing values have been replaced in, [we can use *seasonal_decompose()*](https:\/\/www.statsmodels.org\/0.9.0\/_modules\/statsmodels\/tsa\/seasonal.html) method. \n\nAssumptions for using *seasonal_decompose()* that must be satisfied:\n* no missing values --> satisfied\n* [specified type of model](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.seasonal.seasonal_decompose.html) (that is seasonality: additive or multiplicative?) --> not satisfied yet","f2039751":"We can see that there are only positive correlations and strong correlation (<0.9) occurs between:\n\n* Jerusalem and San Francisco, Miami, Nahariyya, Haifa and Beersheba.\n\n* Nahariyya and San Francisco, Eilat and Beersheba.\n\n* Haifa and San Francisco, Eilat and Beersheba.\n\n* Eilat and Beersheba.\n\n* Beersheba and San Francisco.","357db155":"### 4.1.2 Statistics to test seasonality:\n1.  [the Chi-Square (${\\chi}^2$) Goodness-of-Fit test](https:\/\/www.statisticssolutions.com\/chi-square-goodness-of-fit-test\/) is a non-parametric test that is used to find out how the observed value of a given phenomena is significantly different from the expected value.  In Chi-Square goodness of fit test, the term goodness of fit is used to compare the observed sample distribution with the expected probability distribution.  Chi-Square goodness of fit test determines how well theoretical distribution (such as normal, binomial, or Poisson) fits the empirical distribution. In Chi-Square goodness of fit test, sample data is divided into intervals. Then the numbers of points that fall into the interval are compared, with the expected numbers of points in each interval.\n\n$T=\\sum_{k}^{i=1}\\left [ \\frac{\\left ( O_{i}-E_{i} \\right )^{2}}{E_{i}} \\right ]$, \nwhere $O_i$ is observed value for $i=1, ..., k$ season (interval) and $E_i$ is expected frequency at the $i^{th}$ season. ","a90d06c9":"[**Seasonal variation** is a difference between value (in this instance a high price) and trend](http:\/\/www.sanandres.esc.edu.ar\/secondary\/marketing\/page_73.htm). ","51e673e8":"We can see that 10 cities (e.g. Vancouver, San Francisco et al.) have missing values at the end of the dataset.","ab126dac":"Here is a humidity for 2 days and as we can see a humidity is the lowest in the evenings (between 18:00 and 0:00 at nigh).","d60cba69":"## 1. Imports ","dad92ad8":"Hence we will work with **time series**, it's needed to **set 'Date' column as an index** and **convert it into datetime** type.","250787f0":"[For time-series problem](https:\/\/towardsdatascience.com\/how-to-handle-missing-data-8646b18db0d4) **it's essential to take a look at trend and seasonality** and get to know **where blanks occur** (at the begining or at the end). ","02a17b97":"Both datasets for Google and Humidity are void of missing values now.","2e4c04aa":"**4.1.1.2 Seasonal sub-series plot**\n\nLet's group our data into months to detect on average how prices are changing over each month.","29b64bfd":"### 3.2 Duplicated values","a1e45c7d":"Because more missing values occurs at the begining of the dataset rather than at the end, we will choose backward filling.","7fb082c1":"## 3. Cleaing and preparing time series data","513af0d9":"It's still hard to detect seasonal fluctuations, hence we will try another method.","251ebd23":"It's quite normal to have similar humidity measurements in the dataset so we don't have to handle these dupes."}}