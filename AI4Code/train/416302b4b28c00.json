{"cell_type":{"43edf4f0":"code","98aa535b":"code","ed9fdcfa":"code","291b6c3c":"code","e939d596":"code","676d1f1c":"code","59c00e8e":"code","9efb6426":"code","119e1c74":"code","5d2cd5e4":"code","13611d9e":"code","66d33b09":"code","ad3b401b":"code","bac9e271":"code","2375e112":"code","556d3b99":"code","448f841c":"code","ce625775":"code","afe34372":"markdown","ab83922b":"markdown","ba606931":"markdown"},"source":{"43edf4f0":"pip install faker","98aa535b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport re, random\n\nfrom faker import Faker\nfrom babel.dates import format_date\n\npd.options.display.max_colwidth = None\nsns.set_style('darkgrid')","ed9fdcfa":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras import layers, losses, callbacks, utils","291b6c3c":"class config():  \n    SAMPLE_SIZE = 1_000_000\n    \n    MAX_LEN = 30\n    LATENT_DIM = 8\n    Y_LEN = 10\n    NUM_DECODER_TOKENS = 12\n    \n    VALIDATION_SIZE = 0.1\n    BATCH_SIZE = 8\n    MAX_EPOCHS = 25\n        \n    DATE_FORMATS = [\n        'short', 'medium', 'long', 'full',\n        'd MMM YYY', 'd MMMM YYY', 'dd\/MM\/YYY',\n        'EE d, MMM YYY', 'EEEE d, MMMM YYY', 'd of MMMM YYY',\n    ]","e939d596":"faker = Faker()\nprint('Sample dates for each format\\n')\n\nfor fmt in set(config.DATE_FORMATS):\n    print(f'{fmt:20} =>  {format_date(faker.date_object(), format=fmt, locale=\"en\")}')","676d1f1c":"def clean_date(raw_date):\n    return raw_date.lower().replace(',', '')\n\n\ndef create_dataset(num_rows):\n    dataset = []\n    \n    for i in tqdm(range(num_rows)):\n        dt = faker.date_object()\n        for fmt in config.DATE_FORMATS:\n            try:\n                date = format_date(dt, format=fmt, locale='en')\n                human_readable = clean_date(date)\n                machine_readable = dt.isoformat()\n            except AttributeError as e:\n                date = None\n                human_readable = None\n                machine_readable = None\n            if human_readable is not None and machine_readable is not None:\n                dataset.append((human_readable, machine_readable))\n \n    return pd.DataFrame(dataset, columns=['human_readable', 'machine_readable'])","59c00e8e":"dataset = create_dataset(config.SAMPLE_SIZE)\ndataset  = dataset.drop_duplicates(subset=['human_readable']).sample(frac=1.0).reset_index(drop=True)\nprint(dataset.shape)\ndataset.head()","9efb6426":"human_tokenizer = Tokenizer(char_level=True, oov_token='<oov>')\nmachine_tokenizer = Tokenizer(char_level=True)\n\nhuman_tokenizer.fit_on_texts(dataset['human_readable'].values)\nmachine_tokenizer.fit_on_texts(dataset['machine_readable'].values)\n\nprint(human_tokenizer.word_index)","119e1c74":"def preprocess_input(date, tokenizer, max_len):\n    seq = [i[0] for i in tokenizer.texts_to_sequences(date.lower().replace(',', ''))]\n    seq = pad_sequences([seq], padding='post', maxlen=max_len)[0]\n    return utils.to_categorical(seq, num_classes=len(tokenizer.word_index)+1)","5d2cd5e4":"%%time\n\nX = np.array(list(map(lambda x: preprocess_input(x, human_tokenizer, config.MAX_LEN), dataset['human_readable'])))\ny = np.array(list(map(lambda x: preprocess_input(x, machine_tokenizer, 10), dataset['machine_readable'])))\n\nX.shape, y.shape","13611d9e":"class NMTAutoencoder(Model):\n    def __init__(self):\n        super(NMTAutoencoder, self).__init__()\n        self.encoder = tf.keras.Sequential([\n            layers.Bidirectional(layers.LSTM(config.LATENT_DIM))\n        ])\n\n        self.decoder = tf.keras.Sequential([\n            layers.RepeatVector(config.Y_LEN),\n            layers.Bidirectional(layers.LSTM(config.LATENT_DIM, return_sequences=True)),\n            layers.TimeDistributed(layers.Dense(config.NUM_DECODER_TOKENS, activation='softmax')),\n        ])\n\n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\nmodel = NMTAutoencoder()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')","66d33b09":"history = model.fit(\n    X, y, epochs=config.MAX_EPOCHS, batch_size=config.BATCH_SIZE, \n    validation_split=config.VALIDATION_SIZE\n)","ad3b401b":"utils.plot_model(model.encoder, show_shapes=True, expand_nested=True, to_file='encoder.png')\nutils.plot_model(model.decoder, show_shapes=True, expand_nested=True, to_file='decoder.png')\nfig, ax = plt.subplots(1, 2, figsize=(20, 8))\nax[0].imshow(plt.imread('encoder.png')); ax[0].axis('off'); ax[0].set_title('Encoder', fontsize=18)\nax[1].imshow(plt.imread('decoder.png')); ax[1].axis('off'); ax[1].set_title('Decoder', fontsize=18)\nfig.suptitle('Model Architecture', fontsize=24);","bac9e271":"fig, ax = plt.subplots(figsize=(20, 6))\npd.DataFrame(history.history).plot(ax=ax)\ndel history","2375e112":"model.encoder.save('nmt_encoder.h5')\nmodel.decoder.save('nmt_decoder.h5')","556d3b99":"def word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\ndef predict_sequence(model, tokenizer, source):\n    prediction = model.predict(source, verbose=0)[0]\n    integers = [np.argmax(vector) for vector in prediction]\n    target = list()\n    for i in integers:\n        word = word_for_id(i, tokenizer)\n        if word is None:\n            break\n        target.append(word)\n    return ''.join(target)\n\ndef decode_sequence(tokenizer, source):\n    integers = [np.argmax(vector) for vector in source]\n    target = list()\n    for i in integers:\n        word = word_for_id(i, tokenizer)\n        if word is None:\n            break\n        target.append(word)\n    return ''.join(target)","448f841c":"query_idx = 101\n\nprint('SOURCE     :', decode_sequence(human_tokenizer, np.squeeze(X[query_idx:query_idx+1])))\nprint('TARGET     :', decode_sequence(machine_tokenizer, np.squeeze(y[query_idx:query_idx+1])))\nprint('PREDICTION :', predict_sequence(model, machine_tokenizer, X[query_idx:query_idx+1]))","ce625775":"query_text = 'saturday 19 september 1998'\nquery = np.array(list(map(lambda x: preprocess_input(x, human_tokenizer, config.MAX_LEN), [query_text])))\n\nprint('SOURCE     :', query_text)\nprint('PREDICTION :', predict_sequence(model, machine_tokenizer, query))","afe34372":"# Evaluation","ab83922b":"# Data Generation","ba606931":"# Modelling"}}