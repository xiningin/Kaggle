{"cell_type":{"8625c20b":"code","cdfc07bd":"code","8795d6a3":"code","91a692b1":"code","ad36b97e":"code","55657ec5":"code","26aa9d5f":"code","91b99094":"code","4fe40584":"code","7f9cf1b6":"code","845dd152":"code","70dd1e73":"code","25256436":"code","cc7be831":"code","5f7fde0c":"code","183b9cf5":"code","c2e46093":"code","91948a24":"code","26a34753":"code","1d57dae5":"code","0a780fa2":"code","63946776":"code","72db13ee":"code","463fdd51":"code","e657d258":"code","2a50d96a":"markdown","dde36f29":"markdown","46323cf2":"markdown","bec57f0c":"markdown","6e5186eb":"markdown","32474b49":"markdown","95872089":"markdown","1199858d":"markdown","a81aec59":"markdown","fda6d145":"markdown","7b331a06":"markdown","453b1695":"markdown","fb4c95bb":"markdown","a1fe31f2":"markdown","2d077219":"markdown"},"source":{"8625c20b":"# Import numpy, pandas, and matplotlib using the standard aliases. \n# Import the following tools from sklearn: \n#     Pipeline, SimpleImputer, ColumnTransformer, OneHotEncoder, StandardScaler\n#     LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GridSearchCV\n# Import joblib\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport joblib\nimport gc\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import GridSearchCV","cdfc07bd":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain = df.sample(frac=1, random_state=42)\ntrain.shape","8795d6a3":"# Display the head of the train DataFrame. \ntrain.head()","91a692b1":"# Calculate and print the number of number of missing values in each column.\nprint(train.isnull().sum())\nprint(\" \")\npercent_missing = train.isnull().sum() * 100 \/ len(train)\nprint(percent_missing)","ad36b97e":"train.isna().sum(axis=0).to_frame().T","55657ec5":"# Display a DataFrame showing the proportion of observations with each \n# possible of the target variable (which is Survived). \ntrain['Survived'].value_counts(normalize= True)\n# Survived 0-No, 1-Yes","26aa9d5f":"# We will start with some feature engineering. \n\n# Add a new column named 'FamSize' to the DataFrame. \n# This should be the sum of the 'SibSp' and 'Parch' columns. \ntrain['FamSize'] = train.SibSp + train.Parch\n# We will use the function below to determine the deck letter for each passenger:\ndef set_deck(cabin):\n    if str(cabin) == 'nan':\n        return 'Missing'\n    return cabin[0]\n\n# Use the map() method of the train DataFrame to apply the function above \n# to the 'Cabin' column. Store the results in a new column named 'Deck'. \ntrain['Deck'] = train['Cabin'].map(set_deck)\ntrain.head(20)","91b99094":"train.isna().sum(axis=0).to_frame().T","4fe40584":"y_train = train.Survived.values\ntrain.drop(['PassengerId', 'Survived', 'Name', 'Ticket', 'SibSp','Parch','Cabin'], axis=1, inplace=True)\ntrain.head()","7f9cf1b6":"import seaborn as sns\nsns.histplot(data = train\n            ,x = 'Age'\n            )","845dd152":"train.describe()","70dd1e73":"sns.histplot(data = train\n            ,x = 'FamSize'\n            )","25256436":"sns.histplot(data = train\n            ,x = 'Fare'\n            )","cc7be831":"# Create a list of numberical feature names. Use the following features: 'Age', 'FamSize', 'Fare'\n# Create a list of categorical feature names. Use the following features: 'Sex', 'Pclass', 'Deck', 'Embarked'\n# Combine the two previous lists into one list named 'features'\nnum_features = ['Age', 'FamSize', 'Fare']\ncat_features = ['Sex', 'Pclass', 'Deck', 'Embarked']\n\nprint(num_features)\nprint(cat_features)\n\nfeatures = num_features + cat_features\n# Create a Pipeline object for processing the numerical features. \n# This pipeline should consist of a SimpleImputer and a StandardScaler\n\nnum_transformer = Pipeline(\n    steps = [\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('scaler', StandardScaler())  \n    ]\n)\n\n# Create a Pipeline object for processing the categorical features. \n# This pipeline should consist of a SimpleImputer and a OneHotEncoder\ncat_transformer = Pipeline(\n    steps = [\n        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ]\n)\n# Create a ColumnTransformer object that combines the two pipelines created above. \n# Name this ColumnTransformer 'preprocessor'\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features)\n    ]\n)","5f7fde0c":"train[cat_features].nunique()","183b9cf5":"# Fit the preprocessor to the training data, selecting only the columns in the 'features' list. \n# Apply the fitted preprocessor to the training data, again selecting only the relevant columns. \n# Store the array created in the previous step into a variable named 'X_train'.\npreprocessor.fit(train)\nX_train = preprocessor.transform(train)\n\n# Create a variable named 'y_train' that contains the training labels. \n## y_train was created in previous step\n\n# Print the shapes of X_train and y_train.\nprint('X_train shape:', X_train.shape)\nprint('y_train shape:', y_train.shape)","c2e46093":"## No need to sample because the dataset is small.\n'''\nX_sample, X_valid, y_sample, y_valid = train_test_split(X_train, y_train, test_size=0.8, stratify=y_train, random_state=1)\n\nprint(X_sample.shape)\nprint(X_valid.shape)\n'''","91948a24":"\nlr_clf = LogisticRegression(max_iter=1000, solver='saga', penalty='elasticnet')\n\nlr_parameters = {\n    'l1_ratio': [0, 0.25, 0.5, 0.75, 1],\n    'C': [0.01, 0.1, 1, 10]\n}\n\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=10, scoring='accuracy')\nlr_grid.fit(X_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\nprint('Best Parameters:', lr_grid.best_params_)\nprint('Best CV Score:  ', lr_grid.best_score_)\nprint('Training Acc:   ', lr_model.score(X_train, y_train))","26a34753":"# Run this cell without any changes to view the CV results.\n\nlr_summary = pd.DataFrame(lr_grid.cv_results_['params'])\nlr_summary['cv_score'] = lr_grid.cv_results_['mean_test_score']\n\nfor r in lr_parameters['l1_ratio']:\n    temp = lr_summary.query(f'l1_ratio == {r}')\n    plt.plot(temp.C, temp.cv_score, label=r)\nplt.xscale('log')\nplt.ylim([0.75, 0.82])\nplt.xlabel('Regularization Parameter (C)')\nplt.ylabel('CV Score')\nplt.legend(title='L1 Ratio', loc='lower right')\nplt.grid()\nplt.show()\n\nprint(lr_summary.to_string(index=False))","1d57dae5":"%%time \n# Select values to consider for the max_depth and min_samples_leaf hyperparameters.\n# You might need to experiment to find a good range of parameter values. \n# Update the code below to perform 10-Fold cross-validation. \n# Set the scoring parameter below to 'accuracy'\n\ndt_clf = DecisionTreeClassifier(random_state=42)\n\ndt_parameters = {\n    'max_depth': [2, 4, 6, 8, 10, 12, 14, 16],\n    'min_samples_leaf': [2, 4, 8, 16]\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\ndt_grid.fit(X_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\nprint('Best Parameters:', dt_grid.best_params_)\nprint('Best CV Score:  ', dt_grid.best_score_)\nprint('Training Acc:   ', dt_model.score(X_train, y_train))","0a780fa2":"# Run this cell without any changes to view the CV results.\n\ndt_summary = pd.DataFrame(dt_grid.cv_results_['params'])\ndt_summary['cv_score'] = dt_grid.cv_results_['mean_test_score']\n\nfor ms in dt_parameters['min_samples_leaf']:\n    temp = dt_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(dt_summary.to_string(index=False))","63946776":"%%time \n# Select a number of trees to use in your random forest.\n# Select values to consider for the max_depth and min_samples_leaf hyperparameters.\n# You might need to experiment to find a good range of parameter values. \n# Update the code below to perform 10-Fold cross-validation. \n# Set the scoring parameter below to 'accuracy'\n\nrf_clf = RandomForestClassifier(random_state=42, n_estimators=100)\n\nrf_parameters = {\n    #'max_depth': [4, 8, 16, 20, 24, 28, 32],\n    #'min_samples_leaf': [1, 2, 4]\n    'max_depth': range(1, 33),\n    'min_samples_leaf': range(1, 10),\n    'min_samples_split': [2, 5, 10]\n}\n\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring=\"accuracy\")\nrf_grid.fit(X_train, y_train)\n\nrf_model = rf_grid.best_estimator_\n\nprint('Best Parameters:', rf_grid.best_params_)\nprint('Best CV Score:  ', rf_grid.best_score_)\nprint('Training Acc:   ', rf_model.score(X_train, y_train))","72db13ee":"# Run this cell without any changes to view the CV results.\n\nrf_summary = pd.DataFrame(rf_grid.cv_results_['params'])\nrf_summary['cv_score'] = rf_grid.cv_results_['mean_test_score']\n\nfor ms in rf_parameters['min_samples_leaf']:\n    temp = rf_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(rf_summary.to_string(index=False))","463fdd51":"# Save your pipeline to a file. \n# Determine the best model found above and save that to a file. \n# Download both files to your local device and then upload them as a Kaggle dataset. \n\nprint(rf_grid.best_params_)\n\nfinal_model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=13, min_samples_leaf=2, min_samples_split = 2)\nfinal_model.fit(X_train, y_train)\n\nprint(final_model.score(X_train, y_train))\n\n","e657d258":"joblib.dump(preprocessor, 'Titanic_preprocessor_04.joblib')\njoblib.dump(final_model, 'Titanic_model_04.joblib')\nprint('Model written to file.')","2a50d96a":"## Logistic Regression","dde36f29":"## Decicion Trees","46323cf2":"### Explore Catergorical Levels","bec57f0c":"### Save Final Model","6e5186eb":"### Sample Training Data","32474b49":"# Preprocessing","95872089":"# Titanic Dataset\n\nMost of the code cells below include comments explaining the task to be performed in those cells. Please delete the comments and add code to perform those tasks. There are a few code cells in which code has already been provided for you. In some cases, you will need to compelte this code. ","1199858d":"# Save Pipeline and Model","a81aec59":"### create boxplot, histogram for numeric variables","fda6d145":"# Check Label Distribution","7b331a06":"# Load Training Data","453b1695":"# Import Statements","fb4c95bb":"# Check for Missing Values","a1fe31f2":"## Random Forests","2d077219":"# Model Selection"}}