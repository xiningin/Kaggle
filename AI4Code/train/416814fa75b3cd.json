{"cell_type":{"e2c376c7":"code","0ed177e2":"code","a747456f":"code","264e4783":"code","66ee7deb":"code","13acc292":"code","978007e3":"code","4821d78b":"code","1cfb504d":"code","1928867a":"markdown","f8491f41":"markdown","bdd7b343":"markdown"},"source":{"e2c376c7":"import pandas as pd\nimport seaborn as sb\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor \nfrom math import sqrt\n\ntrain_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","0ed177e2":"num_bins = math.floor(len(train_df) \/ 30)\nplt.hist(x=train_df['SalePrice'], bins=num_bins)\nplt.ylabel('Count')\nplt.title('SalePrice')","a747456f":"train_df['SalePrice'] = np.log(train_df['SalePrice'])\nplt.hist(x=train_df['SalePrice'], bins=num_bins)\nplt.ylabel('Count')\nplt.title('Log of SalePrice')","264e4783":"\ntrain_df['GrLivArea'] = np.log(train_df['GrLivArea'])","66ee7deb":"# Find features that\n# highly correlate with price\n# and prune outliers, if any\ncorr = train_df.corr()\ncorr = corr['SalePrice'].sort_values(ascending=False)\n\ncorr = corr[0:10]\nprint(corr)\n\n# Get the z-scores for the highly correlated values\nz_score_df = np.abs((train_df[corr.index] - train_df[corr.index].mean())\/train_df[corr.index].std())\n\n# Drop records with extreme values\ntrain_df = train_df.drop(\n        train_df[ z_score_df['SalePrice'] > 5].index)\n\ntrain_df = train_df.drop(\n        train_df[ z_score_df['GrLivArea'] > 5].index)\n\ntrain_df = train_df.drop(\n        train_df[ z_score_df['GarageArea'] > 4].index)\n\ntrain_df = train_df.drop(\n        train_df[ z_score_df['TotalBsmtSF'] > 5].index)\n","13acc292":"\n# Hang on to original values\ny_salesprice = train_df.SalePrice.values\nnum_train_recs = train_df.shape[0]\nnum_test_recs = test_df.shape[0]\n\n# Combine train and test data so\n# we can look for categorical values\n# present in the entire data set\nall_df = pd.concat((train_df, test_df)).reset_index(drop=True)\nall_df = all_df.drop(['SalePrice'], axis=1)\n\n# Identify columns that are mostly \n# null and drop them. Keep a list of\n# junk columns.\njunk_cols = []\nrec_count = len(all_df)\njunk_threshold = 0.25\n\nfor col in all_df.columns:\n    if (all_df[col].count() \/ rec_count) < junk_threshold:\n        junk_cols.append(col)\n    \nprint(f\"Junk columns: {', '.join(junk_cols)}\")  \n\nall_df = all_df.drop(junk_cols, axis=1)","978007e3":"# Missing categoricals to mode,\n# missing continuous values to mean\n#for col in all_df.columns:\n    \n#    if all_df[col].isnull().values.any():\n    \n#        if all_df[col].dtype == 'object':\n#            all_df[col].fillna(all_df[col].mode()[0], inplace=True)\n#        else:\n#            all_df[col].fillna(all_df[col].mean(), inplace=True)\n\n\ngroup_df = all_df.groupby(['Neighborhood','HouseStyle'])\n\nfor col in all_df.columns:\n    \n    if all_df[col].isnull().values.any():\n\n        if np.issubdtype(all_df[col].dtype, np.number):\n            \n            all_df[col] = group_df[col].apply(\n                lambda x: x.fillna(all_df[col].median() if pd.isnull(x.median()) else x.median() ))\n\n        else:\n            all_df[col] = group_df[col].apply(\n                lambda x: x.fillna(x.mode().iloc[0] if len(x.mode()) else all_df[col].mode().iloc[0]))\n","4821d78b":"# One hot encode categorical cols\n\nfor col in all_df.columns:\n    if train_df[col].dtype == 'object':\n        print(f\"Get dummies for {col}\")\n        dummies = pd.get_dummies(all_df[col], prefix=col)\n        all_df = all_df.drop(col, axis=1)\n        all_df = all_df.join(dummies)\n\n#for col in all_df.columns:\n#    if all_df[col].dtype == 'object':\n#        d = all_df[col].value_counts().to_dict()\n#        all_df[col] = all_df[col].map(d)","1cfb504d":"all_df = all_df.drop('Id', axis=1)\n\n\ntrain_new = all_df[:num_train_recs]\ntest_new = all_df[num_train_recs:]\n\nprint(train_new.shape)\nprint(test_new.shape)\n\n\ntrain_X, test_X, train_y, test_y = train_test_split(train_new, y_salesprice, test_size=0.25, random_state=42)\n\n\n\n#https:\/\/www.kaggle.com\/tanushreewandkar\/eda-and-xgboost-model-building\n\nparams = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7],\n        'min_child_weight': [3, 5, 7, 9],\n        'subsample': [0.6, 0.7, 0.8, 0.9],\n        'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n        'n_estimators' : [1000],\n        'objective': ['reg:squarederror']\n}\n\n#xgb_model = XGBRegressor()\n\n#gsearch = GridSearchCV(estimator = xgb_model,\n#                           param_grid = params,                        \n#                           cv = 5,\n#                           n_jobs = -1,\n#                           verbose = 1)\n#gsearch.fit(train_X, train_y )\n#print(gsearch.best_params_)\n\n#gsearch = RandomizedSearchCV(estimator=xgb_model, param_distributions=params, n_jobs=-1,\n#                             cv = 5, scoring='neg_mean_absolute_error', n_iter=50,                              \n#                             return_train_score=True, random_state=42, refit=True, verbose=1)\n                         \n#gsearch.fit(train_X, train_y)\n#print(gsearch.best_params_)\n\n# Set param to output of cross-validation\nxgbr = XGBRegressor(\n    colsample_bytree= 0.6, \n    learning_rate= 0.01,\n    max_depth= 7, \n    min_child_weight= 9, \n    n_estimators=1000, \n    objective='reg:squarederror', \n    subsample= 0.6,\n    random_state=42,\n    seed=42)\n\nxgbr.fit(train_X, train_y, early_stopping_rounds=5, \n         eval_set=[(test_X, test_y)], verbose=False)\n\ny_pred_xgb = xgbr.predict(test_X)\n\n\nprint(\"Root Mean squared error: \" + \n      str(sqrt(mean_squared_error(test_y,y_pred_xgb))))\n\nprint(\"Root Mean squared error, recovered: \" + \n      str(sqrt(mean_squared_error(np.exp(test_y),np.exp(y_pred_xgb)))))\n\n\n# Plot actual vs. predicted\n\ndf = pd.DataFrame( columns = ['test','pred'])\ndf['test'] = np.exp(test_y)\ndf['pred'] = np.exp(y_pred_xgb)\n\ndf = df.sort_values('test')\ndf = df.reset_index(drop=True)\n\nplt.plot( 'pred', data=df, marker='', color='red', linewidth=1, label=\"Predicted\")\nplt.plot( 'test', data=df, marker='', color='blue', linewidth=1, label=\"Actual\")\n\nplt.legend(loc=\"upper left\")\nplt.title(\"Actual vs. Predicted\")","1928867a":"# Handle Missing Values\n\nGo through all columns in the data set and set missing categorical values to the mode of the column. Set missing numeric values to the acerage of the column.","f8491f41":"# Idenify and remove outliers.\n\nFind columns that correlate with sale price and remove outliers. Find the outliers by calcualting the z-score (number of standard deviations from the mean) for some of the columns. Remove rows with high z-scores from the dataset.","bdd7b343":"First off, the sale price, the thing we are trying to predict, is heavily skewed. Models seem to cope better with normal distributions, so apply np.log() to the column.\n\nWhen the model makes predictions, remember to invert the operation with np.exp()."}}