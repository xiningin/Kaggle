{"cell_type":{"5af3b810":"code","76c4b773":"code","bbe3c772":"code","6b1d4363":"code","cf636a92":"code","fb8854f6":"code","95aa8bd9":"code","41e3ef49":"code","9b82aab3":"code","f1fd6af8":"code","2060e073":"code","56512576":"code","9b7d0141":"code","7b66029b":"code","10fe1a30":"code","14982353":"code","859c66c0":"code","5c3cdbea":"code","1e906d7c":"code","521edd6a":"code","64ba086c":"code","2ed24018":"code","94164721":"code","be13c99f":"code","9fbba665":"code","d543ad4c":"code","10ab5ec9":"code","77d039ae":"code","7a0c818b":"code","cf5aea35":"markdown","85956e13":"markdown","5410e46f":"markdown","21f6b46f":"markdown","b6b11f74":"markdown","a7cd0e8a":"markdown","a3f2e82e":"markdown","b8ee9571":"markdown","98137e87":"markdown","0c0f2649":"markdown","01cfd5ff":"markdown","6b9ffcaf":"markdown","df1112ac":"markdown","c70a6e17":"markdown"},"source":{"5af3b810":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgbm\nimport xgboost as xgb\nimport datetime\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_rows', 150)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","76c4b773":"train = pd.read_csv('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv', dtype={'WindSpeed': 'object'})","bbe3c772":"train.head(3)","6b1d4363":"train.info()","cf636a92":"outcomes = train[['GameId','PlayId','Yards']].drop_duplicates()","fb8854f6":"def strtoseconds(txt):\n    txt = txt.split(':')\n    ans = int(txt[0])*60 + int(txt[1]) + int(txt[2])\/60\n    return ans\n\ndef strtofloat(x):\n    try:\n        return float(x)\n    except:\n        return -1\n\ndef map_weather(txt):\n    ans = 1\n    if pd.isna(txt):\n        return 0\n    if 'partly' in txt:\n        ans*=0.5\n    if 'climate controlled' in txt or 'indoor' in txt:\n        return ans*3\n    if 'sunny' in txt or 'sun' in txt:\n        return ans*2\n    if 'clear' in txt:\n        return ans\n    if 'cloudy' in txt:\n        return -ans\n    if 'rain' in txt or 'rainy' in txt:\n        return -2*ans\n    if 'snow' in txt:\n        return -3*ans\n    return 0\n\ndef OffensePersonnelSplit(x):\n    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0, 'QB' : 0, 'RB' : 0, 'TE' : 0, 'WR' : 0}\n    for xx in x.split(\",\"):\n        xxs = xx.split(\" \")\n        dic[xxs[-1]] = int(xxs[-2])\n    return dic\n\ndef DefensePersonnelSplit(x):\n    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0}\n    for xx in x.split(\",\"):\n        xxs = xx.split(\" \")\n        dic[xxs[-1]] = int(xxs[-2])\n    return dic\n\ndef orientation_to_cat(x):\n    x = np.clip(x, 0, 360 - 1)\n    try:\n        return str(int(x\/15))\n    except:\n        return \"nan\"\ndef preprocess(train):\n    ## GameClock\n    train['GameClock_sec'] = train['GameClock'].apply(strtoseconds)\n    train[\"GameClock_minute\"] = train[\"GameClock\"].apply(lambda x : x.split(\":\")[0]).astype(\"object\")\n\n    ## Height\n    train['PlayerHeight_dense'] = train['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n\n    ## Time\n    train['TimeHandoff'] = train['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    train['TimeSnap'] = train['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n\n    train['TimeDelta'] = train.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n    train['PlayerBirthDate'] = train['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m\/%d\/%Y\"))\n\n    ## Age\n    seconds_in_year = 60*60*24*365.25\n    train['PlayerAge'] = train.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()\/seconds_in_year, axis=1)\n    train[\"PlayerAge_ob\"] = train['PlayerAge'].astype(np.int).astype(\"object\")\n\n    ## WindSpeed\n    train['WindSpeed_ob'] = train['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n    train['WindSpeed_ob'] = train['WindSpeed_ob'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))\/2 if not pd.isna(x) and '-' in x else x)\n    train['WindSpeed_ob'] = train['WindSpeed_ob'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))\/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n    train['WindSpeed_dense'] = train['WindSpeed_ob'].apply(strtofloat)\n\n    ## Weather\n    train['GameWeather_process'] = train['GameWeather'].str.lower()\n    train['GameWeather_process'] = train['GameWeather_process'].apply(lambda x: \"indoor\" if not pd.isna(x) and \"indoor\" in x else x)\n    train['GameWeather_process'] = train['GameWeather_process'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\n    train['GameWeather_process'] = train['GameWeather_process'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\n    train['GameWeather_process'] = train['GameWeather_process'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n    train['GameWeather_dense'] = train['GameWeather_process'].apply(map_weather)\n\n    ## Rusher\n    train['IsRusher'] = (train['NflId'] == train['NflIdRusher'])\n    train['IsRusher_ob'] = (train['NflId'] == train['NflIdRusher']).astype(\"object\")\n    temp = train[train[\"IsRusher\"]][[\"Team\", \"PlayId\"]].rename(columns={\"Team\":\"RusherTeam\"})\n    train = train.merge(temp, on = \"PlayId\")\n    train[\"IsRusherTeam\"] = train[\"Team\"] == train[\"RusherTeam\"]\n\n    ## dense -> categorical\n    train[\"Quarter_ob\"] = train[\"Quarter\"].astype(\"object\")\n    train[\"Down_ob\"] = train[\"Down\"].astype(\"object\")\n    train[\"JerseyNumber_ob\"] = train[\"JerseyNumber\"].astype(\"object\")\n    train[\"YardLine_ob\"] = train[\"YardLine\"].astype(\"object\")\n  \n\n\n    ## Orientation and Dir\n    train[\"Orientation_ob\"] = train[\"Orientation\"].apply(lambda x : orientation_to_cat(x)).astype(\"object\")\n    train[\"Dir_ob\"] = train[\"Dir\"].apply(lambda x : orientation_to_cat(x)).astype(\"object\")\n\n    train[\"Orientation_sin\"] = train[\"Orientation\"].apply(lambda x : np.sin(x\/360 * 2 * np.pi))\n    train[\"Orientation_cos\"] = train[\"Orientation\"].apply(lambda x : np.cos(x\/360 * 2 * np.pi))\n    train[\"Dir_sin\"] = train[\"Dir\"].apply(lambda x : np.sin(x\/360 * 2 * np.pi))\n    train[\"Dir_cos\"] = train[\"Dir\"].apply(lambda x : np.cos(x\/360 * 2 * np.pi))\n\n    ## diff Score\n    train[\"diffScoreBeforePlay\"] = train[\"HomeScoreBeforePlay\"] - train[\"VisitorScoreBeforePlay\"]\n    train[\"diffScoreBeforePlay_binary_ob\"] = (train[\"HomeScoreBeforePlay\"] > train[\"VisitorScoreBeforePlay\"]).astype(\"object\")\n\n    ## Turf\n    Turf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', 'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', 'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', 'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', 'SISGrass':'Artificial', 'Twenty-Four\/Seven Turf':'Artificial', 'natural grass':'Natural'} \n    train['Turf'] = train['Turf'].map(Turf)\n\n    ## OffensePersonnel\n    temp = train[\"OffensePersonnel\"].iloc[np.arange(0, len(train), 22)].apply(lambda x : pd.Series(OffensePersonnelSplit(x)))\n    temp.columns = [\"Offense\" + c for c in temp.columns]\n    temp[\"PlayId\"] = train[\"PlayId\"].iloc[np.arange(0, len(train), 22)]\n    train = train.merge(temp, on = \"PlayId\")\n\n    ## DefensePersonnel\n    temp = train[\"DefensePersonnel\"].iloc[np.arange(0, len(train), 22)].apply(lambda x : pd.Series(DefensePersonnelSplit(x)))\n    temp.columns = [\"Defense\" + c for c in temp.columns]\n    temp[\"PlayId\"] = train[\"PlayId\"].iloc[np.arange(0, len(train), 22)]\n    train = train.merge(temp, on = \"PlayId\")\n\n    ## sort\n    train = train.sort_values(by = ['X']).sort_values(by = ['Dis']).sort_values(by=['PlayId', 'IsRusherTeam', 'IsRusher']).reset_index(drop = True)\n    return train","95aa8bd9":"train = preprocess(train)","41e3ef49":"train.head(5)","9b82aab3":"train.info()","f1fd6af8":"# Thanks to : https:\/\/www.kaggle.com\/aantonova\/some-new-risk-and-clusters-features\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","2060e073":"numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train.columns.values.tolist()\nfor col in features:\n    if train[col].dtype in numerics: continue\n    categorical_columns.append(col)\nindexer = {}\nfor col in categorical_columns:\n    if train[col].dtype in numerics: continue\n    _, indexer[col] = pd.factorize(train[col])\n    \nfor col in categorical_columns:\n    if train[col].dtype in numerics: continue\n    train[col] = indexer[col].get_indexer(train[col])","56512576":"target = train['Yards']\ndel train['Yards']","9b7d0141":"train = train.fillna(-999)","7b66029b":"train = reduce_mem_usage(train)","10fe1a30":"train.info()","14982353":"X = train\nz = target","859c66c0":"#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgbm.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgbm.Dataset(Xval, Zval, silent=False)","5c3cdbea":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)","1e906d7c":"fig =  plt.figure(figsize = (25,30))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","521edd6a":"feature_score = pd.DataFrame(train.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()","64ba086c":"#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","2ed24018":"parms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:squarederror',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","94164721":"fig =  plt.figure(figsize = (15,30))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","be13c99f":"feature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score","9fbba665":"# Standardization for regression model\ntrain = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(train),\n    columns=train.columns,\n    index=train.index\n)","d543ad4c":"# Linear Regression\n\nlinreg = LinearRegression()\nlinreg.fit(train, target)\ncoeff_linreg = pd.DataFrame(train.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","10ab5ec9":"# the level of importance of features is not associated with the sign\ncoeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs()\n\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","77d039ae":"#Thanks to https:\/\/www.kaggle.com\/nanomathias\/feature-engineering-importance-testing\n# MinMax scale all importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\n\n# Create total column with different weights\nfeature_score['weighted average - 0.5\/0.35\/0.15'] = 0.5*feature_score['score_lgb'] + 0.35*feature_score['score_xgb'] + 0.15*feature_score['score_linreg']\n\n# Plot the feature importances\nfeature_score.sort_values('weighted average - 0.5\/0.35\/0.15', ascending=False).plot(kind='bar', figsize=(20, 15))","7a0c818b":"feature_score.sort_values('weighted average - 0.5\/0.35\/0.15', ascending=False)","cf5aea35":"## 5. Tuning models and building the feature importance diagrams<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","85956e13":"### 6. Comparison of the all feature importance diagrams <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","5410e46f":"**FE & EDA from the kernel [Location EDA with rusher features](https:\/\/www.kaggle.com\/dandrocec\/location-eda-with-rusher-features)**\n\n**Technology for feature importance building from the my kernel [Feature importance - xgb, lgbm, logreg, linreg](https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg)**","21f6b46f":"### 5.3 Linear Regression <a class=\"anchor\" id=\"5.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","b6b11f74":"Technology for feature importance building from the my kernel [Feature importance - xgb, lgbm, logreg, linreg](https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg)","a7cd0e8a":"## 3. FE & EDA <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","a3f2e82e":"### 5.1 LGBM <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","b8ee9571":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","98137e87":"## 4. Preparing to modeling <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","0c0f2649":"Thanks to [Location EDA with rusher features](https:\/\/www.kaggle.com\/dandrocec\/location-eda-with-rusher-features)","01cfd5ff":"<a class=\"anchor\" id=\"0\"><\/a>\n\n# Building the feature importance diagrams in 3 ways for the competition \"NFL Big Data Bowl\":\n* XGB\n* LGBM\n* LinearRegression","6b9ffcaf":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download datasets](#2)\n1. [FE & EDA](#3)\n1. [Preparing to modeling](#4)\n1. [Tuning models and building the feature importance diagrams](#5)\n    -  [LGBM](#5.1)\n    -  [XGB](#5.2)\n    -  [Linear Regression](#5.3)\n1. [Comparison of the all feature importance diagrams](#6)","df1112ac":"### 5.2 XGB<a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","c70a6e17":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}