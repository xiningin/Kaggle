{"cell_type":{"f58f1d66":"code","10af8f86":"code","cfe2e2c2":"code","01c54900":"code","fcb5bad1":"code","74f963c0":"code","85c33fb8":"code","87fcbae0":"code","a9ae32ae":"code","9fddd8e3":"code","4c2a64a6":"code","92f26947":"code","c8d99b10":"code","7066bca1":"code","ba237523":"code","47adf172":"code","d370efd0":"code","b38e962f":"code","03f592a8":"code","a597308b":"code","16f5ff72":"code","b189870a":"code","f2231864":"code","74537dcb":"code","723e1e3c":"code","840a1cb3":"code","f25e343e":"code","3fa9975f":"code","57ef4b73":"code","3bc0972e":"code","3a62381c":"code","d100d684":"code","cf1c4b5f":"code","849c3e6a":"code","a30fd4d9":"code","0d0d0929":"code","764eae05":"code","07568da1":"code","0c2f265f":"code","3071c24f":"code","c62830ca":"code","29c6debb":"code","84db5d51":"code","f1424477":"code","a9ef5c38":"code","fe1eb176":"code","696b49e0":"code","c317ade5":"code","12340840":"code","3bac9341":"code","84827336":"code","571bec07":"code","1e86d367":"code","8d01f0bc":"code","e7d3236a":"code","fa5e2acf":"code","f96eb2fa":"code","3884d592":"code","5da66117":"markdown","489e23a2":"markdown","ab3f71fd":"markdown","36f1243b":"markdown","0e403189":"markdown","edbe8413":"markdown","e03ac483":"markdown"},"source":{"f58f1d66":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/pd-speech-features'):\n    \n    for filename in filenames:\n        print(os.path.join(dirname, filename))","10af8f86":"## Initial Imports\n\n%matplotlib inline\n\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom tqdm import tqdm_notebook as tqdm\n\nimport lime\nimport eli5\nimport mlxtend\nimport warnings\nwarnings.filterwarnings('ignore')","cfe2e2c2":"#pd_speech_features = pd.read_csv('pd_speech_features.csv')\npd_speech_features = pd.read_csv(os.path.join(dirname,filename))\nnew_header = pd_speech_features.iloc[0] #grab the first row for the header\npd_speech_features = pd_speech_features[1:] #take the data less the header row\npd_speech_features.columns = new_header #set the header row as the df header\npd_speech_features.head()\nprint('The shape of the matrix is :', pd_speech_features.shape)","01c54900":"pd_speech_features.info() # Gives type of columns\npd_speech_features.describe()","fcb5bad1":"pd_speech_features['patient\/healthy count'] = 1\npd_speech_features.groupby('class').sum()\/3","74f963c0":"pd_speech_features = pd_speech_features.drop(['patient\/healthy count'], axis = 1)  #756x755","85c33fb8":"pd_speech_features =  pd_speech_features.astype(float) #per default all floats \npd_speech_features[['id', 'numPulses', 'numPeriodsPulses']] = pd_speech_features[['id', 'numPulses', 'numPeriodsPulses']].astype(int) #ints\npd_speech_features[['gender', 'class']] = pd_speech_features[['gender', 'class']].astype('category') #categoricals\npd_speech_features.dtypes","87fcbae0":"corr = pd_speech_features.corr() \n\n#too many variable to plot correlation matrix \nsns.heatmap(np.abs(corr), \n        xticklabels=[],\n        yticklabels=[])","a9ae32ae":"pd_speech_features_no_tqwt = pd_speech_features[pd_speech_features.columns[1: -433]]\npd_speech_features_no_tqwt.head()","9fddd8e3":"\ntrain_df=pd_speech_features\ntrain_df_tqwt=pd_speech_features_no_tqwt\n\ny_train = train_df['class']\ny_train = np.array(y_train.values, dtype = 'int')\nx_train = train_df.drop(['class','id'], axis = 1) \n\nx_train = x_train.values\n#y_validation = validation_df['class']\n#y_validation = np.array(y_validation.values, dtype = 'int')\n#x_validation = validation_df.drop(['class'], axis = 1) ","4c2a64a6":"features=train_df.columns.values.tolist()\nfeatures.remove('id')\nfeatures.remove('class')\nprint(len(features))","92f26947":"print(x_train.shape)\nprint(y_train.shape)\n","c8d99b10":"from sklearn.linear_model import LogisticRegression\n\n#from mlxtend.feature_selection import SequentialFeatureSelector\n\n#selector = SequentialFeatureSelector(LogisticRegression(), forward=True, scoring='neg_log_loss', \n                                   #  verbose=2, k_features=10, forward=False, n_jobs=-1)\n\n#selections=selector.fit(x_train, y_train)","7066bca1":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n#x_train_transformed= sc.fit_transform(x_train_transformed)\n#x_validation = sc.transform(x_validation)\n#newly added\n#x_train_tqwt= sc.fit_transform(x_train_tqwt)\n#x_validation_tqwt = sc.transform(x_validation_tqwt)\nx_train= sc.fit_transform(x_train)","ba237523":"from sklearn.feature_selection import SequentialFeatureSelector\nsfs_7 = SequentialFeatureSelector(LogisticRegression(random_state=10),n_features_to_select=7, n_jobs=-1)\n\nsfs_7=sfs_7.fit(x_train, y_train)","47adf172":"x_train_transformed=sfs_7.transform(x_train)\nx_train_transformed.shape","d370efd0":"y_train_tqwt = y_train #train_df_tqwt['class']\n#y_train_tqwt = np.array(y_train_tqwt.values, dtype = 'int')\nx_train_tqwt = train_df_tqwt.values \n\n#y_validation_tqwt = validation_df_tqwt['class']\n#y_validation_tqwt = np.array(y_validation_tqwt.values, dtype = 'int')\n#x_validation_tqwt = validation_df_tqwt.drop(['class'], axis = 1) ","b38e962f":"print(x_train_tqwt.shape)\nprint(y_train_tqwt.shape)","03f592a8":"import keras\nfrom keras.preprocessing import sequence\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.models import load_model\nfrom keras import regularizers\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.model_selection import LeaveOneOut, KFold\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report","a597308b":"k_fold=KFold(10,True,10)","16f5ff72":"from sklearn import svm \nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import classification_report\n\n\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  ","b189870a":"grid_7_features = GridSearchCV(svm.SVC(probability=True, random_state=10), param_grid, cv=k_fold, refit = True, verbose = 0) \ngrid_7_features.fit(x_train_transformed, y_train)","f2231864":"print(grid_7_features.best_score_)","74537dcb":"sfs_7 = SequentialFeatureSelector(LogisticRegression(random_state=10),n_features_to_select=7, n_jobs=-1)\n\nsfs_7=sfs_7.fit(x_train, y_train)","723e1e3c":"x_train_transformed_7_features=sfs_7.transform(x_train)\nprint(x_train_transformed_7_features.shape)\n\ngrid_7_features = GridSearchCV(svm.SVC(probability=True, random_state=10), param_grid,cv=k_fold, refit = True, verbose = 0) \ngrid_7_features.fit(x_train_transformed_7_features, y_train)","840a1cb3":"grid_7_features.best_score_","f25e343e":"sfs_9 = SequentialFeatureSelector(LogisticRegression(random_state=10),n_features_to_select=9, n_jobs=-1)\n\nsfs_9=sfs_9.fit(x_train, y_train)","3fa9975f":"x_train_transformed_9_features=sfs_9.transform(x_train)\nprint(x_train_transformed_7_features.shape)\n\ngrid_9_features = GridSearchCV(svm.SVC(probability=True, random_state=10), param_grid,cv=k_fold, refit = True, verbose = 0) \ngrid_9_features.fit(x_train_transformed_9_features, y_train)","57ef4b73":"grid_9_features.best_score_","3bc0972e":"sfs_20 = SequentialFeatureSelector(LogisticRegression(random_state=10),n_features_to_select=20, n_jobs=-1)\n\nsfs_20=sfs_20.fit(x_train, y_train)","3a62381c":"x_train_transformed_20_features=sfs_20.transform(x_train)\nprint(x_train_transformed_20_features.shape)\n\ngrid_20_features = GridSearchCV(svm.SVC(probability=True, random_state=10), param_grid,cv=k_fold, refit = True, verbose = 0) \ngrid_20_features.fit(x_train_transformed_20_features, y_train)","d100d684":"grid_20_features.best_score_","cf1c4b5f":"grid_all_features= GridSearchCV(svm.SVC(probability=True, random_state=10), param_grid, cv=k_fold,refit = True, verbose = 0) \ngrid_all_features.fit(x_train, y_train)","849c3e6a":"grid_all_features.best_score_","a30fd4d9":"sfs_3 = SequentialFeatureSelector(LogisticRegression(random_state=10),n_features_to_select=3, n_jobs=-1)\n\nsfs_3=sfs_3.fit(x_train, y_train)","0d0d0929":"x_train_transformed_3_features=sfs_3.transform(x_train)\nprint(x_train_transformed_3_features.shape)\n\ngrid_3_features = GridSearchCV(svm.SVC(probability=True, random_state=10), param_grid, cv=k_fold,refit = True, verbose = 0) \ngrid_3_features.fit(x_train_transformed_3_features, y_train)","764eae05":"grid_3_features.best_score_","07568da1":"sfs_2 = SequentialFeatureSelector(LogisticRegression(random_state=10),n_features_to_select=2, n_jobs=-1)\n\nsfs_2=sfs_2.fit(x_train, y_train)","0c2f265f":"x_train_transformed_2_features=sfs_2.transform(x_train)\nprint(x_train_transformed_2_features.shape)\n\ngrid_2_features = GridSearchCV(svm.SVC(probability=True, random_state=10), param_grid, cv=k_fold,refit = True, verbose = 0) \ngrid_2_features.fit(x_train_transformed_2_features, y_train)","3071c24f":"grid_2_features.best_score_","c62830ca":"sfs_6 = SequentialFeatureSelector(LogisticRegression(random_state=10),n_features_to_select=6, n_jobs=-1)\n\nsfs_6=sfs_6.fit(x_train, y_train)","29c6debb":"x_train_transformed_6_features=sfs_6.transform(x_train)\nprint(x_train_transformed_6_features.shape)\n\ngrid_6_features = GridSearchCV(svm.SVC(probability=True, random_state=10), param_grid, cv=k_fold,refit = True, verbose = 0) \ngrid_6_features.fit(x_train_transformed_6_features, y_train)","84db5d51":"grid_6_features.best_score_","f1424477":"sfs_8 = SequentialFeatureSelector(LogisticRegression(random_state=10),n_features_to_select=8, n_jobs=-1)\n\nsfs_8=sfs_8.fit(x_train, y_train)","a9ef5c38":"x_train_transformed_8_features=sfs_8.transform(x_train)\nprint(x_train_transformed_8_features.shape)\n\ngrid_8_features = GridSearchCV(svm.SVC(probability=True, random_state=10), param_grid, cv=k_fold,refit = True, verbose = 0) \ngrid_8_features.fit(x_train_transformed_8_features, y_train)","fe1eb176":"grid_8_features.best_score_","696b49e0":"import torch.nn as nn\nimport torch\nimport torch.nn.functional as F","c317ade5":"model=nn.Sequential(nn.Linear(9,18), nn.ReLU(),nn.Linear(18,18),nn.ReLU(),nn.Linear(18,18),\n                   nn.ReLU(),nn.Linear(18,1))\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1=nn.Linear(753,18)\n        self.l2=nn.Linear(18,18)\n        self.l3=nn.Linear(18,18)\n        self.l4=nn.Linear(18,1)\n        \n    def forward(self,x):\n        x=self.l1(x)\n        x=F.relu(F.dropout(x,0.20))\n        x=self.l2(x)\n        x=F.relu(F.dropout(x,0.5))\n        x=self.l3(x)\n        x=F.relu(F.dropout(x,0.2))\n        x=self.l3(x)\n        x=F.relu(F.dropout(x,0.20))\n        x=self.l4(x)\n        \n        return x\n        \n","12340840":"#model=lstm_model#Model()\nmodel=model\ncriterion = nn.BCEWithLogitsLoss()\noptim=torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.99, nesterov=True)","3bac9341":"x=torch.from_numpy(x_train_transformed_9_features[:600]).float()\n#x=torch.from_numpy(x_train[:600]).float()\ny=torch.from_numpy(y_train[:600]).float()\n\ndef train(model,x,y):\n    \n    losses=[]\n    \n    for i in range(501):\n        output=model(x)\n        loss=criterion(output.view(y.shape[0]),y)\n    \n        losses.append(loss)\n    \n        #if (i%50==0):\n         #   print(i,' ',loss.item())\n    \n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n    \n    # training accuracy\n    with torch.no_grad():\n        preds=torch.sigmoid(model(x)).round()\n        print('\\ntraining report: \\n',classification_report(preds.view(preds.shape[0]),y))\n        \n    return model, losses\n\nmodel, losses= train(model,x,y)","84827336":"%matplotlib inline\nplt.plot(losses)\nplt.show()","571bec07":"def test(model, tstx, tsty):\n    with torch.no_grad():\n    \n        preds=torch.sigmoid(model(tstx)).round()\n        print('testing report:')\n        print(classification_report(preds.view(preds.shape[0]), tsty))\n    \n    \n    \n        print(torch.sigmoid(preds).round().shape)","1e86d367":"tstx=torch.from_numpy(x_train_transformed_9_features[600:]).float()\n#tstx=torch.from_numpy(x_train[600:]).float()\ntsty=torch.from_numpy(y_train[600:]).float()\n\ntest(model,tstx,tsty)","8d01f0bc":"x_train.shape","e7d3236a":"def run_network(model, trainx,trainy, testx, testy):\n    model=model\n    criterion = nn.BCEWithLogitsLoss()\n    optim=torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.99, nesterov=True)\n    model, losses= train(model,trainx, trainy)\n    test(model, testx, testy)\n    \n    return model, losses","fa5e2acf":"fold=0\nlosses=[[] for i in range(10)]\n\nfor trids, tstids in k_fold.split(x_train_transformed_9_features):\n    trainx, testx=x_train_transformed_9_features[trids,:], x_train_transformed_9_features[tstids,:]\n    trainy, testy=y_train[trids], y_train[tstids]\n    \n    trainx=torch.from_numpy(trainx).float()\n    trainy=torch.from_numpy(trainy).float()\n    testx=torch.from_numpy(testx).float()\n    testy=torch.from_numpy(testy).float()\n    \n    print('fold: ',fold)\n    \n    model=model\n    \n    _,losses[fold]=run_network(model, trainx,trainy, testx,testy)\n    \n\n\n\n","f96eb2fa":"losses=[[] for i in range(10)]\nlosses","3884d592":"run_network(model,x,y,tstx,tsty)","5da66117":"# LSTM","489e23a2":"All the data was imported in the object type, we now need to take care about the types in the dataframe","ab3f71fd":"## Remove some corelated features \nIn the paper it is suggested to remove the TQWT features ","36f1243b":"\npip install -r requirements.txt\npip install pipwin\npipwin install -r requirements.txt","0e403189":"#### Gather most general metadata about the data","edbe8413":"Note: If we remove all the outlier in all the columns, one could end up with a dataset which is too small in order to predict anything.","e03ac483":"**`Read data and Display data dimension`**"}}