{"cell_type":{"a73778c4":"code","4ff2bd98":"code","6548cbf6":"code","0b9c8af0":"code","8147bd6f":"code","1f570c3f":"code","ffe9d8bf":"code","7b10d761":"code","37e32239":"code","ac47b412":"code","12d7f2c9":"code","85ccef8b":"code","d7172bf7":"code","eb97ea7b":"code","29b49aab":"code","7f9878ce":"code","ac9a4272":"code","acd77699":"code","fca91419":"code","8cf7dae7":"code","a9290257":"code","b3caf293":"code","2e837edd":"code","0cb0ecdf":"code","afc2c47a":"code","550b408b":"code","3994aed8":"markdown","78548521":"markdown","1f52f986":"markdown","b9c3de84":"markdown","41b2a0f5":"markdown","31b90a1e":"markdown","8a4d8e2c":"markdown","7337f713":"markdown","58da3693":"markdown","e277956a":"markdown"},"source":{"a73778c4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\n\n# import required libraries for clustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler","4ff2bd98":"df = pd.read_csv('\/kaggle\/input\/online-retail-customer-clustering\/OnlineRetail.csv', sep=\",\", encoding=\"ISO-8859-1\", header=0)","6548cbf6":"# first five row\ndf.head()","0b9c8af0":"# size of datset\ndf.shape","8147bd6f":"# statistical summary of numerical variables\ndf.describe()","1f570c3f":"# summary about dataset\ndf.info()","ffe9d8bf":"# check for missing values\ndf.isna().sum() \/ df.shape[0] * 100","7b10d761":"# Droping rows having missing values\n\ndf = df.dropna()\ndf.shape","37e32239":"# New Attribute : Monetary\n\ndf['Amount'] = df['Quantity']*df['UnitPrice']\n\nrfm_m = df.groupby('CustomerID')['Amount'].sum()\nrfm_m = rfm_m.reset_index()\nrfm_m.head()","ac47b412":"# New Attribute : Frequency\n\nrfm_f = df.groupby('CustomerID')['InvoiceNo'].count()\n\nrfm_f = rfm_f.reset_index()\nrfm_f.columns = ['CustomerID', 'Frequency']\nrfm_f.head()","12d7f2c9":"# Merging the two dfs\n\nrfm = pd.merge(rfm_m, rfm_f, on='CustomerID', how='inner')\nrfm.head()","85ccef8b":"# New Attribute : Recency\n\n# Convert to datetime to proper datatype\n\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'],format='%d-%m-%Y %H:%M')","d7172bf7":"# Compute the maximum date to know the last transaction date\n\nmax_date = max(df['InvoiceDate'])\nmax_date","eb97ea7b":"# Compute the difference between max date and transaction date\n\ndf['Diff'] = max_date - df['InvoiceDate']\ndf.head()","29b49aab":"# Compute last transaction date to get the recency of customers\n\nrfm_p = df.groupby('CustomerID')['Diff'].min()\n\nrfm_p = rfm_p.reset_index()\nrfm_p.head()","7f9878ce":"# Extract number of days only\n\nrfm_p['Diff'] = rfm_p['Diff'].dt.days\nrfm_p.head()","ac9a4272":"# Merge tha dataframes to get the final RFM dataframe\n\nrfm = pd.merge(rfm, rfm_p, on='CustomerID', how='inner')\n\nrfm.columns = ['CustomerID', 'Amount', 'Frequency', 'Recency']\nrfm.head()","acd77699":"# Outlier Analysis of Amount Frequency and Recency\n\nattributes = ['Amount','Frequency','Recency']\nplt.rcParams['figure.figsize'] = [10,8]\nsns.boxplot(data = rfm[attributes], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Range\", fontweight = 'bold')\nplt.xlabel(\"Attributes\", fontweight = 'bold')","fca91419":"# Removing (statistical) outliers for Amount\nQ1 = rfm.Amount.quantile(0.05)\nQ3 = rfm.Amount.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Amount >= Q1 - 1.5*IQR) & (rfm.Amount <= Q3 + 1.5*IQR)]\n\n# Removing (statistical) outliers for Recency\nQ1 = rfm.Recency.quantile(0.05)\nQ3 = rfm.Recency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Recency >= Q1 - 1.5*IQR) & (rfm.Recency <= Q3 + 1.5*IQR)]\n\n# Removing (statistical) outliers for Frequency\nQ1 = rfm.Frequency.quantile(0.05)\nQ3 = rfm.Frequency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Frequency >= Q1 - 1.5*IQR) & (rfm.Frequency <= Q3 + 1.5*IQR)]","8cf7dae7":"# Rescaling the attributes\n\nrfm_df = rfm[['Amount', 'Frequency', 'Recency']]\n\n# Instantiate\nscaler = StandardScaler()\n\n# fit_transform\nrfm_df_scaled = scaler.fit_transform(rfm_df)\nrfm_df_scaled.shape","a9290257":"rfm_df_scaled = pd.DataFrame(rfm_df_scaled)\nrfm_df_scaled.columns = ['Amount', 'Frequency', 'Recency']\n\nrfm_df_scaled.head()","b3caf293":"# create an object\ndb = DBSCAN(eps=0.8, min_samples=7, metric='euclidean')\n\n# fit the model\ndb.fit(rfm_df_scaled)","2e837edd":"# Cluster labled\ndb.labels_","0cb0ecdf":"from sklearn.metrics import silhouette_score\n\ncluster_labels = db.labels_   \n\n# silhouette score\nsilhouette_avg = silhouette_score(rfm_df_scaled, cluster_labels)\nprint(\"The silhouette score is\", format(silhouette_avg))\n","afc2c47a":"rfm_df_scaled['label']=db.labels_\n\nrfm_df_scaled.head()","550b408b":"for c in rfm_df_scaled.columns[:-1]:\n    plt.figure(figsize=(6,4))\n    sns.boxplot(data=rfm_df_scaled, y=c, x='label')\n    plt.show()","3994aed8":"There are 2 types of outliers and we will treat outliers as it can skew our dataset\u00b6\n* Statistical\n* Domain specific","78548521":"### Rescaling the Attributes\nIt is extremely important to rescale the variables so that they have a comparable scale.| There are two common ways of rescaling:\n\n* Min-Max scaling\n* Standardisation\n\nHere, we will use Standardisation Scaling.","1f52f986":"# Building the Model","b9c3de84":"# Exploratory data analysis","41b2a0f5":"## If this Kernel helped you in any way, UPVOTES would be very much appreciated","31b90a1e":"# Load dataset","8a4d8e2c":"# Evaluation\n\n### Silhouette","7337f713":"# What is DBSCAN?\nDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise and is one of clustering algorithms.\nAs the name of paper suggests the core idea of DBSCAN is around concept of dense regions. The assumption is that natural clusters are composed of densely located points. This requires definition of \u201cdense region\u201d. To do these two parameters are required for DBSCAN algorithm.\n\n* Eps, \u03b5 - distance\n* MinPts \u2013 Minimum number of points within distance Eps\n\nA \u201cdense region\u201d is therefore created by a minimum number of points within distance between all of them, Eps. Points which are within this distance but not close to minimum number of other points are treated as \u201cborder points\u201d. Remaining ones are noise or outliers. This is shown in the picture below (for MinPts=3). Red points (D) are in a \u201cdense region\u201d \u2013 each one has minimum of 3 neighbours within distance Eps. Green points (B) are border ones \u2013 they have a neighbour within distance Eps but less than 3. Blue point (O) is an outlier \u2013 no neighbours within distance Eps.\n\n### Advantages of this approach:\n\n* it finds number of clusters itself, based on eps and MinPts parameters\n* It it able to differentiate elongated clusters or clusters surrounded by other clusters in contrary to e.g. K-Means where clusters are always convex.\n* It is also able to find points not fitting into any cluster \u2013 it detects outliers.\n\n### The biggest drawback of DBSCAN:\n\n* High computational expense of average O(n log(n)) coming from a need to execute a neighbourhood query for each point.\n* Poorly identifies clusters with various densities","58da3693":"# Import libraries","e277956a":"### Data Preparation\n\nWe are going to analysis the Customers based on below 3 factors:\n* R (Recency): Number of days since last purchase\n* F (Frequency): Number of tracsactions\n* M (Monetary): Total amount of transactions (revenue contributed)"}}