{"cell_type":{"e11b7b4d":"code","088d010b":"code","b6148bc0":"code","4e54723f":"code","7cdcec5a":"code","8b6a5778":"code","f276ac72":"code","93fb6a09":"code","fca6d595":"code","e5990803":"code","7d46e859":"code","2f29a138":"code","94a2fe7d":"code","ca8180ac":"code","5ee82ead":"code","0a84ee58":"code","d01bffce":"code","bcfb0df0":"code","4c118144":"code","2cd87aeb":"code","b6a4865f":"code","9a56f56c":"code","2a0800bf":"code","f55fa285":"code","9e28d953":"code","d6cb43b1":"code","745d807e":"code","4b4da1ad":"code","3e562fb4":"code","b003a606":"code","5f7d9592":"code","3c054e16":"code","5f95df59":"code","94b5e4a6":"code","90fb97c4":"code","1c3e3638":"code","f0c9ae64":"code","2177b744":"code","41fc7e01":"code","a4b96641":"code","783f1599":"code","05989cfe":"code","43e091b3":"code","c972243c":"code","75ef0ee2":"code","a6adc4c9":"code","b61bacad":"code","f1a0b219":"code","2cdcf1f8":"code","3dfcbe42":"code","546ba37d":"code","fe21684e":"markdown","55506b81":"markdown","30e1b17a":"markdown","86a3b82f":"markdown","41af7442":"markdown","94ad6f34":"markdown","271f6a09":"markdown","8aab2678":"markdown","2c6790ed":"markdown","25ec6253":"markdown","1337092e":"markdown","3a8d57e0":"markdown","21496437":"markdown","01b98558":"markdown","cf1d5565":"markdown","c4f33333":"markdown","6ceae3fc":"markdown"},"source":{"e11b7b4d":"!pip install -q timm >> \/dev\/null","088d010b":"import os\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport shortuuid\nimport timm\n\nimport torch\nimport torchvision\nimport torch.onnx\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import transforms as T\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import roc_auc_score","b6148bc0":"class Config:\n    \"\"\"Configuration class\n    \"\"\"\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    RANDOM_STATE = 4711\n    TEST_SIZE = 0.3\n    \n    ENCODER = 'tf_efficientnet_b0'  \n    SUBMISSION_FILE = 'submission.csv'\n    INPUT_DIR = '..\/input\/seti-breakthrough-listen'\n    OUTPUT_DIR = '.\/output\/'\n    MIXUP_DIR = '.\/mixups'\n    MODEL_FILE = os.path.join('.\/', f'seti_{ENCODER}_{DEVICE}.pth')\n    \n    TRAIN_LABELS = '..\/input\/seti-breakthrough-listen\/train_labels.csv'\n    TEST_LABELS = '..\/input\/seti-breakthrough-listen\/sample_submission.csv'\n    TRAIN_FILE_FORMAT = '..\/input\/seti-breakthrough-listen\/train\/{}\/{}.npy'\n    TEST_FILE_FORMAT = '..\/input\/seti-breakthrough-listen\/test\/{}\/{}.npy'\n    IMAGE_SIZE = (256, 3 * 273)\n    BATCH_SIZE = 8\n    N_EPOCH = 10\n    N_CHANNELS = 1\n    TARGET_SIZE = 1\n    SAMPLE_FRAC = 1.0\n    LEARNING_RATE = 0.0001\n\n    TARGET = 'target'\n    ID = 'id'\n    FILE_COL = 'file_path'\n    GROUP_COL = 'group'\n    \n    @staticmethod\n    def set_seed():\n        torch.manual_seed(Config.RANDOM_STATE)\n        random.seed(Config.RANDOM_STATE)\n        np.random.seed(Config.RANDOM_STATE)\n    \n    @staticmethod\n    def settings():\n        # matplotlib\n        plt.rc('font', size=15)\n        plt.rc('axes', titlesize=18)  \n        plt.rc('xtick', labelsize=10)  \n        plt.rc('ytick', labelsize=10)\n        \n        # seaborn\n        sns.set_style(\"whitegrid\")","4e54723f":"Config.set_seed()\nConfig.settings()  ","7cdcec5a":"def to_numpy(tensor):\n    \"\"\"Auxiliary function to convert tensors into numpy arrays\n    \"\"\"\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()","8b6a5778":"print(torch.__version__)\nprint(f'Using {Config.DEVICE} device.')","f276ac72":"def load_labels(\n    file_name:str=Config.TRAIN_LABELS, \n    file_format:str=Config.TRAIN_FILE_FORMAT\n) -> pd.DataFrame:\n    \"\"\"\n    \"\"\"\n    df = pd.read_csv(file_name)\n\n    df[Config.GROUP_COL] = df[Config.ID].apply(lambda r: r[0])\n    df[Config.FILE_COL] = df[Config.ID].apply(lambda r: file_format.format(r[0], r))\n\n    return df.set_index(Config.ID)","93fb6a09":"train_labels = load_labels().sample(frac=Config.SAMPLE_FRAC, random_state=Config.RANDOM_STATE)\ntrain_labels","fca6d595":"test_labels = load_labels(Config.TEST_LABELS, Config.TEST_FILE_FORMAT)\ntest_labels","e5990803":"def plot_target(data:pd.DataFrame) -> None:\n    \"\"\"\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(7, 5))\n    sns.countplot(\n        x=Config.TARGET,\n        data=train_labels,\n    );\n    \n    plt.ylabel(\"# Observations\", size=20);\n    plt.xlabel(\"Target\", size=20);\n    \n    plt.title('Label distribution', size=20)\n    plt.tight_layout()        \n    plt.show()","7d46e859":"plot_target(train_labels)","2f29a138":"def plot_groups(data:pd.DataFrame) -> None:\n    \"\"\"\n    \"\"\"\n    df = train_labels.groupby([Config.GROUP_COL, Config.TARGET]).count()\n    df.columns = ['count']\n    df = df.reset_index()\n    \n    fig, ax = plt.subplots(figsize=(10, 5))\n    sns.barplot(\n        x=df['group'],\n        y=df['count'],\n        hue = df['target']\n    );\n    \n    plt.ylabel(\"# Observations\", size=20);\n    plt.xlabel(\"Groups\", size=20);\n    \n    plt.title('Group distribution', size=20)\n    plt.tight_layout()        \n    plt.show()","94a2fe7d":"plot_groups(train_labels)","ca8180ac":"def get_image(\n    sigmal_id:str, \n    labels:pd.DataFrame\n) -> np.array:\n    \"\"\"\n    \"\"\"\n    file_name = labels.loc[sigmal_id, Config.FILE_COL]\n    data = np.load(file_name).astype(np.float32)\n    \n    signal = data[0]\n    for i in [2, 4]:\n        signal = np.vstack((signal, data[i]))\n    \n    return signal.transpose()","5ee82ead":"def visualize_data(\n    sigmal_id:str,\n    labels:pd.DataFrame,\n    transform=None\n) -> None:\n    \"\"\"\n    \"\"\"\n    img = get_image(sigmal_id, labels)\n    label = labels.loc[sigmal_id][Config.TARGET]\n    \n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n    \n    if transform:\n        img = transform(image=img)['image']\n    \n    ax.imshow(img, cmap='gray')\n    \n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    plt.title(f'id: {sigmal_id} - label: {label}')\n    plt.tight_layout()        \n    plt.show()","0a84ee58":"signal_ids = train_labels.sample(n=5, random_state=Config.RANDOM_STATE).index\n\nfor signal_id in signal_ids:\n    visualize_data(signal_id, train_labels)","d01bffce":"def sample_beta_distribution(alpha=0.4):\n    \"\"\"Draw samples from a Beta distribution with a=b=alpha.\n    \"\"\"\n    p = np.random.beta(a=alpha, b=alpha, size=None)\n    return np.min([p, 1-p])","bcfb0df0":"import uuid\n\ndef mix_up(\n    labels:pd.DataFrame, \n    signal1_id:str, \n    signal2_id:str\n) -> (str, float, str, str):\n    \"\"\"\n    \"\"\"\n    p = sample_beta_distribution()\n\n    label1 = labels.loc[signal1_id]\n    label2 = labels.loc[signal2_id]\n\n    fname1 = label1[Config.FILE_COL]\n    fname2 = label2[Config.FILE_COL]\n\n    target1 = label1[Config.TARGET]\n    target2 = label2[Config.TARGET]\n\n    target = (1 - p) * target1 + p * target2\n\n    data1 = np.load(fname1).astype(np.float32)\n    data2 = np.load(fname2).astype(np.float32)\n\n    data = np.zeros(data1.shape)\n    for i in range(0, 6):\n        data[i] = p * data1[i] + (1 - p) * data2[i]\n    \n    signal_id = uuid.uuid4().hex[0:12]\n    fname = f'{signal_id}.npy'\n    file_path = os.path.join(Config.MIXUP_DIR, fname)\n    \n    return data, signal_id, target, 'm', file_path ","4c118144":"import shutil\n\nshutil.rmtree('.\/mixups', ignore_errors=True)\nos.mkdir('.\/mixups')","2cd87aeb":"def create_mixup_labels(labels:pd.DataFrame, seed:int):\n    \"\"\"\n    \"\"\"\n    df = pd.DataFrame(columns=[\n        'id', \n        'target', \n        'group', \n        'file_path'\n    ])\n\n    class_0 = labels[labels[Config.TARGET] == 0]\n    class_1 = labels[labels[Config.TARGET] == 1]\n\n    n_sample = class_1.shape[0]\n    class_0 = class_0.sample(\n        n=n_sample, \n        random_state=seed\n    )\n\n    stream = tqdm(class_1.index)\n    for idx, signal1_id in enumerate(stream):\n        signal2_id = class_0.iloc[idx].name\n        data, signal_id, target, group, file_path = mix_up(\n            labels, \n            signal1_id, # class 1\n            signal2_id  # class 0\n        )\n\n        row = {\n            'id': signal_id,\n            'target': target,\n            'group': group,\n            'file_path': file_path\n        }\n        df = df.append(row, ignore_index=True)\n        np.save(file_path, data) \n\n    df.set_index(Config.ID, inplace=True)\n    return df","b6a4865f":"mixup_labels = create_mixup_labels(train_labels, seed=2021)","9a56f56c":"signal_ids = mixup_labels.sample(n=5, random_state=Config.RANDOM_STATE).index\n\nfor signal_id in signal_ids:\n    visualize_data(signal_id, mixup_labels)","2a0800bf":"train_labels = load_labels().append(mixup_labels).sample(frac=1)\ntrain_labels","f55fa285":"def get_train_transforms(image_size=Config.IMAGE_SIZE):\n    w, h = image_size\n    return A.Compose([\n        A.Resize(w, h),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomBrightness(limit=0.6, p=0.5),\n        A.Cutout(\n            num_holes=10, \n            max_h_size=12, \n            max_w_size=12,\n            fill_value=0, \n            always_apply=False, \n            p=0.5\n        ),\n        A.ShiftScaleRotate(\n            shift_limit=0.25, \n            scale_limit=0.1, \n            rotate_limit=0,\n            p=0.3\n        ),\n    ])","9e28d953":"def get_valid_transforms(image_size=Config.IMAGE_SIZE):\n    w, h = image_size\n    return A.Compose([\n        A.Resize(w, h),\n    ])","d6cb43b1":"train_transform = get_train_transforms()\ntest_transform = get_valid_transforms()","745d807e":"signal_ids = train_labels.sample(n=5, random_state=Config.RANDOM_STATE).index\n\nfor signal_id in signal_ids:\n    visualize_data(signal_id, train_labels, transform=train_transform)","4b4da1ad":"from torch.utils.data import Dataset\n\nclass SETIDataset(Dataset):\n    \"\"\"\n    \"\"\"\n    def __init__(self,\n                 labels:pd.DataFrame,\n                 targets:pd.Series,\n                 transform=None,\n                 is_train:bool=True):\n        self.labels = labels\n        self.targets = targets\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        image_id = self.labels.iloc[idx].name\n        img = get_image(image_id, self.labels)\n\n        if self.transform:\n            img = self.transform(image=img)['image']\n        \n        target = torch.tensor(self.targets.iloc[idx]).float()\n        img = torch.tensor(img).unsqueeze(0)\n        \n        return img, target ","3e562fb4":"from sklearn.model_selection import train_test_split\n\ndata = train_labels.sample(\n    frac=Config.SAMPLE_FRAC, \n    random_state=Config.RANDOM_STATE\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data[[Config.FILE_COL]], \n    pd.Series(data[Config.TARGET]),  \n    test_size=Config.TEST_SIZE, \n    random_state=Config.RANDOM_STATE\n)","b003a606":"from torch.utils.data import WeightedRandomSampler\n\ny = y_train.apply(lambda x: 1 if x > 0.5 else 0)\nclass_counts = y.value_counts().to_list()\nnum_samples = sum(class_counts)\nlabels = y.to_list()\n\nclass_weights = [num_samples \/ class_counts[i] for i in range(len(class_counts))]\nweights = [class_weights[labels[i]] for i in range(int(num_samples))]\nsampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))","5f7d9592":"train_set = SETIDataset(X_train, y_train, transform=train_transform, is_train=True)\ntest_set = SETIDataset(X_test, y_test, transform=test_transform, is_train=False)","3c054e16":"print(f'Train size: {len(train_set)}')\nprint(f'Test size: {len(test_set)}')","5f95df59":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    train_set, \n    batch_size=Config.BATCH_SIZE, \n    num_workers=4, \n    pin_memory=True, \n    drop_last=True,\n    sampler=sampler\n)\n\nvalid_loader = DataLoader(\n    test_set, \n    batch_size=Config.BATCH_SIZE, \n    shuffle=True, \n    num_workers=4, \n    pin_memory=True, \n    drop_last=True\n)","94b5e4a6":"class SetiModel(nn.Module):\n    \"\"\"\n    \"\"\"\n    def __init__(self, \n                 model_name=Config.ENCODER,\n                 in_channels=Config.N_CHANNELS,\n                 pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(\n            model_name, \n            pretrained=pretrained, \n            in_chans=in_channels\n        )\n\n        self.model.classifier = nn.Linear(\n            self.model.classifier.in_features, \n            Config.TARGET_SIZE\n        )\n         \n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \n    def roc_score(self, output, target):\n        try:\n            y_pred = torch.sigmoid(output).cpu() \n            y_pred = y_pred.detach().cpu().numpy()\n            target = target.cpu() \n\n            return roc_auc_score(target, y_pred)\n        except:\n            return 0.5\n    \n    def __get_desc_(self, phase, epoch, loss, roc):\n        n_epoch = Config.N_EPOCH\n        return f'Epoch {epoch:3d}\/{n_epoch} - {phase} - Loss:{loss:.4f}, ROC:{roc:.4f}'\n    \n    def train_one_epoch(self, epoch, dataloader, criterion, optimizer):\n        epoch_loss = 0.0\n        epoch_roc = 0.0\n       \n        self.model.train()\n        \n        stream = tqdm(dataloader)\n        for batch, (X, y) in enumerate(stream, start=1):\n            X = X.to(Config.DEVICE)\n            y = y.to(Config.DEVICE)\n        \n            # compute prediction and loss\n            y_preds = self.forward(X).view(-1)\n            loss = criterion(y_preds, y)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            roc = self.roc_score(y_preds, y)\n            \n            epoch_loss += loss.item()\n            epoch_roc += roc\n            \n            desc = self.__get_desc_('Train', epoch, epoch_loss\/batch, epoch_roc\/batch)\n            stream.set_description(desc)\n            \n        count = len(dataloader)\n        return epoch_loss \/ count, epoch_roc \/ count\n        \n        \n    def validate_one_epoch(self, epoch, dataloader, criterion):\n        epoch_loss = 0.0\n        epoch_roc = 0.0\n\n        self.model.eval()\n\n        stream = tqdm(dataloader)\n        for batch, (X, y) in enumerate(stream, start=1):\n            with torch.no_grad():\n                X = X.to(Config.DEVICE)\n                y = y.to(Config.DEVICE)\n\n                # compute prediction and loss\n                y_preds = self.forward(X).view(-1)\n                loss = criterion(y_preds, y)\n\n                roc = self.roc_score(y_preds, y)\n\n                epoch_loss += loss.item()\n                epoch_roc += roc\n\n                desc = self.__get_desc_('Val', epoch, epoch_loss\/batch, epoch_roc\/batch)\n                stream.set_description(desc)\n            \n        count = len(dataloader)\n        return epoch_loss \/ count, epoch_roc \/ count","90fb97c4":"model = SetiModel(model_name=Config.ENCODER, pretrained=True).to(Config.DEVICE);","1c3e3638":"class MetricMonitor:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.loss = []\n        self.roc = []\n        self.metrics = dict({\n            'loss': self.loss,\n            'roc': self.roc\n        })\n\n    def update(self, metric_name, value):\n        self.metrics[metric_name] += [value]","f0c9ae64":"# initialize the loss function\ncriterion = nn.BCEWithLogitsLoss()\n\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=Config.LEARNING_RATE\n)","2177b744":"def fit_model(\n    model, \n    criterion, \n    optimizer, \n    train_loader,\n    valid_loader=None\n):\n    \"\"\"\n    \"\"\"\n    train_monitor = MetricMonitor()\n    val_monitor = MetricMonitor()\n    \n    # train model\n    for epoch in range(1, Config.N_EPOCH + 1):\n        epoch_loss, epoch_roc = model.train_one_epoch(\n            epoch,\n            train_loader, \n            criterion, \n            optimizer)\n        \n        # update training metrics\n        train_monitor.update('loss', epoch_loss)\n        train_monitor.update('roc', epoch_roc)\n\n        # validatie model\n        epoch_loss, epoch_roc = model.validate_one_epoch(\n            epoch,\n            valid_loader, \n            criterion)\n    \n        # update validation metrics\n        val_monitor.update('loss', epoch_loss)\n        val_monitor.update('roc', epoch_roc)\n    \n    return train_monitor, val_monitor","41fc7e01":"%%time\n\ntrain_monitor, val_monitor = fit_model(model, criterion, optimizer, train_loader, valid_loader)","a4b96641":"from matplotlib.ticker import MaxNLocator \n\ndef plot_result(\n    train_loss, \n    val_loss, \n    train_roc,\n    val_roc\n) -> None:\n    \n    epochs = range(1, len(train_loss) + 1)\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))\n    \n    # plot loss values\n    idx = 0\n    \n    ax[idx].plot(epochs, train_loss, label='Training loss', marker ='o')\n    ax[idx].plot(epochs, val_loss, label='Validation loss', marker ='o')\n    ax[idx].legend(frameon=False, fontsize=14)\n    \n    ax[idx].get_xaxis().set_major_locator(MaxNLocator(integer=True))\n    ax[idx].set_title('Loss', fontsize=18)\n    ax[idx].set_xlabel('Epoch', fontsize=14) \n    ax[idx].set_ylabel('Loss', fontsize=14)  \n    \n    # plot ROC score\n    idx = idx + 1\n\n    ax[idx].plot(epochs, train_roc, label='Training ROC-Score', marker ='o')\n    ax[idx].plot(epochs, val_roc, label='Validation ROC-Score', marker ='o')\n    ax[idx].legend(frameon=False, fontsize=14)\n    \n    ax[idx].get_xaxis().set_major_locator(MaxNLocator(integer=True))\n    ax[idx].set_title('ROC-Score', fontsize=18)\n    ax[idx].set_xlabel('Epoch', fontsize=14) \n    ax[idx].set_ylabel('ROC-Score', fontsize=14) \n        \n    plt.show()","783f1599":"plot_result(\n    train_monitor.loss, \n    val_monitor.loss,\n    train_monitor.roc,\n    val_monitor.roc\n)","05989cfe":"def save_model(\n    model, \n    save_path:str=Config.MODEL_FILE\n) -> None:\n    \"\"\"Save final model\n    \"\"\"\n    torch.save(model.state_dict(), save_path)","43e091b3":"save_model(model)","c972243c":"def load_model(\n    model, \n    load_path=Config.MODEL_FILE\n) -> None:\n    model.load_state_dict(torch.load(load_path))\n    model.eval()","75ef0ee2":"model = model = SetiModel(model_name=Config.ENCODER).to(Config.DEVICE)\nload_model(model)","a6adc4c9":"submission_set = SETIDataset(\n    test_labels, \n    test_labels[Config.TARGET], \n    transform=test_transform, \n    is_train=False\n)\n\nsubmission_loader = DataLoader(\n    submission_set, \n    batch_size=Config.BATCH_SIZE\n)","b61bacad":"model.eval()\ny_pred_proba = None\n\nstream = tqdm(submission_loader)\nfor batch, (X, y) in enumerate(stream, start=1):\n    X = X.to(Config.DEVICE)\n    y = to_numpy(y.to(Config.DEVICE))\n    \n    output = model(X).to(Config.DEVICE)\n    predictions = torch.sigmoid(output).cpu().detach().numpy()\n    \n    if y_pred_proba is None:\n        y_pred_proba = predictions\n    else:\n        y_pred_proba = np.vstack((y_pred_proba, predictions))","f1a0b219":"test_labels[Config.TARGET] = y_pred_proba.reshape(-1)\ntest_labels","2cdcf1f8":"test_labels[[Config.TARGET]].to_csv(Config.SUBMISSION_FILE, index=True)","3dfcbe42":"signal_ids = [\n    'ffd062e29fe5',\n    'ff74eb48288b',\n    '11fd2f876dd4',\n    '68cc1bcacd48',\n    '4091a18dca18'\n]\n\nfor signal_id in signal_ids:\n    visualize_data(signal_id, test_labels)","546ba37d":"# remove mixup data\nshutil.rmtree('.\/mixups', ignore_errors=True)","fe21684e":"## Overview\n\n* EfficientNetB0 Model\n* MixUp augmentation\n* PyTorch\n\n## Data\n\n*    `train\/`  a training set of cadence snippet files stored in numpy float16 format (v1.20.1), one file per cadence snippet id, with corresponding labels found in the train_labels.csv file. Each file has dimension (6, 273, 256), with the 1st dimension representing the 6 positions of the cadence, and the 2nd and 3rd dimensions representing the 2D spectrogram.\n\n* `test\/` - the test set cadence snippet files; you must predict whether or not the cadence contains a \"needle\", which is the target for this competition\n\n* `sample_submission.csv` - a sample submission file in the correct format\n\n* `train_labels.csv` - targets corresponding (by id) to the cadence snippet files found in the train\/ folder\n","55506b81":"## Exploratory data analysis (EDA)","30e1b17a":"## Load data","86a3b82f":"## Train model","41af7442":"## Model","94ad6f34":"## Submission","271f6a09":"\n## Imports","8aab2678":"# [SETI Breakthrough Listen - E.T. Signal Search](https:\/\/www.kaggle.com\/c\/seti-breakthrough-listen)\nFind extraterrestrial signals in data from deep space\n\n\n","2c6790ed":"## Plot metrics","25ec6253":"## Configuration","1337092e":"## Group distribution","3a8d57e0":"## Visualize data","21496437":"## Dataset","01b98558":"## Label distribution","cf1d5565":"## Save model","c4f33333":"## Augmentation pipeline","6ceae3fc":"## MixUp augmentation"}}