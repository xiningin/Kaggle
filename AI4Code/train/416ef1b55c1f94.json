{"cell_type":{"6fabd1de":"code","b1a7272a":"code","8bc1fe55":"code","e1d9eafc":"code","f0f928f5":"code","057e1dc7":"code","89177204":"code","2becc7d0":"code","f42abec9":"code","f9608f16":"code","06d1687d":"code","f4bb4427":"code","c5a2f541":"code","fe92b607":"code","9f9a1d11":"code","e0312d77":"code","8882da4a":"code","682c736a":"code","420febc7":"code","3a3547aa":"code","0a5ac0e8":"code","2bcecf22":"code","1c638919":"code","b0db8d8c":"code","9b4988b5":"code","e035e9a2":"code","90056b21":"code","129b7053":"code","134c61e1":"code","5f80cb2d":"code","72a6ce6f":"code","8f069af7":"code","fb64ad68":"code","9830cdc5":"code","c966dfbb":"code","39c49ac8":"code","385707cc":"code","2b527437":"code","f308797a":"code","795a43f5":"markdown","4d8a9ad3":"markdown","849a51c8":"markdown","9b63a952":"markdown","e2487a7a":"markdown","f78b957d":"markdown","9949e1dc":"markdown","b851c08d":"markdown","49a04eb1":"markdown","7bcf824f":"markdown","84720aa7":"markdown","f8ac8a86":"markdown","67edf0ba":"markdown","b4f0ada2":"markdown","8aaa34d6":"markdown","dc72f966":"markdown","17c28cf7":"markdown","0e28ea9c":"markdown","c75303f4":"markdown"},"source":{"6fabd1de":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA, LatentDirichletAllocation, KernelPCA\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest, SelectFromModel, RFECV\nfrom sklearn.model_selection import KFold, cross_val_score, cross_val_predict, train_test_split\nfrom sklearn.preprocessing import MaxAbsScaler, StandardScaler, Normalizer, LabelEncoder, OrdinalEncoder,MinMaxScaler, RobustScaler, QuantileTransformer, PowerTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nimport xgboost as xgb\n\n%matplotlib inline","b1a7272a":"labelled = pd.read_csv('..\/input\/train.csv')\nunlabelled = pd.read_csv('..\/input\/test.csv')","8bc1fe55":"ID = unlabelled.Id","e1d9eafc":"data = pd.concat([labelled, unlabelled], axis= 0)","f0f928f5":"# Check features with null values\nnan_features = data.isnull().sum()[data.isnull().sum() != 0].index.drop(['SalePrice'])","057e1dc7":"# Check features with null values\ndata.isnull().sum()[data.isnull().sum() != 0]","89177204":"# Check numeric and categorical features\ndata.dtypes.value_counts()","2becc7d0":"# Creat a list of categorical features\ncategorical = data.dtypes[data.dtypes == 'object'].index.tolist()\n# Creat a list of integer number features\nnumeric = data.dtypes[data.dtypes != 'object'].index.drop(['Id','SalePrice']).tolist()","f42abec9":"# Visualizing null values\nplt.figure(figsize= (12,6))\nsns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap= 'viridis')","f9608f16":"# Filling missing values in each feature based on feature data dtype.\nfor i in range(len(nan_features)):\n    if data[nan_features].dtypes[i] == 'O':\n        imp1 = SimpleImputer(strategy= 'most_frequent')\n        data[[nan_features[i]]] = imp1.fit_transform(data[[nan_features[i]]])\n    else :\n        imp2 = SimpleImputer(strategy= 'median')\n        data[[nan_features[i]]] = imp2.fit_transform(data[[nan_features[i]]])","06d1687d":"data.isnull().sum()[data.isnull().sum() != 0]","f4bb4427":"# Convert Categorical features into numeric features.\nle = LabelEncoder()\nfor ii in range(len(categorical)):\n    data[categorical[ii]] = le.fit_transform(data[categorical[ii]])","c5a2f541":"# Getting dummy variables fot Categorical features\ndata = pd.get_dummies(data, columns=categorical, drop_first=True)","fe92b607":"# Scaling numeric features\nnull_label = data.SalePrice.isnull()\nwith_label = data.SalePrice.isnull() == False\nscaler = StandardScaler()\ndata.loc[with_label, numeric] = scaler.fit_transform(data.loc[with_label, numeric])\ndata.loc[null_label, numeric] = scaler.transform(data.loc[null_label, numeric])","9f9a1d11":"data['SalePrice'] = np.log(data.SalePrice)","e0312d77":"# Splitting data back into labelled\/unlabelled sets\nlabelled = data.loc[data.SalePrice.isnull() == False]\nunlabelled = data.loc[data.SalePrice.isnull() == True].drop(columns = ['SalePrice'])","8882da4a":"x_train, x_other, y_train, y_other = train_test_split(\n                labelled.drop(columns=['SalePrice']), labelled.SalePrice, train_size=0.7)","682c736a":"x_valid, x_test, y_valid, y_test = train_test_split(\n                                    x_other, y_other, train_size=0.5)","420febc7":"features = labelled.drop(columns=['SalePrice'])\ntarget = labelled.SalePrice\n                ","3a3547aa":"ridge = RidgeCV(cv= 10)","0a5ac0e8":"threshold = np.arange(1, 10, 0.5) *1e-2","2bcecf22":"scores = []\nfor i in threshold:\n    selector = VarianceThreshold(threshold= i)\n    selected_features = selector.fit_transform(features)\n    ridge.fit(selected_features, target)\n    y_pred = ridge.predict(features.loc[:, selector.get_support()])\n    scores.append(mean_squared_error(target, y_pred))\n\n# Plot variance threshold VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"variance threshold\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(threshold, np.array(scores))","1c638919":"print('The minimum MSE score is:', np.min(np.array(scores)))","b0db8d8c":"number_of_features = list(range(1,x_train.shape[1]))","9b4988b5":"scores_k = []\nfor i in number_of_features:\n    selector = SelectKBest(k=i)\n    selected_features = selector.fit_transform(features, target)\n    ridge.fit(selected_features, target)\n    y_pred = ridge.predict(features.loc[:, selector.get_support()])\n    scores_k.append(mean_squared_error(target, y_pred))\n\n# Plot number of selected features VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"Cross validated accuracy score\")    \nplt.plot(number_of_features, scores_k)","e035e9a2":"print(\"Minimum MSE is :\", min(scores_k))","90056b21":"print(\"Optimal number of features :\", np.argmin(np.array(scores_k)) + 1)","129b7053":"selector = RFECV(ridge, step= 1, cv= 5)\nselector.fit(features, target)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)","134c61e1":"print(\"Optimal number of features : %d\" % selector.n_features_)","5f80cb2d":"print(\"Minimum MSE score is :\", np.min(selector.grid_scores_))","72a6ce6f":"threshold = np.arange(1, 5, 0.1) *1e-2","8f069af7":"scores_sfm = []\nfor i in threshold:\n    selector = SelectFromModel(ridge, threshold= i)\n    selector.fit(features, target)\n    selected_features = features.loc[:, selector.get_support()]\n    ridge.fit(selected_features, target)\n    y_pred = ridge.predict(selected_features)\n    scores_sfm.append(mean_squared_error(target, y_pred))\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Threshold Value\")\nplt.ylabel(\"Cross validation score\")    \nplt.plot(threshold, scores_sfm)","fb64ad68":"print(\"Minimum MSE score is :\", np.min(np.array(scores_sfm)))","9830cdc5":"print(\"Optimal threshold :\", threshold[np.argmin(np.array(scores_sfm))])","c966dfbb":"# Fit the model with features selected by RFECV method and the training set\nselector =RFECV(ridge)\nselector.fit(features, target)\nselected_features = selector.get_support()","39c49ac8":"ridge = RidgeCV(cv= 10)\nridge.fit(features.loc[:, selected_features], target)","385707cc":"# Make Prediction fot test data\npred_ridge = pd.DataFrame(np.exp(ridge.predict(unlabelled.loc[:, selected_features])), \n                          columns=['SalePrice'], dtype='int32')","2b527437":"ridge_model = pd.concat([ID, pred_ridge], axis=1)","f308797a":"ridge_model.to_csv('ridgeModel.csv', index= False)","795a43f5":"####  SelectKbest method","4d8a9ad3":"#### VarianceThreshold method","849a51c8":"#### SelectFromModel method","9b63a952":"## Baseline Model - LassoCV Regression","e2487a7a":"## Imputing missing values","f78b957d":"The highest accuracy is obtained after execluding features whose variance is less than 0.01","9949e1dc":"# 5. Features\/Target","b851c08d":"# Data Exploring","49a04eb1":"We conclude the best feature selection method is RFECV.","7bcf824f":"# Train\/Validation\/Test.","84720aa7":"We will seperate the features and target columns from the label data so that it can be used in the feature selection step.","f8ac8a86":"# Model Building","67edf0ba":"We will perform data split on two steps using train_test_split function:\n   1. we split data into training set and other set.\n   2. we split the other set into validation set and test set.","b4f0ada2":"We will split the labelled data into 3 sets:\n1. Training set: used for model training. (Size = %70)\n2. Validation set: used for hyperparameter tunning. (Size = %15)\n3. Test set: used for model assessment and comparison of different models. (Size = %15)","8aaa34d6":"####  RFECV method","dc72f966":"# Feature Engineering","17c28cf7":"In this dataset, some features has the value of 'NA' which means 'Not existing' or 'None' and it's not actually missing value.<br\/>\nTo distinguish between 'NA' which means 'Not existing' and 'NA' which means actually missing value, we will check the other features describing the same aspect of the house and we will try to infere whether it's actually missing value or not.","0e28ea9c":"# Data Cleaning","c75303f4":"### Feature Selection"}}