{"cell_type":{"84e4db80":"code","197f9c7b":"code","4e64cf94":"code","367ae26d":"code","c1ef3b04":"code","52127a33":"code","75f4c45e":"code","6bf2b191":"code","d01fc2a8":"code","4a43bfaa":"code","5fcfd21a":"code","6c7589b6":"code","ff300e7a":"code","ffb4fa92":"code","79ea0efd":"code","9d5d1239":"code","5d3786cc":"code","aadab23a":"code","c15c5f5f":"code","f7591d5f":"markdown","9d55550b":"markdown","66fc1244":"markdown","acb896b4":"markdown","968d46aa":"markdown","dc5f4a84":"markdown","7186451a":"markdown","52a4375b":"markdown"},"source":{"84e4db80":"!pip install optuna==2.0","197f9c7b":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nimport optuna","4e64cf94":"optuna.__version__","367ae26d":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nsample_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","c1ef3b04":"# Sex\u3068Embarked\u306eOne-Hot\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\ntrain = pd.get_dummies(train, columns=['Sex', 'Embarked'])\ntest = pd.get_dummies(test, columns=['Sex', 'Embarked'])\n\n# \u4e0d\u8981\u306a\u5217\u306e\u524a\u9664\ntrain.drop(['PassengerId', 'Name', 'Cabin', 'Ticket'], axis=1, inplace=True)\ntest.drop(['PassengerId', 'Name', 'Cabin', 'Ticket'], axis=1, inplace=True)\n\n# train\u306e\u8868\u793a\ndisplay(train.head())\n\nX_train = train.drop(['Survived'], axis=1)  # X_train\u306ftrain\u306eSurvived\u5217\u4ee5\u5916\ny_train = train['Survived']  # Y_train\u306ftrain\u306eSurvived\u5217","52127a33":"def objective(trial):\n    kf = KFold(n_splits=3)\n    gbm = lgb.LGBMClassifier(objective='binary')\n    oof = np.zeros(len(train))\n\n    for fold, (train_index, valid_index) in enumerate(kf.split(X_train, y_train)):\n        train_x, valid_x = X_train.iloc[train_index], X_train.iloc[valid_index]\n        train_y, valid_y = y_train[train_index], y_train[valid_index]\n        gbm = lgb.LGBMClassifier(objective='binary',\n                                 reg_alpha=trial.suggest_loguniform('reg_alpha', 1e-4, 100.0),\n                                 reg_lambda=trial.suggest_loguniform('reg_lambda', 1e-4, 100.0),\n                                 num_leaves=trial.suggest_int('num_leaves', 10, 40),\n                                 silent=True)\n        gbm.fit(train_x, train_y, eval_set = [(valid_x, valid_y)],\n                early_stopping_rounds=20,\n                verbose=-1) # \u5b66\u7fd2\u306e\u72b6\u6cc1\u3092\u8868\u793a\u3057\u306a\u3044\n        oof[valid_index] = gbm.predict(valid_x)\n\n    accuracy = accuracy_score(y_train, oof)\n    return 1.0 - accuracy","75f4c45e":"study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n# \u30b7\u30fc\u30c9\u3092\u56fa\u5b9a\u3002\n# \u53c2\u8003\uff1ahttps:\/\/qiita.com\/phorizon20\/items\/1b795beb202c2dc378ed\n\nstudy.optimize(objective, n_trials=50)","6bf2b191":"optuna.importance.get_param_importances(study)","d01fc2a8":"fig = optuna.visualization.plot_param_importances(study)\nfig.show()","4a43bfaa":"importances = optuna.importance.get_param_importances(study)","5fcfd21a":"from collections import OrderedDict\n\nimportances = OrderedDict(reversed(list(importances.items())))\nimportance_values = list(importances.values())\nparam_names = list(importances.keys())","6c7589b6":"importances, importance_values, param_names","ff300e7a":"import matplotlib.pyplot as plt\nplt.barh(range(len(importance_values)), importance_values)\nplt.yticks(range(3), param_names)\nplt.title('Hyperparameter importance');","ffb4fa92":"def objective(trial):\n    kf = KFold(n_splits=3)\n    oof = np.zeros(len(train))\n\n    for fold, (train_index, valid_index) in enumerate(kf.split(X_train, y_train)):\n        train_x, valid_x = X_train.iloc[train_index], X_train.iloc[valid_index]\n        train_y, valid_y = y_train[train_index], y_train[valid_index]\n        dtrain = lgb.Dataset(train_x, label=train_y)\n        dvalid = lgb.Dataset(valid_x, label=valid_y)\n        param = {\n            'objective': 'binary',\n            'metric': 'binary_logloss',\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n            'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        }\n        \n        gbm = lgb.train(param, dtrain, valid_sets=[dvalid], num_boost_round=100, early_stopping_rounds=20, verbose_eval=-1)\n        oof[valid_index] = gbm.predict(valid_x)\n    \n    accuracy = accuracy_score(y_train, np.rint(oof))\n    return accuracy\n     \nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\n \n","79ea0efd":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","9d5d1239":"fig = optuna.visualization.plot_param_importances(study)\nfig.show()","5d3786cc":"study = optuna.create_study()","aadab23a":"import optuna.integration.lightgbm as lgb\n\ndef main():\n    kf = KFold(n_splits=3)\n    oof = np.zeros(len(train))\n    lgb_train = lgb.Dataset(X_train, y_train)\n    param = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1\n    }\n    \n    tuner_cv = lgb.LightGBMTunerCV(\n        param,\n        lgb_train,\n        num_boost_round=100,\n        early_stopping_rounds=20,\n        verbose_eval=20,\n        folds=kf,\n        study=study\n    )\n    \n    tuner_cv.run()\n\n    print(f'Best score: {tuner_cv.best_score}')\n    print('Best params:')\n    print(tuner_cv.best_params)\n\nif __name__ == '__main__':\n    main()","c15c5f5f":"fig = optuna.visualization.plot_param_importances(study)\nfig.show()\n\n# lgb.LightGBMTunerCV \u3067\u306f\u3001importance\u306f\u8868\u793a\u3067\u304d\u306a\u3044\u3093\u3067\u3057\u3087\u3046\u304b\uff1f","f7591d5f":"## \u91cd\u8981\u5ea6\u306e\u8868\u793a\n\ndocument: https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.visualization.plot_param_importances.html#optuna.visualization.plot_param_importances","9d55550b":"kaggle\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u7b2c\uff14\u7248\u306e\u30b3\u30fc\u30c9\u3092\u57fa\u306b\u3001\n* optuna.importance\n\n\u306a\u3069\u8a66\u3057\u3066\u307f\u307e\u3059\u3002\n\n\noptuna\u304cv2.0\u3068\u306a\u308a\u3001 hyperparameter importance module\u304cstable version\u306b\u306a\u3063\u305f\u3002\n> The stable version of the hyperparameter importance module is available.  \n> https:\/\/github.com\/optuna\/optuna\/releases\/tag\/v2.0.0","66fc1244":"## LightGBMTuner\u3092\u4f7f\u3046 ","acb896b4":"# \u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\n\n* reg_alpha\u3068reg_lambda\u306f\u30ed\u30b0\u30b9\u30b1\u30fc\u30eb\uff08trial.suggest_loguniform\uff09\n* num_leaves\u3092int\uff08trial.suggest_int\uff09\n\n\u3092\u9078\u629e\u3057\u3066\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3002  \n\n\u305d\u306e\u4ed6\u3001suggest_categorical\u3001suggest_discrete_uniform\u3001suggest_uniform\u306a\u3069\u3092\u9078\u629e\u3067\u304d\u308b\u3002\n\u8a73\u3057\u304f\u306f\u3001[\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8](https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/trial.html)\u3092\u53c2\u7167","968d46aa":"## \u81ea\u5206\u3067\u63cf\u753b\u3059\u308b","dc5f4a84":"sorce\u3092\u53c2\u8003\u306b  \nhttps:\/\/optuna.readthedocs.io\/en\/stable\/_modules\/optuna\/visualization\/_param_importances.html#plot_param_importances","7186451a":"## lightgbm tuner\u3092\u4f7f\u308f\u306a\u3044","52a4375b":"# lightgbm tuner\n\n\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8: https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.integration.lightgbm.LightGBMTuner.html?highlight=LightGBMTuner  \n> \nYou can find the details of the algorithm and benchmark results in this [blog](https:\/\/medium.com\/optuna\/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258) article by Kohei Ozaki, a Kaggle Grandmaster.\n"}}