{"cell_type":{"3065f7e8":"code","862a88af":"code","dc5ecf26":"code","03170377":"code","5ac69f6a":"code","c50e917c":"code","9f8aad82":"code","40b31374":"code","7b8071af":"code","e68a1ae7":"code","4f1733f4":"code","ab6885c8":"code","461c5f8a":"code","722367dd":"code","32320512":"code","4dbb1aa9":"code","756b886e":"code","7a40c293":"code","864525de":"code","81a7f2d0":"code","b7435421":"code","04659d95":"code","ed24a803":"code","238a3731":"code","a181400d":"code","b6dfa2d0":"code","ca0479a9":"code","cfd30bfd":"code","d797633d":"code","ae1ed996":"code","e1d270f4":"code","56c356c8":"code","e215e3cb":"code","bcc803f9":"code","1f99f3f3":"code","2ef2f996":"code","a518a9c5":"code","57ecc343":"code","d08df194":"code","c3754f85":"code","5d3c6df2":"code","429042df":"code","6dbfab0d":"code","1506787a":"code","76bfa8ab":"code","88642dc4":"markdown","d37585c9":"markdown","df7549e1":"markdown","15236280":"markdown","e9c4bd1f":"markdown","1528f486":"markdown","c137a19a":"markdown","14815052":"markdown","642634cf":"markdown","5b2d7e0f":"markdown","73183b7c":"markdown","b89fd20e":"markdown","3fa0fcee":"markdown","1047dde9":"markdown","8cbefdec":"markdown","3fea640d":"markdown","ff196d13":"markdown","a218f224":"markdown","21ddabbb":"markdown","3662b7f2":"markdown","9b78a28d":"markdown","664f3531":"markdown","6c81e758":"markdown","e70c9d7f":"markdown","8a42a01a":"markdown"},"source":{"3065f7e8":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 50)\nsns.set_style('darkgrid')","862a88af":"train = pd.read_csv('\/kaggle\/input\/otto-group-product-classification-challenge\/train.csv')\ntrain.head(7)","dc5ecf26":"test = pd.read_csv('\/kaggle\/input\/otto-group-product-classification-challenge\/test.csv')\ntest.head(7)","03170377":"sns.countplot(x = train.target)","5ac69f6a":"class_to_order = dict()\norder_to_class = dict()\n\nfor idx, col in enumerate(train.target.unique()):\n    order_to_class[idx] = col\n    class_to_order[col] = idx\n\ntrain[\"target_ord\"] = train[\"target\"].map(class_to_order).astype(\"int16\")\nfeature_columns = [col for col in train.columns if col.startswith(\"feat_\")]\ntarget_column = [\"target_ord\"]","c50e917c":"order_to_class","9f8aad82":"class_to_order","40b31374":"from scipy.stats import skew","7b8071af":"skew = []\nfor i in train[feature_columns].columns:\n    skew.append(train[str(i)].skew())\n    \nskew_df = pd.DataFrame({'Feature': train[feature_columns].columns, 'Skewness': skew})\nskew_df.plot(kind='bar',figsize=(18,10))","e68a1ae7":"from sklearn.preprocessing import QuantileTransformer\ntrain[feature_columns] = QuantileTransformer(copy=False, output_distribution='normal').fit_transform(train[feature_columns])\ntest[feature_columns] = QuantileTransformer(copy=False, output_distribution='normal').fit_transform(test[feature_columns])","4f1733f4":"skew = []\nfor i in train[feature_columns].columns:\n    skew.append(train[str(i)].skew())\n    \nskew_df = pd.DataFrame({'Feature': train[feature_columns].columns, 'Skewness': skew})\nskew_df.plot(kind='bar',figsize=(18,10))","ab6885c8":"# check features for skew\nskew_feats = train[feature_columns].skew().sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew': skew_feats})\nskewness = skewness[abs(skewness) > 3.75].dropna()\nskewed_features = skewness.index.values.tolist()\nskewed_features","461c5f8a":"train_new = train.drop(skewed_features, axis = 1)\ntrain_new","722367dd":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    train_new.drop(['id', 'target', 'target_ord'], axis = 1), \n    train_new[target_column],\n    test_size = 0.275, \n    random_state = 7, \n    stratify = train_new[target_column]\n)","32320512":"from sklearn.neighbors import KNeighborsClassifier","4dbb1aa9":"knc = KNeighborsClassifier(n_neighbors = 25, weights = 'distance')\nknc.fit(X_train, y_train)\nyhat = knc.predict(X_valid)","756b886e":"from sklearn.metrics import classification_report, confusion_matrix","7a40c293":"result = confusion_matrix(y_valid, yhat)\nprint(\"Confusion Matrix:\")\nprint(result)","864525de":"result1 = classification_report(y_valid, yhat)\nprint(\"Classification Report:\")\nprint(result1)","81a7f2d0":"yhat_KNN = knc.predict_proba(X_valid)\nlogloss_KNN = log_loss(y_valid, yhat_KNN)\nprint('Log loss using KNN classifier:', logloss_KNN)","b7435421":"from sklearn.tree import DecisionTreeClassifier","04659d95":"dtree = DecisionTreeClassifier(criterion='entropy', max_depth=8,\n                              min_samples_leaf = 6, max_leaf_nodes = 40,\n                              splitter = 'best')\ndtree.fit(X_train, y_train)\nyhat_tree = dtree.predict_proba(X_valid)\nlogloss_DTree = log_loss(y_valid, yhat_tree)\nprint('Log loss using Decision Tree: ', logloss_DTree)","ed24a803":"# Plot decision tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nfrom sklearn import tree\n\ntree.plot_tree(dtree)","238a3731":"# Export our trained model as a .dot file\nwith open(\"otto.dot\", 'w') as f:\n     f = tree.export_graphviz(dtree, out_file=f, max_depth = 3, impurity = True, \n                              feature_names = train_new.drop(['id', 'target', 'target_ord'], axis = 1).columns.values.tolist(), \n                              class_names = train_new.target.unique().tolist(), \n                              rounded = True, filled = True)\n        \n#Convert .dot to .png to allow display in web notebook\ncheck_call(['dot','-Tpng','otto.dot','-o','otto.png'])\n\n# Annotating chart with PIL\nimg = Image.open(\"otto.png\")\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage(\"sample-out.png\")","a181400d":"from sklearn.linear_model import LogisticRegression","b6dfa2d0":"lr = LogisticRegression(solver = 'saga', warm_start = True,\n                        penalty = 'elasticnet', l1_ratio = 0.3,\n                        random_state = 5, C = 1, max_iter = 500)\nlr.fit(X_train, y_train)\n\nyhat = lr.predict(X_valid)\nyhat_lr = lr.predict_proba(X_valid)\nlogloss_lr = log_loss(y_valid, yhat_lr)\nprint('Log loss using Logistic Regression:', logloss_lr)","ca0479a9":"import itertools\ndef plot_confusion_matrix(cm, classes, normalize = False, title = 'Confusion matrix', cmap = plt.cm.Blues):\n    '''\n    This function prints and plots the confusion matrix. Normalization can be applied by setting normalize = True.\n    '''\n    if normalize:\n        cm = cm.astype('float')\/cm.sum(axis=1)[:,np.newaxis]\n        print('Normalized Confusion matrix')\n    else:\n        print('Confusion matrix without normalization')\n    \n    print(cm)\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 45)\n    plt.yticks(tick_marks, classes)\n    \n    if normalize:\n        fmt = '.2f'\n    else:\n        fmt = 'd'\n    \n    thresh = cm.max()\/2\n    \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment = 'center',\n                color = 'white' if cm[i, j] > thresh else 'black')\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","cfd30bfd":"cnf_matrix = confusion_matrix(y_valid, yhat, labels = train_new.target_ord.unique().tolist())\nnp.set_printoptions(precision = 2)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes = train_new.target.unique().tolist())","d797633d":"print('Classification Report:')\nprint(classification_report(y_valid, yhat))","ae1ed996":"from sklearn import svm","e1d270f4":"svm = svm.SVC(kernel = 'rbf', probability = True, random_state = 7)\nsvm.fit(X_train, y_train)\n\nyhat = svm.predict(X_valid)\nyhat_svm = svm.predict_proba(X_valid)\nlogloss_svm = log_loss(y_valid, yhat_svm)\nprint('Logloss using Support Vector Machines:', logloss_svm)","56c356c8":"cnf_matrix = confusion_matrix(y_valid, yhat, labels = train_new.target_ord.unique().tolist())\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes = train_new.target.unique().tolist())","e215e3cb":"print('Classification Report:')\nprint(classification_report(y_valid, yhat))","bcc803f9":"from xgboost import XGBClassifier","1f99f3f3":"xgb_params = {'n_estimators': 2500,\n             'max_depth': 5,\n             'learning_rate': 0.01,\n             'min_child_weight': 4,\n             'colsample_bytree': 0.4,\n             'subsample': 0.4,\n             'reg_alpha': 0.6,\n             'reg_lambda': 0.6\n             }\nxgb = XGBClassifier(**xgb_params)\nxgb.fit(X_train, y_train, early_stopping_rounds = 5,\n       eval_set = [(X_train, y_train), (X_valid, y_valid)],\n       verbose = False)","2ef2f996":"#To calculate log-loss, we need the probability of each prediction\nyhat_xgbc = xgb.predict_proba(X_valid)\nlogloss_XGBC = log_loss(y_valid, yhat_xgbc)\nprint(\"Log loss using XGB Classifier:\", logloss_XGBC)","a518a9c5":"from xgboost import plot_importance\n# Plot feature importance\nax = plot_importance(xgb, max_num_features=12, show_values=True) \nfig = ax.figure\nfig.set_size_inches(10, 3)\nplt.show()","57ecc343":"results = xgb.evals_result()","d08df194":"# Plot learning curves\nplt.plot(results['validation_0']['mlogloss'], label='train')\nplt.plot(results['validation_1']['mlogloss'], label='test')\nplt.legend()\nplt.show()","c3754f85":"from sklearn.ensemble import AdaBoostClassifier","5d3c6df2":"abc = AdaBoostClassifier(n_estimators = 1000, random_state = 0, learning_rate = 0.12)\nabc.fit(X_train, y_train)\n\nyhat_ABC = abc.predict_proba(X_valid)\nlogloss_ABC = log_loss(y_valid, yhat_ABC)\nprint('Log loss using Ada Boost Classifier:', logloss_ABC)","429042df":"from catboost import CatBoostClassifier","6dbfab0d":"CBC_params = {\n                'iterations': 5000, \n                'od_wait': 250,\n                'use_best_model': True,\n                'loss_function': 'MultiClass',\n                'eval_metric': 'MultiClass',\n                'leaf_estimation_method': 'Newton',\n                'bootstrap_type': 'Bernoulli',\n                'subsample': 0.4,\n                'learning_rate': 0.05,\n                'l2_leaf_reg': 0.5, #L2 Regularization\n                'random_strength': 10, #amount of randomness to use for scoring splits when tree structure is selected\n                'depth': 6, #Tree depth\n                'min_data_in_leaf': 3, #minimum number of training samples in a leaf\n                'leaf_estimation_iterations': 4, #Earlier = 7\n                'task_type': 'GPU',\n                'border_count': 128, #Number of splits for numerical features\n                'grow_policy': 'SymmetricTree'\n            }","1506787a":"cbc = CatBoostClassifier(**CBC_params)\ncbc.fit(X_train, y_train,\n       eval_set = [(X_valid, y_valid)],\n       early_stopping_rounds = 20,\n       verbose = False)","76bfa8ab":"yhat_CBC = cbc.predict_proba(X_valid)\nlogloss_CBC = log_loss(y_valid, yhat_CBC)\nprint('Log loss using CatBoost Classifier:', logloss_CBC)","88642dc4":"In this notebook, we are going to explore various classification techniques using the Otto Group Product Challenge classification dataset.\n\nFrom machinelearningmastery:\n> This dataset describes the 93 obfuscated details of more than 61,000 products grouped into 10 product categories (e.g. fashion, electronics, etc.). Input attributes are counts of different events of some kind. The goal is to make predictions for new products as an array of probabilities for each of the 10 categories and models are evaluated using multiclass logarithmic loss (also called cross entropy).","d37585c9":"From the scikit-learn documentation:\n> Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.","df7549e1":"From tutorialspoint:\n> An SVM model is a representation of different classes in a hyperplane in multidimensional space. The hyperplane is generated in an iterative manner by SVM so that the error can be minimized. The goal of SVM is to divide the datasets into classes to find a `maximum margin(al) hyperplane`.","15236280":"Let us use a custom function to plot our confusion matrix.","e9c4bd1f":"The following 3 code cells are due to [@nagamiso](https:\/\/www.kaggle.com\/nagomiso\/feature-extraction-tfidf).","1528f486":"> A confusion matrix $C$ is such that $C_{i, j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$.","c137a19a":"## Using Logistic Regression","14815052":"### Using the XGBoost Feature Importance Plot","642634cf":"Let us now export the tree in Graphviz format. Thanks to [@dmilla](https:\/\/www.kaggle.com\/dmilla\/introduction-to-decision-trees-titanic-dataset) for the code cell.","5b2d7e0f":"## Using KNN","73183b7c":"From the scikit-learn documentation:\n\n> Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n\n> The $k$-neighbors classification in KNeighborsClassifier is the most commonly used technique. The optimal choice of $k$ is highly data-dependent: in general a larger $k$ suppresses the effects of noise, but makes the classification boundaries less distinct.\n\n> The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the weights keyword. The default value, `weights = uniform`, assigns uniform weights to each neighbor. `weights = distance` assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.","b89fd20e":"## Using DecisionTree","3fa0fcee":"## Using XGBoostClassifier","1047dde9":"The following code cell shows that we have a class imbalance in the target column of the `train` dataset.","8cbefdec":"We are now going to see how each of the features are skewed. This would help us in further analysis.","3fea640d":"## Using Support Vector Machines","ff196d13":"The `classification_report` function builds a text report showing the main classification metrics.\n\n$$\\text{Precision} = \\frac{\\text{# of True positives}}{\\text{# of True positives + # of False positives}}$$\n$$\\text{Recall} = \\frac{\\text{# of True positives}}{\\text{# of True positives + # of False negatives}}$$\n$$\\text{f1-score} = \\text{harmonic average of precision and recall} = \\frac{\\text{2 * Precision * Recall}}{\\text{Precision + Recall}}$$\n***\n$$ \\text{Accuracy} = \\frac{\\text{# of correct predictions}}{\\text{Total number of predictions}}$$\n$$\\text{Macro average - obtained by averaging the unweighted mean per label}$$\n$$\\text{Weighted average - obtained by averaging the support-weighted mean per label}$$\n***","a218f224":"The following parameters have been set by trial and error from [Parameter tuning](https:\/\/catboost.ai\/docs\/concepts\/parameter-tuning.html) and [Speeding up training](https:\/\/catboost.ai\/docs\/concepts\/speed-up-training.html).","21ddabbb":"From Wikipedia:\n> In statistics, the logistic model is used to model the probability of a certain class or event existing such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one.","3662b7f2":"We are now going to remove the features that have a skew value > 3.75 (my arbitrary choice).","9b78a28d":"Let us now check the skew values of the features.","664f3531":"## Using AdaBoostClassifier","6c81e758":"## Using CatBoostClassifier","e70c9d7f":"We are now going to apply the [QuantileTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer) from scikit-learn. I first used `StandardScaler` but found that there was no change in the skew value of the features. \n\nIf anyone knows why I didn't see any change, please drop a comment!\n***","8a42a01a":"From the scikit-learn website:\n> QuantileTransformer applies a non-linear transformation such that the probability density function of each feature will be mapped to a uniform or Gaussian distribution. In this case, all the data, including outliers, will be mapped to a uniform distribution with the range $[0, 1]$, making outliers indistinguishable from inliers.\n\n> RobustScaler and QuantileTransformer are robust to outliers in the sense that adding or removing outliers in the training set will yield approximately the same transformation. But contrary to RobustScaler, QuantileTransformer will also automatically collapse any outlier by setting them to the a priori defined range boundaries (0 and 1). This can result in saturation artifacts for extreme values.\n\n> To map to a Gaussian distribution, set the parameter `output_distribution='normal'`."}}