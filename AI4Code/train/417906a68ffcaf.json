{"cell_type":{"66b06e14":"code","c75cbacc":"code","aa366393":"code","b2e1feec":"code","0d1a4042":"code","d16f807d":"code","f878cad6":"code","607a0f03":"code","b63a6f5b":"code","6b76c333":"code","7a0e562f":"code","43895517":"code","a904d836":"code","9f374a0b":"code","aa51aaf4":"code","89d78392":"code","0442f968":"code","2da02728":"code","ab2c7508":"code","43c850a1":"code","35bbc923":"code","c87fa875":"code","d720215e":"code","8438dc56":"code","954ebc2a":"code","ed6152b7":"code","67f058d6":"code","998939c3":"code","965fa7d8":"code","c2cbf77a":"code","baac54b1":"code","e5205eff":"markdown","f2cff171":"markdown","6638081d":"markdown","fa22b743":"markdown","283f5125":"markdown","717a1638":"markdown","857dd768":"markdown","85df18bd":"markdown","9ff48e57":"markdown","79deb9c7":"markdown","e6a3130a":"markdown","dd55101f":"markdown","fccebe95":"markdown","890d2246":"markdown","49ac3191":"markdown","002d3dc9":"markdown","48e84239":"markdown","6f4d9591":"markdown","bf934540":"markdown","d0d14eb7":"markdown","40ad4d21":"markdown","841206df":"markdown","05cf7abb":"markdown","ac736f0c":"markdown","66d3ca9c":"markdown","517669e4":"markdown","cc4c9ea4":"markdown","6da79812":"markdown","3c0e1402":"markdown","c36dbc98":"markdown","0b879c1c":"markdown","6db7c1d7":"markdown","c22e4a19":"markdown","57c4f77c":"markdown","df41ec2d":"markdown","02c31acb":"markdown","61625230":"markdown","b358c69a":"markdown"},"source":{"66b06e14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c75cbacc":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nfrom sklearn.model_selection import train_test_split\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","aa366393":"# Load train test dataframes\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain_size = int(len(train)*0.8)\n# Sort by time and drop target and id columns\ntrain = train.sort_values(['YrSold', 'MoSold'], axis=0).reset_index(drop=True)\ntrain_X = train.drop(['SalePrice', 'Id'], axis=1).loc[:train_size-1, :]\nvalid_X = train.drop(['SalePrice', 'Id'], axis=1).loc[train_size:, :]\ntrain_y = train['SalePrice'][:train_size]\nvalid_y = train['SalePrice'][train_size:]\ntest_X = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\ntest_X = test_X.sort_values(['YrSold', 'MoSold'], axis=0)","b2e1feec":"train_X.head()","0d1a4042":"train_X.info()","d16f807d":"class CustomImputer:\n    def __init__(self):\n        pass\n    \n    def fit(self, data):\n        cat_cols = data.columns[data.dtypes == 'object']\n        self.impute_cols = cat_cols[data[cat_cols].isna().sum() > 0] \n    \n    def transform(self, data):\n        for column in self.impute_cols:\n            if data[column].isin(['None', 'No', 'Othr']).sum() > 0:\n                replace_value = data.loc[data[column].isin(['None', 'No', 'Othr']), column].unique()[0]\n                data[column] = data[column].fillna(replace_value)\n            else:\n                data[column] = data[column].fillna('None')\n        \n        return data\n                \n    def fit_transform(self, data):\n        self.fit(data)\n        return self.transform(data)\n    \n    def get_params(self):\n        pass","f878cad6":"na_cols = train_X.columns[(train_X.isna().sum() > 0).values]\nprint(f'Columns with NA: {na_cols}')\n# Replace NA values in categorical features with None and similar values        \ntrain_X.loc[:, na_cols] = CustomImputer().fit_transform(train_X.loc[:, na_cols])\ntest_X.loc[:, na_cols] = CustomImputer().fit_transform(test_X.loc[:, na_cols])\nvalid_X.loc[:, na_cols] = CustomImputer().fit_transform(valid_X.loc[:, na_cols])\n# Replace NA values in continuos features with 0 value\ntrain_X.loc[:, na_cols[train_X.loc[:, na_cols].dtypes == 'float64']] = train_X.loc[:, na_cols[train_X.loc[:, na_cols].dtypes == 'float64']].fillna(0)","607a0f03":"train_X.columns[train_X.isna().sum() > 0]","b63a6f5b":"def get_sold_last_mnth(df):\n    \"\"\" Generates sold houses lats month feature \"\"\"\n    timeline = pd.to_datetime(df['YrSold'].astype('str') + '-' + df['MoSold'].astype('str'), format='%Y-%m')\n    tm_ln_indexed = pd.Series(data=timeline.index ,index=timeline.values)\n    tm_ln_sold_last_mnth = tm_ln_indexed.rolling('62d').count() - tm_ln_indexed.rolling('31d').count()\n    sold_lst_mnth = pd.Series(data=tm_ln_sold_last_mnth.values, index=tm_ln_indexed.values).reindex(df.index)\n    return sold_lst_mnth\n\ndef get_fireplaces_per_room(df):\n    \"\"\" Generates number of fireplaces per room feature \"\"\"\n    fp_per_house = df['Fireplaces'] \/ df['TotRmsAbvGrd']\n    return fp_per_house\n\ndef get_quality_per_room(df):\n    \"\"\" Generates quality per room feature \"\"\"\n    qual_per_room = df['OverallQual'] \/ df['TotRmsAbvGrd']\n    return qual_per_room","6b76c333":"# Add sold houses last month feature\ntrain_X = train_X.assign(Sold_Lst_Mnth=lambda df: get_sold_last_mnth(df))\nvalid_X = valid_X.assign(Sold_Lst_Mnth=lambda df: get_sold_last_mnth(df))\ntest_X = test_X.assign(Sold_Lst_Mnth=lambda df: get_sold_last_mnth(df))\n# Add number of fireplaces per room feature\ntrain_X = train_X.assign(FireplacesPerRm=lambda df: get_fireplaces_per_room(df))\nvalid_X = valid_X.assign(FireplacesPerRm=lambda df: get_fireplaces_per_room(df))\ntest_X = test_X.assign(FireplacesPerRm=lambda df: get_fireplaces_per_room(df))\n# Add quality per room feature\ntrain_X = train_X.assign(QualPerRm=lambda df: get_quality_per_room(df))\nvalid_X = valid_X.assign(QualPerRm=lambda df: get_quality_per_room(df))\ntest_X = test_X.assign(QualPerRm=lambda df: get_quality_per_room(df))","7a0e562f":"ta_leakage_cols = ['YrSold', 'MoSold']\ntrain_X = train_X.drop(ta_leakage_cols, axis=1)\nvalid_X = valid_X.drop(ta_leakage_cols, axis=1)\ntest_X =test_X.drop(ta_leakage_cols, axis=1)","43895517":"# Define catgerical and numerical features\ncat_features = train_X.columns[train_X.dtypes == 'object']\nnum_features = train_X.columns[(train_X.dtypes == 'int64') | (train_X.dtypes == 'float64')]","a904d836":"train_X[num_features].hist(bins=15, figsize=(20, 20))","9f374a0b":"fig, ax = plt.subplots(len(cat_features)\/\/5 + 1, 5)\nfig.set_size_inches(20, 30)\nfor idx, feature in enumerate(cat_features):\n    sns.countplot(data=train_X, x=feature, ax=ax[idx\/\/5, idx%5])\n    \nplt.tight_layout()","aa51aaf4":"plt.figure(figsize=(30, 18))\nsns.heatmap(pd.concat([train_X[num_features], train_y], axis=1).corr(), annot=True)","89d78392":"fig, ax = plt.subplots(len(cat_features)\/\/3+1, 3, figsize=(20, len(cat_features)*2))\nfor idx, feature in enumerate(cat_features):\n    sns.violinplot(x=feature, y=train_y, data=train_X, ax=ax[idx\/\/3, idx%3])\n    ax[idx\/\/3, idx%3].set_title(f'Violin plot of {feature} x SalePrice')\n    ax[idx\/\/3, idx%3].xaxis.set_tick_params(rotation=45)\n    \nplt.tight_layout()","0442f968":"plt.style.use('seaborn-darkgrid')\nfig, ax = plt.subplots(len(num_features)\/\/3+1, 3, figsize=(36, len(cat_features)*3))\nfor idx, num_feature in enumerate(num_features):\n    sns.scatterplot(data=train_X, x=num_feature, y=train_y, ax=ax[idx\/\/3, idx%3])\n    ax[idx\/\/3, idx%3].set_title(f'Regplot of Sale Price x {num_feature}', size=24)\n    ax[idx\/\/3, idx%3].set_xlabel(num_feature, size=20)\n    ax[idx\/\/3, idx%3].set_ylabel('Sale Price', size=20)\n    \nplt.tight_layout()","2da02728":"from sklearn import pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom category_encoders import CatBoostEncoder, OrdinalEncoder","ab2c7508":"# Lets divide or categorical features on ordinal and nominal\nordinal_features = [\n    'LandSlope',\n    'Condition1',\n    'Condition2',\n    'ExterQual',\n    'ExterCond',\n    'BsmtQual',\n    'BsmtCond',\n    'BsmtFinType1',\n    'BsmtFinType2',\n    'HeatingQC',\n    'KitchenQual',\n    'FireplaceQu',\n    'GarageQual',\n    'GarageCond',\n    'PoolQC'\n]\nnominal_features = cat_features.drop(ordinal_features)\n# Next define transformer of nominal features\nnominal_transformer = pipeline.Pipeline(steps=[\n    ('cat_boost', CatBoostEncoder()),\n    ('scaler', StandardScaler())\n])\n# Nest feature transformations of numerical ans ordinal cols\nnum_transformer = pipeline.Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=0, missing_values=np.nan)),\n    ('scaler', StandardScaler())\n])\nnum_ord_transformer = ColumnTransformer(transformers=[\n    ('ordinal_transformer', OrdinalEncoder(), ordinal_features),\n    ('num_transformer', num_transformer, num_features)\n])\n# First transform our nominal features\ntrain_X_nominal = nominal_transformer.fit_transform(train_X[nominal_features], train_y)\nvalid_X_nominal = nominal_transformer.transform(valid_X[nominal_features])\ntest_X_nominal = nominal_transformer.transform(test_X[nominal_features])\n# Next transform ordinal and numerical features\ntrain_X_num_ord = num_ord_transformer.fit_transform(train_X)\nvalid_X_num_ord = num_ord_transformer.transform(valid_X)\ntest_X_num_ord = num_ord_transformer.transform(test_X)\n# Finally we concatenate such arrays\ntrain_X_transformed = np.concatenate((train_X_nominal, train_X_num_ord), axis=1)\nvalid_X_transformed = np.concatenate((valid_X_nominal, valid_X_num_ord), axis=1)\ntest_X_transformed = np.concatenate((test_X_nominal, test_X_num_ord), axis=1)","43c850a1":"from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error","35bbc923":"params = {\n    'learning_rate': np.arange(0.01, 0.11, 0.01)\n}\noptimizer = GridSearchCV(XGBRegressor(n_estimators=500, tree_method='gpu_hist', max_depth=4, min_child_weight=3, gamma=0, colsample_bytree=0.4, subsample=0.7, learning_rate=0.05), params, cv=TimeSeriesSplit(n_splits=3), n_jobs=-1)\noptimizer.fit(train_X_transformed, train_y)\nvalid_score = mean_absolute_error(valid_y, optimizer.predict(valid_X_transformed))","c87fa875":"optimizer.best_params_","d720215e":"pd.DataFrame(optimizer.cv_results_)","8438dc56":"print(f'Mean validation score is {valid_score}')","954ebc2a":"xgb_params = {\n    'colsample_bytree': 0.4,\n    'n_estimators': 1000,\n    'min_child_weight': 3,\n    'max_depth': 6,\n    'subsample': 0.4,\n    'learning_rate': 0.01,\n    'gamma': 0,\n    'reg_lambda': 0.02\n}\nmodel = XGBRegressor(tree_method='gpu_hist', **xgb_params).fit(train_X_transformed, train_y)","ed6152b7":"train_score = mean_absolute_error(train_y, model.predict(train_X_transformed))\nprint(f'Mean train score is {train_score}')","67f058d6":"valid_score = mean_absolute_error(valid_y, model.predict(valid_X_transformed))\nprint(f'Mean validation score is {valid_score}')","998939c3":"from sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator, X_train, y_train, cv, train_sizes=np.linspace(0.1, 1, 10)):\n    plt.style.use('seaborn-darkgrid')\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X_train, y_train, cv=cv, n_jobs=-1, train_sizes=train_sizes)\n    train_mean_scores = np.mean(train_scores, axis=1)\n    test_mean_scores = np.mean(test_scores, axis=1)\n    plt.title('Learning curve')\n    plt.plot(train_sizes, train_mean_scores, 'y', label='Train Learning curve')\n    plt.plot(train_sizes, test_mean_scores, 'b', label='Test Learning curve')\n    plt.legend()\n    ","965fa7d8":"plot_learning_curve(model, train_X_transformed, train_y, TimeSeriesSplit(n_splits=3))","c2cbf77a":"features = nominal_features.values.tolist() + ordinal_features + num_features.values.tolist() \nplt.figure(figsize=(25, 20))\nsns.barplot(y=features, x=model.feature_importances_)","baac54b1":"preds_test = model.predict(test_X_transformed)\noutput = pd.DataFrame({'Id': test_X.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","e5205eff":"# House Prices Prediction","f2cff171":"Countplots of categorical features","6638081d":"First we calculate MAE on train and validation datasets, next we'll plot learning curves","fa22b743":"Pour champagne ladies and gentelmans, after long way we can make our predictions and submit our results. Sure It's not finall version, for example numerical feature imputation can be improved(we just replaced all Na values with 0), also we can generate lot more features than we did","283f5125":"Heatmap!!!)","717a1638":"Lets plot learning curve to define whether we have high biase or variance(underfitting and overfitting)","857dd768":"![](https:\/\/paulitaylor.files.wordpress.com\/2016\/10\/lessons-in-rapid-experiments-and-learning-from-failure.png?w=960)","85df18bd":"And numearical columns","9ff48e57":"![](https:\/\/pmp-practitioners.com\/wp-content\/uploads\/2019\/02\/Brainstorming.jpg)","79deb9c7":"# Feature Generation","e6a3130a":"## Na Values\n\n\n![](https:\/\/image.freepik.com\/free-photo\/fill-missing-parts-fragment-white-jigsaw-concept-puzzle-succeed_33807-777.jpg)","dd55101f":"# Target leakage","fccebe95":"Ohh, it's time for tuning... And it's nice, but takes a very long time(and even more if you have no some kind of strategy). I used presented one by Aarshay Jain: https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\nI've already made some hyperparameter tuning for XGB, so I just leave tuning of learning rate","890d2246":"# Model evaluation","49ac3191":"# Data Visualization","002d3dc9":"For ordinal data I'll use OrdinalEncoder(as it much more apropriated for Tree-Based models than OHE) and CatBoosEncoder for nominal, and for numerical Standard Scaler","48e84239":"# Data exploration\nLet's load and exemaine our data","6f4d9591":"It seeems that there is overfitting problem, but due to small dataset underfitting is more frightening than overfitting(I suppose so, if I'm not correct please write it in comment)","bf934540":"Obviously if we had large dataset or more features I would do some feature elimination(RFE for example), but here I'll leave all of them","d0d14eb7":"Lets try make some features. Feature engineering is very important step in improving our model","40ad4d21":"As we see many columns has Na values. My way to deal with that is next: replace all NA in categorical columns with None or similar value and numerical features with 0","841206df":"# Data Transformation","05cf7abb":"# Hyper Parameter Tuning","ac736f0c":"<h3>Hello friend, today I'm going to make my spot on house prices prediction problems, so here we go)<\/h3>","66d3ca9c":"Next lets exemine how our Sale Prices depends on different categorical columns ","517669e4":"![](https:\/\/eige.europa.eu\/sites\/default\/files\/styles\/eige_original_optimised\/public\/images\/evaluation.jpg?itok=DPuDMaP8)","cc4c9ea4":"# Training Model","6da79812":"First lets take a look of how our numerical features distributed","3c0e1402":"![](https:\/\/media.remax-dev.booj.io\/91319a69-7a4b-3a88-83f0-e1a5be6c4d33\/06_MiracleHomes.jpg)","c36dbc98":"![](https:\/\/www.verisk.com\/siteassets\/media\/images\/verisk_commercial_premium_leakage_analysis.jpg)","0b879c1c":"# Making Predictions\n\n\n\n","6db7c1d7":"![](https:\/\/familyproject.sfsu.edu\/sites\/default\/files\/Training%20Image_Medium.jpg)","c22e4a19":"![](https:\/\/images.immediate.co.uk\/production\/volatile\/sites\/7\/2018\/04\/BBC-WH9-final-red-2-3239864.jpg?quality=90&resize=620,413)","57c4f77c":"![](https:\/\/techcrunch.com\/wp-content\/uploads\/2017\/03\/5daa8a29b65f5d8422aaeece44ed0a2d_original.jpg?w=1280&h=2300)","df41ec2d":"![](https:\/\/i.imgflip.com\/zcyxp.jpg)","02c31acb":"#### As we can see the highest influence on Sale Price have Quality features(Overall, Bath, Fireplace, etc), Conditions(Overall, Basement, etc.), Garage Area, 1stFlSF(Square metres on the 1st floor), Roof Materials, Garage Type and Neighbourhoods. \n#### So, if you want a home with the area like Disneyland, a roof that can protect you from the alien laser during the alien invasion, with best conditions and porcelain birds floating in an artificial pond, be next door to Keanu Reeves and have Big High-Quality Garage, where you and Keanu can put your cars and bikes after fascinating travel, you should pay a big amount of money for that( I hope such houses exist) ","61625230":"Lets look at feature importance","b358c69a":"Lets look at the data and check whether is there target leakage. It seems that **YrSold(Year when house is sold)** and **MoSold(Month when house is sold)** will cause target leakage, as we wil not have such values when predicting real world house prices."}}