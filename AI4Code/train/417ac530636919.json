{"cell_type":{"e128f8b2":"code","e8e0d600":"code","15236ce1":"code","80e11688":"code","772aa24a":"code","3823d719":"code","e994d3c4":"code","f096d663":"code","fde0f9f6":"code","cb505b58":"code","f78348c5":"code","eed80a06":"code","42f90ca1":"code","736d18ca":"code","a53309c0":"code","bc3d4634":"code","cbfa3f6d":"code","8958b796":"code","6c3411be":"code","b226e43e":"code","427f985d":"code","f79d8907":"code","c3120616":"code","69291247":"code","868a0efa":"code","ae950b57":"code","c39606f9":"code","4346af80":"code","8805b2ca":"markdown","3baa2ec1":"markdown","b829a08b":"markdown","6c42434f":"markdown"},"source":{"e128f8b2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import make_scorer\nfrom scipy.stats import ks_2samp\nfrom sklearn.feature_selection import chi2\nimport json\nimport matplotlib.image as mpimg\nfrom scipy.stats import linregress\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import f_classif\n\ndef kappa_score(y1, y2, labels=None, sample_weight=None):\n    return cohen_kappa_score(y1, y2, labels=labels, weights=\"quadratic\", sample_weight=sample_weight)\nkappa_scorer = make_scorer(kappa_score)\n\nsns.set(style=\"darkgrid\", context=\"notebook\")\nxsize = 12.0\nysize = 8.0\n\nrand_seed = 24680\nnp.random.seed(rand_seed)\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/train\"))\nprint(os.listdir(\"..\/input\/test\"))","e8e0d600":"train_df = pd.read_csv(\"..\/input\/train\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test\/test.csv\")","15236ce1":"train_df.info()","80e11688":"test_df.info()","772aa24a":"train_df.head()","3823d719":"test_df.head()","e994d3c4":"X_cols = [\"Type\", \"Age\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\", \"MaturitySize\", \n          \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\", \"Quantity\", \"Fee\", \"State\", \n          \"VideoAmt\", \"PhotoAmt\"]","f096d663":"fig, axes = plt.subplots(ncols=2, nrows=len(X_cols))\nfig.set_size_inches(2.0*xsize, len(X_cols)*ysize)\naxes = axes.flatten()\n\nfor i, col in enumerate(X_cols):\n    ix = 2*i\n    ixx = ix + 1\n    sns.countplot(train_df[col], ax=axes[ix])\n    axes[ix].set_title(\"Train \"+col+\" Feature Distribution\")\n    sns.countplot(test_df[col], ax=axes[ixx])\n    axes[ixx].set_title(\"Test \"+col+\" Feature Distribution\")\n\nplt.show()","fde0f9f6":"count_cols = [\"Type\", \"Gender\", \"Color1\", \"Color2\", \"Color3\", \"MaturitySize\", \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\"]","cb505b58":"fig, axes = plt.subplots(nrows=len(count_cols))\nfig.set_size_inches(xsize, len(count_cols)*ysize)\naxes = axes.flatten()\n\nfor i, col in enumerate(count_cols):\n    sns.countplot(x=col, hue=\"AdoptionSpeed\", data=train_df, ax=axes[i])\n    axes[i].set_title(col+\" Feature Count by AdoptionSpeed\")\n\nplt.show()","f78348c5":"reg_cols = [\"Age\", \"Quantity\", \"Fee\", \"VideoAmt\", \"PhotoAmt\"]","eed80a06":"fig, axes = plt.subplots(nrows=len(reg_cols), ncols=2)\nfig.set_size_inches(2.0*xsize, len(reg_cols)*ysize)\naxes = axes.flatten()\n\ny = train_df[\"AdoptionSpeed\"].values\nfor i, col in enumerate(reg_cols):\n    ix = 2*i\n    ixx = ix+1\n    \n    x = train_df[col].values\n    logx = np.log(1.0 + x)\n    \n    m, b, r, pval, stderr = linregress(x=x, y=y)\n    axes[ix].plot(x, y, \"o\")\n    axes[ix].plot(x, m*x + b, \"-\", label=r\"$r^2=%.4f$\"%(r**2.0)+\"\\n\"+r\"$pval=%.4f$\"%pval)\n    axes[ix].legend()\n    axes[ix].set_xlabel(col)\n    axes[ix].set_ylabel(\"AdoptionSpeed\")\n    \n    m, b, r, pval, stderr = linregress(x=logx, y=y)\n    axes[ixx].plot(logx, y, \"o\")\n    axes[ixx].plot(logx, m*logx + b, \"-\", label=r\"$r^2=%.4f$\"%(r**2.0)+\"\\n\"+r\"$pval=%.4f$\"%pval)\n    axes[ixx].legend()\n    axes[ixx].set_xlabel(\"Log \"+col)\n    axes[ixx].set_ylabel(\"AdoptionSpeed\")\n\nplt.show()","42f90ca1":"def highlight_pvals(val):\n    if val < 0.01:\n        return \"background-color: #99ff99\"\n    elif val < 0.05:\n        return \"background-color: #ffff99\"\n    else:\n        return \"background-color: #ff9999\"","736d18ca":"ks_stat = np.zeros(len(X_cols))\nks_pval = np.zeros(len(X_cols))\n\nfor i, col in enumerate(X_cols):\n    ks_stat[i], ks_pval[i] = ks_2samp(train_df[col], test_df[col])\n\nks_test_df = pd.DataFrame(data={\"Feature\":X_cols, \"KS Statistic\":ks_stat, \"P-Value\":ks_pval})\nks_test_df.style.applymap(highlight_pvals, subset=[\"P-Value\"])","a53309c0":"chi_stat = np.zeros(len(X_cols))\nchi_pval = np.zeros(len(X_cols))\n\nfor i, col in enumerate(X_cols):\n    chi_stat[i], chi_pval[i] = chi2(train_df[col].values.reshape(-1, 1), train_df[\"AdoptionSpeed\"].values)\n\nchi_test_df = pd.DataFrame(data={\"Feature\":X_cols, \"Chi^2 Statistic\":chi_stat, \"P-Value\":chi_pval})\nchi_test_df.style.applymap(highlight_pvals, subset=[\"P-Value\"])","bc3d4634":"print(os.listdir(\"..\/input\/train_images\")[:12])","cbfa3f6d":"imgs = []\nfor i, file in enumerate(os.listdir(\"..\/input\/train_images\")[:12]):\n    imgs.append(mpimg.imread(\"..\/input\/train_images\/\"+file))","8958b796":"fig, axes = plt.subplots(ncols=4, nrows=3)\nfig.set_size_inches(24.0, 24.0)\naxes = axes.flatten()\n\nfor i in range(12):\n    axes[i].imshow(imgs[i])\n\nplt.show()","6c3411be":"train_df[\"PetID\"].values[:12]","b226e43e":"with open(\"..\/input\/train_sentiment\/86e1089a3.json\", \"r\") as file:\n    sentiment = json.load(file)\nprint(json.dumps(sentiment, indent=2))","427f985d":"sentence = \"\"\nfor obj in sentiment[\"sentences\"]:\n    sentence += \" \"+obj[\"text\"][\"content\"]\nprint(sentence)","f79d8907":"pet_ids = train_df[\"PetID\"].values\nsentiment_magnitude = np.zeros(len(pet_ids))\nsentiment_score = np.zeros(len(pet_ids))\nmissing_count = 0\n\nfor i, pet_id in enumerate(pet_ids):\n    try:\n        with open(\"..\/input\/train_sentiment\/\"+pet_id+\".json\", \"r\") as file:\n            sentiment = json.load(file)\n        sentiment_magnitude[i] = sentiment[\"documentSentiment\"][\"magnitude\"]\n        sentiment_score[i] = sentiment[\"documentSentiment\"][\"score\"]\n    except FileNotFoundError:\n        missing_count += 1\n        sentiment_magnitude[i] = np.nan\n        sentiment_score[i] = np.nan\n\ntrain_df[\"SentimentMagnitude\"] = sentiment_magnitude\ntrain_df[\"SentimentScore\"] = sentiment_score\ntrain_df[\"LogSentimentMagnitude\"] = np.log(1.0 + sentiment_magnitude)\ntrain_df[\"LogSentimentScore\"] = np.log(1.0 + sentiment_score)","c3120616":"fig, axes = plt.subplots(ncols=2, nrows=2)\nfig.set_size_inches(2.0*xsize, 2.0*ysize)\naxes = axes.flatten()\n\nsns.regplot(x=\"SentimentMagnitude\", y=\"AdoptionSpeed\", data=train_df, ax=axes[0])\n\nsns.regplot(x=\"SentimentScore\", y=\"AdoptionSpeed\", data=train_df, ax=axes[1])\n\nsns.regplot(x=\"LogSentimentMagnitude\", y=\"AdoptionSpeed\", data=train_df, ax=axes[2])\n\nsns.regplot(x=\"LogSentimentScore\", y=\"AdoptionSpeed\", data=train_df, ax=axes[3])\n\nplt.show()","69291247":"pet_ids = train_df[\"PetID\"].values\nsentiment_sentences = []\nmissing_count = 0\n\nfor i, pet_id in enumerate(pet_ids):\n    try:\n        with open(\"..\/input\/train_sentiment\/\"+pet_id+\".json\", \"r\") as file:\n            sentiment = json.load(file)\n        sentence = \"\"\n        for obj in sentiment[\"sentences\"]:\n            sentence += \" \"+obj[\"text\"][\"content\"]\n        sentiment_sentences.append(sentence)\n    except FileNotFoundError:\n        missing_count += 1\n        sentiment_sentences.append(\"\")\n\nprint(missing_count)\nsentiment_sentences[:3]","868a0efa":"tfidf_vectorizer = TfidfVectorizer()\ntfidf_train = tfidf_vectorizer.fit_transform(sentiment_sentences).todense()\ntfidf_train","ae950b57":"F, pvals = f_classif(tfidf_train, train_df[\"AdoptionSpeed\"].values)","c39606f9":"fig, axes = plt.subplots(ncols=2)\nfig.set_size_inches(2.0*xsize, ysize)\n\nsns.distplot(F, ax=axes[0], kde=False)\n\nsns.distplot(pvals, ax=axes[1], kde=False)\naxes[1].axvline(x=0.05, linestyle=\":\", color=\"k\")\naxes[1].axvline(x=0.01, linestyle=\"--\", color=\"k\")\n\nplt.show()","4346af80":"print(\"Statistically Significant: \"+str(\"%.3f\"%(100.0*(len(pvals[pvals<0.01])\/len(pvals))))+\"%\")\nprint(\"Statistically Ambiguous: \"+str(\"%.3f\"%(100.0*(len(pvals[(pvals>0.01)&(pvals<0.05)])\/len(pvals))))+\"%\")\nprint(\"Not Statistically Significant: \"+str(\"%.3f\"%(100.0*(len(pvals[pvals>0.05])\/len(pvals))))+\"%\")","8805b2ca":"The above are the results of the Kolmogorov-Smirnov 2 sample test with a null hypothesis $H_0$: the train and test distribution of each feature are drawn from the same distribution and an alternative hypothesis $H_A$: the train and test distribution of each feature are not drawn from the same distribution. Red indicates that we do not reject the null ($p-value>0.05$), yellow indicates we could reject the null ($0.05 > p-value > 0.01$), and green indicates we do reject the null ($p-value < 0.01$). From the results above we can see that most of the features are not drawn from the same distribution which is not stellar, but hopefully the two distributions are not significantly different enough to affect any models.","3baa2ec1":"So my first thoughts here are \"those dogs and cats look so cute\" and building a NN model of any kind is going to be a pain because the images are of all different sizes taken at different angles and at different distances with different focuses. ","b829a08b":"Interestingly, it looks like the train and test set data may not have the same distribution just by eyeballing it (e.g. Type), but it's best to test this.","6c42434f":"The above are the results of the chi squared test with a null hypothesis $H_0$: the adoption speed is independent of the feature and an alternative hypothesis $H_A$: the adoption speed is dependent on the feature. Red indicates that we do not reject the null ($p-value>0.05$), yellow indicates we could reject the null ($0.05 > p-value > 0.01$), and green indicates we do reject the null ($p-value < 0.01$). From the results above we can see that the adoption speed is dependent on most of the features in the train dataset, except for MaturitySize, Health, and State. This suggests that (1) people don't care about size when adopting pets, in general, (2) people don't mind adopting pets with injuries (yay!), and (3) people all across Malaysia like to adopt pets."}}