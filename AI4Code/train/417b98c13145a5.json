{"cell_type":{"1405635e":"code","10b1444c":"code","3f049ac8":"code","d31cdb22":"code","57a3fcd0":"code","80a7f0d4":"code","fb4d4939":"code","ee8d1369":"code","e415639f":"code","90adf528":"code","a270427e":"markdown","f9bb58a7":"markdown","d00a9965":"markdown","7f6ab096":"markdown","bb7729fa":"markdown","18557837":"markdown","68cfa121":"markdown","a49b6e70":"markdown","396c0f81":"markdown","4d0a6753":"markdown","ab5986ca":"markdown","c4e2850d":"markdown","5a002c1e":"markdown","cde526ed":"markdown","0c70517f":"markdown","21d28530":"markdown","981df902":"markdown","a38d92de":"markdown","ec164d9d":"markdown","e5c6b539":"markdown","4ce5fe4d":"markdown","bb43ae82":"markdown","69991b8f":"markdown","24c859c9":"markdown","b94b14a7":"markdown","56fc0fef":"markdown","84c67e73":"markdown","3d477cbd":"markdown"},"source":{"1405635e":"%%capture\n!pip install trax==1.3.4","10b1444c":"import trax\nfrom trax import layers as tl\nfrom trax.supervised import training\n\n# Trax offers the WideResnet architecture in it's models module\nfrom trax.models.resnet import WideResnet","3f049ac8":"%%capture\ntrain_stream = trax.data.TFDS('cifar10', keys=('image', 'label'), train=True)()\neval_stream = trax.data.TFDS('cifar10', keys=('image', 'label'), train=False)()","d31cdb22":"train_data_pipeline = trax.data.Serial(\n    trax.data.Shuffle(),\n    trax.data.Batch(64),\n    trax.data.AddLossWeights(),\n)\n\ntrain_batches_stream = train_data_pipeline(train_stream)\n\neval_data_pipeline = trax.data.Serial(\n    trax.data.Batch(64),\n    trax.data.AddLossWeights(),\n)\n\neval_batches_stream = eval_data_pipeline(eval_stream)","57a3fcd0":"thin_model = tl.Serial(\n    WideResnet(widen_factor = 1),\n    tl.LogSoftmax()\n)\n\nwide_model = tl.Serial(\n    WideResnet(widen_factor = 2),\n    tl.LogSoftmax()\n)\n\nwider_model = tl.Serial(\n    WideResnet(widen_factor = 3),\n    tl.LogSoftmax()\n)\n\nwidest_model = tl.Serial(\n    WideResnet(widen_factor = 4),\n    tl.LogSoftmax()\n)","80a7f0d4":"train_task = training.TrainTask(\n    labeled_data=train_batches_stream,\n    loss_layer=tl.CrossEntropyLoss(),\n    optimizer=trax.optimizers.Adam(0.01),\n    n_steps_per_checkpoint=1000,\n)\n\neval_task = training.EvalTask(\n    labeled_data=eval_batches_stream,\n    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n    n_eval_batches=20,\n)","fb4d4939":"training_loop = training.Loop(thin_model, \n                              train_task, \n                              eval_tasks=[eval_task], \n                              output_dir='.\/thin_model')\n\ntraining_loop.run(5000)","ee8d1369":"training_loop = training.Loop(wide_model, \n                              train_task, \n                              eval_tasks=[eval_task], \n                              output_dir='.\/wide_model')\n\ntraining_loop.run(5000)","e415639f":"training_loop = training.Loop(wider_model, \n                              train_task, \n                              eval_tasks=[eval_task], \n                              output_dir='.\/wider_model')\n\ntraining_loop.run(5000)","90adf528":"training_loop = training.Loop(widest_model, \n                              train_task, \n                              eval_tasks=[eval_task], \n                              output_dir='.\/widest_model')\n\ntraining_loop.run(5000)","a270427e":"# Residual Networks","f9bb58a7":"# Issues with Traditional Residual Networks","d00a9965":"# Model Architecture","7f6ab096":"![WRN-1.png](attachment:WRN-1.png)","bb7729fa":"Figure 1(a) and 1(c) represent the fundamental difference between the *basic* and the *basic-wide* blocks used.","18557837":"Trax offers a rich collection of [.data](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.data.html) API's to create input pipelines. One of which is the [`trax.data.TFDS()`](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.data.html#trax.data.tf_inputs.TFDS) which returns an iterator of numpy arrays representing the dataset. \n\nIf you'd like to learn more about the trax.data API's please checkout the notebook [here](https:\/\/www.kaggle.com\/sauravmaheshkar\/trax-data-explained) where I explain the most common API's in a in-depth manner","68cfa121":"Figure 1: *Various ResNet Blocks*","a49b6e70":"# Batch Generator","396c0f81":"A **Residual block with a identity mapping**, which allows us to train very deep networks is a **weakness**. As the gradient flows through the network there is nothing to force it to go through the residual block weights and thus it can avoid learning during training. This only a few blocks can run valuable representations or many blocks could share very little information with small contributions to the final goal. This problem was tried to be addressed using a special case of dropout applied to residual blocks in which an identity scalar weight is added to each residual block on which dropout is applied.\n\nAs we are widening our residual blocks, this results in an increase in the number of parameters, and the authors decided to study the effects of dropout to regularize training and prevent overfitting. They argued that the dropout should be inserted between convolutional layers instead of being inserted in the identity part of the block and showed that this results in consistent gains, yielding new SOTA results.","4d0a6753":"$\\large \nx_{l+1} = x_l + \\mathbb{F}(x_l, W_l)\n$\n\n\nThis is the representation of a Residual block with an identity mapping. \n\n* $x_{l+1}$ and $x_l$ represent the input and output of the $l$-th unit in the network\n\n* $\\mathbb{F}$ is a residual function\n\n* $W_l$ are the parameters","ab5986ca":"The paper highlights a method, giving a total improvement of 4.4% over ResNet-1001 and showing that:-\n\n* widening consistently improves performance across residual networks of different depth\n\n* incresing both depth and width helps until the number of parameters becomes too high and stronger regularization is required\n\n* there doesn't seem to be a regularization effect from very high depth in residual networks as wide networks with the same number of parameters as thin ones can learn same or better representations. Furthermore, wide networks can successfully learn with a 2 or more times larger number of parameters than thin ones, which would require doubling the depth of thin networks, making them infeasibly expensive to train.","c4e2850d":"Prior to the introduction of [Wide Residual Networks](https:\/\/arxiv.org\/pdf\/1605.07146.pdf) (WRNs) by Sergey Zagoruyko and Nikos Komodakis, deep residual networks were shown to have a fractional increase in performance but at the cost of **doubling** the number of layers. This led to the problem of diminishing feature reuse and overall made the models slow to train. WRNs showed that having a wider residual network leads to better performance and increased the then SOTA results on CIFAR, SVHN and COCO. \n\nIn this notebook we run through a simple demonstration of training a WideResnet on the `cifar10` dataset using the [Trax](https:\/\/github.com\/google\/trax) framework. Trax is an end-to-end library for deep learning that focuses on **clear code and speed**. It is actively used and maintained in the *Google Brain team*.","5a002c1e":"The paper [Wide Residual Networks](https:\/\/arxiv.org\/pdf\/1605.07146.pdf) attemptsto answer the question of how wide deep residual networks should be and address the problem of training.","cde526ed":"## Diminishing Feature Reuse","0c70517f":"# Architecture","21d28530":"![WRN-3.png](attachment:WRN-3.png)","981df902":"# Key Takeaways","a38d92de":"Here, we create pre-processing pipelines, by using the [`Shuffle()`](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.data.html#trax.data.inputs.Shuffle), [`Batch()`](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.data.html#trax.data.inputs.Batch) and [`AddLossWeights()`](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.data.html#trax.data.inputs.AddLossWeights) functions from the trax.data API","ec164d9d":"*Test error (%, median over 5 runs) on CIFAR-10 of residual networks with k = 1 and different block types. Time represents one training epoch*","e5c6b539":"# Experimental Results","4ce5fe4d":"# Downloading Dataset","bb43ae82":"![WRN-2.png](attachment:WRN-2.png)","69991b8f":"When we have our model and the data, we use [`trax.supervised.training`](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.supervised.html#module-trax.supervised.training) to define training and eval tasks and create a training loop. The Trax training loop optimizes training and will create TensorBoard logs and model checkpoints for you.","24c859c9":"# Importing Libraries","b94b14a7":"This is the basic structure of Wide Residual Networks. In the papers the size of `conv1` was fixed in all the experiments, while the \"widening\" factor `k` was experimented with in the next three groups. Here `k` is the. widening factor which multiplies the number of features in convolutional layers\n\nLet B(M) denote various residual block structures, where M is a list with the kernel sizes of the convoutional layers in a block.\nThe following architectures were used in experimentation:-\n\n* B(3,3) - The Original \"basic\" block. (Figure 1(a))\n* B(3,1,3) - Same as basic but with a extra 1x1 layer in between\n* B(1,3,1) - For Bottleneck (Figure 1(b))\n* B(1,3) - Having Alternative 1x1-3x3 convolutions\n* B(3,1) - Having Alternative 3x3-1x1 convolutions\n* B(3,1,1) - A Network-in-Network style block","56fc0fef":"# Introduction","84c67e73":"The paper highlights that the block structure B(3,3) beats B(3,1) and B(3,1,3) by a little margin. ","3d477cbd":"We use the `WideResnet` architecture defined in `trax.models.resnet` module. By Default the \"widening factor\" is set to 1, thus we experiment with four values of the `widen_factor` 1,2, 3 and 4. The Architecture doesn't contain a [`tl.LogSoftmax()`](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.layers.html#trax.layers.core.LogSoftmax) function so we add it to our model using the [`tl.Serial()`](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.layers.html#trax.layers.combinators.Serial) combinator"}}