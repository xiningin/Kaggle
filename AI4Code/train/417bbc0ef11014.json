{"cell_type":{"8f06c789":"code","1bc5e1b9":"code","53ca390e":"code","fd2aa093":"code","25475f9c":"code","7b3fbb0b":"code","f12fa13c":"code","c6a5296f":"code","1f650169":"code","ef77e3d7":"code","3446c97f":"code","b2bcd9b9":"code","fc84b462":"code","17df9e1e":"code","19daeb37":"code","56111917":"code","8f948b60":"code","6f9ad013":"code","3fa7c45f":"code","fbb3111a":"code","542e3acb":"code","57c1c65d":"code","8488e5e3":"code","87636e48":"code","8a1d28cc":"code","9c246e67":"code","5f00380e":"code","5d02ec4d":"code","b45218fd":"code","72727c56":"code","444307c5":"code","acc4f714":"code","5c99c9ae":"code","ce8b2c0c":"code","79a1d8dc":"code","d3a1f441":"code","d8738489":"code","805940e0":"code","3885cf97":"code","0bbd7101":"code","6c74a5dc":"code","248bd2b1":"markdown","b517633f":"markdown","7110d269":"markdown","9c8027a3":"markdown","18ef3db0":"markdown","4988aa95":"markdown","568873cf":"markdown","bdc6c76d":"markdown","edccd1dd":"markdown","b3e36034":"markdown","42dd3930":"markdown","0598d45e":"markdown","4345f9cf":"markdown","8b13d482":"markdown","e95a7868":"markdown","3cded98c":"markdown","fa5ebb65":"markdown","f309e681":"markdown","f7c8b9dd":"markdown","f8a59cb1":"markdown","1f582dc7":"markdown","d65b85e4":"markdown","fa693cd6":"markdown"},"source":{"8f06c789":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm \nfrom nltk.corpus import stopwords","1bc5e1b9":"# Input data files are available in the \"..\/input\/\" directory.\ntrain = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\ntest = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")","53ca390e":"# Converting data in Data framres \ntrain=pd.DataFrame(train)\ntest=pd.DataFrame(test)","fd2aa093":"print(\"Train NULL \",train.isnull().sum())\nprint(\"Test NULL\",test.isnull().sum())","25475f9c":"miss_val_train_df = train.isnull().sum(axis=0) \/ len(train)\nmiss_val_train_df = miss_val_train_df[miss_val_train_df > 0] * 100\nmiss_val_train_df","7b3fbb0b":"#Source:simple-eda-text-preprocessing-jigsaw\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndemographics = train.loc[:, ['target']+list(train)[slice(8,32)]].dropna()\nweighted_toxic = demographics.iloc[:, 1:].multiply(demographics.iloc[:, 0], axis=\"index\").sum()\/demographics.iloc[:, 1:][demographics.iloc[:, 1:]>0].count()\nweighted_toxic = weighted_toxic.sort_values(ascending=False)\nplt.figure(figsize=(30,20))\nsns.set(font_scale=3)\nax = sns.barplot(x = weighted_toxic.values, y = weighted_toxic.index, alpha=0.8)\nplt.ylabel('Demographics')\nplt.xlabel('Weighted Toxic')\nplt.show()\ndel sns,weighted_toxic,demographics,ax","f12fa13c":"# lets create a list of all the identities tagged in this dataset. This list given in the data section of this competition. \nidentities = ['male','female','transgender','other_gender','heterosexual','homosexual_gay_or_lesbian',\n              'bisexual','other_sexual_orientation','christian','jewish','muslim','hindu','buddhist',\n              'atheist','other_religion','black','white','asian','latino','other_race_or_ethnicity',\n              'physical_disability','intellectual_or_learning_disability','psychiatric_or_mental_illness',\n              'other_disability']","c6a5296f":"# getting the dataframe with identities tagged\ntrain_labeled_df = train.loc[:, ['target'] + identities ].dropna()\n# lets define toxicity as a comment with a score being equal or .5\n# in that case we divide it into two dataframe so we can count toxic vs non toxic comment per identity\ntoxic_df = train_labeled_df[train_labeled_df['target'] >= .5][identities]\nnon_toxic_df = train_labeled_df[train_labeled_df['target'] < .5][identities]","1f650169":"# at first, we just want to consider the identity tags in binary format. So if the tag is any value other than 0 we consider it as 1.\ntoxic_count = toxic_df.where(train_labeled_df == 0, other = 1).sum()\nnon_toxic_count = non_toxic_df.where(train_labeled_df == 0, other = 1).sum()","ef77e3d7":"# now we can concat the two series together to get a toxic count vs non toxic count for each identity\ntoxic_vs_non_toxic = pd.concat([toxic_count, non_toxic_count], axis=1)\ntoxic_vs_non_toxic = toxic_vs_non_toxic.rename(index=str, columns={1: \"non-toxic\", 0: \"toxic\"})\n# here we plot the stacked graph but we sort it by toxic comments to (perhaps) see something interesting\ntoxic_vs_non_toxic.sort_values(by='toxic').plot(kind='bar', stacked=True, figsize=(30,10), fontsize=20).legend(prop={'size': 20})","3446c97f":"with_date_df = train.loc[:, ['created_date', 'target'] + identities].dropna()\ncomments_with_date_df = train.loc[:, ['created_date', 'target','comment_text'] + identities].dropna()\ncomments_with_date_df['created_date'] = pd.to_datetime(with_date_df['created_date'].apply(lambda dt: dt[:10]))\ncomments_with_date_df['comment_count'] = 1","b2bcd9b9":"# Lets import seaborn\nimport seaborn as sns\n# Compute the correlation matrix\ncorr = comments_with_date_df[identities].corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nsns.set(font_scale = 1)\nf, ax = plt.subplots(figsize=(11, 9))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","fc84b462":"contract={'albertans':'inhabitant of the Canadian province of Alberta','SB91': 'establishes a pretrial services program at the department of corrections to conduct pretrial risk assessments using an objective data based validated pretrial risk assessment tool', 'tRump': 'trump', 'utmterm': 'utm term', 'FakeNews': 'fake news', 'G\u0280\u1d07at': 'great', '\u0299\u1d0f\u1d1bto\u1d0d': 'bottom', 'washingtontimes': 'washington times', 'garycrum': 'gary crum', 'htmlutmterm': 'html utm term', 'RangerMC': 'car', 'TFWs': 'tuition fee waiver scheme', 'SJWs': 'sjw', 'Koncerned': 'concerned', 'Vinis': 'vinys', 'Y\u1d0f\u1d1c': 'you', 'Trumpsters': 'avid supporter and follower of donald trump', 'Trumpian': 'resembling or pertaining to the philosophy rhetoric and style of donald trump', 'bigly': 'big league', 'Trumpism': 'the political positions of usa president donald trump', 'Yoyou': 'you', 'Auwe': 'exclamation of wonder', 'Drumpf': 'trump', 'Brexit': 'british exit', 'utilitas': 'academic journal covering political philosophy and jurisprudence', '\u1d00': 'a', '\ud83d\ude09': 'wink', '\ud83d\ude02': 'joy', '\ud83d\ude00': 'context of laughing, rejoicing, and being excited at something like the upcoming holiday', 'theguardian': 'the guardian', 'deplorables': 'half of the supporters of trump', 'theglobeandmail': 'the global and mail', 'justiciaries': 'justiciary', 'creditdation': 'Accreditation', 'doctrne': 'doctrine', 'fentayal': 'fentanyl', 'designation-': 'designation', 'CONartist': 'con-artist', 'Mutilitated': 'Mutilated', 'Obumblers': 'bumblers', 'negotiatiations': 'negotiations', 'dood-': 'dood', 'irakis': 'iraki', 'cooerate': 'cooperate', 'COx': 'cox', 'racistcomments': 'racist comments', 'envirnmetalists': 'environmentalists', \"Trump's\": 'trump is', \"'cause\": 'because', ',cause': 'because', ';cause': 'because', 'ain\"t': 'am not', 'ain,t': 'am not', 'ain;t': 'am not', 'ain\u00b4t': 'am not', 'ain\u2019t': 'am not', \"aren't\": 'are not', 'aren,t': 'are not', 'aren;t': 'are not', 'aren\u00b4t': 'are not', 'aren\u2019t': 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', 'can,t': 'cannot', 'can,t,ve': 'cannot have', 'can;t': 'cannot', 'can;t;ve': 'cannot have', 'can\u00b4t': 'cannot', 'can\u00b4t\u00b4ve': 'cannot have', 'can\u2019t': 'cannot', 'can\u2019t\u2019ve': 'cannot have', \"could've\": 'could have', 'could,ve': 'could have', 'could;ve': 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', 'couldn,t': 'could not', 'couldn,t,ve': 'could not have', 'couldn;t': 'could not', 'couldn;t;ve': 'could not have', 'couldn\u00b4t': 'could not', 'couldn\u00b4t\u00b4ve': 'could not have', 'couldn\u2019t': 'could not', 'couldn\u2019t\u2019ve': 'could not have', 'could\u00b4ve': 'could have', 'could\u2019ve': 'could have', \"didn't\": 'did not', 'didn,t': 'did not', 'didn;t': 'did not', 'didn\u00b4t': 'did not', 'didn\u2019t': 'did not', \"doesn't\": 'does not', 'doesn,t': 'does not', 'doesn;t': 'does not', 'doesn\u00b4t': 'does not', 'doesn\u2019t': 'does not', \"don't\": 'do not', 'don,t': 'do not', 'don;t': 'do not', 'don\u00b4t': 'do not', 'don\u2019t': 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', 'hadn,t': 'had not', 'hadn,t,ve': 'had not have', 'hadn;t': 'had not', 'hadn;t;ve': 'had not have', 'hadn\u00b4t': 'had not', 'hadn\u00b4t\u00b4ve': 'had not have', 'hadn\u2019t': 'had not', 'hadn\u2019t\u2019ve': 'had not have', \"hasn't\": 'has not', 'hasn,t': 'has not', 'hasn;t': 'has not', 'hasn\u00b4t': 'has not', 'hasn\u2019t': 'has not', \"haven't\": 'have not', 'haven,t': 'have not', 'haven;t': 'have not', 'haven\u00b4t': 'have not', 'haven\u2019t': 'have not', \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he's\": 'he is', 'he,d': 'he would', 'he,d,ve': 'he would have', 'he,ll': 'he will', 'he,s': 'he is', 'he;d': 'he would', 'he;d;ve': 'he would have', 'he;ll': 'he will', 'he;s': 'he is', 'he\u00b4d': 'he would', 'he\u00b4d\u00b4ve': 'he would have', 'he\u00b4ll': 'he will', 'he\u00b4s': 'he is', 'he\u2019d': 'he would', 'he\u2019d\u2019ve': 'he would have', 'he\u2019ll': 'he will', 'he\u2019s': 'he is', \"how'd\": 'how did', \"how'll\": 'how will', \"how's\": 'how is', 'how,d': 'how did', 'how,ll': 'how will', 'how,s': 'how is', 'how;d': 'how did', 'how;ll': 'how will', 'how;s': 'how is', 'how\u00b4d': 'how did', 'how\u00b4ll': 'how will', 'how\u00b4s': 'how is', 'how\u2019d': 'how did', 'how\u2019ll': 'how will', 'how\u2019s': 'how is', \"i'd\": 'i would', \"i'll\": 'i will', \"i'm\": 'i am', \"i've\": 'i have', 'i,d': 'i would', 'i,ll': 'i will', 'i,m': 'i am', 'i,ve': 'i have', 'i;d': 'i would', 'i;ll': 'i will', 'i;m': 'i am', 'i;ve': 'i have', \"isn't\": 'is not', 'isn,t': 'is not', 'isn;t': 'is not', 'isn\u00b4t': 'is not', 'isn\u2019t': 'is not', \"it'd\": 'it would', \"it'll\": 'it will', \"It's\": 'it is', \"it's\": 'it is', 'it,d': 'it would', 'it,ll': 'it will', 'it,s': 'it is', 'it;d': 'it would', 'it;ll': 'it will', 'it;s': 'it is', 'it\u00b4d': 'it would', 'it\u00b4ll': 'it will', 'it\u00b4s': 'it is', 'it\u2019d': 'it would', 'it\u2019ll': 'it will', 'it\u2019s': 'it is', 'tRump:trumpi\u00b4d': 'i would', 'i\u00b4ll': 'i will', 'i\u00b4m': 'i am', 'i\u00b4ve': 'i have', 'i\u2019d': 'i would', 'i\u2019ll': 'i will', 'i\u2019m': 'i am', 'i\u2019ve': 'i have', \"let's\": 'let us', 'let,s': 'let us', 'let;s': 'let us', 'let\u00b4s': 'let us', 'let\u2019s': 'let us', \"ma'am\": 'madam', 'ma,am': 'madam', 'ma;am': 'madam', \"mayn't\": 'may not', 'mayn,t': 'may not', 'mayn;t': 'may not', 'mayn\u00b4t': 'may not', 'mayn\u2019t': 'may not', 'ma\u00b4am': 'madam', 'ma\u2019am': 'madam', \"might've\": 'might have', 'might,ve': 'might have', 'might;ve': 'might have', \"mightn't\": 'might not', 'mightn,t': 'might not', 'mightn;t': 'might not', 'mightn\u00b4t': 'might not', 'mightn\u2019t': 'might not', 'might\u00b4ve': 'might have', 'might\u2019ve': 'might have', \"must've\": 'must have', 'must,ve': 'must have', 'must;ve': 'must have', \"mustn't\": 'must not', 'mustn,t': 'must not', 'mustn;t': 'must not', 'mustn\u00b4t': 'must not', 'mustn\u2019t': 'must not', 'must\u00b4ve': 'must have', 'must\u2019ve': 'must have', \"needn't\": 'need not', 'needn,t': 'need not', 'needn;t': 'need not', 'needn\u00b4t': 'need not', 'needn\u2019t': 'need not', \"oughtn't\": 'ought not', 'oughtn,t': 'ought not', 'oughtn;t': 'ought not', 'oughtn\u00b4t': 'ought not', 'oughtn\u2019t': 'ought not', \"sha'n't\": 'shall not', 'sha,n,t': 'shall not', 'sha;n;t': 'shall not', \"shan't\": 'shall not', 'shan,t': 'shall not', 'shan;t': 'shall not','theglobeandmail':'newspaper','chaput':'american prelate','manafort':'American lobbyist, political consultant, former lawyer, and convicted felon', 'shan\u2019t': 'shall not', 'sha\u00b4n\u00b4t': 'shall not', 'sha\u2019n\u2019t': 'shall not', \"she'd\": 'she would', \"she'll\": 'she will', \"she's\": 'she is', 'she,d': 'she would', 'she,ll': 'she will', 'she,s': 'she is', 'she;d': 'she would', 'she;ll': 'she will', 'she;s': 'she is', 'she\u00b4d': 'she would', 'she\u00b4ll': 'she will', 'she\u00b4s': 'she is', 'she\u2019d': 'she would', 'she\u2019ll': 'she will', 'she\u2019s': 'she is', \"should've\": 'should have', 'should,ve': 'should have', 'should;ve': 'should have', \"shouldn't\": 'should not', 'shouldn,t': 'should not', 'shouldn;t': 'should not', 'shouldn\u00b4t': 'should not', 'shouldn\u2019t': 'should not', 'should\u00b4ve': 'should have', 'should\u2019ve': 'should have', \"that'd\": 'that would', \"that's\": 'that is', 'that,d': 'that would', 'that,s': 'that is', 'that;d': 'that would', 'that;s': 'that is', 'that\u00b4d': 'that would', 'that\u00b4s': 'that is', 'that\u2019d': 'that would', 'that\u2019s': 'that is', \"there'd\": 'there had', \"there's\": 'there is', 'there,d': 'there had', 'there,s': 'there is', 'there;d': 'there had', 'there;s': 'there is', 'there\u00b4d': 'there had', 'there\u00b4s': 'there is', 'there\u2019d': 'there had', 'there\u2019s': 'there is', \"they'd\": 'they would', \"they'll\": 'they will', \"they're\": 'they are', \"they've\": 'they have', 'they,d': 'they would', 'they,ll': 'they will', 'they,re': 'they are', 'they,ve': 'they have', 'they;d': 'they would', 'they;ll': 'they will', 'they;re': 'they are', 'they;ve': 'they have', 'they\u00b4d': 'they would', 'they\u00b4ll': 'they will', 'they\u00b4re': 'they are', 'they\u00b4ve': 'they have', 'they\u2019d': 'they would', 'they\u2019ll': 'they will', 'they\u2019re': 'they are', 'they\u2019ve': 'they have', \"wasn't\": 'was not', 'wasn,t': 'was not', 'wasn;t': 'was not', 'wasn\u00b4t': 'was not', 'wasn\u2019t': 'was not', \"we'd\": 'we would', \"we'll\": 'we will', \"we're\": 'we are', \"we've\": 'we have', 'we,d': 'we would', 'we,ll': 'we will', 'we,re': 'we are', 'we,ve': 'we have', 'we;d': 'we would', 'we;ll': 'we will', 'we;re': 'we are', 'we;ve': 'we have', \"weren't\": 'were not', 'weren,t': 'were not', 'weren;t': 'were not', 'weren\u00b4t': 'were not', 'weren\u2019t': 'were not', 'we\u00b4d': 'we would', 'we\u00b4ll': 'we will', 'we\u00b4re': 'we are', 'we\u00b4ve': 'we have', 'we\u2019d': 'we would', 'we\u2019ll': 'we will', 'we\u2019re': 'we are', 'we\u2019ve': 'we have', \"what'll\": 'what will', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', 'what,ll': 'what will', 'what,re': 'what are', 'what,s': 'what is', 'what,ve': 'what have', 'what;ll': 'what will', 'what;re': 'what are', 'what;s': 'what is', 'what;ve': 'what have', 'what\u00b4ll': 'what will', 'what\u00b4re': 'what are', 'what\u00b4s': 'what is', 'what\u00b4ve': 'what have', 'what\u2019ll': 'what will', 'what\u2019re': 'what are', 'what\u2019s': 'what is', 'what\u2019ve': 'what have', \"where'd\": 'where did', \"where's\": 'where is', 'where,d': 'where did', 'where,s': 'where is', 'where;d': 'where did', 'where;s': 'where is', 'where\u00b4d': 'where did', 'where\u00b4s': 'where is', 'where\u2019d': 'where did', 'where\u2019s': 'where is', \"who'll\": 'who will', \"who's\": 'who is', 'who,ll': 'who will', 'who,s': 'who is', 'who;ll': 'who will', 'who;s': 'who is', 'who\u00b4ll': 'who will', 'who\u00b4s': 'who is', 'who\u2019ll': 'who will', 'who\u2019s': 'who is', \"won't\": 'will not', 'won,t': 'will not', 'won;t': 'will not', 'won\u00b4t': 'will not', 'won\u2019t': 'will not', \"wouldn't\": 'would not', 'wouldn,t': 'would not', 'wouldn;t': 'would not', 'wouldn\u00b4t': 'would not', 'wouldn\u2019t': 'would not', \"you'd\": 'you would', \"you'll\": 'you will', \"you're\": 'you are', 'you,d': 'you would', 'you,ll': 'you will', 'you,re': 'you are', 'you;d': 'you would', 'you;ll': 'you will', 'you;re': 'you are', 'you\u00b4d': 'you would', 'you\u00b4ll': 'you will', 'you\u00b4re': 'you are', 'you\u2019d': 'you would', 'you\u2019ll': 'you will', 'you\u2019re': 'you are', '\u00b4cause': 'because', '\u2019cause': 'because', \"you've\": 'you have', \"could'nt\": 'could not', \"havn't\": 'have not', 'here\u2019s': 'here is', 'i\"\"m': 'i am', \"i'am\": 'i am', \"i'l\": 'i will', \"i'v\": 'i have',\n         }\n","17df9e1e":"contract1={\"wan't\": 'want', \"was'nt\": 'was not', \"who'd\": 'who would', \"who're\": 'who are', \"who've\": 'who have', \"why'd\": 'why would','khadr':'Omar khadr pleaded guilty to the murder of U.S. Army Sergeant 1st Class Christopher Speer and other charges',\n\"would've\": 'would have', \"y'all\": 'you all', \"y'know\": 'you know', 'you.i': 'you i', \"your'e\": 'you are', \"arn't\": 'are not', \"agains't\": 'against', \"c'mon\": 'common', \"doens't\": 'does not', 'don\"\"t': 'do not', \"dosen't\": 'does not', \"dosn't\": 'does not', \"shoudn't\": 'should not', \"that'll\": 'that will', \"there'll\": 'there will', \"there're\": 'there are', \"this'll\": 'this all', \"u're\": 'you are', \"ya'll\": 'you all', \"you'r\": 'you are', 'you\u2019ve': 'you have', \"d'int\": 'did not', \"did'nt\": 'did not', \"din't\": 'did not', \"dont't\": 'do not', \"gov't\": 'government', \"i'ma\": 'i am', \"is'nt\": 'is not', '\u2018I': 'I', '\u1d00\u0274\u1d05': 'and', '\u1d1b\u029c\u1d07': 'the', '\u029c\u1d0f\u1d0d\u1d07': 'home', '\u1d1c\u1d18': 'up', '\u0299\u028f': 'by', '\u1d00\u1d1b': 'at', '\u2026and': 'and', 'civilbeat': 'practices watchdog journalism', 'TrumpCare': 'American Health Care Act of 2017', 'Trumpcare': 'trump care', 'OBAMAcare': 'patient protection and affordable care act ', '\u1d04\u029c\u1d07\u1d04\u1d0b': 'check', '\u0493\u1d0f\u0280': 'for', '\u1d1b\u029c\u026as': 'this', '\u1d04\u1d0f\u1d0d\u1d18\u1d1c\u1d1b\u1d07\u0280': 'computer', '\u1d0d\u1d0f\u0274\u1d1b\u029c': 'month', '\u1d21\u1d0f\u0280\u1d0b\u026a\u0274\u0262': 'working', '\u1d0a\u1d0f\u0299': 'job', '\u0493\u0280\u1d0f\u1d0d': 'from', 'S\u1d1b\u1d00\u0280\u1d1b': 'start', 'gubmit': 'submit', 'CO\u2082': 'carbondioxide', '\u0493\u026a\u0280s\u1d1b': 'first', '\u1d07\u0274\u1d05': 'end', '\u1d04\u1d00\u0274': 'can', '\u029c\u1d00\u1d20\u1d07': 'have', '\u1d1b\u1d0f': 'to', '\u029f\u026a\u0274\u1d0b': 'link', '\u1d0f\u0493': 'of', '\u029c\u1d0f\u1d1c\u0280\u029f\u028f': 'hourly', '\u1d21\u1d07\u1d07\u1d0b': 'week', '\u1d07x\u1d1b\u0280\u1d00': 'extra', 'G\u0280\u1d07\u1d00\u1d1b': 'great', 's\u1d1b\u1d1c\u1d05\u1d07\u0274\u1d1bs': 'student', 'th\u00c8': 'the', 'thi\u015a': 'this', 'needtoimpeach': 'house of representatives, democrats can initiate impeachment proceedings against Donald Trump', 'whitefragilitycankissmyass': 'white fragility can kiss my ass', 'altrightpubs': 'loosely connected far right white nationalist movement', 's\u1d1b\u1d00\u028f': 'stay', '\u1d0d\u1d0f\u1d0ds': 'mother', '\u1d0f\u0280': 'or', '\u1d00\u0274\u028f\u1d0f\u0274\u1d07': 'anyone', '\u0274\u1d07\u1d07\u1d05\u026a\u0274\u0262': 'needing', '\u1d00\u0274': 'an', '\u026a\u0274\u1d04\u1d0f\u1d0d\u1d07': 'income', '\u0280\u1d07\u029f\u026a\u1d00\u0299\u029f\u1d07': 'reliable', '\u1d04an': 'can', 'Zuptas': 'american gupta family', '\u028f\u1d0f\u1d1c\u0280': 'your', 's\u026a\u0262\u0274\u026a\u0274\u0262': 'signing', '\u0299\u1d0f\u1d1b\u1d1b\u1d0f\u1d0d': 'bottom', '\u0493\u1d0f\u029f\u029f\u1d0f\u1d21\u026a\u0274\u0262': 'following', 'M\u1d00\u1d0b\u1d07': 'make', '\u1d04\u1d0f\u0274\u0274\u1d07\u1d04\u1d1b\u026a\u1d0f\u0274': 'connection', 'broadwayinsurance': 'insurance company', 'Papadopoulus': 'former member of the foreign policy advisory panel to Donald Trump', 'AgriSA': 'agriculture company', 'Nevillea': 'group of plants ', 'DTJR': 'donald trump', 'SSRAA': 'Southern Southeast Regional Aquaculture Association', 'Osweiller': 'american football quarterback who is currently a free agent', 'cruxnow': 'online newspaper that focuses on news related to the Catholic Church', 'LotL': 'magazine published in australia', 'Turdeau': 'shit poop excrement', 'Victimitis': 'silent killer moving so stealthily that sufferers do not recognize its symptoms', '\ud83d\ude01': 'beaming face with smiling eyes', '\u026a\u0274\u1d1b\u1d07\u0280\u0274\u1d07\u1d1b': 'internet', 'financialpost': 'newspaper', '\u029ca\u1d20\u1d07': ' have ', '\u1d04a\u0274': ' can ', 'Ma\u1d0b\u1d07': ' make ', '\u0280\u1d07\u029f\u026aa\u0299\u029f\u1d07': ' reliable ', '\u0274\u1d07\u1d07\u1d05': ' need ', '\u1d0f\u0274\u029f\u028f': ' only ', 'Sayfullo': 'terror suspect', 'Bumpski': 'gun', 'SLOTER': 'american football quarterback ', 'Vectum': 'between the vagina and the anus', 'etroit': 'narrow', 'u5a1a1': ' human mitochondrial dna haplogroup', 'ANN7': 'news channel', 'Chinp': 'chimpanzee', 'Khadrs': 'detained by bay for ten years, during which he pleaded guilty to the murder', 'Hanomansing': 'canadian television journalist', 'fRANCIS': 'sacred pope francis', 'TIFTFY': 'there i fixed that for you', 'WTFMSM': 'what the fuck main stream media', 'Timol': 'drug used to treat hypertension acute myocardial infarction migraine prophylaxis', 'SISs': 'make a hissing sound', 'nadcp': 'national association of drug court professionals', 'Chumpty': 'most often is short with blonde hair and is a bit of a numpty', 'Kahdr': 'detained by bay for ten years, during which he pleaded guilty to the murder', 'Gorkov': 'russian banker and attorney', 'DACAs': 'drug and alcohol clinical advisory service', 'onYouTube': ' on youtube', 'theDonald': 'president donald trump', 'Kizla': 'journalst', 'HitLIARy': 'hilaryclinton', 'onkey': 'Monkeys', 'Trumplethinskin': 'donald trump in a nutshell', 'thetyee': 'canadian news magazine ', 'AntiFa': 'a political protest movement comprising autonomous groups affiliated by their militant opposition to fascism and other forms of extreme right wing ideology', 'Birdich': 'american baseball executive', 'torontosun': 'tabloid', 'vancouversun': 'newspaper', 'consservativereviews': 'american news media company owned', 'muckamuck': 'a person of great importance or self importance', 'gubmut': 'person who can be classed as a total geek or nerd', 'whataboutism': 'attempts to discredit an opponent position by charging them with hypocrisy without directly refuting or disproving their argument', '\ud83d\ude0a': 'happiness and warm', 'Dotard': 'dotard', 'Ossoff': 'american politician', 'Moyane': 'commissioner of south african revenue service', 'djou': 'hawaii house of representatives and the honolulu city council', 'Pizzagate': 'debunked conspiracy theory', 'akleg': 'state legislature of the usa', 'Trumpettes': 'are a group of US socialites all women who have one mission in their sight to see donald trump reelected as usa president', 'khadr': 'detained by the united states at guantanamo Bay for ten years during which he pleaded guilty to the murder and other charges', 'commondreams': 'breaking news and views for the progressive community', 'Trumpland': 'documentary film usa presidential election campaign', 'talkingpointsmemo': ' political journalism website', 'TheDonald': 'president trump', 'ncronline': 'newspaper', 'scientificamerican': 'magazine', 'tRUMP': 'trump', '\ud83d\ude44': 'expressing disbelief annoyance impatience boredom and disdain', 'motelycrew ': 'roughly organized assembly of individuals of various backgrounds appearance and character', 'Nageak': 'American Politician', 'Zupta': 'wealthy indian-born South african family', 'Tridentinus': 'edible mushroom', 'dailycaller': 'american news and opinion website', 'Cheetolini': 'president trump', 'Trudeaus': 'prime minister of canada', 'Finicum': 'american spokesman', 'Trumpkins': 'carving donald trumps face onto a pumpkin', 'Donkel': 'dark', 'Daesh': 'terrorist organization', 'Trudope': 'corrupt prime minister of canada', 'shibai': 'state of hawaii', '2gTbpns': 'electronic cigarettes', 'klastri': 'cluster', '\u1d07x\u1d1b\u0280a': ' extra ', 'a\u0274': ' an ', 'a\u0274\u028f\u1d0f\u0274\u1d07': ' anyone ', 'Kurdistanis': 'kurdistan', 's\u1d1ba\u028f': 'stay', 'S\u1d1ba\u0280\u1d1b': ' start', 'SHOPO': 'shop'} ","19daeb37":"Stopwords=['it', \"it's\", 'its', 'itself', 'themselves', \n           'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these','tillerson'\n           'those','is','are','was','were','been','be', 'being', 'have', 'has',\n           'had', 'having', 'do', 'does', 'did', 'doing','an', 'the',\n           'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of','gorsuch'\n           'at', 'by', 'for', 'with', 'about','notley',\n           'between', 'into', 'through', 'during', 'before', 'after', 'above',\n           'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'over', 'under',\n           'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n           'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',\n           'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', \n           'very', 's', 't', 'can',\n           'will', 'just',\"don't\", 'should','have', 'now', 'd', 'll', 'm',\n           'o', 're', 've', 'y', 'ain', 'aren', \"arent\", 'couldn', \"couldnt\",\n           'didn', \"didnt\", 'doesn', \"doesn\", 'hadn', \"hadnt\", 'has','haven',\n           \"have\", 'isn', \"isnt\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n           'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n           \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\",'MAGAphant','PMPK','mcubz','Utqiagvik','Siemien','BLOTUS','JSB1','NFLGTV','Sniktaw','kiragirl','Tpubs','twittler','siemian','tJZ783','sloter','gubmit','guptas','getcivil','ZANC','KD48','mtf1953','NP5491','Covfefe','RAILFAIL','bsdetection','hapaguy','BCFN','Bavius','Larsy','jobpro22','xianleft','s1600','s2017','brad12','cn535aU5UsN7Lj8X8','HitLIARy','newsletterad','PETROWS','keaukaha','AKLNG','TransMountain'\n          'ITMFA','jerry69','RadirD','McWynnety','Pandora17','Lazeelink','covfefe','BCLibs','Krookwell','Trumpty','Exedus',\n          'LesterP','flexpipe','pgtype','907AK','diverdave','Outsider77','Nurnie','MAGAphants','Crapwell','Meggsy','11e6',\n          '11e7','bavius','Ontariowe','Layla4','garycrum','Brexit','bigly','wiliki','Saullie','Koncerned','RangerMC'                       \n          'ChickenLittle','Sambrailo','Binkleys','AtheO','JSwr','HIHS','brexit','sb21',\n          '20JPI','2green','Naole','ScreenViewsHD','physicaled','I\u200et',\n          'Kanekuni','arfc','Kilber','NFLHQTV','Trustsy']","56111917":"def preprocess(data):\n    '''\n    Reference: https:\/\/www.kaggle.com\/abhigupta4981\/pytorch-train-with-callbacks\n               and\n               simple-eda-text-preprocessing-jigsaw\n    '''\n    print(\"pre-processing\")\n    punct = [',', '.', '\"', ':', ')','\u1d00','(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']',\n          '>', '%', '=', '#', '*', '+', '\\\\', '\u2022', '~', '@', '\u00a3', '\u00b7', '_', '{', '}', '\u00a9', '^','\\n'\n          '\u00ae', '`', '<', '\u2192','Vinis', '\u00b0', '\u20ac', '\u2122', '\u203a', '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588',\n          '\u00bd', '\u00e0','an\u1d05', '\u2026', '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6',\n          '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\u2592', '\uff1a', '\u00bc',\n          '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2',\n          '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a',\n          '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a']\n    \n    def clean_special_chars(text, punct):\n        def get_misspell(text):\n            global contract\n            global contract1\n            global Stopwords\n            for word in text.split():\n                word_l=word.lower()\n                if word_l in contract:\n                    text = text.replace(word, contract[word_l])\n                elif word in contract:\n                    text = text.replace(word, contract[word])\n                elif word_l in contract1:\n                    text = text.replace(word,contract1[word_l])\n                elif word in contract1:\n                    text = text.replace(word,contract1[word])\n                    \n            return text\n        text=get_misspell(text)    \n        for p in (punct):\n            text = text.replace(p,' ')\n        return text\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n\n    return data","8f948b60":"# Pre-proccesing train data\ndata=preprocess(train['comment_text'].astype(str))\ntest_data=preprocess(test['comment_text'].astype(str))","6f9ad013":"data[1]","3fa7c45f":"\nvocab={}\nfor comment in tqdm(data):                                     # For each comment\n    sent=[]\n    comment=comment.lower()\n    for word in comment.split():                               # For each word\n        if word in Stopwords:                                  # If version of word in Stopwords than delete\n            comment=comment.replace(word,'')  \n        elif word in vocab:                                    # If word not in stop words than add to vocab as it is not deleted\n            vocab[word]=vocab[word]+1\n            sent.append(word)\n        else:\n            vocab[word]=1\n            sent.append(word)\n    comment=\" \".join(sent)\nprint(\"Training Data Prepared\")\n\nfor comment1 in tqdm(test_data):\n    sent=[]\n    comment1.lower()\n    for word in comment1.split():\n        word_l=word.lower()\n        if word_l in Stopwords:                                # If lower version of word in Stopwords than delete\n            comment=comment.replace(word_l,'')\n        elif word in Stopwords:                                # If word in Stopwords than delete\n            comment=comment.replace(word,'')\n        \n        elif word in vocab:                                    # If word not in stop words than add to vocab as it is not deleted\n            vocab[word]=vocab[word]+1\n            sent.append(word)\n        else:\n            vocab[word]=1\n            sent.append(word)\n    comment1=\" \".join(sent)\nprint(\"Testing Data Prepared\")\nprint(\"Total Vocab %d\"%len(vocab))\ndel comment,comment1,Stopwords,sent","fbb3111a":"data[1]","542e3acb":"f = open(\"..\/input\/glove840b300dtxt\/glove.840B.300d.txt\",'r') # Load Model\nembedding_values = {}\nfor line in tqdm(f):\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:],dtype = 'float32')\n    embedding_values[word]=coef\nprint(\"Model Loaded\",len(embedding_values))","57c1c65d":"left_vocab={}\nfor j in vocab.keys():\n    if j not in embedding_values:\n        left_vocab[vocab[j]]=j\nz=float((len(vocab)-len(left_vocab))*100\/len(vocab))\nprint(f\"Total Coverage %.2f \"%z + '%')","8488e5e3":"for j in sorted(left_vocab.keys(),reverse=True):\n        print(left_vocab[j]+\":\"+str(j))","87636e48":"# Tokenize_lammatize and Stop Words removal\nfrom keras.preprocessing.text import Tokenizer\nToken=Tokenizer()\nTotal=data\nTotal.append(test_data) #Total Data available for Vocab\nToken.fit_on_texts(Total)\nsequence = Token.texts_to_sequences(data)\nvocab_size = len(sequence)+1","8a1d28cc":"print(dir())","9c246e67":"del toxic_vs_non_toxic,toxic_df,toxic_count,vocab,word,word_l,preprocess,identities,j,line,left_vocab,contract,contract1,WordNetLemmatizer","5f00380e":"vocab_size","5d02ec4d":"\"\"\"https:\/\/www.kaggle.com\/gpreda\/jigsaw-fast-compact-solution\"\"\"\nidentity_columns = ['asian', 'atheist',\n       'bisexual', 'black', 'buddhist', 'christian', 'female',\n       'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',\n       'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\n       'muslim', 'other_disability', 'other_gender',\n       'other_race_or_ethnicity', 'other_religion',\n       'other_sexual_orientation', 'physical_disability',\n       'psychiatric_or_mental_illness', 'transgender', 'white']\n# Overall\nweights = np.ones((len(train),)) \/ 4\n\n# Subgroup\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) \/ 4\n\n# Background Positive, Subgroup Negative\nweights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) \/ 4\n\n# Background Negative, Subgroup Positive\nweights += (( (train['target'].values<0.5).astype(bool).astype(np.int) + (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) \/ 4\n\nloss_weight = 1.0 \/ weights.mean()\n\ny_train = np.vstack((train['target'],weights)).T\n\ndel train,test,data,test_data","b45218fd":"# Check All Loaded files and imports in RAM\nprint(dir())\ndel Total\nimport gc","72727c56":"#Loading Keras for training \nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,CuDNNLSTM\nfrom keras.layers import Convolution1D, GlobalMaxPooling1D,GlobalAveragePooling1D\nfrom keras.layers import Bidirectional\nfrom keras.preprocessing.sequence import pad_sequences\ngc.collect()","444307c5":"\n\nembedding_matrix = np.zeros((1804875,300))\n# Padding Sequence Which are nothing just integer value Encoded sentences\npad_seq = pad_sequences(sequence,maxlen = 100)\n\n# Preparing Matrix for word Embeddings out of Vocab used\nfor word,i in tqdm(Token.word_index.items()):\n    values = embedding_values.get(word)\n    if values is not None:\n        embedding_matrix[i] = values\ndel embedding_values,f","acc4f714":"print(dir())","5c99c9ae":"del stopwords,threading,identity_columns,loss_weight,miss_val_train_df,non_toxic_count,non_toxic_df,np","ce8b2c0c":"# Sentence Via Biderectional LSTM\nmodel=Sequential()\nmodel.add(Embedding(1804875,300,input_length = 300,weights = [embedding_matrix],trainable = False))\nmodel.add(Bidirectional(CuDNNLSTM(300,return_sequences=True)))\nmodel.add(Bidirectional(CuDNNLSTM(300,return_sequences=True)))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dense(900,activation = 'relu'))\nmodel.add(Dense(400, activation = 'sigmoid'))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nmodel.summary()\n","79a1d8dc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test=train_test_split(pad_seq,y_train, test_size=0.3, random_state=42)","d3a1f441":"history=model.fit(X_train,y_train,epochs=5,batch_size=100,validation_data=(X_test,y_test))","d8738489":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n","805940e0":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","3885cf97":"del pad_seq,X_train, X_test, y_train, y_test\nsequence = Token.texts_to_sequences(test_data)\ntest_pad = pad_sequences(sequence,maxlen = 100)\nprediction = model.predict(test_pad)\ndel test_data,Total,Token,embedding_matrix,embedding_values","0bbd7101":"#Alining the values to submit\n#Submiting the results\nsubmission = pd.DataFrame([test['id']]).T\nsubmission['prediction'] = [float(x) for x,_ in prediction]","6c74a5dc":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","248bd2b1":"Bi directional LSTM used in model. Total trainable Features: 187,586","b517633f":"PREPROCESS FILTERS.","7110d269":"The heatmap plot of the correlation between the identities is very insightful. I will summarize my observations below. As always, if you see something interesting please mention it to me in the comment section.\nIt is interesting to see that strong correlations form triangular area at the edge of diagonal.\nThis basically means that there is a strong correlation between the groups of the identity (gender, religion, races, disabilities). This means, the comments where male identity is the target, female identity is also very likely to be the target.\nIn another words, in toxic and non-toxic comments, people tend to make it about one group vs another quiet frequently.","9c8027a3":"But we can skip them as most of them are names of people and places. ","18ef3db0":"**Demographics** : Statistical data relating to the population and particular groups within it","4988aa95":"<h3> A good idea to count the amount of missing values before diving into any analysis.<\/h1>\n<h3> Lets also see how many missing values (in percentage) we are dealing with. <\/h1>\n","568873cf":"    Train Test Split","bdc6c76d":"* Loading Glove Embeddings. \n* 840B embeddings, each corresponds to 300 dimensions.","edccd1dd":"Checking the Vocab coverage","b3e36034":"<h3>Toxic vs Non-Toxic","42dd3930":"Contractions Which includes abrevations and slangs.","0598d45e":"Checking vocab coverage and trying to reach 100%.","4345f9cf":"To Check All Loaded files and imports in RAM.","8b13d482":"**According to results 5 Epochs suits best **","e95a7868":"Check for Null Values for train","3cded98c":"Removing Stop words.","fa5ebb65":"FILTERING ENTIRE RAW TRAINING DATA.","f309e681":"TOKENIZE AND LET THE WHOLE DATA FIT FOR UNIQUE TOKENS.\n- Optional: Pickling of files to save on temporary stages and Let some space created.","f7c8b9dd":"Correlation b\/w data","f8a59cb1":"Remaining Contraction or Stopwords that need to be resolved.","1f582dc7":"Train Model","d65b85e4":"RAW DATA LOADING.","fa693cd6":"It is seen clear that non-essential columns have higher null values."}}