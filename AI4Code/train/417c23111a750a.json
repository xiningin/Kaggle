{"cell_type":{"024fc0e6":"code","16a09e80":"code","021fe67e":"code","1a41cf1a":"code","43d47da7":"code","26f20bc6":"code","bc052371":"code","a8fd41f7":"code","44fa61fc":"code","a37edaae":"code","7f34fb58":"code","4ddfee1d":"code","11dcf29f":"code","1e6606f0":"code","5e673e5c":"code","c567ac06":"code","4b959294":"code","5d3d1796":"code","06c6bb30":"code","aa8abdd1":"markdown","08ef3e24":"markdown","7e41db3b":"markdown","870e5024":"markdown","6e65a80c":"markdown","ba68a69c":"markdown"},"source":{"024fc0e6":"import os\nimport sys\nimport time\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize\n\nfrom IPython.display import clear_output\n\nfrom PIL import Image\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms.functional as TF\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import classification_report, confusion_matrix","16a09e80":"imgs_dir='..\/input\/brats-nifti-to-jpg\/BraTs'\nall_dirs = glob (f'{imgs_dir}\/*')\nlen (all_dirs)\nall_dirs.sort()\n# test_dirs = np.copy(all_dirs[45000:])\n# all_dirs = all_dirs[:45000]\n# len(all_dirs), all_dirs[:10]","021fe67e":"def shuffle_split (all_dirs, val_pct = 0.15, seed = 99):\n    \"\"\" shuffling dataset with random state and split to train and valid \"\"\"\n    n_val = int (len (all_dirs) * val_pct)\n    np.random.seed (seed)\n    idx = np.random.permutation (len (all_dirs))\n    all_dirs = np.array (all_dirs) [idx]\n    \n    return all_dirs [n_val:], all_dirs [:n_val]\n\ntrain_dirs, valid_dirs = shuffle_split (all_dirs, seed = 1)\nlen(valid_dirs), len(train_dirs)","1a41cf1a":"class BratsDataset (Dataset):\n    def __init__ (self, img_dirs, modality_types, transform = None):\n        self.img_dirs = img_dirs\n        self.transform = transform\n\n    def __len__ (self):\n        return len (self.img_dirs)\n\n    def __getitem__ (self, index):\n        imgs_path = self.img_dirs [index]\n        image = self.concat_imgs (imgs_path)\n        mask = np.array (Image.open (f'{imgs_path}\/seg.jpg'))\n        mask = (mask \/ 255 * 4).round ()\n        mask = self.preprocess_mask_labels(mask)\n        \n        \n        if self.transform is not None:\n            augmented = self.transform(image = image, mask = mask)\n            image = augmented ['image']\n            mask = augmented ['mask']\n\n        return image.astype(np.float), mask.astype(np.float)\n\n    def concat_imgs (self, path: str):\n        types = []\n        for modality_type in modality_types:\n            img = np.array (Image.open (f'{path}\/{modality_type}.jpg'))\n            img = self.normalize(img)\n            types.append (img)\n#         cat_img = np.concatenate (types, axis = -1)\n\n        return np.array(types)\n    \n    def preprocess_mask_labels(self, mask: np.ndarray):\n        mask_WT = np.zeros(mask.shape)\n        mask_WT[mask == 2] = 1\n       \n        mask_TC = np.zeros(mask.shape)\n        mask_TC[mask == 1] = 1\n\n        mask_ET = np.zeros(mask.shape)\n        mask_ET[mask == 3] = 1\n        \n        mask_BG = np.zeros(mask.shape)\n        mask_BG[mask == 0] = 1\n\n        mask = np.stack([mask_WT, mask_TC, mask_ET, mask_BG])\n        # mask = np.moveaxis(mask, (0, 1, 2), (0, 2, 1))\n        return mask\n    \n    def normalize(self, data: np.ndarray):\n        data_min = np.min(data)\n        if np.max(data) == 0:\n            return data\n        if (np.max(data) - data_min) == 0:\n            return data \/ data_min \n        \n        return (data - data_min) \/ (np.max(data) - data_min)\n\n    trn_tfms = A.Compose (\n[\n    A.Resize (height = 240, width = 240),\n    #     A.Rotate (limit = 35, p = 1.0),\n    #     A.HorizontalFlip (p = 0.5),\n    #     A.VerticalFlip (p = 0.1),\n        # A.Normalize (mean=0.5, std=0.5, max_pixel_value = 255.0), \n            # img = (img - mean * max_pixel_value) \/ (std * max_pixel_value)\n    #     ToTensorV2 ()\n            # The numpy HWC image is converted to pytorch CHW tensor. If the image is in HW format (grayscale image), it will be converted to pytorch HW tensor.\n])\n\nval_tfms = A.Compose (\n[\n    A.Resize (height = 240, width = 240),\n    #     A.Normalize (0.5, 0.5, max_pixel_value = 255.0),\n    #     ToTensorV2 ()\n])\n\nmodality_types = ['flair', 't1', 't1ce', 't2']","43d47da7":"train_ds = BratsDataset(train_dirs, modality_types)\nvalid_ds = BratsDataset(valid_dirs, modality_types)\ntrain_dl = DataLoader(train_ds, batch_size = 8, shuffle = False, num_workers = 2, pin_memory = True)\nvalid_dl = DataLoader(valid_ds, batch_size = 8, shuffle = False, num_workers = 2, pin_memory = True)\n","26f20bc6":"dl_it=iter (train_dl)\ndl_it.next ()\ndl_it.next ()\ndl_it.next ()\ndl_it.next ()\nimgs, msks = dl_it.next ()\nprint (imgs.shape)\nprint (msks.shape)\n\nidx = np.random.permutation (imgs.shape [0])\nimgs = imgs [idx]\nmsks = msks [idx]\n\nplt.figure (figsize = (25, 20))\nfor i in range (4):\n    for j in range (len (modality_types)):\n        plt.subplot (4, 5, 5 * i + j + 1)\n        plt.imshow (imgs[i][j], cmap = 'bone')\n        plt.axis ('off')\n        plt.title (modality_types [j])\n    plt.subplot (4, 5, 5 * i + 5)\n    plt.imshow (256-(np.argmax(msks[i], axis=0)*80 ), cmap='gray')\n    plt.axis ('off')\n    plt.title ('Mask')","bc052371":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels \/\/ 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels \/\/ 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX \/\/ 2, diffX - diffX \/\/ 2,\n                        diffY \/\/ 2, diffY - diffY \/\/ 2])\n        # if you have padding issues, see\n        # https:\/\/github.com\/HaiyongJiang\/U-Net-Pytorch-Unstructured-Buggy\/commit\/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https:\/\/github.com\/xiaopeng-liao\/Pytorch-UNet\/commit\/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        # self.activation = nn.Softmax()\n    def forward(self, x):\n        return self.conv(x)\n        # return self.activation(self.conv(x))\n\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 \/\/ factor)\n        self.up1 = Up(1024, 512 \/\/ factor, bilinear)\n        self.up2 = Up(512, 256 \/\/ factor, bilinear)\n        self.up3 = Up(256, 128 \/\/ factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits","a8fd41f7":"def dice_coef_metric(probabilities: torch.Tensor,\n                     truth: torch.Tensor,\n                     treshold: float = 0.5,\n                     eps: float = 1e-9) -> np.ndarray:\n    \"\"\"\n    Calculate Dice score for data batch.\n    Params:\n        probobilities: model outputs after activation function.\n        truth: truth values.\n        threshold: threshold for probabilities.\n        eps: additive to refine the estimate.\n        Returns: dice score aka f1.\n    \"\"\"\n    scores = []\n    num = probabilities.shape[0]\n    predictions = (probabilities >= treshold).float()\n    assert(predictions.shape == truth.shape)\n    for i in range(num):\n        prediction = predictions[i]\n        truth_ = truth[i]\n        intersection = 2.0 * (truth_ * prediction).sum()\n        union = truth_.sum() + prediction.sum()\n        if truth_.sum() == 0 and prediction.sum() == 0:\n            scores.append(1.0)\n        else:\n            scores.append((intersection + eps) \/ union)\n    return np.mean(scores)\n\n\ndef jaccard_coef_metric(probabilities: torch.Tensor,\n               truth: torch.Tensor,\n               treshold: float = 0.5,\n               eps: float = 1e-9) -> np.ndarray:\n    \"\"\"\n    Calculate Jaccard index for data batch.\n    Params:\n        probobilities: model outputs after activation function.\n        truth: truth values.\n        threshold: threshold for probabilities.\n        eps: additive to refine the estimate.\n        Returns: jaccard score aka iou.\"\n    \"\"\"\n    scores = []\n    num = probabilities.shape[0]\n    predictions = (probabilities >= treshold).float()\n    assert(predictions.shape == truth.shape)\n\n    for i in range(num):\n        prediction = predictions[i]\n        truth_ = truth[i]\n        intersection = (prediction * truth_).sum()\n        union = (prediction.sum() + truth_.sum()) - intersection + eps\n        if truth_.sum() == 0 and prediction.sum() == 0:\n            scores.append(1.0)\n        else:\n            scores.append((intersection + eps) \/ union)\n    return np.mean(scores)\n\n\nclass Meter:\n    '''factory for storing and updating iou and dice scores.'''\n    def __init__(self, treshold: float = 0.5):\n        self.threshold: float = treshold\n        self.dice_scores: list = []\n        self.iou_scores: list = []\n    \n    def update(self, logits: torch.Tensor, targets: torch.Tensor):\n        \"\"\"\n        Takes: logits from output model and targets,\n        calculates dice and iou scores, and stores them in lists.\n        \"\"\"\n        probs = torch.sigmoid(logits)\n        dice = dice_coef_metric(probs, targets, self.threshold)\n        iou = jaccard_coef_metric(probs, targets, self.threshold)\n        \n        self.dice_scores.append(dice)\n        self.iou_scores.append(iou)\n    \n    def get_metrics(self) -> np.ndarray:\n        \"\"\"\n        Returns: the average of the accumulated dice and iou scores.\n        \"\"\"\n        dice = np.mean(self.dice_scores)\n        iou = np.mean(self.iou_scores)\n        return dice, iou\n\n\nclass DiceLoss(nn.Module):\n    \"\"\"Calculate dice loss.\"\"\"\n    def __init__(self, eps: float = 1e-9):\n        super(DiceLoss, self).__init__()\n        self.eps = eps\n        \n    def forward(self,\n                logits: torch.Tensor,\n                targets: torch.Tensor) -> torch.Tensor:\n        \n        num = targets.size(0)\n        probability = torch.sigmoid(logits)\n        probability = probability.view(num, -1)\n        targets = targets.view(num, -1)\n        assert(probability.shape == targets.shape)\n        \n        intersection = 2.0 * (probability * targets).sum()\n        union = probability.sum() + targets.sum()\n        dice_score = (intersection + self.eps) \/ union\n        #print(\"intersection\", intersection, union, dice_score)\n        return 1.0 - dice_score\n        \n        \nclass BCEDiceLoss(nn.Module):\n    \"\"\"Compute objective loss: BCE loss + DICE loss.\"\"\"\n    def __init__(self):\n        super(BCEDiceLoss, self).__init__()\n        self.bce = nn.BCEWithLogitsLoss()\n        self.dice = DiceLoss()\n        \n    def forward(self, \n                logits: torch.Tensor,\n                targets: torch.Tensor) -> torch.Tensor:\n        assert(logits.shape == targets.shape)\n        dice_loss = self.dice(logits, targets)\n        bce_loss = self.bce(logits, targets)\n        \n        return bce_loss + dice_loss\n    \n# helper functions for testing.  \ndef dice_coef_metric_per_classes(probabilities: np.ndarray,\n                                    truth: np.ndarray,\n                                    treshold: float = 0.5,\n                                    eps: float = 1e-9,\n                                    classes: list = ['WT', 'TC', 'ET', 'BG']) -> np.ndarray:\n    \"\"\"\n    Calculate Dice score for data batch and for each class.\n    Params:\n        probobilities: model outputs after activation function.\n        truth: model targets.\n        threshold: threshold for probabilities.\n        eps: additive to refine the estimate.\n        classes: list with name classes.\n        Returns: dict with dice scores for each class.\n    \"\"\"\n    scores = {key: list() for key in classes}\n    num = probabilities.shape[0]\n    num_classes = probabilities.shape[1]\n    predictions = (probabilities >= treshold).astype(np.float32)\n    assert(predictions.shape == truth.shape)\n\n    for i in range(num):\n        for class_ in range(num_classes):\n            prediction = predictions[i][class_]\n            truth_ = truth[i][class_]\n            intersection = 2.0 * (truth_ * prediction).sum()\n            union = truth_.sum() + prediction.sum()\n            if truth_.sum() == 0 and prediction.sum() == 0:\n                 scores[classes[class_]].append(1.0)\n            else:\n                scores[classes[class_]].append((intersection + eps) \/ union)\n                \n    return scores\n\n\ndef jaccard_coef_metric_per_classes(probabilities: np.ndarray,\n               truth: np.ndarray,\n               treshold: float = 0.5,\n               eps: float = 1e-9,\n               classes: list = ['WT', 'TC', 'ET', 'BG']) -> np.ndarray:\n    \"\"\"\n    Calculate Jaccard index for data batch and for each class.\n    Params:\n        probobilities: model outputs after activation function.\n        truth: model targets.\n        threshold: threshold for probabilities.\n        eps: additive to refine the estimate.\n        classes: list with name classes.\n        Returns: dict with jaccard scores for each class.\"\n    \"\"\"\n    scores = {key: list() for key in classes}\n    num = probabilities.shape[0]\n    num_classes = probabilities.shape[1]\n    predictions = (probabilities >= treshold).astype(np.float32)\n    assert(predictions.shape == truth.shape)\n\n    for i in range(num):\n        for class_ in range(num_classes):\n            prediction = predictions[i][class_]\n            truth_ = truth[i][class_]\n            intersection = (prediction * truth_).sum()\n            union = (prediction.sum() + truth_.sum()) - intersection + eps\n            if truth_.sum() == 0 and prediction.sum() == 0:\n                 scores[classes[class_]].append(1.0)\n            else:\n                scores[classes[class_]].append((intersection + eps) \/ union)\n\n    return scores","44fa61fc":"class Trainer:\n    \"\"\"\n    Factory for training proccess.\n    Args:\n        display_plot: if True - plot train history after each epoch.\n        net: neural network for mask prediction.\n        criterion: factory for calculating objective loss.\n        optimizer: optimizer for weights updating.\n        phases: list with train and validation phases.\n        dataloaders: dict with data loaders for train and val phases.\n        path_to_csv: path to csv file.\n        meter: factory for storing and updating metrics.\n        batch_size: data batch size for one step weights updating.\n        num_epochs: num weights updation for all data.\n        accumulation_steps: the number of steps after which the optimization step can be taken\n                    (https:\/\/www.kaggle.com\/c\/understanding_cloud_organization\/discussion\/105614).\n        lr: learning rate for optimizer.\n        scheduler: scheduler for control learning rate.\n        losses: dict for storing lists with losses for each phase.\n        jaccard_scores: dict for storing lists with jaccard scores for each phase.\n        dice_scores: dict for storing lists with dice scores for each phase.\n    \"\"\"\n    def __init__(self,\n                 net: nn.Module,\n                 train_dl: DataLoader,\n                 val_dl: DataLoader,\n                 criterion: nn.Module,\n                 lr: float,\n                 accumulation_steps: int,\n                 batch_size: int,\n                 num_epochs: int,\n                 display_plot: bool = True,\n\n                ):\n\n        \"\"\"Initialization.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        print(\"device:\", self.device)\n        self.display_plot = display_plot\n        self.net = net\n        self.net = self.net.to(self.device)\n        self.criterion = criterion\n        self.optimizer = Adam(self.net.parameters(), lr=lr)\n        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\",\n                                           patience=2, verbose=True)\n        self.accumulation_steps = accumulation_steps \/\/ batch_size\n        self.phases = [\"train\", \"val\"]\n        self.num_epochs = num_epochs\n        self.dataloaders = {\n            \"train\": train_dl,\n            \"val\"  : val_dl \n        }\n        self.best_loss = float(\"inf\")\n        self.losses = {phase: [] for phase in self.phases}\n        self.dice_scores = {phase: [] for phase in self.phases}\n        self.jaccard_scores = {phase: [] for phase in self.phases}\n         \n    def _compute_loss_and_outputs(self,\n                                  images: torch.Tensor,\n                                  targets: torch.Tensor):\n        images = images.to(self.device)\n        targets = targets.to(self.device)\n        logits = self.net(images.float())\n        loss = self.criterion(logits, targets)\n        return loss, logits\n        \n    def _do_epoch(self, epoch: int, phase: str):\n        print(f\"{phase} epoch: {epoch} | time: {time.strftime('%H:%M:%S')}\")\n\n        self.net.train() if phase == \"train\" else self.net.eval()\n        meter = Meter()\n        dataloader = self.dataloaders[phase]\n        total_batches = len(dataloader)\n        running_loss = 0.0\n        self.optimizer.zero_grad()\n        for itr, (images, targets) in enumerate(dataloader):\n#             print(images.shape, targets.shape)\n#             images, targets = data_batch['image'], data_batch['mask']\n            loss, logits = self._compute_loss_and_outputs(images, targets)\n            loss = loss \/ self.accumulation_steps\n            if phase == \"train\":\n                loss.backward()\n                if (itr + 1) % self.accumulation_steps == 0:\n                    self.optimizer.step()\n                    self.optimizer.zero_grad()\n            running_loss += loss.item()\n            meter.update(logits.detach().cpu(),\n                         targets.detach().cpu()\n                        )\n            \n        epoch_loss = (running_loss * self.accumulation_steps) \/ total_batches\n        epoch_dice, epoch_iou = meter.get_metrics()\n        \n        self.losses[phase].append(epoch_loss)\n        self.dice_scores[phase].append(epoch_dice)\n        self.jaccard_scores[phase].append(epoch_iou)\n\n        return epoch_loss\n        \n    def run(self):\n        for epoch in range(self.num_epochs):\n            self._do_epoch(epoch, \"train\")\n            with torch.no_grad():\n                val_loss = self._do_epoch(epoch, \"val\")\n                self.scheduler.step(val_loss)\n            if self.display_plot:\n                self._plot_train_history()\n                \n            if val_loss < self.best_loss:\n                print(f\"\\n{'#'*20}\\nSaved new checkpoint\\n{'#'*20}\\n\")\n                self.best_loss = val_loss\n                torch.save(self.net.state_dict(), \"best_model.pth\")\n            print()\n        self._save_train_history()\n            \n    def _plot_train_history(self):\n        data = [self.losses, self.dice_scores, self.jaccard_scores]\n        colors = ['deepskyblue', \"crimson\"]\n        labels = [\n            f\"\"\"\n            train loss {self.losses['train'][-1]}\n            val loss {self.losses['val'][-1]}\n            \"\"\",\n            \n            f\"\"\"\n            train dice score {self.dice_scores['train'][-1]}\n            val dice score {self.dice_scores['val'][-1]} \n            \"\"\", \n                  \n            f\"\"\"\n            train jaccard score {self.jaccard_scores['train'][-1]}\n            val jaccard score {self.jaccard_scores['val'][-1]}\n            \"\"\",\n        ]\n        \n        clear_output(True)\n        with plt.style.context(\"seaborn-dark-palette\"):\n            fig, axes = plt.subplots(3, 1, figsize=(8, 10))\n            for i, ax in enumerate(axes):\n                ax.plot(data[i]['val'], c=colors[0], label=\"val\")\n                ax.plot(data[i]['train'], c=colors[-1], label=\"train\")\n                ax.set_title(labels[i])\n                ax.legend(loc=\"upper right\")\n                \n            plt.tight_layout()\n            plt.show()\n            \n    def load_predtrain_model(self,\n                             state_path: str):\n        self.net.load_state_dict(torch.load(state_path))\n        print(\"Predtrain model loaded\")\n        \n    def _save_train_history(self):\n        \"\"\"writing model weights and training logs to files.\"\"\"\n        torch.save(self.net.state_dict(),\n                   f\"last_epoch_model.pth\")\n\n        logs_ = [self.losses, self.dice_scores, self.jaccard_scores]\n        log_names_ = [\"_loss\", \"_dice\", \"_jaccard\"]\n        logs = [logs_[i][key] for i in list(range(len(logs_)))\n                         for key in logs_[i]]\n        log_names = [key+log_names_[i] \n                     for i in list(range(len(logs_))) \n                     for key in logs_[i]\n                    ]\n        pd.DataFrame(\n            dict(zip(log_names, logs))\n        ).to_csv(\"train_log.csv\", index=False)","a37edaae":"batch_size = 8\n# train_dirs=train_dirs[:200]\n# valid_dirs=valid_dirs[:200]\ntrain_ds = BratsDataset(train_dirs, modality_types)\nvalid_ds = BratsDataset(valid_dirs, modality_types)\ntrain_dl = DataLoader(train_ds, batch_size = batch_size, shuffle = False, num_workers = 2, pin_memory = True)\nvalid_dl = DataLoader(valid_ds, batch_size = batch_size, shuffle = False, num_workers = 2, pin_memory = True)\nprint(len(valid_dl ), len(train_dl))\ndevice = torch.device ('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = UNet(n_channels=4, n_classes=4, bilinear=True).to(device).float()","7f34fb58":"from torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\n\ntrainer = Trainer(net=model,\n                  train_dl=train_dl,\n                  val_dl=valid_dl,\n                  criterion=BCEDiceLoss(),\n                  lr=5e-4,\n                  accumulation_steps=batch_size,\n                  batch_size=batch_size,\n                  num_epochs=10,\n                 )","4ddfee1d":"%%time\ntrainer.run()","11dcf29f":"def compute_scores_per_classes(model,\n                               dataloader,\n                               classes):\n    \"\"\"\n    Compute Dice and Jaccard coefficients for each class.\n    Params:\n        model: neural net for make predictions.\n        dataloader: dataset object to load data from.\n        classes: list with classes.\n        Returns: dictionaries with dice and jaccard coefficients for each class for each slice.\n    \"\"\"\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    dice_scores_per_classes = {key: list() for key in classes}\n    iou_scores_per_classes = {key: list() for key in classes}\n\n    with torch.no_grad():\n        for i, (imgs, targets) in enumerate(dataloader):\n#             imgs, targets = data['image'], data['mask']\n            imgs, targets = imgs.to(device), targets.to(device)\n            logits = model(imgs.float())\n            logits = logits.detach().cpu().numpy()\n            targets = targets.detach().cpu().numpy()\n            \n\n            dice_scores = dice_coef_metric_per_classes(logits, targets)\n            iou_scores = jaccard_coef_metric_per_classes(logits, targets)\n            for key in dice_scores.keys():\n                dice_scores_per_classes[key].extend(dice_scores[key])\n\n            for key in iou_scores.keys():\n                iou_scores_per_classes[key].extend(iou_scores[key])\n\n    return dice_scores_per_classes, iou_scores_per_classes\n","1e6606f0":"valid_ds = BratsDataset(valid_dirs, modality_types)\nvalid_dl = DataLoader(valid_ds, batch_size = batch_size, shuffle = False, num_workers = 2, pin_memory = True)\n","5e673e5c":"model.eval();","c567ac06":"%%time\ndice_scores_per_classes, iou_scores_per_classes = compute_scores_per_classes(\n    model, valid_dl, ['WT', 'TC', 'ET', 'BG']\n    )","4b959294":"dice_df = pd.DataFrame(dice_scores_per_classes)\ndice_df.columns = ['WT dice', 'TC dice', 'ET dice', 'BG dice']\n\niou_df = pd.DataFrame(iou_scores_per_classes)\niou_df.columns = ['WT jaccard', 'TC jaccard', 'ET jaccard', 'BG jaccard']\nval_metics_df = pd.concat([dice_df, iou_df], axis=1, sort=True)\nval_metics_df = val_metics_df.loc[:, ['WT dice', 'WT jaccard', \n                                      'TC dice', 'TC jaccard', \n                                      'ET dice', 'ET jaccard',\n                                      'BG dice', 'BG jaccard']]\nval_metics_df.sample(5)","5d3d1796":"colors = ['#35FCFF', '#FF355A', '#96C503', '#C5035B', '#28B463', '#35FFAF']\npalette = sns.color_palette(colors, 6)\n\nfig, ax = plt.subplots(figsize=(12, 6));\nsns.barplot(x=val_metics_df.mean().index, y=val_metics_df.mean(), palette=palette, ax=ax);\nax.set_xticklabels(val_metics_df.columns, fontsize=14, rotation=15);\nax.set_title(\"Dice and Jaccard Coefficients from Validation\", fontsize=20)\n\nfor idx, p in enumerate(ax.patches):\n        percentage = '{:.1f}%'.format(100 * val_metics_df.mean().values[idx])\n        x = p.get_x() + p.get_width() \/ 2 - 0.15\n        y = p.get_y() + p.get_height()\n        ax.annotate(percentage, (x, y), fontsize=15, fontweight=\"bold\")\n\nfig.savefig(\"result1.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\nfig.savefig(\"result1.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')","06c6bb30":"def show_results (model, test_dl, checkpoint_path = None):\n    \"\"\" showing image, mask and predicted mask for one batch \"\"\"\n    if checkpoint_path is not None:\n        load_checkpoint (torch.load (checkpoint_path), model)\n    dl = iter(test_dl)\n#     dl.next()\n#     dl.next()\n#     dl.next()\n#     dl.next()\n#     dl.next()\n#     dl.next()\n#     dl.next()\n    images, masks = dl.next()\n\n    for BB in range(4):\n        images, masks = dl.next()\n        images = images.to(device)\n        masks = masks.to(device)\n        outputs = model (images.float())\n\n        preds = torch.argmax (outputs, dim = 1)\n        masks = torch.argmax (masks, dim = 1)\n#         print(torch.unique(preds), torch.unique(masks))\n        masks = masks*84\n        preds = preds*84\n\n        mean = 0.5\n        std = 0.5\n        plt.figure (figsize = (20, 40))\n        for i in range (8):\n            for j in range (len (modality_types)):\n                # show all type of images\n                plt.subplot (16, 6, 6 * i + j + 1)\n                plt.axis ('off')\n                plt.title (modality_types [j])\n    #             image = gpu_to_cpu (images [i][j], std, mean)\n                plt.imshow (images [i][j].cpu(), cmap = 'bone')\n            # show True Mask\n            plt.subplot (16, 6, 6 * i + 5)\n            plt.title ('True Mask')\n            plt.axis ('off')\n            plt.imshow (255 - masks[i].cpu(), cmap = 'bone')\n            # show Predicted Mask\n            plt.subplot (16, 6, 6 * i + 6)\n            plt.title ('Predicted Mask')\n            plt.axis ('off')\n    #         pred = gpu_to_cpu (preds [i], std, mean)\n            plt.imshow (255 - preds[i].cpu(), cmap = 'bone')\n\n        plt.show ()\n    return masks, preds\nshow_results (model, valid_dl, checkpoint_path = None)\n","aa8abdd1":"# Model","08ef3e24":"# Imports","7e41db3b":"# DataLoader","870e5024":"# Preprocess","6e65a80c":"# Loss && Metrics","ba68a69c":"# Visualization"}}