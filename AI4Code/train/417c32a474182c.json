{"cell_type":{"6d98c7a1":"code","7c1e0161":"code","6fc337a7":"code","c42afb1c":"code","7d92b4cd":"code","7c0d9bb6":"code","75964ba5":"code","7839e62f":"code","bbc1d116":"code","6b9ffc57":"code","1ea48ab0":"code","632bbcbb":"code","4fbfc07b":"code","ec36b4e3":"code","b44d115e":"code","c3ea7644":"code","63f0380c":"code","a4ebe7fa":"code","b8613eb5":"code","acf92997":"code","045ccb95":"code","37f5ec81":"code","7267d7a9":"code","16f9b8c5":"code","471a017e":"code","cf7ba562":"code","343263e6":"code","291788cf":"code","c285b69a":"code","81f4f17f":"code","70442638":"markdown","62014615":"markdown","955815dd":"markdown","a2c48f0d":"markdown","e5f517ba":"markdown","6f022caf":"markdown","cc1c0db5":"markdown","3888d60f":"markdown","66b00cea":"markdown","85b6ca46":"markdown","99a05360":"markdown","a287ba7a":"markdown","d5fe2788":"markdown","308e21b2":"markdown","34066042":"markdown","0e7fbb12":"markdown","92ec91ae":"markdown","baaec536":"markdown","7b8ba2e1":"markdown","78fabdcb":"markdown","5b05e137":"markdown","e02deb99":"markdown","cf377c1a":"markdown","345b9dad":"markdown","6df30827":"markdown","990b1f05":"markdown","f9a18809":"markdown","0a3e84c1":"markdown","14002b49":"markdown","8c7ab3ca":"markdown","bbd91d0d":"markdown"},"source":{"6d98c7a1":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport ast\nimport os\nimport json\nimport pandas as pd\nimport torch\nimport importlib\nimport cv2 \n\nimport shutil\nfrom shutil import copyfile\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nfrom sklearn.model_selection import GroupKFold\nfrom PIL import Image\nfrom string import Template\nfrom IPython.display import display\nTRAIN_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef'","7c1e0161":"print(f\"Torch: {torch.__version__}\")\n!nvcc --version","6fc337a7":"!git clone https:\/\/github.com\/Megvii-BaseDetection\/YOLOX -q\n\n%cd YOLOX\n!pip install -U pip && pip install -r requirements.txt\n!pip install -v -e . ","c42afb1c":"!pip install 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'","7d92b4cd":"def get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\ndef get_path(row):\n    row['image_path'] = f'{TRAIN_PATH}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    return row\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)","7c0d9bb6":"# Read in the data CSV files\ndf = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\ndf.head(5)","75964ba5":"df[\"NumBBox\"]=df['annotations'].apply(lambda x: str.count(x, 'x'))\ndf.head(5)","7839e62f":"print(df[\"NumBBox\"].unique())","bbc1d116":"df_train=df[df[\"NumBBox\"]>0]\ndf_train.sample(2)","6b9ffc57":"print(df_train['NumBBox'].sum())","1ea48ab0":"df_train['annotations'] = df_train['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf_train['bboxes'] = df_train.annotations.progress_apply(get_bbox)\ndf_train.sample(2)","632bbcbb":"df_train[\"width\"]=1280\ndf_train[\"height\"]=720\ndf_train.sample(2)","4fbfc07b":"df_train = df_train.progress_apply(get_path, axis=1)\ndf_train.sample(2)","ec36b4e3":"n_spl=3\nSelected_Fold=2 #0..2\n\nfrom sklearn.model_selection import GroupKFold\ngkf  = GroupKFold(n_splits = n_spl) # num_folds=3 as there are total 3 videos\ndf_train = df_train.reset_index(drop=True)\ndf_train['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(df_train, groups = df_train.video_id.tolist())):\n    df_train.loc[val_idx, 'fold'] = fold\ndisplay(df_train.fold.value_counts())","b44d115e":"Work_Dir = '\/kaggle\/working\/' \nDataSet_Path = 'dataset\/images'\n\nos.makedirs(f'{Work_Dir}{DataSet_Path}\/train2017', exist_ok=True)\nos.makedirs(f'{Work_Dir}{DataSet_Path}\/val2017', exist_ok=True)\nos.makedirs(f'{Work_Dir}{DataSet_Path}\/annotations', exist_ok=True)","c3ea7644":"for i in tqdm(range(len(df_train))):\n    row = df_train.loc[i]\n    if row.fold != Selected_Fold:\n        copyfile(f'{row.image_path}', f'{Work_Dir}{DataSet_Path}\/train2017\/{row.image_id}.jpg')\n    else:\n        copyfile(f'{row.image_path}', f'{Work_Dir}{DataSet_Path}\/val2017\/{row.image_id}.jpg') ","63f0380c":"print(f'Number of training files: {len(os.listdir(f\"{Work_Dir}{DataSet_Path}\/train2017\/\"))}')\nprint(f'Number of validation files: {len(os.listdir(f\"{Work_Dir}{DataSet_Path}\/val2017\/\"))}')","a4ebe7fa":"annotion_id = 0\n\ndef save_annot_json(json_annotation, filename):\n    with open(filename, 'w') as f:\n        output_json = json.dumps(json_annotation)\n        f.write(output_json)","b8613eb5":"def dataset2coco(df, dest_path):\n    \n    global annotion_id\n    \n    annotations_json = {\n        \"info\": [],\n        \"licenses\": [],\n        \"categories\": [],\n        \"images\": [],\n        \"annotations\": []\n    }\n    \n    info = {\n        \"year\": \"2021\",\n        \"version\": \"1\",\n        \"description\": \"COTS dataset - COCO format\",\n        \"contributor\": \"\",\n        \"url\": \"https:\/\/kaggle.com\",\n        \"date_created\": \"2021-11-30T15:01:26+00:00\"\n    }\n    annotations_json[\"info\"].append(info)\n    \n    lic = {\n            \"id\": 1,\n            \"url\": \"\",\n            \"name\": \"Unknown\"\n        }\n    annotations_json[\"licenses\"].append(lic)\n\n    classes = {\"id\": 0, \"name\": \"starfish\", \"supercategory\": \"none\"}\n\n    annotations_json[\"categories\"].append(classes)\n\n    \n    for ann_row in df.itertuples():\n            \n        images = {\n            \"id\": ann_row[0],\n            \"license\": 1,\n            \"file_name\": ann_row.image_id + '.jpg',\n            \"height\": ann_row.height,\n            \"width\": ann_row.width,\n            \"date_captured\": \"2021-11-30T15:01:26+00:00\"\n        }\n        \n        annotations_json[\"images\"].append(images)\n        \n        bbox_list = ann_row.bboxes\n        \n        for bbox in bbox_list:\n            b_width = bbox[2]\n            b_height = bbox[3]\n            \n            # some boxes in COTS are outside the image height and width\n            if (bbox[0] + bbox[2] > 1280):\n                b_width = 1280 - bbox[0] \n            if (bbox[1] + bbox[3] > 720):\n                b_height = 720 - bbox[1] \n                \n            image_annotations = {\n                \"id\": annotion_id,\n                \"image_id\": ann_row[0],\n                \"category_id\": 0,\n                \"bbox\": [bbox[0], bbox[1], b_width, b_height],\n                \"area\": bbox[2] * bbox[3],\n                \"segmentation\": [],\n                \"iscrowd\": 0\n            }\n            \n            annotion_id += 1\n            annotations_json[\"annotations\"].append(image_annotations)\n        \n        \n    print(f\"Dataset COTS annotation to COCO json format completed! Files: {len(df)}\")\n    return annotations_json","acf92997":"\n# Convert COTS dataset to JSON COCO\ntrain_annot_json = dataset2coco(df_train[df_train.fold != Selected_Fold], f\"{Work_Dir}{DataSet_Path}\/train2017\/\")\nval_annot_json = dataset2coco(df_train[df_train.fold == Selected_Fold], f\"{Work_Dir}{DataSet_Path}\/val2017\/\")\n\n# Save converted annotations\nsave_annot_json(train_annot_json, f\"{Work_Dir}{DataSet_Path}\/annotations\/train.json\")\nsave_annot_json(val_annot_json, f\"{Work_Dir}{DataSet_Path}\/annotations\/valid.json\")\n","045ccb95":"def draw_yolox_predictions(img, bboxes, scores, bbclasses, confthre, classes_dict):\n    alpha=0.5\n    for i in range(len(bboxes)):\n            box = bboxes[i]\n            cls_id = int(bbclasses[i])\n            score = scores[i]\n            if score < confthre:\n                continue\n            x0 = int(box[0])\n            y0 = int(box[1])\n            x1 =int(box[0])+ int(box[2])\n            y1 = int(box[1])+ int(box[3])\n\n            cv2.rectangle(img, (x0, y0), (x1, y1), (255, 0, 255), 1)\n            cv2.putText(img, '{}:{:.1f}%'.format(classes_dict[cls_id], score * 100), (x0, y0 - 1), cv2.FONT_HERSHEY_SIMPLEX, 0.7,(255,0,255), thickness = 2)\n         \n    return img","37f5ec81":"COCO_CLASSES = (\n  \"starfish\",\n)\n\nscores =[]\nconfthre=0.5\nbbclasses=[]\nbboxes=[]\n\nid=1270\n\nimageid=train_annot_json[\"images\"][id][\"id\"]\nfile_name=train_annot_json[\"images\"][id][\"file_name\"]\nheight=train_annot_json[\"images\"][id][\"height\"]\nwidth=train_annot_json[\"images\"][id][\"width\"]\n#img=load_image(\"\/kaggle\/working\/dataset\/images\/train_cot\/\"+file_name)\nimg=load_image(f'{Work_Dir}{DataSet_Path}\/train2017\/{file_name}')\n\n\nfor i in train_annot_json[\"annotations\"]:\n    if i[\"image_id\"]==imageid :\n        bboxes.append(i[\"bbox\"])\n        scores.append(1)\n        bbclasses.append(0)\n        \nout_image = draw_yolox_predictions(img, bboxes, scores, bbclasses, confthre, COCO_CLASSES)\ndisplay(Image.fromarray(out_image))","7267d7a9":"config_file_template = '''\n\n#!\/usr\/bin\/env python3\n# -*- coding:utf-8 -*-\n# Copyright (c) Megvii, Inc. and its affiliates.\n\nimport os\n\nfrom yolox.exp import Exp as MyExp\n\n\nclass Exp(MyExp):\n    def __init__(self):\n        super(Exp, self).__init__()\n        self.depth = 1.33\n        self.width = 1.25\n        self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(\".\")[0]\n        \n        # Define yourself dataset path\n        self.data_dir = \"\/kaggle\/working\/dataset\/images\"\n        self.train_ann = \"train.json\"\n        self.val_ann = \"valid.json\"\n\n        self.num_classes = 1\n\n        self.max_epoch = $max_epoch\n        self.data_num_workers = 2\n        self.eval_interval = 1\n        \n        self.mosaic_prob = 1.0\n        self.mixup_prob = 1.0\n        self.hsv_prob = 1.0\n        self.flip_prob = 0.5\n        self.no_aug_epochs = 2\n        \n        self.input_size = (960, 960)\n        self.mosaic_scale = (0.5, 1.5)\n        self.random_size = (10, 20)\n        self.test_size = (960, 960)\n'''","16f9b8c5":"PIPELINE_CONFIG_PATH='cots_config.py'\npipeline = Template(config_file_template).substitute(max_epoch = 15)\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)","471a017e":"# .\/yolox\/data\/datasets\/voc_classes.py\n\nvoc_cls = '''\nVOC_CLASSES = (\n  \"starfish\",\n)\n'''\nwith open('.\/yolox\/data\/datasets\/voc_classes.py', 'w') as f:\n    f.write(voc_cls)\n\n# .\/yolox\/data\/datasets\/coco_classes.py\n\ncoco_cls = '''\nCOCO_CLASSES = (\n  \"starfish\",\n)\n'''\nwith open('.\/yolox\/data\/datasets\/coco_classes.py', 'w') as f:\n    f.write(coco_cls)\n\n# check if everything is ok    \n!more .\/yolox\/data\/datasets\/coco_classes.py","cf7ba562":"#%cp -r \/kaggle\/input\/yolox-models \/kaggle\/working\/\n\nsh = 'wget https:\/\/github.com\/Megvii-BaseDetection\/storage\/releases\/download\/0.0.1\/yolox_x.pth'\nMODEL_FILE = 'yolox_x.pth'\n\n#MODEL_FILE ='\/kaggle\/input\/yolox-models\/best_v3_yolox_s.pth.pth'\n\nwith open('script.sh', 'w') as file:\n  file.write(sh)\n\n!bash script.sh","343263e6":"!cp .\/tools\/train.py .\/","291788cf":"print(os.getcwd())","c285b69a":"!python train.py \\\n    -f cots_config.py \\\n    -d 1 \\\n    -b 4 \\\n    --fp16 \\\n    -o \\\n    -c {MODEL_FILE}","81f4f17f":"path = \"\/kaggle\/working\/dataset\"\nshutil.rmtree(path)","70442638":"#  \u2b07\ufe0f Download YOLOX","62014615":"# \ud83c\udf5c Creating COCO Annitation","955815dd":"# BBoxes\n##### \ud83d\udccc Note \n> We can see there are many images without any BBox. ","a2c48f0d":"![03.JPG](attachment:cf3eaaf7-7cc1-4d13-9e31-56be270c1e09.JPG)","e5f517ba":"# Path of Images","6f022caf":"![download.jpg](attachment:07de9c65-7c16-40e7-a821-d5354296394c.jpg)","cc1c0db5":"![01.JPG](attachment:50a58e3e-1971-4495-87f7-182916019b52.JPG)","3888d60f":"### Light Models.","66b00cea":"### Please if this kernel is useful, <font color='red'>please upvote !!<\/font>","85b6ca46":"1. https:\/\/www.kaggle.com\/remekkinas\/yolox-training-pipeline-cots-dataset-lb-0-507\n1. https:\/\/github.com\/Megvii-BaseDetection\/YOLOX\n","99a05360":"# \u2600\ufe0f Importing Libraries","a287ba7a":"![logo.png](attachment:655ae146-1300-4143-bd34-9d011cacf766.png)","d5fe2788":"# Training The Model","308e21b2":"# \ud83c\udf5a Splitting Dataset","34066042":"> We have just 4919 images with 11898 BBoxs, we will use them in training.","0e7fbb12":"# \ud83d\ude80 Barrier Reef YOLOX [Training]","92ec91ae":"# \ud83d\udd28 Functions","baaec536":" We will select YOLOX-L Model","7b8ba2e1":"#### I will train the model for 30 Epochs.","78fabdcb":"## \ud83c\udf5a Organize Directories\n\nI organized train and val images according to the example below.\n\n```\n\/Kaggle\/working\n    \/dataset\n         \/images\n             \/train2017\n             \/val2017\n             \/annotations\n     \/YOLOX\n \n    \n```","5b05e137":"# YOLOX-L Experiment Configuration File \nTraining parameters could be set up in experiment config files.","e02deb99":"# References","cf377c1a":"# Size of Images\n##### \ud83d\udccc Note \n> All images have Width=1280 & Height=720 ","345b9dad":"# Getting Weights\nWe selected  YOLOLX_x","6df30827":"# Select a Model\n### Standard Models.\n","990b1f05":"# Check Torch & CUDA ","f9a18809":"# \ud83c\udf6e Loading Data","0a3e84c1":"YOLOX is an anchor-free version of YOLO, with a simpler design but better performance! It aims to bridge the gap between research and industrial communities.","14002b49":"![02.JPG](attachment:6b2ec2bf-b9df-467e-9ce4-9e301d3edaa6.JPG)","8c7ab3ca":"### Hi kagglers, This is `Training` notebook using `YOLOX`.\n\n\n### Other notebooks in the competition\n- [Barrier Reef YOLOX [Inference]](https:\/\/www.kaggle.com\/ammarnassanalhajali\/barrier-reef-yolox-inference\/edit)\n\n\n\n### Please if this kernel is useful, <font color='red'>please upvote !!<\/font>","bbd91d0d":"# \ud83c\udf08 Visualizing BBoxes"}}