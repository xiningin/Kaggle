{"cell_type":{"d02da559":"code","9a904bea":"code","2c1a3e43":"code","6338e3fc":"code","bb6b25d1":"code","bbee8117":"code","f970353f":"code","a3bba7b2":"code","75ef45b7":"code","5aef75e3":"code","b92dff75":"code","d32007b9":"code","b349e932":"code","7939c4a1":"code","0774f873":"code","ea81f518":"code","218d4140":"code","fa2922dd":"code","f5a4be7f":"code","5f8753d0":"code","27770ce2":"code","8bec4333":"markdown","31a11da2":"markdown","4109db78":"markdown","7a4f6e9d":"markdown","0b4cdea4":"markdown","58c74307":"markdown","4551e42f":"markdown","264a64ee":"markdown","6adaf7b4":"markdown","24c897fc":"markdown","440348cf":"markdown","a9e78942":"markdown","694e5815":"markdown","5313b39e":"markdown","2ab4c21b":"markdown","fb352bc2":"markdown"},"source":{"d02da559":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a904bea":"data = pd.read_csv('..\/input\/google-play-store-apps\/googleplaystore_user_reviews.csv',encoding = 'latin1')\ndata.head(10)","2c1a3e43":"# here we go! we need the drop useless columns\nreview = pd.concat([data.Translated_Review,data.Sentiment], axis = 1)\nreview.dropna(axis = 0,inplace = True)\nreview.head(5)","6338e3fc":"review.Sentiment.value_counts()","bb6b25d1":"plt.figure(figsize = (12,12))\nsns.barplot(x = data['Sentiment'].value_counts().index,\n           y = data['Sentiment'].value_counts().values)\nplt.xlabel('Sentiments')\nplt.ylabel('Frequency')\nplt.title('Sentiment Bar Plot')\nplt.show()","bbee8117":"labels = review['Sentiment'].value_counts(sort = True).index\nsizes = review['Sentiment'].value_counts(sort = True)\n\ncolors = [\"blue\",\"orange\",\"green\"]\nexplode = (0,1,0)\n#plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n       autopct='%1.1f%%',shadow=True, startangle=270,)\nplt.title('Reviews',size = 20)\nplt.show()","f970353f":"review.Sentiment = [0 if i == 'Negative' else 1 if i == 'Positive' else 2 for i in review.Sentiment]\nreview.head(20)","a3bba7b2":"#Cleaning Data\nimport re\ntext = review.Translated_Review[12]\nrev = re.sub(\"[^a-zA-Z]\", \" \",text)\nrev = rev.lower()\nprint(review.Translated_Review[12])\nprint(rev)","75ef45b7":"#Stopwords\nimport nltk\n#nltk.download(\"stop words\") if this package is not installed\nfrom nltk.corpus import stopwords\nrev = nltk.word_tokenize(rev)","5aef75e3":"rev","b92dff75":"#Lemmatization\nimport nltk as nlp\nlemma = nlp.WordNetLemmatizer()\nrev = [lemma.lemmatize(word) for word in rev]\nrev = \" \".join(rev) # Let's take a look together\nrev","d32007b9":"review_list = [] \nfor i in review.Translated_Review:\n    rev=re.sub(\"[^a-zA-Z]\",\" \", i)\n    rev=rev.lower()\n    rev=nltk.word_tokenize(rev)\n    lemma=nlp.WordNetLemmatizer()\n    rev=[lemma.lemmatize(word) for word in rev]\n    rev=\" \".join(rev)\n    review_list.append(rev)","b349e932":" review_list[:5]","7939c4a1":"#Bag Of Words\nfrom sklearn.feature_extraction.text import CountVectorizer\nmax_features = 300000\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words=\"english\")\nsparce_matrix = count_vectorizer.fit_transform(review_list).toarray()\nallw = count_vectorizer.get_feature_names()\nprint(\"Top 100 used words: \",allw[:100])","0774f873":"from wordcloud import WordCloud\nplt.subplots(figsize=(12,12))\ncloud = WordCloud(background_color=\"white\",width = 1024,height = 768).generate(\" \".join(allw[:100]))\nplt.imshow(cloud)\nplt.axis(\"off\")\nplt.show()","ea81f518":"y = review.iloc[:,1].values\nx= sparce_matrix\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)","218d4140":"#Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train,y_train)\ny_pred = nb.predict(X_test)\nprint(\"accuracy:\",nb.score(y_pred.reshape(-1,1),y_test))","fa2922dd":"#LGBM\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV , StratifiedKFold\nlgb_model=LGBMClassifier().fit(X_train,y_train)\nsc_fold=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\nnames=lgb_model.__class__.__name__\naccuracy=cross_val_score(lgb_model,X_train,y_train,cv=sc_fold)\nprint(\"{}s score:{}\".format(names,accuracy.mean()))","f5a4be7f":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 10,random_state = 42)\nrf.fit(X_train,y_train)\nprint(\"Accuracy\",rf.score(X_test,y_test))","5f8753d0":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\nprint(\"Logistic Regression Accuracy is\",lr.score(X_test,y_test))","27770ce2":"y_predict=lr.predict(X_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nnames=[\"Positive\",\"Negative\",\"Neutral\"]\nconm=confusion_matrix(y_true,y_pred)\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(conm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_predict\")\nplt.ylabel(\"y_true\")\nax.set_xticklabels(names)\nax.set_yticklabels(names)\nplt.show()","8bec4333":"# Create the model","31a11da2":"# What is Natural Language Processing (NLP)?\nNatural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\nIn fact, a typical interaction between humans and machines using Natural Language Processing could go as follows:\n* A human talks to the machine\n* The machine captures the audio\n* Audio to text conversion takes place\n* Processing of the text\u2019s data\n* Data to audio conversion takes place\n* The machine responds to the human by playing the audio file\n\n\n# What is NLP used for?\nNatural Language Processing is the driving force behind the following common applications:\n* Language translation applications such as Google Translate\n* Word Processors such as Microsoft Word and Grammarly that employ NLP to check grammatical accuracy of texts.\n* Interactive Voice Response (IVR) applications used in call centers to respond to certain users\u2019 requests.\n* Personal assistant applications such as OK Google, Siri, Cortana, and Alexa.\n\n\n# Why is NLP difficult?\nNatural Language processing is considered a difficult problem in computer science. It\u2019s the nature of the human language that makes NLP difficult.\nThe rules that dictate the passing of information using natural languages are not easy for computers to understand.\nSome of these rules can be high-leveled and abstract; for example, when someone uses a sarcastic remark to pass information.\nOn the other hand, some of these rules can be low-levelled; for example, using the character \u201cs\u201d to signify the plurality of items.\nComprehensively understanding the human language requires understanding both the words and how the concepts are connected to deliver the intended message.\nWhile humans can easily master a language, the ambiguity and imprecise characteristics of the natural languages are what make NLP difficult for machines to implement.\n\n# How does Natural Language Processing Works?\nNLP entails applying algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\nWhen the text has been provided, the computer will utilize algorithms to extract meaning associated with every sentence and collect the essential data from them.\nSometimes, the computer may fail to understand the meaning of a sentence well, leading to obscure results.\nFor example, a humorous incident occurred in the 1950s during the translation of some words between the English and the Russian languages.\nHere is the biblical sentence that required translation:\n\u201cThe spirit is willing, but the flesh is weak.\u201d\nHere is the result when the sentence was translated to Russian and back to English:\n\u201cThe vodka is good, but the meat is rotten.\u201d\n\n# What are the techniques used in NLP?\n\nSyntactic analysis and semantic analysis are the main techniques used to complete Natural Language Processing tasks.\nHere is a description on how they can be used.\n# 1.Syntax\nSyntax refers to the arrangement of words in a sentence such that they make grammatical sense.\nIn NLP, syntactic analysis is used to assess how the natural language aligns with the grammatical rules.\nComputer algorithms are used to apply grammatical rules to a group of words and derive meaning from them.\nHere are some syntax techniques that can be used:\n\n* Lemmatization: It entails reducing the various inflected forms of a word into a single form for easy analysis.\n* Morphological segmentation: It involves dividing words into individual units called morphemes.\n* Word segmentation: It involves dividing a large piece of continuous text into distinct units.\n* Part-of-speech tagging: It involves identifying the part of speech for every word.\n* Parsing: It involves undertaking grammatical analysis for the provided sentence.\n* Sentence breaking: It involves placing sentence boundaries on a large piece of text.\n* Stemming: It involves cutting the inflected words to their root form.\n# 2.Semantics\nemantics refers to the meaning that is conveyed by a text. Semantic analysis is one of the difficult aspects of Natural Language Processing that has not been fully resolved yet.\nIt involves applying computer algorithms to understand the meaning and interpretation of words and how sentences are structured.\nHere are some techniques in semantic analysis:\n\n* Named entity recognition (NER): It involves determining the parts of a text that can be identified and categorized into preset groups. Examples of such groups include names of people and names of places.\n* Word sense disambiguation: It involves giving meaning to a word based on the context.\n* Natural language generation: It involves using databases to derive semantic intentions and convert them into human language.","4109db78":"very interesting words are used","7a4f6e9d":"# NLP Pipeline\n![](https:\/\/miro.medium.com\/max\/1000\/1*zHLs87sp8R61ehUoXepWHA.png)\n\n\n# Building an NLP Pipeline, Step-by-Step\n\n# Step 1: Sentence Segmentation\nThe first step in the pipeline is to break the text apart into separate sentences. That gives us this:\n\u201cLondon is the capital and most populous city of England and the United Kingdom.\u201d\n\u201cStanding on the River Thames in the south east of the island of Great Britain, London has been a major settlement for two millennia.\u201d\n\u201cIt was founded by the Romans, who named it Londinium.\u201d\nWe can assume that each sentence in English is a separate thought or idea. It will be a lot easier to write a program to understand a single sentence than to understand a whole paragraph.\nCoding a Sentence Segmentation model can be as simple as splitting apart sentences whenever you see a punctuation mark. But modern NLP pipelines often use more complex techniques that work even when a document isn\u2019t formatted cleanly.\n\n\n# Step 2: Word Tokenization\nNow that we\u2019ve split our document into sentences, we can process them one at a time. Let\u2019s start with the first sentence from our document:\n\u201cLondon is the capital and most populous city of England and the United Kingdom.\u201d\nThe next step in our pipeline is to break this sentence into separate words or tokens. This is called tokenization. This is the result:\n\u201cLondon\u201d, \u201cis\u201d, \u201c the\u201d, \u201ccapital\u201d, \u201cand\u201d, \u201cmost\u201d, \u201cpopulous\u201d, \u201ccity\u201d, \u201cof\u201d, \u201cEngland\u201d, \u201cand\u201d, \u201cthe\u201d, \u201cUnited\u201d, \u201cKingdom\u201d, \u201c.\u201d\nTokenization is easy to do in English. We\u2019ll just split apart words whenever there\u2019s a space between them. And we\u2019ll also treat punctuation marks as separate tokens since punctuation also has meaning.\n\nAnd we have 4 tokenization types;\n# Word Tokenizer:\nSeparates and separates the punctuation marks, together with the \"apostroph s\", which indicates possessive in English.\n![](https:\/\/miro.medium.com\/max\/700\/1*ihw7yPEndoWIbP6y9TzPJA.png)\n![](https:\/\/miro.medium.com\/max\/485\/1*lTbH3UlmPxSpUZZmOHoXUA.png)\n\n\n# Sentence Tokenizer:\nSplitting a paragraph into sentences\n![](https:\/\/miro.medium.com\/max\/700\/1*LAvjrwBdCPf99TuGSMFoMg.png)\n\n\n# Treebank Word Tokenizer:\nSeparates words in sentences according to spaces and punctuation marks.\n![](https:\/\/miro.medium.com\/max\/700\/1*0FtOgNKHxtn7YLBIgehQ8g.png)\n\n\n# WordPunctTokenizer:\nExtracts the punctuation marks in the sentence.\n![](https:\/\/miro.medium.com\/max\/700\/1*JS16faH84_rpq_sevO19vw.png)\n\n\n# Step 3: Predicting Parts of Speech for Each Token\nNext, we\u2019ll look at each token and try to guess its part of speech \u2014 whether it is a noun, a verb, an adjective and so on. Knowing the role of each word in the sentence will help us start to figure out what the sentence is talking about.\nWe can do this by feeding each word (and some extra words around it for context) into a pre-trained part-of-speech classification model:\n![](https:\/\/miro.medium.com\/max\/700\/1*u7Z1B1TIYe68V8lS2f8GNg.png)\nthe part-of-speech model was originally trained by feeding it millions of English sentences with each word\u2019s part of speech already tagged and having it learn to replicate that behavior.\nKeep in mind that the model is completely based on statistics \u2014 it doesn\u2019t actually understand what the words mean in the same way that humans do. It just knows how to guess a part of speech based on similar sentences and words it has seen before.\nAfter processing the whole sentence, we\u2019ll have a result like this:\n![](https:\/\/miro.medium.com\/max\/1000\/1*O0gIbvPd-weZw4IGmA5ywQ.png)\nWith this information, we can already start to glean some very basic meaning. For example, we can see that the nouns in the sentence include \u201cLondon\u201d and \u201ccapital\u201d, so the sentence is probably talking about London.\n\n\n# Step 4: Text Lemmatization\nIn English (and most languages), words appear in different forms. Look at these two sentences:\nI had a pony.\nI had two ponies.\nBoth sentences talk about the noun pony, but they are using different inflections. When working with text in a computer, it is helpful to know the base form of each word so that you know that both sentences are talking about the same concept. Otherwise the strings \u201cpony\u201d and \u201cponies\u201d look like two totally different words to a computer.\nIn NLP, we call finding this process lemmatization \u2014 figuring out the most basic form or lemma of each word in the sentence.\nThe same thing applies to verbs. We can also lemmatize verbs by finding their root, unconjugated form. So \u201cI had two ponies\u201d becomes \u201cI [have] two [pony].\u201d\nLemmatization is typically done by having a look-up table of the lemma forms of words based on their part of speech and possibly having some custom rules to handle words that you\u2019ve never seen before.\n![](https:\/\/miro.medium.com\/max\/1000\/1*EgYJsyjBNk074TQf87_CqA.png)\nThe only change we made was turning \u201cis\u201d into \u201cbe\u201d.\n![](https:\/\/miro.medium.com\/max\/700\/1*L2dWgKERU_6YtPdHRSXqfA.jpeg)\n\n\n# Step 5: Identifying Stop Words\nNext, we want to consider the importance of a each word in the sentence. English has a lot of filler words that appear very frequently like \u201cand\u201d, \u201cthe\u201d, and \u201ca\u201d. When doing statistics on text, these words introduce a lot of noise since they appear way more frequently than other words. Some NLP pipelines will flag them as stop words \u2014that is, words that you might want to filter out before doing any statistical analysis.\nStop words are usually identified by just by checking a hardcoded list of known stop words. But there\u2019s no standard list of stop words that is appropriate for all applications. The list of words to ignore can vary depending on your application.\nFor example if you are building a rock band search engine, you want to make sure you don\u2019t ignore the word \u201cThe\u201d. Because not only does the word \u201cThe\u201d appear in a lot of band names, there\u2019s a famous 1980\u2019s rock band called The The!\n\n\n# Step 6: Dependency Parsing\nThe next step is to figure out how all the words in our sentence relate to each other. This is called dependency parsing.\nThe goal is to build a tree that assigns a single parent word to each word in the sentence. The root of the tree will be the main verb in the sentence.\nBut we can go one step further. In addition to identifying the parent word of each word\n![](https:\/\/miro.medium.com\/max\/700\/1*nteaQRxNNSXMlAnT31iXjw.png)\nBut we can go one step further. In addition to identifying the parent word of each word,we can also predict the type of relationship that exists between those two words:\n![](https:\/\/miro.medium.com\/max\/700\/1*onc_4Mnq2L7cetMAowYAbA.png)\nThis parse tree shows us that the subject of the sentence is the noun \u201cLondon\u201d and it has a \u201cbe\u201d relationship with \u201ccapital\u201d. We finally know something useful \u2014 London is a capital! And if we followed the complete parse tree for the sentence (beyond what is shown), we would even found out that London is the capital of the United Kingdom.\nJust like how we predicted parts of speech earlier using a machine learning model, dependency parsing also works by feeding words into a machine learning model and outputting a result. But parsing word dependencies is particularly complex task and would require an entire article to explain in any detail. If you are curious how it works, a great place to start reading is Matthew Honnibal\u2019s excellent article \u201cParsing English in 500 Lines of Python\u201d.\nBut despite a note from the author in 2015 saying that this approach is now standard, it\u2019s actually out of date and not even used by the author anymore. In 2016, Google released a new dependency parser called Parsey McParseface which outperformed previous benchmarks using a new deep learning approach which quickly spread throughout the industry. Then a year later, they released an even newer model called ParseySaurus which improved things further. In other words, parsing techniques are still an active area of research and constantly changing and improving.\nIt\u2019s also important to remember that many English sentences are ambiguous and just really hard to parse. In those cases, the model will make a guess based on what parsed version of the sentence seems most likely but it\u2019s not perfect and sometimes the model will be embarrassingly wrong. But over time our NLP models will continue to get better at parsing text in a sensible way.\nWant to try out dependency parsing on your own sentence? There\u2019s a great interactive demo from the spaCy team here.\n\n\n# Step 6b:Finding Noun Phrases\nSo far, we\u2019ve treated every word in our sentence as a separate entity. But sometimes it makes more sense to group together the words that represent a single idea or thing. We can use the information from the dependency parse tree to automatically group together words that are all talking about the same thing.\nFor example, instead of this:\n![](https:\/\/miro.medium.com\/max\/700\/1*EgYJsyjBNk074TQf87_CqA.png)\nWhether or not we do this step depends on our end goal. But it\u2019s often a quick and easy way to simplify the sentence if we don\u2019t need extra detail about which words are adjectives and instead care more about extracting complete ideas.\nStep 7: Named Entity Recognition (NER)\nNow that we\u2019ve done all that hard work, we can finally move beyond grade-school grammar and start actually extracting ideas.\nIn our sentence, we have the following nouns:\n![](https:\/\/miro.medium.com\/max\/1000\/1*JMXGOrdx4oQsfZC5t-Ksgw.png)\nSome of these nouns present real things in the world. For example, \u201cLondon\u201d, \u201cEngland\u201d and \u201cUnited Kingdom\u201d represent physical places on a map. It would be nice to be able to detect that! With that information, we could automatically extract a list of real-world places mentioned in a document using NLP.\nThe goal of Named Entity Recognition, or NER, is to detect and label these nouns with the real-world concepts that they represent. Here\u2019s what our sentence looks like after running each token through our NER tagging model:\n![](https:\/\/miro.medium.com\/max\/1000\/1*x1kwwACli8Fcvjos_6oS-A.png)\nBut NER systems aren\u2019t just doing a simple dictionary lookup. Instead, they are using the context of how a word appears in the sentence and a statistical model to guess which type of noun a word represents. A good NER system can tell the difference between \u201cBrooklyn Decker\u201d the person and the place \u201cBrooklyn\u201d using context clues.\n\nHere are just some of the kinds of objects that a typical NER system can tag:\n* People\u2019s names\n* Company names\n* Geographic locations (Both physical and political)\n* Product names\n* Dates and times\n* Amounts of money\n* Names of events\n\n\n# Step 8: Coreference Resolution\nAt this point, we already have a useful representation of our sentence. We know the parts of speech for each word, how the words relate to each other and which words are talking about named entities.\nHowever, we still have one big problem. English is full of pronouns \u2014 words like he, she, and it. These are shortcuts that we use instead of writing out names over and over in each sentence. Humans can keep track of what these words represent based on context. But our NLP model doesn\u2019t know what pronouns mean because it only examines one sentence at a time.\nLet\u2019s look at the third sentence in our document:\n\u201cIt was founded by the Romans, who named it Londinium.\u201d\nIf we parse this with our NLP pipeline, we\u2019ll know that \u201cit\u201d was founded by Romans. But it\u2019s a lot more useful to know that \u201cLondon\u201d was founded by Romans.\nAs a human reading this sentence, you can easily figure out that \u201cit\u201d means \u201cLondon\u201d. The goal of coreference resolution is to figure out this same mapping by tracking pronouns across sentences. We want to figure out all the words that are referring to the same entity.\n\nThe great references Adam Geitgey on www.medium.com\nand Merve Noyan on www.medium.com ","0b4cdea4":"As we can see, let's make this technique that we apply on a single data more easily on all data.","58c74307":"And finally we have 90% accuracy rate, let's see how many correct and how many wrong guesses","4551e42f":"# Bag Of Words\nThe word bag model is a simplifying representation used in natural language processing and information retrieval. In this model, a text (a text such as a sentence or document) is represented as a bag (plurality) of words, while keeping polygyny, grammar and even word order are ignored. The word bag model has also been used in computer vision. [https:\/\/tr.wikipedia.org\/wiki\/Kelime_\u00e7antas\u0131_modeli]\n\nThe word bag model is widely used in document classification methods: the occurrence (frequency) of each word is used as a feature in training a classifier.\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/02\/BoWBag-of-Words-model-2.png)","264a64ee":"And we can use wordcloud for some visualization with top 100 used words","6adaf7b4":"# Let's see how Natural Language processing works in a single sentence.","24c897fc":"# Let's start extracting comments from play store using Natural language processing","440348cf":"As we see we have 37427 rewies with non-null\nand now we need to convert our values, which are strings, to integers.\n* 0 == Negative %22.1\n* 1 == Positive %64.1\n* 2 == Neutral %13.8","a9e78942":"This model is absolutely not suitable for our data.","694e5815":"# Libraries\n","5313b39e":"%88!We got a good score","2ab4c21b":"* Thank you for viewing my article, I am waiting for your comments\n* If you are wondering how machine learning algorithms work, you can check out my kernel on Heart Disease.\n* You can also check my Rating prediction kernel on this dataset.","fb352bc2":"# Machine Learning Methods"}}