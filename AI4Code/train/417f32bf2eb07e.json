{"cell_type":{"ed4937ce":"code","d513f160":"code","627d8b8c":"code","93e08b45":"code","1af64f41":"code","2f0167fa":"code","dafa562d":"code","711aa131":"code","b27aee94":"code","067bc6e3":"code","b48914e9":"code","8b02c696":"code","9453554b":"code","152e6ae4":"code","42f39f80":"code","f89dabeb":"code","a0289dcb":"code","65073412":"code","7b1f2adb":"code","60f5b2fd":"code","acdcc55d":"code","8cc1bc7e":"code","6b3d5da9":"code","7e53fcc1":"code","0cdecc27":"code","6f44ee06":"code","59991024":"code","10b3017a":"code","dda411fd":"code","3e0f1938":"code","b69d556d":"code","91a257f5":"code","04b9e569":"code","a76a4a5c":"code","48c56266":"code","94404412":"code","1f38f494":"code","58fa7b4f":"code","78c1a5fd":"code","2997487b":"code","fe67cfc2":"code","0d4a0dd6":"code","b38585af":"code","bed5cca4":"code","2473e988":"code","005eea9d":"code","e788fa31":"code","c7d02243":"code","908f3cc1":"markdown","0ed8ff8f":"markdown","33c5d395":"markdown","b8ec8f92":"markdown","74808343":"markdown","af5388c6":"markdown","8d8a79f5":"markdown","a4151daa":"markdown","88ad0db5":"markdown","a9688512":"markdown","99089c3e":"markdown","9e46cff2":"markdown","eca064ab":"markdown","ef0dc03d":"markdown","e5621696":"markdown","309ac16e":"markdown","4cd04a1d":"markdown","996c80b6":"markdown","c5e120f5":"markdown","2b77f273":"markdown","fd52a3f9":"markdown","a040a2ca":"markdown","db2f06e2":"markdown","03e33229":"markdown","d43ab9c1":"markdown","e426cdb0":"markdown","afaf396a":"markdown","2c61773f":"markdown","498584fe":"markdown","ed345931":"markdown"},"source":{"ed4937ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport yaml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d513f160":"path = \"..\/input\/mymusicalprefrences\/\" \ntrain = pd.read_csv(f\"{path}train.csv\")\ntest = pd.read_csv(f\"{path}test.csv\")\ndf = pd.concat([train,test]).reset_index(drop=True)\nprint(df.columns.tolist())\ndf","627d8b8c":"artists=[]\ndf.Artists = df.Artists.fillna(\"NA\")\nfor i in df.index:\n    artists.extend(df.loc[i, \"Artists\"].split(\"|\"))\nunique=np.unique(artists)\nprint(len(artists))\nprint(len(unique))","93e08b45":"rare=[]\ncommon=[]\nfor i in unique:\n    if(artists.count(i)<5):\n        rare.append(i)\n    else:\n        common.append(i)\nprint(\"rare Artist \",len(rare))\nprint(\"common Artists \",len(common))\n# check common artists fot includes only in test or only in train\nmask = ~df.Category.isna()\nartistiInTrain=[]\nfor i in df[mask].index:\n    artistiInTrain.extend(df.loc[mask].Artists[i].split(\"|\"))\nartistiInTest=[]\nfor i in df[~mask].index:\n    artistiInTest.extend(df.loc[~mask].Artists[i].split(\"|\"))\nonlyInTrain=[]\nonlyInTest=[]\nfor i in common:\n    if(artists.count(i)==artistiInTrain.count(i)):\n        onlyInTrain.append(i)\n    if(artists.count(i)==artistiInTest.count(i)):\n        onlyInTest.append(i)\n\nprint(\"common Artists only in tran set \",onlyInTrain)\nprint(\"common Artists only in test set \", onlyInTest)\n\nfor i in onlyInTrain:\n    common.remove(i)\n    rare.append(i)\ndf[\"Other artists\"]=0\nfor i in common:\n    df[i]=0\nfor j in df.index:\n    for i in df.loc[j,\"Artists\"].split('|'):\n        if  i in common:\n            df.loc[j,i]=1\n        else:\n            df.loc[j,\"Other artists\"]=1\ndf = df.drop(\"Artists\", axis=1)","1af64f41":"df = df.drop(\"Track\", axis=1)","2f0167fa":"versionlist=[]\nunicverison=[]\ndf.Version = df.Version.fillna(\"NA\")\nfor i in df.index:\n    versionlist.append(df.loc[i, \"Version\"])\nunicverison=np.unique(versionlist)\nfor i in unicverison:\n    print(i, versionlist.count(i)) ","dafa562d":"versionlist1=[]\nunicverison1=[]\nfor i in df[mask].index:\n    versionlist1.append(df[mask].loc[i, \"Version\"])\nunicverison1=np.unique(versionlist1)\nfor i in unicverison1:\n    \n    print(i, versionlist1.count(i),\"in test set\") ","711aa131":"for i in unicverison1:\n    s=df[mask][df[\"Version\"]==i]\n#    print (i,len(s[s[\"Category\"]==1]),len(s))\n    print(i,len(s[s[\"Category\"]==1])\/len(s))\n","b27aee94":"df[\"Remaster\"]=0\ndf[\"Remix\"]=0\nfor i in df.index:\n    if(df.loc[i,\"Version\"]==\"Remaster\"):\n        df.loc[i,\"Remaster\"]=1\n    if(df.loc[i,\"Version\"]==\"Remix\"):\n        df.loc[i,\"Remix\"]=1\ndf = df.drop(\"Version\", axis=1)\ndf","067bc6e3":"albumtype=[]\nunicalbumtype=[]\ndf.Album_type = df.Album_type.fillna(\"NA\")\nfor i in df.index:\n    albumtype.append(df.loc[i, \"Album_type\"])\nunicalbumtype=np.unique(albumtype)\nfor i in unicalbumtype:\n    print(i, albumtype.count(i))","b48914e9":"albumtype1=[]\nunicalbumtype1=[]\nfor i in df[mask].index:\n    albumtype1.append(df[mask].loc[i, \"Album_type\"])\nunicalbumtype1=np.unique(albumtype1)\nfor i in unicalbumtype1:\n    print(i, albumtype1.count(i),\"in test set\") ","8b02c696":"for i in unicalbumtype1:\n    s=df[mask][df[\"Album_type\"]==i]\n#    print (i,len(s[s[\"Category\"]==1]),len(s))\n    print(i,len(s[s[\"Category\"]==1])\/len(s))","9453554b":"df[\"Compilation\"]=0\ndf[\"Single\"]=0\nfor i in df.index:\n    if(df.loc[i,\"Album_type\"]==\"Compilation\"):\n        df.loc[i,\"compilation\"]=1\n    if(df.loc[i,\"Album_type\"]==\"single\"):\n        df.loc[i,\"Single\"]=1\ndf = df.drop(\"Album_type\", axis=1)\n","152e6ae4":"album=[]\nunicalbum=[]\nrarealbum=[]\ncommonalbum=[]\ndf.Album = df.Album.fillna(\"NA\")\nfor i in df.index:\n    album.append(df.loc[i, \"Album\"])\nunicalbum=np.unique(album)\nfor i in unicalbum:\n    if(album.count(i)<4):\n        rarealbum.append(i)\n    else:\n        commonalbum.append(i) \n\nlen(commonalbum)","42f39f80":"albumInTrain=[]\nfor i in df[mask].index:\n    albumInTrain.append(df.loc[mask].Album[i])\nalbumInTest=[]\nfor i in df[~mask].index:\n    albumInTest.append(df.loc[~mask].Album[i])\nalbumonlyInTrain=[]\nalbumonlyInTest=[]\nfor i in commonalbum:\n    if(album.count(i)==albumInTrain.count(i)):\n        albumonlyInTrain.append(i)\n    if(album.count(i)==albumInTest.count(i)):\n        albumonlyInTest.append(i)\nprint(\"common Album only in train set \",albumonlyInTrain)\nprint(\"common Album only in test set \", albumonlyInTest)\nfor i in albumonlyInTrain:\n    commonalbum.remove(i)\n    rarealbum.append(i)\n","f89dabeb":"for i in commonalbum:\n    s=df[mask][df[\"Album\"]==i]\n#    print (i,len(s[s[\"Category\"]==1]),len(s))\n    print(i,len(s[s[\"Category\"]==1])\/len(s))","a0289dcb":"bad=[]\nnormal=[]\ngood=[]\nfor i in commonalbum:\n    s=df[mask][df[\"Album\"]==i]\n    if len(s[s[\"Category\"]==1])\/len(s)<0.35:\n        bad.append(i)\n    elif len(s[s[\"Category\"]==1])\/len(s)>0.65:\n        good.append(i)\n    else:\n        normal.append(i)\n\ndf[\"Bad Album\"]=0\ndf[\"Normal Album\"]=0\ndf[\"Good Album\"]=0\ndf[\"Other Album\"]=0\nfor i in df.index:\n    if(df.loc[i,\"Album\"] in bad):\n        df.loc[i,\"Bad Album\"]=1\n    elif(df.loc[i,\"Album\"] in normal):\n        df.loc[i,\"Normal Album\"]=1\n    elif(df.loc[i,\"Album\"] in good):\n        df.loc[i,\"Good Album\"]=1\n    else:\n        df.loc[i,\"Other Album\"]=1\ndf = df.drop(\"Album\", axis=1)\ndf","65073412":"genres=[]\nunicgenres=[]\nraregenres=[]\ncommongenres=[]\ndf.Artists_Genres = df.Artists_Genres.fillna(\"NA\")\nfor i in df.index:\n    genres.extend(df.loc[i, \"Artists_Genres\"].split('|'))\nunicgenres=np.unique(genres)\nfor i in unicgenres:\n    if(genres.count(i)<4):\n        raregenres.append(i)\n    else:\n        commongenres.append(i)\nprint(\"In train and tetst set \",len(unicgenres),\" unique artists genres\")\nprint(\"In train and tetst set \",len(raregenres),\" rare artists genres which meet 3 or less times\")\nprint(\"In train and tetst set \",len(commongenres),\" common artists genres which meet 4 or more times\")","7b1f2adb":"generesInTrain=[]\nfor i in df[mask].index:\n    generesInTrain.extend(df.loc[mask].Artists_Genres[i].split('|'))\n    generesInTest=[]\nfor i in df[~mask].index:\n    generesInTest.extend(df.loc[~mask].Artists_Genres[i].split('|'))\ngeneresOnlyInTrain=[]\ngeneresOnlyInTest=[]\nfor i in commongenres:    \n    if(genres.count(i)==generesInTrain.count(i)):      \n        generesOnlyInTrain.append(i)\n    if(genres.count(i)==generesInTest.count(i)):\n        generesOnlyInTest.append(i)\nprint(\"common generes only in train set \",generesOnlyInTrain)\nprint(\"common generes only in test set \", generesOnlyInTest)\nfor i in generesOnlyInTrain:\n    commongenres.remove(i)\n    raregenres.append(i)\nlen(commongenres)","60f5b2fd":"df[\"Other generes\"]=0\nfor i in commongenres:\n    df[i]=0\nfor j in df.index:\n    for i in df.loc[j,\"Artists_Genres\"].split('|'):\n        if  i in commongenres:\n            df.loc[j,i]=1\n        else:\n            df.loc[j,\"Other generes\"]=1\ndf = df.drop(\"Artists_Genres\", axis=1)\ndf","acdcc55d":"labels=[]\nuniclabels=[]\nrarelabels=[]\ncommonlabels=[]\ndf.Labels = df.Labels.fillna(\"NA\")\nfor i in df.index:\n    labels.extend(df.loc[i, \"Labels\"].split('|'))\nuniclabels=np.unique(labels)","8cc1bc7e":"labelsInTrain=[]\nfor i in df[mask].index:\n    labelsInTrain.extend(df.loc[mask].Labels[i].split('|'))\n    labelsInTest=[]\nfor i in df[~mask].index:\n    labelsInTest.extend(df.loc[~mask].Labels[i].split('|'))\nlabelsOnlyInTrain=[]\nlabelsOnlyInTest=[]\nfor i in commonlabels:    \n    if(labels.count(i)==labelsInTrain.count(i)):      \n        labelsOnlyInTrain.append(i)\n    if(labels.count(i)==labelsInTest.count(i)):\n        labelsOnlyInTest.append(i)\nprint(\"common generes only in train set \",labelsOnlyInTrain)\nprint(\"common generes only in test set \", labelsOnlyInTest)\nfor i in labelsOnlyInTrain:\n    commonlabels.remove(i)\n    rarelabels.append(i)\nlen(commonlabels)\ndf[\"Other labels\"]=0\nfor i in commonlabels:\n    df[i]=0\nfor j in df.index:\n    for i in df.loc[j,\"Labels\"].split('|'):\n        if  i in commonlabels:\n            df.loc[j,i]=1\n        else:\n            df.loc[j,\"Other labels\"]=1\ndf = df.drop(\"Labels\", axis=1)\ndf","6b3d5da9":"df[\"Country\"] = df[\"Country\"].fillna(\"NA\")\ncountry=[]\nuniccountry=[]\nrarecountry=[]\ncommoncountry=[]\nfor i in df.index:\n    country.extend(df.Country[i].split('|'))\nuniccountry=np.unique(country)\nlen(uniccountry)\nfor i in uniccountry:\n    if(country.count(i)<5):\n        rarecountry.append(i)\n    else:\n        commoncountry.append(i)\nprint(\"In train and tetst set \",len(uniccountry),\" unique country genres\")\nprint(\"In train and tetst set \",len(rarecountry),\" rare country genres which meet 4 or less times\")\nprint(\"In train and tetst set \",len(commoncountry),\" common country genres which meet 5 or more times\")\nprint (\"common country are \", commoncountry)\n#delete country only in train or test set\ncountryInTrain=[]\ncountryInTest=[]\ncountryOnlyInTrain=[]\ncountryOnlyInTest=[]\nfor i in df[mask].index:\n    countryInTrain.extend(df.loc[mask].Country[i].split('|'))\nfor i in df[~mask].index:\n    countryInTest.extend(df.loc[~mask].Country[i].split('|'))\nfor i in uniccountry:\n    if(country.count(i)==countryOnlyInTrain.count(i)):      \n        countryOnlyInTrain.append(i)\n    if(country.count(i)==countryOnlyInTest.count(i)):\n        countryOnlyInTest.append(i)\nprint(\"common generes only in train set \",countryOnlyInTrain)\nprint(\"common generes only in test set \", countryOnlyInTest)","7e53fcc1":"df[\"Other countries\"]=0\nfor i in commoncountry:\n    df[i]=0\nfor j in df.index:\n    for i in df.loc[j,\"Country\"].split('|'):\n        if  i in commoncountry:\n            df.loc[j,i]=1\n        else:\n            df.loc[j,\"Other countries\"]=1\ndf = df.drop(\"Country\", axis=1)\ndf","0cdecc27":"df.rename(columns = {'Vocal ' : 'Vocal'}, inplace = True)\ndf[\"Vocal\"] = df[\"Vocal\"].fillna(\"NA\")\nvocal=[]\nunicvocal=[]\nfor i in df.index:\n    vocal.extend(df.Vocal[i].split('|'))\nunicvocal= np.unique(vocal) \nvocal.count(\"N\")+vocal.count(\"F\")+vocal.count(\"M\")==len(vocal)\n#there is no na values\nfor i in unicvocal:\n    df[i]=0\nfor j in df.index:\n    for i in df.loc[j,\"Vocal\"].split('|'):\n        if  i in unicvocal:\n            df.loc[j,i]=1\ndf = df.drop(\"Vocal\", axis=1)\ndf","6f44ee06":"df[\"Key\"] = df[\"Key\"].fillna(\"NA\")\nprint(\"number of empty elements\",len(df[df[\"Key\"]==\"NA\"]))\ndf[\"isMajor\"], df[\"Key\"] = df[\"Key\"].apply(lambda x: x.split(\" \")[1]), df[\"Key\"].apply(lambda x: x.split(\" \")[0])\ndf.loc[:,\"isMajor\"] = df[\"isMajor\"].replace({\"Major\": \"1\", \"Minor\": \"0\"})","59991024":"print(np.unique(df.Key))\n#check % likely for each key\nunickey=np.unique(df.Key)\nfor i in unickey:\n    s=df[mask][df[\"Key\"]==i]\n#    print (i,len(s[s[\"Category\"]==1]),len(s))\n    print(i,round(len(s[s[\"Category\"]==1])\/len(s),2))","10b3017a":"for i in unickey:\n    df[i]=0\nfor j in df.index:\n    if  df.loc[j,\"Key\"] in unickey:\n        df.loc[j,df.loc[j,\"Key\"]]=1\ndf = df.drop(\"Key\", axis=1)\ndf","dda411fd":"df[\"Release_year\"] = df[\"Release_year\"].fillna(\"NA\")\nprint(\"number of empty elements\",len(df[df[\"Release_year\"]==\"NA\"]))    ","3e0f1938":"year=[]\nunicyear=[]\nfor i in df.index:\n    year.append(df.Release_year[i])\nunicyear=np.unique(year)\nunicyear\nfor i in unicyear:\n    print (\"number of track in \", i, year.count(i))\n    if year.count(i)<10:\n        unicyear = np.delete(unicyear,np.where(unicyear == i))","b69d556d":"for i in unicyear:\n    s=df[mask][df[\"Release_year\"]==i]\n    print(i,round(len(s[s[\"Category\"]==1])\/len(s),2))","91a257f5":"old=[]\nclassic=[]\nnew=[]\nsupernew=[]\ndf[\"old\"]=0\ndf[\"classic\"]=(df.loc[:,\"Release_year\"]<2000).astype(int)\ndf[\"new\"]=0\ndf[\"supernew\"]=(df.loc[:,\"Release_year\"]>=2020).astype(int)\n\nfor i in df.index:\n    if df.loc[i,\"Release_year\"] in range(2000,2010) :\n        df.loc[i,\"old\"]=1\n    if df.loc[i,\"Release_year\"] in range(2010,2020) :\n        df.loc[i,\"new\"]=1\ndf","04b9e569":"df[\"Duration\"] = df[\"Duration\"].fillna(\"NA\")\nprint(\"number of empty elements in Duration\",len(df[df[\"Duration\"]==\"NA\"]))    \n\ndf[\"BPM\"] = df[\"BPM\"].fillna(\"NA\")\nprint(\"number of empty elements in BPM\",len(df[df[\"BPM\"]==\"NA\"])) \n\ndf[\"Energy\"] = df[\"Energy\"].fillna(\"NA\")\nprint(\"number of empty elements in Energy\",len(df[df[\"Energy\"]==\"NA\"])) \n\ndf[\"Dancebility\"] = df[\"Dancebility\"].fillna(\"NA\")\nprint(\"number of empty elements in Dancebility\",len(df[df[\"Dancebility\"]==\"NA\"])) \n\ndf[\"Happiness\"] = df[\"Happiness\"].fillna(\"NA\")\nprint(\"number of empty elements in Happiness\",len(df[df[\"Happiness\"]==\"NA\"])) ","a76a4a5c":"df[df[\"Energy\"]==\"NA\"]","48c56266":"df[df[\"Dancebility\"]==\"NA\"]","94404412":"df[df[\"Happiness\"]==\"NA\"]","1f38f494":"df = df.drop(index=661)","58fa7b4f":"train, test=df[mask],df[~mask]\ntest = test.drop(\"Category\", axis=1)\ntrain.loc[:,\"Category\"] = train.loc[:,\"Category\"].astype(bool)\nx_train, y_train = train.drop(\"Category\", axis=1), train[\"Category\"].astype(int)","78c1a5fd":"model = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=1)\n#model.fit(x_train,y_train)\n#predictions = model.predict(test)\n#sample = pd.read_csv(f\"{path}sample_submition.csv\")\n#sample[\"Category\"] = predictions\n#sample.to_csv(\"deploy.csv\", index=False)\n#sample","2997487b":"def functionCheck (model, data):\n    train_acc=0.0\n    for i in range(3):\n        chunkSize=len(data)\/\/3   \n        mask2 = (data.index<((i+1)*chunkSize))\n        mask3 = (i*chunkSize<data.index)\n        mask4= mask2 == mask3\n        train, test=data[~mask4],data[mask4]\n        this_train_acc=0\n        accCheck=test.Category\n        test = test.drop(\"Category\", axis=1)\n        train.loc[:,\"Category\"] = train.loc[:,\"Category\"].astype(bool)\n        x_train, y_train = train.drop(\"Category\", axis=1), train[\"Category\"].astype(int)\n        model.fit(x_train,y_train)\n        predictions = model.predict(test)\n        this_train_acc=accuracy_score(accCheck,predictions)  \n        train_acc = train_acc+this_train_acc\n    return train_acc\/3","fe67cfc2":"#pd.options.mode.chained_assignment = None  \nrandomForestAccuracy=functionCheck(model,train)\nrandomForestAccuracy","0d4a0dd6":"#maxEst=0\n#maxDepth=0\n#maxAccuracy=0;\n#for i in range(1,11):\n#    for j in range(1,11):\n#        model = RandomForestClassifier(n_estimators=200*i, max_depth=j, random_state=1)\n#        accuracy=functionCheck(model,train)\n#        if(maxAccuracy<accuracy):\n#            maxAccuracy=accuracy\n#            maxEst=200*i\n#            maxDepth=j\n#        print(\"for n_estimators=\",200*i,\"max_depth=\",j,\"accuracy on train set is \",accuracy)\n#print(\"best accuracy with n_estimators=\",maxEst,\"max_depth=\",maxDepth,\"accuracy is \",maxAccuracy)","b38585af":"modelforSubmit = RandomForestClassifier(n_estimators=1200, max_depth=7, random_state=1)\n#modelforSubmit.fit(x_train,y_train)\n#new_predictions = modelforSubmit.predict(test)\n#newsample = pd.read_csv(f\"{path}sample_submition.csv\")\n#ewsample[\"Category\"] = new_predictions\n#newsample.to_csv(\"submition2.csv\", index=False)","bed5cca4":"from sklearn.svm import  SVC\nmodel = SVC()\naccuracy=functionCheck(model,train)\naccuracy","2473e988":"from catboost import CatBoostClassifier\nmodel = CatBoostClassifier(verbose=False)\naccuracy=functionCheck(model,train)\naccuracy","005eea9d":"\nmodel = CatBoostClassifier(rsm=0.1,learning_rate=0.005,iterations = 500,l2_leaf_reg=5, verbose=False)\naccuracy=functionCheck(model,train)\naccuracy","e788fa31":"#maxlearning_rate=0\n#maxiterations=0\n#maxAccuracy=0;\n#for i in range(1,11):\n#    for j in range(1,11):\n#        model = CatBoostClassifier(learning_rate=0.002*i,iterations = 100*j,verbose=False)\n#        accuracy=functionCheck(model,train)\n#        if(maxAccuracy<accuracy):\n#            maxAccuracy=accuracy\n#            maxlearning_rate=0.002*i\n#            maxiterations=100*j\n#        print(\"for learning_rate=\",0.002*i,\"iterations=\",100*j,\"accuracy on train set is \",accuracy)\n#print(\"best accuracy with learning_rate=\",maxlearning_rate,\"iterations=\",maxiterations,\"accuracy is \",maxAccuracy)","c7d02243":"modelforSubmit =CatBoostClassifier(learning_rate=0.008,iterations = 500,verbose=False)\nmodelforSubmit.fit(x_train,y_train)\nnew_predictions = modelforSubmit.predict(test)\nnewsample = pd.read_csv(f\"{path}sample_submition.csv\")\nnewsample[\"Category\"] = new_predictions\nnewsample.to_csv(\"submition3.csv\", index=False)","908f3cc1":"lets check Artists_Genres","0ed8ff8f":"lets check test set for this data","33c5d395":"for learning_rate= 0.002 iterations= 100 accuracy on train set is  0.6236543517365435\n\nfor learning_rate= 0.002 iterations= 200 accuracy on train set is  0.6358101563581016\n\nfor learning_rate= 0.002 iterations= 300 accuracy on train set is  0.6343019233430193\n\nfor learning_rate= 0.002 iterations= 400 accuracy on train set is  0.6388750518887506\n\nfor learning_rate= 0.002 iterations= 500 accuracy on train set is  0.6388612148886121\n\nfor learning_rate= 0.002 iterations= 600 accuracy on train set is  0.6464646464646465\n\nfor learning_rate= 0.002 iterations= 700 accuracy on train set is  0.6540473225404733\n\nfor learning_rate= 0.002 iterations= 800 accuracy on train set is  0.6570845440708454\n\nfor learning_rate= 0.002 iterations= 900 accuracy on train set is  0.6570983810709837\n\nfor learning_rate= 0.002 iterations= 1000 accuracy on train set is  0.6570914625709147\n\nfor learning_rate= 0.004 iterations= 100 accuracy on train set is  0.6297426317974263\n\nfor learning_rate= 0.004 iterations= 200 accuracy on train set is  0.638854296388543\n\nfor learning_rate= 0.004 iterations= 300 accuracy on train set is  0.6388681333886813\n\nfor learning_rate= 0.004 iterations= 400 accuracy on train set is  0.6464646464646465\n\nfor learning_rate= 0.004 iterations= 500 accuracy on train set is  0.6495018679950187\n\nfor learning_rate= 0.004 iterations= 600 accuracy on train set is  0.6555763110557632\n\nfor learning_rate= 0.004 iterations= 700 accuracy on train set is  0.6540680780406808\n\nfor learning_rate= 0.004 iterations= 800 accuracy on train set is  0.6510446935104469\n\nfor learning_rate= 0.004 iterations= 900 accuracy on train set is  0.6540680780406808\n\nfor learning_rate= 0.004 iterations= 1000 accuracy on train set is  0.6525390895253909\n\nfor learning_rate= 0.006 iterations= 100 accuracy on train set is  0.6251833402518333\n\nfor learning_rate= 0.006 iterations= 200 accuracy on train set is  0.6449425764494258\n\nfor learning_rate= 0.006 iterations= 300 accuracy on train set is  0.6449564134495641\n\nfor learning_rate= 0.006 iterations= 400 accuracy on train set is  0.658634288086343\n\nfor learning_rate= 0.006 iterations= 500 accuracy on train set is  0.6616645911166459\n\nfor learning_rate= 0.006 iterations= 600 accuracy on train set is  0.6571122180711222\n\nfor learning_rate= 0.006 iterations= 700 accuracy on train set is  0.6631866611318666\n\nfor learning_rate= 0.006 iterations= 800 accuracy on train set is  0.6646948941469489\n\nfor learning_rate= 0.006 iterations= 900 accuracy on train set is  0.6631728241317282\n\nfor learning_rate= 0.006 iterations= 1000 accuracy on train set is  0.6631797426317975\n\nfor learning_rate= 0.008 iterations= 100 accuracy on train set is  0.6328075273280753\n\nfor learning_rate= 0.008 iterations= 200 accuracy on train set is  0.6586273695862737\n\nfor learning_rate= 0.008 iterations= 300 accuracy on train set is  0.6677321156773212\n\nfor learning_rate= 0.008 iterations= 400 accuracy on train set is  0.666230801162308\n\nfor learning_rate= 0.008 iterations= 500 accuracy on train set is  0.6707693372076934\n\nfor learning_rate= 0.008 iterations= 600 accuracy on train set is  0.6677390341773903\n\nfor learning_rate= 0.008 iterations= 700 accuracy on train set is  0.6586412065864121\n\nfor learning_rate= 0.008 iterations= 800 accuracy on train set is  0.6495364604953645\n\nfor learning_rate= 0.008 iterations= 900 accuracy on train set is  0.6419330289193302\n\nfor learning_rate= 0.008 iterations= 1000 accuracy on train set is  0.6449840874498409\n\nfor learning_rate= 0.01 iterations= 100 accuracy on train set is  0.6404109589041096\n\nfor learning_rate= 0.01 iterations= 200 accuracy on train set is  0.6510516120105162\n\nfor learning_rate= 0.01 iterations= 300 accuracy on train set is  0.6464784834647849\n\nfor learning_rate= 0.01 iterations= 400 accuracy on train set is  0.6570914625709147\n\nfor learning_rate= 0.01 iterations= 500 accuracy on train set is  0.65254600802546\n\nfor learning_rate= 0.01 iterations= 600 accuracy on train set is  0.6510308565103086\n\nfor learning_rate= 0.01 iterations= 700 accuracy on train set is  0.6555832295558323\n\nfor learning_rate= 0.01 iterations= 800 accuracy on train set is  0.6480005534800055\n\nfor learning_rate= 0.01 iterations= 900 accuracy on train set is  0.6571260550712604\n\nfor learning_rate= 0.01 iterations= 1000 accuracy on train set is  0.6419468659194686\n\nfor learning_rate= 0.012 iterations= 100 accuracy on train set is  0.6312854573128547\n\nfor learning_rate= 0.012 iterations= 200 accuracy on train set is  0.649550297495503\n\nfor learning_rate= 0.012 iterations= 300 accuracy on train set is  0.6540819150408191\n\nfor learning_rate= 0.012 iterations= 400 accuracy on train set is  0.6449564134495641\n\nfor learning_rate= 0.012 iterations= 500 accuracy on train set is  0.649515704995157\n\nfor learning_rate= 0.012 iterations= 600 accuracy on train set is  0.6419330289193302\n\nfor learning_rate= 0.012 iterations= 700 accuracy on train set is  0.6540819150408191\n\nfor learning_rate= 0.012 iterations= 800 accuracy on train set is  0.6464992389649924\n\nfor learning_rate= 0.012 iterations= 900 accuracy on train set is  0.6449840874498409\n\nfor learning_rate= 0.012 iterations= 1000 accuracy on train set is  0.648021308980213\n\nfor learning_rate= 0.014 iterations= 100 accuracy on train set is  0.6373737373737374\n\nfor learning_rate= 0.014 iterations= 200 accuracy on train set is  0.6465130759651307\n\nfor learning_rate= 0.014 iterations= 300 accuracy on train set is  0.6540957520409575\n\nfor learning_rate= 0.014 iterations= 400 accuracy on train set is  0.6586273695862737\n\nfor learning_rate= 0.014 iterations= 500 accuracy on train set is  0.6510308565103086\n\nfor learning_rate= 0.014 iterations= 600 accuracy on train set is  0.6510585305105853\n\nfor learning_rate= 0.014 iterations= 700 accuracy on train set is  0.6434758544347585\n\nfor learning_rate= 0.014 iterations= 800 accuracy on train set is  0.6449840874498408\n\nfor learning_rate= 0.014 iterations= 900 accuracy on train set is  0.6464923204649232\n\nfor learning_rate= 0.014 iterations= 1000 accuracy on train set is  0.6480143904801439\n\nfor learning_rate= 0.016 iterations= 100 accuracy on train set is  0.6479936349799363\n\nfor learning_rate= 0.016 iterations= 200 accuracy on train set is  0.6586135325861354\n\nfor learning_rate= 0.016 iterations= 300 accuracy on train set is  0.6631866611318666\n\nfor learning_rate= 0.016 iterations= 400 accuracy on train set is  0.6662169641621696\n\nfor learning_rate= 0.016 iterations= 500 accuracy on train set is  0.6586412065864121\n\nfor learning_rate= 0.016 iterations= 600 accuracy on train set is  0.649515704995157\n\nfor learning_rate= 0.016 iterations= 700 accuracy on train set is  0.6555624740556247\n\nfor learning_rate= 0.016 iterations= 800 accuracy on train set is  0.6495018679950187\n\nfor learning_rate= 0.016 iterations= 900 accuracy on train set is  0.6540819150408191\n\nfor learning_rate= 0.016 iterations= 1000 accuracy on train set is  0.6525598450255984\n\nfor learning_rate= 0.018000000000000002 iterations= 100 accuracy on train set is  0.6358447488584474\n\nfor learning_rate= 0.018000000000000002 iterations= 200 accuracy on train set is  0.6479867164798672\n\nfor learning_rate= 0.018000000000000002 iterations= 300 accuracy on train set is  0.6464784834647849\n\nfor learning_rate= 0.018000000000000002 iterations= 400 accuracy on train set is  0.6419537844195379\n\nfor learning_rate= 0.018000000000000002 iterations= 500 accuracy on train set is  0.6449771689497718\n\nfor learning_rate= 0.018000000000000002 iterations= 600 accuracy on train set is  0.643455098934551\n\nfor learning_rate= 0.018000000000000002 iterations= 700 accuracy on train set is  0.6434481804344818\n\nfor learning_rate= 0.018000000000000002 iterations= 800 accuracy on train set is  0.6373737373737374\n\nfor learning_rate= 0.018000000000000002 iterations= 900 accuracy on train set is  0.6510308565103086\n\nfor learning_rate= 0.018000000000000002 iterations= 1000 accuracy on train set is  0.6540542410405424\n\nfor learning_rate= 0.02 iterations= 100 accuracy on train set is  0.6328213643282137\n\nfor learning_rate= 0.02 iterations= 200 accuracy on train set is  0.6464784834647849\n\nfor learning_rate= 0.02 iterations= 300 accuracy on train set is  0.6601356026013561\n\nfor learning_rate= 0.02 iterations= 400 accuracy on train set is  0.6510170195101702\n\nfor learning_rate= 0.02 iterations= 500 accuracy on train set is  0.6616784281167843\n\nfor learning_rate= 0.02 iterations= 600 accuracy on train set is  0.6571122180711222\n\nfor learning_rate= 0.02 iterations= 700 accuracy on train set is  0.6525598450255984\n\nfor learning_rate= 0.02 iterations= 800 accuracy on train set is  0.6586204510862045\n\nfor learning_rate= 0.02 iterations= 900 accuracy on train set is  0.6540611595406115\n\nfor learning_rate= 0.02 iterations= 1000 accuracy on train set is  0.6540611595406115\n\nbest accuracy with learning_rate= 0.008 iterations= 500 accuracy is  0.6707693372076934","b8ec8f92":"just try a random model and it's top 1 leaderboard","74808343":"we can see that you prefer compilation  to single album type","af5388c6":"lets try to do better with changing some parameters","8d8a79f5":"let's delete common artists genres wich meet only in train or test set","a4151daa":"Key","88ad0db5":"this is accuracy on train test for my random model","a9688512":"no counrty only in train or test set","99089c3e":"lets try to check another models","9e46cff2":"this model improoved my score to 70","eca064ab":"lets check next feature: country","ef0dc03d":"lets take test and train data and combaine them into one frame","e5621696":"add evry common genres in unique colum and other to \"Oter generes\"","309ac16e":"Labels","4cd04a1d":"lets try to change koef for CatBoost mobel","996c80b6":"we can see that you like some keys more then others: C, B, f#, Bb, Ab\nAnd don't like: Eb, C#,E\n#i know nothing about key in music thats why just add a binar column for each value","c5e120f5":"Release year\n","2b77f273":"Vocal","fd52a3f9":"let's check what artists do you prefer","a040a2ca":"lets check likely % for each album","db2f06e2":"lets separete it on 3 group: good >65%; bad <35%; and normal between 35% and 65%","03e33229":"as we can see only one elements have na in this features. Just drop it","d43ab9c1":"lets check version","e426cdb0":"for n_estimators= 200 max_depth= 1 accuracy on train set is  0.5674484571744847\n\nfor n_estimators= 200 max_depth= 2 accuracy on train set is  0.6054033485540334\n\nfor n_estimators= 200 max_depth= 3 accuracy on train set is  0.6175660716756607\n\nfor n_estimators= 200 max_depth= 4 accuracy on train set is  0.6464092984640929\n\nfor n_estimators= 200 max_depth= 5 accuracy on train set is  0.638854296388543\n\nfor n_estimators= 200 max_depth= 6 accuracy on train set is  0.6419053549190535\n\nfor n_estimators= 200 max_depth= 7 accuracy on train set is  0.6449702504497025\n\nfor n_estimators= 200 max_depth= 8 accuracy on train set is  0.6479659609796596\n\nfor n_estimators= 200 max_depth= 9 accuracy on train set is  0.6601356026013561\n\nfor n_estimators= 200 max_depth= 10 accuracy on train set is  0.6601217656012177\n\nfor n_estimators= 400 max_depth= 1 accuracy on train set is  0.5735021447350214\n\nfor n_estimators= 400 max_depth= 2 accuracy on train set is  0.5977860799778608\n\nfor n_estimators= 400 max_depth= 3 accuracy on train set is  0.6221046077210461\n\nfor n_estimators= 400 max_depth= 4 accuracy on train set is  0.6449010654490107\n\nfor n_estimators= 400 max_depth= 5 accuracy on train set is  0.6373460633734607\n\nfor n_estimators= 400 max_depth= 6 accuracy on train set is  0.6403832849038328\n\nfor n_estimators= 400 max_depth= 7 accuracy on train set is  0.6495226234952263\n\nfor n_estimators= 400 max_depth= 8 accuracy on train set is  0.6495018679950187\n\nfor n_estimators= 400 max_depth= 9 accuracy on train set is  0.6480143904801439\n\nfor n_estimators= 400 max_depth= 10 accuracy on train set is  0.6616507541165075\n\nfor n_estimators= 600 max_depth= 1 accuracy on train set is  0.5719939117199391\n\nfor n_estimators= 600 max_depth= 2 accuracy on train set is  0.6068908260689082\n\nfor n_estimators= 600 max_depth= 3 accuracy on train set is  0.6281721322817213\n\nfor n_estimators= 600 max_depth= 4 accuracy on train set is  0.6479590424795905\n\nfor n_estimators= 600 max_depth= 5 accuracy on train set is  0.6358170748581707\n\nfor n_estimators= 600 max_depth= 6 accuracy on train set is  0.6464508094645081\n\nfor n_estimators= 600 max_depth= 7 accuracy on train set is  0.6495226234952263\n\nfor n_estimators= 600 max_depth= 8 accuracy on train set is  0.6510308565103086\n\nfor n_estimators= 600 max_depth= 9 accuracy on train set is  0.6601494396014944\n\nfor n_estimators= 600 max_depth= 10 accuracy on train set is  0.6631728241317282\n\nfor n_estimators= 800 max_depth= 1 accuracy on train set is  0.5765324477653245\n\nfor n_estimators= 800 max_depth= 2 accuracy on train set is  0.6023384530233845\n\nfor n_estimators= 800 max_depth= 3 accuracy on train set is  0.6221115262211153\n\nfor n_estimators= 800 max_depth= 4 accuracy on train set is  0.6479382869793829\n\nfor n_estimators= 800 max_depth= 5 accuracy on train set is  0.6434066694340667\n\nfor n_estimators= 800 max_depth= 6 accuracy on train set is  0.6434066694340667\n\nfor n_estimators= 800 max_depth= 7 accuracy on train set is  0.6479867164798672\n\nfor n_estimators= 800 max_depth= 8 accuracy on train set is  0.6555693925556939\n\nfor n_estimators= 800 max_depth= 9 accuracy on train set is  0.649515704995157\n\nfor n_estimators= 800 max_depth= 10 accuracy on train set is  0.6586204510862045\n\nfor n_estimators= 1000 max_depth= 1 accuracy on train set is  0.578047599280476\n\nfor n_estimators= 1000 max_depth= 2 accuracy on train set is  0.6023384530233845\n\nfor n_estimators= 1000 max_depth= 3 accuracy on train set is  0.626663899266639\n\nfor n_estimators= 1000 max_depth= 4 accuracy on train set is  0.6494534384945344\n\nfor n_estimators= 1000 max_depth= 5 accuracy on train set is  0.6449287394492874\n\nfor n_estimators= 1000 max_depth= 6 accuracy on train set is  0.6464508094645081\n\nfor n_estimators= 1000 max_depth= 7 accuracy on train set is  0.6601425211014252\n\nfor n_estimators= 1000 max_depth= 8 accuracy on train set is  0.6555763110557632\n\nfor n_estimators= 1000 max_depth= 9 accuracy on train set is  0.6571191365711914\n\nfor n_estimators= 1000 max_depth= 10 accuracy on train set is  0.6601217656012177\n\nfor n_estimators= 1200 max_depth= 1 accuracy on train set is  0.5841151238411513\n\nfor n_estimators= 1200 max_depth= 2 accuracy on train set is  0.6053756745537567\n\nfor n_estimators= 1200 max_depth= 3 accuracy on train set is  0.6297011207970112\n\nfor n_estimators= 1200 max_depth= 4 accuracy on train set is  0.6464231354642314\n\nfor n_estimators= 1200 max_depth= 5 accuracy on train set is  0.6419053549190535\n\nfor n_estimators= 1200 max_depth= 6 accuracy on train set is  0.6555555555555556\n\nfor n_estimators= 1200 max_depth= 7 accuracy on train set is  0.6646879756468798\n\nfor n_estimators= 1200 max_depth= 8 accuracy on train set is  0.6601217656012177\n\nfor n_estimators= 1200 max_depth= 9 accuracy on train set is  0.6601356026013561\n\nfor n_estimators= 1200 max_depth= 10 accuracy on train set is  0.6555763110557632\n\nfor n_estimators= 1400 max_depth= 1 accuracy on train set is  0.5765324477653245\n\nfor n_estimators= 1400 max_depth= 2 accuracy on train set is  0.6053756745537567\n\nfor n_estimators= 1400 max_depth= 3 accuracy on train set is  0.6342534938425349\n\nfor n_estimators= 1400 max_depth= 4 accuracy on train set is  0.6464231354642314\n\nfor n_estimators= 1400 max_depth= 5 accuracy on train set is  0.6403763664037636\n\nfor n_estimators= 1400 max_depth= 6 accuracy on train set is  0.6570776255707762\n\nfor n_estimators= 1400 max_depth= 7 accuracy on train set is  0.6555763110557632\n\nfor n_estimators= 1400 max_depth= 8 accuracy on train set is  0.6555693925556939\n\nfor n_estimators= 1400 max_depth= 9 accuracy on train set is  0.6616576726165767\n\nfor n_estimators= 1400 max_depth= 10 accuracy on train set is  0.649515704995157\n\nfor n_estimators= 1600 max_depth= 1 accuracy on train set is  0.5765324477653245\n\nfor n_estimators= 1600 max_depth= 2 accuracy on train set is  0.6038605230386053\n\nfor n_estimators= 1600 max_depth= 3 accuracy on train set is  0.6342534938425349\n\nfor n_estimators= 1600 max_depth= 4 accuracy on train set is  0.6479452054794521\n\nfor n_estimators= 1600 max_depth= 5 accuracy on train set is  0.6449287394492874\n\nfor n_estimators= 1600 max_depth= 6 accuracy on train set is  0.6555763110557632\n\nfor n_estimators= 1600 max_depth= 7 accuracy on train set is  0.6510308565103086\n\nfor n_estimators= 1600 max_depth= 8 accuracy on train set is  0.6570983810709837\n\nfor n_estimators= 1600 max_depth= 9 accuracy on train set is  0.6586273695862737\n\nfor n_estimators= 1600 max_depth= 10 accuracy on train set is  0.65407499654075\n\nfor n_estimators= 1800 max_depth= 1 accuracy on train set is  0.5750172962501731\n\nfor n_estimators= 1800 max_depth= 2 accuracy on train set is  0.605382593053826\n\nfor n_estimators= 1800 max_depth= 3 accuracy on train set is  0.6327314238273142\n\nfor n_estimators= 1800 max_depth= 4 accuracy on train set is  0.6449079839490798\n\nfor n_estimators= 1800 max_depth= 5 accuracy on train set is  0.6479728794797288\n\nfor n_estimators= 1800 max_depth= 6 accuracy on train set is  0.6555693925556939\n\nfor n_estimators= 1800 max_depth= 7 accuracy on train set is  0.649515704995157\n\nfor n_estimators= 1800 max_depth= 8 accuracy on train set is  0.6570914625709147\n\nfor n_estimators= 1800 max_depth= 9 accuracy on train set is  0.6601425211014252\n\nfor n_estimators= 1800 max_depth= 10 accuracy on train set is  0.6555832295558323\n\nfor n_estimators= 2000 max_depth= 1 accuracy on train set is  0.5735090632350907\n\nfor n_estimators= 2000 max_depth= 2 accuracy on train set is  0.6069046630690466\n\nfor n_estimators= 2000 max_depth= 3 accuracy on train set is  0.6342534938425349\n\nfor n_estimators= 2000 max_depth= 4 accuracy on train set is  0.650975508509755\n\nfor n_estimators= 2000 max_depth= 5 accuracy on train set is  0.6464577279645773\n\nfor n_estimators= 2000 max_depth= 6 accuracy on train set is  0.6525321710253217\n\nfor n_estimators= 2000 max_depth= 7 accuracy on train set is  0.649515704995157\n\nfor n_estimators= 2000 max_depth= 8 accuracy on train set is  0.6525390895253909\n\nfor n_estimators= 2000 max_depth= 9 accuracy on train set is  0.6601494396014944\n\nfor n_estimators= 2000 max_depth= 10 accuracy on train set is  0.6510377750103777\n\nbest accuracy with n_estimators= 1200 max_depth= 7 accuracy is  0.6646879756468798","afaf396a":"We can see that you like remaster version and have a ~50% for Radio Edit and SoundTrack. Also you dont like Remix and Single Edit. But this sample is too little. I will save data for remaster version and remix. Another data i will drop","2c61773f":"Just drop track name","498584fe":"define which artists are common and which are rare","ed345931":"let's separet it on 4 group: super_new year>2020; new year between 2010 and 2020; old year between 2000 and 2010; classic year less 2000"}}