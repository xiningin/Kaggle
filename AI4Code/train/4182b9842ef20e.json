{"cell_type":{"3d49d129":"code","9ef7260f":"code","dbc85fca":"code","27acd091":"code","d52d7083":"code","3ce7e094":"code","7168dd47":"code","e042750e":"code","89f24aeb":"code","1b2f9a5e":"code","7cc86444":"code","a85a51da":"code","dbf30b52":"code","d5b1de35":"code","8d45dc87":"code","9cf9b067":"code","a8ee0adc":"code","bb883736":"code","793d7fab":"code","14003ff6":"markdown","3e94153e":"markdown","f3120c21":"markdown","0eea07cb":"markdown","cfdb2339":"markdown","3db6b096":"markdown","5e966ab6":"markdown","ca280d9e":"markdown","e6e43031":"markdown","7fb9800f":"markdown","ab68ddef":"markdown","4e155746":"markdown","2406f136":"markdown","8e8f4096":"markdown","e715a09f":"markdown","a69d094d":"markdown","10e0d3f0":"markdown","1371adc1":"markdown","3dddf80c":"markdown","30ed2aec":"markdown","0734867e":"markdown","43970fc6":"markdown","3ba48019":"markdown","a3e5049c":"markdown"},"source":{"3d49d129":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# import data\ndata_path = '\/kaggle\/input\/insurance-premium-prediction\/insurance.csv'\ndf = pd.read_csv(data_path)\n\n# first five rows\ndf.head()","9ef7260f":"df.info()","dbc85fca":"def cool_countplot(data=None, x=None, y=None, hue=None, text_size=10):\n    ax = sns.countplot(data=data, x=x, hue=hue)\n    for p in ax.patches:\n        ax.annotate(text=str(int(p.get_height())) + ' (' + '{:.0%}'.format(p.get_height()\/data.shape[0]) + ')',\n                       xy=(p.get_x()+p.get_width()\/2, p.get_height()\/2), ha='center', color='black', size=text_size)\n    return ax","27acd091":"for col in df.drop('expenses',axis=1).columns:\n    if col in ['age', 'bmi']:\n        sns.histplot(data=df, x=col, binwidth=2)\n    else:\n        cool_countplot(data=df, x=col)\n    plt.show()\n\nsns.histplot(data=df, x='expenses', binwidth=1500)\nplt.show()","d52d7083":"for category in ['sex', 'region', 'smoker']:\n    sns.pairplot(df, hue=category, diag_kind='kde')","3ce7e094":"sns.lmplot(x='age', y='expenses', data=df, hue='smoker')\nsns.lmplot(x='bmi', y='expenses', data=df, hue='smoker')","7168dd47":"cool_countplot(data=df, x='region', hue='sex', text_size=10)\nplt.show()\ncool_countplot(data=df, x='sex', hue='smoker')\nplt.show()\ncool_countplot(data=df, x='region', hue='smoker')\nplt.show()","e042750e":"sns.histplot(data=df, x='age', hue='sex')\nplt.show()\nsns.histplot(data=df, x='bmi', hue='sex')\nplt.show()","89f24aeb":"sns.regplot(data=df, x='bmi', y='expenses') ","1b2f9a5e":"new_df = df.copy()\nnew_df.drop('expenses', axis=1, inplace=True)\n\n# applying label encoding for categorical features and standard scaling for continuous features\nle = LabelEncoder()\nstd_sc = StandardScaler()\n\nfor col in new_df.columns:\n    if col in ['age', 'bmi']:\n         new_df[[col]] = std_sc.fit_transform(new_df[[col]])\n    else:\n        new_df[col] = le.fit_transform(new_df[col])\nnew_df.head()","7cc86444":"# linearize expenses data\nnew_df['log_expenses'] = np.log2(df['expenses'])\n\n# show the transformation\nsns.kdeplot(data=df, x='expenses')\nplt.show()\nsns.kdeplot(data=new_df, x='log_expenses')","a85a51da":"corr = new_df.corr()\nvalue_mask2 = np.abs(corr) < 0.1\ntriu_mask2 = np.triu(corr)\n\nplt.figure(figsize=(12,5))\nsns.heatmap(data=corr, mask=triu_mask2 | value_mask2, annot=True, cmap='BrBG')","dbf30b52":"sns.histplot(data=df, x='bmi', hue='region', binwidth=2)","d5b1de35":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom statsmodels.api import qqplot\n\nX = new_df.copy()\ny = X.pop('log_expenses')","8d45dc87":"def show_score(model, X, y, cv):\n    r2 = cross_val_score(estimator=model, X=X, y=y, cv=cv, scoring='r2')\n    MAE = abs(cross_val_score(estimator=model, X=X, y=y, cv=cv, scoring='neg_mean_absolute_error'))\n    MSE = abs(cross_val_score(estimator=model, X=X, y=y, cv=cv, scoring='neg_mean_squared_error'))\n\n    mean = pd.Series(data=np.array([r2.mean(), MAE.mean(), MSE.mean()]), index=['r2','MAE','MSE'], name='mean')\n    std = pd.Series(data=np.array([r2.std(), MAE.std(), MSE.std()]), index=['r2','MAE','MSE'], name='std')\n    print(model)\n    print(pd.concat([mean, std], axis=1))\n    print('')","9cf9b067":"k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\nlr = LinearRegression()\nshow_score(lr, X, y, cv=k_fold)","a8ee0adc":"# regression hyperplane coefficients\nlr.fit(X,y)\nsns.barplot(x =X.columns, y=lr.coef_)","bb883736":"# check the heteroscedasticity assumption\nresidual = 2**y - 2**lr.predict(X)\nsns.scatterplot(x=residual, y=2**lr.predict(X))","793d7fab":"# check if residuals have normal distribution\nqqplot(residual, fit=True, line='45')\nplt.show()\n\nresidual.plot(kind='kde')\nplt.title('Residuals distribution')","14003ff6":"To evaluate the model I find it useful to define a function *show_score* which takes as input the model to evaluate, features and target and cross validation method. It uses sklearn.metrics.cross_val_score to compute the r2, MAE and MSE score (see section 7) for each of the 10 folds and then shows a dataframe reporting mean and std for each scoring method.","3e94153e":"The correlation matrix shows that the target variable is mainly related to the `smoker` and `age` features and there's no strong collinearity among the independent variables, though we can detect a weak pattern that I missed by looking at the distribution of bmi using region as hue. As shown below, southeastern people tend to have higher bmi while both northeastern and northweasthern have lower values (This turned out while watching the heatmap obtained by encoding with get_dummies).","f3120c21":"## 4) Feature Engineering <a id=\"Feature_Engineering\"><\/a> <a id=\"Feature_Engineering\"><\/a>\n\nI think of feature engineering as made of two parts, one of which is related to the elimination of noise within the data while the other one involves proper manipulation of data in order to make them satisfy the assumptions of a certain model.\n\nHowever since the features does not share any common property nor interesting patterns there is no need to make great changes to the data. One thing to be noticed though is that Linear Regression assumes the target to be linearly related to the data while EDA pointed out that expenses has a logarithmic distribution. Hence Feature Engineering reduces to linearize the target data, encode the categorical variables.","0eea07cb":"The first two grids does not seem to carry any remarkable information but by looking at the diagonal in the last one we can see that smoking has great relations with the target but also with other variables.","cfdb2339":"## 1) About the kernel <a id=\"introduction\"><\/a> <a id=\"introduction\"><\/a>\n\nThe purposes of this exercise is to look into different features to observe their relationship, and plot a multiple linear regression based on several features of individuals against their existing medical expense to be used for predicting future medical expenses of individuals that help medical insurance to make decision on charging the premium.","3db6b096":"Then we arrange things for cross validation using KFold with k=10. KFold devides the input dataset into K parts with same size and then shuffle them so that 1 of them acts like a test set and the other k-1 act like train set and this is repeated for k times and stops when each splitted part has played the role of test set. It will return then k values of accuracy for the model and we can take the mean of these values as the real accuracy. This is usually the preferred method as it gives a better indication of how well your model will perform on unseen data. Of course it is useful to look also at the standard deviation of the accuracy vector as low std means high consistency in the value we get. It also follows that while during train test split we scale train and test data separetely with KFold this is not possible, as we want to plug the entire train set into it.","5e966ab6":"## 3.2) Multivariate Analisys <a id=\"multivariate\"><\/a>\n\nMultivariate Analisys in the context of EDA is about looking at the impact that features have (or have not) on the target. In this phase I would look also for possible patterns among the independent variables.\n\nThis purpose can be easily achieved for the given dataframe as it consist of few columns, hence pairplots can be created using categorical features as hue. The KDEs in the diagonal show the distribution of a single variable while the scatter plots on the upper and lower triangles show the relationship (or lack thereof) between two variables.","ca280d9e":"* The age distribution has a bed shape: the highest concentration of sample is at the margin of the plot (18-19 and 62-63 years) while among the range 20 to 61 the frequence is about the half.\n* The sample is perfectly balanced in both the sex and region categories.\n* Most people (43%) does not have any child and the distribution of the relative variable seems to follow a logarithmic trend.\n* The bmi variable has a gaussian distribution.\n* Only 20% of the sampled people do smoke.\n* Medical expenses are low for most of the population and the distribution is logarithmic.","e6e43031":"## 6) Linear Regression Assumptions <a id=\"assumptions\"><\/a> <a id=\"assumptions\"><\/a>\n\nThere are five basic assumptions for the Linear Regression algorithm.\n1. Linear Relationship between the features and target.\n\nLinear regression captures only linear relationships. This can be validated by plotting a scatter plot between the features and the target. <br>\n\n2. Little or no Multicollinearity between the features.\n\nMulticollinearity is a state of very high inter-correlations among the independent variables. It is therefore a type of disturbance in the data and if present weakens the statistical power of the regression model. Pair plots and heatmaps (correlation matrix) can be used for identifying highly correlated features.\n\nWhy removing highly correlated features is important?\\\nA regression coefficient represents the mean change in the target for each unit change in a feature when you hold all of the other features constant. However, when there is correlation, changes in one feature determine a change in all the features correlated to it. The stronger the correlation, the more difficult it is to change one feature without changing another. It becomes difficult for the model to estimate the relationship between each feature and the target independently because the features tend to change in unison. <br>\n\n3. Homoscedasticity.\n\nHomoscedasticity describes a situation in which the error term (that is, the \u201cnoise\u201d or random disturbance in the relationship between the features and the target) is the same across all values of the independent variables. A scatter plot of residual values vs predicted values is a good way to check for homoscedasticity. If the data is heteroscedastic there should be some pattern in the distribution. <br>\n\n4. Normal distribution of the residuals.\n\nThe fourth assumption is that the error(residuals) follow a normal distribution. However, a less widely known fact is that, as sample sizes increase, the normality assumption for the residuals is not needed. More precisely, if we consider repeated sampling from our population, for large sample sizes, the distribution (across repeated samples) of the ordinary least squares estimates of the regression coefficients follow a normal distribution. As a consequence, for moderate to large sample sizes, non-normality of residuals should not adversely affect the usual inferential procedures. This result is a consequence of an extremely important result in statistics, known as the central limit theorem.\n\nNormal distribution of the residuals can be validated by plotting a q-q plot.\\\nIf the data comes from a normal distribution, the q-q plot would show a fairly straight line. Absence of normality in the errors can be seen with deviation in the straight line. <br>\n\n5. Little or No autocorrelation in the residuals.\n\nThe presence of correlation in error terms drastically reduces model\u2019s accuracy. Autocorrelation occurs when the residual errors are dependent on each other. This usually occurs in time series models where the next instant is dependent on previous instant.\n\nSource: https:\/\/towardsdatascience.com\/assumptions-of-linear-regression-algorithm-ed9ea32224e1","7fb9800f":"## Modeling <a id=\"modeling\"><\/a> <a id=\"modeling\"><\/a>\n\nFirst we load libraries and separate the target from features data","ab68ddef":"The last thing to notice is that expenses is linearly related to bmi, as shown below.","4e155746":"The last part of this section is dedicated to the verifying if the assumptions 3 and 4 (see section 6) of the model hold. Notice that assumptions 1 and 2 have been verified through EDA while the 5th should not be considered for this instance as it is mainly about time series.\n\nThe plots below show that there is a weak pattern in the residuals-vs-prediction and that the gaussian distribution does not fit very well the residuals. Thus the model that have been built cannot perform at best on this data.","2406f136":"## 7) Metrics <a id=\"metrics\"><\/a> <a id=\"metrics\"><\/a>\n\nTo assess the validity of Linear Regression predictions the first thing to look at is the residuals distribution, where the residuals are defined by\n$$Residuals = y - \\hat y$$\nwhere with y I will denote the actual values and with $\\hat y$ I will denote the predicted values.\nIf everything went right, the distribution should almost fit a Gaussian curve.\nNotice that this only checks if the followed procedure is right (i.e. the model is unbiased) and does not give information about how well the model performed. To look at how good the the actual data are fitted one must introduce metrics. If the sample length is n then the most popular metrics are:\n* Mean Absolute Error\n$$ MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat y_i| $$\n* Mean Squared Error\n$$ MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat y_i)^2 $$\n* $R^2$ score\n$$ R^2 = 1 - \\frac{RSS}{TSS} $$\nwhere RSS is the residual sum of squares $$RSS = \\sum_{i=1}^n (y_i - \\hat y_i)^2$$\nand TSS is the total sum of squares $$TSS = \\sum_{i=1}^n (y_i - y_{mean})^2$$\n\n<br>\n\nLet me spend some words about the $R^2$ coefficient. In the first place it is the only one among the three presented metrics that one prefers to maximize. It is a statistical measure that represents the proportion of the variance for a dependent variable that\u2019s explained by an independent variable; in other words it explains to what extent the variance of one variable explains the variance of the second variable. The $R^2$ coefficient takes values in the 0 to 1 range, where the 0 stands for a model that always predict the mean of the aactual values, while the 1 indicates that the model predicts each data point perfectly.","8e8f4096":"## 2) **Getting started** <a id=\"start\"><\/a> <a id=\"start\"><\/a>\n\nThis section's only purpose is to import the necessary libraries and the dataset, also taking a peek at it.","e715a09f":"The three plots above are a further confirm that this data are perfectly balanced.\nThe first one shows that the data is roughly composed by 50% males and 50% females from each region.\nThis allow to state, by looking at the second and the third one that there is the same amount of smokers for both sexes and for all the regions.\n\nAlso the distributions of age and bmi are roughly the same for both sexes as shown below. Hence no information can be detected.","a69d094d":" ## Table of Contents\n1. [About the kernel](#introduction)\n2. [Getting started](#start)\n3. [Exploratory Data Analisys](#EDA) <br>\n    3.1 [Univariate Analisys](#univariate) <br>\n    3.2 [Multivariate Analisys](#multivariate) <br>\n4. [Feature Engineering](#Feature_Engineering) <br>\n    4.1 [Correlation Analisys](#correlation) <br>\n5. [Modeling](#modeling) <br>\n6. [Linear Regression Assumptions](#assumptions) <br>\n7. [Metrics](#metrics)","10e0d3f0":"I always begin my EDA with the pd.info command, which gives a summary of the df about missing values and feature types.","1371adc1":"The dataset consist of 1338 observation and a really great info is that there are no missing data. Regarding the columns we have:\n* 3 continuous features: `age`, `bmi`, `expenses`.\n* 3 nominal features: `sex`, `smoker`, `region`.\n* 1 ordinal feature: `children`.","3dddf80c":"## 3.1) Univariate Analysis <a id=\"univariate\"><\/a>\n\nNow that we know what kind of features are presented within the df it's time to visualize them. Showing barplots and histograms help to understand what is the distribution of the indipendent variables among the data points. I also defined a function to display the count and the percentage of values with respect to the total sample length in the countplots because better visualization can be important sometimes.","30ed2aec":"Here are some takeaways:\n* Smokers are quite young with respect to the sample age.\n* They have a far lower bmi index compared to non-smoker.\n* They have less children than non-smokers.\n* Their medical expenses are significantly higher as compared to non-smokers.\n\nFurthermore, the \"smoking factor\" distincts two different populations in the bmi-expenses and age-expenses plot. In particular:\n* For the age-expenses plot we see that smokers tend to pay more than non-smokers but the line slope seems almost the same, implying that the increase of medical expenses with respect to the age is the same for both categories.\n* For the bmi-expenses plot we see that smokers tend to pay more than non-smokers but here the line slope is much higher, resulting in a massive medical expenses increase when the bmi raises.\n\nThese informations are even more interesting when we look at the following plots.","0734867e":"## 3) **Exploratory Data Analisys** <a id=\"EDA\"><\/a> <a id=\"EDA\"><\/a>\n\nEDA involves using statistical and visualization techniques to **describe the data** and highligth important aspects of the problem that may need further analisys. Hence no assumption about the data must be done in this section, though it could be useful to formulate hypotesis which have to be checked in a second time.\nA good data exploration allows the data scientist to:\n1. Verify expected relationships actually exist in the data, thus formulating and validating planned techniques of analysis.\n2. To find some unexpected structure (think: outliers etc) in the data that must be taken into account, thereby suggesting some changes in the planned analysis.\n3. Deliver data-driven insights to business stakeholders by confirming they are asking the right questions and not biasing the investigation with their assumptions.\n4. Provide the context around the problem to make sure the potential value of the data scientist\u2019s output can be maximized.\n\nThe main weapon of this section is Data visualization. Here is how it will be used:\n* Histograms to show the distribution of a single continuous variable.\n* Countplots to confront quantities corresponding to discrete groups.","43970fc6":"As expected the most relevance in the model is assumed by the smoking factor, followed by the age.","3ba48019":"## 4.1) Correlation Analysis <a id=\"correlation\"><\/a>\n\nCorrelation analysis examines the linear correlation between variable pairs. There are two reasons why we need correlation analysis:\n* Identify indipendent variables that are strongly correlated with the targets as they are strong predictors.\n* Detect collinearity between independent variables. This is essential if we are using models that assume no collinearity among them (like linear regression).","a3e5049c":" <a id=\"Feature_Engineering\"><\/a> <a id=\"Feature_Engineering\"><\/a>"}}