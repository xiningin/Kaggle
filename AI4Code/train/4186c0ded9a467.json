{"cell_type":{"c7549964":"code","c27443c6":"code","509989d8":"code","da8a836c":"code","366cd176":"code","0fbc539b":"code","d9e12acc":"code","597c2eb1":"code","75eb3ec5":"code","e93da9d8":"code","5586562f":"code","748cb1c9":"code","e8cd5a1e":"markdown","c4c156a3":"markdown","69fb8f3d":"markdown","b972c8d5":"markdown","3bcc98d2":"markdown","2cff1205":"markdown","483f28c2":"markdown","537bbfee":"markdown","86fdaa47":"markdown","5fed8111":"markdown","b1adb029":"markdown","b4021f53":"markdown"},"source":{"c7549964":"import gc\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow as tf\n\nfrom transformers import RobertaTokenizer, RobertaConfig, TFRobertaPreTrainedModel\nfrom transformers.modeling_tf_roberta import TFRobertaMainLayer\nfrom transformers.modeling_tf_utils import get_initializer","c27443c6":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)","509989d8":"MODEL_NAME = 'roberta-base'\nMAX_LEN = 128\ntokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\ndf_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv', dtype={'id': np.int16, 'target': np.int8})\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv', dtype={'id': np.int16, 'target': np.int8})\ndf_sample_submission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","da8a836c":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ndf_train.at[df_train['id'].isin(ids_with_target_error),'target'] = 0","366cd176":"def to_tokens(input_text, tokenizer):\n    output = tokenizer.encode_plus(input_text, max_length=MAX_LEN, pad_to_max_length=True)\n    return output\n\ndef select_field(features, field):\n    return [feature[field] for feature in features]","0fbc539b":"tokenizer_output_train = df_train['text'].apply(lambda x: to_tokens(x, tokenizer))\ntokenizer_output_test = df_test['text'].apply(lambda x: to_tokens(x, tokenizer))","d9e12acc":"input_ids_train = np.array(select_field(tokenizer_output_train, 'input_ids'))\nattention_masks_train = np.array(select_field(tokenizer_output_train, 'attention_mask'))\n\ninput_ids_test = np.array(select_field(tokenizer_output_test, 'input_ids'))\nattention_masks_test = np.array(select_field(tokenizer_output_test, 'attention_mask'))","597c2eb1":" class CustomModel(TFRobertaPreTrainedModel):\n    def __init__(self, config, *inputs, **kwargs):\n        super(CustomModel, self).__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n        self.roberta = TFRobertaMainLayer(config, name=\"roberta\")\n        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n        self.classifier = tf.keras.layers.Dense(units=config.num_labels,\n                                                name='classifier', \n                                                kernel_initializer=get_initializer(\n                                                    config.initializer_range))\n\n    def call(self, inputs, **kwargs):\n        outputs = self.roberta(inputs, **kwargs)\n        pooled_output = outputs[1]\n        pooled_output = self.dropout_1(pooled_output, training=kwargs.get('training', False))\n        logits = self.classifier(pooled_output)\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        return outputs","75eb3ec5":"def init_model(model_name):\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n    with strategy.scope():\n        config = RobertaConfig.from_pretrained(model_name, num_labels=2)\n        model = CustomModel.from_pretrained(model_name)\n        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n        metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n    return model","e93da9d8":"BATCH_SIZE = 128\nEPOCHS = 10\nSPLITS = 5\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n                                              patience=3, verbose=0, \n                                              restore_best_weights=True)]\nmodel_output = np.zeros((df_sample_submission['target'].shape[0], 2))\nskf = StratifiedKFold(n_splits=SPLITS, shuffle=False)\nX, y = input_ids_train, df_train['target'].values.reshape(-1, 1)\nskf.get_n_splits(X, y)\nfor i, (train_index, test_index) in enumerate(skf.split(X, y)):\n    X_train, attention_masks_train_stratified = X[train_index], attention_masks_train[train_index]\n    X_test, attention_masks_test_stratified =  X[test_index], attention_masks_train[test_index]\n    y_train, y_test = tf.keras.utils.to_categorical(y[train_index]), tf.keras.utils.to_categorical(y[test_index])\n    X_train = X_train[:-divmod(X_train.shape[0], BATCH_SIZE)[1]]\n    attention_masks_train_stratified = attention_masks_train_stratified[:-divmod(attention_masks_train_stratified.shape[0], \n                                                                                 BATCH_SIZE)[1]]\n    y_train = y_train[:-divmod(y_train.shape[0], BATCH_SIZE)[1]]\n    model = init_model(MODEL_NAME)\n    if i == 0:\n        print(model.summary())\n    model.fit([X_train, attention_masks_train_stratified], y_train, \n              validation_data=([X_test, attention_masks_test_stratified], y_test), \n              batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks)\n    model_output += model.predict([input_ids_test, attention_masks_test])\n    del model\n    gc.collect()\n    print('='*22 + ' Split ' + str(i+1) + ' finished ' + '='*22)\nmodel_output \/= SPLITS","5586562f":"df_sample_submission['target'] = np.argmax(model_output, axis=1).flatten()\ndf_sample_submission['target'].value_counts()","748cb1c9":"df_sample_submission.to_csv('submission.csv', index=False)","e8cd5a1e":"Here we load the pretrained roberta model, create the optimizer, loss and metric functions. Since TPU support was added to Kaggle, we are using this by enclosing model.compile() in the scope of distribution strategy.","c4c156a3":"We fix erroneous labels. Check kernel link given at the end to find out why they are erroneous.","69fb8f3d":"These lines are used to connect and initialize with the TPU system ","b972c8d5":"Thanks to [this](Thanks to https:\/\/www.kaggle.com\/wrrosa\/keras-bert-using-tfhub-modified-train-data) notebook for finding out flaws with some of the train data.\nSpecial thanks to Huggingface \ud83e\udd17 for providing such a wonderful library and Kaggle team for bringing TPUs to notebooks.","3bcc98d2":"We import the necessary libraries. ","2cff1205":"The output is predicted and saved to submission csv file.","483f28c2":"We convert the target labels to categorical one hot encoded format as the roberta model is by default configured to return a tensor with 2 labels (which is the minimum for classification). For training with TPUs, the batch size should be optimally 128. I had used the batch size of 64 because it is divisible by 8 which is recommended by GCP page on TPUs. The gpus are usually severely constrained by Video RAM which is quite low (~15.9GBs) for P100 gpus used by kaggle thus they form a bottleneck. We don't face such problems with TPUs.","537bbfee":"This loads the pretrained tokenizers from Transformer library.","86fdaa47":"We do entire encoding and tokenizing using the below function.\nEncoding formats the text supported by Roberta vocab and removes unwanted characters. Tokenizer creates an array from these encoded tokens. This is a numpy array and can be used for training. It is called input_ids. We also pad these input ids to a fixed length so that there is no variations in lenth between indivual rows.","5fed8111":"TPUs can only work with data if their length is a multiple of the batch size. Since there are 7613 individual rows of texts we have to remove 61 rows of training data because 7613-61 = 7552 which is perfectly divisible by the batch size of 128. In other words, 128 * 59 = 7552. The ideal batch size for TPU training is 128 which also perfectly divides 7552.\n\nFor training with GPUs this is not required. Running the training in TPU takes ~20s per epoch (initially it takes more due to loading being a bottleneck). This is in contrast with a NVIDIA P100 GPU taking 10mins per epoch.\n\nBelow I am also using Stratified KFold.","b1adb029":"Edit:\nFinally TPU acceleration is enabled in Kaggle Notebooks !!! The difference is as much as 20x lower training time.\n\n**Background on tech that is used:\n**\n**DNN architecture:\n**\n> RoBERTa iterates on BERT's pretraining procedure, including training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data. See the associated paper for more details.\n\n**Implementation:**\n\nTensorflow implementation of Roberta pretrained for classification, provided by:\nhttps:\/\/github.com\/huggingface\/transformers","b4021f53":"We create and store the input ids as well as attention masks as an numpy array. The input ids are the numbers which are understood by the Neural Network."}}