{"cell_type":{"f0ab7588":"code","fd308bd6":"code","4d026bc1":"code","5dc8684c":"code","5d7aff1c":"code","bd93719c":"code","67b499c6":"code","54520d9f":"code","1e4d3be8":"code","7891ac78":"code","f2bea4a1":"code","a9d5d143":"code","74b97c62":"code","33ef8af5":"code","265d7e49":"code","16aba3ba":"code","6f131d90":"code","1d5978cb":"code","2699e993":"code","357b9c78":"code","bb5cc4d8":"code","d69aa43b":"code","c292ee20":"code","0dcd83f3":"code","b3f026ef":"code","ffa60fb2":"code","9d05be93":"code","cff64116":"code","a7dbd482":"code","278325d1":"code","e740671f":"code","a273203d":"code","bb455104":"code","d952357b":"code","3f1f9c1c":"code","81b851df":"code","98b44f7d":"code","6470d77d":"markdown","31626701":"markdown","2f1a74f4":"markdown","640780b1":"markdown","416133a4":"markdown","9a0b92b2":"markdown","81149f4b":"markdown","664e2b06":"markdown","f0d3578b":"markdown","198e14b5":"markdown","6cd25e2f":"markdown","14a8ea8f":"markdown","13451a81":"markdown","55ae8312":"markdown"},"source":{"f0ab7588":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm # for Support Vector Machine\nfrom sklearn import metrics # for the check the error and accuracy\nimport matplotlib as mpl","fd308bd6":"df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\n","4d026bc1":"df.shape#569 rows and 33 columns\n","5dc8684c":"df.columns","5d7aff1c":"df.drop('Unnamed: 32',axis = 1,inplace = True)\n","bd93719c":"X = df.iloc[:,:]\ny = df.iloc[:,1]\n","67b499c6":"X = X.drop(['diagnosis','id'],axis = 1)\n","54520d9f":"X.info()\n","1e4d3be8":"# VISUALIZATION\nmpl.style.use(['ggplot']) \n# for ggplot-like style\n","7891ac78":"sns.pairplot(df.loc[:,'diagnosis':'area_mean'], hue=\"diagnosis\");\n","f2bea4a1":"y.value_counts().plot(kind =\"bar\")\n","a9d5d143":"y.value_counts().plot(kind =\"pie\")\n","74b97c62":"new_data_B= df[df.diagnosis !='M']\nnew_data_M= df[df.diagnosis !='B']","33ef8af5":"new_data_B.plot(kind = \"density\",x= 'radius_mean', y = 'concavity_mean')\nplt.xlabel(\"mean radius for benigm\")\nplt.ylabel(\"mean concavity for benigm\")","265d7e49":"new_data_M.plot(kind = \"density\",x= 'radius_mean', y = 'concavity_mean')\nplt.xlabel(\"mean radius for malignant\")\nplt.ylabel(\"mean concavity for malignant\")","16aba3ba":"new_data_B.plot(kind = \"scatter\",x= 'radius_mean', y = 'area_mean')\nplt.xlabel(\"mean radius for benigm\")\nplt.ylabel(\"mean area for benigm\")\n","6f131d90":"new_data_M.plot(kind = \"scatter\",x= 'radius_mean', y = 'area_mean')\nplt.xlabel(\"mean radius for malignant\")\nplt.ylabel(\"mean area for malignant\")\n","1d5978cb":"g = sns.jointplot(x=new_data_M['radius_mean'], y=new_data_M['texture_mean'], data=new_data_M, kind=\"kde\", color=\"m\")\ng.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")\ng.ax_joint.collections[0].set_alpha(0)\ng.set_axis_labels(\"$mean radius for malignant$\", \"$mean texture for malignant$\");\n\n","2699e993":"g = sns.jointplot(x=new_data_B['radius_mean'], y=new_data_B['texture_mean'], data=new_data_B, kind=\"kde\", color=\"m\")\ng.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")\ng.ax_joint.collections[0].set_alpha(0)\ng.set_axis_labels(\"$mean radius$\", \"$mean texture$\");\n\n","357b9c78":"avgB = {}\nfor i in range(2,new_data_B.shape[1]):\n    m = np.mean(new_data_B.iloc[:,i])\n    avgB.update({new_data_B.columns[i]:m})\n\navgB_df = pd.DataFrame(avgB,index = np.arange(1,31))\navgB_df = avgB_df.transpose()\navgB_df = avgB_df.iloc[:,:1]\n","bb5cc4d8":"avgM = {}\nfor i in range(2,new_data_M.shape[1]):\n    m = np.mean(new_data_M.iloc[:,i])\n    avgM.update({new_data_M.columns[i]:m})\n\navgM_df = pd.DataFrame(avgM,index = np.arange(1,31))\navgM_df = avgM_df.transpose()\navgM_df = avgM_df.iloc[:,:1]\n\n","d69aa43b":"#so now, i have 2 data frames and i want to have a combined barplot\n\navgB_df['hue']='B'\navgM_df['hue']='M'\nres=pd.concat([avgB_df,avgM_df])\nres = res.reset_index(level =0)\nsns.barplot(x = res.iloc[:,0],y = res.iloc[:,1],data=res,hue='hue')\nplt.xticks(rotation=90)\nplt.ylabel('average of feature mentioned on X axis')\nplt.show()","c292ee20":"g = sns.PairGrid(new_data_B.loc[:,'radius_mean':'smoothness_mean'])\ng.map_diag(sns.kdeplot)\ng.map_offdiag(sns.kdeplot, n_levels=6);","0dcd83f3":"\ng = sns.PairGrid(new_data_M.loc[:,'radius_mean':'smoothness_mean'])\ng.map_diag(sns.kdeplot)\ng.map_offdiag(sns.kdeplot, n_levels=6);\n","b3f026ef":"data = pd.DataFrame(X)\ndata_n_2 = (data - data.mean()) \/ (data.std())  \ndata = pd.concat([y,data_n_2.iloc[:,10:25]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=45);","ffa60fb2":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(X.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax);","9d05be93":"#lets see the amount of benigan and melignant tissues:\n#lets use countplot for this.\nB,M = y.value_counts()\n\nprint(B,M)\n#we can see that there are 357 B type and 212 M type cells\n","cff64116":"#lets split the data now\nfrom sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.2,random_state = 1)\n","a7dbd482":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(xtrain,ytrain)# now fit our model for traiing data\nprediction=model.predict(xtest)# predict for the test data\nprint(metrics.accuracy_score(prediction,ytest))\n\n","278325d1":"    \nmodel = svm.SVC()\nmodel.fit(xtrain,ytrain)# now fit our model for traiing data\nprediction=model.predict(xtest)# predict for the test data\n\nmetrics.accuracy_score(prediction,ytest)\nprint(metrics.accuracy_score(prediction,ytest))\nmetrics.confusion_matrix(ytest,prediction)\n","e740671f":"\n#knn\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(xtrain,ytrain)\nypred = knn.predict(xtest)\nprint(metrics.accuracy_score(ypred,ytest))\nmetrics.confusion_matrix(ytest,prediction)\n\n\n","a273203d":"\n#naive bayes\nknn = GaussianNB()\nknn.fit(xtrain,ytrain)\nypred = knn.predict(xtest)\nprint(metrics.accuracy_score(ypred,ytest))\nmetrics.confusion_matrix(ytest,prediction)\n","bb455104":"\n#decision tree\n\ndt = DecisionTreeClassifier()\ndt.fit(xtrain,ytrain)\nypred = dt.predict(xtest)\nprint(metrics.accuracy_score(ypred,ytest))\nmetrics.confusion_matrix(ytest,prediction)\n","d952357b":"#decision tree\n\ndt = DecisionTreeClassifier()\ndt.fit(xtrain,ytrain)\nypred = dt.predict(xtest)\nprint('DECISION TREE CLASSIFIER:: ',metrics.accuracy_score(ypred,ytest))\n\n#random forest\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(xtrain,ytrain)# now fit our model for traiing data\nprediction=model.predict(xtest)# predict for the test data\nprint('FORSEST TREE CLASSIFICATION:: ',metrics.accuracy_score(prediction,ytest))\n\n\n#SVM\nmodel = svm.SVC()\nmodel.fit(xtrain,ytrain)# now fit our model for traiing data\nprediction=model.predict(xtest)# predict for the test data\nmetrics.accuracy_score(prediction,ytest)\nprint('SUPPORT VECTOR MACHINE:: ',metrics.accuracy_score(prediction,ytest))\n\n#knn\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(xtrain,ytrain)\nypred = knn.predict(xtest)\nprint('K NEAREST NEIGHBOURS:: ',metrics.accuracy_score(ypred,ytest))\n\n\n#naive bayes\nNB = GaussianNB()\nNB.fit(xtrain,ytrain)\nypred = NB.predict(xtest)\nprint('NAIVE BAYES ALGORITHM:: ',metrics.accuracy_score(ypred,ytest))\n\n\n\n","3f1f9c1c":"#what if i scale the data now::","81b851df":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","98b44f7d":"#lets split the data now\nfrom sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.2,random_state = 1)\n\n#decision tree\ndt = DecisionTreeClassifier()\ndt.fit(xtrain,ytrain)\nypred = dt.predict(xtest)\nprint('DECISION TREE CLASSIFIER:: ',metrics.accuracy_score(ypred,ytest))\n\n#random forest\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(xtrain,ytrain)# now fit our model for traiing data\nprediction=model.predict(xtest)# predict for the test data\nprint('FORSEST TREE CLASSIFICATION:: ',metrics.accuracy_score(prediction,ytest))\n\n\n#SVM\nmodel = svm.SVC()\nmodel.fit(xtrain,ytrain)# now fit our model for traiing data\nprediction=model.predict(xtest)# predict for the test data\nmetrics.accuracy_score(prediction,ytest)\nprint('SUPPORT VECTOR MACHINE:: ',metrics.accuracy_score(prediction,ytest))\n\n#knn\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(xtrain,ytrain)\nypred = knn.predict(xtest)\nprint('K NEAREST NEIGHBOURS:: ',metrics.accuracy_score(ypred,ytest))\n\n\n#naive bayes\nNB = GaussianNB()\nNB.fit(xtrain,ytrain)\nypred = NB.predict(xtest)\nprint('NAIVE BAYES ALGORITHM:: ',metrics.accuracy_score(ypred,ytest))\n\n\n\n","6470d77d":"## VISUALIZATION","31626701":"# About the Dataset and notebook:\n\n**A tumor is an abnormal lump or growth of cells. When the cells in the tumor are normal, it is benign. Something just went wrong, and they overgrew and produced a lump. When the cells are abnormal and can grow uncontrollably, they are cancerous cells, and the tumor is malignant.**\n\n**In this note book i have tried to compare the 2 tumors- benign and malignant\n  i have added visualisatiion to show that how are these two tumors different\n  then i have applied various classification models for predictions for given features that whether the tumour is B or M.\n  i have also illustrated the importance of feature scaling.**","2f1a74f4":"# ABOVE TWO GRAPHS PROVE THE DIFFERENCE BETWEEN 2 TUMORS.","640780b1":"\n# BASIC PREPROCESSING OF THE DATA","416133a4":"# LOADING THE DATA SET USING READ_CSV","9a0b92b2":"### WE CAN SEE THAT RANDOM FOREST CLASSIFIER GIVES US THE BEST ACCURACY RESULTS.\n\n### SVM BEING THE WORST FOR THIS DATA\n\n###but,scaling the data gives different results","81149f4b":"##### SIMILARLY , ABOVE TWO GRAPHS PROVE THAT M TUMORS ARE MUCH BIGGER THAN B TUMORS.","664e2b06":"## comparing accuracy of unscaled data all together","f0d3578b":"# AFTER SCALING, SVM PERFORMS THE BEST","198e14b5":"### FOLLOWNG GRAPHS SHOW THE DIFFERENCE BETWEEN VARIOUS PARAMTERS OF BENIGN AND MALIGNANT TUMORS\n","6cd25e2f":"**ANALYZING THE GRAPHS ABOVE, GIVE US A GOOD PICTORIAL IDEA FOR DIFFERENCES BETWEEN THE TUMORS**","14a8ea8f":"# Loading Libraries and Data","13451a81":"# FITTING & TESTING THE CLASSIFICATION MODELS (without scaling the data)::","55ae8312":"### #lets split the data now\n"}}