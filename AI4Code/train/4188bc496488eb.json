{"cell_type":{"739a0faa":"code","d84094f6":"code","e9b87cdf":"code","1dd479ee":"code","04d4268d":"code","87288a88":"code","4443ab7b":"code","26648fe6":"code","a241e187":"code","3864dece":"code","f5a7ec13":"code","56666ce4":"code","1ed463db":"code","d2938256":"code","42a92cc3":"code","b0129002":"code","dd6c0cb9":"code","8355cd40":"code","7f0f2682":"code","e094dae5":"markdown","07943143":"markdown","feb135f1":"markdown","30d996bf":"markdown","d6fc3bfc":"markdown","9ad872d1":"markdown","88f827a1":"markdown","43d899b6":"markdown","22d1aa5b":"markdown","04f451c1":"markdown","23483691":"markdown","8b996923":"markdown","bc7ac66f":"markdown","2c8d5f9d":"markdown","65e5fe13":"markdown","c8be74ab":"markdown","518733a2":"markdown","ba066cb9":"markdown"},"source":{"739a0faa":"import numpy as np\nimport pandas as pd\nimport random\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom mpl_toolkits.basemap import Basemap\nimport math\nfrom math import cos, asin, sqrt\nfrom numpy import nansum\nfrom numpy import nanmean\nimport netCDF4\n%matplotlib inline","d84094f6":"n = 11061987\ns = 11061987\nskip = sorted(random.sample(range(n),n-s))\n        \ndf_path = \"..\/input\/geonames-database\/geonames.csv\"\ndf = pd.read_csv(df_path,index_col='geonameid',skiprows=skip)","e9b87cdf":"C = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\nMissing_Percentage = (df.isnull().sum()).sum()\/np.product(df.shape)*100\n\nprint(\"The number of missing entries: \" + str(round(Missing_Percentage,2)) + \" %\")","1dd479ee":"All_NaN = df.isnull().sum()\nRowsCount = len(df.index)\n\nprint(\"The percentage number of missing entries per variable: \", format(round(All_NaN\/RowsCount * 100,5)) )","04d4268d":"df=df.drop(['alternatenames','admin2 code','admin3 code','admin4 code','cc2'], axis=1)","87288a88":"def distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    a = 0.5 - cos((lat2-lat1)*p)\/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) \/ 2\n    return 12742 * asin(sqrt(a))\n\nprint(\"The length is circa: \"+ format(round(distance(51,14,55,24))) + \" kilometers.\")","4443ab7b":"def round_up_to_even(f):\n    return math.ceil(f \/ 2.) * 2","26648fe6":"df[\"latitude_app\"] = df.apply(lambda row: round_up_to_even(row['latitude']),axis=1)\ndf[\"longitude_app\"] = df.apply(lambda row: round_up_to_even(row['longitude']),axis=1)\n\nelevation_table = df[['elevation','latitude_app','longitude_app']].groupby(['latitude_app',\n                'longitude_app']).agg({'elevation': lambda x: x.mean(skipna=True)}).sort_values(by=['latitude_app', \n                'longitude_app'], ascending=False).reset_index()\n\ndf = pd.merge(df,  elevation_table,  on =['latitude_app', 'longitude_app'],  how ='inner')","a241e187":"print(\"Still NAs in 'elevation': \"+ format(      round((df[['elevation_y']].isnull().sum()).sum()\/np.product(df.shape[0])*100,2)     )  + \" %.\")","3864dece":"WorldAverageElevation = 840\ndf['elevation_y']=df['elevation_y'].fillna(WorldAverageElevation)\n\ndf=df.drop(['elevation_x'], axis=1)\ndf","f5a7ec13":"ISO = pd.read_csv('..\/input\/alpha-country-codes\/Alpha__2_and_3_country_codes.csv', sep=';')\n\nISO['Country'] = ISO.apply(lambda row: str.rstrip(row['Country']),axis=1)\n\nISO_toMerge = ISO.drop(['Alpha-3 code','Numeric'], axis=1)\nISO_toMerge=ISO_toMerge.rename(columns={\"Alpha-2 code\": \"country code\"})\ndf = pd.merge(df, ISO_toMerge,  on ='country code',  how ='inner')\n\ndf","56666ce4":"df_sample = df.sample(n=100000)\n\nplt.figure(1, figsize=(12,6))\nm1 = Basemap(projection='merc',llcrnrlat=-60,urcrnrlat=65,llcrnrlon=-180,urcrnrlon=180,\n             lat_ts=0,resolution='c')\n\nm1.fillcontinents(color='#191919',lake_color='#000000') \nm1.drawmapboundary(fill_color='#000000')                \nm1.drawcountries(linewidth=0.2, color=\"w\")              \n\n# Plot the data\nmxy = m1(df_sample[\"longitude\"].tolist(), df_sample[\"latitude\"].tolist())\nm1.scatter(mxy[0], mxy[1], s=3, c=\"#1292db\", lw=0, alpha=1, zorder=5)\n\nplt.title(\"Sample of 100.000 locations in the world\")\nplt.show()","1ed463db":"lon_min, lon_max = -10, 40\nlat_min, lat_max = 35, 65\n\nidx_europe = (df[\"longitude\"]>lon_min) &\\\n            (df[\"longitude\"]<lon_max) &\\\n            (df[\"latitude\"]>lat_min) &\\\n            (df[\"latitude\"]<lat_max)\n\ndf_europe = df[idx_europe].sample(n=100000)\n\nplt.figure(2, figsize=(12,6))\nm2 = Basemap(projection='merc',llcrnrlat=lat_min,urcrnrlat=lat_max,llcrnrlon=lon_min,\n             urcrnrlon=lon_max,lat_ts=35,resolution='c')\n\nm2.fillcontinents(color='#191919',lake_color='#000000') \nm2.drawmapboundary(fill_color='#000000')                \nm2.drawcountries(linewidth=0.2, color=\"w\")              \n\nmxy = m2(df_europe[\"longitude\"].tolist(), df_europe[\"latitude\"].tolist())\nm2.scatter(mxy[0], mxy[1], s=5, c=\"#1292db\", lw=0, alpha=0.05, zorder=5)\n\nplt.title(\"Sample of 100.000 locations in Europe\")\nplt.show()","d2938256":"Aggregated = df[['name','Country']]\nAggregated = Aggregated.groupby(['Country']).agg(['count']).sort_values([('name', 'count')], ascending=False)\nAggregated['Percentage'] = round(Aggregated[['name']] \/ df.shape[0],2)\nAggregated.columns = Aggregated.columns.get_level_values(0)\nAggregated.columns = [''.join(col).strip() for col in Aggregated.columns.values]\nAggregated","42a92cc3":"Population = pd.read_csv('..\/input\/population-by-country-2020\/population_by_country_2020.csv')\n\nPopulation = Population.rename(columns={\"Country (or dependency)\": \"Country\"})\n\nPopulation[['Country']] = Population[['Country']].replace(\"Czech Republic (Czechia)\", \"Czechia\")\nPopulation[['Country']] = Population[['Country']].replace(\"United States\", \"United States of America\")\nPopulation[['Country']] = Population[['Country']].replace(\"United Kingdom\", \"United Kingdom of Great Britain and Northern Ireland\")\nPopulation[['Country']] = Population[['Country']].replace(\"Vietnam\", \"Viet Nam\")\nPopulation[['Country']] = Population[['Country']].replace(\"Laos\", \"Lao People Democratic Republic\")\nPopulation[['Country']] = Population[['Country']].replace(\"State of Palestine\", \"Palestine\")\nPopulation[['Country']] = Population[['Country']].replace(\"North Macedonia\", \"Republic of North Macedonia\")\nPopulation[['Country']] = Population[['Country']].replace(\"Russia\", \"Russian Federation\")\nPopulation[['Country']] = Population[['Country']].replace(\"Syria\", \"Syrian Arab Republic\")","b0129002":"Population_Merged = pd.merge(ISO_toMerge,Population,  on ='Country',  how ='inner')\n\nPopulation_Merged[['Population Perc']] = Population_Merged[['Population (2020)']]\/Population_Merged[['Population (2020)']].sum()\n\nprint(   format(   round(Population_Merged[['Population (2020)']].sum()\/Population[['Population (2020)']].sum() ,3) ) )","dd6c0cb9":"Sample_Size = 1000000\n\nPopulation_Merged = pd.merge(Population_Merged,Aggregated,  on ='Country',  how ='inner')\n\nPopulation_Merged[['Sample size']] = Population_Merged['Population Perc']  \/ Population_Merged['name']*Population_Merged['name'].sum()*Sample_Size\n\nPopulation_toMerge = Population_Merged.loc[:, Population_Merged.columns.intersection(['Country','Sample size'])]\n\ndf = pd.merge(df,Population_toMerge,  on ='Country',  how ='inner')\n\nTotal_Probability = df[['Sample size']].sum()\n\ndf[['Sample size']] = df[['Sample size']] \/ Total_Probability\n\nvec = df[['Sample size']]\n\ndf_sampled = df.sample(n=Sample_Size, weights='Sample size')","8355cd40":"Aggregated = df_sampled[['name','Country']]\nAggregated = Aggregated.groupby(['Country']).agg(['count']).sort_values([('name', 'count')], ascending=False)\nAggregated['Sampled records'] = round(Aggregated[['name']] \/ df_sampled.shape[0],2)\nAggregated.columns = Aggregated.columns.get_level_values(0)\nAggregated.columns = [''.join(col).strip() for col in Aggregated.columns.values]\n\nPopulation_toMerge_2 = Population_Merged.loc[:, Population_Merged.columns.intersection(['Country','Population Perc'])]\n\nAggregated = pd.merge(Aggregated,Population_toMerge_2,  on ='Country',  how ='inner')\nAggregated","7f0f2682":"lat_min, lat_max = 35, 65\n\nidx_europe = (df_sampled[\"longitude\"]>lon_min) &\\\n            (df_sampled[\"longitude\"]<lon_max) &\\\n            (df_sampled[\"latitude\"]>lat_min) &\\\n            (df_sampled[\"latitude\"]<lat_max)\n\ndf_sampled_europe = df_sampled[idx_europe].sample(n=100000)\n\nplt.figure(2, figsize=(12,6))\nm2 = Basemap(projection='merc',llcrnrlat=lat_min,urcrnrlat=lat_max,llcrnrlon=lon_min,\n             urcrnrlon=lon_max,lat_ts=35,resolution='c')\n\nm2.fillcontinents(color='#191919',lake_color='#000000') \nm2.drawmapboundary(fill_color='#000000')                \nm2.drawcountries(linewidth=0.2, color=\"w\")              \n\nmxy = m2(df_sampled_europe[\"longitude\"].tolist(), df_sampled_europe[\"latitude\"].tolist())\nm2.scatter(mxy[0], mxy[1], s=5, c=\"#1292db\", lw=0, alpha=0.05, zorder=5)\n\nplt.title(\"Sample of 100.000 locations in Europe\")\nplt.show()","e094dae5":"# 1.Introduction","07943143":"Alright, number of points more less reflect the population's world density as well. Similarly, I will zoom a bit for Europe.","feb135f1":"Next, I estimate elevation by its local neighbours. We define this function:","30d996bf":"First, I check the missing entries for whole the data set.","d6fc3bfc":"# 2.Data preparation","9ad872d1":"Let's list some cleaning decisions:\n* cc2 and admin codes higher than 1 are to be dropped\n* I will estimate elevation by surroudning areas\n* I drop alternatenames","88f827a1":"In this case, situation is different than expected. Countries like UK, Benelux are not dense enough. Surprisingly, Norway which has very low population received big number of points. I can see that these values do not reflect the population's density then.","43d899b6":"This is not very obvious (at least for me) what all these country shortcuts mean. But I identify them as *universal Alpha-2 code* used in whole the world. I decide to match to them to simple country names by use of translation data in standard ISO-3166, Alpha 2 digits code.","22d1aa5b":"First, I define libraries. Second, the underlying data set with the list of all locations in the Earth is very big. This means ~11 million records. I decide to load take all 11 million rows. And it can be changed to other value below.","04f451c1":"The goal of this notebook is to **prepare universal geographical data** with the use of huge spatial data set.\n\n* I clean the 11 million records data set approximating the values of elevation etc.\n* Second, I plot the created data set and check whether it is reasonable\n* I merge the data set with population information and sample on the basis of this\n\nAs a result I created big data set containing information about location (latitude, longitude), geogrphical elevation and denisty which can be used as auxiliary data set for any other analysis.\n\n**Majority of code was hidden for clarity. Click \"unhide\" to look in there !**","23483691":"The next steps:\n* For each country I divide its population by total one\n* I introduce sample size equals to 1000.000 and I multiply the percentage ratio. In that way every country receives the number of rows it should have\n* This ratio is applied to the main data set to assess how many points should be sampled for each country","8b996923":"Some regions (like desserts and polar areas are absolutely empty. I will apply there world elevation average - it is 840m, more than I expected.","bc7ac66f":"It lets us to calculate the distance between two points by use of Harversine function. Above I checked what is the distance between two points with latitutde and longitude different of Polish territory. It is how I expected around 800 kilometers difference between furthest points. The function works. I will use in the further part of analysis. For now, simple approximation will be enough.\n\nTo approximate elevation I am applying simplified net of latitude and logitude where each value is rounded to full grade (for example: 42.523432 E = ~43 E).\n\nWhat I find, round in that way is still not enough. Data for elevation is so bad, that I decide to round to even (for example: 42.523432 E = ~42 E).","2c8d5f9d":"# 3.Data analysis\n\nFor the memory reasons, let's plot sample equals to 100.000 records.","65e5fe13":"And this is it. The map reflects correctly the denisty thanks to the weighing vector. \n\n**Perfect stage to do further analysis!**","c8be74ab":"I check how much population from one data set was successfully merged after corrections. Ok, 97.6% is enough for me.","518733a2":"USA, China and India or even Mexico make sense being in the top. Norway is not expected. I have an idea: let's use this data set but apply sampling to the countries based on its population. In other words, I use the given points, but I sample it by density. So for example in the above table, both China and India will grow, USA and Mexico will remain high, Norway and other small countries will drop a lot.\n\nI merge the population data to our ISO data, it requires of course some corrections (due to differences between countries' naming). ","ba066cb9":"Alright, my base spatial data is ready for use."}}