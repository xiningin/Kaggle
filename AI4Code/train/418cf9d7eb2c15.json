{"cell_type":{"66f433af":"code","a9478cf3":"code","98dec4a3":"code","fdc20866":"code","9a457bba":"code","340e9c3d":"code","81970942":"code","495f6b78":"code","25e340c4":"code","33c8f81c":"code","707a886b":"code","d99f3941":"code","9e585f1b":"code","544b7278":"code","1f252b9c":"code","619463d7":"code","f5f208e1":"code","58de8bfa":"code","3226bcec":"code","7c18a0b7":"code","5cfad11d":"code","d90b41ad":"code","16dda5b2":"code","af152477":"code","dd319284":"code","d2d30c50":"code","3226046b":"code","23fbbc8b":"code","5026cfa0":"code","c20e67b5":"code","56333417":"code","cdaff450":"code","e7716fb1":"code","addbfa28":"code","f4be6bf3":"code","a846b5ef":"code","09df1dab":"code","5ffc7d6e":"code","f22d637a":"markdown","a857842a":"markdown","91c717ee":"markdown","9cf2c030":"markdown","e624b59f":"markdown","f73aa416":"markdown","7d752e49":"markdown","a2a7eaac":"markdown","296b6c65":"markdown","8380ac5f":"markdown","81320d8d":"markdown","e1355495":"markdown","3a7af4eb":"markdown","49761571":"markdown","8b3dbfa8":"markdown","997ecfc1":"markdown","50464cb4":"markdown","19bd7f90":"markdown","d47a1e25":"markdown","ba9dc7be":"markdown","edb8b513":"markdown","1edc6b35":"markdown","bec66000":"markdown","1ac035cc":"markdown","0bf94045":"markdown"},"source":{"66f433af":"import os\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore') # Mute warnings (thrown from Optuna\/XGBoost)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nfrom pandas.api.types import CategoricalDtype\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor","a9478cf3":"# Preprocessing steps collected together into this one function, then defined seperately in following cells\n\ndef preprocess_data(verbose=False):\n    if verbose:\n        print(\"*** BEGIN PREPROCESSING ***\")\n    data_directory = Path(\"..\/input\/house-prices-advanced-regression-techniques\/\")\n    df, train_indices, test_indices = load(data_directory, verbose)\n    df = clean(df, verbose)\n    df = order_ordinals(df, verbose)\n    df = encode_nominative_categoricals(df, verbose)\n    df = impute(df, verbose)\n\n    df_train = df.loc[train_indices, :] # Reform splits: train\n    df_test = df.loc[test_indices, :] # Reform splits: test\n    \n    if verbose:\n        print(\"*** END PREPROCESSING ***\\n\")\n    return df_train, df_test","98dec4a3":"# LOAD: read and merge\n\ndef load(data_dir, verbose):\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")  # read_csv assumes first column is index, which you can name w\/index_col\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"Id\")  # use the (better be!) unique Ids to merge and split as needed\n    df = pd.concat([df_train, df_test]) # Merge splits, so can preprocess (clean, encode, impute) together\n    if verbose:\n        print(\"Train and test splits read from CSV and merged.\")\n        print(\"Missing values before any preprocessing: \", df.isnull().values.sum())\n    return df, df_train.index, df_test.index","fdc20866":"# CLEAN: Examine data_description.txt further ... Any more cleaning to add here?\n\ndef clean(df, verbose):\n    cleaned_features = [\"Exterior2nd\", \"GarageYrBlt\"]\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})  # df.Exterior2nd.unique() displayed multiple names for BrkComm\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Wd Shng\": \"WdShing\"})  # to match same name in Exterior1st\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"CmentBd\": \"CemntBd\"})  # to match same name in Exterior1st\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt) # replace nonsensical GarageYrBlt values with year house was built\n    \n    # Names beginning with numbers make for messy Python\n    name_pairs = {\"1stFlrSF\": \"FirstFlrSF\",\n                  \"2ndFlrSF\": \"SecondFlrSF\",\n                  \"3SsnPorch\": \"ThreeSeaPorch\",}\n    df.rename(columns=name_pairs, inplace=True)\n    if verbose:\n        print(\"Cleaned: \", cleaned_features, sep='\\n    ')\n        print(\"Renamed (From, To): \", *name_pairs.items(), sep='\\n    ')\n    return df","9a457bba":"# ENCODE ordinal categorical features \n\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\n\n# map each ordinal category to its ORDERED range of possible values (called levels in Pandas)\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels, # one-hot encoding for XGBoost makes score worse\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"], # swapped Maj2 and Maj1 to match data description \n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"], # all but 1 row have all utilities - yet keeping this feature helps both val and lb scores significantly!\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"], # IMHO, Mix better than FuseP but only 1 row has it so doesn't matter\n    \"Fence\": [\"MnPrv\", \"MnWw\", \"GdWo\", \"GdPrv\"], # one-hot-encoding didn't help wood-quality\/privacy-level mix, but moving MnPrv to bottom in order DID help\n}\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()} # Add a None level, which will be assigned for missing values\n\ndef print_n_per_line(items, n):\n    for idx, item in enumerate(items):\n        print(\"    \" + item + ''.join([' ' for i in range(14 - len(item))]), end='')\n        if idx % n == n-1:\n            print()\n    print()\n\ndef order_ordinals(df, verbose):\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels, ordered=True))\n    if verbose:\n        print(\"Created ordinal categories and ordered with levels: \") # a display alternative: , *list(ordered_levels.keys()), sep='\\n    ')\n        print_n_per_line(list(ordered_levels.keys()), 5)\n    return df","340e9c3d":"# ENCODE nominative categorical features\n\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\ndef encode_nominative_categoricals(df, verbose):\n    for name in features_nom: # Nominal categories\n        df[name] = df[name].astype(\"category\")\n        if \"None\" not in df[name].cat.categories: # Add None category for missing values\n            df[name].cat.add_categories(\"None\", inplace=True)\n    if verbose:\n        print(\"Created nominative categories (mostly conversions from numericals): \") # , *features_nom, sep='\\n    '\n        print_n_per_line(features_nom, 5)\n    return df","81970942":"def impute(df, verbose):\n    if verbose:\n        print(\"\\nMissing values before imputing:\")\n        sums_df = pd.DataFrame(df.isnull().sum(), columns = ['Missing Values'])\n        print(sums_df[sums_df['Missing Values'] > 0])\n        total_missing_values = df.isnull().values.sum()\n        print(\"Total missing values: \", total_missing_values)\n    \n    # Some experiments for better imputing (if commented out, they didn't help with XGBoost):\n    df['MSZoning'] = df.groupby(\"Neighborhood\")['MSZoning'].transform(lambda x: x.fillna(x.mode()))\n#     df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median())) # didn't much help or hurt\n#     df['MSZoning'] = df['MSZoning'].fillna(df['MSZoning'].mode()[0])\n#     df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])\n#     df['KitchenQual'] = df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])\n    df['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\n    df['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])\n    df['SaleType'] = df['SaleType'].fillna(df['SaleType'].mode()[0])\n    \n    # the baseline generic strategy for remaining missing values\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n    if verbose:\n        print(\"\\nmissing values imputed: \", total_missing_values, )\n        print(\"missing values remaining after imputing: \", df.isnull().values.sum())\n    return df","495f6b78":"df_train, df_test = preprocess_data(verbose=False) # load, clean, encode, impute","25e340c4":"# Peek at data - uncomment and run individual lines in this cell if you'd like to see what df's contain.\n# Extensive EDA is in a different notebook\n\n# display(df_train)\n# display(df_test)\n\n# Display information about dtypes and missing values\n# display(df_train.info())\n# display(df_test.info())\n\ndisplay(df_train.Functional.unique()) # Note how order is displayed using \"<\" for an ordered category such as 'Functional'\ndisplay(df_train.Functional.value_counts())\n\ndisplay(df_train.head().T.head(40))\ndisplay(df_train.head().T.tail(40))","33c8f81c":"def score_dataset(X, y, model=XGBRegressor()):\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    \n    log_y = np.log(y) # RMSLE (Root Mean Squared Log Error) is metric for Housing competition\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\")\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","707a886b":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nbaseline_score = score_dataset(X, y) # , model=lass\nprint(f\"Baseline score: {baseline_score:.5f} RMSLE\")","d99f3941":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","9e585f1b":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nmi_scores = make_mi_scores(X, y)\nmi_scores","544b7278":"def drop_uninformative(df, mi_scores, verbose=False):\n    if verbose:\n        print('Dropping the following features with low mi_scores:')\n        print(df.loc[:, mi_scores == 0.0].columns)\n    return df.loc[:, mi_scores > 0.0]","1f252b9c":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\nX = drop_uninformative(X, mi_scores, verbose=True)\n\nscore_dataset(X, y)","619463d7":"def label_encode_these(df, cols=[], verbose=False):\n    # Label encoding works great for XGBoost and RandomForest. Models like Lasso\n    # or Ridge work better with one-hot, especially nominal categories.\n    # Categories not passed to label_encode_these will be automatically one-hot encoded.\n    X = df.copy()\n    for colname in cols:\n        X[colname] = X[colname].cat.codes # holds category labels (alternative: sklearn's LabelEncoder)\n    if verbose:\n        print(len(cols), \"categorical features label encoded:\", )\n        print_n_per_line(cols, 5)\n    return X\n\n# The above more fleixbile function works in conjunction with the one_hot_encode_except function\n# that follows, and replaces the function that was initially supplied:\n    \n# def label_encode_all(df, verbose=False):\n#     X = df.copy()\n#     category_columns = X.select_dtypes([\"category\"])\n#     for colname in category_columns:\n#         X[colname] = X[colname].cat.codes\n#     return X","f5f208e1":"def one_hot_encode_except(df, label_encoded_cols=[], verbose=False):\n    # label_encoded_cols is supplied as a parameter because don't want\n    # to one hot encode columns that have already been label encoded. This is because either:\n    # 1) the label encoded columns have a natural order to them (ordinal) OR\n    # 2) through experimentation, found label encoding did better than one-hot (i.e. with XGBoost)\n    X = df.copy()\n    remaining_col_names = []\n    for col_name in list(X):\n        if X[col_name].dtype.name == 'category' and col_name not in label_encoded_cols:\n            remaining_col_names.append(col_name)\n    X = pd.get_dummies(X, columns=remaining_col_names)\n    \n    if verbose:\n        print(\"One Hot encoding applied to remaining\", len(remaining_col_names), \"categorical features:\")\n        print_n_per_line(remaining_col_names, 5)\n    return X","58de8bfa":"def mathematical_transforms(df, verbose=False):\n    X = pd.DataFrame()  # dataframe to hold new features\n    X[\"LivLotRatio\"] = df.GrLivArea \/ df.LotArea\n    X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) \/ df.TotRmsAbvGrd\n    X[\"TotalSF\"] = df.TotalBsmtSF + df.FirstFlrSF + df.SecondFlrSF\n    # following experiments tested poorly with XGboost:\n#     X['YrBuiltPlusRemod'] = df.YearBuilt + df.YearRemodAdd\n#     X['Total_Bathrooms'] = df.FullBath + df.BsmtFullBath + .5 * df.HalfBath + .5 * df.BsmtHalfBath\n#     X[\"TotalRooms\"] = df.TotRmsAbvGrd + df.FullBath + df.HalfBath + df.BsmtFullBath + df.BsmtHalfBath + df.BedroomAbvGr + df.KitchenAbvGr\n#     X[\"TotalOutsideSF\"] = df.WoodDeckSF + df.OpenPorchSF + df.EnclosedPorch + df.Threeseasonporch + df.ScreenPorch\n    new_features = [\"LivLotRatio\", \"Spaciousness\", \"TotalSF\"]\n    if verbose and new_features:\n        print(len(new_features), \"new features created via mathematical transforms: \")\n        print(\"    \", new_features)\n    return X\n\ndef interactions(df, verbose=False):\n    # interactions with GrLivArea that did NOT help include HouseStyle, Neighborhood, OverallQual, MSZoning, LotConfig, YearBuilt, ExterQual, TotRmsAbvGrd\n    # interactions with GrLivArea that DID help according to cross validation, but not leaderboard: BedroomAbvGr\n    new_interactions = [\"Bldg with GrLivArea\"]\n    columns_to_interact_with_GrLivArea = ['BldgType'] # if you add to this list, also add to lists in adjacent lines (above and below)\n    new_prefixes = ['Bldg']\n    X = pd.get_dummies(df[columns_to_interact_with_GrLivArea], columns=columns_to_interact_with_GrLivArea, prefix=new_prefixes)\n    X = X.mul(df.GrLivArea, axis=0)\n\n    if verbose and new_interactions:\n        print(len(new_interactions), \"new interaction features created: \")\n        print(\"    \", new_interactions, \" :\" )\n        for prefix in new_prefixes:\n            print_n_per_line(list(X.columns[X.columns.str.startswith(prefix)]), 5)\n    return X\n\ndef counts(df, verbose=False):\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n        \"ThreeSeaPorch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1)\n    new_counted_features = [\"PorchTypes\"]\n    if verbose and new_counted_features:\n        print(len(new_counted_features), 'new \"feature count\" type features created: ')\n        print(\"    \", new_counted_features)\n    return X\n\ndef break_down(df, verbose=False):\n# This function made sense in Creating Features Exercise 3, but\n# it makes no sense with this data set because\n# 1) in this data set, MSSubClass is a number, not text\n# 2) Even if we replace the number with the text description . . .\n# 3) The extra info is already covered by several other features: HouseStyle, YearBuilt, BldgType\n    X = pd.DataFrame()\n    X[\"MSClass\"] = df.MSSubClass.str.split(\"_\", n=1, expand=True)[0] \n    return X\n\ndef group_transforms(df, verbose=False):\n    X = pd.DataFrame()\n    X[\"MedNeighLivArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n    X[\"MedNeighLotArea\"] = df.groupby(\"Neighborhood\")[\"LotArea\"].transform(\"median\")\n    X[\"MedQualLivArea\"] = df.groupby(\"OverallQual\")[\"GrLivArea\"].transform(\"median\")\n#     X[\"MedQualLotArea\"] = df.groupby(\"OverallQual\")[\"LotArea\"].transform(\"median\")\n#     X[\"MedCondLivArea\"] = df.groupby(\"OverallCond\")[\"GrLivArea\"].transform(\"median\")\n#     X[\"MedYearBuiltLivArea\"] = df.groupby(\"YearBuilt\")[\"GrLivArea\"].transform(\"median\")\n#     X[\"MedGarageCarArea\"] = df.groupby(\"GarageCars\")[\"GarageArea\"].transform(\"median\")\n#     X[\"MedGarageCarArea\"] = df.groupby(\"GarageCars\")[\"GarageArea\"].transform(\"median\")\n\n    new_transforms = [\"MedNeighLivArea\", \"MedNeighLotArea\", \"MedQualLivArea\"]\n    if verbose and new_transforms:\n        print(len(new_transforms), \"new features created via groupby transform of a feature: \")\n        print(\"    \", new_transforms)\n    return X","3226bcec":"# This feature set is often in the 1-3 line descrption of a home for sale on info sheets\n# developed by real estate agents. But didn't help:\n# cluster_features = [\n#     \"BedroomAbvGr\",\n#     \"FullBath\",\n#     \"HalfBath\",\n#     \"GrLivArea\",\n# ]\n\ncluster_features = [\n    \"LotArea\",\n    \"TotalBsmtSF\",\n    \"FirstFlrSF\",\n    \"SecondFlrSF\",\n    \"GrLivArea\",\n]\n\ndef cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new\n\n\ndef cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd","7c18a0b7":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n","5cfad11d":"def pca_inspired(df, verbose=False):\n    X = pd.DataFrame()\n    X[\"TotSqFt\"] = df.GrLivArea + df.TotalBsmtSF  # adding in + df.GarageArea makes worse\n    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF\n#     X[\"NewSqFt\"] = ((df.GrLivArea + df.TotalBsmtSF) \/ df.YearRemodAdd.subtract(2011.5).mul(-1))  # my feature from ex. 5 ... adding GarageArea makes things worse\n#     X[\"BsmtSF_to_AboveGrSF_ratio\"] = df.TotalBsmtSF \/ df.GrLivArea  # my feature from ex. 5 ... hurt more than helped\n    return X\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca\n\npca_features = [\n    \"GarageArea\",\n    \"YearRemodAdd\",\n    \"TotalBsmtSF\",\n    \"GrLivArea\",]","d90b41ad":"def indicate_outliers(df, verbose=False):\n    X_new = pd.DataFrame()\n    X_new[\"Outlier\"] = (df.Neighborhood == \"Edwards\") & (df.SaleCondition == \"Partial\")\n    return X_new","16dda5b2":"# The advantages of using CrossFoldEncoder as opposed to something with no validation like:\n# X[\"MedNeighPrice\"] = X.groupby(\"Neighborhood\")[\"SalePrice\"].transform(\"median\") )\n# is that:\n    # 1) you get crossfold validation which prevents target leakage\n    # 2) you get to choose m, the smoothing factor which makes the model deal better with tiny amounts of data for a specific Neighborhood (for example)\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","af152477":"def drop_unhelpful_features(df, features_to_drop=None, verbose=False):\n    if features_to_drop is None:\n        return df\n    else:\n        if verbose:\n            print(len(features_to_drop), \"features dropped: \")\n            print(\"    \", features_to_drop)\n        return df.drop(features_to_drop, axis=1)\n\n#     def drop_uninformative(df, mi_scores):\n#     print('Dropping the following features with low mi_scores:')\n#     print(df.loc[:, mi_scores == 0.0].columns)\n#     return df.loc[:, mi_scores > 0.0]\n\n# features Boruta-SHAP suggests with XGBoost (must modify this after each time changing feature set or big change to XGBoost HyperParameters):\n\n#     features_to_drop = ['Exterior1st', 'LowQualFinSF', 'Condition2', 'BedroomAbvGr', 'LotShape', 'Electrical', 'BldgType', 'OpenPorchSF', 'WoodDeckSF', 'Condition1', 'BsmtFinType2', 'LotConfig', 'HalfBath', 'Bldg_2fmCon', 'YearBuilt_encoded', 'PavedDrive', 'RoofMatl', 'RoofStyle', 'ScreenPorch', 'Functional', 'BsmtFullBath', 'HouseStyle', 'BsmtCond', 'TotRmsAbvGrd', 'MasVnrType', 'Foundation', 'GarageQual', 'Bldg_TwnhsE', 'Heating', 'Street', 'LivLotRatio', 'Threeseasonporch', 'Bldg_Duplex', 'GarageCond', 'LandSlope', 'MSZoning', 'Exterior2nd', 'MSZoning_encoded', 'BsmtHalfBath', 'GarageYrBlt', 'PoolArea', 'Bldg_None', 'SecondFlrSF', 'SaleType', 'MSSubClass', 'LotFrontage', 'MasVnrArea', 'EnclosedPorch', 'MedNeighLotArea', 'BsmtFinSF2', 'KitchenAbvGr', 'Alley', 'Bldg_Twnhs', 'Neighborhood', 'LandContour', 'Fence', 'TotalBsmtSF', 'Utilities', 'Spaciousness', 'ExterCond']\n#     features_to_drop = ['ExterCond', 'LotFrontage', 'Alley', 'SaleType', 'BedroomAbvGr', 'LandSlope', 'Bldg_Twnhs', 'LotShape', 'LandContour', 'Exterior1st', 'Neighborhood', 'BsmtFinSF2', 'Bldg_2fmCon', 'MSSubClass', 'NewSqFt_encoded', 'CentralAir', 'OpenPorchSF', 'GarageArea', 'BsmtHalfBath', 'LivLotRatio', 'EnclosedPorch', 'Foundation', 'LotConfig', 'BldgType', 'RoofStyle', 'Threeseasonporch', 'RoofMatl', 'Bldg_TwnhsE', 'Spaciousness', 'Fence', 'GarageYrBlt', 'WoodDeckSF', 'Bldg_Duplex', 'Functional', 'SecondFlrSF', 'PavedDrive', 'Bldg_None', 'TotSqFt_encoded', 'TotRmsAbvGrd', 'Electrical', 'Condition2', 'MSZoning_encoded', 'MSZoning', 'BsmtFinType2', 'HalfBath', 'HouseStyle', 'BsmtCond', 'KitchenAbvGr', 'GarageQual', 'GarageCond', 'MedNeighLotArea', 'ScreenPorch', 'YearBuilt_encoded', 'Exterior2nd', 'Condition1', 'MasVnrType', 'BsmtFullBath', 'Heating', 'MasVnrArea', 'Street', 'PoolArea', 'LowQualFinSF', 'BsmtSF_to_AboveGrSF_ratio', 'Utilities']\n#     features_to_drop = ['ExterCond', 'LotFrontage', 'Alley', 'SaleType', 'BedroomAbvGr', 'HeatingQC', 'LandSlope', 'Bldg_Twnhs', 'LotShape', 'LandContour', 'Exterior1st', 'BsmtFinSF2', 'Bldg_2fmCon', 'OverallCond_encoded', 'MSSubClass', 'Neighborhood', 'NewSqFt_encoded', 'CentralAir', 'BsmtHalfBath', 'LivLotRatio', 'EnclosedPorch', 'Foundation', 'LotConfig', 'BldgType', 'RoofStyle', 'Threeseasonporch', 'RoofMatl', 'Bldg_TwnhsE', 'Fence', 'GarageYrBlt', 'Bldg_Duplex', 'PavedDrive', 'Bldg_None', 'TotSqFt_encoded', 'Electrical', 'Condition2', 'MSZoning', 'BsmtFinType2', 'HalfBath', 'HouseStyle', 'BsmtCond', 'KitchenAbvGr', 'GarageQual', 'GarageCond', 'MSZoning_encoded', 'ScreenPorch', 'Condition1', 'Exterior2nd', 'MasVnrType', 'Heating', 'MasVnrArea', 'Street', 'PoolArea', 'LowQualFinSF', 'BsmtSF_to_AboveGrSF_ratio', 'Utilities']\n#     features_to_drop = ['BsmtFullBath', 'LivLotRatio', 'LotShape', 'PavedDrive', 'BsmtSF_to_AboveGrSF_ratio', 'Spaciousness', 'EnclosedPorch', 'MSZoning', 'SaleType', 'RoofMatl', 'MSZoning_encoded', 'MSSubClass', 'Utilities', 'GarageQual', 'TotRmsAbvGrd', 'YearBuilt_encoded', 'LotConfig', 'RoofStyle', 'Bldg_Twnhs', 'Bldg_Duplex', 'MasVnrArea', 'HouseStyle', 'Bldg_2fmCon', 'BsmtFinType2', 'Condition1', 'LotFrontage', 'MasVnrType', 'Exterior1st', 'Foundation', 'HalfBath', 'GarageYrBlt', 'Fence', 'NewSqFt_encoded', 'KitchenAbvGr', 'BldgType', 'ExterCond', 'MedNeighLotArea', 'Exterior2nd', 'LandSlope', 'ScreenPorch', 'Bldg_TwnhsE', 'SecondFlrSF', 'Heating', 'Neighborhood', 'YearRemodAdd', 'LowQualFinSF', 'GarageCond', 'BsmtFinSF2', 'PoolArea', 'BedroomAbvGr', 'WoodDeckSF', 'TotSqFt_encoded', 'Threeseasonporch', 'LandContour', 'Condition2', 'BsmtHalfBath', 'Alley', 'BsmtCond', 'Street', 'Bldg_None', 'Electrical'] # 20 iterations BS\n#     features_to_drop =['Bldg_Duplex', 'Bldg_TwnhsE', 'Exterior1st', 'MasVnrArea', 'BsmtCond', 'Electrical', 'BedroomAbvGr', 'TotSqFt_encoded', 'TotRmsAbvGrd', 'EnclosedPorch', 'LivLotRatio', 'ExterCond', 'MSZoning', 'PorchTypes', 'Condition2', 'MedNeighLotArea', 'LotConfig', 'LowQualFinSF', 'LotFrontage', 'Foundation', 'Threeseasonporch', 'Utilities', 'BsmtHalfBath', 'WoodDeckSF', 'Neighborhood', 'ScreenPorch', 'SaleType', 'GarageQual', 'Bldg_Twnhs', 'LotShape', 'Street', 'Alley', 'Condition1', 'HalfBath', 'BsmtFinType2', 'MSSubClass', 'RoofStyle', 'BldgType', 'Exterior2nd', 'PavedDrive', 'LandContour', 'HouseStyle', 'Heating', 'LandSlope', 'KitchenAbvGr', 'NewSqFt_encoded', 'Bldg_None', 'OpenPorchSF', 'Fence', 'MasVnrType', 'FullBath', 'Spaciousness', 'BsmtFinSF2', 'GarageCond', 'BsmtSF_to_AboveGrSF_ratio', 'PoolArea', 'Bldg_2fmCon', 'BsmtFullBath', 'RoofMatl'] # 12 iterations BS, RS=2\n\n# hand picked by JG\n#     features_to_drop = ['GarageCond', 'RoofStyle'] # hand picked by JG - didn't help\n\n# XGBoost's low feature importances\n# features_to_drop = ['Street', 'Threeseasonporch', 'PoolQC', 'Condition2', 'PoolArea', 'Heating', 'Bldg_2fmCon'] # 7 least important features according to XGBoost importances\n# features_to_drop = ['Street', 'Threeseasonporch', 'Condition2'] # these 3 of the 4 features XGboost rated has having low importances gave best results\n","dd319284":"def create_features(df, df_test=None, verbose=False):\n    if verbose:\n        print(\"*** BEGIN FEATURE ENGINEERING PIPELINE ***\\n\")\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    mi_scores = make_mi_scores(X, y)\n\n    # Combine splits (so test data gets feature engineering too)\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    # Lesson 2 - Mutual Information\n    # X = drop_uninformative(X, mi_scores, verbose)  # XGboost makes use of some of the info in these \"uninformative\" features\n\n    # Lesson 3 - Transformations\n    X = X.join(mathematical_transforms(X, verbose))\n    X = X.join(interactions(X, verbose))\n    X = X.join(counts(X, verbose))\n    # X = X.join(break_down(X, verbose))\n    X = X.join(group_transforms(X, verbose))\n\n    # Lesson 4 - Clustering\n    # X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n    # X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n\n    # Lesson 5 - PCA\n    X = X.join(pca_inspired(X, verbose))\n    # X = X.join(pca_components(X, pca_features))\n    # X = X.join(indicate_outliers(X))\n    \n    # Reform splits (so target encoding can use entire data set for cross validation)\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n    \n    # Lesson 6 - Target Encoder  \n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    columns_to_encode = [\"MSSubClass\", \"Neighborhood\", \"YrSold\", ]\n    # tested \"MSZoning\",\"OverallCond\", \"YearBuilt\", \"TotSqFt\", \"NewSqFt\" : led to worse results w XGBoost\n    # \"MoSold\" tested: important but no score change w XGBoost\n    X = X.join(encoder.fit_transform(X, y, cols=columns_to_encode))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n    if verbose:\n        print(len(columns_to_encode), \"features target encoded (with crossfold validation): \")\n        print(\"    \", columns_to_encode)\n        \n    # Combine splits (so test data gets feature engineering too)\n    if df_test is not None:\n        X_test = X_test.copy()\n        X = pd.concat([X, X_test])\n        \n    # Mutual Information is usually too crude. So ....\n    # drop features based on EDA, a feature selection algorithm, or (post fit) model feature importances\n    # This is late in pipeline in case an earlier step extracts info from a feature to later be dropped.\n    X = drop_unhelpful_features(X, features_to_drop=['Street', 'ThreeSeaPorch', 'Condition2'], verbose=verbose)\n\n    # choose to label encode 0, some, or all category features depending on which algorithm will train data\n    # label_encode_columns = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope', 'LotShape', 'PavedDrive', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond'] # can't label enclode 'YrSold' and 'MoSold' with cat.codes unless I make them into categories first\n    # label_encode_columns = [] # label encoding for no category features - all will be one hot encoded.\n    label_encode_columns = list(X.select_dtypes([\"category\"])) # label encoding for all category features - good for tree ensemble algos such as XGBoost\n    # label_encode_columns.remove('Fence') #one-hot encode just this one\n    # final_list = list(set(item_list) - set(list_to_remove)) # if need to remove a bunch of features, do it this way\n    X = label_encode_these(X, cols=label_encode_columns, verbose=verbose) \n    X = one_hot_encode_except(X, label_encoded_cols=label_encode_columns, verbose=verbose) # use for linear regression, SVM, etc. especially for nominal (unordered) categories\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    if verbose:\n        print(\"*** END FEATURE ENGINEERING PIPELINE ***\\n\")\n        \n    if df_test is not None:\n        return X, X_test\n    else:\n        return X","d2d30c50":"df_train, df_test = preprocess_data(verbose=True) # load, clean, encode, impute\n# # drop outliers here. Example:\n# df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\n    \nX_train, X_test = create_features(df_train, df_test, verbose=True)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nprint(\"\\nX_train shape: \", X_train.shape)\nprint(\"\\nX_test shape: \", X_test.shape)\nprint(\"cross validiation score: \", score_dataset(X_train, y_train))\n# X_train.head()\n# X_train.filter(like='Neigh', axis=1).head()","3226046b":"# before any feature engineering, mi_scores were:\n\n# OverallQual     0.571457\n# Neighborhood    0.526220\n# GrLivArea       0.430395\n# YearBuilt       0.407974\n# LotArea         0.394468\n#                   ...   \n# PoolQC          0.000000\n# MiscFeature     0.000000\n# MiscVal         0.000000\n# MoSold          0.000000\n# YrSold          0.000000\n# Name: MI Scores, Length: 79, dtype: float64\n\n# post feature enginnering mi_scores:\nmi_scores = make_mi_scores(X_train, y)\nmi_scores","23fbbc8b":"# !pip install BorutaShap","5026cfa0":"# import pprint\n# import joblib\n# from functools import partial\n\n# from BorutaShap import BorutaShap","c20e67b5":"# %%time\n# # params = dict([('colsample_bytree', 0.11807135201147481),\n# #                ('learning_rate', 0.03628302216953097),\n# #                ('max_depth', 3),\n# #                ('n_estimators', 1000), # tried 10,000 but took way too long! 1000 is 3m\/iteration so feasible - could even bump to 2000\n# #                ('reg_alpha', 23.13181079976304),\n# #                ('reg_lambda', .0008746338866473539),\n# #                ('subsample', 0.7875490025178415)])\n# params = {'max_depth': 4, 'learning_rate': 0.008756709153431472, 'n_estimators': 3508, 'min_child_weight': 2, 'colsample_bytree': 0.2050378195385253, 'subsample': 0.40369887914955715, 'reg_alpha': 0.3301567121037565, 'reg_lambda': 0.046181862052743}\n\n# model = XGBRegressor(random_state=2, objective='reg:squarederror', **params)\n# Feature_Selector = BorutaShap(model=model,\n#                               importance_measure='shap', \n#                               classification=False)\n\n# Feature_Selector.fit(X=X_train, y=np.log(y_train), n_trials=12, random_state=2)  # tried both y=np.log(y_train) and y=y_train . . . log version worked slightly better","56333417":"# Feature_Selector.plot(which_features='all', figsize=(48,12))","cdaff450":"# Feature_Selector.Subset()","e7716fb1":"# %%time\n# import optuna\n\n# X_train = create_features(df_train)\n# y_train = df_train.loc[:, \"SalePrice\"]\n\n# def objective(trial):\n#     xgb_params = dict(\n#         max_depth=trial.suggest_int(\"max_depth\", 3, 5),\n#         learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n#         n_estimators=trial.suggest_int(\"n_estimators\", 2000, 5000),\n#         min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 3),\n#         colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, .7),\n#         subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n#         reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n#         reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n#     )\n#     xgb = XGBRegressor(**xgb_params, random_state = 42) # , tree_method = 'gpu_hist'\n#     return score_dataset(X_train, y_train, xgb)\n\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=20)\n# xgb_params = study.best_params\n# xgb_params","addbfa28":"# xgb_params = dict(\n#     max_depth=4,           # maximum depth of each tree - try 2 to 10\n#     learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n#     n_estimators=2800,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n#     min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n#     colsample_bytree=0.5,  # fraction of features (columns) per tree - try 0.2 to 1.0\n#     subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n#     reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n#     reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n#     num_parallel_tree=1,   # set > 1 for boosted random forests\n# )\n        \n# v9 trial 3\/10, 4508 n_est, val 0.11651, leaderboard = 0.11999 (val slightly better, leaderboard same as before I put random state 42 - changing random state doesn't have much impact)\n# v10 trial 3\/10, 3508 n_est, val 0.11655, leaderboard = 0.11988\n# v19 trial 3\/10, 3300 n_est, val 0.11657, leaderboard = 0.11993\n# v20 trial 3\/10, 3800 n_est, val 0.11649, leaderboard = 0.11994 (m=1 for target encoding on this and prior models)\n# v21 trial 3\/10, 3800 n_est, val 0.11644, leaderboard = 0.12030 (m=2 for target encoding)\n# v22 trial 3\/10, 3800 n_est, val 0.11662, leaderboard = 0.12024 (m=0 for target encoding) - so looks like m=1 is best\n# v25 trial 3\/10, 3508 n_est, val 0.11655, leaderboard = 0.11972 (m=1 for target encoding), random state fixed\n# v26 same as v25 except DF sorted by SaleCondition first val 0.11655, leaderboard=.44368\n# why v26 sort drastic  impact on leaderboard? Because joining than resplitting test\/train screws up test rows order if train rows order changed,\n# which causes house values to be assigned to the wrong rows in submission file\n# v29 add some target encodings, trial 3\/10, 3508 n_est, val 0.11630, leaderboard = 0.12144\n# v31 after Boruta-SHAP feature selection(YES log y), trial 3\/10, 3508 n_est, val 0.11992, leaderboard = 0.12373 pruning feautures hurt score with existing hyperP.\n# v35 after Boruta-SHAP feature selection (NO log y), trial 3\/10, 3508 n_est, val 0.12299, leaderboard = 0.12387 little difference between using log (or not) of y\n# v37 trial 3\/10 after BS (YES log y, 20 BS trials), val .12053, leaderboard: 0.12219\n# v38 trial 3\/10 after BS (YES log y, 12 BS trials, dropped 59 features), val 0.11862, leaderboard: 12420\n# v43 trial 3\/10 similar to v25 with a couple of added veatures, val: 11779, leaderboard: 0.12101\n# v44 trial 3\/10 almost same as v25 (no neighorhood median sales price), val: 0.11643, leaderboard=12169\n# v45 t 3\/10 (with neigh median sales price) val 0.11631, leaderboard=0.12132 so something else is different from best score\n# v46 t 3\/10 same as v25 except kept the 5 features I have always been dropping due to mutal information score 0.0, val = 0.11719, leaderboard=0.11906 NEW RECORD!\n# v47 t 3\/10 but only features with few values are dropped: ['PoolQC', 'MiscFeature', 'Fence'], val = 0.11724, leaderboard=0.11957\n# v48 t 3\/10 but only 2 of 3 features with few values are dropped: ['PoolQC', 'MiscFeature'], val = 0.11712, leaderboard=0.11978\n# v49 t 3\/10 but only 'Fence' feature dropped, val = 0.11752, leaderboard=0.11883 (perhaps 'fence' throws off XGboost slightly, at least for leaderboard)\n# v50 drop only ['Fence', 'Street', 'Threeseasonporch'] as inspired by XGBoost importances, val=0.11736, leaderboard=0.11854 ANOTHER RECORD !!\n# v51 drop only ['Street', 'Threeseasonporch', 'PoolQC'], inspired by XGBoost import., val=.11704, leaderboard: 0.11873\n# v52 drop only ['Street', 'Threeseasonporch'], val: 0.11755, leaderboard=0.11854 (TIED - so dropping Fence doesn't hurt or help. Weird given that it's of modest importance)\n# v53 drop 7 features that XGBoost ranks least important, val: 0.11794, leaderboard: 0.11913 a hair worse than leave all features in\n# v54 drop only ['Street', 'Threeseasonporch', 'Condition2'], val: 0.11706, leaderboard: 0.11838 BEAT PRIOR RECORD - using 3 of the 4 least important features according to XGboost\n# v57 same as v54 but dropped 2 outliers, val: 0.11256, leaderboard: 0.12210 hugely worse. WHY??? had to change y to y_train before final fit - does that have anything to do with it?\n# v58 added new feature TotalSF val: 11653, leaderboard: 0.11773 FEATURE ADD is BIG WINNER!\n# v59 added TotalRooms feature val: 11750, leaderboard: 0.11918 this feature made things a lot worse. out it goes.\n# v60 tested TotalOutsideSF feature val: 11722, leaderboard: 0.11882 another bad feature for XGboost\n# v61 test Total_Bathrooms feature val: 0.11664, leaderboard: 0.11824 nope\n# v62 test YrBuiltPlusRemod feature val: 0.11730, leaderboard: 0.11859 nope\n\n# new notebook\n# v4 val: 11703 instead of val 11653 with exact same setup as v58. The only thing I changed was that I called create_features() with both train and test.\n# This causes validation score to come out differently, even though submission to leaderboard is identical and leaderboard score is identical.\n# Why? Very confused - is this due to a change in random state? Is it possible that all validations scores going forward will come out closer to LB score?\n# v5 one-hot encoding makes XGBoost worse with features going from 92-->377, val: 11988, leaderboard: 0.12599 though maybe just needs more estimators to deal with so many features?\n# v8 test lot frontage by neighborhood imputation, val 11708, lb 0.11781 - so hurt a tiny bit; enough to be noise\n# v9 test imputing mode for a bunch of features, val 11698, lb 0.11775 - hardly changed\n# v10 tests imputing modes for 2 exterior features, val 0.11703 lb 0.11770\n# v11 impute mode ext1 only, val 0.11703, lb 0.11771\n# v12 impute mode ext2 only, val 0.11703, lb 0.11773\n# v13 impute mode 2 Exts, SaleType, val 0.11703, lb 0.11770\n# v15 25 label encoded features b4 one hot, val: 0.11825, lb 0.12210 an improvement over v5 when the 25 features were not label encoded\n# v17 different method for label encoding 25 features b4 one-hot, val .11773, lb 0.12015 which is pretty good for 256 features on XGBoost!\n# what is weird though is why should this be any different from v15? does method of doing label encoding (pandas vs scikit learn) actually matter?\n# v19 impute neighborhood mode for MSZoning NAs val: 11703, lb 0.11770 (no impact?)\n# v20 fix ordering of Functional feature's values: val 11703, lb 0.11735 - wow, even though no val change, lb improved a signficantly from minor change!\n# v21 dropped Utilities val 11743 lb 0.11847 - wow! has just 1 row without full utilities yet has dramatic impact!\n# v22 hot encode BsmtQual val 11703 lb 0.11735 - whoops - coding error didn't one hot encode\n# v23 hot encode BsmtCond val 11703 lb 0.11735 - whoops - coding error didn't one hot encode\n# v25 hot encode BsmtQual val 11722 lb 0.11934 so one hot encode was way worse\n# v26 changed Electrical order slightly val 11703 lb 0.11735 - no difference which is not surprising as only 1 row had mixed\n# v27 one-hot encode Fence val 0.11668, 0.11873 - lb got worse even though validation was better. odd.\n# v28 changed Fence order val 0.11702 lb 0.11725 a tiny boost\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.0088, 'n_estimators': 3400, 'min_child_weight': 2, 'colsample_bytree': 0.205, 'subsample': 0.4037, 'reg_alpha': 0.3301567121037565, 'reg_lambda': 0.046181862052743}  \n# v42 tweaked favorite (just above) val 0.11701 lb 0.11746 slightly worse on lb\n# optuna: 11807 (with GPU)\n# # the one I've used for over a week:\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.008756709153431472, 'n_estimators': 3508, 'min_child_weight': 2, 'colsample_bytree': 0.2050378195385253, 'subsample': 0.40369887914955715, 'reg_alpha': 0.3301567121037565, 'reg_lambda': 0.046181862052743}  \n\n\n\n# xgb_params = {'max_depth': 3, 'learning_rate': 0.02114934913088701, 'n_estimators': 2523, 'min_child_weight': 2, 'colsample_bytree': 0.5547814801304602, 'subsample': 0.5239731194871952, 'reg_alpha': 0.00026074805799699554, 'reg_lambda': 0.006531563710312441}\n# trial 2\/10 optuna: 0.11713 without generating test set\n# v29 new hyperP 2\/10 val: 0.11699, lb: 0.12258 worse than expected lb given good val. Maybe the prior one is overfitted to lb? This was just out of 10 trials so I'll see what happens with 200 trials overnight.\n\n# v30 optuna 0.11336 trial 164\/200, val 0.11394, lb 0.11935\n# xgb_params ={'max_depth': 4, 'learning_rate': 0.006698471590173297, 'n_estimators': 4567, 'min_child_weight': 2, 'colsample_bytree': 0.24129246054168077, 'subsample': 0.44513118412794983, 'reg_alpha': 0.0004519705422846089, 'reg_lambda': 0.00020474392345395197}\n\n# v31 optuna 0.11363 Trial 155\/200, val 0.11404, lb 0.11936\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.006831727298305696, 'n_estimators': 4621, 'min_child_weight': 2, 'colsample_bytree': 0.2566112440621832, 'subsample': 0.4447205088335255, 'reg_alpha': 0.0007900851888091328, 'reg_lambda': 0.00013972524409906255}\n\n# v32 optuna 0.11386 Trial 124\/200, val 0.11385, lb 0.11950\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.005195780138622257, 'n_estimators': 4886, 'min_child_weight': 2, 'colsample_bytree': 0.2208822800528527, 'subsample': 0.6144261364605068, 'reg_alpha': 0.00421214459632269, 'reg_lambda': 0.000591878759903201}\n\n# v33 optuna 0.11492 trial 27\/200 val 0.11562, lb 0.11945\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.012805027367186405, 'n_estimators': 4794, 'min_child_weight': 2, 'colsample_bytree': 0.20165415726310923, 'subsample': 0.47523199921737347, 'reg_alpha': 0.00033877374644280514, 'reg_lambda': 0.31708616538063455}\n\n# # v34 optuna 0.11488 trial 44\/200 val 11450, lb 0.12194\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.017081472889643453, 'n_estimators': 4589, 'min_child_weight': 2, 'colsample_bytree': 0.27559556105601635, 'subsample': 0.5631124749718487, 'reg_alpha': 0.0052208767846268775, 'reg_lambda': 0.07070285925896815}\n\n# v35 optuna 0.11436 trial 63\/200 val 0.11430, lb 0.12015\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.008067194679663887, 'n_estimators': 4917, 'min_child_weight': 2, 'colsample_bytree': 0.23380166692436696, 'subsample': 0.6182032462404947, 'reg_alpha': 0.0030756455512833247, 'reg_lambda': 0.00848093006044966}\n\n# v36 optuna 0.11437 trial 70\/200 val 0.11453 lb 0.12017 \n# xgb_params = {'max_depth': 4, 'learning_rate': 0.004152609847352432, 'n_estimators': 4928, 'min_child_weight': 2, 'colsample_bytree': 0.24500537489579363, 'subsample': 0.7162925206124766, 'reg_alpha': 0.00935417891464248, 'reg_lambda': 0.0017248422134794907}\n\n# v37 optuna 0.11392 trial 129\/200 val  0.11412 lb 0.11981\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.007247015583102812, 'n_estimators': 4910, 'min_child_weight': 2, 'colsample_bytree': 0.2186797801083439, 'subsample': 0.5730531634592627, 'reg_alpha': 0.001428890225504756, 'reg_lambda': 0.0006153299687041702}\n\n# v38 optuna 0.11350 trial 198\/200 val 0.11361, lb 0.11907\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.007081042884678392, 'n_estimators': 4491, 'min_child_weight': 2, 'colsample_bytree': 0.2233667118614051, 'subsample': 0.4572573800301871, 'reg_alpha': 0.00033322844976968274, 'reg_lambda': 0.00016251307640631252}\n\n# v39 optuna .11364 trial 193\/200 val 0.11379, lb 0.11855 finally one that is getting close - should tweak?\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.0064678641044469114, 'n_estimators': 4460, 'min_child_weight': 2, 'colsample_bytree': 0.22290140130032263, 'subsample': 0.45774929968395833, 'reg_alpha': 0.0003479070886226114, 'reg_lambda': 0.00017416758313069236}\n\n# v40 trial 193\/200 hyperP tweaks val .11357, lb 0.11889 made lb score worse\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.0068, 'n_estimators': 3800, 'min_child_weight': 2, 'colsample_bytree': 0.22290140130032263, 'subsample': 0.45774929968395833, 'reg_alpha': 0.0003479070886226114, 'reg_lambda': 0.00017416758313069236}\n\n# v41 optuna 0.11355 trial 183\/200 val 0.11370, lb 0.11915\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.007263975459755825, 'n_estimators': 4657, 'min_child_weight': 2, 'colsample_bytree': 0.22275346430359858, 'subsample': 0.4412986244746556, 'reg_alpha': 0.0001939620035983133, 'reg_lambda': 0.00010185983943986789}\n\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.00602, 'n_estimators': 4100, 'min_child_weight': 2, 'colsample_bytree': 0.22, 'subsample': 0.45, 'reg_alpha': 0.00035, 'reg_lambda': 0.0002}\n# v43 trial 193\/200 hyperP tweaks val 0.11408, optuna .11364, lb 0.11893 slightly worse\n# v44 trial 193\/200 hyperP tweaks val 0.11397, optuna .11364, lb 0.11828 slightly better with lower learning rate of .0062\n# v45 trial 193\/200 hyperP tweaks val 0.11423, optuna .11364, lb 0.11811 with lower learning rate of .0058, n_est 3600\n# v46 trial 193\/200 hyperP tweaks val 0.11401, optuna .11364, lb 0.11825 with learning_rate .0058, n_est 4600\n# v47 trial 193\/200 hyperP tweaks val 0.11423, optuna .11364, lb 0.11802 with learning_rate .0059, n_est 3700\n# v48 trial 193\/200 hyperP tweaks val 0.11435, optuna .11364, lb 0.11806 with learning_rate .0059, n_est 3300\n# v49 trial 193\/200 hyperP tweaks val 0.11503, optuna .11364, lb 0.11908 with learning_rate .0051, n_est 3300\n# v50 trial 193\/200 hyperP tweaks val 0.11413, optuna .11364, lb 0.11805 with learning_rate .00605, n_est 3800\n# v51 trial 193\/200 hyperP tweaks val 0.11410, optuna .11364, lb 0.11795 with learning_rate .00602, n_est 4100\n\n# # the great Hyperparameters I've used for over a week and can't seem to beat:\n# xgb_params = {'max_depth': 4, 'learning_rate': 0.008756709153431472, 'n_estimators': 3508, 'min_child_weight': 2, 'colsample_bytree': 0.2050378195385253, 'subsample': 0.40369887914955715, 'reg_alpha': 0.3301567121037565, 'reg_lambda': 0.046181862052743}  \n# v28 changed Fence order val 0.11702 lb 0.11725 a tiny boost\n\n# v52 favorite hyperP n_est 3450 val 11706 lb 0.11727\n# v53 favorite hyperP n_est 3530 val 11703 lb 0.11726\n# v54 favorite hyperP n_est 3495 val 0.11702 lb 0.11725\n# v55 favorite hyperP n_est 3480 val 0.11702 lb 0.11726\n# v56 favorite hyperP n_est 3515 val 0.11702 lb 0.11724 tiny improvement, so 3515 is new n_est\n# v57 favorite hyperP lrn_rate .00875 val 0.11684 0.11684104079783438    lb 0.11717 NEW RECORD !\n# v58 favorite hyperP lrn_rate .00873 val 0.11676 lb 0.11723\n# v59 favorite hyperP subsample .4 val 0.11666 lb 0.11779\n# v60 favorite hyperP subsample .41 val 0.11655 lb 0.11745\n# v61 favorite hyperP subsample .4037 val 11682 lb 0.11722\n# v62 favorite hyperP dropped PoolQC, PoolArea val 0.11698 lb 0.11833\n# v63 favorite hyperP dropped Fence val 0.11703 lb 0.11776\n# v64 removed interaction features val 11729 lb 0.11827\n# v65 2 interaction features val 0.11742 lb 0.11730 slightly worse\n# v66 HouseType interaction features val 0.11663 lb 0.11848 bit worse - likes other interaction better\n# v67 NH, Bldg interaction features val 0.11709 lb 0.11799\n# v68 MSZ, Bldg interaction features val 0.11689 lb 0.11909\n# v69 BRAbvGr, Bldg interaction features val 0.11546 lb 0.11775 weird that saw big improvement with val, but worse with lb\n# v70 is v69 less 3 least used BRAG ftrs val .11600, lb 0.11813\n# v71 added OverallQual group transform val 0.11670, lb 0.11677 *** NEW RECORD ***\n# v72 added OverallCond group transform val 0.11626, lb 0.11705\n# v74 is v71 with more data cleaning val 0.11670 lb 0.11677 (as expected, data clean change had no effect)\n# v75 MedQualLotArea group transform val 0.11626 lb 0.11770\n# v76 MedYearBuiltLivArea group transform val 0.11678 lb 0.11725\n# v77 MedGarageCarArea group transform val 0.11678 lb 0.11734\n# v78 MoSold target encoding val 0.11671 lb 0.11681 very high importance yet was a (very) slight hit to score. Should I keep?\n# v79 is v78 with slight tinkering HyperP val .11652 lb 0.11680 'learning_rate': 0.00878, 'n_estimators': 3400\n# v80 specified early_stopping_rounds=300 val 0.11670 lb 0.11677 whoops - added fit_params in score_dataset, but not elsewhere when fitting model\n# v81 specified early_stopping_rounds=50 val 0.11670 lb 0.11677 whoops - added fit_params in score_dataset, but not elsewhere when fitting model\n# v82 early_stopping_rounds: 300 val 0.11670 lb b 0.11677 - was same score anyway after doing it correctly\n# v83 early_stopping_rounds: 50 val 0.11670 lb 0.11677 seemed identical home prices. Not sure if I got everything right with whether to do rmse or rmsle or log_y, etc.\n# v84 cluster_labels Kmeans n=20 val 0.11634 lb 0.11718\n# v85 cluster_labels Kmeans n=10 val 0.11618, lb 0.11751\n# v86 cluster_labels Kmeans n=30 val 0.11621 lb 0.11700\n# v87 cluster_labels Kmeans n=50 val 0.11649 lb 0.11738\n# v88 cluster_labels Kmeans n=14 val 0.11615 lb 0.11701\n# v89 MoSold_enc replaces MoSold val 0.11577 lb 0.11703 another instance of improving val, worse lb\n# v90 YrSold_enc val 0.11721 lb 0.11653 NEW RECORD\n# v91 YrSold_enc replaces YrSold val 0.11644 lb 0.11667 a hair worse\n# v92 OverallQual_enc val .11714 lb 0.11657\n# v93 OverallQual_enc replaces OverallQual val 0.11707 lb 0.11747\n# v94 OverallCond_enc val 0.11751 lb 0.11752\n# v95 cluster_labels Bed\/Bath\/SQFT n=20 val 0.11721 lb whoops - cluster was commented out\n# v96 cluster_labels Bed\/Bath\/SQFT n=20 val 0.11721 lb 0.11756","f4be6bf3":"%%time\n\ndf_train, df_test = preprocess_data() # load, clean, encode, impute\nX_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb_params = {'max_depth': 4, 'learning_rate': 0.00875, 'n_estimators': 3515, 'min_child_weight': 2, 'colsample_bytree': 0.2050378195385253, 'subsample': 0.40369887914955715, 'reg_alpha': 0.3301567121037565, 'reg_lambda': 0.046181862052743}\n\nxgb = XGBRegressor(**xgb_params, random_state=42)\n\nprint(\"\\nX_train shape: \", X_train.shape)\nprint(\"X_test shape: \", X_test.shape)\n\nprint(\"score: \", score_dataset(X_train, y_train, xgb))","a846b5ef":"# fit the model to entire dataset and then save to submission file so it can be submitted to leaderboard\n\ndf_train, df_test = preprocess_data() # load, clean, encode, impute\nX_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb = XGBRegressor(**xgb_params, random_state=42)\n# XGB minimizes MSE, but competition loss is RMSLE\n# So, we need to log-transform y to train and exp-transform the predictions\n\nxgb.fit(X_train, np.log(y_train))\npredictions = np.exp(xgb.predict(X_test))\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","09df1dab":"output.head()","5ffc7d6e":"# feature importances from the XGBoost algorithm\n\nfeature_important = xgb.get_booster().get_score(importance_type='weight')  # 'weight' seems to produce most sensible results, 'gain' is nearly as good, while 'cover' is horrible\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\ndata.plot(kind='barh', figsize = (6,24))","f22d637a":"We can reuse this scoring function anytime we want to try out a new feature set. We'll run it now on the processed data with no additional features and get a baseline score:","a857842a":"## Create Final Feature Pipeline Function ##\n\nCombine all preprocessing and feature engineering into one pipeline function. Comment in or out individual lines to see whether it hurts or helps a given model. Also modify or add other types of feature engineering or transformations, keeping it tidy within small, testible functions.\n\nAfter much experimentintation, the best performing combination will remain.","91c717ee":"## Establish Baseline ##\n\nFinally, let's establish a baseline score to judge our feature engineering against.\n\nHere is the function created in Lesson 1 that computes cross-validated RMSLE score for a feature set. XGBoost was used throughout, but this framework can certainly be used to experiment with other models.\n","9cf2c030":"## Encode ##\n\nConvert to expected formats so functions, transformations, and models work correctly\/consistently.\n\nNumeric features already encoded correctly (`float` for continuous, `int` for discrete)\nCategoricals need work. Some int type classes (i.e. MSSubClass) are really nominative categoricals.\n* ordinal - ordered such as poor, fair, typical\/average, good, excellent\n* nominative - unordered such as eye color (whether or not initially coded with strings or ints)\n","e624b59f":"Here are some ideas for other transforms you could explore:\n- Interactions between the quality `Qual` and condition `Cond` features. `OverallQual`, for instance, was a high-scoring feature. You could try combining it with `OverallCond` by converting both to integer type and taking a product.\n- Square roots of area features. This would convert units of square feet to just feet.\n- Logarithms of numeric features. If a feature has a skewed distribution, applying a logarithm can help normalize it.\n- Interactions between numeric and categorical features that describe the same thing. You could look at interactions between `BsmtQual` and `TotalBsmtSF`, for instance.\n- Other group statistics in `Neighboorhood`. We did the median of `GrLivArea`. Looking at `mean`, `std`, or `count` could be interesting. You could also try combining the group statistics with other features. Maybe the *difference* of `GrLivArea` and the median is important?\n\n## k-Means Clustering ##\n\nThe first unsupervised algorithm we used to create features was k-means clustering. We saw that you could either use the cluster labels as a feature (a column with `0, 1, 2, ...`) or you could use the *distance* of the observations to each cluster. We saw how these features can sometimes be effective at untangling complicated spatial relationships.","f73aa416":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum\/221677) to chat with other Learners.*","7d752e49":"## Imports and Configuration ##","a2a7eaac":"A label encoding is okay for any kind of categorical feature when you're using a tree-ensemble like XGBoost, even for unordered categories. If you wanted to try a linear regression model (also popular in this competition), you would instead want to use a one-hot encoding, especially for the features with unordered categories.\n\n## Create Features with Pandas ##\n\nThis cell reproduces the work you did in Exercise 3, where you applied strategies for creating features in Pandas. Modify or add to these functions to try out other feature combinations.","296b6c65":"And here are transforms that produce the features from the Exercise 5. You might want to change these if you came up with a different answer.\n","8380ac5f":"These are only a couple ways you could use the principal components. You could also try clustering using one or more components. One thing to note is that PCA doesn't change the distance between points -- it's just like a rotation. So clustering with the full set of components is the same as clustering with the original features. Instead, pick some subset of components, maybe those with the most variance or the highest MI scores.\n\nFor further analysis, look at a correlation matrix for the dataset, as groups of highly correlated features often yield interesting loadings. EDA for this data set is done in a seperate notebook, which includes a correlation matrix and heat map.","81320d8d":"## Impute ##\n\nHandle Missing Values with Imputation\n\nThe \"starter\" imputation function (last few lines of cell below)\n* imputes 0 for missing numeric values\n* imputes \"None\" for missing categorical values\n\nImprovements:\n* using mode for some missing values of categorical features\n* using mean for same neighborhood for features where it seems inutitively warranted\n\nPotential further improvements:\n* creating missing value indicators (1 when value imputed, 0 otherwise)","e1355495":"### PCA Application - Indicate Outliers ###\n\nIn Exercise 5, you applied PCA to determine houses that were **outliers**, that is, houses having values not well represented in the rest of the data. You saw that there was a group of houses in the `Edwards` neighborhood having a `SaleCondition` of `Partial` whose values were especially extreme.\n\nSome models can benefit from having these outliers indicated, which is what this next transform will do.","3a7af4eb":"## Introduction ##\n\nThis is FilterJoe's final version of the House Price Learning competition for the Feature Engineering Course on Kaggle. The base notebook that came from the bonus lesson of this course was extensively modified\/expanded. The goal was to learn as much as possible about feature engineering, using only XGBoost. If you want to do the same, I recommend you do NOT just run this notebook . . . but rather start with the bonus lesson as your base code and gradually experiment over the course of a week or two like I did, perhaps glancing at this notebook from time to time to get inspired with new ideas.\n\nAfter weeks of experimentation with many forms of feature engineering, this notebook ended up with a very good score, possibly the best score on the leaderboard that uses only XGBoost. If getting the best score were the goal, next steps would be to experiment with other types of models besides XGboost. Then, you'd want to blend and\/or stack several models together.\n\n## Data Preprocessing ##\n\nBefore we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. The feature engineering course data was simpler than this competition data. For the *Ames* competition dataset, we'll need to:\n- **Load** the data from CSV files\n- **Clean** the data to fix any errors or inconsistencies\n- **Encode** the statistical data type (numeric, categorical)\n- **Impute** any missing values\n\nThese steps are wrapped in a function, which makes it easy to get a fresh dataframe when needed. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions for the leaderboard test set.\n\nThe base code for all this was provided in the bonus lesson (from Kaggle's feature engineering course), but was extensively modified (Kaggle user FilterJoe) to include:\n\n- verbose flags: summary of each step in preprocessing and feature engineering is displayed when called with verbose=True\n- one-hot encoding function\n- feature dropping function\n- added\/customized features (kept only those that boosted score)\n- improvements to imputation strategies\n- experimented with Boruta-SHAP feature selection (commented out as it always hurt XGBoost score a little)\n- feature importances output from XGBoost (used this info to test dropping features of low importance to XGBoost)\n- I did a bunch of EDA, but it's in a different notebook to keep this notebook smaller\n\n\n## Load and Clean ##","49761571":"# Step 4 - Hyperparameter Tuning #\n\nAt this stage, you might like to do some hyperparameter tuning with XGBoost before creating your final submission. Uncomment code block below to try some hyperparameter tuning using Optuna.","8b3dbfa8":"Later, we'll add the `drop_uninformative` function to our feature-creation pipeline. However . . .\n\nMutual information is not the best way to decide which features to drop, and that this gets impacted by model selection and hyperparameter optimization, as well as other feature development. After much experimentation, I found that with XGBoost, the best guide to deciding which features to drop was XGBoost itself - getting feature importances from it's .get_booster method and experimenting with dropping the least important features according to this XGBoost ranking.\n\nI found that a better use of Mutual information was looking at the highest ranking features and spending the majority of my feature engineering time on those top 5-10 features.\n\n# Step 3 - Create Features #\n\nNow we'll start developing our feature set.\n\nTo make our feature engineering workflow more modular, we'll define a function that will take a prepared dataframe and pass it through a pipeline of transformations to get the final feature set. It will look something like this:\n\n```\ndef create_features(df):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    X = X.join(create_features_1(X))\n    X = X.join(create_features_2(X))\n    X = X.join(create_features_3(X))\n    # ...\n    return X\n```\n\nLet's go ahead and define one transformation now, a [label encoding](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables) for the categorical features:","997ecfc1":"Let's look at our feature scores again:","50464cb4":"This baseline score helps us to know whether some set of features we've assembled has actually led to any improvement or not.\n\n# Step 2 - Feature Utility Scores #\n\nIn Lesson 2 we saw how to use mutual information to compute a *utility score* for a feature, giving you an indication of how much potential the feature has. This hidden cell defines the two utility functions we used, `make_mi_scores` and `plot_mi_scores`: ","19bd7f90":"## Drop Features ##\nDrop features that, for a given model, have very low importance or negative impact.<br>\nMutual Information is a crude method that doesn't capture interactions between feature.<br>Better methods include:\n* Intuitition developed from EDA\n* feature selection algorithm such as Boruta-SHAP\n* feature importances given after fitting a particular model, such as using XGBoost's feature importances","d47a1e25":"You could also consider applying some sort of robust scaler from scikit-learn's `sklearn.preprocessing` module to the outlying values, especially those in `GrLivArea`. [Here](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html) is a tutorial illustrating some of them. Another option could be to create a feature of \"outlier scores\" using one of scikit-learn's [outlier detectors](https:\/\/scikit-learn.org\/stable\/modules\/outlier_detection.html).","ba9dc7be":"Removing them does lead to a modest performance gain:","edb8b513":"The next hidden cell shows a log of many different experiments performed. If you want to develop intuition about feature engineering and a good experimentation mindset, it's great to take advantage of being able to submit up to 10 times per day. And you don't have to submit - if you're validation score gets a lot worse from adding or changing a feature, than you probably won't want to bother seeing if the score also gets worse on the leaderboard.\n\nFollowing this hidden cell is the final set of hyperparameters for XGBoost, which is fed data that has already been through the feature engineering pipeline. It took weeks of experimentation to get to a pure XGBoost model (no blending) to score so well:","1edc6b35":"## Principal Component Analysis ##\n\nPCA was the second unsupervised model we used for feature creation. We saw how it could be used to decompose the variational structure in the data. The PCA algorithm gave us *loadings* which described each component of variation, and also the *components* which were the transformed datapoints. The loadings can suggest features to create and the components we can use as features directly.\n\nHere are the utility functions from the PCA lesson:","bec66000":"## Target Encoding ##\n\nNeeding a separate holdout set to create a target encoding is rather wasteful of data. In *Tutorial 6* we used 25% of our dataset just to encode a single feature, `Zipcode`. The data from the other features in that 25% we didn't get to use at all.\n\nThere is, however, a way you can use target encoding without having to use held-out encoding data. It's basically the same trick used in cross-validation:\n1. Split the data into folds, each fold having two splits of the dataset.\n2. Train the encoder on one split but transform the values of the other.\n3. Repeat for all the splits.\n\nThis way, training and transformation always take place on independent sets of data, just like when you use a holdout set but without any data going to waste.\n\nIn the next hidden cell is a wrapper you can use with any target encoder. Use it like:\n\n```\nencoder = CrossFoldEncoder(MEstimateEncoder, m=1)\nX_encoded = encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n```\n\nYou can turn any of the encoders from the [`category_encoders`](http:\/\/contrib.scikit-learn.org\/category_encoders\/) library into a cross-fold encoder. The [`CatBoostEncoder`](http:\/\/contrib.scikit-learn.org\/category_encoders\/catboost.html) would be worth trying. It's similar to `MEstimateEncoder` but uses some tricks to better prevent overfitting. Its smoothing parameter is called `a` instead of `m`.","1ac035cc":"## Preprocess Data ##\n\nEverything is defined so we can now call the data preprocessor to get the processed data splits. Then we examine:","0bf94045":"You can see that we have a number of features that are highly informative and also some that don't seem to be informative at all (at least by themselves). As we talked about in Tutorial 2, the top scoring features will usually pay-off the most during feature development, so it could be a good idea to focus your efforts on those. On the other hand, training on uninformative features can lead to overfitting. So, the features with 0.0 scores we'll drop entirely:"}}