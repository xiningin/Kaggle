{"cell_type":{"32097179":"code","0d4adbfd":"code","301847da":"code","367f58b5":"code","47c21181":"code","bc65da01":"code","630556aa":"code","f94ada0b":"code","4476624e":"code","8d7c0fb9":"code","d4383cc4":"code","6445bd9e":"code","400b0da2":"code","50dff4f6":"code","d1a6140f":"code","af50d61f":"code","bfbe2369":"code","3d1a0038":"code","5aae14cf":"code","f327dcd5":"code","39674141":"code","971cce92":"code","a708d123":"code","ab8de402":"code","b5780de5":"code","ae94434f":"code","32c5684a":"code","69c6323f":"markdown","292bc365":"markdown","39751a49":"markdown","61be229b":"markdown","e3b3fce6":"markdown","a75c89a2":"markdown","e8967b13":"markdown","31945ee3":"markdown","6b802ec9":"markdown","2ec13971":"markdown","6f69a3eb":"markdown","13a64567":"markdown","87c60d8e":"markdown","1f3751c7":"markdown","41a045a5":"markdown","b4b9b962":"markdown","031f4a43":"markdown","90952ab0":"markdown","57a60da4":"markdown","9a1c88da":"markdown","7236b069":"markdown","9243d8ff":"markdown","eac7d638":"markdown","3e243285":"markdown","013cc711":"markdown","3b42da06":"markdown","5243d15c":"markdown","6139d02b":"markdown"},"source":{"32097179":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d4adbfd":"import pandas as pd\nimport numpy as np\n# Importing the csv data files \ndata = pd.read_csv('..\/input\/chocolate-bar-ratings\/flavors_of_cacao.csv',error_bad_lines=False, warn_bad_lines=True)\ndata.head()","301847da":"# Renaming the column name\nnew_col_names = ['company', 'bean_origin', 'REF', 'review_date', 'cocoa_percent',\n                'company_location', 'rating', 'bean_typ', 'country_origin']\ndata_clean = data.rename(columns=dict(zip(data.columns, new_col_names)))\ndata_clean.head()","367f58b5":"# Function to count the number of null values in a column\ndef count_null_values(dataset, column_list):\n    for i in range (len(column_list)):\n        print (\"The total number of null values in :\",column_list[i])\n        print (dataset[column_list[i]].isnull().sum())\n    return\n\n# Function to dispplay the unique counts in a column\ndef print_uniques(dataset, column_list):\n    for i in range (len(column_list)):\n        print (\"Unique values for the column:\",column_list[i])\n        print (dataset[column_list[i]].unique())\n        print ('\\n')\n    return\n\n# Printing the null and unique values for each attribute in the dataset\nprint_uniques(data_clean, data_clean.columns)\ncount_null_values(data_clean, data_clean.columns)\n#print len(data['Rating'].unique().tolist())","47c21181":"# Creating new column named maker \ndata_clean['company_coffee'], data_clean['maker'] = data_clean['company'].str.split('(', 1).str\n# Replacing the missing values with \"Unknown\"\nprint (data_clean.head(4))\ndata_clean['maker'].fillna(value='Unknown', inplace = True) \n#data_clean[\"maker\"].replace(np.nan, \"Unknown\")\nprint (data_clean.head())\nprint (data_clean.info())\n# Removing unwanted character\ndata_clean['maker'] = data_clean['maker'].apply(lambda x: x.split(')')[0])\n# Dropping the original column\ndata_clean = data_clean.drop('company', 1)\n\nprint(data_clean.head(4))\n\n","bc65da01":"# Converting the string values to lower case\ndata_clean['bean_typ'] = data_clean['bean_typ'].str.lower()\n\n# Creating new column named sub_bean_type \ndata_clean['bean_type'], data_clean['sub_bean_type'] = data_clean['bean_typ'].str.split(',', 1).str\n# Replacing the missing values with \"Unknown\"\ndata_clean[\"sub_bean_type\"].fillna(\"unknown\", inplace = True) \n# Removing unwanted character\ndata_clean['sub_bean_type'] = data_clean['sub_bean_type'].apply(lambda x: x.split(')')[0])\n# Dropping the original column\ndata_clean = data_clean.drop('bean_typ', 1)\n\nprint (data_clean.head(10))\nprint (data_clean['bean_type'].unique())","630556aa":"# Some data cleaning regarding bean type name\ndata_clean['bean_type'] = data_clean['bean_type'].replace('forastero (arriba) asss', 'forastero (arriba)')\ndata_clean['bean_type'] = data_clean['bean_type'].replace('forastero (arriba) ass', 'forastero (arriba)')\nprint (data_clean['bean_type'].unique())","f94ada0b":"# Data cleaning regarding the Broad Bean Origin column\nprint (\"Before Cleaning country_origin column: \")\nprint (data_clean['country_origin'].unique())\ndata_clean['country_origin'] = data_clean['country_origin'].replace('Domincan Republic', 'Dominican Republic')\ndata_clean['country_origin'] = data_clean['country_origin'].replace('Carribean(DR\/Jam\/Tri)', 'Carribean')\ndata_clean['country_origin'] = data_clean['country_origin'].replace('Trinidad-Tobago', 'Trinidad, Tobago')\ndata_clean['country_origin'] = data_clean['country_origin'].replace(\"Peru, Mad., Dom. Rep.\", \"Peru, Madagascar, Dominican Republic\")\ndata_clean['country_origin'] = data_clean['country_origin'].replace(\"Central and S. America\", \"Central and South America\")\ndata_clean['country_origin'] = data_clean['country_origin'].replace(\"PNG, Vanuatu, Mad\", \"Papua New Guinea, Vanuatu, Madagascar\")\ndata_clean['country_origin'] = data_clean['country_origin'].replace(\"Ven., Trinidad, Mad.\", \"Venezuela, Trinidad, Madagascar\")\ndata_clean['country_origin'] = data_clean['country_origin'].replace(\"Ven.,Ecu.,Peru,Nic.\", \"Venezuela, Ecuador, Peru, Nicaragua\")\ndata_clean['country_origin'] = data_clean['country_origin'].replace(\"Ven, Trinidad, Ecuador\",\"Venezuela, Trinidad, Ecuador\")\ndata_clean['country_origin'] = data_clean['country_origin'].replace(\"Ghana, Domin. Rep\", \"Ghana, Dominican Republic\")\ndata_clean['country_origin'] = data_clean['country_origin'].replace(\"Ecuador, Mad., PNG\",\"Ecuador, Madagascar, Papua New Guinea\")\ndata_clean['country_origin'] = data_clean['country_origin'].replace(\"Mad., Java, PNG\",\"Madagascar, Java, Papua New Guinea\")\ndata_clean['country_origin'] = data_clean['country_origin'].replace(\"Gre., PNG, Haw., Haiti, Mad\", \"Grenada, Papua New Guinea, Hawaii, Haiti, Madagascar\")\n\nprint (\"After Cleaning country_origin column: \")\nprint (data_clean['country_origin'].unique())","4476624e":"# Data cleaning the bean origin column\n\ndata_clean['bean_origin'] = data_clean['bean_origin'].str.lower()\n\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split(',')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('\/')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('*')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('.')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('+')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split(';')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('-')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('(')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('#')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('1')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('2')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('3')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('4')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('5')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('6')[0])\ndata_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('7')[0])\nprint (data_clean['bean_origin'].unique())\n#print data_clean['bean_origin'].head()","8d7c0fb9":"# Converting cocoa_percent to numeric\n\nprint (data_clean.info())\ndata_clean['cocoa_percent'] = data_clean['cocoa_percent'].apply(lambda x: x.split('%')[0])\ndata_clean['cocoa_percent'] = pd.to_numeric(data_clean['cocoa_percent'], errors='coerce')\nprint (data_clean.info())","d4383cc4":"# Replacing the empty cells with null\ndata_clean = data_clean.replace('\\xa0', np.nan)\ncount_null_values(data_clean, data_clean.columns)","6445bd9e":"# Changing the type for review_date from int to object\ndata_clean['review_date'] = data_clean['review_date'].astype(str)\ndata_clean['rating'] = data_clean['rating'].astype(str)\nprint(data_clean.info())","400b0da2":"# Normalizing the column with integer type\n\n# Data Normalizing\nfrom sklearn.preprocessing import StandardScaler \ndata_norm = data\nscaler_z = StandardScaler()\n# Only the columns with integer and float type values are normalized\nnum_d = data_clean.select_dtypes(exclude=['object'])\ndata_clean[num_d.columns] = scaler_z.fit(num_d).transform(num_d)\n\n# Getting information of the dataset after normalization\nprint (data_clean.head(10))\nprint (data_clean[num_d.columns].mean(axis= 0))\nprint (data_clean.info())","50dff4f6":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\n\n# Creating a temp dataset\ntemp_data_co = data_clean\n# dropping the bean_type column since it has missing values\ntemp_data_co= temp_data_co.drop('bean_type', 1)\n\n# Splitting the dataset into null and not null dataframe\ntest_data_co = temp_data_co[temp_data_co[\"country_origin\"].isnull()]\ntrain_data_co = temp_data_co[temp_data_co[\"country_origin\"].notnull()]\n\n# Label encoding only the categorical columns \ntest_data_co_l = test_data_co.apply(LabelEncoder().fit_transform)\ntest_data_co_l['REF'] = data_clean['REF']\ntest_data_co_l['cocoa_percent'] = data_clean['cocoa_percent']\n\ntrain_data_co_l = train_data_co.apply(LabelEncoder().fit_transform)\ntrain_data_co_l ['REF'] = data_clean['REF']\ntrain_data_co_l['cocoa_percent'] = data_clean['cocoa_percent']\n\n# Defining the X and y \nX = train_data_co_l.drop('country_origin', axis=1).values\ny = train_data_co_l['country_origin'].values\n\nprint (test_data_co.shape, train_data_co.shape)\n\nX_train_co, X_test_co, y_train_co, y_test_co = train_test_split(X, y, test_size= 0.1, train_size=0.9, random_state=42)\n\nprint (X_train_co.shape, X_test_co.shape)\n\n# Training a KNN machine leanring model to replace the missing values\nfrom sklearn.neighbors import KNeighborsClassifier\n# The model gave the best result at n_neighbors = 3\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train_co, y_train_co)\ny_pred = knn.predict(X_test_co)\nprint (y_pred)\nprint (y_pred.size)\n\n# Getting the accuracy metric\nacc = accuracy_score(y_pred, y_test_co)\npre = precision_score(y_pred, y_test_co, average='micro')\nrec = recall_score(y_pred, y_test_co, average='micro')\nf1 = f1_score(y_pred, y_test_co, average='micro')\n\nprint ('Model performace for replacing the missing values: ')\nprint ('Accuracy: ', acc)\nprint ('Precision: ', pre)\nprint ('Recall: ',rec)\nprint ('F1 Score: ', f1)","d1a6140f":"# Predicting the null values for country_origin\npred_data_co_l = test_data_co_l.drop('country_origin', axis=1).values\n\n# y_pred containing the predicted value\ny_pred = knn.predict(pred_data_co_l)\n\n# Storing the y_pred values in a column of a dataframe\ntemp1 = test_data_co_l\ntemp1[\"country_origin\"] = y_pred\n\n# Incrporating the prediction for the missing values into the main dataset\ndataset_clean_co = pd.concat([train_data_co_l, temp1], join = 'inner')\nprint (dataset_clean_co.head())\nprint (dataset_clean_co.shape)","af50d61f":"# Preparing the dataset by propping the country_origin column since it had missing values\ntemp_data_bt = data_clean\ntemp_data_bt = temp_data_bt.drop('country_origin', 1)\n\n# Splitting the dataset into null and not null dataframe\ntest_data_bt = temp_data_bt[temp_data_bt[\"bean_type\"].isnull()]\ntrain_data_bt = temp_data_bt[temp_data_bt[\"bean_type\"].notnull()]\n\n# Label encoding only the categorical columns \ntest_data_bt = test_data_bt.apply(LabelEncoder().fit_transform)\ntest_data_bt['REF'] = data_clean['REF']\ntest_data_bt['cocoa_percent'] = data_clean['cocoa_percent']\n\ntrain_data_bt = train_data_bt.apply(LabelEncoder().fit_transform)\ntrain_data_bt['cocoa_percent'] = data_clean['cocoa_percent']\ntrain_data_bt['REF'] = data_clean['REF']\n\n# Defining the X and y \nX = train_data_bt.drop('bean_type', axis=1).values\ny = train_data_bt['bean_type'].values\n\nX_train_bt, X_test_bt, y_train_bt, y_test_bt = train_test_split(X, y, test_size= 0.2, train_size=0.8, random_state=42)\n\n\n# Training a KNN machine leanring model to replace the missing values\nfrom sklearn.neighbors import KNeighborsClassifier\n# The model gave the best result at n_neighbors = 80\nknn = KNeighborsClassifier(n_neighbors = 80)\nknn.fit(X_train_bt, y_train_bt)\ny_pred = knn.predict(X_test_bt)\n\n# Getting the accuracy metric\nacc = accuracy_score(y_pred, y_test_bt)\npre = precision_score(y_pred, y_test_bt, average='micro')\nrec = recall_score(y_pred, y_test_bt, average='micro')\nf1 = f1_score(y_pred, y_test_bt, average='micro')\n\nprint ('Model performace for replacing the missing values: ')\nprint ('Accuracy: ', acc)\nprint ('Precision: ', pre)\nprint ('Recall: ',rec)\nprint ('F1 Score: ', f1)","bfbe2369":"# Predicting the null values for bean_type with the KNN model\n\npred_data_bean_l = test_data_bt.drop('bean_type', axis=1).values\n\n# y_pred storing the predicted values\ny_pred = knn.predict(pred_data_bean_l)\n\ntemp = test_data_bt\ntemp[\"bean_type\"] = y_pred\n\n# Incrporating the prediction for the missing values into the dataset\ndataset_clean_bean = pd.concat([train_data_bt, temp], join = 'inner')\nprint (dataset_clean_bean.head())\nprint (dataset_clean_bean.shape)","3d1a0038":"data_clean_label_encoding = dataset_clean_bean\ndata_clean_label_encoding['country_origin'] = dataset_clean_co['country_origin']\n# Checking the dataset for null values after data processing\ncount_null_values(data_clean_label_encoding, data_clean_label_encoding.columns)\n","5aae14cf":"# Finding out the correlation between the features in the dataset\n\nfrom string import ascii_letters\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas\nfrom sklearn.preprocessing import LabelEncoder\n\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = data_clean_label_encoding [['bean_origin','REF','review_date','cocoa_percent','company_location'\\\n                   ,'rating','maker','bean_type','maker', 'sub_bean_type',\\\n                   'country_origin']].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(8, 8))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.set_context(\"paper\", rc={\"font.size\":12,\"axes.titlesize\":12, \"axes.labelsize\":12}) \n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nax.set_title('Correlation Between The Pairs of Columns ')\nplt.show()","f327dcd5":"# Finding the distribution of rating in the dataset\nfrom scipy import stats\nsns.distplot(data_clean[\"rating\"], kde=False, fit=stats.gamma).set(title = 'Distribution for Chocolate Rating', xlabel = 'Rating', ylabel = 'Proportion Distribution' )\nplt.show()","39674141":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import SelectFromModel\n%matplotlib inline\n#Splitting the variables into features and target\nX = data_clean_label_encoding.drop('rating', axis=1)\ny = data_clean_label_encoding['rating'].values\n\nprint(X.shape)#printing dimensions of features\nprint(y.shape)#printing dimensions of label\n\n#Printing the variability of all the features\nprint(X.var())\n#Since the Variability of any column is not very low so selecting all the features based on variability\n\n\n# Using ExtraTreesClassifier for feature selection\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint (\"Dataset Size Before Feature Selection \")\nprint( X.shape)\nclf = ExtraTreesClassifier(n_estimators=50)\nclf = clf.fit(X, y)\nclf.feature_importances_  \nmodel = SelectFromModel(clf, prefit=True)\nX_feat_select = model.transform(X)\nprint (\"Dataset Size After Feature Selection \")\nprint (X_feat_select.shape )           \nprint (\"Relative Feature importance for each of the Features- \")\nprint(clf.feature_importances_)\n\nfeat_importances = pd.Series(clf.feature_importances_, index=X.columns)\nfeat_importances.nlargest(5).plot(kind='barh')\nplt.title('Important Features')\nplt.xlabel('Relative Importance')\nplt.ylabel('Features')\nplt.show()\n","971cce92":"# Training- 80% Testing- 20%\nX_train, X_test, y_train, y_test = train_test_split(X_feat_select, y, test_size= 0.2, random_state=42)\nprint ('Training and testing size')\nprint (X_train.shape)\nprint (X_test.shape)","a708d123":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier()\n# Parameter for performing hyper tuning\nparameters = {'n_estimators': [4, 5, 6], 'max_depth': [2, 3, 4], \\\n              'min_samples_split': [25, 30, 35], 'max_leaf_nodes': [4, 5, 6]}\n\nrandom_forest_classifier = GridSearchCV(random_forest, parameters,  cv = 5)\nrandom_forest_classifier.fit(X_train, y_train)\nprint(random_forest_classifier.best_params_)\nprint(random_forest_classifier.best_score_)","ab8de402":"\ntemp = data_clean\ntemp['rating'] = temp['rating'].astype(float)\n\n# Function to convert the values for the  rating to its nearest whole number\ndef round_rating(rating):\n    if (rating < 1 ):\n        return 0\n    elif (rating > 0 ) and (rating < 2 ):\n        return 1\n    elif (rating >= 2 ) and (rating < 3 ):\n        return 2\n    elif (rating >= 3 ) and (rating < 4 ):\n        return 3\n    elif (rating >= 4 ) and (rating < 5 ):\n        return 4\n    else:\n        return 5\n\n\ntemp['rating'] = temp['rating'].apply(round_rating)\nprint(temp['rating'].unique())","b5780de5":"y_new = temp['rating'].values\nX_train, X_test, y_train, y_test = train_test_split(X_feat_select, y_new, test_size= 0.2, random_state=42)\nprint ('Suite-1 training and testing size')\nprint (X_train.shape)\nprint (X_test.shape)\nprint(y_new)","ae94434f":"# Training the model\nrandom_forest_classifier.fit(X_train, y_train)\nprint(random_forest_classifier.best_params_)\nprint(random_forest_classifier.best_score_)","32c5684a":"# Testing the model\nmodel = RandomForestClassifier(random_state=42, max_depth= 4, max_leaf_nodes= 4,\\\n                                       min_samples_split= 35, n_estimators= 6)\nmodel.fit(X_train, y_train)\n#Predict the response for test dataset\ny_pred = model.predict(X_test)\n\n# Getting the accuracy metric\nacc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test, average='micro')\nrec = recall_score(y_pred, y_test,average='micro')\nf1 = f1_score(y_pred, y_test, average='micro')\nprint ('Model Performance Statistic: ')\nprint ('Accuracy: ', acc)\nprint ('Precision: ', pre)\nprint ('Recall: ',rec)\nprint ('F1 Score: ', f1)","69c6323f":"### Dealing with the null values for country_origin","292bc365":"There were two attributes- Company(Maker-if known) and bean_typ which stored multiple infomation as a same instance value and thus required feature engineering so that we do not lose any valuable information during the data cleaning process. ","39751a49":"After applying feature selection we ended up with 5 features down from 10.","61be229b":"In the data cleaning stage the anomalies in the data were resolved","e3b3fce6":"The attibutes with the null values were both categorical. In order to replace the missing values a KNN model was developed to predict the missing values. ","a75c89a2":"### Performing Feature Selection","e8967b13":"We can increase the prediction of the model if we round the values for the ratings to its nearest whole number","31945ee3":"The best test accuracy for the Random forest model is 25% which is very less as it can be interpreted as- The model will be able to predict the rating for the chocolate correctly once in every four attempts. However, this problem is a multiclass classification problem with 13 distinct values for the rating  which are- 3.75, 2.75, 3, 3.5, 4, 3.25, 2.5, 5, 1.75, 1.5, 2.25, 2, 1","6b802ec9":"In this dataset there were two numerical attributes which required scaling so that during training the machine learning model does not learn more from the attribute with the higher values when compared to the attribute with the low values.","2ec13971":"### Performing Data Partitioning","6f69a3eb":"### Conducting Data Cleaning","13a64567":"### Preparing the final dataset","87c60d8e":"The name of the attributes(column names) are renamed to make it easier to work with. The main reason is that in the original dataset the column names were taking two rows which would make our references to those columns clumsy as we need to include '\\n' for the column names ","1f3751c7":"### Conducting Feature Engineering","41a045a5":"### Data Preprocessing","b4b9b962":"### Random Forest Model Development","031f4a43":"Let's explore the correlation between the categorical values. This will give us a better understanding of how the attributes are related to one another.","90952ab0":"### Exploratory Data Analysis","57a60da4":"Some functions are being created so that we avoid repetation of the code and to make our code more succinct.","9a1c88da":"### Dealing with the null values for bean_type","7236b069":"Previously we have seen that cocoa_percent attribute had the data type of object which was then tranfomed into integer ","9243d8ff":"The graph above shows the distribution of rating in the dataset. We can see that the pattern follows a normal distribution with having the peak rating around the value of 3","eac7d638":"### Rounding the values of rating","3e243285":"The dataset had total 9 attributes including the target attribute which is the rating for the chocolate. The problem statement is to classify the rating for the chocolated based on the value of the attributes. This problem was addressed as a classification problem since there were discrete values for the rating in the dataset. ","013cc711":"The accuracy of the model is now 70.75%","3b42da06":"Analysing the correlation matrix shown above we can notice that there are some high correlation between the attributes which means some of the attributes will not provide significant input to our machine leanring model. Moreover, there is a risk of having an overfitted model due to the presence of high correlation. This issue has been addressed by feature selection; discussed in the later part of the implementation.","5243d15c":"In the dataset there were some special character epresent due to encoding such as '\\xa0'. This characters were replaced with null","6139d02b":"First of all lets import the dataset into our notebook."}}