{"cell_type":{"97bba717":"code","59e33b0e":"code","819b897f":"code","aa04e2a4":"code","aae72cef":"code","99326ae8":"code","cb3b39eb":"code","a8c8ffb8":"code","fe538466":"code","45cdfd6f":"code","8fc4c2bb":"code","d6539f26":"code","715704b6":"code","8b315c27":"code","c76ae473":"code","337e4304":"code","1609ab54":"code","89b92d19":"code","b7b2b3bc":"code","dcfb5498":"code","2b47338e":"code","6f483c7d":"code","94699e0d":"code","149977e8":"code","2612364a":"code","d80b592f":"code","da16ae4e":"code","0a793d7b":"code","be81fe78":"code","330e4954":"code","0e897e0e":"code","4ed84602":"code","af6d10ac":"code","d404d391":"code","0082775d":"code","4f2bc569":"code","8de74a8f":"code","52af339a":"code","a62c1856":"code","dfb2f7ac":"code","09bdf0cc":"code","26670e17":"code","b777d999":"code","c081c277":"code","137a682c":"code","5723b35f":"code","31ff0fef":"markdown","4767cad1":"markdown","33012bd0":"markdown","600c8481":"markdown","c0486055":"markdown","45782ec4":"markdown","bea77071":"markdown","481f7269":"markdown","9d23c37a":"markdown","7be5780a":"markdown","3680aafc":"markdown","ed5e037e":"markdown","5ffe32e2":"markdown","d50c2548":"markdown","fdc485bd":"markdown","1b9074d7":"markdown","99dd390d":"markdown","22bbd8c9":"markdown","5d34141d":"markdown","597502d4":"markdown","fc5a2be7":"markdown","02f22a9d":"markdown","0883d887":"markdown","4549171f":"markdown","a4a1d603":"markdown","37630add":"markdown","f69bc37e":"markdown","4ab3d1d8":"markdown","770450d1":"markdown","ab68ecce":"markdown","1955fd64":"markdown","744101a2":"markdown","cb38bfbc":"markdown","70929574":"markdown","80a8259c":"markdown","366b808b":"markdown","086d471c":"markdown","470c24c8":"markdown","4b289e7a":"markdown","de7c56fb":"markdown"},"source":{"97bba717":"from IPython.display import HTML\nHTML('<center><iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/AfK9LPNj-Zo\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe><\/center>')","59e33b0e":"import numpy as np\nimport random\nimport pandas as pd\nimport pydicom\nimport os\nimport matplotlib.pyplot as plt\nfrom timeit import timeit\nfrom tqdm import tqdm\nfrom PIL import Image\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n\n#color\nfrom colorama import Fore, Back, Style\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as Layers\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.\n","819b897f":"def seed_everything(seed): \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)","aa04e2a4":"ROOT = '..\/input\/osic-pulmonary-fibrosis-progression'\n\ntrain_df = pd.read_csv(f'{ROOT}\/train.csv')\nprint(f'Train data has {train_df.shape[0]} rows and {train_df.shape[1]} columnns and looks like this:')\n","aae72cef":"train_df.sample(10)","99326ae8":"train_unique_df = train_df.drop_duplicates(subset = ['Patient'], keep = 'first')\ntrain_unique_df.head()","cb3b39eb":"# CHECK FOR DUPLICATES & DEAL WITH THEM\n# keep = False: All duplicates will be shown\ndupRows_df = train_df[train_df.duplicated(subset = ['Patient', 'Weeks'], keep = False )]\ndupRows_df.head()","a8c8ffb8":"train_df.drop_duplicates(subset=['Patient','Weeks'], keep = False, inplace = True)","fe538466":"print(f'So there are {dupRows_df.shape[0]} (= {dupRows_df.shape[0] \/ train_df.shape[0] * 100:.2f}%) duplicates.')","45cdfd6f":"test_df = pd.read_csv(f'{ROOT}\/test.csv')\nprint(f'Test data has {test_df.shape[0]} rows and {test_df.shape[1]} columnns, has no duplicates and looks like this:')\ntest_df.head()","8fc4c2bb":"## CHECK SUBMISSION FORMAT\nsub_df = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\n\nprint(f\"The sample submission contains: {sub_df.shape[0]} rows and {sub_df.shape[1]} columns.\")","d6539f26":"sub_df.head()","715704b6":"# split Patient_Week Column and re-arrage columns\nsub_df[['Patient','Weeks']] = sub_df.Patient_Week.str.split(\"_\",expand = True)\nsub_df =  sub_df[['Patient','Weeks','Confidence', 'Patient_Week']]","8b315c27":"sub_df = sub_df.merge(test_df.drop('Weeks', axis = 1), on = \"Patient\")","c76ae473":"# introduce a column to indicate the source (train\/test) for the data\ntrain_df['Source'] = 'train'\nsub_df['Source'] = 'test'\n\ndata_df = train_df.append([sub_df])\ndata_df.reset_index(inplace = True)\ndata_df.head()","337e4304":"def get_baseline_week(df):\n    # make a copy to not change original df    \n    _df = df.copy()\n    # ensure all Weeks values are INT and not accidentaly saved as string\n    _df['Weeks'] = _df['Weeks'].astype(int)\n    _df['min_week'] = _df['Weeks']\n    # as test data is containing all weeks, \n    _df.loc[_df.Source == 'test','min_week'] = np.nan\n    _df[\"min_week\"] = _df.groupby('Patient')['Weeks'].transform('min')\n    _df['baselined_week'] = _df['Weeks'] - _df['min_week']\n    \n    return _df   ","1609ab54":"data_df = get_baseline_week(data_df)\ndata_df.head()","89b92d19":"def get_baseline_FVC_old(df):\n    # copy the DF to not in-place change the original one\n    _df = df.copy()\n    # get only the rows containing the baseline (= min_weeks) and therefore the baseline FVC\n    baseline = _df.loc[_df.Weeks == _df.min_week]\n    baseline = baseline[['Patient','FVC']].copy()\n    baseline.columns = ['Patient','base_FVC']      \n    \n    # fill the df with the baseline FVC values\n    for idx in _df.index:\n        patient_id = _df.at[idx,'Patient']\n        _df.at[idx,'base_FVC'] = baseline.loc[baseline.Patient == patient_id, 'base_FVC'].iloc[0]\n    _df.drop(['min_week'], axis = 1)\n    \n    return _df","b7b2b3bc":"def get_baseline_FVC(df):\n    # same as above\n    _df = df.copy()\n    base = _df.loc[_df.Weeks == _df.min_week]\n    base = base[['Patient','FVC']].copy()\n    base.columns = ['Patient','base_FVC']\n    \n    # add a row which contains the cumulated sum of rows for each patient\n    base['nb'] = 1\n    base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n    \n    # drop all except the first row for each patient (= unique rows!), containing the min_week\n    base = base[base.nb == 1]\n    base.drop('nb', axis = 1, inplace = True)\n    \n    # merge the rows containing the base_FVC on the original _df\n    _df = _df.merge(base, on = 'Patient', how = 'left')    \n    _df.drop(['min_week'], axis = 1)\n    \n    return _df","dcfb5498":"def get_baseline_FVC_new(df):\n    _df = (\n        df\n        .loc[df.Weeks == df.min_week][['Patient','FVC']]\n        .rename({'FVC': 'min_FVC'}, axis=1)\n        .groupby('Patient')\n        .first()\n        .reset_index()\n    )\n    \n    return _df","2b47338e":"def old_baseline_FVC():\n    return get_baseline_FVC_old(data_df)\n    \n\ndef baseline_FVC():\n    return get_baseline_FVC(data_df)\n\ndef baseline_FVC_new():\n    return get_baseline_FVC_new(data_df)\n    \n\nduration_old = timeit(old_baseline_FVC, number = 3)\nduration_baseline = timeit(baseline_FVC, number = 3)\nduration_new = timeit(baseline_FVC_new, number = 3)\n\nprint(f\"Taking the first, old & non-vectorized approach took {duration_old \/ 3:.2f} sec, while the 2nd, vectorized approach only took {duration_baseline \/ 3:.3f} sec. That's {duration_old\/duration_baseline:.0f} times faster!\" )\nprint(f\"Taking the 3rd, newest, shortest and cleanest approach took {duration_new \/ 3:.3f} sec. That's {duration_old\/duration_new:.0f} (!!) times faster!\" )","6f483c7d":"data_df = get_baseline_FVC(data_df)\ndata_df.head()","94699e0d":"# import the necessary Encoders & Transformers\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.compose import ColumnTransformer\n\n# define which attributes shall not be transformed, are numeric or categorical\nno_transform_attribs = ['Patient', 'Weeks', 'min_week']\nnum_attribs = ['FVC', 'Percent', 'Age', 'baselined_week', 'base_FVC']\ncat_attribs = ['Sex', 'SmokingStatus']","149977e8":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass NoTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Passes through data without any change and is compatible with ColumnTransformer class\"\"\"\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n        return X","2612364a":"## GET TRANSFORMED DATAFRAME\n\n# create an instance of the ColumnTransformer\ndatawrangler = ColumnTransformer(([\n     # the No-Transformer does not change the data and is applied to all no_transform_attribs \n     ('original', NoTransformer(), no_transform_attribs),\n     # Apply MinMax to the numerical attributes, here you can change to e.g. StdScaler()   \n     ('MinMax', MinMaxScaler(), num_attribs),\n     # OneHotEncoder all categorical attributes.   \n     ('cat_encoder', OneHotEncoder(), cat_attribs),\n    ]))\n\ntransformed_data_series = []\ntransformed_data_series = datawrangler.fit_transform(data_df)","d80b592f":"# get column names for non-categorical data\nnew_col_names = no_transform_attribs + num_attribs\n\n# extract possible values from the fitted transformer\ncategorical_values = [s for s in datawrangler.named_transformers_[\"cat_encoder\"].get_feature_names()]\nnew_col_names += categorical_values\n\n# create Dataframe based on the extracted Column-Names\ntrain_sklearn_df = pd.DataFrame(transformed_data_series, columns=new_col_names)\ntrain_sklearn_df.head()","da16ae4e":"def own_MinMaxColumnScaler(df, columns):\n    \"\"\"Adds columns with scaled numeric values to range [0, 1]\n    using the formula X_scld = (X - X.min) \/ (X.max - X.min)\"\"\"\n    for col in columns:\n        new_col_name = col + '_scld'\n        col_min = df[col].min()\n        col_max = df[col].max()        \n        df[new_col_name] = (df[col] - col_min) \/ ( col_max - col_min )","0a793d7b":"def own_OneHotColumnCreator(df, columns):\n    \"\"\"OneHot Encodes categorical features. Adds a column for each unique value per column\"\"\"\n    for col in cat_attribs:\n        for value in df[col].unique():\n            df[value] = (df[col] == value).astype(int)","be81fe78":"## APPLY DEFINED TRANSFORMATIONS\nown_MinMaxColumnScaler(data_df, num_attribs)\nown_OneHotColumnCreator(data_df, cat_attribs)\n\ndata_df[data_df.Source != \"train\"].head()","330e4954":"# get back original data split\ntrain_df = data_df.loc[data_df.Source == 'train']\nsub = data_df.loc[data_df.Source == 'test']","0e897e0e":"######## CONFIG ########\n# be careful, the resulsts are very SEED-DEPENDEND!\nseed_everything(1989)\n\n\n### Features: choose which features you want to use\n# you can exclude and include features by extending this feature list\nfeatures_list = ['baselined_week_scld', 'Age_scld', 'base_FVC_scld', 'Male', 'Female', 'Ex-smoker', 'Never smoked', 'Currently smokes']\n\n### Basics for training:\nEPOCHS = 1500\nBATCH_SIZE = 128\n\n\n### LOSS; set tradeoff btw. Pinball-loss and adding score\n_lambda = 0.8 # 0.8 default\n\n\n### Optimizers\n# choose ADAM or SGD\noptimizer = 'SGD'\n\n### Learning Rate Scheduler\ndef get_lr_callback(batch_size = 64, plot = False):\n    \"\"\"Returns a lr_scheduler callback which is used for training.\n    Feel free to change the values below!\n    \"\"\"\n    LR_START   = 0.001\n    LR_MAX     = 0.0001 * BATCH_SIZE # higher batch size --> higher lr\n    LR_MIN     = 0.000001\n    # 30% of all epochs are used for ramping up the LR and then declining starts\n    LR_RAMP_EP = EPOCHS * 0.3\n    # how many epochs shall L_RMAX be sustained\n    LR_SUS_EP  = 0\n    # rate of decay\n    LR_DECAY   = 0.993\n\n    def lr_scheduler(epoch):\n            if epoch < LR_RAMP_EP:\n                lr = (LR_MAX - LR_START) \/ LR_RAMP_EP * epoch + LR_START\n\n            elif epoch < LR_RAMP_EP + LR_SUS_EP:\n                lr = LR_MAX\n\n            else:\n                lr = (LR_MAX - LR_MIN) * LR_DECAY ** (epoch - LR_RAMP_EP - LR_SUS_EP) + LR_MIN\n\n            return lr\n    \n    if plot == False:\n        # get the Keras-required callback with our LR for training\n        lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler,verbose = False)\n        return lr_callback \n    \n    else: \n        return lr_scheduler\n    \n# plot & check the LR-Scheulder for sanity-check\nlr_scheduler_plot = get_lr_callback(batch_size = 64, plot = True)\nrng = [i for i in range(EPOCHS)]\ny = [lr_scheduler_plot(x) for x in rng]\nplt.plot(rng, y)\nprint(f\"Learning rate schedule: {y[0]:.3f} to {max(y):.3f} to {y[-1]:.3f}\")\n\n\n# logging & saving\nLOGGING = True\n\n# defining custom callbacks\nclass LogPrintingCallback(tf.keras.callbacks.Callback):\n    \n    # defining a class variable which is used in the following\n    # to ensure LogPrinting, evaluation & saving is consitent\n    # choose between 'val_score' and 'val_loss'\n    optimization_variable = 'val_loss'\n    \n    def on_train_begin(self, logs = None):\n        #print(\"Training started for this fold\")\n        self.val_loss = []\n        self.val_score = []        \n        \n    def on_epoch_end(self, epoch, logs = None):\n        self.val_loss.append(logs['val_loss']) \n        self.val_score.append(logs['val_score'])\n        if epoch % 250 == 0 or epoch == (EPOCHS -1 ):\n            print(f\"The average val-loss for epoch {epoch} is {logs['val_loss']:.2f}\"\n                  f\" and the score is {logs['val_score']}\")\n            \n    def on_train_end(self, logs = None):\n        # get index of best epoch\n        monitored_variable = self.val_loss if LogPrintingCallback.optimization_variable == 'val_loss' else self.val_score\n        best_epoch = np.argmin(monitored_variable)        \n        # get score in best epoch\n        best_result_loss = self.val_loss[best_epoch]\n        best_result_score = self.val_score[best_epoch]\n        print(f\"Best model was found and saved in epoch {best_epoch + 1} with val_loss: {best_result_loss} and val_score: {best_result_score} \") \n        \n        \ndef get_checkpoint_saver_callback(fold):\n    checkpt_saver = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold,\n        monitor = 'val_score',\n        verbose = 0,\n        save_best_only = True,\n        save_weights_only = True,\n        mode = 'min',\n        save_freq = 'epoch')\n    \n    return checkpt_saver","4ed84602":"# create constants for the loss function\nC1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n\n# define competition metric\ndef score(y_true, y_pred):\n    \"\"\"Calculate the competition metric\"\"\"\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    sigma_clip = tf.maximum(sigma, C1)\n    # Python is automatically broadcasting y_true with shape (1,0) to \n    # shape (3,0) in order to make this subtraction work\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype = tf.float32) )\n    metric = (delta \/ sigma_clip) * sq2 + tf.math.log(sigma_clip * sq2)\n    return K.mean(metric)\n\n# define pinball loss\ndef qloss(y_true, y_pred):\n    \"\"\"Calculate Pinball loss\"\"\"\n    # IMPORTANT: define quartiles, feel free to change here!\n    qs = [0.2, 0.50, 0.8]\n    q = tf.constant(np.array([qs]), dtype = tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q * e, (q-1) * e)\n    return K.mean(v)\n\n# combine competition metric and pinball loss to a joint loss function\ndef mloss(_lambda):\n    \"\"\"Combine Score and qloss\"\"\"\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda) * score(y_true, y_pred)\n    return loss","af6d10ac":"import tensorflow_addons as tfa\n\ndef get_model(optimizer = 'ADAM', lr = 0.01):\n    \"Creates and returns a model\"\n    # instantiate optimizer\n    optimizer = tf.keras.optimizers.Adam(lr = lr) if optimizer == 'ADAM' else tf.keras.optimizers.SGD(lr)\n    \n    # create model    \n    inp = Layers.Input((len(features_list),), name = \"Patient\")\n    x = Layers.BatchNormalization()(inp)\n    x = tfa.layers.WeightNormalization(Layers.Dense(160, activation = \"elu\", name = \"d1\"))(x)\n    x = Layers.BatchNormalization()(x)\n    x = Layers.Dropout(0.3)(x)\n    x = tfa.layers.WeightNormalization(Layers.Dense(128, activation = \"elu\", name = \"d2\"))(x)\n    x = Layers.BatchNormalization()(x)\n    x = Layers.Dropout(0.25)(x)\n    # predicting the 3 quantiles\n    q1 = Layers.Dense(3, activation = \"relu\", name = \"p1\")(x)\n    # generating another output for quantile adjusting the quantile predictions\n    q_adjust = Layers.Dense(3, activation = \"relu\", name = \"p2\")(x)\n    \n    # adding the tf.cumsum of q_adjust to the output q1\n    # to ensure increasing values [a < b < c]\n    # tf.cumsum([a, b, c]) --> [a, a + b, a + b + c]\n    preds = Layers.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis = 1), \n                     name = \"preds\")([q1, q_adjust])\n    \n    model = tf.keras.Model(inputs = inp, outputs = preds, name = \"NeuralNet\")\n    model.compile(loss = mloss(_lambda), optimizer = optimizer, metrics = [score])\n    \n    return model","d404d391":"# create neural Network\nneuralNet = get_model(optimizer, lr = 0.01)\nneuralNet.summary()","0082775d":"## GET TRAINING DATA AND TARGET VALUE\n\n# get target value\ny = train_df['FVC'].values.astype(float)\n\n\n# get training & test data\nX_train = train_df[features_list].values\nX_test = sub[features_list].values\n\n# instantiate target arrays\ntrain_preds = np.zeros((X_train.shape[0], 3))\ntest_preds = np.zeros((X_test.shape[0], 3))","4f2bc569":"## Non-Stratified GroupKFold-split (can be further enhanced with stratification!)\n\"\"\"K-fold variant with non-overlapping groups.\nThe same group will not appear in two different folds: in this case we dont want to have overlapping patientIDs in TRAIN and VAL-Data!\nThe folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.\"\"\"\n\nNFOLDS = 5\ngkf = GroupKFold(n_splits = NFOLDS)\n# extract Patient IDs for ensuring \ngroups = train_df['Patient'].values\n\nOOF_val_score = []\nfold = 0\n\nfor train_idx, val_idx in gkf.split(X_train, y, groups = groups):\n    fold += 1\n    print(f\"FOLD {fold}:\")\n    \n    # callbacks: logging & model saving with checkpoints each fold\n    # callbacks = [get_lr_callback(BATCH_SIZE)]  # un-comment for using LRScheduler\n    reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n                                                          factor = 0.6,\n                                                          patience = 150,\n                                                          verbose = 1,\n                                                          epsilon = 1e-4,\n                                                          mode = 'min',\n                                                          min_lr = 0.00001)\n    \n    callbacks = [reduce_lr_loss]\n    \n    if LOGGING == True:\n        callbacks +=  [get_checkpoint_saver_callback(fold),                     \n                     LogPrintingCallback()]\n\n    # build and train model\n    model = get_model(optimizer, lr = 0.005)\n    history = model.fit(X_train[train_idx], y[train_idx], \n              batch_size = BATCH_SIZE, \n              epochs = EPOCHS, \n              validation_data = (X_train[val_idx], y[val_idx]), \n              callbacks = callbacks,\n              verbose = 0) \n    \n    # evaluate\n    print(\"Train:\", model.evaluate(X_train[train_idx], y[train_idx], verbose = 0, batch_size = BATCH_SIZE, return_dict = True))\n    print(\"Val:\", model.evaluate(X_train[val_idx], y[val_idx], verbose = 0, batch_size = BATCH_SIZE, return_dict = True))\n    \n    ## Load best model to make pred\n    model.load_weights('fold-%i.h5'%fold)\n    train_preds[val_idx] = model.predict(X_train[val_idx],\n                                         batch_size = BATCH_SIZE,\n                                         verbose = 0)\n    \n    # append OOF evaluation to calculate OFF_Score\n    OOF_val_score.append(model.evaluate(X_train[val_idx], y[val_idx], verbose = 0, batch_size = BATCH_SIZE, return_dict = True)['score'])\n    \n    # predict on test set and average the predictions over all folds\n    print(\"Predicting Test...\")\n    test_preds += model.predict(X_test, batch_size = BATCH_SIZE, verbose = 0) \/ NFOLDS","8de74a8f":"## PLOT results\n# fetch results from history\nscore = history.history['score']\nval_score = history.history['val_score']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\n# create subplots\nplt.figure(figsize = (20,5))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, score, label = 'Training Score')\nplt.plot(epochs_range, val_score, label = 'Validation Score')\n# limit y-values for better zoom-scale. Remember that roughly -4.5 is the best possible score\n# plt.ylim(0.8 * np.mean(val_score), 1.2 * np.mean(val_score))\nplt.legend(loc = 'lower right')\nplt.title('Training and Validation Score')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label = 'Training Loss')\nplt.plot(epochs_range, val_loss, label = 'Validation Loss')\n# limit y-values for beter zoom-scale\nplt.ylim(0.3 * np.mean(val_loss), 1.8 * np.mean(val_loss))\n\nplt.legend(loc = 'upper right')\nplt.title('Training and Validation Loss')\nplt.show()","52af339a":"np.mean(OOF_val_score)","a62c1856":"## FIND OPTIMIZED STANDARD-DEVIATION\nsigma_opt = mean_absolute_error(y, train_preds[:,1])\nsigma_uncertain = train_preds[:,2] - train_preds[:,0]\nsigma_mean = np.mean(sigma_uncertain)\nprint(sigma_opt, sigma_mean)","dfb2f7ac":"sub.head()","09bdf0cc":"## PREPARE SUBMISSION FILE WITH OUR PREDICTIONS\nsub['FVC1'] = test_preds[:, 1]\nsub['Confidence1'] = test_preds[:,2] - test_preds[:,0]\n\n# get rid of unused data and show some non-empty data\nsubmission = sub[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()\nsubmission.loc[~submission.FVC1.isnull()].head(10)","26670e17":"submission.loc[~submission.FVC1.isnull(),'FVC'] = submission.loc[~submission.FVC1.isnull(),'FVC1']\n\nif sigma_mean < 70:\n    submission['Confidence'] = sigma_opt\nelse:\n    submission.loc[~submission.FVC1.isnull(),'Confidence'] = submission.loc[~submission.FVC1.isnull(),'Confidence1']","b777d999":"submission.head()","c081c277":"submission.describe().T","137a682c":"org_test = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\n\nfor i in range(len(org_test)):\n    submission.loc[submission['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'FVC'] = org_test.FVC[i]\n    submission.loc[submission['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'Confidence'] = 70","5723b35f":"submission[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index = False)","31ff0fef":"### OOF Evaluation","4767cad1":"Not the data has the format we need to work with.\nIn the next section, we go trough two possibilities on how to normalize, standardize and prepare the data for the neural Network.","33012bd0":"### What about those mysterious quantiles (q1) and quantiles_adjustment (q_adjust) layers?\nThe idea of the q1 layer is to actually predict the 3 quantiles. As you can see in the loss function and in the neural network output: we expect 3 values, one value for each of the defined quantiles.\n\nThere are 2 reasons for the q_adjust layer:  \n* First: With adding the cumulative sum on top of q1, we ensure (or at least very strongly support) that the output (preds) are in increasing order.\nLet's consider two arrays\/vectors: ```q1 = [a, b, c]``` ```q_adjust = [e, f, g]```. Then our output preds are ```q1 + tf.cumsum(q_adjust) = [a + e, b + e + f, c + e + f + g]```\n* Second: We can see q_adjust as a *baseline* which is added to the q1 layers. It adds additional degrees of freedom (more neurons == more trainable weights), which provides better results. Theoretically we could also use only q1 and add tf.cumsum(q1), then we would guarantee that we have an increasing order, but we have less degrees of freedom and the OOF-Score and LB score is worse.","600c8481":"## Getting the format right","c0486055":"# Evaluation & submission\n\n### Check loss and accuracy\nOkay, we made it! Let's evaluate our model, check our stats (Out-Of-Fold log-loss) and submit it! Let's start with some plots.\nThose plots can tell us, whether our model training is working as expected, or if it strongly overfits.  \nThe below **EXAMPLE-IMAGE** is an example of a strongly overfitting model:\n![image.png](attachment:image.png)\nLong before we hit the 10th epoch, the validation loss is increasing again, while the training loss keeps decreasing. We can also clearly ovserve that there is no improvement in our accuracy anymore. \n\nWhat can we do against strongly overfitting models?\nWe could do the following:\n\n* Collect more training data or use augmentation to generate new data: Sadly I have no brilliant idea on how to do this for this specific Kaggle competition.\n* Reduce the network\u2019s size (width andor\/ dept) by removing layers or reducing the number of neurons in the hidden layers\n* Use regularization like LASSO (=Least Absolute Shrinkage and Selection Operator; aka L1 regularization) or Ridge (aka L2 regularization) which results in adding a cost-term to the loss function\n* Use higher dropout-rate in the Dropout-Layers, which will randomly remove more connections by setting them to zero and forcing the network to generalize better (=avoid relying on a limitied number of strong-influence neurons).\n\nBut now check our model's result and evaluate it:","45782ec4":"Let's start seeding everything to make results somewhat reproducible. Anyway, in keras it is quite hard to get 100% reproducible results.","bea77071":"Clearly, the third approach works best and I have learned a very clean way of processing data. ","481f7269":"In the following last step we overwrite our predictions with the known data from the orginal submission file to not waste known data.","9d23c37a":"### Loss Function","7be5780a":"# Update history:\n\n## New Notebook for CV-backed Random Grid Hyperparameter Search, click [here](https:\/\/www.kaggle.com\/chrisden\/6-82x-cv-backed-hyperp-gridsearch-quantile-reg)\nV39: Optimized Wording, Hyperparameters, reduced NFOLDs  \nV33: Improved logging\nV30, 31: enhanced model-explanations (q1 and q_adjust layers) and outputs.\nV24-29: Minor adoptions, testing multiple hyperparameters, correcting \"MONITORING\" during training: now correctly monitoring 'val_score' instead of 'score', which improves OOF score.  \nV23: Minor optimizations, wording, hyperparameters\nV19: Introduced plotting & evaluation of results  \nV18: Introduced usage of tensorflow.addons (tfa): WeightNormalization  \nV16: [Code optimizations](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/179033), see \"third approach\" in the Data-Wrangling section   \nV13&14: small corrections, edited some links  \nV12: Corrected bug for saving models correctly (see comment-section) & enhanced readabilty  \nV11: Introduced model checkpoints\/saving  \nV10: Introduced configurable Learning-Rate-Schedulers  \nV8&9: Introduced GroupKFolds to get leak-free cross-validation strategy to evaluate models & training\n","3680aafc":"# First glimpse at the data","ed5e037e":"# Model & Loss\nIn this section we are going to define the loss & a first model.\nFirst we are taking care of the loss. We are trying to minimize the following:\n\n![image.png](attachment:image.png)\n\nThe global minimum of this function is achieved for delta = 0 and sigma = 70, which [results in a loss of roughly -4.59.](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/168469).\n\nGetting our model to predict the Confidence and FVC values (which is what we need!) is not working fine so far, as you can read [here](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/167764).\nCurrently the way to go seems to be pinball loss. \n","5ffe32e2":"# Quick intro\n\n\n## Competition description\nImagine one day, your breathing became consistently labored and shallow. Months later you were finally diagnosed with pulmonary fibrosis, a disorder with no known cause and no known cure, created by scarring of the lungs. If that happened to you, you would want to know your prognosis. That\u2019s where a troubling disease becomes frightening for the patient: outcomes can range from long-term stability to rapid deterioration, but doctors aren\u2019t easily able to tell where an individual may fall on that spectrum. Your help, and data science, may be able to aid in this prediction, which would dramatically help both patients and clinicians.  \n\nCurrent methods make fibrotic lung diseases difficult to treat, even with access to a chest CT scan. In addition, the wide range of varied prognoses create issues organizing clinical trials. Finally, patients suffer extreme anxiety\u2014in addition to fibrosis-related symptoms\u2014from the disease\u2019s opaque path of progression.\n\nOpen Source Imaging Consortium (OSIC) is a not-for-profit, co-operative effort between academia, industry and philanthropy. The group enables rapid advances in the fight against Idiopathic Pulmonary Fibrosis (IPF), fibrosing interstitial lung diseases (ILDs), and other respiratory diseases, including emphysematous conditions. Its mission is to bring together radiologists, clinicians and computational scientists from around the world to improve imaging-based treatments.\n\nIn this competition, you\u2019ll predict a patient\u2019s severity of decline in lung function based on a CT scan of their lungs. You\u2019ll determine lung function based on output from a spirometer, which measures the volume of air inhaled and exhaled. The challenge is to use machine learning techniques to make a prediction with the image, metadata, and baseline FVC as input.\n\nIf successful, patients and their families would better understand their prognosis when they are first diagnosed with this incurable lung disease. Improved severity detection would also positively impact treatment trial design and accelerate the clinical development of novel treatments.\n \n \n### What should I expect the data format to be & what am I predicting?\nEach row in the dataset contains a Patiend_ID, and a week; you must predict the FVC and a confidence. To avoid potential leakage in the timing of follow up visits, you are asked to predict every patient's FVC measurement for every possible week. Those weeks which are not in the final three visits are ignored in scoring.\n\n\nThe final submission-file should contain a header and have the following format:\n\n```      Patient_Week,                 FVC,    Confidence\nID00002637202176704235138_1, 2000,   100\nID00002637202176704235138_2, 2000,   100\nID00002637202176704235138_3, 2000,   100\n```\n\n","d50c2548":"Okay, now we encoded all the data and want to have a look at it. But wait, we only get back a series element? How to put that in a Dataframe again?\nSadly, not as easy as I had hoped. We need to use ```pd.DataFrame(data, columns=column_names)``` function, for which we need the column names. But we used OneHot-Encoding. So the number of columns now depends on how many different values\/categories a categorical value has, because for each unique value we get a separate column: e.g. If for the column \"SmokingStatus\" we only have the values \"Smoker\" and \"Never-Smoked\", we have two resulting columns if there are additional possible values like \"Ex_Smoker\" we get more columns. And we also should not get things wrong and mix the columns up. The following code is getting our data back to a Dataframe and preserving the correct order.","fdc485bd":"\nThe first big challenge is data wrangling: \nWe could see that some patients take FVE measurements only after their baseline CT-Images, and some took measurements before that.\nSo let's first find out what the actual baseline-week and baseline-FVC for each Patient is.  \nWe start with the baseline week:\n","1b9074d7":"We can eighter drop all except the first or last duplicate or average them.As there are only a few duplicates, we can drop them without a bad conciousness for loosing to much data for our first apporach.","99dd390d":"What we can see here, is that the Patient with ID ending on \"430\" had his first FVC measure 4 weeks before the first (baseline) CT images ( = \"Weeks\" column -4) were taken. Then the patient took the next FVC measurement 9 weeks later. \nIn the next step we need to baseline the FVC values. Note, that the **BASELINE-FVC it not the minimum FVC**, but the first measurement, meaning the measurement taken in the \"min_week\" or ```baselined_week = 0```.\n\nFor getting the baselined FVC I first wrote the following straightforward function:","22bbd8c9":"\nI wanted to know how much this speeds up the processing, you can find the results in the following:","5d34141d":"To get a ColumnTransformer which outputs the whole DataFrame compatible format, we can create a class that takes attributes that we don't want to change, and simply passes them through.\nThis class needs a ```fit``` and a ```transform``` method, so that the ColumnTransformer itself can use ```fit_transform``` like for the numerical and categorical attributes.\n\nAs I learned in a comment from @frederiklaubisch, sklearns' ColumnTransformer has a 'remainder' parameter that can be set to 'passtrough', which would eliminate the need for the NoTranformer.","597502d4":"## Neural Network Model\nIn this section we build an initial neural Network. The code of this section is derived from [Ulrich's](https:\/\/www.kaggle.com\/ulrich07) great [notebook](https:\/\/www.kaggle.com\/ulrich07\/osic-multiple-quantile-regression-starter), which also inspired me to change my loss to the above coded version. Please support the original Notebook creators! The chosen quartiles are simply derived by testing; using 0.25 and 0.75 leads to worse results.\n\nFor the architecture: It's good practice to use numbers of units following the schema 2^x, with x element of N (= resulting in 1, 2, 4, 8, 16, 32, 64, 128,..).  \nWe are going to use dropout for regularization and not a too broad and deep network, as the training data is very limited.\n\n### Normalization & Regularization\nWe are going to use weight-normalization (link to the paper, click [here](https:\/\/arxiv.org\/abs\/1602.07868)) from ```tensorflow_addons``` to support faster convergence. \nThe authors describe the method like this:\n> Weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch.\n\nAdditionally we gain more robustness for the choosing of the hyperparameter learning rate. Cited from page 3:\n> Empirically, we find that the ability to grow the norm ||v|| makes optimization of neural networks\n> with weight normalization very robust to the value of the learning rate: If the learning rate is too\n> large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate\n> is reached.\n\n### Activation function\nAs the given task without the usage of images is not very compute-intensive (you don't need a GPU, CPU will do), we will change the activation-function from 'relu' to 'elu'.\nFor more info you can read [here.](https:\/\/mlfromscratch.com\/activation-functions-explained\/#elu), below you can find a short summary:\n\n**Pros**\n* Avoids the \"dead ReLu\" problem: ReLus provides activation-values & gradients of 0 for negative input values\n* Produces activations for negative inputs instead of letting them be zero when calculating the gradient.\n* Produces negative outputs, which helps the network nudge weights and biases in the right directions for negative inputs, too.\n\n**Cons**\n* Introduces longer computation time, because of the exponential operation included.\n* Does not avoid the exploding gradient problem.","fc5a2be7":"Okay, we made it! Let's finally check our stats and submit it!","02f22a9d":"In the next section we are going to use the ```train_preds``` to calculate the optimized sigma, which is a measure for certainty or rather uncertainty. We can do that, as we have both: the model's estimate and the real data. We subtract the lower quartile from the upper quartile (defined in the loss function) and average it.","0883d887":"## Domain knowledge\n\n### Some domain knowledge can be gained from watching the following video and from reading [here.](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/165727)","4549171f":"To get an idea of the meaning of the weeks column, let's check some of their values.","a4a1d603":"## <font color='blue'>Thanks a lot for reading, I hope you could gain as much insights from reading this as I got from writing it.","37630add":"Okay, so the second apporach (using our own implementation) was more straightforward and less code.  Downside: if you want to replace the MinMaxScaler with another scaling method (RobustScaler, StdScaler), you need to implement it first.","f69bc37e":"The second apporach is using ```transform```, which is not as known as ```apply```, but faster for basic-operations not involving multiple columns of a dataframe. Here is an interesting post about it for those, who want to learn more:\n[Apply vs transform.](https:\/\/stackoverflow.com\/questions\/27517425\/apply-vs-transform-on-a-group-object)\n\nIt still wasn't perfect and I found a **third, super-clean approach** [here](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/179033):\n    \n","4ab3d1d8":"In this section you can configure the following:\n* Features used for training\n* Basic training setup: BATCH_SIZE and EPOCHS,\n* Configuration for the loss function\n* Optimizers, Learning-Rate-Schedulers incl. Learning Rate start- & endpoint\n* Custom Logging Callback\n* Checkpoint-Saving Callback\n\nThe Learning-Rate scheduler below is inspired by Chris great [Melanoma-detection notebook](https:\/\/www.kaggle.com\/cdeotte\/triple-stratified-kfold-with-tfrecords).  \nFeel free to experiment with the scheduler and it's max\/min and decay values.\n\n**Ever wondered why lr_max is scaled by BATCH_SIZE and therefore bigger for larger batches?** The reason for this is the following: the larger the BATCH_SIZE, the more averaged & smoothened a step of gradient decent is and the bigger our confidence in the *direction* of the step is. As there is less \"randomness\" in a huge averaged batch (compared with for example Stochastic Gradient Decent (=SGD) with batch size = 1) and our confidence in the direction is higher, the learning rate can be bigger to advance fast to the optimum.","770450d1":"As we can clearly see, some patients took the first measure of FVC before and some after their baseline CT images.\n> the relative number of weeks pre\/post the baseline CT (may be negative)","ab68ecce":"## Preparing the data for the Neural Network","1955fd64":"# Data Wrangling","744101a2":"In this section we are going to do all the Data-Wrangling and pre-processing. For this we are going to define some functions and transformations, which then are applied to the data.\nIt's good practice to concatinate all tabular data (train, test, submission), to ensure all data get's the same & correct treatment.\nIf you don't do that, you need to be careful with some steps, e.g.: \n* Standardization or Normalization (e.g. MinMax Scaling) in ```test_df``` will not have the same range of values (e.g. min\/max values) and therefore scaling than in ```train_df```.\n* The categorical features might have different categories in ```test_df``` than in ```train_df``` (e.g. ```test_df``` only contains male, Ex-smokers).\n\nSo let's concatinate all our data first and then start with the transformations.","cb38bfbc":"## Load all dependencies you need\n<span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> from  <\/span> coffee  <span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> import  <\/span> ***** ","70929574":"This apporach works fine, but as it contains a lot of look-ups, its slow and didn't feel right.  \nBtw: there is an even worse approach: Using ```for row in df.iterrows()``` is roughly 8 times slower than using ```for idx in df.index```.  \nSo I looked up how other people solved it and I found a rough equivalent to the following function:","80a8259c":"We need to get this into a format which we can easily use for making our predictions, so let's split up the ```Patient_Week``` column into a ```Patient``` and a ```Weeks``` column to align it with the train & test data-format.\nThen we merge our info from the test data to the ```submission_df```: that's the fastest way of getting the correct format for predictions and submissions later on.","366b808b":"### The first apporach is using sklearn, as it is super famous and used frequently.","086d471c":"# <font color='blue'>CONFIG Section <\/font>","470c24c8":"### The 2nd approach is doing all the legwork ourselfs.\nThe good thing is: we dont need a NoTransformer here, as we simply can work in the DataFrame itself and not change any data which we want to preserve.\nDownside is, we need to implement the MinMaxScaler by hand. Make sure to not call it MinMaxScaler and shadow the already important MinMaxScaler from Sklearn!\n","4b289e7a":"Okay wow, that was not as easy as I expected. The upside is, it's easy to add some additional features and pump them through the Pipeline or to change the Pipeline itself (e.g. exchanging MiNmaxScaler with StdScaler, RobustScaler etc).\nThe downside is: it's not intuitive (at least not for me) and takes quite some lines of code.\nDoes anybody have an idea on HOW to improve this? ","de7c56fb":"# Training\nIn the following we want to create leak-free folds to get a robust **cross-validation strategy** in order to evaluate all our models & our training. The idea is to avoid having the same patient (= PatientID) in training- and in validation-Data, as this might lead to evaluate a higher CV-score for a model which is luckily learning\/memorizing the data for a particular patientID which is also frequently occuring in the validation-data.  \n\n\nThe idea on how to do that is coming from @PAB97 [Pierre's great notebook (CHECK IT OUT!)](https:\/\/www.kaggle.com\/rftexas\/osic-eda-leak-free-kfold-cv-lgb-baseline#kln-440)\nPlease note, that we still don't use propoer stratification based on 'Age', 'Sex', 'SmokingStatus'."}}