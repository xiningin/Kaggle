{"cell_type":{"bab59634":"code","7a7b4cd9":"code","b1c25024":"code","e14f82bd":"code","9863648b":"code","36721668":"code","e0ac39b5":"code","f8d46167":"code","823bcf23":"code","3a5c6c48":"code","e6473dcb":"code","6344655a":"code","81073f23":"code","526ee7bf":"code","070c5c0a":"code","91855619":"code","f1001918":"code","330f8fdf":"code","ad0cbab9":"code","6f0ba8be":"code","3c35df5b":"code","00900e42":"code","ed26cc43":"code","5088396b":"code","62b9c282":"code","4754aeb4":"code","63e823b5":"code","08a83cf8":"code","c5837247":"code","0f71a182":"code","6dc507f3":"code","abce3425":"code","e350fefc":"markdown","ce47f4a7":"markdown","485255e7":"markdown","966a5382":"markdown","befc17d3":"markdown","bc8c31c6":"markdown","e277329c":"markdown","fdb6f079":"markdown","db1d8980":"markdown","66c3dc64":"markdown","592f7ec6":"markdown","c8439d1f":"markdown","e8523fe4":"markdown","62a41d3e":"markdown","4c3bbc72":"markdown","8ebd0ba6":"markdown","95d3324d":"markdown","d2ef3ff3":"markdown","5914876d":"markdown","04ac68e8":"markdown","7aaff497":"markdown","ec02fd7f":"markdown","af850941":"markdown","02ddc2e5":"markdown","33d22566":"markdown","bf3e23e5":"markdown"},"source":{"bab59634":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a7b4cd9":"import numpy as np \nimport pandas as pd\nimport pandas\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n%matplotlib inline\nimport seaborn as sns; sns.set()\n\nfrom sklearn import tree\nimport graphviz \nimport os\nimport preprocessing \n\nimport numpy as np \nimport pandas as pd \nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom xgboost import plot_tree, plot_importance\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b1c25024":"dataset = pandas.read_csv('\/kaggle\/input\/glass\/glass.csv')\ndataset.sample(5)","e14f82bd":"dataset.info()","9863648b":"def bar_plot(variable):\n    # get feature\n    var = dataset[variable]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable,varValue))","36721668":"categorical = (dataset.dtypes == \"object\")\ncategorical_list = list(categorical[categorical].index)\n\nprint(\"Categorical variables:\")\nprint(categorical_list)","e0ac39b5":"sns.set_style('darkgrid')\nfor c in categorical_list:\n    bar_plot(c)","f8d46167":"numerical_float64 = (dataset.dtypes == \"float64\")\nnumerical_float64_list = list(numerical_float64[numerical_float64].index)\n\nprint(\"Numerical variables:\")\nprint(numerical_float64_list)","823bcf23":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(dataset[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} Distribution with Histogram\".format(variable))\n    plt.show()","3a5c6c48":"for n in numerical_float64_list:\n    plot_hist(n)","e6473dcb":"plt.figure(figsize=(40,25))\n\nplt.subplot(3,3,1)\nsns.histplot(dataset['RI'], color = 'red', kde = True).set_title('RI Interval and Counts')\n\nplt.subplot(3,3,2)\nsns.histplot(dataset['Na'], color = 'green', kde = True).set_title('Na Interval and Counts')\n\nplt.subplot(3,3,3)\nsns.histplot(dataset['Mg'], kde = True, color = 'blue').set_title('Mg Interval and Counts')\n\nplt.subplot(3,3,4)\nsns.histplot(dataset['Al'], kde = True, color = 'black').set_title('Al Interval and Counts')\n\nplt.subplot(3,3,5)\nsns.histplot(dataset['Si'], kde = True, color = 'yellow').set_title('Si Interval and Counts')\n\nplt.subplot(3,3,6)\nsns.histplot(dataset['K'], kde = True, color = 'orange').set_title('K Interval and Counts')\n\nplt.subplot(3,3,7)\nsns.histplot(dataset['Ca'], kde = True, color = 'brown').set_title('Ca Interval and Counts')\n\nplt.subplot(3,3,8)\nsns.histplot(dataset['Ba'], kde = True, color = 'cyan').set_title('Ba Interval and Counts')\n\nplt.subplot(3,3,9)\nsns.histplot(dataset['Fe'], kde = True, color = 'purple').set_title('Fe Interval and Counts')","6344655a":"features = dataset.columns\nsns.set_style('darkgrid')\nsns.pairplot(dataset[features])","81073f23":"sns.pairplot(dataset, hue = 'Type')","526ee7bf":"dataset.corr()","070c5c0a":"plt.figure(figsize=(12,8)) \nsns.heatmap(dataset.corr(), annot=True, cmap='Dark2_r', linewidths = 2)\nplt.show()","91855619":"sns.set_style('darkgrid')\naxes = pandas.plotting.scatter_matrix(dataset, alpha = 0.3, figsize = (10,7), diagonal = 'kde' ,s=80)\ncorr = dataset.corr().values\n\nplt.xticks(fontsize =10,rotation =0)\nplt.yticks(fontsize =10)\nfor ax in axes.ravel():\n    ax.set_xlabel(ax.get_xlabel(),fontsize = 15, rotation = 60)\n    ax.set_ylabel(ax.get_ylabel(),fontsize = 15, rotation = 60)\n# put the correlation between each pair of variables on each graph\nfor i, j in zip(*np.triu_indices_from(axes, k=1)):\n    axes[i, j].annotate(\"%.3f\" %corr[i, j], (0.8, 0.8), xycoords=\"axes fraction\", ha=\"center\", va=\"center\")","f1001918":"plt.figure(figsize=(30,25))\nplt.subplot(3,3,1)\nsns.barplot(x = 'Type', y = 'RI', data = dataset, palette=\"cubehelix\")\nplt.subplot(3,3,2)\nsns.barplot(x = 'Type', y = 'Na', data = dataset, palette=\"Oranges\")\nplt.subplot(3,3,3)\nsns.barplot(x = 'Type', y = 'Mg', data = dataset, palette=\"Oranges\")\nplt.subplot(3,3,4)\nsns.barplot(x = 'Type', y = 'Al', data = dataset, palette=\"cubehelix\")\nplt.subplot(3,3,5)\nsns.barplot(x = 'Type', y = 'Si', data = dataset, palette=\"cubehelix\")\nplt.subplot(3,3,6)\nsns.barplot(x = 'Type', y = 'K', data = dataset, palette=\"Oranges\")\nplt.subplot(3,3,7)\nsns.barplot(x = 'Type', y = 'Ca', data = dataset, palette=\"Oranges\")\nplt.subplot(3,3,8)\nsns.barplot(x = 'Type', y = 'Ba', data = dataset, palette=\"cubehelix\")\nplt.subplot(3,3,9)\nsns.barplot(x = 'Type', y = 'Fe', data = dataset, palette=\"Oranges\")","330f8fdf":"plt.figure(figsize=(30,25))\nplt.subplot(3,3,1)\nsns.violinplot(x = 'Type', y = 'RI', data = dataset, palette=\"rocket_r\")\nplt.subplot(3,3,2)\nsns.violinplot(x = 'Type', y = 'Na', data = dataset, palette=\"rocket_r\")\nplt.subplot(3,3,3)\nsns.violinplot(x = 'Type', y = 'Mg', data = dataset, palette=\"rocket_r\")\nplt.subplot(3,3,4)\nsns.violinplot(x = 'Type', y = 'Al', data = dataset, palette=\"rocket_r\")\nplt.subplot(3,3,5)\nsns.violinplot(x = 'Type', y = 'Si', data = dataset, palette=\"rocket_r\")\nplt.subplot(3,3,6)\nsns.violinplot(x = 'Type', y = 'K', data = dataset, palette=\"rocket_r\")\nplt.subplot(3,3,7)\nsns.violinplot(x = 'Type', y = 'Ca', data = dataset, palette=\"rocket_r\")\nplt.subplot(3,3,8)\nsns.violinplot(x = 'Type', y = 'Ba', data = dataset, palette=\"rocket_r\")\nplt.subplot(3,3,9)\nsns.violinplot(x = 'Type', y = 'Fe', data = dataset, palette=\"rocket_r\")","ad0cbab9":"plt.figure(figsize=(30,25))\nplt.subplot(3,3,1)\nsns.boxplot(x = 'Type', y = 'RI', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3,3,2)\nsns.boxplot(x = 'Type', y = 'Na', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3,3,3)\nsns.boxplot(x = 'Type', y = 'Mg', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3,3,4)\nsns.boxplot(x = 'Type', y = 'Al', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3,3,5)\nsns.boxplot(x = 'Type', y = 'Si', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3,3,6)\nsns.boxplot(x = 'Type', y = 'K', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3,3,7)\nsns.boxplot(x = 'Type', y = 'Ca', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3,3,8)\nsns.boxplot(x = 'Type', y = 'Ba', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3,3,9)\nsns.boxplot(x = 'Type', y = 'Fe', data = dataset, palette=\"gist_ncar_r\")","6f0ba8be":"plt.figure(figsize=(30,25))\nplt.subplot(3,3,1)\nsns.distplot(dataset['RI'], color=\"red\").set_title('RI Interval')\nplt.subplot(3,3,2)\nsns.distplot(dataset['Na'], color=\"green\").set_title('Na Interval')\nplt.subplot(3,3,3)\nsns.distplot(dataset['Mg'], color=\"blue\").set_title('Mg Interval')\nplt.subplot(3,3,4)\nsns.distplot(dataset['Al'], color=\"black\").set_title('Al Interval')\nplt.subplot(3,3,5)\nsns.distplot(dataset['Si'], color=\"yellow\").set_title('Si Interval')\nplt.subplot(3,3,6)\nsns.distplot(dataset['K'], color=\"orange\").set_title('K Interval')\nplt.subplot(3,3,7)\nsns.distplot(dataset['Ca'], color=\"grey\").set_title('Ca Interval')\nplt.subplot(3,3,8)\nsns.distplot(dataset['Ba'], color=\"cyan\").set_title('Ba Interval')\nplt.subplot(3,3,9)\nsns.distplot(dataset['Fe'], color=\"purple\").set_title('Fe Interval')","3c35df5b":"plt.figure(1, figsize=(5,5))\nplt.title(\"Distribution of Type\")\ndataset['Type'].value_counts().plot.pie(autopct=\"%1.1f%%\")","00900e42":"import pandas_profiling as pp\npp.ProfileReport(dataset)","ed26cc43":"X = dataset.iloc[:,0:9].values \ny = dataset.iloc[:,9:].values ","5088396b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101) \nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in validation dataset: {len(X_valid)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')","62b9c282":"sc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","4754aeb4":"models = {\n    'GaussianNB': GaussianNB(),\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'SupportVectorMachine': SVC(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n    'Neural Nets': MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1),\n}\n\nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n              'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n              'Stochastic Gradient Descent', 'Neural Nets']\n\ntrainScores = []\nvalidationScores = []\ntestScores = []\n\nfor m in models:\n    model = models[m]\n    model.fit(X_train, y_train)\n    score = model.score(X_valid, y_valid)\n    print(f'{m} validation score => {score*100}')\n    \n    print(f'{m}') \n    train_score = model.score(X_train, y_train)\n    print(f'Train score of trained model: {train_score*100}')\n    trainScores.append(train_score*100)\n\n    validation_score = model.score(X_valid, y_valid)\n    print(f'Validation score of trained model: {validation_score*100}')\n    validationScores.append(validation_score*100)\n\n    test_score = model.score(X_test, y_test)\n    print(f'Test score of trained model: {test_score*100}')\n    testScores.append(test_score*100)\n    print(\" \")\n   \n    y_predictions = model.predict(X_test)\n    conf_matrix = confusion_matrix(y_predictions, y_test)\n\n    print(f'Confussion Matrix: \\n{conf_matrix}\\n')\n\n    predictions = model.predict(X_test)\n    cm = confusion_matrix(predictions, y_test)\n\n    tn = conf_matrix[0,0]\n    fp = conf_matrix[0,1]\n    tp = conf_matrix[1,1]\n    fn = conf_matrix[1,0]\n    accuracy  = (tp + tn) \/ (tp + fp + tn + fn)\n    precision = tp \/ (tp + fp)\n    recall    = tp \/ (tp + fn)\n    f1score  = 2 * precision * recall \/ (precision + recall)\n    specificity = tn \/ (tn + fp)\n    print(f'Accuracy : {accuracy}')\n    print(f'Precision: {precision}')\n    print(f'Recall   : {recall}')\n    print(f'F1 score : {f1score}')\n    print(f'Specificity : {specificity}')\n    print(\"\") \n    print(f'Classification Report: \\n{classification_report(predictions, y_test)}\\n')\n    print(\"\")\n      \nfor m in range (1):\n    current = modelNames[m]\n    modelNames.remove(modelNames[m])\n\n    preds = model.predict(X_test)\n    confusion_matr = confusion_matrix(y_test, preds) #normalize = 'true'\n    print(\"############################################################################\")\n    print(\"\")\n    print(\"\")\n    print(\"\")","63e823b5":"plt.figure(figsize=(20,10))\nsns.set_style('darkgrid')\nplt.title('Train - Validation - Test Scores of Models', fontweight='bold', size = 24)\n\nbarWidth = 0.25\n \nbars1 = trainScores\nbars2 = validationScores\nbars3 = testScores\n \nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \nplt.bar(r1, bars1, color='blue', width=barWidth, edgecolor='white', label='train', yerr=0.5,ecolor=\"black\",capsize=10)\nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='validation', yerr=0.5,ecolor=\"black\",capsize=10, alpha = .50)\nplt.bar(r3, bars3, color='red', width=barWidth, edgecolor='white', label='test', yerr=0.5,ecolor=\"black\",capsize=10, hatch = '-')\n \nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n              'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n    \nplt.xlabel('Algorithms', fontweight='bold', size = 24)\nplt.ylabel('Scores', fontweight='bold', size = 24)\nplt.xticks([r + barWidth for r in range(len(bars1))], modelNames, rotation = 75)\n \nplt.legend()\nplt.show()","08a83cf8":"for i in range(10):\n    print(f'Accuracy of {modelNames[i]} -----> {testScores[i]}')","c5837247":"models = {\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n}\n\nfor m in models:\n    model = models[m]\n    model.fit(X_train, y_train)\n  \n    print(f'{m}') \n    best_features = SelectFromModel(model)\n    best_features.fit(X, y)\n\n    transformedX = best_features.transform(X)\n    print(f\"Old Shape: {X.shape} New shape: {transformedX.shape}\")\n    print(\"\\n\")","0f71a182":"from sklearn.decomposition import PCA\nselectedX = X[:,]\n\npcaX = PCA(n_components=2)\npcaX = pcaX.fit(selectedX)\npcaX = pcaX.transform(X)\nprint(pcaX.shape)\n\nplt.scatter(pcaX[:,0], pcaX[:,1])\nplt.show()","6dc507f3":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nldaX = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)\nprint(ldaX.shape)\n\nplt.scatter(ldaX[:,0], ldaX[:,1])\nplt.show()","abce3425":"ldaX = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)\npcaX = PCA(n_components=2).fit_transform(X, y)\n\n\nplt.figure(figsize=(25,8))\n\nplt.subplot(1,2,1)\nplt.title('PCA')\nplt.scatter(pcaX[:,0], pcaX[:, 1])\n\nplt.subplot(1,2,2)\nplt.title('LDA')\nplt.scatter(ldaX[:,0], ldaX[:, 1])\n\n\nplt.show()","e350fefc":"PCA \u00e9 m at\u00e9cnica esta\u00edstica muito \u00fatil usada na \u00e1rea de reconhecimento e classifica\u00e7\u00e3o e compreens\u00e3o de imagens, por exemplo. O princiapal objetivo \u00e9 manter o conjunto de dados com alta varian\u00e7a, reduzindo as suas dimens\u00f5es. As dimens\u00f5es suprimidas contem pouca informa\u00e7\u00e3o sobre a popula\u00e7\u00e3o. O m\u00e9todo combina vari\u00e1veis altamente correlacionadaspara criar um menor conjunto de vari\u00e1veis artificiais chamadas de componente principais que representamm as maiores varia\u00e7\u00f5es dos dados. \n\nPCA \u00e9 um m\u00e9todo muito efetivo para revelar as informa\u00e7\u00f5es importantes nos dados. O m\u00e9todo busca mostrar dados multidimensionais com menores veri\u00e1veis as quais capturam as caracter\u00edsticas b\u00e1sicas das amostras.","ce47f4a7":"Em ci\u00eancia de dados, redu\u00e7\u00e3o dimensional \u00e9 uma transforma\u00e7\u00e3o nos dados que diminui o n\u00famero de dimens\u00f5es que os representam sem perda do seu significado. Esta redu\u00e7\u00e3o conduz a menor necessidade de processamento, sendo frequentemente usada em processamento de sinais, recionhecimento de fala, neuroinform\u00e1tica, bioinform\u00e1tica, onde um grande conjunto de observa\u00e7\u00f5es e vari\u00e1veis s\u00e3o examinadas.","485255e7":"Standardization \u00e9 um m\u00e9todo que transforma os dados de forma a se ter m\u00e9dia zero e desvio padr\u00e3o de 1 e a distribui\u00e7\u00e3o tende a ser normal. A f\u00f3rmula envolve a subtra\u00e7\u00e3o do valor m\u00e9dio seguida pela divis\u00e3o pela varian\u00e7a.","966a5382":"<a id=\"16\"><\/a> \n# Conclusion","befc17d3":"Neste databook foi examinado o Glass Identification Database: an\u00e1lise explorat\u00f3ria dos dados, visualiza\u00e7\u00e3o, t\u00e9cnicas de ML e m\u00e9tricas de avalia\u00e7\u00e3o, bem como redu\u00e7\u00e3o dimensional. \n","bc8c31c6":"<a id=\"3\"><\/a> \n## Descri\u00e7\u00e3o das Vari\u00e1veis","e277329c":"<a id=\"5\"><\/a> \n### Vari\u00e1veis Categ\u00f3ricas","fdb6f079":"Aplicam-se algoritmos de ML ao dataset. Os resultados conter\u00e3o scores de treinamento, teste e valida\u00e7\u00e3o, matriz de confus\u00e3o, informa\u00e7\u00f5es estat\u00edsticas e relat\u00f3rios de classifica\u00e7\u00e3o para cada algoritmo.","db1d8980":"*** Categorical Variables:** [ ]\n\n*** Numerical Variables:** ['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe']","66c3dc64":"<a id=\"8\"><\/a> \n# Visualiza\u00e7\u00e3o","592f7ec6":"Content:\n\n1. [Importing the Necessary Libraries](#1)\n1. [Read Datas & Explanation of Features & Information About Datasets](#2)\n   1. [Variable Descriptions](#3)\n   1. [Univariate Variable Analysis](#4)\n      1. [Categorical Variables](#5)\n      1. [Numerical Variables](#6)\n1. [Correlation](#7)\n1. [Data Visualization](#8)\n1. [Pandas Profiling](#9)\n1. [Train-Test Split](#10)\n1. [Scores of Models](#11)\n1. [Best Features Selection](#12)\n1. [Dimensionality Reduction](#13)\n   1. [Principle Component Analysis (PCA)](#14)\n   1. [Linear Discriminant Analysis (LDA)](#15)\n1. [Conclusion](#16)      ","c8439d1f":"<a id=\"14\"><\/a>\n## An\u00e1lise de Componentes Principais (Principle Component Analysis - PCA)","e8523fe4":"<a id=\"9\"><\/a> \n# PERFIL Pandas","62a41d3e":"# Explorando An\u00e1lise de Dados no Glass Identification Database e ML usando o Kaggle e Colab","4c3bbc72":"Pandas profiling \u00e9 uma biblioteca bem \u00fatil que gera relat\u00f3rios sobre os dados. Com ele pode-se recuperar os tipos de dados, sua  distribui\u00e7\u00e3o e v\u00e1rias informa\u00e7\u00f5es estat\u00edsticas. A ferramenta tem muitas t\u00e9cnicas para preapra\u00e7\u00e3o dos dados. Bibliotecas gr\u00e1ficas envolvendo mapas de caracter\u00edsticas e correla\u00e7\u00e3o. Mais detalhes em: https:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/master\/rtd\/","8ebd0ba6":"<a id=\"7\"><\/a> \n# Correla\u00e7\u00e3o","95d3324d":"<a id=\"6\"><\/a> \n### Vari\u00e1veis Num\u00e9ricas","d2ef3ff3":"<a id=\"15\"><\/a>\n## An\u00e1lise Discriminat\u00f3ria Linear (Linear Discriminant Analysis LDA)","5914876d":"An\u00e1lise Discriminat\u00f3ria Linear \u00e9 usada como t\u00e9cnica de redu\u00e7\u00e3o dimensional durante o est\u00e1gio de pr\u00e9-processamento em aplica\u00e7\u00f5es de ML. O objetivo \u00e9 impedir overfitting enquanto reduz custo computacional. Mesmo sendo similar ao PCA, o LDA tem como objetivo maximizar a dist\u00e2ncia entre as classes, enquanto o PCA tentar maximizar a dist\u00e2ncia entre os pontos do dataset.\n\nEm resumo o LDA reduz o tamanho do data set atrav\u00e9s da maximiza\u00e7\u00e3o da diferen\u00e7a entre as classes.","04ac68e8":"<a id=\"12\"><\/a>\n# Sele\u00e7\u00e3o das Melhores Features","7aaff497":"<a id=\"10\"><\/a> \n# Divis\u00e3o entre Treinamento e Teste","ec02fd7f":"<a id=\"13\"><\/a>\n# Redu\u00e7\u00e3o Dimensional","af850941":"<a id=\"11\"><\/a> \n# Scores dos Modelos","02ddc2e5":"<a id=\"1\"><\/a> \n# Importando Bibliotecas Necess\u00e1rias","33d22566":"<a id=\"4\"><\/a> \n## An\u00e1lise das Vari\u00e1veis","bf3e23e5":"<a id=\"2\"><\/a> \n# L\u00ea e apresenta informa\u00e7\u00f5es sobre o Dataset"}}