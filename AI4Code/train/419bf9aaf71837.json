{"cell_type":{"f38ca47c":"code","a957b713":"code","607e17da":"code","24eb4ae1":"code","9b093b6c":"code","35daddc8":"code","bd4aaa83":"code","95e060fa":"code","2f6900b8":"code","530fb45e":"code","2f57cbbc":"code","741c3829":"code","5da97d8b":"code","751eebcc":"code","ba3922a6":"code","d095f4ed":"code","0e95972f":"code","d69f61d5":"code","8244805a":"code","ae1e12d9":"code","5bc78c29":"code","623262f5":"code","41485ab9":"code","bc4bdd47":"code","a5e63b85":"code","c40afcbb":"code","622000bb":"code","6cc359d2":"code","a6f973d9":"code","0ad6bae1":"code","d418bae2":"code","73390ada":"code","8500e14f":"code","6ea31af1":"code","5bd10bc1":"code","f3ba1e24":"code","190d1b5a":"markdown","48d5c759":"markdown","602f22b9":"markdown","e1a5703c":"markdown","6d22de38":"markdown","5cee2274":"markdown","65ed71fb":"markdown","f2f4bbab":"markdown","db068dd5":"markdown","d98f3bd1":"markdown","f574990b":"markdown","1291cbf5":"markdown","c81d2c49":"markdown","3a16ea4a":"markdown"},"source":{"f38ca47c":"!pip install -q efficientnet\nimport efficientnet.tfkeras as efn\nimport re\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)\n\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False","a957b713":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path()\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [400, 400]\nEPOCHS = 15","607e17da":"def show_batch(image_batch, label_batch):\n    plt.figure(figsize=(30,30))\n    for n in range(25):\n        ax = plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n])\n        plt.axis(\"off\")","24eb4ae1":"show_batch(image_batch.numpy(), label_batch.numpy())","9b093b6c":"filenames = tf.io.gfile.glob(str(GCS_PATH + '\/train\/train\/train\/*\/*'))\n\ntrain_filenames, val_filenames = train_test_split(filenames, test_size=0.2)","35daddc8":"train_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)\nval_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)\n\n\nTRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()\nprint(\"Training images count: \" + str(TRAIN_IMG_COUNT))\n\nVAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()\nprint(\"Validating images count: \" + str(VAL_IMG_COUNT))","bd4aaa83":"# PREDICT CLASSNAMES\nCLASS_NAMES = np.array([str(tf.strings.split(item, os.path.sep)[-1].numpy())[2:-1]\n                        for item in tf.io.gfile.glob(str(GCS_PATH + \"\/train\/train\/train\/*\"))])\nCLASS_NAMES","95e060fa":"def get_label(file_path):\n    # convert the path to a list of path components\n    parts = tf.strings.split(file_path, os.path.sep)\n    # The second to last is the class-directory\n    return int(parts[-2])","2f6900b8":"def decode_img(img):\n  # convert the compressed string to a 3D uint8 tensor\n  img = tf.image.decode_jpeg(img, channels=3,try_recover_truncated=True,acceptable_fraction=0.5)\n  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n  img = tf.image.convert_image_dtype(img, tf.float32)\n  # resize the image to the desired size.\n  return tf.image.resize(img, IMAGE_SIZE)","530fb45e":"def process_path(file_path):\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    img = tf.image.random_flip_left_right(img)\n    return img, label","2f57cbbc":"def process_path_test(file_path):\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img","741c3829":"train_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n\nval_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)","5da97d8b":"for image, label in train_ds.take(5):\n    print(\"Image shape: \", image.numpy().shape)\n    print(\"Label: \", label.numpy())","751eebcc":"files=pd.read_csv(\"..\/input\/shopee-product-detection-student\/test.csv\")[\"filename\"]\nflist=[]\nfor i in range(12186):\n    flist.append(str(GCS_PATH + '\/test\/test\/test\/'+ files[i]))","ba3922a6":"def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n    # This is a small dataset, only load it once, and keep it in memory.\n    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n    # fit in memory.\n    if cache:\n        if isinstance(cache, str):\n            ds = ds.cache(filename)\n        else:\n            ds = ds.cache()\n\n    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n\n    # Repeat forever\n    ds = ds.repeat()\n\n    ds = ds.batch(BATCH_SIZE)\n\n    # `prefetch` lets the dataset fetch batches in the background while the model\n    # is training.\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n\n    return ds","d095f4ed":"train_ds = prepare_for_training(train_ds)\nval_ds = prepare_for_training(val_ds)\n\nimage_batch, label_batch = next(iter(train_ds))","0e95972f":"print(\"Start\")\ntest_list_ds = tf.data.Dataset.from_tensor_slices(flist)\nTEST_IMAGE_COUNT = tf.data.experimental.cardinality(test_list_ds).numpy()\ntest_ds = test_list_ds.map(process_path_test, num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.batch(BATCH_SIZE)\n\nTEST_IMAGE_COUNT","d69f61d5":"with strategy.scope():\n    enb7 = efn.EfficientNetB7(input_shape=[*IMAGE_SIZE, 3], weights='noisy-student', include_top=False)\n    enb7.trainable = False \n    \n    model1 = tf.keras.Sequential([\n        enb7,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dropout(0.14),\n        tf.keras.layers.Dense(len(CLASS_NAMES), activation='softmax')\n    ])\n        \nmodel1.compile(\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4 * strategy.num_replicas_in_sync, beta_1=0.9, beta_2=0.999, amsgrad=False),\n    loss = 'sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy']\n)\nmodel1.summary()","8244805a":"COUNT_00 = len([filename for filename in train_filenames if \"\/00\/\" in filename])\nprint(\"category 00 images count in training set: \" + str(COUNT_00))\nCOUNT_01 = len([filename for filename in train_filenames if \"\/01\/\" in filename])\nprint(\"category 01 images count in training set: \" + str(COUNT_01))\nCOUNT_02 = len([filename for filename in train_filenames if \"\/02\/\" in filename])\nprint(\"category 02 images count in training set: \" + str(COUNT_02))\nCOUNT_03 = len([filename for filename in train_filenames if \"\/03\/\" in filename])\nprint(\"category 03 images count in training set: \" + str(COUNT_03))\nCOUNT_04 = len([filename for filename in train_filenames if \"\/04\/\" in filename])\nprint(\"category 04 images count in training set: \" + str(COUNT_04))\nCOUNT_05 = len([filename for filename in train_filenames if \"\/05\/\" in filename])\nprint(\"category 05 images count in training set: \" + str(COUNT_05))\nCOUNT_06 = len([filename for filename in train_filenames if \"\/06\/\" in filename])\nprint(\"category 06 images count in training set: \" + str(COUNT_06))\nCOUNT_07 = len([filename for filename in train_filenames if \"\/07\/\" in filename])\nprint(\"category 07 images count in training set: \" + str(COUNT_07))\nCOUNT_08 = len([filename for filename in train_filenames if \"\/08\/\" in filename])\nprint(\"category 08 images count in training set: \" + str(COUNT_08))\nCOUNT_09 = len([filename for filename in train_filenames if \"\/09\/\" in filename])\nprint(\"category 09 images count in training set: \" + str(COUNT_09))\nCOUNT_10 = len([filename for filename in train_filenames if \"\/10\/\" in filename])\nprint(\"category 10 images count in training set: \" + str(COUNT_10))\nCOUNT_11 = len([filename for filename in train_filenames if \"\/11\/\" in filename])\nprint(\"category 11 images count in training set: \" + str(COUNT_11))\nCOUNT_12 = len([filename for filename in train_filenames if \"\/12\/\" in filename])\nprint(\"category 12 images count in training set: \" + str(COUNT_12))\nCOUNT_13 = len([filename for filename in train_filenames if \"\/13\/\" in filename])\nprint(\"category 13 images count in training set: \" + str(COUNT_13))\nCOUNT_14 = len([filename for filename in train_filenames if \"\/14\/\" in filename])\nprint(\"category 14 images count in training set: \" + str(COUNT_14))\nCOUNT_15 = len([filename for filename in train_filenames if \"\/15\/\" in filename])\nprint(\"category 15 images count in training set: \" + str(COUNT_15))\nCOUNT_16 = len([filename for filename in train_filenames if \"\/16\/\" in filename])\nprint(\"category 16 images count in training set: \" + str(COUNT_16))\nCOUNT_17 = len([filename for filename in train_filenames if \"\/17\/\" in filename])\nprint(\"category 17 images count in training set: \" + str(COUNT_17))\nCOUNT_18 = len([filename for filename in train_filenames if \"\/18\/\" in filename])\nprint(\"category 18 images count in training set: \" + str(COUNT_18))\nCOUNT_19 = len([filename for filename in train_filenames if \"\/19\/\" in filename])\nprint(\"category 19 images count in training set: \" + str(COUNT_19))\nCOUNT_20 = len([filename for filename in train_filenames if \"\/20\/\" in filename])\nprint(\"category 20 images count in training set: \" + str(COUNT_20))\nCOUNT_21 = len([filename for filename in train_filenames if \"\/21\/\" in filename])\nprint(\"category 21 images count in training set: \" + str(COUNT_21))\nCOUNT_22 = len([filename for filename in train_filenames if \"\/22\/\" in filename])\nprint(\"category 22 images count in training set: \" + str(COUNT_22))\nCOUNT_23 = len([filename for filename in train_filenames if \"\/23\/\" in filename])\nprint(\"category 23 images count in training set: \" + str(COUNT_23))\nCOUNT_24 = len([filename for filename in train_filenames if \"\/24\/\" in filename])\nprint(\"category 24 images count in training set: \" + str(COUNT_24))\nCOUNT_25 = len([filename for filename in train_filenames if \"\/25\/\" in filename])\nprint(\"category 25 images count in training set: \" + str(COUNT_25))\nCOUNT_26 = len([filename for filename in train_filenames if \"\/26\/\" in filename])\nprint(\"category 26 images count in training set: \" + str(COUNT_26))\nCOUNT_27 = len([filename for filename in train_filenames if \"\/27\/\" in filename])\nprint(\"category 27 images count in training set: \" + str(COUNT_27))\nCOUNT_28 = len([filename for filename in train_filenames if \"\/28\/\" in filename])\nprint(\"category 28 images count in training set: \" + str(COUNT_28))\nCOUNT_29 = len([filename for filename in train_filenames if \"\/29\/\" in filename])\nprint(\"category 29 images count in training set: \" + str(COUNT_29))\nCOUNT_30 = len([filename for filename in train_filenames if \"\/30\/\" in filename])\nprint(\"category 30 images count in training set: \" + str(COUNT_30))\nCOUNT_31 = len([filename for filename in train_filenames if \"\/31\/\" in filename])\nprint(\"category 31 images count in training set: \" + str(COUNT_31))\nCOUNT_32 = len([filename for filename in train_filenames if \"\/32\/\" in filename])\nprint(\"category 32 images count in training set: \" + str(COUNT_32))\nCOUNT_33 = len([filename for filename in train_filenames if \"\/33\/\" in filename])\nprint(\"category 33 images count in training set: \" + str(COUNT_33))\nCOUNT_34 = len([filename for filename in train_filenames if \"\/34\/\" in filename])\nprint(\"category 34 images count in training set: \" + str(COUNT_34))\nCOUNT_35 = len([filename for filename in train_filenames if \"\/35\/\" in filename])\nprint(\"category 35 images count in training set: \" + str(COUNT_35))\nCOUNT_36 = len([filename for filename in train_filenames if \"\/36\/\" in filename])\nprint(\"category 36 images count in training set: \" + str(COUNT_36))\nCOUNT_37 = len([filename for filename in train_filenames if \"\/37\/\" in filename])\nprint(\"category 27 images count in training set: \" + str(COUNT_37))\nCOUNT_38 = len([filename for filename in train_filenames if \"\/38\/\" in filename])\nprint(\"category 28 images count in training set: \" + str(COUNT_38))\nCOUNT_39 = len([filename for filename in train_filenames if \"\/39\/\" in filename])\nprint(\"category 29 images count in training set: \" + str(COUNT_39))\nCOUNT_40 = len([filename for filename in train_filenames if \"\/40\/\" in filename])\nprint(\"category 30 images count in training set: \" + str(COUNT_40))\nCOUNT_41 = len([filename for filename in train_filenames if \"\/41\/\" in filename])\nprint(\"category 31 images count in training set: \" + str(COUNT_41))\n\nweight_for_0 = (1 \/ COUNT_00)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_1 = (1 \/ COUNT_01)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_2 = (1 \/ COUNT_02)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_3 = (1 \/ COUNT_03)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_4 = (1 \/ COUNT_04)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_5 = (1 \/ COUNT_05)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_6 = (1 \/ COUNT_06)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_7 = (1 \/ COUNT_07)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_8 = (1 \/ COUNT_08)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_9 = (1 \/ COUNT_09)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_10 = (1 \/ COUNT_10)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_11 = (1 \/ COUNT_11)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_12 = (1 \/ COUNT_12)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_13 = (1 \/ COUNT_13)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_14 = (1 \/ COUNT_14)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_15 = (1 \/ COUNT_15)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_16 = (1 \/ COUNT_16)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_17 = (1 \/ COUNT_17)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_18 = (1 \/ COUNT_18)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_19 = (1 \/ COUNT_19)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_20 = (1 \/ COUNT_20)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_21 = (1 \/ COUNT_21)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_22 = (1 \/ COUNT_22)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_23 = (1 \/ COUNT_23)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_24 = (1 \/ COUNT_24)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_25 = (1 \/ COUNT_25)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_26 = (1 \/ COUNT_26)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_27 = (1 \/ COUNT_27)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_28 = (1 \/ COUNT_28)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_29 = (1 \/ COUNT_29)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_30 = (1 \/ COUNT_30)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_31 = (1 \/ COUNT_31)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_32 = (1 \/ COUNT_32)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_33 = (1 \/ COUNT_33)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_34 = (1 \/ COUNT_34)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_35 = (1 \/ COUNT_35)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_36 = (1 \/ COUNT_36)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_37 = (1 \/ COUNT_37)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_38 = (1 \/ COUNT_38)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_39 = (1 \/ COUNT_39)*(TRAIN_IMG_COUNT)\/42.0\nweight_for_40 = (1 \/ COUNT_40)*(TRAIN_IMG_COUNT)\/42.0 \nweight_for_41 = (1 \/ COUNT_41)*(TRAIN_IMG_COUNT)\/42.0\n\n\nclass_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2, 3: weight_for_3, 4: weight_for_4, 5: weight_for_5,\n                6: weight_for_6, 7: weight_for_7, 8: weight_for_8, 9: weight_for_9, 10: weight_for_10, 11: weight_for_11,\n                12: weight_for_12, 13: weight_for_13, 14: weight_for_14, 15: weight_for_15, 16: weight_for_16, \n                17: weight_for_17, 18: weight_for_18, 19: weight_for_19, 20: weight_for_20, 21: weight_for_21,\n                22: weight_for_22, 23: weight_for_23, 24: weight_for_24, 25: weight_for_25, 26: weight_for_26, \n                27: weight_for_27, 28: weight_for_28, 29: weight_for_29, 30: weight_for_30, 31: weight_for_31,\n                32: weight_for_32, 33: weight_for_33, 34: weight_for_34, 35: weight_for_35, 36: weight_for_36, \n                37: weight_for_37, 38: weight_for_38, 39: weight_for_39, 40: weight_for_40, 41: weight_for_41,\n               }","ae1e12d9":"class_weight","5bc78c29":"history = model1.fit(\n    train_ds,\n    steps_per_epoch=TRAIN_IMG_COUNT \/\/ BATCH_SIZE,\n    epochs=5,\n    validation_data=val_ds,\n    validation_steps=VAL_IMG_COUNT \/\/ BATCH_SIZE,\n    class_weight=class_weight,\n)","623262f5":"import seaborn as sns\n\nLR_START = 0.00000001\nLR_MIN = 0.000001\nLEARNING_RATE = 3e-5 * strategy.num_replicas_in_sync\nLR_MAX = LEARNING_RATE\nLR_RAMPUP_EPOCHS = 3\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\n\nsns.set(style=\"whitegrid\")\nfig, ax = plt.subplots(figsize=(20, 6))\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","41485ab9":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom tensorflow.keras import optimizers\n\nES_PATIENCE = 2\n\nfor layer in model1.layers:\n    layer.trainable = True # Unfreeze layers\n\n\nes = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, \n                   restore_best_weights=True, verbose=1)\nlr_callback = LearningRateScheduler(lrfn, verbose=1)\n\ncallback_list = [es, lr_callback]\n\noptimizer = optimizers.Adam(lr=3e-5 * strategy.num_replicas_in_sync)\nmodel1.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics='sparse_categorical_accuracy')\nmodel1.summary()","bc4bdd47":"history = model1.fit(\n    train_ds,\n    steps_per_epoch=TRAIN_IMG_COUNT \/\/ BATCH_SIZE,\n    epochs=14,\n    validation_data=val_ds,\n    validation_steps=VAL_IMG_COUNT \/\/ BATCH_SIZE,\n    callbacks = callback_list,\n    class_weight=class_weight,\n)","a5e63b85":"def plot_metrics(history, metric_list):\n    fig, axes = plt.subplots(1, 2, sharex='col', figsize=(24, 12))\n    axes = axes.flatten()\n    \n    for index, metric in enumerate(metric_list):\n        axes[index].plot(history.history[metric], label='Train %s' % metric)\n        axes[index].plot(history.history['val_%s' % metric], label='Validation %s' % metric)\n        axes[index].legend(loc='best', fontsize=16)\n        axes[index].set_title(metric)\n\n    plt.xlabel('Epochs', fontsize=16)\n    sns.despine()\n    plt.show()\n\nplot_metrics(history, metric_list=['sparse_categorical_accuracy','loss'])\n","c40afcbb":"# i wanted to get retain the model with train+val before making a predicition, but a TPU session can only be open for 3 hours :(\n\n# all_list_ds = tf.data.Dataset.from_tensor_slices(filenames)\n# all_ds = all_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n# all_ds = all_ds.batch(BATCH_SIZE)\n# optimizer = optimizers.Adam(lr=4e-5)\n# model1.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics='sparse_categorical_accuracy')\n# model1.summary()\n\n# history = model1.fit(\n#     all_ds,\n#     steps_per_epoch=TRAIN_IMG_COUNT \/\/ BATCH_SIZE,\n#     epochs=3,\n#     validation_data=val_ds,\n#     validation_steps=VAL_IMG_COUNT \/\/ BATCH_SIZE,\n#     callbacks = callback_list,\n#     class_weight=class_weight,\n# )","622000bb":"ignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False","6cc359d2":"files=pd.read_csv(\"..\/input\/shopee-product-detection-student\/test.csv\")[\"filename\"]\nfiles","a6f973d9":"probabilities = model1.predict(test_ds)","0ad6bae1":"predictions = np.argmax(probabilities, axis=-1)","d418bae2":"pred = pd.Series(predictions)\npred.to_csv('predEffnet.csv')","73390ada":"fnames = pd.Series(files)\ndf2=pd.concat([fnames, pred], axis=1)","8500e14f":"df2","6ea31af1":"df2.columns = ['filename', 'category']\ndf2","5bd10bc1":"# Thanks Tong Hui Kang for the function\ndf2[\"category\"] = df2[\"category\"].apply(lambda x: \"{:02}\".format(x))  # pad zeroes\ndf2.to_csv(\"submission.csv\", index=False)","f3ba1e24":"# if you have reached the end of the notebook,i really appreciate that you took the time out to\n# read my solution to this image classification challenge, i am currently a 3rd year cs major in ntu\n# for those who were not able to solve this challenge, i urge you to keep trying! For shopee challenge last year, \n# my rank was 274\/360, and i have progressed quite abit in only a year to do well in ai matters it is important \n# to keep up to date with the latest models and technology, the reason why i was able to do well is in part due to TPU usage\n# next year a new sota model and hardward will be realised if you are familiar enough to utilise it i am certain you'll do well too\n\n# cleaning this notebook up was truly a pain i would appreciate it if you can like the notebook\n# make it feel like someone actually read this,\n\n# Cheers and stay safe,\n# Hong Jun","190d1b5a":"## Sanity Check\n------------------------------\n- Image Shape, Channel(RGB)\n- corr label","48d5c759":"## Visualizing model performance\n- Ploting the model accuray and loss, ","602f22b9":"## Balancing biased Dataset\n- There an imbalance in the data classes, to address that i choose to given different weightage, instead of something like SMOTE\n- Weights = (1 \/ CLASS_COUNT)*(TRAIN_IMG_COUNT)\/CLASS_LEN","e1a5703c":"## Learning rate finder","6d22de38":"## Warmup Top Layers","5cee2274":"I overslept admission time if not this suibmission would have gotten  12 on the leaderboard\n\n![Screenshot%20from%202020-07-05%2010-15-38.png](attachment:Screenshot%20from%202020-07-05%2010-15-38.png)","65ed71fb":"## Dirty way of getting test\n- Count of 12186 is correct\n- The first time i ran my test, i shuffled my test bad idea!","f2f4bbab":"## Helper functions\n----------------------------\n- Fetching and processing of training data\n- get class label\n- simple data augmentation flip LR","db068dd5":"## Dependencies\n----------------------------","d98f3bd1":"## 1.Introduction\n---------------------\nFine grain classification of fashion image\n\n\nthis is pretty much a fork of the this starter notebook and i followed his code structure https:\/\/www.kaggle.com\/xhlulu\/flowers-tpu-concise-efficientnet-b7\n\n## 2.Motivation\n----------------------\nRecently i came across tensorflow2's API and TPU and i have seen some impressive speed ups (x100) when moving from GPU to TPU. \n\nMy motivation for the project is really to get a better feel of TF2 and the general workflow of TPU projects.\n\nBefore trying out the TPU, I was running a GCP EC2 instance with P4 GPU, and each EPOCH took 2hrs and now the fastest EPOCH was only 80 Seconds. \nA whopping 60X increase in speed\n\n\nIn order to reach ~75% Accuracy took a grand total of 70 Compute hours @ 40cents per hour, around 28 dollars worth of GCP credits for just training \n![rsz_hjtraining.png](attachment:rsz_hjtraining.png)\n\n\n## 3.Model\n----------------------\nTried state of the art Effcientnet models. Theses models are trained on imagenet and noisy-student - both variations of the weights are available in the Keras version. I prefer noisy-student pretrained.\n\n\nTo balance out the bias in the dataset, i choose not to oversample\/undersample, and choose class weights instead, which underweight the influence of overrepresented classes and overweight for underrepresented. Im not exactly, sure how does the class weightage works but i believe that it affects the optimiser.\n\n\nMy inital model used EfficientB7+mish+ranger but implementing the exact same model in TF2 was unrealistic in 1 day.\n\nThe learning rate schedular use was generic from the user [afshiin] (https:\/\/www.kaggle.com\/afshiin\/flower-classification-focal-loss-0-98)\n\n\n## 4.TPU Tips\n-----------------------\nWhen using TPU for model training, the bottle neck in most cases is fetching images from memory to the TPU. In order to combat the bottleneck there are a couple of tricks you can use:\n1. HUGE batch size (128bs)\n2. HIGH resolution (512,512) <- **Couldn't get this to work, if someone knows why I would love to know**\n3. Enable out of order processing, so process whichever image arrives at the TPU first.\n4. CACHE !!!\n(did not try)\n5. Mixed Precision\n6. XLA Accelerate\n\n\n## 5.Potential Areas for improvement\n-----------------------\n1. I spent a 2 days trying to get an ensemble to work..  but it kept shutting down, later realised that TPU access on google colab is limited to 3 hrs per session. \n   My intended submission was to submit an ensemble of effnet B7,B6,B5,B4. This can still be ideally achieved by saving down each model ensemble at a later time.\n   \n2. I have a hunch that because i did not use TFREC format when loading the images into the TPU, which resulted in an overall decrease in MXU usage and an increase in TPU idle time\n\n3. I did not do much of data augmentation, the only tranforms i tried was flip LR\n\n\n## 6.Closing Comments\n-----------------------\nThe Notebook\/model takes about 16 bucks to run (2hrs) on the TPU, Compared to 28 bucks (70hrs) on a GPU.     \n**Thank you Google for being generous  :)**\n\nEDA was done on a seperate notebook, if anyone is interested i can share them with you \n\n\nI would also love to find out how did the other better performing teams achieve their score! \n\nIn addition intend on doing a detailed write up and deep dive on TPU as well as new ways of doing data augmentation, \nif you are interested to discuss or talk about it hit me up on my **social media** [Hong Jun](https:\/\/www.linkedin.com\/in\/chewhongjun\/) \nand check out my write up [*Here*](https:\/\/www.linkedin.com\/posts\/chewhongjun_data-datascience-ai-activity-6685401910876471296-q2WD)\n\n","f574990b":"### Start model training\n- freeze bottom layer\n- start with a small lr to warm the model","1291cbf5":"## A Look into the images\n-------------------------------\n","c81d2c49":"## Data Processing\n----------------------------\n- file path from GCP\n- train-val-split 80\/20","3a16ea4a":"# Caching\n---------------------\n- I'm not exactly sure how it works, but you can see the speed up after caching of data in the training phase"}}