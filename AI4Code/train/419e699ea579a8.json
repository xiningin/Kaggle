{"cell_type":{"506e0c9c":"code","1f03fecd":"code","384917a8":"code","a8780b10":"code","52e84105":"code","ddfe8df6":"code","89ad297c":"code","ad50dc38":"code","cb2b41d3":"code","ef0de054":"markdown","c119c6fd":"markdown","8d418610":"markdown","6b9b87b7":"markdown","6a3cf7fd":"markdown","22ded5b8":"markdown","a0a6cec8":"markdown","25968282":"markdown"},"source":{"506e0c9c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","1f03fecd":"X_full = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col='Id')","384917a8":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Get numeric and categorical columns\nnum_cols = [cname for cname in X_full.columns if X_full[cname].dtype in ['int64','float64']]\ncat_cols = [cname for cname in X_full.columns if X_full[cname].dtype == 'object']\n\n# Nominal data exploration\nX_full.hist(bins=30, figsize=(25,25))\nplt.show()","a8780b10":"f = plt.figure(figsize=(25,25))\n\nfor i in range(len(num_cols)):\n    f.add_subplot(10, 5, i+1)\n    sns.regplot(X_full[num_cols].iloc[:,i], X_full['SalePrice'])\n    \nplt.tight_layout()\nplt.show()","52e84105":"f = plt.figure(figsize=(25,25))\n\nfor i in range(len(cat_cols)):\n    f.add_subplot(10, 5, i+1, ylim=[0,X_full.shape[0]])\n    sns.countplot(x=X_full[cat_cols].iloc[:,i], data=X_full)\n    \nplt.tight_layout()\nplt.show()","ddfe8df6":"# Remove outliers visible in scatter plots\nX_full.drop(X_full[(X_full['OverallQual'] > 9) & (X_full['SalePrice'] < 220000)].index, inplace=True)\nX_full.drop(X_full[(X_full['GrLivArea'] > 4000) & (X_full['SalePrice'] < 250000)].index, inplace=True)\nX_full.drop(X_full[(X_full['OverallCond'] < 3) & (X_full['SalePrice'] > 300000)].index, inplace=True)\nX_full.drop(X_full[(X_full['GarageArea'] > 1230)].index, inplace=True)\nX_full.drop(X_full[(X_full['TotalBsmtSF'] > 3100)].index, inplace=True)\n\n# Log the SalePrice\nX_full['SalePrice'] = np.log1p(X_full['SalePrice'])\n\n# Drop columns with the majority of data missing\ncols_to_drop = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','Street','Utilities']\n\n# Drop any columns that are not useful (assessed categorical columns)\nX_full.drop(axis=1, columns=cols_to_drop, inplace=True)\nX_test_full.drop(axis=1, columns=cols_to_drop, inplace=True)\n\n# Set the target value y to the SalePrice and drop it from the training data\ny = X_full['SalePrice']\nX_full.drop(axis=1, columns=['SalePrice'], inplace=True)\n\n# Get the numeric and categorical columns again since some of them have been dropped\nnum_cols = [cname for cname in X_full.columns if X_full[cname].dtype in ['int64','float64']]\ncat_cols = [cname for cname in X_full.columns if X_full[cname].dtype == 'object']\n\n# Make copies so the original isn't being manipulated\nX = X_full.copy()\nX_test = X_test_full.copy()","89ad297c":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nimport category_encoders as ce\n\n# Need to be outside function for cache\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\ntarget_encoder = ce.TargetEncoder(cols=cat_cols)\ntest_data = False\n\ndef general_cleaner(X, y=None):\n    \n    # Numerical columns\n    # Decide if numerical columns should be replaced with 0 or the median\n    num_cols_missing = [cname for cname in num_cols if X[cname].isnull().sum() > 0]\n\n    num_cols_zeros = []\n    num_cols_median = []\n\n    for column in num_cols_missing:\n        if X[column].min() == 0:\n            num_cols_zeros.append(column)\n        else:\n            num_cols_median.append(column)\n\n    # Categorical cols\n    # If NaN occurs a lot, it's quite likely it stands for a category (like NA = no basement) so fill it with 'none'\n    # otherwise it's probably genuine missing data so should be filled with the most frequent value.\n    # 30 was determined as a good threshold for determining by visually checking the data.\n    cat_cols_missing = [cname for cname in cat_cols if X[cname].isnull().sum() > 0]\n\n    cat_cols_freq = []\n    cat_cols_none = []\n\n    for column in cat_cols_missing:\n        if X[column].isnull().sum() < 30:\n            cat_cols_freq.append(column)\n        else:\n            cat_cols_none.append(column)\n\n    # Apply zero numeric imputer\n    for cname in num_cols_zeros:\n        X[cname] = SimpleImputer(strategy='constant', fill_value=0.0).fit_transform(X[[cname]])\n\n    # Apply median numeric imputer\n    for cname in num_cols_median:\n        X[cname] = SimpleImputer(strategy='median').fit_transform(X[[cname]])\n\n    # Apply highest frequency categorical imputer\n    for cname in cat_cols_freq:\n        X[cname] = SimpleImputer(strategy='most_frequent').fit_transform(X[[cname]])\n\n    # Apply 'none' categorical imputer\n    for cname in cat_cols_none:\n        X[cname] = SimpleImputer(strategy='constant', fill_value='none').fit_transform(X[[cname]])\n\n    # Scale all numerical values\n    for cname in num_cols:\n        X[cname] = StandardScaler().fit_transform(X[[cname]])\n\n    # Apply count encoding\n    count_encoded = ce.CountEncoder().fit_transform(X[cat_cols])\n    X = X.join(count_encoded.add_suffix(\"_count\"))\n    \n    # Apply target encoding (fit on training data, and apply to test data)\n    if not test_data:\n        target_encoder.fit(X[cat_cols], y)\n        X = X.join(target_encoder.transform(X[cat_cols]).add_suffix('_target'))\n    else:\n        X = X.join(target_encoder.transform(X[cat_cols]).add_suffix('_target'))\n\n    # Apply one hot encoding (fit on training data, and apply to test data)\n    if not test_data:\n        OH_X = pd.DataFrame(OH_encoder.fit_transform(X[cat_cols].astype('str')))\n    else:\n        OH_X = pd.DataFrame(OH_encoder.transform(X[cat_cols].astype('str')))\n\n    # Put the correct indexes back in since OH encoding reset the index\n    OH_X.index = X.index\n\n    # Drop the columns that have now been OH encoded\n    X = X.drop(cat_cols, axis=1)\n\n    # Add the new OH columns back in\n    X = pd.concat([X, OH_X], axis=1)\n\n    return X","ad50dc38":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = XGBRegressor()\n\n# Set of parameters for XGBRegressor to try randomly\nparams = {\n    'n_estimators' : np.arange(100,2000,100),\n    'learning_rate' : np.arange(0.01,0.2,0.01),\n    'max_depth' : np.arange(1,4,1),\n}\n\n# Define a cross validator, RandomizedSearchCV to iterate over the best hyper parameters\nparam_search = RandomizedSearchCV(model, param_distributions=params,\n                                 n_iter=1,\n                                 cv=5,\n                                 scoring='neg_mean_squared_error',\n                                 verbose=False,\n                                 random_state=1)\n\n# Clean and pre-process the training data\nX_preprocessed = general_cleaner(X, y)\n\n# Find the best model parameters for the data data using cross validation\nparam_search.fit(X_preprocessed, y)\n\n# See the best results in a list\nprint(\"\\n\\nRandom search:\")\ncv_results = param_search.cv_results_\nfor mean_score, params in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n    print(-1*mean_score, params)\n\nprint(-1*param_search.best_score_,param_search.best_params_)\n\n# 0.013460942446364737 {'learning_rate': 0.052, 'max_depth': 2, 'n_estimators': 943}\n\n# Now that we have a first pass at the best parameters, use GridSearchCV to do a more detailed check of the best hyper parameters\nparams = {\n    'n_estimators' : np.arange(942,944,1),\n    'learning_rate' : np.arange(0.051,0.053,0.001),\n    'max_depth' : [2],\n}\n\nparam_search = GridSearchCV(model, param_grid=params,\n                                 cv=5,\n                                 scoring='neg_mean_squared_error',\n                                 verbose=False)\n\n# Find the best model parameters for the data data using cross validation\nparam_search.fit(X_preprocessed, y)\n\n# See the best results in a list\nprint(\"\\n\\nGrid search:\")\ncv_results = param_search.cv_results_\nfor mean_score, params in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n    print(-1*mean_score, params)\n    \nprint(-1*param_search.best_score_,param_search.best_params_)\n\n# Re-fit the model using the whole data set\nparam_search.refit\n\n# Flag to switch between train and test data (used in the general_cleaner function)\ntest_data = True\n\n# Clean and pre-process the test data\nX_test_preprocessed = general_cleaner(X_test)\n\n# Predict house prices using the test data\npreds = param_search.predict(X_test_preprocessed)\n\n# Inverse log predictions\npreds = np.expm1(preds)","cb2b41d3":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds})\noutput.to_csv('submission.csv', index=False)","ef0de054":"# House Pricing Prediction\n**Objectives:**\n* Put topics learnt in the Kaggle Intermediate Machine Learning course into practice \n* Create a clean template for tackling future linear regression problems\n* Learn how to use pipelines (failed)\n* Learn how to cross validate data (succeeded)\n* Tune hyper parameters (succeeded)\n\nThis prediction scored top 3%","c119c6fd":"# Count plots of categorical features \nThis is used to visualise the distribution and missing values in the categorical data.","8d418610":"# Impute and preprocess numeric and categorical data\nI spent 2 days trying to do this with a pipeline and customer estimators but couldn't figure it out. I needed to segregate columns for median and zero imputation, which varied between train and test sets. To then have access to those columns in the ColumnTransformer to determine which columns to impute was extremely difficult with a pipeline. I think using a general_cleaner function serves the same purpose as the pipeline and feels more flexible. Let me know what you think!","6b9b87b7":"# Histograms of numeric data","6a3cf7fd":"# Model","22ded5b8":"# Select target variable and drop outliers\nFrom the histograms above it's clear that SalePrice is skewed. To improve fitting XGBoost the Sale Price log is taken, outliers and columns with no obvious correllation and very little data are dropped.","a0a6cec8":"# Submission","25968282":"# Scatter plots of numeric features to show correlation with Sale Price\nThis is used to show correlation with Sale Price and determine outliers or useless features"}}