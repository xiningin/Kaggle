{"cell_type":{"01cdb318":"code","7484da5a":"code","89e71416":"code","12ccc84a":"code","e70903a4":"code","0a7bd6be":"code","4db6ed65":"code","3f1de3a2":"code","029f02db":"code","cb45cfbe":"code","720358ec":"markdown","c7a15c88":"markdown","ca086d49":"markdown","e30fcf3b":"markdown","6e52dc83":"markdown","ce02669c":"markdown","4ffd3915":"markdown","74ba5a4d":"markdown","0855ea86":"markdown","ed6d7d7c":"markdown"},"source":{"01cdb318":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n\n# Metrics\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\n# Model Selection\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import RidgeCV, Lasso, BayesianRidge, Ridge, ElasticNet","7484da5a":"train = pd.read_csv('..\/input\/comprehensive-exploration-cleaning\/train_.csv')\nholdout = pd.read_csv('..\/input\/comprehensive-exploration-cleaning\/test_.csv')","89e71416":"train_Id = train['Id'] ; holdout_Id = holdout['Id']\ntrain_data = train.drop('Id', axis=1) ; holdout_data = holdout.drop('Id', axis=1)","12ccc84a":"X = train_data.drop('SalePrice', axis=1)\ny = train_data['SalePrice']\nlog_y = np.log(y)\n\nX_train,  X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=101)","e70903a4":"# Step 1: Fit Lasso\nlasso = Lasso().fit(X_train, y_train)\n# Step 2: Build parameters dictionary\nparameters = {'alpha':np.logspace(-10, 6, 20)}\n\n\n# Fit Lasso Model with Gridsearch\n#====================================================================\n# Step 1: Run Gridsearch\nclf = GridSearchCV(lasso, parameters, cv=5, verbose=0)\n# Step 2: Fit best model from Gridsearch \nbest_model = clf.fit(X_train, y_train)\n# Step 3: Predict\ny_pred = clf.predict(X_test)\n\n\n# Step 4: Calculate metrics\nrsquare_lasso = round(r2_score(y_test, y_pred),4)\nrmse_lasso = round(np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(y_pred))),4)\nprint('Lasso - RMSE : ', rmse_lasso ,'\\nLasso - rsquare : ',rsquare_lasso)\n\n# Lasso + RobustScaler Model with Gridsearch\n#====================================================================\n# Step 1: Run Gridsearch\nclf = make_pipeline(RobustScaler(),GridSearchCV(lasso, parameters, cv=5, verbose=0))\n# Step 2: Fit best model from Gridsearch \nbest_model = clf.fit(X_train, y_train)\n# Step 3: Predict\ny_pred = clf.predict(X_test)\n\n\n# Step 4: Calculate metrics\nrsquare_lasso_RobustScaler = round(r2_score(y_test, y_pred),4)\nrmse_lasso_RobustScaler = round(np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(y_pred))),4)\nprint('Lasso + RobustScaler - RMSE : ', rmse_lasso_RobustScaler ,'\\nLasso + RobustScaler - rsquare : ',rsquare_lasso_RobustScaler)","0a7bd6be":"n_samples, n_features = X_train.shape[0], X_train.shape[1]\n\n# Step 1: Fit Lasso\nridge = KernelRidge().fit(X_train, y_train)\n# Step 2: Build parameters dictionary\nparameters = {'alpha':np.logspace(-10, 6, 20)}\n\n\n# Kernel Ridge Regression Model with Gridsearch\n#====================================================================\n# Step 1: Run Gridsearch\nclf = GridSearchCV(ridge, parameters, cv=5, verbose=0)\n# Step 2: Fit Lasso again with the best parameters from Gridsearch \nbest_model = clf.fit(X_train, y_train)\n# Step 3: Predict\ny_pred = clf.predict(X_test)\n\n# Step 4: Calculate metrics\nrsquare_ridge = round(r2_score(y_test, y_pred),4)\nrmse_ridge = round(np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(y_pred))),4)\nprint('Kernel Ridge Regression - RMSE : ', rmse_ridge,'\\nKernel Ridge Regression - rsquare : ',rsquare_ridge)","4db6ed65":"# Step 1: Fit Lasso\neNet = ElasticNet().fit(X_train, y_train)\n# Step 2: Build parameters dictionary\nparameters = {\"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n\n\n# elastic-net Regression Model with Gridsearch\n#====================================================================\n# Step 1: Run Gridsearch\nclf = GridSearchCV(eNet, parameters, cv=5, verbose=0)\n# Step 2: Fit Lasso again with the best parameters from Gridsearch \nbest_model = clf.fit(X_train, y_train)\n# Step 3: Predict\ny_pred = clf.predict(X_test)\n\n# Step 4: Calculate metrics\nrsquare_enet = round(r2_score(y_test, y_pred),4)\nrmse_enet = round(np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(y_pred))),4)\nprint('elastic-net - RMSE : ', rmse_enet ,'\\nelastic-net - rsquare :',rsquare_enet)","3f1de3a2":"'''\n\n# Step 1. Scale the features and convert data into tensors\nscaler = MinMaxScaler()\nscaler.fit(X_train)\n\nX_train = pd.DataFrame(data = scaler.transform(X_train),columns = X_train.columns, index = X_train.index)\nX_test = pd.DataFrame(data = scaler.transform(X_test),  columns = X_test.columns, index = X_test.index)\n\n\n# Step 2. Convert data to tensors\ndef make_features_cols():\n    input_cols = [tf.feature_column.numeric_column(k) for k in X_train.columns]\n    return input_cols\nfeature_columns = make_features_cols()  \n\n\n# Step 3. Build input function, model and train the model\n#-------------------------------------------------------------------------\n# Build input function\ninput_func = tf.estimator.inputs.pandas_input_fn(x = X_train, y=y_train,batch_size=10,num_epochs = 1000,shuffle = True)\n# Build model\nmodel = tf.estimator.DNNRegressor(hidden_units=[100,100,100], feature_columns=feature_columns)\n# Train the model\nmodel.train(input_fn=input_func, steps = 10000)\n\n\n# Step 4: Predict\npredict_input_func = tf.estimator.inputs.pandas_input_fn(x = X_test, batch_size=10, num_epochs=1,shuffle = False)\npred_gen = model.predict(predict_input_func)\npredictions = list(pred_gen)\n\ny_pred = []\nfor pred in predictions:\n    y_pred.append(pred['predictions'])\n\n\n\n# Step 5: Calculate metrics\nrsquare_DNN_Regressor = round(r2_score(y_test, y_pred),4)\nrmse_DNN_Regressor = round(np.sqrt(metrics.mean_squared_error(log(y_test), log(y_pred))),4)\nprint('DNN Regressor - RMSE : ', rmse_DNN_Regressor ,'\\nDNN Regressor - rsquare : ',rsquare_DNN_Regressor)\n'''\n\n","029f02db":"print('Lasso            RMSE : ', rmse_lasso ,'  rsquare : ',rsquare_lasso)\nprint('Lasso+RS         RMSE : ', rmse_lasso_RobustScaler , '  rsquare : ',rsquare_lasso_RobustScaler)\nprint('Kernel Ridge     RMSE : ', rmse_ridge,'   rsquare : ',rsquare_ridge)\nprint('Enet             RMSE : ', rmse_enet ,'  rsquare : ',rsquare_enet)\n#print('DNN-Regressor    RMSE : ', rmse_DNN_Regressor ,'  rsquare : ',rsquare_DNN_Regressor)\n","cb45cfbe":"X_holdout = pd.DataFrame(data = holdout.drop('Id', axis=1),columns = X_train.columns, index = holdout.index)\n\n\n# Step 1: Fit ElasticNet\neNet = ElasticNet().fit(X_train, y_train)\n# Step 2: Build parameters dictionary\nparameters = {\"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n\n\n# elastic-net Regression Model with Gridsearch\n#====================================================================\n\n# Step 1: Run Gridsearch\nclf = GridSearchCV(eNet, parameters, cv=5, verbose=0)\n# Step 2: Fit Lasso again with the best parameters from Gridsearch \nbest_model = clf.fit(X_train, y_train)\n# Step 3: Predict\ny_pred_holdout = clf.predict(X_holdout)\n\n\nsubmission = pd.DataFrame({\"Id\": holdout[\"Id\"],\"SalePrice\": y_pred_holdout})\nsubmission.loc[submission['SalePrice'] <= 0, 'SalePrice'] = 0\nfileName = \"submission.csv\"\nsubmission.to_csv(fileName, index=False)\n\n\n","720358ec":"# File Submission","c7a15c88":"## 2.1 Regularisation Techniques","ca086d49":"# Compare accuracies of different models","e30fcf3b":"## ** [Lasso Regression : ](https:\/\/www.statisticshowto.datasciencecentral.com\/lasso-regression\/) **\nPerforms L1 regularisation, where it penalises the less significant features by adding a penalty equal to the absolute value of the magnitude of coefficients. The result of Lasso regression is sparse model with fewer coeficients as some of the coeficients can become zero in the process. A tuning parameter, \u03bb controls the strength of the L1 penalty. When \u03bb = 0, no parameters are eliminated. As \u03bb increases, bias increases, as it decreases, variance increases.","6e52dc83":"## ** [Kernel Ridge Regression](https:\/\/ncss-wpengine.netdna-ssl.com\/wp-content\/themes\/ncss\/pdf\/Procedures\/NCSS\/Ridge_Regression.pdf)**\nRidge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.","ce02669c":"# Data Modelling\n\n**Characteristics of the data : **\n\n* **Multicolinearity** : Since we are delaing with highly correlated variables, the standard OLS parameter estimates will have large variance. To counter this, you can use regularization - a technique allowing to decrease this variance at the cost of introducing some bias.\n\n\n* ** Highly skewed **\n\n* ** Outliers (high and low)**","4ffd3915":"Load datasets","74ba5a4d":"## DNN Regressor - Tensorflow","0855ea86":"## ** [ElasticNet Regression](https:\/\/www.datacamp.com\/community\/tutorials\/tutorial-ridge-lasso-elastic-net)**\nElastic Net first emerged as a result of critique on lasso, whose variable selection can be too dependent on data and thus unstable. It is a convex combination of Ridge and Lasso. The solution is to combine the penalties of ridge regression and lasso to get the best of both worlds. The tuning parameter \u03b1 is the mixing parameter between ridge (\u03b1\u2004=\u20040) and lasso (\u03b1\u2004=\u20041)","ed6d7d7c":"### Split dataset to training and testing"}}