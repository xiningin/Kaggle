{"cell_type":{"571f24ee":"code","71d1a30a":"code","e5a81960":"code","9f60492b":"code","1f1335e6":"code","ead645ff":"code","7b234264":"code","ec1afa0a":"code","07dc5b74":"code","90826655":"code","850a3c10":"code","557ebb5a":"code","1fbe7d92":"code","61610619":"code","221dc637":"code","d4b7c057":"code","057b3c83":"code","a7779f5f":"code","d24a9954":"code","b649e0a6":"code","93434562":"code","d73aeda0":"code","f1c0ee98":"code","c2262c2c":"code","233e3b40":"code","81b9f23c":"code","8d62707e":"code","a32d47c5":"markdown","0edd8946":"markdown","1bbd1def":"markdown","d0029b21":"markdown","dd0257de":"markdown","95e2d82a":"markdown","428669d2":"markdown","a27f8590":"markdown","7ecc4ef5":"markdown","48da41c6":"markdown"},"source":{"571f24ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71d1a30a":"# library imports\nimport numpy as np \nimport pandas as pd \nfrom sklearn.naive_bayes import GaussianNB\nimport matplotlib.pyplot as plt\n\n","e5a81960":"# reading data\ndf = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\")\ndf = df[[\"v1\", \"v2\"]]","9f60492b":"df","1f1335e6":"df.info()","ead645ff":"# checking unique values on v1\nprint(\"Number of Unique Values: \\n\", df['v1'].value_counts(),'\\n\\n')\n\n# value percentage\nprint(\"Value Percentage: \\n\",df['v1'].value_counts() * 100 \/ len(df['v1']))","7b234264":"import nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","ec1afa0a":"ps = PorterStemmer()","07dc5b74":"stop_words = stopwords.words(\"english\")\nprint(len(stop_words))\nprint(stop_words[:10])","90826655":"# # 1. Tokenization\n# df['tokens'] = df.v2.str.split(\" \")\n# df.head()","850a3c10":"def remove_stopwords(text):\n    \"\"\"Function to replace stopords with an empty space and removes double spaces\n    removing all chars other then alphabet and stemming words\n    \n    returns: corpus of stemmed words\"\"\"\n\n    text = re.sub('^a-zA-Z',' ',text)\n    text = text.split()\n    text = [ps.stem(word) for word in text if word not in stop_words]\n    text = ' '.join(text).replace('  ', ' ')\n    return text\n","557ebb5a":"df[\"removed_stopwords\"]= df.v2.apply(remove_stopwords)","1fbe7d92":"df","61610619":"# vecotrizing\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features= 2500)\n","221dc637":"X = cv.fit_transform(df.removed_stopwords).toarray()\nprint(X.shape)","d4b7c057":"# target\ny = pd.get_dummies(df.v1, drop_first=True)\nprint(y.shape)","057b3c83":"y = np.array(y)\nprint(y.shape)","a7779f5f":"print(y)","d24a9954":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import average_precision_score, precision_recall_curve, plot_precision_recall_curve","b649e0a6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, shuffle=False)","93434562":"X_train.shape","d73aeda0":"gauss_clf = GaussianNB()\nmulti_clf = MultinomialNB()","f1c0ee98":"gauss_clf.fit(X_train, y_train)\nmulti_clf.fit(X_train, y_train)","c2262c2c":"gauss_pred = gauss_clf.predict(X_test)\nmulti_pred = multi_clf.predict(X_test)","233e3b40":"score_gauss = accuracy_score(gauss_pred, y_test)\nscore_multi = accuracy_score(multi_pred, y_test)\nprint(f\"GaussNB Accuracy Score: {score_gauss*100:.2f}%\")\nprint(f\"MultinomialNB Accuracy Score: {score_multi*100:.2f}%\")","81b9f23c":"average_precision_gauss = average_precision_score(y_test, gauss_pred)\naverage_precision_multi = average_precision_score(y_test, multi_pred)\n\nprint(f\"Avg Precision for GaussianNB: {average_precision_gauss:.2f}\")\nprint(f\"Avg Precision for MultinomialNB: {average_precision_multi:.2f}\")","8d62707e":"# plotting the MultinomialNB Precision\nfig, ax = plt.subplots(figsize=(12,8))\ncurve = plot_precision_recall_curve(multi_clf, X_test, y_test, ax=ax)\ncurve.ax_.set_title(\"MultinomialNB Precision\/Recall Curve\");\n","a32d47c5":"<p style=\"font-size:16px\">In this Notebook, We go over some of the Naive Bayes basic definitions and formulas. The workflow of the notbook is explained. Following preprocessing steps and some evaluation metrics. Two Naive Bayes Classifiers were used. The MultinomialNB classifier had the better performance.   <\/p>\n\n### Achieved Accuracy: 98.8% \n","0edd8946":"## Formula:\n$$P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$$\n\n## Example: Spam filters\n## $P(Spam | email) = P(Spam | \\vec{w}) = \\frac{P(spam) \\cdot P(\\vec{w} | Spam)}{P(spam)\\:\\cdot \\: P(\\vec{w}|Spam)\\:\\cdot \\: P(not \\:Spam)\\: \\cdot \\: P(\\vec{w} | not \\:spam)}$\n\n### Terminolgy\nPriors --> P(spam)  \nLikelihood --> P(spam | $\\vec{w}$) and other | terms \nEvidence --> The lower term of the formula $P(spam)\\:\\cdot \\: P(\\vec{w}|Spam)\\:\\cdot \\: P(not \\:Spam)\\: \\cdot \\: P(\\vec{w} | not \\:spam)$","1bbd1def":"## Exploratory Data Analysis","d0029b21":"# Supervised Learning - Naive Bayes","dd0257de":"## Preprocessing (aka Featurising)\n<ol style=\"font-size:16px\">\n    <li>Tokenization: splitting the words <\/li>\n    <li>Stop words removal<\/li>\n    <li>Remove non-alphabetical charachters.<\/li>\n    <li>Stemming: keeping the root of the word but stripping things like ing, ed etc. More for large data<\/li>\n    <li>Lemmatization: Alternative to stemming by assinging to the same root. more taxing of resources than stemming<\/li>\n    <li>Lowercasing: could be bad in cases where the name turns into a verb like \"Mark\" and \"mark\"<\/li>\n\n<\/ol>","95e2d82a":"## Modeling","428669d2":"## Preprocessing\n\n<ol style=\"font-size:16px\">\n    <li>Tokenization: splitting the words <\/li>\n    <li>Stop words removal<\/li>\n    <li>Remove non-alphabetical charachters.<\/li>\n    <li>Stemming: keeping the root of the word but stripping things like ing, ed etc. More for large data<\/li>\n    <li>Lemmatization: Alternative to stemming by assinging to the same root. more taxing of resources than stemming<\/li>\n    <li>Lowercasing: could be bad in cases where the name turns into a verb like \"Mark\" and \"mark\"<\/li>\n\n<\/ol>","a27f8590":"## Building Naive Bayes Classifer","7ecc4ef5":"## Evaluation ","48da41c6":"## There are 2 phases in the Classifer workflow:\n\n<ol style=\"font-size:16px\">\n    <li>Learning phase: splitting the data into training and testing data<\/li>\n    <li>Evaluation phase: testing the classifier performance using key metrics.\n        Given that TP\/FP is True\/False Positive, TN\/FN True\/False Negative:\n        <ol> \n            <li>Accuracy: ratio of correctly predicted observation to the total observations\n            $$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$$<\/li>\n            <li>Precision: is the ratio of correctly predicted positive observations to the total predicted positive observations:\n            $$Precision = \\frac{TP}{TP + FP}$$<\/li>\n            <li>Recall or Sensitivity: the ratio of correctly predicted positive observations to the all observations in actual class \n            $$Recall = \\frac{TP}{TP+FN}$$<\/li>\n        <\/ol>\n    <\/li>\n<\/ol>"}}