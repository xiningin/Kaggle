{"cell_type":{"aa2a9425":"code","45b6f361":"code","28ab217f":"code","22cd8568":"code","4d228a3d":"code","59efcea2":"code","ac41fd9f":"code","c585d186":"code","15591930":"code","8d0e25d2":"code","545694ef":"code","4e405041":"code","81d7d59b":"code","bc5e7c8e":"code","bb06c421":"code","76be84c6":"code","e5258004":"code","f4d200a9":"markdown","7181fb1e":"markdown","bb5bc791":"markdown"},"source":{"aa2a9425":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","45b6f361":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"comments\" table\ntable_ref = dataset_ref.table(\"comments\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"comments\" table\nclient.list_rows(table, max_results=5).to_dataframe()","28ab217f":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"comments\" table\ntable_ref = dataset_ref.table(\"comments\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"comments\" table\nclient.list_rows(table, max_results=5).to_dataframe()","22cd8568":"# Query to select comments tha received more than 10 replis\nquery_popular = \"\"\"\n                SELECT parent, COUNT(id)\n                FROM `bigquery-public-data.hacker_news.comments`\n                GROUP By parent\n                HAVING COUNT(id) > 10\n                \"\"\"","4d228a3d":"# Query to select comments that received more than 10 replies\nquery_popular = \"\"\"\n                SELECT parent, COUNT(id)\n                FROM `bigquery-public-data.hacker_news.comments`\n                GROUP BY parent\n                HAVING COUNT(id) > 10\n                \"\"\"","59efcea2":"import pandas as pd","ac41fd9f":"# Set up query(cancel the query if it would use too much of, your quota, with the limit set to 10GB)\n\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_popular, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\n\n#popular_comments = query_job.to_dataframe()\n\n#popular_comments = query_job.to_dataframe()\n\npopular_comments = query_job.to_dataframe()\n\n#Print the first five rows of the DataFrame\npopular_comments.head()","c585d186":"# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_popular, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\npopular_comments = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\npopular_comments.head()","15591930":"# Improve version of earlier query, now with aliasing & improved readability\n\nquery_improved = \"\"\"\n                 SELECT parent, COUNT(1) AS NumPosts\n                 FROM `bigquery-public-data.hacker_news.comments`\n                 GROUP BY parent\n                 HAVING COUNT(1) > 10\n                \"\"\"","8d0e25d2":"safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_improved, job_config=safe_config)\n\n# API request - run the query and convert the results to a pandas Dataframe\n\nimproved_df = query_job.to_dataframe()\n\n# Print the First five rows of the Dataframe\nimproved_df.head()","545694ef":"query_good = \"\"\"\n             SELECT parent, COUNT(id)\n             FROM `bigquery-public-data.hacker_news.comments`\n             GROUP BY parent\n             \"\"\"","4e405041":"safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_good, job_config=safe_config)\n\n# API request - run the query and convert the results to a pandas Dataframe\n\ngood_df = query_job.to_dataframe()\n\n# Print the First five rows of the Dataframe\ngood_df.head()","81d7d59b":"good_df","bc5e7c8e":"query_bad = \"\"\"\n             SELECT  parent, COUNT(id)\n             FROM `bigquery-public-data.hacker_news.comments`\n             GROUP BY parent\n             \"\"\"","bb06c421":"safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_bad, job_config=safe_config)\n\n# API request - run the query and convert the results to a pandas Dataframe\n\nbad_df = query_job.to_dataframe()\n\n# Print the First five rows of the Dataframe\nbad_df.head()","76be84c6":"bad_df.describe().T","e5258004":"good_df.describe().T","f4d200a9":"---","7181fb1e":"---","bb5bc791":"# Note on using GROUP BY"}}