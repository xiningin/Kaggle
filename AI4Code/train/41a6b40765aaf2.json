{"cell_type":{"3595c458":"code","407aa807":"code","ec4d88dc":"code","e49dabd0":"code","2dca833e":"code","f9083578":"code","ec07cd41":"code","b0251aa9":"code","11fa808c":"code","586090f3":"code","487bd82d":"code","41e40ec0":"code","5b03186a":"code","baeea7bf":"code","4dfb3b34":"code","f8e0f4d7":"code","4f57dd71":"markdown","4510c06c":"markdown","cb58d75e":"markdown","cba17a66":"markdown","c3832fcb":"markdown","e20e3bec":"markdown","5c198d68":"markdown","d40652d4":"markdown","583f8856":"markdown","7efee9ca":"markdown","8adc9e8c":"markdown"},"source":{"3595c458":"# Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Importing the models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score","407aa807":"# Importing datasets\ntrain_X = pd.read_csv('..\/input\/mlcourse-dota2-win-prediction\/train_features.csv', index_col = 0)\ntrain_y = pd.read_csv('..\/input\/mlcourse-dota2-win-prediction\/train_targets.csv', index_col = 0)\ntest_X = pd.read_csv('..\/input\/mlcourse-dota2-win-prediction\/test_features.csv', index_col = 0)\ntraindf = pd.merge(train_X, train_y, left_index = True, right_index = True)\n\ntraindf.head()","ec4d88dc":"traindf.dtypes","e49dabd0":"traindf.select_dtypes(exclude='number')","2dca833e":"# Convert radiant_win into numerical data\ntraindf['radiant_win'] = (traindf['radiant_win'])*1 \ntraindf['radiant_win'].dtypes\n\n# Convert next_roshan_team into numerical data\ntraindf['next_roshan_team_radiant'] = (traindf['next_roshan_team'] == 'Radiant')*1\ntraindf['next_roshan_team_dire'] = (traindf['next_roshan_team'] == 'Dire')*1\ntraindf = traindf.drop('next_roshan_team', axis=1)\n\n# Checking Dtype again\ntempdf = traindf.filter(['radiant_win','next_roshan_team_radiant','next_roshan_team_dire'],axis=1)\ntempdf.dtypes","f9083578":"# Creating Radiant Team columns\ntraindf['radiant_gold'] = traindf['r1_gold'] + traindf['r2_gold'] + traindf['r3_gold'] + traindf['r4_gold'] + traindf['r5_gold']\ntraindf['radiant_xp'] = traindf['r1_xp'] + traindf['r2_xp'] + traindf['r3_xp'] + traindf['r4_xp'] + traindf['r5_xp']\ntraindf['radiant_gold_xp_ratio'] = traindf['radiant_gold']\/traindf['radiant_xp']\ntraindf['radiant_vision'] = traindf['r1_obs_placed'] + traindf['r2_obs_placed'] + traindf['r3_obs_placed'] + traindf['r4_obs_placed'] + traindf['r5_obs_placed'] + traindf['r1_sen_placed'] + traindf['r2_sen_placed'] + traindf['r3_sen_placed'] + traindf['r4_sen_placed'] + traindf['r5_sen_placed']\ntraindf['radiant_sen'] = traindf['r1_sen_placed'] + traindf['r2_sen_placed'] + traindf['r3_sen_placed'] + traindf['r4_sen_placed'] + traindf['r5_sen_placed']\ntraindf['radiant_towers_killed'] = traindf['r1_towers_killed'] + traindf['r2_towers_killed'] + traindf['r3_towers_killed'] + traindf['r4_towers_killed'] + traindf['r5_towers_killed']\ntraindf['radiant_stun'] = traindf['r1_stuns'] + traindf['r2_stuns'] + traindf['r3_stuns'] + traindf['r4_stuns'] + traindf['r5_stuns']","ec07cd41":"# Creating Dire Team columns\ntraindf['dire_gold'] = traindf['d1_gold'] + traindf['d2_gold'] + traindf['d3_gold'] + traindf['d4_gold'] + traindf['d5_gold']\ntraindf['dire_xp'] = traindf['d1_xp'] + traindf['d2_xp'] + traindf['d3_xp'] + traindf['d4_xp'] + traindf['d5_xp']\ntraindf['dire_gold_xp_ratio'] = traindf['dire_gold']\/traindf['dire_xp']\ntraindf['dire_vision'] = traindf['d1_obs_placed'] + traindf['d2_obs_placed'] + traindf['d3_obs_placed'] + traindf['d4_obs_placed'] + traindf['d5_obs_placed'] + traindf['d1_sen_placed'] + traindf['d2_sen_placed'] + traindf['d3_sen_placed'] + traindf['d4_sen_placed'] + traindf['d5_sen_placed']\ntraindf['dire_sen'] = traindf['d1_sen_placed'] + traindf['d2_sen_placed'] + traindf['d3_sen_placed'] + traindf['d4_sen_placed'] + traindf['d5_sen_placed']\ntraindf['dire_towers_killed'] = traindf['d1_towers_killed'] + traindf['d2_towers_killed'] + traindf['d3_towers_killed'] + traindf['d4_towers_killed'] + traindf['d5_towers_killed']\ntraindf['dire_stun'] = traindf['d1_stuns'] + traindf['d2_stuns'] + traindf['d3_stuns'] + traindf['d4_stuns'] + traindf['d5_stuns']\n","b0251aa9":"# Creating Team Variance Columns (From Radiance POV)\ntraindf['var_gold'] = traindf['radiant_gold'] - traindf['dire_gold']\ntraindf['var_xp'] = traindf['radiant_xp'] - traindf['dire_xp']\ntraindf['var_gold_xp_ratio'] = traindf['radiant_gold_xp_ratio']\/traindf['dire_gold_xp_ratio']\ntraindf['var_vision'] = traindf['radiant_vision'] - traindf['dire_vision']\ntraindf['var_sen'] = traindf['radiant_sen'] - traindf['dire_sen']\ntraindf['var_towers_killed'] = traindf['radiant_towers_killed'] - traindf['dire_towers_killed']\ntraindf = traindf.replace([np.inf, -np.inf], np.nan).fillna(0)\n","11fa808c":"# Print top 10 correlated features to 'radiant_win'\ncor = traindf.corr()\ncor_target = abs(cor['radiant_win'])\ncor_target = cor_target.sort_values(ascending=False)\nprint(cor_target.head(10))\n\n# Plot top 10 correlated features into heatmap\nselected_col = cor_target.head(10).index \ntempdf = traindf[selected_col]\nf, ax = plt.subplots(figsize=(12, 9))\ncor2 = tempdf.corr()\nsns.heatmap(cor2, vmax=.8, annot=True, square=True, cmap=\"YlGnBu\");\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5 )\nplt.show()","586090f3":"# Keeping only relevant columns\ncols_to_keep = ['var_xp','var_gold','var_towers_killed','game_time_x']\ntraindf_X = traindf.filter(cols_to_keep,axis=1)\ntraindf_y = traindf[['radiant_win']]\n\ntraindf_X.head()","487bd82d":"traindf_y.head()","41e40ec0":"# Comparing scatterplots among key features\nsns.set()\nsns.pairplot(traindf[cols_to_keep], height = 2.5)\nplt.show();","5b03186a":"# Dropping outliers where game_time_x == 0\ntraindf = traindf_X\ntraindf['radiant_win'] = traindf_y\ntraindf = traindf[traindf.game_time_x > 0]\ntraindf_y = traindf[['radiant_win']]\ntraindf_X = traindf.drop(['radiant_win'],axis=1)","baeea7bf":"# Modelling with Logistics Regression\nmodel = LogisticRegression(solver='lbfgs')\n\n# calcuate ROC-AUC for each split\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=8)\ncv_score_mean = (cross_val_score(model, traindf_X, traindf_y.values.ravel(), cv=cv, scoring='roc_auc')).mean()\nprint(cv_score_mean)","4dfb3b34":"# prep test_X data\ntest_X['radiant_gold'] = test_X['r1_gold'] + test_X['r2_gold'] + test_X['r3_gold'] + test_X['r4_gold'] + test_X['r5_gold']\ntest_X['radiant_xp'] = test_X['r1_xp'] + test_X['r2_xp'] + test_X['r3_xp'] + test_X['r4_xp'] + test_X['r5_xp']\ntest_X['radiant_towers_killed'] = test_X['r1_towers_killed'] + test_X['r2_towers_killed'] + test_X['r3_towers_killed'] + test_X['r4_towers_killed'] + test_X['r5_towers_killed']\ntest_X['dire_gold'] = test_X['d1_gold'] + test_X['d2_gold'] + test_X['d3_gold'] + test_X['d4_gold'] + test_X['d5_gold']\ntest_X['dire_xp'] = test_X['d1_xp'] + test_X['d2_xp'] + test_X['d3_xp'] + test_X['d4_xp'] + test_X['d5_xp']\ntest_X['dire_towers_killed'] = test_X['d1_towers_killed'] + test_X['d2_towers_killed'] + test_X['d3_towers_killed'] + test_X['d4_towers_killed'] + test_X['d5_towers_killed']\ntest_X['var_gold'] = test_X['radiant_gold'] - test_X['dire_gold']\ntest_X['var_xp'] = test_X['radiant_xp'] - test_X['dire_xp']\ntest_X['var_towers_killed'] = test_X['radiant_towers_killed'] - test_X['dire_towers_killed']\n\n# replace game_time_x with game_time for test_X\ncols_to_keep = ['var_xp','var_gold','var_towers_killed','game_time']\ntest_X = test_X.filter(cols_to_keep,axis=1)\n","f8e0f4d7":"# Make predictions and create submission file\nmodel.fit(traindf_X, traindf_y.values.ravel())\nsubmission = pd.DataFrame()\nsubmission['match_id_hash'] = test_X.index\nsubmission['radiant_win_prob']= model.predict_proba(test_X)[:, 1]\nsubmission.to_csv('submission.csv', index=False)\n","4f57dd71":"We can see that our target feature 'radiant_win' is a boolean, whereas 'next_roshan_team' is a string\/object. We can easily convert radiant_win into a binary column made up of 1 and 0. And then we'll split 'next_roshan_team' into 2 binary columns - one for each team.","4510c06c":"**Step 8: Prediction and submission**","cb58d75e":"**Step 3: Feature engineering**\n\n\nInstinctively, there are two things that jumps at me from this dataset.\n\nFirstly, total team metrics should be more imporant than individual player's metric. This is because Dota2 is a team game where members play different roles, however the teams need to work together in order to play well.\n\nSecondly, relative metrics are more important than absolute metrics. For example, instead of looking at how much gold each team has, we should look at the difference in gold between the teams.\n\nIn order to demonstrate the above two points, we'll go through a series of steps to create the following features:\n1. variance in gold\n2. variance in xp\n3. variance in gold-to-xp ratio\n4. variance in vision\n5. variance in sentries planted\n6. variance in towers killed\n\nPoint 1,2 and 5 are pretty self explanatory, we're just taking the variance between team gold, xp and number of sentry wards planted. The reason why we singled sentry wards is because in the recent versions of the game (observer wards are free while sentry wards cost 90 gold each, perhaps teams who are more willing to spend gold on sentry wards have clearer vision over their opponents).\n\nWith regards to point 3 (gold-to-xp ratio), we will compute the ratio by dividing total team gold by total team xp. The reason for this metric is because there is a distinction between gold and xp. While both features have high correlation (as we can see later), the ratio indicates the team's ability to 'last-hit' and 'grab bounties'. The more efficient the team is at these last 2 events, the higher the ratio.\n\nWith regards to point 4 (vision), we will add both observer wards and sentry wards planted and call them vision. The idea is the more wards being planted, the clearer vision the team will have over the map. This allows team to escape or set-up ganks effectively.\n\nWith regards to point 6 (towers killed), taking or having towers taken are key objectives that closely correlates to the stage of the game. The more towers killed, the closer the team is to taking down the throne. The number of towers taken indicates the amount of space created in the map. Space allows more effective manuveures and farm potential.\n","cba17a66":"**Step 6: Model selection and evaluation**\n\nFor this challenge, I have decided on using Logistic Regression. Instinctively, when \"Gold, XP, Tower\" advantages crosses a threshold value, the possibility of \"comeback\" happening gets close to zero. Therefore, the shape of the points should coincide with the shape of our typical Logistic Regression model.\n\nAs we're using regression to solve for a classification problem, the ROC_AUC will be an appropriate scoring method of choice. To learn more about this, you can visit: https:\/\/towardsdatascience.com\/understanding-auc-roc-curve-68b2303cc9c5\n\nUsing the Logistic Regression seems to yield encouraging results on the testset. Cross Validation shows a mean score of ~0.81. It is a good score, as any higher, we may run into the risk of overfitting.","c3832fcb":"**Step 5: Dealing with outliers**\n\nNow that the features have been selected, the final step would be to weed out outliers. One way to do it is by looking at a scatterplots among these features.","e20e3bec":"**About Me**\n\nI publish kaggle notebooks to outline my learning journey in Data Science. This is the third notebook that I have pusblished. after the last two kaggle challenges, I decided to try my hands on a challenge that coincides with my interests. And I honestly had alot of fun working on this. I am new to Data Science and aspiring to be make a career switch into the industry. However, I have been a Dota  player for far longer than I can remember.\n\nWhile working on this challenge, it occurred to me the importance of Domain Knowledge for a Data Scientist. Due to my understanding about the game mechanics, I am able to quickly identify relevant features, create important features, eradicate outliers and interpret my findings meaningfully.\n\n\nI have structured this notebook into 8 essential steps:\n1. Importing libraries and dataset\n2. Performing data cleaning on trainset\n3. Feature engineering\n4. Feature selection\n5. Dealing with outliers\n6. Model selection and evaluation\n7. Performing data cleaning on testset\n8. Prediction and submission\n\n\n\nLet's dive in!\n\n**Step 1: Importing libraries and dataset**","5c198d68":"**Step 4: Feature selection**\n\nNow that we have created the features we find to be relevant, let's perform some data exploration to find out the correlations between these features. However as there are more than 250 features within our dataframe, let's just pick the top 10 features that correlates to our target variable - 'radiant_win'.","d40652d4":"It is obvious at first glance that entries with \"game_time_x == 0\" are the outliers. Not sure why it happened, but one thing is for certain, it's impossible for teams to gain such level of gold and experience before the game begins - therefore the conclusion is that \"game_time_x\" was not accurately recorded for these entries. As we're looking at only about 20 entries among the dataset of more than 30,000 entries, we can choose to remove them as part of the trainset.","583f8856":"Let's look at the correlation heatmap and try to interpret the results.\n0. radiant_win: Obviously we'll disregard this correlation\n\n1. var_gold: When a team has gold advantage, it likely means that their heroes can be equiped with more or better items and be more effective in team fights and taking towers.\n\n2. var_xp: When a team has xp advantage, it likely means that their heroes are of higher levels and therefore have stronger stats and skill levels - also causing them to be more effective in team fights and taking towers.\n\n3. next_roshan_team_dire: Taking down Roshan means a key player within the team is now in possession of \"Aegis of the Immortal\". In addition to granting an extra life to the key player, it also effectively changes the strategy of the team's gameplay. Having \"Aegis\" usually means the team's ability to take down a critical tower (e.g. tier-3 towers) or win a critical fight that could result in a sharp turning point for the game.\n\n4. var_towers_killed: The more tower advantage a team has, the closer the team is to taking down the throne. It also means that the team likely has more space compared to the other team.\n\n5. next_roshan_team_radiant: Similar to point 3, but from the viewpoint of radiant.\n\n6. dires_towers_killed: In absolute numbers, dires towers need to be killed in succession before reaching the final throne. Therefore, we can see why there is a relatively high correlation. However, as mentioned before, relative metrics should take precedence over absolute metrics - and the relative metrics in point 4 should present a more accurate image of the game's state.\n\nI have chosen to ignore the last 3 features. As discussed previously, we assume that team metrics play a more important role compared to individual metrics. In addition, the position of individual players are extremely fluid. A possible reason why we're seeing a relatively high correlation, is probably because players tend to get closer to the opponent's throne when they're pushing and winning the game, and vice versa. However this is highly related to the number of towers killed, as a sequence of towers need to be taken before reaching the final throne. Therefore sporadic positioning of individual players may not present new information to us, and can therefore be disregarded.\n\n\nAnd now that we have interpreted the results, the next step will be to decide which features we should include in our predictive model. It is safe to say, based on above interpretations, that only point 1 to 5 should be considered in the inclusion of the models. However, point 3 and 5, while a useful feature, can only be obtained in retrospect after the game end. Therefore, at the point of making game predictions, this information will not be available to us, and cannot be included in our model. And that leaves us with 3 features: \"var_gold\", \"var_xp\" and \"var_towers_killed\". \n\n\n**Addressing possibility of comeback with \"time\" information**\n\nIt is at this point of the notebook, that I must make an argument for the inclusion of \"game_time_x\"(train set) and \"game_time\"(test set). Even though this metric does not show high correlation to \"radiant_win\". To understand the reason, we must first talk about the possibilities of \"comebacks\". In dota games, it is not uncommon to see the seemingly losing team turn the tides in the final hours and make a win in later parts of the game - this is what's known as \"comeback\".\n\nIf you're a dota fan, you'll know how Team Liquid and Miracle's Arc Warden manage an epic comeback despite Team VG's 43k gold advantage. While this comeback was epic and will probably be remembered for the remaining history of dota2, it was an extremely unlikely scenario - thats what made it so epic :P\n\nThis is because gold and xp advantages carries more weight in the later parts of the game. Huge advantages in late games makes the possibility of comeback even slimmer, and inversely advantages in early games are being downplayed proportionately. Therefore, I would argue the case for a \"time\" component to be included in our prediction model - as it moderates the significance of Gold, XP and Tower advantages at different stages of the game.","7efee9ca":"**Step 2: Performing data cleaning on trainset**\n\nAs we can see above, not every feature is numerical. As we know, almost all data science models work better with numerical data. So let's identify which are the non-numeric features.","8adc9e8c":"**Step 7: Perform data cleaning on testset**\n\nNow all that's left is to process out testset the same method we did for our trainset."}}