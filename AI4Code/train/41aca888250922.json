{"cell_type":{"1a683c22":"code","bdd76c5c":"code","f095e001":"code","aa95b440":"code","57d53986":"code","7520f8dc":"code","1391bb49":"code","ec104802":"code","b931f5c8":"code","422bb67d":"code","be1929c8":"code","9814f4cc":"code","c2fcf46a":"code","ce5a045d":"code","9900f11c":"code","eb500c59":"code","b838ec99":"code","1144ce1b":"code","689f46d6":"markdown","9de48500":"markdown","17dc41d7":"markdown"},"source":{"1a683c22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport spacy\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\n#Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","bdd76c5c":"df_train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ndf_test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\ndf_sample = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')","f095e001":"df_train.head(20)","aa95b440":"df_test.head(20)","57d53986":"df_train['comment_text'][0]","7520f8dc":"df_train.shape, df_test.shape","1391bb49":"lengths = df_train.comment_text.str.len()\nlengths.mean(), lengths.std(), lengths.min(), lengths.max()","ec104802":"lengths = df_test.comment_text.str.len()\nlengths.mean(), lengths.std(), lengths.min(), lengths.max()","b931f5c8":"def preprocess_reviews(text):\n    text = re.sub(r'<[^>]*>', ' ', text, re.UNICODE)\n    text = re.sub(r'[^\\w\\s]', '', text, re.UNICODE)\n    text = re.sub(r'[^0-9a-zA-Z]+',' ',text, re.UNICODE)\n    text = text.lower()\n    return text\n\ndf_train['processed_comment_text'] = df_train.comment_text.apply(lambda x: preprocess_reviews(x))\ndf_test['processed_comment_text'] = df_test.comment_text.apply(lambda x: preprocess_reviews(x))\n\ndf_train = df_train.sample(frac=0.4)","422bb67d":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\nfor f in os.listdir('..\/input\/xlnetcode\/'):\n    try:\n        if f.split('.')[1] in ['py', 'json']:\n            copyfile(src = \"..\/input\/xlnetcode\/\"+f, dst = \"..\/working\/\"+f)\n    except:\n        continue\nprint(os.listdir('..\/working'))","be1929c8":"!mkdir '..\/input\/train'\n!mkdir '..\/input\/test'","9814f4cc":"!mkdir '..\/input\/train\/pos'\n!mkdir '..\/input\/train\/neg'","c2fcf46a":"train = df_train[['target','processed_comment_text']]\ntrain['target'] = np.where(train['target']>=0.5,1,0)\ntrain.head(10)","ce5a045d":"for index, data in train.iterrows():\n    if data['target'] == 0:\n        f = open('..\/input\/train\/neg\/' + str(index) + '.txt', \"w\")\n        f.write(data['processed_comment_text'])\n        f.close()\n    else:\n        f = open('..\/input\/train\/pos\/' + str(index) + '.txt', \"w\")\n        f.write(data['processed_comment_text'])\n        f.close()","9900f11c":"test = df_test[['processed_comment_text']]\ntest.head(10)","eb500c59":"overwrite_data = True\noutput_dir = '..\/working'\ndata_dir = '..\/working'\niterations_per_loop = 1000\nnum_hosts = 1\nnum_core_per_host = 8\nmax_save = 0\nsave_steps = None\nstrategy = None\nmax_seq_length = 128\nnum_passes = 1","b838ec99":"SCRIPTS_DIR = '..\/working' #@param {type:\"string\"}\nDATA_DIR = '..\/input\/' #@param {type:\"string\"}\nOUTPUT_DIR = '..\/' #@param {type:\"string\"}\nPRETRAINED_MODEL_DIR = '..\/input\/xlnetcode' #@param {type:\"string\"}\nCHECKPOINT_DIR = '..\/' #@param {type:\"string\"}","1144ce1b":"train_command = \"python run_classifier.py \\\n  --do_train=True \\\n  --do_eval=False \\\n  --eval_all_ckpt=True \\\n  --task_name=imdb \\\n  --data_dir=\"+DATA_DIR+\" \\\n  --output_dir=\"+OUTPUT_DIR+\" \\\n  --model_dir=\"+CHECKPOINT_DIR+\" \\\n  --uncased=False \\\n  --spiece_model_file=\"+PRETRAINED_MODEL_DIR+\"\/spiece.model \\\n  --model_config_path=\"+PRETRAINED_MODEL_DIR+\"\/xlnet_config.json \\\n  --init_checkpoint=\"+PRETRAINED_MODEL_DIR+\"\/xlnet_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=8 \\\n  --eval_batch_size=8 \\\n  --num_hosts=1 \\\n  --num_core_per_host=1 \\\n  --learning_rate=2e-5 \\\n  --train_steps=4000 \\\n  --warmup_steps=500 \\\n  --save_steps=500 \\\n  --iterations=2\"\n\n! {train_command}","689f46d6":"# Kernel Details\nWe will create a classifier using XLNET - Base model for Jigsaw Unintended Bias in Toxicity Classification.","9de48500":"** Creating folders which will suit XLNet implementation for reading data and creating examples for IMDB **","17dc41d7":"**Create parameters for running the classifier**"}}