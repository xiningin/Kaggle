{"cell_type":{"0217d4f7":"code","182f5fc3":"code","61412d79":"code","7fda1bdf":"code","1475598c":"code","563008d0":"code","a816c282":"code","4c57b393":"code","433fe056":"code","30482a3e":"code","6608c677":"code","2e08d01b":"code","de1422b8":"code","c5d3ce27":"code","fa7460dc":"code","f1409a2e":"code","ab0e58af":"code","d6892a39":"code","ba02e95c":"code","3e066193":"code","4594c574":"code","38e2f5c7":"code","a58062c4":"code","cdf6ac1b":"code","dab36d27":"code","42d2bfbf":"code","0a64e995":"code","b808e269":"code","f8895a98":"code","7f7b0413":"code","4768ce8b":"code","1416c8c3":"code","923c3df7":"code","134a6a14":"code","32f9f138":"code","6ada8d37":"code","f22a52d7":"code","5e387323":"code","c030517a":"markdown","f69e5e41":"markdown","c064aa48":"markdown","d13b594f":"markdown","076f0e76":"markdown","4fd11296":"markdown","3bb8910d":"markdown","fed5044f":"markdown","72d6054a":"markdown","1ca2ad7b":"markdown","fddd5409":"markdown","06ab4517":"markdown","c40ee7ff":"markdown","05dcadef":"markdown","85743d8f":"markdown"},"source":{"0217d4f7":"#If you find it helpful then please upvote this case study !!","182f5fc3":"import sklearn\nimport numpy as np\nimport pandas as pd","61412d79":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression","7fda1bdf":"data = load_breast_cancer()","1475598c":"results = []","563008d0":"data","a816c282":"print(data['DESCR'])","4c57b393":"X,y = load_breast_cancer(return_X_y = True)","433fe056":"print(X.shape)\nprint(y.shape)","30482a3e":"from sklearn.model_selection import train_test_split\ntrain_X, test_X, train_y, test_y = train_test_split(X,y ,test_size = 0.2,random_state = 24)","6608c677":"print(train_X.shape)\nprint(test_X.shape)\nprint(train_y.shape)\nprint(test_y.shape)","2e08d01b":"train_y.sum()\/455","de1422b8":"test_y.sum()\/114","c5d3ce27":"from sklearn.metrics import accuracy_score\nmodel = LogisticRegression(random_state= 24,penalty='none')\nmodel.fit(train_X,train_y)\npreds = model.predict(test_X)\naccuracy = accuracy_score(test_y,preds)\nprint(\"Accuracy of naive model is : \",accuracy)","fa7460dc":"from sklearn.metrics import accuracy_score\nmodel = LogisticRegression(random_state= 24,max_iter = 15000)\nmodel.fit(train_X,train_y)\npreds = model.predict(test_X)\naccuracy = accuracy_score(test_y,preds)\nprint(\"Accuracy of naive model is : \",accuracy)\nprint(model.coef_)\nresults.append({'accuracy':accuracy,'scaling': 'No','regularization' : 'None'})","f1409a2e":"from sklearn.metrics import confusion_matrix","ab0e58af":"confusion_matrix(test_y,preds)","d6892a39":"from sklearn.metrics import accuracy_score\nmodel = LogisticRegression(random_state= 24,max_iter = 15000, penalty = 'none')\nmodel.fit(train_X,train_y)\npreds = model.predict(test_X)\naccuracy = accuracy_score(test_y,preds)\nprint(\"Accuracy of naive model is : \",accuracy)\nprint(model.coef_)\nresults.append({'accuracy':accuracy,'scaling': 'No','regularization' : 'None'})","ba02e95c":"model.get_params()","3e066193":"model.coef_","4594c574":"from sklearn.metrics import accuracy_score\nmodel = LogisticRegression(random_state= 24,max_iter = 15000, solver = 'liblinear', penalty = 'l1')\nmodel.fit(train_X,train_y)\npreds = model.predict(test_X)\naccuracy = accuracy_score(test_y,preds)\nprint(\"Accuracy of naive model is : \",accuracy)\nprint(model.coef_)\nresults.append({'accuracy':accuracy,'scaling': 'No','regularization' : 'L1'})","38e2f5c7":"import pandas as pd","a58062c4":"data_df = pd.DataFrame(X, columns = data['feature_names'])\ndata_df.describe()","cdf6ac1b":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","dab36d27":"data_df","42d2bfbf":"model = LogisticRegression(random_state= 24, max_iter = 15000)\nmodel.fit(train_X, train_y)\npreds = model.predict(test_X)\naccuracy = accuracy_score(test_y, preds)\nprint('Accuracy of l2 regularization is : ', accuracy)\nprint(model.coef_)\nresults.append( {'accuracy': accuracy,'scaling':'Yes', 'regularization':'l2'})","0a64e995":"model = LogisticRegression(random_state= 24, max_iter = 15000, penalty='none')\nmodel.fit(train_X, train_y)\npreds = model.predict(test_X)\naccuracy = accuracy_score(test_y, preds)\nprint('Accuracy of naive model is : ', accuracy)\nprint(model.coef_)\nresults.append( {'accuracy': accuracy,'scaling':'Yes', 'regularization':'None'})","b808e269":"model = LogisticRegression(random_state= 24, max_iter = 15000, solver='liblinear', penalty = 'l1' )\nmodel.fit(train_X, train_y)\npreds = model.predict(test_X)\naccuracy = accuracy_score(test_y, preds)\nprint('Accuracy of naive model with l1 regularization is : ', accuracy)\nprint(model.coef_)\nresults.append( {'accuracy': accuracy,'scaling':'Yes', 'regularization':'l1'})","f8895a98":"pd.DataFrame(results)","7f7b0413":"from sklearn.model_selection import GridSearchCV\nmodel = LogisticRegression(random_state = 24, max_iter = 20000)\nparams_to_tune = [ {'C': [1e-4, 1e-2, 1e-1,1,10, 100, 1000] },\n                   {'tol': [ 1e-4, 1e-3,100,1000 ]},\n                   \n                  ]\n\ncv_object = GridSearchCV(model , params_to_tune, scoring = 'accuracy',n_jobs=-1)\n\ncv_object.fit(train_X, train_y)\n","4768ce8b":"cv_object.best_estimator_","1416c8c3":"cv_object.best_score_","923c3df7":"cv_object.best_params_","134a6a14":"# running on test dataset \nfinal_model = cv_object.best_estimator_\n\npreds = final_model.predict(test_X)\n\nprint('Accuracy Score : ', accuracy_score(test_y, preds))","32f9f138":"import numpy as np \nmodel = LogisticRegression(C = 10,solver='liblinear', penalty = 'l1')\nmodel.fit(train_X, train_y)\n\n\nprint(model.coef_)\nprint(np.count_nonzero(model.coef_))","6ada8d37":"import numpy as np \nmodel = LogisticRegression(C = 100,solver='liblinear', penalty = 'l1')\nmodel.fit(train_X, train_y)\n\n\nprint(model.coef_)\nprint(np.count_nonzero(model.coef_))","f22a52d7":"import numpy as np \nmodel = LogisticRegression(C = 0.0001,solver='liblinear', penalty = 'l1')\nmodel.fit(train_X, train_y)\n\n\nprint(model.coef_)\nprint(np.count_nonzero(model.coef_))","5e387323":"import numpy as np \nmodel = LogisticRegression(C = 0.01,solver='liblinear', penalty = 'l1')\nmodel.fit(train_X, train_y)\n\n\nprint(model.coef_)\nprint(np.count_nonzero(model.coef_))","c030517a":"# Hyper parameter tuning \n\nUsing GridSearchCV to find best parameters for Regularization","f69e5e41":"### Let's see how my weights change","c064aa48":"Accuracy on basic model with l1 regularization and scaling is : 95.61","d13b594f":"Accuracy without regularization and more iterations : 94.73 and we see that the coefficients are big ","076f0e76":"Accuracy with L2 regularization and scaling : 93.85","4fd11296":"### Accuracy after Scaling\nHere we have not selected any regularization parameter","3bb8910d":"### Before applying any regularization parameter","fed5044f":"Accuracy with L2 regularization and without scaling : 93.85","72d6054a":"Now we see the data and do some minor exploration","1ca2ad7b":"## About this case study\n\nI have used regularization technique methods :-\nHere in this project you will see that I have firstly showed you the model with no regularization technique and model with  regularization technique.\nHere L1 and L2 are used to prevent Overfitting.You will be seeing the different weights.L2 just minimizes all weights which are bigger to prevent overfitting and L1 also does in the same way and also it does some weights zero(0)i.e means it ignores the features which has no importance by giving us the sparse matrix\n\nAbout GridSearch:\n\nUsed to calculate the Value of Lambda which we  saw in our Logistic Loss Function. \nIt is gives me the best value of C.And after getting the best value of C I have applied that value in Logistic Regression","fddd5409":"Hey Everyone,\n\nI have just worked on breast cancer dataset.I have explained various few important things which will prevent your model to be overfitted and will improve your accuracy :-\n\n I have explained :-\n\n1.When to use L1 and L2 regularization techniques ?\n\n2.How to find out the best value of Lambda using sklearn package GridSearchCV?\n\n3.How my L1 and L2 reduces the weights and prevent overfitting in regression problems?\n\n4.How different values of weights changes with different values of lambda?\n\n5.How using GridSearch you can hypertune your maximum no of hyperparameters in any algorithm?\n\n6.Sparse Matrix & Why L1 is used for fast prediction\n\n","06ab4517":"### Let's see how weights change with different values of C ","c40ee7ff":"## After applying any regularization parameter\n\n### So here we have to increase our iterations to converge  and by default L2 is applied","05dcadef":"Accuracy with L1 regularization and without scaling : 95.61\n\nWe see that the coefficients are 0 now and we get a sparse vector","85743d8f":"We will work with the toy example of breast cancer dataset already avaiable in sklearn."}}