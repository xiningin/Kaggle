{"cell_type":{"b8a0496c":"code","66109316":"code","1a297fea":"code","59026978":"code","43b4b1dd":"code","e197100f":"code","31dbc7fb":"code","d9da2806":"code","893b9b68":"code","f62f0f84":"code","f890ebe5":"code","94161c27":"code","5995594d":"code","2bd334c7":"code","7d96fee0":"code","9e31a000":"code","025a06fe":"code","cae4b904":"code","fe113ec6":"code","3924c564":"code","1dfca445":"code","946fa252":"code","6039f1b0":"code","811440d9":"code","61e4a164":"code","aa307d56":"code","0d1ad065":"code","06b2877e":"code","1dd54df1":"code","8e51911c":"code","fe18e0f2":"code","58974c38":"code","27249ea1":"code","78a61e62":"code","d0aae70e":"code","fc25e07a":"code","32b804f7":"code","64d2008c":"code","378b3662":"code","c43bb35b":"code","edb7a990":"code","0c81aef8":"code","a66ff8f8":"code","5c646fe2":"code","a20f9c11":"code","77e493f5":"code","3184c542":"code","a1b8795e":"code","e5ba5abc":"code","4c93dc7b":"code","4bc10045":"code","fff0c2cc":"code","9e44a528":"code","452032f6":"code","31efe06c":"code","7665445b":"code","7a650166":"code","ce380431":"code","c4f1bad3":"code","d663d8bd":"code","aa9e3d2a":"code","800d6e33":"code","2e3a9a80":"code","90381738":"code","e7fceb61":"code","a4d31af0":"code","68d692bd":"code","99751c39":"code","08182b5a":"code","1caed601":"code","178a41f9":"code","6af8f0c2":"code","d19615b4":"code","df13cfa2":"code","418f73b2":"code","85dd4d65":"code","43634c81":"code","765c42f3":"code","8f9a058c":"markdown","f446128e":"markdown","aa8f4cd8":"markdown","7f1d6a5d":"markdown","d7eac201":"markdown","6ef97163":"markdown","5d1b1e05":"markdown","7bb81356":"markdown","e1902055":"markdown","abfd016c":"markdown","d3f9bb32":"markdown","7af7b83d":"markdown","005e1c43":"markdown","22570cfb":"markdown","898fc4d9":"markdown"},"source":{"b8a0496c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66109316":"df = pd.read_csv(r'\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","1a297fea":"df.head()","59026978":"df.shape","43b4b1dd":"df.describe()","e197100f":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.figure(figsize=(20,25), facecolor='white')\nplotnumber = 1\n\nfor column in df:\n    if plotnumber<=9 :     # as there are 9 columns in the data\n        ax = plt.subplot(3,3,plotnumber)\n        sns.distplot(df[column])\n        plt.xlabel(column,fontsize=20)\n    plotnumber+=1\nplt.show()","31dbc7fb":"df['BMI'] = df['BMI'].replace(0,df['BMI'].mean())\ndf['BloodPressure'] = df['BloodPressure'].replace(0,df['BloodPressure'].mean())\ndf['Glucose'] = df['Glucose'].replace(0,df['Glucose'].mean())\ndf['Insulin'] = df['Insulin'].replace(0,df['Insulin'].mean())\ndf['SkinThickness'] = df['SkinThickness'].replace(0,df['SkinThickness'].mean())","d9da2806":"plt.figure(figsize=(20,25), facecolor='white')\nplotnumber = 1\n\nfor column in df:\n    if plotnumber<=9 :\n        ax = plt.subplot(3,3,plotnumber)\n        sns.distplot(df[column])\n        plt.xlabel(column,fontsize=20)\n    plotnumber+=1\nplt.show()","893b9b68":"fig, ax = plt.subplots(figsize=(15,10))\nsns.boxplot(data=df, width= 0.5,ax=ax,  fliersize=3)","f62f0f84":"q = df['Pregnancies'].quantile(0.98)\n# we are removing the top 2% data from the Pregnancies column\ndata_cleaned = df[df['Pregnancies']<q]\nq = data_cleaned['BMI'].quantile(0.99)\n# we are removing the top 1% data from the BMI column\ndata_cleaned  = data_cleaned[data_cleaned['BMI']<q]\nq = data_cleaned['SkinThickness'].quantile(0.99)\n# we are removing the top 1% data from the SkinThickness column\ndata_cleaned  = data_cleaned[data_cleaned['SkinThickness']<q]\nq = data_cleaned['Insulin'].quantile(0.95)\n# we are removing the top 5% data from the Insulin column\ndata_cleaned  = data_cleaned[data_cleaned['Insulin']<q]\nq = data_cleaned['DiabetesPedigreeFunction'].quantile(0.99)\n# we are removing the top 1% data from the DiabetesPedigreeFunction column\ndata_cleaned  = data_cleaned[data_cleaned['DiabetesPedigreeFunction']<q]\nq = data_cleaned['Age'].quantile(0.99)\n# we are removing the top 1% data from the Age column\ndata_cleaned  = data_cleaned[data_cleaned['Age']<q]","f890ebe5":"fig, ax = plt.subplots(figsize=(15,10))\nsns.boxplot(data=data_cleaned, width= 0.5,ax=ax,  fliersize=3)","94161c27":"from scipy import stats     \nimport numpy as np\nz = np.abs(stats.zscore(data_cleaned))      #Using Z-score for removing some more outliers\nprint(z)","5995594d":"threshold = 3\nprint(np.where(z > 3))","2bd334c7":"data1 = data_cleaned[(z < 3).all(axis=1)]","7d96fee0":"fig, ax = plt.subplots(figsize=(15,10))\nsns.boxplot(data=data1, width= 0.5,ax=ax,  fliersize=3)","9e31a000":"data1.shape","025a06fe":"y = data1['Outcome']\nX = data1.drop('Outcome',axis=1)","cae4b904":"plt.figure(figsize=(20,25), facecolor='white')\nplotnumber = 1\n\nfor column in data1:\n    if plotnumber<=9 :\n        ax = plt.subplot(3,3,plotnumber)\n        sns.distplot(data1[column])\n        plt.xlabel(column,fontsize=20)\n    plotnumber+=1\nplt.show()","fe113ec6":"plt.figure(figsize=(20,25), facecolor='white')\nplotnumber = 1\n\nfor column in X:\n    if plotnumber<=9 :\n        ax = plt.subplot(3,3,plotnumber)\n        sns.stripplot(y,X[column])\n    plotnumber+=1\nplt.tight_layout()","3924c564":"#Standard-Scaling\nfrom sklearn.preprocessing import StandardScaler   \nscalar = StandardScaler()                               \nX_scaled = scalar.fit_transform(X)","1dfca445":"#Check for multicollinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"vif\"] = [variance_inflation_factor(X_scaled,i) for i in range(X_scaled.shape[1])]\nvif[\"Features\"] = X.columns\n\n#let's check the values\nvif","946fa252":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X_scaled,y, test_size= 0.2, random_state = 60)","6039f1b0":"from sklearn.linear_model  import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train)","811440d9":"y_pred = log_reg.predict(x_test)","61e4a164":"from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\naccuracy = accuracy_score(y_test,y_pred)\naccuracy","aa307d56":"# Confusion Matrix\nconf_mat = confusion_matrix(y_test,y_pred)\nconf_mat","0d1ad065":"true_positive = conf_mat[0][0]\nfalse_positive = conf_mat[0][1]\nfalse_negative = conf_mat[1][0]\ntrue_negative = conf_mat[1][1]","06b2877e":"# Breaking down the formula for Accuracy\nAccuracy = (true_positive + true_negative) \/ (true_positive +false_positive + false_negative + true_negative)\nAccuracy","1dd54df1":"# Precison\nPrecision = true_positive\/(true_positive+false_positive)\nPrecision","8e51911c":"# Recall\nRecall = true_positive\/(true_positive+false_negative)\nRecall","fe18e0f2":"# F1 Score\nF1_Score = 2*(Recall * Precision) \/ (Recall + Precision)\nF1_Score","58974c38":"# Area Under Curve\nauc = roc_auc_score(y_test, y_pred)\nauc","27249ea1":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf.fit(x_train,y_train)","78a61e62":"clf.score(x_train,y_train)","d0aae70e":"clf.score(x_test,y_test)","fc25e07a":"grid_param = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth' : range(2,32,1),\n    'min_samples_leaf' : range(1,10,1),\n    'min_samples_split': range(2,10,1),\n    'splitter' : ['best', 'random']\n    \n}","32b804f7":"from sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(estimator=clf,\n                     param_grid=grid_param,\n                     cv=5,\n                    n_jobs =-1)","64d2008c":"grid_search.fit(x_train,y_train)","378b3662":"best_parameters = grid_search.best_params_\nprint(best_parameters)","c43bb35b":"clf = DecisionTreeClassifier(criterion = 'entropy', max_depth =10, min_samples_leaf= 9, min_samples_split= 2, splitter ='random')\nclf.fit(x_train,y_train)","edb7a990":"clf.score(x_test,y_test)","0c81aef8":"y_pred = clf.predict(x_test)","a66ff8f8":"#Area under the curve\nauc = roc_auc_score(y_test, y_pred)\nauc","5c646fe2":"from sklearn.ensemble import RandomForestClassifier\nrand_clf = RandomForestClassifier(random_state=6)","a20f9c11":"rand_clf.fit(x_train,y_train)","77e493f5":"rand_clf.score(x_test,y_test)","3184c542":"grid_param = {\n    \"n_estimators\" : [90,100,115],\n    'criterion': ['gini', 'entropy'],\n    'max_depth' : range(2,20,2),\n    'min_samples_leaf' : range(1,10,1),\n    'min_samples_split': range(2,10,1),\n    'max_features' : ['auto','log2']\n}","a1b8795e":"grid_search = GridSearchCV(estimator=rand_clf,param_grid=grid_param,cv=5,n_jobs =-1,verbose = 3)","e5ba5abc":"grid_search.fit(x_train,y_train)","4c93dc7b":"grid_search.best_params_","4bc10045":"rand_clf = RandomForestClassifier(criterion= 'entropy',\n max_depth = 10,\n max_features = 'auto',\n min_samples_leaf = 3,\n min_samples_split= 8,\n n_estimators = 100,random_state=6)","fff0c2cc":"rand_clf.fit(x_train,y_train)","9e44a528":"rand_clf.score(x_test,y_test)","452032f6":"y_pred = rand_clf.predict(x_test)","31efe06c":"auc = roc_auc_score(y_test, y_pred)\nauc","7665445b":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(x_train,y_train)","7a650166":"knn.score(x_train,y_train)","ce380431":"knn.score(x_test,y_test)","c4f1bad3":"param_grid = { 'algorithm' : ['ball_tree', 'kd_tree', 'brute'],\n               'leaf_size' : [18,20,25,27,30,32,34],\n               'n_neighbors' : [3,5,7,9,10,11,12,13,15,17,19]\n              }","d663d8bd":"gridsearch = GridSearchCV(knn, param_grid,verbose=3)","aa9e3d2a":"gridsearch.fit(x_train,y_train)","800d6e33":"gridsearch.best_params_","2e3a9a80":"knn = KNeighborsClassifier(algorithm = 'ball_tree', leaf_size =18, n_neighbors =13)","90381738":"knn.fit(x_train,y_train)","e7fceb61":"knn.score(x_train,y_train)","a4d31af0":"knn.score(x_test,y_test)","68d692bd":"y_pred = knn.predict(x_test)","99751c39":"auc = roc_auc_score(y_test, y_pred)\nauc","08182b5a":"from xgboost import XGBClassifier\nmodel = XGBClassifier(objective='binary:logistic')\nmodel.fit(x_train,y_train)","1caed601":"# cheking training accuracy\ny_pred = model.predict(x_train)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_train,predictions)\naccuracy","178a41f9":"# cheking initial test accuracy\ny_pred = model.predict(x_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test,predictions)\naccuracy","6af8f0c2":"param_grid={\n   \n    'learning_rate':[1,0.5,0.1,0.01,0.001],\n    'max_depth': [3,5,10,20],\n    'n_estimators':[10,50,100,200]\n    \n}","d19615b4":"grid= GridSearchCV(XGBClassifier(objective='binary:logistic'),param_grid, verbose=3)","df13cfa2":"grid.fit(x_train,y_train)","418f73b2":"grid.best_params_","85dd4d65":"new_model=XGBClassifier(learning_rate=0.01, max_depth= 3, n_estimators= 200)\nnew_model.fit(x_train, y_train)","43634c81":"y_pred_new = new_model.predict(x_test)\npredictions_new = [round(value) for value in y_pred_new]\naccuracy_new = accuracy_score(y_test,predictions_new)\naccuracy_new","765c42f3":"auc = roc_auc_score(y_test, y_pred_new)\nauc","8f9a058c":"The accuracy has increased for the test dataset.","f446128e":"Seems before hyperparameter tuning, the model was overfitting, now it is better.\n","aa8f4cd8":"The model is highly overfitting. Need to perform hyperparameter tuning","7f1d6a5d":"No multicollinearity as such(Since vif values are below 5)","d7eac201":"Performing hyperparameter tuning","6ef97163":"Getting almost the same accuracy after hyperparameter tuning, maybe it needed more parameters","5d1b1e05":"# Random Forest","7bb81356":"# XGBoost","e1902055":"# Decision Tree","abfd016c":"# ** Considering the accuracy and AUC scores of all the models taken here, Random Forest Classifier and XGBoost Classifier models perform the best","d3f9bb32":"Seems the model is highly overfitting, lets perform hyperparameter tuning using Grid Search CV and see whether the accuracy improves.","7af7b83d":"# KNN","005e1c43":"Accuracy improved marginally.","22570cfb":"# Logistic Regression","898fc4d9":"Performing hyperparameter tuning"}}