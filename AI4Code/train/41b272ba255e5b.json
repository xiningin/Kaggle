{"cell_type":{"fa4b9099":"code","75105a66":"code","3a447b27":"code","3ae1ff35":"code","d34060c2":"code","46997cb8":"code","600728a4":"code","08bb9b85":"code","ae49a0ae":"code","412e6d77":"code","be200947":"code","5b08e11b":"code","337b04c7":"code","0c51326f":"code","06ae1bec":"code","452cb4a1":"code","b6aa4f11":"code","ec3eb66d":"code","4af51eea":"code","ff424a76":"code","401b75bd":"code","794ba586":"code","273f66a3":"code","5fd56f23":"code","bf0afae2":"code","b2b83213":"code","31d8f64d":"code","a261bffb":"code","eebd2154":"code","2a9010c6":"code","40a0c271":"code","89e14a1f":"code","5cf5cf5f":"code","1b7090fb":"code","37b23a8b":"code","0d00e471":"code","6f64a62d":"code","3815cb03":"code","870ff840":"code","7517b01c":"code","3b5a1ed7":"code","1b74b641":"code","6c9b2489":"code","e7324174":"code","58173724":"code","5509c47f":"markdown","286f4c33":"markdown","5945c6e0":"markdown","81c56db2":"markdown","ecced4f6":"markdown","bd391347":"markdown","2517d033":"markdown","4480fbc7":"markdown","fd98c770":"markdown","b9713ff8":"markdown","074eca69":"markdown","b9b4235e":"markdown","8e812230":"markdown","795d1f8e":"markdown","c9d7e4ba":"markdown","9992b540":"markdown","54a028d4":"markdown","e6b1188f":"markdown","0b509e07":"markdown"},"source":{"fa4b9099":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75105a66":"lung_cancer_file_path = '\/kaggle\/input\/lung-cancer\/survey lung cancer.csv'","3a447b27":"lung_cancer_data = pd.read_csv(lung_cancer_file_path)","3ae1ff35":"lung_cancer_data.describe()","d34060c2":"lung_cancer_data.columns","46997cb8":"lung_cancer_data.shape","600728a4":"lung_cancer_data.isnull().sum()","08bb9b85":"lung_cancer_data = lung_cancer_data = lung_cancer_data.dropna(axis=0)","ae49a0ae":"# label encoding\nlung_cancer_data.replace({\"LUNG_CANCER\":{'YES':0,'NO':1}},inplace=True)\n# printing the first 5 rows of the dataframe\nlung_cancer_data.head(5)","412e6d77":"# label encoding\nlung_cancer_data.replace({\"GENDER\":{'M':0,'F':1}},inplace=True)\n# printing the first 5 rows of the dataframe\nlung_cancer_data.head(5)","be200947":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize = (20, 25))\nplotnumber = 1\nfor column in lung_cancer_data:\n    if plotnumber <= 9:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.distplot(lung_cancer_data[column])\n        plt.xlabel(column, fontsize = 15)\n        \n    plotnumber += 1\nplt.show()","5b08e11b":"y = lung_cancer_data.LUNG_CANCER","337b04c7":"lung_cancer_datafeatures = ['AGE', 'GENDER', 'SMOKING', 'ANXIETY', 'CHRONIC DISEASE', 'ALCOHOL CONSUMING', 'SHORTNESS OF BREATH']","0c51326f":"X = lung_cancer_data[lung_cancer_datafeatures]","06ae1bec":"X.describe()","452cb4a1":"X.head","b6aa4f11":"from sklearn.tree import DecisionTreeRegressor","ec3eb66d":"lung_cancer_model = DecisionTreeRegressor(random_state = 1)","4af51eea":"lung_cancer_model.fit(X,y)","ff424a76":"print(X.head())","401b75bd":"print(lung_cancer_model.predict(X.head()))","794ba586":"predictions = lung_cancer_model.predict(X)\nprint(predictions)","273f66a3":"print(lung_cancer_model.predict(X.head()))","5fd56f23":"from sklearn.model_selection import train_test_split\n# separating into train and testing\nX_train, X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=1)\nprint(\"Shape of X_train is \" ,X_train.shape)\nprint(\"Shape of X_test  is \" ,X_test.shape)\nprint(\"Shape of Y_train is \" ,y_train.shape)\nprint(\"Shape of Y_test  is \" ,y_test.shape)","bf0afae2":"import pandas as pd\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split","b2b83213":"train_X, val_X, train_y, val_y = train_test_split(X,y,random_state = 1)\n\n#lung_cancer_model = DecisionTreeRegressor() \n#lung_cancer_model.fit(train_X, y)","31d8f64d":"lung_cancer_model = DecisionTreeRegressor(random_state=1) \n\n# Fit Lung Cancer Model with the training data.\nlung_cancer_model.fit(train_X, train_y)","a261bffb":"val_predictions = lung_cancer_model.predict(val_X)","eebd2154":"# print the top few validation predictions\nprint(X.head())\n# print the top few actual prices from validation data\nprint()","2a9010c6":"from sklearn.metrics import mean_absolute_error\nval_mae = mean_absolute_error(val_predictions, val_y) \n\n# uncomment following line to see the validation_mae\nprint(val_mae)","40a0c271":"from sklearn.metrics import mean_absolute_error \nfrom sklearn.tree import DecisionTreeRegressor","89e14a1f":"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes, random_state = 0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","5cf5cf5f":"from sklearn.tree import DecisionTreeClassifier","1b7090fb":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\ndtc = DecisionTreeClassifier()\ndtc.fit(train_X,train_y)\n\ny_pred = dtc.predict(val_X)\n\ndtc_train_accuracy = accuracy_score(train_y, dtc.predict(train_X))\ndtc_test_accuracy = accuracy_score(val_y, val_predictions)\n\nprint(f\"Training Accuracy of Decision Tree Model is: {dtc_train_accuracy}\")\nprint(f\"Testing Accuracy of Decision Tree Model is: {dtc_test_accuracy}\")","37b23a8b":"from sklearn.model_selection import GridSearchCV ","0d00e471":"grid_params = {\n    'criterion':['gini', 'entropy'],\n    'max_depth':[3,5,7,10],\n    'min_samples_split':range(2,10,1),\n    'min_samples_leaf': range(2,10,1)\n}","6f64a62d":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ngrid_search = GridSearchCV(dtc, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(train_X, train_y)\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n\ndtc = grid_search.best_estimator_\ny_pred = dtc.predict(X_test)\ndtc_train_accuracy = accuracy_score(train_y, dtc.predict(train_X))\ndtc_test_acc = accuracy_score(y_test, y_pred)","3815cb03":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error","870ff840":"forest_model = RandomForestRegressor(random_state = 1)\nforest_model.fit(train_X, train_y)\nlung_preds = forest_model.predict(val_X)\nprint(\"Validation MAE for Random Forest Model is: {}\".format(mean_absolute_error(val_y, lung_preds)))","7517b01c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nlr_train_acc = accuracy_score(y_train, lr.predict(X_train))\nlr_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Logistic Regression Model is {lr_train_acc}\")\nprint(f\"Test Accuracy of Logistic Regression Model is {lr_test_acc}\")","3b5a1ed7":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nprint(classification_report(y_test, y_pred))","1b74b641":"from sklearn import tree \nplt.figure(figsize = (15,10))\ntree.plot_tree(dtc,filled = True)","6c9b2489":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(X_train, y_train)\nmelb_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, melb_preds))","e7324174":"from sklearn.ensemble import RandomForestClassifier\n\nrand_clf = RandomForestClassifier(criterion = 'gini', max_depth = 1000, max_features = 'sqrt', min_samples_leaf = 2, min_samples_split = 4, n_estimators = 10000)\nrand_clf.fit(X_train, y_train)\n\ny_pred = rand_clf.predict(X_test)\n\nrand_clf_train_acc = accuracy_score(y_train, rand_clf.predict(X_train))\nrand_clf_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Random Forest Model is {rand_clf_train_acc}\")\nprint(f\"Test Accuracy of Random Forest Model is {rand_clf_test_acc}\")","58173724":"from sklearn.datasets import load_iris\niris = load_iris()\n\n# Model (can also use single decision tree)\nfrom sklearn.ensemble import RandomForestClassifier\nrand_clf = RandomForestClassifier(n_estimators=10)\n\n# Train\nrand_clf.fit(iris.data, iris.target)\n# Extract single tree\nestimator = rand_clf.estimators_[5]\n\nfrom sklearn.tree import export_graphviz\n# Export as dot file\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = iris.feature_names,\n                class_names = iris.target_names,\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n\nfrom IPython.display import Image\nImage(filename = 'tree.png')\n","5509c47f":"# **Model Validation**","286f4c33":"Below since the values 'M' for Male and 'F' for Female can't be used as data values, we will be turning them into 1's and 0's in the cell below. The gender male, would have the value 0, and the gender female would have the value 1. ","5945c6e0":"I got this code to visualize Random Forest Classifier's from this website. https:\/\/towardsdatascience.com\/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c\nAll other code is provided in the pytorch and scikit learning from the documentation https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n\nSome portions of code (displaying data, turning data to 1's and 0's) were from these links","81c56db2":"# **Decision Tree Regressor (Classifier)**","ecced4f6":"The Lung Cancer Data value will be the target value the model will try and predict whether or not the patient will have lung cancer","bd391347":"The Random Forest Classifier is the most accurate model boasting an 88% accuracy being the most accurate model possible. This model is the most efficient way to determine whether or not a patient has lung cancer based on their behavior and symptoms. ","2517d033":"**Mean Absolute Error is a value of 0.1538 (The Lower the Better only for Error)**","4480fbc7":"# **Data Visualization**","fd98c770":"Logistic Regression is the easiest and most efficient way of implementing machine learning to predict the following values which we want to find. Our model was able to find the optimal path to find whether or not ","b9713ff8":"Below we are going to split the training and validation sets to reduce the number of errors","074eca69":"As shown above, this model will utitlize the Decision Tree Classifier Model as its more accurate. This is due to the fact that logistic regression only generalizes the data, and finds a generalized answer. A decision tree regressor on the otherhand is more specific and utilizes everything to create a more specific answer to the problem. ","b9b4235e":"# **Checking for Null Values**","8e812230":"The training model has a 91% accuracy with the training model and an 88% accurate model with the trandom forests","795d1f8e":"# **Logistic Prediction Model**","c9d7e4ba":"We are going to use the decision tree regressor as it more accurate to predict models. Previously we used Logistic Regression to generalize the dataset. ","9992b540":"Below will be the features for our problem, these are teh inputs or \"x-values\" our model will use to predict whether or not the patient has lung cancer","54a028d4":"**Dropping Null Values From the Data Set**","e6b1188f":"The Decision Tree Classifier has a 99% accuracy with the training model, but only a measely 84% training accuracy with the testing model. We need to use better classifiers.","0b509e07":"***The Random Forest Prediction Model produced a value of 0.19742460317460317 still good, but not good enough***"}}