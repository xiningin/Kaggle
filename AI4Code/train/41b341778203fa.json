{"cell_type":{"9250e52f":"code","07d98d6d":"code","c7c56f9a":"code","3ee18634":"code","b69fcf97":"code","079d3acd":"code","f80176de":"code","a2e5c886":"code","cc7e4c6f":"code","20fe81c3":"code","0338cb8b":"code","5ce4303c":"code","c245e9ab":"code","02595331":"code","58c90a80":"code","5ba489d4":"code","9c348bb5":"code","2e7fd4ce":"code","63442316":"code","ac6d1c37":"code","4d8b9541":"code","3321681a":"code","52782a50":"code","b1da8d23":"code","1fa91750":"code","e44137d2":"code","1f48106c":"code","218058bd":"markdown","f1ee8d7d":"markdown"},"source":{"9250e52f":"# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport fasttext as ft\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nimport time\nimport datetime\nimport re\nimport random as rand\n\nfrom catboost import CatBoostRegressor\n\npd.options.mode.chained_assignment = None","07d98d6d":"# Taking a look at our data\ndata = pd.read_csv('..\/input\/medium-data-science-articles-dataset\/medium-data-science-articles-2020.csv')\ndata","c7c56f9a":"# There are only 7 possible tags, so we can convert the tag name into numerical ID,\n# without necessity to vectorize its text\ndata['tag'].value_counts()","3ee18634":"# Constructing dictionary of tag names and corresponding integer IDs\ntag_id_dict = {}\ntag_id = 0\n\nfor tag in data['tag'].unique():\n    tag_id_dict[tag] = tag_id\n    tag_id += 1","b69fcf97":"# We have ten columns, but the 'url' and 'author_page' are dropped as their semantics is similar\n# to 'title' and 'author' respectively and contains redundant HTTP tags\n# 'responses' and 'reading_time' make us aware of article popularity which we don't want our model to know\ndata = data.drop(columns=['responses', 'reading_time', 'url', 'author_page'])\ndata","079d3acd":"# 'Subtitle has NAs (sometimes they are not specified in the article)'\ndata.isna().any()","f80176de":"# Filling NAs with 'none'\ndata['subtitle'] = data['subtitle'].fillna('none')\ndata.isna().any()","a2e5c886":"# Text preprocessing: removing stopwords, punctuation signs,\n# words that are 1 and 2 letters long\n# remaining words are lemmatized and made lowercase\nstop_words = stopwords.words('english')\nlemmatizer = WordNetLemmatizer()\n\nfor col in data.drop(columns=['tag', 'claps', 'date']):\n    data[col] = data[col].apply(lambda x: ' '.join([lemmatizer.lemmatize(w) if w not in stop_words and len(w) > 2\n                                                    else '' for w in re.sub('[^a-zA-Z0-9]', ' ', x).lower().split()]))\n\n\n# Replacing tags with their numerical IDs created above\ndata['tag'] = data['tag'].apply(lambda x: tag_id_dict[x])\n\ndata","cc7e4c6f":"# Let's add one date-based feature: for each month (except January) we'll count\n# an estimate of tag (topic) popularity as ratio of number of articles\n# with this tag and number of all articles published in previous month\n# For each article, we'll add the popularity of its tag during previous month\ndata_w_tag_rates = pd.DataFrame()\n\nfor i in range(1, 13):\n    cur_mon_data = data[data.date.str.contains(f'-{i}-' if i >= 10 else f'-0{i}-')]\n    \n    if i == 1:\n        cur_mon_data['prev_mon_topic_rate'] = 0\n    else:\n        prev_mon_data = data[data.date.str.contains(f'-{i-1}-' if i-1 >= 10 else f'-0{i-1}-')]\n        prev_mon_tag_rates = {}\n        for tag in tag_id_dict.values():\n            prev_mon_cur_tag = prev_mon_data[prev_mon_data.tag == tag]\n            prev_mon_tag_rates[tag] = len(prev_mon_cur_tag) \/ len(prev_mon_data)\n        \n        cur_mon_data['prev_mon_topic_rate'] = cur_mon_data.tag.apply(lambda x: prev_mon_tag_rates[x])\n        \n    data_w_tag_rates = data_w_tag_rates.append(cur_mon_data)\n    \ndata = data_w_tag_rates\n\n# We also need to make train, validation and test sets;\n# the model will be trained on articles from Jan to Oct,\n# validated on Nov and tested on Dec\n# We keep date column to simplify the split,\n# but we need to convert the date to milliseconds\n# to use it as numerical training parameter\ndata['date_ms'] = data['date'].apply(lambda x: float(time.mktime(datetime.datetime.strptime(x, \"%Y-%m-%d\").timetuple())))\nmax_ts = data['date_ms'].max()\nmin_ts = data['date_ms'].min()\ndata['date_ms'] = data['date_ms'].apply(lambda x: (x - min_ts) \/ (max_ts - min_ts))","20fe81c3":"# Final look of our data before text vectoring\ndata","0338cb8b":"# Splitting to train, val and test, removing date string\ndata_train = data[(data.date.str.contains('-11-') == False) & (data.date.str.contains('-12-') == False)].drop(columns=['date'])\ndata_val = data[data.date.str.contains('-11-')].drop(columns=['date']).reset_index(drop=True)\ndata_test = data[data.date.str.contains('-12-')].drop(columns=['date']).reset_index(drop=True)","5ce4303c":"# We'll use mean absolute error as quality metric, so let's look\n# at target variable range in test set to be able to understand\n# whether MAE is high or low\nprint(f\"Min claps in test: {data_test.claps.min()}, max: {data_test.claps.max()}\")","c245e9ab":"# Now, the most interesting part - the text vectoring - comes in\n# We need a sentence length parameter for FastText model training,\n# so let's find its average across the dataset\ntitle_avg_len = np.mean([len(t.split()) for t in data_train.title])\nauthor_avg_len = np.mean([len(a.split()) for a in data_train.author])\nsub_avg_len = np.mean([len(s.split()) for s in data_train.subtitle])\n\ntotal_avg_len = round(np.mean([title_avg_len, author_avg_len, sub_avg_len]))\n\nprint(f'Average sentence length is {total_avg_len}')","02595331":"# Saving our preprocessed texts to file that will be fed as input to\n# FastText\nwith open('.\/ft_train_corpus.txt', 'w+') as f:\n    f.writelines(data_train.title + data_train.author + data_train.subtitle)","58c90a80":"# FastText is Facebook's library to create word embeddings\n# Its advantage that it works not with words, but symbols and\n# n-grams, so it can easily vectorize words that were not present\n# in training data\n# FastText's docs are available at https:\/\/fasttext.cc\/docs\/en\/python-module.html#word-representation-model\n\n# Here we train a vectorizer which can transform words and sentences to vectors of 60 elements\n# We can vary the vector size, but 60 was used in several popular FastText guides :)\nvector_dim = 60\n\nft_model = ft.train_unsupervised('.\/ft_train_corpus.txt', model='skipgram', dim=vector_dim, ws=round(total_avg_len \/ 2),\n                                   epoch=10, minCount=5, t=1e-2)","5ba489d4":"# Function that vectorizes sentences from dataframe\n# and returns a dataframe which has 60 columns corresponding to vector dimensions\n# and 1 column for target variable\ndef vectorize_text_df(old_df, vector_dim):\n    vectorized_dfs = []\n\n    for col in ['title', 'author', 'subtitle']:\n        print(f'Processing {col}')\n        vectorized_dfs.append(pd.DataFrame(columns=['el' + str(j) for j in range(vector_dim)]))\n\n        for i in range(len(old_df)):\n            if i % 10000 == 0:\n                print(f'{i}\/{len(old_df)} ready')\n            next_sent_vector = pd.DataFrame(ft_model.get_sentence_vector(old_df[col].iloc[i]).reshape(1, -1),\n                                            columns=vectorized_dfs[-1].columns)\n            vectorized_dfs[-1] = vectorized_dfs[-1].append(next_sent_vector, ignore_index=True)\n\n        vectorized_dfs[-1]['claps'] = old_df.claps\n\n    return vectorized_dfs","9c348bb5":"# Vectoring text columns of train data; each column goes to its own regressor,\n# so we create 3 separate DFs\n[data_train_title, data_train_author, data_train_subtitle] = vectorize_text_df(data_train, vector_dim)","2e7fd4ce":"# Example of vectorized column\ndata_train_title","63442316":"# Numerical columns are kept in the 4th DF for 4th regressor\ndata_train_numbers = data_train[['tag', 'date_ms', 'prev_mon_topic_rate', 'claps']]","ac6d1c37":"data_train_numbers","4d8b9541":"# Vectoring text columns of validation data\n[data_val_title, data_val_author, data_val_subtitle] = vectorize_text_df(data_val, vector_dim)","3321681a":"data_val_numbers = data_val[['tag', 'date_ms', 'prev_mon_topic_rate', 'claps']]","52782a50":"# Vectoring text columns of test data\n[data_test_title, data_test_author, data_test_subtitle] = vectorize_text_df(data_test, vector_dim)","b1da8d23":"data_test_numbers = data_test[['tag', 'date_ms', 'prev_mon_topic_rate', 'claps']]","1fa91750":"# Function to train 4 CatBoost-regressors: each for three vectorized columns and three numerical features\ndef train_single_cbr(data_train, data_val, data_test, col_name):\n    cbr = CatBoostRegressor(loss_function='MAE', eval_metric='MAE', random_state=42, early_stopping_rounds=50,\n                             use_best_model=True, verbose=0, task_type='GPU')\n    cbr.fit(data_train.drop(columns=['claps']), data_train.claps,\n            eval_set=(data_val.drop(columns=['claps']), data_val.claps))\n    Y_pred = cbr.predict(data_test.drop(columns=['claps']))\n    print(f'Performance for {col_name}:')\n    print(f'MAE={round(mean_absolute_error(data_test.claps, Y_pred), 4)}')\n    \n    return cbr","e44137d2":"# Training our models\ncbr_title = train_single_cbr(data_train_title, data_val_title, data_test_title, 'Title')\ncbr_author = train_single_cbr(data_train_author, data_val_author, data_test_author, 'Author')\ncbr_subtitle = train_single_cbr(data_train_subtitle, data_val_subtitle, data_test_subtitle, 'Subtitle')\ncbr_numbers = train_single_cbr(data_train_numbers, data_val_numbers, data_test_numbers, 'Numbers')","1f48106c":"# Simple random search for optimal weights of regressors in final prediction;\n# generating 4 random coefficients that give 1 in sum and finding a combo which gives minimal MAE\nbest_coefs = []\nbest_mae = np.inf\n\npreds = [cbr_title.predict(data_test_title.drop(columns=['claps'])),\n         cbr_author.predict(data_test_author.drop(columns=['claps'])),\n         cbr_subtitle.predict(data_test_subtitle.drop(columns=['claps'])),\n         cbr_numbers.predict(data_test_numbers.drop(columns=['claps']))]\n\nrand.seed(42)  # making coefficient generation more deterministic\n\n# We can perfoem many experiments as the predictions were generated only once\nfor i in range(100000):\n    if i % 10000 == 0:\n        print(f'Test {i}')\n    \n    a = rand.uniform(0, 1)\n    b = rand.uniform(0, 1 - a)\n    c = rand.uniform(0, 1 - a - b)\n    d = 1 - a - b - c\n    \n    cur_weighted_pred = a * preds[0] + b * preds[1] + c * preds[2] + d * preds[3]\n    cur_mae = mean_absolute_error(data_test.claps, cur_weighted_pred)\n    \n    if cur_mae < best_mae:\n        print(f\"MAE for weights [{a, b, c, d}] improved to {cur_mae}\")\n        best_mae = cur_mae\n        best_coefs = [a, b, c, d]\n        \nprint('************')\nprint(f'Minimal MAE={best_mae} is achieved with weights={best_coefs}')","218058bd":"MAE of 62-64 for single model and 61 for ensemble looks like a good result as the target variable from test set ranges from 0 to 7600, and MAE is less than 1% from the range. Model ensembling doesn't improve the score much, but it does, and we can fine-tune our models if we wish.\n\nThe next notebook for this problem will (hopefully :)) be about using Keras RNNs for NLP regression problems.\n\n**Thank you for your attention! Feel free to leave your suggestions and questions!**","f1ee8d7d":"**Greetings!**\n\nToday we'll try to find out the amount of claps ('likes') that the data science-related article can get according to text of its title and author. It is a regression problem that can be solved either by neural networks, as it's NLP-related, or by more lightweight machile learning (e.g. CatBoost).\n\nIn this notebook the second version is considered. The text is transformed into vectors using FastText embedding library, and the regression model is built as an ensemble of four CatBoost models. Let's go solving!"}}