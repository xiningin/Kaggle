{"cell_type":{"ac0dde59":"code","e5143763":"code","43cf0b4d":"code","a3fd4e7b":"code","b74d6796":"code","17e075a6":"code","c6ce73eb":"code","61c4302f":"code","356b6b1e":"code","f6e0e311":"code","4cb74179":"code","206f7646":"code","accb3ba6":"code","92dbabd4":"code","2c050748":"code","1c5659fd":"code","937efa54":"code","a95fc37b":"markdown","057c29f3":"markdown","30b004a8":"markdown","f052de81":"markdown","a72c6f34":"markdown","a5436ff1":"markdown","8d5eed75":"markdown","99607c2b":"markdown","64fcaad1":"markdown","5b8d135d":"markdown","a12bc1e1":"markdown","d17a6d5a":"markdown"},"source":{"ac0dde59":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.layers import Dense, LeakyReLU, Input, Dropout\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom tqdm.notebook import tqdm","e5143763":"data_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","43cf0b4d":"train_df, validate_df = train_test_split(data_df, random_state=22, shuffle=True, test_size=0.1)\nprint(train_df.shape)\nprint(validate_df.shape)","a3fd4e7b":"x_train = train_df['text'].values.reshape(-1)\ny_train = train_df['target'].values.reshape(-1, 1)\n\nx_validate = validate_df['text'].values.reshape(-1)\ny_validate = validate_df['target'].values.reshape(-1, 1)","b74d6796":"use_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\",\n                           input_shape=[],\n                           dtype=tf.string,\n                           trainable=True)","17e075a6":"use_layer([\"hello\"])[:100]","c6ce73eb":"def create_model():\n    \n    use_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\",\n                           input_shape=[],\n                           dtype=tf.string,\n                           trainable=True)\n\n    model = tf.keras.models.Sequential([\n    use_layer,\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n    ])\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.00001),\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.25),\n                  metrics=['accuracy'])\n    \n    return model","61c4302f":"kf = KFold(n_splits=5, random_state=22, shuffle=True)\nresults = []\n\nfor train_index, val_index in tqdm(kf.split(data_df)):\n    fold_index = len(results)\n    train_df = data_df.iloc[train_index]\n    val_df = data_df.iloc[val_index]\n\n    x_train = train_df['text'].values.reshape(-1)\n    y_train = train_df['target'].values.reshape(-1, 1)\n\n    x_validate = val_df['text'].values.reshape(-1)\n    y_validate = val_df['target'].values.reshape(-1, 1)\n    \n    reduce_lr = ReduceLROnPlateau(factor=0.5, patience=2,  verbose=0, min_lr=1e-10)\n    model_checkpoint = ModelCheckpoint(f\"model_{fold_index}.h5\", save_weights_only=True, patience=1, save_best_only=True, verbose=0)\n    early_stopping = EarlyStopping(patience=5, verbose=0)\n    \n    model = create_model()\n    baseline_history = model.fit(x_train, y_train,\n                             validation_data=(x_validate, y_validate),\n                             epochs=50,\n                             batch_size=32,\n                                 verbose=0,\n                             callbacks=[reduce_lr, model_checkpoint, early_stopping])\n    \n\n    model.load_weights(f\"model_{fold_index}.h5\")\n    \n    ev_data = model.evaluate(x_validate, y_validate, verbose=0)\n\n    results.append({\n        \"fold\": fold_index,\n        \"Accuracy\": ev_data[1],\n        \"Loss\": ev_data[0],\n        \"History\": baseline_history.history\n    })\n    \n    print(f\"Fold {fold_index}\")\n    print(f\"Loss: {ev_data[0]}\")\n    print(f\"Accuracy: {ev_data[1]}\")    \n    print(\"=\"*40)","356b6b1e":"best_fold = max(results, key = lambda item: item['Accuracy'])","f6e0e311":"fig = plt.figure(figsize=(14, 6))\nfig.patch.set_facecolor('white')\n\ncolors = ['r', 'b', 'g', 'o', 'y']\n\nplt.plot(best_fold['History']['loss'],\n        label=\"train model\", ls='-', c='r')\nplt.plot(best_fold['History']['val_loss'],\n        label=\"validation model\",ls='--', c='b')\n\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.ylim(top=0.8)\nplt.xlim(right=10)\nplt.title(\"Loss\")\nplt.legend(loc='upper right')\nplt.show()","4cb74179":"fig = plt.figure(figsize=(14, 6))\nfig.patch.set_facecolor('white')\n\ncolors = ['r', 'b', 'g', 'o', 'y']\n\nplt.plot(best_fold['History']['accuracy'],\n        label=\"train model\", ls='-', c='r')\nplt.plot(best_fold['History']['val_accuracy'],\n        label=\"validation model\",ls='--', c='b')\n\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.ylim(top=1)\nplt.xlim(right=10)\nplt.title(\"Accuracy\")\nplt.legend(loc='upper right')\nplt.show()","206f7646":"model = create_model()\nmodel.load_weights(f\"model_{best_fold['fold']}.h5\")","accb3ba6":"predictions = model.predict(test_df['text'].values)","92dbabd4":"predictions[:10]","2c050748":"submission_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission_df['target'] = np.where(predictions > 0, 1, 0)\nsubmission_df.to_csv(\"submission.csv\", index=False)","1c5659fd":"import os","937efa54":"for fold_index in range(5):\n    os.remove(f\"model_{fold_index}.h5\")","a95fc37b":"### Read train and test data","057c29f3":"### Submit\nGenerate submission __csv__ file","30b004a8":"### PS\nDelete training weights","f052de81":"You can see that we defined input as list of strings. They already decorated tokenize process inside `TFHub`.\nSo, how to get embedding for our text? Call with array of strings and result is our embeddings for each item\n\n> use_layer([\"hello\", \"who r u?\"])\n","a72c6f34":"We set up `from_logits=True`, our prediction isn't in range [0;1]","a5436ff1":"Restore __best__ weights","8d5eed75":"### Select the model\n\nLet's get best fold by *Accuracy*","99607c2b":"### Model\nDefine simple model with Keras Sequential API","64fcaad1":"### Plot history learning information","5b8d135d":"## Preprocessing\n\nDo nothing, we're going to use raw text","a12bc1e1":"### Universal Sentence Encoder\nPretty interesting to know, how `Universal Sentence Encoder` will perform with this task?\n\n`Universal Sentence Encoder` - sentence embedding model trained on sentences, phrases or short paragraphs.\n\nWith `Tensorflow Hub` module we can not only use it for inference, but also easily finetune according to our source data.\nIt's really simple to use this trained model with `KerasLayer` wrapper.\n\nLet's create instance of `KerasLayer` object","d17a6d5a":"### Conclusion\n\nWith simplest model we archived realy good result. I remember, we did it without any preprocessing, no research, no feature engineering. We've only passed raw text to classifier layer through pretrained Universal Sentence Encoder with simplest model and results are greats!"}}