{"cell_type":{"dc92abdb":"code","2bcdceb3":"code","b73b4634":"code","ee3df88e":"code","a62ab89b":"code","7d29df69":"code","7456eb34":"code","85db5b78":"code","bbec80b4":"code","132a9747":"code","c6494abc":"code","ee7e2466":"code","2c04d8a1":"code","4cab6d47":"code","6349f0aa":"code","370e9760":"code","ad5a273f":"code","6e8ba993":"code","e55dde81":"code","7e445db4":"code","c867c7ca":"code","6997e641":"code","172411ca":"code","b68e236f":"code","307f3fac":"code","fa32a8a0":"code","1a881849":"code","3a3e69b2":"code","6601dca9":"code","6d4ae844":"code","9163189c":"code","64019277":"code","fe36d54a":"code","0c594cf1":"code","bb878a5d":"code","45fec0af":"code","1493cb9e":"code","d8aec5da":"code","fba121f9":"code","74e379b5":"code","0a267cf6":"code","25155bfa":"code","d668a963":"code","2b9db7e1":"code","90e74896":"code","c1e6bdb4":"code","3ac35679":"code","ea07b1f0":"code","67a92fcf":"code","1e9a8356":"code","4295ec89":"code","fad88bf3":"code","27974d40":"code","0b63f370":"code","2c9a4687":"code","11d856c4":"code","c15e77ae":"code","a35b95b0":"code","c602b1b9":"code","81fe0546":"code","f63f5fe5":"code","74173999":"code","a96ac5d7":"code","58c64d32":"code","acd5067f":"code","81437911":"code","1b83470d":"code","71dd1112":"code","597b4c8e":"code","626f4a51":"code","216121a9":"code","75c57fa3":"code","193a8bc2":"code","44615015":"code","565732b3":"code","7eb8378a":"code","8cc612b5":"code","9115af83":"code","695ed867":"code","24c1a48e":"code","9db0adf1":"code","6f9f6f03":"code","3bb738d2":"code","f3569ec0":"markdown","4864c27f":"markdown","4ab6dabe":"markdown","e10a4307":"markdown","a0db7383":"markdown","9cc07725":"markdown","62fc0b88":"markdown","806b8842":"markdown","cefad057":"markdown","84f3c5ff":"markdown","ccb12e55":"markdown","5b6837de":"markdown","6856dec0":"markdown","2e515b9d":"markdown","1b97cc18":"markdown","801afa6a":"markdown","67733fa3":"markdown","5b8403d5":"markdown","f8014501":"markdown","d81a7e69":"markdown","5c366632":"markdown","7ad91a28":"markdown","564ad8ec":"markdown","87af8161":"markdown","1abf28c9":"markdown","1effc84a":"markdown","6d3d68c9":"markdown"},"source":{"dc92abdb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2bcdceb3":"# A dependency of the preprocessing for BERT inputs\n!pip install -q tensorflow-text","b73b4634":"!pip install -q tf-models-official","ee3df88e":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport tensorflow.keras.backend as K\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import *\n\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)\n\nimport shutil\n\nfrom official.nlp import optimization  # to create AdamW optmizer\n\ntf.get_logger().setLevel('ERROR')","a62ab89b":"PATH = '..\/input\/hate-speech-and-offensive-language-dataset\/'\ndf = pd.read_csv(PATH+'labeled_data.csv')","7d29df69":"nRowsRead = None # specify 'None' if want to read whole file\n# labeled_data.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndf0 = pd.read_csv('..\/input\/hate-speech-and-offensive-language-dataset\/labeled_data.csv', delimiter=',', nrows = nRowsRead)\ndf0.dataframeName = 'labeled_data.csv'\nnRow, nCol = df0.shape\nprint('There are {} rows and {} columns'.format(nRow, nCol))","7456eb34":"df0.head(5)","85db5b78":"#Doing some adjustments\n\nc=df0['class']\ndf0.rename(columns={'tweet' : 'text',\n                   'class' : 'category'}, \n                    inplace=True)\na=df0['text']\nb=df0['category'].map({0: 'hate_speech', 1: 'offensive_language',2: 'neither'})\n\ndf= pd.concat([a,b,c], axis=1)\ndf.rename(columns={'class' : 'label'}, \n                    inplace=True)\ndf","bbec80b4":"# Grouping data by label\ndf.groupby('label').count()","132a9747":"hate, ofensive, neither = np.bincount(df['label'])\ntotal = hate + ofensive + neither\nprint('Examples:\\n    Total: {}\\n    hate: {} ({:.2f}% of total)\\n'.format(\n    total, hate, 100 * hate \/ total))\nprint('Examples:\\n    Total: {}\\n    Ofensive: {} ({:.2f}% of total)\\n'.format(\n    total, ofensive, 100 * ofensive \/ total))\nprint('Examples:\\n    Total: {}\\n    Neither: {} ({:.2f}% of total)\\n'.format(\n    total, neither, 100 * neither \/ total))\n","c6494abc":"X_train_, X_test, y_train_, y_test = train_test_split(\n    df.index.values,\n    df.label.values,\n    test_size=0.10,\n    random_state=42,\n    stratify=df.label.values,    \n)","ee7e2466":"X_train, X_val, y_train, y_val = train_test_split(\n    df.loc[X_train_].index.values,\n    df.loc[X_train_].label.values,\n    test_size=0.10,\n    random_state=42,\n    stratify=df.loc[X_train_].label.values,  \n)","2c04d8a1":"df['data_type'] = ['not_set']*df.shape[0]\ndf.loc[X_train, 'data_type'] = 'train'\ndf.loc[X_val, 'data_type'] = 'val'\ndf.loc[X_test, 'data_type'] = 'test'","4cab6d47":"df.groupby(['category', 'label', 'data_type']).count()","6349f0aa":"df","370e9760":"df_train = df.loc[df[\"data_type\"]==\"train\"]\ndf_train.head(5)","ad5a273f":"df_val = df.loc[df[\"data_type\"]==\"val\"]\ndf_val.head(5)","6e8ba993":"df_test = df.loc[df[\"data_type\"]==\"test\"]\ndf_test.head(5)","e55dde81":"df.dtypes","7e445db4":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\nstopwords.add(\"RT\")\n\nprint(type(STOPWORDS))\n\nimport random\n\ndef random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n    h = 344\n    s = int(100.0 * 255.0 \/ 255.0)\n    l = int(100.0 * float(random_state.randint(60, 120)) \/ 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=60, \n                          random_state=42\n                         ).generate(str(df.loc[df[\"category\"]==\"offensive_language\"].text))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),\n           interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","c867c7ca":"\n\ndef random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n    h = 20\n    s = int(100.0 * 255.0 \/ 255.0)\n    l = int(100.0 * float(random_state.randint(60, 120)) \/ 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=60, \n                          random_state=42\n                         ).generate(str((df.loc[df[\"category\"]==\"neither\"].text)))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),\n           interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n","6997e641":"stopwords.add(\"Name\")\n\ndef random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n    h = 180\n    s = int(100.0 * 255.0 \/ 255.0)\n    l = int(100.0 * float(random_state.randint(60, 120)) \/ 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=60, \n                          random_state=42\n                         ).generate(str((df.loc[df[\"category\"]==\"hate_speech\"].text)))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),\n           interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n","172411ca":"train_ds = tf.data.Dataset.from_tensor_slices((df_train.text.values, df_train.label.values))\nval_ds = tf.data.Dataset.from_tensor_slices((df_val.text.values, df_val.label.values))\ntest_ds = tf.data.Dataset.from_tensor_slices((df_test.text.values, df_test.label.values))","b68e236f":"train_ds","307f3fac":"train_ds = train_ds.shuffle(len(df_train)).batch(32, drop_remainder=False)\ntrain_ds","fa32a8a0":"val_ds = val_ds.shuffle(len(df_val)).batch(32, drop_remainder=False)\nval_ds","1a881849":"test_ds = test_ds.shuffle(len(df_test)).batch(32, drop_remainder=False)\ntest_ds","3a3e69b2":"for feat, targ in train_ds.take(1):\n  print ('Features: {}, Target: {}'.format(feat, targ))","6601dca9":"bert_model_name = 'small_bert\/bert_en_uncased_L-4_H-512_A-8' \n#bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_L-12_H-768_A-12\/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_L-12_H-768_A-12\/3',\n    'small_bert\/bert_en_uncased_L-2_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-2_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-2_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-2_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-4_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-4_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-4_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-4_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-6_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-6_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-6_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-6_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-8_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-8_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-8_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-8_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-10_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-10_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-10_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-10_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-12_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-12_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-12_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-768_A-12\/1',\n    'albert_en_base':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_base\/2',\n    'electra_small':\n        'https:\/\/tfhub.dev\/google\/electra_small\/2',\n    'electra_base':\n        'https:\/\/tfhub.dev\/google\/electra_base\/2',\n    'experts_pubmed':\n        'https:\/\/tfhub.dev\/google\/experts\/bert\/pubmed\/2',\n    'experts_wiki_books':\n        'https:\/\/tfhub.dev\/google\/experts\/bert\/wiki_books\/2',\n    'talking-heads_base':\n        'https:\/\/tfhub.dev\/tensorflow\/talkheads_ggelu_bert_en_base\/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-2_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-2_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-2_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-2_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-4_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-4_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-4_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-4_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-6_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-6_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-6_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-6_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-8_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-8_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-8_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-8_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-10_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-10_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-10_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-10_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-12_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-12_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-12_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'small_bert\/bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_preprocess\/1',\n    'albert_en_base':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_preprocess\/1',\n    'electra_small':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'electra_base':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'experts_pubmed':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'experts_wiki_books':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n    'talking-heads_base':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/1',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n","6d4ae844":"bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)","9163189c":"for text_batch, label_batch in train_ds.take(1):\n  for i in range(1):\n    tweet = text_batch.numpy()[i]\n    print(f'Tweet: {text_batch.numpy()[i]}')\n    label = label_batch.numpy()[i]\n    print(f'Label : {label}')\n\ntext_test = ['this is such an amazing movie!']\ntext_test = [tweet]\n\n\ntext_preprocessed = bert_preprocess_model(text_test)\n\nprint(f'Keys       : {list(text_preprocessed.keys())}')\nprint(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\nprint(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\nprint(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\nprint(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')\n","64019277":"bert_model = hub.KerasLayer(tfhub_handle_encoder)","fe36d54a":"bert_results = bert_model(text_preprocessed)\n\nprint(f'Loaded BERT: {tfhub_handle_encoder}')\nprint(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\nprint(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\nprint(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\nprint(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')","0c594cf1":"weight_for_0 = (1 \/ hate)*(total)\/3.0 \nweight_for_1 = (1 \/ ofensive)*(total)\/3.0\nweight_for_2 = (1 \/ neither)*(total)\/3.0\n\n\nclass_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))\nprint('Weight for class 2: {:.2f}'.format(weight_for_2))","bb878a5d":"#initial_output_bias = np.array([3.938462, 6.535164, 5.])\ninitial_output_bias = np.array([3.938462, 15, 5.])\ninitial_output_bias ","45fec0af":"def build_classifier_model(output_bias=None):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n        #print(output_bias)\n        \n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dense(512, activation=\"relu\")(net)\n    net = tf.keras.layers.Dropout(0.2)(net)\n#   net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n    net = tf.keras.layers.Dense(3, activation=\"softmax\", name='classifier', bias_initializer=output_bias)(net)\n    \n    return tf.keras.Model(text_input, net)","1493cb9e":"classifier_model = build_classifier_model(output_bias=initial_output_bias)\nbert_raw_result = classifier_model(tf.constant(text_test))\nprint(tf.sigmoid(bert_raw_result))","d8aec5da":"classifier_model.get_weights()[-1]","fba121f9":"classifier_model.summary()","74e379b5":"tf.keras.utils.plot_model(classifier_model)","0a267cf6":"loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n#metrics = tf.metrics.Accuracy()","25155bfa":"epochs = 80\nsteps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')","d668a963":"#  classifier_model.compile(optimizer=optimizer,\n#                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n#                           metrics=['accuracy'])\nclassifier_model.compile(optimizer=optimizer,\n                         loss=loss,\n                         metrics=metrics)","2b9db7e1":"print(f'Training model with {tfhub_handle_encoder}')\nhistory = classifier_model.fit(x=train_ds,\n                               validation_data=val_ds,\n                               epochs=epochs,\n                               # The class weights go here\n                               class_weight=class_weight\n)","90e74896":"loss, accuracy = classifier_model.evaluate(test_ds)\n\nprint(f'Loss: {loss}')\nprint(f'Accuracy: {accuracy}')","c1e6bdb4":"history_dict = history.history\nprint(history_dict.keys())\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n# acc = history_dict['binary_accuracy']\n# val_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(12, 10))\nfig.tight_layout()\n\nplt.subplot(2, 1, 1)\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'r', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\n# plt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","3ac35679":"dataset_name = 'mpl_hate_speech'\nsaved_model_path = '.\/{}_bert'.format(dataset_name.replace('\/', '_'))\n\nclassifier_model.save(saved_model_path, include_optimizer=False)","ea07b1f0":"result =  classifier_model.predict(test_ds)\nprint(result.shape)","67a92fcf":"result[0:2]","1e9a8356":"classes = np.argmax(result, axis=-1)","4295ec89":"tweet = []\ntest_labels = []\npredictions = []\nfor tweet, labels in test_ds.take(-1):\n  tweet = tweet.numpy()\n  test_labels.append(labels.numpy())\n  predictions.append(classifier_model.predict(tweet))","fad88bf3":"test_labels[0:2]","27974d40":"predictions[0:2]","0b63f370":"from itertools import chain\nflatten_list = list(chain.from_iterable(predictions))\ny_pred = np.argmax(flatten_list, axis=-1)","2c9a4687":"y_test = np.array(list(chain.from_iterable(test_labels)))","11d856c4":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred)","c15e77ae":"# random_classifier_model = build_classifier_model()\n# bert_raw_result = random_classifier_model(tf.constant(text_test))\n# print(tf.sigmoid(bert_raw_result))\n\n# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n# metrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n# #metrics = tf.metrics.Accuracy()\n\n# epochs = 5\n# steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n# num_train_steps = steps_per_epoch * epochs\n# num_warmup_steps = int(0.1*num_train_steps)\n\n# init_lr = 3e-5\n# optimizer = optimization.create_optimizer(init_lr=init_lr,\n#                                           num_train_steps=num_train_steps,\n#                                           num_warmup_steps=num_warmup_steps,\n#                                           optimizer_type='adamw')\n\n# random_classifier_model.compile(optimizer=optimizer,\n#                          loss=loss,\n#                          metrics=metrics)","a35b95b0":"# print(f'Training model with {tfhub_handle_encoder}')\n# random_history = random_classifier_model.fit(x=train_ds,\n#                                validation_data=val_ds,\n#                                epochs=epochs,\n#                                # The class weights go here\n#                                class_weight=class_weight\n# )","c602b1b9":"# history_dict = random_history.history\n# print(history_dict.keys())\n\n# acc = history_dict['accuracy']\n# val_acc = history_dict['val_accuracy']\n# # acc = history_dict['binary_accuracy']\n# # val_acc = history_dict['val_binary_accuracy']\n# loss = history_dict['loss']\n# val_loss = history_dict['val_loss']\n\n# epochs = range(1, len(acc) + 1)\n# fig = plt.figure(figsize=(10, 6))\n# fig.tight_layout()\n\n# plt.subplot(2, 1, 1)\n# # \"bo\" is for \"blue dot\"\n# plt.plot(epochs, loss, 'r', label='Training loss')\n# # b is for \"solid blue line\"\n# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n# plt.title('Training and validation loss')\n# # plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend()\n\n# plt.subplot(2, 1, 2)\n# plt.plot(epochs, acc, 'r', label='Training acc')\n# plt.plot(epochs, val_acc, 'b', label='Validation acc')\n# plt.title('Training and validation accuracy')\n# plt.xlabel('Epochs')\n# plt.ylabel('Accuracy')\n# plt.legend(loc='lower right')","81fe0546":"def build_CNN_classifier_model():\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    #net = outputs['pooled_output'] # [batch_size, 768].\n    net = sequence_output = outputs[\"sequence_output\"] # [batch_size, seq_length, 768]\n      \n    \n    net = tf.keras.layers.Conv1D(32, (2), activation='relu')(net)\n    #net = tf.keras.layers.MaxPooling1D(2)(net)\n    \n    net = tf.keras.layers.Conv1D(64, (2), activation='relu')(net)\n    #net = tf.keras.layers.MaxPooling1D(2)(net)\n    net = tf.keras.layers.GlobalMaxPool1D()(net)\n    \n#    net = tf.keras.layers.Flatten()(net)\n    \n    net = tf.keras.layers.Dense(512, activation=\"relu\")(net)\n    \n    net = tf.keras.layers.Dropout(0.1)(net)\n#   net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n    net = tf.keras.layers.Dense(3, activation=\"softmax\", name='classifier')(net)\n    \n    return tf.keras.Model(text_input, net)","f63f5fe5":"cnn_classifier_model = build_CNN_classifier_model()\nbert_raw_result = cnn_classifier_model(tf.constant(text_test))\nprint(tf.sigmoid(bert_raw_result))","74173999":"text_test","a96ac5d7":"cnn_classifier_model.summary()","58c64d32":"tf.keras.utils.plot_model(cnn_classifier_model)","acd5067f":"loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n#metrics = tf.metrics.CategoricalCrossentropy()\n#metrics = tf.metrics.Accuracy()","81437911":"epochs = 80\nsteps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')\n\ncnn_classifier_model.compile(optimizer=optimizer,\n                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                          metrics=tf.keras.metrics.SparseCategoricalAccuracy('accuracy'))","1b83470d":"print(f'Training model with {tfhub_handle_encoder}')\ncnn_history = cnn_classifier_model.fit(x=train_ds,\n                                       validation_data=val_ds,\n                                       epochs=epochs,\n                                       class_weight=class_weight\n                                      )","71dd1112":"loss, accuracy = cnn_classifier_model.evaluate(test_ds)\n\nprint(f'Loss: {loss}')\nprint(f'Accuracy: {accuracy}')","597b4c8e":"history_dict = cnn_history.history\nprint(history_dict.keys())\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n# acc = history_dict['binary_accuracy']\n# val_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(12, 10))\nfig.tight_layout()\n\nplt.subplot(2, 1, 1)\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'r', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\n# plt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","626f4a51":"dataset_name = 'cnn_hate_speech'\nsaved_model_path = '.\/{}_bert'.format(dataset_name.replace('\/', '_'))\n\ncnn_classifier_model.save(saved_model_path, include_optimizer=False)","216121a9":"reloaded_model = tf.saved_model.load(saved_model_path)","75c57fa3":"result =  cnn_classifier_model.predict(test_ds)\nprint(result.shape)","193a8bc2":"result[0:2]","44615015":"# for tweet, classes in test_ds:\n#     for i in classes:\n#         print(i)","565732b3":"classes = np.argmax(result, axis=-1)","7eb8378a":"tweet = []\ntest_labels = []\npredictions = []\nfor tweet, labels in test_ds.take(-1):\n  tweet = tweet.numpy()\n  test_labels.append(labels.numpy())\n  predictions.append(cnn_classifier_model.predict(tweet))","8cc612b5":"test_labels[0:2]","9115af83":"predictions[0:2]","695ed867":"flatten_list = list(chain.from_iterable(predictions))\ny_pred = np.argmax(flatten_list, axis=-1)","24c1a48e":"type(y_pred)","9db0adf1":"y_test = np.array(list(chain.from_iterable(test_labels)))","6f9f6f03":"type(y_test)","3bb738d2":"confusion_matrix(y_test, y_pred)","f3569ec0":"# Results for MLP","4864c27f":"### Calculate class weights\n\nOne of the goals is to identify hate speech, but we don't have very many of those samples to work with, so I would want to have the classifier heavily weight the few examples that are available. I am going to do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class.","4ab6dabe":"### The preprocessing model\n\nText inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models, which implements this transformation using TF ops from the TF.text library. Hence, It is not necessary to run pure Python code outside the TensorFlow model to preprocess text.\n\nThe preprocessing model must be the one referenced by the documentation of the BERT model, which can be read at the URL printed above. For BERT models from the drop-down above, the preprocessing model is selected automatically.","e10a4307":"# BERT + MLP\n\nI am going to create a simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.","a0db7383":"### Splitting the data between train, validation and test sets:","9cc07725":"# BERT + CNN","62fc0b88":"### Doing predictions and saving to np.array","806b8842":"## Test with Random Initialization of softmax bias.","cefad057":"This is an unbalanced dataset. ","84f3c5ff":"I've chosen \"bert_en_uncased_L-12_H-768_A-12\"\n\nThis TF Hub model uses the implementation of BERT from the TensorFlow Models repository on GitHub at tensorflow\/models\/official\/nlp\/bert. It uses L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768, and A=12 attention heads.","ccb12e55":"### Doing predictions and saving to np.array","5b6837de":"# Techniques to deal with unbalanced data","6856dec0":"# Implementation of text classification with BERT \n\nStill Working on it.\n\nThis notebook is based in this TensorFlow tutorial: [Classify text with BERT](https:\/\/www.tensorflow.org\/tutorials\/text\/classify_text_with_bert)\n\nBERT [(article link)](https:\/\/arxiv.org\/abs\/1810.04805) and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.\n\n![](http:\/\/www.d2l.ai\/_images\/nlp-map-pretrain.svg)\n\nSource: http:\/\/www.d2l.ai\/chapter_natural-language-processing-pretraining\/index.html\n\nBERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n\nIn this notebook, I am going to use a pretreined BERT to compute vector-space representations of a hate speech dataset to feed two different downsteam Archtectures (CNN and MLP).\n\nSentiment Analysis\n\nThis notebook trains a sentiment analysis model to classify the [Hate Speech and Offensive Language Dataset]( https:\/\/www.kaggle.com\/mrmorj\/hate-speech-and-offensive-language-dataset) tweets in three classes:\n \n* 0 - hate speech \n* 1 - offensive language \n* 2 - neither as positive or negative","2e515b9d":"# Loading models from TensorFlow Hub","1b97cc18":"## Export for inference\n\nNow you just save your fine-tuned model for later use.","801afa6a":"### Set the correct initial bias\n\nThese initial guesses (for the bias) are not great. The dataset is imbalanced. Set the output layer's bias to reflect that (See: [A Recipe for Training Neural Networks: \"init well\"](http:\/\/karpathy.github.io\/2019\/04\/25\/recipe\/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines)). This can help with initial convergence.\n\nWith the default bias initialization the loss should be about log(1\/n_classes): math.log(3) = 1,098612","67733fa3":"## Build TensorFlow input \n[Reference](https:\/\/www.tensorflow.org\/guide\/data)","5b8403d5":"While tf.data tries to propagate shape information, the default settings of Dataset.batch result in an unknown batch size because the last batch may not be full. Note the Nones in the shape:\n\nbatched_dataset\n```\n<BatchDataset shapes: ((None,), (None,)), types: (tf.int64, tf.int64)>\n```\nUse the drop_remainder argument to ignore that last batch, and get full shape propagation:","f8014501":"# Results for CNN","d81a7e69":"## Installing dependencies and importing packages","5c366632":"# Confusion Matrix CNN","7ad91a28":"For the implementation with CNN, I am using the sequence_output as input to the convolutional layer. It represents each input token in the context. The shape is [batch_size, seq_length, H]. You can think of this as a contextual embedding for every token in the tweet. I belive that this outputs saves positional information about the inputs, then it would male cense to feed a convolutional layer.","564ad8ec":"# Confusion Matrix MLP","87af8161":"Let's try the preprocessing model on some text and see the output:","1abf28c9":"## Reading and preparing the dataset","1effc84a":"# Printing some Tweets","6d3d68c9":"## Export for inference\n\nNow you just save your fine-tuned model for later use."}}