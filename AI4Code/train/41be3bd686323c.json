{"cell_type":{"ff4f5f3a":"code","a4a53f07":"code","4dc0c3ea":"code","9365564d":"code","19f842db":"code","9c8fd505":"code","958239fa":"code","fd36e336":"code","090e2ca5":"code","0956b69f":"code","9c6d2f3d":"code","3d9c2092":"code","91b3d04f":"code","69ce3ec6":"code","8ce90b30":"code","0d722df9":"code","51e7bb49":"code","28cca093":"code","b9e7c33e":"code","4ccc7bd1":"code","c8b17972":"code","c89c5cbc":"code","de0fb72d":"code","88cc8817":"code","53a47aa4":"code","50d0afd9":"code","b2841251":"code","7a12e55f":"code","6da234e4":"code","f9d43bcc":"code","3ac0d354":"code","06b11c2a":"code","3b4f26da":"code","3f0c81c5":"code","2a2ae863":"code","0261f9e4":"code","2bc9b25c":"code","c37010a5":"code","b3900018":"code","9acbf424":"code","09e8c00a":"code","9a957c76":"code","9dc34c80":"code","8a81fc11":"code","07fbe714":"code","8a7b8951":"code","b1f74ef4":"code","c03f8d44":"code","935553ef":"code","07f36fe5":"code","a8064ac5":"code","ad3f31d4":"code","c73c92c8":"code","bc5c8bb3":"code","01940c51":"code","3a2905a3":"code","b16ac142":"code","e63ae3e3":"code","5742b3cc":"code","02e9e7be":"code","7ff544af":"code","ef114635":"code","66a68a86":"code","94eeae5f":"code","819dfc83":"code","b63ae9cf":"code","b4fc08e1":"code","a56b52a3":"markdown","af587e83":"markdown","1f5881fc":"markdown","939bd540":"markdown","05501663":"markdown","95708f89":"markdown","539eecf2":"markdown","f81cf0c1":"markdown","4268a79d":"markdown","5d71edfd":"markdown","ed9b19d8":"markdown","01334e23":"markdown","027024e7":"markdown","484d718c":"markdown","23ce055f":"markdown","8c2d04e2":"markdown","145ceb89":"markdown","e01e82e3":"markdown","579a0808":"markdown","0b598665":"markdown","18d7950c":"markdown","768b134b":"markdown","0b4c3418":"markdown","56c8fc02":"markdown","81929df9":"markdown","1c8c3c04":"markdown","3f403354":"markdown","47c4e404":"markdown","e8bc5d74":"markdown","1bc26c61":"markdown","3883c0d7":"markdown","bcf379bb":"markdown","a805cbe2":"markdown"},"source":{"ff4f5f3a":"# system related\nimport os\nimport sys\n# path manipulation\nfrom pathlib import Path\n# regex\nimport re\n# plotting\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib.patches import Circle\n# data manipulation \/ preparation\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n# image manipulation\n\nfrom PIL import Image\nfrom IPython.display import Image\n# metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\n# keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam","a4a53f07":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom matplotlib.patches import Circle\n\n\nclass MammoScan:\n    def __init__(self, scan, sc_info):\n        self.__scan = scan\n        self.__sc_info = sc_info\n    \n    @property # scan image\n    def scan(self):\n        return self.__scan\n    \n    @property # scan info\n    def scan_info(self):\n        return self.__sc_info\n    \n    @property # from the scan file name\n    def scan_name(self):\n        return self.__sc_info.name\n    \n    @property # x coordinate of the abnormality\n    def x(self):\n        return self.__sc_info.x\n    \n    @property # y coordinate of the abnormality\n    def y(self):\n        return self.__sc_info.y\n    \n    @property # radius of the abnormality\n    def radius(self):\n        return self.__sc_info.radius\n\n    @property # class of the abnormality\n    def ab_class(self):\n        return self.__sc_info.ab_class\n    \n    @property # background tissue of the abnormality\n    def bg(self):\n        return self.__sc_info.bg\n    \n    @property # severity of the abnormality\n    def severity(self):\n        return self.__sc_info.severity\n\n    @property \n    # generates cropped downsized rotated images of the abnormality\n    def transformations(self):\n        return self.__transform()\n    \n    @property\n    # returns the matricial representation fo the scan image\n    def pixel_matrix(self):\n        return np.array(self.scan)\n    \n    # instance method\n    def plot(self):\n\n        # Create a figure. Equal aspect so circles look circular\n        fig, ax = plt.subplots(1)\n\n        fig.set_size_inches(8, 6)\n        ax.set_aspect('equal')\n\n        # Show the image\n        ax.imshow(self.scan, cmap=plt.cm.gray_r)\n        ax.set_ylim(bottom=0, top=1024)\n        ax.set_title(self.scan_name)\n        \n\n        # create a circle to patch on the image\n        x, y, r = self.__get_crop_coords()\n        print(f'{x}, {y}, {r}')\n        circ = Circle((x,y), r, fill=False)\n        ax.add_patch(circ)\n    \n    # private method\n    def __set_x(self, xValue):\n        self.__sc_info.x = xValue\n    \n    # private method\n    def __set_y(self, yValue):\n        self.__sc_info.y = yValue\n    \n    # private method\n    def __set_radius(self, rValue):\n        self.__sc_info.radius = rValue\n        \n    # private method\n    def __get_crop_coords(self):\n        '''Returns a tuple with x, y and r'''\n        # check scan class to decide on how to crop\n        if pd.isnull(self.radius):\n            radius = 48.0\n            self.__set_radius(radius)\n        if pd.isnull(self.x):\n            x = float(np.random.randint(500, 513))\n            self.__set_x(x)\n        if pd.isnull(self.y):\n            y = float(np.random.randint(500, 513))\n            self.__set_y(y)\n            \n        return (self.x, 1024.0-self.y, self.radius)\n    \n    # private method\n    def __transform(self):\n        '''Creates a dict \n                  with rotated and mirrored versions of self.scan'''\n        # create dictionary\n        transformations = dict()\n        # get crop values\n        x, y, r = self.__get_crop_coords()\n        # crop and resize scan\n        cropped_scan = self.scan.crop((x-r, y-r, x+r, y+r))\n        resized_scan = cropped_scan.resize((256,256))\n        # create rotated images\n        for angle in (0, 90, 180, 270):\n            rotated = resized_scan.rotate(angle) # rotated by angle\n            mirr_tp = rotated.transpose(Image.FLIP_TOP_BOTTOM)\n            mirr_lr = rotated.transpose(Image.FLIP_LEFT_RIGHT)\n            \n            transformations[angle] = dict(zip(['rotated', 'mirr_lr', 'mirr_tp'], \n                                              [rotated, mirr_lr, mirr_tp]))\n\n        return transformations","4dc0c3ea":"def clean_ds_files(df: pd.DataFrame) -> pd.DataFrame:\n    '''Removes records with invalid data\n         and cast x and y to float'''\n    new_df = df.copy()\n    # search for invalid x values for removal\n    indices = new_df.x[lambda x: x == '*NOTE'].index\n    \n    for idx in indices:\n        n_idx = new_df.index.get_loc(idx)\n        # drop from dataset\n        new_df.drop(new_df.index[n_idx], inplace=True)\n        # delete from directory\n        delete_image(idx)\n\n    # make x and y float values\n    new_df.x = new_df.x.astype(float)\n    new_df.y = new_df.y.astype(float)\n    \n    return new_df\n\n\ndef delete_image(filename: str, directory='..\/all-mias\/'):\n    '''Deletes original image files that won't be initially used'''\n    paths = Path(directory).glob('**\/*.pgm')\n    filename += '.pgm'\n    for f_path in sorted(paths):\n        try:\n            if f_path.name == filename:\n                os.remove(f_path)\n                break\n        except FileNotFoundError as fnf:\n            print('{fnf}') \n            \n            \ndef create_scan_filenames_dic(path: str) -> dict:\n    '''Creates a dictionary with image filenames'''\n    paths = Path(path).glob('**\/*.pgm')\n    img_dic = dict()\n    for f_path in sorted(paths):\n        # get full filename\n        full_fname = f_path.name\n        # get filename (no extension)\n        filename = f_path.stem\n        # create dictionary\n        img_dic[filename] = f_path.as_posix()\n    \n    return img_dic\n        \n\ndef save_subsamples(scans_dic: dict(), df: pd.DataFrame) -> pd.DataFrame:\n    '''Save subsamples to the subsamples folder'''\n    # define subsamples folder\n    folder = '..\/subsamples'\n    df_sub = pd.DataFrame()\n    try:\n        # create if not yet\n        if not os.path.exists(folder):\n            os.mkdir(folder)\n    except:\n        print('An error occurred when searching for the folder')\n        \n    # iterate dictionary of filenames\n    for scan_name, filename in scans_dic.items():\n        \n        # create image and scan info objects\n        try:\n            scan = Image.open(filename)\n        except FileNotFoundError as fnf:\n            print({fnf})\n            \n        scan_info = df.loc[scan_name].copy()\n        # create the MammoScan object\n        m_scan = MammoScan(scan, scan_info)\n        # get the transformations\n        transf_scans = m_scan.transformations\n        # create filenames\n        filenames = create_subsample_filename(scan_name, transf_scans)\n        # get transformed scans Image objects\n        imgs = get_transformed_scans(transf_scans)\n        # prepare for saving\n        fs_and_is = list(zip(filenames, imgs))\n        \n        for filename, image in fs_and_is:\n            # create new observation with subsample name\n            # name the series to become an index in the new dataframe\n            scan_info.name = re.match(r'(.*)\\.[^.]+$', filename).group(1)\n            # create pixel matrix\n            pixel_matrix = np.asarray(image)\n            \n            scan_info['p_matrix'] = pixel_matrix\n            \n            scan_info['subsample_path'] = os.path.join('..\/subsamples', filename)\n\n            # append to dataframe\n            df_sub = df_sub.append(scan_info.loc[['ab_class', 'bg', \n                                                  'severity', 'subsample_path',\n                                                  'p_matrix']])\n            \n            #print(scan_name).\/.\/\n            path = os.path.join('..\/subsamples', filename) \n            #print(path)\n            try:\n                image.save(path, compress_level=0)\n            except ValueError as ve:\n                print('Output format could not be determined from the file name.')\n            except OSError as ose:\n                print('File could not be written.')\n                print({ose})\n        \n    return df_sub\n\n\ndef create_subsample_filename(scan_name: str, transf_dic: dict) -> list:\n    '''Creates suffix pattern filename for transformed scans'''\n    filename = ''\n    file_names = list()\n    for angle, transfs in transf_dic.items():\n        for tf in transfs.keys():\n            filename += f'{scan_name}_{angle}_{tf}.png'\n            #print(filename)\n            file_names.append(filename)\n            filename = ''\n            \n    return file_names\n\n\ndef get_transformed_scans(transf_dic: dict) -> list:\n    scans = list()\n    for angle, transfs in transf_dic.items():\n        for scan in transfs.values():\n            scans.append(scan)\n    \n    return scans\n\n\ndef generate_subsamples(path_to_originals: str, mias_df: pd.DataFrame) -> pd.DataFrame:\n    ''' Generates the subsamples for training and testing.\n        Files are saved in ..\/subsamples\n        it returns a dataframe with the path to each subsample '''\n    scans_filenames_dic = create_scan_filenames_dic(path_to_originals)\n    final = save_subsamples(scans_filenames_dic, mias_df)\n    return final\n\n\ndef balance_by_severity(df: pd.DataFrame, ab_class: str) -> pd.DataFrame:\n    ''' Balances an abnormality class based on severity '''\n    # deep copy\n    df = df.copy()\n    # if class == NORM, reduce norm to the avg class sample amount\n    if ab_class == 'NORM':\n        avg = int(df[df.ab_class != 'NORM'].groupby(['ab_class']).severity.size().mean())\n        return df[(df.ab_class == ab_class)].sample(avg)   \n    \n    sev_counts = df[df.ab_class== ab_class].severity.value_counts()\n    n_benign = sev_counts.loc['B']\n    n_malign = sev_counts.loc['M']\n    \n    if n_benign > n_malign:\n        # downsize 'B'\n        benign = df[(df.ab_class == ab_class) & (df.severity == 'B')].sample(n_malign, replace=False)\n        malign = df[(df.ab_class == ab_class) & (df.severity == 'M')]\n    else:\n        benign = df[(df.ab_class == ab_class) & (df.severity == 'B')]\n        malign = df[(df.ab_class == ab_class) & (df.severity == 'M')].sample(n_benign, replace=False)\n        \n    return pd.concat([benign, malign])\n\n\ndef create_mias_dataset(file_path: str) -> pd.DataFrame:\n    ''' Creates a dataset with the data about the scans '''\n    # create a dataset\n    mammo = pd.read_table(file_path, delimiter='\\s', engine='python')\n    # rename the class column to avoid conflicts with the class keyword in python\n    mammo.columns = ['refnum', 'bg', 'ab_class', 'severity', 'x', 'y', 'radius']\n    # fill null severity with A for NORM class\n    mammo.severity = mammo.severity.fillna('A')\n    # drop duplicates\n    mammo.drop_duplicates(subset='refnum', keep='first', inplace=True)\n    # set refnum as index\n    mammo.set_index(keys='refnum', drop=True, inplace=True)\n    # return clean df and delete unuseful images\n    return clean_ds_files(mammo)\n\ndef plot_results(acc,val_acc,loss, val_loss):\n    # create grit\n    fig, (ax1, ax2) = plt.subplots(nrows = 1,\n                                   ncols = 2,\n                                   figsize = (15,6),\n                                   sharex =True)\n    \n    # set plots\n    plot1 = ax1.plot(range(0, len(acc)),\n                     acc,\n                     label = 'accuracy')\n    \n    \n    plot2 = ax1.plot(range(0, len(val_acc)),\n                     val_acc,\n                     label = 'val_accuracy')\n\n    ax1.set(title = 'Accuracy And Val Accuracy progress',\n            xlabel = 'epoch',\n            ylabel = 'accuracy\/ validation accuracy')\n\n    ax1.legend()\n\n    plot3 = ax2.plot(range(0, len(loss)),\n                     loss,\n                     label = 'loss')\n    \n    plot4 = ax2.plot(range(0, len(val_loss)),\n                     val_loss,\n                     label = 'val_loss')\n    \n    ax2.set(title = 'Loss And Val loss progress',\n            xlabel = 'epoch',\n            ylabel = 'loss\/ validation loss')\n\n    ax2.legend()\n\n    fig.suptitle('Result Of Model', fontsize = 20, fontweight = 'bold')\n    fig.savefig('Accuracy_Loss_figure.png')\n    plt.tight_layout()\n    plt.show()\n    \n    \ndef balance_df_by_severity(df: pd.DataFrame) -> pd.DataFrame:\n    final_df = pd.DataFrame()\n    for ab_class in df.ab_class.unique():\n        class_df = balance_by_severity(df, ab_class)\n        final_df = pd.concat([class_df, final_df])\n        \n    return final_df.sample(len(final_df), replace = False)\n\n\ndef full_balance_df_by_severity(df: pd.DataFrame) -> pd.DataFrame:\n    final_df = pd.DataFrame()\n    for ab_class in df.ab_class.unique():\n        if ab_class == 'NORM':\n            class_df = balance_by_severity(df, ab_class).sample(96, replace = False)\n        else:\n            class_df = balance_by_severity(df, ab_class)\n            class_df_B = class_df[class_df.severity == 'B'].sample(48, replace = False)\n            class_df_A = class_df[class_df.severity == 'M'].sample(48, replace = False)\n            class_df = pd.concat([class_df_A, class_df_B])\n            class_df = class_df.sample(len(class_df), replace = False)\n        final_df = pd.concat([class_df, final_df])\n        \n    return final_df.sample(len(final_df), replace = False)\n\ndef display_probabilities(prediction):\n    for index, probability in enumerate(prediction):\n        print(f'{index}: {probability:.10%}')\n        \ndef create_final_results_df() -> pd.DataFrame:\n    # create dataframe for results\n    data = {\"calcifications\": [0, 0, 0, 0]}\n    return pd.DataFrame.from_dict(data, orient='index', columns=['test_accuracy', 'precision','recall', 'f1-score'])\n","9365564d":"mammo_df = create_mias_dataset(\"..\/input\/mias-mammography\/Info.txt\")\nmammo_df.head()","19f842db":"mammo = generate_subsamples(\"..\/input\/mias-mammography\/all-mias\/\",mammo_df)\nmammo.head()","9c8fd505":"mammo.info()","958239fa":"mammo.index.get_loc(\"mdb256_0_mirr_tp\")","fd36e336":"import cv2\nimport numpy as np\nsample_img2 = cv2.imread(mammo.subsample_path[3062])\ncolor = cv2.cvtColor(sample_img2, cv2.COLOR_BGR2RGB)\nplt.imshow(color)\nplt.title(mammo.index[3062])\nplt.show()","090e2ca5":"import cv2\nimport numpy as np\nsample_img2 = cv2.imread(\"..\/input\/mias-mammography\/all-mias\/mdb256.pgm\")\ncolor = cv2.cvtColor(sample_img2, cv2.COLOR_BGR2RGB)\nplt.imshow(color)\nplt.title(\"mbd256\")\nplt.show()","0956b69f":"calc_raw = mammo[mammo.ab_class == \"CALC\"]\ncalc_raw.severity.value_counts()","9c6d2f3d":"calc_bal = balance_by_severity(mammo,\"CALC\")\ncalc_bal.severity.value_counts()","3d9c2092":"y = calc_bal.sample(28,random_state=42)\nX = calc_bal.drop(y.index)","91b3d04f":"data_gen = ImageDataGenerator(validation_split=0.20, height_shift_range=0.10,width_shift_range=0.10,rotation_range=30,rescale=1\/255.)","69ce3ec6":"X_train = data_gen.flow_from_dataframe(X,x_col=\"subsample_path\",y_col=\"severity\",class_mode=\"categorical\",target_size=(256,256),subset=\"training\",color_mode=\"rgb\",shuffle=True)\nX_val = data_gen.flow_from_dataframe(X,x_col=\"subsample_path\",y_col=\"severity\",class_mode=\"categorical\",target_size=(256,256),subset=\"validation\",color_mode=\"rgb\",shuffle=False)\ny_val = data_gen.flow_from_dataframe(y,x_col=\"subsample_path\",y_col=\"severity\",class_mode=\"categorical\",target_size=(256,256),subset=\"training\",color_mode=\"rgb\",shuffle=False)\ny_test = data_gen.flow_from_dataframe(y,x_col=\"subsample_path\",y_col=\"severity\",class_mode=\"categorical\",target_size=(256,256),subset=\"validation\",color_mode=\"rgb\",shuffle=False)","8ce90b30":"from tensorflow import keras","0d722df9":"\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.applications.densenet import DenseNet121\nmodel16 = VGG16()\nmodel19 = VGG19()\nmodelDN = DenseNet121()\nmodelMNV2 = MobileNetV2()\n#model16.summary()","51e7bb49":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten","28cca093":"model_16 = VGG16(weights = \"imagenet\", include_top=False,input_shape=(256,256,3))\nmodel_16.trainable = False\nflat1 = Flatten()(model_16.layers[-1].output)\nclass1 = Dense(512,activation=\"relu\")(flat1)\n#class2 = Dense(50,activation=\"relu\")(class1)\n#class3 = Dense(1024,activation=\"relu\")(class2)\noutput = Dense(2,activation=\"softmax\")(class1)\nmodel_16 = Model(inputs=model_16.inputs, outputs=output)","b9e7c33e":"model_19 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(256,256,3))\nmodel_19.trainable = False\nflat19 = Flatten()(model_19.layers[-1].output)\nclass19 = Dense(512,activation=\"relu\")(flat19)\n#class2 = Dense(50,activation=\"relu\")(class1)\n#class3 = Dense(1024,activation=\"relu\")(class2)\noutput19 = Dense(2,activation=\"softmax\")(class19)\nmodel_19 = Model(inputs=model_19.inputs, outputs=output19)","4ccc7bd1":"modelMNV2 = MobileNetV2(weights = \"imagenet\", include_top=False,input_shape=(256,256,3))\nmodelMNV2.trainable = False\nflatMNV2 = Flatten()(modelMNV2.layers[-1].output)\nclassMNV2 = Dense(512,activation=\"relu\")(flatMNV2)\n#class2 = Dense(50,activation=\"relu\")(class1)\n#class3 = Dense(1024,activation=\"relu\")(class2)\noutputMNV2 = Dense(2,activation=\"softmax\")(classMNV2)\nmodelMNV2 = Model(inputs=modelMNV2.inputs, outputs=outputMNV2)","c8b17972":"modelDN = DenseNet121(weights = \"imagenet\", include_top=False,input_shape=(256,256,3))\nmodelDN.trainable = False\nflatDN = Flatten()(modelDN.layers[-1].output)\nclassDN = Dense(512,activation=\"relu\")(flatDN)\n#class2 = Dense(50,activation=\"relu\")(class1)\n#class3 = Dense(1024,activation=\"relu\")(class2)\noutputDN = Dense(2,activation=\"softmax\")(classDN)\nmodelDN = Model(inputs=modelDN.inputs, outputs=outputDN)","c89c5cbc":"adam_opt = Adam()\nmodel_loss = \"categorical_crossentropy\"\nearly_stopping = EarlyStopping(patience=200, restore_best_weights=True)\nmodel_check_point_16 = ModelCheckpoint(filepath= '.\/vgg16', \n                                    monitor='val_loss', verbose=0, \n                                    save_best_only=True,\n                                    save_weights_only=False, \n                                    mode='auto', save_freq='epoch')\nmodel_check_point_19 = ModelCheckpoint(filepath= '.\/vgg19', \n                                    monitor='val_loss', verbose=0, \n                                    save_best_only=True,\n                                    save_weights_only=False, \n                                    mode='auto', save_freq='epoch')\nmodel_check_point_MNV2 = ModelCheckpoint(filepath= '.\/mnv2', \n                                    monitor='val_loss', verbose=0, \n                                    save_best_only=True,\n                                    save_weights_only=False, \n                                    mode='auto', save_freq='epoch')\nmodel_check_point_DN = ModelCheckpoint(filepath= '.\/dn', \n                                    monitor='val_loss', verbose=0, \n                                    save_best_only=True,\n                                    save_weights_only=False, \n                                    mode='auto', save_freq='epoch')","de0fb72d":"model_16.compile(optimizer=adam_opt,loss=model_loss,metrics=[\"accuracy\"])\nmodel_19.compile(optimizer=adam_opt,loss=model_loss,metrics=[\"accuracy\"])\nmodelMNV2.compile(optimizer=adam_opt,loss=model_loss,metrics=[\"accuracy\"])\nmodelDN.compile(optimizer=adam_opt,loss=model_loss,metrics=[\"accuracy\"])","88cc8817":"import time","53a47aa4":"start_time = time.time()\nhistory_16 = model_16.fit(X_train,validation_data=X_val,epochs=200,callbacks=[early_stopping,model_check_point_16],verbose=2)\nend_time = time.time()\nprint(\"Training time for VGG16: \", end_time - start_time,\"secs\")","50d0afd9":"vgg16_train_time = (end_time - start_time) \/ 60","b2841251":"vgg16_train_time","7a12e55f":"vgg16_train_history = pd.DataFrame(history_16.history)\nfile_name = \"vgg_16_history.csv\"\nvgg16_train_history.to_csv(file_name)","6da234e4":"vgg16_train_history.head()","f9d43bcc":"ls \".\/vgg16\"","3ac0d354":"model16_a = tf.saved_model.load(\".\/vgg16\")","06b11c2a":"model_16.summary()","3b4f26da":"start_time = time.time()\nhistory_19 = model_19.fit(X_train,validation_data=X_val,epochs=200,callbacks=[early_stopping,model_check_point_19],verbose=2)\nend_time = time.time()\nprint(\"Training time for VGG19: \", end_time - start_time,\"secs\")\nvgg19_train_time = (end_time - start_time) \/ 60","3f0c81c5":"vgg19_train_time = (end_time - start_time) \/ 60\nvgg19_train_time","2a2ae863":"vgg19_train_history = pd.DataFrame(history_19.history)\nfile_name = \"vgg_19_history.csv\"\nvgg19_train_history.to_csv(file_name)","0261f9e4":"start_time = time.time()\nhistory_MNV2 = modelMNV2.fit(X_train,validation_data=X_val,epochs=200,callbacks=[early_stopping,model_check_point_MNV2],verbose=2)\nend_time = time.time()\nprint(\"Training time for MNV2: \", end_time - start_time,\"secs\")\nmnv2_train_time = (end_time - start_time) \/ 60","2bc9b25c":"mnv2_train_history = pd.DataFrame(history_MNV2.history)\nfile_name = \"mnv2_history.csv\"\nmnv2_train_history.to_csv(file_name)","c37010a5":"mnv2_train_time","b3900018":"modelDN.summary()","9acbf424":"from tensorflow.keras.utils import plot_model\nfrom IPython.display import Image","09e8c00a":"plot_model(modelDN, to_file='DN121.png', show_shapes=True, show_layer_names=True)\nImage(filename='DN121.png')","9a957c76":"modelDN.compile(optimizer=adam_opt,loss=model_loss,metrics=[\"accuracy\"])","9dc34c80":"start_time = time.time()\nhistory_DN = modelDN.fit(X_train,validation_data=X_val,epochs=200,callbacks=[early_stopping,model_check_point_DN],verbose=2)\nend_time = time.time()\nprint(\"Training time for DN: \", end_time - start_time,\"secs\")\ndn_train_time = (end_time - start_time) \/ 60","8a81fc11":"dn_train_history = pd.DataFrame(history_DN.history)\nfile_name = \"dn_history.csv\"\ndn_train_history.to_csv(file_name)","07fbe714":"# predictions\ns16 = time.time()\npredictions16 = model_16.predict(y_val)\ne16 = time.time()\ns19 = time.time()\npredictions19 = model_19.predict(y_val)\ne19 = time.time()\nsMNV2 = time.time()\npredictionsMNV2 = modelMNV2.predict(y_val)\neMNV2 = time.time()\nsDN = time.time()\npredictionsDN = modelDN.predict(y_val)\neDN = time.time()\n\nprint(\"Test time for VGG16: \", e16 - s16,\"secs\")\nvgg16_test_time = (e16 - s16) \/ 60\nprint(\"Test time for VGG19: \", e19 - s19,\"secs\")\nvgg19_test_time = (e19 - s19) \/ 60\nprint(\"Test time for MNV2: \", eMNV2 - sMNV2,\"secs\")\nmnv2_test_time = (eMNV2 - sMNV2) \/ 60\nprint(\"Test time for DN: \", eDN - sDN,\"secs\")\ndn_test_time = (eDN - sDN) \/ 60\n\npredicted16 = [np.argmax(w) for w in predictions16]\nexpected16 = y_val.labels\n\npredicted19 = [np.argmax(w) for w in predictions19]\nexpected19 = y_val.labels\n\npredictedMNV2 = [np.argmax(w) for w in predictionsMNV2]\nexpectedMNV2 = y_val.labels\n\npredictedDN = [np.argmax(w) for w in predictionsDN]\nexpectedDN = y_val.labels\n\n","8a7b8951":"# Evaluation Results\nloss_value16 , accuracy16 = model_16.evaluate(y_val)\n\nprint(f'Test loss_value VGG16: {loss_value16}')\nprint(f'Test accuracy VGG16: {accuracy16}')\n\nloss_value19 , accuracy19 = model_19.evaluate(y_val)\n\nprint(f'Test loss_value VGG19: {loss_value19}')\nprint(f'Test accuracy VGG19: {accuracy19}')\n\nloss_valueMNV2 , accuracyMNV2 = modelMNV2.evaluate(y_val)\n\nprint(f'Test loss_value MNV2: {loss_valueMNV2}')\nprint(f'Test accuracy MNV2: {accuracyMNV2}')\n\nloss_valueDN , accuracyDN = modelDN.evaluate(y_val)\n\nprint(f'Test loss_value DN: {loss_valueDN}')\nprint(f'Test accuracy DN: {accuracyDN}')","b1f74ef4":"print(\"VGG16 confusion matrix: \")\nconfusion_matrix(expected16, predicted16)","c03f8d44":"print(\"VGG19 confusion matrix: \")\nconfusion_matrix(expected19, predicted19)","935553ef":"print(\"MNV2 confusion matrix: \")\nconfusion_matrix(expectedMNV2, predictedMNV2)","07f36fe5":"print(\"DN confusion matrix: \")\nconfusion_matrix(expectedDN, predictedDN)","a8064ac5":"print(f'Classes: {y_val.class_indices}\\n')\nprint(classification_report(expected16, predicted16))","ad3f31d4":"print(f'Classes: {y_val.class_indices}\\n')\nprint(classification_report(expected19, predicted19))","c73c92c8":"print(f'Classes: {y_val.class_indices}\\n')\nprint(classification_report(expectedMNV2, predictedMNV2))","bc5c8bb3":"print(f'Classes: {y_val.class_indices}\\n')\nprint(classification_report(expectedDN, predictedDN))","01940c51":"\nresults_plot = plot_results(history_16.history['accuracy'],\n                            history_16.history['val_accuracy'], \n                            history_16.history['loss'], \n                            history_16.history['val_loss'])\n\n# enter data to the final results dataframe\nresults_calc = classification_report(expected16, predicted16, output_dict=True)","3a2905a3":"\nresults_plot = plot_results(history_19.history['accuracy'],\n                            history_19.history['val_accuracy'], \n                            history_19.history['loss'], \n                            history_19.history['val_loss'])\n\n# enter data to the final results dataframe\nresults_calc = classification_report(expected19, predicted19, output_dict=True)","b16ac142":"\nresults_plot = plot_results(history_MNV2.history['accuracy'],\n                            history_MNV2.history['val_accuracy'], \n                            history_MNV2.history['loss'], \n                            history_MNV2.history['val_loss'])\n\n# enter data to the final results dataframe\nresults_calc = classification_report(expectedMNV2, predictedMNV2, output_dict=True)","e63ae3e3":"\nresults_plot = plot_results(history_DN.history['accuracy'],\n                            history_DN.history['val_accuracy'], \n                            history_DN.history['loss'], \n                            history_DN.history['val_loss'])\n\n# enter data to the final results dataframe\nresults_calc = classification_report(expectedDN, predictedDN, output_dict=True)","5742b3cc":"y_test","02e9e7be":"x= y_test.next()\nimg1 = x[0]\nplt.imshow(img1[0])\nplt.show()","7ff544af":"plt.imshow(img1[1])\nplt.show()","ef114635":"plt.imshow(img1[2])\nplt.show()","66a68a86":"plt.imshow(img1[3])\nplt.show()","94eeae5f":"plt.imshow(img1[4])\nplt.show()","819dfc83":"y_test.labels","b63ae9cf":"# predictions\ns16 = time.time()\npredictions16 = model_16.predict(y_test)\ne16 = time.time()\ns19 = time.time()\npredictions19 = model_19.predict(y_test)\ne19 = time.time()\nsMNV2 = time.time()\npredictionsMNV2 = modelMNV2.predict(y_test)\neMNV2 = time.time()\nsDN = time.time()\npredictionsDN = modelDN.predict(y_test)\neDN = time.time()\n\nprint(\"Test time for VGG16: \", e16 - s16,\"secs\")\nvgg16_test_time_5 = (e16 - s16) \/ 60\nprint(\"Test time for VGG19: \", e19 - s19,\"secs\")\nvgg19_test_time_5 = (e19 - s19) \/ 60\nprint(\"Test time for MNV2: \", eMNV2 - sMNV2,\"secs\")\nmnv2_test_time_5 = (eMNV2 - sMNV2) \/ 60\nprint(\"Test time for DN: \", eDN - sDN,\"secs\")\ndn_test_time_5 = (eDN - sDN) \/ 60\n\npredicted16 = [np.argmax(w) for w in predictions16]\nexpected16 = y_test.labels\n\npredicted19 = [np.argmax(w) for w in predictions19]\nexpected19 = y_test.labels\n\npredictedMNV2 = [np.argmax(w) for w in predictionsMNV2]\nexpectedMNV2 = y_test.labels\n\npredictedDN = [np.argmax(w) for w in predictionsDN]\nexpectedDN = y_test.labels","b4fc08e1":"print(\"VGG16 confusion matrix: \")\nprint(confusion_matrix(expected16, predicted16))\nprint(\"VGG19 confusion matrix: \")\nprint(confusion_matrix(expected16, predicted16))\nprint(\"MNV2 confusion matrix: \")\nprint(confusion_matrix(expectedMNV2, predictedMNV2))\nprint(\"DN confusion matrix: \")\nprint(confusion_matrix(expectedDN, predictedDN))","a56b52a3":"We are going to train and test four populer CNN alogrithms with thier ImangeNet weights and add only last layer as 512 neurons. In order to see how they perform. ","af587e83":"## Helper Functions Module","1f5881fc":"## Data Preparation ","939bd540":"If you don\u2019t like to see progress, please change verbose to 0 which will be complete silence. Also please check in model checkpoints I made verbose = 0. If you set to 1, you will be notfied when the model values are saved based on improvement at given criteria. Here is a blog post about verbose : https:\/\/stackoverflow.com\/questions\/47902295\/what-is-the-use-of-verbose-in-keras-while-validating-the-model","05501663":"So let's find out what happened to our images after transformation. I picked random mbd256 image to see, you can pick many others to see how transformation worked. ","95708f89":"Look above!!! Amazing isn't it?","539eecf2":"# Optimizers ES and Model Checkpoint","f81cf0c1":"I try to make 4 datasets to train and test our models. X -> X_train and X_val 208\/52 and I will not use validation dataset to test my accuracy. y-> y_val and y_test, I will use y_val to predict and y_test for random test images with closer examination. \nBasically, I will train model with 208 images and validate my model with 52 images. Then I will predict 23 images. Finally I will test with random 5 images. ","4268a79d":"# Image Check","5d71edfd":"## MammoScan Object \nThis object is very useful to transform data for feeding model. ","ed9b19d8":"# Model 1 - VGG16 + 512 ","01334e23":"# Model 2 - VGG19 + 512 ","027024e7":"## Imoport Libraries ","484d718c":"random_state helps us to work on exactly the same data split in order to make a reproductive research. I picked number 42. Here is the link why I picked : https:\/\/grsahagian.medium.com\/what-is-random-state-42-d803402ee76b","23ce055f":"# Train ALL","8c2d04e2":"Each model checkpoints will be saved to a separate file. This is very useful if we don't want to train our model again and use the latest version of our models with their weights and graph. ","145ceb89":"I will measure time and evaluate predictions of the model based on our test set. Time you see below actually minutes to predcit 23 images with size 256x256x3. ","e01e82e3":"# MobileNet V2 + 512","579a0808":"DenseNet is an amazing structure with normalization and pooling layers between convolution blocks. I would like to show you specifically the structure of this model. ","0b598665":"So let's look at newly generated dataset \"mammo\". We have **\"ab_class\"** which can be *\"CIRC,NORM,ARCH,ASYM,MISC,SPIC and CALC\"*. We have **\"bg\"** class *\"F-Fatty, G-Granular and D-Dense\"*. We have **\"p_matrix\"** class which is the matrix representation of image and **\"severity\"** or label. Finally path of the generated images. ","18d7950c":"# Important Note : Please read. \n\nThis kind of porjects have different stages. The biggest workload is preparing data for the model. My concern is to find an optimum CNN model that can be trained and make predictions with low-cost at acceptable accuracy. In order to achieve this I applied transfer learning for the most popular CNN algorithms in order to compare new shallow CNN. \n\nI used \"MammoScan\" object, and \"helper functions\" from an API at this link :\"https:\/\/github.com\/cpoles\/data_science\/tree\/main\/projects\/deep_learning\/mias_mammography\" which belongs to a data science school project. ","768b134b":"Thank you for reading all. ","0b4c3418":"I save all accuracy and loss data as dataframe to local files. Later on we can compare each of them based on hyperparameter changes. ","56c8fc02":"Here we can check two different approaches; balanced dataset vs. unbalanced dataset. There is no silver bullet for better approcah but training based on balanced dataset is more favorable. Especially if your dataset is highly imbalanced, you need such transdformation in order to prevent overfitting from the first place. ","81929df9":"# DenseNet 121 + 512","1c8c3c04":"# Compile ALL","3f403354":"We measure time elapsed as seconds for training and testing. Remember this images are very small if you compare to normal life dicom images. ","47c4e404":"We can simply load model from the working directory. Remember this files are not small. I recommend to use cloud storage if you save all model trials to remake your models. Generally speaking for 256x256x3 input shape model variables around 250 MB. ","e8bc5d74":"\"create_mias_dataset\" function is a part of helper function module. \"generate_subsamples\" is a data augmentation function, that creates new images from the orginal image. Fucntion generates 0-90-180-270 degrees rotated version of the original image and reduces the size. This way we can increase the dataset size from 319 to 3828. In the original version of the \"helper\" module, image sizes are reduced to 48x48. I prefer to use a bigger size images which effects the computation of course. My image sizes will be 256x256. My target is to work with original image size 1024x1024 but this work will be a seperate notebook. ","1bc26c61":"I import libraries upfront that may come confusing, generally while working on new project I prefer to import libaries as I need them to remember why\/when I use such application. ","3883c0d7":"Checking model summaries is a very handy method to visualize your model. Also you will find the summary of parameters as trainable and untrainable. We are applying transfer learning with original Imagenet weights and we don\u2019t want to modify orginal weights but final fully connected layer. We want to see how original weighted models perform. ","bcf379bb":"# Prediction ","a805cbe2":"Let's see our final images that has never been introduced to model. So we want model to predict these new images. Below you will find each of them. The most critical ones are malignant labels which are indexed as \"1\". You will see how these models perfom. "}}