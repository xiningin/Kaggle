{"cell_type":{"5e8e9b1e":"code","3592a5c0":"code","bf44f80a":"code","7ab7adcf":"code","deced181":"code","70dce77e":"code","fab48b43":"code","11134394":"code","035aa180":"code","7a7d611e":"code","56754cb8":"code","40849346":"code","da2a1b0a":"code","2b60d8ac":"code","99ff8a9c":"code","a2a43266":"code","496210a1":"code","64b13220":"code","9240116f":"code","05ad1564":"code","68fb9dab":"code","6b801b15":"code","9da38cbe":"code","6227c00d":"code","95fc5ecf":"code","7928bc8b":"code","78119a04":"code","95f53645":"code","27810522":"code","b750bb70":"code","6d394dc1":"code","b61c33cb":"code","ca173318":"code","23fc33f9":"code","115f7589":"code","78833d37":"code","8b7b3414":"code","201507e2":"markdown","dced2d0a":"markdown","ddc23b1c":"markdown","69d8c30e":"markdown","f75405e0":"markdown","4ac63aac":"markdown","d5280f01":"markdown","a09d4b42":"markdown","b1abb146":"markdown","e02d8642":"markdown","569f10ac":"markdown"},"source":{"5e8e9b1e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","3592a5c0":"df = pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")","bf44f80a":"df.head()","7ab7adcf":"df.describe()","deced181":"df.info()","70dce77e":"plt.figure(figsize=[10,10])\nplt.scatter(df.pelvic_incidence,df.sacral_slope)\n\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\n\nplt.show()","fab48b43":"from sklearn.linear_model import LinearRegression\n\n\nlinear_reg=LinearRegression()\n\n\nx=df.pelvic_incidence.values.reshape(-1,1)\ny=df.sacral_slope.values.reshape(-1,1)\nlinear_reg.fit(x,y)\ny_head=linear_reg.predict(x)","11134394":"plt.figure(figsize=[10,10])\nplt.scatter(x,y)\n\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\n\nplt.plot(x,y_head,color=\"red\")\nplt.show()","035aa180":"sns.pairplot(df)","7a7d611e":"df.corr()","56754cb8":"sns.heatmap(df.corr())","40849346":"from sklearn.preprocessing import LabelEncoder","da2a1b0a":"# Determine categorical features\n\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = df.columns.values.tolist()\nfor col in features:\n    if df[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","2b60d8ac":"# Encode categorical features\n\nfor col in categorical_columns:\n    if col in df.columns:\n        label = LabelEncoder()\n        label.fit(list(df[col].astype(str).values))\n        df[col] = label.transform(list(df[col].astype(str).values))","99ff8a9c":"target_name = 'class'\ndata_target = df[target_name]\ndf = df.drop([target_name], axis=1)","a2a43266":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df, data_target, test_size=0.3, random_state=1)","496210a1":"from sklearn.linear_model import LogisticRegression\n\n# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\nacc_log = round(logreg.score(x_train, y_train) * 100, 2)\nacc_log","64b13220":"acc_test_log = round(logreg.score(x_test, y_test) * 100, 10)\nprint(\"Accuracy: \",acc_test_log)","9240116f":"from sklearn.svm import SVC\n\n# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\nacc_svc = round(svc.score(x_train, y_train) * 100, 2)\nacc_svc","05ad1564":"acc_test_svc = round(svc.score(x_test, y_test) * 100, 2)\nacc_test_svc","68fb9dab":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(x_train, y_train)\nprint(\" {} nn score: {} \".format(5,knn.score(x_test,y_test)))","6b801b15":"score_list = []\nfor i in range(1,50):\n    knn2 = KNeighborsClassifier(n_neighbors = i)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test, y_test))\nscore_list","9da38cbe":"plt.plot(range(1,50),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","6227c00d":"knn3 = KNeighborsClassifier(n_neighbors = 20)\nknn3.fit(x_train, y_train)\nprint(\" {} nn score: {} \".format(20,knn3.score(x_test,y_test)))","95fc5ecf":"y_pred_knn = knn3.predict(x_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_knn)\nprint(\"KNN Confusion Matrix Result: \\n \\n\", cm)","7928bc8b":"# Confusion matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Real\")\nplt.show()","78119a04":"# Decision Tree Classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(x_train, y_train)\nacc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\nacc_decision_tree","95f53645":"acc_test_decision_tree = round(decision_tree.score(x_test, y_test) * 100, 2)\nacc_test_decision_tree","27810522":"# Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nrandom_forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [100, 300]}, cv=5).fit(x_train, y_train)\nrandom_forest.fit(x_train, y_train)\nacc_random_forest = round(random_forest.score(x_train, y_train) * 100, 2)\nprint(acc_random_forest,random_forest.best_params_)","b750bb70":"acc_test_random_forest = round(random_forest.score(x_test, y_test) * 100, 3)\nacc_test_random_forest","6d394dc1":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint","b61c33cb":"# Model\nmodel = Sequential()\nmodel.add(Dense(16, input_dim = x_train.shape[1], activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.summary()","ca173318":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","23fc33f9":"hist = model.fit(x_train, y_train, batch_size=64, \n               epochs=500, verbose=1)","115f7589":"plt.plot(hist.history['accuracy'], label='acc')\n\nplt.ylim((0, 1))\nplt.legend()","78833d37":"from sklearn import metrics\n\n# Predicting the Train set results\nnn_prediction = model.predict(x_train)\nnn_prediction = (nn_prediction > 0.5)*1 # convert probabilities to binary output\n\n# Compute error between predicted data & true answer\nacc_ann2 = round(metrics.accuracy_score(y_train, nn_prediction) * 100, 3)\nacc_ann2","8b7b3414":"# Predicting the Test set results\nnn_prediction_test = model.predict(x_test)\nnn_prediction_test = (nn_prediction_test > 0.5)*1 # convert probabilities to binary output\n\n# Compute error between predicted data & true answer\nacc_test_ann2 = round(metrics.accuracy_score(y_test, nn_prediction_test) * 100, 10)\nacc_test_ann2","201507e2":"# Neural Networks","dced2d0a":"* Decision Tree Classifier: 77.4%\n* Random Forests Classifier: 82.8%\n* Support Vector Machine: 84.9%\n----\n* K-Nearest Neighbors: 87.09%\n* Neural Networks: 87.09%\n* Logistic Regression: 87.09%","ddc23b1c":"# Test & Train Sets","69d8c30e":"# Conclusion","f75405e0":"# Support Vector Machines","4ac63aac":"# K-Nearest Neighbors with Evaluation","d5280f01":"# Random Forests","a09d4b42":"# Explore","b1abb146":"# Preprocessing","e02d8642":"# Decision Tree Classifier","569f10ac":"# Logistic Regression"}}