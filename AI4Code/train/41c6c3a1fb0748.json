{"cell_type":{"87310c73":"code","0c60690c":"code","15cbcce3":"code","21e69f84":"code","800b382c":"code","809a17dd":"code","8ea2ba75":"code","719a451b":"code","de9b0fb0":"code","b65c1deb":"code","143a3339":"code","895685ef":"code","35be213e":"code","cb75f8ba":"code","74fd44b5":"code","f83e17e5":"code","09e5be04":"code","bcd6f9ed":"code","c909e083":"code","16a5fca4":"markdown","3d69cf8e":"markdown"},"source":{"87310c73":"import numpy as np\nimport os\nimport pandas as pd\nimport sys","0c60690c":"carb = '..\/input\/garbage-classification\/Garbage classification\/Garbage classification\/cardboard'\nglass =\"..\/input\/garbage-classification\/Garbage classification\/Garbage classification\/glass\"\nmetal =\"..\/input\/garbage-classification\/Garbage classification\/Garbage classification\/metal\"\npaper = \"..\/input\/garbage-classification\/Garbage classification\/Garbage classification\/paper\"\nplastic =\"..\/input\/garbage-classification\/Garbage classification\/Garbage classification\/plastic\"\ntrash =\"..\/input\/garbage-classification\/Garbage classification\/Garbage classification\/trash\"\n","15cbcce3":"import cv2  \nimport os\nX=[]\ny_label =[]\nimgsize = 150\n#\u5b9a\u4e49\u51fd\u6570\u8bfb\u5165\u56fe\u50cf\ndef train_data(label,data_dir):\n    print(\"\u6b63\u5728\u5f55\u5165\u4fe1\u606f\uff1a\",data_dir)\n    for img in os.listdir(data_dir):\n        path = os.path.join(data_dir,img)\n        print(path)\n        img = cv2.imread(path,1)\n        print(img)\n        \n        img = cv2.resize(img,(imgsize,imgsize))\n        X.append(np.array(img))\n        y_label.append(str(label))\n        \n#\u8bfb\u5165\u56fe\u50cf\ntrain_data('carboard',carb)\ntrain_data('class',glass)\ntrain_data('metal',metal)\ntrain_data('paper',paper)\ntrain_data('plastic',plastic)\ntrain_data('trash',trash)\n# train_data('d7',d7)\n# train_data('d8',d8)\n# train_data('d9',d9)\n# train_data('d10',d10)","21e69f84":"from sklearn.preprocessing import LabelEncoder   #\u6807\u7b7e\u7f16\u7801\u5de5\u5177\nfrom keras.utils.np_utils import to_categorical   #\u5bfc\u5165one-hot\u7f16\u7801\u5de5\u5177\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y_label)  #\u6807\u7b7e\u7f16\u7801\ny = to_categorical(y,6)  #\u5c06\u6807\u7b7e\u8f6c\u5316\u4e3aone-hot\u7f16\u7801\nX = np.array(X)  #\u5c06X\u5217\u8868\u8f6c\u5316\u4e3a\u5f20\u91cf\u6570\u7ec4\nX = X\/255  # \u7b80\u5355\u538b\u7f29\uff0c\u5c06X\u5f20\u91cf\u7edf\u4e00\u5f52\u4e00\u5316","800b382c":"print('X\u5f20\u91cf\u7684\u5f62\u72b6\uff1a',X.shape)\nprint('X\u5f20\u91cf\u7684\u7b2c\u4e00\u4e2a\u6570\u636e',X[1])","809a17dd":"print('y\u5f20\u91cf\u7684\u5f62\u72b6\uff1a',y.shape)\nprint(\"y\u5f20\u91cf\u7684\u7b2c\u4e00\u4e2a\u6570\u636e\",y[1])  #\u9a8c\u8bc1one-hot","8ea2ba75":"import matplotlib.pyplot as plt\nimport random as rdm\n#\u968f\u673a\u663e\u793a\u56fe\u7247\nfig,ax = plt.subplots(5,3)\nfig.set_size_inches(15,15)\nfor i in range(5):\n    for j in range(3):\n        r = rdm.randint(0,len(X))\n        ax[i,j].imshow(X[r])\n        ax[i,j].set_title(\"garbage:\"+y_label[r])\nplt.tight_layout()          #\u81ea\u52a8\u8c03\u6574\u5b50\u56fe\u53c2\u6570","719a451b":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test =train_test_split(X,y,test_size =0.2,random_state =0)","de9b0fb0":"#\u5efa\u7acb\u7b80\u5355\u5377\u79ef\u7f51\u7edc\nfrom keras import layers  #\u5bfc\u5165\u6240\u6709\u5c42\nfrom keras import models\ncnn = models.Sequential()   #\u5e8f\u8d2f\u6a21\u578b\ncnn.add(layers.Conv2D(32,(3,3), activation = \"relu\",input_shape=(150,150,3)))  #\u5377\u79ef\u5c42\ncnn.add(layers.MaxPooling2D((2,2)))    #\u6700\u5927\u6c60\u5316\u5c42\ncnn.add(layers.Conv2D(64,(3,3),activation = 'relu'))   #\u5377\u79ef\u5c4264\ncnn.add(layers.MaxPooling2D((2,2)))   #\u6700\u5927\u6c60\u5316\u5c42\ncnn.add(layers.Conv2D(128,(3,3),activation = 'relu'))  #\u5377\u79ef\u5c42 128\ncnn.add(layers.MaxPooling2D((2,2)))   #\u6700\u5927\u6c60\u5316\u5c42\ncnn.add(layers.Conv2D(128,(3,3),activation=\"relu\"))   #\u5377\u79ef\u5c42128\ncnn.add(layers.MaxPooling2D((2,2)))  #\u6700\u5927\u6c60\u5316\u5c42\ncnn.add(layers.Flatten())   #\u5c55\u5e73\u5c42\ncnn.add(layers.Dense(512,activation=\"relu\"))     #\u5168\u8fde\u63a5\u5c42\ncnn.add(layers.Dense(6,activation=\"softmax\"))   #\u5206\u7c7b\u8f93\u51fa\ncnn.compile(loss='categorical_crossentropy', #\u635f\u5931\u51fd\u6570\n           optimizer ='rmsprop',#  \u4f18\u5316\u5668\n            metrics=['acc']  #\u8bc4\u4f30\u6307\u6807\n           )","b65c1deb":"def show_history(history):    \n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1,len(loss)+1)\n    plt.figure(figsize=(12,4))\n    plt.subplot(1,2,1)\n    plt.plot(epochs,loss,'bo',label=\"Training loss\")\n    plt.plot(epochs,val_loss,'b',label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel('Epochs')\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    plt.subplot(1,2,2)\n    plt.plot(epochs,acc,'bo',label=\"Training acc\")\n    plt.plot(epochs,val_acc,'b', label ='Validation acc')\n    plt.title(\"Training and validation accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('Accuray')\n    plt.legend()\n    plt.show()\n# show_history(history)  ","143a3339":"history = cnn.fit(X_train,y_train, #\u6307\u5b9a\u8bad\u7ec3\u96c6\n                 epochs=50,   #\u6307\u5b9a\u5faa\u73af\u8f6e\u6b21\n                  batch_size=256,  #\u6307\u5b9a\u6279\u91cf\u5927\u5c0f\n                  validation_data=(X_test,y_test)  #\u6307\u5b9a\u9a8c\u8bc1\u96c6\n                 )","895685ef":"show_history(history)","35be213e":"from keras import optimizers\ncnn = models.Sequential()   #\u5e8f\u8d2f\u6a21\u578b\ncnn.add(layers.Conv2D(32,(3,3),activation= 'relu', #\u5377\u79ef\u5c42\n           input_shape=(150,150,3)))\ncnn.add(layers.MaxPooling2D((2,2)))    #\u6700\u5927\u6c60\u5316\u5c42\ncnn.add(layers.Conv2D(64,(3,3),activation = 'relu'))   #\u5377\u79ef\u5c4264\ncnn.add(layers.MaxPooling2D((2,2)))   #\u6700\u5927\u6c60\u5316\u5c42\ncnn.add(layers.Conv2D(128,(3,3),activation = 'relu'))  #\u5377\u79ef\u5c42 128\ncnn.add(layers.MaxPooling2D((2,2)))   #\u6700\u5927\u6c60\u5316\u5c42\ncnn.add(layers.Conv2D(256,(3,3),activation=\"relu\"))   #\u5377\u79ef\u5c42256\ncnn.add(layers.MaxPooling2D((2,2)))  #\u6700\u5927\u6c60\u5316\u5c42\ncnn.add(layers.Flatten())   #\u5c55\u5e73\u5c42\ncnn.add(layers.Dropout(0.9))           ### Dropout\ncnn.add(layers.Dense(512,activation=\"relu\"))    #\u5168\u8fde\u63a5\u5c42\ncnn.add(layers.Dense(512,activation=\"elu\"))    #\u5168\u8fde\u63a5\u5c42\ncnn.add(layers.Dense(6,activation=\"sigmoid\"))   #\u7528sigmod\u51fd\u6570\u5206\u7c7b\u8f93\u51fa\ncnn.compile(loss=\"categorical_crossentropy\",    #\u635f\u5931\u51fd\u6570\n            optimizer = optimizers.Adam(lr =1e-4), #\u66f4\u65b0\u4f18\u5316\u5668\u5e76\u8bbe\u5b9a\u5b66\u4e60\u901f\u7387\n            metrics=['acc']   #\u8bc4\u4f30\u6307\u6807\n\n            )","cb75f8ba":"history = cnn.fit(X_train,y_train, #\u6307\u5b9a\u8bad\u7ec3\u96c6\n                 epochs=60,   #\u6307\u5b9a\u5faa\u73af\u6b21\u6570\n                  batch_size=200,  #\u6307\u5b9a\u6279\u91cf\u5927\u5c0f256\n                  validation_data=(X_test,y_test)  #\u6307\u5b9a\u9a8c\u8bc1\u96c6\n                 )\n\n# \u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u7387\n# 1\u3001\u6279\u6b21\u7684\u5927\u5c0f\u53ef\u4ee5\u4fee\u6539batch_size=200\n# 2\u3001\u53ef\u4ee5\u589e\u52a0\u9690\u85cf\u5c42 \u6fc0\u6d3b\u51fd\u6570\u3001\u795e\u7ecf\u5143\u90fd\u53ef\u5c1d\u8bd5\n# 3\u3001\u5c06\u6743\u503c\u548c\u504f\u7f6e\u503c\u521d\u59cb\u5316\u4e3a0 \u6216\u521d\u59cb\u5316\u65b9\u5f0f \u662f\u5426\u6548\u679c\u4f1a\u597d\n# 4\u3001\u4ee3\u4ef7\u51fd\u6570 \u4f7f\u7528\u4ea4\u53c9\u71b5\n# 5\u3001\u5b66\u4e60\u7387\n# 6\u3001\u53ef\u4ee5\u5c1d\u8bd5\u66f4\u591a\u7684\u8bad\u7ec3\u6b21\u6570","74fd44b5":"show_history(history)","f83e17e5":"from keras.models import load_model\ncnn.save('garbage_model.h5')  # creates a HDF5 file 'my_model.h5'\n \n# returns a compiled model\n# identical to the previous one\n# model = load_model('my_model.h5')","09e5be04":"model = load_model('garbage_model.h5')","bcd6f9ed":"classes_name_list=['car','glass','metal','paper','plastic','trash']","c909e083":"import skimage\nfrom skimage import io\nfrom skimage import transform\nfilename=r'..\/input\/garbage-classification\/Garbage classification\/Garbage classification\/cardboard\/cardboard1.jpg' \ntest_img=skimage.io.imread(filename) #\u52a0\u8f7d\u6d4b\u8bd5\u56fe\u50cf\uff0c\u76f4\u63a5\u5f97\u5230\u591a\u7ef4\u6570\u7ec4\ntest_img=skimage.transform.resize(test_img,(150,150,3))  #\u6839\u636e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u8981\u6c42\uff0c\u91cd\u5b9a\u4e49\u5927\u5c0f\ntest_img=np.expand_dims(test_img,0) #\u6a21\u578b\u8f93\u5165\u8981\u6c42\u4e3a\u56db\u7ef4\u5f20\u91cf\uff0c\u6240\u4ee5\u9700\u8981\u589e\u52a0\u4e00\u4e2abatch_size\u7ef4\u5ea6\npred=model.predict(test_img) #\u5c06\u6d4b\u8bd5\u56fe\u50cf\u9001\u5165\u6a21\u578b\uff0c\u8fdb\u884c\u9884\u6d4b\nprint('\u9884\u6d4b\u7ed3\u679c\uff1a',end='')\nprint(classes_name_list[pred.argmax()]) #\u8f93\u51fa\u7c7b\u522b\u9884\u6d4b\u7ed3\u679c\n# print(pred.argmax())","16a5fca4":"# \u635f\u5931\u4e0e\u51c6\u786e\u7387\u56fe\u8868","3d69cf8e":"# \u66f4\u6362\u4f18\u5316\u5668\uff0c\u6dfb\u52a0dropout\u5c42"}}