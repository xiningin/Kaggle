{"cell_type":{"53f81954":"code","dcb5f09f":"code","1c40a2c6":"code","b6d6bfa4":"code","5a77588a":"code","4717a74f":"code","bb88151e":"code","f48ab9bf":"code","680f8492":"code","0343383b":"code","304379e2":"code","602bf1d1":"code","772e0acf":"code","6120a68b":"code","8346468e":"code","9ca6cb77":"code","10566070":"code","d1597f0a":"code","a1adeb83":"markdown","bc4dc278":"markdown","4c90aaeb":"markdown","4f69aa43":"markdown","9bbc0671":"markdown","b046444d":"markdown","50246268":"markdown","920fde3a":"markdown","d221f46e":"markdown","187721c9":"markdown","97c8568d":"markdown","2445ef63":"markdown","95974320":"markdown"},"source":{"53f81954":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Library to work with emojis\n!pip install emoji\n\nimport numpy as np\nimport emoji\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport os\n\n# Standard plotly imports\nimport chart_studio.plotly as py\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport shapely\nimport plotly.graph_objects as go\n\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot, init_notebook_mode\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dcb5f09f":"data = pd.read_csv(\"\/kaggle\/input\/twitter-airline-sentiment\/Tweets.csv\",sep=\",\")\ndata.head()","1c40a2c6":"tweets = data[[\"airline\",\"text\",\"airline_sentiment\"]]  \ntweets.head()","b6d6bfa4":"print(\"The dataset has {0} rows and {1} columns\".format(tweets.shape[0],tweets.shape[1]))","5a77588a":"tweets.dtypes","4717a74f":"tweets.isnull().sum().sort_values(ascending=False)","bb88151e":"# Grouping by sentiment and air companies\ntweets_group=data.groupby([\"airline_sentiment\",\"airline\"])[\"airline\"].count()\n\n\n## Pie chart\nlabels = tweets.airline_sentiment.value_counts().index\nvalues = tweets.airline_sentiment.value_counts().values\nfig_pie = go.Figure(data=[go.Pie(labels=labels, values=values)],layout=go.Layout(title=go.layout.Title(text=\"Total number of sentiments per category\")))\nfig_pie.show()\n\n\n## Barplot\nair_company=tweets_group.index.levels[1]\nsentiment=tweets_group.index.levels[0]\nfig_bar = go.Figure(go.Bar(x=air_company, y=tweets_group[\"negative\"], name=sentiment[0]),layout=go.Layout(title=go.layout.Title(text=\"Sentiment per company\")))\nfig_bar.add_trace(go.Bar(x=air_company, y=tweets_group[\"neutral\"], name=sentiment[1]))\nfig_bar.add_trace(go.Bar(x=air_company, y=tweets_group[\"positive\"], name=sentiment[2]))\nfig_bar.update_layout(title=\"Number of each sentiment\",barmode='group', xaxis={'categoryorder':'category ascending'})\n","f48ab9bf":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# Class to clean the Tweets\nclass CleanText(BaseEstimator, TransformerMixin):\n \n  # Removing URLs and hastag symbols and URLs\n    def removeURL_MentionsSymbol_HastagSymbol(self,tweet):\n        self.tweet_clean_mention = re.sub(r\"@[a-zA-Z0-9]+|\\#[a-zA-Z0-9]+\",\"\",str(tweet))\n        self.tweet_clean_url = re.sub(r\"(https|http)?:\\\/\\\/(\\w|\\.|\\\/|\\?|\\=|\\&|\\%)*\\b\",\"\", self.tweet_clean_mention, flags=re.MULTILINE).strip()\n        return self.tweet_clean_url\n\n  # Transforming emojis into text\n    def removeEmoji(self,text):\n        self.tweet = text\n        for token in self.tweet:\n            if token in emoji.UNICODE_EMOJI:\n                self.tweet = self.tweet.replace(token,emoji.demojize(token))\n        return self.tweet\n\n  # Removing digits\n    def removeDigits(self,tweet):\n        return re.sub(r\"[0-9]\",\"\",tweet)\n  \n  # Removing symbols which are not ASCII\n    def clean_ascii(self,text):\n        return ''.join(i for i in text if ord(i) < 128)\n\n  # Transforming text into lowercase\n    def lower_case(self,tweet):\n        return tweet.lower()\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def transform(self, X, **transform_params):\n        clean_X = X.apply(self.removeURL_MentionsSymbol_HastagSymbol).apply(self.removeEmoji).apply(self.removeDigits).apply(self.clean_ascii).apply(self.lower_case)\n        return clean_X\n    \n    def get_params(self, deep=True):\n        return {}      ","680f8492":"# Spacy library for nlp\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English","0343383b":"# Load English tokenizer, tagger, parser, NER and word vectors\nnlp=English()\n\n# List of StopWords\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n# List with punctuations \npunctuations = string.punctuation\n","304379e2":"# Function to tokenize the tweets\ndef spacy_tokenizer(tweet):\n  \n  # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = nlp(tweet)\n\n  # Lemmatizing each token and converting each token into lowercase\n    mytokens = [word.lemma_.strip() for word in mytokens if word.lemma_ != \"-PRON-\" ]\n\n  # Removing stop words\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n  # return preprocessed list of tokens\n    return mytokens","602bf1d1":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# TF-IDF\ntfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer)","772e0acf":"from sklearn.svm import LinearSVC  \nfrom sklearn.model_selection import train_test_split\n\nsvm = LinearSVC(C=1,loss=\"hinge\")","6120a68b":"X_train,X_test,y_train,y_test = train_test_split(tweets.text,tweets.airline_sentiment,test_size=0.33,random_state=42)","8346468e":"X_train.head()","9ca6cb77":"from sklearn.pipeline import Pipeline\n\n# Defining the pipeline:\n#   1. Cleaning text\n#   2. Vectorizen\n#   3. Classifying with logistic regression\n\npipe_tweets = Pipeline([(\"clean\",CleanText()),\n                        (\"vectorizer\",tfidf_vector),\n                        (\"svm\",svm)])\n","10566070":"pipe_tweets.fit(X_train,y_train)","d1597f0a":"from sklearn import metrics\nfrom sklearn.metrics import classification_report\n\n# Predicting with a test dataset\ny_predicted = pipe_tweets.predict(X_test)\n\n# Model Accuracy\nprint(\"SVM Accuracy:\",metrics.accuracy_score(y_test, y_predicted))\nprint(\"SVM Precision:\",metrics.precision_score(y_test, y_predicted,average=\"macro\"))\nprint(\"SVM Recall:\",metrics.recall_score(y_test, y_predicted,average=\"macro\"))\n\n\n# Classification Report\n\nprint(\"\\n\"+classification_report(y_test, y_predicted, target_names=[\"negative\",\"neutral\",\"positive\"], digits=4))","a1adeb83":"# **3.CLEANING DATA**","bc4dc278":"# **1. BASIC ANALISYS**\n","4c90aaeb":"![Captura%20de%20pantalla%202019-11-30%20a%20las%2022.52.40.png](attachment:Captura%20de%20pantalla%202019-11-30%20a%20las%2022.52.40.png)","4f69aa43":"Hi everyone! This is my second kernel, focus on sentiment classification. I have been learning through different resources , which I am gonna leave at the end.\n\nI am still improving some things like the unbalanced dataset, that I am going to include, by means of different resampling methods. So, please any feedback is welcome! :)","9bbc0671":"# **8. EVALUATING THE MODEL**","b046444d":"# **7.TRAINING THE MODEL**","50246268":"# **Twitter Sentiment Analysis**\n","920fde3a":"As we can observe, the class labels are unbalanced and this is something that we should keep in mind later.","d221f46e":"# **6.CREATING THE PIPELINE**","187721c9":"# **2.VISUALIZATION**","97c8568d":"# 9 . REFERENCES:\n * Spacy course: https:\/\/course.spacy.io\/chapter1\n * Sentiment classificatione exampler with spacy: https:\/\/www.dataquest.io\/blog\/tutorial-text-classification-in-python-using-spacy\/\n * Emoji python library: https:\/\/pypi.org\/project\/emoji\/\n * Plotly library: https:\/\/plot.ly\/python\/pie-charts\/\n * Sentiment analysis planification: https:\/\/www.datacamp.com\/community\/tutorials\/simplifying-sentiment-analysis-python","2445ef63":"# **4.TOKENIZATION AND VECTORIZATION**","95974320":"# **5.CLASSIFIERS AND SPLITTING DATA INTO TRAINING AND TEST SETS**\n"}}