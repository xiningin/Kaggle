{"cell_type":{"646ac7a5":"code","a287e6b3":"code","476e099c":"code","83c4a4e4":"code","b40c3393":"code","21f7abce":"code","91b5d0b1":"code","ddf71b59":"code","2c7ffbe7":"code","a3fa73e6":"code","7be51c2b":"code","6b2e7d3d":"code","fc2f2bf6":"code","7ddacb45":"code","797052aa":"code","c47e839a":"code","f2b20bf5":"code","aee17b05":"code","1f32275e":"code","c71e59c6":"code","279c56d1":"code","0f0a6459":"code","cc153dbe":"code","6d40ed83":"code","5c0dc82a":"code","ff031b16":"code","5d913c54":"code","797d38a0":"code","c9b1b0e8":"code","841b0b84":"code","44e83fbb":"code","53de8eb4":"code","9d63692d":"code","8943f74f":"code","b3b55d6a":"code","fc8a49b1":"code","aebc6d48":"code","c4b4bdc8":"code","ae825b49":"code","f55d348e":"markdown","1c55ceb9":"markdown","22686954":"markdown","bb061ed7":"markdown","4cd161e5":"markdown","c1705622":"markdown","1ef15e3d":"markdown","0a5b4854":"markdown","d90dd506":"markdown","f8bcd671":"markdown","4b5ec584":"markdown","8c60097a":"markdown","02ea6129":"markdown","ce9cff78":"markdown","a86c3635":"markdown","4e3319bd":"markdown","e385f750":"markdown","686f4ca3":"markdown","81c0df16":"markdown","b1be7d8f":"markdown","4d57e44a":"markdown","4fe9c0c2":"markdown","84aeb602":"markdown","4d6a1c84":"markdown","8e7fcd1e":"markdown","17ffb256":"markdown","d49763d9":"markdown","f549d2ae":"markdown","c2a30a89":"markdown","c91057e5":"markdown","547c7861":"markdown","c72e42e8":"markdown","fc27b80f":"markdown","4a103b86":"markdown","ed10878f":"markdown","102f9b4b":"markdown","e110ea11":"markdown","2139b65b":"markdown","2adf258a":"markdown","165f14b3":"markdown","e7cc6964":"markdown","4e507020":"markdown","4286b962":"markdown","42ad3b39":"markdown","b00373a0":"markdown","f9bf281b":"markdown","76f0a4a7":"markdown","38e4e502":"markdown","be9d21d6":"markdown","1cff2f7c":"markdown","bf068e67":"markdown","9b82eb13":"markdown"},"source":{"646ac7a5":"!pip install -qU wandb\n!pip install -qU bbox-utility # check https:\/\/github.com\/awsaf49\/bbox for source code","a287e6b3":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\n\nimport shutil\nimport sys\nsys.path.append('..\/input\/tensorflow-great-barrier-reef')\n\nfrom joblib import Parallel, delayed\n\nfrom IPython.display import display, HTML\n\nfrom matplotlib import animation, rc\nrc('animation', html='jshtml')\n\n# for DA\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.utils.data as Data\nimport ast #?\nfrom fastprogress.fastprogress import master_bar, progress_bar #?","476e099c":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_team_iforine\")\n    wandb.login(key=api_key)\n    anonymous = None\nexcept:\n    wandb.login(anonymous='must')\n    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","83c4a4e4":"FOLD      = 4 # which fold to train\nDIM       = 2016\nMODEL     = 'yolov5s'\nBATCH     = 4\nEPOCHS    = 10\nOPTIM     = 'Adam'\nAUG       = 'HE'\n\nPROJECT   = 'iforine\/great-barrier-reef-public' # w&b in yolov5\nNAME      = f'{MODEL}-dim{DIM}-fold{FOLD}-bat{BATCH}-opt{OPTIM}-aug{AUG}-epch{EPOCHS}-addNoCot' # w&b for yolov5\n\nREMOVE_NOBBOX = False # remove images with no bbox\nADD_NOBBOX = True # bbox\u306e\u3042\u308b\u753b\u50cf\u3068\u540c\u3058\u679a\u6570\u5206bbox\u306e\u7121\u3044\u753b\u50cf\u3092\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u52a0\u3048\u308b\nROOT_DIR  = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nIMAGE_DIR = '\/kaggle\/working\/images' # directory to save images\nLABEL_DIR = '\/kaggle\/working\/labels' # directory to save labels\n\nWORKER = 4 # \u3088\u304f\u308f\u304b\u3063\u3066\u306a\u3044\u3002\u30b9\u30ec\u30c3\u30c9\u306e\u6570\u3068\u304b\uff1f\n\nnp.random.seed(42)","b40c3393":"!mkdir -p {IMAGE_DIR}\n!mkdir -p {LABEL_DIR}","21f7abce":"pd.set_option('display.max_columns', 10)","91b5d0b1":"# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}\/train.csv')\ndf['old_image_path'] = f'{ROOT_DIR}\/train_images\/video_'+df.video_id.astype(str)+'\/'+df.video_frame.astype(str)+'.jpg'\ndf['image_path']  = f'{IMAGE_DIR}\/'+df.image_id+'.jpg' # '\/kaggle\/working\/images'\ndf['label_path']  = f'{LABEL_DIR}\/'+df.image_id+'.txt' # '\/kaggle\/working\/labels'\ndf['annotations'] = df['annotations'].progress_apply(eval) # apply(\u5404\u8981\u7d20\u306b\u95a2\u6570\u3092\u9069\u7528\u3059\u308b)\u306e\u9032\u6357\u3092\u8868\u793a\u3059\u308b\u3002eval\u306f\u4f55\uff1f\ndisplay(df.head(100))","ddf71b59":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts(normalize=True)*100 # \u30e6\u30cb\u30fc\u30af\u306a\u8981\u7d20\u306e\u51fa\u73fe\u983b\u5ea6\u3092\u7b97\u51fa\u3002normalize=True\u306b\u3059\u308b\u3068\u5408\u8a08\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u898f\u5316\u3055\u308c\u308b(\u5272\u5408\u306b\u306a\u308b)\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","2c7ffbe7":"if REMOVE_NOBBOX:\n    df = df.query(\"num_bbox>0\") # df[df['num_bbox'] > 0]\u3068\u540c\u7b49\u3002\u76f4\u89b3\u7684\u3067\u4fbf\u5229\u3060\u30fb\u30fb\u30fb","a3fa73e6":"# \u80cc\u666f\u753b\u50cf\u3092\u5168\u4f53\u306e10%\u542b\u3081\u308b\nif ADD_NOBBOX:\n    df = pd.concat([df.query(\"num_bbox>0\"), df.query(\"num_bbox==0\").sample(int(len(df.query(\"num_bbox>0\")) * 0.1))])","7be51c2b":"def make_copy(row):\n    shutil.copyfile(row.old_image_path, row.image_path)\n    return","6b2e7d3d":"image_paths = df.old_image_path.tolist()\n_ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(row) for _, row in tqdm(df.iterrows(), total=len(df)))","fc2f2bf6":"# check https:\/\/github.com\/awsaf49\/bbox for source code of following utility functions\n# \u4f5c\u8005\u304c\u4f5c\u3063\u305f\u30d8\u30eb\u30d1\u30fc\u95a2\u6570\nfrom bbox.utils import coco2yolo, coco2voc, voc2yolo\nfrom bbox.utils import draw_bboxes, load_image\nfrom bbox.utils import clip_bbox, str2annot, annot2str\n\n# bbox\u3092\u30ea\u30b9\u30c8\u306b\u3057\u3066\u8fd4\u3059\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\n# row\u306b\u5e45\u3068\u9ad8\u3055\u306e\u5217\u3092\u8ffd\u52a0\u3059\u308b\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path']) # \u3053\u306eimagesize\u3063\u3066\u4f55\uff1f\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","7ddacb45":"# annotions\u304b\u3089bboxes\u5217\u3092\u4f5c\u6210\ndf['bboxes'] = df.annotations.progress_apply(get_bbox)\ndf.head(2)","797052aa":"df['width']  = 1280\ndf['height'] = 720\ndisplay(df.head(2))","c47e839a":"pd.set_option('display.max_columns', 100) # \u5217\u304c\u591a\u3044\u3068\u7701\u7565\u3055\u308c\u308b\u306e\u3092\u9632\u3050","f2b20bf5":"cnt = 0\nall_bboxes = []\nbboxes_info = []\nfor row_idx in tqdm(range(df.shape[0])):\n    row = df.iloc[row_idx]\n    image_height = row.height\n    image_width  = row.width\n    bboxes_coco  = np.array(row.bboxes).astype(np.float32).copy() #bboxes\u3092numpy\u5f62\u5f0f\u306b\u5909\u63db\n    num_bbox     = len(bboxes_coco)\n    names        = ['cots']*num_bbox\n    labels       = np.array([0]*num_bbox)[..., None].astype(str) # \u6b21\u5143\u3092++(\u30ea\u30b9\u30c8\u304b\u3089shape:(N, 1)\u306e\u884c\u5217\u3078)\n    ## Create Annotation(YOLO)\n    with open(row.label_path, 'w') as f:\n        if num_bbox<1:\n            annot = ''\n            f.write(annot)\n            cnt+=1\n            continue\n        bboxes_voc  = coco2voc(bboxes_coco, image_height, image_width)\n        bboxes_voc  = clip_bbox(bboxes_voc, image_height, image_width)\n        bboxes_yolo = voc2yolo(bboxes_voc, image_height, image_width).astype(str)\n        all_bboxes.extend(bboxes_yolo.astype(float)) # all_bboxes\u306bbboxes_yolo\u3092\u8ffd\u52a0\n        bboxes_info.extend([[row.image_id, row.video_id, row.sequence]]*len(bboxes_yolo)) # bboxes_info\u306bbbox\u306e\u6570\u3060\u3051[image_id, video_id, sequence]\u3092\u8ffd\u52a0\n        annots = np.concatenate([labels, bboxes_yolo], axis=1) # labels\u306e\u6a2a\u306bbboxes_yolo\u3092\u304f\u3063\u3064\u3051\u308b(bbox\u306e\u6570\u3060\u3051\u884c\u304c\u3067\u304d\u308b)\n        string = annot2str(annots) # annotation\u3092str\u306b\u3057\u3066\u308b\n        f.write(string)\nprint('Missing:',cnt)","aee17b05":"from sklearn.model_selection import GroupKFold\nkf = GroupKFold(n_splits = 5) # n_split: train,val\u306e\u30d1\u30bf\u30fc\u30f3\u306e\u6570\u3002\u5143\u30c7\u30fc\u30bf\u30925\u30d1\u30bf\u30fc\u30f3\u306etrain,val\u306b\u5206\u3051\u308b\ndf = df.reset_index(drop=True)\ndf['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, y = df.video_id.tolist(), groups=df.sequence)): # sequence: \u52d5\u753b\u306e\u30b5\u30d6\u30bb\u30c3\u30c8ID(\u540c\u3058ID\u306e\u753b\u50e7\u306f\u540c\u3058\u52d5\u753b)\n    df.loc[val_idx, 'fold'] = fold\ndisplay(df.fold.value_counts())","1f32275e":"train_files = []\nval_files   = []\ntrain_df = df.query(\"fold!=@FOLD\")\nvalid_df = df.query(\"fold==@FOLD\")\ntrain_files += list(train_df.image_path.unique())\nval_files += list(valid_df.image_path.unique())\nlen(train_files), len(val_files)","c71e59c6":"import albumentations as A","279c56d1":"class HE_HSV(A.ImageOnlyTransform):\n    def __init__(self, p: float = 0.5, always_apply=False):\n        super().__init__(always_apply, p)\n        \n    def apply(self, image,**params):\n        img_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n        # Histogram equalisation on the V-channel\n        img_hsv[:, :, 2] = cv2.equalizeHist(img_hsv[:, :, 2])\n\n        # convert image back from HSV to RGB\n        image_hsv = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2RGB)\n\n        return image_hsv","0f0a6459":"class AUG_DATASET(Dataset):\n    \n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n        \n    def coco2yolo(self, bboxes, image_height=720, image_width=1280):\n        \"\"\"\n        coco => [xmin, ymin, w, h]\n        yolo => [xmid, ymid, w, h] (normalized)\n        \"\"\"\n        bboxes = np.array(bboxes)\n        bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n        # normolizinig\n        bboxes[..., [0, 2]]= bboxes[..., [0, 2]]\/ image_width\n        bboxes[..., [1, 3]]= bboxes[..., [1, 3]]\/ image_height\n\n        # converstion (xmin, ymin) => (xmid, ymid)\n        bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\/2\n\n        return bboxes\n    \n    def coord_to_box(self, bouding_box, image):\n        box_yolo_format = []\n        height, width = image.shape[0], image.shape[1]\n        \n        if False: # CFG.use_coco2yolo\n            box_yolo_format = self.coco2yolo(bouding_box)\n            box_yolo_format = np.clip(box_yolo_format,0,1)\n            label = np.repeat([0],box_yolo_format.shape[0]).reshape(-1,1)\n            box_yolo_format = np.append(box_yolo_format,label, axis=1)\n        else:\n            for bb in bouding_box:\n                label = [max(0,bb[0]), max(0,bb[1]), min(bb[0]+bb[2], 1280), min(720,bb[1]+bb[3]), '0']\n                bbox_albu = A.convert_bbox_to_albumentations(label, source_format='pascal_voc', rows=height, cols=width)\n                bbox_yolo = A.convert_bbox_from_albumentations(bbox_albu, target_format='yolo', rows=height, cols=width, check_validity=True)\n                clip_box = [np.clip(value,0,1) for value in bbox_yolo[:-1]] + [bbox_yolo[-1]]\n                box_yolo_format.append(clip_box)\n        return box_yolo_format\n\n    def bbox_to_txt(self, bboxes):\n        \"\"\"\n        Convert a list of bbox into a string in YOLO format (to write a file).\n        @bboxes : numpy array of bounding boxes \n        return : a string for each object in new line: <object-class> <x> <y> <width> <height>\n        \"\"\"\n        txt=''\n        for index,l in enumerate(bboxes):\n            l = [str(x) for x in l[:4]]\n            l = ' '.join(l)\n            txt +=  '0' +' ' + l + '\\n'\n        return txt\n\n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self,index):\n        row = self.df.iloc[index]\n        path = row['image_path']\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        #aug_index = row['aug_index']\n        list_info = path.split('\/')\n        image_name = list_info[-2] + '_' + list_info[-1].split('.')[0]\n        box = row['bboxes']\n        bounding_box = self.coord_to_box(box, img)\n\n        if self.transform is not None:\n            res = self.transform(image=img, bboxes=bounding_box)\n            img = res['image']\n            bounding_box = res['bboxes']\n            \n        box_yolo_format = self.bbox_to_txt(bounding_box)\n        return img, box_yolo_format, image_name","cc153dbe":"def get_transforms():\n    return A.Compose([\n            HE_HSV(always_apply=True) # \u73fe\u5728\u306f\u5168\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066DA\u304b\u3051\u305f\u524d\u63d0\u3067train_df\u306e\u884c\u3092\u8ffd\u52a0\u3057\u3066\u3044\u308b\n            ], bbox_params=A.BboxParams(format='yolo' , min_visibility=0.4,min_area=500))","6d40ed83":"train_df","5c0dc82a":"if AUG is not None:\n    \n    train_df_he = train_df.copy(deep=True)\n    \n    # dataloader\u3092\u4f7f\u3063\u3066\u30c7\u30fc\u30bf\u62e1\u5f35\n    dataset = AUG_DATASET(train_df_he, transform = get_transforms())\n    dataloader = Data.DataLoader(dataset=dataset, num_workers=WORKER, batch_size=BATCH, shuffle=False, drop_last=False,\\\n                               pin_memory = False)\n    \n    for aug_img, aug_box, image_name in progress_bar(dataloader):\n        for idx, image in enumerate(aug_img):\n            name = image_name[idx]\n            new_name = \"{}_HE\".format(name)\n            image = aug_img[idx]\n            box = aug_box[idx]\n            \n            path_txt = LABEL_DIR + \"\/\" + new_name + \".txt\"\n            path_jpg = IMAGE_DIR + \"\/\" + new_name + \".jpg\"\n            is_path = os.path.exists(path_jpg)\n            image = image.numpy()\n            cv2.imwrite(path_jpg, image[...,::-1])\n            txt_file = open(path_txt, \"w\")\n            txt_file.write(box)\n            txt_file.close()\n        break\n    \n    # train_df\u306bDA\u3057\u305f\u884c\u3092\u8ffd\u52a0\n    func_he_path = lambda x: '{}_HE'.format(x)\n    train_df_he.image_path = train_df_he.image_path.map(func_he_path) # image_path\u3092\u66f4\u65b0\n    train_df_he.label_path = train_df_he.label_path.map(func_he_path) # label_path\u3092\u66f4\u65b0\n    train_df = pd.concat([train_df, train_df_he], axis=0, ignore_index=True) #index\u518d\u5ea6\u964d\u308a\u76f4\u3057","ff031b16":"len(train_df.image_path.unique())","5d913c54":"import yaml\n\ncwd = '\/kaggle\/working\/'\n\nwith open(os.path.join( cwd , 'train.txt'), 'w') as f:\n    for path in train_df.image_path.tolist():\n        f.write(path+'\\n')\n            \nwith open(os.path.join(cwd , 'val.txt'), 'w') as f:\n    for path in valid_df.image_path.tolist():\n        f.write(path+'\\n')\n\ndata = dict(\n    path  = '\/kaggle\/working',\n    train =  os.path.join( cwd , 'train.txt') ,\n    val   =  os.path.join( cwd , 'val.txt' ),\n    nc    = 1,\n    names = ['cots'],\n    )\n\nwith open(os.path.join( cwd , 'gbr.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(os.path.join( cwd , 'gbr.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","797d38a0":"%%writefile \/kaggle\/working\/hyp.yaml\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum\/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+\/- deg)\ntranslate: 0.10  # image translation (+\/- fraction)\nscale: 0.5  # image scale (+\/- gain)\nshear: 0.0  # image shear (+\/- deg)\nperspective: 0.0  # image perspective (+\/- fraction), range 0-0.001\nflipud: 0.5  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 0.5  # image mosaic (probability)\nmixup: 0.5 # image mixup (probability)\ncopy_paste: 0.0  # segment copy-paste (probability)","c9b1b0e8":"%cd \/kaggle\/working\n!rm -r \/kaggle\/working\/yolov5\n# !git clone https:\/\/github.com\/ultralytics\/yolov5 # clone\n!cp -r \/kaggle\/input\/yolov5-lib-ds \/kaggle\/working\/yolov5\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nfrom yolov5 import utils\ndisplay = utils.notebook_init()  # check","841b0b84":"!python train.py --img {DIM}\\\n--batch {BATCH}\\\n--epochs {EPOCHS}\\\n--data \/kaggle\/working\/gbr.yaml\\\n--hyp \/kaggle\/working\/hyp.yaml\\\n--weights {MODEL}.pt\\\n--optimizer {OPTIM}\\\n--project {PROJECT} --name {NAME}\\\n--exist-ok","44e83fbb":"OUTPUT_DIR = '{}\/{}'.format(PROJECT, NAME)\n!ls {OUTPUT_DIR}","53de8eb4":"plt.figure(figsize = (10,10))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}\/labels_correlogram.jpg'));","9d63692d":"plt.figure(figsize = (10,10))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}\/labels.jpg'));","8943f74f":"import matplotlib.pyplot as plt\nplt.figure(figsize = (10, 10))\nplt.imshow(plt.imread(f'{OUTPUT_DIR}\/train_batch0.jpg'))\n\nplt.figure(figsize = (10, 10))\nplt.imshow(plt.imread(f'{OUTPUT_DIR}\/train_batch1.jpg'))\n\nplt.figure(figsize = (10, 10))\nplt.imshow(plt.imread(f'{OUTPUT_DIR}\/train_batch2.jpg'))","b3b55d6a":"fig, ax = plt.subplots(3, 2, figsize = (2*9,3*5), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'{OUTPUT_DIR}\/val_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'{OUTPUT_DIR}\/val_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'{OUTPUT_DIR}\/val_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'{OUTPUT_DIR}\/val_batch{row}_pred.jpg', fontsize = 12)\nplt.show()","fc8a49b1":"plt.figure(figsize=(30,15))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}\/results.png'));","aebc6d48":"plt.figure(figsize=(12,10))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}\/confusion_matrix.png'));","c4b4bdc8":"for metric in ['F1', 'PR', 'P', 'R']:\n    print(f'Metric: {metric}')\n    plt.figure(figsize=(12,10))\n    plt.axis('off')\n    plt.imshow(plt.imread(f'{OUTPUT_DIR}\/{metric}_curve.png'));\n    plt.show()","ae825b49":"!rm -r {IMAGE_DIR}\n!rm -r {LABEL_DIR}","f55d348e":"\u5b9f\u969b\u306bbbox\u306e\u5b58\u5728\u3059\u308b\u753b\u50cf\u3092\u898b\u3066\u307f\u308b","1c55ceb9":"# \ud83d\udccc Key-Points\n* \u63d0\u4f9b\u3055\u308c\u3066\u3044\u308bpython\u6642\u7cfb\u5217API\u3092\u4f7f\u7528\u3057\u3066\u4e88\u6e2c\u3092\u9001\u4fe1\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u3053\u306e\u30b3\u30f3\u30c6\u30b9\u30c8\u306f\u4ee5\u524d\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u691c\u51fa\u30b3\u30f3\u30c6\u30b9\u30c8\u3068\u306f\u7570\u306a\u308a\u307e\u3059\u3002\n* \u5404\u4e88\u6e2c\u884c\u306b\u306f\u3001\u753b\u50cf\u306e\u3059\u3079\u3066\u306e\u5883\u754c\u30dc\u30c3\u30af\u30b9\u3092\u542b\u3081\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u63d0\u51fa\u306f\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3082COCO\u306e\u3088\u3046\u3067\u3059\u3002\u3053\u308c\u306f`[x_min\u3001y_min\u3001\u5e45\u3001\u9ad8\u3055]`\u3092\u610f\u5473\u3057\u307e\u3059\n* Copmetition\u30e1\u30c8\u30ea\u30c3\u30afF2\u306f\u3001\u30d2\u30c8\u30c7\u3092\u898b\u9003\u3059\u3053\u3068\u304c\u307b\u3068\u3093\u3069\u306a\u3044\u3053\u3068\u3092\u4fdd\u8a3c\u3059\u308b\u305f\u3081\u306b\u3001\u3044\u304f\u3064\u304b\u306e\u8aa4\u691c\u77e5\uff08FP\uff09\u3092\u8a31\u5bb9\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u8aa4\u691c\u77e5\uff08FN\uff09\u306f\u3001\u8aa4\u691c\u77e5\uff08FP\uff09\u3088\u308a\u3082\u91cd\u8981\u3067\u3059\u3002\n$$F2 = 5 \\cdot \\frac{precision \\cdot recall}{4\\cdot precision + recall}$$","22686954":"# \ud83d\udcc8 Class Distribution","bb061ed7":"\u6307\u5b9a\u3057\u305f\u5834\u6240`kaggle\/image\u3068\u304blabel`\u306b\u30d5\u30a1\u30a4\u30eb\u304c\u3067\u304d\u306a\u3044\u305e\uff1f\uff1f\uff1f","4cd161e5":"# \ud83d\udce6 [YOLOv5](https:\/\/github.com\/ultralytics\/yolov5\/)","c1705622":"\u3053\u308c\u304c\u30e9\u30d9\u30eb\uff1f3\u306e\u3068\u3053\u306b\u306fbbox\u306e\u6570\u304c\u5165\u308b","1ef15e3d":"\u3084\u308b\u4e8b\n- yolov5\u3068\u306f\u4f55\u304b\u8abf\u67fb\n    - \u4f55\u3092\u5b66\u7fd2\u3059\u308b\u306e\u304b\uff1f\n    - \u7cbe\u5ea6\u3068\u304b\u306f\u3069\u3046\u3084\u3063\u3066\u51fa\u3059\u306e\u304b\uff1f\n    - \u898b\u3064\u3051\u305f\u3044\u3082\u306e\u304c\u6620\u3063\u3066\u306a\u3044\u30c7\u30fc\u30bf\u306f\u5b66\u7fd2\u306b\u542b\u3081\u3066\u3088\u3044\u306e\u304b\uff1f\n- \u3044\u3089\u306a\u3044\u90e8\u5206\u3092\u524a\u3063\u3066\u30b7\u30f3\u30d7\u30eb\u306b\u3059\u308b\n- W&B\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3066\u5b66\u7fd2\u7d4c\u904e\u306e\u30b0\u30e9\u30d5\u306a\u3069\u3092\u898b\u308b\n    - ~\u3068\u3044\u3046\u304b\u3001\u7d30\u304b\u3044\u51e6\u7406\u3068\u304bwandb\u306b\u767b\u9332\u3057\u3066\u3053\u3063\u3061\u3067\u3084\u3063\u3066\u305d\u3046~\n- \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3059\u308b\u30e2\u30c7\u30eb5\u500b\u4f5c\u3063\u3066\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb","0a5b4854":"\u308f\u304b\u3063\u305f\u3053\u3068\n- yolov5\u306b\u3064\u3044\u3066\n    - \u5b66\u7fd2\u306b\u306ftrain.py\u3068\u3044\u3046\u5143\u3005\u3042\u308b\u30b3\u30fc\u30c9\u3092\u4f7f\u3063\u3066\u3044\u308b\n    - \u305d\u308c\u306b\u5f15\u6570\u3068\u3057\u3066\u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u30d1\u30b9\u3084\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u60c5\u5831\u304c\u8a18\u8ff0\u3055\u308c\u305f.yaml\u30d5\u30a1\u30a4\u30eb\u3092\u4e0e\u3048\u3066\u3044\u308b","d90dd506":"\u3067\u304d\u305f\u753b\u50cf\u306f`working\/images\/{video_id}-{video_frame}-aug.jpg`\u306b\u5165\u308c\u308b\u3002\n\n\u305d\u306e\u753b\u50cf\u306b\u5bfe\u3059\u308b\u30e9\u30d9\u30eb\u306f\u5143\u753b\u50cf\u306elabel.txt\u304b\u3089\u6d41\u7528`working\/labels\/{video_id}-{video_frame}-aug.txt`\u306b\u5165\u308c\u308b\u3002\n\ntrain_df\u306b\u884c\u3092\u8ffd\u52a0\u3002(\u5143\u753b\u50cf\u306e\u884c\u3092\u30b3\u30d4\u30fc\u3002image_path, label_path\u3092\u2191\u306e\u3082\u306e\u306b\u5909\u3048\u308c\u3070OK\u306e\u306f\u305a)\n\n---\n\n\u6700\u521d\u304b\u3089train_df\u3092\u30b3\u30d4\u30fc\u3059\u308b\u3002=tran_aug_df\n\ntrain_aug_df\u306b\u5bfe\u3057\u3066DA\u3092\u304b\u3051\u308b\n\n\u540c\u3058train\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4fdd\u5b58\u3059\u308c\u3070ok","f8bcd671":"## Get Image-Size\n> All Images have same dimension, [Width, Height] =  `[1280, 720]`","4b5ec584":"## Please Upvote if you find this Helpful","8c60097a":"# \ud83c\udff7\ufe0f Create Labels\nWe need to export our labels to **YOLO** format, with one `*.txt` file per image (if no objects in image, no `*.txt` file is required). The *.txt file specifications are:\n\n* One row per object\n* Each row is class `[x_center, y_center, width, height]` format.\n* Box coordinates must be in **normalized** `xywh` format (from `0 - 1`). If your boxes are in pixels, divide `x_center` and `width` by `image width`, and `y_center` and `height` by `image height`.\n* Class numbers are **zero-indexed** (start from `0`).\n\n> Competition bbox format is **COCO** hence `[x_min, y_min, width, height]`. So, we need to convert form **COCO** to **YOLO** format.\n\n\u5404\u753b\u50cf\u306b\u5bfe\u3057\u3066.txt\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u3063\u3066YOLO\u306b\u5bfe\u5fdc\u3059\u308b\u5f62\u5f0f\u306b\u3059\u308b\n\n* \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u4e00\u3064\u306b\u3064\u304d1\u884c\n* \u5404\u884c\u4ee5\u4e0b\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8`[x_center, y_center, width, height]`\n* box\u5ea7\u6a19\u306f**0-1\u3067\u6b63\u898f\u5316\u3055\u308c\u305f**xywh\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3002\u30d4\u30af\u30bb\u30eb\u5358\u4f4d\u306e\u5834\u5408\u306f\u3001 `x_center` \u3068 `width` \u3092 `image width` \u3067\u5272\u3063\u3066\u3001 `y_center` \u3068 `height` \u3092 `image height` \u3067\u5272\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n* \u30af\u30e9\u30b9\u756a\u53f7\u306f **0\u304b\u3089\u59cb\u307e\u308b\u30bc\u30ed\u30fb\u30a4\u30f3\u30c7\u30c3\u30af\u30b9** \u3067\u3059\u3002\n\n> \u30b3\u30f3\u30da\u306ebbox\u5f62\u5f0f\u306fCOCO\u3067\u3042\u308b\u305f\u3081\u3001[x_min\u3001y_min\u3001width\u3001height]\u3067\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001\u30d5\u30a9\u30fc\u30e0COCO\u3092YOLO\u5f62\u5f0f\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002","02ea6129":"# \ud83d\udcc1 Create Folds\n> Number of samples aren't same in each fold which can create large variance in **Cross-Validation**.\n\n> \u5404\u30d5\u30a9\u30fc\u30eb\u30c9\u306e\u30b5\u30f3\u30d7\u30eb\u6570\u304c\u540c\u3058\u3067\u306a\u3044\u305f\u3081\u3001\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3067\u5927\u304d\u306a\u3070\u3089\u3064\u304d\u304c\u751f\u3058\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\n\nGroupKFold: \u540c\u3058\u30b0\u30eb\u30fc\u30d7\u304c\u7570\u306a\u308b\u5206\u5272\u30d1\u30bf\u30fc\u30f3\u306b\u51fa\u73fe\u3057\u306a\u3044\u3088\u3046\u306b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u5206\u5272\u3059\u308b\u3002\n\u53c2\u8003\uff1ahttps:\/\/upura.hatenablog.com\/entry\/2018\/12\/04\/224436\n\n> \u30af\u30e9\u30b9\u3068\u306f\u5225\u306e\u6982\u5ff5\u3068\u3057\u3066\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u5168\u4f53\u306f\u5747\u7b49\u306a10\u30b0\u30eb\u30fc\u30d7\u306b\u5206\u5272\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u30b0\u30eb\u30fc\u30d7\u306f\u306a\u304b\u306a\u304b\u30a4\u30e1\u30fc\u30b8\u304c\u4ed8\u304d\u3065\u3089\u3044\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u4f8b\u3048\u3070\u300c\u540c\u3058\u30e6\u30fc\u30b6\u306e\u30c7\u30fc\u30bf\u3092\u4e00\u3064\u306e\u30b0\u30eb\u30fc\u30d7\u306b\u307e\u3068\u3081\u3066\u304a\u304f\u300d\u3068\u3044\u3063\u305f\u4f7f\u3044\u65b9\u304c\u60f3\u5b9a\u3067\u304d\u307e\u3059\u3002**\u540c\u3058\u30e6\u30fc\u30b6\u306e\u30c7\u30fc\u30bf\u304ctrain\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3068validation\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e21\u8005\u306b\u5b58\u5728\u3059\u308b\u3068\u3001\u4e0d\u5f53\u306b\u7cbe\u5ea6\u304c\u9ad8\u304f\u306a\u308b\u6050\u308c\u304c\u3042\u308b**\u305f\u3081\u3067\u3059\u3002\n\n\u4eca\u56de\u306f\u52d5\u753b\u306e\u6570(`len(df['sequence'].unique())`\u306e\u4e8b)","ce9cff78":"## Create Directories","a86c3635":"shutil.copyfile(src, dst, *, follow_symlinks=True)\n\nsrc \u3068\u3044\u3046\u540d\u524d\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u5185\u5bb9 (\u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u542b\u307e\u306a\u3044) \u3092 dst \u3068\u3044\u3046\u540d\u524d\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u30b3\u30d4\u30fc\u3057\u3001\u6700\u3082\u52b9\u7387\u7684\u306a\u65b9\u6cd5\u3067 dst \u3092\u8fd4\u3057\u307e\u3059\u3002 src \u3068 dst \u306f path-like object \u307e\u305f\u306f\u6587\u5b57\u5217\u3067\u30d1\u30b9\u540d\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002","4e3319bd":"## Score Vs Epoch","e385f750":"## \u5f53\u305f\u308a\u524d\u306b\u3084\u3089\u308c\u3066\u3044\u308b\u3053\u3068\n- [ ] Data\u306e\u4fee\u6b63\u30fb\u8ffd\u52a0\n    - [ ] \u30b5\u30a4\u30ba\u304c\u5927\u304d\u3059\u304e\u308b\u30fb\u5c0f\u3055\u3059\u304e\u308bbbox\u3092\u7121\u8996\uff08\u5b9f\u88c5\uff09\n    - [ ] \u624b\u4f5c\u696d\u3067bbox\u3092\u8ffd\u52a0\uff08\u89e3\u8aac\uff09\n- [ ] Data Augmentation\n    - [ ] Albumentations\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u3063\u305faugmentation\uff08RandomSizedCrop, HueSaturationValue, RandomBrightnessContrast, ToGray, HorizontalFlip, VerticalFlip, Cutout, etc\uff09\n    - [ ] mixup\uff08\u753b\u50cf\u306e\u5408\u6210\u3002\u5b9f\u88c5\uff09\n    - [ ] cutmix\uff08cutout+mixup: cutout\u3057\u305f\u90e8\u5206\u306b\u5225\u753b\u50cf\u3092\u5408\u6210\u3002\u5b9f\u88c5 \uff09\n    - [ ] \u30b8\u30b0\u30bd\u30fc\u30d1\u30ba\u30eb\u306b\u3088\u308b\u753b\u50cf\u751f\u6210\uff08\u65e2\u8ff0\uff09\n- [ ] \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u9078\u629e\n    - [ ] YOLO\uff08\u30b3\u30f3\u30da\u53c2\u52a0\u76f4\u5f8c\u306b\u89e6\u3063\u3066\u3044\u305f\u3002v5\u306f\u30e9\u30a4\u30bb\u30f3\u30b9\u306e\u554f\u984c\u3067\u4f7f\u7528\u7981\u6b62\u306b\u3002\u5358\u72ec\u3067\u306f\u304a\u305d\u3089\u304f\u6700\u9ad8\u7cbe\u5ea6\u304c\u51fa\u305b\u308b\u30e2\u30c7\u30eb\u3060\u3063\u305f\uff09\n    - [ ] EfficientDet\uff08YOLOv5\u304c\u7981\u6b62\u306b\u306a\u3063\u3066\u304b\u3089\u306f\u3072\u305f\u3059\u3089D5\u3092\u4e2d\u5fc3\u306bEfficientDet\u3067\u5b9f\u9a13\u3057\u3066\u3044\u305f\u3002EfficientNet\u306e\u8003\u3048\u65b9\u3092\u53d6\u308a\u5165\u308c\u305f\u7269\u4f53\u691c\u51fa\u30e2\u30c7\u30eb\u3002\u5b9f\u88c5\uff09\n    - [ ] \u4ed6\u306f\u8a66\u3057\u3066\u306a\u3044\u304c\u3001DetectorRS\u3084UniverseNet\u304c\u826f\u3044\u306a\u3069\u306e\u5831\u544a\u3042\u308a\n- [ ] \u9ad8\u89e3\u50cf\u5ea6\u3067\u5b66\u7fd2\n    - [ ] \u30ea\u30b5\u30a4\u30ba\u3092\u884c\u308f\u305a1024 x 1024\u306e\u753b\u50cf\u3067\u5b66\u7fd2\uff08Colab Pro\u3067\u306fbatch size 1\u3067\u30ae\u30ea\u30ae\u30eaCUDA out of memory\u3092\u56de\u907f\u3067\u304d\u308b\uff09\n- [x] TTA\uff08\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3082augmentation\u3002\u5b9f\u88c5\uff09\n- [ ] Pseudo Labeling (\u30c6\u30fc\u30d6\u30eb\u30c7\u30fc\u30bf\u3067\u3082\u304a\u99b4\u67d3\u307f\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u4e88\u6e2c\u3057\u78ba\u4fe1\u5ea6\u306e\u9ad8\u3044\u30e9\u30d9\u30eb\u306e\u307f\u8a13\u7df4\u30c7\u30fc\u30bf\u306b\u53d6\u308a\u5165\u308c\u3066\u518d\u4e88\u6e2c\u3002\u5b9f\u88c5)\n- [ ] Ensemble (\u7cbe\u5ea6\u3092\u6c42\u3081\u308bKaggle\u3067\u306fWBF\u304c\u5f37\u3044\u5834\u5408\u304c\u591a\u305d\u3046\u3002\u5b9f\u88c5, \u89e3\u8aac)\n    - [ ] NMS\uff08IoU\u304c\u3042\u308b\u95be\u5024\u3092\u8d85\u3048\u3066\u91cd\u306a\u3063\u3066\u3044\u308bbbox\u306e\u96c6\u5408\u304b\u3089\u3001\u30b9\u30b3\u30a2\u304c\u6700\u5927\u306ebbox\u3092\u6b8b\u3057\u3066\u3001\u305d\u308c\u4ee5\u5916\u3092\u9664\u53bb\uff09\n    - [ ] SoftNMS\uff08IoU\u95be\u5024\u3092\u8d85\u3048\u305fbbox\u3092\u6b8b\u3057\u3064\u3064\u3001\u30b9\u30b3\u30a2\u304c\u6700\u5927\u306ebbox\u4ee5\u5916\u3082\u9664\u53bb\u305b\u305a\u3001\u30b9\u30b3\u30a2\u3092\u5272\u308a\u5f15\u3044\u3066\u6b8b\u3059\uff09\n    - [ ] NMW (\u91cd\u306a\u308a\u3042\u3063\u305fbbox\u3092\u30b9\u30b3\u30a2\u3068IoU\u3067\u91cd\u307f\u4ed8\u3051\u3057\u3066\u8db3\u3057\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u30011\u3064\u306e\u65b0\u305f\u306abbox\u3092\u4f5c\u308a\u51fa\u3059)\n    - [ ] WBF\uff08\u691c\u51fa\u3055\u308c\u305f\u30e2\u30c7\u30eb\u306e\u6570\u304c\u5c11\u306a\u3044bbox\u307b\u3069\u30b9\u30b3\u30a2\u3092\u4e0b\u3052\u308b\u3053\u3068\u3067\u3001\u5c11\u6570\u306e\u30e2\u30c7\u30eb\u3060\u3051\u3067\u691c\u51fa\u3055\u308c\u305fbbox\u3092\u30b9\u30b3\u30a2\u3067\u8db3\u5207\u308a\u3059\u308b\uff09","686f4ca3":"## train.txt, val.txt\u3092\u4f5c\u308b(\u5b66\u7fd2\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u30d1\u30b9\u304c\u8a18\u8ff0\u3055\u308c\u3066\u3044\u308b)","81c0df16":"# \u2699\ufe0f Configuration\nThe dataset config file requires\n1. The dataset root directory path and relative paths to `train \/ val \/ test` image directories (or *.txt files with image paths)\n2. The number of classes `nc` and \n3. A list of class `names`:`['cots']`\n\n\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3082\u306e\u304c\u5fc5\u8981\u3067\u3059\u3002\n1. 1. \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30d1\u30b9\u3068\uff0c`train \/ val \/ test` \u753b\u50cf\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u76f8\u5bfe\u30d1\u30b9 (\u307e\u305f\u306f\u753b\u50cf\u30d1\u30b9\u3092\u542b\u3080 *.txt \u30d5\u30a1\u30a4\u30eb)\n2. \u30af\u30e9\u30b9\u6570 `nc` \u3068 \n3. \u30af\u30e9\u30b9\u540d`:`['cots']`\u306e\u30ea\u30b9\u30c8\u3002","b1be7d8f":"## GT Vs Pred","4d57e44a":"iterrows()\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u3046\u3068\u30011\u884c\u305a\u3064\u3001\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u540d\uff08\u884c\u540d\uff09\u3068\u305d\u306e\u884c\u306e\u30c7\u30fc\u30bf\uff08pandas.Series\u578b\uff09\u306e\u30bf\u30d7\u30eb(index, Series)\u3092\u53d6\u5f97\u3067\u304d\u308b\u3002","4fe9c0c2":"# \ud83d\ude85 Training","84aeb602":"## Output Files","4d6a1c84":"class AUGDATA\u3000\u306eimage_path\u5468\u308a\u306e\u8a2d\u5b9a\u3044\u3058\u308b\u5fc5\u8981\u3042\u308a","8e7fcd1e":"# \u270f\ufe0f Write Images\n* We need to copy the Images to Current Directory(`\/kaggle\/working`) as `\/kaggle\/input` doesn't have **write access** which is needed for **YOLOv5**.\n* We can make this process faster using **Joblib** which uses **Parallel** computing.\n\n* \/ kaggle \/ input\u306b\u306fYOLOv5\u306b\u5fc5\u8981\u306a\u66f8\u304d\u8fbc\u307f\u30a2\u30af\u30bb\u30b9\u6a29\u304c\u306a\u3044\u305f\u3081\u3001\u30a4\u30e1\u30fc\u30b8\u3092\u73fe\u5728\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\uff08\/ kaggle \/ working\uff09\u306b\u30b3\u30d4\u30fc\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n* \u3053\u306e\u51e6\u7406\u3092\u9ad8\u901f\u5316\u3059\u308b\u306b\u306f\u3001**\u4e26\u5217**\u8a08\u7b97\u3092\u5229\u7528\u3059\u308b**Joblib**\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002","17ffb256":"<img src=\"https:\/\/www.pngall.com\/wp-content\/uploads\/2018\/04\/Under-Construction-PNG-File.png\">","d49763d9":"## Create BBox","f549d2ae":"train_df\u306b\u5bfe\u3057\u3066DA\u3092\u304b\u3051\u308b\n\n\u4eca\u56de\u306fbbox\u306e\u4f4d\u7f6e\u304c\u5909\u308f\u308b\u51e6\u7406\u306f\u3057\u306a\u3044(label.txt\u3092\u6d41\u7528\u3057\u305f\u3044\u305f\u3081)\n\n\u3067\u304d\u305f\u753b\u50cf\u306f`working\/images\/{video_id}-{video_frame}-aug.jpg`\u306b\u5165\u308c\u308b\u3002\n\n\u305d\u306e\u753b\u50cf\u306b\u5bfe\u3059\u308b\u30e9\u30d9\u30eb\u306f\u5143\u753b\u50cf\u306elabel.txt\u304b\u3089\u6d41\u7528`working\/labels\/{video_id}-{video_frame}-aug.txt`\u306b\u5165\u308c\u308b\u3002\n\ntrain_df\u306b\u884c\u3092\u8ffd\u52a0\u3002(\u5143\u753b\u50cf\u306e\u884c\u3092\u30b3\u30d4\u30fc\u3002image_path, label_path\u3092\u2191\u306e\u3082\u306e\u306b\u5909\u3048\u308c\u3070OK\u306e\u306f\u305a)","c2a30a89":"\u30a2\u30a4\u30c7\u30a2\n\n- \u30d2\u30c8\u30c7\u304c\u6620\u3063\u3066\u306a\u3044\u753b\u50cf\u3082\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u3057\u305f\u3089\u30c0\u30e1\u304b\uff1f\n    - \u5168\u90e8\u4f7f\u3046\u3068\u307b\u3068\u3093\u3069\u30d2\u30c8\u30c7\u304c\u6620\u3063\u3066\u306a\u3044\u3001\u3068\u5224\u5b9a\u3057\u305d\u3046\n        - \u540c\u3058\u679a\u6570(5k)\u3060\u3063\u305f\u3089\u3044\u3044\u304b\u3082\uff1f\n- \u30e2\u30c7\u30eb\u3092\u5909\u3048\u308b\n    - YoloX\n    - EfficientDet\n    - FasterRCNN\n    - DETR\n- nofair tracking\n- \u30b3\u30f3\u30c8\u30e9\u30b9\u30c8\u5747\u7b49\u5316\u3059\u308b\n- Augmentation\u3092\u3057\u306a\u3044\n    - \u30e1\u30e2\u30ea\u7bc0\u7d04\u306e\u305f\u3081\n- Adam\u3092\u4f7f\u3046","c91057e5":"# \u2b50 WandB","547c7861":"# \ud83e\uddf9 Clean Data\n* In this notebook, we use only **bboxed-images** (`~5k`). We can use all `~23K` images for train but most of them don't have any labels. So it would be easier to carry out experiments using only **bboxed images**.\n* \u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001bboxed-images\uff08\u301c5k\uff09\u306e\u307f\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002train\u306b\u306f\u7d0423K\u306e\u753b\u50cf\u3092\u3059\u3079\u3066\u4f7f\u7528\u3067\u304d\u307e\u3059\u304c\u3001\u307b\u3068\u3093\u3069\u306e\u753b\u50cf\u306b\u306f\u30e9\u30d9\u30eb\u304c\u3042\u308a\u307e\u305b\u3093\u3002\u3057\u305f\u304c\u3063\u3066\u3001bboxed\u753b\u50cf\u306e\u307f\u3092\u4f7f\u7528\u3057\u3066\u5b9f\u9a13\u3092\u5b9f\u884c\u3059\u308b\u65b9\u304c\u7c21\u5358\u3067\u3059","c72e42e8":"# [Tensorflow - Help Protect the Great Barrier Reef](https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef)\n> Detect crown-of-thorns starfish in underwater image data\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/31703\/logos\/header.png?t=2021-10-29-00-30-04\">","fc27b80f":"## label.txt\u3092\u4f5c\u6210","4a103b86":"# \ud83d\udcd6 Meta Data\n* `train_images\/` - Folder containing training set photos of the form `video_{video_id}\/{video_frame}.jpg`.\n\n* `[train\/test].csv` - \u753b\u50cf\u306e\u30e1\u30bf\u30c7\u30fc\u30bf\u3067\u3059\u3002\u4ed6\u306e\u30c6\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u3068\u540c\u69d8\u306b\u3001\u30c6\u30b9\u30c8\u306e\u30e1\u30bf\u30c7\u30fc\u30bf\u30c7\u30fc\u30bf\u306e\u307b\u3068\u3093\u3069\u306f\u3001\u63d0\u51fa\u6642\u306b\u3057\u304b\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306b\u8868\u793a\u3055\u308c\u307e\u305b\u3093\u3002\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3067\u304d\u308b\u306e\u306f\u6700\u521d\u306e\u6570\u884c\u3060\u3051\u3067\u3059\u3002\n\n* `video_id` - \u753b\u50cf\u304c\u542b\u307e\u308c\u308b\u30d3\u30c7\u30aa\u306eID\u756a\u53f7\u3002\u30d3\u30c7\u30aaID\u306f\u610f\u5473\u306e\u3042\u308b\u9806\u5e8f\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n* `video_frame` - \u6620\u50cf\u5185\u306e\u753b\u50cf\u306e\u30d5\u30ec\u30fc\u30e0\u756a\u53f7\u3067\u3059\u3002\u30c0\u30a4\u30d0\u30fc\u304c\u6d6e\u4e0a\u3057\u305f\u3068\u304d\u304b\u3089\u30d5\u30ec\u30fc\u30e0\u756a\u53f7\u306b\u305a\u308c\u304c\u751f\u3058\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\n* `sequence` - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\u6307\u5b9a\u3055\u308c\u305f\u30d3\u30c7\u30aa\u306e\u30ae\u30e3\u30c3\u30d7\u30d5\u30ea\u30fc\u90e8\u5206\u96c6\u5408\u306eID\u3002\u30b7\u30fc\u30b1\u30f3\u30b9ID\u306f\u610f\u5473\u306e\u3042\u308b\u9806\u5e8f\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n* `sequence_frame` - \u6307\u5b9a\u3055\u308c\u305f\u30b7\u30fc\u30b1\u30f3\u30b9\u5185\u306e\u30d5\u30ec\u30fc\u30e0\u756a\u53f7\u3002\n* `image_id` - ID code for the image, in the format `{video_id}-{video_frame}`\n* `annotations` - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate `(x_min, y_min)` of its lower left corner within the image together with its `width` and `height` in pixels --> (COCO format).Python\u3067\u76f4\u63a5\u8a55\u4fa1\u53ef\u80fd\u306a\u6587\u5b57\u5217\u5f62\u5f0f\u306e\u30d2\u30c8\u30c7\u691c\u51fa\u306e\u30d0\u30a6\u30f3\u30c7\u30a3\u30f3\u30b0\u30dc\u30c3\u30af\u30b9\u3002\u63d0\u51fa\u3059\u308b\u4e88\u6e2c\u5024\u3068\u540c\u3058\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002test.csv\u3067\u306f\u5229\u7528\u3067\u304d\u307e\u305b\u3093\u3002\u30d0\u30a6\u30f3\u30c7\u30a3\u30f3\u30b0\u30dc\u30c3\u30af\u30b9\u306f\u3001\u753b\u50cf\u5185\u306e\u5de6\u4e0b\u9685\u306e\u30d4\u30af\u30bb\u30eb\u5ea7\u6a19 `(x_min, y_min)` \u3068\u3001\u30d4\u30af\u30bb\u30eb\u5358\u4f4d\u306e `width` \u3068 `height` \u3067\u8a18\u8ff0\u3055\u308c\u308b --> (COCO \u5f62\u5f0f)\u3002","ed10878f":"# \u2728 Overview","102f9b4b":"## \u30d5\u30a1\u30a4\u30eb\u69cb\u9020\n\nworking\/labels\/\u4ee5\u4e0b\u306b\u3059\u3079\u3066\u306e\u30d5\u30a1\u30a4\u30eb\u306elabel.txt\u304c\u5165\u3063\u3066\u3044\u308b\n\nimages\u3092\u540c\u3058","e110ea11":"# \ud83d\udd2d Batch Image","2139b65b":"# **Data Augmentation**","2adf258a":"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold\n\n> class sklearn.model_selection.GroupKFold(n_splits=5)[source]\n>\n> \u30aa\u30fc\u30d0\u30fc\u30e9\u30c3\u30d7\u3057\u306a\u3044\u30b0\u30eb\u30fc\u30d7\u3092\u6301\u3064K-fold\u30a4\u30c6\u30ec\u30fc\u30bf\u306e\u5909\u5f62\u3002\n>\n> \u540c\u3058\u30b0\u30eb\u30fc\u30d7\u304c2\u3064\u306e\u7570\u306a\u308b\u30d5\u30a9\u30fc\u30eb\u30c9\u306b\u73fe\u308c\u308b\u3053\u3068\u306f\u3042\u308a\u307e\u305b\u3093\uff08\u7570\u306a\u308b\u30b0\u30eb\u30fc\u30d7\u306e\u6570\u306f\u3001\u5c11\u306a\u304f\u3068\u3082\u30d5\u30a9\u30fc\u30eb\u30c9\u306e\u6570\u3068\u540c\u3058\u3067\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\uff09\u3002\n>\n> \u5404\u30d5\u30a9\u30fc\u30eb\u30c9\u306f\u3001\u305d\u308c\u305e\u308c\u306e\u30d5\u30a9\u30fc\u30eb\u30c9\u3067\u7570\u306a\u308b\u30b0\u30eb\u30fc\u30d7\u306e\u6570\u304c\u307b\u307c\u540c\u3058\u3068\u3044\u3046\u610f\u5473\u3067\u3001\u307b\u307c\u30d0\u30e9\u30f3\u30b9\u304c\u53d6\u308c\u3066\u3044\u307e\u3059\u3002","165f14b3":"\u4e26\u5217\u51e6\u7406\n\n```\njoblib.Parallel(<Parallel\u3078\u306e\u5f15\u6570>)(\n    joblib.delayed(<\u5b9f\u884c\u3059\u308b\u95a2\u6570>)(<\u95a2\u6570\u3078\u306e\u5f15\u6570>) for \u5909\u6570\u540d in \u30a4\u30c6\u30e9\u30d6\u30eb\n)\n```","e7cc6964":"## Metrics","4e507020":"# \ud83d\udd0d Result","4286b962":"pandas.eval()\n\n\u69d8\u3005\u306a\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001Python\u5f0f\u3092\u6587\u5b57\u5217\u3068\u3057\u3066\u8a55\u4fa1\u3057\u307e\u3059\u3002","42ad3b39":"# \ud83d\udd28 Helper","b00373a0":"# \u2702\ufe0f Remove Files","f9bf281b":"## Get Paths","76f0a4a7":"# \ud83d\udee0 Install Libraries","38e4e502":"# \ud83d\udcda Import Libraries","be9d21d6":"`Error displaying widget: model not found`\n\n\u4f55\u3089\u304b\u306e\u539f\u56e0\u3067progress\u3092\u51fa\u305b\u306a\u3044\u306e\u304b\u3082\u3002\u30e2\u30c7\u30eb\u304c\u306a\u3044\u3001\u3068\u306f\u3069\u3046\u3044\u3046\u3053\u3068\uff1f","1cff2f7c":"coco => **[xmin, ymin, w, h]**\n\nvoc  => **[xmin, ymin, xmax, ymax]**\n\nyolo => **[xmid, ymid, w, h]** (normalized)\n\n```\ndef clip_bbox(bboxes_voc, height=720, width=1280):\n\n    Clip bounding boxes to image boundaries.\n    Args:\n        bboxes_voc (np.ndarray): bboxes in [xmin, ymin, xmax, ymax] format.\n        height (int, optional): height of bbox. Defaults to 720.\n        width (int, optional): width of bbox. Defaults to 1280.\n    Returns:\n        np.ndarray : clipped bboxes in [xmin, ymin, xmax, ymax] format.\n```","bf068e67":"## Confusion Matrix","9b82eb13":"## Number of BBoxes\n> Nearly 80% images are without any bbox."}}