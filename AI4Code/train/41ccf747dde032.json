{"cell_type":{"0181c2c4":"code","fa3aeab0":"code","7813dda2":"code","a322c965":"code","ecfa5a43":"code","9ca45fdc":"code","3a53df78":"code","3d80415b":"code","a23b4c93":"code","653411c6":"code","d73df509":"code","659687d9":"code","54ec12bf":"code","a917e43d":"code","479cd33a":"code","6a928a34":"code","928f3119":"code","2cb8ee7a":"code","85a694fe":"code","7dbfcbb8":"code","ec015aac":"code","56118406":"code","c59f2d82":"code","edcebc2f":"code","0c960327":"code","dd7b33db":"code","fd3e16ac":"code","c166c45f":"code","b3176d4b":"code","1ea39c5e":"code","c100946b":"code","992a2716":"code","e9f526ad":"code","ec734ed0":"code","8c402f9d":"code","b196ca99":"code","a37e5601":"code","056c4d87":"code","45d10492":"code","4dba048b":"code","fc58bbba":"code","ce24b0ba":"code","c4f1c4a7":"code","5b740bd8":"code","45b18d4c":"code","451cf630":"code","66982ec6":"code","13e13074":"code","8a6e5537":"code","ef50883d":"code","175ff877":"code","185dd951":"code","97412e9a":"code","d46eac8b":"code","5b44ba40":"code","ce933ba3":"code","5cc183fe":"code","8355b406":"code","b69bc979":"code","d4af6633":"code","1d3caab8":"code","f6593f5d":"code","e5944d33":"code","ba7d74d0":"code","06cdbea5":"code","f97ad848":"code","7e1d901a":"code","69b3cc01":"code","df24fbc5":"code","b478e237":"code","2ad2bd45":"code","2411800e":"code","85578fcc":"code","32c21095":"code","18b11ae3":"code","1a0796fb":"code","dcd09586":"code","92925e2d":"code","23ba8d2d":"code","e4251bc4":"code","c3c9a353":"code","f0b3f579":"code","2ec1b8c1":"code","030fb054":"code","a047f1ac":"code","fea8b26f":"code","38378cb7":"code","fc222814":"code","00012479":"code","dff56dbd":"code","25903a32":"code","e5d94f87":"code","160372b3":"code","02d719e8":"code","036b889f":"code","799463ee":"code","7ee3b03e":"code","a92d8768":"code","42b615c3":"code","8d0ffdca":"code","bbeb7d86":"code","7ede2665":"code","ccb25354":"code","7debf8d5":"code","5e8bc98f":"code","7f1b9415":"code","d5b32e0a":"code","dcb628d4":"code","4aa79df1":"code","1084a2bf":"code","d6163664":"code","4b1f98b1":"code","b97d3b08":"code","bdc126da":"code","a156b312":"code","93be8359":"code","a4e2193d":"code","80aca8a1":"code","d50b8181":"code","ef06d9f1":"code","19a4689c":"code","55d076f7":"code","14b74028":"code","f96d9209":"code","ff319378":"code","cbaf60ba":"code","730d2647":"code","73d7657e":"code","b09fb801":"code","1725c0e9":"code","0ef0824a":"code","110fefb4":"code","f3d134ab":"code","e640b785":"code","735e58c2":"code","2cea0c18":"code","db4d73e2":"code","0c232b18":"markdown","ff982a61":"markdown","be98dc15":"markdown","b03e0ea1":"markdown","491fe28c":"markdown","beb9f901":"markdown","096f9e2b":"markdown"},"source":{"0181c2c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra  \nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa3aeab0":"bureau_balance=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/bureau_balance.csv')\ncash=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/POS_CASH_balance.csv')\nclient=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_train.csv')\nHomeCredit=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/HomeCredit_columns_description.csv')\ntest=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_test.csv')\nprevious=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/previous_application.csv')\ncredit=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/credit_card_balance.csv')\ninstallements=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/installments_payments.csv')\nbureau=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/bureau.csv')","7813dda2":"HomeCredit[HomeCredit[\"Table\"]==\"application_{train|test}.csv\"][HomeCredit[\"Row\"]==\"EXT_SOURCE_3\"]['Description']","a322c965":"import random \nL=list(np.arange(len(client)))\nrandomList=random.sample(L,int(len(L)\/10))\ndata=client.iloc[np.setdiff1d(list(client.index),randomList)]\nindice=list(data[data['TARGET']==1].index)\nindice=random.sample(indice,int(len(indice)\/10))\nrandomList=randomList+indice\nsample_client=client.iloc[randomList,:].reset_index()\nsample_client.drop('index',axis=1,inplace=True)","ecfa5a43":"#L=list(np.arange(len(test)))\n#randomList=random.sample(L,int(len(L)\/10))\n#sample_test=test.iloc[randomList,:].reset_index()\n#sample_test.drop('index',axis=1,inplace=True)","9ca45fdc":"test['TARGET']=np.nan\ntest['set']='test'\nsample_client['set']='train'\nsample=sample_client.append(test,ignore_index=True)","3a53df78":"len(sample_client[sample_client['TARGET']==1])\/len(sample_client[sample_client['TARGET']==0])","3d80415b":"len(client[client['TARGET']==1])\/len(client[client['TARGET']==0])","a23b4c93":"def sampling(data_father,data_child,col):\n    ID=data_father[col].values\n    sample_data=data_child[data_child[col].isin(ID)].reset_index()\n    sample_data.drop('index',axis=1,inplace=True)\n    return sample_data","653411c6":"sample_bureau=sampling(sample,bureau,'SK_ID_CURR')","d73df509":"sample_bureau['SK_ID_CURR'].unique().sort()==sample['SK_ID_CURR'].unique().sort()","659687d9":"sample_bureau_balance=sampling(sample_bureau,bureau_balance,'SK_ID_BUREAU')","54ec12bf":"sample_previous=sampling(sample,previous,'SK_ID_CURR')","a917e43d":"sample_cash=sampling(sample_previous,cash,'SK_ID_PREV')","479cd33a":"sample_credit=sampling(sample_previous,credit,'SK_ID_PREV')","6a928a34":"sample_installements=sampling(sample_previous,installements,'SK_ID_PREV')","928f3119":"def replace_day_outliers(df):\n    for col in df.columns:\n        if \"DAYS \" in col:\n            df[col]=df[col].replace({365243:np.nan})","2cb8ee7a":"sample=replace_day_outliers(sample)","85a694fe":"sample_bureau=replace_day_outliers(sample_bureau)\nsample_bureau_balance=replace_day_outliers(sample_bureau_balance)\nsample_previous=replace_day_outliers(sample_previous)\nsample_credit=replace_day_outliers(sample_credit)\nsample_cash=replace_day_outliers(sample_cash)\nsample_installements=replace_day_outliers(sample_installements)","7dbfcbb8":"start_date=pd.Timestamp(\"2021\/07\/10\")","ec015aac":"for col in ['DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE']:\n    sample_bureau[col] = pd.to_timedelta(sample_bureau[col], 'D')","56118406":"sample_bureau[['DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE']].head()","c59f2d82":"sample_bureau['bureau_credit_application_date'] = start_date + sample_bureau['DAYS_CREDIT']\nsample_bureau['bureau_credit_end_date'] = start_date + sample_bureau['DAYS_CREDIT_ENDDATE']\nsample_bureau['bureau_credit_close_date'] = start_date + sample_bureau['DAYS_ENDDATE_FACT']\nsample_bureau['bureau_credit_update_date'] = start_date + sample_bureau['DAYS_CREDIT_UPDATE']","edcebc2f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# Set up default plot styles\nplt.rcParams['font.size'] = 26\nplt.style.use('fivethirtyeight')\n\n# Drop the time offset columns\n#sample_bureau = sample_bureau.drop(columns = ['DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE'])\nplt.figure(figsize = (10, 8))\nsns.distplot((sample_bureau['bureau_credit_end_date'] - sample_bureau['bureau_credit_application_date']).dropna().dt.days);\nplt.xlabel('Length of Loan (Days)', size = 24); plt.ylabel('Density', size = 24); plt.title('Loan Length', size = 30);","0c960327":"sample_bureau_balance['SK_ID_BUREAU'].unique()","dd7b33db":"# Convert to timedelta\nsample_bureau_balance['MONTHS_BALANCE'] = pd.to_timedelta(sample_bureau_balance['MONTHS_BALANCE'], 'M')\n# Make a date column\nsample_bureau_balance['bureau_balance_date'] = start_date + sample_bureau_balance['MONTHS_BALANCE']\nsample_bureau_balance = sample_bureau_balance.drop(columns = ['MONTHS_BALANCE'])\n\n# Select one loan and plot\nexample_credit = sample_bureau_balance[sample_bureau_balance['SK_ID_BUREAU'] ==  5718492]\nplt.plot(example_credit['bureau_balance_date'], example_credit['STATUS'], 'ro');\nplt.title('Loan 5001709 over Time'); plt.xlabel('Date'); plt.ylabel('Status');\n\n","fd3e16ac":"for col in sample.columns:\n    print(col)\n    ,sample[col].max(),sample[col].min()","c166c45f":"pip install dask","b3176d4b":"pip install \"dask[dataframe]\"","1ea39c5e":"pip install featuretools","c100946b":"import featuretools as ft ","992a2716":"es_pandas=ft.EntitySet(id='client_pandas')\nes=ft.EntitySet(id='clients')","e9f526ad":"import dask.dataframe as dd","ec734ed0":"sample_dask=dd.from_pandas(sample,4)\nsample_bureau_dask=dd.from_pandas(sample_bureau,4)\nsample_bureau_balance_dask=dd.from_pandas(sample_bureau_balance,4)\nsample_previous_dask=dd.from_pandas(sample_previous,4)\nsample_cash_dask=dd.from_pandas(sample_cash,4)\nsample_credit_dask=dd.from_pandas(sample_credit,4)\nsample_installements_dask=dd.from_pandas(sample_installements,4)\n","8c402f9d":"dic={}\nL=['set','TARGET','SK_ID_CURR','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_CAR','FLAG_OWN_REALTY','EMERGENCYSTATE_MODE','REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY']\ndic['SK_ID_CURR']=ft.variable_types.Id\ndic['CODE_GENDER']=ft.variable_types.Boolean\ndic['FLAG_OWN_CAR']=ft.variable_types.Boolean\ndic['FLAG_OWN_REALTY']=ft.variable_types.Boolean\ndic['EMERGENCYSTATE_MODE']=ft.variable_types.Boolean\ndic['REGION_RATING_CLIENT']=ft.variable_types.Ordinal\ndic['REGION_RATING_CLIENT_W_CITY']=ft.variable_types.Ordinal\ndic['TARGET']=ft.variable_types.Boolean\ndic['set']=ft.variable_types.Boolean\nL.append('SK_ID_CURR')\nfor col in sample.columns:\n    if(col not in L):\n        if(sample[col].dtype=='O'):\n            dic[col]=ft.variable_types.Categorical\n        elif(sample[col].dtype=='int64'):\n            if(len(sample[col].unique())==2):\n                 dic[col]=ft.variable_types.Boolean\n            else:\n                dic[col]=ft.variable_types.Numeric\n        else:\n                dic[col]=ft.variable_types.Numeric\n","b196ca99":"es=es.entity_from_dataframe(entity_id='sample',dataframe=sample_dask,index='SK_ID_CURR',variable_types=dic)\nes_pandas=es_pandas.entity_from_dataframe(entity_id='sample',dataframe=sample,index='SK_ID_CURR',variable_types=dic)","a37e5601":"dic={}\ndic['SK_ID_CURR']=ft.variable_types.Id\ndic['SK_ID_BUREAU']=ft.variable_types.Id\nL=['SK_ID_CURR','SK_ID_BUREAU']\nfor col in sample_bureau.columns:\n        if(col not in L):\n            if(sample_bureau[col].dtype=='O'):\n                dic[col]=ft.variable_types.Categorical\n            elif(sample_bureau[col].dtype=='int64'):\n                if(len(sample_bureau[col].unique())==2):\n                    dic[col]=ft.variable_types.Boolean\n                else:\n                    dic[col]=ft.variable_types.Numeric\n            else:\n                dic[col]=ft.variable_types.Numeric","056c4d87":"es=es.entity_from_dataframe(entity_id='sample_bureau',dataframe=sample_bureau_dask,index='SK_ID_BUREAU',variable_types=dic)\nes_pandas=es_pandas.entity_from_dataframe(entity_id='sample_bureau',dataframe=sample_bureau,index='SK_ID_BUREAU',variable_types=dic)","45d10492":"dic={'SK_ID_BUREAU':ft.variable_types.Id,'MONTHS_BALANCE':ft.variable_types.Numeric,'STATUS':ft.variable_types.Categorical}","4dba048b":"es=es.entity_from_dataframe(entity_id='sample_bureau_balance',dataframe=sample_bureau_balance_dask,make_index=True,index = 'bureaubalance_index',variable_types=dic)","fc58bbba":"es_pandas=es_pandas.entity_from_dataframe(entity_id='sample_bureau_balance',dataframe=sample_bureau_balance,index = 'bureaubalance_index',variable_types=dic)","ce24b0ba":"dic={}\ndic['SK_ID_PREV']=ft.variable_types.Id\ndic['SK_ID_CURR']=ft.variable_types.Id\ndic['FLAG_LAST_APPL_PER_CONTRACT']=ft.variable_types.Boolean\ndic['NAME_YIELD_GROUP']=ft.variable_types.Ordinal\ndic['NFLAG_INSURED_ON_APPROVAL']=ft.variable_types.Boolean\nL=['SK_ID_PREV','SK_ID_CURR','FLAG_LAST_APPL_PER_CONTRACT','NAME_YIELD_GROUP','NFLAG_INSURED_ON_APPROVAL']\nfor col in sample_previous.columns:\n        if(col not in L):\n            if(sample_previous[col].dtype=='O'):\n                dic[col]=ft.variable_types.Categorical\n            elif(sample_previous[col].dtype=='int64'):\n                if(len(sample_previous[col].unique())==2):\n                    dic[col]=ft.variable_types.Boolean\n                else:\n                    dic[col]=ft.variable_types.Numeric\n            else:\n                dic[col]=ft.variable_types.Numeric","c4f1c4a7":"es=es.entity_from_dataframe(entity_id='sample_previous',dataframe=sample_previous_dask,index = 'SK_ID_PREV',variable_types=dic)\nes_pandas=es_pandas.entity_from_dataframe(entity_id='sample_previous',dataframe=sample_previous,index = 'SK_ID_PREV',variable_types=dic)","5b740bd8":"dic={'SK_ID_PREV':ft.variable_types.Id,'SK_ID_CURR':ft.variable_types.Id,'MONTHS_BALANCE':ft.variable_types.Numeric,'CNT_INSTALMENT':ft.variable_types.Numeric,'CNT_INSTALMENT_FUTURE':ft.variable_types.Numeric,'NAME_CONTRACT_STATUS':ft.variable_types.Categorical,'SK_DPD':ft.variable_types.Id,'SK_DPD_DEF':ft.variable_types.Id}","45b18d4c":"es = es.entity_from_dataframe(entity_id = 'sample_cash', dataframe = sample_cash_dask,make_index=True,index=\"cash_id\",variable_types=dic)\n","451cf630":"es_pandas = es_pandas.entity_from_dataframe(entity_id = 'sample_cash', dataframe = sample_cash,make_index=True,index=\"cash_id\",variable_types=dic)","66982ec6":"dic={}\ndic['SK_ID_PREV']=ft.variable_types.Id\ndic['SK_ID_CURR']=ft.variable_types.Id\ndic['SK_DPD']=ft.variable_types.Id\ndic['SK_DPD_DEF']=ft.variable_types.Id\nL=['SK_ID_PREV','SK_ID_CURR','SK_DPD','SK_DPD_DEF']\nfor col in sample_credit.columns:\n        if(col not in L):\n            if(sample_credit[col].dtype=='O'):\n                dic[col]=ft.variable_types.Categorical\n            elif(sample_credit[col].dtype=='int64'):\n                if(len(sample_credit[col].unique())==2):\n                    dic[col]=ft.variable_types.Boolean\n                else:\n                    dic[col]=ft.variable_types.Numeric\n            else:\n                dic[col]=ft.variable_types.Numeric","13e13074":"es = es.entity_from_dataframe(entity_id = 'sample_credit', dataframe = sample_credit_dask,make_index=True,index=\"credit_id\",variable_types=dic)\n","8a6e5537":"es_pandas = es_pandas.entity_from_dataframe(entity_id = 'sample_credit', dataframe = sample_credit,make_index=True,index=\"credit_id\",variable_types=dic)","ef50883d":"dic={}\ndic['SK_ID_PREV']=ft.variable_types.Id\ndic['SK_ID_CURR']=ft.variable_types.Id\nL=['SK_ID_PREV','SK_ID_CURR']\nfor col in sample_installements.columns:\n        if(col not in L):\n            if(sample_installements[col].dtype=='O'):\n                dic[col]=ft.variable_types.Categorical\n            elif(sample_installements[col].dtype=='int64'):\n                if(len(sample_installements[col].unique())==2):\n                    dic[col]=ft.variable_types.Boolean\n                else:\n                    dic[col]=ft.variable_types.Numeric\n            else:\n                dic[col]=ft.variable_types.Numeric","175ff877":"es = es.entity_from_dataframe(entity_id = 'sample_installements', dataframe = sample_installements_dask,make_index=True,index=\"installements_id\",variable_types=dic)\n","185dd951":"es_pandas = es_pandas.entity_from_dataframe(entity_id = 'sample_installements', dataframe = sample_installements,make_index=True,index=\"installements_id\",variable_types=dic)","97412e9a":"es","d46eac8b":"es_pandas","5b44ba40":"# Relationship between app and bureau\nr_app_bureau = ft.Relationship(es['sample']['SK_ID_CURR'], es['sample_bureau']['SK_ID_CURR'])\n\n# Relationship between bureau and bureau balance\nr_bureau_balance = ft.Relationship(es['sample_bureau']['SK_ID_BUREAU'], es['sample_bureau_balance']['SK_ID_BUREAU'])\n\n# Relationship between current app and previous apps\nr_app_previous = ft.Relationship(es['sample']['SK_ID_CURR'], es['sample_previous']['SK_ID_CURR'])\n\n# Relationships between previous apps and cash, installments, and credit\nr_previous_cash = ft.Relationship(es['sample_previous']['SK_ID_PREV'], es['sample_cash']['SK_ID_PREV'])\nr_previous_installments = ft.Relationship(es['sample_previous']['SK_ID_PREV'], es['sample_installements']['SK_ID_PREV'])\nr_previous_credit = ft.Relationship(es['sample_previous']['SK_ID_PREV'], es['sample_credit']['SK_ID_PREV'])","ce933ba3":"es_pandas = es_pandas.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n                           r_previous_cash, r_previous_installments, r_previous_credit])","5cc183fe":"es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n                           r_previous_cash, r_previous_installments, r_previous_credit])","8355b406":"    feature_matrix_spec_pandas, feature_names_spec_pandas = ft.dfs(entityset = es_pandas, target_entity = 'sample',  \n                                                     agg_primitives = ['sum', 'count', 'min', 'max', 'mean'], \n                                                     max_depth = 2, features_only = False, verbose = True)","b69bc979":"feature_matrix_spec, feature_names_spec = ft.dfs(entityset = es, target_entity = 'sample',  \n                                                 agg_primitives = ['sum', 'count', 'min', 'max', 'mean'], \n                                                 max_depth = 2, features_only = False, verbose = True)","d4af6633":"#rest=np.setdiff1d(cols_to_remove,missing_columns)","1d3caab8":"sample_train=feature_matrix_spec[feature_matrix_spec['set']=='train'].compute()","f6593f5d":"sample_test=feature_matrix_spec[feature_matrix_spec['set']=='test'].compute()","e5944d33":"sample_train.to_csv('ssample_train.csv', index=False)\nsample_test.to_csv('ssample_test.csv', index=False)","ba7d74d0":"def missing_value(train,alpha):\n    train_miss_value=pd.DataFrame(train.isna().sum())\n    train_miss_value['percentage']=train_miss_value\/len(train)\n    train_miss_value=train_miss_value.sort_values('percentage',ascending=False)\n    return list(train_miss_value[train_miss_value['percentage']>alpha].index)\n    ","06cdbea5":"L=missing_value(sample_train.compute(),0.6)","f97ad848":"L_test=missing_value(sample_test.compute(),0.6)","7e1d901a":"missing_columns=list(set(L+L_test))","69b3cc01":"missing_columns.remove(\"TARGET\")","df24fbc5":"sample_train=sample_train.drop(columns=['set'])","b478e237":"sample_test=sample_test.drop(columns=['set','TARGET'])","2ad2bd45":"#train, test = sample_train.align(sample_test, join = 'inner', axis = 1)","2411800e":"sample_train=sample_train.drop(columns=missing_columns)\nsample_test=sample_test.drop(columns=missing_columns)","85578fcc":"correlation=sample_train.corr()","32c21095":"correlation_pandas=correlation.compute()","18b11ae3":"correlations_target=correlation_pandas['TARGET'].dropna()","1a0796fb":"corr_series=correlations_target.sort_values()","dcd09586":"corr_series.sort_values().head()","92925e2d":"corr_series.sort_values().tail()","23ba8d2d":"import matplotlib.pyplot as plt\nimport seaborn as sns","e4251bc4":"def kde_target(var_name, df):\n    corr = df['TARGET'].corr(df[var_name])\n    # Calculate medians for repaid vs not repaid\n    avg_repaid = df.loc[df['TARGET'] == 0, var_name].median()\n    avg_not_repaid = df.loc[df['TARGET'] == 1, var_name].median()\n    plt.figure(figsize = (12, 6))\n    sns.kdeplot(df.loc[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n    sns.kdeplot(df.loc[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)","c3c9a353":"kde_target('MEAN(sample_bureau.DAYS_CREDIT)',feature_matrix_spec_pandas)","f0b3f579":"kde_target('EXT_SOURCE_3',feature_matrix_spec_pandas)","2ec1b8c1":"threshold=0.8\nabove_threshold_vars = {}\nfor col in correlation_pandas:\n    above_threshold_vars[col] = list(correlation_pandas.index[correlation_pandas[col] > threshold])","030fb054":"cols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# Iterate through columns and correlated columns\nfor key, value in above_threshold_vars.items():\n    # Keep track of columns already examined\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # Only want to remove one in a pair\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n            \ncols_to_remove = list(set(cols_to_remove))\nprint('Number of columns to remove: ', len(cols_to_remove))","a047f1ac":"sample_train=sample_train.drop(columns=cols_to_remove)\nsample_test=sample_test.drop(columns=cols_to_remove)","fea8b26f":"sample_train.select_dtypes('object').columns","38378cb7":"def countplot_withY(label, dataset):\n  plt.figure(figsize=(10,5))\n  Y = dataset[label]\n  total = len(Y)*1.\n  ax=sns.countplot(x=label, data=dataset, hue=\"TARGET\")\n  for p in ax.patches:\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\n\n  #put 11 ticks (therefore 10 steps), from 0 to the total number of rows in the dataframe\n  ax.yaxis.set_ticks(np.linspace(0, total, 11))\n  #adjust the ticklabel to the desired format, without changing the position of the ticks.\n  ax.set_yticklabels(map('{:.1f}%'.format, 100*ax.yaxis.get_majorticklocs()\/total))\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n  # ax.legend(labels=[\"no\",\"yes\"])\n  plt.show()","fc222814":"sample_train_pandas=feature_matrix_spec_pandas[feature_matrix_spec_pandas['set']=='train']","00012479":"countplot_withY('CODE_GENDER',sample_train_pandas)","dff56dbd":"countplot_withY('OCCUPATION_TYPE',sample_train_pandas)","25903a32":"countplot_withY('FLAG_OWN_CAR',sample_train_pandas)","e5d94f87":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport matplotlib.pyplot as plt","160372b3":"pip install optuna ","02d719e8":"import optuna ","036b889f":"k_fold = KFold(n_splits =5, shuffle = True, random_state = 50)","799463ee":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    #features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    features = features.drop(columns = ['TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    param_grid={\"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\"reg_alpha\":trial.suggest_float(\"reg_alpha\",0.05,0.2),\"reg_lambda\":trial.suggest_float(\"reg_lambda\",0.05,0.1)}\n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        \"\"\"model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.01, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\"\"\"\n        model=lgb.LGBMClassifier(objective=\"binary\",subsample=0.8,n_jobs=-1,random_state=50,class_weight=\"balanced\",**param_grid)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ \/ k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] \/ k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","7ee3b03e":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    print(df['feature'].head(15).values)\n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","a92d8768":"sample_train_pandas.reset_index(inplace=True)\nsample_train_pandas=sample_train_pandas[sample_train.columns]","42b615c3":"sample_test_pandas=feature_matrix_spec_pandas[feature_matrix_spec_pandas['set']=='test']","8d0ffdca":"sample_test_pandas.reset_index(inplace=True)\nsample_test_pandas=sample_test_pandas[sample_test.columns]","bbeb7d86":"submission, fi, metrics = model(sample_train_pandas, sample_test_pandas)","7ede2665":"metrics","ccb25354":"submission.to_csv('submission2.csv', index=False)","7debf8d5":"sample_client.reset_index(inplace=True)","5e8bc98f":"sample_client.drop('index',axis=1,inplace=True)","7f1b9415":"pd.value_counts(sample_client['TARGET'])","d5b32e0a":"sample_previous_2=sampling(sample_client,previous,'SK_ID_CURR')","dcb628d4":"sample_cash_2=sampling(sample_previous_2,cash,'SK_ID_PREV')\nsample_bureau_2=sampling(sample_client,bureau,'SK_ID_CURR')","4aa79df1":"data1=sample_previous_2.groupby('SK_ID_CURR')['RATE_DOWN_PAYMENT'].max()","1084a2bf":"data1","d6163664":"sample_client=sample_client.merge(data1,on='SK_ID_CURR',how='left')","4b1f98b1":"data2=sample_bureau_2.groupby('SK_ID_CURR')['DAYS_CREDIT_ENDDATE'].max()\nsample_client=sample_client.merge(data2,on='SK_ID_CURR',how='left')","b97d3b08":"data3=sample_previous_2.groupby('SK_ID_CURR')['AMT_DOWN_PAYMENT'].max()\nsample_client=sample_client.merge(data3,on='SK_ID_CURR',how='left')","bdc126da":"data4=sample_bureau_2.groupby('SK_ID_CURR')['DAYS_ENDDATE_FACT'].max()\nsample_client=sample_client.merge(data4,on='SK_ID_CURR',how='left')","a156b312":"X_prime=sample_client[['SK_ID_CURR','CODE_GENDER','RATE_DOWN_PAYMENT','DAYS_CREDIT_ENDDATE','AMT_DOWN_PAYMENT','DAYS_ENDDATE_FACT','EXT_SOURCE_3', 'EXT_SOURCE_2', 'EXT_SOURCE_1', 'DAYS_BIRTH', 'AMT_CREDIT',\n 'DAYS_EMPLOYED', 'AMT_ANNUITY' ,'DAYS_ID_PUBLISH','DAYS_LAST_PHONE_CHANGE','YEARS_BEGINEXPLUATATION_AVG','TARGET']]\nX_prime.rename(columns={'RATE_DOWN_PAYMENT':'MAX(sample_previous.RATE_DOWN_PAYMENT)','DAYS_CREDIT_ENDDATE':'MAX(sample_bureau.DAYS_CREDIT_ENDDATE)','AMT_DOWN_PAYMENT':'MAX(sample_previous.AMT_DOWN_PAYMENT)','DAYS_ENDDATE_FACT':'MAX(sample_bureau.DAYS_ENDDATE_FACT)'},inplace=True)","93be8359":"fi_sorted = plot_feature_importances(fi)","a4e2193d":"submission, fi, metrics = model(X_prime, sample_test_pandas[X_prime.columns[:-1]])","80aca8a1":"metrics","d50b8181":"submission.to_csv('submission3.csv',index=False)","ef06d9f1":"sample_train_pandas.select_dtypes('object').columns","19a4689c":"pd.value_counts(sample_train_pandas['TARGET']).plot.bar()\nplt.title('Fraud class histogram')\nplt.xlabel('TARGET')\nplt.ylabel('Frequency')\nsample_train_pandas['TARGET'].value_counts()","55d076f7":"from imblearn.over_sampling import SMOTE","14b74028":"data=pd.DataFrame(sample_train_pandas.isna().sum())\nL=list(data[data[0]==0].index)","f96d9209":"list(sample_train_pandas.select_dtypes('object'))","ff319378":"L=list(np.setdiff1d(L,list(sample_train_pandas.select_dtypes('object').columns)))","cbaf60ba":"cat=['CODE_GENDER',\n 'FLAG_OWN_CAR',\n 'FLAG_OWN_REALTY',\n 'NAME_CONTRACT_TYPE',\n\n 'NAME_INCOME_TYPE',\n 'NAME_EDUCATION_TYPE',\n 'NAME_FAMILY_STATUS',\n 'NAME_HOUSING_TYPE',\n \n 'WEEKDAY_APPR_PROCESS_START',\n 'ORGANIZATION_TYPE',\n\n ]","730d2647":"X=sample_train_pandas[L]","73d7657e":"smote = SMOTE(random_state = 101)\nX_oversample, y_oversample = smote.fit_resample(X, X['TARGET'])","b09fb801":"pd.value_counts(X_oversample['TARGET']).plot.bar()\nplt.title('Fraud class histogram')\nplt.xlabel('TARGET')\nplt.ylabel('Frequency')\nX_oversample['TARGET'].value_counts()","1725c0e9":"L=list(X_oversample.columns)","0ef0824a":"L.append('SK_ID_CURR')","110fefb4":"submission, fi, metrics = model(X_oversample, sample_test_pandas[L])","f3d134ab":"submission=submission.reset_index()","e640b785":"#submission.to_csv('submission.csv', index=False)","735e58c2":"sns.scatterplot(x=sample_train_pandas.index,y='DAYS_BIRTH',hue='TARGET',data=sample_train_pandas)","2cea0c18":"'EXT_SOURCE_3' in sample_train_pandas.columns","db4d73e2":"sample_train_pandas","0c232b18":"**Sampling**","ff982a61":"**Work with Dask and Featuretools made**","be98dc15":"**Explanatory Data** ","b03e0ea1":"**Correlation**","491fe28c":"##### ","beb9f901":"**Modeling**","096f9e2b":"**SMOTE**"}}