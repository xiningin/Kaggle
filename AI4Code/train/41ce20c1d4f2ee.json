{"cell_type":{"a73d119f":"code","4486aa87":"code","87c17eb4":"code","2c13434e":"code","187b21a7":"code","dc88ac3c":"code","c5cfc190":"code","00f81b9b":"code","cf325e0a":"code","fbd14afb":"code","594f90c4":"markdown","9cdd5646":"markdown","db9eb20f":"markdown","210061a1":"markdown","9573eb6c":"markdown","1774fc9a":"markdown","2869c1f6":"markdown","6f983eaf":"markdown","3e246a3e":"markdown"},"source":{"a73d119f":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error","4486aa87":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","87c17eb4":"# Separate target from features\ny = train['target']\nX = train.drop(['target'], axis=1)\n\n# Preview features\nX.head()","2c13434e":"\nCAT_FEATURES = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\nNUM_FEATURES = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8',\n                'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nALL_FEATURES = CAT_FEATURES+NUM_FEATURES\n","187b21a7":"ordinal_encoder = OrdinalEncoder()\nX[CAT_FEATURES] = ordinal_encoder.fit_transform(X[CAT_FEATURES])\ntest[CAT_FEATURES] = ordinal_encoder.transform(test[CAT_FEATURES])\nprint(X.head())","dc88ac3c":"# from sklearn.preprocessing import OneHotEncoder\n\n# object_cols = [col for col in X.columns if 'cat' in col]\n# # Columns that will be one-hot encoded\n# low_cardinality_cols = [col for col in object_cols if X[col].nunique() < 10]\n\n\n# # Apply one-hot encoder to each column with categorical data\n# OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n# OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X[low_cardinality_cols]))\n# OH_cols_test = pd.DataFrame(OH_encoder.transform(test[low_cardinality_cols]))\n\n# # One-hot encoding removed index; put it back\n# OH_cols_train.index = X.index\n# OH_cols_test.index = test.index\n\n# # Remove categorical columns (will replace with one-hot encoding)\n# num_X_train = X.drop(object_cols, axis=1)\n# num_X_test = test.drop(object_cols, axis=1)\n\n# # Add one-hot encoded columns to numerical features\n# OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n# OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\n\n# print(OH_X_train.columns)\n# print(y)","c5cfc190":"SEED = 91\n# xgb_params = {\n#     'booster': 'gbtree',\n#     'n_estimators': 10000,\n#     'learning_rate': 0.05,\n#     'reg_lambda': 10,\n#     'reg_alpha': 26,\n#     'subsample': 0.9,\n#     'colsample_bytree': 0.12,\n#     'max_depth': 3,\n#     'random_state': SEED,\n#     'tree_method':'gpu_hist',\n#     'gpu_id': 0,\n#     'predictor':'gpu_predictor'\n# }","00f81b9b":" xgb_params = {\n    'random_state': SEED, \n    # gpu\n#     'tree_method': 'gpu_hist', \n#     'gpu_id': 0, \n#     'predictor': 'gpu_predictor',\n    # cpu\n    'n_jobs': 4,\n    'booster': 'gbtree',\n    'n_estimators': 15000,\n    # optimized params\n    'learning_rate': 0.02528302216953097,\n    'reg_lambda': 10,\n    'reg_alpha': 25.13181079976304,\n    'subsample': 0.7875490025178415,\n    'colsample_bytree': 0.11807135201147481,\n    'max_depth': 3\n }","cf325e0a":"N_FOLD = 12\n\n#Setting the kfold parameters\nkf = KFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\n\noof_preds = np.zeros((X.shape[0],))\npredictions = 0\nmodel_fi = 0\nmean_rmse = 0\n# ALL_FEATURES = OH_X_train.columns\nfor num, (train_idx, valid_idx) in enumerate(kf.split(X)):\n    # split the train data into train and validation\n    X_train = X.iloc[train_idx][ALL_FEATURES]\n    X_valid = X.iloc[valid_idx][ALL_FEATURES]\n    y_train = y.iloc[train_idx]\n    y_valid = y.iloc[valid_idx]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n             verbose = 1000,\n             eval_set = [(X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 350)\n    \n    #Mean of the predictions\n    predictions += model.predict(test) \/ N_FOLD\n    \n    #Mean of feature importance\n    model_fi += model.feature_importances_ \/ N_FOLD \n    \n    #Out of Fold predictions\n    oof_preds[valid_idx] = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ N_FOLD\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","fbd14afb":"# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","594f90c4":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","9cdd5646":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","db9eb20f":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","210061a1":"Welcome to the **[30 Days of ML competition](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview)**!  In this notebook, you'll learn how to make your first submission.\n\nBefore getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n\n# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course.","9573eb6c":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nIf you took the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** courses, then you learned about **[Random Forests](https:\/\/www.kaggle.com\/dansbecker\/random-forests)**.  In the code cell below, we fit a random forest model to the data.","1774fc9a":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course, then you learned about **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset.","2869c1f6":"Next, we break off a validation set from the training data.","6f983eaf":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`.","3e246a3e":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file."}}