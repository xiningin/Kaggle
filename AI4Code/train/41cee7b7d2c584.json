{"cell_type":{"1d365152":"code","f6182035":"code","12f86e2e":"code","1b286734":"code","03cb1a6c":"code","edccdd9b":"code","1b3360a5":"code","2122a168":"code","eaf37cbb":"code","8ed1c591":"code","c787ea63":"code","f43de109":"code","d3f05015":"code","817f566a":"code","e9cddc77":"code","a1bc8f3f":"code","747403e5":"code","89c6f67f":"code","102ebff0":"code","cfd5dcff":"code","883c9ced":"code","82d6ac06":"code","897d00e8":"code","42a4d43c":"code","2396b357":"code","3371aa6e":"code","9d9c8139":"code","5dfa9db2":"code","583a67a9":"code","4d4d840d":"code","88e6a351":"code","2dd5471f":"code","5e587082":"code","e66d656d":"code","5fd34dda":"code","9d1cef2a":"code","c7f95137":"code","47cbadcb":"code","a7c91ba1":"code","76c3c11f":"code","89be9b9f":"code","b36f7a2e":"code","a20c5e0b":"code","ee846752":"code","7eb3adf0":"code","a66e9d1f":"code","e0f32d8b":"code","0947dd3a":"code","103595a4":"code","9b6fca1f":"code","2f6f0dfa":"code","a6b47ebc":"code","ca2b1b50":"code","4a623891":"code","38475edd":"markdown","5a00d5f0":"markdown","1ff44417":"markdown","6fe561ff":"markdown","f4e43c47":"markdown","91dc7023":"markdown","f11c4730":"markdown","813607c1":"markdown","5043cb2e":"markdown","eee52832":"markdown","536998be":"markdown","4a75e6b5":"markdown","6506f2eb":"markdown","3f4281ad":"markdown","c75cb88a":"markdown","c64f7846":"markdown","3c0bbc38":"markdown","86de633e":"markdown","a251f2bd":"markdown","730e4e49":"markdown","8d604985":"markdown","02402f41":"markdown","de9bb1d0":"markdown","2a2ac42f":"markdown","0c660731":"markdown"},"source":{"1d365152":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np \nimport pandas as pd \nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport emoji\nimport os\nimport sys\nimport fasttext\nimport re\nimport nltk \n\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\ntf.keras.backend.clear_session()\n\nimport itertools\nimport collections\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f6182035":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubm = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n","12f86e2e":"train.head()","1b286734":"train.info()","03cb1a6c":"print(\"For Training Set\")\nfor col in train.columns:\n    print(\"% of nan in \",col,\" - \",train[col].isna().sum()\/train.shape[0])","edccdd9b":"print(\"For Test Set\")\nfor col in test.columns:\n    print(\"% of nan in \",col,\" - \",test[col].isna().sum()\/test.shape[0])","1b3360a5":"import plotly.express as px\n\n# df = px.data.target()\nfig = px.parallel_categories(train)\n\nfig.show()","2122a168":"#remove url\nimport re\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","eaf37cbb":"import re\ndef remove_URLs(text):\n    url = re.compile(r'http?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","8ed1c591":"train['text'] = train['text'].apply(remove_URL)\ntest['text'] = test['text'].apply(remove_URL)","c787ea63":"train['text'] = train['text'].apply(remove_URLs)\ntest['text'] = test['text'].apply(remove_URLs)","f43de109":"#remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","d3f05015":"train['text'] = train['text'].apply(remove_emoji)\ntest['text'] = test['text'].apply(remove_emoji)","817f566a":"#remove contractions\ncontractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\",\n\"thx\"   : \"thanks\"\n}","e9cddc77":"def remove_contractions(text):\n    return contractions[text.lower()] if text.lower() in contractions.keys() else text","a1bc8f3f":"train['text']=train['text'].apply(remove_contractions)\ntest['text']=test['text'].apply(remove_contractions)","747403e5":"#remove punctuation\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","89c6f67f":"train['text'] = train['text'].apply(remove_punct)\ntest['text'] = test['text'].apply(remove_punct)\n","102ebff0":"import nltk\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords ","cfd5dcff":"tweets_train = train['text'].values\ntarget = train['target'].values\n\ntweets_test = test['text'].values","883c9ced":"stop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()","82d6ac06":"#Most common words\nfreq = pd.Series(' '.join(train['text']).split()).value_counts()[:20]\nfreq","897d00e8":"#Identify uncommon words\nfreq1 =  pd.Series(' '.join(train \n         ['text']).split()).value_counts()[-20:]\nfreq1","42a4d43c":"thislist = ['were', 'the', 'amp', 'dont', 'got', 'know', 'gon', 'na', 'wan', 'like', 'im', 'hers', 'why', 'over', \"'d\",'our', 'these', 'nevertheless', 'its', 'them', 'empty', 'how', 'whereas', 'whether', 'fifteen', 'about', 'four', 'give', 'otherwise', 'move', 'do', 'say', '\u2018ve', 'hence', 'n\u2018t', 'between', 'bottom', 'some', 'against', 'whole', 'i', 'into', 'they', 'already', 'she', 'either', 'an', 'both', 'him', 'due', 'using', 'five', 'across', 'front', 'in', 'off', 'only', 'really', 'twelve', 'twenty', 'show', 'whereupon', '\u2018m', 'n\u2019t', 'himself', '\u2019m', 'from', 'often', 'three', 'various', 'thereupon', 'should', 'put', 'take', 'who', 'above', 'their', 'been', 'towards', 'however', \"n't\", 'her', 'go', 'thereby', 'just', 'yourselves', 'become', 'thru', 'while', 'nowhere', 'neither', 'anyway', 'because', 'ca', 'which', 'moreover', 'forty', 'besides', 'us', 'more', 'third', 'wherein', 'whoever', 'used', 'every', 'whose', 'onto', 'your', 'hereafter', 'itself', 'sometimes', 'name', 'too', 'own', 'somewhere', 'there', 'we', 'you', '\u2019ve', 'ourselves', 'sixty', 'would', 'first', 'must', 'whereafter', 'wherever', 'his', 'around', 'has', 'yours', 'became', 'doing','the', 'below', 'then', 'everyone', 'else', 'any', 'latterly', 'noone', 'part', 'might', \"'ve\", 'becoming', 'same', 'top', 'yourself', 'he', 'each', 'anyone', 'my', 'seeming', 'six', 'the', 'during', 'afterwards', 'throughout', 'formerly', 'seem', 'therefore', 'another', 'keep', 'without', 'being', 'can', 'had', 'per', \"'s\", 'other', 'side', '\u2019s', 'also', 'herself', '\u2019ll', 'eight', 'what', 'please', 'a', 'therein', 'back', 'me', 'never', 'not', 'does', 'enough', 'meanwhile', 'toward', 'even', 'get', 'and', 'it', 'perhaps', 'this', 'regarding', 'somehow', 'cannot', 'anyhow', 'through', 'whenever', 'thereafter', 'rather', 'by', 'still', 'where', 'than', 'made', 'of', 'will', 'within', 'are', 'amongst', 'although', 'former', 'full', 'nobody', 'was', 'to', 'is', 'at', 'hundred', 'all', 'on', 'such', 'after', 'almost', 'most', 'no', 'our', 'see', 'thus', 'upon', \"'ll\", 'whence', 'make', '\u2018s', 'could', 'quite', 'or', 'beyond', 'thence', 'mostly', 'though', 'alone', 'for', 'under', 'seemed', 'until', 'much', 'nine', 'least', 'that', 'nor', 'further', 'themselves', 'whatever', 'whom', 'anywhere', 'myself', 'eleven', 'none', 'with', 'as', 'have', '\u2018ll', \"'m\", 'up', 'if', 'several', 'whereby', 'now', 'always', 'amount', 'done', 'hereupon', 'others', 'may', 'one', 'everything', 'so', 'hereby', 'anything', 'fifty', 'last', 'am', 'beforehand', 'few', 'ever', 'together', 'unless', 'ten', 'behind', 'when', 'those', 'mine', 'everywhere', 'be', 'less', 'nothing', 'something', 'very', \"'re\", 'here', '\u2018re', 'since', 'seems', 'down', 'did', 'before', 'serious', '\u2018d', '\u2019d', 'many', 'call', 'along', 'once', 'herein', 'out', 'namely', 'someone', 'becomes', 'whither', 're', 'two', 'but', 'again', 'elsewhere', 'well', 'next', 'sometime', 'indeed', 'ours', 'yet', '\u2019re', 'via', 'latter', 'except', 'among', 'beside']\n","2396b357":"stop_words.update(thislist)","3371aa6e":"for i in range(len(tweets_train)):\n    \n    sentences = sent_tokenize(tweets_train[i])\n    \n    word_list = []\n    for sent in sentences:\n        \n        words = word_tokenize(sent)\n        \n        for word in words:\n            \n            if words not in word_list:\n                word_list.append(word)\n                \n    \n    word_list = [lemmatizer.lemmatize(w) for w in word_list if w not in stop_words]\n    \n    tweets_train[i] = ' '.join(w for w in word_list)","9d9c8139":"for i in range(len(tweets_test)):\n    \n    sentences = sent_tokenize(tweets_test[i])\n    \n    word_list = []\n    for sent in sentences:\n        \n        words = word_tokenize(sent)\n        \n        for word in words:\n            \n            if words not in word_list:\n                word_list.append(word)\n                \n    \n    word_list = [lemmatizer.lemmatize(w) for w in word_list if w not in stop_words]\n    \n    tweets_test[i] = ' '.join(w for w in word_list)\n","5dfa9db2":"cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\nX=cv.fit_transform(tweets_train)\nlist(cv.vocabulary_.keys())[:10]\n","583a67a9":"wordcloud = WordCloud(background_color='coral',\n                          stopwords=stop_words,\n                          max_words=100,\n                          max_font_size=50, \n                          random_state=42\n                         ).generate(str(tweets_train))\nplt.figure(figsize=(10,6))\nplt.figure(figsize=(15,10))\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='Bilinear')\nplt.axis(\"off\")\nplt.show()","4d4d840d":"#Combined statistical representations with px.histogram\n\ntrain_ = train[train['keyword'].notna()]\ntrain__ = train_[train_['location'].notna()]\nimport plotly.express as px\n# df = px.data.tips()\nfig = px.histogram(train__, x=\"id\", y=\"text\", color=\"target\", marginal=\"rug\",\n                   hover_data=train__.columns)\nfig.show()","88e6a351":"m = list(train['text'].values)","2dd5471f":"#Most frequently occuring words\ndef get_top_n_words(m, n=None):\n#     vec = CountVectorizer().fit(m)\n    bag_of_words = cv.transform(m)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n                   cv.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                       reverse=True)\n    return words_freq[:n]\n#Convert most freq words to dataframe for plotting bar plot\ntop_words = get_top_n_words(m, n=20)\ntop_df = pd.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]\ntop_df.head()\n#Barplot of most freq words\n# import seaborn as sns\n# sns.set(rc={'figure.figsize':(13,8)})\n# g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n# g.set_xticklabels(g.get_xticklabels(), rotation=30)","5e587082":"import plotly.graph_objects as go\n\nfig = go.Figure(go.Bar(\n            x=top_df['Freq'].tolist(),\n            y=top_df['Word'].tolist(),\n            orientation='h'))\n\nfig.show()","e66d656d":"#Most frequently occuring Bi-grams\ndef get_top_n2_words(m, n=None):\n    vec1=CountVectorizer(stop_words=stop_words, max_features=10000, ngram_range=(2,2)).fit(m)\n#     vec1 = CountVectorizer(ngram_range=(2,2),  \n#             max_features=2000).fit(m)\n    bag_of_words = vec1.transform(m)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop2_words = get_top_n2_words(m, n=20)\ntop2_df = pd.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\nprint(top2_df)\n# #Barplot of most freq Bi-grams\n# import seaborn as sns\n# sns.set(rc={'figure.figsize':(13,8)})\n# h=sns.boxplot(y=\"Bi-gram\", x=\"Freq\", data=top2_df)\n# h.set_yticklabels(h.get_yticklabels(), rotation=0)","5fd34dda":"#Barplot of most freq Bi-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(18,10)})\nh=sns.boxplot(y=\"Bi-gram\", x=\"Freq\", data=top2_df)\nh.set_yticklabels(h.get_yticklabels(), rotation=0)","9d1cef2a":"def get_top_n3_words(m, n=None):\n    vec1 = CountVectorizer(ngram_range=(3,3), \n           max_features=2000).fit(m)\n    bag_of_words = vec1.transform(m)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop3_words = get_top_n3_words(m, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Tri-gram\", \"Freq\"]\nprint(top3_df)","c7f95137":"import plotly.express as px\n\n# df = px.data.gapminder().query(\"continent == 'Europe' and year == 2007 and pop > 2.e6\")\nfig = px.bar(top3_df,x=\"Tri-gram\", y=\"Freq\")\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","47cbadcb":"tf1 = (train['text'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1.columns = ['words','tf']\ntf1","a7c91ba1":"for i,word in enumerate(tf1['words']):\n  tf1.loc[i, 'idf'] = np.log(train.shape[0]\/(len(train[train['text'].str.contains(word)])))\n\ntf1","76c3c11f":"tf1['tfidf'] = tf1['tf'] * tf1['idf']\ntf1","89be9b9f":"import re\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): \n    return re_tok.sub(r' \\1 ', s).split()","b36f7a2e":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(tokenizer=tokenize)\ntrain_vec = vectorizer.fit_transform(tweets_train)","a20c5e0b":"test_vec = vectorizer.transform(tweets_test)","ee846752":"print(train_vec.shape,test_vec.shape)","7eb3adf0":"from sklearn import feature_extraction, linear_model, model_selection, preprocessing\nclf = linear_model.RidgeClassifier()","a66e9d1f":"scores = model_selection.cross_val_score(clf, train_vec, train[\"target\"], cv=3, scoring=\"f1\")\nscores","e0f32d8b":"from sklearn.linear_model import LogisticRegression","0947dd3a":"lr = LogisticRegression(C=2, dual=True,solver='liblinear',max_iter=5000)\n","103595a4":"lr.fit(train_vec,target)","9b6fca1f":"scores = model_selection.cross_val_score(lr, train_vec, train[\"target\"], cv=5, scoring=\"f1\")\nscores","2f6f0dfa":"lr.fit(train_vec, train[\"target\"])","a6b47ebc":"subm[\"target\"] = lr.predict(test_vec)","ca2b1b50":"subm.head()","4a623891":"subm.to_csv(\"submission.csv\", index=False)","38475edd":"**Basic EDA**","5a00d5f0":"Tri-grams","1ff44417":"**Advance Data Preprocessing**","6fe561ff":"![twitter](https:\/\/www.crained.com\/wp-content\/uploads\/2020\/01\/Screen-Shot-2020-01-19-at-12.19.40-PM.png)","f4e43c47":"1 represents Disasterous Tweets whereas 0 represents Non-Disastrous","91dc7023":"**Basic Data Preprocessing**","f11c4730":"**Inverse Document Frequency**\nThe intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it\u2019s appearing in all the documents.\n\nTherefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n\nIDF = log(N\/n), where, N is the total number of rows and n is the number of rows in which the word was present.\n\nSo, let\u2019s calculate IDF for the same tweets for which we calculated the term frequency.","813607c1":"**Importing Libraries**","5043cb2e":"*Disaster tweets are Less in Number as compared to Non Disaster Tweets*","eee52832":"**Building a Text Classification model**\n\nNow the data is ready to be fed into a classification model. Let's create a basic claasification model using commonly used classification algorithms and see how our model performs.","536998be":"* N-grams\n* Term frequency\n* Inverse Document Frequency\n* Term Frequency \u2013 Inverse Document Frequency (TF-IDF)","4a75e6b5":"**Constructing Custom Stop Word Lists**","6506f2eb":"**Term frequency**\nTerm frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n\nTherefore, we can generalize term frequency as:\n\nTF = (Number of times term T appears in the particular row) \/ (number of terms in that row)\n\nTo understand more about Term Frequency, have a look at [this article](https:\/\/www.analyticsvidhya.com\/blog\/2015\/04\/information-retrieval-system-explained\/).\n\nBelow, I have tried to show you the term frequency table of a tweet.\n\n","3f4281ad":"**Reading Dataset**","c75cb88a":"**N-Grams**\n\nN-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n\nUnigrams do not usually contain as much information as compared to bigrams and trigrams. The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. The longer the n-gram (the higher the n), the more context you have to work with. Optimum length really depends on the application \u2013 if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the \u201cgeneral knowledge\u201d and only stick to particular cases.","c64f7846":"**Visualization**","3c0bbc38":"**Exploring Target Column**","86de633e":"**Lemmatizing the text**\n\nLemmatizing, maps common words into one base. Unlike stemming though, it always still returns a proper word that can be found in the dictionary\n![lemmatizing](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*z4f7My5peI28lNpZdHk_Iw.png)","a251f2bd":"![twitter](https:\/\/media2.s-nbcnews.com\/j\/newscms\/2018_05\/2314116\/180201-twitter-logo-ac-702p_c7d1da2f557bd5d16b23618cfcd702d6.fit-760w.jpg)","730e4e49":"**Removing Stopwords**\n\nWe imported a list of the most frequently used words from the NL Toolkit at the beginning with from nltk.corpus import stopwords.There are 179 English words, including \u2018i\u2019, \u2018me\u2019, \u2018my\u2019, \u2018myself\u2019, \u2018we\u2019, \u2018you\u2019, \u2018he\u2019, \u2018his\u2019, for example. We usually want to remove these because they have low predictive power. There are occasions when you may want to keep them though. Such as, if your corpus is very small and removing stop words would decrease the total number of words by a large percent.\n![stopwords](https:\/\/media.geeksforgeeks.org\/wp-content\/cdn-uploads\/Stop-word-removal-using-NLTK.png)\n\n","8d604985":"Unigram","02402f41":"**Term Frequency \u2013 Inverse Document Frequency (TF-IDF)**\nTF-IDF is the multiplication of the TF and IDF which we calculated above.\n\n","de9bb1d0":"The following workflow is followed:\n* Remove HTML\n* Remove Emoji's + contractions\n* Remove punctuation\n* Tokenization + Remove  stopwords\n* Lemmatization \n\n\n\n","2a2ac42f":"Bi-grams","0c660731":"![twitter](https:\/\/cloverchronicle.com\/wp-content\/uploads\/2019\/04\/Obama-Clinton-Twitter-Easter-Worshippers-Muslim-Community.png)"}}