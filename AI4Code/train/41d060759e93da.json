{"cell_type":{"8c22af16":"code","cf12fda9":"code","9e5b7ff4":"code","9450c7aa":"code","9233193b":"code","a38b065f":"code","21d71ce5":"code","c509ff0c":"code","1bd1d056":"code","5c227919":"code","b341f832":"code","00778354":"code","479967fb":"code","e17ec9b7":"code","7b857170":"code","0dfce453":"code","ea82378c":"code","66902f9f":"code","a6dc7ae6":"code","24a4b1b5":"code","15585908":"code","08ed341a":"code","2891fd86":"code","43236c4b":"code","8f4c9710":"code","42699c40":"code","ee26c852":"code","83e39e8b":"code","1b89d6d3":"code","d7bec8e7":"code","bf6f3da9":"code","a04b5641":"markdown","feb46807":"markdown","a4e5efef":"markdown","ee694831":"markdown","54a5a201":"markdown","632b5ec2":"markdown","6d531d72":"markdown","cce51c69":"markdown","ed79850c":"markdown","c814ab11":"markdown","554fa88e":"markdown","4d7563d0":"markdown","5ca0c0fe":"markdown","b06ff147":"markdown","ca746894":"markdown","014e616c":"markdown","0d8fd163":"markdown","cbbe149c":"markdown"},"source":{"8c22af16":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import Counter\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport wordcloud as wc","cf12fda9":"df = pd.read_csv(r'..\/input\/smsspamcollectioncsv\/SMSSpamCollection.csv')\ndf","9e5b7ff4":"def remove_special_chars(word):\n    special_chars = '!()-_?*.:\\'\\\",;<>&%#{}[]=\/+|~\u00a8'\n    for char in special_chars:\n        word = word.replace(char, '')\n    return word.lower()","9450c7aa":"def get_words_from_messages():\n    messages = df['message']\n    unique_words = []\n    idx = 0\n    for msg in messages:\n        lbl = df['label'][idx]\n        idx = idx + 1\n        words = msg.split(\" \")\n        for wrd in words:\n            wrd = remove_special_chars(wrd)\n            if len(wrd) < 1 or not str(wrd).isalnum():\n                continue\n            unique_words.append((wrd, lbl))\n    return unique_words","9233193b":"words_and_labels = get_words_from_messages()\nwords = [d[0] for d in words_and_labels]\nham_words = [d[0] for d in words_and_labels if d[1] in ['ham']]\nspam_words = [d[0] for d in words_and_labels if d[1] in ['spam']]\n","a38b065f":"occurences = Counter(words)\ndict(occurences)\n\nhamWordcloud = wc.WordCloud(max_words=100).generate(\" \".join(ham_words))\nspamWordcloud = wc.WordCloud(max_words=100).generate(\" \".join(spam_words))\n\nfig, axs = plt.subplots(1, 2, figsize=(18,6))\n\naxs[0].imshow(hamWordcloud, interpolation='bilinear')\naxs[1].imshow(spamWordcloud, interpolation='bilinear')\naxs[0].axis(\"off\")\naxs[1].axis(\"off\")\naxs[0].set_title('Word Cloud of Ham Messages')\naxs[1].set_title('Word Cloud of Spam Messages')\nplt.show()","21d71ce5":"value_counts = Counter(occurences.values())\ndict(value_counts)","c509ff0c":"names = list(range(0, len(occurences.keys())))\nvalues = list(occurences.values())\n\nfig, axs = plt.subplots(1, 1, figsize=(15,8), sharey=True)\n_ = plt.scatter(names, values)\n\n_ = fig.suptitle('Words distribution')","1bd1d056":"sorted_occ = dict(sorted(occurences.items(), key=lambda item: item[1],reverse=True))\nprint('1:', list(sorted_occ)[0])\nprint('2:', list(sorted_occ)[1])\nprint('3:', list(sorted_occ)[2])\n\n","5c227919":"bins = np.arange(start = 1, stop = 2000, step = 10)\na = np.array(list(value_counts.values()))\nplt.figure(figsize=(21, 6))\nplt.hist(a, bins =bins, cumulative=False)\nplt.title(\"Words Count Distribution\") \nplt.show()","b341f832":"lst = list(occurences.items())\nprint(\"count of unique words before removing least occured words from dataset: \", len(lst))\ndef remove_by_treshold(lst, trsh):\n    new_lst = []\n    for itm in lst:\n        if itm[1] > trsh:\n            new_lst.append(itm)\n\n    return new_lst\n\n\nstrong_words_and_counts = dict(remove_by_treshold(lst, 0))\nstrong_words = list(map(lambda tple: tple[0], list(strong_words_and_counts.items())))\nprint(\"count of unique words after removing least occured words from dataset:  \", len(strong_words_and_counts))\n\n# remove_by_treshold(lst, 25)\n","00778354":"names = list(range(0, len(strong_words_and_counts.keys())))\nvalues = list(strong_words_and_counts.values())\n\nfig, axs = plt.subplots(1, 2, figsize=(21,6), sharey=True)\naxs[0].scatter(names, values)\naxs[1].plot(names, values)\n_ = fig.suptitle('Strong Words distribution')","479967fb":"def check_if_word_is_strong(strong_words: list, word):\n    if word in strong_words:\n        return True\n    else:\n        return False","e17ec9b7":"def reshape_message(message: str):\n    word_list = message.split(' ')\n    new_list = []\n    for wrd in word_list:\n            wrd = remove_special_chars(wrd)\n            if check_if_word_is_strong(strong_words, wrd):\n                new_list.append(wrd)\n    return ' '.join(new_list)","7b857170":"msg = \"deneme yes deneme yes deneme yes no hi hey hii babe\"\nprint(reshape_message(msg))","0dfce453":"def reshape_label(lbl: str):\n    if lbl == 'ham' or lbl == 0:\n        return 0\n    elif lbl == 'spam' or lbl == 1:\n        return 1\n    else:\n        return","ea82378c":"df['label'] = df['label'].apply(reshape_label)\ndf['message'] = df['message'].apply(reshape_message)\ndf","66902f9f":"def calculate_acc(conf_mat):\n    sum = 0\n    total = 0\n    for i in range(len(conf_mat)):\n        for j in range(len(conf_mat[i])):\n            if i == j:\n                sum += conf_mat[i][j]\n            total += conf_mat[i][j]\n    return sum, total, (sum \/ total)","a6dc7ae6":"def print_acc(conf_mat):\n    print(conf_mat)\n    trues, total, acc = calculate_acc(conf_mat)\n    print()\n    print(trues, '\/', total , '=', acc)","24a4b1b5":"from sklearn import ensemble, linear_model, neighbors, svm, tree, neural_network, gaussian_process, naive_bayes\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, f1_score,roc_curve\nfrom sklearn.neighbors import KNeighborsClassifier\n\nvectorizer = CountVectorizer()\ntfidfconverter = TfidfTransformer()\n\nX_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.3, random_state=0)\n\nvectorizer.fit(X_train)\nX_train = vectorizer.transform(X_train)\ntfidfconverter.fit(X_train)\nX_train = tfidfconverter.transform(X_train).toarray()\n\n\nX_test = vectorizer.transform(X_test)\nX_test = tfidfconverter.transform(X_test).toarray()","15585908":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\nk3nn_predict = knn.predict(X_test)\n\nprint(classification_report(y_test, k3nn_predict))","08ed341a":"knn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\nk1nn_predict = knn.predict(X_test)\n\nprint(classification_report(y_test, k1nn_predict))","2891fd86":"nb_model= naive_bayes.GaussianNB()\nfit_nb = nb_model.fit(X_train,y_train)\nnaive_predict = fit_nb.predict(X_test)\nprint(classification_report(y_test, naive_predict))","43236c4b":"mnb_model = naive_bayes.MultinomialNB()\nfit_mnb = mnb_model.fit(X_train, y_train)\nmnb_predict = fit_mnb.predict(X_test)\n\nprint(classification_report(y_test, mnb_predict))","8f4c9710":"\nclassifier = svm.SVC(kernel='linear', random_state = 0)\nclassifier.fit(X_train, y_train)\n\nsvc_pred = classifier.predict(X_test)\nprint(classification_report(y_test, svc_pred))","42699c40":"classifier = linear_model.LogisticRegression(solver='liblinear', random_state=0)\nclassifier.fit(X_train, y_train)\n\nlnr_pred = classifier.predict(X_test)\nprint(classification_report(y_test, lnr_pred))\n\nprint(f1_score(y_test, lnr_pred))","ee26c852":"MLA = [\n    ensemble.RandomForestClassifier(),\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    \n    linear_model.LogisticRegression(solver='liblinear'),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    naive_bayes.MultinomialNB(),\n\n    neighbors.KNeighborsClassifier(n_neighbors=1),\n    neighbors.KNeighborsClassifier(n_neighbors=3),\n\n    svm.SVC(),\n    svm.LinearSVC(),  \n]","83e39e8b":"predicteds_and_names = []\n\nalg = 0;\nfor algorithm in MLA:\n        predicted = algorithm.fit(X_train, y_train).predict(X_test)\n        name = algorithm.__class__.__name__\n        predicteds_and_names.append((name, predicted))","1b89d6d3":"fig, ax = plt.subplots(3,5, figsize=(16, 8))\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.5)\nalg = 0\nfor i in range(3):\n    for j in range(5):\n        algorithm = predicteds_and_names[alg]\n        _ = sns.heatmap(confusion_matrix(y_pred=algorithm[1], y_true=y_test, normalize='pred'), ax=ax[i, j])\n        ax[i,j].set_title(algorithm[0])\n        alg += 1","d7bec8e7":"algorithm_columns = []\ncompare_algorithms_df = pd.DataFrame(columns = algorithm_columns)\nrow_index = 0\n\nfor algorithm in MLA:\n    \n    predicted = algorithm.fit(X_train, y_train).predict(X_test)\n    name = algorithm.__class__.__name__\n    compare_algorithms_df.loc[row_index,'Name'] = name\n    compare_algorithms_df.loc[row_index, 'Train Accuracy'] = round(algorithm.score(X_train, y_train), 4)\n    compare_algorithms_df.loc[row_index, 'Test Accuracy'] = round(algorithm.score(X_test, y_test), 4)\n    compare_algorithms_df.loc[row_index, 'Precision'] = precision_score(y_test, predicted)\n    compare_algorithms_df.loc[row_index, 'Recall'] = recall_score(y_test, predicted)\n    compare_algorithms_df.loc[row_index, 'F1 Score'] = f1_score(y_test, predicted)\n\n    row_index+=1\n    \ncompare_algorithms_df.sort_values(by = ['Test Accuracy'], ascending = False, inplace = True)    \ncompare_algorithms_df\n\n","bf6f3da9":"fig, ax = plt.subplots(2,3, figsize=(20, 11))\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=1)\nalg = 0\n\nsns.set_theme(style=\"whitegrid\")\ndf = compare_algorithms_df.melt('Name', var_name='Values',  value_name='Algorithms')\n_ = sns.lineplot(x=\"Name\", y=\"Algorithms\", hue='Values', data=df, ax=ax[0,0])\nax[0,0].set_title('All')\nax[0,0].tick_params(axis='x', labelrotation=90 )\n\nfor i in range(2):\n    for j in range(3):\n        if alg != 0:\n            sns.set_theme(style=\"whitegrid\")\n            name = compare_algorithms_df.columns[alg]\n            _ = sns.lineplot(x=\"Name\", y=name, data=compare_algorithms_df, ax=ax[i,j])\n            ax[i,j].set_title(name + ' of Algorithms')\n            ax[i,j].tick_params(axis='x', labelrotation=90)\n        alg += 1\n\n\n","a04b5641":"# BA\u015eARI ORANLARININ KAR\u015eILA\u015eTIRILMASI","feb46807":"## Guassian Naive Bayes","a4e5efef":"# VER\u0130 SET\u0130N\u0130N BEL\u0130RLENEN \u00d6ZELL\u0130KLERE G\u00d6RE F\u0130LTRELENMES\u0130","ee694831":"## Kelime Adetlerinin Da\u011f\u0131l\u0131m\u0131","54a5a201":"# VER\u0130 SET\u0130NDEK\u0130 MESAJLARIN \u0130\u015eLENMES\u0130","632b5ec2":"***Yaln\u0131zca bir kez ge\u00e7en kelimelerin \u00e7\u0131kar\u0131lmas\u0131*** : T\u00fcm veri setinde yaln\u0131zca 1 kez bulunan kelimeler mesaj\u0131n tipi hakk\u0131nda yeterli bir bilgi vermez. Bu y\u00fczden bu kelimeler g\u00f6rmezden gelinip veri hafifletilmi\u015ftir.","6d531d72":"## En fazla tekrar eden kelimeler","cce51c69":"## MultinomialNB","ed79850c":"# MAK\u0130NE \u00d6\u011eREN\u0130M\u0130 ALGOR\u0130TMALARI","c814ab11":"## Linear Regression","554fa88e":"# VER\u0130N\u0130N OKUNMASI","4d7563d0":"## 3NN Classifier","5ca0c0fe":"# VER\u0130 SET\u0130N\u0130N B\u00d6L\u00dcNMES\u0130","b06ff147":"## G\u00fc\u00e7l\u00fc kelimelerin da\u011f\u0131l\u0131m\u0131","ca746894":"## SVM","014e616c":"# KEL\u0130MELER\u0130N DA\u011eILIMI","0d8fd163":"# TEXT VER\u0130S\u0130N\u0130N VEKT\u00d6REL HALE D\u00d6N\u00dc\u015eT\u00dcR\u00dcLMES\u0130","cbbe149c":"## 1NN Classifier"}}