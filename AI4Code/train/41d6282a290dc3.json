{"cell_type":{"57058384":"code","9ba17dd8":"code","99123ba2":"code","5cba0140":"code","0291710c":"code","9260a35e":"code","c31ba980":"code","c270e331":"code","18aa82d0":"code","76f1cf6b":"code","c96e3c32":"code","88e7a15d":"code","5026e85c":"code","d859828b":"code","7e1975e1":"code","f01ae374":"code","eb9a76d1":"code","2709e266":"code","19cafc60":"code","25692da4":"markdown","5d113756":"markdown","5384043e":"markdown","ae4c6a0b":"markdown","aa69c3e7":"markdown","8ac4262e":"markdown","a1005745":"markdown"},"source":{"57058384":"import warnings\nwarnings.simplefilter(action=\"ignore\")\n\nimport os\nimport random\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold\n\n# Seed Everything\nseed = 13\nrandom.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\nnp.random.seed(seed)","9ba17dd8":"from sklearn.model_selection import KFold\n\n\nclass TargetEncoder:\n    def __init__(self, target, alpha=5):\n        self.target = target\n        self.alpha = alpha\n\n    def fit_transform(self, train, categorical):\n        self.train = train\n        self.categorical = categorical\n\n        # Create 5-fold cross-validation\n        kf = KFold(n_splits=5, random_state=1, shuffle=True)\n        train_feature = np.zeros(len(train))\n\n        # For each folds split\n        for train_index, test_index in kf.split(train):\n            cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n\n            # Calculate out-of-fold statistics and apply to cv_test\n            cv_test_feature = self._test_mean_target_encoding(cv_train, cv_test)\n\n            # Save new feature for this particular fold\n            train_feature[test_index] = cv_test_feature\n        return train_feature\n\n    def transform(self, test):\n        \n        # Get test target-encoded feature\n        test_feature = self._test_mean_target_encoding(self.train, test)\n\n        return test_feature\n\n    def _test_mean_target_encoding(self, train, test):\n        # Calculate global mean on the train data\n        global_mean = train[self.target].mean()\n\n        # Group by the categorical feature and calculate its properties\n        train_groups = train.groupby(self.categorical)\n        category_sum = train_groups[self.target].sum()\n        category_size = train_groups.size()\n\n        # Calculate smoothed mean target statistics\n        train_statistics = (category_sum + global_mean * self.alpha) \/ (category_size + self.alpha)\n\n        # Apply statistics to the test data and fill new categories\n        test_feature = test[self.categorical].map(train_statistics).fillna(global_mean)\n        return test_feature.values\n\n\nfrom sklearn.metrics import f1_score\n\ndef evaluate_macroF1_lgb(truth, predictions):  \n    # Follows the discussion in https:\/\/github.com\/Microsoft\/LightGBM\/issues\/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average=\"macro\")\n    \n    return (\"macroF1\", f1, True)","99123ba2":"def read_data(train_path, test_path):\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n    \n    return train, test","5cba0140":"def preprocess_data(train, test):\n    \n    # Concatenate train and test data together\n    data = pd.concat([train, test], sort=False)\n\n    # Drop duplicate and useless features\n    features_to_drop = [\"area2\", \"tamhog\", \"hhsize\", \"agesq\"]\n    features_to_drop += [x for x in data.columns if \"SQB\" in x]\n    data.drop(features_to_drop, axis=1, inplace=True)\n\n    # Transform original One-Hot-encoded features to a single column\n    for ohe in [\n        \"pared\",\n        \"piso\",\n        \"techo\",\n        \"abastagua\",\n        \"sanitario\",\n        \"energcocinar\",\n        \"elimbasu\",\n        \"epared\",\n        \"etecho\",\n        \"eviv\",\n        \"lugar\",\n        \"tipovivi\",\n        \"electricity\",\n    ]:\n        if ohe != \"electricity\":\n            ohe_cols = [x for x in train.columns if x.startswith(ohe)]\n        else:\n            ohe_cols = [\"public\", \"planpri\", \"noelec\", \"coopele\"]\n\n        data[ohe] = np.where(\n            data[ohe_cols].sum(axis=1) == 0, \"NEW_CAT\", data[ohe_cols].idxmax(axis=1)\n        )\n        data.drop(ohe_cols, axis=1, inplace=True)\n\n    # Fill in the missing data\n    data.fillna(-999, inplace=True)\n\n    train = data[: len(train)]\n    test = data[-len(test) :]\n\n    return train, test","0291710c":"def generate_features(train, test):\n    data = pd.concat([train, test], sort=False)\n\n    # Some feature engineering from: https:\/\/www.kaggle.com\/gaxxxx\/exploratory-data-analysis-lightgbm\n    data[\"adult\"] = data[\"hogar_adul\"] - data[\"hogar_mayor\"]\n    data[\"dependency_count\"] = data[\"hogar_nin\"] + data[\"hogar_mayor\"]\n    data[\"dependency\"] = np.where(data[\"adult\"] == 0, 1, data[\"dependency_count\"] \/ data[\"adult\"])\n    data[\"child_percent\"] = data[\"hogar_nin\"] \/ data[\"hogar_total\"]\n    data[\"elder_percent\"] = data[\"hogar_mayor\"] \/ data[\"hogar_total\"]\n    data[\"adult_percent\"] = data[\"hogar_adul\"] \/ data[\"hogar_total\"]\n\n    data[\"rent_per_bedroom\"] = data[\"v2a1\"] \/ data[\"bedrooms\"]\n    data[\"male_per_bedroom\"] = data[\"r4h3\"] \/ data[\"bedrooms\"]\n    data[\"female_per_bedroom\"] = data[\"r4m3\"] \/ data[\"bedrooms\"]\n    data[\"bedrooms_per_person_household\"] = data[\"hogar_total\"] \/ data[\"bedrooms\"]\n\n    data[\"escolari_age\"] = data[\"escolari\"] \/ data[\"age\"]\n\n    # Groupping features by a household (ID is idhogar)\n    aggr_mean_list = [\"rez_esc\", \"dis\", \"male\", \"female\"]\n    aggr_mean_list += [f\"estadocivil{x}\" for x in range(1, 8)]\n    aggr_mean_list += [f\"parentesco{x}\" for x in range(2, 13)]\n    aggr_mean_list += [f\"instlevel{x}\" for x in range(1, 10)]\n\n    other_list = [\"escolari\", \"age\", \"escolari_age\"]\n\n    for item in aggr_mean_list:\n        data[item + \"_mean\"] = data.groupby(\"idhogar\")[item].transform(\"mean\")\n\n    for item in other_list:\n        for function in [\"mean\", \"std\", \"min\", \"max\", \"sum\"]:\n            data[item + \"_\" + function] = (\n                data.groupby(\"idhogar\")[item].transform(function).fillna(0)\n            )\n\n    train = data[: len(train)]\n    test = data[-len(test) :]\n\n    return train, test","9260a35e":"# Read the data\ntrain, test = read_data(\n    train_path=\"..\/input\/costa-rican-household-poverty-prediction\/train.csv\",\n    test_path=\"..\/input\/costa-rican-household-poverty-prediction\/test.csv\",\n)","c31ba980":"train.head()","c270e331":"# Preprocess the data\ntrain, test = preprocess_data(train, test)","18aa82d0":"train.head()","76f1cf6b":"# Generate some features\ntrain, test = generate_features(train, test)","c96e3c32":"# Keep only the heads of the households\ntrain = train[train[\"parentesco1\"] == 1]\n\n# Transform target variables to the labels\ntarget_encoder = preprocessing.LabelEncoder()\ny = target_encoder.fit_transform(train[\"Target\"])\n\n# Drop all the ID variables\nX = train.drop([\"Id\", \"idhogar\", \"parentesco1\"], axis=1)\nX_test = test.drop([\"Id\", \"idhogar\", \"parentesco1\", \"Target\"], axis=1)","88e7a15d":"params = {\n    \"learning_rate\": 0.1,\n    \"objective\": \"multiclass\",\n    \"metric\": \"multi_logloss\",\n    \"n_estimators\": 1000,\n    \"class_weight\": \"balanced\",\n    \"colsample_bytree\": 0.9,\n    \"subsample\": 0.8,\n    \"subsample_freq\": 1,\n    \"num_class\": 4,\n    \"lambda_l2\": 1,\n}","5026e85c":"# Stratified K-Fold\nnum_folds = 5\nskf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1)\n\n# Initialize variables\ny_preds = np.zeros((len(X_test), 4))\nval_scores = []\n\nfor fold_n, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    # Get cross-validation split\n    X_train = X.iloc[train_index]\n    X_valid = X.iloc[valid_index]\n    \n    y_train = y[train_index]\n    y_valid = y[valid_index]\n    \n    # Transform all the categorical features into Target-Encoded\n    for f in X_train.columns:\n        if X_train[f].dtype == \"object\" and f not in [\"Id\", \"idhogar\", \"Target\"]:\n            te = TargetEncoder(target=\"Target\", alpha=5)\n\n            X_train[f] = te.fit_transform(X_train, f)\n            X_valid[f] = te.transform(X_valid)\n            X_test[f] = te.transform(X_test)\n            \n    X_train = X_train.drop([\"Target\"], axis=1)\n    X_valid = X_valid.drop([\"Target\"], axis=1)\n\n    # Train LightGBM model\n    clf = lgb.LGBMClassifier(**params)\n    clf = clf.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        verbose=100,\n        early_stopping_rounds=200,\n    )\n\n    # Make validation predictions\n    y_pred_valid = clf.predict(X_valid)\n    \n    importances = pd.DataFrame({\"feature\": X_train.columns, \"importance\": clf.feature_importances_})\n\n    # Evaluate the validation score\n    score = f1_score(y_valid, y_pred_valid, average=\"macro\")\n    val_scores.append(score)\n    print(f\"Fold {fold_n}. F1 Score: {score:.5f}\\n\")\n    \n    # Make predictions on the test set (summing up the folds)\n    y_preds += clf.predict_proba(X_test) \/ num_folds\n\nprint(\"Overall F1 Score: {:.3f}\".format(np.mean(val_scores) + np.std(val_scores)))","d859828b":"importances","7e1975e1":"\nsns.barplot(data=importances.sort_values(\"importance\", ascending=False).head(10), x=\"importance\", y=\"feature\")","f01ae374":"y_preds = np.argmax(y_preds, axis=1)\n\nsubmission = pd.DataFrame(\n    {\n        \"Id\": test[\"Id\"],\n        \"Target\": target_encoder.inverse_transform(y_preds).astype(int),\n    }\n)\nsubmission.to_csv(\"submission.csv\", index=False)","eb9a76d1":"submission.head()","2709e266":"train.Target.value_counts()","19cafc60":"submission.Target.value_counts()","25692da4":"### Preprocess data","5d113756":"### Machine Learning pipeline","5384043e":"### Train a model","ae4c6a0b":"### Target encoder definition","aa69c3e7":"### Generate some features","8ac4262e":"### Read data","a1005745":"### Make test predictions"}}