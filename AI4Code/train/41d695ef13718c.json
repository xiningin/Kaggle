{"cell_type":{"c6a7332f":"code","d8029584":"code","5a87fe9a":"code","41e16a79":"code","d31f0a83":"code","5a08a9dd":"code","8eb2f5ed":"code","4a76f0c8":"code","3f023ed3":"code","a6cf2dbd":"code","64ca9521":"code","07443c4e":"code","44740e8c":"code","59229825":"code","566d04e3":"code","a938c360":"code","0a95d748":"code","8b942bef":"code","5e6eae74":"code","a60dcdb5":"code","3bc0bd97":"code","397f5791":"code","f249e4ce":"code","d76a312d":"code","6b1868d6":"code","4400d81b":"code","a1610b78":"code","30993cd8":"code","0e14d494":"code","809eb2cc":"code","91519588":"code","6177ac88":"code","316199cf":"code","1d28b59b":"code","3e958a0c":"code","c63b703e":"code","f549a757":"code","e6ea1dc7":"code","25060eaf":"code","4cd46288":"code","583bef59":"code","48424fc6":"code","ceab1573":"code","b86b4b78":"code","c7b9092d":"code","eb398d62":"code","2dd3c7d7":"code","82f1a611":"code","673ae63d":"code","86990d31":"code","7820a648":"code","3528c724":"code","3559942b":"code","c5d684d0":"code","8e271057":"code","e6c4cc58":"code","97810ad4":"code","f72c4750":"code","d7f24ac8":"code","a65e9264":"code","9a52324d":"code","a91bc444":"code","32ff07ff":"code","3d82334c":"code","67ef0a72":"code","930e1ec6":"code","8ff98ad6":"code","c52b7b7b":"code","0a36f882":"code","f6e254ca":"code","a7b490a4":"code","b17c3c61":"code","80149531":"code","8977fac3":"code","3abf321a":"code","1dd899c6":"markdown","adecee4f":"markdown","595c208b":"markdown","294e0650":"markdown","b43a705f":"markdown","43f87a1a":"markdown","7e30f42b":"markdown","43dd986c":"markdown","56d01c53":"markdown","88887374":"markdown","7de6cf87":"markdown","ba739798":"markdown","ca9c5ef7":"markdown","b5e645ab":"markdown","239540b9":"markdown","58dd1776":"markdown"},"source":{"c6a7332f":"import pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem.snowball import SnowballStemmer\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas_profiling as pp\nimport seaborn as sns\nimport matplotlib as plt\n%matplotlib inline","d8029584":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a87fe9a":"#load the full set of the data\nblog_df = pd.read_csv(\"\/kaggle\/input\/blog-authorship-corpus\/blogtext.csv\")","41e16a79":"# check the shape of the data frame by using the shape attribute of the data frame\nblog_df.shape","d31f0a83":"#check if the data frame is properly loaded using the sample() method\nblog_df.sample(5)","5a08a9dd":"#Tip: As the dataset is large, use fewer rows. Check what is working well on your machine and decide accordingly.\n#Limiting the data and using fewer rows as the data size is large\n#blog_df = blog_df.head(10000)\n\nblog_df = pd.read_csv(\"\/kaggle\/input\/blog-authorship-corpus\/blogtext.csv\",nrows=100000)","8eb2f5ed":"blog_df.head()","4a76f0c8":"#check if the last 5 rows of the data frame using the tail() method\nblog_df.tail()","3f023ed3":"#check the feature\/columns  using the info method\nblog_df.info()","a6cf2dbd":"blog_df.gender.value_counts()","64ca9521":"#pp.ProfileReport(blog_df)","07443c4e":"#chceck for na values\nblog_df.isna().sum()","44740e8c":"sns.countplot(x='gender',data=blog_df)","59229825":"sns.countplot(x='sign',data=blog_df)","566d04e3":"#chceck for null values\nblog_df.isnull().sum()","a938c360":"# remove unwanted chars other than alphanumeric\npattern = \"[^\\w ]\"\nblog_df.text = blog_df.text.apply(lambda s : re.sub(pattern,\"\",s))","0a95d748":"blog_df.head(5)","8b942bef":"#covert text to lower\nblog_df.text = blog_df.text.apply(lambda s: s.lower())","5e6eae74":"blog_df.head(5)","a60dcdb5":"#remove unwanted spaces\nblog_df.text = blog_df.text.apply(lambda s: s.strip())","3bc0bd97":"blog_df.head()","397f5791":"#remove stopwords\nstopwords=set(stopwords.words('english'))","f249e4ce":"blog_df.text = blog_df.text.apply(lambda t: ' '.join([words for words in t.split() if words not in stopwords]) )","d76a312d":"blog_df.head()","6b1868d6":"# drop id and date columns\nblog_df.drop(labels=['id','date'], axis=1,inplace=True)","4400d81b":"blog_df.head()","a1610b78":"blog_df['labels'] = blog_df.apply(lambda col : [col['gender'],col['age'],col['topic'],col['sign']], axis=1)","30993cd8":"blog_df.head()","0e14d494":"#drop  gender,age,topic & sign as they are already merged to labels column\nblog_df.drop(columns=['gender','age','topic','sign'], axis=1, inplace=True)","809eb2cc":"blog_df.head()","91519588":"X= blog_df.text\ny = blog_df.labels","6177ac88":"# split X and y into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=2,test_size = 0.2)","316199cf":"print(X_train.shape)\nprint(y_train.shape)","1d28b59b":"print(X_test.shape)\nprint(y_test.shape)","3e958a0c":"X_test","c63b703e":"cvect = CountVectorizer(ngram_range=(1,2))","f549a757":"#Feed SMS data to CountVectorizer\ncvect.fit(X_train)\n\n#Check the vocablury size\nlen(cvect.vocabulary_)","e6ea1dc7":"cvect.get_feature_names()","25060eaf":"X_train_ct = cvect.transform(X_train)","4cd46288":"type(X_train_ct)","583bef59":"X_train_ct","48424fc6":"X_train_ct[0]","ceab1573":"X_test_ct = cvect.transform(X_test)","b86b4b78":"X_test_ct","c7b9092d":"cvect.get_feature_names()[:10]","eb398d62":"print(X_train_ct)","2dd3c7d7":"print(X_test_ct)","82f1a611":"label_counts=dict()\n\nfor labels in blog_df.labels.values:\n    for label in labels:\n        if label in label_counts:\n            label_counts[str(label)]+=1\n        else:\n            label_counts[str(label)]=1","673ae63d":"label_counts","86990d31":"from sklearn.preprocessing import MultiLabelBinarizer\nbinarizer=MultiLabelBinarizer(classes=sorted(label_counts.keys()))","7820a648":"y_train = binarizer.fit_transform(y_train)","3528c724":"y_test = binarizer.transform(y_test)","3559942b":"y_test","c5d684d0":"y_train","8e271057":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression","e6c4cc58":"model=LogisticRegression(solver='lbfgs', max_iter=100)\nmodel=OneVsRestClassifier(model)\nmodel.fit(X_train_ct,y_train)","97810ad4":"Ypred=model.predict(X_test_ct)","f72c4750":"Ypred","d7f24ac8":"y_test","a65e9264":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import recall_score\n\ndef display_metrics_micro(Ytest, Ypred):\n    print('Accuracy score: ', accuracy_score(Ytest, Ypred))\n    print('F1 score: Micro', f1_score(Ytest, Ypred, average='micro'))\n    print('Average precision score: Micro', average_precision_score(Ytest, Ypred, average='micro'))\n    print('Average recall score: Micro', recall_score(Ytest, Ypred, average='micro'))\n    \n    \ndef display_metrics_macro(Ytest, Ypred):\n    print('Accuracy score: ', accuracy_score(Ytest, Ypred))\n    print('F1 score: Macro', f1_score(Ytest, Ypred, average='macro'))\n    print('Average recall score: MAcro', recall_score(Ytest, Ypred, average='macro'))\n    \ndef display_metrics_weighted(Ytest, Ypred):\n    print('Accuracy score: ', accuracy_score(Ytest, Ypred))\n    print('F1 score: weighted', f1_score(Ytest, Ypred, average='weighted'))\n    print('Average precision score: weighted', average_precision_score(Ytest, Ypred, average='weighted'))\n    print('Average recall score: weighted', recall_score(Ytest, Ypred, average='weighted'))\n    \n    ","9a52324d":"display_metrics_micro(y_test,Ypred)","a91bc444":"display_metrics_macro(y_test,Ypred)","32ff07ff":"display_metrics_weighted(y_test,Ypred)","3d82334c":"preds = Ypred[:15]\nactuals = y_test[:15]","67ef0a72":"five_actual = binarizer.inverse_transform(actuals)\nfive_actual","930e1ec6":"five_pred = binarizer.inverse_transform(preds)\nfive_pred","8ff98ad6":"print(binarizer.inverse_transform(Ypred)[877])\nprint(binarizer.inverse_transform(y_test)[877])","c52b7b7b":"print(binarizer.inverse_transform(Ypred)[514])\nprint(binarizer.inverse_transform(y_test)[514])","0a36f882":"print(binarizer.inverse_transform(Ypred)[99])\nprint(binarizer.inverse_transform(y_test)[99])","f6e254ca":"print(binarizer.inverse_transform(Ypred)[499])\nprint(binarizer.inverse_transform(y_test)[499])","a7b490a4":"print(binarizer.inverse_transform(Ypred)[699])\nprint(binarizer.inverse_transform(y_test)[699])","b17c3c61":"import random\nj=[]\nfor i in range(5):\n    j.append(random.randint(300,len(Ypred)))\n   \nprint(j)\n\nfor k in j:    \n    print(binarizer.inverse_transform(Ypred)[k])\n    print(binarizer.inverse_transform(y_test)[k])\n    \n        ","80149531":"Ypred_inversed = binarizer.inverse_transform(Ypred)\ny_test_inversed = binarizer.inverse_transform(y_test)\nfor i in range(5):\n    print('Text:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n        X_test_ct[i],\n        ','.join(y_test_inversed[i]),\n        ','.join(Ypred_inversed[i])\n    ))","8977fac3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\n\ndef build_model_train(X_train, y_train, X_valid=None, y_valid=None, C=1.0, model='lr'):\n    if model=='lr':\n        model = LogisticRegression(C=C, penalty='l1', dual=False, solver='liblinear')\n        model = OneVsRestClassifier(model)\n        model.fit(X_train, y_train)\n    \n    elif model=='svm':\n        model = LinearSVC(C=C, penalty='l1', dual=False, loss='squared_hinge')\n        model = OneVsRestClassifier(model)\n        model.fit(X_train, y_train)\n    \n    elif model=='nbayes':\n        model = MultinomialNB(alpha=1.0)\n        model = OneVsRestClassifier(model)\n        model.fit(X_train, y_train)\n        \n    elif model=='lda':\n        model = LinearDiscriminantAnalysis(solver='svd')\n        model = OneVsRestClassifier(model)\n        model.fit(X_train, y_train)\n\n    return model","3abf321a":"models = ['lr','svm','nbayes']\nfor model in models:\n    model = build_model_train(X_train_ct,y_train,model=model)\n    model.fit(X_train_ct,y_train)\n    Ypred=model.predict(X_test_ct)\n    print(\"\\n\")\n    print(f\"**displaying  metrics for the mode {model}\\n\")\n    display_metrics_micro(y_test,Ypred)\n    print(\"\\n\")\n    print(\"\\n\")\n    display_metrics_macro(y_test,Ypred)\n    print(\"\\n\")\n    print(\"\\n\")\n    display_metrics_weighted(y_test,Ypred)\n    print(\"\\n\")\n    print(\"\\n\")\n    ","1dd899c6":"# In Micro-average method, \nyou sum up the individual true positives, false positives, and false negatives of the system for different sets and the apply them to get the statistics.\n\n# Macro-average Method\nThe method is straight forward. Just take the average of the precision and recall of the system on different sets\n\n","adecee4f":"# 10.Print true label and predicted label for any five examples","595c208b":" # Print true label and predicted label for any five examples","294e0650":"# 6 Create a dictionary to get the count of every label i.e. the key will be label name and value will be the total count of the label. Check below image for reference ","b43a705f":"__Key Notes__:\n\n1. we have solved  Multilabel classification problem that predicts multiple features of the author of a given text\n2. Loading the data and  required basic EDA and data inspection has been done\n2. The text has been pre processed like cleansing it(removing the unnecessary chars, removing the spaces, converting the case to lower) and also removing the stop words, vectorizing the features\n3. Preparing the date, splitting them to train and test\n4. using multilable binarizers, also various classifier models are trained and the predictions are made and also the accuracy, f1 score, Avg precision and recall scores are calculated.\n","43f87a1a":"# 4. Separate features and labels, and split the data into training and testing ","7e30f42b":"# 5. Vectorize the features\na. Create a Bag of Words using count vectorizer\n\ni. Use ngram_range=(1, 2)\n\nii. Vectorize training and testing features\n\nb. Print the term-document matrix","43dd986c":"# EDA with Pandas Profiler","56d01c53":"# 7. Transform the labels \nAs we have noticed before, in this task each example can have multiple tags. To deal with\nsuch kind of prediction, we need to transform labels in a binary form and the prediction will be\na mask of 0s and 1s. For this purpose, it is convenient to use MultiLabelBinarizer from sklearn\na. Convert your train and test labels using MultiLabelBinarizer","88887374":"# 1. Load the dataset","7de6cf87":"# 2 Preprocess rows of the \u201ctext\u201d column \n\na. Remove unwanted characters\n\nb. Convert text to lowercase\n\nc. Remove unwanted spaces\n\nd. Remove stopwords","ba739798":"# Use a linear classifier (LinearSVC is used in the following) of your choice, wrap it up in OneVsRestClassifier to train it on every label","ca9c5ef7":"# Blog Authorship Corpus\nClassification is probably the most popular task that you would deal with in real life.\nText in the form of blogs, posts, articles, etc. is written every second. It is a challenge to predict the\ninformation about the writer without knowing about him\/her.\nWe are going to create a classifier that predicts multiple features of the author of a given text.\nWe have designed it as a Multilabel classification problem.\nDataset\nBlog Authorship Corpus\n\nOver 600,000 posts from more than 19 thousand bloggers\nThe Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from\nblogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million\nwords - or approximately 35 posts and 7250 words per person.\nEach blog is presented as a separate file, the name of which indicates a blogger id# and the\nblogger\u2019s self-provided gender, age, industry, and astrological sign. (All are labeled for gender and\nage but for many, industry and\/or sign is marked as unknown.)\n\n\nAll bloggers included in the corpus fall into one of three age groups:\n8240 \"10s\" blogs (ages 13-17),\n8086 \"20s\" blogs(ages 23-27)\n2994 \"30s\" blogs (ages 33-47)\n\nFor each age group, there is an equal number of male and female bloggers.\nEach blog in the corpus includes at least 200 occurrences of common English words. All formatting\nhas been stripped with two exceptions. Individual posts within a single blogger are separated by the\ndate of the following post and links within a post are denoted by the label urllink.","b5e645ab":"# 8 Choose a classifier \nIn this task, we suggest using the One-vs-Rest approach, which is implemented in\nOneVsRestClassifier class. In this approach k classifiers (= number of tags) are trained. As a\nbasic classifier, use LogisticRegression . It is one of the simplest methods, but often it\nperforms good enough in text classification tasks. It might take some time because the\nnumber of classifiers to train is large.\n\n\n# 9. Fit the classifier, make predictions and get the accuracy \na. Print the following\ni. Accuracy score\nii. F1 score\niii. Average precision score\niv. Average recall score\nv. Tip: Make sure you are familiar with all of them. How would you expect the\nthings to work for the multi-label scenario? Read about micro\/macro\/weighted\naveraging","239540b9":"# 3. As we want to make this into a multi-label classification problem, you are required to merge all the label columns together, so that we have all the labels together for a particular sentence","58dd1776":"# Print true label and predicted label for any five examples"}}