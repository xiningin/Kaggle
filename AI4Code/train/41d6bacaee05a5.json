{"cell_type":{"c2ccf5fa":"code","7771d472":"code","2c452780":"code","8ff79145":"code","44138c9f":"code","491d9a71":"code","90b162a7":"code","f54d1514":"code","9e5e29a8":"code","e45ca0e0":"code","93e6f374":"code","297354df":"code","1779defe":"code","d043455b":"code","6a457e0f":"code","d7bf3381":"code","735e69d7":"code","11353dcd":"code","2628d846":"code","f564722a":"code","6e8ed79a":"code","86bad291":"code","3f1879a8":"code","0cb111d7":"code","f26da638":"code","f1ccd83f":"code","93bd38a3":"code","eab83faa":"code","7c4a7335":"code","1944b7a5":"code","833321f7":"code","256b2833":"code","3c32d392":"code","9f1db667":"code","40d37e8f":"code","ca112653":"code","3b074cfa":"code","a0b8d6ea":"code","0f2baaa3":"code","b08850b4":"code","cdf225a3":"code","1e4356d4":"code","9dccffaa":"code","9ddcf18e":"code","e9b82103":"code","b3e1fa47":"code","87c53a99":"code","faf961d1":"code","0caa33a2":"code","c28324e5":"markdown","f15288f2":"markdown","345085c0":"markdown","2b0f22b5":"markdown","209ba8a6":"markdown","b1aa292d":"markdown"},"source":{"c2ccf5fa":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport re \nimport scipy\nfrom scipy import sparse\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.linear_model import Ridge\nimport zipfile\nimport string\nimport nltk\nimport string\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\")) \nlemmatizer = WordNetLemmatizer() ","7771d472":"!pip install seaborn==0.11.0","2c452780":"# train_csv_zip_path = '..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip'\n# with zipfile.ZipFile(train_csv_zip_path) as zf:\n#     zf.extractall('.\/')\n","8ff79145":"# train_csv_path = '..\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/train.csv'\n# sample_sub_path = '..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv'\n# comments_to_score_path = '..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv'\n# val_path='..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv'","44138c9f":"df_train = pd.read_csv(\"..\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\ndf_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","491d9a71":"df_train.head()","90b162a7":"df_sub.head()","f54d1514":"for col in ['toxic','severe_toxic','obscene','threat','insult','identity_hate']:\n#     print(f'------------------------{col}-----------------------')\n    print(col.center(40, '.'))\n    display(df_train.loc[df_train[col]==1,['comment_text',col]].sample(2))","9e5e29a8":"def clean_text(text):\n#replace the html characters with \" \"\n    text=re.sub('<.*?>', ' ', text)  \n#remove the punctuations\n    text = text.translate(str.maketrans(' ',' ',string.punctuation))\n#consider only alphabets and numerics\n    text = re.sub('[^a-zA-Z]',' ',text)  \n#replace newline with space\n    text = re.sub(\"\\n\",\" \",text)\n#convert to lower case\n    text = text.lower()\n#split and join the words\n    text=' '.join(text.split())\n    return text\n\ndef stopwords(input_text, stop_words):\n    word_tokens = word_tokenize(input_text) \n    output_text = [w for w in word_tokens if not w in stop_words]\n    output = [] \n    for w in word_tokens: \n        if w not in stop_words:\n            output.append(w)\n            \n    text = ' '.join(output)\n    return text\n\n","e45ca0e0":"unrelevant_words = ['wiki','wikipedia','page']\n#Clean step 1, 2 and 3\ndf_train['comment_text'] = df_train['comment_text'].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\n\n#Clean Step 4\ndf_train['comment_text'] = df_train['comment_text'].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\n\n#Clean Step 5\ndf_train['comment_text'] = df_train['comment_text'].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\n\ndf_train.head()","93e6f374":"for col in ['toxic','severe_toxic','obscene','threat','insult','identity_hate']:\n    print(col.center(40, '.'))\n    display(df_train.loc[df_train[col]==1,['comment_text',col]].sample(2))","297354df":"df_train['y'] = (df_train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\ndf_train_binary = df_train[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf_train_binary.sample(5)","1779defe":" display(df_train_binary.loc[df_train_binary['y']==1,['text','y']].sample(5))","d043455b":" display(df_train_binary.loc[df_train_binary['y']==0,['text','y']].sample(5))","6a457e0f":"df_lt=df_train_binary.loc[df_train_binary['y']==0]\ndf_lt.head()\n# print(len(df_lt))","d7bf3381":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n# text_lt = \" \".join(t for t in df[\"less_toxic\"])\n\ntext_lt = \" \".join(t for t in df_lt['text'])\nprint (\"There are {} words in the combination of all less_toxic reviews.\".format(len(text_lt)))\n\n#Generate a word cloud image\nwordcloud = WordCloud(stopwords=STOPWORDS,background_color=\"white\").generate(text_lt)\n\n# Display the generated image:\n# the matplotlib way:\nplt.figure(figsize=[10,10])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","735e69d7":"df_mt=df_train_binary.loc[df_train_binary['y']==1]\ndf_mt.head()","11353dcd":"text_mt = \" \".join(t for t in df_mt['text'])\nprint (\"There are {} words in the combination of all more_toxic reviews.\".format(len(text_mt)))\n\n#Generate a word cloud image\nwordcloud = WordCloud(stopwords=STOPWORDS,background_color=\"black\").generate(text_mt)\n\n# Display the generated image:\n# the matplotlib way:\nplt.figure(figsize=[10,10])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","2628d846":"df_train_binary['y'].value_counts(normalize=True)","f564722a":"toxic_len = (df_train_binary['y'] == 1).sum()\nprint(toxic_len)","6e8ed79a":"df_train_balanced = df_train_binary[df_train_binary['y'] == 0].sample(n=toxic_len)\ndf_train_balanced['y'].value_counts(normalize=True)","86bad291":"df_train_b = pd.concat([df_train_binary[df_train_binary['y'] == 1], df_train_balanced])\ndf_train_b['y'].value_counts()","3f1879a8":"# vec = TfidfVectorizer()","0cb111d7":"# X = vec.fit_transform(df_train_b['text'])\n# X","f26da638":"# from sklearn.naive_bayes import MultinomialNB\n# model = MultinomialNB()\n# model.fit(X, df_train_b['y'])","f1ccd83f":"# df_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\n# df_val.head()","93bd38a3":"# unrelevant_words = ['wiki','wikipedia','page']\n# #Clean step 1, 2 and 3\n# df_val['less_toxic'] = df_val['less_toxic'].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\n\n# #Clean Step 4\n# df_val['less_toxic'] = df_val['less_toxic'].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\n\n# #Clean Step 5\n# df_val['less_toxic'] = df_val['less_toxic'].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\n\n# df_val.head()","eab83faa":"# df_val['more_toxic'] = df_val['more_toxic'].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\n\n# #Clean Step 4\n# df_val['more_toxic'] = df_val['more_toxic'].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\n\n# #Clean Step 5\n# df_val['more_toxic'] = df_val['more_toxic'].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\n\n# df_val.head()","7c4a7335":"# X_less_toxic = vec.transform(df_val['less_toxic'])\n# X_more_toxic = vec.transform(df_val['more_toxic'])","1944b7a5":"# p1 = model.predict_proba(X_less_toxic)\n# p2 = model.predict_proba(X_more_toxic)","833321f7":"# (p1[:, 1] < p2[:, 1]).mean()","256b2833":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\nfeatures = tfidf.fit_transform(df_train_b['text']).toarray()\nlabels = df_train_b['y']\nfeatures.shape","3c32d392":"# features_less_toxic = (df_val['less_toxic'])\n# tfidf.transform(features_less_toxic).toarray()\n# X_val_less_toxic =tfidf.fit_transform(features_less_toxic).toarray()\n# # features_more_toxic = tfidf.fit_transform(df_val['more_toxic']).toarray()\n# X_val_less_toxic.shape","9f1db667":"# from io import StringIO\n# col = ['y', 'text']\n# df = df_train_binary[col]\n# df = df[pd.notnull(df['text'])]\n# df.columns = ['y', 'text']\n# df['category_id'] = df['y'] .factorize()[0]\n# category_id_df = df[['y', 'category_id']].drop_duplicates().sort_values('category_id')\n# category_to_id = dict(category_id_df.values)\n# id_to_category = dict(category_id_df[['category_id', 'y']].values)\n# df.head()\n","40d37e8f":"# from sklearn.feature_selection import chi2\n# import numpy as np\n# N = 2\n# for y,category_id in sorted(category_to_id.items()):\n#   features_chi2 = chi2(features, labels == category_id)\n#   indices = np.argsort(features_chi2[0])\n#   feature_names = np.array(tfidf.get_feature_names())[indices]\n#   unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n#   bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n#   print(\"# '{}':\".format(y))\n#   print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n#   print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))","ca112653":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB\nmodels = [\n    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state=0),\n]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\nimport seaborn as sns\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()","3b074cfa":"cv_df.groupby('model_name').accuracy.mean()","a0b8d6ea":"\n# features_less_toxic = tfidf.transform(df_val['less_toxic'])\n# features_less_toxic =features_less_toxic.fit_transform(president)\n","0f2baaa3":"\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\n\nsvm = LinearSVC()\nclf = CalibratedClassifierCV(svm) \nclf.fit(features, labels)\n\n\n","b08850b4":"import seaborn as sns\ny_pred = clf.predict(features)\nfrom sklearn.metrics import confusion_matrix\nconf_mat = confusion_matrix(labels, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\ncmap = \"tab20\"\nsns.heatmap(conf_mat, annot=True, fmt='d',cmap=cmap)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","cdf225a3":"df_test = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\ndf_test.head()","1e4356d4":"df_test.info()","9dccffaa":"X_test = df_test['text']\nX_test.head()","9ddcf18e":"# X_test= tfidf.fit_transform(df_test['text']).toarray()\nX_test= tfidf.transform(df_test['text'])\nX_test.shape","e9b82103":"# svm = LinearSVC()\n# clf = CalibratedClassifierCV(svm) \n# clf.fit(features, labels)\ny_test = clf.predict_proba(X_test)\nlen(y_test)","b3e1fa47":"df_test['score'] = y_test[:, 1]\ndf_test['score'].head()","87c53a99":"df_test.head()","faf961d1":"df_test[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","0caa33a2":"# model = LogisticRegression(random_state=0)\n# # X_train, y_train = train_test_split(features, labels)\n# model.fit(features, labels)\n# y_pred = model.predict(X_test)\n# len(y_pred)","c28324e5":"data preprocessing\n\nfollow the instructions: https:\/\/medium.com\/analytics-vidhya\/text-cleaning-in-natural-language-processing-nlp-bea2c27035a6\n\nData Preprocessing must include the follows:\n\nRemoving HTML characters,ASCII\n\nConvert Text to Lowercase\n\nRemove Punctuation's\n\nRemove Stop words\n\nTokenization\n\nStemming vs Lemmatization\n","f15288f2":"**TF-IDF**","345085c0":"reference:\n\nhttps:\/\/medium.com\/analytics-vidhya\/text-cleaning-in-natural-language-processing-nlp-bea2c27035a6","2b0f22b5":"**Unbalanced dataset**","209ba8a6":"**Balanced dataset**","b1aa292d":"data collecting"}}