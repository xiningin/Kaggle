{"cell_type":{"6fee4716":"code","5943563f":"code","f5436be7":"code","002f3991":"code","b7ba4fe7":"code","4be81a64":"code","5e71afa5":"code","3b62ed15":"code","aa6fed65":"code","ec8ca9bc":"code","2717c670":"code","b7d5da75":"code","ceb14306":"code","886a86aa":"code","8f484e7c":"code","e9f19df0":"code","2fa5b6da":"code","c5344f14":"code","21ffe2a6":"code","4fcc6c2e":"code","c5ba9485":"code","41d3b327":"code","c7be87fa":"code","ef8ca9e1":"code","3b1b0392":"code","47b68bf5":"code","ce2a0d48":"code","720fe8bb":"code","121142f2":"code","354f7124":"code","0fa4e4fd":"code","aa35fe77":"code","68251d06":"code","82be3e97":"code","a56b0eff":"code","d154891a":"code","f9bba33f":"code","d69f8aef":"code","b30587a4":"code","d6c45806":"code","6dfa9714":"code","b1aa74ea":"code","e7ee3260":"code","01fba9c9":"code","ad760634":"code","b373cdcd":"code","353156b6":"code","918f183e":"code","6ad4874a":"code","9d868dbf":"code","033f2d04":"code","021581d8":"code","eaa47787":"code","8ae5d2c3":"code","7e7935c4":"code","375a090d":"code","4b9b5c68":"code","a8c55e78":"code","f5a3b911":"code","76306c3e":"code","23ec97bb":"code","00da1dc3":"code","8be9c911":"code","fe6bf85c":"code","0a3b087f":"code","001b0b47":"code","ce36abd9":"code","32b803a6":"code","a6e1de49":"code","454f8b73":"code","7122bd69":"code","46243bdc":"code","5dbbee27":"code","246a9352":"code","38259212":"code","a447e836":"code","a177979c":"code","9cb4d0a6":"code","ce6e5ffd":"code","94f70d6a":"code","36c97bdc":"code","b84406e0":"code","7f7e7403":"code","3de9d0d1":"code","f8f8bc4d":"code","5b85b546":"code","88f1400f":"code","05b5cec2":"code","e76039f6":"code","c362ab89":"code","ce2afe3f":"code","61382637":"code","91d20c0d":"code","f69a1be4":"code","b2b68e78":"code","b4dc4ef9":"code","0ec7ea13":"code","14b27ec9":"code","32739c4c":"code","c0d52eb5":"code","5dd6c09c":"code","46ca899c":"code","5e4e591c":"markdown","74455293":"markdown","30fb31cf":"markdown","b9fd1243":"markdown","bd03b00e":"markdown","f132d193":"markdown","700a723c":"markdown","7fb4295d":"markdown","13c06f37":"markdown","fba3fd00":"markdown","296bf7a3":"markdown"},"source":{"6fee4716":"##Importing Packages for Data Manipulation and Analysis##\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport sklearn as sk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.feature_selection import SelectFromModel\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\n\n##Importing Packages for Data Visualization##\nimport seaborn as sns\nsns.set()\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","5943563f":"##Importing PIMA Population Diabetes Dataset - obtained from from https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database##\npima_diabetes_data = pd.read_csv(r'..\/input\/pima-indians-diabetes-database\/diabetes.csv')\n\n##General Overview of Data Column Characteristics##\npima_diabetes_data.info() #9 Columns (8 predictors, 1 response (Outcome)); 768 non-null observations for all columns.","f5436be7":"##Applying train-test split before proceeding with exploratory data analysis segment##\n\n##Splitting y (response) from X variables (predictors)\ny = pima_diabetes_data.loc[:,['Outcome']]\n\nX = pima_diabetes_data\nX = X.drop(['Outcome'], axis=1)\n\n##Splitting data into training (80%) and test (20%) sets (while keeping balanced)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, train_size = 0.80, random_state = 2021, stratify=y)","002f3991":"##Overview of Columns of Training Dataset##\n##X_train Column Characteristics\nX_train.info() #614 Observations, 7 columns.\n\n##y_train Column Characteristics\ny_train.info() #614 Observations, 1 columns.\n\n#Therefore, the test data portion comprises of 154 observations.","b7ba4fe7":"##Cross-checking for any null values (double checking##\nprint(X_train.isnull().sum()) #No null values in training predictor set.\nprint(y_train.isnull().sum()) #No null values in training target set.","4be81a64":"##Determining Descriptive Statistics for Training Predictor Set##\nX_train.describe().round(1) ##Rounding descriptive statistics to 1 decimal place.","5e71afa5":"##Determining Descriptive Statistics for Training Target Set##\ny_train_cat = y_train.astype('category') #Creating a variable within which y_train is converted to a categorical datatype.\n\ny_train_cat.describe() #Describing count split; The top occurence is 0, which corresponds to patients with no onset of diabetes. 400 out of 614 patients (65%) had no indicated onset of diabetes in this dataset.","3b62ed15":"##Creating distribution plots for all predictor variables (all are continuous numerical variables)##\nprint(X_train.columns) #Printing column names for reference.","aa6fed65":"##Histogram for 'Pregnancies' Predictor Variable.\nplt.hist(X_train['Pregnancies'], color='green');\nplt.xlabel('Number of Times Pregnant')\nplt.ylabel('Patient Count')\nplt.title(\"Distribution of 'Pregnancies' Predictor Variable\");","ec8ca9bc":"##Histogram for 'Glucose' Predictor Variable.\nplt.hist(X_train['Glucose'], color='green');\nplt.xlabel('Plasma glucose concentration after tolerance test')\nplt.ylabel('Patient Count')\nplt.title(\"Distribution of 'Glucose' Predictor Variable\");","2717c670":"##Histogram for 'BloodPressure' Predictor Variable.\nplt.hist(X_train['BloodPressure'], color='green');\nplt.xlabel('Diastolic blood pressure (mm Hg)')\nplt.ylabel('Patient Count')\nplt.title(\"Distribution of 'BloodPressure' Predictor Variable\");","b7d5da75":"##Histogram for 'SkinThickness' Predictor Variable.\nplt.hist(X_train['SkinThickness'], color='green');\nplt.xlabel('Triceps skin fold thickness (mm)')\nplt.ylabel('Patient Count')\nplt.title(\"Distribution of 'SkinThickness' Predictor Variable\");","ceb14306":"##Histogram for 'Insulin' Predictor Variable.\nplt.hist(X_train['Insulin'], color='green');\nplt.xlabel('2-Hour serum insulin (mu U\/ml)')\nplt.ylabel('Patient Count')\nplt.title(\"Distribution of 'Insulin' Predictor Variable\");","886a86aa":"##Histogram for 'BMI' Predictor Variable.\nplt.hist(X_train['BMI'], color='green');\nplt.xlabel('Body mass index (weight in kg\/(height in m)^2)')\nplt.ylabel('Patient Count')\nplt.title(\"Distribution of 'BMI' Predictor Variable\");","8f484e7c":"##Histogram for 'Diabetes pedigree function' Predictor Variable.\nplt.hist(X_train['DiabetesPedigreeFunction'], color='green');\nplt.xlabel('Diabetes pedigree function')\nplt.ylabel('Patient Count')\nplt.title(\"Distribution of 'Diabetespedigreefunction' Predictor Variable\");","e9f19df0":"##Histogram for 'Age' Predictor Variable.\nplt.hist(X_train['Age'], color='green');\nplt.xlabel('Age(years)')\nplt.ylabel('Patient Count')\nplt.title(\"Distribution of 'Age' Predictor Variable\");","2fa5b6da":"##Creating Boxplots to display relative difference in means of each variable between the Occurrence (1) group and Non-Ocurrence (0) groups\n\n##Creating a combined training dataframe dedicated to making these subplots.\ncombined_training_data = pd.concat([X_train, y_train], axis=1)\ncombined_training_data.head()","c5344f14":"##Creating Series of Subplots\nf, axes = plt.subplots(4,2,figsize=(20,15))\n\nsns.boxplot(x='Outcome', y='Pregnancies', data=combined_training_data, orient='v', ax=axes[0,0])\nsns.boxplot(x='Outcome', y='Glucose', data=combined_training_data, orient='v', ax=axes[0,1])\nsns.boxplot(x='Outcome', y='BloodPressure', data=combined_training_data, orient='v', ax=axes[1,0])\nsns.boxplot(x='Outcome', y='SkinThickness', data=combined_training_data, orient='v', ax=axes[1,1])\nsns.boxplot(x='Outcome', y='Insulin', data=combined_training_data, orient='v', ax=axes[2,0])\nsns.boxplot(x='Outcome', y='BMI', data=combined_training_data, orient='v', ax=axes[2,1])\nsns.boxplot(x='Outcome', y='DiabetesPedigreeFunction', data=combined_training_data, orient='v', ax=axes[3,0])\nsns.boxplot(x='Outcome', y='Age', data=combined_training_data, orient='v', ax=axes[3,1]);","21ffe2a6":"##Building A Pearson\/Point Biserial Correlation Matrix for Training Data##\ncorr_combined = combined_training_data\nact_corr = corr_combined.corr()\nmatrix = np.tril(act_corr)\nf, ax = plt.subplots(figsize=(15,12))\nsns.heatmap(act_corr, vmax=0.8, annot=True, mask=matrix)","4fcc6c2e":"##Importing Random Forest Classifier##\nfrom sklearn.ensemble import RandomForestClassifier","c5ba9485":"##Applying Recursive Feature Elimination (RFE) with cross-validation for Random Forest Classification feature selection.##\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=2021)\n\n##Identification of optimal number of features to select with RFECV approach. Selecting 3 folds in attempt to avoid overfitting.\nopt_feat_num_rfecv = RFECV(estimator = rf_classifier, step=1, cv=StratifiedKFold(3), scoring='balanced_accuracy', min_features_to_select=1)\n\nopt_feat_num_rfecv.fit(X_train, np.ravel(y_train))\nprint(\"Optimal number of features selected using RFECV: %d\"%opt_feat_num_rfecv.n_features_) #6 out of 8 selected as important.\n\n#Plot reference cited from: https:\/\/scikit-learn.org\/stable\/auto_examples\/feature_selection\/plot_rfe_with_cross_validation.html\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (# of correct classifications)\")\nplt.plot(range(1,\n               len(opt_feat_num_rfecv.grid_scores_) + 1),\n         opt_feat_num_rfecv.grid_scores_)\nplt.show() ","41d3b327":"##Training the random forest classifier with optimal number of features already identified.\nrfe_classifier = RFE(estimator=rf_classifier, n_features_to_select=6, step=1)\nrfe_classifier.fit(X_train, np.ravel(y_train))","c7be87fa":"##Determining features of highest importance for the random forest model.\nrf_feat = pd.DataFrame()\nrf_feat['feature_name'] = X_train.columns\nrf_feat['importance'] = rfe_classifier.support_\nprint(rfe_classifier.ranking_)\nrf_feat","ef8ca9e1":"##Only columns found to have  importance to the random forest model via RFECV.\nX_train_reduced = X_train.filter(['Pregnancies', 'Glucose', 'BloodPressure', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\nX_test_reduced = X_test.filter(['Pregnancies', 'Glucose', 'BloodPressure', 'BMI', 'DiabetesPedigreeFunction', 'Age'])","3b1b0392":"##Building Random Forest Classification Model with Selected Variables\nmodel1_varimp = RandomForestClassifier(n_estimators=100, random_state=2021).fit(X_train_reduced, np.ravel(y_train))","47b68bf5":"##Cross-Validation Accuracy Score\nmodel1_cvs= cross_val_score(model1_varimp, X_train_reduced, np.ravel(y_train), cv=StratifiedKFold(3))\nmodel1_cvs.mean() #0.7622588872947552","ce2a0d48":"#Response Prediction\ny_pred = model1_varimp.predict(X_test_reduced)","720fe8bb":"#Creating classification report for random forest.\nprint(classification_report(y_test, y_pred))","121142f2":"#Creating confusion matrix for logistic regression model. True negatives (TN) are in the upper-left position, False Negatives (FN) are in the lower-left position, False Positives (FP) are in the upper-right position, True Positives (TP) are in the lower-right position.\nconfusion_matrix(y_test, y_pred)","354f7124":"#Determining AUC score for random forest model.\nroc_auc_score(y_test, y_pred) #0.7077777777777777","0fa4e4fd":"#Determining F1 score for the random forest model\nf1_score(y_test, y_pred,average='binary') #0.6122448979591836; a poor F1 score, is close to 0.0. Best F1 score is close to 1.","aa35fe77":"##Checking for multicollinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX_train_constant_vif = sm.add_constant(X_train_reduced) #For evaluating VIF only.\n\nvif= [variance_inflation_factor(X_train_constant_vif.values,i) for i in range(X_train_constant_vif.shape[1])]\n\npd.DataFrame({'vif': vif[1:]}, index=X_train_reduced.columns).T #Multicollinearity interpretted as high when VIF > 5. All found to be below 5 (no multicollinearity issues indicated)\n","68251d06":"##Generating values for feature importance plot.\nreduced_list_rf = list(['Pregnancies', 'Glucose', 'BloodPressure', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\n\n#Numerical Importance of Predictors\nrfr_importance_rf = list(model1_varimp.feature_importances_)\n\n#Merged and Sorted with Predictors of importance\nvar_importance_merge_rf = [(predictor,round(importance,2)) for predictor, importance in zip(reduced_list_rf,rfr_importance_rf)]\n\nvar_importance_merge_rf = sorted(var_importance_merge_rf, key = lambda x: x[1], reverse = True)\n\nprint(var_importance_merge_rf)","82be3e97":"##Plotting feature importance.\ndf_importance_rf = pd.DataFrame(var_importance_merge_rf, columns = ['PREDICTOR','IMPORTANCE_LEVEL'])\n\n#Predictor Rank Plot\nsns.catplot(x=\"IMPORTANCE_LEVEL\", y='PREDICTOR', data = df_importance_rf, kind = \"bar\", height =14)\n\n","a56b0eff":"##Hypertuning with GridSearchCV\nparam_grid = {\n    'max_depth':[6,9,12],\n    'min_samples_split':[5,10,15],\n    'n_estimators':[80,100,120]\n}\n\nrf_gscv = GridSearchCV(estimator = rf_classifier, param_grid = param_grid, cv=StratifiedKFold(3), n_jobs=-1, verbose = 2)","d154891a":"##Fitting GSCV with training data \nrf_gscv.fit(X_train_reduced, np.ravel(y_train))","f9bba33f":"##Extracting best params from GSCV\nrf_gscv.best_params_ #{'max_depth': 6, 'min_samples_split': 5, 'n_estimators': 80}","d69f8aef":"##Re-fitting a second random forest classification model with hypertuned parameters\nmodel_rf_final= RandomForestClassifier(n_estimators=80, max_depth=6, min_samples_split=5, random_state=2021).fit(X_train_reduced, np.ravel(y_train))","b30587a4":"##Cross-Validation Accuracy Score\nmodel_rf_final_cvs = cross_val_score(model_rf_final, X_train_reduced, np.ravel(y_train), cv=StratifiedKFold(3))\nmodel_rf_final_cvs.mean() #0.7720149848557308","d6c45806":"##Response Prediction\ny_pred_rf_final = model_rf_final.predict(X_test_reduced)","6dfa9714":"##Determining Test Accuracy Score\naccuracy_score(y_test, y_pred_rf_final)#0.7857142857142857","b1aa74ea":"##Creating classification report for random forest classification.\nprint(classification_report(y_test, y_pred_rf_final))","e7ee3260":"##Creating confusion matrix for random forest classification model. True negatives (TN) are in the upper-left position, False Negatives (FN) are in the lower-left position, False Positives (FP) are in the upper-right position, True Positives (TP) are in the lower-right position.\nconfusion_matrix(y_test, y_pred_rf_final)","01fba9c9":"##Determining AUC score for random forest classification model.\nroc_auc_score(y_test, y_pred_rf_final) #0.7370370370370369","ad760634":"#Determining F1 score for the for random forest classification model.\nf1_score(y_test, y_pred_rf_final,average='binary') #0.6526315789473683; a poor F1 score, is close to 0.0. Best F1 score is close to 1.","b373cdcd":"##Generating values for feature importance plot.\nreduced_list_rf_final = list(['Pregnancies', 'Glucose', 'BloodPressure', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\n\n#Numerical Importance of Predictors\nrfr_importance_rf_final = list(model_rf_final.feature_importances_)\n\n#Merged and Sorted with Predictors of importance\nvar_importance_merge_rf_final= [(predictor,round(importance,2)) for predictor, importance in zip(reduced_list_rf_final,rfr_importance_rf_final)]\n\nvar_importance_merge_rf_final = sorted(var_importance_merge_rf_final, key = lambda x: x[1], reverse = True)\n\nprint(var_importance_merge_rf_final)","353156b6":"##Plotting feature importance.\ndf_importance_rf_final = pd.DataFrame(var_importance_merge_rf_final, columns = ['PREDICTOR','IMPORTANCE_LEVEL'])\n\n#Predictor Rank Plot\nsns.catplot(x=\"IMPORTANCE_LEVEL\", y='PREDICTOR', data = df_importance_rf_final, kind = \"bar\", height =14)","918f183e":"##Importing Gradient Boosting Classifier##\nfrom sklearn.ensemble import GradientBoostingClassifier","6ad4874a":"#Building GB Classification Model for Sklearn Prediction\ngb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=2021).fit(X_train, np.ravel(y_train))\n\n##Identification of optimal number of features to select with RFECV approach. Selecting 3 folds in attempt to avoid overfitting.\nopt_gb_rfecv = RFECV(estimator = gb_classifier, step=1, cv=StratifiedKFold(3), scoring='balanced_accuracy', min_features_to_select=1)\n\nopt_gb_rfecv.fit(X_train, np.ravel(y_train))\nprint(\"Optimal number of features selected using RFECV: %d\"%opt_gb_rfecv.n_features_) #6 out of 8 selected as important.\n\n#Plot reference cited from: https:\/\/scikit-learn.org\/stable\/auto_examples\/feature_selection\/plot_rfe_with_cross_validation.html\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (# of correct classifications)\")\nplt.plot(range(1,\n               len(opt_gb_rfecv.grid_scores_) + 1),\n         opt_gb_rfecv.grid_scores_)\nplt.show() ","9d868dbf":"##Training the gradient boost classifier with optimal number of features already identified.\ngb_rfe_classifier = RFE(estimator=gb_classifier, n_features_to_select=6, step=1)\ngb_rfe_classifier.fit(X_train, np.ravel(y_train))","033f2d04":"#Determining features of highest importance for the gradient boost model.\ngb_feat = pd.DataFrame()\ngb_feat['feature_name'] = X_train.columns\ngb_feat['importance'] = gb_rfe_classifier.support_\nprint(gb_rfe_classifier.ranking_)\ngb_feat","021581d8":"##Only columns found to have  importance to the gradient boost model via RFECV.\nX_train_reduced_gb = X_train.filter(['Pregnancies', 'Glucose','Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\nX_test_reduced_gb = X_test.filter(['Pregnancies', 'Glucose','Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])","eaa47787":"##Building Gradient Boost Classification Model with Selected Variables\nmodel2_varimp = GradientBoostingClassifier(n_estimators=100, random_state=2021).fit(X_train_reduced_gb, np.ravel(y_train))","8ae5d2c3":"##Cross-Validation Accuracy Score\ngb_model_cvs = cross_val_score(model2_varimp, X_train_reduced_gb, np.ravel(y_train), cv=StratifiedKFold(3))\ngb_model_cvs.mean() #0.7704128805993943","7e7935c4":"##Response Prediction\ny_pred_gb = model2_varimp.predict(X_test_reduced_gb)","375a090d":"##Determining test accuracy score\naccuracy_score(y_test, y_pred_gb) #0.7857142857142857","4b9b5c68":"##Creating classification report for GB Classification Model \nprint(classification_report(y_test, y_pred_gb))","a8c55e78":"##Creating confusion matrix for GB Classification Model. True negatives (TN) are in the upper-left position, False Negatives (FN) are in the lower-left position, False Positives (FP) are in the upper-right position, True Positives (TP) are in the lower-right position.\nconfusion_matrix(y_test, y_pred_gb)","f5a3b911":"##Determining AUC score for the GB Classification Model.\nroc_auc_score(y_test, y_pred_gb) #0.7412962962962963","76306c3e":"##Determining F1 score for the GB Classification Model.\nf1_score(y_test, y_pred_gb,average='binary') #0.6597938144329897; a poor F1 score, is close to 0.0. Best F1 score is close to 1.","23ec97bb":"##Checking for multicollinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX_train_constant_vif = sm.add_constant(X_train_reduced_gb) #For evaluating VIF only.\n\nvif= [variance_inflation_factor(X_train_constant_vif.values,i) for i in range(X_train_constant_vif.shape[1])]\n\npd.DataFrame({'vif': vif[1:]}, index=X_train_reduced_gb.columns).T #Multicollinearity interpretted as high when VIF > 5. All found to be below 5 (no multicollinearity issues indicated)\n","00da1dc3":"##Generating values for feature importance plot.\nreduced_list_gb = list(['Pregnancies', 'Glucose','Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\n\n#Numerical Importance of Predictors\nimportance_gb = list(model2_varimp.feature_importances_)\n\n#Merged and Sorted with Predictors of importance\nvar_importance_merge_gb = [(predictor,round(importance,2)) for predictor, importance in zip(reduced_list_gb,importance_gb)]\n\nvar_importance_merge_gb = sorted(var_importance_merge_gb, key = lambda x: x[1], reverse = True)\n\nprint(var_importance_merge_gb)","8be9c911":"##Plotting feature importance.\ndf_importance_gb = pd.DataFrame(var_importance_merge_gb, columns = ['PREDICTOR','IMPORTANCE_LEVEL'])\n\n#Predictor Rank Plot\nsns.catplot(x=\"IMPORTANCE_LEVEL\", y='PREDICTOR', data = df_importance_gb, kind = \"bar\", height =14)\n","fe6bf85c":"##Hypertuning with GridSearchCV\nparam_grid = {\n    'max_depth':[6,9,12],\n    'min_samples_split':[5,10,15],\n    'n_estimators':[80,100,120]\n}\n\ngb_gscv = GridSearchCV(estimator = gb_classifier, param_grid = param_grid, cv=StratifiedKFold(3), n_jobs=-1, verbose = 2)","0a3b087f":"##Fitting GSCV with training data \ngb_gscv.fit(X_train_reduced_gb, np.ravel(y_train))","001b0b47":"##Extracting best params from GSCV\ngb_gscv.best_params_ #{'max_depth': 6, 'min_samples_split': 10, 'n_estimators': 120}","ce36abd9":"##Re-fitting a second gradient boosting classification model with hypertuned parameters\nmodel_gb_final= GradientBoostingClassifier(n_estimators=120, max_depth=6, min_samples_split=10, random_state=2021).fit(X_train_reduced_gb, np.ravel(y_train))","32b803a6":"##Cross-Validation Accuracy Score\nmodel_gb_final_cvs = cross_val_score(model_gb_final, X_train_reduced_gb, np.ravel(y_train), cv=StratifiedKFold(3))\nmodel_gb_final_cvs.mean() #0.7573489558424996","a6e1de49":"##Response Prediction\ny_pred_gb_final = model_gb_final.predict(X_test_reduced_gb)","454f8b73":"##Determining Test Score\naccuracy_score(y_test, y_pred_gb_final)#0.7922077922077922","7122bd69":"##Creating classification report for gradient boosting classification.\nprint(classification_report(y_test, y_pred_gb_final))","46243bdc":"##Creating confusion matrix for gradient boosting model. True negatives (TN) are in the upper-left position, False Negatives (FN) are in the lower-left position, False Positives (FP) are in the upper-right position, True Positives (TP) are in the lower-right position.\nconfusion_matrix(y_test, y_pred_gb_final)","5dbbee27":"##Determining AUC score for gradient boosting classification model.\nroc_auc_score(y_test, y_pred_gb_final) #0.7633333333333332","246a9352":"#Determining F1 score for the for random forest classification model.\nf1_score(y_test, y_pred_gb_final,average='binary') #0.6923076923076923; a poor F1 score, is close to 0.0. Best F1 score is close to 1.","38259212":"##Generating values for feature importance plot.\nreduced_list_gb_final = list(['Pregnancies', 'Glucose','Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\n\n#Numerical Importance of Predictors\nimportance_gb_final = list(model_gb_final.feature_importances_)\n\n#Merged and Sorted with Predictors of importance\nvar_importance_merge_gb_final = [(predictor,round(importance,2)) for predictor, importance in zip(reduced_list_gb_final,importance_gb_final)]\n\nvar_importance_merge_gb_final = sorted(var_importance_merge_gb_final, key = lambda x: x[1], reverse = True)\n\nprint(var_importance_merge_gb_final)","a447e836":"##Plotting feature importance.\ndf_importance_gb_final= pd.DataFrame(var_importance_merge_gb_final, columns = ['PREDICTOR','IMPORTANCE_LEVEL'])\n\n#Predictor Rank Plot\nsns.catplot(x=\"IMPORTANCE_LEVEL\", y='PREDICTOR', data = df_importance_gb_final, kind = \"bar\", height =14)\n","a177979c":"##Importing xgboost Classifier##\nimport xgboost as xgb\nfrom xgboost import XGBClassifier","9cb4d0a6":"#Building XGB Classification Model for Sklearn Prediction\nxgb_classifier = XGBClassifier(objective='binary:logistic',  eval_metric='logloss', use_label_encoder=False, n_estimators=100, random_state=2021).fit(X_train, np.ravel(y_train))\n\n##Identification of optimal number of features to select with RFECV approach. Selecting 3 folds in attempt to avoid overfitting.\nopt_xgb_rfecv = RFECV(estimator = xgb_classifier, step=1, cv=StratifiedKFold(3), scoring='balanced_accuracy', min_features_to_select=1)\n\nopt_xgb_rfecv.fit(X_train, np.ravel(y_train))\nprint(\"Optimal number of features selected using RFECV: %d\"%opt_xgb_rfecv.n_features_) #All 8 selected as important.\n\n#Plot reference cited from: https:\/\/scikit-learn.org\/stable\/auto_examples\/feature_selection\/plot_rfe_with_cross_validation.html\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (# of correct classifications)\")\nplt.plot(range(1,\n               len(opt_xgb_rfecv.grid_scores_) + 1),\n         opt_xgb_rfecv.grid_scores_)\nplt.show() ","ce6e5ffd":"##Training the gradient boost classifier with optimal number of features already identified.\nxgb_rfe_classifier = RFE(estimator=xgb_classifier, n_features_to_select=8, step=1)\nxgb_rfe_classifier.fit(X_train, np.ravel(y_train))","94f70d6a":"#Determining features of highest importance for the extreme gradient boost model.\nxgb_feat = pd.DataFrame()\nxgb_feat['feature_name'] = X_train.columns\nxgb_feat['importance'] = xgb_rfe_classifier.support_\nprint(xgb_rfe_classifier.ranking_)\nxgb_feat","36c97bdc":"##Building Extreme Gradient Boost Classification Model with all Variables\nmodel3_varimp = XGBClassifier(objective='binary:logistic',  eval_metric='logloss', use_label_encoder=False, n_estimators=100, random_state=2021).fit(X_train, np.ravel(y_train))","b84406e0":"##Cross-Validation Accuracy Score\nxgb_model_cvs = cross_val_score(model3_varimp, X_train, np.ravel(y_train), cv=StratifiedKFold(3))\nxgb_model_cvs.mean() #0.7736170891120676","7f7e7403":"##Response Prediction\ny_pred_xgb = model3_varimp.predict(X_test)","3de9d0d1":"##Determining test accuracy score\naccuracy_score(y_test, y_pred_xgb) #0.7987012987012987","f8f8bc4d":"#Creating classification report for XGB classification model.\nprint(classification_report(y_test, y_pred_xgb))","5b85b546":"#Creating confusion matrix for XGB classification model. True negatives (TN) are in the upper-left position, False Negatives (FN) are in the lower-left position, False Positives (FP) are in the upper-right position, True Positives (TP) are in the lower-right position.\nconfusion_matrix(y_test, y_pred_xgb)","88f1400f":"#Determining AUC score for the XGB classification model.\nroc_auc_score(y_test, y_pred_xgb) #0.764074074074074","05b5cec2":"#Determining F1 score for the XGB classification model\nf1_score(y_test, y_pred_xgb,average='binary') #0.6930693069306931; a poor F1 score, is close to 0.0. Best F1 score is close to 1.","e76039f6":"##Hypertuning parameters: max_depth, min_child_weight, eta.\n#Code referenced as guide for tuning procedure: https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\nparam_grid = {\n    'max_depth':[6,8,10],\n    'n_estimators':range(80,120,20),\n    'learning_rate': [0.10,0.15,0.20]\n}","c362ab89":"##Employing GridSearchCV to identify optimal parameters based on specified range.\nxgb_optmodel = xgb.XGBClassifier(random_state=2021)\noptimal_params = GridSearchCV(xgb_optmodel, param_grid, verbose=0,n_jobs=-1, cv=StratifiedKFold(3))","ce2afe3f":"##Determing optimal parameters\noptimal_params.fit(X_train,np.ravel(y_train))\nprint(optimal_params.best_params_) #{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 80}","61382637":"##Final Optimized Gradient Boosting Model Object\nxgb_final = xgb.XGBClassifier(objective='binary:logistic',  eval_metric='logloss', use_label_encoder=False, n_estimators=80, max_depth=10,learning_rate=0.1, n_jobs=-1, random_state=2021)","91d20c0d":"##Fitting xgb_final on training data and predicting on test data\nxgb_final.fit(X_train,np.ravel(y_train))","f69a1be4":"#Cross-Validation Accuracy Score\nxgb_cvs = cross_val_score(xgb_final, X_train, np.ravel(y_train), cv=StratifiedKFold(3))\nxgb_cvs.mean() #0.7524868483978957","b2b68e78":"##Response Prediction\nxgb_pred_final = xgb_final.predict(X_test)","b4dc4ef9":"##Determining accuracy scores\naccuracy_score(y_test, xgb_pred_final) #0.7597402597402597","0ec7ea13":"##Creating classification report for XGB classification.\nprint(classification_report(y_test, xgb_pred_final))","14b27ec9":"##Creating confusion matrix for XGB classification model. True negatives (TN) are in the upper-left position, False Negatives (FN) are in the lower-left position, False Positives (FP) are in the upper-right position, True Positives (TP) are in the lower-right position.\nconfusion_matrix(y_test, xgb_pred_final)","32739c4c":"##Determining AUC score for XGB classification model.\nroc_auc_score(y_test, xgb_pred_final) #0.7383333333333333","c0d52eb5":"#Determining F1 score for the for XGB classification model.\nf1_score(y_test, xgb_pred_final,average='binary') #0.6605504587155963; a poor F1 score, is close to 0.0. Best F1 score is close to 1.","5dd6c09c":"##Checking for multicollinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX_train_constant_vif = sm.add_constant(X_train) #For evaluating VIF only.\n\nvif= [variance_inflation_factor(X_train_constant_vif.values,i) for i in range(X_train_constant_vif.shape[1])]\n\npd.DataFrame({'vif': vif[1:]}, index=X_train.columns).T #Multicollinearity interpretted as high when VIF > 5. All found to be below 5 (no multicollinearity issues indicated)\n","46ca899c":"##Plotting features by ranked importance (Training Data) (Importance_type=Weight is default)\n\n#Converting dataframes to Dmatrix optimized structure\nxgb_trainmatrix = xgb.DMatrix(X_train, label = y_train)\nxgb_testmatrix = xgb.DMatrix(X_test, label = y_test)\n\nparams={'learning_rate': 0.1, 'max_depth': 10}\n\nxgb_finalimportance = xgb.train(params=params, dtrain=xgb_trainmatrix, num_boost_round=80)\n\nxgb.plot_importance(xgb_finalimportance, importance_type='gain')\n\nplt.show()","5e4e591c":"Step 3b:Gradient Boost Classification Modelling","74455293":"Step 1: Reviewing Training Dataset and Applying Cleaning if Necessary","30fb31cf":"-- Data Project Goal: Predict the onset of diabetes based on diagnostic measures --","b9fd1243":"3c. Extreme Gradient Boost Classification (Similar to Gradient Boosting but with higher computation power and more regularization to combat overfitting while aiming to reduce bias reduction)","bd03b00e":"Step 3: Performing Data Modelling. \n    \n    The following models will be attempted: Random Forest Classification, Gradient Boost Classification, Extreme Gradient Boost Classification.","f132d193":"**The final metrics obtained for this Extreme Gradient Boosting Classification Model were: 1. Final Test Score: 0.76; 2. Sensitivity: TP\/(TP+FN) = (81\/(81+18)) = 0.82; 3. Specificity: TN\/(TN+FP) = (36\/(36+19)) = 0.75; 4. AUC Score: 0.7; 5. F1 Score: 0.7**","700a723c":"Step 3a: Random Forest Classification Modelling","7fb4295d":"**The final metrics obtained for this Random Forest Classification Model were:\n    1. Final Test Score: 0.78;\n    2. Sensitivity: TP\/(TP+FN) = (90\/(90+23)) = 0.80;\n    3. Specificity: TN\/(TN+FP) = (31\/(31+10)) = 0.76;\n    4. AUC Score: 0.7;\n    5. F1 Score: 0.7;**","13c06f37":"Step 2. Performing Exploratory Data Analysis","fba3fd00":"**The final metrics obtained for this Gradient Boosting Classification Model were: 1. Final Test Score: 0.79; 2. Sensitivity: TP\/(TP+FN) = (86\/(86+18)) = 0.83; 3. Specificity: TN\/(TN+FP) = (36\/(36+14)) = 0.72; 4. AUC Score: 0.7; 5. F1 Score: 0.7**","296bf7a3":"**Ultimately, Gradient Boosting (not extreme) seems to have the strongest performance, in terms of sensitivity (ability to detect true positive occurences (onset of diabetes) ~0.80), whereas the random forest model had the strongest performance in terms of being able to determine a true negative occurence (when onset of diabetes would not occur) ~0.76). Gradient boosting also had the highest test accuracy score (0.79).**"}}