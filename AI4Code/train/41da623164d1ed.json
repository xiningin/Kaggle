{"cell_type":{"58461c02":"code","ff9c73a2":"code","248f4473":"code","f2f8c1c2":"code","2e0f8e7b":"code","df3532cd":"code","650f4624":"code","b287d908":"code","2faf793e":"code","912d35b7":"code","e4b79d22":"code","8814aa19":"code","1be6397f":"code","d30b36f7":"code","c3be19be":"code","51944e99":"code","f347054e":"code","3537eb62":"code","7b8b0eac":"code","2bbf279d":"code","34f9d92b":"code","5896c95e":"code","75446cb5":"code","6c768e7b":"code","134aa4cd":"code","8297a440":"code","e6f45f46":"code","bd41511f":"code","e9f6de90":"code","b1adc5dc":"code","b343ac5e":"code","a06589c9":"code","356eb099":"code","9f5e2cd9":"code","e98c05e8":"code","d7316b0e":"code","c6fd845d":"code","56f9ef07":"markdown","f5d48c7b":"markdown","5b558dc3":"markdown","91b3e300":"markdown","7873597f":"markdown","aa1e1649":"markdown","cf57a2f6":"markdown","d4260b87":"markdown","8e76b942":"markdown","97edf22d":"markdown","3458f0ba":"markdown","83b5dc63":"markdown","2e34f15e":"markdown","c8ac12c0":"markdown","db1bde98":"markdown","bc24d4f1":"markdown","f36e5815":"markdown","0caa87c4":"markdown","bfd64b53":"markdown","da90d781":"markdown","90e812f1":"markdown","faf8bc86":"markdown","14136ae5":"markdown","9921cd69":"markdown","f5330221":"markdown","f5d6ca0f":"markdown","921eabd9":"markdown","4d47f636":"markdown","a90456b1":"markdown"},"source":{"58461c02":"import numpy as np \nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom scipy.spatial import distance\nfrom tqdm.notebook import tqdm\n\nfrom kaggle_datasets import KaggleDatasets","ff9c73a2":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","248f4473":"PT1_GCS_DS_PATH = KaggleDatasets().get_gcs_path('google-landmarks-2020-tfrecords')\nprint(PT1_GCS_DS_PATH)\n\nPT2_GCS_DS_PATH = KaggleDatasets().get_gcs_path('google-landmarks-2020-triplet-loss-tfrecords-pt2')\nprint(PT2_GCS_DS_PATH)\n\nPT3_GCS_DS_PATH = KaggleDatasets().get_gcs_path('google-landmarks-2020-triplet-loss-tfrecords-pt3')\nprint(PT3_GCS_DS_PATH)\n\nPT4_GCS_DS_PATH = KaggleDatasets().get_gcs_path('google-landmarks-2020-triplet-loss-tfrecords-pt4')\nprint(PT4_GCS_DS_PATH)\n\nPT5_GCS_DS_PATH = KaggleDatasets().get_gcs_path('google-landmarks-2020-triplet-loss-tfrecords-pt5')\nprint(PT5_GCS_DS_PATH)\n\nPT6_GCS_DS_PATH = KaggleDatasets().get_gcs_path('google-landmarks-2020-triplet-loss-tfrecords-pt6')\nprint(PT6_GCS_DS_PATH)\n\nPT7_GCS_DS_PATH = KaggleDatasets().get_gcs_path('google-landmarks-2020-triplet-loss-tfrecords-pt7')\nprint(PT7_GCS_DS_PATH)\n\nPT8_GCS_DS_PATH = KaggleDatasets().get_gcs_path('google-landmarks-2020-triplet-loss-tfrecords-pt8')\nprint(PT8_GCS_DS_PATH)\n\nPT9_GCS_DS_PATH = KaggleDatasets().get_gcs_path('google-landmarks-2020-tripley-loss-tfrecords-pt9')\nprint(PT9_GCS_DS_PATH)","f2f8c1c2":"BATCH_SIZE = 64 * strategy.num_replicas_in_sync\nEPOCHS = 20\nSTEPS_PER_EPOCH = 1451645 \/\/ BATCH_SIZE\nRATE = 0.0001\n\nIMAGE_SIZE = 128\nEMBED_SIZE = 2048","2e0f8e7b":"filenames = tf.io.gfile.glob([\n    PT1_GCS_DS_PATH + '\/*.tfrec', \n    PT2_GCS_DS_PATH + '\/*.tfrec', \n    PT3_GCS_DS_PATH + '\/*.tfrec',\n    PT4_GCS_DS_PATH + '\/*.tfrec', \n    PT5_GCS_DS_PATH + '\/*.tfrec', \n    PT6_GCS_DS_PATH + '\/*.tfrec',\n    PT7_GCS_DS_PATH + '\/*.tfrec', \n    PT8_GCS_DS_PATH + '\/*.tfrec',\n    PT9_GCS_DS_PATH + '\/*.tfrec',\n])","df3532cd":"train_data = tf.data.TFRecordDataset(\n    filenames,\n    num_parallel_reads = tf.data.experimental.AUTOTUNE\n)","650f4624":"ignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False \ntrain_data = train_data.with_options(ignore_order)","b287d908":"def get_triplet(example):\n    tfrec_format = {\n        \"anchor_img\": tf.io.FixedLenFeature([], tf.string),\n        \"positive_img\": tf.io.FixedLenFeature([], tf.string),\n        \"negative_img\": tf.io.FixedLenFeature([], tf.string),\n    }\n    \n    example = tf.io.parse_single_example(example, tfrec_format)\n        \n    x = {\n        'anchor_input': decode_image(example['anchor_img']),\n        'positive_input': decode_image(example['positive_img']),\n        'negative_input': decode_image(example['negative_img']),\n    }\n    \n    return x, [0, 0, 0]\n\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.\n    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE), method='nearest')\n    \n    image = augment(image)\n    \n    return image\n\n\ndef augment(image):\n    rand_aug = np.random.choice([0, 1, 2, 3])\n    \n    if rand_aug == 0:\n        image = tf.image.random_brightness(image, max_delta=0.4)\n    elif rand_aug == 1:\n        image = tf.image.random_contrast(image, lower=0.2, upper=0.5)\n    elif rand_aug == 2:\n        image = tf.image.random_hue(image, max_delta=0.2)\n    else:\n        image = tf.image.random_saturation(image, lower=0.2, upper=0.5)\n    \n    rand_aug = np.random.choice([0, 1, 2, 3])\n    \n    if rand_aug == 0:\n        image = tf.image.random_flip_left_right(image)\n    elif rand_aug == 1:\n        image = tf.image.random_flip_up_down(image)\n    elif rand_aug == 2:\n        rand_rot = np.random.randn() * 45\n        image = tfa.image.rotate(image, rand_rot)\n    else:\n        image = tfa.image.transform(image, [1.0, 1.0, -50, 0.0, 1.0, 0.0, 0.0, 0.0])\n\n    image = tf.image.random_crop(image, size=[100, 100, 3])\n    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n    \n    return image","2faf793e":"train_data = train_data.map(\n    get_triplet, \n    num_parallel_calls=tf.data.experimental.AUTOTUNE\n)","912d35b7":"train_data = train_data.repeat()\ntrain_data = train_data.shuffle(1024)\ntrain_data = train_data.batch(BATCH_SIZE)\ntrain_data = train_data.prefetch(tf.data.experimental.AUTOTUNE)","e4b79d22":"fig, axes = plt.subplots(5, 3, figsize=(15, 15))\n\nfor images, landmark_id in train_data.take(1):\n    anchors = images['anchor_input']\n    positives = images['positive_input']\n    negatives = images['negative_input']\n    \n    for i in range(5):\n        axes[i, 0].set_title('Anchor')\n        axes[i, 0].imshow(anchors[i])\n\n        axes[i, 1].set_title('Positive')\n        axes[i, 1].imshow(positives[i])\n\n        axes[i, 2].set_title('Negative')\n        axes[i, 2].imshow(negatives[i])","8814aa19":"class GeMPoolingLayer(tf.keras.layers.Layer):\n    def __init__(self, p=1., eps=1e-6):\n        super().__init__()\n        self.p = p\n        self.eps = eps\n\n    def call(self, inputs: tf.Tensor, **kwargs):\n        inputs = tf.clip_by_value(\n            inputs, \n            clip_value_min=self.eps, \n            clip_value_max=tf.reduce_max(inputs)\n        )\n        inputs = tf.pow(inputs, self.p)\n        inputs = tf.reduce_mean(inputs, axis=[1, 2], keepdims=False)\n        inputs = tf.pow(inputs, 1. \/ self.p)\n        \n        return inputs\n    \n    def get_config(self):\n        return {\n            'p': self.p,\n            'eps': self.eps\n        }","1be6397f":"reg = tf.keras.regularizers\n\nwith strategy.scope():\n    # backbone\n    backbone = tf.keras.applications.Xception(\n        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n        weights='imagenet',\n        include_top=False\n    )\n    \n    backbone.trainable = False\n    \n    # embedding model\n    x_input = tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n    x = backbone(x_input)\n    x = GeMPoolingLayer()(x)\n    x = tf.keras.layers.Dense(EMBED_SIZE, activation='softplus', kernel_regularizer=reg.l2(), dtype='float32')(x)\n\n    embedding_model = tf.keras.models.Model(inputs=x_input, outputs=x, name=\"embedding\")\n\n    # anchor encoding\n    anchor_input = tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name='anchor_input')\n    anchor_x = embedding_model(anchor_input)\n\n    # positive encoding\n    positive_input = tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name='positive_input')\n    positive_x = embedding_model(positive_input)\n\n    # anchor encoding\n    negative_input = tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name='negative_input')\n    negative_x = embedding_model(negative_input)\n\n    # construct model\n    model = tf.keras.models.Model(\n        inputs=[anchor_input, positive_input, negative_input], \n        outputs=[anchor_x, positive_x, negative_x]\n    )","d30b36f7":"embedding_model.summary()","c3be19be":"def triplet_loss(y_true, y_pred, alpha=0.2):     \n    anchors = y_pred[0]\n    positives = y_pred[1]\n    negatives = y_pred[2]\n    \n    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchors, positives)), axis=-1)\n    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchors, negatives)), axis=-1)\n\n    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n    loss = tf.reduce_sum(tf.maximum(basic_loss, 0))\n\n    return loss","51944e99":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=RATE),\n    loss = triplet_loss\n)","f347054e":"callbacks = [\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, verbose=1),\n    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1, restore_best_weights=True),\n]","3537eb62":"history = model.fit_generator(\n    train_data,\n    epochs = EPOCHS,\n    steps_per_epoch = STEPS_PER_EPOCH,\n    callbacks = callbacks,\n)","7b8b0eac":"plt.title('Model loss')\nplt.plot(history.history['loss'])","2bbf279d":"def distance_test(anchors, positives, negatives):    \n    pos_dist = []\n    neg_dist = []\n    \n    anchor_encodings = embedding_model.predict(anchors)\n    positive_encodings = embedding_model.predict(positives)\n    negative_encodings = embedding_model.predict(negatives)\n    \n    for i in range(len(anchors)):\n        pos_dist.append(\n            distance.euclidean(anchor_encodings[i], positive_encodings[i])\n        )\n        \n        neg_dist.append(\n            distance.euclidean(anchor_encodings[i], negative_encodings[i])\n        )\n    \n    return pos_dist, neg_dist","34f9d92b":"pos_dist, neg_dist = distance_test(anchors[0:5], positives[0:5], negatives[0:5])","5896c95e":"fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n\nfor i in range(5):\n    axes[i, 0].set_title('Anchor')\n    axes[i, 0].imshow(anchors[i])\n\n    axes[i, 1].set_title('Positive dist: {:.2f}'.format(pos_dist[i]))\n    axes[i, 1].imshow(positives[i])\n\n    axes[i, 2].set_title('Negative dist: {:.2f}'.format(neg_dist[i]))\n    axes[i, 2].imshow(negatives[i])","75446cb5":"image_ids = pd.read_csv(\n    '..\/input\/landmark-retrieval-2020\/train.csv',\n    nrows=100\n)","6c768e7b":"def get_image(img_id):    \n    chars = [char for char in img_id]\n    dir_1, dir_2, dir_3 = chars[0], chars[1], chars[2]\n    \n    image = Image.open('..\/input\/landmark-retrieval-2020\/train\/' + dir_1 + '\/' + dir_2 + '\/' + dir_3 + '\/' + img_id + '.jpg')\n    image = image.resize((IMAGE_SIZE, IMAGE_SIZE))\n    image = np.asarray(image) \/ 255.0\n    \n    return image","134aa4cd":"images = [get_image(img_id) for img_id in image_ids.id]\nimages = np.array(images)","8297a440":"embeddings = embedding_model.predict(images)","e6f45f46":"distances = distance.cdist(embeddings, embeddings, 'euclidean')\nprint(distances.shape)","bd41511f":"predicted_positions = np.argpartition(distances, 10, axis=1)[:, :10]","e9f6de90":"anchor_img = get_image(\n    image_ids.loc[41].id\n)\n\nplt.title('Landmark: {}'.format(image_ids.loc[40].landmark_id))\nplt.imshow(anchor_img)","b1adc5dc":"fig, ax = plt.subplots(3, 3, figsize=(12, 12))\n\ntop_ten_indexes = predicted_positions[41]\n\nfor i in range(3):\n    for j in range(3):\n        img_index = top_ten_indexes[j + (i*3)]\n        \n        landmark_id = image_ids.loc[img_index].landmark_id\n        dist = distances[41, img_index]\n\n        ax[i,j].set_title('landmark: {}, dist: {:.2f}'.format(landmark_id, dist))\n        ax[i,j].imshow(\n            get_image(image_ids.loc[img_index].id)\n        )","b343ac5e":"anchor_img = get_image(\n    image_ids.loc[80].id\n)\n\nplt.imshow(anchor_img)","a06589c9":"fig, ax = plt.subplots(3, 3, figsize=(12, 12))\n\ntop_ten_indexes = predicted_positions[80]\n\nfor i in range(3):\n    for j in range(3):\n        img_index = top_ten_indexes[j + (i*3)]\n        \n        landmark_id = image_ids.loc[img_index].landmark_id\n        dist = distances[80, img_index]\n\n        ax[i,j].set_title('landmark: {}, dist: {:.2f}'.format(landmark_id, dist))\n        ax[i,j].imshow(\n            get_image(image_ids.loc[img_index].id)\n        )","356eb099":"embedding_model.save(\n    'embedding_model.h5', \n    save_format='h5',\n    overwrite=True\n)","9f5e2cd9":"# embedding_model = tf.keras.models.load_model(\n#     'embedding_model.h5',\n#     custom_objects={'GeMPoolingLayer': GeMPoolingLayer}\n# )","e98c05e8":"# class MyModel(tf.keras.Model):\n#     def __init__(self):\n#         super(MyModel, self).__init__()\n#         self.model = embedding_model\n    \n#     @tf.function(input_signature=[\n#       tf.TensorSpec(shape=[None, None, 3], dtype=tf.uint8, name='input_image')\n#     ])\n    \n#     def call(self, input_image):\n#         output_tensors = {}\n        \n#         input_image = tf.cast(input_image, tf.float32) \/ 255.0\n#         input_image = tf.image.resize(input_image, (IMAGE_SIZE, IMAGE_SIZE)) \n                \n#         extracted_features = self.model(tf.convert_to_tensor([input_image], dtype=tf.float32))[0]\n#         output_tensors['global_descriptor'] = tf.identity(extracted_features, name='global_descriptor')\n#         return output_tensors","d7316b0e":"# m = MyModel()\n\n# served_function = m.call\n\n# tf.saved_model.save(\n#     m, \n#     export_dir=\".\/model\", \n#     signatures={'serving_default': served_function}\n# )","c6fd845d":"# from zipfile import ZipFile\n\n# with ZipFile('submission.zip','w') as output_zip_file:\n#     for filename in os.listdir('.\/model'):\n#         if os.path.isfile('.\/model\/'+filename):\n#             output_zip_file.write('.\/model\/'+filename, arcname=filename) \n    \n#     for filename in os.listdir('.\/model\/variables'):\n#         if os.path.isfile('.\/model\/variables\/'+filename):\n#             output_zip_file.write('.\/model\/variables\/'+filename, arcname='variables\/'+filename)\n    \n#     for filename in os.listdir('.\/model\/assets'):\n#         if os.path.isfile('.\/model\/assets\/'+filename):\n#             output_zip_file.write('.\/model\/assets\/'+filename, arcname='assets\/'+filename)","56f9ef07":"And then get the ten images closest to it including the euclidean distance. There are only four images containing this landmark so there will be some other landmarks that appear in the top ten. If the model is good though the distance for these other landmarks should be lower than the distance for the images containing the same landmark.","f5d48c7b":"Display each triplet set alongside the distance between the anchor and positive images and the anchor and the negative images. The distances are in the positive and negative image titles.","5b558dc3":"## Save model\n\nFinally save the model. The competition needs the model to accept only one image as input and output only one embedding. I'll only be saving the embedding layers of the model then.\n\nIt's worth noting that at the time of writing models trained with TPUs cannot be saved in the tf format. They instead need to be saved as h5 files.","91b3e300":"Now get the indexes of the ten images that are closest to each image in the sample.","7873597f":"Next define a function to load the three images in a triplet example. These functions get an example from a tfrecord file and decode the three images inside it. I've also added some random augmentations to the images so to add further variety to the dataset.","aa1e1649":"Compile the model with the loss function and the adam optimizer.","cf57a2f6":"The TPUs in Google Cloud (the ones Kaggle are using in the background) require the data to be close to the location of the TPU. Thus a Google Cloud bucket needs setting up to put the data in. Kaggle provides a handy library to get the path to the nearby bucket for us. \n\nTo use tfrecords I needed to create a number of public datasets including the records. The paths to the GCS buckets containing them are found here.","d4260b87":"TPUs work a little faster if a batch of data is not necessarily inserted in the order the batch started in. Setting the experimental deterministic option to false gives us a little bit of extra speed.","8e76b942":"Let's have a look at a small part of a batch of triplet sets.","97edf22d":"Feed the images into the model to get the embeddings for each one.","3458f0ba":"Now define the triplet loss function. This function splits the model output into its three parts and then compares the anchor embeddings to the positive and negative embeddings. A reduce sum has also been added at the end as the batch size is likely to push more than one triplet set through on a single step.","83b5dc63":"Then I'll begin the pipeline by telling it to expect tfrecords.","2e34f15e":"## Pipeline\n\nTPUs and tensorflow work best when the data is fed into the model with a tf dataset pipeline. For those new to the API, tf data is a preset series of functions that converts the dataset into tensors (tensorflows favoured data type), transforms it (e.g. resize the images) and applies a bunch of other useful functions for training such as shuffling and batching.\n\nI'll start by getting a list of the tfrecord filenames.","c8ac12c0":"I would also finish this off with the below functions to save the model into the exact format needed for a submission. However the competition does not allow notebook using TPU or the internet to be entered for submission. As such I have moved the below functions into an inference notebook where the h5 file can be loaded and inserted into the below.","db1bde98":"## Load and pre-process data\n\nTo get the most out of the TPU I have converted a sample of the dataset into tfrecords and published it in this [dataset](https:\/\/www.kaggle.com\/mattbast\/google-landmarks-2020-tfrecords). If you're interested in how I created this dataset and what it looks like you can find the notebook I used to create it [here](https:\/\/www.kaggle.com\/mattbast\/google-landmarks-2020-create-a-tfrecord-dataset\/notebook).","bc24d4f1":"## Model\n\nNow the model can be defined. To enable triplet loss the model has three inputs (one each for the anchor, positive and negative images) and three outputs (again one each for the anchor, positive and negative embeddings). The three inputs use the same middle embedding layers to ensure they are embedded the same way. This embedding centre is made up of a pre-trained backbone (that does not get trained), one GEM pooling layer (generalised mean pooling which you can find out more about in this paper [link](https:\/\/arxiv.org\/pdf\/1711.02512.pdf)), one dense layer to create the embeddings and a normalisation layer to help the embeddings generalise. I've kept this model small as training will already take a long time thanks to the size of the dataset.\n\n![Screenshot%202020-07-31%20at%2009.35.49.png](attachment:Screenshot%202020-07-31%20at%2009.35.49.png)\n\nNotice that the strategy variable has made an appearance again. This replicates whatever model is defined for each of the TPU chips and works out how to merge the model again once the training ends.","f36e5815":"## Hyper-parameters\n\nWith the TPU setup I'll define some hyper-parameters as global variables. Sticking them up at the top of the notebook makes them quick to adjust when experimenting with the model.\n\nYou'll notice that the batch size has been multiplied by the strategy.num_replicas_in_sync. It's worth mentioning here that when using a TPU the notebook will effectively replicate the model for as many TPU chips there are available (at the time of writing, Kaggle make eight available). The batch size thus needs multiplying by the number of TPU chips available.\n\nThis all means that we effectively get eight models training at the same time which is one of the reasons why TPUs are so fast. The strategy that was set is a set of instructions that manage how the model is replicated, distributed across the eight chips and then merged back into one model at the end.","0caa87c4":"Let's have a look at an image containing a different landmark.","bfd64b53":"Finally train the model.","da90d781":"Since I completed the Convolutional Neural Networks course on [Coursera](https:\/\/www.coursera.org\/learn\/convolutional-neural-networks?) I've been looking for a challenge where I can practice a model with triplet loss. While the size and complexity of this dataset may not be ideal I thought this would be a good opportunity to do so.","90e812f1":"Now we can have a look at how an example anchor image from this sample compares against all the other images in the dataset. Let's pick an image in the sample.","faf8bc86":"## Evaluate\n\nTo evaluate the model, let's begin with a simple loss curve.","14136ae5":"Next we can eyeball some results. Here I'll use the five triplet sets that I displayed earlier in this notebook. I'll pass them through the model and calculate the euclidean distance between the anchor and the positive and negative embeddings.","9921cd69":"Add a couple of callbacks to catch the model if successive epochs are not converging.","f5330221":"As there are many large images to work with this challenge will need a bit more processing power than usual to get through even a fraction of the training dataset. The below piece of code was taken from the tutorials in the [Petal to the Metal getting started competition](https:\/\/www.kaggle.com\/c\/tpu-getting-started). It requests a TPU for the notebook. If there isn't one (if the notebook was not configured to use TPU) it will get a CPU or GPU instead.","f5d6ca0f":"This competition uses the embeddings created by the model to cluster an image with index images. To see how this model performs it might be worth taking one example image, getting its embeddings and seeing what images it is most similar to.\n\nBegin by getting a sample of 100 images.","921eabd9":"Finally add some helper functions. \n\n* Shuffle helps to prevent overfit \n* Batching ensures the right amount of data gets put into the model per step\n* Repeat enables us to train for more than one epoch\n* Prefetch gets the next batch of data while the model is training on the previous batch","4d47f636":"Next get the pixels for each of the images in the sample and put them in a numpy array.","a90456b1":"Now generate a matrix comparing each image against the others. This gives us a euclidean distance between all the images. The smaller this distance the more similar the model believes the images to be. If we have 100 images in the sample then we will have a matrix of shape (100,100) that contains all the distances."}}