{"cell_type":{"14e495c1":"code","e319fdc0":"code","d0804c8a":"code","aa5a5d02":"code","414bc304":"code","3f4ba0cd":"code","3e8b710a":"code","9a607002":"code","4c53ab63":"code","756134d2":"code","6f625e0b":"code","e1089960":"code","b08cf955":"code","174fb8df":"code","498a8786":"code","0b3a0321":"code","627ca092":"code","d5f2fb46":"markdown","13e18308":"markdown","521ab6c4":"markdown","bc833c95":"markdown","e941ea13":"markdown","722500a0":"markdown","4c950b8c":"markdown","ca95b932":"markdown","cd5474a6":"markdown","0495a62b":"markdown","4728b707":"markdown","619c0772":"markdown","05eedbf0":"markdown","8d9598f9":"markdown","10b03cf7":"markdown","ae33bd19":"markdown","56dede84":"markdown","13fa5538":"markdown","db35799d":"markdown","d9875055":"markdown","8d6e0734":"markdown","9974c908":"markdown","0bfec335":"markdown","fc8c85ca":"markdown","629d5280":"markdown","50101b23":"markdown","5ffd7887":"markdown","e6c683b4":"markdown","3d8891c5":"markdown","a92363ae":"markdown","c2c47faf":"markdown","37597b24":"markdown","5656fd94":"markdown","2435de40":"markdown","5bc4fb48":"markdown","c6c1ad7e":"markdown"},"source":{"14e495c1":"import warnings\nwarnings.filterwarnings('ignore')","e319fdc0":"import pandas as pd\nimport numpy as np\n\n# Load the dataset\ndata=pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\n\ndata['category'] = data['quality'] >= 7\n\nX = data[data.columns[0:11]].values\nY = data['category'].values.astype(np.int)\ndata.head()","d0804c8a":"# let's standardize our data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","aa5a5d02":"from sklearn.model_selection import train_test_split\n\n# Let's split our data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=42)\n\nprint('X train size: ', x_train.shape)\nprint('y train size: ', y_train.shape)\nprint('X test size: ', x_test.shape)\nprint('y test size: ', y_test.shape)","414bc304":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\n\n\ndtc= DecisionTreeClassifier(random_state=0)\nsvc= SVC(random_state=0)\nlr= LogisticRegression(random_state=0)","3f4ba0cd":"for clf in (dtc, lr, svc):\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    print(f'{clf.__class__.__name__ } Accuracy Score  -  {accuracy_score(y_test, y_pred)}')\n    print(f'{clf.__class__.__name__ } Precision Score  -  {precision_score(y_test,y_pred)}')\n    print()","3e8b710a":"# Now, let's use the ensemble method\nvoting_clf = VotingClassifier(estimators=[('lr', lr), ('dtc', dtc), ('svc', svc)],voting='hard')\nvoting_clf.fit(x_train, y_train)\ny_pred = voting_clf.predict(x_test)\nprint(f'{voting_clf.__class__.__name__ } Accuracy Score  -  {accuracy_score(y_test, y_pred)}')\nprint(f'{voting_clf.__class__.__name__ } Precision Score  -  {precision_score(y_test,y_pred)}')","9a607002":"# Splliting our data\nx_train, x_test, y_train, y_test = train_test_split(X , Y, test_size=0.2,random_state=42)\n\nprint('X train size: ', x_train.shape)\nprint('y train size: ', y_train.shape)\nprint('X test size: ', x_test.shape)\nprint('y test size: ', y_test.shape)","4c53ab63":"# First, training a single predictor\nlr=LogisticRegression(random_state=0, solver='lbfgs')\nlr.fit(x_train, y_train)\ny_pred = lr.predict(x_test)\nprint(f'{lr.__class__.__name__ } Accuracy Score  -  {accuracy_score(y_test, y_pred)}')\nprint(f'{lr.__class__.__name__ } Precision Score  -  {precision_score(y_test,y_pred)}')","756134d2":"# Now, let's have an ensemble of 500  logistic regression classifiers with the bagging method\nfrom sklearn.ensemble import BaggingClassifier\n\nbag_clf = BaggingClassifier(LogisticRegression(random_state=0, solver='lbfgs'), \n                            n_estimators = 500,\n                            oob_score = True, # We will see what it is in a bit\n                            random_state = 90) \nbag_clf.fit(x_train, y_train)\ny_pred = bag_clf.predict(x_test)\nprint(f'{bag_clf.__class__.__name__ } OOB Score  -  {bag_clf.oob_score_ }')","6f625e0b":"bag_clf = BaggingClassifier(LogisticRegression(random_state=0, solver='lbfgs'), n_estimators = 500, \n                           bootstrap_features = True, max_features = 1.0, oob_score = True, random_state = 90)\nbag_clf.fit(x_train, y_train)\nprint(f'{bag_clf.__class__.__name__ } OOB Score  -  {bag_clf.oob_score_ }')\n","e1089960":"# First, training a single predictor\ndtc=DecisionTreeClassifier(random_state=0)\ndtc.fit(x_train, y_train)\ny_pred = dtc.predict(x_test)\nprint(f'{dtc.__class__.__name__ } Accuracy Score  -  {accuracy_score(y_test, y_pred)}')\nprint(f'{dtc.__class__.__name__ } Precision Score  -  {precision_score(y_test,y_pred)}')","b08cf955":"# Let's now use AdaBoost\nfrom sklearn.ensemble import AdaBoostClassifier\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(random_state=0), n_estimators=200,algorithm=\"SAMME.R\", learning_rate=0.5)\nada_clf.fit(x_train, y_train)\ny_pred = ada_clf.predict(x_test)\nprint(f'{ada_clf.__class__.__name__ } Accuracy Score  -  {accuracy_score(y_test, y_pred)}')\nprint(f'{ada_clf.__class__.__name__ } Precision Score  -  {precision_score(y_test,y_pred)}')","174fb8df":"# First, training a single predictor\ndtc=DecisionTreeClassifier(random_state=0)\ndtc.fit(x_train, y_train)\ny_pred = dtc.predict(x_test)\nprint(f'{dtc.__class__.__name__ } Accuracy Score  -  {accuracy_score(y_test, y_pred)}')\nprint(f'{dtc.__class__.__name__ } Precision Score  -  {precision_score(y_test,y_pred)}')","498a8786":"# Let's now use Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbrt = GradientBoostingClassifier(max_depth=2, n_estimators=200, learning_rate=1.0)\ngbrt.fit(x_train, y_train)\ny_pred = gbrt.predict(x_test)\nprint(f'{gbrt.__class__.__name__ } Accuracy Score  -  {accuracy_score(y_test, y_pred)}')\nprint(f'{gbrt.__class__.__name__ } Precision Score  -  {precision_score(y_test,y_pred)}')","0b3a0321":"import xgboost\nxgb_reg = xgboost.XGBClassifier(eval_metric='mlogloss')\nxgb_reg.fit(x_train, y_train)\ny_pred = xgb_reg.predict(x_test)\nprint(f'{xgb_reg.__class__.__name__ } Accuracy Score  -  {accuracy_score(y_test, y_pred)}')\nprint(f'{xgb_reg.__class__.__name__ } Precision Score  -  {precision_score(y_test,y_pred)}')","627ca092":"from sklearn.ensemble import StackingClassifier\nestimators = [\n            ('rf', RandomForestClassifier(n_estimators=10, random_state=0)),\n            ('svr',SVC(random_state=0))\n            ]\nclf = StackingClassifier(\n     estimators=estimators, final_estimator=LogisticRegression()\n        )\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\nprint(f'{clf.__class__.__name__ } Accuracy Score  -  {accuracy_score(y_test, y_pred)}')\nprint(f'{clf.__class__.__name__ } Precision Score  -  {precision_score(y_test,y_pred)}')","d5f2fb46":"------------------------------------------------------------------------------------------------------------------","13e18308":"------------------------------------------------------------------------------------------------------------------","521ab6c4":"### **2. Sampling the training features**\nInstead of sampling the instances we can also sample the features. Sampling is\ncontrolled by two hyperparameters: **max_features** and **bootstrap_features**. They\nwork the same way as **max_samples** and **bootstrap** (in Scikit Learn), but for feature sampling instead of\ninstance sampling. Thus, each predictor will be trained on a random subset of the input\nfeatures. Sampling features results in even more predictor diversity, trading a bit more bias for a\nlower variance.\n\nSampling both training instances and features is called the **Random\nPatches** method. Keeping all training instances but sampling features is called the **Random Subspaces\nmethod**.","bc833c95":"As we can see, it's far better than any other ensemble technique.\n\n**Bear with me, we are just left with 1 more ensumble technique.**","e941ea13":"### **3. Boosting**\nBoosting refers to any Ensemble method that can\ncombine several weak learners into a strong learner. The general idea of most boosting\nmethods is to train predictors sequentially, each trying to correct its predecessor. There\nare many boosting methods available, but by far the most popular are **AdaBoost** (short\nfor Adaptive Boosting) and **Gradient Boosting**.\n### AdaBoost\nOne way for a new predictor to correct its predecessor is to pay a bit more attention to\nthe training instances that the predecessor underfitted. This results in new predictors\nfocusing more and more on the hard cases. This is the technique used by AdaBoost.\n\nFor example, when training an AdaBoost classifier, the algorithm first trains a base\nclassifier (such as a Decision Tree) and uses it to make predictions on the training set.\nThe algorithm then increases the relative weight of misclassified training instances.\nThen it trains a second classifier, using the updated weights, and again makes\npredictions on the training set, updates the instance weights, and so on.\n<div>\n<img src=\"https:\/\/i.imgur.com\/zUWqFz2.jpg\" width=\"800\" align=\"center\"\/>\n<\/div>\n<!-- ![image.png](attachment:51f7e233-2503-4c43-baf4-ae9c4da4b257.png) -->\n\n\n","722500a0":"------------------------------------------------------------------------------------------------------------------","4c950b8c":"------------------------------------------------------------------------------------------------------------------","ca95b932":"### **4. Stacking**\nIt is based on a simple idea: instead of using trivial functions\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble, why\ndon\u2019t we train a model to perform this aggregation? \n\nWe can have such an\nensemble performing a regression task on a new instance. Each of the\npredictors predicts a different value, and then the final predictor\n(called a **blender**, or a **meta learner**) takes these predictions as inputs and makes the\nfinal prediction.\n<div>\n<img src=\"https:\/\/i.imgur.com\/2vp8IP0.png\" width=\"500\" align=\"center\"\/>\n<\/div>\n","cd5474a6":"------------------------------------------------------------------------------------------------------------------","0495a62b":"This sequential learning technique has some\nsimilarities with Gradient Descent, except that instead of tweaking a single predictor\u2019s\nparameters to minimize a cost function, AdaBoost adds predictors to the ensemble,\ngradually making it better.\n\nYou can see how AdaBoost Algo works [here](https:\/\/machinelearningmastery.com\/boosting-and-adaboost-for-machine-learning\/)\n\n**Let's see how it works.**","4728b707":"AdaBoost Improved the performance\n\nIf the predictors can estimate\nclass probabilities (i.e., if they have a predict_proba() method) like DecisionTreeClassifier here, Scikit-Learn can use\na variant of SAMME called SAMME.R (the R stands for \u201cReal\u201d), which relies on class\nprobabilities rather than predictions and generally performs better","619c0772":"------------------------------------------------------------------------------------------------------------------","05eedbf0":"As expected, bagging performed better. Just one more thing to look on\n#### Out-Of-Bag Evaluation\nWith bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. By default a BaggingClassifier samples m training instances with replacement (bootstrap=True), where m is the size of the training set. This means that only about 63% of the training instances are sampled on average for each predictor. The remaining 37% of the training instances that are not sampled are called out-of-bag (oob) instances. Note that they are not the same 37% for all predictors.\n\nSince a predictor never sees the oob instances during training, it can be evaluated on\nthese instances, without the need for a separate validation set. You can evaluate the\nensemble itself by averaging out the oob evaluations of each predictor.","8d9598f9":"------------------------------------------------------------------------------------------------------------------\n","10b03cf7":"------------------------------------------------------------------------------------------------------------------","ae33bd19":"The following code creates and trains a voting classifier in Scikit-Learn, composed of\nthree diverse classifiers.","56dede84":"------------------------------------------------------------------------------------------------------------------","13fa5538":"------------------------------------------------------------------------------------------------------------------","db35799d":"The GradientBoostingClassifier class also supports a subsample hyperparameter,\nwhich specifies the fraction of training instances to be used for training each tree. For\nexample, if subsample=0.25, then each tree is trained on 25% of the training instances,\nselected randomly. As you can probably guess by now, this technique trades a higher\nbias for a lower variance. It also speeds up training considerably. This is called\n**Stochastic Gradient Boosting**.","d9875055":"# That's all folks! \n### You now have the basic idea about the different Ensemble Techniques. Make sure to apply them on your own and read some articles for in-depth Understanding.\n\n","8d6e0734":"------------------------------------------------------------------------------------------------------------------","9974c908":"Even if the xlassifier is a weak learner(meaning it does only slightly better than random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of\nweak learners and they are sufficiently diverse.\n\nNow, we discussed the hard-voting classifier, so is there any soft-voting classifier? Yes, If all classifiers are able to estimate class probabilities, then you can predict the class with the highest class probability, averaged over all the individual classifiers.\n<div>\n<img src=\"https:\/\/i.imgur.com\/ik8B4un.png\" width=\"500\" align=\"center\"\/>\n<\/div>\n\nSometimes, you could see no improvement in accuracy or even decrement in accuracy. But, if you will look at the net result, the model generally has similar bias but lower variance.\n#### Ok, enough theory. Let's see it in action","0bfec335":"## Ensemble learning using same predictors\nNow, a obvious question that appears is why always have different classifiers, why not have same classifiers and use ensemble method. We could definetely do that but we need to make sure that we have that diversity that we had in previous section with diffent classifiers. Let's discuss the multiple ways to achieve it.\n\n","fc8c85ca":"------------------------------------------------------------------------------------------------------------------","629d5280":"The wine quality is binarized into either \"good\" ( y=1 , quality>=7) or \"not good\" ( y=0 , quality<7). The input  X  consists of 11 features such as fixed acidity and pH. We will then split the data set into a trianing set and a test set:","50101b23":"Here, the OOB score went down, but still it's preforming equally. We can do hyperparameter tuning to imporve the performance furthur.","5ffd7887":"## Introduction\nWe all know different types of classifiers\/regressors like - Logistic Regression, SVM, Decision Trees etc. They are preety good at predicting and classification tasks but let's consider a scenario. Suppose, you pose a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble; thus, this technique is called Ensemble Learning.\n\n## Voting Classifiers \/ Ensemble learning using different predictors\nSuppose, you have a trained multiple classifiers, each having a good accuracy, say 70%. A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier.\n<div>\n<img src=\"https:\/\/i.imgur.com\/vnV4NF5.png\" width=\"500\" align=\"center\"\/>\n<\/div>","e6c683b4":"To train the blender, a common approach is to use a hold-out set. Let\u2019s see how it\nworks. \n\nFirst, the training set is split into two subsets. The first subset is used to train the\npredictors in the first layer.\n\nNext, the first layer\u2019s predictors are used to make predictions on the second (held-out)\nset. We can create a new training set using these predicted values\nas input features, and keeping the target values.\nThe blender is trained on this new training set, so it learns to predict the target value,\ngiven the first layer\u2019s predictions.\n\n### **Let's implement the stacking**","3d8891c5":"### Extreme Gradient Boosting (XGBoost)\nIt is an optimized implementation of **Gradient Boosting** based on Decision-Tree ensemble and stands for Extreme Gradient Boosting. It aims to be extremely fast, scalable, and\nportable. In fact, XGBoost is often an important component of the winning entries in\nML competitions. To know how it perfrom better go [here](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)","a92363ae":"### Let's train our models separatly","c2c47faf":"### Gradient Boosting\nJust like **AdaBoost**,\n**Gradient Boosting** works by sequentially adding predictors to an ensemble, each one\ncorrecting its predecessor. However, instead of tweaking the instance weights at every\niteration like AdaBoost does, this method tries to fit the new predictor to the residual\nerrors made by the previous predictor.\n<div>\n<img src=\"https:\/\/i.imgur.com\/cXXmSQB.png\" width=\"500\" align=\"center\"\/>\n<\/div>\n","37597b24":"------------------------------------------------------------------------------------------------------------------","5656fd94":"------------------------------------------------------------------------------------------------------------------","2435de40":"We can clearly see that Ensemble Model worked better with improved Accuracy and Precision Score.","5bc4fb48":"![Imgur](https:\/\/i.imgur.com\/N39ixv7.png)","c6c1ad7e":"### **1. Sampling the training instances**\n### Bagging\n\nThe idea is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed **with replacement**, this method is called **Bagging** (short for bootstrap aggregating). \nHere, **with replacement** means that training instaces can be sampled multiple times for the same predictor as well across the other predictors. \n<div>\n<img src=\"https:\/\/i.imgur.com\/aJDLWJy.png\" width=\"800\" align=\"center\"\/>\n<\/div>\n\n### Pasting\nPasting refers to the method of randomly sampling training instances without replacement. This means that, in a certain subsample, the same instance can only appear at most once within a predictor.\n\n#### Let's see bagging in action -"}}