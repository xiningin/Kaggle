{"cell_type":{"a6d70ba6":"code","dc8d66c6":"code","95ddcfc2":"code","4a115fcf":"code","03e0332f":"code","af194b81":"code","1d56bf5f":"code","6664203a":"code","2c00a5b7":"code","dd97d8b7":"code","0780ec75":"code","686c2c1a":"code","3619f527":"code","626163e1":"code","3c98a887":"code","0dc7dde6":"code","df74bdc6":"code","710c1482":"code","231f4b0b":"code","e5b31bea":"code","5c5c718f":"code","51b85243":"code","62f68424":"code","4e664fcf":"code","35f937ce":"code","f6b10f56":"code","06b95246":"code","f9e3f8c5":"code","55021c1b":"code","8c00dce7":"code","5cf8afa8":"code","10b27c02":"code","181d1fec":"code","fe50b249":"code","a2519f45":"code","84db7df8":"code","592ee9c2":"code","288baf3a":"code","4c192d4d":"code","498ac3a4":"code","b1a38267":"code","ae3a0fc5":"code","b931c8b4":"code","2bf28198":"code","43638f11":"code","734ab00c":"code","76220b00":"code","ab79d5ca":"code","2578978f":"code","586d1763":"code","d75b05be":"code","fc304e84":"markdown","16fdfe0b":"markdown","61df3849":"markdown","3582e676":"markdown","282b3e86":"markdown","98bad664":"markdown","11a9e8e1":"markdown","92472eb0":"markdown","920daeb9":"markdown","8767b336":"markdown","de469b5e":"markdown","22ca44e9":"markdown","0553ce3c":"markdown","31d2c8a4":"markdown","9ee5c91d":"markdown","ef49e98e":"markdown","192f2684":"markdown","569fd575":"markdown","9385b314":"markdown","75060fa5":"markdown","82bdbbe5":"markdown","675bd822":"markdown","f99c3b90":"markdown","1a4b76fb":"markdown","643a603a":"markdown","f364d4cb":"markdown","ff64636d":"markdown","1c716a6c":"markdown","74da57ba":"markdown","599c7a52":"markdown","9795c2c9":"markdown","3f36ce78":"markdown","1dacff9c":"markdown","1425a56d":"markdown","13ab71d2":"markdown","cc80da62":"markdown","9835aa2c":"markdown","5d96af67":"markdown"},"source":{"a6d70ba6":"# Artificial Neural Network","dc8d66c6":"import matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nplt.style.use('dark_background')\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.metrics import plot_confusion_matrix\nfrom scipy.stats import norm, boxcox\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom collections import Counter\nfrom scipy import stats\nimport tensorflow as tf\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)","95ddcfc2":"dataset = pd.read_csv('..\/input\/churn-modelling\/Churn_Modelling.csv')","4a115fcf":"dataset.head()","03e0332f":"dataset.shape","af194b81":"dataset.describe().T.style.bar(\n    subset=['mean'],\n    color='#606ff2').background_gradient(\n    subset=['std'], cmap='PuBu').background_gradient(subset=['50%'], cmap='PuBu')","1d56bf5f":"dataset.isnull().values.any()","6664203a":"plt.figure(figsize=(12, 6))\nsns.countplot(x=\"Exited\", data=dataset, palette='husl');","2c00a5b7":"def boxPlotter(columnName):\n        sns.catplot(x=\"Exited\", y=columnName, data=dataset, kind=\"box\");","dd97d8b7":"cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary']\nfor column in cols:\n    boxPlotter(column)","0780ec75":"plt.figure(figsize=(20, 17))\nmatrix = np.triu(dataset.corr())\nsns.heatmap(dataset.corr(), annot=True,linewidth=.8, mask=matrix, cmap=\"rocket\");","686c2c1a":"plot_data =  dataset.drop(['RowNumber', 'CustomerId', 'Surname','Gender','NumOfProducts', 'HasCrCard','Geography',\n       'IsActiveMember', ], axis=1)","3619f527":"def pieChartPlotter(dataset, columnName):\n    values = dataset[columnName].value_counts()\n    labels = dataset[columnName].unique()\n    pie, ax = plt.subplots(figsize=[10, 6])\n\n    patches, texts, autotexts = ax.pie(values, labels=labels, autopct='%1.2f%%', shadow=True, pctdistance=.5,# explode=[0.06]*dataset['Exited'].unique()\n                                       )\n\n    plt.legend(patches, labels, loc=\"best\")\n    plt.title(columnName, color='white', fontsize=14)\n    plt.setp(texts, color='white', fontsize=20)\n    plt.setp(autotexts, size=10, color='black')\n    autotexts[1].set_color('black')\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.show()","626163e1":"pieChartPlotter(dataset, 'Exited') \npieChartPlotter(dataset, 'Gender')\npieChartPlotter(dataset, 'Geography')","3c98a887":"def groupBarPlotter(dataset):\n    \"\"\"\n    Group Plots columns with Exited column\n    \"\"\"\n    fig = plt.figure(figsize=(20, 30))\n    for i in range(len(dataset.columns)):\n        if not dataset.columns[i] == 'Exited':\n            groups = dataset.groupby(dataset.columns[i])['Exited'].mean()\n            fig.add_subplot(np.ceil(len(dataset.columns)\/2), 2, i+1)\n            plt.xlabel('price')\n            groups.plot.barh()\n            fig.tight_layout(pad=3.0)","0dc7dde6":"groupBarPlotter(dataset.loc[:, ['Tenure', 'Age','Exited']])","df74bdc6":"def distributionPlot(dataset):\n    fig = plt.figure(figsize=(20, 20))\n    for i in range(len(dataset.columns)):\n        fig.add_subplot(np.ceil(len(dataset.columns)\/5), 3, i+1)\n        sns.distplot(\n            dataset.iloc[:, i], color=\"lightcoral\", rug=True)\n        fig.tight_layout(pad=3.0)","710c1482":"distributionPlot(plot_data)","231f4b0b":"sns.pairplot(plot_data, hue=\"Exited\", palette=\"husl\");","e5b31bea":"def skewnessCorrector(dataset,columnName):\n    import seaborn as sns\n    from scipy import stats\n    from scipy.stats import norm, boxcox\n    \"\"\"\n    This function returns two plots distplot and probability plot for non-normalized data and after normalizing the provided data. \n    Just provide it with two parameters dataset and the name of column.\n    It corrects the skewness of data applying Boxcox transformation on the provided data\n    \"\"\"\n    print('''Before Correcting''')\n    (mu, sigma) = norm.fit(dataset[columnName])\n    print(\"Mu before correcting {} : {}, Sigma before correcting {} : {}\".format(\n        columnName.capitalize(), mu, columnName.capitalize(), sigma))\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"lightcoral\");\n    plt.title(columnName.capitalize() +\n              \" Distplot before Skewness Correction\", color=\"black\")\n    plt.subplot(1, 2, 2)\n    stats.probplot(dataset[columnName], plot=plt)\n    plt.show()\n    # Applying BoxCox Transformation\n    dataset[columnName], lam_fixed_acidity = boxcox(\n        dataset[columnName])\n    \n    print('''After Correcting''')\n    (mu, sigma) = norm.fit(dataset[columnName])\n    print(\"Mu after correcting {} : {}, Sigma after correcting {} : {}\".format(\n        columnName.capitalize(), mu, columnName.capitalize(), sigma))\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"orange\");\n    plt.title(columnName.capitalize() +\n              \" Distplot After Skewness Correction\", color=\"black\")\n    plt.subplot(1, 2, 2)\n    stats.probplot(dataset[columnName], plot=plt)\n    plt.show()\n","5c5c718f":"skewed_columns = ['CreditScore', 'Age', 'EstimatedSalary']\nfor column in skewed_columns:\n    skewnessCorrector(dataset,column)","51b85243":"X = dataset.iloc[:, 3:-1].values\ny = dataset.iloc[:, -1].values","62f68424":"print(X)","4e664fcf":"print(y)","35f937ce":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX[:, 2] = le.fit_transform(X[:, 2])","f6b10f56":"print(X)","06b95246":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))","f9e3f8c5":"print(X)","55021c1b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= .2, random_state= 0)","8c00dce7":"print(X_train)","5cf8afa8":"print(X_test)","10b27c02":"print(y_test)","181d1fec":"print(y_train)","fe50b249":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","a2519f45":"print(X_train)","84db7df8":"print(X_test)","592ee9c2":"ann = tf.keras.models.Sequential()\n","288baf3a":"ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu'))","4c192d4d":"ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu'))","498ac3a4":"ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu'))","b1a38267":"ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu'))","ae3a0fc5":"ann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))","b931c8b4":"ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'],)","2bf28198":"ann_trained = ann.fit(X_train, y_train,validation_split = 0.20, validation_data = (X_test, y_test), epochs = 100)","43638f11":"print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > .5)","734ab00c":"y_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","76220b00":"cm = confusion_matrix(y_test, y_pred)\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  \nax.set_xlabel('Predicted labels');\nax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['0', '1']); \nax.yaxis.set_ticklabels(['0', '1']);\n","ab79d5ca":"print(classification_report(y_test, y_pred,\n          target_names=['0', '1'], zero_division=1))","2578978f":"print('Accuracy of Model is')\naccuracy_score(y_test, y_pred)","586d1763":"def annPlotter(model, plotType):\n    plt.figure(figsize = (12, 6))\n    if plotType == 'loss':\n        param_1 = 'loss'\n        param_2 = 'val_loss'\n    elif plotType == 'accuracy':\n        param_1 = 'accuracy'\n        param_2 = 'val_accuracy'\n    train = model.history[param_1]\n    val = model.history[param_2] \n    epoch = range(1, 101)\n    sns.lineplot(epoch, train, label = 'Training '+plotType.capitalize())\n    sns.lineplot(epoch, val, label = 'Validation '+plotType.capitalize())\n    plt.title('Training and Validation '+plotType.capitalize())\n    plt.xlabel('Epochs')\n    plt.ylabel(plotType.capitalize())\n    plt.legend()\n    plt.show()","d75b05be":"annPlotter(ann_trained, 'loss')\nannPlotter(ann_trained, 'accuracy')","fc304e84":"### Importing the libraries","16fdfe0b":"## From above correlation plot we can say that there is no multicolinearity present in data","61df3849":"### From Above plot\n1. We can see that 79.63% of customers have churned out \n1. There are 54.57% Males and 45.43% females\n1. About 50.14% customers are from France, 25.09% from Spain and 24.77% from germany","3582e676":"### Adding the output layer","282b3e86":"## Making the predictions and evaluating the model","98bad664":"# Exploratory Data Analysis","11a9e8e1":"# Importing the dataset","92472eb0":"## Encoding categorical data","920daeb9":"## Some BoxPlots","8767b336":"Therefore, our ANN model predicts that this customer leaves the bank!\n\n**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array.\n\n**Important note 2:** Notice also that the \"France\" country was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, \"France\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns.","de469b5e":"## Thanks for Reading\n---\n## Please share your feedback by commenting below and if you like my work please consider upvoting.","22ca44e9":"## Plotting Count for Qualities","0553ce3c":"### From above plots we can say there's skewness in the data","31d2c8a4":"# Data Preprocessing","9ee5c91d":"### Predicting the Test set results","ef49e98e":"## Skewness Correction","192f2684":"# Building the ANN","569fd575":"## Feature Scaling","9385b314":"## Plotting Confusion Matrix","75060fa5":"### Compiling the ANN","82bdbbe5":"### Adding the input layer and the first hidden layer","675bd822":"### Adding hidden layers","f99c3b90":"### One Hot Encoding the \"Geography\" column","1a4b76fb":"## Group Plots","643a603a":"## Training the ANN","f364d4cb":"### Training the ANN on the Training set","ff64636d":"### Label Encoding the \"Gender\" column","1c716a6c":"From above Plots we can conclude that:-\n1. There is no significant difference in credit score distribution between customers which are churned or not.\n1. The older customers are churning more than younger ones.\n1. Bank is loosing customers with significant bank balance.\n1. Estimated Salary does not have a significant on the likelihood to churn.\nInterestingly, majority of customers that churned are those with credit cards but this can be a coincidence as majority of customers have credit cards.\nUnsurprisingly the inactive members have a greater churn and the overall proportion of inactive members is also very high.","74da57ba":"## Classification Report","599c7a52":"### Splitting the dataset into the Training set and Test set","9795c2c9":"## Correlation Plot","3f36ce78":"## Pairplots","1dacff9c":"### Initializing the ANN","1425a56d":"## Some PieCharts","13ab71d2":"## Distribution Plot","cc80da62":"### Our Ann's Accuracy is 85.95%","9835aa2c":"# Visualising Our ANN's Loss and Accuracy","5d96af67":"### Predicting the result of a single observation"}}