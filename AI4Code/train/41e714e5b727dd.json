{"cell_type":{"ea174ac4":"code","c528eb70":"code","d738d44f":"code","6585167c":"code","7360a2fe":"code","f3075a3b":"code","0eff1e98":"code","ec6033e3":"code","0590c737":"code","5e32cb02":"code","c6df4d5c":"code","fddb0506":"code","720756de":"code","b7407e29":"code","e30a526a":"code","51203b84":"code","dd04bbf9":"code","50bce368":"code","374588ad":"code","1e17594b":"code","99edadd3":"code","176ea224":"code","0724be5e":"code","ae11da20":"code","2dbf2db1":"code","3b67c5f6":"code","9cee31e0":"code","3021e130":"code","157a891e":"code","151ce314":"code","bc9be676":"code","7495a6f6":"code","3102b13d":"code","2515a0b5":"code","937c442e":"code","db10385b":"code","30e1c07b":"code","bc163acd":"code","d428a530":"code","9bef5278":"code","7a8ca079":"code","786f5c38":"code","ef266efa":"code","0f6d599f":"code","ac6c2b67":"code","b3860e05":"code","575324f7":"markdown","61c3f27e":"markdown","1a9b5aaa":"markdown","806bc44c":"markdown","c1bd5b4d":"markdown","f87411f4":"markdown","35f555a8":"markdown","d89b18cb":"markdown","327d1aa2":"markdown","ea15aca5":"markdown","8d3e46f2":"markdown","f454d3a8":"markdown"},"source":{"ea174ac4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c528eb70":"dataset = pd.read_csv('..\/input\/airpressure\/Folds5x2_pp.csv')","d738d44f":"dataset.info()","6585167c":"dataset.head()","7360a2fe":"dataset.describe()","f3075a3b":"# correlation map\nf,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(dataset.corr(), annot=True, linewidths=.5, fmt='.2f', ax=ax)\nplt.show()","0eff1e98":"X1 = dataset.iloc[:, 0:1].values # We choose AT since correlation between AT and PE is -0.95\ny1 = dataset.iloc[:, -1].values # PE","ec6033e3":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.2, random_state = 0)","0590c737":"# Training the Simple Linear Regression model on the Training set\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X1_train, y1_train)","5e32cb02":"# Predicting the Test set results\ny1_pred = regressor.predict(X1_test) \nnp.set_printoptions(precision=2)  # to print 2 decimals\nprint(np.concatenate(\n    (y1_pred.reshape(len(y1_pred),1), y1_test.reshape(len(y1_test),1)),axis=1)\n) # so we can compare prediction and test values","c6df4d5c":"# Evaluating the Model Performance\nfrom sklearn.metrics import r2_score\nlr_score = r2_score(y1_test, y1_pred)\nprint(lr_score)\nscores = {}\nscores.update({'lr_score': lr_score})","fddb0506":"# Visualising the Test set results\nplt.scatter(X1_test, y1_test, color = 'red')  \nplt.plot(X1_test, regressor.predict(X1_test), color = 'blue')   \nplt.title('AT vs PE')\nplt.xlabel('AT')\nplt.ylabel('PE')\nplt.show()","720756de":"X2 = dataset.iloc[:, :-1].values\ny2 = dataset.iloc[:, -1].values","b7407e29":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = 0.2, random_state = 0)","e30a526a":"# Training the Multiple Linear Regression model on the Training set\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X2_train, y2_train)","51203b84":"# Predicting the Test set results\ny2_pred = regressor.predict(X2_test)\nnp.set_printoptions(precision=2) # we specify the number of decimals\nprint(np.concatenate(\n    (y2_pred.reshape(len(y2_pred),1), y2_test.reshape(len(y2_test),1)), axis=1)\n    )   ","dd04bbf9":"# Evaluating the Model Performance\nmlr_score = r2_score(y2_test, y2_pred)\nprint(mlr_score)\nscores.update({'mlr_score': mlr_score})","50bce368":"# Final Equation\nb0 = regressor.intercept_\ncoefs = regressor.coef_\nprint (b0, coefs)\nprint (\"PE = {0:.2f} + {1:.2f}*AT + {2:.2f}*V + {3:.2f}*AP + {4:.2f}*RH\".\n       format(b0, coefs[0], coefs[1], coefs[2], coefs[3]))","374588ad":"X3 = dataset.iloc[:, :-1].values\ny3 = dataset.iloc[:, -1].values","1e17594b":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size = 0.2, random_state = 0)","99edadd3":"#Training the Polynomial Regression model on the Training set\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\npoly_reg = PolynomialFeatures(degree = 2)\nX3_poly = poly_reg.fit_transform(X3_train)\nregressor = LinearRegression()\nregressor.fit(X3_poly, y3_train)","176ea224":"# Predicting the Test set results\ny3_pred = regressor.predict(poly_reg.transform(X3_test))\nnp.set_printoptions(precision=2)\nprint(np.concatenate\n      ((y3_pred.reshape(len(y3_pred),1), y3_test.reshape(len(y3_test),1)),axis=1)\n      )","0724be5e":"# Evaluating the Model Performance\npr_score = r2_score(y3_test, y3_pred)\nprint(pr_score)\nscores.update({'pr_score': pr_score})","ae11da20":"X4 = dataset.iloc[:, :-1].values\ny4 = dataset.iloc[:, -1].values\ny4 = y4.reshape(len(y4),1) # StandartScaler class expects 2D Array as input","2dbf2db1":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size = 0.2, random_state = 0)","3b67c5f6":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_y = StandardScaler()    \nX4_train = sc_X.fit_transform(X4_train)\ny4_train = sc_y.fit_transform(y4_train)","9cee31e0":"# Training the SVR model on the Training set\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')   # Gaussian Radial Basis Function\nregressor.fit(X4_train, y4_train)","3021e130":"# Predicting the Test set results\ny4_pred = sc_y.inverse_transform(regressor.predict(sc_X.transform(X4_test)))\nnp.set_printoptions(precision=2)\nprint(np.concatenate\n      ((y4_pred.reshape(len(y4_pred),1), y4_test.reshape(len(y4_test),1)),\n       axis=1))","157a891e":"# Evaluating the Model Performance\nsvr_score = r2_score(y4_test, y4_pred)\nprint(svr_score)\nscores.update({'svr_score': svr_score})","151ce314":"X5 = dataset.iloc[:, :-1].values\ny5 = dataset.iloc[:, -1].values","bc9be676":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX5_train, X5_test, y5_train, y5_test = train_test_split(X5, y5, test_size = 0.2, random_state = 0)","7495a6f6":"# Training the Decision Tree Regression model on the Training set\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(X5_train, y5_train)","3102b13d":"# Predicting the Test set results\ny5_pred = regressor.predict(X5_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate\n      ((y5_pred.reshape(len(y5_pred),1), y5_test.reshape(len(y5_test),1)),\n       axis=1))","2515a0b5":"# Evaluating the Model Performance\ndtr_score = r2_score(y5_test, y5_pred)\nprint(dtr_score)\nscores.update({'dtr_score': dtr_score})","937c442e":"X6 = dataset.iloc[:, :-1].values\ny6 = dataset.iloc[:, -1].values","db10385b":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX6_train, X6_test, y6_train, y6_test = train_test_split(X6, y6, test_size = 0.2, random_state = 0)","30e1c07b":"# Training the Random Forest Regression model on the whole dataset\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10, random_state = 0)    \nregressor.fit(X6_train, y6_train)","bc163acd":"# Predicting the Test set results\ny6_pred = regressor.predict(X6_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate\n      ((y6_pred.reshape(len(y6_pred),1), y6_test.reshape(len(y6_test),1)),\n       axis=1))","d428a530":"# Evaluating the Model Performance\nrfr_score = r2_score(y6_test, y6_pred)\nprint(rfr_score)\nscores.update({'rfr_score': rfr_score})","9bef5278":"X7 = dataset.iloc[:, :-1].values\ny7 = dataset.iloc[:, -1].values","7a8ca079":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX7_train, X7_test, y7_train, y7_test = train_test_split(X7, y7, test_size = 0.2, random_state = 0)","786f5c38":"# libraries for ann\nimport tensorflow.keras\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\n\nregressor = Sequential() # initializing ann\nregressor.add(Dense(units = 6, activation = 'relu')) # Adding the the first hidden layer (input dimension will be automatically recognized)\nregressor.add(Dense(units = 1)) # Adding the output layer (no activation is needed in regression)\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error') # Compiling the ANN","ef266efa":"regressor.fit(X7_train, y7_train, batch_size = 32, epochs = 100)","0f6d599f":"# Predicting the Test set results\ny7_pred = regressor.predict(X7_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate\n      ((y7_pred.reshape(len(y7_pred),1), y7_test.reshape(len(y7_test),1)),\n       axis=1))","ac6c2b67":"# Evaluating the Model Performance\nann_score = r2_score(y7_test, y7_pred)\nprint(ann_score)\nscores.update({'ann_score': ann_score})","b3860e05":"print(scores)","575324f7":"In this notebook , performances of different regression models are compared using a combined cycle power plant (CCPP) dataset.\n\nThe dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. Features consist of hourly average ambient variables Temperature (AT), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the net hourly electrical energy output (PE) of the plant.\nCCPP is composed of gas turbines (GT), steam turbines (ST) and heat recovery steam generators. In a CCPP, the electricity is generated by gas and steam turbines, which are combined in one cycle, and is transferred from one turbine to another. While the Vacuum is collected from and has effect on the Steam Turbine, the other three of the ambient variables effect the GT performance.The averages are taken from various sensors located around the plant that record the ambient variables every second. The variables are given without normalization.","61c3f27e":"**8. Conclusion**","1a9b5aaa":"**Explatory Data Analysis**","806bc44c":"**7.Artificial Neural Network (ANN)**","c1bd5b4d":"As you can see above, Random Forest Regression Model has the highest r2_score (0.96) which means this model gives the best prediction for net hourly electrical energy output (PE) of the plant.","f87411f4":"**3. Polynomial Regression**","35f555a8":"**1. Linear Regression**","d89b18cb":"**2. Multiple Linear Regression**","327d1aa2":"**Overview**","ea15aca5":"**5. Decision Tree Regression**","8d3e46f2":"**4. Support Vector Regression (SVR)**","f454d3a8":"**6. Random Forest Regression**"}}