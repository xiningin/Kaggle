{"cell_type":{"173db1a9":"code","b33768dc":"code","f73e312b":"code","3c430a55":"code","3a016eab":"code","c74ec76d":"code","95541480":"code","6d9b0517":"code","1cb9863f":"code","a797e3a5":"code","36de832c":"code","0a396fae":"code","7285a61d":"code","cc6e87ec":"code","db497e7a":"code","e4fece53":"code","d164a6ad":"code","80d622b2":"code","139047f9":"code","c025ba55":"code","d3a626df":"code","c7948d26":"code","e980b165":"code","90b020e5":"code","030a8aa3":"markdown","b0ae3ff9":"markdown","78a34bc2":"markdown","4eec95b9":"markdown","28c7183d":"markdown","cde18f14":"markdown","fd40133a":"markdown","a25016ca":"markdown","98a07da0":"markdown","e79ebecb":"markdown","000fdc31":"markdown","5941b08f":"markdown","6e224080":"markdown","28c3583a":"markdown","f79a6d3e":"markdown","d200a14c":"markdown","98c4f231":"markdown","0ecf99c5":"markdown","24664317":"markdown"},"source":{"173db1a9":"import math\nimport numpy as np\nimport pandas as pd\nimport logging\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB,  BernoulliNB\nfrom sklearn.metrics import accuracy_score, log_loss,jaccard_similarity_score\nfrom sklearn.preprocessing import normalize\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\n#xboost and heamy\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom heamy.dataset import Dataset\nfrom heamy.estimator import Classifier\nfrom heamy.pipeline import ModelsPipeline\n#Keras modules\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.utils import to_categorical\nfrom keras.datasets import mnist\nfrom keras.utils.vis_utils import model_to_dot\nfrom IPython.display import SVG\nfrom keras.utils import np_utils","b33768dc":"DATA_DIR = \"..\/input\"\nSUBMISSION_FILE = \"{0}\/sample_submission.csv\".format(DATA_DIR)\nTRAIN_FILE = \"{0}\/train.csv\".format(DATA_DIR)\nTEST_FILE = \"{0}\/test.csv\".format(DATA_DIR)\nid_test = pd.read_csv('..\/input\/test.csv').Id","f73e312b":"def plot_distplots(train_path, colum_num_from=1, colum_num_to=11):\n    train = pd.read_csv(train_path)\n    _ = plt.figure(figsize=(20, 20))\n    i = 0\n    for feature in train.columns[colum_num_from:colum_num_to]:\n        i += 1\n        plt.subplot(5, 5, i)\n        sns.distplot(train[train.Cover_Type == 1][feature], hist=False, label='1')\n        sns.distplot(train[train.Cover_Type == 2][feature], hist=False, label='2')\n        sns.distplot(train[train.Cover_Type == 3][feature], hist=False, label='3')\n        sns.distplot(train[train.Cover_Type == 4][feature], hist=False, label='4')\n        sns.distplot(train[train.Cover_Type == 5][feature], hist=False, label='5')\n        sns.distplot(train[train.Cover_Type == 6][feature], hist=False, label='6')\n        sns.distplot(train[train.Cover_Type == 7][feature], hist=False, label='7')","3c430a55":"plot_distplots(TRAIN_FILE)","3a016eab":"CACHE = False\nNFOLDS = 5\nSEED = 1337\n\n#cross_entropy loss\n#-(yt log(yp) + (1 - yt) log(1 - yp))\nMETRIC = log_loss\n\nID = 'Id'\nTARGET = 'Cover_Type'\n\n#set precision to 5 decimal places\nnp.set_printoptions(precision=5)\nnp.set_printoptions(suppress=True)\n\n#seed value is set\nnp.random.seed(SEED)\nlogging.basicConfig(level=logging.WARNING)","c74ec76d":"def add_feats(df):\n    #Hydrology - Fire Points\n    df['HF1'] = (df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points'])\n    df['HF2'] = (df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n    \n    #Hydrology - Roadways\n    df['HR1'] = (df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n    df['HR2'] = (df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n    \n    #Firepoints - Roadways\n    df['FR1'] = (df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n    df['FR2'] = (df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n    \n    #Elevation & Vertical_Distance_To_Hydrology\n    df['EV1'] = (df['Elevation'] + df['Vertical_Distance_To_Hydrology'])\n    df['EV2'] = (df['Elevation'] - df['Vertical_Distance_To_Hydrology'])\n    \n    #Mean of created features\n    df['Mean_HF1'] = df.HF1 \/ 2\n    df['Mean_HF2'] = df.HF2 \/ 2\n    df['Mean_HR1'] = df.HR1 \/ 2\n    df['Mean_HR2'] = df.HR2 \/ 2\n    df['Mean_FR1'] = df.FR1 \/ 2\n    df['Mean_FR2'] = df.FR2 \/ 2\n    df['Mean_EV1'] = df.EV1 \/ 2\n    df['Mean_EV2'] = df.EV2 \/ 2    \n    \n    #Oblique Distance\n    df['Elevation_Vertical'] = df['Elevation'] + df['Vertical_Distance_To_Hydrology']    \n    df['Neg_Elevation_Vertical'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n    \n    #Given the horizontal & vertical distance to hydrology, \n    #it will be more intuitive to obtain the euclidean distance: sqrt{(verticaldistance)^2 + (horizontaldistance)^2}    \n    df['slope_hyd_sqrt'] = (df['Horizontal_Distance_To_Hydrology']**2+df['Vertical_Distance_To_Hydrology']**2)**0.5\n    \n    #remove infinite value if any\n    df['slope_hyd_sqrt'] = df.slope_hyd_sqrt.map(lambda x: 0 if np.isinf(x) else x)\n    \n    df['slope_hyd2'] = np.sqrt(df['Horizontal_Distance_To_Hydrology']**2+df['Vertical_Distance_To_Hydrology']**2)\n    df['slope_hyd2'] = df.slope_hyd2.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n    \n    #Mean distance to Amenities \n    df['Mean_Amenities'] = (df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology + df.Horizontal_Distance_To_Roadways) \/ 3 \n    \n    #Mean Distance to Fire and Water \n    df['Mean_Fire_Hyd1'] = (df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology) \/ 2\n    df['Mean_Fire_Hyd2'] = (df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Roadways) \/ 2\n    \n    #Shadiness\n    df['Shadiness_morn_noon'] = df.Hillshade_9am \/ (df.Hillshade_Noon+1)\n    df['Shadiness_noon_3pm'] = df.Hillshade_Noon \/ (df.Hillshade_3pm+1)\n    df['Shadiness_morn_3'] = df.Hillshade_9am \/ (df.Hillshade_3pm+1)\n    df['Shadiness_morn_avg'] = (df.Hillshade_9am + df.Hillshade_Noon)\/2\n    df['Shadiness_afternoon'] = (df.Hillshade_Noon + df.Hillshade_3pm)\/2\n    df['Shadiness_mean_hillshade'] =  (df['Hillshade_9am']  + df['Hillshade_Noon'] + df['Hillshade_3pm'] ) \/ 3    \n    \n    #Shade Difference\n    df[\"Hillshade-9_Noon_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_Noon\"]\n    df[\"Hillshade-noon_3pm_diff\"] = df[\"Hillshade_Noon\"] - df[\"Hillshade_3pm\"]\n    df[\"Hillshade-9am_3pm_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_3pm\"]\n\n    # Mountain Trees\n    df[\"Slope*Elevation\"] = df[\"Slope\"] * df[\"Elevation\"]\n    # Only some trees can grow on steep montain\n    \n    ### More features\n    df['Neg_HorizontalHydrology_HorizontalFire'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])\n    df['Neg_HorizontalHydrology_HorizontalRoadways'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])\n    df['Neg_HorizontalFire_Points_HorizontalRoadways'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])\n    \n    df['MeanNeg_Mean_HorizontalHydrology_HorizontalFire'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])\/2\n    df['MeanNeg_HorizontalHydrology_HorizontalRoadways'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])\/2\n    df['MeanNeg_HorizontalFire_Points_HorizontalRoadways'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])\/2   \n    df[\"Vertical_Distance_To_Hydrology\"] = abs(df['Vertical_Distance_To_Hydrology'])\n    \n    df['Neg_Elev_Hyd'] = df.Elevation-df.Horizontal_Distance_To_Hydrology*0.2\n    \n    # Bin Features\n    bin_defs = [\n        # col name, bin size, new name\n        ('Elevation', 200, 'Binned_Elevation'), # Elevation is different in train vs. test!?\n        ('Aspect', 45, 'Binned_Aspect'),\n        ('Slope', 6, 'Binned_Slope'),\n        ('Horizontal_Distance_To_Hydrology', 140, 'Binned_Horizontal_Distance_To_Hydrology'),\n        ('Horizontal_Distance_To_Roadways', 712, 'Binned_Horizontal_Distance_To_Roadways'),\n        ('Hillshade_9am', 32, 'Binned_Hillshade_9am'),\n        ('Hillshade_Noon', 32, 'Binned_Hillshade_Noon'),\n        ('Hillshade_3pm', 32, 'Binned_Hillshade_3pm'),\n        ('Horizontal_Distance_To_Fire_Points', 717, 'Binned_Horizontal_Distance_To_Fire_Points')\n    ]\n    \n    for col_name, bin_size, new_name in bin_defs:\n        df[new_name] = np.floor(df[col_name] \/ bin_size)\n        \n    print('Total number of features : %d' % (df.shape)[1])\n    return df","95541480":"def load_and_process_dataset():\n    train = pd.read_csv(TRAIN_FILE)\n    test = pd.read_csv(TEST_FILE)\n\n    # XGB needs labels starting with 0!\n    # now 7 become 6, 6 become 5 and so on ..\n    y_train = train[TARGET].ravel() - 1\n    \n    classes = train.Cover_Type.unique()\n    num_classes = len(classes)\n    print(\"There are {0} classes: {1} \".format(num_classes, classes))        \n\n    train.drop([ID, TARGET], axis=1, inplace=True)\n    test.drop([ID], axis=1, inplace=True)\n    \n    train = add_feats(train)    \n    test = add_feats(test)    \n    \n    cols_to_normalize = [ 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n                       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n                       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n                       'Horizontal_Distance_To_Fire_Points', \n                       'Shadiness_morn_noon', 'Shadiness_noon_3pm', 'Shadiness_morn_3',\n                       'Shadiness_morn_avg',\n                       'Shadiness_afternoon', \n                       'Shadiness_mean_hillshade',\n                       'HF1', 'HF2', \n                       'HR1', 'HR2', \n                       'FR1', 'FR2'\n                       ]\n\n    train[cols_to_normalize] = normalize(train[cols_to_normalize])\n    test[cols_to_normalize] = normalize(test[cols_to_normalize])\n\n    # elevation was found to have very different distributions on test and training sets\n    # lets just drop it for now to see if we can implememnt a more robust classifier!\n    train = train.drop('Elevation', axis=1)\n    test = test.drop('Elevation', axis=1)    \n    \n    x_train = train.values\n    x_test = test.values\n\n    return {'X_train': x_train, 'X_test': x_test, 'y_train': y_train}","6d9b0517":"dataset = Dataset(preprocessor=load_and_process_dataset, use_cache=True)","1cb9863f":"#Multiple Classifiers are used for multiple results\n\n#RandomForestClassifier\nrf_params = {'n_estimators': 200, 'criterion': 'entropy', 'random_state': 0}\nrf = Classifier(dataset=dataset, estimator=RandomForestClassifier, \n                use_cache=False, parameters=rf_params, name='rf')\n\n#RandomForestClassifier\nrf1_params = {'n_estimators': 200, 'criterion': 'gini', 'random_state': 0}\nrf1 = Classifier(dataset=dataset, estimator=RandomForestClassifier, \n                 use_cache=False, parameters=rf1_params,name='rf1')\n\n#ExtraTreesClassifier\net_params = {'n_estimators': 200, 'criterion': 'entropy', 'random_state': 0}\net = Classifier(dataset=dataset, estimator=ExtraTreesClassifier, \n                use_cache=False, parameters=et_params,name='et')\n\n#ExtraTreesClassifier\net1_params = {'n_estimators': 200, 'criterion': 'gini', 'random_state': 0}\net1 = Classifier(dataset=dataset, use_cache=False, estimator=ExtraTreesClassifier,\n                 parameters=et1_params,name='et1')\n\n#LGBMClassifier\nlgb_params = {'n_estimators': 200, 'learning_rate':0.1}\nlgbc = Classifier(dataset=dataset, estimator=LGBMClassifier, \n                  use_cache=False, parameters=lgb_params,name='lgbc')\n\n#LogisticRegression\nlogr_params = {'solver' : 'liblinear', 'multi_class' : 'ovr', 'C': 1, 'random_state': 0}\nlogr = Classifier(dataset=dataset, estimator=LogisticRegression, \n                  use_cache=False, parameters=logr_params,name='logr')\n\n#Naive Bayes\ngnb = Classifier(dataset=dataset,estimator=GaussianNB, use_cache=False, name='gnb')","a797e3a5":"def xgb_classifier(X_train, y_train, X_test, y_test=None):\n    xg_params = {'seed': 0,\n                'colsample_bytree': 0.7,\n                'silent': 1,\n                'subsample': 0.7,\n                'learning_rate': 0.1,\n                'objective': 'multi:softprob',   \n                'num_class': 7,\n                'max_depth': 4,\n                'min_child_weight': 1,\n                'eval_metric': 'mlogloss',\n                'nrounds': 200}\n    \n    X_train = xgb.DMatrix(X_train, label=y_train)\n    model = xgb.train(xg_params, X_train, xg_params['nrounds'])\n    return model.predict(xgb.DMatrix(X_test))\n\nxgb_first = Classifier(estimator=xgb_classifier, dataset=dataset, use_cache=CACHE, name='xgb_classifier')","36de832c":"pipeline = ModelsPipeline(rf, et, et1, lgbc, logr, gnb, xgb_first)\nstack_ds = pipeline.stack(k=NFOLDS,seed=SEED)","0a396fae":"stack_ds.X_train.head()","7285a61d":"stack_ds.X_test.head()","cc6e87ec":"print(\"Shape of out-of-fold predictions:\", \"X shape: \", stack_ds.X_train.shape, \"y shape: \", stack_ds.y_train.shape)","db497e7a":"X_train_outfold = stack_ds.X_train.values\nX_test_outfold = stack_ds.X_test.values\nX = X_train_outfold\ny_train_sv = stack_ds.y_train + 1\ny = y_train_sv","e4fece53":"# Train LogisticRegression on stacked data (second stage)\nlr = LogisticRegression\nlr_params = {'C': 5, 'random_state' : SEED, 'solver' : 'liblinear', 'multi_class' : 'ovr',}\nstacker = Classifier(dataset=stack_ds, estimator=lr, use_cache=False, parameters=lr_params)","d164a6ad":"preds_proba = stacker.predict()\n# Note: labels starting with 0 in xgboost, therefore adding +1!\npredictions = np.round(np.argmax(preds_proba, axis=1)).astype(int) + 1\n\nsubmission = pd.read_csv(SUBMISSION_FILE)\nsubmission[TARGET] = predictions\nsubmission.to_csv('Stage_2_1_logregr_out_of_fold.csv', index=None)","80d622b2":"Cs = [0.001, 0.01, 0.1, 1, 10]\ngammas = [0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ngrid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=5, verbose=True)\n#grid_search.fit(X, y)\n#grid_search.best_params_\n#best_svc = grid_search.best_estimator_","139047f9":"best_params_svc = {'C': 10, 'gamma': 0.01}","c025ba55":"best_svc = svm.SVC(**best_params_svc)\nbest_svc.fit(X, y)","d3a626df":"preds_svc = best_svc.predict(X_test_outfold)\nsub_svc = pd.DataFrame({\"Id\": id_test.values,\"Cover_Type\": preds_svc})\nsub_svc.to_csv(\"Stage_2_2_svc_out_of_fold.csv\", index=False)","c7948d26":"y_train_nn = np_utils.to_categorical(stack_ds.y_train + 1)","e980b165":"model = Sequential()\nmodel.add(Dense(1024, input_dim=49, kernel_initializer='uniform', activation='selu'))\nmodel.add(Dense(512, kernel_initializer='uniform', activation='softplus'))\nmodel.add(Dense(256, kernel_initializer='uniform', activation='elu'))\nmodel.add(Dense(128, kernel_initializer='uniform', activation='selu'))\nmodel.add(Dense(64, kernel_initializer='uniform', activation='softplus'))\nmodel.add(Dense(32, kernel_initializer='uniform', activation='elu'))\nmodel.add(Dense(16, kernel_initializer='uniform', activation='softplus'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='softmax'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n# Fit model\nmodel.fit(X, y_train_nn, epochs=10, batch_size=32)","90b020e5":"preds_nn = model.predict(X_test_outfold)\nsub_nn = pd.DataFrame({\"Id\": id_test.values,\"Cover_Type\": np.argmax(preds_nn,axis=1)})\nsub_nn.to_csv(\"Stage_2_3_ann_out_of_fold.csv\", index=False)","030a8aa3":"**Distribution plots of each feature with each Cover_Type**","b0ae3ff9":"# Data Loading and preprocessing\n> Data preprocessing and loading is done","78a34bc2":"# Data Visualization","4eec95b9":"# Importing Required Libraries","28c7183d":"**Pre defined values for later use**","cde18f14":"# Ensemble Learning\n> Ensemble learning is implemented. Model is trained in two stages.\n> 1. In stage 1, training data is fed into multiple classifiers with k-cross folds. The output is out-of-fold predictions.\n> 2. In stage 2 training, out-of-fold predictions are fed to another single classifier (in our case svm and neural networks).\n> 3. In the end, the output of stage 2 classifier is final output.","fd40133a":"# Feature Engineering\n> Creating new features from existing features.","a25016ca":"**Converting dataset to 'Dataset' object using Heamy**\n> Dataset oject is used in every heamy object including classifiers.","98a07da0":"**Initializing the classifiers with their Hyperparameters for Ensemble Learning**\n> * Classifier object is used from Heamy library.\n> * Classifiers from different modules including sklearn are implemented into Classifier object of Heamy.\n> * It is convenient to work with Heamy.","e79ebecb":"# Data Path","000fdc31":"Took reference from:\nhttps:\/\/www.kaggle.com\/justfor\/ensembling-and-stacking-with-heamy","5941b08f":"**Out-of-folds predictions**","6e224080":"**Grid Search for svc**","28c3583a":"Log-Loss is used. Also called as cross entropy loss.\n> -(yt log(yp) + (1 - yt) log(1 - yp))","f79a6d3e":"**Setting up the pipeline to train using multiple models**\n> * Stack the models and returns new dataset with out-of-fold predictions","d200a14c":"# Stage 2.2: SVM\n> Now, training with SVM","98c4f231":"# Stage 2.3: Neural Networks\n> Training with artificial neural networks","0ecf99c5":"# Stage 2.1: Logistic Regression\n> Training on out-of-fold predictions with LogisticRegression","24664317":"**XGB Classifier**\n> * Classifiers can also work through a defined function. A classifier should have four inputs of data and then it can easily be used with Heamy."}}