{"cell_type":{"71a5d376":"code","675e6e46":"code","5ea83aed":"code","a0c1c8fc":"code","6ad375e1":"code","a21e7f77":"code","6791d760":"code","4a7268dd":"code","2600ddfc":"code","216f65bf":"code","b652a455":"markdown","e1df9368":"markdown","a96ef543":"markdown","83c6a05b":"markdown","734585de":"markdown","1f499c06":"markdown","a9164c0b":"markdown","b167646c":"markdown","fb8327b7":"markdown","6b67984c":"markdown","3061c6c0":"markdown","a50bf0de":"markdown","eed522f5":"markdown","8eb8491a":"markdown","3815ee95":"markdown","510561bb":"markdown","5d9dcdba":"markdown","7753b6b8":"markdown"},"source":{"71a5d376":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","675e6e46":"import numpy as np\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom scipy.stats import boxcox\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score,GridSearchCV\nfrom matplotlib.ticker import MaxNLocator\nimport math\nimport sklearn.pipeline as pip\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import quantile_transform\nfrom sklearn.ensemble import VotingClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","5ea83aed":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain.info()","a0c1c8fc":"\"\"\"----------------------DATA PREPROCESSING-----------------------\"\"\"\n\"\"\"method used to extract 'Title' \"\"\"\ndef substrings_in_string(big_string, substrings):\n    for substring in substrings:\n        if big_string.find(substring) != -1:\n            return substring\n    print (big_string)\n    return np.nan\n\n\"\"\"PHASE 1 : Extracting 'Deck' , 'Title' , 'Family_Size'\"\"\"\ndef phase1clean(df):\n    \n    #setting silly values to nan\n    df.Fare = df.Fare.map(lambda x: np.nan if x==0 else x)\n    \n    #Special case for cabins as 'nan' may be signal\n    df.Cabin = df.Cabin.fillna('Unknown')    \n    cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\n    df['Deck']=df['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n        \n    \n    #creating a title column from name\n    title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n                'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n                'Don', 'Jonkheer']\n    \n    df['Title']=df['Name'].map(lambda x: substrings_in_string(x, title_list))\n    \n    #replacing all titles with mr, mrs, miss, master\n    def replace_titles(x):\n        title=x['Title']\n        if title in ['Countess','Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n            return 'Rare'\n        elif title in ['Countess', 'Mme']:\n            return 'Mrs'\n        elif title in ['Mlle', 'Ms']:\n            return 'Miss'\n        elif title =='Dr':\n            if x['Sex']=='Male':\n                return 'Mr'\n            else:\n                return 'Mrs'\n        else:\n            return title\n    \n    df['Title']=df.apply(replace_titles, axis=1)\n    \n    #Creating new family_size column\n    df['Family_Size']=df['SibSp']+df['Parch']\n    \n    return df\n\n#---------------------------------------------------------------------------------------\n\n\"\"\"Creating 'Fare_Bin' : Binning fare using manually set boundaries\"\"\"\ndef fare_grouping(x):\n    # Ranging and grouping Fare using historical data\n    bins = [-1, 7.91, 14.454, 31, 99, 250, np.inf]\n    names = ['a', 'b','c', 'd', 'e', 'f']\n    names = [1, 2, 3, 4, 5, 6]\n    x['Fare_Bin'] = pd.cut(x['Fare'], bins ,labels=names).astype('int')\n    dict_age={1 : 'a' , 2 : 'b' , 3 : 'c' , 4 : 'd' , 5: 'e' , 6 : 'f'}\n    x['Fare_Bin']=x['Fare_Bin'].map(dict_age)\n    \"\"\"visualizing fares\"\"\"\n    # sns.factorplot(x=\"Fare_Bin\", data=x , kind=\"count\",size=6, aspect=.7)\n    # plt.show()\n    # sns.scatterplot(x=\"PassengerId\",y=\"fare_gauss\",hue=\"Fare_Bin\",data=x,palette='RdYlGn' , legend='full')\n    # plt.show()\n    # sns.distplot(x['Age'],axlabel='training set');\n    # plt.show()\n    return x\n\n#----------------------------------------------------------------------------------------\n\n\"\"\"Removing all missing values\"\"\"\ndef fill_nan(x):\n    \"\"\"filling age nan's\"\"\"\n    null_ind=x.loc[x['Age'].isnull(),:].index\n    null_count=x.loc[x['Age'].isnull(),]['PassengerId'].count()\n    num_ages=x.groupby('Title')['Age'].mean().to_dict()\n    x.loc[x['Age'].isnull(),'Age']=x.loc[x['Age'].isnull(),'Title'].map(num_ages)\n\n    \n    \"\"\"filling fare and binning fare\"\"\"\n    null_ind=x.loc[x['Fare'].isnull(),:].index\n    null_count=x.loc[x['Fare'].isnull(),]['PassengerId'].count()\n    num_fare=x.groupby('Pclass')['Fare'].mean().to_dict()\n    x.loc[x['Fare'].isnull(),'Fare']=x.loc[x['Fare'].isnull(),'Pclass'].map(num_fare)\n    fare_grouping(x)\n    \n    \"\"\"removing embarked nan's\"\"\"\n    f_index=x[x['Embarked'].isnull()].index\n    x=x.drop(f_index,axis=0)\n    \n    return x\n\n#----------------------------------------------------------------------------------------\n\n\"\"\"Creating 'Age_Grp' : Binning age using silhouette method and clustering \"\"\"\ndef age_grouping(x):\n    \"\"\"visualizing age feature\"\"\"\n    # sns.distplot(x['Age'],axlabel='training set');\n    # plt.show()\n    # sns.factorplot(x=\"Age_Grp\", data=x , kind=\"count\",size=6, aspect=.7)\n    # plt.show()\n    # sns.factorplot(x=\"Age_Grp\",col=\"Survived\", data=x , kind=\"count\",size=6, aspect=.7)\n    # plt.show()\n    k_data=pd.concat([x['Age'],x['Survived']],axis=1)\n    k_data.rename( columns={ 0 :'Age' , 1 :'Survived'}, inplace=True )\n    \"\"\"finding optimal no of clusters using silhouette method\"\"\"\n    model = KMeans()\n    sil = []\n    # dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n    for k in range(2, 9):\n      kmeans = KMeans(n_clusters = k).fit(k_data)\n      labels = kmeans.labels_\n      sil.append(silhouette_score(k_data, labels, metric = 'euclidean'))\n    plt_data=pd.concat([pd.Series(range(2,9)),pd.Series(sil)],axis=1)\n    plt_data.rename( columns={ 0 :'clusters' , 1 :'silhouette scores'}, inplace=True )\n    \"\"\"visualizing the silhouette score plot\"\"\"\n    # ax = sns.lineplot(x=\"clusters\", y=\"silhouette scores\",\n    #                   estimator=None, lw=1,\n    #                   err_style=\"bars\", ci=68, \n    #                   data=plt_data)\n    kmeans = KMeans(n_clusters=4, random_state=0).fit(k_data)\n    labels_=kmeans.labels_\n    k_data['age_grp']=labels_+1\n    k_data['Passenger_Id']=x['PassengerId']\n    \"\"\"visualizing the distribution of passengers of each cluster\"\"\"\n    # sns.scatterplot(x=\"Passenger_Id\",y=\"Age\",data=k_data,hue=\"age_grp\",palette='viridis' , legend='full')\n    # plt.show()\n    # age_grp_survcount=x.loc[x['Survived']==1,:].groupby('Age_Grp')['Survived'].count()\n    \n    \"\"\"finding boundaries of the clusters\"\"\"\n    #age_grp_max=k_data.groupby('Age_Grp').Age.max()\n    #age_grp_min=k_data.groupby('Age_Grp').Age.min()\n    \"\"\"making the boundary map for the different clusters \"\"\"\n    agelist=[] \n    for i in range(0, 15):\n        agelist.append('a')\n    for i in range(15, 29):\n        agelist.append('b')\n    for i in range(29, 45): \n        agelist.append('c')\n    for i in range(45, 90):\n        agelist.append('d')\n    age_dict={v: k for v, k in enumerate(agelist)}\n    x['Age_Grp']=x['Age'].astype(int).map(age_dict)\n\n    return x,age_dict\n\n#---------------------------------------------------------------------------------------------\n\n\"\"\"Creating 'Family_Survived' feature\"\"\"\ndef survived_fams(df):\n    # A function working on family survival rate using last names and ticket features\n    df['Last_Name'] = df['Name'].apply(\n        lambda df: str.split(df, \",\")[0])\n    \n    # Adding new feature: 'Survived'\n    default_survival_rate = 0.5\n    df['Family_Survival'] = default_survival_rate\n    \n    for grp, grp_df in df[['Survived', 'Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId','SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n        if (len(grp_df) != 1):\n            # A Family group is found.\n            for ind, row in grp_df.iterrows():\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    df.loc[df['PassengerId'] ==\n                                  passID, 'Family_Survival'] = 1\n                elif (smin == 0.0):\n                    df.loc[df['PassengerId'] ==\n                                  passID, 'Family_Survival'] = 0\n    \n    for _, grp_df in df.groupby('Ticket'):\n        if (len(grp_df) != 1):\n            for ind, row in grp_df.iterrows():\n                if (row['Family_Survival'] == 0) | (\n                        row['Family_Survival'] == 0.5):\n                    smax = grp_df.drop(ind)['Survived'].max()\n                    smin = grp_df.drop(ind)['Survived'].min()\n                    passID = row['PassengerId']\n                    if (smax == 1.0):\n                        df.loc[df['PassengerId'] ==\n                                      passID, 'Family_Survival'] = 1\n                    elif (smin == 0.0):\n                        df.loc[df['PassengerId'] ==\n                                      passID, 'Family_Survival'] = 0\n    \n    \n    return df\n\n#---------------------------------------------------------------------------------------------\n\n\"\"\"Creating 'is_alone' feature\"\"\"\ndef is_alone(x):\n    fam_list={False : 0 , True : 1}\n    x['is_alone'] = (x['Family_Size']==0).map(fam_list)\n    \"\"\"visualization\"\"\"\n    # sns.factorplot(x=\"is_alone\", data=clean_train_reg , kind=\"count\",size=6, aspect=.7)\n    # f_1=clean_train_reg[clean_train_reg['Survived']==1].groupby('is_alone')['Survived'].count()\n    # f_0=clean_train_reg[clean_train_reg['Survived']==0].groupby('is_alone')['Survived'].count()\n    # f_s=f_1\/f_1+f_0\n    return x\n\n#---------------------------------------------------------------------------------------------\n\n\"\"\"Creating 'has_cabin' feature\"\"\"\ndef has_cabin(x):\n    fam_list={False : 0 , True : 1}\n    x['has_cabin'] = (x['Deck']!='Unknown').map(fam_list)\n    \"\"\"visualization\"\"\"\n    return x\n    \n#---------------------------------------------------------------------------------------------\n\n\"\"\"Creating 'is_3rdclass' feature\"\"\"    \ndef is_3stclass(x):\n    class_list={False : 0 , True : 1}\n    x['is_3rdclass'] = (x['Pclass']== 3 ).map(class_list)\n    \"\"\"visualization\"\"\"\n    return x\n\n#---------------------------------------------------------------------------------------------\n\n\"\"\"Coverting Pclass.type from 'int' to 'object' \"\"\" \ndef class_categorizer(x):\n    dict_class={1 : 'a' , 2 : 'b' , 3 : 'c' }\n    x['Pclass']=x['Pclass'].map(dict_class)\n    return x\n\n#---------------------------------------------------------------------------------------------\n\n\n\"\"\"PHASE 2 : Extracting 'Age_Grp' , 'Fare_Per_Person' , 'Age*Class' , 'Family_Survival' , 'is_alone' , 'has_cabin' , 'is_3rdclass' \"\"\"\ndef phase2clean(train, test):\n            #data type dictionary\n    # data_type_dict={'Pclass':'ordinal', 'Sex':'nominal', \n    #                 'Age':'numeric', \n    #                 'Fare':'numeric', 'Embarked':'nominal', 'Title':'nominal',\n    #                 'Deck':'nominal', 'Family_Size':'ordinal'}      \n    \n    \n    #imputing nan values\n    train=fill_nan(train)\n    train,age_dict=age_grouping(train)\n    test=fill_nan(test)\n    test['Age_Grp']=test['Age'].astype(int).map(age_dict)\n    \n    \n    #Fare per person\n    for df in [train, test]:\n        df['Fare_Per_Person']=df['Fare']\/(df['Family_Size']+1)\n    \n    #Age times class\n    for df in [train, test]:\n        df['Age*Class']=df['Age']*df['Pclass']\n\n\n    combined=pd.concat([train,test])\n    combined=survived_fams(combined)\n    combined=is_alone(combined)\n    combined=has_cabin(combined)\n    combined=is_3stclass(combined)\n    train=combined.iloc[:len(train),:]\n    test=combined.iloc[len(train):,:]\n    test=test.drop(['Survived'],axis=1)\n    \n    return [train,test]\n\n#---------------------------------------------------------------------------------------------\n\n\"\"\"Container function for all the data preprocessing methods\"\"\"\n\"\"\"It also handles the removal of the unwanted features ,it also returns the original training and test datasets \"\"\"\ndef get_data():\n    train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n    submit_x =pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n    original_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n    original_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n    x=phase1clean(train_data)\n    pred_set=phase1clean(submit_x)\n    x,pred_set=phase2clean(x, pred_set)\n    x=x.drop(['PassengerId','Name','Ticket','Cabin','Last_Name'],axis=1)\n    pred_set=pred_set.drop(['PassengerId','Name','Ticket','Cabin','Last_Name'],axis=1)\n    y = x.Survived\n    x = x.loc[:,x.columns!='Survived'] \n    return x,y,pred_set,original_train,original_test\n\n#---------------------------------------------------------------------------------------------\n\n\n\n\n","6ad375e1":"x,y,pred_set,original_train,pred_set_original=get_data()\nx.info()\nx.head()","a21e7f77":"\"\"\"converting functions to transformers\"\"\"\nfrom sklearn.preprocessing import FunctionTransformer\nclass_cat=FunctionTransformer(class_categorizer)\nquantile_transformer = FunctionTransformer(quantile_transform)\n\n\n\"\"\"xgboost pipe_line\"\"\"\ncat_features=['Title', 'Deck' ,'Pclass' , 'Sex' , 'Embarked' , 'Age_Grp' , 'Fare_Bin']\nnum_features = list(x.select_dtypes(include=['int64','float64']).columns)\ncategorical_transformer = pip.Pipeline(steps=[('class_cat',class_cat),\n                                              ('enc', CatBoostEncoder())\n                                              ])\nnumerical_transformer = pip.Pipeline([('just','passthrough')])\npreprocessor = ColumnTransformer(transformers=[('cat', categorical_transformer , cat_features),\n                                               ('num' , numerical_transformer , num_features)\n                                               ])\npipeline_xgb =pip.Pipeline(steps=[('preprocessor', preprocessor),\n                                  ('feature_select', SelectKBest(chi2 , k = 15)),\n                                  ('classifier',XGBClassifier(learning_rate=0.01 ,\n                                                              n_estimators=860,\n                                                              max_depth=3,\n                                                              subsample=1,\n                                                              colsample_bytree=1,\n                                                              gamma=6,\n                                                              reg_alpha = 14,\n                                                              reg_lambda = 3))\n                                  ])\n\n#---------------------------------------------------------------------------------------------\n\n\"\"\"model evaluation\"\"\"\ncv = StratifiedKFold(5, shuffle=True, random_state=42)\naccuracies = cross_val_score(pipeline_xgb, x , y , cv = cv)\nprint(\"5 fold cross validation accuracies {}\".format(accuracies))","6791d760":"\"\"\"Template used for hyperparameter tuning\"\"\"\nparams=[{\n    # 'classifier__n_estimators' : [i for i in range(700,910,10)]\n    # 'classifier__subsample' : [i\/100 for i in range(80,101)]\n    'feature_select' : [SelectKBest(chi2)],\n    'feature_select__k' : [i for i in range(5,19)]\n    }]\n\ncv = StratifiedKFold(5, shuffle=True, random_state=42)\n\nsearch=GridSearchCV(estimator=pipeline_xgb,\n                    param_grid=params,\n                    n_jobs=-1,\n                    cv=cv)\n\nsearch.fit(x, y)\nprint(\"best score : {}  , best params : {}  \".format(search.best_score_ , search.best_params_))\n#search.cv_results_","4a7268dd":"\"\"\"logistic regression pipeline\"\"\"\ncat_features=['Title', 'Deck' ,'Pclass' , 'Sex' , 'Embarked' , 'Age_Grp' , 'Fare_Bin']\nnum_features = list(x.select_dtypes(include=['int64','float64']).columns)\ncategorical_transformer = pip.Pipeline(steps=[('class_cat',class_cat),\n                                              ('enc', CatBoostEncoder())\n                                              ])\nnumerical_transformer = pip.Pipeline([('normal_trans',quantile_transformer)])\npreprocessor = ColumnTransformer(transformers=[('cat', categorical_transformer , cat_features),\n                                               ('num' , numerical_transformer , num_features)])\npipeline_log =pip.Pipeline(steps=[('preprocessor', preprocessor),\n                                  ('feature_select',SelectKBest(chi2, k = 17 )),\n                                  ('classifier',LogisticRegression(penalty = 'l2',\n                                                                   solver = 'liblinear',\n                                                                   C = 0.25))\n                                  ])\n\n#---------------------------------------------------------------------------------------------\n\n\"\"\"model evaluation\"\"\"\ncv = StratifiedKFold(5, shuffle=True, random_state=42)\naccuracies = cross_val_score(pipeline_log, x , y , cv = cv)\nprint(\"5 fold cross validation accuracies {}\".format(accuracies))\n","2600ddfc":"classifier = VotingClassifier(estimators=[('XGB', pipeline_xgb), ('LOG', pipeline_log)])\n\n\"\"\"model evaluation\"\"\"\ncv = StratifiedKFold(5, shuffle=True, random_state=42)\naccuracies = cross_val_score(classifier, x , y , cv = cv)\nprint(\"5 fold cross validation accuracies {}\".format(accuracies))","216f65bf":"classifier.fit(x,y)\ny_submit=pd.Series((classifier.predict(pred_set)))\ny_submit=y_submit.astype(int)\ny_1=pred_set_original.PassengerId\ny_submit_f=pd.concat([y_1,y_submit],axis=1)\ny_submit_f.rename( columns={ 0 :'Survived'}, inplace=True )\ny_submit_f.to_csv('submission.csv',index=False)        ","b652a455":"<h4>We will now implement the data preprocessing and have a look at out processed data<\/h4>  ","e1df9368":"<p><h3>features that will be removed : <\/h3><\/p>\n<ul>\n<li>'PassengerId'<\/li>\n<li>'Name'<\/li>\n<li>'Ticket'<\/li>\n<li>'Cabin'<\/li>\n<li>'Last_Name'<\/li>\n<\/ul>\n<p>These features are almost unique for each passenger and advanced processing must be done to make use of these features . Therefore , for simplicity we drop these features . <\/p>","a96ef543":"<p><h1>Finding survivors of the titanic tragedy<\/h1><\/p>\n<h4>Countering data contamination and attempt at a clean code<\/h4>","83c6a05b":"We will mainly emphasize on the optimal usage of pipelines to reduce risks of data leakage . ","734585de":" <h4> <p>Parameters of the transformers were already tuned using gridsearchCV .<\/p><p>Given below is the template used to optimize the hyperparameters of the transformers.<\/p> <\/h4>","1f499c06":"<h1>DATA PREPROCESSING<\/h1>","a9164c0b":"<p>In this notebook:<ul>\n    <li>We will not be doing a detailed EDA since there are many EDA based notebooks out there on this particular problem (titanic survivors)<\/li>\n    <li>We will enlist and explain the originally existing features , the features that are extracted , and the features that are removed<\/li>  \n    <li>We will build an xgboost classifier and a logistic regression classifier using pipelines<\/li>\n    <li>We will evaluate our models using cross validation<\/li>\n    <li>We will build a voting ensemble classifier using the above models<\/li>\n    <\/ul>","b167646c":"<p><h3>Features that are extracted : <\/h3><\/p>\n<ul><li>'Deck' :    The alphabet (prefix) from the 'Cabin' feature is extracted  as deck .<\/li>\n    <li>'Title' :    The Title alone is extracted from the 'Name' feature and is binned into the major categories .<\/li>\n    <li>'Family_Size :    = 'SibSp' + 'Parch' .<\/li>\n    <li>'Fare_Bin' :    'Fare_Bin' is a feature where the 'Fare' column is binned into different categories (the number and boundaries of these categories were decided while analaysing the data)<\/li>\n    <li>'Age_grp' :    This is the binned form of the 'Age' column and binnning is done using clustering analysis. The optimal no of clusters (k) of age was found using the sillhouette method (find the maxima on the sillhouette curve) . These k clusters of age were then analyzed and their boundaries were extracted . these boundaries were used to map and bin the 'Age' column into different k different categories . <\/li>\n    <li>'Fare_per_person' :    = 'Fare' \/ ('Family_Size'+1)<\/li>\n    <li>'Age*Class' : = 'Age' x 'Class' <\/li>\n    <li>'Family_Survival' : This feature is extracted based on the assumption that a family either has the same 'Last_name'(extracted from the 'Name' feature) or the same 'Ticket' . This feature states : if any one person from a family survives , the 'Family_Survival' for all the passengers in the family will be = 1 . Subsequently , if any one person from a family dies , the 'Family_Survival' for the whole family will = 0 . All passengers start out with a basic 'Family_Survival' of 0.5 . <\/li>\n    <li>'is_alone' : If 'Family_Size' == 0 , then 'is_alone' = 1<\/li>\n    <li>'has_cabin' : If 'Deck'!='Unknown' , then 'has_cabin' = 1<\/li>\n    <li>'is_3rdclass' : If 'Pclass' == 3 , then 'is_3rdclass' = 1<\/li>","fb8327b7":"<p><h3>pipeline_xgb :<\/h3><ul>\n    <li><\/li>\n    <\/ul>\n\n\n<p><h3>Pipelines : <\/h3><ul>\n    <li>Pipelines act as a reusable container of different transformers that need to be used in a particular order , repeatedly . <\/li>\n    <li>Pipelines make the code look cleaner , make the workflow easier , and makes your work reproducible.<\/li>\n    <li>Pipelines use a systematic stepwise approach , this approach avoids train-test data contamination .<\/li>\n    <\/ul><\/p>\n<p><h4>We will be using 2 different pipelines in this notebook :<\/h4> <ol>\n    <li>pipeline_xgb : Pipeline contains XGBClassifier() and  xgboost specific data preprocessing <\/li>\n    <li>pipeline_log : Pipeline contains LogisticRegression() and  logistic regression specific data preprocessing <\/li>\n    <\/ol><\/p>\n<p><h3>pipeline_xgb : Elements<\/h3><ol>\n    <li>Changing 'Pclass' to type : 'object' . <\/li>\n    <li>Applying CatBoostEncoder() to all the categorical features .<\/li>\n    <li>selecting optimum k features based on chi2 .<\/li>\n    <li>containing the classifier .<\/li>\n    <\/ol>\n<p><h3>pipeline_log : Elements<\/h3><ol>\n    <li>Changing 'Pclass' to type : 'object' . <\/li>\n    <li>Applying CatBoostEncoder() to all the categorical features .<\/li>\n    <li>Applying Quantile transformer to all the numerical features to make them normally distributed . <\/li>\n    <li>selecting optimum k features based on chi2 .<\/li>\n    <li>containing the classifier .<\/li>\n    <\/ol>\n\n<p><h3>pipeline_xgb implemention is given below :<\/h3><ul>","6b67984c":"<p><h3>Over the course of data preprocessing many functions are used that will help in extracting features<\/h3><\/p> \n<p><h3>All the functions that are used for processing the data and extracting the features are given below  : <\/h3><\/p> ","3061c6c0":"<h3> hope this helps all the readers  .  if u like this notebook , give an upvote to keep me motivated  . critical comments are appreciated  , cheers :) ","a50bf0de":"<p><h1>ENSEMBLE : Weighted Voting classifier<\/h1><\/p>\n<h4><ul>\n    <li>Voting classifier considers the majority prediction among the predictions of the various classifiers<\/li>\n    <li>Classifiers included in ensemble models must be diverse by function and by the features they are trained on for maximum benefit.<\/li>\n    <\/ul><\/h4>\n<p><h4>We will use sklearn implementation of weighter voting classifier ( class sklearn.ensemble.VotingClassifier() ) which will contain pipeline_xgb and pipeline_log as its estimators .<\/h4><\/p>","eed522f5":"<h3>Originally existing features : (description for these features can be found in the data tab of the titanic competition)<\/h3>","8eb8491a":"let us first import out libraries","3815ee95":"<h1>MODELLING<\/h1>","510561bb":"<p><h3>pipeline_log implemention is given below :<\/h3><ul>","5d9dcdba":"<p><h4>We see that the weighted voting classifier is more stable than its estimators when they are seperate .<\/h4><\/p>\n<p><h4>We will now proceed to making our submition using our ensemble model .<\/h4><\/p>","7753b6b8":"<h3>Let us now look at our features<\/h3>\n"}}