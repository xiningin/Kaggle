{"cell_type":{"3ab262cd":"code","f298a86e":"code","a03914a7":"code","a070ef5e":"code","da7b5972":"code","8577a681":"code","40e5e399":"code","e8e03ae3":"code","f22b8464":"code","47ae3d90":"code","ebdcc14e":"code","0ba1aa7d":"code","e4b4671b":"code","a9fa370b":"code","f19bf2b2":"code","0f9457e6":"code","3c6ea070":"code","f17abee8":"code","e72f2963":"code","4b1aa2e7":"code","9c853ef7":"code","6fee2eb9":"code","5c1ab833":"code","dbfe99e0":"code","aa33475e":"code","ba759d8d":"code","ad05b902":"code","db509c82":"code","ea7e37ab":"code","4b200138":"code","d6351de0":"code","c8bab953":"code","8c6a51b2":"code","65bfbd50":"code","4c16cc82":"code","a80a9ec2":"code","6879770b":"code","62688ef4":"code","762df76c":"code","eb5f9274":"code","ceb0c6ea":"code","5fd00e87":"code","b28c0274":"code","caa6a3a9":"code","2bb4c822":"code","178f2698":"code","1dfc2ecb":"code","5e393cb2":"code","9b396576":"code","e3d07796":"code","c9231975":"code","4ab534a1":"code","6190ce03":"code","74d1c2e6":"code","d5e94705":"code","083312c2":"code","085e624e":"code","093ffac3":"code","2d886dcd":"code","a544cc56":"code","e16b2f9f":"code","9ed21daa":"code","2236c7dc":"code","d11b05bd":"code","8fd55920":"code","d8053629":"code","3a9602a9":"code","6aaab505":"code","6787ffee":"code","ff0ab21d":"code","7e4b2319":"code","527d623d":"code","d884622d":"code","6e70136e":"code","dbe0ddaa":"code","c032fb9f":"code","e989e522":"code","8f8fa488":"code","e68c2202":"code","5fb03a43":"code","fb0543ff":"code","6af7f348":"code","bc46b12a":"code","ffa118eb":"code","aeca0182":"code","0e7910db":"code","1dc28136":"code","814119af":"code","ec088ec3":"code","7371dd03":"code","bb930595":"code","34681e90":"code","2339dbe6":"code","7753e731":"code","8c1e88aa":"code","c5a167b9":"code","de7ac9f0":"code","66923d02":"code","e646f1e4":"code","4e8115d5":"code","9a6e95e7":"code","76e82c48":"code","528606d3":"code","188817b1":"code","5dcd6c76":"code","20c8822f":"code","3e29d129":"code","a52ed9a3":"code","22804dee":"code","d4d2dd63":"code","54ba6a39":"code","bbf4ccfd":"code","db42fa19":"code","535230d1":"code","f577aaa0":"code","6badad7d":"code","f3f782f5":"code","5cf1db3e":"code","a431b800":"markdown","251841c7":"markdown","32586603":"markdown","81cef4e3":"markdown"},"source":{"3ab262cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f298a86e":"from matplotlib import pyplot as plt\nimport seaborn as sns","a03914a7":"lines =[]\nwith open('\/kaggle\/input\/frenchenglish-bilingual-pairs\/fra-eng\/fra.txt','r') as f:\n    lines.extend(f.readline() for i in range(5))","a070ef5e":"lines","da7b5972":"df = pd.read_csv('\/kaggle\/input\/frenchenglish-bilingual-pairs\/fra-eng\/fra.txt', sep = '\\t', header = None)","8577a681":"df.head()","40e5e399":"df.tail()","e8e03ae3":"#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.rename.html\ndf.columns = ['English','French']","f22b8464":"df","47ae3d90":"#How to calculate number of words in a string in DataFrame:\n#https:\/\/stackoverflow.com\/a\/37483537\/4084039\nword_count = df['English'].str.split().apply(len).value_counts()","ebdcc14e":"type(word_count)","0ba1aa7d":"word_count","e4b4671b":"word_dict = dict(word_count)\nword_dict = dict(sorted(word_dict.items(), key=lambda kv: kv[1]))","a9fa370b":"word_dict","f19bf2b2":"index  = np.arange(len(word_dict))","0f9457e6":"values1 = word_dict.values()","3c6ea070":"values1","f17abee8":"plt.figure(figsize=(20,5))\nplt.bar(index,values1)\nplt.xlabel('length of sentences in english')\nplt.ylabel('occurances')\nplt.xticks(index,word_dict.keys())\nplt.show()","e72f2963":"word_count = df['French'].str.split().apply(len).value_counts()","4b1aa2e7":"word_count","9c853ef7":"word_dict = dict(word_count)\nword_dict = dict(sorted(word_dict.items(), key=lambda kv: kv[1]))","6fee2eb9":"word_dict","5c1ab833":"index  = np.arange(len(word_dict))\nvalues2 = word_dict.values()","dbfe99e0":"plt.figure(figsize=(20,5))\nplt.bar(index,values2)\nplt.xlabel('length of sentences in english')\nplt.ylabel('occurances')\nplt.xticks(index,word_dict.keys())\nplt.show()","aa33475e":"len(word_dict)","ba759d8d":"word_count = df['French'].str.split().apply(len)","ad05b902":"word_count","db509c82":"french_lenght_of_sentences = word_count.values","ea7e37ab":"word_count = df['English'].str.split().apply(len)","4b200138":"english_lenght_of_sentences = word_count.values","d6351de0":"plt.boxplot([english_lenght_of_sentences,french_lenght_of_sentences])\nplt.xticks([1,2],['English','French'])\nplt.ylabel('Lenght of sentences')\nplt.grid()\nplt.show()","c8bab953":"plt.figure(figsize=(10,3))\nsns.kdeplot(english_lenght_of_sentences,label=\"english sentences\")\nsns.kdeplot(french_lenght_of_sentences,label=\"french sentences\")\nplt.legend()\nplt.show()","8c6a51b2":"import numpy as np\nimport tensorflow as tf\nimport keras\nimport tqdm as tqdm\nfrom tensorflow.keras.layers import Dense,concatenate,Activation,Dropout,Input,LSTM,Embedding,Flatten,Conv1D,BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","65bfbd50":"batch_size = 64  # Batch size for training.\nepochs = 20  # Number of epochs to train for.\nlatent_dim = 256  # Latent dimensionality of the encoding space.\nnum_samples = 15437  # Number of samples to train on.\n# Path to the data txt file on disk.\ndata_path = '\/kaggle\/input\/frenchenglish-bilingual-pairs\/fra-eng\/fra.txt'","4c16cc82":"# Vectorize the data.\ninput_texts = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()","a80a9ec2":"with open(data_path, 'r', encoding='utf-8') as f:\n    lines = f.read().split('\\n')","6879770b":"for line in lines[: min(num_samples, len(lines) - 1)]:\n    input_text, target_text, = line.split('\\t')\n    # We use \"tab\" as the \"start sequence\" character\n    # for the targets, and \"\\n\" as \"end sequence\" character.\n    target_text = '\\t' + target_text + '\\n'\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)","62688ef4":"target_characters","762df76c":"input_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])","eb5f9274":"print('Number of samples:', len(input_texts))\nprint('Number of unique input tokens:', num_encoder_tokens)\nprint('Number of unique output tokens:', num_decoder_tokens)\nprint('Max sequence length for inputs:', max_encoder_seq_length)\nprint('Max sequence length for outputs:', max_decoder_seq_length)","ceb0c6ea":"input_token_index = dict(\n    [(char, i) for i, char in enumerate(input_characters)])\ntarget_token_index = dict(\n    [(char, i) for i, char in enumerate(target_characters)])","5fd00e87":"encoder_input_data = np.zeros(\n    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n    dtype='float32')\ndecoder_input_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n    dtype='float32')\ndecoder_target_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n    dtype='float32')","b28c0274":"for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    for t, char in enumerate(input_text):\n        encoder_input_data[i, t, input_token_index[char]] = 1.\n    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n    for t, char in enumerate(target_text):\n        # decoder_target_data is ahead of decoder_input_data by one timestep\n        decoder_input_data[i, t, target_token_index[char]] = 1.\n        if t > 0:\n            # decoder_target_data will be ahead by one timestep\n            # and will not include the start character.\n            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n    decoder_target_data[i, t:, target_token_index[' ']] = 1.","caa6a3a9":"# Define an input sequence and process it.\nencoder_inputs = Input(shape=(None, num_encoder_tokens))\nencoder = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]","2bb4c822":"# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None, num_decoder_tokens))\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n                                     initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)","178f2698":"# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","1dfc2ecb":"model.summary()","5e393cb2":"# Run training\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_split=0.2)","9b396576":"# Save model\nmodel.save('s2s.h5')","e3d07796":"from keras.models import load_model\n\nmodel = load_model(\"s2s.h5\")","c9231975":"\n# Next: inference mode (sampling).\n# Here's the drill:\n# 1) encode input and retrieve initial decoder state\n# 2) run one step of decoder with this initial state\n# and a \"start of sequence\" token as target.\n# Output will be the next target token\n# 3) Repeat with the current target token and current states\n\n# Define sampling models\nencoder_model = Model(encoder_inputs, encoder_states)","4ab534a1":"decoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_outputs, state_h, state_c = decoder_lstm(\n    decoder_inputs, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states)\n","6190ce03":"# Reverse-lookup token index to decode sequences back to\n# something readable.\nreverse_input_char_index = dict(\n    (i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict(\n    (i, char) for char, i in target_token_index.items())\n","74d1c2e6":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, target_token_index['\\t']] = 1.\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '\\n' or\n           len(decoded_sentence) > max_decoder_seq_length):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence","d5e94705":"import warnings\nwarnings.simplefilter(\"ignore\")","083312c2":"from nltk.translate.bleu_score import sentence_bleu\nfor seq_index in range(10,12):\n    # Take one sequence (part of the training set)\n    # for trying out decoding.\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Input sentence:', input_texts[seq_index])\n    print('Decoded sentence:', decoded_sentence)\n    \n    score = sentence_bleu(input_texts[seq_index], target_texts[seq_index])\n    print('from original sentence',score)\n    \n    score = sentence_bleu(input_texts[seq_index], decoded_sentence)\n    print('from converted sentence',score)","085e624e":"df[10:12]","093ffac3":"df  = df[0:12000]","2d886dcd":"df.shape","a544cc56":"df['Decoder'] = df['French'] + ' \\n'","e16b2f9f":"df['French']  = '\\t ' + df['French']  + ' \\n'","9ed21daa":"df[0:10]","2236c7dc":"df=  df.applymap(str.lower)","d11b05bd":"df[0:10]","8fd55920":"#y = df['Decoder']\n#X = df.drop(['Decoder'],axis =1)","d8053629":"#print(y.shape)\n#print(X.shape)","3a9602a9":"#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=42)","6aaab505":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","6787ffee":"max_english_sentence_lenght = 0\nfor i in range(df['English'].shape[0]):\n    a = len(df['English'].iloc[i].split())\n    if a > max_english_sentence_lenght:\n        max_english_sentence_lenght =a\n","ff0ab21d":"max_english_sentence_lenght","7e4b2319":"tokenizer = Tokenizer( filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n')\ntokenizer.fit_on_texts(df['English'])","527d623d":"sequence_train = tokenizer.texts_to_sequences(df['English'])\n#sequence_test = tokenizer.texts_to_sequences(X_test['English'])","d884622d":"english_dic = tokenizer.word_index","6e70136e":"english_vocab = len(tokenizer.word_index) + 1","dbe0ddaa":"english_vocab","c032fb9f":"sequence_train[0:10]","e989e522":"#need to cover the case if state that is present in test data may not be present in train data by padding although X_train \n# consists all the state\n#english_test_text = pad_sequences(sequence_test, max_english_sentence_lenght,padding ='post')\nenglish_train_text = pad_sequences(sequence_train, max_english_sentence_lenght,padding = 'post')\n","8f8fa488":"english_train_text[0:10]","e68c2202":"max_french_sentence_lenght = 0\nfor i in range(df['French'].shape[0]):\n    a = len(df['French'].iloc[i].split())\n    if a > max_french_sentence_lenght:\n        max_french_sentence_lenght =a\n","5fb03a43":"max_french_sentence_lenght","fb0543ff":"tokenizer = Tokenizer( filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~')\ntokenizer.fit_on_texts(df['French'])","6af7f348":"sequence_train = tokenizer.texts_to_sequences(df['French'])\n#sequence_test = tokenizer.texts_to_sequences(X_test['French'])","bc46b12a":"decoder_train = tokenizer.texts_to_sequences(df['Decoder'])\n#decoder_test = tokenizer.texts_to_sequences(y_test)","ffa118eb":"french_dic = tokenizer.word_index","aeca0182":"french_dic","0e7910db":"french_vocab = len(tokenizer.word_index) +1","1dc28136":"french_vocab","814119af":"sequence_train[0:10]","ec088ec3":"decoder_train[0:10]","7371dd03":"#need to cover the case if state that is present in test data may not be present in train data by padding although X_train \n# consists all the state\n#french_test_text = pad_sequences(sequence_test, max_french_sentence_lenght,padding = 'post')\nfrench_train_text = pad_sequences(sequence_train, max_french_sentence_lenght,padding = 'post')","bb930595":"#need to cover the case if state that is present in test data may not be present in train data by padding although X_train \n# consists all the state\n#decoder_test_text = pad_sequences(decoder_test, max_french_sentence_lenght,padding = 'post')\ndecoder_train_text = pad_sequences(decoder_train, max_french_sentence_lenght,padding = 'post')","34681e90":"french_train_text[0:10]","2339dbe6":"decoder_train_text[0:10]","7753e731":"shape_of_decoder = decoder_train_text.shape","8c1e88aa":"np.amax(decoder_train_text)","c5a167b9":"shape_of_decoder[0]","de7ac9f0":"decoder_encoded_array = np.zeros((12000,11,5202))","66923d02":"for i in range(12000):\n    for j in range(11):\n        decoder_encoded_array[i,j,decoder_train_text[i,j]] =1 ","e646f1e4":"#decoder_input_array = np.zeros((8000,10,4063))","4e8115d5":"#for i in range(8000):\n #   for j in range(10):\n  #      decoder_input_array[i,j,french_train_text[i,j]] =1","9a6e95e7":"#encoder_input_array = np.zeros((8000,5,2056))","76e82c48":"#for i in range(8000):\n #   for j in range(5):\n  #      encoder_input_array[i,j,english_train_text[i,j]] =1","528606d3":"#decoder_encoded_array","188817b1":"from tensorflow.keras.layers import Dense,concatenate,Activation,Dropout,Input,LSTM,Embedding,Flatten,Conv1D,BatchNormalization\nfrom keras.models import Model","5dcd6c76":"encoder_input = Input(shape =(5,))#adding voc","20c8822f":"#encoder_embedding = Embedding(english_vocab,256)(encoder_input)\n#encoder_lstm,state_h,state_c = LSTM(256,return_state = True)(encoder_embedding)","3e29d129":"encoder_embedding = Embedding(english_vocab,256)(encoder_input)\nencoder = LSTM(256, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_embedding)","a52ed9a3":"encoder_states = [state_h,state_c]","22804dee":"state_h.shape","d4d2dd63":"decoder_input =Input(shape=(11,))","54ba6a39":"#decoder_embedding = Embedding(french_vocab,256)(decoder_input)\n#decoder_lstm = LSTM(256,return_sequences = True)(decoder_embedding , initial_state =encoder_states)\n#decoder_dense_output = Dense(french_vocab,activation ='softmax')(decoder_lstm)","bbf4ccfd":"decoder_embedding = Embedding(french_vocab,256)(decoder_input)\ndecoder_lstm = LSTM(256, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding,initial_state=encoder_states)\ndecoder_dense = Dense(french_vocab, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)","db42fa19":"model = Model([encoder_input,decoder_input],decoder_outputs)","535230d1":"model.summary()","f577aaa0":"import tensorflow as tf\ntf.keras.utils.plot_model(\nmodel,\nto_file=\"model1.png\",\nshow_shapes=False,\nshow_layer_names=True,\nrankdir=\"TB\",\nexpand_nested=False,\ndpi=96,\n)\n","6badad7d":"model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics = ['accuracy'])","f3f782f5":"batch_size =100\nepochs = 10","5cf1db3e":"model.fit([english_train_text, french_train_text], decoder_encoded_array,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_split = 0.2)","a431b800":"basic eda","251841c7":"# now instead of using characters we will be using word embeddings","32586603":"source : https:\/\/blog.keras.io\/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html","81cef4e3":"similarly we can get the sentence in french from english as we have done using decode_sequence fuction in characters representation"}}