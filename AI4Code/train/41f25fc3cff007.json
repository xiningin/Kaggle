{"cell_type":{"06a504dc":"code","e2a89f9a":"code","d8467bf8":"code","aab88910":"code","7e28157e":"code","a79b3094":"code","2ee5a912":"code","3568a27a":"code","f49e68f9":"code","b9d32b4c":"code","c677b405":"code","67010fb7":"code","23029528":"code","967a82cb":"code","952f1607":"code","09d3fafc":"code","86a079a6":"code","b81c6f49":"code","4ec2ae17":"code","62b92e84":"code","ae13913d":"code","80226d31":"code","0aa54968":"code","c61233c5":"code","e8336192":"code","74b2b6af":"code","1e30b243":"code","7f7c19a1":"code","83b2c301":"code","f1dd9831":"code","eae18696":"code","15bb598e":"code","8dfb7c7e":"code","53db8a20":"code","4160a1b9":"code","f54dde56":"code","a08bd651":"code","68483689":"code","36f577b4":"code","d6cc2ebc":"code","5553a370":"code","2f61c710":"markdown","d2006855":"markdown","3517ca5c":"markdown","628ac693":"markdown","3c9cc3e7":"markdown","2a6a45bd":"markdown","5387c164":"markdown","5492d739":"markdown","4faa0524":"markdown","2864c8db":"markdown","78993bd5":"markdown","6ccf9819":"markdown","2e8ae8ed":"markdown","fc2ce24a":"markdown","6ffabf48":"markdown","d3204040":"markdown","a66b3a63":"markdown","c8797bb4":"markdown","2881c14f":"markdown","ddc2d385":"markdown","0df630b7":"markdown","7602a5ba":"markdown","080fe164":"markdown","44a907db":"markdown","932e9bea":"markdown","4e2ad8cc":"markdown","4e65ee27":"markdown","03d989ee":"markdown","e40f434c":"markdown","1b880437":"markdown","a60d387d":"markdown","b4abc340":"markdown","7810a38d":"markdown","4d499270":"markdown","c74ff1c9":"markdown","6982867c":"markdown","5233538b":"markdown","a3a2372f":"markdown","20211d8e":"markdown","214b2b46":"markdown","7db6a1a5":"markdown","ec4b9836":"markdown","85bf2c77":"markdown","a00b84d2":"markdown","50397377":"markdown","dc269fb0":"markdown","00e69a14":"markdown"},"source":{"06a504dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime as dt\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix,f1_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e2a89f9a":"df = pd.read_csv('..\/input\/incident_event_logdata.csv',delimiter=',')","d8467bf8":"pd.set_option('max_rows', None)\npd.set_option('max_columns', None)\npd.set_option('max_colwidth', None)\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head(5)","aab88910":"# with the initial inspection, there are a lot of ? in data \ndf.replace('?', np.NaN, inplace = True)\ndf.info()","7e28157e":"# drop the columns that most values is nan\ndf1 = df.copy()\ndf1.drop(columns = ['cmdb_ci','problem_id','rfc','vendor','caused_by'], inplace = True)\n# remove impact and urgency, since Priority value is directly computed from them.\ndf1.drop(columns = ['impact','urgency'], inplace = True)","a79b3094":"# extract the numbers from the data \npattern = r'(\\d{1,4})'\ncolum = ['caller_id','opened_by','sys_created_by','sys_updated_by','location','category','subcategory','u_symptom','priority','assignment_group','assigned_to', 'closed_code', 'resolved_by']\nfor col in colum:\n    df1[col] = df1[col].str.extract(pattern)\n\n# time    \nfrom datetime import datetime, date\ntimeColum = ['opened_at', 'sys_created_at','sys_updated_at','resolved_at','closed_at']    \nfor col in timeColum:\n    df1[col] = pd.to_datetime(df1[col], format='%d\/%m\/%Y %H:%M',errors='coerce')","2ee5a912":"nRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","3568a27a":"for col in df1.columns:\n    print(col, df1[col].nunique())","f49e68f9":"# Distribution and correlation among columns\n#%matplotlib notebook\nidencolum = ['opened_by','sys_created_by','sys_updated_by','assignment_group','assigned_to','resolved_by']    \ndf_identify = df1.loc[:, idencolum]\nfor col in idencolum:\n    df_identify[col] = pd.to_numeric(df_identify[col], errors='coerce').fillna(0).astype(np.int64)\nplt.figure()\npd.plotting.scatter_matrix(df_identify,figsize=(12,12))\nplt.savefig(r\"Distribution and correlation among features.png\")","b9d32b4c":"# continue \nothercolum = ['reassignment_count','reopen_count','made_sla','category','priority','closed_code']\n\ndf_other = df1.loc[:, othercolum]\nfor col in othercolum:\n    df_other[col] = pd.to_numeric(df_other[col], errors='coerce').fillna(0).astype(np.int64)\nplt.figure()\npd.plotting.scatter_matrix(df_other,figsize=(12,12))\nplt.savefig(r\"Distribution and correlation among features_2.png\")","c677b405":"plt.figure()\nbins = np.arange(0,df1.incident_state.nunique()+2,1)\n\nax = df1.incident_state.hist(width =0.6,bins= bins,figsize=(6,4),align='mid')\nplt.xticks(rotation=45)\nax.grid(False)\nax.set_xticks(bins[:-1])\nax.set_ylabel('Numbers')\nax.set_title('Distribution of the incident_state')","67010fb7":"sla = (df1[(df1.made_sla == True) & (df1.reopen_count>0)].groupby('number')['reopen_count'].mean()).mean()\nnosla = (df1[(df1.made_sla == False) & (df1.reopen_count>0)].groupby('number')['reopen_count'].mean()).mean()\nprint(f'mean reopen_count for having SLA {sla} and without SLA {nosla}')","23029528":"# Distribution of closed_code; relationship between close_code and reopen_count\nimport seaborn as sns\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(12,4))\nbins=np.arange(0,df1.closed_code.nunique()+2,1)\ndf1[df1.incident_state=='Closed'].sort_values('closed_code').closed_code.hist(width =0.8,bins = bins,align='left',ax=ax1)\nax1.grid(False)\nax1.set_xticks(bins[:-1])\nax1.set_xlabel('closed code')\nax1.set_ylabel('Numbers')\nax1.set_title('Distribution of closed_code')\n\n\ndfclosecode = df1[(df1.reopen_count>0) & (df1.incident_state=='Closed')]\ndfclose_reopen = dfclosecode.groupby('closed_code').reopen_count.mean()\ndfclose_reopen.plot.bar(ax=ax2)\nax2.grid(False)\nax2.set_ylabel('mean of reopen_count')\nax2.set_xticks(bins[:-1])\nax2.set_title('closed_code vs. reopen_count')\nplt.show()","967a82cb":"df_ar = df1.loc[:,['assigned_to','resolved_by']]\ndf_ar['equal'] = np.where(df_ar.assigned_to == df_ar.resolved_by,1,0)\nequal_num = df_ar['equal'].sum()\nprint(equal_num\/df_ar.shape[0] * 100)","952f1607":"# completion time for incident resolution \ndf_closed = df1[df1.incident_state=='Closed'].reset_index()\ndf_closed['completion_time_days'] = (df_closed.closed_at- df_closed.opened_at).dt.total_seconds()\/3600\/24\n#print(f'The mean of completion time for incident resolution is {df_closed.completion_time_days.mean()} days.')\n\n#plots\nplt.figure()\nax = df_closed['completion_time_days'].plot(figsize=(24,4))\nax.grid(False)\nax.set_ylabel('completion time in days')\nax.set_title('Distribution of completion_time')","09d3fafc":"df_closed['completion_time_days'].describe()","86a079a6":"# completion time vs closed code; completion time vs made_sla\n\nplt.figure()\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(12,4))\ndf_closecode_time = df_closed.groupby('closed_code')['completion_time_days'].mean()\ndf_closecode_time.plot.bar(ax=ax1)\nax1.grid(False)\nax1.set_ylabel(' mean completion time in days')\nax1.set_title('completion time vs closed code')\n\ndf_made_sla_time = df_closed.groupby('made_sla')['completion_time_days'].mean()\ndf_made_sla_time.plot.bar(ax=ax2)\nax2.grid(False)\nax2.set_ylabel('mean completion time in days')\nax2.set_title('completion time vs made_sla')\nplt.show()","b81c6f49":"X = df1[['made_sla', 'caller_id', 'contact_type', 'location','category', 'subcategory','u_symptom']]\ny = df1.priority\n#X.head(2)","4ec2ae17":"for col in ['caller_id','location','category', 'subcategory','u_symptom']:\n    X.loc[:,col] = pd.to_numeric(X.loc[:,col], errors='coerce').fillna(0).astype(np.int64)\n\n# Label Encoding\nenc= LabelEncoder()\nfor col in ['made_sla', 'contact_type']:\n    X.loc[:,col] = enc.fit_transform(X.loc[:,col])\nX.head(2)\ny = pd.to_numeric(y, errors='coerce').fillna(0).astype(np.int64)","62b92e84":"# Splitting the data into test and train \nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=10)","ae13913d":"from sklearn.metrics import roc_curve, auc\n# baseline, accuracy and confusion matrix of predicting 3 for all incidents \nprint(f' The baseline of accuracy is {accuracy_score(y_test, np.full(y_test.shape, 3))}')\nprint(classification_report(y_test,np.full(y_test.shape, 3)))","80226d31":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\n\nmodel_xgb = XGBClassifier()\nmodel_xgb.fit(X_train, y_train)\ny_predict_xgb = model_xgb.predict(X_test)","0aa54968":"# Predicting the model\ny_predict_xgb = model_xgb.predict(X_test)\n# Finding accuracy, precision, recall and confusion matrix\nf1 = f1_score(y_test,y_predict_xgb,average='macro')\nprint(f'The macro F1 score for initial XGB model:{f1}')\nprint(classification_report(y_test,y_predict_xgb))","c61233c5":"from sklearn.utils import class_weight\nclass_weights = list(class_weight.compute_class_weight('balanced',np.unique(y_train), y_train))\n\nw_array = np.ones(y_train.shape[0], dtype = 'float')\nfor i, val in enumerate(y_train):\n    w_array[i] = class_weights[val-1]\n\nmodel_xgb_weight = XGBClassifier()   \nmodel_xgb_weight.fit(X_train, y_train,sample_weight=w_array)","e8336192":"# Predicting the model\ny_predict_xgb_weight = model_xgb_weight.predict(X_test) \nf1 = f1_score(y_test,y_predict_xgb_weight,average='macro')\nprint(f'The F1 score for XGB_weighted model:{f1}')\nprint(classification_report(y_test,y_predict_xgb_weight))","74b2b6af":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\npara_search ={'learning_rate':[0.2, 0.6, 1.2], 'n_estimators':[600, 800, 1200]}\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\ngrid_search = GridSearchCV(model_xgb_weight, param_grid = para_search, scoring='f1_macro',cv=kfold)\ngrid_search.fit(X_train, y_train,sample_weight=w_array)","1e30b243":"print(grid_search.best_params_)\ny_decision_fn = grid_search.predict(X_test) \nf1 = f1_score(y_test,y_decision_fn,average='macro')\nprint(f'The F1 score for XGB_weighted model after 1st tuning:{f1}')\nprint(classification_report(y_test,y_decision_fn))","7f7c19a1":"# plot feature importance\nfrom xgboost import plot_importance\nmodel_grid = grid_search.best_estimator_\n_ = plot_importance(model_grid, height = 0.9)\nprint(model_grid.feature_importances_)","83b2c301":"# Standardization technique\nsc = StandardScaler()\nX_train_svm = sc.fit_transform(X_train)\nX_test_svm = sc.transform(X_test)","f1dd9831":"#Initial model \nfrom sklearn.svm import SVC\nrbf_svc = SVC(kernel='rbf',C=10,gamma=0.1).fit(X_train_svm,y_train)","eae18696":"# create target\ndf_ar = df1[['assigned_to','resolved_by']]\ny2 = np.where(df_ar.assigned_to == df_ar.resolved_by,1,0)","15bb598e":"df_closed.head(2)","8dfb7c7e":"import matplotlib.dates as mdates\nfrom matplotlib.dates import MO, TU, WE, TH, FR, SA, SU\n# tick on sundays every week\nloc = mdates.WeekdayLocator(byweekday=SU)\n\nplt.figure()\ndf_opened_at_time = df_closed.groupby('opened_at')['completion_time_days'].mean()\naxt = df_opened_at_time.plot(figsize=(20, 4))\naxt.xaxis.set_minor_locator(loc)\naxt.set_ylabel('mean completion time in days')\naxt.set_xlabel('opened_at time')\nplt.show()","53db8a20":"df_closed['open_month'] = df_closed.opened_at.dt.month\ndf_closed['open_year'] = df_closed.opened_at.dt.year\ndf_closed['open_day'] = df_closed.opened_at.dt.day","4160a1b9":"plt.figure()\nfig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3,figsize=(15,3))\ndf_open_year_time = df_closed.groupby('open_year')['completion_time_days'].mean()\ndf_open_year_time.plot.bar(ax=ax1)\nax1.grid(False)\nax1.set_ylabel('Mean completion time in days')\nax1.set_xlabel('Year')\n\ndf_open_month_time = df_closed.groupby('open_month')['completion_time_days'].mean()\ndf_open_month_time.plot(ax=ax2)\nax2.grid(False)\nax2.set_xlabel('Month')\n\ndf_open_day_time = df_closed.groupby('open_day')['completion_time_days'].mean()\ndf_open_day_time.plot(ax=ax3)\nax3.grid(False)\nax3.set_xlabel('day')\nplt.show()","f54dde56":"# time difference between resolved_at and closed_at \ndf_closed['resolved_closed'] = ((df_closed.closed_at- df_closed.resolved_at).dt.total_seconds()\/3600\/24).fillna(0)\nprint(f'The mean time difference between resolved_at and closed_at is {df_closed.resolved_closed.mean()} days.')\n\n#plots\nplt.figure()\nax = df_closed['resolved_closed'].plot(figsize=(10,4))\nax.grid(False)\nax.set_ylabel('time difference in days')","a08bd651":"y3 = df_closed.completion_time_days\nX3 = df_closed[['category','subcategory', 'priority','caller_id','made_sla']]\n# Label Encoding\n# enc= LabelEncoder()\n# X3['incident_state'] = enc.fit_transform(X3['incident_state'])\nfor col in ['category','subcategory', 'priority','caller_id']:\n    X3[col] = pd.to_numeric(X3[col], errors='coerce').fillna(0).astype(np.int64)","68483689":"# Splitting the data into test and train \nX3_train, X3_test, y3_train, y3_test = train_test_split(X3,y3,test_size=0.3,random_state=10)","36f577b4":"import xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error\n\nxg_reg = xgb.XGBRegressor(max_depth = 10, min_child_weight=0.5, subsample = 1, eta = 0.3, num_round = 500, seed = 1)\nxg_reg.fit(X3_train, y3_train, eval_metric='mae')\ny3_preds = xg_reg.predict(X3_test)","d6cc2ebc":"y3_preds[y3_preds <0] = 0\nmae = mean_absolute_error(y3_test.values, y3_preds)\nprint(\"MAE: %f\" % (mae))","5553a370":"plt.figure(figsize=(12,4))\nplt.plot(y3_test.values,label=\"y3_test\")\nplt.plot(y3_preds,label=\"y3_preds\")\nplt.legend(loc='upper left') ","2f61c710":"With simple trials, the macro F1 score increases from 0.65 to 0.83. \n","d2006855":"<font size=\"6.5\">**Part 2**<\/font>","3517ca5c":"<font size=\"6.5\">**Part 1**<\/font>\n# **Exploratory Data Analysis**","628ac693":"<font size=\"4.5\">**2. Support Vector Machine**<\/font>\n\n(1) might be inproper since data size is large. scale feature is required; \n(2) with cost sensitive training: unequal costs for each class by setting class_weight : {dict, 'balanced'}. ","3c9cc3e7":"This figure indicates that:\n1. For closed_code of 13, the mean completion time is about 40 days, which is much larger than the others. This observation is unexpected since its reopen_count is 0 based on previous plot. \n2. The mean completion time for the instances without SLA is much longer than those having SLA. Therefore, making SLA would be helpful for enhancing the efficiency of incident management process.","2a6a45bd":"## Ideas","5387c164":"The attribute resolved_at is highly correlated withclosed_at, use it might cause data leakage. ","5492d739":"<font size=\"4\">3: Feature importance analysis<\/font>","4faa0524":"# **Data analysis**","2864c8db":"<font size=\"4.5\">**Idea 0: Dig into the cases that completion time is large**<\/font>","78993bd5":"Future work:\n1. continue tuning paras \n2. transfer to onevsall data, then use 'scale_pos_weight' to scale the class weights in the classifier.\n3. try roc_auc metric in the grid search.","6ccf9819":"From the figure, the incident can be in 7 different states as given time before it get closed. The number of 'New' and 'Active' is larger than the 'closed' case is because that the instance can be reopened in case incident resolution was rejected by the caller and in the data set there is a field i.e. reopen_count that captures the number of times got reopened.","2e8ae8ed":"**Import Raw Data**","fc2ce24a":"<font size=\"4.5\">**1.  relationship between made_sla and reopen_count**<\/font>","6ffabf48":"From this figure, it can be observed that:\n1. the dominated closed_code is 6\n2. the reopen_count is 0 for closed_code = 12,13,14,15.\n3. the mean reopen_count is large for closed_code 10, which means the instance resolution is easily rejected for this closed_code.","d3204040":"## Exploratory data analysis","a66b3a63":"<font size=\"4.5\">**3. Thoughts on future trials**<\/font>\n1. Bagging With Random Undersampling: BalancedBaggingClassifier\n2. Random Forest With Class Weighting; Random Forest With Random Undersampling.\n3. EasyEnsembleClassifier.","c8797bb4":"Comments based on this scatter matrix plot:\n1. the most of reassignment_count and reopen_count is 0. \n2. about 90% instance have SLA (service level agreement).\n3. Most cases are in priority 3, i.e. moderate. Therefore, the selection of the proper metrics is important for predicting the priority of incidents.\n4. the dominated closed_code is about 6. \n5. Data is significatnly imbalanced.","2881c14f":"## Feature selection and target creation\n\n1. First trial: 'incident_state', 'sys_mod_count', 'made_sla', 'caller_id', 'opened_by', 'sys_updated_by', 'location', 'category', 'subcategory','u_symptom', 'priority', 'assignment_group', 'knowledge', 'u_priority_confirmation'.\n\n2. create the target 'ident_equal' which specifies, if 'assigned to' was really responsible for the incident occurrence or not.   ","ddc2d385":"<font size=\"4.5\">**Idea 4: Mean encodings: mean completion time corresponding to the month it opened.**<\/font>","0df630b7":"The MAE is large but the plot seems not too bad. The large error comes from the prediction when the completion time is large.","7602a5ba":"<font size=\"4.5\">**Idea 2: Add the 'day' of the open time in a month**<\/font>","080fe164":"<font size=\"4.5\">**Idea 3: Add the 'day' of the open time in a week**<\/font>","44a907db":"## Distribution of columns","932e9bea":"## Data processing \/ Data manipulating","4e2ad8cc":"# 4. Predict the completion time for incident\n\n1. Regression problem.\n2. Each incident has a completion time, so it should be analyzed for each incident.\n3. use MAE as metrics. \n","4e65ee27":"**Recall from previous EDA:**\n(1) For closed_code of 13, the mean completion time is about 1000 hours, which is relative large compared to the others; \n(2) The mean completion time for the instances without SLA is much longer than those having SLA. Therefore, making SLA would be helpful for enhancing the efficiency of incident management process.","03d989ee":"# Introduction\n\nIncident management is a process for restoring the normal IT service as fast as possible, so that ensuring the lowest impact on the business and customers. Currently, the process is mostly manual rule-based and time-consuming, which can cause delays. Therefore, there is a need for tools assisting in the particular process activities in order to establish more effective process execution, and Machine learning looks prospective to improve the processes through prediction and automation.\n![Picture1.png](attachment:Picture1.png)\nThe main idea of this project is to leverage the publicly available Incident event log data from UCI ML repository and to build the predictive models based on the extracted information for assisting the operators during the incident management process. Specific aims are: \n\n1. Predict the priority of incidents, so that the operators can take preventive measures for High Priority incidents and reassigning can be reduced.\n2. Predict the closed code, then as soon ticket comes, the operators can know what it will take to close this incident and which automation process or team will be best to solve this in minimum required time ensuring best customer experience.\n3. Predict if the analyst that the incident was initially assigned to is actually the one can solve the incident. With the prediction, the Service Desk is expected to be more Effective.\n4. Predict the completion time for incident, so that the operators can be better prepared with resources and technology planning.\n\nAdditional information about the data: The event log was obtained from data gathered from the audit system of an instance of the ServiceNow platform used by an IT company. The data is composed by 24,918 cases (incidents) and 141,712 events extracted from Mar-2016 to Feb-2017 and it has 36 different features related to an incident.\n\n[http:\/\/archive.ics.uci.edu\/ml\/datasets\/Incident+management+process+enriched+event+log](http:\/\/) \n","e40f434c":"<font size=\"4.5\">**4. completion time for incident**<\/font>","1b880437":"Even the macro F1_score decreases, but the recall is improved a lot. Next, try to use gridsearch to tuning paras. ","a60d387d":"# 3. Predict equivalence of the analyst \u2018assigned to\u2019 and 'resolved by'\n\n1. bi-classfication problem.\n2. Each event need to be analyzed.\n3. Severe imbalanced data. \n4. Start with Random Forest and GBM, and use class weights. Use ROC_AUC as metrics.    ","b4abc340":"<font size=\"4\">0: initial model<\/font>","7810a38d":"From the plot of scatter matrix, it can be observed that:\n1. Identifier of the user who resolved the incident (resolved_by) is relative-uniformly distributed while the other features are inbalanced. \n2. It is clearly indicated that there is a linear correlation between 'opened_by' and 'sys_created_by': replicated information. \n3. A vague linear line is observed in the plot between 'assigned_to' and 'resolved_by', inferring that the correlation between these two features: some CI which was initially assigned to the incident is actually the one can reslove the incident.","4d499270":"## Specific Features Understanding","c74ff1c9":"Only 0.4% of the analyst that the incident was initially assigned to is actually the one can reslove it. Hence, an accurate prediction of 'resolved_by' can significantly improve the efficiency of the incidence management process.","6982867c":"<font size=\"4.5\">**0. incident_state**<\/font>\n","5233538b":"# 2. Predict the closed code\n\n1. closed code:identifier of the resolution of the incident. Hence, it should be analyzed for each incident. \n2. imbalanced data. 17 classes.\n3. multi-classfication problem. \n\n## Feature selection\n\nThe purpose of the prediction is the operators can know what it will take to close this incident as soon ticket comes. Therefore, the features which are available when the tickets arrives is reasonable for the prediction, which can be 'made_sla', 'caller_id', 'contact_type', 'location','category', 'subcategory','u_symptom'. \n","a3a2372f":"# 1. Predict the priority of incidents\n\n1. Multi-classfication problem.\n2. it is found that the priority can be different for the same incident, so every event should be analyzed. \n3. recall that most incidents (93%) has the priority of moderate. Hence, (1) sampling; (2) remedies for imbalance by selecting and adjusting model; (3) select proper metrics for evaluation:F1_score, AUC-ROC.  \n\n## Feature selection\n\nOnly the features which are available when the tickets arrives can be used to predict priority, which can be 'made_sla', 'caller_id', 'contact_type', 'location','category', 'subcategory','u_symptom'.\n","20211d8e":"<font size=\"4.5\">**3.  assigned to and resolved by**<\/font>","214b2b46":"<font size=\"4.5\">**Idea 1: Add the time difference between opened time and 2016-02-29**<\/font>","7db6a1a5":"1. mean completion time decrease with the time, so can create a feature measuring the days from 2016-02-29. \n2. clear pattern for every week.","ec4b9836":"## Models","85bf2c77":"## Quick Baseline with XGBoost regression \nHere, use the following features (Expert-Driven Selection) to make a quick baseline solution for the problem: 'category','subcategory', 'priority','caller_id','made_sla'.\n\nNote the target is 'completion time'.","a00b84d2":"<font size=\"4.5\">**2.  closed_code**<\/font>","50397377":"<font size=\"4\">1: Add class weights on the training data<\/font>","dc269fb0":"<font size=\"4\">2: Gridsearch<\/font>","00e69a14":"<font size=\"4.5\">**1. XGBoost Classfier**<\/font>\n\nTo deal with the class imbalance\n(1) cross-validate and perform grid-search to find the best parameter settings for the model;\n(2) balance class weights."}}