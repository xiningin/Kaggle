{"cell_type":{"5924ee0b":"code","b87274b7":"code","26fe6d25":"code","e520cc47":"code","9873a720":"code","f1d98108":"code","5eba1c5f":"code","554c09c6":"code","204df113":"code","02b9c67d":"code","79abf8f7":"code","b0f905c0":"code","5fa0a96c":"code","9cbcb1c4":"code","6373a190":"code","9394eb44":"code","9f9e5640":"code","a2c5a071":"code","2518990b":"code","a72be190":"code","349a8f17":"code","1bc9e3f9":"code","cd030bf5":"code","fbf1ec53":"code","116b8938":"code","20abe763":"markdown","51d8bde3":"markdown","d47e4e81":"markdown","5bca28e8":"markdown","11e3743e":"markdown","0b7fa76b":"markdown","00674ff5":"markdown","9d54fcc9":"markdown","3d88ec01":"markdown","26c1e173":"markdown","7ae1a358":"markdown","48b225f1":"markdown","d040d937":"markdown","443d2766":"markdown","ab55085b":"markdown","48011cd5":"markdown"},"source":{"5924ee0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b87274b7":"import matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport seaborn as sns\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport cv2\nimport os","26fe6d25":"labels = ['PNEUMONIA', 'NORMAL']\nimg_size = 150\ndef get_training_data(data_dir):\n    data = [] \n    for label in labels: \n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)","e520cc47":"train = get_training_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train')\ntest = get_training_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test')\nval = get_training_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val')","9873a720":"l = []\nfor i in train:\n    if(i[1] == 0):\n        l.append(\"Pneumonia\")\n    else:\n        l.append(\"Normal\")\n\nfrom collections import Counter\n\nCounter(l).keys() # equals to list(set(words))\nCounter(l).values() # counts the elements' frequency","f1d98108":"dataDist = {'Pneumonia': 3875, 'Normal': 1341}","5eba1c5f":"xrays=['Pneumonia', 'Normal']\nfig = go.Figure([go.Bar(x=xrays, y=[3875, 1341])])\nfig.update_layout(\n    autosize=False,\n    width=400,\n    height=500,\n    paper_bgcolor=\"white\")\nfig.show()","554c09c6":"plt.figure(figsize = (5,5))\nplt.imshow(train[0][0], cmap='gray')\nplt.title(labels[train[0][1]],fontsize=20, color=\"white\")\n\nplt.figure(figsize = (5,5))\nplt.imshow(train[-1][0], cmap='gray')\nplt.title(labels[train[-1][1]], fontsize=20, color=\"white\")","204df113":"x_train = []\ny_train = []\n\nx_val = []\ny_val = []\n\nx_test = []\ny_test = []\n\nfor feature, label in train:\n    x_train.append(feature)\n    y_train.append(label)\n\nfor feature, label in test:\n    x_test.append(feature)\n    y_test.append(label)\n    \nfor feature, label in val:\n    x_val.append(feature)\n    y_val.append(label)","02b9c67d":"train[:4]","79abf8f7":"# Normalize the data\nx_train = np.array(x_train) \/ 255\nx_val = np.array(x_val) \/ 255\nx_test = np.array(x_test) \/ 255","b0f905c0":"# resize data for deep learning \nx_train = x_train.reshape(-1, img_size, img_size, 1)\ny_train = np.array(y_train)\n\nx_val = x_val.reshape(-1, img_size, img_size, 1)\ny_val = np.array(y_val)\n\nx_test = x_test.reshape(-1, img_size, img_size, 1)\ny_test = np.array(y_test)","5fa0a96c":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip = False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)","9cbcb1c4":"model = Sequential()\nmodel.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (150,150,1)))\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Flatten())\nmodel.add(Dense(units = 128 , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\nmodel.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","6373a190":"history = model.fit(datagen.flow(x_train,y_train, batch_size = 32) ,epochs = 10 , validation_data = datagen.flow(x_val, y_val))","9394eb44":"print(\"Loss of the model is - \" , model.evaluate(x_test,y_test)[0]*100 , \"%\")\nprint(\"Accuracy of the model is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","9f9e5640":"epochs = [i for i in range(10)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\nax[0].set_title('Training & Validation Accuracy',fontsize=20, color=\"white\")\nax[0].legend()\nax[0].set_xlabel(\"Epochs\",fontsize=10, color=\"white\")\nax[0].set_ylabel(\"Accuracy\",fontsize=10, color=\"white\")\n\nax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\nax[1].set_title('Testing Accuracy & Loss',fontsize=20, color=\"white\")\nax[1].legend()\nax[1].set_xlabel(\"Epochs\",fontsize=10, color=\"white\")\nax[1].set_ylabel(\"Training & Validation Loss\",fontsize=10, color=\"white\")\nplt.show()","a2c5a071":"predictions = model.predict_classes(x_test)\npredictions = predictions.reshape(1,-1)[0]\npredictions[:15]","2518990b":"print(classification_report(y_test, predictions, target_names = ['Pneumonia (Class 0)','Normal (Class 1)']))","a72be190":"cm = confusion_matrix(y_test,predictions)\ncm","349a8f17":"cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])","1bc9e3f9":"plt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"coolwarm\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","cd030bf5":"correct = np.nonzero(predictions == y_test)[0]\nincorrect = np.nonzero(predictions != y_test)[0]","fbf1ec53":"i = 0\nfor c in correct[:6]:\n    plt.subplot(3,2,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(x_test[c].reshape(150,150), cmap=\"gray\", interpolation='none')\n    plt.title(\"Predicted Class {},Actual Class {}\".format(predictions[c], y_test[c]),color=\"white\")\n    plt.tight_layout()\n    i += 1","116b8938":"i = 0\nfor c in incorrect[:6]:\n    plt.subplot(3,2,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(x_test[c].reshape(150,150), cmap=\"gray\", interpolation='none')\n    plt.title(\"Predicted Class {},Actual Class {}\".format(predictions[c], y_test[c]),color=\"white\")\n    plt.tight_layout()\n    i += 1","20abe763":"# Data Augmentation","51d8bde3":"We perform a grayscale normalization to reduce the effect of illumination's differences.\n\nMoreover CNN converges faster on [0-1] data than on [0-255] data.","d47e4e81":"# Model Analysis","5bca28e8":"# Data Visualization","11e3743e":"Pneumonia is an inflammatory condition of the lung affecting primarily the small air sacs known as alveoli.\nSymptoms typically include some combination of productive or dry cough, chest pain, fever and difficulty breathing. The severity of the condition is variable.\n\nPneumonia is usually caused by infection with viruses or bacteria and less commonly by other microorganisms, certain medications or conditions such as autoimmune diseases.\nRisk factors include cystic fibrosis, chronic obstructive pulmonary disease (COPD), sickle cell disease, asthma, diabetes, heart failure, a history of smoking, a poor ability to cough (such as following a stroke), and a weak immune system.\nDiagnosis is often based on symptoms and physical examination.\nChest X-ray, blood tests, and culture of the sputum may help confirm the diagnosis.The disease may be classified by where it was acquired, such as community- or hospital-acquired or healthcare-associated pneumonia.","0b7fa76b":"# Dataset Description","00674ff5":"# Loading DataSet","9d54fcc9":"![image.png](attachment:image.png)","3d88ec01":"For the data augmentation, I choose to :-\n\n.\n\nRandomly rotate some training images by 30 degrees\n\nRandomly Zoom by 20% some training images\n\nRandomly shift images horizontally by 10% of the width\n\nRandomly shift images vertically by 10% of the height Once our model is ready, we fit the training dataset.","26c1e173":"# Correctly Predicted Classes","7ae1a358":"The dataset is organized into 3 folders **(train, test, val)** and contains subfolders for each image category (Pneumonia\/Normal). \n\nThere are **5,863 X-Ray images (JPEG) and 2 categories (Pneumonia\/Normal)**. \n\nChest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children\u2019s Medical Center, Guangzhou. \nAll chest X-ray imaging was performed as part of patients\u2019 routine clinical care. For the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. \n\nThe diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.","48b225f1":"# Importing Libraries","d040d937":"In order to avoid overfitting problem, we need to expand artificially our dataset. \nWe can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations. \nApproaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. \nSome popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more. \nBy applying just a couple of these transformations to our training data, we can easily double or triple the number of training examples and create a very robust model.","443d2766":"# Incorrectly Predicted Classes","ab55085b":"# What is Pneumonia & How to identify the difference?","48011cd5":"# Comparing the two different images - Pneumonia & Normal"}}