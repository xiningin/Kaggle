{"cell_type":{"56a09717":"code","45baa410":"code","6f6dd880":"code","f4549db9":"code","3205e88a":"code","2625cfc2":"code","fa37bb6f":"code","b5668ff1":"code","61728349":"code","d8ba670a":"code","8eeb4330":"code","ae13385d":"code","add6bac9":"code","cccf4d02":"code","54882864":"code","e5b303d6":"code","b9183528":"code","90e20c60":"code","86efb28e":"code","e558f287":"code","2a9812ce":"code","4817c34d":"code","48b5145c":"code","45374aa9":"code","73c2db28":"code","53f62ca6":"code","0a0a814f":"markdown","17b6be41":"markdown","277fd655":"markdown","8ff8e6d4":"markdown","15492fef":"markdown","802d6d8e":"markdown","0260b90a":"markdown"},"source":{"56a09717":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","45baa410":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n","6f6dd880":"df = pd.read_csv(\"..\/input\/auto-preise-ml-ue-1\/CarPrice_Assignment.csv\", index_col=\"car_ID\")\ndf.head()","f4549db9":"data = df[['curbweight', 'enginesize', 'highwaympg', 'horsepower', 'citympg', 'peakrpm', 'price']]\ndata.head()","3205e88a":"from pandas.plotting import scatter_matrix\nimport seaborn as sn \n\nsn.heatmap(data.corr(),annot=True)\n","2625cfc2":"scatter_matrix(data , figsize=(12,8))","fa37bb6f":"from sklearn.model_selection import train_test_split\nX = data[['curbweight', 'enginesize', 'highwaympg', 'horsepower', 'citympg', 'peakrpm']]\ny = data.price\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)","b5668ff1":"\ndf.plot(kind='scatter',x='curbweight',y='price',color='red')\ndf.plot(kind='scatter',x='enginesize',y='price',color='blue')\ndf.plot(kind='scatter',x='horsepower',y='price',color='green')\n\n","61728349":"from sklearn import preprocessing","d8ba670a":"X=preprocessing.scale(X)","8eeb4330":"linear_classifier=LinearRegression()\nlinear_classifier.fit(X_train, y_train)\n","ae13385d":"score=linear_classifier.score(X_train,y_train)\nscore","add6bac9":"from sklearn.model_selection import cross_val_score\ncross_val_score(linear_classifier,X_train,y_train)\n","cccf4d02":"np.mean(cross_val_score(linear_classifier,X_train,y_train))","54882864":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_reg = Pipeline([\n    (\"poly_features\", PolynomialFeatures()), # Ersetze `...` mit dem richtigen Code.\n    (\"lin_reg\", linear_classifier)\n])","e5b303d6":"from sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(poly_reg, {\"poly_features__degree\": [1, 2, 3]})\ngrid_search.fit(X_train, y_train)","b9183528":"cross_val_score(grid_search,X_train,y_train)\nnp.mean(cross_val_score(grid_search,X_train,y_train))","90e20c60":"cross_val_score(poly_reg,X_train,y_train)\nnp.mean(cross_val_score(poly_reg,X_train,y_train))","86efb28e":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge","e558f287":"poly_reg2 = Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree=1)), # Ersetze `...` mit dem richtigen Code.\n    (\"ridge-reg\", Ridge(normalize=True))\n])","2a9812ce":"grid_search2=GridSearchCV(poly_reg2,{\"ridge-reg__alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10]})\ngrid_search2.fit(X_train,y_train)\nbest_grid=grid_search2.best_params_\nbest_grid\n","4817c34d":"np.mean(cross_val_score(grid_search2.best_estimator_,X_train,y_train))","48b5145c":"from sklearn.linear_model import SGDRegressor\n\nelastic_net = Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree=4)),\n    (\"std_scaler\", StandardScaler()),\n    (\"elastic_net\", SGDRegressor(penalty='elasticnet', max_iter=2000, eta0=0.01))  \n])\ngrid_search3 = GridSearchCV(elastic_net, {\"elastic_net__alpha\":[0.0001, 0.001, 0.01, 0.1, 1, 10,100,1000], \"elastic_net__l1_ratio\":[0,0.0001,0.001,0.01,0.1,1] }, cv=10,)\n\n\ngrid_search3.fit(X_train, y_train)\nprint(f\"best parameter {grid_search3.best_params_}\")","45374aa9":"\npd.DataFrame(grid_search3.cv_results_).sort_values(\"rank_test_score\").head()\n\n","73c2db28":"\n\nsgd = Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree=4)),\n    (\"std_scaler\", StandardScaler()),\n    (\"sgd\", SGDRegressor(max_iter=3000,alpha=0.001, l1_ratio=0.01, early_stopping=True, n_iter_no_change=20))  \n])\ngrid_search4 = GridSearchCV(sgd, {\"sgd__eta0\":[0.0001, 0.001,0.005,0.001, 0.01,0.05,0.04, 0.1, 1]}, cv=10,)\n\n\ngrid_search4.fit(X_train, y_train)\nprint(f\"best parameter {grid_search4.best_params_}\")","53f62ca6":"pd.DataFrame(grid_search4.cv_results_).sort_values(\"rank_test_score\").head()","0a0a814f":"# **2d) Elastic Net**","17b6be41":"# **2e) Early Stopping**","277fd655":"# **2b. Polynomregression**","8ff8e6d4":"# **Setup**","15492fef":"# **2a) Lineare Regression**","802d6d8e":"# **2c) Ridge Regression**","0260b90a":"Hello. I believe the thing is:\n\n    you train one model logreg.train(X_train,Y_train)\n\n    the resulting model does not explain the training data 100% well (that would probably be overfitting).\n\n    if you use now the X_train to make predictions you will not get exactly Y_train, but Y_train', different somehow Y_train' = logreg.predict(X_train)\n\n    logreg.score(X_train,Y_train) is calculating the difference between Y_train and Y_train' (an accuracy measure), but you did not need to explicitly calculate Y_train'. The library does this internally.\n\nIf you try this (once the model trained with the train data):\n\n    Y_pred = logreg.predict(X_test)\n\n    logreg.score(X_test,Y_pred)\n\nthis score will always give 1.0\n\n(because it compares Y_pred' (which the library calculates internally as Y_pred'= logreg.predict(X_test) ) with Y_pred; but Y_pred is also logreg.predict(X_test), because the code we wrote)\n\nif Y_test is the real labels for X_test\n\n    logreg.score(X_test, Y_test) \n\nis comparing the predictions of the model against the real labels.\n\nIn other words:\n\nA. predictor.score(X,Y) internally calculates Y'=predictor.predict(X) and then compares Y' against Y to give an accuracy measure. This applies not only to logistic regression but to any other model.\n\nB. logreg.score(X_train,Y_train) is measuring the accuracy of the model against the training data. (How well the model explains the data it was trained with). <-- But note that this has nothing to do with test data.\n\nC. logreg.score(X_test, Y_test) is equivalent to your print(classification_report(Y_test, Y_pred)). But you do not need to calculate Y_pred; that is done internally by the library\n\nI believe is this. Hope you find my answer helpful."}}