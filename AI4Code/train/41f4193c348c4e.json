{"cell_type":{"2eb4424c":"code","2f0bcc64":"code","1a9b0e8b":"code","fa64a9d8":"code","2608b208":"code","9d78b35d":"code","fbc2bc14":"code","2d37c3ed":"code","46c5a7db":"code","0aa44180":"code","0074d802":"code","210fae27":"code","44ed650b":"code","085c46fd":"code","b2964894":"code","059e3067":"code","f4c944ba":"code","ea169406":"code","3da8bebf":"code","6da86eec":"code","541d5477":"code","28f53aba":"code","c76e0454":"code","0aeb03d7":"markdown","fab03885":"markdown","561766cb":"markdown","3ea6be46":"markdown","c3383eb8":"markdown","891cb50c":"markdown","434efc95":"markdown","a701bd00":"markdown","30ce7bb6":"markdown","99c95293":"markdown","0672e57f":"markdown","aa687978":"markdown","607cee52":"markdown","6e175d4f":"markdown","fdbcf1b6":"markdown","37934aac":"markdown","e4188ebc":"markdown"},"source":{"2eb4424c":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\nfrom tqdm import tqdm\nimport os","2f0bcc64":"path = \"\/kaggle\/input\/aclimdb\/aclImdb\/\"\npositiveFiles = [x for x in os.listdir(path+\"train\/pos\/\") if x.endswith(\".txt\")]\nnegativeFiles = [x for x in os.listdir(path+\"train\/neg\/\") if x.endswith(\".txt\")]\ntestFiles = [x for x in os.listdir(path+\"test\/\") if x.endswith(\".txt\")]","1a9b0e8b":"positiveReviews, negativeReviews= [], []\nfor pfile in positiveFiles:\n    with open(path+\"train\/pos\/\"+pfile, encoding=\"latin1\") as f:\n        positiveReviews.append(f.read())\nfor nfile in negativeFiles:\n    with open(path+\"train\/neg\/\"+nfile, encoding=\"latin1\") as f:\n        negativeReviews.append(f.read())\n","fa64a9d8":"reviews = pd.concat([\n    pd.DataFrame({\"review\":positiveReviews, \"label\":1, \"file\":positiveFiles}),\n    pd.DataFrame({\"review\":negativeReviews, \"label\":0, \"file\":negativeFiles}),\n], ignore_index=True).sample(frac=1, random_state=1)\nreviews.head()","2608b208":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","9d78b35d":"stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","fbc2bc14":"from bs4 import BeautifulSoup\ni = 0\nfrom tqdm import tqdm\npreprocessed_reviews = []\npositive_word = []\nnegative_word = []\n","2d37c3ed":"class Preprocess_text():\n    \n    \n    def decontracted(self,phrase):\n        self.phrase =phrase\n        phrase = re.sub(r\"won't\", \"will not\", phrase)\n        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n        # general\n        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n        return phrase\n\n    def fit(self,text):\n        self.text = text\n        for sentance in tqdm(text):\n            sentance = re.sub(r\"http\\S+\", \"\", sentance)\n            sentance = BeautifulSoup(sentance, 'lxml').get_text()\n            sentence = decontracted(sentance)\n            sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n            sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n            # https:\/\/gist.github.com\/sebleier\/554280\n            sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n            preprocessed_reviews.append(sentance.strip())\n       \n        return preprocessed_reviews\n\n    \n    \n       ","46c5a7db":"preprocess = Preprocess_text()","0aa44180":"preprocess_review = preprocess.fit(reviews['review'].values)","0074d802":"#BoW\ncount_vect = CountVectorizer() #in scikit-learn\ncount_vect.fit(preprocess_review)\n\n\nprint(\"some feature names \", count_vect.get_feature_names()[:10])\nprint('='*50)\n\nfinal_counts = count_vect.transform(preprocess_review)\nprint(\"the type of count vectorizer \",type(final_counts))\nprint(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\nprint(\"the number of unique words \", final_counts.get_shape()[1])","210fae27":"#bi-gram, tri-gram and n-gram\n\n#removing stop words like \"not\" should be avoided before building n-grams\n# count_vect = CountVectorizer(ngram_range=(1,2))\n# please do read the CountVectorizer documentation http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html\n\n# you can choose these numebrs min_df=10, max_features=5000, of your choice\n\n# min_df means minimum document freq will be 10 means at least 10 times if word present thne only consider\ncount_vect = CountVectorizer(ngram_range=(1,2), min_df=10)\nfinal_bigram_counts = count_vect.fit_transform(preprocess_review)\nprint(\"the type of count vectorizer \",type(final_bigram_counts))\nprint(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])\n\n","44ed650b":"x = final_bigram_counts\n\ny = reviews['label'].values\n\nfrom sklearn.model_selection import train_test_split\n\n#split dataset into train and test data\nX_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1, stratify=y)\n","085c46fd":"print(preprocess.uniqueStems.shape)\npreprocess.uniqueStems[preprocess.uniqueStems.word.str.contains(\"disappoint\")]","b2964894":"\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\ndef k_classifier_brute(X_train, y_train):\n    # creating odd list of K for KNN\n    myList = list(range(0,40))\n    neighbors = list(filter(lambda x: x % 2 != 0, myList))\n\n    # empty list that will hold cv scores\n    cv_scores = []\n\n    # perform 10-fold cross validation\n    for k in tqdm(neighbors):\n        knn = KNeighborsClassifier(n_neighbors=k, algorithm = \"brute\")\n        scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n        cv_scores.append(scores.mean())\n\n    # changing to misclassification error\n    MSE = [1 - x for x in cv_scores]\n\n    # determining best k\n    optimal_k = neighbors[MSE.index(min(MSE))]\n    print('\\nThe optimal number of neighbors is %d.' % optimal_k)\n\n    # plot misclassification error vs k \n    plt.plot(neighbors, MSE)\n\n    for xy in zip(neighbors, np.round(MSE,3)):\n        plt.annotate('(%s, %s)' % xy, xy=xy, textcoords='data')\n    plt.title(\"Misclassification Error vs K\")\n    plt.xlabel('Number of Neighbors K')\n    plt.ylabel('Misclassification Error')\n    plt.show()\n\n    print(\"the misclassification error for each k value is : \", np.round(MSE,3))\n    return optimal_k","059e3067":"optimal_k_bow = k_classifier_brute(X_train, y_train)\n\n","f4c944ba":"knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k_bow)\n\n# fitting the model\nknn_optimal.fit(X_train, y_train)\n#knn_optimal.fit(bow_data, y_train)\n\n# predict the response\npred = knn_optimal.predict(x_test)\n\nprint(X_train.get_shape())\n\n","ea169406":"train_acc_bow = knn_optimal.score(X_train, y_train)\nprint(\"Train accuracy\", train_acc_bow)\n\nfrom sklearn.metrics import accuracy_score\nacc_bow = accuracy_score(y_test, pred) * 100\nprint('\\nThe accuracy of the knn classifier on test data for k = %d is %f%%' % (optimal_k_bow, acc_bow))","3da8bebf":"N = [\"I saw movie it was very Great movie i am very happy must watch movie\"]\ntest_vectors = count_vect.transform(N)\n\ntest_vectors.get_shape()\n\nprediction = knn_optimal.predict(test_vectors)\n\n\ny_score = knn_optimal.predict_proba(test_vectors)\n\nneg_prob = str(y_score[0][0]*100)\npos_prob = str(y_score[0][1]*100)\n\nif prediction == 0:\n    print(\"Negative review with PRobability : \"+neg_prob)\nelse:\n    print(\"Positive review with probability : \"+pos_prob)\n","6da86eec":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\ntf_idf_vect.fit(preprocess_review)\nprint(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\nprint('='*50)\n\nfinal_tf_idf = tf_idf_vect.transform(preprocess_review)\nprint(\"the type of count vectorizer \",type(final_tf_idf))\nprint(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])","541d5477":"x = final_tf_idf\n\n\nfrom sklearn.model_selection import train_test_split\n\n#split dataset into train and test data\nX_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1, stratify=y)\n","28f53aba":"# Please write all the code with proper documentation\nknn_optimal = KNeighborsClassifier(n_neighbors=optimal_k_bow)\n\n# fitting the model\nknn_optimal.fit(X_train, y_train)\n#knn_optimal.fit(bow_data, y_train)\n\n# predict the response\npred = knn_optimal.predict(x_test)\n\nprint(X_train.get_shape())\n\n\n\ntrain_acc_bow = knn_optimal.score(X_train, y_train)\nprint(\"Train accuracy\", train_acc_bow)\n\nfrom sklearn.metrics import accuracy_score\nacc_bow = accuracy_score(y_test, pred) * 100\nprint('\\nThe accuracy of the knn classifier for test data with k = %d is %f%%' % (optimal_k_bow, acc_bow))","c76e0454":"N = [\"I saw movie it was very Great movie i am very happy must watch movie, acting was fantastic\"]\ntest_vectors = count_vect.transform(N)\n\ntest_vectors.get_shape()\n\nprediction = knn_optimal.predict(test_vectors)\ny_score = knn_optimal.predict_proba(test_vectors)\n\nprint(y_score)\nneg_prob = str(y_score[0][0]*100)\npos_prob = str(y_score[0][1]*100)\n\nif prediction == 0:\n    print(\"Negative review with PRobability : \"+neg_prob)\nelse:\n    print(\"Positive review with probability : \"+pos_prob)\n","0aeb03d7":"# [4.1] BAG OF WORDS\n","fab03885":"# Sentiment Analysis on Movie Reviews","561766cb":"In this notebook Sentiment Analysis is performed on movie reviews.\n\n---","3ea6be46":"**With everything centralized in 1 dataframe, we now perform train, validation and test set splits.","c3383eb8":"---\n\n## Data Preprocessing\nThe next step is data preprocessing. The following class behaves like your typical SKLearn vectorizer.\n\nIt can perform the following operations.\n* Discard non alpha-numeric characters\n* Set everything to lower case\n* Stems all words using PorterStemmer, and change the stems back to the most occurring existent word.\n* Discard non-Egnlish words (not by default).","891cb50c":"#  See on test data we get very bad accuracy on BOW \n\nLets try to predict our own movie review lets what our model will predict .","434efc95":"<h1>Content<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Import\" data-toc-modified-id=\"Data-Import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Data Import<\/a><\/span><\/li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span><\/li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Feature Engineering<\/a><\/span><\/li><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Feature Selection<\/a><\/span><\/li><li><span><a href=\"#Model-Architecture\" data-toc-modified-id=\"Model-Architecture-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Model Architecture<\/a><\/span><\/li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Model Training<\/a><\/span><\/li><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Model Evaluation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy-&amp;-Loss\" data-toc-modified-id=\"Accuracy-&amp;-Loss-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Accuracy &amp; Loss<\/a><\/span><\/li><li><span><a href=\"#Error-Analysis\" data-toc-modified-id=\"Error-Analysis-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>Error Analysis<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Application\" data-toc-modified-id=\"Model-Application-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Model Application<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Test-Predictions\" data-toc-modified-id=\"Test-Predictions-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>Test Predictions<\/a><\/span><\/li><li><span><a href=\"#Custom-Reviews\" data-toc-modified-id=\"Custom-Reviews-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Custom Reviews<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","a701bd00":"#  See TF-IDF give 80% acc and BOW gives 66% acc Hence we can surely say TF-IDF works far better than BOW ","30ce7bb6":"# Here we get little improvment in model performance \n\n<h4>So we can say TF-IDF works better than BOW<\/h4>","99c95293":"# Lets Try TF-TDF ","0672e57f":"#  Split our data in train test \n","aa687978":"---\n\n## Model Application","607cee52":"---\n\n## Data Import\nFirst, we need to import the data.","6e175d4f":"# We already find best k for BOW so we will use k=39 and try to fit model with tf-idf\n","fdbcf1b6":"# See our model predicted correctly but probability is very less 48% only model confident that review is positive  ","37934aac":"#  Applying KNN brute force on BOW\n\nSo for knn we need to find best K then only we can fit our model so to find best K we need to do Cross validation \nBelow code is cross validation to find best K for given data ","e4188ebc":"#  Bi-Grams and n-Grams."}}