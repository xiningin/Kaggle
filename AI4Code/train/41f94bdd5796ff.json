{"cell_type":{"cee7c455":"code","d0b2aeee":"code","1d43905b":"code","97e6053e":"code","410fea8b":"code","ce25ba7d":"code","f948a293":"code","db93b026":"code","b175758e":"code","3f856aa6":"code","a60e2540":"code","58c94328":"code","e61aade6":"code","997db671":"code","7f7d1dde":"code","fda73cd5":"code","04bb1c63":"code","07d9299f":"code","301b66ae":"code","678442ad":"code","ec3565d6":"code","b76e16fd":"code","9a7f8f87":"code","66e571b3":"code","73842de5":"code","aa261270":"code","ef2dd8f9":"code","9c203ad1":"code","b6c5a36b":"code","1b495668":"code","cc2345da":"code","8d0f8f1c":"code","24cb6d8a":"code","0418de41":"code","329eb6b2":"code","06cb9d92":"code","75a6d60c":"code","cf955894":"code","678d398c":"code","9091229f":"code","6de6913d":"code","0b45707d":"code","2392e9f3":"code","2d492955":"code","ead9751d":"code","9703b029":"code","08ebd1ca":"code","04b17f52":"code","3f48e880":"code","2fa43bf6":"code","4fd2b24c":"code","5d1abd02":"code","5e271b0b":"code","e181f525":"markdown","e82ba2be":"markdown","2f7f867f":"markdown","5cedf0b2":"markdown","cc612000":"markdown","1bd7c925":"markdown","22c2922b":"markdown","27b578b8":"markdown","c440d973":"markdown","86084754":"markdown"},"source":{"cee7c455":"import tensorflow as tf\nprint(tf.__version__)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# split training and testing\nfrom sklearn.model_selection import train_test_split\n\n# Import metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n\n# check GPU \n!nvidia-smi\n","d0b2aeee":"train_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntrain_df.head()","1d43905b":"test_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntest_df.head()","97e6053e":"# sample_submission.csv\nsample_submission =pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission.head()","410fea8b":"# Check any null values\ntrain_df.isna().sum()","ce25ba7d":"# Value counts \ntrain_df['target'].value_counts()","f948a293":"train_df_shuffled = train_df.sample(frac=1, random_state=42)\ntrain_df_shuffled","db93b026":"train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled['text'].to_numpy(),\n                                                                           train_df_shuffled['target'].to_numpy(),\n                                                                           test_size=0.3,\n                                                                           random_state=42)\nlen(train_sentences), len(val_sentences), len(train_labels), len(val_labels)","b175758e":"# Average tokens\nmax_vocab_length =round(sum([len(i.split()) for i in train_sentences])\/len(train_sentences))\nmax_vocab_length","3f856aa6":"# Create Tokenization Layers\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\nmax_tokens = 10000\n\ntext_vectorization = TextVectorization(max_tokens=max_tokens,\n                                      output_mode='int',\n                                      output_sequence_length=max_vocab_length)\n\n# fix the text vectorizer to the training set\ntext_vectorization.adapt(train_sentences)","a60e2540":"# Check with sample_sentences\nsample_sentences = \"I'm in love with the shape of you We push and pull like a magnet do\"\ntext_vectorization([sample_sentences])","58c94328":"# Check with random train sentences\nimport random\nrandom_sentences  = random.choice(train_sentences)\n\nprint(f\"Original Sentence : \\n {random_sentences}\\\n      \\n\\nText_Vectorization : \")\ntext_vectorization([random_sentences])\n","e61aade6":"# Create an Embedding Layer\nfrom tensorflow.keras import layers\n\nembedding = layers.Embedding(input_dim=max_tokens,\n                             output_dim=128,\n                             embeddings_initializer='uniform',\n                             input_length=max_vocab_length)","997db671":"#check random\nrandom_sentences = random.choice(train_sentences)\nprint(f\"Original Sentences : \\n{random_sentences}\\\n     \\n\\nEmbeddings : \")\nembedding(text_vectorization([random_sentences]))","7f7d1dde":"# Early Stopping\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ncallbacks = EarlyStopping(monitor='val_binary_crossentropy', \n                             patience=3)","fda73cd5":"# Using Long-Short Term Memory (LSTM)\n\n# Pass the input layers\ninputs = layers.Input(shape=(1,), dtype='string',name='input_shape')\n\n# Pass the inputs to text vectorization layer\nx = text_vectorization(inputs)\n\n# Pass the text vectorization layer to embeddings\nx = embedding(x)\n\n\n# Build a model\n\n# return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n# LSTM\nx = layers.LSTM(units=32)(x)\n\n#\n\n# Output Layer\noutputs = layers.Dense(1, activation='sigmoid', name='output_layer')(x)\n\n# Pass the inputs and outputs to model\nmodel = tf.keras.Model(inputs, outputs, name='model')\n\n# Compile the model\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n             optimizer=tf.keras.optimizers.Adam(),\n             metrics=['accuracy']\n             )\n\n# model summary\nmodel.summary()","04bb1c63":"# fit the model\nhistory = model.fit(train_sentences, \n                   train_labels,\n                   epochs=5,\n                   validation_data=(val_sentences, val_labels))","07d9299f":"# plot loss curves\ndef plot_loss_curves(history):\n    \n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs = range(len(history.history['loss']))\n    \n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']\n    \n    plt.title(\"Loss\")\n    plt.plot(epochs, loss, label='loss')\n    plt.plot(epochs, val_loss, label='val_loss')\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n    \n    plt.figure()\n    plt.title(\"Accuracy\")\n    plt.plot(epochs, accuracy, label='accuracy')\n    plt.plot(epochs, val_accuracy, label='val_accuracy')\n    plt.xlabel(\"Epochs\")\n    plt.legend()","301b66ae":"plot_loss_curves(history=history)","678442ad":"# Evaluate model\nmodel.evaluate(val_sentences, val_labels)","ec3565d6":"# preds_probs\nmodel_pred_probs = model.predict(val_sentences)\nmodel_pred_probs","b76e16fd":"# predictions\nmodel_preds = tf.squeeze(tf.round(model_pred_probs))\nmodel_preds","9a7f8f87":"# evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef calculate_results(y_true, y_preds):\n    \n    # model_accuracy\n    model_accuracy = accuracy_score(y_true, y_preds)* 100\n    # calculate model precision, recall and f1 score using \"weighted\" average\n    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_preds, average=\"weighted\")\n    model_results = {\"accuracy\": model_accuracy,\n                    \"precision\": model_precision,\n                    \"recall\": model_recall,\n                    \"f1\": model_f1}\n    return model_results","66e571b3":"model_1_results = calculate_results(val_labels, model_preds)\nmodel_1_results","73842de5":"# Dense model\ninputs = tf.keras.Input(shape=(1,), dtype=\"string\", name=\"inputs\") # inputs\nx = text_vectorization(inputs) # text_vectorization layer to inputs\nx = embedding(x) # pass both text_vectorization and inputs to our embeddings layer\nx = layers.GlobalAveragePooling1D()(x) # pooling layer\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"outputs\")(x)# outputs\nmodel_2 = tf.keras.Model(inputs, outputs, name=\"model2\") # Build dense model\nmodel_2.compile(loss=\"binary_crossentropy\", # Compile the model\n               optimizer=\"adam\",\n               metrics=['accuracy'])\nmodel_2.summary()","aa261270":"history = model_2.fit(train_sentences, train_labels,\n                     epochs=5,validation_data=(val_sentences, val_labels))","ef2dd8f9":"# plot loss curves\nplot_loss_curves(history)","9c203ad1":"# Evaluate model\nmodel_2.evaluate(val_sentences, val_labels)","b6c5a36b":"# Get prediction_probabilities\nmodel_2_pred_probs = model_2.predict(val_sentences)\nmodel_2_pred_probs","1b495668":"# Get the predictions\nmodel_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\nmodel_2_preds","cc2345da":"# Calculate metrics\nmodel_2_results = calculate_results(val_labels, model_2_preds)\nmodel_2_results","8d0f8f1c":"# Let's use transfer learning\n# we can use this encoding layer in place of our text_vectorizer and embedding_layer\n# we will be using bert model from tensorflow hub\n\n# import tensorflow hub\nimport tensorflow_hub as hub\n\nsentence_encoder_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\",\n                                       input_shape=[], # shape of inputs coming to our model\n                                       dtype=tf.string,\n                                       trainable=False)","24cb6d8a":"# Create model using the Sequential API\nmodel_3 = tf.keras.Sequential([\n    sentence_encoder_layer,\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n],name='USE')\n\n# Compile model\nmodel_3.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n\nmodel_3.summary()","0418de41":"history_3 = model_3.fit(train_sentences,\n                       train_labels,\n                       epochs=5,\n                       validation_data=(val_sentences, val_labels))\n","329eb6b2":"# plot_loss_curves\nplot_loss_curves(history_3)","06cb9d92":"# make prediction probabilities\nmodel_3_prediction_probs = model_3.predict(val_sentences)\nmodel_3_prediction_probs[:10]","75a6d60c":"# prediction \nmodel_3_preds = tf.squeeze(tf.round(model_3_prediction_probs))\nmodel_3_preds[:10]","cf955894":"# Calculate results\nmodel_3_results = calculate_results(val_labels, model_3_preds)\nmodel_3_results","678d398c":"# Combine model results into a DataFrame\nmodel_results = pd.DataFrame({\"model_1\": model_1_results,\n                             \"model_2\": model_2_results,\n                             \"model_3\":model_3_results})\nmodel_results = model_results.transpose()\nmodel_results","9091229f":"# change the accuracy to same scale as other metrics\nmodel_results['accuracy'] = model_results['accuracy']\/100\n","6de6913d":"# plot and compare\nmodel_results.plot(kind='bar', figsize=(10,5)).legend(bbox_to_anchor=(1.0, 1.0))","0b45707d":"# Check which model got more accuracy\nmodel_results.sort_values(\"accuracy\", ascending=False)['accuracy'].plot(kind='bar', figsize=(10,5))","2392e9f3":"\n\n# precision\nmodel_results.sort_values('precision', ascending=False)['precision'].plot(kind='bar', figsize=(10,5))","2d492955":"# recall\nmodel_results.sort_values(\"recall\", ascending=False)['recall'].plot(kind='bar', figsize=(10,5))","ead9751d":"# f1\nmodel_results.sort_values('f1',ascending=False)['f1'].plot(kind='bar', figsize=(10,5))","9703b029":"test_sentences = test_df['text'].to_list()\ntest_sentences[:10]","08ebd1ca":"# Making predictions on the test dataset\n\n# Keep all the sentences in a list\ntest_sentences = test_df[\"text\"].to_list()\n# take random sample from the list upto 10 samples\ntest_samples = random.sample(test_sentences, 10)\n# loop through to test_samples\nfor test_sample in test_samples:\n    pred_prob = tf.squeeze(model_3.predict([test_sample])) # has to be list\n    pred = tf.round(pred_prob)\n    print(f\"Pred: {int(pred)}, Prob: {pred_prob}\")\n    print(f\"Text:\\n{test_sample}\\n\")\n    print(\"----\\n\")\n\n","04b17f52":"prediction_probs  = model_3.predict([test_df['text']])\nprediction_probs","3f48e880":"predictions = tf.squeeze(tf.round(prediction_probs))\npredictions","2fa43bf6":"test_df","4fd2b24c":"submission = pd.DataFrame()\nsubmission['id'] = test_df['id']\nsubmission['target'] = predictions","5d1abd02":"submission","5e271b0b":"submission.to_csv(\"submission\", index=False)","e181f525":"## Make Predictions on the Test Dataset","e82ba2be":"## Convert Text Into Numers\nIn NLP, there are two main concepts for turning text into numbers:\n\n**Tokenization** - A straight mapping from word or character or sub-word to a numerical value. \n\nThere are three main levels of tokenization:\n\n* Using **word-level tokenization** with the sentence \"I love TensorFlow\" might result in \"I\" being 0, \"love\" being 1 and \"TensorFlow\" being 2. In this case, every word in a sequence considered a single token.\n* **Character-level tokenization**, such as converting the letters A-Z to values 1-26. In this case, every character in a sequence considered a single token.\n* **Sub-word tokenization** is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple tokens.\n\n**Embeddings** - An embedding is a representation of natural language which can be learned. Representation comes in the form of a feature vector. For example, the word \"dance\" could be represented by the 5-dimensional vector [-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings:\n\n* **Create your own embedding** - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as tf.keras.layers.Embedding) and an embedding representation will be learned during model training.\n\n* **Reuse a pre-learned embedding** - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task.","2f7f867f":"## Build a model","5cedf0b2":"## Import Dataset","cc612000":"**Competition Description**\n\nTwitter has become an important *communication* channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster\n\n**Dataset**\n\nThe dataset contains three sub-datasets `train.csv`, `test.csv`, `sample_submission.csv`\n\n**Columns**\n\n* `id` - a unique identifier for each tweet\n* `text` - the text of the tweet\n* `location` - the location the tweet was sent from (may be blank)\n* `keyword` - a particular keyword from the tweet (may be blank)\n* `target` - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n**Evaluation Metrics**\n\nSubmissions are evaluated using **F1** between the predicted and expected answers.","1bd7c925":"## Shuffle the dataset","22c2922b":"we can see `model_3` performed better than other models regarding `accuracy`, `recall`, `precision`, `f1`","27b578b8":"## Import Libraries","c440d973":"## Comparing the performance of each of our models","86084754":"## Split the dataset into training and validation"}}