{"cell_type":{"46c89265":"code","68e12b55":"code","de49c64f":"code","e16e6550":"code","d88eea97":"code","6680288a":"code","c669d763":"code","97fda9fb":"code","e2fb8c39":"code","3c8420e4":"code","754e3926":"code","53c49ab0":"markdown","8da1c1ba":"markdown","2cc903b8":"markdown","a4a7e7ce":"markdown","214944c1":"markdown","df23be57":"markdown","0be8085b":"markdown","26f637ed":"markdown","f9cb01c6":"markdown","d7d37c23":"markdown","e03c5779":"markdown","91462c35":"markdown"},"source":{"46c89265":"! conda install -y hvplot=0.5.2 bokeh==1.4.0","68e12b55":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.neural_network import MLPRegressor\nimport numpy as np\nfrom toolz.curried import map, pipe, compose_left, partial\nfrom typing import Union, Tuple, List, Dict\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport warnings\nfrom abc import ABCMeta\nfrom itertools import chain\nfrom operator import add\nimport holoviews as hv\nimport pandas as pd\nimport hvplot.pandas\nfrom sklearn.datasets import load_digits, load_boston\nimport tensorflow as tf\nfrom functools import reduce\nfrom sklearn.inspection import permutation_importance, plot_partial_dependence, partial_dependence\n\nhv.extension('bokeh')","de49c64f":"data = load_boston()\nprint(data.DESCR)","e16e6550":"pd.Series(data.target).hvplot.kde(xlabel='Log-Target Value')","d88eea97":"estimator = MLPRegressor((4,)) \n\nX, y = data.data, np.log(data.target)\n\nestimator.fit(X, y)\ny_pred = estimator.predict(X)\n(pd.Series(y - y_pred).hvplot.kde(xlabel='Model Errors', title='MLP Model') +\\\npd.Series(y - y_pred).hvplot.box(ylabel='').opts(invert_axes=True, height=100)).cols(1)","6680288a":"plot_partial_dependence(estimator, X, [(1,2), 2, 1], feature_names=data.feature_names, n_cols=2)","c669d763":"imp = permutation_importance(estimator, X, y, scoring=None, n_repeats=1000, n_jobs=-1)\n\n(pd.DataFrame(imp['importances'].T, columns=data.feature_names)\n .melt(var_name='Feature', value_name='Importance')\n .hvplot.violin(y='Importance', by='Feature'))","97fda9fb":"EPOCHS = 50\n\nclass FFNN(tf.keras.Model):\n    def __init__(self, layers = (4, )):\n        super(FFNN, self).__init__()\n        \n        self.inputs = tf.keras.layers.InputLayer((3, 3))\n        self.dense = list(map(lambda units: tf.keras.layers.Dense(units, activation='relu'), layers))\n        self.final = tf.keras.layers.Dense(1, activation='linear')\n        \n    def call(self, inputs):\n        \n        return reduce(lambda x, f: f(x), [inputs, self.inputs, *self.dense, self.final])\n    \n@tf.function\ndef train_step(inputs, labels):\n    with tf.GradientTape() as tape:\n        predictions = model(inputs)\n        \n        loss = tf.keras.losses.mse(predictions, label)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \ntrain_ds = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(data.data.astype('float32')),\n                                               tf.convert_to_tensor(np.log(data.target.astype('float32'))))).batch(32)\n\nmodel = FFNN()\nmodel.compile(loss='mse')\n\noptimizer = tf.keras.optimizers.Adam()\nfor epoch in range(EPOCHS):\n    for sample in train_ds:\n        inputs, label = sample\n        gradients = train_step(inputs, label)\n        \ny_pred = model(data.data.astype('float32')).numpy()\n\nmodel.summary()","e2fb8c39":"def sensitivity_importance(X: tf.Tensor, \n                                reference: tf.Tensor, \n                                model: tf.keras.Model,\n                                sample=1000):\n    \"\"\"\n    \"\"\"\n    length = tf.shape(X)[0]\n    features = tf.shape(X)[1]\n    all_subs = tf.dtypes.cast(tf.random.uniform((sample, features)) > 0.5, 'float32')\n    \n    f_mean = model(reference)[0]\n    \n    count_subs = tf.shape(all_subs)[0]\n    \n    @tf.function\n    def apply(x):\n        return tf.reduce_mean(model(tf.where(all_subs==1, \n                                     tf.ones((count_subs,features))*x, \n                                     tf.ones((count_subs,features))*reference_point)) - f_mean, axis=1)\n    \n    all_sub_float = tf.dtypes.cast(all_subs, 'float32')\n    return tf.map_fn(apply, X) @ all_sub_float \/ tf.reduce_sum(all_sub_float, axis=0)\n\nreference_point = data.data.mean(0).reshape(1, -1)\nX = data.data.astype('float32')","3c8420e4":"%%timeit\npermutive_values = sensitivity_importance(X, reference_point, model)","754e3926":"permutive_values = sensitivity_importance(X, reference_point, model)\npd.Series(permutive_values.numpy().mean(0) - permutive_values.numpy().mean(), index=data.feature_names).hvplot.bar(title='Average Sensitivity')","53c49ab0":"# Local\n## Sensitivity Analysis","8da1c1ba":"## Permutive Importance\nUsing Permutive Importance, we drop groups of features, fit a model and compare how our scores change without certain features.  For models which are rely on stochastic optimization, this approach can be expensive or misleading as we may converge on many different models with the same set of features and hyperparamters.  Another challenge with Feature Importance, is that it can eaily be misinterpretted. Feature Importances say little about the effect which changes in that feature have on predictions and must not be conflated the importance of a feature t all possible classes of models. ","2cc903b8":"# Global","a4a7e7ce":"After analyzing the effect of random substitutions of our features against our reference we can average across these explanations to look at the average effect changes in the input on our models predictions using senstitivity analysis.  ","214944c1":"For our examples in this notebook we am going to be looking at the Boston Housing Dataset, which is a simple, well understood dataset provided by default in the Scikit-learn API. The goal here is not find a good model, but to describe a model.  For this reason, we will not be discussing why we choose a particular model, or its hyperparameters, and we are not going to be looking into methods for cross-validation.  ","df23be57":"There are many very complicated methods in black-box model explainability and there are some simpler ones. The family of methods involving Permutation and Sampling are amoung the simplist. The main advantage of this family of methods in in its simplicity- sampling and permuation can be really easy to explain and really easy for domain experts to understand.  This approach also gives a great deal of flexibility in terms of the insights you provide.  The main challenge with black-box methods which rely solely on Purmutation and Sampling is that they can be inefficient and in some cases misleading based on the number of features in your dataset and the complexity of the model. ","0be8085b":"The only transformation I have opten to do is to take the log of our housing price target to make our assumption about our conditional distribution being symmetic, more realistic. ","26f637ed":"One simple approach to model explainability would be to evaluable how the output of our model changes if we permute the input space. This could be done using random sampling or by substituting our features with a univariate means, medians or a reference point.  This can be very computationally intensive and can suffer from many issues caused by sampling points which do not realistically come from our data generating process or failing to model interaction between substrituted variables.  \n\nThis is easy to implement but can be incredibly computationally expensive, for that reason I opted to write this in TensorFlow 2.0, so that we can benefit from end-to-end hardware Acceleration. ","f9cb01c6":"# Data","d7d37c23":"I have opted to make use of the Dense Feed-forward Neural Network (DNN) with 4 hidden neurals and a relu activation function. This is a relatively contrained model, with the ability to model particular non-linearities in the data. ","e03c5779":"The model does present singificant bias in its estimates, as we might expect from such a flexible class of model.  ","91462c35":"## Partial Dependence\nThe Partial Dependence Plot, or Partial Dependence Curves, show how our average prediction changes as we substitute features with points from our dataset. While it may be difficult to visualize and interpret more than two interactions, these plots can be really easy to interpret for users trying to get a global overview of how the model responds to changes in the data. As with many of these methods which rely on permutation and sampling, we do run the risk of over-sampling highly improbably points based on the joint distribution of our model. "}}