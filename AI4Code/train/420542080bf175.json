{"cell_type":{"fefe528f":"code","ef22589d":"code","5a06471d":"code","9888e973":"code","5ac23aea":"code","2f1c0fae":"code","5dd98794":"code","eb9fc3f9":"code","7d6e8cbd":"code","8bb1bacc":"code","7896aba0":"code","52a06f46":"markdown","0f7e93dd":"markdown","ff4fe42c":"markdown","534569b0":"markdown","88de4641":"markdown","7083d485":"markdown","f373734b":"markdown","b5bcc45a":"markdown","e6990f72":"markdown","da9265ea":"markdown","9cdcfc85":"markdown","af15843f":"markdown","3c64fc42":"markdown","5bf53b33":"markdown","919e8965":"markdown","9db1ec9d":"markdown","2b9cc7ef":"markdown","51b95a60":"markdown","894b4fe7":"markdown","2e51bc50":"markdown","29baface":"markdown"},"source":{"fefe528f":"import pandas as pd\nimport sklearn\nimport math\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pyaug import PyAugLinear, PyAugNormal, PyAugLogistic, PyAugLaplace\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, accuracy_score, recall_score","ef22589d":"df = pd.read_csv('..\/input\/creditcard.csv')\n\nprint('DataFrame Shape:', df.shape, '\\n')\nprint('Number of NaN\/Null Values:')\nprint(df.isna().sum(), '\\n')\nprint('DataFrame Head:')\nprint(df.head())","5a06471d":"df.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)\nplt.show()","9888e973":"correlation_matrix = df.corr()\nfig = plt.figure(figsize=(12,9))\nsns.heatmap(correlation_matrix, vmax=0.8, square = True)\nplt.show()","5ac23aea":"df['Time'].hist(bins=100)\nplt.xlim([0, 175000])\nplt.xlabel('Time (s)')\nplt.ylabel('Frequency')\nplt.show()","2f1c0fae":"df['Amount'].hist(bins=500)\nplt.xlim([0, 1500])\nplt.xlabel('Amount')\nplt.ylabel('Frequency')\nplt.show()","5dd98794":"scaler = StandardScaler()\n\nX = df.drop('Class', axis = 1) # Remove target feature.\ny = df['Class']\n\nprint('X DataFrame Shape:', X.shape)\nprint('y DataFrame Shape:', y.shape)\n\nX_scaled = StandardScaler().fit_transform(X).reshape(-1, 1) # Reshape the NumPy array.\nX_scaled = pd.DataFrame(X, columns = X.columns.values)\n\nprint(X.describe())","eb9fc3f9":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.3, random_state = 0)\n\nlr = LogisticRegression(C=1e5, solver='liblinear') # Initiate LogisticRegression model.\n\nlr.fit(X_train, y_train) # Fit the LogisticRegression model.\ny_pred = lr.predict(X_test) # Predict y_pred values using X_test features.\n\n\naccuracy_baseline = float('%.3f' % accuracy_score(y_test, y_pred))\nROCAUC_baseline =  float('%.3f' % roc_auc_score(y_test, y_pred))\nrecall_baseline = float('%.3f' % recall_score(y_test, y_pred))\n\nprint('Accuracy:', accuracy_baseline)\nprint('ROC AUC Score:', ROCAUC_baseline)\nprint('Recall Score:', recall_baseline)","7d6e8cbd":"df = pd.read_csv('..\/input\/creditcard.csv')\ndf_features = df.drop('Class', axis = 1) # Remove target feature.\ndf_labels = df['Class']\n\ndef undersampling():\n    n_fraud = Counter(df_labels)[1] # Count the number of fraudulent transactions.\n\n    df_class0 = df.loc[df['Class'] == 0]\n    df_class0 = df_class0.sample(Counter(df_labels)[1]) # Take a sample of normal transactions with the same length as the fraudulent transactions.\n\n    df_class1 = df.loc[df['Class'] == 1]\n\n\n    df_undersample = pd.concat([df_class0, df_class1]) # Concatenate the DataFrames.\n\n    df_undersample_features = df_undersample.drop('Class', axis = 1) # Remove target feature.\n    df_undersample_labels = df_undersample['Class']\n\n    X_scaled = StandardScaler().fit_transform(df_undersample_features).reshape(-1, 1) # Reshape the NumPy array. \n    X_scaled = pd.DataFrame(df_undersample_features, columns = df_undersample_features.columns.values)\n\n    X_train, X_test, y_train, y_test = train_test_split(df_undersample_features, df_undersample_labels, test_size = 0.3, random_state = 0)\n\n    lr = LogisticRegression(C = 1e5, solver='liblinear') # Initiate LogisticRegression model.\n\n    lr.fit(X_train, y_train) # Fit the LogisticRegression model.\n    y_pred = lr.predict(X_test) # Predict y_pred values using X_test features.\n\n\n    #Assess Machine Learning Model.\n    results_matrix = confusion_matrix(y_test, y_pred)\n    accuracyscore = float('%.3f' % accuracy_score(y_test, y_pred))\n    ROCAUCscore =  float('%.3f' % roc_auc_score(y_test, y_pred))\n    recallscore = float('%.3f' % recall_score(y_test, y_pred))\n    \n    metrics = [accuracyscore, ROCAUCscore, recallscore]\n    return metrics\n\n\n#Iterating and Evaluating Average Metrics.\niteration_results_undersampling = []\nfor _ in range(10):\n    iteration_results_undersampling.append(undersampling())\n \nundersampling_df = pd.DataFrame(iteration_results_undersampling, columns = ['Accuracy', 'ROCAUC', 'Recall'])\n\naverage_accuracy_undersampling = undersampling_df['Accuracy'].mean()\naverage_ROCAUC_undersampling = undersampling_df['ROCAUC'].mean()\naverage_recall_undersampling = undersampling_df['Recall'].mean()\n\nprint(undersampling_df.head())\nprint('Average Accuracy', float('%.3f' % average_accuracy_undersampling))\nprint('Average ROCAUC', float('%.3f' % average_ROCAUC_undersampling))\nprint('Average Recall', float('%.3f' % average_recall_undersampling))","8bb1bacc":"df = pd.read_csv('..\/input\/creditcard.csv')\ndf_features = df.drop('Class', axis = 1)\ndf_labels = df['Class']\n\n\ndef PyAugOversampling():\n    #Scale DataFrame.\n    df_scaled = StandardScaler().fit_transform(df_features).reshape(-1, 30) # Reshape the NumPy array. \n    df_scaled = pd.DataFrame(df_scaled, columns = df_features.columns.values)\n\n    #Concatenate Scaled DataFrame with Unscaled Labels.\n    df_scaled = pd.concat([df_scaled, df_labels], axis = 1)\n\n    df_class0 = df_scaled.loc[df_scaled['Class'] == 0] #Extract Class = 0 (Non-Fraud).\n    df_class0 = df_class0.sample(2000) #Take Random Sample of 2000.\n    df_class1 = df_scaled.loc[df_scaled['Class'] == 1] #Extract Class = 1 (Fraud).\n\n    #Define Features and Targets for Class 0 and Class 1.\n    X0 = df_class0.drop('Class', axis = 1) #Remove target feature.\n    y0 = df_class0['Class']\n\n    X1 = df_class1.drop('Class', axis = 1) #Remove target feature.\n    y1 = df_class1['Class']\n\n    X_train0, X_test0, y_train0, y_test0 = train_test_split(X0, y0, test_size = 0.5, random_state = 0)\n    X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size = 0.5, random_state = 0)\n\n\n    Xtrain1_ytrain1_concat = pd.concat([X_train1, y_train1], axis = 1) #Concatenate X_train1 and y_train1 to augment 1000 news rows.\n    aug_train1 = PyAugLinear(Xtrain1_ytrain1_concat, 1000, 3, ['Class']) #Generate 1000 synthetic results using PyAug.\n\n    aug_train1_features = aug_train1.drop('Class', axis = 1) #Remove target feature.\n    aug_train1_class = aug_train1['Class']\n\n    X_train = pd.concat([X_train0, aug_train1_features], axis = 0) #Concatenate X_train0 and aug_train1_features.\n    y_train = pd.concat([y_train0, aug_train1_class], axis = 0) #Concatenate y_train0 and aug_train1_class.\n\n    X_test = pd.concat([X_test0.sample(len(X_test1)), X_test1], axis = 0) #Make Sure Sample size correct\n    y_test = pd.concat([y_test0.sample(len(y_test1)), y_test1], axis = 0) #Make Sure Sample size correct\n\n\n    #Machine Learning.\n    lr = LogisticRegression(C = 1e5, solver = 'liblinear') # Initiate LogisticRegression model.\n    lr.fit(X_train, y_train) # Fit the LogisticRegression model.\n    y_pred = lr.predict(X_test) # Predict y_pred values using X_test features.\n\n    \n    #Assess Machine Learning Model.\n    results_matrix = confusion_matrix(y_test, y_pred)\n    accuracyscore = float('%.3f' % accuracy_score(y_test, y_pred))\n    ROCAUCscore =  float('%.3f' % roc_auc_score(y_test, y_pred))\n    recallscore = float('%.3f' % recall_score(y_test, y_pred))\n    \n    metrics = [accuracyscore, ROCAUCscore, recallscore]\n    return metrics\n\n\n#Iterating and Evaluating Average Metrics.\niteration_results_oversampling = []\nfor _ in range(10):\n    iteration_results_oversampling.append(PyAugOversampling())\n \nPyAug_df = pd.DataFrame(iteration_results_oversampling, columns = ['Accuracy', 'ROCAUC', 'Recall'])\n\naverage_accuracy_oversampling = PyAug_df['Accuracy'].mean()\naverage_ROCAUC_oversampling = PyAug_df['ROCAUC'].mean()\naverage_recall_oversampling = PyAug_df['Recall'].mean()\n\nprint(PyAug_df.head())\nprint('Average Accuracy', float('%.3f' % average_accuracy_oversampling))\nprint('Average ROCAUC', float('%.3f' % average_ROCAUC_oversampling))\nprint('Average Recall', float('%.3f' % average_recall_oversampling))","7896aba0":"x_values = ['Baseline', 'Undersampling', 'Oversampling (PyAug)']\naccuracy_data = [accuracy_baseline, average_accuracy_undersampling, average_accuracy_oversampling]\nROCAUC_data = [ROCAUC_baseline, average_ROCAUC_undersampling, average_ROCAUC_oversampling]\nrecall_data = [recall_baseline, average_recall_undersampling, average_recall_oversampling]\n\nplt.bar(x_values, accuracy_data, color = ['r', 'g', 'b'])\nplt.ylabel('Accuracy Metric')\nplt.ylim((0.5, 1.05))\nplt.show()\n\nplt.bar(x_values, ROCAUC_data, color = ['r', 'g', 'b'])\nplt.ylabel('ROC AUC Metric')\nplt.ylim((0.5, 1.05))\nplt.show()\n\nplt.bar(x_values, recall_data, color = ['r', 'g', 'b'])\nplt.ylabel('Recall Metric')\nplt.ylim((0.5, 1.05))\nplt.show()\n\n\n#Assessing Accuracy Improvement\nprint('Accuracy Improvement:', ((average_accuracy_oversampling - average_accuracy_undersampling) * 100), '%')\nprint('ROC AUC Improvement:', ((average_ROCAUC_oversampling - average_ROCAUC_undersampling)* 100), '%')\nprint('Recall Improvement:', ((average_recall_oversampling - average_recall_undersampling) * 100), '%')","52a06f46":"## Exploratory Data Analysis (EDA)","0f7e93dd":"## Data Preprocessing for Machine Learning","ff4fe42c":"The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","534569b0":"## Comparison of Results","88de4641":"3) Inspecting 'Time' feature in more detail.","7083d485":"2) Correlation Matrix.","f373734b":"Applying undersampling and oversampling techniques is an effective methodology for handling imbalanced datasets. When machine learning (logistic regression model) was applied to the dataset we observe a high reported accuracy. However, this is deceiving as the ROC AUC and recall scores that are significantly lower which suggests the model reports a high number of false positive and false negative results. That is to say, the model incorrectly classifies instances of fraud. When undersampling is applied to the dataset we observe a slightly lower accuracy by vastly improved ROC AUC and recall scores. Oversampling was achieved through the PyAug Python library to generate synthetic data for training the logistic regression model. We observe an increase in accuracy, ROC AUC and recall scores between 1-4% over several iterations. Subsequrntly, we suggest that PyAug is successful in developing logistic regression models that handle imbalanced datasets.\n","b5bcc45a":"4) Investigating 'Amount' feature in more detail.","e6990f72":"# Credit Card Fraud Detection: ULB Dataset","da9265ea":"## Logistic Regression Without Undersampling or Oversampling","9cdcfc85":"PyAug: PyAug is a library developed to augment Pandas DataFrames using statistical distributions. Currently, PyAug supports linear, normal, logistic and Laplace statistical distributions. An interactive web-application has been developed using the PyAug framework and is available at www.pyaug.com. ","af15843f":"NOTE: No NaN values are present so we will not need to apply pd.fillna().","3c64fc42":"# Conclusion","5bf53b33":"NOTE: No correlation between PCA components is expected as they have already been subject to PCA whereas the 'Time' and 'Amount' features have not been subject to PCA.","919e8965":"### Undersampling","9db1ec9d":"## Inspecting Data","2b9cc7ef":"#### Creating an Undersampled DataFrame with 50:50 Class Split","51b95a60":"## Data Class Imbalance: Undersampling and Oversampling\n\nObjective: have an equal split in data of both classifications.\n\n<b>Methodology: Undersampling<\/b> <br>\nRemove copies of the over-represented class for instances where the sample contains large amounts of data.\n\n<b>Methodology: Oversampling<\/b> <br>\nAdd additional copies of the under-represented class for instances where the sample contains low amounts of data.","894b4fe7":"1) Histogram for each DataFrame feature.","2e51bc50":"### Oversampling","29baface":"#### Creating an Oversampled DataFrame Using PyAug"}}