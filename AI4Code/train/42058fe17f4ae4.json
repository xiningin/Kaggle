{"cell_type":{"065c7225":"code","fc94f33f":"code","952b4111":"code","b61f9325":"code","4c2cb970":"code","7fe31159":"code","f21b9088":"code","793a4e00":"code","1ef44570":"code","2e2411c9":"code","953b1fcf":"code","58010b19":"code","a01b0d22":"code","0f09996c":"code","bca8c853":"code","afd2fe89":"code","ff328b3b":"markdown","fee10e2b":"markdown","bea2d4ca":"markdown","df817e4f":"markdown","67f1474b":"markdown","2cd6b3a8":"markdown","82f45b73":"markdown","f97de697":"markdown","58b21a17":"markdown","2ee0ad97":"markdown","ee0894e7":"markdown","387603c8":"markdown","4edcb3f3":"markdown","f2ca3f82":"markdown","2fe55095":"markdown","0f24ae5e":"markdown","0a3dfae7":"markdown","a4c353b7":"markdown","4f9057e7":"markdown","24fd0760":"markdown"},"source":{"065c7225":"!pip install advertools --quiet\n","fc94f33f":"#Basic libraries\nimport numpy as np\nimport pandas as pd\n\n#EDA: \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as py\nimport plotly.tools as tls\nfrom plotly.offline import init_notebook_mode\n## Wordclouds\nimport altair as alt\nfrom  altair.vega import v5\nfrom IPython.display import HTML\nimport json\n\n#Basic Preprocessing\nfrom collections import defaultdict, Counter\nimport advertools as adv\n\n#Question-Answering\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline","952b4111":"df = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')","b61f9325":"# Defining functions for visualizations: \n\ndef pie_plot(labels, values, colors, title):\n    fig = {\n      \"data\": [\n        {\n          \"values\": values,\n          \"labels\": labels,\n          \"domain\": {\"x\": [0, .48]},\n          \"name\": \"Job Type\",\n          \"sort\": False,\n          \"marker\": {'colors': colors},\n          \"textinfo\":\"percent+label\",\n          \"textfont\": {'color': '#FFFFFF', 'size': 10},\n          \"hole\": .6,\n          \"type\": \"pie\"\n        } ],\n        \"layout\": {\n            \"title\":title,\n            \"annotations\": [\n                {\n                    \"font\": {\n                        \"size\": 25,\n\n                    },\n                    \"showarrow\": False,\n                    \"text\": \"\"\n\n                }\n            ]\n        }\n    }\n    return fig","4c2cb970":"##-----------------------------------------------------------\n# This whole section \nvega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + v5.SCHEMA_VERSION\nvega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\nvega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\nvega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\nnoext = \"?noext\"\n\npaths = {\n    'vega': vega_url + noext,\n    'vega-lib': vega_lib_url + noext,\n    'vega-lite': vega_lite_url + noext,\n    'vega-embed': vega_embed_url + noext\n}\n\nworkaround = \"\"\"\nrequirejs.config({{\n    baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n    paths: {}\n}});\n\"\"\"\n\n#------------------------------------------------ Defs for future rendering\ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n            \n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"works?\");\n    }});\n    console.log(\"recheck to see if it works?\");\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\n\n\nHTML(\"\".join((\n    \"<script>\",\n    workaround.format(json.dumps(paths)),\n    \"<\/script>\")))","7fe31159":"# Wordcloud function\n\n\ndef word_cloud(df, pixwidth=6000, pixheight=350, column=\"index\", counts=\"count\"):\n    data= [dict(name=\"dataset\", values=df.to_dict(orient=\"records\"))]\n    wordcloud = {\n        \"$schema\": \"https:\/\/vega.github.io\/schema\/vega\/v5.json\",\n        \"width\": pixwidth,\n        \"height\": pixheight,\n        \"padding\": 0,\n        \"title\": \"Hover to see number of occureances from all the sequences\",\n        \"data\": data\n    }\n    scale = dict(\n        name=\"color\",\n        type=\"ordinal\",\n        range=[\"cadetblue\", \"royalblue\", \"steelblue\", \"navy\", \"teal\"]\n    )\n    mark = {\n        \"type\":\"text\",\n        \"from\":dict(data=\"dataset\"),\n        \"encode\":dict(\n            enter=dict(\n                text=dict(field=column),\n                align=dict(value=\"center\"),\n                baseline=dict(value=\"alphabetic\"),\n                fill=dict(scale=\"color\", field=column),\n                tooltip=dict(signal=\"datum.count + ' occurrances'\")\n            )\n        ),\n        \"transform\": [{\n            \"type\": \"wordcloud\",\n            \"text\": dict(field=column),\n            \"size\": [pixwidth, pixheight],\n            \"font\": \"Helvetica Neue, Arial\",\n            \"fontSize\": dict(field=\"datum.{}\".format(counts)),\n            \"fontSizeRange\": [10, 60],\n            \"padding\": 2\n        }]\n    }\n    wordcloud[\"scales\"] = [scale]\n    wordcloud[\"marks\"] = [mark]\n    \n    return wordcloud\n\n\n\ndef wordcloud_create(df, field):\n    ult = {}\n    corpus = df[field].values.tolist()\n    final = defaultdict(int) #Declaring an empty dictionary for count (Saves ram usage)\n    for words in corpus:\n        for word in words.split():\n             final[word]+=1\n    temp = Counter(final)\n    for k, v in  temp.most_common(300):\n        ult[k] = v\n    corpus = pd.Series(ult) #Creating a dataframe from the final default dict\n    return render(word_cloud(corpus.to_frame(name=\"count\").reset_index(), pixheight=600, pixwidth=900))","f21b9088":"df.head()","793a4e00":"value_counts = df['language'].value_counts()\nlabels = value_counts.index.tolist()\npy.iplot(pie_plot(labels, value_counts, ['#1B9E77', '#D95F02'], \"Language breakdown\"))","1ef44570":"hindi_df = df[df['language']=='hindi']\nwordcloud_create(hindi_df, 'context')","2e2411c9":"tamil_df = df[df['language']=='tamil']\nwordcloud_create(tamil_df, 'context')","953b1fcf":"hindi_stopwords = adv.stopwords['hindi']\ntamil_stopwords = adv.stopwords['tamil']","58010b19":"def remove_stopwords(sentence, stopwords):\n    final_sentence = ''\n    for word in sentence.split(): \n        if word not in stopwords:\n            final_sentence += ' ' + word\n            \n    return final_sentence.lstrip()\n            ","a01b0d22":"df_hindi_stop = hindi_df.copy()\ndf_hindi_stop['answer_text'] = df_hindi_stop['answer_text'].apply(lambda x: remove_stopwords(x, hindi_stopwords))","0f09996c":"wordcloud_create(df_hindi_stop, 'answer_text')","bca8c853":"df_tamil_stop = tamil_df.copy()\ndf_tamil_stop['answer_text'] = df_tamil_stop['answer_text'].apply(lambda x: remove_stopwords(x, tamil_stopwords))","afd2fe89":"wordcloud_create(df_tamil_stop, 'answer_text')","ff328b3b":"### [Necessary Functions]()<a id=\"necessary_functions\"><\/a> <br>","fee10e2b":"## [Exploratory Data Analysis]()<a id=\"3\"><\/a> <br>","bea2d4ca":"#### [What is the number of tamil and hindi questions in the dataset?]()<a id=\"4\"><\/a> <br>","df817e4f":"#### [What are the most prevelant words for Tamil answers, after stopword removal?]()<a id=\"9\"><\/a> <br>","67f1474b":"But first, what are stopwords and why do they need to be removed? \n- Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document.\n\nHere are a few key benefits of removing stopwords:\n\n- On removing stopwords, dataset size decreases and the time to train the model also decreases\n- Removing stopwords can potentially help improve the performance as there are fewer and only meaningful tokens left. Thus, it could increase classification accuracy\n- Even search engines like Google remove stopwords for fast and relevant retrieval of data from the database\n\nFor our usecase, removing stopwords can help us figure which words are most prevelant in the answers. \n\n**But**, we should not remove them when training for the QA model, for these words are important in the context of generating answers. This excercise is only useful to understand from a human's perspective on which words are the most common. Nothing more, nothing less. \n","2cd6b3a8":"## [Understanding Data]()<a id=\"2\"><\/a> <br>\n","82f45b73":"## [Preprocessing Data]()<a id=\"7\"><\/a> <br>","f97de697":"There probably is a lot more to do, learn a lot about QA models and how they function. But that's for a new kernel. \nThank you for going through this, Kudos! ","58b21a17":"#### [What are the most prevelant words for Tamil?]()<a id=\"6\"><\/a> <br>","2ee0ad97":"#### [What are the most prevelant words for Hindi?]()<a id=\"5\"><\/a> <br>","ee0894e7":"Let's try to figure out which words are more prevelant inside the answers after removing the stopwords. ","387603c8":"# Introduction\n\nThis competition is related to creating a Question-Answer based model surrounding the languages: Hindi - Tamil. Over the past few years, I have dabbled with NLP and some techniques of NLU. With this competition, I hope to learn and create a notebook that will solidify the foundations for Question-Answer based NLU models for the upcoming new learners. \n","4edcb3f3":"## [Importing Libraries and Data]()<a id=\"1\"><\/a> <br>\n","f2ca3f82":"From the columns, we can observe the following columns: \n- **id**: The unique id for that particular row. \n- **context**: The text in hindi\/tamil from which the answer needs to be derived. \n- **question**: The question in the respective language\n- **answer_text**: This is the text which signifies the answer. We are trying to predict this for the test set. As this is a text based competition, we will be using jaccard score to evaluate how closely related to the true answer it was. \n- **answer_start**: The starting character index in the context from where the answer begins. \n- **language**: The language of the context and question. ","2fe55095":"Since I do not understand tamil, I'd approach this with a novel thinking of what I can see: \n1. More numbers exist in Tamil than in HIndi. \n2. I could observe the following punctuations occurring more: - = ) + \n\nAll of that data can be trimmed down to make it more clean, especially values such as (link) which has 176 occurrances. \n","0f24ae5e":"#### [What are the most prevelant words for Hindi answers, after stopword removal?]()<a id=\"8\"><\/a> <br>","0a3dfae7":"# Contents: \n1. [Importing Libraries and Data](#1) \n    1. [Necessary Functions](#necessary_functions)\n3. [Understanding Data](#2)\n3. [Exploratory Data Analysis](#3)\n    1. [What is the number of tamil and hindi rows in the database?](#4) \n    2. [What are the most prevelant words for Hindi?](#5)\n    3. [What are the most prevelant words for Tamil?](#6)\n\n4. [Preprocessing Data](#7)\n    1. [What are the most prevelant words for Hindi answers, after stopword removal?](#8)\n    2. [What are the most prevelant words for Tamil answers, after stopword removal?](#9)","a4c353b7":"Although I don't understand tamil, I will make the plot for it. ","4f9057e7":"As I know hindi, I can look at the above wordcloud and figure out that: \n- The most common words, like English, are the stop words. \n- Punctuation is present for hindi dataset too, such as: , | ? - \n\n\nBased on these, we can formulate a plan for the next steps. \nLet's draw the same for Tamil and see if something can be inferred. ","24fd0760":"Huh, would you look at that? A lot of the answers seem to be about: \n- Kilometers\/Meters\n- Names of cities, months and the country. \n- Numbers, in both hindi and numerical form\n\nThis does provide insight to the fact that all of this is most likely a factual dataset. "}}