{"cell_type":{"087bb525":"code","9306f6f8":"code","e3a721e2":"code","fa096b77":"code","7e40afdb":"code","b59b6ac0":"code","6fb28781":"code","af9a2d2b":"code","8ca58081":"code","e75dd385":"code","346239b5":"code","220eaedd":"code","ec1fb323":"code","afe1e436":"code","cbc49aa0":"code","82a054a5":"code","5a0b1b27":"code","90a43557":"code","71010047":"code","49a87232":"code","79c0f19d":"code","4ffd9fbd":"code","e04cfd3b":"code","65f664e6":"code","a1b4c20e":"code","7898a6b2":"code","2f0f4ec4":"code","f9325bdd":"code","e87f4a07":"code","bdb0f6cb":"code","f3be3285":"code","8be292b8":"code","d86d1c39":"code","0044d87d":"code","02fb0886":"code","b6d462cc":"code","d2090043":"code","a043cc29":"code","009c43d6":"code","cd158076":"code","67284183":"code","3131129c":"code","757809b7":"code","2c5646fa":"code","dbea667f":"code","520a55a4":"code","5e8b7ca0":"code","d986b007":"code","cd957bc6":"code","a3503a22":"code","1f9380ae":"code","06babd42":"code","4d44044b":"code","261db0cb":"code","8afe8f46":"code","a560a9d2":"code","9b92bfa9":"code","aa63c249":"code","dc209c74":"code","f8bf3356":"code","a795a504":"code","14b70b30":"code","2705d4fe":"code","71535614":"code","017ebcc5":"code","d9d92e4a":"code","0cbff6ca":"code","a4652f50":"code","4695796b":"code","73eb1e48":"code","a98aeead":"code","b8072cf1":"code","5bc911fd":"code","dbd27b24":"code","b7f3960d":"markdown","def82b20":"markdown","e4f21995":"markdown","0da6888c":"markdown","d9fa6b31":"markdown","d1dbd6dd":"markdown","8660832a":"markdown"},"source":{"087bb525":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9306f6f8":"df_train = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\n","e3a721e2":"df_train.head()","fa096b77":"#df_test\n#print([i for i in df_test.columns])\n","7e40afdb":"for i in df_train.columns: #to check is theres any missing value present\n    print(df_train[i].isnull().sum(),i)","b59b6ac0":"y = df_train['target']\nX=  df_train.drop(columns=[\"target\"])","6fb28781":"X[\"id\"] = pd.to_numeric(X['id'])\nX.set_index('id',inplace =True)","af9a2d2b":"X.info()","8ca58081":"X.isna().any().sum()","e75dd385":"df_test[\"id\"] = pd.to_numeric(df_test['id'])\ndf_test.set_index('id',inplace =True)\n","346239b5":"memory_train = sum(X.memory_usage()) \/ 1e6\nprint(f'[INFO] Memory usage train_before: {memory_train:.2f} MB.')\n\nmemory_test = sum(df_test.memory_usage()) \/ 1e6\nprint(f'[INFO] Memory usage test_before: {memory_test:.2f} MB.\\n')","220eaedd":"for i in X.columns:\n    if X[i].dtype == \"float64\":\n        X[i] = pd.to_numeric(X[i],downcast=\"float\")\n    if X[i].dtype == \"int64\":\n        X[i] = pd.to_numeric(X[i],downcast=\"integer\")\n\nfor i in df_test.columns:\n    if df_test[i].dtype == \"float64\":\n        df_test[i] = pd.to_numeric(df_test[i],downcast=\"float\")\n    if df_test[i].dtype == \"int64\":\n        df_test[i] = pd.to_numeric(df_test[i],downcast=\"integer\")\n        \n    ","ec1fb323":"memory_train = sum(X.memory_usage()) \/ 1e6\nprint(f'[INFO] Memory usage train: {memory_train:.2f} MB.')\n\nmemory_test = sum(df_test.memory_usage()) \/ 1e6\nprint(f'[INFO] Memory usage test: {memory_test:.2f} MB.')","afe1e436":"import  matplotlib.pyplot as plt\nimport seaborn as sns","cbc49aa0":"fig, axes = plt.subplots(10,10, figsize=(20, 12))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    sns.kdeplot(\n        data=X, ax=ax, hue= y , fill=True, x=f'f{idx}', palette=['#4DB6AC', 'red'], legend=idx==0)\n \nplt.show()","82a054a5":"import tensorflow as tf\nimport keras\nfrom tensorflow.keras.layers import Dense,Activation,BatchNormalization,Dropout,Input,Multiply,Concatenate,Add\nfrom tensorflow.keras.models import Sequential,Model\ntf.random.set_seed(100)","5a0b1b27":"\nfrom sklearn.preprocessing import RobustScaler\nrb = RobustScaler()\n","90a43557":"X_scaled= rb.fit_transform(X)\n","71010047":"X_scaled_cleaned = pd.DataFrame(X_scaled, columns= ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99'])","49a87232":"# ROBUST, TRANSFORM, BEST MODEL\n\ndef create_model():\n    x_input = Input(shape=(X_scaled_cleaned.shape[1],))\n    \n    x1 = Dense(100,activation='relu' )(x_input)\n    x2 = Dense(128, activation='relu')(x1)\n    x3 = Dense(256,activation='relu' )(x2)\n    z2 = Dense(100,activation='relu')(x3)\n    \n    z31 = Multiply()([x1, z2])\n    z31 = BatchNormalization()(z31)\n    \n    z31 = Dropout(0.3)(z31)\n    \n    x4 =  Dense(128,activation='relu')(z31)\n    x5 =  Dense(64,activation='relu')(x4)\n    z3=  Dense(128,activation='relu')(x5)\n    \n    z32 = Multiply()([x4, z3])\n    z32 = BatchNormalization()(z32)\n  \n    z32 = Dropout(0.3)(z32)\n\n    x7 =  Dense(32,activation='relu')(z32)\n    \n#     x8 =  Dense(32,activation='relu')(x7)\n#     x9 =  Dense(16,activation='relu')(x8)\n#     z4 =  Dense(100,activation='relu')(x9)\n#     z33 = Concatenate()([x1,x4, z4])\n#     z33 = BatchNormalization()(z33)\n#     x10 =  Dense(8,activation='relu')(x9)\n    \n#     z41 = Multiply()([x3, z3])\n#     z41 = BatchNormalization()(z41)\n    \n#     z4 =  Dense(128,activation='relu')(z41)\n#     z51 = Multiply()([x5, z4])\n#     z51 = BatchNormalization()(z51)\n    \n#     z4 =  Dense(64,activation='relu')(z41)\n#     x =  Concatenate()([z2, z3, z4])\n#     x =  Dense(32, activation='relu')(x)\n    x_output = Dense(units=1,activation='sigmoid')(x7)\n    model = Model(inputs=x_input, outputs=x_output,name='Model')\n    return model\n","79c0f19d":"# pca = PCA(n_components=0.98)\n# X_reduced = pca.fit_transform(X_scaled_cleaned)\n","4ffd9fbd":"# X_reduced.shape","e04cfd3b":"model = create_model()\nmodel.summary()\n","65f664e6":"from tensorflow.keras.utils import plot_model\n\nplot_model(model,  to_file='TPS_Model.png', show_shapes=True,show_layer_names=True)\n","a1b4c20e":"from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\nearly = EarlyStopping(monitor=\"loss\", mode= \"min\", patience=3)\nlearning_rate_reduction = ReduceLROnPlateau( monitor=\"auc\",patience =3, verbose=1 , factor=0.2, min_learning_rate=0.0001 )\ncallbacks_list = [early,learning_rate_reduction]\n# Compile model\nmodel.compile(optimizer='adam',loss='binary_crossentropy', metrics=['AUC']) \n# history = model.fit(train_features,train_labels_encoded, batch_size =100 , callbacks= callbacks_list , class_weight = cw , epochs=100, validation_data= (test_features,test_labels_encoded))\nhistory = model.fit(X_scaled_cleaned, y , callbacks= callbacks_list,batch_size = 100 , epochs=100)","7898a6b2":"test = rb.transform(df_test)","2f0f4ec4":"test\n","f9325bdd":"X_test_cleaned = pd.DataFrame(test, columns= ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99'])","e87f4a07":"target = model.predict(X_test_cleaned)","bdb0f6cb":"ids = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\nids = ids['id']\ndata = {\n  \"id\": ids,\n  \"target\": target[:,-1]\n}\n\nsubmission = pd.DataFrame(data)","f3be3285":"submission","8be292b8":"submission.to_csv(\"submission2.csv\",index =False)","d86d1c39":"\n# fig, axes = plt.subplots(100,1,figsize=(10,300))\n# for i,j in zip(X_scaled.columns,range(0,100)):\n#     axes[j].set_title(i)\n#     axes[j].boxplot(X_scaled[i])\n","0044d87d":"# fig, axes = plt.subplots(100,1,figsize=(10,300))\n# for i,j in zip(X_scaled.columns,range(0,100)):\n#     axes[j].set_title(i)\n#     axes[j].hist(X_scaled[i],bins = 70)\n","02fb0886":"# first_quantile = list(X_scaled.quantile(0.10))\n# second_quantile = list(X_scaled.quantile(0.90))\n# # print(first_quantile)\n# # print(second_quantile)","b6d462cc":"\n#  # IQR\n# Q1 = X_scaled.quantile(0.25)\n# Q3 = X_scaled.quantile(0.75)\n# IQR = Q3 - Q1\n# #  print(\"Old Shape: \", X_scaled.shape)\n# lower = Q1 - 1.5*IQR\n# upper = Q3 + 1.5*IQR\n# # X_scaled.where(upper[0], inplace = True)\n# # X_scaled.drop(lower[0], inplace = True)\n# # print(\"New Shape: \", X_scaled.shape)\n# X_clean = np.where((X_scaled >lower)&(X_scaled <upper),X_scaled, np.mean(X_scaled))\n","d2090043":"# X_scaled_cleaned = pd.DataFrame(X_scaled, columns= ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99'])","a043cc29":"# X_scaled_cleaned.isnull().sum()","009c43d6":"# for i,j in zip(X_scaled.columns,second_quantile):\n#     X_scaled[i] = np.where(X_scaled[i]>j,np.median(X_scaled[i]),X_scaled[i])\n# a = X_clean['f0'].fillna(np.median(X_clean['f0']))","cd158076":"\n# = [ \"f0\",\"f2\",\"f4\",\"f9\",\"f12\",\"f16\",\"f19\",\"f20\",\"f21\",\"f23\",\"f24\",\"f27\",\"f28\",\"f30\",\"f31\",\"f32\",\"f33\",\"f35\",\"f36\",\"f39\",\"f42\",\"f44\",\"f46\",\"f48\",\"f49\",\"f51\",\"f52\",\"f53\",\"f58\",\"f59\",\"f60\",\"f61\",\"f62\",\"f63\",\"f64\",\"f68\",\"f69\",\"f72\",\"f73\",\"f76\",\"f78\",\"f79\",\"f81\",\"f83\",\"f84\",\"f87\",\"f88\",\"f89\",\"f90\",\"f92\",\"f93\",\"f94\",\"f95\",\"f98\",\"f99\"]","67284183":"# for i in l:\n#     print(\"mean:\", np.mean(X_scaled[i]))\n#     print(\"median:\", np.median(X_scaled[i]))","3131129c":"# first =[]\n# second =[]\n# for i in l:\n#     first_quantile = X_scaled[i].quantile(0.10)\n#     first.append(first_quantile)\n#     second_quantile = X_scaled[i].quantile(0.90)\n#     second.append(second_quantile)\n# print(first)","757809b7":"# first_quantile = X_scaled.quantile(0.10)\n# second_quantile = X_scaled.quantile(0.90)\n","2c5646fa":"# for i,j in zip(X_scaled.columns, first_quantile):\n#     X_scaled[i] = np.where(X_scaled[i]<j,np.median(X_scaled[i]),X_scaled[i])\n","dbea667f":"# for i,j in zip(X_scaled.columns ,second_quantile):\n#     X_scaled[i] = np.where(X_scaled[i]>j,np.median(X_scaled[i]),X_scaled[i])\n","520a55a4":"# from sklearn.ensemble import IsolationForest\n\n# iso = IsolationForest(contamination=0.1)\n# yhat = iso.fit_predict(X_scaled)","5e8b7ca0":"# fig, axes = plt.subplots(100,1,figsize=(10,300))\n# for i,j in zip(X_clean.columns,range(0,100)):\n#     axes[j].set_title(i)\n#     axes[j].boxplot(X_clean[i])\n","d986b007":"# fig, axes = plt.subplots(100,1,figsize=(10,300))\n# for i,j in zip(X_scaled_cleaned.columns,range(0,100)):\n#     axes[j].set_title(i)\n#     axes[j].hist(X_scaled_cleaned[i],bins = 200)","cd957bc6":"# to find out features that are negatively  correlated with target so that we can remove them\n\n# x = [j for i,j in zip(X.corr()[\"target\"],X.corr()[\"target\"].index) if i <0]\n# print(x)","a3503a22":"# #FUNCTION FOR REMOVING OUTLIERS AND IQR\n# outliers = []\n# def detect_outliers_zscore(data):\n#     thres = 3\n#     mean = np.mean(data)\n#     std = np.std(data)\n#     # print(mean, std)\n#     for i in data:\n#         z_score = (i-mean)\/std\n#         if (np.abs(z_score) > thres):\n#             outliers.append(i)\n#     return outliers# Driver code\n\n\n\n","1f9380ae":"\n# Computing 10th, 90th percentiles and replacing the outliers\n\n\n# mean = np.mean(X_scaled['f4'])# Replace with median\n# for i in sample_outliers:\n#     c = np.where(X_scaled['f4'] ==i ,mean , X_scaled['f4'])\n# print(\"New array: \",c)","06babd42":"\n# X_scaled_cleaned = X_scaled.drop(['f1', 'f2', 'f3', 'f5', 'f9', 'f14', 'f15', 'f16', 'f17', 'f21', 'f22', 'f23', 'f25', 'f26', 'f31', 'f36', 'f37', 'f38', 'f39', 'f47', 'f48', 'f49', 'f53', 'f54', 'f55', 'f59', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f71', 'f72', 'f75', 'f76', 'f78', 'f80', 'f82', 'f83', 'f84', 'f86', 'f87', 'f90', 'f91', 'f92', 'f93', 'f94', 'f97', 'f99'],axis =1)","4d44044b":"# from sklearn.ensemble import ExtraTreesClassifier\n# ex = ExtraTreesClassifier()\n# ex.fit(X_scaled_cleaned,y)\n# print(ex.feature_importances_)","261db0cb":"# feat_importances = pd.Series(ex.feature_importances_,index =X_scaled_cleaned.columns)\n# feat_importances.nlargest(50).plot(kind='barh')","8afe8f46":"# top_features = ['f34',\n#  'f43',\n#  'f8',\n#  'f50',\n#  'f27',\n#  'f41',\n#  'f57',\n#  'f96',\n#  'f40',\n#  'f81',\n#  'f10',\n#  'f45',\n#  'f77',\n#  'f70',\n#  'f11',\n#  'f6',\n#  'f74',\n#  'f85',\n#  'f7',\n#  'f18']\n# list(set(X_scaled_cleaned.columns).difference(top_features))","a560a9d2":"# X_scaled_cleaned = X_scaled_cleaned.drop(['f19',\n#  'f56',\n#  'f89',\n#  'f73',\n#  'f0',\n#  'f88',\n#  'f42',\n#  'f29',\n#  'f4',\n#  'f52',\n#  'f60',\n#  'f33',\n#  'f28',\n#  'f32',\n#  'f69',\n#  'f95',\n#  'f35',\n#  'f24',\n#  'f30',\n#  'f12',\n#  'f51',\n#  'f44',\n#  'f46',\n#  'f98',\n#  'f13',\n#  'f79',\n#  'f20',\n#  'f58',\n#  'f68',\n#  'f61'],axis =1)\n\n# X_scaled1 = X_scaled.drop(l,axis=1)\n# print(X_scaled1.shape)","9b92bfa9":"# from sklearn.utils.class_weight import compute_class_weight\n# weights = compute_class_weight('balanced', np.unique(np.array(y)),np.array(y))\n# cw = dict(zip( np.unique(y), weights))\n# print(cw)","aa63c249":"# from sklearn.preprocessing import StandardScaler\n# rb = StandardScaler()\n# X_scaled= rb.fit_transform(X)","dc209c74":"# X_scaled_cleaned = pd.DataFrame(X_scaled, columns= ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99'])","f8bf3356":"# model = create_model()\n# model.summary()\n","a795a504":"\n\n# model = Sequential()\n# model.add(Dense(100,input_shape = (X_scaled_cleaned.shape[1],)))\n\n# model.add(Dense(100, activation='relu'))\n# model.add(keras.layers.BatchNormalization())\n# model.add(Dropout(0.3))\n\n# model.add(Dense(128, activation='relu'))\n# model.add(keras.layers.BatchNormalization())\n# model.add(Dropout(0.3))\n\n# # model.add(Dense(256, activation='relu'))\n# # model.add(keras.layers.BatchNormalization())\n# # model.add(Dropout(0.3))\n\n# # model.add(Dense(512, activation='relu'))\n# # model.add(keras.layers.BatchNormalization())\n# # model.add(Dropout(0.3))\n\n# # model.add(Dense(256, activation='relu'))\n# # model.add(keras.layers.BatchNormalization())\n# # model.add(Dropout(0.3))\n\n\n\n# # model.add(Dense(128, activation='relu'))\n# # model.add(keras.layers.BatchNormalization())\n# # model.add(Dropout(0.3))\n\n\n# model.add(Dense(64, activation='relu'))\n# model.add(keras.layers.BatchNormalization())\n# model.add(Dropout(0.3))\n\n\n# model.add(Dense(32, activation='relu'))\n# model.add(keras.layers.BatchNormalization())\n# model.add(Dropout(0.3))\n\n\n# model.add(Dense(16, activation='relu'))\n# model.add(keras.layers.BatchNormalization())\n# model.add(Dropout(0.3))\n\n# model.add(Dense(8, activation='relu'))\n# model.add(keras.layers.BatchNormalization())\n# model.add(Dropout(0.3))\n\n# model.add(Dense(1,activation=\"sigmoid\"))\n# model.summary()","14b70b30":"# !pip install scikit-learn  -U","2705d4fe":"# from sklearn.ensemble import RandomForestClassifier\n# rf = RandomForestClassifier()","71535614":"#HyperParameters\n\n# n_estimators = [int(i) for i in np.linspace(start = 100 ,stop= 1200, num =12)]\n# max_features = ['auto','sqrt']\n# max_depth = [int(i) for i in np.linspace(5,30,num =6) ]\n# min_samples_split = [2,3,10,15,100]\n# min_samples_leaf =[1,2,5,10]","017ebcc5":"# from sklearn.model_selection import RandomizedSearchCV\n# random_grid = {'n_estimators': n_estimators,\n#               'max_features': max_features,\n#               'max_depth': max_depth,\n#               'min_samples_split': min_samples_split,\n#               'min_samples_leaf': min_samples_leaf}\n# print(random_grid)","d9d92e4a":"#rf_random = RandomizedSearchCV(estimator=rf,param_distributions=random_grid,scoring='neg_mean_squared_error',n_iter =5,random_state=42,n_jobs =1,verbose =2)","0cbff6ca":"# # rf_random.fit(X_scaled_cleaned,y)\n# df_test = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\n","a4652f50":"# df_test[\"id\"] = pd.to_numeric(df_test['id'])\n# df_test.set_index('id',inplace =True)\n","4695796b":"\n# from sklearn.preprocessing import RobustScaler\n# test_scaled= RobustScaler().fit_transform(df_test)","73eb1e48":"#X_test = pd.DataFrame(test_scaled, columns= ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99'])","a98aeead":"# X_test_cleaned = X_test.drop(['f1', 'f2', 'f3', 'f5', 'f9', 'f14', 'f15', 'f16', 'f17', 'f21', 'f22', 'f23', 'f25', 'f26', 'f31', 'f36', 'f37', 'f38', 'f39', 'f47', 'f48', 'f49', 'f53', 'f54', 'f55', 'f59', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f71', 'f72', 'f75', 'f76', 'f78', 'f80', 'f82', 'f83', 'f84', 'f86', 'f87', 'f90', 'f91', 'f92', 'f93', 'f94', 'f97', 'f99'],axis =1)","b8072cf1":"# target = model.predict(X_test_cleaned)","5bc911fd":"# #  # IQR\n# Q1 = X_test.quantile(0.25)\n# Q3 = X_test.quantile(0.75)\n# IQR = Q3 - Q1\n# #  print(\"Old Shape: \", X_scaled.shape)\n# lower = Q1 - 1.5*IQR\n# upper = Q3 + 1.5*IQR\n# # X_scaled.where(upper[0], inplace = True)\n# # X_scaled.drop(lower[0], inplace = True)\n# # print(\"New Shape: \", X_scaled.shape)\n# X_clean_test = np.where((X_test >lower)&(X_test <upper),X_test, np.mean(X_test))\n","dbd27b24":"# X_t = pd.DataFrame(X_clean_test, columns= ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99'])","b7f3960d":"**CUSTOM ANN FOR BETTER AUC SCORE**\n\nA custom ANN was made using skip connections. Skip connections preserves the knowledge and avoid vanishing gradient problems in deep network. Also multiplication of features was considered instead of addition as addition badly hampers the correlation between features. Batch normalisation is used along with drop-out layers to prevent overfitting.","def82b20":"****ROBUST SCALER GAVE BETTER RESULTS THAN STANDARD SCALER AS IT PENALIZES OUTLIERS****","e4f21995":"LOADING DATA","0da6888c":"OUTLIER DETECTIONS IN FEATURES ","d9fa6b31":"PDF DISTRIBUTIONS OF ALL FEATURES","d1dbd6dd":"PDFs POST HANDLING THE OUTLIERS","8660832a":"****EXPLORING PDFs OF ALL FEATURES****\n\nConverted some of the distributions into normal one, but shockingly, after doing feature engineering , the AUC was limited to .70 . But as these features were passed just after scaling into ANN classifier, results were much better( 0.7493 auc) .Hence feature engineering is of no use on this synthetic dataset"}}