{"cell_type":{"a32c6296":"code","c9a4538b":"code","d29f7f66":"code","6283d9df":"code","180d848c":"code","a64b9c34":"code","75ef61e8":"code","e7fda900":"code","0e5987f5":"code","6ce94e8c":"code","60b8d17d":"code","3594ca1a":"code","27ee6522":"code","4f842733":"code","134b3da3":"code","c0a3ac97":"code","6ae6c20c":"code","5da544cf":"code","9535ce3a":"code","59625654":"code","ca94b25a":"code","4a31246c":"code","c090d5b0":"code","8ea54b56":"code","8516e5da":"code","63cc430a":"code","89dc4768":"code","46357822":"code","223324b1":"code","dbcd7ae6":"code","14fd5254":"code","aa482dec":"code","ff83222f":"code","28f3a7f2":"code","76c6f212":"code","531006cb":"code","8416cc31":"code","1ee71b7a":"code","3c32443d":"code","b3b0d7ce":"code","1c88002e":"code","a435bcd1":"code","8c254232":"code","66364485":"code","bd616498":"code","92bf5a93":"code","5faa5135":"code","ec09394d":"code","1ed60eda":"code","2507c29e":"code","b309db7f":"code","1c3c7961":"code","032c19a1":"code","87bb1ed1":"code","345019be":"code","0f41d0ab":"code","a69b49fc":"code","af1839af":"code","e703a88d":"code","9dc56879":"code","1bbdbe63":"code","8c8bed48":"markdown","a517a7fe":"markdown","eaa42aad":"markdown","8cf3c266":"markdown","3127156d":"markdown","38f0285a":"markdown","2f899fe6":"markdown","e2ecc709":"markdown","737da6a6":"markdown","c8a91d97":"markdown","1a923e07":"markdown"},"source":{"a32c6296":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport gc\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport gensim\nfrom gensim.models import KeyedVectors\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, roc_auc_score\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras import layers, losses, optimizers\nfrom transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification","c9a4538b":"df = pd.read_csv('..\/input\/yelp-coffee-reviews\/ratings_and_sentiments.csv')\n\ndf.head()","d29f7f66":"df.info()","6283d9df":"df.drop(['vibe_sent', 'hours_sent'], axis=1, inplace=True)\ndf.dropna(inplace=True)","180d848c":"data = df[['review_text', 'bool_HIGH']]\ndata['bool_HIGH'] = data['bool_HIGH'].astype('int')","a64b9c34":"sns.countplot(x='bool_HIGH', data=df);","75ef61e8":"def text_preprocessing(text, for_vec_models=False):\n    if for_vec_models:\n        text = text.lower()\n        text = re.sub('[^a-z]+', ' ', text)\n        text = text.strip()\n    else:\n        text = text.lower()\n        text = re.sub('[^a-z]+', ' ', text)\n        text = ' '.join(word for word in text.split() if word not in stopwords.words('english'))\n        text = ' '.join(PorterStemmer().stem(word) for word in text.split())\n        text = text.strip()\n    return text","e7fda900":"texts = data.review_text.apply(text_preprocessing)","0e5987f5":"SEED = 20\nTEST_SIZE = .2","6ce94e8c":"x = texts\ny = data['bool_HIGH']\n\nx_train, x_test, y_train, y_test = train_test_split(\n    x,\n    y,\n    test_size=TEST_SIZE,\n    random_state=SEED,\n    stratify=y\n)","60b8d17d":"def transform_text(tokenizer, train_text, test_text):\n    x_train_ = tokenizer.fit_transform(train_text)\n    x_test_ = tokenizer.transform(test_text)\n    return x_train_, x_test_","3594ca1a":"cv = CountVectorizer()\n\nx_train_, x_test_ = transform_text(cv, x_train, x_test)","27ee6522":"def get_score(model, train_set, test_set, get_roc_auc=False):\n    decimals = 5\n    model.fit(train_set[0], train_set[1])\n    y_pred = model.predict(test_set[0])\n    y_true = test_set[1]\n    f1 = f1_score(y_true, y_pred)\n    if get_roc_auc:\n        y_score = model.predict_proba(test_set[0])[:, 1]\n        roc_auc = roc_auc_score(y_true, y_score)\n        print('ROC AUC:', np.round(roc_auc, decimals))\n    print('F1 score:', np.round(f1, decimals))","4f842733":"rac = PassiveAggressiveClassifier(random_state=SEED)\n\nget_score(rac, (x_train_, y_train), (x_test_, y_test))","134b3da3":"mnb = MultinomialNB()\n\nget_score(mnb, (x_train_, y_train), (x_test_, y_test), True)","c0a3ac97":"rfc = RandomForestClassifier(random_state=SEED)\n\nget_score(rfc, (x_train_, y_train), (x_test_, y_test), True)","6ae6c20c":"del df, cv\n_ = gc.collect()","5da544cf":"tfidf = TfidfVectorizer()\n\nx_train_, x_test_ = transform_text(tfidf, x_train, x_test)","9535ce3a":"get_score(rac, (x_train_, y_train), (x_test_, y_test))","59625654":"get_score(mnb, (x_train_, y_train), (x_test_, y_test), True)","ca94b25a":"get_score(rfc, (x_train_, y_train), (x_test_, y_test), True)","4a31246c":"del tfidf, x_train_, x_test_, mnb\n_ = gc.collect()","c090d5b0":"w2v_model = KeyedVectors.load_word2vec_format(\n    '..\/input\/google-word2vec-pretrained-model\/GoogleNews-vectors-negative300.bin',\n    binary=True\n)","8ea54b56":"def average_feature_vector(sen, model, num_features):\n#     split words\n    words = sen.replace('\\n', ' ').replace(',', ' ').replace('.', ' ').split()\n#     create vector\n    feature_vec = np.zeros((num_features,), dtype='float32')\n    i = 0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, model[word])\n        except KeyError as error:\n            feature_vec\n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words) - i)\n    return feature_vec","8516e5da":"texts = data.review_text.apply(lambda x: text_preprocessing(x, True))","63cc430a":"word2vec_embeddings = np.zeros((len(list(texts)), 300),dtype=\"float32\")\n\nfor i in range(len(list(texts))):\n    word2vec_embeddings[i] = average_feature_vector(texts[i], w2v_model, 300)","89dc4768":"x = word2vec_embeddings\ny = data['bool_HIGH']","46357822":"x_train, x_test, y_train, y_test = train_test_split(\n    x,\n    y,\n    test_size=TEST_SIZE,\n    random_state=SEED,\n    stratify=y\n)","223324b1":"get_score(rac, (x_train, y_train), (x_test, y_test))","dbcd7ae6":"get_score(rfc, (x_train, y_train), (x_test, y_test), True)","14fd5254":"del w2v_model, word2vec_embeddings\n_ = gc.collect()","aa482dec":"def load_fasttext_model(path):\n    embeddings = {}\n    f = open(path, encoding='utf-8')\n    for line in f:\n        values = line.strip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings[word] = coefs\n    f.close()\n    return embeddings","ff83222f":"ft_model = load_fasttext_model('..\/input\/fasttext-300d-2m-commoncrawl\/crawl-300d-2M.vec')","28f3a7f2":"fasttext_embeddings = np.zeros((len(list(texts)), 300),dtype=\"float32\")\n\nfor i in range(len(list(texts))):\n    fasttext_embeddings[i] = average_feature_vector(texts[i], ft_model, 300)","76c6f212":"x = fasttext_embeddings\ny = data['bool_HIGH']","531006cb":"x_train, x_test, y_train, y_test = train_test_split(\n    x,\n    y,\n    test_size=TEST_SIZE,\n    random_state=SEED,\n    stratify=y\n)","8416cc31":"get_score(rac, (x_train, y_train), (x_test, y_test))","1ee71b7a":"get_score(rfc, (x_train, y_train), (x_test, y_test), True)","3c32443d":"del ft_model, fasttext_embeddings\n_ = gc.collect()","b3b0d7ce":"def load_glove_model(path):\n    embeddings = {}\n    with open(path, 'r') as f:\n        for line in f:\n            values = line.split(' ')\n            word = values[0]\n            vectors = np.asarray(values[1:], 'float32')\n            embeddings[word] = vectors\n    f.close()\n    return embeddings","1c88002e":"gl_model = load_glove_model('..\/input\/glove-300d-22m-commomcrawl\/glove.840B.300d.txt')","a435bcd1":"gl_embeddings = np.zeros((len(list(texts)), 300),dtype=\"float32\")\n\nfor i in range(len(list(texts))):\n    gl_embeddings[i] = average_feature_vector(texts[i], gl_model, 300)","8c254232":"x = gl_embeddings\ny = data['bool_HIGH']","66364485":"x_train, x_test, y_train, y_test = train_test_split(\n    x,\n    y,\n    test_size=TEST_SIZE,\n    random_state=SEED,\n    stratify=y\n)","bd616498":"get_score(rac, (x_train, y_train), (x_test, y_test))","92bf5a93":"get_score(rfc, (x_train, y_train), (x_test, y_test), True)","5faa5135":"del gl_model, gl_embeddings, rac, rfc\n_ = gc.collect()","ec09394d":"x = texts\ny = data['bool_HIGH']","1ed60eda":"x_train, x_val, y_train, y_val = train_test_split(\n    x,\n    y,\n    test_size=TEST_SIZE,\n    random_state=SEED,\n    stratify=y\n)","2507c29e":"tokenizer = Tokenizer()\n\ntrain_data = x_train\ntrain_labels = y_train\nval_data = x_val\nval_labels = y_val\n\ntokenizer.fit_on_texts(train_data)\n\ntrain_text = tokenizer.texts_to_sequences(train_data)\nval_text = tokenizer.texts_to_sequences(val_data)","b309db7f":"MAX_SEQ_LEN = 80\nVOC_SIZE = len(tokenizer.index_word) + 1\nBATCH_SIZE = 1024\n\nepochs = 10","1c3c7961":"train_text = tf.keras.preprocessing.sequence.pad_sequences(\n    train_text,\n    padding='post',\n    truncating='post',\n    maxlen=MAX_SEQ_LEN\n)\n\nval_text = tf.keras.preprocessing.sequence.pad_sequences(\n    val_text,\n    padding='post',\n    truncating='post',\n    maxlen=MAX_SEQ_LEN\n)","032c19a1":"class RNNModel(tf.keras.Model):\n    \n    def __init__(self):\n        super().__init__()\n        self.embedding = layers.Embedding(VOC_SIZE, 128, mask_zero=True)\n        self.bilstm1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))\n        self.bilstm2 = layers.Bidirectional(layers.LSTM(128, return_sequences=False))\n        self.dense1 = layers.Dense(128, activation='relu')\n        self.dense2 = layers.Dense(64, activation='relu')\n        self.dense3 = layers.Dense(1, activation='sigmoid')\n        \n    def call(self, inputs):\n        x = self.embedding(inputs)\n        x = self.bilstm1(x)\n        x = self.bilstm2(x)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        x = self.dense3(x)\n        return x","87bb1ed1":"model = RNNModel()\n\nmodel.compile(\n    optimizer=optimizers.Adam(),\n    loss='binary_crossentropy'\n)","345019be":"model.fit(\n    train_text,\n    train_labels,\n    validation_data=(val_text, val_labels),\n    epochs=epochs,\n    batch_size=BATCH_SIZE\n)","0f41d0ab":"del tokenizer, train_data, train_labels, val_data, val_labels, train_text, val_text, model\n_ = gc.collect()","a69b49fc":"tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')","af1839af":"train_encodings = tokenizer(list(x_train), truncation=True, padding=True)\nval_encodings = tokenizer(list(x_val), truncation=True, padding=True)","e703a88d":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    y_train\n))\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(val_encodings),\n    y_val\n))","9dc56879":"BATCH_SIZE = 16\nVAL_BATCH_SIZE = 64","1bbdbe63":"model.compile(\n    optimizer=optimizers.Adam(learning_rate=5e-5),\n    loss='binary_crossentropy'\n)\n\nmodel.fit(\n    train_dataset.shuffle(1000, seed=SEED).batch(BATCH_SIZE),\n    validation_data=val_dataset.batch(VAL_BATCH_SIZE),\n    epochs=epochs,\n    batch_size=BATCH_SIZE\n)","8c8bed48":"# 1. Bag of Words","a517a7fe":"# 2. TF-IDF","eaa42aad":"# 4. FastText","8cf3c266":"# Import libs and load data","3127156d":"**Thanks for watching!** If you got questions feel free to ask them or just comment this notebook. :)","38f0285a":"# 3. Word2Vec","2f899fe6":"# 6. Simple RNN","e2ecc709":"At the end of each model training I delete needless variables since it's import to save RAM for further executions.","737da6a6":"# 7. Fine-tuning transformer","c8a91d97":"# 5. GloVe","1a923e07":"Here I would show you several ways to solve classic sentiment analys problem using these methods:\n1. Bag of Words;\n2. TF-IDF;\n3. Word2Vec;\n4. FastText;\n5. GloVe;\n6. Simple RNN;\n7. Fine-tuning transformer"}}