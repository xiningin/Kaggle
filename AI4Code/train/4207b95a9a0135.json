{"cell_type":{"d5dd3ee2":"code","c07aabd7":"code","fa457954":"code","1d256c9d":"code","a14a03f9":"code","23edc2e6":"code","36811d31":"code","52b31eec":"code","f8aa6339":"code","2aa8c06a":"code","6a7ff981":"code","42afbf39":"code","18ae35ff":"code","c6a13155":"code","e845e5a8":"code","bc9a2ff9":"code","bbb96911":"code","236ffc96":"code","1b36d855":"code","815224ac":"code","3146da48":"code","f712c8f9":"code","e204a353":"code","433197d8":"code","45f70d39":"code","6b9417a7":"code","cc3f94c2":"code","1158aa5c":"code","7ea3557d":"code","5f623733":"code","9083da1f":"code","0b2f3da3":"code","e3306859":"code","8ed6b0b6":"code","1059af8b":"code","50576998":"code","d4039fd9":"code","5fd5767e":"code","d6aac397":"code","fd8fca35":"code","536cef85":"code","c202a753":"code","b9845226":"code","7e367c86":"code","b73820c6":"code","3e49dd1e":"code","36951ea9":"code","79185be1":"code","ec3fad6f":"markdown","69c00d98":"markdown","cb97a36e":"markdown","3dbb50c9":"markdown","1ba9dc01":"markdown","20ff6775":"markdown","658d46ed":"markdown","78bf81a4":"markdown","65185605":"markdown","155f16aa":"markdown","7cf72665":"markdown","f803a3e8":"markdown","8f64787e":"markdown"},"source":{"d5dd3ee2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c07aabd7":"# import required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport seaborn as sns\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection","fa457954":"# load dataset in notebook by giving path\ntrain_path = '..\/input\/nlp-getting-started\/train.csv'\ntest_path ='..\/input\/nlp-getting-started\/test.csv'\nsubmission_path = '..\/input\/nlp-getting-started\/sample_submission.csv'","1d256c9d":"# read dataset\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\nsubmission_sample = pd.read_csv(submission_path)","a14a03f9":"# first 5 lines of train dataset\ntrain_df.head(5)","23edc2e6":"# first 5 lines of test dataset\ntest_df.head(5)","36811d31":"# first 5 lines of submission sample dataset\nsubmission_sample.head(5)","52b31eec":"# shape of dataset\nprint(\"Total number of rows in train dataset are \",train_df.shape[0],'and total number of columns in train dataset are',train_df.shape[1])\nprint(\"Total number of rows in test dataset are \",test_df.shape[0],'and total number of columns in test dataset are',test_df.shape[1])","f8aa6339":"# basic info of train dataset\ntrain_df.info()","2aa8c06a":"#some basic info of test data\ntest_df.info()","6a7ff981":"#null values in train dataset\ntrain_df.isnull().sum()","42afbf39":"#null values in test dataset\ntest_df.isnull().sum()","18ae35ff":"train_df.isna().sum().plot(kind=\"bar\")\nplt.title(\"no of null values in train data\")\nplt.show()","c6a13155":"test_df.isna().sum().plot(kind=\"bar\")\nplt.title(\"no of null values in test data\")\nplt.show()","e845e5a8":"# drop location and keyword column\ntrain_df = train_df.drop(['location','keyword'],axis=1)\ntest_df = test_df.drop(['location','keyword'],axis=1)","bc9a2ff9":"# train dataset after dropping location and keyword columns\ntrain_df.head()","bbb96911":"# test dataset after dropping location and keyword columns\ntest_df.head()","236ffc96":"# finding percentage of 0 and 1 target\nreal_tweets = len(train_df[train_df[\"target\"] == 1])\nreal_tweets_percentage = real_tweets\/train_df.shape[0]*100\nfake_tweets_percentage = 100-real_tweets_percentage\n\n#print\nprint(\"Real tweets percentage: \",real_tweets_percentage)\nprint(\"Fake tweets percentage: \",fake_tweets_percentage)","1b36d855":"# plot of traget values\nsns.countplot(x='target',data=train_df)","815224ac":"length_train = train_df['text'].str.len() \nlength_test = test_df['text'].str.len() \nplt.hist(length_train, label=\"train_tweets\") \nplt.hist(length_test, label=\"test_tweets\") \nplt.legend() \nplt.show()","3146da48":"# disaster tweets\ndisaster_tweets = train_df[train_df['target'] ==1 ]['text']\nfor i in range(1,10):\n    print(disaster_tweets[i])","f712c8f9":"# non-disaster tweets\nnon_disaster_tweets = train_df[train_df['target'] !=1 ]['text']","e204a353":"# word cloud of disaster and non-disaster tweets\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","433197d8":"# cleaning the text\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and train datasets\ntrain_df['text'] = train_df['text'].apply(lambda x: clean_text(x))\ntest_df['text'] = test_df['text'].apply(lambda x: clean_text(x))\n\n# updated text\ntrain_df['text'].head()","45f70d39":"tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain_df['text'] = train_df['text'].apply(lambda x:tokenizer.tokenize(x))\ntest_df['text'] = test_df['text'].apply(lambda x:tokenizer.tokenize(x))\ntrain_df['text'].head()","6b9417a7":"# stopwords\nstopwords.words('english')","cc3f94c2":"len(stopwords.words('english'))","1158aa5c":"# removing stopwords\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words \ntrain_df['text'] = train_df['text'].apply(lambda x : remove_stopwords(x))\ntest_df['text'] = test_df['text'].apply(lambda x : remove_stopwords(x))\ntest_df.head()","7ea3557d":"# lemmatization\nlem = WordNetLemmatizer()\ndef lem_word(x):\n    return [lem.lemmatize(w) for w in x]","5f623733":"train_df['text'] = train_df['text'].apply(lem_word)\ntest_df['text'] = test_df['text'].apply(lem_word)","9083da1f":"train_df['text'][:10]","0b2f3da3":"def combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain_df['text'] = train_df['text'].apply(lambda x : combine_text(x))\ntest_df['text'] = test_df['text'].apply(lambda x : combine_text(x))\ntrain_df['text']\ntrain_df.head()","e3306859":"count_vectorizer = CountVectorizer()\ntrain_vector = count_vectorizer.fit_transform(train_df['text'])\ntest_vector = count_vectorizer.transform(test_df['text'])\nprint(train_vector[0].todense())","8ed6b0b6":"tfidf = TfidfVectorizer(min_df = 2,max_df = 0.5,ngram_range = (1,2))\ntrain_tfidf = tfidf.fit_transform(train_df['text'])\ntest_tfidf = tfidf.transform(test_df['text'])","1059af8b":"xgb_param = xgb.XGBClassifier(max_depth=5,n_estimators=500,colsample_bytree=0.8,nthread=10,learning_rate=0.05)","50576998":"scores_vector = model_selection.cross_val_score(xgb_param,train_vector,train_df['target'],cv=5,scoring='f1')\nscores_vector","d4039fd9":"scores_tfidf = model_selection.cross_val_score(xgb_param,train_tfidf,train_df['target'],cv=5,scoring='f1')\nscores_tfidf","5fd5767e":"xgb_param.get_params()","d6aac397":"mnb = MultinomialNB(alpha = 2.0)\nscores_vector = model_selection.cross_val_score(mnb,train_vector,train_df['target'],cv = 10,scoring = 'f1')\nprint(\"score:\",scores_vector)\nscores_tfidf = model_selection.cross_val_score(mnb,train_tfidf,train_df['target'],cv = 10,scoring = 'f1')\nprint(\"score of tfidf:\",scores_tfidf)","fd8fca35":"mnb.get_params()","536cef85":"lg = LogisticRegression(C = 1.0)\nscores_vector = model_selection.cross_val_score(lg, train_vector, train_df[\"target\"], cv = 5, scoring = \"f1\")\nprint(\"score:\",scores_vector)\nscores_tfidf = model_selection.cross_val_score(lg, train_tfidf, train_df[\"target\"], cv = 5, scoring = \"f1\")\nprint(\"score of tfidf:\",scores_tfidf)","c202a753":"lg.get_params()","b9845226":"mnb.fit(train_tfidf, train_df[\"target\"])\ny_pred = mnb.predict(test_tfidf)","7e367c86":"y_pred","b73820c6":"submission_df2 = pd.DataFrame({'Id':test_df['id'],'target':y_pred})","3e49dd1e":"submission_df2.to_csv('submission_df2.csv',index=False)","36951ea9":"submission_df2 = pd.read_csv('submission_df2.csv')","79185be1":"submission_df2.head()","ec3fad6f":"# Submission","69c00d98":"# Logistic Regression","cb97a36e":"# Exploratory Data Analysis","3dbb50c9":"# TF-IDF\n\nIt stands for Term Frequency-Inverse document frequency.It is a techinque to quantify a word in documents,we generally compute a weight to each word which signifies the importance of the word which signifies the importance of the word in the document and corpus","1ba9dc01":"# XGB Classifier","20ff6775":"# Lemmatization \n\nLemmatization is the process of grouping  together the different inflected forms of a word so they can be analyzed as a single item. \n\nExamples of lemmatization:\n\n1.**playing** ,**plays** and **played** all these 3 letters will be converted to **play** after lemmatization\n\n2.**change** , **changing** , **changes** , **changed** and **changer** all these letters will be converted to **change** after lemmatization","658d46ed":"# Prediction","78bf81a4":"# Stopwords\n\nStopwords are those english words which do not add much meaning to a sentence.They are very commonly used words and we do not required those words.\nSo we can remove those stopwords","65185605":"**Now we will prepare submission file**","155f16aa":"# Count-Vector\n\nCountVectorizer is used to transform a given text into a vector on the basis of the frequency(count) of each word that occurs in the entire text.It involves counting the number of occurences each words appears in a document(text) ","7cf72665":"# MultiNomial Naive Bayes","f803a3e8":"# Data Cleaning","8f64787e":"**We do not need location and keyword columns.So we are going to drop these two columns**"}}