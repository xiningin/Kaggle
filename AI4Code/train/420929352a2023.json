{"cell_type":{"641b25f1":"code","e0176464":"code","def0431a":"code","939fcc72":"code","ff1a5c39":"code","e1604aad":"code","f44388e2":"code","b9912909":"code","ce4e4769":"code","ff569560":"code","f414b383":"code","cb543f99":"code","023cedec":"code","7c5726bb":"code","44558cb6":"markdown","051314b8":"markdown","a7f0db36":"markdown","0678149e":"markdown","08a83104":"markdown","4ab7f7a1":"markdown","2add77b5":"markdown","f3117253":"markdown","10d628d3":"markdown","92a117e3":"markdown","2126a731":"markdown","479b9cee":"markdown","4095ec12":"markdown"},"source":{"641b25f1":"# TensorFlow related\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import * \n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\n\n# HuggingFace related\nfrom transformers import DistilBertTokenizerFast\nfrom transformers import TFDistilBertModel\n\nfrom kaggle_secrets import UserSecretsClient","e0176464":"# Get the latest version of W&B\n!pip install -q wandb\n\n# Weights and Biases related imports\nimport wandb\nfrom wandb.keras import WandbCallback\n\n# W&B login - please visit wandb.ai\/authorize to get your auth key\nwandb.login()","def0431a":"CONFIG = dict(\n    # Dataset related \n    num_splits = 5, \n    \n    # Model related\n    model_name = 'DistilBERT',\n    max_token_length = 256,\n    \n    # Training related\n    batch_size = 64,\n    epochs = 100,\n    init_lr = 1e-4,\n    earlys_patience = 10,\n    reduce_lr_plateau = 5,\n    \n    # Misc\n    seed = 42,\n    wandb_kernel = True,\n    competition = 'commonlit'\n)\n\nsave_dir = 'trained\/'\nos.makedirs(save_dir, exist_ok=True)","939fcc72":"# Ref: https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\ndef create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","ff1a5c39":"# read training data\ndf = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\n\n# create folds\ndf = create_folds(df, num_splits=CONFIG['num_splits'])\ndf.head()","e1604aad":"# Use the tokenizer of your choice\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n# Save the tokenizer so that you can download the files and move it to a Kaggle dataset.\ntokenizer.save_pretrained(save_dir)","f44388e2":"def get_train_val_split(fold_num):\n    # Get training split\n    train_df = df.loc[df['kfold']!=fold_num]\n    train_df = train_df[['excerpt', 'target']]\n    \n    # Get validation split\n    val_df = df.loc[df['kfold']==fold_num]\n    val_df = val_df[['excerpt', 'target']]\n    \n    # Extract texts and labels.\n    train_text, train_label = list(train_df.excerpt.values), list(train_df.target.values)\n    val_text, val_label = list(val_df.excerpt.values), list(val_df.target.values)\n\n    return train_text, train_label, val_text, val_label\n\ntrain_text, train_label, val_text, val_label = get_train_val_split(0)","b9912909":"train_encodings = tokenizer(train_text, truncation=True, padding=True, max_length=CONFIG['max_token_length'])\nval_encodings = tokenizer(val_text, truncation=True, padding=True, max_length=CONFIG['max_token_length'])","ce4e4769":"AUTOTUNE = tf.data.AUTOTUNE\n\n# Note that some tokenizers also returns 'token_id'. Modify this function accordingly. \n@tf.function\ndef parse_data(from_tokenizer, target):\n    input_ids = from_tokenizer['input_ids']\n    attention_mask = from_tokenizer['attention_mask']\n    \n    target = tf.cast(target, tf.float32)\n    \n    return {'input_ids': input_ids,\n            'attention_mask': attention_mask}, target\n\n# Utility function to build dataloaders\ndef get_dataloaders(train_encodings, train_label, val_encodings, val_label):\n    trainloader = tf.data.Dataset.from_tensor_slices((dict(train_encodings), list(train_label)))\n    validloader = tf.data.Dataset.from_tensor_slices((dict(val_encodings), list(val_label)))\n\n    trainloader = (\n        trainloader\n        .shuffle(1024)\n        .map(parse_data, num_parallel_calls=AUTOTUNE)\n        .batch(CONFIG['batch_size'])\n        .prefetch(AUTOTUNE)\n    )\n\n    validloader = (\n        validloader\n        .map(parse_data, num_parallel_calls=AUTOTUNE)\n        .batch(CONFIG['batch_size'])\n        .prefetch(AUTOTUNE)\n    )\n    \n    return trainloader, validloader\n\ntrainloader, validloader = get_dataloaders(train_encodings, train_label, val_encodings, val_label)","ff569560":"# Visualize a batch of data\nnext(iter(trainloader))","f414b383":"# You can use a Transformer model of your choice.\ntransformer_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')","cb543f99":"def CommonLitModel():\n    # Input layers\n    input_ids = Input(shape=(CONFIG['max_token_length'],), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(CONFIG['max_token_length'],), dtype=tf.int32, name=\"attention_mask\")\n    \n    # Transformer backbone to extract features\n    sequence_output = transformer_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n    clf_output = sequence_output[:, 0, :]\n    \n    # Dropout to regularize \n    clf_output = Dropout(0.1)(clf_output)\n    \n    # Output layer with linear activation as we are doing regression. \n    out = Dense(1, activation='linear')(clf_output)\n    \n    # Build model \n    model = Model(inputs=[input_ids, attention_mask], outputs=out)\n    \n    return model\n\n# Sanity check model\ntf.keras.backend.clear_session()\nmodel = CommonLitModel()\nmodel.summary()","023cedec":"# Early stopping \nearlystopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=CONFIG['earlys_patience'], verbose=0, mode='min',\n    restore_best_weights=True\n)\n\n# Reduce LR on Plateau\nreducelrplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=CONFIG['reduce_lr_plateau']\n)","7c5726bb":"for fold in range(CONFIG['num_splits']):\n    # 1. Get the training and validation dataset.\n    train_text, train_label, val_text, val_label = get_train_val_split(fold)\n    \n    # 2. Pass the dataset to the tokenizer.\n    train_encodings = tokenizer(train_text, truncation=True, padding=True, max_length=CONFIG['max_token_length'])\n    val_encodings = tokenizer(val_text, truncation=True, padding=True, max_length=CONFIG['max_token_length'])\n    \n    # 3. Prepare training and validation dataloader.\n    trainloader, validloader = get_dataloaders(train_encodings, train_label, val_encodings, val_label)\n    \n    # 4. Initialize model\n    tf.keras.backend.clear_session()\n    model = CommonLitModel()\n    \n    # Compile\n    optimizer = tf.keras.optimizers.Adam(lr=1e-5)\n    model.compile(optimizer, loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])    \n    \n    # Initialize Weights and Biases run\n    run = wandb.init(project='commonlit', \n                     config=CONFIG,\n                     group='DistilBERT-K-Fold',\n                     job_type='train_kfold')\n    \n    # 5. Train the model\n    _ = model.fit(trainloader, \n              epochs=CONFIG['epochs'], \n              validation_data=validloader,\n              callbacks=[WandbCallback(),\n                         reducelrplateau,\n                         earlystopper])\n    \n    # 6. Evaluate on validation dataset.\n    loss, rmse = model.evaluate(validloader)\n    wandb.log({'valid_rmse': rmse})\n    \n    # 7. Save model\n    model.save(f'{save_dir}\/distil-bert_{fold}')\n    \n    # Close W&B run\n    run.finish()","44558cb6":"# \ud83d\udc24 Build Model","051314b8":"3. We will use `tf.data` wrap the encoding and label. `tf.data` can create highly efficient input pipeline. In case your tokenizer's output also has a key `token_ids` consider modifying the `parse_data` appropriately. ","a7f0db36":"## 2. Create Dataloader\n\n**Two words on Tokenization**: Tokenizing a text is splitting it into words or subwords, which then are converted to ids through a look-up table. The conversion of tokens to ids through a look-up table depends on the vocabulary(the set of all unique words and tokens used) which depends on the dataset, the task, and the resulting pre-trained model. **HuggingFace tokenizer automatically downloads the vocab used during pretraining or fine-tuning a given model. We need not create our own vocab from the dataset for fine-tuning.**","0678149e":"Being a novice in Natural Language Processing based DL tasks, this Kaggle competition seems like a great opportunity to break the ice and be comfortable with this subdomain of Deep Learning. \n\nHaving said that I have some prior experience using HuggingFace Transformers in TensorFlow\/Keras ecosystem. I have also written this W&B report on [How to Fine-Tune HuggingFace Tranformer with W&B?](https:\/\/wandb.ai\/ayush-thakur\/huggingface\/reports\/How-to-Fine-Tune-HuggingFace-Tranformer-with-W-B---Vmlldzo0MzQ2MDc) that might come in handy to few.\n\nThis kernel is about training a HuggingFace transformer using TensorFlow\/Keras. I have tried to make it as intuitive as possible for a regular Keras users and is trying out NLP or HuggingFace for the first time. This Kernel can be divided into few blocks:\n\n* **Import and Setups** - here we will import relevant libraries and setup Weights and Biases related steps for tracking experiments. \n\n* **Hyperparameters** - configuration dictionary for all hyperparameters.\n\n* **Prepare Dataset** - here we will create K-fold split of the training data based on this easy to understand [kernel](https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds) by [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek). This is followed by building an input pipeline using `tf.data` API.\n\n* **Build Model** - here we will build our model definition. Note that you can use any Transformer of your choice. \n\n* **Train with W&B** - here we do a K (5) fold training and will use Weights and Biases for experiment tracking. \n\n* **Evaluate** - here we will evaluate the model for local CV score.","08a83104":"# \ud83d\udd28 Prepare Dataset","4ab7f7a1":"2. Pass in the raw text to the tokenizer. The output of this process is a dictionary `*_encodings` with `input_ids` and `attention_mask` as keys. For some tokenizers you will have `token_ids` as another key. Please modify the training pipeline and model accordingly. It will be as easy as cooking noodles. \n","2add77b5":"## 1. Create Folds\n\nThis is based on the [Step 1: Create Folds](https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds) kernel by [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek). Even though the dataset is small this competition is not that easy to crack.","f3117253":"> \ud83d\udccc The name of the tokenizer will depend on the choice of your Transformer model. ","10d628d3":"# \ud83e\uddf0 Imports and Setups","92a117e3":"# \ud83d\ude84 Train with W&B\n\nSince it's going to be a K-fold training we will have a for loop that will iterate `CONFIG['num_splits']` number of times. In the loop the following steps will be repeated:\n\n1. Get the training and validation dataset.\n2. Pass the dataset to the tokenizer.\n3. Prepare training and validation dataloader.\n4. Initialize model.\n5. Train the model.\n6. Evaluate on validation dataset.\n7. Save model.\n\nWe will use W&B to log all the metrics. We will use a `group` argument when initializing a W&B run (`wandb.init`) that will enable us to group all the runs in the W&B dashboard. \n\n### [Check out the W&B dashboard $\\rightarrow$](https:\/\/wandb.ai\/ayush-thakur\/commonlit?workspace=user-ayush-thakur)\n![img](https:\/\/i.imgur.com\/WYo0b4q.gif)\n","2126a731":"# \ud83d\udcc0 Hyperparameters","479b9cee":"### For explaination purposes\n\nFew of the cells below are just for explaination pusposes. Since we are training using K-Folds, the input pipeline will be created inside the for loop. The cells below will give you an insight on how I am using `tf.data` and HuggingFace's Tokenizer to build the input pipeline.\n\n1. Here (K-1)th split is taken as validation data. We will train on the rest of the data. ","4095ec12":"> \ud83d\udccc Learn more about why and how to use Weights and Biases in this kernel: [Experiment Tracking with Weights and Biases](https:\/\/www.kaggle.com\/ayuraj\/experiment-tracking-with-weights-and-biases)"}}