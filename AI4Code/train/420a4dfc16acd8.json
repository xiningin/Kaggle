{"cell_type":{"496dc21b":"code","84d31bb0":"code","48f1b267":"code","23e220b5":"code","834cb7ec":"code","be552352":"code","bb4ebb7a":"code","ae01f741":"code","ed9cc687":"code","5893cfbb":"code","bbb3b24c":"code","aafceef6":"code","bbaac015":"code","fa7ab904":"code","3ee9c234":"code","2ce2c4ff":"code","df7f1bea":"code","fb2046c5":"code","775198ad":"code","4a214533":"code","6dd9bce1":"code","b17d287b":"code","5225a57c":"code","4e574659":"code","69dcd50b":"code","a0decdaa":"code","47028273":"code","9b7cef87":"code","4d3cfd64":"code","dcf3d3f6":"code","976e73f6":"code","f4346cc7":"code","a3be918f":"code","5b386f88":"code","8fd34fce":"code","87f365b2":"code","30dfcdd3":"code","d898d0e8":"code","f278ddc4":"code","69cd2564":"code","bd53b0a6":"code","0eae80d7":"code","489ebb85":"code","eb1e1209":"code","1f1c97ed":"code","472b0998":"code","4ddd0445":"code","28b415a9":"code","a21ebd11":"code","b571ad0f":"code","b7db2259":"code","ec70b6a2":"code","98b8e6a2":"code","bdee500d":"code","46735bb4":"code","9ac61cd4":"code","9820b464":"code","0cfda1d8":"code","9ed0c2c3":"code","5c1ce9a1":"code","67f44cab":"code","6a748d0f":"code","53361699":"code","cacf516c":"code","8dca0f9f":"code","971bebdd":"code","4050a163":"code","a4bc8164":"code","65ac7f98":"code","bafe815d":"markdown","49910ca3":"markdown","a9ba0cfe":"markdown","6c982eb3":"markdown","b1ac67a6":"markdown","3691ac90":"markdown","0bd1178b":"markdown","b79c21fb":"markdown","c4b5cd43":"markdown","7ea2d5ee":"markdown","2ae0825a":"markdown","e497002a":"markdown","be109bcd":"markdown","068dcd4f":"markdown","ff850dd1":"markdown","11fe1752":"markdown","e8c13edf":"markdown","88f3b7c8":"markdown","3f42ecf1":"markdown","3247240f":"markdown","0bf4467f":"markdown","49161845":"markdown","c75824ea":"markdown","65321158":"markdown","e43de0c0":"markdown","5fcbfcf7":"markdown","c3ba7f06":"markdown","3fd09d41":"markdown","af511872":"markdown","4555eecc":"markdown","08c1769a":"markdown","60763c3a":"markdown","fb3e953c":"markdown","965e8ab6":"markdown","6c215519":"markdown","51c532d6":"markdown","06a32232":"markdown","06418af5":"markdown","e672b803":"markdown","d7259e69":"markdown","77bd3417":"markdown","ebbbaf1a":"markdown","8a25655e":"markdown","b8e83aeb":"markdown","3cfa4c3b":"markdown"},"source":{"496dc21b":"# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns #for better and easier plots\n%matplotlib inline\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","84d31bb0":"app_df = pd.read_csv(\"..\/input\/application_train.csv\")","48f1b267":"app_df.shape","23e220b5":"app_df.head()","834cb7ec":"app_test_df = pd.read_csv(\"..\/input\/application_test.csv\")","be552352":"app_test_df.shape","bb4ebb7a":"#let's create a function to check for null values, calculate the percentage relative to the total size\n#only shows the null values present in the dataset\ndef missing_values_calculate(trainset): \n    nulldata = (trainset.isnull().sum() \/ len(trainset)) * 100\n    nulldata = nulldata.drop(nulldata[nulldata == 0].index).sort_values(ascending=False)\n    ratio_missing_data = pd.DataFrame({'Ratio' : abs(nulldata)})\n    return ratio_missing_data.head(30)","ae01f741":"def remove_missing_columns(train, test, threshold = 90): #threshold is set by default at 90%\n    # Calculate missing stats for train and test (remember to calculate a percent!)\n    train_miss = pd.DataFrame(train.isnull().sum())\n    train_miss['percent'] = 100 * train_miss[0] \/ len(train)\n    \n    test_miss = pd.DataFrame(test.isnull().sum())\n    test_miss['percent'] = 100 * test_miss[0] \/ len(test)\n    \n    # list of missing columns for train and test\n    missing_train_columns = list(train_miss.index[train_miss['percent'] > threshold])\n    missing_test_columns = list(test_miss.index[test_miss['percent'] > threshold])\n    \n    # Combine the two lists together\n    missing_columns = list(set(missing_train_columns + missing_test_columns))\n    \n    # Print information\n    print('There are %d columns with greater than %d%% missing values.' % (len(missing_columns), threshold))\n    \n    # Drop the missing columns and return\n    train = train.drop(columns = missing_columns)\n    test = test.drop(columns = missing_columns)\n    \n    return train, test","ed9cc687":"missing_values_calculate(app_df)","5893cfbb":"app_df.info()","bbb3b24c":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","aafceef6":"app_df.head()","bbaac015":"(app_df['DAYS_BIRTH'] \/ -365).describe()","fa7ab904":"app_df['DAYS_EMPLOYED'].describe()","3ee9c234":"app_df[app_df['DAYS_EMPLOYED'] >= 300000].describe() ","2ce2c4ff":"app_df['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napp_test_df['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True) #imputing the testing set as well...","df7f1bea":"corr = app_df.corr()['TARGET'].sort_values(ascending=False)\n\ncorr.head(10) #looking at the first 10 positively correlated","fb2046c5":"corr.tail(10).sort_values() #now looking at the first 10 negatively correlated","775198ad":"plt.style.use('dark_background')\nplt.figure(figsize = (10, 8))\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_df.loc[app_df['TARGET'] == 0, 'DAYS_BIRTH'] * -1\/ 365, label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_df.loc[app_df['TARGET'] == 1, 'DAYS_BIRTH'] * -1\/ 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","4a214533":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_df[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","6dd9bce1":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_df.loc[app_df['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_df.loc[app_df['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)\n    ","b17d287b":"app_df.shape","5225a57c":"#let's use the Imputer to fill the NAN values with the median value\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,Imputer, RobustScaler\n\n#imputing all NaN value\npipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")), \n        #('scale', MinMaxScaler(feature_range = (0, 1))),\n        ('robustScaler', RobustScaler()),\n])","4e574659":"from sklearn.metrics import precision_score, recall_score, accuracy_score,confusion_matrix,classification_report,f1_score, roc_auc_score\nimport time #implementing in this function the time spent on training the model\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV,cross_val_score,train_test_split, KFold\nimport gc\n\nnfolds = 5\nfolds = KFold(n_splits=nfolds, shuffle=True,random_state=42)\n\n\n#Generic function for making a classification model and accessing performance:\ndef classification_model(train, train_labels, test_set, pipeline, params={}, fold=folds, plot_confusion_matrix=False, model=None, GridSearch=False, plot_features_importances=False):\n    \n    time_start = time.perf_counter() #start counting the time\n    #creating our validation set out of the training set and labels provided\n    X_train, x_val, y_train, y_val = train_test_split(train, train_labels, test_size=0.1, random_state=42)\n    X_train = pipeline.fit_transform(X_train) #fiting and transforming the dataset using the pipeline provided\n    x_val = pipeline.fit_transform(x_val)\n    \n    test_sub = np.zeros(test_set.shape[0])\n    test_set = pipeline.fit_transform(test_set)\n    \n    predict_val = np.zeros(train.shape[0])\n    score = {}\n    \n    if model != None: grid_model = GridSearchCV(model, params,verbose=1, cv=3) #initializing the grid search model\n\n    if GridSearch:\n        grid_model.fit(X_train, y_train)\n        score_grid = grid_model.best_score_\n        \n        #predicting using the model that has been trained above\n        \n        predict_val = grid_model.predict(x_val)\n        score['Accuracy'] = (accuracy_score(y_val, predict_val))\n        score['Precision'] = (precision_score(y_val, predict_val))\n        score['F1 score'] = (f1_score(y_val, predict_val))\n        score['ROC AUC'] = (roc_auc_score(y_val, predict_val))\n        \n        print(\"Model Report\")\n\n        print(\"Accuracy: \"+ str(score[\"Accuracy\"]))\n        print(\"Precision: \"+ str(score[\"Precision\"]))\n        print(\"F1 score: \"+ str(score[\"F1 score\"]))\n        print(\"ROC AUC: \"+ str(score[\"ROC AUC\"]))\n        print('\\n')\n        \n        print(\"         -------Classification Report----------\")\n        print(classification_report(y_val, predict_val))\n    \n        test_sub = grid_model.predict(test_set) \n        \n    else:\n        model = lgb.LGBMClassifier(**params, n_estimators = 5000, nthread = 4, n_jobs = -1)\n\n        for n, (index, val_index) in enumerate(folds.split(train)):\n            \n            print('Starting Fold number: %d' %n)\n            X, X_val = train.values[index], train.values[val_index]\n            Y, Y_val = train_labels[index], train_labels[val_index]\n            X = pipeline.fit_transform(X)\n            X_val = pipeline.fit_transform(X_val)\n            \n            model.fit(X, Y, \n                    eval_set=[(X, Y), (X_val, Y_val)],\n                    verbose=1000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_val)\n            test_temp = model.predict(test_set, num_iteration=model.best_iteration_)\n            test_sub += test_temp               \n            if score == {}:\n                score['Accuracy'] = accuracy_score(Y_val, y_pred_valid)\n                score['Precision']= precision_score(Y_val, y_pred_valid)\n                score['F1 score'] = f1_score(Y_val, y_pred_valid)\n                score['ROC AUC'] = roc_auc_score(Y_val, y_pred_valid)\n            else:\n                score['Accuracy'] += accuracy_score(Y_val, y_pred_valid)\n                score['Precision'] += precision_score(Y_val, y_pred_valid)\n                score['F1 score'] += f1_score(Y_val, y_pred_valid)\n                score['ROC AUC'] += roc_auc_score(Y_val, y_pred_valid)\n                        \n        test_sub \/= nfolds\n                        \n        print(\"Model Report\")\n\n        print(\"Accuracy(avg across folds): \"+ str(score[\"Accuracy\"]\/nfolds))\n        print(\"Precision(avg across folds): \"+ str(score[\"Precision\"]\/nfolds))\n        print(\"F1 score(avg across folds): \"+ str(score[\"F1 score\"]\/nfolds))\n        print(\"ROC AUC(avg across folds): \"+ str(score[\"ROC AUC\"]\/nfolds))\n        print('\\n')\n\n        \n    #################### PLOTTING FEATURES IMPORTANCE ####################\n    \n    # Sort features according to importance\n    if plot_features_importances:\n        if GridSearch:\n            # Extract feature importances\n            feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': grid_model.best_estimator_.feature_importances_})\n        else:\n            feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': model.feature_importances_})\n        \n        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index()\n\n        # Normalize the feature importances to add up to one\n        feature_importances['importance_normalized'] = feature_importances['importance'] \/ feature_importances['importance'].sum()\n\n        # Make a horizontal bar chart of feature importances\n        plt.figure(figsize = (10, 6))\n        ax = plt.subplot()\n\n        # Need to reverse the index to plot most important on top\n        ax.barh(list(reversed(list(feature_importances.index[:15]))), \n                feature_importances['importance_normalized'].head(15), \n                align = 'center', edgecolor = 'k')\n\n        # Set the yticks and labels\n        ax.set_yticks(list(reversed(list(feature_importances.index[:15]))))\n        ax.set_yticklabels(feature_importances['feature'].head(15))\n\n        # Plot labeling\n        plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    \n    #################### PLOTTING CONFUSION MATRIX #######################\n    \n    if plot_confusion_matrix:\n        fig, ax = plt.subplots(figsize=(8,8)) #setting the figure size and ax\n        mtx = confusion_matrix(y_val, predict_val)\n        sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  cbar=True, ax=ax) #create a heatmap with the values of our confusion matrix\n        plt.ylabel('true label')\n        plt.xlabel('predicted label')\n\n    \n    time_end = time.perf_counter() #end of counting the time\n    \n    total_time = time_end-time_start #total time spent during training and cross_validation\n    \n    print(\"Amount of time spent during training the model and cross validation: %4.3f seconds\" % (total_time))\n    # Clean up memory\n    gc.enable()\n    del model, X_train, x_val, y_train, y_val,score, total_time, time_end, time_start,predict_val,test_set\n    gc.collect()\n                        \n    return test_sub","69dcd50b":"bureau_df = pd.read_csv('..\/input\/bureau.csv')#loading the dataset\nbureau_df.shape #checking the shape of our dataset","a0decdaa":"bureau_df.head()","47028273":"num_of_previous_credits = bureau_df.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'}) \n#grouping by the client's ID and counting the number of credits using the ID bureau. then,renaming the column to something more informative\nnum_of_previous_credits.head()","9b7cef87":"app_df = app_df.merge(num_of_previous_credits, on = 'SK_ID_CURR', how = 'left')\napp_df['previous_loan_counts'] = app_df['previous_loan_counts'].fillna(0) #filling all the clients that doesn't have past loans with 0","4d3cfd64":"#getting only the relevant columns that we would like to see the correlation, in this case, getting previous columns plus the new feature\nrelevant_columns = app_df[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'previous_loan_counts']]\ncorr = relevant_columns.corr()\n\n# Heatmap of correlations\nsns.heatmap(corr, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","dcf3d3f6":"## CREDITS TO WILL KOERSEHN ##\n# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['TARGET'].corr(df[var_name])\n    \n    # Calculate medians for repaid vs not repaid\n    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n        \n    # Plot the distribution for target == 0 and target == 1\n    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)\n    ","976e73f6":"plt.figure(figsize = (10, 12))\nfor i,column in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'previous_loan_counts']):\n    plt.subplot(5,1, i+1)\n    kde_target(column, app_df)\nplt.tight_layout(h_pad = 2.5)","f4346cc7":"## CREDITS TO WILL KOERSEHN ##\n\ndef agg_numeric(df, group_var, df_name):\n    \"\"\"Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg","a3be918f":"bureau_grouped = agg_numeric(bureau_df.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name= 'bureau_df')\nbureau_grouped.head()","5b386f88":"bureau_grouped.shape","8fd34fce":"app_df = app_df.merge(bureau_grouped, on = 'SK_ID_CURR', how = 'left')\napp_df.head()","87f365b2":"app_df.shape","30dfcdd3":"## CREDITS TO WILL KOERSEHN ##\n\ndef count_categorical(df, group_var, df_name):\n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` of each unique category in every categorical variable\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    group_var : string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with counts and normalized counts of each unique category in every categorical variable\n        with one row for every unique value of the `group_var`.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical\n","d898d0e8":"bureau_grouped_categories = count_categorical(bureau_df, group_var = 'SK_ID_CURR', df_name= 'bureau_df')\nbureau_grouped_categories.head()","f278ddc4":"app_df = app_df.merge(bureau_grouped_categories, on = 'SK_ID_CURR', how = 'left')\napp_df.head()","69cd2564":"app_df.shape","bd53b0a6":"bureau_df_balance = pd.read_csv('..\/input\/bureau_balance.csv')\nbureau_df_balance.head()","0eae80d7":"previous_ap_df = pd.read_csv('..\/input\/previous_application.csv')\nprevious_ap_df.head()","489ebb85":"reduce_mem_usage(previous_ap_df) #reducing memory usage","eb1e1209":"previous_agg = agg_numeric(previous_ap_df, group_var = 'SK_ID_CURR', df_name = 'previous')\nprint(previous_ap_df.shape, previous_agg.shape)","1f1c97ed":"previous_count = count_categorical(previous_ap_df, group_var = 'SK_ID_CURR', df_name='previous')\nprint(previous_ap_df.shape, previous_count.shape)","472b0998":"# Counts of each type of status for each previous loan, it's a categorical feature\nbureau_balance_counts = count_categorical(bureau_df_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_df_balance')\nbureau_balance_counts.head()","4ddd0445":"# Calculate value count statistics for each `SK_ID_CURR`\nbureau_balance_agg = agg_numeric(bureau_df_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_df_balance')\nbureau_balance_agg.head()","28b415a9":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau_df[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on = 'SK_ID_BUREAU', how = 'left')\n\n# Aggregate the stats for each client\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')","a21ebd11":"bureau_balance_by_client.head()","b571ad0f":"app_df = app_df.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\napp_df = app_df.merge(previous_count, on = 'SK_ID_CURR', how = 'left')\napp_test_df = app_test_df.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\napp_test_df = app_test_df.merge(previous_count, on = 'SK_ID_CURR', how = 'left')","b7db2259":"# Merge with the monthly information grouped by client\napp_df = app_df.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')\n#applying all the changes that has been made to the training set, to the testing set.\napp_test_df = app_test_df.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')\napp_test_df = app_test_df.merge(bureau_grouped, on = 'SK_ID_CURR', how = 'left')\napp_test_df = app_test_df.merge(bureau_grouped_categories, on = 'SK_ID_CURR', how = 'left')\napp_test_df = app_test_df.merge(num_of_previous_credits, on = 'SK_ID_CURR', how = 'left')","ec70b6a2":"print(app_df.shape, app_test_df.shape)","98b8e6a2":"#getting dummies for our dataset\napp_df = pd.get_dummies(app_df)\napp_test_df = pd.get_dummies(app_test_df)\nprint(app_df.shape, app_test_df.shape)","bdee500d":"gc.enable()\ndel bureau_df,previous_agg,previous_ap_df,previous_count, num_of_previous_credits, bureau_df_balance, bureau_grouped,bureau_grouped_categories, bureau_balance_agg,bureau_by_loan, bureau_balance_by_client\ngc.collect()","46735bb4":"app_df = reduce_mem_usage(app_df) #using the function to reduce the amount of memory used by our dataframe(function has been defined at the beggining of the notebook)\napp_test_df = reduce_mem_usage(app_test_df)","9ac61cd4":"app_df.head() #taking a look at how our dataframe looks now.","9820b464":"missing_values_calculate(app_df) #using our function to calculate the missing values","0cfda1d8":"app_df, app_test_df = remove_missing_columns(app_df,app_test_df, threshold=73)","9ed0c2c3":"corr = app_df.corr()\ncorr['TARGET'].sort_values(ascending=False).head(20)","5c1ce9a1":"corr['TARGET'].sort_values(ascending=False).tail(20)","67f44cab":"#creating our Y, our target\ntrain_labels = app_df['TARGET'].copy()\napp_df_no_ids = app_df.drop(['SK_ID_CURR','DAYS_ID_PUBLISH'], axis=1)\napp_df_no_ids, app_test_df_no_ids = app_df_no_ids.align(app_test_df, join = 'inner', axis = 1)","6a748d0f":"from sklearn.linear_model import LogisticRegression\n\nparam_grid = {'C': [0.0001], 'multi_class': ['multinomial'],  \n              'penalty': ['l1'],'solver': ['saga'], 'tol': [0.1] }\n\nlog_reg = LogisticRegression()\n\n# Train on the training data\ntest_log_reg = classification_model(app_df_no_ids[:30000:], train_labels[:30000:],test_set=app_test_df_no_ids,pipeline=pipeline ,params=param_grid, GridSearch=True, model=log_reg)","53361699":"from sklearn.ensemble import RandomForestClassifier\n\nparam_grid_random = {'n_estimators': [100]}\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(random_state = 42, verbose = 1, n_jobs = -1)\nprediction_lgr = classification_model(app_df_no_ids[:30000:], train_labels[:30000:], test_set=app_test_df_no_ids, pipeline=pipeline ,params=param_grid_random, GridSearch=True, model=random_forest, plot_features_importances=True)","cacf516c":"import lightgbm as lgb\n\nparams_lgb = { \n              \"learning_rate\": [0.05],\n              \"reg_alpha\": [0.1],\n              \"reg_lambda\": [0.1],\n              \"subsample\": [0.8],\n                'class_weight': 'balanced'\n}","8dca0f9f":"submission_prediction_LGB = classification_model(app_df_no_ids, train_labels, pipeline=pipeline ,params=params_lgb, GridSearch=False, plot_features_importances=True,test_set=app_test_df_no_ids)","971bebdd":"submission_prediction_LGB = submission_prediction_LGB > 0.8 #as the prediction we got is an average across all the folds, we have a percentage for each target, I am gonna get only percentages greater than 80% here for target = 1","4050a163":"sub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub['TARGET'] = submission_prediction_LGB.astype('int8')\nsub.to_csv('lgb_new_features.csv', index=False)","a4bc8164":"sub.head()","65ac7f98":"sub[\"TARGET\"].value_counts()","bafe815d":"***","49910ca3":"# Home credit default risk\n## All the information used in this kernel has been inspired by this amazing kernel: [here](https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-manual-feature-engineering\/notebook) \n## so grateful for all the information\n### If you have any suggestions, critics or feedback, feel free to leave me a heads up here...","a9ba0cfe":"* Loading the application train, this is gonna be the base on which we are going to add more information from other datasets that has been provided to us.","6c982eb3":"** Now we have 61 new columns, that we're gonna merge to our training set **","b1ac67a6":"***\n# Fitting our models...\n## Notice that I am using only a small part of the dataset for models that won't give us a good score, to increase performance.\n","3691ac90":"> DAYS_BIRTH is the age in days of the client at the time of the loan in negative days. Meaning, the older the client gets, it is less likely that the loan defaults","0bd1178b":"* Creating a funtion that gets a dataframe, column name and dataframe name, groups by the column name and aggregate using mean, count,min, max and sum.","b79c21fb":"1. Logistic Reression","c4b5cd43":"* as can be seen, the max value is not reasonable at all, here we have an outlier that might be a mistake when the data was created","7ea2d5ee":"#### let's have an overall picture using .head()","2ae0825a":"2. Random Forest","e497002a":"### The 3 most negativaly correlated features continue to be the 3 EXT source... some of the features create appears here as well..","be109bcd":"### I would like to plot a heat map with the top positive correlation feature(DAYS BIRTH) and all the top 3 negativaly correlated(all External Sources)","068dcd4f":"* Loading the testing set, checking the shape as well","ff850dd1":"3. LGB model","11fe1752":"## that function is truly amazing to help us manage the numerical values of a given dataset, now, it's possible to create a very similar funtion using pd.getdummies and peform on categorical features.","e8c13edf":"** We have increased the number of features quite significantly, it's advisable to clean up the memory, freeing up space, deleting objects that won't be used anymore, but before that, let's prepare our code for our models, creating dummies for both the training and testing set and align them**","88f3b7c8":"### Previous application","3f42ecf1":"* We have created a validation dataset in order to test our models, now let's take a look at the shape before implementing the functions to help us test our models","3247240f":"* We have 3 different types in our dataset(float64, int64, object), Gonna implement a function that reduce the amount of memory used.","0bf4467f":"***\n### Now, with those functions is gonna be easy to process another dataframe, like the bureau balance","49161845":"#### A bunch of null values that need to be imputed, gonna address it soon","c75824ea":"**Now, let's plot the KDE correlation of each of the columns I used before and see what the KDE plot show us **","65321158":"** As we have plotted the KDE for days birth, gonna do the same for all the external sources features **","e43de0c0":"# Second Part - Improving our model -\n## it was provided extra datasets with plenty of information that can be added to our training set, help us engineering some extra features and improve our model\n### Gonna start loading bureau.csv, which contains information of previous loans of each client","5fcbfcf7":"> The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:","c3ba7f06":"#### We have the ID's of each client, which is the SK_ID_CURR, and information like when the loan was taken, end date, is it overdue, days credit and so forth....so we have here a credit history","3fd09d41":"#### looking for outliers or some anomalies...","af511872":"***","4555eecc":"#### gonna plot the KDE distribution relative to target, trying to show the relationship described above","08c1769a":"** Nice, now we have the number of past credits each client have**\n* let's add this information to our training dataset","60763c3a":"### Dropping id columns and the target, as it's not gonna help our model","fb3e953c":"* checking the shape of the data\n* calling head to see the first entries","965e8ab6":"* well, we have 55374 values where our 'days employed' column is more than 300000, which is not a correct value. However, with such a high amount of entries showing this issue, one good approach would be to change the values to NAN and then, afterwards, impute those values with a median value","6c215519":"#### Implementing a function to check for null values and plot a countplot","51c532d6":"#### at least using pearson's correlation, it seems that some of the features that has been created has a positive (linear) correlation with our target.","06a32232":"**Taking a look at the pearson's correlation using the .corr() method**","06418af5":"### we have an expressive number of missing values, I am gonna use the function implemented before to drop all the columns with 73% or more of null values","e672b803":"* no issues here, the max and min are reasonable... \n* what about days employed","d7259e69":"* Let's see the correlation with our target:","77bd3417":"* The correlation is really low, and negatively correlated, the higher this value(more past loans), less likely to default(remember, past loans that have been paid)\n\n## as it was used previously, the KDE plot is gonna also show the relationship between one variable and other variables. It's gonna be implemented a function to help us plot using different variables","ebbbaf1a":"##### it's clear that the older the client gets, it's more likely that he is gonna pay the credit\n* the higher amount of credit default can be found within ages 20-25","8a25655e":"## Before diving into feature engineering and creating features that helps our models learn more from our data, I am gonna implement pipelines and a function to help us test our first models and see how they perform.","b8e83aeb":"** Great! Now we have processed the bureau dataframe and extracted information that might be useful for our models. We are gonna check correlation later **","3cfa4c3b":"#### I am repeating the KDE graphs for the other columns to a base for comparison\n##### Unfortunately, tho, our new feature is not really helpful...\n\n* gonna continue with feature engineering"}}