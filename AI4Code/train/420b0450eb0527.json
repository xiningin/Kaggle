{"cell_type":{"adcfb7e3":"code","308b6f48":"code","6d33807f":"code","ff69f783":"code","7bf6a788":"code","57141a47":"code","48184b79":"code","fef3790a":"code","b35fe7f9":"code","a4b48589":"code","8df79b53":"code","517fe8ee":"code","38942083":"code","741e1642":"code","24871b94":"code","4486d910":"code","44fc5add":"code","b606f011":"code","a92b0d87":"code","589bb7a8":"code","3c5fa7bc":"code","2a08240e":"code","8b9e773c":"code","d90f9141":"code","4c8d5df7":"code","095f2b09":"code","d56a51f9":"code","270a379b":"code","36c038fd":"code","b54f4b8c":"code","93b0abe8":"code","22d8a554":"code","3514ecc1":"code","cae47624":"code","7ccdc08b":"code","080ff3c2":"code","0ee6b309":"code","443f31d1":"code","f0649888":"markdown","bfaf4319":"markdown","c976d79c":"markdown","486d9415":"markdown","d4e23dea":"markdown","6010e2f8":"markdown"},"source":{"adcfb7e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport plotly.offline as pyo\nsns.set(color_codes=True) \n\n#Predictive Modeling\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n\nfrom sklearn import metrics\n\n# Evaluation metrics\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, accuracy_score, precision_recall_curve,roc_auc_score\nfrom sklearn.model_selection import train_test_split,GridSearchCV\n# Suppress warnings\nimport warnings; warnings.filterwarnings('ignore')\n\n# Visualize Tree\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\nfrom os import system\n\n# Display settings\npd.options.display.max_rows = 10000\npd.options.display.max_columns = 10000\n\nrandom_state = 42\nnp.random.seed(random_state)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","308b6f48":"%time\ntrain = pd.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\",nrows=1e5)","6d33807f":"print(f\"Train data has {train.shape[0]} rows and {train.shape[1]} features\")","ff69f783":"train.head()","7bf6a788":"train.info()","57141a47":"train.describe().T","48184b79":"# Checking missing values in dataframe\ntrain.isnull().sum()","fef3790a":"nullvaluecheck = pd.DataFrame(train.isna().sum().sort_values(ascending=False)*100\/train.shape[0],columns=['missing %']).head(60)\nnullvaluecheck.style.background_gradient(cmap='PuBu')","b35fe7f9":"from time import time\nimport itertools\nimport warnings\n# Analyze the body of the distributions\ncols = [i for i in nullvaluecheck.index]\nfig = plt.figure(figsize=(17,60))\nfor i,j,k in itertools.zip_longest(cols, range(len(cols)), [\"c\"]):\n    plt.subplot(15,4,j+1)\n    ax = sns.distplot(train[i],color=k)\n    plt.axvline(train[i].mean(),linestyle=\"dashed\",label=\"mean\",color=\"k\")\n    plt.legend(loc=\"best\")","a4b48589":"null_columns = train.columns[train.isnull().any()]; columns = list(train.columns)\n\nprint('Descriptive Stats for columns with missing values before imputation : \\n', '--'*30)\ndisplay(train[null_columns].describe().T)\n\n# Using SimpleImputer to fill missing values by mean\nimpute = SimpleImputer(missing_values = np.nan, strategy = 'mean', verbose = 1)\ntrain = pd.DataFrame(impute.fit_transform(train), columns = columns)\n\nprint('Descriptive Stats after imputation: \\n', '--'*30)\ndisplay(train[null_columns].describe().T)\n\ndel null_columns","8df79b53":"train=train[train['weight']!=0]\ntrain['action']=(train['resp']>0)*1\ntrain.action.value_counts()","517fe8ee":"import plotly.express as px\ndf = px.data.tips()\nfig = px.histogram(train, x=\"action\")\nfig.show()\n","38942083":"for i,col in enumerate([f'resp_{i}' for i in range(1,5)]):\n fig = px.scatter(x=train[col], y=train['weight'],color=train['action'],labels={\"x\":f'resp_{i}',\"y\":\"weight\",\"color\":\"action\"},title=\"Response variable Vs Weight\")\n fig.show() \n","741e1642":"# Copying all the predictor variables into X dataframe. Since 'action' is dependent variable drop it\nX = train.drop([\"action\"], axis=1) \n\n# Copy the 'action' column alone into the y dataframe. This is the dependent variable\ny = train[[\"action\"]]\n\n#Apolying standardization the variable\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nsc = StandardScaler()\nX_scaled = sc.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled, columns=X.columns)","24871b94":"# Covariance matrix\ncov_matrix = np.cov(X_scaled.T)\nprint('Covariance Matrix \\n%s', cov_matrix)","4486d910":"# Eigen values and vector\neig_vals, eig_vecs = np.linalg.eig(cov_matrix)\nprint('Eigen Vectors \\n%s', eig_vecs)\nprint('\\n Eigen Values \\n%s', eig_vals)","44fc5add":"# Cumulative variance explained\ntot = sum(eig_vals)\nvar_exp = [(i \/tot) * 100 for i in sorted(eig_vals, reverse = True)]\ncum_var_exp = np.cumsum(var_exp)\n\nprint('Cumulative Variance Explained', cum_var_exp)","b606f011":"plt.figure(figsize = (15 , 7.2))\nplt.plot(var_exp)\nplt.xlabel('# of Components')\nplt.ylabel('Eigen Values')","a92b0d87":"# Ploting \nplt.figure(figsize = (15 , 7.2))\nplt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\nplt.step(range(1, eig_vals.size + 1), cum_var_exp, where = 'mid', label = 'Cumulative explained variance')\nplt.axhline(y = 95, color = 'r', linestyle = '--')\nplt.axvline(x = 6, color = 'r', linestyle = '--')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()","589bb7a8":"# Reducing the dimensions to 6\npca = PCA(n_components = 6, random_state = random_state)\npca.fit(X_scaled)\nX_reduced = pca.fit_transform(X_scaled)\ndisplay(X_reduced.shape)","3c5fa7bc":"pca.components_","2a08240e":"# Pairplot after dimension reduction\nsns.pairplot(pd.DataFrame(X_reduced), diag_kind = 'kde')","8b9e773c":"# Let's create a generic method to train and test the model\ndef run_classification(estimator, X_train, X_test, y_train, y_test, prec_rcl=True):\n    timer.start()\n    # train the model\n    clf = estimator.fit(X_train, y_train)\n    # predict from the claffier\n    y_pred = clf.predict(X_test)\n    print('Estimator:', clf)\n    print('='*80)\n    print('Training accuracy: %.2f%%' % (accuracy_score(y_train, clf.predict(X_train)) * 100))\n    print('Testing accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n    print('='*80)\n    print('Classification report:\\n %s' % (classification_report(y_test, y_pred)))\n    print(timer.stop(), 'to run the model')\n    cm=metrics.confusion_matrix(y_test, y_pred, labels=[1, 0])\n    df_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\n    plt.figure(figsize = (7,5))\n    sns.heatmap(df_cm, annot=True,fmt='g')\n    plt.title('Confusion matrix')\n    plt.show()\n        \n    if prec_rcl:\n        print('='*80)\n        y_proba = clf.predict_proba(X_test)\n        roc_auc = roc_auc_score(y_test,y_pred )\n        fpr, tpr, thresholds = roc_curve(y_test, y_proba[:,1])\n        plt.figure()\n        plt.plot(fpr, tpr, label='(area = %0.2f)' % roc_auc)\n        plt.plot([0, 1], [0, 1],'r--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver operating characteristic')\n        plt.legend(loc=\"lower right\")\n        plt.show()","d90f9141":"# Divide the projected dataset into train and test split\nX_reduced_train, X_reduced_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=1)\nX_reduced_train.shape, X_reduced_test.shape, y_train.shape, y_test.shape","4c8d5df7":"#Utilities\nfrom time import time\n# A class that logs the time\nclass Timer():\n    '''\n    A generic class to log the time\n    '''\n    def __init__(self):\n        self.start_ts = None\n    def start(self):\n        self.start_ts = time()\n    def stop(self):\n        return 'Time taken: %2fs' % (time()-self.start_ts)\n    \ntimer = Timer()","095f2b09":"# Run Classification for Logistic Regression\nrun_classification(LogisticRegression(), X_reduced_train, X_reduced_test, y_train, y_test)","d56a51f9":"#Run Classification for Gaussian Naive Bayes Classifier\nrun_classification(GaussianNB(), X_reduced_train, X_reduced_test, y_train, y_test)","270a379b":"#Run Classification for Support Vector Classifier\nrun_classification(SVC(C= 1, kernel='rbf', gamma= 1), X_reduced_train, X_reduced_test, y_train, y_test,False)","36c038fd":"# Run Classification for K-nearest neighbors Classifier\nrun_classification(KNeighborsClassifier(n_neighbors=5), X_reduced_train, X_reduced_test, y_train, y_test)","b54f4b8c":"# Run Classification for Decision Tree Classifier\nrun_classification(DecisionTreeClassifier(criterion='gini', max_depth=7), X_reduced_train, X_reduced_test, y_train, y_test)","93b0abe8":"run_classification(RandomForestClassifier(),  X_reduced_train, X_reduced_test, y_train, y_test)","22d8a554":"from scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import RandomizedSearchCV","3514ecc1":"# specify parameters and distributions to sample from\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": sp_randint(1, 11),\n              \"min_samples_split\": sp_randint(2, 11),\n              \"min_samples_leaf\": sp_randint(1, 11),\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n","cae47624":"# build a classifier\nclf = RandomForestClassifier(n_estimators=50)","7ccdc08b":"# run randomized search\nsamples = 10  # number of random samples \nrandomCV = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=samples) #default cv = 3\nrandomCV.fit(X_reduced_train,y_train)\nprint(randomCV.best_params_)","080ff3c2":"run_classification(RandomForestClassifier(n_estimators=50,bootstrap= True,criterion= 'gini',max_depth= None, max_features= 2, min_samples_leaf= 10, min_samples_split= 10),  X_reduced_train, X_reduced_test, y_train, y_test)","0ee6b309":"clf=RandomForestClassifier(n_estimators=50,bootstrap= True,criterion= 'gini',max_depth= None, max_features= 2, min_samples_leaf= 10, min_samples_split= 10)","443f31d1":"import janestreet\n#env = janestreet.make_env() # initialize the environment\n#iter_test = env.iter_test() # an iterator which loops over the test set\n\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df.action = 0\n    X = test_df\n    null_columns = X.columns[train.isnull().any()]; columns = list(X.columns)\n    impute = SimpleImputer(missing_values = np.nan, strategy = 'mean', verbose = 1)\n    X = pd.DataFrame(impute.fit_transform(X), columns = columns)\n    X_scaled = sc.fit_transform(X)\n    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n    pca.fit(X_scaled)\n    X_reduced = pca.fit_transform(X_scaled)\n    sample_prediction_df.action=clf.predict(X_reduced)\n    env.predict(sample_prediction_df)","f0649888":"**Hyperparameter Tuning**\n\nRandom Forest Regressor is coming out to be best performing algorithm among all.So we chose this algorithm to proceed further with Model Tuning.  We will use the Randomized SearchCV for tuning.","bfaf4319":"**Principal Component Analysis**\nPrincipal component Analysis is a method to identify the patterns in data using their similarities and dissimilarities between the sample points. The patterns within data is hard to find especially when we cannot visualize it graphically. The principal component analysis is a powerful tool to explore data with its hidden patterns and reduce the dimensions.\n\nSteps of principal Component Analysis\n* Normalize all the data variables within same range of values (-1, 1)\n* Compute PCA is to calculate covariance matrix\n* Compute eigenvalues and corresponding eigenvectors\n* Arrange eigenvalues with corresponding eigenvectors in descending order. \nThe higher value eigen vectors have more significance over the data and form principal components whereas the lower value eigen vectors can be removed in order to reduce the dimensions","c976d79c":"From the distplot, it is more or less clear that the majority values are concentrated around the mean value of the features. So we will apply imputation on missing values on the basis of mean.","486d9415":"The distribution looks kind of normal without much bais.","d4e23dea":"Calculation for the target varaiable","6010e2f8":"Lets apply the following set of alogorithms for our prediction\n1. Logistic Regression\n2. Gaussian Naive Bayes Classifier\n3. Support Vector Machine\n4. K-nearest neighbors Classifier\n5. Decision Tree Classifier\n6. Random Forest Classifier\n"}}