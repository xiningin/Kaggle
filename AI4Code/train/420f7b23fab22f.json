{"cell_type":{"e6b2fdbf":"code","158db20d":"code","fb4676af":"code","61155cf7":"code","9ab1d51c":"code","ef326155":"code","1b10b52b":"code","2bdba434":"code","6ed2d0f2":"code","7c3fa3bc":"code","564ea042":"code","2a578f8d":"code","61c9c8b7":"code","c733bc2c":"code","9bfab717":"code","870e77ba":"code","2ddf7795":"code","8b9a9c99":"markdown","a24c700d":"markdown","1fa78e0e":"markdown","1d7d12ec":"markdown","2f0d40d6":"markdown","b9de9025":"markdown","7ccf9be0":"markdown"},"source":{"e6b2fdbf":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport copy\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport pyproj\nimport json\nimport bisect\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\nimport pickle\nimport random\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.simplefilter('ignore')\npd.set_option('display.max_rows',30)\npd.set_option('display.max_columns',None)","158db20d":"def visualize_trafic(df, center={\"lat\":37.423576, \"lon\":-122.094132}, zoom=9):\n    fig = px.scatter_mapbox(df,\n                            \n                            # Here, plotly gets, (x,y) coordinates\n                            lat=\"latDeg\",\n                            lon=\"lngDeg\",\n                            \n                            #Here, plotly detects color of series\n                            color=\"phoneName\",\n                            labels=\"phoneName\",\n                            \n                            zoom=zoom,\n                            center=center,\n                            height=300,\n                            width=500)\n    fig.update_layout(mapbox_style='stamen-terrain')\n    fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n    fig.update_layout(title_text=\"GPS trafic\")\n    fig.show()\n    \ndef visualize_collection(df, collection, center={\"lat\":37.423576, \"lon\":-122.094132}, zoom=9):\n    df_traj = df[df['collectionName'] == collection]\n    visualize_trafic(df_traj, center, zoom)","fb4676af":"def calc_haversine(lat1, lon1, lat2, lon2):\n    \"\"\"Calculates the great circle distance between two points\n    on the earth. Inputs are array-like and specified in decimal degrees.\n    \"\"\"\n    RADIUS = 6_367_000\n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat\/2)**2 + \\\n        np.cos(lat1) * np.cos(lat2) * np.sin(dlon\/2)**2\n    dist = 2 * RADIUS * np.arcsin(a**0.5)\n    return dist","61155cf7":"def percentile50(x):\n    return np.percentile(x, 50)\ndef percentile95(x):\n    return np.percentile(x, 95)\n\ndef get_train_score(df, gt):\n    gt = gt.rename(columns={'latDeg':'latDeg_gt', 'lngDeg':'lngDeg_gt'})\n    df = df.merge(gt, on=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], how='inner')\n    # calc_distance_error\n    df['err'] = calc_haversine(df['latDeg_gt'], df['lngDeg_gt'], df['latDeg'], df['lngDeg'])\n    # calc_evaluate_score\n    df['phone'] = df['collectionName'] + '_' + df['phoneName']\n    res = df.groupby('phone')['err'].agg([percentile50, percentile95])\n    res['p50_p90_mean'] = (res['percentile50'] + res['percentile95']) \/ 2 \n    score = res['p50_p90_mean'].mean()\n    return score","9ab1d51c":"def eval_all(df_pred, df_gt):\n    scores = []\n    compared_cols = [\"latDeg_truth\",\"lngDeg_truth\",\"latDeg_pred\",\"lngDeg_pred\"]\n    collections = sorted(df_gt['collectionName'].unique())\n    for collection in collections:\n        df_pred_col = df_pred[df_pred['collectionName'] == collection]\n        df_gt_col = df_gt[df_gt['collectionName'] == collection]\n        \n        score = get_train_score(df_pred_col, df_gt_col)\n        \n        df_merged = pd.merge_asof(df_gt_col.sort_values('millisSinceGpsEpoch'), df_pred_col.sort_values('millisSinceGpsEpoch'), \n                                  on=\"millisSinceGpsEpoch\", by=[\"collectionName\", \"phoneName\"], \n                                  direction='nearest',tolerance=100000, suffixes=('_truth', '_pred'))\n        df_merged = df_merged.sort_values(by=[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"], ignore_index=True)\n\n        haversine = calc_haversine(*df_merged[compared_cols].to_numpy().transpose()).mean()\n        scores.append([collection, haversine, score])\n    \n    score = get_train_score(df_pred, df_gt)\n    df_merged = pd.merge_asof(df_gt.sort_values('millisSinceGpsEpoch'), df_pred.sort_values('millisSinceGpsEpoch'), \n                              on=\"millisSinceGpsEpoch\", by=[\"collectionName\", \"phoneName\"], \n                              direction='nearest',tolerance=100000, suffixes=('_truth', '_pred'))\n    haversine = calc_haversine(*df_merged[compared_cols].to_numpy().transpose()).mean()\n    scores.append(['all', haversine, score])\n    \n    df_scores = pd.DataFrame(scores, columns=['collection', 'haversine', 'score'])\n    return df_scores","ef326155":"def apply_dtw_snap_to_grid(df_input, df_gt, collections_to_snap = None, th_dtw=30.0 \/ 100_000, th_snap=10.0 \/ 100_000):\n    df_snapped = df_input.copy()\n    collections = df_snapped['collectionName'].unique()\n    for collection in tqdm(collections, desc = 'Apply DTW snap-to-grid (for {} trajs)'.format(\"whole\" if collections_to_snap is None else len(collections_to_snap))):\n        if collections_to_snap is not None:\n            if collection not in collections_to_snap:\n                continue\n        cond_col = df_snapped['collectionName'] == collection\n        phones = df_snapped[cond_col]['phoneName'].unique()\n        for phone in phones:\n            cond_traj = cond_col & (df_snapped['phoneName'] == phone)\n            time_traj = df_snapped[cond_traj]['millisSinceGpsEpoch'].values\n            latlng_traj = df_snapped[cond_traj][['latDeg', 'lngDeg']].values\n\n            segment_ids_list = get_segment_ids(latlng_traj, base_len=60, stride=30)\n            \n            snapped_count_list = np.zeros_like(time_traj)\n            snapped_results_list = np.zeros((len(time_traj), 2))\n            \n            for ids in segment_ids_list:\n                cond_seg = np.arange(0, latlng_traj.shape[0], 1)\n                cond_seg = (cond_seg >= ids[0]) & (cond_seg < ids[1])\n                latlng_seg = latlng_traj[cond_seg]\n                snapped_count_list += cond_seg\n                subtraj_opt, _ = search_closest_subtraj(latlng_seg, df_gt, threshold=th_dtw)\n                if subtraj_opt is not None: # \u3082\u3057\u826f\u3044\u611f\u3058\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3089\u3001\u3001\u3001\n                    grids = increase_points_array(subtraj_opt)\n                    snapped_latlng = snap_to_grid_array(latlng_seg, grids, threshold=th_snap)\n                    snapped_results_list[ids[0]:ids[1]] += snapped_latlng\n                else:\n                    snapped_results_list[ids[0]:ids[1]] += latlng_seg\n            \n            # Add original value if never counted.\n            snapped_results_list[snapped_count_list == 0] += latlng_traj[snapped_count_list == 0]\n            snapped_count_list[snapped_count_list == 0] += 1\n\n            df_snapped.loc[cond_traj, ['latDeg', 'lngDeg']] = snapped_results_list \/ snapped_count_list.reshape(-1, 1)\n    return df_snapped\n\n\ndef search_closest_subtraj(latlng, df_gt, threshold=30\/100_000, expand_idx_len=10):\n    \n    dist_opt = 1e9\n    subtraj_opt = None\n    for collection, df_col in df_gt.groupby('collectionName'):\n        # Phone\u306f1\u3064\u3060\u3051\u4f7f\u3048\u3070OK\n        phone_here = df_col['phoneName'].unique()[0]\n        df_traj = df_col[df_col['phoneName'] == phone_here]\n        latlng_traj = df_traj[['latDeg', 'lngDeg']].values\n        \n        dist_start = np.linalg.norm(latlng_traj - latlng[0, :], axis=1)\n        dist_end = np.linalg.norm(latlng_traj - latlng[-1, :], axis=1)\n        start_cand_idx = np.where(dist_start < threshold)[0]\n        end_cand_idx = np.where(dist_end < threshold)[0]\n\n        for start_idx in reduce_array(start_cand_idx):\n            for end_idx in reduce_array(end_cand_idx):\n                if start_idx >= end_idx:\n                    continue\n                dist = calc_dtw(latlng[::20], latlng_traj[start_idx: end_idx][::20])[-1][-1][0]\n                # try:\n                #     dist = calc_dtw(latlng[::20], latlng_traj[start_idx: end_idx][::20])[-1][-1][0]\n                # except IndexError as e:\n                #     print(start_idx, end_idx)\n                if dist_opt > dist:\n                    dist_opt = dist\n                    subtraj_opt = latlng_traj[max(start_idx-expand_idx_len, 0): end_idx+expand_idx_len]\n\n    return subtraj_opt, dist_opt\n\n\ndef increase_points_array(data, min_dist=0.2\/100_000):\n    data_increased = []\n    for i, point in enumerate(data):\n        if i == data.shape[0] - 1:\n            continue\n        data_increased.append(point)\n        if np.array_equal(data[i, 1:], data[i + 1, 1:]):\n            continue\n        latDeg0 = data[i, 0]\n        lngDeg0 = data[i, 1]\n        latDeg1 = data[i+1, 0]\n        lngDeg1 = data[i+1, 1]\n        num_to_increase = np.ceil(np.linalg.norm([latDeg0 - latDeg1, lngDeg0 - lngDeg1]) \/ min_dist)\n        for j in range(int(num_to_increase)):\n            latDeg = ((num_to_increase-j) * latDeg0 + j * latDeg1) \/ num_to_increase\n            lngDeg = ((num_to_increase-j) * lngDeg0 + j * lngDeg1) \/ num_to_increase\n            data_increased.append([latDeg, lngDeg])\n    data_increased = np.array(data_increased)\n    return data_increased\n\n\ndef snap_to_grid_array(query, grids, threshold):\n    snapped_query = copy.deepcopy(query)\n    grids_sorted = np.array(sorted(grids, key=lambda x: x[0]))\n    for i, point in enumerate(query):\n        left_lat_idx = bisect.bisect_left(grids_sorted[:, 0], point[0] - threshold)\n        right_lat_idx = bisect.bisect_left(grids_sorted[:, 0], point[0] + threshold)\n        grids_of_interest = grids_sorted[left_lat_idx : right_lat_idx, :]\n        if len(grids_of_interest) == 0:\n            continue\n\n        dist = np.linalg.norm(grids_of_interest - point, axis=1)        \n        if np.min(dist) > threshold:\n             continue\n        idx_opt = np.argmin(dist)\n        snapped_query[i] = grids_of_interest[idx_opt]\n    return snapped_query\n\n\ndef reduce_array(array, length=50):\n    reduced = []\n    for idx in array:\n        if len(reduced) == 0:\n            reduced.append(idx)\n        else:\n            if reduced[-1] + length <= idx:\n                reduced.append(idx)\n    return reduced\n\n\ndef get_segment_ids(latlng, base_len=60, stride=30):\n    segment_ids = []\n    start = 0\n    end = start + base_len\n    while True:\n        if np.linalg.norm(latlng[min(end, latlng.shape[0]-1), :] - latlng[start, :]) > 200\/100_000: \n            segment_ids.append([start, end])\n            start = end - stride\n            end = start + base_len\n        else:\n            end += stride\n        if end >= len(latlng):\n            segment_ids.append([latlng.shape[0]-1 - 250, latlng.shape[0]-1])\n            break\n    return segment_ids\n\n\ndelta = lambda a, b: np.linalg.norm(a - b)\nfirst = lambda x: x[0]\nsecond = lambda x: x[1]\n\ndef minVal(v1, v2, v3):\n    if first(v1) <= min(first(v2), first(v3)):\n        return v1, 0\n    elif first(v2) <= first(v3):\n        return v2, 1\n    else:\n        return v3, 2 \n\ndef calc_dtw(A, B):\n    S = len(A)\n    T = len(B)\n\n    m = [[0 for j in range(T)] for i in range(S)]\n    m[0][0] = (delta(A[0],B[0]), (-1,-1))\n    for i in range(1,S):\n        m[i][0] = (m[i-1][0][0] + delta(A[i], B[0]), (i-1,0))\n    for j in range(1,T):\n        m[0][j] = (m[0][j-1][0] + delta(A[0], B[j]), (0,j-1))\n\n    for i in range(1,S):\n        for j in range(1,T):\n            minimum, index = minVal(m[i-1][j], m[i][j-1], m[i-1][j-1])\n            indexes = [(i-1,j), (i,j-1), (i-1,j-1)]\n            m[i][j] = (first(minimum)+delta(A[i], B[j]), indexes[index])\n    return m","1b10b52b":"datapath = Path(\"..\/input\/google-smartphone-decimeter-challenge\/\")\nground_truths = (datapath \/ \"train\").rglob(\"ground_truth.csv\")\ndf_gt = pd.concat([pd.read_csv(filepath) for filepath in tqdm(ground_truths, total=73, desc=\"Reading ground truth data\")], ignore_index=True)\n\ndf_pred_train = pd.read_csv('..\/input\/k\/minomonter\/gnss-ensembled\/train_submission_filtered.csv')\ndf_pred_train['collectionName'] = df_pred_train['phone'].apply(lambda x: x.split('_')[0])\ndf_pred_train['phoneName'] = df_pred_train['phone'].apply(lambda x: x.split('_')[1])\n\ndf_pred_test = pd.read_csv('..\/input\/k\/minomonter\/gnss-ensembled\/submission_filtered.csv')\ndf_pred_test['collectionName'] = df_pred_test['phone'].apply(lambda x: x.split('_')[0])\ndf_pred_test['phoneName'] = df_pred_test['phone'].apply(lambda x: x.split('_')[1])","2bdba434":"with open('..\/input\/region-classification\/region_type_train.json') as f:\n    region_type_train = json.load(f)\nwith open('..\/input\/region-classification\/region_type_test.json') as f:\n    region_type_test = json.load(f)\n\ndowntowns = [*[key for (key, val) in region_type_train.items() if 'downtown' in val], *[key for (key, val) in region_type_test.items() if 'downtown' in val]]\ntrees = [*[key for (key, val) in region_type_train.items() if 'tree' in val], *[key for (key, val) in region_type_test.items() if 'tree' in val]]\nhighways = [*[key for (key, val) in region_type_train.items() if 'tree' not in val and 'downtown' not in val],\n          *[key for (key, val) in region_type_test.items() if 'tree' not in val and 'downtown' not in val]]\n\nprocess_snap_to_grid_dtw_downtown_train = lambda df: apply_dtw_snap_to_grid(\n    df,\n    df_gt,\n    collections_to_snap = downtowns,\n    th_dtw=30.0 \/ 100_000,\n    th_snap=50.0 \/ 100_000\n)\nprocess_snap_to_grid_dtw_downtown_test = lambda df: apply_dtw_snap_to_grid(\n    df,\n    df_gt,\n    collections_to_snap = downtowns,\n    th_dtw=30.0 \/ 100_000,\n    th_snap=50.0 \/ 100_000\n)","6ed2d0f2":"eval_all(df_gt, df_pred_train)","7c3fa3bc":"visualize_collection(df_pred_train, '2021-04-29-US-SJC-2', center={\"lat\":37.333929, \"lon\":-121.888997}, zoom=14)","564ea042":"best_funcs = [\n    process_snap_to_grid_dtw_downtown_train,\n]\n\ndf_pred_train_processed = df_pred_train.copy()\nfor func in best_funcs:\n    df_pred_train_processed = func(df_pred_train_processed)\n    print(func.__name__, get_train_score(df_pred_train_processed, df_gt))\n\neval_all(df_pred_train_processed, df_gt)","2a578f8d":"visualize_collection(df_pred_train_processed, '2021-04-29-US-SJC-2', center={\"lat\":37.333929, \"lon\":-121.888997}, zoom=14)","61c9c8b7":"df_pred_train_processed[['phone', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']].to_csv('train_submission.csv', index=False)","c733bc2c":"visualize_trafic(df_pred_test)","9bfab717":"best_funcs = [\n    process_snap_to_grid_dtw_downtown_test,\n]\n\ndf_pred_test_processed = df_pred_test.copy()\nfor func in best_funcs:\n    df_pred_test_processed = func(df_pred_test_processed)","870e77ba":"visualize_trafic(df_pred_test_processed)","2ddf7795":"sub = pd.read_csv('..\/input\/google-smartphone-decimeter-challenge\/sample_submission.csv')\nsub = sub.assign(\n    latDeg = df_pred_test_processed.latDeg,\n    lngDeg = df_pred_test_processed.lngDeg\n)\nsub.to_csv('submission.csv', index=False)","8b9a9c99":"# Dynamic Time Warping Snap-To-Grid\n\nThis notebook demonstrates a snap-to-grid incorporating dynamic time warping (DTW). \n\n## A problem in snap-to-grid\nEspecially in downtown area where GNSS-based localization tend to be noisy, it is very effective to snap the estimated points to the ground truth positions. A simple snap-to-grid algorithm, however, does not consider time series information, which induces some problems such as:\n- a point is snapped to different road, especially in crossroads\n- a point is snapped to the left lane, in spite of the right lane rule in the US\n\n\n## Dynamic Time Warping (DTW)\nDynamic time warping (DTW) is an algorithm for calculating the similarity between two time series data. Unlike other distance methods like Euclid, DTW is able to calculate the similarity of the shape of the data. The implementation can be simply done using dynamic programming.\n\n![dtw_sample.png](attachment:76f83eaa-c84b-4ab0-9adf-de1f85614fc5.png)\n\n## DTW-based snap-to-grid\nThe algorithm is as follows: for every sub-trajectory query $(x_i)$ in the estimated data, \n1. find the closest sub-trajectory $(y_i)$ in ground truth data based on DTW\n2. snap the query $(x_i)$ to the grid $(y_i)$\n\nIn our implementation, the length of $x_i$ is $60 [\\rm{s}]$. Using this algorithm, you can avoid snapping a point to an opposite lane or to the different roads. \n\nIn this competition, the method works pretty well on downtown area, where the multi-path errors are significant and plenty of ground truth data is available. For example on 2021-04-22-US-SJC-1, the score reduces from $18.4 [\\rm{m}]$ to $11.7 [\\rm{m}]$.\n\n![dtw-snap.png](attachment:ca45c260-6f37-4a87-8c92-eadd57456d4a.png)\n\n## Some tips\nOne of the drawbacks of DTW is the computational complexity. When calculating the distance between two time-series data whose length is $N$ and $M$ each, the complexity is $\\mathcal{O}(MN)$. This is highly complex compared to other distance method such as Euclidean distance. Indeed, it took too much time to calculate between a query trajectory and all the possible sub-trajectory in ground truth data.\n\nTo overcome this issue, for each query trajectory $(x_i) = (x_0, x_1, \\cdots, x_{n-1})$, we first pick up the points which is close enough to $x_0$ and $x_{n-1}$ each. Then, among the possible set of trajectory, we choose the trajectory with the closest DTW distance.\n\nWe also tried to apply this algorithm to highway and tree area, but did not work very well. ","a24c700d":"## Visualize DTW snap-to-grid results","1fa78e0e":"# Test","1d7d12ec":"## Visualize baseline","2f0d40d6":"# Visualize","b9de9025":"# Submit","7ccf9be0":"# Train"}}