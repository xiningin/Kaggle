{"cell_type":{"d3a5a4ac":"code","938b2d58":"code","ff9dd124":"code","74b4472c":"code","2ca65441":"code","cf41fb8b":"code","de00ea6c":"code","752d4dd4":"code","11d541a5":"code","24082699":"code","3935da38":"code","4c6b27aa":"code","340ceba5":"code","09562a46":"code","f06f1929":"code","ab19c3f5":"code","0d49cefa":"code","0880e081":"code","82ac7f5b":"code","87ca11e3":"code","074395a5":"code","3f4ebeb5":"code","fef6f8c1":"code","454739aa":"code","44abca1a":"code","d6521264":"code","0bc35d09":"code","9f20ac36":"code","0ebe2aad":"code","b405f5c0":"code","e15a3520":"code","842f850c":"code","a0bc1ad4":"code","92692d22":"code","fab61e77":"code","62cf3720":"code","6982ab07":"code","9f713db6":"code","210bc489":"code","02db28f6":"code","fdf080f6":"code","cfb07290":"code","d56ec15d":"code","809f0664":"code","5477ce3a":"code","6982b1d9":"code","0c2ba63e":"code","0d679719":"code","1bf2bb88":"code","b4b23c0a":"code","9b962790":"code","2563240a":"code","948f6b31":"code","55150cbf":"code","5147fa59":"code","6acdf149":"code","c7092996":"code","140702b7":"code","a897b1fb":"code","40312ded":"code","223f893d":"code","d647e9ce":"code","85c386e1":"code","94cf5acb":"code","e20fbc77":"code","6bbd75d1":"code","c5cadf9d":"markdown","95134e16":"markdown","32b14b5f":"markdown","69b0766a":"markdown","89b56e67":"markdown","e2b07823":"markdown","43817d1d":"markdown","416cd3a4":"markdown","fd48062f":"markdown","782ada66":"markdown","17afe45a":"markdown","61df9338":"markdown","7b81b8ad":"markdown","8e47b196":"markdown","31a7f7f9":"markdown","274dcf97":"markdown","aac4a4b8":"markdown","dc53ed98":"markdown","42206d43":"markdown","a46082fe":"markdown","40b77277":"markdown","fe0abc66":"markdown","b6f97282":"markdown","07b8cb84":"markdown","f2e9c8ba":"markdown","979ab288":"markdown","66aeddd4":"markdown","d5756eab":"markdown","074b576c":"markdown","a76ca639":"markdown"},"source":{"d3a5a4ac":"# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport warnings\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nimport missingno as msno\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.set_option(\"display.max_rows\", 2000)\npd.set_option(\"display.max_columns\", 500)\npd.set_option(\"display.width\", 100)\npd.set_option(\"display.max_colwidth\", 500)\npd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)\n\nfrom sklearn.inspection import permutation_importance\nfrom IPython.display import display, HTML, display_html\n\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.ticker as mticker\nfrom lightgbm import LGBMRegressor\nimport optuna\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.compose import ColumnTransformer\nimport math\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.feature_selection import VarianceThreshold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\ncwd = os.getcwd()\nprint(cwd)\n\n\nimport gc\n\ngc.enable()\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.models import (\n    BasicTicker,\n    ColorBar,\n    ColumnDataSource,\n    LinearColorMapper,\n    PrintfTickFormatter,\n)\nfrom bokeh.plotting import figure\nfrom bokeh.transform import transform\n\n\nrandom_state = 55\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","938b2d58":"%%javascript\nIPython.OutputArea.auto_scroll_threshold = 9999;","ff9dd124":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","74b4472c":"print(\n    f\"Train set contains {train.shape[0]} rows,{train.shape[1]} columns. \\nTest set contains {test.shape[0]} rows, {test.shape[1]} columns.\\n\"\n)\nprint(\n    f\"{set(train.columns) - set(test.columns)} are the fields that are IN TRAIN and NOT IN TEST.\\n {set(test.columns) - set(train.columns)} are the fields that are IN TEST and NOT IN TRAIN. \"\n)\n","2ca65441":"train.info()","cf41fb8b":"display(\n    train.describe().iloc[:, 0:18].applymap(\"{:,g}\".format),\n    train.describe().iloc[:, 18:].applymap(\"{:,g}\".format),\n)\n","de00ea6c":"sample_count = 5\n\ndisplay(\n    train.sample(sample_count, random_state=random_state)\n    .iloc[:, :30]\n    .style.hide_index(),\n    train.sample(sample_count, random_state=random_state)\n    .iloc[:, 30:60]\n    .style.hide_index(),\n    train.sample(sample_count, random_state=random_state)\n    .iloc[:, 60:]\n    .style.hide_index(),\n)\n","752d4dd4":"Id = \"Id\"\n\nsubmission_ID = test.loc[:, Id]\n\ntrain.drop(Id, axis=1, inplace=True)\ntest.drop(Id, axis=1, inplace=True)\n\n# For identification purposes\ntrain.loc[:, \"Train\"] = 1\ntest.loc[:, \"Train\"] = 0\n\ntest[\"SalePrice\"] = 0\n\nstacked_DF = pd.concat([train, test], ignore_index=True)\n","11d541a5":"stacked_DF[\"Train\"]","24082699":"plt.style.use('ggplot')\nparams = {\n    \"axes.labelsize\": 18,\n    \"axes.titlesize\": 20,\n    \"xtick.labelsize\": 16,\n    \"ytick.labelsize\": 16,\n}\nplt.rcParams.update(params)\n\n(fig, ax) = plt.subplots(nrows=2, ncols=1, figsize=[12, 12], sharex=True)\n\nax[0].set_xlabel(\"SalePrice\")\nax[0].set_ylabel(\"Count\")\n\nax[1].set_xlabel(\"SalePrice\")\nax[1].set_ylabel(\"Count\")\n\nplot_X = stacked_DF.loc[stacked_DF[\"Train\"] == 1][\"SalePrice\"]\n\nplot = ax[0].hist(plot_X, bins=75, log=False, color=\"darkslategrey\")\nplot = ax[1].hist(plot_X, bins=75, log=True, color=\"darkslategrey\")\n\nax[0].set_title(\"Distribution of SalePrice\")\nax[1].set_title(\"Distribution of SalePrice (Log Transformed)\")\n","3935da38":"# params = {\"axes.labelsize\": 20, \n#           \"xtick.labelsize\": 14, \n#           \"ytick.labelsize\": 14}\n# plt.rcParams.update(params)\n\n# features_to_viz = [\n#     \"GrLivArea\",\n#     \"YearBuilt\",\n#     \"WoodDeckSF\",\n#     \"LotArea\",\n#     \"GarageArea\",\n#     \"1stFlrSF\",\n#     \"2ndFlrSF\",\n#     \"TotalBsmtSF\",\n#     \"LotFrontage\",\n#     \"GarageYrBlt\",\n# ]\n\n# # Because there are a lot of variables to vizualize,\n# # sorting them helps me keep track of which variable is where\n\n# features_to_viz = sorted(features_to_viz)\n\n# ncols = 1\n# nrows = math.ceil(len(features_to_viz) \/ ncols) # \u5e72\u561b?\n# unused = nrows * ncols - len(features_to_viz)\n\n# figw = ncols * 10\n# figh = nrows * 8\n\n# (fig, ax) = plt.subplots(nrows, ncols, sharey=True, figsize=(figw, figh))\n# fig.subplots_adjust(hspace=0.3)\n# ax = ax.flatten()\n\n# for i in range(unused, 0, -1):\n#     fig.delaxes(ax[-i])\n\n\n# for (n, col) in enumerate(features_to_viz):\n#     if n % 2 != 0:\n#         print(n)","4c6b27aa":"params = {\"axes.labelsize\": 20, \n          \"xtick.labelsize\": 14, \n          \"ytick.labelsize\": 14}\nplt.rcParams.update(params)\n\nfeatures_to_viz = [\n    \"GrLivArea\",\n    \"YearBuilt\",\n    \"WoodDeckSF\",\n    \"LotArea\",\n    \"GarageArea\",\n    \"1stFlrSF\",\n    \"2ndFlrSF\",\n    \"TotalBsmtSF\",\n    \"LotFrontage\",\n    \"GarageYrBlt\",\n]\n\n# Because there are a lot of variables to vizualize,\n# sorting them helps me keep track of which variable is where\n\nfeatures_to_viz = sorted(features_to_viz)\n\nncols = 1\nnrows = math.ceil(len(features_to_viz) \/ ncols) # \u5e72\u561b?\nunused = nrows * ncols - len(features_to_viz)\n\nfigw = ncols * 10\nfigh = nrows * 8\n\n(fig, ax) = plt.subplots(nrows, ncols, sharey=True, figsize=(figw, figh))\nfig.subplots_adjust(hspace=0.3)\nax = ax.flatten()\n\nfor i in range(unused, 0, -1):\n    fig.delaxes(ax[-i])\n\n\nfor (n, col) in enumerate(features_to_viz):\n    #if n % 2 != 0:\n        #ax[n].yaxis.label.set_visible(False)\n    ax[n].set_xlabel(col)\n    ax[n].set_ylabel(\"SalePrice\")\n    sns.scatterplot(\n        x=col,\n        y=\"SalePrice\",\n        data=stacked_DF.loc[stacked_DF[\"Train\"] == 1],\n        hue=\"SalePrice\",\n        palette=\"gist_earth\",\n        s=75,\n        legend=False,\n        ax=ax[n],\n    )\n\nplt.show()\n","340ceba5":"features_to_viz = [\n    \"Neighborhood\",\n    \"BsmtQual\",\n    \"ExterQual\",\n    \"FireplaceQu\",\n    \"ExterCond\",\n    \"KitchenQual\",\n    \"LotShape\",\n    \"OverallQual\",\n    \"FullBath\",\n    \"HalfBath\",\n    \"TotRmsAbvGrd\",\n    \"Fireplaces\",\n    \"KitchenAbvGr\",\n]\n\n# Because there are a lot of variables to vizualize,\n# sorting them helps me keep track of which var is where\n\nfeatures_to_viz = sorted(features_to_viz)\n\ntmp = (\n        stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n        .groupby(by=col)[\"SalePrice\"]\n        .median()\n        .sort_values()\n        .index\n    )\nprint(pd.Series(tmp))","09562a46":"features_to_viz = [\n    \n    \"BsmtQual\",\n    \"ExterQual\",\n    \"FireplaceQu\",\n    \"ExterCond\",\n    \"KitchenQual\",\n    \"LotShape\",\n    \"OverallQual\",\n    \"FullBath\",\n    \"HalfBath\",\n    \"TotRmsAbvGrd\",\n    \"Fireplaces\",\n    \"KitchenAbvGr\",\n]\n\n# Because there are a lot of variables to vizualize,\n# sorting them helps me keep track of which var is where\n\nfeatures_to_viz = sorted(features_to_viz)\n\nncols = 1\nnrows = math.ceil(len(features_to_viz) \/ ncols)\nunused = nrows * ncols - len(features_to_viz)\n\n(figw, figh) = (ncols * 10, nrows * 8)\n\n(fig, ax) = plt.subplots(nrows, ncols, figsize=(figw, figh))\nfig.subplots_adjust(hspace=0.2, wspace=0.2)\n\n# ax = ax.flatten()\n# for i in range(unused, 0, -1):\n#     fig.delaxes(ax[-i])\n\nfor (n, col) in enumerate(features_to_viz):\n    ordering = (\n        stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n        .groupby(by=col)[\"SalePrice\"]\n        .median()\n        .sort_values()\n        .index\n    )\n    sns.boxplot(\n        x=col,\n        y=\"SalePrice\",\n        data=stacked_DF.loc[stacked_DF[\"Train\"] == 1],\n        order=ordering,\n        ax=ax[n],\n        orient=\"v\",\n    )\n\nplt.show()\n","f06f1929":"print(\"Missing Value Counts in Train DF\")\nstacked_DF[stacked_DF[\"Train\"] == 1].isna().sum(axis=0)[\n    stacked_DF[stacked_DF[\"Train\"] == 1].isna().sum() > 0\n].sort_values(ascending=False)","ab19c3f5":"train_missing = stacked_DF[stacked_DF[\"Train\"] == 1].isna().sum(axis=0)[\n    stacked_DF[stacked_DF[\"Train\"] == 1].isna().sum() > 0\n].sort_values(ascending=False).index","0d49cefa":"print(\"Missing Values in Test DF\")\nstacked_DF[stacked_DF[\"Train\"] == 0].isna().sum()[\n    stacked_DF[stacked_DF[\"Train\"] == 0].isna().sum() > 0\n].sort_values(ascending=False).index.to_list()\n","0880e081":"test_missing = stacked_DF[stacked_DF[\"Train\"] == 0].isna().sum()[\n    stacked_DF[stacked_DF[\"Train\"] == 0].isna().sum() > 0\n].sort_values(ascending=False).index\n\nmissing_names = np.unique(train_missing.append(test_missing))\nmissing_names\n#len(train_missing)\n#len(test_missing)\nlen(missing_names)","82ac7f5b":"# Check missing records in train set, I think it's a typo here, 'train' should mean 'all'\nna_cols = (stacked_DF.isna().sum()[stacked_DF.isna().sum() > 0]).index\nmat = msno.matrix(\n    stacked_DF.loc[:, na_cols], labels=True, figsize=(16, 14), fontsize=16, inline=False\n)","87ca11e3":"lookup = (\n    stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n    .groupby(by=\"Neighborhood\")[\"MSZoning\"]\n    .agg(pd.Series.mode)\n)\nlookup","074395a5":"# Assuming Neighborhood and MSZoning are related.\n# Neighborhood\u6ca1\u6709\u7f3a\u5931\u503c\nlookup = (\n    stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n    .groupby(by=\"Neighborhood\")[\"MSZoning\"]\n    .agg(pd.Series.mode)\n)\n\nstacked_DF[\"MSZoning\"] = stacked_DF[\"MSZoning\"].fillna(\n    stacked_DF[\"Neighborhood\"].map(lookup)\n)\nlookup","3f4ebeb5":"stacked_DF.loc[stacked_DF[\"Train\"] == 1, \"Electrical\"].mode()","fef6f8c1":"# \u7f3a\u5931\u503c\u586b\u5145\n# Assuming Neighborhood and MSZoning are related. MSZoning\u4ee3\u8868\u533a\u57df\u7684\u7c7b\u578b\uff0c\u6bd4\u5982\u662f\u519c\u4e1a\u8fd8\u662f\u5de5\u4e1a\uff0c\u4ea6\u6216\u8005\u662f\u5c45\u6c11\u533a\n\nlookup = (\n    stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n    .groupby(by=\"Neighborhood\")[\"MSZoning\"]\n    .agg(pd.Series.mode)\n)\nstacked_DF[\"MSZoning\"] = stacked_DF[\"MSZoning\"].fillna(\n    stacked_DF[\"Neighborhood\"].map(lookup)\n)\n\n# Assuming KitchenQual and OverallQual are related. \u56e0\u4e3akitchen\u6bcf\u5bb6\u90fd\u6709\uff0c\u6240\u4ee5\u7f3a\u5931\u503c\u5e76\u4e0d\u610f\u5473\u7740no kitchen\uff0c\u53ef\u4ee5\u7528\u4f17\u6570\u586b\u5145\nlookup = (\n    stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n    .groupby(by=\"OverallQual\")[\"KitchenQual\"]\n    .agg(pd.Series.mode)\n)\nstacked_DF[\"KitchenQual\"] = stacked_DF[\"KitchenQual\"].fillna(\n    stacked_DF[\"OverallQual\"].map(lookup)\n)\n\n# \u641e\u6e05\u695a\u4e3a\u4ec0\u4e48\u6709\u7684\u7528missing\u586b\u5145\uff0c\u6709\u7684\u75280\uff0c\u8fd8\u6709\u7684\u7528mode\n# For these features I replace nan with a string indicator: \"missing\"\uff0c \u8fd9\u91cc\u603b\u5171\u670916\u4e2a\ncols_na_to_missing = {\n    \"Alley\", # Type of alley access to property\n    \"BsmtCond\", # Evaluates the general condition of the basement\n    \"BsmtExposure\", # Refers to walkout or garden level walls\n    \"BsmtFinType1\", # Rating of basement finished area\n    \"BsmtFullBath\", # Basement full bathrooms\n    \"BsmtQual\", # Evaluates the height of the basement\n    \"Fence\",# Fence quality\n    \"FireplaceQu\", # Fireplace quality, \u706b\u7089\n    \"GarageCond\", # Garage condition\n    \"GarageFinish\",\n    \"GarageQual\",\n    \"GarageType\",\n    \"MasVnrType\", # \u5899\u4f53\u7c7b\u578b \u5b83\u7684NA\u610f\u5473\u7740\u4ec0\u4e48\uff1f\n    \"MiscFeature\", # \u5176\u4ed6\u6742\u4e71\u7684\u9879, NA\u5c31\u662fnone\n    \"PoolQC\", # PoolQC\u6709\u5f88\u591anan\uff0c\u610f\u5473\u7740\u6ca1\u6709pool\uff0c\u5c31\u7528missing\u53bb\u586b\u5145\n    \"BsmtFinType2\", #Rating of basement finished area\n}\n\n# For these features I replace nan with the integer 0\uff0c\u8fd9\u91cc\u67099\u4e2a\ncols_na_to_zero = {\n     'BsmtUnfSF', # Unfinished square feet of basement area,\u4e3a\u4ec0\u4e48\u6ce8\u91ca\u6389\u4e86? \u5e94\u8be5\u8981\u5305\u62ec\u8fdb\u6765\u7684\uff0c\u56e0\u4e3abasement\u53ef\u4ee5\u6ca1\u6709\n    \"GarageArea\", # \u4e3a\u4ec0\u4e48Area\u8bbe\u4e3a0:\u610f\u5473\u6ca1\u6709\u505c\u8f66\u573a\n    \"GarageCars\",\n    \"TotalBsmtSF\", # Total square feet of basement area\n    \"MasVnrArea\", # \u5899\u4f53\u9762\u79ef\n    \"BsmtFinSF1\", # Type 1 finished square feet\n    \"BsmtFinSF2\",\n    \"BsmtFullBath\", # Basement full bathrooms\n    \"BsmtHalfBath\",\n    \"GarageYrBlt\", # Year garage was built\n}\n\n# For these features I replace nan with the mode of the feature the record is missing,\u8fd9\u91cc\u67096\u4e2a \ncols_na_to_mode = { # \u56e0\u4e3a\u4e0b\u9762\u8fd9\u4e9b\uff0c\u6bcf\u4e00\u4e2a\u623f\u5b50\u90fd\u4e00\u5b9a\u4f1a\u6709\n    \"Functional\",\n    \"Electrical\",\n    \"Utilities\",\n    \"Exterior1st\",\n    \"Exterior2nd\",\n    \"SaleType\",\n}\n\nfor col in cols_na_to_missing:\n    stacked_DF[col] = stacked_DF[col].astype(object).fillna(\"Missing\")\n\nfor col in cols_na_to_zero:\n    stacked_DF[col] = stacked_DF[col].astype(object).fillna(0)\n\nfor col in cols_na_to_mode:\n    stacked_DF[col] = (\n        stacked_DF[col]\n        .astype(object)\n        .fillna(stacked_DF.loc[stacked_DF[\"Train\"] == 1, col].mode()[0])\n    )\n","454739aa":"print(\"Missing Value Counts in Train DF\")\nstacked_DF[stacked_DF[\"Train\"] == 1].isna().sum(axis=0)[\n    stacked_DF[stacked_DF[\"Train\"] == 1].isna().sum() > 0\n].sort_values(ascending=False)","44abca1a":"print(\"Missing Value Counts in test DF\")\nstacked_DF[stacked_DF[\"Train\"] == 0].isna().sum(axis=0)[\n    stacked_DF[stacked_DF[\"Train\"] == 0].isna().sum() > 0\n].sort_values(ascending=False)","d6521264":"# \u8fd8\u5269\u4e0bLotFrontage(Linear feet of street connected to property)\u8fd9\u4e2a\u53d8\u91cf\u672a\u586b\u5145\u7f3a\u5931\u503c\uff0c\u91c7\u7528iterative imputer\u7684\u65b9\u6cd5\u6765\u586b\u5145 \n# \u641e\u6e05\u695a iterative imputer\u662f\u4ec0\u4e48 \u5df2\u641e\u5b9a \n# Imputing remaining missing values with the help of iterative imputer.\nnum_features = stacked_DF.drop(columns=[\"Train\"]).select_dtypes(\"number\").columns\nimputer = IterativeImputer(\n    RandomForestRegressor(max_depth=8),\n    n_nearest_features=10,\n    max_iter=10,\n    random_state=random_state,\n)\nstacked_DF.loc[stacked_DF[\"Train\"] == 1, num_features] = imputer.fit_transform(\n    stacked_DF.loc[stacked_DF[\"Train\"] == 1, num_features].values\n)\nstacked_DF.loc[stacked_DF[\"Train\"] == 0, num_features] = imputer.transform(\n    stacked_DF.loc[stacked_DF[\"Train\"] == 0, num_features].values\n)\n","0bc35d09":"print(\"Missing Values in Test DF\")\nstacked_DF[stacked_DF[\"Train\"] == 1].isna().sum()[\n    stacked_DF[stacked_DF[\"Train\"] == 1].isna().sum() > 0\n].sort_values(ascending=False)","9f20ac36":"#np.where(stacked_DF[\"MoSold\"].isin([10, 11, 12, 1, 2, 3]),0,1)\nstacked_DF[\"Train\"] == 1","0ebe2aad":"stacked_DF","b405f5c0":"cat_features = [col for col in stacked_DF.select_dtypes(exclude=[\"number\"]).columns]\nstacked_DF[cat_features].nunique(axis=0).sort_values(ascending=False)","e15a3520":"ext2_map = {\"AsphShn\": \"Oth1\", \"CBlock\": \"Oth1\", \"CmentBd\": \"Oth2\", \"Other\": \"Oth2\"}\nstacked_DF[\"Exterior2nd\"] = (\n    stacked_DF[\"Exterior2nd\"].map(ext2_map).fillna(stacked_DF[\"Exterior2nd\"])\n)\nstacked_DF[\"Exterior2nd\"].unique()","842f850c":"# \u628a\u67d0\u4e9bfeature\u91cc\u9762\u7684\u53d6\u503c\u5f52\u6210\u66f4\u5c0f\u7684\u7c7b\n# I combine underrepresented categories under one umbrella and\/or with another category in the same field\next2_map = {\"AsphShn\": \"Oth1\", \"CBlock\": \"Oth1\", \"CmentBd\": \"Oth2\", \"Other\": \"Oth2\"}\ncond2_map = {\"PosA\": \"Pos\", \"PosN\": \"Pos\", \"RRAe\": \"RRe\", \"RRNe\": \"RRe\", \"RRNn\": \"RRn\", \"RRAn\": \"RRn\" }   \nroofmatl_map = {\n    \"Roll\": \"Oth1\",\n    \"ClyTile\": \"Oth1\",\n    \"Metal\": \"Oth1\",\n    \"CompShg\": \"Oth1\",\n    \"Membran\": \"Oth2\",\n    \"WdShake\": \"Oth2\",\n}\n\n\nstacked_DF[\"Exterior2nd\"] = (\n    stacked_DF[\"Exterior2nd\"].map(ext2_map).fillna(stacked_DF[\"Exterior2nd\"])\n)\nstacked_DF[\"Condition2\"] = (\n    stacked_DF[\"Condition2\"].map(cond2_map).fillna(stacked_DF[\"Condition2\"])\n)\nstacked_DF[\"RoofMatl\"] = (\n    stacked_DF[\"RoofMatl\"].map(roofmatl_map).fillna(stacked_DF[\"RoofMatl\"])\n)\n","a0bc1ad4":"field_val = (\n    stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n    .groupby(by=\"BsmtQual\")[\"SalePrice\"]\n    .median()\n    .sort_values()\n    .index\n)\n\nrank = np.array(\n        range(0, len(stacked_DF.loc[stacked_DF[\"Train\"] == 1, \"BsmtQual\"].unique()))\n    )\ndict(zip(field_val, rank))","92692d22":"# Converting MSSubClass to categorical\nstacked_DF[\"MSSubClass\"] = stacked_DF[\"MSSubClass\"].astype(str)\n\n# \u4e3a\u4ec0\u4e48\u8981\u6784\u9020\u8fd9\u4e9b\u53d8\u91cf\u5462\uff1f\n# \u5c06categorical\u53d8\u91cf\u8fd8\u6709\u6570\u503c\u53d8\u91cf\u6392\u540d\uff0c\u6784\u9020\u65b0\u7684col_name_ranking\u53d8\u91cf\n# Below I establish ranking among categories within a handful of features,\n# Ranking is based on the median SalePrice they show for each category.\ncategories_to_rank = [\n    \"BsmtQual\",\n    \"ExterQual\",\n    \"ExterCond\",\n    \"Exterior1st\",\n    \"FireplaceQu\",\n    \"GarageCond\",\n    \"GarageQual\",\n    \"Heating\",\n    \"Fence\", # categorical\u53d8\u91cf\n    \"HeatingQC\",\n    \"OverallQual\",# \u6570\u503c\u53d8\u91cf\n    \"OverallCond\",\n    \"HouseStyle\",\n    \"KitchenQual\",\n    \"LotShape\",\n    \"BsmtCond\",\n    \"MSSubClass\",\n    \"Neighborhood\",\n    \"SaleCondition\",\n    \"SaleType\",\n    \"MasVnrType\",\n    \"ExterQual\",\n    \"Condition1\", # \u52a0\u7684\n    \"Condition2\" # \u52a0\u7684, \u4e0d\u5bf9\uff0c \u8981\u6620\u5c04\u6210\u6570\u503c\n]\n\nfor col in categories_to_rank:\n    rank = np.array(\n        range(0, len(stacked_DF.loc[stacked_DF[\"Train\"] == 1, col].unique()))\n    )\n    field_val = (\n        stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n        .groupby(by=col)[\"SalePrice\"]\n        .median()\n        .sort_values()\n        .index\n    )\n    rankval_mapping = dict(zip(field_val, rank))\n    stacked_DF[col + \"_ranking\"] = stacked_DF[col].map(\n        rankval_mapping, na_action=\"ignore\"\n    )\n\n# There is an MSSubClass category in test set but not in train. This creates an nan record during ranking.\n# I impute that nan record with the mode of the field (based on the train set)\nstacked_DF.loc[stacked_DF[\"MSSubClass_ranking\"].isna(), \"MSSubClass_ranking\"] = 10\n","fab61e77":"len(categories_to_rank)\nrank_tmp = [categories_to_rank[i:i+5] for i in range(0,25,5)]\npd.DataFrame(rank_tmp)","62cf3720":"# \u5728\u5bd2\u51b7\u5b63\u8282\u8fd8\u65f6\u6696\u548c\u5b63\u8282\u4e70\u623f\nstacked_DF[\"WarmSeason\"] = np.where(\n    stacked_DF[\"MoSold\"].isin([10, 11, 12, 1, 2, 3]), 0, 1\n)\n\n# For ex, if PoolArea = 0 , Then HasPool = 0 too\nstacked_DF['haspool'] = stacked_DF['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nstacked_DF['has2ndfloor'] = stacked_DF['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nstacked_DF['hasgarage'] = stacked_DF['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nstacked_DF['hasbsmt'] = stacked_DF['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nstacked_DF['hasfireplace'] = stacked_DF['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n# GrLivArea\uff1a Above grade (ground) living area square feet\n# TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n# FullBath: Full bathrooms above grade\n# HalfBath: Half baths above grade\n# Kitchen: Kitchens above grade\nstacked_DF[\"SqFtPerRoom\"] = stacked_DF[\"GrLivArea\"] \/ (\n    stacked_DF[\"TotRmsAbvGrd\"]\n    + stacked_DF[\"FullBath\"]\n    + stacked_DF[\"HalfBath\"]\n    + stacked_DF[\"KitchenAbvGr\"]\n)","6982ab07":"sqft_price_table = (\n    stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n    .groupby(by=\"Neighborhood\")[\"SalePrice\", \"GrLivArea\"]\n    .agg(pd.Series.sum)\n)\n\n\nsqft_price_table[\"AvgPricePerSqFt\"] = (\n    sqft_price_table[\"SalePrice\"] \/ sqft_price_table[\"GrLivArea\"]\n)\n\nsqft_price_table.drop(columns=[\"SalePrice\", \"GrLivArea\"], inplace=True)\nstacked_DF[\"AvgPricePerSqFtPerHood\"] = stacked_DF[\"Neighborhood\"].map(\n    sqft_price_table.to_dict()[\"AvgPricePerSqFt\"]\n)","9f713db6":"#stacked_DF['BsmtFullBath']\n#stacked_DF['BsmtFullBath']\n#stacked_DF['BsmtFullBath']\n#(0.5 * stacked_DF['BsmtHalfBath'])).apply(str)\n\n# \u8981\u5c06BsmtHalfBath\u4ecefloat\u8f6c\u6362\u4e3astr\nstacked_DF['BsmtHalfBath']\n#stacked_DF['Total_Bathrooms'] = (stacked_DF['FullBath'] + (0.5 * stacked_DF['HalfBath']) +\n#                         stacked_DF['BsmtFullBath'] + (0.5 * stacked_DF['BsmtHalfBath']))","210bc489":"# \u7279\u5f81\u4ea4\u53c9\n# \u4ea4\u53c9\u7684\u4f9d\u636e\u662f\u4ec0\u4e48\n# Below I establish new features, mainly via feature crossing\n\nstacked_DF[\"QualCond\"] = (\n    stacked_DF[\"OverallQual_ranking\"] * stacked_DF[\"OverallCond_ranking\"] # OverallQual: Rates the overall material and finish of the house\n) # OverallCond: Rates the overall condition of the house\n\nstacked_DF[\"HighQualSF\"] = stacked_DF[\"1stFlrSF\"] + stacked_DF[\"2ndFlrSF\"] # 1stFlrSF: First Floor square feet\n\nstacked_DF[\"HoodNExtCond\"] = (\n    stacked_DF[\"Neighborhood_ranking\"] * stacked_DF[\"ExterCond_ranking\"]\n)\n\nstacked_DF[\"HoodNPrivacy\"] = (\n    stacked_DF[\"Neighborhood_ranking\"] * stacked_DF[\"Fence_ranking\"]\n)\nstacked_DF[\"AreaOverallQualCond\"] = (\n    stacked_DF[\"HighQualSF\"]\n    * stacked_DF[\"OverallQual_ranking\"]\n    * stacked_DF[\"OverallCond_ranking\"]\n)\nstacked_DF[\"KitchenQCHighQualSF\"] = (\n    stacked_DF[\"HighQualSF\"] * stacked_DF[\"KitchenQual_ranking\"]\n)\nstacked_DF[\"HoodNOverallQual\"] = (\n    stacked_DF[\"Neighborhood_ranking\"] * stacked_DF[\"OverallQual_ranking\"]\n)\nstacked_DF[\"HoodNMasVnrType\"] = (\n    stacked_DF[\"Neighborhood_ranking\"] * stacked_DF[\"MasVnrType_ranking\"]\n)\nstacked_DF[\"HoodNKitchenQual\"] = (\n    stacked_DF[\"Neighborhood_ranking\"] * stacked_DF[\"KitchenQual_ranking\"]\n)\n#stacked_DF[\"HoodNCond1\"] = stacked_DF[\"Neighborhood_ranking\"] * stacked_DF[\"Condition1\"]\n#stacked_DF[\"HoodNCond2\"] = stacked_DF[\"Neighborhood_ranking\"] * stacked_DF[\"Condition2\"]\nstacked_DF[\"HoodNPorch\"] = stacked_DF[\"Neighborhood_ranking\"] * (\n    stacked_DF[\"3SsnPorch\"] + stacked_DF[\"EnclosedPorch\"] + stacked_DF[\"OpenPorchSF\"]\n)\n\nstacked_DF[\"Age_YrBuilt\"] = stacked_DF[\"YrSold\"] - stacked_DF[\"YearBuilt\"]\nstacked_DF[\"Age_YrRemod\"] = stacked_DF[\"YrSold\"] - stacked_DF[\"YearRemodAdd\"]\nstacked_DF[\"Age_Garage\"] = stacked_DF[\"YrSold\"] - stacked_DF[\"GarageYrBlt\"]\nstacked_DF[\"Remodeled\"] = stacked_DF[\"YearBuilt\"] != stacked_DF[\"YearRemodAdd\"]\nstacked_DF[\"Age_YrBuilt\"] = stacked_DF[\"Age_YrBuilt\"].apply(lambda x: 0 if x < 0 else x)\nstacked_DF[\"Age_YrRemod\"] = stacked_DF[\"Age_YrRemod\"].apply(lambda x: 0 if x < 0 else x)\nstacked_DF[\"Age_Garage\"] = stacked_DF[\"Age_Garage\"].apply(lambda x: 0 if x < 0 else x)\n\n\n# \u4e0b\u9762\u662f\u522b\u7684notebook\u7684\n\nstacked_DF['TotalHouseSF'] = stacked_DF['TotalBsmtSF'] + stacked_DF['1stFlrSF'] + stacked_DF['2ndFlrSF']\n\n# TotalArea \u91cd\u8981\nstacked_DF['TotalArea'] = stacked_DF['TotalBsmtSF'] + stacked_DF['1stFlrSF'] + stacked_DF['2ndFlrSF'] + stacked_DF['GrLivArea'] + stacked_DF['GarageArea']\n\n\nstacked_DF['TotalBath'] = stacked_DF['FullBath'] + 0.5 * stacked_DF['HalfBath']\n\nstacked_DF['TotalLot'] = stacked_DF['LotFrontage'] + stacked_DF['LotArea']\nstacked_DF['TotalPorch'] = stacked_DF['OpenPorchSF'] + stacked_DF['EnclosedPorch'] + stacked_DF['ScreenPorch']\n\nstacked_DF['Total_sqr_footage'] = (stacked_DF['BsmtFinSF1'] + stacked_DF['BsmtFinSF2'] +\n                           stacked_DF['1stFlrSF'] + stacked_DF['2ndFlrSF'])\n\nstacked_DF['TotalBsmtFin'] = stacked_DF['BsmtFinSF1'] + stacked_DF['BsmtFinSF2']\n\nstacked_DF['Total_porch_sf'] = (stacked_DF['OpenPorchSF'] + stacked_DF['3SsnPorch'] +\n                        stacked_DF['EnclosedPorch'] + stacked_DF['ScreenPorch'] +\n                        stacked_DF['WoodDeckSF'])\n\n\n# Dropping a handful of features as there are other variables that are perfectly correlated to these\n# I did trial and error here based on the impact of removing features on RMSE.\nstacked_DF.drop(columns=[\"GarageYrBlt\", \"Utilities\"], inplace=True)\n\n# Would like to visualize some of the newly established features to see their relationship with target.\n# Hoping to see some correlation\ncols_to_viz = [\n    \"HighQualSF\", # 1\u697c\u548c2\u697c\u7684\u4f4f\u623f\u9762\u79ef\n    \"KitchenQCHighQualSF\",\n    \"AreaOverallQualCond\",\n    \"HoodNOverallQual\",\n    \"HoodNMasVnrType\",\n    \"HoodNKitchenQual\",\n]","02db28f6":"# \u9009\u4e00\u4e9b\u65b0feature\u505a\u53ef\u89c6\u5316\nncols = 1\nnrows = math.ceil(len(cols_to_viz) \/ ncols)\nunused = (nrows * ncols) - len(cols_to_viz)\n\n\nfigw, figh = ncols * 10, nrows * 8\n\nfig, ax = plt.subplots(nrows, ncols, sharey=True, figsize=(figw, figh))\nfig.subplots_adjust(hspace=0.2, wspace=0.2)\nax = ax.flatten()\nfor i in range(unused, 0, -1):\n    fig.delaxes(ax[-i])\n\n\nfor n, col in enumerate(cols_to_viz):\n    if n % 2 != 0:\n        ax[n].yaxis.label.set_visible(False)\n    ax[n].set_xlabel(col)\n    ax[n].set_ylabel(\"SalePrice\")\n    sns.scatterplot(\n        x=col,\n        y=\"SalePrice\",\n        data=stacked_DF.loc[stacked_DF[\"Train\"] == 1],\n        hue=\"SalePrice\",\n        palette='gist_earth',\n        s=75,\n        legend=False,\n        ax=ax[n],\n    )\n\nplt.show()\n","fdf080f6":"output_notebook()\n\ndf_to_viz = stacked_DF[stacked_DF[\"Train\"] == 1].drop(columns=\"Train\")\n\nxcorr = abs(df_to_viz.corr())\nxcorr.index.name = \"Feature1\"\nxcorr.columns.name = \"Feature2\"\n\ndf = pd.DataFrame(xcorr.stack(), columns=[\"Corr\"]).reset_index()\n\nsource = ColumnDataSource(df)\n\ncolors = [\n    \"#75968f\",\n    \"#a5bab7\",\n    \"#c9d9d3\",\n    \"#e2e2e2\",\n    \"#dfccce\",\n    \"#ddb7b1\",\n    \"#cc7878\",\n    \"#933b41\",\n    \"#550b1d\",\n]\n\nmapper = LinearColorMapper(palette=colors, low=df.Corr.min(), high=df.Corr.max())\n\nf1 = figure(\n    plot_width=1000,\n    plot_height=1000,\n    title=\"Correlation Heat Map\",\n    x_range=list(sorted(xcorr.index)),\n    y_range=list(reversed(sorted(xcorr.columns))),\n    toolbar_location=None,\n    tools=\"hover\",\n    x_axis_location=\"above\",\n)\n\nf1.rect(\n    x=\"Feature2\",\n    y=\"Feature1\",\n    width=1,\n    height=1,\n    source=source,\n    line_color=None,\n    fill_color=transform(\"Corr\", mapper),\n)\n\ncolor_bar = ColorBar(\n    color_mapper=mapper,\n    location=(0, 0),\n    ticker=BasicTicker(desired_num_ticks=len(colors)),\n    formatter=PrintfTickFormatter(format=\"%d%%\"),\n)\nf1.add_layout(color_bar, \"right\")\n\nf1.hover.tooltips = [\n    (\"Feature1\", \"@{Feature1}\"),\n    (\"Feature2\", \"@{Feature2}\"),\n    (\"Corr\", \"@{Corr}{1.1111}\"),\n]\n\nf1.axis.axis_line_color = None\nf1.axis.major_tick_line_color = None\nf1.axis.major_label_text_font_size = \"10px\"\nf1.axis.major_label_standoff = 2\nf1.xaxis.major_label_orientation = 1.0\n\nshow(f1)\n","cfb07290":"pd.DataFrame(stacked_DF[stacked_DF[\"Train\"] == 1].skew()[\n    abs(stacked_DF[stacked_DF[\"Train\"] == 1].skew()) > 5\n])","d56ec15d":"# threshold\u8bbe\u4e3a5\u7684\u8bdd\uff0cscore\u4e3a0.11948\nhighly_skewed_cols = (\n    stacked_DF[stacked_DF[\"Train\"] == 1]\n    .skew()[abs(stacked_DF[stacked_DF[\"Train\"] == 1].skew()) > 6]\n    .index.to_list()\n)\n\nptransformer = PowerTransformer(standardize=False)\n\nstacked_DF.loc[\n    stacked_DF[\"Train\"] == 1, highly_skewed_cols\n] = ptransformer.fit_transform(\n    stacked_DF.loc[stacked_DF[\"Train\"] == 1, highly_skewed_cols]\n)\nstacked_DF.loc[stacked_DF[\"Train\"] == 0, highly_skewed_cols] = ptransformer.transform(\n    stacked_DF.loc[stacked_DF[\"Train\"] == 0, highly_skewed_cols]\n)","809f0664":"#stacked_DF.dtypes\n#stacked_DF.select_dtypes(include=[\"number\"]).columns\n#np.sort([0, 1], axis=0)\n# cat_features = [col for col in stacked_DF.select_dtypes(exclude=[\"number\"]).columns]\n# num_features = [\n#     col\n#     for col in stacked_DF.select_dtypes(include=[\"number\"]).columns\n#     if col not in (bool_features) and col != \"SalePrice\"\n# ]\n# cat_features\nstacked_DF.head(3)","5477ce3a":"# Obtaining a list of categorical, numerical, and boolean - like features.\nbool_features = [\n    col\n    for col in stacked_DF.select_dtypes(include=[\"number\"]).columns\n    if np.array_equal(\n        np.sort(stacked_DF[col].unique(), axis=0), np.sort([0, 1], axis=0)\n    )\n]\n\ncat_features = [col for col in stacked_DF.select_dtypes(exclude=[\"number\"]).columns]\nnum_features = [\n    col\n    for col in stacked_DF.select_dtypes(include=[\"number\"]).columns\n    if col not in (bool_features) and col != \"SalePrice\"\n]\n\n# Holding these two DF 's on the side.\n# Will need to concatenate later with the preprocessed(scaled and oh encoded) DF.\nbool_features.remove(\"Train\")\nX_train_bool = stacked_DF.loc[stacked_DF[\"Train\"] == 1, bool_features]\nX_test_bool = stacked_DF.loc[stacked_DF[\"Train\"] == 0, bool_features]","6982b1d9":"len(num_features)","0c2ba63e":"cat_tmp = [cat_features[i:i+5] for i in range(0,45,9)]\npd.DataFrame(cat_tmp)","0d679719":"num_tmp = [num_features[i:i+5] for i in range(0,80,5)]\npd.DataFrame(num_tmp)","1bf2bb88":"# This list contains features that has the same set of values between train - test\nohe_cols_a = []\n\n# This list contains features that has different set of values between train - test\nohe_cols_b = []\n\nfor col in cat_features:\n    if set(stacked_DF.loc[stacked_DF[\"Train\"] == 1, col].unique()) != set(\n        stacked_DF.loc[stacked_DF[\"Train\"] == 0, col].unique()\n    ):\n        ohe_cols_b.append(col)\n\nohe_cols_a = list(set(cat_features) - set(ohe_cols_b))\nohe_cols_a","b4b23c0a":"X_train = stacked_DF.loc[stacked_DF[\"Train\"] == 1].drop(\n    labels=[\"SalePrice\", \"Train\"], axis=1\n)\n# Applying log transformation to the target variable\ny_train = stacked_DF.loc[stacked_DF[\"Train\"] == 1, \"SalePrice\"].apply(np.log)\nX_test = stacked_DF.loc[stacked_DF[\"Train\"] == 0].drop(\n    labels=[\"SalePrice\", \"Train\"], axis=1\n)\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"onehota\", OneHotEncoder(sparse=False, drop=\"first\"), ohe_cols_a),\n        (\"onehotb\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"), ohe_cols_b),\n        (\"scaler\", StandardScaler(), num_features),\n    ],\n    remainder=\"drop\",\n)\n\n\npipeline = Pipeline(\n    [(\"Preprocessor\", preprocessor), (\"VarThreshold\", VarianceThreshold(0.01))]\n)\n\nX_train_preprocessed = pipeline.fit_transform(X_train)\nX_test_preprocessed = pipeline.transform(X_test)","9b962790":"# Get the list of one hot encoded columns and combine them\noh_encoded_a = list(\n    preprocessor.named_transformers_.onehota.get_feature_names(ohe_cols_a)\n)\noh_encoded_b = list(\n    preprocessor.named_transformers_.onehotb.get_feature_names(ohe_cols_b)\n)\noh_encoded_cols = oh_encoded_a + oh_encoded_b\noh_encoded_a","2563240a":"feature_names = np.array(oh_encoded_cols + num_features, order=\"K\")\n#feature_names\n# Filtering out the features dropped by variance threshold\nfeature_names = feature_names[pipeline.named_steps.VarThreshold.get_support()]\nfeature_names","948f6b31":"\n# Putting back the column names to help with analysis\nX_train_preprocessed = pd.DataFrame(data=X_train_preprocessed, columns=feature_names)\nX_test_preprocessed = pd.DataFrame(\n    data=X_test_preprocessed, columns=feature_names, index=X_test_bool.index\n)\n\n# Combine the DF's back together\nX_train = pd.concat([X_train_bool, X_train_preprocessed], axis=1)\nX_test = pd.concat([X_test_bool, X_test_preprocessed], axis=1)\n","55150cbf":"X_train.head(3)","5147fa59":"model = Lasso(alpha=0.01)\nmodel.fit(X_train, y_train)\n\n\nfeature_imp = permutation_importance(\n    model, X_train, y_train, n_repeats=10, n_jobs=-1, random_state=random_state\n)\n\nperm_ft_imp_df = pd.DataFrame(\n    data=feature_imp.importances_mean, columns=[\"FeatureImp\"], index=X_train.columns\n).sort_values(by=\"FeatureImp\", ascending=False)\nmodel_ft_imp_df = pd.DataFrame(\n    data=model.coef_, columns=[\"FeatureImp\"], index=X_train.columns\n).sort_values(by=\"FeatureImp\", ascending=False)\n\nfig, ax = plt.subplots(2, 1, figsize=(12, 22))\n\nperm_ft_imp_df_nonzero = perm_ft_imp_df[perm_ft_imp_df[\"FeatureImp\"] != 0]\nmodel_ft_imp_df_nonzero = model_ft_imp_df[model_ft_imp_df[\"FeatureImp\"] != 0]\n\nsns.barplot(\n    x=perm_ft_imp_df_nonzero[\"FeatureImp\"],\n    y=perm_ft_imp_df_nonzero.index,\n    ax=ax[0],\n    palette=\"vlag\",\n)\nsns.barplot(\n    x=model_ft_imp_df_nonzero[\"FeatureImp\"],\n    y=model_ft_imp_df_nonzero.index,\n    ax=ax[1],\n    palette=sns.diverging_palette(10, 220, sep=2, n=80),\n)\n\nax[0].set_title(\"Permutation Feature Importance\")\nax[1].set_title(\"Lasso Feature Importance\")\n\nplt.show()","6acdf149":"# cv = KFold(n_splits=4, random_state=random_state)\n# def objective(trial):\n#     _C = trial.suggest_float(\"C\", 0.1, 0.5)\n#     _epsilon = trial.suggest_float(\"epsilon\", 0.01, 0.1)\n#     _coef = trial.suggest_float(\"coef0\", 0.5, 1)\n\n#     svr = SVR(cache_size=5000, kernel=\"poly\", C=_C, epsilon=_epsilon, coef0=_coef)\n\n#     score = cross_val_score(\n#         svr, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\"\n#     ).mean()\n#     return score\n\n\n# optuna.logging.set_verbosity(0)\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=100)\n\n# svr_params = study.best_params\n# svr_best_score = study.best_value\n# print(f\"Best score:{svr_best_score} \\nOptimized parameters: {svr_params}\")","c7092996":"# def objective(trial):\n\n#     _alpha = trial.suggest_float(\"alpha\", 0.5, 1)\n#     _tol = trial.suggest_float(\"tol\", 0.5, 0.9)\n\n#     ridge = Ridge(alpha=_alpha, tol=_tol, random_state=random_state)\n\n#     score = cross_val_score(\n#         ridge, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\"\n#     ).mean()\n#     return score\n\n\n# optuna.logging.set_verbosity(0)\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=100)\n\n# ridge_params = study.best_params\n# ridge_best_score = study.best_value\n# print(f\"Best score:{ridge_best_score} \\nOptimized parameters: {ridge_params}\")\n","140702b7":"# def objective(trial):\n\n#     _alpha = trial.suggest_float(\"alpha\", 0.0001, 0.01)\n\n#     lasso = Lasso(alpha=_alpha, random_state=random_state)\n\n#     score = cross_val_score(\n#         lasso, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\"\n#     ).mean()\n#     return score\n\n\n# optuna.logging.set_verbosity(0)\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=100)\n\n# lasso_params = study.best_params\n# lasso_best_score = study.best_value\n# print(f\"Best score:{lasso_best_score} \\nOptimized parameters: {lasso_params}\")\n","a897b1fb":"# def objective(trial):\n#     _n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n#     _max_depth = trial.suggest_int(\"max_depth\", 5, 12)\n#     _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 8)\n#     _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 3, 6)\n#     _max_features = trial.suggest_int(\"max_features\", 10, 50)\n\n#     rf = RandomForestRegressor(\n#         max_depth=_max_depth,\n#         min_samples_split=_min_samp_split,\n#         #ccp_alpha=_ccp_alpha,\n#         min_samples_leaf=_min_samples_leaf,\n#         max_features=_max_features,\n#         n_estimators=_n_estimators,\n#         n_jobs=-1,\n#         random_state=random_state,\n#     )\n\n#     score = cross_val_score(\n#         rf, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\"\n#     ).mean()\n#     return score\n\n\n# optuna.logging.set_verbosity(0)\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=100)\n\n# rf_params = study.best_params\n# rf_best_score = study.best_value\n# print(f\"Best score:{rf_best_score} \\nOptimized parameters: {rf_params}\")\n","40312ded":"# def objective(trial):\n#     _num_leaves = trial.suggest_int(\"num_leaves\", 5, 20)\n#     _max_depth = trial.suggest_int(\"max_depth\", 3, 8)\n#     _learning_rate = trial.suggest_float(\"learning_rate\", 0.1, 0.4)\n#     _n_estimators = trial.suggest_int(\"n_estimators\", 50, 150)\n#     _min_child_weight = trial.suggest_float(\"min_child_weight\", 0.2, 0.6)\n\n#     lgbm = LGBMRegressor(\n#         num_leaves=_num_leaves,\n#         max_depth=_max_depth,\n#         learning_rate=_learning_rate,\n#         n_estimators=_n_estimators,\n#         min_child_weight=_min_child_weight,\n#     )\n\n#     score = cross_val_score(\n#         lgbm, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\"\n#     ).mean()\n#     return score\n\n\n# optuna.logging.set_verbosity(0)\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=100)\n\n# lgbm_params = study.best_params\n# lgbm_best_score = study.best_value\n# print(f\"Best score:{lgbm_best_score} \\nOptimized parameters: {lgbm_params}\")\n","223f893d":"# threshold\u8bbe\u4e3a5\u7684\u8bdd\uff0cscore\u4e3a0.11948\n# rf_params = {\"max_depth\": 11, \"max_features\": 38, \"n_estimators\": 167, 'min_samples_split': 3, 'min_samples_leaf': 3}\n# svr_params = {\n#     \"kernel\": \"poly\",\n#     \"C\": 0.10952244232464854,\n#     \"epsilon\": 0.051787227114861695,\n#     \"coef0\": 0.9643705899263619,\n# }\n# ridge_params = {\n#     \"alpha\": 0.9999937584810443,\n#     \"tol\": 0.8363259432571677,\n#     \"solver\": \"cholesky\",\n# }\n# lasso_params = {\"alpha\": 0.0001785478329689988, \"selection\": \"random\"}\n# lgbm_params = {\n#     \"num_leaves\": 5,\n#     \"max_depth\": 7,\n#     \"learning_rate\": 0.12027160881285007,\n#     \"n_estimators\": 150,\n#     \"min_child_weight\": 0.36458656654487176,\n# }\n\n# threshold\u8bbe\u7f6e\u4e3a4\uff0c score\u4e3a0.11955\nrf_params = {\"max_depth\": 11, \"max_features\": 33, \"n_estimators\": 153, 'min_samples_split': 3, 'min_samples_leaf': 3}\nsvr_params = {\n    \"kernel\": \"poly\",\n    \"C\": 0.10068558674268308,\n    \"epsilon\": 0.0502342620642869,\n    \"coef0\": 0.9940297816998438,\n}\nridge_params = {\n    \"alpha\": 0.9999482839195368,\n    \"tol\": 0.8413165803982278,\n    \"solver\": \"cholesky\",\n}\nlasso_params = {\"alpha\": 0.00019845957465597944, \"selection\": \"random\"}\nlgbm_params = {\n    \"num_leaves\": 16,\n    \"max_depth\": 3,\n    \"learning_rate\": 0.11142530414723092,\n    \"n_estimators\": 150,\n    \"min_child_weight\": 0.27272834311556926,\n}\n","d647e9ce":"# cv = KFold(n_splits=4, random_state=random_state)\n\n# svr = SVR(**svr_params)\n# ridge = Ridge(**ridge_params, random_state=random_state)\n# lasso = Lasso(**lasso_params, random_state=random_state)\n# lgbm = LGBMRegressor(**lgbm_params, random_state=random_state)\n# rf = RandomForestRegressor(**rf_params, random_state=random_state)\n# stack = StackingCVRegressor(\n#     regressors=[svr, ridge, lasso, lgbm, rf],\n#     meta_regressor=LinearRegression(n_jobs=-1),\n#     random_state=random_state,\n#     cv=cv,\n#     n_jobs=-1,\n# )\n\n# svr_scores = cross_val_score(\n#     svr, X_train, y_train, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n# )\n# ridge_scores = cross_val_score(\n#     ridge, X_train, y_train, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n# )\n# lasso_scores = cross_val_score(\n#     lasso, X_train, y_train, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n# )\n# lgbm_scores = cross_val_score(\n#     lgbm, X_train, y_train, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n# )\n# rf_scores = cross_val_score(\n#     rf, X_train, y_train, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n# )\n# stack_scores = cross_val_score(\n#     stack, X_train, y_train, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n# )\n\n# scores = [svr_scores, ridge_scores, lasso_scores, lgbm_scores, rf_scores, stack_scores]\n# models = [\"SVR\", \"RIDGE\", \"LASSO\", \"LGBM\", \"RF\", \"STACK\"]\n# score_medians = [\n#     round(np.median([mean for mean in modelscore]), 5) for modelscore in scores\n# ]\n","85c386e1":"fig, ax = plt.subplots(figsize=(14, 8))\n\nvertical_offset = 0.001\n\nax.set_title(\"Model Score Comparison\")\nbp = sns.boxplot(x=models, y=scores, ax=ax)\n\n\nfor xtick in bp.get_xticks():\n    bp.text(\n        xtick,\n        score_medians[xtick] - vertical_offset,\n        score_medians[xtick],\n        horizontalalignment=\"center\",\n        size=18,\n        color=\"w\",\n        weight=\"semibold\",\n    )\n\nplt.show()\n","94cf5acb":"stack.fit(X_train.values, y_train.values)\n\npredictions = stack.predict(X_test.values)\npredictions = np.exp(predictions)\n\nsubmission = pd.DataFrame({\"Id\": submission_ID, \"SalePrice\": predictions})\nsubmission.to_csv(\"submission.csv\", index=False)","e20fbc77":"os.getcwd()","6bbd75d1":"from IPython.display import FileLink\nFileLink(r'submission.csv')","c5cadf9d":"# Model Comparison\n\nLet's use cross-validate-score to help us see how different models perform.","95134e16":"# Final Submission\n\nBelow I submit the inverse log-transformed results.<br><br>","32b14b5f":"# Final Preprocessing Steps\n\nIn the next two sections, I will apply one hot encoding to categoricals, and scaling on numericals (except boolean-like features). Once that's done, I will have all the transformed variables go through Variance Threshold. Variance Threshold will remove any feature that shows less variance than what I want.  ","69b0766a":"Extracting the ID since we need to use it for submission later.<br>\nI also concatenate train and test sets into one dataframe.","89b56e67":"When I run the models, I see that the stacked model scores slightly better than SVR.<br> Even if the accuracy of one or two individual models rated higher than stacked model, I still would have picked stacked model.<br> This is because I believe that it will do a better job at generalizing\/reduce impact of any overfitting compared to individual models. <br><br> Since we don't have a huge amount of data, the model runs quite fast! And it is ~92% accurate, I find this quite impressive.","e2b07823":"# Missing Values\n\nI will first look at raw missing counts. Then, I will visualize the dataframe to see if there is any relation between missing records\/features. Such relation would impact my imputing strategy for those records.","43817d1d":"Let's look at a sample of records...","416cd3a4":"# Target Variable Distribution - Univariate\n\nLet's look at the target variable distribution. I am creating an additional plot to show how log transformation impacts the variable since it's skewed.","fd48062f":"There are a few columns with different set of categories among train and test sets\nThis causes an issue for one hot encoder's \"drop\" rule.<br> Therefore, I am applying seperate OneHot encoding to two different subset of categoricals.","782ada66":"Looking at the figure above and reading data descriptions provided by data source, it seems like missing records **mostly** indicate the non-existance of a feature. <br> \nFor example, if basement condition is missing, it means that the record belongs to a house with no basement. Similarly, if PoolQC is missing, that means the house has no pool. However, there are a few exceptions to these rules.<br><br>\nTo keep things simple, I will mostly employ mass imputing strategy as opposed to deal with exceptions.","17afe45a":"**Descriptive statistics:**","61df9338":"**Checking to see data types and potential missing values**:","7b81b8ad":"# Feature Importance\n\nI will obtain feature importances using two models. Based on what I read online, Lasso seems like a good feature selection model for datasets like ours (high feature, low sample size). <br>In addition to Lasso, I'll be using sklearn's permutation feature importance model. The pitfall of permutation importance is that it doesn't do the best job when it comes to correlated independent variables. This is something to keep in mind when we're interpreting the results.","8e47b196":"# Hyperparameter optimizing with Optuna","31a7f7f9":"# Correlation Heatmap\n\nLet's look at a heatmap that shows correlations between our variables. I want to see a lot of red squares next to SalePrice. That means we have a lot of independent variables correlating with the target variable.<br><br> In an ideal scenario, we wouldn't see high correlation **among independent variables**; however, that's not the case. I trust our models will pick features that are important to them, without us needing to deal with multicollinearity.<br><br>\n\nAs useful as they are, heatmaps get messy pretty quickly dealing with high number of variables. Bokeh provides interactive plotting, which means I can hover over a red square to find out which two features have high correlation. Otherwise I'd have to figure those out looking at tick labels. ","274dcf97":"It looks like log transformation will be relatively successful gaussianize the target variable.\nEven after the log transformation, I see some outliers on the right hand side of the spectrum. However, I am not sure if there's anything I can do for those.\n\n","aac4a4b8":"**Looking at the graphs above, I find a few things to note:**\n\n* GrLivArea, YrBuilt, LotArea and a few others have linear-like relationship with the target variable.\n* There are a few outliers. Data source's notes mention these outliers, and recommends to remove them. I, however, don't like the idea of removing records as we're already working with limited amount of samples.\n\nMoving on with the categorical bivariate analysis.","dc53ed98":"Loading Train and Test data sets.","42206d43":"Below I impute missing records.","a46082fe":"Importing libraries.","40b77277":"Below I establish synthetic features via feature crossing and value mapping. These new features sometimes allow models capture patterns that they wouldn't be able to capture from individual features.<br> More about feature crossing -> https:\/\/developers.google.com\/machine-learning\/crash-course\/feature-crosses\/video-lecture","fe0abc66":"# Imputing Missing Values and Feature Engineering","b6f97282":"As I am happy with the input data, I won't be making any further changes to it. <br>\nLet's continue with model selection and hyperparameter optimization. <br>\n<br> Due to high feature\/low sample size of our data, there's a great chance of overfitting. Models with regularization mechanism such as Lasso and Ridge do well in regards to such data. In addition Lasso and Ridge, I'll use SVR, LGBM and RandomForest regressor. \n<br><br>\nFor hyperparameter tuning\/optimization, I first used Optuna's search algorithm, then did a little manual tweaking to further increase the accuracy. <br> Optuna works similar to GridSearch algorithm. What I like more about this one is that it gives me the opportunity to set a \"search range\" instead of me having to declare values one by one for it to search from.<br> I also feel it works faster compared to GridSearch. <br><br> I am leaving hyperparameter search section commented out, since it takes a bit of time to run and leaves a long\/ugly output. I stored the parameters in variables below.","07b8cb84":"# Bivariate Analyis\n\nI will first plot a few numeric variables against the target variable, then I will do the same for categorical variables.\nWhat I am looking for is correlation, outliers and the distribution of the target variable with respect to dependent variables.<br>\nScatter Plot is good for numeric variable visualization. For categorical variables, I will use Box-plot.","f2e9c8ba":"* OverallQual impacts SalePrice exponentially!\n* Neighborhood matters. Features regarding quality also matter.\n* **The more irregular the lot shape is, the higher the Sale Price seems**. This was a surprise to me. That being said, I can relate irregular lot shape to architectural originality, which costs money. Regular lot shape relates to just regular homes that more of us can afford. But this is only a guess.","979ab288":"**Dealing with highly skewed variables**\n\nI will first identify any variable with skew coefficient greater than 5, then I will apply yeo-johnson transformation on them.<br><br> Generally speaking, any variable with absolute skew coefficient greater than 1(and abs kurtosis greater than 2) is considered a red flag. That being said, when I apply transformation based on +1\/-1 threshold, the RMSE icreases significantly, and accuracy reduces (as well as the LB positon). With trial and error, I settled on threshold of 5.","66aeddd4":"Features like OverallQual, Neighborhood, GrLivArea and Year built rank high in our models, as one would expect. <br><br> What excites me is that some of the newly engineered features rank high in both lists.","d5756eab":"A general overview of the data we need to work with. \nA few lines of code going from high level overview to lower\/more detailed levels.","074b576c":"Running the code below so that IPython shows the entire result of the code I run. \nThis becomes helpful to me while visualizing\/analyzing high number of plots in the same output.","a76ca639":"# House Prices: Advanced Regression Techniques\n\n\nThe [Ames Housing](http:\/\/jse.amstat.org\/v19n3\/decock.pdf) dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. <br><br>\n\nI challenged myself not to copy\/peek at any piece of code from other data science notebooks for this competition (although I googled a heckload of questions, and read many Kaggle discussion threads on this competition). I ended up spending a lot hours trying to figure things out on my own. On the plus side, I ended up learning a whole lot more than I originally thought, which has been very beneficial.\n\nI hope that you find this notebook useful, if so please upvote :)"}}