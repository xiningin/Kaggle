{"cell_type":{"79fba831":"code","de572dfd":"code","f3078d83":"code","20e2c55a":"code","719e87ce":"code","62ae3f1f":"code","7e602b4b":"code","8581e486":"code","fcd2a0f7":"code","f23f84bf":"code","7fd89bc2":"code","5d8f75e0":"code","5bcf00ee":"code","441adb98":"code","c06b961d":"code","e2af4c7e":"code","1054976a":"code","66fb942f":"code","4c72d629":"code","13920a7a":"code","af2c0066":"code","1bdc99a2":"code","3808a9c7":"code","46f575a2":"code","9a5c9672":"code","cce0e954":"code","5c91e299":"code","d94539eb":"code","2ad58ea6":"code","a63489a0":"code","fdcedcd3":"code","87dbae20":"code","cbbde537":"code","af1f89e3":"code","cb5a5d0e":"code","337998c5":"code","eb073eb0":"code","f61f10ca":"code","18bd7385":"code","b9e8ed6c":"code","efa44c37":"code","4a6a7996":"code","f67426d6":"markdown","5fb00862":"markdown","69457b7b":"markdown","0594a287":"markdown","fead9c6d":"markdown","b334646a":"markdown","428b27fa":"markdown","8e671fee":"markdown","b972e9f4":"markdown","750e4e71":"markdown","c71c7873":"markdown","e631bff4":"markdown","436bc35f":"markdown","f0afce79":"markdown","174a2a0d":"markdown","3991cbe8":"markdown","5ac11601":"markdown","e14b84cd":"markdown","7941cb3e":"markdown","98cec2dc":"markdown","8055ddc0":"markdown","adcbe795":"markdown","1212afe2":"markdown","85eda12a":"markdown","c5ba710d":"markdown","7c1da1da":"markdown","3d1647bc":"markdown","6f4883a1":"markdown"},"source":{"79fba831":"#import your libraries\nimport pandas as pd\nimport sklearn as sk\nimport numpy as np\nimport scipy\nfrom scipy import stats\nimport plotly.express as px\nimport sqlite3\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score","de572dfd":"\n#Reading the training datafile and saving it as a dataframe\nTitanic_Train_table = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n#Reading the test datafile and saving it as a dataframe\nTitanic_Test_table = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","f3078d83":"class Load:\n    def __init__(self, df):\n        \"\"\"Initialising the constructor to save data in the different attributes\"\"\"\n        self.data = df\n        self._calculate(df)\n\n    def _calculate(self, df):\n        \"\"\"calls internal functions to show headers, descriptive stats and nulls\"\"\"\n        self._heading(df)\n        self._stats(df)\n        self._nulls(df)\n\n    def _heading(self, df):\n        \"\"\"Using pandas library to get a glimpse of the dataset\"\"\"\n        self.lines = df.head()\n        self.inf = df.info()\n        self.matrix = df.shape\n        self.cols = df.columns\n    \n    def _stats(self, df):\n        \"\"\"Displays all the descriptive stats of the dataset\"\"\"\n        self.description=df.describe()\n        self.relation=df.corr()\n\n        \n    def _nulls(self, df):\n        \"\"\"Inspects the whole dataset and returns a list of columns and the respective number of Null values\"\"\"\n        self.missing = df.isnull().sum()\n        \"\"\"Inspects the whole data frame and replaces any blanks with NaNs. The NaNs can then be preproessed on the basis of column.\"\"\"\n        self.cl = df.replace(r'^\\s*$', np.nan, regex=True)","20e2c55a":"#Instantiating an object for the training dataset from the Load class\ntrain = Load(Titanic_Train_table)\n\n# Calling the functions of the Load Class\ntrain._heading(Titanic_Train_table)\ntrain._nulls(Titanic_Train_table)\ntrain._stats(Titanic_Train_table)","719e87ce":"# Having look at the data stored in the attributes and a Glimpse of the data\ntrain.lines","62ae3f1f":"train.description  \n#The description shows a high number of blanks\/NaNs for Age\"\"\"","7e602b4b":"train.missing       \n\n#There are no nulls, apart from Age, Cabin and Embarked\"\"\"","8581e486":"train.cl            \n#Returns a data frame with all the blank cells replaced with NaNs.","fcd2a0f7":"#Storing the result of the cleaned data in a new dataframe\ntrain_cleaned = train.cl\n\n#Converting the dtypes of the values in dataframe to numeric\ntrain_cleaned[[\"Age\", \"Fare\",\"Survived\",\"SibSp\",\"Parch\",\"Pclass\"]] = train_cleaned[[\"Age\", \"Fare\",\"Survived\",\"SibSp\",\"Parch\",\"Pclass\"]].apply(pd.to_numeric)\n\n#Performing quality checks: if there are values in age column above 120 (Approximate upper limit), convert them to NaN.\ntrain_cleaned.loc[train_cleaned.Age > 120, 'Age'] = np.nan","f23f84bf":"#Check for the most frequently occuring value in a particular column of a dataframe. In Particular Embarked to replace NaNs.\ntrain_cleaned['Embarked'].value_counts()\n#Most Common value is S: Southanmpton\n","7fd89bc2":"#Replacing the NaNs in Age column by the median Age Value\ntrain_cleaned['Age'].fillna(train_cleaned['Age'].median(), inplace=True)\n\n\n#Replacing the NaNs in Embarked column by the most common value, i.e. S\ncommonvalue_embarked=\"S\"\ntrain_cleaned[\"Embarked\"].fillna(commonvalue_embarked, inplace = True) \n","5d8f75e0":"#Quality Check: Check if all the NANs have been replaced and only Cabin Column left. \ntrain_cleaned.isnull().sum()","5bcf00ee":"class Visuals:\n    def __init__(self, df):\n        self.data = df\n        self._calculate(df)\n\n    def _calculate(self, df):\n        \"\"\"calls internal functions to show basic pandas visualisations\"\"\"\n        self._visualisations(df)\n\n    def _visualisations(self, df):\n        \"\"\"Display the outliers and the distribution of the data\"\"\"\n        self.descriptions = df.boxplot(vert = False)\n        self.histogram = df.hist(color='k')","441adb98":"#Instantiating an object of the Visualisation class and inspecting the outliers and data distribution\nv_train=Visuals(train_cleaned)","c06b961d":"#Visualisations using Seaborn\n\n\n#lm plots using the Logistic model predict the passenger survival\nfigure1 = sns.lmplot(x=\"Age\", y=\"Survived\", logistic=True, col=\"Sex\",\n                         data=train_cleaned, aspect=1, x_jitter=.1, palette=\"Set1\")\n    \nfigure2 = sns.lmplot(x=\"Fare\", y=\"Survived\", logistic=True, col=\"Sex\",\n                         data=train_cleaned, aspect=1, x_jitter=.1, palette=\"Set1\")\n\n\n\n#Jointplot to understand the Pearson Corelation coefficient between the variables\n     \nfigure3 = sns.jointplot(\"Age\", \"Fare\", data=train_cleaned, color='b', kind='reg')\n    ","e2af4c7e":"#Using Heatmap to visualise the Pearson corelation coefficient between the different , especially useful to understand \n#impact on the Survived column, due to other independent variables\n\n\nfig, ax = plt.subplots(figsize=(14,14))         # Sample figsize in inches\nsns.heatmap(train_cleaned.corr(), annot=True, linewidths=.5, ax=ax)","1054976a":"# Detailed histogram of the training data using plotly\n\nfigure5 = px.histogram(train_cleaned, x=\"Age\", y=\"Fare\", color=\"Survived\",\n                   marginal=\"box\", # or violin, rug\n                  hover_data=train_cleaned.columns)\n                     \nfigure5.show()\n\n\n\n#ViolinPlots to better understand distiribution of Fare data as a function of Sex and Survival\n\nfigure6 = px.violin(train_cleaned, y=\"Fare\", x=\"Sex\", color=\"Survived\", box=True, points=\"all\",\n          hover_data=train_cleaned.columns)\nfigure6.show()","66fb942f":"# scatter plots using Plotly to visualise the survival for each Parch leval as a function of Age and Fare\n\nfigure7 = px.scatter(train_cleaned, x=\"Fare\", y=\"Age\", color='Survived',facet_col=\"Parch\")\nfigure7.show()\n\n\n# scatter plots using Plotly to visualise the survival for each Parch leval as a function of Age and Fare\n\nfigure8 = px.scatter(train_cleaned, x='Fare', y='Age', color='Survived',\n                facet_col='SibSp')\nfigure8.show()\n","4c72d629":"# Scatter Plots Using Plotly to understand the survival rate for different classes and as a function of Age and Fare\n\nfigure9 = px.scatter(train_cleaned, x='Fare', y='Age', color='Survived',\n                facet_col='Pclass')\nfigure9.show()\n\n\n# Scatter Plots Using Plotly to understand the survival rate for different Boarding stations and as a function of Age and Fare\nfigure10 = px.scatter(train_cleaned, x='Fare', y='Age', color='Survived',\n                facet_col='Embarked')\nfigure10.show()","13920a7a":"class Engineering:\n    def __init__(self, df, var1, var2):\n        \"\"\"Initialising the constructor to save the data\"\"\"\n        self.data = df\n        self.variable1 = var1\n        self.variable2 = var2\n        self._calculate(df, var1, var2)\n\n\n    def _calculate(self, df, var1, var2):\n        \"\"\"Calculates all the internal functions defined, i.e. featureadded, featuredivision and featureSubtracted\"\"\"\n        self._featureadded(var1,var2)\n        self._featuredivision(var1,var2)\n        self._featuremultiplied(var1,var2)\n        self._featuresubtracted(var1,var2)\n        \n    \n    def _featuredivision(self,var1,var2):\n        \"\"\"calculates ratio of given two variables\"\"\"\n        self.dividedfeature=var1\/\/var2\n        \n    \n    def _featuremultiplied(self,var1,var2):\n        \"\"\"calculates product of given two variables\"\"\"\n        self.multipliedfeature=var1*var2\n        \n        \n    def _featureadded(self,var1,var2):\n        \"\"\"Calculates the addition of two variables\"\"\"\n        self.addedfeature=var1+var2\n        \n        \n    def _featuresubtracted(self,var1,var2):\n        \"\"\"calculates the subtraction of two variables\"\"\"\n        self.subtractedfeature=var1-var2","af2c0066":"#Instantiating two objects of the training data from Engineering class\neng_feature1 = Engineering(train_cleaned,train_cleaned['SibSp'], train_cleaned['Parch'])\neng_feature2 = Engineering(train_cleaned,train_cleaned['Age'], train_cleaned['Pclass'])\n\n\n#Using addition method from the Engineering class on the first object\neng_feature1._featureadded(train_cleaned['SibSp'], train_cleaned['Parch'])\n\n\n#Using multiplication method from the Engineering class on the second object\neng_feature2._featuremultiplied(train_cleaned['Age'], train_cleaned['Pclass'])\n\n\n#Saving the data of above two methods into new columns of the cleaned Train dataset. \ntrain_cleaned['Family'] = eng_feature1.addedfeature\ntrain_cleaned['AgeandClass'] = eng_feature2.multipliedfeature\n","1bdc99a2":"# Inspecting the training dataset after Feature Engineering\ntrain_cleaned","3808a9c7":"# Getting Dummy Variables for the categorical variables and inspecting the data\ntrain_cleaned = pd.get_dummies(train_cleaned, columns=['Sex','Embarked'])\ntrain_cleaned.head()\n","46f575a2":"# Segregating the cleaned training dataset into Features and Target variables\n\ntrain_target=train_cleaned.iloc[:,1]\ntrain_features=train_cleaned.iloc[:,lambda train_cleaned:[2,4,8,10,11,12,13,14,15,16]]\n\n#Inspecting the features dataset\ntrain_features","9a5c9672":"class Outliers:\n\n    def __init__(self,df):\n        self.data=df\n        self._calculate(df)\n    \n    def _calculate(self, df):\n        \"\"\"calls internal functions to calculate outliers using the zscore method and the IQR Method\"\"\"\n        self._zscore(df)\n        self._iqr(df)\n        self._showiqr(df)\n        \n    def _zscore(self, df):\n        self.z = np.abs(stats.zscore(df))\n        print(self.z)\n        \n    def _iqr(self, df):\n        self.Q1 = df.quantile(0.25)\n        self.Q3 = df.quantile(0.75)\n        self.IQR = self.Q3 - self.Q1\n        print(self.IQR)\n    \n    def _showiqr(self, df):\n        print(df < (self.Q1 - 1.5 * self.IQR)) |(df > (self.Q3 + 1.5 * self.IQR))\n        \n#https:\/\/towardsdatascience.com\/ways-to-detect-and-remove-the-outliers-404d16608dba","cce0e954":"#Instantiating an object for the test dataset from the Load class\ntest = Load(Titanic_Test_table)\n\n# Calling the functions of the Load Class and storing all the values\ntest._heading(Titanic_Test_table)\ntest._nulls(Titanic_Test_table)\ntest._stats(Titanic_Test_table)","5c91e299":"# Having look at the data stored in the attributes\ntest.description  \n#The description shows a high number of blanks for Age, Cabin and i value for Fare\"\"\"","d94539eb":"test.missing       \n#There are no nulls, but there is presence of blank cells\"\"\"","2ad58ea6":"test.cl            \n#Returned a data frame with all the blank cells replaced with NaNs.","a63489a0":"#Storing the result of the cleaned data in a new dataframe\ntest_cleaned = test.cl\n\n#Converting the dtypes of the values in dataframe to numeric\ntest_cleaned[[\"Age\", \"Fare\",\"SibSp\",\"Parch\",\"Pclass\"]] = test_cleaned[[\"Age\", \"Fare\",\"SibSp\",\"Parch\",\"Pclass\"]].apply(pd.to_numeric)\n\n#Performing quality checks: if there are values in age column above 120, convert them to NaN.\ntest_cleaned.loc[test_cleaned.Age > 120, 'Age'] = np.nan","fdcedcd3":"#Quality Check\ntest_cleaned.isnull().sum()","87dbae20":"#Replacing the NaNs in Age column by the median Age Value\"\"\"\n\ntest_cleaned['Age'].fillna(test_cleaned['Age'].median(), inplace=True)\n\n\n#Replacing the NaNs in Fare column by the median Fare Value\"\"\"\ntest_cleaned['Fare'].fillna(test_cleaned['Fare'].median(), inplace=True)","cbbde537":"#Instantiating two objects of test dataset from the Engineering class\neng_feature1 = Engineering(test_cleaned,test_cleaned['SibSp'],test_cleaned['Parch'])\neng_feature2 = Engineering(test_cleaned,test_cleaned['Age'],test_cleaned['Pclass'])\n\n#Using addition method from the Engineering class on the first object\neng_feature1._featureadded(test_cleaned['SibSp'],test_cleaned['Parch'])\n\n#Using multiplication method from the Engineering class on the second object\neng_feature2._featuremultiplied(test_cleaned['Age'],test_cleaned['Pclass'])\n\n#Saving the data of above two methods into new columns of the original dataset. \ntest_cleaned['Family'] = eng_feature1.addedfeature\ntest_cleaned['AgeandClass'] = eng_feature2.multipliedfeature","af1f89e3":"# Getting Dummy Variables for the categorical variables in the test dataset\n\ntest_cleaned = pd.get_dummies(test_cleaned, columns=['Sex','Embarked'])\ntest_cleaned.head()","cb5a5d0e":"# Removing the un-important columns from the test dataset and making it exactly similar to the train dataset.\n\ntest_features=test_cleaned.iloc[:,lambda test_cleaned:[1,3,7,9,10,11,12,13,14,15]]\n\n# Inspecting the cleaned test dataset features\ntest_features","337998c5":"class Modelling:\n    def __init__(self,var1,var2):\n        \"\"\"Initialising the constructor method to save the data\"\"\"\n        self.X=var1\n        self.Y=var2\n        self._calculate(var1, var2)\n        \n    def _calculate(self, var1, var2):\n        \"\"\"Calculates all the internal functions that contain the different models\"\"\"\n        self._baselinemodel(var1,var2)\n        self._forest(var1,var2)\n        self._boosting(var1,var2)\n        self._neighbours(var1,var2)\n        self._vectors(var1,var2)\n        \n    def _baselinemodel(self,var1,var2):\n        \"\"\"We start by building a simple logistic regression and then improve upon the results\"\"\"\n        self.model=LogisticRegression(random_state=42)\n        self.model_1=self.model.fit(var1,var2)\n        \n    def _forest(self,var1,var2):\n        \"\"\"Use the ensemble methods to improve on the baseline model\"\"\"\n        self.model=RandomForestClassifier(n_estimators=25, max_depth=7, random_state=42)\n        self.model_2=self.model.fit(var1,var2)\n        \n    def _boosting(self,var1,var2):\n        \"\"\"Implemetation of Boosting algorithm to understand differences from the ensemble ones\"\"\"\n        self.model=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.05, loss='deviance', max_depth=7,\n              max_features=1.0, max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=9, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=50,\n              presort='auto', random_state=42, subsample=1.0, verbose=0,\n              warm_start=False)\n        self.model_3=self.model.fit(var1,var2)\n        \n    def _neighbours(self,var1,var2):\n        \"\"\"A simple model based on nearest neighbours methodology\"\"\"\n        self.model=KNeighborsClassifier(n_neighbors = 7)\n        self.model_4=self.model.fit(var1,var2)\n        \n    def _vectors(self,var1,var2):\n        \"\"\"implementing support vector machines\"\"\"\n        self.model=SVC(kernel = 'linear', C = 1)\n        self.model_5=self.model.fit(var1,var2)","eb073eb0":"#Instantiating an object of the Modelling class\nmod=Modelling(train_features, train_target)\n\n# Using the calculate function to Implement all the models\nmod._calculate(train_features, train_target)\n\n\n#Saving the results of all the models in different variables\nlr=mod.model_1\nrf=mod.model_2\ngb=mod.model_3\nkn=mod.model_4\nsv=mod.model_5","f61f10ca":"class Eval:\n    def __init__(self, v1, v2, v3, v4):\n        \"\"\"Initialising the constructor and saving all the variable data\"\"\"\n        self.X_test=v1\n        self.X_train=v2\n        self.model=v3\n        self.Y_train=v4\n        self._calculate(v1, v2, v3, v4)\n        \n    def _calculate(self, v1, v2, v3, v4):\n        self._pred(v1,v3)\n        self._cv(v3, v2, v4)\n        \n    def _pred(self, v1, v3):\n        \"\"\"Predicting the target variable\"\"\"\n        self.predicted_value=v3.predict(v1)\n        \n    def _cv(self, v3, v2, v4):\n        \"\"\"Cross validation\"\"\"\n        self.scores=cross_val_score(v3, v2, v4, cv=4)\n","18bd7385":"#Instantiating objects for the five different models\neval1=Eval(test_features, train_features, lr, train_target)\neval2=Eval(test_features, train_features, rf, train_target)\neval3=Eval(test_features, train_features, gb, train_target)\neval4=Eval(test_features, train_features, kn, train_target)\neval5=Eval(test_features, train_features, sv, train_target)\n\n\n# Calculating the cross-validation scores as well as prediction values for all the models\neval1._calculate(test_features, train_features, lr, train_target)\neval2._calculate(test_features, train_features, rf, train_target)\neval3._calculate(test_features, train_features, gb, train_target)\neval4._calculate(test_features, train_features, kn, train_target)\neval5._calculate(test_features, train_features, sv, train_target)\n\n#Storing the cross validation scores of all the models in different variables\nscores_lr = eval1.scores\nscores_rf = eval2.scores\nscores_gb = eval3.scores\nscores_kn = eval4.scores\nscores_sv = eval5.scores\n\n\n# Creating a list of all the Cross validation scores and plotting them\ncv_scores=[scores_lr, scores_rf, scores_gb, scores_kn, scores_sv]\n\nfor x in cv_scores:\n    print(x.mean())\n    plt.plot(x)\n    \n\n","b9e8ed6c":"# Inspecting the feature importance from the random Forest Model (Prescriptive Statistics)\nrf.feature_importances_\n\n\n#Plottng the Feature Importances\nfeature_importances = pd.Series(rf.feature_importances_, index=train_features.columns)\nprint(feature_importances)\nfeature_importances.sort_values(inplace=True)\nfeature_importances.plot(kind='barh', figsize=(7,6))","efa44c37":"# Hyper parameter Tuning Using Grid Search\n# Define Parameters for Gradient Boosting Model as it shows the best cv score\n\nparam_grid = {\"max_depth\": [2,3,7],\n              \"max_features\" : [1.0,0.3,0.1],\n              \"min_samples_leaf\" : [3,5,9],\n              \"n_estimators\": [8,10,25,50],\n              \"learning_rate\": [0.05,0.1,0.02,0.2]}\n\n\n# Perform Grid Search CV\nfrom sklearn.model_selection import GridSearchCV\ngb_cv = GridSearchCV(gb, param_grid=param_grid, cv = 4, verbose=10, n_jobs=-1 ).fit(train_features, train_target)\n\n\n\n# Best hyperparmeter setting\ngb_cv.best_estimator_","4a6a7996":"# Selecting the best model results out of all the models, (Gradient Boosting)\n\nBest_prediction_result=pd.DataFrame(eval3.predicted_value)\nPassID=pd.DataFrame(test_cleaned['PassengerId'])\nSubmission_file=pd.concat([PassID, Best_prediction_result], axis = 1)\nSubmission_file.columns=['PassengerId', 'Survived']\n\n","f67426d6":"### 1: DEFINE\n<a id=\"definition\"><\/a>","5fb00862":"### 3: DEVELOP\n<a id=\"BaselineModel\"><\/a>","69457b7b":"*LAYING DOWN THE GROUNDWORK*\n***\n- What are we analyzing?\n\n**In the first step, we will be analyzing the distributions of the different variables of the dataset and write down the inferences. We will look for missing values and impute them.**\n\n- What our variables mean?\n\n**The different variables signify the characteristics of all the passengers that boarded the Titanic.**\n\n- Why are we analyzing this data set?\n\n**To develop a working classification model that can be deployed and scaled up. In addition, the predictions will help us estimate the accuracy of the baseline models.**","0594a287":"### 2: DISCOVER\n<a id=\"discover\"><\/a>","fead9c6d":"### Table of Contents\n***\n1 [DEFINE](#definition)\n\n1.1 [BUSINESS PROBLEM](#problem)\n\n2 [DISCOVER](#discover)\n\n2.1 [Loading](#loadthefile)\n\n2.2 [Exploring the data](#etl)\n\n2.3 [Visualisations using seaborn, plotly](#Visualise)\n\n2.4 [Feature Engineering](#Engineer)\n\n2.5 [Preprocessing dataset for model creation](#preprocess)\n\n2.5.1 [Creation of dummy variables](#dummies)\n\n2.5.2 [Dealing With Outliers (IQR, Zscore)](#outliers)\n\n3 [DEVELOP](#BaselineModel)\n\n3.1 [ModelDevelopment](#development)\n\n3.2 [ModelEvaluation](#evaluate)\n\n3.3 [ModelTuning](#tuning)\n\n4 [DEPLOY](#deploy)\n","b334646a":"#### 2.5.3 Implementing all the above changes on test dataset","428b27fa":"### 3.1 Model Development\n<a id=\"development\"><\/a>","8e671fee":"#### 2.5.2 Dealing with Outliers: IQR and Z-score method\n<a id=\"outliers\"><\/a>","b972e9f4":"### Summary and inferences of Model Development and Model Evaluation\n***\n\n*a) What kind of models and evaluation we have performd on the dataset till now?:*\n\n**We have built a baseline model (Logistic Regression), ensemble models like random Forest, gradient boosting. KNN and SVM.**\n\n\n*b) Inferences:*\n\n**Highest Accuracy using Cross Validation:**\n\n*Gradient Boosting Classifier outputs the highest mean accuracy of 0.83 for the Titanic Dataset, using a four fold cross validation technique.*\n\n\n**Feature Importances**\n\n*The Feature Importnaces calulcated for the Random Forest model shows Sex (Male) with the highest value and Embarked_Q with lowest values*\n\n*In the next section, we should work on tuning the Gradient Boosting Classifier and achive better accuracy rate with it.*\n\n","750e4e71":"### 1.1 BUSINESS PROBLEM\n<a id=\"problem\"><\/a>","c71c7873":"### 2.4 Feature Engineering\n<a id=\"Engineer\"><\/a>","e631bff4":"The only column now left with NaNs is Cabin, around 687. The number is large enough in a dataset of 891 rows. Replacing the NaNs with a certain value will bias the data. It is better to omit this column in later stages of data preparation","436bc35f":"### 2.1 Loading the data\n<a id=\"loadthefile\"><\/a>","f0afce79":"***Hypothesis***\n\n***\n\n**1. To predict the Survival of Titanic Passengers, we will start by implementing a baseline Logistic Regression model. Without any complexities, it will help to judge how to tune further higher order models.**\n\n**2. In the next steps, we make use of ensemble, boosting and neighbours algorithms to tune the predictions.**","174a2a0d":"**Classify Passengers of the test set into survived or not survived in this classic ML dataset.**\n***\n","3991cbe8":"*The train dataset includes three 12 categories or variables.*\n\nThe columns in this dataset are:\n\n1. Passenger: IdUnique ID of the passenger\n\n2. Survived: Survived (1) or died (0)\n\n3. Pclass: Passenger's class (1st, 2nd, or 3rd)\n\n4. Name: Passenger's name\n\n5. Sex: Passenger's sex\n\n6. Age: Passenger's age\n\n7. SibSp: Number of siblings\/spouses aboard the Titanic\n\n8. Parch: Number of parents\/children aboard the Titanic\n\n9. Ticket: Ticket number\n\n10. Fare: Fare paid for ticket\n\n11. Cabin: Cabin number\n\n12. Embarked: Where the passenger got on the ship (C - Cherbourg, S - Southampton, Q = Queenstown)","5ac11601":"### 2.3 Visualisations using seaborn, plotly\n<a id=\"Visualise\"><\/a>","e14b84cd":"#### 2.5.1 Creation of dummy variables\n<a id=\"dummies\"><\/a>","7941cb3e":"**EXPLAINING OUR FINDINGS FOR THE FIRST PART OF THE ANALYSIS.**\n***\n*a) What analyses we\u2019ve done on the data:*\n\n**Till now, we have created a Load Class that loads sqlite database file. Inspected the Blanks by querying the database and replaced them with NaNs using Regex.**\n\n*b) Why we did these analysis:*\n\n**It is important to understand the baselines of the dataset from the beginning. For instance, everytime we get a dataset to explore, it is necessary to perform a basic quality check assurance.**","98cec2dc":"### Summary of Model Tuning\n***\n\n*a) How did GridSearch CV work and how to incorporate the parametres back in the model?*\n\n****\n\n**1. With n_estimators=50 and a max_depth of 7, we can try and observe how does the accuracy of GB model change**\n\n","8055ddc0":"### Summary and inferences of Histogram and Categorical plots Using Plotly\n***\n\n*a) What kind of categorical plots and analyses we\u2019ve done on the data:*\n\n**We have used three different types of categorical plots to identify trends between Survived and the Independent Variables.**\n\n**1. Histogram: Between Age and Fare as a function of Survival rate.**\n\n**2. ViolinPlot: Between Fare and Sex as a function of Survival rate.**\n\n**3. ScatterPlots: Between Age and Fare as a function of Parch, SibSp, Embarked and Pclass.**\n\n*b) Inferences:*\n\n**Histogram and Violin  Plot:**\n\n*The highest count of people Survived are in the Age Group 28-30 and Females. Indicates that a strong preference was given to Young Females, when life boats were being taken out*\n\n\n**Scatterplots**\n\n*Lower the number of Parch (Parents and Children) and SibSp (Siblings), higher are chances of survival*\n\n*Pclass=1 had the highest number of Passenger Survival Rate.*\n\n*Although, intuitively the boarding station should not have an impact on the survival chances, it is observed that highest number of passengers survived boarded from Southampton.*\n\n","adcbe795":"### 3.3 ModelTuning\n<a id=\"tuning\"><\/a>","1212afe2":"### Summary and inference of lmplots, JointPlot and Heatmap\n***\n\n*a) What analyses we\u2019ve done on the data:*\n\n**We have used three different plots to bring out the relations between the different variables of the training dataset.**\n\n   **1. Lmplots: Higher Survival rates obsrved for Females as a function of Age and fare. Older Males are observed to show even lower survival rates.**\n   \n   **2. JointPlot: The correlation betwee Fare and Age is quite low and doesnt provide any clear evidence of Ticket Fare being dependent on Age.**\n   \n   **3. Heatmap: Highest Corelation of 0.26 for (Survived) observed with Age, i.e. higher the Age, greater the chance of Survival.**\n\n*b) Key Inference:*\n\n   **There are a few outliers in Fare column, followed by the Age Column, as seen from the Boxplots made using Pandas Library. If the models demonstrate lower accuracy, it might be worth coming back and addressing the Outliers.**\n   ","85eda12a":"### 2.2 Exploring the Data\n<a id=\"etl\"><\/a>","c5ba710d":"### 3.2 Model Evaluation\n<a id=\"evaluate\"><\/a>","7c1da1da":"### 4: DEPLOY\n<a id=\"deploy\"><\/a>","3d1647bc":"### 2.5 Preprocessing dataset for model creation\n<a id=\"preprocess\"><\/a>","6f4883a1":"*Classifying Titanic Passengers into two categories (Survived or not survived), by creatig a classification model that learns from the historical Shipwreck data. In addition, we will identify the correlation of the survival index with the different variables like Age, fare etc and deliver actionable insights from the existing dataset.*\n***\n\n**The Kernel can be found at https:\/\/github.com\/ivm25\/PortFolio-Titanic-Passenger-Prediction**\n***\n\n*I look forward to feedback as I try to improve my analysis, improve the kernel and also contribute to the Data Science community*\n***\n\n*STATING THE ASSUMPTIONS:*\n    \n\n*The data is a digital collection of the most discussed shipwreck, i.e. Titanic and it is assumed that the entries are correct for analysis purposes.*\n"}}