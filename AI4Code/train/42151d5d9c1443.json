{"cell_type":{"52b91d50":"code","410772b3":"code","41b258c0":"code","8157f034":"code","8bfc69db":"code","35f3eea1":"code","c12a1610":"code","1985fa63":"code","788bf898":"code","08df84c9":"code","87c08022":"code","78a15a97":"code","f352ab65":"code","a31111da":"code","2e77c2f1":"code","143eb0c5":"code","a163f732":"code","12ccf847":"code","8ca8f7df":"code","a91984f5":"code","def8141c":"code","c9bee41d":"code","4269cfee":"code","4914cd5b":"code","d7629c61":"code","f620c8a5":"code","aa11d1fe":"code","c4dc8e92":"code","e299f20a":"code","2d6ea533":"markdown","136d5aca":"markdown","77d5eeb9":"markdown","6b808318":"markdown","87f78b00":"markdown","2cae0354":"markdown","dba2798f":"markdown","1c710eba":"markdown","96210635":"markdown","973630bb":"markdown","79188462":"markdown","cb712ddd":"markdown","3666ddb3":"markdown","e74555fd":"markdown"},"source":{"52b91d50":"#Importing the necessary packages\n\nimport collections\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\n\nimport scipy\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, plot_roc_curve, accuracy_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","410772b3":"data = pd.read_csv(\"..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")\ndata.head()","41b258c0":"data.describe()","8157f034":"data.isnull().sum()","8bfc69db":"corr = data.corr()\ncorr.style.background_gradient(cmap='coolwarm')","35f3eea1":"df = data.drop('Serial No.', axis=1)\ndf.rename(columns={'Chance of Admit ': 'Chance of Admit'}, inplace=True)\ndf.columns","c12a1610":"df.head()","1985fa63":"plt.hist(df['Chance of Admit'])\nplt.xlabel(\"Chance of Admit\")\nplt.ylabel(\"Count\")\nplt.show()","788bf898":"sns.pairplot(df)","08df84c9":"sns.kdeplot(df['Chance of Admit'], df['GRE Score'], cmap='Blues', shade=True, shade_lowest=False)","87c08022":"sns.kdeplot(df['Chance of Admit'], df['University Rating'], cmap='Blues', shade=True, shade_lowest=False)","78a15a97":"sns.scatterplot(df['University Rating'], df['GRE Score'])","f352ab65":"for i in np.arange(0, 1, 0.18):\n    #print(i)\n    print(i, df[df['Chance of Admit'] > i].shape[0]\/len(df))","a31111da":"df['Label'] = np.where(df['Chance of Admit']>0.72, 1, 0)","2e77c2f1":"print(df.Label.value_counts())\ndf.sample(10)","143eb0c5":"df.shape","a163f732":"#Checking feature importance with DTree classifier\n# define the model\nmodel = DecisionTreeClassifier()\n\nX = df.drop(columns = ['Chance of Admit', 'Label'])\ny = df['Label']\n\n# fit the model\nmodel.fit(X, y)\n\nfeature_importance = model.feature_importances_\n\nfor i,v in enumerate(feature_importance):\n    print('Feature: %0d,  Score: %.5f' % (i,v))\n    \nfeature_importance = pd.Series(feature_importance, index=X.columns)\nfeature_importance.plot(kind='barh')","12ccf847":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","8ca8f7df":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","a91984f5":"def plot_roc(false_positive_rate, true_positive_rate, roc_auc):\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],linestyle='--')\n    plt.axis('tight')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","def8141c":"parameters = [\n    {\n        'penalty' : ['l1', 'l2', 'elasticnet'],\n        'C' : [0.1, 0.4, 0.5],\n        'random_state' : [0]\n    }\n]\nmodel_logit = LogisticRegression()\ngscv = GridSearchCV(model_logit, parameters, scoring='accuracy')\ngscv.fit(X_train, y_train)\n\nprint('Best parameters set:')\nprint(gscv.best_params_)\nprint()\n\nprint(\"*\"*50)\nprint(\"Train classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_train), y_train))\nprint(confusion_matrix(gscv.predict(X_train), y_train))\n\nprint()\nprint(\"*\"*50)\nprint(\"Test classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_test), y_test))\nprint(confusion_matrix(gscv.predict(X_test), y_test))\n\ncvs = cross_val_score(estimator=model_logit, X=X_train, y=y_train, cv=12)\nprint()\nprint(\"*\"*50)\nprint(cvs.mean())\nprint(cvs.std())","c9bee41d":"lr_model = LogisticRegression(C= 0.1, penalty= 'l2', random_state= 0)\nlr_model.fit(X_train, y_train)\n\ny_pred = lr_model.predict(X_test)\ny_pred_proba = lr_model.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nplot_roc(false_positive_rate, true_positive_rate, roc_auc)\n\nprint('Accurancy Score :',accuracy_score(y_test, y_pred))\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n#print(\"Accurecy is : \", (cm[0,0]+cm[1,1])\/cm.sum())","4269cfee":"parameters = [\n    {\n        'criterion' : ['gini', 'entropy'],\n        'max_depth' : [3, 4, 5],\n        'min_samples_split' : [10, 20, 5],\n        'random_state': [0],\n        \n    }\n]\nmodel_dt = DecisionTreeClassifier()\ngscv = GridSearchCV(model_dt, parameters, scoring='accuracy')\ngscv.fit(X_train, y_train)\n\nprint('Best parameters set:')\nprint(gscv.best_params_)\nprint()\n\nprint(\"*\"*50)\nprint(\"Train classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_train), y_train))\nprint(confusion_matrix(gscv.predict(X_train), y_train))\n\nprint()\nprint(\"*\"*50)\nprint(\"Test classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_test), y_test))\nprint(confusion_matrix(gscv.predict(X_test), y_test))\n\ncvs = cross_val_score(estimator=model_logit, X=X_train, y=y_train, cv=12)\nprint()\nprint(\"*\"*50)\nprint(cvs.mean())\nprint(cvs.std())","4914cd5b":"dt_model = DecisionTreeClassifier(criterion= 'entropy', max_depth= 3, min_samples_split= 10, random_state= 0)\ndt_model.fit(X_train, y_train)\n\ny_pred = dt_model.predict(X_test)\ny_pred_proba = dt_model.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nplot_roc(false_positive_rate, true_positive_rate, roc_auc)\n\nprint('Accurancy Score :',accuracy_score(y_test, y_pred))\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n#print(\"Accurecy is : \", (cm[0,0]+cm[1,1])\/cm.sum())","d7629c61":"parameters = [\n    {\n        'n_estimators': np.arange(10, 40, 5),\n        'criterion' : ['gini', 'entropy'],\n        'max_depth' : [3, 4, 5],\n        'min_samples_split' : [10, 20, 5],\n        'random_state': [0],\n        \n    }\n]\nmodel_rf = RandomForestClassifier()\ngscv = GridSearchCV(model_rf, parameters, scoring='accuracy')\ngscv.fit(X_train, y_train)\n\nprint('Best parameters set:')\nprint(gscv.best_params_)\nprint()\n\nprint(\"*\"*50)\nprint(\"Train classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_train), y_train))\nprint(confusion_matrix(gscv.predict(X_train), y_train))\n\nprint()\nprint(\"*\"*50)\nprint(\"Test classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_test), y_test))\nprint(confusion_matrix(gscv.predict(X_test), y_test))\n\ncvs = cross_val_score(estimator=model_logit, X=X_train, y=y_train, cv=12)\nprint()\nprint(\"*\"*50)\nprint(cvs.mean())\nprint(cvs.std())","f620c8a5":"rf_model = RandomForestClassifier(criterion= 'entropy', max_depth= 4, min_samples_split= 10,n_estimators= 10, random_state= 0)\nrf_model.fit(X_train, y_train)\n\ny_pred = rf_model.predict(X_test)\ny_pred_proba = rf_model.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nplot_roc(false_positive_rate, true_positive_rate, roc_auc)\n\nprint('Accurancy Score :',accuracy_score(y_test, y_pred))\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n#print(\"Accurecy is : \", (cm[0,0]+cm[1,1])\/cm.sum())","aa11d1fe":"parameters = [\n    {\n        'learning_rate': [0.01, 0.02, 0.002],\n        'n_estimators': np.arange(10, 40, 5),\n        'max_depth' : [3, 4, 5],\n        'min_samples_split' : [10, 20, 5],\n        'random_state': [0],\n        \n    }\n]\nmodel_gbc = GradientBoostingClassifier()\ngscv = GridSearchCV(model_gbc, parameters, scoring='accuracy')\ngscv.fit(X_train, y_train)\n\nprint('Best parameters set:')\nprint(gscv.best_params_)\nprint()\n\nprint(\"*\"*50)\nprint(\"Train classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_train), y_train))\nprint(confusion_matrix(gscv.predict(X_train), y_train))\n\nprint()\nprint(\"*\"*50)\nprint(\"Test classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_test), y_test))\nprint(confusion_matrix(gscv.predict(X_test), y_test))\n\ncvs = cross_val_score(estimator=model_logit, X=X_train, y=y_train, cv=12)\nprint()\nprint(\"*\"*50)\nprint(cvs.mean())\nprint(cvs.std())","c4dc8e92":"gbc_model = GradientBoostingClassifier(learning_rate= 0.01, max_depth= 3, min_samples_split= 20,n_estimators= 20, random_state= 0)\ngbc_model.fit(X_train, y_train)\n\ny_pred = gbc_model.predict(X_test)\ny_pred_proba = gbc_model.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nplot_roc(false_positive_rate, true_positive_rate, roc_auc)\n\nprint('Accurancy Score :',accuracy_score(y_test, y_pred))\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n#print(\"Accurecy is : \", (cm[0,0]+cm[1,1])\/cm.sum())","e299f20a":"y_proba=rf_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_proba, y_test))\n","2d6ea533":"## Checking variable importance","136d5aca":"For ease of working with the classifier, it will be nice to have a 50\/50 split on the data.\n\nFor class balance, let us assume that the bottom 50% of the observations fall in class 0 (no or less chance of admit), and the top 50% of the observations fall in class 1.\n\nBinning the Chance of Admit variable and seeing where the 50% lies\n\n","77d5eeb9":"# Importing and exploring the datasets","6b808318":"## Model 2: Decision tree","87f78b00":"## Model 1: Logistic regression","2cae0354":"Let us now check what variables are important for out labels. For checking variable importance, we will use a basic decision tree classifier and then check what is the variable importance within the classifier","dba2798f":"# Exploratory data analysis","1c710eba":"# Introduction:\n\nI have structured the notebook into the following tasks:\n\n1. Importing and exploring the dataset\n2. EDA on the dataset\n3. Defining classification labels\n4. Modelling\n5. Conclusion\n6. References","96210635":"## Model 3: Random forest","973630bb":"# Modeling","79188462":"The main EDA that I performed on this dataset is to see how the variables are distributed, to check if the variables are distributed normally. For that the pair plot is used to check the histogram of the variables as well as for the scatter plot to see how the variables are corelated to each other.","cb712ddd":"# Defining the class labels for classification","3666ddb3":"## Model 4: Gradient boost classifier","e74555fd":"We now have 252 observations in class 0 and 248 observations in class 1, which is good enough balance that we are expecting"}}