{"cell_type":{"5cb190de":"code","e3785813":"code","9de78ca7":"code","28182f80":"code","44212acd":"code","b5b3e399":"code","7ed9200a":"code","c1e09e69":"code","6a7be31f":"code","6fc06d91":"code","cf2d68a6":"code","dc448394":"code","30ce1b93":"code","679d7503":"code","f0d1d287":"code","6184326b":"code","6d14939a":"code","ed4b26ef":"code","e28b8bea":"code","46006722":"code","53256e0f":"code","f6d6ef77":"code","3bb02933":"code","d179e262":"code","1a773227":"code","53705b58":"code","29cc98e0":"code","fe01116a":"code","2e39cc8f":"code","ca680cd6":"markdown","c743117c":"markdown","19742960":"markdown","9cfd794f":"markdown","e55694aa":"markdown","c47b674d":"markdown","1a3e7aaf":"markdown","a0c65aa2":"markdown","0fd72ea7":"markdown","7d342256":"markdown","ed4b3ff2":"markdown","23968b5a":"markdown","659b463d":"markdown","fcf58710":"markdown","2f4b2db3":"markdown","dbbcfd69":"markdown","fd5e08d0":"markdown","d74b7da7":"markdown","0f94a30a":"markdown","99399c49":"markdown"},"source":{"5cb190de":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('dark')\nfrom datetime import datetime, timedelta\n\nimport keras\nimport tensorflow\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import TimeSeriesSplit\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# Any results you write to the current directory are saved as output.\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","e3785813":"#load the data\ndata = pd.read_csv('\/kaggle\/input\/energy-consumption-generation-prices-and-weather\/energy_dataset.csv',\n                   index_col=[0],\n                   parse_dates=True)\n\ndates = pd.date_range(start='2014-12-31T2300', end='2018-12-31T2200', freq='H')\ndata.index = pd.DatetimeIndex(dates).tz_localize('UTC').tz_convert('Europe\/Madrid')\ndf = data[['total load actual', 'total load forecast']]\n\nnull_vals = df.isnull().sum()\nprint('Null values in the target column {}'.format(null_vals))\ndf.head()","9de78ca7":"df.index.min(), df.index.max()","28182f80":"df = df.interpolate(method='linear', axis=0)\ndf.isnull().sum()","44212acd":"def transform_to_hour_cols(series):\n    df = pd.DataFrame()\n\n    start = series.index.min()\n    end = series.index.max()\n    \n    df['year'] = series.index.year\n    df['month'] = series.index.month\n    df['day'] = series.index.day\n    df['hours'] = series.index.hour\n    df['loads'] = series.values\n    \n    df = df.set_index(['year', 'month', 'day', 'hours'], append=True).unstack()\n    df = df.groupby(['year', 'month', 'day']).sum()\n    \n    df.reset_index(inplace=True)\n    df.drop(['year', 'month', 'day'], axis=1, inplace=True)\n    \n    date_list = pd.date_range(start=start, end=end, freq='D').strftime('%Y-%m-%d')\n    \n    df.index = pd.DatetimeIndex(date_list, name='date')\n    \n    return df\n\nday_energy = transform_to_hour_cols(df['total load actual'])\nday_energy.head()","b5b3e399":"#make life easier\nday_energy.columns = [\"h\"+ str(x) for x in range(0, 24)]\n\n#get dates for dailight savings times\nidx = day_energy.loc[day_energy['h2']==0, 'h2'].index\n\n#set values zero values to NaN\nday_energy.loc[day_energy['h2']==0, 'h2'] = np.NaN\n\nday_energy.loc[idx, 'h2']","7ed9200a":"day_energy = day_energy.interpolate(method='linear', axis=0)\nday_energy.loc[idx, 'h2']","c1e09e69":"#isolate the original series of demand data\nenergy_demand_univar = df['total load actual']\n\nfig, axs = plt.subplots(1,2, figsize=(20,8))\n\n#we will plot the last 30 and 90 days\nlags = [30*24, 90*24]\n\nfor ax, lag in zip(axs.flatten(), lags):\n    plot_acf(energy_demand_univar, ax=ax, lags=lag)\nplt.plot()","6a7be31f":"plots = len(day_energy.columns)\nfig, axs = plt.subplots(int(plots\/2), 2, figsize=(15, 2*plots))\n\nfor hour, ax in zip(day_energy.columns, axs.flatten()):\n        plot_acf(day_energy.loc[:,hour], ax=ax, lags=60)\n        ax.set_title('Autocorrelation hour ' + str(hour))\nplt.plot()\n    ","6fc06d91":"plots = len(day_energy.columns)\nfig, axs = plt.subplots(int(plots\/2), 2, figsize=(15, 2*plots))\n\nfor hour, ax in zip(day_energy.columns, axs.flatten()):\n        plot_pacf(day_energy.loc[:,hour], ax=ax, lags=60)\n        ax.set_title('Partial Autocorrelation hour ' + str(hour))\nplt.plot()","cf2d68a6":"def normalize_df(data):\n    \n    #normalize the dataset for working with the lstm nn\n    scaler = MinMaxScaler().fit(data.values)\n    data_normd = scaler.transform(data.values)\n    \n    #return as dataframe\n    data = pd.DataFrame(data_normd, index=data.index, columns=data.columns)\n    \n    return data, scaler","dc448394":"#normalize the energy dataframe\nday_energy_normed, scaler = normalize_df(day_energy)","30ce1b93":"def split_sequences(sequences, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False):\n    \"\"\"\n    Modified based on content by Jason Brownlee from MachineLearningMastery.\n    \n    n_step - is the number of consecutive past steps from the target day to construct\n    long_lag_step - is the size of the trailing lag beyond the n_steps. Ie. if n_steps is 3, and long_lag_step is 7 will return day -1, -2, -3, -7, -14 etc to max_step\n    idx - is the starting index for the first slice\n    multivari - true if the input data has multiple variable structure. false for univariate\n    \"\"\"\n    \n    #if not adding extra lag features adjust max_step and n_steps to aling\n    if not extra_lag:\n        max_step=n_steps\n        n_steps+=1\n        \n    \n    X, y = list(), list()\n    for i in range(len(sequences)):\n        \n        # find the end of this pattern\n        #end_ix = i + n_steps\n        end_ix = i + max_step\n        \n        #create a list with the indexes we want to include in each sample\n        slices = [x for x in range(end_ix-1,end_ix-n_steps, -1)] + [y for y in range(end_ix-n_steps, i, -long_lag_step)]\n        \n        #reverse the slice indexes\n        slices = list(reversed(slices))\n        \n        # check if we are beyond the dataset\n        if end_ix > len(sequences)-1:\n            break\n\n\n        # gather input and output parts of the pattern\n        seq_x = sequences[slices, :]\n        seq_y = sequences[end_ix, :]\n\n        X.append(seq_x)\n        y.append(seq_y)\n        \n    X = np.array(X)\n    y = np.array(y)\n    \n    if multivar:\n        #unstack the 3rd dimension and select the first element(energy load)\n        y = y[:,idx]\n    \n    return X, y","679d7503":"#create the supervised learning problem\nn_steps = 21\n\nX, Y = split_sequences(day_energy_normed.values, n_steps, extra_lag=True, long_lag_step=7, max_step=60, idx=0, multivar=False)\nprint(X.shape, Y.shape)\nX[:5], Y[:5]","f0d1d287":"###define an LSTM model\n#takes in parallel inputs and outputs an equal number of parallel outputs\ndef lstm_parallel_out(n_lags, n_hours, cells=50, learning_rate=5e-3):\n    \n    #define the model\n    model = keras.models.Sequential()\n    model.add(keras.layers.LSTM(cells, activation='relu', return_sequences=True, input_shape=(n_lags, n_hours)))\n    model.add(keras.layers.LSTM(int(cells\/2), activation='relu'))\n    model.add(keras.layers.Dropout(0.3))\n    model.add(keras.layers.Dense(n_hours))\n    \n    #define the learning rate\n    optimizer = keras.optimizers.Adam(lr=learning_rate)\n    \n    #compile model\n    model.compile(optimizer=optimizer, loss='mae')\n    \n    return model","6184326b":"def crossval_testbench(X, y, n_crossvals, epochs=5, verbose=0):\n    \n    n_hours = X.shape[-1]\n    n_features = X.shape[1]\n    \n    tscv = TimeSeriesSplit(n_splits=n_crossvals)\n\n    #initalize lists to capture the output\n    predictions = []\n    actuals = []\n\n\n    #run the LSTM model on each of the time series splits\n    for train, test in tscv.split(X, y):\n        \n        #initalize the lstm model\n        lstm_base = lstm_parallel_out(n_features, n_hours, learning_rate=5e-3)\n        \n        #fit the model\n        lstm_base.fit(X[train], y[train], epochs=epochs, verbose=verbose, shuffle=False)\n        \n        #make predictions\n        predict = lstm_base.predict(X[test], verbose=verbose)\n\n\n        #inverse transform the predictions and actual values\n        prediction = scaler.inverse_transform(predict)\n        actual = scaler.inverse_transform(y[test].copy())\n\n        #save the results in a list\n        predictions.append(prediction)\n        actuals.append(actual)\n        \n    predictions = np.array(predictions)\n    actuals = np.array(actuals)\n    \n    return predictions, actuals","6d14939a":"preds, actuals = crossval_testbench(X, Y, 2, epochs=150, verbose=1)","ed4b26ef":"preds.shape, actuals.shape","e28b8bea":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","46006722":"#MAPE for a specific hour\nerror_h0 = mean_absolute_percentage_error(actuals[0,:5,0], preds[0, :5,0])\nprint(f'MAPE for Hour 0: {round(error_h0, 2)}')","53256e0f":"crossvals = actuals.shape[0]\nhours = actuals.shape[2]\n\nerrors_crossvals = list()\nfor crossval in range(crossvals):\n    errors_hourly = [mean_absolute_percentage_error(actuals[crossval, :, hour], preds[crossval, :, hour]) for hour in range(hours)]\n    errors_crossvals.append(errors_hourly)\n    \nerrors = pd.DataFrame(errors_crossvals)\nerrors['mean'] = errors.mean(axis=1)\nerrors.index.name='crossval set'\nerrors.columns.name='hours'\nerrors","f6d6ef77":"plt.figure(figsize=(8,6))\nplt.plot(errors.drop(columns='mean').T)\nplt.title('MAPE per hourly prediction')\nplt.legend(errors.index, title='crossval set')\nplt.xlabel('Hour of the day')\nplt.ylabel('MAPE')\nplt.show()","3bb02933":"def train_test_split(df, split_date):\n    \n    \n    train_date = pd.Timestamp(split_date).strftime('%Y-%m-%d')\n    test_date = (pd.Timestamp(split_date) + timedelta(1)).strftime('%Y-%m-%d')\n    \n    df_train = df[:train_date]\n    df_test = df[test_date:]\n    \n    return df_train, df_test\n\ntrain, test = train_test_split(day_energy, '2017-12-31')\n\nprint(f'Training start date {train.index.min()} end date {train.index.max()}')\nprint(f'Training start date {test.index.min()} end date {test.index.max()}')","d179e262":"train_norm, scalar = normalize_df(train)\ntest_norm = scalar.transform(test)\n\n#create the supervised learning problem\nn_steps = 21\n\nX_train, Y_train = split_sequences(train_norm.values, n_steps, extra_lag=True, long_lag_step=7, max_step=60, idx=0, multivar=False)\n\nprint(f'Training Set X {X_train.shape} and Y {Y_train.shape}')","1a773227":"test_set = np.vstack([train_norm.values[-60:], test_norm])\nprint(f'Dimensions of the test set with training data needed for predictions: {test_set.shape}')\n\nX_test, Y_test = split_sequences(test_set, n_steps, extra_lag=True, long_lag_step=7, max_step=60, idx=0, multivar=False)\n\nprint(f'Testing Set X {X_test.shape} and Y {Y_test.shape}')","53705b58":"n_features=X_train.shape[1]\nn_hours=X_train.shape[2]\n#initalize the lstm model\nlstm_eval = lstm_parallel_out(n_features, n_hours, learning_rate=5e-3)\n        \n#fit the model\nlstm_eval.fit(X_train, Y_train, epochs=350, verbose=1, shuffle=False)\n        \n#check predictions on the train set\ntrain_predictions = lstm_eval.predict(X_train, verbose=1)\n\n#run predictions on test data\ntest_predictions = lstm_eval.predict(X_test, verbose=1)","29cc98e0":"train_preds = scalar.inverse_transform(train_predictions)\ntest_preds = scalar.inverse_transform(test_predictions)\nY_train = scalar.inverse_transform(Y_train)\nY_test = scalar.inverse_transform(Y_test)","fe01116a":"train_error = pd.DataFrame([mean_absolute_percentage_error(Y_train[:, hour], train_preds[:, hour]) for hour in range(hours)], columns=['train'])\ntest_error = pd.DataFrame([mean_absolute_percentage_error(Y_test[:, hour], test_preds[:, hour]) for hour in range(hours)], columns=['test'])\n\nerrors = pd.concat([train_error, test_error], axis=1)\nerrors.index.name = 'hour'\nerrors.plot()","2e39cc8f":"test_df = pd.DataFrame(test_preds).stack()\nY_test_df = pd.DataFrame(Y_test).stack()\n\npreds_df = pd.concat([Y_test_df, test_df], axis=1)\npreds_df.columns = ['actual', 'predicted']\n\npreds_df.index = pd.DatetimeIndex(pd.date_range(start='2018-01-01T0000', end='2018-12-31T2300', freq='H'))\n\nfig = plt.figure(figsize=(10,10))\n\nfor week in range(52):\n\n    fig.add_subplot()\n    preds_df.iloc[week*7*24:(week+1)*7* 24].plot()\n    plt.title(f'Consumption profile for 2018 Week: {week+1}')","ca680cd6":"#### Define a crossvalidation testbench\nWe are training on data from 2015 to 2018 inclusive. This is a relatively small amount of energy data. Therefore we use time series split to cross validate within the 4 year period.","c743117c":"This is not 100% correct because we introduce data where a time does not exist. However it prevents MAPE returning inf from division by zero calculations.","19742960":"## Clean Target Column\n\nWe know there are null values in the target column. We can linerally interpolate them as a good approximation.","9cfd794f":"## Building the model\n\n#### Normalize & Create Samples (feature creation)\n\nNormalization of the values in preparation for the LSTM model is done with MinMaxScaler\n\nNext we setup the supervised learning problem. Following the tutorial on [MachineLearningMastery](https:\/\/machinelearningmastery.com\/convert-time-series-supervised-learning-problem-python\/) we create paired windows of X and Y. \n\nX is composed of rows of multiple days (referenced as lags), and columns of the hours of the day. \nY is then the 24 hours of observed energy demand for the target day.\n\nThe function split_sequences returns the target day as the Y and allows a combination of past days used in the prediction up to some maximum. ","e55694aa":"## Investigating Model Performance\n\nWe calcualte and plot MAE for each hour predicted. This gives an idea of where the model is predicting well, and where it is making errors.\n\nTo compare model runs we can calcualte total model MAE. However to do this we must sum all errors for each hour, with all other errors and then take the mean.\n","c47b674d":"Following the comments in the (partial)autocorrelation analysis we will use a daily lookback of 21 days to account for the hours 0-1 and 21-23. After 21 days, we will use multiples of 7 up to a max of 60 days.","1a3e7aaf":"#### Define the LSTM model","a0c65aa2":"### Plot Holdout Test Predictions","0fd72ea7":"## Predicting on an Unseen Test Set\n\nThe test bench allowed to modify and iterate on the model. To get an evaluation on model performance with unknown data we create a typical train test split.\n\nTo do this we will also need to run the train and test sets through the preprocessing pipeline again.[](http:\/\/)","7d342256":"### Rescale Predictions and Evaluate","ed4b3ff2":"# Forecasting Energy Demand With Keras\nThis notebook implements a simple univariate LSTM model with Kearas structured as a multiple-input, multiple-outputs configuration such that each input and output is the univariate sequence of a given hour of the day.\n\nThe model presented here was implemented as part of a project that explored different models to make 24 hour load predictions. The full project can be found in [this github repo](https:\/\/github.com\/nicholasjhana\/short-term-energy-demand-forecasting).\n\n## Model Descrption\nThe model makes a prediction each 24 hours at midnight and forecasts the next 24 hours of demand. Traditional sequence predictions take in hourly consumption in sequnces corresponding to each day's 24 hours followed by the next until the end of the sequence. The method described in this notebook treats each hour of the day as independent. Therefore it considers 24 indivdual forecasts corresponding to each hour of the day. The benefit of this method is that 1) we take advantage of stronger direct (partial) autocorrelations between h0...h23 of today, the day prior and so on (compared with the autocrrelation between h0 and h1, h2, h3 etc) adn 2) it allows us to train on smaller datasets and capture seasonal effects.\n\n","23968b5a":"### Comments on Hour-by-Hour transformation\nCompared with the houly sequetnal plot, the hour-by-hour (partial) autocorrelation plots drop off rapidly. This indicates there is a stronger moving average process present when looking at the data in this way. \n\n#### Autocorrelation in the hour-by-hour plots.\n- Hours 2-21 show clear cyclic autocorrelation every 7 days. The cycle of using multiples of 7 days look backs up to about 30-60 days seems like a good feature.\n- Hours 0-1, and 20-23 have a diffierent structure. Here the corelation is strongest going back the first 21-30 days. The cycle of using multiples of 7 days does not seem as strong. For these hours using the last 21-30 days directly might be a better option.\n\n#### Partial Autocorrelation\n- We observe a similar correlation with the cycle of every 7ths day a strong partial autocrrelation. This time there is no clear distinction between the different hourly slices. The strongest partial autocorrelation seen through all the plots appears to be the 28th day. This ","659b463d":"### Train model\nUse the entire training set for the model to learn.","fcf58710":"To construct the testing set samples we must add the last 60 (corresponding to max_step) values to the train. These will be used to make the first prediction in the test set.","2f4b2db3":"## Analysis and Next Steps: How could this model be improved\nThe model performs reasonably well on hours 12 to 23 and poorly on select hours early in the day. Considering the making predictions on a test set the model preforms almost equally well and shows limited overfitting. One thing to notice in the weekly plots is that some weeks the model performs very well. Other weeks not well at all. This indicates that processes outside the scope of the univaraite lag features is not being captured.\n\n\nTo improve the model we can consider several courses of action.\n1. Add more energy consumption data either to the dataset directly, or in the form of adjusting the lag times. We saw that for some hours (21-23) there was a strong autocorrelation with the most recent days while with other hours (0-21) it was a longer lag period of about each 7 days that showed the stronger autocorrelation. In this workbook we have left the LSTM model to figure out the difference between the hours. We could feed the model different configurations of lags for the different hours of the day to improve model performance in specfic hours.\n2. Add more features. The dataset contains other relevant features that may be used to forecast the coming 24 hours of demand. Several that might be of interests are:\n    - Weather features in major cities. Major cities consume the majority of power. We can investigate the correlation between weather and energy demand and use it as a correlated input in the model.\n    - Energy consumption in shared power regions. In this case we ware working with energy demand from Spain. Spain engages in transborder interactions with portugal and france. There is possibly a correlation between power transfers that would help the model forecast better.\n    - Encode a day of the week and holidays feature. The model is already considering day of the week implicitly by looking at the 7th, 14th, 21st etc lags. So its unlikely day of the week encoding will improve the perforamnce. Holidays however are not consdiered and would add new information to the model. The challenge here is that for each holiday the model will only have 4 data points (corresponding to the 4 years of data). This is limited data for the model to learn from.\n    - Model strcuture and hyperparameters. In 1 we identify how using different lag structures on a per hour basis. We could test the model structure with different architectures that support this. One example might be to use a sequence to sequence model architecture. Another option is to train on more epochs and see the relationship between more training and performance.\n3. Apply regularization techniques such as dropout to account for any overfitting.\n4. Use the remaining data from 2019 as a test set and evaluate the model on unseen data.\n    \n    \nIf you have found this model interesting please upvote! And if you have any ideas or comments on this work please share, would love to hear what you think!","dbbcfd69":"## Create Dataset from Univariate Energy Loads","fd5e08d0":"## Transform Data: Hour by Hour\n\nIn the following section we will do a partial\/autocorrelation analysis on the original sequence of the form h1, h2, h3 etc. We will also analyize the partial\/autocorrelation sequence D1 h1, D2 h1, D3 h1 for each of the 24 hours in the day. This section transforms the original data into the second form. ","d74b7da7":"Missing values were accounted for before the previous step. However the above transformation will intorduce new missing values because of daylight savings time. We can see below this is true.","0f94a30a":"## Autocorrelation and Partial-Autocorrelation Analysis\n\n1. Energy demand as hourly-sequental view. I.e. Day 1 h1, h2...h23, Day 2, h1, h2... \n2. Energy demand as hour-by-hour transformation I.e. Day1, h1, Day 2, h1 ... Day 365 h1 for each hour in the day.","99399c49":"### Comments on Hourly Sequential (Partial) Autocorrelation Plots\nIn the above plots there is clear autocorrelation between consequtive hours. Nearly every (partial) autoregressive feature is significant as indicated by it's magnitude being larger than the light blue cone. This means the strong influecne between the previous hours and the current state. The difficulty is that there is such a strong correlation, that it is an indication of a low correlation to a moving average process. This means we will have a lot of noise to filter when making predictions using the data in this view. Selecting features is difficult. We see some lookbacks are stronger correlated than others, and this gives us a clue to the features that are of interest. However, we don't know if predicting each hour of the day should use the same features. "}}