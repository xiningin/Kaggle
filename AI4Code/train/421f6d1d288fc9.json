{"cell_type":{"93bc8564":"code","8a6cf0e0":"code","272f91b3":"code","973b5598":"code","542ef11a":"code","273dd711":"code","0e0aefaa":"code","26060114":"code","273b85e5":"code","5f6786cf":"code","1d0aab85":"code","66af9edd":"code","585b65b0":"code","7e433a40":"code","9fecb942":"code","084ae154":"code","031efd71":"code","27d04959":"code","d90abe5a":"code","b0440502":"code","6b7a98bf":"code","af8c34c0":"code","a0c62938":"code","cafaf3be":"code","53bfbb0f":"code","71c0d0cb":"code","81e2d670":"code","07eac80b":"code","a0efdd66":"code","da28fa92":"code","626041b4":"code","9e6079c8":"code","f3477a87":"code","59483dfe":"code","05db03d9":"code","70649e90":"code","6da829eb":"code","c1cf3886":"code","bc72e6e4":"code","3b0288db":"code","7dfd540b":"code","07a50194":"code","5f1ea7cc":"code","645cf900":"code","67884977":"code","8a2aff99":"code","8d0d0dd0":"code","bc37d60c":"code","17ddb0fa":"code","ea853c76":"code","31b2337c":"markdown","81f363eb":"markdown","5fb0f005":"markdown","51a32acf":"markdown","606c579d":"markdown","64094ed6":"markdown","a2d16604":"markdown","e50aaecb":"markdown","a4b1c8c2":"markdown","db1951a4":"markdown","44f71929":"markdown","8dd149f0":"markdown","70075d46":"markdown","444dc63a":"markdown","a3e547a6":"markdown","5d9cd819":"markdown","6d96e825":"markdown","274880c3":"markdown","5d9ee220":"markdown","4d1b9d85":"markdown","6a780c95":"markdown","9222159d":"markdown","5d6aa540":"markdown","07f356f4":"markdown","9799219d":"markdown","5f51e733":"markdown","6e151180":"markdown","6a21b38d":"markdown","94bb64de":"markdown","f1e56c99":"markdown","78c2400a":"markdown","2267050b":"markdown","9b8c0c8b":"markdown","0aaeb00a":"markdown","2924dc66":"markdown","d16bae13":"markdown","43f5450b":"markdown"},"source":{"93bc8564":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8a6cf0e0":"df=pd.read_csv(\"..\/input\/telecom-customer-churn-dataset\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf.head()","272f91b3":"df.columns","973b5598":"df.info()","542ef11a":"df.describe(include='all').transpose()","273dd711":"#Check shape\ndf.shape","0e0aefaa":"#Check duplicates\ndf.duplicated().sum()","26060114":"#Check missing values\ndf.isnull().sum()","273b85e5":"#Replace ' ' with NaN\ndf = df.replace(r'^\\s*$', np.nan, regex=True)","5f6786cf":"#Check for missing values\ndf.isnull().sum()","1d0aab85":"df['TotalCharges']=df['TotalCharges'].fillna(df['TotalCharges'].median())","66af9edd":"#Check missing values again\ndf.isnull().sum()","585b65b0":"#drop duplicates\ndf.drop_duplicates(inplace=True)","7e433a40":"df.shape","9fecb942":"#Now let's change data types for senior citizen tenure and Total Charges\ndf['TotalCharges']=df['TotalCharges'].astype(float)\ndf['tenure']=df['tenure'].astype(float)\ndf['SeniorCitizen']=df['SeniorCitizen'].astype(object)","084ae154":"#Let's check data types again\ndf.dtypes","031efd71":"num_col = df.select_dtypes(include=np.number).columns.tolist()\n\nplt.figure(figsize=(20,30))\n\nfor i, variable in enumerate(num_col):\n                     plt.subplot(5,4,i+1)\n                     sns.histplot(df[variable])\n                     plt.tight_layout()\n                     plt.title(variable)\n\nplt.show()","27d04959":"num_col = df.select_dtypes(include=np.number).columns.tolist()\n\nplt.figure(figsize=(20,30))\n\nfor i, variable in enumerate(num_col):\n                     plt.subplot(5,4,i+1)\n                     sns.boxplot(df[variable])\n                     plt.tight_layout()\n                     plt.title(variable)\n\nplt.show()","d90abe5a":"cat_col = df.select_dtypes(include=object).columns.tolist()\ncat_col.remove(\"customerID\")\nplt.figure(figsize=(20,30))\n\nfor i, variable in enumerate(cat_col):\n                     plt.subplot(5,4,i+1)\n                     sns.countplot(df[variable],palette=\"PuBu\")\n                     plt.tight_layout()\n                     plt.title(variable)\n\nplt.show()","b0440502":"#Paperless billing is relatively high\n#Most contracts are month-to-month\n#Most have a phone service\n#there are equal genders\n#Most don't have dependents\n#There are far less senior citizens","6b7a98bf":"#bivariate analysis","af8c34c0":"cols = df.select_dtypes(include=np.number).columns.tolist()\nplt.figure(figsize=(12,8))\n\nfor i, variable in enumerate(cols):\n                     plt.subplot(3,2,i+1)\n                     sns.boxplot(df[\"Churn\"],df[variable],palette=\"PuBu\")\n                     plt.tight_layout()\n                     plt.title(variable)\nplt.show()\n\n","a0c62938":"#The less tenure the higher the churn rate\n#The higher the monthly charges the higher the churn rate\n#Total charges are related to monthly charges and tenure. Obviousely the longer the tenure the higher the total charges.The higher the total\n#charges churn rate becomes less","cafaf3be":"cols1 = df.select_dtypes(include=object).columns.tolist()\nplt.figure(figsize=(20,30))\n\nfor i, variable in enumerate(cols1):\n                     plt.subplot(5,4,i+1)\n                     sns.countplot(df[variable],hue=df[\"Churn\"],palette=\"PuBu\")\n                     plt.tight_layout()\n                     plt.title(variable)\nplt.show()","53bfbb0f":"#Churn rate is low which means that we are dealing with an imbalanced dataset\n#Electronic check has the highest churn rate among payment methods\n#Churn rate is high among customers who don't use optional sevices\n##Churn rate is high for customers who use paperless billing\n#Month-to-Month contract has the highest churn rate\n#Fiber optic has the highest churn rate\n#Churn rate is high for customers who use a phone service\n#Churn rate is high for customers who don't have dependents or partners\n","71c0d0cb":"#Correlation analysis","81e2d670":"sns.heatmap(df.corr())","07eac80b":"#TotalCharges and tenure have a high correlation but this is irrelevant to us","a0efdd66":"df=df.drop(df[['customerID','MultipleLines','SeniorCitizen']],axis=1)","da28fa92":"from sklearn.preprocessing import MinMaxScaler\nfeatures_mms = ['tenure', 'MonthlyCharges', 'TotalCharges']\ndf_features_mms = pd.DataFrame(df, columns=features_mms)\ndf_remaining_features = df.drop(columns=features_mms)\n\nmms = MinMaxScaler()\nrescaled_features = mms.fit_transform(df_features_mms)\n\ndf_rescaled_features = pd.DataFrame(rescaled_features, columns=features_mms, index=df_remaining_features.index)\n\ndf = pd.concat([df_remaining_features, df_rescaled_features], axis=1)\n\n\n","626041b4":"from sklearn.preprocessing import LabelEncoder\nfeatures_le = ['gender', 'Partner', 'Dependents', 'Churn', 'PhoneService', 'PaperlessBilling']\n\ndef label_encoding(features, df):\n    for i in features:\n        df[i] = df[i].map({'Yes': 1, 'No': 0})\n    return       \n\nlabel_encoding(['Partner', 'Dependents', 'Churn', 'PhoneService', 'PaperlessBilling'], df)\n\ndf['gender'] = df['gender'].map({'Female': 1, 'Male': 0})","9e6079c8":"features = ['InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']\n\ndf = pd.get_dummies(df, columns=features) \n","f3477a87":"from sklearn.model_selection import train_test_split \nX = df.drop('Churn', axis=1)\ny = df['Churn']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)","59483dfe":"def evaluate_model(model, X_test, y_test):\n    from sklearn import metrics\n\n    # Predict Test Data \n    y_pred = model.predict(X_test)\n\n    # Calculate accuracy, precision, recall, f1-score, and kappa score\n    acc = metrics.accuracy_score(y_test, y_pred)\n    prec = metrics.precision_score(y_test, y_pred)\n    rec = metrics.recall_score(y_test, y_pred)\n    f1 = metrics.f1_score(y_test, y_pred)\n    kappa = metrics.cohen_kappa_score(y_test, y_pred)\n\n    # Calculate area under curve (AUC)\n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n\n    # Display confussion matrix\n    cm = metrics.confusion_matrix(y_test, y_pred)\n\n    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'kappa': kappa, \n            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'confusion matrix': cm}","05db03d9":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report,plot_confusion_matrix","70649e90":"knn=KNeighborsClassifier(n_jobs=-1)\nknn.fit(X_train,y_train)","6da829eb":"# Evaluate Model\nknn_eval = evaluate_model(knn, X_test, y_test)\n\n# Print result\nprint('Accuracy:', knn_eval['accuracy'])\nprint('Precision:', knn_eval['precision'])\nprint('Recall:', knn_eval['recall'])\nprint('F1 Score:', knn_eval['f1'])\nprint('Cohens Kappa Score:', knn_eval['kappa'])\nprint('Area Under Curve:', knn_eval['auc'])\nprint('Confusion Matrix:\\n', knn_eval['confusion matrix'])","c1cf3886":"from sklearn.naive_bayes import BernoulliNB ","bc72e6e4":"BN=BernoulliNB()\nBN.fit(X_train, y_train)","3b0288db":"# Evaluate Model\nBN_eval = evaluate_model(BN, X_test, y_test)\n\n# Print result\nprint('Accuracy:', BN_eval['accuracy'])\nprint('Precision:',  BN_eval['precision'])\nprint('Recall:', BN_eval['recall'])\nprint('F1 Score:', BN_eval['f1'])\nprint('Cohens Kappa Score:', BN_eval['kappa'])\nprint('Area Under Curve:', BN_eval['auc'])\nprint('Confusion Matrix:\\n', BN_eval['confusion matrix'])","7dfd540b":"plot_confusion_matrix(BN,X_test,y_test,cmap=plt.cm.Blues)","07a50194":"from sklearn.linear_model import LogisticRegression","5f1ea7cc":"LR = LogisticRegression(random_state = 123,n_jobs = -1)\nLR.fit(X_train, y_train)","645cf900":"# Evaluate Model\nLR_eval = evaluate_model(LR, X_test, y_test)\n\n# Print result\nprint('Accuracy:', LR_eval['accuracy'])\nprint('Precision:', LR_eval['precision'])\nprint('Recall:', LR_eval['recall'])\nprint('F1 Score:', LR_eval['f1'])\nprint('Cohens Kappa Score:', LR_eval['kappa'])\nprint('Area Under Curve:', LR_eval['auc'])\nprint('Confusion Matrix:\\n', LR_eval['confusion matrix'])","67884977":"from sklearn import tree\n\n# Building Decision Tree model \ndtc = tree.DecisionTreeClassifier(random_state=0)\ndtc.fit(X_train, y_train)","8a2aff99":"# Evaluate Model\ndtc_eval = evaluate_model(LR, X_test, y_test)\n\n# Print result\nprint('Accuracy:',dtc_eval['accuracy'])\nprint('Precision:', dtc_eval['precision'])\nprint('Recall:', dtc_eval['recall'])\nprint('F1 Score:', dtc_eval['f1'])\nprint('Cohens Kappa Score:', dtc_eval['kappa'])\nprint('Area Under Curve:', dtc_eval['auc'])\nprint('Confusion Matrix:\\n', dtc_eval['confusion matrix'])","8d0d0dd0":"from sklearn.ensemble import RandomForestClassifier\n\n# Building Random Forest model \nrf = RandomForestClassifier(random_state=0)\nrf.fit(X_train, y_train)","bc37d60c":"# Evaluate Model\nrf_eval = evaluate_model(LR, X_test, y_test)\n\n# Print result\nprint('Accuracy:',rf_eval['accuracy'])\nprint('Precision:', rf_eval['precision'])\nprint('Recall:', rf_eval['recall'])\nprint('F1 Score:', rf_eval['f1'])\nprint('Cohens Kappa Score:', rf_eval['kappa'])\nprint('Area Under Curve:', rf_eval['auc'])\nprint('Confusion Matrix:\\n', rf_eval['confusion matrix'])","17ddb0fa":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=3)\nlogreg_cv.fit(X_train,y_train)","ea853c76":"logreg2=LogisticRegression(C=2,penalty=\"l2\")\nlogreg2.fit(X_train,y_train)\nprint(\"score\",logreg2.score(X_test,y_test))","31b2337c":"##### Train_Test_Split","81f363eb":"##### Check dataset shape, duplicates and missing values","5fb0f005":"##### Model building","51a32acf":"### Feature engineering","606c579d":"##### OneHotEncoding","64094ed6":"##### Model accuracy","a2d16604":"##### Model building","e50aaecb":"Numerical data aren't normally distributed, and they don't have outliers.Therefore we will use MinMaxScaler.","a4b1c8c2":"\u25cfSeniorCitizen and TotalCharges have the wrong data types.\\\n\u25cfThe ratio of non churners to churners seems to be imbalanced","db1951a4":"#### Decision tree","44f71929":"##### Label encoding","8dd149f0":"##### Model building","70075d46":"##  Model building","444dc63a":"#### Random tree","a3e547a6":"### Import libraries","5d9cd819":"##### Accuracy","6d96e825":"#### Logistic regression","274880c3":"##### Feature scaling","5d9ee220":"##### Logistic regression","4d1b9d85":"##### info() and describe()","6a780c95":"##### Model accuracy","9222159d":"##### insights","5d6aa540":"Based on EDA results, customerID, multiple lines don't seem to have a high impact on the churn rate. Senior citizen variable has a big imbalance which means It might not give us an accurate prediction for the churn rate. Therefore we need to drop these 3 features","07f356f4":"##### Check empty cells","9799219d":"##### Model building","5f51e733":"### Naive Bayes classifier","6e151180":"##### Model building","6a21b38d":"\u25cfThere are 7043 and 21 columns\\\n\u25cfThere 26 duplicates that need to be dropped\\\n\u25cfThere seem to be no missing values, but we need to check if there are blanks or unrelated values first","94bb64de":"##### Check columns","f1e56c99":"##### Model accuracy","78c2400a":"##### Insights","2267050b":"##### Feature rescaling","9b8c0c8b":"### Hyperparameter tuning","0aaeb00a":"##### Model accuracy","2924dc66":"#### KNN","d16bae13":"### Load dataset and basic data exploration","43f5450b":"##### Load dataset"}}