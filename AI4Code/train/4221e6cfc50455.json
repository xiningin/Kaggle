{"cell_type":{"f6d708ec":"code","d1fb9c60":"code","c7af71c4":"code","b024fc00":"code","cb061238":"code","3b132798":"code","32af74e0":"code","55bdb50f":"code","aa50e5ad":"code","856a4cac":"code","72df1521":"code","b63f4c83":"code","3cbb0b9b":"code","95fb2dd1":"code","786a4fe7":"code","5d9acbb9":"code","b0ec959a":"code","326d5685":"code","e81ab975":"code","f58110ae":"code","cd953b8e":"code","467dcef5":"code","34fce773":"code","8bd02102":"code","37d704fd":"code","3e596aca":"code","a7ae47fa":"code","27fdd0d6":"code","12fdcd7c":"code","f124a77d":"markdown","8caef603":"markdown","51b70c44":"markdown","b379707d":"markdown","fa0b6bad":"markdown","d476bc5e":"markdown","e58f43e8":"markdown","25f25b16":"markdown","e6f4e2c3":"markdown","c2c051ec":"markdown"},"source":{"f6d708ec":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","d1fb9c60":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#neural layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import to_categorical\n\n#matplotlib defaults\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')","c7af71c4":"from tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.python.ops.numpy_ops import np_utils","b024fc00":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","cb061238":"#snapshot on the training and test data shapes\ntrain.shape, test.shape","3b132798":"#lets take a shapshot on the train data\ntrain.head()","32af74e0":"#lets take a shape shot on the test data\ntest.head()","55bdb50f":"#preprocessing the datasets to obtain working train and test data as well as obtain the target label\ntrainX = (train.iloc[:,1:].values).astype('float32') # obtaining pixel values as features for each train data point\ny_label = (train.iloc[:,0].values).astype('int32') # obtain the target  labels.\ntestX = test.values.astype('float32') # obtaining pixel values as features for each test data point","aa50e5ad":"#new shapes of the data\ntrainX.shape, testX.shape, y_label.shape","856a4cac":"#reshaping the target label\ny_label = y_label.reshape(-1,1)\ny_label.shape","72df1521":"#number of unique values in the target variable to be classified.\nprint(\"There are {} different labels in this dataset. \".format(len(np.unique(y_label))))\nprint(\"The labels are: {} \".format(np.unique(y_label)))","b63f4c83":"#convert train and test datasets to (num_images, img_rows, img_cols,1) format  where 1 is the color channel\ntrainX = trainX.reshape(trainX.shape[0], 28, 28,1)\ntestX = testX.reshape(testX.shape[0], 28, 28,1)","3cbb0b9b":"#sample data visualization\n#Lets look at 6 images from data set with their labels\n\nfor i in range(3, 9):\n    plt.subplot(330 + (i+1))\n    plt.imshow(trainX[i], cmap=plt.get_cmap('gray'))\n    plt.title(y_label[i]);","95fb2dd1":"trainX = trainX \/ 255\ntestX = testX \/ 255","786a4fe7":"#One Hot encoding of labels\ntrain_labels= to_categorical(y_label)\nnum_classes = train_labels.shape[1]\nnum_classes","5d9acbb9":"model = Sequential([\n\n    # First Convolutional Block\n    layers.Conv2D(filters=32, kernel_size=5, activation=\"relu\", padding='same',\n                  # give the input dimensions in the first layer\n                  # [height, width, color channels(RGB)]\n                  input_shape=[28, 28, 1]),\n    layers.Dropout(0.3),\n    layers.MaxPool2D(),\n\n    # Second Convolutional Block\n    layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.Dropout(0.3),\n    layers.MaxPool2D(),\n\n\n    # Classifier Head\n    layers.Flatten(),\n    layers.Dense(units=512, activation=\"relu\"),\n    layers.Dropout(0.3),\n    layers.Dense(units=10, activation=\"softmax\"),\n])","b0ec959a":"model.summary()","326d5685":"from tensorflow.keras.optimizers import RMSprop\nmodel.compile(\n    \noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)","e81ab975":"#making splits to obtain data for cross validation\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(trainX, train_labels, test_size=0.20, random_state=1)","f58110ae":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True,\n)","cd953b8e":"import time\nstart = time.process_time()\nhistory1 = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=1000,\n    callbacks=[early_stopping],    \n    verbose=0, # hide the output because we have so many epochs\n)\nend=time.process_time()","467dcef5":"history_df1 = pd.DataFrame(history1.history) #.0298,.9924\n# plot for the dataframe\nhistory_df1.loc[:, ['loss', 'val_loss']].plot()\nhistory_df1.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df1['val_loss'].min(), \n              history_df1['val_accuracy'].max()))\nprint (\"Computation time without data augmentation = \" + str( (end - start)\/60 ) + \" Mins\")","34fce773":"# With data augmentation to prevent overfitting and improve on generalization.\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1) # randomly shift images vertically (fraction of total heigh)\n\n\ndatagen.fit(X_train)","8bd02102":"batch_size = 512\nepochs = 1000","37d704fd":"import time\nstart = time.process_time()\n# Fit the model\nhistory2 = model.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = epochs, \n                              validation_data = (X_valid, y_valid),\n                              steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                              validation_steps= 100,\n                              callbacks=[early_stopping],verbose=0) # hide the output because we have so many epochs)\n\nend=time.process_time()","3e596aca":"#Model performance\nhistory_df2 = pd.DataFrame(history2.history) #0.239,.9939\n# plot for the dataframe\nhistory_df2.loc[:, ['loss', 'val_loss']].plot()\nhistory_df2.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df2['val_loss'].min(), \n              history_df2['val_accuracy'].max()))\nprint (\"Computation time with data augmentation = \" + str( (end - start)\/60 ) + \" Mins\")","a7ae47fa":"#final predictions\npredictions = model.predict_classes(testX, verbose=0)","27fdd0d6":"submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"digit_classifier.csv\", index=False, header=True)","12fdcd7c":"#Sample print\nsubmissions.head().set_index('ImageId')","f124a77d":"# 2. Data Inception and formatting","8caef603":"### Case 1: Without Application of Data Augmentation","51b70c44":"### Case 2: With Application of Data Augmentation","b379707d":"# 1. Importation of libraries","fa0b6bad":"# 3. Image preprocessing","d476bc5e":"### Feature Standardization\n\nFor picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\nIt is important preprocessing step. It is used to centre the data around zero mean and unit variance","e58f43e8":"# 5. Model Training, Validation and Evaluation","25f25b16":"MNIST (\"Modified National Institute of Standards and Technology\") is the de facto \u201chello world\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n## The goal is to take an image of a handwritten single digit, and determine what that digit is.\n### For every digit in the test set, you should predict the correct label","e6f4e2c3":"## 4. Model Building","c2c051ec":"This has been a simple implementations of convnets with mnsit digit dataset. if you like this notebook, you can please upvote for it.\nIt's still under improvement.\ncheers!!"}}