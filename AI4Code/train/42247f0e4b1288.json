{"cell_type":{"2eb13fcf":"code","714bd98e":"code","36a70531":"code","b77410c8":"code","c3f58085":"code","31ecc52e":"code","be139bc0":"code","376a17b3":"code","d791444e":"code","521e8740":"code","9f71fdd8":"code","80000be6":"code","38b0ebc6":"code","5f5c9ec7":"code","7490f64d":"code","e7da12aa":"code","8ed804b3":"code","ea32835d":"code","0528b9b7":"code","b5e84384":"code","5e3dded7":"code","19ece9a7":"code","58fb275b":"code","aebc83da":"code","52ffe703":"code","bcb7842f":"code","3e1a9a63":"code","c474e369":"code","523fbfd4":"code","44b56c8b":"code","1d3b6f58":"code","566f7288":"code","6b590a43":"code","50e55074":"code","8ecee192":"code","8251ccc4":"code","d89cbc01":"code","cb35d1f9":"code","3bbeb05b":"code","bc065802":"code","1d68d05e":"code","50ef75e8":"code","eeb31769":"code","2d65d3aa":"code","10a00829":"code","54c4267d":"code","95189217":"markdown","694e5aec":"markdown","5148dbd8":"markdown","0e233b7f":"markdown","967bcfae":"markdown","a758dfe0":"markdown","28c2b2d1":"markdown","619a1f7d":"markdown","fda01c93":"markdown","9d61ed1c":"markdown","03ed6a03":"markdown","b06cb4fa":"markdown","e4d9bbe0":"markdown","d2b0b3b8":"markdown","d6450ac4":"markdown","1cf50e56":"markdown"},"source":{"2eb13fcf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","714bd98e":"df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')","36a70531":"!pip install textstat","b77410c8":"!mkdir pip; cd pip; pip download textstat pyphen","c3f58085":"import textstat","31ecc52e":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error","be139bc0":"tfidf = TfidfVectorizer()","376a17b3":"X = df['excerpt'].values\nY = df['target'].values","d791444e":"tfidf.fit(X, Y)","521e8740":"# Voy a hacer el fit_transform directamente porque es lo que recomienda la doc: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html\n# Haci\u00e9ndolo as\u00ed, el paso anterior (fit) me lo puedo saltar\nXtfidf = tfidf.fit_transform(X)\nXtfidf","9f71fdd8":"vector_primer_texto = Xtfidf[0]","80000be6":"vector_primer_texto[vector_primer_texto != 0]","38b0ebc6":"vector_primer_texto[vector_primer_texto != 0].max()","5f5c9ec7":"vector_primer_texto[vector_primer_texto != 0].min()","7490f64d":"reg = LinearRegression().fit(Xtfidf, Y)","e7da12aa":"reg.coef_.shape","8ed804b3":"pred = reg.predict(Xtfidf)","ea32835d":"# Muestra las predicciones\npred","0528b9b7":"# Muestra los targets\nY","b5e84384":"# Un poco sospechoso... Vamos a ver el RMSE\nmean_squared_error(pred, Y, squared=False)","5e3dded7":"Xtfidf.shape","19ece9a7":"type(Xtfidf)","58fb275b":"(Xtfidf != 0).sum(axis=1)","aebc83da":"(Xtfidf != 0).sum(axis=0)","52ffe703":"f'{(Xtfidf != 0).sum() \/ (Xtfidf.shape[0] * Xtfidf.shape[1]) * 100:.3f}%'","bcb7842f":"from sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom tqdm.auto import tqdm","3e1a9a63":"model = make_pipeline(\n    TfidfVectorizer(),\n    LinearRegression()\n)","c474e369":"kf = KFold(n_splits=5, shuffle=True, random_state=33)","523fbfd4":"for train_idx, valid_idx in tqdm(kf.split(X), total=5):\n    Xt = X[train_idx]\n    Yt = Y[train_idx]\n    Xv = X[valid_idx]\n    reg = model.fit(Xt, Yt)\n    df.loc[valid_idx, 'pred'] = model.predict(Xv)\n    print(mean_squared_error(df.loc[valid_idx, 'pred'], df.loc[valid_idx, 'target'], squared=False))","44b56c8b":"mean_squared_error(df['pred'], df['target'], squared=False)","1d3b6f58":"model = make_pipeline(\n    TfidfVectorizer(),\n    Ridge()\n)","566f7288":"for train_idx, valid_idx in tqdm(kf.split(X), total=5):\n    Xt = X[train_idx]\n    Yt = Y[train_idx]\n    Xv = X[valid_idx]\n    reg = model.fit(Xt, Yt)\n    df.loc[valid_idx, 'pred'] = model.predict(Xv)\n    print(mean_squared_error(df.loc[valid_idx, 'pred'], df.loc[valid_idx, 'target'], squared=False))","6b590a43":"mean_squared_error(df['pred'], df['target'], squared=False)","50e55074":"!mkdir model","8ecee192":"from joblib import dump, load","8251ccc4":"# Comentamos esta l\u00ednea porque m\u00e1s abajo los vamos a guardar todos\n#dump(model, 'model\/tfidf_ridge_fold_5_cv_0.7177.joblib')","d89cbc01":"model[1].coef_.shape","cb35d1f9":"(model[1].coef_ == 0).sum()","3bbeb05b":"model = make_pipeline(\n    TfidfVectorizer(),\n    Lasso(alpha=1)\n)\n\nfor train_idx, valid_idx in tqdm(kf.split(X), total=5):\n    Xt = X[train_idx]\n    Yt = Y[train_idx]\n    Xv = X[valid_idx]\n    reg = model.fit(Xt, Yt)\n    df.loc[valid_idx, 'pred'] = model.predict(Xv)\n    print(mean_squared_error(df.loc[valid_idx, 'pred'], df.loc[valid_idx, 'target'], squared=False))","bc065802":"model = make_pipeline(\n    TfidfVectorizer(),\n    Lasso(alpha=0.0001)\n)\n\nfor train_idx, valid_idx in tqdm(kf.split(X), total=5):\n    Xt = X[train_idx]\n    Yt = Y[train_idx]\n    Xv = X[valid_idx]\n    reg = model.fit(Xt, Yt)\n    df.loc[valid_idx, 'pred'] = model.predict(Xv)\n    print(mean_squared_error(df.loc[valid_idx, 'pred'], df.loc[valid_idx, 'target'], squared=False))","1d68d05e":"model = make_pipeline(\n    TfidfVectorizer(),\n    TruncatedSVD(100),\n    Ridge()\n)\nfor train_idx, valid_idx in tqdm(kf.split(X), total=5):\n    Xt = X[train_idx]\n    Yt = Y[train_idx]\n    Xv = X[valid_idx]\n    reg = model.fit(Xt, Yt)\n    df.loc[valid_idx, 'pred'] = model.predict(Xv)\n    print(mean_squared_error(df.loc[valid_idx, 'pred'], df.loc[valid_idx, 'target'], squared=False))\nprint(\"cv\", mean_squared_error(df['pred'], df['target'], squared=False))","50ef75e8":"a = 0.1234567890","eeb31769":"print(\"La variable a vale\", a)","2d65d3aa":"print(f\"La variable a vale {a:.3f}\")","10a00829":"# Todo lo que hay entre las {} se eval\u00faa. Ejemplo:\nprint(f\"La variable a vale {a+1:.3f}\")","54c4267d":"model = make_pipeline(\n    TfidfVectorizer(),\n    Ridge()\n)\n\nfor fold, (train_idx, valid_idx) in enumerate(tqdm(kf.split(X), total=5)):\n    Xt = X[train_idx]\n    Yt = Y[train_idx]\n    Xv = X[valid_idx]\n    reg = model.fit(Xt, Yt)\n    df.loc[valid_idx, 'pred'] = model.predict(Xv)\n    val_rmse = mean_squared_error(df.loc[valid_idx, 'pred'], df.loc[valid_idx, 'target'], squared=False)\n    file_name = f\"model\/tfidf_ridge_fold_{fold+1}_cv_{val_rmse:.4f}.joblib\"\n    print(file_name)\n    dump(model, file_name)","95189217":"# \u00bfCu\u00e1l es el porcentaje de datos distintos de 0 en la matrix Xtfidf?","694e5aec":"* Probar con regularizaci\u00f3n L1 (Lasso) y ver si mejora el CV","5148dbd8":"* Probar cambiando el hiperpar\u00e1metro alpha de Ridge\/Lasso","0e233b7f":"# Soluci\u00f3n","967bcfae":"* \u00bfCu\u00e1ntos de ellos son 0 gracias a la regularizaci\u00f3n L2? (cuanto m\u00e1s coeficientes a 0 haya, m\u00e1s efectiva habr\u00e1 sido la regularizaci\u00f3n del modelo)","a758dfe0":"* \u00bfCu\u00e1ntos coeficientes tenemos en nuestro regresor Ridge? (la dificultad de este ejercicio es que ahora nuestro regresor est\u00e1 dentro de un pipeline. Deben ser tantos como palabras haya en nuestro vocabulario, o columnas tenga Xtfidf)","28c2b2d1":"# Solucion al sobreajuste (overfit)\nVamos a hacer la misma regresi\u00f3n PERO validando con 5-folds. Esto **no** va a evitar el sobreajuste a los datos de entrenamiento, pero nos permitir\u00e1 comprobar c\u00f3mo generaliza el modelo prediciendo datos de validaci\u00f3n (que no ha visto nunca).\n\nPero antes, un inciso para que tengamos claro el contenido de la matriz Xtfidf: Xtfidf es la matriz de \u00edndices TF-IDF por cada palabra de nuestro vocabulario (columnas) y por cada extracto (filas).\n\nCon `Xtfidf.shape` podemos ver que es una matriz bastante grande (2834 x 26833). Como cada texto tiene entre 100 y 300 palabras aprox y el \u00edndice TF-IDF de una palabra que no aparece en el texto es 0, se trata de una matriz dispersa donde la gran mayor\u00eda de valores estar\u00e1n a 0 y solo unos pocos tendr\u00e1n valores distintos de 0.","619a1f7d":"# Ahora s\u00ed: entrena con 5 folds","fda01c93":"* Usar alg\u00fan reductor de dimensionalidad (ej. PCA) para reducir la complejidad del modelo y ver si mejora.","9d61ed1c":"# Ejercicios\n- \u00bfCu\u00e1ntos coeficientes tenemos en nuestro regresor Ridge? (la dificultad de este ejercicio es que ahora nuestro regresor est\u00e1 dentro de un `pipeline`. Deben ser tantos como palabras haya en nuestro vocabulario, o columnas tenga `Xtfidf`)\n- \u00bfCu\u00e1ntos de ellos son 0 gracias a la regularizaci\u00f3n L2? (cuanto m\u00e1s coeficientes a 0 haya, m\u00e1s efectiva habr\u00e1 sido la regularizaci\u00f3n del modelo)\n- Probar con regularizaci\u00f3n L1 (Lasso) y ver si mejora el CV\n- Probar cambiando el hiperpar\u00e1metro `alpha` de Ridge\/Lasso\n- Usar alg\u00fan reductor de dimensionalidad (ej. PCA) para reducir la complejidad del modelo y ver si mejora. \n  - Puntos extra por integrar PCA dentro del `pipeline`.\n- Muestra el vocabulario aprendido por el vectorizador TF-IDF\n- \u00bfQu\u00e9 palabra es la que m\u00e1s se repite en todo el corpus?","03ed6a03":"# Hay sobreajuste porque tenemos n\u00ba de muestras << n\u00ba par\u00e1metros\nhttps:\/\/witeboard.com\/8f405e60-de37-11eb-8ca9-7b665841bc76","b06cb4fa":"## \u00bfEn cu\u00e1ntos documentos aparece cada palabra?","e4d9bbe0":"## A partir de Xtfidf, \u00bfCu\u00e1ntas palabras hay en cada texto?","d2b0b3b8":"# Ejercicio\n- Persistir los **5 modelos**. Los vamos a llamar `tfidf_ridge_fold_N_cv_X.XXXX.joblib`, donde N = n\u00famero de fold (del 1 al 5) y X.XXXX es el RMSE del fold con 4 decimales. Ver las celdas de abajo para truco \u00fatil para nombrar el modelo.","d6450ac4":"# Soluciones","1cf50e56":"# \u00bfFuncionar\u00eda mejor con regularizaci\u00f3n?"}}