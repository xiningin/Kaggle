{"cell_type":{"fc52d4ac":"code","54bdfdd4":"code","5182244c":"markdown","2b656bfa":"markdown","6f8854c9":"markdown"},"source":{"fc52d4ac":"import numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\n\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n                            random_state=0)\n\nX = StandardScaler().fit_transform(X)\n\n# Compute DBSCAN\ndb = DBSCAN(eps=0.3, min_samples=10).fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\nprint(\"Adjusted Rand Index: %0.3f\"\n      % metrics.adjusted_rand_score(labels_true, labels))\nprint(\"Adjusted Mutual Information: %0.3f\"\n      % metrics.adjusted_mutual_info_score(labels_true, labels))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, labels))\n\n# Plot result\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Black removed and is used for noise instead.\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=14)\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=6)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()","54bdfdd4":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.datasets import make_circles\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nX, y = make_circles(n_samples=750, factor=0.3, noise=0.1)\nX = StandardScaler().fit_transform(X)\ny_pred = DBSCAN(eps=0.3, min_samples=10).fit_predict(X)\n\nplt.scatter(X[:,0], X[:,1], c=y_pred)\nprint('Number of clusters: {}'.format(len(set(y_pred[np.where(y_pred != -1)]))))\nprint('Homogeneity: {}'.format(metrics.homogeneity_score(y, y_pred)))\nprint('Completeness: {}'.format(metrics.completeness_score(y, y_pred)))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\nprint(\"Adjusted Rand Index: %0.3f\"\n      % metrics.adjusted_rand_score(labels_true, labels))\nprint(\"Adjusted Mutual Information: %0.3f\"\n      % metrics.adjusted_mutual_info_score(labels_true, labels))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, labels))","5182244c":"# Links:\n- [KD nuggets](https:\/\/www.kdnuggets.com\/2020\/04\/dbscan-clustering-algorithm-machine-learning.html)\n- [Sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.DBSCAN.html)\n- [github](https:\/\/github.com\/NSHipster\/DBSCAN)","2b656bfa":"# Why do we need a Density-Based clustering algorithm like DBSCAN when we already have K-means clustering?\n\n> #### K-Means clustering may cluster loosely related observations together. Every observation becomes a part of some cluster eventually, even if the observations are scattered far away in the vector space. Since clusters depend on the mean value of cluster elements, each data point plays a role in forming the clusters. A slight change in data points might affect the clustering outcome. This problem is greatly reduced in DBSCAN due to the way clusters are formed. This is usually not a big problem unless we come across some odd shape data.\n> #### Another challenge with k-means is that you need to specify the number of clusters (\u201ck\u201d) in order to use it. Much of the time, we won\u2019t know what a reasonable k value is a priori.\n> #### What\u2019s nice about DBSCAN is that you don\u2019t have to specify the number of clusters to use it. All you need is a function to calculate the distance between values and some guidance for what amount of distance is considered \u201cclose\u201d. DBSCAN also produces more reasonable results than k-means across a variety of different distributions. Below figure illustrates the fact:\n\n![](https:\/\/miro.medium.com\/max\/1339\/0*xu3GYMsWu9QiKNOo.png)","6f8854c9":"# Density-Based Clustering Algorithms\n> #### Clustering analysis is an unsupervised learning method that separates the data points into several specific bunches or groups, such that the data points in the same groups have similar properties and data points in different groups have different properties in some sense.\n> #### Density-Based Clustering refers to unsupervised learning methods that identify distinctive groups\/clusters in the data, based on the idea that a cluster in data space is a contiguous region of high point density, separated from other such clusters by contiguous regions of low point density.\n> #### Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a base algorithm for density-based clustering. It can discover clusters of different shapes and sizes from a large amount of data, which is containing noise and outliers.\n## Steps in DBSCAN\n> - #### The algorithm proceeds by arbitrarily picking up a point in the dataset (until all points have been visited).\n> - #### If there are at least \u2018minPoint\u2019 points within a radius of \u2018\u03b5\u2019 to the point then we consider all these points to be part of the same cluster.\n> - #### The clusters are then expanded by recursively repeating the neighborhood calculation for each neighboring point\n![](https:\/\/miro.medium.com\/proxy\/1*tc8UF-h0nQqUfLC8-0uInQ.gif)"}}