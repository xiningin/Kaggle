{"cell_type":{"4677a395":"code","83b9733f":"code","c4168a03":"code","14b111e4":"code","676874e8":"code","50814c1f":"code","47898498":"code","3ad96077":"code","1950c9d1":"code","34933d96":"code","f38431b4":"code","64c9cbf9":"code","345b93ad":"code","62d46691":"code","2540e644":"code","ef5e609f":"code","af36553d":"code","c60e8a63":"code","4bc0ba57":"code","0b6484a9":"code","df310d05":"code","5d847b2e":"code","a56f7924":"code","6cec00a2":"markdown","4c72355d":"markdown","b9bef8b0":"markdown","bbf5bc66":"markdown","498c3514":"markdown","8cf91c28":"markdown","b06dae92":"markdown","6d758e7e":"markdown","2527600f":"markdown","71754f35":"markdown","0fd0733a":"markdown","cfc459e0":"markdown","958eb4cf":"markdown","150bd75e":"markdown","8cf9bd1a":"markdown","ba4b180d":"markdown","a3f5968e":"markdown"},"source":{"4677a395":"!pip install keras==2.2.4 # critical dependency, ","83b9733f":"!pip freeze > kaggle_image_requirements.txt","c4168a03":"# Import neural network libraries\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom keras import backend as K\nimport keras.layers as layers\nfrom keras.models import Model, load_model\nfrom keras.engine import Layer\n\n# Initialize tensorflow\/keras session\nsess = tf.Session()\nK.set_session(sess)","14b111e4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\nfilepath = \"..\/input\/enron-email-dataset\/emails.csv\"\n\n# Read the data into a pandas dataframe called emails\nemails = pd.read_csv(filepath)\n\nprint(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0], emails.shape[1]))\nprint(emails.head())","676874e8":"# take a closer look at the first email\nprint(emails.loc[0][\"message\"])","50814c1f":"import email\n\ndef extract_messages(df):\n    messages = []\n    for item in df[\"message\"]:\n        # Return a message object structure from a string\n        e = email.message_from_string(item)    \n        # get message body  \n        message_body = e.get_payload()\n        messages.append(message_body)\n    print(\"Successfully retrieved message body from e-mails!\")\n    return messages\n\nbodies = extract_messages(emails)","47898498":"# extract random 10000 enron email bodies for building dataset\nimport random\nbodies_df = pd.DataFrame(random.sample(bodies, 10000))\n\n# expand default pandas display options to make emails more clearly visible when printed\npd.set_option('display.max_colwidth', 300)\n\nbodies_df.head() # you could do print(bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames","3ad96077":"#messages = emails[\"message\"].apply(email.message_from_string)\n#bodies_df = messages.apply(lambda x: x.get_payload()).sample(10000)","1950c9d1":"filepath = \"..\/input\/fraudulent-email-corpus\/fradulent_emails.txt\"\nwith open(filepath, 'r',encoding=\"latin1\") as file:\n    data = file.read()\n    \n# split on a code word appearing close to the beginning of each email\nfraud_emails = data.split(\"From r\")\n\nprint(\"Successfully loaded {} spam emails!\".format(len(fraud_emails)))","34933d96":"fraud_bodies = extract_messages(pd.DataFrame(fraud_emails,columns=[\"message\"],dtype=str))\nfraud_bodies_df = pd.DataFrame(fraud_bodies[1:])\n\nfraud_bodies_df.head() # you could do print(fraud_bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames","f38431b4":"Nsamp =1000 # number of samples to generate in each class - 'spam', 'not spam'\nmaxtokens = 200 # the maximum number of tokens per document\nmaxtokenlen = 100 # the maximum length of each token","64c9cbf9":"def tokenize(row):\n    if row is None or row is '':\n        tokens = \"\"\n    else:\n        tokens = str(row).split(\" \")[:maxtokens]\n    return tokens","345b93ad":"import re\n\ndef reg_expressions(row):\n    tokens = []\n    try:\n        for token in row:\n            token = token.lower()\n            token = re.sub(r'[\\W\\d]', \"\", token)\n            token = token[:maxtokenlen] # truncate token\n            tokens.append(token)\n    except:\n        token = \"\"\n        tokens.append(token)\n    return tokens","62d46691":"import nltk\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')    \n\ndef stop_word_removal(row):\n    token = [token for token in row if token not in stopwords]\n    token = filter(None, token)\n    return token","2540e644":"import random\n\n# Convert everything to lower-case, truncate to maxtokens and truncate each token to maxtokenlen\nEnronEmails = bodies_df.iloc[:,0].apply(tokenize)\nEnronEmails = EnronEmails.apply(stop_word_removal)\nEnronEmails = EnronEmails.apply(reg_expressions)\nEnronEmails = EnronEmails.sample(Nsamp)\n\nSpamEmails = fraud_bodies_df.iloc[:,0].apply(tokenize)\nSpamEmails = SpamEmails.apply(stop_word_removal)\nSpamEmails = SpamEmails.apply(reg_expressions)\nSpamEmails = SpamEmails.sample(Nsamp)\n\nraw_data = pd.concat([SpamEmails,EnronEmails], axis=0).values","ef5e609f":"print(\"Shape of combined data is:\")\nprint(raw_data.shape)\nprint(\"Data represented as numpy array is:\")\nprint(raw_data)\n\n# corresponding labels\nCategories = ['spam','notspam']\nheader = ([1]*Nsamp)\nheader.extend(([0]*Nsamp))","af36553d":"# function for shuffling data\ndef unison_shuffle(a, b):\n    p = np.random.permutation(len(b))\n    data = a[p]\n    header = np.asarray(b)[p]\n    return data, header\n\n# function for converting data into the right format, due to the difference in required format from sklearn models\n# we expect a single string per email here, versus a list of tokens for the sklearn models previously explored\ndef convert_data(raw_data,header):\n    converted_data, labels = [], []\n    for i in range(raw_data.shape[0]):\n        # combine list of tokens representing each email into single string\n        out = ' '.join(raw_data[i])\n        converted_data.append(out)\n        labels.append(header[i])\n        #print(i)\n    converted_data = np.array(converted_data, dtype=object)[:, np.newaxis]\n    return converted_data, np.array(labels)\n\nraw_data, header = unison_shuffle(raw_data, header)\n\n# split into independent 70% training and 30% testing sets\nidx = int(0.7*raw_data.shape[0])\n# 70% of data for training\ntrain_x, train_y = convert_data(raw_data[:idx],header[:idx])\n# remaining 30% for testing\ntest_x, test_y = convert_data(raw_data[idx:],header[idx:])\n\nprint(\"train_x\/train_y list details, to make sure it is of the right form:\")\nprint(len(train_x))\nprint(train_x)\nprint(train_y[:5])\nprint(train_y.shape)","c60e8a63":"class ElmoEmbeddingLayer(Layer):\n    def __init__(self, **kwargs):\n        self.dimensions = 1024 # initialize output dimension of ELMo embedding\n        self.trainable=True\n        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape): # function for building ELMo embedding\n        self.elmo = hub.Module('https:\/\/tfhub.dev\/google\/elmo\/2', trainable=self.trainable,\n                               name=\"{}_module\".format(self.name)) # download pretrained ELMo model\n        \n        # extract trainable parameters, which are only a small subset of the total - this is a constraint of\n        # the tf hub module as shared by the authors - see https:\/\/tfhub.dev\/google\/elmo\/2\n        # the trainable parameters are 4 scalar weights on the sum of the outputs of ELMo layers \n        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module\/.*\".format(self.name))\n        super(ElmoEmbeddingLayer, self).build(input_shape)\n\n    def call(self, x, mask=None): # specify function for calling embedding\n        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n                      as_dict=True,\n                      signature='default',\n                      )['default']\n        return result\n\n    def compute_output_shape(self, input_shape): # specify output shape\n        return (input_shape[0], self.dimensions)","4bc0ba57":"# Function to build overall model\ndef build_model():\n    input_text = layers.Input(shape=(1,), dtype=\"string\")\n    embedding = ElmoEmbeddingLayer()(input_text)\n    dense = layers.Dense(256, activation='relu')(embedding)\n    pred = layers.Dense(1, activation='sigmoid')(dense) # we could use sigmoid activation as well, but we choose softmax\n                                                        # to enable us use sparse_categorical_crossentropy and \n                                                        # sparse_categorical_accuracy below\n    \n    model = Model(inputs=[input_text], outputs=pred)\n    # use sparse_categorical_crossentropy and sparse_categorical_accuracy do avoid having to\n    # one-hot encode the labels\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    model.summary()\n    \n    return model","0b6484a9":"# Build and fit\nmodel = build_model()\nhistory = model.fit(train_x, \n          train_y,\n          validation_data=(test_x, test_y),\n          epochs=5,\n          batch_size=32)","df310d05":"model.save('ELMoModel.h5')\n","5d847b2e":"import matplotlib.pyplot as plt\n\ndf_history = pd.DataFrame(history.history)\n\nfig,ax = plt.subplots()\nplt.plot(range(df_history.shape[0]),df_history['val_acc'],'bs--',label='validation')\nplt.plot(range(df_history.shape[0]),df_history['acc'],'r^--',label='training')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('ELMo Email Classification Training')\nplt.legend(loc='best')\nplt.grid()\nplt.show()\n# Save figures\nfig.savefig('ELMoConvergence.eps', format='eps')\nfig.savefig('ELMoConvergence.pdf', format='pdf')\nfig.savefig('ELMoConvergence.png', format='png')\nfig.savefig('ELMoConvergence.svg', format='svg')","a56f7924":"from IPython.display import HTML\ndef create_download_link(title = \"Download file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(filename='ELMoConvergence.svg')","6cec00a2":"We now use the custom TF hub ELMo embedding layer within a higher-level function to define the overall model. \nMore specifically, we put a dense trainable layer of output dimension 256 on top of the ELMo embedding. ","4c72355d":"The following (commented out) code is arguably the more \"pythonic\" way of achieving the extraction of bodies from messages. It is only 2 lines long and achieves the same result. However, we feel the code above is more transparent with regards to how the processing is carried out, and as such leave this here for the python experts if they prefer.","b9bef8b0":"**Visualize Convergence**","bbf5bc66":"**Make figures downloadable to local system in interactive mode**","498c3514":"# Read and Preprocess Enron dataset\nRead Enron dataset and get a sense for the data by printing sample messages to screen","8cf91c28":"# Define Tokenization, Stop-word and Punctuation Removal Functions\nBefore proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters","b06dae92":"**Save trained model**","6d758e7e":"# Build, Train and Evaluate ELMo Model\nCreate a custom tf hub ELMO embedding layer","2527600f":"**Stop-word removal**\n\nLet\u2019s define a function to remove stopwords - words that occur so frequently in language that they offer no useful information for classification. This includes words such as \u201cthe\u201d and \u201care\u201d, and the popular library NLTK provides a heavily-used list that will employ.","71754f35":"# Read and Preprocess Fraudulent \"419\" Email Corpus","0fd0733a":"**Tokenization**","cfc459e0":"# WARNING\n**Please make sure to \"COPY AND EDIT NOTEBOOK\" to use compatible library dependencies! DO NOT CREATE A NEW NOTEBOOK AND COPY+PASTE THE CODE - this will use latest Kaggle dependencies at the time you do that, and the code will need to be modified to make it work. Also make sure internet connectivity is enabled on your notebook**","958eb4cf":"Write requirements to file, anytime you run it, in case you have to go back and recover dependencies.\n\nLatest known such requirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https:\/\/github.com\/azunre\/transfer-learning-for-nlp","150bd75e":"# Preliminaries\nFirst install a critical dependency for our code. **NOTE THAT THIS NOTEBOOK USES TENSORFLOW 1.14 BECAUSE ELMo WAS NOT PORTED TO TENSORFLOW 2.X AT THE TIME OF DEVELOPMENT. You can confirm if that is still the case now by going to https:\/\/tfhub.dev\/s?q=elmo To see equivalent Tensorflow 2.X BERT Code, see https:\/\/www.kaggle.com\/azunre\/tlfornlp-chapters2-3-spam-bert-tf2**","8cf9bd1a":"**Use regular expressions to remove unnecessary characters**\n\nNext, we define a function to remove punctuation marks and other nonword characters (using regular expressions) from the emails with the help of the ubiquitous python regex library. In the same step, we truncate all tokens to hyperparameter maxtokenlen defined above.","ba4b180d":"# Putting It All Together To Assemble Dataset\n\nNow, putting all the preprocessing steps together we assemble our dataset...","a3f5968e":"Separate headers from the message bodies"}}