{"cell_type":{"bfe30500":"code","99d1dde3":"code","d17a4fb1":"code","143b6315":"code","c2bb309a":"code","1ced7ba8":"code","a9acd84d":"code","c7dfce6f":"code","f3df9b53":"code","89b919ac":"code","ce66f663":"code","ab99b038":"code","d151f0df":"code","ea014fce":"code","0cc918e8":"code","228e4b87":"code","5676603e":"code","bd578848":"code","ef471922":"code","771df638":"code","75446f65":"code","e76d742a":"code","ac95afeb":"code","147fde49":"code","fe2baba0":"code","22b01528":"code","5486baec":"markdown","94248c1f":"markdown","4c457d3f":"markdown","fc6d8c52":"markdown","e9a4556e":"markdown","2465c0e1":"markdown","6dfc477c":"markdown","8dc9dd38":"markdown","91940e09":"markdown","e4629863":"markdown","ab3501b7":"markdown","0ff555d3":"markdown","b4942819":"markdown","2b0f0964":"markdown","9b1c0103":"markdown","a82146dd":"markdown","1076d5f3":"markdown","7bbe4e73":"markdown","7a39247e":"markdown","bb3e942a":"markdown","7f3af1da":"markdown","a201918d":"markdown","bbafa28e":"markdown","0841cdc7":"markdown","804ac674":"markdown","9f7d929a":"markdown","3a976a7a":"markdown","a433e8fa":"markdown","db9f3014":"markdown","5748e228":"markdown","5651318f":"markdown","9270a75d":"markdown"},"source":{"bfe30500":"def visualize_input(img, ax):\n    ax.imshow(img, cmap='gray')\n    width, height = img.shape\n    thresh = img.max()\/2.5\n    for x in range(width):\n        for y in range(height):\n            ax.annotate(str(round(img[x][y],2)), xy=(y,x),\n                        horizontalalignment='center',\n                        verticalalignment='center',\n                        color='white' if img[x][y]<thresh else 'black')","99d1dde3":"from tensorflow import keras\n\n(imagenes_entrenamiento, etiquetas_entrenamiento), (imagenes_test, etiquetas_test) = keras.datasets.mnist.load_data()\n","d17a4fb1":"keras.datasets.mnist.load_data()","143b6315":"print(f'N\u00famero que representa la matriz matriz: {etiquetas_entrenamiento[1]}')\nprint(f'Cada elemento corresponde a un pixel: \\n {imagenes_entrenamiento[1]}')","c2bb309a":"imagenes_entrenamiento[0].shape","1ced7ba8":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (12,12)) \nax = fig.add_subplot(111)\nax.set_yticklabels([])\nax.set_xticklabels([])\nplt.axis('off')\nvisualize_input(imagenes_entrenamiento[1], ax)","a9acd84d":"imagenes_entrenamiento = imagenes_entrenamiento\/255.0\nimagenes_test = imagenes_test\/255.0","c7dfce6f":"red_neuronal = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"sigmoid\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])","f3df9b53":"red_neuronal.compile(loss='sparse_categorical_crossentropy',\n              optimizer=keras.optimizers.SGD(lr=0.1),\n              metrics=['accuracy'])","89b919ac":"early_stopping = keras.callbacks.EarlyStopping()\nred_neuronal.fit(imagenes_entrenamiento, etiquetas_entrenamiento, epochs=10, validation_split=0.05, callbacks=[early_stopping])","ce66f663":"red_neuronal.evaluate(imagenes_test, etiquetas_test)","ab99b038":"red_neuronal_2 = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(200, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(10, activation=\"softmax\")\n])","d151f0df":"red_neuronal_2.compile(loss='sparse_categorical_crossentropy',\n              optimizer=keras.optimizers.SGD(lr=0.05),\n              metrics=['accuracy'])","ea014fce":"imagenes_entrenamiento = imagenes_entrenamiento\/255\nred_neuronal_2.fit(imagenes_entrenamiento, etiquetas_entrenamiento, epochs=10, validation_split=0.1, callbacks=[early_stopping])","0cc918e8":"red_neuronal_2.evaluate(imagenes_test\/255, etiquetas_test)","228e4b87":"cnn = keras.models.Sequential([\n        keras.layers.Conv2D(64, kernel_size=3, activation='relu', input_shape= (28,28, 1)),\n        keras.layers.MaxPooling2D(),\n        keras.layers.Flatten(),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(200, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(10, activation=\"softmax\")\n])","5676603e":"cnn.compile(optimizer=keras.optimizers.SGD(lr=0.05), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","bd578848":"imagenes_test = imagenes_test.reshape(10000, 28, 28, 1)\nimagenes_entrenamiento = imagenes_entrenamiento.reshape(60000, 28, 28, 1)\ncnn.fit(imagenes_entrenamiento, etiquetas_entrenamiento, epochs=30, callbacks=[early_stopping])","ef471922":"cnn.evaluate(imagenes_test, etiquetas_test)","771df638":"neural_network_on_steroids = keras.applications.resnet50.ResNet50(weights=\"imagenet\")","75446f65":"neural_network_on_steroids.summary()","e76d742a":"from urllib.request import urlopen, Request \nimport matplotlib.pyplot as plt\nheaders = {'User-Agent': 'Mozilla\/5.0 (Windows NT 6.1) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/41.0.2228.0 Safari\/537.3'}\nimagen_perro = plt.imread(urlopen(Request('https:\/\/cdn.pixabay.com\/photo\/2015\/11\/17\/13\/13\/dogue-de-bordeaux-1047521_1280.jpg', headers=headers)), format='jpg')\ncastillo = plt.imread(urlopen(Request('https:\/\/www.audioguiaroma.com\/imagenes\/castillo-san-angelo.jpg', headers=headers)), format='jpg')","ac95afeb":"import tensorflow as tf \nimport numpy as np\n\nimagen_perro_crop = tf.image.resize_with_pad(imagen_perro, 224, 224, antialias=True)\ncastillo_crop = tf.image.resize_with_pad(castillo, 224, 224, antialias=True)\nimagenes = keras.applications.resnet50.preprocess_input(np.array([imagen_perro_crop, castillo_crop]))","147fde49":"imagenes_test = imagenes_test.reshape(10000, 28, 28, 1);\nimagenes_entrenamiento = imagenes_entrenamiento.reshape(60000, 28, 28, 1);","fe2baba0":"pred = neural_network_on_steroids.predict(imagenes)","22b01528":"top_K = keras.applications.resnet50.decode_predictions(pred, top=3)\nfor image_index in range(2):\n    print(\"Image #{}\".format(image_index))\n    for class_id, name, y_proba in top_K[image_index]:\n        print(\"  {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100))\n    print()","5486baec":"# Redes neuronales artificiales\nEsta neurona biol\u00f3gica nos lleva a hablar de su hom\u00f3loga computacional, si, hablamos de neuronas artificiales. Precisamente el Deep Learning est\u00e1 fundamentado en el uso de redes neuronales artificales. Pero, \u00bfC\u00f3mo funcionan? Imaginemos que queremos contruir un clasificador de animales a partir de una foto, es decir le introducimos como entrada un foto de un animal y nos devuelve como salida el nombre de dicho animal. Para ello podemos utilizar una red neuronal. De una manera muy simple podemos definir una red neuronal como un modelo que recibe una entrada, en este caso una imagen, y nos devuelve una salida, es decir, nos dice si la imagen conten\u00eda un perro o un gato.","94248c1f":"# Gradient Descent \nGradient Descent (o Descenso de Gradiente) es un algoritmo que permite hallar el m\u00ednimo de una funci\u00f3n, que en nuestro caso ser\u00e1 la funci\u00f3n de coste o de p\u00e9rdida, que depende de los par\u00e1metros de la red y tiene como imagen el error para dichos par\u00e1metros. Un ejemplo de funci\u00f3n de coste podr\u00eda ser $\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} (y_i-\\hat{y_i})^{2}$ donde $y$ ser\u00eda el valor real e $\\hat{y}$ el valor predicho. El sumatorio recorre todos los ejemplos en nuestro conjunto de entrenamiento. \nImaginemos que nuestra predicci\u00f3n viene dada por una recta, es decir $\\hat{y} = \\theta_1 x + \\theta_0$, donde $\\theta_1$ denota la pendiente y $\\theta_0$ el t\u00e9rmino independiente. Podemos realizar la siguiente sustituci\u00f3n $\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} (y_i- \\theta_1 x_i - \\theta_0)^{2}$. Nuestro objetivo es encontrar los par\u00e1metros de $\\theta_0$ y $\\theta_1$ que minimizan esta funci\u00f3n de coste. Gradient Descent actualiza los par\u00e1metros en cada iteraci\u00f3n, rest\u00e1ndoles el gradiente de la funci\u00f3n, dado que sabemos que $\\nabla f(\\theta_0, \\theta_1)$ es la direcci\u00f3n de ascenso m\u00e1s pronunciado, de lo que sigue inmediatamente que $-\\nabla f(\\theta_0, \\theta_1)$ es la de descenso m\u00e1s pronunciado, por lo que ir en la direcci\u00f3n de -$\\nabla f(\\theta_0, \\theta_1)$ nos garantiza que en cada iteraci\u00f3n nos estaremos acercando al m\u00ednimo (aunque, por razones que no explicaremos, podr\u00eda darse el caso de que GD falle en la tarea de dar con el m\u00ednimo).\n","4c457d3f":"La primera tupla, **(imagenes_entrenamiento, etiquetas_entrenamiento)**, se reservar\u00e1 para entrenar a nuestra red neuronal, y la segunda, **(imagenes_test, etiquetas_test)**, para evaluar su porcentaje de acierto sin ser entrenada en esta evaluaci\u00f3n. Evidentemente, no debemos poner a prueba a la red con las mismos datos de entrenamiento; de otra forma, tratar\u00e1 con im\u00e1genes que ya ha estudiado.\n\nAntes de explicar la creaci\u00f3n de este cerebro electr\u00f3nico, explicaremos dos conceptos esenciales que intervienen en la modificaci\u00f3n de la red neuronal.","fc6d8c52":"# QUE ENCONTRAR\u00c1S EN ESTE NOTEBOOK \n* [\u00bfEs posible que una m\u00e1quina pueda imitar el comportamiento del cerebro humano?](#1)\n* [Neuronas biol\u00f3gicas](#2)\n* [Redes neuronales artificiales](#3)\n* [MNIST](#4)\n* [Gradient Descent](#5)\n* [Retropropagaci\u00f3n](#6)\n* [HANDS-ON DEEP LEARNING](#7)\n* [Redes Neuronales Convolucionales](#8)\n* [Modelos pre-entrenados](#9)\n* [Instalaci\u00f3n de TensorFlow](#10)\n* [A hombros de gigante](#11)","e9a4556e":"Vamos a ver que prediccion nos ofrece ResNet-50 para las siguientes imagenes\n<div class=\"row\">\n  <div class=\"column\">\n    <img src=\"https:\/\/www.audioguiaroma.com\/imagenes\/castillo-san-angelo.jpg\" style=\"width:100%\">\n  <\/div>\n  <div class=\"column\">\n    <img src=\"https:\/\/cdn.pixabay.com\/photo\/2015\/11\/17\/13\/13\/dogue-de-bordeaux-1047521_1280.jpg\" style=\"width:100%\">\n  <\/div>\n<\/div>","2465c0e1":"Esta es la arquitectura de **ResNet-50**","6dfc477c":"Cada imagen de MNIST tienen unas dimensiones de 28x28 pixeles, por lo que para cada imagen contamos con una matriz de 28 filas y 28 columnas. Lo que visulizamos debajo es una imagen que corresponde a un 0 y cada p\u00edxel viene identificado con su escala de gris, la cual va desde 0 hasta 255.","8dc9dd38":"# \u00bfEs posible que una m\u00e1quina pueda imitar el comportamiento del cerebro humano?\n![](https:\/\/cdn.pixabay.com\/photo\/2014\/04\/09\/17\/48\/man-320274_960_720.png)\nSeguramente cuando pensamos en m\u00e1quinas comport\u00e1ndose de manera inteligente no podemos evitar pensar en Inteligencia Artificial(IA), y nuestra intuici\u00f3n es correcta ya que la IA es un campo que busca que las computadoras se comporten de manera inteligente. Con computadores compart\u00e1ndose de manera inteligente no nos referimos a robots del futuro que han venido a destruir la humanidad, nos referimos a una realidad que lleva muchas d\u00e9cadas de estudio. Desde cosas tan cotidianas como vuestro filtro de spam, Siri, el traductor de Google o recomendaciones de videos en YouTube hasta coches aut\u00f3nomos son ejemplos de m\u00e1quinas comport\u00e1ndose de manera inteligente. \u00bfPor qu\u00e9? Porque dichas m\u00e1quina contienen una serie de algoritmos que les hacen comportarse de esa manera, queremos decir que cuando te llega un email y se clasifica en spam, y no en tu bandeja de entrada, no es porque haya una persona que mira cada uno de tus emails y los clasifica, si no que hay un algoritmo que a trav\u00e9s de unos datos de entrada, como el emisor del correo, las palabras que hay en el correo, etc... clasifica entre spam y no spam. Es evidente que si el correo contiene palabras como 'descuento', 'compra', 'oferta' es probable que estemos hablando de spam. Visto esto podemos afirmar que tu filtro de spam esta exhibiendo un comportamiento inteligente.\n\nUno de los subcampos de la IA es el Machine Learning el cual busca darle a un programa las herramientas necesarias para que pueda aprender. Basicamente, cuando hablamos de ML estamos hablando de crear un modelo que intente predecir la realidad. \u00bfC\u00f3mo llegamos a ese modelo?. Se suele decir que los datos son el oro del siglo XXI, y efectivamente utilizamos datos para 'entrenar' nuestro modelo. Cuando nos referimos a entrenar estamos haciendo alusi\u00f3n al proceso a partir del cual un computadora aprende a partir de datos como comportarse cuando se encuentre en nuevas situaciones con nuevos datos.\n\nEn el caso de un filtrador de spam, los datos con los que entrenamos nuestro modelo son los ya comentados remitente, palabras...\n\nAlgoritmos de Machine Learning hay muchos, en este notebook vamos a ver como resuelven este tipo de problemas los algoritmos de Deep Learning.\n\n### Neuronas biol\u00f3gicas\nEl ser humano ha basado alguna de sus invenciones en la naturaleza, \u00bfPor qu\u00e9 no hacerlo para conseguir un comportamiento inteligente? Cuando pensamos en el cerebro humano seguramente se nos venga a la cabeza las decenas de miles de millones de neuronas que este tiene. Estas neuronas, como vemos en la imagen inferior, cuenta con cuerpo celular, dentritas y una larga extensi\u00f3n llamada ax\u00f3n. Estas se comunican a trav\u00e9s de se\u00f1ales el\u00e9cticas que se producen desde el ax\u00f3n de unas, y son recibidas por las dendritas de otras.\n![](https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/assets\/mls2_1001.png)\nSi estuviesemos interesados en conseguir un comportamiento inteligente mirar al cerebro humano como inspiraci\u00f3n parece una buena idea.","91940e09":"### Improving our model ","e4629863":"# Redes Neuronales Convolucionales(CNN)\n\n<div style=\"text-align: justify\">La arquitectura de red neuronal que hemos visto hasta ahora funcionaba bien con MNIST, tomando en cuenta que est\u00e1bamos tratando im\u00e1genes en blanco y negro, y su resoluci\u00f3n, 28px x 28px, no era gran cosa. Esto resultaba en una capa de entrada de 784 neuronas, lo cual no era demasiado, con una hipot\u00e9tica segunda capa de 200 neuronas, obtendr\u00edamos casi 160000 par\u00e1metros, solo en la primera capa. Este n\u00famero de neuronas, no resulta muy demandante a nivel computacional.\nSin embargo, cuando pasamos a imagenes en color, y con mayor resoluci\u00f3n, nuestra red neuronal empieza a flaquear, debido a la gran dimensionalidad de la capa de entrada.\n![](https:\/\/www.audioguiaroma.com\/imagenes\/castillo-san-angelo.jpg)\nEsta imagen tiene 1920px de ancho y 1079px de alto, a lo que habr\u00eda que sumarle los 3 canales de color por cada pixel, en total, tendriamos una primera capa de entrada con m\u00e1s de 6 millones de neuronas, suponiendo una primera capa oculta de 200 neuronas, tendriamos mil millones de par\u00e1metros solo en la primera capa. Una Red Neuronal Convolucional soluciona esto usando capas parcialmente conectadas y par\u00e1metros(pesos) compartidos. \n\nDesventajas de una Red Neuronal com\u00fan frente a una convolucional:\n* Los inputs son independientes, en el caso de una imagen, se asume que cada p\u00edxel es independiente del resto, lo cual puede no ser adecuado, si estamos trabajando con una imagen que contiene una cara, los pixeles que est\u00e1n alrededor del ojo, tienen cierta relaci\u00f3n, es decir, forman un patr\u00f3n que est\u00e1 presente en todas las caras humanas. Y esta es la suposici\u00f3n a la hora de formar una red neuronal convolucional. Los p\u00edxeles de la capa de entrada (el array contenedor de la imagen) que est\u00e1n cercanos entre ellos tienen un relaci\u00f3n.\n![](https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/assets\/mls2_1401.png)\n* Otro factor importante, es que una red neuronal com\u00fan, una vez aprende a reconocer un patr\u00f3n en una determinada localizaci\u00f3n, s\u00f3lo puede reconocerla en dicha localizaci\u00f3n.\n\nEl bloque fundamental en una CNN es la capa convolucional, donde a la imagen de entrada se le aplican convoluciones, podemos ver debajo que las neuronas de entrada de una capa convolucional no est\u00e1n conectadas a todas las neuronas de la segunda, si no \u00fanicamente a ciertas neuronas en un determinado rango. Ocurre lo mismo con las neuronas de la tercera capa convolucional con respecto a la segunda.\n\n![](https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/assets\/mls2_1402.png)\n\nLas convoluciones nos permiten capturar caracter\u00edsticas de una imagen, aplicando filtros. Tomando como ejemplo esta imagen, vamos a explorar estos filtros:![](https:\/\/codelabs.developers.google.com\/codelabs\/tensorflow-lab3-convolutions\/img\/e13b3ec8029cbc52.png)\nEl filtro en la siguiente imagen, es la matriz roja que vemos, en este caso nos permite resaltar las l\u00edneas verticales:![](https:\/\/codelabs.developers.google.com\/codelabs\/tensorflow-lab3-convolutions\/img\/a6a65d1a58c44e39.png)\nEste filtro resalta las l\u00edneas horizontales:\n![](https:\/\/codelabs.developers.google.com\/codelabs\/tensorflow-lab3-convolutions\/img\/e1f63b004a901122.png)\nEl proceso en el que aplicamos un filtro a una imagen, es la convoluci\u00f3n. Los filtros que se aplican se pueden tratar como par\u00e1metros, por lo que pueden ser aprendidos en el proceso de entrenamiento.\n\nEl otro bloque que construye una red neuronal convolucional es la capa de pooling, como en la capa convolucional las neuronas de una capa s\u00f3lo est\u00e1n relacionadas con algunas neuronas de la capa anterior:\n![](https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/assets\/mls2_1408.png)\nLa capa de pooling consiste en reducir la informaci\u00f3n sin reducir las caracter\u00edsticas presentes en nuestros datos. Un ejemplo ser\u00eda el aplicado en la imagen de arriba, donde cogemos el n\u00famero m\u00e1s grande en un determinado rango.\n\nUna vez aplicadas las operaciones de convoluci\u00f3n y pooling pasamos nuestras neuronas a una red totalmente conectado como la que vimos con MNIST, esta ser\u00eda un arquitectura de una red neuronal convolucional:\n![](https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/assets\/mls2_1411.png)\n","ab3501b7":"El m\u00e9todo \"load_data\" nos permite cargar los datos de la base. La asignaci\u00f3n de esas dos tuplas que vemos arriba puede resultar confusa, vamos a explicarlo brevemente.","0ff555d3":"![](https:\/\/miro.medium.com\/max\/2482\/0*xqJA1mCMLc7b64H1.png)","b4942819":"![image.png](attachment:image.png)","2b0f0964":"Tras a\u00f1adir alguna capa oculta y el m\u00e9todo $BatchNormalization$, el cu\u00e1l transforma los valores de la funci\u00f3n de activaci\u00f3n de tal forma que la media se mantenga pr\u00f3xima a 0 y la desviaci\u00f3n t\u00edpica a 1, nuestra red neuronal mejorar\u00e1 considerablemente","9b1c0103":"Las imagenes est\u00e1n albergadas en un array de 3 dimensiones(ancho, alto, canales de color)","a82146dd":"Finalmente, ponemos a prueba nuestra red mediante los datos de test.","1076d5f3":"Primero de todo,vamos a ver qu\u00e9 es realmente una red neuronal. Una red neuronal artificial b\u00e1sica se compone de tres capas, una de entrada, una oculta y una de salida, veremos, que puede haber m\u00faltiples capas ocultas, tantas como de compleja queremos que sea nuestra red, una capa es un conjunto de neuronas cuyas entradas provienen de una capa anterior (o de los datos de entrada en el caso de la primera capa) y cuyas salidas son la entrada de una capa posterior. En la capa de entrada las neuronas reciben los datos que el usuario le proporciona, ya sean im\u00e1genes etc... las capas ocultas son las que se encargan de realizar todo el aprendizaje reduciendo en cada iteraci\u00f3n su error a la hora de decidir, por ejemplo, si la foto se trata de un perro o un gato. Por \u00faltimo, la capa de salida consta de el \"output\", es decir, la que nos va a indicar de que se trata la imagen, en el caso de que los datos de entrada sean una imagen","7bbe4e73":"![](https:\/\/miro.medium.com\/max\/2000\/1*bhFifratH9DjKqMBTeQG5A.gif)","7a39247e":"# Modelos pre-entrenados\nTensorFlow nos da la oportunidad de descargar modelos ya entrenados, en el modulo **keras.applications**, en este caso estamos seleccionando como modelo **ResNet-50**, el cual ha sido entrenado en el conjunto de datos [**ImageNet**](http:\/\/www.image-net.org\/), este dataset contiene 14 millones de imagenes etiquetadas, las cuales ocupan 14GB!!\nMNIST tiene 10 clases, los numeros del 0 al 9, **ImageNet** cuenta con 1000 CLASES!!\n\n<center><img src=\"https:\/\/miro.medium.com\/max\/3840\/1*YIlwSt98Uu8SwssxYEWSHQ.jpeg\"><\/center>\n      <center>ImageNet Dataset<\/center>","bb3e942a":"Desde luego, una red digna de admiraci\u00f3n.","7f3af1da":"Para terminar de configurar el modelo, habemos de especificar una funci\u00f3n de p\u00e9rdida, otra de optimizaci\u00f3n (en este caso, Descenso de Gradiente Estoc\u00e1stico) y un tipo de informaci\u00f3n (que la red no tendr\u00e1 en cuenta, pero al humano le resultar\u00e1 conveniente), como puede ser el ratio de salidas acertadas y totales, que viene indicado por 'accuracy' o precisi\u00f3n en espa\u00f1ol.","a201918d":"Sequential$ es un m\u00e9todo que permite crear $redneuronal$ mediante la definici\u00f3n de cada capa. $Flatten$ se encarga de \"desenrollar\" la matriz, cuyo tama\u00f1o indicamos en el par\u00e1metro \"input_shape\", esto es, tomar sus elementos e insertarlos en un vector (por ello mismo $Flatten$ define la primera capa) y $Dense$ crea capas seg\u00fan el n\u00famero de neuronas y funci\u00f3n de activaci\u00f3n requeridos.","bbafa28e":"# MNIST\nVamos a ver m\u00e1s a fondo como funciona una red neuronal, para ello vamos a usar la API de TensorFlow Keras, una API es un programa con nos permite interactuar con otro simplificando su uso, esto quiere que decir que Keras nos permite trabajar con TensorFlow de una manera m\u00e1s sencilla. Cargamos el conjunto de datos MNIST, este conjunto cuanta con 70000 imagenes de n\u00fameros manuscritos, para cada imagen tenemos una matriz donde cada elemento es un pixel de la imagen.","0841cdc7":"# INSTALACI\u00d3N DE TENSORFLOW\n\n**\u00bfTensorFlow GPU o TensorFlow CPU?** Si vas a instalar TensorFlow en tu ordenador tienes la posibilidad de escoger su versi\u00f3n GPU o CPU, si eres a\u00fan principiante ser\u00eda recomendable instalar TensorFlow CPU, puedes aprender como hacerlo [aqu\u00ed](https:\/\/medium.com\/analytics-vidhya\/install-tensorflow-2-0-along-with-all-packages-on-anaconda-for-windows-10-and-ubuntu-86a89ba51983)\n\n**\u00bfQu\u00e9 es una CPU?** es un procesador de tipo general, esto quiere decir que ejecuta todo tipo de comandos en una computadora, al no tener especifidad resulta menos eficiente realizando las operaciones matem\u00e1ticas necesarias para desarrollar un modelo de Deep Learning.\n\n**\u00bfQu\u00e9 es una GPU?** es un procesador de tipo espec\u00edfico, que se caracteriza por tener una gran cantidad de n\u00facleos, lo cual facilita la ejecuci\u00f3n de comandos en paralelo.\n<img src=\"https:\/\/s3.amazonaws.com\/quantstartmedia\/images\/qs-valerio-mat-mat-fig1.png\" alt=\"drawing\" width=\"200\"\/>\nComo veis podemos paralelizar la multiplicaci\u00f3n de una matriz, en lugar de calcular la matriz resultado elemento por elemento, podemos calcular de manera sim\u00faltanea los 4 elementos asignando un procesador a cada elemento. Esta es de manera general la idea detr\u00e1s de usar GPU's en Deep Learning\n\nSi no quieres instalar TensorFlow en tu ordenador, puedes usar servicios como **Google Colab o Kaggle**, donde dispones de un limitado uso de GPU's y CPU's.  \n","804ac674":"Estos datos conforman dos tuplas, cada una de ellas contienendo un array de tres dimensiones, donde cada imagen est\u00e1 almacenada en una matriz; y un vector, donde cada elemento es el n\u00famero que corresponde a dicha matriz.","9f7d929a":"# A hombros de gigante\nQueremos dar las gracias a la vez que recomendar las siguientes fuentes que nos han servido de gran inspiraci\u00f3n para elaborar este notebook:\n* [Andrew NG Standford Machine Learning](https:\/\/www.coursera.org\/learn\/machine-learning?skipBrowseRedirect=true)\n* [Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow](https:\/\/www.amazon.es\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=dp_ob_image_bk)\n* [3Blue1Brown Deep Learning(MUY RECOMENDABLE)](https:\/\/www.youtube.com\/playlist?list=PLd7R6J6iS6r4sITNFQ3ZMvj7BzWqTXfIe)\n* [MIT Introduction to Deep Learning](http:\/\/introtodeeplearning.com\/)\n* [Dot CSV Aprendiendo Inteligencia Artificial(ESPA\u00d1OL)](https:\/\/www.youtube.com\/playlist?list=PL-Ogd76BhmcDxef4liOGXGXLL-4h65bs4)","3a976a7a":"# HANDS-ON DEEP LEARNING\nAhora estamos listos para la apasionante tarea de creaci\u00f3n de la red neuronal.","a433e8fa":"Este notebook ha sido creado con el objetivo de dar una primera toma de contacto con el mundo del Deep Learning, aportando un poco de historia y motivaci\u00f3n de las redes neuronales, intentando otorgar un suficiente conocimiento del perceptr\u00f3n de un capa y multicapa, y describiendo brevemente arquitecturas m\u00e1s complejas.","db9f3014":"Ahora que la red ha sido creada, debemos entrenarla. Este proceso lo indicamos mediante el m\u00e9todo $fit$, en el que especificamos los datos de entrenamiento y el n\u00famero de iteraciones o epochs sobre dichos datos. Adicionalmente, se han incluido \"validation_split\", que excluye de la fase de entrenamiento un 5% de los datos, escogidos del final del conjunto de entrenamiento, para realizar peque\u00f1as pruebas de la red al final de cada epoch; y \"callbacks\", en el que escogemos un callback, propiamente dicho, en este caso, \"early stopping\", que detendr\u00e1 el entrenamiento si \u00e9ste no va a mejor, en otras palabras, si la funci\u00f3n de p\u00e9rdida empieza a incrementar su valor.\n\nEn cuanto a las salidas, obs\u00e9rvese que obtenemos informaci\u00f3n como la iteraci\u00f3n actual, el tiempo en completarla y el valor de la funci\u00f3n de p\u00e9rdida y la precisi\u00f3n, tanto para el conjunto de entrenamiento como para el de validaci\u00f3n (\"val_loss\" y \"val_accuracy\").","5748e228":"# DEEP LEARNING CRASH COURSE CON KERAS Y TENSORFLOW","5651318f":"Ahora, vamos a explicar brevemente el funcionamiento de una red neuronal, para ello vamos a explicar como funciona una sola neurona de la red. Una neurona recibe est\u00e1 conectada a todas las neuronas de las capas anterior y posterior, y \u00e9sta puede activarse y por consecuencia trasladar la informaci\u00f3n a la siguiente capa, o no activarse. Esa decisi\u00f3n la \"toma\" la llamada funci\u00f3n de activaci\u00f3n, existen m\u00faltiples funciones de activaci\u00f3n pero antes de ver qu\u00e9 es lo que hace esta funci\u00f3n vamos a ver lo que contiene esta funci\u00f3n que provoca que la neurona se active, o por el contrario, no lo haga. Bien, cada neurona almacena un valor generalmente entre 0 y 1, vamos a denotar este valor como $a_{j}$ d\u00f3nde $j$ identifica la posici\u00f3n de la neurona en la capa actual, por otra parte, cada conexi\u00f3n guarda un peso $\\omega$, \u00e9sta neurona de la que estamos hablando ser\u00eda la neurona $j$ y cada neurona de la capa anterior asociada a \u00e9sta le traslada un producto $(a_{j}\\omega_{jk})$, donde $k$ marca la posici\u00f3n de la neurona de la capa anterior, por otro lado, cada neurona almacena un valor BIAS o sesgo $b$ el cu\u00e1l va a ayudar a que esa neurona se active antes que otras. Sabiendo \u00e9sto, el sumatorio $(\\displaystyle\\sum_{j=1}^n{a}^{(L)}_{j}\\omega_{jk}^{(L)})+b$, donde $n$ es el numero de neuronas en la capa $L$, ser\u00eda el valor con el que la funci\u00f3n de activaci\u00f3n debe decidir si activar a la neurona o no; muy simplificadamente, si el resultado del sumatorio supera un determinado valor, la funci\u00f3n activa la neurona. ","9270a75d":"# Retropropagaci\u00f3n\n\nLa distribuci\u00f3n correcta de pesos entre neuronas es un problema crucial. Una red neuronal es incapaz de determinar el resultado correcto si estos pesos son adjudicados aleatoriamente. Precisamente existe una elegante soluci\u00f3n para lidiar con ellos, que viene dada por el algoritmo de retropropagaci\u00f3n (o backpropagation en ingl\u00e9s). Cabe mencionar que tal algoritmo tiene una base matem\u00e1tica que, aunque interesante y desde luego esencial, dejaremos de lado para as\u00ed tomar un enfoque intuitivo.\n\nLa idea de la retropropagaci\u00f3n consiste en ajustar los par\u00e1metros de la red neuronal de acuerdo con la minimizaci\u00f3n de la funci\u00f3n de coste o p\u00e9rdida, de la que se puede encargar un optimizador como Descenso de Gradiente. Tal funci\u00f3n de p\u00e9rdida podr\u00eda ser, por ejemplo, la funci\u00f3n de error cuadr\u00e1tico. Como ya se ha mencionado, Retropropagaci\u00f3n y Descenso de Gradiente trabajan conjuntamente, puesto que Backpropagation proporcionar\u00e1 a GD el gradiente.\n\nSi nos fijamos en la \u00fatlima capa (la salida), observaremos que las activaciones de las neuronas no concuerdan con las activaciones debidas. Centr\u00e9monos en una neurona espec\u00edfica cuyo valor de activaci\u00f3n, $a^{(L)}_j$, correspondiente a la neurona $j$-\u00e9sima de la capa $L$, queremos incrementar, y que viene asociado a un peso $\\omega_{jk}^{(L)}$, el asociado a las neuronas $j$-\u00e9sima de la capa $L$ y $k$-\u00e9sima de la capa $L-1$. Para llevar a cabo esta tarea, consideraremos tres m\u00e9todos:\n- Cambio del sesgo (o bias, $b$): aunque trivial, es \u00fatil hacer uso de $b$ para incrementar o decrementar $a^{(L)}_j$.\n- Incremento del peso: es evidente que un mayor peso implicar\u00e1, de nuevo, un incremento de activaci\u00f3n. Adem\u00e1s, es importante notar que \u00e9ste se realizar\u00e1 en proporci\u00f3n con $a_j^{(L)}$, puesto que interesa aumentar m\u00e1s los pesos que est\u00e9n asociados a $a_j^{(L)}$ grandes, para obtener en definitiva un producto $a_j^{(L)}\\cdot\\omega_{jk}^{(L)}$ grande.\n- Modificaci\u00f3n de activaci\u00f3n: por supuesto, tambi\u00e9n querremos modificaciones de $a_k^{(L-1)}$, para influir en el valor de $a_j^{(L)}$. En este caso, interesa que cada activaci\u00f3n aumente o disminuya en funci\u00f3n de $\\omega_{jk}^{(L)}>0$  o  $\\omega_{jk}^{(L)}<0$, respectivamente, manteniendo siempre un cambio de $a_k^{(L-1)}$ en proporci\u00f3n con su $\\omega_{jk}^{(L)}$ asociado.\n\nPor supuesto, no podemos simplemente transformar $a_k^{(L-1)}$, como es el caso del \u00faltimo m\u00e9todo, pero s\u00ed podemos cambiar los pesos de la capa anterior. De esta forma, repitiendo el proceso para cada capa, se ir\u00e1n modificando los pesos, activaciones y sesgos, hasta retornar de nuevo a la primera. Por supuesto, semejante desarrollo habr\u00e1 de ser aplicado a cada neurona, completando el transcurso del algoritmo, que se aplicar\u00e1 a cada ejemplo de entrenamiento $X_i$.\n\nFinalmente, no hay que olvidar que cada ejemplo de entrenamiento producir\u00e1 cambios espec\u00edficos para que la red pueda reconocer, espec\u00edficamente, cada uno de ellos. Por ello, es esencial tener en cuenta todas las modificaciones \"propuestas\" por cada $X_i$ (si no, la red neuronal tender\u00e1 a reconocer todos los datos como el mismo). Esto se puede lograr f\u00e1cilmente si se computa la media del peso de cada neurona. \n"}}