{"cell_type":{"a03d2878":"code","34a78f0a":"code","206a1693":"code","e4016178":"code","893d5a8a":"code","1f527490":"code","808fb346":"code","400ea7cf":"code","aad927c9":"code","99f85fb5":"code","59ed3265":"code","b6c4c428":"code","621c1347":"code","80a2cb77":"code","fbc474f9":"code","599d314e":"code","67c1d7eb":"code","90aa173a":"code","49c3045c":"code","63a79aa1":"code","4d2e01aa":"code","e5b4db88":"code","fee7e13f":"code","d27817ec":"code","c00024cf":"code","23016345":"code","416b44ae":"code","fbfe4c2e":"code","7aac09c1":"code","ed093247":"code","505bdd1b":"code","dd667e12":"code","9ae2d5cb":"code","59e8241e":"code","6746bdb0":"code","08931ce1":"code","39c532fe":"code","2605a7b9":"code","2c4fb6c2":"code","9d1766cf":"code","1499f6d5":"code","c1082a00":"code","d7b139e3":"code","95861ea1":"code","1633ee30":"code","2c212fc7":"code","ba6404b6":"code","afb43fb4":"code","e9d4c726":"code","7094a73d":"code","5a3f2385":"code","2a199cc7":"code","0fd7e6db":"code","f420a556":"code","c260287e":"code","89ac5c82":"code","12d48e97":"code","5185c7de":"code","1768f8d3":"code","90bef1d7":"code","75ffacf6":"code","e60d0ede":"code","363953f7":"code","e662dea3":"code","36d5e6a5":"code","c598871a":"code","b62a520e":"code","75ec224c":"code","19200a6e":"code","74041105":"code","c0ca2cd6":"code","a43da15a":"code","a3616e18":"code","a1e526c2":"code","1693294c":"code","e1427dee":"code","27f4b9ec":"code","f9dea076":"code","1b46935d":"code","b182be93":"code","acb64331":"code","97b17aaa":"code","66f55991":"code","06e7b1c1":"code","897b883a":"code","536513ac":"code","c7e380d6":"code","1e48d978":"code","43328c40":"code","07c3a60a":"code","094af51a":"code","d132a36d":"code","e74f418d":"code","d02cd020":"code","36971747":"code","d0c43d43":"code","1a0c8a7b":"code","c033dca0":"code","8cbc5f36":"code","7bee786c":"code","a02be1d6":"code","31f155e1":"code","2e781ecb":"code","4567819c":"code","4cac7e5c":"code","c3dd0e1e":"code","7a98d52e":"code","8a8f456d":"code","fdf3d7df":"code","7113bfd3":"code","dffc046c":"code","84eed336":"code","d959fd05":"code","9423eda2":"markdown","289786e8":"markdown","8b29b248":"markdown","68bed9e5":"markdown","7a0605d5":"markdown","3a5b420f":"markdown","f0b343c0":"markdown","b546a864":"markdown","e40ed1b5":"markdown","a98cf011":"markdown","c4a03748":"markdown","9a8f64da":"markdown","aaf1ddc2":"markdown","ca78ba6c":"markdown","04083355":"markdown","c6a55e0a":"markdown","128a6c33":"markdown","054c1375":"markdown","9b48a82b":"markdown","118b2a7b":"markdown","53d73dda":"markdown","0e47a2c5":"markdown","1a118512":"markdown","93525acc":"markdown","f48cf037":"markdown","ba2a4618":"markdown","93aac939":"markdown","8c7d789f":"markdown","91c91ac8":"markdown","d6b9a3a2":"markdown","2a58d540":"markdown","5820794e":"markdown","3c29ae38":"markdown","cfe5a345":"markdown","e72c9585":"markdown","f9a7d4f8":"markdown","99725e31":"markdown","440964f1":"markdown","292cd680":"markdown","7f7eec2e":"markdown","b5aa0758":"markdown","e6394f5c":"markdown","f85e8b50":"markdown","71f28650":"markdown","b6f01da2":"markdown","efc241f1":"markdown","9cec2f70":"markdown","9f78526b":"markdown","818b9dd4":"markdown"},"source":{"a03d2878":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","34a78f0a":"### import necessary libraries\nfrom sklearn.model_selection import train_test_split # used for splitting training and testing data\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nimport sklearn.metrics as mt\nimport seaborn as sns # Seaborn visualization library\nimport matplotlib.pyplot as plt\nimport plotly.express as px #Plotly Express\nfrom plotly.offline import iplot\n#to link plotly to pandas\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline = False, world_readable = True)\n\nimport plotly.io as pio\npio.templates.default = 'plotly_white'\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter('ignore')","206a1693":"data = pd.read_csv('\/kaggle\/input\/machine-learning-24-hrs-hackathon\/train_SJC.csv',header = 1) # to read the train dataset \ntest = pd.read_csv('\/kaggle\/input\/machine-learning-24-hrs-hackathon\/Test_SJC.csv') #to read the test dataset \nsample = pd.read_csv('\/kaggle\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv') #to read the sample submission ","e4016178":"## check first 5 rows of train dataset\ndata.head()","893d5a8a":"# rename unnamed column with proper column name mentioned in dataset\ndata.rename( columns={'Unnamed: 2':'DateReported','Unnamed: 7':'DependentsOther', 'Unnamed: 11':'DaysWorkedPerWeek' }, inplace=True )","1f527490":"data.head()","808fb346":"## check first 5 rows test dataset values\ntest.head()","400ea7cf":"# Data Summary\ndata.describe()","aad927c9":"## Check no. of columns and rows\ndata.shape","99f85fb5":"## Check no. of columns and rows of test dataset\ntest.shape","59ed3265":"## check data type of each column\ndata.info()","b6c4c428":"data.duplicated(['ClaimNumber']).sum()","621c1347":"test.duplicated(['ClaimNumber']).sum()","80a2cb77":"data.apply(pd.Series.nunique)","fbc474f9":"## check null values\ndata.isnull().sum()","599d314e":"## check unique count of each column\ndata.apply(lambda x: len(x.unique()))","67c1d7eb":"plt.figure(figsize = (9, 6))\nplt.suptitle('Countplot of Marital Status: M - Married; S - Single; U - Unknown')\nplt.subplot(1, 2, 1)\ndata['MaritalStatus'].value_counts(dropna = False).plot(kind = 'bar');\n\nplt.subplot(1, 2, 2)\ntest['MaritalStatus'].value_counts(dropna = False).plot(kind = 'bar');","90aa173a":"data['MaritalStatus'].unique()","49c3045c":"## check null values\ndata['MaritalStatus'].isnull().sum()","63a79aa1":"## check null values in test dataset\ntest.isnull().sum()","4d2e01aa":"test['MaritalStatus'].isnull().sum()","e5b4db88":"data['MaritalStatus'].fillna('U', inplace = True)\ntest['MaritalStatus'].fillna('U', inplace = True)\n","fee7e13f":"## check null values in test dataset\ntest.isnull().sum()","d27817ec":"## check null values in data\ndata.isnull().sum()","c00024cf":"data['WeeklyWages'].unique().size","23016345":"# data=data.dropna()\ndata['WeeklyWages'] = data['WeeklyWages'].fillna(data['WeeklyWages'].median())\ndata['HoursWorkedPerWeek'] = data['HoursWorkedPerWeek'].fillna(data['HoursWorkedPerWeek'].median())","416b44ae":"data.isnull().sum()","fbfe4c2e":"data.info()\n","7aac09c1":"test.info()","ed093247":"data['DateTimeOfAccident'] = pd.to_datetime(data['DateTimeOfAccident'])\ndata['DateReported'] = pd.to_datetime(data['DateReported'])\n\ntest['DateTimeOfAccident'] = pd.to_datetime(test['DateTimeOfAccident'])\ntest['DateReported'] = pd.to_datetime(test['DateReported'])","505bdd1b":"data.info()","dd667e12":"plt.figure(figsize = (9, 6))\nplt.subplot(1, 2 , 1)\nsns.distplot(data['UltimateIncurredClaimCost']);\nplt.subplot(1, 2 , 2)\nplt.title('Log Scale')\nsns.distplot(np.log1p(data['UltimateIncurredClaimCost']));","9ae2d5cb":"plt.figure(figsize=(9,6))\nsns.barplot(x='MaritalStatus',y='UltimateIncurredClaimCost',data=data,palette=\"Blues_d\",ci = None)\nplt.show()","59e8241e":"plt.figure(figsize=(9,6))\nsns.barplot(x='Gender',y='UltimateIncurredClaimCost',data=data,palette=\"Blues_d\",ci = None)\nplt.show()","6746bdb0":"plt.figure(figsize=(9,6))\nsns.barplot(x='PartTimeFullTime',y='UltimateIncurredClaimCost',data=data,palette=\"Blues_d\",ci = None)\nplt.show()","08931ce1":"plt.figure(figsize=(9,6))\nsns.barplot(x='DaysWorkedPerWeek',y='UltimateIncurredClaimCost',data=data,palette=\"Blues_d\",ci = None)\nplt.show()","39c532fe":"plt.figure(figsize=(9,6))\nplt.subplot(1, 2 , 1)\nsns.countplot(data['DependentChildren'])\nplt.subplot(1, 2 , 2)\nsns.countplot(data['DependentsOther']);","2605a7b9":"data['DependentChildren'].value_counts()","2c4fb6c2":"plt.figure(figsize=(9,6))\nsns.barplot(x='DependentChildren',y='UltimateIncurredClaimCost',data=data)\nplt.show()\n","9d1766cf":"plt.figure(figsize=(9,6))\nsns.barplot(x='DependentsOther',y='UltimateIncurredClaimCost',data=data,palette=\"Blues_d\",ci = None)\nplt.show()","1499f6d5":"plt.figure(figsize=(16,10))\nsns.barplot(x='Age',y='UltimateIncurredClaimCost',data=data,ci = None)\n\nplt.show()","c1082a00":"##Binning\ndata['Age'].min(), data['Age'].max()","d7b139e3":"data['Age_bins'] = pd.cut(x=data['Age'], bins=[9,29, 49, 69,89])","95861ea1":"data['Age_bins'].unique()","1633ee30":"plt.figure(figsize=(9,6))\nsns.barplot(x='Age_bins',y='UltimateIncurredClaimCost',data=data,palette=\"Blues\",ci = None)\n\nplt.show()","2c212fc7":"data.apply(pd.Series.nunique)","ba6404b6":"plt.title('Distribution of Weekly Wages')\nsns.distplot(data['WeeklyWages']);","afb43fb4":"plt.figure(figsize=(9,6))\nsns.scatterplot(x='WeeklyWages',y='UltimateIncurredClaimCost',data=data)\nplt.show()","e9d4c726":"plt.title('Distribution of Weekly Wages')\nsns.distplot(data['HoursWorkedPerWeek']);","7094a73d":"plt.figure(figsize=(9,6))\nsns.scatterplot(x='HoursWorkedPerWeek',y='UltimateIncurredClaimCost',data=data)\nplt.show()","5a3f2385":"## Check max and min Weekly Wages and its mean value\ndata['WeeklyWages'].max(), data['WeeklyWages'].min(), data['WeeklyWages'].mean()\n","2a199cc7":"# Scatter plot\ndata.plot(x='InitialIncurredCalimsCost', y='UltimateIncurredClaimCost', kind='scatter')\nplt.show()","0fd7e6db":"pd.pivot_table(data = data, index = 'Gender', \n               columns = ['MaritalStatus'], \n               values = ['InitialIncurredCalimsCost', 'UltimateIncurredClaimCost']).iplot(kind = 'bar', \n                                                                                         xTitle = 'Gender', \n                                                                                         yTitle = 'Claims Cost', \n                                                                                         title = 'Claims by MaritalStatus')","f420a556":"##  time difference from Accident to Reported date in hours . DateReported and DateTimeOfAccident is already transformed into datetime.\ndata['TimeDiffHrs'] = (data['DateReported'] - data['DateTimeOfAccident']).astype('timedelta64[h]')\ntest['TimeDiffHrs'] = (test['DateReported'] - test['DateTimeOfAccident']).astype('timedelta64[h]')\n\n## Create Accident in a day of the week\ndata['AccDay'] = data['DateTimeOfAccident'].dt.dayofweek\ntest['AccDay'] = test['DateTimeOfAccident'].dt.dayofweek\n\n## Create Accident in a month\ndata['AccMonth'] = data['DateTimeOfAccident'].dt.month\ntest['AccMonth'] = test['DateTimeOfAccident'].dt.month\n\n## Create Accident in a year\ndata['AccYear'] = data['DateTimeOfAccident'].dt.year\ntest['AccYear'] = test['DateTimeOfAccident'].dt.year","c260287e":"# calculate Reported year in year, month and date seperately for the dataset\ndata[\"RepYear\"] =data[\"DateReported\"].dt.year\ndata[\"RepDay\"] = data[\"DateReported\"].dt.weekday\ndata['RepMonth'] =data['DateReported'].dt.month\n\ntest[\"RepYear\"] =data[\"DateReported\"].dt.year\ntest[\"RepDay\"] = data[\"DateReported\"].dt.weekday\ntest['RepMonth'] =data['DateReported'].dt.month","89ac5c82":"#difference between accident month and reported month\n\ndata['TimeDiffMnths']=(data['RepYear']-data['AccYear'])*12+(data['RepMonth']-data['AccMonth'])\ntest['TimeDiffMnths']=(test['RepYear']-test['AccYear'])*12+(test['RepMonth']-test['AccMonth'])","12d48e97":"#difference between accident month and reported month\n\ndata['TimeDiffyrs']=(data['TimeDiffMnths']\/12).astype(int)\ntest['TimeDiffyrs']=(test['TimeDiffMnths']\/12).astype(int)","5185c7de":"data[['AccDay','AccMonth','AccYear','TimeDiffHrs','TimeDiffMnths','TimeDiffyrs']] ","1768f8d3":"plt.figure(figsize=(12,6))\nsns.barplot(x='TimeDiffMnths',y='UltimateIncurredClaimCost',data=data,ci = None)\n\nplt.show()","90bef1d7":"plt.figure(figsize=(9,6))\nsns.barplot(x='TimeDiffyrs',y='UltimateIncurredClaimCost',data=data,palette=\"Blues\",ci = None)\n\nplt.show()","75ffacf6":"plt.figure(figsize=(9,6))\nsns.barplot(x='AccDay',y='UltimateIncurredClaimCost',data=data,palette=\"Blues_d\")\n\nplt.show()","e60d0ede":"plt.figure(figsize=(9,6))\nsns.barplot(x='AccMonth',y='UltimateIncurredClaimCost',data=data,palette=\"Blues\",ci = None)\n\nplt.show()","363953f7":"\nplt.figure(figsize=(9,6))\nsns.barplot(x='AccYear',y='UltimateIncurredClaimCost',data=data,palette=\"Blues\",ci = None)\n\nplt.show()","e662dea3":"sns.distplot(data['TimeDiffHrs'])","36d5e6a5":"## Max min time hours difference\n\nprint('Train:')\nprint('max : ' , data['TimeDiffHrs'].max(),'min : ' ,data['TimeDiffHrs'].min())\n\n\nprint('Test:')\nprint(f\"Max. : {test['TimeDiffHrs'].max()} Min. : {test['TimeDiffHrs'].min()}\")\n# print(f\"Avg. Time Diff in Hrs: {round(test['TimeDiff_Hrs'].mean(), 2)}\")","c598871a":"display(data,test)","b62a520e":"display(data[data['TimeDiffHrs'] < 0], test[test['TimeDiffHrs'] < 0] )","75ec224c":"data['TimeDiffHrs'] = data['TimeDiffHrs'].apply(lambda x: 0.0 if x < 0 else x)\ntest['TimeDiffHrs'] = test['TimeDiffHrs'].apply(lambda x: 0.0 if x < 0 else x)","19200a6e":"# No need of datetime features so drop thses columns\ndata.drop(['DateTimeOfAccident', 'DateReported'], axis = 1, inplace = True)\ntest.drop(['DateTimeOfAccident', 'DateReported'], axis = 1, inplace = True)","74041105":"display(data.info(), test.info())","c0ca2cd6":"num_data = [c for c in data.columns if data[c].dtype in ['float64', 'int64'] if c not in ['AccDay', 'AccMonth', 'AccYear', 'RepYear', 'RepDay', 'RepMonth']]\ncat_data = [c for c in data.columns if c not in num_data]\nprint(f\"numerical columns: { num_data}\")\n\nprint(f\"Categorical colmns :, { cat_data}\")","a43da15a":"#Descrpitive statistics\ndata[num_data].describe()","a3616e18":"corr1 = data[num_data].corr(method = 'pearson')\n\nfig = plt.figure(figsize = (10, 8))\nsns.heatmap(corr1, mask = None, annot = True, cmap = 'PiYG', vmin = -1, vmax = +1)\nplt.title('Pearson Correlation')\nplt.xticks(rotation = 90)\nplt.show()","a1e526c2":"Q1 = data.quantile(0.25)\nQ3 = data.quantile(0.75)\nIQR = Q3 - Q1","1693294c":"((data[num_data] < (Q1 - 1.5 * IQR)) | (data[num_data] > (Q3 + 1.5 * IQR))).sum()","e1427dee":"# data = data[~((data[num_data] < (Q1 - 1.5 * IQR)) | (data[num_data] > (Q3 + 1.5 * IQR))).any(axis=1)]\n# print(data.shape)","27f4b9ec":"## replace InitialIncurredCalimsCost with its log\ndata['InitialIncurredCalimsCost'] = np.log1p(data['InitialIncurredCalimsCost'])\ntest['InitialIncurredCalimsCost'] = np.log1p(test['InitialIncurredCalimsCost'])\n\n## define target variable by taking log of UltimateIncurredClaimCost\ntarget = np.log1p(data['UltimateIncurredClaimCost'])\n\n","f9dea076":"## drop unnecessary columns \ndata.drop(['ClaimNumber', 'ClaimDescription', 'UltimateIncurredClaimCost','Age_bins'], axis = 1, inplace = True)\ntest.drop(['ClaimNumber', 'ClaimDescription'], axis = 1, inplace = True)\ndata.drop(['TimeDiffMnths', 'TimeDiffyrs'], axis = 1, inplace = True)\ntest.drop(['TimeDiffMnths', 'TimeDiffyrs'], axis = 1, inplace = True)\n\ndata.shape, test.shape\n","1b46935d":"## drop unnecessary columns \ndata.drop(['AccDay', 'AccMonth', 'AccYear','RepYear','RepDay','RepMonth'], axis = 1, inplace = True)\ntest.drop(['AccDay', 'AccMonth', 'AccYear','RepYear','RepDay','RepMonth'], axis = 1, inplace = True)\n","b182be93":"data","acb64331":"cat_data","97b17aaa":"num_data.remove('UltimateIncurredClaimCost')\ncat_data.remove('ClaimNumber')\ncat_data.remove('ClaimDescription')\ncat_data.remove('Age_bins')\nnum_data.remove('TimeDiffMnths')\nnum_data.remove('TimeDiffyrs')\n\ndata.shape, test.shape","66f55991":"cat_data.remove('AccDay')\ncat_data.remove('AccMonth')\ncat_data.remove('RepYear')\ncat_data.remove('AccYear')\ncat_data.remove('RepDay')\ncat_data.remove('RepMonth')","06e7b1c1":"data.head ","897b883a":"test.head","536513ac":"## Label encoding for each categorical features of dataset\n\nlbl = LabelEncoder()\n\nfor c in data[cat_data]:\n    print(f\"Label Encoding Categorical Feature - {c.upper()}\")\n    data[c] = lbl.fit_transform(data[c])\n    test[c] = lbl.fit_transform(test[c])\n","c7e380d6":"data[cat_data].head()","1e48d978":"std = StandardScaler()\n\ndata[num_data] = std.fit_transform(data[num_data])\ntest[num_data] = std.fit_transform(test[num_data])\n\ndata[num_data].head()","43328c40":"data.head()","07c3a60a":"Xtrain, Xtest, ytrain, ytest = train_test_split(data, target, test_size = 0.2, random_state = 2021)","094af51a":"print(Xtrain.shape, ytrain.shape, Xtest.shape, ytest.shape)","d132a36d":"# creating the model\nreg = LinearRegression()","e74f418d":"# feeding the training data to the model\nreg.fit(Xtrain, ytrain)","d02cd020":"# predicting the test set results\ny_pred = reg.predict(Xtest)\n\n# Calculating the r2 score\nr2 = r2_score(ytest, y_pred)\nprint(\"r2 score :\", r2)","36971747":"print((f\"Regression RMSE: {np.sqrt(mean_squared_error(ytest, y_pred))}\"))","d0c43d43":"mt.mean_squared_error(y_pred=reg.predict(Xtest),y_true=ytest)","1a0c8a7b":"reg.score(Xtrain,ytrain)","c033dca0":"y_preds = reg.predict(test)\nsample_reg = sample\nsample_reg['UltimateIncurredClaimCost'] = np.expm1(y_preds)","8cbc5f36":"sample_reg.head()","7bee786c":"sample_reg.to_csv(\"sample_submission_reg.csv\",index=False)\n","a02be1d6":"from lightgbm import LGBMRegressor\n\nlgbm = LGBMRegressor(\n               objective = 'regression', \n               num_leaves = 4,\n               learning_rate = 0.01, \n               n_estimators = 10000,\n               max_bin = 200, \n               bagging_fraction = 0.75,\n               bagging_freq = 5, \n               bagging_seed = 7,\n               feature_fraction = 0.2,\n               feature_fraction_seed = 7,\n               verbose = 1,\n            )\n\nlgbm_model = lgbm.fit(Xtrain, ytrain)\nlg_vpreds = lgbm_model.predict(Xtest)\nprint((f\"LGBM RMSE: {np.sqrt(mean_squared_error(ytest, lg_vpreds))}\"))","31f155e1":"lg_preds = lgbm_model.predict(test)\nsample_lgb = sample\nsample_lgb['UltimateIncurredClaimCost'] = np.expm1(lg_preds)","2e781ecb":"sample_lgb.head()","4567819c":"sample_lgb.to_csv(\"sample_submission_lgb.csv\",index=False)","4cac7e5c":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(\n                    learning_rate = 0.01, \n                    n_estimators = 10000,\n                    max_depth = 3, \n                    min_child_weight = 0,\n                    gamma = 0, \n                    subsample = 0.7,\n                    colsample_bytree = 0.7,\n                    objective = 'reg:squarederror', \n                    nthread = 1,\n                    scale_pos_weight = 1, \n                    seed = 27,\n                    reg_alpha = 0.00006\n                    )\nxgb_model = xgb.fit(Xtrain, ytrain)\nxg_vpreds = xgb_model.predict(Xtest)\nprint((f\"XGBRegressor RMSE: {np.sqrt(mean_squared_error(ytest, xg_vpreds))}\"))","c3dd0e1e":"xg_preds = xgb_model.predict(test)\nsample_xgr = sample\nsample_xgr['UltimateIncurredClaimCost'] = np.expm1(xg_preds)","7a98d52e":"sample_xgr.head()","8a8f456d":"sample_xgr.to_csv(\"sample_submission_xgr.csv\",index=False)","fdf3d7df":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\nregressor.fit(Xtrain, ytrain)\n\n# Predicting a new result\nrfr_pred = regressor.predict(Xtest)\nregressor.score(Xtrain,ytrain)","7113bfd3":"print((f\"RandomForestRegressor RMSE: {np.sqrt(mean_squared_error(ytest, rfr_pred))}\"))","dffc046c":"xg_preds = xgb_model.predict(test)\nsample_rfr = sample\nsample_rfr['UltimateIncurredClaimCost'] = np.expm1(xg_preds)","84eed336":"sample_rfr.head()","d959fd05":"sample_rfr.to_csv(\"sample_submission_rfr.csv\",index=False)","9423eda2":"> #### There are 6720 duplicate claim numbers in data","289786e8":"> We can say, Age group 49 - 69 have higher claim rate, followed by 29-49","8b29b248":"# Data Normilization\n## Standardization and Label Encoding","68bed9e5":"> #### As the reporting claim and accident time difference increases claim amount also increases. It can be better visulaise through year difference in accident and claim report","7a0605d5":"## Label Encoding","3a5b420f":"## Simple Linear regression","f0b343c0":"## Missing Values and its treatment","b546a864":"> #### This dataset includes 36176 worker compensation insurance policies, all of which have had an accident. For each record there is demographic and worker related information, as well as a text description of the accident for claim.","e40ed1b5":"# Data processing & EDA","a98cf011":"### Check duplicate rows for ClaimNumber in train dataset","c4a03748":"## Outlier count","9a8f64da":">  This represent how using log imporove the data visualisation and its understandig","aaf1ddc2":"## Visualisation of features vs target and features distribution","ca78ba6c":"> #### **DateTimeOfAccident and DateReported of train and test data set to be transformed into date time as it is given in object datatype**","04083355":"> #### There are 1769 duplicate claim numbers in test dataset","c6a55e0a":"> #### It seems, that for data set, marital status has 22 missing values  Weekly Wages has 56 missing values and Weekly Wages has 49 missing values","128a6c33":"### **check the claims that were reported before the accident happened**\n\n","054c1375":"> #### We can see that 2 years difference in accident date and reported date have more insurance claim cost","9b48a82b":"## Problem Statement :\n### The challenge is to predict Workers Compensation claims using the realistic data .\n\n### Data fields\n- ClaimNumber: Unique policy identifier\n- DateTimeOfAccident: Date and time of accident\n- DateReported: Date that accident was reported\n- Age: Age of worker\n- Gender: Gender of worker\n- MaritalStatus: Martial status of worker. (M)arried, (S)ingle, (U)unknown.- \n- DependentChildren: The number of dependent children\n- DependentsOther: The number of dependants excluding children\n- WeeklyWages: Total weekly wage\n- PartTimeFullTime: Binary (P) or (F)\n- HoursWorkedPerWeek: Total hours worked per week\n- DaysWorkedPerWeek: Number of days worked per week\n- ClaimDescription: Free text description of the claim\n- InitialIncurredClaimCost: Initial estimate by the insurer of the claim cost\n- UltimateIncurredClaimCost: Total claims payments by the insurance company. This is the field you are asked to predict in the test set.","118b2a7b":"> #### we see pair having highest (Pearson) correlation i.e, DaysWorkedPerWeek and HoursWorkedPerWeek , InitialIncurredCalimsCost and UltimateIncurredClaimCost","53d73dda":">  ","0e47a2c5":"> #### Gender distribution appears consistent between the train an test datadet. Both dataset has nan value which need to imputed","1a118512":"### Check duplicate rows for ClaimNumber in test dataset","93525acc":"## LGBMRegressor\nLightGBM is a gradient boosting framework based on decision trees to increases the efficiency of the model and reduces memory usage.Light GBM is almost 7 times faster than XGBOOST and is a much better approach when dealing with large datasets. This turns out to be a huge advantage when you are working on large datasets in limited time competitions.","f48cf037":">  ultimate incurred claims cost are higher in female than male","ba2a4618":"\n## Random Forest\nA Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a bagging technique.But it leads to overfitting in several cases","93aac939":"> #### **the data above shows, that 25 accidents in train datatest and 23 in test data set were reported on the same day of the accident but time of reporting weren't registered.**\n\n> #### **These data should be changed from negative to 0**","8c7d789f":"# Model Selection and model building\n- Create Features and Outcomes\n- Create Train and Test Datasets\n- Build a Machine learning pipeline\n- Check for model performance","91c91ac8":"## LGBMRegressor to predict Workers Compensation claims\nHere we want to predict a continuous dependent variable from a number of independent variables, hence this is regression problem.To solve this problem consisting of several unknown data and outliers, we preprocess and did feature engineering to select the best variables to fit the model. While using some of algorithm to get best fit model and less rmse,I finaaly used LGBMRegressor algorithim to build the model.As it provide fast and less rmse for our regression problem.\nLight GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for regression and many other machine learning tasks.\nAdvatages of using LGBMRegressor overother model : \n- Faster training speed and higher efficiency\n- Lower memory usage\n- Better accuracy than any other boosting algorithm\n- Compatibility with Large Datasets\n- Parallel learning supported\n\n\nBrief Description of other used algorithms are mentioned below with code.Among those due to better performance and above advantages, LGBM Regressor is selected.\n\n","d6b9a3a2":"> it is seen that most of the person has 0 dependent chilren and any other person","2a58d540":"#### Check no. of unique values of train dataset","5820794e":"> Ultimate Incurred Claim Cost are higher for married person compared to single person","3c29ae38":"## Correlation","cfe5a345":"#### Check if any missing values are present","e72c9585":"> #### Claim amount on average keeps increasing with upcoming years","f9a7d4f8":"### Imputing missing values with median for columns Weekly wages and Hours worked per week","99725e31":"> person working 6 days per week have more incurred claim cost followed by 7 days per week then 4 days then 3 days then 1 day and finnally 5 days and 2 days","440964f1":"## Extreme Gradient Boosting\n\n (XGBoost) is an open-source library that provides an efficient and effective implementation of the gradient boosting algorithm.\nXGBoost can be used directly for regression predictive modeling.\n\n","292cd680":"> the person having 1 dependent person other than children has highest ultimate claimed cost ","7f7eec2e":"### Replace nan values of Martial Status column of both data and test dataframe with U(unknowm)**","b5aa0758":"> Highest clain cost is by person with 9 dependent children","e6394f5c":"> person with parttime job have more Ultimate incurred Claim Cost than the fulltime job","f85e8b50":"# Data loading\n\n**read the csv files**","71f28650":"# Feature Engineering\n\nFeature engineering is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data.","b6f01da2":"## Data Transformation","efc241f1":"> #### we can visulaise that married female have higher claim cost in initial as well as ultimate, Also Ultimate claim cost is higher than Intial claim cost in every case","9cec2f70":"### Time Difference in hours","9f78526b":"> person of age 72 has higher claim cost but we can visulaise is perfectly with binning","818b9dd4":"> Scatter plot is one of the way to visulaise the data distribution and also here, we see some outliers for Weekly Wages"}}