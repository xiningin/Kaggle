{"cell_type":{"43234f61":"code","bd25d46e":"code","c8f4d036":"code","01da4363":"code","01644cc7":"code","bbd28cd6":"code","f77cd993":"code","f1da845d":"code","40fcdf45":"code","16f79635":"code","ea2b28fd":"code","e1c8736d":"code","316746c6":"code","900cade9":"code","286bea3e":"code","b7c1c3f6":"code","f506614c":"code","b8d2b87a":"code","98b7f1e9":"code","917d3527":"code","742bfa01":"code","7e3bda6c":"code","9612ca2f":"markdown","865dab30":"markdown","c99a771e":"markdown","da4bb0e1":"markdown","0c60faf1":"markdown","ced32500":"markdown","7b6dcb36":"markdown","a4f16311":"markdown","345772c1":"markdown","7e3546ae":"markdown","26dd7270":"markdown","f221c914":"markdown","a126a040":"markdown","78066b39":"markdown"},"source":{"43234f61":"!nvidia-smi","bd25d46e":"!nvcc --version","c8f4d036":"# General Data Manipulation Libraries\nimport numpy as np; print('Numpy Version:', np.__version__)\nimport pandas as pd; print('Pandas Version:', pd.__version__)\n\n# Model & Helper Libraries\nimport xgboost; print('XGBoost Version:', xgboost.__version__)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nimport torch; print('PyTorch Version:', torch.__version__)\n\n# Plotting Tools\nimport matplotlib.pyplot as plt\nimport plotly; print('Plotly Version:', plotly.__version__)\nfrom xgboost import plot_importance\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_param_importances\n\n# Hyper-parameter Optimization\nimport optuna; print('Optuna Version:', optuna.__version__)","01da4363":"if torch.cuda.is_available():\n    import cudf; print('cuDF Version:', cudf.__version__)","01644cc7":"# Load Data\ninput_dir = '\/kaggle\/input\/santander-customer-transaction-prediction\/'\nif torch.cuda.is_available():\n    df_train = cudf.read_csv(input_dir + '\/train.csv')\nelse:\n    df_train = pd.read_csv(input_dir + '\/train.csv')\ndf_train","bbd28cd6":"print(f'There are {len(df_train)} rows and {len(df_train.columns)} columns.')","f77cd993":"df_train.describe()","f1da845d":"# Check for NaN values\nprint(f'Are there Nan values? {df_train.isnull().values.any()}')","40fcdf45":"var_colums = [c for c in df_train.columns if c not in ['ID_code','target']]\nX = df_train.loc[:, var_colums]\ny = df_train.loc[:, 'target']\n\n# We are performing a 80-20 split for Training and Validation\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","16f79635":"# View of Xgboost Parameters\nxgboost.XGBClassifier().get_params()","ea2b28fd":"# Model instantiation\n\n# GPU Parameter\ndevice_method = 'gpu_hist' if torch.cuda.is_available() else 'auto'\nmodel_xgboost = xgboost.XGBClassifier(learning_rate=0.1,\n                                      max_depth=5,\n                                      n_estimators=5000,\n                                      subsample=0.5,\n                                      colsample_bytree=0.5,\n                                      eval_metric='auc',\n                                      use_label_encoder=False,\n                                      tree_method = device_method,\n                                      verbosity=1)\n# Validation Set\neval_set = [(X_valid, y_valid)]\n\n# Creating the DMatrix\nd_matrix = xgboost.DMatrix(data=X_train, label=y_train)\n\nxgb_param = model_xgboost.get_xgb_params()\n\ncv_folds = 10\nearly_stopping_rounds = 10\n# Cross-validation with 10 folds\ncvresult = xgboost.cv(xgb_param, d_matrix, num_boost_round=model_xgboost.get_params()['n_estimators'], \n            nfold=cv_folds, metrics='auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n\nmodel_xgboost.set_params(n_estimators=cvresult.shape[0])","e1c8736d":"# Training\nmodel_xgboost.fit(X_train,\n                  y_train,\n                  early_stopping_rounds=10,\n                  eval_set=eval_set,                  \n                  verbose=True)","316746c6":"# Print Results\nprint(\"AUC Train Mean Score: {:.4f} with Standard Deviation {:.4f}\\nAUC Valid Mean Score: {:.4f} with Standard Deviation {:.4f}\".format(cvresult['train-auc-mean'].iloc[-1],\n                                                    cvresult['train-auc-std'].iloc[-1], cvresult['test-auc-mean'].iloc[-1], cvresult['test-auc-std'].iloc[-1]))","900cade9":"# Print Results on Test-Data\ny_train_pred = model_xgboost.predict_proba(X_train)[:,1]\ny_valid_pred = model_xgboost.predict_proba(X_valid)[:,1]\n\nif torch.cuda.is_available():\n    y_train = y_train.to_array()\n    y_valid = y_valid.to_array()\n\nprint(\"AUC Train: {:.4f}\\nAUC Test: {:.4f}\".format(roc_auc_score(y_train, y_train_pred),\n                                                    roc_auc_score(y_valid, y_valid_pred)))","286bea3e":"# Feature Importance Plot\nplot_importance(model_xgboost, max_num_features=15, importance_type='gain')\nplt.figure(figsize = (25, 16))\nplt.show()","b7c1c3f6":"def objective(trial, X_train, y_train, X_valid, y_valid):\n    \n    # Model Parameters to be optimized\n    xgboost_params = {\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-7, 0.3, log=True),\n        \"n_estimators\": trial.suggest_int(name=\"n_estimators\", low=100, high=2000, step=100),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8), \n        \"subsample\": trial.suggest_categorical(name=\"subsample\", choices=[0.4, 0.5, 0.6]),\n        \"colsample_bytree\": trial.suggest_categorical(name=\"colsample_bytree\", choices=[0.4, 0.5, 0.6]),\n        \"random_state\": 1121217\n    }\n    \n    # Model Initialisation\n    model_xgboost = xgboost.XGBClassifier(eval_metric='auc', use_label_encoder=False,\n                                      tree_method = device_method, verbosity=0, **xgboost_params)\n    eval_set = [(X_valid, y_valid)]\n    \n    # Model Training\n    model_xgboost.fit(X_train, y_train, early_stopping_rounds=10, eval_set=eval_set, verbose=False)\n    \n    # Model Prediction\n    y_valid_pred = model_xgboost.predict_proba(X_valid)[:,1]\n    \n    # Optimization Metric    \n    return roc_auc_score(y_valid, y_valid_pred)","f506614c":"# Create Study Object for Optuna\nstudy = optuna.create_study(direction=\"maximize\")\n# Optimize\nstudy.optimize(lambda trial: objective(trial, X_train, y_train, X_valid, y_valid), n_trials=100)","b8d2b87a":"print(f\"Optimized roc_auc_score: {study.best_value:.5f}\")","98b7f1e9":"print(\"Best params:\")\n\nfor key, value in study.best_params.items():\n    print(f\"\\t{key}: {value}\")","917d3527":"# Check if Plotly library is available\noptuna.visualization.is_available()","742bfa01":"# Optimization History Plot\nplot_optimization_history(study)","7e3bda6c":"# Plot Hyperparameter Importance\nplot_param_importances(study)","9612ca2f":"# **Training Notebook for Santander Dataset**","865dab30":"We have multiple choices for ranking feature Importance.\n\nRefer: <a href = 'https:\/\/towardsdatascience.com\/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7'> The Multiple faces of \u2018Feature importance\u2019 in XGBoost <\/a>\n\n\n* Gain:  Implies the relative contribution of the corresponding feature to the model calculated by taking each feature\u2019s contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction.\n* Coverage: Metric means the relative number of observations related to this feature. How many times is this feature used in the classification process for all constructed trees. Expressed as a percentage for all features\u2019 cover metrics.\n* Frequency (R)\/Weight (python): Percentage representing the relative number of times a particular feature occurs in the trees of the model. \n\n*The Gain is the most relevant attribute to interpret the relative importance of each feature.*","c99a771e":"Second Part: https:\/\/www.kaggle.com\/akhilnasser\/santander-customer-transaction-training-2","da4bb0e1":"## **2. Short EDA of Data**","0c60faf1":"## **1. Required Libraries & Setup**","ced32500":"Since there are no unique identifiable characteristics among the column labels we now proceed with the rest of the Data pipeline.","7b6dcb36":"### **4.2 Cross-validation with XGBoost**\n\nRefer: <a href = 'https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f'>Hyperparameter tuning in XGBoost<\/a>\n\nThe cross-validation function is splitting the train dataset into `nfolds` and iteratively keeps one of the folds for validation purposes. `cv` returns a table where the rows correspond to the number of boosting trees used. The 4 columns correspond to the mean and standard deviation of MAE on the validation dataset and on the train dataset.","a4f16311":"### **4.1 XGBoost Parameter Selection**\n\n1. Learning Rate: Weightage of each tree in the XGBoost Classifier.\n2. Maximum Depth: The maximum depth of each tree in the XGBoost Classifier.\n3. Number of Estimators: The Maximum number of trees to be created.\n4. Subsample: The sampling percentage of the Training data used to create a Tree. Each Tree is trained on a new subsample of the trainign data.\n5. Colsample By Tree: Percentage of Features to be used while building a tree in the model. Similar to Subsample. Each Tree is trained on a new subset of the original feature space.\n6. Evaluation Metric: Evaluation Metric for the model.\n7. Use Label Encoding: The target labels have to be encoded as integers startign with 0. This will be removed soon in a new release.\n8. Verbosity: Verbosity of printing messages.\n9. Early Stopping Rounds: The stopping Criteria for the training phase. If the Validation score does not improve for the specified number of iterations the training is stopped.","345772c1":"**Recommended: GPU**","7e3546ae":"### **5.1 Plots of Results**","26dd7270":"### **4.3 Plot of Results of Training**","f221c914":"## **3. Data Preperation**","a126a040":"## **4. Model Setup & Training**","78066b39":"## **5. Hyper-parameter Optimization**"}}