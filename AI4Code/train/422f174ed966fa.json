{"cell_type":{"f5db742c":"code","7725bcf0":"code","e198a5ac":"code","1b87d6a8":"code","5b774897":"code","10829a37":"code","161ec697":"code","e9c157b7":"code","a53743fa":"code","801f7a6d":"code","3ad4b769":"code","a614231c":"code","09eab72d":"code","b0185193":"code","f788a7d2":"markdown","d4bd9d4b":"markdown","11746f52":"markdown","3604d654":"markdown"},"source":{"f5db742c":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7725bcf0":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import f1_score, accuracy_score\n\nnltk.download('stopwords')","e198a5ac":"def preprocess(raw_text):\n\n    # keep only words\n    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n\n    # convert to lower case and split \n    words = letters_only_text.lower().split()\n\n    # remove stopwords\n    stopword_set = set(stopwords.words(\"english\"))\n    meaningful_words = [w for w in words if w not in stopword_set]\n    \n    #stemmed words\n    ps = PorterStemmer()\n    stemmed_words = [ps.stem(word) for word in meaningful_words]\n    \n    #join the cleaned words in a list\n    cleaned_word_list = \" \".join(stemmed_words)\n\n    return cleaned_word_list\n","1b87d6a8":"#reading data\ndf = pd.read_csv('\/kaggle\/input\/text-classification-int20h\/train.csv', encoding='UTF-8',index_col=\"id\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/text-classification-int20h\/test.csv\", index_col=\"id\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/text-classification-int20h\/submission.csv\", index_col=\"id\")","5b774897":"#cleaning data\ndf.review = df.review.apply(lambda line : preprocess(line))\ntest_data.review = test_data.review.apply(lambda line : preprocess(line))","10829a37":"#separating y\ny = df.sentiment\n#defining vectorizer\nvectorizer = TfidfVectorizer(min_df=5, max_df=0.7, ngram_range=(1,5))\nvectorizer.fit(df.review)\nX = vectorizer.transform(df.review)\nX_sub = vectorizer.transform(test_data.review)\n#words = vectorizer.get_feature_names()","161ec697":"#choosing random indexes\nidx = np.arange(df.shape[0])\nnp.random.shuffle(idx)\n\n#choosing train_set_size\ntrain_set_size = int(df.shape[0] * 0.8)\n\n#deviding to subsets\nX_train = X[idx[:train_set_size]]\ny_train = y[idx[:train_set_size]]\nX_test = X[idx[train_set_size:]]\ny_test = y[idx[train_set_size:]]","e9c157b7":"#fitting data\nparams = {'C': [0.01, 0.05, 0.125,0.17, 0.2, 0.25, 0.30, 0.5, 0.75, 1, 1.5, 2, 3, 5, 10], \"penalty\":[\"l1\", \"l2\"]}\nclf = GridSearchCV(LinearSVC(), params, scoring = 'f1', n_jobs = -1, cv=5)\nclf.fit(X_train, y_train)","a53743fa":"#predictions\ny_hat = clf.predict(X_test)\ny_train_hat = clf.predict(X_train[:1000])","801f7a6d":"#scores\nprint(\"Unseen\")\nprint(\"F1\", f1_score(y_test, y_hat))\nprint(\"Accuracy\", accuracy_score(y_test, y_hat))\nprint()\nprint(\"Seen\")\nprint(\"F1\", f1_score(y_train[:1000], y_train_hat))\nprint(\"Accuracy\", accuracy_score(y_train[:1000], y_train_hat))","3ad4b769":"#fitting all data\nparams = {'C': [0.01, 0.05, 0.125,0.17, 0.2, 0.25, 0.30, 0.5, 0.75, 1, 1.5, 2, 3, 5, 10], \"penalty\":[\"l1\", \"l2\"]}\nclf = GridSearchCV(LinearSVC(), params, scoring = 'f1', n_jobs = -1, cv=5)\nclf.fit(X, y)","a614231c":"y_pred = clf.predict(X_sub)","09eab72d":"submission[\"sentiment\"] = y_pred","b0185193":"submission.to_csv(\".\/sub.csv\")","f788a7d2":"Cleaning and vectorising data","d4bd9d4b":"Task 1 (ML). \u0423 \u0446\u044c\u043e\u043c\u0443 \u0437\u0430\u0432\u0434\u0430\u043d\u043d\u0456\u044f \u043d\u0435\u043e\u0431\u0445\u0456\u0434\u043d\u043e \u043f\u043e\u0431\u0443\u0434\u0443\u0432\u0430\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u044c \u043a\u043b\u0430\u0441\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u0457\n\u0442\u0435\u043a\u0441\u0442\u043e\u0432\u0438\u0445 \u043a\u043e\u043c\u0435\u043d\u0442\u0430\u0440\u0456\u0432. \n\n\u0426\u0435 \u0437\u0430\u0434\u0430\u0447\u0430 \u0431\u0456\u043d\u0430\u0440\u043d\u043e\u0457 \u043a\u043b\u0430\u0441\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u0457:\n\n\u00ab0\u00bb \u2013 \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447 \u043d\u0430\u043f\u0438\u0441\u0430\u0432 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u0438\u0439 \u043a\u043e\u043c\u0435\u043d\u0442\u0430\u0440,\n\n\u00ab1\u00bb \u2013 \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447 \u043d\u0430\u043f\u0438\u0441\u0430\u0432 \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u0438\u0439 \u043a\u043e\u043c\u0435\u043d\u0442\u0430\u0440.\n\n\u0424\u0430\u0439\u043b train.csv \u2013 \u0442\u0440\u0435\u043d\u0443\u0432\u0430\u043b\u044c\u043d\u0438\u0439 \u043d\u0430\u0431\u0456\u0440 \u043a\u043e\u043c\u0435\u043d\u0442\u0430\u0440\u0456\u0432.\n\u0424\u0430\u0439\u043b test.csv \u2013 \u043a\u043e\u043c\u0435\u043d\u0442\u0430\u0440\u0456, \u0434\u043b\u044f \u044f\u043a\u0438\u0445 \u043d\u0435\u043e\u0431\u0445\u0456\u0434\u043d\u043e \u0437\u0440\u043e\u0431\u0438\u0442\u0438 \u043a\u043b\u0430\u0441\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u044e, \u0437\u0430 \u044f\u043a\u0438\u043c\u0438\n\u0439 \u0431\u0443\u0434\u0435 \u043e\u0446\u0456\u043d\u044e\u0432\u0430\u0442\u0438\u0441\u044c \u044f\u043a\u0456\u0441\u0442\u044c \u0432\u0430\u0448\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439\n\n\u0422\u0430\u043a\u0438\u043c \u0447\u0438\u043d\u043e\u043c, \u043f\u043e\u0442\u0440\u0456\u0431\u043d\u043e \u043f\u043e\u0431\u0443\u0434\u0443\u0432\u0430\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u043e \u0434\u0430\u043d\u0438\u043c, \u044f\u043a\u0456 \u043c\u0456\u0441\u0442\u044f\u0442\u044c\u0441\u044f \u0443 \u0444\u0430\u0439\u043b\u0456\ntrain.csv. \u041f\u0456\u0441\u043b\u044f \u0446\u044c\u043e\u0433\u043e, \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u044e\u0447\u0438 \u0432\u0430\u0448\u0443 \u043c\u043e\u0434\u0435\u043b\u044c, \u043f\u043e\u0442\u0440\u0456\u0431\u043d\u043e \u0434\u043b\u044f \u043a\u043e\u043c\u0435\u043d\u0442\u0430\u0440\u0456\u0432 \u0437\n\u0444\u0430\u0439\u043b\u0443 test.csv \u043e\u0431\u0440\u0430\u0445\u0443\u0432\u0430\u0442\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0443 sentiment.\n \n\n**\u0414\u043b\u044f \u0432\u0438\u043a\u043e\u043d\u0430\u043d\u043d\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u0437\u0430\u0432\u0434\u0430\u043d\u043d\u044f \u0432\u0438 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0432\u0430\u0442\u0438 \u0431\u0443\u0434\u044c-\u044f\u043a\u0456\n\u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0438 \u0442\u0430 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0438.**","11746f52":"Reading data from task","3604d654":"Defining function for cleaning text"}}