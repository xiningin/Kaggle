{"cell_type":{"fb873037":"code","ccf323c5":"code","ef876e1f":"code","17fc301e":"code","4fdbbd7e":"code","77f1f849":"code","5bc20ef5":"code","6a279908":"code","397d7e62":"code","291b39d5":"code","8fada630":"code","1a2dd060":"code","eb191992":"markdown","cb0f4af7":"markdown","9b2dba7c":"markdown","6fa914e4":"markdown","871f35d5":"markdown","56f1acf7":"markdown","6c1fb0fd":"markdown","a07de3b3":"markdown","36028bed":"markdown","9f5f25e3":"markdown"},"source":{"fb873037":"import numpy as np \nimport pandas as pd \nimport nltk\nimport string\nimport re\n\nfrom itertools import repeat\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize","ccf323c5":"df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv',index_col='id')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv',index_col='id')","ef876e1f":"df.head(5)","17fc301e":"def prep_step1(df):\n    df['text_prep'] = df['text'].copy()\n\n\n    def count_regex(text, regex):\n        return len(re.findall(regex,str(text)))\n\n    df['count_links'] = list(map(count_regex, df['text_prep'],repeat('((http|https|www)[^ ]+\\040?)|((.*\\.com)[^ ]*\\040?)')))\n    df['text_prep'] = df['text_prep'].str.replace('((http|https|www)[^ ]+\\040?)|((.*\\.com)[^ ]*\\040?)', '')\n\n    df['count_uppercasew'] = list(map(count_regex, df['text_prep'],repeat('([A-Z]{2,}) ')))\n    df['text_prep'] =  df['text_prep'].str.lower()\n\n    df['count_punct'] = list(map(count_regex, df['text_prep'],repeat('[!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~]')))\n    df['text_prep'] = df['text_prep'].str.replace('[{}]'.format(string.punctuation), '')\n\n    df['count_numbers'] = list(map(count_regex, df['text_prep'],repeat('[0-9]')))\n    df['text_prep'] = df['text_prep'].str.replace('[0-9]', '')\n\n    df['text_prep'] = df['text_prep'].str.strip()\n\n    df['text_prep'] = df['text_prep'].astype(str)\n    \n    return df.copy()\n\ndf = prep_step1(df)","4fdbbd7e":"def prep_step2(df):\n    stop_words = set(stopwords.words('english')) \n\n    def remove_stopwords(text):\n        return ' '.join([x for x in word_tokenize(text) if x not in stop_words ])\n\n    df['text_prep_nonstop'] = list(map(remove_stopwords,df['text_prep'].to_list()))\n    \n    return df.copy()\n\ndf = prep_step2(df)","77f1f849":"# https:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/\n# Ugly code, I know\n\ndef prep_step3(df):\n    from nltk.stem import WordNetLemmatizer \n    lemmatizer = WordNetLemmatizer()\n\n    def lemmatize_text(text):\n        word_and_pos = nltk.pos_tag(word_tokenize(text))    \n        lemmas = list()\n        for x in word_and_pos:\n            lemma = ''\n            try:\n                # lemmatize(Word, POS) can raise errors but lemmatize(Word) always returns a value\n                lemma = lemmatizer.lemmatize(x[0],x[1])\n            except:\n                lemma = lemmatizer.lemmatize(x[0])\n            lemmas.append(lemma)\n\n        return ' '.join(lemmas)\n\n    df['text_prep_nonstop_lemmatized'] = list(map(lemmatize_text, df['text_prep_nonstop'].to_list()))\n    return df.copy()\n\ndf = prep_step3(df)","5bc20ef5":"tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english',min_df=0.005)\ntfidf = tfidfvectorizer.fit_transform(df['text_prep_nonstop_lemmatized'])","6a279908":"x = pd.DataFrame.sparse.from_spmatrix(tfidf,columns=tfidfvectorizer.get_feature_names(), index=df.index)\ny = df['target'].copy()\n\nx = x.merge(df[['count_links','count_uppercasew','count_numbers','count_punct']], left_index=True, right_index=True)\n\nx","397d7e62":"from sklearn.linear_model import LogisticRegression\n\n# Create the model with 100 trees\nmodel = LogisticRegression(max_iter=1500)\n# Fit on training data\nmodel.fit(x, y)","291b39d5":"from sklearn import model_selection\nscores = model_selection.cross_val_score(model, x, y, cv=3, scoring=\"f1\")","8fada630":"scores","1a2dd060":"#df_test = prep_step1(df_test)\n#df_test = prep_step2(df_test)\n#df_test = prep_step3(df_test)\n\n#tfidf_test = tfidfvectorizer.transform(df_test['text_prep_nonstop_lemmatized'])\n\n#x_test = pd.DataFrame.sparse.from_spmatrix(tfidf_test,columns=tfidfvectorizer.get_feature_names(), index=df_test.index)\n#y_test = df_test['target'].copy()\n\n#pred = model.predict(x_test)","eb191992":"### Basic text cleaning\nRemoving links, punctuation, numbers, stripping whitespace and transforming lowercase <br>\nMaking it into a function to apply on the test dataset","cb0f4af7":"### Scoring test dataset","9b2dba7c":"TF-IDF Representation","6fa914e4":"## Preprocessing","871f35d5":"Removing stopwords","56f1acf7":"## Training a model\nto-do: add feature selection step","6c1fb0fd":"Lemmatizing with POS","a07de3b3":"nevermind, the competitions' labeled test dataset is available on the internet","36028bed":"### F1","9f5f25e3":"## Sources<br>\nhttps:\/\/www.kaggle.com\/faressayah\/natural-language-processing-nlp-for-beginners#4.-Text-Pre-processing<br>\nhttps:\/\/medium.com\/@datamonsters\/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908<br>\nhttps:\/\/becominghuman.ai\/nlp-for-beginners-using-nltk-f58ec22005cd<br>\nhttps:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/<br>"}}