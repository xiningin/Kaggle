{"cell_type":{"35ec78a9":"code","959d8c9a":"code","dbf25519":"code","ead7ea08":"code","57177d0f":"code","22c2d9d6":"code","f62d7f68":"code","31090a4a":"code","4398faa1":"code","cc8c4959":"code","99d39c87":"code","b46ea17c":"code","06680524":"code","03ec2ea3":"code","b6745f78":"code","9107a4f4":"code","19fc636e":"code","46ed79bf":"code","6fe98f54":"code","7500abdb":"code","0f83215d":"code","da5cfc6e":"code","d25e6aa5":"code","dc1564b3":"code","8087ea59":"code","33272e81":"code","a85d6756":"code","33b52167":"code","04c75829":"code","6fb88041":"code","8bba3b8e":"code","2581090d":"code","121f454b":"code","1d7f69be":"code","7c1a8fc4":"code","8abe8b67":"code","eb215f11":"code","d7c71a99":"code","9819e6e8":"code","0a543c84":"code","7ce15e82":"code","f5ac7871":"code","ae85052e":"code","651a8f4d":"code","a77391e2":"code","952f95a7":"code","9fa0e795":"code","f57eddb3":"code","ffaf7dff":"code","7796d3b9":"code","cb8372a8":"code","52617fd5":"markdown","ac73896b":"markdown","c2fd48eb":"markdown","39fc9a3b":"markdown","6253fd36":"markdown","b3fa0142":"markdown","ba51bc42":"markdown","7173ebfc":"markdown","033a302f":"markdown","ccbe48be":"markdown","4efe771b":"markdown","574bd5f7":"markdown","e18eeb44":"markdown","afb63fda":"markdown","2ad51f65":"markdown","e6a277e7":"markdown","71cf0783":"markdown","81866e63":"markdown","a0e682ed":"markdown","4d69e58c":"markdown","b9884451":"markdown","dd9cb3b4":"markdown","35ea7d8c":"markdown","1e2193ba":"markdown","c3ebe3fc":"markdown","1e776cc6":"markdown","fc5d048f":"markdown","59328988":"markdown","b7837a0b":"markdown","94372249":"markdown","2dac63f7":"markdown","8098a8c1":"markdown","6e6b937f":"markdown","aba3a6d1":"markdown","d795f771":"markdown","6fba78d2":"markdown","4765ad51":"markdown","8edcb2d3":"markdown","c739aafc":"markdown","ad5d7d1a":"markdown","be9be370":"markdown","15688460":"markdown","0de8d279":"markdown","77004236":"markdown","aa37712f":"markdown","659b1458":"markdown","791802a3":"markdown","01107b16":"markdown","042082e8":"markdown"},"source":{"35ec78a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","959d8c9a":"PATH_WEEK2 = '\/kaggle\/input\/covid19-global-forecasting-week-2'\n\ndf_Train = pd.read_csv(f'{PATH_WEEK2}\/train.csv')\ndf_test = pd.read_csv(f'{PATH_WEEK2}\/test.csv')","dbf25519":"PATH_POPULATION = '\/kaggle\/input\/population-by-country-2020'\n\ndf_Population = pd.read_csv(f'{PATH_POPULATION}\/population_by_country_2020.csv')","ead7ea08":"df_Train.iloc[np.r_[0:5, -6:-1], :]","57177d0f":"df_test.iloc[np.r_[0:5, -6:-1], :]","22c2d9d6":"df_Population.iloc[np.r_[0:5, -6:-1], :]","f62d7f68":"df_Train.rename(columns={'Country_Region':'Country'}, inplace=True)\ndf_test.rename(columns={'Country_Region':'Country'}, inplace=True)\n\ndf_Train.rename(columns={'Province_State':'State'}, inplace=True)\ndf_test.rename(columns={'Province_State':'State'}, inplace=True)","31090a4a":"import plotly.graph_objects as go\nimport plotly.express as px\nimport matplotlib.pyplot as plt","4398faa1":"df_Train.loc[: , ['Country', 'ConfirmedCases', 'Fatalities']].groupby(['Country']).max().sort_values(by='ConfirmedCases', ascending=False).reset_index()[:15].style.background_gradient(cmap='rainbow')","cc8c4959":"df_Train['Date'] = pd.to_datetime(df_Train['Date'], infer_datetime_format=True)\ndf_plot = df_Train.loc[: , ['Date', 'Country', 'ConfirmedCases', 'Fatalities']].groupby(['Date', 'Country']).max().reset_index()\n\ndf_plot.loc[:, 'Date'] = df_plot.Date.dt.strftime(\"%Y-%m-%d\")\ndf_plot.loc[:, 'Size'] = np.power(df_plot[\"ConfirmedCases\"]+1,0.3)-1 #np.where(df_plot['Country'].isin(['China', 'Italy']), df_plot['ConfirmedCases'], df_plot['ConfirmedCases']*300)\nfig = px.scatter_geo(df_plot,\n                     locations=\"Country\",\n                     locationmode = \"country names\",\n                     hover_name=\"Country\",\n                     color=\"ConfirmedCases\",\n                     animation_frame=\"Date\", \n                     size='Size',\n                     #projection=\"natural earth\",\n                     title=\"Rise of Coronavirus Confirmed Cases\")\nfig.show()","99d39c87":"top_10_countries = df_Train.groupby('Country')['Date', 'ConfirmedCases', 'Fatalities'].max().sort_values(by='ConfirmedCases', ascending=False).reset_index().loc[:, 'Country'][:10]\ndf_plot = df_Train.loc[df_Train.Country.isin(top_10_countries), ['Date', 'Country', 'ConfirmedCases', 'Fatalities']].groupby(['Date', 'Country']).max().reset_index()\n\nfig = px.line(df_plot, x=\"Date\", y=\"ConfirmedCases\", color='Country')\nfig.update_layout(title='No.of Confirmed Cases per Day for Top 10 Countries',\n                   xaxis_title='Date',\n                   yaxis_title='No.of Confirmed Cases')\nfig.show()","b46ea17c":"df_Population.columns","06680524":"df_Population.rename(columns={'Country (or dependency)':'Country'}, inplace=True)","03ec2ea3":"train_countries = df_Train.Country.unique().tolist()\npop_countries = df_Population.Country.unique().tolist()\n\nfor country in train_countries:\n    if country not in pop_countries:\n        print (country)","b6745f78":"renameCountryNames = {\n    \"Congo (Brazzaville)\": \"Congo\",\n    \"Congo (Kinshasa)\": \"Congo\",\n    \"Cote d'Ivoire\": \"C\u00f4te d'Ivoire\",\n    \"Czechia\": \"Czech Republic (Czechia)\",\n    \"Korea, South\": \"South Korea\",\n    \"Saint Kitts and Nevis\": \"Saint Kitts & Nevis\",\n    \"Saint Vincent and the Grenadines\": \"St. Vincent & Grenadines\",\n    \"Taiwan*\": \"Taiwan\",\n    \"US\": \"United States\"\n}","9107a4f4":"#df_Train.loc[df_Train.Country in renameCountryNames.keys(), 'Country'] = df_Train.loc[df_Train.Country in renameCountryNames.keys(), 'Country'].map(country_map)\ndf_Train.replace({'Country': renameCountryNames}, inplace=True)\ndf_test.replace({'Country': renameCountryNames}, inplace=True)","19fc636e":"df_test.tail()","46ed79bf":"df_Population.loc[df_Population['Med. Age']=='N.A.', 'Med. Age'] = df_Population.loc[df_Population['Med. Age']!='N.A.', 'Med. Age'].mode()[0]\ndf_Population.loc[df_Population['Urban Pop %']=='N.A.', 'Urban Pop %'] = df_Population.loc[df_Population['Urban Pop %']!='N.A.', 'Urban Pop %'].mode()[0]\ndf_Population.loc[df_Population['Fert. Rate']=='N.A.', 'Fert. Rate'] = df_Population.loc[df_Population['Fert. Rate']!='N.A.', 'Fert. Rate'].mode()[0]\ndf_Population.loc[:, 'Migrants (net)'] = df_Population.loc[:, 'Migrants (net)'].fillna(0)\ndf_Population['Yearly Change'] = df_Population['Yearly Change'].str.rstrip('%')\ndf_Population['World Share'] = df_Population['World Share'].str.rstrip('%')\ndf_Population['Urban Pop %'] = df_Population['Urban Pop %'].str.rstrip('%')\ndf_Population = df_Population.astype({\"Net Change\": int,\"Density (P\/Km\u00b2)\": int,\"Population (2020)\": int,\"Land Area (Km\u00b2)\": int,\"Yearly Change\": float,\"Urban Pop %\": int,\"Fert. Rate\": float,\"Med. Age\": int,\"World Share\": float, \"Migrants (net)\": float,})\n\n# As the Country value \"Diamond Princess\" is a CRUISE, we replace the population \ndf_Population = df_Population.append(pd.Series(['Diamond Princess', 3500, 0, 0, 0, 0, 0.0, 1, 30, 0, 0.0], index=df_Population.columns ), ignore_index=True)","6fe98f54":"df_Population.describe()","7500abdb":"df_Population[df_Population['Population (2020)'] <= 5000]","0f83215d":"df_Train = df_Train.merge(df_Population, how='left', left_on='Country', right_on='Country')\ndf_test = df_test.merge(df_Population, how='left', left_on='Country', right_on='Country')","da5cfc6e":"df_Train.info()","d25e6aa5":"df_test.info()","dc1564b3":"df_Population.info()","8087ea59":"df_Train['Date'] = pd.to_datetime(df_Train['Date'], infer_datetime_format=True)\ndf_test['Date'] = pd.to_datetime(df_test['Date'], infer_datetime_format=True)","33272e81":"MIN_TEST_DATE = df_test.Date.min()","a85d6756":"df_train = df_Train.loc[df_Train.Date < MIN_TEST_DATE, :]","33b52167":"y1_Train = df_train.iloc[:, -2]\ny1_Train.head()","04c75829":"y2_Train = df_train.iloc[:, -1]\ny2_Train.head()","6fb88041":"EMPTY_VAL = \"EMPTY_VAL\"\n\ndef fillState(state, country):\n    if state == EMPTY_VAL: return country\n    return state","8bba3b8e":"#X_Train = df_train.loc[:, ['State', 'Country', 'Date']]\nX_Train = df_train.copy()\n\nX_Train['State'].fillna(EMPTY_VAL, inplace=True)\nX_Train['State'] = X_Train.loc[:, ['State', 'Country']].apply(lambda x : fillState(x['State'], x['Country']), axis=1)\n\nX_Train['year'] = X_Train['Date'].dt.year\nX_Train['month'] = X_Train['Date'].dt.month\nX_Train['week'] = X_Train['Date'].dt.week\nX_Train['day'] = X_Train['Date'].dt.day\nX_Train['dayofweek'] = X_Train['Date'].dt.dayofweek\n\nX_Train.loc[:, 'Date'] = X_Train.Date.dt.strftime(\"%m%d\")\nX_Train[\"Date\"]  = X_Train[\"Date\"].astype(int)\n\n#X_Train.drop(columns=['Date'], axis=1, inplace=True)\n\nX_Train.head()","2581090d":"#X_Test = df_test.loc[:, ['State', 'Country', 'Date']]\nX_Test = df_test.copy()\n\nX_Test['State'].fillna(EMPTY_VAL, inplace=True)\nX_Test['State'] = X_Test.loc[:, ['State', 'Country']].apply(lambda x : fillState(x['State'], x['Country']), axis=1)\n\nX_Test['year'] = X_Test['Date'].dt.year\nX_Test['month'] = X_Test['Date'].dt.month\nX_Test['week'] = X_Test['Date'].dt.week\nX_Test['day'] = X_Test['Date'].dt.day\nX_Test['dayofweek'] = X_Test['Date'].dt.dayofweek\n\nX_Test.loc[:, 'Date'] = X_Test.Date.dt.strftime(\"%m%d\")\nX_Test[\"Date\"]  = X_Test[\"Date\"].astype(int)\n\n#X_Test.drop(columns=['Date'], axis=1, inplace=True)\n\nX_Test.head()","121f454b":"from sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()","1d7f69be":"X_Train.Country = le.fit_transform(X_Train.Country)\nX_Train['State'] = le.fit_transform(X_Train['State'])\n\nX_Train.head()","7c1a8fc4":"X_Test.Country = le.fit_transform(X_Test.Country)\nX_Test['State'] = le.fit_transform(X_Test['State'])\n\nX_Test.head()","8abe8b67":"df_train.head()","eb215f11":"df_train.loc[df_train.Country == 'Afghanistan', :]","d7c71a99":"df_test.tail()","9819e6e8":"X_Train.head()","0a543c84":"X_Train.iloc[3990:4020]","7ce15e82":"from warnings import filterwarnings\nfilterwarnings('ignore')","f5ac7871":"'''\nfrom sklearn.model_selection import GridSearchCV\nimport time\nparam_grid = {'n_estimators': [1000]}\n#param_grid = {'nthread':[4], 'objective':['reg:linear'], 'learning_rate': [.03, 0.05], 'max_depth': [5, 6], 'min_child_weight': [4], 'silent': [1], 'subsample': [0.7], 'colsample_bytree': [0.7], 'n_estimators': [500, 1000]}\n\ndef gridSearchCV(model, X_Train, y_Train, param_grid, cv=10, scoring='neg_mean_squared_error'):\n    start = time.time()\n    \n    grid_cv = GridSearchCV(model, param_grid, cv=cv, scoring=scoring)\n    grid_cv.fit(X_Train, y_Train)\n    \n    print (f'{type(model).__name__} Hyper Paramter Tuning took a Time: {time.time() - start}')\n    print (f'Best {scoring}: {grid_cv.best_score_}')\n    print (\"Best Hyper Parameters:\\n{}\".format(grid_cv.best_params_))\n    \n    return grid_cv.best_estimator_\n'''","ae85052e":"'''\nfrom xgboost import XGBRegressor\n\nmodel = XGBRegressor()\n\nmodel1 = gridSearchCV(model, X_Train, y1_Train, param_grid, 10, 'neg_mean_squared_error')\nmodel2 = gridSearchCV(model, X_Train, y2_Train, param_grid, 10, 'neg_mean_squared_error')\n'''","651a8f4d":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics","a77391e2":"def applyMLA(model, X_train, X_test, y_train, y_test):\n    start_time = time.time()\n    model.fit(X_train, y_train)\n    end_time = time.time()\n    time2 = end_time-start_time\n\n    predictions = model.predict(X_test)\n    RMSE_test = np.sqrt(metrics.mean_squared_error(y_test, predictions))\n    \n    return [RMSE_test, time2]","952f95a7":"from sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()","9fa0e795":"from xgboost import XGBRegressor\n#from sklearn.ensemble import RandomForestRegressor\n\ncountries = X_Train.Country.unique().tolist()\n\n#models_C = {}\n#models_F = {}\n\ndf_out = pd.DataFrame({'ForecastId': [], 'ConfirmedCases': [], 'Fatalities': []})\n\nfor country in countries:\n    states = X_Train.loc[X_Train.Country == country, :].State.unique().tolist()\n    #print(country, states)\n    # check whether string is nan or not\n    for state in states:\n        X_Train_CS = X_Train.loc[(X_Train.Country == country) & (X_Train.State == state), :]\n        \n        y1_Train_CS = X_Train_CS.loc[:, 'ConfirmedCases']\n        y2_Train_CS = X_Train_CS.loc[:, 'Fatalities']\n        #y1_Train_CS_log = np.log1p(X_Train_CS.loc[:, 'ConfirmedCases'])\n        #y2_Train_CS_log = np.log1p(X_Train_CS.loc[:, 'Fatalities'])\n        \n        X_Train_CS.drop(columns=['Id', 'ConfirmedCases', 'Fatalities'], axis=1, inplace=True)\n        \n        X_Train_CS_PCA = X_Train_CS#applyPCA(18, X_Train_CS)\n        \n        #X_Train_CS.Country = le.fit_transform(X_Train_CS.Country)\n        #X_Train_CS['State'] = le.fit_transform(X_Train_CS['State'])\n        \n        X_Test_CS = X_Test.loc[(X_Test.Country == country) & (X_Test.State == state), :]\n        \n        X_Test_CS_Id = X_Test_CS.loc[:, 'ForecastId']\n        X_Test_CS.drop(columns=['ForecastId'], axis=1, inplace=True)\n        \n        X_Test_CS_PCA = X_Test_CS#applyPCA(18, X_Test_CS)\n        \n        #X_Test_CS.Country = le.fit_transform(X_Test_CS.Country)\n        #X_Test_CS['State'] = le.fit_transform(X_Test_CS['State'])\n        \n        #models_C[country] = gridSearchCV(model, X_Train_CS, y1_Train_CS, param_grid, 10, 'neg_mean_squared_error')\n        #models_F[country] = gridSearchCV(model, X_Train_CS, y2_Train_CS, param_grid, 10, 'neg_mean_squared_error')\n        \n        model1 = XGBRegressor(n_estimators=1250)\n        #model1 = RandomForestRegressor(bootstrap=True, max_depth=80, max_features=3, min_samples_leaf=5, min_samples_split=12, n_estimators=100)\n        model1.fit(X_Train_CS_PCA, y1_Train_CS)\n        y1_pred = model1.predict(X_Test_CS_PCA)\n        #model1.fit(X_Train_CS_PCA, y1_Train_CS_log)\n        #y1_pred = np.expm1(model1.predict(X_Test_CS_PCA))\n        \n        model2 = XGBRegressor(n_estimators=1000)\n        #model2 = RandomForestRegressor(bootstrap=True, max_depth=80, max_features=3, min_samples_leaf=5, min_samples_split=12, n_estimators=100)\n        model2.fit(X_Train_CS_PCA, y2_Train_CS)\n        y2_pred = model2.predict(X_Test_CS_PCA)\n        #model2.fit(X_Train_CS_PCA, y2_Train_CS_log)\n        #y2_pred = np.expm1(model2.predict(X_Test_CS_PCA))\n        \n        df = pd.DataFrame({'ForecastId': X_Test_CS_Id, 'ConfirmedCases': y1_pred, 'Fatalities': y2_pred})\n        df_out = pd.concat([df_out, df], axis=0)\n    # Done for state loop\n# Done for country Loop","f57eddb3":"df_out.ForecastId = df_out.ForecastId.astype('int')","ffaf7dff":"df_out[3990:4020]","7796d3b9":"df_out.tail()","cb8372a8":"df_out.to_csv('submission.csv', index=False)","52617fd5":"# Modeling","ac73896b":"## Print first and last few rows of Government Response Tracker Dataset","c2fd48eb":"print(dict(zip(X_Train1.columns, model.feature_importances_)))","39fc9a3b":"### Target#1 ConfirmedCases Series","6253fd36":"X_Train['confirmed_lag_t3'] = X_Train.groupby(['State', 'Country'])['ConfirmedCases'].transform(lambda x: x.shift(3))\nX_Train['confirmed_lag_t5'] = X_Train.groupby(['State', 'Country'])['ConfirmedCases'].transform(lambda x: x.shift(5))\nX_Train['confirmed_lag_t9'] = X_Train.groupby(['State', 'Country'])['ConfirmedCases'].transform(lambda x: x.shift(9))\n\nX_Train['confirmed_rolling_mean_t3'] = X_Train.groupby(['State', 'Country'])['ConfirmedCases'].transform(lambda x: x.shift(15).rolling(3).mean())\nX_Train['confirmed_rolling_mean_t5'] = X_Train.groupby(['State', 'Country'])['ConfirmedCases'].transform(lambda x: x.shift(15).rolling(5).mean())\nX_Train['confirmed_rolling_mean_t7'] = X_Train.groupby(['State', 'Country'])['ConfirmedCases'].transform(lambda x: x.shift(15).rolling(7).mean())\n\nX_Train['confirmed_rolling_std_t3'] = X_Train.groupby(['State', 'Country'])['ConfirmedCases'].transform(lambda x: x.shift(15).rolling(3).std())\nX_Train['confirmed_rolling_std_t5'] = X_Train.groupby(['State', 'Country'])['ConfirmedCases'].transform(lambda x: x.shift(15).rolling(5).std())\nX_Train['confirmed_rolling_std_t9'] = X_Train.groupby(['State', 'Country'])['ConfirmedCases'].transform(lambda x: x.shift(15).rolling(9).std())\n\nX_Train['confirmed_rolling_skew_t15'] = X_Train.groupby(['State', 'Country'])['ConfirmedCases'].transform(lambda x: x.shift(21).rolling(15).skew())\nX_Train['confirmed_rolling_kurt_t15'] = X_Train.groupby(['State', 'Country'])['ConfirmedCases'].transform(lambda x: x.shift(21).rolling(15).kurt())\n\nX_Train['fatalities_lag_t3'] = X_Train.groupby(['State', 'Country'])['Fatalities'].transform(lambda x: x.shift(3))\nX_Train['fatalities_lag_t5'] = X_Train.groupby(['State', 'Country'])['Fatalities'].transform(lambda x: x.shift(5))\nX_Train['fatalities_lag_t9'] = X_Train.groupby(['State', 'Country'])['Fatalities'].transform(lambda x: x.shift(9))\n\nX_Train['fatalities_rolling_mean_t3'] = X_Train.groupby(['State', 'Country'])['Fatalities'].transform(lambda x: x.shift(15).rolling(3).mean())\nX_Train['fatalities_rolling_mean_t5'] = X_Train.groupby(['State', 'Country'])['Fatalities'].transform(lambda x: x.shift(15).rolling(5).mean())\nX_Train['fatalities_rolling_mean_t7'] = X_Train.groupby(['State', 'Country'])['Fatalities'].transform(lambda x: x.shift(15).rolling(7).mean())\n\nX_Train['fatalities_rolling_std_t3'] = X_Train.groupby(['State', 'Country'])['Fatalities'].transform(lambda x: x.shift(15).rolling(3).std())\nX_Train['fatalities_rolling_std_t5'] = X_Train.groupby(['State', 'Country'])['Fatalities'].transform(lambda x: x.shift(15).rolling(5).std())\nX_Train['fatalities_rolling_std_t9'] = X_Train.groupby(['State', 'Country'])['Fatalities'].transform(lambda x: x.shift(15).rolling(9).std())\n\nX_Train['fatalities_rolling_skew_t15'] = X_Train.groupby(['State', 'Country'])['Fatalities'].transform(lambda x: x.shift(21).rolling(15).skew())\nX_Train['fatalities_rolling_kurt_t15'] = X_Train.groupby(['State', 'Country'])['Fatalities'].transform(lambda x: x.shift(21).rolling(15).kurt())\n\nX_Train['lag_confirmed_t1'] = X_Train.groupby(['State', 'Country'])['ConfirmedCases'].transform(lambda x: x.shift(1))\nX_Train['lag_fatalities_t1'] = X_Train.groupby(['State', 'Country'])['Fatalities'].transform(lambda x: x.shift(1))\n\nX_Train['confirmed_change_t1'] = (X_Train['lag_confirmed_t1'] - X_Train['ConfirmedCases']) \/ (X_Train['lag_confirmed_t1'])\nX_Train['fatalities_change_t1'] = (X_Train['lag_fatalities_t1'] - X_Train['Fatalities']) \/ (X_Train['lag_fatalities_t1'])\n\nX_Train.drop(columns=['lag_confirmed_t1', 'lag_fatalities_t1'], axis=1, inplace=True)","b3fa0142":"df_Gov_Response.iloc[np.r_[0:5, -6:-1], :]","ba51bc42":"df.to_csv('submission.csv', index=False)","7173ebfc":"df_GT.iloc[np.r_[0:5, -6:-1], :]","033a302f":"## Print first and last few rows of Population Dataset","ccbe48be":"## Dataset for Model Training ","4efe771b":"df_GT.fillna(0, inplace=True)","574bd5f7":"## Submit\n### Use pandas to_csv to create a submission.csv file","e18eeb44":"train_countries = df_Train.Country.unique().tolist()\ngt_countries = df_GT.Country.unique().tolist()\n\nfor country in train_countries:\n    if country not in pop_countries:\n        print (country)","afb63fda":"X_Train1 = X_Train.copy()\nX_Test1 = X_Test.copy()\n\ny1_Train = X_Train1.loc[:, 'ConfirmedCases']\ny2_Train = X_Train1.loc[:, 'Fatalities']\n\nX_Train1.drop(columns=['Id', 'ConfirmedCases', 'Fatalities'], axis=1, inplace=True)\n#X_Train1.drop(columns=['Migrants (net)', 'Net Change', 'Yearly Change'], axis=1, inplace=True)\n\nX_Test1.drop(columns=['ForecastId'], axis=1, inplace=True)\ncols = X_Train1.columns","2ad51f65":"## Avoid Data Leakage\nAs the Train Dataset has records till 27th March 2020 and Test Dataset has partial intersection of records from 19th March 2020. Let us concise the Train Dataset to 18th March 2020.","e6a277e7":"## Transform the Date to Pandas DataTime","71cf0783":"### Transforming the Country and State to Numerical values","81866e63":"## Print first and last few rows of Covid19 Google Trends Dataset","a0e682ed":"df.info()","4d69e58c":"### Change the ForecastId datatype from float to int","b9884451":"## Print first and last few rows of Test Dataset","dd9cb3b4":"from sklearn.decomposition import PCA\n\ndef applyPCA(ncomponents, X):\n    pca = PCA(n_components = ncomponents, random_state = 0)\n    pca.fit(X)\n    return pca.transform(X) ","35ea7d8c":"y2_pred = model2.predict(X_Test)\ny2_pred = y2_pred.round()","1e2193ba":"df_Train1 = df_Train.merge(df_GT, how='left', left_on=['Country', 'Date'], right_on=['Country', 'date'])\ndf_test1 = df_test.merge(df_GT, how='left', left_on=['Country', 'Date'], right_on=['Country', 'date'])","c3ebe3fc":"# Data Transformation","1e776cc6":"## Print first and last few rows of Train Dataset","fc5d048f":"df_sub = pd.read_csv(f'{PATH_WEEK2}\/submission.csv')","59328988":"from sklearn.ensemble import AdaBoostRegressor\n\nmodel = AdaBoostRegressor()\n\ny1_rmsle = evalModel(model, X_Train1, y1_Train)\ny2_rmsle = evalModel(model, X_Train1, y2_Train)\n\nprint(y1_rmsle, y2_rmsle)","b7837a0b":"from sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor()\n\ny1_rmsle = evalModel(model, X_Train1, y1_Train)\ny2_rmsle = evalModel(model, X_Train1, y2_Train)\n\nprint(y1_rmsle, y2_rmsle)","94372249":"X_Train_PCA = applyPCA(21, X_Train)\nX_Test_PCA = applyPCA(19, X_Test)","2dac63f7":"## Fill NaN from State feature","8098a8c1":"# Data Preprocessing","6e6b937f":"PATH_GOOGLE_TRENDS = '\/kaggle\/input\/covid19-googletrends'\n\ndf_GT = pd.read_csv(f'{PATH_GOOGLE_TRENDS}\/GoogleTrend_Latest.csv')","aba3a6d1":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor(n_estimators=1000)\n\ny1_rmsle = evalModel(model, X_Train1, y1_Train)\ny2_rmsle = evalModel(model, X_Train1, y2_Train)\n\nprint(y1_rmsle, y2_rmsle)","d795f771":"df_GT[df_GT.Country == 'India']","6fba78d2":"### Target#1 Fatalities Series","4765ad51":"df_GT.isna().sum()","8edcb2d3":"### Train Dataset Information","c739aafc":"# Load Dataset","ad5d7d1a":"PATH_GOV_RESPONSE_TRACKER = '\/kaggle\/input\/oxford-covid19-government-response-tracker'\n\ndf_Gov_Response = pd.read_excel(f'{PATH_GOV_RESPONSE_TRACKER}\/OxCGRT_Download_latest_data.xlsx')","be9be370":"y1_pred = model1.predict(X_Test)\ny1_pred = y1_pred.round()","15688460":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import make_scorer, r2_score, mean_squared_log_error\n\nscore = 'neg_root_mean_squared_error'\n\ndef evalModel(model, X_Train, y_Train):\n    cv = KFold(n_splits=10, shuffle=True, random_state=25).get_n_splits(X_Train.values)\n    return cross_val_score(model, X_Train, y_Train, cv=cv, scoring=score)","0de8d279":"### Test Dataset Information","77004236":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor()\n\ny1_rmsle = evalModel(model, X_Train1, y1_Train)\ny2_rmsle = evalModel(model, X_Train1, y2_Train)\n\nprint(y1_rmsle, y2_rmsle)\n","aa37712f":"## Rename the Columns of Train and Test Datasets","659b1458":"1. For each State of a Country,<br>\n2. Train the Model<br>\n3. Predict the Target from trained the Model<br>","791802a3":"df = pd.DataFrame({'ForecastId': df_sub.ForecastId, 'ConfirmedCases': y1_pred, 'Fatalities': y2_pred})\ndf.head(10)","01107b16":"## Categorical Encoding using Label Encoder","042082e8":"px.bar(x=cols, y=model.feature_importances_)"}}