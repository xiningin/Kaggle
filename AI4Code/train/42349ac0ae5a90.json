{"cell_type":{"1edd519b":"code","3edd5948":"code","bf5a8a68":"code","cd9cd248":"code","07b15e3d":"code","6e51d0c9":"code","3d92b63c":"code","1c2e5d55":"code","55aa2c45":"code","55c79fa1":"code","fb6a07ed":"code","60da98c1":"code","a01e20c1":"code","76fa3e46":"code","648c066a":"code","c5e20987":"code","b9967aa2":"code","b079e7b0":"code","05927825":"code","10fdce2e":"code","22b85b06":"code","9f1fa804":"code","1061bcee":"code","8c7f6caf":"code","0980c05d":"code","8fb39e44":"code","fd76b8d5":"code","940d294f":"code","5a682708":"code","fd9fd4fe":"code","3b951301":"code","91dc972a":"code","d3722651":"code","6e4993dc":"code","d9f8174c":"code","a3cf824a":"code","5668c876":"code","dd40ebb2":"code","628ee838":"code","1d9f4b01":"code","325fa2cc":"code","2f6bd2ec":"code","5b9c4a37":"code","97c86c67":"code","0b2a8475":"code","d4a10423":"markdown","ef9cba7e":"markdown","d5011ce5":"markdown","477666a6":"markdown","94fc1ded":"markdown","620967ce":"markdown","ba1b33d2":"markdown","b7e3882e":"markdown","790c8ed7":"markdown","e9fe926f":"markdown","d755d6e7":"markdown","83771609":"markdown","9d751c7a":"markdown","498963fb":"markdown","f358d6f2":"markdown","d1b11ce8":"markdown","97935121":"markdown","461a3bb7":"markdown","a46e6f97":"markdown","07157259":"markdown","1f412a88":"markdown","1df2cd10":"markdown","d99913b5":"markdown","98743dd6":"markdown","2ae39e3d":"markdown"},"source":{"1edd519b":"# Import Libraries\nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\nimport IPython\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly\nimport plotly.figure_factory as ff\nfrom plotly.offline import init_notebook_mode, iplot\nfrom pylab import rcParams\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# Other Libraries\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.utils import resample\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, accuracy_score, precision_recall_curve, classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, KFold, StratifiedKFold, train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.model_selection import StratifiedShuffleSplit as sss\nimport time\nimport matplotlib.patches as mpatches\nimport collections\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nrcParams['figure.figsize'] = 16, 8","3edd5948":"import os\nprint(os.listdir(\"..\/input\"))","bf5a8a68":"df = pd.read_csv('..\/input\/creditcard.csv')\ndf.head(5)","cd9cd248":"df.describe()","07b15e3d":"df.shape","6e51d0c9":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","3d92b63c":"def reduce_memory(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\ndf = reduce_memory(df)","1c2e5d55":"%%time\nmissing_data(df)","55aa2c45":"# Our data set is heavily imbalanced\nprint('Valid Transaction vs Fraud ')\nprint(round(df['Class'].value_counts()[0]\/len(df) * 100,2),':', round(df['Class'].value_counts()[1]\/len(df) * 100,2))","55c79fa1":"count_classes = pd.value_counts(df['Class'], sort = True).sort_index()\nprint(count_classes)\ncolors = [\"#0101DF\", \"#DF0101\"]\nsns.countplot('Class', data=df, palette=colors)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=16)\nplt.title(\"Class histogram \\n (0: Valid || 1: Fraud)\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","fb6a07ed":"fig, ax = plt.subplots(1, 2, figsize=(16,4))\nfig.suptitle('Distribution', fontsize=16)\namount_val = df['Amount'].values\ntime_val = df['Time'].values\ncolors = [\"#0101DF\", \"#DF0101\"]\nsns.distplot(amount_val, ax=ax[0], color=colors[0])\nax[0].set_title('Distribution of Transaction Amount')\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color=colors[1])\nax[1].set_title('Distribution of Transaction Time')\nax[1].set_xlim([min(time_val), max(time_val)])\n\nplt.show()","60da98c1":"fig, ax = plt.subplots(1, 2, figsize=(16,4), sharex=True)\nfig.suptitle('Amount\/transaction', fontsize=16)\ncolors = [\"#0101DF\", \"#DF0101\"]\n\nsns.distplot(df[df['Class']==1].Amount, ax=ax[0], color=colors[0])\nplt.xlabel('Amount')\nplt.ylabel('Number of Transactions')\n# plt.xlim((0, 20000))\n# plt.yscale('log')\nax[0].set_title('Distribution of Transaction Amount (Fraud)')\n\nsns.distplot(df[df['Class']==0].Amount, ax=ax[1], color=colors[1])\nax[1].set_title('Distribution of Transaction Amount (Valid)')\nplt.xlabel('Amount')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","a01e20c1":"fig, ax = plt.subplots(1, 2, figsize=(16,4), sharex=True)\nfig.suptitle('Time of transaction vs Amount', fontsize=16)\ncolors = [\"#0101DF\", \"#DF0101\"]\n\nax[0].scatter(df[df['Class']==1].Time, df[df['Class']==1].Amount)\nax[0].set_title('Fraud')\nplt.xlabel('Time')\nplt.ylabel('Amount')\nax[1].scatter(df[df['Class']==0].Time, df[df['Class']==0].Amount)\nax[1].set_title('Valid')\nplt.xlabel('Time')\nplt.ylabel('Amount')\nplt.show()","76fa3e46":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(6,5,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(6,5,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show()\n    \nt0 = df.loc[df['Class'] == 0]\nt1 = df.loc[df['Class'] == 1]\nfeatures = df.columns.values[:-1]\nplot_feature_distribution(t0, t1, '0', '1', features)","648c066a":"plt.figure(figsize=(16,4))\nplt.title(\"Distribution of mean values per column in data\", fontsize=16)\nsns.distplot(t0[features].mean(axis=0),color=\"blue\",kde=True, label='Valid')\nsns.distplot(t1[features].mean(axis=0),color=\"green\",kde=True, label='Fraud')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16,4))\nfeatures = df.columns.values\nplt.title(\"Distribution of mean values per row in data\", fontsize=16)\nsns.distplot(t0[features].mean(axis=1),color=\"blue\",kde=True, label='Valid')\nsns.distplot(t1[features].mean(axis=1),color=\"green\",kde=True, label='Fraud')\nplt.legend()\nplt.show()\n\nprint(\"Distribution per row seems meaningless\")","c5e20987":"plt.figure(figsize=(16,4))\nplt.title(\"Distribution of skew values per column in data\", fontsize=16)\nsns.distplot(t0[features].skew(axis=0),color=\"blue\",kde=True, label='Valid')\nsns.distplot(t1[features].skew(axis=0),color=\"green\",kde=True, label='Fraud')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16,4))\nfeatures = df.columns.values\nplt.title(\"Distribution of skew values per row in data\", fontsize=16)\nsns.distplot(t0[features].skew(axis=1),color=\"blue\",kde=True, label='Valid')\nsns.distplot(t1[features].skew(axis=1),color=\"green\",kde=True, label='Fraud')\nplt.legend()\nplt.show()\n\nprint(\"Distribution per row seems meaningless\")","b9967aa2":"plt.figure(figsize=(16,4))\nplt.title(\"Distribution of kurtosis values per column in data\", fontsize=16)\nsns.distplot(t0[features].kurtosis(axis=0),color=\"blue\",kde=True, label='Valid')\nsns.distplot(t1[features].kurtosis(axis=0),color=\"green\",kde=True, label='Fraud')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16,4))\nfeatures = df.columns.values\nplt.title(\"Distribution of kurtosis values per row in data\", fontsize=16)\nsns.distplot(t0[features].kurtosis(axis=1),color=\"blue\",kde=True, label='Valid')\nsns.distplot(t1[features].kurtosis(axis=1),color=\"green\",kde=True, label='Fraud')\nplt.legend()\nplt.show()\n\nprint(\"Distribution per row seems meaningless\")","b079e7b0":"init_notebook_mode(connected=True)\nplotly.offline.init_notebook_mode(connected=True)\n\nfig = go.Scatter(\n    x = df[df['Class']==1].Time,\n    y = df[df['Class']==1].Amount,\n    mode = 'markers'\n)\n\nplotly.offline.iplot({\n    \"data\": [fig],\n    \"layout\": go.Layout(title=\"Scatter Plot of Amount vs Time\")\n})","05927825":"correlation_matrix = df.corr()\nfig = plt.figure(figsize=(12,8))\nfig.suptitle('Correlation Plot', fontsize=16)\nsns.heatmap(correlation_matrix,vmax=0.8,square = True)\nplt.show()","10fdce2e":"correlations = df[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\nprint(\"Top 5 correlated features\")\nprint(correlations.head(10))\nprint(\"\\n Least 5 correlated features\")\nprint(correlations.tail(10))","22b85b06":"rob_scaler = RobustScaler()\n\ndf['Amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))","9f1fa804":"X_train,X_test,y_train,y_test=train_test_split(df.drop('Class',axis=1),df['Class'],test_size=.2,random_state=21)","1061bcee":"def plot_roc(y_test,prob):\n    fpr, tpr, thresholds = roc_curve(y_test, probs)\n    plt.figure(figsize=(16,4))\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    plt.plot(fpr, tpr, marker='.')\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.title(\"ROC curve\")\n    plt.xlabel('false positive rate')\n    plt.ylabel('true positive rate')\n    plt.show()","8c7f6caf":"def logistic(X,y):\n    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n    y = y.replace([np.inf, -np.inf], np.nan).fillna(0)\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2,random_state=1)\n    lr=LogisticRegression()\n    lr.fit(X_train,y_train)\n    prob=lr.predict_proba(X_test)\n    return (prob[:,1],y_test)","0980c05d":"print('ROC plot analysis for imbalanced data')\nX=pd.concat([X_train,y_train],axis=1)\nprobs,y_test=logistic(X.drop('Class', axis=1),X['Class'])\nplot_roc(y_test,probs)","8fb39e44":"X=pd.concat([X_train,y_train],axis=1)\n\nupsampled_fraud = resample(X[X.Class==1],replace=True, n_samples=len(X[X.Class==0]), random_state=21) \nupsampled = pd.concat([X[X.Class==0], upsampled_fraud])\n\nprint(upsampled.Class.value_counts())\ncolors = [\"#0101DF\", \"#DF0101\"]\nsns.countplot(upsampled.Class, palette=colors)\nplt.title('Upsampled Data Count', fontsize=16)\nplt.ylabel('Count')\nplt.xlabel('Class')","fd76b8d5":"print('ROC plot analysis for imbalanced data after Upsampling')\nprobs,y_test=logistic(upsampled.drop('Class', axis=1),upsampled['Class'])\nplot_roc(y_test,probs)","940d294f":"undersampled_valid = resample(X[X.Class==0],replace = False, n_samples = len(X[X.Class==1]),random_state = 21) # reproducible results\n\ndownsampled = pd.concat([undersampled_valid, X[X.Class==1]])\n\nprint(downsampled.Class.value_counts())\ncolors = [\"#0101DF\", \"#DF0101\"]\nsns.countplot(downsampled.Class, palette=colors)\nplt.title('Downsampled Data Count', fontsize=16)\nplt.ylabel('Count')\nplt.xlabel('Class')","5a682708":"print('ROC plot analysis for imbalanced data after Downsampling')\nprobs,y_test=logistic(downsampled.drop('Class', axis=1),downsampled['Class'])\nplot_roc(y_test,probs)","fd9fd4fe":"X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n\ntl = TomekLinks(return_indices=True, ratio='majority')\nX_tl, y_tl, id_tl = tl.fit_sample(X.drop(['Class'], axis=1), X['Class'])","3b951301":"print('ROC plot analysis for imbalanced data after Downsampling with TomekLinks')\nprobs,y_test=logistic(pd.DataFrame(X_tl),pd.DataFrame(y_tl))\nplot_roc(y_test,probs)","91dc972a":"smote = SMOTE(ratio='minority')\nX_sm, y_sm = smote.fit_sample(X.drop(['Class'], axis=1), X['Class'])","d3722651":"print('ROC plot analysis for imbalanced data after Upsampling with SMOTE')\nprobs,y_test=logistic(pd.DataFrame(X_sm),pd.DataFrame(y_sm))\nplot_roc(y_test,probs)","6e4993dc":"# T-SNE Implementation\nt0 = time.time()\nX = X.replace([np.inf, -np.inf], np.nan).fillna(0)\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"PCA took {:.2} s\".format(t1 - t0))\n\n# TruncatedSVD\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"Truncated SVD took {:.2} s\".format(t1 - t0))","d9f8174c":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,8))\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\nblue_patch = mpatches.Patch(color='#0A0AFF', label='Valid')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(X['Class'] == 0), cmap='coolwarm', label='Valid', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(X['Class'] == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\nax1.grid(True)\nax1.legend(handles=[blue_patch, red_patch])\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(X['Class'] == 0), cmap='coolwarm', label='Valid', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(X['Class'] == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\nax2.grid(True)\nax2.legend(handles=[blue_patch, red_patch])\n\n# TruncatedSVD scatter plot\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(X['Class'] == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(X['Class'] == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax3.set_title('Truncated SVD', fontsize=14)\nax3.grid(True)\nax3.legend(handles=[blue_patch, red_patch])\nplt.show()\n","a3cf824a":"# Let us convert dataframes into arrays\nX_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\nX_train = X_train.values\nX_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\nX_test = X_test.values\ny_train = y_train.replace([np.inf, -np.inf], np.nan).fillna(0)\ny_train = y_train.values\ny_test = y_test.replace([np.inf, -np.inf], np.nan).fillna(0)\ny_test = y_test.values","5668c876":"# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.01, 0.1, 1]}\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\nlog_reg = grid_log_reg.best_estimator_","dd40ebb2":"log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')","628ee838":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1)) = plt.subplots(1,1, figsize=(16,8), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    return plt","1d9f4b01":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=21)\nplot_learning_curve(log_reg, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","325fa2cc":"# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n                             method=\"decision_function\")","2f6bd2ec":"print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))","5b9c4a37":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr)\nplt.show()","97c86c67":"classifiers = {\n    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(X), verbose=0),\n    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n                                              leaf_size=30, metric='minkowski',\n                                              p=2, metric_params=None),\n    \"Support Vector Machine\":OneClassSVM(kernel='rbf', degree=3, gamma=0.1,nu=0.05, \n                                         max_iter=-1)\n   \n}","0b2a8475":"for i, (clf_name,clf) in enumerate(classifiers.items()):\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(X_train)\n        scores_prediction = clf.negative_outlier_factor_\n    elif clf_name == \"Support Vector Machine\":\n        clf.fit(X_train)\n        y_pred = clf.predict(X_train)\n    else:    \n        clf.fit(X_train)\n        scores_prediction = clf.decision_function(X_train)\n        y_pred = clf.predict(X_train)\n\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    n_errors = (y_pred != y_train).sum()\n\n    print(\"{}: {}\".format(clf_name,n_errors))\n    print(\"Accuracy Score :\")\n    print(accuracy_score(y_train,y_pred))\n    print(\"Classification Report :\")\n    print(classification_report(y_train,y_pred))\n    print('\\n')","d4a10423":"## Data Exploration\n\nThe first basic step is to understand our data. The unique thing about our data set is that other than the columns transaction and amount, rest of the columns are anonymised for privacy concerns. \nIt is mentioned in the data description that the data went through PCA transformation. It can be assumed that prior to the application of PCA, data was scaled as per normal convention. Normalization is important in PCA since it is a variance maximizing exercise. It projects your original data onto directions which maximize the variance. ","ef9cba7e":"> There are 284807 entries and 31 columns","d5011ce5":"## Undersampling using Tomek links\n> Tomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.","477666a6":"## Primary Models\n> Let us prepare a set of primary algorithms before venturing to more complex ones!","94fc1ded":"## Dealing with Imbalanced Data Set\n\n>It is obvious looking at the data that there is a clear majority and minority class in the distribution. Data is heavily skewed. We will counter ways to deal with this issue in the notebook. We will check whether preprocessing techniques work better when there is an overwhelming majority class in the data.\n\n#### Implication of imbalanced classes\n>- <b>Overfitting: <\/b>From the analysis it is clear that the data is heavily imbalanced as most of the transactions are valid. By default our model might predict every scenario to be valid and still have a good accuracy. We do not want that, do we! So let us try to balance the classes.\n>- <b>Wrong Correlations: <\/b>It will be useful to understand how each feature influence the result (Valid or Fraud). Having an imbalance dataframe causes us to miss the true correlations between class and features.\n\n#### There are several ways to approach this classification problem taking into consideration this unbalance. \n\n>- Collect more data\n\n>- Changing the performance metric:\n    - **Confusion matrix**: Table showing correct predictions and types of incorrect predictions.\n    - **Precision**: The number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier\u2019s exactness. Low precision indicates a high number of false positives.\n    - **Recall**: The number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier\u2019s completeness. Low recall indicates a high number of false negatives.\n    - **F1score**: The weighted average of precision and recall.\n    - **Kappa**: The classification accuracy normalized by the imbalance of classes.\n    - **ROC curves**: The ratio between sensitivity and specificity.\n\n>ROC curves should be used if there are roughly equal numbers of observations for each class.\nPrecision-Recall curves should be used when there is a moderate to large class imbalance.\n\n>- Resampling the dataset\n    - Process the data to have approximately 1:1 ratio between the classes.\n    - Over-sampling (add copies of under-represented class)\n     - Under-sampling, (delete instances from over-represented class)","620967ce":"## Oversampling using SMOTE\n> SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.","ba1b33d2":"## Split into train and test data\n> Split data into 80% train and 20% test data.","b7e3882e":">#### Distribution of the mean values per column and ****row in the data","790c8ed7":" #### Distribution of skewness per column and row\n >Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive or negative, or undefined.","e9fe926f":"#### Undersampling Minority Classes\n> Oversampling involves the removal of some observations from majority class. It is an optimum choice when we have millions of rows of data. This does enable faster training time but we are throwing away data that could be valuable and undersampling may lead to underfitting and poor generalization.","d755d6e7":"#### Correlation Matrix\n>We observe that none of the V1 to V28 componenets are correlated and have no correlation to each other","83771609":"## Model Prediction","9d751c7a":"## Machine Learning Modelling\n\nThere are various machine learning techniques that can be used to deal with anomaly detection. Below listed are some of the common ones.\n\n#### K-nearest neighbor\nk-NN is a simple, non-parametric lazy learning technique used to classify data based on similarities in distance metrics such as Eucledian, Manhattan, Minkowski, or Hamming distance.\n\n#### Relative density of data \nCommonly known as local outlier factor (LOF). This concept is based on a distance metric called reachability distance.\n\n#### K-means \nK-means is a popular clustering algorithm  and is based on the assumption that data points that are similar tend to belong to similar groups or clusters, as determined by their distance from local centroids. It creates 'k' similar clusters of data points. Data instances that fall outside of these groups could potentially be marked as anomalies.\n\n#### Support Vector Machine\n\nSVM is typically associated with supervised learning, but there are extensions (OneClassCVM) can be used to identify anomalies as an unsupervised problems. SVM learns a soft boundary in order to cluster the normal data instances using the training set, and then, using the testing instance, it tunes itself to identify the abnormalities that fall outside the learned region.\n\n#### Isolation Forest Anomaly Detection Algorithm\n\n#### Density-Based Anomaly Detection (Local Outlier Factor)Algorithm\n\n#### Support Vector Machine Anomaly Detection Algorithm \n","498963fb":"## Dimensionality Reduction\n>TSNE\n\n>PCA\n\n>Truncated SVD","f358d6f2":"### Data Distribution\n\nLet us explore the distribution to have an idea how data is spread out. The distribution of transaction time is binomial in nature.","d1b11ce8":"# Introduction \n\nCredit card fraud analysis can be viewed as an example of Anomaly detection, a technique used to detect unusual patterns, called outliers. Anomaly detection has applications ranging from  is a technique used to identify unusual patterns that do not conform to expected behavior, called outliers. It has many applications in the field of fraud detection, fault detection, system health monitoring, all the way to detecting ecological disturbances. \n\nLet us dwell into anomaly detection using a the case of credit card fraud and combat it using the machine learning algorithms at our disposal! We will use various predictive models to gauge the accuracy of these models in detecting whether a transaction is a normal payment or a fraud.\n\n## Problem:\n\nCredit Card Fraud Detection involves modelling the card transactions using the the transaction history based on the ones that turned out to be fraud. It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\nWe obtained the dataset used for credit card fraud detection from Kaggle: https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\n<h2> References: <\/h2>\n<ul> \n<li><a src=\"https:\/\/www.kaggle.com\/lane203j\/auprc-5-fold-c-v-and-resampling-methods\"> auprc, 5-fold c-v, and resampling methods\n<li><\/a> by Jeremy Lane (Kaggle Notebook) <\/li>\n<li><\/a> by Janio Martinez (Kaggle Notebook) <\/li>\n<li><\/a> by Dominik Stuerzer  (Kaggle Notebook) <\/li>\n<li><\/a> by Shivam Bansal  (Kaggle Notebook) <\/li>\n<li><\/a> by Nanashi  (Kaggle Notebook) <\/li>\n<\/ul>","97935121":"#### Check for Overfitting","461a3bb7":"#### Missing data in the dataframe","a46e6f97":"#### 1. Isolation Forest Algorithm: \n> Isolation Forest ia an anomaly detection algorithm and it explicitly identifies anomalies instead of profiling normal data points. Isolation Forest, like any tree ensemble method, is built on the basis of decision trees. In these trees, partitions are created by first randomly selecting a feature and then selecting a random split value between the minimum and maximum value of the selected feature.\n\n> In principle, outliers are less frequent than regular observations and are different from them in terms of values (they lie further away from the regular observations in the feature space). That is why by using such random partitioning they should be identified closer to the root of the tree (shorter average path length, i.e., the number of edges an observation must pass in the tree going from the root to the terminal node), with fewer splits necessary.\n\n> As with other outlier detection methods, an anomaly score is required for decision making.Each observation is given an anomaly score and the following decision can be made on its basis:\n> * A score close to 1 indicates anomalies\n> * Score much smaller than 0.5 indicates normal observations\n> * If all scores are close to 0.5 then the entire sample does not seem to have clearly distinct anomalies\n\n>Similarly to Random Forest, it is built on an ensemble of binary (isolation) trees\n\n> [*https:\/\/towardsdatascience.com\/outlier-detection-with-isolation-forest-3d190448d45e*]\n\n#### 2. Local Outlier Factor(LOF) Algorithm\n>Local Outlier Factor algorithm is an unsupervised outlier detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outlier samples that have a substantially lower density than their neighbors. Local Outlier Factor (LOF) is a score that tells how likely a certain data point is an outlier\/anomaly.\n\n>The local outlier factor is based on a concept of a local density, where locality is given by k nearest neighbors, whose distance is used to estimate the density. By comparing the local density of an object to the local densities of its neighbors, one can identify regions of similar density, and points that have a substantially lower density than their neighbors. These are considered to be outliers.\n\n#### 3. One Class SVM\n> One-class SVM is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set. It is not a 2-class problem rather it is (1+x)-class classification. Also, its an unsupervised algorithm that learns  decision boundary for  novelty detection.\n\n> A One-Class Support Vector Machine is an unsupervised learning algorithm that is trained only on the \u2018normal\u2019 data, in our case the negative examples. It learns the boundaries of these points and is therefore able to classify any points that lie outside the boundary as, you guessed it, outliers.","07157259":"## Anomaly Detection Algorithms","1f412a88":"## Normalization\n> We use RobustScaler as it is less prone to outliers.","1df2cd10":"#### Technique to reduce memory usage of the dataframe","d99913b5":"#### Oversampling Minority Classes\n> Oversampling involves the addition of copies of minority class. It is an optimum choice when we do not have much data.","98743dd6":"#### Distribution of kurtosis per column and row\n >Kurtosis is a measure of the \"tailedness\" of the probability distribution of a real-valued random variable. Kurtosis is a measure of the combined weight of a distribution's tails relative to the center of the distribution.","2ae39e3d":"#### ROC Curve Plot"}}