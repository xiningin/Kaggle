{"cell_type":{"5b13533c":"code","1a742c1d":"code","c5cddd40":"code","085bf1a2":"code","fcdc5069":"code","701f851a":"code","24700592":"code","39e4476a":"code","f99bada2":"code","68bdf4bb":"code","c7552f62":"code","3dc7c98b":"code","38b3c4b6":"code","7cc3ab3a":"code","c7a8cd01":"code","1448c781":"code","2812552f":"code","1d14a1e2":"code","eba6cae9":"markdown","c4275502":"markdown","b78bffb9":"markdown","658c92d1":"markdown","843f8d1e":"markdown","011649b6":"markdown","2095d067":"markdown","f0587dc5":"markdown","cdd55db7":"markdown","92ad6046":"markdown","d54a11c7":"markdown","c9dbd103":"markdown","f8b2b64a":"markdown","c41f0fcb":"markdown","70c3bec1":"markdown","52b50e01":"markdown","5e14664c":"markdown","467aa9b1":"markdown","08243021":"markdown"},"source":{"5b13533c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a742c1d":"# import needed modules\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n\n# this line is needed for plotting later\nmpl.rcParams['agg.path.chunksize'] = 10000","c5cddd40":"train_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')\n\nprint(\"successfully loaded!\")","085bf1a2":"print(train_data.shape)\nprint(test_data.shape)","fcdc5069":"# .info() is a helpful command to get a nice overview of a dataframe\n\nprint(train_data.info(), \"\\n\")\nprint(test_data.info())","701f851a":"# create list containing all column names of train_data\n\nlist_of_train_features = train_data.columns.to_list()\n\nprint(list_of_train_features)","24700592":"# plot all 14 'cont' features \n\nfor i in range(1,len(list_of_train_features)-1):\n    \n    fig = plt.figure(figsize=(7,4.5))\n    plt.plot(train_data[\"id\"], train_data[list_of_train_features[i]])\n    plt.title(list_of_train_features[i])\n    plt.show()","39e4476a":"# create list containing all column names of test_data\n\nlist_of_test_features = test_data.columns.to_list()\n\nprint(list_of_test_features)","f99bada2":"for i in range(1,len(list_of_test_features)):\n    \n    fig = plt.figure(figsize=(7,4.5))\n    plt.plot(test_data[\"id\"], test_data[list_of_test_features[i]])\n    plt.title(list_of_test_features[i])\n    plt.show()","68bdf4bb":"fig = plt.figure(figsize=(7,4.5))\nplt.plot(train_data[\"id\"], train_data[\"target\"])\nplt.title(\"target\")\nplt.show()","c7552f62":"# find the outlier by looking for all values in the\n# train_data set that have a target value smaller than 1.0\n\noutlier = train_data.loc[train_data.target < 1.0]\nprint(outlier)","3dc7c98b":"# remove the outlier from the train_data set\ntrain_data.drop([170514], inplace = True)","38b3c4b6":"for i in range(1,len(list_of_train_features)-1):\n    \n    fig = plt.figure(figsize=(7,4.5))\n    plt.plot(train_data[list_of_train_features[i]], train_data[\"target\"], linestyle = '', marker = 'x')\n    plt.title(list_of_train_features[i])\n    plt.show()","7cc3ab3a":"# create y_train which only contains the target\ny_train = train_data[\"target\"]\n\n# remove target column from train_data set\ntrain_data.drop(columns = [\"target\"], inplace = True)","c7a8cd01":"from sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\n\n\n# parameter combinations\nparams = {  'n_estimators' : [1500, 2000, 2500],\n            'learning_rate' : [0.01, 0.02]\n        }\n\n\n# xgb regressor model\nxgb = XGBRegressor(\n        objective = 'reg:squarederror',\n        subsample = 0.8,\n        colsample_bytree = 0.8,\n        learning_rate = 0.01,\n        tree_method = 'gpu_hist')\n        #colsample_bynode = 0.85,\n        #colsample_bylevel = 0.2,\n        #gamma = 0.01,\n        #reg_lambda = 0.01,\n        #learning_rate = 0.02)\n        #max_depth = 3,\n        #min_child_weight = 3,\n        #n_estimators = 6100)\n\n\ngrid_search = GridSearchCV(xgb, \n                           param_grid = params, \n                           scoring = 'neg_root_mean_squared_error', \n                           n_jobs = -1, \n                           verbose = 10)\n\ngrid_search.fit(train_data, y_train)\n\n\nprint('\\n Best estimator:')\nprint(grid_search.best_estimator_)\n\nprint('\\n Best score:')\nprint(grid_search.best_score_)\n\nprint('\\n Best hyperparameters:')\nprint(grid_search.best_params_)","1448c781":"# we make a copy of test_data to use the 'id' column later\ntest_data_backup = test_data.copy()\n\ntrain_data.drop(columns = [\"id\"], inplace = True)\ntest_data.drop(columns = [\"id\"], inplace = True)","2812552f":"clf = XGBRegressor(\n    objective = 'reg:squarederror',\n    subsample = 0.8,\n    learning_rate = 0.02,\n    max_depth = 7,\n    n_estimators = 2000,\n    tree_method = 'gpu_hist')\n\n\nclf.fit(train_data, y_train)\n\ny_pred_xgb = clf.predict(test_data)\n\nprint(y_pred_xgb)","1d14a1e2":"solution = pd.DataFrame({\"id\":test_data_backup.id, \"target\":y_pred_xgb})\n\nsolution.to_csv(\"solution.csv\", index = False)\n\nprint(\"saved successful!\")","eba6cae9":"## 3.2) Plot x-axis = feature, y-axis = target \n\n**In these plots we will be able to see the relation between the 14 features and the 'target' column.**","c4275502":"**As we can see, n_estimators = 2000  and learning_rate = 0.02 are the best parameters.**\n\n**You can optimize every parameter of xgboost and other models with this GridSearchCV method, but always keep an eye on the GridSearch score, the score should always improve as well when you optimize your parameters more and more.**","b78bffb9":"**We then have to click on \"Save Version\", then choose  \"Save & Run All (Commit)\"  such that the submission file will be saved in the output tab of our notebooks.**\n\n**Then we can click on \"Output\" of this notebook and submit the submission file to the competition to see how good it scored.**","658c92d1":"# Thank you for reading this Tabular Playground Tutorial !\n\n# If you have any questions or ideas, feel free to ask and comment :)","843f8d1e":"# 2.) Have a first look at data\n\n**In this chapter we will simply print some interesting properties and information of our train_data and test_data sets.**","011649b6":"**The next kind of plots will have the 'cont' feature of the x-axis, and the 'target' column on the y-axis.**","2095d067":"# Beginner-friendly Tabular Playground tutorial (0.69924 score)\n\n\n\n**Hello and welcome to my beginner-friendly tutorial for the Tabular Playground series Januar 2021 competition!**\n\n**This tutorial is meant for anybody who is new to kaggle competitions. Doenst matter if you have absolutely no experience with kaggle competitions or if you already gained some experience in a few competitions, this tutorial should be helpful for both scenarios.**\n\n**I was very happy about this new kaggle competition series, every first day of each month a competition like this will be hosted :)**\n\n**It is very helpful for beginners, because the datasets are very friendly and nicely structured, and in this competition we dont even have any categorical features.**\n\n\n# What is going to happen in this tutorial?\n\n\n**In this tutorial we will first look at the data and then simply train a xgb regressor model.** \n","f0587dc5":"**The train_data has 300k rows and 16 columns, the test_data has 200k rows and 15 columns.**","cdd55db7":"# 3.) Plot data\n\n## 3.1) Plot x-axis = id, y-axis = feature\n\n\n**The first kind of plots we are going to look at will have the 'id' column on the x-axis and the features 'cont1' to 'cont14' on the y-axis.** ","92ad6046":"**The 14 'cont' features of the test_data look extremely similar to the features of the train_data.**\n\n**Finally let's plot the target before we move on to the next kind of plots:**","d54a11c7":"**We can see that many features simply look like a cloud, a broad distribution, no clear relation or dependency between any of the features and the target.**\n\n**Interesting are the stripes in 'cont2', this is the only feature that does not have this cloud-like distribution.**","c9dbd103":"**The target shows no interesting effects, shapes or relations, it's just numbers between rougly 6 and 10.**\n\n**The only interesting thing is that one outlier at the bottom with a target value of about 0.0, let's remove this outlier.**","f8b2b64a":"# 4.) Optimize xgb model\n\n\n\n**Before we will train our xgb regressor model, we must find the optimal values for our datasets.**\n\n**Most models like XGBoost, LightGBM or CatBoost have many many parameters that can be optimized such that the model prediction is more accurate.**\n\n**Feel free to look at the list of xgb parameters, we will only use and optimize a few of them:** https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html","c41f0fcb":"**When we look at these 14 plots and scroll through them, we see immediately that all 14 features have the same kind of distribution.**\n\n**The range of all 14 features goes from 0.0 to 1.0, sometimes a little more, sometimes a little less.**\n\n**But there are no interesting shapes or distributions or dependencies noticeable.**\n\n**Let's see if this is the same for the 14 'cont' features of the test_data set:**","70c3bec1":"# 1.) Load data","52b50e01":"# 5.) Train model\n\n## xgb regressor model\n\n**We will now train a xgb regressor model with the parameters we optimized with help of a GridSearchCV.**\n\n**Then we will use the test_data to make a prediction, and then save this prediction as the submission file.**\n\n**But before we are going to train our model, we will drop the 'id' column from train and test data set, because this column does not contain any useful information.**","5e14664c":"**Now we can save the prediction:**","467aa9b1":"## xgb GridSearchCV\n\n**GridSearchCV is a helpful tool for parameter optimization.**\n\n**It works by trying out every possible combination of a given set of parameters and then detecting the best combination.**\n\n**These GridSearchCV calculations can take very long depending on the size of the dataset and the number of parameter combinations given.**\n\n**It is recommended to keep the number of parameter combinations low in order to keep the total computation time as short as possible.**\n\n**Down below you can find the code for making a GridSearchCV in order to find the best parameter combination of the given 'n_estimators' and 'learning_rate' parameters.**","08243021":"**Here we can see nicely that the dataset of this competition simply consists of one 'id' column, 14 'cont' columns\/features consisting of float64 numbers, and the train_data also has the 'target' feature. This is the feature we want to predict as accurately as possible, this is the goal of the entire competition.** \n\n**The next step will be to plot the train_data and test_data. We do this to get a better insight and understanding of our data.**"}}