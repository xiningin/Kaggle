{"cell_type":{"c93cf1c3":"code","759964aa":"code","3c92509f":"code","d72044da":"code","e6b1f6df":"code","0882725b":"code","5a06b691":"code","f761d89c":"code","90154eff":"code","ce6bf5e9":"code","2da354ed":"code","c877dea1":"code","1303f407":"code","d505c661":"code","c4a30503":"code","0b2d6ebe":"code","8f97d61c":"code","d7dd6422":"code","d9060516":"code","c8a29aae":"code","837e24c1":"code","f843186a":"code","2c4f3156":"code","79bfa6b7":"code","b04b0323":"code","fd396820":"code","4849b0f1":"code","6118fe7e":"code","9841d419":"code","d90f9f6a":"code","5198adab":"code","db74be7b":"code","d6669bf9":"code","10913951":"code","19d2ca01":"code","d7cb81a5":"code","5738d92b":"code","825784f4":"code","7bcd7fb4":"code","cbd1a242":"code","fb842c3d":"code","b4b62664":"code","99449b8e":"code","0cbb16ed":"markdown","91af55b7":"markdown","83520874":"markdown","3f155391":"markdown","d91751ac":"markdown","595b4e3f":"markdown","74b136fe":"markdown","bbb4e492":"markdown","46a0a040":"markdown","4dcd8447":"markdown","faa480d8":"markdown","ed60976a":"markdown","32e25ae4":"markdown","9bf7f193":"markdown","9fd864b3":"markdown","2f2bd230":"markdown","b2d2d72e":"markdown","66858819":"markdown","18a533f3":"markdown","5d537d07":"markdown","4c9619dc":"markdown","378981e9":"markdown","78a4813c":"markdown","f9678fb3":"markdown","62aec91d":"markdown","916dccdd":"markdown","462c86c5":"markdown","2acc255f":"markdown","647d51ee":"markdown","30656bd8":"markdown","5e4ad929":"markdown","e771022c":"markdown","15f1ccb3":"markdown"},"source":{"c93cf1c3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Importing Data\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nweatherAUS_data = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","759964aa":"# Dataframe Dimensions: How many rows and columns are there?\ndata = weatherAUS_data.copy()\ndata.shape","3c92509f":"data.head().T","d72044da":"data.info()","e6b1f6df":"# Unique values in categorical variables\ncat_col = data.select_dtypes(include=['object'])\ncat_col.nunique()","0882725b":"data['Location'].value_counts()","5a06b691":"cat_col.isna().sum()","f761d89c":"# Breaking down date column to Year, Month and Date\n\ndata['Date'] = pd.to_datetime(data['Date']) # parse as datatime\n\ndata['Year'] = data['Date'].dt.year\ndata['Month'] = data['Date'].dt.month\ndata['Day'] = data['Date'].dt.day\n\ndata[['Date', 'Year', 'Month', 'Day']] # preview changes made\n\ndata.drop('Date', axis=1, inplace = True)\ndata.info()","90154eff":"# Unique Values\nprint(data['Location'].value_counts())\nprint(\"\")\nprint(\"\")\nprint(f\"Missing Values: {data['Location'].isna().sum()}\")","ce6bf5e9":"# One Hot Encoding (Preview) for Location column\npd.get_dummies(data.Location, drop_first=True).head() # 'drop first' means that we drop the first value if everything else is 0","2da354ed":"# Unique Values\nprint(data['WindGustDir'].value_counts())\nprint(\"\")\nprint(\"\")\nprint(f\"Missing Values: {data['WindGustDir'].isna().sum()}\")","c877dea1":"# One-Hot Encoding (Preview) of 'WindGustDir'\n\npd.get_dummies(data.WindGustDir, drop_first=True, dummy_na=True).head()","1303f407":"# Unique Values\nprint(data['WindDir9am'].value_counts())\nprint(\"\")\nprint(\"\")\nprint(f\"Missing Values: {data['WindDir9am'].isna().sum()}\")","d505c661":"# One Hot Encoding (Preview) for WindDir9am column\n\npd.get_dummies(data.WindDir9am, drop_first=True, dummy_na=True).head()","c4a30503":"# Unique Values\nprint(data['WindDir3pm'].value_counts())\nprint(\"\")\nprint(\"\")\nprint(f\"Missing Values: {data['WindDir3pm'].isna().sum()}\")","0b2d6ebe":"# One Hot Encoding (Preview) for WindDir3pm column\n\npd.get_dummies(data.WindDir3pm, drop_first=True, dummy_na=True).head()","8f97d61c":"print(data['RainToday'].value_counts())\nprint(\"\")\nprint(\"\")\nprint(f\"Missing Values: {data['RainToday'].isna().sum()}\")","d7dd6422":"pd.get_dummies(data.RainToday, drop_first=True, dummy_na=True).head()","d9060516":"# Missing Values\n\nvar_data = data.select_dtypes(include=['float64'])\nvar_data.isna().sum()","c8a29aae":"# Continuous variables\n\nvar_data = data.select_dtypes(include=['float64'])\nvar_data.describe().apply(round) # round the data to first decimal place","837e24c1":"fig, axes = plt.subplots(4,1, figsize=(10, 10), sharey=False)\nfig.suptitle('Distribution of (some) continuous variables')\n\n# Rainfall\nsns.boxplot(x= 'Rainfall', data = data, palette = 'Set2', ax = axes[0])\naxes[0].set_title(\"\")\n\n# Evaporation\nsns.boxplot(x= 'Evaporation', data = data, palette = 'Set2', ax = axes[1])\naxes[1].set_title(\"\")\n\n# Windspeed (9AM)\nsns.boxplot(x= 'WindSpeed9am', data = data, palette = 'Set2', ax = axes[2])\naxes[2].set_title(\"\")\n\n# Windspeed (3PM)\nsns.boxplot(x= 'WindSpeed3pm', data = data, palette = 'Set2', ax = axes[3])\naxes[3].set_title(\"\")\n\nplt.tight_layout()","f843186a":"fig, axes = plt.subplots(4,1, figsize=(10, 10), sharey=False)\nfig.suptitle('Distribution of (some) continuous variables')\n\n# Rainfall\nsns.histplot(x= 'Rainfall', data = data, palette = 'Set2', ax = axes[0], bins = 10)\naxes[0].set_title(\"\")\n\n# Evaporation\nsns.histplot(x= 'Evaporation', data = data, palette = 'Set2', ax = axes[1], bins = 10)\naxes[1].set_title(\"\")\n\n# Windspeed (9AM)\nsns.histplot(x= 'WindSpeed9am', data = data, palette = 'Set2', ax = axes[2], bins = 10)\naxes[2].set_title(\"\")\n\n# Windspeed (3PM)\nsns.histplot(x= 'WindSpeed3pm', data = data, palette = 'Set2', ax = axes[3], bins = 10)\naxes[3].set_title(\"\")\n\nplt.tight_layout()","2c4f3156":"# Calculating IQR, Upper and Lower bounds for all four columns\n\nfor column in ['Rainfall', 'Evaporation', 'WindSpeed9am', 'WindSpeed3pm']:\n    IQR = data[column].quantile(0.75) - data[column].quantile(0.25)\n    Lower_fence = data[column].quantile(0.25) - (IQR * 3)\n    Upper_fence = data[column].quantile(0.75) + (IQR * 3)\n    print(f'{column} outliers are values < {round(Lower_fence,2)} or > {round(Upper_fence,2)}')","79bfa6b7":"print(f\"Number of missing values in response variable: {data['RainTomorrow'].isna().sum()}\")\n\ndata = data.dropna(subset = ['RainTomorrow'])\n\nprint(f\"Number of missing values in response variable after dropping NA values: {data['RainTomorrow'].isna().sum()}\")","b04b0323":"from sklearn.model_selection import train_test_split\n\nX = data.drop(['RainTomorrow'], axis = 1)\ny = data['RainTomorrow']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nprint(f\"The training sets have the following shapes: {X_train.shape, y_train.shape}\")\nprint(f\"The testing sets have the following shapes: {X_test.shape, y_test.shape}\")","fd396820":"# Listing out categorical columns and their missing values\n# This doesn't include 'Year', 'Month', and 'Day' because they're integers\n\ncategorical_cols = list(X_train.select_dtypes(include=['object']).columns)\nX_train[categorical_cols].isna().sum()","4849b0f1":"# Filling in missing values in X_train with the mode.\nfor i in categorical_cols:\n    X_train[i].fillna(X_train[i].mode()[0], inplace=True)\n    X_test[i].fillna(X_test[i].mode()[0], inplace=True)\n    \n# Checking missing values in X_train\nX_train[categorical_cols].isna().sum()","6118fe7e":"# Checking missing values in X_test\nX_test[categorical_cols].isna().sum()","9841d419":"# Encoding categorical variables\nimport category_encoders as ce\n\nonehotencoder = ce.BinaryEncoder(cols=['RainToday'])\nX_train = onehotencoder.fit_transform(X_train)\nX_test = onehotencoder.fit_transform(X_test)\n\nX_train.head()","d90f9f6a":"continuous_cols = list(X_train.select_dtypes(include=['float64']).columns)\nX_train[continuous_cols].isna().sum()","5198adab":"# Fill in missing values with median\n\nfor column in continuous_cols:\n    X_train[column].fillna(X_train[column].median(), inplace = True)\n    X_test[column].fillna(X_test[column].median(), inplace = True)\n    \n# Checking missing values \nX_train.isna().sum()","db74be7b":"# Checking missing values \nX_test.isna().sum()","d6669bf9":"# Removing outliers in certain continous columns\ndef upper_outlier(df, variable, top):\n    return np.where(df[variable]>top, top, df[variable])\ndef lower_outlier(df, variable, bot):\n    return np.where(df[variable]<bot, bot, df[variable])\n\nfor X_df in [X_train, X_test]:\n    X_df['Rainfall'] = upper_outlier(X_df, 'Rainfall', 3.2)\n    X_df['Rainfall'] = lower_outlier(X_df, 'Rainfall', -2.4)\n    \n    X_df['Evaporation'] = upper_outlier(X_df, 'Evaporation', 21.8)\n    X_df['Evaporation'] = lower_outlier(X_df, 'Evaporation', -11.8)\n\n    X_df['WindSpeed9am'] = upper_outlier(X_df, 'WindSpeed9am', 55.0)\n    X_df['WindSpeed9am'] = lower_outlier(X_df, 'WindSpeed9am', -29.0)\n\n    X_df['WindSpeed3pm'] = upper_outlier(X_df, 'WindSpeed3pm', 57.0)\n    X_df['WindSpeed3pm'] = lower_outlier(X_df, 'WindSpeed3pm', -20.0)\n\nX_train.describe().apply(round)","10913951":"# Include the other columns that are to be encoded (outlined above)\n\nX_train = pd.concat([X_train[continuous_cols], X_train[['RainToday_0', 'RainToday_1']],\n                     pd.get_dummies(X_train.Location), \n                     pd.get_dummies(X_train.WindGustDir, prefix = 'WGD'),\n                     pd.get_dummies(X_train.WindDir9am, prefix = 'WD9am'),\n                     pd.get_dummies(X_train.WindDir3pm, prefix = 'WD3pm')], axis=1)\n\nX_test = pd.concat([X_test[continuous_cols], X_test[['RainToday_0', 'RainToday_1']],\n                     pd.get_dummies(X_test.Location), \n                     pd.get_dummies(X_test.WindGustDir, prefix = 'WGD'),\n                     pd.get_dummies(X_test.WindDir9am, prefix = 'WD9am'),\n                     pd.get_dummies(X_test.WindDir3pm, prefix = 'WD3pm')], axis=1)","19d2ca01":"y_test.isna().sum()","d7cb81a5":"y_train.isna().sum()","5738d92b":"X_train.describe()","825784f4":"# Scaling using MinMax Scaler\nfrom sklearn.preprocessing import MinMaxScaler\n\ncols = list(X_train.columns)\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_train = pd.DataFrame(X_train, columns=[cols])\n\nX_test = scaler.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=[cols])\n\nX_train.describe()","7bcd7fb4":"X_train.describe()","cbd1a242":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Initiatlize the model\nlogreg = LogisticRegression(solver='liblinear', random_state = 0)\n\n# Fit the model\nlogreg.fit(X_train, y_train)\n\n# Predict data points \ny_pred_test = logreg.predict(X_test)\n\n# Print accuracy scores\nprint(f'Model accuracy score: {round(accuracy_score(y_test, y_pred_test) * 100, 2)}%')\n","fb842c3d":"# Additional Metrics\n\nprint(f'Training set score: {round(logreg.score(X_train, y_train) * 100, 2)}%')\nprint(f'Test set score: {round(logreg.score(X_test, y_test) * 100, 2)}%')\n\n# Calculating null accuracy\nnull_accuracy = (y_test.value_counts()[0]\/(y_test.value_counts()[0]+y_test.value_counts()[1]))\nprint(f'Null Accuracy score: {round(null_accuracy * 100, 2)}%')","b4b62664":"# Initiatlize the model with C=100\nlogreg_c100 = LogisticRegression(C=100, solver='liblinear', random_state = 0)\n\n# Fit the model\nlogreg_c100.fit(X_train, y_train)\n\nprint(f'Training set score: {round(logreg_c100.score(X_train, y_train) * 100, 2)}%')\nprint(f'Test set score: {round(logreg_c100.score(X_test, y_test) * 100, 2)}%')","99449b8e":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_test))","0cbb16ed":"It seems like there's a pre-defined set of directions (16 unique values) for columns pertaining to wind direction. There are also two dummy variables for `RainToday` and `RainTomorrow`. Lastly, there are 49 unique values in `Location`, which is assumed to be all in Australia. \n\nThe `Date` column also has a high number of unique values, which is something that will need to be adjusted before it goes into any model. Any columns with high cardinality will require some type of pre","91af55b7":"## 2.6 `WindDir3pm` Column","83520874":"At this point, it's not certain why we have missing data. Because of this reason, we should be wary of simply coding them as the mode or something similar. Deleting them completely may also be a risk, as it can introduce bias. However, that may be the best solution we have for now.","3f155391":"## 2.7 `RainToday` Column","d91751ac":"Increasing $C$ doesn't seem to have a large impact. I recognize that employing a hyperparamter optimization technique here will help me find an ideal $C$ value.","595b4e3f":"# 4 Missing Values\n\nFor this dataset, it is fair to assume that the missing values are missing at random. (MCAR, Missing Completely At Random). This means that we can either impute the missing values with the mean value or the median value. Because we have shown that this dataset is prone to outliers, inputing the missing values with the median value seems to be a better approach. \n\nWe should only be filling in these missing values over the training dataset and not the train dataset, so lets split the data first and apply these measures along with all the other things listed in sections 2 and 3.\n\nRegarding the response variable,it's not certain why we have missing response variable data. Because of this reason, we should be wary of simply coding them as the mode or something similar. Deleting them completely may also be a risk, as it can introduce bias. However, that may be the best solution we have for now.","74b136fe":"# 8 Response Variable","bbb4e492":"What we have now is 48 columns that represent our `Location` column. Instead of having one column that explains what location the data is from, now we have a dummy variable that represents this. This means that in every row of data, only one row will have a $1$ that denotes where the `Location` is. Additionally, if everything is $0$, this means that it is referring to 'Albany'. See [here](https:\/\/www.dataindependent.com\/pandas\/pandas-get-dummies\/) for more information on `drop_first = True` parameter.\n\n## 2.4 `WindGustDir` Column","46a0a040":"All four of these columns are heavily right-skewed, so we should employ a way to remove outliers using the IQR range.","4dcd8447":"Based on the same thought process, it is best to do One-Hot Encoding.","faa480d8":"We can employ the same type of One-Hot encoding, but implement an additional value `NaN` for missing values. The reason why we are doing this is because we are not sure why there are missing values. It is best to leave it in. ","ed60976a":"## 2.3 `Location` Column","32e25ae4":"# 2. Categorical Variables\nLets take a quick look at our categorical variables","9bf7f193":"The distribution of these value counts are quite nice. It seems that all we need to do is to capture this data using dummy variables. The best way to do this is to employ One-Hot Encoding","9fd864b3":"Lets look at a recap of all our columns, and what we are going to do with each one:\n\n`Date` - turn into \u2018year\u2019, \u2018month\u2019, \u2018day\u2019\\\n`Location` - one-hot encoding, drop first, no missing data\\\n`MinTemp` - \\\n`MaxTemp`\\\n`Rainfall` - remove outliers < -2.4 or > 3.2\\\n`Evaporation` - remove outliers < -11.8 or > 21.8\\\n`Sunshine`\\\n`WindGustDir` - one-hot encoding, drop first, NaN for missing data\\\n`WindGustSpeed`\\\n`WindDir9am` - one-hot encoding, drop first, NaN for missing data\\\n`WindDir3pm` - one-hot encoding, drop first, NaN for missing data\\\n`WindSpeed9am` - remove outliers < -29.0 or > 55.0\\\n`WindSpeed3pm` - remove outliers < -20.0 or > 57.0\\\n`Humidity9am`\\\n`Humidity3pm`\\\n`Pressure9am`\\\n`Pressure3pm`\\\n`Cloud9am`\\\n`Cloud3pm`\\\n`Temp9am`\\\n`Temp3pm`\\\n`RainToday` - one-hot encoding, drop first, NaN for missing data\\\n`RainTomorrow`\\\n\\\nNow, we need to deal with the missing values.","2f2bd230":"The `Location` column does not seem to have a huge issue, although we will have to do more digging to see. \n\n`RainTomorrow` will be the response variable, as that is the column we are trying to predict.\n\n## 2.1 Problems with Categorical Variables\n\nBefore doing any feature engineering, lets look at the number of missing values in the categorical columns\n","b2d2d72e":"## 2.5 `WindDir9am` Column","66858819":"A little less missing values compared to the previous few columns. Lets apply the same type of encoding as well.","18a533f3":"We see that there are a little under 150,000 rows in 23 columns. Lets take a sneak peek at the data:","5d537d07":"In fact, there are a lot of outliers in these four columns. Before removing any outliers, I should check how these columns are distributed. Any type of drastic skew will change the way I remove these outliers.","4c9619dc":"The null accuracy is significantly lower than that of our model, which is a great sign - it means that our model is better than if we predicted the most popular class every time. In other words, our model is better than nothing!\n\nThe training and test set are quite comparable, which means that there are little signs of overfitting. I may want to consider increasing the value of $C$ for a more flexible model.","378981e9":"# 10 Model Training and Results","78a4813c":"# 9 Scaling","f9678fb3":"The classification report provides a great summary of our model. The precision is the proportion of correctly predicted positive outcomes; the Recall is the proportion of correctly predicted actual positives; F1 score is the harmonic mean of precision and r ecall. Because we're only using one model, the F1 score is quite irrelevant in this case. \n\nOverall, I'm quite happy with the results. The model has a model accuracy score of 85%, which means that it is an adequate model in predicting the raining status of given locations in Australia. Luckily, it seems that the model is predicting 'no rain tomorrow' more than 'rain tomorrow'. \n\nThe model does not show any signs of overfitting, which is also a great sign. I would recommend the following next steps for the model that can potentially improve its performance:\n\n* k-fold Cross Validation\n* Hyperparameter Optimization \n* Consider adjusting the threshold level. ","62aec91d":"# 1. Introduction\n\nIn this notebook, I will perform some EDA and feature engineering before fitting a logistic regression model. The focus of this notebook is more for the EDA and feature engineering process. I recognize that there are additional opportunities to explore that can potentially improve my model, like k-Fold Cross Validation or Hyperparameter Optimization methods. I may come back to this notebook to execute these opportunities, but for now the focus is on the data pre-processing stage. \n\nThe notebook structure is listed in the Table of Contents. I first go through each column and list out potential problems or actions I want to take. This includes removing outliers or dealing with missing values. Afterwards, I will execute these actions after I split my data to training and test sets. I then apply scaling (min-max scaling) to my features before fitting and training a logistic regression model. This notebook concludes with a set of results that I am quite happy with (~85% accuracy with no signs of overfitting or underfitting) and further steps that can be taken to potentially increase model performance.\n","916dccdd":"This is quite similar to our previous column, where we should employ One-Hot encoding while taking missing values into account.","462c86c5":"# 6 Categorical Features (Feature Engineering)","2acc255f":"There seems to be a (relatively) high number of missing values in `WindGustDir` and `WindDir9am`. We will deal with these afterwards. For now, we see that there are no missing values with `Date`, which is a good sign for our next steps in feature engineering.\n\n## 2.2 `Date` Column\n\nAs mentioned, we want to do some feature engineering to lower the cardinality of the `Date` column. It seems reasonable to break the `Date` column down into:\n* `Year`\n* `Month`\n* `Day`","647d51ee":"# 3 Continous Variables\nNow, lets look at the continous variables.","30656bd8":"This model accuracy is quite high, although we cannot make any inferences from this yet. For example, this might be a high score because our model is overfitting. We should not only test for overfitting, but also compare this to the null accuracy (when the model predicts the most frequent class)","5e4ad929":"Based on the table above, there may be outliers in `Rainfall`, `Evaporation`,`Windspeed9am` and `Windspeed3pm`. This is inferred from the difference between the row `max` and the other rows, and how far it is away from `75%`. Lets visualize this to further explore this intuition.\n\n## 3.1 Outliers in `Rainfall`, `Evaporation`, `Windspeed9am`, `Windspeed3pm` \n","e771022c":"# 5 Splitting Data into Training and Test Sets","15f1ccb3":"Note that the binary encoder has created an additional column: `RainToday_0` and `RainToday_1`. \n\n# 7. Continuous Features (Feature Engineering)"}}