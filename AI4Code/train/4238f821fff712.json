{"cell_type":{"c9e78fff":"code","15f10380":"code","a86ee7bd":"code","79e2292b":"code","72c3b489":"code","3767ca35":"code","0bf00eee":"code","5fd0b3db":"code","19c592cb":"code","26a95dbb":"code","06129268":"code","83c474b5":"code","e1d44b4d":"markdown","d89b1cd2":"markdown","36588295":"markdown","4b50585a":"markdown","6d93250f":"markdown","a70f5a39":"markdown","8adb8509":"markdown","dab20efe":"markdown","1e33a4b0":"markdown","86442d74":"markdown","42aa6db1":"markdown","1010d56f":"markdown","793ece93":"markdown","db544ea6":"markdown"},"source":{"c9e78fff":"import numpy as np\nimport pandas as pd\nimport string\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\ncolors = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(colors))\nmatplotlib.rcParams['figure.figsize'] = 12, 8\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","15f10380":"train = pd.read_csv('..\/input\/emotion-dataset\/training.csv')\ntest = pd.read_csv('..\/input\/emotion-dataset\/test.csv')\nvalidation = pd.read_csv('..\/input\/emotion-dataset\/validation.csv')","a86ee7bd":"labels_dict = {0:'sadness', 1:'joy', 2:'love', 3:'anger', 4:'fear', 5:'surprise'}\ntrain['description'] = train['label'].map(labels_dict )\ntrain.head()\n","79e2292b":"train['description'].value_counts(normalize=True)","72c3b489":"sns.countplot(train['description'],order = train['description'].value_counts(normalize=True).index)","3767ca35":"train['text_length'] = train['text'].astype(str).apply(len)\ntrain['text_word_count'] = train['text'].apply(lambda x: len(str(x).split()))","0bf00eee":"sns.distplot(train['text_length'])\nplt.xlim([0, 512]);\nplt.xlabel('Text Length');","5fd0b3db":"sns.boxplot(x=\"description\", y=\"text_word_count\", data=train)","19c592cb":"# Creating individual dataframes of different emotions\nsadness = train[train['label']==0]['text']\njoy = train[train['label']==1]['text']\nlove = train[train['label']==2]['text']\nanger = train[train['label']==3]['text']\nfear = train[train['label']==4]['text']\nsurprise = train[train['label']==5]['text']","26a95dbb":"# Code to calculate ngrams\n# source of code : https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","06129268":"# calculating unigrmas, bigrams and trigrams for all given emotions\n\nsad_unigrams = get_top_n_gram(sadness.values,(1,1),7)[2:]\njoy_unigrams= get_top_n_gram(joy.values,(1,1),7)[2:]\nlove_unigrams= get_top_n_gram(love.values,(1,1),7)[2:]\nanger_unigrams= get_top_n_gram(anger.values,(1,1),7)[2:]\nfear_unigrams= get_top_n_gram(fear.values,(1,1),7)[2:]\nsurprise_unigrams= get_top_n_gram(surprise.values,(1,1),7)[2:]\n\n\n\nsad_bigrams = get_top_n_gram(sadness.values,(2,2),7)[2:]\njoy_bigrams= get_top_n_gram(joy.values,(2,2),7)[2:]\nlove_bigrams= get_top_n_gram(love.values,(2,2),7)[2:]\nanger_bigrams= get_top_n_gram(anger.values,(2,2),7)[2:]\nfear_bigrams= get_top_n_gram(fear.values,(2,2),7)[2:]\nsurprise_bigrams= get_top_n_gram(surprise.values,(2,2),7)[2:]\n\n\nsad_trigrams = get_top_n_gram(sadness.values,(3,3),7)[2:]\njoy_trigrams= get_top_n_gram(joy.values,(3,3),7)[2:]\nlove_trigrams= get_top_n_gram(love.values,(3,3),7)[2:]\nanger_trigrams= get_top_n_gram(anger.values,(3,3),7)[2:]\nfear_trigrams= get_top_n_gram(fear.values,(3,3),7)[2:]\nsurprise_trigrams= get_top_n_gram(surprise.values,(3,3),7)[2:]\n","83c474b5":"fig, axes = plt.subplots(1, 3, figsize=(15, 6), sharey=True)\nfig.suptitle('Emotion : Sadness')\n\n\n\nsns.barplot( list(dict(sad_unigrams).keys()), list(dict(sad_unigrams).values()),ax=axes[0],)\naxes[0].set_title('Unigrams')\naxes[0].tick_params(labelrotation=45)\n\n\n\nsns.barplot( list(dict(sad_bigrams).keys()), list(dict(sad_bigrams).values()),ax=axes[1],)\naxes[1].set_title('Bigrams')\naxes[1].tick_params(labelrotation=45)\n\n\n\nsns.barplot( list(dict(sad_trigrams).keys()), list(dict(sad_trigrams).values()),ax=axes[2],)\naxes[2].set_title('Trigrams')\naxes[2].tick_params(labelrotation=45)\nfig.savefig('sadness.png')\n\n\n#Joy\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 6), sharey=True)\nfig.suptitle('Emotion : Joy')\n\nsns.barplot( list(dict(joy_unigrams).keys()), list(dict(joy_unigrams).values()),ax=axes[0],)\naxes[0].set_title('Unigrams')\naxes[0].tick_params(labelrotation=45)\n\nsns.barplot( list(dict(joy_bigrams).keys()), list(dict(joy_bigrams).values()),ax=axes[1],)\naxes[1].set_title('Bigrams')\naxes[1].tick_params(labelrotation=45)\n\n\nsns.barplot( list(dict(joy_trigrams).keys()), list(dict(joy_trigrams).values()),ax=axes[2],)\naxes[2].set_title('Trigrams')\naxes[2].tick_params(labelrotation=45)\nfig.savefig('joy.png')\n\n#Love\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 6), sharey=True)\nfig.suptitle('Emotion : Love')\n\nsns.barplot( list(dict(love_unigrams).keys()), list(dict(love_unigrams).values()),ax=axes[0],)\naxes[0].set_title('Unigrams')\naxes[0].tick_params(labelrotation=45)\n\nsns.barplot( list(dict(love_bigrams).keys()), list(dict(love_bigrams).values()),ax=axes[1],)\naxes[1].set_title('Bigrams')\naxes[1].tick_params(labelrotation=45)\n\n\n\nsns.barplot( list(dict(love_trigrams).keys()), list(dict(love_trigrams).values()),ax=axes[2],)\naxes[2].set_title('Trigrams')\naxes[2].tick_params(labelrotation=45)\n\n\n#Anger\nfig, axes = plt.subplots(1, 3, figsize=(15, 6), sharey=True)\nfig.suptitle('Emotion : Anger')\n\nsns.barplot( list(dict(anger_unigrams).keys()), list(dict(anger_unigrams).values()),ax=axes[0],)\naxes[0].set_title('Unigrams')\naxes[0].tick_params(labelrotation=45)\n\nsns.barplot( list(dict(anger_bigrams).keys()), list(dict(anger_bigrams).values()),ax=axes[1],)\naxes[1].set_title('Bigrams')\naxes[1].tick_params(labelrotation=45)\n\n\n\nsns.barplot( list(dict(anger_trigrams).keys()), list(dict(anger_trigrams).values()),ax=axes[2],)\naxes[2].set_title('Trigrams')\naxes[2].tick_params(labelrotation=45)\n","e1d44b4d":"## Analyzing Text Statistics\n* We can now do some statistical analysis to explore the fundamental characteristics of the text data. Some of the analyses which can be helpful are:\n    * Text length analysis: calculating the length of the text, and\n    * word frequency analysis: calculating the word count in the form of unigrams, bigrams and trigrams.","d89b1cd2":"## Visualizing the ngrams for some of the emotions","36588295":"About 33 percent of the tweets are joyful, followed by sad and angry tweets.","4b50585a":"## Tweet word count analysis\nNow let\u2019s analyze the frequency of the words per tweet per class.","6d93250f":"What next?\nNow that we have done some preliminary exploration of the dataset, the next is to use this dataset to create an emotion classifier. This could be a great project to add to your resume and you can also share your trained model with the community.","a70f5a39":"## What next?\nNow that we have done some preliminary exploration of the dataset, the next is to use this dataset to create an emotion classifier. This could be a great project to add to your resume and you can also share your trained model with the community. You can also checkout some of the notebooks below:\n\n### Helper Notebooks provided by authors\n- Here is a [notebook](https:\/\/colab.research.google.com\/drive\/1nwCE6b9PXIKhv2hvbqf1oZKIGkXMTi1X#scrollTo=t23zHggkEpc-) showing how to use it for fine-tuning a pretrained language model for the task of emotion classification.\n\n- Here is another [notebook](https:\/\/colab.research.google.com\/drive\/176NSaYjc2eeI-78oLH_F9-YV3po3qQQO?usp=sharing) which shows how to fine-tune T5 model for emotion classification along with other tasks.\n\n- Here is also a hosted [fine-tuned model](https:\/\/huggingface.co\/mrm8488\/distilroberta-base-finetuned-sentiment) on HuggingFace which you can directly use for inference in your NLP pipeline. ","8adb8509":"## Importing the dataset","dab20efe":"## Tweet length analysis\n","1e33a4b0":"Most of the tweets have an average of 15 words. Also, all the tweets appear to have the more or less the same length. Hence, the length of the tweet isn\u2019t a powerful indicator of polarity.","86442d74":"\n<div class=\"alert alert-block alert-info\">\n    \nThis is a starter notebook to showcase the [Emotion Dataset for Emotion Recognition Tasks](https:\/\/www.kaggle.com\/parulpandey\/emotion-dataset) \n    \n<\/div>","42aa6db1":"## Distribution of the labels in the training set\n\nIt\u2019ll be informative to look at the distribution of the labels. This will also give us an idea of the imbalance in the dataset, if any.","1010d56f":"The histogram above shows that the length of the tweet ranges from around 2 to 300 characters.","793ece93":"## Distribution of top n-grams\nAn [n-gram is a contiguous sequence of n items from a given sample of text or speec](https:\/\/en.wikipedia.org\/wiki\/N-gram)h. It is also a good idea to look at various n-grams to understand which words mainly occur together. For instance, we look at the distribution of unigrams, bigrams, and trigrams across emotions- sadness, anger, and love. You can replicate the same for other categories using this code.","db544ea6":"## Creating a column with label names.\nThe label column currently has integers. To make it more understandable, we\u2019ll create a new column called description containing the description of each integer in the label column."}}