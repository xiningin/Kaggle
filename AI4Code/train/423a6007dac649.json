{"cell_type":{"bd9fe2c9":"code","b9810d2a":"code","11fd9a80":"code","64b06812":"code","a99f1eeb":"code","84e42fc7":"code","ea2ec7c9":"code","e711c5ef":"code","a2101f2c":"code","1c9ea2b6":"code","6a80a560":"code","537cb052":"code","9ce9abfa":"code","7c9e09af":"code","5918fc31":"code","cfbc90df":"code","890f7916":"code","7c367d13":"code","b3321050":"code","2b583021":"code","0c805875":"code","c36a9f81":"code","51696441":"code","7867557a":"code","96e5f803":"code","fe2d8922":"code","a70b7425":"code","e24ca502":"code","c8d1599c":"code","52399cb3":"code","5f1c9232":"code","4fb5c546":"code","8552c130":"code","2730c730":"code","ec351d16":"code","1c391964":"code","537cb19a":"code","706927ab":"code","0ea97a2d":"code","5dff1706":"code","4468a0a9":"code","d4f1e456":"code","818470e4":"code","ac9032df":"code","b11cfd33":"code","08605dbf":"code","c9a4fc6e":"markdown","d4de82bc":"markdown","ddae1c57":"markdown","ef10897d":"markdown","eb62f05e":"markdown","f088042e":"markdown","6db8aa02":"markdown","141f0af8":"markdown","845bd3fe":"markdown","809de7bd":"markdown","27f02516":"markdown","d524034e":"markdown","af509954":"markdown","b24d4158":"markdown","85ea7a7a":"markdown","4ca8e70f":"markdown","0d03c88b":"markdown","7832eb5a":"markdown","89476d16":"markdown","693191b4":"markdown","c14cfa69":"markdown","6f350642":"markdown","0a8945e5":"markdown","93038029":"markdown"},"source":{"bd9fe2c9":"# Basic Pydata Libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt     \nimport seaborn as sns\nimport html\nimport unicodedata\n\n# for reproducibility , to get the same results when evry your run\nnp.random.seed(2021) \n\n\n# sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.svm import LinearSVC, SVC\n\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# ML\nimport tensorflow as tf\nimport keras.backend as K\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Concatenate\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers.merge import concatenate\n\n\n\n#string\nimport string\nimport re\n\n#nlp\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk import ngrams\n\n\nstop_words = set(stopwords.words(\"english\"))\n\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nfrom wordcloud import WordCloud, STOPWORDS\n\n## warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Graphics in retina format are more sharp and legible\n%config InlineBackend.figure_format = 'retina'","b9810d2a":"PATH = '\/kaggle\/input\/jigsaw-toxic-severity-rating\/'\ntrain = pd.read_csv(PATH + 'validation_data.csv')\ntest = pd.read_csv(PATH + 'comments_to_score.csv')\nsub = pd.read_csv(PATH + 'sample_submission.csv')","11fd9a80":"train.head()","64b06812":"rows = train.shape[0]\nrows","a99f1eeb":"train.less_toxic[0], train.more_toxic[0]","84e42fc7":"train.less_toxic[rows-1], train.more_toxic[rows-1]","ea2ec7c9":"def remove_special_chars(text):\n    sequencePattern   = r\"(.)\\1\\1+\"\n    seqReplacePattern = r\"\\1\\1\"\n    text = re.sub(sequencePattern, seqReplacePattern, text)         # Replace 3 or more consecutive letters by 2 letter.\n    text = re.sub('<.*?>+', '', text)                               # remove tages\n    return text\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\n\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text, and by defult lemmatize nouns\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text  = remove_special_chars(text)\n    text  = remove_non_ascii(text)\n    text  = remove_punctuation(text)\n    text  = to_lowercase(text)\n    text  = replace_numbers(text)\n    words = text2words(text)\n    words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    words = lemmatize_words(words)\n    words = lemmatize_verbs(words)\n\n    return ''.join(words)\n","e711c5ef":"# Make a clean text for  less_toxic\ncleaned_lees_toxic = [normalize_text(sent) for sent in train['less_toxic']]\ntrain['cleaned_lees_toxic'] = cleaned_lees_toxic\ntrain.head()","a2101f2c":"# Make a clean text for  more_toxic\ncleaned_more_toxic = [normalize_text(sent) for sent in train['more_toxic']]\ntrain['cleaned_more_toxic'] = cleaned_more_toxic\ntrain.head()","1c9ea2b6":"train.cleaned_lees_toxic[0], train.cleaned_more_toxic[0]","6a80a560":"train.cleaned_lees_toxic[rows-1], train.cleaned_more_toxic[rows-1]","537cb052":"less_toxic_score = [0] * rows\nmore_toxic_score = [1] * rows\ntrain['less_toxic_score'] = less_toxic_score\ntrain['more_toxic_score'] = more_toxic_score\n# drop the original toxic data\ntrain.drop(['less_toxic', 'more_toxic'], inplace= True, axis = 1)\ntrain.head()","9ce9abfa":"toxic_data = cleaned_lees_toxic + cleaned_more_toxic\ntarget = less_toxic_score + more_toxic_score","7c9e09af":"toxic_data[:5], target[:5]","5918fc31":"toxic_data[-5:], target[-5:]","cfbc90df":"import random\n\na = ['a', 'b', 'c']\nb = [1, 2, 3]\n\nc = list(zip(a, b))\nprint(c)\n\nrandom.shuffle(c)\n\na, b = zip(*c)\n\nprint(a)\nprint(b)\n","890f7916":"shuffled_data = list(zip(toxic_data, target))\nrandom.shuffle(shuffled_data)\ntoxic_data, target = zip(*shuffled_data)","7c367d13":"toxic_data[:5], target[:5]","b3321050":"toxic_data[-5:], target[-5:]","2b583021":"df = pd.DataFrame({'toxic_text': toxic_data,\n                  'target': target})\ndf.head()","0c805875":"df.target.value_counts()","c36a9f81":"sns.countplot(data = df, x= 'target');","51696441":"from collections import Counter","7867557a":"def freq_words(text,score, num):\n    '''\n        take the whole data, and return data which is have # of words in each sentiment has been passed\n    '''\n    words = [word for sent in text['toxic_text'][text['target'] == float(score)] for word in sent.split()]    \n    freq_words = Counter(words)\n    freq_words_sorted = sorted(freq_words.items(), key=lambda pair: pair[1], reverse=True)\n    freq_words_df = pd.DataFrame(freq_words_sorted[:num], columns=['word', 'counts'])\n    return freq_words_df\n\ndef plot_freq(data, st, num):\n    '''\n        take the data, and st refeere to kind of sentiment\n    '''\n    plt.figure(figsize=(12, 6))\n    sns.barplot(data= data , x= 'counts', y= 'word')\n    plt.title(f'Top {num} words in {st}')\n    plt.show();\n","96e5f803":"num = 30\nless_toxic_df = freq_words(df, 0, num)\nless_toxic_df.T","fe2d8922":"plot_freq(less_toxic_df, 'less toxic', num)","a70b7425":"more_toxic_df = freq_words(df, 1, num)\nmore_toxic_df.T","e24ca502":"plot_freq(more_toxic_df, 'more toxic', num)","c8d1599c":"def get_top_n_gram(corpus, score,  n_gram, top_n=None):\n    # list of splited senteces, which is just list of words\n    text = [word for sent in corpus['toxic_text'][corpus['target'] == float(score)] for word in sent.split()]    \n\n    grams = ngrams(text, n_gram)\n    grams = (' '.join(g) for g in grams)\n    num_of_grams = [words for words in grams]\n    freq_words = Counter(num_of_grams)\n    freq_words_sorted = sorted(freq_words.items(), key=lambda pair: pair[1], reverse=True)\n    freq_words_df = pd.DataFrame(freq_words_sorted[:top_n], columns=['word', 'counts'])\n    return freq_words_df[:top_n]\n","52399cb3":"less_toxic_2_gram_df = get_top_n_gram(df, 0, 2, num)\nless_toxic_2_gram_df.T","5f1c9232":"plot_freq(less_toxic_2_gram_df, 'less toxic', num)","4fb5c546":"more_toxic_2_gram_df = get_top_n_gram(df, 1, 2, num)\nmore_toxic_2_gram_df.T","8552c130":"plot_freq(less_toxic_2_gram_df, 'more toxic', num)","2730c730":"from wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(cleaned_lees_toxic))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Less Toxic',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='black',\n                        width=600,\n                        height=400).generate(\" \".join(cleaned_more_toxic))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('More Toxic',fontsize=40);","ec351d16":"df.head()","1c391964":"# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['toxic_text'], df['target'], test_size = 0.2,  random_state=42)\nlen(X_train), len(y_train), len(X_test), len(y_test)\n","537cb19a":"tf_idf = TfidfVectorizer(analyzer= 'word', max_features= 10000, ngram_range= (1, 1))\nX_train = tf_idf.fit_transform(X_train)\nX_test = tf_idf.transform(X_test)\nX_train.shape, X_train.shape","706927ab":"def model_Evaluate(model):\n    \n    # Predict values for Test dataset\n    y_pred = model.predict(X_test)\n    \n    acc = accuracy_score(y_test, y_pred)\n\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n    \n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    categories  = ['Non Toxic','Toxic']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() \/ np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n    return acc\n","0ea97a2d":"%%time\nlr = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\nlr.fit(X_train, y_train)\nlr_acc = model_Evaluate(lr)","5dff1706":"%%time\nSVCmodel = LinearSVC(C= 1, loss = 'hinge')\nSVCmodel.fit(X_train, y_train)\nSVC_acc = model_Evaluate(SVCmodel)","4468a0a9":"test.head()","d4f1e456":"X_test_sub = tf_idf.transform(test['text'])\npreds = lr.predict_proba(X_test_sub)[:,1]","818470e4":"preds = [int(p >= 0.5) for p in preds]\ntest['score'] = preds","ac9032df":"submission = test\nsubmission.head()","b11cfd33":"submission.to_csv('submission.csv', index=False)","08605dbf":"%%time\n# SVCmodel = LinearSVC(C= 1, loss = 'hinge')\nSVM = SVC(kernel = 'linear', gamma = 'auto', probability = True)\nSVM.fit(X_train, y_train)\nSVM_acc = model_Evaluate(SVM)","c9a4fc6e":"## 2- LinearSVC","d4de82bc":"### Two-grams for less toxic data\n","ddae1c57":"### Explanation:\n\n**The data tells when we useing uni-gram, there's a huge different in words for each less and more toxic data, but when using bi-grams there's no different there!!!**\n\n**So in our TF-IDF we will pass the i for `ngram_range` in `TfidfVectorizer`**\n\n","ef10897d":"### Frequent words for each `less toxic data`","eb62f05e":"# Modeling","f088042e":"**Shuffel the data**","6db8aa02":"## Distribution of top n-grams","141f0af8":"**Evaluate Model Function:**","845bd3fe":"## Word Cloud","809de7bd":"# EDA","27f02516":"## Most frequent words\n","d524034e":"## 3- SVC\nIt takes a long time for get good results!","af509954":"# Data Preprocessing","b24d4158":"# Importing libs","85ea7a7a":"**Here we will make two columns target \"score\" for each toxic level, and we indicate each one with `0 for less_toxic` and `1 for more_toxic`.**","4ca8e70f":"**Makes a new DataFrame for shuffedled data**","0d03c88b":"# Submission","7832eb5a":"## Feature Engineering","89476d16":"# Importing data","693191b4":"**TF-IDF Vectorizer**","c14cfa69":"**We will make a `toxic_data` column for cleaned_less_toxic and cleaned_more_toxic and make also a `target`  column which is indicate for each toxic score.**\n\n**Shuffel the `toxic_data` and `target` with the same random state for makes better in modeling.**","6f350642":"## 1- Logistic Regression","0a8945e5":"### Frequent words for each `more toxic data`","93038029":"### Two-grams for more toxic data\n"}}