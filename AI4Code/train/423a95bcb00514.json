{"cell_type":{"147e00ef":"code","5864b5ed":"code","7118e81b":"code","f0fc6e59":"code","a5da5b8e":"code","ea2a242a":"code","af01cfc9":"code","9c5317bf":"code","84722d7c":"code","2e270ccf":"code","76de51d2":"code","dce788b3":"code","7d1806e5":"code","9ef02d88":"code","695acfc7":"code","8c6d572e":"code","2a06a391":"code","5748c446":"code","bc299884":"code","1bca5b0d":"code","f83c6d17":"code","2583ca6f":"code","3e42d93c":"code","e22ec19e":"code","7ebb98a7":"code","b1eb77cd":"code","aed68bb6":"code","6c8985ef":"code","a3600143":"code","3099b6f7":"code","1893bfa0":"code","5b673d73":"code","e152589a":"code","cb273a8b":"code","73e45c53":"code","4e75d41f":"code","7073ced1":"code","e6cbe2c8":"code","8048db7e":"code","44506188":"code","a26a4ee5":"code","abbaf717":"code","86792bb3":"code","d65dbb94":"code","a36b40e2":"code","5ffcbe53":"code","4c8475d0":"code","e51453bf":"code","e9c3f736":"code","48f74d2a":"code","8e421e54":"code","6320247b":"code","728d42c3":"code","29b4b0c9":"code","b7a18451":"code","a14fd2c7":"markdown","3f9f56a3":"markdown","f71344e7":"markdown","c9f03b14":"markdown","8390f25a":"markdown","d67f0291":"markdown","f36e121a":"markdown","e714971c":"markdown","12fe5d9d":"markdown","b03f3aac":"markdown","422462b9":"markdown","23b7faa0":"markdown","8e7072a6":"markdown","016337e3":"markdown","79d6fa54":"markdown","af715e90":"markdown","03a82689":"markdown","b80723f2":"markdown","fa022548":"markdown","7a5f0067":"markdown","b99c5964":"markdown","279dccff":"markdown","fc00e6a9":"markdown"},"source":{"147e00ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5864b5ed":"boston=pd.read_csv('..\/input\/boston-housing\/boston_housing.csv')\nboston.head()","7118e81b":"boston.isnull().sum()","f0fc6e59":"#Show missing variable\nmsno.matrix(boston);","a5da5b8e":"def MissingUniqueStatistics(df):\n    variable_name_list = []\n    total_entry_list = []\n    data_type_list = []\n    unique_values_list = []\n    number_of_unique_values_list = []\n    missing_value_number_list = []\n    missing_value_ratio_list = []\n    mean_list=[]\n    std_list=[]\n    min_list=[]\n    Q1_list=[]\n    Q2_list=[]\n    Q3_list=[]\n    max_list=[]\n\n    df_statistics = boston.describe().copy()\n\n    for col in boston.columns:\n        variable_name_list.append(col)\n        total_entry_list.append(boston.loc[:,col].shape[0])\n        data_type_list.append(boston.loc[:,col].dtype)\n        unique_values_list.append(list(boston.loc[:,col].unique()))\n        number_of_unique_values_list.append(len(list(boston.loc[:,col].unique())))\n        missing_value_number_list.append(boston.loc[:,col].isna().sum())\n        missing_value_ratio_list.append(round((boston.loc[:,col].isna().sum()\/boston.loc[:,col].shape[0]),4))\n        \n        try:\n            mean_list.append(df_statistics.loc[:,col][1])\n            std_list.append(df_statistics.loc[:,col][2])\n            min_list.append(df_statistics.loc[:,col][3])\n            Q1_list.append(df_statistics.loc[:,col][4])\n            Q2_list.append(df_statistics.loc[:,col][5])\n            Q3_list.append(df_statistics.loc[:,col][6])\n            max_list.append(df_statistics.loc[:,col][7])\n    \n        except:\n            mean_list.append('NaN')\n            std_list.append('NaN')\n            min_list.append('NaN')\n            Q1_list.append('NaN')\n            Q2_list.append('NaN')\n            Q3_list.append('NaN')\n            max_list.append('NaN')\n\n\n    data_info_df = pd.DataFrame({'Variable': variable_name_list, \n                               '#_Total_Entry':total_entry_list,\n                               '#_Missing_Value': missing_value_number_list,\n                               '%_Missing_Value':missing_value_ratio_list,\n                               'Data_Type': data_type_list, \n                               'Unique_Values': unique_values_list,\n                               '#_Unique_Values':number_of_unique_values_list,\n                               'Mean':mean_list,\n                               'STD':std_list,\n                               'Min':min_list,\n                               'Q1':Q1_list,\n                               'Q2':Q2_list,\n                               'Q3':Q3_list,\n                               'Max':max_list\n                               })\n\n    data_info_df = data_info_df.set_index(\"Variable\", inplace=False)\n\n    \n    return data_info_df.sort_values(by='%_Missing_Value', ascending=False)\n\n","ea2a242a":"data_info = MissingUniqueStatistics(boston)\ndata_info","af01cfc9":"#Target Value Distribution\nplt.subplots(figsize=(12, 9))\nsns.distplot(boston['medv'], fit = stats.norm)\n\n(mu, sigma) = stats.norm.fit(boston['medv'])\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma = $ {: .2f})'.format(mu, sigma)], loc = 'best')\nplt.ylabel('Frekans')\n\n#Probability Plot\nfig = plt.figure()\nstats.probplot(boston['medv'], plot = plt)\nplt.show()","9c5317bf":"boston.tail()","84722d7c":"boston.corr()","2e270ccf":"#High Correlation between features\ncorr_matrix = boston.corr().abs()\nhigh_corr_var=np.where(corr_matrix>0.8)\nhigh_corr_var=[(corr_matrix.columns[x],corr_matrix.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\nhigh_corr_var","76de51d2":"#High Correlation with Dependent Value\ncorr = boston.corr().abs()\nk = 10 #number of variables for heatmap\ncols = corr.nlargest(k, 'medv')['medv'].index\ncm = np.corrcoef(boston[cols].values.T)\nsns.set(font_scale=1.25)\nfig, ax = plt.subplots(figsize=(10,10))       \nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},\n                 yticklabels=cols.values, xticklabels=cols.values,cmap='RdYlGn')\nplt.show()","dce788b3":"#variables that are highly correlated with each other except the dependent variable\ncorrelated_features = set()\ncorrelation_matrix = boston.loc[:, boston.columns != 'medv'].corr()\n\nfor i in range(len(correlation_matrix .columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)\n            \ncorrelated_features","7d1806e5":"#Correlation with dependent variable\ncor_target = abs(boston.corr()[\"medv\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.7]\nrelevant_features","9ef02d88":"sns.pairplot(boston,palette='coolwarm',height=1.5,corner=True,plot_kws=dict(marker=\"+\", linewidth=1),diag_kws=dict(fill=False));","695acfc7":"pp = sns.pairplot(data=boston,\n                  y_vars=['medv'],\n                  x_vars=['crim','zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','black','lstat'],\n                  plot_kws=dict(marker=\"D\", linewidth=1))\npp.fig.set_size_inches(20,3)","8c6d572e":"from statsmodels.stats.outliers_influence import variance_inflation_factor \nfrom statsmodels.tools.tools import add_constant\n\n\n# the independent variables set \nX = boston.iloc[:,:-1]\nX = add_constant(X)\n# VIF dataframe \nvif_data = pd.DataFrame() \nvif_data[\"feature\"] = X.columns \n  \n# calculating VIF for each feature \nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) \n                          for i in range(len(X.columns))] \n  \nprint(vif_data)","2a06a391":"#Box Plot Each Numeric Features in Data\nfor col in boston.columns:\n    sns.boxplot(data = [boston[col]], linewidth = 1, width = 0.5) \n    plt.ylabel(col)\n    plt.title(\"IQR\")\n    plt.show()","5748c446":"from sklearn.model_selection import train_test_split\nX=boston.iloc[:,:-1]\ny=boston.iloc[:,-1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","bc299884":"from sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom scipy import stats\n\nX2 = sm.add_constant(X)\nest = sm.OLS(y, X2)\nest2 = est.fit()\nprint(est2.summary())","1bca5b0d":"#Partial Regression Plots\nfig = sm.graphics.plot_partregress_grid(est2)\nfig.set_size_inches(15.5, 18.5)\nfig.tight_layout(pad=1.0)","f83c6d17":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\n\nregressor.fit(X_train,y_train)\ny_pred= regressor.predict(X_test)\n\nprint(y_pred[0:5])","2583ca6f":"print('coefficients of all features (\u00df1,\u00df2,...): ' + str(regressor.coef_))\nprint('intercept of model (\u00df0): ' + str(regressor.intercept_))","3e42d93c":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\ntest_set_rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\ntest_set_r2 = r2_score(y_test, y_pred)\n\nprint(test_set_rmse)\nprint(test_set_r2)","e22ec19e":"r_squared = 0.71\nplt.scatter(y_test,y_pred)\nplt.xlabel('Actual values')\nplt.ylabel('Predicted values')\n\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_pred, 1))(np.unique(y_test)))\n\nplt.text(7,0.5, 'R-squared = %0.2f' % r_squared)\nplt.show()","7ebb98a7":"model = LinearRegression()\nfit_model = model.fit(X_train, y_train)\npredictions = fit_model.predict(X_test)\n\ndef get_prediction_interval(prediction, y_test, test_predictions, pi=.95):    \n#get standard deviation of y_test\n    sum_errs = np.sum((y_test - test_predictions)**2)\n    stdev = np.sqrt(1 \/ (len(y_test) - 2) * sum_errs)\n#get interval from standard deviation\n    one_minus_pi = 1 - pi\n    ppf_lookup = 1 - (one_minus_pi \/ 2)\n    z_score = stats.norm.ppf(ppf_lookup)\n    interval = z_score * stdev\n#generate prediction interval lower and upper bound\n    lower, upper = prediction - interval, prediction + interval\n    return lower, prediction, upper\nprint('prediction interval of first value :')\nget_prediction_interval(predictions[0], y_test, predictions)","b1eb77cd":"#1-\nresiduals = y_test-y_pred\nplt.plot(X_test,residuals, 'o', color='darkblue')\nplt.title(\"Residual Plot\")\nplt.xlabel(\"Independent Variable\")\nplt.ylabel(\"Residual\");","aed68bb6":"#2-\nfrom yellowbrick.regressor import ResidualsPlot\nfrom sklearn.linear_model import Ridge\n\nmodel = Ridge()\nvisualizer = ResidualsPlot(model)\n\nvisualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)  \nvisualizer.show();                ","6c8985ef":"visualizer = ResidualsPlot(model, hist=False, qqplot=True)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show();","a3600143":"fig = sm.graphics.influence_plot(est2, criterion=\"cooks\")","3099b6f7":"import statsmodels.formula.api as smf\n\n#fit regression model\nfit = smf.ols('medv ~ crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat', data=boston).fit()\n\n#view model summary\nprint(fit.summary())","1893bfa0":"from statsmodels.compat import lzip\nimport statsmodels.stats.api as sms\n\n#perform Bresuch-Pagan test\nnames = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(fit.resid, fit.model.exog)\n\nlzip(names, test)","5b673d73":"#Drop RAD\nboston.drop(columns=['rad'],inplace=True)","e152589a":"#Drop Outlier according to LOF\nfrom sklearn.neighbors import LocalOutlierFactor\nclf=LocalOutlierFactor(n_neighbors=20)\n\npred=clf.fit_predict(boston)\npred","cb273a8b":"boston=boston[pred==1]","73e45c53":"#Min-Max Scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\nmms = MinMaxScaler()\n\nboston[['crim', 'zn', 'indus', 'chas',\n      'nox', 'rm', 'age', 'dis', 'tax',\n       'ptratio', 'black', 'lstat']] = mms.fit_transform(boston[['crim', 'zn', 'indus', 'chas',\n                                                                           'nox', 'rm', 'age', 'dis', 'tax',\n                                                                           'ptratio', 'black', 'lstat']])","4e75d41f":"\"\"\"#Standardization\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nboston[['crim', 'zn', 'indus', 'chas',\n      'nox', 'rm', 'age', 'dis', 'tax',\n       'ptratio', 'black', 'lstat', 'medv']] = scaler.fit_transform(boston[['crim', 'zn', 'indus', 'chas',\n                                                                           'nox', 'rm', 'age', 'dis', 'tax',\n                                                                           'ptratio', 'black', 'lstat', 'medv']])\"\"\"","7073ced1":"boston.head()","e6cbe2c8":"X=boston.iloc[:,:-1]\nY=boston[['medv']]","8048db7e":"#LassoCV Feature Selection\nfrom sklearn.linear_model import LassoCV\n\nreg=LassoCV(cv=10)\nreg.fit(X,Y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,Y))\ncoef = pd.Series(reg.coef_, index = X.columns)","44506188":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","a26a4ee5":"imp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\");","abbaf717":"X.drop(columns=['crim'],inplace=True)","86792bb3":"x, X_test, y, y_test = train_test_split(X, Y, train_size=0.8,test_size=0.2, random_state=101)","d65dbb94":"X_train, X_valid, y_train, y_valid = train_test_split(x, y, train_size=0.75,test_size=0.25, random_state=101)","a36b40e2":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\ntrain_errors = []\nvalid_errors = []\nparam_range = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n\nfor max_depth in param_range:\n    random_forest = RandomForestRegressor(max_depth=max_depth, n_estimators=100, random_state=1)\n    random_forest.fit(X_train, y_train)\n    \n    train_errors.append(np.sqrt(mean_squared_error(y_train, random_forest.predict(X_train))))\n    valid_errors.append(np.sqrt(mean_squared_error(y_valid, random_forest.predict(X_valid))))\n    \n\nplt.xlabel('max_depth')\nplt.ylabel('root mean_squared_error')\nplt.plot(param_range, train_errors, label=\"train rmse\")\nplt.plot(param_range, valid_errors, label=\"validation rmse\")\nplt.legend()\nplt.show()","5ffcbe53":"random_forest = RandomForestRegressor(max_depth=4, n_estimators=100, random_state=1)\nrandom_forest.fit(X_train, y_train)","4c8475d0":"root_mean_squared_error = np.sqrt(mean_squared_error(y_train, random_forest.predict(X_train)))\nprint(root_mean_squared_error)\n\ntrain_set_r2 = r2_score(y_train, random_forest.predict(X_train))\nprint(train_set_r2)","e51453bf":"root_mean_squared_error = np.sqrt(mean_squared_error(y_valid, random_forest.predict(X_valid)))\nprint(root_mean_squared_error)\n\nvalid_set_r2 = r2_score(y_valid, random_forest.predict(X_valid))\nprint(valid_set_r2)","e9c3f736":"root_mean_squared_error = np.sqrt(mean_squared_error(y_test, random_forest.predict(X_test)))\nprint(root_mean_squared_error)\n\ntest_set_r2 = r2_score(y_test, random_forest.predict(X_test))\nprint(test_set_r2)","48f74d2a":"#Random Forest Regressor with CV\nfrom sklearn.model_selection import cross_val_score\ncross_val_scores = cross_val_score(RandomForestRegressor(max_depth=4, n_estimators=100, random_state=1),\\\n                                   X_test, y_test, scoring='neg_mean_squared_error', cv=5)\ncross_val_scores = np.sqrt(np.abs(cross_val_scores)) \nprint(cross_val_scores)\nprint(\"mean:\", np.mean(cross_val_scores))","8e421e54":"from sklearn.model_selection import validation_curve\ntrain_scores, valid_scores = validation_curve(RandomForestRegressor(n_estimators=100, random_state=1), X_train, y_train, \"max_depth\",\n                                               param_range, scoring='neg_mean_squared_error', cv=5)\ntrain_scores = np.sqrt(np.abs(train_scores))\nvalid_scores = np.sqrt(np.abs(valid_scores))\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\nvalid_scores_mean = np.mean(valid_scores, axis=1)\n\nplt.title(\"Validation Curve with Random Forest\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"RMSE\")\nplt.plot(param_range, train_scores_mean, label=\"train rmse\")\nplt.plot(param_range, valid_scores_mean, label=\"validation rmse\")\n\nplt.legend()\nplt.show()","6320247b":"from sklearn.linear_model import LinearRegression, Ridge \nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor \nimport xgboost as xgb \nimport lightgbm as lgb\n\nmods = [LinearRegression(),Ridge(),GradientBoostingRegressor(),\n  RandomForestRegressor(),BaggingRegressor(),\n  xgb.XGBRegressor(), lgb.LGBMRegressor()]\n\nfitted = [mod.fit(X_train,y_train) for mod in mods]\n\nmodel_df = pd.DataFrame({\n    'Model': [type(i).__name__ for i in fitted],\n    'Score': [i.score(X_train,y_train) for i in fitted]\n    })\n\nmodel_df","728d42c3":"mods = [LinearRegression(),Ridge(),GradientBoostingRegressor(),\n  RandomForestRegressor(),BaggingRegressor(),\n  xgb.XGBRegressor(), lgb.LGBMRegressor()]\n\nfitted = [mod.fit(X_train,y_train) for mod in mods]\n\nmodel_df = pd.DataFrame({\n    'Model': [type(i).__name__ for i in fitted],\n    'Score': [i.score(X_valid,y_valid) for i in fitted]\n    })\n\nmodel_df","29b4b0c9":"mods = [LinearRegression(),Ridge(),GradientBoostingRegressor(),\n  RandomForestRegressor(),BaggingRegressor(),\n  xgb.XGBRegressor(), lgb.LGBMRegressor()]\n\nfitted = [mod.fit(X_train,y_train) for mod in mods]\n\nmodel_df = pd.DataFrame({\n    'Model': [type(i).__name__ for i in fitted],\n    'Score': [i.score(X_test,y_test) for i in fitted]\n    })\n\nmodel_df","b7a18451":"plt.bar(model_df['Model'], model_df['Score'], color = (0.5,0.1,0.5,0.6))\nplt.title('Performance Compare')\nplt.xlabel('Algorithms')\nplt.ylabel('Values')\nplt.ylim(0.50,0.95)\nplt.xticks(model_df['Model'],rotation='vertical');","a14fd2c7":"# ***Variable Definitions and OLS Regression Results***","3f9f56a3":"As you can see here, there are situations that break normality in data (such as Influence or leverage points etc.).","f71344e7":"If the points are randomly dispersed around the horizontal axis, a linear regression model is usually appropriate for the data; otherwise, a non-linear model is more appropriate. ","c9f03b14":"There is no variable with VIF value greater than 10. But 'rad' and 'tax' s VIF value >5.","8390f25a":"# *Generate Prediction Intervals*","d67f0291":"When we put it into the basic multiple regression model without any transformation etc.;\n* our adjusted R^2 is 0.73. (The regression result of the given model shows that 73% of the change in the medv rate is explained together by these explanatory variables.)\n* F statistic is 108.1 \n* indus and age features p_value is >0.05\n","f36e121a":"Random Forest Regressor","e714971c":"As you can see there are a few worrisome observations. 380,418,405,410 have high leverage but a low residual. 364,368,372,371,369,370 has high residual and small leverage.","12fe5d9d":"Here Lasso model has taken all the features except CRIM. So I will drop CRIM.","b03f3aac":"# *Heteroscedasticity*\n* In regression analysis, heteroscedasticity refers to the unequal scatter of residuals.\n* Heteroscedasticity is a problem because ordinary least squares (OLS) regression assumes that the residuals come from a population that has homoscedasticity (constant variance)","422462b9":"# ***Feature Selection and Modelling***","23b7faa0":"We can use ridge regression or principal components to solve the multicollinearity problem. Since the values of multicollinearity are low, there is no need to subtract variables.","8e7072a6":"p-value is less than 0.05,we have to reject the null hypothesis.(The null hypothesis (H0): Homoscedasticity is present.).\n\nVariable transformations can be done, but we will use the standardize method to minimize variance while setting up the final model.","016337e3":"# ***Outlier Plotting***","79d6fa54":"# ***Detecting Multicollinearity with VIF***","af715e90":"As you can see, there is a strong relationship between explanatory variables. Multiple linear linkage can be reduced by standardizing the data.","03a82689":"# *Residual Plotting*\nTo analyze the variance of the error of the regressor. ","b80723f2":"* 1:Normal observation\n* -1: Anomaly observation","fa022548":"I will drop the variable 'RAD' according to the above situations. ('TAX' affects the target variable more than the 'RAD' variable.)","7a5f0067":"# ***Multiple Linear Regression***","b99c5964":"***Descriptive Statistics of Each Features***","279dccff":"Q-Q plot which is a common way to check that residuals are normally distributed. We can see that there are outliers with the Q-Q plot.","fc00e6a9":"# ***Influence plots***"}}