{"cell_type":{"823ea625":"code","b46bc8ec":"code","393cf437":"code","7ddbad97":"code","f44b9d5b":"code","66d26bae":"code","b23191be":"code","a1c5d293":"code","9f9084ca":"code","b1c94f8b":"code","03250af2":"code","c8522d68":"code","8010c4f9":"code","b62ab56d":"code","14072751":"code","18826eaf":"code","57fb8fb6":"code","dfa2480b":"code","fd156f66":"code","4d9d8948":"code","be075045":"code","53eed72f":"code","89721daa":"code","4dabd5a3":"code","8451a7ed":"code","5e754d16":"code","8c6b01fb":"code","0c44fbc6":"code","28535bcc":"code","262f3f16":"code","9366e38d":"markdown","2568faec":"markdown","ecb896ba":"markdown","cce21e69":"markdown","6ff48e1f":"markdown","226b480e":"markdown","acf536a8":"markdown","fe06d9d5":"markdown"},"source":{"823ea625":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Deep Learning necessities\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, LSTM, Bidirectional, Dropout, Conv1D, MaxPool1D\nfrom keras.layers import GlobalMaxPool1D, GRU\nfrom keras import optimizers\n\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b46bc8ec":"# Util functions\ndef prep_data(text, tok):\n    seq = tok.texts_to_sequences([text])\n    data = pad_sequences(seq, MAX_SEQ_LENGTH)\n    return data\n\ndef plot(history):\n    hist = history.history\n    train_loss, train_acc = hist['loss'], hist['acc']\n    val_loss, val_acc = hist['val_loss'], hist['val_acc']\n    epochs = range(1, len(train_acc)+1)\n    \n    plt.plot(epochs, train_acc, 'g', label='Training acc')\n    plt.plot(epochs, val_acc, 'o', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.figure()\n    plt.plot(epochs, train_loss, 'g', label='Training loss')\n    plt.plot(epochs, val_loss, 'o', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    plt.show()","393cf437":"df = pd.read_json('..\/input\/Sarcasm_Headlines_Dataset.json', lines=True)\ndf.head()","7ddbad97":"df = df[['headline', 'is_sarcastic']]","f44b9d5b":"MAX_WORDS = 20000","66d26bae":"tok = Tokenizer(num_words = MAX_WORDS) # keeping 10000 now for first iteration\ntok.fit_on_texts(df.headline)\nseqs = tok.texts_to_sequences(df.headline)","b23191be":"# Find length of sentence \ndf['length'] = df['headline'].apply(lambda x: len(x.split(' ')))","a1c5d293":"# Dataset seems balanced\ndf.is_sarcastic.value_counts()","9f9084ca":"MAX_SEQ_LENGTH = 40","b1c94f8b":"data = pad_sequences(seqs, MAX_SEQ_LENGTH)","03250af2":"seqs[0]","c8522d68":"labels = np.asarray(df.is_sarcastic)","8010c4f9":"data.shape","b62ab56d":"labels.shape","14072751":"training_samples  = 24000\nvalidation_samples = 2709","18826eaf":"# Train val split\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nx_train = data[:training_samples]\ny_train = labels[:training_samples]\nx_val = data[training_samples: training_samples + validation_samples]\ny_val = labels[training_samples: training_samples + validation_samples]","57fb8fb6":"print('X train shape',x_train.shape)\nprint('y train shape',y_train.shape)","dfa2480b":"print('X val shape',x_val.shape)\nprint('y val shape',y_val.shape)","fd156f66":"# For using pretrained embeddings\n# emb_path = 'path of the embedding'\n# embeddings_index = {}\n# f = open(emb_path)\n# for line in f:\n#     values = line.split()\n#     word = values[0]\n#     coefs = np.asarray(values[1:], dtype='float32')\n#     embeddings_index[word] = coefs\n# f.close()\n\n# embedding_dim = 100\n# embedding_matrix = np.zeros((max_words, embedding_dim))\n# for word, i in word_index.items():\n# if i < max_words:\n# embedding_vector = embeddings_index.get(word)\n# if embedding_vector is not None:\n# embedding_matrix[i] = embedding_vector","4d9d8948":"# define dense model\nEMB_DIM = 6\ndef fcmodel():\n    model = Sequential()\n    model.add(Embedding(input_dim=MAX_WORDS, output_dim= EMB_DIM, input_length=MAX_SEQ_LENGTH))    \n    \n    # Flatten Layer\n    model.add(Flatten())\n    \n    # FC1\n    model.add(Dense(64, activation='relu'))\n    \n    # Output layer\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # print model summary\n    model.summary()\n    \n    # When using pretrained embeddings\n    #model.layers[0].set_weights([embedding_matrix])\n    #model.layers[0].trainable = False\n              \n    # Compile the model\n    model.compile(optimizer = 'rmsprop',\n                 loss = 'binary_crossentropy',\n                 metrics = ['acc'])\n    return model\n\nfcmod = fcmodel()","be075045":"EPOCHS = 9\nBATCH_SIZE =512","53eed72f":"# Train the model\nfchist = fcmod.fit(x_train, y_train,\n         epochs = EPOCHS,\n         batch_size = BATCH_SIZE,\n         validation_data = (x_val, y_val))","89721daa":"plot(fchist)","4dabd5a3":"# Lstm model\n\ndef lstm():\n    model = Sequential()\n    \n    model.add(Embedding(input_dim=MAX_WORDS, output_dim=EMB_DIM, input_length=MAX_SEQ_LENGTH))\n    \n    model.add(Bidirectional(LSTM(16, return_sequences=True, recurrent_dropout=0.1, dropout=0.1)))\n    \n    model.add(Bidirectional(LSTM(32, recurrent_dropout=0.1, dropout=0.1)))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    \n    # Compile the model\n    model.compile(optimizer = 'rmsprop',\n                  loss = 'binary_crossentropy',\n                  metrics = ['acc'])\n    \n    return model\n\nlsmod = lstm()","8451a7ed":"# Train the model\nlshist = lsmod.fit(x_train, y_train,\n         epochs = EPOCHS,\n         batch_size = BATCH_SIZE,\n         validation_data = (x_val, y_val))","5e754d16":"plot(lshist)","8c6b01fb":"# Using CONV 1D\n\ndef conv1d():\n    \n    model = Sequential()\n    \n    model.add(Embedding(input_dim=MAX_WORDS,output_dim=EMB_DIM, input_length=MAX_SEQ_LENGTH))\n    \n    model.add(Conv1D(filters=32, kernel_size=7, activation='relu'))\n    \n    model.add(MaxPool1D(pool_size=5))\n    \n    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n    \n    model.add(Dropout(0.1))\n    \n    model.add(GlobalMaxPool1D())\n    \n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(optimizer='rmsprop',\n                  loss='binary_crossentropy',\n                  metrics= ['acc'])\n    \n    return model\n\nconvmod = conv1d()","0c44fbc6":"convhist = convmod.fit(x_train, y_train,\n                      epochs = 20,\n                      batch_size = BATCH_SIZE,\n                      validation_data = (x_val, y_val))\nplot(convhist)","28535bcc":"# Using CONV 1D with GRU\n\ndef convgru():\n    \n    model = Sequential()\n    \n    model.add(Embedding(input_dim=MAX_WORDS,output_dim=EMB_DIM, input_length=MAX_SEQ_LENGTH))\n    \n    model.add(Conv1D(filters=32, kernel_size=7, activation='relu'))\n    \n    model.add(MaxPool1D(pool_size=5))\n    \n    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n    \n    model.add(GRU(32, dropout=0.1, recurrent_dropout=0.5))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(optimizer='rmsprop',\n                  loss='binary_crossentropy',\n                  metrics= ['acc'])\n    \n    return model\n\nconvgrumod = convgru()","262f3f16":"convgruhist = convgrumod.fit(x_train, y_train, \n                           epochs = 6,\n                           batch_size=BATCH_SIZE,\n                           validation_data = (x_val, y_val))\nplot(convgruhist)","9366e38d":"<a id='2.1'><\/a>\n# 2. Prepare Data for Modelling","2568faec":"<a id='6'><\/a>\n# 6. Build and Train Conv1D + GRU model","ecb896ba":"<a id='5'><\/a>\n# 5. Build and Train Conv1D model","cce21e69":"## It's been a long EPOCH, THANKS FOR STAYING TILL THE END and NOT EARLY STOPPING :)\n### Your comments, corrections, advice are whole heartedly welcome","6ff48e1f":"<a id='4'><\/a>\n# 4. Build and Train LSTM model","226b480e":"<a id='3.1'><\/a>\n# 3. Build and Train a Fully Connected model\n","acf536a8":"<a id='1.1'><\/a>\n# 1. Understand folder structure, view a few data points ","fe06d9d5":"<a id='contents'><\/a>\n## Contents:\n\n### 1. [Understand folder structure, view a few data points](#1.1)\n### 2. [Prepare Data for modelling](#2.1)\n### 3. [Build and train Fully Connected Model](#3.1)\n### 4. [Build and train LSTM model ](#4)\n### 5. [Build and train Conv1D model](#5)\n### 6. [Build and train Conv1D + GRU model](#6)"}}