{"cell_type":{"0b4cac4e":"code","83106757":"code","a675834b":"code","410eccad":"code","9a58592b":"code","a6a81025":"code","d15ed658":"code","25ea9b96":"code","c1017990":"code","94961f79":"code","96c8908c":"code","f0b9a4db":"code","05db9a5f":"code","c613f9b9":"code","b31cfa17":"code","24e460f1":"code","ef0e33c5":"code","17cfe77f":"code","1715126e":"code","23d2e62f":"code","22debd4e":"code","c5d75538":"code","33ca6a81":"markdown","74137e01":"markdown","0cc4adf7":"markdown","66ba2fce":"markdown","5d70dde4":"markdown","c9d3f74e":"markdown","a53c77a3":"markdown","bf4c7043":"markdown","3fb3f1ad":"markdown","dd74e716":"markdown","d150d4b8":"markdown","3379508c":"markdown","0d5b7085":"markdown"},"source":{"0b4cac4e":"from part1_cleaning import get_clean_data\nfrom part3_textblobdata import get_textblob_data\ndf1, df2, df3 = get_clean_data()\ntextblob_df1, textblob_df2, textblob_df3 = get_textblob_data(df1, df2, df3)","83106757":"X = textblob_df1.iloc[:, -2:].values","a675834b":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 0)\ny_kmeans = kmeans.fit_predict(X)","410eccad":"import matplotlib.pyplot as plt\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 20, c = 'red', label = 'C1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 20, c = 'blue', label = 'C2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 20, c = 'green', label = 'C3')","9a58592b":"# import scipy.cluster.hierarchy as sch\n# d = sch.dendrogram(sch.linkage(X, method = 'ward'))","a6a81025":"from sklearn.cluster import AgglomerativeClustering\nhc3 = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\ny_hc3 = hc3.fit_predict(X)","d15ed658":"plt.scatter(X[y_hc3 == 0, 0], X[y_hc3 == 0, 1], s = 20, c = 'red', label = 'C1')\nplt.scatter(X[y_hc3 == 1, 0], X[y_hc3 == 1, 1], s = 20, c = 'blue', label = 'C2')\nplt.scatter(X[y_hc3 == 2, 0], X[y_hc3 == 2, 1], s = 20, c = 'green', label = 'C3')","25ea9b96":"from sklearn.metrics import confusion_matrix, accuracy_score\nprint(confusion_matrix(y_kmeans, y_hc3))\nprint(accuracy_score(y_kmeans, y_hc3))","c1017990":"# reassign the values, which 2, 1, 0 being a positive sentiment, neutral sentiment, and negative sentiment, respectively\nc_sentiments = [2 if y == 1 else 1 if y == 2 else 0 for y in y_kmeans]\nc_sentiments[0:10]","94961f79":"# Saving textblob sentiments\n# %store -r df1\n# final_df1 = df1\n# final_df1['tb_sentiment'] = c_sentiments\n# %store final_df1","96c8908c":"X = textblob_df2.iloc[:, -2:].values","f0b9a4db":"kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 0)\ny_kmeans = kmeans.fit_predict(X)","05db9a5f":"plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 20, c = 'red', label = 'C1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 20, c = 'blue', label = 'C2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 20, c = 'green', label = 'C3')","c613f9b9":"# The codelines below are for Hierarchical clustering\nhc3 = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\ny_hc3 = hc3.fit_predict(X)","b31cfa17":"plt.scatter(X[y_hc3 == 0, 0], X[y_hc3 == 0, 1], s = 20, c = 'red', label = 'C1')\nplt.scatter(X[y_hc3 == 1, 0], X[y_hc3 == 1, 1], s = 20, c = 'blue', label = 'C2')\nplt.scatter(X[y_hc3 == 2, 0], X[y_hc3 == 2, 1], s = 20, c = 'green', label = 'C3')","24e460f1":"# reassign the values, which 2, 1, 0 being a positive sentiment, neutral sentiment, and negative sentiment, respectively\nr_sentiments = [2 if y == 2 else 1 if y == 0 else 0 for y in y_kmeans]\nr_sentiments[0:10]","ef0e33c5":"# Saving textblob sentiments\n# %store -r df2\n# final_df2 = df2\n# final_df2['tb_sentiment'] = r_sentiments\n# %store final_df2","17cfe77f":"%store -r textblob_df3\nX = textblob_df3.iloc[:, -1:].values","1715126e":"!pip install jenkspy\nimport jenkspy\nbreaks = jenkspy.jenks_breaks(X, nb_class=3)\nplt.hist(X, bins = 50)\nfor b in breaks:\n    plt.vlines(b, ymin=0, ymax=11000)","23d2e62f":"breaks","22debd4e":"g_sentiments = [0 if x <= breaks[1] else 2 if x >= breaks[2] else 1 for x in X]\ng_sentiments[0:10]","c5d75538":"# Saving textblob sentiments\n# %store -r df3\n# final_df3 = df3\n# final_df3['tb_sentiment'] = g_sentiments\n# %store final_df3","33ca6a81":"For CNBC dataset, I use the 2 most popular clustering algorithms, K-Means Clustering and Hierarchical clustering, to categorized the sentiments of CNBC articles.","74137e01":"## The Guardian data","0cc4adf7":"Assessing both K-Means and Hierarchical clusters, I can see that hierchical clustering method grouped data points that clearly have negative connotation (data points in the lower-left corner) with data points that are in the middle (implying that they have a neutral-positive connotation). Therefore, the favorable clustering model for this data is K-Means Clustering","66ba2fce":"## CNBC data","5d70dde4":"## Import data","c9d3f74e":"From the scatter plot, I can conclude that data points colored in red (y=0) represent negative sentiment, data points colored in blue (y=1) represent positive sentiment, and data points colored in green (y=2) represent a mixed sentiment, in which I will assign as neutral.","a53c77a3":"Different from the other 2 datasets, the Guardian data only contains headlines. Therefore, instead of applying a clustering model, I apply a Natural Breaks Optimization to this dataset, specifically Jenks Natural Breaks.","bf4c7043":"In part 3.1, TextBlob calculated and returned the polarity of each article's headline and preview as a value between -1 and 1. However, that does not tell me what group (negative, neutral, positive) that each article belongs to, especially for articles with positive headlines and negative previews or vice versa. Therefore, utilizing clustering method, an unsupervised learning method of grouping data, I can put each article into their respective group of sentiment.","3fb3f1ad":"From the scatter plot, I can conclude that data points colored in blue (y=1) represent negative sentiment, data points colored in green (y=2) represent positive sentiment, and data points colored in red (y=0) represent a mixed sentiment, in which I will assign as neutral.","dd74e716":"# Polarity Analysis with TextBlob - Clustering","d150d4b8":"For Reuters data, since my device does not have the memory capacity to run hierchical clustering, I am also using K-Means Clustering for the sentiment scores.","3379508c":"From the histogram, I can conclude that data points with sentiments below -0.22499999999999992 represent negative sentiment, data points with sentiments above 0.20833333333333334 represent positive sentiment, and data points with sentiments in between those 2 values represent neutral sentiment","0d5b7085":"# Reuters data"}}