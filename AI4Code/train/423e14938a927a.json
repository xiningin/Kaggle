{"cell_type":{"864c345f":"code","86561a9d":"code","2d058f8b":"code","4491ec68":"code","99f26811":"code","d6673bb3":"code","04d5c0fd":"code","9b58185e":"code","a1d3974d":"code","62231b44":"code","aeb292bd":"code","df416b05":"code","ce321448":"code","353e9744":"code","a31b7280":"code","57e81f2d":"code","a8db7cf0":"code","71d05338":"code","bf34a85d":"code","51396e6f":"code","93cfb2bd":"code","f7e29643":"code","3dcf5a61":"code","a4b94867":"code","84a06201":"code","06ca124a":"code","770d02cb":"code","2e14683b":"code","9c87afce":"code","3f3ce83e":"code","1c3c09c5":"code","bf50a0df":"code","415cb76f":"code","6f963320":"code","fa0db3ca":"code","285469d5":"code","9a1fd9d5":"code","9724e051":"code","bb06c390":"code","a6c2e221":"code","cf053d48":"code","eda11102":"code","80b60876":"code","c8f6aece":"code","adf03069":"code","fa92f666":"code","117c1b25":"code","9ecdcfd6":"code","dfd8ae0b":"code","114c1945":"code","c3beeba4":"code","77ecde79":"code","877822e0":"code","955aa844":"code","a573adef":"code","ef6d166d":"code","4e9a8c9c":"code","63b7982a":"code","e7901351":"code","8a366533":"code","a455c956":"code","a5a9d843":"code","7b14e675":"code","86e69fa6":"code","fc51a964":"code","7b3932cb":"markdown","d2f83330":"markdown","9775a0e0":"markdown","644b8201":"markdown","62eee18f":"markdown","4caa8c90":"markdown","9d79e093":"markdown","b8eb1833":"markdown","0458e53a":"markdown","63daf9f5":"markdown","e538357b":"markdown","28648eac":"markdown","bf218e1d":"markdown","e05cc77c":"markdown","7eea78e9":"markdown","47b7d462":"markdown","5211ab43":"markdown","45001d82":"markdown","87e4ad47":"markdown","16232fa3":"markdown","40fa2c0e":"markdown","6742d4cd":"markdown","fed649b2":"markdown","73834a38":"markdown","788f3d80":"markdown","6e15a09a":"markdown","120d2bf4":"markdown","2fe2b716":"markdown","49b044f1":"markdown","88455b41":"markdown","fb22fe42":"markdown","801e46b7":"markdown","fa232864":"markdown","a01597c0":"markdown","cc06177a":"markdown","88864fcd":"markdown","d66ec131":"markdown","266aec31":"markdown","2a2254ca":"markdown","e1aa82cf":"markdown","15952ca3":"markdown","51cbf9d7":"markdown","05d67480":"markdown","bae8aa15":"markdown","faab3f3b":"markdown","37a0332b":"markdown","b6c50cf0":"markdown","bd5a86d1":"markdown"},"source":{"864c345f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86561a9d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","2d058f8b":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","4491ec68":"df_train.head(5)","99f26811":"df_train.describe()","d6673bb3":"df_test.head()","04d5c0fd":"df_test.describe()","9b58185e":"df_train.info()","a1d3974d":"df_test.info()","62231b44":"sns.distplot(df_train['SalePrice'])","aeb292bd":"df_train['SalePrice'].skew()","df416b05":"df_train['SalePrice_log'] = np.log(df_train['SalePrice'])\ndf_train.drop('SalePrice', axis=1, inplace=True)","ce321448":"sns.distplot(df_train['SalePrice_log'])\nprint(\"Skewness: %f\" % df_train['SalePrice_log'].skew())","353e9744":"plt.figure(figsize=(15,9))\nsns.heatmap(df_train.isnull())","a31b7280":"total = df_train.isnull().sum().sort_values(ascending = False)\npercentage = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percentage], axis=1, keys=['total', 'percent'])\nmissing_data.head(10)","57e81f2d":"df_train = df_train.drop((missing_data[missing_data['total']>81]).index,1)\ndf_train.head()","a8db7cf0":"df_train.isnull().sum().sort_values(ascending=False)","71d05338":"total_test = df_test.isnull().sum().sort_values(ascending = False)\npercentage_test = (df_test.isnull().sum()\/df_test.isnull().count()).sort_values(ascending=False)\nmissing_data_test = pd.concat([total_test, percentage_test], axis=1, keys=['total', 'percent'])\nmissing_data_test.head(10)","bf34a85d":"df_test = df_test.drop((missing_data_test[missing_data_test['total']>78]).index,1)\ndf_test.head()","51396e6f":"df_test.isnull().sum().sort_values(ascending = False)","93cfb2bd":"train_num_cols = df_train.select_dtypes(exclude='object').columns\ntrain_cat_cols = df_train.select_dtypes(include='object').columns","f7e29643":"train_num_cols","3dcf5a61":"train_cat_cols","a4b94867":"test_num_cols = df_test.select_dtypes(exclude='object').columns\ntest_cat_cols = df_test.select_dtypes(include='object').columns","84a06201":"for i in range(0, len(train_num_cols)):\n    df_train[train_num_cols[i]] = df_train[train_num_cols[i]].fillna(df_train[train_num_cols[i]].mean())\n\nfor i in range(0, len(test_num_cols)):\n    df_test[test_num_cols[i]] = df_test[test_num_cols[i]].fillna(df_test[test_num_cols[i]].mean()) ","06ca124a":"for i in range(0, len(train_cat_cols)):\n    df_train[train_cat_cols[i]] = df_train[train_cat_cols[i]].fillna(df_train[train_cat_cols[i]].mode()[0])\n\nfor i in range(0, len(test_cat_cols)):\n    df_test[test_cat_cols[i]] = df_test[test_cat_cols[i]].fillna(df_test[test_cat_cols[i]].mode()[0])","770d02cb":"df_train.isnull().sum().max()","2e14683b":"df_test.isnull().sum().max()","9c87afce":"df_train.corr()['SalePrice_log']","3f3ce83e":"plt.figure(figsize=(20,9))\nsns.heatmap(df_train.corr())","1c3c09c5":"df_train.corr()['SalePrice_log'].sort_values(ascending=False)","bf50a0df":"df_train.drop(['MasVnrArea', 'GarageYrBlt','Fireplaces','BsmtFinSF1','WoodDeckSF','2ndFlrSF','OpenPorchSF','HalfBath','LotArea',\n         'BsmtFullBath','BsmtUnfSF','BedroomAbvGr','ScreenPorch','PoolArea','MoSold','3SsnPorch','BsmtFinSF2','BsmtHalfBath',\n         'MiscVal','Id','LowQualFinSF','YrSold','OverallCond','MSSubClass','EnclosedPorch','KitchenAbvGr'], axis=1, inplace=True)\n","415cb76f":"df_train.head()","6f963320":"df_train.corr()['SalePrice_log']","fa0db3ca":"train_num_cols1 = df_train.select_dtypes(exclude='object').columns\nfor col in train_num_cols1:\n    print('{:15}'.format(col),'skewness: {}'.format(df_train[col].skew()))","285469d5":"sns.distplot(df_train['GrLivArea'])","9a1fd9d5":"df_train['GrLivArea_log'] = np.log(df_train['GrLivArea'])\ndf_train.drop('GrLivArea', axis=1, inplace=True)","9724e051":"sns.distplot(df_train['GrLivArea_log'])","bb06c390":"df_test['GrLivArea_log'] = np.log(df_test['GrLivArea'])\ndf_test.drop('GrLivArea', axis=1, inplace=True)","a6c2e221":"df_test.drop(['MasVnrArea', 'GarageYrBlt','Fireplaces','BsmtFinSF1','WoodDeckSF','2ndFlrSF','OpenPorchSF','HalfBath','LotArea',\n         'BsmtFullBath','BsmtUnfSF','BedroomAbvGr','ScreenPorch','PoolArea','MoSold','3SsnPorch','BsmtFinSF2','BsmtHalfBath',\n         'MiscVal','Id','LowQualFinSF','YrSold','OverallCond','MSSubClass','EnclosedPorch','KitchenAbvGr'], axis=1, inplace=True)","cf053d48":"df_test.head(5)","eda11102":"sns.scatterplot('GrLivArea_log', 'SalePrice_log', data=df_train)","80b60876":"train_cat_cols1 = df_train.select_dtypes(include='object').columns\ntrain_cat_cols1","c8f6aece":"nrows = 13\nncols = 3\n\nfig, ax = plt.subplots(nrows, ncols, figsize=(ncols*4,nrows*3))\n\nfor r in range(0, nrows):\n    for c in range(0, ncols):\n        i = r*ncols + c\n        if i < len(train_cat_cols1):\n            sns.boxplot(x=train_cat_cols1[i], y= 'SalePrice_log', data= df_train, ax= ax[r][c])\n\nplt.tight_layout()\nplt.show()","adf03069":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","fa92f666":"df_train[train_cat_cols1] = df_train[train_cat_cols1].apply(lambda col:le.fit_transform(col.astype(str)))","117c1b25":"test_cat_cols1 = df_test.select_dtypes(include='object').columns\ntest_cat_cols1","9ecdcfd6":"df_test[test_cat_cols1] = df_test[test_cat_cols1].apply(lambda col:le.fit_transform(col.astype(str)))","dfd8ae0b":"X_train = df_train.drop('SalePrice_log', axis=1)\ny_train = df_train['SalePrice_log']","114c1945":"X_test = df_test","c3beeba4":"from sklearn.linear_model import LinearRegression","77ecde79":"lm = LinearRegression()","877822e0":"lm.fit(X_train, y_train)","955aa844":"lm.score(X_train, y_train)","a573adef":"from sklearn.ensemble import GradientBoostingRegressor","ef6d166d":"gbr = GradientBoostingRegressor(n_estimators=3000, max_depth=4, learning_rate=0.05)","4e9a8c9c":"gbr.fit(X_train, y_train)","63b7982a":"gbr.score(X_train, y_train)","e7901351":"predictions_lm = lm.predict(X_test)\npredictions_lm = np.exp(predictions_lm)\npredictions_lm","8a366533":"predictions_gbr = gbr.predict(X_test)\npredictions_gbr = np.exp(predictions_gbr)\npredictions_gbr","a455c956":"final_predict = (predictions_lm + predictions_gbr)*0.5\nfinal_predict","a5a9d843":"sub_file = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsub_file.head()","7b14e675":"sub_file['SalePrice'] = final_predict","86e69fa6":"sub_file.head()","fc51a964":"sub_file.to_csv('final_submission1.csv', index=False)","7b3932cb":"At first look, we can observe that there are some variables strongly correlated with the target variable (Value closer to 1 or -1 indicates strong correlation) such as OverallCond, GrLivArea etc. In order to have a better understanding, we can sort the variables in order.","d2f83330":"Creating an instance of the Linear Regression class in sklearn","9775a0e0":"### Missing Data","644b8201":"# Model Training and Evaluation","62eee18f":"Categorical columns will be filled with mode value of the column","4caa8c90":"## Correlation of Numerical Variables with Target Variable","9d79e093":"### Check if there are still missing values","b8eb1833":"### Shape, info, head","0458e53a":"Now, it is clear that certain (Numerical) variables are strongly correlated with target variable. We shall keep only those variables whose correlation coefficient is greater than a certain threshold value (here, 0.5)","63daf9f5":"## Correlation with the Categorical variables with Target Variable","e538357b":"Few variables have more than 10% data missing. So we can remove these variables, since filling these variables can affect the resulting distribution significantly.","28648eac":"We will try to figure out which variables are strongly correlated with our target variable (SalePrice_log). Let's have a look at correlation function with SalePrice_log","bf218e1d":"\n#### Test Data\n","e05cc77c":"Numerical columns will be filled with mean value of the column","7eea78e9":"## Predictions from the models","47b7d462":"To lessen the impact of outliers we can transform the variable or remove the outliers. To reduce skewness we can transform it to log function of SalePrice.\n","5211ab43":"Train data contains 81 columns (including our target variable) and 1460 observations\/entries. While, our test data contains 80 columns(except target variable) and 1459 observations. Both seems to have same columns. There are both numerical variables(type: int64) and categorical variables(type: object) in both datasets. Some variables have missing data such as PoolQC,Fence,Alley etc. ","45001d82":"### Checking the correlation visually ","87e4ad47":"Looks like SalePrice is slightly right skewed. Let's quantify our observation using skew function.","16232fa3":"Next, we drop same weakly correlated variables as in Training data","40fa2c0e":"### Missing Values in Training Data","6742d4cd":"### Import plotting libraries","fed649b2":"Note that we can do a similar transform on variables 'TotalBsmtSF' and '1stFlrSF' but it didn't improve the correlation with target variable.","73834a38":"### Separating variables into Numerical and Categorical ","788f3d80":"Predictor variables as 'X' and response variable as 'y'.","6e15a09a":"We have been given a target variable 'SalePrice'. We need to find how different variables influence the change in our target variable. Using that knowledge we can train a mathematical model and use it to predict for any unknown data given in future (here we have test data). Let's try to understand the data visually through its shape, variables what it contains.","120d2bf4":"# Exploratory Data Analysis","2fe2b716":"Let's check the distribution of the target variable.","49b044f1":"score() function calculates the $R^2$ value","88455b41":"### Load Data","fb22fe42":"## 2. Gradient Boosting Regression","801e46b7":"We can observe that the variable 'GrLivArea' is slighlty right skewed.","fa232864":"### Don't Hesitate to comment your suggestions. Thankyou! : )","a01597c0":"Since there is a linear relation between variables and target variables, we can use a simple linear regression model. Let's import linear regression model from sklearn libraries.","cc06177a":"This is my first kaggle notebook for House Price Competition. The analysis is divided into major sections as follows:\n\n1. Importing data and libraries\n2. Exploratory Data Analysis\n3. Feature Engineering\n4. Model training and evaluation","88864fcd":"We can visualize the missing data to get an overview.","d66ec131":"### Checking the skewness in Numerical Variables","266aec31":"### Missing data in Test Data","2a2254ca":"Et Voil\u00e0 ! Similarly, we need to log transform the test variable for 'GrLivArea'","e1aa82cf":"# Feature Engineering","15952ca3":"## 1. Linear Regression Model","51cbf9d7":"# House Price Prediction : A beginner's approach (EDA + ML)","05d67480":"### Creating Submission File","bae8aa15":"We shall remove the same variables in Test data as we did in our training data.","faab3f3b":"## Encoding Categorical Variables","37a0332b":"#### Training Data","b6c50cf0":"Gradient boosting is an ensemble technique in which prediction models are produced in a sequential manner. In other words, it produces a prediction model in the form of an ensemble of weak prediction models.","bd5a86d1":"### Fill missing columns"}}