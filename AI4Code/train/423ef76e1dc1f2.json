{"cell_type":{"086ec37a":"code","5c3d299c":"code","13ecb8b7":"code","b4164749":"code","0221db15":"code","07f32676":"code","ba44fbbc":"code","ddc7526f":"code","3dfc7c49":"code","69ae712a":"code","c115af9f":"code","c3b3c470":"code","45d417b1":"code","83c235cc":"code","fbdc25f5":"code","42a08fb9":"code","7f2839e3":"code","bc2dbe39":"code","bca668b9":"code","b86234fa":"code","6f5089f8":"code","d2b22299":"code","03fc0870":"code","f71c1cd6":"code","622ed55f":"code","0d61ac0a":"code","ba91b807":"code","5a3c7e97":"code","77de95fc":"code","40faa855":"code","884796b9":"code","a66abcb8":"code","f722d6ec":"code","0ee9e90e":"code","271dc84a":"code","51fc9031":"code","b48f9e80":"code","95a39db5":"code","6acc6b50":"code","83d037c5":"code","8c36d7e1":"code","f573a259":"code","1743461c":"code","9e50d01a":"code","d29d04f6":"code","8b8ff064":"code","a60e36e6":"code","44c20959":"code","e45f3ca5":"code","a7af609a":"code","f4b49a14":"code","13d201f6":"code","6e12a58d":"code","d876228d":"code","d2c6cff4":"code","bedd7f42":"code","828f4af8":"code","2691cb67":"code","4f2ed280":"code","73ee4e92":"code","8d788f96":"code","996f76e3":"code","86901130":"code","0f53cde2":"code","ac966d91":"code","4afbc769":"code","ebfbac12":"code","bca79693":"code","db84dbc1":"code","e3fd444e":"code","e40b119d":"code","03c01f22":"code","36cebf03":"code","75da2f39":"code","75a4b7a1":"code","07ab5eb7":"code","e9e907a1":"code","64305964":"code","f47d0415":"code","f0f7ec23":"code","bdb976e3":"code","36881fa4":"code","032ac5b9":"code","f5680a94":"code","e0655caf":"code","0bc50a65":"code","15d73da7":"code","82ab8207":"code","8f0a0ee1":"code","b7a85930":"code","624bbccd":"code","5d777703":"code","7fdc4020":"code","88665a53":"code","68662412":"code","7c762e69":"code","99802cde":"code","b357fa19":"code","0f8c01ab":"code","24515cd6":"code","562fa6f9":"code","ba404977":"code","40b2c407":"code","6433a8c1":"markdown","bfe5119d":"markdown","04362bd7":"markdown","417caf10":"markdown","248b343f":"markdown","bad3bab0":"markdown","a7e6f5fc":"markdown","2739a467":"markdown","bb3a6e9a":"markdown","e217ae1c":"markdown","eb4332ca":"markdown","7c1de2e1":"markdown","5bb4a7b0":"markdown","e32aa35d":"markdown","49cfc684":"markdown","3aea3617":"markdown","9df99284":"markdown","938e1a72":"markdown","1e3d7e86":"markdown","cfd84862":"markdown","4d8cb73b":"markdown","2b3c1c74":"markdown","a0890c5d":"markdown","4ab60bd5":"markdown","13334636":"markdown","befbc849":"markdown","432477cc":"markdown","d2930d48":"markdown","0e53cf82":"markdown","0048dbc1":"markdown","9b34a0a6":"markdown","6957bcce":"markdown","c5ad315c":"markdown","a47bc3fe":"markdown","81983ff3":"markdown","3cddca0b":"markdown","4f827788":"markdown","39ad2a44":"markdown","5c6b7f1f":"markdown","ad5a03ec":"markdown","78dd98e7":"markdown","80992af7":"markdown","cf261349":"markdown","735df7ec":"markdown","6c0ab8a6":"markdown","9839526c":"markdown","fe585dc3":"markdown","84883fb6":"markdown","aa58bd03":"markdown","73c42179":"markdown","bef82a5f":"markdown","d52f3224":"markdown","9c9e17cb":"markdown","e4a0078d":"markdown","4d17fbaa":"markdown","4086db15":"markdown","6fa3d477":"markdown","1c506558":"markdown","98ec75e8":"markdown","be9e675a":"markdown","759007da":"markdown","de605d55":"markdown","c6d7410b":"markdown","04f2776a":"markdown","e23baf50":"markdown","65148555":"markdown","a8b43083":"markdown","bfc23333":"markdown","be6a4b97":"markdown","468d351c":"markdown","f5bc60f3":"markdown","8beee9d3":"markdown","6a3a8d39":"markdown","30133ba1":"markdown"},"source":{"086ec37a":"from IPython.core.interactiveshell import InteractiveShell\n\n# Set shell to show all lines of output\nInteractiveShell.ast_node_interactivity = 'all'","5c3d299c":"import os\nprint(os.listdir(\"..\/input\"))","13ecb8b7":"import json\n\nbooks = []\n\nwith open('..\/input\/found_books_filtered.ndjson', 'r') as fin:\n    # Append each line to the books\n    books = [json.loads(l) for l in fin]\n\n# Remove non-book articles\nbooks_with_wikipedia = [book for book in books if 'Wikipedia:' in book[0]]\nbooks = [book for book in books if 'Wikipedia:' not in book[0]]\nprint(f'Found {len(books)} books.')","b4164749":"[book[0] for book in books_with_wikipedia][:5]","0221db15":"[book[0] for book in books][:5]","07f32676":"n = 0\nbooks[n][0], print(\"\\n\\n\"),\nbooks[n][1], print(\"\\n\\n\"),\nbooks[n][2][:5], print(\"\\n\\n\"),\nbooks[n][3][:5], print(\"\\n\\n\"),\nbooks[n][4], print(\"\\n\\n\"),\nbooks[n][5], print(\"\\n\\n\")","ba44fbbc":"book_index = {book[0]: idx for idx, book in enumerate(books)}\nindex_book = {idx: book for book, idx in book_index.items()}\n\nbook_index['Anna Karenina']\nindex_book[22494]","ddc7526f":"from itertools import chain\n\nwikilinks = list(chain(*[book[2] for book in books]))\nprint(f\"There are {len(set(wikilinks))} unique wikilinks.\")","3dfc7c49":"wikilinks_other_books = [link for link in wikilinks if link in book_index.keys()]\nprint(f\"There are {len(set(wikilinks_other_books))} unique wikilinks to other books.\")","69ae712a":"from collections import Counter, OrderedDict\n\ndef count_items(l):\n    \"\"\"Return ordered dictionary of counts of objects in `l`\"\"\"\n    \n    # Create a counter object\n    counts = Counter(l)\n    \n    # Sort by highest count first and place in ordered dictionary\n    counts = sorted(counts.items(), key = lambda x: x[1], reverse = True)\n    counts = OrderedDict(counts)\n    \n    return counts","c115af9f":"# Find set of wikilinks for each book and convert to a flattened list\nunique_wikilinks = list(chain(*[list(set(book[2])) for book in books]))\n\nwikilink_counts = count_items(unique_wikilinks)\nlist(wikilink_counts.items())[:10]","c3b3c470":"wikilinks = [link.lower() for link in unique_wikilinks]\nprint(f\"There are {len(set(wikilinks))} unique wikilinks.\")\n\nwikilink_counts = count_items(wikilinks)\nlist(wikilink_counts.items())[:10]","45d417b1":"to_remove = ['hardcover', 'paperback', 'hardback', 'e-book', 'wikipedia:wikiproject books', 'wikipedia:wikiproject novels']\nfor t in to_remove:\n    wikilinks.remove(t)\n    _ = wikilink_counts.pop(t)","83c235cc":"# Limit to greater than 3 links\nlinks = [t[0] for t in wikilink_counts.items() if t[1] >= 4]\nprint(len(links))","fbdc25f5":"# Find set of book wikilinks for each book\nunique_wikilinks_books = list(chain(*[list(set(link for link in book[2] if link in book_index.keys())) for book in books]))\n\n# Count the number of books linked to by other books\nwikilink_book_counts = count_items(unique_wikilinks_books)\nlist(wikilink_book_counts.items())[:10]","42a08fb9":"for book in books:\n    if 'The New York Times' in book[2] and 'New York Times' in book[2]:\n        print(book[0], book[2])\n        break","7f2839e3":"wikilink_counts.get('the new york times')\nwikilink_counts.get('new york times')","bc2dbe39":"print((links[:10]))","bca668b9":"link_index = {link: idx for idx, link in enumerate(links)}\nindex_link = {idx: link for link, idx in link_index.items()}\n\nlink_index['the economist']\nindex_link[300]\nprint(f'There are {len(link_index)} wikilinks that will be used.')","b86234fa":"pairs = []\n\n# Iterate through each book\nfor book in books:\n    # Iterate through the links in the book\n    pairs.extend((book_index[book[0]], link_index[link.lower()]) for link in book[2] if link.lower() in links)\n    \nlen(pairs), len(links), len(books)\npairs[5000]","6f5089f8":"index_book[pairs[5000][0]], index_link[pairs[5000][1]]","d2b22299":"index_book[pairs[900][0]], index_link[pairs[900][1]]","03fc0870":"pairs_set = set(pairs)","f71c1cd6":"x = Counter(pairs)\nsorted(x.items(), key = lambda x: x[1], reverse = True)[:5]","622ed55f":"index_book[13337], index_link[31111]\nindex_book[31899], index_link[65]\nindex_book[25899], index_link[30465]","0d61ac0a":"import numpy as np\nimport random\nrandom.seed(100)\n\ndef generate_batch(pairs, n_positive = 50, negative_ratio = 1.0, classification = False):\n    \"\"\"Generate batches of samples for training\"\"\"\n    batch_size = n_positive * (1 + negative_ratio)\n    batch = np.zeros((batch_size, 3))\n    \n    # Adjust label based on task\n    if classification:\n        neg_label = 0\n    else:\n        neg_label = -1\n    \n    # This creates a generator\n    while True:\n        # randomly choose positive examples\n        for idx, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n            batch[idx, :] = (book_id, link_id, 1)\n\n        # Increment idx by 1\n        idx += 1\n        \n        # Add negative examples until reach batch size\n        while idx < batch_size:\n            \n            # random selection\n            random_book = random.randrange(len(books))\n            random_link = random.randrange(len(links))\n            \n            # Check to make sure this is not a positive example\n            if (random_book, random_link) not in pairs_set:\n                \n                # Add to batch and increment index\n                batch[idx, :] = (random_book, random_link, neg_label)\n                idx += 1\n                \n        # Make sure to shuffle order\n        np.random.shuffle(batch)\n        yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]","ba91b807":"next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))","5a3c7e97":"x, y = next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))\n\n# Show a few example training pairs\nfor label, b_idx, l_idx in zip(y, x['book'], x['link']):\n    print(f'Book: {index_book[b_idx]:30} Link: {index_link[l_idx]:40} Label: {label}') ","77de95fc":"from keras.layers import Input, Embedding, Dot, Reshape, Dense\nfrom keras.models import Model","40faa855":"def book_embedding_model(embedding_size = 50, classification = False):\n    \"\"\"Model to embed books and wikilinks using the functional API.\n       Trained to discern if a link is present in a article\"\"\"\n    \n    # Both inputs are 1-dimensional\n    book = Input(name = 'book', shape = [1])\n    link = Input(name = 'link', shape = [1])\n    \n    # Embedding the book (shape will be (None, 1, 50))\n    book_embedding = Embedding(name = 'book_embedding',\n                               input_dim = len(book_index),\n                               output_dim = embedding_size)(book)\n    \n    # Embedding the link (shape will be (None, 1, 50))\n    link_embedding = Embedding(name = 'link_embedding',\n                               input_dim = len(link_index),\n                               output_dim = embedding_size)(link)\n    \n    # Merge the layers with a dot product along the second axis (shape will be (None, 1, 1))\n    merged = Dot(name = 'dot_product', normalize = True, axes = 2)([book_embedding, link_embedding])\n    \n    # Reshape to be a single number (shape will be (None, 1))\n    merged = Reshape(target_shape = [1])(merged)\n    \n    # If classifcation, add extra layer and loss function is binary cross entropy\n    if classification:\n        merged = Dense(1, activation = 'sigmoid')(merged)\n        model = Model(inputs = [book, link], outputs = merged)\n        model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    # Otherwise loss function is mean squared error\n    else:\n        model = Model(inputs = [book, link], outputs = merged)\n        model.compile(optimizer = 'Adam', loss = 'mse')\n    \n    return model\n\n# Instantiate model and show parameters\nmodel = book_embedding_model()\nmodel.summary()","884796b9":"n_positive = 1024\n\ngen = generate_batch(pairs, n_positive, negative_ratio = 2)\n\n# Train\nh = model.fit_generator(gen, epochs = 15, \n                        steps_per_epoch = len(pairs) \/\/ n_positive,\n                        verbose = 2)","a66abcb8":"#model.save('..\/input\/first_attempt.h5')","f722d6ec":"# Extract embeddings\nbook_layer = model.get_layer('book_embedding')\nbook_weights = book_layer.get_weights()[0]\nbook_weights.shape","0ee9e90e":"book_weights = book_weights \/ np.linalg.norm(book_weights, axis = 1).reshape((-1, 1))\nbook_weights[0][:10]\nnp.sum(np.square(book_weights[0]))","271dc84a":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 15\n\ndef find_similar(name, weights, index_name = 'book', n = 10, least = False, return_dist = False, plot = False):\n    \"\"\"Find n most similar items (or least) to name based on embeddings. Option to also plot the results\"\"\"\n    \n    # Select index and reverse index\n    if index_name == 'book':\n        index = book_index\n        rindex = index_book\n    elif index_name == 'page':\n        index = link_index\n        rindex = index_link\n    \n    # Check to make sure `name` is in index\n    try:\n        # Calculate dot product between book and all others\n        dists = np.dot(weights, weights[index[name]])\n    except KeyError:\n        print(f'{name} Not Found.')\n        return\n    \n    # Sort distance indexes from smallest to largest\n    sorted_dists = np.argsort(dists)\n    \n    # Plot results if specified\n    if plot:\n        \n        # Find furthest and closest items\n        furthest = sorted_dists[:(n \/\/ 2)]\n        closest = sorted_dists[-n-1: len(dists) - 1]\n        items = [rindex[c] for c in furthest]\n        items.extend(rindex[c] for c in closest)\n        \n        # Find furthest and closets distances\n        distances = [dists[c] for c in furthest]\n        distances.extend(dists[c] for c in closest)\n        \n        colors = ['r' for _ in range(n \/\/2)]\n        colors.extend('g' for _ in range(n))\n        \n        data = pd.DataFrame({'distance': distances}, index = items)\n        \n        # Horizontal bar chart\n        data['distance'].plot.barh(color = colors, figsize = (10, 8),\n                                   edgecolor = 'k', linewidth = 2)\n        plt.xlabel('Cosine Similarity');\n        plt.axvline(x = 0, color = 'k');\n        \n        # Formatting for italicized title\n        name_str = f'{index_name.capitalize()}s Most and Least Similar to'\n        for word in name.split():\n            # Title uses latex for italize\n            name_str += ' $\\it{' + word + '}$'\n        plt.title(name_str, x = 0.2, size = 28, y = 1.05)\n        \n        return None\n    \n    # If specified, find the least similar\n    if least:\n        # Take the first n from sorted distances\n        closest = sorted_dists[:n]\n         \n        print(f'{index_name.capitalize()}s furthest from {name}.\\n')\n        \n    # Otherwise find the most similar\n    else:\n        # Take the last n sorted distances\n        closest = sorted_dists[-n:]\n        \n        # Need distances later on\n        if return_dist:\n            return dists, closest\n        \n        \n        print(f'{index_name.capitalize()}s closest to {name}.\\n')\n        \n    # Need distances later on\n    if return_dist:\n        return dists, closest\n    \n    \n    # Print formatting\n    max_width = max([len(rindex[c]) for c in closest])\n    \n    # Print the most similar and distances\n    for c in reversed(closest):\n        print(f'{index_name.capitalize()}: {rindex[c]:{max_width + 2}} Similarity: {dists[c]:.{2}}')\n        \n    ","51fc9031":"find_similar('War and Peace', book_weights)","b48f9e80":"find_similar('War and Peace', book_weights, least = True, n = 5)","95a39db5":"find_similar('War and Peace', book_weights, n = 5, plot = True)","6acc6b50":"find_similar('The Fellowship of the Ring', book_weights, n = 5)","83d037c5":"find_similar('Artificial Intelligence: A Modern Approach', book_weights, n = 5)","8c36d7e1":"find_similar('Weapons of Math Destruction', book_weights, n = 5)","f573a259":"find_similar('Bully for Brontosaurus', book_weights, n = 5)","1743461c":"find_similar('Bully for Brontosaurus', book_weights, n = 5, plot = True)","9e50d01a":"def extract_weights(name, model):\n    \"\"\"Extract weights from a neural network model\"\"\"\n    \n    # Extract weights\n    weight_layer = model.get_layer(name)\n    weights = weight_layer.get_weights()[0]\n    \n    # Normalize\n    weights = weights \/ np.linalg.norm(weights, axis = 1).reshape((-1, 1))\n    return weights","d29d04f6":"link_weights = extract_weights('link_embedding', model)","8b8ff064":"find_similar('science fiction', link_weights, index_name = 'page')","a60e36e6":"find_similar('biography', link_weights, index_name = 'page')","44c20959":"find_similar('biography', link_weights, index_name = 'page', n = 5, plot = True)","e45f3ca5":"find_similar('new york city', link_weights, index_name = 'page', n = 5)","a7af609a":"model_class = book_embedding_model(50, classification = True)\ngen = generate_batch(pairs, n_positive, negative_ratio=2, classification = True)","f4b49a14":"# Train the model to learn embeddings\nh = model_class.fit_generator(gen, epochs = 15, steps_per_epoch= len(pairs) \/\/ n_positive,\n                            verbose = 0)","13d201f6":"#model_class.save('..\/models\/first_attempt_class.h5')","6e12a58d":"book_weights_class = extract_weights('book_embedding', model_class)\nbook_weights_class.shape","d876228d":"find_similar('War and Peace', book_weights_class, n = 5)","d2c6cff4":"find_similar('The Fellowship of the Ring', book_weights_class, n = 5)","bedd7f42":"find_similar('The Better Angels of Our Nature', book_weights_class, n = 5)","828f4af8":"link_weights_class = extract_weights('link_embedding', model_class)","2691cb67":"find_similar('the washington post', link_weights_class, index_name = 'page', n = 5)","4f2ed280":"find_similar('category:almanacs', link_weights_class, index_name = 'page', n = 5)","73ee4e92":"find_similar('steven pinker', link_weights_class, index_name = 'page', n = 5)","8d788f96":"find_similar('richard dawkins', link_weights_class, index_name = 'page', n = 5)","996f76e3":"from sklearn.manifold import TSNE\nfrom umap import UMAP","86901130":"def reduce_dim(weights, components = 3, method = 'tsne'):\n    \"\"\"Reduce dimensions of embeddings\"\"\"\n    if method == 'tsne':\n        return TSNE(components, metric = 'cosine').fit_transform(weights)\n    elif method == 'umap':\n        # Might want to try different parameters for UMAP\n        return UMAP(n_components=components, metric = 'cosine', \n                    init = 'random', n_neighbors = 5).fit_transform(weights)","0f53cde2":"book_r = reduce_dim(book_weights_class, components = 2, method = 'tsne')\nbook_r.shape","ac966d91":"InteractiveShell.ast_node_interactivity = 'last'\n\nplt.figure(figsize = (10, 8))\nplt.plot(book_r[:, 0], book_r[:, 1], 'r.')\nplt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('Book Embeddings Visualized with TSNE');","4afbc769":"book_ru = reduce_dim(book_weights_class, components = 2, method = 'umap')\n\nplt.figure(figsize = (10, 8))\nplt.plot(book_ru[:, 0], book_ru[:, 1], 'g.');\nplt.xlabel('UMAP 1'); plt.ylabel('UMAP 2'); plt.title('Book Embeddings Visualized with UMAP');","ebfbac12":"info = list(chain(*[set(book[1]) for book in books]))\ninfo_counts = count_items(info)\nlist(info_counts.items())[:10]","bca79693":"genres = [book[1].get('genre', 'None').lower() for book in books]\n\n# Remove genres not found\ngenre_counts = count_items(genres)\ndel genre_counts['none']\nlist(genre_counts.items())[:10]","db84dbc1":"# Include 10 most popular genres\ngenre_to_include = list(genre_counts.keys())[:10]","e3fd444e":"idx_include = []\ngenres = []\n\nfor i, book in enumerate(books):\n    if 'genre' in book[1].keys():\n        if book[1]['genre'].lower() in genre_to_include:\n            idx_include.append(i)\n            genres.append(book[1]['genre'].capitalize())\n            \nlen(idx_include)","e40b119d":"ints, gen = pd.factorize(genres)\ngen[:5]","03c01f22":"plt.figure(figsize = (10, 8))\n\n# Plot embedding\nplt.scatter(book_r[idx_include, 0], book_r[idx_include, 1], \n            c = ints, cmap = plt.cm.tab10)\n\n# Add colorbar and appropriate labels\ncbar = plt.colorbar()\ncbar.set_ticks([])\nfor j, lab in enumerate(gen):\n    cbar.ax.text(1, (2 * j + 1) \/ ((10) * 2), lab, ha='left', va='center')\ncbar.ax.set_title('Genre', loc = 'left')\n\n\nplt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('TSNE Visualization of Book Embeddings');","36cebf03":"plt.figure(figsize = (10, 8))\n\n# Plot embedding\nplt.scatter(book_ru[idx_include, 0], book_ru[idx_include, 1], \n            c = ints, cmap = plt.cm.tab10)\n\n# Add colorbar and appropriate labels\ncbar = plt.colorbar()\ncbar.set_ticks([])\nfor j, lab in enumerate(gen):\n    cbar.ax.text(1, (2 * j + 1) \/ ((10) * 2), lab, ha='left', va='center')\ncbar.ax.set_title('Genre', loc = 'left')\n\n\nplt.xlabel('UMAP 1'); plt.ylabel('UMAP 2'); plt.title('UMAP Visualization of Book Embeddings');","75da2f39":"plt.figure(figsize = (14, 12))\n\n# Plot all books\nplt.scatter(book_r[:, 0], book_r[:, 1], marker = '.', color = 'lightblue', alpha = 0.2)\n\n# Plot genres\nplt.scatter(book_r[idx_include, 0], book_r[idx_include, 1], \n            c = ints, cmap = plt.cm.tab10, alpha = 0.6)\n\n# Add colorbar and appropriate labels\ncbar = plt.colorbar()\ncbar.set_ticks([])\nfor j, lab in enumerate(gen):\n    cbar.ax.text(1, (2 * j + 1) \/ ((10) * 2), lab, ha='left', va='center')\n    \ncbar.ax.set_title('Genre', loc = 'left')\nplt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('TSNE Visualization of Book Embeddings');\n\nfor book in list(wikilink_book_counts.keys())[:10]:\n    \n    x, y = book_r[book_index[book], 0], book_r[book_index[book], 1];\n    # Italize book title using latex\n    s =  ''.join([' $\\it{' + word + '}$' for word in book.split()])\n    _ = plt.scatter(x, y, s = 250, color = 'r',\n                    marker = '*', edgecolor = 'k')\n    _ = plt.text(x - 10, y + 2, s, fontsize = 14);\n","75a4b7a1":"book_r[book_index['The Encyclopedia of Science Fiction']]","07ab5eb7":"book_r[book_index['The Encyclopedia of Fantasy']]","e9e907a1":"def plot_by_attribute(attribute):\n    \"\"\"Color book embedding by `attribute`\"\"\"\n    # Find all the attribute values\n    attrs = [book[1].get(attribute, 0) for book in books]\n\n    # Remove attributes not found\n    attr_counts = count_items(attrs)\n    del attr_counts[0]\n    \n    # Include 10 most popular attributes\n    attr_to_include, counts = list(attr_counts.keys())[:10], list(attr_counts.values())[:10]\n    \n    idx_include = []\n    attributes = []\n\n    # Iterate through books searching for the attribute\n    for i, book in enumerate(books):\n        # Limit to books with the attribute\n        if attribute in book[1].keys():\n            # Limit to attribute in the 10 most popular\n            if book[1][attribute] in attr_to_include:\n                idx_include.append(i)\n                attributes.append(book[1][attribute])\n                \n    # Map to integers\n    ints, attrs = pd.factorize(attributes)\n    plt.figure(figsize = (12, 10))\n\n    plt.scatter(book_r[:, 0], book_r[:, 1], marker = '.', color = 'lightblue', alpha = 0.2)\n    \n    # Plot embedding with only specific attribute highlighted\n    plt.scatter(book_r[idx_include, 0], book_r[idx_include, 1], alpha = 0.6,\n                c = ints, cmap = plt.cm.tab10, marker = 'o', s = 50)\n\n    # Add colorbar and appropriate labels\n    cbar = plt.colorbar()\n    cbar.set_ticks([])\n    tick_labels = [f'{attr}: {count}' for attr, count in zip(attr_to_include, counts)]\n    # Labeling\n    for j, lab in enumerate(tick_labels):\n        cbar.ax.text(1, (2 * j + 1) \/ ((10) * 2), lab, ha='left', va='center')\n    cbar.ax.set_title(f'{attribute.capitalize()}: Count', loc = 'left')\n\n\n    plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title(f'Book Embeddings with {attribute.capitalize()}');","64305964":"plot_by_attribute('genre')","f47d0415":"plot_by_attribute('author')","f0f7ec23":"plot_by_attribute('country')","bdb976e3":"plot_by_attribute('language')","36881fa4":"def plot_closest(item, weights, index_name, n, plot_data):\n    \"\"\"Plot n most closest items to item\"\"\"\n    \n    # Find the closest items\n    dist, closest = find_similar(item, weights, index_name, n, return_dist=True)\n    \n    # Choose mapping for look up\n    if index_name == 'book':\n        index = book_index\n        rindex = index_book\n    elif index_name == 'page':\n        index = link_index\n        rindex = index_link \n    \n    plt.figure(figsize = (10, 9))\n    plt.rcParams['font.size'] = 14\n    \n    # Limit distances\n    dist = dist[closest]\n    \n    # Plot all of the data\n    plt.scatter(plot_data[:, 0], plot_data[:, 1], alpha = 0.1, color = 'goldenrod')\n    \n    # Plot the item\n    plt.scatter(plot_data[closest[-1], 0], plot_data[closest[-1], 1], s = 600, edgecolor = 'k', color = 'forestgreen')\n    \n    # Plot the closest items\n    p = plt.scatter(plot_data[closest[:-1], 0], plot_data[closest[:-1], 1], \n                c = dist[:-1], cmap = plt.cm.RdBu_r, s = 200, alpha = 1, marker = '*')\n    \n    # Colorbar management\n    cbar = plt.colorbar()\n    cbar.set_ticks([])\n    \n    tick_labels = []\n    # Tick labeling for colorbar\n    for idx, distance in zip(closest[:-1], dist[:-1]):\n        name_str = ''\n        for word in rindex[idx].split():\n            # Title uses latex for italize\n            name_str += ' $\\it{' + word + '}$'\n        name_str += ': ' + str(round(distance, 2))\n        tick_labels.append(name_str)\n    \n    for j, lab in enumerate(tick_labels):\n        cbar.ax.text(1, (2 * j + 1) \/ ((n - 1) * 2), lab, ha='left', va='center', size = 12)\n    cbar.ax.set_title(f'{index_name.capitalize()} and Cosine Distance', loc = 'left', size = 14)\n    \n    # Formatting for italicized title\n    name_str = f'{index_name.capitalize()}s Most Similar to'\n    for word in item.split():\n        # Title uses latex for italize\n        name_str += ' $\\it{' + word + '}$'\n    \n    # Labeling\n    plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); \n    plt.title(name_str);","032ac5b9":"plot_closest('War and Peace', book_weights_class, 'book', 10, book_r)","f5680a94":"plot_closest('A Brief History of Time', book_weights_class, 'book', 10, book_r)","e0655caf":"link_r = reduce_dim(link_weights_class, components = 2, method = 'tsne')\nlink_r.shape","0bc50a65":"idx_to_include = [idx for link, idx in link_index.items() if 'category:' in link]\n\ncategories = []\n\nfor book in books:\n    for link in book[2]:\n        if 'category:' in link.lower():\n            categories.append(link)\n            \nc_counts = count_items(categories)\nlist(c_counts.items())[:5]","15d73da7":"idx = []\n\n# Find the index of the most popular links\nfor link in list(c_counts.keys())[:10]:\n    link_idx = link_index[link.lower()]\n    \n    # Find index of category \n    index = int(np.where(np.array(idx_to_include) == link_idx)[0])\n    idx.append(index)","82ab8207":"plt.figure(figsize = (12, 12))\nplt.scatter(link_r[:, 0], link_r[:, 1], alpha = 0.6)\n\nfor i in idx:\n    x, y = link_r[i, 0], link_r[i, 1]\n    s = index_link[idx_to_include[i]].split(':')[-1]\n    _ = plt.text(x, y, s, fontsize = 18);\n    \nplt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('Wikilinks Category Embedding Visualized with TSNE');","8f0a0ee1":"plot_closest('new york times', link_weights_class, 'page', 10, link_r)","b7a85930":"plot_closest('james joyce', link_weights_class, 'page', 10, link_r)","624bbccd":"plot_closest('margaret atwood', link_weights_class, 'page', 20, link_r)","5d777703":"plot_closest('leo tolstoy', link_weights_class, 'page', 20, link_r)","7fdc4020":"def plot_groups(items, weights, embed, index_name = 'book', n = 5):\n    closest = []\n    dists = []\n    \n    # Select index and reverse index\n    if index_name == 'book':\n        index = book_index\n        rindex = index_book\n    elif index_name == 'page':\n        index = link_index\n        rindex = index_link\n    \n    for item in items:\n        d, c = find_similar(item, weights, index_name, n, return_dist = True)\n        d = d[c]\n        closest.extend(c)\n        dists.extend(d)\n        \n    ax = plt.figure(figsize = (10, 8))\n    p = plt.scatter(embed[:, 0], embed[:, 1], alpha = 0.2, marker = 'o', color =  'lightblue')\n\n    cmap = plt.cm.get_cmap('tab10_r', len(items))\n    color_list = []\n    \n    for i, item in enumerate(items):\n         # Plot the item\n        plt.scatter(embed[index[item], 0], embed[index[item], 1], \n                    s = 200, alpha = 0.4, edgecolor = 'k', color = cmap(i))\n        color_list.extend(i for _ in range(n))\n        \n    p = plt.scatter(embed[closest, 0], embed[closest, 1], c = color_list, cmap = cmap, s = 150,\n                    marker = '*', alpha = 0.8)\n\n    \n    cbar = plt.colorbar(p)\n    cbar.set_ticks([])\n    \n    tick_labels = []\n    # Tick labeling for colorbar\n    for item in items:\n        name_str = ''\n        for word in item.split():\n            # Title uses latex for italize\n            name_str += ' $\\it{' + word + '}$'\n        tick_labels.append(name_str)\n        \n    for j, lab in enumerate(tick_labels):\n        cbar.ax.text(1, (2 * j + 1) \/ (len(items) * 2), lab, ha='left', va='center', size = 12)\n    cbar.ax.set_title(f'Highlighted {index_name.capitalize()}s', loc = 'left', size = 14)\n    plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title(f'Embedded {index_name.capitalize()}s with Closest Neighbors');","88665a53":"plot_groups(['War and Peace', \"The Once and Future King\", \n             \"Weapons of Math Destruction\", \"The Fellowship of the Ring\",\n             \"A Brief History of Time\", \"Enlightenment Now\"],\n            book_weights_class, book_r, 'book')","68662412":"plot_groups(['short stories', 'novel',\n            'biography', 'historical novel', \n             'science fiction', 'non-fiction'], \n            link_weights_class, link_r, 'page')","7c762e69":"random.seed(150)\nplot_groups(list(random.sample(book_index.keys(), 6)),\n            book_weights_class, book_r, 'book')","99802cde":"random.seed(150)\nplot_groups(list(random.sample(link_index.keys(), 6)), link_weights_class, link_r, 'page')","b357fa19":"# %%capture\n# with open('..\/embeddings\/link_names.tsv', 'w' , encoding = 'utf-8') as fout:\n#     for l in link_index.keys():\n#         fout.write(str(l))\n#         fout.write('\\n')","0f8c01ab":"import re\npattern = re.compile('[\\\\n]|<.*?>')\npattern.sub('', 'bill moushey \\n < br >').strip()","24515cd6":"import re\npattern = re.compile('[\\\\n]|<.*?>')\n\n# Extract book metadata\nauthors = []\ngenres = []\npages = []\nlanguages = []\ncountries = []\npublishers = []\n\n\nfor book in books:\n    info = book[1]\n    for attr, l in zip(['author', 'genre', 'pages', 'langauge', 'country', 'publisher'],\n                        [authors, genres, pages, languages, countries, publishers]):\n        l.append(pattern.sub('', info.get(attr, 'None').lower()).strip())\n\nbook_info = pd.DataFrame({'author': authors, 'genre': genres, 'pages': pages,\n                          'language': languages, 'country': countries,\n                          'publisher': publishers}, index = list(book_index.keys()))\n\nbook_info = book_info.replace({'none': np.nan})\nbook_info.index.name = 'title'\nbook_info.head()","562fa6f9":"# book_info.to_csv('..\/embeddings\/book_info.tsv', sep = '\\t')","ba404977":"# np.savetxt('..\/embeddings\/book_embedding.tsv', book_weights_class, delimiter='\\t')\n# np.savetxt('..\/embeddings\/link_embedding.tsv', link_weights_class, delimiter='\\t')","40b2c407":"# book_embedding = np.loadtxt('..\/embeddings\/book_embedding.tsv', delimiter = '\\t')\n# book_embedding.shape","6433a8c1":"We only want to count wikilinks from each book once, so we first find the set of links for each book, then we flatten the list of lists to a single list, and finally pass it to the `count_items` function.","bfe5119d":"There's nothing wrong with books that link to the same page many times. They are just more likely to be trained on since there are more of them.","04362bd7":"# Introduction: Book Recommendation System\n\nIn this notebook, we will build a book recommendation system based on a simple principle: books with Wikipedia pages that link to similar Wikipedia pages are similar to each other. In order to create this representation of similar books, we'll use the concept of neural network entity embeddings, mapping each book and each Wikipedia link (Wikilink) to a 50-number vector. \n\nThe idea of entity embeddings is to map high-dimensional categorical variables to a low-dimensional _learned_ representation that _places similar entities closer together in the embedding space_. If we were to one-hot-encode the books (another representation of categorical data) we would have a 37,000 dimension vector for each book, with a single 1 indicating the book. In a one-hot encoding, similar books would not be \"closer\" to one another. By  training a neural network to learn entity embeddings, we not only get a reduced dimension representation of the books, we also get a representation that _keeps similar books closer to each other_. Therefore, the basic approach for a recommendation system is to create entity embeddings of all the books, and then for any book, find the closest other books in the embedding space. [Fortunately, thanks to a previous notebook](https:\/\/github.com\/WillKoehrsen\/wikipedia-data-science\/blob\/master\/Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb), we have access to every single book article on Wikipedia, which will let us create an effective recommendation system.\n\n## Approach\n\nTo create entity embeddings, we need to build an embedding neural network and train it on a supervised machine learning task that will result in similar books (and similar links) having closer representations in embedding space. The parameters of the neural network - the weights - are the embeddings, and so during training, these numbers are adjusted to minimize the loss on the prediction problem. In other words, the network tries to accurately complete the task by changing the representations of the books and the links. \n\nOnce we have the embeddings for the books and the links, we can find the most similar book to a given book by computing the distance between the embedded vector for that book and all the other book embeddings. We'll use the cosine distance which measures the angle between two vectors as a measure of similarity (another valid option is the Euclidean distance). We can also do the same with the links, finding the most similar page to a given page. (I use links and wikilinks interchangeably in this notebook). The steps we will follow are:\n\n1. Load in data and clean\n2. Prepare data for supervised machine learning task\n3. Build the entity embedding neural network\n4. Train the neural network on prediction task\n5. Extract embeddings and find most similar books and wikilinks\n6. Visualize the embeddings using dimension reduction techniques \n\n### Supervised Machine Learning Task: Map Books to Links\n\nFor our machine learning task, we'll set up the problem as identifying whether or not a particular link was present in a book article. The training examples will consist of (book, link) pairs, with some pairs true examples - actually  in the data - and others negative examples - do not occur in the data. It will be the network's job to adjust the entity embeddings of the books and the links in order to accurately make this classification. Although we are training for a supervised machine learning task, our end objective is not to make accurate predictions on new data, but learn the best entitiy embeddings, so we do not use a validation or testing set. We use the prediction problem as a means to an end rather than the final outcome. \n\n## Neural Network Embeddings\n\nNeural Network embeddings have proven to be very powerful concepts both for modeling language and for representing categorical variables. For example, the [Word2Vec word embeddings](https:\/\/www.tensorflow.org\/tutorials\/representation\/word2vec) map a word to a vector based on training a neural network on millions of words. These embeddings can be used in any supervised model because they are just numerical representations of categorical variables. Much as we one-hot-encode categorical variables to use them in a random forest for a supervised task, we can also use entity embeddings to include categorical variables in a model. The embeddings are also useful because we can find entities that are close to one another in embedding space which might - as in a book recommendation system - allow us to find the most similar categories among tens of thousands of choices. \n\nWe can also use the Entity Embeddings to visualize words or categorical variables, such as creating a map of all books on Wikipedia. The entity embeddings typically are still high-dimensional - we'll use 50 numbers for each entity - so we need to use a dimension reduction technique such as TSNE or UMAP to visualize the embeddings in lower dimensions. (These are both manifold embedding methods so in effect we will embed the embeddings for visualization!) We'll take a look at doing this at the end of the notebook and later will upload the embeddings into a application custom-built for this purpose ([projector.tensorflow.org](https:\/\/projector.tensorflow.org)). Entity embeddings are becoming more widespread thanks to the ease of development of neural networks in Keras and are a useful approach when we want to represent categorical variables with vectors that place similar categories close to one another. Other approaches for encoding categorical variables do not represent similar entities as being closer to one another, and entity embedding is a _learning-based method_ for this important task.\n\nOverall, this project is a great look at the potential for neural networks to create meaningful embeddings of high dimensional data and a practical application of deep learning. The code itself is relatively simple, and the Keras library makes developing deep learning models enjoyable!\n\nThe code here is adapted from the excellent [Deep Learning Cookbook](http:\/\/shop.oreilly.com\/product\/0636920097471.do), the [notebooks for which can be found on GitHub](https:\/\/github.com\/DOsinga\/deep_learning_cookbook). Check out this book for practical applications of deep learning and great projects! ","417caf10":"We will only use the wikilinks, which are saved as the third element (index 2) for each book.","248b343f":"How many of these are links to other books? ","bad3bab0":"The `Encyclopedia`s of Science Fiction and Fantasy have nearly perfect overlap. ","a7e6f5fc":"We can use the same `find_similar` function to find the most similar links to a given link.","2739a467":"The next image shows all the link embedded with the 10 most popular categories labeled.","bb3a6e9a":"Later on we'll create the negative examples by randomly sampling from the links and the books and making sure the resulting pair is not in `pairs`. ","e217ae1c":"Not bad by the looks of the results! I encourage you to play around with the model and explore the resulting embeddings. __We have successfully built a book recommendation system using neural network embeddings.__\n\nWhat should I read next? Currently I'm working through a fantastic collection of essays by Stephen Jay Gould.","eb4332ca":"We can see that even though these are the closest books in the 50-dimensional embedding space, when we reduce it down to 2 dimensions, the same separations are not preserved. ","7c1de2e1":"The book embeddings and link embeddings can be saved as tab separated files. We'll save all 50 dimensions of the embeddings because the tool will perform PCA (principal components analysis) or TSNE for us.","5bb4a7b0":"Another option we have to try and make sense of this data is plot books that originally were closest together in the original embedding space. Reducing the dimensions to 2 may distort the distances between similar books in the 50 dimensional space.","e32aa35d":"The least similar books are quite a grab bag!","49cfc684":"The most linked to pages are in fact not that surprising! One thing we should notice is that there are discrepancies in capitalization. We want to normalize across capitalization, so we'll lowercase all of the links and redo the counts.","3aea3617":"We've now taken the initial 37,000 dimension book vector and reduced it to just 2 dimensions.","9df99284":"Normalize just means divide each vector by the square root of the sum of squared components.","938e1a72":"There doesn't appear to be much separation between the categories in the UMAP clustering. There are a lot of parameters to play around with in UMAP, and changing some of them might result in better clusters.","1e3d7e86":"Things are looking pretty good with this model as well. It's hard to tell if this model is \"better\" than the regression model in the sense that the recommendations are improved. I would say that both are useful in surfacing similar books and pages.\n\nLet's take a look at the link recommendations for this model.","cfd84862":"As a final visualization effort, we can try to identify clumpings of books or articles by plotting multiple books and those most similar. We'll show all of the embedding books in the background and then highlight the most similar books around a list of query books. ","4d8cb73b":"Now you not only know what books to look for, you know the books to avoid for a given category! ","2b3c1c74":"That actually changes the rankings! This illustrates an important point: __make sure to take a look at your data before modeling!__ \n\n#### Remove Most Popular Wikilinks\n\nI'm going to remove the __most popular__ wikilinks because these are not very informative. Knowing whether a book is hardcover or paperback is not that important to the content. We also don't need the two `Wikipedia:` links since these do not distinguish the books based on content. __I'd recommend playing around with the wikilinks that are removed because it might have a large effect on the recommendations.__\n\n(This step is similar to the idea of [TF-IDF (Term Frequency Inverse Document Frequency](http:\/\/www.tfidf.com\/). When dealing with words in documents, the words that appear most often across documents are usually not that helpful because they don't distinguish documents. TF-IDF is a way to weight a word higher for appearing more often within an article but decrease the weighting for a word appearing more often between articles.)","a0890c5d":"## Plot Book Nearest Neighbors\n\nTo get a better sense of which books are located where, we can plot a book along with its nearest neighbors. These will be the nearest neighbors in the original embedding space, so they are not necessarily the closest in the reduced dimension representation.","4ab60bd5":"We'll get the 10 most popular categories so we can plot them on the embedding.","13334636":"Each legitimate book contains the __title, the information from the `Infobox book` template, the internal wikipedia links, the external links, the date of last edit, and the number of characters in the article__ (a rough estimate of the length of the article).","befbc849":"This time, we get a much better grouping! I guess we have found the corner of Wikipedia books about the entirety of the universe.","432477cc":"We can also plot the closest pages to a given page. ","d2930d48":"# Neural Network Embedding Model\n\nWith our dataset and a supervised machine learning task, we're almost there. The next step is the most technically complicated but thankfully fairly simple with Keras. We are going to construct the neural network that learns the entity embeddings. The input to this network is the (book, link) (either positive or negative) as integers, and the output will be a prediction of whether or not the link was present in the book article. However, we're not actually interested in the prediction except as the device used to train the network by comparison to the label. What we are after is at the heart of the network: the embedding layers, one for the book and one for the link each of which maps the input entity to a 50 dimensional vector. The layers of our network are as follows:\n\n1. Input: parallel inputs for the book and link\n2. Embedding: parallel embeddings for the book and link\n3. Dot: computes the dot product between the embeddings to merge them together\n4. Reshape: utility layer needed to correct the shape of the dot product\n5. [Optional] Dense: fully connected layer with sigmoid activation to generate output for classification\n\nAfter converting the inputs to an embedding, we need a way to combine the embeddings into a single number. For this we can use the dot product which does element-wise multiplication of numbers in the vectors and then sums the result to a single number. This raw number (after reshaping) is then the ouput of the model for the case of regression. In regression, our labels are either -1 or 1, and so the model loss function will be mean squared error in order to minimize the distance between the prediction and the output. Using the dot product with normalization means that the `Dot` layer is finding the cosine similarity between the embedding for the book and the link. Using this method for combining the embeddings means we are trying to make the network learn similar embeddings for books that link to similar pages. \n\n### Classification vs Regression\n\nFor classification, we add an extra fully connected `Dense` layer with a `sigmoid` activation to squash the outputs between 0 and 1 because the labels are either 0 or 1. The loss function for classification is `binary_crossentropy` which measures the [error of the neural network predictions in a binary classification problem](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/loss_functions.html), and is a measure of the similarity between two distributions. We can train with either classification or regression, and in practice, I found that both approaches produced similar embeddings. I'm not sure about the technical merits of these methods, and I'd be interested to hear if one is better than the other. \n\nThe optimizer - the algorithm used to update the parameters (also called weights) of the neural network after calculating the gradients through backpropagation - is Adam in both cases ([Adam is a modification to Stochastic Gradient Descent](https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/)). We use the default parameters for this optimizer. The nice thing about modern neural network frameworks is we don't have to worry about backpropagation or updating the model parameters because that is done for us. It's nice to have an idea of what is occuring behind the scenes, but it's not entirely necessary to use a neural network effectively. ","0e53cf82":"# Train Model\n\nWe have the training data - in a generator - and a model. The next step is to train the model to learn the entity embeddings. During this process, the model will update the embeddings (change the model parameters) to accomplish the task of predicting whether a certain link is on a book page or not. The resulting embeddings can then be used as a representation of books and links. \n\nThere are a few parameters to adjust for training. The batch size should generally be as large as possible given the memory constraints of your machine. The negative ratio can be adjusted based on results. I tried 2 and it seemed to work well. The number of steps per epoch is chosen such that the model sees a number of examples equal to the number of pairs on each epoch. This is repeated for 15 epochs (which might be more than necessary).","0048dbc1":"Each book is now represented as a 50-dimensional vector. \n\nWe need to normalize the embeddings so that the dot product between two embeddings becomes the cosine similarity.","9b34a0a6":"### Book Embeddings by Genre\n\nThe above graphs are difficult to interpret. Let's plot the embeddings by the `genre` which is contained in the `Infobox` template data for each book. We'll limit it to the 10 most popular genres.","6957bcce":"https:\/\/github.com\/WillKoehrsen\/wikipedia-data-science\/blob\/master\/notebooks\/Book%20Recommendation%20System.ipynb","c5ad315c":"The algorithm has spoken! ","a47bc3fe":"We do see some clumpings, but it's difficult to label them. If this was interactive, then we could get a lot more use from it. (This will be an upcoming topic for an article).","81983ff3":"## Wikilinks to Index\n\nAs with the books, we need to map the Wikilinks to integers. We'll also create the reverse mapping.","3cddca0b":"Based on my knowledge, it appears the neural network embeddings are also a good representation of the pages! Not only can we find the most similar books to a given book, but if we have a category of books that we enjoy, we can find another category that is similar.","4f827788":"## Pages Visualization\n\nLet's look at reducing the dimension of the embedding for the wikilinks. We'll then visualize them in the same way as with the books.","39ad2a44":"## Wikilink Embeddings\n\nWe also have the embeddings of wikipedia links (which are themselves Wikipedia pages). We can take a similar approach to extract these and find the most similar to a query page. \n\nLet's write a quick function to extract weights from a model given the name of the layer.","5c6b7f1f":"Let's make a function that can color the plot by any attribute in the book infobox. We already saw this with the genre, but we can extend it to the author or any other information.","ad5a03ec":"## Data Cleaning\n\nThere are a few articles that were caught which are clearly not books (feel free to check out these articles yourself).","78dd98e7":"### Exploring Wikilinks\n\nAlthough it's not our main focus, we can do a little exploration. Let's find the number of unique Wikilinks  and the most common ones. To create a single list from a list of lists, we can use the `itertools` chain method.","80992af7":"We now have over 770,000 positive examples on which to train! Each pair represents one Wikilink for one book. Let's look at a few examples.","cf261349":"Finally, we can plot the embedding colored by the genre","735df7ec":"# Finding Similar Books\n\nWe've trained the model and extracted the embeddings - great - but where is the book recommendation system? Now that we have the embeddings, we can use them to recommend books that our model has learned are most similar to a given book.\n\n\n### Function to Find Most Similar Entities\n\nThe function below takes in either a book or a link, a set of embeddings, and returns the `n` most similar items to the query. It does this by computing the dot product between the query and embeddings. Because we normalized the embeddings, the dot product represents the [cosine similarity](http:\/\/blog.christianperone.com\/2013\/09\/machine-learning-cosine-similarity-for-vector-space-models-part-iii\/) between two vectors. This is a measure of similarity that does not depend on the magnitude of the vector in contrast to the Euclidean distance. (The Euclidean distance would be another valid metric of similary to use to compare the embeddings.)\n\nOnce we have the dot products, we can sort the results to find the closest entities in the embedding space. With cosine similarity, higher numbers indicate entities that are closer together, with -1 the furthest apart and +1 closest together.","6c0ab8a6":"The loss decreases as training progresses which should give us confidence the model is learning something! \n\n\nThe entire trained model can be saved and later loaded in so you don't have to repeat the training. It's also possible to save certain layers.","9839526c":"Now we need to map the genres to integers to plot them as colors. ","fe585dc3":"## Most Popular Books in Embedding\n\nLet's see the embedding labeled with the 10 books most often mentioned by other books. ","84883fb6":"It's not surprising that several of these are references. We also see that a few classics make it into the list! \n\n### Potential Additional Cleaning Step\n\nIf you want to try more data cleaning, one option would be to clean the link entities. For example, both `the new york times` and `new york times` are in the links. These could clearly be combined into a single entry because they link to the same exact page. This might require manual inspection of the links, and I decided not to do this because of the time involved! The final embeddings turned out well even without this step, but it might make sense to do in the future. \n\nI'm not sure why the same link is represented as two different names (I extracted the title of the link to try and alleviate this issue), but it occurs many times, even for the same book! ","aa58bd03":"## Generator For Training Samples\n\nWe need to generate positive samples and negative samples to train the neural network. The positive samples are simple: pick a pair from `pairs` and assign it a 1. The negative samples are also fairly easy: pick one random link and one random book, make sure they are not in `pairs`, and assign them a -1 or a 0. (We'll use either a -1 or 0 for the negative labels depending on whether we want to make this a regression or a classification problem. Either approach is valid, and we'll try out both methods.)\n\nThe code below creates a generator that yields batches of samples each time it is called. Neural networks are trained incrementally - a batch at a time - which means that a generator is a useful function for returning examples on which to train. Using a generator alleviates the need to store all of the training data in memory which might be an issue if we were working with a larger dataset such as images. ","73c42179":"## Note about Training \/ Testing Set\n\nTo compute the embeddings, we are not going to create a separate validation or testing set. While this is a __must__ for a normal supervised machine learning task, in this case, our primary objective is not to make the most accurate model, but to generate the best embeddings. The prediction task is just the method through which we train our network to make the embeddings. At the end of training, we are not going to be testing our model on new data, so we don't need to evaluate the performance. Instead of testing on new data, we'll look at the embeddings themselves to see if books that we think are similar have embeddings that are close to each other. \n\nIf we kept a separate validation \/ testing set, then we would be limiting the amount of data that our network can use to train. This would result in less accurate embeddings. Normally with any supervised model, we need to be concerned about overfitting, but again, because we do not need our model to generalize to new data and our goal is the embeddings, we will make our model as effective as possible by using all the data for training. In general, always have a separate validation and testing set (or use cross validation) and make sure to regularize your model to prevent overfitting. ","bef82a5f":"Just for fun, let's look at the (book, link) pairs that are represented most often in the data. ","d52f3224":"### Save Embeddings for Visualization\n\nWe can save these embeddings for visualization in [projector.tensorflow.org](https:\/\/projector.tensorflow.org\/). In order to work with this tool, the files must be tab separated. First, we'll save the names and book info to use as metadata.","9c9e17cb":"It doesn't work perfectly for every book as we can see.","e4a0078d":"# Conclusions\n\nIn this notebook, we built an effective book recommendation system using the principle that books with similar outgoing links are similar and all of the book articles on Wikipedia. We embedded both the wikilinks and the books using a neural networ. To train the neural network, we developed a supervised machine learning problem of classifying if a given link was present on the page for a book.\n\nMore than just training the neural network, we saw how to thoroughly inspect the embeddings in order to find the closest books to a given book in embedding space. We also saw how to visualize the embeddings which sometimes can show us interesting clusterings. In the process, we took the original 37,000 dimensions of the books and reduced it first to 50 using the neural network and then to 2 using a manifold learning method. The neural network embedding was useful for making recommendations based on closest entities while the TSNE embedding is useful primarily for visualization.\n\nWe covered a number of topics that may be useful in a data science position:\n\n1. How to clean data for a machine learning project\n2. How to set up a supervised learning task from a raw dataset\n3. How to train a neural network to learn entity embeddings\n4. How to inspect entity embeddings to find closest entities to a query\n5. How to reduce the dimension of a dataset for visualization\n6. How to format a plot to try and make sense of the data (we'll work a little more on this)\n\nThe next step that I'll work on is to visualize these embeddings in an interactive tool. Interactivity adds another element of insightto data analysis and I enjoy the exploratory nature of plots that let you move through the data. \n\n## Potential Other Projects\n\nThere are several ways you can take this and build on it:\n\n1. Train the embeddings using not the internal links, but the external links\n2. Train the embeddings using the infobox information. This would require framing a different machine learning problem\n3. Make a recommendation system for other wikipedia articles such as people or landmarks.\n4. Adjust the parameters of the models and data cleaning in this notebook to arrive at any other insights.\n\nThe most important part about learning data science is to practice data science! Therefore, I encourage anyone to alter this notebook, build on the code, share it with others, and best of all, teach it to others! The best way to learn is by doing and then teaching, so get started! \n\nBest,\n\nWill","4d17fbaa":"## Map Books to Integers\n\nFirst we want to create a mapping of book titles to integers. When we feed books into the embedding neural network, we will have to represent them as numbers, and this mapping will let us keep track of the books. We'll also create the reverse mapping, from integers back to the title.","4086db15":"## Plot Multiple Books and Pages","6fa3d477":"The neural network will take in the book index and the link index and try to embed them in such a way that it can predict the label from the embeddings.","1c506558":"## Read in Data\n\nThe data is stored as json with line for every book. This data contains every single book article on Wikipedia which was parsed in the [Downloading and Parsing Wikipedia Data Notebook](https:\/\/github.com\/WillKoehrsen\/wikipedia-data-science\/blob\/master\/Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb).","98ec75e8":"It works! The most similar books make sense at least for _War and Peace._\n\n","be9e675a":"The books do seem to be slightly separated based on the genre. The categories aren't exactly that helpful but we did what we could! We can also try UMAP. ","759007da":"# Supervised Machine Learning Task\n\nNow that we have clean data, we'll move on to the second step: developing a supervised machine learning task to train an embedding neural network. As a reminder, we'll state the problem as: given a book title and a link, identify if the link is in the book's article.\n\n## Build a Training Set\n\nIn order for any machine learning model to learn, it needs a training set. We are going to treat this as a supervised learning problem: given a pair (book, link), we want the neural network to learn to predict whether this is a legitimate pair - present in the data - or not.\n\nTo create a training set, for each book, we'll iterate through the wikilinks on the book page and record the book title and each link as a tuple. The final `pairs` list will consist of tuples of every (book, link) pairing on all of Wikipedia.","de605d55":"# Extract Embeddings and Analyze\n\nThe trained model has learned - hopefully - representations of books and wikilinks that place similar entities next to one another in the embedding space. To find out if this is the case, we extract the embeddings and use them to find similar books and links.","c6d7410b":"Since there are so many unique wikilinks, I'm going to limit the list to wikilinks mentioned 4 or more times. Hopefully this reduces the noise that might come from wikilinks that only appear a few times. Keeping every single link will increas the training time significantly, but experiment with this parameter if you are interested.","04f2776a":"#### Most Linked-to Books\n\nAs a final bit of exploration, let's look at the books that are mentioned the most by other books on Wikipedia. We'll take the set of links for each book so that we don't have multiple counts for books that are linked to by another book more than once. ","e23baf50":"(We know that this function works if the most similar book is the book itself. Because we multiply the item vector times all the other embeddings, the most similar should be the item itself with a similarity of 1.0.)","65148555":"## Classification Model\n\nI was curious if training for the mean squared error as a regression problem was the ideal approach, so I also decided to experiment with a classification model. For this model, the negative examples receive a label of 0 and the loss function is binary cross entropy. The procedure for the neural network to learn the embeddings is exactly the same, only it will be optimizing for a slightly different measure.","a8b43083":"We'll go through the same process, extracting the weights and finding similar books based on the embedding space representation.","bfc23333":"### Most Linked-to Articles\n\nLet's take a look at which pages are most linked to by books on Wikipedia. \n\nWe'll make a utility function that takes in a list and returns a sorted ordered dictionary of the counts of the items in the list. The `collections` module has a number of useful functions for dealing with groups of objects.","be6a4b97":"There are nearly 4.0 million weights (parameters) that need to be learned by the neural network. Each of these represents one number in an embedding for one entity. During training, the neural network adjusts these parameters in order to minimize the loss function on the training data. ","468d351c":"The best clumpings are probably by genre although the countries also appear to be separated. For the language and country, it might be more accurate to do this will book articles on Wikipedias other than just the English version.","f5bc60f3":"There do appear to be a few noticeable clumps. However, it's difficult to derive any meaning from this plot since we aren't distinguishing books in any way.","8beee9d3":"Looks like I have some new topics to explore!","6a3a8d39":"# Visualizations\n\nOne of the most interesting parts about embeddings is that we can use them to visualize concepts such as _War and Peace_ or _biography_. First we have to take the embeddings from 50 dimensions down to either 3 or 2. We can do this using `pca`, `tsne`, or `umap`. We'll try both tsne and umap for comparison. TSNE takes much longer and is designed to retain local structure within the data. UMAP is generally quicker and is designed for a balance between local and global structure in the embedding.\n\n## Manifold Embeddings\n\n[TSNE: t-Stochastic Distributed Neighbors Embedding](https:\/\/lvdmaaten.github.io\/tsne\/), and [UMAP: Uniform Manifold Approximation and Projection](https:\/\/github.com\/lmcinnes\/umap), are both methods that use the idea of a manifold to map vecotrs to a lower dimensional embedded space. Therefore, we are taking the 37,000 dimensions in the case of books, embedding them to 50 dimensions with the neural network, and then embedding them down to 2 dimensions with a manifold. The primary idea behind dimension reduction with a manifold is that there is a lower dimensional representation of the vectors that can still capture the variation between different groups. We want the embeddings to represent similar entities close to one another but in fewer dimensions that allow us to visualize the entities. ","30133ba1":"To get a new batch, call `next` on the generator. "}}