{"cell_type":{"02a3ee2a":"code","352263de":"code","80c1f3e8":"code","5382aea4":"code","0a241345":"code","87d3f3be":"code","31df5b32":"code","8d8bc44c":"code","70790fc7":"code","9545f3db":"code","6d97cb40":"code","64502e3f":"code","124c23e7":"code","13f1c40c":"code","f12cfb1b":"code","462fe4cd":"code","06505cd3":"code","9d091ef0":"code","afc3a97f":"code","cbf12a46":"code","e0077308":"code","3d80c8b6":"code","5b1cdef4":"code","6f2803b8":"code","91c1af14":"code","a82f847c":"code","2fdbeb91":"code","4d5f59e3":"code","265b3404":"code","e6d305cd":"code","cc1b389a":"code","a4e4ba6b":"code","411e46ee":"code","306a5f16":"code","a461c538":"code","228015ea":"code","a4518c56":"code","2a4bb93f":"code","1388d133":"code","2d454bf1":"code","c8448e4a":"code","6a7fa126":"code","912c0b74":"code","f9299b81":"code","0d6b6259":"code","3b6a86b9":"code","e3c66ad2":"code","c3618c08":"code","44b286a4":"code","be181d71":"code","156e6ba1":"markdown","5bf4d630":"markdown","33548c65":"markdown","59cea51e":"markdown"},"source":{"02a3ee2a":"from collections import OrderedDict\nimport os\nimport string\n\nimport cycler\nimport gensim\nfrom IPython.display import display, HTML\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport pyLDAvis.gensim\nimport PyPDF2\nfrom sklearn import feature_extraction, decomposition\nimport unidecode\nimport warnings\nimport wordcloud\n\n%matplotlib inline\nnltk.download(\"stopwords\", \"punkt\")\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\ncolor = plt.cm.tab20b(np.linspace(0, 1, 15))\nmpl.rcParams['axes.prop_cycle'] = cycler.cycler('color', color)\nwarnings.filterwarnings('ignore')\nSTOPWORDS = unidecode.unidecode(\" \".join(nltk.corpus.stopwords.words(\"portuguese\"))).split()","352263de":"PASTA_PROGRAMAS = \"..\/input\/\"\ndf = pd.DataFrame(os.listdir(PASTA_PROGRAMAS), columns=[\"arquivo\"])\ndf = df[df[\"arquivo\"].str.endswith(\"pdf\")]\ndf[\"candidato\"] = df[\"arquivo\"].str.replace(\".pdf\", \"\").str.split(\"_\").str.join(\" \").str.strip()\ndf[\"arquivo\"] = PASTA_PROGRAMAS+df[\"arquivo\"]\ndf = df.set_index(\"candidato\").sort_index()","80c1f3e8":"df[\"paginas\"] = df[\"arquivo\"].apply(lambda linha: PyPDF2.PdfFileReader(linha).getNumPages())\ndf.loc[\"Marina Silva\", \"paginas\"] *=2 # Layout de livreto","5382aea4":"def ler_texto(row):\n    with open(row, \"r\") as fp:\n        return fp.read()\n    \ndf[\"texto\"] = df[\"arquivo\"].str.replace(\".pdf\", \".txt\").apply(ler_texto)\n\ndf.loc[\"Ciro Gomes\", [\"texto\"]] = df.loc[\"Ciro Gomes\", [\"texto\"]]\\\n    .str.replace(\"\\nn \\x07\", \"\\n\")\\\n    .str.replace(\"\\n\\x07 |\\x0cn|\\x07|(\\\\nn)+|\\tn\", \"\")  # WTF Ciro?!?\n\ndf[\"texto\"] = df[\"texto\"].str.strip().str.replace(\"\\n+\", \"\\n\").str.lower().apply(unidecode.unidecode)\ndf[\"caracteres\"] = df[\"texto\"].apply(len)","0a241345":"def estatisticas_periodo(row):\n    lista_de_per\u00edodos = nltk.tokenize.sent_tokenize(row[\"texto\"])\n    palavras = pd.Series(lista_de_per\u00edodos)\\\n        .str.replace(f\"[{string.punctuation}]\", \"\")\\\n        .apply(nltk.word_tokenize).apply(len)\n    pontuacao = pd.Series(lista_de_per\u00edodos)\\\n        .str.replace(f\"[^{string.punctuation}]\", \"\")\\\n        .apply(nltk.word_tokenize).apply(len)\n    return pd.Series({\n        \"lista_de_periodos\": lista_de_per\u00edodos,\n        \"periodos\": len(palavras),\n        \"periodos_por_pagina\": len(palavras)\/row[\"paginas\"],\n        \"palavras_por_periodo\": palavras.mean(),\n        \"palavras_por_periodo_std\": palavras.std(),\n        \"palavras_por_periodo_max\": palavras.max(),\n        \"pontuacao_por_periodo\": pontuacao.mean(),\n        \"pontuacao_por_periodo_std\": pontuacao.std(),\n        \"pontuacao_por_periodo_max\": pontuacao.max()\n    })\n\ndf = pd.concat([df, df[[\"texto\", \"paginas\"]].apply(estatisticas_periodo, axis=1)], axis=1)","87d3f3be":"def estat\u00edsticas_palavras(row):\n    lista_de_palavras = row[[\"texto\"]].str.replace(\"\\n\", \" \")\\\n        .str.replace(f\"[{string.punctuation}]\", \"\")\\\n        .apply(nltk.word_tokenize)\n    pontuacao = row[[\"texto\"]].str.replace(\"\\n\", \" \")\\\n        .str.replace(f\"[^{string.punctuation}]\", \"\")\\\n        .apply(nltk.word_tokenize).apply(len).values[0]\n    palavras = pd.Series(lista_de_palavras.values[0]).apply(len)\n    return pd.Series({\n        \"lista_de_palavras\": lista_de_palavras[\"texto\"],\n        \"palavras\": len(palavras),\n        \"palavras_unicas\": len(set(lista_de_palavras.values[0])),\n        \"palavras_por_pagina\": len(palavras)\/row[\"paginas\"],\n        \"pontuacao\": pontuacao,\n        \"pontuacao_por_pagina\": pontuacao\/row[\"paginas\"],\n        \"palavras_por_pontuacao\": len(palavras)\/pontuacao,\n        \"tamanho_das_palavras\": palavras.mean(),\n        \"tamanho_das_palavras_std\": palavras.std(),\n    })\n\ndf = pd.concat([df, df[[\"texto\", \"paginas\"]].apply(estat\u00edsticas_palavras, axis=1)], axis=1)","31df5b32":"df[\"paginas\"].plot.bar(figsize=(15, 10), rot=60, title=\"N\u00famero de P\u00e1ginas\")","8d8bc44c":"df[\"caracteres\"].plot.bar(figsize=(15, 10), rot=60, title=\"N\u00famero de Caracteres\")","70790fc7":"df[\"periodos\"].plot.bar(figsize=(15, 10), rot=60, title=\"N\u00famero de Per\u00edodos\")","9545f3db":"df[\"periodos_por_pagina\"].plot.bar(figsize=(15, 10), rot=60, title=\"Per\u00edodos por P\u00e1gina\")","6d97cb40":"df[\"palavras_por_periodo\"].plot.bar(figsize=(15, 10), rot=60, title=\"Per\u00edodos por P\u00e1gina\", yerr=df[\"palavras_por_periodo_std\"])","64502e3f":"df[\"pontuacao_por_periodo\"].plot.bar(figsize=(15, 10), rot=60, title=\"Pontua\u00e7\u00e3o por Per\u00edodo\", yerr=df[\"pontuacao_por_periodo_std\"])","124c23e7":"df[\"palavras\"].transpose().plot.bar(figsize=(15, 10), rot=60, title=\"N\u00famero de Palavras\")","13f1c40c":"df[\"palavras_unicas\"].transpose().plot.bar(figsize=(15, 10), rot=60, title=\"Palavras \u00danicas\")","f12cfb1b":"df[\"palavras_por_pagina\"].plot.bar(figsize=(15, 10), rot=60, title=\"Palavras por P\u00e1gina\")","462fe4cd":"df[\"pontuacao\"].plot.bar(figsize=(15, 10), rot=60, title=\"Quantidade de Pontua\u00e7\u00e3o\")","06505cd3":"df[\"pontuacao_por_pagina\"].plot.bar(figsize=(15, 10), rot=60, title=\"Pontua\u00e7\u00e3o por P\u00e1gina\")","9d091ef0":"df[\"palavras_por_pontuacao\"].plot.bar(figsize=(15, 10), rot=60, title=\"Palavras por Pontua\u00e7\u00e3o\")","afc3a97f":"df[\"tamanho_das_palavras\"].plot.bar(figsize=(15, 10), rot=60, title=\"Tamanho M\u00e9dio das Palavras\", yerr=df[\"tamanho_das_palavras_std\"])","cbf12a46":"df.drop(columns=[\"arquivo\", \"texto\", \"lista_de_periodos\", \"lista_de_palavras\"])","e0077308":"def calcular_palavras_unicas(row):\n    bag = set()\n    data = []\n    for palavra in row:\n        bag.add(palavra)\n        data.append(len(bag))\n    return data\n    \ndf_palavras_unicas = df[\"lista_de_palavras\"].apply(calcular_palavras_unicas).apply(pd.Series).transpose()","3d80c8b6":"select_candidatos = OrderedDict((\n    (\"Guilherme Boulos\", {\"c\": color[7], \"offset\": (400, -500)}),\n    (\"Fernando Haddad 2t\", {\"c\": color[5], \"offset\": (400, -500)}),\n    (\"Geraldo Alckmin\", {\"c\": color[6], \"offset\": (400, -500)}),\n    (\"Ciro Gomes\", {\"c\": color[2], \"offset\": (400, -500)}),\n    (\"Jair Bolsonaro 2t\", {\"c\": color[10], \"offset\": (400, -500)}),\n    (\"Joo Amodo\", {\"c\": color[11], \"offset\": (400, -500)}),\n))\nfontsize = \"x-large\"\n\nfig, ax = plt.subplots(figsize=(12, 8))\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.set_xticks(np.arange(0, 40000+1, 10000))\nax.set_yticks(np.arange(0, 6000+1, 2000))\nax.set_xlabel(\"palavras\", fontsize=fontsize)\nax.set_ylabel(\"palavras sem repeti\u00e7\u00e3o\", fontsize=fontsize)\nwith pd.plotting.plot_params.use('x_compat', True):\n    for candidato, item in select_candidatos.items():\n        sr_tmp = df_palavras_unicas[candidato]\n        sr_tmp[:40000].plot(c=item[\"c\"], linewidth=20, alpha=0.7, ax=ax)\n        y = sr_tmp.max().astype(int)\n        x = sr_tmp[sr_tmp == sr_tmp.max()].index[-1]\n        candidato_offset = np.array((x, y))+item[\"offset\"]\n        if candidato != \"Guilherme Boulos\":\n            ax.annotate(candidato, candidato_offset, fontsize=\"large\")\n        else:\n            ax.annotate(\"\", (40000, 6100), (37500, 5800), arrowprops=dict(facecolor='black', shrink=0.05, width=2, headwidth=5))\n            ax.annotate(\"Guilherme Boulos\\nAd Infinitum\", (35000, 5400), fontsize=\"large\")\nax.set_title(\"Palavras \u00fanicas ao longo do texto\\nPlanos de governo - Elei\u00e7\u00f5es 2018\", fontsize=fontsize)","5b1cdef4":"df[\"quebras_de_linha\"] = df[\"texto\"].str.count(\"[\\n]\")\npontua\u00e7\u00e3o = {\n    \"ponto_final\": \"[.]\",\n    \"v\u00edrgula\": \"[,]\",\n    \"parenteses\": \"[()]\",\n    \"ponto_virgula\": \"[;]\",\n    \"dois_pontos\": \"[:]\",\n    \"asterisco\": \"[*]\",\n    \"hifen\/menos\": \"[-]\",\n    \"porcentagem\": \"[%]\",\n    \"barra\": \"[\\\\\/]\",\n    \"aspas\": \"[\\\"\\']\",\n    \"moeda\": \"[$]\",\n    \"mais\": \"[+]\",\n    \"exclama\u00e7\u00e3o\": \"[!]\",\n    \"interroga\u00e7\u00e3o\": \"[?]\",\n    \"subtra\u00e7o\": \"[_]\",\n    \"colchetes\": \"[\\[\\]]\",\n    \"chaves\": \"[\\{\\}]\",\n    \"e_comercial\": \"[&]\",\n    \"cerquilha\": \"#\"\n}\nfor key, value in pontua\u00e7\u00e3o.items():\n    df[key] = df[\"texto\"].str.count(value)","6f2803b8":"df[list(pontua\u00e7\u00e3o)]\\\n    .divide(df[list(pontua\u00e7\u00e3o)].sum(axis=1), axis=0)\\\n    [list(pontua\u00e7\u00e3o)[:len(pontua\u00e7\u00e3o)\/\/2]]\\\n    .transpose().plot.bar(figsize=(18, 10), title=\"Pontua\u00e7\u00f5es Normalizadas #1\", rot=20)","91c1af14":"df[list(pontua\u00e7\u00e3o)]\\\n    .divide(df[list(pontua\u00e7\u00e3o)].sum(axis=1), axis=0)\\\n    [list(pontua\u00e7\u00e3o)[len(pontua\u00e7\u00e3o)\/\/2:]]\\\n    .transpose().plot.bar(figsize=(18, 10), title=\"Pontua\u00e7\u00f5es Normalizadas #2\", rot=20)","a82f847c":"select_candidatos = OrderedDict((\n    (\"Cabo Daciolo\", {\"c\": color[1], \"offset\": (400, -500)}),\n    (\"Ciro Gomes\", {\"c\": color[2], \"offset\": (400, -500)}),\n    (\"Fernando Haddad 2t\", {\"c\": color[5], \"offset\": (400, -500)}),\n    (\"Geraldo Alckmin\", {\"c\": color[6], \"offset\": (400, -500)}),\n    (\"Jair Bolsonaro 2t\", {\"c\": color[10], \"offset\": (400, -500)}),\n    (\"Vera Lucia\", {\"c\": color[14], \"offset\": (400, -500)}),\n))\nfontsize = \"large\"\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.set_xticks(np.arange(0, len(select_candidatos)+1, 1))\nax.set_xticklabels(list(select_candidatos.keys()), rotation=20, fontsize=fontsize)\nax.set_yticks(np.arange(0, 0.061, 0.02))\nax.tick_params(axis='x', which='both',length=0)\n\n\nwith pd.plotting.plot_params.use('x_compat', True):\n    for idx, (candidato, item) in enumerate(select_candidatos.items()):\n        sr_tmp = df.loc[candidato]\n        ax.bar(idx, sr_tmp[\"exclama\u00e7\u00e3o\"]\/sr_tmp[list(pontua\u00e7\u00e3o)].sum(),color=item[\"c\"])\nax.set_title(\"Quantidade de exclama\u00e7\u00f5es relativas ao total de pontua\u00e7\u00f5es\\nPlanos de governo - Elei\u00e7\u00f5es 2018\", fontsize=fontsize)","2fdbeb91":"df[[\"quebras_de_linha\"]+list(pontua\u00e7\u00e3o.keys())]","4d5f59e3":"df[\"lista_de_palavras_stopwords\"] = df[\"lista_de_palavras\"].apply(lambda row: [palavra for palavra in row if palavra not in STOPWORDS])\ndf[\"palavras_frequentes\"] = df[\"lista_de_palavras_stopwords\"].apply(lambda row: [item for item in nltk.FreqDist(row).items() if item[1] >= 4])","265b3404":"df[\"wordcloud_palavras_frequentes\"] = \"\"\ndisplay(HTML(\"<h3>Palavras mais frequentes<\/h3>\"))\nfor key, value in df[\"palavras_frequentes\"].iteritems():\n    display(HTML(f\"<h4>{key}<\/h4>\"))\n    df_candidato = pd.DataFrame(value, columns=[\"Palavra\", \"Quantidade\"])\n    df_candidato[\"%\"] = df_candidato[\"Quantidade\"]\/df.loc[key, \"palavras\"]  # Normalizada pela quantidade de palavras do plano\n        \n    frequencias = df_candidato[[\"Palavra\", \"%\"]].set_index(\"Palavra\")[\"%\"].to_dict()\n    df.loc[key, \"wordcloud_palavras_frequentes\"] = wordcloud.WordCloud(\n        width=800, height=400, background_color='white', random_state=50)\\\n        .generate_from_frequencies(frequencias).to_image()\n    display(df_candidato.sort_values(\"Quantidade\", ascending=False).head(20))","e6d305cd":"display(HTML(\"<h3>Nuvens de palavras mais frequentes<\/h3>\"))\nfor key, value in df[\"palavras_frequentes\"].iteritems():\n    display(HTML(f\"<h4>{key}<\/h4>\"))\n    display(df.loc[key, \"wordcloud_palavras_frequentes\"])\n    display(HTML(\"<hr\/>\"))","cc1b389a":"def heatmap(df, title, xlabel, ylabel):\n    fig, ax1 = plt.subplots(figsize=(15, 15))\n    cax = ax1.imshow(df, interpolation=\"nearest\")\n    ax1.grid(True)\n    plt.title(title)\n    ax1.tick_params(\"x\", labelrotation=-40)\n    ax1.set_xticks(np.arange(len(df.columns)))\n    ax1.set_yticks(np.arange(len(df.index)))\n    ax1.set_xticklabels(df.columns, fontsize=10, horizontalalignment=\"left\")\n    ax1.set_yticklabels(df.index,fontsize=10)\n    ax1.set_xlabel(xlabel)\n    ax1.set_ylabel(ylabel)\n    plt.show()\n\n\ndef normalizar_score(row, limite=20):\n    sr = pd.Series(dict(row)).sort_values(ascending=False)[:limite]\n    return sr\/sr.sum()\n\n\nTOP_PALAVRAS = 25\ndf_palavras_frequentes = df[\"palavras_frequentes\"].apply(normalizar_score, args=(TOP_PALAVRAS,)).fillna(0)\ntop_palavras_frequentes = df_palavras_frequentes.sum().sort_values(ascending=False)[:TOP_PALAVRAS].index\ntop_palavras_frequentes_index = df_palavras_frequentes[top_palavras_frequentes].sum(axis=1).sort_values(ascending=False).index\nheatmap(\n    df_palavras_frequentes[top_palavras_frequentes].reindex(top_palavras_frequentes_index), \n    f\"{TOP_PALAVRAS} Palavras mais frequentes por candidato\",\n    \"Palavra\",\n    \"Candidato\")","a4e4ba6b":"df_palavras_frequentes","411e46ee":"df[\"bigramas_frequentes\"] = df[\"lista_de_palavras_stopwords\"]\\\n    .apply(lambda row: [item for item in nltk.FreqDist(nltk.bigrams(row)).items() if item[1] >= 4])\\\n    .apply(lambda row: list(map(lambda x: (\" \".join(x[0]), x[1]), row)))","306a5f16":"df[\"wordcloud_bigramas_frequentes\"] = \"\"\ndisplay(HTML(\"<h3>Bigramas mais frequentes<\/h3>\"))\nfor key, value in df[\"bigramas_frequentes\"].iteritems():\n    display(HTML(f\"<h4>{key}<\/h4>\"))\n    df_candidato = pd.DataFrame(value, columns=[\"Bigrama\", \"Quantidade\"]).sort_values(\"Quantidade\", ascending=False)\n    df_candidato[\"%\"] = 2*df_candidato[\"Quantidade\"]\/df.loc[key, \"palavras\"]  # Normalizada pela quantidade de palavras do plano\n        \n    frequencias = df_candidato[[\"Bigrama\", \"%\"]].set_index(\"Bigrama\")[\"%\"].to_dict()\n    df.loc[key, \"wordcloud_bigramas_frequentes\"] = wordcloud.WordCloud(\n        width=800, height=400, background_color='white', random_state=50)\\\n        .generate_from_frequencies(frequencias).to_image()\n    display(df_candidato.head(20))","a461c538":"display(HTML(\"<h3>Nuvens de bigramas mais frequentes<\/h3>\"))\nfor key, value in df[\"bigramas_frequentes\"].iteritems():\n    display(HTML(f\"<h4>{key}<\/h4>\"))\n    display(df.loc[key, \"wordcloud_bigramas_frequentes\"])\n    display(HTML(\"<hr\/>\"))","228015ea":"df_bigramas_frequentes = df[\"bigramas_frequentes\"].apply(normalizar_score, args=(TOP_PALAVRAS,)).fillna(0)\ntop_bigramas_frequentes = df_bigramas_frequentes.sum().sort_values(ascending=False)[:TOP_PALAVRAS].index\ntop_bigramas_frequentes_index = df_bigramas_frequentes[top_bigramas_frequentes].sum(axis=1).sort_values(ascending=False).index\nheatmap(\n    df_bigramas_frequentes[top_bigramas_frequentes].reindex(top_bigramas_frequentes_index), \n    f\"{TOP_PALAVRAS} Bigramas mais frequentes por candidato\",\n    \"Bigrama\",\n    \"Candidato\")","a4518c56":"def calcular_pmi(row):\n    bigram_measures = nltk.collocations.BigramAssocMeasures()\n    finder = nltk.collocations.BigramCollocationFinder.from_words(row)\n    finder.apply_freq_filter(4)\n    scored = finder.score_ngrams(bigram_measures.pmi)\n    return list(map(lambda x: (\" \".join(x[0]), x[1]), scored))\n\ndf[\"bigramas_pmi\"] = df[\"lista_de_palavras\"].apply(calcular_pmi)","2a4bb93f":"df[\"wordcloud_bigramas_pmi\"] = \"\"\ndisplay(HTML(\"<h3>Bigramas PMI<\/h3>\"))\nfor key, value in df[\"bigramas_pmi\"].iteritems():\n    display(HTML(f\"<h4>{key}<\/h4>\"))\n    df_candidato = pd.DataFrame(value, columns=[\"Bigrama\", \"Score\"])\n    \n    frequencias = df_candidato[[\"Bigrama\", \"Score\"]].set_index(\"Bigrama\")[\"Score\"].to_dict()\n    df.loc[key, \"wordcloud_bigramas_pmi\"] = wordcloud.WordCloud(\n        width=800, height=400, background_color='white', random_state=50)\\\n        .generate_from_frequencies(frequencias).to_image()\n    display(df_candidato.head(20))","1388d133":"display(HTML(\"<h3>Nuvens de bigramas PMI<\/h3>\"))\nfor key, value in df[\"bigramas_frequentes\"].iteritems():\n    display(HTML(f\"<h4>{key}<\/h4>\"))\n    display(df.loc[key, \"wordcloud_bigramas_pmi\"])\n    display(HTML(\"<hr\/>\"))","2d454bf1":"df_bigramas_pmi = df[\"bigramas_pmi\"].apply(normalizar_score, args=(TOP_PALAVRAS,)).fillna(0)\ntop_bigramas_pmi = df_bigramas_pmi.sum().sort_values(ascending=False)[:TOP_PALAVRAS].index\ntop_bigramas_pmi_index = df_bigramas_pmi[top_bigramas_pmi].sum(axis=1).sort_values(ascending=False).index\nheatmap(\n    df_bigramas_pmi[top_bigramas_pmi].reindex(top_bigramas_pmi_index), \n    f\"{TOP_PALAVRAS} Bigramas PMI por candidato\",\n    \"Bigrama\",\n    \"Candidato\")","c8448e4a":"vetorizador_tfidf = feature_extraction.text.TfidfVectorizer(\n    stop_words=STOPWORDS, ngram_range=(1, 3), max_features=df[\"palavras_unicas\"].sum()\/\/len(df))\nfeatures = vetorizador_tfidf.fit_transform(df[\"texto\"])\nfeature_names = vetorizador_tfidf.get_feature_names()\ncorpus_index = df.index\ndf_tfidf = pd.DataFrame(features.T.todense(), index=feature_names, columns=corpus_index)","6a7fa126":"df[\"termos_tfidf\"] = \"\"\nfor key in corpus_index:\n    df.loc[key, \"termos_tfidf\"] = df_tfidf[[key]].apply(lambda row: list(item for item in zip(row.index, row.values) if item[1] > 0)).values","912c0b74":"df[\"wordcloud_tfidf\"] = \"\"\ndisplay(HTML(\"<h3>Palavras relevantes TF-IDF<\/h3>\"))\nfor key, value in df[\"termos_tfidf\"].iteritems():\n    df_candidato = pd.DataFrame(value, columns=[\"Palavra\", \"Score\"])\n    frequencias = df_candidato[[\"Palavra\", \"Score\"]].set_index(\"Palavra\")[\"Score\"].to_dict()\n    df.loc[key, \"wordcloud_tfidf\"] = wordcloud.WordCloud(\n        width=800, height=400, background_color='white', random_state=50)\\\n        .generate_from_frequencies(frequencias).to_image()\n    display(HTML(f\"<h4>{key}<\/h4>\"))\n    display(df_candidato.sort_values(\"Score\", ascending=False).head(20).reset_index(drop=True))","f9299b81":"display(HTML(\"<h3>Nuvens de termos TF-IDF<\/h3>\"))\nfor key, value in df[\"termos_tfidf\"].iteritems():\n    display(HTML(f\"<h4>{key}<\/h4>\"))\n    display(df.loc[key, \"wordcloud_tfidf\"])\n    display(HTML(\"<hr\/>\"))","0d6b6259":"def heatmap(df, title, xlabel, ylabel):\n    fig, ax1 = plt.subplots(figsize=(15, 15))\n    cax = ax1.imshow(df, interpolation=\"nearest\")\n    ax1.grid(True)\n    ax1.tick_params(\"x\", labelrotation=-40)\n    ax1.set_xticks(np.arange(len(df.columns)))\n    ax1.set_yticks(np.arange(len(df.index)))\n    ax1.set_xticklabels(df.columns, fontsize=12, horizontalalignment=\"left\")\n    ax1.set_yticklabels(df.index,fontsize=12)\n    plt.show()\n    \n\ntop_termos_tfidf = df_tfidf.sum(axis=1).sort_values(ascending=False)[:TOP_PALAVRAS].index\ntop_termos_tfidf_index = df_tfidf.transpose()[top_palavras_frequentes].sum(axis=1).sort_values(ascending=False).index\n\nheatmap(\n    df_tfidf.transpose()[top_palavras_frequentes].reindex(top_palavras_frequentes_index).drop(index=[\"Fernando Haddad 1t\", \"Jair Bolsonaro 1t\"]), \n    f\"{TOP_PALAVRAS} Termos TF-IDF por candidato\",\n    \"Palavra\",\n    \"Candidato\")","3b6a86b9":"modelo_pca = decomposition.PCA(n_components=2)\nprojecao_pca = modelo_pca.fit_transform(df_tfidf.transpose())","e3c66ad2":"offset = {\n    \"Vera Lucia\": (-0.14, -0.07),\n    \"Alvaro Dias\": (-0.14, -0.15),\n    \"Jair Bolsonaro 1\u00bat\": (-0.4, 0),\n    \"Jair Bolsonaro 2\u00bat\": (-0.2, -0.06),\n    \"Henrique Meirelles\": (-0.2, -0.07),\n    \"Jo\u00e3o Amo\u00eado\": (-0.16, -0.037),\n    \"Cabo Daciolo\": (0.0, -0.11),\n    \"Jo\u00e3o Goulart\": (-0.22, -0.07),\n    \"Eymael\": (-0.01, -0.11),\n    \"Marina Silva\": (-0.01, -0.11),\n    \"Geraldo Alckmin\": (-0.16, -0.16),\n    \"Fernando Haddad 2\u00bat\": (-0.12, -0.06),\n    \"Guilherme Boulos\": (-0.14, -0.06),\n}\nfig, ax = plt.subplots(figsize=(10, 10))\nfor idx, (candidato, value) in enumerate(zip(corpus_index, projecao_pca)):\n    ax.scatter(value[0], value[1], label=candidato, s=400, linewidth=0.1)\n    offset_candidato = np.array((value[0]+0.05, value[1]+0.1))\n    offset_candidato += offset.get(candidato, (0, 0))\n    if candidato not in [\"Jair Bolsonaro 1\u00bat\", \"Fernando Haddad 1\u00bat\", \"Ciro Gomes\"]:\n        if candidato == \"Marina Silva\":\n            candidato = \"Ciro Gomes\/Marina Silva\"\n        ax.annotate(candidato, offset_candidato,fontsize=13)\nax.set_title(\"PCA das caracter\u00edsticas TF-IDF dos planos de governo\\nElei\u00e7\u00f5es 2018\")\nax.axis('off')\nplt.show()","c3618c08":"pd.DataFrame(projecao_pca, columns=(\"x\", \"y\"), index=corpus_index)","44b286a4":"def _substituir_bigramas(row, bigramas):\n    b_iter = iter(zip(row, row[1:]))\n    for p1, p2 in b_iter:\n        bigrama = f\"{p1} {p2}\"\n        if bigrama in bigramas:\n            yield bigrama\n            try:\n                next(b_iter)\n            except(StopIteration):\n                pass\n        else:\n            yield p1\n\n            \ndef substituir_bigramas(row, bigramas):\n    return list(_substituir_bigramas(row, bigramas))\n    \nbigramas = np.unique(np.concatenate(df[\"bigramas_frequentes\"].apply(lambda row: list(map(lambda x: x[0], row))).values))\ndf[\"tokens\"] = df[\"lista_de_palavras_stopwords\"].apply(substituir_bigramas, args=(bigramas,))","be181d71":"NUM_TOPICS = 4\ndictionary = gensim.corpora.Dictionary(df[\"tokens\"])\ncorpus = [dictionary.doc2bow(text) for text in df[\"tokens\"]]\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15, random_state=42)\nlda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\npyLDAvis.display(lda_display)","156e6ba1":"## 4. Estudo das palavras relevantes","5bf4d630":"## 2. Quantidade de p\u00e1ginas, palavras e per\u00edodos.","33548c65":"## 3. Estudo da pontua\u00e7\u00e3o","59cea51e":"# Estudo de Linguagem Natural nos Planos de Governo - Elei\u00e7\u00f5es 2018\n\nNeste trabalho, pretendo computar algumas estat\u00edsticas dos planos de governo dos candidatos a presid\u00eancia de 2018.\nPretendo apenas gerar gr\u00e1ficos e tamb\u00e9m disponibilizar as tabelas com os dados.\n\nUsando alguns resultados desta an\u00e1lise, publiquei um [artigo no Medium](https:\/\/medium.com\/@luizamaral306\/deus-ou-quinta-s%C3%A9rie-fa0e4e81694d).\n\nO dataset tamb\u00e9m est\u00e1 dispon\u00edvel no [Kaggle](https:\/\/www.kaggle.com\/luxedo\/planos-de-governo-eleies-presidenciais-2018).\n\nCome\u00e7amos importantdo as bibliotecas necess\u00e1rias e ajustando o ambiente:"}}