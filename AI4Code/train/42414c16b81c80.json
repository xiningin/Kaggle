{"cell_type":{"02ca885c":"code","b7b59f87":"code","d2313328":"code","302f6712":"code","719fa076":"code","0efd4c2c":"code","93b5a0ee":"code","101708e6":"code","726e3ca0":"code","4f0e62ca":"code","03161d0f":"code","c4355117":"code","4e5e459b":"code","130065f2":"code","a2dc1b6f":"code","1de7eedb":"code","4e806f49":"code","c8806cb6":"code","f48bbd4e":"code","2d22432f":"code","fd2eabd7":"code","23a9dd1c":"code","e322a9aa":"code","2a43ff69":"code","a4f0dd98":"code","09ff6d3e":"code","eb06b8b3":"code","9a3a4591":"code","e398cf03":"code","43c365b5":"code","e205ccdf":"code","6ce2a97a":"code","5f5882e5":"code","e26b0efa":"code","3353fc6b":"code","94d913e4":"code","aedc0e4f":"code","1bd31ae5":"code","e8e40f20":"code","5dc8a7fc":"code","c496438e":"code","62ac8138":"code","00b87268":"code","e3195b23":"code","5f2b1cde":"markdown","48102fd4":"markdown","547b2afd":"markdown","e2779efa":"markdown","519deaeb":"markdown","ce85e6a2":"markdown","b82bcba5":"markdown","f1972986":"markdown","101db96f":"markdown","0b175cad":"markdown","e462f937":"markdown","cdf7d347":"markdown","8c08c0e3":"markdown","3e4b651b":"markdown","ef25f321":"markdown"},"source":{"02ca885c":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nimport pickle ","b7b59f87":"data_processed = pd.read_csv('..\/input\/absenteeism-at-work-data-set\/df_preprocessed.csv')","d2313328":"data_processed.head()","302f6712":"#calculate the median\nmedian = data_processed['Absenteeism Time in Hours'].median()\nmedian","719fa076":"#create the targets\ntargets = np.where(data_processed['Absenteeism Time in Hours']>median, 1, 0)","0efd4c2c":"targets","93b5a0ee":"#add it to the dataframe\ndata_processed[\"absenteeism\"] = targets","101708e6":"#Check Class Balance\ndata_processed[\"absenteeism\"].value_counts(normalize = True) * 100","726e3ca0":"#drop Absenteeism Time in Hours\ndata_with_targets = data_processed.drop(['Absenteeism Time in Hours'], axis = 1)","4f0e62ca":"data_with_targets.head()","03161d0f":"unscaled_inputs = data_with_targets.iloc[:,:-1]\ntargets = data_with_targets['absenteeism']","c4355117":"#intialize the scaler \nscaler = StandardScaler()","4e5e459b":"#fit the scaler \nscaler.fit(unscaled_inputs)","130065f2":"#transform\nscaled_inputs = scaler.transform(unscaled_inputs)","a2dc1b6f":"X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, targets, test_size=0.2, random_state=42)","1de7eedb":"X_train.shape","4e806f49":"X_test.shape","c8806cb6":"y_train.shape","f48bbd4e":"#intialize the model\nlr = LogisticRegression()","2d22432f":"#fit the model\nlr.fit(X_train, y_train)","fd2eabd7":"#evaluate the mode on the training set\nlr.score(X_train, y_train)","23a9dd1c":"#model predictions\ny_hat = lr.predict(X_train) ","e322a9aa":"#compare y_hat to true y\n#number of correctly predicted outputs\ncorrect = (y_hat == y_train).sum()\ncorrect","2a43ff69":"#percent of correctly predicted of total\ncorrect \/ y_train.shape[0]","a4f0dd98":"#intercept \nlr.intercept_[0]","09ff6d3e":"#coeffs\nlr.coef_","eb06b8b3":"#column names \nunscaled_inputs.columns.values","9a3a4591":"feature_names = unscaled_inputs.columns.values","e398cf03":"#create a dataframe with feature names and corresponding coeff\nsummary_table = pd.DataFrame(columns = ['feature_names'], data = feature_names)\nsummary_table['coeff'] = np.transpose(lr.coef_)\nsummary_table","43c365b5":"#empty the zero index\nsummary_table.index = summary_table.index + 1\nsummary_table ","e205ccdf":"#now the index 0 is empty lets fill it\nsummary_table.loc[0] = ['intercept', lr.intercept_[0]]","6ce2a97a":"summary_table","5f5882e5":"#sort by index \nsummary_table = summary_table.sort_index()\nsummary_table","e26b0efa":"summary_table['odds_ratio'] = np.exp(summary_table.coeff)\nsummary_table","3353fc6b":"#Sort by odds ratio \nsummary_table.sort_values('odds_ratio', ascending = False)","94d913e4":"#Test accuracy \nlr.score(X_test, y_test)","aedc0e4f":"predicted_proba = lr.predict_proba(X_test)\npredicted_proba","1bd31ae5":"#the probability of absenteeism\npredicted_proba[:,1]","e8e40f20":"#save the model\nwith open('model', 'wb') as file:\n    pickle.dump(lr, file)","5dc8a7fc":"#save the scaler\nwith open('scaler', 'wb') as file:\n    pickle.dump(scaler, file)","c496438e":"# import all libraries needed\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# the custom scaler class \nclass CustomScaler(BaseEstimator,TransformerMixin): \n    \n    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n        self.scaler = StandardScaler(copy,with_mean,with_std)\n        self.columns = columns\n        self.mean_ = None\n        self.var_ = None\n\n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.array(np.mean(X[self.columns]))\n        self.var_ = np.array(np.var(X[self.columns]))\n        return self\n\n    def transform(self, X, y=None, copy=None):\n        init_col_order = X.columns\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]\n\n\n# create the special class that we are going to use from here on to predict new data\nclass absenteeism_model():\n      \n        def __init__(self, model_file, scaler_file):\n            # read the 'model' and 'scaler' files which were saved\n            with open('model','rb') as model_file, open('scaler', 'rb') as scaler_file:\n                self.reg = pickle.load(model_file)\n                self.scaler = pickle.load(scaler_file)\n                self.data = None\n        \n        # take a data file (*.csv) and preprocess it in the same way as in the lectures\n        def load_and_clean_data(self, data_file):\n            \n            # import the data\n            df = pd.read_csv(data_file,delimiter=',')\n            # store the data in a new variable for later use\n            self.df_with_predictions = df.copy()\n            # drop the 'ID' column\n            df = df.drop(['ID'], axis = 1)\n            # to preserve the code we've created in the previous section, we will add a column with 'NaN' strings\n            df['Absenteeism Time in Hours'] = 'NaN'\n\n            # create a separate dataframe, containing dummy values for ALL avaiable reasons\n            reason_columns = pd.get_dummies(df['Reason for Absence'], drop_first = True)\n            \n            # split reason_columns into 4 types\n            reason_type_1 = reason_columns.loc[:,1:14].max(axis=1)\n            reason_type_2 = reason_columns.loc[:,15:17].max(axis=1)\n            reason_type_3 = reason_columns.loc[:,18:21].max(axis=1)\n            reason_type_4 = reason_columns.loc[:,22:].max(axis=1)\n            \n            # to avoid multicollinearity, drop the 'Reason for Absence' column from df\n            df = df.drop(['Reason for Absence'], axis = 1)\n            \n            # concatenate df and the 4 types of reason for absence\n            df = pd.concat([df, reason_type_1, reason_type_2, reason_type_3, reason_type_4], axis = 1)\n            \n            # assign names to the 4 reason type columns\n            # note: there is a more universal version of this code, however the following will best suit our current purposes             \n            column_names = ['Date', 'Transportation Expense', 'Distance to Work', 'Age',\n                           'Daily Work Load Average', 'Body Mass Index', 'Education', 'Children',\n                           'Pet', 'Absenteeism Time in Hours', 'Reason_1', 'Reason_2', 'Reason_3', 'Reason_4']\n            df.columns = column_names\n\n            # re-order the columns in df\n            column_names_reordered = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4', 'Date', 'Transportation Expense', \n                                      'Distance to Work', 'Age', 'Daily Work Load Average', 'Body Mass Index', 'Education', \n                                      'Children', 'Pet', 'Absenteeism Time in Hours']\n            df = df[column_names_reordered]\n      \n            # convert the 'Date' column into datetime\n            df['Date'] = pd.to_datetime(df['Date'], format='%d\/%m\/%Y')\n\n            # create a list with month values retrieved from the 'Date' column\n            list_months = []\n            for i in range(df.shape[0]):\n                list_months.append(df['Date'][i].month)\n\n            # insert the values in a new column in df, called 'Month Value'\n            df['Month Value'] = list_months\n\n            # create a new feature called 'Day of the Week'\n            df['Day of the Week'] = df['Date'].apply(lambda x: x.weekday())\n\n\n            # drop the 'Date' column from df\n            df = df.drop(['Date'], axis = 1)\n\n            # re-order the columns in df\n            column_names_upd = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4', 'Month Value', 'Day of the Week',\n                                'Transportation Expense', 'Distance to Work', 'Age',\n                               'Daily Work Load Average', 'Body Mass Index', 'Education', 'Children',\n                              'Pet', 'Absenteeism Time in Hours']\n            df = df[column_names_upd]\n\n\n            # map 'Education' variables; the result is a dummy\n            df['Education'] = df['Education'].map({1:0, 2:1, 3:1, 4:1})\n\n            # replace the NaN values\n            df = df.fillna(value=0)\n\n            # drop the original absenteeism time\n            df = df.drop(['Absenteeism Time in Hours'],axis=1)\n            \n            # drop the variables we decide we don't need\n            #df = df.drop(['Day of the Week','Daily Work Load Average','Distance to Work'],axis=1)\n            \n            # we have included this line of code if you want to call the 'preprocessed data'\n            self.preprocessed_data = df.copy()\n            \n            # we need this line so we can use it in the next functions\n            self.data = self.scaler.transform(df)\n    \n        # a function which outputs the probability of a data point to be 1\n        def predicted_probability(self):\n            if (self.data is not None):  \n                pred = self.reg.predict_proba(self.data)[:,1]\n                return pred\n        \n        # a function which outputs 0 or 1 based on our model\n        def predicted_output_category(self):\n            if (self.data is not None):\n                pred_outputs = self.reg.predict(self.data)\n                return pred_outputs\n        \n        # predict the outputs and the probabilities and \n        # add columns with these values at the end of the new data\n        def predicted_outputs(self):\n            if (self.data is not None):\n                self.preprocessed_data['Probability'] = self.reg.predict_proba(self.data)[:,1]\n                self.preprocessed_data ['Prediction'] = self.reg.predict(self.data)\n                return self.preprocessed_data","62ac8138":"model = absenteeism_model('model', 'scaler')","00b87268":"model.load_and_clean_data('..\/input\/absenteeism-at-work-data-set\/Absenteeism_new_data.csv')","e3195b23":"model.predicted_outputs()","5f2b1cde":"## Necessary Packages","48102fd4":"## Create the Target\nWe will create a new variable based on \"Absenteeism Time in Hours\". I the value less than the median then it is a moderate absenteeism rate, if it is above the median then it is high absenteeism rate. Why the median? in order to balance the data: nearly 50% will be 1 and nearly 50% will be 0. This will help the machine learning model make accurate predictions.","547b2afd":"## Standerdize the data ","e2779efa":"## Create A Scalable Module \nIn this part of the notebook we will focus on building a module that can be easily applied to any new data. The module is simply a more organized version of the code used in [part 1](https:\/\/www.kaggle.com\/ahmedmohameddawoud\/absenteeism-analysis-part-1-data-cleaning) and [part 2](https:\/\/www.kaggle.com\/ahmedmohameddawoud\/absenteeism-analysis-part-2-machine-learning) of this project.","519deaeb":"## Logestic Regression ","ce85e6a2":"This notebook is part of a bigger project. The project is consisted of 3 stages:\n    \n1. Data cleaning in python\n2. Model building, evaluation and prediction\n3. Storing the cleaned data in SQL database\n4. Connceting the database to tableau and creating a detailed visual analysis\n\nIn a previous [notebook](https:\/\/www.kaggle.com\/ahmedmohameddawoud\/absenteeism-analysis-part-1-data-cleaning#Education) we processed the data to make:\n\n- Data types consistent\n- Feature engineering (creating new feature from existing ones)\n\nIn this notebook we will focus on the machine learning part of the project, we will start by a logestic regression model and take it from there.\nWe will alos build a module that can be easily applied to any new data. The module is simply a more organized version of the code used in [part 1](https:\/\/www.kaggle.com\/ahmedmohameddawoud\/absenteeism-analysis-part-1-data-cleaning) and [part 2](https:\/\/www.kaggle.com\/ahmedmohameddawoud\/absenteeism-analysis-part-2-machine-learning) of this project. I is supposed to enable us to generalize every thing we did so far to any new data with just 3 lines of code.","b82bcba5":"## Interpreting the Coeff","f1972986":"## Read the Data","101db96f":"## Manually check the accuracy ","0b175cad":"## Save the model","e462f937":"## Testing the model","cdf7d347":"## Finding the intercept and coefficients ","8c08c0e3":"## Check Class Balance ","3e4b651b":"## Split the data into X and Y","ef25f321":"## Train Test Split"}}