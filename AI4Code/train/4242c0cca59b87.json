{"cell_type":{"adc15d6c":"code","23a26666":"code","fb27524e":"code","878a1a67":"code","481fe42e":"code","438624bc":"code","41d9321d":"code","95ba3b1f":"code","35e0ce97":"code","f344e7c5":"code","5085810a":"code","8ea71875":"code","ab4863f9":"code","e370b276":"code","8d3df2e3":"code","a895dffe":"code","5288cab3":"code","945fbdc3":"markdown","7d1f01cd":"markdown","0c27b9aa":"markdown","859db396":"markdown","95e66b66":"markdown","a6c4dd51":"markdown","2df9dee1":"markdown","6588e9cc":"markdown","8715b2b4":"markdown","19b71345":"markdown","6a8520d5":"markdown","c493a950":"markdown","93e84efd":"markdown","76e646a0":"markdown","bdb5ebd6":"markdown","ff0be2a0":"markdown","9e78ebf4":"markdown"},"source":{"adc15d6c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","23a26666":"df_train = pd.read_csv(\"..\/input\/train.csv\",index_col='qid')\ndf_test = pd.read_csv(\"..\/input\/test.csv\",index_col='qid')\n#df = pd.concat([df_train ,df_test],sort=True)","fb27524e":"df_train.info()","878a1a67":"df_train.head()","481fe42e":"df_train.target.value_counts()","438624bc":"p = sns.countplot(x=None,y= 'target',data=df_train)","41d9321d":"df_train.dropna(inplace=True)","95ba3b1f":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nvect = CountVectorizer()\nvect","35e0ce97":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport lightgbm as lgbm\n\n\ntext_clf_lgbm = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', lgbm.LGBMClassifier()),\n])\n","f344e7c5":"from sklearn.model_selection import train_test_split\n\nX = df_train['question_text']\ny = df_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,stratify=y)","5085810a":"text_clf_lgbm.fit(X_train, y_train)\n# Form a prediction set\npredictions = text_clf_lgbm.predict(X_test)","8ea71875":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, predictions)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","ab4863f9":"# Print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","e370b276":"print(metrics.classification_report(y_test,predictions))","8d3df2e3":"df_test['prediction'] = text_clf_lgbm.predict(df_test['question_text'])\ndf_test.head()","a895dffe":"df_test.drop(['question_text'],axis=1,inplace=True)\ndf_test.to_csv('submission.csv',index=True)","5288cab3":"df_test.head()","945fbdc3":"## Taking a deep dive in NLP techniques with the famous Kaggle competition Quora Insincere Questions Classification problem. ","7d1f01cd":"A Big Thanks to:\n\n* https:\/\/medium.com\/greyatom\/an-introduction-to-bag-of-words-in-nlp-ac967d43b428\n* https:\/\/medium.com\/@paritosh_30025\/natural-language-processing-text-data-vectorization-af2520529cf7\n\n\n\n![](https:\/\/www.humorside.com\/wp-content\/uploads\/2017\/12\/thank-you-meme-01.jpg)\n","0c27b9aa":"## Business problem asks us to classify question texts given to us with the help of the target variable in the training dataset. Given the involvement of text data, it requires to be treated with NLP techniques to be able to extract useful information from text data.","859db396":"![](http:\/\/www.quickmeme.com\/img\/8d\/8da6189f6c24b2f66498daccefdba903f70e45b1fc4e305d52848c5942e1c066.jpg)","95e66b66":"#### Below is a pipeline based LGBM Classifier implementation of the above NLP concept Bag of Words on the Quora business problem.","a6c4dd51":"# Business Problem","2df9dee1":"![](https:\/\/cdn-images-1.medium.com\/max\/1200\/0*dIUk8mZ4ADIqBjUj.jpg)","6588e9cc":"# Bag of Words\n The bag-of-words model is simple to understand and implement. It is a way of extracting features from the text for use in machine learning algorithms.In this approach, we use the tokenized words for each observation and find out the frequency of each token.\nLet\u2019s take an example to understand this concept in depth.\n\n* \u201cIt was the best of times\u201d\n* \u201cIt was the worst of times\u201d\n* \u201cIt was the age of wisdom\u201d\n* \u201cIt was the age of foolishness\u201d\n\nWe treat each sentence as a separate document and we make a list of all words from all the four documents excluding the punctuation. We get,\n\n\u2018It\u2019,  \u2018was\u2019,  \u2018the\u2019,  \u2018best\u2019,  \u2018of\u2019,  \u2018times\u2019,  \u2018worst\u2019,  \u2018age\u2019,  \u2018wisdom\u2019,  \u2018foolishness\u2019\n\nThe next step is the create vectors. Vectors convert text that can be used by the machine learning algorithm.\n\nWe take the first document\u200a\u2014\u200a\u201cIt was the best of times\u201d and we check the frequency of words from the 10 unique words.\n* \u201cit\u201d = 1\n* \u201cwas\u201d = 1\n* \u201cthe\u201d = 1\n* \u201cbest\u201d = 1\n* \u201cof\u201d = 1\n* \u201ctimes\u201d = 1\n* \u201cworst\u201d = 0\n* \u201cage\u201d = 0\n* \u201cwisdom\u201d = 0\n* \u201cfoolishness\u201d = 0","8715b2b4":"## 2. TF-IDF Vectorizer\nTF-IDF stands for term frequency-inverse document frequency. TF-IDF weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n\n**Term Frequency (TF)**: is a scoring of the frequency of the word in the current document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. The term frequency is often divided by the document length to normalize.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*SUAeubfQGK_w0XZWQW6V1Q.png)\n\n**Inverse Document Frequency** (IDF): is a scoring of how rare the word is across documents. IDF is a measure of how rare a term is. Rarer the term, more is the IDF score.\n\n\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*T57j-UDzXizqG40FUfmkLw.png)\nThus,\n\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*YrgmAeG7KNRB4dQcGcsdyg.png)","19b71345":"# Evaluation\n\n\n![](http:\/\/www.quickmeme.com\/img\/30\/300ace809c3c2dca48f2f55ca39cbab24693a9bd470867d2eb4e869c645acd42.jpg)","6a8520d5":"#### In this approach, each word or token is called a \u201cgram\u201d. Creating a vocabulary of two-word pairs is called a bigram model.\n\nFor example, the bigrams in the first document : \u201cIt was the best of times\u201d are as follows:\n* \u201cit was\u201d\n* \u201cwas the\u201d\n* \u201cthe best\u201d\n* \u201cbest of\u201d\n* \u201cof times\u201d","c493a950":"# Submission","93e84efd":"#### Rest of the documents will be:\n* \u201cIt was the best of times\u201d = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n* \u201cIt was the worst of times\u201d = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n* \u201cIt was the age of wisdom\u201d = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n* \u201cIt was the age of foolishness\u201d = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]","76e646a0":"# Time for some NLP","bdb5ebd6":"# Vectorization\n\n#### The process of converting NLP text into numbers is called vectorization in ML. Different ways to convert text into vectors are:\n\n* Counting the number of times each word appears in a document.\n* Calculating the frequency that each word appears in a document out of all the words in the document.","ff0be2a0":"# Implementation","9e78ebf4":"## 1. CountVectorizer\nCountVectorizer works on Terms Frequency, i.e. counting the occurrences of tokens and building a sparse matrix of documents x tokens."}}