{"cell_type":{"6d34fc41":"code","e09c417f":"code","1a67c9b2":"code","bfa15dee":"code","270ced7c":"code","2f597eb8":"code","bf552cd6":"code","4f9c4291":"code","9ad9dd67":"code","39b24eda":"code","e472b741":"code","9c39ce50":"code","46f422e1":"code","563cdbfe":"code","ca0ef539":"code","c4011630":"code","decb369b":"code","142df14a":"code","7f7090b0":"code","1e436276":"code","14664175":"code","ff46fe8e":"code","5619894d":"code","bd82cf71":"code","3638304f":"code","30fc28f9":"code","6e362a23":"code","a2fe106f":"code","ebc70f12":"code","47450fea":"code","a61d64f1":"code","ffecee06":"code","4c2db7bc":"code","40647982":"code","526b6976":"code","dc00eaa1":"code","5a42742f":"code","39f03d23":"code","10449f42":"code","3e7c4cbf":"code","f6a3f10f":"code","7eb3648d":"code","40102925":"code","9206a133":"code","7f3ae8fb":"code","e92ccc86":"code","368112f3":"code","29221cbf":"code","796ab7e8":"code","7a19ab52":"code","c79aa65f":"code","63c30ba8":"code","d6e6bcab":"code","48004907":"code","d26ecbe1":"code","1d0db670":"code","0778ca22":"code","74e23951":"code","651eee5a":"code","d150e699":"code","2ab3faaf":"code","59197dbc":"code","0b2bf597":"code","8fd101f5":"code","16f54fd9":"code","d8133998":"code","0cc17573":"code","8a40d2df":"code","90ab5925":"code","d8a5a384":"code","3a16a0fa":"code","f0a3b891":"code","eb17cb05":"code","4293f5c4":"code","ddafd4ed":"code","f36ff5dc":"code","8c955a1c":"code","882492eb":"code","dd2bb52b":"code","e9e5b5bc":"code","19c0cdcd":"code","6f73c824":"code","e4e288e0":"code","3b34a58c":"code","33ee5ee1":"code","cca01b67":"code","bfab044c":"code","da23a6de":"code","cf51761c":"code","c86fdc3a":"code","5b47b114":"code","b56436d7":"code","8709b902":"code","ea8f4d65":"code","ddb6993d":"code","b5fc7bc3":"code","d23ea836":"code","c9536526":"code","8c5e109e":"code","9f9c14de":"code","219e6f6e":"code","f23f1185":"code","64a66e63":"code","b0167dfd":"code","e77cc0da":"code","6d4ced7f":"code","0aa90ee7":"code","35b3778a":"code","084602a1":"code","f60af750":"code","f07d52df":"code","f98b1653":"code","7540aba7":"code","31daf26b":"code","7d5025ae":"code","b19d672a":"code","73019580":"code","9ac3e14a":"code","8d6273d2":"code","ddbae99d":"code","2793ff51":"code","dd1e9d78":"code","35ba1549":"code","d49b1d40":"code","e70b1363":"code","6a339eb8":"markdown","27ac5642":"markdown","895feea8":"markdown","f90e11d0":"markdown","94801ad2":"markdown","678962b4":"markdown","1900dcdb":"markdown","915f62c4":"markdown","dcebbc24":"markdown","5c0e8656":"markdown","92d096db":"markdown","00d5bbd2":"markdown","ead34145":"markdown","4d28c0fb":"markdown","74a42833":"markdown","c5ceb488":"markdown","16f47580":"markdown","4274d97d":"markdown","9beb2bf0":"markdown","7e897798":"markdown","7986dc31":"markdown","ab3e4373":"markdown","1bdaa035":"markdown","a6080fbf":"markdown","8eebe896":"markdown","b22c7fa3":"markdown","c18bbdd3":"markdown","3a5e6c8d":"markdown","7ac45431":"markdown","27bd90d0":"markdown","9668417e":"markdown","fe2d785b":"markdown","b58072f8":"markdown","9970cabe":"markdown","17ed2213":"markdown","c437cea2":"markdown","e1476afa":"markdown","2ce4c845":"markdown","4fc0a24f":"markdown","74ae6f61":"markdown","48ca7245":"markdown","cf0fc060":"markdown","3fe8652b":"markdown","64f821c5":"markdown","e1d96452":"markdown","4a237aa8":"markdown","f322ceec":"markdown","bbae700a":"markdown","d75ed4ee":"markdown","2a371057":"markdown","5b4f63ea":"markdown","8d1f63d2":"markdown","9cbee456":"markdown","ccf0be17":"markdown","67cf74e9":"markdown","8e6667ae":"markdown","db776108":"markdown","a316e802":"markdown","73b9a2ab":"markdown","95bfaf12":"markdown","d7284982":"markdown","b6f2330a":"markdown","e0ee7d11":"markdown","a83e5e85":"markdown","14992e06":"markdown"},"source":{"6d34fc41":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\nimport re\nfrom fuzzywuzzy import fuzz\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy import sparse\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, log_loss\nfrom sklearn.calibration import CalibratedClassifierCV\nimport xgboost as xgb\nimport nltk\nimport time\nfrom matplotlib.pyplot import figure\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom joblib import dump, load\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\n# import optuna\nimport hyperopt\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nimport warnings\nimport gc\nfrom sklearn.model_selection import cross_val_score\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')","e09c417f":"data = pd.read_csv('train.csv.zip')","1a67c9b2":"print(data.columns)\nprint(data.is_duplicate.unique())\nprint(data.is_duplicate.value_counts())\nprint(data.shape)","bfa15dee":"testdata = pd.read_csv('test.csv')\nprint(testdata.shape)","270ced7c":"data.head(5)","2f597eb8":"testdata.head(5)","bf552cd6":"data.info()","4f9c4291":"data = data.dropna()\nprint(data.shape)","9ad9dd67":"print(data.duplicated(('question1', 'question2')).sum())","39b24eda":"duplicate_value_counts = data.is_duplicate.value_counts()\nprint(duplicate_value_counts\/duplicate_value_counts.sum())\nplt.title('Distribution of classes')\nduplicate_value_counts.plot.bar()","e472b741":"qids = np.append(data.qid1.values,data.qid2.values)\nprint(len(set(qids)))\nprint(len(qids))","9c39ce50":"occurences = np.bincount(qids)\nplt.figure(figsize=(10,5)) \nplt.hist(occurences, bins=range(0,np.max(occurences)))\nplt.yscale('log')\nplt.xlabel('Number of times question repeated')\nplt.ylabel('Number of questions')\nplt.title('Question vs Repeatition')\nplt.show()\nprint(np.min(occurences), np.max(occurences))","46f422e1":"print(data.question1.apply(len).min())\nprint(data.loc[data.question1.apply(len).argmin()])\nprint(data.question2.apply(len).min())\nprint(data.loc[data.question2.apply(len).argmin()])","563cdbfe":"def preprocess_text(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"\u2032\", \"'\").replace(\"\u2019\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"\u20b9\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"\u20ac\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    x = re.sub(r\"http\\S+\", \"\", x)\n    x = re.sub('\\W', ' ', x)\n    \n    lemmatizer = WordNetLemmatizer()\n    x = lemmatizer.lemmatize(x)\n    bfs = BeautifulSoup(x)\n    x = bfs.get_text()\n    x = x.strip()\n    return x","ca0ef539":"def data_cleaning(data):\n    newdata = pd.DataFrame()\n    newdata['question1_final'] = data.question1.apply(preprocess_text)\n    newdata['question2_final'] = data.question2.apply(preprocess_text)\n    return newdata","c4011630":"traindata = data_cleaning(data)","decb369b":"testdata = data_cleaning(testdata)","142df14a":"print(data.head())","7f7090b0":"print(traindata.head())","1e436276":"def doesMatch (q, match):\n    q1, q2 = q['question1_final'], q['question2_final']\n    q1 = q1.split()\n    q2 = q2.split()\n    if len(q1)>0 and len(q2)>0 and q1[match]==q2[match]:\n        return 1\n    else:\n        return 0","14664175":"def feature_extract(data):\n    data['q1_char_num'] = data.question1_final.apply(len)\n    data['q2_char_num'] = data.question2_final.apply(len)\n    data['q1_word_num'] = data.question1_final.apply(lambda x: len(x.split()))\n    data['q2_word_num'] = data.question2_final.apply(lambda x: len(x.split()))\n    \n    data['total_word_num'] = data['q1_word_num'] + data['q2_word_num']\n    data['differ_word_num'] = abs(data['q1_word_num'] - data['q2_word_num'])\n    data['same_first_word'] = data.apply(lambda x: doesMatch(x, 0) ,axis=1)\n    data['same_last_word'] = data.apply(lambda x: doesMatch(x, -1) ,axis=1)\n    data['total_unique_word_num'] = data.apply(lambda x: len(set(x.question1_final.split()).union(set(x.question2_final.split()))) ,axis=1)\n    data['total_unique_word_withoutstopword_num'] = data.apply(lambda x: len(set(x.question1_final.split()).union(set(x.question2_final.split())) - set(stopwords.words('english'))) ,axis=1)\n    data['total_unique_word_num_ratio'] = data['total_unique_word_num'] \/ data['total_word_num']\n    \n    data['common_word_num'] = data.apply(lambda x: len(set(x.question1_final.split()).intersection(set(x.question2_final.split()))) ,axis=1)\n    data['common_word_ratio'] = data['common_word_num'] \/ data['total_unique_word_num']\n    data['common_word_ratio_min'] = data['common_word_num'] \/ data.apply(lambda x: min(len(set(x.question1_final.split())), len(set(x.question2_final.split()))) ,axis=1) \n    data['common_word_ratio_max'] = data['common_word_num'] \/ data.apply(lambda x: max(len(set(x.question1_final.split())), len(set(x.question2_final.split()))) ,axis=1) \n    \n    data['common_word_withoutstopword_num'] = data.apply(lambda x: len(set(x.question1_final.split()).intersection(set(x.question2_final.split())) - set(stopwords.words('english'))) ,axis=1)\n    data['common_word_withoutstopword_ratio'] = data['common_word_withoutstopword_num'] \/ data['total_unique_word_withoutstopword_num']\n    data['common_word_withoutstopword_ratio_min'] = data['common_word_withoutstopword_num'] \/ data.apply(lambda x: min(len(set(x.question1_final.split()) - set(stopwords.words('english'))), len(set(x.question2_final.split()) - set(stopwords.words('english')))) ,axis=1) \n    data['common_word_withoutstopword_ratio_max'] = data['common_word_withoutstopword_num'] \/ data.apply(lambda x: max(len(set(x.question1_final.split()) - set(stopwords.words('english'))), len(set(x.question2_final.split()) - set(stopwords.words('english')))) ,axis=1) \n    \n    data[\"fuzz_ratio\"] = data.apply(lambda x: fuzz.ratio(x.question1_final, x.question2_final), axis=1)\n    data[\"fuzz_partial_ratio\"] = data.apply(lambda x: fuzz.partial_ratio(x.question1_final, x.question2_final), axis=1)\n    data[\"fuzz_token_set_ratio\"] = data.apply(lambda x: fuzz.token_set_ratio(x.question1_final, x.question2_final), axis=1)\n    data[\"fuzz_token_sort_ratio\"] = data.apply(lambda x: fuzz.token_sort_ratio(x.question1_final, x.question2_final), axis=1)\n    data.fillna(0, inplace=True)\n    return data","ff46fe8e":"traindata = feature_extract(traindata)","5619894d":"testdata = feature_extract(testdata)","bd82cf71":"traindata.head()","3638304f":"traindata.shape","30fc28f9":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Total Number of Words')\nsns.kdeplot(traindata['total_word_num'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Total Number of Words')\nsns.boxplot(x=data.is_duplicate, y=traindata['total_word_num'], palette=\"Dark2\", ax=ax[1])\nplt.show()","6e362a23":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Difference in Number of Words')\nsns.kdeplot(traindata['differ_word_num'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Difference in Number of Words')\nsns.boxplot(x=data.is_duplicate, y=traindata['differ_word_num'], palette=\"Dark2\", ax=ax[1])\nplt.show()","a2fe106f":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('Have same First word?')\nsns.kdeplot(traindata['same_first_word'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Have same First word?')\nsns.countplot(x=traindata['same_first_word'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[1])\nplt.show()","ebc70f12":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('Have same Last word?')\nsns.kdeplot(traindata['same_last_word'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Have same Last word?')\nsns.countplot(x=traindata['same_last_word'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[1])\nplt.show()","47450fea":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Total Number of Unique Words')\nsns.kdeplot(traindata['total_unique_word_num'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Total Number of Unique Words')\nsns.boxplot(x=data.is_duplicate, y=traindata['total_unique_word_num'], palette=\"Dark2\", ax=ax[1])\nplt.show()","a61d64f1":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Total Number of Unique Words without Stop words')\nsns.kdeplot(traindata['total_unique_word_withoutstopword_num'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Total Number of Unique Words without Stop words')\nsns.boxplot(x=data.is_duplicate, y=traindata['total_unique_word_withoutstopword_num'], palette=\"Dark2\", ax=ax[1])\nplt.show()","ffecee06":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Total Unique words to Total words Ratio')\nsns.kdeplot(traindata['total_unique_word_num_ratio'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Total Unique words to Total words Ratio')\nsns.boxplot(x=data.is_duplicate, y=traindata['total_unique_word_num_ratio'], palette=\"Dark2\", ax=ax[1])\nplt.show()","4c2db7bc":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Number of Common words')\nsns.kdeplot(traindata['common_word_num'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Number of Common words')\nsns.boxplot(x=data.is_duplicate, y=traindata['common_word_num'], palette=\"Dark2\", ax=ax[1])\nplt.show()","40647982":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Ratio of number of Common words to total Unique words')\nsns.kdeplot(traindata['common_word_ratio'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Ratio of number of Common words to total Unique words')\nsns.boxplot(x=data.is_duplicate, y=traindata['common_word_ratio'], palette=\"Dark2\", ax=ax[1])\nplt.show()","526b6976":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Ratio of number of Common words to Minimum of Unique words')\nsns.kdeplot(traindata['common_word_ratio_min'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Ratio of number of Common words to Minimum of Unique words')\nsns.boxplot(x=data.is_duplicate, y=traindata['common_word_ratio_min'], palette=\"Dark2\", ax=ax[1])\nplt.show()","dc00eaa1":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Ratio of number of Common words to Maximum of Unique words')\nsns.kdeplot(traindata['common_word_ratio_max'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Ratio of number of Common words to Maximum of Unique words')\nsns.boxplot(x=data.is_duplicate, y=traindata['common_word_ratio_max'], palette=\"Dark2\", ax=ax[1])\nplt.show()","5a42742f":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Number of Common words Without Stop words')\nsns.kdeplot(traindata['common_word_withoutstopword_num'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Number of Common words Without Stop words')\nsns.boxplot(x=data.is_duplicate, y=traindata['common_word_withoutstopword_num'], palette=\"Dark2\", ax=ax[1])\nplt.show()","39f03d23":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('Ratio of no. of Common words to total Unique words Without Stop words')\nsns.kdeplot(traindata['common_word_withoutstopword_ratio'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Ratio of no. of Common words to total Unique words Without Stop words')\nsns.boxplot(x=data.is_duplicate, y=traindata['common_word_withoutstopword_ratio'], palette=\"Dark2\", ax=ax[1])\nplt.show()","10449f42":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('Ratio of no. of Common words to Minimum Unique words w\/o Stop words')\nsns.kdeplot(traindata['common_word_withoutstopword_ratio_min'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Ratio of no. of Common words to Minimum Unique words w\/o Stop words')\nsns.boxplot(x=data.is_duplicate, y=traindata['common_word_withoutstopword_ratio_min'], palette=\"Dark2\", ax=ax[1])\nplt.show()","3e7c4cbf":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('Ratio of no. of Common words to Maximum Unique words w\/o Stop words')\nsns.kdeplot(traindata['common_word_withoutstopword_ratio_max'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Ratio of no. of Common words to Maximum Unique words w\/o Stop words')\nsns.boxplot(x=data.is_duplicate, y=traindata['common_word_withoutstopword_ratio_max'], palette=\"Dark2\", ax=ax[1])\nplt.show()","f6a3f10f":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Fuzz Ratio')\nsns.kdeplot(traindata['fuzz_ratio'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Fuzz Ratio')\nsns.boxplot(x=data.is_duplicate, y=traindata['fuzz_ratio'], palette=\"Dark2\", ax=ax[1])\nplt.show()","7eb3648d":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Fuzz Partial Ratio')\nsns.kdeplot(traindata['fuzz_partial_ratio'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Fuzz Partial Ratio')\nsns.boxplot(x=data.is_duplicate, y=traindata['fuzz_partial_ratio'], palette=\"Dark2\", ax=ax[1])\nplt.show()","40102925":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Fuzz Token Set Ratio')\nsns.kdeplot(traindata['fuzz_token_set_ratio'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Fuzz Token Set Ratio')\nsns.boxplot(x=data.is_duplicate, y=traindata['fuzz_token_set_ratio'], palette=\"Dark2\", ax=ax[1])\nplt.show()","9206a133":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('PDF of Fuzz Token Sort Ratio')\nsns.kdeplot(traindata['fuzz_token_sort_ratio'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Boxplot of Fuzz Token Sort Ratio')\nsns.boxplot(x=data.is_duplicate, y=traindata['fuzz_token_sort_ratio'], palette=\"Dark2\", ax=ax[1])\nplt.show()","7f3ae8fb":"g = sns.jointplot(x = 'q1_char_num', y = 'q2_char_num', kind = \"scatter\", hue=data.is_duplicate, data = traindata, palette=\"Dark2\")\ng.fig.suptitle(\"Joint plot between Number of Characters in Quesion1 and Quesion2\", y=1.02)\nplt.show()","e92ccc86":"g = sns.jointplot(x = 'q1_word_num', y = 'q2_word_num', kind = \"scatter\", hue=data.is_duplicate, data = traindata, palette=\"Dark2\")\ng.fig.suptitle(\"Joint plot between Number of Words in Quesion1 and Quesion2\", y=1.02)\nplt.show()","368112f3":"modelST = SentenceTransformer('paraphrase-mpnet-base-v2')","29221cbf":"# It took a lot of time, caused gpu overheat.\n# So I decided to do it in batch and save them in file.\ndef getBertEmbeddings(data, filename):\n    batch = 20000\n    with open(filename, 'wb') as f:\n        while(len(data)):\n            tempdata = data[:batch]\n            data = data[batch:]\n            tempembed = modelST.encode(tempdata.values, device='cuda')\n            np.save(f, tempembed, allow_pickle=True)\n#             time.sleep(60) # for gpu heating issue\n            ","796ab7e8":"# Get SentenceBERT embedding of train data\ngetBertEmbeddings(traindata.question1_final, 'temp_train_question1_sentenceBERT.npy')\ngetBertEmbeddings(traindata.question2_final, 'temp_train_question2_sentenceBERT.npy')","7a19ab52":"# Get SentenceBERT embedding of test data\ngetBertEmbeddings(testdata.question1_final, 'temp_test_question1_sentenceBERT.npy')\ngetBertEmbeddings(testdata.question2_final, 'temp_test_question2_sentenceBERT.npy')","c79aa65f":"# Get cosine similarity and euclidean distance between two vectors\ndef cosine_euclidean(u, v):\n    return np.array([np.dot(u, v) \/ (np.linalg.norm(u) * np.linalg.norm(v)), np.linalg.norm(u - v)])","63c30ba8":"# open .npy files and loop through the sentence embeddings\nwith open('temp_train_question1_sentenceBERT.npy', 'rb') as q1_vec, open('temp_train_question2_sentenceBERT.npy', 'rb') as q2_vec:\n    distances = []\n    while True:\n        try:\n            q1_20k = np.load(q1_vec, allow_pickle=True)\n            q2_20k = np.load(q2_vec, allow_pickle=True)\n            for q1,q2 in zip(q1_20k, q2_20k):\n                dists = cosine_euclidean(q1, q2)\n                distances.append(dists)\n        except IOError as e:\n            distances = np.array(distances)\n            break","d6e6bcab":"distances = pd.DataFrame(distances, columns=['cosine_simlarity_bert', 'euclidean_distance_bert'])","48004907":"traindata = pd.concat([traindata, pd.DataFrame(distances)], axis=1)","d26ecbe1":"# open .npy files and loop through the sentence embeddings\nwith open('temp_test_question1_sentenceBERT.npy', 'rb') as q1_vec, open('temp_test_question2_sentenceBERT.npy', 'rb') as q2_vec:\n    distances = []\n    while True:\n        try:\n            q1_20k = np.load(q1_vec, allow_pickle=True)\n            q2_20k = np.load(q2_vec, allow_pickle=True)\n            for q1,q2 in zip(q1_20k, q2_20k):\n                dists = cosine_euclidean(q1, q2)\n                distances.append(dists)\n        except IOError as e:\n            distances = np.array(distances)\n            break\ndistances = pd.DataFrame(distances, columns=['cosine_simlarity_bert', 'euclidean_distance_bert'])\ntestdata = pd.concat([testdata, pd.DataFrame(distances)], axis=1)","1d0db670":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('Cosine Similarity based on SentenceBERT b\/w Question 1 and Question 2')\nsns.kdeplot(traindata['cosine_simlarity_bert'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Cosine Similarity based on SentenceBERT b\/w Question 1 and Question 2')\nsns.boxplot(x=data.is_duplicate, y=traindata['cosine_simlarity_bert'], palette=\"Dark2\", ax=ax[1])\nplt.show()","0778ca22":"plt.title('ECDF plot of Cosine Similarity based on SentenceBERT b\/w Question 1 and Question 2')\nsns.axes_style(\"whitegrid\")\nsns.ecdfplot(x=traindata['cosine_simlarity_bert'], hue=data.is_duplicate, palette=\"Dark2\")\nplt.show()","74e23951":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nax[0].title.set_text('Euclidean Distance based on SentenceBERT b\/w Question 1 and 2')\nsns.kdeplot(traindata['euclidean_distance_bert'], hue=data.is_duplicate, palette=\"Dark2\", ax=ax[0])\nax[1].title.set_text('Euclidean Distance based on SentenceBERT b\/w Question 1 and 2')\nsns.boxplot(x=data.is_duplicate, y=traindata['euclidean_distance_bert'], palette=\"Dark2\", ax=ax[1])\nplt.show()","651eee5a":"plt.title('ECDF plot of Euclidean Distance based on SentenceBERT b\/w Question 1 and Question 2')\nsns.axes_style(\"whitegrid\")\nsns.ecdfplot(x=traindata['euclidean_distance_bert'], hue=data.is_duplicate, palette=\"Dark2\")\nplt.show()","d150e699":"traindata.drop(columns=['question1_final', 'question2_final'], inplace=True)","2ab3faaf":"traindata = traindata.to_numpy()","59197dbc":"scaler = MinMaxScaler()","0b2bf597":"scaler.fit(traindata)","8fd101f5":"traindata = scaler.transform(traindata)","16f54fd9":"testdata.drop(columns=['question1_final', 'question2_final'], inplace=True)\ntestdata = testdata.to_numpy()\ntestdata = scaler.transform(testdata)","d8133998":"with open('temp_testdata.npy', 'wb') as f:\n    batch = 20000\n    while(len(testdata)):\n        tempdata = testdata[:batch]\n        testdata = testdata[batch:]\n        np.save(f, tempdata, allow_pickle=True)","0cc17573":"def loadVectors(filename):\n    with open(filename, 'rb') as f:\n        q_vectors = []\n        while True:\n            try:\n                q_vec = np.load(f, allow_pickle=True)\n                q_vectors.extend(list(q_vec))\n            except IOError as e:\n                q_vectors = np.array(q_vectors)\n                break\n    return q_vectors","8a40d2df":"train_question1_vec = loadVectors('temp_train_question1_sentenceBERT.npy')","90ab5925":"train_question2_vec = loadVectors('temp_train_question2_sentenceBERT.npy')","d8a5a384":"traindata = np.hstack((traindata, train_question1_vec, train_question2_vec))","3a16a0fa":"traindata.shape","f0a3b891":"oversample = RandomOverSampler(sampling_strategy='minority')\nX_train, y_train = oversample.fit_resample(traindata, data.is_duplicate.to_numpy())","eb17cb05":"print(np.count_nonzero(y_train == 0))\nprint(np.count_nonzero(y_train == 1))","4293f5c4":"splits = ShuffleSplit(n_splits=1, test_size=.3, random_state=42)","ddafd4ed":"svc_param_grid = {'C':[1e-2, 1e-1, 1e0, 1e1, 1e2]}","f36ff5dc":"svc_clf = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, max_iter=3000)","8c955a1c":"svc_clf_search = HalvingGridSearchCV(svc_clf, svc_param_grid, cv=splits, factor=2, scoring='accuracy', verbose=3)","882492eb":"svc_clf_search.fit(X_train, y_train)","dd2bb52b":"svc_clf_search.best_params_","e9e5b5bc":"svc_clf_search.best_score_","19c0cdcd":"svc_clf_model = svc_clf_search.best_estimator_","6f73c824":"svc_clf_model","e4e288e0":"svc_calibrated = CalibratedClassifierCV(base_estimator=svc_clf_model, method=\"sigmoid\", cv=splits)","3b34a58c":"svc_calibrated.fit(X_train, y_train)","33ee5ee1":"with open('testdata.npy', 'rb') as X_test_1, \\\n    open('test_question1_sentenceBERT.npy', 'rb') as X_test_q1, \\\n    open('test_question2_sentenceBERT.npy', 'rb') as X_test_q2:\n    y_pred_proba_svc = []\n    while True:\n        try:\n            test_20k = np.load(X_test_1, allow_pickle=True)\n            q1_20k = np.load(X_test_q1, allow_pickle=True)\n            q2_20k = np.load(X_test_q2, allow_pickle=True)\n            X_test = np.hstack((test_20k, q1_20k, q2_20k))\n            y_pred_proba_svc.extend(list(svc_calibrated.predict_proba(X_test)[:,1]))\n        except IOError as e:\n            break","cca01b67":"testids = pd.read_csv('test_id.csv', na_filter=False)","bfab044c":"submission_svc = pd.DataFrame({'test_id':testids.test_id.values, 'is_duplicate':y_pred_proba_svc})","da23a6de":"submission_svc.to_csv('submission_svc.csv', index=False)","cf51761c":"splits = ShuffleSplit(n_splits=1, test_size=.3, random_state=42)","c86fdc3a":"rf_param_grid = {\n                    'n_estimators':[200, 500, 800], \n                    'min_samples_split':[5, 15],\n                    'max_depth': [70, 150, None]\n                }","5b47b114":"rf_clf = RandomForestClassifier()","b56436d7":"rf_clf_search = HalvingGridSearchCV(rf_clf, rf_param_grid, cv=splits, factor=2, scoring='accuracy', verbose=3)","8709b902":"rf_clf_search.fit(X_train, y_train)","ea8f4d65":"rf_clf_search.best_params_","ddb6993d":"rf_clf_search.best_score_","b5fc7bc3":"rf_clf_model = rf_clf_search.best_estimator_","d23ea836":"rf_clf_model","c9536526":"with open('testdata.npy', 'rb') as X_test_1, \\\n    open('test_question1_sentenceBERT.npy', 'rb') as X_test_q1, \\\n    open('test_question2_sentenceBERT.npy', 'rb') as X_test_q2:\n    y_pred_proba_rf = []\n    while True:\n        try:\n            test_20k = np.load(X_test_1, allow_pickle=True)\n            q1_20k = np.load(X_test_q1, allow_pickle=True)\n            q2_20k = np.load(X_test_q2, allow_pickle=True)\n            X_test = np.hstack((test_20k, q1_20k, q2_20k))\n            y_pred_proba_rf.extend(list(rf_clf_model.predict_proba(X_test)[:,1]))\n        except IOError as e:\n            break","8c5e109e":"testids = pd.read_csv('test_id.csv', na_filter=False)","9f9c14de":"submission_rf = pd.DataFrame({'test_id':testids.test_id.values, 'is_duplicate':y_pred_proba_rf})","219e6f6e":"submission_rf.to_csv('submission_rf.csv', index=False)","f23f1185":"random_2l = np.random.choice(range(len(X_train)), size=200000, replace=False)","64a66e63":"X_train_2l = X_train[random_2l]\ny_train_2l = y_train[random_2l]","b0167dfd":"def objective(space):\n    warnings.filterwarnings(action='ignore', category=UserWarning)\n    classifier = xgb.XGBClassifier(\n                    objective = \"binary:logistic\",\n                    eval_metric = \"logloss\",\n                    booster = \"gbtree\",\n                    tree_method = \"hist\",\n                    grow_policy = \"lossguide\",\n                    n_estimators = 300, \n                    max_depth = space['max_depth'],\n                    learning_rate = space['learning_rate'],\n                )\n    \n    X_train, X_cv, y_train, y_cv = train_test_split(X_train_2l, y_train_2l, test_size=0.25)\n    \n    classifier.fit(X_train, y_train)\n    \n    predicted_probs = classifier.predict_proba(X_cv)\n\n    logloss = log_loss(y_cv, predicted_probs)\n\n    print(\"Log loss = \" + str(logloss))\n\n    return{'loss':logloss, 'status': STATUS_OK }\n","e77cc0da":"space = {\n    'max_depth' : hp.choice('max_depth', range(4, 10, 1)),\n    \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.5, 0.01)\n}","6d4ced7f":"trials = Trials()\nbest_param = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5,\n            trials=trials)","0aa90ee7":"print(\"Best Param : \", best_param)","35b3778a":"params = dict(\n            objective = \"binary:logistic\",\n            eval_metric = \"logloss\",\n            booster = \"gbtree\",\n            tree_method = \"hist\",\n            grow_policy = \"lossguide\",\n            max_depth = 4,\n            eta = 0.14\n        )","084602a1":"X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.2)","f60af750":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_cv, label=y_cv)","f07d52df":"watchlist = [(dtrain, 'train'), (dvalid, 'valid')]","f98b1653":"xgb_model = xgb.train(params, dtrain, 600, watchlist, early_stopping_rounds=20, verbose_eval=10)","7540aba7":"with open('testdata.npy', 'rb') as X_test_1, \\\n    open('test_question1_sentenceBERT.npy', 'rb') as X_test_q1, \\\n    open('test_question2_sentenceBERT.npy', 'rb') as X_test_q2:\n    y_pred_proba_xgb = []\n    while True:\n        try:\n            test_20k = np.load(X_test_1, allow_pickle=True)\n            q1_20k = np.load(X_test_q1, allow_pickle=True)\n            q2_20k = np.load(X_test_q2, allow_pickle=True)\n            X_test = xgb.DMatrix(np.hstack((test_20k, q1_20k, q2_20k)))\n            y_pred_proba_xgb.extend(list(xgb_model.predict(X_test)))\n        except IOError as e:\n            break","31daf26b":"testids = pd.read_csv('test_id.csv', na_filter=False)","7d5025ae":"submission_xgb = pd.DataFrame({'test_id':testids.test_id.values, 'is_duplicate':y_pred_proba_xgb})","b19d672a":"submission_xgb.to_csv('submission_xgb.csv', index=False)","73019580":"X_train_orig, idices = np.unique(X_train, axis=0, return_index=True)","9ac3e14a":"y_train_orig = y_train[idices]","8d6273d2":"X_train_orig.shape","ddbae99d":"params = dict(\n            objective = \"binary:logistic\",\n            eval_metric = \"logloss\",\n            booster = \"gbtree\",\n            tree_method = \"hist\",\n            grow_policy = \"lossguide\",\n            max_depth = 4,\n            eta = 0.15,\n            subsample = .8,\n            colsample_bytree = .8,\n            reg_lambda = 1,\n            reg_alpha = 1\n        )","2793ff51":"X_train, X_cv, y_train, y_cv = train_test_split(X_train_orig, y_train_orig, test_size=0.25)","dd1e9d78":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_cv, label=y_cv)","35ba1549":"watchlist = [(dtrain, 'train'), (dvalid, 'valid')]","d49b1d40":"xgb_model2 = xgb.train(params, dtrain, 500, watchlist, early_stopping_rounds=20, verbose_eval=10)","e70b1363":"with open('testdata.npy', 'rb') as X_test_1, \\\n    open('test_question1_sentenceBERT.npy', 'rb') as X_test_q1, \\\n    open('test_question2_sentenceBERT.npy', 'rb') as X_test_q2:\n    y_pred_proba_xgb = []\n    while True:\n        try:\n            test_20k = np.load(X_test_1, allow_pickle=True)\n            q1_20k = np.load(X_test_q1, allow_pickle=True)\n            q2_20k = np.load(X_test_q2, allow_pickle=True)\n            X_test = xgb.DMatrix(np.hstack((test_20k, q1_20k, q2_20k)))\n            y_pred_proba_xgb.extend(list(xgb_model2.predict(X_test)))\n        except IOError as e:\n            break\ntestids = pd.read_csv('test_id.csv', na_filter=False)\nsubmission_xgb2 = pd.DataFrame({'test_id':testids.test_id.values, 'is_duplicate':y_pred_proba_xgb})\nsubmission_xgb2.to_csv('submission_xgb2.csv', index=False)","6a339eb8":"These features seems to be the most successful ones. It seems we can separate most of the classes just by using one of these features.\n* Cosine Similarity is larger for duplicate pairs.\n* 80% of non-duplicate question pairs and only 20% of duplicate question pairs have cosine similarity of <= .815\n* Euclidean Distance is smaller for duplicate pairs.\n* 20% of non-duplicate question pairs and approx 80% of duplicate question pairs have euclidean distance of <= 2.","27ac5642":"The public leader board score for the Kaggle submission is **0.32105**, which slightly better than the other models. I was expecting a better result than this. Which is possible with more fine-tuning the hyperparameters. XGBoost have tons of hyperparameters https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html","895feea8":"Test data don't have question ids. So the independent variables are **question1**, **question2** and the dependent variable is **is_duplicate**.","f90e11d0":"# Data Overview","94801ad2":"### Testing","678962b4":"Train the model on whole data with the tuned parameters.","1900dcdb":"# Featurization with SentenceBERT","915f62c4":"For example, we have a question like \"How can I be a good geologist?\" and there are some answers to that question. Later someone else asks another question like \"What should I do to be a great geologist?\".<br>\nWe can see that both the questions are asking the same thing. Even though the wordings for the question are different, the intention of both questions is same.\u00a0<br>\nSo the answers will be same for both questions. That means we can just show the answers of the first question. That way the person who is asking the question will get the answers immediately and people who have answered already the first question don't have to repeat themselves.","dcebbc24":"Due to time and system configuration constrained, I decided to use 200000 data points to estimate a few of the params.<br>\nAt first, I was using Optuna for hyperparameter tuning but it had some issues because of which it was not releasing memory after the trials. So the system was crash after few trials.<br>\nLater on, I decided to use HyperOpt for the tuning.","5c0e8656":"* Out of **808574** total questions (including both question1 and question2), **537929** are unique.\n* Most of the questions are repeated very few times. Only a few of them are repeated multiple times.\n* One question is repeated **157** times which is the max number of repetitions.","92d096db":"The public leader board score for the Kaggle submission is **0.32372**, which slightly better than SVC.<br>\nI was expecting a little less logloss but remember we have not done calibration (due to time constraints).","00d5bbd2":"# Data Pre-processing","ead34145":"### Testing","4d28c0fb":"We have created two more features **cosine_simlarity_bert** and **euclidean_distance_bert** which measures similarity and distance between both pairs of questions.","74a42833":"# Future Work","c5ceb488":"**36.92%** of question pairs are duplicates and **63.08%** of questions pair non-duplicate.","16f47580":"FuzzyWuzzy uses Levenshtein Distance to calculate the differences between sequences. https:\/\/github.com\/seatgeek\/fuzzywuzzy","4274d97d":"### Training","9beb2bf0":"The public leader board score for the Kaggle submission is **0.36980**<br>\nIt is very good considering that the model assumes linear separability.","7e897798":"We can try Deep learning based models to get even better result.","7986dc31":"### Testing","ab3e4373":"We have created **23** features from the questions.\n* We have created features q1_word_num, q2_word_num with count of characters for both questions.\n* We have created total_word_num feature which is equal to sum of q1_word_num and q2_word_num.\n* We have created differ_word_num feature which is absolute difference between q1_word_num and q2_word_num.\n* We have created same_first_word feature which is 1 if both questions have same first word otherwise 0.\n* We have created same_last_word feature which is 1 if both questions have same last word otherwise 0.\n* We have created total_unique_word_num feature which is equal to total number of unique words in both questions.\n* We have created total_unique_word_withoutstopword_num feature which is equal to total number of unique words in both questions without the stop words.\n* The total_unique_word_num_ratio is equal to total_unique_word_num divided by total_word_num.\n* We have created common_word_num feature which is count of total common words in both questions.\n* The common_word_ratio feature is equal to common_word_num divided by total_unique_word_num.\n* The common_word_ratio_min is equal to common_word_num divided by minimum number of words between question 1 and question 2.\n* The common_word_ratio_max is equal to common_word_num divided by maximum number of words between question 1 and question 2.\n* We have created common_word_withoutstopword_num feature which is count of total common words in both questions excluding the stopwords.\n* The common_word_withoutstopword_ratio feature is equal to common_word_withoutstopword_num divided by total_unique_word_withoutstopword_num.\n* The common_word_withoutstopword_ratio_min is equal to common_word_withoutstopword_num divided by minimum number of words between question 1 and question 2 excluding the stopwords.\n* The common_word_withoutstopword_ratio_max is equal to common_word_withoutstopword_num divided by maximum number of words between question 1 and question 2 excluding the stopwords.\n* Then we have extracted fuzz_ratio, fuzz_partial_ratio, fuzz_token_set_ratio and fuzz_token_sort_ratio features with fuzzywuzzy string matching tool.\u00a0Reference: https:\/\/github.com\/seatgeek\/fuzzywuzzy","1bdaa035":"We got rid of oversampled data by removing the duplicate rows.","a6080fbf":"### Training","8eebe896":"* If First word or Last word is the same then there is a high chance that the question pairs are duplicates.\n* The number of total unique words (q1 and q2 both combined) with and without stopwords is less if question pairs are duplicate.\n* For duplicate question pairs, the total unique words to total words ratio is generally smaller.\n* Duplicate question pairs tend to have more common words between both the questions. Hence extracted features related to common words are also showing differences in distributions.\n* The fuzz ratios tend to be generally higher for duplicate question pairs.","b22c7fa3":"# <b>Quora Question Pairs<\/b>","c18bbdd3":"* We have converted everything to lower case.\n* We have removed contractions.\n* We have replaced currency symbols with currency names.\n* We have also removed hyperlinks.\n* We have removed non-alphanumeric characters.\n* We have removed inflections with word lemmatizer.\n* We have also removed HTML tags.","3a5e6c8d":"Since the dataset was imbalanced. We did **oversample** by sampling from the minority class. <br>\nNow we have **510048** data points. **255024** from each class.","7ac45431":"# Business Metrics\n\nIt is a binary classification.\n* We need to minimize the log loss for this challenge.","27bd90d0":"One of the many problems that quora face is the duplication of questions. Duplication of question ruins the experience for both the questioner and the answerer. Since the questioner is asking a duplicate question, we can just show him\/her the answers to the previous question. And the answerer doesn't have to repeat his\/her answer for essentially the same questions.","9668417e":"We have **1561** features (25 + 768 + 768). <br>\n* **25** are extracted features.<br>\n* **768+768** for sentence embedding of question 1 and question 2.","fe2d785b":"We have normalized the test data also. And save them in batch of 20k, just like we did with the embeddings.","b58072f8":"# Basic EDA","9970cabe":"\ud83e\udd41 Voila! We have a winner. **This submission resulted in public LB score of 0.28170**<br>\nThis seems a very good result.","17ed2213":"### Training","c437cea2":"Available Columns : <b>id, qid1, qid2, question1, question2, is_duplicate<\/b><br>\nClass labels : <b>0, 1<\/b><br>\nTotal training data \/ No. of rows :  <b>404290<\/b><br>\nNo. of columns :  <b>6<\/b><br>\n**is_duplicate** is the dependent variable.<br>\nNo. of non-duplicate data points is <b>255027<\/b><br>\nNo. of duplicate data points is <b>149263<\/b>","e1476afa":"Quora is a platform for Q&A, just like StackOverflow. But quora is more of a general-purpose Q&A platform that means there is not much code like in StackOverflow.","2ce4c845":"It is showing the Pareto Principle (80-20 rule).","4fc0a24f":"The total number of features till now is **25**.","74ae6f61":"I chose few more params based on instinct.","48ca7245":"I was not happy with the result of the XGBoost model so I decided to tune the parameters with gut feeling.","cf0fc060":"# Business Objectives and Constraints","3fe8652b":"3 rows had null values. So We removed them and now We have **404287** question pairs.","64f821c5":"# Data Cleaning","e1d96452":"SentenceBERT is a BERT based sentence embedding technique. We will use pre-trained SentenceBERT model paraphrase-mpnet-base-v2, which is recommended for best quality. The SentenceBERT produces an output of 768 dimensions. https:\/\/www.sbert.net\/","4a237aa8":"## Another XGBoost","f322ceec":"Note that I have not set aside any data for testing locally. Because our main goal is to get a good score on Kaggle.","bbae700a":"There are some questions with very few characters, which does not make sense. It will be taken care of later with Data Cleaning.","d75ed4ee":"* There is no strict latency requirement.\n* We would like to have interpretability but it is not absolutely mandatory.\n* The cost of misclassification is medium.\n* Both classes (duplicate or not) are equally important.","2a371057":"### Testing","5b4f63ea":"## XGBoost","8d1f63d2":"## Random Forest","9cbee456":"# Training Models","ccf0be17":"### Training","67cf74e9":"This problem is available on Kaggle as a competition. https:\/\/www.kaggle.com\/c\/quora-question-pairs","8e6667ae":"I tried InferSent sentence embeddings. But it returns 4096 dimension representation. And after applying it the train data became huge. So I discarded it. And I chose SentenceBERT for this problem.","db776108":"### EDA on new features related to SentenceBERT","a316e802":"Now since we need to minimize log loss for the competition. We would want a good predicted probability. Calibrated Classifier can be used to get a good predicted probability.","73b9a2ab":"We have normalized (min-max scaling) the extracted features of train data. We have not normalized the embeddings because it is not recommended.","95bfaf12":"# EDA with Features","d7284982":"## Support Vector Classifier","b6f2330a":"The Halving Grid Search CV found C=100 to be the best param. And the best accuracy is 85.79%.","e0ee7d11":"We have **404290** training data points. And only **36.92%** are positive. That means it is an imbalanced dataset.","a83e5e85":"# Feature Extraction","14992e06":"# Introduction"}}