{"cell_type":{"6457bbc8":"code","4cc8eb5e":"code","bf327f7d":"code","44891dcf":"code","740f8669":"code","744551f1":"code","6bf12ff3":"code","ee3aa087":"code","fea5fa02":"code","6f0d7f8d":"code","c11d2159":"code","a2de5c15":"code","e088cdd3":"code","5921fd68":"code","5e271b2e":"code","d64c14cd":"code","f723fe3b":"code","262e3a8d":"code","4fc93a5a":"code","8e263b25":"code","58d81fb7":"code","cf2ac5bb":"code","c2804dbe":"code","c1b50544":"code","0c7212a0":"code","dc00f117":"code","6af08907":"code","9bcd8dad":"code","38564551":"code","9efdf5ba":"code","b6cb844a":"code","684b5754":"code","0d53157f":"code","b93e6055":"code","d056fca9":"code","404fc717":"code","7172cff4":"code","cffc6051":"code","13575d12":"code","0975804e":"code","4e02a2ec":"code","29b63f05":"code","bbd3ccfa":"code","6bf22e5d":"code","7a89f96e":"markdown","d6c1f801":"markdown","91d5c924":"markdown","383cd2cc":"markdown","8165ea38":"markdown","596fa4c0":"markdown","3b9299e9":"markdown","d7d0ac7c":"markdown","2f891811":"markdown","e8217223":"markdown","686bac4e":"markdown","9e10eea7":"markdown","f1b3488a":"markdown","bdc8355f":"markdown","4ec07382":"markdown","8bdfbf60":"markdown","5902e4ee":"markdown","85a33a15":"markdown","5cd0e89a":"markdown","a919f2f4":"markdown","d3ef50e3":"markdown","a267d96f":"markdown","b8ca6f17":"markdown","b924e4f0":"markdown","b639f20e":"markdown","78a8c460":"markdown"},"source":{"6457bbc8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ndic_df={}\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        \n     \n        name=filename.split(\"-\")\n        name=name[0]\n        dic_df[\"Data {}\".format(name)]=pd.read_csv(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nfor name in dic_df:\n    print(name)\n    df=dic_df[name]\n    df.info()\n    print(\"-------------------------------------------------------\")","4cc8eb5e":"df15=dic_df[\"Data 2015\"]\n\n\ndf15=df15.loc[:,['GHGEmissions(MetricTonsCO2e)','OSEBuildingID', 'BuildingType'\n                 , 'PrimaryPropertyType','Neighborhood', 'YearBuilt'\n                 ,'NumberofFloors'\n                 , 'PropertyGFATotal', 'PropertyGFAParking'\n                 ,'PropertyGFABuilding(s)', 'LargestPropertyUseType']]\n\n        \n\n\n","bf327f7d":"#count_nan is a function which takes as argument a pd.Serie\n #and retrieve the percentage of np.nan of this series\ndef count_nan(serie):\n    N=serie.shape[0]\n    n=serie.isna().sum()\n    taux_nan=n\/N*100\n    return taux_nan\n\nremplissage=[]\nfor i in df15.columns:\n    les_nan=count_nan(df15[i])\n    rempl=100-les_nan\n    remplissage.append(rempl)\n    \n#Visualization of the filling rate\n(fig, ax) = plt.subplots(figsize=(13, 7))\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Filling rate \\n Dataset 2015\")\nax=plt.bar(range(0,len(df15.columns)), remplissage\n           ,edgecolor='black',color=\"#6fd67b\")\n\nplt.ylim(40,105)\nplt.xlabel(\"Variables\")\nplt.ylabel(\"% of data\")\nplt.xticks(range(0,len(df15.columns))\n           ,df15.columns,rotation=90)\nplt.grid()\nplt.show(block=True)    \n","44891dcf":"df15=df15.dropna()\nremplissage=[]\nfor i in df15.columns:\n    les_nan=count_nan(df15[i])\n    rempl=100-les_nan\n    remplissage.append(rempl)\n    \n#Visualization of the filling rate\n# after droping nans \n(fig, ax) = plt.subplots(figsize=(8, 5))\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Filling rate \\n Dataset 2015\")\nax=plt.bar(range(0,len(df15.columns)), remplissage\n           ,edgecolor='black',color=\"#6fd67b\")\n\nplt.ylim(40,105)\nplt.xlabel(\"Variables\")\nplt.ylabel(\"% of data\")\nplt.xticks(range(0,len(df15.columns))\n           ,df15.columns,rotation=90)\nplt.grid()\nplt.show(block=True)    ","740f8669":"cat_var=[ 'BuildingType','Neighborhood', 'LargestPropertyUseType']\n\n#This function allows you to view the distribution \n#of CO2 emissions within the classes of a variable.\ndef visualisation(variable,df):\n    the_mean=df[\"GHGEmissions(MetricTonsCO2e)\"].mean()\n    fig=plt.figure(figsize=[18,7])\n    fig.patch.set_facecolor('#E0E0E0')\n    fig.patch.set_alpha(0.7)\n    plt.title(\"C02 emissions distribution by {}\".format(variable),size=16)\n    sns.boxplot(x=variable, y=\"GHGEmissions(MetricTonsCO2e)\", data=df,color=\"#cbd1db\",width=0.5,showfliers=False,showmeans=True)\n    plt.hlines(y=the_mean,xmin=-0.5,xmax=len(df[variable].unique())-0.5,color=\"#6d788b\",ls=\"--\",label=\"Global mean\")\n\n    plt.ylabel(\" C02 emissions\",size=14)\n    plt.xticks(range(0,len(df[variable].unique()))\n               ,df[variable].unique(),rotation=90)\n    plt.legend()\n    plt.grid()\n    plt.show()\nvisualisation('Neighborhood',df15)","744551f1":"visualisation('BuildingType',df15)","6bf12ff3":"visualisation('LargestPropertyUseType',df15)","ee3aa087":"borne=round(np.percentile(df15[\"GHGEmissions(MetricTonsCO2e)\"].values, 95),2)\ndf15=df15.loc[df15[\"GHGEmissions(MetricTonsCO2e)\"]<borne]\nborne2=round(np.percentile(df15[\"GHGEmissions(MetricTonsCO2e)\"].values, 5),2)\ndf15=df15.loc[df15[\"GHGEmissions(MetricTonsCO2e)\"]>borne2]","fea5fa02":"for v in cat_var:\n    visualisation(v,df15)","6f0d7f8d":"#This function will return \"True\" if the average within the category \n#is greater than x times the overall average. return \"False\" otherwise\ndef x_times_more(x,val1,global_mean):\n    if val1 > x*global_mean:\n        return True\n    else:\n        return False\n#This function will return \"True\" if the average within the category \n#is lower than x times the overall average. return \"False\" otherwise    \ndef x_times_less(x,val1,global_mean):\n    if val1 < x*global_mean:\n        return True\n    else:\n        return False \n    \n    \n#This function returns the list of categories with high and very low CO2 emissions,\n#with a threshold (a proportion of the global average)\n\ndef large_emission_track(variable, df,tresholdmax,tresholdmin):\n    the_mean=df[\"GHGEmissions(MetricTonsCO2e)\"].mean()\n    cat=df[variable].unique()\n    high_cat=[]\n    low_cat=[]\n    for c in cat:\n        x=df.loc[df[variable]==c]\n        m=x[\"GHGEmissions(MetricTonsCO2e)\"].mean()\n        if x_times_more(tresholdmax,m,the_mean):\n            high_cat.append(c)\n        elif x_times_less(tresholdmin,m,the_mean):\n            low_cat.append(c)\n    return high_cat, low_cat\n    ","c11d2159":"use_high,use_low=large_emission_track('LargestPropertyUseType', df15,2.5,0.4)\nprint(\"Categories with high CO2 emissions: \\n\",use_high)\nprint(\"\\n Categories with low CO2 emissions: \\n\",use_low)","a2de5c15":"\n#Function allowing to encode the level of emission of co2 \n#according to the class.\n#0 = low emissions\n#1 = Average emissions\n#2 = high emissions\ndef encode_usetype(usetype):\n    if usetype in use_high:\n        return 2\n    elif usetype in use_low:\n        return 0\n    else:\n        return 1\n#Encoding LargestUseType\ndf15[\"largestUseType\"]=df15['LargestPropertyUseType'].apply(encode_usetype)\ndf15=df15.drop(columns=['LargestPropertyUseType'])","e088cdd3":"df15.head(1)","5921fd68":"visualisation(\"PrimaryPropertyType\",df15)","5e271b2e":"use_high,use_low=large_emission_track('PrimaryPropertyType', df15,2.5,0.4)\nprint(\"Categories with high CO2 emissions: \\n\",use_high)\nprint(\"\\n Categories with low CO2 emissions: \\n\",use_low)","d64c14cd":"df15[\"PropertyType\"]=df15['PrimaryPropertyType'].apply(encode_usetype)\ndf15=df15.drop(columns=['PrimaryPropertyType'])","f723fe3b":"use_high,use_low=large_emission_track('BuildingType', df15,1.8,0.5)\nprint(\"Categories with high CO2 emissions: \\n\",use_high)\nprint(\"\\n Categories with low CO2 emissions: \\n\",use_low)","262e3a8d":"df15[\"BuildingType\"]=df15['BuildingType'].apply(encode_usetype)\n","4fc93a5a":"import statistics\ndef large_emission_track_med(variable, df,tresholdmax,tresholdmin):\n    the_mean=statistics.median(df[\"GHGEmissions(MetricTonsCO2e)\"].values)\n    cat=df[variable].unique()\n    high_cat=[]\n    low_cat=[]\n    for c in cat:\n        x=df.loc[df[variable]==c]\n        m=statistics.median(x[\"GHGEmissions(MetricTonsCO2e)\"].values)\n        if x_times_more(tresholdmax,m,the_mean):\n            high_cat.append(c)\n        elif x_times_less(tresholdmin,m,the_mean):\n            low_cat.append(c)\n    return high_cat, low_cat","8e263b25":"use_high,use_low=large_emission_track('Neighborhood', df15,1.5,0.6)\nprint(\"Categories with high CO2 emissions: \\n\",use_high)\nprint(\"\\n Categories with low CO2 emissions: \\n\",use_low)\ndef encode_downtown(area):\n    if area==\"DOWNTOWN\":\n        return 1\n    else:\n        return 0\ndf15['Neighborhood']=df15['Neighborhood'].apply(encode_usetype)","58d81fb7":"df15[\"Parking\/building rate\"]=df15[\"PropertyGFAParking\"]\/df15[\"PropertyGFATotal\"]*100\nsns.displot(data=df15, x=\"Parking\/building rate\")","cf2ac5bb":"outliers=df15.loc[df15[\"Parking\/building rate\"]>100]\ndf15=df15.loc[df15[\"Parking\/building rate\"]<=100]\nprint(df15.shape)","c2804dbe":"print(\"{} values exceeding 100 for the variable'Parking\/building rate'\".format(outliers.shape[0]))\noutliers.head(5)","c1b50544":"variables_num=['GHGEmissions(MetricTonsCO2e)', 'BuildingType',\n       'Neighborhood', 'YearBuilt', 'NumberofFloors',\n       'PropertyGFATotal', 'PropertyGFAParking', 'PropertyGFABuilding(s)',\n       'largestUseType', 'PropertyType', 'Parking\/building rate']\nx=df15.loc[:, variables_num]\n# calculate the correlation matrix\ncorr = x.corr()\nlab=x.columns\n# plot the heatmap\nfig=plt.figure(figsize=[12,9])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\n\nplt.title(\"Linear correlation between features\",size=18)\nax=sns.heatmap(corr, vmin=-1, vmax=1,cmap=\"bwr\",\n        xticklabels=variables_num,\n        yticklabels=variables_num)","0c7212a0":"df=df15.copy()\n# to retrieve the coefficients\ncoef_pearson=[]\ncoef_spearman=[]\n#to retrieve the variables names\nlabelp=[]\nlabels=[]\n#to retrieve the colors to be assigned to the bars\ncolorp=[]\ncolors=[]\nbleu=\"#9fb4ff\"\nrouge=\"#ffae9f\"\n\n#Variables studied\nvariables=[  'BuildingType',\n       'Neighborhood', 'YearBuilt', 'NumberofFloors',\n       'PropertyGFATotal', 'PropertyGFAParking', 'PropertyGFABuilding(s)',\n       'largestUseType', 'PropertyType', 'Parking\/building rate']\n\n#Variable display threshold\nseuil=0.1\n\n\nfor var in variables:\n    cp=stats.pearsonr(df['GHGEmissions(MetricTonsCO2e)'].values,df[var])[0]\n    cs=stats.spearmanr(df['GHGEmissions(MetricTonsCO2e)'].values,df[var])[0]\n   #Pearson coefficients\n    if abs(cp)>= seuil:\n        coef_pearson.append(cp)\n        labelp.append(var)\n        if cp>0:\n            colorp.append(rouge)\n        else:\n            colorp.append(bleu)\n    #Spearman coefficients                       \n    if abs(cs)>= seuil:\n        coef_spearman.append(cs)\n        labels.append(var)\n        if cs>0:\n            colors.append(rouge)\n        else:\n            colors.append(bleu)","dc00f117":"fig=plt.figure(1,figsize=[15,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.suptitle(\"correlation with CO2 emissions.\",size=16)\n\n# premier barplot des coefficients de corr\u00e9lation de Pearson\nplt.subplot(1,2,1)\nplt.title(\"Pearson correlation\")\nplt.bar(np.arange(len(labelp))+1,coef_pearson,color=colorp,edgecolor='black')\nplt.xticks(np.arange(len(labelp))+1,labelp,rotation=90)\nplt.hlines(0,0.5,len(labelp)+0.5,color='black')\nplt.ylabel(\"Coefficient value\")\nplt.ylim(-0.4,1)\nplt.grid()\n\n# 2eme barplot des coefficients de corr\u00e9lation de Spearman\nplt.subplot(1,2,2)\nplt.title(\"Spearman correlation.\")\nplt.bar(np.arange(len(labels))+1,coef_spearman,color=colors,edgecolor='black')\nplt.xticks(np.arange(len(labels))+1,labels,rotation=90)\nplt.hlines(0,0.5,len(labels)+0.5,color='black')\nplt.ylim(-0.4,1)\nplt.grid()","6af08907":"# Function returning a sample of n rows\ndef get_sample(N,df):\n    n =df.shape[0]\n    p =N\/n\n    sample=df.sample(frac=p, replace=True)\n    print(\"A {} rows sample  has been extract.\".format(N))\n    return sample\n","9bcd8dad":"sample=get_sample(4000,df15)","38564551":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ny=sample[\"GHGEmissions(MetricTonsCO2e)\"]\nX=sample.drop(columns=['GHGEmissions(MetricTonsCO2e)', 'OSEBuildingID'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","9efdf5ba":"from sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import *\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import  make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\n#function returning dictionary of model performances on the dataset received as argument\n#We work with the r2 scores obtained after a cross validation, with 10 splits\ndef lets_try(train, y):\n    results = {}\n    ss=StandardScaler()\n    scaled_train=ss.fit_transform(train)\n    \n   \n    def test_model(clf):\n        cv = KFold(n_splits=10, shuffle=True, random_state=45)\n        r2 = make_scorer(r2_score)\n        r2_val_score = cross_val_score(clf, train, y, cv=cv, scoring=r2)\n        scores = r2_val_score\n        return scores\n    \n    #for the model which needed standardized data \n    def test_model_scaler(clf):\n        cv = KFold(n_splits=10, shuffle=True, random_state=45)\n        r2 = make_scorer(r2_score)\n        r2_val_score = cross_val_score(clf, scaled_train, y, cv=cv, scoring=r2)\n        scores = r2_val_score\n        return scores\n    \n    clf = LinearRegression()\n    results[\"Linear\"] = test_model_scaler(clf)\n    print(\"Linear done\")\n    \n    clf = Ridge()\n    results[\"Ridge\"] = test_model_scaler(clf)\n    print(\"Ridge done\")\n\n    clf = BayesianRidge()\n    results[\"Bayesian Ridge\"] = test_model(clf)\n    print(\"Bayesian Ridge done\")\n\n    clf = Lasso(alpha=1e-4)\n    results[\"Lasso\"] = test_model_scaler(clf)\n    print(\"Lasso done.\")\n\n    clf = BaggingRegressor()\n    results[\"Bagging\"] = test_model(clf)\n    print(\"Baggind done\")\n\n    clf = RandomForestRegressor()\n    results[\"RandomForest\"] = test_model(clf)\n    print(\"Random forest done\")\n   \n\n    clf = AdaBoostRegressor()\n    results[\"AdaBoost\"] = test_model(clf)\n    print(\"Adaboost done\")\n\n    clf =SVR(kernel='rbf',gamma='auto',C=8)\n    results[\"SVM RBF\"] = test_model_scaler(clf)\n    print(\"SVM rbf done\")\n\n   \n    return results ","b6cb844a":"dico_results=lets_try(X_train,y_train)\n","684b5754":"fig=plt.figure(figsize=[10,10])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Distribution of Cross-validation score  on the trainning set.\",size=16)\nplt.boxplot(dico_results.values(),labels=dico_results.keys(),showmeans=True)\nplt.ylabel(\"Scores CV \\n R2\",size=14)\nplt.ylim(0,1)\nplt.grid()\n","0d53157f":"def test_model_mean(clf,train,y):\n        cv = KFold(n_splits=6, shuffle=True, random_state=45)\n        r2 = make_scorer(r2_score)\n        r2_val_score = cross_val_score(clf, train, y, cv=cv, scoring=r2)\n        scores = r2_val_score.mean()\n        return scores\n    \n# Maximum depth    \nparams=[2,3,5,10,15,20,25,30,40,50 ,60]\nscore_train=[]\nfor p in params:\n    model=RandomForestRegressor() \n    model.set_params(max_depth=p)\n    score=test_model_mean(model,X_train,y_train)\n    score_train.append(score)\n    ","b93e6055":"#Visualisation\nfig=plt.figure(figsize=[12,5])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Mean of Cross-validation score  on the trainning set.\",size=16)\nplt.plot(params,score_train)\nplt.grid()\nplt.xlabel(\"Params max depth value\")\nplt.ylabel(\"Mean of the CV score\")","d056fca9":"\nparams=[1,2,3,4,5,6,7,8,9,10]\nscore_train=[]\nfor p in params:\n    model=RandomForestRegressor() \n    model.set_params(max_features=p)\n    score=test_model_mean(model,X_train,y_train)\n    score_train.append(score)\n    \n","404fc717":"#visualisation\nfig=plt.figure(figsize=[12,5])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Mean of Cross-validation score  on the trainning set.\",size=16)\nplt.plot(params,score_train)\nplt.grid()\nplt.xlabel(\"Params max_features value\")\nplt.ylabel(\"Mean of the CV score\")\n    ","7172cff4":"best_model=RandomForestRegressor(n_estimators=300, max_depth=15,max_features=2)\ndef test_model(clf,train,y):\n        cv = KFold(n_splits=10, shuffle=True, random_state=45)\n        r2 = make_scorer(r2_score)\n        r2_val_score = cross_val_score(clf, train, y, cv=cv, scoring=r2)\n        scores = r2_val_score\n        return scores\nthe_scores=test_model(best_model,X_train,y_train)\nbest_model.fit(X_train,y_train)\ny_pred=best_model.predict(X_test)\nvalidation_score=r2_score(y_test,y_pred)","cffc6051":"fig=plt.figure(figsize=[10,10])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Distribution of Cross-validation score  on the trainning set and testing set.\",size=16)\nplt.boxplot(the_scores,showmeans=True)\nplt.scatter(x=[1],y=[validation_score],s=70,c=\"r\",marker=\"D\",label=\"Testing CV score \\n R2={}\".format(round(validation_score,2)))\nplt.ylabel(\"Scores CV \\n R2\",size=14)\nplt.xticks([1],[\"Random forest\"])\nplt.ylim(0.5,1)\nplt.xlim(0.5,1.5)\nplt.grid()\nplt.legend()","13575d12":"fig=plt.figure(figsize=[10,10])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Feature importances of the Random forest.\",size=16)\nplt.barh(X.columns, best_model.feature_importances_,color=\"#28a2b4\",edgecolor='black')\nplt.grid()","0975804e":"df_res=pd.DataFrame({\"True values\": y_test,\"Predictions\":y_pred})\ndf_res[\"Error\"]=abs(df_res[\"True values\"]-df_res[\"Predictions\"])\/df_res[\"True values\"]*100\ndf_res.head()\ndef color_to_use(num):\n    if np.isinf(num):\n        return \"#cc2900\"\n    elif num> 80:\n        return \"#cc2900\"\n    elif num >50:\n        return \"#dfa61f\"\n    elif num>30:\n        return \"#dddf1f\"\n    elif num>10:\n        return \"#7cce2e\"\n    else:\n        return \"#13a90d\"\ndf_res[\"color to use\"]=df_res[\"Error\"].apply(color_to_use)\ndf_res.head()","4e02a2ec":"df_res=df_res.sort_values(by = \"Error\")\ndict_lab={\"#cc2900\":'+80%','#dfa61f':\"between 50% and 80%\"\n          ,'#dddf1f':\"between 30% and 50%\"\n          ,'#7cce2e':\"between 10% and 30%\"\n          ,'#13a90d':\"Less than 10 %\"}\nfig=plt.figure(figsize=[10,10])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Testing set results, colored by the error rate (%).\",size=16)\nfor c in df_res[\"color to use\"].unique():\n    d=df_res.loc[df_res[\"color to use\"]==c]\n    sns.scatterplot(data=d,x=\"True values\",y=\"Predictions\",color=c,label=dict_lab[c])\n\nplt.xlabel('True Values ',size=13)\nplt.ylabel('Predictions ',size=13)\nplt.grid(color='#dddddd')\n\n","29b63f05":"def regroup(num):\n    if num<50:\n        return \"- 50\"\n    elif num<150:\n        return \"50-150\"\n    elif num<250:\n        return \"150-250\"\n    else:\n        return \"+250\"\ndf_res[\"regroup\"]=df_res[\"True values\"].apply(regroup)\ndf_res=df_res.sort_values(by = \"True values\")","bbd3ccfa":"fig=plt.figure(figsize=[12,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Distribution of the error rate according to the true values on the testing set.\",size=16)\nsns.countplot(data=df_res,x=\"regroup\",hue=\"color to use\",hue_order=dict_lab.keys(),palette=['#cc2900', '#dfa61f', '#dddf1f', '#7cce2e', '#13a90d'], edgecolor=\"black\")\nplt.legend(dict_lab.values())\nplt.grid()\nplt.xlabel(\"CO2 emissions (Metric Tones)\")","6bf22e5d":"t,p=df_res[\"True values\"].sum(),df_res[\"Predictions\"].sum()\nerror=abs(t-p)\/t*100\nprint(error)\n\nfig=plt.figure(figsize=[6,8])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Sum of CO2 emissions. \\n True values VS Predictions\",size=16)\nplt.bar(x=[1,2],height=[df_res[\"True values\"].sum(),df_res[\"Predictions\"].sum()],color=\"#28a2b4\",width=0.4, edgecolor=\"black\")\nplt.xticks([1,2],[\"True values\",\"Predictions\"])\nplt.text(1.7,77000,\"Error rate ={} %\".format(round(error,1)),size=12)\nplt.ylabel(\"Total CO2 emissions\")\nplt.ylim(50000,80000)\nplt.xlim(0.6,2.6)\nplt.grid()","7a89f96e":"We remove the rows where the ratio between the parking area and the total area is greater than 100","d6c1f801":"The score is maximum  for **max_features=15**","91d5c924":"First,I determine the features that impact the prediction the most","383cd2cc":"If we consider the total quantity of CO2 emissions for the year 2015, the error rate is + 2.2% which is very low. In this context, the model is efficient.","8165ea38":"\nI now use the ratio between the parking area and the total area to detect possible outliers.\n\nThis new variable can be used for the prediction, and will be added to the work set.","596fa4c0":"same approach for variable \"PrimaryPropertyType\"","3b9299e9":"Check the linear correlations between the different variables of the dataset","d7d0ac7c":"#  2.2.  Improve the setting .","2f891811":"# 2.1 Comparison of models on the training set","e8217223":"# 2. PREDICTION OF CO2 EMISSIONS .","686bac4e":"Let's check in particular the correlation between the different variables of the dataset and the target variable.","9e10eea7":"# **Student project:** Predict the CO2 emissions of  Seattle city from the data available ( 2015 or 2016).\n\nThis notebook consists of 3 parts\n\n1. Cleanin and feature engineering \n> with a correlation check\n\n2. Prediction of CO2 emissions\n\n>2.1 Comparison of models on the training set\n\n>2.2 Improvement of the Hyperparameters setting of the most efficient model\n\n3. Results analysis on the testing set. \n\n\n\n*I hope you will enjoy reading!!*\n\n","f1b3488a":"The score is maximum and stable from **max_depth=15**","bdc8355f":"I identify the categories emitting large quantities of CO2. \n\nThe threshold is set at 2.5 times more than the overall average for the largest Property use type","4ec07382":"New visualization after outliers removal","8bdfbf60":"**Random forest:**\nNow we will try to improve the setting of the Max_depth and max_features parameters.\n\nwe will use the average of the cross validation scores of the training dataset with 6 splits","5902e4ee":"before the transformation of the qualitative variables, I delete some lines with extreme values of co2 emissions\nI delete the 5% highest  values and 5% lowest values , \nI consider them as outliers","85a33a15":"The large prediction errors (in percentage) are mainly present for the small values to be predicted.\n\nFor quantities greater than 50 metric tons of CO2, the model is much more precise","5cd0e89a":"# 3.  Results analysis: Testing Set  ","a919f2f4":"# 1. Cleaning and feature engineering","d3ef50e3":"**Now i will analyze the prediction error on the test set**","a267d96f":"\nthe random forest model seems to be the most efficient.\n\n**Can we improve the setting of the random forest?**","b8ca6f17":"let's check the results of the optimized Random forest model","b924e4f0":"Same approach for the variable BuildingType with differents treshold","b639f20e":"\nI use the same type of approach for the \"neighborhood\" variable but using the median  instead of the mean","78a8c460":"The   prediction result depends mainly on the area of the buildings,  the overall area of the property, and the year of construction.\n\nMore modestly, the number of floors and the largest use type also impact the result."}}