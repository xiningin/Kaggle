{"cell_type":{"76b348fa":"code","dd0ff1e3":"code","76f84172":"code","0855e253":"code","c272b66e":"code","0475fe9c":"code","beb1b55a":"code","43752acc":"code","f7291ece":"code","75974662":"code","52dc6c24":"code","4a877901":"code","9c6be0ba":"code","079132aa":"code","ece6b55b":"code","556b5790":"code","f96cb05a":"code","5fa7eafe":"code","847ed27e":"code","d1504924":"code","c68640a5":"code","50443552":"code","2e3c8327":"markdown","5ddd8994":"markdown","c2537763":"markdown","550d3493":"markdown","c399fe9a":"markdown","7590bd31":"markdown","029be6a9":"markdown","269847a8":"markdown","a0a5eb7c":"markdown","0224a4ab":"markdown","4c26f3e1":"markdown","ab586c43":"markdown","505cc094":"markdown","1bf0f1dd":"markdown","4c770c95":"markdown","b10a5d19":"markdown","e2a33f5f":"markdown","fb96122e":"markdown"},"source":{"76b348fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n%env JOBLIB_TEMP_FOLDER=\/tmp\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib import colors\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom PIL import Image\nfrom imageio import imread\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\nfrom imageio import imread\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score\n\nfrom skimage.morphology import closing, disk, opening\nimport random\nimport time\nimport copy\nfrom tqdm import tqdm_notebook as tqdm\nfrom os import listdir\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dropout\nfrom keras.layers.core import Dense\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense, Flatten\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.core import Dropout\nfrom keras.layers.convolutional import *\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.applications.inception_v3 import decode_predictions\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom keras.models import model_from_json\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom keras import optimizers\nfrom keras.models import Sequential, Model \nfrom keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD\n\nfrom IPython.core.display import display, HTML\nfrom PIL import Image\nfrom io import BytesIO\nimport base64\n\nfrom os import listdir\nfrom skimage.segmentation import mark_boundaries\n\nimport cv2\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dd0ff1e3":"Directory='..\/input\/v2-plant-seedlings-dataset\/nonsegmentedv2\/'\nsubfolders = listdir(Directory)\nprint(os.listdir('..\/input\/v2-plant-seedlings-dataset\/nonsegmentedv2\/'))","76f84172":"sfc=\"Small-flowered Cranesbill\"\nplt.figure(figsize=(15,12))\n\nfor n in range(12):\n        folder=subfolders[n]\n        plt.subplot(3,4,n+1)\n        files = listdir(Directory + folder + \"\/\") \n        image=cv2.imread(Directory + folder + \"\/\" + files[n+221])\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.axis(\"off\")\n        plt.title(folder, fontsize=14, weight='bold')","0855e253":"p1 = cv2.imread('\/kaggle\/input\/v2-plant-seedlings-dataset\/nonsegmentedv2\/Small-flowered Cranesbill\/135.png')\np1 = cv2.cvtColor(p1, cv2.COLOR_BGR2RGB)\nplt.imshow(p1)\nplt.grid(False)\nplt.show()","c272b66e":"def set_size(w,h, ax=None):\n    \"\"\" w, h: width, height in inches \"\"\"\n    if not ax: ax=plt.gca()\n    l = ax.figure.subplotpars.left\n    r = ax.figure.subplotpars.right\n    t = ax.figure.subplotpars.top\n    b = ax.figure.subplotpars.bottom\n    figw = float(w)\/(r-l)\n    figh = float(h)\/(t-b)\n    ax.figure.set_size_inches(figw, figh)\n    \npixel_colors = p1.reshape((np.shape(p1)[0]*np.shape(p1)[1], 3))\nnorm = colors.Normalize(vmin=-1.,vmax=1.)\nnorm.autoscale(pixel_colors)\npixel_colors = norm(pixel_colors).tolist()","0475fe9c":"r, g, b = cv2.split(p1)\nfig = plt.figure()\naxis = fig.add_subplot(1, 1, 1, projection=\"3d\")\nset_size(6,6)\npixel_colors = p1.reshape((np.shape(p1)[0]*np.shape(p1)[1], 3))\nnorm = colors.Normalize(vmin=-1.,vmax=1.)\nnorm.autoscale(pixel_colors)\npixel_colors = norm(pixel_colors).tolist()\n\naxis.scatter(r.flatten(), g.flatten(), b.flatten(), facecolors=pixel_colors, marker=\".\")\naxis.set_xlabel(\"Red\", weight='bold')\naxis.set_ylabel(\"Green\", weight='bold')\naxis.set_zlabel(\"Blue\", weight='bold')\n\nplt.show()","beb1b55a":"hsv_p1 = cv2.cvtColor(p1, cv2.COLOR_RGB2HSV)","43752acc":"h, s, v = cv2.split(hsv_p1)\nfig = plt.figure()\naxis = fig.add_subplot(1, 1, 1, projection=\"3d\")\n\n\naxis.scatter(h.flatten(), s.flatten(), v.flatten(), facecolors=pixel_colors, marker=\".\")\naxis.set_xlabel(\"Hue\",weight='bold')\naxis.set_ylabel(\"Saturation\", weight='bold')\naxis.set_zlabel(\"Value\", weight='bold')\nset_size(6,6)\nplt.show()","f7291ece":"def plot_mask(image, colormin, colormax):\n        hsv_p1 = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)   \n        mask = cv2.inRange(hsv_p1, colormin , colormax)\n        result = cv2.bitwise_and(image, image, mask=mask)\n        plt.figure(figsize=(15,10))\n        plt.subplot(1, 3, 1)\n        plt.imshow(image)\n        plt.grid(False)\n        plt.subplot(1, 3, 2)\n        plt.imshow(mask, cmap=\"gray\")\n        plt.grid(False)\n        plt.subplot(1, 3, 3)\n        plt.imshow(result)\n        plt.grid(False)\n        return plt.show()","75974662":"colormin=(36, 25, 25)\ncolormax=(70, 255,255)\n\nplot_mask(p1, colormin, colormax)","52dc6c24":"new_colormin=(25,50,50)\nnew_colormax=(80,255,255)\nplot_mask(p1, new_colormin, new_colormax)","4a877901":"def segmented(image):\n    foto = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    hsv_foto = cv2.cvtColor(foto, cv2.COLOR_RGB2HSV)\n    #print(\"hsvh\",hsv_foto.dtype)\n    colormin=(25,50,50)\n    colormax=(86,255,255)\n\n    mask = cv2.inRange(hsv_foto, colormin , colormax)\n    #print(\"mask\",mask.dtype)\n    result = cv2.bitwise_and(foto, foto, mask=mask)\n    #print(\"result\",result.dtype)\n    pil_image= Image.fromarray(result)\n\n\n    return result","9c6be0ba":"plt.figure(figsize=(15,10))\n\nfor n in range(12):\n        folder = subfolders[n]\n        plt.subplot(3,4,n+1)\n        files = listdir(Directory + folder + \"\/\") \n        image=cv2.imread(Directory + folder + \"\/\" + files[n+221])\n        plt.imshow(segmented(image))\n        plt.axis(\"off\")\n        plt.title(folder, weight='bold', fontsize=14)\n","079132aa":"def segmented2(image):\n    image=np.array(image)\n    #foto=image.copy().astype(np.uint8)\n    #foto = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    hsv_foto = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n    colormin=(25,50,50)\n    colormax=(90,255,255)\n    mask = cv2.inRange(hsv_foto, colormin , colormax)\n    \n    #result = cv2.bitwise_and(foto, foto, mask=mask)\n    result = cv2.bitwise_and(image, image, mask=mask)\n    result2=np.array(result)\n    #pil_image= Image.fromarray(result, mode='RGB')\n    #pil_image= Image.fromarray(result)\n\n\n    return result2","ece6b55b":"train_datagen = ImageDataGenerator(\n        rescale=1.\/255,\n        shear_range=0.2,\n        zoom_range=0.1,\n        horizontal_flip=True,\n        validation_split=0.2,\n        #preprocessing_function = segmented2\n                     )\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    '..\/input\/v2-plant-seedlings-dataset\/nonsegmentedv2',\n        target_size=(64,64),\n        batch_size=32,\n        class_mode='categorical',\n        subset='training')\n\nvalidation_generator = train_datagen.flow_from_directory(\n        '..\/input\/v2-plant-seedlings-dataset\/nonsegmentedv2',\n        target_size=(64, 64),\n        batch_size=32,\n        class_mode='categorical',\n        subset='validation')","556b5790":"#x,y = train_generator.next()\n#for i in range(0,2):\n#    image = x[i]\n#    plt.imshow(image)\n#    plt.show()","f96cb05a":"#weights_incv3 = '..\/input\/inceptionv3\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n# load pre-trained weights and add global average pooling layer\n#base_model_incv3 = InceptionV3(weights=weights_incv3, input_shape=(150,150,3), include_top=False, pooling='avg')\n\n# freeze convolutional layers\n#for layer in base_model_incv3.layers:\n#    layer.trainable = False\n\n#define classification layers\n#x = Dense(1024, activation='relu')(base_model_incv3.output)\n#predictions = Dense(1, activation='sigmoid')(x)\n#x = Dense(256, activation='relu')(base_model_incv3.output)\n#x = Dropout(0.5)(x)\n#predictions = Dense(12, activation='softmax')(x)\n\n#model = Model(inputs=base_model_incv3.input, outputs=predictions)\n#print(model.summary())","5fa7eafe":"from keras.applications.vgg16 import VGG16\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten\n\nvgg_conv = VGG16(weights=None, include_top=False, input_shape=(64, 64, 3))\nvgg_conv.load_weights('..\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n\nfor layer in vgg_conv.layers[:-4]:\n    layer.trainable = False\n\nmodel = Sequential()\nmodel.add(vgg_conv)\n \nmodel.add(Flatten())\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(12, activation='softmax'))\n \nmodel.summary()","847ed27e":"model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', \n              metrics=['accuracy'])","d1504924":"# We'll stop training if no improvement after some epochs\nearlystopper1 = EarlyStopping(monitor='loss', patience=10, verbose=1)\n\n# Save the best model during the traning\ncheckpointer1 = ModelCheckpoint('best_model1.hdf5'\n                                ,monitor='val_accuracy'\n                                ,verbose=1\n                                ,save_best_only=True\n                                ,save_weights_only=True)","c68640a5":"history = model.fit_generator(train_generator, steps_per_epoch=800, \n                    validation_data=validation_generator,\n                    validation_steps=128,\n                    epochs=15, verbose=1,\n                   callbacks=[checkpointer1])","50443552":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,10))\n#First Model\nax1 = plt.subplot2grid((2,2),(0,0))\ntrain_loss = history.history['loss']\ntest_loss = history.history['val_loss']\nx = list(range(1, len(test_loss) + 1))\nplt.plot(x, test_loss, color = 'cyan', label = 'Test loss')\nplt.plot(x, train_loss, label = 'Training losss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Model 1: Loss vs. Epoch',weight='bold', fontsize=18)\nax1 = plt.subplot2grid((2,2),(0,1))\ntrain_acc = history.history['accuracy']\ntest_acc = history.history['val_accuracy']\nx = list(range(1, len(test_acc) + 1))\nplt.plot(x, test_acc, color = 'cyan', label = 'Test accuracy')\nplt.plot(x, train_acc, label = 'Training accuracy')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Model 1: Accuracy vs. Epoch', weight='bold', fontsize=18)\nplt.show()","2e3c8327":"From this plot, you can see that the green parts of the image are mixed with the red and blue values.  Segmenting our plant image out in RGB space based on ranges of RGB values would not be easy. \n\nThe solution is to try another color space, we go next with HSV color space.","5ddd8994":"> ### 3.3 Visualize the results of segmentation on the 12 plant species","c2537763":"# References:\n\n[1] Weed Control in Clean Agriculture: A Review http:\/\/www.scielo.br\/scielo.php?script=sci_arttext&pid=S0100-83582016000200377\n\n[2] Image Segmentation Using Color Spaces in OpenCV https:\/\/realpython.com\/python-opencv-color-spaces\/\n\n[3] Pick out color ranges https:\/\/toolstud.io\/color\/rgb.php\n\n[4] Computer vision with seedlings https:\/\/www.kaggle.com\/allunia\/computer-vision-with-seedlings","550d3493":"# 3- Segmentation:\n***\n> ### 3.1 Color-based segmentation:\n\nTo pick out a range color, we can use a [color picking app online [3]](https:\/\/toolstud.io\/color\/rgb.php). I found the green color ranges in a [forum](https:\/\/stackoverflow.com\/questions\/47483951\/how-to-define-a-threshold-value-to-detect-only-green-colour-objects-in-an-image): \n* **minimum green**(H=**36**, S=**25**, V=**25**)\n* **maximum green**(H=**70**, S=**255**,V=**255**)\n\nWe will use the openCV function *cv2.inRange()* to try to threshold our plant image with the minimum and maximum green values. \n\n*inRange()* takes three parameters: the image, the lower range, and the higher range. It returns a binary mask (an ndarray of 1s and 0s) the size of the image where values of 1 indicate values within the range, and zero values indicate values outside [2].","c399fe9a":"> ### 3.2 Segmentation function:\n\nNow, we create a function that applies segmentation on the rest of the images. This function will be used to feed the computer vision model with the segmented images instead of the original ones.","7590bd31":"**Great!** The segmentation looks fine. We got rid of the stones and the containers pixels. Let's apply this mask on the rest of pictures and visualize the results.","029be6a9":"# 4- Computer vision model:\n***\n\nIn this section, we will try to feed those plant images to a pretrained VGG16 convolutional neural network. We will train with both original and segmented images in order to find out the impact of the target leakage on training.\n> ### 4.1 VGG16 with original plant images:","269847a8":"> ### 4.3 VGG16: segmented images\n\n***[WORK IN PROGRESS]***","a0a5eb7c":"We will try to process this image in order to get rid of the unnecessary pixels. Feeding this picture to the model directly will result in a target leakage. The classifier will learn the stones and the container's pixels and build its prediction upon these pixels as well. This is something we would like to avoid.\n\nWe start our preprocessing with simply segment an object from an image based on color using OpenCV. A popular computer vision library written in C\/C++ with bindings for Python, [OpenCV provides easy ways of manipulating color spaces [2].](https:\/\/realpython.com\/python-opencv-color-spaces\/)","0224a4ab":"In HSV space, our plant\u2019s greens are much more localized and visually separable. The saturation and value of the greens do vary, but they are mostly located within a small range along the hue axis. This is the key point that can be leveraged for segmentation.","4c26f3e1":"The result of our initial threshold is not bad, but we are missing some pixels of the plants. We will try to play manually with the saturation and brightness values to get a better a mask.","ab586c43":"> ### 2.2 Visualize the plant image in RGB color space:\n\nHere, we visualize the small-flowered Cranesbill image we opened above in RGB space to see the distributions of the color pixels.\n\nRGB is considered an \u201cadditive\u201d color space, and colors can be imagined as being produced from shining quantities of red, blue, and green light onto a black background.","505cc094":"After visualizing the images, we have an idea of the how to deal with this project:\n\n\n **Aim:** Classify the plant species images so that the machine distinguishes between crop seedlings and weed. \n\n**Challenge:** The images include some stones in the background, we don't want the machine to learn the stones pixels and base its prediction on all the components of the image: seedlings + stones  \n\n**Solution:** Color-based segmentation:  Keep just the green color pixels = Keep just the seedlings pixels  \n\n**Excepted result:** Clean plant images without stones in the background. Ready to be fed to the model.\n","1bf0f1dd":"> ### 1.2 Crop seedlings images visualization:","4c770c95":"# 2- Image preprocessing:\n***\n\n> ### 2.1 Import the images\n\nWe start our image preprocessing by importing a Small-flowered Cranesbill image as a test image.","b10a5d19":"# 1- Introduction:\n***\n> ### 1.1 Weed control:\n\nWeed control is considered a major obstacle for the growers in organic farming. Lower plant productivity in organic farming is mainly related to poor weed control. It is widely known, in most cases, that losses caused by weeds exceeded the losses from any category of agricultural pests. Under water-stress condition, [weeds can reduce crop yields more than 50% through moisture competition alone[1]](http:\/\/www.scielo.br\/scielo.php?script=sci_arttext&pid=S0100-83582016000200377).\n\nIn this study, we are provided with a dataset of 12 plant species, including weed. The idea is to have an effective weed control and in order to do so, the first critical requirement is a correct weed identification. \n\nWe will try to solve this weed identification problem with Machine Learning. The aim is to build a computer vision classifier that recognizes plant species images and differentiates between crop seedlings and weed.\n\n> ### 1.2 The 12 plant species available in this dataset: ","e2a33f5f":"> ### 4.2 Model evaluation:","fb96122e":"> ### 2.3 Visualize the plant image in HSV color space:\n\n**HSV** stands for **Hue, Saturation, and Value (or brightness)**, and is a cylindrical color space. The colors, or hues, are modeled as an angular dimension rotating around a central, vertical axis, which represents the value channel. Values go from dark (0 at the bottom) to light at the top. The third axis, saturation, defines the shades of hue from least saturated, at the vertical axis, to most saturated furthest away from the center:"}}