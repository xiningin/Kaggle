{"cell_type":{"26e0a4b7":"code","d8212b13":"code","11c2901a":"code","8216be1d":"code","db5831d1":"code","36dc1e71":"code","c749e021":"code","99b3e33e":"code","4d7373cd":"code","a1574cd0":"code","a9d1d3c5":"code","eb2f0e3e":"code","7b04077e":"code","1a7660ed":"code","897ddaf3":"code","7aeb03bc":"code","f24fa0ed":"code","b875270e":"code","b16c74c4":"code","08fd2bb6":"code","49a052de":"code","83c72b79":"code","8d949fdb":"code","92575114":"code","e230bf15":"markdown"},"source":{"26e0a4b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d8212b13":"from sklearn.model_selection import train_test_split  \nfrom sklearn.model_selection import cross_val_score\n\nimport xgboost as xgb\nimport time\n\nimport numpy as np\nimport pandas as pd","11c2901a":"from scipy.stats import norm\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold","8216be1d":"train_df = pd.read_csv(\"..\/input\/X_train.csv\")\ntest_df = pd.read_csv(\"..\/input\/X_test.csv\")\ny = pd.read_csv(\"..\/input\/y_train.csv\")\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\")","db5831d1":"def _kurtosis(x):\n    return kurtosis(x)\n\ndef CPT5(x):\n    den = len(x)*np.exp(np.std(x))\n    return sum(np.exp(x))\/den\n\ndef skewness(x):\n    return skew(x)\n\ndef SSC(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    xn_i1 = x[0:len(x)-2]  # xn-1\n    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2),0)\n    return sum(ans[1:]) \n\ndef wave_length(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    return sum(abs(xn_i2-xn))\n    \ndef norm_entropy(x):\n    tresh = 3\n    return sum(np.power(abs(x),tresh))\n\ndef SRAV(x):    \n    SRA = sum(np.sqrt(abs(x)))\n    return np.power(SRA\/len(x),2)\n\ndef mean_abs(x):\n    return sum(abs(x))\/len(x)\n\ndef zero_crossing(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1\n    return sum(np.heaviside(-xn*xn_i2,0))\n","36dc1e71":"def feature_extraction(raw_frame):\n    frame = pd.DataFrame()\n    raw_frame['angular_velocity'] = raw_frame['angular_velocity_X'] + raw_frame['angular_velocity_Y'] + raw_frame['angular_velocity_Z']\n    raw_frame['linear_acceleration'] = raw_frame['linear_acceleration_X'] + raw_frame['linear_acceleration_Y'] + raw_frame['linear_acceleration_Y']\n    raw_frame['velocity_to_acceleration'] = raw_frame['angular_velocity'] \/ raw_frame['linear_acceleration']\n    \n    for col in raw_frame.columns[3:]:\n        frame[col + '_mean'] = raw_frame.groupby(['series_id'])[col].mean()        \n        frame[col + '_CPT5'] = raw_frame.groupby(['series_id'])[col].apply(CPT5) \n        frame[col + '_SSC'] = raw_frame.groupby(['series_id'])[col].apply(SSC) \n        frame[col + '_skewness'] = raw_frame.groupby(['series_id'])[col].apply(skewness)\n        frame[col + '_wave_lenght'] = raw_frame.groupby(['series_id'])[col].apply(wave_length)\n        frame[col + '_norm_entropy'] = raw_frame.groupby(['series_id'])[col].apply(norm_entropy)\n        frame[col + '_SRAV'] = raw_frame.groupby(['series_id'])[col].apply(SRAV)\n        frame[col + '_kurtosis'] = raw_frame.groupby(['series_id'])[col].apply(_kurtosis) \n        frame[col + '_mean_abs'] = raw_frame.groupby(['series_id'])[col].apply(mean_abs) \n        frame[col + '_zero_crossing'] = raw_frame.groupby(['series_id'])[col].apply(zero_crossing) \n    return frame","c749e021":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","99b3e33e":"def multiclass_accuracy(preds, train_data):\n    labels = train_data.get_label()\n    pred_class = np.argmax(preds.reshape(9, -1).T, axis=1)\n    return 'multi_accuracy', np.mean(labels == pred_class), True","4d7373cd":"train_df=feature_extraction(train_df)","a1574cd0":"test_df=feature_extraction(test_df)","a9d1d3c5":"le = LabelEncoder()\ntarget = le.fit_transform(y['surface'])","eb2f0e3e":"# Create 0.75\/0.25 train\/test split\nX_train, X_test, y_train, y_test = train_test_split(train_df, target, test_size=0.25, train_size=0.75,\n                                                    random_state=42)","7b04077e":"# Leave most parameters as default\nparam = {'objective': 'multi:softmax', # Specify multiclass classification\n         'num_class': 9, # Number of possible output classes\n         'tree_method': 'hist' # Use gpu_hist for GPU accelerated algorithm.\n         }","1a7660ed":"# Convert input data from numpy to XGBoost format\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)","897ddaf3":"dtext_X = xgb.DMatrix(test_df)","7aeb03bc":"num_round = 500\ngpu_res = {} # Store accuracy result\ntmp = time.time()\n# Train model\nbst=xgb.train(param, dtrain, num_round, evals=[(dtest, 'test')], evals_result=gpu_res)\nprint(\"CPU Training Time: %s seconds\" % (str(time.time() - tmp))) #tried to time it aganst GPU ","f24fa0ed":"predictions = bst.predict(dtext_X)  ","b875270e":"sub['surface'] = le.inverse_transform(predictions.astype(int))\nsub.to_csv('submission.csv', index=False)","b16c74c4":"sub.head()","08fd2bb6":"# Leave most parameters as default\nparam = {'objective': 'multi:softprob', # usieng Probilities for multiclass classification\n         'num_class': 9, # Number of possible output classes\n         'tree_method': 'gpu_hist' \n         }","49a052de":"num_round = 500 \ngpu_res = {} # Store accuracy result\ntmp = time.time()\n# Train model\nbst=xgb.train(param, dtrain, num_round, evals=[(dtest, 'test')], evals_result=gpu_res)\nprint(\"GPU Training Time: %s seconds\" % (str(time.time() - tmp))) #tried to time it aganst GPU ","83c72b79":"predictions = bst.predict(dtext_X)  ","8d949fdb":"predictions.shape","92575114":"sub['surface'] = le.inverse_transform(predictions.argmax(axis=1))\nsub.to_csv('submission-xgb-prob.csv', index=False)","e230bf15":"I tried to use LGBM with GPU but even after strugging to install for more than 24 hrs, resulted in various errors, FindBoost, then error 2026 and I gave up. \n<br\/>But bent upon using GPU as I invested in the laptop, so I installed XGBoost with gpu. It was so easy just followed the steps.\n<br\/>This kerenl uses GPU but for CPU you can cnange the params 'tree_method':'gpu_hist' to 'hist' \n<br\/>With default params it scores very well 0.65!\n<br\/>Thanks to the feature engineering code here https:\/\/www.kaggle.com\/jsaguiar\/surface-recognition-baseline\/notebook"}}