{"cell_type":{"4d4cd011":"code","c064567d":"code","49f2e061":"code","3174ff61":"code","8072794e":"code","0571f04b":"code","85d20110":"code","ab1e76e3":"code","e0178ca7":"code","342d8c36":"code","6eb15f43":"code","92a0169b":"code","54b73ef9":"code","021a557d":"code","6eae7945":"code","8e6b817b":"code","c429234f":"code","8e7f643e":"code","cab85ebd":"code","7c8a28a1":"code","ce7414c4":"code","856d0578":"code","2af660fd":"code","fe036de7":"code","2be89b8d":"code","0f767ec2":"code","e3625abe":"code","22fd8996":"code","93ce354e":"code","20fe72dd":"code","cc81a264":"code","ae9fdf28":"code","2e402520":"code","d684ac9a":"markdown","04a1b07c":"markdown","0e753324":"markdown","09fb5c4b":"markdown","45cd9c41":"markdown","a52af58f":"markdown","927a67d2":"markdown","12cb9575":"markdown","a5ddd65a":"markdown"},"source":{"4d4cd011":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows',50)\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import confusion_matrix, classification_report, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV","c064567d":"dataset = pd.read_csv(\"..\/input\/marketing-data\/marketing_data.csv\")","49f2e061":"dataset","3174ff61":"dataset.info()","8072794e":"dataset.columns","0571f04b":"#renaming column\ndataset.rename(columns={\" Income \": \"Income\"}, inplace = True )","85d20110":"dataset.isnull().sum() ","ab1e76e3":"#checking unique values\ndataset['Education'].unique().tolist()\n","e0178ca7":"dataset['Marital_Status'].unique().tolist()","342d8c36":"dataset['Country'].unique().tolist()","6eb15f43":"#checking if target data are balanced\ndataset['Response'].value_counts()","92a0169b":"def preprocessing(dataset):\n    dataset = dataset.copy()\n    \n    \n    #filling nans and converting to numeric\n    dataset[\"Income\"] = dataset[\"Income\"].replace('[$,]', '', regex=True).astype(float)\n    dataset[\"Income\"] = dataset[\"Income\"].fillna(dataset[\"Income\"].mean())\n    \n    \n    #converting Dt_customer to date time\n    dataset[\"Dt_Customer\"] =  pd.to_datetime(dataset[\"Dt_Customer\"])\n    dataset[\"Year\"]=dataset[\"Dt_Customer\"].dt.year\n    dataset[\"Month\"]=dataset[\"Dt_Customer\"].dt.month\n    dataset[\"Day\"]= dataset[\"Dt_Customer\"].dt.day\n    dataset = dataset.drop(\"Dt_Customer\", axis = 1)\n    \n    \n    #replacing 2nd cycle in Education column\n    dataset[\"Education\"] = dataset[\"Education\"].str.replace('2n Cycle','Master')\n    \n    #replacing marital statuses\n    dataset[\"Marital_Status\"] = dataset[\"Marital_Status\"].str.replace('Alone','Single')\n    dataset[\"Marital_Status\"] = dataset[\"Marital_Status\"].str.replace('YOLO','Other')\n    dataset[\"Marital_Status\"] = dataset[\"Marital_Status\"].str.replace('Absurd','Other')\n    \n    #dropping ID\n    dataset = dataset.drop(\"ID\", axis=1)\n    \n    #getting dummies\n    dataset = pd.get_dummies(data=dataset, columns=['Education', \"Marital_Status\", \"Country\"])\n    #X, y split\n    X = dataset.drop(\"Response\", axis=1)\n    y = dataset[\"Response\"]\n    \n    #scaling\n    sc = StandardScaler()\n    X = pd.DataFrame(sc.fit_transform(X), index=X.index, columns=X.columns)\n    \n    #train, test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n    \n    #balacing data\n    sm = SMOTE(random_state=2)\n    X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n    \n    return X_train_res, X_test, y_train_res, y_test","54b73ef9":"X_train_res, X_test, y_train_res, y_test = preprocessing(dataset)","021a557d":"xg = xgb.XGBClassifier()\nxg.fit(X_train_res, y_train_res)\ny_pred = xg.predict(X_test)\ny_pred_train = xg.predict(X_train_res)","6eae7945":"print('Model Accuracy : ', accuracy_score(y_test, y_pred) *  100)\nprint('Model Recall : ', recall_score(y_test, y_pred) *  100)\nprint('Model Precision : ', precision_score(y_test, y_pred) *  100)\nprint(\"F1 Score: \", f1_score(y_test, y_pred) * 100)","8e6b817b":"metrics_before_tune = (\"Accuracy before tune\", \"Recall  before tune\", \"Precision  before tune\", \"F1  before tune\")","c429234f":"eval_before_tune = pd.DataFrame(xg, index=metrics_before_tune, columns=[\"Score\"])","8e7f643e":"eval_before_tune.loc[\"Recall  before tune\", \"Score\"] = recall_score(y_test, y_pred)\neval_before_tune.loc[\"Accuracy before tune\", \"Score\"] =  accuracy_score(y_test, y_pred)\neval_before_tune.loc[\"Precision  before tune\", \"Score\"] = precision_score(y_test, y_pred)\neval_before_tune.loc[\"F1  before tune\", \"Score\"] = f1_score(y_test, y_pred)","cab85ebd":"eval_before_tune","7c8a28a1":"print('Model Accuracy : ', accuracy_score(y_train_res, y_pred_train) *  100)\nprint('Model Recall : ', recall_score(y_train_res, y_pred_train) *  100)\nprint('Model Precision : ', precision_score(y_train_res, y_pred_train) *  100)\nprint(\"F1 Score: \", f1_score(y_train_res, y_pred_train) * 100)","ce7414c4":"name = X_train.columns","856d0578":"importance = pd.DataFrame(xg.feature_importances_, index = name, columns = [\"Score\"]).sort_values(\"Score\", ascending = False)","2af660fd":"importance","fe036de7":"import seaborn as sns\nimport matplotlib.pyplot as plt\n#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = X_train.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)\nplt.show()","2be89b8d":"parameters = {\n        'learning_rate': [0.01, 0.5],\n        'max_depth': [3, 5, 7, 10,50],\n        'min_child_weight': [1, 3, 5, 10],\n        'subsample': [0.1, 0.7],\n        'colsample_bytree': [0.5, 0.9],\n        'n_estimators' : [1, 20, 50],\n        'objective': ['reg:squarederror']\n    }\nscorer = make_scorer(accuracy_score)","0f767ec2":"scorer_accuracy = make_scorer(accuracy_score)\nscorer_recall = make_scorer(recall_score)\nscorer_precision = make_scorer(precision_score)\nscorer_f1 = make_scorer(f1_score)","e3625abe":"def generate_xg_from_search(xg, parameters, scorer, X, y):\n    search_obj = RandomizedSearchCV(xg, parameters, scoring=scorer)\n    fit_obj = search_obj.fit(X, y)\n    best_xg = fit_obj.best_estimator_\n    return best_xg","22fd8996":"scores = cross_val_score(best_xg, X_train_res, y_train_res, cv=5, scoring= \"accuracy\")\nscores.mean()\n","93ce354e":"scorer_recall = cross_val_score(best_xg, X_train_res, y_train_res, cv=5, scoring= \"recall\")\nscorer_accuracy = cross_val_score(best_xg, X_train_res, y_train_res, cv=5, scoring= \"accuracy\")\nscorer_precision = cross_val_score(best_xg, X_train_res, y_train_res, cv=5, scoring= \"precision\")\nscorer_f1_score = cross_val_score(best_xg, X_train_res, y_train_res, cv=5, scoring= \"f1_macro\")\n","20fe72dd":"metrics_after_tune = (\"Accuracy after tuning\", \"Recall after tuning\", \"Precision after tuning\", \"F1 after tuning\")","cc81a264":"eval_after_tune = pd.DataFrame(make_scorer, index=metrics_after_tune, columns=[\"Score\"])","ae9fdf28":"eval_after_tune.loc[\"Recall after tuning\", \"Score\"] = (scorer_recall.mean() * 100)\neval_after_tune.loc[\"Accuracy after tuning\", \"Score\"] = (scorer_accuracy.mean()*100)\neval_after_tune.loc[\"Precision after tuning\", \"Score\"] = (scorer_f1_score.mean()*100)\neval_after_tune.loc[\"F1 after tuning\", \"Score\"] = (scorer_precision.mean()*100)","2e402520":"eval_after_tune","d684ac9a":"# Introduction","04a1b07c":"# Fitting into XGBoost and evaluatin results before tuning","0e753324":"# Reading and exploring basic information","09fb5c4b":"# Cheking which features are important for model","45cd9c41":"# Importing neccessary libraries ","a52af58f":"# Preprocessing","927a67d2":"# Checking correlation among features","12cb9575":"Hello guys, this code is part of Maven analytics challenge. EDA will be performed in PowerBI, but i\u00b4d like to create simple predictive model using XGBoost to spice things up!\n\nLink to challenge: \n\nhttps:\/\/www.mavenanalytics.io\/blog\/maven-marketing-challenge?utm_source=linkedin&utm_campaign=marketingchallenge_li_maven","a5ddd65a":"# Parameter tuning"}}