{"cell_type":{"da6a12cc":"code","d575200c":"code","1bab6958":"code","8f1a92ae":"code","66f6e53c":"code","0eb5dbac":"code","fd233864":"code","d71e5536":"code","0f700f03":"code","226e1298":"code","8e9af01e":"code","5e4ce503":"code","3b9d51b4":"code","c848be1c":"code","dced9239":"code","3f76490e":"code","347c27e8":"code","fae5f331":"code","e68c3280":"code","6b076a30":"code","544e8692":"code","4296e58a":"code","57f942c2":"code","b22f07a1":"code","2c96de1d":"code","b0eaaaf6":"code","2ef33eed":"code","08dec80f":"code","7c75905e":"code","10428808":"code","36dc87de":"code","4a9917bc":"code","99f32b99":"code","a8568f7a":"code","b28a81a5":"code","741d30e0":"code","88eea82d":"code","04d4b07f":"code","4fe836cd":"code","0c96d3ae":"code","be8a45f9":"code","d49f496d":"code","6a82fbee":"code","52267f02":"code","f9b84213":"code","9b325351":"markdown","dddf0b39":"markdown","dc4af13c":"markdown","df4a2fbd":"markdown","6a79f534":"markdown","a6b87926":"markdown","b567738c":"markdown","49a1d09b":"markdown","76b01c54":"markdown","34a20453":"markdown","0471768d":"markdown","c1364770":"markdown","dac398e2":"markdown","e56d0ca3":"markdown","2b1d1fb2":"markdown","67439ef8":"markdown","b1930c37":"markdown","987c3251":"markdown","b67c596a":"markdown","28d5c314":"markdown","ddf6c994":"markdown","4f8a5c11":"markdown","4d40ebbe":"markdown","2e72f5e1":"markdown"},"source":{"da6a12cc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode()\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport seaborn as sns","d575200c":"import pandas as pd\n\nclick_data = pd.read_csv('..\/input\/talkingdata-adtracking-fraud-detection\/train_sample.csv',parse_dates=['click_time'])","1bab6958":"click_data.head()","8f1a92ae":"print(\"Shape of click_data is : {}\".format(click_data.shape))","66f6e53c":"# Add new columns for timestamp features day, hour, minute, and second\n\nclicks = click_data.copy()\nclicks['day'] = clicks['click_time'].dt.day.astype('uint8')\nclicks['hour'] = clicks['click_time'].dt.hour.astype('uint8')\nclicks['minute'] = clicks['click_time'].dt.minute.astype('uint8')\nclicks['second'] = clicks['click_time'].dt.second.astype('uint8')","0eb5dbac":"clicks.head()","fd233864":"from sklearn import preprocessing\n\ncat_features = ['ip', 'app', 'device', 'os', 'channel']\nlable_encoder = preprocessing.LabelEncoder()\n\nfor feature in cat_features:\n    encoded = lable_encoder.fit_transform(clicks[feature])\n    clicks[feature +'_labels'] = encoded","d71e5536":"clicks.head()","0f700f03":"feature_cols = ['day', 'hour', 'minute', 'second', \n                'ip_labels', 'app_labels', 'device_labels',\n                'os_labels', 'channel_labels']\n\nvalid_fraction = 0.1\nclicks_srt = clicks.sort_values('click_time')\nvalid_rows = int(len(clicks_srt) * valid_fraction)\ntrain = clicks_srt[:-valid_rows * 2]\nvalid = clicks_srt[-valid_rows * 2:-valid_rows]\ntest = clicks_srt[-valid_rows:]","226e1298":"train.head()","8e9af01e":"valid.head()","5e4ce503":"test.head()","3b9d51b4":"# msno.bar(train)\n\nplt.style.use('seaborn-colorblind')\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (16, 6))\n\nmsno.bar(train, ax = ax1, color=(10\/255, 3\/255, 250\/255), fontsize=10)\nmsno.bar(test, ax = ax2, color=(251\/255, 0\/255, 0\/255), fontsize=10)\n\nax1.set_title('Train Missing Values Map', fontsize = 16)\nax2.set_title('Test Missing Values Map', fontsize = 16);","c848be1c":"plt.style.use('dark_background')\nplt.figure(figsize=(10,8))  \n# sns.set(style=\"darkgrid\")\nax = sns.countplot(x = train['is_attributed'])","dced9239":"plt.style.use('dark_background')\nplt.figure(figsize=(10,8))  \n# sns.set(style=\"darkgrid\")\nax = sns.countplot(x = valid['is_attributed'])","3f76490e":"plt.style.use('dark_background')\nplt.figure(figsize=(10,8))  \n# sns.set(style=\"darkgrid\")\nax = sns.countplot(x = test['is_attributed'])","347c27e8":"import lightgbm as lgb\n\ndtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\ndvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\ndtest = lgb.Dataset(test[feature_cols], label=test['is_attributed'])\n\nparam = {'num_leaves': 64, 'objective': 'binary'}\nparam['metric'] = 'auc'\nnum_round = 1000\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=3)","fae5f331":"from sklearn import metrics\n\nypred = bst.predict(test[feature_cols])\nscore = metrics.roc_auc_score(test['is_attributed'], ypred)\nprint(f\"Test score: {score}\")","e68c3280":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing, metrics\nimport lightgbm as lgb\n\nclicks = pd.read_parquet('..\/input\/feature-engineering-data\/baseline_data.pqt')","6b076a30":"def get_data_splits(dataframe, valid_fraction=0.1):\n    \"\"\"Splits a dataframe into train, validation, and test sets.\n\n    First, orders by the column 'click_time'. Set the size of the \n    validation and test sets with the valid_fraction keyword argument.\n    \"\"\"\n\n    dataframe = dataframe.sort_values('click_time')\n    valid_rows = int(len(dataframe) * valid_fraction)\n    train = dataframe[:-valid_rows * 2]\n\n    valid = dataframe[-valid_rows * 2:-valid_rows]\n    test = dataframe[-valid_rows:]\n    \n    return train, valid, test\n\ndef train_model(train, valid, test=None, feature_cols=None):\n    if feature_cols is None:\n        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n                                           'is_attributed'])\n    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n    \n    param = {'num_leaves': 64, 'objective': 'binary', \n             'metric': 'auc', 'seed': 7}\n    num_round = 1000\n    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n                    early_stopping_rounds=20, verbose_eval=False)\n    \n    valid_pred = bst.predict(valid[feature_cols])\n    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n    print(f\"Validation AUC score: {valid_score}\")\n    \n    if test is not None: \n        test_pred = bst.predict(test[feature_cols])\n        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n        return bst, valid_score, test_score\n    else:\n        return bst, valid_score","544e8692":"print(\"Baseline model\")\ntrain, valid, test = get_data_splits(clicks)\n_ = train_model(train, valid)","4296e58a":"import category_encoders as ce\n\ncat_features = ['ip', 'app', 'device', 'os', 'channel']\ntrain, valid, test = get_data_splits(clicks)","57f942c2":"import category_encoders as ce\n\ncount_enc = ce.CountEncoder(cols=cat_features)\n\ncount_enc.fit(train[cat_features])\n\ntrain_encoded = train.join(count_enc.transform(train[cat_features]).add_suffix(\"_count\"))\nvalid_encoded = valid.join(count_enc.transform(valid[cat_features]).add_suffix(\"_count\"))","b22f07a1":"_ = train_model(train_encoded, valid_encoded)","2c96de1d":"import category_encoders as ce\n\n\ntarget_enc = ce.TargetEncoder(cols = cat_features)\n\ntarget_enc.fit(train[cat_features], train['is_attributed'])\n\ntrain_encoded = train.join(target_enc.transform(train[cat_features]).add_suffix(\"_target\"))\n\nvalid_encoded = valid.join(target_enc.transform(valid[cat_features]).add_suffix(\"_target\"))","b0eaaaf6":"# train_encoded = train_encoded.drop(['ip_target'],axis = 1)\n# valid_encoded = valid_encoded.drop(['ip_target'],axis = 1)\n_ = train_model(train_encoded, valid_encoded)","2ef33eed":"\ncat_features = ['app', 'device', 'os', 'channel']\n\ncb_enc = ce.CatBoostEncoder(cols=cat_features, random_state=7)\n\ncb_enc.fit(train[cat_features], train['is_attributed'])\n\ntrain_encoded = train.join(cb_enc.transform(train[cat_features]).add_suffix('_cb'))\nvalid_encoded = valid.join(cb_enc.transform(valid[cat_features]).add_suffix('_cb'))","08dec80f":"_ = train_model(train_encoded, valid_encoded)","7c75905e":"import itertools\n\ncat_features = ['ip', 'app', 'device', 'os', 'channel']\ninteractions = pd.DataFrame(index=clicks.index)\n\n# Iterate through each pair of features, combine them into interaction features\n\nfor c1, c2 in itertools.combinations(cat_features,2):\n    new_col = '_'.join([c1,c2])\n    values = clicks[c1].map(str) + \"_\" + clicks[c2].map(str)\n    encoder = preprocessing.LabelEncoder()\n    interactions[new_col] = encoder.fit_transform(values)","10428808":"clicks = clicks.join(interactions)\nprint(\"Score with interactions\")\ntrain, valid, test = get_data_splits(clicks)\n_ = train_model(train, valid)","36dc87de":"interaction_features = ['ip_app','ip_device', 'ip_os', 'ip_channel', 'app_device', 'app_os','app_channel', 'device_os', \n                        'device_channel', 'os_channel']\nclicks[interaction_features].head()","4a9917bc":"def count_past_events(series):\n    activity = pd.Series(series.index, index = series, name=\"past_6_hours\").sort_index()\n    past_6_hours = activity.rolling('6H').count() - 1\n    return past_6_hours","99f32b99":"past_events = pd.read_parquet('..\/input\/feature-engineering-data\/past_6hr_events.pqt')\nclicks['ip_past_6hr_counts'] = past_events\n\ntrain, valid, test = get_data_splits(clicks)\n_ = train_model(train, valid)","a8568f7a":"import matplotlib.pyplot as plt\n\nplt.plot(clicks.ip_past_6hr_counts[6:]);\nplt.title(\"Activity of ip within 6 hrs\");","b28a81a5":"def time_diff(series):\n    \"\"\"Returns a series with the time since the last timestamp in seconds.\"\"\"\n    return series.diff().dt.total_seconds()","741d30e0":"past_events = pd.read_parquet('..\/input\/feature-engineering-data\/time_deltas.pqt')\nclicks['past_events_6hr'] = past_events\n\ntrain, valid, test = get_data_splits(clicks.join(past_events))\n_ = train_model(train, valid)","88eea82d":"def previous_attributions(series):\n    \"\"\"Returns a series with the number of times an app has been downloaded.\"\"\"\n    sums = series.expanding(min_periods=2).sum() - series\n    return sums","04d4b07f":"past_events = pd.read_parquet('..\/input\/feature-engineering-data\/downloads.pqt')\nclicks['ip_past_6hr_counts'] = past_events\n\ntrain, valid, test = get_data_splits(clicks)\n_ = train_model(train, valid)","4fe836cd":"test = pd.read_csv('..\/input\/talkingdata-adtracking-fraud-detection\/test.csv',\n                         parse_dates=['click_time'])\ntest.head()","0c96d3ae":"# feature_cols = ['day', 'hour', 'minute', 'second', \n#                 'ip_labels', 'app_labels', 'device_labels',\n#                 'os_labels', 'channel_labels']\n\ntest_x = test.copy()\ntest_x['day'] = test_x['click_time'].dt.day.astype('uint8')\ntest_x['hour'] = test_x['click_time'].dt.hour.astype('uint8')\ntest_x['minute'] = test_x['click_time'].dt.minute.astype('uint8')\ntest_x['second'] = test_x['click_time'].dt.second.astype('uint8')","be8a45f9":"from sklearn import preprocessing\n\ncat_features = ['ip', 'app', 'device', 'os', 'channel']\nlable_encoder = preprocessing.LabelEncoder()\n\nfor feature in cat_features:\n    encoded = lable_encoder.fit_transform(test_x[feature])\n    test_x[feature +'_labels'] = encoded","d49f496d":"test_x.head()","6a82fbee":"test_x = test_x.drop([\"click_time\",\"ip\",\"channel\",\"click_id\",\"app\",\"device\",\"os\"], axis = 1)\ntest_x.head()","52267f02":"predictions = bst.predict(test_x)\nprint(predictions.shape)\n\npredict = np.array(predictions)\npredict = np.around(predict,decimals = 0)","f9b84213":"data = {\n    \"click_id\": test.click_id,\n    \"is_attributed\": predict\n}\nsub = pd.DataFrame(data = data)\nsub['is_attributed'] = sub['is_attributed'].astype(int)\n# sub.to_csv(\"submission.csv\",index = False)\nsub.head()","9b325351":"### Validation Data","dddf0b39":"## References:\n\n1. [Feature Engineering Kaggle Course](https:\/\/www.kaggle.com\/learn\/feature-engineering)\n\n2. [Wronsinski Github Blog on Categorical Encoding](https:\/\/wrosinski.github.io\/fe_categorical_encoding\/)\n\n3. [Talking Data ADtracking fraud detection Competition](https:\/\/www.kaggle.com\/c\/talkingdata-adtracking-fraud-detection)\n\n4. [Feature Engineering Data](https:\/\/www.kaggle.com\/matleonard\/feature-engineering-data)\n\n5. [Light GBM Documentation](https:\/\/lightgbm.readthedocs.io\/en\/latest)\n","dc4af13c":"**This is just a basic model which achieved score of 0.703**\n- Alot can be done with it and there is a huge scope of improvement which will come up future versions.\n","df4a2fbd":"- As this dataset is huge calculating activity of each ip for past 6 hours can be a lot time consuming step.\n- So I have used the dataset provided by [Matt Leonard](https:\/\/www.kaggle.com\/matleonard) for these files.\n\n- Credits: [feature engineering data](https:\/\/www.kaggle.com\/matleonard\/feature-engineering-data)","6a79f534":"## Preparing Test Data.","a6b87926":"## [Feature Generation.](https:\/\/www.kaggle.com\/matleonard\/feature-generation)\n\n### 1. Interactions\n\n### 2. Last Activity \n\n### 3. Time since last activity\n\n### 4. Past downloads","b567738c":"## Encoding the categorical features: \n\n- As the data is split into train and validation set encoding will not overestimate the model's performance that means there will be not any kind of Data leakages.\n\n- All of them will be executed using [category_encoders package](https:\/\/github.com\/scikit-learn-contrib\/category_encoders).\n\n### 1. Count Encoding\n\n### 2. Target Encoding\n\n### 3. CatBoost Encoding.","49a1d09b":"### Target Variable Analysis.","76b01c54":"## 1. Interactions:\n\n- As the name suggest interactions is a method of combining two features in order to up the performance of the model.\n\n- What we have done here is we have combine the categorical features together to obtain some new features.\n\n- If you wanna go deeper into this I found this article helpful: [Interactions by christophm](https:\/\/christophm.github.io\/interpretable-ml-book\/interaction.html)\n\n- The features we will create are: \n\n`interaction_features = ['ip_app','ip_device', 'ip_os', 'ip_channel', 'app_device', \n'app_os','app_channel', 'device_os','device_channel', 'os_channel']`","34a20453":"## Time since last activity","0471768d":"### Modelling.","c1364770":"## Count Encoding:\n\n- The concept is quite simple, it's basically **replacing categorical variables by it's count.**\n- Count Encoding just requires the categorical variables when instantiated.\n\n\n![count-encode.png](attachment:count-encode.png)\n\nImage Credits: [Socrates Data Science Blog](https:\/\/blog.socratesk.com\/blog\/2018\/06\/17\/featuren-engineering-and-extraction)","dac398e2":"### Test Data","e56d0ca3":"**This is the end of this kernel. Feel free to fork and if you found this useful do upvote.**","2b1d1fb2":"## [Label Encoding:](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html)\n\n- Giving labels to categorical features.","67439ef8":"### Missing Values.","b1930c37":"## Introduction\n\n- This kernel is dedicated to implementing various feature engineering methodologies which is quite essential in Applied machine learning. The main objective is to maximize the score and achive higher rank using various techniques.\n\n- This kernel is referenced by [Feature Engineering Kaggle Course](https:\/\/www.kaggle.com\/learn\/feature-engineering) and few other blog posts which are mentioned in the end.\n\n- The data used in this notebook is provided in the [Talking Data ADtracking fraud detection Competition](https:\/\/www.kaggle.com\/c\/talkingdata-adtracking-fraud-detection)\n\n### Feature Engineering:\n\n![fe.jpg](attachment:fe.jpg)\n\n\n### Data Description\n\n- `ip:` ip address of click.\n\n- `app:` app id for marketing.\n\n- `device:` device type id of user mobile phone (e.g., iphone 6 plus, iphone 7, huawei mate 7, etc.)\n\n- `os:` os version id of user mobile phone\n\n- `channel:` channel id of mobile ad publisher\n\n- `click_time:` timestamp of click (UTC)\n\n- `attributed_time:` if user download the app for after clicking an ad, this is the time of the app download\n\n- **`is_attributed:`** the target that is to be predicted, indicating the app was downloaded **[target Variable]**\n\nTest data is similar, with the following differences:\n\n- `click_id:` reference for making predictions\n\n- `is_attributed:` not included\n\n### Content:\n\n- Data Preparation\n\n- Extracting features of timmestamp.\n\n- Label Encoding\n\n- Light GBM Modelling\n\n- Encoding categorical features:\n   \n   `1. Count Encoding`\n    \n   `2. Target Encoding`\n   \n   `3. CatBoost Encoding`\n    \n- Feature Generation\n    \n    `1. Interactions`\n    \n    `2. Last Activity` \n    \n    `3. Time since last activity`\n    \n    `4. Past downloads`\n\n- Test Data Preparation\n\n- Submissions.\n","987c3251":"## Target Encoding:\n\n-  It is based on encoding categorical variable values with mean of **target variable per value.**\n- Target Encoding requires a target variable when instantiated.\n- In this case feature **\"is_attributed\"** is the target variable.\n\n- **NOTE: When using target variable, it is very important not to leak any information into the validation set(Data Leakage). Every such feature should be computed on the training set and then only merged or concatenated with the validation and test subsets.**\n\n![target-encode.png](attachment:target-encode.png)\n\nImage Credits: [Socrates Data Science Blog](https:\/\/blog.socratesk.com\/blog\/2018\/06\/17\/featuren-engineering-and-extraction)","b67c596a":"## Activity\n\n- This feature will give the number of events from the same IP in the last six hours. It's likely that someone who is visiting often will download the app.\n\n- The function count_past_events will execute the process using a random series.","28d5c314":"## Baseline Model:\n\n- The model used here is Light GBM(Gradient Boosting Method).\n\n#### **Why LightGBM**\n\n- Light GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks.\n\n- Since it is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. \n\n- So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms.\n\n- Also, it is surprisingly **very fast, hence the word \u2018Light\u2019.**\n\nSource: [Light GBM vs XGBoost](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/which-algorithm-takes-the-crown-light-gbm-vs-xgboost\/)","ddf6c994":"## CatBoost Encoding:\n\n","4f8a5c11":"## Number of previous Downloads","4d40ebbe":"### Let's Begin.","2e72f5e1":"### Training Data"}}