{"cell_type":{"1bb04d7d":"code","11353ffa":"code","9f160f5b":"code","4b179f0f":"code","41293b1c":"code","5e606aaf":"code","2deb7488":"code","bb8fab4a":"code","fba8e0d0":"code","8d239f23":"code","91016a12":"code","bc4c1a5c":"code","edca6565":"code","f74948e6":"code","7ec881fb":"code","d9a6201a":"markdown","d1bc8a48":"markdown","1e048d7d":"markdown","ed3a8aef":"markdown","24281fa2":"markdown","374f59f9":"markdown","837783c9":"markdown","f4e08f03":"markdown","0fe00f80":"markdown","e17b19ff":"markdown","60f845c2":"markdown","61615eca":"markdown","2e2fb0c1":"markdown","859a543b":"markdown","9bf49980":"markdown","47bccecb":"markdown","fa9398ba":"markdown","d5b2db28":"markdown","9fea781f":"markdown","4df20343":"markdown","d94e1f27":"markdown","0b14a6e5":"markdown","f48e4563":"markdown","4cda52f0":"markdown"},"source":{"1bb04d7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n'''\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n%config Completer.use_jedi = False","11353ffa":"# Import module and define global variables\nimport re\nimport os\nimport math\nimport pandas as pd\nimport random\nfrom tqdm import tqdm\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\nPUNCTUATION = \"~!@#$%^&*()_+`{}|\\[\\]\\:\\\";\\-\\\\\\='<>?,.\/\"\nSTEMMER = PorterStemmer()","9f160f5b":"# test cell\nfrom sklearn.naive_bayes import GaussianNB","4b179f0f":"# Build the vocabulary base with dictionary data sturcture.\n\"\"\"\nFollow the steps below:\n- Read all the documents\n- Remove punctuation\n- Divide the words with spaces\n- Filter stop words\n- Implement word stemming\n- Calculate the number of occurrences of each word\nNotice:\n- Make sure each word is lowercase before calculate\n\"\"\"\ndef is_stop_word(word):\n    return word in  stopwords.words('english')\n\ndef stem_word(word):\n    return STEMMER.stem(word)\n\ndef filter_line(line):\n    return re.sub(r'[{}]+'.format(PUNCTUATION), ' ', line.strip('\\n')).strip().lower()\n\ndef walk_load_files(path):\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n             with open(os.path.join(dirname, filename),errors='ignore') as f:\n                yield filename,f\n                \ndef walk_filter_lines(path):\n    for _,f in walk_load_files(path):\n        for line in f.readlines():\n            yield filter_line(line)\n\ndef build_vocabulary_base(base_dir='\/kaggle\/input\/emailtexts\/data\/emailtexts'):\n    vocabulary_base = {}\n    for line in walk_filter_lines(base_dir):\n        for i in line.split():\n            if is_stop_word(i):\n                continue\n            word = stem_word(i)\n            vocabulary_base[word] = vocabulary_base.get(word,0) + 1\n    return vocabulary_base\n\nvb = build_vocabulary_base()","41293b1c":"# compute the tf-idf value\n\"\"\"\nFollow the steps below:\n- Calculate the tf value of each word\n- Calculate the idf value of each word\n- Compute the tf-idf value\n\"\"\"\ndef cal_file_tf(vocabulary_base,filehandler):\n    res = {}\n    all_words = 0\n    # init res dict\n    for word,_ in vocabulary_base.items():\n        res[word] = 0\n    for line in filehandler:\n        for word in filter_line(line).split():\n            all_words += 1\n            if is_stop_word(word):\n                continue\n            word = stem_word(word)\n            res[word] += 1\n    # compute tf\n    for word,count in res.items():\n        res[word] = count\/all_words\n    return res\n                \ndef tf(vocabulary_base,base_dir):\n    tf_dic = {}\n    for filename,f in walk_load_files(base_dir):\n        tf_dic[filename] = cal_file_tf(vocabulary_base,f)\n    # tf_df = pd.DataFrame(tf_dic)\n    return tf_dic\n\ndef idf(vocabulary_base,tf_dic):\n    idf_dic = {}\n    for word,_ in vocabulary_base.items():\n        count = 0\n        file_num = 0\n        for _,tf_v in tf_dic.items():\n            file_num += 1\n            if tf_v[word] == 0:\n                continue\n            count += 1\n        if not count:print(word)\n        idf_dic[word] = math.log((1+file_num)\/count)\n    return idf_dic\n\ndef tf_idf(vocabulary_base,base_dir='\/kaggle\/input\/emailtexts\/data\/emailtexts'):\n    tf_dic = tf(vocabulary_base,base_dir)\n    idf_dic = idf(vocabulary_base,tf_dic)\n    tf_idf_dic = {}\n    for filename,tf_vec in tf_dic.items():\n        value = {}\n        for word,_ in vocabulary_base.items():\n            value[word] = tf_vec[word]*idf_dic[word]\n        tf_idf_dic[filename] = value\n    tfidf_df = pd.DataFrame(tf_idf_dic).T\n    return tfidf_df,tf_idf_dic\n    \ntfidf_df,tf_idf_dic = tf_idf(vocabulary_base=vb,base_dir='\/kaggle\/input\/emailtexts\/data\/emailtexts')\ntfidf_df","5e606aaf":"\"\"\"\nWe use the filename as docID.\nUse the hash value of the vocabulary as the termID.\nAnd the tf-idf value as weight\nFilter zero-weight term\nStore this three-element tuples through lists\n\"\"\"\ndef gen_index(score_dic):\n    index = []\n    for filename,words_score in score_dic.items():\n        for word,weight in words_score.items():\n            if weight == 0:\n                continue\n            index.append((filename,hash(word),weight))\n    return index\n\nporder_index = gen_index(tf_idf_dic)\nporder_index = sorted(porder_index,key=lambda x:x[1])\nporder_index[:10]","2deb7488":"def gen_inverted_index(porder_index):\n    inverted_index = []\n    for i in porder_index:\n        inverted_index.append((i[1],i[0],i[2]))\n    return inverted_index\n\ninverted_index = gen_inverted_index(porder_index)\ninverted_index[:10]","bb8fab4a":"# boolean model to retrive keywords \"home run\" \"ESPN hockey\"\ndef retrieval_single_by_boolean(keyword, inverted_index):\n    keyword = keyword.lower()\n    keyhash = hash(keyword)\n    res = []\n    for i in inverted_index:\n        if i[0] == keyhash:\n            res.append(i)\n    return sorted(res,key=lambda x:x[-1])[::-1]\n\ndef retrieval_by_boolean(keywords, inverted_index, type_=\"conjunction\",limit=None):\n    res = []\n    temp = []\n    for keyword in keywords.split():\n        temp.append([x[1] for x in retrieval_single_by_boolean(keyword, inverted_index)])\n    if type_ == \"conjunction\":\n        check_times = len(temp)\n        res_dic = {}\n        for res_list in temp:\n            for doc in res_list:\n                res_dic[doc] = res_dic.get(doc,0) + 1\n        for doc,times in res_dic.items():\n            if times == check_times:\n                res.append(doc)\n    elif type_ == \"disjunction\":\n        for res_list in temp:\n            for doc in res_list:\n                if doc not in res:\n                    res.append()\n    if limit:\n        return res[:limit]\n    return res\n\nretrieval_by_boolean(\"home run\",inverted_index,limit=100)\nretrieval_by_boolean(\"ESPN hockey\",inverted_index,limit=100)","fba8e0d0":"# vector space model to retrive keywords \"home run\" \"ESPN hockey\"\ndef load_doc_info(query, doc_path):\n    doc_info = {'doc':{}} # struct: N -> total number, avdl -> average document length, doc -> {dl -> doc length, tf -> {query_item_frequency} }\n    doc_count = 0\n    total_word = 0\n    query_dic = {}\n    for i in query.split(\" \"):\n        query_dic[i] = 0\n    for dirname, _, filenames in os.walk(doc_path):\n        for filename in filenames:\n            doc_count += 1\n            query_item = query_dic.copy()\n            file_path = os.path.join(dirname, filename)\n            with open(file_path,errors='ignore') as f:\n                word_count = 0\n                for word in ' '.join(map(filter_line,f.readlines())).split():\n                    if is_stop_word(word):\n                        continue\n                    for k,v in query_item.items():\n                        if k == word:\n                            query_item[k] = v+1\n                            break\n                    word_count += 1\n                doc_info[\"doc\"][filename] = {'filepath':file_path,'dl':word_count,'tf':query_item}\n                total_word += word_count\n    doc_info[\"avdl\"] = total_word\/doc_count\n    doc_info['N'] = doc_count\n    return doc_info\n            \n\ndef retrieval_doc_by_vsm(query, doc, doc_info, s=0.2):\n    res = 0\n    N = doc_info['N']\n    avdl = doc_info[\"avdl\"]\n    dl = doc_info['doc'][doc][\"dl\"]\n    for t in query.split(\" \"):\n        temp = 0\n        qtf = query.split(\" \").count(t)\n        tf = doc_info['doc'][doc][\"tf\"][t]\n        if tf == 0: # if this item not apparence in this doc, just skip it\n            continue\n        df = 0\n        for v in doc_info['doc'].values():\n            if v[\"tf\"][t] != 0:\n                df += 1\n        temp = qtf * math.log((N+1)\/df) * (1+math.log(1+math.log(tf)))\/((1-s)+s*dl\/avdl)\n    res += temp\n    return res\n\ndef retrieval_by_vsm(query, doc_path = '\/kaggle\/input\/emailtexts\/data\/emailtexts',limit=None):\n    query = query.lower()\n    res = []\n    doc_info = load_doc_info(query, doc_path)\n    for doc in doc_info['doc'].keys():\n        res.append((doc,retrieval_doc_by_vsm(query, doc, doc_info, s=0.2)))\n    res = sorted(res,key=lambda x:x[1])[::-1]\n    if limit:\n        return res[:limit] \n    return res\n\nretrieval_by_vsm(\"home run\",limit=100)\nretrieval_by_vsm(\"ESPN hockey\",limit=100)","8d239f23":"# BM25 model to retrive keywords \"home run\" \"ESPN hockey\"\ndef retrieval_doc_by_bm25(query, doc, doc_info,k1=2, k3=0, b=0.75):\n    res = 0\n    N = doc_info['N']\n    avdl = doc_info[\"avdl\"]\n    dl = doc_info['doc'][doc][\"dl\"]\n    for t in query.split(\" \"):\n        temp = 0\n        qtf = query.split(\" \").count(t)\n        tf = doc_info['doc'][doc][\"tf\"][t]\n        if tf == 0: # if this item not apparence in this doc, just skip it\n            continue\n        df = 0\n        for v in doc_info['doc'].values():\n            if v[\"tf\"][t] != 0:\n                df += 1\n        temp = math.log(N\/df)*((k3+1)*qtf\/(k3+qtf))*((k1+1)*tf\/(k1*((1-b)+b*dl\/avdl)+tf))\n    res += temp\n    return res\n\ndef retrieval_by_bm25(query, doc_path = '\/kaggle\/input\/emailtexts\/data\/emailtexts',limit=None):\n    query = query.lower()\n    res = []\n    doc_info = load_doc_info(query, doc_path)\n    for doc in doc_info['doc'].keys():\n        res.append((doc,retrieval_doc_by_bm25(query, doc, doc_info)))\n    res = sorted(res,key=lambda x:x[1])[::-1]\n    if limit:\n        return res[:limit] \n    return res\n\nretrieval_by_bm25(\"home run\",limit=100)\nretrieval_by_bm25(\"ESPN hockey\",limit=100)","91016a12":"# use tf-idf to preprocess the documents into numerical data.\nBASE_DIR4 = '\/kaggle\/input\/emailtexts\/data\/classification'\ntc_vb = build_vocabulary_base(base_dir=BASE_DIR4)\ntc_tfidf_df,tc_tf_idf_dic = tf_idf(vocabulary_base=tc_vb,base_dir=BASE_DIR4)\ntc_tfidf_df","bc4c1a5c":"# add label to tf-idf dataframe,0 for baseball and 1 for hockey\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import precision_score,recall_score,f1_score\n\n\ndef add_label(df,label_dir):\n    label = {}\n    for dirname, _, filenames in os.walk(label_dir):\n        for filename in filenames:\n            label[filename] = dirname.split('\/')[-1]\n    for i in df.index:\n        if label[i] == 'baseball':\n            df.loc[i, 'LABEL'] = 0\n        else:\n            df.loc[i, 'LABEL'] = 1\n    df[\"LABEL\"] = df[\"LABEL\"].astype(\"int\")\n    return df\n\nemail_dataset = add_label(tc_tfidf_df,BASE_DIR4)\nemail_dataset","edca6565":"# use Naive Bayes to classification the email.\ndef train_naive_bayes_model(data):\n    models = []\n    tests = []\n    kf = KFold(n_splits=5,shuffle=True)\n    for train_index,test_index in tqdm(kf.split(data)):\n        gnb = GaussianNB()\n        train = data.iloc[train_index]\n        test = data.iloc[test_index]\n        models.append(gnb.fit(train.iloc[:,:-1], train[\"LABEL\"]))\n        tests.append(test)\n    return models,tests\n\nmodels,tests = train_naive_bayes_model(email_dataset)","f74948e6":"# report result for the classification\n# precision, recall, and F1-measure of each fold and the average values.\n\ndef report_result(models,tests):\n    ps,rs,fs = [],[],[]\n    for i in range(len(models)):\n        y_true = tests[i][\"LABEL\"]\n        y_pred = models[i].predict(tests[i].iloc[:,:-1])\n        ps.append(precision_score(y_true,y_pred))\n        rs.append(recall_score(y_true,y_pred))\n        fs.append(f1_score(y_true,y_pred))\n        print(f\"Fold {i}: precision {ps[-1]}, recall {rs[-1]}, F1-measure {fs[-1]}.\")\n    print(f\"Avg: precision {np.mean(ps)}, recall {np.mean(rs)}, F1-measure {np.mean(fs)}.\")\n\nreport_result(models,tests)","7ec881fb":"# \u672c\u4ee3\u7801\u90e8\u5206\u5185\u5bb9\u53c2\u8003\u81ea https:\/\/zhuanlan.zhihu.com\/p\/157858995\n# \u672c\u4ee3\u7801\u90e8\u5206\u5185\u5bb9\u53c2\u8003\u81ea \n\ndef decision_function_output(i):\n    global m,b\n    return np.sum([alpha[j] * target[j] * kernel(point[j], point[i]) for j in range(m)]) - b\n\ndef svm_output(alphas, target, kernel, X_train, x_test, b):\n    result = (alphas * target) @ kernel(X_train, x_test) - b\n    return result\n\ndef linear_kernel(x,y,b=1):\n    #\u7ebf\u6027\u6838\u51fd\u6570\n    result = x @ y.T + b\n    return result\n\ndef gaussian_kernel(x,y, sigma=1):\n    #\u9ad8\u65af\u6838\u51fd\u6570\n    if np.ndim(x) == 1 and np.ndim(y) == 1:\n        result = np.exp(-(np.linalg.norm(x-y,2))**2\/(2*sigma**2))\n    elif(np.ndim(x)>1 and np.ndim(y) == 1) or (np.ndim(x) == 1 and np.ndim(y)>1):\n        result = np.exp(-(np.linalg.norm(x-y, 2, axis=1)**2)\/(2*sigma**2))\n    elif np.ndim(x) > 1 and np.ndim(y) > 1 :\n        result = np.exp(-(np.linalg.norm(x[:, np.newaxis]- y[np.newaxis, :], 2, axis = 2) ** 2)\/(2*sigma**2))\n    return result\n\ndef get_error(i1):\n    if 0< alpha[i1] < C:\n        return errors[i1]\n    else:\n        return decision_function_output(i1) - target[i1]\n\nC = 20\nb = 0\ntarget = email_dataset.iloc[500:1500,:][\"LABEL\"]\ntarget[target == 0] = -1\ntarget = np.array(target)\npoint = np.array(email_dataset.iloc[500:1500,400:1400])\nm,n = np.shape(point)\ntol = 0.01\neps = 0.01\nalpha = [0 for _ in range(len(point))]\nkernel = linear_kernel\nerrors = svm_output(alpha, target, kernel, point, point, b) - target\n\n\ndef takeStep(i1,i2):\n    global b\n    if i1 == i2:\n        return 0\n    alph1 = alpha[i1]\n    y1 = target[i1]\n    E1 = get_error(i1)\n    alph2 = alpha[i2]\n    y2 = target[i2]\n    E2 = get_error(i2)\n    s = y1*y2\n    # Compute L, H\n    if(y1 != y2):   \n        L = max(0, alph2 - alph1)\n        H = min(C, C + alph2 - alph1)\n    elif (y1 == y2):\n        L = max(0, alph1+alph2 - C)\n        H = min(C, alph1 + alph2)\n    if L == H:\n        return 0\n    k11 = kernel(point[i1],point[i1])\n    k12 = kernel(point[i1],point[i2])\n    k22 = kernel(point[i2],point[i2])\n    eta = 2*k12-k11-k22\n    if eta < 0:\n        a2 = alph2 - y2*(E1-E2)\/eta\n        if a2 < L:\n            a2 = L\n        elif a2 > H:\n            a2 = H\n    else:\n        f1 = y1*(E1 + b) - alph1*k11 - s*alph2*k12\n        f2 = y2 * (E2 + b) - s* alph1 * k12 - alph2 * k22\n        L1 = alph1 + s*(alph2 - L)\n        H1 = alph1 + s*(alph2 - H)\n        Lobj = L1 * f1 + L * f2 + 0.5 * (L1 ** 2) * k11 + 0.5 * (L**2) * k22 + s * L * L1 * k12\n        Hobj = H1 * f1 + H * f2 + 0.5 * (H1**2) * k11 + 0.5 * (H**2) * k22 + s * H * H1 * k12\n\n        if Lobj < Hobj - eps:\n            a2 = H\n        elif Lobj > Hobj + eps:\n            a2 = L\n        else:\n            a2 = alph2\n\n    if a2 <1e-8:\n        a2 = 0.0\n    elif a2 > (C - 1e-8):\n        a2 = C\n\n    if (np.abs(a2 - alph2) < eps * (a2 + alph2 + eps)):\n        return 0\n\n    a1 = alph1 + s * (alph2 - a2)\n\n    #\u66f4\u65b0 bias b\u7684\u503c Equation (J20)\n    b1 = E1 + y1*(a1 - alph1) * k11 + y2 * (a2 - alph2) * k12 + b\n    #equation (J21)\n    b2 = E2 + y1*(a1 - alph1) * k12 + y2 * (a2 - alph2) * k22 + b\n    # Set new threshoold based on if a1 or a2 is bound by L and\/or H\n    if 0 < a1 and a1 < C:\n        b_new =b1\n    elif 0 < a2 and a2 < C:\n        b_new = b2\n    #Average thresholds if both are bound\n    else:\n        b_new = (b1 + b2) * 0.5\n    #update model threshold\n    b = b_new\n\n    #\u4f18\u5316\u5b8c\u4e86\uff0c\u66f4\u65b0\u5dee\u503c\u77e9\u9635\u7684\u5bf9\u5e94\u503c\n    #\u540c\u65f6\u66f4\u65b0\u5dee\u503c\u77e9\u9635\u5176\u5b83\u503c\n    errors[i1] = 0\n    errors[i2] = 0\n    #\u66f4\u65b0\u5dee\u503c Equation (12)\n    for i in range(m):\n        if 0 < alpha[i] < C:\n            errors[i] += y1*(a1 - alph1)*kernel(point[i1],point[i]) + y2*(a2 - alph2)*kernel(point[i2], point[i]) + b - b_new\n    alpha[i1] = a1\n    alpha[i2] = a2\n    return 1\n\n\n\ndef examineExample(i2):\n    y2 = target[i2]\n    alph2 = alpha[i2]\n    E2 = get_error(i2)\n    r2 = E2*y2\n    if (r2 < -tol and alph2 < C) or (r2 > tol and alph2 > 0):\n        if (len(alpha) - alpha.count(0) - alpha.count(C) > 1):\n            if errors[i2] > 0:\n                i1 = np.argmin(errors)\n            elif errors[i2] <= 0:\n                i1 = np.argmax(errors)\n            if takeStep(i1,i2):\n                return 1\n        rand = random.randint(0,len(alpha))\n        for i in range(len(alpha)):\n            if alpha[(i+rand)%len(alpha)] == 0 or alpha[(i+rand)%len(alpha)] == C:\n                continue\n            if takeStep((i+rand)%len(alpha),i2):\n                return 1\n        rand = random.randint(0,len(alpha))\n        for i in range(len(alpha)):\n            i1 = (i+rand)%len(alpha)\n            if takeStep(i1,i2):\n                return 1\n    return 0\n\ndef train():\n    numChanged = 0\n    examineAll = 1\n    loopnum = 0\n    totaloop = 100\n    while (numChanged > 0 or examineAll):\n        numChanged = 0\n        if loopnum == totaloop:\n           break\n        else:\n            loopnum += 1\n            print(f\"\\rLoops:{loopnum}\/{totaloop}\",end=\"\")\n        if examineAll:\n            for i in range(len(point)):\n                numChanged != examineExample(i)\n        else:\n            for i in range(len(alpha)):\n                if alpha[i] != 0 or alpha[i] != C:\n                    numChanged += examineExample(i)\n        if examineAll == 1:\n            examineAll = 1\n        elif numChanged == 0:\n            examineAll = 1\n    print(\" Finish!\")\n    return -1\n\ntrain()\n\noutput = svm_output(alpha, target, kernel, point, point, b)\nres = []\nfor i in output:\n    if i > 0:\n        res.append(1)\n    else:\n        res.append(-1)\nprint(target)\nprint(res)\n\nprint(precision_score(target,res))\nprint(recall_score(target,res))\nprint(f1_score(target,res))","d9a6201a":"# 3 Ranking\nGiven two queries: \u201chome run\u201d, \u201cESPN hockey\u201d, return the top 100 documents for each query based on:","d1bc8a48":"\u4f7f\u7528\u5411\u91cf\u6a21\u578b\u8fdb\u884c\u641c\u7d22\uff0c\u5c06\u641c\u7d22\u5185\u5bb9\u548c\u6587\u6863\u5747\u4ee5\u5411\u91cf\u7684\u5f62\u5f0f\u8868\u793a\uff0c\u901a\u8fc7\u8861\u91cf2\u4e2a\u5411\u91cf\u4e4b\u95f4\u8ddd\u79bb\u7684\u65b9\u5f0f\u6765\u8861\u91cf\u6587\u6863\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u5f97\u51fa\u641c\u7d22\u7ed3\u679c\u3002","1e048d7d":"\u5bf9\u8bed\u6599\u5e93\u4e2d\u7684\u5355\u8bcd\uff0c\u8ba1\u7b97\u5176tf-idf\u503c","ed3a8aef":"\u4f7f\u7528\u7d20\u6734\u8d1d\u53f6\u65af\u8fdb\u884c\u5206\u7c7b\u5e76\u8fdb\u884c5\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u53ef\u4ee5\u770b\u5230\u6240\u6709\u7ed3\u679c\u5747\u5728 95% \u5de6\u53f3\uff0c\u5176\u5206\u7c7b\u6548\u679c\u4e4b\u597d\u4e0d\u8a00\u800c\u55bb\u3002","24281fa2":"## ii) vector space model.","374f59f9":"\u5bf9\u6570\u636e\u8fdb\u884c\u9006\u7d22\u5f15\uff0c\u4e5f\u5c31\u662f\u5c06\u8bcd\u6c47\u54c8\u5e0c\u4e0e\u6587\u6863ID\u7684\u4f4d\u7f6e\u8fdb\u884c\u8c03\u6362\uff0c\u65b9\u4fbf\u6309\u8bcd\u8fdb\u884c\u641c\u7d22","837783c9":"## ii) compute the tf-idf value for each word in the vocabulary","f4e08f03":"\u6309\u8001\u5e08\u7ed9\u7684\u53c2\u8003\u8d44\u6599\u548c\u7b97\u6cd5\u539f\u8bba\u6587\u5b9e\u73b0SMO\u6a21\u578b\uff0c\u5176\u662f\u4e00\u79cdSVM\u7684\u4f18\u5316\u6a21\u578b\u3002\n\n\u5e76\u4f7f\u7528\u5176\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\uff0c\u53ef\u4ee5\u770b\u5230\u5176\u6709\u7740\u4e0d\u9519\u7684\u6548\u679c\uff0cprecision score \u62e5\u670975%\uff0c\u800c\u53ec\u56de\u7387\u66f4\u662f\u9ad8\u8fbe 100%\uff0cf1 score\u4e5f\u670983%","0fe00f80":"# 1 Text Processing\nGiven the email dataset emailtexts, process the texts according to the following steps.","e17b19ff":"\u4f7f\u7528 boolean \u6a21\u578b\u8fdb\u884c\u641c\u7d22\uff0c\u5176\u4e3b\u8981\u7684\u641c\u7d22\u6b65\u9aa4\u662f\u5c06\u641c\u7d22\u5185\u5bb9\u5206\u8bcd\uff0c\u5e76\u5206\u522b\u641c\u7d22\u6bcf\u4e2a\u8bcd\u51fa\u73b0\u7684\u6587\u7ae0\uff0c\u6700\u540e\u5c06\u6240\u6709\u6587\u7ae0\u6c47\u603b\u8d77\u6765\u3002\u8f93\u51fa\u5373\u4e3a\u641c\u7d22\u7ed3\u679c\uff0c\u4f46\u8f93\u51fa\u7684\u6587\u6863\u6ca1\u6709\u5148\u540e\u987a\u5e8f\u53ef\u8a00\u3002","60f845c2":"## i) Build the vocabulary base, remove stop words, and implement word stemming","61615eca":"## \u4e00\u4e9b\u5199\u5728\u6700\u540e\u4e00\u9898\u540e\u9762\u7684\u8bdd\n\u6211\u76f8\u4fe1\u8001\u5e08\u548c\u52a9\u6559\u770b\u6211\u6700\u540e\u4e00\u9898\u7684\u4ee3\u7801\u4e00\u5b9a\u5145\u6ee1\u7740\u7591\u95ee\uff0c\u5728\u6b64\u8bf4\u660e\u4e00\u4e9b\u60c5\u51b5\n\n1. \u4e3a\u4ec0\u4e48\u53ea\u9009\u7528 1000 \u4e2a\u6837\u672c\u70b9\n- \u57fa\u4e8e\u5185\u5b58\u548c\u8bad\u7ec3\u65f6\u95f4\u8003\u8651\uff0c\u53ea\u9009\u53d6\u4e86\u5176\u4e2d1000\u4e2a\u6837\u672c\u70b9\u8fdb\u884c\u8bad\u7ec3\n\n2. \u4e3a\u4ec0\u4e48\u53ea\u9009\u7528\u4e86 1000 \u4e2a\u7279\u5f81\n- \u57fa\u4e8e\u5185\u5b58\u548c\u8bad\u7ec3\u65f6\u95f4\u8003\u8651\uff0c\u53ea\u9009\u53d6\u4e86\u5176\u4e2d1000\u4e2a\u7279\u5f81\u8fdb\u884c\u8bad\u7ec3\n\n3. \u4e3a\u4ec0\u4e48\u53ea\u8fed\u4ee3\u4e86 100 \u8f6e\n- \u9996\u5148\u662f\u6839\u636e\u6700\u540e\u7684\u7ed3\u679c\uff0c100\u8f6e\u8fed\u4ee3\u4e5f\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u4e0d\u9519\u7684\u6548\u679c\n- \u8fed\u4ee3\u4e00\u8f6e\u6570\u636e\u8981\u82b1\u8d39\u7684\u65f6\u95f4\u8f83\u9ad8\uff08\u5927\u698220s\uff09\uff0c\u5982\u679c\u8fed\u4ee32000\u8f6e\u7684\u8bdd\u521d\u6b65\u4f30\u7b97\u65f6\u95f4\u4e3a 20\\*2000 = 40000s \u5927\u6982\u82b1\u8d3910\u591a\u4e2a\u5c0f\u65f6\u3002\n\n4. \u4e3a\u4ec0\u4e48\u6ca1\u6709\u4f7f\u7528\u9ad8\u65af\u6838\u51fd\u6570\n- \u4ee3\u7801\u4e2d\u5199\u4e86\u4e24\u79cd\u6838\u51fd\u6570\u7684\u4ee3\u7801\uff0c\u4f46\u5e76\u6ca1\u6709\u4f7f\u7528\u9ad8\u65af\u6838\u51fd\u6570\uff0c\u56e0\u4e3a\u5728\u4f7f\u7528\u7684\u8fc7\u7a0b\u4e2d\u53d1\u73b0\u9ad8\u65af\u51fd\u6570\u7684\u8ba1\u7b97\u9700\u8981\u5bf9\u77e9\u9635\u8fdb\u884c\u5347\u7ef4\uff0c\u5c06\u5bfc\u81f4\u5185\u5b58\u6210\u6307\u6570\u5f0f\u4e0a\u5347\u3002\u5168\u90e8\u6837\u672c\u8bad\u7ec3\u7ea6\u9700\u8981 500G \u5185\u5b58\u3002\n","2e2fb0c1":"\u5bf9\u6570\u636e\u8fdb\u884c\u7d22\u5f15\uff0c\u91c7\u53d6\u7684\u65b9\u5f0f\u662f\uff08\u6587\u6863ID\uff0c\u8bcd\u6c47\u54c8\u5e0c\uff0c\u8bcd\u6c47\u6743\u91cd\uff09\n\n\u5728\u6b64\u91c7\u7528tf-idf\u503c\u4f5c\u4e3a\u8bcd\u6c47\u7684\u6743\u91cd\uff0c\u5176\u53ef\u4ee5\u8f83\u597d\u7684\u53cd\u5e94\u6bcf\u4e2a\u8bcd\u6c47\u7684\u5728\u6587\u6863\u4e2d\u7684\u91cd\u8981\u7a0b\u5ea6","859a543b":"# 4 Text Classification\nWe have two categories of emails, in which one category is about hockey and the other is about baseball. The data is in the folder classification, the text file is indeed the email file in Question 1-3 with labels.","9bf49980":"# \u5199\u5728\u524d\u9762 \n\u611f\u8c22 \u5218\u51a0\u7537\u8001\u5e08 \u63d0\u4f9b\u7684\u9898\u76ee\u3001\u8d44\u6599\u4e0e\u6570\u636e\u96c6\n\n\u611f\u8c22 [kaggle](https:\/\/www.kaggle.com\/) \u4e3a\u672c\u6b21\u4f5c\u4e1a\u63d0\u4f9b\u4e91\u8ba1\u7b97\u5e73\u53f0\n\n\u8fd9\u6b21\u4f5c\u4e1a\u8ba9\u6211\u5bf9 \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u6709\u4e86\u4e00\u4e2a\u65b0\u7684\u8ba4\u77e5\uff0c\u5f00\u59cb\u9010\u6e10\u5bf9\u5176\u7684\u539f\u7406\u8fdb\u884c\u7406\u89e3\u3002\n\n\u4e5f\u7b97\u662f\u6211\u7b2c\u4e00\u6b21\u81ea\u5df1\u624b\u5199\u5b9e\u73b0\u4e00\u4e2a\u7b97\u6cd5\uff0c\u5176\u4e2d\u867d\u7136\u9047\u5230\u5f88\u591a\u5751\uff0c\u5f88\u591a\u81ea\u5df1\u4e0d\u4e86\u89e3\u7684\u77e5\u8bc6\uff0c\u4f46\u8fd9\u4e2a\u8fc7\u7a0b\u8ba9\u6211\u5bf9\u7b97\u6cd5\u4ea7\u751f\u4e86\u8f83\u4e3a\u6d53\u539a\u7684\u5174\u8da3\u3002\n\n","47bccecb":"## iii) BM25 model.","fa9398ba":"## a) Firstly preprocess the documents into numerical data, e.g., you can use tf-idf or word2vec.","d5b2db28":"## i) boolean model","9fea781f":"\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u9884\u5904\u7406\u4e4b\u540e\u7684\u751f\u6210tf-idf\u77e9\u9635\uff0c\u5e76\u7ed9\u8be5\u77e9\u9635\u52a0\u4e0a\u7c7b\u522b\u6807\u7b7e","4df20343":"## c) Implement Sequential Minimal Optimization (SMO) by following the introductive slides Reference-SMO and report the classification results .","d94e1f27":"\u5bf9\u8981\u8fdb\u884c\u7d22\u5f15\u7684\u6570\u636e\u5efa\u7acb\u5176\u8bed\u6599\u5e93\uff0c\u4e3b\u8981\u505a\u6cd5\u662f\u901a\u8fc7\u8bfb\u53d6\u6240\u6709\u6587\u4ef6\u91cc\u7684\u5185\u5bb9\uff0c\u5c06\u5176\u4e2d\u7684\u53e5\u5b50\u8fdb\u884c\u5206\u79bb\uff0c\u5e76\u8fdb\u884c\u5206\u8bcd\uff0c\u518d\u8fc7\u6ee4\u6389\u5e38\u7528\u7684\u8bcd\uff0c\u6700\u540e\u8fdb\u884c\u6c47\u603b\u53bb\u91cd\u3002\n\n\u5176\u4e2d\u6bd4\u8f83\u91cd\u8981\u7684\u51e0\u70b9\u4e3a\uff1a\n- \u7531\u4e8e\u8bcd\u6c47\u6709\u7740\u4e0d\u540c\u7684\u5927\u5c0f\u5199\uff0c\u4e3a\u4e86\u907f\u514d\u7531\u4e8e\u5f62\u5f0f\u4e0d\u540c\u800c\u5bfc\u81f4\u7684\u8bed\u6599\u5e93\u6570\u636e\u5197\u4f59\u7684\u60c5\u51b5\uff0c\u5728\u5408\u9002\u7684\u4f4d\u7f6e\u8981\u628a\u6570\u636e\u5168\u90e8\u8f6c\u6362\u6210\u5c0f\u5199\u3002\n- \u7531\u4e8e\u82f1\u8bed\u4e2d\u5355\u8bcd\u5b58\u5728\u4e0d\u540c\u7684\u5f62\u5f0f\uff0c\u5982\u539f\u578b\u3001\u8fc7\u53bb\u5206\u8bcd\u3001\u73b0\u5728\u5206\u8bcd\u7b49\u7b49\uff0c\u4e0d\u540c\u5f62\u5f0f\u4e0d\u5e94\u8be5\u88ab\u5f52\u7ed3\u4e3a\u4e0d\u540c\u8bcd\u6c47\uff0c\u56e0\u6b64\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u8bcd\u6839\u7684\u63d0\u53d6\u3002\n- Mac os \u4e2d\uff0c\u4f1a\u5bf9\u6587\u4ef6\u8fdb\u884c\u81ea\u52a8\u7d22\u5f15\u5e76\u751f\u6210 .\\_filename \u7684\u7d22\u5f15\u6587\u4ef6\uff0c\u4e3a\u907f\u514d\u53d7\u5230\u5e72\u6270\uff0c\u5728\u8bfb\u53d6\u6587\u4ef6\u65f6\u5e94\u8be5\u8fc7\u6ee4\u6389\u6b64\u7c7b\u6587\u4ef6\u3002","0b14a6e5":"# 2 Inverted Index\nTransfer all the texts into three-element tuples ( docID, termID, weight), order the tuples according to termID and construct the inverted index for each word in the vocabulary.","f48e4563":"use this formula\n\n$$\nRSV_d = \\sum_{t\\in q}log[\\frac{N}{df_t}]\\cdot\\frac{(k_1+1)tf_{td}}{k_1[(1-b)+b\\times(\\frac{L_d}{L_{ave}})]+tf_{td}}\\cdot\\frac{(k_3+1)tf_{tq}}{k_3+tf_{tq}}\n$$","4cda52f0":"## b) Use Naive Bayes to classify the documents and test the classification results with 5-fold cross validation. You should report the precision, recall, and F1-measure of each fold and the average values. (You can implement the algorithm by yourself.)"}}