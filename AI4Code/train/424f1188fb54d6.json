{"cell_type":{"6700d6ce":"code","c246579e":"code","c0ed4e7b":"code","f2750772":"code","0dabb7f7":"code","45f6d656":"code","19eebac9":"code","28fac48a":"code","3a60c046":"code","79c205b5":"code","3e8dd4aa":"code","669cd1eb":"code","1d55dabe":"code","29fe5c46":"markdown","150cdd50":"markdown","014dab2a":"markdown","0e790158":"markdown","55af048b":"markdown","2206768d":"markdown","ef894bea":"markdown","1d8fe41b":"markdown"},"source":{"6700d6ce":"!pip install -q frozendict > \/dev\/null","c246579e":"import numpy  as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nimport pydash\nimport math\nimport os\nimport itertools\n\nfrom pydash import flatten, flatten_deep\nfrom collections import Counter, OrderedDict\nfrom frozendict import frozendict\nfrom humanize import intcomma\nfrom operator import itemgetter\nfrom typing import *\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom itertools import product, combinations\nfrom joblib import Parallel, delayed","c0ed4e7b":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col=0)\ndf_test  = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', index_col=0)\ndf_train","f2750772":"print(nltk.corpus.stopwords.words('english'))","0dabb7f7":"def tokenize_df(\n    dfs: List[pd.DataFrame], \n    keys          = ('text', 'keyword', 'location'), \n    stemmer       = True, \n    preserve_case = True, \n    reduce_len    = False, \n    strip_handles = True,\n    use_stopwords = True,\n    **kwargs,\n) -> List[List[str]]:\n    # tokenizer = nltk.TweetTokenizer(preserve_case=True,  reduce_len=False, strip_handles=False)  # defaults \n    tokenizer = nltk.TweetTokenizer(preserve_case=preserve_case, reduce_len=reduce_len, strip_handles=strip_handles) \n    porter    = nltk.PorterStemmer()\n    stopwords = set(nltk.corpus.stopwords.words('english') + [ 'nan' ])\n\n    output    = []\n    for df in flatten([ dfs ]):\n        for index, row in df.iterrows():\n            tokens = flatten([\n                tokenizer.tokenize(str(row[key] or \"\"))\n                for key in keys    \n            ])\n            if use_stopwords:\n                tokens = [ \n                    token \n                    for token in tokens \n                    if token.lower() not in stopwords\n                    and len(token) >= 2\n                ]                \n            if stemmer:\n                tokens = [ \n                    porter.stem(token) \n                    for token in tokens \n                ]\n            output.append(tokens)\n\n    return output\n\n\ndef word_frequencies(df, **kwargs) -> Dict[int, Counter]:\n    tokens = {\n        0: flatten(tokenize_df( df[df['target'] == 0], **kwargs )),\n        1: flatten(tokenize_df( df[df['target'] == 1], **kwargs )),\n    }\n    freqs = { \n        target: Counter(dict(Counter(tokens[target]).most_common())) \n        for target in [0, 1]\n    }  # sort and cast\n    return freqs","45f6d656":"tokenize_df(df_train)[:2]","19eebac9":"freqs = word_frequencies(df_train)\nprint('freqs[0]', len(freqs[0]), freqs[0].most_common(10))\nprint('freqs[1]', len(freqs[1]), freqs[1].most_common(10))","28fac48a":"def inverse_document_frequency( tokens: List[str] ) -> Counter:\n    tokens = flatten_deep(tokens)\n    idf = {\n        token: math.log( len(tokens) \/ count ) \n        for token, count in Counter(tokens).items()\n    }\n    idf = Counter(dict(Counter(idf).most_common()))  # sort and cast\n    return idf\n\ndef inverse_document_frequency_df( dfs ) -> Counter:\n    tokens = flatten_deep([ tokenize_df(df) for df in flatten([ dfs ]) ])\n    return inverse_document_frequency(tokens)\n\nidf = inverse_document_frequency_df([ df_train, df_test ])\nlist(reversed(idf.most_common()))[:20]","3a60c046":"def extract_features(df, freqs, use_idf=True, use_log=True, **kwargs) -> np.array:\n    features = []\n    tokens   = tokenize_df(df, **kwargs)\n    for n in range(len(tokens)):\n        bias     = 1  # bias term is implict when using sklearn\n        positive = 1\n        negative = 1        \n        for token in tokens[n]:\n            if use_idf:\n                positive += freqs[0].get(token, 0) * idf.get(token, 1) \n                negative += freqs[1].get(token, 0) * idf.get(token, 1)\n            else:\n                positive += freqs[0].get(token, 0) \n                negative += freqs[1].get(token, 0) \n        features.append([ positive, negative ])  \n\n    features = np.array(features)   # accuracy = 0.7166688559043741\n    if use_log:\n        features = np.log(features) # accuracy = 0.7136477078681204\n    return features\n\n\nY_train = df_train['target'].to_numpy()\nX_train = extract_features(df_train, freqs)\nX_test  = extract_features(df_test,  freqs)\n\nprint('df_train', df_train.shape)\nprint('df_test ', df_test.shape)\nprint('Y_train ', Y_train.shape)\nprint('X_train ', X_train.shape)\nprint('X_test  ', X_test.shape)\nprint(X_test[:5])","79c205b5":"def predict_df(df_train, df_test, **kwargs):\n    freqs   = word_frequencies(df_train, **kwargs)\n\n    Y_train = df_train['target'].to_numpy()\n    X_train = extract_features(df_train, freqs, **kwargs)\n    X_test  = extract_features(df_test,  freqs, **kwargs) \\\n              if df_train is not df_test else X_train\n\n    model      = LinearRegression().fit(X_train, Y_train)\n    prediction = model.predict(X_test)\n    prediction = np.round(prediction).astype(np.int)\n    return prediction\n\n\ndef get_train_accuracy(splits=3, **kwargs):\n    \"\"\" K-Fold Split Accuracy \"\"\"\n    accuracy = 0.0\n    for _ in range(splits):\n        train, test = train_test_split(df_train, test_size=1\/splits)      \n        prediction  = predict_df(train, test, **kwargs)\n        Y_train     = test['target'].to_numpy()\n        accuracy   += np.sum( Y_train == prediction ) \/ len(Y_train) \/ splits    \n    return accuracy\n    \n    \ndef train_accuracy_hyperparameter_search():\n    results = Counter()\n    jobs    = []\n    \n    # NOTE: reducing input fields has no effect on accuracy\n    for keys in [ ('text', 'keyword', 'location'), ]: # ('text', 'keyword'), ('text',) ]:\n        strip_handles = 1  # no effect on accuracy \n        # use_log       = 1  # no effect on accuracy\n        for stemmer, preserve_case, reduce_len, use_stopwords, use_idf, use_log in product([1,0],[1,0],[1,0],[1,0],[1,0],[1,0]):\n            def fn(keys, stemmer, preserve_case, reduce_len, strip_handles, use_stopwords, use_idf, use_log):\n                kwargs = {\n                    \"stemmer\":        stemmer,          # stemmer = True is always better\n                    \"preserve_case\":  preserve_case, \n                    \"reduce_len\":     reduce_len, \n                    # \"strip_handles\": strip_handles,   # no effect on accuracy\n                    \"use_stopwords\":  use_stopwords,    # use_stopwords = True is always better\n                    \"use_idf\":        use_idf,          # use_idf = True is always better\n                    \"use_log\":        use_log,          # use_log = True is always better\n                }\n                label = frozendict({\n                    **kwargs,\n                    # \"keys\": keys,                     # no effect on accuracy\n                })\n                accuracy = get_train_accuracy(**kwargs)\n                return (label, accuracy)\n            \n            # hyperparameter search is slow, so multiprocess it\n            jobs.append( delayed(fn)(keys, stemmer, preserve_case, reduce_len, strip_handles, use_stopwords, use_idf, use_log) )\n            \n    results = Counter(dict( Parallel(-1)(jobs) ))\n    results = Counter(dict(results.most_common()))  # sort and cast\n    return results","3e8dd4aa":"%%time\nif True or os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') == 'Batch':\n    results = train_accuracy_hyperparameter_search()\n    for label, value in results.items():\n        print(f'{value:.5f} |', \"  \".join(f\"{k.split('_')[-1]} = {v}\" for k,v in label.items() ))  # pretty printdd","669cd1eb":"print('train_accuracy = ', get_train_accuracy())","1d55dabe":"df_submission = pd.DataFrame({\n    \"id\":     df_test.index,\n    \"target\": predict_df(df_train, df_test)\n})\ndf_submission.to_csv('submission.csv', index=False)\n! head submission.csv","29fe5c46":"# Hyperparameter Search\n\n\nThe optimal settings are:\n- stemmer = True\n- preserve_case = True\n- strip_handles = Any\n- use_stopwords = True\n\nThe above are exactly opposite compared to [TF-IDF Classifier](https:\/\/www.kaggle.com\/jamesmcguigan\/disaster-tweets-tf-idf-classifier?scriptVersionId=50898834), \nbut the following settings are shared:\n\n- use_idf = True\n- use_log = True","150cdd50":"# Submission\n\nWithout additional feature engineering, LinearRegression scores worst than my [TF-IDF Classifier](https:\/\/www.kaggle.com\/jamesmcguigan\/disaster-tweets-tf-idf-classifier?scriptVersionId=50898834)","014dab2a":"# NLP Logistic Regression\n\n[Disaster Tweets Dataset](https:\/\/www.kaggle.com\/c\/nlp-getting-started)\n\nThis notebook is inspired by Course 1, Week 1 of the [deeplearning.ai Natural Language Processing Specialization](https:\/\/www.deeplearning.ai\/natural-language-processing-specialization\/), but the code here is implemented using moden library functions rather than using hand-coded implementions.\n\nThe approach here is to tokenize the text, and create word frequencies tables for each of the labels. \nA Nx2 numeric feature matrix is created for each tweet, containing the sum of positive and negative frequencies for each word tokens in the tweet.\nLinear Regression solves the problem via gradient decent, and is trained to predict labels given an extracted feature matrix.","0e790158":"# Imports","55af048b":"# Feature Extraction\n\nHere we create a Nx2 feature matrix containing the sum of positive and negative word frequencies for each tweet","2206768d":"# Further Reading\n\nThis notebook is part of a series exploring Natural Language Processing\n- 0.74164 - [NLP Logistic Regression](https:\/\/www.kaggle.com\/jamesmcguigan\/disaster-tweets-logistic-regression\/)\n- 0.77536 - [NLP TF-IDF Classifier](https:\/\/www.kaggle.com\/jamesmcguigan\/disaster-tweets-tf-idf-classifier)\n- 0.79742 - [NLP Naive Bayes](https:\/\/www.kaggle.com\/jamesmcguigan\/nlp-naive-bayes)","ef894bea":"# Tokenization and Word Frequencies\n\nHere we tokenize the text using nltk.TweetTokenizer, apply lowercasing, tweet preprocessing, and stemming.\n\nThen compute a dictionary lookup of word counts for each label","1d8fe41b":"Verify train accuracy given default settings"}}