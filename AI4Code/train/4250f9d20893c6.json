{"cell_type":{"214abb29":"code","c1b51c12":"code","0666ca3a":"code","c7c9a922":"code","6e607eb8":"code","2abe0ee0":"code","e63aa58f":"code","75c1623a":"code","6db9dd3e":"code","17a81c05":"code","c898ca73":"code","cee69f27":"code","6fe43a5b":"code","688ae378":"code","a2a09503":"code","349e4dc7":"code","230680d0":"code","857466d4":"code","cb1e9116":"code","6eacd009":"code","8062ee45":"code","d1c5c0a9":"markdown","1d93b556":"markdown","7e2bfe9b":"markdown","b721365f":"markdown","89611aff":"markdown","7fd00aaa":"markdown"},"source":{"214abb29":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\npd.set_option('max_columns',50)","c1b51c12":"train = pd.read_csv('..\/input\/hse-practical-ml-1\/car_loan_train.csv')\ntest = pd.read_csv('..\/input\/hse-practical-ml-1\/car_loan_test.csv')\n\ntarget = train.target\ndel train['target']\n\nsubmission = pd.DataFrame()\nsubmission.loc[:, 'ID'] = test.index\nsubmission.loc[:, 'Predicted'] = 0","0666ca3a":"train.head()","c7c9a922":"def preproc_date(df, cols, remove=True):\n    for col in cols:\n        time = pd.to_datetime(df[col])\n        df.loc[:, col + '_year'] = [i.year if i.year<=2020 else i.year - 100 for i in time] # If \u2018unix\u2019 (or POSIX) time; origin is set to 1970-01-01.\n        df.loc[:, col + '_month'] = [i.month for i in time]\n        df.loc[:, col + '_day'] = [i.day for i in time]\n        if remove:\n            del df[col]\n    return df\n\ndef parse_date(df, cols, remove=True):\n    for col in cols:\n        time = df[col].str.split(' ')\n        df.loc[:, col + '_year'] = [int(i[0].split('yrs')[0]) for i in time]\n        df.loc[:, col + '_month'] = [int(i[1].split('mon')[0]) for i in time]\n        if remove:\n            del df[col]\n    return df\n\nclass PreprocText(object):\n    def __init__(self, columns):\n        self.columns = columns\n        self.res = {}\n        \n    def fit(self, X, y=None):\n        for col in self.columns:\n            encoder = LabelEncoder()\n            encoder.fit(X.loc[:, col].astype(str))\n            self.res[col] = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n            \n    def transform(self, X, y=None):\n        for col in self.columns:\n            X.loc[:, col] = X.loc[:, col].map(self.res[col])\n        return X\n    \n    def fit_transform(self, X, y=None):\n        self.fit(X)\n        return self.transform(X)\n            \ndef data_preproc(df):\n    df = preproc_date(df, ['Date.of.Birth', 'DisbursalDate'])\n    df = parse_date(df, ['AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH'])\n    return df\n\nclass DataPreproc(object):\n    def __init__(self):\n        pass\n        \n    def fit(self, X):\n        self.text_prep = PreprocText(columns=['Employment.Type', 'PERFORM_CNS.SCORE.DESCRIPTION'])\n        self.text_prep.fit(X)\n        \n            \n    def transform(self, X, y=None):\n        X = self.text_prep.transform(X)\n        X = data_preproc(X)\n        return X\n    \n    def fit_transform(self, X, y=None):\n        self.fit(X)\n        return self.transform(X)    ","6e607eb8":"%%time\n\nprep = DataPreproc()\ntrain = prep.fit_transform(train)\ntest = prep.transform(test)","2abe0ee0":"columns_for_del = []","e63aa58f":"columns_for_del.extend([i for i in train.columns if i.startswith('magic')])\ncolumns_for_del.extend([i for i in train.columns if i.startswith('f')])","75c1623a":"columns_for_del.extend(['UniqueID'])\n","6db9dd3e":"columns_for_del.extend(['MobileNo_Avl_Flag', 'DisbursalDate_year'])","17a81c05":"def create_features(df):\n    df.loc[:, 'ltv_round'] = df.loc[:, 'ltv'] - df.loc[:, 'ltv'] % 1 # \u0446\u0435\u043b\u0430\u044f \u0447\u0430\u0441\u0442\u044c\n    df.loc[:, 'ltv_%1'] = df.loc[:, 'ltv'] % 1 # \u043a\u0440\u0430\u0442\u043d\u044b \u043e\u0434\u043d\u043e\u043c\u0443 \/ \u0434\u0440\u043e\u0431\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u044c\n    df.loc[:, 'ltv_round_%5'] = df.loc[:, 'ltv_round'] % 5 # \u043a\u0440\u0430\u0442\u043d\u044b 5\n    df.loc[:, 'ltv_round_%10'] = df.loc[:, 'ltv_round'] % 10 # \u043a\u0440\u0430\u0442\u043d\u044b 10\n    return df\n\nclass CreateCounts(object):\n    def __init__(self, columns):\n        self.columns = columns\n        self.res = {}\n        \n    def fit(self, X, y=None):\n        for col in self.columns:\n            _dict = X[col].value_counts().to_dict()\n            self.res[col] = _dict\n    \n    def transform(self, X, y=None):\n        for col in self.columns:\n            X.loc[:, col + '_counts'] = X.loc[:, col].map(self.res[col])\n        return X\n    \n    def fit_transform(self, X, y=None):\n        self.fit(X)\n        return self.transform(X)\n    \nclass CreateFeats(object):\n    def __init__(self):\n        self.counts = None\n        \n    def fit(self, X):\n        self.counts = CreateCounts(columns=['disbursed_amount', 'asset_cost', 'branch_id', 'supplier_id',\n                                              'manufacturer_id', 'Current_pincode_ID', 'State_ID',\n                                              'Employee_code_ID', 'PERFORM_CNS.SCORE.DESCRIPTION'])\n        self.counts.fit(X)\n        \n            \n    def transform(self, X, y=None):\n        X = self.counts.transform(X)\n        X = create_features(X)\n        return X\n    \n    def fit_transform(self, X, y=None):\n        self.fit(X)\n        return self.transform(X)    ","c898ca73":"%%time\n\nfgen = CreateFeats()\ntrain = fgen.fit_transform(train)\ntest = fgen.transform(test)","cee69f27":"train.head()","6fe43a5b":"for col in columns_for_del:\n    del train[col]\n    del test[col]","688ae378":"from sklearn.metrics import roc_auc_score as auc\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom copy import deepcopy, copy\n\nclass LGBMWrapper(object):\n    def __init__(self, params, meta_params):\n        self.params = params\n        self.meta_params = meta_params\n        \n    def fit(self, X, y, X_val, y_val):\n        train_data = lgb.Dataset(X, label=y)\n        val_data = lgb.Dataset(X_val, label=y_val)\n        self.model = lgb.train(self.params,\n                               train_data,\n                               valid_sets=val_data,\n                               **self.meta_params,\n                              )\n        \n    def predict(self, X):\n        return self.model.predict(X)\n    \nclass Experiment(object):\n    def __init__(self, model, params):\n        self.model = model\n        self.params = params\n        self.models = []\n        self.metrics = []\n        \n    def cv(self, X, y, imp=True, verbose=True):\n        self.models = []\n        oof = np.zeros(len(X))\n        \n        if imp:\n            self.metrics = []\n            self.gain_imp = np.zeros(X.shape[1])\n            self.split_imp = np.zeros(X.shape[1])\n        \n        for n, (tr, vl) in enumerate(self.params['folds']):\n            X_tr, X_vl = X.loc[tr], X.loc[vl]\n            y_tr, y_vl = y.loc[tr], y.loc[vl]\n            self.model.fit(X_tr, y_tr, X_vl, y_vl)\n            if imp:\n                self.gain_imp += self.model.model.feature_importance(importance_type='gain')\n                self.split_imp += self.model.model.feature_importance(importance_type='split')                \n            \n            self.models.append(copy(self.model))\n            oof[vl] = self.model.predict(X_vl)\n            \n            \n            if verbose:  \n                self.metrics.append(self.params['metric'](y_vl, oof[vl]))\n                print('Fold {}, metric {:f}'.format(n, self.metrics[-1]))\n                \n        \n        if verbose: \n            self.oof_metric = self.params['metric'](y, oof)\n            print('==========')\n            print('OOF metric {:f}'.format(self.oof_metric))\n            print('Mean folds metric {:f}'.format(np.mean(self.metrics)))\n        \n        if imp:\n            self.gain_imp \/= (n + 1)\n            self.split_imp \/= (n + 1)\n        \n        return oof\n    \n    def fit(self, X, y, imp=True, verbose=True):\n        self.columns = list(X.columns)\n        oof = self.cv(X, y, imp, verbose)\n        self.imp = pd.DataFrame(index=self.columns)\n        self.imp.loc[:, 'gain'] = self.gain_imp\n        self.imp.loc[:, 'split'] = self.split_imp\n    \n    def predict(self, X):\n        preds = np.zeros(len(X))\n        for n, model in enumerate(self.models):\n            preds += self.model.predict(X)\n            \n        return preds \/ (n+1)\n    \n    def get_importances(self, sort_by='gain'):\n        return self.imp.sort_values(sort_by, ascending=False)\n    \n    def forward(self, X, y, imp='gain', tol=4):\n        imps = self.get_importances(sort_by=imp)\n        sorted_cols = list(imps.index)\n        \n        c = 0\n        best_metric = 0\n        usefull_columns = []\n        for col in sorted_cols:\n            usefull_columns.append(col)\n            oof = self.cv(X[usefull_columns], y, False, False)\n            metric = self.params['metric'](y, oof)\n            if metric > best_metric:\n                print('Column {} added, {:f} -> {:f}'.format(col, best_metric, metric))\n                best_metric = metric\n                c = 0\n            else:\n                usefull_columns.pop()\n                c += 1\n                \n            if c >= tol:\n                break\n        \n        print('========')\n        print('Best metric {:f}'.format(best_metric))\n                \n        return usefull_columns\n    \n    def backward(self, X, y, imp='gain', tol=4):\n        imps = self.get_importances(sort_by=imp)\n        sorted_cols = list(reversed(imps.index))\n        \n        c = 0\n        best_metric = self.oof_metric\n        usefull_columns = sorted_cols\n        for col in sorted_cols:\n            usefull_columns = sorted(list(set(usefull_columns) - {col}))\n            oof = self.cv(X[usefull_columns], y, False, False)\n            metric = self.params['metric'](y, oof)\n            if metric > best_metric:\n                print('Column {} removed, {:f} -> {:f}'.format(col, best_metric, metric))\n                best_metric = metric\n                c = 0\n            else:\n                usefull_columns.append(col)\n                c += 1\n                \n            if c >= tol:\n                break\n        print('========')\n        print('Best metric {:f}'.format(best_metric))\n                \n        return usefull_columns\n                \n            \n    \n    ","a2a09503":"params = {'application': 'binary',\n          'objective': 'binary',\n          'metric': 'auc'\n         }\n\nmeta_params = {'num_boost_round': 5000,\n               'early_stopping_rounds': 100,\n               'verbose_eval': None\n              }\n\nmodel = LGBMWrapper(params, meta_params)\n\nexpparams = {'folds': list(KFold(n_splits=5, random_state=0, shuffle=True).split(train)),\n             'metric': auc}\n\nexp = Experiment(model, expparams)\n","349e4dc7":"exp.fit(train, target)","230680d0":"exp.get_importances().head(10)","857466d4":"ff_split = exp.forward(train, target, imp='split')","cb1e9116":"params = {'application': 'binary',\n          'objective': 'binary',\n          'metric': 'auc',\n          'learning_rate': 0.02,\n          'min_data_in_leaf': 200,\n          'bagging_fraction': 0.8,\n          'bagging_freq': 1,\n          'feature_fraction': 0.3,\n         }\n\nmeta_params = {'num_boost_round': 5000,\n               'early_stopping_rounds': 200,\n               'verbose_eval': None\n              }\n\nmodel = LGBMWrapper(params, meta_params)\n\nexpparams = {'folds': list(KFold(n_splits=5, random_state=0, shuffle=True).split(train)),\n             'metric': auc}\n\nexp = Experiment(model, expparams)\nexp.fit(train[ff_split], target)","6eacd009":"predict = exp.predict(test[ff_split])","8062ee45":"submission.loc[:, 'Predicted'] = predict\nsubmission.to_csv('submission.csv', index=None)","d1c5c0a9":"\u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.","1d93b556":"## \u041d\u0435\u043c\u043d\u043e\u0433\u043e \u0442\u044e\u043d\u0430","7e2bfe9b":"## \u041e\u0442\u0431\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","b721365f":"## \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","89611aff":"## \u041f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043d\u0433 \u0434\u0430\u0442 \u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0445 \u043f\u043e\u043b\u0435\u0439:","7fd00aaa":"## \u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435"}}