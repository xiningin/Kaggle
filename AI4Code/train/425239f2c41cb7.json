{"cell_type":{"da885cb7":"code","b35cc2ca":"code","005df6ea":"code","c5ccc6a3":"code","60ccc38c":"code","7353b4d3":"code","8501cfd9":"code","3055c58c":"code","dd956a94":"code","a43061b5":"code","540f35a2":"code","d8e6849f":"code","7be2b657":"code","4b80769f":"code","6f5c25c9":"code","1ea0dfee":"code","e8283d56":"code","0afcd27a":"code","383c67d1":"code","d32e7d48":"markdown","4dffd980":"markdown","b2aaadce":"markdown","2be9d0e2":"markdown","ac343aee":"markdown","d05dd8ea":"markdown","362bb278":"markdown","368354d3":"markdown","825110de":"markdown","19058a93":"markdown","bdfffbd4":"markdown","2af3c105":"markdown"},"source":{"da885cb7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b35cc2ca":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)","005df6ea":"data = pd.read_csv('..\/input\/heart.csv')","c5ccc6a3":"data.head()","60ccc38c":"data.dtypes","7353b4d3":"data.isnull().sum()","8501cfd9":"data['cp'] = data['cp'].astype(object)\ndata['restecg'] = data['restecg'].astype(object)\ndata['slope'] = data['slope'].astype(object)\ndata['ca'] = data['ca'].astype(object)\ndata['thal'] = data['thal'].astype(object)\ndata = pd.get_dummies(data,drop_first=True)\ndata.head()","3055c58c":"target = data['target']","dd956a94":"data = data.drop(columns='target')","a43061b5":"X_train,X_test,Y_train,Y_test = train_test_split(data,target,random_state=0)\nscaler = StandardScaler()\nscaler.fit(X_train)\n# Logistic Regression and SVM use scaled data.\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","540f35a2":"\n# LogisticRegression Train\nparam_grid = {'C':[0.001,0.01,0.1,1,10,100],\n              'solver':['lbfgs','liblinear','sag','saga'],\n              'max_iter':[1,5,10,25,50,75,100,125,150]}\n\nlr = GridSearchCV(LogisticRegression(random_state=0),param_grid,cv=5)\nlr.fit(X_train_scaled,Y_train)\n\n# SVM Train\nparam_grid = {'C':[0.001,0.01,0.1,1,10,100],\n             'gamma':[0.001,0.01,0.1,1,10,100]}\n\n\nsvm = GridSearchCV(SVC(),param_grid,cv=5)\nsvm.fit(X_train_scaled,Y_train)\n\n# RandomForest Train\n# When the depth is 2 or more, it becomes over fitting.\nparam_grid = {'n_estimators':[10,25,50,75,100,125,150],\n              'max_depth':[1]}\n\nrf = GridSearchCV(RandomForestClassifier(random_state=0),param_grid,cv=5)\nrf.fit(X_train,Y_train)","d8e6849f":"print('LogisticRegression train set score: {:.2f}'.format(lr.score(X_train_scaled,Y_train)))\nprint('LogisticRegression test set score: {:.2f}'.format(lr.score(X_test_scaled,Y_test)))\nprint('LogisticRegression best paramerters: {}'.format(lr.best_params_))\n\nprint('SVM train set score: {:.2f}'.format(svm.score(X_train_scaled,Y_train)))\nprint('SVM test set score: {:.2f}'.format(svm.score(X_test_scaled,Y_test)))\nprint('SVM best paramerters: {}'.format(svm.best_params_))\n\nprint('RandomForest train set score: {:.2f}'.format(rf.score(X_train,Y_train)))\nprint('RandomForest test set score: {:.2f}'.format(rf.score(X_test,Y_test)))\nprint('RandomForest best paramerters: {}'.format(rf.best_params_))","7be2b657":"\ndata = pd.concat([X_train,X_test])\ndata_scaled = np.concatenate([X_train_scaled,X_test_scaled])\ntarget = pd.concat([Y_train,Y_test])\nlr_scores = cross_val_score(lr,data_scaled,target,cv=5)\nsvm_scores = cross_val_score(svm,data_scaled,target,cv=5)\nrf_scores = cross_val_score(rf,data,target,cv=5)","4b80769f":"print('LR Cross-validation scores: ',lr_scores)\nprint('LR Mean Cross-validation scores: ',lr_scores.mean())\n\nprint('SVM Cross-validation scores: ',svm_scores)\nprint('SVM Mean Cross-validation scores: ',svm_scores.mean())\n\nprint('RF Cross-validation scores: ',rf_scores)\nprint('RF Mean Cross-validation scores: ',rf_scores.mean())","6f5c25c9":"predict = lr.predict(X_test_scaled)\nprint(\"---Classification Report about LR---\")\nprint(classification_report(Y_test,predict))","1ea0dfee":"predict = svm.predict(X_test_scaled)\nprint(\"---Classification Report about SVM---\")\nprint(classification_report(Y_test,predict))","e8283d56":"predict = rf.predict(X_test)\nprint(\"---Classification Report about RF---\")\nprint(classification_report(Y_test,predict))","0afcd27a":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve\nprecision_lr,recall_lr,thresholds_lr = precision_recall_curve(Y_test,lr.decision_function(X_test_scaled))\nprecision_svm,recall_svm,thresholds_svm = precision_recall_curve(Y_test,svm.decision_function(X_test_scaled))\nprecision_rf,recall_rf,thresholds_rf = precision_recall_curve(Y_test,rf.predict_proba(X_test)[:,1])\n\nplt.plot(precision_lr,recall_lr,label='LR')\nplt.plot(precision_svm,recall_svm,label='SVM')\nplt.plot(precision_rf,recall_rf,label='RF')\n\nclose_zero_lr = np.argmin(np.abs(thresholds_lr))\nclose_zero_svm = np.argmin(np.abs(thresholds_svm))\nclose_default_rf = np.argmin(np.abs(thresholds_rf -0.5))\n\nplt.plot(precision_lr[close_zero_lr],recall_lr[close_zero_lr],'o',markersize=10,label=\"threshold zero lr\",fillstyle=\"none\",mew=2)\nplt.plot(precision_svm[close_zero_svm],recall_svm[close_zero_svm],'^',markersize=10,label=\"threshold zero svm\",fillstyle=\"none\",mew=2)\nplt.plot(precision_rf[close_default_rf],recall_rf[close_default_rf],'x',markersize=10,label=\"threshold 0.5 rf\",fillstyle=\"none\",mew=2)\n\nplt.xlabel('Precision')\nplt.ylabel('Recall')\nplt.legend()\n","383c67d1":"fpr_lr,tpr_lr,thresholds_lr = roc_curve(Y_test,lr.decision_function(X_test_scaled))\nfpr_svm,tpr_svm,thresholds_svm = roc_curve(Y_test,svm.decision_function(X_test_scaled))\nfpr_rf,tpr_rf,thresholds_rf = roc_curve(Y_test,rf.predict_proba(X_test)[:,1])\n\nplt.plot(fpr_lr,tpr_lr,label=\"ROC Curve LR\")\nplt.plot(fpr_svm,tpr_svm,label=\"ROC Curve SVM\")\nplt.plot(fpr_rf,tpr_rf,label=\"ROC Curve RF\")\n\nclose_zero_lr = np.argmin(np.abs(thresholds_lr))\nclose_zero_svm = np.argmin(np.abs(thresholds_svm))\nclose_default_rf = np.argmin(np.abs(thresholds_rf -0.5))\n\nplt.plot(fpr_lr[close_zero_lr],tpr_lr[close_zero_lr],'o',markersize=10,label=\"threshold zero lr\",fillstyle=\"none\",mew=2)\nplt.plot(fpr_svm[close_zero_svm],tpr_svm[close_zero_svm],'^',markersize=10,label=\"threshold zero svm\",fillstyle=\"none\",mew=2)\nplt.plot(fpr_rf[close_default_rf],tpr_rf[close_default_rf],'x',markersize=10,label=\"threshold 0.5 rf\",fillstyle=\"none\",mew=2)\n\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.legend(loc=4)\n","d32e7d48":"**Logistic regression has the best result. **  \nNext, check the result of cross validation.  ","4dffd980":"## Load and check the dataset","b2aaadce":"\nThe above shows **precision, recall, f1-score** of the three models.  \n  \nI'll clarify each definition.  \n![](https:\/\/cdn-ak.f.st-hatena.com\/images\/fotolife\/p\/protoidea\/20190210\/20190210130811.png)  \n  \n**precision = TP\/(TP + FP)  \nrecall = TP\/(TP+FN)  \nf1-score = 2 * (precision * recall)\/(presion + recall)  \nTPR = recall  \nFPR = FP\/(FP + TN)**  \n  \nNext, let's check the precision-recall-curve and ROC-curve.","2be9d0e2":"\n**As a result of cross validation, the result of logistic regression is the best.**  \nI'll analyze the results from different perspectives.  ","ac343aee":"## Check cross-validation score","d05dd8ea":"Logistic regression and SVM have marked a threshold of 0, marking 0.5 in random forest.  \nLogistic regression has good results in total.  \nBut, if you want to raise 'recall', it seems better to use SVM.  \nI guess that  **It is important to increase 'recall' in this problem** , because It is dangerous to mistakenly predict that heart disease is not a heart disease.  \nI think we should choose SVM that can increase recall while keeping precison high.","362bb278":"## Check precision-recall-curve and ROC-curve","368354d3":"\nAll columns are numerical.  \nHowever, there are columns that should be treated as categories rather than numbers. ","825110de":"# Introduction\nI'll build a model with logistic regression, SVM and random forest by using sklearn.  \nAnd I'll evaluate the three models and consider which one to choose.","19058a93":"## Check precision , recall and f1-score ","bdfffbd4":"This time I'll compare the performance of Logistic Regression, SVM and Random Forest.  \n*In the code, it is abbreviated as follows.  \nLogistic Regression : LR  \nSVM: SVM  \nRandom Forest : RF  ","2af3c105":"## Train three models"}}