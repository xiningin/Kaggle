{"cell_type":{"d0e10605":"code","8cba46ef":"code","626f778f":"code","4dcdb49b":"code","2ea503c3":"code","854c5a96":"code","5a98ef6a":"code","b9e1220e":"code","e5dc78fc":"code","6d973824":"code","1a5a4cdf":"code","51ca81a6":"code","56ee9c77":"code","0333d71f":"code","29c02fb5":"code","f4615851":"code","78baabd0":"code","d81a84d7":"code","c4bdbb83":"code","cab3e486":"code","3a209778":"code","d3c97f6f":"code","da9b2bc4":"code","7ad610c7":"code","d5be6fa9":"code","6828a702":"code","e3d4a956":"code","670f6882":"code","7073afc9":"code","233f5284":"code","c4b76e42":"markdown","a332180b":"markdown","2474990f":"markdown","4a8753e1":"markdown","abe63d35":"markdown","ee739a85":"markdown","6160a42c":"markdown","4575318b":"markdown","0d30e454":"markdown","78bd580d":"markdown","deb1207a":"markdown","9e3e6f50":"markdown","770d9170":"markdown","20c078d6":"markdown","6089d0d7":"markdown","c26c4af5":"markdown"},"source":{"d0e10605":"# basics\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# preprocess\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import PowerTransformer\n\n# model\nfrom lightgbm import LGBMClassifier\n\n# imbalanced\nfrom imblearn.pipeline import Pipeline\n\n## hyperopt functions\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval\nfrom hyperopt.pyll import scope as ho_scope\nfrom hyperopt.pyll.stochastic import sample as ho_sample\nfrom functools import partial\n\n# evalue\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, average_precision_score, roc_auc_score, fbeta_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, auc, log_loss\nfrom sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, roc_curve, plot_roc_curve\n\n# resample\nfrom imblearn.over_sampling import ADASYN, SMOTE\nfrom imblearn.under_sampling import OneSidedSelection, NeighbourhoodCleaningRule, TomekLinks\n\n# turn off warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8cba46ef":"# Set model\ndef instance_model(hyperparameters):\n    \n    # LightGBM classifier\n    if pd.DataFrame(hyperparameters.keys())[0][0] == 'lgbm':\n        model = LGBMClassifier(**hyperparameters['lgbm'], \n                            n_jobs = -1,\n                            random_state = 42,      \n                            objective = \"binary\", \n                            #categorical_feature = categorical_features_index,\n                            n_estimators = 9999999,\n                            bagging_freq = 1,       \n                            #is_unbalance = True,   \n                            learning_rate = 0.01)  \n       \n        # Resampling\n        # ADASYN: Adaptive synthetic sampling\n        if hyperparameters['lgbm']['sample'] == 'adasyn':\n            undersample = ADASYN(random_state=42)\n            if hyperparameters['lgbm']['power'] == True:\n                power = PowerTransformer(method='yeo-johnson', standardize=True)\n                model = Pipeline([('sampling', undersample), \n                                  ('power', power), ('lgbm', model) ])\n            else: \n                model = Pipeline([('sampling', undersample), ('lgbm', model) ])\n                \n        # SMOTE: Synthetic Minority Oversampling Technique\n        if hyperparameters['lgbm']['sample'] == 'smote':\n            undersample = SMOTE()\n            if hyperparameters['lgbm']['power'] == True:\n                power = PowerTransformer(method='yeo-johnson', standardize=True)\n                model = Pipeline([('sampling', undersample),\n                                  ('power', power), ('lgbm', model) ])\n            else: \n                model = Pipeline([('sampling', undersample), ('lgbm', model) ])\n                \n        # Tomek Links: remover exemplos ambiguos\n        elif hyperparameters['lgbm']['sample'] == 'tomek':\n            undersample = TomekLinks()\n            if hyperparameters['lgbm']['power'] == True:\n                power = PowerTransformer(method='yeo-johnson', standardize=True)\n                model = Pipeline([('sampling', undersample), \n                                  ('power', power), ('lgbm', model) ])\n            else:\n                model = Pipeline([('sampling', undersample), ('lgbm', model) ])\n                \n        # Neighborhood Cleaning Rule for Undersampling: \n        # Condensed Nearest Neighbor + Edited Nearest Neighbors\n        elif hyperparameters['lgbm']['sample'] == 'ncr': \n            undersample  = NeighbourhoodCleaningRule(n_neighbors=3,\n                                                     threshold_cleaning=0.5)\n            if hyperparameters['lgbm']['power'] == True:\n                power = PowerTransformer(method='yeo-johnson', standardize=True)\n                model = Pipeline([('sampling', undersample),\n                                  ('power', power), ('lgbm', model) ])\n            else:\n                model = Pipeline([('sampling', undersample), ('lgbm', model) ])\n                \n        # One-Sided Selection : \n        # Tomek Links + Condensed Nearest Neighbor \n        elif hyperparameters['lgbm']['sample'] == 'oss':\n            undersample = OneSidedSelection(n_neighbors=1, n_seeds_S=200)\n            if hyperparameters['lgbm']['power'] == True:\n                power = PowerTransformer(method='yeo-johnson', standardize=True)\n                model = Pipeline([('sampling', undersample), \n                                  ('power', power), ('lgbm', model) ])\n            else:\n                model = Pipeline([('sampling', undersample), \n                                  ('lgbm', model) ])\n        ## No resampling\n        else:\n            if hyperparameters['lgbm']['power'] == True:\n                power = PowerTransformer(method='yeo-johnson', standardize=True)\n                model = Pipeline([('sampling', None), \n                                  ('power', power), ('lgbm', model) ])\n            else:\n                model = Pipeline([('sampling', None), ('lgbm', model) ])\n     \n    return model\n\ndef to_minimize(hyperparameters, features, target, fit_params):\n    # create an instance of the model \n    model = instance_model(hyperparameters)\n    \n    # train with cross-validation\n    resultado = cross_val_score(estimator = model, \n                                X = features, \n                                y = target, \n                                scoring = \"average_precision\",\n                                cv = cv, \n                                fit_params = fit_params,\n                                n_jobs = -1)\n    \n    return -resultado.mean()\n\n# function to get the optimization history\ndef extract_space_eval(hp_space, trial):\n    \n    ## get results\n    desempacota_trial = space_eval(space = hp_space, \n                                   hp_assignment = {k: v[0] for (k, v) in trial['misc']['vals'].items() if len(v) > 0})\n    \n    return desempacota_trial\n\ndef unpack_dictionary(dictionary):\n    unpacked = {}\n    for (key, value) in dictionary.items():\n        if isinstance(value, dict):\n            unpacked = {**unpacked, **unpack_dictionary(value)}\n        else:\n            unpacked[key] = value\n            \n    return unpacked\n\n# Metrics to evaluate model\ndef evalue_model(model, y_test, X_test, model_name):\n    # default cut off\n    threshold = 0.5\n    \n    # predict\n    pred_prob = model.predict_proba(X_test)\n    \n    # pr curve to best threshold\n    precision, recall, thresholds = precision_recall_curve(y_test, pred_prob[:, 1])\n\n    # calcule fscore\n    fscore_f2  = ((1+4)*precision*recall)\/(4*precision+recall) # f2\n    fscore_f1  = (2*precision*recall)\/(precision+recall) #f1\n    fscore_f05 = (1.25*precision*recall)\/(0.25*precision+recall) #f05\n    \n    # get max\n    threshold_f2  = thresholds[np.argmax(fscore_f2)]\n    threshold_f1  = thresholds[np.argmax(fscore_f1)]\n    threshold_f05 = thresholds[np.argmax(fscore_f05)]\n    \n    # prob true class\n    pred_prob = [predicao[1] for predicao in pred_prob]\n    \n    # apply threshold 05\n    pred_class = [instancia >= threshold_f05 for instancia in pred_prob]\n    \n    # confusion matrix\n    cm = confusion_matrix(y_true = y_test, y_pred = pred_class)\n    \n    # metrics\n    dictionary = {'accuracy': accuracy_score(y_true = y_test,y_pred=pred_class),\n                  'F05': fbeta_score(y_true=y_test,y_pred=pred_class,beta=0.5),\n                  'F1': fbeta_score(y_true=y_test,y_pred=pred_class,beta=1),\n                  'F2': fbeta_score(y_true=y_test,y_pred=pred_class,beta=2),\n                  'recall': recall_score(y_true = y_test, y_pred = pred_class),\n                  'precision': precision_score(y_true=y_test,y_pred=pred_class),\n                  'tn': cm[0][0],\n                  'fn': cm[1][0],\n                  'tp': cm[1][1],\n                  'fp': cm[0][1],\n                  'logloss': log_loss(y_test, pred_prob),\n                  'threshold_f2': threshold_f2,\n                  'threshold_f05': threshold_f05,\n                  'threshold_f1': threshold_f1,\n                  'auc': roc_auc_score(y_true = y_test, y_score = pred_prob),\n                  'average_precision':average_precision_score(y_true=y_test,y_score=pred_prob),\n                  'aucpr': auc(recall, precision),\n                  'model_name': model_name}\n    \n    return dictionary\n\ndef plot_auc_pr(name, labels, predictions,n=0.5, **kwargs):\n  p, r, _ = precision_recall_curve(labels, predictions)\n\n  plt.plot(100*r, 100*p, label=name, linewidth=2, **kwargs)\n  plt.xlabel('Recall [%]')\n  plt.ylabel('Precision [%]')\n  plt.xlim([-0.5,100])\n  plt.title('Precision-Recall Curve')\n  plt.ylim([20,100.5])\n  plt.grid(True)\n  ax = plt.gca()\n  ax.set_aspect('equal')","626f778f":"####################### ELIMINACION de filas con null o inf\ndef clean_dataset(df):\n    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n    df.dropna(inplace=True)\n    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n    return df[indices_to_keep].astype(np.float64)\n\n\n# Datos de entrada\ndf = pd.read_csv('..\/input\/bolsa-dl\/COMPLETO_train.csv')\n\n# Se eliminan las columnas no num\u00e9ricas o que no nos interesan\ncolumnsNoCategorizables = [col for col in df.columns if col not in ['empresa', 'antiguedad', 'mercado']]\ndf = df[columnsNoCategorizables]\n\n# Se eliminan las filas con null o inf\ndf = clean_dataset(df)\n\nprint(\"Full dataset has\",df.shape[0], \"rows and\", df.shape[1], \"columns\")","4dcdb49b":"df.head()","2ea503c3":"print(\"Missing data: \", df.isnull().sum().sum())","854c5a96":"# The classes are heavily skewed we need to solve this issue later.\nprint('No Frauds', round(df['TARGET'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Fraud', round(df['TARGET'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\n\nsns.countplot('TARGET', data=df)\nplt.title('TARGET Distributions \\n 0: No Fraud 1: Fraud')","5a98ef6a":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\nsns.distplot(df['volumen'].values, ax=ax[0])\nax[0].set_title('Distribution of Transaction volumen')\nax[0].set_xlim([min(df['volumen'].values), max(df['volumen'].values)])\n\nsns.distplot(df['dia'].values, ax=ax[1])\nax[1].set_title('Distribution of Transaction dia')\nax[1].set_xlim([min(df['dia'].values), max(df['dia'].values)])","b9e1220e":"X = df.drop('TARGET', axis=1)\ny = df['TARGET']","e5dc78fc":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nX_train = pd.DataFrame(X_train.values, columns=X.columns)\nX_test  = pd.DataFrame(X_test.values, columns=X.columns)\ny_train = y_train.values\ny_test  = y_test.values","6d973824":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\nprint(\"Train: Fraud =\", sum(y_train), \"No Fraud =\", len(y_train) - sum(y_train))\nprint(\"Val:   Fraud =\", sum(y_val), \" No Fraud =\", len(y_val) - sum(y_val))\nprint(\"Test:  Fraud =\", sum(y_test), \" No Fraud =\", len(y_test) - sum(y_test))\n\neval_set = [(pd.DataFrame(X_val), pd.DataFrame(y_val))]","1a5a4cdf":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","51ca81a6":"%%time\n\nmodel_lgbm = LGBMClassifier(n_jobs = -1, random_state = 42, objective = \"binary\",\n                            #categorical_feature= categorical_features_index,\n                            n_estimators = 9999999,\n                            bagging_freq = 1,\n                            #boosting = \"dart\",\n                            learning_rate = 0.01,\n                            is_unbalance = True)\n\nfit_params={'lgbm__early_stopping_rounds': 100, \n            'lgbm__eval_metric': 'average_precision',\n            'lgbm__verbose': True,\n            'lgbm__eval_set': eval_set}\n\nundersample = None\n\nmodel_lgbm_baseline = Pipeline([('sample', undersample), \n                                ('lgbm', model_lgbm) ])\n\nmodel_lgbm_baseline_cv = cross_val_score(model_lgbm_baseline, X_train, y_train, \n                                        cv = cv, \n                                        scoring = \"average_precision\", \n                                        fit_params = fit_params, \n                                        n_jobs=-1)\n\nprint(\"cross-validation Average Precision:\",f\"{model_lgbm_baseline_cv.mean():.3f} STD:{model_lgbm_baseline_cv.std():.2f}\")","56ee9c77":"scale_pos_weight_max = int((len(y_train) - sum(y_train)) \/ sum(y_train))\nscale_pos_weight_max","0333d71f":"# choices source: https:\/\/github.com\/microsoft\/LightGBM\/issues\/695#issuecomment-315591634\nhp_space_lgbm = {\n    'lgbm': {\n        # number of trees and learning rate -------------\n        #'n_estimators': ho_scope.int(hp.quniform('n_estimators',100,600,100)), # eval autotune\n        #'learning_rate': hp.loguniform('learning_rate',np.log(1e-5),np.log(0.05)), # eval autotune\n        # tree depth ------------------------------------\n        #'max_depth':  ho_scope.int(hp.quniform('max_depth',2,12,1)),\n        #'num_leaves': hp.choice(label = 'num_leaves', options = [15, 31, 63, 127, 255, 511, 1023, 2047, 4095]),\n        #'min_child_weight':  ho_scope.int(hp.quniform('min_child_weight',0,X_train.shape[0]\/100,1)),\n        # conservative update step ----------------------\n        ##'max_delta_step': ho_scope.int(hp.quniform('max_delta_step',1,10,1)),\n        # sampling --------------------------------------\n        'subsample': hp.uniform('subsample',0.4,1), \n        'colsample_bytree': hp.uniform('colsample_bytree',0.4,1),\n        #'feature_fraction': hp.uniform('feature_fraction',0.2,0.7),\n        # regularization --------------------------------\n        'reg_lambda': hp.loguniform('reg_lambda',np.log(1e-4),np.log(10)),\n        'reg_alpha': hp.loguniform('reg_alpha',np.log(1e-4),np.log(10)),\n        ##'min_gain_to_split': hp.loguniform('min_gain_to_split',np.log(1e-4),np.log(2)),\n        # specific lgbm ---------------------------------\n        #'min_child_samples': ho_scope.int(hp.quniform('min_child_samples',10,500,100)),\n        # set weights for balancing ---------------------\n        'scale_pos_weight' : ho_scope.int(hp.loguniform('scale_pos_weight',np.log(1),np.log(scale_pos_weight_max))),\n        # Dart Booster ----------------------------------\n        #'drop_rate': hp.uniform('drop_rate',0,1),\n        #'skip_drop': hp.uniform('skip_drop',0,1)\n        # Pipeline parameters ---------------------------\n        # Sampling\n        'sample':  hp.choice(label = 'sample', options = [None, 'tomek', 'ncr','oss', 'smote']),\n        # Boxcox\n        'power': hp.choice(label = 'power', options = [False, True])\n    }\n}","29c02fb5":"## criando instancia do Trials\ninteractions_lgbm = Trials()","f4615851":"fit_params={'lgbm__early_stopping_rounds': 100,\n            'lgbm__eval_metric': 'average_precision',\n            'lgbm__verbose': False,\n            'lgbm__eval_set': eval_set}","78baabd0":"%%time\n\n# N\u00famero de intentos\nMAX_EVALS= 60\n\n## run optimization\noptimization = fmin(fn = partial(to_minimize, features = X_train, target = y_train, fit_params = fit_params),\n                  space = hp_space_lgbm, \n                  algo = tpe.suggest,\n                  trials = interactions_lgbm,\n                  max_evals = int(MAX_EVALS), \n                  rstate = np.random.RandomState(42))","d81a84d7":"## save history \nlgbm_history = pd.DataFrame([unpack_dictionary(extract_space_eval(hp_space_lgbm, x)) for x in interactions_lgbm.trials])","c4bdbb83":"## save results in new col\nlgbm_history['average_precision'] = pd.DataFrame(interactions_lgbm.results).loss * -1","cab3e486":"sns.scatterplot(x = lgbm_history.index, y = 'average_precision', data = lgbm_history)\nplt.title('Optimization History')\nplt.xlabel(xlabel = 'Interaction')\nplt.ylabel(ylabel = 'Average Precision')","3a209778":"sns.countplot(x = 'sample', data = pd.DataFrame(lgbm_history['sample'].apply(lambda x: \"None\" if x == None else x)))\nplt.title('Times each criterion was selected')\nplt.xlabel(xlabel = 'sample')\nplt.ylabel(ylabel = 'Interactions')","d3c97f6f":"sns.countplot(x = 'power', data = lgbm_history)\nplt.title('Times each criterion was selected')\nplt.xlabel(xlabel = 'YeoJhonson')\nplt.ylabel(ylabel = 'Interactions')","da9b2bc4":"selected_hyperparameters = space_eval(space = hp_space_lgbm, hp_assignment = optimization)\nselected_hyperparameters","7ad610c7":"model_lgbm_bayeshp = instance_model(hyperparameters=selected_hyperparameters)","d5be6fa9":"print(\"Historical max Average Precision:\",f\"{lgbm_history.average_precision.max():.3f} STD:{lgbm_history.average_precision.std():.2f}\")","6828a702":"%%time\nclassifiers = {\n    \"LGBMBaseline\": model_lgbm_baseline,\n    \"LGBMBayesOpt\": model_lgbm_bayeshp\n}\n\n# df to store metrics \nresults_lgbm = pd.DataFrame(columns= ['metric', 'model_name', 'aucpr', 'average_precision', 'auc', 'accuracy', 'F05', 'F1', 'F2', 'recall', 'precision', 'tn', 'fn', 'tp', 'fp', 'logloss', 'threshold_f2', 'threshold_f05', 'threshold_f1'])\n\n# df to save predictions\npred_df = pd.DataFrame(y_test,index=None)\n\nfor key, classifier in classifiers.items():\n    print(\"Running\", key)\n    model          = classifier.fit(X_train, y_train, \n                                    lgbm__early_stopping_rounds= 100, \n                                    lgbm__eval_metric= 'average_precision',\n                                    lgbm__verbose = 0,\n                                    lgbm__eval_set= eval_set)\n    pred_df[key]   = model.predict_proba(X_test)[:,1]\n    training_score = evalue_model(model,y_test, X_test, key)\n    df             = pd.DataFrame(training_score.items(), columns = [\"metric\", \"value\"])\n    results_lgbm   = results_lgbm.append(df.set_index('metric').T)","e3d4a956":"results_lgbm.drop('metric', axis=1).reset_index().drop('index', axis=1).sort_values(\"aucpr\", ascending = False)","670f6882":"colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\nsns.set_style(\"whitegrid\")\nplot_auc_pr(\"LGBMBaseline\", y_test, pred_df[\"LGBMBaseline\"], color=colors[0])\nplot_auc_pr(\"LGBMBayesOpt\", y_test ,pred_df[\"LGBMBayesOpt\"], color=colors[1],linestyle='--')\nplt.legend(loc='lower left')","7073afc9":"plt.rcdefaults()\nfig, ax = plt.subplots()\nimportancias=model_lgbm_bayeshp.steps[1][1].feature_importances_\nN_features_mas_importantes=25\nimportancias=importancias[:N_features_mas_importantes]\nax.barh(X.columns[np.argsort(importancias)][::-1], \n        sorted(importancias, reverse=True),\n        align='center')\nax.set_yticks(X.columns[:N_features_mas_importantes])\nax.invert_yaxis() \nax.set_xlabel('Importance')","233f5284":"import os\n\npath = '\/kaggle\/input\/bolsa-dl\/'\npathOutput = '.\/'\ncompletoTest='COMPLETO_test.csv'\ncompletoTestBasicas='COMPLETO_test_basicas.csv'\ncompletoTestConSolucion='COMPLETO_test_basicas_consolucion.csv'\n\n# MODELO a usar, ya entrenado\nmodel=model_lgbm_bayeshp\n\n# Input de validaci\u00f3n independiente\ntest = pd.read_csv(os.path.join(path, completoTest))\n\n# Se eliminan las columnas no num\u00e9ricas o que no nos interesan\ncolumns = [col for col in test.columns if col not in ['empresa', 'antiguedad', 'mercado']]\ntest = test[columns]\n\n# Se eliminan las filas con null o inf\n#test = clean_dataset(test)\n\n# PREDICCI\u00d3N independiente\nsubmission = pd.read_csv(os.path.join(path, completoTestBasicas))\nsubmission[\"TARGET\"] = (model.predict(test[columns]) > 0.5).astype(\"int32\")\nsubmission.to_csv(os.path.join(pathOutput, 'Bolsa_DL_submission.csv'), index=False)\n\n# Accuracy independiente\ntargetPredicha=submission[\"TARGET\"]\nsolucion = pd.read_csv(os.path.join(path, completoTestConSolucion))\ntargetSolucion=solucion[\"TARGET\"]\n# a: real. B: predicho\na=targetSolucion\nb=targetPredicha\n\nacc=-1\nif sum(b) > 0:\n    acc=sum(1 for x,y in zip(a,b) if (x == y and y == 1)) \/ sum(b)\n\nprint(\"Accuracy REAL: \", acc)\n\nprint(\"FIN\")\n\n\n","c4b76e42":"AUCPR","a332180b":"# Problem definition","2474990f":"Define cross-validation method as stratified (due to unbalanced database) and shuffle to avoid ordering bias","4a8753e1":"## Baseline model","abe63d35":"VALIDACI\u00d3N INDEPENDIENTE:\n\n\n","ee739a85":"Check data balance","6160a42c":"#### Details:\n\nAs mentioned in the description of the data set: these are transactions made by credit cards in September 2013 by European cardholders for 2 days.\n\nImportant informations:\n\n- The data set is highly unbalanced, the positive class (fraud) is responsible for 0.172% of all transactions.\n- The input variables are all numeric and most were obtained by the PCA (common procedure due to confidentiality)\n- The `Time` feature contains the seconds that elapse between each transaction and a first transaction in the data set.\n- The `Amount` resource is the Transaction Amount\n- The `Class` resource is a response variable and assumes a value of 1 in case of fraud and 0 if not.\n\n#### Solution:\n\nA LightGBM model will be adjusted using Bayesian optimization with lib hyperopt (optimize hyperparameters and resampling methods) to classify whether the transition is fraud. The goal will be to maximize aucpr (or average precision)\n\nThe resampling methods tested will be:\n\n- SMOTE\n- ADASYN\n- OneSidedSelection\n- NeighbourhoodCleaningRule\n- TomekLinks\n","4575318b":"Sources:\n\n- <https:\/\/github.com\/microsoft\/LightGBM\/issues\/695#issuecomment-315591634>\n- <https:\/\/machinelearningmastery.com\/framework-for-imbalanced-classification-projects\/>\n- <https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/>\n- <https:\/\/sites.google.com\/view\/lauraepp\/parameters>\n- <https:\/\/sanchom.wordpress.com\/tag\/average-precision\/>","0d30e454":"## Understand train results","78bd580d":"# Explore","deb1207a":"Authors:\n\n  * [Fellipe Gomes](https:\/\/github.com\/gomesfellipe) (Statistician - UFF, Data Scientist - Accenture) \n  * [Nicholas Marino](https:\/\/github.com\/nacmarino) (Ecologist - UFJF, Data Scientist - Accenture)\n\n\n<p align=\"right\"><span style=\"color:firebrick\">Dont forget to upvote if you liked! \ud83e\udd18<\/span> <\/p>\n","9e3e6f50":"Check distribution of non-PCA features","770d9170":"# Modeling\n\nSeparate explanatory variables from the target","20c078d6":"## Bayes Optimization ","6089d0d7":"# Evalue","c26c4af5":"# Import dependencies"}}