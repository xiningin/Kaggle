{"cell_type":{"9cec6fcb":"code","a4657db9":"code","ad3c50c0":"code","f1423133":"code","5641f61e":"code","a6a12be0":"code","97e07cc4":"code","52e62e9b":"code","56007a77":"code","9421d5ba":"code","a2c8c25b":"code","bb891af7":"code","c49f8e02":"code","abb43b62":"code","700e7b19":"code","3aa50b0f":"code","ad22423c":"code","3b87b0db":"code","cf4cf069":"code","7237f26a":"code","acb86b17":"code","a6edc210":"markdown","9f2c97e5":"markdown","d43ce6de":"markdown","d6e8b49f":"markdown","c05e2449":"markdown","3a2aea6f":"markdown","1f257752":"markdown","2d501086":"markdown","a8e41a72":"markdown","0551a746":"markdown","af977114":"markdown","758b04cf":"markdown","0f21cae9":"markdown","f6f203a7":"markdown","d9abd097":"markdown","8bf2a71d":"markdown","880ec1f2":"markdown","4c2a42fc":"markdown","df58dede":"markdown","8629f403":"markdown","52dd4137":"markdown","a4e042e1":"markdown","fb24cb25":"markdown","b9b678f3":"markdown","2174ebcb":"markdown","a35b0a5a":"markdown","792f106f":"markdown","28feed8d":"markdown"},"source":{"9cec6fcb":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom scipy.stats import loguniform\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set_style()\npd.set_option(\"display.precision\",1)","a4657db9":"data = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")\ndata.drop('Id',axis=1, inplace=True)","ad3c50c0":"# first 5 rows\ndata.head()","f1423133":"print(f\"We have {data.shape[0]} rows and {data.shape[1]} columns\")\nprint()\nprint(f\"Columns : {data.columns.tolist()}\")\nprint()\nprint(f\"We have {data.Species.nunique()} labels in target\")\nprint()\nprint(f\"Labels: {data.Species.unique()}\")","5641f61e":"# statistics of data\ndata.describe()","a6a12be0":"data.isna().sum().to_frame(\"# of null values\")","97e07cc4":"fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,6))\n\np1 = sns.scatterplot(x=\"SepalLengthCm\", y=\"PetalLengthCm\", data=data, hue=\"Species\", ax=ax1)\np1.set_title(\"Sepal length Vs Petal length\", fontweight=\"bold\", fontsize=15)\np1.set_xlabel(\"Sepal Length cm\",  fontsize=15)\np1.set_ylabel(\"Petal Length cm\", fontsize=15)\n\np2 = sns.scatterplot(x=\"SepalWidthCm\", y=\"PetalWidthCm\", data=data, hue=\"Species\", ax=ax2)\np2.set_title(\"Sepal width Vs Petal width\", fontweight=\"bold\", fontsize=15)\np2.set_xlabel(\"Sepal width cm\",  fontsize=15)\np2.set_ylabel(\"Petal width cm\", fontsize=15)\nplt.show()","52e62e9b":"fig, axes = plt.subplots(2,2, figsize=(20,12))\naxes=np.ravel(axes)\n\nfor i, col in enumerate(['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']):\n    sns.kdeplot(data = data, x=col,hue=\"Species\", fill=True,ax=axes[i])","56007a77":"fig, axes = plt.subplots(2,2, figsize=(20,12))\naxes=np.ravel(axes)\n\nfor i, col in enumerate(['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']):\n    sns.boxplot(data = data, y=col,x=\"Species\",ax=axes[i])","9421d5ba":"fig, axes = plt.subplots(2,2, figsize=(20,12))\naxes=np.ravel(axes)\n\nfor i, col in enumerate(['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']):\n    sns.violinplot(data = data, y=col,x=\"Species\",ax=axes[i])","a2c8c25b":"sns.pairplot(data, hue=\"Species\", height=3);","bb891af7":"fig = px.scatter_3d(data, x='SepalLengthCm', y='PetalLengthCm', z='SepalWidthCm',\n              color='Species')\nfig.show()","c49f8e02":"labelEncoder = LabelEncoder()\n\nX = data.drop(\"Species\", axis=1)\nlabels = data.Species\n\ny = labelEncoder.fit_transform(labels)\n\nlabelEncoder.classes_ # index is the respective class. Ex: index = 0 -> Iris-setosa","abb43b62":"X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.25, \n                                                    random_state=1, \n                                                    stratify=y)\nprint(\"Train set shape\", X_train.shape)\nprint(\"Test set shape\", X_test.shape)","700e7b19":"# Base models\nmodels = {\n    'log_reg':LogisticRegression(random_state=1),\n    \n    'svm':SVC(random_state=1),\n    \n    'dtree':DecisionTreeClassifier(random_state=1),\n    \n    'rforest':RandomForestClassifier(random_state=1)\n}","3aa50b0f":"for m in models.keys():\n    print(m)\n    scores = cross_val_score(models[m],\n                             X_train, y_train, \n                             scoring='accuracy',\n                             cv=5)\n    \n    print(f'Mean accuracy {scores.mean()} \\n')\n    print(40*'*','\\n')","ad22423c":"# random search logistic regression model\n\n# define model\nmodel = LogisticRegression(random_state=1)\n\n# define evaluation\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n\n# define search space\nspace = dict()\nspace['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\nspace['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\nspace['C'] = loguniform(1e-5, 100)\n\n# define search\nsearch = RandomizedSearchCV(model,\n                            space,\n                            n_iter=100,\n                            scoring='accuracy',\n                            n_jobs=-1,\n                            verbose=1,\n                            cv=cv, \n                            random_state=1)\n\n# execute search\nresult = search.fit(X_train, y_train)\n\n# summarize result\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)","3b87b0db":"# Final model\nfmodel = result.best_estimator_\n\n# Test set predictions\ny_test_preds = fmodel.predict(X_test)","cf4cf069":"print(\"Test Accuracy : \", accuracy_score(y_test, y_test_preds))","7237f26a":"plt.figure(figsize=(6,6))\n\ncm = confusion_matrix(y_test, y_test_preds)\nl = labelEncoder.classes_\n\nsns.heatmap(cm, annot=True, cmap='Blues', cbar=False, fmt='.0f')\nplt.xticks([0.5, 1.5, 2.5], l, fontsize=15)\nplt.yticks([0.05, 1.1, 2.2], l, fontsize=15)\n\nplt.show()","acb86b17":"import pickle\n\nwith open(\"model.pkl\", 'wb') as f:\n    pickle.dump(fmodel, f)","a6edc210":"### Observations\n#### - The Petal length, petal width of setosa flowers are smaller than other species\n#### - The Sepal width of setosa flowers is larger than other species\n#### - Some overlap between features of iris and virginica","9f2c97e5":"<h1 style=\"color: blue; background-color: orange; text-align:center; padding: 10px;\">II. Modeling<\/h1>","d43ce6de":"### Null values","d6e8b49f":"### Box plots","c05e2449":"### Base models :\n#### Logistic regression\n#### Support Vector Machine\n#### Decision tree\n#### Random Forest","3a2aea6f":"<h1 style=\"color: black; background-color: yellow; text-align:left ; padding: 5px\">K-Fold cross validation<\/h1>\n<p style=\"font-size: 18px; background-color: lightgray; padding: 20px\">\n1. Randomly divide a dataset into k groups, or \u201cfolds\u201d, of roughly equal size.<br>\n2. Choose one of the folds to be the holdout set. Fit the model on the remaining k-1 folds.<br>\n3. Calculate the test accuracy on the observations in the fold that was held out.<br>\n4. Repeat this process k times, using a different set each time as the holdout set.<br>\n5. Calculate the overall test accuracy by averaging the k test accuracies.<br>\n<\/p>","1f257752":"### Pair plot","2d501086":"## Thanks for reading!! Upvote this kernel if you like it!\ud83d\udc4d\ud83d\ude00","a8e41a72":"### Label encoding\n#### converting the target labels into numeric form","0551a746":"<h1 style=\"color: black; background-color: yellow; text-align:left ; padding: 5px\">Save the model<\/h1>","af977114":"### Importing libraries","758b04cf":"### 5-fold cross validation on train set","0f21cae9":"### Observation\n#### Nearly, all base models are performing well on cross validation data\n#### We can use logistic regression as our model. Why?? It is simple, interpretable and also performing well on our data.\n\n#### SIMPLE IS BETTER THAN COMPLEX","f6f203a7":"<h1 style=\"color: black; background-color: yellow; text-align:left ; padding: 5px\">Model Evaluation<\/h1>\n\n### Evaluating Logistic regression model on test set\n#### Accuracy\n#### Confusion matrix","d9abd097":"### Observations\n#### Petal length and Sepal length are most useful features for classification\n#### Setosa can be linearly separable but virginica and vercicolor have some overlap (almost linearly separable)","8bf2a71d":"### 3D Scatter plot","880ec1f2":"### Loading data into memory","4c2a42fc":"### Scatter plot","df58dede":"### Confusion matrix","8629f403":"### Observations :\n#### - By using (Sepal length and Petal length) or (Sepal width and petal width), we can differentiate setosa flowers\n#### - Some overlap between versicolor and virginica","52dd4137":"<b>OBSERVE THE FIGURE BELOW :<\/b><br><br>\n<img src=\"https:\/\/github.com\/ashok49473\/images\/blob\/main\/0_7NMQ0YUOvUGtRcSa.png?raw=true\" align=\"left\" >","a4e042e1":"<h1 style=\"color: green; background-color:orange; text-align:center; padding: 10px; font-weight: bold\">AIM : Classify iris flowers into three species.<\/h1>\n<br>\n<img src = \"https:\/\/newengland.com\/wp-content\/uploads\/iris-1420047_1920.jpg\" width=\"70%\">\n","fb24cb25":"<h1 style=\"color: blue; background-color: orange; text-align:center; padding: 10px;\">I. Exploratory Data Analysis (EDA)<\/h1>\n\n### Overview of data","b9b678f3":"### Violin Plot (combination of boxplots and dist plots)","2174ebcb":"### train test Split\n#### Split data matrix into random train and test subsets","a35b0a5a":"### Random search\n#### Tuning Hyperparameters of Logistic regression","792f106f":"### Distribution of variables w.r.t target","28feed8d":"<h1 style=\"color: black; background-color: yellow; text-align:left ; padding: 5px\">Hyperparameter tuning<\/h1>\n<p style=\"font-size: 18px; background-color: lightgray; padding: 20px\">\nHyperparameter tuning is choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a model argument whose value is set before the learning process begins.<br>\nExamples: <br>Learning rate for logistic regression,<br> No. of leaf nodes in decision tree, <br>No. of trees in random forest, etc.."}}