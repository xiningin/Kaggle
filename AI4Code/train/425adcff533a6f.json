{"cell_type":{"10e242ee":"code","5697a969":"code","29c78d67":"code","0782c95b":"code","b97b83b4":"code","7e628718":"code","ed720243":"code","36e627ea":"code","1a565f04":"code","fb31cb19":"code","faab7b93":"code","f49edb56":"code","bdb03c44":"code","8a9e6545":"code","dc4cf77c":"code","a4317758":"code","15e3dbd9":"code","5df1f973":"code","8e0fd574":"code","fa236ae3":"code","49d63bb4":"code","8154bd34":"code","9255c4ae":"markdown","f45c587a":"markdown","f62d8d69":"markdown","3d279978":"markdown","93ab99ec":"markdown","9e167e80":"markdown","1418baa3":"markdown","fd2162c4":"markdown","883f8e3f":"markdown","a3ddb9de":"markdown","bcfcfcf2":"markdown","b661767a":"markdown","e66092eb":"markdown","babaa1b6":"markdown","06e1ddcf":"markdown"},"source":{"10e242ee":"import os\nimport random\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nimport optuna\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5697a969":"# Tabular data file paths\nTRAIN_DATA_PATH = '..\/input\/petfinder-pawpularity-score\/train.csv'\nTEST_DATA_PATH = '..\/input\/petfinder-pawpularity-score\/test.csv'","29c78d67":"TARGET_NAME = 'Pawpularity'\nVAL_SIZE = 0.15\nSEED = 5\nEARLY_ROUNDS = 50","0782c95b":"def set_seed(seed=42):\n    \"\"\"Utility function to use for reproducibility.\n    :param seed: Random seed\n    :return: None\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\ndef set_display():\n    \"\"\"Function sets display options for charts and pd.DataFrames.\n    \"\"\"\n    # Plots display settings\n    plt.style.use('fivethirtyeight')\n    plt.rcParams['figure.figsize'] = 12, 8\n    plt.rcParams.update({'font.size': 14})\n    # DataFrame display settings\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.max_rows', None)\n    pd.options.display.float_format = '{:.4f}'.format\n\n\ndef get_features(df: pd.DataFrame) -> list:\n    \"\"\"Function selects input features from a DataFrame.\n    :param df: DataFrame containing features, Ids and possibly target values\n    :return: List of input features\n    \"\"\"\n    return [column for column in df.columns\n            if column != 'Id' and column != TARGET_NAME]\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Function adds new features to the DataFrame\n    by summing up existing features. Uses variable \"features\"\n    defined outside the scope of this function.\n    :param df: Original DataFrame\n    :return: Updated DataFrame\n    \"\"\"\n    # Normalized sum of all original features\n    df['features_sum'] = df[features].sum(axis=1) \/ len(features)\n\n    # Feature pairs (normalized)\n    for i in range(len(features) - 1):\n        for j in range(i + 1, len(features)):\n            feature_1 = features[i]\n            feature_2 = features[j]\n            df[f'{feature_1}_{feature_2}'] = (df[feature_1] + df[feature_2]) \/ 2\n\n    # Feature triplets (normalized)\n    for i in range(len(features) - 2):\n        for j in range(i + 1, len(features) - 1):\n            for z in range(j + 1, len(features)):\n                feature_1 = features[i]\n                feature_2 = features[j]\n                feature_3 = features[z]\n                df[f'{feature_1}_{feature_2}_{feature_3}'] = (\n                    df[feature_1] + df[feature_2] + df[feature_3]) \/ 3\n\n    return df\n\n\ndef rmse(y_true, y_pred) -> float:\n    \"\"\"Function calculates Root Mean Squared Error\n    for predicted and actual values.\n    :param y_true: Actual values\n    :param y_pred: Predicted values\n    :return: RMSE value\n    \"\"\"\n    return np.sqrt(np.mean(np.square(y_true - y_pred)))\n\n\ndef objective(trial):\n    \"\"\"Function performs trials of parameter optimization\n    for XGBoost model.\n    :param trial: optuna trial object\n    :return: RMSE score\n    \"\"\"\n    global model\n    params = {\n        'tree_method': 'gpu_hist',\n        'predictor': 'gpu_predictor',\n        'objective': 'reg:squarederror',\n        'booster': 'gbtree',\n        'n_estimators': trial.suggest_int('n_estimators', 250, 10_000, 250),\n        'reg_lambda': trial.suggest_int('reg_lambda', 1, 100),\n        'reg_alpha': trial.suggest_int('reg_alpha', 1, 100),\n        'subsample': trial.suggest_float('subsample', 0.1, 1.0, step=0.1),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0, step=0.1),\n        'max_depth': trial.suggest_int('max_depth', 1, 15),\n        'min_child_weight': trial.suggest_int('min_child_weight', 5, 100, step=5),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.95),\n        'gamma': trial.suggest_float('gamma', 0.0, 5.0)\n    }\n\n    fit_params = dict(eval_set=[(valid_x, valid_y)], eval_metric='rmse',\n                      early_stopping_rounds=EARLY_ROUNDS, verbose=False)\n\n    pruning_callback = optuna.integration.XGBoostPruningCallback(\n        trial, 'validation_0-rmse')\n\n    model = XGBRegressor(**params)\n    model.fit(train_x, train_y, **fit_params, callbacks=[pruning_callback])\n    y_pred = model.predict(valid_x)\n    val_rmse = rmse(valid_y, y_pred)\n\n    return val_rmse","b97b83b4":"set_seed(SEED)\nset_display()","7e628718":"# Train data set\ndata_train = pd.read_csv(TRAIN_DATA_PATH)\nprint(f'Train data shape: {data_train.shape}')\ndata_train.head()","ed720243":"# Test data set\ndata_test = pd.read_csv(TEST_DATA_PATH)\nprint(f'Test data shape: {data_test.shape}')\ndata_test.head()","36e627ea":"# Distribution of the target values\nprint(f'Target values: {data_train[TARGET_NAME].min()} - {data_train[TARGET_NAME].max()}\\n'\n      f'Mean value: {data_train[TARGET_NAME].mean()}\\n'\n      f'Median value: {data_train[TARGET_NAME].median()}\\n'\n      f'Standard deviation: {data_train[TARGET_NAME].std()}')\n\nsns.histplot(data=data_train, x=TARGET_NAME, kde=True)\nplt.axvline(data_train[TARGET_NAME].mean(), c='orange', ls='-', lw=3, label='Mean')\nplt.axvline(data_train[TARGET_NAME].median(), c='green', ls='-', lw=3, label='Median')\nplt.legend()\nplt.title('Pawpularity Score')\nplt.tight_layout()\nplt.show()","1a565f04":"# Binary features correlation with the target.\ncorrelation = data_train.corr()\nax = sns.heatmap(correlation, center=0, annot=True, cmap='RdBu_r', fmt='0.3f')\nl, r = ax.get_ylim()\nax.set_ylim(l + 0.5, r - 0.5)\nplt.yticks(rotation=0)\nplt.title('Correlation Matrix')\nplt.show()\n\ncorrelation[TARGET_NAME].sort_values()","fb31cb19":"# Before crossing the features we need to transform negatively correlated features\n# into positively correlated. We do it by switching 0 and 1 values.\nneg_features = correlation[correlation[TARGET_NAME] < 0].index.to_list()\ndata_train[neg_features] = data_train[neg_features].apply(lambda x: (x + 1) % 2)\ndata_test[neg_features] = data_test[neg_features].apply(lambda x: (x + 1) % 2)","faab7b93":"# List of original input features\nfeatures = get_features(data_train)\n\n# Add new features\ndata_train = add_features(data_train)\ndata_test = add_features(data_test)","f49edb56":"# Check correlation of the new features with the target.\ncorrelation = data_train.corr()\ncorrelation[TARGET_NAME].sort_values()","bdb03c44":"# Updated input features\nfeatures = get_features(data_train)\n\n# Split the data into train and validation sets\ny = data_train[TARGET_NAME]\nx = data_train[features]\n\ntrain_x, valid_x, train_y, valid_y = train_test_split(\n    x, y, test_size=VAL_SIZE, shuffle=True, random_state=SEED)\nprint(f'Train data shape: {train_x.shape}\\n'\n      f'Validation data shape: {valid_x.shape}')","8a9e6545":"# Train the base model\nxgb_model = XGBRegressor(tree_method='gpu_hist', predictor='gpu_predictor',\n                         objective='reg:squarederror', booster='gbtree')\n\nxgb_model.fit(train_x, train_y, eval_set=[(valid_x, valid_y)],\n              eval_metric='rmse', early_stopping_rounds=EARLY_ROUNDS)","dc4cf77c":"# Check the feature importance.\nimportance = pd.DataFrame({\n    'features': features,\n    'importance': xgb_model.feature_importances_\n})\nimportance.sort_values(by='importance', inplace=True)\n\nplt.barh([i for i in range(len(importance))], importance['importance'])\nplt.title('XGBoost Feature Importance')\nplt.show()","a4317758":"# Select informative features.\nthreshold = 0.005\nimportance = importance[importance['importance'] >= threshold]\nplt.figure(figsize=(12, 16))\nplt.barh(importance['features'], importance['importance'])\nplt.title('XGBoost Feature Importance')\nplt.savefig('features.png', dpi=300)\nplt.show()","15e3dbd9":"# Split the data using only selected features.\nfeatures = importance['features'].to_list()\nx = data_train[features]\n\ntrain_x, valid_x, train_y, valid_y = train_test_split(\n    x, y, test_size=VAL_SIZE, shuffle=True, random_state=SEED)\nprint(f'Train data shape: {train_x.shape}\\n'\n      f'Validation data shape: {valid_x.shape}')","5df1f973":"# Search for optimal parameters\nstudy = optuna.create_study(\n    sampler=optuna.samplers.TPESampler(seed=SEED),\n    direction='minimize',\n    study_name='xgb')","8e0fd574":"study.optimize(objective, n_trials=200)","fa236ae3":"xgb_params = study.best_params\nprint('XGBoost best RMSE:', study.best_value)\nprint('Optimal parameters:')\nfor key, value in xgb_params.items():\n    print(f'\\t{key}: {value}')","49d63bb4":"# Retrain the model with the best parameters\nxgb_model = XGBRegressor(tree_method='gpu_hist', predictor='gpu_predictor',\n                         objective='reg:squarederror', booster='gbtree',\n                         **xgb_params)\n\nxgb_model.fit(train_x, train_y, eval_set=[(valid_x, valid_y)],\n              eval_metric='rmse', early_stopping_rounds=EARLY_ROUNDS)\n\nprint('Validation RMSE:', xgb_model.best_score)","8154bd34":"# Predict values for the test set.\ndata_test[TARGET_NAME] = xgb_model.predict(data_test[features])\ndata_test[['Id', TARGET_NAME]].to_csv('submission.csv', index=False)\ndata_test[['Id', TARGET_NAME]].head()","9255c4ae":"## Exploratory Data Analysis","f45c587a":"We will try 200 various combinations of hyper-parameters defined in the objective() function using early stopping to prevent the model from overfitting and pruning callback to stop unpromising trials early and save time.","f62d8d69":"## Feature Selection","3d279978":"All binary features have very low correlation with the target value. The most negatively correlated feature is \"Blur\", and the most positively correlated feature is \"Group\".\n\nSome feature pairs demonstrate correlation to each other like \"Blur\" and \"Eyes\", \"Face\" and \"Eyes\", \"Info\" and \"Collage\", \"Occlusion\" and \"Human\". We will test the hypothesis that by combining all or some of the features we can obtain more meaningful parameters for predicting \"Powpolarity score\".","93ab99ec":"We cross the original features to produce new aggregate parameters, which would demonstrate higher correlation with the target.\n\nWe apply several basic approaches:\n- Simple sum of all binary features divided by the number of features. Assumption: the higher the aggregated score - the more popular the pet should be.\n- Normalized sum of feature pairs and triplets.\n\nXGBoost does not require input features to be normalized, but we keep new features scaled to the range between 0 and 1 in case we fit any other models on this data in the future.","9e167e80":"## Inference","1418baa3":"After the feature engineering we have a large set of features. Some of them are more useful that others. We need to discard the less informative features to make the training faster and avoid noise in the data. For this purpose we will train a base XGBoost model and use feature importance to select useful parameters in the train set.","fd2162c4":"Significant number of input features have zero importance to the model. We will filter out insignificant features applying a threshold of 0.005.","883f8e3f":"Target values are unevenly distributed. Train set contains a small portion of samples describing images of pet that are very popular (near 100 score). Most of the pets are scored below the mean \"Powpularity score\".","a3ddb9de":"## Feature Engineering","bcfcfcf2":"## Functions","b661767a":"The notebook illustrates how to apply feature engineering and feature selection techniques and create an XGBoost model with optimal hyper-parameters to make a prediction on pet \"Powpularity score\" based only on the tabular data from a csv file.\n\nThis model could be used as a supplement to the image regression model.","e66092eb":"# Feature Engineering + Optimized XGBoost","babaa1b6":"From the sorted feature-target correlation matrix we can see that feature engineering in fact produced new parameters that demonstrate higher correlation with the target compared to single features from the original data set.","06e1ddcf":"## Optimization of Hyper-Parameters"}}