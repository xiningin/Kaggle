{"cell_type":{"6477247b":"code","b1cf368b":"code","62ca14a3":"code","3d088b57":"code","d4557e90":"code","101a0bc1":"code","2ac06c8c":"code","4f41e527":"code","7941cda3":"code","85c94619":"code","9d78820a":"code","cccdd2e6":"code","253d9548":"code","d819295b":"code","dceba245":"code","734d7d9e":"code","19ef350e":"code","c9ff5bac":"code","e58d4482":"code","5225c480":"code","a043d0d2":"code","8865bc29":"code","251d7990":"code","fde663d6":"code","2d7d0fb6":"code","fa9a8157":"code","cecd914d":"code","3b67e350":"code","93254618":"code","9243ae5e":"code","8a1304e4":"code","760522f0":"code","de207447":"code","c6f0680c":"code","ffa4574b":"code","8423d48f":"code","8e675fd0":"code","812b694e":"code","8ee976ff":"code","292106a7":"code","e293ed96":"code","09fc2483":"code","3bc3cca1":"code","a000cdc0":"code","58ba040a":"code","f6a1e748":"code","231238d8":"code","c13df127":"code","54b48f0a":"code","a9171449":"code","a8c99ad3":"code","ac3388b4":"code","349c2bdf":"code","fa43217d":"code","28309816":"code","a4028776":"code","77b3fc7f":"code","9f8ce872":"code","42267319":"code","9ac5eff0":"code","2d503f07":"code","8085a818":"code","e82dde15":"code","2f533568":"code","4c53f935":"code","68b8feb1":"code","36125b05":"code","6b67eb6f":"markdown","fb7ee858":"markdown","fc6a4679":"markdown","2dfd0531":"markdown","a1d28f6f":"markdown","0d9cc049":"markdown","997d0e60":"markdown","baf8339c":"markdown","e1721c95":"markdown","0abd7b97":"markdown","80af424e":"markdown","4b18ff32":"markdown","e6528eeb":"markdown","85a396d0":"markdown","76b8c17f":"markdown","0369b16c":"markdown","c7ddb0fe":"markdown","a1467d60":"markdown","5b964a8d":"markdown","443038a8":"markdown","5e8018ce":"markdown","baae9d37":"markdown","30761351":"markdown","3c536427":"markdown","45e08b61":"markdown","4f9a7db8":"markdown","a13ce11f":"markdown","9dce3ddb":"markdown","2e5a9add":"markdown","fe7ef5e3":"markdown","2c042fee":"markdown","9056d198":"markdown","1acee6c2":"markdown","103731d3":"markdown","1a78bb76":"markdown","fad35c7e":"markdown","65c2ae6e":"markdown","b3c75552":"markdown","8b1960db":"markdown","34a4c10d":"markdown","7c646385":"markdown","36ed3168":"markdown","1773ca7f":"markdown","712ad943":"markdown","0140046e":"markdown","41807507":"markdown","dffc9f0f":"markdown","12948b7d":"markdown","455bf818":"markdown","9db227ca":"markdown","2cd11ff6":"markdown","070122ce":"markdown","a9914814":"markdown","f7be28e4":"markdown","ef6e7a62":"markdown","73ec93dd":"markdown","3716a92c":"markdown","219e6047":"markdown","e0d79690":"markdown","2c5aa6fa":"markdown"},"source":{"6477247b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b1cf368b":"import pandas as pd\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")","62ca14a3":"train.head(), train.describe()","3d088b57":"test.head(), test.describe()","d4557e90":"train.isnull().sum()","101a0bc1":"test.isnull().sum()","2ac06c8c":"def null_percentage (df):\n    for column in df:\n        print(column +':', 100 * df[column].isnull().sum()\/len(df[column]))","4f41e527":"null_percentage(train)","7941cda3":"null_percentage(test)","85c94619":"train.drop(['PassengerId', 'Cabin', 'Ticket'], axis=1, inplace=True)\ntest.drop(['Cabin', 'Ticket'], axis=1, inplace=True)","9d78820a":"train.Age.fillna(train['Age'].median(), inplace=True), test.Age.fillna(test['Age'].median(), inplace=True)","cccdd2e6":"train.Embarked.fillna(train['Embarked'].mode()[0], inplace=True)","253d9548":"train.Fare.fillna(train['Fare'].median(), inplace=True), test.Fare.fillna(test['Fare'].median(), inplace=True)","d819295b":"train.info(), train.describe()","dceba245":"test.info(), test.describe()","734d7d9e":"for data in [train, test]:\n    data['family_members'] = data['SibSp'] + data['Parch'] + 1\n    data['single'] = data['family_members'].map(lambda s: 1 if s == 1 else 0)\n    data['small_fam'] = data['family_members'].map(lambda s: 1 if  s == 2  else 0)\n    data['med_fam'] = data['family_members'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n    data['large_fam'] = data['family_members'].map(lambda s: 1 if s >= 5 else 0)\n    data['fare_bin'] = pd.qcut(data['Fare'], 4, labels=False)\n    data['age_bin'] = pd.cut(data['Age'].astype(int), 5, labels=False)","19ef350e":"sex_dummy_train= pd.get_dummies(train['Sex'], prefix='Sex')\ntrain = pd.concat([train, sex_dummy_train], axis=1)\nsex_dummy_test = pd.get_dummies(test['Sex'], prefix='Sex')\ntest = pd.concat([test, sex_dummy_test], axis=1)","c9ff5bac":"embarked_dummy_train= pd.get_dummies(train['Embarked'], prefix='Embarked')\ntrain = pd.concat([train, embarked_dummy_train], axis=1)\nembarked_dummy_test = pd.get_dummies(test['Embarked'], prefix='Embarked')\ntest = pd.concat([test, embarked_dummy_test], axis=1)","e58d4482":"pclass_dummy_train= pd.get_dummies(train['Pclass'], prefix='Pclass')\ntrain = pd.concat([train, pclass_dummy_train], axis=1)\npclass_dummy_test = pd.get_dummies(test['Pclass'], prefix='Pclass')\ntest = pd.concat([test, pclass_dummy_test], axis=1)","5225c480":"for data in [train, test]:\n    data_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in data[\"Name\"]]\n    data[\"Title\"] = pd.Series(data_title)\n    data[\"Title\"].head()\n    data[\"Title\"] = data[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    data[\"Title\"] = data[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n    data[\"Title\"] = data[\"Title\"].astype(int)\n    data.drop('Name', axis=1, inplace=True)","a043d0d2":"#train['Cabin'] = pd.Series(i[0] if not pd.isnull(i) else 'X' for i in train['Cabin'])\n#train_dummies = pd.get_dummies(train['Cabin'], prefix='cabin')\n#train = pd.concat([train, train_dummies], axis=1).drop('Cabin', axis=1)\n\n#test['Cabin'] = pd.Series(i[0] if not pd.isnull(i) else 'X' for i in test['Cabin'])\n#test_dummies = pd.get_dummies(test['Cabin'], prefix='cabin')\n#test = pd.concat([test, test_dummies], axis=1).drop('Cabin', axis=1)","8865bc29":"#ticket = []\n#for i in list(train.Ticket):\n    #if not i.isdigit() :\n        #ticket.append(i.replace('.', '').replace('\/', '').strip().split(' ')[0])\n    #else:\n        #ticket.append('X')\n#train['Ticket'] = ticket\n#train['Ticket'].head()\n#train_dummies = pd.get_dummies(train['Ticket'], prefix='T')\n#train = pd.concat([train, train_dummies], axis=1).drop('Ticket', axis=1)\n\n#ticket = []\n#for i in list(test.Ticket):\n    #if not i.isdigit() :\n        #ticket.append(i.replace('.', '').replace('\/', '').strip().split(' ')[0])\n    #else:\n        #ticket.append('X')\n#test['Ticket'] = ticket\n#test['Ticket'].head()\n#test_dummies = pd.get_dummies(test['Ticket'], prefix='T')\n#test = pd.concat([test, test_dummies], axis=1).drop('Ticket', axis=1)","251d7990":"train.head(), test.head()","fde663d6":"train.head(), test.head()","2d7d0fb6":"train.Sex.value_counts()","fa9a8157":"import seaborn as sns\nsns.set(style=\"darkgrid\")\nsns.countplot(x='Survived', hue='Sex', data=train)","cecd914d":"sns.violinplot(x='Sex', y='Age', hue='Survived', data=train)","3b67e350":"train.Pclass.value_counts()","93254618":"sns.catplot(x='Sex', hue='Pclass', col='Survived', kind='count', data=train)","9243ae5e":"sns.catplot(x='Embarked', hue='Pclass', col='Survived', kind='count', data=train)","8a1304e4":"sns.violinplot(x='Pclass', y='Age', hue='Survived', data=train)","760522f0":"train.Age.value_counts()","de207447":"sns.distplot(train.Age, bins=20, kde=False, rug=True)","c6f0680c":"sns.pairplot(train[['Survived', 'Age', 'Sex', 'Pclass', 'SibSp', 'Parch', 'Fare']])","ffa4574b":"import matplotlib.pyplot as plt\n_ , ax = plt.subplots(figsize =(14, 12))\ncolormap = sns.diverging_palette(220, 10, as_cmap = True)\n_ = sns.heatmap(train.corr(), square=True, annot=True, cmap=colormap)","8423d48f":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nval_test = test.drop('PassengerId', axis=1)\ntrain[['Sex', 'Embarked', 'age_bin', 'fare_bin']] = ordinal_encoder.fit_transform(train[['Sex', 'Embarked', 'age_bin', 'fare_bin']]).astype('int64')\nval_test[['Sex', 'Embarked', 'age_bin', 'fare_bin']] = ordinal_encoder.transform(val_test[['Sex', 'Embarked', 'age_bin', 'fare_bin']]).astype('int64')","8e675fd0":"from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nX = train.drop('Survived', axis=1)\ny = train['Survived']\n#X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\nfor train_index, test_index in split.split(X, y):\n    X_train, X_test = X.loc[train_index], X.loc[test_index]\n    y_train, y_test = y.loc[train_index], y.loc[test_index]","812b694e":"train.Survived.value_counts()\/len(train)","8ee976ff":"y_train.value_counts() \/len(y_train), y_test.value_counts() \/ len(y_test)","292106a7":"from sklearn.preprocessing import StandardScaler\nstandard_scaler = StandardScaler()\nX_train = pd.DataFrame(standard_scaler.fit_transform(X_train), columns = X_train.columns)\nX_test = pd.DataFrame(standard_scaler.transform(X_test), columns = X_test.columns)\nval_test = pd.DataFrame(standard_scaler.transform(val_test), columns = val_test.columns)","e293ed96":"X_train.head(), X_test.head(),","09fc2483":"import time\nfrom sklearn import tree\nfrom sklearn.metrics import mean_squared_error, confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import LinearSVC, SVC\nimport xgboost\nrandom_state=2\nmodels = [tree.DecisionTreeClassifier(random_state=random_state), RandomForestClassifier(random_state=random_state), SGDClassifier(random_state=random_state), \n          LinearSVC(random_state=random_state, max_iter=10000), SVC(random_state=random_state, max_iter=10000),\n            xgboost.XGBClassifier(random_state=random_state), AdaBoostClassifier(tree.DecisionTreeClassifier(random_state=random_state), random_state=random_state, learning_rate=0.1), \n          ExtraTreesClassifier(random_state=random_state), GradientBoostingClassifier(random_state=random_state)]\ncolumns = ['Name', 'Score', 'RMSE', 'Precision', 'Recall', 'F1 Score']\nmodels_compare = pd.DataFrame(columns=columns)\ni=0\nfor model in models:\n    start_time = time.time()\n    clf = model\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n    models_compare.loc[i, 'Name'] = clf.__class__.__name__\n    models_compare.loc[i, 'Score'] = clf.score(X_test, y_test)\n    models_compare.loc[i, 'RMSE'] = np.sqrt(mean_squared_error(y_test, predictions))\n    models_compare.loc[i, 'Precision'] = precision_score(y_test, predictions)\n    models_compare.loc[i, 'Recall'] = recall_score(y_test, predictions)\n    models_compare.loc[i, 'F1 Score'] = f1_score(y_test, predictions)\n    models_compare.loc[i, 'Execution time'] = time.time()- start_time\n    i+=1\nmodels_compare.sort_values(by='Score', ascending=False)","3bc3cca1":"models_compare = models_compare.drop('Execution time', axis=1)\ndf = models_compare.melt('Name', var_name='Metrics',  value_name='')\nimport matplotlib.pyplot as plt\n\ng = sns.catplot(x=\"Name\", y=\"\", hue='Metrics', kind='point', aspect=4, markers=\"o\", linestyles= \"--\", data=df)","a000cdc0":"from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\nmodels = [tree.DecisionTreeClassifier(random_state=random_state), RandomForestClassifier(random_state=random_state), SGDClassifier(random_state=random_state), \n          LinearSVC(random_state=random_state, max_iter=10000), SVC(random_state=random_state, max_iter=10000),\n            xgboost.XGBClassifier(random_state=random_state), AdaBoostClassifier(tree.DecisionTreeClassifier(random_state=random_state), random_state=random_state, learning_rate=0.1), \n          ExtraTreesClassifier(random_state=random_state), GradientBoostingClassifier(random_state=random_state)]\ncolumns = ['Name', 'Score', 'RMSE', 'Precision', 'Recall', 'F1 Score']\nmodels_compare_cv = pd.DataFrame(columns=columns)\ni=0\nkfold = StratifiedKFold(n_splits=10)\nfor model in models: \n    start_time = time.time()\n    clf = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    predictions = cross_val_predict(model, X_test, y_test, cv=5)\n    mse = mean_squared_error(y_test, predictions)\n    rmse =np.sqrt(mse)\n    precision = precision_score(y_test, predictions)\n    recall = recall_score(y_test, predictions)\n    models_compare_cv.loc[i, 'Name'] = model.__class__.__name__\n    models_compare_cv.loc[i, 'Score'] = clf.mean()\n    models_compare_cv.loc[i, 'RMSE'] = rmse\n    models_compare_cv.loc[i, 'Precision'] = precision\n    models_compare_cv.loc[i, 'Recall'] = recall\n    models_compare_cv.loc[i, 'F1 Score'] = f1_score(y_test, predictions)\n    models_compare_cv.loc[i, 'Execution time'] = time.time()- start_time\n    i+=1\nmodels_compare_cv.sort_values(by='Score', ascending=False)","58ba040a":"models_compare_cv = models_compare_cv.drop('Execution time', axis=1)\ndf = models_compare_cv.melt('Name', var_name='Metrics',  value_name='')\ng = sns.catplot(x=\"Name\", y=\"\", hue='Metrics', kind='point', aspect=4, markers=\"o\", linestyles= \"--\", data=df)","f6a1e748":"models_compare.sort_values('Score', ascending=False).Name.head()","231238d8":"models_compare_cv.sort_values('Score', ascending=False).Name.head()","c13df127":"from sklearn.model_selection import GridSearchCV\nforest_grid = {'n_estimators': [10,100,1000], 'max_features':[2,4,6,8], 'bootstrap': [False], 'min_samples_leaf': [2, 4, 6, 8], 'max_depth': [10, 20, 30, 40]},\nforest_reg = RandomForestClassifier(random_state=random_state)\ngrid_search_forest = GridSearchCV(forest_reg, forest_grid, cv=kfold, scoring='accuracy', return_train_score=True, n_jobs=-1)\nstart_time = time.time()\ngrid_search_forest.fit(X_train, y_train)\nprint('Execution time:', time.time() - start_time)","54b48f0a":"grid_search_forest.best_params_, grid_search_forest.best_score_","a9171449":"forest_be = grid_search_forest.best_estimator_","a8c99ad3":"linear_svc_grid = [\n    {'tol': [0.01, 0.1, 0.3, 0.5], 'loss': ['hinge', 'squared_hinge'], 'max_iter':[760000]}\n]\nlinear_svc = LinearSVC(random_state=random_state)\ngrid_search_linearsvc = GridSearchCV(linear_svc, linear_svc_grid, cv=kfold, scoring='accuracy', return_train_score=True, n_jobs=-1)\nstart_time = time.time()\ngrid_search_linearsvc.fit(X_train, y_train)\nprint('Execution time:', time.time() - start_time)","ac3388b4":"grid_search_linearsvc.best_params_, grid_search_linearsvc.best_score_","349c2bdf":"linearsvc_be = grid_search_linearsvc.best_estimator_","fa43217d":"gbc = GradientBoostingClassifier(random_state=random_state)\ngbc_grid = {'loss': ['deviance'], 'n_estimators' : [100,200,300],\n            'learning_rate' : [0.1, 0.05, 0.01], 'max_depth': [4,6,8],\n            'min_samples_leaf': [2,3,4], 'max_features': [1.0,0.3, 0.1]}\ngrid_search_gbc = GridSearchCV(gbc, param_grid = gbc_grid, cv=kfold, scoring='accuracy', return_train_score=True, n_jobs=-1)\nstart_time = time.time()\ngrid_search_gbc.fit(X_train, y_train)\nprint('Execution time:', time.time() - start_time)","28309816":"grid_search_gbc.best_params_, grid_search_gbc.best_score_","a4028776":"gbc_be = grid_search_gbc.best_estimator_","77b3fc7f":"svc = SVC(probability=True, random_state=random_state)\nsvc_grid = {'kernel': ['rbf'], 'gamma' : [0.001, 0.01, 0.1], 'C':[1,5,10]}\ngrid_search_svc = GridSearchCV(svc, param_grid=svc_grid, cv=kfold, scoring='accuracy')\nstart_time = time.time()\ngrid_search_svc.fit(X_train, y_train)\nprint('Execution time:', time.time() - start_time)","9f8ce872":"grid_search_svc.best_params_, grid_search_svc.best_score_","42267319":"svc_be = grid_search_svc.best_estimator_","9ac5eff0":"xgb = xgboost.XGBClassifier(random_state=random_state)\nxgb_grid = {'max_depth':[2,4,8], 'learning_rate':[0.001, 0.01,0.1,0.5], 'random_state': [random_state]}\ngrid_search_xgb = GridSearchCV(xgb, param_grid=xgb_grid, cv=kfold, scoring='accuracy')\nstart_time = time.time()\ngrid_search_xgb.fit(X_train, y_train)\nprint('Execution time:', time.time()- start_time)","2d503f07":"grid_search_xgb.best_params_, grid_search_xgb.best_score_","8085a818":"xgb_be = grid_search_xgb.best_estimator_","e82dde15":"from sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators= [('rf', forest_be), ('svc', svc_be), ('linear_svc', linearsvc_be), ('gb', gbc_be), ('xgb', xgb_be)], voting='hard')\nvoting_clf.fit(X_train, y_train)","2f533568":"from sklearn.metrics import accuracy_score\nfor clf in [forest_be, svc_be, linearsvc_be, gbc_be, xgb_be, voting_clf]:\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))","4c53f935":"test_final = val_test.copy()\nvoting_clf.fit(X_train, y_train)\nvoting_clf.score(test_final, gender_submission['Survived'])","68b8feb1":"test_final['Survived'] = voting_clf.predict(test_final)","36125b05":"submission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId'].copy().astype('int')\nsubmission['Survived'] = test_final['Survived']\nsubmission.to_csv('..\/working\/submit.csv', index=False)","6b67eb6f":"and visualize it, if the passenger survived (1) or not (0):","fb7ee858":"We will start with a list of basic ML classification models, and train and evaluate the models with the train and test data, respectively. In addition, the model score, RMSE, precision, recall and F1 Score will be computed for each model, providing a new dataframe that will allow a future comparision between different models.","fc6a4679":"# 4. ML Models","2dfd0531":"### 6.1.2 Linear SVC","a1d28f6f":"## 4.4 Defining, training and evaluating the standard models","0d9cc049":"Define a function to compute the percentage of null-values in each column:","997d0e60":"Now, it's the turn of scaling the features. If we pick two different numerical features, we may find that they have a very different scale: for example, Age can go up to 80, while Pclass has only 3 values (0,1,2). This difference in the numeric scale negatively impacts in the ML models performance, and that's why scaling it's so important. We use StandardScaler to scale all the numerical features (X). For this project, the labels (y) are 0 or 1, so they don't need to be scaled.","baf8339c":"and to the test set:","e1721c95":"We drop a list of columns as a base, but probably we will reinclude some of them again to improve the models.\nEDIT: we ended reincluding them, and using feature engineering to give more info and features to the ML models.","0abd7b97":"First, the categorical features (those that are not numerical), must be converted to numbers. We will use OrdinalEncoder to do it. Important: the OrdinalEncoder object must be fitted only to the train data, and then used to transform both the train and the test data:","80af424e":"Apply the function to the train set:","4b18ff32":"Most of the male passengers died, while the female passengers survived by ~ 70%.","e6528eeb":"With a Seaborn's violinplot we can see the relation between the Sex, Age and if the passenger survived or not.","85a396d0":"### 6.1.4 SVC","76b8c17f":"Fine. The proportion of 0 and 1 for the labels ('Survived') is conserved, and hence there is no introduced bias.","0369b16c":"# 2. Cleaning and preparing data","c7ddb0fe":"In both cases, with and without 5-fold cross-validations, the same 5 models perform the best once trained: LinearSVC, Gradient Boosting Classifier, SVC, Random Forest Classifier and XGBClassifier. Then, these models are selected to pass to the next phase, where we will perform a Grid Search to find the best hyperparameter configuration for each model.","a1467d60":"I use the *head* and *describe* methods to get a first sight of both datasets:","5b964a8d":"Once we finished the EDA, we can move to the maim goal of this project: the ML algorithms. Before starting to feed the models with the data, some steps are missing. We should preprocess the data in a way that improve the performance and accuracy of the proposed ML models.","443038a8":"We start by reading and loading the train and test datasets:","5e8018ce":"## 4.2 Preparing the datasets","baae9d37":"Next we are going to focus on the Class of the passenger. We compute a count as with Sex:","30761351":"## Class analysis","3c536427":"# 3. Exploratory Data Analysis (EDA)","45e08b61":"or the count per embarked port, class and survived or not:","4f9a7db8":"## Gender analysis","a13ce11f":"We start by the gender analysis. We compute the count of males and females:","9dce3ddb":"### 6.1.3 Gradient Boosting Classifier","2e5a9add":"## 6.1 Searching the best hyperparameters: Grid Search","fe7ef5e3":"and with a catplot we show the count per class, Sex and survived or not.","2c042fee":"## 2.4 Feature Engineering","9056d198":"Before starting to feed the ML algorithms with the data, we need to make some Exploratory Data Analysis (EDA) to gain a deep knowledge of the datasets, which can be helpful in the future.","1acee6c2":"Let's see the number of null values in each column in both datasets:","103731d3":"Now it's the turn of the Age. Again, we start with the count:","1a78bb76":"## 4.3 Scaling the numerical features","fad35c7e":"With these results, we can compare the accuracy of the models for each metric using Seaborn's catplot:","65c2ae6e":"## 2.2 Droping useless columns","b3c75552":"Apart from creating more useful features using imagination, we can pass from a categorical feature to encoded columns, as follows:\n* Sex: if the instance is a male or a female. We can pass from *male\/female* to 2 columns:\n    * if male, male=1 and female=0. If female, male=0 and female=1.\n* Embarked: the same as Sex, but with 3 options, so 3 new features.\n\nThis approach is very similar if not equal to One Hot Encoding.","8b1960db":"followed by a distplot with 20 bins:","34a4c10d":"## 2.3 Fill NaN values in the remaining columns","7c646385":"And now, the train and test datasets are 100% complete, without null values.","36ed3168":"## 4.1 Encoding categorical and text features","1773ca7f":"And we end using the 'Name' feature. We can use it to extract more information about the passenger.","712ad943":"### 6.1.5 XGBClassifier","0140046e":"and we obtain the following:","41807507":"# 1.  Importing the datasets and first sight","dffc9f0f":"The pairplot and the heatmap (of the correlation matrix) help us to look for correlations between the features, the provided and the created using feature engineering.","12948b7d":"In the case that a feature has a low percentage of null values, we can try to fill them using several methods: median, mode, regression models to predict the values... I choose to input the median for the numerical features and the mode for the categorical column:","455bf818":"## 2.1 Dealing with Null values","9db227ca":"## Final Model Submission","2cd11ff6":"### 6.1.1 Random Forest Classifier","070122ce":"## Age analysis","a9914814":"## 6.2 Ensemble Learning","f7be28e4":"# 5. Implementing Cross-Validation","ef6e7a62":"Once we finished we the feature engineering, we can use again the head and describe methods to review our datasets:","73ec93dd":"The next step, and a very important one, is feature engineering, which consists in creating new features from existing ones, always with the aim of improving the performance of the ML models, providing more information to feed them. Creating features as a hobby, if they are not useful, could end in a decrease of the accuracy while increasing the time consumption.\nI created the following features:\n* family_members: the number of family members of each instance. It's the result of suming 'SibSp' and 'Parch'.\n* single: wheter an instance has a family or not (single). It's a binary feature then, as will be some of the following.\n* small_fam: if the family is small (1), with 2 members, or not (0). \n* med_fam: if the family has 3-4 members (1), or not (0).\n* large_fam: if the family has 5+ members (1), or not (0).\n* fare_bin: bins for the 'Fare' feature. \n* age_bin: bins for the 'Age' feature.","3716a92c":"We see that the 'Cabin' column has a 77% of null values, while the 'Age' column is only 20% null. Knowing the % of null values for each feature will help in the task of discarding not useful features. In this case, as 'Cabin' has a lot of null values, maybe it's better to drop it. ","219e6047":"Now, we split the train data in train and test, and assign the labels ('Survived') to y, and the features to X. We assign 75% of the train data to train and 25% to test, using StratifiedShuffleSplit, to try to conserve the proportion of labels in all the sets, because we have a small dataset.","e0d79690":"# 6. Fine-tuning the models","2c5aa6fa":"and similar with the violinplot:"}}