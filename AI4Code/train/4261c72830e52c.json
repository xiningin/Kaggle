{"cell_type":{"8d4c4b7f":"code","71bb0f61":"code","0b8955f1":"code","a793a962":"code","2d784775":"code","b3789350":"code","d9e358d7":"code","01170775":"code","3ad1231e":"code","7a3e56fb":"code","713fd709":"code","f494d9be":"code","123fafc0":"code","246bcb00":"markdown","c6396145":"markdown","1d53dc9e":"markdown","83834473":"markdown","dde33b00":"markdown","d41379f9":"markdown","fbb492f9":"markdown","ce6fa9f4":"markdown","ea220b15":"markdown","17dd2bb8":"markdown","ed877cfc":"markdown","91f8a99b":"markdown"},"source":{"8d4c4b7f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","71bb0f61":"df = pd.read_csv('..\/input\/nips-papers-1987-2019-updated\/papers.csv')\ndf","0b8955f1":"import seaborn as sns\n\nsns.heatmap(df.isna())","a793a962":"%%time\n# 2min 30s\nimport nltk\nfrom gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short, preprocess_string\n\nlemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n\ndf['full_text'] = df['full_text'].astype('str')\ndf['full_text_tokenized'] = df['full_text'].apply(lambda text: preprocess_string(text, [\n    strip_tags, \n    strip_punctuation, \n    strip_multiple_whitespaces, \n    strip_numeric, \n    remove_stopwords, \n    strip_short, \n    lemmatizer.lemmatize, \n    lambda x: x.lower()\n]))","2d784775":"df['full_text_tokenized'].sample(n=20)","b3789350":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\ndf['full_text_tokenized_len'] = df['full_text_tokenized'].apply(lambda text: len(text))\nsns.histplot(df[df['full_text_tokenized_len'] > 0]['full_text_tokenized_len'], log_scale=True)\nplt.show()","d9e358d7":"%%time\n# 2min 50s\nfrom wordcloud import WordCloud\n\nlong_string = ' '.join([' '.join(words) for words in df['full_text_tokenized'].values])\nwordcloud = WordCloud(width=800, height=400)\nwordcloud.generate(long_string)\nwordcloud.to_image()","01170775":"%%time\n# 30s\nimport gensim\n\ndictionary = gensim.corpora.Dictionary(df['full_text_tokenized'].values)\ndictionary.filter_extremes(no_below=20, no_above=0.5)","3ad1231e":"%%time\n# 13.2s\ncorpus = [dictionary.doc2bow(doc) for doc in df['full_text_tokenized'].values]","7a3e56fb":"print(f'Number of unique tokens: {len(dictionary):,}')\nprint(f'Number of documents: {len(corpus):,}')","713fd709":"%%time \n# 13mins\nimport logging\nfrom gensim.models.ldamulticore import LdaMulticore\n\n# Take too much time for kaggle save version. Set it to True during development.\nenable_debug = False\n\nif enable_debug:\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n\nmodel = LdaMulticore(corpus, num_topics=10, id2word=dictionary, passes=40, iterations=100)\n\nif enable_debug:\n    logger.setLevel(logging.WARNING)    ","f494d9be":"model.print_topics(num_topics=10)","123fafc0":"%%time\nimport pyLDAvis\nimport pyLDAvis.gensim\n\nprep_display = pyLDAvis.gensim.prepare(model, corpus, dictionary)\npyLDAvis.display(prep_display)","246bcb00":"Preprocess the `full_text` column with a series of functions. <br>\nThe operations are quite obvious from their function names so not to be repeated here. <br>\nLemmatization instead of Port Stemmer is used to preserve more meaningful full words from the documents. <br>\nNoun is used as the part of speech in lemmatization.","c6396145":"<a id=\"#section-1\"><\/a>\n# Environment Setup","1d53dc9e":"# Topic Modelling on NeurIPS Papers\n\nIn this notebook, we will explore a dataset containing more than 9,000 documents, which are papers from <br>\nThe Conference and Workshop on Neural Information Processing Systems (abbreviated as NeurIPS and formerly NIPS). <br>\nIt is a machine learning and computational neuroscience conference. <br>\n<br>\nWe will conduct LDA topic modelling on these papers, and explore the groups in an interative manner.\n\nTable of Content\n* Environment Setup\n* Load and Preprocess Data\n* Word Cloud\n* LDA Topic Modelling\n* Result Visualisation\n* Further Study","83834473":"# Thank you for reading\n\nLet me know your thoughts in the comments below :D ","dde33b00":"<a id=\"#section-2\"><\/a>\n# Load and Preprocess Data","d41379f9":"<a id=\"#section-5\"><\/a>\n# Result Visualisation\n\nHere comes the fruit. <br>\nWe will use gensim model's `print_topics` function to see popular terms in each group. <br>\nAlso, pyLDAvis will be used to see the groups in a graph. <br>","fbb492f9":"<a id=\"#section-3\"><\/a>\n# Word Cloud\nNext, the typical word cloud. <br>\nNot surpisingly, machine learning, processing system, neural network, information processing, reinforcement learning, loss function are some common terms.","ce6fa9f4":"<a id=\"#section-6\"><\/a>\n# Further Study\n\nHere are some possible directions to study the dataset further:\n* Change `num_topics` from 10 to 20, 50, etc to explore more detailed papers groupings\n* Compare and contrast the resulted groups using other simlarity algorithms such as TF-IDF, LSA","ea220b15":"A quick look on tokenized documents' lengths","17dd2bb8":"A quick look on missing data. <br>\nIn this notebook we will focus on topic-modelling on `full_text`[](http:\/\/) column only, which has no missing data. <br>\nGood to go.","ed877cfc":"<a id=\"#section-4\"><\/a>\n# LDA Topic Modelling\n\nWe will go through the following steps:\n* Create a \"dictionary\" containing all unique words in all documents\n* Create a \"corpus\". Each document will be converted into a bag of words, e.g. [(0, 1), (1, 1), (4, 2), ...]. <br> \nEach tuple means (word index, word occurrence in the document)\n* Train the LDA model. Tune hyper-parameter `passes` and `iterations` until most documents are \"converged\"\n* Visualise the result, exploring different groups of documents","91f8a99b":"`passess` and `iterations` are tuned to be high enough so that most of the documents are converged at the last pass.\nBelow are some logs when running the cell in DEBUG logging mode:\n\n> 2021-03-15 15:28:37,074 : DEBUG : 1666\/2000 documents converged within 100 iterations <br>\n2021-03-15 15:28:40,471 : DEBUG : 1509\/2000 documents converged within 100 iterations <br>\n2021-03-15 15:28:42,532 : DEBUG : 1454\/2000 documents converged within 100 iterations <br>\n2021-03-15 15:28:46,568 : DEBUG : 1489\/2000 documents converged within 100 iterations <br>\n2021-03-15 15:28:49,296 : DEBUG : 1265\/1680 documents converged within 100 iterations <br>"}}