{"cell_type":{"b7456ca4":"code","d07cc6c2":"code","dc158306":"code","8e38c3e2":"code","f53e4186":"code","9deb7f31":"code","3294f0a5":"code","c4daad8e":"code","fbfe9bd9":"code","c78cd6e8":"code","abc51113":"code","fb22cc31":"code","ccb2c321":"markdown","15c2c288":"markdown"},"source":{"b7456ca4":"import numpy as np \nimport torch\nfrom torch import nn\nimport torchvision\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom tqdm import trange","d07cc6c2":"train_set = torchvision.datasets.FashionMNIST(\n    root = '.\/data\/FashionMNIST',\n    train = True,\n    download = True,\n    transform = transforms.Compose([\n        transforms.ToTensor()\n    ])\n)\ntest_set = torchvision.datasets.FashionMNIST(\n    root = '.\/data\/FashionMNIST',\n    train = False,\n    download = True,\n    transform = transforms.Compose([\n        transforms.ToTensor()\n    ])\n)","dc158306":"train = torch.utils.data.DataLoader(train_set,batch_size=256) \ntest = torch.utils.data.DataLoader(test_set,batch_size=512) ","8e38c3e2":"#Kod sieci\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","f53e4186":"class PrimitiveNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten(start_dim=1)\n        self.layer = nn.Linear(784, 10)\n        self.softmax = nn.LogSoftmax(dim=1)\n        \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.layer(x)\n        x = self.softmax(x)\n        return x","9deb7f31":"class DeepNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten(start_dim=1)\n        self.input = nn.Linear(28*28, 128)\n        self.relu1 = nn.ReLU()\n        self.hidden = nn.Linear(128, 64)\n        self.relu2 = nn.ReLU()\n        self.output = nn.Linear(64, 10)\n        self.softmax = nn.LogSoftmax(dim=1)\n        \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.input(x)\n        x = self.relu1(x)\n        x = self.hidden(x)\n        x = self.relu1(x)\n        x = self.output(x)\n        x = self.softmax(x)\n        return x","3294f0a5":"c_model = nn.Sequential(\n    nn.Conv2d(1, 6, 5, padding=2),\n    nn.ReLU(),\n    nn.AvgPool2d(2, stride=2),\n    nn.Conv2d(6, 16, 5, padding=0),\n    nn.ReLU(),\n    nn.AvgPool2d(2, stride=2),\n    nn.Flatten(),\n    nn.Linear(400, 120),\n    nn.ReLU(),\n    nn.Linear(120, 84),\n    nn.ReLU(),\n    nn.Linear(84, 10),\n    nn.Softmax(dim=1)\n)","c4daad8e":"p_model = PrimitiveNet()\nd_model = DeepNet()","fbfe9bd9":"def train_simple_net(train, test, device, model):\n    model = model.to(device)\n    if model.__class__.__name__ == \"CNN\":\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    else:\n        criterion = nn.NLLLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n\n    all_accuracies_train = []\n    all_accuracies_test = []\n\n    num_epochs = 100\n\n    for _ in trange(num_epochs):\n        \n        correct_count = 0\n        all_count = 0\n        \n        for local_train_batch, local_train_labels in train:\n            local_train_batch, local_train_labels = local_train_batch.to(device), local_train_labels.to(device)\n\n            optimizer.zero_grad()\n            y_hat = model(local_train_batch)\n            loss = criterion(y_hat, local_train_labels)\n            loss.backward()\n            optimizer.step()\n            y_hat = torch.argmax(y_hat,axis=-1)\n            correct_count += torch.sum(torch.eq(y_hat,local_train_labels)).cpu().item()\n            all_count += local_train_labels.shape[-1]\n        \n        \n        all_accuracies_train.append(correct_count\/all_count)\n\n\n        correct_count = 0\n        all_count = 0\n\n        for local_test_batch, local_test_labels in test:\n            local_test_batch, local_test_labels = local_test_batch.to(device), local_test_labels.to(device)\n            with torch.no_grad():\n                preds = model(local_test_batch)\n            preds = torch.argmax(preds,axis=-1)\n            correct_count += torch.sum(torch.eq(preds,local_test_labels)).cpu().item()\n            all_count += local_test_labels.shape[-1]\n\n        all_accuracies_test.append(correct_count\/all_count) \n        \n    plt.plot(all_accuracies_train)\n    plt.plot(all_accuracies_test)\n    plt.legend(['train','test'])\n    plt.show()\n","c78cd6e8":"train_simple_net(train,test,device, p_model)","abc51113":"train_simple_net(train,test,device, d_model)","fb22cc31":"train_simple_net(train,test,device, c_model)","ccb2c321":"# Zadanie: Stw\u00f3rz klasow\u0105 wersj\u0119 modelu sekwencyjnego wzorowanego na architekturze lenet5","15c2c288":"# Wprowadzenie do konwolucji w Pytorchu\nW Pytorchu jak w prawie ka\u017cdej bibliotece uczenia g\u0142\u0119bokiego posiada warstw\u0119 implementuj\u0105c\u0105 konwolucje. Podstawowymi implementacjami konwolucji w Pytorchu s\u0105:\n- [Konwolucja 1d](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Conv1d.html) - Jest to konwolucja w pojedynczym wymiarze. Mo\u017ce ona s\u0142u\u017cy\u0107 do interpretacji wykres\u00f3w, operacjach na tek\u015bcie oraz szeregach czasowych. Obecnie istniej\u0105 ju\u017c lepsze implementacje dla wi\u0119kszo\u015bci podobnych problem\u00f3w.\n- [Konwolucja 2d](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Conv2d.html) - Jest to konwolucja w 2 wymiarach, a zarazem standardowa konwolucja om\u00f3wiona wcze\u015bniej. Implementacja ta najcz\u0119\u015bciej stosowana jest na obrazach, st\u0105d te\u017c posiada argumenty szczeg\u00f3lnie istotne dla przetwarzania obrazu, takie jak padding i stride. By wykorzysta\u0107 ka\u017cdy pixel naszego obrazu, mo\u017cliwe jest otoczeni obrazu arbitralnymi pixelami na czas konwolucji. Domy\u015bl\u0105 warto\u015bci\u0105 takiej otoczki jest 0, natomiast mo\u017cliwe jest wybranie innej arbitralnej warto\u015bci. Stride okre\u015bla, o ile przemieszcza si\u0119 okno naszej konwolucji, mo\u017ce by\u0107 to znowu dowolna warto\u015b\u0107, tak d\u0142ugo jak wymiary b\u0119d\u0105 si\u0119 zgadza\u0107.\n\n- [MaxPoolling (w tym przyk\u0142adzie 2d)](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.MaxPool2d.html) - Warstwa (max) pooling s\u0142u\u017cy do wyci\u0105gni\u0119cia (maksymalnej) warto\u015bci danego filtru, w celu pozyskania konkretnej cechy)\n\n![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20190721025744\/Screenshot-2019-07-21-at-2.57.13-AM.png)\n- [Konwolucja 3d](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Conv3d.html) - Jest to konwolucja w 3 wymiarze, s\u0142u\u017c\u0105ca do przemieszczania si\u0119 po 3 wymiarowych obiektach takich jak Skany, obiekty 3d czy zdj\u0119cia hiperspektralne.\n\n## Popularne architektury:\n- LeNet5 - Architektura z 1998 roku s\u0142u\u017c\u0105ca pierwotnie do rozr\u00f3\u017cniania cyfr kodu pocztowego. Jest to jedna z historycznie wa\u017cniejszych implementacji, gdy\u017c rozpropagowa\u0142a ona u\u017cycie sieci konwolucyjnych oraz model ten mo\u017ce s\u0142u\u017cy\u0107 za dobry punkt odniesienia przy por\u00f3wnywaniu prostych modeli.\n\n![](https:\/\/www.researchgate.net\/profile\/Sheraz-Khan-14\/publication\/321586653\/figure\/fig4\/AS:568546847014912@1512563539828\/The-LeNet-5-Architecture-a-convolutional-neural-network.png)\n\n- AlexNet - Jest to model z 2012 roku przestawiony w artykule \"ImageNet Classification with Deep Convolutional Neural Networks\". W tym modelu poza zwi\u0119kszonym rozmiarem zosta\u0142 dodany tak zwany overlapping pooling, kt\u00f3ry, w przeciwienstwie do poprzednich modeli korzysta\u0142 wiele razy z tych samych pixeli, w przeciwie\u0144stwie do standardowej jak na tamte czasy wersji poolingu z identycznym stridem. Model ten osi\u0105gn\u0105\u0142 b\u0142\u0119dy na poziomie 47.1% top-1 error oraz 28.2% top-5 error. By\u0142 do du\u017cy przeskok w por\u00f3wnaniu do poprzedniego rekordu, kt\u00f3ry wynosi\u0142 47.1% top-1 error i 28.2% top-5 error.\n\n![](https:\/\/www.researchgate.net\/profile\/Nicola-Strisciuglio\/publication\/339756908\/figure\/fig5\/AS:866265283457032@1583545146587\/AlexNet-architecture-used-as-the-baseline-model-for-the-analysis-of-results-on-the.png)\n\n- VGG16 - Model ten zosta\u0142 zaproponowany w artykule naukowym \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition\u201d. Mia\u0142 to by\u0107 krok w prz\u00f3d w por\u00f3wnaniu do poprzednich modeli i implementacji i osi\u0105ga\u0142 92.7% (top 5) accuracy na zbiorze testowym ImageNet. Jak na rok 2015 by\u0142 to bardzo imponuj\u0105cy wynik. Ze wzgl\u0119du na swoj\u0105 struktur\u0119 jest on bardzo wolny w treningu.\n\n![](https:\/\/www.researchgate.net\/profile\/Max-Ferguson\/publication\/322512435\/figure\/fig3\/AS:697390994567179@1543282378794\/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png)\n\n- Resnet(18) - W 2015 roku, powsta\u0142\u0105 kolejna architektura, kt\u00f3ra zrewolucjonizowa\u0142a nie tylko wizj\u0119, ale i nlp oraz wiele innych dziedzin. Pierwszy raz zamys\u0142 ten pojawi\u0142 si\u0119 w artykule \"Deep Residual Learning for Image Recognition\". Wprowadza\u0142 on po\u0142\u0105czenia pomi\u0119dzy kolejnymi blokami sieci, co niwelowa\u0142o du\u017c\u0105 cz\u0119\u015b\u0107 problem\u00f3w zwi\u0105zanych z zanikaj\u0105cym gradietnem. Dzi\u0119ki temu rozwi\u0105zaniu, a tak\u017ce ograniczeniu rozmiaru sieci powsta\u0142y kolejne modele nazwane kolejno Resnet18, Resnet50 oraz Resnet 101. By\u0142y to wcze\u015bniej niespotykane rozmiary sieci, kt\u00f3re pozwoli\u0142y na znaczny wzrost rozmiar\u00f3w modeli bez wcze\u015bniej wyst\u0119puj\u0105cych problem\u00f3w.\n\n![](https:\/\/www.researchgate.net\/profile\/Muhammad-Hasan-27\/publication\/323063171\/figure\/fig1\/AS:603178554904576@1520820382219\/Proposed-Modified-ResNet-18-architecture-for-Bangla-HCR-In-the-diagram-conv-stands-for.png)"}}