{"cell_type":{"469db743":"code","a201f35a":"code","fed6786f":"code","43599310":"code","c88f8a81":"code","24a8ecb5":"code","ece944ae":"code","6c608aa1":"code","42e5ed58":"code","4ac7bbd5":"code","119fcdc7":"code","3751b945":"code","ecf126b1":"code","7a449988":"code","e4473f54":"code","d63a43d7":"code","6c57ab1d":"code","819af96c":"code","c637706e":"code","9e5e8f3e":"code","2bcb1007":"code","52499c26":"code","95536f41":"code","1d39cfaa":"code","2d8a8815":"code","affeab81":"code","f471dde8":"code","989fccb4":"code","37e5d203":"code","bda11492":"code","c51b0602":"code","f1c1dd4f":"code","7c5d1f28":"code","ccda7351":"code","7cd0b5ec":"code","d58daeca":"markdown","86134f11":"markdown","3e0a990f":"markdown","41ad87f0":"markdown","7f6fcebd":"markdown","47b2d827":"markdown","9f325b45":"markdown","0c7e7280":"markdown","b238060e":"markdown"},"source":{"469db743":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom glob import glob\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nimport plotly.express as px #Plotly Express\n\nfrom plotly.offline import iplot\n#to link plotly to pandas\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline = False, world_readable = True)\n\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas()\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nplt.style.use('seaborn-whitegrid')\nsns.set_palette('Set2')\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\n\nimport os\nprint(os.listdir('..\/input\/'))\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))","a201f35a":"base_dir = '..\/input\/commonlitreadabilityprize\/'","fed6786f":"train = pd.read_csv(base_dir + 'train.csv')\nprint(train.shape)\ntrain.head()","43599310":"test = pd.read_csv(base_dir + 'test.csv')\nprint(test.shape)\ntest.head()","c88f8a81":"sub = pd.read_csv(base_dir + 'sample_submission.csv')\nprint(sub.shape)\nsub.head()","24a8ecb5":"print(f\"Number of unique id in trainset: {train['id'].nunique()}\")\nprint(f\"Number of unique id in testset: {test['id'].nunique()}\")","ece944ae":"missing = train.isna().sum().reset_index()\nmissing.columns = ['features', 'total_missing']\nmissing['percent'] = (missing['total_missing'] \/ len(train)) * 100\nmissing.index = missing['features']\ndel missing['features']\n\nmissing['total_missing'].iplot(kind = 'bar', \n                               title = 'Missing Values Count in train',\n                               xTitle = 'Features',\n                               colors = 'blue',\n                               yTitle = 'Count')\nmissing.T","6c608aa1":"missing = test.isna().sum().reset_index()\nmissing.columns = ['features', 'total_missing']\nmissing['percent'] = (missing['total_missing'] \/ len(test)) * 100\nmissing.index = missing['features']\ndel missing['features']\n\nmissing['total_missing'].iplot(kind = 'bar', \n                               title = 'Missing Values Count in test',\n                               xTitle = 'Features',\n                               colors = 'red',\n                               yTitle = 'Count')\nmissing.T","42e5ed58":"sns.kdeplot(train['target'], shade = True, color = 'green');","4ac7bbd5":"sns.kdeplot(train['standard_error'], shade = True, color = 'grey');","119fcdc7":"train.head(2)","3751b945":"train['excerpt_len'] = train['excerpt'].apply(lambda x: len(str(x)))\ntrain['excerpt_wordlen'] = train['excerpt'].apply(lambda x: len(str(x).split(' ')))\n\ntest['excerpt_len'] = test['excerpt'].apply(lambda x: len(str(x)))\ntest['excerpt_wordlen'] = test['excerpt'].apply(lambda x: len(str(x).split(' ')))","ecf126b1":"print(f\"Max. word length in train - Excerpt: {train['excerpt_wordlen'].max()}\")\nprint(f\"Min. word length in train - Excerpt: {train['excerpt_wordlen'].min()}\")\nprint()\nprint(f\"Max. word length in train - Excerpt: {test['excerpt_wordlen'].max()}\")\nprint(f\"Min. word length in train - Excerpt: {test['excerpt_wordlen'].min()}\")","7a449988":"plt.subplot(1, 2, 1)\nsns.distplot(train['excerpt_len'], bins = 50)\nplt.title('Train Character Length')\n\nplt.subplot(1, 2, 2)\nsns.distplot(train['excerpt_wordlen'], bins = 50)\nplt.title('Train Word Length');","e4473f54":"plt.subplot(1, 2, 1)\nsns.distplot(test['excerpt_len'], bins = 50)\nplt.title('Test Character Length')\n\nplt.subplot(1, 2, 2)\nsns.distplot(test['excerpt_wordlen'], bins = 50)\nplt.title('Test Word Length');","d63a43d7":"import itertools\nimport collections\nfrom collections import Counter\n\nfrom nltk.corpus import stopwords\n\nimport re\nfrom wordcloud import WordCloud\n\ndef plot_wordcloud(data, col, text = None):\n    stop = stopwords.words('english')\n    all_words = [word for each in data[col] for word in each.split(' ') if word not in stop]\n    word_freq = Counter(all_words)\n\n    wordcloud = WordCloud(width = 900,\n                          height = 500,\n                          max_words = 200,\n                          max_font_size = 100,\n                          relative_scaling = 0.5,\n                          background_color = \"rgba(255, 255, 255, 0)\", \n                          mode = \"RGBA\",\n                          normalize_plurals = True).generate_from_frequencies(word_freq)\n    plt.figure(figsize = (18, 16))\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.title(text, fontsize = 20, color = 'grey', y = 1.05)\n    plt.axis(\"off\")\n    plt.show()","6c57ab1d":"plot_wordcloud(train, 'excerpt', 'WordCloud of Train - Excerpt')","819af96c":"plot_wordcloud(test, 'excerpt', 'WordCloud of Test - Excerpt')","c637706e":"from transformers import *","9e5e8f3e":"model_name = 'bert-large-uncased'\nbert_url = '..\/input\/bert-base-uncased-huggingface-transformer\/'\n\nmax_len = train['excerpt_wordlen'].max()","2bcb1007":"tokenizer = BertTokenizer.from_pretrained(bert_url + 'bert-base-uncased-vocab.txt')","52499c26":"#https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\ndef bert_encode(texts, tokenizer, max_len = max_len):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        \n        text = text[:max_len - 2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","95536f41":"Xtrain = bert_encode(train['excerpt'].values, tokenizer, max_len = max_len)\ntargets = train['target'].values\n\nprint(Xtrain[0].shape, targets.shape)","1d39cfaa":"Xtest = bert_encode(test['excerpt'].values, tokenizer, max_len = max_len)\nprint(Xtest[0].shape)","2d8a8815":"epochs = 10","affeab81":"def build_model(max_len = max_len):\n    input_word_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_word_ids\")\n    input_mask = Input(shape = (max_len,), dtype = tf.int32, name = \"input_mask\")\n    segment_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"segment_ids\")\n    \n    config = BertConfig()\n    config.output_hidden_states = False\n    \n    bert_model = TFBertModel.from_pretrained(\n        bert_url + 'bert-base-uncased-tf_model.h5', config = config)\n\n    sequence_output = bert_model([input_word_ids, input_mask, segment_ids])[0]\n    \n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    out = tf.keras.layers.Dense(1, activation = 'linear')(x)\n    \n    model = Model(inputs = [input_word_ids, input_mask, segment_ids], outputs = out)\n    \n    model.compile(Adam(lr = 1e-5), loss = tf.keras.losses.MeanSquaredError())\n    \n    return model","f471dde8":"early = EarlyStopping(monitor = 'val_loss', min_delta = 0., patience = 2,\n                   verbose = 1, mode = 'min', restore_best_weights = True)\ncheck = ModelCheckpoint(filepath = 'commonlit_model.h5', monitor = 'val_loss', verbose = 1, \n                                               ave_weights_only = True)\nreduce = ReduceLROnPlateau(monitor = 'val_loss', patience = 2, verbose = 1, factor = 0.5)\n\nmodel = build_model(max_len = max_len)\n\nmodel.summary()","989fccb4":"history = model.fit(Xtrain, targets, validation_split = 0.2, epochs = epochs, batch_size = 16, \n                   callbacks = [reduce])","37e5d203":"def display_training_curves(training, validation, title, subplot):\n    \"\"\"\n    Source: https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu\n    \"\"\"\n    plt.subplots(figsize = (10, 10), facecolor = '#F0F0F0')\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\n    plt.show()","bda11492":"history.history.keys()","c51b0602":"display_training_curves(\n                    history.history['loss'], \n                    history.history['val_loss'], \n                    'loss', 211)","f1c1dd4f":"preds = model.predict(Xtest, verbose = 1)\npreds[:10]","7c5d1f28":"sub['target'] = preds\nsub.to_csv('.\/submission.csv', index = False)","ccda7351":"plt.subplot(1, 2, 1)\nsns.kdeplot(train['target'], shade = True, color = 'green');\nplt.title('Train Target')\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(sub['target'], shade = True, color = 'blue');\nplt.title('Predicted Target');","7cd0b5ec":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","d58daeca":"<code>__Distribution of Target__<\/code>","86134f11":"# Tokenize","3e0a990f":"Check for number of missing values","41ad87f0":"# __Train Model__","7f6fcebd":"<code>__Distribution of Text Lengths__<\/code>","47b2d827":"- The max word length is useful to determine the tokenizer's max_len ","9f325b45":"# Competition Challenge\n\n- In this competition, we are required to build models to rate the complexity of reading passages for grade 3 to 12 class students\n\n# Evaluation Metrics\n\n- Root Mean Squared Error (RMSE)","0c7e7280":"- As per competition, URL and License are blank in testset","b238060e":"<code>__Distribution of Standard Error__<\/code>\n\n- Not provided for testset"}}