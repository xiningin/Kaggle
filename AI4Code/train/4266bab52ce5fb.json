{"cell_type":{"9bf1980b":"code","37d6e1c9":"code","c103e917":"code","75135559":"code","00277737":"code","a3a10a4e":"code","1259c062":"code","8820aaf7":"code","9c75799f":"code","851f4f92":"code","1ef820dc":"code","71879e8c":"code","f5d619ec":"code","7cb3d7bd":"markdown","ba05d4a8":"markdown","702d27c5":"markdown","0182a4a2":"markdown","1a7af987":"markdown","8f64fdf7":"markdown","c850c025":"markdown","72dcf0ce":"markdown","9ca01c03":"markdown","4d74e5fd":"markdown","fd81de22":"markdown","71b77e64":"markdown"},"source":{"9bf1980b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (20.0, 10.0)\n%matplotlib inline","37d6e1c9":"df = pd.read_csv('..\/input\/social-network-ads\/Social_Network_Ads.csv')\nX = df.iloc[:,2:4].values\ny = df.iloc[:,4].values","c103e917":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25, random_state=0)","75135559":"\nfrom sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler(feature_range=(0, 1))\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.fit_transform(X_test)","00277737":"\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","a3a10a4e":"def sigmoid(x):\n    return 1.0\/(1.0+np.exp(-x))","1259c062":"def bpn(epochs, learning_rate, X_train, y_train):\n    \n    V = np.random.randn(X_train.shape[1], 4)  #4 is the number of neuron in hidden layer\n    W = np.random.randn(4, 1)  #w is another set of weight,  4 is hidden layer, 1 is output layer\n    Loss = []\n    \n    for i in range(epochs):\n        \n        for j in range(X_train.shape[0]):\n            \n            x = np.array(X_train[j], ndmin=2)\n            A = np.dot(x, V)\n            B = sigmoid(A)\n            C = np.dot(B, W)\n            P = sigmoid(C)\n            y = np.array(y_train[j], ndmin=2)    # claculation of the cost\/loss function\n            L = 0.5 * (y - P) ** 2\n            \n            dLdP = -(y - P)\n            dPdC = sigmoid(C) * (1-sigmoid(C))\n            dCdW = B.T \n            dLdW = np.dot(dCdW, dLdP * dPdC)    #dL\/dW\n            dCdB = W.T\n            dBdA = sigmoid(A) * (1-sigmoid(A))\n            dAdV = x.T\n            dLdV = np.dot(dAdV, np.dot(dLdP * dPdC, dCdB) * dBdA)    #dL\/dV\n            \n            W = W - (learning_rate * dLdW)        # updating weights \n            V = V - (learning_rate * dLdV)\n        print('Epoch:',i+1,' -----> Loss:',L.item(0))\n        Loss.append(L.item(0))\n    \n    print(\"---------Execution Finished Successfully---------\")\n    return V, W, Loss","8820aaf7":"def pred(V,W,X_test,threshold = 0.5):\n    y_pred = []\n    test_loss = []\n    for j in range(X_test.shape[0]): \n            x = np.array(X_train[j], ndmin=2)\n            A = np.dot(x, V)\n            B = sigmoid(A)\n            C = np.dot(B, W)\n            P = sigmoid(C)\n            \n            y = np.array(y_train[j], ndmin=2)    # calculation of the cost\/loss function\n            L = 0.5 * (y - P) ** 2\n            test_loss.append(L.item(0))\n            \n            if P.item(0) >= threshold:\n                y_pred.append(1)\n            else:\n                y_pred.append(0)\n                \n                \n    return np.array(y_pred), test_loss","9c75799f":"final_V, final_W, Loss = bpn(2000, 0.01, X_train, y_train)","851f4f92":"y_pred, t_loss = pred(final_V, final_W, X_test)","1ef820dc":"print(y_pred)","71879e8c":"from sklearn.metrics import confusion_matrix,classification_report\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)","f5d619ec":"accuracy = (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])*100\nprint('Accuracy = ',accuracy,'%')","7cb3d7bd":"# Neural Network From Scratch\n\nIn this notebook, we will have a look at bulding a neural network from scratch (to be more specific, using numpy library). Here, we will have a look at a fixed architecture and see how the neural network backpropagation actually works. For better understanding, the following architecture will be used in this notebook.\n![NN.JPG](attachment:NN.JPG)","ba05d4a8":"## Libraries","702d27c5":"## Data Preprocessing","0182a4a2":"## Evaluation","1a7af987":"**Predict Function**\n\nNow, we fed the returned weights d to the **predict(weights, ,X_test, threshold)** function which repeats the steps 1, 2 and 3 to get the predicted outcome.","8f64fdf7":"This was the 2 layered neural network from scratch with back propagation. Though it could be more convenient to use classes and methods to build the model, I have prefered to go for the function based approach. It seemed easy to me. I hope you have enjoyed this notebook. If so, then I will be pleased. ","c850c025":"## Neural Network Code\n\nFrom here, we will start coding our neural network using numpy. At first we will define the sigmoid activation function, which is as follows. \n![Sigmoid](https:\/\/miro.medium.com\/max\/3268\/1*a04iKNbchayCAJ7-0QlesA.png)","72dcf0ce":"## Dataset Import \n\nWe will be using the [Social Network Ads dataset](https:\/\/www.kaggle.com\/rakeshrau\/social-network-ads), which is one of the famous and simplest classification dataset. Click on the link to explore about the dataset.","9ca01c03":"Now, we will define **bpn(epochs, learning_rate, X_train, y_train)** with initial weights set to some random numbers for each layers. Then the forward propagation is done. For the forward propagation, the following equations are performed. \n\n1. \tThe Weighted sum of inputs and weights added with bias is calculated.\n>   A = \u2211 (X*W)+b\n\n2.  The weighted sum is then sigmoided i.e. an activation is applied to it. \n>   B = \u03c3(A)\n\n3.  The cost function is calculated using the actual with the predicted one.\n>   L =  0.5 (y-B)^2\n\n3.  Now, the back propagation is started with the cost function. The target is to update the wights and the biases. For that the derivatives of the loss with respect to the biases are calculated. \n>   \u2202L\/\u2202W=  \u2202L\/\u2202B * \u2202B\/\u2202A * \u2202A\/\u2202W\n\n4.  Now, the weights are updates with the derivatives and the learning rates using this formula-\n> W=W+( \u03b1*  \u2202L\/\u2202W)\n\n*Where \u03b1  is the learning rate.*\n\nIn this way, the **epoch times** iteration is done and finally the adjusted sets of weights and biases are returned by the function.\n","4d74e5fd":"## Training The Model\n\nTo store the trained weights and biases, we are using two variables final_W and final V. Along with that, for each epoch, the loss is counted and stored in Loss array. ","fd81de22":"## Predicting the Output","71b77e64":"## Conclustion"}}