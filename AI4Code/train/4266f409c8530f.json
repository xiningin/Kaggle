{"cell_type":{"5254fc7e":"code","a12761a1":"code","43e3817c":"code","cf529539":"code","8c049f9b":"code","4e5b2f2e":"code","984427fb":"code","c0dfdd23":"code","2a5a3025":"code","73fe652c":"code","b53877e2":"code","73d171f3":"code","bd9d981e":"code","5f69bf2a":"code","f52d05d8":"code","c6393eda":"code","74adf6f5":"code","b10b1b4a":"code","bf55a7a5":"code","3359c446":"code","6fa0010a":"code","e2ae301f":"code","94e0abaf":"code","d050ff86":"code","97d26114":"code","7f8353ee":"code","d12872bd":"code","c0ed1870":"code","9f40663f":"code","9cb7811f":"code","3c4583c5":"code","5fc21652":"code","b2ad7026":"code","e28d786f":"code","4a62bfe0":"code","69b58275":"code","5a4c18ca":"code","98915d07":"code","dd93a7a3":"code","7a98c728":"code","3b0a2558":"code","0a09ff7e":"code","6b7d3fb5":"code","f884df10":"code","e8b79229":"code","2250c324":"code","56e061e2":"code","fde9cc60":"code","bcd8c33c":"code","ab896c86":"code","4c0942a0":"code","89eb1593":"code","1742b44c":"code","c493eb2f":"code","eb1eece8":"code","c437271d":"code","d877df89":"code","3337457a":"code","1ab87894":"code","830cbfa5":"code","b3801a6b":"code","92cdfb15":"code","95acce4a":"code","5a21bd79":"code","941a986d":"code","1092da73":"code","c35d70ab":"code","73b4218b":"code","0d0441e2":"code","13d904c5":"markdown","e19a5054":"markdown","3af101e0":"markdown","fb31c07b":"markdown","1b8266f5":"markdown","02603200":"markdown","0d892321":"markdown","fb979f7a":"markdown","a7747267":"markdown","0300ad3f":"markdown","eee6f509":"markdown","34a6a73a":"markdown","63907b16":"markdown","102d553f":"markdown","383c2df1":"markdown","e39ef465":"markdown","00f0342c":"markdown","85c95b8c":"markdown","984b6982":"markdown","d8091cc8":"markdown","a395d51f":"markdown","7a179852":"markdown","e48571ab":"markdown","3bdfa910":"markdown","110bcfe6":"markdown","bfed9619":"markdown","17716740":"markdown","10f6883c":"markdown","a27fc140":"markdown","3d4a707f":"markdown","23136125":"markdown","7cfc50e2":"markdown","cff7991b":"markdown","0a31557c":"markdown","4cbe5e3f":"markdown"},"source":{"5254fc7e":"# this may need to be installed separately with\n# !pip install category-encoders\nimport category_encoders as ce\n\n# python general\nimport pandas as pd\nimport numpy as np\nfrom collections import OrderedDict\n\n#scikit learn\n\nimport sklearn\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.base import clone\n\n# model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n# ML models\nfrom sklearn import tree\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# error metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error\n\n# plotting and display\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\n\nfrom IPython.display import display\npd.options.display.max_columns = None\n\n# widgets and widgets based libraries\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interactive","a12761a1":"def rmse(y_true, y_pred):\n    res = np.sqrt(((y_true - y_pred) ** 2).mean())\n    return res\n\ndef mape(y_true, y_pred):\n    y_val = np.maximum(np.array(y_true), 1e-8)\n    return (np.abs(y_true -y_pred)\/y_val).mean()","43e3817c":"metrics_dict_res = OrderedDict([\n            ('mean_absolute_error', mean_absolute_error),\n            ('median_absolute_error', median_absolute_error),\n            ('root_mean_squared_error', rmse),\n            ('mean abs perc error', mape)\n            ])","cf529539":"def regression_metrics_yin(y_train, y_train_pred, y_test, y_test_pred,\n                           metrics_dict, format_digits=None):\n    df_results = pd.DataFrame()\n    for metric, v in metrics_dict.items():\n        df_results.at[metric, 'train'] = v(y_train, y_train_pred)\n        df_results.at[metric, 'test'] = v(y_test, y_test_pred)\n\n    if format_digits is not None:\n        df_results = df_results.applymap(('{:,.%df}' % format_digits).format)\n\n    return df_results","8c049f9b":"def describe_col(df, col):\n    display(df[col].describe())\n\ndef val_count(df, col):\n    display(df[col].value_counts())\n\ndef show_values(df, col):\n    print(\"Number of unique values:\", len(df[col].unique()))\n    return display(df[col].value_counts(dropna=False))","4e5b2f2e":"def plot_distribution(df, col, bins=100, figsize=None, xlim=None, font=None, histtype='step'):\n    if font is not None:\n        mpl.rc('font', **font)\n\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(10, 6))\n    dev = df[col]    \n    dev.plot(kind='hist', bins=bins, density=True, histtype=histtype, color='b', lw=2,alpha=0.99)\n    print('mean:', dev.mean())\n    print('median:', dev.median())\n    if xlim is not None:\n        plt.xlim(xlim)\n    return plt.gca()","984427fb":"def plot_feature_importances(model, feature_names=None, n_features=20):\n    if feature_names is None:\n        feature_names = range(n_features)\n    \n    importances = model.feature_importances_\n    importances_rescaled = 100 * (importances \/ importances.max())\n    xlabel = \"Relative importance\"\n\n    sorted_idx = np.argsort(-importances_rescaled)\n\n    names_sorted = [feature_names[k] for k in sorted_idx]\n    importances_sorted = [importances_rescaled[k] for k in sorted_idx]\n\n    pos = np.arange(n_features) + 0.5\n    plt.barh(pos, importances_sorted[:n_features], align='center')\n\n    plt.yticks(pos, names_sorted[:n_features])\n    plt.xlabel(xlabel)\n\n    plt.title(\"Feature importances\")\n\n    return plt.gca()","c0dfdd23":"def plot_act_vs_pred(y_act, y_pred, scale=1, act_label='actual', pred_label='predicted', figsize=None, xlim=None,\n                     ylim=None, font=None):\n    \n    if font is not None:\n        mpl.rc('font', **font)\n\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(10, 6))\n    plt.scatter(y_act\/scale, y_pred\/scale)\n    x = np.linspace(0, y_act.max()\/scale, 10)\n    plt.plot(x, x)\n    plt.xlabel(act_label)\n    plt.ylabel(pred_label)\n    if xlim is not None:\n        plt.xlim(xlim)\n    else:\n        plt.xlim([0, 1e2])\n    if ylim is not None:\n        plt.ylim(ylim)\n    else:\n        plt.ylim([0, 1e2])\n    return plt.gca()","2a5a3025":"def compute_perc_deviation(y_act, y_pred, absolute=False):\n    dev = (y_pred - y_act)\/y_act * 100\n    if absolute:\n        dev = np.abs(dev)\n        dev.name = 'abs % error'\n    else:\n        dev.name = '% error'\n    return dev\n\ndef plot_dev_distribution(y_act, y_pred, absolute=False, bins=100, figsize=None, xlim=None, font=None):\n    if font is not None:\n        mpl.rc('font', **font)\n\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(10, 6))\n    dev = compute_perc_deviation(y_act, y_pred, absolute=absolute)\n    dev.plot(kind='hist', bins=bins, density=True)\n    print('mean % dev:', dev.mean())\n    print('median % dev:', dev.median())\n    # plt.vlines(dev.mean(), 0, 0.05)\n    plt.title('Distribution of errors')\n    plt.xlabel('% deviation')\n    if xlim is not None:\n        plt.xlim(xlim)\n    else:\n        plt.xlim([-1e2, 1e2])\n    return plt.gca()","73fe652c":"categorical_features = [\n    'Body_Type',\n    'Driven_Wheels',\n    'Global_Sales_Sub-Segment',\n    'Brand',\n    'Nameplate',\n    'Transmission',\n    'Turbo',\n    'Fuel_Type',\n    'PropSysDesign',\n    'Plugin',\n    'Registration_Type',\n    'country_name'\n]\n\nnumeric_features = [\n    'Generation_Year',\n    'Length',\n    'Height',\n    'Width',\n    'Engine_KW',\n    'No_of_Gears',\n    'Curb_Weight',\n    'CO2',\n    'Fuel_cons_combined',\n    'year'\n]\n\nall_numeric_features = list(numeric_features)\nall_categorical_features = list(categorical_features)\n\ntarget = [\n    'Price_USD'\n]\n\ntarget_name = 'Price_USD'","b53877e2":"len(all_categorical_features)","73d171f3":"#ml_model_type = 'Linear Regression'\n#ml_model_type = 'Decision Tree'\n#ml_model_type = 'Random Forest'\n#ml_model_type = 'CatBoost'\n#ml_model_type = 'LightGBM'\nml_model_type = 'Combined'\n\nregression_metric = 'mean abs perc error'\n\ndo_grid_search_cv = False\nscoring_greater_is_better = False  # THIS NEEDS TO BE SET CORRECTLY FOR CV GRID SEARCH\n\ndo_retrain_total = True\nwrite_predictions_file = True\n\n# relative size of test set\ntest_size = 0.3\nrandom_state = 33","bd9d981e":"def convert_categories(dataFrame):\n    for column in dataFrame.columns:\n        print(column)\n        col_type = dataFrame[column].dtype\n        if col_type == 'object' or col_type.name == 'category':\n            dataFrame[column] = dataFrame[column].astype('category')","5f69bf2a":"df = pd.read_csv('\/kaggle\/input\/ihsmarkit-hackathon-june2020\/train_data.csv',index_col='vehicle_id')\ndf['date'] = pd.to_datetime(df['date'])","f52d05d8":"df.info()","c6393eda":"# basic commands on a dataframe\n#df.info()\ndf.head(5)\n#df.shape\n# df.head()\n# df.tail()","74adf6f5":"df_oos = pd.read_csv('\/kaggle\/input\/ihsmarkit-hackathon-june2020\/oos_data.csv', index_col='vehicle_id')\ndf_oos['date'] = pd.to_datetime(df_oos['date'])\ndf_oos['year'] = df_oos['date'].map(lambda d: d.year)","b10b1b4a":"# df_oos.shape\ndf_oos.head()","bf55a7a5":"convert_categories(df_oos)","3359c446":"df_oos.groupby(['year', 'country_name'])['date'].count()","6fa0010a":"# unique values, categorical variables\nfor col in all_categorical_features:\n    print(col, len(df[col].unique()))","e2ae301f":"# unique values, categorical variables\nfor col in all_categorical_features:\n    print(col, len(df_oos[col].unique()))","94e0abaf":"interactive(lambda col: show_values(df, col), col=all_categorical_features)","d050ff86":"# summary statistics\ndf[numeric_features + target].describe()","97d26114":"figsize = (16,12)\nsns.set(style='whitegrid', font_scale=2)\n\nbins = 1000\nbins = 40\n#xlim = [0,100000]\nxlim = None\nprice_mask = df['Price_USD'] < 1000000\ninteractive(lambda col: plot_distribution(df[price_mask], col, bins=bins, xlim=xlim), col=sorted(all_numeric_features + target))\n#interactive(lambda col: plot_distribution(df, col, bins=bins, xlim=xlim), col=sorted(all_numeric_features + target))","7f8353ee":"# this is quite slow\nsns.set(style='whitegrid', font_scale=1)\n# sns.pairplot(df[numeric_features[:6] + target].iloc[:10000])\n#sns.pairplot(df[['Engine_KW'] + target].iloc[:10000])\n#price_mask = df['Price_USD'] < 100000\ndf_temp = df[price_mask].copy()\nsns.pairplot(df_temp[['Engine_KW'] + target])","d12872bd":"additional_numeric_features = []","c0ed1870":"categorical_features","9f40663f":"numeric_features","9cb7811f":"# Mainly we drop features with high correlation to other features used for model.\n# Like an example CO2 has high correlation with Fuel_cons_combined so we used only latter.\nfeatures_drop = [\n    'Length',\n    'Width',\n    'No_of_Gears',\n    'CO2'\n]\n\nif ml_model_type == 'Linear Regression':\n    features_drop = categorical_features + numeric_features\n    features_to_use = ['Engine_KW']\n    # features_to_use = ['country_name', 'Engine_KW']\n    for feature in features_to_use:\n        features_drop.remove(feature)\n    \n\ncategorical_features = list(filter(lambda f: f not in features_drop, categorical_features))\nnumeric_features = list(filter(lambda f: f not in features_drop, numeric_features))","3c4583c5":"features = categorical_features + numeric_features + additional_numeric_features\nmodel_columns = features + [target_name]\nlen(model_columns)","5fc21652":"model_columns","b2ad7026":"#dataframe for further processing\ndf_proc = df[model_columns].copy()\ndf_proc.shape","e28d786f":"df_proc = df_proc.query('Plugin != \"unknown\" and Body_Type != \"chassis-cab\" and Fuel_Type != \"hydrogen\" and (not (country_name == \"USA\" and Engine_KW < 60))')\ndf_proc = df_proc[df_proc[\"Price_USD\"] < 500000]","4a62bfe0":"df_proc.shape","69b58275":"df_proc_outliers = df_proc.copy()\ndf_proc_outliers['car_identity'] = df_proc_outliers['Brand'] + df_proc_outliers['Nameplate'] + df_proc_outliers['country_name']\ndf_proc_outliers.head()","5a4c18ca":"lower_q = 0.2\nhigher_q = 0.98\noutlires = df_proc_outliers.groupby('car_identity')[\"Price_USD\"].quantile([lower_q, higher_q]).unstack(level=1)\ndf_proc_outliers = df_proc_outliers.loc[(((outlires.loc[df_proc_outliers[\"car_identity\"], lower_q] * 0.5) < df_proc_outliers[\"Price_USD\"].values) \n                                         & (df_proc_outliers[\"Price_USD\"].values < outlires.loc[df_proc_outliers[\"car_identity\"], higher_q] * 2)).values]\ndf_proc = df_proc_outliers.drop('car_identity', 1)\ndf_proc.shape","98915d07":"convert_categories(df_proc)","dd93a7a3":"# One-hot encoding\nif ml_model_type == 'Random Forest':\n    encoder = ce.OneHotEncoder(cols=categorical_features, handle_unknown='value', \n                           use_cat_names=True)\n    encoder.fit(df_proc)\n    df_comb_ext = encoder.transform(df_proc)\n    features_ext = list(df_comb_ext.columns)\n    features_ext.remove(target_name)\nelse:\n    df_comb_ext = df_proc.copy()\n    features_ext = features","7a98c728":"#del df_proc\ndf_comb_ext","3b0a2558":"# df_comb_ext.memory_usage(deep=True).sum()\/1e9\n#features_model\ndf_comb_ext.shape","0a09ff7e":"X_train, X_test, y_train, y_test = train_test_split(df_comb_ext[features_ext], df_comb_ext[target_name], \n                                                    test_size=test_size, random_state=random_state)\n\nprint(X_train.shape)\nprint(X_test.shape)","6b7d3fb5":"y_train.hist()\ny_test.hist()","f884df10":"# since price has log-normal distribution with long tail we are applying log transformation to have normally distributed target\ny_train = np.log1p(y_train)\ny_test = np.log1p(y_test)","e8b79229":"y_train.hist()\ny_test.hist()","2250c324":"if ml_model_type == 'Linear Regression':\n    model_hyper_parameters_dict = OrderedDict(fit_intercept=True, normalize=False)\n    regressor =  LinearRegression(**model_hyper_parameters_dict)\n\nif ml_model_type == 'Decision Tree':\n    model_hyper_parameters_dict = OrderedDict(max_depth=3, random_state=random_state)\n    regressor =  DecisionTreeRegressor(**model_hyper_parameters_dict)\n      \nif ml_model_type == 'Random Forest':\n\n    model_hyper_parameters_dict = OrderedDict(n_estimators=10, \n                                              max_depth=4, \n                                              min_samples_split=2, \n                                              max_features='sqrt',\n                                              min_samples_leaf=1, \n                                              random_state=random_state, \n                                              n_jobs=4)\n   \n    regressor = RandomForestRegressor(**model_hyper_parameters_dict)\n    \nif ml_model_type == 'CatBoost': \n    regressor = CatBoostRegressor(verbose=0, \n                                  depth=10, \n                                  iterations= 500,\n                                  l2_leaf_reg= 9,\n                                  learning_rate= 0.15,\n                                  one_hot_max_size = 15)\nif ml_model_type == 'LightGBM': \n    regressor = LGBMRegressor(verbose=0, \n                              max_depth=20, \n                              iterations= 500,\n                              learning_rate= 0.2,\n                              early_stop_rounds = 15)\nif ml_model_type == 'Combined':\n    model_hyper_parameters_dict1 = OrderedDict(verbose=0, \n                                  depth=10, \n                                  iterations= 500,\n                                  l2_leaf_reg= 9,\n                                  learning_rate= 0.15,\n                                  one_hot_max_size = 15,\n                                  objective ='MAPE')\n    \n    regressor1 = CatBoostRegressor(**model_hyper_parameters_dict1)\n    \n    model_hyper_parameters_dict2 = OrderedDict(verbose=0, \n                              max_depth=20, \n                              iterations= 500,\n                              learning_rate= 0.22,\n                              early_stop_rounds = 15,\n                              num_leaves=36,\n                              n_estimators = 125)\n    \n    regressor2 = LGBMRegressor(**model_hyper_parameters_dict2)\n    \n    base_regressor1 = clone(regressor1)\n    base_regressor2 = clone(regressor2)\nelse:\n    base_regressor = clone(regressor)\n    \nif do_grid_search_cv:\n    \n    scoring = make_scorer(metrics_dict_res[regression_metric], greater_is_better=scoring_greater_is_better)\n    \n    if ml_model_type == 'Random Forest':\n        \n\n        grid_parameters = [{'n_estimators': [10], 'max_depth': [3, 5, 10], \n                             'min_samples_split': [2,4], 'min_samples_leaf': [1]} ]\n        \n    n_splits = 4\n    n_jobs = 4\n    cv_regressor = GridSearchCV(regressor, grid_parameters, cv=n_splits, scoring=scoring, return_train_score=True,\n                                refit=True, n_jobs=n_jobs)    ","56e061e2":"if do_grid_search_cv:\n    cv_regressor.fit(X_train, y_train)\n    regressor_best = cv_regressor.best_estimator_\n    model_hyper_parameters_dict = cv_regressor.best_params_\n    train_scores = cv_regressor.cv_results_['mean_train_score']\n    test_scores = cv_regressor.cv_results_['mean_test_score']\n    test_scores_std = cv_regressor.cv_results_['std_test_score']\n    cv_results = cv_regressor.cv_results_\nelse:\n    if ml_model_type == 'CatBoost':\n        regressor.fit(X_train, y_train, cat_features = categorical_features)\n    elif ml_model_type == 'Combined':\n        regressor1.fit(X_train, y_train, cat_features = categorical_features)\n        regressor2.fit(X_train, y_train)\n    else:\n        regressor.fit(X_train, y_train)","fde9cc60":"if do_grid_search_cv:\n    # print(cv_results)\n    print(model_hyper_parameters_dict)\n    plt.plot(-train_scores, label='train')\n    plt.plot(-test_scores, label='test')\n    plt.xlabel('Parameter set #')\n    plt.legend()\n    regressor = regressor_best","bcd8c33c":"def get_mean_prediction(model1, model2, X):\n    y_pred1 = model1.predict(X)\n    y_pred2 = model2.predict(X)\n    return (y_pred1 + y_pred2) \/ 2\n    ","ab896c86":"if ml_model_type == 'Combined':\n    y_train_pred = get_mean_prediction(regressor1, regressor2, X_train)\n    y_test_pred = get_mean_prediction(regressor1, regressor2, X_test)\nelse:\n    y_train_pred = regressor.predict(X_train)\n    y_test_pred = regressor.predict(X_test)","4c0942a0":"if ml_model_type == 'Linear Regression':\n    df_reg_coef = (pd.DataFrame(zip(['intercept'] + list(X_train.columns), \n                               [regressor.intercept_] + list(regressor.coef_)))\n                 .rename({0: 'feature', 1: 'coefficient value'}, axis=1))\n    display(df_reg_coef)","89eb1593":"def feature_importance(model):\n    if hasattr(model, 'feature_importances_'):\n        sns.set(style='whitegrid', font_scale=1.5)\n        plt.figure(figsize=(12,10))\n        plot_feature_importances(model, features_ext, n_features=np.minimum(20, X_train.shape[1]))","1742b44c":"if ml_model_type == 'Combined':\n    feature_importance(regressor1)\n    feature_importance(regressor2)\nelse:\n    feature_importance(regressor)","c493eb2f":"y_train = np.expm1(y_train)\ny_train_pred = np.expm1(y_train_pred)\ny_test = np.expm1(y_test)\ny_test_pred = np.expm1(y_test_pred)","eb1eece8":"df_regression_metrics = regression_metrics_yin(y_train, y_train_pred, y_test, y_test_pred,\n                                               metrics_dict_res, format_digits=3)\n\ndf_output = df_regression_metrics.copy()\ndf_output.loc['Counts','train'] = len(y_train)\ndf_output.loc['Counts','test'] = len(y_test)\ndf_output","c437271d":"figsize = (16,10)\nxlim = [0, 250]\nfont={'size': 20}\nsns.set(style='whitegrid', font_scale=2.5)\nact_label = 'actual price [k$]'\npred_label='predicted price [k$]'\nplot_act_vs_pred(y_test, y_test_pred, scale=1000, act_label=act_label, pred_label=pred_label, \n                 figsize=figsize, xlim=xlim, ylim=xlim, font=font)\nprint()","d877df89":"df_error = pd.merge(X_test, y_test, left_index=True, right_index=True, how='outer')\ndf_error[\"Price_USD_predict\"] = y_test_pred\ndf_error[\"Error_pct\"] = abs(df_error[\"Price_USD_predict\"] - df_error[\"Price_USD\"])\/df_error[\"Price_USD\"] *100\n\n# analysis of high error rate\ndf_high_error_rate = df_error[df_error[\"Error_pct\"] > 80]\ninteractive(lambda col: show_values(df_high_error_rate, col), col=all_categorical_features)","3337457a":"figsize = (14,8)\nxlim = [0, 100]\n#xlim = [-100, 100]\n# xlim = [-50, 50]\n#xlim = [-20, 20]\n\nfont={'size': 20}\nsns.set(style='whitegrid', font_scale=1.5)\n\np_error = (y_test_pred - y_test)\/y_test *100\ndf_p_error = pd.DataFrame(p_error.values, columns=['percent_error'])\n#display(df_p_error['percent_error'].describe().to_frame())\n\nbins=1000\nbins=500\n#bins=100\nabsolute = True\n#absolute = False\nplot_dev_distribution(y_test, y_test_pred, absolute=absolute, figsize=figsize, \n                      xlim=xlim, bins=bins, font=font)\nprint()","1ab87894":"if do_retrain_total:\n    if ml_model_type == 'Combined':      \n        cv_opt_model1 = clone(base_regressor1.set_params(**model_hyper_parameters_dict1))\n        cv_opt_model2 = clone(base_regressor2.set_params(**model_hyper_parameters_dict2))\n        # train on complete data set\n        X_train_full = df_comb_ext[features_ext].copy()\n        y_train_full = np.log1p(df_comb_ext[target_name].values)\n        cv_opt_model1.fit(X_train_full, y_train_full, cat_features = categorical_features) \n        cv_opt_model2.fit(X_train_full, y_train_full)\n        regressor1 = cv_opt_model1\n        regressor2 = cv_opt_model2\n    else:\n        cv_opt_model = clone(base_regressor.set_params(**model_hyper_parameters_dict))\n        # train on complete data set\n        X_train_full = df_comb_ext[features_ext].copy()\n        y_train_full = df_comb_ext[target_name].values\n        cv_opt_model.fit(X_train_full, y_train_full) \n        regressor = cv_opt_model","830cbfa5":"df_proc_oos = df_oos[model_columns[:-1]].copy()\ndf_proc_oos[target_name] = 1","b3801a6b":"df_proc_oos","92cdfb15":"df_comb_ext_oos = df_proc_oos","95acce4a":"df_comb_ext_oos.drop(target_name, axis=1, inplace=True)","5a21bd79":"if ml_model_type == 'Combined':\n    y_oos_pred = get_mean_prediction(regressor1, regressor2, df_comb_ext_oos)\nelse:\n    y_oos_pred = regressor.predict(df_comb_ext_oos)","941a986d":"y_oos_pred = np.expm1(y_oos_pred)","1092da73":"id_col = 'vehicle_id'\ndf_out = (pd.DataFrame(y_oos_pred, columns=[target_name], index=df_comb_ext_oos.index)\n            .reset_index()\n            .rename({'index': id_col}, axis=1))","c35d70ab":"df_out.head()","73b4218b":"df_out.shape","0d0441e2":"if write_predictions_file:\n    df_out.to_csv('submission.csv', index=False)","13d904c5":"## Categorical feature encoding","e19a5054":"## Training data","3af101e0":"## Train test split","fb31c07b":"# Feature exploration","1b8266f5":"## Categorical features","02603200":"## Optionally retrain on the whole data set","0d892321":"## Metrics","fb979f7a":"# Import libraries","a7747267":"## Regression coefficients\/Feature importance","0300ad3f":"## Train, test predictions","eee6f509":"## Pair plot","34a6a73a":"# Feature selection\n\nYou can read about feature selection here\nhttps:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html#","63907b16":"## Out of sample data (to predict)","102d553f":"#  ML data preparation","383c2df1":"# Machine learning model\n\nSupervised learning\n\nhttps:\/\/scikit-learn.org\/stable\/supervised_learning.html\n\nEnsemble methods in scikit learn\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\n\n\nDecision trees\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/tree.html\n","e39ef465":"# Model evaluation","00f0342c":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries\" data-toc-modified-id=\"Import-libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Import libraries<\/a><\/span><\/li><li><span><a href=\"#Locally-defined-functions\" data-toc-modified-id=\"Locally-defined-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Locally defined functions<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Metrics<\/a><\/span><\/li><li><span><a href=\"#Display-functions\" data-toc-modified-id=\"Display-functions-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Display functions<\/a><\/span><\/li><li><span><a href=\"#Define-features\" data-toc-modified-id=\"Define-features-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Define features<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Global-options\" data-toc-modified-id=\"Global-options-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Global options<\/a><\/span><\/li><li><span><a href=\"#Load--data\" data-toc-modified-id=\"Load--data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Load  data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Training-data\" data-toc-modified-id=\"Training-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Training data<\/a><\/span><\/li><li><span><a href=\"#Out-of-sample-data-(to-predict)\" data-toc-modified-id=\"Out-of-sample-data-(to-predict)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Out of sample data (to predict)<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-exploration\" data-toc-modified-id=\"Feature-exploration-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Feature exploration<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-features\" data-toc-modified-id=\"Categorical-features-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Categorical features<\/a><\/span><\/li><li><span><a href=\"#Numerical-features\" data-toc-modified-id=\"Numerical-features-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Numerical features<\/a><\/span><\/li><li><span><a href=\"#Pair-plot\" data-toc-modified-id=\"Pair-plot-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Pair plot<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-generation\" data-toc-modified-id=\"Feature-generation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Feature generation<\/a><\/span><\/li><li><span><a href=\"#Feature-selection\" data-toc-modified-id=\"Feature-selection-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Feature selection<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Drop-features-(optional)\" data-toc-modified-id=\"Drop-features-(optional)-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Drop features (optional)<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#ML-data-preparation\" data-toc-modified-id=\"ML-data-preparation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>ML data preparation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-feature-encoding\" data-toc-modified-id=\"Categorical-feature-encoding-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>Categorical feature encoding<\/a><\/span><\/li><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Train test split<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Machine-learning-model\" data-toc-modified-id=\"Machine-learning-model-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Machine learning model<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-definition\" data-toc-modified-id=\"Model-definition-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;<\/span>Model definition<\/a><\/span><\/li><li><span><a href=\"#ML-model-training\" data-toc-modified-id=\"ML-model-training-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;<\/span>ML model training<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-evaluation\" data-toc-modified-id=\"Model-evaluation-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Model evaluation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Train,-test-predictions\" data-toc-modified-id=\"Train,-test-predictions-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;<\/span>Train, test predictions<\/a><\/span><\/li><li><span><a href=\"#Regression-coefficients\/Feature-importance\" data-toc-modified-id=\"Regression-coefficients\/Feature-importance-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;<\/span>Regression coefficients\/Feature importance<\/a><\/span><\/li><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;<\/span>Metrics<\/a><\/span><\/li><li><span><a href=\"#Model-Performance-plots\" data-toc-modified-id=\"Model-Performance-plots-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;<\/span>Model Performance plots<\/a><\/span><\/li><li><span><a href=\"#Optionally-retrain-on-the-whole-data-set\" data-toc-modified-id=\"Optionally-retrain-on-the-whole-data-set-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;<\/span>Optionally retrain on the whole data set<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Apply-model-to-OOS-data\" data-toc-modified-id=\"Apply-model-to-OOS-data-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;<\/span>Apply model to OOS data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Subset-to-relevant-columns\" data-toc-modified-id=\"Subset-to-relevant-columns-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;<\/span>Subset to relevant columns<\/a><\/span><\/li><li><span><a href=\"#Apply-categorical-encoding\" data-toc-modified-id=\"Apply-categorical-encoding-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;<\/span>Apply categorical encoding<\/a><\/span><\/li><li><span><a href=\"#Apply-model-and-produce-output\" data-toc-modified-id=\"Apply-model-and-produce-output-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;<\/span>Apply model and produce output<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","85c95b8c":"## ML model training","984b6982":"##  Model Performance plots","d8091cc8":"# Load  data\n","a395d51f":"# Brief process description\n\n**Overall we used next process to come up with the result:**\n1. Initial data and features analysis. Here we focused mostly on features correlation in order to reduce number of features used.\n2. Create baseline model (can be done in paralel with data analysis). For first model we have used CatBoost since it handle very well categorical features out of the box (the same as LightGBM).\n3. Analyze result on test dataset and error distribution.\n4. Use baseline model and selected features for submission.\n5. Perform hyper parameters tuning of the model (could be time consuming so better to this in parallel with other activities).\n6. Feature engineering. Mainly create new features and experement with feature combinations. We tried to use age of car, volume, but they had negative affect so we haven't used them. Also adjust some features we used for model.\n7. Data analysis, removing outliers. Main focus on most important features for selected model. Things we did here are: removed too cheap\/expensive cars, remove some anomalies, normalized target distribution.\n8. Try different model(s). In our case we tried LightGBM to check how it will perform against CatBoost.\n9. Try combination of models. Based on error distribution and main features of models we combined CatBoost and LightGBM models.","7a179852":"## Apply model and produce output","e48571ab":"## Display functions","3bdfa910":"# Locally defined functions","110bcfe6":"## Define features","bfed9619":"## Metrics","17716740":"# Clean Data for processing","10f6883c":"## Apply categorical encoding","a27fc140":"# Apply model to OOS data","3d4a707f":"## Numerical features","23136125":"## Drop features (optional)","7cfc50e2":"# Global options","cff7991b":"## Subset to relevant columns","0a31557c":"##  Model definition","4cbe5e3f":"# Feature generation"}}