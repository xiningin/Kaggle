{"cell_type":{"6c86c9ac":"code","3d3781b7":"code","40e429e5":"code","f267b1fc":"code","730dbdca":"code","02addac3":"code","d9cc47a0":"code","4306211f":"code","ac7f97dc":"code","bb357e70":"code","0176e434":"code","b1ba827a":"code","50463110":"code","fd1a41c2":"code","1d6b5271":"code","6db64f3d":"code","85a1fd40":"markdown"},"source":{"6c86c9ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d3781b7":"import tensorflow as tf\nimport keras\nfrom keras.models import Model, Input, Sequential\nfrom keras.layers import LSTM,GRU, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, BatchNormalization, Activation, Add\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom numpy.random import seed","40e429e5":"import torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport transformers \n\nimport pandas as pd\nimport numpy as np \n\nimport joblib\n\n\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","f267b1fc":"class Config:\n    def __init__(self):\n        self.MAX_LEN = 128\n        self.TRAIN_BATCH_SIZE = 8\n        self.VALID_BATCH_SIZE = 8\n        self.EPOCHS = 5\n        self.BASE_MODEL_PATH = \"..\/input\/bert-base-uncased\"\n        self.MODEL_PATH = \".\/model.bin\"\n        self.TRAINING_FILE = \"..\/input\/ner-dataset\/ner_dataset.csv\"\n        self.TOKENIZER = transformers.BertTokenizer.from_pretrained(\n                self.BASE_MODEL_PATH,\n                do_lower_case=True\n            )\nconfig = Config()","730dbdca":"\ndef process_data(data_path):\n    df = pd.read_csv(data_path,encoding='latin-1')\n    df.loc[:,\"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n\n    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values \n    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n    \n    vocab_dict = {'_unk_': 1, '_w_pad_': 0}\n    tag_dict = {'_t_pad_': 0}\n    \n    # add padding token to sentence, and build vocab dict\n    padded_sents = []\n    for sent in sentences:\n        padded_sent = []\n        for word in sent:\n            padded_sent.append(word)\n            if word not in vocab_dict:\n                vocab_dict[word] = len(vocab_dict)\n        if len(padded_sent) < config.MAX_LEN:\n            added_len = config.MAX_LEN - len(padded_sent)\n            padded_sent += ['_w_pad_']*added_len\n        assert len(padded_sent) == config.MAX_LEN\n        padded_sents.append(padded_sent)\n        \n    # do the same with tag\n    tag_seqs = []\n    for tag_seq in tag:\n        padded_tags = []\n        for tag_item in tag_seq:\n            padded_tags.append(tag_item)\n            if tag_item not in tag_dict:\n                tag_dict[tag_item] = len(tag_dict)\n        if len(padded_tags) < config.MAX_LEN:\n            added_len = config.MAX_LEN - len(padded_tags)\n            padded_tags += ['_t_pad_']*added_len\n        assert len(padded_tags) == config.MAX_LEN\n        tag_seqs.append(padded_tags)\n        \n    word2idx = vocab_dict\n    tag2idx = tag_dict\n    \n    sent_tokens = np.array([[word2idx[w] for w in doc] for doc in padded_sents])\n    tag_tokens = [[tag2idx[t] for t in t_seq] for t_seq in tag_seqs]\n    tag_tokens = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in tag_tokens])\n    \n    return sent_tokens, tag_tokens, word2idx, tag2idx\n\nif __name__ == '__main__':\n    sent_tokens, tag_tokens, word2idx, tag2idx = process_data(config.TRAINING_FILE)\n    idx2word = {v:k for k,v in word2idx.items()}\n    idx2tag = {v:k for k,v in tag2idx.items()} ","02addac3":"(\n        train_tokens, \n        val_tokens, \n        train_tags,\n        val_tags\n     ) = model_selection.train_test_split(sent_tokens, tag_tokens,random_state = 42,test_size = 0.2)\n\n","d9cc47a0":"def build_model(max_sent_length, vocab_size, hidden_size, embedding_size, \n                output_size, learning_rate,num_mlp_layers,activation,dropout_rate):\n    \n    \n    inputs = Input(shape=(max_sent_length,), dtype='int32')\n    emb = Embedding(input_dim=vocab_size,\n                    output_dim=embedding_size,\n                    input_length=max_sent_length,\n                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(inputs)\n    \n    bilstm_layer_1 = Bidirectional(LSTM(hidden_size,\n                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n                   return_sequences=True))(emb)\n    bilstm_layer_1_dropout = Dropout(dropout_rate, seed=0)(bilstm_layer_1)\n    bilstm_layer_2 = Bidirectional(LSTM(hidden_size,\n                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n                   return_sequences=True))(bilstm_layer_1_dropout)\n    bilstm_layer_2_dropout = Dropout(dropout_rate, seed=0)(bilstm_layer_2)\n\n    bilstm_layer_3 = Bidirectional(LSTM(hidden_size,\n                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n                   return_sequences=True))(bilstm_layer_2_dropout)\n    bilstm_layer_3_dropout = Dropout(dropout_rate, seed=0)(bilstm_layer_3)\n\n    bilstm_layer_4 = Bidirectional(LSTM(hidden_size,\n                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n                   return_sequences=True))(bilstm_layer_3_dropout)\n    bilstm_layer_4_dropout = Dropout(dropout_rate, seed=0)(bilstm_layer_4)\n    \n    h = bilstm_layer_4_dropout\n    \n    for i in range(num_mlp_layers-1):\n        new_h = Dense(hidden_size,\n                      kernel_initializer=keras.initializers.he_normal(seed=0),\n                      bias_initializer=\"zeros\",\n                      kernel_regularizer=keras.regularizers.l2(0.0))(h)\n        # add batch normalization layer\n        new_h = BatchNormalization()(new_h)\n        # add residual connection\n        if i == 0:\n            h = new_h\n        else:\n            h = Add()([h, new_h])\n        # add activation\n        h = Activation(activation)(h)\n    dense = Dense(output_size,\n              activation=\"softmax\",\n              kernel_initializer=keras.initializers.he_normal(seed=0),\n              bias_initializer=\"zeros\")(h)\n    \n    model = Model(inputs, dense)\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n                                 \n    return model","4306211f":"import csv\nfrom pathlib import Path\n\ncheckpointer = keras.callbacks.ModelCheckpoint(\n    filepath=os.path.join(\"models\", \"LSTM_CRF.hdf5\"),\n    monitor=\"val_accuracy\",\n    verbose=0,\n    save_best_only=True)\n\nclass StepLogCallback(keras.callbacks.Callback):\n    def __init__(self):\n        self.count = 0\n        self.step_log = []\n        self.log_fpath = Path('.\/step_log.csv')\n    \n    def on_train_batch_end(self, batch, logs=None):\n        self.count += 1\n        self.step_log.append([self.count, logs['loss'], logs['accuracy']])\n    \n    def on_epoch_end(self, epoch, logs=None):\n        log_exists = self.log_fpath.exists()\n        with open(self.log_fpath, 'a' if log_exists else 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            if not log_exists:\n                writer.writerow(['step','loss','accuracy'])\n            for log in self.step_log:\n                writer.writerow(log)\n            self.step_log.clear()\n\nclass TestCallback(keras.callbacks.Callback):\n    def __init__(self, test_data, ground_file, model):\n        self.count = 0\n        self.test_data = test_data\n        self.model = model\n        self.ground_file = ground_file\n        self.epoch_log = []\n        self.log_fpath = Path('.\/epoch_log.csv')\n        \n    def calc_accuracy(self, preds, tags, padding_id=\"_t_pad_\"):\n        preds_flatten = preds.flatten()\n        tags_flatten = tags.flatten()\n        non_padding_idx = np.where(tags_flatten!=padding_id)[0]\n        return sum(preds_flatten[non_padding_idx]==tags_flatten[non_padding_idx])\/len(non_padding_idx)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        val_preds_numerical = self.model.predict(self.test_data)\n        val_preds = np.array([[idx2tag[p] for p in preds] for preds in np.argmax(val_preds_numerical,axis=2)])\n        file_dict = pkl.load(open(self.ground_file, \"rb\"))\n        val_accuracy = self.calc_accuracy(val_preds, np.array(file_dict[\"tag_seq\"]))\n        \n        print('\\nValidation Set Accuracy: %f\\n' % val_accuracy)\n        \n        self.count += 1\n        log_exists = self.log_fpath.exists()\n        with open(self.log_fpath, 'a' if log_exists else 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            if not log_exists:\n                writer.writerow(['epoch','loss','train_accuracy','val_accuracy'])\n            writer.writerow([self.count, logs['loss'], logs['accuracy'], val_accuracy])\n            self.epoch_log.clear()","ac7f97dc":"input_length = config.MAX_LEN\nvocab_size = len(word2idx)\nhidden_size = 100\nembedding_size = 300\noutput_size = train_tags.shape[2]\nlearning_rate = 0.0001\nnum_mlp_layers = 3\nactivation = \"tanh\"\ndropout_rate = 0.5","bb357e70":"np.random.seed(0)\ntf.random.set_seed(0)\nmodel = build_model(input_length, vocab_size, hidden_size, embedding_size, \n                    output_size, learning_rate, num_mlp_layers, activation, dropout_rate)\nmodel.summary()","0176e434":"from keras.callbacks import CSVLogger\ncsv_logger = CSVLogger('rnn-ner-training.log', append=False)\nearlystopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    verbose=0)","b1ba827a":"!rm -f *.csv\nhistory = model.fit(train_tokens, train_tags,\n                    batch_size=128, epochs=20, validation_data = (val_tokens, val_tags),\n                    callbacks=[checkpointer,csv_logger])","50463110":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,4))\nplt.suptitle(\"RNN - learning_rate=0.0001\", fontsize=16)\nplt.subplot(1,2,1)\nplt.plot(history.history[\"loss\"], label=\"training\", color=\"blue\", linestyle=\"dashed\")\nplt.plot(history.history[\"val_loss\"], label=\"validation\", color=\"orange\")\nplt.xticks((0,5,10,15,20))\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(history.history[\"accuracy\"], label=\"training\", color=\"blue\", linestyle=\"dashed\")\nplt.plot(history.history[\"val_accuracy\"], label=\"validation\", color=\"orange\")\nplt.xticks((0,5,10,15,20))\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","fd1a41c2":"# save model\nmodel.save('models\/BiLSTM_model.h5')","1d6b5271":"model =  keras.models.load_model('models\/BiLSTM_model.h5')\n","6db64f3d":"sentence = \"HKUST honors faculty and staff members who have served the University for at least two decades.\"\nsent = sentence.split(\" \")\nencoded_sent = []\nfor word in sent:\n    if word in word2idx:\n        encoded_sent.append(word2idx[word])\n    else:\n        encoded_sent.append(1)\nif len(encoded_sent) < config.MAX_LEN:\n    encoded_sent += [0]*(config.MAX_LEN - len(encoded_sent))\nassert len(encoded_sent) == config.MAX_LEN\n\ntest_dataset = []\ntest_dataset.append(encoded_sent)\ntest_dataset = np.array(test_dataset)\ntest_preds_numerical = model.predict(test_dataset)\ntest_preds = np.array([[idx2tag[p] for p in preds] for preds in np.argmax(test_preds_numerical,axis=2)])\nprint(test_preds.shape)\nprint(test_preds[0])","85a1fd40":"## Prediction"}}