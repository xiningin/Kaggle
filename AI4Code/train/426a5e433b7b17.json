{"cell_type":{"0666f91c":"code","d4723815":"code","2d59f0b1":"code","5b4a8362":"code","8702766d":"code","c38012a1":"code","6d9063cf":"code","b50b0f66":"code","06bbf1f7":"code","992e6586":"code","1583dc0d":"code","9c3238a9":"code","191f49fb":"code","fb1cf0ed":"code","1834906b":"code","8a74753a":"code","f9b03d98":"code","ca390bab":"code","11c8b40d":"code","e2562b06":"code","358fb429":"code","4fe2ecff":"code","7f75600c":"code","b0d67262":"code","30433591":"markdown","95ca27c6":"markdown","620211e0":"markdown","0a610eb9":"markdown","b1e4641d":"markdown","73a92329":"markdown","8505cad4":"markdown","8fe3e192":"markdown","853adf94":"markdown","053837bd":"markdown","d1cbfa11":"markdown","baeceb38":"markdown","19887964":"markdown","b6750094":"markdown","88211657":"markdown","9743f845":"markdown","f32454a4":"markdown","2d077b46":"markdown","11c8c7a9":"markdown","0faef568":"markdown","52952a05":"markdown","366fef04":"markdown"},"source":{"0666f91c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4723815":"# importing libraries\n# preprocesing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# classification models\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost\nfrom xgboost import XGBRFClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\n\n# model tuning\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)","2d59f0b1":"# reading data here, let's go\ntrain_data=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","5b4a8362":"train_data.head()","8702766d":"train_data.info()","c38012a1":"test_data.isnull().sum()","6d9063cf":"test_data['Fare']=test_data['Fare'].fillna(0)","b50b0f66":"# let's try and find null values\ntrain_data.isnull().sum()","06bbf1f7":"# unique Cabin values\ntrain_data['Cabin'].unique()","992e6586":"train_data['Cabin'].str[:1].value_counts()","1583dc0d":"# to begin with, let's take a look at the distribution\ntrain_data['Age'].plot(kind='hist', bins=20, alpha=0.5)","9c3238a9":"# mean age\nprint(\"Average age:\", round(train_data['Age'].mean()))","191f49fb":"train_data['Embarked'].value_counts()","fb1cf0ed":"train_data.groupby('Embarked')['Survived'].sum()","1834906b":"train_data['Fare'].plot(kind='hist', bins=20, alpha=0.5)","8a74753a":"train_data.info()","f9b03d98":"def clean_data(df):\n    # drop columns\n    #df=df.drop(columns=['Name', 'SibSp', 'Parch', 'PassengerId', 'Cabin', 'Ticket', 'Embarked'])\n    df=df.drop(columns=['Name', 'Ticket'])\n    \n    # impute Cabin and select only first letter of cabin\n    df['Cabin']=df['Cabin'].fillna(\"UNKNOWN\")\n    df['Cabin']=df['Cabin'].str[:1]\n    \n    # impute Age\n    df['Age']=df.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\n    \n    # convert age to integer type\n    df['Age']=df['Age'].astype(int)\n    \n    # impute Embarked\n    df['Embarked']=df['Embarked'].fillna(\"U\")\n    \n    # let's label encode the categorical columns\n    df=pd.get_dummies(df, columns=['Sex', 'Pclass', 'Embarked', 'Parch', 'SibSp', 'Cabin'])\n    \n    return df","ca390bab":"train_df=clean_data(train_data)\ntest_df=clean_data(test_data)","11c8b40d":"train_df.info()","e2562b06":"feature_selector=ExtraTreesClassifier()\nfeature_selector.fit(train_df.drop(columns=['Survived']), train_df['Survived'])\n\nfeature_importance=pd.DataFrame({'feature':train_df.drop(columns=['Survived']).columns, \n             'importance':feature_selector.feature_importances_}).sort_values(by='importance', ascending=False)\n\nfeature_importance","358fb429":"# model testing function\nX=train_df.drop(columns=['Survived'])\n#X=X[list(feature_importance[feature_importance['importance']>0.01]['feature'])]\ny=train_df['Survived']\n\nselector = SelectKBest(chi2, k=10)\nselector.fit(X, y)\ncols = selector.get_support(indices=True)\nX_new = X.iloc[:,cols]\n\ndt=DecisionTreeClassifier(random_state=0)\nsvc=SVC(probability=True, random_state=0)\nknn=KNeighborsClassifier()\nrf=RandomForestClassifier(random_state=0)\ngb=GradientBoostingClassifier(random_state=0)\nxb=xgboost.XGBClassifier(objective='binary:logistic', random_state=0)\nxgbrf=XGBRFClassifier(random_state=0)\nhard_vc=VotingClassifier(estimators=[('Decision Tree', dt), \n                                     ('SVC', svc), ('KNN', knn), \n                                     ('Random Forest', rf), ('Gradient Boosting', gb), \n                                     ('Xgboost',xb), ('Xgboost RF', xgbrf)], voting='hard')\n\nsoft_vc=VotingClassifier(estimators=[('Decision Tree', dt), \n                                     ('SVC', svc), ('KNN', knn), \n                                     ('Random Forest', rf), ('Gradient Boosting', gb), \n                                     ('Xgboost',xb), ('Xgboost RF', xgbrf)], voting='soft')\n\nmodels=[('Decision Tree', dt),\n        ('SVC', svc), ('KNN', knn), \n        ('Random Forest', rf), ('Gradient Boosting', gb), \n        ('Xgboost',xb), ('Xgboost RF', xgbrf), \n        ('Hard Voting Classifier', hard_vc), \n        ('Soft Voting Classifier', soft_vc)]\n\nx_train, x_test, y_train, y_test=train_test_split(X_new, y, test_size=0.3, random_state=0)\ndef model_testing(models):\n    model_names=[]\n    accuracy=[]\n    auc=[]\n    \n    for name, classifier in models:\n        model_names.append(name)\n        classifier.fit(x_train, y_train)\n        preds=classifier.predict(x_test)\n        accuracy.append(accuracy_score(y_test, preds))\n        auc.append(roc_auc_score(y_test, preds))\n        \n    model_score_df=pd.DataFrame(data={'model':model_names, \n                                      'accuracy':accuracy, \n                                      'auc':auc})\n    return model_score_df\n    ","4fe2ecff":"scoring_df=model_testing(models)\nscoring_df.sort_values(by=['accuracy'], ascending=False)","7f75600c":"print(\"AUC:\", round(roc_auc_score(y_test, soft_vc.predict(x_test)),2))","b0d67262":"test_df=test_df[X_new.columns]\noutput_predictions=soft_vc.predict(test_df)\noutput=pd.DataFrame({'PassengerId':test_data.PassengerId, 'Survived':output_predictions})\noutput.to_csv(\"submission.csv\", index=False)\nprint(\"Submission successfully saved\")","30433591":"Now that we know what the data looks like, let's see what's happening with the data at a deeper level. ","95ca27c6":"Let's take a look at the data. ","620211e0":"Let's look at Fare","0a610eb9":"Now let's look at Age","b1e4641d":"So here's the situation, there are 3 columns with null values:\n- Cabin\n- Age\n- Embarked\n\nNow we could drop these columns outright, but let's see if we could impute values first. \n\nLet's begin with Cabin. ","73a92329":"Alright! We can now clean the data.","8505cad4":"Now the average age can be used to fill missing values based on the assumption that most people on the ship were around that age.","8fe3e192":"Now this doesn't look too complicated, but let's see if there's any implications regarding survivability.","853adf94":"We can impute that one row with a missing fare as 0.","053837bd":"## Cleaning Data","d1cbfa11":"And finally we come to embarked.","baeceb38":"This does not follow a normal distribution so we could normalize this variable","19887964":"# Step 3. Submission","b6750094":"Clearly, XGBoost seems to be performing very well here. Not surprised because tree based models perform very well when it comes to classification problems.","88211657":"Even though Accuracy is a good metric, let's take a look at AUC and see if it's actually performing well.","9743f845":"# Feature Selection","f32454a4":"So clearly, most of the people on the ship were in the 20-30 age bracket","2d077b46":"Data's cleaned, let's get to modelling.","11c8c7a9":"All done! \nI have a bunch of models to test and it will be cumbersome writing a lot of code to test each model, so I will be writing a function to fit these models and test them.","0faef568":"Let's use the first letter of the cabin instead.","52952a05":"# Step 0. Data Cleaning and Pre-processing","366fef04":"So it looks like it does have some implication regarding survivability, so we could drop off the missing rows instead of removing this column outright."}}