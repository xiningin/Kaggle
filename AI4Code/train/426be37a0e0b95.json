{"cell_type":{"1248c56b":"code","58445e0d":"code","2a21cc08":"code","b6289228":"code","a95776c1":"code","44f69b71":"code","8e3125b9":"code","a1210a8a":"code","9b4d89bd":"code","2fe98939":"code","a434f1d4":"code","fc0db8ce":"code","42e4566d":"code","4df006e7":"code","79e664f3":"code","685b9088":"code","94b3eda7":"code","a26ac103":"code","96c7d7a2":"code","da403bac":"code","b4c8e302":"code","fc66189e":"code","8b215279":"code","a5fb5b24":"code","760c3d3b":"code","11949783":"code","e11eece8":"code","24be59ef":"code","6c944086":"markdown","e8161730":"markdown","2c88dc7e":"markdown","df943ad2":"markdown","e8a55e59":"markdown","a3e39798":"markdown","f40d8253":"markdown","2a875677":"markdown","314cdfd9":"markdown","56819c66":"markdown","e40528d6":"markdown","92d503b9":"markdown","110bfef2":"markdown","e6d7f302":"markdown","8d740a20":"markdown","142871ac":"markdown","9b22ef9b":"markdown","00ad12e8":"markdown","1b531726":"markdown","35db796e":"markdown","5eb4a3ca":"markdown","c3e43107":"markdown","099c3f23":"markdown","d2a9ee94":"markdown","9a33a6ab":"markdown","667d4ede":"markdown","1db40556":"markdown","28b680ae":"markdown","b86a78ce":"markdown","706de983":"markdown","a3d7277d":"markdown","5acbe1b2":"markdown","a23e032a":"markdown","90ef1d04":"markdown","b8fd14ae":"markdown","641a4505":"markdown","155796a2":"markdown","31ecc8ad":"markdown","0e0b9cd8":"markdown"},"source":{"1248c56b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport random\nimport warnings\n\nfrom time import time  # To time our operations\nfrom collections import defaultdict  # For word frequency\n\nimport spacy  # For preprocessing\n\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve\n\nwarnings.simplefilter(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58445e0d":"data = pd.read_csv(\"\/kaggle\/input\/60k-stack-overflow-questions-with-quality-rate\/data.csv\")","2a21cc08":"display(data.info(),data.head())","b6289228":"dummy_data = data[['Title','Body','Tags','Y']]\ndummy_data['Y'] = dummy_data['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT': 1, 'HQ':2})\ndummy_data.head()","a95776c1":"dummy_data['text'] = dummy_data['Title'] + ' ' + dummy_data['Body']\ndummy_data = dummy_data.drop(['Title', 'Body'], axis=1)\ndummy_data.head()\n","44f69b71":"nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n\ndef cleaning(doc):\n    #remove stopwords + Lemmitze them \n    txt = [token.lemma_ for token in doc if not token.is_stop]\n    \n    # word2vec uses context words to learn the vector representation of a terget word \n    # if sentence is only one or two worlds long,\n    # the benifit for training them is small thus we may drop them \n    if len(txt)>2:\n        return ' '.join(txt)","8e3125b9":"init_cleaning = (re.sub(\"[^A-Za-z]+\",' ',str(row)).lower() for row in dummy_data['text'])","a1210a8a":"t = time()\n\n#batch size used if a document is having words more than 5000 treat it as a seperate doc\ntxt = [cleaning(doc) for doc in nlp.pipe(init_cleaning,batch_size=5000,n_threads=-1)]\n\nprint('Time too clean up everything: {} mins'.format(round((time() - t)\/60,2)))","9b4d89bd":"df_clean = pd.DataFrame({'clean':txt})\ndf_clean = df_clean.dropna().drop_duplicates()\ndf_clean.shape","2fe98939":"from gensim.models.phrases import Phrases,Phraser","a434f1d4":"sent = [row.split() for row in df_clean['clean']]","fc0db8ce":"phrases = Phrases(sent,min_count=30, progress_per=10000)","42e4566d":"bigram = Phraser(phrases)","4df006e7":"sentences = bigram[sent]","79e664f3":"word_freq = defaultdict(int)\nfor sent in sentences:\n    for i in sent:\n        word_freq[i] += 1\nlen(word_freq)","685b9088":"sorted(word_freq, key=word_freq.get, reverse=True)[:10]","94b3eda7":"import multiprocessing\nfrom gensim.models import Word2Vec","a26ac103":"cores = multiprocessing.cpu_count() # Count the number of cores in a computer","96c7d7a2":"w2v_model = Word2Vec(min_count=20,\n                     window=2,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)","da403bac":"t = time()\n\nw2v_model.build_vocab(sentences, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time() - t) \/ 60, 2)))","b4c8e302":"w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to train the model: {} mins'.format(round((time() - t) \/ 60, 2)))","fc66189e":"w2v_model.wv.most_similar(positive=[\"java\"])","8b215279":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n \nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","a5fb5b24":"def tsnescatterplot(model, word, list_names):\n    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n    its list of most similar words, and a list of words.\n    \"\"\"\n    arrays = np.empty((0, 300), dtype='f')\n    word_labels = [word]\n    color_list  = ['red']\n\n    # adds the vector of the query word\n    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n    \n    # gets list of most similar words\n    close_words = model.wv.most_similar([word])\n    \n    # adds the vector for each of the closest words to the array\n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        color_list.append('blue')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n    \n    # adds the vector for each of the words from list_names to the array\n    for wrd in list_names:\n        wrd_vector = model.wv.__getitem__([wrd])\n        word_labels.append(wrd)\n        color_list.append('green')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n        \n    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n    reduc = PCA(n_components=20).fit_transform(arrays)\n    \n    # Finds t-SNE coordinates for 2 dimensions\n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n    \n    # Sets everything up to plot\n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_labels,\n                       'color': color_list})\n    \n    fig, _ = plt.subplots()\n    fig.set_size_inches(9, 9)\n    \n    # Basic plot\n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 40,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    # Adds annotations one by one with a loop\n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n\n    \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE visualization for {}'.format(word.title()))\n    ","760c3d3b":"tsnescatterplot(w2v_model, 'java',[t[0] for t in w2v_model.wv.most_similar(positive=[\"java\"], topn=20)][10:])","11949783":"tsnescatterplot(w2v_model, 'java',[t[0] for t in w2v_model.wv.most_similar(positive=[\"java\"], topn=30)][10:])","e11eece8":"tsnescatterplot(w2v_model, 'swift',[t[0] for t in w2v_model.wv.most_similar(positive=[\"swift\"], topn=20)][10:])","24be59ef":"tsnescatterplot(w2v_model, 'child',[t[0] for t in w2v_model.wv.most_similar(positive=[\"child\"], topn=20)][10:])","6c944086":"## <a id='Model'>Training the model<\/a>\n## Gensim Word2Vec Implementation:\nWe use Gensim implementation of word2vec:","e8161730":"## <a id='PLOTS'>Let's check the 20 similar words to Java and plot it. <\/a>","2c88dc7e":"To make the visualizations more relevant, we will look at the relationships between a query word (in <font color='red'>**red**<\/font>), its most similar words in the model (in <font color=\"blue\">**blue**<\/font>), and other words from the vocabulary (in <font color='green'>**green**<\/font>).","df943ad2":"### Removing characters other than alpha numeric characters ","e8a55e59":"### Recently I started with NLP and after going through several articles and algorithms I thought to implement it. So in this kernel I am try to implement them. Data is of Stack OverFlow and I'll try to understand what the key words are in the coupus. ","a3e39798":"### <a id='t-SNE'>t-SNE visualizations:<\/a>\nt-SNE is a non-linear dimensionality reduction algorithm that attempts to represent high-dimensional data and the underlying relationships between vectors in a lower-dimensional space.<br>\nHere is a good tutorial on it: https:\/\/medium.com\/@luckylwk\/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b","f40d8253":"Creates the relevant phrases from the list of sentences:","2a875677":"## <a id='Pre-Processing'>Pre-Processing<\/a> ","314cdfd9":"## Cleaned Data: \nNow that we have cleaned data lets store it in a new dataframe and drop any null\/ missing values or duplicates ","56819c66":"## Let's see the difference as to how top 30 similar words make a difference","e40528d6":"## Let's see if it works for other words","92d503b9":"## <a id=\"Display\"> Let's look at the Data<\/a>","110bfef2":"Transform the corpus based on the bigrams detected:","e6d7f302":"### Taking advantage of spaCy .pipe() attribute to speed-up the cleaning process:","8d740a20":"The goal of Phraser() is to cut down memory consumption of Phrases(), by discarding model state not strictly needed for the bigram detection task:","142871ac":"## Hope it was great and you learned something. Please UpVote and Comment if you like.Thank You\ud83d\ude03\ud83d\ude4f","9b22ef9b":"Note:- We are currently going to use only two columns but we are still going to keep 2 more columns just for later use\n<ul>\n  <li>1. <code> Id , CreationDate <\/code> were not usefull at the current point. <\/li>\n  <li>2. And let's transform the <code> 'Y' <\/code> column into \" 0,1,2 \" <\/li>\n<\/ul>","00ad12e8":"## <a id=\"Lib\">Import Liberay <\/a>","1b531726":"## Table -- \n1. [ Getting Started](#The_Data) \n    * [Import Liberay](#Lib)\n    * [Explore Data](#Display)\n2. [Pre-Processing](#Pre-Processing) \n    * [Cleaning Text](#Cleaning)\n    * [Bigrams](#Bigrams)\n    * [Most Frequent Words](#Frequent)\n3. [Training the model](#Model)\n    * [The parameters](#parms)\n    * [Building the Vocabulary Table](#Vocab)\n    * [Training of the model](#Training)\n4. [Exploring the model](#Exploring)\n    * [Most similar to](#similar)\n    * [t-SNE visualizations](#t-SNE)\n5. [PLOTS](#PLOTS)\n6. [Conclusion](#Conclusion)","35db796e":"## <a id='Conclusion'>Conclusion:<\/a>\nWell this is not the end of this kernel. Next I will include the part where I will try classifing it into their respective categories.","5eb4a3ca":"## <a id=\"The_Data\">The Data : <\/a>\n\nIts a collection of 60,000 Stack Overflow questions from 2016-2020 and classified them into three categories:\n\nHQ: High-quality posts with 30+ score and without a single edit.\nLQ_EDIT: Low-quality posts with a negative score and with multiple community edits. However, they still remain open after the edits.\nLQ_CLOSE: Low-quality posts that were closed by the community without a single edit.\n","c3e43107":"## <a id='Training'> Training of the model:<\/a>\n_Parameters of the training:_\n* `total_examples` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Count of sentences;\n* `epochs` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Number of iterations (epochs) over the corpus - [10, 20, 30]","099c3f23":"## Introduction","d2a9ee94":"## <a id='Cleaning'>Cleaning Text<\/a> ","9a33a6ab":"Again I can find words that are related to Java Which is pretty good.  ","667d4ede":"As Phrases() takes a list of list of words as input:","1db40556":"### We are lemmatizing and removing the stopwords and non-alphabetic characters for each line . \n### For this purpose we are using SpaCy and its build-in pipeline for this purpose.","28b680ae":"![stack-overflow-for-teams.webp](attachment:stack-overflow-for-teams.webp)","b86a78ce":"As being from a coding background I can find some words and terms similar to Java. ","706de983":"## <a id='parms'> The parameters <\/a> :\n\n* `min_count` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Ignores all words with total absolute frequency lower than this - (2, 100)\n\n\n* `window` <font color='purple'>=<\/font> <font color='green'>int<\/font> - The maximum distance between the current and predicted word within a sentence. E.g. `window` words on the left and `window` words on the left of our target - (2, 10)\n\n\n* `size` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Dimensionality of the feature vectors. - (50, 300)\n\n\n* `negative` <font color='purple'>=<\/font> <font color='green'>int<\/font> - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n\n\n* `workers` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Use these many worker threads to train the model (=faster training with multicore machines)","a3d7277d":"## Let's see if it works for something random","5acbe1b2":"### Again let's combine <code> 'Text' = Title and Body <\/code> ","a23e032a":"## <a id='Frequent'>Most Frequent Words:<\/a>\nMainly a sanity check of the effectiveness of the lemmatization, removal of stopwords, and addition of bigrams.","90ef1d04":"## <a id='Bigrams'>Bigrams<\/a>\nWe are using Gensim Phrases package to automatically detect common phrases (bigrams) from a list of sentences.","b8fd14ae":"It is again giving great results.","641a4505":"The results are great it actually found similar words to Child and it was able to comprehend it with  \"Classes and Child\" which is great. ","155796a2":"## Model in 3 Steps: \n\n1. `Word2Vec()`: \n>In this first step, I set up the parameters of the model one-by-one. <br>I do not supply the parameter `sentences`, and therefore leave the model uninitialized, purposefully.\n2. `.build_vocab()`: \n>Here it builds the vocabulary from a sequence of sentences and thus initialized the model. <br>With the loggings, I can follow the progress and even more important, the effect of `min_count` and `sample` on the word corpus. \n3. `.train()`:\n>Finally, trains the model.<br>\nThe loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously.","31ecc8ad":"# <a id='Exploring'>Exploring the model<\/a>\n## <a id='similar'>Most similar to:<\/a>\n\nHere, we will ask our model to find the word most similar to some of the words in the corpus","0e0b9cd8":"## <a id='Vocab'> Building the Vocabulary Table:<\/a>\nWord2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"}}