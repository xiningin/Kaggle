{"cell_type":{"abe7fd72":"code","5d006175":"code","40a31127":"code","201d7f8d":"code","d19153f1":"code","3e720a5a":"code","836c3262":"code","242295c6":"code","314372db":"code","9743bb1c":"code","4064a774":"code","f778c2d9":"markdown","c80513bd":"markdown","02b2ba2c":"markdown","1e111b2b":"markdown","52250b5a":"markdown","f80ac2b6":"markdown","cbc6e80b":"markdown","27c73cc3":"markdown","a9b88208":"markdown","4f689812":"markdown","d05c463a":"markdown","21a98c2f":"markdown"},"source":{"abe7fd72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d006175":"df = pd.read_csv('..\/input\/german-credit-data-with-risk\/german_credit_data.csv')\n\nprint('The dataset consists of {} entries and {} features'.format(df.shape[0], df.shape[1]))\ndf.head()","40a31127":"df.info()","201d7f8d":"df.drop(['Unnamed: 0'], axis=1, inplace=True)","d19153f1":"df['Saving accounts'].fillna('none', inplace=True)\ndf['Checking account'].fillna('none', inplace=True)\n\ndf.isnull().sum()","3e720a5a":"import matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\n\nplt.figure(figsize=(20, 15))\n\nplt.subplot(3,2,1)\nsns.countplot(df['Sex']);\n\nplt.subplot(3,2,2)\nsns.distplot(df['Age'], kde=False);\n\nplt.subplot(3,2,3)\nsns.countplot(df['Job']);\n\nplt.subplot(3,2,4)\nsns.countplot(df['Housing'], order=df['Housing'].value_counts().index);\n\nplt.subplot(3,2,5)\nsns.countplot(df['Saving accounts'], order=df['Saving accounts'].value_counts().index);\n\nplt.subplot(3,2,6)\nsns.countplot(df['Checking account'], order=df['Checking account'].value_counts().index);","836c3262":"sns.distplot(df['Credit amount'], kde=False);\nplt.show()\n\nsns.countplot(df['Purpose'], order=df['Purpose'].value_counts().index);\nplt.xticks(rotation=45)\nplt.show()\n\nsns.countplot(df['Risk']);\nplt.show()","242295c6":"from sklearn.preprocessing import LabelEncoder\n\nnew_df = df.copy()\n# binary encoding\nfeatures_toencode = ['Sex', 'Risk']\nfor f in features_toencode:\n    le = LabelEncoder()\n    new_df[f] = le.fit_transform(df[f])\n\n# encoder in order\nnew_df['Checking account'] = df['Checking account'].map({'none': 0, 'little': 1, 'moderate': 2, 'rich': 3})\nnew_df['Saving accounts'] = df['Saving accounts'].map({'none': 0, 'little': 1, 'moderate': 2, 'rich': 3, 'quite rich': 4})\n\n# encode in dummies\nnew_df = pd.get_dummies(new_df)\n\nnew_df.head()","314372db":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX = new_df.drop(['Risk'], axis=1)\ny = new_df['Risk']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify=y)\n\ndt = DecisionTreeClassifier().fit(X_train, y_train)\ntrain_pred = dt.predict(X_train)\ntest_pred = dt.predict(X_test)\n\nprint('Train set score:', accuracy_score(y_train, train_pred))\nprint('Test set score:', accuracy_score(y_test, test_pred))","9743bb1c":"from sklearn.model_selection import GridSearchCV\n\nparams = {'max_depth': range(1,10,2), 'min_samples_split' : range(2,200,10)}\n\nmodel = GridSearchCV(DecisionTreeClassifier(random_state=123), params, n_jobs=3, cv=3)\nmodel.fit(X_train, y_train)\n\nmodel.best_estimator_","4064a774":"print(model.score(X_train, y_train))\nprint(model.score(X_test, y_test))","f778c2d9":"# Fill Null Value","c80513bd":"# Feature Engineering","02b2ba2c":"We can see here the dataset consists of numerous categorical features, which we will need to transform them so that ML model can work properly later.","1e111b2b":"I decided to use LabelEncoder on Sex and Risk features because they are only binary categorical data, whereas if the number of categorical values is more than 2, one hot encoding or get dummies is preferred (for instance, the Purpose feature). However, although Checking and Saving account features are not binary, I did not apply get_dummies on them because their values are in hierarchy. I encoded them in order instead. ","52250b5a":"The charts are here to show the count and ditribution of the features.","f80ac2b6":"# Classification","cbc6e80b":"Apparently without tuning the hyperparameters, the decision tree model shows significant overfitting.","27c73cc3":"# Goal\nIn this project I would like to explore the dataset of credit risk to find its insight. I will use the simple data manipulating and visualization skills in order to work on it. Then, decision tree algorithm will be implemented to see if I can predict whether a credit risk is good or bad based on the applicant's information.","a9b88208":"After several tries, the test set score is more or less around 0.71 approximately. I think it is because we only have 1000 entries in this dataset, which are far from enough for ML model to be in its full capacity. \n\nIf you have any thought on how to improve the model or this whole project itself, feel free to comment below. As a beginner, I'm eager to improve my data science skill!","4f689812":"There are not many null values we need to fill here. Only little needed to be performed.","d05c463a":"# Visualization","21a98c2f":"# First glimpse of the data"}}