{"cell_type":{"b0d27429":"code","fc33387d":"code","602109a4":"code","2689a2f1":"code","b2ef05fb":"code","03e5f747":"code","57a83cbf":"code","645a9284":"code","1dea2259":"code","6b96ef40":"code","6e0994f4":"code","c6cbb6ed":"code","f3528628":"code","b98c293a":"code","8252ea32":"code","4d837005":"code","a265abb0":"code","0380790a":"code","2a98487d":"code","63c06a3f":"code","ca501c91":"code","9886a391":"code","ebf3927a":"code","65bf088c":"code","69e33ff5":"code","280dd363":"code","c3cae56d":"code","a376fc19":"code","de88ad10":"code","d5db3c28":"code","1c4215f7":"code","6d824cc7":"code","d9cf6071":"code","ad1e70fc":"code","4b3fa92d":"code","29683f7b":"code","c8614f7d":"code","c79a2f16":"code","4e97d2ee":"code","d7081a2e":"code","04717d8d":"code","c3a7fe9d":"code","1f7b94c3":"code","decd3efc":"markdown","7266f89d":"markdown","69d2f11b":"markdown","f943824c":"markdown","6a423e62":"markdown","9fb2f50d":"markdown","464d8c1a":"markdown","136de2ba":"markdown","59cc431e":"markdown","5b76a045":"markdown","bad14f95":"markdown","1f396882":"markdown","b3df7ca3":"markdown","2bd915a6":"markdown","228e2f49":"markdown","5f8cd80f":"markdown","0fb15c48":"markdown","588e73eb":"markdown","ca9ab722":"markdown","e406e1a7":"markdown","21a7003b":"markdown","b79d65e9":"markdown","e3f46bb3":"markdown"},"source":{"b0d27429":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42","fc33387d":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set on axis 0\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)\ndf_all.head(10)","602109a4":"def display_missing(df):    \n    for col in df.columns.tolist():          \n        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n    print('\\n')\n    \nfor df in dfs:\n    print('{}'.format(df.name))\n    display_missing(df)","2689a2f1":"df_all['Age'] = df_all.groupby(['Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","b2ef05fb":"df_all['Embarked'].fillna(df_all['Embarked'].mode()[0], inplace = True)","03e5f747":"med_fare = df_all.groupby(['Pclass']).Fare.median()[3]\n# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\ndf_all['Fare'] = df_all['Fare'].fillna(med_fare)","57a83cbf":"# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\ndf_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\ndf_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n                                                                        'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\n\ndef get_pclass_dist(df):\n    \n    # Creating a dictionary for every passenger class count in every deck\n    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    decks = df.columns.levels[0]    \n    \n    for deck in decks:\n        for pclass in range(1, 4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count \n            except KeyError:\n                deck_counts[deck][pclass] = 0\n                \n    df_decks = pd.DataFrame(deck_counts)    \n    deck_percentages = {}\n\n    # Creating a dictionary for every passenger class percentage in every deck\n    for col in df_decks.columns:\n        deck_percentages[col] = [(count \/ df_decks[col].sum()) * 100 for count in df_decks[col]]\n        \n    return deck_counts, deck_percentages\n\ndef display_pclass_dist(percentages):\n    \n    df_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85\n    \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, pclass1, color='#b5ffb9', edgecolor='white', width=bar_width, label='Passenger Class 1')\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='#f9bc86', edgecolor='white', width=bar_width, label='Passenger Class 2')\n    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#a3acff', edgecolor='white', width=bar_width, label='Passenger Class 3')\n\n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Passenger Class Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Passenger Class Distribution in Decks', size=18, y=1.05)   \n    \n    plt.show()    \n\nall_deck_count, all_deck_per = get_pclass_dist(df_all_decks)\ndisplay_pclass_dist(all_deck_per)","645a9284":"# Passenger in the T deck is changed to A\nidx = df_all[df_all['Deck'] == 'T'].index\ndf_all.loc[idx, 'Deck'] = 'A'","1dea2259":"df_all_decks_survived = df_all.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n                                                                                   'Embarked', 'Pclass', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name':'Count'}).transpose()\n\ndef get_survived_dist(df):\n    \n    # Creating a dictionary for every survival count in every deck\n    surv_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'M':{}}\n    decks = df.columns.levels[0]    \n\n    for deck in decks:\n        for survive in range(0, 2):\n            surv_counts[deck][survive] = df[deck][survive][0]\n            \n    df_surv = pd.DataFrame(surv_counts)\n    surv_percentages = {}\n\n    for col in df_surv.columns:\n        surv_percentages[col] = [(count \/ df_surv[col].sum()) * 100 for count in df_surv[col]]\n        \n    return surv_counts, surv_percentages\n\ndef display_surv_dist(percentages):\n    \n    df_survived_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85    \n\n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, not_survived, color='#b5ffb9', edgecolor='white', width=bar_width, label=\"Not Survived\")\n    plt.bar(bar_count, survived, bottom=not_survived, color='#f9bc86', edgecolor='white', width=bar_width, label=\"Survived\")\n \n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Survival Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Survival Percentage in Decks', size=18, y=1.05)\n    \n    plt.show()\n\nall_surv_count, all_surv_per = get_survived_dist(df_all_decks_survived)\ndisplay_surv_dist(all_surv_per)","6b96ef40":"df_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')\ndf_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG')\n\ndf_all['Deck'].value_counts()","6e0994f4":"df_all.drop(['Cabin'], inplace=True, axis=1)\n\ndf_train, df_test = divide_df(df_all)\ndfs = [df_train, df_test]\n\nfor df in dfs:\n    display_missing(df)","c6cbb6ed":"survived = df_train['Survived'].value_counts()[1]\nnot_survived = df_train['Survived'].value_counts()[0]\nsurvived_per = survived \/ df_train.shape[0] * 100\nnot_survived_per = not_survived \/ df_train.shape[0] * 100\n\nprint('{} of {} passengers survived and it is the {:.2f}% of the training set.'.format(survived, df_train.shape[0], survived_per))\nprint('{} of {} passengers didnt survive and it is the {:.2f}% of the training set.'.format(not_survived, df_train.shape[0], not_survived_per))\n\nplt.figure(figsize=(10, 8))\nsns.countplot(df_train['Survived'])\n\nplt.xlabel('Survival', size=15, labelpad=15)\nplt.ylabel('Passenger Count', size=15, labelpad=15)\nplt.xticks((0, 1), ['Not Survived ({0:.2f}%)'.format(not_survived_per), 'Survived ({0:.2f}%)'.format(survived_per)])\nplt.tick_params(axis='x', labelsize=13)\nplt.tick_params(axis='y', labelsize=13)\n\nplt.title('Training Set Survival Distribution', size=15, y=1.05)\n\nplt.show()","f3528628":"df_all = concat_df(df_train, df_test)\ndf_all.head()","b98c293a":"df_all['Fare'] = pd.qcut(df_all['Fare'], 13)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=df_all)\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","8252ea32":"df_all['Age'] = pd.qcut(df_all['Age'], 6)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Age', hue='Survived', data=df_all)\n\nplt.xlabel('Age', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15, y=1.05)\n\nplt.show()","4d837005":"df_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all['Family_Size'].value_counts().index, y=df_all['Family_Size'].value_counts().values, ax=axs[0][0])\nsns.countplot(x='Family_Size', hue='Survived', data=df_all, ax=axs[0][1])\n\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n\nsns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, y=df_all['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all, ax=axs[1][1])\n\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n\nplt.show()","a265abb0":"df_train = df_all.loc[:890]\ndf_test = df_all.loc[891:]\ndfs = [df_train, df_test]","0380790a":"df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\nfig, axs = plt.subplots(nrows=2, figsize=(20, 20))\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[0])\n\naxs[0].tick_params(axis='x', labelsize=10)\naxs[1].tick_params(axis='x', labelsize=15)\n\nfor i in range(2):    \n    axs[i].tick_params(axis='y', labelsize=15)\n\naxs[0].set_title('Title Feature Value Counts', size=20, y=1.05)\n\ndf_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[1])\naxs[1].set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\n\nplt.show()\ndf_train = df_all.loc[:890]\ndf_test = df_all.loc[891:]\ndfs = [df_train, df_test]","2a98487d":"non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n\nfor df in dfs:\n    for feature in non_numeric_features:        \n        df[feature] = LabelEncoder().fit_transform(df[feature])","63c06a3f":"cat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\nencoded_features = []\n\nfor df in dfs:\n    for feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n\ndf_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[6:]], axis=1)","ca501c91":"df_train","9886a391":"df_all = concat_df(df_train, df_test)\ndrop_cols = ['Deck', 'Embarked', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n             'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title']\n\ndf_all.drop(columns=drop_cols, inplace=True)\n\ndf_all.head()","ebf3927a":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression, Ridge, LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer","65bf088c":"X_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))\ny_train = df_train['Survived'].values\nX_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))","69e33ff5":"random_state = 4\nclassifiers = []\nclassifiers.append(('SVC',SVC(random_state=random_state)))\nclassifiers.append(('DecisionTree', DecisionTreeClassifier(random_state=random_state)))\nclassifiers.append(('AdaBoost', AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),\\\n                                                  random_state=random_state,learning_rate=0.1)))\nclassifiers.append(('RandomForest', RandomForestClassifier(random_state=random_state)))\nclassifiers.append(('GradientBoost', GradientBoostingClassifier(random_state=random_state)))\nclassifiers.append(('MPL', make_pipeline(StandardScaler(), MLPClassifier(random_state=random_state))))\nclassifiers.append(('KNN',make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=7))))\n\n# evaluate each model \nresults = []\nnames = []\nfor name, classifier in classifiers:\n    kfold = model_selection.KFold(n_splits= 3, random_state=random_state, shuffle = True)\n    cv_results = model_selection.cross_val_score(classifier, X_train, y = y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n","280dd363":"def random_forest(X, Y, X_test):\n    parameters = {'max_depth' : [2, 4, 5, 10], \n                  'n_estimators' : [200, 500, 1000, 2000], \n                  'min_samples_split' : [3, 4, 5], \n\n                 }\n    kfold = model_selection.KFold(n_splits=3, random_state=random_state, shuffle = True)\n    model_RFC = RandomForestClassifier(random_state = 4, n_jobs = -1)\n    search_RFC = GridSearchCV(model_RFC, parameters, n_jobs = -1, cv = kfold, scoring = 'accuracy',verbose=1)\n    search_RFC.fit(X, Y)\n    predicted= search_RFC.predict(X_test)\n    \n    print(\"Best parameters are:\", search_RFC.best_params_)\n    print(\"Best accuracy achieved:\",search_RFC.best_score_)\n    \n    return search_RFC.best_params_, model_RFC, search_RFC, predicted","c3cae56d":"# param_RFC_onehot, model_RFC_onehot, search_RFC_onehot, predicted_cv_RFC_onehot = random_forest(X_train, y_train, X_test)","a376fc19":"def fit_pred_RF(X, Y, X_test):\n\n    model_RFC = RandomForestClassifier(max_depth =5,  min_samples_split =4, n_estimators = 5000,\n                                     random_state = 4, n_jobs = -1)\n    model_RFC.fit(X, Y)\n    \n    predicted= model_RFC.predict(X_test)\n    \n    return predicted, model_RFC","de88ad10":"predicted_RFC, model_RFC= fit_pred_RF(X_train, y_train, X_test)","d5db3c28":"def grad_boost(X, Y, X_test):\n\n    parameters = {'max_depth' : [2, 4, 10, 15], \n                  'n_estimators' : [10, 50, 100], \n                  'min_samples_split' : [5, 10, 15],\n                 }\n    kfold = model_selection.KFold(n_splits=3, random_state=random_state, shuffle = True)\n    model_GBC = GradientBoostingClassifier(random_state = 4)\n    search_GBC = GridSearchCV(model_GBC, parameters, n_jobs = -1, cv = kfold, scoring = 'accuracy',verbose=1)\n    search_GBC.fit(X, Y)\n    predicted= search_GBC.predict(X_test)\n    \n    print(\"Best parameters are:\", search_GBC.best_params_)\n    print(\"Best accuracy achieved:\",search_GBC.cv_results_['mean_test_score'].mean())\n    \n    return search_GBC.best_params_, model_GBC, search_GBC, predicted\n    ","1c4215f7":"# param_GBC_onehot, model_GBC_onehot, search_GBC_onehot, predicted_cv_GBC_onehot = grad_boost(X_train, y_train, X_test)","6d824cc7":"def fit_pred_GBC(X, Y, X_test):\n\n    model_GBC = GradientBoostingClassifier(max_depth = 3, min_samples_split = 15, n_estimators = 20,\\\n                                 random_state = 4, max_features= 'auto')\n    model_GBC.fit(X, Y)\n    \n    predicted= model_GBC.predict(X_test)\n    \n    return predicted, model_GBC","d9cf6071":"predicted_GBC, model_GBC = fit_pred_GBC(X_train, y_train, X_test)","ad1e70fc":"def mod_KNN(X, Y, X_test):\n    \n    model_KNN=make_pipeline(MinMaxScaler(),KNeighborsClassifier())\n    #KNN.get_params().keys()\n    kfold = model_selection.KFold(n_splits=3, random_state=random_state, shuffle = True)\n    parameters=[{'kneighborsclassifier__n_neighbors': [2,3,4,5,6,7,8,9,10]}]\n    search_KNN = GridSearchCV(estimator=model_KNN, param_grid=parameters, scoring='accuracy', cv=kfold)\n    scores_KNN=cross_val_score(search_KNN, X, Y,scoring='accuracy', cv=kfold, verbose=1)\n    search_KNN.fit(X, Y)\n    predicted= search_KNN.predict(X_test)\n    \n    print(\"Best parameters are:\", search_KNN.best_params_)\n    print(\"Best accuracy achieved:\",search_KNN.cv_results_['mean_test_score'].mean())\n    \n    return search_KNN.best_params_, model_KNN, search_KNN, predicted","4b3fa92d":"# param_KNN_onehot, model_KNN_onehot, search_KNN_onehot, predicted_cv_KNN_onehot = mod_KNN(X_train, y_train, X_test)","29683f7b":"def fit_pred_KNN(X, Y, X_test):\n\n    model_KNN = make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=12))\n    \n    model_KNN.fit(X, Y)\n    \n    predicted= model_KNN.predict(X_test)\n    \n    return predicted, model_KNN","c8614f7d":"predicted_KNN, model_KNN = fit_pred_KNN(X_train, y_train, X_test)","c79a2f16":"def mod_SVC(X, Y, X_test):\n\n    model_SVC=make_pipeline(StandardScaler(),SVC(random_state=1))\n    parameters=[{'svc__C': [0.0001,0.001,0.1,1, 10, 100], \n           'svc__gamma':[0.0001,0.001,0.1,1,10,50,100],\n           'svc__kernel':['rbf'],\n           'svc__degree' : [1,2,3,4]\n          }]\n    kfold = model_selection.KFold(n_splits=3, random_state=random_state, shuffle = True)\n    search_SVC = GridSearchCV(estimator=model_SVC, param_grid = parameters, scoring='accuracy', cv=kfold)\n    scores_SVC=cross_val_score(search_SVC, X, Y,scoring='accuracy', cv=kfold, verbose =1)\n    search_SVC.fit(X, Y)\n    predicted= search_SVC.predict(X_test)\n    \n    print(\"Best parameters are:\", search_SVC.best_params_)\n    print(\"Best accuracy achieved:\",search_SVC.cv_results_['mean_test_score'].mean())\n    \n    return search_SVC.best_params_, model_SVC, search_SVC, predicted","4e97d2ee":"# param_SVC_onehot, model_SVC_onehot, search_SVC_onehot, predicted_cv_SVC_onehot = mod_SVC(X_train, y_train, X_test)","d7081a2e":"def fit_pred_SVC(X, Y, X_test):\n\n    model_SVC =SVC(random_state=random_state, C= 1, gamma = 0.001, kernel = 'rbf', degree =1)\n    \n    model_SVC.fit(X, Y)\n    \n    predicted= model_SVC.predict(X_test)\n    \n    return predicted, model_SVC","04717d8d":"predicted_SVC, model_SVC = fit_pred_SVC(X_train, y_train, X_test)","c3a7fe9d":"predicted = np.where(((predicted_SVC + predicted_KNN+predicted_RFC+predicted_RFC)\/4) > 0.5, 1, 0)","1f7b94c3":"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = df_test['PassengerId']\nsubmission_df['Survived'] = predicted\nsubmission_df.to_csv('submissions.csv', header=True, index=False)\nsubmission_df.head(10)","decd3efc":"* Deck **D** has **87%** 1st class and **13%** 2nd class passengers.\n* Deck **E** has **83%** 1st class, **10%** 2nd class and **7%** 3rd class passengers.\n* Deck **F** has **62%** 2nd class and **38%** 3rd class passengers.\n* **100%** of **G** deck are 3rd class passengers.\n* There is one person on the boat deck in the **T** cabin and he is a 1st class passenger. **T** cabin passenger has the closest resemblance to **A** deck passengers, so he is grouped in **A** deck.\n* Passengers labeled as **M** are the missing values in the `Cabin` feature. I don't think it is possible to find those passengers' real `Deck`. I decided to use **M** like a deck itself.","7266f89d":"### Gradient Boosting","69d2f11b":"### Family size\n* `Family_Size` is created by adding `SibSp`, `Parch` and **1**. `SibSp` is the count of siblings and spouse, and `Parch` is the count of parents and children. Those columns are added to find the total size of families. Adding **1** at the end, is the current passenger.\n* Graphs have clearly shown that family size is a predictor of survival because different values have different survival rates.\n* Family Size with **1** are labeled as **Alone**.\n* Family Size with **2**, **3** and **4** are labeled as **Small**.\n* Family Size with **5** and **6** are labeled as **Medium**.\n* Family Size with **7**, **8** and **11** are labeled as **Large**.","f943824c":"### SVC","6a423e62":"### One-Hot Encoding the Categorical Features\nThe categorical features (`Pclass`, `Sex`, `Deck`, `Embarked`, `Title`) are converted to one-hot encoded features with `OneHotEncoder`. `Age` and `Fare` features are not converted because they are ordinal unlike the previous ones.","9fb2f50d":"# Machine Learning","464d8c1a":"### Age","136de2ba":"### Embaraked\nWe will just fill them with most frequent value for `Embarked` ","59cc431e":"## Missing values","5b76a045":"### Title\n* `Title` is created by extracting the prefix before the `Name` feature values.\n* According to the graph below, there are many titles which are occuring very few times. Some of those titles doesn't seem correct and they need to be replaced.\n* **Miss**, **Mrs**, **Ms**, **Mlle**, **Lady**, **Mme**, **the Countess**, **Dona** titles are replaced with **Miss\/Mrs\/Ms** because all of them are actually females. Values like **Mlle**, **Mme** and **Dona** are actually the name of the passengers, but they are classified as titles because `Name` feature is split by comma.\n* **Dr**, **Col**, **Major**, **Jonkheer**, **Capt**, **Sir**, **Don** and **Rev** titles are replaced with **Dr\/Military\/Noble\/Clergy** because those passengers have similar characteristics.\n* **Master** is a unique title. It is given to male passengers below age **26**. They have the highest survival rate among all male groups.**","bad14f95":"### Age\nMissing values in `Age` feature are filled with the median age, but using the median age of the whole data set is not a good choice. Median age of a group is much better because the new values would be more informative. Median age of `Pclass` groups is the best choice because of its high correlation with `Age` **(0.408106)** and `Survived` **(0.338481)** features,","1f396882":"# Feature Engineering\n## Binning the Continuous Features\n### Fare\n","b3df7ca3":"## Feature Transformation\n### Label Encoding the Non-Numerical Features\n`Embarked`, `Sex`, `Deck` , `Title` and `Family_Size_Grouped` are object type, and `Age` and `Fare` features are category type. They are converted to numerical type with `LabelEncoder`. `LabelEncoder` basically labels the classes from **0** to **n**. This process is necessary for Machine Learning algorithms to learn from those features.","2bd915a6":"As I suspected, every deck has different survival rates and that information can't be discarded. Deck **B**, **C**, **D** and **E** have the highest survival rates. Those decks are mostly occupied by 1st class passengers. **M** has the lowest survival rate which is mostly occupied by 2nd and 3rd class passengers. To conclude cabins used by 1st class passengers have higher survival rates than cabins used by 2nd and 3rd class passengers. In my opinion **M** (Missing `Cabin` values) has the lowest survival rate because they couldn't retrieve the cabin data of the victims. That's why I believe, labeling that group as **M** is a reasonable way to handle the missing data. It is a unique group with same characteristics. `Deck` feature has high-cardinality right now, so some of the values are grouped with each other depending on their resemblance.\n* **A**, **B** and **C** decks are labeled as **ABC** because all of them have only 1st class passengers.\n* **D** and **E** decks are labeled as **DE** because both of them have similar passenger class distribution and same survival rate.\n* **F** and **G** decks are labeled as **FG** because of the previous reasons.\n* **M** deck doesn't need to be grouped with other decks because it is very different from others and has the lowest survival rate.","228e2f49":"### KNN","5f8cd80f":"## Survival Distribution\n* **38.38%** (342\/891) of the training set is **Class 1**.\n* **61.62%** (549\/891) of the training set is **Class 0**.","0fb15c48":"For further modeling we will chose four best performing models: SVC, Random Forest, Gradient Boost, and KNN","588e73eb":"# Loading the Dataset","ca9ab722":"### Cabin\nThe `Cabin` feature is little bit tricky and it needs further exploration. The large portion of the `Cabin` feature is missing and the feature itself can't be ignored completely because some cabins might have higher survival rates. It turns out to be the first letter of the `Cabin` values are the decks in which the cabins are located. Those decks were mainly separated for one passenger class, but some of them were used by multiple passenger classes.","e406e1a7":"### Fare\n\nThere is only one passenger with missing `Fare` value. We can assume that `Fare` is related to `Pclass` features.","21a7003b":"# Import Libararies\n","b79d65e9":"## Overview\n* `PassengerId` feature is the unique id of the row and it doesn't have any effect on `Survived`.\n* `Survived` feature is binary (**0** or **1**); \n    - **1 = Survived**\n    - **0 = Not Survived**\n* `Pclass` (Passenger Class) feature is the socio-economic status of the passenger. It is a categorical ordinal feature which has **3** unique values (**1**,  **2 **or **3**);\n    - **1 = Upper Class**\n    - **2 = Middle Class**\n    - **3 = Lower Class**\n* `Name`, `Sex` and `Age` features are self-explanatory.\n* `SibSp` feature is the total number of the passengers' siblings and spouse.\n* `Parch` feature is the total number of the passengers' parents and children.\n* `Ticket` feature is the ticket number of the passenger.\n* `Fare` feature is the passenger fare.\n* `Cabin` feature is the cabin number of the passenger.\n* `Embarked` is port of embarkation. It is a categorical feature and it has **3** unique values (**C**, **Q** or **S**);\n    - **C = Cherbourg**\n    - **Q = Queenstown**\n    - **S = Southampton**","e3f46bb3":"As seen from above, there are some columns have missing values:\n* Training set have missing values in `Age`, `Cabin` and `Embarked` columns.\n* Test set have missing values in `Age`, `Cabin` and `Fare` columns.\n\nThe amount of missing values in `Age`, `Embarked` and `Fare` features are relatively smaller compared to the total samples, but roughly **80%** of the `Cabin` feature is missing. Missing values in `Age`, `Embarked` and `Fare` features can be filled with descriptive statistical measures, but that wouldn't work for `Cabin` feature."}}