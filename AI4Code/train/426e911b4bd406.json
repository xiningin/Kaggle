{"cell_type":{"5da8bbca":"code","2773b6f1":"code","eab33aed":"code","cc795566":"code","23da449a":"code","314ebd64":"code","27a1befa":"code","8d003085":"code","eef466bb":"code","eecb360f":"code","0bb6739a":"code","f552116f":"code","6e578dbf":"code","cdc71e74":"code","bca89f3c":"code","c4f952f0":"code","cc610b80":"code","bd812a0c":"code","295ad316":"code","53934ce2":"code","7abbb558":"code","4f8e79d4":"code","cfcad270":"code","88e07ad2":"code","3bf7395b":"code","c133a43b":"code","4ce02785":"code","1b8dd441":"code","ab660676":"code","ff12305f":"code","e79a72c3":"code","2ba02445":"code","7d019674":"code","c9f1300d":"code","d9e98fda":"code","ef34a77b":"code","5060964a":"code","06cc9069":"code","394cdc5c":"code","c4f77ac8":"code","bac81c33":"code","8e5d3a02":"code","662e3aea":"code","17c9ba75":"markdown","6cd8d91e":"markdown","78130f2e":"markdown","e17ae733":"markdown","48b38eef":"markdown","55dd2af8":"markdown","22d73535":"markdown","dc6c568c":"markdown","a017be60":"markdown","44eafc15":"markdown","8dbb6118":"markdown","63204434":"markdown","e502e6f5":"markdown","24edca4e":"markdown","e12588a3":"markdown","a204da11":"markdown","605e53c7":"markdown","85c80be7":"markdown","4a1c85fd":"markdown","cc8c3d88":"markdown","3c189508":"markdown","84fbc8e4":"markdown","99511898":"markdown","51a8397f":"markdown","47af98ce":"markdown","645afc82":"markdown","48701c73":"markdown","7a66a4a9":"markdown","170dfa14":"markdown","4e752704":"markdown","0fc43995":"markdown","2c293ef5":"markdown","a1c92bc7":"markdown","ab8b9b73":"markdown","2d62f694":"markdown","69ac897c":"markdown"},"source":{"5da8bbca":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport itertools\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\n%matplotlib inline","2773b6f1":"train = pd.read_csv('..\/input\/fake-news-dataset\/train.csv')\ntrain.head()","eab33aed":"train.head()","cc795566":"train.shape","23da449a":"train.isnull().sum()","314ebd64":"train['class'].value_counts()","27a1befa":"train[train['class'] == 'February 5, 2017']","8d003085":"#shifting the column values in the respective places\ntrain.iloc[504, 2] = train.iloc[504, 3]\ntrain.iloc[504, 3] = train.iloc[504, 4]\ntrain.iloc[504, 4] = train.iloc[504, 5]\ntrain.iloc[504, 5] = train.iloc[504, 6]\ntrain.iloc[504, 6] = np.nan","eef466bb":"train.iloc[[504]]","eecb360f":"train.drop(['index', 'Unnamed: 6'], axis = 1, inplace=True)\ntrain.head()","0bb6739a":"train.describe(include = 'all').T","f552116f":"train.drop_duplicates(subset = ['text'], inplace=True)\ntrain.reset_index(drop = True, inplace = True)\ntrain.describe(include = 'all').T","6e578dbf":"train['subject'].value_counts().plot.pie(figsize = (7, 7));","cdc71e74":"import re\nfrom nltk.corpus import stopwords\n\ndef stopwordsRemover(document):\n    corpus = []\n    for i in range(len(train)):\n        temp = re.sub('[^a-zA-Z]', ' ', document[i])\n        temp = temp.lower()\n        temp = temp.split()\n\n        temp = [word for word in temp if not word in stopwords.words('english')]\n        temp = ' '.join(temp)\n        corpus.append(temp)\n    return(corpus)\nnoStopWordTitle = stopwordsRemover(train['title'])\nnoStopWordText = stopwordsRemover(train['text'])","bca89f3c":"#first 10 titles\nnoStopWordTitle[:10]","c4f952f0":"train.insert(0, 'noStopWordTitle', noStopWordTitle, True)\ntrain.insert(1, 'noStopWordText', noStopWordText, True)","cc610b80":"train.drop(['title', 'text'], axis = 1, inplace = True)","bd812a0c":"train.head()","295ad316":"fakeTitles = train.noStopWordTitle[train['class'] == 'Fake']\nrealTitles = train.noStopWordTitle[train['class'] == 'Real']\n\nmergedFake = ' '.join(fakeTitles)\nmergedReal = ' '.join(realTitles)","53934ce2":"from nltk import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import Counter\n\ndef ngramFunct(corpus, n):\n    token = nltk.word_tokenize(corpus)\n    ans = ngrams(token,n)\n    return(Counter(ans))","7abbb558":"unigramReal = ngramFunct(mergedReal, 1)\nunigramFake = ngramFunct(mergedFake, 1)\nufreqReal = (nltk.FreqDist(unigramReal))\nufreqFake = (nltk.FreqDist(unigramFake))","4f8e79d4":"plt.title('Top 20 Unigrams in Real News')\nufreqReal.plot(20, cumulative=False, color = 'b');\n\nplt.title('Top 20 Unigrams in Fake News')\nufreqFake.plot(20, cumulative=False, color = 'r');","cfcad270":"bigramReal = ngramFunct(mergedReal, 2)\nbigramFake = ngramFunct(mergedFake, 2)\nbfreqReal = (nltk.FreqDist(bigramReal))\nbfreqFake = (nltk.FreqDist(bigramFake))","88e07ad2":"plt.title('Top 20 Bigrams in Real News')\nbfreqReal.plot(20, cumulative=False, color = 'b');\n\nplt.title('Top 20 Bigrams in Fake News')\nbfreqFake.plot(20, cumulative=False, color = 'r');","3bf7395b":"trigramReal = ngramFunct(mergedReal, 3)\ntrigramFake = ngramFunct(mergedFake, 3)\ntfreqReal = (nltk.FreqDist(trigramReal))\ntfreqFake = (nltk.FreqDist(trigramFake))","c133a43b":"plt.title('Top 20 Trigrams in Real News')\ntfreqReal.plot(20, cumulative=False, color = 'b');\n\nplt.title('Top 20 Trigrams in Fake News')\ntfreqFake.plot(20, cumulative=False, color = 'r');","4ce02785":"from nltk.stem.snowball import SnowballStemmer\n\ndef stem(data):    \n    stemmer = SnowballStemmer('english')\n    stemmed = []\n    for i in range(len(data)):\n        temp = data[i]\n        temp = [stemmer.stem(word) for word in temp]\n        temp = ''.join(temp)\n        stemmed.append(temp)\n    return(stemmed)","1b8dd441":"titleCorpus = stem(train.noStopWordTitle)\ntitleCorpus[:10]","ab660676":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\nX = tf.fit_transform(titleCorpus).toarray()\ny = train['class']","ff12305f":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.33)","e79a72c3":"from sklearn.naive_bayes import MultinomialNB\nclassifierMNB = MultinomialNB()\nclassifierMNB.fit(X_train, y_train) \npred = classifierMNB.predict(X_val)\nscore = metrics.accuracy_score(y_val, pred)\nprint('Accuracy : %0.3f' %score)\n\ncm = plot_confusion_matrix(classifierMNB, X_val, y_val, cmap = 'coolwarm')","2ba02445":"from sklearn.linear_model import PassiveAggressiveClassifier\nclassifierPAC = PassiveAggressiveClassifier(n_iter_no_change=50)\nclassifierPAC.fit(X_train, y_train)\npred = classifierPAC.predict(X_val)\nscore = metrics.accuracy_score(y_val, pred)\nprint('Accuracy : %0.3f'%score)\ncm = plot_confusion_matrix(classifierPAC, X_val, y_val, cmap = 'coolwarm')","7d019674":"textCorpus = stem(train.noStopWordText)\ntextCorpus[:10]","c9f1300d":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\nX = tf.fit_transform(textCorpus).toarray()\ny = train['class']","d9e98fda":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.33)","ef34a77b":"from sklearn.naive_bayes import MultinomialNB\nclassifierMNB = MultinomialNB()\nclassifierMNB.fit(X_train, y_train) \npred = classifierMNB.predict(X_val)\nscore = metrics.accuracy_score(y_val, pred)\nprint('Accuracy : %0.3f' %score)\n\ncm = plot_confusion_matrix(classifierMNB, X_val, y_val, cmap = 'coolwarm')","5060964a":"from sklearn.linear_model import PassiveAggressiveClassifier\nclassifierPAC = PassiveAggressiveClassifier(n_iter_no_change=50)\nclassifierPAC.fit(X_train, y_train)\npred = classifierPAC.predict(X_val)\nscore = metrics.accuracy_score(y_val, pred)\nprint('Accuracy : %0.3f'%score)\ncm = plot_confusion_matrix(classifierPAC, X_val, y_val, cmap = 'coolwarm')","06cc9069":"train['Title and Text'] = train[['noStopWordTitle', 'noStopWordText']].apply(' '.join, axis=1)\ntrain.head()","394cdc5c":"titleTextCorpus = stem(train['Title and Text'])\ntitleTextCorpus[:5]","c4f77ac8":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\nX = tf.fit_transform(titleTextCorpus).toarray()\ny = train['class']","bac81c33":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.33)","8e5d3a02":"from sklearn.naive_bayes import MultinomialNB\nclassifierMNB = MultinomialNB()\nclassifierMNB.fit(X_train, y_train) \npred = classifierMNB.predict(X_val)\nscore = metrics.accuracy_score(y_val, pred)\nprint('Accuracy : %0.3f' %score)\n\ncm = plot_confusion_matrix(classifierMNB, X_val, y_val, cmap = 'coolwarm')","662e3aea":"from sklearn.linear_model import PassiveAggressiveClassifier\nclassifierPAC = PassiveAggressiveClassifier(n_iter_no_change=50)\nclassifierPAC.fit(X_train, y_train)\npred = classifierPAC.predict(X_val)\nscore = metrics.accuracy_score(y_val, pred)\nprint('Accuracy : %0.3f'%score)\ncm = plot_confusion_matrix(classifierPAC, X_val, y_val, cmap = 'coolwarm')","17c9ba75":"As can be seen, out of 34,965 texts, 34,653 are unique. The remaining 312 (34965-34653) are non-unique. Since it's a small number, I let it as it is.","6cd8d91e":"# Dataset\nThe [dataset used here](https:\/\/www.kaggle.com\/pnkjgpt\/fake-news-dataset) consists of a train and a test file. The test file can be ignored as it doesn't contain the labels (as I'm doing this project as a part of a competition). We will work only on the train data.\n\nThe train dataset contains 7 columns - **'index', 'title', 'text', 'subject', 'date', 'class', 'Unnamed: 6'**","78130f2e":"### Passive Aggressive Classifier","e17ae733":"### Multinomial Naive Bayes","48b38eef":"# Pie chart showing the type of articles","55dd2af8":"The Internet has not only made information accessible to the masses but also has become a hotspot of misinformation and fake news. Fake news can lead to more harm if not correctly identified and tagged. The severity of of the effects of misinformation can be judged from the fact that there have been riots and killings attributed to fake news.\n\nFake news can even sway people's opinions and affiliations - a fact that political parties have used (and still use) to make people vote in their favour.\n\nAs such, it has become necessary to segregate the real from the fake news. But this is not feasible manually thanks to the huge amount of information that is churned out every minute on the internet.\n\nTo overcome this problem, machine learnng and natural languaging processing can be to automatically classify the fake from the real news.","22d73535":"## Combining the Title and Text and applying Models","dc6c568c":"### TF-IDF","a017be60":"### Passive Aggressive Classifier","44eafc15":"### Bigrams","8dbb6118":"The dataset is fairly balanced with the number of Real and Fake classes almost equal. \n\nTheir seems to be another class with the name **'February 5, 2017'** consisting of only one data point. On further inspection, I find that the features for this data point has been shifted one column ahead for all the features. Since it is just one point, it can be removed or the features shifted in the reverse direction. \n\nI chose to shift the columns in the right places.","63204434":"## Applying models to the Titles","e502e6f5":"**Checking for null values**","24edca4e":"I first tried to use the models on the titles only and then the text only and then merged the titles and text.","e12588a3":"**Checking whether the dataset is balanced or imbalanced**","a204da11":"### Trigrams","605e53c7":"## Applying models to the Text","85c80be7":"From the above table, it can be seen that out of 40,000 titles and texts, 35,075 and 34,965 are unique, respectively.\n\nSo the remaining non-unique titles and texts must be removed. I removed the **text**, as it has more non-unique values comapred to **title**.","4a1c85fd":"# Model Building","cc8c3d88":"### Unigrams","3c189508":"I removed the **title** and **text** and inserted the **noStopWordTitle** and **noStopWordText** in the dataframe","84fbc8e4":"### Multinomial Naive Bayes","99511898":"The **index** and **Unnamed: 6** columns can now be removed as these are redundant and don't convey any information.","51a8397f":"## TF-IDF","47af98ce":"### Passive Aggressive Classifier","645afc82":"### Stemming the words","48701c73":"## TD-IDF","7a66a4a9":"### Multinomial Naive Bayes","170dfa14":"As it can be seen, the classification accuracy when the models are applied to the **titles** only is the least followed by the **text**.\n\nThe accuracy is highest when the **title** and **text** both are combined together. \n\nCompared with Naive Bayes, Passive Aggressive Classifier gives the best accuracy.","4e752704":"Separating the Fake and Real titles","0fc43995":"# Data description","2c293ef5":"**Do upvote if this notebook helped you in learning something new.**\n\n**Suggestions and discussions are welcome**","a1c92bc7":"# Overview\nIn this project, I've tried to classify the given news as fake or real, by using Naive Bayes classifier and Passive Aggressive Classifier. I tried using Naive Bayes with hyperparameter tuning. \n\nThe best result was given by Passive Aggressive Classifier, with an accuracy of over 99.5%","ab8b9b73":"# Importing necessary Libraries","2d62f694":"# Removing stopwords","69ac897c":"# Top unigrams, bigrams and trigrams used in the title"}}