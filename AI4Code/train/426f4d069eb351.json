{"cell_type":{"939a6261":"code","6ba22e24":"code","4b1518c8":"code","6236ec9f":"code","d5354af7":"code","38826b72":"code","7caef654":"code","788d349a":"code","04e02946":"code","b7b9244e":"code","a31fb870":"code","9652d5ec":"code","368ac2a6":"code","4888a438":"code","ef6f337e":"code","3504150a":"code","19538d66":"code","30c9e4ed":"code","b761762c":"code","0622c1d1":"code","3794c744":"code","f09ff07b":"code","c9c39238":"code","16b933fa":"code","8c9fe29f":"code","3dafeb4b":"code","7a5ac882":"markdown","5b5f0ff7":"markdown","d9164076":"markdown","b2153d83":"markdown","1684712e":"markdown","d95485e8":"markdown","ce9da527":"markdown","6f770531":"markdown","1d650093":"markdown","0eb8f611":"markdown","bfa81fcf":"markdown","e9d64fce":"markdown","b58c03f5":"markdown"},"source":{"939a6261":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf","6ba22e24":"plt.pie([len(df[df[\"Sex\"]=='male']),len(df[df[\"Sex\"]=='female'])], \n        labels=[\"Male({:.2f}%)\".format((len(df[df[\"Sex\"]=='male'])\/len(df[\"Sex\"]))*100),\n                \"Female({:.2f}%)\".format((len(df[df[\"Sex\"]=='female'])\/len(df[\"Sex\"]))*100)], explode=[0.1,0],\n        startangle=90,colors=[\"royalblue\",\"skyblue\"])\nplt.title(\"Number of Male Females aboard the titanic\")\nplt.show()","4b1518c8":"plt.pie([len(df[df[\"Pclass\"]==1]),len(df[df[\"Pclass\"]==2]),len(df[df[\"Pclass\"]==3])], \n        labels=[\"Class 1 ({:.2f}%)\".format((len(df[df[\"Pclass\"]==1])\/len(df[\"Pclass\"]))*100),\n                \"Class 2 ({:.2f}%)\".format((len(df[df[\"Pclass\"]==2])\/len(df[\"Pclass\"]))*100),\n                \"Class 3 ({:.2f}%)\".format((len(df[df[\"Pclass\"]==3])\/len(df[\"Pclass\"]))*100)], explode=[0.1,0.1,0],\n        startangle=90,colors=[\"royalblue\",\"skyblue\", \"steelblue\"])\nplt.title(\"Number of Passengers of each class aboard the titanic\")\nplt.show()","6236ec9f":"df[\"Embarked\"]=df[\"Embarked\"].fillna(value=\"S\")\n\nplt.pie([len(df[df[\"Embarked\"]=='S']),len(df[df[\"Embarked\"]=='C']),len(df[df[\"Embarked\"]=='Q'])], \n        labels=[\"Southampton ({:.2f}%)\".format((len(df[df[\"Embarked\"]=='S'])\/len(df[\"Embarked\"]))*100),\n                \"Cherbourg ({:.2f}%)\".format((len(df[df[\"Embarked\"]=='C'])\/len(df[\"Embarked\"]))*100),\n                \"Queenstown ({:.2f}%)\".format((len(df[df[\"Embarked\"]=='Q'])\/len(df[\"Embarked\"]))*100)], explode=[0.1,0.05,0],\n        startangle=90,colors=[\"royalblue\",\"skyblue\", \"steelblue\"])\nplt.title(\"Port of Embarkation\")\nplt.show()","d5354af7":"df[\"Age\"]=df[\"Age\"].fillna(value=np.nanmean(df[\"Age\"]))\n\nplt.hist(df[\"Age\"])\nplt.title(\"Age of passengers aboard the titanic\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Number of paasengers\")\nplt.show()","38826b72":"plt.pie([len(df[df[\"Survived\"]==1]),len(df[df[\"Survived\"]==0])],\n        labels=[\"Survived ({:.2f}%)\".format((len(df[df[\"Survived\"]==1])\/len(df[\"Survived\"]))*100),\n                \"Not Survived ({:.2f}%)\".format((len(df[df[\"Survived\"]==0])\/len(df[\"Survived\"]))*100) ], explode=[0.1,0],\n        startangle=90,colors=[\"royalblue\",\"skyblue\"])\nplt.title(\"Number of survivors in the titanic\")\nplt.show()","7caef654":"dt=df[df[\"Survived\"]==1]\nplt.pie([len(dt[dt[\"Sex\"]=='male']),len(dt[dt[\"Sex\"]=='female'])], \n        labels=[\"Male ({:.2f}%)\".format((len(dt[dt[\"Sex\"]=='male'])\/len(dt))*100), \n                \"Female ({:.2f}%)\".format((len(dt[dt[\"Sex\"]=='female'])\/len(dt))*100)], explode=[0.1,0],\n        startangle=90,colors=[\"royalblue\",\"skyblue\"])\nplt.title(\"Number of Male Females survived the titanic\")\nplt.show()","788d349a":"feature_names=[\"Pclass\", \"Age\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\nX=df[feature_names]\nX","04e02946":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder=LabelEncoder()\nX[\"Sex\"]=label_encoder.fit_transform(X[\"Sex\"])\nX[\"Embarked\"]=label_encoder.fit_transform(X[\"Embarked\"])\nX","b7b9244e":"y=df[\"Survived\"]\ny","a31fb870":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test=train_test_split(X,y,random_state=0)","9652d5ec":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogreg=LogisticRegression().fit(X_train, y_train)\ny_pred_logreg=logreg.predict(X_test)\n\naccuracy_score(y_test,y_pred_logreg)","368ac2a6":"from sklearn.ensemble import RandomForestClassifier\n\nrfc=RandomForestClassifier(n_estimators=10, max_depth=5).fit(X_train, y_train)\ny_pred_rfc=rfc.predict(X_test)\n\naccuracy_score(y_test,y_pred_rfc)","4888a438":"from sklearn.naive_bayes import GaussianNB\n\ngnb=GaussianNB().fit(X_train, y_train)\ny_pred_gnb=gnb.predict(X_test)\n\naccuracy_score(y_test,y_pred_gnb)","ef6f337e":"from sklearn.tree import DecisionTreeClassifier\n\ndtc=DecisionTreeClassifier(max_depth=5).fit(X_train, y_train)\ny_pred_dtc=dtc.predict(X_test)\n\naccuracy_score(y_test,y_pred_dtc)","3504150a":"from sklearn.svm import SVC\n\nsvc=SVC(kernel='linear').fit(X_train, y_train)\ny_pred_svc=svc.predict(X_test)\n\naccuracy_score(y_test,y_pred_svc)","19538d66":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc=GradientBoostingClassifier(n_estimators=10, max_depth=5, learning_rate=0.3).fit(X_train, y_train)\ny_pred_gbc=gbc.predict(X_test)\n\naccuracy_score(y_test,y_pred_gbc)","30c9e4ed":"grad_boost=GradientBoostingClassifier(n_estimators=10, max_depth=5, learning_rate=0.3).fit(X, y)","b761762c":"df_test=pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndf_test","0622c1d1":"df_test[\"Embarked\"]=df_test[\"Embarked\"].fillna(value=\"S\")\ndf_test[\"Age\"]=df_test[\"Age\"].fillna(value=np.nanmean(df_test[\"Age\"]))\nfeature_names=[\"Pclass\", \"Age\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\nX_testing=df_test[feature_names]\nX_testing","3794c744":"X_testing[\"Sex\"]=label_encoder.fit_transform(X_testing[\"Sex\"])\nX_testing[\"Embarked\"]=label_encoder.fit_transform(X_testing[\"Embarked\"])\nX_testing","f09ff07b":"y_predict=grad_boost.predict(X_testing)\ny_predict","c9c39238":"y_predict.shape","16b933fa":"df_test[\"Survived\"]=y_predict\ndf_test","8c9fe29f":"col=[\"PassengerId\", \"Survived\"]\nkaggle_inp=df_test[col]\nkaggle_inp=kaggle_inp.set_index(\"PassengerId\")\nkaggle_inp","3dafeb4b":"kaggle_inp.to_csv(\"Titani_Survival_Prediction.csv\")","7a5ac882":"### Random Forest Classifier","5b5f0ff7":"# Analysis of passengers in Titanic and preprocessing\n\nWe apply basic preprocessing to our dataset and analyse the various features in the dataset e.g. Passengers' Sex, Age, Survival rate, etc. We replace null values with mean or mode depending on the feature type and represent the features using the pie charts","d9164076":"### Decision Tree Classifier","b2153d83":"As we can see that the <b>Gradient Boosting Classifier<\/b> has the most accuracy among all the classifiers with an verall accuracy score of around 84%. So we will use the Gradient Boosting Classifier to predict the survival of passengers in Titanic.","1684712e":"Applying Gradient Boosting Classifier on whole train.csv dataset","d95485e8":"# Apllying Gradient Boosting Classifier on Test.csv","ce9da527":"# Splitting the data\n\nWe now split the data and apply label encoding to the sex and embarked column.","6f770531":"# Titanic Survival Prediction Model\n\n<b>RMS Titanic<\/b> was a British passenger liner, operated by the White Star Line, which sank in the North Atlantic Ocean on 15 April 1912 after striking an iceberg during her maiden voyage from Southampton, UK, to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, which made the sinking possibly one of the deadliest for a single ship up to that time. It remains to this day the deadliest peacetime sinking of a superliner or cruise ship. (Wikipedia)\n\nThis is the submission for Titanic Machine Learning Task on Kaggle. This notebook analyses the titanic dataset of passengers and looks at the accuracy score of different models for classificaton of survival of passengers selecting the best model for predicting the survival.","1d650093":"# Machine Learning Models and their Accuracy\n\nWe will use many machine learning models and select the best out of them on the basis of accuracy score. The models would be:\n- Logistic Regression\n- Random Forest Classifier\n- Naive Bayes Classifier\n- Decision Tree Classifier\n- Support Vector Machines\n- Gradient Boosted Clssifier","0eb8f611":"### Naive Bayes Classifier","bfa81fcf":"### Gradient Boosted Classifier","e9d64fce":"### Support  Vector Machines ( SVM )","b58c03f5":"### Logistic Regression"}}