{"cell_type":{"f625b553":"code","06d58fb7":"code","e794bb97":"code","ed3db1d8":"code","2ab7e204":"code","51d5fc1e":"code","b57f76d6":"code","90a7ba57":"code","e0d6ffca":"code","490a9f38":"code","ab710c6f":"code","22e6f6ad":"code","0e44a322":"code","82ad8454":"code","13efe195":"code","dbf89445":"code","d513651e":"code","606ba7fa":"code","c03ffcff":"code","5bf55aea":"code","95f58024":"code","4466be9a":"code","eba6409a":"code","99ed330d":"code","4bb1b901":"code","b294ede2":"code","05b7b836":"code","b67c1a23":"code","2eecb897":"code","f4c723e4":"code","34ea35b3":"code","07c96c85":"code","729c7e7b":"code","3434e9f3":"code","dabd437f":"code","0a8f6317":"code","0a99f76b":"code","bb900dc3":"code","6646e29c":"code","d014798c":"code","6f5bab9e":"code","e08e4bfb":"code","4917a4bf":"code","e4af0b12":"code","9618cb69":"code","07ca1a7c":"code","7cc5f067":"code","7139c7df":"code","22d82898":"code","0801b8c4":"code","03364b7f":"code","0b71b121":"code","4f0ff44e":"code","efd1b7d5":"code","976e7182":"code","3d73880c":"code","e6a68cf6":"code","727834be":"code","60bb320b":"code","930b5110":"code","3327db23":"code","e3db1911":"code","d18899da":"code","fc431708":"code","61e262f4":"code","3b24504a":"code","1b251999":"code","4f4af736":"code","8c25def7":"code","37f7d101":"code","f992d222":"code","e9ad4571":"code","6a374815":"code","4c598a14":"code","75b022a2":"markdown","6751576b":"markdown","e604dc92":"markdown","1697ad60":"markdown","d5f0bf5b":"markdown","eaec86fd":"markdown","9394f12d":"markdown","df152358":"markdown","fbd1876a":"markdown","9474492a":"markdown","c503f649":"markdown","0bee96ec":"markdown","1022dd46":"markdown","24f031b0":"markdown","37e1c6c0":"markdown","d2bbeffb":"markdown","a547714e":"markdown","806d5af1":"markdown"},"source":{"f625b553":"# Took help from here \n# https:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8","06d58fb7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm , skew\nfrom scipy import stats\nimport sklearn.preprocessing as StandardScaler\n%matplotlib inline\n\n#loading data\ntrain = pd.read_csv(r\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(r\"\/kaggle\/input\/titanic\/test.csv\")\n\n#train.head()\n","e794bb97":"train.shape\n#(891,12)","ed3db1d8":"train.describe()","2ab7e204":"train.info()","51d5fc1e":"test.info()","b57f76d6":"\"\"\"\npercentage of null values to total no.of rows\n\"\"\"\ntotal_miss = train.isnull().sum()\ntotal_miss.fillna(0)\npercent_miss = train.isnull().sum()\/train.shape[0] \ndata_miss = pd.concat([total_miss, percent_miss], axis= 1)\ndata_miss = data_miss[data_miss[0] > 0]\ndata_miss.sort_values(by = [0],inplace = True)\ndata_miss.columns = ['Count','Proportion']\ndata_miss.index.names = ['Names']\ndata_miss['Names'] = data_miss.index\ndata_miss","90a7ba57":"train[['Pclass','Survived']].groupby('Pclass' , as_index = False).mean().sort_values(by = 'Survived' , ascending = False)","e0d6ffca":"train[['Sex','Survived']].groupby('Sex' , as_index = False).mean().sort_values(by = 'Survived' , ascending = False)","490a9f38":"train[['Embarked','Survived']].groupby('Embarked' , as_index = False).mean().sort_values(by = 'Survived' , ascending = False)","ab710c6f":"train[['SibSp','Survived']].groupby(['SibSp'] , as_index = False).mean().sort_values(by = 'Survived' , ascending = False)","22e6f6ad":"train[['Parch','Survived']].groupby(['Parch'] , as_index = False).mean().sort_values(by = 'Survived' , ascending = False)","0e44a322":"#between name and survival  \ntrain['Title'] = train.Name.str.extract('([A-Za-z]+)\\.',expand = False)\ntest['Title'] = test.Name.str.extract('([A-Za-z]+)\\.',expand = False)\n\npd.crosstab(train['Title'], train['Sex'])","82ad8454":"#analysing by visualizing data(age and survival)\ng = sns.FacetGrid(train , col = 'Survived')\ng.map(plt.hist , 'Age' , bins = 20)","13efe195":"#analysing by visualizing data(age and survival)\nx1 = train.loc[train.Survived == 0, 'Age']\nx2 = train.loc[train.Survived == 1, 'Age']\n\nkwargs = dict(alpha = 0.2, bins=20)\n\nplt.hist(x1.dropna(), **kwargs, color='g', label='Dead')\nplt.hist(x2.dropna(), **kwargs, color='r', label='Survived')\nplt.gca().set(title='Survival based on Age', ylabel='Total')\nplt.xlim(0,100)\nplt.legend();","dbf89445":"#analysing by visualizing data(age and survival)\ng = sns.FacetGrid(train , col = 'Survived', row = 'Pclass')\ng.map(plt.hist , 'Age' , bins = 20)\n#max pop in age bet 20 to 40 ","d513651e":"\n#analysing by visualizing data(age , embarked , sex , pclass and survival)\ngrid = sns.FacetGrid(train, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","606ba7fa":"#  above result may be because male population may be much larger on \"C\"\n# lets check\nprint(len(train[train.Embarked == 'S']['Sex']), \" male pop = \" ,len(train[train.Embarked == 'S']['Sex'][train[train.Embarked == 'S']['Sex'] == 'male'])) \nprint(len(train[train.Embarked == 'C']['Sex']), \" male pop = \" ,len(train[train.Embarked == 'C']['Sex'][train[train.Embarked == 'C']['Sex'] == 'male'])) #so that's not the case\nprint(len(train[train.Embarked == 'Q']['Sex']), \" male pop = \" ,len(train[train.Embarked == 'Q']['Sex'][train[train.Embarked == 'Q']['Sex'] == 'male'])) \n","c03ffcff":"#analysing by visualizing data( embarked , sex , fare and survival)\n# grid = sns.FacetGrid(train, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(train, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()\n#female passenger paid higher fares somehow...nothing useful","5bf55aea":"#combining datasets.\nall_data = train.append(test)","95f58024":"all_data.shape","4466be9a":"#making a new variable having total family members","eba6409a":"all_data['total_relatives'] = all_data['SibSp'] + all_data['Parch'] \n","99ed330d":"all_data.loc[all_data['total_relatives']==0 ,'alone'] = 1","4bb1b901":"all_data.loc[all_data['total_relatives']>0 ,'alone'] = 0","b294ede2":"all_data[['Survived','alone']].groupby(['alone']).count().sort_values(by = 'Survived')","05b7b836":"#plotting this new variable\ng = sns.FacetGrid(all_data, col = 'Survived')\ng.map(plt.hist, 'total_relatives', bins = 10)\ng.add_legend()","b67c1a23":"# removing passengerId\nall_data.drop('PassengerId', inplace = True, axis = 1)","2eecb897":"#cabin data\ndictionary_deck = {'A':1, 'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'U':0}\n#dictionary_deck = {'A':1, 'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7}\nall_data['Cabin'].fillna('U', inplace = True)\nall_data['Deck'] = all_data['Cabin'].str.extract('([a-zA-Z])', expand = True)\nall_data['Deck'] = all_data['Deck'].map(dictionary_deck)","f4c723e4":"#dropping cabin column\nall_data.drop('Cabin', inplace = True, axis = 1)","34ea35b3":"all_data.columns","07c96c85":"# finding age with Sex and Pclass\nage_ = sns.FacetGrid(all_data, col = 'Sex', row = 'Pclass')\nage_.map(plt.hist, 'Age')\nage_.add_legend()","729c7e7b":"for i in ['male', 'female']:    #Sex\n    for j in [1, 2, 3]:   #Pclass \n        mean = all_data[(all_data['Sex'] == i) & (all_data['Pclass'] == j)]['Age'].mean()\n        st_d = all_data[(all_data['Sex'] == i) & (all_data['Pclass'] == j)]['Age'].std()\n        is_null = all_data[(all_data['Sex'] == i) & (all_data['Pclass'] == j)]['Age'].isnull().sum()\n        rand_age = np.random.randint(mean - st_d, mean + st_d, is_null)\n        age_slice = all_data[(all_data['Sex'] == i) & (all_data['Pclass'] == j)]['Age'].copy()\n        age_slice[np.isnan(age_slice)] = rand_age\n        all_data.loc[((all_data['Sex'] == i) & (all_data['Pclass'] == j)), 'Age'] = age_slice\n        #print(all_data[(all_data['Sex'] == i) & (all_data['Pclass'] == j)]['Age'])","3434e9f3":"all_data['Age'].isnull().sum()","dabd437f":"#all_data['last_name'] = all_data['Name'].str.extract('([A-Za-z]+)\\,' , expand = True)","0a8f6317":"#all_data[all_data['last_name']=='Sage']\n# this shows all Sage are indeed related , so if we can find one deck , we will know all others as well","0a99f76b":"#all_data['Ticket_Deck'] = all_data['Ticket'].str.extract('([a-zA-Z])', expand = True)\n#dictionary_deck = {'A':1, 'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7}\n#all_data['Cabin'].fillna('U', inplace = True)\n#all_data['Deck'] = all_data['Cabin'].str.extract('([a-zA-Z])', expand = True)\n#all_data['Ticket_Deck'] = all_data['Ticket_Deck'].map(dictionary_deck)","bb900dc3":"#all_data.drop(['Ticket_Deck'], axis = 1, inplace = True)","6646e29c":"#all_data['last_name'].value_counts().head()","d014798c":"# making age bands\nall_data['AgeBand'] = pd.cut(all_data['Age'], 5).cat.codes","6f5bab9e":"all_data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index = False).mean()","e08e4bfb":"all_data.drop(['Age'], axis = 1, inplace = True)","4917a4bf":"#drop parch , sibsb , total_relatives\nall_data.drop(['Parch', 'SibSp', 'total_relatives'], axis = 1, inplace = True)","e4af0b12":"#drop Ticket , Name\nall_data.drop(['Name', 'Ticket'], axis = 1, inplace = True)","9618cb69":"#extracting titles from Name\n#all_data['Title'] = all_data.Name.str.extract('([A-Za-z]+)\\.',expand = False)\n","07ca1a7c":"#making bands of Fare\nall_data['Fare'].fillna(all_data['Fare'].median() , inplace = True)\nall_data['FareBand'] = pd.qcut(all_data['Fare'] , 5).cat.codes","7cc5f067":"#dropping Fare\nall_data.drop(['Fare'], axis = 1, inplace = True)","7139c7df":"#filling null values\nall_data['Embarked'].fillna(all_data['Embarked'].mode()[0], inplace = True)\nall_data['Deck'].fillna(all_data['Deck'].mode()[0], inplace = True)","22d82898":"# Title ","0801b8c4":"\nall_data['Title'] = all_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\nall_data['Title'] = all_data['Title'].replace('Mlle', 'Miss')\nall_data['Title'] = all_data['Title'].replace('Ms', 'Miss')\nall_data['Title'] = all_data['Title'].replace('Mme', 'Mrs')","03364b7f":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nall_data['Title'] = all_data['Title'].map(title_mapping)","0b71b121":"all_data.head()","4f0ff44e":"#labeling the data\nfrom sklearn.preprocessing import LabelEncoder \nle = LabelEncoder() \n  \nall_data['Embarked']= le.fit_transform(all_data['Embarked'])\nall_data['Sex']= le.fit_transform(all_data['Sex'])\nall_data['alone']= le.fit_transform(all_data['alone']) \nall_data['Deck']= le.fit_transform(all_data['Deck']) \n\n#Ordinal data columns are Pclass, AgeBand, FareBand\n#Nominal data col are Embarked, Sex, Title, Alone, Deck\n\n#from sklearn.preprocessing import OneHotEncoder\n#encoder = OneHotEncoder(categorical_features = [0,2,4,5,6] , drop = [0])\n#all_data = encoder.fit_transform(all_data)\n#all_data\n\n\ndef encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]].astype(str))\n    dummies.drop(dummies.columns[0], axis = 1, inplace = True)\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    res = res.drop([feature_to_encode], axis=1)\n    return(res)\n\nall_data = encode_and_bind(all_data, 'Deck')\nall_data = encode_and_bind(all_data, 'Embarked')","efd1b7d5":"all_data.head()","976e7182":"X_train = all_data[~(all_data.Survived.isnull())].drop('Survived', axis = 1) \nY_train = all_data[~(all_data.Survived.isnull())]['Survived']\nX_test = all_data[all_data.Survived.isnull()].drop('Survived', axis = 1) ","3d73880c":"X_train.shape","e6a68cf6":"Y_train.shape","727834be":"from sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","60bb320b":"sgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)","930b5110":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)","3327db23":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)","e3db1911":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, Y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n","d18899da":"gaussian = GaussianNB() \ngaussian.fit(X_train, Y_train) \nY_pred = gaussian.predict(X_test)  \nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)","fc431708":"perceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)","61e262f4":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)","3b24504a":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train) \nY_pred = decision_tree.predict(X_test) \nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)","1b251999":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","4f4af736":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100, oob_score = True)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","8c25def7":"## Feature Importance","37f7d101":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)","f992d222":"print(\"oob score:\", round(rf.oob_score, 4)*100, \"%\")","e9ad4571":"\"\"\"\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\nclf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\nclf.fit(X_train, Y_train)\nclf.bestparams\n\"\"\"","6a374815":"# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 10,   \n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","4c598a14":"#submissions\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Y_prediction\n    })\n    \nsubmission.to_csv(r'submission.csv', index=False) ","75b022a2":"## K fold CV","6751576b":"### Perceptron","e604dc92":"### KNN","1697ad60":"## Data Exploration","d5f0bf5b":"### Gaussian Naive Bayes:","eaec86fd":"#### will find about this relation afterwords!!!","9394f12d":"### Random Forest","df152358":"### can you fill deck col with 'total_relatives', 'alone' and 'last_name' and 'Ticket'\n### is ticket related with deck???\n### also what about fares","fbd1876a":"### Predictions","9474492a":"## TRAINING THE MODEL","c503f649":"### Stochastic Gradient Descent","0bee96ec":"### LR","1022dd46":"# feature engineering\n\n","24f031b0":"### Hyperparameter Tuning","37e1c6c0":"### Finding Correlation","d2bbeffb":"## Best Model","a547714e":"### SVC","806d5af1":"### Decision_Tree"}}