{"cell_type":{"278391a7":"code","1fa4c8b2":"code","e21c21c8":"code","5e1c2fb7":"code","d6c53d77":"code","3f79a1fe":"code","8399751a":"code","fb908de0":"code","22ef7a91":"code","756700ac":"code","0fc90304":"code","eea79af9":"code","389023f9":"code","2d6c8b4b":"code","665d61e8":"code","8eac2c52":"code","91480b93":"code","33300677":"code","b9ab6fd1":"code","26054fcf":"code","90d85411":"code","8c7f10a5":"code","a3c14b7d":"code","bca97740":"code","b2db51e8":"code","5ca9eb5e":"code","6b0b1f03":"code","190e68e3":"code","5a877b49":"code","2988ae26":"code","21b64e3c":"code","34fe6c64":"code","c42b40da":"code","7d6133d0":"code","43f5125b":"code","a6eb304f":"code","ab8195f2":"code","f1f15864":"code","39237fe2":"code","04daa86d":"code","9f303bb9":"code","34c010a5":"code","e0553be5":"code","90b924a3":"code","150f0297":"code","4e1a2bdd":"code","45c70830":"code","8aedc1e8":"code","50f16d02":"code","01ced86e":"code","f76108b8":"code","d889b58b":"code","4087ad64":"code","5803746c":"code","55e11900":"code","6f9f422d":"code","fcd8162c":"code","39062135":"code","ded98d38":"code","232d8b5b":"code","5dd91cbb":"code","0f529b56":"markdown","aa8f585a":"markdown","0462e11f":"markdown","e136b3d9":"markdown","78a18cbc":"markdown","f603bea3":"markdown","b1729dd8":"markdown","3dee0aec":"markdown","dda5cee4":"markdown","999768dc":"markdown","d88d3188":"markdown","0bf7fca0":"markdown","f1c26422":"markdown","cdd5d083":"markdown","bf947960":"markdown","ac3bc424":"markdown","4193aa79":"markdown","44004bce":"markdown","79ef197c":"markdown","aa8c37dd":"markdown","1c2afa6c":"markdown","c4b4dbb7":"markdown","8f3d78d2":"markdown","09eab77b":"markdown","b6468cf5":"markdown","7820f888":"markdown","fca96ec7":"markdown","3bf8e408":"markdown","585dbfc7":"markdown","3019163c":"markdown"},"source":{"278391a7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nsns.set()","1fa4c8b2":"# Load the data from a .csv in the same folder\n\nraw_data = pd.read_csv(\"\/kaggle\/input\/1.04. Real-life example.csv\")\n# Exploring the first 5 row of the data\nraw_data.head()","e21c21c8":"# Descriptive statistics are very useful for initial exploration of the variables\n# By default, only descriptives for the numerical variables are shown\n# To include the categorical ones, you should specify this with an argument(include = all)\n\nraw_data.describe(include='all')\n","5e1c2fb7":"# I removed the variable 'model' because it has too many unique values\ndata = raw_data.drop(['Model'],axis=1)\n\n# Description of the dataframe after 'model' is removed\ndata.describe(include='all')","d6c53d77":"# data.isnull() # shows a df with the information whether a data point is null \n# Since True = the data point is missing, while False = the data point is not missing, we can sum them\n# This will give us the total number of missing values feature-wise\n\n## we check for missing values to improve the accuracy of our model\ndata.isnull().sum()","3f79a1fe":"# Let's simply drop all missing values\n# This is not always recommended, however, when we remove less than 5% of the data, it is okay\n\ndata_no_mv = data.dropna(axis=0)","8399751a":"# Let's check the descriptives without the missing values\n\ndata_no_mv.describe(include='all')","fb908de0":"\nsns.distplot(data_no_mv['Price'])","22ef7a91":"# Obviously there are some outliers present \n\n# Without diving too deep into the topic, we can deal with the problem easily by removing 0.5%, or 1% of the problematic samples\n# Here, the outliers are situated around the higher prices (right side of the graph)\n# Logic should also be applied\n# This is a dataset about used cars, therefore one can imagine how $300,000 is an excessive price\n\n# Outliers are a great issue for OLS, thus we must deal with them in some way\n","756700ac":"# Declaring a variable that will be equal to the 99th percentile of the 'Price' variable\nq = data_no_mv['Price'].quantile(0.99)\n\n# Then we can create a new df, with the condition that all prices must be below the 99 percentile of 'Price'\ndata_1 = data_no_mv[data_no_mv['Price']<q]\n\n# In this way we have essentially removed the top 1% of the data about 'Price'\ndata_1.describe(include='all')","0fc90304":"# Check the PDF once again, here we can see that the outliers have reduced drastically\n\nsns.distplot(data_1['Price'])\n","eea79af9":"sns.distplot(data_no_mv['Mileage'])","389023f9":"# We can solve other variables in a similar way\n\nq = data_1['Mileage'].quantile(0.99)\ndata_2 = data_1[data_1['Mileage']<q]\n","2d6c8b4b":"sns.distplot(data_2['Mileage'])","665d61e8":"sns.distplot(data_no_mv['EngineV'])","8eac2c52":"# From the above we can see that| engine volume is very strange\n# In such cases it makes sense to manually check what may be causing the problem\n# In our case the issue comes from the fact that most missing values are indicated with 99.99 or 99\n# There are also some incorrect entries like 75","91480b93":"# A simple Google search can indicate the natural domain of this variable\n# Car engine volumes are usually (always?) below 6.5l\n# This is a prime example of the fact that a domain expert (a person working in the car industry)\n# may find it much easier to determine problems with the data than an outsider\n\ndata_3 = data_2[data_2['EngineV']<6.5]","33300677":"sns.distplot(data_3['EngineV'])\n\n# After plotting the graph to see PDF we can see great improvment and outliers has decreased significantly","b9ab6fd1":"# Finally, the situation with 'Year' is similar to 'Price' and 'Mileage'\n# However, the outliers are on the low end\n\nsns.distplot(data_no_mv['Year'])","26054fcf":"# Declaring a variable that will be equal to the 1st percentile of the 'Year' variable\nq = data_3['Year'].quantile(0.01)\n\n# Then we can create a new df, with the condition that all year must be below the 1 percentile of 'Year'\ndata_4 = data_3[data_3['Year']>q]","90d85411":"# Check out the result now\n\nsns.distplot(data_4['Year'])","8c7f10a5":"# When we remove observations, the original indexes are preserved\n# If we remove observations with indexes 2 and 3, the indexes will go as: 0,1,4,5,6\n# That's very problematic as we tend to forget about it (later you will see an example of such a problem)\n\n# Finally, once we reset the index, a new column will be created containing the old index (just in case)\n# We won't be needing it, thus 'drop=True' to completely forget about it","a3c14b7d":"data_cleaned = data_4.reset_index(drop=True)","bca97740":"# Lets have a look at our clean dataset\n\ndata_cleaned.describe(include='all')","b2db51e8":"# Lets do some matplotlib code and plot variables against each other on a scatter plot\n# since Price is the 'y' axis of all the plots, it made sense to plot them side-by-side (so we can compare them)\n\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3))\nax1.scatter(data_cleaned['Year'],data_cleaned['Price'])\nax1.set_title('Price and Year')\nax2.scatter(data_cleaned['EngineV'],data_cleaned['Price'])\nax2.set_title('Price and EngineV')\nax3.scatter(data_cleaned['Mileage'],data_cleaned['Price'])\nax3.set_title('Price and Mileage')\n\nplt.show()","5ca9eb5e":"## Let's transform 'Price' with a log transformation\ndata_log = np.log(data_cleaned['Price'])\n\n# Add the new price to our data frame\ndata_cleaned['log_price'] = data_log\ndata_cleaned\n","6b0b1f03":"# Lets check again\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3))\nax1.scatter(data_cleaned['Year'],data_cleaned['log_price'])\nax1.set_title('log_price and Year')\nax2.scatter(data_cleaned['EngineV'],data_cleaned['log_price'])\nax2.set_title('log_price and EngineV')\nax3.scatter(data_cleaned['Mileage'],data_cleaned['log_price'])\nax3.set_title('log_price and Mileage')\n\nplt.show()\n\n# # The relationships show a clear linear relationship\n# This is some good linear regression material\n\n# Alternatively we could have transformed each of the independent variables","190e68e3":"# Since we will be using the log price variable, we can drop the old 'Price' one\ndata_cleaned = data_cleaned.drop(['Price'],axis=1)","5a877b49":"#The columns of our data frame\ndata_cleaned.columns.values","2988ae26":"# sklearn does not have a built-in way to check for multicollinearity\n\n# Here's the relevant module\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# we declare a variable where we put all features where we want to check for multicollinearity\n# since our categorical data is not yet preprocessed, we will only take the numerical ones\nvariables = data_cleaned[['Mileage','Year','EngineV']]\n\n# we create a new data frame which will include all the VIFs\n# note that each variable has its own variance inflation factor as this measure is variable specific (not model specific)\nvif = pd.DataFrame()\n\n# here we make use of the variance_inflation_factor, which will basically output the respective VIFs \nvif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif[\"Features\"] = variables.columns","21b64e3c":"# Explore the result\nvif","34fe6c64":"data_no_multicollinearity = data_cleaned.drop(['Year'],axis=1)","c42b40da":"# Lets use the same method shown above to check the VIFs to see if the reduced since 'Year' has been dropped\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvariables = data_cleaned[['Mileage','EngineV']]\nvif = pd.DataFrame()\nvif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif[\"Features\"] = variables.columns","7d6133d0":"vif\n# From our result you can see that VIFs has drastically reduced for each variable","43f5125b":"# To include the categorical data in the regression, let's create dummies\n# There is a very convenient method called: 'get_dummies' which does that seemlessly\n# It is extremely important that we drop one of the dummies, alternatively we will introduce multicollinearity\n\ndata_with_dummies = pd.get_dummies(data_no_multicollinearity, drop_first=True)","a6eb304f":"# Here's the result\n\ndata_with_dummies.head()","ab8195f2":"# To make our data frame more organized, we prefer to place the dependent variable in the beginning of the \ndata_with_dummies.columns.values","f1f15864":"# To make the code a bit more parametrized, let's declare a new variable that will contain the preferred order\n# Conventionally, the most intuitive order is: dependent variable, indepedendent numerical variab;es and dummi\n\ncols = ['log_price', 'Mileage', 'EngineV', 'Brand_BMW',\n       'Brand_Mercedes-Benz', 'Brand_Mitsubishi', 'Brand_Renault',\n       'Brand_Toyota', 'Brand_Volkswagen', 'Body_hatch', 'Body_other',\n       'Body_sedan', 'Body_vagon', 'Body_van', 'Engine Type_Gas',\n       'Engine Type_Other', 'Engine Type_Petrol', 'Registration_yes']","39237fe2":"# To implement the reordering, we will create a new df, which is equal to the old one but with the new order of features\ndata_preprocessed = data_with_dummies[cols]\ndata_preprocessed.head()","04daa86d":"# The target is the dependent variable which is the 'log_price'\ntargets = data_preprocessed['log_price']\n\n# The independent variabke is everything else but the log)orice so it wise to just drop it\ninputs = data_preprocessed.drop(['log_price'],axis=1)","9f303bb9":"# Import scaling module\nfrom sklearn.preprocessing import StandardScaler\n\n# Create scaling object\nscaler = StandardScaler()\n\n# fit the inputs\nscaler.fit(inputs)\n\n\n# we scale when we want to handle disparities in units and improve performance of your model\n","34c010a5":"# Scale the features and store them in a new variable \n\ninputs_scaled = scaler.transform(inputs)","e0553be5":"# Import the split model\nfrom sklearn.model_selection import train_test_split\n\n# Split the variables with an 80-20 split and some random state\n# To have the same split as mine, use random_state = 200\nx_train, x_test, y_train, y_test = train_test_split(inputs_scaled,targets)","90b924a3":"# Create a regression object\nreg = LinearRegression()\n# Fit the regression with the scaled TRAIN inputs and targets\nreg.fit(x_train,y_train)","150f0297":" # Let's check the outputs of the regression\n# I'll store them in y_hat as this is the 'theoretical' name of the predictions\ny_hat = reg.predict(x_train)","4e1a2bdd":"# The simplest way to compare the targets (y_train) and the predictions (y_hat) is to plot them on a scatter plot\n# The closer the points to the 45-degree line, the better the prediction\nplt.scatter(y_train, y_hat)\n# Let's also name the axes\nplt.xlabel('Targets (y_train)',size=18)\nplt.ylabel('Predictions (y_hat)',size=18)\n# Sometimes the plot will have different scales of the x-axis and the y-axis\n# We want the x-axis and the y-axis to be the same\nplt.xlim(6,13)\nplt.ylim(6,13)\nplt.show()","45c70830":"# We can plot the PDF of the residuals and check for anomalies\nsns.distplot(y_train - y_hat)\n\n# Include a title\nplt.title(\"Residuals PDF\", size=18)\n\n# In the best case scenario this plot should be normally distributed\n# In our case we notice that there are many negative residuals (far away from the mean)\n# Given the definition of the residuals (y_train - y_hat), negative values imply\n# that y_hat (predictions) are much higher than y_train (the targets)\n# This is food for thought to improve our model","8aedc1e8":"# Find the R-squared of the model\nreg.score(x_train,y_train)","50f16d02":"# Obtain the bias (intercept) of the regression\nreg.intercept_","01ced86e":"# Obtain the weights (coefficients) of the regression\nreg.coef_\n","f76108b8":"# Create a regression summary where we can compare them with one-another\nreg_summary = pd.DataFrame(inputs.columns.values, columns=['Features'])\nreg_summary['Weights'] = reg.coef_\nreg_summary","d889b58b":"# Once we have trained our model, we can test it on a dataset that the algorithm has never seen\n# Our test inputs are 'x_test', while the outputs: 'y_test'\n# If the predictions are far off, we will know that our model overfitted\ny_hat_test = reg.predict(x_test)","4087ad64":"# Create a scatter plot with the test targets and the test predictions\n# You can include the argument 'alpha' which will introduce opacity to the graph\nplt.scatter(y_test, y_hat_test, alpha=0.2)\nplt.xlabel('Targets (y_test)',size=18)\nplt.ylabel('Predictions (y_hat_test)',size=18)\nplt.xlim(6,13)\nplt.ylim(6,13)\nplt.show()","5803746c":"# lets check these predictions\n# To obtain the actual prices, we take the exponential of the log_price\ndf_pf = pd.DataFrame(np.exp(y_hat_test), columns=['Prediction'])\ndf_pf.head()","55e11900":"# Include the test targets in that data frame to compare them with the predictions\ndf_pf['Target'] = np.exp(y_test)\ndf_pf\n\n# Note that we have a lot of missing values\n# There is no reason to have ANY missing values, though\n# This suggests that something is wrong with the data frame","6f9f422d":"# After displaying y_test, we find what the issue is\n# The old indexes are preserved (recall earlier in that code we made a note on that)\n# The code was: data_cleaned = data_4.reset_index(drop=True)\n\n# Therefore, to get a proper result, we must reset the index and drop the old indexing\ny_test = y_test.reset_index(drop=True)\n\n# Check the result\ny_test.head()","fcd8162c":"# Let's overwrite the 'Target' column with the appropriate values\n# Again, we need the exponential of the test log price\ndf_pf['Target'] = np.exp(y_test)\ndf_pf","39062135":"# Calculate the difference between the targets and the predictions\n# Note that this is actually the residual\ndf_pf['Residual'] = df_pf['Target'] - df_pf['Prediction']","ded98d38":"# Finally, lets see how far off we are from the result percentage-wise\n# Here, we take the absolute difference in %, so we can easily order the data frame\ndf_pf['Difference%'] = np.absolute(df_pf['Residual']\/df_pf['Target']*100)\ndf_pf","232d8b5b":"# Exploring the descriptives here gives us additional insights\ndf_pf.describe()","5dd91cbb":"# Check these outputs manually\n# To see all rows, we use the relevant pandas syntax\npd.options.display.max_rows = 999\n# Moreover, to make the dataset clear, we can display the result with only 2 digits after the dot \npd.set_option('display.float_format', lambda x: '%.2f' % x)\n# Finally, we sort by difference in % and manually check the model\ndf_pf.sort_values(by=['Difference%'])","0f529b56":"### Create regression","aa8f585a":"I'm trying to do the very basics with this excercise.\nMy goal is to train a linear regression model with some set of variables in this data set to determine price of used car","0462e11f":"Going to the bottom of the data frame we can see that they are very few predictions that are far off from the observed values.\nIf you look closely at the observed column you will notice that the observed prices are extermely low.\n","e136b3d9":"### Train Test Split","78a18cbc":"### LOAD DATA","f603bea3":"### Testing","b1729dd8":"### PREPROCESSING","3dee0aec":"### Dealing With Multicollinearity","dda5cee4":"### Dealing with missing values","999768dc":"### Removing outliers","d88d3188":"### Finding the weight and bias","0bf7fca0":"### IMPORTING LIBRARIES","f1c26422":"### Checking OLS assumptions","cdd5d083":"### How to improve our model","bf947960":"1. Use a different set of variables\n2. Remove a bigger part of the outlier observation\n3. Use different kinds of transformations","ac3bc424":"#### Scale the data","4193aa79":"### Conclusion\n","44004bce":"### Exploring the descriptive statistics of the variable","79ef197c":"### Determining the variables of interest","aa8c37dd":"### Linear regression model\n","1c2afa6c":"### Exploring probability distribution functions for each feature","c4b4dbb7":"From the subplots and the PDF of price, we can easily determine that 'Price' is exponentially distributed instead of a linear relationship\n\nA good transformation in this case is a log transformation","8f3d78d2":"#### A great step in the data exploration is to display the probability distribution function (PDF) of a variable\n#### The PDF will show us how that variable is distributed \n#### This makes it very easy to spot anomalies, such as outliers\n#### The PDF is often the basis on which we decide whether we want to transform a feature","09eab77b":"From our result we can see that 'Year' has the highest VIFs.\nIf i remove 'Year' from the data frame it will cause the other varibles VIFs to reduce.\nHence, i will remove 'Year'.","b6468cf5":"In conclusion, our model is using mileage, EngineV, Registration, Brand and body type to predict price of a used car.\nOn average it is pretty decent at predicting the price but for the last samples it isnt.\nAll residuals for the outliers are negative. therefore the predictions are higher than the targets. The explanation\nmaybe that we are missing an important feature which drives the price of a used car lower. factors such as model of the car that\nwe removed at the beginning of the analysis or the car was damaged in some way.\n\ni've used\n1. data exploration\n2. feature scaling\n3. data visualization \n4. machine learning algorithm \n\nThere is still so much to improve in our model ","7820f888":"### Intro","fca96ec7":"# Predicting the price of a used car","3bf8e408":"#### Declare x and y","585dbfc7":"### Creating dummies with categorical variables","3019163c":"1. Importing libraries\n2. Load data\n3. Preprocessing\na. Exploring descriptive statistics\nb. Determing variable of interest\nc. Dealing with missing values\n4. Exploring Probability distribution function\n5. Removing outliers\n6. Checking OLS assumptions\n7. Dealing with multicollinearity\n8. Creating dummies with categorical variables\n9. Linear Regression model\na. Declare x and y\nb. Scale the data\nc. Train\\test split\nd. Create Regression\ne. Finding the weight and bias\nf. Testing\n10. Conclusion\n11. How to improve model"}}