{"cell_type":{"e32b6ce4":"code","5f60a44f":"code","859c2d2c":"code","7eee344b":"code","e702f254":"code","823de0ef":"code","d589e236":"code","c3e46f27":"code","7da84afc":"code","32bce969":"code","14c5a4a4":"code","5615c359":"code","e5f8bb4f":"code","6400ca39":"code","1f527a23":"code","04fe39f9":"code","9a22686c":"code","f40fe666":"code","7ceabe78":"code","863baf0f":"code","892c98a6":"code","fd9ac8d6":"code","fddf9c6c":"code","cbcacd3d":"code","25b6ecae":"code","49337653":"code","c6c73f69":"code","7839df30":"code","ba641c73":"code","28e2f793":"markdown"},"source":{"e32b6ce4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler","5f60a44f":"df = pd.read_csv('..\/input\/Churn_Modelling.csv')","859c2d2c":"df.head()","7eee344b":"geograpy_onehot = pd.get_dummies(df.Geography)\ndf = df.drop('Geography', axis=1)\ndf = df.join(geograpy_onehot)\n\ngender_onehot = pd.get_dummies(df.Gender)\ndf = df.drop('Gender', axis=1)\ndf = df.join(gender_onehot)\n\nlableenc = LabelEncoder()\ndf.Surname = lableenc.fit_transform(df.Surname)\n\ndf.head()","e702f254":"def split_train_test(dataset):\n    dataset_length = len(dataset.index)\n    train_length = dataset_length*0.8 - 1\n    train = df.loc[:train_length,:]\n    test = df.loc[train_length+1:,:]\n    return train, test","823de0ef":"train, test = split_train_test(df)","d589e236":"def split_fea_lab(dataset, features_col, labeles_col):\n    features = dataset.loc[:, features_col]\n    labeles = dataset.loc[:, [labeles_col]]\n    return features, labeles","c3e46f27":"features_col = df.columns\nfeatures_col = features_col.drop(['Exited', 'RowNumber', 'CustomerId'])\nlabels_col = 'Exited'\nfeatures, labels = split_fea_lab(train, features_col, labels_col)\ntest_features, test_labels = split_fea_lab(test, features_col, labels_col)","7da84afc":"scaler = MinMaxScaler()\nfeatures = scaler.fit_transform(features)\ntest_features = scaler.fit_transform(test_features)","32bce969":"def reshape_fea_lab(features, labels):\n    features =    features.T\n    labels =      labels.T\n    return features, labels","14c5a4a4":"features, labels = reshape_fea_lab(features, labels)\ntest_features, test_labels = reshape_fea_lab(test_features, test_labels)","5615c359":"def parameter_initialization(n_x, n_h, n_y):\n    W1 = np.random.randn(n_h, n_x) * 0.01\n    b1 = np.zeros((n_h,1))\n    W2 = np.random.randn(n_y, n_h) * 0.01\n    b2 = np.zeros((n_y,1))\n    \n    assert (W1.shape == (n_h, n_x))\n    assert (b1.shape == (n_h,1))\n    assert (W2.shape == (n_y, n_h))\n    assert (b2.shape == (n_y,1))\n    \n    parameters = {\n        'W1':W1,\n        'b1':b1,\n        'W2':W2,\n        'b2':b2\n    }\n    return parameters","e5f8bb4f":"n_x = features.shape[0]\nn_h = int(np.ceil((features.shape[0]+labels.shape[0])\/2))\nn_y = labels.shape[0]\nparameters = parameter_initialization(n_x, n_h, n_y)","6400ca39":"def sigmoid(x):\n    return 1\/(1+np.exp(-x))","1f527a23":"def forward_propagation(features, parameters):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    Z1 = np.dot(W1, features) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = sigmoid(Z2)\n    \n    assert (A2.shape == (1, features.shape[1]))\n    \n    cache = {\n        'Z1':Z1,\n        'A1':A1,\n        'Z2':Z2,\n        'A2':A2\n    }\n    return A2, cache","04fe39f9":"A2, cache = forward_propagation(features, parameters)","9a22686c":"def compute_cost(A2, labels):\n    m = labels.shape[1]\n    logprobs =  np.dot(labels, np.log(A2.T)) + np.dot(1-labels, np.log(1-A2.T))\n    cost = -(1\/m) * np.sum(logprobs)\n    assert(isinstance(cost, float))\n    return cost","f40fe666":"cost = compute_cost(A2, labels)\nprint('cost ' + str(cost))","7ceabe78":"def backward_propagation(cache, parameters, features, labels):\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    \n    A1 = cache['A1']\n    A2 = cache['A2']\n    \n    m = features.shape[1]\n    \n    dZ2 = A2-labels\n    dZ2 = np.array(dZ2)\n    dW2 = (1\/m)*np.dot(dZ2, A1.T)\n    db2 = (1\/m)* np.sum(dZ2, axis=1, keepdims=True)\n    \n    dZ1 = np.multiply(np.dot(dW2.T, dZ2), (1-np.power(A1,2)))\n    dZ1 = np.array(dZ1)\n    dW1 = (1\/m)*np.dot(dZ1, features.T)\n    db1 = (1\/m)*np.sum(dZ1, axis=1, keepdims=True)\n    \n    grads = {\n        'dW1':dW1,\n        'db1':db1,\n        'dW2':dW2,\n        'db2':db2\n    }\n    \n    return grads","863baf0f":"grads = backward_propagation(cache, parameters, features, labels)","892c98a6":"def update_parameters(parameters, grads, learning_rate=0.01):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    dW1 = grads['dW1']\n    db1 = grads['db1']\n    dW2 = grads['dW2']\n    db2 = grads['db2']\n    \n    W1 = W1-learning_rate*dW1\n    b1 = b1=learning_rate*db1\n    W2 = W2-learning_rate*dW2\n    b2 = b2-learning_rate*db2\n    \n    parameters= {\n        'W1': W1,\n        'b1': b1,\n        'W2': W2,\n        'b2': b2\n    }\n    return parameters","fd9ac8d6":"parameters = update_parameters(parameters, grads)","fddf9c6c":"def ANN_Model(features, labels, num_iterations, print_cost=False):\n    parameters = parameter_initialization(n_x,n_h,n_y)\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    costs = []\n    for i in range(0,num_iterations):\n        #forward propagation\n        A2, cache = forward_propagation(features, parameters)\n        \n        #cost\n        cost = compute_cost(A2, labels)\n        \n        #backward propagation\n        grads = backward_propagation(cache, parameters, features, labels)\n        \n        #update parameters\n        parameters = update_parameters(parameters, grads)\n        \n        #print cost\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = 0.01\")\n    plt.show()\n            \n    return parameters","cbcacd3d":"parameters = ANN_Model(features, labels, num_iterations=10000, print_cost=True)","25b6ecae":"def predict(parameters, features):\n    A2, cache = forward_propagation(features, parameters)\n    predictions = np.round(A2)\n    \n    return predictions","49337653":"predictions = predict(parameters, features)","c6c73f69":"print ('Training Accuracy: %d' % float((np.dot(labels,predictions.T) + np.dot(1-labels,1-predictions.T))\/float(labels.size)*100) + '%')","7839df30":"test_predictions = predict(parameters, test_features)","ba641c73":"print ('Testing Accuracy: %d' % float((np.dot(test_labels,test_predictions.T) + np.dot(1-test_labels,1-test_predictions.T))\/float(test_labels.size)*100) + '%')","28e2f793":"**In this notebook i am going to implement ANN from scratch using numpy and pandas**\nI have implemented using numpy\n1. Forward propagation.\n2. Backward propagation.\n3. Parameter updatation.\n\nFuture Scope:\n1. hyper parameter tuning.\n2. regularization."}}