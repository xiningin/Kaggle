{"cell_type":{"22dcab3b":"code","769ede18":"code","b310c8aa":"code","f802cfd1":"code","6651cf45":"code","97c4d224":"code","34004885":"code","f53d7556":"markdown","1f3ddf2e":"markdown","66479e71":"markdown","7e8f6307":"markdown","65e2fc68":"markdown"},"source":{"22dcab3b":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport sys\nimport time\nfrom transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\nimport textwrap\nimport re\nimport attr\nimport abc\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom IPython.display import HTML\nfrom os import listdir\nfrom os.path import isfile, join\n\nimport warnings  \nwarnings.filterwarnings('ignore')\nMAX_ARTICLES = 1000\nbase_dir = '\/kaggle\/input'\ndata_dir = base_dir + '\/covid-19-articles'\ndata_path = data_dir + '\/covid19.csv'\nmodel_path = base_dir + '\/biobert-qa\/biobert_squad2_cased'\n\nclass ResearchQA(object):\n    def __init__(self, data_path, model_path):\n        print('Loading data from', data_path)\n        self.df = pd.read_csv(data_path)\n        print('Initializing model from', model_path)\n        self.model = TFAutoModelForQuestionAnswering.from_pretrained(model_path, from_pt=True)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.retrievers = {}\n        self.build_retrievers()\n        self.main_question_dict = dict()\n        \n    \n    def build_retrievers(self):\n        df = self.df\n        abstracts = df[df.abstract.notna()].abstract\n        self.retrievers['abstract'] = TFIDFRetrieval(abstracts)\n        body_text = df[df.body_text.notna()].body_text\n        self.retrievers['body_text'] = TFIDFRetrieval(body_text)\n\n    def retrieve_candidates(self, section_path, question, top_n):\n        candidates = self.retrievers[section_path[0]].retrieve(question, top_n)\n        return self.df.loc[candidates.index]\n\n    def get_questions(self, question_path):\n        print('Loading questions from', question_path)\n        expert_question_answer = pd.read_csv(question_path, sep='\\t')\n        self.main_question_dict = dict()\n        \n        question_list_str = ''\n        for _, row in expert_question_answer.iterrows():\n          task = row['Task #']\n          main_question = row['Main Question']\n          questions = row['Question']\n          answer = row['Answer']\n          doi = row['DOI']\n          cohort_size = row['Cohort Size']\n          study_type = row['Study type']\n          if pd.notna(main_question):\n            # Get first 5 words in the main question\n            main_question_abbr = '_'.join(main_question.replace('(', ' ').replace(')', ' ').split(' ')[:5])\n            self.main_question_dict[main_question_abbr] = dict()\n          if pd.notna(questions):\n            answer_list = []\n            question_list_str = questions.replace('(', '_').replace(')', '_')\n          if pd.notna(answer):      \n            answer_list.append((answer, doi, cohort_size, study_type))\n          self.main_question_dict[main_question_abbr][question_list_str] = answer_list            \n        \n    def get_answers(self, question, section='abstract', keyword=None, max_articles=1000, batch_size=12):\n        df = self.df\n        answers = []\n        section_path = section.split('\/')\n\n        if keyword:\n            candidates = df[df[section_path[0]].str.contains(keyword, na=False, case=False)]\n        else:\n            candidates = self.retrieve_candidates(section_path, question, top_n=max_articles)\n        if max_articles:\n            candidates = candidates.head(max_articles)\n\n        text_list = []\n        indices = []\n        for idx, row in candidates.iterrows():\n            if section_path[0] == 'body_text':\n                text = self.get_body_section(row.body_text, section_path[1])\n            else:\n                text = row[section]\n            if text and isinstance(text, str):\n                text_list.append(text)\n                indices.append(idx)\n\n        num_batches = len(text_list) \/\/ batch_size\n        all_answers = []\n        for i in range(num_batches):\n            batch = text_list[i * batch_size:(i+1) * batch_size]\n            answers = self.get_answers_from_text_list(question, batch)\n            all_answers.extend(answers)\n\n        last_batch = text_list[batch_size * num_batches:]\n        if last_batch:\n            all_answers.extend(self.get_answers_from_text_list(question, last_batch))\n\n        columns = ['doi', 'authors', 'journal', 'publish_time', 'title', 'cohort_size']\n        processed_answers = []\n        for i, a in enumerate(all_answers):\n            if a:\n                row = candidates.loc[indices[i]]\n                new_row = [a.text, a.start_score, a.end_score, a.input_text]\n                new_row.extend(row[columns].values)\n                processed_answers.append(new_row)\n        answer_df = pd.DataFrame(processed_answers, columns=(['answer', 'start_score',\n                                                 'end_score', 'context'] + columns))\n        return answer_df.sort_values(['start_score', 'end_score'], ascending=False)\n\n    def get_body_section(self, body_text, section_name):\n      sections = body_text.split('<SECTION>\\n')\n      for section in sections:\n        lines = section.split('\\n')\n        if len(lines) > 1:\n          if section_name.lower() in lines[0].lower():\n            return section\n\n    def get_answers_from_text_list(self, question, text_list, max_tokens=512):\n      tokenizer = self.tokenizer\n      model = self.model\n      inputs = tokenizer.batch_encode_plus(\n          [(question, text) for text in text_list], add_special_tokens=True, return_tensors='tf',\n          max_length=max_tokens, truncation_strategy='only_second', pad_to_max_length=True)\n      input_ids = inputs['input_ids'].numpy()\n      answer_start_scores, answer_end_scores = model(inputs)\n      answer_start = tf.argmax(\n          answer_start_scores, axis=1\n      ).numpy()  # Get the most likely beginning of each answer with the argmax of the score\n      answer_end = (\n          tf.argmax(answer_end_scores, axis=1) + 1\n      ).numpy()  # Get the most likely end of each answer with the argmax of the score\n\n      answers = []\n      for i, text in enumerate(text_list):\n        input_text = tokenizer.decode(input_ids[i, :], clean_up_tokenization_spaces=True)\n        input_text = input_text.split('[SEP] ', 2)[1]\n        answer = tokenizer.decode(\n            input_ids[i, answer_start[i]:answer_end[i]], clean_up_tokenization_spaces=True)\n        score_start = answer_start_scores.numpy()[i][answer_start[i]]\n        score_end = answer_end_scores.numpy()[i][answer_end[i]-1]\n        if answer and not '[CLS]' in answer:\n          answers.append(Answer(answer, score_start, score_end, input_text))\n        else:\n          answers.append(None)\n      return answers\n\n    def output_answers_for_task(self, task):\n      question_path = base_dir + '\/cov19questions\/CORD-19-research-challenge-tasks - Question_{}.tsv'.format(task)\n      # Get questions related to this task from the expert generated question file.\n      self.get_questions(question_path)        \n        \n      for main_question, value in self.main_question_dict.items():\n        print(f\"Output for main Question: {main_question}\") \n        output_csvfile = main_question + '.csv'\n        new_main_question = True\n\n        for questions, answers in value.items():\n          question_list = (questions.split(','))\n          for question in question_list:\n            print(f\"Writing answer for question: {question}\")\n            for sec in ['abstract', 'body_text\/discussion', 'body_text\/conclusion']:\n              model_prediction = self.get_answers(question, section=sec, max_articles=MAX_ARTICLES, batch_size=12)\n              # if this is a new main question, create a new answer file. \n              if new_main_question:\n                model_prediction.to_csv(output_csvfile, header=model_prediction.columns.to_list(), index=False)\n                new_main_question = False\n              else:\n                model_prediction.to_csv(output_csvfile, mode='a', header=False, index=False)\n    \n\nclass Retrieval(abc.ABC):\n  \"\"\"Base class for retrieval methods.\"\"\"\n\n  def __init__(self, docs, keys=None):\n    \"\"\"\n    Args:\n      docs: a pd.Series of strings. The text to retrieve.\n      keys: a pd.Series. Keys (e.g. ID, title) associated with each document.\n    \"\"\"\n    self._docs = docs.copy()\n    if keys is not None:\n      self._docs.index = keys\n    self._model = None\n    self._doc_vecs = None\n\n  def _top_documents(self, q_vec, top_n=10):\n    similarity = cosine_similarity(self._doc_vecs, q_vec)\n    rankings = np.argsort(np.squeeze(similarity))[::-1]\n    ranked_indices = self._docs.index[rankings]\n    return self._docs[ranked_indices][:top_n]\n\n  @abc.abstractmethod\n  def retrieve(self, query, top_n=10):\n    pass\n\nclass TFIDFRetrieval(Retrieval):\n  \"\"\"Retrieve documents based on cosine similarity of TF-IDF vectors with query.\"\"\"\n\n  def __init__(self, docs, keys=None):\n    \"\"\"\n    Args:\n      docs: a list or pd.Series of strings. The text to retrieve.\n      keys: a list or pd.Series. Keys (e.g. ID, title) associated with each document.\n    \"\"\"\n    super(TFIDFRetrieval, self).__init__(docs, keys)\n    self._model = TfidfVectorizer()\n    self._doc_vecs = self._model.fit_transform(docs)\n\n  def retrieve(self, query, top_n=10):\n    q_vec = self._model.transform([query])\n    return self._top_documents(q_vec, top_n)\n\n@attr.s\nclass Answer(object):\n    text = attr.ib()\n    start_score = attr.ib()\n    end_score = attr.ib()\n    input_text = attr.ib()\n    \ndef answer_questions(questions, qa, max_articles, section='abstract'):\n  for question_group in questions:\n    main_question = question_group[0]\n    answers = {}\n    for q in question_group[1:]:\n      answers[q] = qa.get_answers(q, section=section, max_articles=max_articles)\n    render_results(main_question, answers)\n  # Return the last set for debugging.\n  return main_question, answers\n\n\nstyle = '''\n<style>\n.hilight {\n  background-color:#cceeff;\n}\na {\n  color: #000 !important;\n  text-decoration: underline;\n}\n.question {\n  font-size: 20px;\n  font-style: italic;\n  margin: 10px 0;\n}\n.info {\n  padding: 10px 0;\n}\ntable.dataframe {\n  max-height: 450px;\n  text-align: left;\n}\n.meta {\n  margin-top: 10px;\n}\n.journal {\n  color: green;\n}\n.footer {\n  position: absolute;\n  bottom: 20px;\n  left: 20px;\n}\n<\/style>\n'''\n\ndef format_context(row):\n  text = row.context\n  answer = row.answer\n  highlight_start = text.find(answer)\n\n  def find_context_start(text):\n    idx = len(text) - 1\n    while idx >= 2:\n      if text[idx].isupper() and re.match(r'\\W ', text[idx - 2:idx]):\n        return idx\n      idx -= 1\n    return 0 \n  context_start = find_context_start(text[:highlight_start])\n  highlight_end = highlight_start + len(answer)\n  context_html = (text[context_start:highlight_start] + '<span class=hilight>' + \n                  text[highlight_start:highlight_end] + '<\/span>' + \n                  text[highlight_end:highlight_end + 1 + text[highlight_end:].find('. ')])\n  context_html += f'<br><br>score: {row.start_score:.2f}'\n  return context_html\n\n\ndef format_author(authors):\n  if not authors or not isinstance(authors, str):\n    return 'Unknown Authors'\n  name = authors.split(';')[0]\n  name = name.split(',')[0]\n  return name + ' et al'\n\ndef format_info(row):\n  meta = []\n  authors = format_author(row.authors) \n  if authors:\n    meta.append(authors)\n  meta.append(row.publish_time)\n  meta = ', '.join(meta)\n \n  html = f'''\\\n  <a class=\"title\" target=_blank href=\"http:\/\/doi.org\/{row.doi}\">{row.title}<\/a>\\\n  <div class=\"meta\">{meta}<\/div>\\\n  '''\n\n  journal = row.journal\n  if journal and isinstance(journal, str):\n    html += f'<div class=\"journal\">{journal}<\/div>'\n\n  return html\n\ndef render_results(main_question, answers):\n  id = main_question[:20].replace(' ', '_')\n  html = f'<h1 id=\"{id}\" style=\"font-size:20px;\">{main_question}<\/h1>'\n  for q, a in answers.items():\n    # TODO: skipping empty answers. Maybe we should show\n    # top retrieved docs.\n    if a.empty:\n      continue\n    # clean up question\n    if '?' in q:\n        q = q.split('?')[0] + '?'\n    html += f'<div class=question>{q}<\/div>' + format_answers(a)\n  display(HTML(style + html))\n\ndef format_answers(a):\n    a = a.sort_values('start_score', ascending=False)\n    a.drop_duplicates('doi', inplace=True)\n    out = []\n    for i, row in a.iterrows():\n      if row.start_score < 0:\n        continue\n      info = format_info(row)\n      context = format_context(row)\n      cohort = ''\n      if not np.isnan(row.cohort_size):\n        cohort = int(row.cohort_size)\n      out.append([context, info, cohort])\n    out = pd.DataFrame(out, columns=['answer', 'article', 'cohort size'])\n    return out.to_html(escape=False, index=False)\n\ndef render_answers(a):\n    display(HTML(style + format_answers(a)))","769ede18":"qa = ResearchQA(data_path, model_path)","b310c8aa":"# Get cohort size for all articles.\nget_cohort_size = False\nif get_cohort_size:\n    qa.df['cohort_size'] = ''\n    cohort = qa.get_answers('How many patients?', section='abstract', max_articles=10000)\n    cohort['cohort_size'] = pd.to_numeric(cohort.answer, errors='coerce')\n    del qa.df['cohort_size']\n    qa.df = qa.df.merge(cohort[['doi', 'cohort_size']], on='doi', how='left')\n    qa.df.cohort_size.describe()","f802cfd1":"answers = qa.get_answers('What are the risk factors?', max_articles=10)\nrender_answers(answers)","6651cf45":"def build_task_main_question_dict():\n    task_main_question_dict = dict()\n    for task in ['Task1', 'Task2', 'Task4', 'Task5']:\n        question_path = base_dir + '\/cov19questions\/CORD-19-research-challenge-tasks - Question_{}.tsv'.format(task)\n        if task not in task_main_question_dict:\n            task_main_question_dict[task] = dict()\n        expert_question_answer = pd.read_csv(question_path, sep='\\t')\n        for _, row in expert_question_answer.iterrows():\n            main_question = row['Main Question']            \n            questions = row['Question']\n            if pd.notna(main_question):\n                main_question_abbr = '_'.join(main_question.replace('(', ' ').replace(')', ' ').split(' ')[:5])\n            if main_question_abbr not in task_main_question_dict[task]:\n                questions_list = []\n                task_main_question_dict[task][main_question_abbr] = (main_question, questions_list)            \n            if pd.notna(questions):\n                main_question, questions_list = task_main_question_dict[task][main_question_abbr]\n                questions_list.append(questions)\n                task_main_question_dict[task][main_question_abbr] = (main_question, questions_list)\n    return task_main_question_dict\n    \ndef get_task_and_main_question_question_list_for(model_prediction_file_name, task_main_question_dict):\n    model_prediction_file_name_prefix = model_prediction_file_name.split('.')[0]\n    for task, main_question_entry in task_main_question_dict.items():\n        for main_question_abbr, main_question_question_list in main_question_entry.items():\n            if model_prediction_file_name_prefix == main_question_abbr:\n                main_question, questions = main_question_question_list\n                return task, main_question, questions\n    return None, None, None\n\ndef calculate_score_buckets():\n    # Calculate the eval scores for each bucket of scores from the QA model. This is used to decide a threshold.\n    eval_path = base_dir + '\/model-eval'\n    model_result_dir = eval_path\n    count = 0\n    main_question_files = [f for f in listdir(model_result_dir) if isfile(join(model_result_dir, f))]\n    score_doi_match = []\n    for model_eval_file in main_question_files:\n        if not model_eval_file.endswith('.csv'):\n            continue\n        model_prediction_path = join(model_result_dir, model_eval_file)\n        model_prediction = pd.read_csv(model_prediction_path, sep=',')\n        for _, row in model_prediction.iterrows():\n            score_doi_match.append((row['start_score'], row['end_score'], row['doi_match'], row['best_rouge1_fmeasure'], row['best_rouge2_fmeasure'], \n                      row['best_rougeL_fmeasure'], row['best_dist_euclidean'], \n                      row['best_dist_cosine']))\n    score_doi_match = np.asarray(score_doi_match, np.float32)\n    sort_indices = np.argsort(score_doi_match[:, 0])\n    for compare_score_index in range(2, 7):\n        print('Comparing score index ' + str(compare_score_index))\n        # Calculate % of DOI matches in each score bucket for the start_score.\n        bucket_counts = {}\n        for i in range(-8, 15):\n            bucket_counts[str(i)] = [0, 0]\n        print('Start score buckets')\n        for row in score_doi_match[sort_indices]:\n            score = int(round(row[0]))\n\n            bucket_counts[str(score)][1] += 1\n            if compare_score_index == 2:\n                if row[2] > 0:\n                    bucket_counts[str(score)][0] += 1\n            else:\n                bucket_counts[str(score)][0] += row[compare_score_index]\n        for (score, count) in bucket_counts.items():\n            if (count[1] > 0):\n                print(score, count[0] \/ count[1])\n        # Calculate % of DOI matches in each score bucket for the end_score.\n        bucket_counts = {}\n        for i in range(-8, 15):\n            bucket_counts[str(i)] = [0, 0]\n        print('End score buckets')\n        for row in score_doi_match[sort_indices]:\n            score = int(round(row[1]))\n            bucket_counts[str(score)][1] += 1\n            if compare_score_index == 2:\n                if row[2] > 0:\n                    bucket_counts[str(score)][0] += 1\n            else:\n                bucket_counts[str(score)][0] += row[compare_score_index]\n        for (score, count) in bucket_counts.items():\n            if (count[1] > 0):\n                print(score, count[0] \/ count[1])\n\ndef get_all_answers_for_task(task, task_main_question_dict, start_threshold=0, end_threshold=2, max_results=20):  \n    pregenerated_model_answers_path = base_dir + '\/model-answers'\n    model_prediction_files = [f for f in listdir(pregenerated_model_answers_path) if isfile(join(pregenerated_model_answers_path, f))]\n\n    doc_columns = ['doi', 'authors', 'journal', 'publish_time', 'title','cohort_size']\n    answer_columns = ['answer', 'start_score', 'end_score', 'context']    \n    \n    for model_prediction_file in model_prediction_files:\n        predict_task, main_question, questions_list = get_task_and_main_question_question_list_for(model_prediction_file, task_main_question_dict) \n        if predict_task == task:\n            model_prediction_path = join(pregenerated_model_answers_path, model_prediction_file)\n            model_prediction = pd.read_csv(model_prediction_path, sep=',')\n            main_question_abbr = model_prediction_file.split('.')[0]\n            question_answer_dict = dict()\n            for _, row in model_prediction.iterrows():\n                if row['start_score'] < start_threshold or row['end_score'] < end_threshold:\n                    continue\n                model_question = row['question']\n                cohort_size = row['cohort_size_from_abstract'] if pd.notna(row['cohort_size_from_abstract']) else row['cohort_size_from_body']      \n            \n                for questions_str in questions_list:\n                    if model_question in questions_str:\n                        if questions_str not in question_answer_dict:\n                            answer_and_eval = pd.DataFrame(columns=(answer_columns + doc_columns))                    \n                            question_answer_dict[questions_str] = answer_and_eval\n                        answer_and_eval = question_answer_dict[questions_str]  \n                        answer_and_eval.loc[len(answer_and_eval)] = [row['answer'], row['start_score'], row['end_score'], row['context'], row['doi'], \n                                                               row['authors'], row['journal'], row['publish_time'], row['title'], cohort_size]\n                # Sort by score before dedup.\n                answer_and_eval = answer_and_eval.sort_values('start_score', ascending=False)\n                answer_and_eval.drop_duplicates('doi', inplace=True)\n            render_results(main_question, question_answer_dict)","97c4d224":"task_main_question_dict = build_task_main_question_dict()\nget_all_answers_for_task('Task2', task_main_question_dict, 0, 2)","34004885":"# qa.output_answers_for_task('Task2')","f53d7556":"View the answers for our chosen questions below:","1f3ddf2e":"# CORD-19 Q&A with BioBERT and domain expertise\n\n## Overview\nWe present an extractive question-answering approach to the COVID-19 Open Research Dataset Challenge, using a pretrained BioBERT model fine-tuned on SQuAD 2.0 to extract relevant information for the challenge tasks from the available documents. Our goal is to produce a smart literature review where we use question-answering to find the relevant answers in the document set as well as the evidence for each answer. We believe the ideal literature review should combine automated machine learning with domain expertise. Our final approach uses a mixture of domain\/expert-knowledge to expand on the given tasks and systems for information retrieval and question answering.\n\n## Methodology\nThe set of steps to produce this work can be split into two parts as follows, one requiring machine learning expertise and one that made use of domain expertise. These parts were done in parallel where possible.\n\n*ML system building*\n\n1. Filter the set of CORD-19 documents to a list relevant to COVID-19 (keyword-based).\n2. Build an information retrieval system using TF-IDF to quickly retrieve the relevant set of documents for a particular part of the task.\n3. Build and use a question answering system to produce answers to task-relevant questions.\n4. Evaluate the answers by using ROUGE and document recall to compare with a set of clinically written answers for a subset of questions and use these to set the thresholds for displaying the answers.\n\n*Clinical expertise*\n\n1. With the aid of clinicians, convert the topics in the task to a set of precise medical questions (included in the data for this notebook, Cov19-questions).\n2. For some of those questions, produce a set of model answers.\n3. Sanity-check the output of the model for a subset of the questions.\n\n\n## Q&A Model Background\nBERT ([Devlin et al. 2018](https:\/\/arxiv.org\/abs\/1810.04805)) is a contextual word representation model learned from large-scale language model pretraining of a bidirectional Transformer ([Vaswani et al. 2017](https:\/\/arxiv.org\/abs\/1706.03762)). Following pretraining on general natural language corpora, BERT can readily be fine-tuned for many downstream NLP tasks, achieving high performance using much smaller datasets. Recent work has shown major improvements in a wide variety of tasks using BERT or similar Transformer models, and variants of BERT have been adapted to specific domains by pretraining on specialized corpora \u2013 e.g. BioBERT ([Lee et al. 2019](https:\/\/arxiv.org\/abs\/1901.08746)) augments the BERT pretraining corpus with scholarly biomedical literature from PubMed.\n\nA common downstream NLP task is extractive question-answering: given a question and a corresponding passage, a model learns to extract the excerpt from the passage that best answers the question. The standard benchmark dataset for this task is [SQuAD](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/). Notably, in the latest version (SQuAD 2.0, [Rajpurkar et al. 2018](https:\/\/arxiv.org\/abs\/1806.03822)), approximately \u2153 of the questions in the dataset are intentionally unanswerable, so that high-quality models must learn to abstain from answering when provided with insufficient evidence. (We found that fine-tuning on this dataset improved the quality of our answers compared to SQuAD 1.1.)\n\n## Implementation\nGiven the biomedical content of the CORD-19 corpus, and the sparse, uncertain nature of question answering with this dataset (i.e. most documents do not contain good answers to most questions), our Q&A system is thus powered by a pretrained BioBERT model fine-tuned for extractive question answering using SQuAD 2.0.\n\nWe use the excellent [Huggingface Transformers](https:\/\/huggingface.co\/transformers\/) library for running inference with our Q&A model. The model checkpoint is included with the submission and can be loaded below. To reproduce this checkpoint, use the run_squad.py script included in the [Huggingface Transformers examples](https:\/\/github.com\/huggingface\/transformers\/tree\/master\/examples#squad) with the following command (takes ~8 hours on a GTX 1080):\n\n```\npython run_squad.py \\\n  --model_type bert \\\n  --model_name_or_path monologg\/biobert_v1.1_pubmed \\\n  --do_train \\\n  --do_eval \\\n  --train_file $SQUAD_DIR\/train-v2.0.json \\\n  --predict_file $SQUAD_DIR\/dev-v2.0.json \\\n  --per_gpu_train_batch_size 8 \\\n  --learning_rate 3e-5 \\\n  --num_train_epochs 4 \\\n  --max_seq_length 384 \\\n  --doc_stride 128 \\\n  --output_dir \/tmp\/biobert_squad2\/ \\\n  --version_2_with_negative\n```\n\nTo improve accuracy and relevance of the results, we pre-filter the corpus using a list of keywords related to COVID-19, as shown in [this kernel](https:\/\/www.kaggle.com\/mimisun\/covid-19-articles). At query time, the top candidate documents are automatically retrieved based on TF-IDF cosine similarity with the query, with the option to manually filter using only keywords if desired. By default, we extract answers from the abstract, discussion, and conclusion sections. Answers are ranked based on the model's start score of the answer span. We also extract cohort size, where available, by simply asking the model \u201cHow many patients?\u201d over the study abstracts.\n","66479e71":"## Task 2: What do we know about COVID-19 risk factors?\n\n### Question Generation\nOur Q&A model requires a natural language question as an input. Using the medical expertise of two clinician-scientists (C. Chen and S. Kang), we generated 5 questions based on the high-level overview for this challenge task. This strategy models that of researchers doing a literature review \u2013 starting with a broad task and then breaking it down to manageable pieces to further explore. We recruited the help of clinical researchers, our potential end users, to ensure that the questions were relevant and accurate.\n\n### Evaluation\nFor a subset of the questions, our clinical experts manually identified relevant articles, key words, and DOIs. We evaluated the quality of the model answers against the expert-selected answers using document recall and ROUGE (Recall-Oriented Understudy for Gisting Evaluation) We bucketed each model-produced answer based on the score from the Q&A model. We then assigned the document recall and ROUGE scores to each model-produced bucket. We set the Q&A threshold by finding an appropriate recall\/precision tradeoff based on these score buckets.\n\n### Model output\nTry asking your own question here:","7e8f6307":"## Discussion\nLiterature reviews, while time-consuming, are perhaps the most important step in understanding a particular research topic or question. During this pandemic, the amount of literature generated has been overwhelming, and manually digesting this volume of information is nearly impossible.  \n\nBy applying NLP methods, we are able to automatically filter out much of the noise and directly surface the most relevant information. Using a streamlined, reproducible process, our Q&A model can easily be rerun to incorporate new literature as it is published, giving researchers easy access to the most up-to-date results. Moreover, the questions we present here were generated with the needs of clinical users in mind, and formulated broadly enough to be reusable in future pandemics.\n\nIn the future, we would like to improve our system by adding summarization capabilities to aggregate results across articles and extract additional context from each article, including credibility or level of evidence supporting the conclusions. These are challenging open problems in NLP. The extracted studies should be critically reviewed by experts, and consensus should be built between healthcare professionals and the public.\n\n## Contributors\nThis notebook was produced by the following contributors from the Google Health medical records research team https:\/\/github.com\/Google-Health\/records-research : \n\n*Modeling:* Andrew M. Dai, Jonas Kemp, Mimi Sun, Zhen Xu, and Emily Xue.\n*Clinical:* Christina Chen and Shawn Kang. ","65e2fc68":"Run this cell to generate the file containing all answers for our questions on the given task. (Cached answer files are provided with the notebook.)"}}