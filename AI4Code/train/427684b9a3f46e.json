{"cell_type":{"fc476dcf":"code","408d919d":"code","c11ee892":"code","05ece679":"code","f3269cfd":"code","01479e22":"code","98ae2a56":"code","e0943b97":"code","4d723a00":"code","c56a8e61":"code","0597e808":"code","663d7225":"code","f10c3ad9":"code","1738259c":"code","e9eb2139":"code","fd05f28a":"code","e35c288d":"code","6534b829":"code","5e3d0889":"code","5d738133":"code","310fb4fe":"code","644ffbfe":"code","77c8540a":"code","12ecf013":"code","3f1a122d":"code","e77cb8dc":"code","4e1fda2e":"code","b86f6061":"code","9d99e993":"code","1c1f576a":"code","5b2a99a7":"code","96c49d08":"code","6b31c636":"code","032c1fd0":"code","832c575e":"code","c380b2e2":"code","1f51792f":"code","df5d2a50":"code","8dece70d":"code","8fa967a0":"code","60821fac":"code","a506ad18":"code","58e06647":"code","5f8042e8":"code","6ca3e6d0":"code","781ab0b4":"code","e413ccd6":"code","bd8d43c7":"code","b61223ad":"code","acea6fd4":"code","087f37e9":"code","5fc5e5ff":"code","4797b7ef":"code","b5d66e0c":"code","876b7237":"code","a176454c":"markdown","7e0a45d7":"markdown","09c28662":"markdown","f8e60c22":"markdown","df7086c7":"markdown","64ea148f":"markdown","4ec79fe7":"markdown","9860adec":"markdown","03d721a4":"markdown","040eab8e":"markdown","56ba9e99":"markdown","55d2ccca":"markdown","25d631a3":"markdown","9588f73a":"markdown","5ab63308":"markdown","fce9c3f2":"markdown","1dc401b7":"markdown","160884a5":"markdown","c42ae9e7":"markdown","d193e0da":"markdown","e8e68e1b":"markdown","d0abd63f":"markdown"},"source":{"fc476dcf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt #visualization\nimport seaborn as sns #visualization\n%matplotlib inline\nimport itertools\nimport plotly.offline as py#visualization\npy.init_notebook_mode(connected=True)#visualization\nimport plotly.graph_objs as go#visualization\nimport plotly.tools as tls#visualization\nimport plotly.figure_factory as ff#visualization\nimport warnings\nimport seaborn as sns # for plot visualization\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nwarnings.filterwarnings(\"ignore\")\n# Any results you write to the current directory are saved as output.","408d919d":"data = pd.read_csv('\/kaggle\/input\/weather_madrid_lemd_1997_2015.csv\/weather_madrid_LEMD_1997_2015.csv', parse_dates=['CET'],index_col='CET')","c11ee892":"data.head()","05ece679":"data.index","f3269cfd":"data.isnull().sum(axis=0)","01479e22":"data = data.drop([' Events',' Max VisibilityKm',' Mean VisibilityKm',' Min VisibilitykM',' Max Gust SpeedKm\/h',' CloudCover'], axis = 1)","98ae2a56":"data.isnull().sum(axis=0)","e0943b97":"data[data.isna().any(axis=1)]","4d723a00":"data.ffill(axis=0, inplace = True)","c56a8e61":"data.isnull().sum()","0597e808":"data.head()","663d7225":"data['Mean TemperatureC'].plot(subplots=True, figsize=(20,12))\nplt.ylabel('Temperature')","f10c3ad9":"data['MeanDew PointC'].plot(subplots=True, figsize=(20,12))\nplt.ylabel('Dew Point')","1738259c":"data[' Mean Humidity'].plot(subplots=True, figsize=(20,12))\nplt.ylabel('Humidity')","e9eb2139":"train=data['2000':'2013'].resample('M').mean().fillna(method='pad')\ntest=data['2014':'2015'].resample('M').mean().fillna(method='pad')","fd05f28a":"train.index","e35c288d":"test.index","6534b829":"ts=train['Max TemperatureC']\ntest_ts=test['Max TemperatureC']\nts.head()","5e3d0889":"def ts_plot(timeseries):\n    fig=plt.figure(figsize=(20,10))\n    plt.plot(timeseries)\n    plt.show()\n\nts_plot(ts);","5d738133":"ts_plot(test_ts)","310fb4fe":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries, wind_size):\n    \n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window=wind_size).mean()\n    rolstd = timeseries.rolling(window=wind_size).std()\n\n    #Plot rolling statistics:\n    fig=plt.figure(figsize=(20,10))\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    \n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey-Fuller test:\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","644ffbfe":"test_stationarity(ts,14)","77c8540a":"def diff_plot(timeseries):\n    plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\n    \n    fig, axes = plt.subplots(3, 2, sharex=True)\n    axes[0, 0].plot(timeseries.values); \n    axes[0, 0].set_title('Original Series')\n    plot_acf(timeseries.values, ax=axes[0, 1])\n    \n    # 1st Differencing\n    axes[1, 0].plot(timeseries.diff().values); \n    axes[1, 0].set_title('1st Order Differencing')\n    plot_acf(timeseries.diff().dropna().values,ax=axes[1, 1])\n    \n    # 2nd Differencing\n    axes[2, 0].plot(timeseries.diff().diff().values); \n    axes[2, 0].set_title('2nd Order Differencing')\n    plot_acf(timeseries.diff().diff().dropna().values,ax=axes[2, 1])\n    \n    plt.xticks(rotation='vertical')\n    plt.show()\n\n    \ndiff_plot(ts)","12ecf013":"def pacf_plot(timeseries):\n    # PACF plot of 1st differenced series\n    plt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120})\n    \n    fig, axes = plt.subplots(1, 2, sharex=True)\n    axes[0].plot(timeseries.diff().values); axes[0].set_title('1st Differencing')\n    axes[1].set(ylim=(0,5))\n    plot_pacf(timeseries.diff().dropna().values, ax=axes[1])\n    \n    plt.show()\n\npacf_plot(ts)","3f1a122d":"def acf_plot(timeseries):\n    fig, axes = plt.subplots(1, 2, sharex=True)\n    axes[0].plot(timeseries.diff().values); axes[0].set_title('1st Differencing')\n    axes[1].set(ylim=(0,1.2))\n    plot_acf(timeseries.diff().dropna().values, ax=axes[1])\n    \n    plt.show()\n\nacf_plot(ts);","e77cb8dc":"def acf_pacf_plot(timeseries):\n    acf_lag = acf(timeseries.diff().dropna().values, nlags=20)\n    pacf_lag = pacf(timeseries.diff().dropna().values, nlags=20, method='ols')\n    \n    plt.figure(figsize=(22,10))\n    \n    plt.subplot(121)\n    plt.plot(acf_lag)\n    plt.axhline(y=0,linestyle='--',color='silver')\n    plt.axhline(y=-1.96\/np.sqrt(len(timeseries.diff().values)),linestyle='--',color='silver')\n    plt.axhline(y=1.96\/np.sqrt(len(timeseries.diff().values)),linestyle='--',color='silver')\n    plt.title(\"Autocorrelation Function\")\n    \n    plt.subplot(122)\n    plt.plot(pacf_lag)\n    plt.axhline(y=0,linestyle='--',color='silver')\n    plt.axhline(y=-1.96\/np.sqrt(len(timeseries.diff().values)),linestyle='--',color='silver')\n    plt.axhline(y=1.96\/np.sqrt(len(timeseries.diff().values)),linestyle='--',color='silver')\n    plt.title(\"Partial Autocorrelation Function\")\n    plt.tight_layout()\n\nacf_pacf_plot(ts)","4e1fda2e":"model = ARIMA(ts.values, order=(2,0,2))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())","b86f6061":"# Plot residual errors\nresiduals = pd.DataFrame(model_fit.resid)\nfig, ax = plt.subplots(1,2)\nresiduals.plot(title=\"Residuals\", ax=ax[0])\nresiduals.plot(kind='kde', title='Density', ax=ax[1])\nplt.show()","9d99e993":"model_fit.plot_predict(dynamic=False)\nplt.show()","1c1f576a":"fc, se, conf = model_fit.forecast(24, alpha=0.05)  # 95% conf\n\n# print(fc)\n# Make as pandas series\nfc_series = pd.Series(fc, index=test_ts.index)\nlower_series = pd.Series(conf[:, 0], index=test_ts.index)\nupper_series = pd.Series(conf[:, 1], index=test_ts.index)\n\n# # Plot\nplt.figure(figsize=(12,5), dpi=100)\nplt.plot(ts, label='training')\nplt.plot(test_ts, label='actual')\nplt.plot(fc_series, label='forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, \n                 color='k', alpha=.15)\nplt.title('Forecast vs Actuals')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()","5b2a99a7":"train.head()","96c49d08":"ts_dew = train['Dew PointC']\ntest_ts_dew=test['Dew PointC']\nts_hum = train[' Mean Humidity']\ntest_ts_hum = test[' Mean Humidity']","6b31c636":"ts_plot(ts_hum)","032c1fd0":"test_stationarity(ts_hum,12)","832c575e":"diff_plot(ts_hum)","c380b2e2":"acf_pacf_plot(ts_hum)","1f51792f":"model1 = ARIMA(ts_hum.values, order=(2,0,2))\nmodel_fit1 = model1.fit(disp=0)\nprint(model_fit1.summary())","df5d2a50":"residuals = pd.DataFrame(model_fit1.resid)\nfig, ax = plt.subplots(1,2)\nresiduals.plot(title=\"Residuals\", ax=ax[0])\nresiduals.plot(kind='kde', title='Density', ax=ax[1])\nplt.show()","8dece70d":"model_fit1.plot_predict(dynamic=False)\nplt.show()","8fa967a0":"fc, se, conf = model_fit1.forecast(24, alpha=0.05)  # 95% conf\n\n# print(fc)\n# Make as pandas series\nfc_series = pd.Series(fc, index=test_ts_hum.index)\nlower_series = pd.Series(conf[:, 0], index=test_ts_hum.index)\nupper_series = pd.Series(conf[:, 1], index=test_ts_hum.index)\n\n# # Plot\nplt.figure(figsize=(12,5), dpi=100)\nplt.plot(ts_hum, label='training')\nplt.plot(test_ts_hum, label='actual')\nplt.plot(fc_series, label='forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, \n                 color='k', alpha=.15)\nplt.title('Forecast vs Actuals')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()","60821fac":"test_stationarity(ts_dew,12)","a506ad18":"diff_plot(ts_dew)","58e06647":"log_ts_dew = np.log(ts_dew)\n","5f8042e8":"log_ts_dew.dropna(inplace=True)","6ca3e6d0":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts_dew)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(ts_dew, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","781ab0b4":"residual.isnull().count()","e413ccd6":"ts_log_decompose = residual\nts_log_decompose.dropna(inplace=True)\ntest_stationarity(ts_log_decompose,14)","bd8d43c7":"acf_pacf_plot(ts_log_decompose)","b61223ad":"ts_log = np.log(ts_dew)\nts_log.isnull().count()","acea6fd4":"ts_log.dropna(inplace=True)\nts_log.isnull().count()","087f37e9":"ts_log_diff = ts_log - ts_log.shift()\nplt.plot(ts_log_diff)","5fc5e5ff":"ts_log_diff.dropna(inplace=True)\ntest_stationarity(ts_log_diff,14)","4797b7ef":"acf_pacf_plot(ts_log_diff)","b5d66e0c":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts_log_diff)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(ts_dew, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","876b7237":"acf_pacf_plot(ts_dew.diff())","a176454c":"So above plots shows that we do have seasonality but there is no trend. Let's check for below necessary conditions:\n    * Constant mean\n    * Constant variance\n    * An auto co-variance that does not depend on time","7e0a45d7":"# <a>Exploratory Data Analysis & Visualizations<\/a>","09c28662":"There is column named **CET** in this dataset, we are going to read that as an index.","f8e60c22":"Not many values are missing, but it will still be great to fill the missing ones instead of removing entire row.","df7086c7":"For prediction we are going to use one of the most popular model for time series, **Autoregressive Integrated Moving Average (ARIMA)** which is a standard statistical model for time series forecast and analysis.\nAn ARIMA model can be understood by outlining each of its components as follows:\n* **Autoregression (AR) -** refers to a model that shows a changing variable that regresses on its own lagged, or prior, values.<br\/>\nThe notation **AR(p)** indicates an autoregressive model of order p.\n\n    *Example*\u200a\u2014\u200aIf p is 3 the predictor for X(t) will be \n        X(t) = \u00b5 + X(t-1) + X(t-2) + X(t-3) + \u03b5t\n\n    Where \u03b5 is error term.\n* **Integrated (I) -** represents the differencing of raw observations to allow for the time series to become stationary, i.e., data values are replaced by the difference between the data values and the previous values.\n* **Moving average (MA) -** incorporates the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\n    The notation **MA(q)** refers to the moving average model of order q.<br\/>\n \n\n    Example \u2014\u200aIf q is 3 the predictor for X(t) will be \n        X(t) = \u00b5 + \u03b5t + \u03b81.\u03b5(t-1) + \u03b82.\u03b5(t-2) + \u03b83.\u03b5(t-3)\n    Here instead of difference from previous term, we take errer term (\u03b5) obtained from the difference from past term.\nNow we need to figure out the values of p and q which are parameters of ARIMA model. We use below two methods to figure out these values  -\n\n**Autocorrelation Function (ACF):** It just measures the correlation between two consecutive (lagged version). example at lag 4, ACF will compare series at time instance t1\u2026t2 with series at instance t1\u20134\u2026t2\u20134\n\n**Partial Autocorrelation Function (PACF):** is used to measure the degree of association between X(t) and X(t-p).","64ea148f":"In **Dickey-Fuller test**, we need only test_statics and critical_value to know if it is stationary or not.","4ec79fe7":"**For non-stationary to stationary conversion**, we can use any of the below technique :\n* **Decomposing**\n* **Differencing**\n","9860adec":"It seems overplotted, let's plot for only two years 2015 and 2016, it will give us the clear picture of seasonality and trend.","03d721a4":"# <a>Import required libraries<\/a>","040eab8e":"*Load the dataset.*","56ba9e99":"# <a>Feature Engineering<\/a>","55d2ccca":"Split the dataset into train and test.","25d631a3":"As you can see here the first series itself is perfectly stationary, So we don't need any differencing here","9588f73a":"Differencing","5ab63308":"**Decompose**","fce9c3f2":"# <a>Check Stationarity<\/a>","1dc401b7":"Let's see how plot for all year's temprature and humidity looks like.","160884a5":"## <a>Timeseries Analysis (ARIMA Model)<\/a>","c42ae9e7":"Here we are going to consider only few of the columns which seems important from some basic EDA and time series prediction's point of view.","d193e0da":"We have constant Mean and Variance, and our **Test statistic** is less than **Critical Values**, so we already have stationary Time series. So our 'd' value will become 0 in ARIMA Model.\n\nConsider a case if it was non-stationary, in that case we would use below techniques to make it stationary\n\nMake Stationary **For non-stationary to stationary conversion**, we can use any of the below technique :\n* Decomposing\n* Differencing\n","e8e68e1b":"Here we can see that Dew Point is not stationary because the __Test Statistic__ value is greater than the __Critical Values__.","d0abd63f":"These grey dotted line are confidence intervels which we are going to use to find out the value of p and q.\n\n__p__ - *the point where PACF crosses the upper confiednce level. In our case it seems to be 2. So we will take *p = 2.\n\n__q__ - the point where ACF crosses the upper confiednce level. In our case it seems to be 2. So we will take q = 2.\n\n__d__ - number of nonseasonal differences needed for stationarity. In this case we are going to take it as 0, since this series is already stationary.\n\nNow we are going fit time series for ARIMA Models. We will compare performance on the basis of RSS score and at last prefer the best one."}}