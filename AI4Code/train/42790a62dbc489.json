{"cell_type":{"836e0c7c":"code","7b531bbb":"code","49881df4":"code","fdbb7939":"code","ca490e56":"code","59348291":"code","8ab727b3":"code","51c5fa26":"code","ac2ab272":"code","0cdcd20b":"code","b0cc5ad0":"code","84cf84dd":"code","3afdfca2":"markdown","ff7ab5b4":"markdown","bdcfee3a":"markdown","3abdee78":"markdown"},"source":{"836e0c7c":"import json\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms","7b531bbb":"!pip install efficientnet_pytorch","49881df4":"from efficientnet_pytorch import EfficientNet","fdbb7939":"\nmodel_name = 'efficientnet-b0'\nimage_size = EfficientNet.get_image_size(model_name)","ca490e56":"!wget \"https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch\/raw\/master\/examples\/simple\/img2.jpg\"","59348291":"!wget \"https:\/\/raw.githubusercontent.com\/lukemelas\/EfficientNet-PyTorch\/master\/examples\/simple\/labels_map.txt\"","8ab727b3":"img = Image.open('img2.jpg')\nimg","51c5fa26":"image_size","ac2ab272":"tfms  =  transforms.Compose([transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize([0.485, 0.456,0.406], [0.229,0.224,0.225]),])\nimg = tfms(img).unsqueeze(0)","0cdcd20b":"labels_map = json.load(open('labels_map.txt'))\nlabels_map = [labels_map[str(i)] for i in range(1000)]","b0cc5ad0":"model = EfficientNet.from_pretrained(model_name)\nmodel.eval()\n\nwith torch.no_grad():\n    logits = model(img)\n\npreds = torch.topk(logits, k=3).indices.squeeze(0).tolist()\n\nprint('-----PREDICTION-----')\nfor idx in preds:\n    label = labels_map[idx]\n    prob = torch.softmax(logits, dim=1)[0, idx].item()\n    print('{:<75}({:.2f}%)'.format(label, prob * 100))","84cf84dd":"#!tar -xvzf \"..\/input\/imagenet-object-localization-challenge\/imagenet_object_localization_patched2019.tar.gz\" ","3afdfca2":"only xml files here","ff7ab5b4":"Efficentnet repo : https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch\/blob\/master\/examples\/simple\/example.ipynb","bdcfee3a":"## INFERENCE","3abdee78":"# GETTING EFFICIENTNET"}}