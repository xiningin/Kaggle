{"cell_type":{"7e0ce5f8":"code","68639fa2":"code","5e9f6166":"code","418758d0":"code","07dbc5b4":"code","6062523b":"code","cdb549c5":"code","a071e699":"code","42114e19":"code","9f7b15bb":"code","c29fc94e":"code","ed71582e":"code","efcfb96a":"code","b67844a2":"code","a1d42d64":"code","399494f9":"markdown","a38497d2":"markdown","86e231b6":"markdown","5a75336f":"markdown","92346e7b":"markdown","6a6d9a5c":"markdown","08b84439":"markdown","e85ae363":"markdown","95599695":"markdown"},"source":{"7e0ce5f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68639fa2":"import numpy as np \nimport pandas as pd \nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom sklearn.feature_selection import chi2, SelectKBest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit","5e9f6166":"train = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')\nsample_sub = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/sample_submission.csv')\n\ntrain.head()","418758d0":"train.isnull().any()","07dbc5b4":"print('Length of train data: {}'.format(len(train)))","6062523b":"data = train.copy()\n\ngender_dist = data['Gender'].value_counts()\ndriveL_dist = data['Driving_License'].value_counts().rename(index={0:'No',1:'Yes'})\nprev_insur_dist = data['Previously_Insured'].value_counts().rename(index={0:'No',1:'Yes'})\nvehicle_damg_dist = data['Vehicle_Damage'].value_counts().rename(index={0:'No',1:'Yes'})\nvehicle_age_dist = data['Vehicle_Age'].value_counts()\nresponse_dist = data['Response'].value_counts().rename(index={0:'No',1:'Yes'})\n\n############\n\nfig = make_subplots(\n    rows=6, cols=2,\n    subplot_titles=(\"Gender\",\"Driving License\",\"Previously Insured\",\"Vehicle Damage\",\n                    \"Vehicle Age\", \"Response\", \"Annual_Premium\", \"Age\", \"Vintage\"),\n    specs=[[{}, {}],[{}, {}],[{},{}],\n           [{\"colspan\": 2}, None],\n          [{\"colspan\": 2}, None],\n          [{\"colspan\": 2}, None]],\n    horizontal_spacing=0.25,\n    vertical_spacing=0.075\n)\n\n############\n\nfig.add_trace(\n    go.Bar(x = gender_dist.index, y = gender_dist.values),\n    col=1, row=1\n)\n\nfig.add_trace(\n    go.Bar(x = driveL_dist.index, y = driveL_dist.values),\n    col=2, row=1\n)\n\nfig.add_trace(\n    go.Bar(x = prev_insur_dist.index, y = prev_insur_dist.values),\n    col=1, row=2\n)\n\nfig.add_trace(\n    go.Bar(x = vehicle_damg_dist.index, y = vehicle_damg_dist.values),\n    col=2, row=2\n)\n\nfig.add_trace(\n    go.Bar(x = response_dist.index, y = response_dist.values),\n    col=2, row=3\n)\n\nfig.add_trace(\n    go.Bar(x = vehicle_age_dist.index, y = vehicle_age_dist.values),\n    col=1, row=3\n)\n\nfig.add_trace(\n    go.Histogram(x = data['Annual_Premium']),\n    row=4, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x = data['Age']),\n    row=5, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x = data['Vintage']),\n    row=6, col=1\n)\n\n\n###########\n\nfig.update_layout(\n    showlegend=False,\n    title_text='Distributions',\n    height=2100\n)\n\nfig.show()","cdb549c5":"data = train.query('Response == 1').copy()\n\ngender_dist = data['Gender'].value_counts()\ndriveL_dist = data['Driving_License'].value_counts().rename(index={0:'No',1:'Yes'})\nprev_insur_dist = data['Previously_Insured'].value_counts().rename(index={0:'No',1:'Yes'})\nvehicle_damg_dist = data['Vehicle_Damage'].value_counts().rename(index={0:'No',1:'Yes'})\nvehicle_age_dist = data['Vehicle_Age'].value_counts()\nresponse_dist = data['Response'].value_counts().rename(index={0:'No',1:'Yes'})\n\n############\n\nfig = make_subplots(\n    rows=6, cols=2,\n    subplot_titles=(\"Gender\",\"Driving License\",\"Previously Insured\",\"Vehicle Damage\",\n                    \"Vehicle Age\", \"Response\", \"Annual_Premium\", \"Age\", \"Vintage\"),\n    specs=[[{}, {}],[{}, {}],[{},{}],\n           [{\"colspan\": 2}, None],\n          [{\"colspan\": 2}, None],\n          [{\"colspan\": 2}, None]],\n    horizontal_spacing=0.25,\n    vertical_spacing=0.075\n)\n\n############\n\nfig.add_trace(\n    go.Bar(x = gender_dist.index, y = gender_dist.values),\n    col=1, row=1\n)\n\nfig.add_trace(\n    go.Bar(x = driveL_dist.index, y = driveL_dist.values),\n    col=2, row=1\n)\n\nfig.add_trace(\n    go.Bar(x = prev_insur_dist.index, y = prev_insur_dist.values),\n    col=1, row=2\n)\n\nfig.add_trace(\n    go.Bar(x = vehicle_damg_dist.index, y = vehicle_damg_dist.values),\n    col=2, row=2\n)\n\nfig.add_trace(\n    go.Bar(x = response_dist.index, y = response_dist.values),\n    col=2, row=3\n)\n\nfig.add_trace(\n    go.Bar(x = vehicle_age_dist.index, y = vehicle_age_dist.values),\n    col=1, row=3\n)\n\nfig.add_trace(\n    go.Histogram(x = data['Annual_Premium']),\n    row=4, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x = data['Age']),\n    row=5, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x = data['Vintage']),\n    row=6, col=1\n)\n\n\n###########\n\nfig.update_layout(\n    showlegend=False,\n    title_text='Distributions',\n    height=2100\n)\n\nfig.show()","a071e699":"data = train.query('Response == 0').copy()\n\ngender_dist = data['Gender'].value_counts()\ndriveL_dist = data['Driving_License'].value_counts().rename(index={0:'No',1:'Yes'})\nprev_insur_dist = data['Previously_Insured'].value_counts().rename(index={0:'No',1:'Yes'})\nvehicle_damg_dist = data['Vehicle_Damage'].value_counts().rename(index={0:'No',1:'Yes'})\nvehicle_age_dist = data['Vehicle_Age'].value_counts()\nresponse_dist = data['Response'].value_counts().rename(index={0:'No',1:'Yes'})\n\n############\n\nfig = make_subplots(\n    rows=6, cols=2,\n    subplot_titles=(\"Gender\",\"Driving License\",\"Previously Insured\",\"Vehicle Damage\",\n                    \"Vehicle Age\", \"Response\", \"Annual_Premium\", \"Age\", \"Vintage\"),\n    specs=[[{}, {}],[{}, {}],[{},{}],\n           [{\"colspan\": 2}, None],\n          [{\"colspan\": 2}, None],\n          [{\"colspan\": 2}, None]],\n    horizontal_spacing=0.25,\n    vertical_spacing=0.075\n)\n\n############\n\nfig.add_trace(\n    go.Bar(x = gender_dist.index, y = gender_dist.values),\n    col=1, row=1\n)\n\nfig.add_trace(\n    go.Bar(x = driveL_dist.index, y = driveL_dist.values),\n    col=2, row=1\n)\n\nfig.add_trace(\n    go.Bar(x = prev_insur_dist.index, y = prev_insur_dist.values),\n    col=1, row=2\n)\n\nfig.add_trace(\n    go.Bar(x = vehicle_damg_dist.index, y = vehicle_damg_dist.values),\n    col=2, row=2\n)\n\nfig.add_trace(\n    go.Bar(x = response_dist.index, y = response_dist.values),\n    col=2, row=3\n)\n\nfig.add_trace(\n    go.Bar(x = vehicle_age_dist.index, y = vehicle_age_dist.values),\n    col=1, row=3\n)\n\nfig.add_trace(\n    go.Histogram(x = data['Annual_Premium']),\n    row=4, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x = data['Age']),\n    row=5, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x = data['Vintage']),\n    row=6, col=1\n)\n\n\n###########\n\nfig.update_layout(\n    showlegend=False,\n    title_text='Distributions',\n    height=2100\n)\n\nfig.show()","42114e19":"def normalize_Data(dfs, Scaler , cols=None):\n    train, test = dfs\n    \n    all_cols = train.columns\n    \n    if not cols: \n        selected_cols = train.columns\n        temp_train = train\n        temp_test = test\n    else:\n        ## Separating columns to be standardize from others\n        selected_cols = cols\n        temp_train = train[selected_cols].copy()\n        temp_test = test[selected_cols].copy()\n        train = train.drop(selected_cols,axis=1)\n        test = test.drop(selected_cols,axis=1)\n    \n    \n    ## Scaling\n    scaler = Scaler()\n    \n    temp_train = pd.DataFrame(\n        scaler.fit_transform(temp_train),\n        columns=selected_cols\n    )\n    \n    temp_test = pd.DataFrame(\n        scaler.transform(temp_test),\n        columns=selected_cols\n    )\n    \n    if not cols:\n        return (temp_train, temp_test)\n    else:\n        return (\n            pd.concat([temp_train,train], axis=1)[all_cols],\n            pd.concat([temp_test,test], axis=1)[all_cols]\n        )\n\ndef feature_selector(score_func, k, X, y):\n    \n    feature_selector = SelectKBest(score_func, k=k)\n    feature_selector.fit_transform(X,y)\n    features = feature_selector.get_support()\n        \n    return [b for a, b in zip(features, X.columns) if a]","9f7b15bb":"## Categorical and numeric features respectively\ncatg_features = ['Gender','Driving_License','Previously_Insured','Vehicle_Age','Vehicle_Damage']\nnumeric_features = ['Age', 'Region_Code','Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\n\ntrain_X = train.copy()\ntest_X = test.copy()\n\ndel train_X['id']\ndel test_X['id']\n\ntrain_y = train_X.pop(\"Response\").values","c29fc94e":"encodings = {\"Vehicle_Age\": {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2},\n             \"Gender\": {'Male': 1, 'Female': 0},\n             \"Vehicle_Damage\": {'Yes': 1, 'No': 0}\n            }\n\ntrain_X.replace(encodings, inplace=True)\ntest_X.replace(encodings, inplace=True)\n\ntrain_X.head()","ed71582e":"train_X, test_X = normalize_Data(\n    (train_X, test_X), Scaler=StandardScaler, cols=numeric_features\n)\n\ntrain_X.head()","efcfb96a":"from sklearn.metrics import accuracy_score, log_loss, f1_score, precision_score, recall_score, roc_auc_score\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import PassiveAggressiveClassifier\n\nclassifiers = [\n    KNeighborsClassifier(5),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=125),\n    LogisticRegression(),\n    PassiveAggressiveClassifier(max_iter=2500),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    GaussianNB()\n]\n\ndef precision(x,y):\n    return precision_score(x,y,zero_division=0)\n    \ndef recall(x,y):\n    return recall_score(x,y,zero_division=0)\n\nmetrics = [\n    accuracy_score,\n    log_loss,\n    f1_score,\n    precision,\n    recall,\n    roc_auc_score\n]","b67844a2":"SPLITS = 3\n\nsss = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.2)\n\nreport = pd.DataFrame(\n    data = 0,\n    index = [x.__class__.__name__ for x in classifiers],\n    columns = [x.__name__ for x in metrics]\n)\n\nk=1\n\nfor train_index, test_index in sss.split(train_X.values, train_y):\n\n    print('Fold:{} is running...'.format(k))\n    X_train, X_test = train_X.values[train_index], train_X.values[test_index]\n    y_train, y_test = train_y[train_index], train_y[test_index]\n    \n    print('  Classifiers:'.format(k))\n    for clf in classifiers:\n        print('     {}...'.format(clf.__class__.__name__))\n        clf.fit(X_train, y_train)\n        pred = clf.predict(X_test)\n\n        for metric in metrics:\n            score = metric(y_test, pred)\n            report.loc[\n                clf.__class__.__name__, metric.__name__\n            ] += score       \n    \n    print()\n    k+=1\n            \nreport = report.applymap(lambda x: x\/SPLITS)","a1d42d64":"report","399494f9":"# Normalizing Data","a38497d2":"# Distributions for interested Customers","86e231b6":"# Modeling","5a75336f":"# Helper Functions","92346e7b":"## In Progress...","6a6d9a5c":"# 1.Distributions","08b84439":"# Encoding Features","e85ae363":"# Health Insurance Cross Sell Prediction \ud83c\udfe0 \ud83c\udfe5","95599695":"# Distributions for not interested Customers"}}