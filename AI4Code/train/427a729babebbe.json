{"cell_type":{"582fe4a1":"code","055cd3fa":"code","5e9bcc08":"code","8e5cfbb1":"code","257363d5":"code","627824b2":"code","4111b1b6":"code","1df5dc2a":"code","9aa30dbf":"code","628fe9a7":"code","d69ef9d7":"code","0378add9":"code","6f8c5464":"code","30195c33":"code","7859f242":"code","7c14842b":"code","18af7d15":"code","70659fe2":"code","6a6dacc8":"code","ac64be35":"code","1ccd0744":"code","9faed437":"code","9a131142":"code","0acf31df":"code","dfddfb3c":"code","ff6dde37":"code","81d156bc":"code","d24db1cf":"code","e1385d31":"code","615a1c69":"code","c2453154":"code","0651f477":"code","0dd8b1ab":"code","7dc5fe81":"code","6226eba9":"code","f3b27241":"code","292421e8":"code","195e439b":"code","568ab966":"code","226ea789":"code","197ef040":"code","c3e30df4":"code","5e123234":"code","633d0371":"code","f380282c":"code","3e2ed43d":"code","51ddfe4d":"code","bd89c470":"code","5bd5aa13":"markdown","614b92cd":"markdown","4979062d":"markdown","1f827f34":"markdown","9e277119":"markdown","4db9907d":"markdown","fd2bbad9":"markdown","94e243e1":"markdown","e20fd374":"markdown","77e1178e":"markdown","222e7b7f":"markdown","b7f59ba4":"markdown","b959af69":"markdown","7491d9fe":"markdown","da697f05":"markdown","45a0c8cc":"markdown","c8d6d476":"markdown","28512137":"markdown","5aed0bc0":"markdown","f2fee4d3":"markdown","70210f2b":"markdown","26206929":"markdown","bc48bd45":"markdown","448d9e87":"markdown","1658aa9d":"markdown","e5a41fce":"markdown","468f69b9":"markdown"},"source":{"582fe4a1":"# This Python 3 environment comes with many helpful analytics libraries installed\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Plot\nimport seaborn as sns\n\n#Nullity matrix is a data-dense display which lets you quickly visually \n#pick out patterns in data completion \nimport missingno as msno \n\n#All of the statistics functions are located in the sub-package scipy.stats\nfrom scipy import stats     \nfrom scipy.stats import norm\n\n#plot mathematical libraries\nimport matplotlib.pyplot as plt \nsns.set(style=\"whitegrid\", color_codes=True) \nsns.set(font_scale=1)\n\n#Ignoring warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.set(font_scale=1)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","055cd3fa":"# Loading Data\ntrain_f= '\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv'\ntest_f ='\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv'\n#subm_df ='\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv'\n\ndf_train= pd.read_csv(train_f)\ndf_test= pd.read_csv(test_f)\n","5e9bcc08":"# Columns of df_train correspond to the features of the dataset\ndf_train.columns","8e5cfbb1":"#Descriptions of the data\ndf_train.describe().transpose()","257363d5":"# Shape of the data\ndf_train.shape ","627824b2":"#Info on dataTrain\ndf_train.info() ","4111b1b6":"df_train['SalePrice'].describe()","1df5dc2a":"#Plotting dataTrain SalePrice\nsns.distplot(df_train['SalePrice'], kde=True, fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( 'mu = {:.2f} and sigma = {:.2f}'.format(mu, sigma))\n\n#Plotting the distribution over\nplt.legend(['Normal distribition ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","9aa30dbf":"# Skewness and kurtosis\nprint(\"Skewness: %4f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %4f\" % df_train['SalePrice'].kurt())","628fe9a7":"numColumns = df_train.columns[df_train.dtypes!=object]\nfor column in numColumns:\n    if df_train['SalePrice'].corr(df_train[column]) > 0.5:\n        print('Correlation SalePrice -',column, '=' ,df_train['SalePrice'].corr(df_train[column]))","d69ef9d7":"#Heatmap for the saleprice correlation matrix\n\nmatcorr=df_train[[\"SalePrice\",\"OverallQual\",\"YearBuilt\",\"YearRemodAdd\",\n                        \"TotalBsmtSF\",\"GrLivArea\",\"1stFlrSF\",\"FullBath\",\n                            \"TotRmsAbvGrd\",\"GarageArea\",\"GarageCars\",'MSZoning']].corr()\n\nsns.set(font_scale=1.0)\nplt.figure(figsize=(10, 8))\nsns.heatmap(matcorr, vmax=1.0, square=True, annot=True, linewidths=0.1,cmap='plasma',linecolor=\"white\")\nplt.title('Correlation matrix');\n","0378add9":"#GrLivArea\/SalePrice plot\nsns.lmplot(x='GrLivArea',y='SalePrice',data=df_train)","6f8c5464":"# GarageArea\/SalePrice plot\nsns.lmplot(x='GarageArea',y='SalePrice',data=df_train)","30195c33":"# TotalBsmtSF\/SalePrice plot\nsns.lmplot(x='TotalBsmtSF',y='SalePrice',data=df_train) ","7859f242":"# SalePrice\/1stFlrSF plot\nvar = '1stFlrSF'\nsns.lmplot(x='1stFlrSF',y='SalePrice',data=df_train) \nprint(type(var))","7c14842b":"#Box plot OverallQual\/SalePrice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(10, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","18af7d15":"var = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(22, 12))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=700000);\nplt.xticks(rotation=90);","70659fe2":"#GarageCars\nvar = 'GarageCars'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(10, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=600000);\nprint(type(var))","6a6dacc8":"#Visualization of Missing Values: white lines denote the presence of missing value\nsns.set(font_scale=1)\nplt.figure(figsize=(16, 7))\nsns.heatmap(df_train.isnull(), square=False, annot=False, yticklabels=False, cmap='bone')\nplt.title('TRAINING DATASET');","ac64be35":"#Visualization of Missing Values: white lines denote the presence of missing value\nsns.set(font_scale=1)\nplt.figure(figsize=(16, 7))\nsns.heatmap(df_test.isnull(), square=False, annot=False, yticklabels=False, cmap='bone')\nplt.title('TEST DATASET');","1ccd0744":"#Dealing with missing data\ntrain_labels = df_train['SalePrice'].reset_index(drop=True)\ntf = df_train.drop(columns=['SalePrice'], axis=1)\nts = df_test\n\ndf_all= pd.concat([tf, ts]).reset_index(drop=True)\n\nprint('------COLUMN WITH MISSING VALUES------')\nfor column in df_all:\n    if df_all[column].isnull().any():\n       print('{0}: {1} NaN  {2}'.format(column, df_all[column].isnull().sum(), df_all[column].dtypes))","9faed437":"#Deleting features with too much missing values\ndf_all.drop(['Alley'], axis=1,inplace = True)\ndf_all.drop(['PoolQC'], axis=1,inplace = True)\ndf_all.drop(['Fence'], axis=1,inplace = True)\ndf_all.drop(['MiscFeature'], axis=1,inplace = True)\ndf_all.drop(['FireplaceQu'], axis=1,inplace = True)\ndf_all.drop(['LotFrontage'], axis=1,inplace = True)","9a131142":"#df_all['GarageType'].value_counts()\n#df_all['GarageFinish'].value_counts()\n#df_all['GarageQual'].value_counts()\n#df_all['GarageCond'].value_counts()\n#df_all['MasVnrType'].value_counts()\n#df_all['BsmtQual'].value_counts()\n#df_all['BsmtCond'].value_counts()\n#df_all['BsmtExposure'].value_counts()\n#df_all['BsmtFinType1'].value_counts()\n#df_all['BsmtFinType2'].value_counts()\n#df_all['Electrical'].value_counts()\n#df_all['GarageYrBlt'].value_counts()","0acf31df":"#Substitution function for missing values\ndef substitution(column, value):\n    df_all.loc[df_all[column].isnull(),column] = value","dfddfb3c":"#categorical features\nsubstitution('MasVnrType', 'None')\nsubstitution('BsmtQual', 'NA')\nsubstitution('BsmtCond', 'NA')\nsubstitution('BsmtExposure', 'NA') # No basement for that houses\nsubstitution('BsmtFinType1', 'NA')\nsubstitution('BsmtFinType2', 'NA')\nsubstitution('Electrical', 'SBrkr')\nsubstitution('GarageType', 'NA ')\nsubstitution('GarageFinish', 'NA')\nsubstitution('GarageQual', 'NA')\nsubstitution('GarageCond', 'NA')\nsubstitution('GarageYrBlt', 'None')\nsubstitution('MSZoning','RL')\nsubstitution('Utilities','AllPub')\nsubstitution('Exterior1st','VinylSd')\nsubstitution('Exterior2nd','VinylSd')\nsubstitution('KitchenQual','TA')\nsubstitution('Functional','Typ')\nsubstitution('SaleType','WD')","ff6dde37":"#These are the numerical features so it's better to fill the blank values with the mean\ndf_all['MasVnrArea'] = df_all['MasVnrArea'].fillna(df_all['MasVnrArea'].mean())\ndf_all['BsmtFinSF2'] = df_all['BsmtFinSF2'].fillna(df_all['BsmtFinSF2'].mean())\ndf_all['BsmtFinSF1'] = df_all['BsmtFinSF1'].fillna(df_all['BsmtFinSF1'].mean())\ndf_all['TotalBsmtSF'] = df_all['TotalBsmtSF'].fillna(df_all['TotalBsmtSF'].mean())\ndf_all['BsmtUnfSF'] = df_all['BsmtUnfSF'].fillna(df_all['BsmtUnfSF'].mean())\ndf_all['BsmtFullBath'] = df_all['BsmtFullBath'].fillna(df_all['BsmtFullBath'].mean())\ndf_all['BsmtHalfBath'] = df_all['BsmtFinSF2'].fillna(df_all['BsmtHalfBath'].mean())\ndf_all['GarageCars'] = df_all['GarageCars'].fillna(df_all['GarageCars'].mean())\ndf_all['GarageArea'] = df_all['GarageArea'].fillna(df_all['GarageArea'].mean())","81d156bc":"print('------COLUMN WITH MISSING VALUES------')\nfor column in df_all:\n    if df_all[column].isnull().any():\n       print('{0}: {1} NaN  {2}'.format(column, df_all[column].isnull().sum(), df_all[column].dtypes))","d24db1cf":"# These features are interpreted as numerical features but actually are categorical\ndf_all['GarageYrBlt'] = df_all['GarageYrBlt'].astype(str)\ndf_all['YrSold'] = df_all['YrSold'].astype(str)\ndf_all['MoSold'] = df_all['MoSold'].astype(str)\ndf_all['OverallCond'] = df_all['OverallCond'].astype(str)","e1385d31":"#normalizing data with LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\nobjColumns =df_all.columns[df_all.dtypes==object]\nlbl = LabelEncoder()\nfor c in objColumns:\n    lbl.fit(list(df_all[c].values)) \n    df_all[c] = lbl.transform(list(df_all[c].values))","615a1c69":"#Splitting the dataset df_all in the two originals datasets\n\n#### TRAIN DATASET ####\nX_train = df_all.iloc[:len(train_labels), :]\n\n#Test Dataset\nX_test = df_all.iloc[len(train_labels):, :]\n\nX_train.shape, X_test.shape","c2453154":"X_train.shape","0651f477":"var= 'SalePrice'\nX_Train=pd.concat([df_train[var],X_train], axis=1)\nX_Train.shape","0dd8b1ab":"#bivariate analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([X_Train['SalePrice'], X_Train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","7dc5fe81":"#deleting points\n#X_Train.sort_values(by = 'GrLivArea', ascending = False)[:2]\n#X_Train = X_Train.drop(X_Train[X_Train['Id'] == 1299].index)\n#X_Train = X_Train.drop(X_Train[X_Train['Id'] == 524].index)","6226eba9":"#bivariate analysis saleprice\/totalBsmtSF\nvar = 'TotalBsmtSF'\ndata = pd.concat([X_Train['SalePrice'], X_Train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","f3b27241":"#bivariate analysis saleprice\/1stFlrSF\nvar = '1stFlrSF'\ndata = pd.concat([X_Train['SalePrice'], X_Train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","292421e8":"#bivariate analysis saleprice\/GarageArea\nvar = 'GarageArea'\ndata = pd.concat([X_Train['SalePrice'], X_Train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","195e439b":"X_Train.columns","568ab966":"#My target\ntarget_train= train_labels\n\n# Making sure that X_Train model and my target has the same shape\nX_train.shape, target_train.shape, X_test.shape","226ea789":"#Cross Validation through K-Fold\nfrom sklearn.model_selection import KFold, cross_val_score\nN_splits=10\nkf = KFold(n_splits=10)\nkf.get_n_splits(tf)","197ef040":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor","c3e30df4":"#Linear Regression\nmymodel_Lin= LinearRegression()\nrmse_Lin= cross_val_score(mymodel_Lin, X_train, target_train, cv=kf)    \nmean_Lin= np.mean(rmse_Lin)\nprint(mean_Lin)","5e123234":"#RandomForest Regression\nmymodel_RF = RandomForestRegressor(n_estimators=100, n_jobs=-1)\nrmse_RF= cross_val_score(mymodel_RF, X_train, target_train, cv=kf)   \nmean_RF= np.mean(rmse_RF)\nprint(mean_RF)","633d0371":"#DecsionTree Regressor model\nmymodel_Tree= DecisionTreeRegressor()\nrmse_Tree= cross_val_score(mymodel_Tree, X_train, target_train, cv=kf)   \nmean_Tree= np.mean(rmse_Tree)\nprint(mean_Tree)","f380282c":"#GradientBoosted Trees model\nmymodel_GBR = GradientBoostingRegressor()\nrmse_GBR= cross_val_score(mymodel_GBR, X_train, target_train, cv=kf)   \nmean_GBR= np.mean(rmse_GBR)\nprint(mean_GBR)","3e2ed43d":"#I'll take the best model\nmymodel_GBR.fit(X_train,target_train)\ny_pred= mymodel_GBR.predict(X_test)","51ddfe4d":"t_pred = pd.DataFrame(X_test, columns=['Id'])\ny_pred = pd.DataFrame(y_pred, columns=['SalePrice'])\nsubmit = pd.DataFrame(zip(t_pred['Id'],y_pred['SalePrice']),columns=['Id','SalePrice'])\nsubmit.to_csv('submission.csv', index=False)","bd89c470":"submit.head(1460)","5bd5aa13":"# **Cross Validation**","614b92cd":"Outliers is something to be cut out. Because outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.","4979062d":"Just for now we can conclude that:\n\n* 'GrLivArea' and 'TotalBsmtSF' has a very linearly relation with 'SalePrice'. \n* All the relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.\n* 'OverallQual' and 'YearBuilt' and also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.\n* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. It'll never be possible to distinguish them.\n* 'TotalBsmtSF' and '1stFlrSF' are quite similar thing too.\n\n\nIn conclusion, we can afferm that the features that in our correlation heatmap seems to be very tied, also in this analysis the relationships is confirmed. ","1f827f34":"Asymmetrical index (Skewness) is positive : 1.88; that's means I have an uneven asymmetry, that is coherent with the graphic above.\nKurtosis index K indeed is 6.5. If K > 0 then it's a leptocurtic curve, so, more pointed respect with the gaussian distribution","9e277119":"# **Correlation matrix**","4db9907d":"More knowledge of most correlated features are in statistics studies. \n\nThe features, as seen before, have differents datatypes. So the analysis is different for numerical and categorical features.","fd2bbad9":"I can feel tempted to eliminate some observations (e.g. TotalBsmtSF > 3000) but I suppose it's not worth it. I can live with that.","94e243e1":"# Data exploration","e20fd374":"Cross validation is used because in machine learning it's needed some kind of assurance that the model I have built has got most of the patterns from the data correct, and its not picking up too much on the noise, or in other words its low on bias and variance.\n\nThe reason you need different data for training and evaluating the model is to protect against overfitting. There are other (slightly more involved) cross-validation techniques, of course, like k-fold cross-validation, which often used in practice. Which is the one I am going to use. \n\nIn K Fold cross validation, the data is divided into k subsets. Now the holdout method is repeated k times, such that each time, one of the k subsets is used as the test set\/ validation set and the other k-1 subsets are put together to form a training set. The error estimation is averaged over all k trials to get total effectiveness of our model. As can be seen, every data point gets to be in a validation set exactly once, and gets to be in a training set k-1 times. This significantly reduces bias as we are using most of the data for fitting, and also significantly reduces variance as most of the data is also being used in validation set. Interchanging the training and test sets also adds to the effectiveness of this method. As a general rule and empirical evidence, K = 5 or 10 is generally preferred, but nothing\u2019s fixed and it can take any value.\n\nIn this scenario, you have several models, each with a different combination of hyper-parameters. Each of these combinations of parameters, which correspond to a single model, can be said to lie on a point of a \"grid\". The goal is then to train each of these models and evaluate them e.g. using cross-validation. Then, I will select the one that performed best.\n\nGrid search is a method to perform hyper-parameter optimization, that is, it is a method to find the best combination of hyper-parameters (an example of an hyper-parameter is the learning rate of the optimiser), for a given model (e.g. a CNN) and test dataset. ","77e1178e":"# **Categorical Features**","222e7b7f":"# **Outliers**","b7f59ba4":"# **Numerical Features**","b959af69":"It was quite expected, because with '.info()' I've already seen that some values was missed. \n\n\nBut the traning dataset is not the only one that I can consider. Indeed, I have just explored that also in the test dataset there are some features with missing values. \nFor better prediction, it is convenient to use a model with both datasets.","7491d9fe":"Now I'll plot the result in a correlation HEATMAP to visualize it better.","da697f05":"Analyzing these results I can see that there are few rows whit a lot of missing values. That's why I'll delete the entires columns from the dataset.The point is that I won't miss these data because none of these variables seem to be very important. For example, MiscFeature has 1406 NaN values, so I'll surely delete this feature.","45a0c8cc":"Once the most 'white' features are deleted, the restance are distinguished between features with all the data and features which columns have some missing values. These latest can be treated in two ways:\n* the empty values can be filled by the median (the most frequence values in the columns)\n* or by the mean of the columns.\n\nI'll start exploring these features using the function \"value_counts()\". The resulting object will be in descending order so that the first element is the most frequently-occurring element. Excludes NA values by default.","c8d6d476":"Going on with the analysis I have to take in account the probability of missing values. To have an idea about the quantities of them, I can use the *missingno* libraries. \n\n\n\n\n> #### What I found out\n> msno.bar(dataTrain, figsize=(10,9), fontsize=(1)) \n> non si pu\u00f2 usare msno.bar per questo dataset perch\u00e8 i dati sono troppi e il grafico non ha abbastanza labels.\n> Error: The number of FixedLocator locations (0), \n> usually from a call to set_ticks, does not match the number of ticklabels (81).\n\n> msno.matrix(dataTrain) \n> ho provato ad usare anche la matrice che da dei risultati.\n> il problema \u00e8 non riesce a plottare i nomi delle feutures come invece dovrebbe fare \n>  Probabilmente perch\u00e8 il dataset \u00e8 troppo grande e collima con lo spazio a disposizione su kaggle.\n\n\n\nSo, I 'll reuse the *sns.heatmap* as before. I'll change only the color of the map.","28512137":"What has been revealed:\n\n* The two values with bigger 'GrLivArea' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, I'll define them as outliers and delete them.\n\n* The two observations in the top of the plot are observations that I should be careful about. They look like two special cases, however they seem to be following the trend. For that reason, I will keep them.","5aed0bc0":"Now, I'll replace the frequently value in the place of the missing values, thanks to the new function \"Substitution\" that I created here down. ","f2fee4d3":"I have just founding out the features contained in the dataset. The Id collumn is useless and so I'll drop it from both the datsets. Then, I'll go in a statistical description of the features, just to have an idea about their distribution. ","70210f2b":"Also here,I can eliminate some observations (e.g. 1stFlrSF > 4000) but it seems that they not affect in a relevant way the distribution. I can live with that, so I can keep them.","26206929":"That's the moment to see the correlation between SalePrice and the other features. Because of the elevated numbers features, I'll check only the ones that have a correlation with SalePrice > 0.5: ","bc48bd45":"# Missing values ","448d9e87":"I notice that the data don't have all the same type, some are categorical variables and other are numerical variables.To be precise I have 3 float variables, 35 integer variables and 43 categorical variables.","1658aa9d":"Coefficients for distribution are Skewness and Kurtosis:","e5a41fce":"The same as above.","468f69b9":"# The models"}}