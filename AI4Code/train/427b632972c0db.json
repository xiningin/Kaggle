{"cell_type":{"d5d679a1":"code","c519dc61":"code","472c4430":"code","b503947b":"code","770c8e28":"code","21c768f1":"code","1ac7f0fa":"code","40595bde":"code","4367ef41":"code","62737d24":"code","e153de16":"code","48f01dd2":"code","68935601":"code","ddb2b207":"code","1bfc16d2":"code","e3163e35":"code","9a634679":"code","31c02b52":"code","ac1063e7":"code","ef28ddae":"code","9c5746b1":"code","c9f60ab2":"code","a54cb871":"markdown","e8f2dedc":"markdown","187f3f1a":"markdown","3161de3d":"markdown","7275e0e8":"markdown","0b5fd82f":"markdown","c60f9a43":"markdown","6f20e044":"markdown","a7675b7f":"markdown"},"source":{"d5d679a1":"import numpy as np\nimport pandas as pd\n\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsample_sub = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","c519dc61":"# Train\nprint(f'Train_df_shape : {train_df.shape}\\n')\nprint(f'{train_df.dtypes} \\n')\ndisplay(train_df.head())","472c4430":"#Test\nprint(f'Test_df_shape : {test_df.shape}\\n')\nprint(f'{test_df.dtypes} \\n')\ndisplay(test_df.head())","b503947b":"# Sample_sub\nprint(f'sample_sub_shape : {sample_sub.shape}\\n')\nprint(f'{sample_sub.dtypes} \\n')\ndisplay(sample_sub.head())","770c8e28":"# Check statistics\n# Train_data\nprint('-'*10 + ' Train ' + '-'*10)\ntrain_df = train_df.astype(\n    {\n        'PassengerId' : str,\n        'Pclass' : str \n        }\n    )\ndisplay(train_df.describe())\ndisplay(train_df.describe(exclude='number'))\n\n# Test data\nprint('-'*10 + ' Test ' + '-'*10)\ntest_df = test_df.astype(\n    {\n        'PassengerId' : str,\n        'Pclass' : str \n        }\n    )\ndisplay(test_df.describe())\ndisplay(test_df.describe(exclude='number'))\n\n# All_data\nprint('-'*10 + ' All ' + '-'*10)\nall_df = pd.concat([train_df,test_df],axis=0).reset_index(drop=True)\nall_df['Test_Flag'] = 0\nall_df.loc[train_df.shape[0]: , 'Test_Flag'] = 1\n\ndisplay(all_df.describe())","21c768f1":"# Check for duplicates\nimport matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\n\nfig ,axes = plt.subplots(figsize=(8,8),ncols=3,nrows=2)\n\nfor col_name,ax in zip(\n    ['Name','Pclass','Cabin','Sex','Ticket','Embarked']\n    ,axes.ravel()\n    ):\n    venn2(\n        subsets=(set(train_df[col_name].unique()), set(test_df[col_name].unique())),\n        set_labels=('Train', 'Test'),\n        ax=ax\n    )\n    ax.set_title(col_name)\n    ","1ac7f0fa":"# Check the distribution of data\nimport seaborn as sns\n\nsns.countplot(x='Survived', data=train_df) \nplt.show()\n\nsns.countplot(x='Sex', hue='Test_Flag', data=all_df) \nplt.show()\n\nfig = sns.FacetGrid(all_df, col='Test_Flag', hue='Test_Flag', height=4)\nfig.map(sns.histplot, 'Age', bins=30, kde=False)\nplt.show()\n\nfig = sns.FacetGrid(all_df, col='Test_Flag', hue='Test_Flag', height=4)\nfig.map(sns.histplot, 'Fare', bins=30, kde=False)\nplt.show()\n\nsns.countplot(x='SibSp',hue='Test_Flag', data=all_df)\nplt.show()\n\nsns.countplot(x='Parch',hue='Test_Flag', data=all_df)\nplt.legend(title='Test_Flag' ,loc='upper right')\nplt.show()","40595bde":"sns.heatmap(\n    train_df[['Survived','Age','SibSp','Parch','Fare']].corr(),\n    vmax=1,vmin=-1,annot=True\n    )\nplt.show()\n\nsns.countplot(x='Sex', hue='Survived', data=train_df) \nplt.show()\n\nsns.countplot(x='Pclass', hue='Survived', data=train_df) \nplt.show()\n\nsns.countplot(x='Embarked', hue='Survived', data=train_df) \nplt.show()\n\nfig = sns.FacetGrid(train_df, col='Survived', hue='Survived', height=4)\nfig.map(sns.histplot, 'Age', bins=30, kde=False)\nplt.show()\n\nfig = sns.FacetGrid(train_df, col='Survived', hue='Survived', height=4)\nfig.map(sns.histplot, 'Fare', bins=25, kde=False)\nplt.show()\n\nsns.countplot(x='SibSp',hue='Survived', data=train_df)\nplt.legend(title='Survived', loc='upper right')\nplt.show()\n\nsns.countplot(x='Parch',hue='Survived', data=train_df)\nplt.legend(title='Survived' ,loc='upper right')\nplt.show()","4367ef41":"all_df['Age'] =  all_df['Age'].fillna( all_df['Age'].median())\nall_df['Fare'] =  all_df['Fare'].fillna( all_df['Fare'].median())\nall_df['Embarked'] =  all_df['Embarked'].fillna('NaN')\nall_df['FareBand'] = pd.qcut(all_df['Fare'], 4)\nall_df['AgeBand'] = pd.qcut(all_df['Age'], 4)\nall_df = pd.get_dummies(all_df, columns= [\"Sex\", \"Pclass\"])\nall_df = pd.get_dummies(all_df, columns=['AgeBand','FareBand','Embarked'])\n\nfrom sklearn.model_selection import train_test_split\n\ntrain = all_df[all_df['Test_Flag']==0]\ntest = all_df[all_df['Test_Flag']==1].reset_index(drop=True)\ntarget = train['Survived']\n\ndrop_col = [\n    'PassengerId','Age', \n    'Ticket', 'Fare','Cabin',\n    'Test_Flag','Name','Survived'\n    ]\n\ntrain = train.drop(drop_col, axis=1)\ntest = test.drop(drop_col, axis=1)\n\nX_train ,X_val ,y_train ,y_val = train_test_split(\n    train, target, \n    test_size=0.2,random_state=0\n    )\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression() \nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_train)\nprint(accuracy_score(y_train, y_pred))\n\ny_pred_val = model.predict(X_val)\nprint(accuracy_score(y_val, y_pred_val))\n\ntest_pred = model.predict(test)\nsample_sub[\"Survived\"] = np.where(test_pred>=0.5, 1, 0)\ndisplay(sample_sub.head(10))\n\nsample_sub.to_csv(\"submission.csv\", index=False)","62737d24":"print('-'*10 + ' Train ' + '-'*10)\ntrain_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\nsns.countplot(x='FamilySize',hue='Survived', data=train_df)\nplt.legend(title='Survived' ,loc='upper right')\nplt.show()\n\ndisplay(train_df['FamilySize'].value_counts(ascending=False,normalize=True))\ndisplay(pd.crosstab(train_df['FamilySize'], train_df['Survived'], normalize='index'))\n\nprint('-'*10 + ' All ' + '-'*10)\nall_df = pd.concat([train_df,test_df],axis=0).reset_index(drop=True)\nall_df['Test_Flag'] = 0\nall_df.loc[train_df.shape[0]: , 'Test_Flag'] = 1\nall_df['FamilySize'] = all_df['SibSp'] + all_df['Parch'] + 1\n\ndisplay(all_df['FamilySize'].value_counts(ascending=False,normalize=True))","e153de16":"train_df['Alone'] = train_df['FamilySize'].map(lambda s: 1 if  s == 1  else 0)\n\nsns.countplot(x='Alone',hue='Survived', data=train_df)\nplt.legend(title='Survived' ,loc='upper right')\nplt.show()\n\ndisplay(pd.crosstab(train_df['Alone'], train_df['Survived'], normalize='index'))","48f01dd2":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsample_sub = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n\nall_df = pd.concat([train_df,test_df],axis=0).reset_index(drop=True)\nall_df['Test_Flag'] = 0\nall_df.loc[train_df.shape[0]: , 'Test_Flag'] = 1\n\nall_df['Age'] =  all_df['Age'].fillna( all_df['Age'].median())\nall_df['Fare'] =  all_df['Fare'].fillna( all_df['Fare'].median())\nall_df['Embarked'] =  all_df['Embarked'].fillna('NaN')\nall_df['FareBand'] = pd.qcut(all_df['Fare'], 4)\nall_df['AgeBand'] = pd.qcut(all_df['Age'], 4)\nall_df['FamilySize'] = all_df['SibSp'] + all_df['Parch'] + 1\nall_df['MedF']   = all_df['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\nall_df['LargeF'] = all_df['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\nall_df['Alone'] = all_df['FamilySize'].map(lambda s: 1 if  s == 1  else 0)\n\nall_df = pd.get_dummies(all_df, columns= [\"Sex\", \"Pclass\"])\nall_df = pd.get_dummies(all_df, columns=['AgeBand','FareBand','Embarked'])\n\ntrain = all_df[all_df['Test_Flag']==0]\ntest = all_df[all_df['Test_Flag']==1].reset_index(drop=True)\ntarget = train['Survived']\n\ndrop_col = [\n    'PassengerId','Age',\n    'Ticket',\n    'Fare','Cabin',\n    'Test_Flag','Name','Survived'\n    ]\n\ntrain = train.drop(drop_col, axis=1)\ntest = test.drop(drop_col, axis=1)\n\nX_train ,X_val ,y_train ,y_val = train_test_split(\n    train, target, \n    test_size=0.2, shuffle=True,random_state=0\n    )\n\nmodel = LogisticRegression() \nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_train)\nprint(accuracy_score(y_train, y_pred))\n\ny_pred_val = model.predict(X_val)\nprint(accuracy_score(y_val, y_pred_val))\n\ntest_pred = model.predict(test)\n\nsample_sub[\"Survived\"] = np.where(test_pred>=0.5, 1, 0)\ndisplay(sample_sub.head(10))\n\nsample_sub.to_csv(\"submission.csv\", index=False)","68935601":"# LogisticRegression\n\nfrom sklearn.model_selection import KFold\n\ncv = KFold(n_splits=3, random_state=0, shuffle=True)\n\ntrain_acc_list = []\nval_acc_list = []\n\nfor i ,(trn_index, val_index) in enumerate(cv.split(train, target)):\n\n    print(f'Fold : {i}')\n    X_train ,X_val = train.loc[trn_index], train.loc[val_index]\n    y_train ,y_val = target[trn_index], target[val_index]\n\n    model = LogisticRegression() \n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_train)\n    train_acc = accuracy_score(y_train, y_pred)\n    print(train_acc)\n    train_acc_list.append(train_acc)\n\n    y_pred_val = model.predict(X_val)\n    val_acc = accuracy_score(y_val, y_pred_val)\n    print(val_acc)\n    val_acc_list.append(val_acc)\n\nprint('-'*10 + 'Result' +'-'*10)\nprint(f'Train_acc : {train_acc_list} , Ave : {np.mean(train_acc_list)}')\nprint(f'Valid_acc : {val_acc_list} , Ave : {np.mean(val_acc_list)}')\n","ddb2b207":"# SVM\nfrom sklearn.svm import SVC\n\ncv = KFold(n_splits=3, random_state=0, shuffle=True)\n\ntrain_acc_list = []\nval_acc_list = []\n\nfor i ,(trn_index, val_index) in enumerate(cv.split(train, target)):\n\n    print(f'Fold : {i}')\n    X_train ,X_val = train.loc[trn_index], train.loc[val_index]\n    y_train ,y_val = target[trn_index], target[val_index]\n\n    model = SVC(random_state=0)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_train)\n    train_acc = accuracy_score(y_train, y_pred)\n    print(train_acc)\n    train_acc_list.append(train_acc)\n\n    y_pred_val = model.predict(X_val)\n    val_acc = accuracy_score(\n        y_val, y_pred_val\n        )\n    print(val_acc)\n    val_acc_list.append(val_acc)\n\nprint('-'*10 + 'Result' +'-'*10)\nprint(f'Train_acc : {train_acc_list} , Ave : {np.mean(train_acc_list)}')\nprint(f'Valid_acc : {val_acc_list} , Ave : {np.mean(val_acc_list)}')","1bfc16d2":"# MLP\nimport tensorflow as tf\nimport random\nimport os\nfrom tensorflow.keras.utils import to_categorical\n\ntf.random.set_seed(0)\nnp.random.seed(0)\nrandom.seed(0)\nos.environ[\"PYTHONHASHSEED\"] = \"0\"\n\ncv = KFold(n_splits=3, random_state=0, shuffle=True)\n\ntrain_acc_list = []\nval_acc_list = []\n\nfor i ,(trn_index, val_index) in enumerate(cv.split(train, target)):\n\n    print(f'Fold : {i}')\n    X_train ,X_val = train.loc[trn_index], train.loc[val_index]\n    y_train ,y_val = target[trn_index], target[val_index]\n\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Input(X_train.shape[1]),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(16, activation='relu'),\n        tf.keras.layers.Dense(2, activation='softmax')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    model.fit(\n        X_train, to_categorical(y_train),\n        batch_size=256, epochs=100, verbose=False\n        )\n\n    y_pred = np.argmax(model.predict(X_train),axis=1)\n    train_acc = accuracy_score(y_train, y_pred)\n    print(train_acc)\n    train_acc_list.append(train_acc)\n\n    y_pred_val = np.argmax(model.predict(X_val),axis=1)\n    val_acc = accuracy_score(y_val, y_pred_val)\n    print(val_acc)\n    val_acc_list.append(val_acc)\n\nprint('-'*10 + 'Result' +'-'*10)\nprint(f'Train_acc : {train_acc_list} , Ave : {np.mean(train_acc_list)}')\nprint(f'Valid_acc : {val_acc_list} , Ave : {np.mean(val_acc_list)}')","e3163e35":"# RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\n\ncv = KFold(n_splits=3, random_state=0, shuffle=True)\n\ntrain_acc_list = []\nval_acc_list = []\n\nfor i ,(trn_index, val_index) in enumerate(cv.split(train, target)):\n\n    print(f'Fold : {i}')\n    X_train ,X_val = train.loc[trn_index], train.loc[val_index]\n    y_train ,y_val = target[trn_index], target[val_index]\n\n    model = RandomForestClassifier(random_state=0)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_train)\n    train_acc = accuracy_score(y_train, y_pred)\n    print(train_acc)\n    train_acc_list.append(train_acc)\n\n    y_pred_val = model.predict(X_val)\n    val_acc = accuracy_score(y_val, y_pred_val)\n    print(val_acc)\n    val_acc_list.append(val_acc)\n\nprint('-'*10 + 'Result' +'-'*10)\nprint(f'Train_acc : {train_acc_list} , Ave : {np.mean(train_acc_list)}')\nprint(f'Valid_acc : {val_acc_list} , Ave : {np.mean(val_acc_list)}')","9a634679":"# LightGBM\nimport lightgbm as lgb\n\ncv = KFold(n_splits=3, random_state=0, shuffle=True)\n\ntrain_acc_list = []\nval_acc_list = []\n\nlgb_params = {\n    \"objective\":\"binary\",\n    \"metric\": \"binary_error\",\n    \"force_row_wise\" : True,\n    \"seed\" : 0,\n    }\n\nrename_dict = {\n    'AgeBand_(0.169, 22.0]' : 'AgeBand_1',\n    'AgeBand_(22.0, 28.0]' : 'AgeBand_2', \n    'AgeBand_(28.0, 35.0]' : 'AgeBand_3', \n    'AgeBand_(35.0, 80.0]' : 'AgeBand_4',\n    'FareBand_(-0.001, 7.896]' : 'FareBand_1', \n    'FareBand_(7.896, 14.454]' : 'FareBand_2',\n    'FareBand_(14.454, 31.275]' : 'FareBand_3',\n    'FareBand_(31.275, 512.329]' : 'FareBand_4'\n    }\n\nfor i ,(trn_index, val_index) in enumerate(cv.split(train, target)):\n\n    print(f'Fold : {i}')\n    X_train ,X_val = train.loc[trn_index].rename(columns =rename_dict), train.loc[val_index].rename(columns =rename_dict)\n    y_train ,y_val = target[trn_index], target[val_index]\n\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid = lgb.Dataset(X_val, y_val)\n\n    model = lgb.train(\n        params = lgb_params, \n        train_set = lgb_train,\n        valid_sets = [lgb_train, lgb_valid], \n        verbose_eval = 100 ,\n        early_stopping_rounds=10\n       )\n\n    y_pred = model.predict(X_train)\n    train_acc = accuracy_score(y_train, np.where(y_pred>=0.5, 1, 0))\n    print(train_acc)\n    train_acc_list.append(train_acc)\n\n    y_pred_val = model.predict(X_val)\n    val_acc = accuracy_score(y_val, np.where(y_pred_val>=0.5, 1, 0))\n    print(val_acc)\n    val_acc_list.append(val_acc)\n\nprint('-'*10 + 'Result' +'-'*10)\nprint(f'Train_acc : {train_acc_list} , Ave : {np.mean(train_acc_list)}')\nprint(f'Valid_acc : {val_acc_list} , Ave : {np.mean(val_acc_list)}')","31c02b52":"import lightgbm as lgb\n\ncv = KFold(n_splits=3, random_state=0, shuffle=True)\n\ntrain_acc_list = []\nval_acc_list = []\nmodels = []\n\nlgb_params = {\n    \"objective\":\"binary\",\n    \"metric\": \"binary_error\",\n    \"force_row_wise\" : True,\n    \"seed\" : 0,\n    }\n\nrename_dict = {\n    'AgeBand_(0.169, 22.0]' : 'AgeBand_1',\n    'AgeBand_(22.0, 28.0]' : 'AgeBand_2', \n    'AgeBand_(28.0, 35.0]' : 'AgeBand_3', \n    'AgeBand_(35.0, 80.0]' : 'AgeBand_4',\n    'FareBand_(-0.001, 7.896]' : 'FareBand_1', \n    'FareBand_(7.896, 14.454]' : 'FareBand_2',\n    'FareBand_(14.454, 31.275]' : 'FareBand_3',\n    'FareBand_(31.275, 512.329]' : 'FareBand_4'\n    }\n\nfor i ,(trn_index, val_index) in enumerate(cv.split(train, target)):\n\n    print(f'Fold : {i}')\n    X_train ,X_val = train.loc[trn_index].rename( columns =rename_dict), train.loc[val_index].rename( columns =rename_dict)\n    y_train ,y_val = target[trn_index], target[val_index]\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid = lgb.Dataset(X_val, y_val)\n\n    model = lgb.train(\n        params = lgb_params, \n        train_set = lgb_train,\n        valid_sets = [lgb_train, lgb_valid], \n        verbose_eval = 100 ,\n        early_stopping_rounds=10\n       )\n\n    y_pred = model.predict(X_train)\n    train_acc = accuracy_score(y_train, np.where(y_pred>=0.5, 1, 0))\n    print(train_acc)\n    train_acc_list.append(train_acc)\n\n    y_pred_val = model.predict(X_val)\n    val_acc = accuracy_score(y_val, np.where(y_pred_val>=0.5, 1, 0))\n    print(val_acc)\n    val_acc_list.append(val_acc)\n\n    models.append(model)\n\nprint('-'*10 + 'Result' +'-'*10)\nprint(f'Train_acc : {train_acc_list} , Ave : {np.mean(train_acc_list)}')\nprint(f'Valid_acc : {val_acc_list} , Ave : {np.mean(val_acc_list)}')\n\ntest_pred = np.zeros((len(test), 3))\nfor fold_index , gbm in enumerate(models):\n    pred_test = gbm.predict(test.rename( columns =rename_dict))\n    test_pred[:, fold_index] = pred_test\n\ntest_pred = (np.mean(test_pred, axis=1) > 0.5).astype(int)\nsample_sub[\"Survived\"] = test_pred\nsample_sub.to_csv(\"submission.csv\", index=False)","ac1063e7":"# Train\ntrain_df['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n\nvenn2(\n    subsets=(\n        set(train_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)), \n        set(test_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False))\n    ),\n    set_labels=('Train', 'Test'),\n)\nplt.show()\n\n# All\nall_df = pd.concat([train_df,test_df],axis=0).reset_index(drop=True)\nall_df['Test_Flag'] = 0\nall_df.loc[train_df.shape[0]: , 'Test_Flag'] = 1\n\nall_df['Title'] = all_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\nprint(all_df['Title'].isna().sum())\n\ndisplay(pd.crosstab(all_df['Test_Flag'],all_df['Title']))\n\nall_df.groupby('Title')['Age'].mean()\n\ndisplay(all_df[all_df['Test_Flag']==0].groupby('Title')['Survived'].agg(['mean','count']))\ndisplay(all_df.groupby('Title')['Fare'].agg(['mean','count']))\ndisplay(pd.crosstab(all_df['Pclass'],all_df['Title']))","ef28ddae":"all_df['Age'] =  all_df['Age'].fillna( all_df['Age'].median())\nall_df['Fare'] =  all_df['Fare'].fillna( all_df['Fare'].median())\nall_df['Embarked'] =  all_df['Embarked'].fillna('NaN')\nall_df['FareBand'] = pd.qcut(all_df['Fare'], 4)\nall_df['AgeBand'] = pd.qcut(all_df['Age'], 4)\nall_df['FamilySize'] = all_df['SibSp'] + all_df['Parch'] + 1\nall_df['MedF']   = all_df['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\nall_df['LargeF'] = all_df['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\nall_df['Alone'] = all_df['FamilySize'].map(lambda s: 1 if  s == 1  else 0)\n\nall_df = pd.get_dummies(all_df, columns= ['Sex', 'Pclass','AgeBand','FareBand','Embarked'])\nall_df['Title'] = all_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\ndef prepro_name_title(Title): \n    if Title == 'Master':\n        return 0\n    elif Title == 'Miss':\n        return 1\n    elif Title == 'Mr':\n        return 2\n    elif Title == 'Mrs':\n        return 3 \n    else:\n        return 4\n\nall_df['Title_Encode'] = all_df['Title'].map(prepro_name_title)\n\ntrain = all_df[all_df['Test_Flag']==0]\ntest = all_df[all_df['Test_Flag']==1].reset_index(drop=True)\ntarget = train['Survived']\n\ndrop_col = [\n    'PassengerId','Age', \n    'Ticket','Title',\n    'Fare','Cabin',\n    'Test_Flag','Name','Survived'\n    ]\n\ntrain = train.drop(drop_col, axis=1)\ntest = test.drop(drop_col, axis=1)\n\n\ntrain_acc_list = []\nval_acc_list = []\nmodels = []\n\n# \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b9a\u7fa9\u3000\nlgb_params = {\n    \"objective\":\"binary\",\n    \"metric\": \"binary_error\",\n    \"force_row_wise\" : True,\n    \"seed\" : 0,\n    'learning_rate': 0.1,\n    'min_data_in_leaf': 5\n    }\n\n# LightGBM\u304c\u30ab\u30e9\u30e0\u540d\u306b[]\u3084{}\u3092\u542b\u3093\u3067\u3044\u308b\u3068\u30a8\u30e9\u30fc\u304c\u51fa\u308b\u306e\u3067\u3001\u5909\u66f4\u3059\u308b\nrename_dict = {\n    'AgeBand_(0.169, 22.0]' : 'AgeBand_1',\n    'AgeBand_(22.0, 28.0]' : 'AgeBand_2', \n    'AgeBand_(28.0, 35.0]' : 'AgeBand_3', \n    'AgeBand_(35.0, 80.0]' : 'AgeBand_4',\n    'FareBand_(-0.001, 7.896]' : 'FareBand_1', \n    'FareBand_(7.896, 14.454]' : 'FareBand_2',\n    'FareBand_(14.454, 31.275]' : 'FareBand_3',\n    'FareBand_(31.275, 512.329]' : 'FareBand_4'\n    }\n\nfor i ,(trn_index, val_index) in enumerate(cv.split(train, target)):\n\n    print(f'Fold : {i}')\n    X_train ,X_val = train.loc[trn_index].rename( columns =rename_dict), train.loc[val_index].rename( columns =rename_dict)\n    y_train ,y_val = target[trn_index], target[val_index]\n\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid = lgb.Dataset(X_val, y_val)\n\n    model = lgb.train(\n        params = lgb_params, \n        train_set = lgb_train,\n        valid_sets = [lgb_train, lgb_valid], \n        verbose_eval = 100 ,\n        early_stopping_rounds=10\n       )\n\n    y_pred = model.predict(X_train)\n    train_acc = accuracy_score(y_train, np.where(y_pred>=0.5, 1, 0))\n    print(train_acc)\n    train_acc_list.append(train_acc)\n\n    y_pred_val = model.predict(X_val)\n    val_acc = accuracy_score(y_val, np.where(y_pred_val>=0.5, 1, 0))\n    print(val_acc)\n    val_acc_list.append(val_acc)\n\n    models.append(model)\n\nprint('-'*10 + 'Result' +'-'*10)\nprint(f'Train_acc : {train_acc_list} , Ave : {np.mean(train_acc_list)}')\nprint(f'Valid_acc : {val_acc_list} , Ave : {np.mean(val_acc_list)}')\n\ntest_pred = np.zeros((len(test), 3))\nfor fold_index , gbm in enumerate(models):\n    pred_test = gbm.predict(test.rename( columns =rename_dict))\n    test_pred[:, fold_index] = pred_test\n\ntest_pred = (np.mean(test_pred, axis=1) > 0.5).astype(int)\nsample_sub[\"Survived\"] = test_pred\nsample_sub.to_csv(\"submission.csv\", index=False)","9c5746b1":"import optuna\n\n\ndef objective(trial):\n\n    cv = KFold(n_splits=3, random_state=0, shuffle=True)\n    val_acc_list = []\n\n    lgb_params = {\n        \"objective\":\"binary\",\n        \"metric\": \"binary_error\",\n        \"force_row_wise\" : True,\n        \"seed\" : 0,\n        'min_data_in_leaf': 5,\n        \"max_depth\": trial.suggest_int('max_depth', 3, 20),\n        \"num_leaves\": trial.suggest_int('num_leaves', 21, 41),\n        \"learning_rate\": trial.suggest_uniform('learning_rate', 0.1, 0.2),\n        }\n\n    rename_dict = {\n        'AgeBand_(0.169, 22.0]' : 'AgeBand_1',\n        'AgeBand_(22.0, 28.0]' : 'AgeBand_2', \n        'AgeBand_(28.0, 35.0]' : 'AgeBand_3', \n        'AgeBand_(35.0, 80.0]' : 'AgeBand_4',\n        'FareBand_(-0.001, 7.896]' : 'FareBand_1', \n        'FareBand_(7.896, 14.454]' : 'FareBand_2',\n        'FareBand_(14.454, 31.275]' : 'FareBand_3',\n        'FareBand_(31.275, 512.329]' : 'FareBand_4'\n        }\n\n    for i ,(trn_index, val_index) in enumerate(cv.split(train, target)):\n\n        print(f'Fold : {i}')\n        X_train ,X_val = train.loc[trn_index].rename( columns =rename_dict), train.loc[val_index].rename( columns =rename_dict)\n        y_train ,y_val = target[trn_index], target[val_index]\n\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_valid = lgb.Dataset(X_val, y_val)\n\n        model = lgb.train(\n            params = lgb_params, \n            train_set = lgb_train,\n            valid_sets = [lgb_train, lgb_valid], \n            verbose_eval = -1 ,\n            early_stopping_rounds=10\n           )\n\n        y_pred = model.predict(X_train)\n\n        y_pred_val = model.predict(X_val)\n        val_acc = accuracy_score(y_val, np.where(y_pred_val>=0.5, 1, 0))\n\n        val_acc_list.append(val_acc)\n\n    return np.mean(val_acc_list)\n\nstudy = optuna.create_study(direction=\"maximize\")\n\nstudy.optimize(objective, n_trials=100)\n\nprint(\"best_value\", study.best_value)\nprint(\"best_params\", study.best_params)\n\noptuna_log_df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\"))\ndisplay(optuna_log_df)\n\nplt.scatter(optuna_log_df[\"params_max_depth\"], optuna_log_df[\"value\"])","c9f60ab2":"from tensorflow.keras.callbacks import EarlyStopping\nimport scipy.stats as stats\n\nall_df = pd.concat([train_df,test_df],axis=0).reset_index(drop=True)\nall_df['Test_Flag'] = 0\nall_df.loc[train_df.shape[0]: , 'Test_Flag'] = 1\n\nall_df['Age'] =  all_df['Age'].fillna( all_df['Age'].median())\nall_df['Fare'] =  all_df['Fare'].fillna( all_df['Fare'].median())\nall_df['Embarked'] =  all_df['Embarked'].fillna('NaN')\nall_df['FareBand'] = pd.qcut(all_df['Fare'], 4)\nall_df['AgeBand'] = pd.qcut(all_df['Age'], 4)\nall_df['FamilySize'] = all_df['SibSp'] + all_df['Parch'] + 1\nall_df['MedF']   = all_df['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\nall_df['LargeF'] = all_df['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\nall_df['Alone'] = all_df['FamilySize'].map(lambda s: 1 if  s == 1  else 0)\nall_df['Title'] = all_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\nall_df['Title_Encode'] = all_df['Title'].map(prepro_name_title)\n\n\nall_df = pd.get_dummies(all_df, columns= [\"Sex\", \"Pclass\"])\nall_df = pd.get_dummies(all_df, columns=['AgeBand','FareBand','Embarked'])\n\ntrain = all_df[all_df['Test_Flag']==0]\ntest = all_df[all_df['Test_Flag']==1].reset_index(drop=True)\ntarget = train['Survived']\n\ndrop_col = [\n    'PassengerId','Age',\n    'Ticket','Title',\n    'Fare','Cabin',\n    'Test_Flag','Name','Survived'\n    ]\n\ntrain = train.drop(drop_col, axis=1)\ntest = test.drop(drop_col, axis=1)\n\ncv = KFold(n_splits=3, random_state=0, shuffle=True)\n\ntrain_acc_list = []\nval_acc_list = []\nmodels_lgb = []\nmodels_rf = []\nmodels_rogi = []\nmodels_svm = []\n\n# \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b9a\u7fa9\u3000\nlgb_params = {\n    \"objective\":\"binary\",\n    \"metric\": \"binary_error\",\n    \"force_row_wise\" : True,\n    \"seed\" : 0,\n    'learning_rate': 0.1,\n    'min_data_in_leaf': 5,\n    'max_depth': 16\n    }\n\n# LightGBM\u304c\u30ab\u30e9\u30e0\u540d\u306b[]\u3084{}\u3092\u542b\u3093\u3067\u3044\u308b\u3068\u30a8\u30e9\u30fc\u304c\u51fa\u308b\u306e\u3067\u3001\u5909\u66f4\u3059\u308b\nrename_dict = {\n    'AgeBand_(0.169, 22.0]' : 'AgeBand_1',\n    'AgeBand_(22.0, 28.0]' : 'AgeBand_2', \n    'AgeBand_(28.0, 35.0]' : 'AgeBand_3', \n    'AgeBand_(35.0, 80.0]' : 'AgeBand_4',\n    'FareBand_(-0.001, 7.896]' : 'FareBand_1', \n    'FareBand_(7.896, 14.454]' : 'FareBand_2',\n    'FareBand_(14.454, 31.275]' : 'FareBand_3',\n    'FareBand_(31.275, 512.329]' : 'FareBand_4'\n    }\n\n\nfor i ,(trn_index, val_index) in enumerate(cv.split(train, target)):\n\n    print(f'Fold : {i}')\n    X_train ,X_val = train.loc[trn_index].rename( columns =rename_dict), train.loc[val_index].rename( columns =rename_dict)\n    y_train ,y_val = target[trn_index], target[val_index]\n\n    # LigthGBM Part\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid = lgb.Dataset(X_val, y_val)\n\n    model_lgb = lgb.train(\n        params = lgb_params, \n        train_set = lgb_train,\n        valid_sets = [lgb_train, lgb_valid], \n        verbose_eval = 0 ,\n        early_stopping_rounds=10\n       )\n\n    models_lgb.append(model_lgb)\n\n    # RandomForest Part\n    print('-' *10 +' Start_rf ' +'-' *10)\n    model_rf = RandomForestClassifier(\n        random_state=0,max_depth=15,\n        min_samples_leaf=5,min_samples_split=5\n        )\n    model_rf.fit(X_train, y_train)\n    models_rf.append(model_rf)\n\n    # MLP Part\n    print('-' *10 +' Start_mlp ' +'-' *10)\n    train_pre = pd.get_dummies(train, columns= ['Title_Encode'])\n    X_train_mlp ,X_val_mlp = train_pre.loc[trn_index].rename( columns =rename_dict), train_pre.loc[val_index].rename( columns =rename_dict)\n    y_train_mlp ,y_val_mlp = target[trn_index], target[val_index]\n\n    model_mlp = tf.keras.models.Sequential([\n        tf.keras.layers.Input(X_train_mlp.shape[1]),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(16, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(16, activation='relu'),\n        tf.keras.layers.Dense(2, activation='softmax')\n    ])\n    early_stopping =  EarlyStopping(\n                            monitor='val_loss',\n                            patience=10,\n                            mode='auto'\n                        )\n    model_mlp.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    model_mlp.fit(\n        X_train_mlp, to_categorical(y_train_mlp),validation_data = (X_val_mlp,to_categorical(y_val_mlp)),\n        batch_size=256, epochs=300, verbose=False,callbacks=[early_stopping]\n    )\n\n    # LogisticRegression Part\n    print('-' *10 +' Start_rogi ' +'-' *10)\n    model_rogi = LogisticRegression()\n    model_rogi.fit(\n        X_train_mlp, y_train\n        )\n    models_rogi.append(model_rogi)\n\n    # SVM Part\n    print('-' *10 +' Start_SVM ' +'-' *10)\n    model_svm = SVC(random_state=0)\n    model_svm.fit(\n        X_train_mlp, y_train\n        )\n    models_svm.append(model_svm)\n\n    train_pred = np.zeros((len(y_train_mlp), 5))\n\n    train_pred[:,0] = np.where(model_lgb.predict(X_train)>=0.5, 1, 0)\n    train_pred[:,1] = model_rf.predict(X_train)\n    train_pred[:,2] = np.argmax(model_mlp.predict(X_train_mlp),axis=1)\n    train_pred[:,3] = model_rogi.predict(X_train_mlp)\n    train_pred[:,4] = model_svm.predict(X_train_mlp)\n\n    train_acc = accuracy_score(y_train, stats.mode(train_pred,axis=1)[0])\n    train_acc_list.append(train_acc)\n\n    val_pred = np.zeros((len(y_val_mlp), 5))\n\n    val_pred[:,0] = np.where(model_lgb.predict(X_val)>=0.5, 1, 0)\n    val_pred[:,1] = model_rf.predict(X_val)\n    val_pred[:,2] = np.argmax(model_mlp.predict(X_val_mlp),axis=1)\n    val_pred[:,3] = model_rogi.predict(X_val_mlp)\n    val_pred[:,4] = model_svm.predict(X_val_mlp)\n\n    val_acc = accuracy_score(y_val, stats.mode(val_pred,axis=1)[0])\n    val_acc_list.append(val_acc)\n\n\nprint('-'*10 + 'Result' +'-'*10)\nprint(f'Train_acc : {train_acc_list} , Ave : {np.mean(train_acc_list)}')\nprint(f'Valid_acc : {val_acc_list} , Ave : {np.mean(val_acc_list)}')","a54cb871":"# Examining the Model","e8f2dedc":"# Base Model","187f3f1a":"# Parameter Tuning","3161de3d":"# Feature Engineering Part_2","7275e0e8":"# Add New Feature Model Part_1","0b5fd82f":"# Feature Engineering Part_1","c60f9a43":"# Ensemble","6f20e044":"# Add New Feature Model Part_2","a7675b7f":"# EDA"}}