{"cell_type":{"4b1a55a2":"code","4699bd02":"code","f3348c48":"code","e03ecfc7":"code","0270b9b7":"code","e7091630":"code","44f7e7d0":"code","6e07c5d6":"code","67332ef0":"code","a0bb1bb5":"code","51908db8":"code","569825cb":"code","5b277fea":"code","230ccff5":"code","cb7ba71d":"code","3a438af4":"code","c6618c25":"code","56e1dca0":"code","621d3dc6":"code","47de00bd":"code","4de9474b":"code","1ec8af8b":"code","0b57cbc1":"code","fd8cb1a9":"code","81ac1a65":"code","5c91c6be":"code","209f845b":"code","a494ba0b":"code","a2f1078c":"code","1aa98178":"code","4a3a8fb0":"code","3c348130":"code","afd76520":"code","43c28932":"code","51132cf2":"code","0bfad1cf":"code","368c6af2":"code","ed8233fc":"code","728ee924":"code","723a8b26":"code","2be59e28":"code","e1d85f82":"code","f703013d":"code","7f4bf446":"code","cb266c0e":"code","70545b1e":"code","f31f8e3e":"code","ae2ed604":"code","aacc1429":"code","59dded52":"code","3c5786ec":"code","0088cbc8":"code","2c80037b":"code","9266912c":"code","a0dbec4c":"code","49a127c8":"code","c261d481":"code","a6dd768f":"code","85f27aa5":"code","bc7c1402":"markdown","5f9975aa":"markdown","d20bb028":"markdown","2c02dbc9":"markdown","50cd7e6c":"markdown","747c09cf":"markdown","1f5d250d":"markdown","1e69d6bd":"markdown","45413e0f":"markdown","73ad4b13":"markdown","29288ac1":"markdown","2fe15808":"markdown","d0b80804":"markdown","ea2b460f":"markdown","a6811a99":"markdown","7f1eb731":"markdown","edc796be":"markdown","300b542d":"markdown","5e98b712":"markdown","6b90de46":"markdown","f4cacaa6":"markdown","c8909a60":"markdown","8be05620":"markdown","ff19b1d2":"markdown","63fcdae9":"markdown","1fcc6ffc":"markdown","c7850887":"markdown","15dfab54":"markdown","6b99c749":"markdown","e516f07f":"markdown","af17ecd9":"markdown","9b462631":"markdown","977eb302":"markdown","3f2d5fc1":"markdown","672323d9":"markdown","d96478b2":"markdown","13263eb0":"markdown","c42a8326":"markdown","da7a7576":"markdown","cabc5e2a":"markdown","9b80283e":"markdown","958e0631":"markdown","aefc4c36":"markdown","2a920aaa":"markdown","caac7498":"markdown"},"source":{"4b1a55a2":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\nimport scipy.stats as stats\n\n#parameter settings\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","4699bd02":"#edit the file location of raw data set\ncust_df=pd.read_csv(\"..\/input\/ca-exam\/cell2celltrain.csv\")","f3348c48":"cust_df.info()","e03ecfc7":"# create a numeric representation of Churn and Credit rating\ncust_df['CreditRating_num'] =cust_df['CreditRating'].str[0].astype('int64')\ncust_df['Churn_num'] = cust_df['Churn'].map({'No':0, 'Yes':1})\n","0270b9b7":"# checking correlation of different numeric col w.r.t. Churn & sort\nchurn_corr = pd.DataFrame(abs(cust_df.corr()['Churn_num']))\nchurn_corr = churn_corr.nlargest(20, 'Churn_num') # prioritise top 20\n\n# drop churn_num because it's irrelevant\nchurn_corr = churn_corr.drop('Churn_num', axis = 0)\n\n# plot\nplt.figure(figsize=(8,8), dpi = 100)\nsns.barplot(data = churn_corr, \n            x = 'Churn_num', y =churn_corr.index)\nplt.title('Ranking features\\' correlations vs churn');","e7091630":"# create function to find missing columns, and compute how many %\ndef percent_missing(df):\n    percent_nan = 100* df.isnull().sum() \/ len(df) # create the null percentage list of col \n    percent_nan = percent_nan[percent_nan>0].sort_values() # filter and sort those > 0\n    return percent_nan","44f7e7d0":"# find the cols and compute the percentage of missing values\npercent_null = percent_missing(cust_df)\n\n# plotting\nplt.figure(figsize=(10,8))\nsns.barplot(y=percent_null.index, x = percent_null)\nplt.xlabel('(%) of missing data')\nplt.title('Percentage of missing data');","6e07c5d6":"# drop rows from columns that containt less than 1% null\ncust_df = cust_df.dropna(axis=0,\n                         subset=percent_null[percent_null<1.0].index)","67332ef0":"# fill the NA of AgeHH2 with 0s\ncust_df['AgeHH2'] = cust_df['AgeHH2'].fillna(value=0)","a0bb1bb5":"# find the column that has high correlation with AgeHH1\nabs(cust_df.corr()['AgeHH1']).sort_values(ascending = False).head()","51908db8":"plt.figure(figsize=(8,8))\nsns.boxplot(x='AgeHH1',y='IncomeGroup',data=cust_df,orient='h');","569825cb":"# groupby income group, apply function to fill the nulls with median of each income group\n\ncust_df['AgeHH1'] = cust_df.groupby('IncomeGroup')['AgeHH1'].transform(lambda val:\n                                                                       val.fillna(val.median()))","5b277fea":"# create a groupby object\ngroupby_data = cust_df.groupby('IncomeGroup')['AgeHH1']\n\n# create a function to replace the 0s with the median of each income group\ndef replace(group):\n    mask = group == 0\n    \n    # Select those values where it is = 0, and replace\n    # them with the median of the values which are not 0.\n    group[mask] = group[~mask].median()\n    \n    return group\n\n# assign back to original column AgeHH1\ncust_df['AgeHH1'] = groupby_data.transform(replace)","230ccff5":"# we have successfully removed all missing values!\n\npercent_null = percent_missing(cust_df)\npercent_null","cb7ba71d":"# extract only object columns\ndf_objs = cust_df.select_dtypes(include='object')\n\ndf_objs.info()","3a438af4":"# checking number of unique values per featuresw\ndf_objs.nunique()","c6618c25":"# checking the unique values of handset price\ndf_objs['HandsetPrice'].value_counts()","56e1dca0":"# changing the unknown values in handset price to 0, in the main df\ncust_df['HandsetPrice'] = cust_df['HandsetPrice'].map({'Unknown':'0'}).fillna(cust_df['HandsetPrice'])\n\n# and then convert col to numeric value\ncust_df['HandsetPrice'] = cust_df['HandsetPrice'].astype('int64')\n\ncust_df['HandsetPrice']","621d3dc6":"# checking how Service Area looks like\ndf_objs['ServiceArea']","47de00bd":"# get the first 3 letter, and store in a new col\ncust_df['ServiceArea_region'] = cust_df['ServiceArea'].str[:3]\n\n# get a preview\ncust_df['ServiceArea_region'].head()","4de9474b":"# checking # of unique values\ncust_df['ServiceArea_region'].nunique()","1ec8af8b":"# create pivot table between credit rating and churn count\ncredit_churn_df = cust_df[['CreditRating', 'Churn']].pivot_table(index='CreditRating',\n                                                                 columns='Churn',\n                                                                 aggfunc=len)\n\n# create the churn rate column\ncredit_churn_df['ChurnRate'] = credit_churn_df['Yes'] \/ (credit_churn_df['Yes'] + credit_churn_df['No'])\n\ncredit_churn_df","0b57cbc1":"# plot to compare churn rate vs credit rating\nplt.figure(figsize=(8,4),dpi=100)\nsns.lineplot(data=credit_churn_df['ChurnRate'])\nplt.ylim(0,1);","fd8cb1a9":"# plotting boxplot to check any signal \/ separation between churn and no churn\n# features to plot\nfeat = ['CurrentEquipmentDays', 'TotalRecurringCharge',\n        'OverageMinutes', 'RetentionCalls']\n\n# set subplot dimension\nnrows, ncols = 2,2\n\n\n# loop over the column names\nfig = plt.figure(figsize=(10,8),dpi=100)\nfor i in range(1,5):\n    ax = fig.add_subplot(nrows, ncols, i)\n    sns.boxplot(data=cust_df, x='Churn', y=feat[i-1], ax = ax)\n    ax.set_title(f'{feat[i-1]} vs Churn')\n\n\nfig.tight_layout();","81ac1a65":"#wrapper function to create additional features for churn prediction\ndef create_features(cust_df):\n    \n    # define function to perform ratio division & handle ZeroDivisionError\n    def ratio_div(a, b):\n        try:\n            return a \/ b\n        except ZeroDivisionError:\n            return 0\n    \n    #  RATIO of recurrent charge to monthly charge\n    cust_df['RecurringCharge_ratio'] = np.vectorize(ratio_div)(cust_df['TotalRecurringCharge'],\n                                                               cust_df['MonthlyRevenue'])\n    \n    # RATIO of overage minutes over total monthly minutes\n    # we assume that MonthlyMinutes are the preallocated by postpaid plan, \n    # so total monthly minutes would be overage+allocated\n    cust_df['OverageMinutes_ratio'] = np.vectorize(ratio_div)(cust_df['OverageMinutes'],\n                                                              cust_df['MonthlyMinutes']\n                                                              +cust_df['OverageMinutes'])\n    \n    return cust_df  \n\n\n#Make a call to the feature engineering wrapper function for churn prediction\ncust_df=create_features(cust_df)\n","5c91c6be":"# droppping the columns mentioned\ncust_df = cust_df.drop(['CustomerID', 'CreditRating', 'Churn_num', 'ServiceArea'], \n                       axis = 1)","209f845b":"# preview of cleaned data\ncust_df.info()","a494ba0b":"# import libraries for preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","a2f1078c":"# splitting into features and label df\nX = cust_df.drop('Churn',axis=1)\ny = cust_df['Churn']","1aa98178":"# get dummies for features columns\nX = pd.get_dummies(X, drop_first=True) # to remove redundant features and prevent collinearity","4a3a8fb0":"# do train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","3c348130":"# create an instance of the scaler\nscaler = StandardScaler()\n\n# fit the scaler with, and transform X_train\nscaled_X_train = scaler.fit_transform(X_train)\n\n# just scale the X_test set (preventing data leakage)\nscaled_X_test = scaler.transform(X_test)\n","afd76520":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV","43c28932":"# create a basic instance of the log reg model\nlog_model = LogisticRegression(solver='saga',max_iter=200)","51132cf2":"# create parameter grid for cross validation\npenalty = ['elasticnet'] # what regularisation to try\nl1_ratio = np.linspace(0,1,5) # ratio between l1 and l2\nC = np.logspace(-2, 2, 5) # inverse magnitude of regularisation (logscale is recommended)\n\n# create the param_grid dictionary\nparam_grid={'C':C,\n            'penalty':penalty,\n           'l1_ratio': l1_ratio}","0bfad1cf":"# create the grid search instance\ngrid_log_model = GridSearchCV(estimator = log_model, \n                              param_grid=param_grid,\n                              cv=5,\n                              verbose = 2, n_jobs = -1)","368c6af2":"%%time\ngrid_log_model.fit(scaled_X_train,y_train)","ed8233fc":"# checking the parameter with the best performance\ngrid_log_model.best_params_","728ee924":"#---METRICS COMPARISON WILL BE DONE TOGETHER WITH RANDOM FOREST MODEL BELOW----------------","723a8b26":"from sklearn.ensemble import RandomForestClassifier","2be59e28":"# create parameter grid for cross validation\n\n# how many trees? standard is about 100, so we do +\/-\nn_estimators=[64,100,128] \n\n# how many random features to select each split? standard is sqrt(n). \n# since we have about 128 col, so it's about +\/- 11.5\nmax_features= list(range(10,14)) # 10 to 13 \n\n# create the param_grid dictionary\nparam_grid = {'n_estimators':n_estimators,\n              'max_features':max_features}","e1d85f82":"# create an instance of the models\nrf_model = RandomForestClassifier(bootstrap=True) # make sure to use bootstrap sampling\n\ngrid_rf_model = GridSearchCV(estimator = rf_model, \n                              param_grid=param_grid,\n                              cv=5,\n                              verbose=2, n_jobs = -1)","f703013d":"%%time\n\n# fit data to the grid rf model\ngrid_rf_model.fit(X_train,y_train) # no need to use the scaled data, won't affect","7f4bf446":"# checking the parameter with the best performance\ngrid_rf_model.best_params_","cb266c0e":"from sklearn.metrics import (accuracy_score,\n                             classification_report,\n                             plot_confusion_matrix,\n                             plot_precision_recall_curve,\n                             plot_roc_curve,\n                             roc_auc_score)","70545b1e":"# predict y from X test dataset, using the log reg model\ny_pred_log = grid_log_model.predict(scaled_X_test)\n\n# predict y from X test dataset, using the random forest model\ny_pred_rf = grid_rf_model.predict(X_test)","f31f8e3e":"# plotting and comparing confusion matrix\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2,\\\n                              figsize=(10,5), dpi = 100)\n\nplot_confusion_matrix(grid_log_model,scaled_X_test,y_test, ax=ax1)\nax1.set_title('Log Regression')\n\nplot_confusion_matrix(grid_rf_model,X_test,y_test, ax=ax2)\nax2.set_title('Random Forest')\n\nfig.suptitle(\"Confusion matrix\",fontsize=16);","ae2ed604":"# plotting and comparing ROC curve and AUC\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2,\n                              figsize=(12,6), dpi=100)\n\n# plotting the ROC curve vs random chances\nplot_roc_curve(grid_log_model,scaled_X_test,y_test, ax = ax1)\nsns.lineplot([0, 1], [0, 1], label='random chances', linestyle = '--', ax = ax1)\nax1.set_title('Log Regression')\n\nplot_roc_curve(grid_rf_model,X_test,y_test, ax = ax2)\nsns.lineplot([0, 1], [0, 1], label='random chances', linestyle = '--', ax = ax2)\nax2.set_title('Random forest')\n\nfig.suptitle(\"ROC curve\",fontsize=16);","aacc1429":"# to check precision, recall, accuracy for both models\n\nprint('CLASSIFICATION REPORT: Threshold = 0.5\\n')\n\nprint('Log regression')\nprint(classification_report(y_test,y_pred_log))\n\nprint('\\nRandom forest')\nprint(classification_report(y_test,y_pred_rf))","59dded52":"# plotting and comparing precision vs recall curve\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2,\n                              figsize=(12,6), dpi=100)\n\n# plotting the ROC curve vs random chances\nplot_precision_recall_curve(grid_log_model,scaled_X_test,y_test, ax = ax1)\nax1.set_title('Log Regression')\nax1.set_ylim(0,1)\nax1.axvline(0.02, linestyle = '--', \n            color = 'r', label = 'Recall at threshold = 0.5')\n\nplot_precision_recall_curve(grid_rf_model,X_test,y_test, ax = ax2)\nax2.set_title('Random forest')\nax2.set_ylim(0,1)\nax2.axvline(0.07, linestyle = '--', \n            color = 'r', label = 'Recall at threshold = 0.5')\n\nax1.legend()\nax2.legend()\nfig.suptitle(\"Precision vs Recall curve\",fontsize=16);","3c5786ec":"# create a wrapper function\n# to generate predicted label, based on a certain threshold\n\ndef threshold_predict(X_test, model, threshold = 0.5):\n    \n    # get the probability score for every X test\n    y_prob = model.predict_proba(X_test)[:, 1]\n    \n    # get the prediction based on threshold value\n    y_thres = np.vectorize(lambda y: 'Yes' if y >= threshold else 'No')(y_prob)\n    \n    \n    return y_thres","0088cbc8":"# setting the threshold\nthreshold = 0.4\n\n# generating prediction\ny_thres = threshold_predict(X_test, grid_rf_model, threshold)\n\ny_thres","2c80037b":"# comparing performance after threshold adjustment\n\nprint('CLASSIFICATION REPORT: threshold adjustment')\n\nprint('Random forest: T = 0.5')\nprint(classification_report(y_test,y_pred_rf))\n\nprint(f'\\nRandom forest: T = {threshold}')\nprint(classification_report(y_test,y_thres))","9266912c":"# extract the features with the largest coefficient from random forest model\ntop_predictors = pd.Series(grid_rf_model.best_estimator_.feature_importances_, index=X_train.columns)\n\n# sort values in desc order\ntop_20_predictors = top_predictors.nlargest(20)\n\n# plotting the top 20 predictors\nplt.figure(figsize=(6,6), dpi = 100)\nsns.barplot(y = top_20_predictors.index,\n            x = top_20_predictors.values)\nplt.title('Top 20 predictors of customer churn');","a0dbec4c":"# make a copy of the prediction probability\ny_prob = grid_rf_model.predict_proba(X_test)[:, 1]\n\ny_prob","49a127c8":"# initiate a new df, containing monthly revenue and churn label \nchurn_prob_df = pd.DataFrame(X_test['MonthlyRevenue'].copy())\n\n# create a bool mapping of Churn, to ease calculation\nchurn_prob_df['ActualChurn'] = y_test.map({'Yes':True, 'No':False})\n\n# concat the prediction probability\nchurn_prob_df['ChurnPredProb'] = y_prob\n\n# adding quantile number\n# note: we are computing the INVERSE, i.e. Top 1 decile, top 2 decile\nchurn_prob_df['TopDecile'] = 10 - pd.qcut(churn_prob_df['ChurnPredProb'], 10, labels=False)\n\n\n## filtering monthly revenue based on ActualChurn: if Churn = False, value is 0\n# to be used for aggregation later on\n\n# filter function based on churn\ndef rev_filter(revenue, churn):\n    if churn:\n        return revenue\n    else:\n        return 0.0\n    \n# map to a new column\nchurn_prob_df['RevenueIfChurn'] = np.vectorize(rev_filter)(churn_prob_df['MonthlyRevenue'], \n                                                           churn_prob_df['ActualChurn'])\n\nchurn_prob_df.head()","c261d481":"# groupby decile, count actual churn\nchurn_decile = churn_prob_df.groupby('TopDecile').sum()\n\n# dropping unnecessary columns \nchurn_decile = churn_decile.drop(['MonthlyRevenue', 'ChurnPredProb'], axis = 1)\n\n# rename col for clarity\nchurn_decile = churn_decile.rename(columns={\"ActualChurn\": \"ChurnCount\"})\n\n\n# create a cumulative churn value & percentage\nchurn_decile['CumChurn'] = churn_decile['ChurnCount'].cumsum()\n\nchurn_decile['CumChurn%'] = churn_decile['CumChurn'] \/ churn_decile['ChurnCount'].sum() * 100\nchurn_decile['CumChurn%'] = round(churn_decile['CumChurn%'],2) # 2 decimal place\n\nchurn_decile","a6dd768f":"# plotting the gains chart and lift chart\nfrom scikitplot.metrics import plot_cumulative_gain, plot_lift_curve\n\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,7)) \n\nplot_cumulative_gain(y_test, grid_rf_model.predict_proba(X_test), ax=ax1)\n\nplot_lift_curve(y_test, grid_rf_model.predict_proba(X_test), ax=ax2);","85f27aa5":"# estimation of revenue saved, by targetting the top 20% customer of the original ~50,000 customers\nrev_saved = churn_decile.loc[[1,2],'RevenueIfChurn'].sum()\n\n# multiplying by 5 since our test set are 20% of the original dataset\n# assuming equal distribution since it was randomly selected\nrev_saved *= 5\n\nprint(f\"Total revenue saved by targetting the top 20% of customer is ${round(rev_saved,2)}\")","bc7c1402":"<a id='2.1'><\/a>\n### 2.1. Reading data and initial processing","5f9975aa":"<a id='3'><\/a>\n## 3. Data cleaning\nIn order to prepare for our model training, we would perform the following:\n1. Handle missing values \/ nulls\n2. Find and handle outliers from the top 20 features we see above","d20bb028":"<a id='6.2'><\/a>\n### 6.2. Logistic regression model training\n","2c02dbc9":"As we can see, we managed to reduce the number of category from 700+ down to just 57!","50cd7e6c":"As we can see, there seems to be an increasing trend in the median, as the income group increase.\nThis may have some real world reasoning, because as someone gets older, they tend to have 'moved up the corporate ladder', and thus having higher income in general.\n\nWe will fill in the null in AgeHH1 by **finding the median age of each income group.**","747c09cf":"As we can see above, only **CurrentEquipmentDays shows a signal to separate between churn and not churn**; the rest doesn't seem so.\nHowever, even with some separation, its *signal is quite weak*; hopefully the other features would help our prediction model eventually to separate between the churn and non churn customer.\n","1f5d250d":"<a id='2'><\/a>\n## 2. Importing packages and setup","1e69d6bd":"## Table of content\n\n[0. Executive summary](#0.-Executive-summary)\n\n[1. Background information](#1.-Background-information)    \n    \n[2. Importing packages and setup](#2)\n* [2.1. Reading data and initial processing](#2.1)\n\n[3. Data cleaning](#data_prep)\n* [3.1. Handling of nulls](#3.1)\n* [3.2. Handling of categorical features](#3.2)\n\n[4. Exploratory data analysis (EDA)](#4)\n* [4.1. Credit rating](#4.1)\n* [4.2. Plotting continuoues features vs churn](#4.2)\n\n[5. Feature engineering](#5)\n\n[6. Model training and evaluations](#6)\n* [6.1. Pre-processing](#6.1)\n* [6.2. Logistic regression model training](#6.2)\n* [6.3. Random forest model training](#6.3)\n* [6.4. Comparing the 2 models](#6.4)\n* [6.5. Adjustment of churn probability threshold](#6.5)\n* [6.6. Which features are the most important predictor of churn?](#6.6)\n\n\n[7. Creating lift chart to quantify business impact](#7)\n\n[8. Recommendation](#8)\n\n[9. Reflection](#9)","45413e0f":"<a id='6'><\/a>\n## 6. Model training and evaluations\nIn our model training, we would do the following:\n1. Data pre-processing\n2. Training and comparing between 2 models (Logistic regression vs random forest)\n3. Adjusting threshold if necessary, to boost recall.","73ad4b13":"#### ASSUMPTIONS ALERT!!!\n1. For AgeHH2, which is **secondary holder**, we can assume that NOT every account have a secondary holder; thus, we will assign the nulls here with 0 (as can be seen in later in the original data)\n2. For AgeHH1, which is **primary holder**, it doesn't make sense for it to be zero. Thus we will find the column with the closest correlation, and try to impute from there. ","29288ac1":"#### ASSUMPTIONS ALERT!!!\n1. Simply one-hot encoding this column may create an issue during model training, as it will generate as many columns\n2. We will make an assumption that the ServiceArea consist of a string of code, and that the **first 3 letters indicate the region.**\n3. And it follows, we will make an assumption that each region will have similar behaviour, thus may similarly affect the Churn outcome.\n4. We will **extract the region** (first 3 letter of ServiceArea), and **use that for our model training instead.**","2fe15808":"Comparing metrics such as accuracy, recall, AUC, etc, our **random forest model performs better** albeit by a small margin. Thus we would be using it for our subsequent threshold adjustment","d0b80804":"#### ASSUMPTIONS ALERT!!!\n1. In this case, one reasonable assumption is 'Unknown' in handset price indicates that the customer only subscribes for the telco services, not buying their phone from the telco (thus 'Unknown').\n2. Since the customer doesn't buy the phone from telco, we can reasonably assume that the 'unknown' value is 0, since it's not part of an experience they receive.","ea2b460f":"<a id='6.3'><\/a>\n### 6.3. Random forest model training\n","a6811a99":"Also we will be dropping some redundant features, namely:\n* CustomerID\n* CreditRating - we will use the numeric values instead, CreditRating_num\n* Churn_num - since this is the target label, we can use the object col instead, Churn\n* ServiceArea - since we're using the ServiceArea_region instead, to reduce the number of columns during one hot encoding","7f1eb731":"<a id='9'><\/a>\n## 9. Reflection\n* This is my first portfolio involving predictive model! \n* I learned how to create balance between runtime vs the ambitiousness of my model and analysis; when I was just taking my online course, I would just create thousands of cross validation without any care. However, in this project I quickly realise doing so would take overnight to run, which causes slow iteration.\n* I also learned how to applied various feature cleaning engineering method, such as imputing based on category while making reasonable assumptions that's specific to the domain.\n* There are other techniques that I applied beyond just the normal model building, such as playing with the threshold of the model (which needs to be done manually), or even creating and understanding the concept of lift chart.\n* If I have more time, I would probably try other method like checking correlation between features, finding out the distributions, etc, so that I can have a greater understanding of the data, thus make a better decision on what kind of model to use.","edc796be":"# Creating churn predictive model using random forest and lift chart","300b542d":"As we can see, credit rating doesn't seem to have any clear correlation to customer's churn rate","5e98b712":"#### ASSUMPTION ALERT!!!\n- we will be using the **X_test and y_test** data to calculate lift.\n- We would assume similar distribution to the entire X set, since X_test was selected at random.\n- Which means, to **get the final revenue \"saved\"** by the model from the initial ~50,000 customer data, we can **multiply the expected revenue saved from X_test prediction by x5** (since X_test was set at 20% of the whole data)","6b90de46":"To get a clue on how to start our exploratory data analysis, we can start by **exploring which features may have strong correlation to our target label 'Churn'**.\n\nWe would first map the churn and credit rating to numeric, and then plot a correlation chart","f4cacaa6":"## 0. Executive summary\n**Brief background**  \nA telco company is struggling due to high churn rate, leading to loss in revenue and marketshare.\n\n**Objective**\n1. To build a model to predict customers that may churn\n2. Identify key churn drivers, allowing the company to take concrete action\n\n**Conclusion and recommendation**\n1. We have successfully **built a model that generates 1.5x lift**. Targetting just the top 20% of customers from 50,000 would **save a total revenue of $237,000**\n2. To minimise potential churn, the company should focus on:   \na) maintaining and incentivizing long term customers, and   \nb) creating a flexible plan for people who tend to exceed their monthly quota.\n","c8909a60":"<a id='4'><\/a>\n## 4. Exploratory data analysis (EDA)\nEDA can be a rabbit hole that we got lost in. For this project, we can focus more on exploring those that can **potentially gives us some 'signal' about the churn \/ churn rate**, mainly:\n1. *CreditRating* -  customers with bad credit score may have higher tendency to churn\n2. *CurrentEquipmentDays* - customers may churn after a certain number of years due to better deals\n3. *TotalRecurringCharge* - the more expensive the monthly charges are, the more likely to churn\n4. *OverageMinutes* - the more minutes used outside of the allocated quota, the more expensive the charges are, thus more likely to churn\n5. *RetentionCalls* - if customer receive more retention calls, the more likely he\/she is to be dissatisfied, thus churning.\n\nNote that these are still hypothesis that need to confirm through EDA.","8be05620":"## 1. Background information\nA telco's postpaid business plan is struggling to maintain a strong foothold in local market because of its high churn rate, leading to a steep decline in revenue, customer base, and thus marketshare.\n\nThe company has provided a dataset for ~50,000 of its customers, including various attributes associated with each customer, and a label indicating if they have churned.\n\nThese leads to **2 business objectives**:\n1. To build a model to predict customers that may churn\n2. Identify key churn drivers, allowing the company to take concrete action\n\n\n(*This project is based on BCG RISE's mini project 3 on Telco churn.\nAlthough the rough outline is similar to the original assignment, many steps have been heavily modified to create a more coherent storyline*)","ff19b1d2":"#### Step 2: Aggregate the above df based on decile number","63fcdae9":"<a id='3.1'><\/a>\n### 3.1. Handling of nulls\nWe will first find out which columns contains null, and how big they are.\nThen, we will decide what approach we should take to tackle each cases","1fcc6ffc":"#### Step 1: Construct dataframe containing churn probability for each customer","c7850887":"#### Step 3: Plot the lift and gains chart","15dfab54":"#### Conclusion:\n1. Before anything, we need to acknowledge that **there are no strong predictors**, as their coefficients are relatively low (i.e. compared to 1). That being said, we can still try to derive some conclusion to provide some practical recommendation.\n2. First, what we notice are **features related to customer's lifetime in the company** (e.g. CurrentEquipmentDays, MonthsInService,AgeHH1). Perhaps we can look into how attractive the telco is for long-term customers, and whether effort have been put to make customer stays longer and have a good experience, instead of focusing most of efforts on acquiring new customers.\n3. Second, there are also **features indicating over-usage** (PercChangeMinutes, MonthlyMinutes,PercChangeRevenues, etc). We can consider looking into areas to accomodate customers that continually exceed their allocated quota, and try to offer a more flexible plan (as an exceeded quota is often far more expensive than subscribing to a subscription with larger quota)","6b99c749":"<a id='6.6'><\/a>\n### 6.6. Which features are the most important predictor of churn?","e516f07f":"<img src= \"https:\/\/www.fintechfutures.com\/files\/2020\/08\/Telco.png\" style='width: 800px;'>","af17ecd9":"<a id='7'><\/a>\n## 7. Creating lift chart to quantify business impact\nWe will plot and compute lift chart, to quantify total monthly revenue saved if we target top 20% of the customers based on their churn probabilities.","9b462631":"As we can see, there are those that has less than 1% missing values, and 2 that has more. We shall:\n1. Drop rows for those columns that's below 1%, and\n2. Impute values for the remaining 2","977eb302":"As we can see above, **by adjusting the threshold to 0.4, it gives us more than 4x improvement in recall (0.30 vs 0.07)**, with just a reasonably slight drop in precision (0.46 vs 0.60). \n\n\nThus, to capture more True churn, we should adjust our model's threshold accordingly to 0.4 for future predictions.","3f2d5fc1":"<a id='6.1'><\/a>\n### 6.1. Pre-processing","672323d9":"There are a few things we can note here:\n1. HandsetPrice (row 17) is supposed to be numeric, not object. We need to further explore this, and try fixing the error.\n2. ServiceArea has 747 unique values, which means one-hot encoding will generate 747 new columns! We will try to find a way to cluster these categorical values later on.","d96478b2":"Also, as we can see above, there seems to be AgeHH1 = 0, which doesn't make sense, as you probably need to be of a legal age (18yo) to be able to hold a telco account. So we will impute the 0s based on the median too","13263eb0":"<a id='4.2'><\/a>\n### 4.2. Plotting continuous features vs churn","c42a8326":"<a id='6.4'><\/a>\n### 6.4. Comparing the 2 models\nFrom the 2 model that we have trained earlier, we would be comparing their metrics to see which one performs better","da7a7576":"<a id='6.5'><\/a>\n### 6.5. Adjustment of churn probability threshold\n\n\nAs we can see from the classification metrics above, the recall is rather low for both model when we use the default threshold of 0.5.\n\nWe will try to **adjust the threshold** of our predicted probabilities, to **try get a better recall at ~0.2 (up from 0.07)**\n\nAlso, we would **use the random forest for this threshold tuning**, since it is has better performance metrics in general (precision, recall, etc)\n\n","cabc5e2a":"<a id='3.2'><\/a>\n### 3.2. Handling of categorical features\nTo deal with categorical variable, we would be using one hot encoding for most columns, instead of just mapping the categories to an integer (i.e. 1,2,3, etc).\nReason being is that, *we do not want to imply some sort of order when there is none in the first place* (nominal vs ordinal category).\n\nThe only one that we will use integer encoding is the CreditRating, because it is an ordinal categorical feature (which we have performed earlier)","9b80283e":"<a id='5'><\/a>\n## 5. Feature engineering\nCreating features that may help to predict churn, mainly:\n1. *Ratio between recurring charge and total monthly revenue* - if someone is paying disproportionately extra compared to their regular monthly charge, they may churn\n2. *Ratio between Overage minutes and total minutes spent a month* - similar to above, if someone keeps exceeding their quota by a big percentage, then the charges may rake up a lot and thus driving them to quit\n\nAgain, these are still hypothesis; the truth shall be revealed once the model has been fitted. ","958e0631":"Seems like **income group** is one that has relatively high correlation. Let's plot it out.  \n(AgeHH2 has pretty high correlation, but secondary holder may not directly affect primary holder's age)","aefc4c36":"#### Conclusion:\n\n1. By targeting the top 20% of customer based on their churn probabilities, we would be **capturing about 30% of actual churn**, which represents about **1.5x lift** from just random selection (baseline)\n2. Thus, if we can target the top 20% from the original ~50,000 customer base, we would be able to **save a total revenue of about $237,000**","2a920aaa":"<a id='4.1'><\/a>\n### 4.1. Credit Rating","caac7498":"<a id='8'><\/a>\n## 8. Recommendation\n1. We have successfully built **a model that generates 1.5x lift**. Targetting just the top 20% of customers from 50,000 would **save a total revenue of $237,000**\n  \n2. To minimise potential churn, the company should focus on:  \na) maintaining and incentivizing long term customers, and  \nb) creating a flexible plan for people who tend to exceed their monthly quota."}}