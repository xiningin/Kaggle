{"cell_type":{"ea5a7c5c":"code","f4910e4d":"code","438840f3":"code","f6590e4b":"code","257cee6e":"code","17652f47":"code","8eb1c7c4":"code","cb469d07":"code","195f3367":"code","c4f84a3c":"code","9df2d0d2":"code","8df34100":"code","c6036a49":"code","1fe73308":"code","80e5c444":"markdown","02812f69":"markdown","dc83b790":"markdown","d73774c9":"markdown","12de69a6":"markdown","9a4482f0":"markdown","0894eaef":"markdown","b480d129":"markdown","3841874c":"markdown","81c2e86a":"markdown","5027a525":"markdown","481ab516":"markdown","c77a352a":"markdown","e685fa88":"markdown","9b3d78ba":"markdown","5b38144c":"markdown","7a942974":"markdown","a58fc69b":"markdown","98bfe09c":"markdown","4118bbfc":"markdown","6630483e":"markdown","f449fa07":"markdown","86b6fbc9":"markdown"},"source":{"ea5a7c5c":"'''example of perceptron using scikit learn'''\nimport numpy as np \nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import Perceptron \n\niris = load_iris()\nX = iris.data[: ,  (2 , 3)] # petal lenght and petal width\ny = (iris.target == 0 ).astype(np.int) # binary classification \n\nclassifier = Perceptron(max_iter = 1000 ,tol = 1e-3 ,random_state = 42)\nclassifier.fit(X , y)\ny_pred = classifier.predict([[1.5 , 0.5] , [3 , 3.2]])\n\nprint(y_pred)","f4910e4d":"import tensorflow as tf\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits \n\ndigits = load_digits()\nm , n = digits.data.shape\n\nn_inputs = n \nn_hidden_neurons_1 = 300\nn_hidden_neurons_2 = 100\nn_outputs = 10 # classes 0 to 9","438840f3":"X = tf.placeholder(tf.float32 ,shape = (None , n_inputs) , name = 'X')\ny = tf.placeholder(tf.int64 , shape = (None) , name = 'y')","f6590e4b":"def fetch_batch(epoch, batch_index, batch_size):\n    np.random.seed(epoch * n_batches + batch_index)      \n    indices = np.random.randint(digits.data[:1700].shape[0] , size = batch_size)  \n    X_batch = digits.data[:1700][indices] \n    y_batch = digits.target[:1700][indices] \n    return X_batch, y_batch","257cee6e":"def neuron_layer(X , n_neurons , name , activation = None):\n    with tf.name_scope(name):\n        n_inputs = int(X.get_shape()[1])\n        stddev = 2 \/np.sqrt(n_inputs)\n        '''initializing weights randomly , using truncated normal distribution\n        with standard deviation of 2\/sqrt(n_inputs)'''\n        init = tf.truncated_normal((n_inputs , n_neurons) , stddev = stddev)\n        W = tf.Variable(init , name = 'weights')\n        '''biases'''\n        b = tf.Variable(tf.zeros([n_neurons]) , name = 'bias')\n        '''weighted sum'''\n        Z = tf.matmul(X , W) + b\n        '''activation function'''\n        if activation is not None:\n            return activation(Z)\n        else:\n            return Z","17652f47":"with tf.name_scope('dnn'):\n    hidden_layer_1 = neuron_layer(X = X , n_neurons = n_hidden_neurons_1 ,\n                                  name = 'hiddenLayer1' , activation = tf.nn.relu)\n    hidden_layer_2 = neuron_layer(X = hidden_layer_1 , n_neurons = n_hidden_neurons_2 ,\n                                 name = 'hiddenLayer2' , activation = tf.nn.relu)\n    logits = neuron_layer(X = hidden_layer_2 , n_neurons = n_outputs , name = 'outputs')","8eb1c7c4":"with tf.name_scope('loss'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y , \n                                                              logits = logits)\n    loss = tf.reduce_mean(xentropy , name = 'loss')","cb469d07":"learning_rate = 0.01 \nwith tf.name_scope('train'):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)","195f3367":"with tf.name_scope('eval'):\n    correct = tf.nn.in_top_k(logits , y , 1)\n    accuracy = tf.reduce_mean(tf.cast(correct , tf.float32))","c4f84a3c":"init = tf.global_variables_initializer()\nsaver = tf.train.Saver()","9df2d0d2":"n_epochs = 1000\nbatch_size = 50 \nn_batches = int(np.ceil(digits.data.shape[0] \/ batch_size))\n\nwith tf.Session() as session:\n    init.run()\n    for epoch in range(n_epochs):\n        \n        for batch_index in range(n_batches):\n            X_batch , y_batch = fetch_batch(epoch , batch_index  , batch_size)\n            session.run(training_op , feed_dict = {X : X_batch , y : y_batch})\n            \n        acc_train = accuracy.eval(feed_dict = {X : X_batch , y : y_batch})\n        acc_test = accuracy.eval(feed_dict = {X : digits.data[1700:] , y : digits.target[1700:]})\n        if epoch % 100 == 0 :\n            print('epoch : {0}, Train Acc : {1}  , Test Acc : {2}'.format(epoch ,acc_train , acc_test))\n    save_path = saver.save(session , '.\/model_final.ckpt') ","8df34100":"with tf.Session() as session:\n    saver.restore(session , '.\/model_final.ckpt')\n    X_test = digits.data[1780:]\n    Z = logits.eval(feed_dict = {X : X_test})\n    y_pred = np.argmax(Z , axis = 1)\n\nplt.figure(1 , figsize = (15 , 10))\nfor n in np.arange(1 , 17):\n    plt.subplot(4 , 4 , n )\n    plt.subplots_adjust(hspace = 0.3 , wspace=0.3)\n    plt.imshow(digits.data[1780:][n].reshape(8 , 8)  ,cmap = matplotlib.cm.binary , interpolation=\"sinc\")\n    x_l = \"True : {0} , Predicted : {1}\".format(digits.target[1780:][n] , y_pred[n])\n    plt.xlabel(x_l)\n    \nplt.show()","c6036a49":"'''creating an input function which returns X , y'''\n'''reason to create this input function is because tf.estimator.DNNClassifier() class\nmethods train() , eval() and predict() needs a function in the parameter which returns \nX and y'''\ndef input(df):\n    return df.data[:1789] , df.target[:1789].astype(np.int32)\n\n'''tf.feature_column are used to convert some sort of input data feature into continuous \nvariables that can be used by a regression or neural network model.'''\n'''In the case of a numeric_column, there is no such conversion needed,\nso the class basically just acts like a tf.placeholder.'''\nfeature_col = [tf.feature_column.numeric_column('x' , shape = [8 , 8])] #shape = (8,8) cause digits data images are of 64 pixel\n\n\n'''creating architecture of DNN with two hidden layer with 300 and 100 neurons \nrespectively and a softmax output layer with 10 neurons'''\ndnn_clf = tf.estimator.DNNClassifier(hidden_units = [200 , 100] ,\n                                     n_classes = 10  ,\n                                     feature_columns = feature_col \n                                    ) \n\n\n'''Defining the training inputs'''\ninput_fn_train = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": input(digits)[0]}, # X\n    y=input(digits)[1], # y\n    num_epochs=None,\n    batch_size=50,\n    shuffle=True # randomness\n)\n\n'''training the classifier'''\ndnn_clf.train(input_fn = input_fn_train , steps = 1000 )\n\n'''Defining the Eval inputs'''\ninput_fn_eval = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": input(digits)[0]},\n    y=input(digits)[1],\n    num_epochs=1,\n    shuffle=False\n)\n\n'''evaluating'''\nmetrics = dnn_clf.evaluate(input_fn=input_fn_eval , steps=10)\n\n'''Defining the predict inputs'''\ninput_fn_predict = tf.estimator.inputs.numpy_input_fn(x = {\"x\" : digits.data[1788:]} , \n                                                          num_epochs = 1 , \n                                                          shuffle = False\n                                                         )\n'''predicting'''\npredictions = dnn_clf.predict(input_fn= input_fn_predict)\n\n'''Note you should always split the data into train , eval and test data which i\nhave not done , because this is just an example.'''","1fe73308":"'''getting the predicted classes'''\ncls = [p['classes'] for p in predictions]\n'''converting into int array'''\ny_pred = np.array(cls , dtype = 'int').squeeze()\n\n'''ploting true value and predicted value'''\nplt.figure(1 , figsize = (15 , 10))\nfor n in np.arange(1 , 9):\n    plt.subplot(4 , 4 , n )\n    plt.subplots_adjust(hspace = 0.3 , wspace=0.3)\n    plt.imshow(digits.data[1788:][n].reshape(8 , 8)  ,cmap = matplotlib.cm.binary , interpolation=\"sinc\")\n    x_l = \"True : {0} , Predicted : {1}\".format(digits.target[1788:][n] , y_pred[n])\n    plt.xlabel(x_l)\n    \nplt.show()","80e5c444":"**now lets predict on unseen data.**","02812f69":"* Now we will create two **placeholder nodes** to represent training data and targets.","dc83b790":"# Perceptron \n* Perceptron is one of the simplest ANN architectures. \n* It uses **Linear Threshold Unit (LTU)**.\n   * The **LTU** computes a weighted sum of its input i.e $$ z = w_{0}x_{0} +  w_{1}x_{1} +  w_{2}x_{2} + ...... + w_{n}x_{n}$$  **Note** $ w_{0}x_{0}$ , are the bias term.\n   * Then applies a **step function** to that sum and outputs the result : $h_{w}(x) = step(z) = step(w^T \\cdotp x)$ \n![pann](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*n6sJ4yZQzwKL9wnF5wnVNg.png)\n\n<center><b>(one of the most simplest ANN architecture)<\/b><\/center>\n\n* The most common step function used in Perceptrons is the **Heaviside step function** :\n![hz](http:\/\/math.feld.cvut.cz\/mt\/txtb\/4\/gifa4\/pc3ba4lc.gif)\n* You can even use**Sign Step Function** :\n![sf](https:\/\/helloacm.com\/wp-content\/uploads\/2016\/10\/math-sgn-function-in-cpp.jpg)\n","d73774c9":"* **Now , we need to create two hidden layers and the output layer.**\n* We will use ReLU activation function for hidden layers.\n* Softmax activation function for output layer.\n\n**Let's create a neuron_layer() function that we will use to create one layer at a time. Parameters will be the inputs , the number of neurons a,  the activation function and the name of the layer**","12de69a6":"# Backpropagation Training algorithm\n* In simple words we can describe Backpropagation training algorithm as **Gradient Decent using reverse-mode autodiff (automatic differentiation)**.\n* # Working of **Backpropagation Training Algorithm** :-\n    * First for each training instances it makes a prediction.(forward pass)\n    * Then it measures the error.\n    * Then goes through each layer in reverse to measure the error contribution from each connection.(reverse pass)\n    * Finally it tweaks the connection weights to reduce the error.\n    \n* **In order for this algorithm to work properly , the authors made a key change to the MLP's architecture** : they replaced the step function with the **Logistic or Sigmoid Function** $$ \\sigma (z)  = \\frac{1}{1+ e^{-z}} $$\n* This was important because step function contains only flat segments , Gradient descent cannot move on a flat surface.\n* While **Logistic Function** has a non - zero derivate every where, allowing Gradient descent to make some progress at every step.\n\n![lgf](https:\/\/www.researchgate.net\/profile\/Farid_Najafi\/publication\/268874045\/figure\/fig1\/AS:295410389274629@1447442733860\/Graph-of-the-Logistic-function-and-its-derivative-function.png)","9a4482f0":"**Note logits is the output of the nn before going through the softmax activation \nfuntion. we will handle softmax computation futher**","0894eaef":"# Training a Perceptron\n* It calculates the error made by the network; it does not reinforce connections that lead to the wrong output.\n* More specifically , the Perceptron is fed one training instance at a time, and for each instance it makes its predictions .\n* For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction.\n* Perceptron learning rule equation:\n$$w_{i,j}^{(next step)} = w_{i ,j} + \\alpha (y_{j} - \\hat{y}_{j})x_{i}$$\n   * $w_{i,j}$ is the connection weight between the $i^{th}$ input neuron and the $j^{th}$ output neuron.\n   * $x_{i}$ is the $i^{th}$ input value of the current training instance.\n   * $\\hat{j}_{j}$ is the output of the $j^{th}$ output neuron of the current training instance.\n   * $y_{j}$ is the target output (actual output) of the $j^{th}$ output neuron for the current training instance.\n   * $\\alpha$ is the learning rate.\n* The decision boundary of each neuron is linear , therefore **Perceptrons are incapable of learning complex patterns**.\n\n* Scikit - Learn provides a Perceptron class that implements a single LTU network ","b480d129":"# Implementing Deep Nerual Network from Scratch using TensorFlow's lower-level API.\n\n* We will implement **Mini-batch Gradient Descent** to train DNN on Digits dataset.\n* We will divide this implementation into 2 parts :\n   1. **Construction Phase.**\n   2. **Execution Phase.**\n   \n# Construction Phase \n\n* First we will import tensroflow library , the Digits dataset from sklearn.\n* Then we will specify the number of inputs and ouputs , and set the number of hidden neurons in each layer\n(architecture : 1 input layer , 2 hidden layer with 300 and 100 nodes each and a output layer)","3841874c":"* **Now  we need to define a GradientDescentOptimizer that will tweak the model parameters to minimize the cost function.**","81c2e86a":"<center><h1>Artificial Neural Networks for beginner <\/h1><\/center>\n**In this chapter we will cover the following topics**\n* Introduction \n* Artificial Neural Networks.\n* Multi layer Perceptrons.\n* Deep Neural Networks , with implementation from scratch.\n---------------------------------------------------------------------------------------------------------------------------------\n![nnw](https:\/\/i1.wp.com\/thedatascientist.com\/wp-content\/uploads\/2015\/12\/cool_neural_network.jpeg?resize=870%2C435)\n\n---------------------------------------------------------------------------------------------------------------------------------\n # Neurons\n![neurons](https:\/\/synapseweb.clm.utexas.edu\/sites\/default\/files\/styles\/os_files_large\/public\/synapseweb\/files\/neuron.gif?m=1484252011&itok=3yWNfD_i)\n* A neuron (also called neurone or nerve cell) is a cell that carries electrical impulses. Neurons are the basic units of the nervous system. Every neuron is made of a cell body (also called a soma), dendrites and an axon. Dendrites and axons are nerve fibres.\n   * Neuron cells are located all over the brain.\n   * Neuron's have an **input wire** called as 'Dendrites'   , and a **cell body** where all the processing work is done  ,  and an  **output wire** are called as 'Axon'.\n   * Axon passes the message to other neurons , the neurons are connected to each other by Axon of one neuron to other neurons Dendrite.\n   * And the message is tranfered with an electric pulse.\n \n* **Auditory Cortex**\n   * The auditory cortex is the part of the temporal lobe that processes auditory information in humans and other vertebrates. It is a part of the auditory system, performing basic and higher functions in hearing, such as possible relations to language switching.\n   * **If we re-wire it to the eye , the Auditory Cortex can learn to see.**\n* **Somatosensory Cortex**\n   * The somatosensory cortex receives all sensory input from the body. Neurons that sense feelings in our skin, pain, visual, or auditory stimuli, all send their information to the somatosensory cortex for processing.\n   * **If we also re-wire it to the eye , the Somatosensory Cortex can learn to see.**\n---------------------------------------------------------------------------------------------------------------------------------\n","5027a525":"* Creating a function that creates mini - batches of the dataset.","481ab516":"**Now lets use neuron_layer() function to create Deep Neural Network**\n* The first hidden layer will take X as input \n* The second hidden layer will take the output of first hidden layer as its input \n* The output layer takes the output of the second hidden layer as its input","c77a352a":"**The following code , is used for evaluation of the model.**","e685fa88":"# Multi-layer Perceptron \n* Some of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons , the resulting **Artificial Neural Network** is called as a **Multi-Layer Perceptron**. It can solve non linear problems.\n* A Multi layer perceptron architecture consists of :\n     * One input layer.\n     * One or more layers of LTU's called hidden layer.\n     * One final layer of LTU's is called output layer.\n     \n     \n# When an ANN has two or more hidden layer's it is known as DEEP NEURAL NETWORK\n\n![mlp](https:\/\/miro.medium.com\/max\/804\/1*xxZXeKfVKTRqh54t10815A.jpeg)\n\n* For training MLP's we use **backpropagation training algorithm**.","9b3d78ba":"# Execution Phase","5b38144c":"**The Construction phase is done , how lets create a node to initialize all the variable and we  will also create a saver to save our trained model parameters to disk**","7a942974":"* A single **LTU** can be used for **simple linear binary classification**. It computes a linear combination of inputs and if the result exceeds a threshold , it outputs positive class or else the negative class.\n* A Perceptron is simply composed of a single layer of LTU's, with each neuron connected to all the inputs . These connections are often represented using special pass through neurons called input neurons: they jus output whatever the input they are fed , an extra bias feature is generally added ($x_{0} = 1 $). The bias feature is typically represented using a special type of neuron called a bias neuron , which just outputs 1 all the time.\n\n![p3](https:\/\/drive.google.com\/uc?export=view&id=1qgicvYULvDG1RB5on5HKJrzysnUWz8-R)\n\n* Above is the diagram of perceptron with two inputs and three outputs . This Perceptron can classify instances simultaneously  into three different binary classes , which makes it a multioutput classifier\n\n**Note : Perceptron does not have  a hidden layer if it has a hidden layer then it is called as Multi Layer Perceptron**","a58fc69b":"![epochvserro](https:\/\/drive.google.com\/uc?export=view&id=1wkIVHHvzaEmezCYf8lLaqUCe8sgLJxPY)\n* The following code  , opens a TensorFlow sesssion , and it runs the init node , that initializes all the variables.\n* Then it runs the main training loop : at the each epoch , the code iterates through a number of mini - batches that corresponds t the training set size\n* Each mini- batch is fetched from the fetch_batch() function , then the code simply runs  the training operation , feeding it the current mini - batch input data and targets.\n* Next , at the end of each epoch , the code evaluates the mode on the last mini batch and on the full test set , and it prints out the result, Finally the model parameters are saved to the disk","98bfe09c":"# Implementation of DNN with TensorFlow's High-Level API","4118bbfc":"# Artificial Neural Network (ANN)\n* Now , that we know how a neuron works in the brain . \n    * First the input is passed from **Axon** to the **cell body** for some processing and then it is passed to the final output wire **Dendrites**.\n* Artificial Neural Networks also follows the same principle.\n* Let's see the Architecture of an ANN .\n![anna](https:\/\/drive.google.com\/uc?export=view&id=1whPd5r8yhJMd-IdMIEcQDDqVZEfsruRQ)\n * The round shapes are called as **nodes or artificial neurons or neurons**.\n * The arrows represent the **output from one neuron to the  input of other neuron**.\n\n* **Input Layer** :- The first layer is called the **input layer** , each node in the input layer represents a **feature variable** and one **bias variable $x_{0}$** . \n* **Hidden Layer** :- The middle 2 layer's are called the **hidden layer's** , there can be **n** number of hidden layers and **n** number of nodes or neurons in each hidden layer.  We will understand what the hidden layer does later. In the above figure we have 2 hidden layer . **Note**: just like input layer it should contain a bias term , but it should not be connected to previous layer. \n* **Output Layer** :- The final layer is called the **output layer** , the number of  nodes in the output layer depends upon the number of classes we have to predict , example in **iris** dataset we have 3 target classes , so we will have 3 nodes in the output layer.\n\n**Now that we know the architecture of Artificial Neural Netwroks , lets understand the working of the ANN.**\n* We will use **Perceptron and Multi-layer Perceptron** to understand the working of ANN, which is very easy to understand .","6630483e":"**Now that we have the neural network ready , we need to define the cost function that we will use to train the network.**\n* We are going to use **cross entropy cost function** to train the neural network.\n* Cross entropy will penalize models that estimate a low probability for the target class.\n\n**TensorFlow provides several functions to compute cross entropy , we will use sparse_soft_max_cross_entropy_with_logits()**\n* It computes the cross entropy based on the **logits** and it expects labels in the form of integers ranging from 0 to the number of classes minus 1 , in our case , from 0 to 9.\n* It will give us a 1D tensor containing the cross entropy for each instance. We can then use TensorFlow's **reduce_mean()** function to compute the mean cross entropy over all instances.","f449fa07":"* **MLP is often used for classification , with each output corresponding to a different binary class. When the classes are exclusive (e.g  classes 0 to 9 for digits image classification) the output layer is typically modified by replacing the individual activation functions by a shared SOFTMAX FUNCTION**. $$ \\sigma(z) = \\frac{exp(z)}{\\sum_{j = 1}^{K} exp(z)}$$\n  * K is the number of classes.\n* The output of each neuron corresponds to the estimated probability of the corresponding class.\n* Cross Entropy cost function is used to measure loss in Softmax Function : $$j(\\theta) = -\\frac{1}{m} \\sum_{i = 1}^{m} \\sum_{k = 1}^{K}  y_{k}^{(i)}  log (\\hat{p}_{k}^{(i)})$$ \n    * $\\hat{p}$ is the predictedt probability\n* Cross entropy gradient Vector for class k:\n$$\\bigtriangledown_{\\theta^{(k)}} j(\\theta) = \\frac{1}{m} \\sum_{i = 1}^{m} (\\hat{p}_{k}^{(i)} - y_{k}^{(i)})  x^{(i)} $$\n**Note that the signal flows only one direction , so this architecture is an example of a FEEDFORWARD NEURAL NETWORK(FNN)**\n![p23](https:\/\/drive.google.com\/uc?export=view&id=1iascQ7p-8t2WLWebis1BzD4Ga9B2DR-M)","86b6fbc9":"* We can use other activation functions to , instead of logistic function .Two other popular activation functions are :\n     * The hyperbolic tangent function : $$ tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$$\n         * just like the **Logistic Function** it is s-shaped , continuous , differentiable , but its output value in the range from -1 to 1 , which tend to make each layers output more or less normalized at the beginning of training, This often helps speed up the convergence.\n     * The ReLU function : $$ ReLU(z) = max(0 , z)$$\n         * It is continuous but not differentiable at  z = 0 , the slope changes abruptly , which can make Gradient Descent bounce around . It works very well and has the advantage of being fast to compute . Most importantly , the fact that it does not have a maximum output value also helps reduce some issues during Gradient Descent.\n         "}}