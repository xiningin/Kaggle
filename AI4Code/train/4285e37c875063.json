{"cell_type":{"3f6ca545":"code","e1baec6a":"code","65efa50b":"code","eda92f8d":"code","bbfdf460":"code","8f6c3484":"code","80e2ad69":"code","2ad6b6d9":"code","0cb849a7":"code","21cb173e":"code","8688d841":"code","98550b55":"code","a025961f":"code","26907f28":"code","16dfaf54":"code","6618b0bd":"code","99b0edfd":"code","6c3724d0":"code","e3485abd":"code","a5cfe9e7":"code","a03143c6":"code","36e8c803":"code","f1fc3c66":"code","102c94e3":"code","dd785f44":"code","52ec5e33":"code","63414819":"code","41eb69bb":"code","7e52ecfa":"code","93dc2c08":"code","29ff5ecf":"code","6bacb8ae":"code","58308606":"code","ec0f199e":"code","977a9657":"code","4e9c4022":"code","27b4386e":"code","29759cad":"code","8df4e63e":"code","fb24a14b":"code","c7584023":"markdown","a339ad7c":"markdown","65011980":"markdown","e9646247":"markdown","6bb06bc5":"markdown"},"source":{"3f6ca545":"!pip install -q efficientnet\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom IPython.core.display import display, HTML\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport efficientnet.tfkeras as efn\nAUTO = tf.data.experimental.AUTOTUNE\n\nimport tensorflow as tf, re, math\n","e1baec6a":"def get_strategy():\n    # Detect hardware, return appropriate distribution strategy\n    gpu = \"\"\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n        \n        # GPU detection\n        \n    except ValueError:\n        tpu = None\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n        gpu = tf.config.list_physical_devices(\"GPU\")\n        if len(gpu) == 1:\n            print('Running on GPU ', gpu)\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        GCS_PATH = KaggleDatasets().get_gcs_path('siim-isic-melanoma-classification')\n    elif len(gpu) == 1:\n        strategy = tf.distribute.OneDeviceStrategy(device=\"\/gpu:0\")\n        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\":True})\n        GCS_PATH = \"\/kaggle\/input\/siim-isic-melanoma-classification\/\"\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n        GCS_PATH = \"\/kaggle\/input\/siim-isic-melanoma-classification\/\"\n\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    base_dir = \"\/kaggle\/input\/siim-isic-melanoma-classification\/\"\n    return strategy, GCS_PATH, base_dir\n\nstrategy,GCS_PATH, base_dir = get_strategy()","65efa50b":"mm = 1; rr= 1\nf = open(\"log-{mm}-{rr}.txt\",\"a\")\nf.write(\"LR, Val Score\")\nf.close()","eda92f8d":"print(\"Content of base directory: {}\".format(\",\".join(os.listdir(base_dir))))","bbfdf460":"train_data = pd.read_csv(\"\/kaggle\/input\/siim-isic-melanoma-classification\/train.csv\")\nprint(\"Num Rows in train_data: {}\".format(train_data.shape[0]))\nprint(display(HTML(train_data.head(1).to_html())))\nprint(train_data[\"target\"].value_counts())","8f6c3484":"train_data[\"anatom_site_general_challenge\"].fillna(\"Unknown\", inplace=True)\ngroup_data = train_data.groupby([\"anatom_site_general_challenge\"])[\"benign_malignant\"].value_counts().unstack(-1)\ngroup_data[\"perc_malignant\"] = np.round((group_data[\"malignant\"] * 100) \/(group_data[\"benign\"] + group_data[\"malignant\"]),2)\ngroup_data","80e2ad69":"IMAGE_SIZE = [224,224]\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one\/height_zoom,zero,zero, zero,one\/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n\ndef transform(image):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM )\n    y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM\/\/2+XDIM+1,DIM\/\/2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM,DIM,3])\n\nimport tensorflow_probability as tfp\n\ndef gaussian_kernel(size: int,\n                    mean: float,\n                    std: float,\n                   ):\n    \"\"\"Makes 2D gaussian Kernel for convolution.\"\"\"\n\n    d = tfp.distributions.Normal(mean, std)\n\n    vals = d.prob(tf.range(start = -size, limit = size + 1, dtype = tf.float32))\n\n    gauss_kernel = tf.einsum('i,j->ij',\n                                  vals,\n                                  vals)\n\n    return gauss_kernel \/ tf.reduce_sum(gauss_kernel)\n\n\ndef smoothing(img):\n    gauss_kernel = gaussian_kernel(7,0.5,1)\n\n    # Expand dimensions of `gauss_kernel` for `tf.nn.conv2d` signature.\n    gauss_kernel = gauss_kernel[:, :, tf.newaxis, tf.newaxis]\n\n    # Convolve.\n    img1 = tf.nn.conv2d(tf.reshape(img[:,:,0], [1,224,224,1]), gauss_kernel, strides=[1, 1, 1, 1], padding=\"SAME\")\n    img2 = tf.nn.conv2d(tf.reshape(img[:,:,1], [1,224,224,1]), gauss_kernel, strides=[1, 1, 1, 1], padding=\"SAME\")\n    img3 = tf.nn.conv2d(tf.reshape(img[:,:,2], [1,224,224,1]), gauss_kernel, strides=[1, 1, 1, 1], padding=\"SAME\")\n    img = tf.reshape(tf.concat([img1,img2,img3], axis=3), [224,224,3])\n    return img","2ad6b6d9":"FEATURE_SET =  {\n      \n}\n\n\ndef parse_rec_train(data):           \n    feature_set = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.int64)\n    }\n    features = tf.io.parse_single_example(data, features= feature_set )\n    return features\n\ndef parse_rec_validate(data):           \n    feature_set = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.int64),\n        'image_name': tf.io.FixedLenFeature([], tf.string)\n    }\n    features = tf.io.parse_single_example(data, features= feature_set )\n    return features\n\ndef parse_rec_test(data):           \n    feature_set = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'image_name': tf.io.FixedLenFeature([], tf.string)\n    }\n    features = tf.io.parse_single_example(data, features= feature_set )\n    return features\n\nseed = 42\ndef process_img(img):\n    img = tf.image.decode_image(img)\n    #img = tf.ensure_shape(img, (1024,1024,3))\n    img = tf.ensure_shape(img, (224,224,3))\n    img = tf.image.resize(img, [224,224])\n    img = tf.ensure_shape(img, (224,224,3))\n    #img = tf.keras.preprocessing.image.random_rotation(img, np.random.randint(360))\n    img = float(img)\/255.00\n    return tf.cast(img, tf.float32)\n\ndef get_img_label(features):\n    target = features[\"target\"]\n    features.pop(\"target\")\n    img = process_img(features[\"image\"])\n    return img, target\n\ndef aug(img):\n    return transform(img)\n\ndef aug1(img):\n    angle_list = [15, 30, 45, 60, 75, 90]\n    img = tfa.image.rotate(img, angle_list[np.random.randint(6)])\n    #img = tf.image.rot90(img,k=np.random.randint(4))\n    img = tf.image.random_flip_left_right(img, seed=seed)\n    img = tf.image.random_flip_up_down(img, seed=seed)\n    return img\n\ndef aug_img_label(img, label):\n    img = aug(img)\n    return img, label\n\ndef aug_img(img):\n    img = aug(img)\n    return img\n\ndef get_img(features, label=None):\n    img = process_img(features[\"image\"])\n    return img\n\ndef get_img_and_name(features, label=None):\n    img = process_img(features[\"image\"])\n    image_name = features[\"image_name\"]\n    return img, image_name\n\ndef get_img_name(features):\n    image_name = features[\"image_name\"]\n    return image_name","0cb849a7":"from sklearn.model_selection import train_test_split\ntfrec_dir = base_dir + \"tfrecords\/\"\ntfrec_dir = base_dir + \"..\/croppedskincancerimagestrain\/\"\ntfrec_files = os.listdir(tfrec_dir)\ntfrec_files_train = [GCS_PATH + \"\/tfrecords\/\" + file for file in tfrec_files if \"test\" not in file]\ntfrec_files_train = [GCS_PATH + \"..\/croppedskincancerimagestrain\/\" + file for file in tfrec_files if file != \"train_\"]\ntfrec_files_train, tfrec_files_valid = train_test_split(tfrec_files_train, test_size=0.2)\ntfrec_files_test = [GCS_PATH + \"\/tfrecords\/\" + file for file in tfrec_files if \"test\" in file]","21cb173e":"dataset_train = tf.data.TFRecordDataset(tfrec_files_train)","8688d841":"for data in dataset_train.take(1):\n    example = tf.train.Example()\n    example.ParseFromString(data.numpy())\n    print(str(example)[-600:])","98550b55":"dataset_train = dataset_train.map(parse_rec_train).map(get_img_label)","a025961f":"def get_img_list(dataset):\n    arr_img = []\n    for img, label in dataset.take(12):\n        arr_img.append(img)\n    return arr_img\n    \ndef show_img(img_list):\n    row=4; col=12;\n    plt.figure(figsize=(20,row*12\/col))\n    x = 1\n    for k in range(2):\n        if k == 0:\n            for img in img_list:\n                plt.subplot(row,col,x)\n                plt.imshow(img)\n                x = x + 1\n        elif k==1:\n            for img in img_list:\n                #img = tf.image.rgb_to_grayscale(img)\n                img = smoothing(img)\n                #img = tf.image.grayscale_to_rgb(img)\n                plt.subplot(row,col,x)\n                plt.imshow(img)\n                x = x + 1\n        else:\n            for img in img_list:\n                img = aug_img(img)\n                plt.subplot(row,col,x)\n                plt.imshow(img)\n                x = x + 1\n\nshow_img(get_img_list(dataset_train)) ","26907f28":"from tensorflow.keras import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, Dense, Input, Flatten, AveragePooling2D, GlobalAveragePooling2D","16dfaf54":"if 1==2:\n    with strategy.scope():\n        model = Sequential([\n            Conv2D(filters=4, kernel_size=(3, 3), input_shape=(224,224,3), name=\"image\", activation=\"relu\"),\n            AveragePooling2D(),\n            Conv2D(filters=16, kernel_size=(3, 3), activation=\"relu\"),\n            AveragePooling2D(),\n            Flatten(),\n            Dense(128, activation=\"relu\"),\n            Dense(1, activation=\"sigmoid\")\n        ])\n        model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.AUC()])\n        model.summary()","6618b0bd":"\nwith strategy.scope():\n    if 1==2:\n        base_model = efn.EfficientNetB6(weights='imagenet', include_top=False, input_shape=(224,224,3))\n        x = base_model.output\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(1024, activation='relu')(x)\n        predictions = Dense(1, activation='sigmoid')(x)\n\n        model = Model(inputs=base_model.input, outputs=predictions)\n    else:\n        model_path = \"\/kaggle\/input\/model-data-pipeline-v18\/model.h5\"\n        model = tf.keras.models.load_model(model_path)\n       \n    opt = tf.keras.optimizers.Adam(lr=0.00001)\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05, name='binary_crossentropy')\n    model.compile(optimizer=opt, loss=loss, metrics=['accuracy',tf.keras.metrics.AUC()])\n    #model.summary()\n","99b0edfd":"BATCH_SIZE = 16 * strategy.num_replicas_in_sync\nDATASET_SIZE = 33126\nsteps_per_epoch=DATASET_SIZE\/\/BATCH_SIZE\ntrain_size = steps_per_epoch * BATCH_SIZE\n\ndef scheduler(epoch, lr):\n    if epoch < 4:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\n    \ncallback = tf.keras.callbacks.LearningRateScheduler(scheduler)","6c3724d0":"if 1==1:\n    from sklearn.model_selection import KFold\n    kf = KFold(n_splits=5)\n\n    bln_aug = False\n    with strategy.scope():\n        tfrec_files_train_all = np.array([GCS_PATH + \"..\/croppedskincancerimagestrain\/\" + file for file in tfrec_files if file != \"train_\"])\n        for train_idx, val_idx in kf.split(range(len(tfrec_files_train_all))):\n            print(train_idx, val_idx)\n            tfrec_files_train = tfrec_files_train_all[list(train_idx)]\n            tfrec_files_valid = tfrec_files_train_all[list(val_idx)]\n            temp_dataset = tf.data.TFRecordDataset(tfrec_files_train).repeat().shuffle(1024).map(parse_rec_train).map(get_img_label)\n            if bln_aug:\n                temp_dataset = temp_dataset.map(aug_img_label, num_parallel_calls=AUTO)\n            dataset_train = temp_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n            dataset_valid = tf.data.TFRecordDataset(tfrec_files_valid).map(parse_rec_train).map(get_img_label).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n            for data,label in dataset_train.take(1):\n                print(data.shape, label.shape)\n\n            model.fit(dataset_train, epochs=1, verbose=1, steps_per_epoch=steps_per_epoch, callbacks=callback, validation_data = dataset_valid)    \n\n    model.save(\"model_tpu.h5\")","e3485abd":"def filter_malign(data):\n    return tf.equal(data[\"target\"], 1)","a5cfe9e7":"tfrec_files_train_all = np.array([GCS_PATH + \"..\/croppedskincancerimagestrain\/\" + file for file in tfrec_files if file != \"train_\"])\n\nmalign_dataset = tf.data.TFRecordDataset(tfrec_files_train_all).map(parse_rec_validate).filter(filter_malign)\n\n#batch_data = dataset_train.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)","a03143c6":"name_list = list(malign_dataset.map(get_img_name).as_numpy_iterator())\nimg_list = []\nfor img in malign_dataset.map(get_img):\n    img_list.append(np.array(img))\n  \nshow_img(img_list[:12])","36e8c803":"pred = model.predict(malign_dataset.map(get_img).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE))\npred=pred.flatten()\nprint(pred[pred>0.5].shape)\nprint(pred[pred<0.5].shape)","f1fc3c66":"arr_pred = []\nfor i in range(5):\n    dataset_test = malign_dataset.map(get_img).map(aug_img).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n    arr_pred.append( model.predict(dataset_test, verbose=1) )\nall_pred = np.concatenate(arr_pred, axis=1)","102c94e3":"pred_mean = all_pred.mean(axis=1)\npred_mean=pred_mean.flatten()\nprint(pred_mean[pred_mean>0.5].shape)\nprint(pred_mean[pred_mean<0.5].shape)\n\npred_max = all_pred.max(axis=1)\npred_max=pred_max.flatten()\nprint(pred_max[pred_max>0.5].shape)\nprint(pred_max[pred_max<0.5].shape)","dd785f44":"df = pd.DataFrame({\"img\":img_list, \"image_name\":name_list, \"pred\": list(pred), \"pred_mean\":list(pred_mean), \"pred_max\":list(pred_max)})","52ec5e33":"df[\"image_name\"] = df[\"image_name\"].map(lambda x: x.decode(\"utf-8\"))\ndf.to_csv(\"malign.csv\",index=False)\n","63414819":"\ndf = pd.merge(df, train_data, on=\"image_name\")\n\ndf[\"Incorrect\"] = df[\"pred_max\"].map(lambda x: 1 if x >= 0.5 else 0)","41eb69bb":"df.head(1)","7e52ecfa":"df[[\"Incorrect\",\"sex\",\"image_name\"]].groupby([\"Incorrect\",\"sex\"]).count().unstack(-1)","93dc2c08":"df[[\"Incorrect\",\"anatom_site_general_challenge\",\"image_name\"]].groupby([\"Incorrect\",\"anatom_site_general_challenge\"]).count().unstack(-1)","29ff5ecf":"df.sort_values(\"pred_max\", inplace=True)\nimg_list = df.head(12)[\"img\"].values\nshow_img(img_list)","6bacb8ae":"dataset_test = tf.data.TFRecordDataset(tfrec_files_test).take(200).map(parse_rec_test)\nimage_name_list = list(dataset_test.map(get_img_name).as_numpy_iterator())","58308606":"BATCH_SIZE = 16 * strategy.num_replicas_in_sync","ec0f199e":"dataset_test_raw = dataset_test.map(get_img)\narr_pred = []\n\nfor i in range(5):\n    dataset_test = dataset_test_raw.map(aug_img).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n    arr_pred.append( model.predict(dataset_test, verbose=1) )\n    \nall_pred = np.concatenate(arr_pred, axis=1)\npred = all_pred.max(axis=1)\npred_mean = all_pred.mean(axis=1)","977a9657":"df = pd.DataFrame({\"image_name\":image_name_list,\"target\":list(pred)})\ndf[\"image_name\"] = df[\"image_name\"].map(lambda x: x.decode(\"utf-8\"))\ndf.to_csv(\"submission.csv\", index=False)","4e9c4022":"df_mean = pd.DataFrame({\"image_name\":image_name_list,\"target\":list(pred_mean)})\ndf_mean[\"image_name\"] = df_mean[\"image_name\"].map(lambda x: x.decode(\"utf-8\"))\ndf_mean.to_csv(\"submission_mean.csv\", index=False)","27b4386e":"print(df.sort_values(\"target\").tail(5))","29759cad":"print(df_mean.sort_values(\"target\").tail(5))","8df4e63e":"df[[\"target\"]].hist()","fb24a14b":"df_mean[[\"target\"]].hist()","c7584023":"### References\n1. [Video by @Chris and @Abhishek for using GPU](https:\/\/www.youtube.com\/watch?v=DEuvGh4ZwaY&feature=youtu.be)\n2. Notebook by Chris Deot (https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96)","a339ad7c":"### Incorrect Image Analysis\nAtleast one image hs lines.\nAlso augmentation of some images is giving black areas, not sure of that could be source of problem.\nOne image is really blurred.\nIn one image the spot is way off center\nSo we will see if by changing some augmentation methods, I can get any better.","65011980":"### Analysis\nLooks like more data needs to be augmented for females\nAlso lower extremity, oral\/genital","e9646247":"### Learnings\n* Using GPU\n* Keeping all layers trainble\n* Label Smoothing\n* Test Time augmentation. I am now confused, if I train enough then given the test imge, model should be able to predict. Will need to experimentations to understand this better.\n* Augmentation at the time of training. I had just done some random augmentation without focussing on the problem. After having a look at the visualizations in  [notebook](https:\/\/www.kaggle.com\/apthagowda\/melanoma-efficientnet-b6-tpu-tta), there has been good learning in this area. So that is the one change I am gong to make. \n* KFold\n","6bb06bc5":"### TFRecord\nTFRecord is a way to store a sequence of binary records."}}