{"cell_type":{"f35beb8e":"code","5fb456cb":"code","f7ec817a":"code","19c2a4c5":"code","bcd7fc8d":"code","9d2867bc":"code","fabb45fc":"code","842efeb0":"code","6c3ed7e1":"code","71197c05":"code","f3000cbe":"code","6f67a100":"code","09831383":"code","d99af2f0":"code","e05ba50b":"code","85835ed1":"markdown","94931756":"markdown","f9bd36a9":"markdown","1a4e3a31":"markdown","44036c78":"markdown","3aaac5f4":"markdown","a6b2cd04":"markdown","0dc3f5d7":"markdown","34763392":"markdown","a7c3d71a":"markdown","113cf093":"markdown","2c4b917c":"markdown"},"source":{"f35beb8e":"%matplotlib inline\nfrom copy import copy\nimport gc\nimport joblib\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport sys\nfrom warnings import simplefilter\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input, backend as K\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Dropout\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import TFBertModel, BertConfig, BertTokenizerFast\n\nsimplefilter('ignore')\nplt.style.use('fivethirtyeight')","5fb456cb":"# limit the GPU memory growth\ngpu = tf.config.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(gpu))\nif len(gpu) > 0:\n    tf.config.experimental.set_memory_growth(gpu[0], True)","f7ec817a":"model_name = 'bert_v13'\n\ndata_dir = Path('..\/input\/commonlitreadabilityprize')\ntrain_file = data_dir \/ 'train.csv'\ntest_file = data_dir \/ 'test.csv'\nsample_file = data_dir \/ 'sample_submission.csv'\n\nbuild_dir = Path('.\/build\/')\noutput_dir = build_dir \/ model_name\ntrn_encoded_file = output_dir \/ 'trn.enc.joblib'\nval_predict_file = output_dir \/ f'{model_name}.val.txt'\nsubmission_file = 'submission.csv'\n\npretrained_dir = '..\/input\/tfbert-large-uncased'\n\nid_col = 'id'\ntarget_col = 'target'\ntext_col = 'excerpt'\n\nmax_len = 205\nn_fold = 5\nn_est = 9\nn_stop = 2\nbatch_size = 8\nseed = 42","19c2a4c5":"output_dir.mkdir(parents=True, exist_ok=True)","bcd7fc8d":"trn = pd.read_csv(train_file, index_col=id_col)\ntst = pd.read_csv(test_file, index_col=id_col)\ny = trn[target_col].values\nprint(trn.shape, y.shape, tst.shape)\ntrn.head()","9d2867bc":"trn[text_col].str.split(' ').apply(len).describe()","fabb45fc":"trn[text_col].str.split(' ').apply(len).hist(bins=50)","842efeb0":"def load_tokenizer():\n    if not os.path.exists(pretrained_dir + '\/vocab.txt'):\n        Path(pretrained_dir).mkdir(parents=True, exist_ok=True)\n        tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-uncased\")\n        tokenizer.save_pretrained(pretrained_dir)\n    else:\n        print('loading the saved pretrained tokenizer')\n        tokenizer = BertTokenizerFast.from_pretrained(pretrained_dir)\n        \n    model_config = BertConfig.from_pretrained(pretrained_dir)\n    model_config.output_hidden_states = True\n    return tokenizer, model_config\n\ndef load_bert(config):\n    if not os.path.exists(pretrained_dir + '\/tf_model.h5'):\n        Path(pretrained_dir).mkdir(parents=True, exist_ok=True)\n        bert_model = TFBertModel.from_pretrained(\"bert-large-uncased\", config=config)\n        bert_model.save_pretrained(pretrained_dir)\n    else:\n        print('loading the saved pretrained model')\n        bert_model = TFBertModel.from_pretrained(pretrained_dir, config=config)\n    return bert_model","6c3ed7e1":"def bert_encode(texts, tokenizer, max_len=max_len):\n    input_ids = []\n    token_type_ids = []\n    attention_mask = []\n    \n    for text in texts:\n        token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n                         add_special_tokens=True)\n        input_ids.append(token['input_ids'])\n        token_type_ids.append(token['token_type_ids'])\n        attention_mask.append(token['attention_mask'])\n    \n    return np.array(input_ids), np.array(token_type_ids), np.array(attention_mask)","71197c05":"tokenizer, bert_config = load_tokenizer()\n\nX = bert_encode(trn[text_col].values, tokenizer, max_len=max_len)\nX_tst = bert_encode(tst[text_col].values, tokenizer, max_len=max_len)\ny = trn[target_col].values\nprint(X[0].shape, X_tst[0].shape, y.shape)","f3000cbe":"joblib.dump(X, trn_encoded_file)","6f67a100":"def build_model(bert_model, max_len=max_len):    \n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"token_type_ids\")\n    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n    sequence_output = bert_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n    clf_output = sequence_output[:, 0, :]\n    clf_output = Dropout(.1)(clf_output)\n    out = Dense(1, activation='linear')(clf_output)\n    \n    model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n    \n    return model","09831383":"def scheduler(epoch, lr, warmup=5, decay_start=10):\n    if epoch <= warmup:\n        return lr \/ (warmup - epoch + 1)\n    elif warmup < epoch <= decay_start:\n        return lr\n    else:\n        return lr * tf.math.exp(-.1)\n\nls = LearningRateScheduler(scheduler)\nes = EarlyStopping(patience=n_stop, restore_best_weights=True)\n\ncv = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n\np = np.zeros_like(y, dtype=float)\np_tst = np.zeros((X_tst[0].shape[0], ), dtype=float)\nfor i, (i_trn, i_val) in enumerate(cv.split(X[0]), 1):\n    print(f'training CV #{i}:')\n    tf.random.set_seed(seed + i)\n\n    bert_model = load_bert(bert_config)\n    clf = build_model(bert_model, max_len=max_len)\n    if i == 1:\n        print(clf.summary())\n    history = clf.fit([x[i_trn] for x in X], y[i_trn],\n                      validation_data=([x[i_val] for x in X], y[i_val]),\n                      epochs=n_est,\n                      batch_size=batch_size,\n                      callbacks=[ls])\n    clf.save_weights(f'{model_name}_cv{i}.h5')\n\n    p[i_val] = clf.predict([x[i_val] for x in X]).flatten()\n    p_tst += clf.predict(X_tst).flatten() \/ n_fold\n    \n    K.clear_session()\n    del clf, bert_model\n    gc.collect()","d99af2f0":"print(f'CV RMSE: {mean_squared_error(y, p, squared=False):.6f}')\nnp.savetxt(val_predict_file, p, fmt='%.6f')","e05ba50b":"sub = pd.read_csv(sample_file, index_col=id_col)\nsub[target_col] = p_tst\nsub.to_csv(submission_file)\nsub.head()","85835ed1":"# Tokenization Using `transformers`","94931756":"Training the model with early stopping and a learning-rate schedulerTraining the model","f9bd36a9":"# Submission","1a4e3a31":"Simple model with only an output dense layer added to the pre-trained BERT model.","44036c78":"## Save Encoded Training Data","3aaac5f4":"# Changelogs","a6b2cd04":"## Print CV RMSE and Save CV Predictions","0dc3f5d7":"This notebook shows how to train a neural network model with pre-trained BART in Tensorflow\/Keras. It is based on @xhlulu's [Disaster NLP: Keras BERT using TFHub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n) notebook.\n\nThis competition is a code competition without access to internet. So we add the `transformers` tokenizer and pre-trained BART base model through Kaggle Datasets instead. You can find the `transformers` tokenizer and pre-trained models for TF\/Keras as follows:\n* [TF BERT Base Uncased Pretrained Model\/Tokenizer](https:\/\/www.kaggle.com\/jeongyoonlee\/tfbert-base-uncased)\n* [TF BERT Large Uncased Pretrained Model\/Tokenizer](https:\/\/www.kaggle.com\/jeongyoonlee\/tfbert-large-uncased)\n* [TF RoBERTa Base Model\/Tokenizer](https:\/\/www.kaggle.com\/jeongyoonlee\/tfroberta-base)\n* [TF RoBERTa Lare Model\/Tokenizer](https:\/\/www.kaggle.com\/jeongyoonlee\/tfroberta-large)\n* [TF BART Base Pretrained Model\/Tokenizer](https:\/\/www.kaggle.com\/jeongyoonlee\/tfbart-base)\n* [TF BART Large Pretrained Model\/Tokenizer](https:\/\/www.kaggle.com\/jeongyoonlee\/tfbart-large)\n\nPlease check out my other notebooks with the same code structure as follows:\n* [PyTorch Lightning RoBERTa (Training\/Inference)](https:\/\/www.kaggle.com\/jeongyoonlee\/pytorch-lightning-roberta-training-inference): PyTorch Lightning RoBERTa using `transformers`\n* [TF\/Keras BART Baseline (Training\/Inference)](https:\/\/www.kaggle.com\/jeongyoonlee\/\/tf-keras-bart-baseline-training-inference): TF\/Keras BART using `transformers`\n\nHope it helps.","34763392":"# Load Libraries and Data","a7c3d71a":"| Version  | CV Score | Public Score | Changes | Comment |\n|----------|----------|--------------|---------|---------|\n| v13 | TBD |   TBD   |  use bert-large instead of bert-base | |\n| v10 | 0.688489 |   0.651   | use transformers' tokenizer\/bert |\n|  v8 | 0.653635 |   0.606   | add 5-fold CV + early-stopping back. | |\n|  v7 | N\/A      |   0.617   | fix the bug in learning rate scheduler | overfitting to train? (n=20) |\n|  v6 | N\/A      |   0.566   | add the warm-up learning rate scheduler | **With a bug. Don't use it** |\n|  v5 | N\/A      |   0.531   | roll back to v3 | |\n|  v4 | N\/A      |   0.573   | add early-stopping | seemed to stop too early with `patience=1` (n=5) |\n|  v3 | N\/A      | **0.530** | initial baseline | |","113cf093":"Simple check to determin `max_len` for tokenizer.","2c4b917c":"# Model Training with Cross-Validation"}}