{"cell_type":{"7ebcd335":"code","f756382a":"code","07879570":"code","259c93f3":"code","751b0380":"code","5a773550":"code","a4f20853":"code","ffbad584":"code","14b06d75":"code","c3edba71":"code","9506413a":"code","1e76170e":"code","2603c6cc":"code","1208f112":"code","4042b4f8":"code","1b32c298":"code","dd7c741f":"code","0497ca27":"code","24c95c1d":"code","76510165":"code","7f2be885":"code","9c147acd":"code","6cfd4706":"code","3f926682":"code","13f31cf5":"code","979acf2e":"code","e3180fbc":"code","e5402be4":"code","be3528ba":"code","4a9aa5a5":"code","fbb41a0e":"code","48767dec":"code","6496e8f7":"code","4a3fd4d1":"code","c322b24f":"code","64a4e6f8":"code","e9264066":"code","168df7fd":"code","fcbfa474":"code","c8322e65":"code","aa850258":"code","6ad8fa52":"code","5e963a5c":"code","271b809a":"code","61a1720a":"code","4f438877":"code","f12c41c3":"code","7fb516a1":"code","83213253":"code","0be5351b":"code","5745b41e":"code","e73a6119":"code","bad92b75":"code","1435f6ff":"code","fdf1da9a":"code","18538adc":"code","2bd3608c":"code","e03a96a0":"code","f726a2d9":"code","def3abde":"code","ee667ebc":"code","1c2691f9":"code","3e3f5420":"code","edbe1be0":"code","cf6c0f91":"code","65d2367e":"code","97db9a2c":"code","c158beef":"code","06e26b32":"code","b5c8cbba":"code","8715030d":"code","201fee27":"code","8bf9e17f":"code","9380d460":"code","5834af93":"code","0b6bee63":"code","8fb9c87b":"code","0f226db1":"code","8cec81ad":"code","d5a63954":"code","d404d132":"code","e47f3aa4":"code","5cafb6d7":"code","260df690":"code","3f13f8f5":"code","77dfd91d":"code","d4d1c85e":"code","2b69f32a":"code","821da8b8":"code","de549a2e":"code","fb3cb929":"code","89badb00":"code","e5e5a24d":"code","86d181df":"code","e37051e3":"code","f4e29b24":"code","d6e9ac33":"markdown","317ac318":"markdown","91b60660":"markdown","a61585a1":"markdown","b1a53625":"markdown","07c1fff5":"markdown","67e0a2b0":"markdown","445cafaa":"markdown","abbbb7b1":"markdown","dd081af1":"markdown","f2f09e02":"markdown","c087d9cd":"markdown","5995eb0a":"markdown","8368aecd":"markdown","0d6a63bb":"markdown","716b89ae":"markdown","fa648b26":"markdown","ce55d0d0":"markdown","ca9f40c9":"markdown","0e1b44b7":"markdown","4b5c8a2b":"markdown","f5caa279":"markdown","50433668":"markdown","8d24d12a":"markdown","beb6efaf":"markdown","789675df":"markdown","0248897c":"markdown","32fd83ae":"markdown","c338e3f6":"markdown","a9fb8343":"markdown","20fe44df":"markdown","7cf2759e":"markdown","a98efff1":"markdown","221966e9":"markdown","f489379e":"markdown","9c523c9f":"markdown","349544f2":"markdown","debbaa45":"markdown"},"source":{"7ebcd335":"# matplotlib\u3067\u65e5\u672c\u8a9e\u3092\u6271\u3048\u308b\u3088\u3046\u306b\n!pip install japanize_matplotlib -Uq\n\n# RainCloud Plot(\u6563\u5e03\u56f3\uff0bBoxPlot\uff0bViolin\u3092\u4e00\u3064\u3067\u8868\u793a)\n!pip install ptitprince -Uq\n\n# \u6b20\u640d\u5024\u3092\u53ef\u8996\u5316\n!pip install missingno -Uq\n\n# \u30d9\u30f3\u56f3\u3092\u4f5c\u6210\n!pip install matplotlib-venn -Uq\n\n# \u30ab\u30c6\u30b4\u30ea\u7279\u5fb4\u4f5c\u6210\u7528\n!pip install category_encoders -Uq","f756382a":"import warnings\nwarnings.simplefilter('ignore')\n\nimport os\nimport gc\ngc.enable()\nimport sys\nimport glob\nimport math\nimport time\nimport random\nimport string\nimport psutil\nimport pathlib\nfrom pathlib import Path\nfrom contextlib import contextmanager\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport japanize_matplotlib\nfrom ptitprince import RainCloud\nfrom matplotlib_venn import venn2\n\n\nfrom tqdm.auto import tqdm as tqdmp\nfrom tqdm.autonotebook import tqdm as tqdm\ntqdmp.pandas()\n\n## Model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nimport lightgbm as lgb\n\nimport category_encoders as ce","07879570":"# \u5b9f\u9a13\u3067\u4f7f\u3046\u30d1\u30e9\u30e1\u30fc\u30bf\u306fConfig\u3067\u7ba1\u7406\u3057\u3066\u3044\u307e\u3059\u3002\n# \u3053\u306e\u5b9f\u9a13\u4f55\u3084\u3063\u305f\u304b\u306a\u3068\u5f8c\u3067\u632f\u308a\u8fd4\u308a\u3084\u3059\u3044\u3088\u3046\u306b\u3001\u306a\u308b\u3079\u304fConfig\u3060\u3051\u898b\u308c\u3070\u308f\u304b\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\n\nclass CFG:\n    \n    def __init__(self):\n        \n        self.debug=False\n        self.seed=42\n        self.n_fold = 5\n        self.environment='Kaggle'  # 'AWS' or 'Kaggle' or 'Colab'\n        self.project='Shiggle_1st',\n        self.exp_name = '001_Baseline'\n        self.objective = 'rmse'\n        self.metric = 'rmse'\n        self.learning_rate = 0.1\n        self.num_boost_round = 1000\n        self.early_stopping_rounds = 30\n        self.num_leaves = 16\n        self.target_encode = False\n        \nCONFIG = CFG()","259c93f3":"import wandb\nfrom wandb.lightgbm import wandb_callback\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_key\")\n\napi_key = secret_value_0\n!wandb login $api_key\n\nif CONFIG.debug:\n    wandb.init(project='shiggle_1st', name=CONFIG.exp_name, mode='offline')\nelse:\n    wandb.init(project='shiggle_1st', name=CONFIG.exp_name)\n\nwandb.config.update(CONFIG)","751b0380":"## \u518d\u73fe\u6027\u78ba\u4fdd\u306e\u305f\u3081\u306eSeed\u56fa\u5b9a\ndef seed_everything(seed:int==42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(CONFIG.seed)","5a773550":"## \u51e6\u7406\u306b\u304b\u304b\u3063\u305f\u6642\u9593\u3068\u4f7f\u7528\u3057\u305f\u30e1\u30e2\u30ea\u3092\u8a08\u6e2c\n@contextmanager\ndef timer(name:str, slack:bool=False):\n    t0 = time.time()\n    p = psutil.Process(os.getpid())\n    m0 = p.memory_info()[0] \/ 2. ** 30\n    print(f'<< {name} >> Start')\n    yield\n    \n    m1 = p.memory_info()[0] \/ 2. ** 30\n    delta = m1 - m0\n    sign = '+' if delta >= 0 else '-'\n    delta = math.fabs(delta)\n    \n    print(f\"<< {name} >> {m1:.1f}GB({sign}{delta:.1f}GB):{time.time() - t0:.1f}sec\", file=sys.stderr)","a4f20853":"# \u500b\u4eba\u7684\u306bAWS\u3084Kaggle\u74b0\u5883\u3084Google Colab\u3092\u884c\u3063\u305f\u308a\u6765\u305f\u308a\u3057\u3066\u3044\u308b\u306e\u3067\u307e\u3068\u3081\u3066\u3044\u307e\u3059\nif CONFIG.environment == 'AWS':\n    INPUT_DIR = Path('\/mnt\/work\/data\/kaggle\/shiggle_1st\/')\n    MODEL_DIR = Path(f'..\/models\/{CONFIG.exp_name}\/')\n    OUTPUT_DIR = Path(f'..\/data\/interim\/{CONFIG.exp_name}\/')\n    \n    os.makedirs(MODEL_DIR, exist_ok=True)\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    print(f\"Your environment is 'AWS'.\\nINPUT_DIR is {INPUT_DIR}\\nMODEL_DIR is {MODEL_DIR}\\nOUTPUT_DIR is {OUTPUT_DIR}\")\n    \n    \nelif CONFIG.environment == 'Kaggle':\n    INPUT_DIR = Path('..\/input\/shigglecup-1st\/DATA\/')\n    MODEL_DIR = Path('.\/')\n    OUTPUT_DIR = Path('.\/')\n    print(f\"Your environment is 'Kaggle'.\\nINPUT_DIR is {INPUT_DIR}\\nMODEL_DIR is {MODEL_DIR}\\nOUTPUT_DIR is {OUTPUT_DIR}\")\n\n    \nelif CONFIG.environment == 'Colab':\n    INPUT_DIR = Path('\/content\/drive\/MyDrive\/kaggle\/Shiggle_1st\/data\/raw')\n    BASE_DIR = Path(\"\/content\/drive\/MyDrive\/kaggle\/Shiggle_1st\/data\/\")\n\n    MODEL_DIR = BASE_DIR \/ f'{CONFIG.exp_name}'\n    OUTPUT_DIR = BASE_DIR \/ f'{CONFIG.exp_name}\/'\n\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    if not os.path.exists(INPUT_DIR):\n        print('Please Mount your Google Drive.')\n    else:\n        print(f\"Your environment is 'Colab'.\\nINPUT_DIR is {INPUT_DIR}\\nMODEL_DIR is {MODEL_DIR}\\nOUTPUT_DIR is {OUTPUT_DIR}\")\n        \nelse:\n    print(\"Please choose 'AWS' or 'Kaggle' or 'Colab'.\\nINPUT_DIR is not found.\")","ffbad584":"with timer('Data Load'):\n    train_df = pd.read_csv(INPUT_DIR \/ 'train.csv')\n    test_df = pd.read_csv(INPUT_DIR \/ 'test.csv')\n    sub_df = pd.read_csv(INPUT_DIR \/ 'sample_submission.csv')\n    \n    print(f'Train: {train_df.shape} | Test: {test_df.shape}')","14b06d75":"process_list = []","c3edba71":"train_df.head().T","9506413a":"# anai\nprocess_name = 'drop \u30cf\u30d4\u30ca\u30b9'\nprocess_list.append(process_name)\nwith timer(process_name):\n    blissey_id = train_df['evolution_chain_id'][train_df['pokemon']=='blissey'].values[0]\n    train_df = train_df[train_df['pokemon']!='blissey']\n    train_df.reset_index(drop=True, inplace=True)","1e76170e":"# # anai\n# process_name = 'drop \u30bf\u30d6\u30f3\u30cd'\n# process_list.append(process_name)\n# with timer(process_name):\n#     train_df = train_df[train_df['pokemon']!='audino']\n#     train_df.reset_index(drop=True, inplace=True)","2603c6cc":"## Train\u3068Test\u3092\u7d50\u5408\u3057\u307e\u3059\n## \u3042\u3068\u3067\u5206\u3051\u3089\u308c\u308b\u3088\u3046\u306bflag\u3092\u4ed8\u4e0e\u3057\u307e\u3059\n\ntrain_df['flag'] = 0\ntest_df['flag'] = 1\n\nprocess_name = 'train test concat'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)","1208f112":"whole_df['ability_1'].value_counts()","4042b4f8":"whole_df['ability_2'].value_counts()","1b32c298":"whole_df['ability_hidden'].value_counts()","dd7c741f":"process_name = 'log transform w\/ target'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df['target'] = whole_df['target'].progress_apply(lambda x: np.log1p(x))\n\n    plt.figure(figsize=(12,5), tight_layout=True)\n    sns.histplot(whole_df['target'], bins=30)\n    plt.show()","0497ca27":"# process_name = 'BMI'\n# process_list.append(process_name)\n# with timer(process_name):\n#     whole_df['BMI'] = whole_df['weight']\/whole_df['height']**2","24c95c1d":"process_name = 'log transform w\/ height & weight'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df['height'] = whole_df['height'].progress_apply(lambda x: np.log1p(x))\n    whole_df['weight'] = whole_df['weight'].progress_apply(lambda x: np.log1p(x))","76510165":"plt.figure(figsize=(8,5), tight_layout=True)\nsns.jointplot(data=whole_df, x='height', y='weight')\nplt.show()","7f2be885":"# anai\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n\nprocess_name = 'SVD'\nprocess_list.append(process_name)\nwith timer(process_name):\n    n_components = 2\n    col_name = 'svd'\n    for n in np.arange(n_components):\n        whole_df[f'{col_name}_{n}'] = 0\n    \n    cat_col = [\n#         'ability_1', 'ability_2', 'ability_hidden',\n        'type_1', 'type_2',\n        'egg_group_1', 'egg_group_2',\n        'shape'\n              ]\n    tmp_df = whole_df[cat_col].fillna('')\n    tmp_df['cat_text'] = tmp_df[cat_col[0]].str.cat([tmp_df[col] for col in cat_col[1:]], sep=' ')\n    count_svd = Pipeline(steps=[\n        (\"CountVectorizer\", CountVectorizer()),\n        (\"TruncatedSVD\", TruncatedSVD(n_components=n_components, random_state=42))\n    ])\n\n    features_svd = count_svd.fit_transform(tmp_df['cat_text'].fillna(\"\"))\n    for n in np.arange(n_components):\n        whole_df[f'{col_name}_{n}'] = features_svd[:,n]","9c147acd":"def count_type(df:pd.DataFrame) -> pd.DataFrame:\n    \n    _df = df.copy()\n    # type_2\u304cnan\u3067\u3042\u308c\u3070\u3001\u30bf\u30a4\u30d7\u6570\u306f1\u3068\u3059\u308b\n    _df['num_type'] = _df['type_2'].progress_apply(lambda x: 1 if x is np.nan else 2)\n\n    return _df","6cfd4706":"process_name = '\u30bf\u30a4\u30d7\u6570\u3092\u30ab\u30a6\u30f3\u30c8'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df = count_type(whole_df)","3f926682":"## type_1\u3068type_2\u306e\u72ec\u81ea\u306e\u3082\u306e\u304c\u306a\u3044\u304b\u3092\u30d9\u30f3\u56f3\u3067\u78ba\u8a8d\u3057\u307e\u3059\nplt.figure(figsize=(8,6))\nvenn2(subsets=[set(train_df['type_1'].unique().tolist()),\n               set(train_df['type_2'].unique().tolist())],\n      set_labels=('A:type_1','B:type_2'))\nplt.title('type_1\u3068type_2\u306e\u30e6\u30cb\u30fc\u30af\u6570',fontsize=20)","13f31cf5":"# anai\n# ## OH Encoding\n# with timer('one encoding w\/ type'):\n#     ohe_df = pd.get_dummies(pd.concat([whole_df['type_1'], whole_df['type_2']], axis=0))\n#     ohe_df = ohe_df.add_prefix('type_')\n#     ohe_df = ohe_df.iloc[:len(whole_df)]+ohe_df.iloc[len(whole_df):]\n#     whole_df = pd.concat([whole_df, ohe_df], axis=1)\n# #     whole_df.drop(['type_1', 'type_2'], axis=1, inplace=True)\n#     display(whole_df.head())","979acf2e":"## Label Encoding\n## Label Encoding\u306fpandas\u306efactorize\u3092\u4f7f\u3046\u3068\u4fbf\u5229\u3067\u3059\n\nprocess_name = 'label encoding w\/ type'\nprocess_list.append(process_name)\nwith timer(process_name):\n    labels, uniques = pd.concat([whole_df['type_1'], whole_df['type_2']], axis=0).factorize()\n    whole_df['type_1'] = labels[:len(whole_df)]\n    whole_df['type_2'] = labels[len(whole_df):]\n    display(whole_df.head())","e3180fbc":"process_name = '\u7a2e\u65cf\u5024\u8a08\u7b97'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df['Base_stats'] = whole_df[['hp', 'attack', 'defense', 'special_attack', 'special_defense', 'speed']].sum(axis=1)\n    display(whole_df.head())","e5402be4":"# # anai\n\n# process_name = 'normal_spec_calc'\n# process_list.append(process_name)\n# with timer(process_name):\n#     whole_df['normal_spec_sum'] = whole_df[['attack', 'defense']].sum(axis=1)\n# #     whole_df['normal_attack_p_defence'] = whole_df['attack']\/whole_df['defense']\n#     display(whole_df.head())","be3528ba":"# # anai\n\n# process_name = 'special_spec_calc'\n# process_list.append(process_name)\n# with timer(process_name):\n# #     whole_df['special_spec_sum'] = whole_df[['special_attack', 'special_defense']].sum(axis=1)\n#     whole_df['special_attack_p_defence'] = whole_df['special_attack']\/whole_df['special_defense']\n#     display(whole_df.head())","4a9aa5a5":"def per_stats(df_, col_name):\n    df_[f'{col_name}_p_Base_stats'] = df_[col_name]\/whole_df['Base_stats']\n    return df_","fbb41a0e":"# # anai\n\n# process_name = 'per_stats'\n# process_list.append(process_name)\n# with timer(process_name):\n#     whole_df = per_stats(whole_df, 'hp')\n#     whole_df = per_stats(whole_df, 'attack')\n#     whole_df = per_stats(whole_df, 'defense')\n#     whole_df = per_stats(whole_df, 'special_attack')\n#     whole_df = per_stats(whole_df, 'special_defense')\n#     whole_df = per_stats(whole_df, 'speed')\n#     display(whole_df.head())","48767dec":"# # anai\n\n# process_name = 'stats_std'\n# process_list.append(process_name)\n# with timer(process_name):\n#     whole_df['stats_std'] = whole_df[['hp', 'attack', 'defense', 'special_attack', 'special_defense', 'speed']].std(axis=1)\n#     display(whole_df.head())","6496e8f7":"def count_ability(df:pd.DataFrame) -> pd.DataFrame:\n    \n    _df = df.copy()\n    # type_2\u304cnan\u3067\u3042\u308c\u3070\u3001\u30bf\u30a4\u30d7\u6570\u306f1\u3068\u3059\u308b\n    _df['num_ability'] = _df['ability_2'].progress_apply(lambda x: 1 if x is np.nan else 2)\n\n    return _df","4a3fd4d1":"process_name = 'ability\u6570\u3092\u30ab\u30a6\u30f3\u30c8'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df = count_ability(whole_df)","c322b24f":"## type_1\u3068type_2\u306e\u72ec\u81ea\u306e\u3082\u306e\u304c\u306a\u3044\u304b\u3092\u30d9\u30f3\u56f3\u3067\u78ba\u8a8d\u3057\u307e\u3059\nplt.figure(figsize=(8,6))\nvenn2(subsets=[set(train_df['ability_1'].unique().tolist()),\n#                set(train_df['ability_2'].unique().tolist()),\n               set(train_df['ability_hidden'].unique().tolist())],\n      set_labels=('A:ability_1','B:ability_hidden'))\nplt.title('ability\u306e\u30e6\u30cb\u30fc\u30af\u6570',fontsize=20)","64a4e6f8":"# # anai\n# # Count Encode  \u80fd\u529b\u306e\u73cd\u3057\u3055\u3092\u8868\u73fe\n# process_name = 'count encoding w\/ all ability'\n# process_list.append(process_name)\n# with timer(process_name):\n#     count = pd.concat([whole_df['ability_1'],\n#                        whole_df['ability_2'],\n#                        whole_df['ability_hidden']], axis=0).value_counts()\n#     whole_df['ability_1_Freq'] = whole_df['ability_1'].map(count)\n#     whole_df['ability_2_Freq'] = whole_df['ability_2'].map(count)\n#     whole_df['ability_hidden_Freq'] = whole_df['ability_hidden'].map(count)\n#     display(whole_df.head().T)","e9264066":"## Label Encoding\n\nprocess_name = 'label encoding w\/ ability'\nprocess_list.append(process_name)\nwith timer(process_name):\n    labels, uniques = pd.concat([whole_df['ability_1'], whole_df['ability_2']], axis=0).factorize()\n    whole_df['ability_1'] = labels[:len(whole_df)]\n    whole_df['ability_2'] = labels[len(whole_df):]\n    display(whole_df.head())","168df7fd":"# Label Encoding\n\nprocess_name = 'label encoding w\/ ability_hidden'\nprocess_list.append(process_name)\nwith timer(process_name):\n    labels, uniques = whole_df['ability_hidden'].factorize()\n    whole_df['ability_hidden'] = labels\n    display(whole_df.head().T)","fcbfa474":"# # anai\n# ## Label Encoding\n\n# process_name = 'label encoding w\/ all ability'\n# process_list.append(process_name)\n# with timer(process_name):\n#     labels, uniques = pd.concat([whole_df['ability_1'], whole_df['ability_2'], whole_df['ability_hidden']], axis=0).factorize()\n#     whole_df['ability_1'] = labels[:len(whole_df)]\n#     whole_df['ability_2'] = labels[len(whole_df):len(whole_df)*2]\n#     whole_df['ability_hidden'] = labels[len(whole_df)*2:]\n#     display(whole_df.head())","c8322e65":"process_name = 'drop color'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df = whole_df.drop(['color_1', 'color_2', 'color_f'], axis=1)","aa850258":"def count_egg_group(df:pd.DataFrame) -> pd.DataFrame:\n    \n    _df = df.copy()\n    # type_2\u304cnan\u3067\u3042\u308c\u3070\u3001\u30bf\u30a4\u30d7\u6570\u306f1\u3068\u3059\u308b\n    _df['num_egg_group'] = _df['egg_group_2'].progress_apply(lambda x: 1 if x is np.nan else 2)\n\n    return _df","6ad8fa52":"process_name = 'egg_group\u6570\u3092\u30ab\u30a6\u30f3\u30c8'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df = count_egg_group(whole_df)","5e963a5c":"## Label Encoding\n\nprocess_name = 'label encoding w\/ egg_group_1_2'\nprocess_list.append(process_name)\nwith timer(process_name):\n    labels, uniques = pd.concat([whole_df['egg_group_1'], whole_df['egg_group_2']], axis=0).factorize()\n    whole_df['egg_group_1'] = labels[:len(whole_df)]\n    whole_df['egg_group_2'] = labels[len(whole_df):]\n    display(whole_df.head())","271b809a":"process_name = 'drop image'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df = whole_df.drop('url_image', axis=1)","61a1720a":"process_name = 'drop shape'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df = whole_df.drop('shape', axis=1)","4f438877":"# anai\n\nprocess_name = 'use mega'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df['mega_flag'] = whole_df['pokemon'].str.contains('-mega')","f12c41c3":"# anai\n\nprocess_name = 'use sub'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df['sub_flag'] = whole_df['pokemon'].str.contains('-')","7fb516a1":"# anai\n\nprocess_name = 'have evolves'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df['have_evolves'] = 0\n    for id_ in whole_df['evolves_from_species_id'].values:\n        if id_:\n            whole_df.loc[whole_df['species_id']==id_, 'have_evolves'] = 1","83213253":"# anai\n\nprocess_name = 'add evolves info'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df['evolves_Base_stats'] = 0\n    for idx, id_ in enumerate(whole_df['evolves_from_species_id'].values):\n        if id_:\n            whole_df.loc[whole_df['species_id']==id_, 'evolves_Base_stats'] = whole_df.loc[idx, 'Base_stats']","0be5351b":"whole_df[whole_df['pokemon'].str.contains('charmander|charmeleon|charizard')].T","5745b41e":"# anai\n# pikachu\u306e\u8aa4\u5dee\u304c\u5927\u304d\u3044\u305f\u3081\u5b9f\u88c5\n\nprocess_name = 'replace sub pokemon'\nprocess_list.append(process_name)\nwith timer(process_name):\n    cols = ['generation_id', 'evolves_from_species_id', \n            'evolution_chain_id', 'shape_id']\n\n    for id_ in tqdm(whole_df['species_id'].unique()):\n        tmp = whole_df[whole_df['species_id']==id_]\n        if tmp[cols].isna().sum().any():\n            for col in cols:\n                if tmp[col].isna().sum():\n                    if len(tmp[col].dropna().unique())==1:\n                        tmp[col].fillna(tmp[col].dropna().unique()[0], inplace=True)\n        whole_df.loc[whole_df['species_id']==id_, cols] = tmp[cols]","e73a6119":"def make_agg_rate(df_, key_col, calc_col):\n    col_name = f'agg_rate_{calc_col}_p_{key_col}'\n    tmp = df_.groupby(key_col)[calc_col].mean()\n    df_[col_name] = np.nan\n    for val in tmp.index:\n        idx = df_[key_col]==val\n        df_.loc[idx, col_name] = df_[calc_col][idx] \/ tmp.loc[val]\n    return df_\n\ndef make_agg_diff(df_, key_col, calc_col):\n    col_name = f'agg_diff_{calc_col}_p_{key_col}'\n    tmp = df_.groupby(key_col)[calc_col].mean()\n    df_[col_name] = np.nan\n    for val in tmp.index:\n        idx = df_[key_col]==val\n        df_.loc[idx, col_name] = df_[calc_col][idx] - tmp.loc[val]\n    return df_","bad92b75":"# anai\n\nprocess_name = 'agg feature'\nprocess_list.append(process_name)\nwith timer(process_name):\n#     for col in ['hp', 'attack', 'defense', 'special_attack', 'special_defense', 'speed', 'Base_stats']:\n    for col in ['Base_stats']:\n        whole_df = make_agg_rate(whole_df, 'generation_id', col)\n        whole_df = make_agg_diff(whole_df, 'generation_id', col)\n        \n        whole_df = make_agg_rate(whole_df, 'shape_id', col)\n        whole_df = make_agg_diff(whole_df, 'shape_id', col)\n    display(whole_df.head())","1435f6ff":"# # anai\n\n# process_name = 'agg feature type'\n# process_list.append(process_name)\n# with timer(process_name):\n#     calc_col = 'Base_stats'\n#     col_name = f'agg_rate_{calc_col}_p_type'\n    \n#     cat_col = [\n#         'type_1', 'type_2',\n#               ]\n#     tmp_df = whole_df[cat_col].astype(str).fillna('')\n#     tmp_df[calc_col] = whole_df[calc_col]\n#     tmp_df['cat_text'] = tmp_df[cat_col[0]].str.cat([tmp_df[col] for col in cat_col[1:]], sep=' ')\n    \n#     tmp = tmp_df.groupby('cat_text')[calc_col].mean()\n        \n#     whole_df[col_name] = np.nan\n#     for val in tmp.index:\n#         idx = tmp_df['cat_text']==val\n#         whole_df.loc[idx, col_name] = whole_df[calc_col][idx] \/ tmp.loc[val]\n#     display(whole_df.head())","fdf1da9a":"# # anai\n\n# process_name = 'agg feature ability'\n# process_list.append(process_name)\n# with timer(process_name):\n#     calc_col = 'Base_stats'\n#     col_name = f'agg_rate_{calc_col}_p_ability'\n    \n#     cat_col = [\n#         'ability_1', 'ability_2',\n#               ]\n#     tmp_df = whole_df[cat_col].astype(str).fillna('')\n#     tmp_df[calc_col] = whole_df[calc_col]\n#     tmp_df['cat_text'] = tmp_df[cat_col[0]].str.cat([tmp_df[col] for col in cat_col[1:]], sep=' ')\n    \n#     tmp = tmp_df.groupby('cat_text')[calc_col].mean()\n        \n#     whole_df[col_name] = np.nan\n#     for val in tmp.index:\n#         idx = tmp_df['cat_text']==val\n#         whole_df.loc[idx, col_name] = whole_df[calc_col][idx] \/ tmp.loc[val]\n#     display(whole_df.head())","18538adc":"# # anai\n\n# process_name = 'agg feature multi'\n# process_list.append(process_name)\n# with timer(process_name):\n#     tmp = pd.concat([whole_df['type_1'], whole_df['type_2']], axis=0)\n#     tmp_df = pd.concat([whole_df, whole_df], axis=0)\n#     tmp_df.reset_index(drop=True, inplace=True)\n#     tmp_df['type'] = tmp.values\n\n# #     for col in ['hp', 'attack', 'defense', 'special_attack', 'special_defense', 'speed', 'Base_stats']:\n#     for col in ['Base_stats']:\n        \n#         col_name1 = f'agg_rate_{col}_p_type_1'\n#         col_name2 = f'agg_rate_{col}_p_type_2'\n        \n#         tmp = tmp_df.groupby('type')[col].mean()\n        \n#         whole_df[col_name1] = np.nan\n#         whole_df[col_name2] = np.nan\n#         for val in tmp.index:\n#             idx = whole_df['type_1']==val\n#             whole_df.loc[idx, col_name1] = whole_df[col][idx] \/ tmp.loc[val]\n            \n#             idx = whole_df['type_2']==val\n#             whole_df.loc[idx, col_name2] = whole_df[col][idx] \/ tmp.loc[val]\n#     display(whole_df.head())","2bd3608c":"# # anai\n\n# process_name = 'agg feature multi'\n# process_list.append(process_name)\n# with timer(process_name):\n#     tmp = pd.concat([whole_df['ability_1'], whole_df['ability_2']], axis=0)\n#     tmp_df = pd.concat([whole_df, whole_df], axis=0)\n#     tmp_df.reset_index(drop=True, inplace=True)\n#     tmp_df['val'] = tmp.values\n\n# #     for col in ['hp', 'attack', 'defense', 'special_attack', 'special_defense', 'speed', 'Base_stats']:\n#     for col in ['Base_stats']:\n        \n#         col_name1 = f'agg_rate_{col}_p_ability_1'\n#         col_name2 = f'agg_rate_{col}_p_ability_2'\n        \n#         tmp = tmp_df.groupby('val')[col].mean()\n        \n#         whole_df[col_name1] = np.nan\n#         whole_df[col_name2] = np.nan\n#         for val in tmp.index:\n#             idx = whole_df['ability_1']==val\n#             whole_df.loc[idx, col_name1] = whole_df[col][idx] \/ tmp.loc[val]\n            \n#             idx = whole_df['ability_2']==val\n#             whole_df.loc[idx, col_name2] = whole_df[col][idx] \/ tmp.loc[val]\n#     display(whole_df.head())","e03a96a0":"# # anai\n\n# process_name = 'agg feature multi'\n# process_list.append(process_name)\n# with timer(process_name):\n#     tmp = pd.concat([whole_df['egg_group_1'], whole_df['egg_group_2']], axis=0)\n#     tmp_df = pd.concat([whole_df, whole_df], axis=0)\n#     tmp_df.reset_index(drop=True, inplace=True)\n#     tmp_df['val'] = tmp.values\n\n# #     for col in ['hp', 'attack', 'defense', 'special_attack', 'special_defense', 'speed', 'Base_stats']:\n#     for col in ['Base_stats']:\n        \n#         col_name1 = f'agg_rate_{col}_p_egg_group_1'\n#         col_name2 = f'agg_rate_{col}_p_egg_group_2'\n        \n#         tmp = tmp_df.groupby('val')[col].mean()\n        \n#         whole_df[col_name1] = np.nan\n#         whole_df[col_name2] = np.nan\n#         for val in tmp.index:\n#             idx = whole_df['egg_group_1']==val\n#             whole_df.loc[idx, col_name1] = whole_df[col][idx] \/ tmp.loc[val]\n            \n#             idx = whole_df['egg_group_2']==val\n#             whole_df.loc[idx, col_name2] = whole_df[col][idx] \/ tmp.loc[val]\n# #         whole_df[f'agg_rate_{col}_p_egg_group'] = np.nanmean([whole_df[col_name1].values,\n# #                                                            whole_df[col_name2].values], axis=0)\n#     display(whole_df.head())","f726a2d9":"# anai\n\nprocess_name = 'blissey_fam'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df['blissey_fam'] = 0\n    idx = whole_df['evolution_chain_id']==blissey_id\n    whole_df.loc[idx, 'blissey_fam'] = 1","def3abde":"# anai\n\nprocess_name = 'audino_fam'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df['audino_fam'] = 0\n    idx = whole_df['evolution_chain_id']==271\n    whole_df.loc[idx, 'audino_fam'] = 1","ee667ebc":"# anai\n\nprocess_name = 'agg Freq'\nprocess_list.append(process_name)\nwith timer(process_name):\n    col_name = 'agg_freq_generation_id_type'\n    whole_df[col_name] = 0\n    \n    tmp = [' '.join([str(t1), str(t2)]) for t1, t2 in zip(whole_df['type_1'].values, whole_df['type_2'].values)]\n    tmp_df = whole_df.copy()\n    tmp_df['type'] = tmp\n    pivot = tmp_df.pivot_table(values='id', index='generation_id',columns='type',aggfunc='count')\n    for col in pivot.columns:\n        for idx in pivot.index:\n            flag = (tmp_df['type']==col)&(tmp_df['generation_id']==idx)\n            whole_df[col_name][flag] = pivot[col][idx]\n    display(whole_df)","1c2691f9":"# anai\n\nprocess_name = 'agg Freq'\nprocess_list.append(process_name)\nwith timer(process_name):\n    col_name = 'agg_freq_generation_id_ability'\n    whole_df[col_name] = 0\n    \n    tmp = [' '.join([str(t1), str(t2)]) for t1, t2 in zip(whole_df['ability_1'].values, whole_df['ability_2'].values)]\n    tmp_df = whole_df.copy()\n    tmp_df['ability'] = tmp\n    pivot = tmp_df.pivot_table(values='id', index='generation_id',columns='ability',aggfunc='count')\n    for col in pivot.columns:\n        for idx in pivot.index:\n            flag = (tmp_df['ability']==col)&(tmp_df['generation_id']==idx)\n            whole_df[col_name][flag] = pivot[col][idx]\n    display(whole_df)","3e3f5420":"# # anai\n\n# process_name = 'agg Freq'\n# process_list.append(process_name)\n# with timer(process_name):\n#     col_name = 'agg_freq_generation_id_shape_id'\n#     whole_df[col_name] = 0\n    \n#     pivot = whole_df.pivot_table(values='id', index='generation_id',columns='shape_id',aggfunc='count')\n#     for col in pivot.columns:\n#         for idx in pivot.index:\n#             flag = (whole_df['shape_id']==col)&(whole_df['generation_id']==idx)\n#             whole_df[col_name][flag] = pivot[col][idx]\n#     display(whole_df)","edbe1be0":"# anai\n\nprocess_name = 'agg Freq'\nprocess_list.append(process_name)\nwith timer(process_name):\n    ref_col = 'shape_id'\n    col_name = f'agg_freq_{ref_col}_type'\n    whole_df[col_name] = 0\n    \n    tmp = [' '.join([str(t1), str(t2)]) for t1, t2 in zip(whole_df['type_1'].values, whole_df['type_2'].values)]\n    tmp_df = whole_df.copy()\n    tmp_df['type'] = tmp\n    pivot = tmp_df.pivot_table(values='id', index=ref_col,columns='type',aggfunc='count')\n    for col in pivot.columns:\n        for idx in pivot.index:\n            flag = (tmp_df['type']==col)&(tmp_df[ref_col]==idx)\n            whole_df[col_name][flag] = pivot[col][idx]\n    display(whole_df)","cf6c0f91":"# anai\n\nprocess_name = 'agg Freq'\nprocess_list.append(process_name)\nwith timer(process_name):\n    ref_col = 'shape_id'\n    col_name = f'agg_freq_{ref_col}_ability'\n    whole_df[col_name] = 0\n    \n    tmp = [' '.join([str(t1), str(t2)]) for t1, t2 in zip(whole_df['ability_1'].values, whole_df['ability_2'].values)]\n    tmp_df = whole_df.copy()\n    tmp_df['ability'] = tmp\n    pivot = tmp_df.pivot_table(values='id', index=ref_col,columns='ability',aggfunc='count')\n    for col in pivot.columns:\n        for idx in pivot.index:\n            flag = (tmp_df['ability']==col)&(tmp_df[ref_col]==idx)\n            whole_df[col_name][flag] = pivot[col][idx]\n    display(whole_df)","65d2367e":"# # anai\n\n# process_name = 'agg Freq'\n# process_list.append(process_name)\n# with timer(process_name):\n#     ref_col = 'shape_id'\n#     col_name = f'agg_freq_{ref_col}_egg_group'\n#     whole_df[col_name] = 0\n    \n#     tmp = [' '.join([str(t1), str(t2)]) for t1, t2 in zip(whole_df['egg_group_1'].values, whole_df['egg_group_2'].values)]\n#     tmp_df = whole_df.copy()\n#     tmp_df['egg_group'] = tmp\n#     pivot = tmp_df.pivot_table(values='id', index=ref_col,columns='egg_group',aggfunc='count')\n#     for col in pivot.columns:\n#         for idx in pivot.index:\n#             flag = (tmp_df['egg_group']==col)&(tmp_df[ref_col]==idx)\n#             whole_df[col_name][flag] = pivot[col][idx]\n#     display(whole_df)","97db9a2c":"# # anai\n\n# process_name = 'agg Freq'\n# process_list.append(process_name)\n# with timer(process_name):\n#     ref_col = 'generation_id'\n#     col_name = f'agg_freq_{ref_col}_egg_group'\n#     whole_df[col_name] = 0\n    \n#     tmp = [' '.join([str(t1), str(t2)]) for t1, t2 in zip(whole_df['egg_group_1'].values, whole_df['egg_group_2'].values)]\n#     tmp_df = whole_df.copy()\n#     tmp_df['egg_group'] = tmp\n#     pivot = tmp_df.pivot_table(values='id', index=ref_col,columns='egg_group',aggfunc='count')\n#     for col in pivot.columns:\n#         for idx in pivot.index:\n#             flag = (tmp_df['egg_group']==col)&(tmp_df[ref_col]==idx)\n#             whole_df[col_name][flag] = pivot[col][idx]\n#     display(whole_df)","c158beef":"# anai\n\nprocess_name = 'spec change same chain'\nprocess_list.append(process_name)\nwith timer(process_name):\n    whole_df['ability_change_same_chain'] = 0\n    whole_df['type_change_same_chain'] = 0\n\n    tmp_df = whole_df.copy()\n    tmp = [' '.join([str(t1), str(t2)]) for t1, t2 in zip(whole_df['ability_1'].values, whole_df['ability_2'].values)]\n    tmp_df['ability'] = tmp\n    tmp = [' '.join([str(t1), str(t2)]) for t1, t2 in zip(whole_df['type_1'].values, whole_df['type_2'].values)]\n    tmp_df['type'] = tmp\n    for id_ in whole_df['evolution_chain_id'].unique():\n        flag = whole_df['evolution_chain_id']==id_\n        if np.sum(flag)>1:\n            if tmp_df['ability'][flag].nunique()>1:\n                whole_df.loc[flag, 'ability_change_same_chain'] = 1\n            if tmp_df['type'][flag].nunique()>1:\n                whole_df.loc[flag, 'type_change_same_chain'] = 1","06e26b32":"# # anai\n\n# process_name = 'spec change from evolves'\n# process_list.append(process_name)\n# with timer(process_name):\n#     whole_df['ability_change_evolves'] = 0\n#     whole_df['type_change_evolves'] = 0\n\n#     tmp_df = whole_df.copy()\n#     tmp = [' '.join([str(t1), str(t2)]) for t1, t2 in zip(whole_df['ability_1'].values, whole_df['ability_2'].values)]\n#     tmp_df['ability'] = tmp\n#     tmp = [' '.join([str(t1), str(t2)]) for t1, t2 in zip(whole_df['type_1'].values, whole_df['type_2'].values)]\n#     tmp_df['type'] = tmp\n#     for idx, row in tqdm(tmp_df[~tmp_df['evolves_from_species_id'].isna()].iterrows()):\n#         id_ = row['evolves_from_species_id']\n#         if row['ability'] != tmp_df['ability'][tmp_df['species_id']==id_].values[0]:\n#             idx = whole_df['evolution_chain_id']==row['evolution_chain_id']\n#             whole_df.loc[idx, 'ability_change_evolves'] = 1\n#         if row['type'] != tmp_df['type'][tmp_df['species_id']==id_].values[0]:\n#             idx = whole_df['evolution_chain_id']==row['evolution_chain_id']\n#             whole_df.loc[idx, 'type_change_evolves'] = 1\n#     display(whole_df.head())","b5c8cbba":"# # anai\n\n# process_name = 'spec change from evolves'\n# process_list.append(process_name)\n# with timer(process_name):\n#     whole_df['ability_change_evolves'] = 0\n#     whole_df['type_change_evolves'] = 0\n\n#     tmp_df = whole_df.copy()\n#     tmp = [' '.join([str(t1), str(t2)]) for t1, t2 in zip(whole_df['ability_1'].values, whole_df['ability_2'].values)]\n#     tmp_df['ability'] = tmp\n#     tmp = [' '.join([str(t1), str(t2)]) for t1, t2 in zip(whole_df['type_1'].values, whole_df['type_2'].values)]\n#     tmp_df['type'] = tmp\n#     for idx, row in tqdm(tmp_df[~tmp_df['evolves_from_species_id'].isna()].iterrows()):\n#         id_ = row['evolves_from_species_id']\n#         if row['ability'] != tmp_df['ability'][tmp_df['species_id']==id_].values[0]:\n# #             idx = whole_df['evolution_chain_id']==row['evolution_chain_id']\n#             whole_df.loc[idx, 'ability_change_evolves'] = 1\n#         if row['type'] != tmp_df['type'][tmp_df['species_id']==id_].values[0]:\n# #             idx = whole_df['evolution_chain_id']==row['evolution_chain_id']\n#             whole_df.loc[idx, 'type_change_evolves'] = 1\n#     display(whole_df.head())","8715030d":"## \u4e0a\u8a18\u307e\u3067\u3067\u4e00\u65e6\u30c7\u30fc\u30bf\u306f\u3059\u3079\u3066\u5909\u63db\u3092\u5b8c\u4e86\n## \u30c7\u30fc\u30bf\u30c1\u30a7\u30c3\u30af\nwhole_df.head().T","201fee27":"## Data Split\n\nprocess_name = 'Data Split'\nprocess_list.append(process_name)\nwith timer(process_name):\n    train_df = whole_df[whole_df['flag'] == 0].reset_index(drop=True)\n    test_df = whole_df[whole_df['flag'] == 1].reset_index(drop=True)\n    \n    print(f'Train: {train_df.shape} | Test: {test_df.shape}')","8bf9e17f":"## Non Training Columns\ndrop_cols = [\"id\", \"pokemon\", \"species_id\", \"target\"]","9380d460":"lgb_params = {\n    \"objective\": CONFIG.objective,\n    \"metric\": CONFIG.metric,\n    \"learning_rate\": CONFIG.learning_rate,\n    \"verbosity\": 0,\n    \"random_state\": CONFIG.seed,\n    \"num_leaves\": CONFIG.num_leaves\n}","5834af93":"# Train\u5168\u4f53\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u5024, Test\u5168\u4f53\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u5024\u3092\u78ba\u8a8d\noof = np.zeros(len(train_df))\npred = np.zeros(len(test_df))\n\n# Feature Importance\u3092\u78ba\u8a8d\nimportances_all = pd.DataFrame()","0b6bee63":"use_cols = []\nCV = KFold(n_splits=CONFIG.n_fold, shuffle=True, random_state=CONFIG.seed)\n# CV = StratifiedKFold(n_splits=CONFIG.n_fold, shuffle=True, random_state=CONFIG.seed)\n\n## Cross Validation\u3067Fold\u3054\u3068\u306b\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3001\u4e88\u6e2c\nfor fold, (tr_idx, va_idx) in enumerate(CV.split(train_df, train_df['target'])):\n    \n    print(f'======================= Fold {fold+1} =============================')\n    \n    # \u6642\u9593\u8a08\u6e2c\n    st_time = time.time()\n    \n    ## \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3068\u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u306b\u5206\u96e2\n    X_train = train_df.loc[tr_idx, :]\n    X_valid = train_df.loc[va_idx, :]\n    X_train.reset_index(drop=True, inplace=True)\n    X_valid.reset_index(drop=True, inplace=True)\n    \n    # target_encode\n    if CONFIG.target_encode:\n        target_col = 'target'\n        target_en_col = ['type_1', 'type_2', 'egg_group_1', 'egg_group_2']\n        jse = ce.JamesSteinEncoder(cols=target_en_col, drop_invariant=True)\n        jse_train = jse.fit_transform(X_train[target_en_col], X_train[target_col])\n        jse_valid = jse.transform(X_valid[target_en_col])\n        X_train = pd.concat([X_train.drop(target_en_col, axis=1), jse_train], axis=1)\n        X_valid = pd.concat([X_valid.drop(target_en_col, axis=1), jse_valid], axis=1)\n        del jse_train, jse_valid\n        \n        wandb.config.update({'target_encode_cols':target_en_col})\n\n    \n    ## id\u306a\u3069\u306eNon-Training\u5217\u306f\u524a\u9664\n    X_train = X_train.drop(drop_cols, axis=1)\n    X_valid = X_valid.drop(drop_cols, axis=1)\n        \n    y_train = train_df.loc[tr_idx, 'target']\n    y_valid = train_df.loc[va_idx, 'target']\n    \n    ## LightGBM\u306eDataset\u5316\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid)\n    use_cols = X_train.columns\n    \n    ## model \u306e\u5b66\u7fd2\n    lgb_model = lgb.train(\n        lgb_params,\n        lgb_train,\n        valid_sets = [lgb_train, lgb_eval], # train,valid\u4e21\u65b9\u8868\u793a\n        num_boost_round = CONFIG.num_boost_round,\n        early_stopping_rounds=CONFIG.early_stopping_rounds,\n        verbose_eval=0,\n        callbacks=[wandb_callback()]\n    )\n    \n    ## \u6700\u3082\u3088\u304b\u3063\u305fiteration\u3092\u6307\u5b9a\n    best_iter = lgb_model.best_iteration\n    print(best_iter)\n    ## valid\u306e\u4e88\u6e2c\u5024\u3092\u683c\u7d0d\n    oof[va_idx] += lgb_model.predict(X_valid, num_iteration=best_iter)\n    \n    ## valid\u306e\u4e88\u6e2c\u5024\u3068\u6b63\u89e3\u5024\u306e\u30b9\u30b3\u30a2\u3092\u78ba\u8a8d\n    score = mean_squared_error(y_valid, oof[va_idx]) ** .5\n    \n    if CONFIG.target_encode:\n        # target_encode\n        jse_test = jse.transform(X_valid[target_en_col])\n        test_df = pd.concat([test_df.drop(target_en_col, axis=1), jse_test], axis=1)\n        del jse_test\n    ## test\u30c7\u30fc\u30bf\u306e\u4e88\u6e2c\u5024\u3092\u683c\u7d0d\n    lgb_pred = lgb_model.predict(\n        test_df.drop(drop_cols, axis=1),\n        num_iteration=best_iter\n    )\n    \n    ## \u4e88\u6e2c\u5024\u306f\u5404Fold\u306e\u5e73\u5747\n    pred += lgb_pred\/CONFIG.n_fold\n    \n    ## Fold\u3054\u3068\u306eFeature Importance\n    importances = pd.DataFrame()\n    importances['feature'] = X_train.columns.tolist()\n    importances['lgb_gain'] = lgb_model.feature_importance()\n    importances['fold'] = fold+1\n    importances_all = pd.concat([importances_all, importances], axis=0, sort=False)\n    \n    ## Model Save\n    lgb_model.save_model(os.path.join(MODEL_DIR, f'lgb_model_fold{fold+1}.txt'))\n    \n    e_time = time.time() - st_time\n    print(f'Fold: {fold+1} | LightGBM: RMSLE={score:.6f} | Elapsed: {e_time:.0f}s')\n    \n    del lgb_model, X_train, X_valid, y_train, y_valid\n    _ = gc.collect()\n    \nprint(\"\u2605\"*50)\ntotal_score = mean_squared_error(train_df['target'], oof) ** .5\nprint(f'Total Scor: {total_score:.6f}')","8fb9c87b":"if CONFIG.target_encode:\n    process_list.append('target_encode')\nwandb.config.update({'use_cols':[col for col in use_cols]})\nwandb.config.update({'feature_process':process_list})\nwandb.log({'CV':total_score})","0f226db1":"len(use_cols)","8cec81ad":"# mean_squared_error(train_df['target'], np.clip(oof, train_df['target'].min(), np.max(oof))) ** .5","d5a63954":"plt.figure(figsize=(16, 5),tight_layout=True)\nsns.distplot(train_df['target'], label='train')\nsns.distplot(oof, label='oof')\nsns.distplot(pred, label='pred')\nplt.legend()\nplt.show()","d404d132":"mean_importance = importances_all.groupby('feature')['lgb_gain'].agg('mean')\nmean_importance = mean_importance.sort_values(ascending=False)\nimportance_list = mean_importance.index.tolist()[:20]\n\nplt.figure(figsize=(20, 6), tight_layout=True)\nsns.boxplot(data=importances_all[importances_all['feature'].isin(importance_list)].sort_values('lgb_gain', ascending=False),\n            x='feature', y='lgb_gain')\nplt.xticks(rotation=90)\nplt.title(\"\u7279\u5fb4\u91cf\u91cd\u8981\u5ea6 LightGBM\")\nplt.show()","e47f3aa4":"## sample_submission.csv\u306etarget\u3092pred\u3067\u7f6e\u304d\u63db\u3048\u3066\u3001csv\u3067\u4fdd\u5b58\u3002\n## pred\u306flog\u5909\u63db\u3057\u3066\u3044\u308b\u306e\u3067\u3001expm1\u3067\u5143\u306b\u623b\u3059\u3053\u3068\u3092\u5fd8\u308c\u306a\u3044\u3088\u3046\u306b\u3057\u307e\u3057\u3087\u3046\u3002\n## submissin\u30d5\u30a1\u30a4\u30eb\u306e\u540d\u524d\u3092\u898b\u308c\u3070\u3001CVscore\u3082\u308f\u304b\u308b\u3088\u3046\u306b\u3001\u30d5\u30a1\u30a4\u30eb\u540d\u306bCV\u5024\u3082\u5165\u308c\u3066\u3057\u307e\u3046\u3053\u3068\u304c\u50d5\u306f\u591a\u3044\u3067\u3059\u3002\n\nsub_df['target'] = np.expm1(pred)\nsub_df.to_csv(f'.\/{CONFIG.exp_name}_CV{total_score:.6f}_submision.csv', index=False)","5cafb6d7":"plt.scatter(train_df['target'], oof)\nplt.xlabel('target')\nplt.ylabel('oof')","260df690":"tmp = train_df.copy()\ntmp['mse']=np.square(tmp['target']-oof)\ntmp['oof'] = oof\ntmp.loc[np.square(tmp['target']-oof).argsort()[::-1][:30]]","3f13f8f5":"p75 = np.percentile(np.square(tmp['target']-oof), 75)\np25 = np.percentile(np.square(tmp['target']-oof), 25)\nth = np.percentile(np.square(tmp['target']-oof), 75)+(p75-p25)*1.5","77dfd91d":"tmp[tmp['mse']>th].sort_values('mse')","d4d1c85e":"tmp.loc[np.square(tmp['target']-oof).argsort()[::-1][:50]]","2b69f32a":"!pip install -q faiss-cpu\nimport faiss","821da8b8":"class FaissKNeighbors:\n    def __init__(self, k=5):\n        self.index = None\n        self.d = None\n        self.k = k\n\n    def fit(self, X):\n        X = X.copy(order='C')\n        self.d = X.shape[1]\n        self.index = faiss.IndexFlatL2(self.d)\n        self.index.add(X.astype(np.float32))\n\n    def predict(self, X):\n        X = X.copy(order='C')\n        X = np.reshape(X, (-1, self.d))\n        distances, indices = self.index.search(X.astype(np.float32), k=self.k)\n        return indices[0]","de549a2e":"spec_col = ['attack', 'defense', 'hp', 'special_attack', 'special_defense', 'speed', 'Base_stats']\n\nkn = FaissKNeighbors(k=10)\n# kn.fit(train_df[spec_col].values)\nkn.fit(train_df[spec_col][train_df['evolves_from_species_id'].isna()].values)","fb3cb929":"idx = kn.predict(train_df[spec_col][train_df['pokemon']=='qwilfish'].values)\n# idx = kn.predict(train_df[spec_col][train_df['pokemon']=='carbink'].values)\n# tmp.loc[idx].T\ntmp[tmp['evolves_from_species_id'].isna()].iloc[idx].T","89badb00":"spec_col = ['attack', 'defense', 'hp', 'special_attack', 'special_defense', 'speed', 'Base_stats']\n\nkn = FaissKNeighbors(k=10)\nkn.fit(train_df[spec_col].values)\n# kn.fit(train_df[spec_col][train_df['evolves_from_species_id'].isna()].values)","e5e5a24d":"# idx = kn.predict(train_df[spec_col][train_df['pokemon']=='qwilfish'].values)\nidx = kn.predict(train_df[spec_col][train_df['pokemon']=='carbink'].values)\ntmp.loc[idx].T","86d181df":"pokemons = ['happiny',\n'audino',\n'qwilfish',\n'carbink',\n'clawitzer',\n'ditto',\n'luvdisc',\n'wartortle',\n'farfetchd',]\n\nfor poke in pokemons:\n    print(poke)\n    idx = kn.predict(train_df[spec_col][train_df['pokemon']==poke].values)\n    display(tmp.loc[idx].T)","e37051e3":"tmp[tmp['species_id']==25].T","f4e29b24":"whole_df[whole_df['evolution_chain_id']==271.0].T","d6e9ac33":"## ability\n- ability\u3082type\u3068\u540c\u69d8\u306b\u3001\u30a2\u30d3\u30ea\u30c6\u30a3\u6570\u3068\u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059","317ac318":"<div class = 'alert alert-block alert-info'\n     style = 'background-color:#107b6a;\n              color:#ff7b73;\n              border-width:5px;\n              border-color:#084152;\n              font-family:Comic Sans MS'>\n    <p style = 'font-size:24px'>Baseline \u4f5c\u6210<\/p>\n    <a href = \"#Settings\"\n       style = \"color:#ff7b73;\n                font-size:14px\">1.Settings<\/a><br>\n    <a href = \"#Data-Load\"\n       style = \"color:#ff7b73;\n                font-size:14px\">2.Data Load<\/a><br>\n    <a href = \"#Preprocess\"\n       style = \"color:#ff7b73;\n                font-size:14px\">3.Preprocess<\/a><br>\n    <a href = \"#Training\"\n       style = \"color:#ff7b73;\n                font-size:14px\">4.Training<\/a><br>\n    <a href = \"#Submit\"\n       style = \"color:#ff7b73;\n                font-size:14px\">5.Submit<\/a><br>\n<\/div>","91b60660":"## blissey_fam","a61585a1":"## shape\n- shape\u306fshape_id\u3067\u65e2\u306bEncoding\u3055\u308c\u3066\u3044\u308b\u3063\u307d\u3044\u306e\u3067\u3001shape\u306f\u4e00\u65e6\u524a\u9664","b1a53625":"## \u30d5\u30a1\u30a4\u30eb\u7d50\u5408","07c1fff5":"- \u30c7\u30fc\u30bf\u306e\u6982\u89b3\u304b\u3089\u78ba\u8a8d\u3057\u307e\u3057\u3087\u3046","67e0a2b0":"## ability_hidden\n- \u3053\u3061\u3089\u3082\u72ec\u7acb\u3067Label Encoding\u3057\u3066\u3044\u304d\u307e\u3059","445cafaa":"## Freq","abbbb7b1":"## Drop \u30cf\u30d4\u30ca\u30b9","dd081af1":"## \u6d3e\u751f\u30dd\u30b1\u30e2\u30f3\u306e\u60c5\u5831\u88dc\u5b8c","f2f09e02":"## \u7279\u5fb4\u91cf\u91cd\u8981\u5ea6\u3092\u78ba\u8a8d","c087d9cd":"# Training\n- \u4eca\u56de\u306f\u56de\u5e30\u4e88\u6e2c\u3067\u3059\u306e\u3067\u3001KFold\u3067\u4e00\u65e6CrossValidation\u3092\u3057\u307e\u3059\u3002\n- test\u306b\u5bfe\u3057\u3066\u306fCrossValidation\u306e\u5404Fold\u3054\u3068\u306e\u4e88\u6e2c\u5024\u3092\u5e73\u5747\u3057\u307e\u3059\u3002\n- \u4f7f\u7528\u3059\u308b\u30e2\u30c7\u30eb\u306fLightGBM\u3067\u3059\u3002","5995eb0a":"<img src=\"https:\/\/automaton-media.com\/wp-content\/uploads\/2019\/11\/20191121-106756-header-696x392.jpg\" width=200%>","8368aecd":"- type_2\u306fnan\u304c\u542b\u307e\u308c\u3066\u3044\u308b\u306e\u3067\u3001nan\u4ee5\u5916\u306f\u3059\u3079\u3066type_1\u3068type_2\u306e\u30e6\u30cb\u30fc\u30af\u6570\u306f\u540c\u3058\u3060\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002","0d6a63bb":"## \u30b5\u30d6\u30cd\u30fc\u30e0\u306e\u6709\u7121","716b89ae":"# Data Load","fa648b26":"## \u7a2e\u65cf\u5024\n- attack, defense, special_attack, special_defense, speed, hp\u306f\u30dd\u30b1\u30e2\u30f3\u56fa\u6709\u306e\u80fd\u529b\u5024\u3092\u793a\u3059\u3082\u306e\u306a\u306e\u3067\u3001\u80fd\u529b\u306e\u5408\u8a08\u5024\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046","ce55d0d0":"## Data Check","ca9f40c9":"## Color\n- Color\u306f\u4e00\u65e6\u4eca\u56de\u306f\u4f7f\u308f\u306a\u3044\u65b9\u5411\u3067\u8003\u3048\u307e\u3059\n  - 16\u9032\u6570\u306e\u30ab\u30e9\u30fc\u30b3\u30fc\u30c9\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u3042\u307e\u308aid\u3068\u5909\u308f\u3089\u306a\u3044\u3068\u601d\u3044\u307e\u3059\u306e\u3067\u524a\u9664\u3057\u307e\u3059","0e1b44b7":"# Submit","4b5c8a2b":"## url_image\n- \u9ad8\u6a4b\u3055\u3093\u304cefficientnetB0\u3092\u7528\u3044\u305f\u5206\u6790\u3092\u516c\u958b\u3057\u3066\u304f\u308c\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u4eca\u56de\u306e\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\u3067\u306f\u753b\u50cf\u306f\u4f7f\u308f\u305a\u4f5c\u6210\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002[\u53c2\u8003](https:\/\/www.kaggle.com\/shunsuketakahashi\/shiggle-1-effb0-training-cv-0-4293-lb-0-46497)","f5caa279":"# Preprocess","50433668":"# Settings","8d24d12a":"- \u307e\u305a\u306f\u4eca\u56de\u306e\u8a55\u4fa1\u65b9\u6cd5\u304cRMSLE\u306a\u306e\u3067Target\u306e\u5024\u306flog\u3092\u3068\u308a\u307e\u3057\u3087\u3046\n  - log\u3092\u53d6\u308b\u969b\u306f\u3001np.log1p\u3092\u4f7f\u3046\u306e\u304c\u4fbf\u5229\u3067\u3059\u3002\n  - \u3042\u3068\u3067\u623b\u3059\u3068\u304d\u306b\u306fnp.exp1m\u3092\u4f7f\u3044\u307e\u3059\u3002","beb6efaf":"## Egg group\n- egg_group1, egg_group2\u3082type\u3068\u540c\u69d8\u306b\u30ab\u30a6\u30f3\u30c8\u6570\u3068\u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u3057\u3066\u3044\u304d\u307e\u3059","789675df":"## CV\u304c\u4f4e\u3044\u30dd\u30b1\u30e2\u30f3\u3092\u78ba\u8a8d","0248897c":"## \u9032\u5316\u60c5\u5831","32fd83ae":"# SVD","c338e3f6":"## Target\u3092log\u5909\u63db","a9fb8343":"- output\u306b\u5410\u304d\u51fa\u3055\u308c\u305fsubmission.csv\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066submit\u3057\u3066\u307f\u307e\u3057\u3087\u3046","20fe44df":"## type_1, type_2\n- \u30bf\u30a4\u30d7\u3092\u8907\u6570\u3082\u3064\u304b\u3069\u3046\u304b\u3092\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f5c\u6210\u3057\u3066\u307f\u307e\u3057\u3087\u3046\n  - \u5358\u30bf\u30a4\u30d7\u306e\u30dd\u30b1\u30e2\u30f3\u306ftype_2\u304cnan\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u3046\u307e\u304f\u4f7f\u3044\u307e\u3057\u3087\u3046\n- \u30bf\u30a4\u30d7\u306ftype_1, type_2\u3067\u5171\u901a\u306e\u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c9\u3092\u4f7f\u3046\u7528\u306b\u5de5\u592b\u3057\u3066\u307f\u307e\u3057\u3087\u3046","7cf2759e":"- \u3053\u3053\u3067\u306f\u7c21\u5358\u306a\u7279\u5fb4\u4f5c\u6210\u3068\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\u4f5c\u6210\u3001Submission\u307e\u3067\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\n- \u4e0a\u306e\u30bb\u30eb\u30ab\u30e9\u30fc\u3068\u540c\u3058\u3088\u3046\u306a\u8272\u3092\u6301\u3064\u30dd\u30b1\u30e2\u30f3\u3001\u307f\u306a\u3055\u3093\u304a\u5206\u304b\u308a\u306b\u306a\u308a\u307e\u3059\u3067\u3057\u3087\u3046\u304b\u3002\n  - EDA\u306enotebook\u3082\u5225\u306e\u30dd\u30b1\u30e2\u30f3\u306e\u30ab\u30e9\u30fc\u3067\u8aac\u660e\u306e\u90e8\u5206\u306f\u66f8\u3044\u3066\u3044\u307e\u3057\u305f\u3002","a98efff1":"## \u4e88\u6e2c\u5024\u306e\u5206\u5e03\u3092\u78ba\u8a8d","221966e9":"## \u540c\u7a2e\u65cf\u3067\u306etype \u5909\u5316","f489379e":"## height, weight\n- EDA\u3067height\u3084weight\u306a\u3069\u306e\u7279\u5fb4\u306f\u5024\u57df\u304c\u5927\u304d\u3044\u3053\u3068\u304c\u5206\u304b\u3063\u3066\u3044\u307e\u3059\n- \u5024\u57df\u304c\u5927\u304d\u3044\u3082\u306e\u3082\u5bfe\u6570\u5909\u63db\u3057\u307e\u3057\u3087\u3046","9c523c9f":"## agg\u60c5\u5831","349544f2":"## mega\u306e\u6709\u7121","debbaa45":"## audino_fam"}}