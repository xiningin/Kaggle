{"cell_type":{"b47e5359":"code","d3fb9567":"code","7b040b18":"code","e2b43bd5":"code","0efe2cd8":"code","3829fc4d":"code","aaa6cab5":"code","f2e1ead2":"code","e6915f00":"code","9b69f714":"code","0e1d8f99":"code","24564091":"code","0c8d0fdd":"code","d03c130f":"code","76dc85f8":"code","1e3d0791":"code","80f3cd7e":"code","228f643e":"code","f1b69475":"code","3f17a156":"code","7afd0a13":"code","fb89cd7c":"code","fe2f04ba":"markdown","e02f454e":"markdown","ece4fac5":"markdown","84542eec":"markdown","94cc5b84":"markdown","98b40d4e":"markdown","6aec05e1":"markdown","ba2c5aef":"markdown","0a930d71":"markdown","c1f3979c":"markdown","ba069a00":"markdown","e40f67d0":"markdown","487fd222":"markdown"},"source":{"b47e5359":"import numpy as np\nimport pandas as pd\nfrom pandas import Series, DataFrame\n\nimport tokenizers\nfrom tqdm.notebook import tqdm\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))","d3fb9567":"def read_train():\n    train=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n    \ndef read_submission():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n    return test\n\ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()\n\ntrain_df = read_train()\ntest_df = read_test()\n\n# there was one NaN value inside tweets in train_df\nassert train_df[\"text\"].isna().sum() <= 1\ntrain_df[\"text\"] = train_df[\"text\"].fillna(\"\")\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=777)\nsplits = list(skf.split(np.arange(len(train_df)), train_df.sentiment.values))\nval_inds_arr = [val_inds for tr_inds, val_inds in splits]\nval_inds_arr","7b040b18":"N_FOLDS = 5\n\ndef get_union_df(name=\"train_prediction\", inds_arr=None, agg_f=None):\n    \"\"\" function for gathering results from each fold (for test with aggregation (agg_f) and for oof without one) \"\"\"\n    df = DataFrame()\n    for n_fold in range(N_FOLDS):\n        fold_df = (\n            pd\n            .read_csv(\"..\/input\/bestoofprediction\/{}_{}.csv\".format(name, n_fold + 1))\n            .drop(\"Unnamed: 0\", axis=1)\n        )\n\n        if inds_arr is not None:\n            fold_df.index = inds_arr[n_fold]\n\n        df = pd.concat([df, fold_df])\n\n    if agg_f:\n        df = df.astype(np.float32)\n        df = df.groupby(df.index).agg(agg_f)\n        \n    return df.sort_index()","e2b43bd5":"oof_start_proba = get_union_df(name=\"validation_start_prediction\", inds_arr=val_inds_arr)\noof_end_proba = get_union_df(name=\"validation_end_prediction\", inds_arr=val_inds_arr)\n\noof_start_proba.shape, oof_end_proba.shape","0efe2cd8":"oof_start_proba.head()","3829fc4d":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ndef get_pred(start_proba, end_proba, df, tokenizer):\n    pred = []\n    n_samples = len(start_proba)\n    for i in range(n_samples):\n        text = df['text'][df.index[i]]\n        a, b = np.argmax(start_proba[i]), np.argmax(end_proba[i])\n        if a > b: \n            pred_ = text # IMPROVE CV\/LB with better choice here\n        else:\n            cleaned_text = \" \" + \" \".join(text.split())\n            encoded_text = tokenizer.encode(cleaned_text)\n            pred_ids = encoded_text.ids[a - 2: b - 1]\n            pred_ = tokenizer.decode(pred_ids)\n        pred += [pred_]\n\n    return pred","aaa6cab5":"PATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt',\n    lowercase=True,\n    add_prefix_space=True\n)","f2e1ead2":"train_df[\"pred_selected_text\"] = get_pred(oof_start_proba.values, oof_end_proba.values, train_df, tokenizer)\ntrain_df[\"jaccard\"] = train_df.apply(lambda row: jaccard(row[\"selected_text\"], row[\"pred_selected_text\"]), axis=1)\ntrain_df = train_df.sort_values(\"jaccard\")\n\ntrain_df.head()","e6915f00":"train_df = train_df.sort_index()\ntrain_df[\"confidence\"] = 0.5 * (oof_start_proba.max(1) + oof_end_proba.max(1))\ntrain_df.head()","9b69f714":"oof_score = train_df[\"jaccard\"].mean()\nprint(f'oof score before optimization: {oof_score:.5f}')","0e1d8f99":"def get_hypo_df(start_proba, end_proba, beam_size=100):\n    start2top_proba = Series(start_proba).sort_values(ascending=False)[:beam_size]\n    end2top_proba   = Series(end_proba  ).sort_values(ascending=False)[:beam_size]\n\n    hypos = []\n    for start, start_proba in start2top_proba.items():\n        for end, end_proba in end2top_proba.items():\n            proba = 0.5 * (start_proba + end_proba)\n            hypos += [(start, end, proba)]\n\n    return DataFrame(hypos, columns=[\"start\", \"end\", \"proba\"]).sort_values(\"proba\")[::-1]","24564091":"ind = 0\nstart_proba = oof_start_proba.values[ind,]\nend_proba   = oof_end_proba.values  [ind,]\n\ntext_len = max((start_proba != 0).sum(), (end_proba != 0).sum())\nstart_proba = start_proba[:text_len + 5]\nend_proba   =   end_proba[:text_len + 5]\n\nstart_proba.shape, end_proba.shape","0c8d0fdd":"hypo_df = get_hypo_df(start_proba, end_proba, beam_size=10)\n\nhypo_df","d03c130f":"def get_jaccard_expectation(start0, end0, hypo_df, encoded_text, tokenizer):\n    jaccard_expectation_logit, logit_sum = 0, 0\n    selected_text0 = tokenizer.decode(encoded_text[start0 - 2: end0 - 1])\n    for start, end, logit in zip(hypo_df[\"start\"], hypo_df[\"end\"], hypo_df[\"proba\"]):\n        selected_text  = tokenizer.decode(encoded_text[start - 2: end - 1])\n        if (start <= end) and (start >= 2) and (selected_text.strip() != ''):\n            jaccard_val = jaccard(selected_text0, selected_text)\n            jaccard_expectation_logit += logit * jaccard_val\n            logit_sum += logit\n\n    return jaccard_expectation_logit \/ logit_sum","76dc85f8":"def get_best_selected_text(ind, oof_start_prediction, oof_end_prediction, train_df, tokenizer, beam_size=1):\n    text = \" \" + \" \".join(train_df[\"text\"][ind].split())\n    encoded_text = tokenizer.encode(text).ids\n    text_len = len(encoded_text)\n    \n    start_proba = oof_start_prediction.values[ind,][:text_len + 5]\n    end_proba   = oof_end_prediction.values  [ind,][:text_len + 5]\n    hypo_df = get_hypo_df(start_proba, end_proba, beam_size=100)\n\n    max_jaccard_expectation, best_selected_text = 0, \"\"\n    for start0, end0 in zip(hypo_df[\"start\"][:beam_size], hypo_df[\"end\"][:beam_size]):\n        selected_text0  = tokenizer.decode(encoded_text[start0 - 2: end0 - 1])\n        if (start0 <= end0) and (start0 >= 2) and (selected_text0.strip() != ''):\n            jaccard_expectation = get_jaccard_expectation(start0, end0, hypo_df, encoded_text, tokenizer)\n            if jaccard_expectation > max_jaccard_expectation:\n                max_jaccard_expectation = jaccard_expectation\n                best_selected_text = tokenizer.decode(encoded_text[start0 - 2: end0 - 1])\n\n    return max_jaccard_expectation, best_selected_text","1e3d0791":"N_SAMPLES = 30000\n\nmax_jaccard_expectations, selected_texts = [], []\nfor ind in tqdm(train_df[:N_SAMPLES].index):\n    max_jaccard_expectation, selected_text =  get_best_selected_text(ind, oof_start_proba, oof_end_proba, train_df, tokenizer, beam_size=2)\n    max_jaccard_expectations += [max_jaccard_expectation]\n    selected_texts += [selected_text]","80f3cd7e":"used_train_df = train_df[:N_SAMPLES].copy()\nused_train_df[\"pred_selected_text2\"] = selected_texts\nused_train_df[\"confidence2\"] = max_jaccard_expectations\nused_train_df[\"jaccard2\"] = used_train_df.apply(lambda row: jaccard(row[\"selected_text\"], row[\"pred_selected_text2\"]), axis=1)\nused_train_df.sort_values(\"confidence2\", ascending=False)","228f643e":"old_jaccards = {}\nnew_jaccards = {}\n\nfor thresh in np.arange(0.1, 1.1, 0.1):\n    old_jaccard = used_train_df[used_train_df[\"confidence2\"] < thresh][\"jaccard\" ].mean()\n    new_jaccard = used_train_df[used_train_df[\"confidence2\"] < thresh][\"jaccard2\"].mean()\n    old_jaccards[thresh] = old_jaccard\n    new_jaccards[thresh] = new_jaccard","f1b69475":"plt.figure(figsize=(16, 8))\nplt.title(\"Jaccard Curves\")\n\nplt.plot(list(old_jaccards.keys()), list(old_jaccards.values()), label=\"old jaccard\")\nplt.plot(list(new_jaccards.keys()), list(new_jaccards.values()), label=\"new jaccard\")\n_ = plt.legend()","3f17a156":"best_thresh = (Series(new_jaccards) - Series(old_jaccards)).idxmax()\nbest_thresh","7afd0a13":"def get_best_prediction(row):\n    if row[\"confidence2\"] < best_thresh:\n        return jaccard(row[\"selected_text\"], row[\"pred_selected_text2\"])\n    return jaccard(row[\"selected_text\"], row[\"pred_selected_text\"])   \n\nused_train_df[\"best_jaccard\"] = used_train_df.apply(lambda row: get_best_prediction(row), axis=1)","fb89cd7c":"old_oof_score = used_train_df[\"jaccard\"].mean()\nnew_oof_score = used_train_df[\"best_jaccard\"].mean()\nprint(f'oof score before optimization: {old_oof_score:.5f}')\nprint(f'oof score after optimization: {new_oof_score:.5f}')","fe2f04ba":"Load tokenizer and get the oof prediction with (oof_start_prediction, oof_end_prediction) tuple:","e02f454e":"That's notebook with implementation for some post process method : Jaccard Expectation Maximization (JEM). \n\nTopic with explantions: https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/158613.","ece4fac5":"# Jaccard Expectation Maximization","84542eec":"Now, let's implement new approach. At first, implement getting **hypo_df** table with next fields:\n- **start** - start index prediction\n- **end** - end index prediction\n- **proba** - prediction probability : $0.5 \\cdot (start\\_proba + end\\_proba)$ (but can use other functions)\n\nsorted by **proba** in decreasing oreder. For compuatinal effectiveness we can compute only first **beam_size** rows (**beam_size** = 100 by default).","94cc5b84":"We can use differen strategies to apply JEM:\n- for samples with small confidence\n- for samples with large confidence2\n- for samples with small confidence2\n- for samples with confidence < **thresh** * confidence2 for some threshold **thresh**\n\nYou can use each of them, but for my case 3rd strategy is the best. Find best treshold:","98b40d4e":"Compute all predictions (**pred_selected_text2**) and corresponding JEM (**confidence2**):","6aec05e1":"# JEP Implementation","ba2c5aef":"And compute the boost:","0a930d71":"Also compute the model confidence:","c1f3979c":"# Data Preparation\n\nI just take one of my prediction for testing strategy, you can fill this part of notebook with the same data. We need the next ones:\n- oof start\/end\/selected_text prediction (+ oof tweet text)\n- tokenizer, that you used in training time\n- [optional]: I also use splitter for recover correct indexes for oof prediction","ba069a00":"Boost is not so high, but it's diffrenet for different probability predictions. Hope, it helps.","e40f67d0":"Now implement JEPP with next algorithm (example in topic https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/158613):\n1. Compute **hypo_df**.\n2. With **hypo_df** for each row compute jaccard expectation with assumption, that correct selected_text is the one, that corresponds this row.\n3. Find row that maximize jaccard expectation.","487fd222":"Example of **hypo_df** computation for some sample:"}}