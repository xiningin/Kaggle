{"cell_type":{"0f893111":"code","f46fb1da":"code","1bc8ec49":"code","41c1613c":"code","27d8073e":"code","534db0f0":"code","6143b145":"code","d8331489":"code","c11e857d":"code","4ff8239c":"code","4d6120ea":"code","33c7a821":"code","c18c4929":"code","99cd672f":"code","36b860ed":"code","a90e130c":"code","fc43e153":"code","d514dcab":"code","c0ce7ccd":"code","e7017997":"code","17f0d758":"code","ea6a7eba":"code","7f822e92":"code","52cb93bf":"code","94bd5553":"code","1fc26174":"code","aa80d295":"code","db2f3334":"code","e6132acf":"code","1df7a432":"code","4bdd323b":"code","972bff09":"code","85a35417":"code","6c2445d7":"code","fbad4a8b":"code","0314242c":"code","a5ccd21d":"code","7cdf1f47":"code","851f74ed":"code","3c71b12a":"code","231f1f79":"code","cb2f5799":"code","5674c010":"code","8d5ed3cc":"code","61fa72c0":"code","c020e1ac":"code","06630942":"code","c092f9dd":"code","cde10708":"code","95fe4b21":"code","d52b77ee":"code","f83e4be6":"code","c9f43f92":"code","085863b7":"code","f0358ce9":"code","c7f14c44":"code","4043a22f":"code","65f3cb6a":"code","7d5a8da5":"code","3ba15dfb":"code","055cccd8":"code","5a148cbd":"code","16949fbf":"markdown","831321e7":"markdown","7d8f689b":"markdown","a6e4c1ec":"markdown","40053097":"markdown","c143229a":"markdown","735b264a":"markdown","75db1dfe":"markdown"},"source":{"0f893111":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f46fb1da":"import pandas as pd\nimport numpy as np\nimport scipy as sp\nimport sklearn \n\n# misc libraries\nimport random\nimport time\n\n#ignore warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n##Data modelling libraries## \n#Common model algorithms \nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis,gaussian_process\nfrom xgboost import XGBClassifier \n\n#Common models\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection \nfrom sklearn import metrics\n\n#visualization libraries\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\nimport missingno\n\n#some more visulization configuarations\n%matplotlib inline\nmpl.style.use(\"ggplot\")\nsns.set_style(\"white\")\npylab.rcParams[\"figure.figsize\"]=12,8\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report,roc_curve, auc\n\n#model to pickel format \nimport pickle\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n","1bc8ec49":"train_raw = pd.read_csv(\"..\/input\/titanic\/train.csv\")","41c1613c":"test_raw = pd.read_csv(\"..\/input\/titanic\/test.csv\")","27d8073e":"# copying train data for preparing \ndata = pd.DataFrame.copy(train_raw)","534db0f0":"#making two datasets together to clear data\ndata_cleaner = [data, test_raw]","6143b145":"data_cleaner","d8331489":"print(train_raw.info(),train_raw.columns)","c11e857d":"data.describe(include=\"all\")","4ff8239c":"#custom function checking for missing values\ndef missingdata(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    missing_data=pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    missing_data= missing_data[missing_data[\"Percent\"] > 0]\n    f,ax =plt.subplots(figsize=(8,6))\n    plt.xticks(rotation='90')\n    fig=sns.barplot(missing_data.index, missing_data[\"Percent\"],color=\"green\",alpha=0.8)\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15)\n    return missing_data ","4d6120ea":"#checking missing values in train data\nmissingdata(data)","33c7a821":"#checking missing values in test data\nmissingdata(test_raw)","c18c4929":"# replacing missing values with mean, median & mode in respective variables\n# for loop because list is not callable \n\nfor dataset in data_cleaner:\n    #replacing with median\n    dataset[\"Age\"].fillna(dataset[\"Age\"].median(), inplace=True)\n    #replacing with median\n    dataset[\"Fare\"].fillna(dataset[\"Fare\"].median(), inplace=True)\n    #replacing with mode\n    dataset[\"Embarked\"].fillna(dataset[\"Embarked\"].mode()[0], inplace=True)","99cd672f":"#checking missing values after imputation\nmissingno.matrix(data, figsize = (30,10))\n","36b860ed":"# drop paasnger ID, cabin, ticket from train1\ndrop_column = [\"PassengerId\",\"Cabin\", \"Ticket\"]\ndata.drop(drop_column, axis=1, inplace=True)","a90e130c":"# making new variables to define family members from SibSp & Parch\n# for continous variables (age & fare), making new discreted variables\nfor dataset in data_cleaner:    \n    # creating \"FamilySize\" variable based on \"SibSp & Parch\" data \n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n    # creating \"IsAlone\" variable with 0s(not alone) and 1s(alone)  \n    dataset['IsAlone'] = 1 \n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 \n    # seperating split title and name from name\n    dataset['Title'] = dataset['Name'].str.split(\",\",expand=True)[1].str.split(\".\",expand=True)[0]\n    #making continous variables into discete variables \n    dataset['FareBin'] = pd.cut(dataset['Fare'],4)\n    dataset['AgeBin'] = pd.cut(dataset['Age'],5)","fc43e153":"# discreting title variable by count (min count is 10)\n# saving below 10 unique name tiltes under \"misc\" title\nmisc_min_range = 10 \ntitle_names = (data['Title'].value_counts()<misc_min_range)\n# this will create a true or false series with title name as index\n\ndata['Title'] = data['Title'].apply(lambda x:'Misc' if title_names.loc[x]==True else x)\nprint(data['Title'].value_counts())\nprint(\"-\"*25)","d514dcab":"data.head(10)","c0ce7ccd":"# segregation of titles\ntitle_names = (test_raw['Title'].value_counts()<misc_min_range)\n# this will create a true or false series with title name as index\n\ntest_raw['Title'] = test_raw['Title'].apply(lambda x:'Misc' if title_names.loc[x]==True else x)\nprint(test_raw['Title'].value_counts())\nprint(\"-\"*25)","e7017997":"# converting objects to categorical data using labelencoder\nlabel = LabelEncoder()\nfor dataset in data_cleaner:\n    dataset[\"Sex_Code\"] = label.fit_transform(dataset[\"Sex\"])\n    dataset[\"Embarked_Code\"] = label.fit_transform(dataset[\"Embarked\"])\n    dataset[\"Title_Code\"] = label.fit_transform(dataset[\"Title\"])\n    dataset[\"FareBin_Code\"] = label.fit_transform(dataset[\"FareBin\"])\n    dataset[\"AgeBin_Code\"] = label.fit_transform(dataset[\"AgeBin\"])","17f0d758":"data.head()","ea6a7eba":"test_raw.head()","7f822e92":"# segregating data for irrespective use\n\n# for model building\ncol_model = [\"Name\",\"Age\",\"Fare\",\"FareBin\",\"AgeBin\",\"Sex_Code\",\"Embarked_Code\",\"Title_Code\"]\ntitan_model = pd.DataFrame.copy(data)\ntitan_model.drop(col_model, axis=1, inplace=True)\ntitan_model.head()","52cb93bf":"# for EDA charts\ncol_EDA = [\"Name\",\"Title_Code\",\"FareBin\",\"AgeBin\",\"Sex_Code\",\"Embarked_Code\"]\ntitan_EDA = pd.DataFrame.copy(data)\ntitan_EDA.drop(col_EDA, axis=1, inplace=True)\ntitan_EDA.head()","94bd5553":"# creating dummy variables for train data (titan_model)\ntitan_model = pd.get_dummies(titan_model, columns = [\"Sex\",\"Embarked\",\"Title\",\"FareBin_Code\",\"AgeBin_Code\"],\n                             prefix=[\"Sex\",\"Embarked_type\",\"Title\",\"Fare_type\",\"Age_type\"])","1fc26174":"titan_model.head()","aa80d295":"x = titan_model.drop('Survived', axis=1)\ny = titan_model['Survived']","db2f3334":"# test data for prediction\ncol_delete = [\"PassengerId\",\"Name\",\"Age\",\"Ticket\",\"Fare\",\"Cabin\",\"FareBin\",\"AgeBin\",\"Sex_Code\",\"Embarked_Code\",\"Title_Code\" ]\ntest_model = pd.DataFrame.copy(test_raw)\ntest_model.drop(col_delete, axis=1, inplace=True)\ntest_model.head()","e6132acf":"# creating dummy variables for test data (test_model)\ntest_model = pd.get_dummies(test_model, columns = [\"Sex\",\"Embarked\",\"Title\",\"FareBin_Code\",\"AgeBin_Code\"],\n                             prefix=[\"Sex\",\"Embarked_type\",\"Title\",\"Fare_type\",\"Age_type\"])","1df7a432":"test_model.head()","4bdd323b":"print((titan_model.columns,test_model.columns))","972bff09":"# percentage ratio of Survived and Not Survived\ncolors = [\"#61d4b3\", \"#ff2e63\"]\nsns.countplot('Survived', data=titan_EDA, palette=colors)\nplt.title('Survival Distribution', fontsize=20)\nplt.xlabel('1: Survived, 0: Not Survived')\nplt.show()\n","85a35417":"# AGE vs SURVIVAL\ng = sns.kdeplot(titan_EDA[\"Age\"][(titan_EDA[\"Survived\"] == 0) & (titan_EDA[\"Age\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(titan_EDA[\"Age\"][(titan_EDA[\"Survived\"] == 1) & (titan_EDA[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","6c2445d7":"# Boxplots AGE vs SEX & Family\nsns.factorplot(y=\"Age\",x=\"Sex\",data=titan_EDA,kind=\"box\")\nsns.factorplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=titan_EDA,kind=\"box\")\nsns.factorplot(y=\"Age\",x=\"Parch\", data=titan_EDA,kind=\"box\")\nsns.factorplot(y=\"Age\",x=\"SibSp\", data=titan_EDA,kind=\"box\")","fbad4a8b":"# Display all possible features with Survival\ncolors1 = [\"#00a8cc\", \"#54123b\"]\npossible_features = ['Pclass', 'Sex', 'SibSp', 'Parch','Embarked', 'FamilySize', 'IsAlone', 'Title', 'FareBin_Code','AgeBin_Code']\nfig, axs = plt.subplots(5, 2, figsize=(20, 30))\nfor i in range(0, 10):\n    sns.countplot(possible_features[i], data=titan_EDA, palette=colors1, hue=\"Survived\", ax=axs[i%5, i\/\/5])\n","0314242c":"titan_EDA.columns","a5ccd21d":"# Heat map corelation\nsns.heatmap(titan_EDA[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\n","7cdf1f47":"plt.subplot(231)\nplt.hist(x = [titan_EDA[titan_EDA['Survived']==1]['Age'], titan_EDA[titan_EDA['Survived']==0]['Age']], \n         stacked=True, color = ['g','r'],label = ['Survived','Not survived'])\nplt.title('Age Histogram with Survival')\nplt.xlabel('Age')\nplt.ylabel('# No of passengers')\nplt.legend()\n\nplt.subplot(232)\nplt.hist(x = [titan_EDA[titan_EDA['Survived']==1]['Fare'], titan_EDA[titan_EDA['Survived']==0]['Fare']], \n         stacked=True, color = ['g','r'],label = ['Survived','Not survived'])\nplt.title('Fare Histogram with Survival')\nplt.xlabel('Fare')\nplt.ylabel('# No of passengers')\nplt.legend()","851f74ed":"# Our data is already scaled we should split our training and test sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33,stratify=y,random_state =42)\n\n# Turn the values into an array for feeding the classification algorithms.\nx_train = x_train.values\nx_test = x_test.values\ny_train = y_train.values\ny_test = y_test.values","3c71b12a":"classifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"Naive Bayes Classifier\": GaussianNB(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"RandomForestClassifier\": RandomForestClassifier(),\n    \"Stochastic Gradient Descent\": SGDClassifier(),\n}\n\nfor key, classifier in classifiers.items():\n    classifier.fit(x_train, y_train)\n    training_score = cross_val_score(classifier, x_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n\n","231f1f79":"# Use GridSearchCV to find the best parameters.\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(x_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_","cb2f5799":"# KNN\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(x_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_","5674c010":"# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(x_train, y_train)\n# SVC best estimator\nsvc = grid_svc.best_estimator_","8d5ed3cc":"# Naive and Bayes classifier\ngnb_params = {}\ngrid_gnb = GridSearchCV(GaussianNB(), gnb_params)\ngrid_gnb.fit(x_train, y_train)\n# nb best estimator\ngnb = grid_gnb.best_estimator_\n","61fa72c0":"# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,35,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(x_train, y_train)\n# tree best estimator\ntree_clf = grid_tree.best_estimator_","c020e1ac":"# RandomForest Classifier\nrfc_params = {\"n_estimators\":list(range(1,100,1)), \"criterion\": [\"gini\", \"entropy\"]}\ngrid_rfc = GridSearchCV(RandomForestClassifier(), rfc_params)\ngrid_rfc.fit(x_train, y_train)\n# rf best estimator\nrfc = grid_rfc.best_estimator_","06630942":"#  SGDClassifier\nsgd_params = {\"max_iter\":list(range(1,5,1))}\ngrid_sgd = GridSearchCV(SGDClassifier(), sgd_params)\ngrid_sgd.fit(x_train, y_train)\n# sgd best estimator\nsgd = grid_sgd.best_estimator_","c092f9dd":"# Overfitting Case\nlog_reg_score = cross_val_score(log_reg, x_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\nknears_score = cross_val_score(knears_neighbors, x_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, x_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ngnb_score = cross_val_score(gnb, x_train, y_train, cv=5)\nprint('Naive Bayes Cross Validation Score', round(gnb_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, x_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')\n\nrfc_score = cross_val_score(rfc, x_train, y_train, cv=5)\nprint('Random Forest Classifier Cross Validation Score', round(rfc_score.mean() * 100, 2).astype(str) + '%')\n\nsgd_score = cross_val_score(sgd, x_train, y_train, cv=5)\nprint('SGD Classifier Cross Validation Score', round(sgd_score.mean() * 100, 2).astype(str) + '%')","cde10708":"# Building Neural Network model also\nfeatures = x.values\ntarget = y.values","95fe4b21":"# libraries for NN\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.model_selection import cross_val_score\nfrom keras.wrappers.scikit_learn import KerasClassifier","d52b77ee":"classifier = Sequential()\nclassifier.add(Dense(activation=\"relu\", input_dim=24, units=12, kernel_initializer=\"uniform\"))\nclassifier.add(Dense(activation=\"relu\", units=12, kernel_initializer=\"uniform\"))\nclassifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nclassifier.fit(features, target, batch_size = 10, nb_epoch = 100)","f83e4be6":"def build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu', input_dim = 24))\n    classifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier)\nparameters = {'batch_size': [25, 32],\n              'epochs': [100,500],\n              'optimizer': ['adam', 'rmsprop']}\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10)\ngrid_search = grid_search.fit(features, target)\nbest_parameters = grid_search.best_params_\nbest_accuracy = grid_search.best_score_","c9f43f92":"[best_parameters, best_accuracy]","085863b7":"# building final model based on above parameters\nclassifier = Sequential()\nclassifier.add(Dense(activation=\"relu\", input_dim=24, units=12, kernel_initializer=\"uniform\"))\nclassifier.add(Dense(activation=\"relu\", units=12, kernel_initializer=\"uniform\"))\nclassifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","f0358ce9":"model_history=classifier.fit(features, target, validation_split=0.33, batch_size = 25, nb_epoch = 100)\nprint(model_history.history.keys())","c7f14c44":"# summarize history for accuracy\nplt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","4043a22f":"y_pred = classifier.predict(test_model)","65f3cb6a":"y_pred.dtype","7d5a8da5":"#Round off the result for submission\ny_pred=y_pred.round()","3ba15dfb":"import pandas as pd\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","055cccd8":"gender_submission['Survived'] = y_pred","5a148cbd":"gender_submission.to_csv(\"..\/working\/submit.csv\", index=False)","16949fbf":"# EDA","831321e7":"Defining X & Y for model building","7d8f689b":"# Data preprocessing","a6e4c1ec":"# Predicting with Neural Network model and Submission","40053097":" # Splitting train and test","c143229a":"# Model Building","735b264a":"# Loading train & test datasets","75db1dfe":"# Feature engineering"}}