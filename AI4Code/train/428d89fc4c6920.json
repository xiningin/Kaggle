{"cell_type":{"ba014512":"code","884aa125":"code","60c56fc7":"code","5b5aaea6":"code","57bf85eb":"code","794299a3":"code","79dd47ea":"code","17d06176":"code","46c18c75":"code","76ad67e1":"code","09539990":"code","03a13e3f":"code","7b5ee31f":"code","86e74c47":"code","399b09f7":"markdown","c46b7b7c":"markdown","d513c8b3":"markdown"},"source":{"ba014512":"# Load a few helpful modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport pandas as pd\nfrom   PIL import Image\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms","884aa125":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","60c56fc7":"train.head()","5b5aaea6":"# check training data\ntrainLabelCounts = train['label'].value_counts(sort = False)\ntrainLabelCounts","57bf85eb":"# Construct the transform\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,))\n    ])\n\n# Get the device we're training on\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef get_digits(df):\n    \"\"\"Loads images as PyTorch tensors\"\"\"\n    # Load the labels if they exist \n    # (they wont for the testing data)\n    labels = []\n    start_inx = 0\n    if 'label' in df.columns:\n        labels = [v for v in df.label.values]\n        start_inx = 1\n        \n    # Load the digit information\n    digits = []\n    for i in range(df.pixel0.size):\n        digit = df.iloc[i].astype(float).values[start_inx:]\n        digit = np.reshape(digit, (28,28))\n        digit = transform(digit).type('torch.FloatTensor')\n        if len(labels) > 0:\n            digits.append([digit, labels[i]])\n        else:\n            digits.append(digit)\n\n    return digits","794299a3":"# Load the training data\ntrain = get_digits(train)\n\n# Some configuration parameters\nnum_workers = 0    # number of subprocesses to use for data loading\nbatch_size  = 64   # how many samples per batch to load\nvalid_size  = 0.2  # percentage of training set to use as validation\n\n# Obtain training indices that will be used for validation\nnum_train = len(train)\nindices   = list(range(num_train))\nnp.random.shuffle(indices)\nsplit     = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# Define samplers for obtaining training and validation batches\nfrom torch.utils.data.sampler import SubsetRandomSampler\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# Construct the data loaders\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n                    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, \n                    sampler=valid_sampler, num_workers=num_workers)\n\n# Test the size and shape of the output\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nprint(type(images))\nprint(images.shape)\nprint(labels.shape)","79dd47ea":"def calc_out(in_layers, stride, padding, kernel_size, pool_stride):\n    \"\"\"\n    Helper function for computing the number of outputs from a\n    conv layer\n    \"\"\"\n    return int((1+(in_layers - kernel_size + (2*padding))\/stride)\/pool_stride)\n\n# define the CNN architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        # Some helpful values\n        inputs      = [1,32,64,64]\n        kernel_size = [5,5,3]\n        stride      = [1,1,1]\n        pool_stride = [2,2,2]\n\n        # Layer lists\n        layers = []\n\n        self.out   = 28\n        self.depth = inputs[-1]\n        for i in range(len(kernel_size)):\n            # Get some variables\n            padding = int(kernel_size[i]\/2)\n\n            # Define the output from this layer\n            self.out = calc_out(self.out, stride[i], padding,\n                                kernel_size[i], pool_stride[i])\n\n            # convolutional layer 1\n            layers.append(nn.Conv2d(inputs[i], inputs[i+1], kernel_size[i], \n                                       stride=stride[i], padding=padding))\n            layers.append(nn.ReLU())\n            \n            # convolutional layer 2\n            layers.append(nn.Conv2d(inputs[i+1], inputs[i+1], kernel_size[i], \n                                       stride=stride[i], padding=padding))\n            layers.append(nn.ReLU())\n            # maxpool layer\n            layers.append(nn.MaxPool2d(pool_stride[i],pool_stride[i]))\n            layers.append(nn.Dropout(p=0.2))\n\n        self.cnn_layers = nn.Sequential(*layers)\n        \n        print(self.depth*self.out*self.out)\n        \n        # Now for our fully connected layers\n        layers2 = []\n        layers2.append(nn.Dropout(p=0.2))\n        layers2.append(nn.Linear(self.depth*self.out*self.out, 512))\n        layers2.append(nn.Dropout(p=0.2))\n        layers2.append(nn.Linear(512, 256))\n        layers2.append(nn.Dropout(p=0.2))\n        layers2.append(nn.Linear(256, 256))\n        layers2.append(nn.Dropout(p=0.2))\n        layers2.append(nn.Linear(256, 10))\n\n        self.fc_layers = nn.Sequential(*layers2)\n\n    def forward(self, x):\n        x = self.cnn_layers(x)\n        x = x.view(-1, self.depth*self.out*self.out)\n        x = self.fc_layers(x)\n        return x\n    \n# create a complete CNN\nmodel = Net()\nmodel","17d06176":"# specify loss function\ncriterion = nn.CrossEntropyLoss()\n\n# specify optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.0005)\n# number of epochs to train the model\nn_epochs = 25 # you may increase this number to train a final model\n\nvalid_loss_min = np.Inf # track change in validation loss\n\nmodel.to(device)\ntLoss, vLoss = [], []\nfor epoch in range(n_epochs):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    #########\n    # train #\n    #########\n    model.train()\n    for data, target in train_loader:\n        # move tensors to GPU if CUDA is available\n        data   = data.to(device)\n        target = target.to(device)\n        \n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()*data.size(0)\n        \n    ############\n    # validate #\n    ############\n    model.eval()\n    for data, target in valid_loader:\n        # move tensors to GPU if CUDA is available\n        data   = data.to(device)\n        target = target.to(device)\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # update average validation loss \n        valid_loss += loss.item()*data.size(0)\n    \n    # calculate average losses\n    train_loss = train_loss\/len(train_loader.dataset)\n    valid_loss = valid_loss\/len(valid_loader.dataset)\n    tLoss.append(train_loss)\n    vLoss.append(valid_loss)\n        \n    # print training\/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'model_cifar.pt')\n        valid_loss_min = valid_loss","46c18c75":"# Plot the resulting loss over time\nplt.plot(tLoss, label='Training Loss')\nplt.plot(vLoss, label='Validation Loss')\nplt.legend();","76ad67e1":"model.load_state_dict(torch.load('model_cifar.pt'));","09539990":"# track test loss\ntest_loss     = 0.0\nclass_correct = [0]*10\nclass_total   = [0]*10\n\nmodel.eval()\n\n# For generating confusion matrix\nconf_matrix = np.zeros((10,10))\n\n# iterate over test data\nfor data, target in valid_loader:\n    # move tensors to GPU if CUDA is available\n    data   = data.to(device)\n    target = target.to(device)\n    # forward pass: compute predicted outputs by passing inputs to the model\n    output = model(data)\n    # calculate the batch loss\n    loss = criterion(output, target)\n    # update test loss \n    test_loss += loss.item()*data.size(0)\n    # convert output probabilities to predicted class\n    _, pred = torch.max(output, 1)    \n    # compare predictions to true label\n    correct_tensor = pred.eq(target.data.view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if device == \"cpu\" else np.squeeze(correct_tensor.cpu().numpy())\n    # calculate test accuracy for each object class\n    for i in range(target.size(0)):\n        label = target.data[i]\n        class_correct[label] += correct[i].item()\n        class_total[label] += 1\n        \n        # Update confusion matrix\n        conf_matrix[label][pred.data[i]] += 1\n\n# average test loss\ntest_loss = test_loss\/len(valid_loader.dataset)\nprint('Test Loss: {:.6f}\\n'.format(test_loss))\n\nfor i in range(10):\n    if class_total[i] > 0:\n        print('Test Accuracy of %3s: %2d%% (%2d\/%2d)' % (\n            i, 100 * class_correct[i] \/ class_total[i],\n            np.sum(class_correct[i]), np.sum(class_total[i])))\n    else:\n        print('Test Accuracy of %3s: N\/A (no training examples)' % (classes[i]))\n\nprint('\\nTest Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n    100. * np.sum(class_correct) \/ np.sum(class_total),\n    np.sum(class_correct), np.sum(class_total)))","03a13e3f":"# Define the test data loader\ntest        = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntest_X      = get_digits(test)\ntest_loader = torch.utils.data.DataLoader(test_X, batch_size=batch_size, \n                                          num_workers=num_workers)","7b5ee31f":"# Create storage objects\nImageId, Label = [],[]\n\n# Loop through the data and get the predictions\nfor data in test_loader:\n    # Move tensors to GPU if CUDA is available\n    data = data.to(device)\n    # Make the predictions\n    output = model(data)\n    # Get the most likely predicted digit\n    _, pred = torch.max(output, 1)\n    \n    for i in range(len(pred)):        \n        ImageId.append(len(ImageId)+1)\n        Label.append(pred[i].cpu().numpy())","86e74c47":"# Submission\noutput = pd.DataFrame(data={'ImageId':ImageId, 'Label':Label})\noutput = output.to_csv(\"submission.csv\", index=False)\nprint('done')","399b09f7":"<h2 style=\"font-weight: bold\">Digit Recognizer<\/h2>\n\n<h4>This is my third published notebook on Kaggle. Well! Well you guess it! it's gonna be about the Digit Recognizer Competition \ud83d\ude04\ud83d\ude04<br><br>I will be doing a simple EDA and Pre-Processing, the I will Build different models<br><br><\/h4>\n\n* <h5 style=\"font-weight: 700\">Your feedback is very welcome<\/h5>\n* <h5 style=\"font-weight: 700\">If you find this notebook useful, please don't forget to upvote it!<\/h5>\n","c46b7b7c":"#  **Checking the Data**","d513c8b3":"#  **Modeling & Submitting**"}}