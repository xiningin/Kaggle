{"cell_type":{"821528c1":"code","295c3463":"code","1457b272":"code","37f409b1":"code","0ab0561a":"code","d82a2ff1":"code","a16044d1":"code","d76f9fa4":"code","cce6fff1":"code","760d96f8":"code","93bfd59f":"code","b2f29221":"code","110954fe":"code","53eb5c2e":"code","2b562758":"code","f7197579":"code","a8768a72":"code","89e0dbec":"code","b67d51c8":"code","afbd6299":"markdown","4eb7debf":"markdown","0aa9e19b":"markdown","ce83dbdf":"markdown","6dd2c3de":"markdown","da0b80e4":"markdown","ff4b192c":"markdown","de2da8cc":"markdown"},"source":{"821528c1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import plot_confusion_matrix, classification_report, f1_score\nfrom sklearn.model_selection import train_test_split ,GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\n#Classification Models:\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\nfrom xgboost import XGBClassifier\n\n#Supress Warnings:\nimport warnings\nwarnings.filterwarnings('ignore')","295c3463":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_df = train_df.set_index(\"PassengerId\")\ntrain_df.head()","1457b272":"train_df.info()","37f409b1":"train_df.isnull().sum()\/len(train_df)","0ab0561a":"train_df.describe()","d82a2ff1":"test_df.isnull().sum()\/len(test_df)","a16044d1":"train_df.dropna(subset=[\"Embarked\"], inplace=True)\ntrain_df.drop(columns=['Cabin'],inplace=True)\n\ntrain_df.info()","d76f9fa4":"for df in [train_df, test_df]:\n    df[\"Sex\"] = df[\"Sex\"].map({\"male\":1, \"female\":0, 1:1, 0:0})\n    df[\"Embarked\"] = df[\"Embarked\"].map({\"Q\":0, \"C\":1 ,\"S\":2 ,2:2 ,1:1, 0:0})\n    \n","cce6fff1":"#select important features\nfeatures=[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\",\"Fare\",\"Embarked\"]\n","760d96f8":"\ntrain_df_model=train_df[features]\ntest_df_model=test_df[features]\n","93bfd59f":"#impute nulls by mean\nfor df in [train_df_model, test_df_model]:\n    df.fillna(df.mean(), inplace=True)\ntrain_df_model","b2f29221":"#Scalling the data using standard scaler \nsc = StandardScaler()\nfeatures = [\"Age\", \"Fare\"]\n\nsc.fit(train_df[features])\n\nfor df in [train_df_model, test_df_model]:\n    sc.fit(df[features])\n    df[features] = sc.transform(df[features])\n    \n","110954fe":"X_train, X_val, y_train, y_val = train_test_split(train_df_model,\n                                                  train_df[\"Survived\"],\n                                                  test_size = 0.2,\n                                                  stratify = train_df[\"Survived\"],\n                                                  random_state=0)\n\nX_test = test_df_model\n\nX_train.head()","53eb5c2e":"#Evaluate model :\ndef Evaluate_model(model, X_train, y_train, X_val, y_val):\n    y_pred_train = model.predict(X_train)\n    y_pred_val = model.predict(X_val)\n    \n    print(f\"  Training set Accuracy   = {model.score(X_train, y_train):.4%}\\n  Validation set Accuracy = {model.score(X_val, y_val):.4%}\\n\")\n    print(\"  Validation set Classification Report:\")\n    print(classification_report(y_val, y_pred_val, digits=4))\n    ","2b562758":"print(\"\\033[1mSupport Vector Classifier:\\033[0m\")\nmodel = SVC(C=11,\n            kernel=\"rbf\",\n            gamma=\"scale\",\n            break_ties=True,\n            random_state=0)\n\n\nmodel.fit(X_train, y_train)\nEvaluate_model(model, X_train, y_train, X_val, y_val)","f7197579":"print(\"\\033[1mK-Nearest Neighbour Classifier:\\033[0m\")\nmodel = KNeighborsClassifier(n_neighbors=5,\n                             p=3,\n                             weights=\"uniform\",\n                             metric=\"minkowski\",\n                             n_jobs=-1)\n\nmodel.fit(X_train, y_train)\nEvaluate_model(model, X_train, y_train, X_val, y_val)","a8768a72":"print(\"\\033[1mRandom Forest Classifier:\\033[0m\")\nmodel = RandomForestClassifier(n_estimators=100,\n                               criterion=\"entropy\",\n                               max_depth=6,\n                               min_samples_split=4,\n                               bootstrap=True,\n                  \n                               max_samples=0.8,\n                               oob_score=True,\n                               n_jobs=-1,\n                               random_state=0)\n\nmodel.fit(X_train, y_train)\nEvaluate_model(model, X_train, y_train, X_val, y_val)\n","89e0dbec":"test_df[\"Survived\"] = model.predict(X_test)\nsubmission = test_df[[\"PassengerId\", \"Survived\"]]\nsubmission","b67d51c8":"submission.to_csv('submission.csv', index=False)","afbd6299":"## **Preprocessing**","4eb7debf":"> Out of three models we find that the Random forest model givs us the best Accuracy ","0aa9e19b":"## **Load Data**","ce83dbdf":"> We try to Encode Our Catigorical data  ","6dd2c3de":"# **Exploring the Data**","da0b80e4":"# Model Selection ","ff4b192c":"> First we try to impute our null values by :\n* > 1 - drop the column Cabin as it will gives us no important data and it is mostly nulls in test and train \n* > 2- drop the  2 rows of column Embarked as it will not harm data to much\n* > 3- we impute the nuls in column Age by the mean of the column ","de2da8cc":"> We Notice that there is null values in cabin and Age and 2 nulls in Embarked"}}