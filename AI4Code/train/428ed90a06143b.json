{"cell_type":{"c7241d3d":"code","a3d2075e":"code","03444d7d":"code","e52ba5d6":"code","d919b04c":"code","e2e3a7d0":"code","416f8af0":"code","0df8b266":"code","6cd4aad0":"code","9533dfe8":"code","7c3c4ebc":"code","69d549ac":"code","743db557":"code","9aab5126":"code","f6acb756":"code","8de476b8":"code","1bca269d":"code","39150632":"code","df7036f5":"code","b3eff0bf":"code","e8f1bda9":"code","6863e49e":"code","12af838c":"code","7a85af1e":"code","2f4a5548":"code","d3f86363":"code","2919040f":"code","1c440706":"code","46526769":"code","e5990fb3":"code","399894b3":"code","640a9b9d":"code","be0ead58":"code","bd721be0":"code","a62f6d24":"code","ec011351":"code","16eb4130":"code","64a5099a":"code","1e411206":"code","332b68dd":"code","20e33596":"code","b3e0df63":"markdown","39b6899d":"markdown","1ef6ad4e":"markdown","09d2806f":"markdown","d756aed7":"markdown","5d6c480b":"markdown","ea919a9f":"markdown","568008e1":"markdown","9b1ca5d1":"markdown","7a76a739":"markdown","fc34efb6":"markdown","efa7d602":"markdown","098b5669":"markdown","4c8d19ea":"markdown","0f06880b":"markdown","6ee3bec9":"markdown","41ffecb0":"markdown","38e2745f":"markdown","b7186881":"markdown","15ff253e":"markdown","e1beea83":"markdown","3c18394c":"markdown","648fc657":"markdown","11743c33":"markdown","5887e82f":"markdown","ce570af1":"markdown","d8d6947c":"markdown","f733eefa":"markdown","746da71e":"markdown","ddb1dc1c":"markdown","b2227c2d":"markdown","022d91f6":"markdown","1591107c":"markdown"},"source":{"c7241d3d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # visualisation\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a3d2075e":"df = pd.read_csv(\"\/kaggle\/input\/lending-club-data\/loan.csv\")","03444d7d":"df.shape","e52ba5d6":"df=df.dropna(how='all',axis=1)","d919b04c":"df.isna().sum()","e2e3a7d0":"df = df.drop([\"mths_since_last_record\", \"mths_since_last_delinq\", \"next_pymnt_d\"], axis= 1)","416f8af0":"df = df.drop([\"revol_bal\", \"out_prncp\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_prncp\", \"total_rec_int\", \n      \"total_rec_late_fee\", \"recoveries\", \"collection_recovery_fee\", \"last_pymnt_d\", \n      \"last_pymnt_amnt\", \"issue_d\", \"earliest_cr_line\"], axis= 1)","0df8b266":"df.loan_status.unique()\ndf.drop(df.loc[df[\"loan_status\"]==\"Current\"].index, inplace=True)","6cd4aad0":"df = df.drop([\"url\",\"id\", \"member_id\", \"desc\", \"zip_code\", \"last_credit_pull_d\", \"emp_title\", \n      \"out_prncp_inv\", \"title\"], axis= 1)","9533dfe8":"df = df[df.columns[~df.apply(lambda x: x.nunique() == 1)]]","7c3c4ebc":"df = df.dropna()","69d549ac":"df.shape","743db557":"df[\"loan_status\"] = df[\"loan_status\"].apply(lambda x: 0 if x== \"Charged Off\" else 1)","9aab5126":"df[\"int_rate\"] = df[\"int_rate\"].apply(lambda x: x.strip(\"%\")).astype(\"float\")\ndf[\"revol_util\"] = df[\"revol_util\"].apply(lambda x: x.strip(\"%\")).astype(\"float\")","f6acb756":"import re","8de476b8":"df[\"emp_length\"] = df[\"emp_length\"].apply(lambda x: re.findall('\\d+[,.]\\d+|\\d+', x)[0]).astype(\"int\")","1bca269d":"df[\"term\"] = df[\"term\"].apply(lambda x: x.split()[0]).astype(\"int\")","39150632":"df.head()","df7036f5":"df.describe()","b3eff0bf":"plt.rcParams['figure.figsize'] = (12, 6)\nax = sns.boxplot(data= df, x= \"loan_amnt\")","e8f1bda9":"sns.boxplot(data= df, x= \"funded_amnt\")","6863e49e":"sns.boxplot(data= df, x= \"installment\")","12af838c":"sns.boxplot(data= df, x= \"annual_inc\")","7a85af1e":"max_loan_amnt = (df[\"loan_amnt\"].quantile(.75) - df[\"loan_amnt\"].quantile(.25)) * 1.5 + df[\"loan_amnt\"].quantile(.75)\nmax_funded_amnt = (df[\"funded_amnt\"].quantile(.75) - df[\"funded_amnt\"].quantile(.25)) * 1.5 + df[\"funded_amnt\"].quantile(.75)\nmax_installment = (df[\"installment\"].quantile(.75) - df[\"installment\"].quantile(.25)) * 1.5 + df[\"installment\"].quantile(.75)\nmax_annual_inc = (df[\"annual_inc\"].quantile(.75) - df[\"annual_inc\"].quantile(.25)) * 1.5 + df[\"annual_inc\"].quantile(.75)","2f4a5548":"df = df[df[\"loan_amnt\"] < max_loan_amnt]\ndf = df[df[\"funded_amnt\"] < max_funded_amnt]\ndf = df[df[\"installment\"] < max_installment]\ndf = df[df[\"annual_inc\"] < max_annual_inc]","d3f86363":"df.shape","2919040f":"ax = sns.countplot(data= df, x= \"grade\", order= df[\"grade\"].value_counts().index, hue= \"loan_status\", palette=['#ff0000',\"#00ff00\"])\nax.set_title(\"Countplot of the grade divided into target variable\", fontsize = 20)","1c440706":"ax = sns.histplot(data= df, x= \"loan_amnt\", hue= \"loan_status\", bins= 30, palette=['#ff0000',\"#00ff00\"])\nax.set_title(\"Histogram of the loan amount (loan_amnt)\", fontsize = 20)","46526769":"ax = sns.countplot(data= df, x= \"term\", order= df[\"term\"].value_counts().index, hue= \"loan_status\", palette=['#ff0000',\"#00ff00\"])\nax.set_title(\"Countplot of the term divided into target variable\", fontsize = 20)","e5990fb3":"ax = sns.histplot(data= df, x= \"loan_amnt\", hue= \"term\", palette= \"dark\")","399894b3":"ax = sns.displot(data= df, x= \"annual_inc\", kind = \"kde\", hue= \"loan_status\", palette=['#ff0000',\"#00ff00\"])\nplt.title(\"Test\")\nplt.xticks(rotation= 45)","640a9b9d":"ax = sns.countplot(data= df, y= \"purpose\", order= df[\"purpose\"].value_counts().index, orient= \"h\", hue= \"loan_status\", palette=['#ff0000',\"#00ff00\"])\nax.set_title(\"Countplot of the purpose divided into target variable\", fontsize = 20)","be0ead58":"ax = sns.boxplot(data= df, y= \"int_rate\", x= \"loan_status\", palette=['#ff0000',\"#00ff00\"])\nax.set_title(\"Boxplot of the int_rate divided into the target variable\", fontsize= 20)","bd721be0":"ax = sns.scatterplot(data= df, x= \"loan_amnt\", y= \"int_rate\", hue= \"loan_status\", alpha= 0.8, palette=['#ff0000',\"#00ff00\"])\nax.set_title(\"Scatterplot of int_rate and loan_amnt\", fontsize = 20)","a62f6d24":"ax = sns.countplot(data= df, x= \"loan_status\", palette=['#ff0000',\"#00ff00\"])\nax.set_title(\"Class (im)balance\", fontsize= 20)","ec011351":"sns.countplot(data= df, x= \"open_acc\", hue= \"loan_status\")","16eb4130":"obj_columns = df.select_dtypes([\"object\"]).columns\ndf[obj_columns] = df[df.select_dtypes([\"object\"]).columns].astype(\"category\").apply(lambda x: x.cat.codes)\ndf.info()","64a5099a":"from imblearn.over_sampling import SMOTE # SMOTE: Synthetic Minority Oversampling Technique\nfrom sklearn.model_selection import train_test_split\n#from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression","1e411206":"X_train, X_test, y_train, y_test = train_test_split(df.drop([\"loan_status\"], axis= 1), df[\"loan_status\"], test_size= 0.30, random_state= 123)","332b68dd":"oversample = SMOTE()\nX_train, y_train = oversample.fit_resample(X_train, y_train)","20e33596":"ax = sns.countplot(x= y_train, palette=['#ff0000',\"#00ff00\"])\nax.set_title(\"Class balance after generating synthetic data\", fontsize= 20)","b3e0df63":"### 33760 rows remain.","39b6899d":"### Maybe the grade could be a good feature for classification. The number of \"Charged Off\" cases seems to increase with the higher grade, compared to the \"Fully Paid\" cases.","1ef6ad4e":"### The class Charged Off is strongly underrepresented. For training purpose of the classification algorithm, this needs to be balanced (later when splitting the dataset into train and test data).","09d2806f":"### Looks like as if a higher loan amount and a higher int rate lead to more Charged Off cases. But additionally what can be seen at all plots, it looks like an imbalanced class problem.","d756aed7":"### Again a countplot. Honestly, I didn't take a look at the function in much detail, but the default settings seem to perform an over- and undersampling.","5d6c480b":"### The number of open accounts doesn't seem to be a good feature, because the distribution of Charged Off and Fully Paid is similiar.","ea919a9f":"### Using the split method to separate the number and the string.","568008e1":"### First, all columns with type \"object\" must be converted to numeric values. The cat.codes method will be used for this. But before, the object types need to be converted to type \"category\". The next lines of code will do this.","9b1ca5d1":"### It makes sense, that the term of 60 months is more strongly represented for higher loan amounts. This and the information from the countplot above, the consequence could be that a higher loan amount could be a good feature. In the histogram \"Histogram of the loan amount\" it is not so well visible.","7a76a739":"### Extract the integer value from the string of the \"emp_length\" column using the \"re\" package and converting it to type integer.","fc34efb6":"### Converting target variable \"loan_status\" into an integer (0 and 1)","efa7d602":"### Checking the number of unique values for each column and remove columns with only one value.","098b5669":"### Overview of columns with NA values","4c8d19ea":"## Coming step: \n## Classification","0f06880b":"### Splitting the dataset into training (70%) and test (30%) data before performing SMOTE (only for training data)","6ee3bec9":"### The columns \"mths_since_last_delinq\", \"mths_since_last_record\" and \"next_pymnt_d\" will be dropped because of the high number of NA values.","41ffecb0":"## Aim of this notebook is the exploratory data analysis and visualisation of the Lending club dataset. The dataset consists of 39717 rows and 111 columns. The dataset reflects different credits of bank customers and their status.","38e2745f":"### Rows with \"loan_status\" == \"Current\" will be removed.","b7186881":"### Remove the remaining rows with NA values in it. After removing, 36800 rows and 23 columns are available.","15ff253e":"### Importing necessary packages for classification","e1beea83":"### The median of the Charged Off cases seems to be a little bit higher as the Fully Paid cases. The int_rate could be a helpful feature.","3c18394c":"### At some columns (f.e. loan_amnt, funded_amnt, installment, especially annual_inc) is the difference between the 3rd quantil and the maximum quite high. Boxplot of this four columns:","648fc657":"### Based on the description, all columns will be removed which only can occur when the loan is already accepted. Target is to create a classifier to classify before the loan is accepted, not afterwards.","11743c33":"### Remove columns which useless information. Based on the description, some information is registered from the customer manually.","5887e82f":"### The distribution seems to be similiar, both target cases have the maximum at around 40000 a year.","ce570af1":"### Column \"int_rate\" and \"revol_util\": removing the percent sign and converting into type float.","d8d6947c":"### First impression: purpose is not a useful feature.","f733eefa":"### Using the \"describe\" method to get an overview of important statistical values.","746da71e":"## Preparing the data for classification","ddb1dc1c":"### The data does have a lot of columns with only NA values. All of these columns are dropped.","b2227c2d":"### The number of Fully Paid with term = 60 is much less compared to term = 36, but the number of Charged Off cases is almost similar. Could be a useful feature.","022d91f6":"### For classification, 36800 lines are available. The outlier will be removed using the interquantile spacing method. 3rd Quantil + (3rd Quantil - 1st Quantil) * 1.5. The distance of the 3rd to the 1st Quantil will be multiplied with 1.5, and added to the 3rd Quantil. The rows with greater value than this will be removed.","1591107c":"## Visualisation"}}