{"cell_type":{"405bcd49":"code","a20a845e":"code","ed854162":"code","6875d011":"code","9022d5d4":"code","f5f0a4d1":"code","4718f64b":"code","9739f0d1":"code","2afcc965":"code","cc9cecee":"code","dfc4ff98":"code","539f3822":"code","f4104e42":"code","648339c0":"code","546f4c24":"code","ad29f6f9":"code","5f0c3f13":"code","86d0df7c":"code","4c68f1b5":"code","5670b7c5":"code","7cf3e799":"code","822b963f":"code","fe2b8a0d":"code","e13a374b":"code","34082fa4":"code","68c09298":"code","5b0cfafc":"code","1d6a316e":"code","49e0903c":"code","73c793f1":"code","c668bbdd":"code","2537d873":"code","4bb562a9":"markdown","705a45f7":"markdown","b67243c0":"markdown","4b614a4b":"markdown","c51d12cb":"markdown","30cfeb80":"markdown","833420c8":"markdown","9aff798f":"markdown","f8d44a0c":"markdown","138bdfca":"markdown","abe82b4a":"markdown","02539701":"markdown","de28e274":"markdown","ed0cc97a":"markdown"},"source":{"405bcd49":"## Importing libraries and iris datset\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.neighbors import KNeighborsClassifier  ## class of sklearn which gives classify the dataset\nfrom sklearn.linear_model import LogisticRegression ## Importing logistic regression model from sklearn\nimport numpy as np\n##Evaluation metrics for KNN\n\nfrom sklearn import metrics\n","a20a845e":"## save iris datset into an object\niris= load_iris()\ntype(iris)","ed854162":"## print out the dataset\n\nprint(iris)","6875d011":"## print out the feature names or columns names or independent variables\n\nprint(iris.feature_names)","9022d5d4":"## print out the response name or dependent variable or target columns. It will print out encoded names.\n\nprint(iris.target_names)","f5f0a4d1":"## print out the integer of dependent varaibles\n\nprint(iris.target)","4718f64b":"## print out the dimension of data(1st dimension= no. of obs, 2nd dimension= no. of features)\n\nprint(iris.data.shape)","9739f0d1":"## print out the dimension of target columns\/dependent variable\n\nprint(iris.target.shape)","2afcc965":"## storing features in x object and target in y object\n\nx=iris.data\ny=iris.target","cc9cecee":"## 1. instantiate the estimator\n\nknn= KNeighborsClassifier(n_neighbors=1)  ### n_neighbours=1 will look for 1 similar group","dfc4ff98":"## 2. To look inside the model and its parameters\n\nprint(knn)","539f3822":"## 3. Fit the data into model\n\nknn.fit(x,y)","f4104e42":"## 4. Predict the unknown data for single obs\n\nknn.predict([[3,5,3,2]])  ## keep in mind in latest scikit learn to give an unknown list use double brackets\n\n##For this it has predicted sertosa\n","648339c0":"## 5. Predict the unknown dat of multiple obs\n\nknn.predict([[3,5,6,7],[7,6,9,3]])\n\n## for this it has predicted virginica","546f4c24":"## instansiate the model\nknn= KNeighborsClassifier(n_neighbors=5)","ad29f6f9":"## fit the data into model\n\nknn.fit(x,y)","5f0c3f13":"##predict the depenent values for KNN=5\n\nknn_predict=knn.predict(x)\n","86d0df7c":"## predict the unknown values\n\ny_new=[[3, 5, 4, 2], [5, 4, 3, 2]]\n\nknn_predict=knn.predict(y_new)\nprint(knn_predict)\n\n## for this it has predicted versicolor","4c68f1b5":"## using logistic regression for same problem and identify the accuracy of its model","5670b7c5":"## instansiate the model\n\nlogreg= LogisticRegression(max_iter=1000)\nprint(logreg)","7cf3e799":"##fit the model into dataset\n\nlogreg.fit(x,y)","822b963f":"## predict the dependent values\n\nlogreg_predict= logreg.predict(x)","fe2b8a0d":"## Predict the unknown values\n\nlog_predict=logreg.predict([[3,5,3,1]])\nprint(log_predict)\n\n## logistic regression predcit the specis for given unknown value","e13a374b":"##checking accuracy of logistic regression model\n\nprint(metrics.accuracy_score(y,logreg_predict))\n\n","34082fa4":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.4,random_state=4)\n\n","68c09298":"## fitting test\/train dataset into knn  , k=5\n\nknn.fit(x_train,y_train)","5b0cfafc":"## predicting the dataset\ny_pred_tt= knn.predict(x_test)","1d6a316e":"## checking the accuracy of dataset\nprint(metrics.accuracy_score(y_test,y_pred_tt))","49e0903c":"log_tt= LogisticRegression()\n","73c793f1":"log_tt.fit(x_train,y_train)","c668bbdd":"y_log_tt=log_tt.predict(x_test)","2537d873":"print(metrics.accuracy_score(y_test,y_log_tt))","4bb562a9":"# Lets understand the IRIS dataset:\n\n**Source:**\n\n**Creator:**\n\nR.A. Fisher\n\n**Donor:**\n\nMichael Marshall (MARSHALL%PLU '@' io.arc.nasa.gov)\n\n\n**Data Set Information:**\n\nThis is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n\nPredicted attribute: class of iris plant.\n\nThis is an exceedingly simple domain.\n\nThis data differs from the data presented in Fishers article (identified by Steve Chadwick, spchadwick '@' espeedaz.net ). The 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\" where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\" where the errors are in the second and third features.\n\n\n**Attribute Information:**\n\n1. sepal length in cm\n2. sepal width in cm\n3. petal length in cm\n4. petal width in cm\n5. class:\n-- Iris Setosa\n-- Iris Versicolour\n-- Iris Virginica\n\n\n**Relevant Papers:**\n\nFisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).\n[Web Link]\n\nDuda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218.\n[Web Link]\n\nDasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71.\n[Web Link]\n\nGates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433.\n[Web Link]\n\nSee also: 1988 MLC Proceedings, 54-64.\n\n","705a45f7":"# Logistic regression on test\/train datasets","b67243c0":"# 2. Building model by splitting datasets into test\/train datasets","4b614a4b":"# Evaluation metrics for Data models","c51d12cb":"Creating test train dataset for model training","30cfeb80":"![1_wW8O-0xVQUFhBGexx2B6hg.png](attachment:1_wW8O-0xVQUFhBGexx2B6hg.png)","833420c8":"# KNN model on test\/train dataset\n \n","9aff798f":"# 1. Training the whole dataset on model building","f8d44a0c":"# For Values of K=1","138bdfca":"# For different values of K, here we are choosing K=5","abe82b4a":"Notice in the image above that most of the time, similar data points are close to each other. The KNN algorithm hinges on this assumption being true enough for the algorithm to be useful. KNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics we might have learned in our childhood\u2014 calculating the distance between points on a graph.\n\nThere are other ways of calculating distance, and one way might be preferable depending on the problem we are solving. However, the straight-line distance (also called the Euclidean distance) is a popular and familiar choice.\n\nSource: https:\/\/towardsdatascience.com\/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761","02539701":"# Supervised Learning Project\n\n  **  Classification:**\n    \n    Classsifcation is an common machine learning problem where computer needed to separate objects into common groups. Say suppose you had\n    gone to a bar and bartender asked you to select alcohol variery like beer, wine ,whiskey, brandy based on color choices.\n    \n    What an human will do will see the color of each color first and record as beer,whiskey,wine etc. Then bartender will do  a test and show\n    some bottle and asked which are type of alcoholic beverage. Huan will choosed based on his experience and tell which bottle is which is \n    which alcoholic beverage.\n    \n    Same computer will also do but to do it needed to first feeded with lots of data so that whner unknown data is presented it will choose \n    the nearby answer as humans do.\n    \n    **To perform classsification there are lots of alogithm present some famous are:**\n    \n    1. Logistic regression\n    2. KNN- K- nearest neighbours\n    3. SVM - Support vector machine\n    4. Decision trees\n    5. Random forest classifier\n    \n    Here we will evaluate the KNN on iris dataset. Let understand KNN first:\n    \n    The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n    \n    \n\n    ","de28e274":"# Model building steps","ed0cc97a":"Comparing with other model for accuracy"}}