{"cell_type":{"13f0beb5":"code","2bf29621":"code","a0f05d61":"code","457a6a47":"code","66139922":"code","6e706fa4":"code","68c2fd33":"code","9e5455c7":"code","702751e7":"code","a82b0ae8":"code","85359adc":"code","3452a5be":"code","31750835":"code","9fad184c":"code","858420ba":"code","d1cfe55a":"code","798acc57":"code","3a181da4":"code","077d88c9":"code","e571d33c":"code","ee7468d8":"code","6ce10e4b":"code","1a214cec":"code","825ff149":"code","a8b60498":"markdown","f0678efc":"markdown","78e0ef29":"markdown","28c4cf7d":"markdown","6329f8b9":"markdown","50d03ef2":"markdown","5e8e9cee":"markdown"},"source":{"13f0beb5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2bf29621":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","a0f05d61":"from transformers import BertTokenizer, TFBertModel\nfrom transformers import AdamWeightDecay, AutoTokenizer, TFAutoModelForSequenceClassification\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","457a6a47":"\"\"\"\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\n\"\"\"","66139922":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","6e706fa4":"!wget https:\/\/raw.githubusercontent.com\/AnthonyDumais\/MAIS202\/main\/train.csv","68c2fd33":"!wget https:\/\/raw.githubusercontent.com\/AnthonyDumais\/MAIS202\/main\/test.csv","9e5455c7":"train = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\ntrain.head()","702751e7":"train.hypothesis.values[1]","a82b0ae8":"labels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","85359adc":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","3452a5be":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s))\n   tokens.append('[SEP]')\n   return tokenizer.convert_tokens_to_ids(tokens)","31750835":"encode_sentence(\"Buffalo buffalo buffalo buffalo\")","9fad184c":"\"\"\"\ndef bert_encode(hypotheses, premises, tokenizer):\n    \n  num_examples = len(hypotheses)\n  \n  sentence1 = tf.ragged.constant([\n      encode_sentence(s)\n      for s in np.array(hypotheses)])\n  sentence2 = tf.ragged.constant([\n      encode_sentence(s)\n       for s in np.array(premises)])\n\n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n  input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n  type_cls = tf.zeros_like(cls)\n  type_s1 = tf.zeros_like(sentence1)\n  type_s2 = tf.ones_like(sentence2)\n  input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n  return inputs\n\"\"\"","858420ba":"def bert_encode(hypotheses, premises, tokenizer, max_length=234):\n\n    x = [h + ' [SEP] ' + p for h, p in zip(np.array(hypotheses), np.array(premises))]\n    x = tokenizer(x, padding=True, truncation=True, max_length=max_length)\n\n    inputs = {\n          'input_word_ids':tf.ragged.constant(x['input_ids']).to_tensor(),\n          'input_mask': tf.ragged.constant(x['attention_mask']).to_tensor(),\n          'input_type_ids': tf.ragged.constant(x['token_type_ids']).to_tensor()}\n\n    return inputs\n","d1cfe55a":"train_input = bert_encode(train.premise.values, train.hypothesis.values, tokenizer)","798acc57":"max_len = 234\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","3a181da4":"with strategy.scope():\n    model = build_model()\n    model.summary()\n\ntf.keras.utils.plot_model(model, \"my_model.png\", show_shapes=True)","077d88c9":"model.fit(train_input, train.label.values, epochs = 2, verbose = 1, batch_size = 64, validation_split = 0.2)","e571d33c":"test_input = bert_encode(test.premise.values, test.hypothesis.values, tokenizer)","ee7468d8":"predictions = [np.argmax(i) for i in model.predict(test_input)]","6ce10e4b":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","1a214cec":"submission.head()","825ff149":"submission.to_csv(\"submission.csv\", index = False)","a8b60498":"## Configuring and connecting to TPU","f0678efc":"## Downloading and parsing the data","78e0ef29":"## Preparing data for input","28c4cf7d":"## Creating & Training Model\nIncorporate the BERT transformer into a Keras Functional Model. ","6329f8b9":"## Importing libraries and making tokenizers\n\n","50d03ef2":"# Natural Language Inference\nThis project was built following the [Tutorial Notebook](https:\/\/www.kaggle.com\/anasofiauzsoy\/tutorial-notebook) by  [@anasofiauzsoy ](https:\/\/www.kaggle.com\/anasofiauzsoy).","5e8e9cee":"## Generating and Submitting Predictions"}}