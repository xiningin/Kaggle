{"cell_type":{"55daeb13":"code","7861cf4e":"code","34565faf":"code","5f858467":"code","e59f5a58":"code","4a622eb3":"code","0ffa3520":"code","00787880":"code","7b5c4e0f":"code","905f0e85":"code","0e896ad0":"code","db0859a7":"code","ec40bd21":"code","a43b9df1":"code","97645e86":"code","aea44e98":"code","20ab68d7":"code","c4548e39":"code","410a4901":"code","54230efd":"code","9cc2975e":"markdown","51103f9e":"markdown","eab8839c":"markdown","47f2ff44":"markdown","06b4dff1":"markdown","1fba9acb":"markdown","58aac8c2":"markdown","b2a61270":"markdown","17cd5e87":"markdown","778e92ac":"markdown","d69db2dd":"markdown","8d7e35bf":"markdown","8a365fc7":"markdown","f3e3e2e6":"markdown","a18352e1":"markdown","511be230":"markdown","5cb6772d":"markdown","e6145578":"markdown"},"source":{"55daeb13":"from IPython.display import IFrame\nIFrame('https:\/\/app.powerbi.com\/view?r=eyJrIjoiNzVjZmJkM2UtYzdhMi00NGQ2LWEwZTYtYjZjNjU4OTc0MDI3IiwidCI6ImRjMWYwNGY1LWMxZTUtNDQyOS1hODEyLTU3OTNiZTQ1YmY5ZCIsImMiOjEwfQ%3D%3D', width=\"100%\", height=500)","7861cf4e":"!pip install faiss-cpu --no-cache -q\n# !pip install faiss\nimport pandas as pd\nimport numpy as np\nimport os\nimport glob\nimport faiss\nfrom faiss import normalize_L2\nimport collections\nfrom collections import defaultdict\nimport json\nfrom datetime import datetime\nimport re\nfrom nltk.tokenize import sent_tokenize","34565faf":"def setup_local_data():\n    input_dir = '\/kaggle\/input\/'\n    for item in glob.glob(os.path.join(input_dir,'*')):\n        print(item)\n    return input_dir\n\ndef read_metadata(input_dir):\n    metadata = pd.read_csv(os.path.join(input_dir,'CORD-19-research-challenge','metadata.csv'))\n    print(metadata.info())\n    return metadata\n\ndef read_cord_19_embeddings(input_dir):                                      \n    emb_path = glob.glob(os.path.join(input_dir,'CORD-19-research-challenge', 'cord_19_embeddings', ''.join(('cord_19_embeddings_2020-07-27.csv'))))#[0]\n    print('input_dir: ', input_dir)\n    print('emb_path: ', emb_path)\n    emb = pd.read_csv(emb_path[0], header = None, index_col = 0)\n    print(emb.head())\n    return emb\n","5f858467":"# Fetching cord_uids of the seed articles by matching article titles\ndef get_ref_cord_uid(target_table, metadata):\n    titles = target_table['Study'].unique().tolist()\n    cord_uids = metadata[metadata['title'].isin(titles)]['cord_uid'].tolist()\n    return cord_uids\n    ","e59f5a58":"local_dir = setup_local_data()\nmetadata = read_metadata(local_dir)\nemb = read_cord_19_embeddings(local_dir)","4a622eb3":"# Creating a matrix to store article embeddings \nxb = np.ascontiguousarray(emb).astype(np.float32)\n# Assigning dimension for the vector space\nd = xb.shape[1]","0ffa3520":"# Building the index\nindex = faiss.IndexFlatIP(d) #IndexFlatIP: taking inner product of the vectors\nprint(index.is_trained)\n# AddING vectors to the index\nnormalize_L2(xb)\nindex.add(xb)                  \nprint(index.ntotal)","00787880":"# Listing summary tables from a task\n\n# task_dir = '3_patient_descriptions'\n# table_path = os.path.join(local_dir,'CORD-19-research-challenge\/Kaggle\/target_tables',task_dir)\n# for root, dirs, files in os.walk(table_path):\n#     for filename in files:\n#         print(filename)\n    ","7b5c4e0f":"# Refreshing from previous summary tables\ntable_path = os.path.join(local_dir,'summary-tables-2020-06-16')","905f0e85":"def refresh_table(table_name:str, metadata, emb):\n    \n    \"\"\"Refresh the target table with new additions if available\n       Params:\n       table_name: string, file name of the target table\n       metadata: df, loaded from metadat.csv\n       emb: df, loaded from cord_19_embeddings\n    \"\"\"\n    \n    # Reading in the target table\n    df = pd.read_csv(os.path.join(table_path, table_name), index_col=0)\n    print('Number of studies in target table: {}'.format(len(df['Study Link'].unique())))\n\n    target_columns = df.columns\n    \n    # Finding k nearest neighbors for each seed article\n    # Retrieving seed article's cord_uid\n    ref_cord_uids = get_ref_cord_uid(df,metadata)\n    k = 10\n    similar_id_list=[]\n    for idx in ref_cord_uids:\n        # Retreiving ref article's embeddings\n        xq = np.ascontiguousarray(emb.loc[idx]).reshape(1,-1).astype(np.float32)\n        # Remember to normalize\n        normalize_L2(xq)\n        # Searching top k similar articles and return a distance array (D) and an index array (I)\n        D, I = index.search(xq, k+1)   # search k+1 to get k articles additional to self\n        similar_id_list.extend(I.tolist()[0])\n        \n    # Counting the frequency of recurrence for each article found \n    similar_cord_uid_list = [ cid for cid in emb.iloc[similar_id_list].index if cid not in ref_cord_uids ]\n    frequency_of_reoccurence = collections.Counter(similar_cord_uid_list)\n    \n    # Finalizing the list of article to include: seed articles + new articles with recurrent freq > 1 (at least 2)\n    final_list = ref_cord_uids + [k for k in frequency_of_reoccurence.keys() if frequency_of_reoccurence[k] > 1]\n    \n    # Geting metadata for all articles (with full text) in the final list \n    final_articles_metadata = metadata[(metadata['cord_uid'].isin(final_list)) & (pd.notnull(metadata['pdf_json_files']) | pd.notnull(metadata['pmc_json_files']))]\n    \n    #Attaching \"Added on\" variable as required in the target table. This variable records the date that the article is added to the summary table.\n    if 'Added on' in df.columns:\n        final_articles_metadata['Added on'] = final_articles_metadata['title'].map(dict(zip(df['Study'],df['Added on'])))\n\n        final_articles_metadata['Added on'] = final_articles_metadata['Added on'].fillna(datetime.today().strftime('%-m\/%d\/%Y'))\n    else:\n        final_articles_metadata['Added on'] = datetime.today().strftime('%-m\/%d\/%Y')\n        \n    return target_columns, final_articles_metadata","0e896ad0":"def get_full_text_sentences(final_articles_metadata):\n    \n    \"\"\"Getting full text for the articles in the final article list for data extraction\n       Params: df derived from refresh_table\n    \"\"\"\n    # Reading json file if exists\n    full_text_dict = defaultdict(list)\n    final_articles_metadata = final_articles_metadata.set_index('cord_uid')\n    for idx in final_articles_metadata.index:\n        if pd.notnull(final_articles_metadata.loc[idx]['pmc_json_files']):\n            pdf = final_articles_metadata.loc[idx]['pmc_json_files']\n        elif pd.notnull(final_articles_metadata.loc[idx]['pdf_json_files']):\n            pdf = final_articles_metadata.loc[idx]['pdf_json_files']\n\n        with open(os.path.join(local_dir,'CORD-19-research-challenge',pdf), 'r') as f:\n            full_text = json.loads(f.read())\n        #  Extracting columns to a new df\n        #  Section labels will be helpful for filtering out noises during data extraction\n        for item in full_text['body_text']:\n            full_text_dict['cord_uid'].append(idx)\n            full_text_dict['section'].append(item['section'])\n            full_text_dict['text'].append(item['text'])\n\n    df = pd.DataFrame(full_text_dict)         \n    \n    # We do a minor preprocessing here mainly to comfrom to our NER training dataset (details to follow)\n    def preprocess(text):\n        text = re.sub('feces|faeces|fecal|faecal|stools|stool samples|stool sample', 'stool', text)\n        text = re.sub('stool', 'stool samples', text)\n        text = re.sub('serum|plasma', 'blood', text)\n        text = re.sub('swabs', 'swab', text)\n        text = re.sub('-swab', ' swab', text)\n        text = re.sub(u'\\u00B7', '.', text)\n        return text.lower()\n    df['text'] = df['text'].apply(lambda x: preprocess(str(x)))\n    \n    # Tokenizing paragraphs from the jsons into sentences\n    df['sentence'] = df['text'].apply(lambda x: sent_tokenize(str(x)))\n    df = df.explode('sentence')\n\n    df = df.join(final_articles_metadata, on='cord_uid')\n    \n    return df\n","db0859a7":"!pip install ijson -q\n!pip install word2number -q\nimport spacy, ijson, re\nfrom itertools import cycle\nfrom word2number import w2n\nfrom dateutil.parser import parse\nimport calendar","ec40bd21":"# Loading the time period model\ntpnlp = spacy.load(os.path.join(local_dir,'time-period-ner-75-v2','TimePeriodNER_75_v2'))\n\ndef get_age(sentence, model):\n    \n    \"\"\"Extract median\/mean\/average age from sentences\"\"\"\n    \n    sentence = str(sentence)\n    # To reduce noise, we extract from senteces reporting median or mean age of the population\n    if any(substring in sentence for substring in ['median age','mean age','average age']):  \n        doc = model(sentence)\n        ents = [ent.text for ent in doc.ents] \n        if len(ents) > 0:\n            return ', '.join(ents)\n\ndef get_time_period(sentence: str, model, period_types: list):\n    \n    \"\"\"Extract median\/mean\/average length of viral shedding time\n    \n    params:\n    sentence: sentence to extract data from\n    model: pretrained\/custom spacy model to call\n    period_types: list of keywords to indicate shedding, incubation, etc...\n    \n    \"\"\"\n    # By switching the \n    PERIOD_TYPES = period_types\n    \n    sentence = str(sentence)\n    # Filtering sentence by context keywords\n    if any(s in sentence for s in ['median','mean','average','period','time','duration']) \\\n    and not any(s in sentence for s in ['diagnosis', 'sampling'])\\\n    and any(s in sentence for s in PERIOD_TYPES): \n        \n        doc = model(sentence)\n        context = [ent.text for ent in doc.ents if ent.label_ == 'TPcontext'] \n        data = [ent.text for ent in doc.ents if ent.label_ == 'TPdata']\n        \n        if len(context)*len(data)>0:\n            output = [item for item in [c + ': ' + d for c, d in zip(cycle(context), data)] \\\n                      if any( s in item for s in PERIOD_TYPES) and ('day' in item) and re.search(r'\\d+(\\.\\d+)?', item)] \n            if len(output) > 0:\n                return  '; '.join(output)","a43b9df1":"\ncord_ner_nlp = spacy.load(os.path.join(local_dir,'cord-ner-space\/cord_ner')) \n  \ndef get_sample_type(sentence, model):\n    \n    \"\"\"Extract SUBSTRATE (sample\/specimen) from sentences\"\"\"\n    \n    sentence = str(sentence)\n    doc = model(sentence)\n    \n    # Getting texts with SUBSTRATE entity label \n    ents = [ent.text for ent in doc.ents if ent.label_ == 'SUBSTRATE']\n    tokens = sentence.split()\n    \n    # We mannually extracted specimens from nasopharygeal swabs as the model wasn't tuned enough to recognize them\n    if 'swab' in tokens:\n        idx = tokens.index('swab')\n        ents.append(' '.join([tokens[idx-1], tokens[idx]]))\n    \n    # Checking for texts wtih CORONAVIRUS entity label \n    has_corona = 'CORONAVIRUS' in [ent.label_ for ent in doc.ents]\n    # Checking mentions of covid-19 in the same sentence increases the confidence that the sentece has relevant information\n    if ents and has_corona:  \n        return ', '.join(ents)","97645e86":"def remove_dates(string):\n    \n    \"\"\"Helper function to remove the dates like Mar 30 2020 or 30 Mar 2020\"\"\"\n    \n    months = '('+ '|'.join([calendar.month_name[i] for i in range(1,13)]) + ')'\n    months_abbr = '('+'|'.join([calendar.month_abbr[i] for i in range(1,13)])+ ')'\n    dates = [months + '\\s\\d{1,2}\\s\\d{4}', \n             '\\d{1,2}\\s'+months+'\\s\\d{4}',\n             months_abbr + '\\s\\d{1,2}\\s\\d{4}', \n             '\\d{1,2}\\s'+months_abbr+'\\s\\d{4}']\n   \n    string = re.sub('|'.join(dates), ' ', string)\n    return string\n\ndef get_sample_size_regex(abstract:str):\n    \n    \"\"\"Extract sample size from the abstracts\"\"\"\n    \n    abstract = re.sub(',','',abstract)\n    abstract = remove_dates(abstract)\n    words_nums = []\n    for word in abstract.split():\n        try:\n            words_nums.append(str(w2n.word_to_num(word)))\n        except:\n            words_nums.append(word)\n    abstract = ' '.join(words_nums)\n    if any(w in abstract for w in ['enroll',\n                                   'includ',\n                                   'review',\n                                   'extract',\n                                   'divide',\n                                   'collect',\n                                   'examin',\n                                   'evaluat',\n                                   'report',\n                                   'identif',\n                                   'admit',\n                                   'of the'])\\\n    and any(w in abstract for w in ['patients',\n                                    'cases',\n                                    'men', \n                                    'males',\n                                    'children',\n                                    'articles',\n                                    'studies'])\\\n    and re.search('\\s\\d+([^\\.%]{1,25})(patients|cases|men|males|chidren|articles|studies)', abstract):\n        return re.search('\\s\\d+([^\\.%]{1,25})(patients|cases|men|males|chidren|articles|studies)', abstract).group().strip()\n\n# # Code to implement the sample size NER model\n# nnlp = spacy.load(os.path.join(local_dir,'sample-size-ner-v3','sentence_level_model_v3'))\n# def get_sample_size(abstract, model):\n#     \"\"\"Extract sample size from sentences\"\"\"\n#     sentences = sent_tokenize(str(abstract).lower())\n#     nums=[]\n#     for sentence in sentences:\n#         doc = model(sentence)\n#         ents = [ent.text for ent in doc.ents if ent.label_=='enrolled' or ent.label_=='enrolled_add']\n#         try:\n#             nums.extend([str(w2n.word_to_num(ent)) for ent in ents if not any(s.isdigit() for s in ent)])\n#         except:\n#             nums.extend(ents)\n#     if len(nums) > 0:\n#         return ', '.join(filter(None,nums))","aea44e98":"!pip install catboost==0.20.1 -q --force-reinstall\nfrom catboost import Pool, CatBoostClassifier","20ab68d7":"def get_study_type(test_pool):\n    \n    \"\"\"Predict study design based on title and abstract of an article\n        Params:\n        test_pool: title and abstract organized in catboost compliant format\n        EX: \n        test_pool = Pool(df[['abstract', 'title']],\n        feature_names=['abstract', 'title'],\n        text_features=['abstract', 'title'])\n    \"\"\"\n    CATBOOST_MODEL_NAME=\"study_design_catboost_classifier_7_June_2020.cbm\"\n    # Instantiating the model\n    model = CatBoostClassifier()\n    # Loading pretrained classifier\n    model.load_model(os.path.join(local_dir,'study-type-classifier',CATBOOST_MODEL_NAME))\n    # Predicting on input data\n    predictions = model.predict(test_pool)\n    return predictions","c4548e39":"def find_target_number(x):\n    \n    \"\"\"Helper function to clean up extracted excerpts and extract the number of days.\n        In most cases, it extracts the mean or median value but when range is reported, \n        the function extracts the upper bound.\n    \"\"\"\n    match = re.match(r'(.*?)(\\d+(\\.\\d+)?)(\\s)(day)', x)\n    if match and \"\u00b1\" not in x:\n        return match.group(2)\n    else:\n        return re.search(r'\\s\\d+(\\.\\d+)?(days)?', x).group()","410a4901":"def assemble_summary_table(table_name:str, metadata, emb):\n    \n    \"\"\" Assemble a summary table from end to end. \n        Params:\n        table_name: str, the file name of the target table\n        metadata: df, laoded from metadat.csv\n        emb: df, loaded from cord_19_embeddings   \n    \"\"\"\n    \n    # Refreshing target table to include potential additions\n    target_columns, final_articles_metadata = refresh_table(table_name, metadata, emb)\n    \n    # Retriving full text for data extraction\n    df = get_full_text_sentences(final_articles_metadata)\n\n    # Removing sections that can include data from previous\/background studies, rather than the study per se\n    sections_to_keep = [s for s in df['section'] if s.lower() not in \n                    ['title',\n                     'abstract', \n                     'background',\n                     'summary background',\n                     'introduction',\n                     'discussion',\n                     'discussions',\n                     'statistical analysis']\n                   ]\n    df = df[df['section'].isin(sections_to_keep)]\n    \n    # Extracting median\/mean\/average age from full text\n    df['Age'] = df['sentence'].apply(lambda x: get_age(x, tpnlp))\n    \n    # Extracting sample size\n    df_sample_size = df[['cord_uid', 'abstract']].drop_duplicates() \n    df_sample_size['Sample Size'] = df_sample_size['abstract'].apply(lambda x: get_sample_size_regex(x) if pd.notnull(x) else x)\n    \n    # Extracting type of sample obtained\n    df['Sample Obtained'] = df['sentence'].apply(lambda x: get_sample_type(x, cord_ner_nlp))\n    \n    # Extracting time period based on context\n    if \"shedding\" in table_name.lower():\n        df['extracted'] = df['sentence'].apply(lambda x: get_time_period(x, tpnlp, ['shedding','positive','clearance']))\n        \n    if 'incubation' in table_name.lower():\n        df['extracted'] = df['sentence'].apply(lambda x: get_time_period(x, tpnlp, ['incubation']))\n    \n    # Extracting numeric values from the extracted \n    df['Days'] = df['extracted']\\\n    .apply(lambda x: find_target_number(x) if pd.notnull(x) and re.search(r'\\s\\d+(\\.\\d+)?(days)?', x) else x)\\\n    .apply(lambda x: ' '.join([x, 'days']) if pd.notnull(x) else x)\n    \n    df['Range (Days)'] = df['extracted'].apply(lambda x: x[x.find(\"(\")+1:x.find(\")\")] if (pd.notnull(x)) and (\")\" in x) else None)\n\n    #### Preparing final component #1: study metadata ####\n    output_metadata = final_articles_metadata[['cord_uid','publish_time','title','abstract','url','journal','source_x','Added on']]\n    output_metadata['journal'] = output_metadata['journal'].fillna(output_metadata['source_x'])\n    \n    # Predicting study type\n    test_pool = Pool(\n    output_metadata[['abstract', 'title']].fillna(\"\"),\n    feature_names=['abstract', 'title'],\n    text_features=['abstract', 'title'])\n    \n    output_metadata['Study Type'] = get_study_type(test_pool)\n\n    #### Preparing final component #2: extracted metadata ####\n    extracted_metadata = df[['cord_uid','Age', 'Sample Obtained']].groupby('cord_uid', as_index=False)\\\n    .agg(lambda x: list(set(x))) #add sample size, study type\n\n    # Compressing multuple results into one and cleaning up\n    extracted_metadata = extracted_metadata.set_index('cord_uid')[['Age','Sample Obtained']]\\\n    .applymap(lambda x: ', '.join(filter(None,x))).reset_index()\n    \n    extracted_metadata['Sample Obtained'] = extracted_metadata['Sample Obtained']\\\n    .apply(lambda x: x.split(',') if pd.notnull(x) else x)\\\n    .apply(lambda x: [s.strip() for s in x] if isinstance(x, list) else x)\\\n    .apply(lambda x: ', '.join(list(set(x))) if isinstance(x, list) else x)\n    \n    extracted_metadata = extracted_metadata.merge(df_sample_size[['cord_uid','Sample Size']], on='cord_uid', how='outer')\n\n    #### Combining metadata with findings ####\n    output = df[pd.notnull(df['extracted'])][['cord_uid','sentence','extracted','Days', 'Range (Days)']]\\\n    .merge(output_metadata, on='cord_uid', how='outer')\\\n    .merge(extracted_metadata,on='cord_uid', how='outer')\n\n    #### Re-arranging and renameing ####\n    output=output[['publish_time',\n                   'title',\n                   'url', \n                   'journal',\n                   'Study Type',\n                   'Sample Size',\n                   'Age', \n                   'Sample Obtained', \n                   'Days', \n                   'Range (Days)',\n                   'sentence', \n                   'Added on']]\n    \n    output.columns = ['Date', \n                      'Study', \n                      'Study Link', \n                      'Journal',\n                      'Study Type',\n                      'Sample Size',\n                      'Age', \n                      'Sample Obtained', \n                      'Days',\n                      'Range (Days)',\n                      'Excerpt', \n                      'Added on']\n    \n    # Discarding additions that end up having no data of interest (i.e. false positives)\n    output = output[~((output['Added on']==datetime.today().strftime('%-m\/%d\/%Y')) & (pd.isnull(output['Excerpt'])))]\n\n    return output","54230efd":"table_names = ['Length of viral shedding after illness onset.csv',\n               'What is the incubation period of the virus_.csv',\n               'Incubation period across different age groups.csv']\n\nfor table_name in table_names:\n    summary_table = assemble_summary_table(table_name, metadata, emb)\n    display(summary_table)\n    summary_table.to_csv(table_name, index = False)","9cc2975e":"#### [CORD-NER Model](https:\/\/github.com\/CoronaWhy\/task-ties\/blob\/master\/task_ties\/train_ner.py)\nWe use this model to extract the types of specimen used in tests for COVID-19.\n\nThe model was trained on data from the [CORD-NER dataset](https:\/\/xuanwang91.github.io\/2020-03-20-cord19-ner\/). This dataset included text from the CORD-19 corpus annotated for general entity types, biomedical entity types, and 9 new entity types related specifically to COVID-19. The annotations were generated using a combination of methods pre-trained NER models, knowledge base-guided NER models, and seed-guided NER models. Since the dataset has not been updated with the latest CORD-19 corpus, we used the annotations to retrain our own NER model. We only included the 9 new COVID-19 entity types and the \u201cActivity\u201d entity types to train our model. We also replaced all underscores in the data with spaces in order to better reflect the format of the CORD-19 corpus texts. Using this data, we trained a blank SpaCy model with 20 iterations and a dropout rate of 0.35.","51103f9e":"<a id=\"4\"><\/a>\n## [Contributors](https:\/\/docs.google.com\/document\/d\/1oP4Qf3OMrbG28ESC74BzIPUaMtpGeyYYvdJ-cnF_wI8\/edit?usp=sharing)\nMany Thanks to all the contributors who have made this project possible.","eab8839c":"## Table of Contents\n1. [Task Overview](#1)\n2. [Results](#2)\n3. [Code](#3)\n4. [Contributors](#4)","47f2ff44":"#### [Study Design Classifier](https:\/\/github.com\/CoronaWhy\/Classy)\n\nThe model was trained on a dataset of approximately 1500 papers annotated by CoronaWhy volunteers, consisting of these classes: \n* Computational: in sillico, modeling or simulation studies\n* Experimental - in vitro: experimental studies performed on cells or othe microorganisms \n* Experimental - in vivo: experimental studies performed on animals\n* Clinical-interventional: interventional studies including randomized or non-randomized controlled trials \n* Clinical-observational: observational studies including cohort studies, case-control studies, case series, etc\n* Systematic review and\/or meta-analysis \n* Review: reviews that do not use systematic review methodology\n\nThe model was traindd using catboost that runs gradient boosting algorithms on decision trees. Hyperparameter tuning was performed using grid search on the base gradient boosting parameters (e.g. learning rate, depth) with 5-fold cross validation to achieve the best F1 macro, per class F1 and accuracy.","06b4dff1":"<a id=\"1\"><\/a>\n## Task Overview\n\nThis notebook responds to Task \"Create summary tables that address patient descriptions related to COVID-19\" and particularly to the three questions pertaining to incubation periods and lengths of viral shedding. \n\nThe notebook kicks off with a search framework that utilizes the seed articles in the target tables to find articles that could have been missed or are published after the initial review. The mechanism is simple. It indexes all articles in the CORD-19 collection with its embedding (i.e. mathematical representation of an article) and conducts a k-nearest neighbor search to discover similar articles. For each seed article, we identify a set of candidates, but only those that surface at least twice across all searches are considered as potential additions to the summary table. In addition, we use the Facebook AI Similarity Search (FAISS) library, which is optimized to perform similarity search in dense vector spaces, to ensure the search remains efficient as the literature body expands.\n\nOnce we gather our list of candidate articles, we apply a suite of AI-powered utility tools trained with supervised learning methods to construct the summary tables. Functionality wise, we have focused on two types of tools: one extracts information that is fundamental to all literature reviews such as study design and sample size, and the other extracts a range of information with shared characteristics. For instance, age, incubation period, and duration of viral shedding are essentially different kinds of time periods and thus can be extracted using the same tool with minimum filtering. While like most supervised approaches, collecting the training datasets can be expensive and labor-intensive, the trained models have great generalizability and can be easily modified and reused in other contexts.\n\n","1fba9acb":"### Voila! Summary Tables!","58aac8c2":"#### [Sample Size NER Model](https:\/\/github.com\/CoronaWhy\/task-ties\/blob\/master\/task_ties\/Sample_Size_Extraction_%26_Modeling.ipynb)\n\nA sample of sentence-level data was obtained via CoronaWhy's [ElasticSearch infrastructure](https:\/\/github.com\/CoronaWhy\/covid-19-infrastructure) and then fed into [Doccano](https:\/\/github.com\/CoronaWhy\/doccano) (an open-source text annotation tool) for manual annotation. We also secured paragraph-level data (to allow annotators an improved sense of linguistic context), though the data turned out to be far less useful than the sentence-level annotations. Annotations were then fed into SpaCy for the development of a custom NER model from a blank slate. The model training utilized a compounding mini-batch size (from 2 to 16) and a decaying dropout rate (from 0.6 to 0.2).\n\nWe planned on using this model to extract sample size data, but due to time constraint, we have not gathered sufficient training data. Therefore, the data extraction is temporarily based on rule-base matching. We first removed dates from the text as we have found that dates often contaminate our extracted output. We also converted all numbers written in words back to numerical format. Lastly, we used a set of keywords that are commonly used in describing sample size in the abstracts (the reason that we used abstracts was that they tend to be more concise and thus have less noise around the target information) to narrow down the extraction scope and extracted the target data. ","b2a61270":"### Creating faiss search index\nWe index all the articles with their embeddings in a dense vector space. By normalizing the vectors and calculating the inner products between vectors, we then perform a K-nearest neighbor search based on cosine distance.","17cd5e87":"![logo](https:\/\/drive.google.com\/uc?id=1VrvlBTHH4D7xsrNp74wtLBamMZygG8Sy)","778e92ac":"### Data extraction\nThe target data fields to extract in this notebook include: \n* Sample Size: the number of patients\/cases or articles included in the analysis\n* Sample Obtained: the types of specimen tested for the presence of the virus\n* Age: age of the included population\n* Days\/Range (Days): incubation period or duration of viral shedding, usually median\/mean (range or variation) is reported\n* Study Type: study design employed by the study\n\nWe use a set of custom Named Entity Recognition (NER) models and a classification model to extract these fields.\n","d69db2dd":"### Conducting similarity search \nOnce we've got the target table and the seed articles, we're ready to conduct the similarity search. First, we retrieve and normalize the seed articles' embedding vectors. Then, we search each article's 10 nearest neighbors from the entire corpus (the whole CORD-19 collection). Lastly, we look at all the search results collectively, and count the frequency of each result. We judge that the more frequently an article surfaces in a search, the more likely the article belongs to this topic and can contain relevant information. We use a recurrent frequency of two as the threshold for the new additions.","8d7e35bf":"## Preamble\n\nThis is a notebook created by a collaborative effort of CoronaWhy.org, multi-disciplinary global effort of volunteers. \n\n\n- Visit our [website](https:\/\/www.coronawhy.org) to learn more.\n- Read our [story](https:\/\/medium.com\/@arturkiulian\/im-an-ai-researcher-and-here-s-how-i-fight-corona-1e0aa8f3e714).\n- Visit our [main notebook](https:\/\/www.kaggle.com\/arturkiulian\/coronawhy-org-global-collaboration-join-slack) for historical context on how this community started.","8a365fc7":"### Assembling summary tables\n\nFinally, with all the components set up, we are ready to assemble the summary tables!","f3e3e2e6":"### Set up\n\nWe will use cord_19_embeddings to perform similarity search and then retrieve articles' metadata from metadata.csv.","a18352e1":"<a id=\"3\"><\/a>\n## Code\n\nBelow we present the annotated code to reproduce the summary tables.","511be230":"<a id=\"2\"><\/a>\n## Results\nWe present the summary tables created by this notebook in PowerBI reports. \n\n[Link to view the reports in full screen](https:\/\/app.powerbi.com\/view?r=eyJrIjoiMzU2YTk5ZjMtODU5My00ZjgyLWFmMWEtZDE4NzRjNzJhZTg1IiwidCI6ImRjMWYwNGY1LWMxZTUtNDQyOS1hODEyLTU3OTNiZTQ1YmY5ZCIsImMiOjEwfQ%3D%3D)\n","5cb6772d":"#### [Time Period NER Model](http:\/\/https:\/\/github.com\/CoronaWhy\/task-ties\/blob\/master\/task_ties\/Time_Period_NER.ipynb)\nWe use this model to extract age, incubation period, and length of viral shedding by different context filtering.\n\nThe model was trained on manually annotated data using [spaCy](https:\/\/spacy.io\/usage\/training#ner). We used mini-batch (i.e. the number of training examples passed to the model at a time) with compounding batch size, where the initial batch size is set at 4 and increases to 32 when updating model parameters. We use Adam solver as the optimization algorithm with a learning rate of 0.001, and a dropout rate of 0.2. We experimented with a variety of epochs (i.e. number of times the complete training data is passed through the model) ranging from 68 to 100 and used an epoch of 75 as it produces the best F-1 score.\n\n","e6145578":"### Listing target tables from task \nLet's take a look at the target tables for the task. As mentioned, in this notebook, we focus on three questions:\n* Length of viral shedding after illness onset\n* Incubation period of the virus\n* Incubation period across different age groups"}}