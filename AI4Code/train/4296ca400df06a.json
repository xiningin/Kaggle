{"cell_type":{"3c7d587e":"code","d3d3fb6f":"code","4cb12520":"code","efcf69f4":"code","f566461e":"code","b2fdc99e":"code","21f1724d":"code","022e8914":"code","dd9d5687":"code","2c094e9d":"code","4066ee72":"code","6510ae87":"code","8b666494":"code","92076309":"code","77f3bbb5":"markdown"},"source":{"3c7d587e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport torch\nimport random\nimport pickle\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom torch.utils.data import DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F","d3d3fb6f":"!unzip '\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip'\n!unzip '\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip'","4cb12520":"train = pd.read_csv('train.tsv', sep='\\t')\ntest = pd.read_csv('test.tsv', sep='\\t')","efcf69f4":"def Corpus_Extr(df):\n    print('Construct Corpus...')\n    corpus = []\n    for i in tqdm(range(len(df))):\n        corpus.append(df.Phrase[i].lower().split())\n    corpus = Counter(np.hstack(corpus))\n    corpus = corpus\n    corpus2 = sorted(corpus,key=corpus.get,reverse=True)\n    print('Convert Corpus to Integers')\n    vocab_to_int = {word: idx for idx,word in enumerate(corpus2,1)}\n    print('Convert Phrase to Integers')\n    phrase_to_int = []\n    for i in tqdm(range(len(df))):\n        phrase_to_int.append([vocab_to_int[word] for word in df.Phrase.values[i].lower().split()])\n    return corpus,vocab_to_int,phrase_to_int\ncorpus,vocab_to_int,phrase_to_int = Corpus_Extr(train)","f566461e":"def Pad_sequences(phrase_to_int,seq_length):\n    pad_sequences = np.zeros((len(phrase_to_int), seq_length),dtype=int)\n    for idx,row in tqdm(enumerate(phrase_to_int),total=len(phrase_to_int)):\n        pad_sequences[idx, :len(row)] = np.array(row)[:seq_length]\n    return pad_sequences","b2fdc99e":"pad_sequences = Pad_sequences(phrase_to_int,30)","21f1724d":"train.sample(50)","022e8914":"class PhraseDataset(Dataset):\n    def __init__(self,df,pad_sequences):\n        super().__init__()\n        self.df = df\n        self.pad_sequences = pad_sequences\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        if 'Sentiment' in self.df.columns:\n            label = self.df['Sentiment'].values[idx]\n            item = self.pad_sequences[idx]\n            return item,label\n        else:\n            item = self.pad_sequences[idx]\n            return item","dd9d5687":"class SentimentRNN(nn.Module):\n    \n    def __init__(self,corpus_size,output_size,embedd_dim,hidden_dim,n_layers):\n        super().__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding = nn.Embedding(corpus_size,embedd_dim)\n        self.lstm = nn.LSTM(embedd_dim, hidden_dim,n_layers,dropout=0.5, batch_first=True)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim,output_size)\n        self.act = nn.Sigmoid()\n        \n    def forward(self,x,hidden):\n        batch_size = x.size(0)\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds,hidden)\n        lstm_out = lstm_out.contiguous().view(-1,self.hidden_dim)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        out = self.act(out)\n        out = out.view(batch_size,-1)\n        out = out[:,-5:]\n        return out, hidden\n    def init_hidden(self,batch_size):\n        \n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        return hidden\n        ","2c094e9d":"vocab_size = len(vocab_to_int)\noutput_size = 5\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2\nnet = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim,n_layers)","4066ee72":"net.train()\nclip=5\nepochs = 200\ncounter = 0\nprint_every = 100\nlr=0.01\n\ndef criterion(input, target, size_average=True):\n    \"\"\"Categorical cross-entropy with logits input and one-hot target\"\"\"\n    l = -(target * torch.log(F.softmax(input, dim=1) + 1e-10)).sum(1)\n    if size_average:\n        l = l.mean()\n    else:\n        l = l.sum()\n    return l\n#criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)","6510ae87":"import gc","8b666494":"batch_size=32\nlosses = []\naccs=[]\nfor e in range(epochs):\n    a = np.random.choice(len(train)-1, 1000)\n    train_set = PhraseDataset(train.loc[train.index.isin(np.sort(a))],pad_sequences[a])\n    train_loader = DataLoader(train_set,batch_size=32,shuffle=True)\n    # initialize hidden state\n    h = net.init_hidden(32)\n    running_loss = 0.0\n    running_acc = 0.0\n    # batch loop\n    for idx,(inputs, labels) in enumerate(train_loader):\n        counter += 1\n        gc.collect()\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        optimizer.zero_grad()\n        if inputs.shape[0] != batch_size:\n            break\n        # get the output from the model\n        output, h = net(inputs, h)\n        labels=torch.nn.functional.one_hot(labels, num_classes=5)\n        # calculate the loss and perform backprop\n        loss = criterion(output, labels)\n        loss.backward()\n        running_loss += loss.cpu().detach().numpy()\n        running_acc += (output.argmax(dim=1) == labels.argmax(dim=1)).float().mean()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n        if idx%20 == 0:\n            print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format((running_loss\/(idx+1))))\n            losses.append(float(running_loss\/(idx+1)))\n            print(f'acc:{running_acc\/(idx+1)}')\n            accs.append(running_acc\/(idx+1))","92076309":"import matplotlib.pyplot as plt\nplt.plot(losses)\nplt.show()\nplt.plot(accs)\nplt.show()","77f3bbb5":"## Frequently Disconnected. just commit without complete."}}