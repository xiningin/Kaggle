{"cell_type":{"2b129391":"code","d59afbe2":"code","8625238a":"code","ef61da32":"code","3bf32550":"code","7ac35ae3":"code","b3320c28":"code","1895d1eb":"code","fb96aa18":"code","75c015c1":"code","7808ace9":"code","59670c91":"code","4f7e02a1":"code","781a331d":"code","cad6431a":"code","5f9df36d":"code","3b4e2839":"code","d0407f1c":"code","a7a12eb1":"code","9d715a18":"code","0c52a26f":"code","3a206d7c":"code","e08369e1":"code","01a76a69":"code","58fd08fb":"code","75667dcd":"code","ee8f606a":"code","923cb6ba":"code","aab25cb8":"code","f5843208":"code","2780ddfc":"markdown","ee3f99a3":"markdown","3953a467":"markdown","1eb747d5":"markdown","164bac73":"markdown","7e144bf8":"markdown","e7f6d37e":"markdown","7be9b7ba":"markdown","ff38b66d":"markdown","3ca7af8b":"markdown","81358302":"markdown","2ec50bda":"markdown","d135f020":"markdown","6ab5c924":"markdown","88880fd2":"markdown"},"source":{"2b129391":"import pandas as pd,numpy as np,matplotlib.pyplot as plt,seaborn as sns\nimport os\nimport warnings\nimport scipy\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_columns\",200)\nimport datetime\nimport gc\nimport xgboost as xgb","d59afbe2":"# Reading training data\ntrain = pd.read_csv(\"\/kaggle\/input\/train.csv\")\n# Making sure there is no duplicate card id\ntrain.shape[0] == train[\"card_id\"].nunique()","8625238a":"# Checking for Null values\ntrain.isnull().sum()","ef61da32":"# break down first active month into year and month\ntrain[\"active_year\"],train[\"active_month\"] = list(zip(*train[\"first_active_month\"].apply(lambda x:x.split(\"-\"))))\n# unique values of features\ntrain.nunique()","3bf32550":"# histogram of target\nplt.figure(figsize=(14,14))\nsns.distplot(train[\"target\"])","7ac35ae3":"# box plot of target based on features present\nfeature = [\"active_year\",\"active_month\",\"feature_1\",\"feature_2\",\"feature_3\"]\nf,ax=plt.subplots(1,5)\nf.set_figheight(8)\nf.set_figwidth(20)\nfor k,i in enumerate(feature):\n    sns.boxplot(x= i,y=\"target\",data = train,ax=ax[k])","b3320c28":"# correlation heat map of features\ntrain[[\"active_year\",\"active_month\"]] = train[[\"active_year\",\"active_month\"]].astype(int)\ncorr = train.drop([\"card_id\",\"first_active_month\"],axis=1).corr()\nplt.figure(figsize =(10,6))\nsns.heatmap(corr,annot=True)","1895d1eb":"# Reading test dataset\ntest = pd.read_csv(\"\/kaggle\/input\/test.csv\")\ntest.head()","fb96aa18":"# checking for null values\ntest.isnull().sum()","75c015c1":"# replacing one null with mode of first active month of train set\nimpute = train[\"first_active_month\"].value_counts()[:1].index[0]\ntest = test.fillna(impute)\n\ntest[\"active_year\"],test[\"active_month\"] = list(zip(*test[\"first_active_month\"].apply(lambda x:x.split(\"-\"))))\ntest[[\"active_year\",\"active_month\"]] = test[[\"active_year\",\"active_month\"]].astype(int)\n","7808ace9":"# created elapsed features using the following kernel\n#https:\/\/www.kaggle.com\/tunguz\/eloda-with-feature-engineering-and-stacking\ntrain[\"first_active_month\"] = pd.to_datetime(train[\"first_active_month\"])\ntrain['elapsed_time'] = (datetime.date(2018, 2, 1) - train['first_active_month'].dt.date).dt.days\ntest[\"first_active_month\"] = pd.to_datetime(test[\"first_active_month\"])\ntest['elapsed_time'] = (datetime.date(2018, 2, 1) - test['first_active_month'].dt.date).dt.days\n","59670c91":"# Reading historical transaction\nhistorical_trans =pd.read_csv(\"\/kaggle\/input\/historical_transactions.csv\")\nhistorical_trans.head()","4f7e02a1":"historical_trans.isnull().sum()","781a331d":"print(historical_trans[\"installments\"].unique())\n# assuming -1 as missing data and 999 as wrong entry, replacing both with 0, i.e mode of remaining data\nhistorical_trans[\"installments\"] = historical_trans[\"installments\"].replace([-1,999],[0,0],inplace=False)\n\n# Created binary feature from installment \nhistorical_trans[\"has_installments\"] = historical_trans[\"installments\"].apply(lambda x: \"No\" if x==0 else \"Yes\")","cad6431a":"new_merchant_transactions = pd.read_csv(\"\/kaggle\/input\/new_merchant_transactions.csv\")\nnew_merchant_transactions.head()","5f9df36d":"# assuming -1 as missing data and 999 as wrong entry, replacing both with 0, i.e mode of remaining data\nnew_merchant_transactions[\"installments\"] = new_merchant_transactions[\"installments\"].replace([-1,999],[0,0],inplace=False)\nnew_merchant_transactions[\"has_installments\"] = new_merchant_transactions[\"installments\"].apply(lambda x: \"No\" if x==0 else \"Yes\")","3b4e2839":"# Imputing missing values \ndef impute(df):\n    df[\"merchant_id\"] = df[\"merchant_id\"].fillna(\"Missing_id\")\n    features = df.columns[df.isna().any()].tolist()\n    for i in features:\n        if df[i].dtype ==\"object\" or i == \"category_2\":\n            mode = df[i].value_counts()[:1].index[0]\n            df[i].fillna(mode,inplace=True)\n        else:\n            df[i].fillna(df[i].mean(),inplace=True)\n    return df\n\nnew_merchant_transactions = impute(new_merchant_transactions)\nhistorical_trans = impute(historical_trans)","d0407f1c":"# mapping of binomial feature to (0,1)\nfor i in [\"authorized_flag\",\"category_1\"]:\n    new_merchant_transactions[i] = new_merchant_transactions[i].map({\"Y\":1,\"N\":0})\n    historical_trans[i] = historical_trans[i].map({\"Y\":1,\"N\":0})\n\nhistorical_trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(historical_trans['purchase_date']).astype(np.int64) * 1e-9\nnew_merchant_transactions.loc[:, 'purchase_date'] = pd.DatetimeIndex(new_merchant_transactions['purchase_date']).astype(np.int64) * 1e-9\n\n# aggregation for numerical features\naggs = {'month_lag':[\"min\",\"max\",\"median\",'sum'],\n       'purchase_amount':['mean','var'],\n       'category_2':['min','max','median'],\n    'installments':['min','max','median','sum'],\n        \"authorized_flag\":['sum','mean'],\n        \"category_1\":['sum','mean'],\n        'merchant_id':[\"nunique\"],'merchant_category_id':[\"nunique\"]\n        }\n\nnew_columns_hist = [k + '_hist_' + agg for k in aggs.keys() for agg in aggs[k]]\nnew_columns_new = [k + '_new_' + agg for k in aggs.keys() for agg in aggs[k]]\n\n# new transaction data\naggs_merge2 = new_merchant_transactions.groupby(\"card_id\").aggregate(aggs)\naggs_merge2 = aggs_merge2.fillna(0) # card id with one entry will have nan variance.\naggs_merge2.columns = new_columns_new\naggs_merge2[\"new_purchase_date\"] = new_merchant_transactions.groupby(\"card_id\").aggregate({'purchase_date':[np.ptp]}).values\ndel new_merchant_transactions\ngc.collect()\n\n# past data\naggs_merge_1 = historical_trans.groupby(\"card_id\").aggregate(aggs)\naggs_merge_1 = aggs_merge_1.fillna(0) # card id with one entry will have nan variance.\naggs_merge_1.columns = new_columns_hist\naggs_merge_1[\"history_purchase_date\"] = historical_trans.groupby(\"card_id\").aggregate({'purchase_date':[np.ptp]}).values\ndel historical_trans\ngc.collect()","a7a12eb1":"final_train = train.merge(aggs_merge_1,on=\"card_id\",how=\"left\")\nfinal_train = final_train.merge(aggs_merge2,on=\"card_id\",how=\"left\")\n\nfinal_test = test.merge(aggs_merge_1,on=\"card_id\",how=\"left\")\nfinal_test = final_test.merge(aggs_merge2,on=\"card_id\",how=\"left\")\ndel train,test\ngc.collect()\n# # imputation in merge dataset\nimputed_columns = final_train.columns[final_train.isna().any()].tolist()\nfor i in imputed_columns:\n    final_train[i].fillna(final_train[i].mean(),inplace=True)\n    final_test[i].fillna(final_train[i].mean(),inplace=True)","9d715a18":"# finding features with only one unique value and dropping them\none_value_features =[]\nfor i in final_train.columns:\n    if final_train[i].nunique() ==1:\n        one_value_features.append(i)\n        \nfinal_train = final_train.drop(one_value_features,axis=1)\nfinal_test = final_test.drop(one_value_features,axis=1)","0c52a26f":"# Correlation Analysis of features\n# top 10 features correlated with target\ncorrelation = final_train.drop([\"first_active_month\",\"card_id\"],axis=1).corr()\ncorrelation[\"target\"].abs().sort_values(ascending=False)[:10]\n","3a206d7c":"# calculating correlation of all features and defining them \"important\" based on their P-value\npairwise_corr=[]\nvalue=[]\ncontinuous_columns=[]\nfor i in final_train.drop([\"target\",\"card_id\",\"first_active_month\"],axis=1).columns:\n    c, p = scipy.stats.pearsonr(final_train[i],final_train[\"target\"])\n    continuous_columns.append(i)\n    value.append(p)\n    pairwise_corr.append(c)\ndf=pd.DataFrame({\"column\":continuous_columns,\"corr_value\":pairwise_corr,\"p-value\":value})\ndf[\"importance\"]=df[\"p-value\"].apply(lambda x:\"important\" if x<0.05 else \"not important\")\nplt.figure(figsize=(20,10))\nplt.title(\"p value of predictor with target variable\")\nplt.xticks(rotation=\"vertical\")\nsns.stripplot(x=\"column\",y=\"p-value\",hue=\"importance\",data=df,size=10)","e08369e1":"y_train = final_train[\"target\"]\nfinal_train = final_train.sample(frac=1, random_state = 7)\nx_train = final_train.drop([\"target\",\"card_id\",\"first_active_month\"],axis=1)\nx_test = final_test.drop([\"card_id\",\"first_active_month\"],axis=1)\nfrom sklearn.model_selection import train_test_split as tts\nTrn_x,val_x,Trn_y,val_y = tts(x_train,y_train,test_size =0.1,random_state = 7)\ntrn_x , test_x, trn_y, test_y = tts(Trn_x , Trn_y, test_size =0.1, random_state = 7)","01a76a69":"# converting into xgb DMatrix\nTrain = xgb.DMatrix(trn_x,label = trn_y)\nValidation = xgb.DMatrix(val_x, label = val_y)\nTest = xgb.DMatrix(test_x)\n","58fd08fb":"params = {\"booster\":\"gbtree\",\"eta\":0.1,'min_split_loss':0,'max_depth':6,\n         'min_child_weight':1, 'max_delta_step':0,'subsample':1,'colsample_bytree':1,\n         'colsample_bylevel':1,'reg_lambda':1,'reg_alpha':0,\n         'grow_policy':'depthwise','max_leaves':0,'objective':'reg:linear','eval_metric':'rmse',\n         'seed':7}\nhistory ={}  # This will record rmse score of training and test set\neval_list =[(Train,\"Training\"),(Validation,\"Validation\")]\n","75667dcd":"clf = xgb.train(params, Train, num_boost_round=119, evals=eval_list, obj=None, feval=None, maximize=False, \n          early_stopping_rounds=40, evals_result=history)","ee8f606a":"# dataframe of progress\nf,ax=plt.subplots(1,1)\nf.set_figheight(10)\nf.set_figwidth(20)\ndf_performance=pd.DataFrame({\"train\":history[\"Training\"][\"rmse\"],\"test\":history[\"Validation\"][\"rmse\"]}).reset_index(drop=False)\nsns.pointplot(ax=ax,y=\"train\",x=\"index\",data=df_performance,color=\"r\")\nsns.pointplot(ax=ax,y=\"test\",x=\"index\",data=df_performance,color=\"g\")\nax.legend(handles=ax.lines[::len(df_performance)+1], labels=[\"Train\",\"Test\"])\nplt.xlabel('iterations'); plt.ylabel('logloss value'); plt.title('learning curve')","923cb6ba":"score=clf.get_score(importance_type=\"gain\")\ndf=pd.DataFrame({\"feature\":list(score.keys()),\"score\":list(score.values())})\ndf=df.sort_values(by=\"score\",ascending=False)\nplt.figure(figsize=(20,20))\nplt.xticks(rotation=\"vertical\")\nsns.barplot(x=\"feature\",y=\"score\",data=df,orient=\"v\")\n","aab25cb8":"# Checking rmse on test set (kept during data splitting)\nfrom sklearn.metrics import mean_squared_error as mse\npred_test = clf.predict(Test)\nscore = mse(test_y , pred_test)\nprint(np.sqrt(score))","f5843208":"prediction = clf.predict(xgb.DMatrix(x_test))\ndf_sub=pd.DataFrame()\ndf_sub[\"card_id\"] = final_test[\"card_id\"].values\ndf_sub[\"target\"] = np.ravel(prediction)\ndf_sub[[\"card_id\",\"target\"]].to_csv(\"new_submission.csv\",index=False)","2780ddfc":"# Aggregation","ee3f99a3":"# Splitting Data into train\/validation\/test set","3953a467":"** No significant correlation between target and predictors  **","1eb747d5":"** one value of first active month is missing. replace that with mode of training data first active month value **","164bac73":"# Making prediction on actual test data","7e144bf8":"# Historical","e7f6d37e":"# Training the model","7be9b7ba":"# final merge","ff38b66d":"# Test data","3ca7af8b":"# Train data","81358302":"# New Merchant data","2ec50bda":"# Plotting importance score of features from xgboost model (ordered by their significance)","d135f020":"# Dropping features having only one unique value","6ab5c924":"# Evaluation of History of model","88880fd2":"**Few values of y are less than -30, apart from that distribution is normal and centered around 0**"}}