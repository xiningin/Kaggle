{"cell_type":{"ec3c5d5b":"code","2b20dc34":"code","d8263f5b":"code","e516df9f":"code","b103dca3":"code","cbb44655":"code","17220736":"code","0f74adde":"code","777ed9a8":"code","53a4c2e3":"code","eb581402":"code","e92f9b01":"code","ae6585ee":"code","cb77fe4d":"code","1a8e589f":"code","3c653cd1":"code","76a23a43":"code","45a36c46":"code","f04065b1":"code","65f5516e":"code","de19db34":"code","7f6a14c3":"code","7833f979":"code","1ab97165":"code","b218a20e":"code","32bd5cf8":"code","c764bda1":"code","c041d072":"code","93ab8c98":"code","eec975ce":"code","f6cb51e5":"code","ec7d6a77":"code","718cce37":"code","502e6cbc":"code","37fe253d":"code","c5e4bc4a":"code","401148d7":"code","676ba81f":"code","939849d7":"code","c1702b66":"code","7ebe3d81":"code","387e0f36":"code","83832058":"code","10f02950":"code","e36ac7ed":"code","95e7b0a9":"code","4d2701ff":"code","91a9ee4e":"code","b613971b":"code","a2f454bd":"code","6d5316de":"code","3a82c809":"code","c8d8461a":"code","59dea069":"code","2e7f7682":"code","521c1cbc":"code","4ad85613":"code","11510c72":"code","5fb7a389":"code","ead8ef6e":"code","ae29277a":"code","6ba086f6":"code","32aeacf5":"code","138f976f":"code","aefc9077":"code","a680ee85":"code","62e85c96":"code","3062c83b":"code","aa6d007c":"code","d208d039":"code","e23b0106":"code","6e87bc18":"code","c0245035":"code","4207a6e1":"code","cdfc4d47":"code","125e80b3":"code","44cb79d4":"code","175fde22":"code","15fde820":"code","bfcb8633":"code","67d2d700":"code","00edccac":"code","a98739fa":"code","6767b903":"code","5b95eb61":"code","3a6498b4":"code","89463c3a":"code","887df0d7":"code","761b2571":"code","b6273399":"code","583f0aa0":"code","66b92b79":"code","2faaaae2":"code","adf84e8b":"code","944b21d2":"code","7e2ab89b":"code","044368f4":"code","e179fe0a":"code","b38fff14":"code","52abb8af":"code","9efe9762":"code","5dc4371f":"code","33241312":"code","f7719994":"code","84aaaeb3":"code","fd499646":"code","72b5baa3":"code","05f93177":"code","67306006":"code","e683dc4b":"code","ff390333":"code","c4296ea6":"code","a3ec825e":"code","d957759a":"code","3bcc3ec1":"code","8c160c56":"code","69563085":"code","910db0a1":"code","e35372b4":"code","de90dc82":"code","72bd1d75":"code","9f8162cc":"code","af34ed59":"code","f6bb3cb8":"code","ec2c3031":"code","2741e8cc":"code","62fc5e9b":"code","f986a4f9":"code","249e9375":"code","106a849f":"code","248d06a3":"code","827b134b":"code","a7931ad6":"code","6c059559":"code","36377940":"code","862a581a":"code","c91fb428":"code","6e8f860c":"code","4337f383":"markdown","f04ac02a":"markdown","c4d2b364":"markdown","53ae134f":"markdown","80c752ec":"markdown","6d87edd1":"markdown","f5fa3e69":"markdown","839949b6":"markdown","686500a3":"markdown","076006f4":"markdown","e8c89454":"markdown","91b3b9d8":"markdown","dbe1c78f":"markdown","241e9868":"markdown","a63e8cfb":"markdown","8abf5928":"markdown","146dd936":"markdown","c2708f60":"markdown","8268d921":"markdown","7fea6dca":"markdown","e796ab62":"markdown","b8b0c583":"markdown","59685589":"markdown","676530bd":"markdown","36c59d61":"markdown","4fc3b171":"markdown","50f31467":"markdown","c01830cd":"markdown","79eec7f5":"markdown","5a251401":"markdown","2121749d":"markdown","f3642ec6":"markdown","fba50d51":"markdown","53ee7f09":"markdown","c8502f98":"markdown","f01ed877":"markdown","666b3c8d":"markdown","b89d9931":"markdown","c18b4514":"markdown","579262be":"markdown","5a7ffba5":"markdown","1b419133":"markdown","9a9566ad":"markdown","6d006373":"markdown","3aee0836":"markdown","4d867fc3":"markdown","e6fa1f5f":"markdown","dfe4590d":"markdown","b1d6596f":"markdown","0642582e":"markdown","0004cd50":"markdown","ed58a439":"markdown","782745b7":"markdown","e56a4721":"markdown"},"source":{"ec3c5d5b":"# Downgraded to the previous version due to the critical bug in 0.16\n!pip install catboost==0.15\n! pip install -U scikit-learn\n!pip install feature_engine\n","2b20dc34":"import pandas as pd\nimport numpy as np\n\nimport re\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport feature_engine.missing_data_imputers as mdi\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve\nfrom catboost import Pool, CatBoostClassifier\n\nfrom scipy.stats import pearsonr, chi2_contingency\nfrom itertools import combinations\nfrom sklearn.impute import MissingIndicator\nfrom statsmodels.stats.proportion import proportion_confint","d8263f5b":"from sklearn.impute import MissingIndicator","e516df9f":"data = pd.read_csv(\n    '..\/input\/lending-club\/accepted_2007_to_2018q4.csv\/accepted_2007_to_2018Q4.csv',\n    parse_dates=['issue_d'], infer_datetime_format=True)\ndata = data[(data.issue_d >= '2018-01-01 00:00:00') & (data.issue_d < '2019-01-01 00:00:00')]\ndata = data.reset_index(drop=True)\ndata.head()","b103dca3":"browse_notes = pd.read_excel('https:\/\/raw.githubusercontent.com\/wendykan\/LendingClubPublicData\/master\/data\/LCDataDictionary.xlsx',\n                             sheet_name=1)\n# browse_notes.head()","cbb44655":"browse_feat = browse_notes['BrowseNotesFile'].dropna().values\nbrowse_feat = [re.sub('(?<![0-9_])(?=[A-Z0-9])', '_', x).lower().strip() for x in browse_feat]","17220736":"browse_feat","0f74adde":"data_feat = data.columns.values\nnp.setdiff1d(browse_feat, data_feat)","777ed9a8":"np.setdiff1d(data_feat, browse_feat)","53a4c2e3":"\nwrong = ['is_inc_v', 'mths_since_most_recent_inq', 'mths_since_oldest_il_open',\n         'mths_since_recent_loan_delinq', 'verified_status_joint']\ncorrect = ['verification_status', 'mths_since_recent_inq','mo_sin_old_il_acct',\n           'mths_since_recent_bc_dlq', 'verification_status_joint']\n\nbrowse_feat = np.setdiff1d(browse_feat, wrong)\nbrowse_feat = np.append(browse_feat, correct)","eb581402":"# data['sec_app_chargeoff_within_12_mths'].unique()[2]\nbrowse_feat","e92f9b01":"# data.sec_app_mths_since_last_major_derog\n# data_feat","ae6585ee":"avail_feat = np.intersect1d(browse_feat, data_feat)\navail_feat=np.append(avail_feat,'sec_app_earliest_cr_line')\nX = data[avail_feat].copy()\nX.info()","cb77fe4d":"X2=X.copy()","1a8e589f":"X.shape","3c653cd1":"X=pd.concat([X,data['loan_status']],axis=1)","76a23a43":"X.head()","45a36c46":"import pandas as pd","f04065b1":"data['loan_status'].shape","65f5516e":"temp_df=pd.Series(X['emp_title'].value_counts()\/len(X))\ntemp_df","de19db34":"# 495242*0.000002*100\n# len(X)\nX.drop('emp_title',inplace=True,axis=1)","7f6a14c3":"X.head()","7833f979":"# X['emp_title']\navail_feat","1ab97165":"X.select_dtypes('object').head()","b218a20e":"X['earliest_cr_line'] = pd.to_datetime(X['earliest_cr_line'], infer_datetime_format=True)\nX['sec_app_earliest_cr_line'] = pd.to_datetime(X['sec_app_earliest_cr_line'], infer_datetime_format=True)","32bd5cf8":"X['emp_length'] = X['emp_length'].replace({'< 1 year': '0 years', '10+ years': '11 years'})\nX['emp_length'] = X['emp_length'].str.extract('(\\d+)').astype('float')\nX['id'] = X['id'].astype('float')","c764bda1":"X['earliest_cr_line'].dtype","c041d072":"X['emp_length'].head()","93ab8c98":"nan_mean = X.isna().mean()\nnan_mean = nan_mean[nan_mean != 0].sort_values()\nnan_mean","eec975ce":"nan_mean = nan_mean[nan_mean != 0].sort_values().index\nnan_mean","f6cb51e5":"X.drop(['desc', 'member_id'], axis=1,inplace=True)","ec7d6a77":"# X['sec_app_mths_since_last_major_derog']\nX.verification_status_joint.unique()","718cce37":"fill_empty = ['verification_status_joint']\nfill_max_mean = ['bc_open_to_buy', 'mo_sin_old_il_acct', 'mths_since_last_delinq',\n            'mths_since_last_major_derog', 'mths_since_last_record',\n            'mths_since_rcnt_il', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n            'mths_since_recent_inq', 'mths_since_recent_revol_delinq',\n            'pct_tl_nvr_dlq']\nfill_min_mean =np.setdiff1d(X.columns.values,np.append(fill_empty, fill_max_mean))\n\nX[fill_empty] = X[fill_empty].fillna('')\n# X[fill_max] = X[fill_max].fillna(X[fill_max].max())\n# X[fill_min] = X[fill_min].fillna(X[fill_min].min())","502e6cbc":"X['acc_now_delinq'].unique()\nX['annual_inc_joint'].min()","37fe253d":"cols_null_fill_min_mean=[cols for cols in fill_min_mean if(X[cols].isnull().mean()>0)]\n\nmissing_min_mean=X[fill_min_mean].isnull().mean()\nvalues_miss=missing_min_mean[missing_min_mean!=0].sort_values()\nvalues_miss\n","c5e4bc4a":"cols_less_than_5=[cols for cols in cols_null_fill_min_mean if(X[cols].isnull().mean()<=0.05)]","401148d7":"cols_less_than_5","676ba81f":"# X[annual_inc_joint,dti_joint,sec_app_earliest_cr_line\nX.head()","939849d7":"from pandas.io.common import is_url\n","c1702b66":"X[['all_util','avg_cur_bal','dti','percent_bc_gt_75','revol_util','bc_util']].head()","7ebe3d81":"X.dropna(subset=cols_less_than_5,inplace=True)","387e0f36":" X[['all_util','avg_cur_bal','dti','percent_bc_gt_75','revol_util','bc_util','num_tl_120dpd_2m']] =X[['all_util','avg_cur_bal','dti','percent_bc_gt_75','revol_util','bc_util','num_tl_120dpd_2m']].fillna(X[['all_util','avg_cur_bal','dti','percent_bc_gt_75','revol_util','bc_util','num_tl_120dpd_2m']].mean())","83832058":"X.dropna(subset=['num_tl_120dpd_2m'],inplace=True)","10f02950":"X.dropna(subset=['emp_length'],inplace=True)","e36ac7ed":"X['emp_length'].head()","95e7b0a9":"import pylab\nimport scipy.stats as stats","4d2701ff":"# # import seaborn as sns\n# # sns.distplot(X['il_util'].dropna(),kde=True,bins=100)\n# # X['il_util'].plot(kind='kde')\n# # plt.figure(figsize=(100,200))\n# # plt.subplot(1,2,2)\n# # fig, ax = plt.subplots(figsize=(10,5))\n# # fig_dims = (30, 10)\n# # fig, ax = plt.subplots(figsize=fig_dims)\n# plt.figure(figsize=(19, 6))\n# plt.subplot(1,3,1)\n# sns.distplot(X['emp_length'].dropna(),hist=False,bins=100)\n\n# plt.subplot(1,3,2)\n# stats.probplot(X['emp_length'],fit=True,plot=pylab)\n\n# plt.subplot(1,3,3)\n# sns.boxplot(y=X['emp_length'])\n# plt.plot(X['emp_length'])\n# plt.show()","91a9ee4e":"# plt.figure(figsize=(19, 6))\n# plt.subplot(1,3,1)\n# sns.distplot(X['il_util'],hist=False,bins=100)\n\n# plt.subplot(1,3,2)\n# stats.probplot(X['il_util'],fit=True,plot=pylab)\n\n# plt.subplot(1,3,3)\n# sns.boxplot(y=X['il_util'])\n# plt.plot(X['il_util'])\n# plt.show()","b613971b":"X.shape","a2f454bd":"#Random sample imputation for the variable emp_length","6d5316de":"imputer = mdi.RandomSampleImputer(variables=['emp_length'])","3a82c809":"imputer.fit(X)\nX=imputer.transform(X)","c8d8461a":"X['emp_length'].isnull().sum()","59dea069":"X['il_util']=X['il_util'].fillna(X['il_util'].median())","2e7f7682":"# X['il_util'].head()\nX['il_util'].median()","521c1cbc":"addBinary_imputer1 = mdi.AddMissingIndicator(\n    variables=['annual_inc_joint','dti_joint','sec_app_earliest_cr_line'])","4ad85613":"addBinary_imputer1.fit(X)","11510c72":"X.shape","5fb7a389":"addBinary_imputer1.transform(X)","ead8ef6e":"X[['annual_inc_joint','dti_joint','emp_length','il_util']]","ae29277a":"X['annual_inc_joint'].head()","6ba086f6":" X[['annual_inc_joint','dti_joint']] = X[['annual_inc_joint','dti_joint']].fillna(X[['annual_inc_joint','dti_joint']].median())","32aeacf5":"yuyu['annual_inc_joint','dti']","138f976f":"# X['il_util'].unique()\n# X['annual_inc_joint_na'].head()\nX.columns","aefc9077":"X[['annual_inc_joint','dti_joint','emp_length','il_util','sec_app_earliest_cr_line']].isnull().sum()","a680ee85":"addBinary_imputer2=mdi.AddMissingIndicator(variables=['sec_app_earliest_cr_line'])","62e85c96":"addBinary_imputer2.fit(X)","3062c83b":"addBinary_imputer2.transform(X)","aa6d007c":"X.columns","d208d039":"X['sec_app_earliest_cr_line']=X['sec_app_earliest_cr_line'].fillna(X['sec_app_earliest_cr_line'].min())","e23b0106":"X['sec_app_earliest_cr_line'].dtype","6e87bc18":"X['num_tl_120dpd_2m'].unique()","c0245035":"X.drop('num_tl_120dpd_2m',axis=1,inplace=True)","4207a6e1":"X['avg_cur_bal'].min()","cdfc4d47":"X['mths_since_last_record'].max()","125e80b3":"fig=plt.figure()\nax=fig.add_subplot(111)\nX['avg_cur_bal'].plot(kind='kde',ax=ax,linewidth=2)","44cb79d4":"X['avg_cur_bal'].max()","175fde22":"X['emp_length'].plot(kind='kde',ax=ax,linewidth=2)","15fde820":"X['emp_length'].median()","bfcb8633":"X['emp_length'].mean()","67d2d700":"fill_min = ['avg_cur_bal','emp_length']\nX[fill_min] = X[fill_min].fillna(X[fill_min].min())","00edccac":"# X['annual_inc_joint','dti_joint','sec_app_earliest_cr_line']\n# X.columns\nfill_min_mean","a98739fa":"X['pct_tl_nvr_dlq'].dropna(inplace=True)","6767b903":"addBinary_imputer3 = mdi.AddMissingIndicator(\n    variables=[\n 'mths_since_last_delinq',\n 'mths_since_last_major_derog',\n 'mths_since_last_record',\n 'mths_since_recent_bc_dlq',\n 'mths_since_recent_inq',\n 'mths_since_recent_revol_delinq'])","5b95eb61":"X[fill_max_mean].isnull().mean()","3a6498b4":"X.dropna(subset=['pct_tl_nvr_dlq'],inplace=True)","89463c3a":"X.dropna(subset=['mo_sin_old_il_acct','mths_since_rcnt_il'],inplace=True)","887df0d7":"addBinary_imputer3.fit(X)\naddBinary_imputer3.transform(X)\nX[['mths_since_last_delinq',\n 'mths_since_last_major_derog',\n 'mths_since_last_record',\n 'mths_since_recent_bc_dlq',\n 'mths_since_recent_inq',\n 'mths_since_recent_revol_delinq']]=X[['mths_since_last_delinq','mths_since_last_major_derog','mths_since_last_record','mths_since_recent_bc_dlq','mths_since_recent_inq','mths_since_recent_revol_delinq']].fillna(X[['mths_since_last_delinq',\n 'mths_since_last_major_derog',\n 'mths_since_last_record',\n 'mths_since_recent_bc_dlq',\n 'mths_since_recent_inq',\n 'mths_since_recent_revol_delinq']].max())","761b2571":"num_feat = X.select_dtypes('number').columns.values\nX[num_feat].nunique().sort_values()\n","b6273399":"X = X.drop(['num_tl_120dpd_2m', 'id','acc_now_delinq'], axis=1, errors='ignore')\nX=X.drop(['num_tl_30dpd'],axis=1,errors='ignore')","583f0aa0":"from scipy import stats","66b92b79":"num_feat = X.select_dtypes('number').columns.values\ncomb_num_feat = np.array(list(combinations(num_feat, 2)))\ncorr_num_feat = np.array([])\nfor comb in comb_num_feat:\n    corr = stats.spearmanr(X[comb[0]], X[comb[1]])[0]\n    corr_num_feat = np.append(corr_num_feat, corr)","2faaaae2":"high_corr_num = comb_num_feat[np.abs(corr_num_feat) >= 0.9]\nhigh_corr_num","adf84e8b":"high_corr_num1=comb_num_feat[np.abs(corr_num_feat)<=-0.9]\nhigh_corr_num1","944b21d2":"np.unique(high_corr_num[:, 1])","7e2ab89b":"X = X.drop(np.unique(high_corr_num[:, 0]), axis=1, errors='ignore')","044368f4":"cat_feat = X.select_dtypes('object').columns.values\nX[cat_feat].nunique().sort_values()","e179fe0a":" X.drop(['url'], axis=1, errors='ignore',inplace=True)","b38fff14":"cat_feat = X.select_dtypes('object').columns.values\ncomb_cat_feat = np.array(list(combinations(cat_feat,2)))\ncorr_cat_feat = np.array([])\nfor comb in comb_cat_feat:\n    table = pd.pivot_table(X,values='loan_amnt', index=comb[0], columns=comb[1], aggfunc='count').fillna(0)\n    corr = np.sqrt(chi2_contingency(table)[0]\/ (table.values.sum() * (np.min(table.shape) - 1) ) )\n    corr_cat_feat = np.append(corr_cat_feat, corr)","52abb8af":"high_corr_cat = comb_cat_feat[corr_cat_feat >= 0.9]\nhigh_corr_cat","9efe9762":"X = X.drop(np.unique(high_corr_cat[:, 1]), axis=1, errors='ignore')","5dc4371f":"datafitting=X.select_dtypes('number')","33241312":"from sklearn.ensemble import IsolationForest\ndata=X.dtypes","f7719994":"contamination=0.01\nmodel=IsolationForest(contamination=contamination,n_estimators=500)\nmodel.fit(datafitting)","84aaaeb3":"X.sec_app_earliest_cr_line.dtype","fd499646":"datafitting['iforest']=pd.Series(model.predict(datafitting))\ndatafitting['iforest']=datafitting['iforest'].map({1:0,-1:1})\nprint(datafitting['iforest'].value_counts())","72b5baa3":"X=pd.concat([X,datafitting['iforest']],axis=1)","05f93177":"X1=X.copy()","67306006":"X1.head()","e683dc4b":"# X.to_csv(\"modified_lending_club_data3.csv\")","ff390333":"# X1=pd.read_csv('..\/input\/modified-lending-club-data3\/modified_lending_club_data3.csv')","c4296ea6":"X1.drop(['iforest.1'],axis=1,inplace=True)","a3ec825e":"X1.sec_app_earliest_cr_line.dtype\n","d957759a":"X1.drop(X1[X1.iforest==1].index,axis=0,inplace=True)","3bcc3ec1":"X1.drop(['iforest'],axis=1,inplace=True)","8c160c56":"y = X1['loan_status'].copy()\ny = y.isin(['Current', 'Fully Paid', 'In Grace Period']).astype('int')","69563085":"X1.drop('loan_status',axis=1,inplace=True)\n","910db0a1":"X_mod = X1[X1.grade == 'E'].copy()\nX_mod = X_mod.drop(['grade', 'int_rate'], axis=1, errors='ignore')\ny_mod = y[X_mod.index]\n\nX_train, X_test, y_train, y_test = train_test_split(X_mod, y_mod, stratify=y_mod, random_state=0)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, random_state=0)","e35372b4":"cat_feat_ind\n","de90dc82":"cat_feat_ind=[]\nfor cols in X_train.columns:\n    if(X_train[cols].dtypes=='object'):\n        cat_feat_ind.append(cols)","72bd1d75":"# cat_feat_ind = (X_train.dtypes == 'object').nonzero()[0]\npool_train = Pool(X_train, y_train, cat_features=cat_feat_ind)\npool_val = Pool(X_val, y_val, cat_features=cat_feat_ind)\npool_test = Pool(X_test, cat_features=cat_feat_ind)\n\nn = y_train.value_counts()\nmodel = CatBoostClassifier(learning_rate=0.03,\n                           iterations=1000,\n                           early_stopping_rounds=100,\n                           class_weights=[1, n[0] \/ n[1]],\n                           verbose=False,\n                           random_state=0)\nmodel.fit(pool_train, eval_set=pool_val, plot=True);","9f8162cc":"from pandas.core.ops import roperator","af34ed59":"y_pred_test = model.predict(pool_test)\nacc_test = accuracy_score(y_test, y_pred_test)\nprec_test = precision_score(y_test, y_pred_test)\nrec_test = recall_score(y_test, y_pred_test)\nprint(f'''Accuracy (test): {acc_test:.3f}\nPrecision (test): {prec_test:.3f}\nRecall (test): {rec_test:.3f}''')\n\ncm = confusion_matrix(y_test, y_pred_test)\nax = sns.heatmap(cm, cmap='viridis_r', annot=True, fmt='d', square=True)\nax.set_xlabel('Predicted')\nax.set_ylabel('True');","f6bb3cb8":"y_pred_const = np.ones(y_test.size)\n\nacc = accuracy_score(y_test, y_pred_const)\nprec = precision_score(y_test, y_pred_const)\nrec = recall_score(y_test, y_pred_const)\nprint(f'''Accuracy (constant prediction): {acc:.3f}\nPrecision (constant prediction): {prec:.3f}\nRecall (constant prediction): {rec:.3f}''')\n\ncm = confusion_matrix(y_test, y_pred_const)\nax = sns.heatmap(cm, cmap='viridis_r', annot=True, fmt='d', square=True)\nax.set_xlabel('Predicted')\nax.set_ylabel('True');","ec2c3031":"feat = model.feature_names_\nimp = model.feature_importances_\ndf = pd.DataFrame({'Feature': feat, 'Importance': imp})\ndf = df.sort_values('Importance', ascending=False)[:10]\nsns.barplot(x='Importance', y='Feature', data=df);","2741e8cc":"corr = X_mod[df['Feature'].values].corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask=mask, square=True, cmap='RdBu_r', vmin=-1, vmax=1, annot=True, fmt='.2f');","62fc5e9b":"good = X_mod.loc[y_mod == 1, 'loan_amnt']\nbad = X_mod.loc[y_mod == 0, 'loan_amnt']\n\nbins = 20\n\nsns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\nax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\nax.set_ylabel('Density')\nax.legend();","f986a4f9":"good = X_mod.loc[y_mod == 1, 'mths_since_recent_inq']\nbad = X_mod.loc[y_mod == 0, 'mths_since_recent_inq']\n\nbins = 20\nsns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\nax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\nax.set_ylabel('Density')\nax.legend();","249e9375":"good = X_mod.loc[y_mod == 1, 'revol_util']\nbad = X_mod.loc[y_mod == 0, 'revol_util']\n\nbins = 20\nsns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\nax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\nax.set_ylabel('Density')\nax.legend();","106a849f":"y_proba_val = model.predict_proba(pool_val)[:, 1]\np_val, r_val, t_val = precision_recall_curve(y_val, y_proba_val)\nplt.plot(r_val, p_val)\nplt.xlabel('Recall')\n\nplt.ylabel('Precision');","248d06a3":"y_proba_val","827b134b":"p_max = p_val[p_val != 1].max()\nt_all = np.insert(t_val, 0, 0)\nt_adj_val = t_all[p_val == p_max]\ny_adj_val = (y_proba_val > t_adj_val).astype(int)\np_adj_val = precision_score(y_val, y_adj_val)\nprint(f'Adjusted precision (validation): {p_adj_val:.3f}')","a7931ad6":"n = y_adj_val.sum()\nci = proportion_confint(p_adj_val * n, n, alpha=0.05, method='wilson')\nprint(f'95% confidence interval for adjusted precision: [{ci[0]:.3f}, {ci[1]:.3f}]')","6c059559":"y_proba_test = model.predict_proba(pool_test)[:, 1]\ny_adj_test = (y_proba_test > t_adj_val).astype(int)\np_adj_test = precision_score(y_test, y_adj_test)\n\nr_adj_test = recall_score(y_test, y_adj_test)\nprint(f'''Adjusted precision (test): {p_adj_test:.3f}\nAdjusted recall (test): {r_adj_test:.3f}''')\n\ncm_test = confusion_matrix(y_test, y_adj_test)\nax = sns.heatmap(cm_test, cmap='viridis_r', annot=True, fmt='d', square=True)\nax.set_xlabel('Predicted')\nax.set_ylabel('True');","36377940":"yk=pd.read_csv(\" \")#enter the name of you csv file in the double inverted commas ,for excel files use pd.read_excel(\" \")","862a581a":"yk=pd.concat([yk,data['loan_status']],axis=1)\nyk.drop('emp_title',inplace=True,axis=1)\nyk['earliest_cr_line'] = pd.to_datetime(yk['earliest_cr_line'], infer_datetime_format=True)\nyk['sec_app_earliest_cr_line'] = pd.to_datetime(yk['sec_app_earliest_cr_line'], infer_datetime_format=True)\nyk['emp_length'] = yk['emp_length'].replace({'< 1 year': '0 years', '10+ years': '11 years'})\nyk['emp_length'] = yk['emp_length'].str.extract('(\\d+)').astype('float')\nyk['id'] = yk['id'].astype('float')\nyk = yk.drop(['desc', 'member_id'], axis=1, errors='ignore')\nyk[['all_util','avg_cur_bal','dti','percent_bc_gt_75','revol_util','bc_util','num_tl_120dpd_2m']] =yk[['all_util','avg_cur_bal','dti','percent_bc_gt_75','revol_util','bc_util','num_tl_120dpd_2m']].fillna(youou[['all_util','avg_cur_bal','dti','percent_bc_gt_75','revol_util','bc_util','num_tl_120dpd_2m']].mean())\nyk=imputer.transform(yk)\nyk['il_util']=yk['il_util'].fillna(yk['il_util'].median())\naddBinary_imputer1.transform(yk)\nyk[['annual_inc_joint','dti_joint']] = yk[['annual_inc_joint','dti_joint']].fillna(yk[['annual_inc_joint','dti_joint']].median())\naddBinary_imputer2.transform(yk)\nyk['sec_app_earliest_cr_line']=yk['sec_app_earliest_cr_line'].fillna(X['sec_app_earliest_cr_line'].min())\nyk.drop('num_tl_120dpd_2m',axis=1,inplace=True)\nfill_min = ['avg_cur_bal','emp_length']\nyk[fill_min] = yk[fill_min].fillna(youou[fill_min].min())\nyk['pct_tl_nvr_dlq'].dropna(inplace=True)\nyk.dropna(subset=['pct_tl_nvr_dlq'],inplace=True)\nyk.dropna(subset=['mo_sin_old_il_acct','mths_since_rcnt_il'],inplace=True)\naddBinary_imputer3.fit(yk)\naddBinary_imputer3.transform(yk)\nyk[['mths_since_last_delinq',\n 'mths_since_last_major_derog',\n 'mths_since_last_record',\n 'mths_since_recent_bc_dlq',\n 'mths_since_recent_inq',\n 'mths_since_recent_revol_delinq']]=yk[['mths_since_last_delinq','mths_since_last_major_derog','mths_since_last_record','mths_since_recent_bc_dlq','mths_since_recent_inq','mths_since_recent_revol_delinq']].fillna(X[['mths_since_last_delinq',\n 'mths_since_last_major_derog',\n 'mths_since_last_record',\n 'mths_since_recent_bc_dlq',\n 'mths_since_recent_inq',\n 'mths_since_recent_revol_delinq']].max())","c91fb428":"yk = yk.drop(['num_tl_120dpd_2m', 'id','acc_now_delinq'], axis=1, errors='ignore')\nyk=yk.drop(['num_tl_30dpd'],axis=1,errors='ignore')\nyk = yk.drop(np.unique(high_corr_num[:, 0]), axis=1, errors='ignore')\nyk.drop(['url'], axis=1, errors='ignore',inplace=True)\nyk = yk.drop(np.unique(high_corr_cat[:, 1]), axis=1, errors='ignore')\nyk.drop('loan_status',axis=1,inplace=True)\nyk.drop(['grade','int_rate'],axis=1,inplace=True)","6e8f860c":"y_proba_test = model.predict_proba(yk)[:, 1]\ny_adj_test = (y_proba_test > t_adj_val).astype(int)\ny_adj_test","4337f383":"The highly correlated pairs with the absolute value of their correlation coefficient \u22650.9 are printed below.","f04ac02a":"Finally all matching features are saved in the list `avail_feat` a new DataFrame `X` that only contains these features, is created. It is a good practice to set every newly created DataFrame as a copy in order to avoid hidden chained assignments and `SettingWithCopyWarning` further down the code.","c4d2b364":"The features `emp_length` and `id` are numeric and their type should be changed to `float`. In case of `emp_length` I replace the extreme cases of \"< 1 year\" and \"10+ years\" with \"0 years\" and \"11 years\" respectively to separate these groups from the rest.","53ae134f":"The missing lines due to Excel formatting are removed and the feature names are saved in `browse_feat`. Some of them, however, have a different spelling format from the one used in the loan dataset. This format uses capital letters instead of underscores so I identify them using using regular expressions and then correct them. For some features there are whitespaces in front of their names which I remove as well.","80c752ec":"The features `earliest_cr_line` and `sec_app_earliest_cr_line` are dates and their type should be changed to `datetime`. Later they will be transformed to ordinal numeric features by the machine learning model.","6d87edd1":"## 2.3. Missing values\n","f5fa3e69":"# 4. Feature importances\n\nDuring the early draft of this project the analysis of feature importances helped me to realize that the [Lending Club dataset provided by Wendy Kan from Kaggle](https:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data) was actually including features that aren't available for investors. So I decided to include this analysis here in case someone finds it useful.\n\nAmong all features I selected 10 with the largest importance values (see below). The top 3 features are `loan_amnt`, `revol_bal` and `revol_util`. The importances of `mths_since_recent_inq` and `revol_util`, however, are quite close to each other and the rest of the features so this ranking might slightly change for a different train-test split.","839949b6":"From the histogram for the feature `mths_since_recent_inq` (top 3) the loan is less likely to be returned (bad loans) if the borrower had an inquiry recently. This also makes sense because inquiries are usually done when someone applies for a loan, a credit card, etc. so recent inquiries could indicate bad financial stability of the borrower.","686500a3":"Since the validation dataset was used to tune hyperparameters (the number of iterations), I predict targets for the testing dataset which the model hasn't seen yet. The metrics reported here are accuracy, precision and recall. They all have sensible values which is also confirmed by the confusion matrix shown below.","076006f4":"In the below cell we are keeping a copy of the processed data so that if loss out data in the following cells we don't have to run from the start rather we can just run the code by taking the copy of the processed data.","e8c89454":"From the precision-recall curve the best precision is 1 but then the recall would be extremely low so in the end the model might not predict good loans at all. Therefore, I exclude 1 from the precision array and find its maximum. The threshold array `t` returned by `precision_recall_curve()` is missing the threshold 0 in the beginning so I add it to match the dimension of the precision array `p`. Then I find the threshold that correspond to the maximum precision and recalculate the predicted labels. The obtained precision score for the adjusted labels is indeed the maximum (excluding 1) as can be seen from the precision-recall curve.","91b3b9d8":"From the confusion matrix it is clear that the model tries to predict both classes and doesn't prefer one over the other due to their imbalance. The latter is a common mistake and if you see accuracy scores without the confusion matrix, be very skeptical about those results. If over-\/undersampling isn't applied for imbalanced classes, the classifier will opt for the constant prediction in favor of the majority class. In this case the accuracy for this testing dataset will be quite high 0.912 (see below), although this classifier doesn't have any predictive power! This skewness in class predictions is very visible on the confusion matrix.","dbe1c78f":"As a result we decide to drop the column emp_title due to its high cardinality and leads to the memory error when creating a contingency table, therefore I shall remove it.","241e9868":"In gradient boosting the importances of highly correlated features usually split between them. From the correlation heatmap (see below) the feature `revol_util` (top 3) is quite highly correlated with `bc_util` (top 5) which leads to the decreased importance of `revol_util`.","a63e8cfb":"Finally by using the adjusted threshold on the testing dataset, the adjusted precision 0.939 is indeed within the above 95% confidence interval. Note, however, that the recall is significantly decreased from 0.632 to 0.068 but the precision only increased from 0.931 to 0.939. Of course the gain in precision depends on the train-test split and for a different testing dataset can be closer to the right boundary of the confidence interval. Getting higher values than that, however, is unlikely.","8abf5928":"first the investor needs to add the data to the kaggle jupyter notebook from the option mentioned on the right side which says \"+ Add data\" and then store the dataset into a variable called yk","146dd936":"The cardinality in the variable emp_title is very high which might cause overfitting as well as operational errors as some categories under the variable emp_title might only be in the training dataset and some might only be in the test dataset.","c2708f60":"The highly correlated pairs with the absolute value of their correlation coefficient \u22650.9 are printed below.","8268d921":"from the ","7fea6dca":"## 2.4. Target feature\n\nThe target feature for this dataset is an indicator if the loan is good (1) or bad (0). To identify good loans, I use their loan statuses and print their counts below. The description for each status is provided by the Lending Club:\n\n- Current: Loan is up to date on all outstanding payments.\n- In Grace Period: Loan is past due but within the 15-day grace period.\n- Late (16-30): Loan has not been current for 16 to 30 days.\n- Late (31-120): Loan has not been current for 31 to 120 days.\n- Fully paid: Loan has been fully repaid, either at the expiration of the 3- or 5-year year term or as a result of a prepayment.\n- Default: Loan has not been current for an extended period of time.\n- Charged Off: Loan for which there is no longer a reasonable expectation of further payments.","e796ab62":"Still not all the features from \"Browse Notes\" `browse_feat` could be matched with the original features `data_feat` so I print out the unmatched features from both lists to see if some of them could be matched manually.","b8b0c583":"# Following are all the steps which the investor has to run to complete all the preprocessing steps for the lenders data before using that data for making predictions in the ML model","59685589":"The feature `url` has a unique value for each entry and should be removed to avoid overfitting.","676530bd":"In the below cell we are dropping all the values which are 1 because the value 1 is an indicator for outliers","36c59d61":"Thanks for **UPVOTING** this kernel! Trying to become a Kernels Master. \ud83e\udd18\n\nCheck out my other cool projects:\n- [\ud83d\udcca Interactive Titanic dashboard using Bokeh](https:\/\/www.kaggle.com\/pavlofesenko\/interactive-titanic-dashboard-using-bokeh)\n- [\ud83c\udf10 Extending Titanic dataset using Wikipedia](https:\/\/www.kaggle.com\/pavlofesenko\/extending-titanic-dataset-using-wikipedia)\n- [\ud83d\udc6a Titanic extended dataset (Kaggle + Wikipedia)](https:\/\/www.kaggle.com\/pavlofesenko\/titanic-extended)","4fc3b171":"Then I print out the number of unique values for categorical features.","50f31467":"# 3. Modelling approach\n\nAs mentioned in the beginning, the goal of this project is to predict good loans among the high risk \/ high interest loans. The Lending Club has two features: 1) `grade` that assigns risk levels to loans (\"A\" for the lowest risk, \"E\" for the highest risk); 2) `int_rate` that assigns the interest rate according to the risk level (lowest rates for the grade \"A\", highest rates for the grade \"E\"). Therefore, for the modelling dataset `X_mod` I choose only the loans with the grade \"E\" and remove the features `grade` and `int_rate` since the latter is correlated with `grade` by design.\n\nThe modelling dataset is then split into training, validation and testing parts. The validation dataset will be used to adjust some of the hyperparameters such as number of iterations, precision\/recall. Both splits are stratified to ensure similar distribution of classes and to avoid one of the classes being left out in the resulting splits. The latter could happen especially in the case of highly imbalanced datasets. The parameter `random_state=0` is added for the reproducibility of results.","c01830cd":"Note that for pairs of a numeric and a categorical feature correlation coefficients can't be interpreted in a meaningful way and therefore shouldn't be used.","79eec7f5":"Note that the above precision-recall curve is only valid for this particular validation dataset. So the precision that corresponds to the adjusted threshold on this dataset will be different from the precision for the same threshold on a different dataset. If these two datasets, however, are sampled from the same population, the precision values will have a certain spread that can be estimated using confidence intervals. Since precision is a proportion, it has the binomial distribution and its confidence interval can be conveniently calculated using the Statsmodels method `proportion_confint()`. Typically 95% confidence interval is calculated which corresponds to the parameter `alpha=0.05`. This means that the precision will be within this interval in 95% of cases. Since the obtained precision values are close to 1 (edge case), it's better to use the Wilson interval by setting the parameter `method='wilson'`. Also one should carefully calculate the denominator for adjusted precision `n` by taking the total amount of predicted good loans (class 1) after adjustment and not before.","5a251401":"The loans with the statuses `Current` and `Fully Paid` are definitely considered good. The loans with the statuses `In Grace Period` can be considered good or not depending on strictness of the investor. In this project I consider them as good loans. All the other statuses are considered as bad loans. Note that this dataset is highly imbalanced with the minority class being 0.037 of the majority class.","2121749d":"From the histogram for the feature `revol_util` (top 3) the loan is less likely to be returned (bad loans) if the revolving utilization is lower. This actually doesn't make much sense because revolving utilization is the percentage of the used credit on your credit card so higher revolving utilization indicates worse financial stability. Nevertheless, this dataset shows otherwise and it could be an interesting topic for discussion.","f3642ec6":"The column named loan_status is the target variable  and it has been added to the dataframe X so that if we remove some of the rows during feature engineerinig so it can also be reflected in target variable as well.","fba50d51":"It is also useful to look at the distributions of the features to see how their values influence predictions.\n\nFrom the histogram for the feature `loan_amnt` (top 1) the loan is more likely to be returned (good loans) if the loan amount is lower. This makes sense because smaller loan amounts usually have smaller monthly installments that are easier to pay.","53ee7f09":"For all pairs of the numeric features `comb_num_feat` I calculate their Pearson's R correlation coefficient and store it in `corr_num_feat`.","c8502f98":"For this project I will be using the libraries for data manipulation (Pandas, Numpy), regular expressions (Re), data visualization (Matplotlib, Seaborn), machine learning (Scikit-learn, CatBoost) and statistics (Scipy Stats, Itertools and Statsmodels).","f01ed877":"For categorical features  `verification_status_joint` the missing values should be filled with an empty string so these features are placed in the list `fill_empty`.\n\nFor some of the numeric features the missing values should be filled using either the maximum value of the respective columns or the mean of the respective column so these features are placed in the list `fill_max_mean`. For example, the feature `mths_since_last_record` indicates the number of months since the last record (like bankruptcy, foreclosure, tax liens, etc.) so if missing, one can assume that no records were made and the number of months since the \"last\" record should be a maximum but this is only done if the number of missing values in the columns is <5% and if it is more than 5 % then we need to add a missing indicator column as well.\n\nFor the rest of the numeric features the missing values should be filled using the minimum value or the mean value of the respective columns so these features are placed in the list `fill_min_mean`. For example, the feature `sec_app_earliest_cr_line` indicates the credit line of the secondary applicant so if it is missing so we can assume that there is no secondary applicant and so we can impute those missing values with the minimum value of column 'sec_app_earliest_cr_line' but if the missing values are very high in the column then we can add missing indicator for those column as well.","666b3c8d":"The first feature (chosen arbitrarily) from each highly correlated feature pair is then removed.","b89d9931":"# 2. Data preprocessing\n\n## 2.1. Available features\n\nThe [Lending Club dataset provided by Wendy Kan from Kaggle](https:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data) doesn't contain some of the features that are available for investors like, for example, FICO scores. Therefore, I use the [Lending Club dataset provided by Nathan George](https:\/\/www.kaggle.com\/wordsforthewise\/lending-club) that has all Lending Club loan features. This dataset contains more than 2 million rows so to reduce the processing times, I only selected the loans issued in 2018 (\u22480.5 million rows).","c18b4514":"This time I remove the second feature from each highly correlated feature pair in order to keep the feature `grade`. It will be used later to select high risk \/ high interest loans.","579262be":"using IsolationForest to find the outliers","5a7ffba5":"The features `desc` and `member_id` are completely empty so I remove them.","1b419133":"# 1. Introduction\n\nNowadays one can invest in the loans of other people using online peer-to-peer lending platforms like, for example, the [Lending Club](https:\/\/www.lendingclub.com\/). On the Lending Club borrowers with higher credit scores (more trustworthy and less risky) get lower interest rates for their loans while borrowers with lower credit scores (less trustworthy and more risky) get higher rates. From the point of view of the investor the loans with higher interest rates are more attractive because they provide higher return on investment (ROI) but on the other hand they pose risks of being not returned at all (defaulted). Therefore, the machine learning model that could predict which of the high interest loans are more likely to be returned, would bring added value by minimizing the associated risks.\n\n<img src=\"https:\/\/i.imgur.com\/T4Chhxw.jpg\" width=\"400\"\/>","9a9566ad":"Dropping all the rows from the variables with missing values in fill_min_mean if the missing values are in less than 4% of missing values.","6d006373":"sec_app_earliest_cr_line=second applicant earliest credit line","3aee0836":"****## 2.2. Feature types\n\nLet's check the categorical features and see if any of them could be transformed to other types.","4d867fc3":"Calculate a Spearman correlation coefficient with associated p-value.\n\nThe Spearman rank-order correlation coefficient is a nonparametric measure of the monotonicity of the relationship \nbetween two datasets. Unlike the Pearson correlation, the Spearman correlation does not assume that both datasets are normally \ndistributed. Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 \nimply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases,\ny decreases.","e6fa1f5f":"This dataset contains more than 150 features but some of them are only relevant after the loan is issued and therefore, not available at the moment of investing. To get the list of features that are visible to investors, I use the [Lending Club Data Dictionary provided by Wendy Kan](https:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data) (the sheet called \"Browse Notes\").","dfe4590d":"For all pairs of the categorical features `comb_cat_feat` I calculate the Cramer's V correlation coefficient that is expressed through the chi-square statistic $\\chi^2$ of the contingency table:\n\n$$ V = \\sqrt{ \\frac{ \\chi^2 }{ n (\\text{min}(K_1, K_2) - 1) } } $$\n\nwhere $n$ is the sum of all elements in the contingency table, $K_1$ and $K_2$ are the dimensions of the contingency table. Note that Pearson's R correlation coefficient or the spearmans correlation coefficient isn't applicable to categorical features and shouldn't be used.","b1d6596f":"# 5. Model adjustment\n\nThe previously reported model was obtained by minimizing both false positive and false negative errors that contribute to precision and recall respectively. In reality, however, one of these errors might have a larger impact so it would be better to optimize for it instead. In case of loan investing, the false positive errors are the number of bad loans that were identified as good so the investor will loose money by investing in them. This is a direct loss and should be avoided. The false negative errors are the number of good loans that were identified as bad so the investor will not earn extra money by not investing in them. This is a missed opportunity and is less critical compared to the direct loss. Therefore, the false positive errors should be decreased (higher precision) even if the false negative errors will be increased (lower recall). The connection between precision and recall can be visualized using the precision-recall curve (see below). To calculate it, one requires probabilities of belonging to class 1 rather than the predicted labels. This precision-recall curve is calculated for the validation dataset because adjusting precision or recall is similar to adjusting hyperparameters. For each precision-recall pair the function `precision_recall_curve()` also returns the corresponding probability threshold. This threshold is the actual hyperparameter that will be used to obtain the best precision.","0642582e":"## 2.3. Multicollinearity\n\nAlthough highly correlated features (*multicollinearity*) aren't a problem for the machine learning models based on decision trees (as used here), these features decrease importances of each other and can make feature analysis more difficult. Therefore, I calculate feature correlations and remove the features with very high correlation coefficients before applying machine learning.\n\nI start with numeric features and before calculating their correlations, it's a good practice to look at the number of their unique values.","0004cd50":"For modelling I use [CatBoost](https:\/\/catboost.ai\/) - a gradient boosting library based on decision trees. CatBoost is very efficient for datasets that contain categorical features with many categories. Instead of traditional one-hot encoding that generates a lot of features and makes gradient boosting of shallow decision tress difficult, CatBoost uses mean encoding that replaces each categorical feature with only one numerical feature. More about mean encoding can be found in [this video](https:\/\/www.coursera.org\/lecture\/competitive-data-science\/concept-of-mean-encoding-b5Gxv).\n\nBefore fitting the model I transform the datasets and their targets into the CatBoost objects `Pool()`. The model is defined using the object `CatBoostClassifier()` with several parameters. In `CatBoostClassifier()` the optimal value for the `learning_rate` is calculated automatically for the chosen `iterations` but it's not always the best value. Therefore, I set it to the quite small value 0.03 to ensure good convergence. The parameter `iterations` is set to 1000 so that the model would converge even with such a small learning rate. Another parameter `early_stopping_rounds` is set to 100 (0.1 of the parameter `iterations`) to stop the training and to save time if overfitting is observed. Since the dataset is highly imbalanced, the ratio of two classes `n[0] \/ n[1]` is passed to the parameter `class_weights`. Using class weights here is equivalent to random oversampling. Note that the final model will be defined not by the last iteration but by the best error score on the evaluation dataset. One can see this optimal point on the CatBoost plot (only visible in the Edit mode of the notebook).\npool is a faster way of passing data to catboost algorithms like catboostregressor,catboostclassifier","ed58a439":"The feature `num_tl_120dpd_2m` has only one value (a constant) and can be removed. The feature `id` has a unique value for each row and should also be removed, otherwise the model will overfit.","782745b7":"Indeed some of the features are spelled differently but mean the same thing, for example `verified_status_joint` and `verification_status_joint`. So I remove wrong and add correct ones to the list `browse_feat`.","e56a4721":"Note that the data should be a proper which means it should contain the same attributes as the variabe X had in the beginning of the notebook"}}