{"cell_type":{"e0c90d32":"code","7363c12b":"code","1682c7a4":"code","3921545b":"code","f04212be":"code","84154270":"code","4b8364ca":"code","32c8e1b8":"code","582f993e":"code","729be140":"code","212b36c4":"code","5d758d9e":"code","6c94f0cf":"code","ab633d9e":"code","471b3770":"code","5776445f":"code","ff43751e":"code","bfe06ae9":"code","8396cf5b":"code","5818785f":"code","209ea676":"code","82aa624e":"code","a1fcf0e1":"code","f9af5499":"code","6d65bbc9":"code","183749eb":"markdown","0a7a4695":"markdown","e432612d":"markdown","2ed118cc":"markdown","30a39431":"markdown","2c85eed8":"markdown","a5993ebe":"markdown","77989715":"markdown","99895539":"markdown","70be1760":"markdown","bff39d0b":"markdown"},"source":{"e0c90d32":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import datasets, transforms, models # this models contains all the highly pretrained models.","7363c12b":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","1682c7a4":"!git clone https:\/\/github.com\/jaddoescad\/ants_and_bees.git","3921545b":"!ls","f04212be":"!ls ants_and_bees","84154270":"!ls ants_and_bees\/train","4b8364ca":"transform_train = transforms.Compose([transforms.Resize((224,224)), #We resize our data as vgg16 is trained on 224*224 size images\n                                      transforms.RandomHorizontalFlip(), # flips the image at horizontal axis\n                                      transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n                                      transforms.ColorJitter(brightness=1, contrast=1, saturation=1), # We can change it according to the accuracy and requirements\n                                      transforms.ToTensor(), # convert the image to tensor to make it work with torch\n                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                               ])\n\n\ntransform = transforms.Compose([transforms.Resize((224,224)),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                               ])\n\ntraining_dataset = datasets.ImageFolder('ants_and_bees\/train', transform=transform_train)\nvalidation_dataset = datasets.ImageFolder('ants_and_bees\/val', transform=transform)\n\ntraining_loader = torch.utils.data.DataLoader(training_dataset, batch_size=20, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size = 20, shuffle=False)","32c8e1b8":"print(len(training_dataset))\nprint(len(validation_dataset))","582f993e":"def im_convert(tensor):\n  image = tensor.cpu().clone().detach().numpy()\n  image = image.transpose(1, 2, 0)\n  image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n  image = image.clip(0, 1)\n  return image","729be140":"classes = ('ant', 'bee') # since our dataset has 2 classes","212b36c4":"dataiter = iter(training_loader) # converting our train folder to iterable so that it can be iterated through a batch of 20.\nimages, labels = dataiter.next() # We get our inputs for our model here.\nfig = plt.figure(figsize=(25, 4))\n\nfor idx in np.arange(20):\n  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n  plt.imshow(im_convert(images[idx]))\n  ax.set_title(classes[labels[idx].item()])","5d758d9e":"model = models.vgg16(pretrained=True) ","6c94f0cf":"model","ab633d9e":"# Here we are freezing the feature parameters and using same as in vgg16. These parameters are very good as this model is trained on 14 million images for 2 weeks with many GPUs\nfor param in model.features.parameters():\n  param.requires_grad = False # the feature layers do not require any gradient.","471b3770":"#We take the last fully connected layer from vgg16 and replace it with our layer as we need to classify only 2 different categories\nimport torch.nn as nn\n\nn_inputs = model.classifier[6].in_features # the i\/p features of our classifier model\nlast_layer = nn.Linear(n_inputs, len(classes))  # new layer that we want to put in there.\nmodel.classifier[6] = last_layer  # replacing last layer of vgg16 with our new layer\nmodel.to(device)  # Put the model in device for higher processing power\nprint(model.classifier[6].out_features)","5776445f":"criterion = nn.CrossEntropyLoss() # same as categorical_crossentropy from keras\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.0001) #lr is very low as we have a very samll dataset","ff43751e":"epochs = 5 # lesser epochs as our dataset is small\nrunning_loss_history = []\nrunning_corrects_history = []\nval_running_loss_history = []\nval_running_corrects_history = []\n\nfor e in range(epochs):\n  \n  running_loss = 0.0\n  running_corrects = 0.0\n  val_running_loss = 0.0\n  val_running_corrects = 0.0\n  \n  for inputs, labels in training_loader: #taking inputs and put it in our model which is inside device\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    _, preds = torch.max(outputs, 1)\n    running_loss += loss.item()\n    running_corrects += torch.sum(preds == labels.data)\n\n  else:\n    with torch.no_grad():\n      for val_inputs, val_labels in validation_loader:\n        val_inputs = val_inputs.to(device)\n        val_labels = val_labels.to(device)\n        val_outputs = model(val_inputs)\n        val_loss = criterion(val_outputs, val_labels)\n\n        _, val_preds = torch.max(val_outputs, 1)\n        val_running_loss += val_loss.item()\n        val_running_corrects += torch.sum(val_preds == val_labels.data)\n      \n    epoch_loss = running_loss\/len(training_loader.dataset)\n    epoch_acc = running_corrects.float()\/ len(training_loader.dataset) # We are now dividing the total loss of one epoch with the enitre length of dataset to get the probability between 1 & 0\n    running_loss_history.append(epoch_loss)\n    running_corrects_history.append(epoch_acc)\n    \n    val_epoch_loss = val_running_loss\/len(validation_loader.dataset)\n    val_epoch_acc = val_running_corrects.float()\/ len(validation_loader.dataset)\n    val_running_loss_history.append(val_epoch_loss)\n    val_running_corrects_history.append(val_epoch_acc)\n    print('epoch :', (e+1))\n    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))","bfe06ae9":"plt.style.use(\"ggplot\")\nplt.plot(running_loss_history, label='training loss')\nplt.plot(val_running_loss_history, label='validation loss')\nplt.legend()","8396cf5b":"plt.style.use(\"ggplot\")\nplt.plot(running_corrects_history, label='training accuracy')\nplt.plot(val_running_corrects_history, label='validation accuracy')\nplt.legend()","5818785f":"import PIL.ImageOps  # # from python imaging library we take this so we can preform operations on our image","209ea676":"import requests\nfrom PIL import Image\n\nurl = 'http:\/\/cdn.sci-news.com\/images\/enlarge5\/image_6425e-Giant-Red-Bull-Ant.jpg'\nresponse = requests.get(url, stream = True)\nimg = Image.open(response.raw)\nplt.imshow(img)","82aa624e":"img = transform(img)  # transform before i\/p to our model.\nplt.imshow(im_convert(img)) #converting the image to plot using plt","a1fcf0e1":"image = img.to(device).unsqueeze(0) # unsqueeze to add one dimension\noutput = model(image)\n_, pred = torch.max(output, 1)\nprint(classes[pred.item()])","f9af5499":"dataiter = iter(validation_loader)\nimages, labels = dataiter.next()\nimages = images.to(device)\nlabels = labels.to(device)\noutput = model(images)\n_, preds = torch.max(output, 1)\n\nfig = plt.figure(figsize=(25, 4))\n\nfor idx in np.arange(20):\n  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n  plt.imshow(im_convert(images[idx]))\n  ax.set_title(\"{} ({})\".format(str(classes[preds[idx].item()]), str(classes[labels[idx].item()])), color=(\"green\" if preds[idx]==labels[idx] else \"red\"))","6d65bbc9":"# The model has performed really well using transfer learning","183749eb":"Convert the images in order to plot them using plt","0a7a4695":"Iter our data to plot our images. ","e432612d":"# Fitting our model with our inputs and displaying the progress.","2ed118cc":"Apply transform and augmentation on our images","30a39431":"# We now plot new images from web and try to predict them using our model","2c85eed8":"Testing my model using random images from the validation dataset.","a5993ebe":"**Image classification using Transfer Learning technique**\n> Objective: Building complex model on Pytorch using Transfer Learning that is able to classify complex images.\n\nWhat is Transfer Learning?\n\nTransfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.\n\nIn this project I have used Transfer Learning technique to classify complex images.\n\nThe model I used here is award wining [vgg16](http:\/https:\/\/neurohive.io\/en\/popular-networks\/vgg16\/\/) model with 13 convolutional layers for feature extraction and 3 fully connected layers for classification. The accuracy this model yield is around 91% which is amazing considering that the dataset I used is small, by increasing the size of dataset, much better accuracy can be achieved.\nI would recommend to use your own dataset and just use my model as a reference.\n\nrequirements:-\n\npip install torch===1.5.0 torchvision===0.6.0 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n","77989715":"Prediction:","99895539":"Clone the git repo for getting our dataset","70be1760":"Initialize GPU","bff39d0b":"We define our model here. We are using vgg16 here which has 16 layers"}}