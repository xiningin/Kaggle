{"cell_type":{"1d5482cb":"code","3f5aadaa":"code","c9e67741":"code","7eb9df70":"code","b75a5998":"code","e4e451a1":"code","7a471feb":"code","080badea":"code","f7b3ecaa":"code","ddef32d1":"code","7437a067":"code","601654a7":"code","66ab5155":"code","292dd777":"code","5667c19e":"code","63c3f860":"code","5358d293":"code","58595649":"code","3816be89":"code","2b5287bc":"code","31f08ea8":"code","94bc7e34":"code","e0418e74":"code","c526182a":"code","9f8e0bdd":"code","09eca153":"code","bfc31308":"code","5ddc4e97":"code","d4b5c499":"code","ea8932d4":"code","8ecf7c41":"code","7b712c04":"code","96bf90c7":"code","1c5a1757":"code","6e0208bf":"code","c3e1e6be":"code","10c7a9bf":"code","edae326d":"code","efadeaa8":"code","cc315d6b":"markdown","0ceb4c16":"markdown","b8d27e9c":"markdown","fde8c5c1":"markdown","65238d76":"markdown","ef52df2b":"markdown","27e4b96e":"markdown","8d2c3dba":"markdown","371fde90":"markdown","ebe4ce41":"markdown","3819883f":"markdown","32970d7a":"markdown","525ece9c":"markdown","31c29d87":"markdown","082b69f1":"markdown","6e3f313e":"markdown","e94cd1d9":"markdown","64a80ef0":"markdown","8c6ce477":"markdown","a5962929":"markdown","0e795a9a":"markdown","3a38ea4d":"markdown","3e54bd12":"markdown","75529b8a":"markdown","a25f7a94":"markdown","2e0d9387":"markdown","ac2ceaa6":"markdown","301b6af4":"markdown"},"source":{"1d5482cb":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew \n\n#evaluation metrics :  \nfrom sklearn import metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error     \nfrom sklearn.metrics import r2_score\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.tree import export_graphviz \nfrom sklearn.metrics import roc_curve, auc \nfrom sklearn.metrics import classification_report \n\n\ndataset = pd.read_csv(\"..\/input\/Admission_Predict_Ver1.1.csv\",sep = \",\")\n\n","3f5aadaa":"dataset.info()","c9e67741":"dataset.drop('Serial No.', axis=1 , inplace=True)","7eb9df70":"dataset.describe()","b75a5998":"dataset.corr()","e4e451a1":"fig,ax = plt.subplots(figsize=(6, 6))\nsns.heatmap(dataset.corr() ,ax=ax ,  annot=True )\n","7a471feb":"sns.boxplot( x = dataset['Chance of Admit '] , orient = 'v')","080badea":"dataset = dataset.drop(dataset[(dataset['Chance of Admit ']<0.4)].index)\n\n\nsns.boxplot( x = dataset['Chance of Admit '] , orient = 'v')\n","f7b3ecaa":"sns.distplot(dataset['Chance of Admit '] , fit=norm)    # line vs normal\n","ddef32d1":"stats.probplot(dataset['Chance of Admit '], plot=plt)     \nplt.show()\n","7437a067":"dataset['Chance of Admit '].skew() ","601654a7":"dataset['Chance of Admit '].kurt()","66ab5155":"sns.distplot(dataset['CGPA'] , fit=norm)    ","292dd777":"sns.boxplot('CGPA' , data = dataset , orient = 'v')","5667c19e":"sns.jointplot('CGPA' , 'Chance of Admit ', data = dataset , kind = 'reg')","63c3f860":"sns.distplot(dataset['GRE Score'] , fit=norm)   \n","5358d293":"sns.boxplot( 'GRE Score' , data = dataset , orient = 'v')\n","58595649":"sns.jointplot('GRE Score' , 'Chance of Admit ', data = dataset , kind = 'reg')\n","3816be89":"sns.distplot(dataset['TOEFL Score'] , fit=norm)   \n","2b5287bc":"sns.boxplot( 'TOEFL Score' , data = dataset , orient = 'v')\n","31f08ea8":"sns.jointplot('TOEFL Score' , 'Chance of Admit ', data = dataset , kind = 'reg')\n","94bc7e34":"sns.countplot('University Rating' ,   data = dataset)\n","e0418e74":"sns.boxplot( 'University Rating' , data = dataset , orient = 'v')\n","c526182a":"sns.countplot('SOP' ,   data = dataset)\n","9f8e0bdd":"sns.boxplot('SOP' , data = dataset , orient = 'v')\n","09eca153":"sns.countplot('LOR ' ,   data = dataset)\n","bfc31308":"sns.boxplot('LOR ' ,  'Chance of Admit ' , data = dataset , orient = 'v')\n","5ddc4e97":"# Eliminate some outliers and check out\n\ndataset = dataset.drop(dataset[(dataset['LOR ']<1.5)].index)\n\nsns.boxplot('LOR ' ,  'Chance of Admit ' , data = dataset , orient = 'v')\n","d4b5c499":"sns.countplot('Research' , data = dataset)\n","ea8932d4":"y = dataset['Chance of Admit '].values\n\nX = dataset.drop(['Chance of Admit '],axis=1)","8ecf7c41":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n\n                                   \nfrom sklearn.preprocessing import StandardScaler       \nsc = StandardScaler()                                  \nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","7b712c04":"\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nregressors = [['Linear Regression :' , LinearRegression()],\n       ['Decision Tree Regression :' , DecisionTreeRegressor()],\n       ['Random Forest Regression :' , RandomForestRegressor()],\n       ['Gradient Boosting Regression :', GradientBoostingRegressor()],\n       ['Ada Boosting Regression :', AdaBoostRegressor()],\n       ['Extra Tree Regression :', ExtraTreesRegressor()],\n       ['K-Neighbors Regression :', KNeighborsRegressor()],\n       ]\n\nfor name,model in regressors:\n        \n    model = model\n    \n    model.fit(X_train,y_train)\n    \n    y_pred = model.predict(X_test)\n        \n\n    print('-----------------------------------')\n    print(name)\n    print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n    print('MSE:', metrics.mean_squared_error(y_test,y_pred))\n    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n    print('R2score:', r2_score(y_test,y_pred))\n    print('---------------------------------')\n\n","96bf90c7":"from sklearn.model_selection import RandomizedSearchCV\n\n\n\nlearning_rate = [0.1 , 0.01 ]\nn_estimators = [int(x) for x in np.linspace(start = 1, stop = 100, num = 4 )]  \nsubsample = [1.0] \nmin_samples_split = [2, 5, 10]                              \nmin_samples_leaf = [1, 2, 4]                                \nmax_depth = [int(x) for x in np.linspace(1, 200, num = 4)]  \nmax_depth.append(None)\n\nmin_impurity_decrease = [0.0]\nmin_impurity_split = [None]\ninit = [None] \nrandom_state = [42]                \nmax_features = ['auto', 'sqrt']    \nverbose = [0 ]\nwarm_start = [False]\npresort = ['auto'] \nvalidation_fraction = [0.1]\nn_iter_no_change = [None]\ntol = [0.0001]\n\n\nparam_distributions = dict(\n                           learning_rate = learning_rate , \n                           n_estimators = n_estimators,\n                           subsample = subsample,\n                           min_samples_split = min_samples_split,\n                           min_samples_leaf = min_samples_leaf,\n                           max_depth = max_depth, \n                           init = init, \n                           random_state = random_state ,           \n                           max_features = max_features ,  \n                           verbose = verbose, \n                           warm_start = warm_start, \n                           presort = presort,\n                           validation_fraction = validation_fraction, \n                           n_iter_no_change = n_iter_no_change, \n                           tol = tol)\n\n\nhyper_model = GradientBoostingRegressor()   \n\n\n\nrandom = RandomizedSearchCV(estimator = hyper_model,         \n                            param_distributions = param_distributions,\n                            n_iter = 100,\n                            cv = 5,\n                            random_state = 42, \n                            verbose = 1, \n                            n_jobs = -1,\n                            )\n\n\nrandom_result = random.fit(X_train, y_train)\n\n\nprint('Best Score: ', random_result.best_score_)\nprint('Best Params: ', random_result.best_params_)\n\n\n\n","1c5a1757":"dataset = pd.read_csv(\"..\/input\/Admission_Predict_Ver1.1.csv\",sep = \",\")","6e0208bf":"y = dataset['Chance of Admit '].values\n\nX = dataset.drop(['Chance of Admit '],axis=1)\n\n\n\ndataset = dataset.drop(dataset[(dataset['Chance of Admit ']<0.4)].index)\n\ndataset = dataset.drop(dataset[(dataset['LOR ']<1.5)].index)\n\n","c3e1e6be":"\ny = [1 if each > 0.8 else 0 for each in y]\n\ny = np.array(y)\n","10c7a9bf":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n\n                                     \nfrom sklearn.preprocessing import StandardScaler       \nsc = StandardScaler()                                  \nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","edae326d":"\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\nclassifiers =    [['Logistic Regression Classifier :', LogisticRegression()] ,\n       ['Decision Tree Classifier :', DecisionTreeClassifier()] ,\n       ['Random Forest Classifier :', RandomForestClassifier()] ,\n       ['Gradient Boosting Classifier :', GradientBoostingClassifier()] ,\n       ['Ada Boosting AdaBoost Classifier :', AdaBoostClassifier()] ,\n       ['Extra Tree Classifier :', ExtraTreesClassifier()] ,\n       ['K-Neighbors Classifier :', KNeighborsClassifier()] ,\n       ['Naive Bayes :' , GaussianNB()] ,\n       ]\n\n\nfor name,model in classifiers:    \n\n    model = model\n    \n    model.fit(X_train,y_train)\n    \n    y_pred = model.predict(X_test)\n        \n\n    print('-----------------------------------')\n    print('-----------------------------------')\n    print(name)\n    print('Accuracy Score ', accuracy_score( y_test, y_pred))\n    print(\"f1_score: \",f1_score( y_test, y_pred))\n    print(\"precision_score: \", precision_score( y_test, y_pred))\n    print(\"recall_score: \", recall_score( y_test, y_pred))\n    print(\"ROC AUC: \", roc_auc_score( y_test, y_pred))   \n    print('---------------------------------')\n\n","efadeaa8":"from sklearn.model_selection import RandomizedSearchCV\n\n\n\nlearning_rate = [0.1 , 0.01 ]\nn_estimators = [int(x) for x in np.linspace(start = 1, stop = 100, num = 4 )]  \nsubsample = [1.0] \nmin_samples_split = [2, 5, 10]                              \nmin_samples_leaf = [1, 2, 4]                                \nmax_depth = [int(x) for x in np.linspace(1, 200, num = 4)]  \nmax_depth.append(None)\n\nmin_impurity_decrease = [0.0]\nmin_impurity_split = [None]\ninit = [None] \nrandom_state = [42]                \nmax_features = ['auto', 'sqrt']    \nverbose = [0 ]\nwarm_start = [False]\npresort = ['auto'] \nvalidation_fraction = [0.1]\nn_iter_no_change = [None]\ntol = [0.0001]\n\n\nparam_distributions = dict(\n                           learning_rate = learning_rate , \n                           n_estimators = n_estimators,\n                           subsample = subsample,\n                           min_samples_split = min_samples_split,\n                           min_samples_leaf = min_samples_leaf,\n                           max_depth = max_depth, \n                           init = init, \n                           random_state = random_state ,           \n                           max_features = max_features ,  \n                           verbose = verbose, \n                           warm_start = warm_start, \n                           presort = presort,\n                           validation_fraction = validation_fraction, \n                           n_iter_no_change = n_iter_no_change, \n                           tol = tol)\n\n\nhyper_model = GradientBoostingClassifier()   \n\n\n\nrandom = RandomizedSearchCV(estimator = hyper_model,         \n                            param_distributions = param_distributions,\n                            n_iter = 100,\n                            cv = 5,\n                            random_state = 42, \n                            verbose = 1, \n                            n_jobs = -1,\n                            )\n\n\nrandom_result = random.fit(X_train, y_train)\n\n\nprint('Best Score: ', random_result.best_score_)\nprint('Best Params: ', random_result.best_params_)","cc315d6b":"## ***TOEFL   VS  Chance of Admission***","0ceb4c16":"\n- EDA\n\n- Regression Pipeline\n\n- RandomizedSearchCV - GradientBoost Regressor\n\n- Classification Pipeline\n\n- RandomizedSearchCV - GradientBoost Classifier","b8d27e9c":"## ***GRE Score   VS   Chance of Admission***","fde8c5c1":"The School of Athens - Raffaello Sanzio - 1509-1511","65238d76":"#     EDA","ef52df2b":"# RandomizedSearchCV - Regression","27e4b96e":"If you have any advice\/suggestion, let me know in the comments and upvote!\nThank you!","8d2c3dba":"Let's try another approach , converting y label ,Chance of Admit into categorical","371fde90":"<img style=\"float: left;\" src=\"https:\/\/arttrip.it\/wp-content\/uploads\/2016\/12\/la-scuola-di-atene-raffaello-sanzio.jpg\" width=\"850px\"\/>","ebe4ce41":"## ***LOR VS Chance of Admission***","3819883f":"## EDA by Feature Correlations:\n\n- CGPA  \n\n- GRE Score \n\n- TOEFL Score    \n\n- University Rating  \n\n- SOP  \n\n- LOR   \n\n- Research   ","32970d7a":"Eliminate Serial No.","525ece9c":"### *Trasform y label into categorical*","31c29d87":"## ***SOP VS Chance of Admission***","082b69f1":"If you have any advice\/suggestion, let me know in the comments and upvote!\nThank you!","6e3f313e":"Chance of Admission, our label, seems to be well distributed","e94cd1d9":"### *Prepare data for Classification*","64a80ef0":"## ***CGPA  VS  Chance of Admission***","8c6ce477":"### ***Prepare data for Regression***","a5962929":"By now, Linear Regressor is the best model , followed by GradientBoostRegressor.\nLet's focus on GradiendBoostRegressor and fit it with hyperparameters through RadomizedSearchCV","0e795a9a":"\n### INTRODUCTION\n\nPurpose:\n\nHelp students to decide to apply for a master's degree or not.\n\n\nFeatures in the dataset:\n\nGRE Scores (290 to 340)\n\nTOEFL Scores (92 to 120)\n\nUniversity Rating (1 to 5)\n\nStatement of Purpose (1 to 5)\n\nLetter of Recommendation Strength (1 to 5)\n\nUndergraduate CGPA (6.8 to 9.92)\n\nResearch Experience (0 or 1)\n\nChance of Admit (0.34 to 0.97)\n\n","3a38ea4d":"## ***University Rating  VS  Chance of Admission***","3e54bd12":"## *Chance of Admission*","75529b8a":"# RandomizedSearchCV - Classification","a25f7a94":"# Classification","2e0d9387":"Eliminate outliers","ac2ceaa6":"# Regression Pipeline","301b6af4":"## ***Research VS Chance of Admission***"}}