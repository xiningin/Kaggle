{"cell_type":{"213c14ab":"code","4134756f":"code","5029c038":"code","f66f0888":"code","b9c745a7":"code","909e03c1":"code","3a6b0951":"code","6b8a8e9e":"code","9bacff30":"code","03b58c19":"code","05b32a2a":"code","15901b1d":"code","a0f6df48":"code","11173fbd":"code","46d174b2":"code","08372e86":"code","1cb94d9e":"code","ac6580d2":"code","2f2bd62e":"markdown","778839fa":"markdown","8948efac":"markdown","8a23f44f":"markdown","1f21ce96":"markdown","a4fbf5c3":"markdown","80b8988a":"markdown"},"source":{"213c14ab":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nfrom time import perf_counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom IPython.display import Markdown, display\n\ndef printmd(string):\n    # Print with Markdowns    \n    display(Markdown(string))","4134756f":"# Create a list with the filepaths for training and testing\ndir_ = Path('..\/input\/dogs-cats-images\/dataset\/training_set')\ntrain_filepaths = list(dir_.glob(r'**\/*.jpg'))\n\ndir_ = Path('..\/input\/dogs-cats-images\/dataset\/test_set')\ntest_filepaths = list(dir_.glob(r'**\/*.jpg'))","5029c038":"def proc_img(filepath):\n    \"\"\" Create a DataFrame with the filepath and the labels of the pictures\n    \"\"\"\n\n    labels = [str(filepath[i]).split(\"\/\")[-2] \\\n              for i in range(len(filepath))]\n\n    filepath = pd.Series(filepath, name='Filepath').astype(str)\n    labels = pd.Series(labels, name='Label')\n\n    # Concatenate filepaths and labels\n    df = pd.concat([filepath, labels], axis=1)\n\n    # Shuffle the DataFrame and reset index\n    df = df.sample(frac=1,random_state=0).reset_index(drop = True)\n    \n    return df\n\ntrain_df = proc_img(train_filepaths)\ntest_df = proc_img(test_filepaths)\n\nprint(f'Number of pictures in the training dataset: {train_df.shape[0]}\\n')\nprint(f'Number of pictures in the test dataset: {test_df.shape[0]}\\n')\nprint(f'Number of different labels: {len(train_df.Label.unique())}\\n')\nprint(f'Labels: {train_df.Label.unique()}')\n\n# The DataFrame with the filepaths in one column and the labels in the other one\ntrain_df.head(5)","f66f0888":"# Display the number of pictures of each category\nvc = train_df['Label'].value_counts()\nplt.figure(figsize=(9,5))\nsns.barplot(x = vc.index, y = vc, palette = \"rocket\")\nplt.title(\"Number of pictures of each category\", fontsize = 15)\nplt.show()","b9c745a7":"# Display some pictures of the dataset\nfig, axes = plt.subplots(nrows=4, ncols=6, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(train_df.Filepath[i]))\n    ax.set_title(train_df.Label[i], fontsize = 15)\nplt.tight_layout(pad=0.5)\nplt.show()","909e03c1":"def create_gen():\n    # Load the Images with a generator and Data Augmentation\n    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n        validation_split=0.1\n    )\n\n    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n    )\n\n    train_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='training',\n        rotation_range=30,\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    val_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='validation',\n        rotation_range=30,\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    test_images = test_generator.flow_from_dataframe(\n        dataframe=test_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=False\n    )\n    \n    return train_generator,test_generator,train_images,val_images,test_images","3a6b0951":"def get_model(model):\n# Load the pretained model\n    kwargs =    {'input_shape':(224, 224, 3),\n                'include_top':False,\n                'weights':'imagenet',\n                'pooling':'avg'}\n    \n    pretrained_model = model(**kwargs)\n    pretrained_model.trainable = False\n    \n    inputs = pretrained_model.input\n\n    x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n\n    outputs = tf.keras.layers.Dense(2, activation='softmax')(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model","6b8a8e9e":"# Dictionary with the models\nmodels = {\n    \"DenseNet121\": {\"model\":tf.keras.applications.DenseNet121, \"perf\":0},\n    \"MobileNetV2\": {\"model\":tf.keras.applications.MobileNetV2, \"perf\":0},\n    \"DenseNet169\": {\"model\":tf.keras.applications.DenseNet169, \"perf\":0},\n    \"DenseNet201\": {\"model\":tf.keras.applications.DenseNet201, \"perf\":0},\n    \"EfficientNetB0\": {\"model\":tf.keras.applications.EfficientNetB0, \"perf\":0},\n    \"EfficientNetB1\": {\"model\":tf.keras.applications.EfficientNetB1, \"perf\":0},\n    \"EfficientNetB2\": {\"model\":tf.keras.applications.EfficientNetB2, \"perf\":0},\n    \"EfficientNetB3\": {\"model\":tf.keras.applications.EfficientNetB3, \"perf\":0},\n    \"EfficientNetB4\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"EfficientNetB5\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"EfficientNetB6\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"EfficientNetB7\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"InceptionResNetV2\": {\"model\":tf.keras.applications.InceptionResNetV2, \"perf\":0},\n    \"InceptionV3\": {\"model\":tf.keras.applications.InceptionV3, \"perf\":0},\n    \"MobileNet\": {\"model\":tf.keras.applications.MobileNet, \"perf\":0},\n    \"MobileNetV2\": {\"model\":tf.keras.applications.MobileNetV2, \"perf\":0},\n    \"MobileNetV3Large\": {\"model\":tf.keras.applications.MobileNetV3Large, \"perf\":0},\n    \"MobileNetV3Small\": {\"model\":tf.keras.applications.MobileNetV3Small, \"perf\":0},\n#     \"NASNetLarge\": {\"model\":tf.keras.applications.NASNetLarge, \"perf\":0}, Deleted because the input shape has to be another one\n    \"NASNetMobile\": {\"model\":tf.keras.applications.NASNetMobile, \"perf\":0},\n    \"ResNet101\": {\"model\":tf.keras.applications.ResNet101, \"perf\":0},\n    \"ResNet101V2\": {\"model\":tf.keras.applications.ResNet101V2, \"perf\":0},\n    \"ResNet152\": {\"model\":tf.keras.applications.ResNet152, \"perf\":0},\n    \"ResNet152V2\": {\"model\":tf.keras.applications.ResNet152V2, \"perf\":0},\n    \"ResNet50\": {\"model\":tf.keras.applications.ResNet50, \"perf\":0},\n    \"ResNet50V2\": {\"model\":tf.keras.applications.ResNet50V2, \"perf\":0},\n    \"VGG16\": {\"model\":tf.keras.applications.VGG16, \"perf\":0},\n    \"VGG19\": {\"model\":tf.keras.applications.VGG19, \"perf\":0},\n    \"Xception\": {\"model\":tf.keras.applications.Xception, \"perf\":0}\n}\n\n# Create the generators\ntrain_generator,test_generator,train_images,val_images,test_images=create_gen()\nprint('\\n')\n\n# Fit the models\nfor name, model in models.items():\n    \n    # Get the model\n    m = get_model(model['model'])\n    models[name]['model'] = m\n    \n    start = perf_counter()\n    \n    # Fit the model\n    history = m.fit(train_images,validation_data=val_images,epochs=1,verbose=0)\n    \n    # Sav the duration and the val_accuracy\n    duration = perf_counter() - start\n    duration = round(duration,2)\n    models[name]['perf'] = duration\n    print(f\"{name:20} trained in {duration} sec\")\n    \n    val_acc = history.history['val_accuracy']\n    models[name]['val_acc'] = [round(v,4) for v in val_acc]","9bacff30":"for name, model in models.items():\n    \n    # Predict the label of the test_images\n    pred = models[name]['model'].predict(test_images)\n    pred = np.argmax(pred,axis=1)\n\n    # Map the label\n    labels = (train_images.class_indices)\n    labels = dict((v,k) for k,v in labels.items())\n    pred = [labels[k] for k in pred]\n\n    y_test = list(test_df.Label)\n    acc = accuracy_score(y_test,pred)\n    models[name]['acc'] = round(acc,4)","03b58c19":"# Create a DataFrame with the results\nmodels_result = []\n\nfor name, v in models.items():\n    models_result.append([ name, models[name]['val_acc'][-1], \n                          models[name]['acc'],\n                          models[name]['perf']])\n    \ndf_results = pd.DataFrame(models_result, \n                          columns = ['model','val_accuracy','accuracy','Training time (sec)'])\ndf_results.sort_values(by='accuracy', ascending=False, inplace=True)\ndf_results.reset_index(inplace=True,drop=True)\ndf_results","05b32a2a":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'model', y = 'accuracy', data = df_results)\nplt.title('Accuracy on the test set (after 1 epoch))', fontsize = 15)\nplt.ylim(0,1)\nplt.xticks(rotation=90)\nplt.show()","15901b1d":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'model', y = 'Training time (sec)', data = df_results)\nplt.title('Training time for each model in sec', fontsize = 15)\nplt.xticks(rotation=90)\nplt.show()","a0f6df48":"# Create and train the model\nmodel = get_model(tf.keras.applications.DenseNet201)\nhistory = model.fit(train_images,\n                    validation_data=val_images,\n                    epochs=50,\n                    callbacks=[\n                        tf.keras.callbacks.EarlyStopping(\n                            monitor='val_loss',\n                            patience=10,\n                            restore_best_weights=True)]\n                    )","11173fbd":"pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\nplt.title(\"Accuracy\")\nplt.show()","46d174b2":"pd.DataFrame(history.history)[['loss','val_loss']].plot()\nplt.title(\"Loss\")\nplt.show()","08372e86":"# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\n# Get the accuracy on the test set\ny_test = list(test_df.Label)\nacc = accuracy_score(y_test,pred)\nprintmd(f'# Accuracy on the test set: {acc * 100:.2f}%')","1cb94d9e":"# Display a confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (4,3))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)),cbar=False)\nplt.title('Normalized Confusion Matrix', fontsize = 23)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()","ac6580d2":"# Display picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=4, ncols=6, figsize=(20, 12),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i].split('_')[0]}\\nPredicted: {pred[i].split('_')[0]}\", fontsize = 15)\nplt.tight_layout()\nplt.show()","2f2bd62e":"# Table of contents\n\n[<h3>1. Data preprocessing and visualization<\/h3>](#1)\n\n[<h3>2. Load the Images with a generator and Data Augmentation<\/h3>](#2)\n\n[<h3>3. Test 27 canned architectures with pre-trained weights<\/h3>](#3)\n\n[<h3>4. Train the best architecture<\/h3>](#4)\n\n[<h3>5. Examples of prediction<\/h3>](#5)\n\n","778839fa":"# 5. Examples of prediction<a class=\"anchor\" id=\"5\"><\/a><a class=\"anchor\" id=\"1\"><\/a>\n","8948efac":"# 2. Load the Images with a generator and Data Augmentation<a class=\"anchor\" id=\"2\"><\/a><a class=\"anchor\" id=\"1\"><\/a>","8a23f44f":"# Dogs&Cats classification with Deep LearningDogs&Cats classification with Deep Learning\n## *Find the best model*\n\n![Dogs and cats](https:\/\/i.imgur.com\/aVdtgUc.pnghttps:\/\/i.imgur.com\/aVdtgUc.png)\n                ","1f21ce96":"# 3. Test 27 canned architectures with pre-trained weights<a class=\"anchor\" id=\"3\"><\/a><a class=\"anchor\" id=\"1\"><\/a>\n\nMore info about the architectures under: [Module: tf.keras.applications](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications?hl=enhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications?hl=en)","a4fbf5c3":"# 4. Train the architecture with the best result (DenseNet201)<a class=\"anchor\" id=\"4\"><\/a><a class=\"anchor\" id=\"1\"><\/a>\n*With more than one epoch*","80b8988a":"# 1. Data preprocessing and visualization<a class=\"anchor\" id=\"1\"><\/a><a class=\"anchor\" id=\"1\"><\/a>"}}