{"cell_type":{"5530d360":"code","d94216d8":"code","50d571db":"code","8440aec6":"code","1edae44e":"code","1e676298":"code","e11b3db8":"code","61d5d6a4":"markdown","6a39f243":"markdown","7ab7e19d":"markdown","9e02ee3c":"markdown","be3b397b":"markdown","7b2fcc31":"markdown","c003b89b":"markdown","e67588bb":"markdown","be11ec2f":"markdown","331d8d7f":"markdown","58da4cc3":"markdown","6eda3a42":"markdown","2fbd67ec":"markdown","ce4e774a":"markdown","6d63fb90":"markdown"},"source":{"5530d360":"# from sklearn.neighbors import KNeighborsClassifier\n# clf = KNeighborsClassifier(n_neighbors=3)","d94216d8":"# clf.fit(X_train, y_train)","50d571db":"# print(\"Test set predictions: {}\".format(clf.predict(X_test)))\n","8440aec6":"# print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))","1edae44e":"!pip install mglearn\n!pip install sklearn\nfrom sklearn.neighbors import KNeighborsRegressor\nimport mglearn\nfrom sklearn.model_selection import train_test_split\nX, y = mglearn.datasets.make_wave(n_samples=40)\n# split the wave dataset into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate the model and set the number of neighbors to consider to 3\nreg = KNeighborsRegressor(n_neighbors=3)\n# fit the model using the training data and training targets\nreg.fit(X_train, y_train)\n","1e676298":"print(\"Test set predictions:\\n{}\".format(reg.predict(X_test)))\n","e11b3db8":"print(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))\n","61d5d6a4":"In its simplest version, the k-NN algorithm only considers exactly one nearest neighbor, which is\nthe closest training data point to the point we want to make a prediction for. The prediction is\nthen simply the known output for this training point.","6a39f243":"To make predictions on the test data, we call the predict method. For each data point in the test\nset, this computes its nearest neighbors in the training set and finds the most common class\namong these:","7ab7e19d":"For this notebook, we are going to use Python 3.x and Scikit-learn (sklearn) which is a tool for\nmachine learning.","9e02ee3c":"To evaluate how well our model generalizes, we can call the score method with the\ntest data together with the test labels:","be3b397b":"**k-Neighbors regression:**","7b2fcc31":"In this lab we will focus on understanding k-neighbor classifier.","c003b89b":"**k-Neighbors classification**","e67588bb":"The k-NN algorithm is arguably the simplest machine learning algorithm. Building the model\nconsists only of storing the training dataset. To make a prediction for a new data point, the\nalgorithm finds the closest data points in the training dataset\u2014its \u201cnearest neighbors.\u201d\nTo build a Na\u00efve Bayes machine learning classifier model, we need the following:\n","be11ec2f":"**Objective(s):**","331d8d7f":"**Tools:**","58da4cc3":"Now we can make predictions on the test set:","6eda3a42":"We can also evaluate the model using the score method, which for regressors returns the R2\nscore. The R2 score, also known as the coefficient of determination, is a measure of goodness of\na prediction for a regression model, and yields a score between 0 and 1. A value of 1 corresponds\nto a perfect prediction, and a value of 0 corresponds to a constant model that just predicts the\nmean of the training set responses, y_train:\n","2fbd67ec":"Now, we fit the classifier using the training set. For KNeighborsClassifier this means storing the\ndataset, so we can compute neighbors during prediction:","ce4e774a":"The k-nearest neighbors algorithm for regression is implemented in the KNeighbors regressor\nclass in scikit-learn. It\u2019s used similarly to KNeighborsClassifier:\n","6d63fb90":"# k-Nearest Neighbors\n"}}