{"cell_type":{"b072bd0c":"code","2c991641":"code","d13a88ea":"code","1a25d451":"code","fe13d43c":"code","caa80e24":"code","85ddae82":"code","7b4acac5":"code","fbdd85cc":"code","ad020a0d":"code","d7423442":"code","dff2b735":"code","0a698459":"code","2b3fb8c4":"code","18a0a615":"code","78fbcd55":"code","0142d900":"markdown","2bce193c":"markdown","87cd30c0":"markdown","d1ef4f21":"markdown","8de70cfa":"markdown","cba36245":"markdown","5e3ed4c5":"markdown","c60eaf62":"markdown","1018d972":"markdown","577b9a20":"markdown","c29336ac":"markdown"},"source":{"b072bd0c":"import nltk\nfrom nltk.corpus import stopwords\nimport string\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report,confusion_matrix","2c991641":"messages = pd.read_csv('..\/input\/Email.csv', encoding='latin-1')\nmessages.head()","d13a88ea":"\nmessages.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\nmessages = messages.rename(columns={'v1': 'class','v2': 'text'})","1a25d451":"messages.head()","fe13d43c":"messages.groupby('class').describe()","caa80e24":"messages['length'] = messages['text'].apply(len)","85ddae82":"messages.hist(column='length',by='class',bins=60, figsize=(15,5))","7b4acac5":"def process_text(text):\n    '''\n    What will be covered:\n    1. Remove punctuation\n    2. Remove stopwords\n    3. Return list of clean text words\n    '''\n    \n    #1\n    nopunc = [char for char in text if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    \n    #2\n    clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    \n    #3\n    return clean_words","fbdd85cc":"messages['text'].apply(process_text).head()","ad020a0d":"msg_train, msg_test, class_train, class_test = train_test_split(messages['text'],messages['class'],test_size=0.2)","d7423442":"pipeline = Pipeline([\n    ('bow',CountVectorizer(analyzer=process_text)), # converts strings to integer counts\n    ('tfidf',TfidfTransformer()), # converts integer counts to weighted TF-IDF scores\n    ('classifier',MultinomialNB()) # train on TF-IDF vectors with Naive Bayes classifier\n])","dff2b735":"pipeline.fit(msg_train,class_train)","0a698459":"predictions = pipeline.predict(msg_test)","2b3fb8c4":"print(classification_report(class_test,predictions))","18a0a615":"import seaborn as sns\nsns.heatmap(confusion_matrix(class_test,predictions),annot=True)","78fbcd55":"from sklearn.metrics import accuracy_score\n\nprint('Accuracy: %.5f' % accuracy_score(class_test,predictions))","0142d900":"**Notes:**\n* we got fairly high but not good enough prediction result here, maybe if the dataset gets higher, maybe naive bayes will do its work better\n\nthanks :)","2bce193c":"**PART 3: SPLITTING DATASET**","87cd30c0":"**PART 6: TESTING**","d1ef4f21":"from above information, we know that:\n1. only about 15% of the text messages is classified as a spam\n2. there are some duplicate messages, since the number of unique values lower than the count values of the text\n\nin the next part, lext check the length of each text messages to see whether it is correlated with the text classified as a spam or not.","8de70cfa":"let's check what above code will produce","cba36245":"**PART 2: CREATE TOKENIZER**","5e3ed4c5":"**INTRODUCTION**\n\nHi, in this notebook, I'll just doing exercise of using nltk and naive bayes classification algorithm to do a *very simple Spam\/Ham* Classification from SMS dataset from UCI","c60eaf62":"**PART 4: DATA PREPROCESSING**\n\nwait, we've just created the tokenizer isn't it? let the pipeline do the rest.","1018d972":"from above figure, we can see that most of ham (or not spam) messages only have length under 200 (100 to be exact) while spam messages tend to have higher lentgh above 130 or 140 approximately.","577b9a20":"**PART 5: MODEL CREATION**\n\nhere I'll just use pipeline in order to minimize effort on doing preprocessing, transforming then training data on both training dataset and test dataset. Using pipeline will handle them all in a few lines of codes.","c29336ac":"**PART 1: DATA PREPROCESSING**\n\nsince the dataset comes with additional unnamed, column, I need to drop them first"}}