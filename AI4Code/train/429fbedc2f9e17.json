{"cell_type":{"3cb07a3e":"code","23206033":"code","09fa41f3":"code","78b7d41d":"code","aa915b8d":"code","4cae4120":"code","b4db43a5":"code","5bc576d6":"code","50e1e3fd":"code","d06e5059":"code","b96340ea":"code","d4f4c902":"code","504f548b":"code","a92f57ce":"code","94477174":"code","854a3dd4":"code","63888d9e":"code","f6162766":"code","b4bba9bd":"code","16f5a9be":"code","01d73420":"code","c9fdf7ff":"code","4136a1b5":"code","404275aa":"code","f35c072c":"code","b5c85a89":"code","b50e4762":"code","37ba6702":"code","5c0515de":"code","14f49ddf":"code","a520bbd3":"code","5391dc36":"code","8cce928b":"code","61867352":"code","28135b6e":"markdown","6c286e53":"markdown","680a9170":"markdown","95bbca86":"markdown","7e9b6933":"markdown","421a41f1":"markdown","84249def":"markdown","449d1fe1":"markdown","e9b401d9":"markdown","f110dd1b":"markdown","f035f49a":"markdown","0739385f":"markdown","bc615267":"markdown","cfd82701":"markdown","11582bcd":"markdown"},"source":{"3cb07a3e":"%config Completer.use_jedi = False","23206033":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\n\nwarnings.filterwarnings(\"ignore\")\nNUM_WORKERS = 4","09fa41f3":"DATA_PATH = \"\/kaggle\/input\/ventilator-pressure-prediction\/\"\n\nsub = pd.read_csv(DATA_PATH + 'sample_submission.csv')\ndf_train = pd.read_csv(DATA_PATH + 'train.csv')\ndf_test = pd.read_csv(DATA_PATH + 'test.csv')\n\n\ndf = df_train[df_train['breath_id'] < 50].reset_index(drop=True)\ndf_train.shape","78b7d41d":"df_train.head()","aa915b8d":"df_train['breath_id'] = (df_train['id']-1)\/\/80 +1","4cae4120":"df_train.breath_id.max()","b4db43a5":"df1_train = df_train.groupby('breath_id').min()\nerror_breath_ids = df1_train[df1_train['pressure'] <= 0].index\nerror_breath_ids","5bc576d6":"# Norm pressure --> 0\nfor i in error_breath_ids:\n    min_pressure = df1_train[df1_train.index == i]['pressure'].values\n    df_train['pressure'][df_train['breath_id']== i]= df_train['pressure'][df_train['breath_id'] ==i] - min_pressure +0.001","50e1e3fd":"df.head()","d06e5059":"df_train[df_train['breath_id'] ==360] # check norm ok ?","b96340ea":"df_train.info()","d4f4c902":"df_train.isnull().sum()","504f548b":"df_train.boxplot('u_in')","a92f57ce":"df_train.boxplot('pressure')","94477174":"df_train.boxplot('R')","854a3dd4":"df_train.boxplot('C')","63888d9e":"def plot_sample(sample_id, df):\n    df_breath = df[df['breath_id'] == sample_id]\n    r, c  = df_breath[['R', 'C']].values[0]\n\n    cols = ['u_in', 'u_out', 'pressure'] if 'pressure' in df.columns else ['u_in', 'u_out']\n    \n    plt.figure(figsize=(12, 4))\n    for col in ['u_in', 'u_out', 'pressure']:\n        plt.plot(df_breath['time_step'], df_breath[col], label=col)\n        \n    plt.legend()\n    plt.title(f'Sample {sample_id} - R={r}, C={c}')","f6162766":"# plot frist 50 breath_ids\n# for i in df['breath_id'].unique():\n#     plot_sample(i, df_train)","b4bba9bd":"import torch\nfrom torch.utils.data import Dataset\n\nclass VentilatorDataset(Dataset):\n    def __init__(self, df):\n        if \"pressure\" not in df.columns:\n            df['pressure'] = 0\n\n        self.df = df.groupby('breath_id').agg(list).reset_index()\n        self.prepare_data()\n                \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def prepare_data(self):\n        self.pressures = np.array(self.df['pressure'].values.tolist())\n        \n        rs = np.array(self.df['R'].values.tolist())\n        cs = np.array(self.df['C'].values.tolist())\n        u_ins = np.array(self.df['u_in'].values.tolist())\n        \n        self.u_outs = np.array(self.df['u_out'].values.tolist())\n        \n        self.inputs = np.concatenate([\n            rs[:, None], \n            cs[:, None], \n            u_ins[:, None], \n#             np.cumsum(u_ins, 1)[:, None], FIXME\n            self.u_outs[:, None]\n        ], 1).transpose(0, 2, 1)\n\n    def __getitem__(self, idx):\n        data = {\n            \"input\": torch.tensor(self.inputs[idx], dtype=torch.float),\n            \"u_out\": torch.tensor(self.u_outs[idx], dtype=torch.float),\n            \"p\": torch.tensor(self.pressures[idx], dtype=torch.float),\n        }\n        \n        return data","16f5a9be":"dataset = VentilatorDataset(df)\ndataset[0]","01d73420":"import torch\nimport torch.nn as nn\n\n\nclass RNNModel(nn.Module):\n    def __init__(\n        self,\n        input_dim=4,\n        lstm_dim=256,\n        dense_dim=256,\n        logit_dim=256,\n        num_classes=1,\n    ):\n        super().__init__()\n\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, dense_dim \/\/ 2),\n            nn.ReLU(),\n            nn.Linear(dense_dim \/\/ 2, dense_dim),\n            nn.ReLU(),\n        )\n\n        self.lstm = nn.LSTM(dense_dim, lstm_dim, batch_first=True, bidirectional=True)\n\n        self.logits = nn.Sequential(\n            nn.Linear(lstm_dim * 2, logit_dim),\n            nn.ReLU(),\n            nn.Linear(logit_dim, num_classes),\n        )\n\n    def forward(self, x):\n        features = self.mlp(x)\n        features, _ = self.lstm(features)\n        pred = self.logits(features)\n        return pred","c9fdf7ff":"import os\nimport torch\nimport random\nimport numpy as np\n\n\ndef seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results.\n\n    Args:\n        seed (int): Number of the seed.\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    \ndef count_parameters(model, all=False):\n    \"\"\"\n    Counts the parameters of a model.\n\n    Args:\n        model (torch model): Model to count the parameters of.\n        all (bool, optional):  Whether to count not trainable parameters. Defaults to False.\n\n    Returns:\n        int: Number of parameters.\n    \"\"\"\n    if all:\n        return sum(p.numel() for p in model.parameters())\n    else:\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    \ndef worker_init_fn(worker_id):\n    \"\"\"\n    Handles PyTorch x Numpy seeding issues.\n\n    Args:\n        worker_id (int): Id of the worker.\n    \"\"\"\n    np.random.seed(np.random.get_state()[1][0] + worker_id)\n    \n\ndef save_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n    \"\"\"\n    Saves the weights of a PyTorch model.\n\n    Args:\n        model (torch model): Model to save the weights of.\n        filename (str): Name of the checkpoint.\n        verbose (int, optional): Whether to display infos. Defaults to 1.\n        cp_folder (str, optional): Folder to save to. Defaults to \"\".\n    \"\"\"\n    if verbose:\n        print(f\"\\n -> Saving weights to {os.path.join(cp_folder, filename)}\\n\")\n    torch.save(model.state_dict(), os.path.join(cp_folder, filename))","4136a1b5":"def compute_metric(df, preds):\n    \"\"\"\n    Metric for the problem, as I understood it.\n    \"\"\"\n    \n    y = np.array(df['pressure'].values.tolist())\n    w = 1 - np.array(df['u_out'].values.tolist())\n    \n    assert y.shape == preds.shape and w.shape == y.shape, (y.shape, preds.shape, w.shape)\n    \n    mae = w * np.abs(y - preds)\n    mae = mae.sum() \/ w.sum()\n    \n    return mae\n\n\nclass VentilatorLoss(nn.Module):\n    \"\"\"\n    Directly optimizes the competition metric\n    \"\"\"\n    def __call__(self, preds, y, u_out):\n        w = 1 - u_out\n        mae = w * (y - preds).abs()\n        mae = mae.sum(-1) \/ w.sum(-1)\n\n        return mae","404275aa":"import gc\nimport time\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup\n\n\ndef fit(\n    model,\n    train_dataset,\n    val_dataset,\n    loss_name=\"L1Loss\",\n    optimizer=\"Adam\",\n    epochs=50,\n    batch_size=32,\n    val_bs=32,\n    warmup_prop=0.1,\n    lr=1e-3,\n    num_classes=1,\n    verbose=1,\n    first_epoch_eval=0,\n    device=\"cuda\"\n):\n    avg_val_loss = 0.\n\n    # Optimizer\n    optimizer = getattr(torch.optim, optimizer)(model.parameters(), lr=lr)\n\n    # Data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n        worker_init_fn=worker_init_fn\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=val_bs,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n\n    # Loss\n#     loss_fct = getattr(torch.nn, loss_name)(reduction=\"none\")\n    loss_fct = VentilatorLoss()\n\n    # Scheduler\n    num_warmup_steps = int(warmup_prop * epochs * len(train_loader))\n    num_training_steps = int(epochs * len(train_loader))\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps, num_training_steps\n    )\n\n    for epoch in range(epochs):\n        model.train()\n        model.zero_grad()\n        start_time = time.time()\n\n        avg_loss = 0\n        for data in train_loader:\n            pred = model(data['input'].to(device)).squeeze(-1)\n\n            loss = loss_fct(\n                pred,\n                data['p'].to(device),\n                data['u_out'].to(device),\n            ).mean()\n            loss.backward()\n            avg_loss += loss.item() \/ len(train_loader)\n\n            optimizer.step()\n            scheduler.step()\n\n            for param in model.parameters():\n                param.grad = None\n\n        model.eval()\n        mae, avg_val_loss = 0, 0\n        preds = []\n\n        with torch.no_grad():\n            for data in val_loader:\n                pred = model(data['input'].to(device)).squeeze(-1)\n\n                loss = loss_fct(\n                    pred.detach(), \n                    data['p'].to(device),\n                    data['u_out'].to(device),\n                ).mean()\n                avg_val_loss += loss.item() \/ len(val_loader)\n\n                preds.append(pred.detach().cpu().numpy())\n        \n        preds = np.concatenate(preds, 0)\n        mae = compute_metric(val_dataset.df, preds)\n\n        elapsed_time = time.time() - start_time\n        if (epoch + 1) % verbose == 0:\n            elapsed_time = elapsed_time * verbose\n            lr = scheduler.get_last_lr()[0]\n            print(\n                f\"Epoch {epoch + 1:02d}\/{epochs:02d} \\t lr={lr:.1e}\\t t={elapsed_time:.0f}s \\t\"\n                f\"loss={avg_loss:.3f}\",\n                end=\"\\t\",\n            )\n\n            if (epoch + 1 >= first_epoch_eval) or (epoch + 1 == epochs):\n                print(f\"val_loss={avg_val_loss:.3f}\\tmae={mae:.3f}\")\n            else:\n                print(\"\")\n\n    del (val_loader, train_loader, loss, data, pred)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return preds\n","f35c072c":"def predict(\n    model,\n    dataset,\n    batch_size=64,\n    device=\"cuda\"\n):\n    \"\"\"\n    Usual torch predict function. Supports sigmoid and softmax activations.\n    Args:\n        model (torch model): Model to predict with.\n        dataset (PathologyDataset): Dataset to predict on.\n        batch_size (int, optional): Batch size. Defaults to 64.\n        device (str, optional): Device for torch. Defaults to \"cuda\".\n\n    Returns:\n        numpy array [len(dataset) x num_classes]: Predictions.\n    \"\"\"\n    model.eval()\n\n    loader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n    )\n    \n    preds = []\n    with torch.no_grad():\n        for data in loader:\n            pred = model(data['input'].to(device)).squeeze(-1)\n            preds.append(pred.detach().cpu().numpy())\n\n    preds = np.concatenate(preds, 0)\n    return preds","b5c85a89":"def train(config, df_train, df_val, df_test, fold):\n    \"\"\"\n    Trains and validate a model.\n\n    Args:\n        config (Config): Parameters.\n        df_train (pandas dataframe): Training metadata.\n        df_val (pandas dataframe): Validation metadata.\n        df_test (pandas dataframe): Test metadata.\n        fold (int): Selected fold.\n\n    Returns:\n        np array: Study validation predictions.\n    \"\"\"\n\n    seed_everything(config.seed)\n\n    model = RNNModel(\n        input_dim=config.input_dim,\n        lstm_dim=config.lstm_dim,\n        dense_dim=config.dense_dim,\n        logit_dim=config.logit_dim,\n        num_classes=config.num_classes,\n    ).to(config.device)\n    model.zero_grad()\n\n    train_dataset = VentilatorDataset(df_train)\n    val_dataset = VentilatorDataset(df_val)\n    test_dataset = VentilatorDataset(df_test)\n\n    n_parameters = count_parameters(model)\n\n    print(f\"    -> {len(train_dataset)} training breathes\")\n    print(f\"    -> {len(val_dataset)} validation breathes\")\n    print(f\"    -> {n_parameters} trainable parameters\\n\")\n\n    pred_val = fit(\n        model,\n        train_dataset,\n        val_dataset,\n        loss_name=config.loss,\n        optimizer=config.optimizer,\n        epochs=config.epochs,\n        batch_size=config.batch_size,\n        val_bs=config.val_bs,\n        lr=config.lr,\n        warmup_prop=config.warmup_prop,\n        verbose=config.verbose,\n        first_epoch_eval=config.first_epoch_eval,\n        device=config.device,\n    )\n    \n    pred_test = predict(\n        model, \n        test_dataset, \n        batch_size=config.val_bs, \n        device=config.device\n    )\n\n    if config.save_weights:\n        save_model_weights(\n            model,\n            f\"{config.selected_model}_{fold}.pt\",\n            cp_folder=\"\",\n        )\n\n    del (model, train_dataset, val_dataset, test_dataset)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return pred_val, pred_test","b50e4762":"from sklearn.model_selection import GroupKFold\n\ndef k_fold(config, df, df_test):\n    \"\"\"\n    Performs a patient grouped k-fold cross validation.\n    \"\"\"\n\n    pred_oof = np.zeros(len(df))\n    preds_test = []\n    \n    gkf = GroupKFold(n_splits=config.k)\n    splits = list(gkf.split(X=df, y=df, groups=df[\"breath_id\"]))\n\n    for i, (train_idx, val_idx) in enumerate(splits):\n        if i in config.selected_folds:\n            print(f\"\\n-------------   Fold {i + 1} \/ {config.k}  -------------\\n\")\n\n            df_train = df.iloc[train_idx].copy().reset_index(drop=True)\n            df_val = df.iloc[val_idx].copy().reset_index(drop=True)\n\n            pred_val, pred_test = train(config, df_train, df_val, df_test, i)\n            \n            pred_oof[val_idx] = pred_val.flatten()\n            preds_test.append(pred_test.flatten())\n\n    print(f'\\n -> CV MAE : {compute_metric(df, pred_oof) :.3f}')\n\n    return pred_oof, np.mean(preds_test, 0)","37ba6702":"class Config:\n    \"\"\"\n    Parameters used for training\n    \"\"\"\n    # General\n    seed = 42\n    verbose = 1\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    save_weights = True\n\n    # k-fold\n    k = 5\n    selected_folds = [0, 1, 2, 3, 4]\n\n    # Model\n    selected_model = 'rnn'\n    input_dim = 4\n\n    dense_dim = 128\n    lstm_dim = 128\n    logit_dim = 128\n    num_classes = 1\n\n    # Training\n    loss = \"L1Loss\"  # not used\n    optimizer = \"Adam\"\n    batch_size = 128\n    epochs = 200\n\n    lr = 1e-3\n    warmup_prop = 0\n\n    val_bs = 256\n    first_epoch_eval = 0","5c0515de":"pred_oof, pred_test = k_fold(\n    Config, \n    df_train,\n    df_test,\n)","14f49ddf":"def plot_prediction(sample_id, df):\n    df_breath = df[df['breath_id'] == sample_id]\n\n    cols = ['u_in', 'u_out', 'pressure'] if 'pressure' in df.columns else ['u_in', 'u_out']\n    \n    plt.figure(figsize=(12, 4))\n    for col in ['pred', 'pressure', 'u_out']:\n        plt.plot(df_breath['time_step'], df_breath[col], label=col)\n        \n    metric = compute_metric(df_breath, df_breath['pred'])\n        \n    plt.legend()\n    plt.title(f'Sample {sample_id} - MAE={metric:.3f}')","a520bbd3":"df_train[\"pred\"] = pred_oof","5391dc36":"for i in df_train['breath_id'].unique()[:5]:\n    plot_prediction(i, df_train)","8cce928b":"df_test['pred'] = pred_test\n\nfor i in df_test['breath_id'].unique()[:5]:\n    plot_prediction(i, df_test)","61867352":"sub['pressure'] = pred_test\nsub.to_csv('submission.csv', index=False)","28135b6e":"### Viz","6c286e53":"### Dataset","680a9170":"### Utils","95bbca86":"### $k$-fold","7e9b6933":"## Main","421a41f1":"## Train","84249def":"### Load","449d1fe1":"## Training","e9b401d9":"### Fit","f110dd1b":"### Predictions","f035f49a":"## Model\n- 2 Layer MLP\n- Bidirectional LSTM\n- Prediction dense layer","0739385f":"### Metric & Loss\n> The competition will be scored as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath. The expiratory phase is not scored.","bc615267":"## Data","cfd82701":"## Sub","11582bcd":"### Predict"}}