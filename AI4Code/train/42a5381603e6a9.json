{"cell_type":{"0e7cf7fc":"code","3be6bc2c":"code","2ce2329e":"code","e75655f2":"code","c3f5d27c":"code","fcc392bb":"code","a0ff7613":"code","ba8de430":"code","8164e905":"code","072d3663":"code","d3833505":"code","96c5df9a":"code","0ccb44e9":"code","a8158631":"code","623543ba":"code","eba89034":"code","5b310222":"code","374a21e6":"code","992ce82a":"code","9d232459":"code","7c5fa74a":"code","8646d6b7":"code","4732f40f":"code","a31a2128":"code","d6fcd02f":"code","188cfeed":"code","05e34522":"code","2e227739":"markdown","3ff4e609":"markdown","7252ddee":"markdown","de0904cf":"markdown","71e441bb":"markdown","b5ad90f6":"markdown","0e49b691":"markdown","e3330644":"markdown","21f703b4":"markdown","acaedce9":"markdown","443e23fc":"markdown","3ef1df3b":"markdown","1251eb5b":"markdown","369cdf6b":"markdown","bd39bf3a":"markdown","4938c21b":"markdown","06a5b1e5":"markdown","9351b22b":"markdown","4fb2bc5e":"markdown","170f5435":"markdown","ada5b17b":"markdown","b89c729f":"markdown","eb4b4cb2":"markdown","57c934fb":"markdown","706595b5":"markdown","d2a27d67":"markdown","1094172c":"markdown","92c3739f":"markdown","d35c5bb1":"markdown","2553bb7f":"markdown","5704e9aa":"markdown","13b2d688":"markdown"},"source":{"0e7cf7fc":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Modules for data manipulation\nimport numpy as np\nimport pandas as pd\nimport re\n\n# Modules for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n# Tools for preprocessing input data\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\n# Tools for creating ngrams and vectorizing input data\nimport gensim\n\nfrom gensim.models import Word2Vec, Phrases\n\n\n\n# Tools for building a model\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing import text\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Tools for assessing the quality of model prediction\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n","3be6bc2c":"import nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')","2ce2329e":"dataset_path = '..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv'\ndataset= pd.read_csv(dataset_path)\ndataset.head()","e75655f2":"dataset.replace({'positive':1,'negative':0},inplace = True)\ndataset.head()","c3f5d27c":"\n#helper functions for lemmatizations\ndef penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n              'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n' \n\ndef clean_text(x):\n    \n    \n    \n    # remove html tags\n    regex = re.compile('<.*?>')\n    input =  re.sub(regex, '', x)\n\n    #remove punctuations, numbers.\n    input = re.sub('[!@#$%^&*()\\n_:><?\\-.{}|+-,;\"\"``~`\u2014]|[0-9]|\/|=|\\[\\]|\\[\\[\\]\\]',' ',input)\n    input = re.sub('[\u201c\u2019\\']','',input)   \n    \n    \n    #lemmatise \n    ls = list(wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in pos_tag(word_tokenize(input)))\n    \n    \n    #remove stopwords\n    return_str = ''\n    for word in ls:\n       #if word its a long word with single character eg.aaaaaa remove it \n        if word not in stop_dict and len(set(word)) > 2:\n            return_str +=word.lower() + \" \"\n       \n\n    \n    #lemmatize the text.\n    \n\n    return return_str\n\n\nwnl = WordNetLemmatizer()\n\nstop_dict = stopwords.words('english')\n\ntmp_sent  = \"AAAAAA <html> <h1> run <i>running<\/i> ban banned dancing dance 1 2 3  4   5 5  5 !@#$%^&*(){{:><<< MMM<>?PLOKIU}} <\/h1> <\/html>\"\n\n\nclean_text(tmp_sent)\n\n","fcc392bb":"dataset['review'] = dataset['review'].map(clean_text)","a0ff7613":"x  = dataset['review']\ny = dataset['sentiment']","ba8de430":"y.shape","8164e905":"\ntokenizer = text.Tokenizer(num_words=1000)\ntokenizer.fit_on_texts(['sample text'])\n\nmetrix = tokenizer.texts_to_matrix(['sample text'])\n\n","072d3663":"\ntokenizer = text.Tokenizer(num_words=1000)\ntokenizer.fit_on_texts(['sample text'])\n\nmetrix = tokenizer.texts_to_matrix(['sample text'],mode = 'tfidf')\n\n","d3833505":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(x)\nsequences = tokenizer.texts_to_sequences(x)","96c5df9a":"\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(x)\n\nsequences = tokenizer.texts_to_sequences(x)\n\n","0ccb44e9":"# !wget  http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n# !unzip glove.6B.zip    ","a8158631":"import numpy as np\nglove_embedding  = 'glove.6B.100d.txt'\n\nembeddings_index = {}\n\nfile =  open(glove_embedding,'r')\n    \n    \nfor line in file:\n    \n    word,embd = line.split(maxsplit = 1)\n  \n    embd = np.fromstring(embd,'f',sep = ' ')\n    \n    embeddings_index[word] = embd\n    \n    \n    \nfile.close()    \n    \n\n","623543ba":"embedding_matrix = np.zeros((max_word_size,100)) \n\nprint(embedding_matrix.shape)\n\n\n\n\nfor index,word in tokenizer.index_word.items():\n    \n    embd =  embeddings_index.get(word)\n    \n    if embd is not None:\n        embedding_matrix[index] = embd\n        \n        \n# embedding_layer = Embedding(vocab_size, 150, weights=[embedding_vectors], input_length=370, trainable=True)        ","eba89034":"bigrams = Phrases(data)\ntrigrams = Phrases(data)","5b310222":"word2vec_model = Word2Vec(\n    sentences = trigrams[data],\n    size = 300,\n    min_count=3, window=5, workers=4)","374a21e6":"sequences = []\nfor i in tqdm.tqdm(data):\n    sent = []\n    for word in i:\n        if word in word2vec_model.wv.vocab:\n            sent.append(word2vec_model.wv.vocab[word].index)\n    sequences.append(sent)    \n    ","992ce82a":"\n#replace your_sequences with the embedding matrix you choose form above.\nvocab_size = len(tokenizer.word_index) + 1\nX_pad =  pad_sequences(sequences,maxlen = 1000,padding = 'post',value = vocab_size - 1)","9d232459":"X_train, X_test, y_train, y_test = train_test_split(\n    X_pad,\n    y,\n    test_size=0.05,\n    shuffle=True,\n    random_state=42)","7c5fa74a":"model = Sequential()\nmodel.add(Embedding(input_dim=vocab_size,output_dim=128,input_length = 1000))\nmodel.add(Flatten())\nmodel.add(Dense(50,activation = 'relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer= 'adam',\n    metrics=['accuracy'])\n\nmodel.summary()\n\nprint(model.input_shape)\n\nmodel.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 1)\n\n","8646d6b7":"model_lstm = Sequential()\nmodel_lstm.add(Embedding(input_dim=vocab_size,output_dim=128,input_length = 1000))\nmodel_lstm.add(LSTM(60, return_sequences = True))\nmodel_lstm.add(GlobalMaxPool1D())\n# model_lstm.add(Flatten())\nmodel_lstm.add(Dropout(0.1))\nmodel_lstm.add(Dense(50,activation = 'relu'))\nmodel_lstm.add(Dropout(0.1))\nmodel_lstm.add(Dense(1, activation='sigmoid'))\n\nmodel_lstm.compile(\n    loss=\"binary_crossentropy\",\n    optimizer= 'adam',\n    metrics=['accuracy'])\n\nmodel_lstm.summary()\n\nprint(model_lstm.input_shape)\n\nmodel_lstm.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 1)\n\n","4732f40f":"model_cnn = Sequential([\n    \n    Embedding(input_dim=vocab_size,output_dim=128,input_length = 1000),\n    Conv1D(16,8,activation = 'relu'),\n    Dropout(0.5),\n    MaxPool1D(2),\n    Flatten(),\n    Dropout(0.5),\n    Dense(64,activation = 'relu'),\n    Dense(1,activation = 'sigmoid')\n    ])\n\n\nmodel_cnn.compile(\n    loss=\"binary_crossentropy\",\n    optimizer= 'adam',\n    metrics=['accuracy'])\n\nmodel_cnn.summary()\n\nprint(model_cnn.input_shape)\n\nmodel_cnn.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 1)\n\n","a31a2128":"model_cnn_lstm =  Sequential([\n    Embedding(input_dim=vocab_size,output_dim=128,input_length = 1000),\n    Conv1D(16,5,activation = 'relu',padding = 'same',strides = 1),\n    MaxPool1D(2),\n    LSTM(64,name = 'lstm_1'),\n    Dropout(0.7),\n    Dense(1,activation = 'sigmoid')\n    ])\n\n\nmodel_cnn_lstm.compile(\n    loss=\"binary_crossentropy\",\n    optimizer= 'adam',\n    metrics=['accuracy'])\n\n\n# model.fit()\nmodel_cnn_lstm.summary()\n# model_cnn_lstm.summary()\n# print(model_cnn_lstm.get_layer('lstm_1').input_shape )\n\n\nmodel_cnn_lstm.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 1)\n\n","d6fcd02f":"import tensorflow as tf\n\nmodel_lstm = Sequential()\nmodel_lstm.add(Embedding(input_dim=vocab_size,output_dim=128,input_length = 1000))\nmodel_lstm.add(tf.compat.v1.keras.layers.CuDNNLSTM(60, return_sequences = True))\nmodel_lstm.add(GlobalMaxPool1D())\n# model_lstm.add(Flatten())\nmodel_lstm.add(Dropout(0.1))\nmodel_lstm.add(Dense(50,activation = 'relu'))\nmodel_lstm.add(Dropout(0.1))\nmodel_lstm.add(Dense(1, activation='sigmoid'))\n\nmodel_lstm.compile(\n    loss=\"binary_crossentropy\",\n    optimizer= 'adam',\n    metrics=['accuracy'])\n\nmodel_lstm.summary()\n\nprint(model_lstm.input_shape)\n\nmodel_lstm.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 1)\n\n","188cfeed":"# def plot_confusion_matrix(y_true, y_pred, ax, class_names, vmax=None,\n#                           normed=True, title='Confusion matrix'):\n#     matrix = confusion_matrix(y_true,y_pred)\n#     if normed:\n#         matrix = matrix.astype('float') \/ matrix.sum(axis=1)[:, np.newaxis]\n#     sb.heatmap(matrix, vmax=vmax, annot=True, square=True, ax=ax,\n#                cmap=plt.cm.Blues_r, cbar=False, linecolor='black',\n#                linewidths=1, xticklabels=class_names)\n#     ax.set_title(title, y=1.20, fontsize=16)\n#     #ax.set_ylabel('True labels', fontsize=12)\n#     ax.set_xlabel('Predicted labels', y=1.10, fontsize=12)\n#     ax.set_yticklabels(class_names, rotation=0)","05e34522":"# fig, axis1 = plt.subplots(nrows=1, ncols=1)\n# plot_confusion_matrix([1,1,1,1,0,0], [1,1,0,1,0,0], ax=axis1,\n#                       title='Confusion matrix (train data)',\n#                       class_names=['Positive', 'Negative'])","2e227739":"# model 3 with CNN Layer ","3ff4e609":"## This notebook is intended to be used as a Template notebook where one can easily load their dataset  apply preprocessing, decide on the type of embedding to use , The type of model in an instant. Feel free to download it modify it according to your need.\n","7252ddee":"# 4 Using Glove Model","de0904cf":"### Please upvote this kernel if you like it. It motivates me to produce more quality content :)","71e441bb":"# Model  1 Simple model with Embed + Dense Layers","b5ad90f6":"# Padding ","0e49b691":"# Evaluation Matrix","e3330644":"## Traning The model","21f703b4":"# 5 Using Word2Vec (traning a custom wor2vec model)","acaedce9":"## 4.2Load pretrained vectors","443e23fc":"# Preprocessig Part\n## incldues\n<pre>\n1. removing stopwords \n2. lowercasing \n3. removing words with len < 2\n4. removing html tags\n5. removing punctuations\n6. removing digits\n7. Lemmatization\n\n\n","3ef1df3b":"# DeepLearning Models","1251eb5b":"# 3 Converting TO sequences  (uses the embdeing layer)","369cdf6b":"## 1 BagOfWords (countVectorizer) ","bd39bf3a":"# DeepLearning Models Implemented include\n1. Simple Dense + EMbedding layer\n2. Lstm DNN\n3. CNN \n4. CNN + LSTM\n5. CuDNNLSTM (comming soon)\n6. with attention layer (comming soon)\n7. Bert (comming soon)","4938c21b":"# model 4 with cnn + lstm","06a5b1e5":"# The preprocessing part includes\n\n1. removing stopwords \n2. lowercasing \n3. removing words with len < 2\n4. removing html tags\n5. removing punctuations\n6. removing digits\n7. Lemmatization\n\n\n","9351b22b":"# 2 Using Tfidf","4fb2bc5e":"# Apply The clean_text function over here","170f5435":"# DL Model using CuDNNLSTM ","ada5b17b":"# Embeddings Implemented\n1. Bagofwords \n2. Tfidf\n3. wor2vec\n4. glove \n5. tensorlfow hub ( comming soon)\n6. The Embedding layer ","b89c729f":"## Using Bigrams and Trigrams","eb4b4cb2":"## 4.3Prepare the weight Matrix","57c934fb":"# Demo \nFor the demo I have used the Imbd dataset that contains 50K reviews \nFor embeddings I have used option 3 which is using converting tokens to sequences and using   embedding layer to produce the embedding vectors.\n\n\n","706595b5":"# Model 2 with Lstm Layer","d2a27d67":"Note:- remember to set the trainable parameter in the embedding layer = False ","1094172c":"## 4.0 Need to create Sequences","92c3739f":"## 4.1Download Pre-traied weights","d35c5bb1":"# Imports  \n","2553bb7f":"# load your dataset","5704e9aa":"# Different Embedding Types ","13b2d688":"## Preparing the embedding matrix "}}