{"cell_type":{"445db606":"code","6041826b":"code","5af43a2d":"code","a5ab01c5":"code","ef354b49":"code","90e90cc5":"code","6e08d256":"code","44dc4164":"code","3e99b4b8":"code","2f56665f":"code","6545175a":"code","2b5e6fb3":"markdown","8d4c4682":"markdown","4a8dbe3c":"markdown","1a6bcfc3":"markdown","4553d6f3":"markdown","d793cfa9":"markdown","f21b54cd":"markdown","2dbc431e":"markdown","8f89de5a":"markdown","cec080a4":"markdown","b0500e7a":"markdown","f7291acc":"markdown","67bb237b":"markdown","637a5c06":"markdown"},"source":{"445db606":"!pip install line_profiler\n%load_ext line_profiler","6041826b":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom numba import njit\nfrom multiprocessing import Pool","5af43a2d":"train_targets = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/train.csv\")\ntrain_targets['row_id'] = train_targets['stock_id'].astype(str) + '-' + train_targets['time_id'].astype(str)\ntrain_targets = train_targets[['row_id','target']].set_index(\"row_id\")\ntrain_files = glob(\"..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*\")","a5ab01c5":"column_names = [\n    \"time_id\",           # 0\n    \"seconds_in_bucket\", # 1\n    \"bid_price1\",        # 2\n    \"ask_price1\",        # 3\n    \"bid_price2\",        # 4\n    \"ask_price2\",        # 5\n    \"bid_size1\",         # 6\n    \"ask_size1\",         # 7\n    \"bid_size2\",         # 8\n    \"ask_size2\"          # 9\n]","ef354b49":"@njit\ndef fill_array(book_data, filled_data):\n    filled_data[0] = book_data[0]\n    last_read_idx = 0\n    for row_idx in range(1, 600):\n        # print(row_idx, last_read_idx, int(book_data[last_read_idx + 1][1]), int(book_data[last_read_idx + 1][1]) == row_idx)\n        if int(book_data[last_read_idx + 1][1]) == row_idx:\n            last_read_idx += 1\n        filled_data[row_idx] = book_data[last_read_idx]\n        filled_data[row_idx][1] = row_idx","90e90cc5":"@njit\ndef calculate_features(filled_data):\n    filled_data = filled_data.transpose()\n    \n    trade_vols1 = filled_data[6] + filled_data[7]\n    trade_vols2 = filled_data[8] + filled_data[9]\n    trade_diffs1 = filled_data[7] - filled_data[6]\n    trade_diffs2 = filled_data[9] - filled_data[8]\n    \n    spreads1 = (filled_data[2] \/ filled_data[3]) - 1\n    spreads2 = (filled_data[4] \/ filled_data[5]) - 1\n    \n    waps1 = (filled_data[2] * filled_data[7] + filled_data[3] * filled_data[6]) \/ (filled_data[6] + filled_data[7])\n    waps2 = (filled_data[4] * filled_data[9] + filled_data[5] * filled_data[8]) \/ (filled_data[8] + filled_data[9])\n    \n    logs1 = np.diff(np.log(waps1))\n    logs2 = np.diff(np.log(waps2))\n    \n    return [\n        waps1.mean(), \n        waps2.mean(),\n        waps1[300:].mean(),\n        waps2[300:].mean(),\n        waps1.std(),\n        waps2.std(),\n        waps1[300:].std(),\n        waps2[300:].std(),\n        logs1.mean(),\n        logs2.mean(),\n        logs1[300:].mean(),\n        logs2[300:].mean(),\n        logs1.std(), # Essentially volatility1\n        logs2.std(), # Essentially volatility2\n        trade_vols1.mean(),\n        trade_vols2.mean(),\n        trade_vols1[300:].mean(),\n        trade_vols2[300:].mean(),\n        trade_diffs1.mean(),\n        trade_diffs2.mean(),\n        trade_diffs1[300:].mean(),\n        trade_diffs2[300:].mean(),\n        int(filled_data[0][0])\n    ]","6e08d256":"@njit\ndef process_groups(dataset, stock_id):\n    ret_lis = []\n    last_split_pos = 0\n    filled_data = np.zeros((600, 10), dtype=np.float32)\n    for split_pos in np.nonzero(np.diff(dataset[:,0]))[0]:\n        data_split = dataset[last_split_pos:split_pos]\n        fill_array(data_split, filled_data)\n        features = calculate_features(filled_data)\n        ret_lis.append(features + [stock_id])\n        last_split_pos = split_pos\n    data_split = dataset[last_split_pos:]\n    fill_array(data_split, filled_data)\n    features = calculate_features(filled_data)\n    ret_lis.append(features + [stock_id])\n    return ret_lis","44dc4164":"feature_columns = [\n    \"wap1\", \"wap2\", \"wap1l\", \"wap2l\", \"wap1_std\", \"wap2_std\", \"wap1l_std\", \"wap2l_std\", \"log1\", \"log2\", \"log1l\", \"log2l\", \"vol1\", \"vol2\",\n    \"volume1\", \"volume2\", \"volume1l\", \"volume2l\", \"diff1\", \"diff2\", \"diff1l\", \"diff2l\", \"time_id\", \"stock_id\"\n]","3e99b4b8":"def process_single_stock(file_path):\n    book = pd.read_parquet(file_path, engine=\"pyarrow\").to_numpy(dtype=np.float32)\n    group_features = process_groups(book, int(file_path.split('=')[1]))\n    return group_features","2f56665f":"def preprocess_data():\n    worker_pool = Pool(processes=None)\n    full_feature_list_matrix = worker_pool.map(process_single_stock, train_files)\n    worker_pool.close()\n    worker_pool.join()\n    return_feature_list = []\n    for feature_list in full_feature_list_matrix:\n        return_feature_list += feature_list\n        \n    return pd.DataFrame(return_feature_list, columns=feature_columns)","6545175a":"%timeit preprocess_data()","2b5e6fb3":"### Stock Processing function\n\nThis function essentially reads data from the parquet file of a single stock and calls the kernels in order to extract features.","8d4c4682":"### Data Structure\n\nThe feature arrays of the input data as in the provided dataset is as follows. Numerical access to data is the trade-off for faster computation of features.","4a8dbe3c":"### Final features names\n\nThis list contains the names of the columns that would be present in the final feature set. **Any new features added in the above kernels must be added here too in the exact order.**","1a6bcfc3":"## Loading Data\n\nThis pre-processing pipeline ignores the trade data completely and is optimized to work on book data alone. Features can be constructed based off of the order book data provided.","4553d6f3":"# Numba for Data Pre-processing and Feature Extraction\n\nThis notebook outlines a method of using numba for faster data pre-processing and feature extraction as compared to baseline pandas implementation. The notebook performs quite fast - check the last cell for `timeit` profile runs. My baseline pandas implementation took over 18 minutes to perform a similar pre-processing, while this notebook performs a slightly modified version of it in just *27.7 s \u00b1 878 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)*. **I hence attain a 36x speedup by using custom-built Numba kernels and pool processing**","d793cfa9":"### Data Processesor Main Handler\n\nHere we are using basic multi-processing of python to process multiple `stock_id`'s at the same time. This adds a slight speed up to the overall processing pipeline.\n\n**Note: The returned dataframe would have all the columns in `float32` format. Any type changes would have to be handled manually**","f21b54cd":"### The prefil kernel\n\nThis kernel fills in for places where the `seconds_in_bucket` parameter does not have an entry in the original dataset. In the FAQ, this is mentioned as durations or seconds when trading does not take place and the data for this should be the exact same as the last traded second's. This kernel fills in those missing seconds as well as forward fills the data.","2dbc431e":"### Per-`time-id` processor\n\nDuring experimentation it was found that compiling the code that essentially handles the processing of a single time_id of a stock_id was slightly faster than performing it directly in python. This could be primarly because of the fact that this piece of code runs ~3800 times per stock over ~120 stocks and compiling it once would help tremendously.","8f89de5a":"### Feature Computing kernel\n\nThis is the kernel where different features are computed. It will return a array of `float32`s containing features. Some generic features are being calculated. Any new features to be calculated should have its computation definition here based on the given dataset's columns directly or indirectly.","cec080a4":"## Processing data using the defined kernels","b0500e7a":"### Time test\n\nThis might be slightly slow for the very first time as the kernels have to compile. But would speed up for any subsequent runs. Even if that is not needed, the overall time is still lower compared to using just pandas.","f7291acc":"## Numba-based feature-engineering and data processing kernels","67bb237b":"__Edits (Final features names cell): Updated order of Volatility column names, this provided a wrong final dataframe labelling, which was detrimental to further data analysis__","637a5c06":"__Edits (Per-time-id proocessor Code Cell): A very stupid error on my part, I made some errors in the publish version, which I found later to be erroraneous. I tested them and updated them, but forgot to publish that version. I am sorry for any issues caused.__\n\n__The error: Due to the way I previously indexed split index calculations in line`for split_pos in np.nonzero(np.diff(dataset[:,0]))[0]`, overlooked one of the splits. Further, the last split was not being handled, so additional lines were added to handle this__\n\n*Thank you [Tobias Tesch](https:\/\/www.kaggle.com\/tobiit) for reminding me to push the latest version in the comments below.*"}}