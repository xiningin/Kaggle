{"cell_type":{"22969140":"code","38fde64b":"code","7fb7e4bb":"code","ce553936":"code","c3fe0932":"code","c7cd8b73":"code","1eeb9606":"code","da1eef6e":"code","4b02aeb3":"code","2dbdcd56":"code","fd858d5b":"code","d7082aa9":"code","42a9958e":"code","b1d03b17":"code","c20570fa":"code","ab697075":"markdown","ce3bc878":"markdown","777bd42f":"markdown","daa76b22":"markdown","15b45a1e":"markdown","7fefc4c4":"markdown","7464aec2":"markdown","4ffcb31e":"markdown"},"source":{"22969140":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","38fde64b":"import pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","7fb7e4bb":"data=pd.concat([train,test],sort=True)\n","ce553936":"data=data.drop([\"Alley\",\"YearBuilt\",\"YearRemodAdd\",\"GarageYrBlt\",\n            \"PoolQC\",\"Fence\",\"MiscFeature\",\"YrSold\"],axis=1)\n","c3fe0932":"mean_value=data['LotFrontage'].mean()\ndata['LotFrontage']=data['LotFrontage'].fillna(mean_value)\n\nmean_value=data['GarageArea'].mean()\ndata['GarageArea']=data['GarageArea'].fillna(mean_value)\n\nmean_value=data['TotalBsmtSF'].mean()\ndata['TotalBsmtSF']=data['TotalBsmtSF'].fillna(mean_value)\n\nmedian_value=data['MasVnrArea'].median()\ndata['MasVnrArea']=data['MasVnrArea'].fillna(median_value)\n\nmedian_value=data['BsmtFinSF1'].median()\ndata['BsmtFinSF1']=data['BsmtFinSF1'].fillna(median_value)\n\nmedian_value=data['BsmtFinSF2'].median()\ndata['BsmtFinSF2']=data['BsmtFinSF2'].fillna(median_value)\n\nmedian_value=data['BsmtUnfSF'].median()\ndata['BsmtUnfSF']=data['BsmtUnfSF'].fillna(median_value)\n\nmedian_value=data['SalePrice'].median()\ndata['SalePrice']=data['SalePrice'].fillna(median_value)","c7cd8b73":"education1=['Electrical','MSZoning','Utilities','Exterior1st','Exterior2nd','MasVnrType',\n            'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n           'BsmtFullBath','BsmtHalfBath','KitchenQual','Functional','FireplaceQu',\n           'GarageType','GarageCars','GarageFinish','GarageQual','GarageCond','SaleType']\n           \ndata[education1]=data[education1].fillna(data.mode().iloc[0])","1eeb9606":"data=pd.get_dummies(data,drop_first=True)\n","da1eef6e":"train1=data[0:1460]\ntest1=data[1460:2919]\n","4b02aeb3":"train1=train1.drop([\"Id\"],axis=1)\nX=train1.iloc[:,train1.columns!='SalePrice']\ny=train1.iloc[:,train1.columns=='SalePrice']\n","2dbdcd56":"test1=test1.drop([\"SalePrice\"],axis=1)\nTestID = test1[\"Id\"]\nId = pd.DataFrame(TestID)","fd858d5b":"test_final=test1.iloc[:,test1.columns!='Id']","d7082aa9":"from sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=2, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=500,\n                       n_jobs=None, oob_score=True, random_state=1112, verbose=0,\n                       warm_start=False)","42a9958e":"model_rf.fit( X, y )\ny_pred_rf=model_rf.predict(test_final)","b1d03b17":"y_pred_rf","c20570fa":"Id = Id['Id']\nSalePrice = y_pred_rf\n\nsubmit = pd.DataFrame({'Id':Id, 'SalePrice':SalePrice})","ab697075":"Some variable are meaning less or doesn't contain important information that why I drop them.","ce3bc878":"Training and testing data set containing categorical feature with missing values.But After making mode imputation and dumming to data it producess different number of features.This condition arises because training data set containing feature doesn't contain categories in training data set i.e in training data set feature X_10 containg \"A\",\"B\",\"C\",\"D\" categories but in testing data set containing feature X_10 has only \"A\",\"B\",\"C\" categories.While applying mode imputation on categorical feature of training data set will impute most frequent observation from 4 feature and applying dumming ,it will create 4 columns but in testing data set imputation will be select from most frequent from \"A\",\"B\",\"C\" and after dumming the X_10 feature contain only 3 columns which actually differ from training data set.To deal with such problem,I just concat training and testing data set, after that dummy data set seperate it .","777bd42f":"In training data set seperating Dependent and Independant variable(feature).","daa76b22":"Some Numeric variable(Feature) conatin less varition and some variable(Feature) having high variation in that.Less varition containg variable are imputed using mean imputation and high variation containing variable are imputed using median.","15b45a1e":"Categorical variable are imputed by mode impuatation.","7fefc4c4":"Training and testing data set are seperated that are allready concatted for imputation.","7464aec2":"Reading Data Set from kaggle.","4ffcb31e":"Random forest model."}}