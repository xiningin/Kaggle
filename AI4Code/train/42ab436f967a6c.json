{"cell_type":{"ad1022ec":"code","6bfc8aa8":"code","d07c5d99":"code","36ccc3d4":"code","357511f5":"code","4fd4a59e":"code","25a0f304":"code","39622171":"code","15563bc5":"code","b20be1fd":"code","316a63d5":"code","eacad21a":"code","1a18c56b":"code","9d2abfd3":"code","3934d10d":"code","aea79758":"code","f6619fa3":"code","570d0eae":"code","6a58b1a7":"code","99f10ed4":"code","2ca77f11":"code","73118c31":"code","332e7a83":"code","81887e39":"markdown","efb699db":"markdown","52cb9bd3":"markdown","7b4f493e":"markdown","886fb97d":"markdown","03213c4b":"markdown"},"source":{"ad1022ec":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader,TensorDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping","6bfc8aa8":"# needed for deterministic output\nSEED = 2\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\n\n# device in which the model will be trained\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","d07c5d99":"dataset = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")\ndataset","36ccc3d4":"# dataset split: train 60% - valid 20% - test 20%\n\nindex = np.array(dataset.index)\nnp.random.shuffle(index)\nn = len(index)\n\ntrain_index = index[0:int(0.6*n)]\nvalid_index = index[int(0.6*n):int(0.8*n)]\ntest_index = index[int(0.8*n):]\n\ntrain_dset = dataset.loc[train_index].reset_index(drop=True)\nvalid_dset = dataset.loc[valid_index].reset_index(drop=True)\ntest_dset = dataset.loc[test_index].reset_index(drop=True)","357511f5":"input_features = dataset.columns[2:].tolist()\ntarget = \"target\"","4fd4a59e":"# parsing inputs as pytorch tensor dataset\n\ntrain_tensor_dset = TensorDataset(\n    torch.tensor(train_dset[input_features].values, dtype=torch.float),\n    torch.tensor(train_dset[target].values.reshape(-1,1), dtype=torch.float)\n)\n\nvalid_tensor_dset = TensorDataset(\n    torch.tensor(valid_dset[input_features].values, dtype=torch.float),\n    torch.tensor(valid_dset[target].values.reshape(-1,1), dtype=torch.float)\n)\n\ntest_tensor_dset = TensorDataset(\n    torch.tensor(test_dset[input_features].values, dtype=torch.float),\n    torch.tensor(test_dset[target].values.reshape(-1,1), dtype=torch.float) \n)","25a0f304":"class DNN(pl.LightningModule):\n\n    def __init__(self, input_dim, output_dim, nn_depth, nn_width, dropout, momentum):\n        super().__init__()\n\n        self.bn_in = nn.BatchNorm1d(input_dim, momentum=momentum)\n        self.dp_in = nn.Dropout(dropout)\n        self.ln_in = nn.Linear(input_dim, nn_width, bias=False)\n\n        self.bnorms = nn.ModuleList([nn.BatchNorm1d(nn_width, momentum=momentum) for i in range(nn_depth-1)])\n        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for i in range(nn_depth-1)])\n        self.linears = nn.ModuleList([nn.Linear(nn_width, nn_width, bias=False) for i in range(nn_depth-1)])\n        \n        self.bn_out = nn.BatchNorm1d(nn_width, momentum=momentum)\n        self.dp_out = nn.Dropout(dropout\/2)\n        self.ln_out = nn.Linear(nn_width, output_dim, bias=False)\n\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        x = self.bn_in(x)\n        x = self.dp_in(x)\n        x = nn.functional.relu(self.ln_in(x))\n\n        for bn_layer,dp_layer,ln_layer in zip(self.bnorms,self.dropouts,self.linears):\n            x = bn_layer(x)\n            x = dp_layer(x)\n            x = ln_layer(x)\n            x = nn.functional.relu(x)\n            \n        x = self.bn_out(x)\n        x = self.dp_out(x)\n        x = self.ln_out(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('valid_loss', loss)\n        \n    def test_step(self, batch, batch_idx):\n        X, y = batch\n        y_logit = self.forward(X)\n        y_probs = torch.sigmoid(y_logit).detach().cpu().numpy()\n        loss = self.loss(y_logit, y)\n        metric = roc_auc_score(y.cpu().numpy(), y_probs)\n        self.log('test_loss', loss)\n        self.log('test_metric', metric)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=1e-2, momentum=0.9)\n        scheduler = {\n            'scheduler': ReduceLROnPlateau(\n                optimizer, \n                mode=\"min\", \n                factor=0.5, \n                patience=5, \n                min_lr=1e-5),\n            'interval': 'epoch',\n            'frequency': 1,\n            'reduce_on_plateau': True,\n            'monitor': 'valid_loss',\n        }\n        return [optimizer], [scheduler]","39622171":"model = DNN(\n    input_dim=len(input_features), \n    output_dim=1, \n    nn_depth=3, \n    nn_width=256, \n    dropout=0.2, \n    momentum=0.1\n)\n\nearly_stop_callback = EarlyStopping(\n   monitor='valid_loss',\n   min_delta=.0,\n   patience=20,\n   verbose=True,\n   mode='min'\n)\n\ntrainer = pl.Trainer(callbacks=[early_stop_callback], min_epochs=10, max_epochs=200, gpus=1)","15563bc5":"model.summarize()","b20be1fd":"trainer.fit(\n    model, \n    DataLoader(train_tensor_dset, batch_size=2048, shuffle=True, num_workers=4),\n    DataLoader(valid_tensor_dset, batch_size=2048, shuffle=False, num_workers=4)\n)","316a63d5":"# AUC on validation dataset\ntrainer.test(model, DataLoader(valid_tensor_dset, batch_size=2048, shuffle=False, num_workers=4))","eacad21a":"# AUC on test dataset\ntrainer.test(model, DataLoader(test_tensor_dset, batch_size=2048, shuffle=False, num_workers=4))","1a18c56b":"class SoftOrdering1DCNN(pl.LightningModule):\n\n    def __init__(self, input_dim, output_dim, sign_size=32, cha_input=16, cha_hidden=32, \n                 K=2, dropout_input=0.2, dropout_hidden=0.2, dropout_output=0.2):\n        super().__init__()\n\n        hidden_size = sign_size*cha_input\n        sign_size1 = sign_size\n        sign_size2 = sign_size\/\/2\n        output_size = (sign_size\/\/4) * cha_hidden\n\n        self.hidden_size = hidden_size\n        self.cha_input = cha_input\n        self.cha_hidden = cha_hidden\n        self.K = K\n        self.sign_size1 = sign_size1\n        self.sign_size2 = sign_size2\n        self.output_size = output_size\n        self.dropout_input = dropout_input\n        self.dropout_hidden = dropout_hidden\n        self.dropout_output = dropout_output\n\n        self.batch_norm1 = nn.BatchNorm1d(input_dim)\n        self.dropout1 = nn.Dropout(dropout_input)\n        dense1 = nn.Linear(input_dim, hidden_size, bias=False)\n        self.dense1 = nn.utils.weight_norm(dense1)\n\n        # 1st conv layer\n        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)\n        conv1 = conv1 = nn.Conv1d(\n            cha_input, \n            cha_input*K, \n            kernel_size=5, \n            stride = 1, \n            padding=2,  \n            groups=cha_input, \n            bias=False)\n        self.conv1 = nn.utils.weight_norm(conv1, dim=None)\n\n        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = sign_size2)\n\n        # 2nd conv layer\n        self.batch_norm_c2 = nn.BatchNorm1d(cha_input*K)\n        self.dropout_c2 = nn.Dropout(dropout_hidden)\n        conv2 = nn.Conv1d(\n            cha_input*K, \n            cha_hidden, \n            kernel_size=3, \n            stride=1, \n            padding=1, \n            bias=False)\n        self.conv2 = nn.utils.weight_norm(conv2, dim=None)\n\n        # 3rd conv layer\n        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)\n        self.dropout_c3 = nn.Dropout(dropout_hidden)\n        conv3 = nn.Conv1d(\n            cha_hidden, \n            cha_hidden, \n            kernel_size=3, \n            stride=1, \n            padding=1, \n            bias=False)\n        self.conv3 = nn.utils.weight_norm(conv3, dim=None)\n        \n\n        # 4th conv layer\n        self.batch_norm_c4 = nn.BatchNorm1d(cha_hidden)\n        conv4 = nn.Conv1d(\n            cha_hidden, \n            cha_hidden, \n            kernel_size=5, \n            stride=1, \n            padding=2, \n            groups=cha_hidden, \n            bias=False)\n        self.conv4 = nn.utils.weight_norm(conv4, dim=None)\n\n        self.avg_po_c4 = nn.AvgPool1d(kernel_size=4, stride=2, padding=1)\n\n        self.flt = nn.Flatten()\n\n        self.batch_norm2 = nn.BatchNorm1d(output_size)\n        self.dropout2 = nn.Dropout(dropout_output)\n        dense2 = nn.Linear(output_size, output_dim, bias=False)\n        self.dense2 = nn.utils.weight_norm(dense2)\n\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = nn.functional.celu(self.dense1(x))\n\n        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1)\n\n        x = self.batch_norm_c1(x)\n        x = nn.functional.relu(self.conv1(x))\n\n        x = self.ave_po_c1(x)\n\n        x = self.batch_norm_c2(x)\n        x = self.dropout_c2(x)\n        x = nn.functional.relu(self.conv2(x))\n        x_s = x\n\n        x = self.batch_norm_c3(x)\n        x = self.dropout_c3(x)\n        x = nn.functional.relu(self.conv3(x))\n\n        x = self.batch_norm_c4(x)\n        x = self.conv4(x)\n        x =  x + x_s\n        x = nn.functional.relu(x)\n\n        x = self.avg_po_c4(x)\n\n        x = self.flt(x)\n\n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = self.dense2(x)\n\n        return x\n\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('valid_loss', loss)\n        \n    def test_step(self, batch, batch_idx):\n        X, y = batch\n        y_logit = self.forward(X)\n        y_probs = torch.sigmoid(y_logit).detach().cpu().numpy()\n        loss = self.loss(y_logit, y)\n        metric = roc_auc_score(y.cpu().numpy(), y_probs)\n        self.log('test_loss', loss)\n        self.log('test_metric', metric)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=1e-2, momentum=0.9)\n        scheduler = {\n            'scheduler': ReduceLROnPlateau(\n                optimizer, \n                mode=\"min\", \n                factor=0.5, \n                patience=5, \n                min_lr=1e-5),\n            'interval': 'epoch',\n            'frequency': 1,\n            'reduce_on_plateau': True,\n            'monitor': 'valid_loss',\n        }\n        return [optimizer], [scheduler]","9d2abfd3":"model = SoftOrdering1DCNN(\n    input_dim=len(input_features), \n    output_dim=1, \n    sign_size=16, \n    cha_input=64, \n    cha_hidden=64, \n    K=2, \n    dropout_input=0.3, \n    dropout_hidden=0.3, \n    dropout_output=0.2\n)\n\nearly_stop_callback = EarlyStopping(\n   monitor='valid_loss',\n   min_delta=.0,\n   patience=21,\n   verbose=True,\n   mode='min'\n)\n\ntrainer = pl.Trainer(callbacks=[early_stop_callback], min_epochs=10, max_epochs=200, gpus=1)","3934d10d":"model.summarize()","aea79758":"trainer.fit(\n    model, \n    DataLoader(train_tensor_dset, batch_size=2048, shuffle=True, num_workers=4),\n    DataLoader(valid_tensor_dset, batch_size=2048, shuffle=False, num_workers=4)\n)","f6619fa3":"# AUC on validation dataset\ntrainer.test(model, DataLoader(valid_tensor_dset, batch_size=2048, shuffle=False, num_workers=4))","570d0eae":"# AUC on test dataset\ntrainer.test(model, DataLoader(test_tensor_dset, batch_size=2048, shuffle=False, num_workers=4))","6a58b1a7":"train_dataset = lgb.Dataset(\n    train_dset[input_features].values,\n    train_dset[target].values,\n    free_raw_data=False\n)\n\nvalid_dataset = lgb.Dataset(\n    valid_dset[input_features].values,\n    valid_dset[target].values,\n    free_raw_data=False\n)","99f10ed4":"model_params = dict(\n    objective = \"binary\",\n    learning_rate = 0.1,\n    num_leaves = 32,\n    seed = 2,\n    deterministic = True,\n    metric = \"auc\"\n)","2ca77f11":"model = lgb.train(\n    model_params, \n    train_dataset, \n    valid_sets=[valid_dataset,],\n    num_boost_round=1000,\n    early_stopping_rounds=20,\n    verbose_eval=50,\n)","73118c31":"# AUC on validation dataset\nmodel.best_score[\"valid_0\"][\"auc\"]","332e7a83":"# AUC on test dataset\npreds = model.predict(test_dset[input_features].values)\nroc_auc_score(test_dset[target].values, preds)","81887e39":"***\n## 3-layers MLP","efb699db":"***\n## LightGBM","52cb9bd3":"# SoftOrdering1DCNN on Santander Customer Transaction Prediction (SCTP)\n\nIn this notebook we will apply the neural network architecture winner of the 2nd place on MoA (https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/202256) to the Santander customer transaction prediction problem.\n\nThe results of this network (which we call SoftOrdering1DCNN) are benchmarked against a plain MLP and LightGBM.","7b4f493e":"***\n## SoftOrdering1DCNN","886fb97d":"***\n## data preparation","03213c4b":"***"}}