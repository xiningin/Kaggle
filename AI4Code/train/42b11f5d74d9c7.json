{"cell_type":{"0eb765e4":"code","658003ed":"code","92d7f127":"code","9d4f00f9":"code","41dde2c2":"code","83255860":"code","2993e5bf":"code","05aa0ae9":"code","3dcbd691":"code","32f172b5":"code","4b8e7bc3":"code","fa65d4bc":"code","77bc2f8c":"code","48281a3d":"code","b0c1c341":"code","871fb18b":"code","5bdf6f93":"code","ca1d0c8a":"code","27a9c070":"code","20377cd6":"code","771fb464":"code","96a2e13e":"code","f38e2128":"code","1c09c1f9":"code","0401ddb7":"code","6bbdd542":"code","89f168f1":"code","fb1a50c8":"code","3334217e":"code","df8d14f0":"code","20d18720":"code","03108c12":"code","c1eea155":"code","2dd17b84":"code","e38bf6bd":"code","11e89adf":"code","e260e936":"code","7b2839f4":"markdown","5f47d293":"markdown","1a0519a2":"markdown","7d708b27":"markdown","847a9085":"markdown","d73d0b59":"markdown","93ed5b8e":"markdown"},"source":{"0eb765e4":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\nimport nltk\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\npd.options.display.max_colwidth = 200\n%matplotlib inline","658003ed":"# building a corpus of documents\ncorpus = ['The sky is blue and beautiful.',\n'Love this blue and beautiful sky!',\n'The quick brown fox jumps over the lazy dog.',\n\"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n'I love green eggs, ham, sausages and bacon!',\n'The brown fox is quick and the blue dog is lazy!',\n'The sky is very blue and the sky is very beautiful today',\n'The dog is lazy but the brown fox is quick!'\n]","92d7f127":"labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals',\n'weather', 'animals']","9d4f00f9":"corpus = np.array(corpus)\ncorpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\ncorpus_df = corpus_df[['Document', 'Category']]\ncorpus_df","41dde2c2":"wpt = nltk.WordPunctTokenizer()\nstop_words = nltk.corpus.stopwords.words('english')","83255860":"def normalize_document(doc):\n    # lowercase and remove special characters\\whitespace\n    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n    doc = doc.lower()\n    doc = doc.strip()\n    # tokenize document\n    tokens = wpt.tokenize(doc)\n    # filter stopwords out of document\n    filtered_tokens = [token for token in tokens if token not in stop_words]\n    # re-create document from filtered tokens\n    doc = ' '.join(filtered_tokens)\n    return doc","2993e5bf":"normalize_corpus = np.vectorize(normalize_document)","05aa0ae9":"norm_corpus = normalize_corpus(corpus)","3dcbd691":"print(norm_corpus)","32f172b5":"cv = CountVectorizer(min_df=0., max_df=1.)\ncv_matrix = cv.fit_transform(norm_corpus)\ncv_matrix","4b8e7bc3":"# view non-zero feature positions in the sparse matrix\nprint(cv_matrix)","fa65d4bc":"# view dense representation\n# warning might give a memory error if data is too big\ncv_matrix = cv_matrix.toarray()\ncv_matrix","77bc2f8c":"# get all unique words in the corpus\nvocab = cv.get_feature_names()\n# show document feature vectors\npd.DataFrame(cv_matrix, columns=vocab)","48281a3d":"# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\nbv = CountVectorizer(ngram_range=(2,2))\nbv_matrix = bv.fit_transform(norm_corpus)","b0c1c341":"bv_matrix = bv_matrix.toarray()\nvocab = bv.get_feature_names()\npd.DataFrame(bv_matrix, columns=vocab)","871fb18b":"tt = TfidfTransformer(norm='l2', use_idf=True)\ntt_matrix = tt.fit_transform(cv_matrix)","5bdf6f93":"tt_matrix = tt_matrix.toarray()\nvocab = cv.get_feature_names()\npd.DataFrame(np.round(tt_matrix, 2), columns=vocab)","ca1d0c8a":"tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2',\nuse_idf=True, smooth_idf=True)","27a9c070":"tv_matrix = tv.fit_transform(norm_corpus)\ntv_matrix = tv_matrix.toarray()","20377cd6":"vocab = tv.get_feature_names()\npd.DataFrame(np.round(tv_matrix, 2), columns=vocab)","771fb464":"vocab = tv.get_feature_names()\npd.DataFrame(np.round(tv_matrix, 2), columns=vocab)","96a2e13e":"# get unique words as feature names\nunique_words = list(set([word for doc in [doc.split() for doc in norm_corpus]\nfor word in doc]))","f38e2128":"def_feature_dict = {w: 0 for w in unique_words}\nprint('Feature Names:', unique_words)\nprint()\nprint('Default Feature Dict:', def_feature_dict)","1c09c1f9":"# build bag of words features for each document - term frequencies\nbow_features = []\nfor doc in norm_corpus:\n    bow_feature_doc = Counter(doc.split())\n    all_features = Counter(def_feature_dict)\n    bow_feature_doc.update(all_features)\n    bow_features.append(bow_feature_doc)\nbow_features = pd.DataFrame(bow_features)\nbow_features","0401ddb7":"import scipy.sparse as sp\nfeature_names = list(bow_features.columns)\n# build the document frequency matrix\ndf = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\ndf = 1 + df # adding 1 to smoothen idf later\n# show smoothened document frequencies\npd.DataFrame([df], columns=feature_names)","6bbdd542":"# compute inverse document frequencies\ntotal_docs = 1 + len(norm_corpus)\nidf = 1.0 + np.log(float(total_docs) \/ df)\n# show smoothened idfs\npd.DataFrame([np.round(idf, 2)], columns=feature_names)","89f168f1":"# compute idf diagonal matrix\ntotal_features = bow_features.shape[1]\nidf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\nidf_dense = idf_diag.todense()\n# print the idf diagonal matrix\npd.DataFrame(np.round(idf_dense, 2))","fb1a50c8":"# compute tfidf feature matrix\ntf = np.array(bow_features, dtype='float64')\ntfidf = tf * idf\n# view raw tfidf feature matrix\npd.DataFrame(np.round(tfidf, 2), columns=feature_names)","3334217e":"from numpy.linalg import norm\n# compute L2 norms\nnorms = norm(tfidf, axis=1)\n# print norms for each document\nprint (np.round(norms, 3))","df8d14f0":"# compute normalized tfidf\nnorm_tfidf = tfidf \/ norms[:, None]\n# show final tfidf feature matrix\npd.DataFrame(np.round(norm_tfidf, 2), columns=feature_names)","20d18720":"new_doc = 'the sky is green today'\npd.DataFrame(np.round(tv.transform([new_doc]).toarray(), 2),\ncolumns=tv.get_feature_names())","03108c12":"from sklearn.metrics.pairwise import cosine_similarity\nsimilarity_matrix = cosine_similarity(tv_matrix)\nsimilarity_df = pd.DataFrame(similarity_matrix)\nsimilarity_df","c1eea155":"from scipy.cluster.hierarchy import dendrogram, linkage\nZ = linkage(similarity_matrix, 'ward')\npd.DataFrame(Z, columns=['Document\\Cluster 1', 'Document\\Cluster 2',\n'Distance', 'Cluster Size'], dtype='object')","2dd17b84":"plt.figure(figsize=(8, 3))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Data point')\nplt.ylabel('Distance')\ndendrogram(Z)\nplt.axhline(y=1.0, c='k', ls='--', lw=0.5)","e38bf6bd":"from scipy.cluster.hierarchy import fcluster\nmax_dist = 1.0\ncluster_labels = fcluster(Z, max_dist, criterion='distance')\ncluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\npd.concat([corpus_df, cluster_labels], axis=1)","11e89adf":"from sklearn.decomposition import LatentDirichletAllocation\nlda = LatentDirichletAllocation(n_components=3, max_iter=10000, random_state=0)\ndt_matrix = lda.fit_transform(cv_matrix)\nfeatures = pd.DataFrame(dt_matrix, columns=['T1', 'T2', 'T3'])\nfeatures","e260e936":"tt_matrix = lda.components_\nfor topic_weights in tt_matrix:\n    topic = [(token, weight) for token, weight in zip(vocab, topic_weights)]\n    topic = sorted(topic, key=lambda x: -x[1])\n    topic = [item for item in topic if item[1] > 0.6]\n    print(topic)\n    print()","7b2839f4":"# Preprocessing Our Text Corpus\nThere can be multiple ways of cleaning and preprocessing textual data. In the following\npoints, we highlight some of the most important ones that are used heavily in Natural\nLanguage Processing (NLP) pipelines.","5f47d293":"# Document Similarity","1a0519a2":"# Extracting Features for New Documents","7d708b27":"The feature matrix is traditionally represented as a sparse matrix since the number\nof features increases phenomenally with each document considering each distinct word\nbecomes a feature. The preceding output tells us the total count for each (x, y) pair. Here,\nx represents a document and y represents a specific word\/feature and the value is the\nnumber of times y occurs in x. We can leverage the following code to view the output in a\ndense matrix representation.","847a9085":"# TfidfVectorizer","d73d0b59":"# Document Clustering with Similarity Features","93ed5b8e":"- ### Removing tags:\nOur text often contains unnecessary content like HTML tags, which do not add much value when analyzing text. The BeautifulSoup library does an excellent job in providing necessary functions for this.\n\n- ### Removing accented characters:\nIn any text corpus, especially if you are dealing with the English language, you might be dealing with accented characters\/letters. Hence, you need to make sure that these characters are converted and standardized into ASCII characters. A simple example is converting \u00e9 to e.\n\n- ### Expanding contractions:\nIn the English language, contractions are basically shortened versions of words or syllables, created by removing specific letters and sounds. Examples include do not to don\u2019t and I would to I\u2019d. Converting each contraction to its expanded, original form often helps with text standardization.\n\n- ### Removing special characters:\nSpecial characters and symbols that are usually non alphanumeric characters often add to the extra noise in unstructured text. More often than not, simple regular expressions (regexes) can be used to achieve this.\n\n- ### Stemming and lemmatization:\nWord stems are the base form of possible words that can be created by attaching affixes like prefixes and suffixes to the stem to create new words. This is known as inflection. The reverse process of obtaining the base form of a word is known as stemming. A simple example are the words watches, watching, and watched. They have the word root stem watch as the base form. Lemmatization is very similar to stemming, where we remove word affixes to get to the base form of a word. However, the base form in this case is known as the root word but not the root stem. The difference being that the root word is always a lexicographically correct word (present in the dictionary) but the root stem may not always be correct.\n\n- ### Removing stopwords:\nWords that have little or no significance, especially when constructing meaningful features from text, are known as stopwords. These are usually words that end up having the maximum frequency if you do a simple term or word frequency in a corpus. Words like \u201ca,\u201d \u201can,\u201d \u201cthe,\u201d and so on are considered to be stopwords. There is no universal stopword list, but we use a standard English language stopwords list from NLTK. You can also add your own domain specific stopwords as needed."}}