{"cell_type":{"ae18fa41":"code","ca7b2e2d":"code","85cc2617":"code","f21a4a7f":"code","35e9e7d2":"code","06a3298e":"code","13ad7f85":"code","0502e3c2":"code","af7d6739":"code","918b2756":"code","3eadf8e3":"code","13700b80":"code","a71c84ea":"code","b9e52c8e":"code","2139fc7f":"code","a5c05681":"code","d7822bca":"code","647f9124":"code","2b4a4620":"code","d04114cd":"code","878c4b15":"code","9cda2331":"code","755f67bc":"code","4052a156":"code","590dc5d6":"code","1cc91495":"code","7b2b9709":"code","3669cb9d":"markdown","433dcb66":"markdown","ec41df53":"markdown","7f8a0c70":"markdown","0329c477":"markdown"},"source":{"ae18fa41":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt# for Plotting graphs\nimport seaborn as sns# same as matplotlib but to make life easier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca7b2e2d":"#loading the dataset\ndf=pd.read_csv(\"\/kaggle\/input\/indian-liver-patient-records\/indian_liver_patient.csv\")\ndf.head()","85cc2617":"#describing the data\ndf.describe()","f21a4a7f":"#printing the shape of data\nprint(df.shape)\ndf.info()","35e9e7d2":"#encoding the Gender attribute\ndf['Gender'].replace({'Male':1,'Female':0},inplace=True)","06a3298e":"#plotting Correlation\nplt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),cmap='Greens',annot=True)","13ad7f85":"sns.pairplot(df, hue='Dataset')","0502e3c2":"# visualize number of patients diagonised with liver diesease\nsns.countplot(data = df, x = 'Dataset');","af7d6739":"#Visualizing data with liver disease along with Gender\nplt.figure(figsize=(6,6))\nax = sns.countplot(x = df['Dataset'].apply(lambda x:'Normal' if x == 1 else 'Liver Disease'), hue=df['Gender'])\nax.set_xlabel('Patient Condition')","918b2756":"#checking for missing values as per column\ndf.isna().sum()","3eadf8e3":"#checking the rows with the missing values\ndf[df['Albumin_and_Globulin_Ratio'].isna()]","13700b80":"#Lets have a look for correlation of Albumin_and_Globulin_Ratio with other columns\nplt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),cmap='Greens',annot=True)","a71c84ea":"#As seen above Albumin_and_Globulin_Ratio is highly correlated with Albumin\n# we apply binning to Albumin and will fill the values in Albumin_and_Globulin_Ratio using median of the bin value\ndf[\"binned_Albumin\"]=pd.cut(df['Albumin'],bins=10,labels=list(range(10)))\n#checking the rows with the missing values\ndf[df['Albumin_and_Globulin_Ratio'].isna()]","b9e52c8e":"#seprating dataframe as per bins of missing data\ndf_bin6=df[df['binned_Albumin']==6]\ndf_bin4=df[df['binned_Albumin']==4]\ndf_bin3=df[df['binned_Albumin']==3]\ndf_bin8=df[df['binned_Albumin']==8]","2139fc7f":"#filling na values for bin 6\ndf_bin6['Albumin_and_Globulin_Ratio'].fillna(df_bin6['Albumin_and_Globulin_Ratio'].median(),inplace=True)\nprint(\"Median for Albumin Globumin Ratio for bin 6: \",df_bin6['Albumin_and_Globulin_Ratio'].median())\n#adding the replaced values\ndf.drop(df[df['binned_Albumin']==6].index, inplace = True)\ndf=df.append(df_bin6,ignore_index=True)\n\n#filling na values for bin 4\ndf_bin4['Albumin_and_Globulin_Ratio'].fillna(df_bin4['Albumin_and_Globulin_Ratio'].median(),inplace=True)\nprint(\"Median for Albumin Globumin Ratio for bin 4: \",df_bin4['Albumin_and_Globulin_Ratio'].median())\n#adding the replaced values\ndf.drop(df[df['binned_Albumin']==4].index, inplace = True)\ndf=df.append(df_bin4,ignore_index=True)\n\n#filling na values for bin 3\ndf_bin3['Albumin_and_Globulin_Ratio'].fillna(df_bin3['Albumin_and_Globulin_Ratio'].median(),inplace=True)\nprint(\"Median for Albumin Globumin Ratio for bin 3: \",df_bin3['Albumin_and_Globulin_Ratio'].median())\n#adding the replaced values\ndf.drop(df[df['binned_Albumin']==3].index, inplace = True)\ndf=df.append(df_bin3,ignore_index=True)\n\n#filling na values for bin 8\ndf_bin8['Albumin_and_Globulin_Ratio'].fillna(df_bin8['Albumin_and_Globulin_Ratio'].median(),inplace=True)\nprint(\"Median for Albumin Globumin Ratio for bin 8: \",df_bin8['Albumin_and_Globulin_Ratio'].median())\n#adding the replaced values\ndf.drop(df[df['binned_Albumin']==8].index, inplace = True)\ndf= df.append(df_bin8,ignore_index=True)\n#Printing Shape of Dataset\nprint(df.shape)","a5c05681":"#remove the binned albumin column\ndf.drop(columns=['binned_Albumin'], inplace=True)","d7822bca":"#Scaling the dataset using Min Max scaler:\n#Getting Numerical Columns\ncols=df.columns.to_list()\ncols.remove('Gender')\ncols.remove('Dataset')\nprint(\"Columns with numerical data:\")\ncols","647f9124":"#getting Numerical columns:\ndf_numerical=df[cols]\n\n#starting scaling process:\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(df_numerical)\nscaled=scaler.transform(df_numerical) #the variable scaled will be in numpy array \nx=pd.DataFrame(scaled, columns=cols) #converting the variable to dataframe.\nx['Gender']=df['Gender']# adding Gender to X or attribute list\ny=df['Dataset']# Getting the labels\nx","2b4a4620":"#moving for feature selection\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(n_estimators=50)\nclf = clf.fit(x, y)\nprint(\"Showing feature importance values\")\nprint(clf.feature_importances_) ","d04114cd":"model=SelectFromModel(clf, prefit=True) #getting features from  the above classifer as per the importances\ncols=x.columns.to_list()#getting list of columns\ntf=model.get_support()#getting which features are important\nselectedcols=[]\nfor i in range(len(cols)):\n    if tf[i]:\n        selectedcols.append(cols[i])\nprint(\"showing selected columns\")\nprint(selectedcols)\n#converting the data\nX_new = model.transform(x)\nX_new.shape ","878c4b15":"#splitting the dataset for Training and testing and using 5-fold Cross validation.\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=5)\nkf.get_n_splits(X_new)\n\n#making a comparative study of 3 different ML Algorithms namely SVM, Random Forest, KNN\n#metrics for SVM\nSVM_accuracy=[]\nSVM_precision=[]\nSVM_recall=[]\nSVM_f1_score=[]\n\n#metrics for Random Forest\nRF_accuracy=[]\nRF_precision=[]\nRF_recall=[]\nRF_f1_score=[]\n\n#metrics for KNN\nKNN_accuracy=[]\nKNN_precision=[]\nKNN_recall=[]\nKNN_f1_score=[]","9cda2331":"#initializing the models\n#importing libraries of the selected algorithms\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n#importing libraries of performance Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\n#Making the classifier Objects\nclf_svm=SVC() #SVM object\nclf_rf=RandomForestClassifier(max_depth=5, random_state=0)#Random Forest Object\nclf_knn = KNeighborsClassifier(n_neighbors=3)#KNN object","755f67bc":"i=1# count the number of folds\n#starting the 5 fold cross valivation\nfor train_index, test_index in kf.split(X_new):\n    print(\"%d Number of fold\"%i)\n    i+=1\n    #Splitting the data\n    X_train, X_test = X_new[train_index], X_new[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    #Training and Evaluating SVM\n    model=clf_svm.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    SVM_accuracy.append(accuracy_score(y_test,y_pred))\n    SVM_precision.append(precision_score(y_test,y_pred))\n    SVM_recall.append(recall_score(y_test,y_pred))\n    SVM_f1_score.append(f1_score(y_test,y_pred))\n    \n    #Training and Evaluating Random Forest\n    model=clf_rf.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    RF_accuracy.append(accuracy_score(y_test,y_pred))\n    RF_precision.append(precision_score(y_test,y_pred))\n    RF_recall.append(recall_score(y_test,y_pred))\n    RF_f1_score.append(f1_score(y_test,y_pred))\n    \n    #Training and Evaluating KNN\n    model=clf_knn.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    KNN_accuracy.append(accuracy_score(y_test,y_pred))\n    KNN_precision.append(precision_score(y_test,y_pred))\n    KNN_recall.append(recall_score(y_test,y_pred))\n    KNN_f1_score.append(f1_score(y_test,y_pred))","4052a156":"#visualizing results of SVM per fold\nx=list(range(1,6))\nplt.plot(x,SVM_accuracy,label='Accuracy')\nplt.plot(x,SVM_precision,label='Precision')\nplt.plot(x,SVM_recall, label='Recall')\nplt.plot(x,SVM_f1_score,label='F1 Score')\nplt.title(\"Performance of SVM\")\nplt.legend()\nplt.xlabel(\"Cross Validation Fold\")\nplt.ylabel(\"performace\")\nplt.show()","590dc5d6":"#visualizing results of Random Forest per fold\nplt.plot(x,RF_accuracy,label='Accuracy')\nplt.plot(x,RF_precision,label='Precision')\nplt.plot(x,RF_recall, label='Recall')\nplt.plot(x,RF_f1_score,label='F1 Score')\nplt.title(\"Performance of Random Forest\")\nplt.xlabel(\"Cross Validation Fold\")\nplt.ylabel(\"performace\")\nplt.legend()\nplt.show()","1cc91495":"#visualizing results of KNN per epoch\nx=list(range(1,6))\nplt.plot(x,KNN_accuracy,label='Accuracy')\nplt.plot(x,KNN_precision,label='Precision')\nplt.plot(x,KNN_recall, label='Recall')\nplt.plot(x,KNN_f1_score,label='F1 Score')\nplt.title(\"Performance of KNN\")\nplt.xlabel(\"Cross Validation Fold\")\nplt.ylabel(\"performace\")\nplt.legend()\nplt.show","7b2b9709":"#visualizing average results:\nSVM=[\"SVM \", (sum(SVM_accuracy)\/len(SVM_accuracy)), (sum(SVM_precision)\/len(SVM_precision)), \n     (sum(SVM_recall)\/len(SVM_recall)), (sum(SVM_f1_score)\/len(SVM_f1_score))]\n\nRF=[\"RF \", (sum(RF_accuracy)\/len(RF_accuracy)), (sum(RF_precision)\/len(RF_precision)), \n     (sum(RF_recall)\/len(RF_recall)), (sum(RF_f1_score)\/len(RF_f1_score))]\n\nKNN=[\"KNN \", (sum(KNN_accuracy)\/len(KNN_accuracy)), (sum(KNN_precision)\/len(KNN_precision)), \n     (sum(KNN_recall)\/len(KNN_recall)), (sum(KNN_f1_score)\/len(KNN_f1_score))]\ndata=[]\ndata.append(SVM)\ndata.append(RF)\ndata.append(KNN)\n#converting results to dataframe\nresults=pd.DataFrame(data,columns=[\"Algorithms\",\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\nresults","3669cb9d":"# Applying ML Algorithms","433dcb66":"# Analyzing The dataset","ec41df53":"# Starting Data Preprocessing","7f8a0c70":"# Starting with EDA","0329c477":"# Analyzing the performance"}}