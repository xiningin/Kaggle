{"cell_type":{"cdf57dbd":"code","412b75b8":"code","3309be2f":"code","1623dbbc":"code","19854ecc":"code","0a729f65":"code","9beb9e50":"code","cd62280c":"code","70b51cd7":"code","3a51dc34":"code","18dc1b3d":"code","edf0084c":"code","973bc531":"code","268d6ac9":"code","a697cabc":"code","46d998a2":"code","ac1b3e53":"code","21d81764":"code","8507bd09":"code","b23cc3c1":"code","6f4a0a9f":"code","76b8a030":"code","100878f5":"code","ba51d2d6":"code","c2488d69":"code","6fc5cf93":"code","407a3c8e":"code","91c746b6":"code","1fcb92f2":"code","8c2fe46c":"code","ce2d778c":"code","f5676ce7":"code","aa2332ae":"code","c42a04e5":"code","0609de8a":"code","6c207ac1":"code","c3871113":"code","78951888":"code","db01ebd0":"code","0f3cfe4c":"code","8ed5fed2":"code","27e6f4ba":"code","8277e380":"markdown","2684b85c":"markdown","60dc3914":"markdown"},"source":{"cdf57dbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport datetime\nimport warnings\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","412b75b8":"# importing seaborn and matplotlabsib\n\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")","3309be2f":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","1623dbbc":"train.head()","19854ecc":"test.head()","0a729f65":"train.describe()","9beb9e50":"test.describe()","cd62280c":"train.info()","70b51cd7":"test.info()","3a51dc34":"train['target'].head()","18dc1b3d":"sns.distplot(train['target'])\nplt.show()","edf0084c":"train['target'].value_counts().plot(kind=\"bar\")\nplt.show()","973bc531":"# importing the sklearn package\n\nimport sklearn\nfrom sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler()","268d6ac9":"training = train.drop(['ID_code', 'target'], axis=1)\ntarget = train['target']","a697cabc":"testing = test.drop('ID_code', axis=1)","46d998a2":"training.head()","ac1b3e53":"target.head()","21d81764":"testing.head()","8507bd09":"# scaling the training data\ntraining_scaled = scaler.fit_transform(training)\n\n# scaling the testing data\ntesting_scaled = scaler.fit_transform(testing)","b23cc3c1":"print(\"Shape of the training data: {}\".format(training_scaled.shape))\nprint(\"Shape of the testing data: {}\".format(testing_scaled.shape))","6f4a0a9f":"# calling the garbage collector\nimport gc\n\ngc.collect()","76b8a030":"# importing packages\n\nimport keras\nfrom keras import layers\nfrom keras import utils\nfrom keras import models\n\nfrom keras.layers.core import (Dense, Activation, Flatten, Dropout)\nfrom keras.layers import BatchNormalization\nfrom keras.models import (Sequential, Model)\nfrom keras import optimizers","100878f5":"# importing the misc., properties\n\nBATCH_SIZE = 64\nNP_EPOCHS = 50\nNP_CLASSES = 1\nVERBOSE = 1\nVALIDATION_SPLIT = 0.2\n\n# taking the instance as RMSProp\noptimizer = optimizers.RMSprop()","ba51d2d6":"# creating the generator the generator code has been taken from https:\/\/www.kaggle.com\/mathormad\/knowledge-distillation-with-nn-rankgauss\n\ndef mixup_data(x, y, alpha=1.0):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    sample_size = x.shape[0]\n    index_array = np.arange(sample_size)\n    np.random.shuffle(index_array)\n\n    mixed_x = lam * x + (1 - lam) * x[index_array]\n    mixed_y = (lam * y) + ((1 - lam) * y[index_array])\n    return mixed_x, mixed_y\n\n\ndef make_batches(size, batch_size):\n    nb_batch = int(np.ceil(size \/ float(batch_size)))\n    return [(i * batch_size, min(size, (i + 1) * batch_size)) for i in range(0, nb_batch)]\n\n\ndef batch_generator(X, y, batch_size=128, shuffle=True, mixup=False):\n    y = np.array(y)\n    sample_size = X.shape[0]\n    indexed = np.arange(sample_size)\n\n    while True:\n        if shuffle:\n            np.random.shuffle(indexed)\n        batches = make_batches(sample_size, batch_size)\n        for batch_index, (batch_start, batch_end) in enumerate(batches):\n            batch_ids = indexed[batch_start:batch_end]\n            x_batch = X[batch_ids]\n            y_batch = y[batch_ids]\n\n            if mixup:\n                x_batch, y_batch = mixup_data(x_batch, y_batch, alpha=1.0)\n            yield x_batch, y_batch","c2488d69":"# creating the checkpoints\n\nfrom keras.callbacks import (ModelCheckpoint, ReduceLROnPlateau, EarlyStopping)\nfrom keras.layers import LeakyReLU\n\nNP_OUTPUT_FUNCTION = \"sigmoid\"\nDROPOUT_FIRST = 0.25\nDROPOUT_SECOND = 0.20\nNP_INPUT_SHAPE = training_scaled.shape[1]\n\nprint(\"Input shape has been taken as: {}\".format(NP_INPUT_SHAPE))","6fc5cf93":"# building the model\n\nmodel = Sequential()\nmodel.add(Dense(256, input_shape=(NP_INPUT_SHAPE,)))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_FIRST))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(128))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_FIRST))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(64))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_SECOND))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(32))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_SECOND))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(16))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_SECOND))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(8))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_SECOND))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(1))\nmodel.add(Activation(NP_OUTPUT_FUNCTION))","407a3c8e":"# printing the summary of the model to see the trainable and non trainable parameters\n\nmodel.summary()","91c746b6":"\n\ncurrent_dt_time = datetime.datetime.now()\nmodel_name = 'model_init' + '_' + str(current_dt_time).replace(' ', '').replace(':', '_') + '\/'\n\nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n    \nfile_path = model_name + \"model-{epoch:05d}-{loss:.5f}-{val_auc:.5f}-{val_loss:.5f}-{val_auc:.5f}.h5\"","1fcb92f2":"!ls -lrt","8c2fe46c":"## creating call back methods\n\n## Call back method to calculate ROC AUC - We can found optins from: https:\/\/stackoverflow.com\/questions\/41032551\/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n## We can calculate the ROC AUC for mini batches - so we can calculate the ROC AUC score at the end of each epoch by using callbacks method\n     \nfrom keras.callbacks import Callback\nfrom sklearn import metrics\n\n\nclass findROC(Callback):\n    def __init__(self, etraining, evalidation):\n        # for training\n        self.x_train = etraining[0]\n        self.y_train = etraining[1]\n\n        # for validation\n        self.x_val = evalidation[0]\n        self.y_val = evalidation[1]\n\n    def on_train_begin(self, logs=None):\n        return\n\n    def on_batch_end(self, batch, logs=None):\n        return\n\n    def on_epoch_begin(self, epoch, logs=None):\n        return\n\n    def on_epoch_end(self, epoch, logs=None):\n        y_pred_training = self.model.predict(self.x_train)\n        roc_score_training = metrics.roc_auc_score(self.y_train, y_pred_training)\n\n        y_pred_validation = self.model.predict(self.x_val)\n        roc_score_validation = metrics.roc_auc_score(self.y_val, y_pred_validation)\n\n        print(\"Training RoC score found: {}, validation RoC score found: {}\".format(roc_score_training,\n                                                                                    roc_score_validation))\n        return\n    \n    def on_batch_begin(self, batch, logs=None):\n        return","ce2d778c":"# creating model checkpoint\ncheckpoint = ModelCheckpoint(filepath=file_path, \n                             monitor='val_loss', \n                             verbose=1, \n                             save_best_only=True, \n                             save_weights_only=False, \n                             mode='auto', \n                             period=1)\n\n# early stopping\nearly = EarlyStopping(monitor='val_loss',\n                      mode='auto',\n                      patience=5,\n                      verbose=1)\n\n\nLR = ReduceLROnPlateau(monitor=\"val_loss\",\n                       factor=0.2,\n                       patience=2,\n                       min_lr=0.000001,\n                       verbose=1,\n                       cooldown=1)","f5676ce7":"# ROC & AUC metric to monitor\nimport tensorflow as tf\nimport keras.backend as B\n\ndef auc(y_true, y_pred):\n    score = tf.metrics.auc(y_true, y_pred)[1]\n    B.get_session().run(tf.local_variables_initializer())\n    return score","aa2332ae":"# creating the optimizer\n\noptimizer = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n# optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, decay=0.0)","c42a04e5":"# compiling the model\n\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', auc])","0609de8a":"# splitting the data into train and test using sklearn library\nfrom sklearn import model_selection\n\nX_train, X_val, y_train, y_val = model_selection.train_test_split(training_scaled, target, test_size=0.20, random_state=123456789)","6c207ac1":"print(\"X training data shape: {}\".format(X_train.shape))\nprint(\"y training data shape: {}\".format(y_train.shape))","c3871113":"print(\"X validation data shape: {}\".format(X_val.shape))\nprint(\"y validation data shape: {}\".format(y_val.shape))","78951888":"# calculating the number of training and validation steps per epoch\n\nnum_training_seq = len(X_train)\nnum_validation_seq = len(X_val)\n\nprint(\"# training sequences: {}\".format(num_training_seq))\nprint(\"# validation sequences: {}\".format(num_validation_seq))","db01ebd0":"if(num_training_seq % BATCH_SIZE) == 0:\n    training_steps_per_epoch = int(num_training_seq \/ BATCH_SIZE)\nelse:\n    training_steps_per_epoch = int(num_training_seq \/ BATCH_SIZE) + 1","0f3cfe4c":"if(num_validation_seq % BATCH_SIZE) == 0:\n    validation_steps_per_epoch = int(num_validation_seq \/ BATCH_SIZE)\nelse:\n    validation_steps_per_epoch = int(num_validation_seq \/ BATCH_SIZE) + 1","8ed5fed2":"print(\"Number of training steps are required for epoch: {}\".format(training_steps_per_epoch))\nprint(\"Number of validation steps are requried for epoch: {}\".format(validation_steps_per_epoch))","27e6f4ba":"# fitting the model\n\nhistory = model.fit_generator(generator=batch_generator(X_train, y_train, BATCH_SIZE),\n                             validation_data=batch_generator(X_val, y_val, BATCH_SIZE),\n                             epochs=NP_EPOCHS,\n                             verbose=VERBOSE,\n                             steps_per_epoch=training_steps_per_epoch,\n                             validation_steps=validation_steps_per_epoch,\n                             class_weight=None,\n                             initial_epoch=0,\n                             # callbacks=[checkpoint, early, LR, findROC(etraining=(X_train, y_train), evalidation=(X_val, y_val))])\n                             callbacks=[checkpoint, LR, findROC(etraining=(X_train, y_train), evalidation=(X_val, y_val))])","8277e380":"### Building the Model using keras","2684b85c":"#### Standardizing \/ normalizing the data using sklearn","60dc3914":"#### Loading the data - creating as dataframe"}}