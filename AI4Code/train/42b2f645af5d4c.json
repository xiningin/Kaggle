{"cell_type":{"72c340dd":"code","3ec3cb69":"code","9c01cb19":"code","26748493":"code","e8ae86a1":"code","91ab44ea":"code","0b48c421":"code","e29c5601":"code","d62a9a65":"code","4f340c36":"code","417c4cd6":"code","9268ed0d":"code","b4221fa3":"code","588ede30":"code","679f8525":"code","8c73d94e":"code","2044abef":"code","03778575":"code","2768d5be":"code","9becc590":"code","ae1ae724":"code","986053fc":"code","897c0e86":"code","01160b57":"code","7fc9f4e8":"code","be70a4dc":"code","54184334":"code","8c0a12e4":"code","c2ad9a3e":"code","a7038a85":"code","0b2d5ed9":"code","4e728524":"code","62de3b33":"code","119cd401":"code","9bb4c97f":"code","7defb005":"code","d2aba3ec":"code","138748e3":"code","57a46405":"code","596de407":"code","2ba3a08a":"code","9557257d":"code","21ec0d7d":"code","c72b7f60":"code","8b23a6ad":"code","5208b175":"code","382ad8aa":"code","4e6905d2":"code","deac6ed8":"code","6e82a38d":"code","9eafe682":"code","ea5a05a5":"code","7eea8da7":"code","3933f1d2":"code","7eebfbc4":"code","cbd4ad84":"code","d2fbe5ed":"code","798a6494":"code","8575edff":"code","503a24e8":"code","0a585896":"markdown","c924c247":"markdown","0097b61d":"markdown","3c0c0d1d":"markdown","de16b555":"markdown","be7bab2a":"markdown","430505f8":"markdown","7bbafe42":"markdown","0982920c":"markdown","959795df":"markdown","084067e0":"markdown","649f97bd":"markdown","f037b35e":"markdown","803b9ca4":"markdown","e00ff821":"markdown","ae8dc1bf":"markdown","c0ae0d21":"markdown","865acc1e":"markdown","8bbd16be":"markdown","6453df5b":"markdown","9b02726f":"markdown","4bb52a45":"markdown","7533c75f":"markdown","e62bd83d":"markdown","8326951b":"markdown","2613aefc":"markdown","d2ce29ae":"markdown","4092a4ae":"markdown","54935a01":"markdown","3af60ea1":"markdown","68de42ee":"markdown","eb78d7e9":"markdown","669bd8f1":"markdown","ec3a0959":"markdown","f8c63a38":"markdown","1df621b5":"markdown","601f3044":"markdown","9979a26f":"markdown"},"source":{"72c340dd":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scikitplot import metrics as mt\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, OrdinalEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report,recall_score,f1_score,roc_auc_score, plot_precision_recall_curve, precision_score,roc_curve\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom scipy.stats import loguniform as sp_loguniform\n\nimport warnings\nwarnings.filterwarnings('ignore')","3ec3cb69":"train = pd.read_csv('..\/input\/adult-pmr3508\/train_data.csv', \n                    sep=r'\\s*,\\s*',\n                    engine='python',\n                    na_values=\"?\")\ntest = pd.read_csv('..\/input\/adult-pmr3508\/test_data.csv', \n                   sep=r'\\s*,\\s*',\n                    engine='python',\n                    na_values=\"?\")","9c01cb19":"train.head()","26748493":"test.head()","e8ae86a1":"print(f\"Training data shape: {train.shape}\")\nprint(f\"Testing data shape: {test.shape}\")","91ab44ea":"train.dtypes","0b48c421":"train.isna().sum().sort_values(ascending = False)","e29c5601":"test.isna().sum().sort_values(ascending = False)","d62a9a65":"# fill nans\noccupation_mc = train.loc[:, 'occupation'].mode()[0]\nworkclass_mc = train.loc[:, 'workclass'].mode()[0]\nnative_country_mc = train.loc[:, 'native.country'].mode()[0]\n\nprint(f'occupation most common attribute: {occupation_mc}')\nprint(f'workclass most common attribute: {workclass_mc}')\nprint(f'native.country most common attribute: {native_country_mc}')\n\ntrain.loc[:, 'occupation'].fillna(occupation_mc, inplace = True)\ntrain.loc[:, 'workclass'].fillna(workclass_mc, inplace = True)\ntrain.loc[:, 'native.country'].fillna(native_country_mc, inplace = True)\n\ntest.loc[:, 'occupation'].fillna(occupation_mc, inplace = True)\ntest.loc[:, 'workclass'].fillna(workclass_mc, inplace = True)\ntest.loc[:, 'native.country'].fillna(native_country_mc, inplace = True)","4f340c36":"# checking missing values again - train\ntrain.isna().sum().sort_values(ascending = False)","417c4cd6":"# checking missing values again - test\ntest.isna().sum().sort_values(ascending = False)","9268ed0d":"num_attributes = train.select_dtypes(include = ['int64','float64']).drop(columns = 'Id')\ncat_attributes = train.select_dtypes(exclude = ['int64','float64'])","b4221fa3":"num_attributes.describe()","588ede30":"# inserting education.num to categorical variables\nnew_cat_variables = list(cat_attributes.columns) + ['education.num']\ncat_attributes = train[new_cat_variables].copy()\n\n# removing education.num from numerical variables\nnum_attributes = num_attributes.drop(columns = 'education.num')","679f8525":"cat_attributes.apply(lambda x: x.unique().shape[0])","8c73d94e":"_ = sns.catplot(x = 'income', kind = 'count', data = train)","2044abef":"fig, ax = plt.subplots(1, 5, figsize = (25,7))\nfor index, column in enumerate(num_attributes.columns):\n    _ = sns.boxplot(y = num_attributes[column], ax = ax[index], orient = 'v')\n    \n_ = num_attributes.hist(bins = 25, figsize = (20, 7))\n_ = plt.tight_layout()","03778575":"fig, axes = plt.subplots(5, 2, figsize = (30,15), constrained_layout=True)\naxes = axes.flatten()\n\nfor index, column in enumerate(cat_attributes.columns):\n    _ = sns.countplot(cat_attributes[column], ax = axes[index], order = cat_attributes[column].value_counts().index)\n    axes[index].tick_params(axis='x', rotation=90)","2768d5be":"# separating publics by income\nless_income = train[train['income'] == '<=50K']\nless_income = less_income[['sex','income']].groupby(\"sex\").count().reset_index()\n\nmore_income = train[train['income'] == '>50K']\nmore_income = more_income[['sex','income']].groupby(\"sex\").count().reset_index()\n\n# plot 1 - income <=50K\nplt.subplot(1,2,1)\n_ = sns.barplot(x = 'sex', y = 'income', data = less_income)\n_ = plt.title(\"Income <=50K\")\n\n#plot2 - gender x cardio (bar plot)\nplt.subplot(1,2,2)\n_ = sns.barplot(x = 'sex', y = 'income', data = more_income)\n_ = plt.title(\"Income >50K\")\n\n_ = plt.tight_layout()","9becc590":"# plot 1 - boxplot\nplt.subplot(1,2,1)\n_ = sns.boxplot(x = 'income', y = 'age', data = train)\n_ = plt.title(\"Age x Income\")\n\n# plot 2 - correlation\nplt.subplot(1,2,2)\ndf_corr = train[['age', 'income']]\ndf_corr['income'] = df_corr['income'].map({'<=50K' : 0, '>50K' : 1})\n_ = sns.heatmap(df_corr[['age', 'income']].corr(method = 'pearson'), annot = True)","ae1ae724":"# separating publics by income\nless_income = train[train['income'] == '<=50K']\nless_income = less_income[['education','income']].groupby(\"education\").count().sort_values('income').reset_index()\n\nmore_income = train[train['income'] == '>50K']\nmore_income = more_income[['education','income']].groupby(\"education\").count().sort_values('income').reset_index()\n\nfig, axes = plt.subplots(1,2,figsize = (25, 5))\n\n# plot 1 - income <=50K\n_ = sns.barplot(x = 'education', y = 'income', data = less_income, ax = axes[0])\n_ = axes[0].set_title(\"Income <=50K\")\n_ = axes[0].tick_params(axis='x', rotation=90)\n\n#plot2 - gender x cardio (bar plot)\n_ = sns.barplot(x = 'education', y = 'income', data = more_income, ax = axes[1])\n_ = axes[1].set_title(\"Income >50K\")\n_ = axes[1].tick_params(axis='x', rotation=90)\n\n_ = plt.tight_layout()","986053fc":"#exclude non-numerical variables\ndf_corr = train.copy()\ndf_corr['income'] = df_corr['income'].map({'<=50K' : 0, '>50K' : 1})\naux1 = df_corr.select_dtypes(exclude = ['object'])\n\n#plot correlation plot to our numerical variables\nfig, ax = plt.subplots(figsize = (20, 8))\n_ = sns.heatmap(aux1.corr(method = 'pearson'), annot = True, ax = ax)","897c0e86":"train['capital'] = train['capital.gain'] - train['capital.loss']\ntest['capital'] = test['capital.gain'] - test['capital.loss']","01160b57":"train[['capital']].describe()","7fc9f4e8":"train = train.drop(columns = ['native.country', 'capital.gain', 'capital.loss'])\ntest = test.drop(columns = ['native.country', 'capital.gain', 'capital.loss'])","be70a4dc":"X = train.drop(columns = ['income', 'Id'])\ny = train['income'].copy()\n\n#split data into training and test dataset\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.20, random_state = 42)\n\ndatabases = [X, X_train, X_val, test]","54184334":"rs = RobustScaler()\ncolumns = X_train.drop(columns = ['education.num']).select_dtypes(include = ['int64','float64']).columns\nprint(columns)\nX_train[columns] = rs.fit_transform(X_train[columns].values)\nX[columns] = rs.transform(X[columns].values)\nX_val[columns] = rs.transform(X_val[columns].values)\ntest[columns] = rs.transform(test[columns].values)","8c0a12e4":"X_train.head()","c2ad9a3e":"X_train.select_dtypes(include = 'object').apply(lambda x: x.unique().shape[0])","a7038a85":"onehot_columns = ['sex', 'workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race']\n    \n# OneHotEncoder\nX = pd.get_dummies(X, prefix=onehot_columns, columns = onehot_columns, drop_first=True)\nX_train = pd.get_dummies(X_train, prefix=onehot_columns, columns = onehot_columns, drop_first=True)\nX_val = pd.get_dummies(X_val, prefix=onehot_columns, columns = onehot_columns, drop_first=True)\ntest = pd.get_dummies(test, prefix=onehot_columns, columns = onehot_columns, drop_first=True)","0b2d5ed9":"X_train.head()","4e728524":"# model definition\nknn = KNeighborsClassifier()\n\n# RandomizedSearchCV\nparameters = {'n_neighbors' : np.arange(5, 31)}\nknn_grid_cv = RandomizedSearchCV(knn, parameters, verbose = True, scoring='accuracy', cv = 5, n_iter = 50, n_jobs = -1, random_state = 42)\n%timeit -n 1 -r 1 knn_grid_cv.fit(X_train, y_train)\n\nprint(f'Best estimator: {knn_grid_cv.best_estimator_}')\nprint(f'Best score: {knn_grid_cv.best_score_}')\n\n# predict\ny_predict_knn = knn_grid_cv.predict(X_val)","62de3b33":"# model definition\nsvc = SVC(random_state=42, probability=True)\n\n# RandomizedSearchCV\nparameters = {'C': np.linspace(1e-2, 20),\n              'gamma': ['scale', 'auto']}\nsvc_grid_cv = RandomizedSearchCV(svc, parameters, verbose = True, scoring='accuracy', cv = 5, n_iter = 30, n_jobs = -1, random_state = 42)\n%timeit -n 1 -r 1 svc_grid_cv.fit(X_train, y_train)\n\nprint(f'Best estimator: {svc_grid_cv.best_estimator_}')\nprint(f'Best score: {svc_grid_cv.best_score_}')\n\n# predict\ny_predict_svc = svc_grid_cv.predict(X_val)","119cd401":"# model definition\nrfc = RandomForestClassifier(random_state=42)\n\n# RandomizedSearchCV\nparameters = {'n_estimators': np.arange(100, 500),\n               'criterion': ['gini', 'entropy'],\n               'max_depth': np.arange(1, 50),}\nrfc_grid_cv = RandomizedSearchCV(rfc, parameters, verbose = True, scoring='accuracy', cv = 5, n_iter = 30, n_jobs = -1, random_state = 42)\n%timeit -n 1 -r 1 rfc_grid_cv.fit(X_train, y_train)\n\nprint(f'Best estimator: {rfc_grid_cv.best_estimator_}')\nprint(f'Best score: {rfc_grid_cv.best_score_}')\n\n# predict\ny_predict_rfc = rfc_grid_cv.predict(X_val)","9bb4c97f":"# model definition\nxgb = XGBClassifier(random_state=42)\n\n# RandomizedSearchCV\nparameters = {'n_estimators': np.arange(10, 500),\n               'learning_rate': np.linspace(1e-3, 1),\n               'max_depth': np.arange(1, 20),\n               'reg_alpha': sp_loguniform(1e-14, 1e1),\n               'reg_lambda': sp_loguniform(1e-14, 1e1),}\nxgb_grid_cv = RandomizedSearchCV(xgb, parameters, verbose = True, cv = 5, n_iter = 30, n_jobs = -1, random_state = 42)\n%timeit -n 1 -r 1 xgb_grid_cv.fit(X_train, y_train)\n\nprint(f'Best estimator: {xgb_grid_cv.best_estimator_}')\nprint(f'Best score: {xgb_grid_cv.best_score_}')\n\n# predict\ny_predict_xgb = xgb_grid_cv.predict(X_val)","7defb005":"# model definition\nmlp = MLPClassifier(random_state=42, early_stopping=True)\n\n# RandomizedSearchCV\nparameters = {'hidden_layer_sizes': [(2 ** i, 2 ** j) for j in np.arange(5, 8) for i in np.arange(4, 7)],\n               'alpha': sp_loguniform(1e-10, 1e-1),\n               'learning_rate': ['constant','adaptive']}\nmlp_grid_cv = RandomizedSearchCV(mlp, parameters, verbose = True, scoring='accuracy', cv = 5, n_iter = 30, n_jobs = -1, random_state = 42)\n%timeit -n 1 -r 1 mlp_grid_cv.fit(X_train, y_train)\n\nprint(f'Best estimator: {mlp_grid_cv.best_estimator_}')\nprint(f'Best score: {mlp_grid_cv.best_score_}')\n\n# predict\ny_predict_mlp = mlp_grid_cv.predict(X_val)","d2aba3ec":"# accuracy_score\ndict_map = {'<=50K' : 0, '>50K' : 1}\naccuracy_knn = accuracy_score(pd.Series(y_val).map(dict_map), pd.Series(y_predict_knn).map(dict_map))\n\nprint(f\"Accuracy Score for KNN: {accuracy_knn}\")","138748e3":"#classification_report\nprint(classification_report(y_val, y_predict_knn))","57a46405":"dict_map = {'<=50K' : 0, '>50K' : 1}\nroc_auc_knn = roc_auc_score(pd.Series(y_val).map(dict_map), pd.Series(y_predict_knn).map(dict_map))\n\nprint(f\"ROC-AUC Score for KNN: {roc_auc_knn}\")","596de407":"#confusion_matrix\n_ = mt.plot_confusion_matrix(y_val,y_predict_knn, normalize = False, figsize = (12,12))","2ba3a08a":"# accuracy_score\ndict_map = {'<=50K' : 0, '>50K' : 1}\naccuracy_svc = accuracy_score(pd.Series(y_val).map(dict_map), pd.Series(y_predict_svc).map(dict_map))\n\nprint(f\"Accuracy Score for SVC: {accuracy_svc}\")","9557257d":"#classification_report\nprint(classification_report(y_val, y_predict_svc))","21ec0d7d":"dict_map = {'<=50K' : 0, '>50K' : 1}\nroc_auc_svc = roc_auc_score(pd.Series(y_val).map(dict_map), pd.Series(y_predict_svc).map(dict_map))\n\nprint(f\"ROC-AUC Score for SVC: {roc_auc_svc}\")","c72b7f60":"#confusion_matrix\n_ = mt.plot_confusion_matrix(y_val,y_predict_svc, normalize = False, figsize = (12,12))","8b23a6ad":"# accuracy_score\ndict_map = {'<=50K' : 0, '>50K' : 1}\naccuracy_rfc = accuracy_score(pd.Series(y_val).map(dict_map), pd.Series(y_predict_rfc).map(dict_map))\n\nprint(f\"Accuracy Score for Random Forest Classifier: {accuracy_rfc}\")","5208b175":"#classification_report\nprint(classification_report(y_val, y_predict_rfc))","382ad8aa":"dict_map = {'<=50K' : 0, '>50K' : 1}\nroc_auc_rfc = roc_auc_score(pd.Series(y_val).map(dict_map), pd.Series(y_predict_rfc).map(dict_map))\n\nprint(f\"ROC-AUC Score for Random Forest Classifier: {roc_auc_rfc}\")","4e6905d2":"#confusion_matrix\n_ = mt.plot_confusion_matrix(y_val,y_predict_rfc, normalize = False, figsize = (12,12))","deac6ed8":"# accuracy_score\ndict_map = {'<=50K' : 0, '>50K' : 1}\naccuracy_xgb = accuracy_score(pd.Series(y_val).map(dict_map), pd.Series(y_predict_xgb).map(dict_map))\n\nprint(f\"Accuracy Score for Extreme Gradient Boosting Classifier: {accuracy_xgb}\")","6e82a38d":"#classification_report\nprint(classification_report(y_val, y_predict_xgb))","9eafe682":"dict_map = {'<=50K' : 0, '>50K' : 1}\nroc_auc_xgb = roc_auc_score(pd.Series(y_val).map(dict_map), pd.Series(y_predict_xgb).map(dict_map))\n\nprint(f\"ROC-AUC Score for Extreme Gradient Boosting Classifier: {roc_auc_xgb}\")","ea5a05a5":"#confusion_matrix\n_ = mt.plot_confusion_matrix(y_val,y_predict_xgb, normalize = False, figsize = (12,12))","7eea8da7":"# accuracy_score\ndict_map = {'<=50K' : 0, '>50K' : 1}\naccuracy_mlp = accuracy_score(pd.Series(y_val).map(dict_map), pd.Series(y_predict_mlp).map(dict_map))\n\nprint(f\"Accuracy Score for Neural Network Classifier: {accuracy_mlp}\")","3933f1d2":"#classification_report\nprint(classification_report(y_val, y_predict_mlp))","7eebfbc4":"dict_map = {'<=50K' : 0, '>50K' : 1}\nroc_auc_mlp = roc_auc_score(pd.Series(y_val).map(dict_map), pd.Series(y_predict_mlp).map(dict_map))\n\nprint(f\"ROC-AUC Score for Neural Network Classifier: {roc_auc_mlp}\")","cbd4ad84":"#confusion_matrix\n_ = mt.plot_confusion_matrix(y_val,y_predict_mlp, normalize = False, figsize = (12,12))","d2fbe5ed":"metrics = [\n            ['KNN', 0.86317, 0.82, 0.79, 0.80, 0.786],\n            ['SVC', 0.87134, 0.84, 0.78, 0.80, 0.779],\n            ['Random Forest Classifier', 0.86379, 0.83, 0.77, 0.80, 0.775],\n            ['Extreme Gradient Boosting Classifier', 0.8736, 0.84, 0.80, 0.81, 0.797],\n            ['Neural Network Classifier', 0.85549, 0.81, 0.77, 0.78, 0.765],\n          ]\n\ndf_metrics = pd.DataFrame(metrics, columns = ['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC Score'])\n\ndf_metrics","798a6494":"# model definition\nbest_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.42914285714285716, max_delta_step=0, max_depth=2,\n              min_child_weight=1, monotone_constraints='()',\n              n_estimators=179, n_jobs=0, num_parallel_tree=1, random_state=42,\n              reg_alpha=0.013237491476160993, reg_lambda=3.169978726120134e-05,\n              scale_pos_weight=1, subsample=1, tree_method='exact',\n              validate_parameters=1, verbosity=None)\n\n# fitting model\nbest_model.fit(X, y)","8575edff":"# predict\ntest['income'] = best_model.predict(test.drop(columns = 'Id'))\n\ntest.head()","503a24e8":"export_path = \"submission.csv\"\ntest[['Id', 'income']].to_csv(export_path, index = False)","0a585896":"## 4. Exploratory Data Analysis\n\nHere the data characteristics will be summarized and understood from plots and statistical inferences.\n\n### 4.1. Univariate Analysis\n\n#### 4.1.1. Response Variable (income)\n\nPlotting a barplot to understand how the target variable is distributed. It is possible to see that the training dataset is unbalanced relative to the target labels, as the people who have a income <=50K is the majority when compared to those who have income >50K.","c924c247":"### 8.5. Neural Network Classifier\n\nFrom RandomizedSearchCV, we see that the best Neural Network Classifier estimator is the one with the following parameters:\n\n* hidden_layer_sizes = (32, 128)\n* alpha = 2.3488812958533118e-07\n* learning_rate = constant\n\nThis set of parameters gives a cross-validation score of 0.854.","0097b61d":"### 3.4. Descriptive statistical\n\nIn this section we will have a statistical overview in the training dataset, considering numerical and categorical variables.","3c0c0d1d":"### 7.1. Rescaling\n\nDue to the outliers, we will use RobustScaler to rescale the numeric data, since it is not sensitive to outliers.","de16b555":"## 10. Fitting the model using all the data, best model and best parameters","be7bab2a":"## 7. Data Preparation","430505f8":"### 9.5. Neural Network Classifier","7bbafe42":"#### 4.2.3. education - income comparison\n\nFrom the data, we see that the majority of people with income less than 50K has only completed High School or some college, while people with bachelor dominates in the higher income public.","0982920c":"### 8.2. Support Vector Classifier (SVC)\n\nFrom RandomizedSearchCV, we see that the best SVC estimator is the one with the following parameters:\n\n* C = 8.985102040816326\n* gamma = 'auto'\n\nThis set of parameters gives a cross-validation score of 0.869.","959795df":"## 2. Loading Dataset","084067e0":"#### 4.1.3. Categorical Variable\n\nFrom the data, we can infere:\n\n* **workclass**: the majority of the public studied in private schools\n* **education**: the most significant categories are HS-grad, Bachelors and Some College\n* **marital.status**: Married-civ-spouse and Never-married are predominant\n* **occupation**: this features has a well distributed data\n* **relationship**: Husband predominates\n* **race**: most part of the public is white\n* **sex**: more men were asked than women\n* **native.country**: clear predominance of american people\n* **education.num**: class 9 predominates","649f97bd":"### 8.3. Random Forest Classifier\n\nFrom RandomizedSearchCV, we see that the best Random Forest Classifier estimator is the one with the following parameters:\n\n* criterion = 'entropy'\n* max_depth = 20\n* n_estimators = 380\n\nThis set of parameters gives a cross-validation score of 0.861.","f037b35e":"### 9.2. Support Vector Classifier (SVC)","803b9ca4":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es\n\nTesting kNN with adult database from UCI repository.\n\n**Author:** Lucas Hideki Takeuchi Okamura\n\n**Hash:** 50\n\n## 1. Imports","e00ff821":"### 9.3. Random Forest Classifier","ae8dc1bf":"### 9.1. KNN\n","c0ae0d21":"## 9. Models Performance\n\nThis section will evaluate each model trained with some specific metrics, such as precision, recall, f1-score and its respectives confusion matrix.","865acc1e":"It can be seen that Extreme Gradient Boosting Classifier returned the best cross validation score between all the classifiers. Thus, it will be used as the final model to predict the test dataset.","8bbd16be":"## 5. Feature Engineering\n\nWe will create a variable called \"capital\", which is calculated by capital.gain - capital.loss:","6453df5b":"Although education.num is a numeric variable, it represents the category relative to the person education level, so it will be treated as a categorical variable.\n\n#### 3.4.2. Categorical Attributes\n\nHere we can see how many categories each variable has, showing that native.country is the feature with most categories.","9b02726f":"### 9.4. Extreme Gradient Boosting Classifier","4bb52a45":"### 4.3. Multivariate Analysis\n\nFrom the multivariate analysis, we look for features that are not correlated to others features and features that are somehow correlated to the target, in order to predict the result more accurately basing on those correlations. From the heatmap, we see that none of the features have a high correlation to each other and age, education.num, capital.gain and hours.per.week have decent correlation to the target variable income, indicating that they are good variables to the model.","7533c75f":"We see from the datasets that the income column is the target of the problem and that's what we want to predict.\n\n## 3. Data Description\n\nIn this section we will see a overview of the datasets, considering these subsections:\n\n* Data Dimensions\n* Data Types\n* Check missing values\n* Fillout missing values\n* Descriptive Statistical\n\n### 3.1. Data Dimensions","e62bd83d":"### 3.2. Data Types\n\nWe can see that the income label is categorical, thus we have a classification problem.","8326951b":"### 3.3. Check and fill missing values\n\nFrom the training data, there are missing data in occupation, workclass and native.country. We will fill them out with their most common values.","2613aefc":"## 6. Feature Selection\n\nSince the majority of the public is from United States, we will drop native.country, that will not bring a lot of information. And since we created a new variable \"capital\", we will drop \"capital.gain\" and \"capital.loss\".","d2ce29ae":"## 11. Predicting","4092a4ae":"#### 3.4.1. Numerical Attributes","54935a01":"From the metrics in the validation dataset, it can be seen that the best model is the Extreme Gradient Boosting Classifier, which will then be used to predict values on the test dataset.","3af60ea1":"### 8.1. KNN\n\nFrom RandomizedSearchCV, we see that the best kNN estimator is the one with 14 neighbors, which has a cross-validation score of 0.861.","68de42ee":"### 4.2. Bivariate Analysis\n\nNow, some features will be evaluated relative to their relation to the target variable income.\n\n#### 4.2.1. sex - income comparison\n\nFrom the graphs we see that the proportion between men and woman is different when the income is considered. While there are, proportionaly more men than women in the >50K class, this proportion reduces when the class <=50K is considered, evidencing that men are more prone to have a income bigger than 50K.","eb78d7e9":"### 7.2. Encoding\n\nWe will apply One-Hot Encoding to the following features:\n\n* **OneHotEncoder**: sex, workclass, education, marital.status, occupation, relationship, race\n\neducation.num is already encoded.","669bd8f1":"### 8.4. Extreme Gradient Boosting Classifier\n\nFrom RandomizedSearchCV, we see that the best Extreme Gradient Boosting Classifier estimator is the one with the following parameters:\n\n* n_estimators = 179\n* learning_rate = 0.42914285714285716\n* max_depth = 2\n* reg_alpha = 0.013237491476160993\n* reg_lambda = 3.169978726120134e-05\n\nThis set of parameters gives a cross-validation score of 0.871.","ec3a0959":"### 9.6. Score Summarization\n\nTo compare each classifier, a consolidated table is elaborated with the pertinent metrics for the validation dataset metrics.","f8c63a38":"#### 4.2.2. age - income comparison\n\nAlthough the correlation between age and income is quite low, it is possible to see a slight trend that older people have higher incomes.","1df621b5":"#### 4.1.2. Numerical Variable\n\nWe can infere some informations from the graphs:\n\n* **age**: most part of the people have less than 60 years\n* **fnlwgt**: almost all the data concentrated in values less than $0.75 * 10^{6}$, despite some outliers\n* **capital.gain** and **capital.loss**: values are concentrated near 0, despite some outliers\n* **hours.per.week**: values are concentrated near 40, an it has a lot of outliers","601f3044":"## 8. Machine Learning Modelling and Hyperparameter Tunning\n\nIn this section, different machine learning algorithms will be evaluated to classify the adult base. They will be:\n\n* KNN\n* Support Vector Classifier (SVC)\n* Random Forest Classifier\n* Extreme Gradient Boosting Classifier \n* Neural Network Classifier","9979a26f":"## 9. Exporting result"}}