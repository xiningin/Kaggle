{"cell_type":{"23e9826b":"code","634425ef":"code","f348c967":"code","ff4ec6de":"code","6bc67627":"code","6685170b":"code","a083d12e":"code","827e1e5c":"code","912790d8":"code","baf59847":"code","3071a081":"code","5b61adc1":"code","a30d758c":"code","0b9ec9b0":"code","68732501":"code","479dffe3":"code","e919e5f5":"code","34f90311":"code","6b21cd60":"code","10b75bb6":"code","34ab2f38":"code","0245987d":"code","e01da35f":"code","d71a3593":"code","da6bbd13":"code","5ced2341":"markdown","3821956b":"markdown","1c1e1f45":"markdown","e2f10f72":"markdown","9c7a0e9b":"markdown","33eab1a5":"markdown","34323263":"markdown","3aa64dbe":"markdown","65852eb3":"markdown","2b2a3e47":"markdown","845554ad":"markdown","a5043ec0":"markdown"},"source":{"23e9826b":"import tensorflow as tf \n\n### Running this script using GPU is recommended\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import Counter\nimport random\nimport matplotlib.pyplot as plt #library for visualiation\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.callbacks import *\nimport keras.backend as K\nfrom keras.models import load_model\n\nimport seaborn as sns\n\n#For data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\n# Listing current data on our folder.\nimport os\nprint(os.listdir(\".\"))","634425ef":"pip install music21","f348c967":"#library for understanding music\nfrom music21 import *\ndef read_midi(file):\n    \n    print(\"Loading Music File:\",file)\n    \n    notes=[]\n    notes_to_parse = None\n    \n    #parsing a midi file\n    midi = converter.parse(file)\n  \n    #grouping based on different instruments\n    s2 = instrument.partitionByInstrument(midi)\n\n    #Looping over all the instruments\n    for part in s2.parts:\n    \n        #select elements of only piano\n        if 'Piano' in str(part): \n        \n            notes_to_parse = part.recurse() \n      \n            #finding whether a particular element is note or a chord\n            for element in notes_to_parse:\n                \n                #note\n                if isinstance(element, note.Note):\n                    notes.append(str(element.pitch))\n                \n                #chord\n                elif isinstance(element, chord.Chord):\n                    notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return np.array(notes)","ff4ec6de":"#specify the path\n#path='..\/input\/maestropianomidi\/maestro-v3.0.0\/2018\/'\npath = '..\/input\/beethoven-midi\/'\n\n#read all the filenames\nfiles=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n\n#reading each midi file\nnotes_array = np.array([read_midi(path+i) for i in files])","6bc67627":"#converting 2D array into 1D array\nnotes_ = [element for note_ in notes_array for element in note_]\n\n#No. of unique notes\nunique_notes = list(set(notes_))\nprint(len(unique_notes))","6685170b":"#computing frequency of each note\nfreq = dict(Counter(notes_))\n\n#consider only the frequencies\nno=[count for _,count in freq.items()]\n\n#set the figure size\nplt.figure(figsize=(5,5))\n\n#plot\nplt.hist(no)","a083d12e":"frequent_notes = [note_ for note_, count in freq.items() if count>=50]\nprint(len(frequent_notes))","827e1e5c":"new_music=[]\n\nfor notes in notes_array:\n    temp=[]\n    for note_ in notes:\n        if note_ in frequent_notes:\n            temp.append(note_)            \n    new_music.append(temp)\n    \nnew_music = np.array(new_music)","912790d8":"no_of_timesteps = 32\nx = []\ny = []\n\nfor note_ in new_music:\n    for i in range(0, len(note_) - no_of_timesteps, 1):\n        \n        #preparing input and output sequences\n        input_ = note_[i:i + no_of_timesteps]\n        output = note_[i + no_of_timesteps]\n        \n        x.append(input_)\n        y.append(output)\n        \nx=np.array(x)\ny=np.array(y)","baf59847":"unique_x = list(set(x.ravel()))\nx_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))","3071a081":"#preparing input sequences\nx_seq=[]\nfor i in x:\n    temp=[]\n    for j in i:\n        #assigning unique integer to every note\n        temp.append(x_note_to_int[j])\n    x_seq.append(temp)\n    \nx_seq = np.array(x_seq)","5b61adc1":"unique_y = list(set(y))\ny_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \ny_seq=np.array([y_note_to_int[i] for i in y])","a30d758c":"#train test split\nx_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)","0b9ec9b0":"K.clear_session()\nmodel = Sequential()\n    \n#embedding layer\nmodel.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n\nmodel.add(Conv1D(64,3, padding='causal',activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(MaxPool1D(2))\n    \nmodel.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\nmodel.add(Dropout(0.2))\nmodel.add(MaxPool1D(2))\n\nmodel.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\nmodel.add(Dropout(0.2))\nmodel.add(MaxPool1D(2))\n          \n#model.add(Conv1D(256,5,activation='relu'))    \nmodel.add(GlobalMaxPool1D())\n    \nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(len(unique_y), activation='softmax'))\n    \nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n\nmodel.summary()","68732501":"tf.keras.utils.plot_model(model, show_shapes=True)","479dffe3":"history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1)#, callbacks=[mc])","e919e5f5":"#checkpoint\nmc=ModelCheckpoint('model_wavenet.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)","34f90311":"model.save('model_wavenet.h5')\nprint('Wavenet model saved')","6b21cd60":"#loading best model\nmodel = load_model('model_wavenet.h5')","10b75bb6":"ind = np.random.randint(0,len(x_val)-1)\n\nrandom_music = x_val[ind]\n\npredictions=[]\nfor i in range(10):\n\n    random_music = random_music.reshape(1,no_of_timesteps)\n\n    prob  = model.predict(random_music)[0]\n    y_pred= np.argmax(prob,axis=0)\n    predictions.append(y_pred)\n\n    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n    random_music = random_music[1:]\n    \nprint(predictions)","34ab2f38":"x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \npredicted_notes = [x_int_to_note[i] for i in predictions]","0245987d":"def convert_to_midi(prediction_output):\n   \n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        \n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                \n                cn=int(current_note)\n                new_note = note.Note(cn)\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n                \n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n            \n        # pattern is a note\n        else:\n            \n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 1\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='music_wavenet.mid')","e01da35f":"convert_to_midi(predicted_notes)","d71a3593":"import keras.backend as K\n\ndef f1_score(precision, recall):\n    ''' Function to calculate f1 score '''\n    \n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val\n\n# Evaluate model on the test set\nloss, accuracy, precision, recall = model.evaluate(x_val, y_val, verbose=0)\n# Print metrics\nprint('')\nprint('Accuracy  : {:.4f}'.format(accuracy))\nprint('Precision : {:.4f}'.format(precision))\nprint('Recall    : {:.4f}'.format(recall))\nprint('F1 Score  : {:.4f}'.format(f1_score(precision, recall)))\n\ndef plot_training_hist(history):\n    '''Function to plot history for accuracy and loss'''\n    \n    fig, ax = plt.subplots(1, 2, figsize=(10,4))\n    # first plot\n    ax[0].plot(history.history['accuracy'])\n    ax[0].plot(history.history['val_accuracy'])\n    ax[0].set_title('Model Accuracy')\n    ax[0].set_xlabel('epoch')\n    ax[0].set_ylabel('accuracy')\n    ax[0].legend(['train', 'validation'], loc='best')\n    # second plot\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('Model Loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].set_ylabel('loss')\n    ax[1].legend(['train', 'validation'], loc='best')\n    \nplot_training_hist(history)","da6bbd13":"K.clear_session()\nmodel2 = Sequential()\n#embedding layer\nmodel.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \nmodel2.add(LSTM(128,return_sequences=True))\nmodel2.add(LSTM(128))\nmodel2.add(Dense(256))\nmodel2.add(Activation('relu'))\nmodel2.add(Dense(len(unique_x)))\nmodel2.add(Activation('softmax'))\nmodel.build(input_shape)\nmodel2.summary()\nmodel2.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n\nmc2=ModelCheckpoint('lstm_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n\nhistory = model2.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc2])","5ced2341":"We have generated a 'music_wavenet.mid' file which is an is a piano tune generated using wavenet on the dataset.","3821956b":"## Defining Model\n\nFirst, we will use the wavenet architecture for generation.","1c1e1f45":"## Training Model","e2f10f72":"## Converting back to MIDI","9c7a0e9b":"### Using Wavenet","33eab1a5":"## Music Generation using WaveNet\n\nApproach for automatic music generation based on the WaveNet\n\nThe building blocks of WaveNet are **Causal Dilated 1D Convolution layers**.\n\n### The Workflow of WaveNet:\n\n  * Input is fed into a causal 1D convolution\n  * The output is then fed to 2 different dilated 1D convolution layers with sigmoid and tanh activations\n  *  The element-wise multiplication of 2 different activation values results in a skip connection\n  *  And the element-wise addition of a skip connection and output of causal 1D results in the residual","34323263":"## Dependencies \nFirst, let's download and install dependencies, and import the relevant packages.","3aa64dbe":"## Using LSTM","65852eb3":"## Make predictions","2b2a3e47":"## Preprocessing","845554ad":"## Loading the dataset\n\nMIDI is a standard format for storing music files. MIDI stands for Musical Instrument Digital Interface. MIDI files contain the instructions rather than the actual audio. Hence, it occupies very little memory. That\u2019s why it is usually preferred while transferring files.","a5043ec0":"`music21` is a Toolkit for Computer-Aided Musicology and Symbolic Music Data. It was developed at MIT. \nWe will use this for understaning our music."}}