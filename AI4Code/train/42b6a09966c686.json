{"cell_type":{"90923189":"code","683eda23":"code","20441ebb":"code","de2571d5":"code","dc3103d5":"code","1f6acd24":"code","75411467":"markdown","bd917726":"markdown","9c7798e1":"markdown","98e838c9":"markdown","973a982c":"markdown","6fcce282":"markdown"},"source":{"90923189":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error","683eda23":"train = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\nTARGET = 'num_sold'\nFEATURES = train.columns.difference([TARGET,'date'])","20441ebb":"for feature in ['country', 'product', 'store'] :\n    le = LabelEncoder()\n    train[feature] = le.fit_transform(train[feature])\n    \ntrain['date'] = pd.to_datetime(train['date'])\ntrain.head(3)","de2571d5":"N_split = 2\nkf = KFold(n_splits=N_split)\n\nimport optuna\n\ndef objective(trial):\n    params = {\n                #'iterations' : 10000, replaced by early stopping\n                'eval_metric': 'SMAPE', \n                'use_best_model': True,\n                'random_seed' : 1,\n                'learning_rate' :trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n                \"depth\": trial.suggest_int(\"depth\", 1, 15),\n                'l2_leaf_reg' :trial.suggest_loguniform('l2_leaf_reg', 1e-8, 20),\n                'random_strength' : trial.suggest_loguniform('random_strength', 1, 50),\n                'grow_policy':trial.suggest_categorical ('grow_policy', ['Lossguide','SymmetricTree']),\n                'max_bin': trial.suggest_int(\"max_bin\", 20, 500),\n                'min_data_in_leaf':trial.suggest_int('min_data_in_leaf', 1, 100),\n                \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\"])\n            }\n    \n    if params['grow_policy'] == 'Lossguide':\n        params['max_leaves']:trial.suggest_int('max_leaves', 1, 100)\n    if params[\"bootstrap_type\"] == \"Bayesian\":\n        params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif params[\"bootstrap_type\"] == \"Bernoulli\":\n        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n        \n    \n    score_list = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(train, train[TARGET])):\n        X_tr = train.loc[train_idx][FEATURES]\n        X_va = train.iloc[val_idx][FEATURES]\n        \n        # Preprocess the data\n        X_tr_f = X_tr # Use the correct preprocessing if required (normalization...)\n        y_tr = train[TARGET].loc[train_idx].values\n\n        X_va_f = X_va # Use the correct preprocessing if required (normalization...)\n        y_va = train[TARGET].loc[val_idx].values\n\n        # Train the model\n        model = CatBoostRegressor(**params) \n        model.fit(        \n                X_tr_f,\n                y_tr,\n                eval_set =[( X_va_f,y_va)],\n                verbose =0,\n                early_stopping_rounds = 200)\n\n        # Predictions\n        y_va_pred = model.predict(X_va_f)\n        score = mean_squared_error(y_va, y_va_pred,squared = True)\n        score_list.append(score)\n        \n    return sum(score_list) \/ len(score_list)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)","dc3103d5":"N_split = 2\nkf = KFold(n_splits=N_split)\n\nimport optuna\n\ndef objective(trial):\n   \n    params = {\n            'tree_method': 'hist',\n            'grow_policy' : trial.suggest_categorical ('grow_policy', ['lossguide','depthwise']),\n            'learning_rate':trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n            'max_depth': trial.suggest_int('max_depth', 3, 20),# a virer avec'depthwise'\n            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1, 10),\n            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-15, 10.0),\n            'max_delta_step':trial.suggest_int('max_delta_step', 1, 10),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n            'colsample_bytree':trial.suggest_loguniform('colsample_bytree', 0.4, 1.0),\n            'subsample': trial.suggest_loguniform('subsample', 0.4, 1.0),\n            'seed':1\n                }\n    if params['grow_policy'] == 'lossguide':\n        params['max_leaves'] = trial.suggest_int('max_leaves', 1, 100)   \n        \n        \n    score_list = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(train, train[TARGET])):\n        X_tr = train.loc[train_idx][FEATURES]\n        X_va = train.iloc[val_idx][FEATURES]\n        \n        # Preprocess the data\n        X_tr_f = X_tr # Use the correct preprocessing if required (normalization...)\n        y_tr = train[TARGET].loc[train_idx].values\n        data_tr = xgb.DMatrix(X_tr_f, label=y_tr)\n\n        X_va_f = X_va # Use the correct preprocessing if required (normalization...)\n        y_va = train[TARGET].loc[val_idx].values\n        data_va = xgb.DMatrix(X_va_f, label=y_va)\n        evallist = [(data_va, 'eval'), (data_tr, 'train')]\n\n        # Train the model\n        model = xgb.train(params, \n                          data_tr,\n                          num_boost_round=2000,\n                          evals = evallist,\n                          verbose_eval=0,\n                          early_stopping_rounds = 200)\n\n        # Predictions\n        y_va_pred = model.predict(data_va)\n        score = mean_squared_error(y_va, y_va_pred,squared = True)\n        score_list.append(score)\n        \n    return sum(score_list) \/ len(score_list)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)","1f6acd24":"import warnings\nwarnings.filterwarnings('ignore')\n\nN_split = 2\nkf = KFold(n_splits=N_split)\n\nimport optuna\n\ndef objective(trial):\n    params = {\n            'objective': 'regression',\n            'metric': 'mse',\n            'learning_rate': trial.suggest_float(\"learning_rate\", 0.04,0.4),\n            'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n            'colsample_bytree':trial.suggest_float(\"colsample\", 0.1,0.3),\n            'subsample': trial.suggest_float(\"subsample\", 0.1,0.3),\n            'max_depth': trial.suggest_int('max_depth', 3, 100),\n            'min_child_samples': trial.suggest_int('min_child_samples', 3, 2000),\n            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n            'cat_smooth':trial.suggest_int('cat_smooth', 1, 100)\n            }\n    \n    score_list = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(train, train[TARGET])):\n        X_tr = train.loc[train_idx][FEATURES]\n        X_va = train.iloc[val_idx][FEATURES]\n        \n        # Preprocess the data\n        X_tr_f = X_tr # Use the correct preprocessing if required (normalization...)\n        y_tr = train[TARGET].loc[train_idx].values\n\n        X_va_f = X_va # Use the correct preprocessing if required (normalization...)\n        y_va = train[TARGET].loc[val_idx].values\n\n        # Train the model\n        model = lgb.LGBMRegressor(**params) \n        model.fit(        \n                X_tr_f,\n                y_tr,\n                eval_set =[( X_va_f,y_va)],\n                verbose =0,\n                early_stopping_rounds = 200)\n\n        # Predictions\n        y_va_pred = model.predict(X_va_f)\n        score = mean_squared_error(y_va, y_va_pred,squared = True)\n        score_list.append(score)\n        \n    return sum(score_list) \/ len(score_list)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)","75411467":"<h1> XGBoost","bd917726":"<h1> LGBMRegressor","9c7798e1":"# Usual advices :\n* XGB and LGB have 2 kind of API, the original and the scikit learn API, with some differences. \n* Prepare your data first (feature engineering, encoding, missing value...)\n* Be close as possible your CV strategy\n* Choose carefully your parameter range (according to the dataset)\n* Choose the correct metric and type of target (classification, regression)\n* Check whether your best parameters found are not to close to the range you gave (don't limit the tuning)\n* Use GPU when possible\n* Adapt the size of the dataset or CV strategy to the volume of the data","98e838c9":"![image.png](attachment:04ccd3f7-97bf-4e20-aa64-577a2a7ffa20.png)","973a982c":"<h1> CatBoostRegressor","6fcce282":"<h1> Minimal preprocessing"}}