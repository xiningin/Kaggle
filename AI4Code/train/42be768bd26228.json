{"cell_type":{"27bb19af":"code","628bd74f":"code","1b1a4000":"code","26b055e4":"code","86511f33":"code","d131f173":"code","daf573ec":"code","980e0dd6":"code","0a5acf73":"code","075ecabe":"code","21430b96":"code","7986ca34":"code","685499e4":"code","ab1a4717":"code","35396fff":"code","000fe0c6":"code","218ada24":"code","56504de4":"code","78f1f683":"code","9a6944c6":"code","84cd2aa3":"code","f65d7e83":"code","7906554a":"code","a87d9ffe":"code","0059367c":"code","10e166ad":"code","e0f90f6d":"code","5291c48b":"code","07078ada":"code","c0bfbb5f":"code","8f669510":"code","732e5e8e":"code","0dcb1517":"code","c998bbe2":"code","c186358d":"code","43621313":"code","d52eef7e":"code","4c7d5e85":"code","230341b2":"code","ea38fa89":"code","ec96964d":"code","c677d13a":"code","ee3de6d5":"code","9187c6b6":"code","e304ba62":"code","c36640d2":"markdown","19aff533":"markdown","d002cfc7":"markdown","3b984a0b":"markdown","758ff856":"markdown","45f94929":"markdown","145dd12c":"markdown","99ac1f21":"markdown","20114e9e":"markdown","0518b0d3":"markdown","051929ef":"markdown","5aef09ea":"markdown","adb2b73f":"markdown","2860b231":"markdown","cf79a5e1":"markdown","810968c0":"markdown","0d7ce33d":"markdown","92018fa9":"markdown","b82e0f93":"markdown","47f34425":"markdown","60b28027":"markdown","a8687f9d":"markdown","77b6cd03":"markdown","b29a5468":"markdown","852105e4":"markdown","5af0c9d5":"markdown","16a86825":"markdown","1861b38f":"markdown","06f10835":"markdown"},"source":{"27bb19af":"import numpy as np # linear algebra\nimport csv as csv\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport os, sys\nimport numpy as np\nfrom datetime import datetime\nfrom collections  import Counter\nfrom nltk import word_tokenize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport calendar\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import word2vec\nfrom sklearn.manifold import TSNE\nfrom nltk import pos_tag\nfrom nltk.help import upenn_tagset\nimport gensim\nimport matplotlib.colors as mcolors\nfrom nltk import jaccard_distance\nfrom nltk import ngrams\nplt.style.use('ggplot')\n\n\nimport spacy\nfrom spacy import displacy\nimport nltk\nimport xml.etree.cElementTree as ET\nfrom collections import OrderedDict\nimport json\nimport networkx as nx","628bd74f":"bulletins=os.listdir(\"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\/\")\nadditional=os.listdir(\"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Additional data\/\")","1b1a4000":"files=[dir for dir in os.walk('..\/input\/data-science-for-good-city-of-los-angeles\/CityofLA\/CityofLA\/')]\nfor file in files:\n    print(os.listdir(file[0]))\n    print(\"\\n\")","26b055e4":"csvfiles=[]\nfor file in additional:\n    if file.endswith('.csv'):\n        print(file)\n        csvfiles.append(\"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Additional data\/\"+file)","86511f33":"job_title=pd.read_csv(csvfiles[2])\nsample_job=pd.read_csv(csvfiles[0])\nkaggle_data=pd.read_csv(csvfiles[1])","d131f173":"job_title.head()","daf573ec":"def get_headings(bulletin):       \n    \n    \"\"\"\"function to get the headings from text file\n        takes a single argument\n        1.takes single argument list of bulletin files\"\"\"\n    \n    with open(\"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\/\"+bulletins[bulletin]) as f:    ##reading text files \n        data=f.read().replace('\\t','').split('\\n')\n        data=[head for head in data if head.isupper()]\n        return data\n        \ndef clean_text(bulletin):      \n    \n    \n    \"\"\"function to do basic data cleaning\n        takes a single argument\n        1.takes single argument list of bulletin files\"\"\"\n                                            \n    \n    with open(\"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\/\"+bulletins[bulletin]) as f:\n        data=f.read().replace('\\t','').replace('\\n','')\n        return data","980e0dd6":"get_headings(1)","0a5acf73":"def to_dataframe(num,df):\n    \"\"\"\"function to extract features from job bulletin text files and convert to\n    pandas dataframe.\n    function take two arguments \n                        1.the number of files to be read\n                        2.dataframe object                                      \"\"\"\n    \n\n    \n    opendate=re.compile(r'(Open [D,d]ate:)(\\s+)(\\d+-\\d\\d-\\d\\d)')       #match open date\n    \n    salary=re.compile(r'\\$(\\d+,\\d+)((\\s(to|and)\\s)(\\$\\d+,\\d+))?')       #match salary\n    \n    requirements=re.compile(r'(REQUIREMENTS?\/\\s?MINIMUM QUALIFICATIONS?)(.*)(PROCESS NOTE)')      #match requirements\n    \n    apply=re.compile(r'(WHERE TO APPLY?)(.*)(NOTE)')\n    \n    for no in range(0,num):\n        with open(\"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\/\"+bulletins[no],encoding=\"ISO-8859-1\") as f:         #reading files \n                try:\n                    file=f.read().replace('\\t','')\n                    data=file.replace('\\n','')\n                    headings=[heading for heading in file.split('\\n') if heading.isupper()]             ##getting heading from job bulletin\n\n                    try:\n                        date=datetime.strptime(re.search(opendate,data).group(3),'%m-%d-%y')\n                    except Exception as e:\n                        date=np.nan\n                    try:\n                        req=re.search(requirements,data).group(2)\n                    except Exception as e:\n                        try: \n                            req=re.search('(.*)POST-Certified',re.findall(r'(REQUIREMENTS?)(.*)(POST-Certified)', \n                                                                  data)[0][1][:1200]).group(1)\n                        except Exception as e:\n                            req=re.search('(.*)NOTES?',re.findall(r'(REQUIREMENTS?)(.*)(NOTES?)',\n                                                              data)[0][1][:1200]).group(1)\n                    \n                    try:\n                        duties=re.search(r'(DUTIES)(.*)(REQ[A-Z])',data).group(2)\n                        \n                    except Exception as e:\n                        duties=np.nan\n                    \n                    app=re.search(apply,data).group(2)\n                    \n                    try:\n                        enddate=re.search(\n                                r'(JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)\\s(\\d{1,2},\\s\\d{4})'\n                                ,data).group()\n                    except Exception as e:\n                        enddate=np.nan\n                        \n                    sal=re.search(salary,data)\n                    selection= [z[0] for z in re.findall('([A-Z][a-z]+)((\\s\\.\\s)+)',data)]     ##match selection criteria\n                    try:\n                        sal=re.search(salary,data)\n                        df=df.append({'FILE_NAME':bulletins[no],'Position':headings[0].lower(), 'salary_start':sal.group(1),'salary_end':sal.group(5),\n                                  \"opendate\":date,\"requirements\":req, 'duties':duties, 'apply':app,#'duties':duties, 'apply':app,\n                                  'deadline':enddate},ignore_index=True)  #,'selection':selection\n                    \n                    except Exception as e:\n                        sal=np.nan\n                        df=df.append({'FILE_NAME':bulletins[no],'Position':headings[0].lower(), 'salary_start':sal,'salary_end':sal,\n                                  \"opendate\":date, \"requirements\":req, 'duties':duties, 'apply':app,#\"requirements\":req,'duties':duties, 'apply':app,\n                                  'deadline':enddate, 'selection':selection},ignore_index=True)  #,'selection':selection\n                    \n                    #selection= [z[0] for z in re.findall('([A-Z][a-z]+)((\\s\\.\\s)+)',data)]     ##match selection criteria\n                    \n                    #df=df.append({'File Name':bulletins[no],'Position':headings[0].lower(), 'salary_start':sal.group(1),'salary_end':sal.group(5),\n                    #              \"opendate\":date,#\"requirements\":req,'duties':duties, 'apply':app,\n                    #              'deadline':enddate},ignore_index=True)  #,'selection':selection\n                    \n                    \n                    reg=re.compile(r'(One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|one|two|three|four)\\s(years?)\\s(of\\sfull(-|\\s)time)')\n                    df['EXPERIENCE_LENGTH']=df['requirements'].apply(lambda x :  re.search(reg,x).group(1) if re.search(reg,x) is not None  else np.nan)\n                    df['FULL_TIME_PART_TIME']=df['EXPERIENCE_LENGTH'].apply(lambda x:  'FULL_TIME' if x is not np.nan else np.nan )\n                    \n                    reg=re.compile(r'(One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|one|two|three|four)(\\s|-)(years?)\\s(college)')\n                    df['EDUCATION_YEARS']=df['requirements'].apply(lambda x :  re.search(reg,x).group(1) if re.search(reg,x) is not None  else np.nan)\n                    df['SCHOOL_TYPE']=df['EDUCATION_YEARS'].apply(lambda x : 'College or University' if x is not np.nan else np.nan)\n                    \n                #except Exception as e:\n                #    print('umatched sequence')\n                #    print(f)\n                except IOError:\n                    print('An error occured trying to read the file.')\n    \n                except ValueError:\n                    print('Non-numeric data found in the file.')\n\n                except ImportError:\n                    print (\"NO module found\")\n    \n                except EOFError:\n                    print('Why did you do an EOF on me?')\n                except KeyboardInterrupt:\n                    print('You cancelled the operation.')\n\n                except Exception as e:\n                    print(e)\n                    print(f)\n                \n                \n        \n           \n    return df","075ecabe":"df=pd.DataFrame(columns=['FILE_NAME','Position','salary_start','salary_end', 'opendate', 'requirements', 'duties', 'apply', 'deadline']) #,'duties', 'apply'\ndf=to_dataframe(len(bulletins),df)\n","21430b96":"df.shape","7986ca34":"df.isnull().sum()","685499e4":"df.head()","ab1a4717":"bulletin_dir = \"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\/\"\nadditional_data_dir = \"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Additional data\/\"","35396fff":"headings = {}\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n        for line in f.readlines():\n            line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()\n            \n            if line.isupper():\n                if line not in headings.keys():\n                    headings[line] = 1\n                else:\n                    count = int(headings[line])\n                    headings[line] = count+1","000fe0c6":"del headings['$103,606 TO $151,484'] #This is not a heading, it's an Annual Salary component\nheadingsFrame = []\nfor i,j in (sorted(headings.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)):\n    headingsFrame.append([i,j])\nheadingsFrame = pd.DataFrame(headingsFrame)\nheadingsFrame.columns = [\"Heading\",\"Count\"]\n#headingsFrame.head()","218ada24":"#Check for note components\nnoteHeadings = [k for k in headingsFrame['Heading'].values if 'note' in k.lower()]\nnote_list = []\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n        readNext = 0\n        for line in f.readlines():\n            clean_line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()  \n            if clean_line in noteHeadings:\n                readNext = 1\n            elif readNext == 1:\n                if clean_line in headingsFrame['Heading'].values:\n                    break\n                elif len(clean_line)<2:\n                    continue\n                else:\n                    note_list.append([filename, clean_line])","56504de4":"df_note = pd.DataFrame(note_list)\ndf_note.columns = ['FILE_NAME','NOTE_TEXT']\ndf_note.head()","78f1f683":"file_name = df_note['FILE_NAME'].unique()","9a6944c6":"note_list = []\nfor title in file_name:\n    d = df_note[df_note['FILE_NAME']==title]\n    context = ' '.join(list(d['NOTE_TEXT']))\n    note_list.append([title, context])","84cd2aa3":"df_note = pd.DataFrame(note_list)\ndf_note.columns = ['FILE_NAME','NOTE_TEXT']\ndf_note.head()","f65d7e83":"result = pd.merge(df, df_note, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)","7906554a":"result.head()","a87d9ffe":"#Check for process components\nproHeadings = [k for k in headingsFrame['Heading'].values if 'process' in k.lower()]\npro_list = []\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n        readNext = 0\n        for line in f.readlines():\n            clean_line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()  \n            if clean_line in proHeadings:\n                readNext = 1\n            elif readNext == 1:\n                if clean_line in headingsFrame['Heading'].values:\n                    break\n                elif len(clean_line)<2:\n                    continue\n                else:\n                    pro_list.append([filename, clean_line])","0059367c":"df_pro = pd.DataFrame(pro_list)\ndf_pro.columns = ['FILE_NAME','PROCESS_TEXT']\ndf_pro.head()","10e166ad":"pro_list = []\nfor title in file_name:\n    df = df_pro[df_pro['FILE_NAME']==title]\n    context = ' '.join(list(df['PROCESS_TEXT']))\n    pro_list.append([title, context])","e0f90f6d":"df_pro = pd.DataFrame(pro_list)\ndf_pro.columns = ['FILE_NAME','PROCESS_TEXT']\ndf_pro.head()","5291c48b":"result = pd.merge(result, df_pro, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)","07078ada":"result.head()","c0bfbb5f":"word_list = pd.read_csv('..\/input\/genderbiasdictionary\/genderbiascatalog1.csv')\nword_list","8f669510":"masculine_list = word_list['Biased Catalog']","732e5e8e":"replace_list = word_list['Replace']","0dcb1517":"master_dic = {}\n\nmasculine_list = [w.lower() for w in masculine_list]\ncolumns = ['requirements', 'NOTE_TEXT', 'duties', 'PROCESS_TEXT', 'apply']\n\nfor c in columns:\n    for sentence in result[c]:\n        for word in str(sentence).split():\n            if word.lower() not in master_dic and word.lower() in masculine_list:\n                master_dic[word.lower()] = 1\n            elif word.lower() in master_dic and word.lower() in masculine_list:\n                master_dic[word.lower()] += 1\n","c998bbe2":"master_dic","c186358d":"file_dic = {}\n\nfor name in result['FILE_NAME']:\n    for c in columns:\n        for sentence in result[result['FILE_NAME']==name][c]:\n            for word in str(sentence).split():\n                if name not in file_dic and word.lower() in masculine_list:\n                    file_dic[name] = [word.lower()]\n                elif name in file_dic and word.lower() in masculine_list and word.lower() not in file_dic[name]:\n                    file_dic[name].append(word.lower())","43621313":"file_dic","d52eef7e":"file_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in file_dic.items() ]))\nfile_df_T = file_df.transpose()\nfile_df_T.columns = ['bias_1', 'bias_2', 'bias_3', 'bias_4', 'bias_5', 'bias_6', 'bias_7']\nfile_df_T","4c7d5e85":"def ifef(col):\n    col = str(col)\n    if col in masculine_list:\n        i = masculine_list.index(col)\n        return  replace_list[i]\n    else:\n        return 'NaN'","230341b2":"file_df_T['replace_1'] = file_df_T['bias_1'].apply(ifef)\nfile_df_T['replace_2'] = file_df_T['bias_2'].apply(ifef)\nfile_df_T['replace_3'] = file_df_T['bias_3'].apply(ifef)\nfile_df_T['replace_4'] = file_df_T['bias_4'].apply(ifef)\nfile_df_T['replace_5'] = file_df_T['bias_5'].apply(ifef)\nfile_df_T['replace_6'] = file_df_T['bias_6'].apply(ifef)\nfile_df_T['replace_7'] = file_df_T['bias_7'].apply(ifef)\nfile_df_T = file_df_T[['bias_1', 'replace_1', 'bias_2', 'replace_2', 'bias_3', 'replace_3', 'bias_4', 'replace_4', 'bias_5', 'replace_5', 'bias_6', 'replace_6', 'bias_7', 'replace_7']]\nfile_df_T","ea38fa89":"# Extract Lower Job Class\nreq_pos = result[['Position', 'requirements']].copy()\n# regular expression for extracting job class\njob_regex = r'(?:(?<=experience\\swith\\s)|(?<=experience\\s))(.+?)(?:\\.|\\;)'\n\nclass_dict2 = {}\n\nfor name in req_pos['Position']:\n    for sentence in req_pos[req_pos['Position']==name][\"requirements\"]:\n            job = re.findall(job_regex, sentence)\n            class_dict2[name] = job","ec96964d":"class_dict2","c677d13a":"degree_dict = {}\n#regex for extracting degree\ndegree_regex = r'(?<=degree\\s).+?\\.'\nfor name in req_pos['Position']:\n    for sentence in req_pos[req_pos['Position']==name][\"requirements\"]:\n            degree = re.findall(degree_regex, sentence)\n            degree_dict[name] = degree\n        ","ee3de6d5":"degree_dict","9187c6b6":"class_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in class_dict2.items() ]))\nclass_df = class_df.transpose()\nclass_df.reset_index(inplace=True)\nclass_df = pd.melt(class_df, id_vars=['index'], var_name='number', value_name = 'requirements')\n\nclass_df.head()","e304ba62":"for name in class_df.loc[0:100,'index']:\n    group = class_df.groupby('index').get_group(name)\n    FG = nx.from_pandas_edgelist(group, source='requirements', target='index', edge_attr=True)\n    plt.figure()\n    nx.draw_networkx(FG, with_labels=True)","c36640d2":"## Goal 1 & 2 Identify language that can negatively bias the pool of applicants and improve the diversity and quality of the applicant pool","19aff533":"Merge the new feature \"process\" into previous dataframe and get the final dataframe","d002cfc7":"## Goal 0 Convert a folder full of plain-text job postings into a single structured CSV file\nchecking all subdirectories","3b984a0b":"Extract degree requirements","758ff856":"## Goal 3 Make it easier to determine which promotions are available to employees in each job class\nExtract Lower Job Class","45f94929":"Merge all \"process\" into one for every job posting","145dd12c":"Get replace words list","99ac1f21":"Extracting the headings from job bulletins","20114e9e":"Our submission aims to solve the challenge that the City of Los Angeles will be facing in the upcoming years: How to improve job bulletins so that as many applicants feel encouraged to apply as possible. We have created a method that will analyze word-choice within the job postings to alert any biases that may be present. \n\nResearch shows that men will apply for a job after only meeting 60% of the qualifications on the job description, while women are more hesitant and will only apply after meeting all 100% of the qualification. Therefore, it is clear that using too many masculine-coded words would create a deficit in applications submitted by women. With our method, identifying these word-choices that include a gender bias is fast and reliable. Words that are flagged as gender-coded can be altered with the provided recommendations of gender-neutral words*. The diversity of the applicant pool is improved as we are definite that the job posting\u2019s content, tone, and requirements do not lean towards one gender. The quality of the applicant pool is improved as we will see a rise in qualified applicants applying that were previously discouraged from applying previously. \n\nLastly, our method made it easier to determine which promotions are available to employees in each job class by extracting the requirement of years, lower job class and required degree needed in one class to be able to move to the next. Our data makes it easy to notify an eligible employee once that time range passes. With our method, job-posting biases are eliminated, word-choice is improved to be more inclusive to the population, and employees are more likely to be aware of new career paths that they are eligible for.\n\n*Further scientific support for our method will be provided in the following sections \n\n","0518b0d3":"## Moodbit's Team\nStephanie Hinck, Chang Qu, Ting Cai, Alfredo Jaldin, Yichao Shen\n","051929ef":"checking missing value","5aef09ea":"Headings","adb2b73f":"Reading the required csv files","2860b231":"Count frequencies for bias words","cf79a5e1":"Create Direct Graph","810968c0":"Read in biased words and replacement list","0d7ce33d":"Extracting all \"note\" parts for every job posting","92018fa9":"Extracting all \"process\" parts for every job posting","b82e0f93":"Importing Libraries","47f34425":"# Moodbit\u2019s method to analyze word-choice within the job postings to alert any biases and determine promotions available\n","60b28027":"Extracting Features","a8687f9d":"Check bias words in each job posting file","77b6cd03":"We created our dictionary of replacement words with information provided by the University of North Carolina Chapel Hill\u2019s Writing Center. The institution used research findings to create a list of non-gender nouns to replace those that were previously biased. See link here: \nhttps:\/\/writingcenter.unc.edu\/tips-and-tools\/gender-inclusive-language\/. \nAlong with this list, it also provided us with tips and tools to expand further from this minimal list. Based on UNC\u2019s suggestions, we used a thesaurus to find replacement words for other known biased words. An additional list of words was retrieved from a research study by Danielle Gaucher, Justin Friesen, and Aaron C. Kay \u201cEvidence That Gendered Wording in Job Advertisements Exists and Sustains Gender Inequality.\u201d Accessible here: http:\/\/gender-decoder.katmatfield.com\/static\/documents\/Gaucher-Friesen-Kay-JPSP-Gendered-Wording-in-Job-ads.pdf\nWe used this list of commonly found gender-biased language in job postings and created synonyms for each word\/phrase on it. We ensured that the synonyms we choose did not include any biases by putting it through the insightful gender decoder created by Kat Matfield. Accessible here: http:\/\/gender-decoder.katmatfield.com\/. \n","b29a5468":"Get bias words list","852105e4":"Define function to get replacement for each bias word","5af0c9d5":"Merge all \"note\" into one for every job posting","16a86825":"Merge the new feature \"note\" into previous dataframe","1861b38f":"Show bias words and its replacement for each job posting file","06f10835":"## Recommendation Sections:\n\nDid the authors use the structured data to make an original insight?\nThe structured data was used to identify the number of masculine coded and feminine coded words used throughout all 683 job posts. The structured data was also extracted to obtain the required past experience for a job position in order to assist with the creation of an easy method for showing employees promotion opportunities available.\n\nDid the authors identify and communicate details about something that they discovered in the data?\nWe communicated that the data showed gender-coded words were being used frequently throughout all job class postings. This finding can be contributing to the problem: Biases existing in job postings preventing people from applying. We communicated the need for these words to be evenly distributed throughout a job positing so that all genders feel qualified to apply and not feel un-wanted due to the language choice. We also found that it was difficult to interpret the promotion possibilities from one job to the next with the current structured data. \n\nDid the authors make an actionable recommendation to the City of LA?\nYes. We recommended that the City of LA analyzes the words that they use prior to posting the job for applicants to read. They first should decode the biased words, then use replace them with the gender-neutral words that we have provided, and then can analyze the posting again with our technology to be sure all biases have been eliminated. We also recommend that the City of LA provides more understandable pathways for each employee to be aware of, as well as give notifications to employees when they have reached the year requirement for a promotional opportunity. This would allow more employees to move up to a more distinguished position as well as open up more lower-class jobs to future applicants. \n\nDid the authors make effective use of data visualizations to communicate their recommendations to the City of LA?\nWe showed the high frequency of gender-coded words in the original job postings provided by the City of LA in a bar graph. We were able to demonstrate the most commonly used words in job postings that should be replaced as well as the amount of biased words found in each individual job posting. We also provided at list of new recommendations for the biased words. \n\nDo the recommendations give helpful solutions to one or more of the following issues? Yes, all of them.\n(1) identify language that can bias the pool of applicants \u2013 Yes, with the gender-decoder.\n(2) improve the diversity and quality of the applicant pool; and\/or \u2013 Yes, with the unbiased replacement words provided.\n(3) increase the discoverability of promotional pathways \u2013 Yes, with our ability to extract the requirements from job postings and create an on-time notification system for employees to be aware of when they reach that point. \n"}}