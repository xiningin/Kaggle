{"cell_type":{"9af64efe":"code","060c9415":"code","e2212726":"code","1e508e80":"code","a7c3f122":"code","8a49827c":"code","e9ab8322":"code","349a2427":"code","37f6cb75":"code","6139d673":"code","6de42fb8":"code","6c011672":"markdown","47f30925":"markdown","f02fa423":"markdown","d5344d1e":"markdown","14a0f96f":"markdown","0d822b4e":"markdown","2fff32ff":"markdown","b63a1e9b":"markdown"},"source":{"9af64efe":"import gc\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.models import load_model\n\nfrom tensorflow.keras.callbacks import (\n    ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'","060c9415":"def seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nSEED = 42\nseed_everything(SEED)","e2212726":"develop_df = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ndevelop_target_df = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_df = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ntarget_cols = develop_target_df.columns[1:]\nN_TARGETS = len(target_cols)","1e508e80":"def preprocess_df(df):\n    if 'sig_id' in df.columns:\n        df = df.drop('sig_id', axis=1)\n    df = df.drop('cp_type', axis=1)\n    #df['cp_type'] = (df['cp_type'] == 'trt_cp').astype(int)\n    df['cp_dose'] = (df['cp_dose'] == 'D2').astype(int)\n    df['cp_time'] = df['cp_time'].map({24:0, 48: 1, 72: 2})\n    return df","a7c3f122":"x_develop = preprocess_df(develop_df)\ny_develop = develop_target_df.drop('sig_id', axis=1)\nx_test = preprocess_df(test_df)","8a49827c":"scaler = StandardScaler()\n\nx_develop = pd.DataFrame(scaler.fit_transform(x_develop), columns=x_develop.columns)\nx_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)","e9ab8322":"def create_folds(df, fold_no, fold_type='mls_kfold', save=False):\n    \"\"\"\n    df: target dataframe\n    \"\"\"\n    if fold_type == 'kfold':\n        kf = KFold(n_splits=fold_no, shuffle=True, random_state=SEED)\n    elif fold_type == 'mls_kfold':\n        kf = MultilabelStratifiedKFold(n_splits=fold_no, random_state=SEED)\n        \n    df['Fold'] = -1\n    df.reset_index(inplace=True)\n    for fold, (t, v) in enumerate(kf.split(df, df)):\n        df.loc[v, 'Fold'] = fold\n    df.set_index('sig_id', inplace=True)    \n    if save:\n        df.to_csv('Folds.csv')\n    return df","349a2427":"class MyModel():\n    def __init__(self, input_shape, N_TARGETS):\n        self.input_shape = input_shape\n        self.output_shape = N_TARGETS\n        \n    def create_model(self, output_bias=None):\n        if output_bias is not None:\n            output_bias = tf.keras.initializers.Constant(output_bias)\n        \n        inputs = tf.keras.Input(shape=self.input_shape)\n        x = L.BatchNormalization()(inputs)\n        x = L.Dropout(0.5)(x)\n        x = tfa.layers.WeightNormalization(L.Dense(100, kernel_initializer='he_normal'))(x)\n        x = L.Activation('relu')(x)\n        x = L.BatchNormalization()(x)\n        x = L.Dropout(0.3)(x)\n        x = tfa.layers.WeightNormalization(L.Dense(100, kernel_initializer='he_normal'))(x)\n        x = L.Activation('relu')(x)\n        x = L.BatchNormalization()(x)\n        x = L.Dropout(0.2)(x)\n        outputs = tfa.layers.WeightNormalization(L.Dense(self.output_shape,\n                                                         activation='sigmoid',\n                                                         bias_initializer=output_bias\n                                                        ))(x)\n\n        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n        \n        OPTIMIZER = tfa.optimizers.Lookahead(\n            tfa.optimizers.AdamW(weight_decay=1e-4),\n            sync_period=5)\n        LOSS = tf.keras.losses.BinaryCrossentropy()\n        \n        model.compile(optimizer=OPTIMIZER, loss=LOSS)\n        return model","37f6cb75":"def cv(output_bias):\n    N_FOLDS = 5\n    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=1984)\n\n    models = {i: '' for i in range(N_FOLDS)}\n    oof = {i: '' for i in range(N_FOLDS)}\n\n    for foldno, (t_i, v_i) in enumerate(kf.split(x_develop, y_develop)):\n        # Data Organization\n        x_train = x_develop.loc[t_i]\n        y_train = y_develop.loc[t_i]\n        x_val = x_develop.loc[v_i]\n        y_val = y_develop.loc[v_i]\n\n        print(f\"\\nFold-%d\" % (foldno))\n        print(\"Train sample size:\", x_train.shape[0], \", Validation sample size:\", x_val.shape[0], '\\n')\n\n        # Training + Model Setup\n        cb_es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n        N_TARGET = y_develop.shape[1]\n        input_shape = x_develop.shape[1]\n        \n        models[foldno] = MyModel(input_shape, N_TARGETS).create_model(output_bias=output_bias)\n\n        # Training\n        models[foldno].fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3, batch_size=48, callbacks=[cb_es])\n\n        # Evaluation\n        oof[foldno] = models[foldno].evaluate(x_val, y_val)\n    \n    # Out-of-fold Score\n    mean_oof = np.mean(list(oof.values()))\n    std_oof = np.std(list(oof.values()))\n    \n    print(f'\\nOut-of-fold Score: %.5f +\/- %.5f' % (mean_oof, std_oof))\n        \n    return models, oof","6139d673":"if len(x_test) == 3982:\n    models, oof = cv(output_bias=0)\n    submission = pd.DataFrame(data=0, columns=sub.columns[1:], index=sub['sig_id'])\n    for i in models:\n        submission += pd.DataFrame(data=models[i].predict(x_test), columns=sub.columns[1:], index=sub['sig_id'])\n    submission = submission \/ len(models)\n    submission.to_csv('submission.csv')\nelse:\n    sub.to_csv('submission.csv', index=False)","6de42fb8":"if len(x_test) == 3982:\n    models, oof = cv(output_bias=-np.log(y_develop.mean(axis=0).to_numpy()))\n    submission = pd.DataFrame(data=0, columns=sub.columns[1:], index=sub['sig_id'])\n    for i in models:\n        submission += pd.DataFrame(data=models[i].predict(x_test), columns=sub.columns[1:], index=sub['sig_id'])\n    submission = submission \/ len(models)\n    submission.to_csv('submission.csv')\nelse:\n    sub.to_csv('submission.csv', index=False)","6c011672":"### Encode Categorical Data","47f30925":"### Define Model Architecture","f02fa423":"### Scale Data","d5344d1e":"Model configured with the proper biases.","14a0f96f":"Model configured with the default bias values (output_bias=0).","0d822b4e":"### Group Data Into Folds","2fff32ff":"# <center> MoA <\/center>\n\n## <center> Modeling with Tensorflow (Starter) <\/center>\n\nThe highest performing models in this competition seem to be the neural networks (NNs). The convergence time of NNs can be reduced by initializing the final layer's bias with a smart guess.\n\nIn a binary classification problem, the bias can be roughly estimated as:\n\n\\begin{equation*}\nb_0 = - ln(pos\/neg),\n\\end{equation*}\n\nwhere pos and neg are the number of positive and negative samples, respectively.\n\nIn this competition, we have 23814 samples in the training set with 206 binary classes. The positivity rates of 206 classes can be easily calculated. Biases calculated from these positivity rates provide a faster convergence time than the default bias initialization of the dense layer in Tensorflow, which is 0.\n\nThis notebook demonstrates how to set the bias term as discussed above and can be used as a starter for the MoA competition.\n\n**If you find the notebook useful, don't forget to upvote!**","b63a1e9b":"### Read Data"}}