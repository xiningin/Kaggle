{"cell_type":{"2cab435d":"code","901a4408":"code","dd948a60":"code","082101a5":"code","789eed0b":"code","c857084e":"code","4a9b30fe":"code","46561499":"code","a5e2568c":"code","6223c687":"code","ea8e8d9b":"code","38ad6392":"code","62365f68":"code","f6d62e54":"code","b134f24e":"code","cddb213c":"code","d1b7cbd9":"code","78ff800f":"code","79da7f47":"markdown","8908f8c3":"markdown","b77b4fb9":"markdown","45d97389":"markdown","635ca3e1":"markdown","30ab5fbd":"markdown"},"source":{"2cab435d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","901a4408":"!git clone https:\/\/github.com\/tensorflow\/models\n    \n# Check out a certain commit to ensure that future changes in the TF ODT API codebase won't affect this notebook.\n!cd models && git checkout new_branch","dd948a60":"%%bash\ncd models\/research\n\n# Compile protos.\nprotoc object_detection\/protos\/*.proto --python_out=.\n\nwget https:\/\/storage.googleapis.com\/odml-dataset\/others\/setup.py\npip install -q --user .\n\n# Test if the Object Dectection API is working correctly\npython object_detection\/builders\/model_builder_tf2_test.py","082101a5":"import contextlib2\nimport io\nimport IPython\nimport json\nimport numpy as np\nimport os\nimport pathlib\nimport pandas as pd\nimport sys\nimport tensorflow as tf\nimport time\n\nfrom PIL import Image, ImageDraw\n\n# Import the library that is used to submit the prediction result.\nINPUT_DIR = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nsys.path.insert(0, INPUT_DIR)\nimport greatbarrierreef","789eed0b":"TRAINING_RATIO = 0.75\n\ndata_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n\n# Split the dataset so that no sequence is leaked from the training dataset into the validation dataset.\nsplit_index = int(TRAINING_RATIO * len(data_df))\nwhile data_df.iloc[split_index - 1].sequence == data_df.iloc[split_index].sequence:\n    split_index += 1\n\n# Shuffle both the training and validation datasets.\ntrain_data_df = data_df.iloc[:split_index].sample(frac=1).reset_index(drop=True)\nval_data_df = data_df.iloc[split_index:].sample(frac=1).reset_index(drop=True)\n\ntrain_positive_count = len(train_data_df[train_data_df.annotations != '[]'])\nval_positive_count = len(val_data_df[val_data_df.annotations != '[]'])\n\nprint('Training ratio (all samples):', \n      float(len(train_data_df)) \/ (len(train_data_df) + len(val_data_df)))\nprint('Training ratio (positive samples):', \n      float(train_positive_count) \/ (train_positive_count + val_positive_count))","c857084e":"train_data_df = train_data_df[train_data_df.annotations != '[]'].reset_index()\nprint('Number of positive images used for training:', len(train_data_df))\nval_data_df = val_data_df[val_data_df.annotations != '[]'].reset_index()\nprint('Number of positive images used for validation:', len(val_data_df))","4a9b30fe":"#ALL THANKS TO @Khanh for providing the fast tf record conversion code , I'll release my xml-to-record kernel soon.\nfrom object_detection.utils import dataset_util\nfrom object_detection.dataset_tools import tf_record_creation_util\nimport os\n\ndef create_tf_example(video_id, video_frame, data_df, image_path):\n    \"\"\"Create a tf.Example entry for a given training image.\"\"\"\n    full_path = os.path.join(image_path, os.path.join(f'video_{video_id}', f'{video_frame}.jpg'))\n    with tf.io.gfile.GFile(full_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n\n    height = image.size[1] # Image height\n    width = image.size[0] # Image width\n    filename = f'{video_id}:{video_frame}'.encode('utf8') # Unique id of the image.\n    encoded_image_data = None # Encoded image bytes\n    image_format = 'jpeg'.encode('utf8') # b'jpeg' or b'png'\n\n    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n    xmaxs = [] # List of normalized right x coordinates in bounding box\n             # (1 per box)\n    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n             # (1 per box)\n    classes_text = [] # List of string class name of bounding box (1 per box)\n    classes = [] # List of integer class id of bounding box (1 per box)\n\n    rows = data_df[(data_df.video_id == video_id) & (data_df.video_frame == video_frame)]\n    for _, row in rows.iterrows():\n        annotations = json.loads(row.annotations.replace(\"'\", '\"'))\n        for annotation in annotations:\n            xmins.append(annotation['x'] \/ width) \n            xmaxs.append((annotation['x'] + annotation['width']) \/ width) \n            ymins.append(annotation['y'] \/ height) \n            ymaxs.append((annotation['y'] + annotation['height']) \/ height) \n\n            classes_text.append('COTS'.encode('utf8'))\n            classes.append(1)\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n      'image\/height': dataset_util.int64_feature(height),\n      'image\/width': dataset_util.int64_feature(width),\n      'image\/filename': dataset_util.bytes_feature(filename),\n      'image\/source_id': dataset_util.bytes_feature(filename),\n      'image\/encoded': dataset_util.bytes_feature(encoded_jpg),\n      'image\/format': dataset_util.bytes_feature(image_format),\n      'image\/object\/bbox\/xmin': dataset_util.float_list_feature(xmins),\n      'image\/object\/bbox\/xmax': dataset_util.float_list_feature(xmaxs),\n      'image\/object\/bbox\/ymin': dataset_util.float_list_feature(ymins),\n      'image\/object\/bbox\/ymax': dataset_util.float_list_feature(ymaxs),\n      'image\/object\/class\/text': dataset_util.bytes_list_feature(classes_text),\n      'image\/object\/class\/label': dataset_util.int64_list_feature(classes),\n    }))\n    \n    return tf_example\n\ndef convert_to_tfrecord(data_df, tfrecord_filebase, image_path, num_shards = 10):\n    \"\"\"Convert the object detection dataset to TFRecord as required by the TF ODT API.\"\"\"\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n            tf_record_close_stack, tfrecord_filebase, num_shards)\n        \n        for index, row in data_df.iterrows():\n            if index % 500 == 0:\n                print('Processed {0} images.'.format(index))\n            tf_example = create_tf_example(row.video_id, row.video_frame, data_df, image_path)\n            output_shard_index = index % num_shards\n            output_tfrecords[output_shard_index].write(tf_example.SerializeToString())\n        \n        print('Completed processing {0} images.'.format(len(data_df)))\n\n!mkdir dataset\nimage_path = os.path.join(INPUT_DIR, 'train_images')\n\n# Convert train images to TFRecord\nprint('Converting TRAIN images...')\nconvert_to_tfrecord(\n  train_data_df,\n  'dataset\/cots_train',\n  image_path,\n  num_shards = 1\n)\n\n# Convert validation images to TFRecord\nprint('Converting VALIDATION images...')\nconvert_to_tfrecord(\n  val_data_df,\n  'dataset\/cots_val',\n  image_path,\n  num_shards = 1\n)","46561499":"#EASY PEASY LABEL MAPS.PBTXT FILE\nlabel_map_str = \"\"\"item {\n  id: 1\n  name: 'COTS'\n}\n\"\"\"\n","a5e2568c":"\nwith open('dataset\/labelmap.pbtxt', 'w') as f:\n  f.write(label_map_str)\n\n","6223c687":"#downloading fasterrcnn model\n!wget http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/faster_rcnn_resnet101_v1_1024x1024_coco17_tpu-8.tar.gz","ea8e8d9b":"!tar -xzvf faster_rcnn_resnet101_v1_1024x1024_coco17_tpu-8.tar.gz","38ad6392":"%rm faster_rcnn_resnet101_v1_1024x1024_coco17_tpu-8.tar.gz","62365f68":"from string import Template\n\nconfig_file_template =\"\"\"\n\n# Faster R-CNN with Resnet-50 (v1)\n# Trained on COCO, initialized from Imagenet classification checkpoint\n\n# This config is TPU compatible.\n\nmodel {\n  faster_rcnn {\n    num_classes: 1\n    image_resizer {\n      fixed_shape_resizer {\n        width: 800\n        height: 800\n      }\n    }\n    feature_extractor {\n      type: 'faster_rcnn_resnet101_keras'\n      batch_norm_trainable: true\n    }\n    first_stage_anchor_generator {\n      grid_anchor_generator {\n        scales: [0.25, 0.5, 1.0, 2.0]\n        aspect_ratios: [0.5, 1.0, 2.0]\n        height_stride: 16\n        width_stride: 16\n      }\n    }\n    first_stage_box_predictor_conv_hyperparams {\n      op: CONV\n      regularizer {\n        l2_regularizer {\n          weight: 0.0\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          stddev: 0.01\n        }\n      }\n    }\n    first_stage_nms_score_threshold: 0.0\n    first_stage_nms_iou_threshold: 0.7\n    first_stage_max_proposals: 300\n    first_stage_localization_loss_weight: 2.0\n    first_stage_objectness_loss_weight: 1.0\n    initial_crop_size: 14\n    maxpool_kernel_size: 2\n    maxpool_stride: 2\n    second_stage_box_predictor {\n      mask_rcnn_box_predictor {\n        use_dropout: false\n        dropout_keep_probability: 1.0\n        fc_hyperparams {\n          op: FC\n          regularizer {\n            l2_regularizer {\n              weight: 0.0\n            }\n          }\n          initializer {\n            variance_scaling_initializer {\n              factor: 1.0\n              uniform: true\n              mode: FAN_AVG\n            }\n          }\n        }\n        share_box_across_classes: true\n      }\n    }\n    second_stage_post_processing {\n      batch_non_max_suppression {\n        score_threshold: 0.3\n        iou_threshold: 0.6\n        max_detections_per_class: 100\n        max_total_detections: 300\n      }\n      score_converter: SOFTMAX\n    }\n    second_stage_localization_loss_weight: 2.0\n    second_stage_classification_loss_weight: 1.0\n    use_static_shapes: true\n    use_matmul_crop_and_resize: true\n    clip_anchors_to_image: true\n    use_static_balanced_label_sampler: true\n    use_matmul_gather_in_matcher: true\n  }\n}\n\ntrain_config: {\n  batch_size: 1\n  sync_replicas: true\n  startup_delay_steps: 0\n  replicas_to_aggregate: 8\n  num_steps: $training_steps\n  optimizer {\n    momentum_optimizer: {\n      learning_rate {\n      manual_step_learning_rate {\n          initial_learning_rate: 0.001\n          schedule {\n              step: 15000\n              learning_rate: 0.0001\n              }\n          schedule {\n              step: 25000\n              learning_rate: 0.00001\n              }\n          \n          schedule {\n              step: 35000\n              learning_rate: 0.000001\n              }\n          \n          }\n        }\n      \n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  fine_tune_checkpoint_version: V2\n  fine_tune_checkpoint: \"\/kaggle\/working\/faster_rcnn_resnet101_v1_1024x1024_coco17_tpu-8\/checkpoint\/ckpt-0\"\n  fine_tune_checkpoint_type: \"detection\"\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n\n  data_augmentation_options {\n    random_adjust_hue {\n    }\n  }\n\n  data_augmentation_options {\n    random_adjust_contrast {\n    }\n  }\n\n  data_augmentation_options {\n    random_adjust_saturation {\n    }\n  }\n\n  data_augmentation_options {\n     random_square_crop_by_scale {\n      scale_min: 0.6\n      scale_max: 1.3\n    }\n  }\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n  use_bfloat16: false  # works only on TPUs\n}\ntrain_input_reader: {\n  label_map_path: \"\/kaggle\/working\/dataset\/labelmap.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"\/kaggle\/working\/dataset\/cots_train-?????-of-00001\"\n  }\n}\n\neval_config: {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n  batch_size: 1;\n}\n\neval_input_reader: {\n  label_map_path: \"\/kaggle\/working\/dataset\/labelmap.pbtxt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"\/kaggle\/working\/dataset\/cots_val-?????-of-00001\"\n  }\n}\n\"\"\"","f6d62e54":"type(config_file_template)","b134f24e":"TRAINING_STEPS = 50000\nPIPELINE_CONFIG_PATH='\/kaggle\/working\/faster_rcnn_resnet101_v1_1024x1024_coco17_tpu-8\/pipeline.config'\n\npipeline = Template(config_file_template).substitute(\n    training_steps=TRAINING_STEPS)\n\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)","cddb213c":"%mkdir \/kaggle\/working\/training_folder","d1b7cbd9":"\n#%load_ext tensorboard\n#USE LOAD IF NOT ALREADY LOADED ELSE USE %reload_ext tensorboard\n%reload_ext tensorboard\n#PATH=mention the path where the model will be trained\n%tensorboard --logdir='\/kaggle\/working\/training_folder'","78ff800f":"MODEL_DIR='\/kaggle\/working\/training_folder\/'\n!python \/kaggle\/working\/models\/research\/object_detection\/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --alsologtostderr","79da7f47":"### **Model is getting trained, as soon as it's done I'll put an inference notebook**\n![02_02_18_508408464_AAB_560x292.jpg](attachment:8ad8138a-1fea-448a-aa7f-af1186d85b2e.jpg)","8908f8c3":"### EASY SETUP CREDITS https:\/\/www.kaggle.com\/khanhlvg\/cots-detection-w-tensorflow-object-detection-api\n### WANT TO SETUP API ON COLAB  https:\/\/colab.research.google.com\/drive\/1lfWnuBZXWmC3DiLWmYs56SPBW4RvWYkp?usp=sharing create a copy\n","b77b4fb9":"**YES you can use tensorboard**","45d97389":"![96522d4e785f203f1b36575f12c04e92.jpg](attachment:b312ce19-7782-4baf-a2cb-85405cc01e02.jpg)","635ca3e1":"# [COTS]TFOD2(API Setup + Faster RCNN)\n\n## Goal of the Competition\nThe goal of this competition is to accurately identify starfish in real-time by building an object detection model trained on underwater videos of coral reefs.\n\n## Context\nAustralia's stunningly beautiful Great Barrier Reef is the world\u2019s largest coral reef and home to 1,500 species of fish, 400 species of corals, 130 species of sharks, rays, and a massive variety of other sea life.\nUnfortunately, the reef is under threat, in part because of the overpopulation of one particular starfish \u2013 the coral-eating crown-of-thorns starfish (or COTS for short). Scientists, tourism operators and reef managers established a large-scale intervention program to control COTS outbreaks to ecologically sustainable levels.\n\n\n## Changes in the pipeline config are mentioned below\n- num_classes =1\n- image width and height\n- score_threshold = 0.3 for getting only threshold above confidence score of 30%\n- optimizer -> ADDED a custom manual training step optimizer which divides Lr by 10\n- fine_tune_checkpoint_type -> from classification to detection or else during training it will it stop after feature extraction\n- use_bfloat16 = False since we are training in gpu\n- batch_size =1 can't afford higher batchsize or get ready to get rekt. Good configuration like 4x v100 can use batch_size = 8 or even 16\n\n\n> Note: `TF2 API pre-trained models were trained on TPU, while using a gpu try to keep a low batch size to avoid OOM errors` \n> Note2: 'Always better to run these models on a local environment or colabpro connected to your gdrive'\n\n## Installation\nInstall the dependencies\n\n## Contribute\n\nWant to contribute? you can always copy and edit\n\nMore on TF OD API at\n\n```sh\nhttps:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/object_detection\n```\n\n## Contact\n\n- [Click here ](https:\/\/twitter.com\/bambose_) - to connect me on twitter\n\n\n## Faster RCNN architecture\n\n\n![fasterrcnn-architecture.png](attachment:f3950d7e-be62-455f-a614-57ea24cee9ee.png)\n\n\n## Thanks\nMajor thanks to @Khanh for providing the base notebook.\nYou can visit his profile here.\n[@Khanh ](https:\/\/www.kaggle.com\/khanhlvg)\n\n## PS - Inference notebook out soon\n\n\n\n\n\n","30ab5fbd":"### CHANGES MADE IN PIPELINE CONFIG __ VV IMPORTANT\n* num_classes =1\n* image width and height\n* score_threshold = 0.3 for getting only threshold above confidence score of 30%\n* optimizer -> ADDED a custom manual training step optimizer which divides Lr by 10\n* fine_tune_checkpoint_type -> from classification to detection or else during training it will it stop after feature extraction\n* use_bfloat16 = False since we training in gpu\n* batch_size =1 can't afford higher batchsize or get ready to get rekt. Good configuration like 4x v100 can use batch_size = 8 or even 16\n\n**Currently the training step is 50k, on personal gpu one can train for 1.5-2L steps and keep on saving the model for each iteration i.e the no of training images**\n"}}