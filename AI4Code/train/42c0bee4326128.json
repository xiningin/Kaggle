{"cell_type":{"15ca9022":"code","9eb1c8e5":"code","2fc29dc2":"code","89b438c5":"code","9157583d":"code","daaf260a":"code","b20490bc":"code","59c75aee":"code","c786c5b3":"code","d5282dad":"code","e4aca7a9":"code","3f3199e1":"code","a3473a45":"code","6a89a56a":"code","163a25c8":"code","7658652e":"code","d0ff0cf5":"code","5e290c8c":"code","47a0af77":"code","535d6399":"code","b936b16d":"code","4694a479":"code","7f6e90db":"code","fe3d9260":"code","a56afc8a":"code","0c17d1cf":"code","fc971567":"code","d0c9296e":"markdown","232c2d23":"markdown","dc3a5258":"markdown","3fb00e9f":"markdown","2c445e67":"markdown","dc79ff2a":"markdown","3f717a4c":"markdown","81511039":"markdown"},"source":{"15ca9022":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9eb1c8e5":"\ndf = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")\ndf.head()","2fc29dc2":"df.head()","89b438c5":"### Drop Id column dont reqired this is Unwanted column","9157583d":"df.drop([\"Id\"],axis=1,inplace=True)   # dropped\n","daaf260a":"df.head()","b20490bc":"df.shape","59c75aee":"df.info()","c786c5b3":"import matplotlib.pyplot as plt\nimport seaborn as sns","d5282dad":"sns.pairplot(data=df,hue='Species',palette='pink')","e4aca7a9":"df.columns","3f3199e1":"data=df.loc[:,['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]","a3473a45":"from sklearn.cluster import KMeans\nkmeans=KMeans(n_clusters=100)","6a89a56a":"#### Finding the best amount of clusters to get most accurate results (KMeans)","163a25c8":"from sklearn.cluster import KMeans\nwcss = [] # within cluster sum of square\nfor k in range(1,15):\n    kmeans=KMeans(n_clusters=k)\n    kmeans.fit(data)\n    wcss.append(kmeans.inertia_)\n    \nplt.figure(figsize=(20,8))\nplt.title(\"The Elbow method\", fontsize=18)\nplt.plot(range(1,15),wcss,\"-o\")\nplt.grid(True)\nplt.xlabel(\"Amount of Clusters\",fontsize=14)\nplt.ylabel(\"Inertia\",fontsize=14)\nplt.xticks(range(1,20))\nplt.tight_layout()\nplt.show()","7658652e":"### Above the figure looks like elbow method","d0ff0cf5":"plt.figure(figsize=(24,4))\n\nplt.suptitle(\"K Means Clustering\",fontsize=20)\n\nplt.subplot(1,5,1)\nplt.title('k=1',fontsize=15)\nplt.xlabel('PetalLengthCm')\nplt.ylabel('PetalWidthCm')\nplt.scatter(data.PetalLengthCm,data.PetalWidthCm)\n\n\nplt.subplot(1,5,2)\nplt.title('k=2',fontsize=15)\nplt.xlabel('PetalLengthCm')\nkmeans = KMeans(n_clusters=2)\ndata['Labels']=kmeans.fit_predict(data)\nplt.scatter(data.PetalLengthCm[data.Labels == 0],data.PetalWidthCm[data.Labels == 0])\nplt.scatter(data.PetalLengthCm[data.Labels == 1],data.PetalWidthCm[data.Labels == 1])\n\n\ndata.drop(['Labels'],axis=1,inplace=True)\n\n\nplt.subplot(1,5,3)\nplt.title('k=3',fontsize=15)\nplt.xlabel('PetalLengthCm')\nkmeans = KMeans(n_clusters=3)\ndata['Labels']=kmeans.fit_predict(data)\nplt.scatter(data.PetalLengthCm[data.Labels == 0],data.PetalWidthCm[data.Labels == 0])\nplt.scatter(data.PetalLengthCm[data.Labels == 1],data.PetalWidthCm[data.Labels == 1])\nplt.scatter(data.PetalLengthCm[data.Labels == 2],data.PetalWidthCm[data.Labels == 2])\n\ndata.drop([\"Labels\"],axis=1,inplace=True)\n\n\nplt.subplot(1,5,4)\nplt.title('k=4',fontsize=15)\nplt.xlabel('PetalLengthCm')\nkmeans = KMeans(n_clusters=4)\ndata['Labels']=kmeans.fit_predict(data)\nplt.scatter(data.PetalLengthCm[data.Labels == 0],data.PetalWidthCm[data.Labels == 0])\nplt.scatter(data.PetalLengthCm[data.Labels == 1],data.PetalWidthCm[data.Labels == 1])\nplt.scatter(data.PetalLengthCm[data.Labels == 2],data.PetalWidthCm[data.Labels == 2])\nplt.scatter(data.PetalLengthCm[data.Labels == 3],data.PetalWidthCm[data.Labels == 3])\n\n\ndata.drop([\"Labels\"],axis=1,inplace=True)\n\n\n\nplt.subplot(1,5,5)\nplt.title('original labels',fontsize=15)\nplt.xlabel('PetalLengthCm')\nplt.scatter(df.PetalLengthCm[df.Species == 'Iris-setosa'],df.PetalWidthCm[df.Species == 'Iris-setosa'])\nplt.scatter(df.PetalLengthCm[df.Species == 'Iris-virginica'],df.PetalWidthCm[df.Species == 'Iris-virginica'])\nplt.scatter(df.PetalLengthCm[df.Species == 'Iris-versicolor'],df.PetalWidthCm[df.Species == 'Iris-versicolor'])\n\n\nplt.subplots_adjust(top=.8)\nplt.show()","5e290c8c":"#data['Labels']=kmeans.fit_predict(data)","47a0af77":"#df['Species'].value_counts()","535d6399":"#data.head()","b936b16d":"#data['Labels'].value_counts()","4694a479":"from sklearn.cluster import AgglomerativeClustering\nhc_cluster = AgglomerativeClustering(n_clusters=100) ","7f6e90db":"## Finding the best amount of clusters to get most accurate results (Hierarchy)","fe3d9260":"from scipy.cluster.hierarchy import dendrogram,linkage","a56afc8a":"merge=linkage(data,method='ward')\n\nplt.figure(figsize=(18,6))\ndendrogram(merge,leaf_rotation=90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidian distance\")\n\nplt.suptitle(\"DENDROGRAM\",fontsize=18)\nplt.show()","0c17d1cf":"plt.figure(figsize=(24,4))\n\nplt.suptitle(\"Hierarchical Clustering\",fontsize=20)\n\nplt.subplot(1,5,1)\nplt.title('k=1',fontsize=15)\nplt.xlabel('PetalLengthCm')\nplt.ylabel('PetalWidthCm')\nplt.scatter(data.PetalLengthCm,data.PetalWidthCm)\n\n\nplt.subplot(1,5,2)\nplt.title('k=2',fontsize=15)\nplt.xlabel('PetalLengthCm')\nhc_cluster = AgglomerativeClustering(n_clusters=2)\ndata['Labels']=kmeans.fit_predict(data)\nplt.scatter(data.PetalLengthCm[data.Labels == 0],data.PetalWidthCm[data.Labels == 0])\nplt.scatter(data.PetalLengthCm[data.Labels == 1],data.PetalWidthCm[data.Labels == 1])\n\n\ndata.drop(['Labels'],axis=1,inplace=True)\n\n\nplt.subplot(1,5,3)\nplt.title('k=3',fontsize=15)\nplt.xlabel('PetalLengthCm')\nhc_cluster = AgglomerativeClustering(n_clusters=3)\ndata['Labels']=kmeans.fit_predict(data)\nplt.scatter(data.PetalLengthCm[data.Labels == 0],data.PetalWidthCm[data.Labels == 0])\nplt.scatter(data.PetalLengthCm[data.Labels == 1],data.PetalWidthCm[data.Labels == 1])\nplt.scatter(data.PetalLengthCm[data.Labels == 2],data.PetalWidthCm[data.Labels == 2])\n\ndata.drop([\"Labels\"],axis=1,inplace=True)\n\n\nplt.subplot(1,5,4)\nplt.title('k=4',fontsize=15)\nplt.xlabel('PetalLengthCm')\nhc_cluster = AgglomerativeClustering(n_clusters=4)\ndata['Labels']=kmeans.fit_predict(data)\nplt.scatter(data.PetalLengthCm[data.Labels == 0],data.PetalWidthCm[data.Labels == 0])\nplt.scatter(data.PetalLengthCm[data.Labels == 1],data.PetalWidthCm[data.Labels == 1])\nplt.scatter(data.PetalLengthCm[data.Labels == 2],data.PetalWidthCm[data.Labels == 2])\nplt.scatter(data.PetalLengthCm[data.Labels == 3],data.PetalWidthCm[data.Labels == 3])\n\n\ndata.drop([\"Labels\"],axis=1,inplace=True)\n\n\n\nplt.subplot(1,5,5)\nplt.title('original labels',fontsize=15)\nplt.xlabel('PetalLengthCm')\nplt.scatter(df.PetalLengthCm[df.Species == 'Iris-setosa'],df.PetalWidthCm[df.Species == 'Iris-setosa'])\nplt.scatter(df.PetalLengthCm[df.Species == 'Iris-virginica'],df.PetalWidthCm[df.Species == 'Iris-virginica'])\nplt.scatter(df.PetalLengthCm[df.Species == 'Iris-versicolor'],df.PetalWidthCm[df.Species == 'Iris-versicolor'])\n\n\nplt.subplots_adjust(top=.8)\nplt.show()","fc971567":"#data.drop([\"Labels\"],axis=1,inplace=True)\n\n# kmeans\nkmeans = KMeans(n_clusters=3)\nkmeans_predict = kmeans.fit_predict(data)\n\n# cross tabulation table for kmeans\ndf1 = pd.DataFrame({'Labels':kmeans_predict,\"Species\":df['Species']})\nct1 = pd.crosstab(df1['Labels'],df1['Species'])\n\n\n# hierarchy\nhc_cluster = AgglomerativeClustering(n_clusters=3)\nhc_predict = hc_cluster.fit_predict(data)\n\n# cross tabulation table for Hierarchy\ndf2 = pd.DataFrame({'Labels':hc_predict,\"Species\":df['Species']})\nct2 = pd.crosstab(df2['Labels'],df2['Species'])\n\n\nplt.figure(figsize=(24,8))\nplt.suptitle(\"CROSS TABULATIONS\",fontsize=18)\nplt.subplot(1,2,1)\nplt.title(\"KMeans\")\nsns.heatmap(ct1,annot=True,cbar=False,cmap=\"vlag\")\n\nplt.subplot(1,2,2)\nplt.title(\"Hierarchy\")\nsns.heatmap(ct2,annot=True,cbar=False,cmap=\"pink\")\n\nplt.show()","d0c9296e":"Again k value 3 is more accurate than 2","232c2d23":"#####  Implementing K-means clustering","dc3a5258":"Kmeans visibly did an amazing job with 3 clusters. Except few data points,\nI can say prediction is identical to the original with labels.\nWhich shows that our ELBOW chart was right.","3fb00e9f":"we see that longest vertical line without any perpendecular matching lines (euclidian distances).\nIf we draw a horizontal line between that values, \nwe will have 2 or 3 interceptions which are representing ideal amount of labels.","2c445e67":"###### Longest Vertical line between Horizontal Lines.\n","dc79ff2a":"##### Unsupervised learning","3f717a4c":"#### Hierarchical Clustering","81511039":"Reason behind this is basically \"iris-setosa\" being too easy to separate while the other two is quite mixed and it made our Dendrogram method a bit unclear.\n"}}