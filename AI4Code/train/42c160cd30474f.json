{"cell_type":{"989fb64e":"code","be91ae5c":"code","addfdfff":"code","d9699cec":"code","f8b3c03d":"code","e8157a21":"code","90ecd0c5":"code","1ac0fdb1":"code","b00680d0":"code","c4fa83fd":"code","aa5c08b8":"code","05f15999":"code","0694d165":"code","625a2fa4":"code","f82f9cd3":"code","44d1947f":"code","2ae885f8":"code","1e978249":"markdown","ffa1cfcf":"markdown","f25ac679":"markdown","e82eca6f":"markdown","e3e5da50":"markdown","ed42d7b5":"markdown","4f5a1173":"markdown","7aa5c2f7":"markdown","8a88160a":"markdown","a7538466":"markdown","5878f6d5":"markdown"},"source":{"989fb64e":"import tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\nfrom tensorflow.python.keras.utils import conv_utils\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm\nimport pickle\n\n\nfrom random import randint, seed\nimport itertools\nimport cv2","be91ae5c":"class PConv2D(tf.keras.layers.Conv2D):\n    def __init__(self, *args, n_channels=3, mono=False, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.input_spec = [tf.keras.layers.InputSpec(ndim=4), tf.keras.layers.InputSpec(ndim=4)]\n\n    def build(self, input_shape):        \n        \"\"\"Adapted from original _Conv() layer of Keras        \n        param input_shape: list of dimensions for [img, mask]\n        \"\"\"\n        \n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n            \n        if input_shape[0][channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\n            \n        self.input_dim = input_shape[0][channel_axis]\n        \n        # Image kernel\n        kernel_shape = self.kernel_size + (self.input_dim, self.filters)\n        self.kernel = self.add_weight(shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='img_kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        # Mask kernel\n        self.kernel_mask = K.ones(shape=self.kernel_size + (self.input_dim, self.filters))\n\n        # Calculate padding size to achieve zero-padding\n        self.pconv_padding = (\n            (int((self.kernel_size[0]-1)\/2), int((self.kernel_size[0]-1)\/2)), \n            (int((self.kernel_size[0]-1)\/2), int((self.kernel_size[0]-1)\/2)), \n        )\n\n        # Window size - used for normalization\n        self.window_size = self.kernel_size[0] * self.kernel_size[1]\n        \n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def call(self, inputs, mask=None):\n\n        # Padding done explicitly so that padding becomes part of the masked partial convolution\n        images = K.spatial_2d_padding(inputs[0], self.pconv_padding, self.data_format)\n        masks = K.spatial_2d_padding(inputs[1], self.pconv_padding, self.data_format)\n\n        # Apply convolutions to mask\n        mask_output = K.conv2d(\n            masks, self.kernel_mask, \n            strides=self.strides,\n            padding='valid',\n            data_format=self.data_format,\n            dilation_rate=self.dilation_rate\n        )\n\n        # Apply convolutions to image\n        img_output = K.conv2d(\n            (images*masks), self.kernel, \n            strides=self.strides,\n            padding='valid',\n            data_format=self.data_format,\n            dilation_rate=self.dilation_rate\n        )        \n\n        # Calculate the mask ratio on each pixel in the output mask\n        mask_ratio = self.window_size \/ (mask_output + 1e-8)\n\n        # Clip output to be between 0 and 1\n        mask_output = K.clip(mask_output, 0, 1)\n\n        # Remove ratio values where there are holes\n        mask_ratio = mask_ratio * mask_output\n\n        # Normalize iamge output\n        img_output = img_output * mask_ratio\n\n        # Apply bias only to the image (if chosen to do so)\n        if self.use_bias:\n            img_output = K.bias_add(\n                img_output,\n                self.bias,\n                data_format=self.data_format)\n        \n        # Apply activations on the image\n        if self.activation is not None:\n            img_output = self.activation(img_output)\n            \n        return [img_output, mask_output]\n    \n    def compute_output_shape(self, input_shape):\n        space = input_shape[0][1:-1]\n        new_space = []\n        for i in range(len(space)):\n            new_dim = conv_utils.conv_output_length(\n                space[i],\n                self.kernel_size[i],\n                padding='same',\n                stride=self.strides[i],\n                dilation=self.dilation_rate[i])\n            new_space.append(new_dim)\n        new_shape = (input_shape[0][0],) + tuple(new_space) + (self.filters,)\n        return [new_shape, new_shape]","addfdfff":"class InstanceNormalization(tf.keras.layers.Layer):\n    def __init__(self, epsilon=1e-5):\n        super(InstanceNormalization, self).__init__()\n        self.epsilon = epsilon\n        \n    def build(self, input_shape):\n        self.scale = self.add_weight(\n            name='scale',\n            shape=input_shape[-1:],\n            initializer=tf.random_normal_initializer(1., 0.02),trainable=True)\n\n        self.offset = self.add_weight(\n            name='offset',\n            shape=input_shape[-1:],\n            initializer='zeros',\n            trainable=True)\n        \n    def call(self, x):\n        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n        inv = tf.math.rsqrt(variance + self.epsilon)\n        normalized = (x - mean) * inv\n        return self.scale * normalized + self.offset","d9699cec":"    def generator(norm_type='Bn',train=True):      \n\n        # INPUTS\n        inputs_img = tf.keras.layers.Input((None, None, 3), name='inputs_img')\n        inputs_mask = tf.keras.layers.Input((None, None, 3), name='inputs_mask')\n        \n        \n        def encoder_layer(img_in, mask_in, filters, kernel_size,norm_type,norm=True):\n            conv, mask = PConv2D(filters, kernel_size, strides=2, padding='same')([img_in, mask_in])\n            if norm:\n                if norm_type=='In':\n                    conv=InstanceNormalization()(conv)\n                else:\n                    conv=tf.keras.layers.BatchNormalization()(conv,training=train)\n                    \n            conv = tf.keras.layers.ReLU()(conv)\n            return conv, mask\n\n        \n        e_conv1, e_mask1 = encoder_layer(inputs_img, inputs_mask, 64, 7, norm_type,norm=False)\n        e_conv2, e_mask2 = encoder_layer(e_conv1, e_mask1, 128, 5,norm_type)\n        e_conv3, e_mask3 = encoder_layer(e_conv2, e_mask2, 256, 5,norm_type)\n        e_conv4, e_mask4 = encoder_layer(e_conv3, e_mask3, 512, 3,norm_type)\n        e_conv5, e_mask5 = encoder_layer(e_conv4, e_mask4, 512, 3,norm_type)\n        e_conv6, e_mask6 = encoder_layer(e_conv5, e_mask5, 512, 3,norm_type)\n        e_conv7, e_mask7 = encoder_layer(e_conv6, e_mask6, 512, 3,norm_type)\n        e_conv8, e_mask8 = encoder_layer(e_conv7, e_mask7, 512, 3,norm_type)\n        \n        \n        def decoder_layer(img_in, mask_in, e_conv, e_mask, filters, kernel_size,norm_type, norm=True):\n            up_img = tf.keras.layers.UpSampling2D(size=(2, 2))(img_in)\n            up_mask = tf.keras.layers.UpSampling2D(size=(2, 2))(mask_in)\n            concat_img = tf.keras.layers.Concatenate(axis=-1)([e_conv, up_img])\n            concat_mask = tf.keras.layers.Concatenate(axis=-1)([e_mask, up_mask])\n            \n            conv, mask = PConv2D(filters, kernel_size, padding='same')([concat_img, concat_mask])\n            \n            if norm:\n                if norm_type=='In':\n                    conv=InstanceNormalization()(conv)\n                else:\n                    conv=tf.keras.layers.BatchNormalization()(conv,training=train)\n            conv = tf.keras.layers.LeakyReLU(alpha=0.2)(conv)\n            return conv, mask\n            \n        d_conv9, d_mask9 = decoder_layer(e_conv8, e_mask8, e_conv7, e_mask7, 512, 3,norm_type)\n        d_conv10, d_mask10 = decoder_layer(d_conv9, d_mask9, e_conv6, e_mask6, 512, 3,norm_type)\n        d_conv11, d_mask11 = decoder_layer(d_conv10, d_mask10, e_conv5, e_mask5, 512, 3,norm_type)\n        d_conv12, d_mask12 = decoder_layer(d_conv11, d_mask11, e_conv4, e_mask4, 512, 3,norm_type)\n        d_conv13, d_mask13 = decoder_layer(d_conv12, d_mask12, e_conv3, e_mask3, 256, 3,norm_type)\n        d_conv14, d_mask14 = decoder_layer(d_conv13, d_mask13, e_conv2, e_mask2, 128, 3,norm_type)\n        d_conv15, d_mask15 = decoder_layer(d_conv14, d_mask14, e_conv1, e_mask1, 64, 3,norm_type)\n        d_conv16, d_mask16 = decoder_layer(d_conv15, d_mask15, inputs_img, inputs_mask, 3, 3, norm_type,norm=False)\n        outputs = tf.keras.layers.Conv2D(3, 1, activation = 'sigmoid', name='outputs_img')(d_conv16)\n        \n        # Setup the model inputs \/ outputs\n        model = tf.keras.Model(inputs=[inputs_img, inputs_mask], outputs=outputs)\n\n        return model","f8b3c03d":"def VGG():\n\n    #pool1, pool2 and pool3 for both perceptual loss & style loss\n    \n    vgg16=VGG16(include_top=False,weights=None)\n    vgg16.load_weights('..\/input\/vgg16-weights\/pytorch_to_keras_vgg16.h5',by_name=True)\n    vgg16.trainable=False\n    \n    layer_names=['block1_pool','block2_pool','block3_pool']\n    \n    outputs = [vgg16.get_layer(name).output for name in layer_names]\n    \n    return tf.keras.Model([vgg16.input], outputs)","e8157a21":"img_size=256\n\nbatch_size=16\n\nlr=1e-4\n\nepochs=50\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nfine_tune=True #fine tune\nfrozen_layers=[5,8,11,14,17,20,23]","90ecd0c5":"pth='..\/input\/imagenet\/imagenet'\npth_mask='..\/input\/inpainting-mask-generator\/mask'\n\n#train\ntrain_folder=sorted(os.listdir(f'{pth}\/train'))\ntrain_mask_folder=sorted(os.listdir(f'{pth_mask}\/train'))\ndf_train=pd.DataFrame(np.vstack([train_folder,train_mask_folder]).T,columns=['pth','pth_mask'])\ndf_train['pth']=df_train['pth'].apply(lambda x: os.path.join(f'{pth}\/train\/{x}'))\ndf_train['pth_mask']=df_train['pth_mask'].apply(lambda x: os.path.join(f'{pth_mask}\/train\/{x}'))\n\n\n#val\nval_folder=sorted(os.listdir(f'{pth}\/val'))\nval_mask_folder=sorted(os.listdir(f'{pth_mask}\/val'))\ndf_val=pd.DataFrame(np.vstack([val_folder,val_mask_folder]).T,columns=['pth','pth_mask'])\ndf_val['pth']=df_val['pth'].apply(lambda x: os.path.join(f'{pth}\/val\/{x}'))\ndf_val['pth_mask']=df_val['pth_mask'].apply(lambda x: os.path.join(f'{pth_mask}\/val\/{x}'))","1ac0fdb1":"def get_image(path,path_mask):\n    image = tf.image.decode_jpeg(tf.io.read_file(path), channels=3)\n    image=tf.cast(tf.image.resize(image,(img_size,img_size)),'float32')\n    image=image\/255.\n    \n    mask = tf.image.decode_jpeg(tf.io.read_file(path_mask), channels=3)\n    mask=tf.cast(tf.image.resize(mask,(img_size,img_size)),'float32')\n    mask=mask\/255.\n    return image,mask","b00680d0":"#all 0~255\nds_train=tf.data.Dataset.from_tensor_slices((df_train['pth'],df_train['pth_mask'])).map(get_image,num_parallel_calls=AUTOTUNE).\\\n                        shuffle(256).batch(batch_size,drop_remainder=True)\n\nds_val=tf.data.Dataset.from_tensor_slices((df_val['pth'],df_val['pth_mask'])).map(get_image,num_parallel_calls=AUTOTUNE).\\\n                        batch(batch_size,drop_remainder=True)","c4fa83fd":"vgg=VGG()","aa5c08b8":"def l1(y_true, y_pred):\n    if K.ndim(y_true) == 4:\n        return K.mean(K.abs(y_pred - y_true), axis=[1,2,3])\n    elif K.ndim(y_true) == 3:\n        return K.mean(K.abs(y_pred - y_true), axis=[1,2])\n    \n    \ndef gram_matrix(x):\n    # Permute channels and get resulting shape\n    x = tf.transpose(x, perm=(0, 3, 1, 2))\n    shape = tf.shape(x)\n    B, C, H, W = shape[0], shape[1], shape[2], shape[3]\n        \n    # Reshape x and do batch dot product\n    features = tf.reshape(x, tf.stack([B, C, H*W]))\n    gram = tf.keras.backend.batch_dot(features, features, axes=2)\n        \n    # Normalize with channels, height and width\n    gram = gram \/  tf.cast(C * H * W, x.dtype)\n        \n    return gram","05f15999":"\ndef loss_hole(mask, y_true, y_pred):\n    return l1((1-mask) * y_true, (1-mask) * y_pred)\n    \ndef loss_valid(mask, y_true, y_pred):\n    return l1(mask * y_true, mask * y_pred)\n    \ndef loss_perceptual(vgg_out, vgg_gt, vgg_comp): \n    loss = 0\n    for o, c, g in zip(vgg_out, vgg_comp, vgg_gt):\n        loss += l1(o, g) + l1(c, g)\n    return loss\n        \ndef loss_style(output, vgg_gt):\n    loss = 0\n    for o, g in zip(output, vgg_gt):\n        loss += l1(gram_matrix(o), gram_matrix(g))\n    return loss\n\n    \ndef loss_tv(mask, y_comp):\n    kernel = tf.ones(shape=(3, 3, mask.shape[3], mask.shape[3]))\n    dilated_mask = K.conv2d(1-mask, kernel, data_format='channels_last', padding='same')\n\n    dilated_mask = tf.cast(K.greater(dilated_mask, 0), 'float32')\n    P = dilated_mask * y_comp\n\n    a = l1(P[:,1:,:,:], P[:,:-1,:,:])\n    b = l1(P[:,:,1:,:], P[:,:,:-1,:])        \n    return a+b","0694d165":"mean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ndef loss_func(y_true, y_pred, mask):\n    y_comp = mask * y_true + (1-mask) * y_pred\n    vgg_out = vgg((y_pred-mean)\/std)\n    vgg_gt = vgg((y_true-mean)\/std)\n    vgg_comp = vgg((y_comp-mean)\/std)\n            \n    l1 = loss_valid(mask, y_true, y_pred)\n    l2 = loss_hole(mask, y_true, y_pred)\n    l3 = loss_perceptual(vgg_out, vgg_gt, vgg_comp)\n    l4 = loss_style(vgg_out, vgg_gt)\n    l5 = loss_style(vgg_comp, vgg_gt)\n    l6 = loss_tv(mask, y_comp)\n    \n    return l1 + 6*l2 + 0.05*l3 + 120*(l4+l5) + 0.1*l6","625a2fa4":"def show(x,mask,model,n=6):\n    \n    x_masked= x*mask+(1-mask)\n    \n    x_pred=model([x_masked,mask],training=False)\n    \n    mask = tf.concat([mask for _ in range(3)], -1)\n    \n    fig,ax=plt.subplots(nrows=3,ncols=n,figsize=(8,8))\n    \n    for i in range(3):\n        for j in range(n):\n            if i==1:\n                x=x_masked\n            elif i==2:\n                x=x_pred\n            ax[i,j].imshow(x[j])\n    plt.show()","f82f9cd3":"@tf.function\ndef train_step(x,mask,model,opt):\n    with tf.GradientTape() as tape:\n        x_masked= x*mask+(1-mask)\n        \n        x_prime=model([x,mask],training=True)\n        \n        loss=tf.reduce_mean(loss_func(x,x_prime,mask))\n        \n    grad=tape.gradient(loss,model.trainable_variables)\n    opt.apply_gradients(zip(grad,model.trainable_variables))\n    \n    return loss","44d1947f":"def train():\n    try:\n        G=tf.keras.models.load_model('..\/input\/inpainting-models\/model_bn\/model_in')\n        \n        if fine_tune:\n            for i in frozen_layers:\n                G.layers[i].trainable=False\n                lr=1e-5\n    except:\n        G=generator('In')\n    optimizer=tf.keras.optimizers.Adam(learning_rate=lr)\n    ckpt = tf.train.Checkpoint(G=G,optimizer=optimizer)\n    ckpt_manager = tf.train.CheckpointManager(ckpt,'.\/ckpt', max_to_keep=1)\n    print('start training')\n    for epoch in range(epochs):\n        if epoch%5==0:\n            print('sampling')\n            for x,mask in ds_val:\n                show(x,mask,G)\n                break \n                \n            #save\n            ckpt_manager.save()\n            tf.keras.models.save_model(G,'.\/model')\n            \n        loop=tqdm(ds_train)\n        for x,mask in loop:\n            loss=train_step(x,mask,G,optimizer)\n            loop.set_postfix(loss=f'loss:{loss}')\n    return G","2ae885f8":"G=train()","1e978249":"#### Codes are mainly from : [Partial Convolution Keras](https:\/\/github.com\/MathiasGruber\/PConv-Keras)\n\n\n#### Purpose : \n\n#### 1. Testing some properties about texture & content synthesis for image inpainting task\n\n#### 2. This notebook trains 2 models to test the idea from [Instance Normalization: The Missing Ingredient for Fast Stylization](https:\/\/arxiv.org\/abs\/1607.08022)\n\n#### 3. And further do some experiment about difference between In and Bn, better understanding the idea from  [A Neural Algorithm of Artistic Style](https:\/\/arxiv.org\/abs\/1508.06576) and [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https:\/\/arxiv.org\/abs\/1603.08155)","ffa1cfcf":"## Generator UNET ","f25ac679":"## Notebook ","e82eca6f":"## Objective","e3e5da50":"## Partial Convolution","ed42d7b5":"## VGG Extractor","4f5a1173":"## Prepare Data","7aa5c2f7":"## Instance Normalization","8a88160a":"# Training","a7538466":"# Model","5878f6d5":"## Train"}}