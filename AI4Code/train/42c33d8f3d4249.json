{"cell_type":{"f5704b1f":"code","05b95049":"code","225aaee1":"code","3ae0c189":"code","c4aa6973":"code","eec85384":"code","236eab5a":"code","90683684":"code","e9a1e0ea":"code","4a8a17c9":"code","65517d49":"code","38c2aded":"code","de03d41f":"code","cbb0817e":"code","d0446583":"code","0384065a":"code","70465454":"code","6b6d97d9":"code","1f0e198c":"markdown","e835a489":"markdown"},"source":{"f5704b1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","05b95049":"import matplotlib.pyplot as plt\n%matplotlib inline","225aaee1":"df=pd.read_csv('\/kaggle\/input\/learn-together\/train.csv',index_col=['Id'])\nprint(df.shape)\nprint(df.describe)","3ae0c189":"cols_with_missing = [col for col in df.columns\n                     if df[col].isnull().any()]\nprint('Columns with missing values:')\nprint(cols_with_missing)","c4aa6973":"y=df['Cover_Type']\nX=df.drop(['Cover_Type'],axis=1)\nprint(X.shape,y.shape)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=42)","eec85384":"sns.distplot(y_train)\ny_train.value_counts()","236eab5a":"s = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","90683684":"numerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\nprint(len(numerical_cols))","e9a1e0ea":"#from xgboost import XGBClassifier\n#from sklearn.metrics import mean_absolute_error\n#from sklearn.model_selection import cross_val_score\n#from sklearn.metrics import accuracy_score\n#from sklearn.model_selection import KFold\n\n#def calc_error(n_estimators):\n#    model=XGBClassifier(n_estimators=n_estimators, random_state=0,learning_rate=0.05)\n#    model.fit(X_train,y_train)\n#    preds = model.predict(X_valid)\n#    return(accuracy_score(preds,y_valid))\n#pass\n\n#error={}\n#for i in range(1,20):\n#    error[50*i]=calc_error(50*i)\n    \n#plt.plot(error.keys(),error.values())","4a8a17c9":"#y_train=y_train-1\n#y_train.head()","65517d49":"#y_valid=y_valid-1","38c2aded":"import xgboost as xgb\nfrom sklearn.metrics import f1_score,accuracy_score\nxgb_clf = xgb.XGBClassifier(objective = \"multi:softmax\")\n# Fit model\nxgb_model = xgb_clf.fit(X_train, y_train)\n# Predictions\ny_train_preds = xgb_model.predict(X_train)\ny_valid_preds = xgb_model.predict(X_valid)\n# Print F1 scores and Accuracy\nprint(\"Training F1 Micro Average: \", f1_score(y_train, y_train_preds, average = \"micro\"))\nprint(\"Valid F1 Micro Average: \", f1_score(y_valid, y_valid_preds, average = \"micro\"))\nprint(\"Valid Accuracy: \", accuracy_score(y_valid, y_valid_preds))","de03d41f":"from sklearn.model_selection import RandomizedSearchCV\nxgb_clf1 = xgb.XGBClassifier(eval_metric = ['merror','auc'], objective = 'multi:softmax')\n# Create parameter grid\nparams = {\"learning_rate\": [0.1, 0.3],\n               \"gamma\" : [0.1, 0.3, 0.5],\n               \"max_depth\": [4, 7],\n               \"colsample_bytree\": [0.3, 0.6],\n               \"subsample\": [0.2, 0.5, 0.6],\n               \"reg_alpha\": [0.5, 1],\n               \"reg_lambda\": [1.5, 2, 3.5],\n               \"min_child_weight\": [3, 5],\n               \"n_estimators\": [250, 500]}\n\n# Create RandomizedSearchCV Object\nxgb_rscv = RandomizedSearchCV(xgb_clf1, param_distributions = params, scoring = \"f1_micro\",\n                             cv = 5, verbose = 3, random_state = 40)\n\n# Fit the model\nxgb_model_tuned = xgb_rscv.fit(X_train, y_train)\n","cbb0817e":"best_params=xgb_model_tuned.best_estimator_.get_params()\n# Predictions\ny_train_preds = xgb_model_tuned.predict(X_train)\ny_valid_preds = xgb_model_tuned.predict(X_valid)\n# Print F1 scores and Accuracy\nprint(\"Training F1 Micro Average: \", f1_score(y_train, y_train_preds, average = \"micro\"))\nprint(\"Valid F1 Micro Average: \", f1_score(y_valid, y_valid_preds, average = \"micro\"))\nprint(\"Valid Accuracy: \", accuracy_score(y_valid, y_valid_preds))","d0446583":"#num_boost_round = model.best_iteration + 1\n#print(num_boost_round)\n#best_model = xgb.train(\n#    params,\n#    dtrain,\n#    num_boost_round=num_boost_round,\n#    evals=[(dtest, \"Test\")]\n#)","0384065a":"X_test=pd.read_csv('\/kaggle\/input\/learn-together\/test.csv')\nfor col in X_test.columns:\n    print(col)","70465454":"X_test=pd.read_csv('\/kaggle\/input\/learn-together\/test.csv',index_col=['Id'])\n#dpredict = xgb.DMatrix(X_test)\nid=X_test.index\npreds_test = xgb_model_tuned.predict(X_test)\n#preds_test=preds_test+1\nout=pd.DataFrame({'Id':id,'Cover_Type':preds_test})\nout.head(5)\nout.to_csv('submission.csv', index=False)","6b6d97d9":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission.csv')","1f0e198c":"Lets try some hyper paramter tuning","e835a489":"Base line model. Inspired by https:\/\/towardsdatascience.com\/from-zero-to-hero-in-xgboost-tuning-e48b59bfaf58"}}