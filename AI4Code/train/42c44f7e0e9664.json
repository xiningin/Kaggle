{"cell_type":{"add95023":"code","2375766e":"code","71a7e44f":"code","f863c2e9":"code","e666c1e8":"code","c8b51751":"code","e767f480":"code","dc14ed46":"code","f0f3e225":"code","6c0634f9":"code","97a441b1":"code","912ba726":"code","6132d87d":"code","127c9daf":"code","6fdedb9a":"code","5acade92":"code","25634891":"code","9bd215b7":"code","f307e23d":"code","181b6448":"code","0613207e":"code","69588cbe":"code","dd51af34":"code","5c620527":"code","4b9c9ac0":"code","90cda9a3":"code","89a1be09":"code","75ebea8d":"code","221b5d88":"code","45f3aaf0":"code","d34d9b54":"code","9fd04cb6":"code","311d2a4c":"code","a3c62def":"code","e3e2ed29":"code","dfeef910":"code","6b419116":"code","eb2e9550":"code","5d1c8b9b":"code","a7932e2e":"code","146e8201":"code","ce0dd9ef":"code","2130270e":"code","56a94655":"code","6d0f659e":"code","57a29428":"code","2a602882":"code","e1ee92d8":"code","eda1471d":"code","2d8b4ccc":"code","f9398a8c":"code","b7f3cf68":"code","6cee88a9":"code","42a60d6b":"code","c0b01e63":"code","1bc2c23b":"code","3d179bc1":"code","ac956928":"code","d1f34134":"code","48352738":"code","f020e9d6":"code","fc323d88":"code","8d43904a":"code","fc51e62a":"code","4abb5d3e":"code","dc0a5445":"code","819ceb33":"code","1ed835aa":"code","0de7bc2b":"code","b05ab69f":"code","f76af8a4":"code","204b5bd2":"code","b030506a":"code","0dc00cb9":"code","43281d49":"code","88229a73":"code","a97e0da6":"code","8d33b674":"code","c8b29e20":"code","bceb812a":"code","07ce92b7":"code","b711abb5":"code","d0a9ce3c":"code","5b6e5e5e":"code","9faadfe4":"code","6976e855":"code","1865d87b":"code","daaf876f":"code","506a8e36":"code","bdf52405":"code","67bafc07":"code","f93ac495":"code","1839eb53":"code","09fc26c8":"code","97ae1994":"code","ce888182":"code","24d5b5aa":"code","a1e65c5d":"code","2c86751d":"code","5593d382":"code","0a545fa3":"code","7d16d92d":"code","43baf6e9":"code","86c5c4a0":"code","fce72196":"code","4bb43e55":"code","2f262608":"code","58705d4d":"code","1d0b690a":"code","7968b42b":"markdown","23072f81":"markdown","5f186ab4":"markdown","6704fa17":"markdown","d1102191":"markdown","4a5da7c6":"markdown","f890fca1":"markdown","3e1ee7fa":"markdown","98fd3924":"markdown","d789c0df":"markdown","53a0a01c":"markdown","d86bef83":"markdown","3f8ca486":"markdown","6df8946c":"markdown","9bde6799":"markdown","89657110":"markdown","2f02b761":"markdown","bfa3cc0f":"markdown","4c117b7b":"markdown","a07bdcba":"markdown","45648dcc":"markdown","02e114d7":"markdown","b094bdb7":"markdown","40d87fb9":"markdown","8c0463ab":"markdown","0c218434":"markdown","45acffc9":"markdown","a2726d23":"markdown","519dbe82":"markdown","22a1e1d6":"markdown","421fd3c5":"markdown","5bc813e7":"markdown","cc97b33d":"markdown","7ab9ff31":"markdown","cf1d6e54":"markdown","3e07aa2a":"markdown","f4969ce4":"markdown","c362a14f":"markdown","688b99ea":"markdown","d9d3b51d":"markdown","722836c2":"markdown","acb85f95":"markdown","4855c6d0":"markdown","aeee0c15":"markdown","e1fbf71f":"markdown","3774a204":"markdown","7dcf498e":"markdown","5cca3756":"markdown","1eac4969":"markdown","c48a2600":"markdown","37654022":"markdown","d44d2683":"markdown","285599c6":"markdown","f92f3470":"markdown","0cf6718d":"markdown","65ddf8b9":"markdown","4024301e":"markdown","a4a24313":"markdown","9a185b45":"markdown","3244f6c8":"markdown","cc9e71ad":"markdown","48d21843":"markdown","394dab14":"markdown","ada4c25d":"markdown","3da4567f":"markdown","a4067ddb":"markdown","22e4283f":"markdown","fc7749e4":"markdown","0893a1c3":"markdown","aa1a9694":"markdown","6c3ebd4c":"markdown","ae3c8a4e":"markdown","ed418484":"markdown","0c0bc932":"markdown","f7cc0ac1":"markdown"},"source":{"add95023":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='whitegrid',color_codes =True)\nsns.set(font_scale=1)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","2375766e":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","71a7e44f":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","f863c2e9":"test_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","e666c1e8":"train_df.head()","c8b51751":"test_df.head()","e767f480":"#Save test Id for prediction\ntest_df_Id = test_df['Id']","dc14ed46":"train_df.shape,test_df.shape","f0f3e225":"test_df_Id.shape","6c0634f9":"train_df.info()","97a441b1":"train_df.get_dtype_counts()","912ba726":"train_df.describe()","6132d87d":"\nplt.figure(figsize=(8,8))\nplt.scatter(x = train_df['GrLivArea'], y = train_df['SalePrice'],color='yellowgreen',edgecolor ='white')\n\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","127c9daf":"plt.figure(figsize=(7,7))\nsns.distplot(train_df['SalePrice'],color = 'hotpink',hist = 25)","6fdedb9a":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())","5acade92":"#Need to remove Outliers\ntrain_df = train_df[train_df.GrLivArea < 4500]\ntrain_df.reset_index(drop=True, inplace=True)\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\ny = train_df['SalePrice'].reset_index(drop=True)","25634891":"train_df.shape","9bd215b7":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())","f307e23d":"plt.figure(figsize=(8,8))\nsns.distplot(train_df['SalePrice'],color='chartreuse',hist=20)\n#stats.probplot(train_df['SalePrice'],plot=plt)","181b6448":"train_df['SalePrice'].shape","0613207e":"corre = train_df.corr()['SalePrice']\ncorre[np.argsort(corre,axis=0)[::-1]]","69588cbe":"num_feat=train_df.columns[train_df.dtypes!=object]\nnum_feat=num_feat[1:-1] \nlabels = []\nvalues = []\nfor col in num_feat:\n    labels.append(col)\n    values.append(np.corrcoef(train_df[col].values, train_df.SalePrice.values)[0,1])\n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(7,10))\nrects = ax.barh(ind, np.array(values), color='magenta')\nax.set_yticks(ind+((width)\/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation Coefficients w.r.t Sale Price\");","dd51af34":"correlations=train_df.corr()\nattrs = correlations.iloc[:-1,:-1] # all except target\n\nthreshold = 0.5\nimportant_corrs = (attrs[abs(attrs) > threshold][attrs != 1.0]) \\\n    .unstack().dropna().to_dict()\n\nunique_important_corrs = pd.DataFrame(\n    list(set([(tuple(sorted(key)), important_corrs[key]) \\\n    for key in important_corrs])), \n        columns=['Attribute Pair', 'Correlation'])\n\n    # sorted by absolute value\nunique_important_corrs = unique_important_corrs.ix[\n    abs(unique_important_corrs['Correlation']).argsort()[::-1]]\n\nunique_important_corrs","5c620527":"corremat = train_df.corr()\ntop_corr_features = corremat.index[(corremat['SalePrice'])> 0.5]\nplt.figure(figsize=(10,10))\ng= sns.heatmap(train_df[top_corr_features].corr(),annot=True,cmap='viridis',linewidths=.5)","4b9c9ac0":"most_corr = pd.DataFrame(top_corr_features)\nmost_corr.columns = ['Most Correlated Features']\n\nmost_corr","90cda9a3":"# Overall Quality vs Sale Price\ndata = pd.concat([train_df['SalePrice'], train_df['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 8))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data,palette=\"Greens\")\n","89a1be09":"plt.figure(figsize=(8,8))\nsns.distplot(train_df[\"YearBuilt\"],color='darkorchid', kde=False,hist=10)","75ebea8d":"plt.figure(figsize=(8,8))\nsns.distplot(train_df[\"YearRemodAdd\"].astype(int),color='lime', kde=False);","221b5d88":"# Total Rooms vs Sale Price\nplt.figure(figsize=(10,10))\nsns.jointplot(x=train_df['YearRemodAdd'], y=train_df['SalePrice'],kind='reg',\n              height=8,color= 'mediumvioletred')","45f3aaf0":"# Basement Area vs Sale Price\nsns.jointplot(x=train_df['TotalBsmtSF'], y=train_df['SalePrice'], kind='reg',color='darkorange',\n             height=8)\n","d34d9b54":"# First Floor Area vs Sale Price\nsns.jointplot(x=train_df['1stFlrSF'], y=train_df['SalePrice'], kind='reg',color='crimson',height=8)","9fd04cb6":"# Living Area vs Sale Price\nsns.jointplot(x=train_df['GrLivArea'], y=train_df['SalePrice'], kind='reg',color='chartreuse',height=8)","311d2a4c":"# Total Rooms vs Sale Price\nplt.figure(figsize=(8,8))\nsns.boxplot(x=train_df['TotRmsAbvGrd'], y=train_df['SalePrice'],color='fuchsia')","a3c62def":"upperlimit = np.percentile(train_df.SalePrice.values, 99.5)\ntrain_df['SalePrice'].ix[train_df['SalePrice']>upperlimit] = upperlimit\nplt.figure(figsize=(10,7))\nplt.scatter(range(train_df.shape[0]), train_df[\"SalePrice\"].values,color='orangered',edgecolor='white')\nplt.title(\"Distribution of Sale Price\")\nplt.xlabel(\"Number of Occurences\")\nplt.ylabel(\"Sale Price\");","e3e2ed29":"# Garage Area vs Sale Price\nplt.figure(figsize=(8,7))\nsns.violinplot(x=train_df['GarageCars'], y=train_df['SalePrice'],\n               palette=['fuchsia','red','yellow','lime','blue'])\n","dfeef910":"# Removing outliers \ntrain_df = train_df.drop(train_df[(train_df['GarageCars']>3) \n                         & (train_df['SalePrice']<350000)].index).reset_index(drop=True)","6b419116":"# Garage Area vs Sale Price after removing Outliers\nplt.figure(figsize=(8,7))\nsns.violinplot(x=train_df['GarageCars'], y=train_df['SalePrice'],\n               palette=['fuchsia','red','yellow','lime'])","eb2e9550":"train_df.shape","5d1c8b9b":"# Garage Area vs Sale Price\nplt.figure(figsize=(9,6))\nsns.scatterplot(x=train_df['GarageArea'], y=train_df['SalePrice'],color='indigo',edgecolor='white')","a7932e2e":"train_df.shape","146e8201":"# It seems we have nulls so we will use the imputer strategy later on.\nMissing = pd.concat([train_df.isnull().sum(), test_df.isnull().sum()], axis=1, keys=['train', 'test'])\nMissing[Missing.sum(axis=1) > 0]","ce0dd9ef":"\n#missing data percent plot\ntotal = train_df.isnull().sum()\ntotal = total[total>0]\ntotal = total.sort_values(ascending=False)\n\npercent = (train_df.isnull().sum()\/train_df.isnull().count())\npercent = percent[percent>0]\npercent = percent.sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","2130270e":"total = test_df.isnull().sum()\ntotal = total[total>0]\ntotal = total.sort_values(ascending=False)\n\npercent = (test_df.isnull().sum()\/test_df.isnull().count())\npercent = percent[percent>0]\npercent = percent.sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","56a94655":"train_df.select_dtypes(include=['object']).columns\n","6d0f659e":"train_df.select_dtypes(exclude=['object']).columns","57a29428":"Categorical = len(train_df.select_dtypes(include=['object']).columns)\nnumerical = len(train_df.select_dtypes(exclude=['object']).columns)\nprint('Categorical:',Categorical)\nprint('Numerical:',numerical)","2a602882":"train_features = train_df.drop(['SalePrice'], axis=1)\ntest_features = test_df\ntotal_features = pd.concat([train_features, test_features]).reset_index(drop=True)","e1ee92d8":"total_features.shape\n","eda1471d":"# Since these column are actually a category , using a numerical number will lead the model to assume\n# that it is numerical , so we convert to string .\ntotal_features['MSSubClass'] = total_features['MSSubClass'].apply(str)\ntotal_features['YrSold'] = total_features['YrSold'].astype(str)\ntotal_features['MoSold'] = total_features['MoSold'].astype(str)\n\n\n\n## Filling these columns With most suitable value for these columns \ntotal_features['Functional'] = total_features['Functional'].fillna('Typ') \ntotal_features['Electrical'] = total_features['Electrical'].fillna(\"SBrkr\") \ntotal_features['KitchenQual'] = total_features['KitchenQual'].fillna(\"TA\") \ntotal_features[\"PoolQC\"] = total_features[\"PoolQC\"].fillna(\"None\")\n\n\n\n## Filling these with MODE , i.e. , the most frequent value in these columns .\ntotal_features['Exterior1st'] = total_features['Exterior1st'].fillna(total_features['Exterior1st'].mode()[0]) \ntotal_features['Exterior2nd'] = total_features['Exterior2nd'].fillna(total_features['Exterior2nd'].mode()[0])\ntotal_features['SaleType'] = total_features['SaleType'].fillna(total_features['SaleType'].mode()[0])","2d8b4ccc":"## Missing data in GarageYrBit most probably means missing Garage , so replace NaN with zero . \n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    total_features[col] = total_features[col].fillna(0)\n\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    total_features[col] = total_features[col].fillna('None')\n\n    \n## Same with basement features\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    total_features[col] = total_features[col].fillna('None')","f9398a8c":"total_features['MSZoning'] = total_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","b7f3cf68":"objects = []\nfor i in total_features.columns:\n    if total_features[i].dtype == object:\n        objects.append(i)\ntotal_features.update(total_features[objects].fillna('None'))\nprint([objects])","6cee88a9":"# We are still filling up missing values \ntotal_features['LotFrontage'] = total_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in total_features.columns:\n    if total_features[i].dtype in numeric_dtypes:\n        numerics.append(i)\ntotal_features.update(total_features[numerics].fillna(0))\nnumerics[1:10]","42a60d6b":"total_features.isnull().sum()","c0b01e63":"#Checking there is any null value or not\nplt.figure(figsize=(10, 5))\nsns.heatmap(total_features.isnull())","1bc2c23b":"#MSSubClass=The building class\ntotal_features['MSSubClass'] = total_features['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ntotal_features['OverallCond'] = total_features['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ntotal_features['YrSold'] = total_features['YrSold'].astype(str)\ntotal_features['MoSold'] = total_features['MoSold'].astype(str)","3d179bc1":"# Removing features that are not very useful . This can be understood only by doing proper EDA on data\n\ntotal_features = total_features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n","ac956928":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive',  'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(total_features[c].values)) \n    total_features[c] = lbl.transform(list(total_features[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(total_features.shape))","d1f34134":"# Adding new features . Make sure that you understand this. \n\ntotal_features['YrBltAndRemod']=total_features['YearBuilt']+total_features['YearRemodAdd']\ntotal_features['TotalSF']=total_features['TotalBsmtSF'] + total_features['1stFlrSF'] + total_features['2ndFlrSF']\n\ntotal_features['Total_sqr_footage'] = (total_features['BsmtFinSF1'] + total_features['BsmtFinSF2'] +\n                                 total_features['1stFlrSF'] + total_features['2ndFlrSF'])\n\ntotal_features['Total_Bathrooms'] = (total_features['FullBath'] + (0.5 * total_features['HalfBath']) +\n                               total_features['BsmtFullBath'] + (0.5 * total_features['BsmtHalfBath']))\n\ntotal_features['Total_porch_sf'] = (total_features['OpenPorchSF'] + total_features['3SsnPorch'] +\n                              total_features['EnclosedPorch'] + total_features['ScreenPorch'] +\n                              total_features['WoodDeckSF'])","48352738":"total_features.shape","f020e9d6":"## For ex, if PoolArea = 0 , Then HasPool = 0 too\n\ntotal_features['haspool'] = total_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntotal_features['has2ndfloor'] = total_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntotal_features['hasgarage'] = total_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntotal_features['hasbsmt'] = total_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntotal_features['hasfireplace'] = total_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","fc323d88":"total_features","8d43904a":"#from scipy import stats\n#from scipy.stats import norm, skew #for some statistics","fc51e62a":"numeric_feats = total_features.dtypes[total_features.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = total_features[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","4abb5d3e":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    total_features[feat] = boxcox1p(total_features[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","dc0a5445":"numeric_feats = total_features.dtypes[total_features.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = total_features[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","819ceb33":"final_features = pd.get_dummies(total_features).reset_index(drop=True)\nfinal_features.shape","1ed835aa":"final_features","0de7bc2b":"#spliting the data into train and test datasets\ntrain_data=final_features.iloc[:1453]\ntest_data=final_features.iloc[1453:]\nprint(train_data.shape)\ntest_data.shape","b05ab69f":"X_train = train_data","f76af8a4":"y_train = train_df['SalePrice']","204b5bd2":"X_test = test_data","b030506a":"X_train.shape,y_train.shape,X_test.shape","0dc00cb9":"X_test","43281d49":"X_train","88229a73":"y_train","a97e0da6":"#Train the model\n\nlin_model = linear_model.LinearRegression()","8d33b674":"#Fit the model\nlin_model.fit(X_train, y_train)","c8b29e20":"linear_accuracy = round(lin_model.score(X_train,y_train)*100,2)\nprint(round(linear_accuracy,2),'%')","bceb812a":"#Train the model\n#from sklearn.ensemble import RandomForestRegressor\nrandom_model = RandomForestRegressor(n_estimators=1000)","07ce92b7":"#Fit\nrandom_model.fit(X_train, y_train)","b711abb5":"random_accuracy = round(random_model.score(X_train,y_train)*100,2)\nprint(round(random_accuracy,2),'%')","d0a9ce3c":"#Train the model\n\nGBR_model = GradientBoostingRegressor(n_estimators=100, max_depth=4)","5b6e5e5e":"#Fit\nGBR_model.fit(X_train, y_train)","9faadfe4":"GBR_accuracy = round(GBR_model.score(X_train,y_train)*100,2)\nprint(round(GBR_accuracy,2),'%')","6976e855":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","1865d87b":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train.values, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","daaf876f":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","506a8e36":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n","bdf52405":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","67bafc07":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","f93ac495":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","1839eb53":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","09fc26c8":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","97ae1994":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ce888182":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","24d5b5aa":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a1e65c5d":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","2c86751d":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","5593d382":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)  ","0a545fa3":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","7d16d92d":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","43baf6e9":"averaged_models.fit(X_train.values, y_train.values)\nstacked_train_pred = averaged_models.predict(X_train.values)\nstacked_pred = np.expm1(averaged_models.predict(X_test.values))\nprint(rmsle(y_train.values, stacked_train_pred))","86c5c4a0":"model_xgb.fit(X_train, y_train)\nxgb_train_pred = model_xgb.predict(X_train)\nxgb_pred = np.expm1(model_xgb.predict(X_test))\nprint(rmsle(y_train, xgb_train_pred))","fce72196":"model_lgb.fit(X_train, y_train)\nlgb_train_pred = model_lgb.predict(X_train)\nlgb_pred = np.expm1(model_lgb.predict(X_test.values))\nprint(rmsle(y_train, lgb_train_pred))","4bb43e55":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","2f262608":"ensemble = stacked_pred*0.70 + xgb_pred*.15 + lgb_pred*0.15","58705d4d":"sub = pd.DataFrame()\nsub['Id'] = test_df_Id\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","1d0b690a":"sub","7968b42b":"\n\nLet's take a look at how each relates to Sale Price and do some pre-cleaning on each feature if necessary.\n\n1.OverallQual: Rates the overall material and finish of the house (1 = Very Poor, 10 = Very Excellent)<br>\n2\tYearBuilt: Original construction date<br>\n3\tYearRemodAdd :Remodel date<br>\n4\tTotalBsmtSF: Total square feet of basement area<br>\n5\t1stFlrSF: First Floor square feet<br>\n6\tGrLivArea: Above grade (ground) living area square feet<br>\n7\tFullBath: Full bathrooms above grade<br>\n8\tTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)<br>\n9\tGarageCars: Size of garage in car capacity<br>\n10\tGarageArea: Size of garage in square feet<br>\n11\tSalePrice\n\n\n","23072f81":"SalePrice is not uniformly distributed and is skewed towards the right . Therefore , we use log1p to remove the skewness .","5f186ab4":"**Import Libraries**","6704fa17":"we can say that bagging will mainly focus at getting an ensemble model with less variance than its components whereas boosting and stacking will mainly try to produce strong models less biased than their components (even if variance can also be reduced).","d1102191":"**Skewedness and Kurtosis: **<br>\nSkewedness:\n\nA skewness of zero or near zero indicates a symmetric distribution. A negative value for the skewness indicate a left skewness (tail to the left) A positive value for te skewness indicate a right skewness (tail to the right) Skewness\nis the degree of distortion from the symmetrical bell curve or the normal curve. So, a symmetrical distribution will have a skewness of \"0\". There are two types of Skewness: Positive and Negative. Positive Skewness(similar to our target variable distribution) means the tail on the right side of the distribution is longer and fatter. In positive Skewness the mean and median will be greater than the mode. This is similar to this dataset. \n\nKourtosis is a measure of how extreme observations are in a dataset. The greater the kurtosis coefficient , the more peaked the distribution around the mean .<br>\n\n\nThere are three types of Kurtosis: Mesokurtic, Leptokurtic and Platykurtic.<br>\nMesokurtic is similar to normal curve with the standard value of 3. This means that the extreme values of this distrubution is similar to that of a normal distribution.<br>\nLeptokurtic Example of leptokurtic distributions are the T-distributions with small degrees of freedom.<br>\nPlatykurtic: Platykurtic describes a particular statistical distribution with thinner tails than a normal distribution. Because this distribution has thin tails, it has fewer outliers (e.g., extreme values three or more standard deviations from the mean) than do mesokurtic and leptokurtic distributions.","4a5da7c6":" Handling the missing values is one of the greatest challenges faced by analysts, because making the right decision on how to handle it generates robust data models. Let us look at different ways of imputing the missing values.","f890fca1":"### This high accuracy may be misleading because of multicollinearity,letus consider that as well.","3e1ee7fa":"Now we noticed that the Score is improved with avarage base model.","98fd3924":"1. Import Libraries\n1. Load Data\n1. Analyse Target variable\n1. Correlation Analysis\n1. Multicollinearity\n1. Most correlated Features\n1. Visulization\n1. Handling Missing value\n1. Filling missing value\n1. Feature Engineering     <br>\n   1.Box Cox Transformation<br>\n   2.Cross validation<br>\n   3.Regularization Model<br>\n   4.Boosting<br>\n   5.Stacking<br>\n1. Final Training and Prediction\n1. Submission","d789c0df":"Multicollinearity can handle by following method.<br>\n1.Completely remove those variables <br>\n2.Make new feature by adding them or by some other operation.<br>\n3.Use PCA, which will reduce feature set to small number of non-collinear features.\n\n\n\n","53a0a01c":"Observation.\nAs we can see, the multicollinearity still exists in various features. However, we will keep them for now for the sake of learning and lets models(e.x. Regularization models such as Lasso, Ridge) do the clean up later on. Let's go through some of the correlations that still exists.<br>\n\nThere is 0.83 or 83% correlation between GarageYrBlt and YearBuilt.<br>\n83% correlation between TotRmsAbvGrd and GrLivArea.<br>\n89% correlation between GarageCars and GarageArea.<br>\nSimilarly many other features such asBsmtUnfSF, FullBath have good correlation with other independent feature but not so much with the dependent feature.\n we will be using many algorithms as scikit learn modules makes it easy to implement them and get the best possible model. Therefore, we will keep all the features for now.","d86bef83":"\n\nThis code will filll the missing values with the mode (The frequently category appearing) By each MSsubclass:\u00b6\nIdea is that similar MSSubClasses will have similar MSZoning","3f8ca486":"### LightGBM :\nLight GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks.","6df8946c":"**XGBoost:**","9bde6799":"### Elastic Net Regression :\n\nElastic net is always preferred over lasso & ridge regression because it solves the limitations of both methods, while also including each as special cases. So if the ridge or lasso solution is, indeed, the best, then any good model selection routine will identify that as part of the modeling process.","89657110":"3. Assigning An Unique Category\n<br>\nA categorical feature will have a definite number of possibilities.This strategy will add more information into the dataset which will result in the change of variance. Since they are categorical, we need to find one hot encoding to convert it to a numeric form for the algorithm to understand it","2f02b761":"**We need to check below assumtions:**\n\n**Normality** - A normal distribution is an arrangement of a data set in which most values cluster in the middle of the range and the rest taper off symmetrically toward either extreme.\n\nA graphical representation of a normal distribution is sometimes called a bell curve because of its flared shape.\n\n**Homoscedasticity** - This assumption means that the variance around the regression line is the same for all values of the predictor variable\n\n**Linearity**- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n\n**Absence of correlated errors(Multicollineraty)** - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.","bfa3cc0f":"We have merged train and test dataset to handle missing data","4c117b7b":"In ensemble learning theory, we call weak learners (or base models) models that can be used as building blocks for designing more complex models by combining several of them","a07bdcba":"\n2. Replacing With Mean\/Median\/Mode \n<br>\nThis strategy can be applied on a feature which has numeric data ","45648dcc":"**Linear Regression**","02e114d7":"Get_dummies converts Categorical data to numerical , as models don't work with Text data","b094bdb7":"Sales price increase based on Basement area","40d87fb9":"This shows multicollinearity.<br>\nIn regression, \"multicollinearity\" refers to predictors that are correlated with other predictors.  Multicollinearity occurs when your model includes multiple factors that are correlated not just to your response variable, but also to each other. In other words, it results when you have factors that are a bit redundant.","8c0463ab":"### Kernel Ridge Regression :\n\nIt is essential to standardize the predictor variables before constructing the models.\nIt is important to check for multicollinearity,","0c218434":"\nThe point of stacking is to explore a space of different models for the same problem. The idea is that you can attack a learning problem with different types of models which are capable to learn some part of the problem, but not the whole space of the problem. So, you can build multiple different learners and you use them to build an intermediate prediction, one prediction for each learned model. Then you add a new model which learns from the intermediate predictions the same target.\nThis final model is said to be stacked on the top of the others, hence the name.","45acffc9":"**Plotting Correlation**","a2726d23":"### Gradient Boosting Regression :\n\nGradient boosting is a  technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do\n","519dbe82":"5. Using Algorithms Which Support Missing Values\n<br>\nKNN is a machine learning algorithm which works on the principle of distance measure.\n<br>\nRef:     https:\/\/analyticsindiamag.com\/5-ways-handle-missing-values-machine-learning-datasets\/","22a1e1d6":"### Final Training and Prediction\n**StackedRegressor:**\n\n","421fd3c5":"So what can we see till now.<br>\nwe have total 81 variables for train and 80 for test variable<br>\nwe don't have SalePrice variable for test variable because this will be our task to infer SalePrice for test set by learning from train set.<br>\nSo SalePrice is our target variable and rest of the variables are our predictor variables.","5bc813e7":"Averaged base models score\n\nWe just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix.","cc97b33d":"### **Stacking**","7ab9ff31":"Now have a list of strongly correlated values but this list is incomplete as we know that correlation is affected by outliers. So we could proceed as follow:\n\nPlot the numerical features and see which ones have very few or explainable outliers\nRemove the outliers from these features and see which one can have a good correlation without their outliers\nBtw, correlation by itself does not always explain the relationship between data so ploting them could even lead us to new insights and in the same manner, check that our correlated values have a linear relationship to the SalePrice.\n","cf1d6e54":"### **Filling missing values**\nFor a few columns there is lots of NaN entries.\nHowever, reading the data description we find this is not missing data:\nFor PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.","3e07aa2a":"Let's check out all the variables! There are two types of features in housing data, categorical and numerical.\n\nCategorical data is just like it sounds. It is in categories. It isn't necessarily linear, but it follows some kind of pattern. For example, take a feature of \"Downtown\". The response is either \"Near\", \"Far\", \"Yes\", and \"No\".  So we can't really establish any particular order of response to be \"better\" or \"worse\" than the other.\n\nNumerical data is data in number form.  These features are in a linear relationship with each other.","f4969ce4":"4. Predicting The Missing Values\n<br>\nUsing the features which do not have missing values, we can predict the nulls with the help of a machine learning algorithm","c362a14f":"### **Feature Engineering**","688b99ea":"### **Simplest Stacking approach** :<br> Averaging base models<br>\nWe begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)\n\n","d9d3b51d":"Please not Outliers extreme right down","722836c2":"GradientBoostingRegressor","acb85f95":"### Most correlated Features","4855c6d0":"### **visualization**","aeee0c15":"Now we know that skew is reduced","e1fbf71f":"### **Regularization Models**<br>\nWhat makes regression model more effective is its ability of regularizing. The term \"regularizing\" stands for models ability to structurally prevent overfitting by imposing a penalty on the coefficients.<br>\n\nThere are three types of regularizations.<br>\n\nRidge <br>\nLasso <br>\nElastic Net <br>\nThese regularization methods work by penalizing the magnitude of the coefficients of features and at the same time minimizing the error between the predicted value and actual observed values. This minimization becomes a balance between the error (the difference between the predicted value and observed value) and the size of the coefficients. The only difference between Ridge and Lasso is the way they penalize the coefficients. Elastic Net is the combination of these two. Elastic Net adds both the sum of the squares errors and the absolute value of the squared error. ","3774a204":"Although it seems like house prices decrease with age, we can't be entirely sure. Is it because of inflation or stock market crashes? Let's leave the years alone.","7dcf498e":"**No or Little multicollinearity:**<br> Multicollinearity is when there is a strong correlation between independent variables. Linear regression or multilinear regression requires independent variables to have little or no similar features. Multicollinearity can lead to a variety of problems, including:\n\nThe effect of predictor variables estimated by our regression will depend on what other variables are included in our model.<br>\nPredictors can have wildly different results depending on the observations in our sample, and small changes in samples can result in very different estimated effects.<br>\nWith very high multicollinearity, the inverse matrix, the computer calculates may not be accurate.<br>\nWe can no longer interpret a coefficient on a variable as the effect on the target of a one-unit increase in that variable holding the other variables constant. The reason behind that is, when predictors are strongly correlated, there is not a scenario in which one variable can change without a conditional change in another variable.","5cca3756":"We need to remove the outliers the Garage area 4 as price is showing less compared to others","1eac4969":"Remodeling have very slight impact on sales price","c48a2600":"Shows Positive skewness","37654022":"**LightGBM:**","d44d2683":"Base models scores <br>\nLet's see how these base models perform on the data by evaluating the cross-validation rmsle error","285599c6":"**Introduction:** <br>\nThe variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home. Great for practicing skills such as:\n\nCreative feature engineering\nAdvanced regression techniques like random forest and gradient boosting","f92f3470":"For missing values in numerical cols , we fillNa with 0.","0cf6718d":"### **bagging, boosting and stacking**","65ddf8b9":"**Numerical Columns**","4024301e":"1. Delete Row\n<br>\nThis method commonly used to handle the null values. Here, we either delete a particular row if it has a null value for a particular feature and a particular column if it has more than 70-75% of missing values.\n\n \n","a4a24313":"**Ensemble prediction:**","9a185b45":"## **Handling Missing Value**","3244f6c8":"Sales price increase based on OverallQual increases.","cc9e71ad":"We can mention three major kinds of meta-algorithms that aims at combining weak learners:<br>\nbagging, that often considers homogeneous weak learners, learns them independently from each other in parallel and combines them following some kind of deterministic averaging process.<br>\nboosting, that often considers homogeneous weak learners, learns them sequentially in a very adaptative way (a base model depends on the previous ones) and combines them following a deterministic strategy.<br>\nstacking, that often considers heterogeneous weak learners, learns them in parallel and combines them by training a meta-model to output a prediction based on the different weak models predictions","48d21843":"**Define a cross validation strategy**","394dab14":"Transforming some variables ","ada4c25d":"### **LASSO Regression **\n\nLasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain parts of model selection, like variable selection\/parameter elimination.\n\nThe acronym \u201cLASSO\u201d stands for Least Absolute Shrinkage and Selection Operator.","3da4567f":"**Analyse Target variable**","a4067ddb":"**Unique importance of Correlation**","22e4283f":"**Box Cox Transformation of (highly) skewed features**\n\nA Box Cox transformation is a way to transform non-normal dependent variables into a normal shape.Transformation technique is useful to stabilize variance, make the data more normal distribution-like.\n\nWe use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n\nNote that setting  \u03bb=0  is equivalent to log1p used above for the target variable.\n\n","fc7749e4":"***I hope you enjoyed this notebook.***","0893a1c3":"### XGBoost :\n\nXGBoost is an algorithm that has recently been dominating applied machine learning  for structured or tabular data.\n\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance.It is a more advanced version of the gradient boosting method. The main aim of this algorithm is to increase speed and to increase the efficiency of your competitions","aa1a9694":"Above columns are filled with 'None'\n\n","6c3ebd4c":"### Distribution of Target value","ae3c8a4e":"**Correlation Analysis**","ed418484":"### Categorical &Numerical Data","0c0bc932":"Cross Validation is used to assess the predictive performance of the models and and to judge how they perform outside the sample to a new data set also known as test data\nThe motivation to use cross validation techniques is that when we fit a model, we are fitting it to a training dataset. Without cross validation we only have information on how does our model perform to our in-sample data. Ideally we would like to see how does the model perform when we have a new data in terms of accuracy of its predictions. In science, theories are judged by its predictive performance.  \n\nWe use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation","f7cc0ac1":"Label Encoding some categorical variables that may contain information in their ordering set"}}