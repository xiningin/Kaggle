{"cell_type":{"30dd801b":"code","b57484f5":"code","f0cdb01c":"code","f820d313":"code","3514bc75":"code","e084af1c":"code","78a130c1":"code","c5846dc5":"code","97d820ea":"code","88973bef":"code","ef9eda2d":"code","1ec446ed":"code","62534bf8":"code","dde23de7":"code","2bd4bf8d":"code","c54b78d9":"code","8cb2d8f1":"code","401ba47a":"code","a4f4076a":"code","053ca8de":"code","7eca640f":"code","7dc36662":"markdown","5421030d":"markdown","75653661":"markdown","175b1774":"markdown","abd0d25a":"markdown","868b3b9b":"markdown","65fee41e":"markdown","8f5065b7":"markdown","761c9864":"markdown","bf8c3478":"markdown","85a8dc0f":"markdown","6946e68a":"markdown","456115bb":"markdown","bc92d040":"markdown"},"source":{"30dd801b":"!pip install -Uqq fastai","b57484f5":"import gc\nimport os\nimport random\nimport numpy as np \nimport pandas as pd \n\n# Torch\nfrom torch.utils.data import Dataset\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\n\n# FASTAI\nfrom fastai.data.core import DataLoaders\nfrom fastai.learner import Learner\nfrom fastai.callback.progress import ProgressCallback\nfrom fastai.optimizer import OptimWrapper\nfrom torch import optim\nfrom fastai.losses import MSELossFlat, L1LossFlat,CrossEntropyLossFlat\nfrom fastai.callback.schedule import Learner\n# https:\/\/docs.fast.ai\/callback.tracker.html\nfrom fastai.callback.tracker import EarlyStoppingCallback, ReduceLROnPlateau, SaveModelCallback\nfrom fastai.data.transforms import IndexSplitter\n\n#scipy for FFT\nimport scipy.signal as signal\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft, fftfreq\nfrom scipy.signal import hilbert, chirp\nfrom scipy.signal import blackman\n\n#sklearn\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import KFold","f0cdb01c":"np.random.seed(10000)\nRANDOM_STATE = 10000","f820d313":"df = pd.read_csv('\/kaggle\/input\/ventilator-pressure-prediction\/train.csv')\ndf_test = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')","3514bc75":"%%time\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e084af1c":"\n\ndict_types = {\n'id': np.int32,\n'breath_id': np.int32,\n'R': np.int8,\n'C': np.int8,\n'time_step': np.float32,\n'u_in': np.float32,\n'u_out': np.int8, \n'pressure': np.float32,\n} \n\nall_pressure = np.sort(df.pressure.unique())\nPRESSURE_MIN = all_pressure[0]\nPRESSURE_MAX = all_pressure[-1]\nPRESSURE_STEP = (all_pressure[1] - all_pressure[0])","78a130c1":"%time\nffta = lambda x: np.abs(fft(np.append(x.values,x.values[0]))[:80])\nffta.__name__ = 'ffta'\n\nfftw = lambda x: np.abs(fft(np.append(x.values,x.values[0])*w)[:80])\nfftw.__name__ = 'fftw'\n\n\n\nN = 80\nw = blackman(N+1)\n\ndef fft_features(df):\n    print(\"Step FFT-features\")\n    df['fft_u_in'] = df.groupby('breath_id')['u_in'].transform(ffta)\n    df['fft_u_in_w'] = df.groupby('breath_id')['u_in'].transform(fftw)\n    df['analytical'] = df.groupby('breath_id')['u_in'].transform(hilbert)\n    df['envelope'] = np.abs(df['analytical'])\n    df['phase'] = np.angle(df['analytical'])\n    df['unwrapped_phase'] = df.groupby('breath_id')['phase'].transform(np.unwrap)\n    df['phase_shift1'] = df.groupby('breath_id')['unwrapped_phase'].shift(1).astype(np.float32)\n    df['IF'] = df['unwrapped_phase'] - df['phase_shift1'].astype(np.float32)\n    df = df.fillna(0)\n    print(\"Finish adding FFT-features\")\n    df = df.drop('analytical',axis=1)\n    return df","c5846dc5":"def add_features(df: pd.DataFrame): \n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n\n    df = df.fillna(0)\n    print(\"Step-1...Completed\")\n\n\n    gc.collect()\n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_in__mean'] = df.groupby(['breath_id'])['u_in'].transform('mean')\n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    print(\"Step-2...Completed\")\n    gc.collect()\n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    \n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    \n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    \n    print(\"Step-3...Completed\")\n    \n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] \/df['count']\n    \n    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n    df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n    df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n    df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n    df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n    print(\"Step-4...Completed\")\n    gc.collect()\n    \n    g = df.groupby('breath_id')['u_in']\n    \n    df['time_step_diff'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n    df['ewm_u_in_mean'] = g.ewm(halflife=9).mean().reset_index(level=0,drop=True)\n    df['ewm_u_in_std'] = g.ewm(halflife=10).std().reset_index(level=0,drop=True)\n    df['ewm_u_in_corr'] = g.ewm(halflife=10).corr().reset_index(level=0,drop=True)\n\n    # adding this make notebook allocate all memory\n    #df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (g.rolling(window=15,min_periods=1).agg({\"15_in_sum\":\"sum\",\"15_in_min\":\"min\",\"15_in_max\":\"max\",\"15_in_mean\":\"mean\"}).reset_index(level=0,drop=True))\n    \n    df['rolling_10_mean'] = g.rolling(window=10, min_periods=1).mean()\\\n                             .reset_index(level=0, drop=True)\n    df['rolling_10_max'] = g.rolling(window=10, min_periods=1).max()\\\n                            .reset_index(level=0, drop=True)\n    df['rolling_10_std'] = g.rolling(window=10, min_periods=1).std()\\\n                            .reset_index(level=0, drop=True)\n\n    df['expand_mean'] = g.expanding(2).mean()\\\n                         .reset_index(level=0, drop=True)\n    df['expand_max'] = g.expanding(2).max()\\\n                        .reset_index(level=0, drop=True)\n    df['expand_std'] = g.expanding(2).std()\\\n                        .reset_index(level=0, drop=True)\n    \n    print(\"Step-5...Completed\")\n    gc.collect()\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    print(\"Step-6...Completed\")\n    return df","97d820ea":"print(\"Train data...\\n\")\ntrain = add_features(df)\ntrain = fft_features(train)\nprint(\"Shape of train df\", train.shape)\ndel df\ngc.collect()\n\nprint(\"\\nTest data...\\n\")\ntest = add_features(df_test)\ntest = fft_features(test)\ngc.collect()","88973bef":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure','id', 'breath_id','one','count','breath_id_lag','breath_id_lag2','breath_id_lagsame','breath_id_lag2same'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id','one','count','breath_id_lag','breath_id_lag2','breath_id_lagsame','breath_id_lag2same'], axis=1)","ef9eda2d":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","1ec446ed":"RS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)","62534bf8":"train = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","dde23de7":"idx = list(range(len(train)))","2bd4bf8d":"train.shape[-2:]","c54b78d9":"class VentilatorDataset(Dataset):\n    def __init__(self, data, target):\n        self.data = torch.from_numpy(data).float()\n        if target is not None:\n            self.targets = torch.from_numpy(target).float()\n                \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if hasattr(self, 'targets'): return self.data[idx], self.targets[idx]\n        else: return self.data[idx]","8cb2d8f1":"class RNNModel(nn.Module):\n    def __init__(self, input_size=64):\n        hidden = [400, 300, 200, 100]\n        super().__init__()\n        self.lstm1 = nn.LSTM(input_size, hidden[0],\n                             batch_first=True, bidirectional=True)\n        self.lstm2 = nn.LSTM(2 * hidden[0], hidden[1],\n                             batch_first=True, bidirectional=True)\n        self.lstm3 = nn.LSTM(2 * hidden[1], hidden[2],\n                             batch_first=True, bidirectional=True)\n        self.lstm4 = nn.LSTM(2 * hidden[2], hidden[3],\n                             batch_first=True, bidirectional=True)\n        self.fc1 = nn.Linear(2 * hidden[3], 50)\n        self.selu = nn.SELU()\n        self.fc2 = nn.Linear(50, 1)\n        self._reinitialize()\n\n    def _reinitialize(self):\n        \"\"\"\n        Tensorflow\/Keras-like initialization\n        \"\"\"\n        for name, p in self.named_parameters():\n            if 'lstm' in name:\n                if 'weight_ih' in name:\n                    nn.init.xavier_uniform_(p.data)\n                elif 'weight_hh' in name:\n                    nn.init.orthogonal_(p.data)\n                elif 'bias_ih' in name:\n                    p.data.fill_(0)\n                    n = p.size(0)\n                    p.data[(n \/\/ 4):(n \/\/ 2)].fill_(1)\n                elif 'bias_hh' in name:\n                    p.data.fill_(0)\n            elif 'fc' in name:\n                if 'weight' in name:\n                    nn.init.xavier_uniform_(p.data)\n                elif 'bias' in name:\n                    p.data.fill_(0)\n\n    def forward(self, x):\n        x, _ = self.lstm1(x)\n        x, _ = self.lstm2(x)\n        x, _ = self.lstm3(x)\n        x, _ = self.lstm4(x)\n        x = self.fc1(x)\n        x = self.selu(x)\n        x = self.fc2(x)\n\n        return x","401ba47a":"batch_size = 1024\nsubmission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\ntest_dataset = VentilatorDataset(test, None)\ntest_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)","a4f4076a":"kf = KFold(n_splits=5, shuffle=True)\npreds_fold = []\noof_df = []\nfor fold, (train_index, valid_index) in enumerate(kf.split(idx)):\n    \n    preds = []\n    model = RNNModel(input_size=train.shape[-1]).to('cuda')\n    print(\"FOLD:\", fold)\n    print(train_index)\n    print(valid_index)\n\n    train_input, valid_input = train[train_index], train[valid_index]\n    train_targets, valid_targets = targets[train_index], targets[valid_index]\n\n    train_dataset = VentilatorDataset(train_input, train_targets)\n    valid_dataset = VentilatorDataset(valid_input, valid_targets)\n    \n    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False)\n    valid_loader = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False)\n    \n    dls = DataLoaders(train_loader, valid_loader)\n    learn = Learner(dls, model, loss_func=L1LossFlat())\n    callbacks = [SaveModelCallback(every_epoch=True, monitor='valid_loss',min_delta=0.1, fname=f\"fastai_fold{fold+1}.pth\"),\n                 ReduceLROnPlateau(monitor='valid_loss', min_delta=0.5, patience=5),\n                EarlyStoppingCallback(monitor='valid_loss', min_delta=0.0005, patience=5)]\n    learn.fit_one_cycle(120, lr_max=2e-3, cbs=callbacks)\n    \n    valid_oof_dataset = VentilatorDataset(valid_input, None)\n    valid_oof_loader = DataLoader(valid_oof_dataset, batch_size = batch_size, shuffle=False)\n    oof_pred = []\n    oof_sample = []\n    \n    with torch.no_grad():\n        for data in valid_oof_loader:\n            pred = model(data.to('cuda')).squeeze(-1).flatten()\n            oof_pred.extend(pred.detach().cpu().numpy())\n        oof_df.append({\n            'pred': oof_pred,\n            'target': valid_targets\n        })\n        for data in test_loader:\n            pred = model(data.to('cuda')).squeeze(-1).flatten()\n            preds.extend(pred.detach().cpu().numpy())\n    preds_fold.append(preds)","053ca8de":"target = []\npred = []\nfor i in range(len(oof_df)):\n    target.extend(oof_df[i]['target'].flatten())\n    pred.extend(oof_df[i]['pred'])\nprint(len(target))    \nprint(len(pred))\nfrom sklearn.metrics import mean_absolute_error\nprint(mean_absolute_error(target,pred))\ntotal = pd.DataFrame(data={'pred': pred, 'target': target})\ntotal.to_csv('oof_score.csv',index=False)","7eca640f":"preds_fold = np.array(preds_fold)\ndf_test['pressure'] = np.median(preds_fold, axis=0)\ndf_test[['id', 'pressure']].to_csv('submission.csv', index=False)","7dc36662":"# EOF","5421030d":"# OOF","75653661":"## FFT features","175b1774":"# Load source datasets","abd0d25a":"# Model","868b3b9b":"# Submission","65fee41e":"# UTILS","8f5065b7":"# Train","761c9864":"Based on:\n* https:\/\/www.kaggle.com\/dienhoa\/ventillator-fastai-lb-0-168-no-kfolds-no-blend\n* https:\/\/www.kaggle.com\/lucasmorin\/spectral-analysis-feature-engineering  \n* https:\/\/www.kaggle.com\/junkoda\/pytorch-lstm-with-tensorflow-like-initialization\/notebook\n\nSo, please upvote them first!\n\nChanges from original\n* Add more callbacks [EarlyStopping and SaveModelCallback]\n* More features\n* Add oof \n* Use folds \n* [L1Loss is MAE](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.L1Loss.html)","bf8c3478":"# Install FASTAI","85a8dc0f":"# Feature Engineering","6946e68a":"# Dataset","456115bb":"# Import","bc92d040":"# Main features"}}