{"cell_type":{"4e7c8f4a":"code","034f961c":"code","5be486a6":"code","32388767":"code","1de212c0":"code","fd7b23a6":"code","25be6df8":"code","52b22495":"code","65a120d3":"code","2b616144":"code","67f43552":"code","19887f56":"code","9a8b4767":"code","5039ec4a":"code","2b152140":"code","2ba74229":"code","81d02c94":"code","b03e3a4a":"code","ab588829":"code","fc122ce8":"code","6cae3735":"code","32d83406":"code","412073be":"markdown","799620f2":"markdown","431212e1":"markdown","526ff135":"markdown","3eeeb2d1":"markdown","98a6de98":"markdown","b1a8897b":"markdown","394a6b4c":"markdown","19c719d6":"markdown","85a9b5b3":"markdown","ed364330":"markdown","6557eefb":"markdown","5d051fb4":"markdown","a7fe136a":"markdown","7f901542":"markdown","7715546e":"markdown","dd59283d":"markdown","8fcd10ab":"markdown","863dc24b":"markdown","d1d6ffc1":"markdown","f1677a3b":"markdown","54203ee9":"markdown","368b9d13":"markdown","68fa0e73":"markdown","9fc37ab6":"markdown"},"source":{"4e7c8f4a":"#importing necessary libraries\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport scipy\nfrom scipy.stats import entropy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,f1_score\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport catboost as cb\nimport lightgbm as lgbm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport optuna","034f961c":"#Data paths\ndataset_path_1st = '..\/input\/bearing-dataset\/1st_test\/1st_test'\ndataset_path_2nd = '..\/input\/bearing-dataset\/2nd_test\/2nd_test'\ndataset_path_3rd = '..\/input\/bearing-dataset\/3rd_test\/4th_test\/txt'","5be486a6":"# Test for the first file\ndataset = pd.read_csv('..\/input\/bearing-dataset\/1st_test\/1st_test\/2003.10.22.12.06.24', sep='\\t')\nax = dataset.plot(figsize = (24,6), title= \"Bearing Vibration\" , legend = True)\nax.set(xlabel=\"cycle(n)\", ylabel=\"vibration\/acceleration(g)\")\nplt.show()","32388767":"# Root Mean Squared Sum\ndef calculate_rms(df):\n    result = []\n    for col in df:\n        r = np.sqrt((df[col]**2).sum() \/ len(df[col]))\n        result.append(r)\n    return result\n\n# extract peak-to-peak features\ndef calculate_p2p(df):\n    return np.array(df.max().abs() + df.min().abs())\n\n# extract shannon entropy (cut signals to 500 bins)\ndef calculate_entropy(df):\n    ent = []\n    for col in df:\n        ent.append(entropy(pd.cut(df[col], 500).value_counts()))\n    return np.array(ent)\n# extract clearence factor\ndef calculate_clearence(df):\n    result = []\n    for col in df:\n        r = ((np.sqrt(df[col].abs())).sum() \/ len(df[col]))**2\n        result.append(r)\n    return result\n\ndef time_features(dataset_path, id_set=None):\n    time_features = ['mean','std','skew','kurtosis','entropy','rms','max','p2p', 'crest', 'clearence', 'shape', 'impulse']\n    cols1 = ['B1_x','B1_y','B2_x','B2_y','B3_x','B3_y','B4_x','B4_y']\n    cols2 = ['B1','B2','B3','B4']\n    \n    # initialize\n    if id_set == 1:\n        columns = [c+'_'+tf for c in cols1 for tf in time_features]\n        data = pd.DataFrame(columns=columns)\n    else:\n        columns = [c+'_'+tf for c in cols2 for tf in time_features]\n        data = pd.DataFrame(columns=columns)\n\n        \n        \n    for filename in os.listdir(dataset_path):\n        # read dataset\n        raw_data = pd.read_csv(os.path.join(dataset_path, filename), sep='\\t')\n        \n        # time features\n        mean_abs = np.array(raw_data.abs().mean())\n        std = np.array(raw_data.std())\n        skew = np.array(raw_data.skew())\n        kurtosis = np.array(raw_data.kurtosis())\n        entropy = calculate_entropy(raw_data)\n        rms = np.array(calculate_rms(raw_data))\n        max_abs = np.array(raw_data.abs().max())\n        p2p = calculate_p2p(raw_data)\n        crest = max_abs \/ rms\n        clearence = max_abs \/ np.array(calculate_clearence(raw_data))\n        shape = rms \/ mean_abs\n        impulse = max_abs \/ mean_abs\n        \n        if id_set == 1:\n            mean_abs = pd.DataFrame(mean_abs.reshape(1,8), columns=[c+'_mean' for c in cols1])\n            std = pd.DataFrame(std.reshape(1,8), columns=[c+'_std' for c in cols1])\n            skew = pd.DataFrame(skew.reshape(1,8), columns=[c+'_skew' for c in cols1])\n            kurtosis = pd.DataFrame(kurtosis.reshape(1,8), columns=[c+'_kurtosis' for c in cols1])\n            entropy = pd.DataFrame(entropy.reshape(1,8), columns=[c+'_entropy' for c in cols1])\n            rms = pd.DataFrame(rms.reshape(1,8), columns=[c+'_rms' for c in cols1])\n            max_abs = pd.DataFrame(max_abs.reshape(1,8), columns=[c+'_max' for c in cols1])\n            p2p = pd.DataFrame(p2p.reshape(1,8), columns=[c+'_p2p' for c in cols1])\n            crest = pd.DataFrame(crest.reshape(1,8), columns=[c+'_crest' for c in cols1])\n            clearence = pd.DataFrame(clearence.reshape(1,8), columns=[c+'_clearence' for c in cols1])\n            shape = pd.DataFrame(shape.reshape(1,8), columns=[c+'_shape' for c in cols1])\n            impulse = pd.DataFrame(impulse.reshape(1,8), columns=[c+'_impulse' for c in cols1])\n            \n        else:\n            mean_abs = pd.DataFrame(mean_abs.reshape(1,4), columns=[c+'_mean' for c in cols2])\n            std = pd.DataFrame(std.reshape(1,4), columns=[c+'_std' for c in cols2])\n            skew = pd.DataFrame(skew.reshape(1,4), columns=[c+'_skew' for c in cols2])\n            kurtosis = pd.DataFrame(kurtosis.reshape(1,4), columns=[c+'_kurtosis' for c in cols2])\n            entropy = pd.DataFrame(entropy.reshape(1,4), columns=[c+'_entropy' for c in cols2])\n            rms = pd.DataFrame(rms.reshape(1,4), columns=[c+'_rms' for c in cols2])\n            max_abs = pd.DataFrame(max_abs.reshape(1,4), columns=[c+'_max' for c in cols2])\n            p2p = pd.DataFrame(p2p.reshape(1,4), columns=[c+'_p2p' for c in cols2])\n            crest = pd.DataFrame(crest.reshape(1,4), columns=[c+'_crest' for c in cols2])\n            clearence = pd.DataFrame(clearence.reshape(1,4), columns=[c+'_clearence' for c in cols2])\n            shape = pd.DataFrame(shape.reshape(1,4), columns=[c+'_shape' for c in cols2])\n            impulse = pd.DataFrame(impulse.reshape(1,4), columns=[c+'_impulse' for c in cols2])\n            \n        mean_abs.index = [filename]\n        std.index = [filename]\n        skew.index = [filename]\n        kurtosis.index = [filename]\n        entropy.index = [filename]\n        rms.index = [filename]\n        max_abs.index = [filename]\n        p2p.index = [filename]\n        crest.index = [filename]\n        clearence.index = [filename]\n        shape.index = [filename]\n        impulse.index = [filename] \n        \n        # concat\n        merge = pd.concat([mean_abs, std, skew, kurtosis, entropy, rms, max_abs, p2p,crest,clearence, shape, impulse], axis=1)\n        data = data.append(merge)\n        \n    if id_set == 1:\n        cols = [c+'_'+tf for c in cols1 for tf in time_features]\n        data = data[cols]\n    else:\n        cols = [c+'_'+tf for c in cols2 for tf in time_features]\n        data = data[cols]\n        \n    data.index = pd.to_datetime(data.index, format='%Y.%m.%d.%H.%M.%S')\n    data = data.sort_index()\n    return data                                  ","1de212c0":"set1 = time_features(dataset_path_1st, id_set=1)\nset1.to_csv('set1_timefeatures.csv')","fd7b23a6":"set1 = pd.read_csv(\".\/set1_timefeatures.csv\")","25be6df8":"set1 = set1.rename(columns={'Unnamed: 0':'time'})\nset1.set_index('time')\nset1.describe()","52b22495":"time_features_list = [\"mean\",\"std\",\"skew\",\"kurtosis\",\"entropy\",\"rms\",\"max\",\"p2p\", \"crest\", \"clearence\", \"shape\", \"impulse\"]\nbearings_xy = [[\"B\"+str(n)+\"_\"+str(o)+\"_\" for n in range(1,5)] for o in ['x','y'] ] \n#print(bearings_xy)\nfor tf in time_features_list:\n    fig = plt.figure()\n    # Divide the figure into a 1x4 grid, and give me the first section\n    ax1 = fig.add_subplot(141)\n    # Divide the figure into a 1x4 grid, and give me the second section\n    ax2 = fig.add_subplot(142)\n    #...so on\n    ax3 = fig.add_subplot(143)\n    ax4 = fig.add_subplot(144)\n    axes = [ax1,ax2,ax3, ax4]\n    \n    for i in range(4):\n        col = bearings_xy[0][i]+tf\n        set1[col].plot(figsize = (36,6), title=\"Bearing{} x-y_\".format(i+1)+tf , legend = True, ax=axes[i])\n        col = bearings_xy[1][i]+tf\n        set1[col].plot(figsize = (36,6) , legend = True, ax=axes[i])\n        axes[i].set(xlabel=\"cycle\", ylabel=\"value\")\n        \n    \n    ","65a120d3":"#Health Status labels are added according to following dictionary\nB1 ={\n    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-10-23 09:14:13\"],\n    \"suspect\" : [\"2003-10-23 09:24:13\" , \"2003-11-08 12:11:44\"],\n    \"normal\" : [\"2003-11-08 12:21:44\" , \"2003-11-19 21:06:07\"],\n    \"suspect_1\" : [\"2003-11-19 21:16:07\" , \"2003-11-24 20:47:32\"],\n    \"imminent_failure\" : [\"2003-11-24 20:57:32\",\"2003-11-25 23:39:56\"]\n}\nB2 = {\n    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-11-01 21:41:44\"],\n    \"normal\" : [\"2003-11-01 21:51:44\" , \"2003-11-24 01:01:24\"],\n    \"suspect\" : [\"2003-11-24 01:11:24\" , \"2003-11-25 10:47:32\"],\n    \"imminient_failure\" : [\"2003-11-25 10:57:32\" , \"2003-11-25 23:39:56\"]\n}\n\nB3 = {\n    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-11-01 21:41:44\"],\n    \"normal\" : [\"2003-11-01 21:51:44\" , \"2003-11-22 09:16:56\"],\n    \"suspect\" : [\"2003-11-22 09:26:56\" , \"2003-11-25 10:47:32\"],\n    \"Inner_race_failure\" : [\"2003-11-25 10:57:32\" , \"2003-11-25 23:39:56\"]\n}\n\nB4 = {\n    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-10-29 21:39:46\"],\n    \"normal\" : [\"2003-10-29 21:49:46\" , \"2003-11-15 05:08:46\"],\n    \"suspect\" : [\"2003-11-15 05:18:46\" , \"2003-11-18 19:12:30\"],\n    \"Rolling_element_failure\" : [\"2003-11-19 09:06:09\" , \"2003-11-22 17:36:56\"],\n    \"Stage_two_failure\" : [\"2003-11-22 17:46:56\" , \"2003-11-25 23:39:56\"]\n}\n","2b616144":"B1_state = list()\nB2_state = list()\nB3_state = list()\nB4_state = list()\ncnt = 0\n\nfor row in set1[\"time\"]:\n    cnt += 1\n    # B1\n    if cnt<=151:\n        B1_state.append(\"early\")\n    if 151 < cnt <=600:\n        B1_state.append(\"suspect\")\n    if 600 < cnt <=1499:\n        B1_state.append(\"normal\")\n    if 1499 < cnt <=2098:\n        B1_state.append(\"suspect\")\n    if 2098 < cnt <= 2156:\n        B1_state.append(\"imminent_failure\")\n    #B2\n    if cnt<=500:\n        B2_state.append(\"early\")\n    if 500 < cnt <=2000:\n        B2_state.append(\"normal\")\n    if 2000 < cnt <=2120:\n        B2_state.append(\"suspect\")\n    if 2120< cnt <=2156:\n        B2_state.append(\"imminet_failure\")\n\n    #B3\n    if cnt<=500:\n        B3_state.append(\"early\")\n    if 500 < cnt <= 1790:\n        B3_state.append(\"normal\")\n    if 1790 < cnt <=2120:\n        B3_state.append(\"suspect\")\n    if 2120 < cnt <=2156:\n        B3_state.append(\"Inner_race_failure\")\n    #B4\n    if cnt<=200:\n        B4_state.append(\"early\")\n    if 200 < cnt <=1000:\n        B4_state.append(\"normal\")\n    if 1000 < cnt <= 1435:\n        B4_state.append(\"suspect\")\n    if 1435 < cnt <=1840:\n        B4_state.append(\"Inner_race_failure\")\n    if 1840 < cnt <=2156:\n        B4_state.append(\"Stage_two_failure\")\n#controlling the counts\nfrom collections import Counter\nprint(Counter(B1_state))\nprint(Counter(B2_state))\nprint(Counter(B3_state))\nprint(Counter(B4_state))\n\nset1[\"B1_state\"] = B1_state\nset1[\"B2_state\"] = B2_state\nset1[\"B3_state\"] = B3_state\nset1[\"B4_state\"] = B4_state\n\nset1.head()","67f43552":"B1_cols = [col for col in set1.columns if \"B1\" in col]\nB2_cols = [col for col in set1.columns if \"B2\" in col]\nB3_cols = [col for col in set1.columns if \"B3\" in col]\nB4_cols = [col for col in set1.columns if \"B4\" in col]\n\nB1 = set1[B1_cols]\nB2 = set1[B2_cols]\nB3 = set1[B3_cols]\nB4 = set1[B4_cols]\ncols = ['Bx_mean','Bx_std','Bx_skew','Bx_kurtosis','Bx_entropy','Bx_rms','Bx_max','Bx_p2p','Bx_crest', 'Bx_clearence', 'Bx_shape', 'Bx_impulse',\n        'By_mean','By_std','By_skew','By_kurtosis','By_entropy','By_rms','By_max','By_p2p','By_crest', 'By_clearence', 'By_shape', 'By_impulse',\n        'class']\nB1.columns = cols\nB2.columns = cols\nB3.columns = cols\nB4.columns = cols\nfinal_data = pd.concat([B1,B2,B3,B4], axis=0, ignore_index=True)\nfinal_data.describe()","19887f56":"X = final_data.copy()\ny = X.pop(\"class\")\nle = preprocessing.LabelEncoder()\nle.fit(y)\ny = le.transform(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state =1)","9a8b4767":"xgb_cl = xgb.XGBClassifier(use_label_encoder=False)\nxgb_cl.fit(X_train, y_train)\npreds = xgb_cl.predict(X_test)\naccuracy_score(y_test, preds)","5039ec4a":"x_axis_cols = [\"Bx_\"+tf for tf in time_features_list]\nprint(x_axis_cols)\nX_xy = X.copy()\n#y_axis data\nX_y = X_xy.drop(x_axis_cols, axis=1)\n#x_axis data\nX_x = X_xy[x_axis_cols]\ncols = ['B_mean','B_std','B_skew','B_kurtosis','B_entropy',\n        'B_rms','B_max','B_p2p','B_crest', 'B_clearence', 'B_shape', 'B_impulse']\nX_y.columns = cols\nX_x.columns = cols\nX_x_train, X_x_test, y_x_train, y_x_test = train_test_split(X_x, y, test_size = 0.3, random_state =1)\nX_y_train, X_y_test, y_y_train, y_y_test = train_test_split(X_y, y, test_size = 0.3, random_state =1)","2b152140":"names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \n         #\"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\",\"XGBoost\",\"CatGBoost\",\"LightGBoost\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    #GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis(),\n    xgb.XGBClassifier(),\n    cb.CatBoostClassifier(verbose = False),\n    lgbm.LGBMClassifier()\n    ]\n\nfor name, clf in zip(names,classifiers):\n    print(\"training \"+name+\" ...\")\n    clf.fit(X_x_train,y_x_train)\n    score = clf.score(X_x_test,y_x_test)\n    print('Score of'+name+' is: '+str(score))\n    #iterate over classifiers\n    ","2ba74229":"y_axis_model = xgb.XGBClassifier(use_label_encoder=False)\ny_axis_model.fit(X_y_train, y_y_train)\npreds = final_model.predict(X_y_test)\naccuracy_score(y_y_test, preds)","81d02c94":"y_train = y_x_train\ny_test = y_x_test\n\ndef objective(trial):\n    xgb_params = dict(\n        max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n        )\n    xgb_cl = xgb.XGBClassifier(\n            booster='gbtree',\n            tree_method ='gpu_hist',\n            use_label_encoder=False,\n            **xgb_params)\n    xgb_cl.fit(X_x_train, y_train)\n    preds = xgb_cl.predict(X_x_test)\n    accuracy_score(y_test, preds)\n    return accuracy_score(y_test,preds)\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=20)\nxgb_params = study.best_params","b03e3a4a":"final_model = xgb.XGBClassifier(use_label_encoder=False,\n                                **xgb_params)\nfinal_model.fit(X_x_train, y_train)\npreds = final_model.predict(X_x_test)\naccuracy_score(y_test, preds)\n#performance is increased","ab588829":"set2 = time_features(dataset_path_2nd, id_set=2)\nset2.to_csv('set2_timefeatures.csv')\nset3 = time_features(dataset_path_3rd, id_set=3)\nset3.to_csv('set3_timefeatures.csv')","fc122ce8":"set2 = pd.read_csv(\".\/set2_timefeatures.csv\")\nset3 = pd.read_csv(\".\/set3_timefeatures.csv\")\ntime_2 = set2['Unnamed: 0']\n#Setting index columns to time\nset2 = set2.rename(columns={'Unnamed: 0':'time'}).set_index('time')\nset3 = set3.rename(columns={'Unnamed: 0':'time'}).set_index('time')","6cae3735":"B1_cols = [col for col in set2.columns if \"B1\" in col]\nB2_cols = [col for col in set2.columns if \"B2\" in col]\nB3_cols = [col for col in set2.columns if \"B3\" in col]\nB4_cols = [col for col in set2.columns if \"B4\" in col]\n\nset2_B1 = set2[B1_cols]\nset2_B2 = set2[B2_cols]\nset2_B3 = set2[B3_cols]\nset2_B4 = set2[B4_cols]\n\nset3_B1 = set3[B1_cols]\nset3_B2 = set3[B2_cols]\nset3_B3 = set3[B3_cols]\nset3_B4 = set3[B4_cols]\n\nset2_B1.columns = cols\nset2_B2.columns = cols\nset2_B3.columns = cols\nset2_B4.columns = cols\nset3_B1.columns = cols\nset3_B2.columns = cols\nset3_B3.columns = cols\nset3_B4.columns = cols\n","32d83406":"# HERE number and dataset of bearing can be changed !!!\nbearing = set2_B2\n\n#predicting state of bearing with final_model\npreds = final_model.predict(bearing)\npreds = le.inverse_transform(preds)\n#inserting prediction and time to the dataframe\nbearing.insert(12,'state',preds)\nbearing.insert(13, 'time',bearing.index)\n\nfor tf in time_features_list:\n    col = \"B_{}\".format(tf)\n    print(col)\n    fig=go.Figure((go.Scatter(x=bearing['time'], y=bearing[col],\n                             mode='lines',\n                             line=dict(color='rgba(0,0,220,0.8)'))))\n    fig.add_traces(px.scatter(bearing, x='time', y=col, color='state').data)\n    fig.update_layout(template='plotly_dark')\n    fig.update_xaxes(showgrid=False)\n    fig.show()\n    ","412073be":"Plotting features of all bearings in both x and y axis","799620f2":"In this section several signal-based statistical features has been extracted from the data                                 \n**Definition and formula of the features:**\n* ***Absolute Mean*** $$\\overline{x} = \\frac{1}{N}\\sum_{i=1}^{N}|x_i| $$\n\n* ***Standart Deviation:*** $$\\sigma         = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\overline{x})^2}$$\n* ***Skewness:*** \nAsymmetry of a signal distribution. Faults can impact distribution symmetry and therefore increase the level of skewness.\n$$\\mathrm{Sk} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{(x_i-\\overline{x})^3}{\\sigma^3}$$\n* ***Kurtosis:***\nLength of the tails of a signal distribution, or equivalently, how outlier prone the signal is. Developing faults can increase the number of outliers, and therefore increase the value of the kurtosis metric.\n$$\\mathrm{K} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{(x_i-\\overline{x})^4}{\\sigma^4}$$\n* ***Entropy:***$$ H(X) = -\\sum_{i=1}^{N} P(x_i)\\log{P(x_i)} $$\n                            \n* ***RMS:*** $$x_{rms} =\\sqrt{(\\frac{1}{N})\\sum_{i=1}^{N}(x)^{2}}$$\n\n* ***Peak to Peak:*** $$ x_p = \\max \\mathrm{value} - \\min \\mathrm{value}$$\n\n* ***Crest Factor:*** \nPeak value divided by the RMS. Faults often first manifest themselves in changes in the peakiness of a signal before they manifest in the energy represented by the signal root mean squared. The crest factor can provide an early warning for faults when they first develop. \n$$x_{crest} =\\frac{\\max \\mathrm{value}}{\\mathrm{x_{rms}}}$$\n\n* ***Clearence Factor:*** \nPeak value divided by the squared mean value of the square roots of the absolute amplitudes. For rotating machinery, this feature is maximum for healthy bearings and goes on decreasing for defective ball, defective outer race, and defective inner race respectively. The clearance factor has the highest separation ability for defective inner race faults.\n$$ x_{clear} = \\frac{x_p}{(\\frac{1}{N}\\sum_{i=1}^{N}\\sqrt{|x_i|})^2}  $$\n\n* ***Shape Factor:*** \nRMS divided by the mean of the absolute value. Shape factor is dependent on the signal shape while being independent of the signal dimensions.\n$$\\frac{x_{rms}}{\\overline{x}}$$\n\n* ***Impulse:*** \nCompare the height of a peak to the mean level of the signal.\n$$\\frac{\\max \\mathrm{value}}{\\overline{x}}  $$\n","431212e1":"**This notebook contains followings:**\n* Data Merging\n* Feature Extraction\n* Model Selection\n* Hyperparameter Tuning\n* Results on All Datasets\n* References\n* Conclusion\n* Additional Resources\n","526ff135":"Now we can compare several learning algorithms","3eeeb2d1":"Final XGBoost parameters were obtained by hyperparametertuning using GPU but final model should be trained by CPU to get possibly better result                                             \n","98a6de98":"# Additional Resources\n1. https:\/\/www.youtube.com\/watch?v=YtebGVx-Fxw&list=RDCMUCtYLUTtgS3k1Fg4y5tAhLbw&start_radio=1\n2. Cavalaglio Camargo Molano, J., Strozzi, M., Rubini, R., & Cocconcelli, M. (2019). Analysis of NASA Bearing Dataset of the University of Cincinnati by Means of Hjorth\u2019s Parameters. In International Conference on Structural Engineering Dynamics ICEDyn 2019.","b1a8897b":"Vertically splitting bearing datum to obtain each bearings' data seperately","394a6b4c":"Train-Test Split (70%-30%)","19c719d6":"# Conclusion\nIt seems that trained model is not realiable on other datasets when it comes to classification of the type of the fault with this type of approach. However, the trained model can distinguish whether the data sourced from healty or faulty bearing when considering \"early\" and \"normal\" states as healty. Morever, although it is not mentioned above, feature evaluation and selection process has been conducted on the extracted features with respect to average of the following three metrics, correlation,monotonicity,robustness[3]. It is observed that when the feature which have minimum value of metric defined in the study exculeded from the features dataframe, performance of the model decreased. In further works,contary the this study vibration data can be regarded as time series data and deep learing methods, especially RNNs can be utilized. Also, frequency based feature can be extracted and advanced feature engineering methods can be applied to increase the performance of the model. ","85a9b5b3":"Splitting each bearing data and adding them end to end vertically","ed364330":"# Results\nHere, we using plotly library which is more useful than matplotlib in terms of interactivity                   \nExtracted features are plotted with their \"health state\" label","6557eefb":"Reading Data again","5d051fb4":"# Hyperparameter Tuning\n* Since accuracy score of the last 3 base model,namely XGBoost,CatBoost, LightBoost, are the highest ones, we will continue with one of them in further investigations  \n* Parameters of the selected model will be tuned with optuna library\n* Typical values were given as interval of parameters","a7fe136a":"Adding labels to the dataframe","7f901542":"Evaluating Performance of Base Models","7715546e":"Changing indexing column to time which is also name of the each file","dd59283d":"# References\n1. https:\/\/www.kaggle.com\/yasirabd\/nasa-bearing-feature-extraction\n2. https:\/\/www.mathworks.com\/help\/predmaint\/ug\/signal-features.html\n3. https:\/\/www.mdpi.com\/2076-3417\/10\/16\/5639\/htm#B13-applsci-10-05639\n","8fcd10ab":"For comparing model trained by X-axis Y-axis data, training model with Y-axis data.","863dc24b":"# Feature Extraction #","d1d6ffc1":"1.  Calling feature extraction function defined above to merge extracted features      \n2.  Saving as .csv file","f1677a3b":"# Merging Data","54203ee9":"**From now on, only X axis data will be used**\n                                                                                                                          \nSo data acquired from y axis will be excluded  ","368b9d13":"# Model Selection","68fa0e73":"Loading Dataset 2 and 3 to observe the result of the final model on them\n","9fc37ab6":"**NOTE**                                                                                                       \nThis work is carried out during my Data Science Internship in SensHero Predictive Maintenance Solutions\n* In the first week general knowledge about rotary machines and bearings acquired and the literature about the dataset was reviewed.\n* In the second week several methods are applied and the code was rectified with respect to feedbacks"}}