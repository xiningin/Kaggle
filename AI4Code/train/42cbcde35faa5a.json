{"cell_type":{"107f027d":"code","e6368c76":"code","39fbf60c":"code","d0385a81":"code","56fc53c5":"code","f142b159":"code","23c10ad7":"code","5a53e2b2":"code","e529e52d":"code","1e9edb71":"code","a7b78747":"code","85498c35":"code","b960fe7d":"code","7d3c8546":"code","6fb90688":"markdown","d919943a":"markdown","392f0fb2":"markdown","c9edd4db":"markdown","0d7b4c31":"markdown","77faf4ab":"markdown","4f1d162c":"markdown","718a9f71":"markdown"},"source":{"107f027d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV","e6368c76":"dt = pd.read_csv('..\/input\/indian-liver-patient-records\/indian_liver_patient.csv')\ndt.head()","39fbf60c":"dt.info()","d0385a81":"np.sum(dt.isnull())","56fc53c5":"dt = dt.dropna()\ndt['Gender'] = dt['Gender'].map({'Male': 1, 'Female': 2})","f142b159":"corr = dt.corr(method='pearson')\nsns.heatmap(corr, xticklabels = corr.columns.values, yticklabels=corr.columns.values)","23c10ad7":"X = dt.drop('Dataset', axis=1)\ny = dt['Dataset']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","5a53e2b2":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\ny_pred = dtc.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint(f'The score of Decision Tree Classifier: {score}')","e529e52d":"bc_dtc = BaggingClassifier(base_estimator=dtc, n_estimators=100, n_jobs=-1, random_state=42)\nbc_dtc.fit(X_train, y_train)\ny_pred = bc_dtc.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint(f'The score of Bagging Classifier: {score}')","1e9edb71":"abc = AdaBoostClassifier(base_estimator=dtc, n_estimators=100, random_state=42)\nabc.fit(X_train, y_train)\ny_pred = abc.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint(f'The score of Adaboost: {score}')","a7b78747":"gbc = GradientBoostingClassifier(n_estimators=100, max_depth=3, max_features='log2', random_state=42)\ngbc.fit(X_train, y_train)\ny_pred = gbc.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint(f'The score of Gradient Boosting: {score}')","85498c35":"gbc.get_params()","b960fe7d":"params_gbc = {'n_estimators': [100, 300, 400, 500],\n              'max_depth': [2, 3, 6, 8],\n              'min_samples_leaf': [1, 0.1, 0.2],\n              'max_features': ['log2', 'sqrt']}\n\ngrid_gbc = GridSearchCV(estimator=gbc,\n                       param_grid=params_gbc,\n                       cv=3,\n                       scoring='neg_mean_squared_error',\n                       verbose=1,\n                       n_jobs=-1)\n\ngrid_gbc.fit(X_train, y_train)\nbest_params = grid_gbc.best_params_\nprint(best_params)","7d3c8546":"gbc = GradientBoostingClassifier(n_estimators=100, max_depth=2, max_features='log2', random_state=42)\ngbc.fit(X_train, y_train)\ny_pred = gbc.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint(f'The score of gradient boosting using best hyperparameter: {score}')","6fb90688":"## Basic EDA","d919943a":"GridSearchCV gave us best hyperparameter values. {'max_depth': 2, 'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 100}. Using those values, we got higher score. If you have any suggestions about my code, please leave comments :)","392f0fb2":"Bagging Classifier results in a little higher score than decision tree itself. Bagging used bootstrapping method to make more datasets. Bootstrap means sampling with replacement allowing duplicate. By bagging, the model can be exposed to more datasets. And it reduces variance and avoids overfitting.  \n## 3. Adaboost (Adaptive Boosting)","c9edd4db":"## Import modules","0d7b4c31":"GradientBoostingClassifier has many hyperparameter, and we don't know which value to use (default value not always works). Instead of trying every case, we can use GridSearchCV to find the best hyperparameter.","77faf4ab":"This algorithm can easily cause overfitting (high variance). When there are many outlier data points, the result can be wrong. So, we can use bagging or boosting method to lower the variance.  \n## 2. Bagging","4f1d162c":"Adaboost makes strong learner by combining several weak learners. A weak learner has error probability less than 0.5, just slightly better than chance. By assigning different weights to each learner, we can make better classifier.  \n## 4. Gradient Boost","718a9f71":"## 1. Decision Tree\nDecision Tree Algorithm is used for both classification and regression. There is no default maximum depth of DecisionTreeClassifier. "}}