{"cell_type":{"4fca1755":"code","48421f10":"code","3e59aa28":"code","dd8f2cf3":"code","9da0174b":"code","38169902":"code","e7e296dd":"code","deccfd29":"code","3de3a8f6":"markdown"},"source":{"4fca1755":"import numpy as np\nimport pandas as pd\nimport re\n\n#SKlearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import BayesianRidge\n\n#NLTK\nimport nltk\nfrom nltk.tokenize import word_tokenize, wordpunct_tokenize\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport keras.backend as K\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Model\nfrom keras.models import load_model\n\n#TF Transformers\nfrom transformers import AutoTokenizer,TFAutoModel","48421f10":"def preProcesstext(tweet_dataframe):\n\n    stop_words=pd.read_csv('..\/input\/stop-words\/stopwords.csv',names=['stopword'])\n    stop_words=stop_words.iloc[:,0].values.tolist()\n    \n    #Seperating out text from the data\n    tweet=tweet_dataframe['excerpt'].values\n\n    #Using Regex functions to remove non-essential characters\n    t1=[]\n    for i in range(len(tweet)):\n        t1.append(re.sub('[^a-zA-Z]+',' ',str(tweet[i])))\n    \n    #Using Regex functions to remove stop words and words shorter than 3 characters\n    for i in range(len(t1)):\n        t1[i] = ' '.join(word for word in t1[i].split() if word not in stop_words)\n        t1[i]=t1[i].lower()\n        t1[i]=' '.join(word for word in t1[i].split() if len(word)>3)\n        \n    corpus_stemmed = []\n    \n    #Lementing words\n    from nltk.stem import WordNetLemmatizer\n    wordnet_lemmatizer = WordNetLemmatizer()\n    for d in t1:\n        words = pd.Series(wordpunct_tokenize(d),dtype='object')\n        stemmed_words = words.apply(wordnet_lemmatizer.lemmatize)\n        corpus_stemmed.append(' '.join(list(stemmed_words)))\n    \n    #return corpus_stemmed\n    return t1","3e59aa28":"BASE_MODEL = '..\/input\/huggingface-roberta-variants\/distilroberta-base\/distilroberta-base'\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\ntransformer_model = TFAutoModel.from_pretrained(BASE_MODEL,num_labels=1)","dd8f2cf3":"#Tokenize input data to generate dictionary of input ids and attention masks\ndef get_data(train_text):\n    x_train=preProcesstext(train_text)\n    tokenized = tokenizer(x_train, padding=True, return_tensors=\"np\")\n    return {feat: tokenized[feat] for feat in tokenizer.model_input_names}\n\n#Get embeddings\ndef get_embedding(X_train):\n    embedding=[]\n    for i in range(0,len(X_train['input_ids']),100):\n        X = transformer_model(input_ids=X_train['input_ids'][i:i+100], attention_mask=X_train['attention_mask'][i:i+100])[0][:,0,:].numpy()\n        embedding.extend(X)\n    embedding=np.array(embedding)\n    return embedding","9da0174b":"train_text=pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_text=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nX_train = get_data(train_text)\nX_test = get_data(test_text)\n\n#Scaling applied to target\nsc = StandardScaler()\ny=train_text['target'].values\ny=sc.fit_transform(y.reshape(-1,1))","38169902":"#Embeddings for train and test dataset\nembedding_train=get_embedding(X_train)    \nembedding_test=get_embedding(X_test)","e7e296dd":"#Multiple regressors were used out of which Bayesian Ridge performed the best\nmodel = BayesianRidge()\nmodel.fit(embedding_train,y)\n\n#Predicting target for test dataset\ny_pred = model.predict(embedding_test)\ny_pred=sc.inverse_transform(y_pred.reshape(-1,1))","deccfd29":"df=pd.DataFrame(test_text['id'],columns=['id'])\ndf['target']=y_pred\ndf.to_csv('.\/submission.csv', index=False)","3de3a8f6":"**Next Steps:**\n\n* Applying grid search CV to get optimal hyperparameters for Bayesian Ridge\n* Better preprocessing of text\n\nOpen to more suggestions!"}}