{"cell_type":{"311f81d2":"code","1a0a610a":"code","957abef2":"code","69a2e597":"code","8a0e90b5":"code","ef0d5369":"code","bca6bd5f":"code","a05e7510":"code","8e115afb":"code","e75cad66":"code","8c3b0e26":"code","e1f9d12e":"code","be639c3b":"code","1cb352bc":"code","c3b5b7ff":"code","8801b355":"code","d8954986":"code","577260cf":"code","feaaace8":"code","cf9fb56d":"code","9de55fb5":"code","714c1e75":"code","ad286d2c":"code","4d205d51":"code","375640c1":"code","d2460c73":"code","b3039ed4":"code","e18d112f":"code","5febf3fb":"code","4785e5ca":"code","9aad8b69":"code","e3e6b5ed":"code","399f1e0c":"code","85966205":"code","0abdae98":"code","d8c3db5a":"code","73d242c2":"markdown","c64fdad5":"markdown","ba8ee24b":"markdown","e5b16ee4":"markdown","fa4f50c4":"markdown","09fc91d7":"markdown","5696c7fa":"markdown","82710228":"markdown","8aea79a6":"markdown","8e393757":"markdown","67699b07":"markdown","6c2bf9ed":"markdown","17366352":"markdown","275f685c":"markdown","4c4a3154":"markdown","611f0af3":"markdown","ce39c03f":"markdown","fe49d238":"markdown","2959aa2c":"markdown","527c8df3":"markdown","d50f0785":"markdown","528c60c5":"markdown","2bc80303":"markdown"},"source":{"311f81d2":"PATH_TO_DATA = '..\/input\/sarcasm\/train-balanced-sarcasm.csv'","1a0a610a":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","957abef2":"train_df = pd.read_csv(PATH_TO_DATA)","69a2e597":"train_df.head()","8a0e90b5":"train_df.info()","ef0d5369":"train_df.dropna(subset=['comment'], inplace=True)","bca6bd5f":"train_df['label'].value_counts()","a05e7510":"train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17)","8e115afb":"train_df.loc[train_df['label'] == 1, 'comment'].str.len().apply(np.log1p).hist(label='sarcastic', alpha=.5)\ntrain_df.loc[train_df['label'] == 0, 'comment'].str.len().apply(np.log1p).hist(label='normal', alpha=.5)\nplt.legend();","e75cad66":"from wordcloud import WordCloud, STOPWORDS","8c3b0e26":"wordcloud = WordCloud(background_color='black', stopwords = STOPWORDS,\n                max_words = 200, max_font_size = 100, \n                random_state = 17, width=800, height=400)","e1f9d12e":"plt.figure(figsize=(16, 12))\nwordcloud.generate(str(train_df.loc[train_df['label'] == 1, 'comment']))\nplt.imshow(wordcloud);","be639c3b":"plt.figure(figsize=(16, 12))\nwordcloud.generate(str(train_df.loc[train_df['label'] == 0, 'comment']))\nplt.imshow(wordcloud);","1cb352bc":"sub_df = train_df.groupby('subreddit')['label'].agg([np.size, np.mean, np.sum])\nsub_df.sort_values(by='sum', ascending=False).head(10)","c3b5b7ff":"sub_df[sub_df['size'] > 1000].sort_values(by='mean', ascending=False).head(10)","8801b355":"sub_df = train_df.groupby('author')['label'].agg([np.size, np.mean, np.sum])\nsub_df[sub_df['size'] > 300].sort_values(by='mean', ascending=False).head(10)","d8954986":"sub_df = train_df[train_df['score'] >= 0].groupby('score')['label'].agg([np.size, np.mean, np.sum])\nsub_df[sub_df['size'] > 300].sort_values(by='mean', ascending=False).head(10)","577260cf":"sub_df = train_df[train_df['score'] < 0].groupby('score')['label'].agg([np.size, np.mean, np.sum])\nsub_df[sub_df['size'] > 300].sort_values(by='mean', ascending=False).head(10)","feaaace8":"# build bigrams, put a limit on maximal number of features\n# and minimal word frequency\ntf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)\n# multinomial logistic regression a.k.a softmax classifier\nlogit = LogisticRegression(C=1, n_jobs=4, solver='lbfgs', \n                           random_state=17, verbose=1)\n# sklearn's pipeline\ntfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n                                 ('logit', logit)])","cf9fb56d":"%%time\ntfidf_logit_pipeline.fit(train_texts, y_train)","9de55fb5":"%%time\nvalid_pred = tfidf_logit_pipeline.predict(valid_texts)","714c1e75":"accuracy_score(y_valid, valid_pred)","ad286d2c":"def plot_confusion_matrix(actual, predicted, classes,\n                          normalize=False,\n                          title='Confusion matrix', figsize=(7,7),\n                          cmap=plt.cm.Blues, path_to_save_fig=None):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    from sklearn.metrics import confusion_matrix\n    cm = confusion_matrix(actual, predicted).T\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=figsize)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Predicted label')\n    plt.xlabel('True label')\n    \n    if path_to_save_fig:\n        plt.savefig(path_to_save_fig, dpi=300, bbox_inches='tight')","4d205d51":"plot_confusion_matrix(y_valid, valid_pred, \n                      tfidf_logit_pipeline.named_steps['logit'].classes_, figsize=(8, 8))","375640c1":"import eli5\neli5.show_weights(estimator=tfidf_logit_pipeline.named_steps['logit'],\n                  vec=tfidf_logit_pipeline.named_steps['tf_idf'])","d2460c73":"subreddits = train_df['subreddit']\ntrain_subreddits, valid_subreddits = train_test_split(subreddits, random_state=17)","b3039ed4":"tf_idf_texts = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)\ntf_idf_subreddits = TfidfVectorizer(ngram_range=(1, 1))","e18d112f":"%%time\nX_train_texts = tf_idf_texts.fit_transform(train_texts)\nX_valid_texts = tf_idf_texts.transform(valid_texts)","5febf3fb":"X_train_texts.shape, X_valid_texts.shape","4785e5ca":"%%time\nX_train_subreddits = tf_idf_subreddits.fit_transform(train_subreddits)\nX_valid_subreddits = tf_idf_subreddits.transform(valid_subreddits)","9aad8b69":"X_train_subreddits.shape, X_valid_subreddits.shape","e3e6b5ed":"from scipy.sparse import hstack\nX_train = hstack([X_train_texts, X_train_subreddits])\nX_valid = hstack([X_valid_texts, X_valid_subreddits])","399f1e0c":"X_train.shape, X_valid.shape","85966205":"logit.fit(X_train, y_train)","0abdae98":"%%time\nvalid_pred = logit.predict(X_valid)","d8c3db5a":"accuracy_score(y_valid, valid_pred)","73d242c2":"Then, stack all features together.","c64fdad5":"As we can see, accuracy slightly increased.","ba8ee24b":"### Part 4. Improving the model","e5b16ee4":"## Links:\n  - Machine learning library [Scikit-learn](https:\/\/scikit-learn.org\/stable\/index.html) (a.k.a. sklearn)\n  - Kernels on [logistic regression](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-2-classification) and its applications to [text classification](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n  - [Kaggle Kernel](https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n  - [ELI5](https:\/\/github.com\/TeamHG-Memex\/eli5) to explain model predictions","fa4f50c4":"## Tasks:\n1. Analyze the dataset, make some plots. This [Kernel](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc) might serve as an example\n2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n3. Plot the words\/bigrams which a most predictive of sarcasm (you can use [eli5](https:\/\/github.com\/TeamHG-Memex\/eli5) for that)\n4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.","09fc91d7":"We'll have separate Tf-Idf vectorizers for comments and for subreddits. It's possible to stick to a pipeline as well, but in that case it becomes a bit less straightforward. [Example](https:\/\/stackoverflow.com\/questions\/36731813\/computing-separate-tfidf-scores-for-two-different-columns-using-sklearn)","5696c7fa":"We split data into training and validation parts.","82710228":"Distribution of lengths for sarcastic and normal comments is almost the same.","8aea79a6":"Indeed, we can recognize some phrases indicative of sarcasm. Like \"yes sure\". ","8e393757":"We notice that the dataset is indeed balanced","67699b07":"Train the same logistic regression.","6c2bf9ed":"### Part 1. Exploratory data analysis","17366352":"## <center> Assignment 4. Sarcasm detection with logistic regression. Solution\n    \nWe'll be using the dataset from the [paper](https:\/\/arxiv.org\/abs\/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https:\/\/www.kaggle.com\/danofer\/sarcasm).","275f685c":"Some comments are missing, so we drop the corresponding rows.","4c4a3154":"Word cloud are nice, but not very useful","611f0af3":"The same for authors doesn't yield much insight. Except for the fact that somebody's comments were sampled - we can see the same amounts of sarcastic and non-sarcastic comments.","ce39c03f":"So sarcasm detection is easy.\n<img src=\"https:\/\/habrastorage.org\/webt\/1f\/0d\/ta\/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" \/>","fe49d238":"Confusion matrix is quite balanced.","2959aa2c":"### Part 3. Explaining the model","527c8df3":"Do transformations separately for comments and subreddits. ","d50f0785":"Let's analyze whether some subreddits are more \"sarcastic\" on average than others","528c60c5":"<center>\n<img src=\"https:\/\/habrastorage.org\/webt\/ia\/m9\/zk\/iam9zkyzqebnf_okxipihkgjwnw.jpeg\">\n    \n## [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \nAuthor: [Yury Kashnitsky](https:\/\/yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose.","2bc80303":"### Part 2. Training the model"}}