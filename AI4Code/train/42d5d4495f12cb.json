{"cell_type":{"22a593c3":"code","02356267":"code","7bce99f6":"code","4ab470c7":"code","162a3a60":"code","f4720d53":"code","e88ce6f9":"code","6b27af42":"code","3e257e75":"code","ffca1fe0":"code","47d692d5":"code","6a38ec0b":"code","3cec645c":"code","2ffbdeb6":"code","499cd8b8":"code","6b9a4674":"code","1c2b77e9":"code","35849759":"code","d4dbf227":"code","8ea2d35f":"code","9129dadb":"code","b5c007c2":"code","e4621a47":"code","59b9621c":"code","69ca7139":"code","9ae43df2":"code","91786758":"code","f3b0dbe6":"code","cc49572d":"code","74bea23b":"code","49e61029":"code","da08ed98":"code","e7312135":"code","e6cd4d17":"code","a2e45263":"code","92bf95d5":"code","6cbe3e29":"markdown","5f1ff042":"markdown","2fcb0fa2":"markdown","35d13734":"markdown","01a5362d":"markdown","b092a718":"markdown","45cce846":"markdown","75ab42e5":"markdown","d76c0d00":"markdown","8457922d":"markdown","6d596619":"markdown"},"source":{"22a593c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","02356267":"# Importando os dados\ndf = pd.read_csv('\/kaggle\/input\/bank-marketing\/bank-additional-full.csv', sep=';')\n\ndf.shape","7bce99f6":"# Verificando quantidades e tipos\ndf.info()","4ab470c7":"# Visualizando os dados\ndf.head().T","162a3a60":"# Analisando a vari\u00e1vel de target\ndf['y'].value_counts(normalize=True)","f4720d53":"# N\u00e3o temos valores nulos mas temos dados como 'unknown'\n# Para facilitar vamos considerar esses dados como um tipo espec\u00edfico\ndf['job'].value_counts()","e88ce6f9":"# Verificando tamb\u00e9m a coluna 'loan'\ndf['loan'].value_counts()","6b27af42":"# O problema da vari\u00e1vel 'duration' - data leak - vazamento de dados\ndf.drop(columns=['duration'], axis=1, inplace=True)","3e257e75":"# Convertendo as colunas object para colunas categ\u00f3ricas\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        df[col] = df[col].astype('category').cat.codes\n        \ndf.info()","ffca1fe0":"# Separando o dataframe\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df, test_size=0.20, random_state=42)\n\ntrain.shape, test.shape","47d692d5":"# Lista das colunas a serem usadas para treino\nfeats = [c for c in df.columns if c not in ['y']]","6a38ec0b":"# Importando o RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Instanciando um objeto RandomForest\nrf = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n\n# Treinando o modelo\nrf.fit(train[feats], train['y'])","3cec645c":"# Fazer previs\u00f5es usando o modelo treinado\npreds = rf.predict(test[feats])\n\n# Importando a m\u00e9trica\nfrom sklearn.metrics import accuracy_score\n\n# Avaliando o modelo com rela\u00e7\u00e3o aos dados de teste\naccuracy_score(test['y'], preds)","2ffbdeb6":"# Verificando a distribui\u00e7\u00e3o da vari\u00e1vel target\ndf['y'].value_counts()","499cd8b8":"# Verificando a distribui\u00e7\u00e3o da vari\u00e1vel target (%)\ndf['y'].value_counts(normalize=True)","6b9a4674":"# Verificando a distribui\u00e7\u00e3o da vari\u00e1vel target (%) - Base de treino\ntrain['y'].value_counts(normalize=True)","1c2b77e9":"# Verificando a distribui\u00e7\u00e3o da vari\u00e1vel target (%) - Base de teste\ntest['y'].value_counts(normalize=True)","35849759":"# Importando uma biblioteca espec\u00edfica para plotar gr\u00e1ficos relacionada\n# com o scikitlearn\nimport scikitplot as skplt\n\n# Matriz de Confus\u00e3o - Dados de teste\nskplt.metrics.plot_confusion_matrix(test['y'], preds)","d4dbf227":"# Importando a biblioteca\nfrom sklearn.utils import resample\n\n# Separando os dados de acordo com a classifica\u00e7\u00e3o\ndf_no = df[df['y'] == 0]\ndf_yes = df[df['y'] == 1]\n\ndf_no.shape, df_yes.shape","8ea2d35f":"# Over-Sampling\ndf_yes_over = resample(df_yes, # vamos aumentar a classe menor\n                       replace=True, # sample com replacement\n                       n_samples=len(df_no), # igualando a maior classe\n                       random_state=42)\n\n# juntando os dados\ndf_over= pd.concat([df_no, df_yes_over])\n\n# check new class counts\ndf_over['y'].value_counts()","9129dadb":"# Executando o modelo com df_over\n\n# Dividindo em treino e teste\ntrain, test = train_test_split(df_over, test_size=0.2, random_state=42)\n\n# Treinar o modelo\nrf.fit(train[feats], train['y'])\n\n# Previs\u00f5es na base de teste\npreds_test = rf.predict(test[feats])\n\n# Medir a acur\u00e1cia\naccuracy_score(test['y'], preds_test)","b5c007c2":"# Plotando a matriz de confusao para os dados de teste\nskplt.metrics.plot_confusion_matrix(test['y'], preds_test)","e4621a47":"# Under-Sampling\ndf_no_under = resample(df_no, # vamos diminuir a classe maior\n                       replace=False, # sample sem replacement\n                       n_samples=len(df_yes), # igualando a menor classe\n                       random_state=42)\n\n# juntando os dados\ndf_under= pd.concat([df_no_under, df_yes])\n\n# check new class counts\ndf_under['y'].value_counts()","59b9621c":"# Executando o modelo com df_under\n\n# Dividindo em treino e teste\ntrain, test = train_test_split(df_under, test_size=0.2, random_state=42)\n\n# Treinar o modelo\nrf.fit(train[feats], train['y'])\n\n# Previs\u00f5es na base de teste\npreds_test = rf.predict(test[feats])\n\n# Medir a acur\u00e1cia\naccuracy_score(test['y'], preds_test)","69ca7139":"# Plotando a matriz de confusao para os dados de teste\nskplt.metrics.plot_confusion_matrix(test['y'], preds_test)","9ae43df2":"# Importando a biblioteca\nimport imblearn","91786758":"# Separando os dados de entrada e o target\nX, y = df[feats], df[['y']]","f3b0dbe6":"# Importando a biblioteca\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Fazendo o over-sampling\nros = RandomOverSampler(random_state=42)\nX_ros,y_ros= ros.fit_resample(X,y)\n\n# Verificando os dados\ny_ros['y'].value_counts()","cc49572d":"# Executando o modelo com imblearn over-sampling\n\n# Juntando os dados\ndf_over = pd.concat([X_ros, y_ros], axis=1)\n\n# Dividindo em treino e teste\ntrain, test = train_test_split(df_over, test_size=0.2, random_state=42)\n\n# Treinar o modelo\nrf.fit(train[feats], train['y'])\n\n# Previs\u00f5es na base de teste\npreds_test = rf.predict(test[feats])\n\n# Medir a acur\u00e1cia\naccuracy_score(test['y'], preds_test)","74bea23b":"# Plotando a matriz de confusao para os dados de teste\nskplt.metrics.plot_confusion_matrix(test['y'], preds_test)","49e61029":"# Importando a biblioteca\nfrom imblearn.under_sampling import TomekLinks\n\n# Fazendo o under-sampling\ntl = TomekLinks()\nX_tl, y_tl = tl.fit_resample(X,y)\n\n# Verificando os dados\ny_tl['y'].value_counts()","da08ed98":"# Executando o modelo com Tomek-links\n\n# Juntando os dados\ndf_under = pd.concat([X_tl, y_tl], axis=1)\n\n# Dividindo em treino e teste\ntrain, test = train_test_split(df_under, test_size=0.2, random_state=42)\n\n# Treinar o modelo\nrf.fit(train[feats], train['y'])\n\n# Previs\u00f5es na base de teste\npreds_test = rf.predict(test[feats])\n\n# Medir a acur\u00e1cia\naccuracy_score(test['y'], preds_test)","e7312135":"# Plotando a matriz de confusao para os dados de teste\nskplt.metrics.plot_confusion_matrix(test['y'], preds_test)","e6cd4d17":"# Importando a biblioteca\nfrom imblearn.over_sampling import SMOTE\n\n# Fazendo o under-sampling\nsm = SMOTE()\nX_sm, y_sm = sm.fit_resample(X,y)\n\n# Verificando os dados\ny_sm['y'].value_counts()","a2e45263":"# Executando o modelo com SMOTE\n\n# Juntando os dados\ndf_over = pd.concat([X_sm, y_sm], axis=1)\n\n# Dividindo em treino e teste\ntrain, test = train_test_split(df_over, test_size=0.2, random_state=42)\n\n# Treinar o modelo\nrf.fit(train[feats], train['y'])\n\n# Previs\u00f5es na base de teste\npreds_test = rf.predict(test[feats])\n\n# Medir a acur\u00e1cia\naccuracy_score(test['y'], preds_test)","92bf95d5":"# Plotando a matriz de confusao para os dados de teste\nskplt.metrics.plot_confusion_matrix(test['y'], preds_test)","6cbe3e29":"### Imblearn Random Over-Sampling","5f1ff042":"## Confusion Matrix","2fcb0fa2":"### Random Under-Sampling","35d13734":"# IESB - Miner II - Aula 08 - Classes desbalanceadas","01a5362d":"# Parte 2\n## Tratando classes desbalanceadas","b092a718":"# Parte 1\n## Tratamento de dados e execu\u00e7\u00e3o do modelo","45cce846":"### Imblearn SMOTE (over-sampling)","75ab42e5":"### Imblearn Tomek-links (under-sampling)","d76c0d00":"### Random Over-Sampling ","8457922d":"## Usando a bilioteca imbalanced-learn\nEssa biblioteca implementa diversos modelos diferentes para tratar classes desabalanceadas","6d596619":"## Fazendo Random Under-Sampling e Random Over-Sampling\nVamos usar o pr\u00f3pio sklearn para realizar under e over sampling de forma aleat\u00f3ria"}}