{"cell_type":{"c9e13a9d":"code","bea3c80f":"code","09e1225f":"code","9c333642":"code","8dbfdbb9":"code","4f6dee8b":"code","c3cb3f4b":"code","00f8ac69":"code","91246a02":"code","9e7ed5af":"code","a1786c2d":"code","39725af4":"code","1cebe55e":"code","83238596":"code","6d94716d":"code","24f4df78":"code","67350caa":"code","ff7f597c":"code","d64ee095":"code","5182a0af":"code","4d7f3eca":"code","547959e0":"code","7f24c3a4":"code","516e25a6":"code","0abd2f17":"code","a24640de":"code","10adbbeb":"code","90590c7e":"code","663f6dde":"code","8e45cb6d":"markdown","9428e9e7":"markdown","94e52624":"markdown","418f9089":"markdown","4fbc40fa":"markdown","cde38715":"markdown","e0f7c626":"markdown","0a48f444":"markdown","e2a87c2d":"markdown","ae4b7f0b":"markdown","419a4562":"markdown","d1e200c8":"markdown","d7a58b8c":"markdown","b940573b":"markdown","5ba85d2c":"markdown","e90cabcd":"markdown","d7ea8560":"markdown","d0eaad1a":"markdown","8e554a42":"markdown","c42a2d0c":"markdown","3498f9eb":"markdown","c871708f":"markdown","aca86012":"markdown","2a2fe284":"markdown","c2c8d8d7":"markdown","521e5b22":"markdown","6fc31774":"markdown","976decd8":"markdown","71fb214b":"markdown","94391000":"markdown","069a1c6a":"markdown","daeadaa6":"markdown","0bfeca5a":"markdown","ee0c38ec":"markdown"},"source":{"c9e13a9d":"import numpy as np \nimport pandas as pd\nimport statistics as sts\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\nfrom sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder,StandardScaler\nfrom sklearn.metrics import plot_confusion_matrix, classification_report\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.pipeline import Pipeline\n\n%matplotlib inline","bea3c80f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","09e1225f":"titanic = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntitanic.head(3)","9c333642":"missing_data = titanic.isnull().sum()\nmissing_data = missing_data[missing_data > 0]\nmissing_data.plot.bar()","8dbfdbb9":"titanic.Survived.mean()","4f6dee8b":"sns.boxplot(x='Survived', y='Age', data=titanic)","c3cb3f4b":"titanic['Fare'].describe()","00f8ac69":"titanic.groupby(titanic['Fare'] < 7.9104).Survived.mean()","91246a02":"titanic.groupby((titanic['Fare'] >= 7.9104) & (titanic['Fare'] <= 31)).Survived.mean()","9e7ed5af":"titanic.groupby(titanic['Fare'] > 31).Survived.mean()","a1786c2d":"cabin_qttot = len(titanic['Cabin'])\ncabin_qtnan = titanic['Cabin'].count()\nprint('{} not null values in {}'.format(cabin_qtnan, cabin_qttot))","39725af4":"titanic.groupby(titanic['Cabin'].notnull()).mean().Survived","1cebe55e":"titanic[['Parch','Survived']].groupby('Parch').mean()","83238596":"titanic[['SibSp','Survived']].groupby('SibSp').mean()","6d94716d":"titanic[['Embarked','Survived']].groupby('Embarked').mean().Survived","24f4df78":"def addFeaturesTitanic(df):\n  features = [\"Pclass\", \"Parch\", \"SibSp\", \"Age\", \"Fare\", \"Sex\", \"Cabin\"]\n  data = df[features].copy()\n\n  # cria novas features\n  for i in data.index:\n    if data.loc[i, 'Fare'] < 7.9104:\n      data.loc[i, 'LowFare'] = 1\n    else:\n      data.loc[i, 'LowFare'] = 0\n\n    if data.loc[i, 'Fare'] > 31:\n      data.loc[i, 'HighFare'] = 1\n    else:\n      data.loc[i, 'HighFare'] = 0\n\n    if data.loc[i, 'Sex'] == 'male':\n      data.loc[i, 'SexBin'] = 1\n    else:\n      data.loc[i, 'SexBin'] = 0\n\n    if data.loc[i, 'Cabin'] == None:\n      data.loc[i, 'HasCabin'] = 0\n    else:\n      data.loc[i, 'HasCabin'] = 1\n\n  data = data.drop(columns = ['Fare', 'Sex', 'Cabin'])\n\n  return data","67350caa":"feature_transformer = FunctionTransformer(addFeaturesTitanic, validate=False)\nfillnan_median_transformer = SimpleImputer(strategy='median')\nfillnan_mode_transformer = SimpleImputer(strategy='most_frequent')\nonehotencoder_transformer = OneHotEncoder()","ff7f597c":"pipeline_numeric = Pipeline(steps=[\n    ('addfeatures', feature_transformer),\n    ('imputer', fillnan_median_transformer),    \n    ('scaler', StandardScaler())\n])\n\npipeline_categorical = Pipeline(steps=[ \n    ('imputer', fillnan_mode_transformer),\n    ('onehot', onehotencoder_transformer)\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n      ('num', pipeline_numeric, [\"Pclass\", \"Parch\", \"SibSp\", \"Age\", \"Fare\", \"Sex\", \"Cabin\"]),\n      ('cat', pipeline_categorical, [\"Embarked\"])\n    ]\n)","d64ee095":"X = titanic.drop('Survived', axis=1)\ny = titanic['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","5182a0af":"prep = Pipeline(steps=[('preprocessor', preprocessor)])\nX_train_prep = prep.fit_transform(X_train)\nX_test_prep = prep.transform(X_test)","4d7f3eca":"metric = 'f1'","547959e0":"model_rl = LogisticRegression()\nmodel_rl.fit(X_train_prep, y_train)\nrl_predictions = model_rl.predict(X_test_prep)\nprint(classification_report(y_test, rl_predictions))","7f24c3a4":"parametros = {'n_neighbors': range(1,31)}\nmodel_kn = KNeighborsClassifier(n_neighbors=3)\ngrid = GridSearchCV(model_kn, parametros, cv=5, scoring=metric)\ngrid.fit(X_train_prep, y_train)\n\nprint('Best K: {}'.format(grid.best_params_['n_neighbors']))\n\nmodel_kn = grid.best_estimator_\nmodel_kn.fit(X_train_prep, y_train)\nkn_predictions = model_kn.predict(X_test_prep)\nprint(classification_report(y_test, kn_predictions))","516e25a6":"parametros = {'max_depth': range(1,20), 'n_estimators': [100, 200, 300, 400, 500]}\nmodel_rf = RandomForestClassifier()\ngrid_rf = GridSearchCV(model_rf, parametros, cv=5, scoring=metric)\ngrid_rf.fit(X_train_prep, y_train)\n\nprint('Best max_depth: {}'.format(grid_rf.best_params_['max_depth']))\nprint('Best nEstimators: {}'.format(grid_rf.best_params_['n_estimators']))\n\nmodel_rf = grid_rf.best_estimator_\nmodel_rf.fit(X_train_prep, y_train)\nrf_predictions = model_rf.predict(X_test_prep)\nprint(classification_report(y_test, rf_predictions))","0abd2f17":"parametros = {'C': range(1,20)}\nmodel_sv = SVC(C=1)\ngrid_sv = GridSearchCV(model_sv, parametros, cv=5, scoring=metric)\ngrid_sv.fit(X_train_prep, y_train)\nprint('Best C:'.format(grid_sv.best_params_['C']))\n\nmodel_sv = grid_sv.best_estimator_\nmodel_sv.fit(X_train_prep, y_train)\nsv_predictions = model_sv.predict(X_test_prep)\nprint(classification_report(y_test, sv_predictions))","a24640de":"cv_rl = cross_validate(model_rl, X_test_prep, y_test, return_train_score=False, \n               scoring=['accuracy',\n                        'average_precision',\n                        'f1',\n                        'precision',\n                        'recall',\n                        'roc_auc'])\ncv_kn = cross_validate(model_kn, X_test_prep, y_test, return_train_score=False, \n               scoring=['accuracy',\n                        'average_precision',\n                        'f1',\n                        'precision',\n                        'recall',\n                        'roc_auc'])\ncv_rf = cross_validate(model_rf, X_test_prep, y_test, return_train_score=False, \n               scoring=['accuracy',\n                        'average_precision',\n                        'f1',\n                        'precision',\n                        'recall',\n                        'roc_auc'])\ncv_sv = cross_validate(model_sv, X_test_prep, y_test, return_train_score=False, \n               scoring=['accuracy',\n                        'average_precision',\n                        'f1',\n                        'precision',\n                        'recall',\n                        'roc_auc'])","10adbbeb":"avl_rl = pd.DataFrame(cv_rl)\navl_rl['model'] = np.repeat(\"Logistic Regression\", 5)\navl_kn = pd.DataFrame(cv_kn)\navl_kn['model'] = np.repeat(\"KNN\", 5)\navl_rf = pd.DataFrame(cv_rf)\navl_rf['model'] = np.repeat(\"Random Forest\", 5)\navl_sv = pd.DataFrame(cv_sv)\navl_sv['model'] = np.repeat(\"SVM\", 5)\npd.concat([avl_rl, avl_kn, avl_rf, avl_sv]).groupby('model').mean()","90590c7e":"titanic_result = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nprep = Pipeline(steps=[('preprocessor', preprocessor)])\nX_result = prep.transform(titanic_result)","663f6dde":"predictions = model_rf.predict(X_result)\npassengers_id = titanic_result['PassengerId']\noutput = pd.DataFrame({'PassengerId': passengers_id, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Arquivo de Submiss\u00e3o Gerado com Sucesso!\")","8e45cb6d":"**Third Quartile**\n\n~58% Survived in the Fourth Quartile\n\nThe survivence grows up among passengers in this quartile","9428e9e7":"**Logistic Regression**","94e52624":"**First Quartile**\n\n~20% Survived","418f9089":"Function addFeaturesTitanic\n\n* Add features LowFare, HighFare, SexBin and HasCabin\n* Remove features Fare, Sex and Cabin","4fbc40fa":"## 1. DATA EXPLORATION ##","cde38715":"Extract predictions in submission file","e0f7c626":"~67% Survived when Cabin is not null\n\n~30% Survived when Cabin is null","0a48f444":"**KNN**","e2a87c2d":"Next, the survived tax by quartile of Fare ","ae4b7f0b":"Analysis the survive percentual by Parch","419a4562":"**Parch**\n\nNumber of parents\/children aboard.","d1e200c8":"**Fare**\n\nAnalysing the value payed by ticket was identified among the passengers that payed high values have more survivers.\n\nDistribution values payed by Ticket:\n","d7a58b8c":"**Distribution of Age between Survived and not Survived**\n\nThe people that survived has a median age under people that not survived. \nThis show the influence of Age in the class. ","b940573b":"## 3. DATA MODELING ##\n\n**Training Models**\n\n+ Logistic Regression\n+ Random Forest\n+ K-Nearest Neighbors\n+ Suport Vector Machine","5ba85d2c":"## 2. DATA PREPARATION ##","e90cabcd":"**Analysing Class**\n\nSurvived **38%** in the Train Dataset","d7ea8560":"**SibSp**\n\nNumber of siblings\/spouses aboard.","d0eaad1a":"**SVM**\n","8e554a42":"Analysis the survive percentual by Port of Embarkation","c42a2d0c":"## Prepare Dataset Test Kaggle\n\n* Import dataset from Kaggle to create submission file\n* Execute Pipeline to preprocessing dataset","3498f9eb":"## 4. MODEL VALIDATION ##","c871708f":"**Cabin**\n\nCabin number.\n\nAlthough the feature Cabin has many missing values, when Cabin is not null the survivence is high","aca86012":"Divide dataset using train_test_split, using 30% to test the models","2a2fe284":"Create Pipelines to organize the preprocessing of datasets","c2c8d8d7":"**Import Train Dataset**","521e5b22":"# Titanic - Machine Learning from Disaster\n\n## Survive Predictions\n\n","6fc31774":"Execute pipeline to preprocessing train and test dataset","976decd8":"Analysis the survive percentual by Number of Siblings\/Spouses","71fb214b":"Create transformers:\n\n* addFeaturesTitanic to Feature Engineering\n* SimpleImputer to Numeric Features using median\n* SimpleImputer to Categorical Features using mode\n* OneHotEncoder to categorial feature Embarked","94391000":"**Embarked**\n\nPort of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)","069a1c6a":"**Random Forest**\n","daeadaa6":"**View Missing Data**\n\nThe dataset has missing data in the features Age, Cabin and Embarked.","0bfeca5a":"## 4. EXTRACT SUBMISSION FILE ##","ee0c38ec":"**Second Quartile**\n\n~38% Survived\n\nThere is not difference survivence between passengers in this quartile to passengers in the others"}}