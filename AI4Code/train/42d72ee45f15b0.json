{"cell_type":{"36eb36be":"code","6510adfb":"code","eba602d7":"code","548b5f3e":"code","d86b7055":"code","997bfea0":"code","8913105e":"code","d970fd3d":"code","36c5c079":"code","37ec07a9":"code","0f2f50a9":"code","e9c4b4cd":"code","d39254ec":"code","9c31fe3c":"code","ef042c6c":"code","790df6b5":"code","b5731764":"code","6ec0e350":"markdown","68d72c2c":"markdown","58cca7ef":"markdown","3ce68898":"markdown"},"source":{"36eb36be":"import os\nimport gc\nimport copy\nimport time\nimport random\nimport string\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import rankdata","6510adfb":"def ridge_cv(vex, X, y, X_test, folds, stratified):\n    kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=123)\n    val_scores = []\n    rmse_scores = []\n    X_less_toxics = []\n    X_more_toxics = []\n\n    preds = []\n\n    for fold, (train_index, val_index) in enumerate(kf.split(X, stratified)):\n        X_train, y_train = X[train_index], y[train_index]\n        X_val, y_val = X[val_index], y[val_index]\n        model = Ridge()\n        model.fit(X_train, y_train)\n\n        rmse_score = mean_squared_error(model.predict(X_val), y_val, squared=False)\n        rmse_scores.append(rmse_score)\n\n        X_less_toxic = vec.transform(df_val['less_toxic'])\n        X_more_toxic = vec.transform(df_val['more_toxic'])\n\n        p1 = model.predict(X_less_toxic)\n        p2 = model.predict(X_more_toxic)\n\n        X_less_toxics.append(p1)\n        X_more_toxics.append(p2)\n\n        val_acc = (p1 < p2).mean()\n        val_scores.append(val_acc)\n\n        pred = model.predict(X_test)\n        preds.append(pred)\n\n        print(f'FOLD:{fold}, rmse_fold:{rmse_score:.5f}, val_acc:{val_acc:.5f}')\n\n    mean_val_acc = np.mean(val_scores)\n    mean_rmse_score = np.mean(rmse_scores)\n\n    p1 = np.mean(np.vstack(X_less_toxics), axis=0)\n    p2 = np.mean(np.vstack(X_more_toxics), axis=0)\n\n    val_acc = (p1 < p2).mean()\n\n    print(f'00F: val_acc:{val_acc:.5f}, mean val_acc:{mean_val_acc:.5f}, mean rmse_score:{mean_rmse_score:.5f}')\n\n    preds = np.mean(np.vstack(preds), axis=0)\n\n    return p1, p2, preds","eba602d7":"data_path = '..\/input\/'","548b5f3e":"df_val = pd.read_csv(data_path + \"jigsaw-toxic-severity-rating\/validation_data.csv\")\ndf_test = pd.read_csv(data_path + \"jigsaw-toxic-severity-rating\/comments_to_score.csv\")","d86b7055":"jf_train_df = pd.read_csv(data_path + \"jigsaw-toxic-comment-classification-challenge\/train.csv\")\njf_train_df.head()","997bfea0":"jf_train_df['y'] = jf_train_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].max(axis=1)\njf_train_df = jf_train_df.query('y > 0')\nprint(jf_train_df.shape)\njf_train_df.head()","8913105e":"toxic = 1.0\nsevere_toxic = 2.0\nobscene = 1.0\nthreat = 1.0\ninsult = 1.0\nidentity_hate = 2.0\n\ndef create_train (df):\n    df['y'] = df['y'] + df['severe_toxic']*severe_toxic\n    df['y'] = df['y'] + df['obscene']*obscene\n    df['y'] = df['y'] + df['threat']*threat\n    df['y'] = df['y'] + df['insult']*insult\n    df['y'] = df['y'] + df['identity_hate']*identity_hate\n\n    df = df[['comment_text', 'y', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].rename(columns={'comment_text': 'text'})\n\n    # undersampling -> 0\uc778 \uac12\uc740 \uc774\ubbf8 \uc81c\uac70\ud588\uc73c\ubbc0\ub85c \uc77c\ub2e8\uc740 \uac74\ub108\ub6f0\uae30\n    # min_len = (df['y'] >= 1).sum()\n\n    return df\n\ndf = create_train(jf_train_df)\nprint(jf_train_df['y'].value_counts())","d970fd3d":"FOLDS = 5\n\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(4, 6))\nX = vec.fit_transform(df['text'])\ny = df['y'].values\nX_test = vec.transform(df_test['text'])\n\nstratified = np.around(y)\n\njf_p1, jf_p2, jf_preds = ridge_cv(vec, X, y, X_test, FOLDS, stratified)","36c5c079":"js_train_df = pd.read_csv(data_path + \"jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\njs_train_df.head()","37ec07a9":"js_train_df['y'] = js_train_df[['target', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack']].max(axis=1)\nprint(js_train_df.shape)\njs_train_df = js_train_df.query('y > 0')\nprint(js_train_df.shape)","0f2f50a9":"js_train_df['y'] = js_train_df[['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].sum(axis=1)\njs_train_df['y'] = js_train_df.apply(lambda row: row['target'] if row['target'] <= 0.5 else row['y'], axis=1)\njs_train_df = js_train_df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\n\n# down sampling\n\ndf = js_train_df\nprint(df['y'].value_counts())","e9c4b4cd":"FOLDS = 5\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(4, 6))\nX = vec.fit_transform(df['text'])\ny = df['y'].values\nX_test = vec.transform(df_test['text'])\n\nstratified = (np.around(y, decimals=1)*10).astype(int)\njs_p1, js_p2, js_preds = ridge_cv(vec, X, y, X_test, FOLDS, stratified)","d39254ec":"rd_train_df = pd.read_csv(data_path + \"ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv\")\nrd_train_df['y'] = rd_train_df['offensiveness_score'].map(lambda x : 0.0 if x <=0 else x)\nprint(rd_train_df.shape)","9c31fe3c":"rd_train_df = rd_train_df.query('y > 0')\nrd_train_df = rd_train_df[['txt', 'y']].rename(columns={'txt': 'text'})\n\nprint(rd_train_df['y'].value_counts())\nprint(rd_train_df.shape)","ef042c6c":"FOLDS = 5\ndf = rd_train_df\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(4, 6))\nX = vec.fit_transform(df['text'])\ny = df['y'].values\nX_test = vec.transform(df_test['text'])\n\nstratified = (np.around(y, decimals=1)*10).astype(int)\nrd_p1, rd_p2, rd_preds = ridge_cv(vec, X, y, X_test, FOLDS, stratified)","790df6b5":"# \uccab \ubc88\uc9f8 \uc131\ub2a5 \ubc15\uc0b4\ub098\uc11c \uc77c\ub2e8 \uc81c\uc678\n# jf_max = max(jf_p1.max(), jf_p2.max())\njs_max = max(js_p1.max(), js_p2.max())\nrd_max = max(rd_p1.max(), rd_p2.max())\n\n# p1 = jf_p1\/jf_max + js_p1\/js_max + rd_p1\/rd_max\n# p2 = jf_p2\/jf_max + js_p2\/js_max + rd_p2\/rd_max\n\np1 = js_p1\/js_max + rd_p1\/rd_max\np2 = js_p2\/js_max + rd_p2\/rd_max\n\nval_acc = (p1 < p2).mean()\nprint(f'Ensemble: val_acc:{val_acc:.5f}')","b5731764":"score = js_preds\/js_max + rd_preds\/rd_max\n\ndf_test['score'] = rankdata(score, method='ordinal')\n\ndf_test[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\n\ndf_test.head()","6ec0e350":"# Ruddit data set","68d72c2c":"# Ensemble","58cca7ef":"# Toxic Comment Classification Challenge data set","3ce68898":"# Jigsaw Unintended Bias in Toxicity Classification"}}