{"cell_type":{"8b6cb6b5":"code","8eadfd49":"code","4e546bd1":"code","230d846a":"code","7e05371b":"code","a69faa42":"code","442ddcb6":"code","e48d078c":"code","0469eee1":"code","6eeb02fe":"code","ba51639a":"code","cb5828cd":"code","915efc44":"markdown","72922859":"markdown","b5146455":"markdown","b2e7a8bc":"markdown","fbd04fdc":"markdown","166192b1":"markdown","d6436bce":"markdown","de5e2fe6":"markdown","f91f0809":"markdown","d883d86f":"markdown","bd590e01":"markdown"},"source":{"8b6cb6b5":"import numpy as np\n\nimport pandas as pd\n\nimport random\n\nimport xgboost as xgb\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_log_error\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","8eadfd49":"import gc\ngc.enable()","4e546bd1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","230d846a":"datasetp = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\nFOREDatasetp = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\n\ndataset = pd.read_csv(datasetp)\nFOREDataset = pd.read_csv(FOREDatasetp)\n\nFOREDatasetID = pd.DataFrame(FOREDataset['Id'])","7e05371b":"dataset.head(15)","a69faa42":"dataset = pd.get_dummies(dataset)\ndataset = dataset.where(pd.notna(dataset), dataset.mean(), axis=\"columns\")\n\n\nFOREDataset = pd.get_dummies(FOREDataset)\nFOREDataset = FOREDataset.where(pd.notna(FOREDataset), FOREDataset.mean(), axis=\"columns\")","442ddcb6":"ext_data_corrs = dataset.corr()\n\ndataPlotColnames = list(ext_data_corrs.columns.values)\n\n\n#ext_data_corrs = ext_data_corrs.drop(columns = dataDeleteCol )\n\n\n#plt.figure(figsize = (300, 300))\n\n#sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.5, annot = True, vmax = 0.5)\n#plt.title('Correlation Heatmap');","e48d078c":"dataset['OverallQual__2'] = dataset['OverallQual']**2\ndataset['OverallQual__3'] = dataset['OverallQual']**3\n\ndataset['YearBuilt__2'] = dataset['YearBuilt']**2\ndataset['YearBuilt__3'] = dataset['YearBuilt']**3\n\ndataset['TotalBsmtSF__2'] = dataset['TotalBsmtSF']**2\ndataset['TotalBsmtSF__3'] = dataset['TotalBsmtSF']**3\n\ndataset['GrLivArea__2'] = dataset['GrLivArea']**2\ndataset['GrLivArea__3'] = dataset['GrLivArea']**3\n\ndataset['GarageArea__2'] = dataset['GarageArea']**2\ndataset['GarageArea__3'] = dataset['GarageArea']**3\n\ndataset['TotalBsmtSF__GarageArea'] = dataset['TotalBsmtSF']*dataset['GarageArea']\n\n\ndataset['LotArea_FLAG'] = dataset['LotArea'] >400\ndataset['OverallQual__2_FLAG'] = dataset['OverallQual__2'] >60\ndataset['GarageArea_FLAG'] = dataset['GarageArea'] >800\ndataset['1stFlrSF_FLAG'] = dataset['1stFlrSF'] >2000","0469eee1":"FOREDataset['OverallQual__2'] = FOREDataset['OverallQual']**2\nFOREDataset['OverallQual__3'] = FOREDataset['OverallQual']**3\n\nFOREDataset['YearBuilt__2'] = FOREDataset['YearBuilt']**2\nFOREDataset['YearBuilt__3'] = FOREDataset['YearBuilt']**3\n\nFOREDataset['TotalBsmtSF__2'] = FOREDataset['TotalBsmtSF']**2\nFOREDataset['TotalBsmtSF__3'] = FOREDataset['TotalBsmtSF']**3\n\nFOREDataset['GrLivArea__2'] = FOREDataset['GrLivArea']**2\nFOREDataset['GrLivArea__3'] = FOREDataset['GrLivArea']**3\n\nFOREDataset['GarageArea__2'] = FOREDataset['GarageArea']**2\nFOREDataset['GarageArea__3'] = FOREDataset['GarageArea']**3\n\nFOREDataset['TotalBsmtSF__GarageArea'] = FOREDataset['TotalBsmtSF']*FOREDataset['GarageArea']\n\nFOREDataset['LotArea_FLAG'] = FOREDataset['LotArea'] >400\nFOREDataset['OverallQual__2_FLAG'] = FOREDataset['OverallQual__2'] >60\nFOREDataset['GarageArea_FLAG'] = FOREDataset['GarageArea'] >800\nFOREDataset['1stFlrSF_FLAG'] = FOREDataset['1stFlrSF'] >2000","6eeb02fe":"FOREList = []\nLRMSEList = []\nR2List = []\n\n\n\nfor x in range(100):\n\n  # Split dataset\n  trainDataset = dataset.sample(frac = 0.8)\n  testDataset = dataset.drop(trainDataset.index)\n\n\n  trainDataset_TARGET = trainDataset[['SalePrice']]\n  testDataset_TARGET = testDataset[['SalePrice']]\n\n\n  #-----------------\n  list1 = list(dataset.columns.values)\n  list2 = list(FOREDataset.columns.values)\n\n  dropList = list()\n\n  dropList = [ ele for ele in list1 ]\n  for a in list2:\n    if a in list1:\n      dropList.remove(a)\n\n  testDataset = testDataset.drop(columns=dropList)\n  trainDataset = trainDataset.drop(columns=dropList)\n  \n  X_train = trainDataset.iloc[:, 0:-1].values\n  y_train = trainDataset_TARGET.iloc[:, 0:1].values\n\n  X_test = testDataset.iloc[:, 0:-1].values\n  y_test = testDataset_TARGET.iloc[:, 0:1].values\n\n  X_FORE = FOREDataset.iloc[:, 0:-1].values\n\n\n  sc = StandardScaler()\n  X_train = sc.fit_transform(X_train)\n  X_test = sc.transform(X_test)\n  X_FORE = sc.transform(X_FORE)\n\n\n  regressor = xgb.XGBRegressor(\n\n      n_estimators=500,\n      min_child_weight=0.01, \n      colsample_bytree=0.8, \n      subsample=0.8, \n      eta=0.01\n  )\n    \n    \n  regressor.fit(X_train, y_train,\n          eval_set=[(X_train, y_train), (X_test, y_test)],\n          eval_metric='rmse',\n          verbose=False)\n\n  evals_result = regressor.evals_result()\n\n  y_pred = regressor.predict(X_test)\n\n\n\n  # RMSE Computation\n  lrmse = mean_squared_log_error(y_pred, y_test)\n  # print(\"LRMSE : % f\" %(lrmse))\n  LRMSEList.append(lrmse)\n  \n  # R2\n  R2 = r2_score(y_pred, y_test)\n  # R2List.append(R2)\n  print(\"R2 : % f\" %(R2))\n\n  y_FORE = regressor.predict(X_FORE)\n\n  # Append prediction to list of predictions\n  FOREList.append(y_FORE)","ba51639a":"FOREList = pd.DataFrame(FOREList)\nFOREList = FOREList.transpose()\n\nLRMSEList = pd.DataFrame(LRMSEList)\nR2List = pd.DataFrame(R2List)\n\n# It is working better with median\nFOREList['mean'] = FOREList.median(axis=1)","cb5828cd":"submission = pd.DataFrame(FOREDatasetID['Id'])\nsubmission['SalePrice'] = FOREList['mean']\n\nsubmission.to_csv(\"submission.csv\", index=False)","915efc44":"# 1. The idea behind approach\nAccurate predictions need precise feature engineering based on expert domain knowledge. In a case that dealt problem does not require expert knowledge it is not a big deal. The sale price of a property based on features of each property seems rational and \u201ceasy to imagine\u201d. On the other hand, dealing with problems requiring prerequisite knowledge is a different story. Uncertainty in the dataset and many other factors may lead to a point where barriers may occur.\n\nOne method which could help Maximum likelihood estimation. Many unprecise predictions may bring one pseudo optimum prediction after some averaging.","72922859":"And some memory management, in case this gets longer.","b5146455":"# 3. Estimation\nXGBRegressor from XGBoost library with standard parameters should for this sufficient.","b2e7a8bc":"# 4. Conclusion\nThe prediction is not good, but it may be good enough. It is the price for averaging. This approach does not consume developing and coding time but it is really demanding for computational time.\n\nPredictions grouped by index Normal distribution testing,\nShapiro-Wilk test, p-val 0.776221\nData are normally distributed\n\n\nSubmitted predictions from this notebook should have error around 0.12610\n","fbd04fdc":"This notebook is a super easy approach combining XGBoost regression and simplified \"Maximum likelihood estimation method\". The goal is to achieve some kind of middle accuracy in this competition without deeper problem knowledge and with basic code.\n\n\nTable of Content\n1. Idea behind approach\n2. Data\n3. Estimation\n4. Conclusion","166192b1":"The correlation matrix provided some deeper insight into the dataset. Identification of independent variables with significant positive or negative correlations with dependent variables is the first step. \n\nNow there is an option to extend the dataset by putting significant variables on powers, creating some interacting members, and creating a grouping flag for outliner cases.","d6436bce":"How does it work?\nPrediction is made again and again. The first step is splitting the dataset into train and test datasets randomly for the model estimation. Then the model is trained. The final prediction is made and added to the list of predictions indexed by ID. In the end, there are 50 predictions for every ID.","de5e2fe6":"\nBasic insight into the dataset may be obtained by plotting correlation matrix.\n","f91f0809":"Dummification and NaN","d883d86f":"Median is made in list.","bd590e01":"# 2. Data\nOnly basic imports, predictions will be made with XGBoost Regressor, Sklearn mainly for metrics and model validation."}}