{"cell_type":{"9c2f9549":"code","4ed4fb3c":"code","4360d652":"code","b0bfda7f":"code","30edb48f":"code","90a46fb2":"code","3281e2a1":"code","119f2f14":"code","7a0b8e00":"code","96fe0abb":"code","3e052b33":"code","03d56518":"markdown","6b38b619":"markdown","ebb34624":"markdown","5d763e1e":"markdown","6e8b5700":"markdown","bb2db068":"markdown"},"source":{"9c2f9549":"import pandas as pd\nimport numpy  as np\nimport matplotlib.pyplot as plt","4ed4fb3c":"# Make the synthetic dataset, with x values between 25 and 75\nX_train = 25 + 50 * np.random.default_rng(100).random((60,)) \n# y = x + some random noise\ny_train = X_train + (np.random.default_rng(30).random((60,))-0.5)*20\n\n# and the test data, having the x values ranging from 0 up to 100\nX_test  = np.linspace(0,100,100)\n\n# and the ground truth\ny_true =  X_test ","4360d652":"X_train = X_train.reshape(-1, 1)\nX_test = X_test.reshape(-1, 1)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.axvspan(25, 75 , color='green', alpha=0.5)\nax.scatter(x=X_train, y=y_train, c='blue')\nax.plot(X_test, y_true, c='orange', lw=3, label='ground truth')\nplt.legend(loc=\"upper left\",fontsize=14)\nax.set(xlim=(0, 100), ylim=(0, 100))\nplt.show();","b0bfda7f":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor()\nregressor.fit(X_train, y_train)\npredictions = regressor.predict(X_test)","30edb48f":"fig, ax = plt.subplots(figsize=(10, 6))\nax.axvspan(25, 75 , color='green', alpha=0.5)\nax.scatter(x=X_train, y=y_train, c='blue')\nax.plot(X_test, predictions, c='red', lw=2, label='Random Forest fit')\nax.plot(X_test, y_true, c='orange', lw=3, label='ground truth')\nplt.legend(loc=\"upper left\",fontsize=14)\nax.set(xlim=(0, 100), ylim=(0, 100))\nplt.show();","90a46fb2":"import xgboost as xgb\nregressor = xgb.XGBRegressor()\nregressor.fit(X_train, y_train)\npredictions = regressor.predict(X_test)","3281e2a1":"fig, ax = plt.subplots(figsize=(10, 6))\nax.axvspan(25, 75 , color='green', alpha=0.5)\nax.scatter(x=X_train, y=y_train, c='blue')\nax.plot(X_test, predictions, c='red', lw=2, label='XGBoost fit')\nax.plot(X_test, y_true, c='orange', lw=3, label='ground truth')\nplt.legend(loc=\"upper left\",fontsize=14)\nax.set(xlim=(0, 100), ylim=(0, 100))\nplt.show();","119f2f14":"from   keras.models import Sequential\nfrom   keras.layers import Dense    \ninput_dim        = X_train.shape[1] # the number of neurons in the input layer\nn_neurons        = 20       # the number of neurons in the first hidden layer\nepochs           = 200      # the number of training cycles\nmodel = Sequential()        # a model consisting of successive layers\n# input layer\nmodel.add(Dense(n_neurons, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n# output layer, with one neuron\nmodel.add(Dense(1, kernel_initializer='normal'))\n# compile the model\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\nmodel.fit(X_train, y_train, epochs=epochs, verbose=0)\npredictions = model.predict(X_test)","7a0b8e00":"fig, ax = plt.subplots(figsize=(10, 6))\nax.axvspan(25, 75 , color='green', alpha=0.5)\nax.scatter(x=X_train, y=y_train, c='blue')\nax.plot(X_test, predictions, c='red', lw=2, label='ANN fit')\nax.plot(X_test, y_true, c='orange', lw=3, label='ground truth')\nplt.legend(loc=\"upper left\",fontsize=14)\nax.set(xlim=(0, 100), ylim=(0, 100))\nplt.show();","96fe0abb":"fit = (np.polyfit(X_train.flatten(),y_train, 1))\nm = fit[0]\nc = fit[1]\nLS_fit = (m*X_test + c)","3e052b33":"fig, ax = plt.subplots(figsize=(10, 6))\nax.axvspan(25, 75 , color='green', alpha=0.5)\nax.scatter(x=X_train, y=y_train, c='blue')\nax.plot(X_test, LS_fit, c='red', lw=2, label='least squares fit')\nax.plot(X_test, y_true, c='orange', lw=3, label='ground truth')\nplt.legend(loc=\"upper left\",fontsize=14)\nax.set(xlim=(0, 100), ylim=(0, 100))\nplt.show();","03d56518":"# 3. Artificial neural network\nNow we shall use an [artificial neural network](https:\/\/en.wikipedia.org\/wiki\/Artificial_neural_network) using the [keras](https:\/\/keras.io\/) framework which is based on [tensorflow](https:\/\/www.tensorflow.org\/) platform. We shall use 20 [neurons](https:\/\/en.wikipedia.org\/wiki\/Artificial_neuron) and train for 200 epochs, using the  [relu activation](https:\/\/en.wikipedia.org\/wiki\/Rectifier_(neural_networks)), a mean squared error [loss funtion](https:\/\/keras.io\/api\/losses\/regression_losses\/) and an [adam optimizer](https:\/\/keras.io\/api\/optimizers\/adam\/):","6b38b619":"# Ok, what is going on with the forests?\nIt is rather simple really; for unseen data decision trees use the average of the data contained in the nearest leaf node to the point in question, thus for data outside of the forest it will simply always use the value of the data located at the very \"*edge*\" of the forest, hence in this case resulting in a horizontal line. ...So be warned: **do not stray out of the forest!**\n\n# Related reading\n* [Haozhe Zhang, Dan Nettleton, Zhengyuan Zhu \"*Regression-Enhanced Random Forests*\", arXiv:1904.10416\n (2019)](https:\/\/arxiv.org\/pdf\/1904.10416.pdf)","ebb34624":"# Extrapolation: Do not stray out of the forest!\nLet us produce a synthetic dataset based on the equation $$ y = x $$ along with some random noise, in the range $(25,75)$.\nWe shall now fit this data using a Random Forest, XGBoost and an artificial neural network, and finally a least squares fit.\nWe shall then look at the predictions **outside** of the range of the original training dataset.","5d763e1e":"# 1. Random Forest\nFirst we shall use the [Random Forest algorithm](https:\/\/en.wikipedia.org\/wiki\/Random_forest), using the scikit-learn [RandomForestRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html) implementation. We shall use the default parameters (which will end up overfitting this tiny dataset, but that is a different story).","6e8b5700":"Wow, that is much better! And finally...\n# 4. Least squares fit\nA [least squares](https:\/\/en.wikipedia.org\/wiki\/Least_squares) fit using [numpy polyfit](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.polyfit.html)","bb2db068":"# 2. XGBoost\nWe shall now use [gradient boostied decision trees](https:\/\/en.wikipedia.org\/wiki\/Gradient_boosting) using [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/index.html)"}}