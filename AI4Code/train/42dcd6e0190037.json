{"cell_type":{"91da9a94":"code","07b6e78a":"code","46bf6dee":"code","3c72bf9c":"code","8b916cbf":"code","0c9cc189":"code","e68fdbec":"code","da3350c4":"code","afbe99e9":"code","42800933":"code","a388999d":"code","95692748":"code","78f13d31":"code","948effae":"code","919df99e":"code","293cd672":"code","634094a4":"code","ce4ecee4":"code","ebb73e44":"code","2749bc92":"code","873d7ad1":"code","0180318e":"code","754871ec":"code","29a4d964":"code","b2b202bc":"code","8942da0f":"code","caaa059e":"code","396a0efe":"code","95c577f3":"code","1f950cc8":"code","f9ac5d48":"code","2f431d62":"code","a65c0af4":"code","8313753e":"code","ffe6b08e":"code","9f155910":"code","572002ac":"code","f57d4142":"code","fd814755":"code","5c664474":"code","11b594fc":"code","15a9fb0e":"code","fa6eba4c":"code","2109133f":"code","a7d10f83":"markdown","05781f94":"markdown","382b8e6a":"markdown","3c03e67c":"markdown","61cfe6c1":"markdown","0678fa84":"markdown","b3d70ff9":"markdown","05e92bd2":"markdown","8fbcb6ec":"markdown","014f2c30":"markdown","0905e561":"markdown"},"source":{"91da9a94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","07b6e78a":"!pip install pycaret","46bf6dee":"from pycaret.classification import *","3c72bf9c":"df = pd.read_csv('..\/input\/end-als\/end-als\/clinical-data\/filtered-metadata\/metadata\/aals_participants.csv', encoding='ISO-8859-2')\npd.set_option('display.max_columns', None)\ndf.head()","8b916cbf":"df.shape","0c9cc189":"df.isnull().sum()","e68fdbec":"#Those columns have 169 of missing values. The shape is 170.   \ncols_to_drop=['Diagnosis','Notes']\ndf=df.drop(cols_to_drop,axis=1)\ndf.columns","da3350c4":"#Code by Firat Gonen https:\/\/www.kaggle.com\/frtgnn\/pycaret-introduction-classification-regression\n\nclf = setup(data = df, \n             target = 'Sex',\n             numeric_imputation = 'mean',\n             categorical_features = ['Participant_ID', 'Case_Control', 'Enrollment Status'], \n             ignore_features = ['GUID','Cohort'],\n             silent = True)","afbe99e9":"compare_models()","42800933":"GBC  = create_model('gbc')","a388999d":"tuned_gbc = tune_model(GBC)","95692748":"plot_model(estimator = tuned_gbc, plot = 'learning')","78f13d31":"#Feature Importance\n\nplot_model(estimator=tuned_gbc, plot='feature')","948effae":"evaluate_model(tuned_gbc)","919df99e":"RF  = create_model('rf')","293cd672":"tuned_rf = tune_model(RF)","634094a4":"plot_model(estimator = tuned_rf, plot = 'learning')","ce4ecee4":"plot_model(estimator = tuned_rf, plot = 'auc')","ebb73e44":"plot_model(estimator = tuned_rf, plot = 'confusion_matrix')","2749bc92":"#Feature Importance\n\nplot_model(estimator=tuned_rf, plot='feature')","873d7ad1":"evaluate_model(tuned_rf)","0180318e":"interpret_model(tuned_rf)","754871ec":"#Make a prediction.\n\npredictions = predict_model(tuned_rf, data=df)\npredictions.head()","29a4d964":"df['Sex'] = round(predictions['Score']).astype(int)\ndf.to_csv('submission.csv',index=False)\ndf.head()","b2b202bc":"#Try an ensemble  tuned RF, catboost and Naive Bayes\n\nCB  = create_model('catboost')\nNB  = create_model('nb')\nblend = blend_models(estimator_list=[tuned_rf, CB, NB])","8942da0f":"tuned_catboost = tune_model(CB)","caaa059e":"#Plot some of the results.\n\nplot_model(estimator = tuned_catboost, plot = 'learning') ","396a0efe":"plot_model(estimator = tuned_catboost, plot = 'auc')","95c577f3":"plot_model(estimator = tuned_catboost, plot = 'confusion_matrix')","1f950cc8":"#Feature Importance\n\nplot_model(estimator=tuned_catboost, plot='feature')","f9ac5d48":"evaluate_model(tuned_catboost)","2f431d62":"interpret_model(tuned_catboost)","a65c0af4":"#Make a prediction\n\npredictions = predict_model(tuned_catboost, data=df)\npredictions.head()","8313753e":"df['Sex'] = round(predictions['Score']).astype(int)\ndf.to_csv('submission.csv',index=False)\ndf.head()","ffe6b08e":"from pycaret.regression import *","9f155910":"#Code by Firat Gonen https:\/\/www.kaggle.com\/frtgnn\/pycaret-introduction-classification-regression\n\nreg = setup(data = df,\n             target = 'Sex',\n             numeric_imputation = 'mean',\n             categorical_features = ['Participant_ID', 'GUID', 'Case_Control', 'Cohort', 'Enrollment Status'],\n            normalize = True,\n             silent = True)","572002ac":"#Compare Regression models.\n\ncompare_models()","f57d4142":"PAR = create_model('par')","fd814755":"#Tun the model.\n\ntuned_par = tune_model(PAR)","5c664474":"plot_model(estimator = tuned_par, plot = 'learning')","11b594fc":"#Feature Importance\n\nplot_model(estimator=tuned_par, plot='feature')","15a9fb0e":"evaluate_model(tuned_par)","fa6eba4c":"#Just to give an example of Passive Aggressive Regressor.\n\nfrom sklearn.linear_model import PassiveAggressiveRegressor\nfrom sklearn.datasets import make_regression","2109133f":"#Just to exemplify Passive Aggressive Regressor\n\n#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.PassiveAggressiveRegressor.html\n\nX, y = make_regression(n_features=4, random_state=0)\nregr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\ntol=1e-3)\nregr.fit(X, y)\n\nprint(regr.coef_)\n\nprint(regr.intercept_)\n\nprint(regr.predict([[0, 0, 0, 0]]))","a7d10f83":"# **<span style=\"color:#DC143C;\">Passive-Aggressive algorithms<\/span>**\n\n\"Passive-Aggressive algorithms are generally used for large-scale learning. It is one of the few \u2018online-learning algorithms\u2018. In online machine learning algorithms, the input data comes in sequential order and the machine learning model is updated step-by-step, as opposed to batch learning, where the entire training dataset is used at once. This is very useful in situations where there is a huge amount of data and it is computationally infeasible to train the entire dataset because of the sheer size of the data.\"\n\n\"Passive-Aggressive algorithms are somewhat similar to a Perceptron model, in the sense that they do not require a learning rate. However, they do include a regularization parameter.\"\n\nPassive-Aggressive algorithms are called so because :\n\n\"PASSIVE: If the prediction is correct, keep the model and do not make any changes. i.e., the data in the example is not enough to cause any changes in the model.\"\n\n\"AGGRESSIVE: If the prediction is incorrect, make changes to the model. i.e., some change to the model may correct it.\nUnderstanding the mathematics behind this algorithm is not very simple and is beyond the scope of a single article.\" \n\n\"For practical usage of this algorithm, huge streams of data are required.\"\n\nhttps:\/\/www.geeksforgeeks.org\/passive-aggressive-classifiers\/","05781f94":"#Median time from ALS diagnosis to symptoms onset stratified on body part weakness among adults with ALS, 19 October 2010\u201331 December 2015.\n\nCitation: Raymond J, Oskarsson B, Mehta P, Horton K. Clinical characteristics of a large cohort of US participants enrolled in the National Amyotrophic Lateral Sclerosis (ALS) Registry, 2010-2015. Amyotroph Lateral Scler Frontotemporal Degener. 2019 Aug;20(5-6):413-420. doi: 10.1080\/21678421.2019.1612435. Epub 2019 May 26. PMID: 31131638; PMCID: PMC6946020.\n\n![](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/instance\/6946020\/bin\/nihms-1064143-f0002.jpg)https:\/\/pubmed.ncbi.nlm.nih.gov\/31131638\/","382b8e6a":"#Algorithms are arranged in descending order of \"Accuracy\". Now, use Gradient Boosting Classifier which has the highest accuracy. ","3c03e67c":"#Install Pycaret before All the rest.","61cfe6c1":"For plotting Passive Agressive Regressor AUC and Confusion Matrix:\n\nValueError: Plot Not Available. Please see docstring for list of available Plots.\n\nFor interpret_model: This function only supports tree based models for binary classification: et, catboost, rf, xgboost, lightgbm, dt","0678fa84":"#Median time from ALS diagnosis to symptoms onset among adults with ALS, 19 October 2010\u201331 December 2015.\n\nCitation: Raymond J, Oskarsson B, Mehta P, Horton K. Clinical characteristics of a large cohort of US participants enrolled in the National Amyotrophic Lateral Sclerosis (ALS) Registry, 2010-2015. Amyotroph Lateral Scler Frontotemporal Degener. 2019 Aug;20(5-6):413-420. doi: 10.1080\/21678421.2019.1612435. Epub 2019 May 26. PMID: 31131638; PMCID: PMC6946020.\n\n\n![](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/instance\/6946020\/bin\/nihms-1064143-f0001.jpg)https:\/\/pubmed.ncbi.nlm.nih.gov\/31131638\/","b3d70ff9":"References https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.PassiveAggressiveRegressor.html\n\nOnline Passive-Aggressive Algorithms\nhttp:\/\/jmlr.csail.mit.edu\/papers\/volume7\/crammer06a\/crammer06a.pdf K. Crammer, O. Dekel,\nJ. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)","05e92bd2":"#Code by Firat Gonen https:\/\/www.kaggle.com\/frtgnn\/pycaret-introduction-classification-regression","8fbcb6ec":"\"The update taken by PA (Passive Agressive) algorithms is aggressive in the sense that even a small loss forces an update of the hypothesis. When using kernels, this property often results in the use of many examples for representing the learned predictor. Thus, the memory requirements imposed when using kernels can be quite demanding. The authors are\ncurrently pursuing extensions of the PA framework that operate in the realm of bounded memory constraints.\"\n\nhttps:\/\/jmlr.csail.mit.edu\/papers\/volume7\/crammer06a\/crammer06a.pdf","014f2c30":"#Now, use Random Forest, which is able to perform all functions of Pycaret. ","0905e561":"<h1 style=\"background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Clinical characteristics of participants enrolled in the Amyotrophic Lateral Sclerosis (ALS) Registry<\/h1>\n\nClinical characteristics of a large cohort of US participants enrolled in the National Amyotrophic Lateral Sclerosis (ALS) Registry, 2010-2015\n\nCitation: Raymond J, Oskarsson B, Mehta P, Horton K. Clinical characteristics of a large cohort of US participants enrolled in the National Amyotrophic Lateral Sclerosis (ALS) Registry, 2010-2015. Amyotroph Lateral Scler Frontotemporal Degener. 2019 Aug;20(5-6):413-420. doi: 10.1080\/21678421.2019.1612435. Epub 2019 May 26. PMID: 31131638; PMCID: PMC6946020.\n\n\"Describing the clinical characteristics in a large cohort of ALS participants enrolled in the National ALS Registry.\" \n\n\"Data from ALS participants who completed the Registry's online clinical survey module during 2010-2015 were analyzed to determine characteristics, such as site of onset, associated symptoms, time of symptom onset to diagnosis, time of diagnosis to hospice referral, and pharmacological and non-pharmacological interventions.\"\n\n\"Of the 1758 participants who completed the survey, 60.9% were male, 62.1% were 50-69 years old, and 95.5% white. Approximately, 72.0% reported initial limb weakness onset of disease, followed by bulbar (22.1%), and trunk\/global onset (6.1%).\" \n\n\"Other symptoms ever experienced included cramps (56.7%), fasciculations (56.3%), and dysarthria (33.0%). The median time between an increase of muscle cramps until an ALS diagnosis was 12 months; limb onset participants had cramps longer preceding diagnosis versus those with bulbar onset.\"\n\n\"The most frequent interventions used included riluzole (48.3% currently using), wheelchairs\/scooters (32.8%), and noninvasive breathing equipment (30.0%). Participants with trunk\/global onset were referred to hospice almost four times earlier than others.\"\n\n\"These data show how ALS clinical characteristics differ widely in a large cohort of participants preceding diagnosis and reflect variations in disease onset, progression, and prognosis. Better characterization of symptom onset may assist clinicians in diagnosing ALS sooner, which could lead to earlier therapeutic interventions.\"\n\nhttps:\/\/pubmed.ncbi.nlm.nih.gov\/31131638\/"}}