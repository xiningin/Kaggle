{"cell_type":{"9edf79f3":"code","9cadfb79":"code","7048d4c1":"code","e80b2973":"code","d7f957d6":"code","e287d449":"code","8151dd55":"code","77ca5f02":"code","0440cf56":"code","e88a83da":"code","ad91e3e9":"code","a60f8a84":"code","371ce24d":"code","8a5d7af6":"code","8241ea7e":"code","d7f0a525":"code","3806eaae":"code","b4e4fe75":"code","a1454d65":"code","92900f14":"code","be6f4226":"code","28c8d853":"code","e4ef4a51":"code","550e360b":"code","2881342b":"code","7f9b1387":"code","ecdc3c80":"code","823e9655":"code","288974d0":"code","fd921973":"code","f59e7bc1":"code","56fb6871":"code","164c60d0":"code","1a91778b":"code","fb430eee":"code","5b34897d":"code","ea03f91e":"code","201e111a":"code","87a66433":"code","b9390112":"code","3dd90a64":"code","101b420c":"code","68a81de9":"code","260e92de":"code","4572dcfb":"code","cad6534a":"markdown","6b917dde":"markdown","1ad7f519":"markdown","8788005b":"markdown","c60a830f":"markdown","7c46d7fe":"markdown","d739485b":"markdown","959e6420":"markdown","6833161e":"markdown","ba53be82":"markdown","0636fc6e":"markdown","feff4a31":"markdown","c6604eee":"markdown","a34b64c1":"markdown","c3f0e2fd":"markdown","6e209ece":"markdown","c7b6a173":"markdown","bcffd1b9":"markdown","af3adc78":"markdown","24a12428":"markdown","1dfd7256":"markdown","e302e6fb":"markdown","5fc4252e":"markdown","7e0da6e0":"markdown","377f3001":"markdown","016c548f":"markdown","25cbea54":"markdown","fa65876b":"markdown","6a406bcb":"markdown","39547774":"markdown","fd167648":"markdown","9590dfb1":"markdown","815a3b6d":"markdown","4bdacddb":"markdown","0ee03a97":"markdown","8aa24401":"markdown","b0e85834":"markdown","c7b7c80c":"markdown","ecb2dba6":"markdown","ecb11b53":"markdown","ff83b1fb":"markdown","14fc2132":"markdown","e01c9322":"markdown","51debd20":"markdown"},"source":{"9edf79f3":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\nrandom.seed(42)","9cadfb79":"df = pd.read_csv('..\/input\/ab-data\/ab_data.csv')\ndf.head()","7048d4c1":"df.shape","e80b2973":"df['user_id'].nunique()","d7f957d6":"df['converted'].mean()","e287d449":"#Adding the two values\nlen(df.query('group==\"treatment\" and landing_page!=\"new_page\"')) + len(df.query('group==\"control\" and landing_page!=\"old_page\"'))","8151dd55":"df.isnull().sum()","77ca5f02":"df.info()","0440cf56":"#Finding the mismatch rows\nmismatch_1 = df.query('group==\"treatment\" and landing_page!=\"new_page\"')\nmismatch_2 = df.query('group==\"control\" and landing_page!=\"old_page\"')\n\n#Dropping those columns by their index\ndf2 = df.drop(mismatch_1.index)\ndf2 = df2.drop(mismatch_2.index)","e88a83da":"# Double Check all of the correct rows were removed - this should be 0\ndf2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]","ad91e3e9":"#No. of unique rows\ndf2['user_id'].nunique()","a60f8a84":"#Comparing with total size\ndf2['user_id'].shape","371ce24d":"#User ID of the repeated row\ndf2[df2['user_id'].duplicated()==True]['user_id']","8a5d7af6":"#Info of the repeated column\ndf2[df2['user_id'].duplicated()==True]","8241ea7e":"#Dropping the duplicated row\ndf2.drop_duplicates(subset='user_id',inplace=True)","d7f0a525":"df2['converted'].mean()","3806eaae":"#Probability of control group\ndf2_control = df2.query('group==\"control\"')\ndf2_control['converted'].mean()","b4e4fe75":"#Probability of treatment group\ndf2_treatment = df2.query('group==\"treatment\"')\ndf2_treatment['converted'].mean()","a1454d65":"#Split of pages between pages\ndf2.query('landing_page==\"new_page\"').shape[0]\/df2.shape[0]","92900f14":"#Calculating convert rate\np_new = df2.converted.mean() \np_new","be6f4226":"#Calculating convert rate\np_old = df2.converted.mean()\np_old ## Both are same!!","28c8d853":"#size of new page group\nn_new = len(df2[df2['group'] =='treatment'])\nn_new","e4ef4a51":"#size of new page group\nn_old = len(df2[df2['group'] =='control'])\nn_old","550e360b":"#Simulating conversion for new page for 0's and 1's\nnew_page_converted = np.random.binomial(1, p_new, size=n_new)\nnew_page_converted","2881342b":"#Simulating conversion for old page 0's and 1's \nold_page_converted = np.random.binomial(1, p_old, size=n_old)\nold_page_converted","7f9b1387":"#Single simuation difference in mean\ndiff = new_page_converted.mean() - old_page_converted.mean()\ndiff","ecdc3c80":"#Simulating for 10000 times\np_diffs=[]\nfor _ in range(10000):\n    new_converted = np.random.binomial(1, p_new, size=n_new).mean()\n    old_converted = np.random.binomial(1, p_old, size=n_old).mean()\n    p_diffs.append(new_converted-old_converted)","823e9655":"#Converting to array\np_diffs = np.array(p_diffs)","288974d0":"#Actual difference from dataset\nacc_diff = df2_treatment['converted'].mean() - df2_control['converted'].mean() \nacc_diff","fd921973":"plt.hist(p_diffs);\nplt.xlabel('Mean of Probability Difference')\nplt.ylabel('#Count')\nplt.axvline(acc_diff,color='red',label='Actual mean difference')\nplt.legend()","f59e7bc1":"#P-Value\n(p_diffs > acc_diff).mean()","56fb6871":"import statsmodels.api as sm\n\nconvert_old = len(df2.query('landing_page == \"old_page\" and converted ==1 '))\nconvert_new = len(df2.query('landing_page == \"new_page\" and converted ==1 '))\nn_old = df2.query('landing_page == \"old_page\"').shape[0]\nn_new = df2.query('landing_page == \"new_page\"').shape[0]","164c60d0":"#Values of the above\nconvert_old,convert_new,n_new,n_old","1a91778b":"#smaller as we need to have p1 < p2 for alternative hypothesis\nz_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') \nz_score, p_value","fb430eee":"df2.head()","5b34897d":"df2['intercept'] = 1","ea03f91e":"df2[['control','ab_page']] = pd.get_dummies(df2['group'])","201e111a":"df2.drop('control',axis=1,inplace=True)","87a66433":"lm = sm.Logit(df2['converted'],df2[['intercept','ab_page']])\nresult = lm.fit()","b9390112":"result.summary()","3dd90a64":"countries_df = pd.read_csv('..\/input\/ab-data\/countries.csv')\ndf_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')","101b420c":"df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])","68a81de9":"lm = sm.Logit(df_new['converted'],df_new[['CA','US','intercept']])\nresult = lm.fit()\nresult.summary()","260e92de":"# Adding Interactions\ndf_new['US_ab_page'] = df_new['US'] * df_new['ab_page']\ndf_new['CA_ab_page'] = df_new['CA'] * df_new['ab_page']","4572dcfb":"lm = sm.Logit(df_new['converted'],df_new[['CA','US','intercept','ab_page','US_ab_page','CA_ab_page']])\nresult = lm.fit()\nresult.summary()","cad6534a":"<a id='conclusions'><\/a>\n## Conclusions\n\nFrom the above results where every situation we carried out the p-value was greater than 0.05 and there was no statistical evidence to reject the null hypothesis. We can say that the company should not implement the new page as it does not significant impact on conversion rates and rather continue with the old page. ","6b917dde":"f. Simulating $n_{old}$ transactions with a convert rate of $p_{old}$ under the null.","1ad7f519":"**`control` group probability that they converted?**","8788005b":"**ANSWER** \n<br>\n$$ H_0: p_{old} \\ge p_{new}$$\n<br>\n$$H_1: p_{old} < p_{new}$$","c60a830f":"**Probability of an individual converting regardless of the page they receive**","7c46d7fe":"**The p-value pf ab_page in the regression model is 0.190 which significantly lower than the p-value in Part II.\n<br>\nThis is because the Null and Alternative Hypothesis of Part II and Part III differed.**\n<br>\n$$Part II:$$\n$$ H_0: p_{old} - p_{new} \\ge 0$$ \n$$ H_1: p_{old} - p_{new} < 0$$\n<br>\n$$Part III:$$\n$$ H_0: p_{old} = p_{new}$$\n$$ H_1: p_{old} \\ne p_{new}$$\n<br>\nAlso in part 2 we implemented a one tailed test whereas in part 3 we implemented two tailed test. ","d739485b":"**The Control group had a conversion rate of 12.03% and Treatment group had a conversion rate of 11.88% among all the testing they did on users. Looking at this probability the new page doesn't seem to have much of an impact on the conversion rate as both are nearly equal. But it seems interesting to find further details of why this is happening.**","959e6420":"**`treatment` group probability that they converted**","6833161e":"Removing **one** of the rows with a duplicate **user_id**","ba53be82":"Proportion of the **p_diffs** greater than the actual difference observed in **ab_data.csv** i.e finding the **p-value**","0636fc6e":"**Convert rate** for $p_{old}$ under the null<br><br>","feff4a31":"<a id='ab_test'><\/a>\n### Part II - A\/B Test\n\nNotice that because of the time stamp associated with each event, we could technically run a hypothesis test continuously as each observation was observed.  \n\nHowever, then the hard question is do you stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time?  How long do we run to render a decision that neither page is better than another?  \n\nThese questions are the difficult parts associated with A\/B tests in general.  \n\n\nFor now, consider we need to make the decision just based on all the data provided.  If we want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%, the null $H_{0}$ and alternative $H_{1}$ hypotheses be? They are stated below in terms of **$p_{old}$** and **$p_{new}$**, which are the converted rates for the old and new pages.","c6604eee":"Probabilty of users received the new page","a34b64c1":"`stats.proportions_ztest` to compute test statistic and p-value.  [Reference Link](http:\/\/knowledgetack.com\/python\/statsmodels\/proportions_ztest\/) ","c3f0e2fd":"Instantiate the model and fit the model using the two columns to predict whether or not an individual converts.","6e209ece":"Simulating $n_{new}$ transactions with a convert rate of $p_{new}$ under the null. ","c7b6a173":"$n_{old}$ size of old page group","bcffd1b9":"### Let's add an effect based on which country a user lives.\n\nCreate dummy variables for these country columns. We will need two columns for the three dummy variables while putting it in the regression model.","af3adc78":"Unique **user_id**s in **df2**","24a12428":"$p_{new}$ - $p_{old}$ for the simulated values","1dfd7256":"**It would yield better results when we use other explanatory variables in the model as they might also have an influence on the decision of whether a user converts or not.\n<br>\nThere may also arise an issue of adding more explanatory variables, if there is any collinearity between these variables the model would not give proper results.**","e302e6fb":"### Though we have now looked at the individual factors of country and page on conversion, we would now like to look at an interaction between page and country to see if there significant effects on conversion.  ","5fc4252e":"**The pages interaction with countries also does not have any impact on the conversion rate as the p values are above 0.05.**<br>\n***Hence we again fail to reject the null hypothesis.***","7e0da6e0":"Summary of the model ","377f3001":"Simulating for 10,000 iteration $p_{new}$ - $p_{old}$ values. Storing all 10,000 values in **p_diffs**.","016c548f":"The number of times the `new_page` and `treatment` don't line up.","25cbea54":"### Using the Z-Test this time to verify the above findings","fa65876b":"The rows where **treatment** is not aligned with **new_page** or **control** is not aligned with **old_page**, we cannot be sure if this row truly received the new or old page. Dropping those rows and storing the new dataframe in **df2**.","6a406bcb":"The proportion of users converted.","39547774":"**Convert rate** for $p_{new}$ under the null","fd167648":"**We found the p-value above, it is the probability of the null hypothesis being true. So in order to make a change the p-value should be as low as possible.\n<br>\nHere the p-value is 0.9089 which is very high than the max acceptable error rate of 0.05. Currently we have no statistical evidence to reject the null hypothesis and there is no significant difference between the old and new pages.**<br><br>\n***Hence, we fail to reject the Null Hypothesis.*** ","9590dfb1":"Looking for duplicated user_ids","815a3b6d":"Assuming under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, they are equal to the **converted** rate in **ab_data.csv** regardless of the page. <br><br>\n\nPerforming the sampling distribution for the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null.  <br><br>","4bdacddb":"$n_{new}$ size of new page group","0ee03a97":"Creating a column for the intercept, and also create a dummy variable column for which page each user received. **ab_page** column, which is 1 when an individual receives the **treatment** and 0 if **control**.","8aa24401":"Number of rows in the dataset.","b0e85834":"<a id='regression'><\/a>\n### Part III - A regression approach\n\nIn this part, we will try to further verify the result acheived in the previous A\/B test by performing regression.<br><br>\n\nSince each row is either a conversion or no conversion, we will be using **LOGISTIC REGRESSION**","c7b7c80c":"**The p-value of both the countries is greater than 0.05, so we do not have enough evidence to show that the countries have an effect on conversion.** <br><br>***We fail to reject the null hypothesis here as well.***","ecb2dba6":"## Analyze A\/B Test Results\nDivyansh Shah - June 2020\n\n## Table of Contents\n- [Introduction](#intro)\n- [Part I - Probability](#probability)\n- [Part II - A\/B Test](#ab_test)\n- [Part III - Regression](#regression)\n\n\n<a id='intro'><\/a>\n### Introduction\n\nA\/B tests are very commonly performed by data analysts and data scientists.  \n\nFor this project, I will be working to understand the results of an A\/B test run by an e-commerce website.  The goal is to work through this notebook to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.\n\n<a id='probability'><\/a>\n#### Part I - Probability\n\nTo get started, let's import our libraries.","ecb11b53":"Missing values","ff83b1fb":"Histogram of **p_diffs**.","14fc2132":"`1.` Read in the `ab_data.csv` data","e01c9322":"**From the above, the Z-Score is 1.31 which is less than the critical value of 1.96 for 95% confidence interval so we** ***fail to reject the null hypothesis*** **and the p-value is 0.90 which also determines that** ***we fail to reject the null hypothesis.***  \n<br>\nSource : https:\/\/www.statisticshowto.com\/probability-and-statistics\/find-critical-values\/#CVZ","51debd20":"Unique users in the dataset."}}