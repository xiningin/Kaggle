{"cell_type":{"319cc32c":"code","5325e040":"code","32de8c89":"code","9b27ff5d":"code","e486e305":"code","fc65c230":"code","d2799ec1":"code","8b901ff3":"code","3d4ce84d":"code","f13c823d":"code","6799e55c":"code","37577d3d":"code","4755be36":"code","6f81b752":"code","23333c5e":"code","009b4bfd":"markdown","779a2380":"markdown","b5a0d66a":"markdown"},"source":{"319cc32c":"import numpy as np \nimport pandas as pd \nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom skimage.filters import threshold_otsu\nimport lightgbm as lgb\nimport gc\nfrom tqdm import tqdm\n\nSEED = 0\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","5325e040":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\", index_col='id')\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\", index_col='id')\n\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\n\nfeatures = [x for x in train.columns.values if x[0]==\"f\"]\n","32de8c89":"new_features_dict = train.isna().astype(int).sum(axis=1)\nnew_features_test = test.isna().astype(int).sum(axis=1)\nfor i in range(10):\n    train['miss_bt_' + str(i)] = (new_features_dict > i).astype(int)\n    test['miss_bt_' + str(i)] = (new_features_test > i).astype(int)","9b27ff5d":"train['n_missing'] = train[features].isna().sum(axis=1)\ntrain['abs_sum'] = train[features].abs().sum(axis=1)\ntrain['sem'] = train[features].sem(axis=1)\ntrain['std'] = train[features].std(axis=1)\ntrain['avg'] = train[features].mean(axis=1)\ntrain['max'] = train[features].max(axis=1)\ntrain['min'] = train[features].min(axis=1)\n\ntest['n_missing'] = test[features].isna().sum(axis=1)\ntest['abs_sum'] = test[features].abs().sum(axis=1)\ntest['sem'] = test[features].sem(axis=1)\ntest['std'] = test[features].std(axis=1)\ntest['avg'] = test[features].mean(axis=1)\ntest['max'] = test[features].min(axis=1)\ntest['min'] = test[features].min(axis=1)","e486e305":"fill_value_dict = {\n    'f1': 'Mean', \n    'f2': 'Median', \n    'f3': 'Median', \n    'f4': 'Median', \n    'f5': 'Mode', \n    'f6': 'Mean', \n    'f7': 'Median', \n    'f8': 'Median', \n    'f9': 'Median', \n    'f10': 'Median', \n    'f11': 'Mean', \n    'f12': 'Median', \n    'f13': 'Mean', \n    'f14': 'Median', \n    'f15': 'Mean', \n    'f16': 'Median', \n    'f17': 'Median', \n    'f18': 'Median', \n    'f19': 'Median', \n    'f20': 'Median', \n    'f21': 'Median', \n    'f22': 'Mean', \n    'f23': 'Mode', \n    'f24': 'Median', \n    'f25': 'Median', \n    'f26': 'Median', \n    'f27': 'Median', \n    'f28': 'Median', \n    'f29': 'Mode', \n    'f30': 'Median', \n    'f31': 'Median', \n    'f32': 'Median', \n    'f33': 'Median', \n    'f34': 'Mean', \n    'f35': 'Median', \n    'f36': 'Mean', \n    'f37': 'Median', \n    'f38': 'Median', \n    'f39': 'Median', \n    'f40': 'Mode', \n    'f41': 'Median', \n    'f42': 'Mode', \n    'f43': 'Mean', \n    'f44': 'Median', \n    'f45': 'Median', \n    'f46': 'Mean', \n    'f47': 'Mode', \n    'f48': 'Mean', \n    'f49': 'Mode', \n    'f50': 'Mode', \n    'f51': 'Median', \n    'f52': 'Median', \n    'f53': 'Median', \n    'f54': 'Mean', \n    'f55': 'Mean', \n    'f56': 'Mode', \n    'f57': 'Mean', \n    'f58': 'Median', \n    'f59': 'Median', \n    'f60': 'Median', \n    'f61': 'Median', \n    'f62': 'Median', \n    'f63': 'Median', \n    'f64': 'Median', \n    'f65': 'Mode', \n    'f66': 'Median', \n    'f67': 'Median', \n    'f68': 'Median', \n    'f69': 'Mean', \n    'f70': 'Mode', \n    'f71': 'Median', \n    'f72': 'Median', \n    'f73': 'Median', \n    'f74': 'Mode', \n    'f75': 'Mode', \n    'f76': 'Mean', \n    'f77': 'Mode', \n    'f78': 'Median', \n    'f79': 'Mean', \n    'f80': 'Median', \n    'f81': 'Mode', \n    'f82': 'Median', \n    'f83': 'Mode', \n    'f84': 'Median', \n    'f85': 'Median', \n    'f86': 'Median', \n    'f87': 'Median', \n    'f88': 'Median', \n    'f89': 'Median', \n    'f90': 'Mean', \n    'f91': 'Mode', \n    'f92': 'Median', \n    'f93': 'Median', \n    'f94': 'Median', \n    'f95': 'Median', \n    'f96': 'Median', \n    'f97': 'Mean', \n    'f98': 'Median', \n    'f99': 'Median', \n    'f100': 'Mode', \n    'f101': 'Median', \n    'f102': 'Median', \n    'f103': 'Median', \n    'f104': 'Median', \n    'f105': 'Median', \n    'f106': 'Median', \n    'f107': 'Median', \n    'f108': 'Median', \n    'f109': 'Mode', \n    'f110': 'Median', \n    'f111': 'Median', \n    'f112': 'Median', \n    'f113': 'Mean', \n    'f114': 'Median', \n    'f115': 'Median', \n    'f116': 'Mode', \n    'f117': 'Median', \n    'f118': 'Mean'\n}\n\n\nfor col in tqdm(features):\n    if fill_value_dict.get(col)=='Mean':\n        fill_value = train[col].mean()\n    elif fill_value_dict.get(col)=='Median':\n        fill_value = train[col].median()\n    elif fill_value_dict.get(col)=='Mode':\n        fill_value = train[col].mode().iloc[0]\n    \n    train[col].fillna(fill_value, inplace=True)\n    test[col].fillna(fill_value, inplace=True)","fc65c230":"X = train.drop([\"claim\"], axis=1)\nX_test = test\ny = train[\"claim\"]","d2799ec1":"scaler = StandardScaler()\nX = pd.DataFrame(scaler.fit_transform(X))\ntest_data = pd.DataFrame(scaler.transform(X_test))","8b901ff3":"del test, train, scaler, X_test\ngc.collect()","3d4ce84d":"# helper functions\ndef get_auc(y_true, y_hat):\n    fpr, tpr, _ = roc_curve(y_true, y_hat)\n    score = auc(fpr, tpr)\n    return score","f13c823d":"# best params\nlgbm1_params = {\n    'metric' : 'auc',\n    'max_depth' : 3,\n    'num_leaves' : 7,\n    'n_estimators' : 5000,\n    'colsample_bytree' : 0.3,\n    'subsample' : 0.5,\n    'random_state' : 42,\n    'reg_alpha' : 18,\n    'reg_lambda' : 17,\n    'learning_rate' : 0.095,\n    'device' : 'gpu',\n    'objective' : 'binary'\n}\n\nlgbm2_params = {\n    'metric' : 'auc',\n    'objective': 'binary',\n    'n_estimators': 10000,\n    'random_state': 42,\n    'learning_rate': 0.095,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'device' : 'gpu',\n    'max_depth' : 3,\n    'num_leaves' : 7\n}\n\nlgbm3_params = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n    'device_type': 'gpu', \n    'n_estimators': 10000, \n    'learning_rate': 0.12230165751633416, \n    'num_leaves': 1400, \n    'max_depth': 8, \n    'min_child_samples': 3100, \n    'reg_alpha': 10, \n    'reg_lambda': 65, \n    'min_split_gain': 5.157818977461183, \n    'subsample': 0.5, \n    'subsample_freq': 1, \n    'colsample_bytree': 0.2\n}\n\ncatb1_params = {\n    'eval_metric' : 'AUC',\n    'iterations': 15585, \n    'objective': 'CrossEntropy',\n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}\n\ncatb2_params = {\n    'eval_metric' : 'AUC',\n    'depth' : 5,\n    'grow_policy' : 'SymmetricTree',\n    'l2_leaf_reg' : 3.0,\n    'random_strength' : 1.0,\n    'learning_rate' : 0.1,\n    'iterations' : 10000,\n    'loss_function' : 'CrossEntropy',\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}\n\nxgb1_params = {\n    'eval_metric' : 'auc',\n    'lambda': 0.004562711234493688, \n    'alpha': 7.268146704546314, \n    'colsample_bytree': 0.6468987558386358, \n    'colsample_bynode': 0.29113878257290376, \n    'colsample_bylevel': 0.8915913499148167, \n    'subsample': 0.37130229826185135, \n    'learning_rate': 0.021671163563123198, \n    'grow_policy': 'lossguide', \n    'max_depth': 18, \n    'min_child_weight': 215, \n    'max_bin': 272,\n    'n_estimators': 10000,\n    'random_state': 0,\n    'use_label_encoder': False,\n    'objective': 'binary:logistic',\n    'tree_method': 'gpu_hist',\n    'gpu_id': 0,\n    'predictor': 'gpu_predictor'\n}\n\nxgb2_params = dict(\n    eval_metric='auc',\n    max_depth=3,\n    subsample=0.5,\n    colsample_bytree=0.5,\n    learning_rate=0.01187431306013263,\n    n_estimators=10000,\n    n_jobs=-1,\n    use_label_encoder=False,\n    objective='binary:logistic',\n    tree_method='gpu_hist',\n    gpu_id=0,\n    predictor='gpu_predictor'\n)\n\nxgb3_params = {\n    'eval_metric': 'auc', \n    'objective': 'binary:logistic', \n    'tree_method': 'gpu_hist', \n    'gpu_id': 0, \n    'predictor': 'gpu_predictor', \n    'n_estimators': 10000, \n    'learning_rate': 0.01063045229441343, \n    'gamma': 0.24652519525750877, \n    'max_depth': 4, \n    'min_child_weight': 366, \n    'subsample': 0.6423040816299684, \n    'colsample_bytree': 0.7751264493218339, \n    'colsample_bylevel': 0.8675692743597421, \n    'lambda': 0, \n    'alpha': 10\n}","6799e55c":"%%time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nmodels = [\n    ('lgbm1', LGBMClassifier(**lgbm1_params)),\n    ('lgbm2', LGBMClassifier(**lgbm2_params)),\n    ('lgbm3', LGBMClassifier(**lgbm3_params)),\n    ('catb1', CatBoostClassifier(**catb1_params)),\n    ('catb2', CatBoostClassifier(**catb2_params)),\n    ('xgb1', XGBClassifier(**xgb1_params)),\n    ('xgb2', XGBClassifier(**xgb2_params)),\n    ('xgb3', XGBClassifier(**xgb3_params))\n]\n\noof_pred_tmp = dict()\ntest_pred_tmp = dict()\nscores_tmp = dict()\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    for name, model in models:\n        if name not in scores_tmp:\n            oof_pred_tmp[name] = list()\n            oof_pred_tmp['y_valid'] = list()\n            test_pred_tmp[name] = list()\n            scores_tmp[name] = list()\n     \n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_valid,y_valid)],\n            verbose=0\n        )\n        \n        pred_valid = model.predict_proba(X_valid)[:,1]\n        score = get_auc(y_valid, pred_valid)\n        \n        scores_tmp[name].append(score)\n        oof_pred_tmp[name].extend(pred_valid)\n        \n        print(f\"Fold: {fold + 1} Model: {name} Score: {score}\")\n        print('--'*20)\n        \n        y_hat = model.predict_proba(test_data)[:,1]\n        test_pred_tmp[name].append(y_hat)\n    \n    oof_pred_tmp['y_valid'].extend(y_valid)\n        \nfor name, model in models:\n    print(f\"Overall Validation Score | {name}: {np.mean(scores_tmp[name])}\")\n    print('::'*20)","37577d3d":"base_test_predictions = pd.DataFrame(\n    {name: np.mean(np.column_stack(test_pred_tmp[name]), axis=1) \n    for name in test_pred_tmp.keys()}\n)\n\nbase_test_predictions.to_csv('.\/base_test_predictions.csv', index=False)\nbase_test_predictions['simple_avg'] = base_test_predictions.mean(axis=1)\nsimple_blend_submission = sample_submission.copy()\nsimple_blend_submission['claim'] = base_test_predictions['simple_avg']\nsimple_blend_submission.to_csv('.\/simple_blend_submission.csv', index=False)","4755be36":"oof_predictions = pd.DataFrame(\n    {name:oof_pred_tmp[name] for name in oof_pred_tmp.keys()}\n)\n\noof_predictions.to_csv('.\/oof_predictions.csv', index=False)\n\ny_valid = oof_predictions['y_valid'].copy()\ny_hat_blend = oof_predictions.drop(columns=['y_valid']).mean(axis=1)\nscore = get_auc(y_valid, y_hat_blend)\n\nprint(f\"Overall Validation Score | Simple Blend: {score}\")\nprint('::'*20)","6f81b752":"%%time\nfrom sklearn.linear_model import LogisticRegression\n\nX_meta = oof_predictions.drop(columns=['y_valid']).copy()\ny_meta = oof_predictions['y_valid'].copy()\ntest_meta = base_test_predictions.drop(columns=['simple_avg']).copy()\n\nmeta_pred_tmp = []\nscores_tmp = []\n\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X_meta, y_meta)):\n    X_train, y_train = X_meta.iloc[idx_train], y_meta.iloc[idx_train]\n    X_valid, y_valid = X_meta.iloc[idx_valid], y_meta.iloc[idx_valid]\n\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    pred_valid = model.predict_proba(X_valid)[:,1]\n    score = get_auc(y_valid, pred_valid)\n    scores_tmp.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*20)\n    \n    y_hat = model.predict_proba(test_meta)[:,1]\n    meta_pred_tmp.append(y_hat)\n    \nprint(f\"Overall Validation Score | Meta: {np.mean(scores_tmp)}\")\nprint('::'*20)","23333c5e":"meta_predictions = np.mean(np.column_stack(meta_pred_tmp), axis=1)\n\nstacked_submission = sample_submission.copy()\nstacked_submission['claim'] = meta_predictions\nstacked_submission.to_csv('.\/stacked_submission.csv', index=False)","009b4bfd":"### <div style='background:#3b606f;color:white;padding:0.5em;border-radius:0.2em'>Train Meta Learner<\/div>","779a2380":"Credits to:\n1. https:\/\/www.kaggle.com\/mlanhenke\/tps-09-simple-blend-stacking-xgb-lgbm-catb\n2. https:\/\/www.kaggle.com\/realtimshady\/single-simple-lightgbm","b5a0d66a":"params taken from:\n- [catb1: my own optuna study](https:\/\/www.kaggle.com\/mlanhenke\/tps-09-optuna-study-catboostclassifier)\n- [Stacking Ensemble for Beginner](https:\/\/www.kaggle.com\/junhyeok99\/stacking-ensemble-for-beginner)"}}