{"cell_type":{"d55ecf21":"code","f1c58615":"code","b6c98ac7":"code","17ebd642":"code","620bec65":"code","a6cc0db4":"code","0e193e51":"code","5ff4dca1":"code","a984ac79":"code","a20144ab":"code","3f8d3808":"code","71acd526":"code","2b92003a":"code","01b0f2da":"code","af7e5f6d":"code","42678201":"code","d6ac4d34":"code","8a997182":"code","69045260":"code","2495ff0f":"code","37b63e33":"code","2ce86401":"code","e96ebaef":"code","7fee0743":"code","59d9ad38":"code","a8c2b421":"code","ce834db4":"code","15bde607":"code","89977af0":"code","a60b0099":"code","c3713e85":"code","946573f7":"code","571df82b":"code","24f69907":"code","2aa7eff3":"code","616ae718":"code","867afdd1":"code","ccd3de2c":"code","38f44f38":"code","af29f434":"code","0f85a83c":"code","d092472c":"code","c25e59b6":"code","06ce61f4":"code","11f9c660":"code","bbf6f510":"code","a7baeb7b":"code","63631411":"code","951625a7":"code","e81b3aa8":"code","380bd27a":"code","bc98c814":"code","47ec43a2":"code","82dfd213":"code","ca758a67":"code","1a27ba43":"code","8571c064":"code","165c4618":"code","3b664ef8":"code","32a942b6":"code","00ba3b01":"code","ea0eb874":"code","93b51985":"code","758e5cd0":"code","dabab802":"code","ee26491b":"code","9d09e897":"code","018766bc":"code","33b2808c":"code","2edd4ee6":"markdown","f38036c8":"markdown","295830ef":"markdown","4b68ac8d":"markdown","5c86b743":"markdown","98975927":"markdown","568c366b":"markdown","cf219d7a":"markdown","999622d4":"markdown","529b1ef5":"markdown","d628511c":"markdown","9301abc8":"markdown","25bff782":"markdown","e60a0418":"markdown","62be2ef6":"markdown","bbb22565":"markdown","713facd5":"markdown","01643a26":"markdown","599d769a":"markdown","6778f1ec":"markdown","ec368854":"markdown","78fcbc29":"markdown","2febaebe":"markdown","204d1449":"markdown","00014af4":"markdown","0483a04e":"markdown","369fe1ed":"markdown"},"source":{"d55ecf21":"import gc\nimport time\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMClassifier\nfrom contextlib import contextmanager\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics, preprocessing\n\nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.decomposition import PCA \n\nimport seaborn as sns\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\npd.set_option('display.max_rows', 999)\npd.set_option('display.max_columns',700)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n","f1c58615":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n","b6c98ac7":"train_transaction = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntrain_identity = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\n\ntest_transaction = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\ntest_identity = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\n\n# Fix Column Name \nfix_col_name = {testIdCol:trainIdCol for testIdCol, trainIdCol in zip(test_identity.columns, train_identity.columns)}\ntest_identity.rename(columns=fix_col_name, inplace=True)\n\n# Reduce Memory\ntrain_transaction = reduce_mem_usage(train_transaction)\ntrain_identity = reduce_mem_usage(train_identity)\n\ntest_transaction = reduce_mem_usage(test_transaction)\ntest_identity = reduce_mem_usage(test_identity)    \n    \n# Merge (transaction - identity)\ntrain = train_transaction.merge(train_identity, on='TransactionID', how='left')\ntest = test_transaction.merge(test_identity, on='TransactionID', how='left')\n\n# Merge (X_train - X_test)\ntrain_test = pd.concat([train, test], ignore_index=True)\n\nprint(f'train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n\ndel train_transaction, train_identity, test_transaction, test_identity; x = gc.collect()","17ebd642":"train_test = train_test.copy()\ntrain = train.copy()\ntest = test.copy()","620bec65":"def check_df(df, head=5):\n    print(\"##################### Shape #####################\")\n    print(df.shape)\n\n    print(\"##################### Types #####################\")\n    print(df.dtypes)\n\n    print(\"##################### Head #####################\")\n    print(df.head(head))\n\n    print(\"##################### NA #####################\")\n    print(df.isnull().sum())\n\ncheck_df(train)","a6cc0db4":"# Function of Missing Values\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 1)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df , end=\"\\n\")\n    if na_name:\n        return na_columns\n\nmissing_values_table(train, na_name=False)","0e193e51":"train_fraud = train.loc[train['isFraud'] == 1]\ntrain_non_fraud = train.loc[train['isFraud'] == 0]\n\ntrain['isFraud'].value_counts(normalize=True)","5ff4dca1":"sns.countplot(x=\"isFraud\", data=train).set_title('Distribution of Target')\nplt.show()","a984ac79":"plt.figure(figsize=(15,5))\nsns.distplot(train[\"TransactionDT\"])\nsns.distplot(test[\"TransactionDT\"])\nplt.title('train vs test TransactionDT distribution')\nplt.show()","a20144ab":"plt.figure(figsize=(15,5))\nsns.distplot(train_fraud[\"TransactionDT\"], color='b', label='Fraud')\nsns.distplot(train_non_fraud[\"TransactionDT\"], color='r', label ='non-Fraud')\nplt.title('Fraud vs non-Fraud TransactionDT Distribution')\nplt.legend()","3f8d3808":"START_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n\ntrain_test['New_Date'] = train_test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\ntrain_test['New_Date_YMD'] = train_test['New_Date'].dt.year.astype(str) + '-' + train_test['New_Date'].dt.month.astype(str) + '-' + train_test['New_Date'].dt.day.astype(str)\ntrain_test['New_Date_YearMonth'] = train_test['New_Date'].dt.year.astype(str) + '-' + train_test['New_Date'].dt.month.astype(str)\ntrain_test['New_Date_Weekday'] = train_test['New_Date'].dt.dayofweek\ntrain_test['New_Date_Hour'] = train_test['New_Date'].dt.hour\ntrain_test['New_Date_Day'] = train_test['New_Date'].dt.day\n\n\nfig,ax = plt.subplots(4, 1, figsize=(16,15))\n\ntrain_test.groupby('New_Date_Weekday')['isFraud'].mean().to_frame().plot.bar(ax=ax[0])\ntrain_test.groupby('New_Date_Hour')['isFraud'].mean().to_frame().plot.bar(ax=ax[1])\ntrain_test.groupby('New_Date_Day')['isFraud'].mean().to_frame().plot.bar(ax=ax[2])\ntrain_test.groupby('New_Date_YearMonth')['isFraud'].mean().to_frame().plot.bar(ax=ax[3])\n","71acd526":"print(pd.concat([train['TransactionAmt'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index(),\n                 train_fraud['TransactionAmt'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index(), \n                 train_non_fraud['TransactionAmt'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index()],\n                   axis=1, keys=['Total','Fraud', \"No Fraud\"]))","2b92003a":"print(' Fraud TransactionAmt mean      :  '+str(train_fraud['TransactionAmt'].mean()))\nprint(' Non - Fraud TransactionAmt mean:  '+str(train_non_fraud['TransactionAmt'].mean()))","01b0f2da":"plt.figure(figsize=(15,5))\nsns.distplot(train_test[\"TransactionAmt\"].apply(np.log))\nplt.title('Train - Test TransactionAmt distribution')\nplt.show()","af7e5f6d":"plt.figure(figsize=(15,5))\nsns.distplot(train_fraud[\"TransactionAmt\"].apply(np.log), label = 'Fraud | isFraud = 1')\nsns.distplot(train_non_fraud[\"TransactionAmt\"].apply(np.log), label = 'non-Fraud | isFraud = 0')\nplt.title('Fraud vs non-Fraud TransactionAmt distribution')\nplt.legend()\nplt.show()","42678201":"train['New_TransactionAmt_Bin'] = pd.qcut(train['TransactionAmt'],15)\ntrain.groupby('New_TransactionAmt_Bin')[['isFraud']].mean()","d6ac4d34":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\ntrain['dist1'].plot(kind='hist',bins=5000,ax=ax1,title='dist1 distribution',logx=True)\ntrain['dist2'].plot(kind='hist',bins=5000,ax=ax2,title='dist2 distribution',logx=True)\nplt.show()","8a997182":"def getCatFeatureDetail(df,cat_cols):\n    cat_detail_dict = {} \n    for col in cat_cols:\n        cat_detail_dict[col] = df[col].nunique()\n    cat_detail_df = pd.DataFrame.from_dict(cat_detail_dict, orient='index', columns=['nunique'])\n    print('There are ' + str(len(cat_cols)) + ' categorical columns.')\n    print(cat_detail_df)\n    \ncat_features = ['isFraud','ProductCD','addr1', 'addr2', 'P_emaildomain','R_emaildomain','DeviceType','DeviceInfo']\nall_cat_features = cat_features+ [f'card{i}' for i in range(1,7)]+ [f'M{i}' for i in range(1,10)] + [f'id_{i}' for i in range(12,39)]\n    \ngetCatFeatureDetail(train_test, cat_features)","69045260":"def ploting_cnt_amt(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    total = len(df)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{col} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\",fontsize=12) \n        \n    g.set_ylim(0,max(sizes)*1.15)\n    plt.show()","2495ff0f":"ploting_cnt_amt(train, 'ProductCD')","37b63e33":"train['addr1'].value_counts().head(10)","2ce86401":"train['addr2'].value_counts().head(10)","e96ebaef":"train.loc[train['addr1'].isin(train['addr1'].value_counts()[train['addr1'].value_counts() <= 5000 ].index), 'addr1'] = \"Others\"\ntrain.loc[train['addr2'].isin(train['addr2'].value_counts()[train['addr2'].value_counts() <= 50 ].index), 'addr2'] = \"Others\"\n\ntest.loc[test['addr1'].isin(test.addr1.value_counts()[test['addr1'].value_counts() <= 5000 ].index), 'addr1'] = \"Others\"\ntest.loc[test['addr2'].isin(test.addr2.value_counts()[test['addr2'].value_counts() <= 50 ].index), 'addr2'] = \"Others\"\n\ntrain['addr1'].fillna(\"NoInf\", inplace=True)\ntest['addr1'].fillna(\"NoInf\", inplace=True)\n\ntrain['addr2'].fillna(\"NoInf\", inplace=True)\ntest['addr2'].fillna(\"NoInf\", inplace=True)","7fee0743":"ploting_cnt_amt(train, \"addr1\")","59d9ad38":"ploting_cnt_amt(train, \"addr2\")","a8c2b421":"train['P_emaildomain'].value_counts()","ce834db4":"train.loc[train['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n\ntrain.loc[train['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                         'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                         'yahoo.es']), 'P_emaildomain'] = 'Yahoo Mail'\ntrain.loc[train['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                         'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                         'outlook.es', 'live.com', 'live.fr',\n                                         'hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\ntrain.loc[train['P_emaildomain'].isin(train['P_emaildomain']\\\n                                         .value_counts()[train.P_emaildomain.value_counts() <= 500 ]\\\n                                         .index), 'P_emaildomain'] = \"Others\"\ntrain['P_emaildomain'].fillna(\"NoInf\", inplace=True)","15bde607":"ploting_cnt_amt(train, 'P_emaildomain')","89977af0":"train['R_emaildomain'].value_counts()","a60b0099":"train.loc[train['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n\ntrain.loc[train['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                             'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                             'yahoo.es']), 'R_emaildomain'] = 'Yahoo Mail'\ntrain.loc[train['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                             'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                             'outlook.es', 'live.com', 'live.fr',\n                                             'hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\ntrain.loc[train['R_emaildomain'].isin(train.R_emaildomain\\\n                                         .value_counts()[train['R_emaildomain'].value_counts() <= 300 ]\\\n                                         .index), 'R_emaildomain'] = \"Others\"\ntrain['R_emaildomain'].fillna(\"NoInf\", inplace=True)","c3713e85":"ploting_cnt_amt(train, 'R_emaildomain')","946573f7":"train['DeviceType'].value_counts()","571df82b":"ploting_cnt_amt(train, 'DeviceType')","24f69907":"train['DeviceInfo'].value_counts()","2aa7eff3":"train['DeviceInfo'].value_counts().head(10).plot(kind='barh', figsize=(15, 5), title='Top 20 Devices in Train')\nplt.show()","616ae718":"train_test['DeviceInfo'] = train_test['DeviceInfo'].fillna('unknown_device').str.lower()\ntrain_test['DeviceName'] = train_test['DeviceInfo'].str.split('\/', expand=True)[0]\n\ntrain_test.loc[train_test['DeviceName'].str.contains('SM', na=False), 'DeviceName'] = 'Samsung'\ntrain_test.loc[train_test['DeviceName'].str.contains('SAMSUNG', na=False), 'DeviceName'] = 'Samsung'\ntrain_test.loc[train_test['DeviceName'].str.contains('GT-', na=False), 'DeviceName'] = 'Samsung'\ntrain_test.loc[train_test['DeviceName'].str.contains('Moto G', na=False), 'DeviceName'] = 'Motorola'\ntrain_test.loc[train_test['DeviceName'].str.contains('Moto', na=False), 'DeviceName'] = 'Motorola'\ntrain_test.loc[train_test['DeviceName'].str.contains('moto', na=False), 'DeviceName'] = 'Motorola'\ntrain_test.loc[train_test['DeviceName'].str.contains('LG-', na=False), 'DeviceName'] = 'LG'\ntrain_test.loc[train_test['DeviceName'].str.contains('rv:', na=False), 'DeviceName'] = 'RV'\ntrain_test.loc[train_test['DeviceName'].str.contains('HUAWEI', na=False), 'DeviceName'] = 'Huawei'\ntrain_test.loc[train_test['DeviceName'].str.contains('ALE-', na=False), 'DeviceName'] = 'Huawei'\ntrain_test.loc[train_test['DeviceName'].str.contains('-L', na=False), 'DeviceName'] = 'Huawei'\ntrain_test.loc[train_test['DeviceName'].str.contains('Blade', na=False), 'DeviceName'] = 'ZTE'\ntrain_test.loc[train_test['DeviceName'].str.contains('BLADE', na=False), 'DeviceName'] = 'ZTE'\ntrain_test.loc[train_test['DeviceName'].str.contains('Linux', na=False), 'DeviceName'] = 'Linux'\ntrain_test.loc[train_test['DeviceName'].str.contains('XT', na=False), 'DeviceName'] = 'Sony'\ntrain_test.loc[train_test['DeviceName'].str.contains('HTC', na=False), 'DeviceName'] = 'HTC'\ntrain_test.loc[train_test['DeviceName'].str.contains('ASUS', na=False), 'DeviceName'] = 'Asus'\n\ntrain_test.loc[train_test['DeviceName'].isin(train_test['DeviceName'].value_counts()[train_test['DeviceName'].value_counts() < 1000].index), 'DeviceName'] = \"Others\"","867afdd1":"ploting_cnt_amt(train_test, 'DeviceName')","ccd3de2c":"card_cols = [c for c in train.columns if 'card' in c]\ntrain[card_cols].head()","38f44f38":"train_test[card_cols].isnull().sum()","af29f434":"for col in card_cols:\n    print(col+'  :' + str(train[col].nunique()))","0f85a83c":"for col in ['card2','card3','card4','card5','card6']:\n    train_test[col] = train_test.groupby(['card1'])[col].transform(lambda x: x.mode(dropna=False).iat[0])\n    train_test[col].fillna(train_test[col].mode()[0], inplace=True)\n    print(col+' has : '+str(train_test[col].isnull().sum())+' missing values')","d092472c":"ploting_cnt_amt(train, 'card4')","c25e59b6":"ploting_cnt_amt(train, 'card6')","06ce61f4":"c_cols = [c for c in train if c[0] == 'C']\ntrain[c_cols].head()","11f9c660":"train[c_cols].describe()","bbf6f510":"train[c_cols].quantile([.01, .1, .25, .5, .75, .9, .99])","a7baeb7b":"# train[train['C6']>118.000]['isFraud'].mean()\n\nfor col in c_cols:\n    print('\\n Fraud '+col+' mean    :  '+str(train_fraud[train_fraud[col]<=37.00][col].mean()))\n    print(' Non - Fraud '+col+' mean:  '+str(train_non_fraud[train_non_fraud[col]<=37.00][col].mean()))","63631411":"d_cols = ['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14']\ntrain[d_cols].head()","951625a7":"train[d_cols].describe()","e81b3aa8":"for col in d_cols:\n    plt.figure(figsize=(15,5))\n    plt.scatter(train['TransactionDT'] ,train[col])\n    plt.title(col + ' Vs TransactionDT')\n    plt.xlabel('Time')\n    plt.ylabel(col)\n    plt.show()","380bd27a":"msno.matrix(train[d_cols]);","bc98c814":"m_cols = [c for c in train if c[0] == 'M']\nfor col in m_cols:\n    ploting_cnt_amt(train, col, lim=2500)","47ec43a2":"msno.matrix(train[m_cols]);","82dfd213":"v_cols = [c for c in train if c[0] == 'V']\ntrain[v_cols].head()","ca758a67":"train[v_cols].describe()","1a27ba43":"v_cols = [c for c in train_test if c[0] == 'V']\nv_nan_df = train_test[v_cols].isna()\nnan_groups={}\n\nfor col in v_cols:\n    cur_group = v_nan_df[col].sum()\n    try:\n        nan_groups[cur_group].append(col)\n    except:\n        nan_groups[cur_group]=[col]\ndel v_nan_df; x=gc.collect()","8571c064":"def plot_corr(v_cols):\n    cols = v_cols + ['TransactionDT']\n    plt.figure(figsize=(15,15))\n    sns.heatmap(train[cols].corr(),cmap='RdBu_r', annot=True, center=0.0)\n    plt.title(v_cols[0]+' - '+v_cols[-1],fontsize=14)\n    plt.show()","165c4618":"for k,v in nan_groups.items():\n    plot_corr(v)","3b664ef8":"id_cols = [c for c in train_test if c[:2] == 'id']\n\nid_num_cols=id_cols[:11]\nid_cat_cols=id_cols[11:]","32a942b6":"train[id_num_cols].describe()","00ba3b01":"for col in id_num_cols:\n    print('\\n'+col)\n    print(' Fraud mean    :  ' + str(train_fraud[col].mean()))\n    print(' Non - Fraud mean:  ' + str(train_non_fraud[col].mean()))","ea0eb874":"getCatFeatureDetail(train,id_cat_cols)","93b51985":"for col in  ['id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29']:\n    ploting_cnt_amt(train, col, lim=2500)","758e5cd0":"@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n    \n# Display\/plot feature importance\ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:100].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(15, 20))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances01.png')\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","dabab802":"def loading_data():\n    print('LOADING DATA')\n    train_transaction = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\n    train_identity = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\n\n    test_transaction = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\n    test_identity = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\n\n    # Fix column name \n    fix_col_name = {testIdCol:trainIdCol for testIdCol, trainIdCol in zip(test_identity.columns, train_identity.columns)}\n    test_identity.rename(columns=fix_col_name, inplace=True)\n\n    ## Reduce memory\n    train_transaction = reduce_mem_usage(train_transaction)\n    train_identity = reduce_mem_usage(train_identity)\n\n    test_transaction = reduce_mem_usage(test_transaction)\n    test_identity = reduce_mem_usage(test_identity)\n\n    # Merge (transaction-identity)\n    train = train_transaction.merge(train_identity, on='TransactionID', how='left')\n    test = test_transaction.merge(test_identity, on='TransactionID', how='left')\n\n    #MERGE (X_train - X_test)\n    train_test = pd.concat([train, test], ignore_index=True)\n\n    print(f'train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n    print(f'test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n\n    del train_transaction, train_identity, test_transaction, test_identity; x = gc.collect()  \n    return train_test","ee26491b":"def processing_data(train_test):\n    print('PROCESSING DATA')\n    drop_col_list = []\n    \n    # TransactionDT\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n    train_test['NewDate'] = train_test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n    train_test['NewDate_YMD'] = train_test['NewDate'].dt.year.astype(str) + '-' + train_test['NewDate'].dt.month.astype(str) + '-' + train_test['NewDate'].dt.day.astype(str)\n    train_test['NewDate_YearMonth'] = train_test['NewDate'].dt.year.astype(str) + '-' + train_test['NewDate'].dt.month.astype(str)\n    train_test['NewDate_Weekday'] = train_test['NewDate'].dt.dayofweek\n    train_test['NewDate_Hour'] = train_test['NewDate'].dt.hour\n    train_test['NewDate_Day'] = train_test['NewDate'].dt.day\n    drop_col_list.extend([\"TransactionDT\",\"NewDate\"])  ## !!!\n    \n    # TransactionAMT\n    train_test['New_Cents'] = (train_test['TransactionAmt'] - np.floor(train_test['TransactionAmt'])).astype('float32')\n    train_test['New_TransactionAmt_Bin'] = pd.qcut(train_test['TransactionAmt'],15)\n    \n    #cardX\n    card_cols = [c for c in train_test if c[0:2] == 'ca']\n    for col in ['card2','card3','card4','card5','card6']:\n        train_test[col] = train_test.groupby(['card1'])[col].transform(lambda x: x.mode(dropna=False).iat[0])\n        train_test[col].fillna(train_test[col].mode()[0], inplace=True)\n    \n    \n    # P_email_domain & R_email_domain \n    train_test.loc[train_test['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n    train_test.loc[train_test['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk','yahoo.co.jp', 'yahoo.de', 'yahoo.fr','yahoo.es']), 'P_emaildomain'] = 'Yahoo'\n    train_test.loc[train_test['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', 'hotmail.es','hotmail.co.uk', 'hotmail.de','outlook.es', 'live.com', 'live.fr','hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\n    train_test.loc[train_test['P_emaildomain'].isin(train_test['P_emaildomain'].value_counts()[train_test['P_emaildomain'].value_counts() <= 500 ].index), 'P_emaildomain'] = \"Others\"\n    train_test['P_emaildomain'].fillna(\"Unknown\", inplace=True)\n\n    train_test.loc[train_test['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n    train_test.loc[train_test['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk','yahoo.co.jp', 'yahoo.de', 'yahoo.fr','yahoo.es']), 'R_emaildomain'] = 'Yahoo'\n    train_test.loc[train_test['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', 'hotmail.es','hotmail.co.uk', 'hotmail.de','outlook.es', 'live.com', 'live.fr','hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\n    train_test.loc[train_test['R_emaildomain'].isin(train_test['R_emaildomain'].value_counts()[train_test['R_emaildomain'].value_counts() <= 300 ].index), 'R_emaildomain'] = \"Others\"\n    train_test['R_emaildomain'].fillna(\"Unknown\", inplace=True)\n\n    # DeviceInfo\n    train_test['DeviceInfo'] = train_test['DeviceInfo'].fillna('unknown_device').str.lower()\n    train_test['DeviceInfo'] = train_test['DeviceInfo'].str.split('\/', expand=True)[0]\n    \n    train_test.loc[train_test['DeviceInfo'].str.contains('SM', na=False), 'DeviceInfo'] = 'Samsung'\n    train_test.loc[train_test['DeviceInfo'].str.contains('SAMSUNG', na=False), 'DeviceInfo'] = 'Samsung'\n    train_test.loc[train_test['DeviceInfo'].str.contains('GT-', na=False), 'DeviceInfo'] = 'Samsung'\n    train_test.loc[train_test['DeviceInfo'].str.contains('Moto G', na=False), 'DeviceInfo'] = 'Motorola'\n    train_test.loc[train_test['DeviceInfo'].str.contains('Moto', na=False), 'DeviceInfo'] = 'Motorola'\n    train_test.loc[train_test['DeviceInfo'].str.contains('moto', na=False), 'DeviceInfo'] = 'Motorola'\n    train_test.loc[train_test['DeviceInfo'].str.contains('LG-', na=False), 'DeviceInfo'] = 'LG'\n    train_test.loc[train_test['DeviceInfo'].str.contains('rv:', na=False), 'DeviceInfo'] = 'RV'\n    train_test.loc[train_test['DeviceInfo'].str.contains('HUAWEI', na=False), 'DeviceInfo'] = 'Huawei'\n    train_test.loc[train_test['DeviceInfo'].str.contains('ALE-', na=False), 'DeviceInfo'] = 'Huawei'\n    train_test.loc[train_test['DeviceInfo'].str.contains('-L', na=False), 'DeviceInfo'] = 'Huawei'\n    train_test.loc[train_test['DeviceInfo'].str.contains('Blade', na=False), 'DeviceInfo'] = 'ZTE'\n    train_test.loc[train_test['DeviceInfo'].str.contains('BLADE', na=False), 'DeviceInfo'] = 'ZTE'\n    train_test.loc[train_test['DeviceInfo'].str.contains('Linux', na=False), 'DeviceInfo'] = 'Linux'\n    train_test.loc[train_test['DeviceInfo'].str.contains('XT', na=False), 'DeviceInfo'] = 'Sony'\n    train_test.loc[train_test['DeviceInfo'].str.contains('HTC', na=False), 'DeviceInfo'] = 'HTC'\n    train_test.loc[train_test['DeviceInfo'].str.contains('ASUS', na=False), 'DeviceInfo'] = 'Asus'\n\n    train_test.loc[train_test['DeviceInfo'].isin(train_test['DeviceInfo'].value_counts()[train_test['DeviceInfo'].value_counts() < 1000].index), 'DeviceInfo'] = \"Others\"\n\n    # V1 - V339\n    v_cols = [c for c in train_test if c[0] == 'V']\n    v_nan_df = train_test[v_cols].isna()\n    nan_groups={}\n\n    for col in v_cols:\n        cur_group = v_nan_df[col].sum()\n        try:\n            nan_groups[cur_group].append(col)\n        except:\n            nan_groups[cur_group]=[col]\n    del v_nan_df; x=gc.collect()\n    \n    for nan_cnt, v_group in nan_groups.items():\n        train_test['New_v_group_'+str(nan_cnt)+'_nulls'] = nan_cnt\n        sc = preprocessing.MinMaxScaler()\n        pca = PCA(n_components=2)\n        v_group_pca = pca.fit_transform(sc.fit_transform(train_test[v_group].fillna(-1)))\n        train_test['New_v_group_'+str(nan_cnt)+'_pca0'] = v_group_pca[:,0]\n        train_test['New_v_group_'+str(nan_cnt)+'_pca1'] = v_group_pca[:,1]\n\n    drop_col_list.extend(v_cols)\n    \n    print('CREATING NEW FEATURES')\n        \n    train_test['New_card1_card2']=train_test['card1'].astype(str)+'_'+train_test['card2'].astype(str)\n    train_test['New_addr1_addr2']=train_test['addr1'].astype(str)+'_'+train_test['addr2'].astype(str)\n    train_test['New_card1_card2_addr1_addr2']=train_test['card1'].astype(str)+'_'+train_test['card2'].astype(str)+'_'+train_test['addr1'].astype(str)+'_'+train_test['addr2'].astype(str)\n\n    train_test['New_P_emaildomain_addr1'] = train_test['P_emaildomain'] + '_' + train_test['addr1'].astype(str)\n    train_test['New_R_emaildomain_addr2'] = train_test['R_emaildomain'] + '_' + train_test['addr2'].astype(str)\n    \n    #Aggregation features\n    train_test['New_TransactionAmt_to_mean_card1'] = train_test['TransactionAmt'] \/ train_test.groupby(['card1'])['TransactionAmt'].transform('mean')\n    train_test['New_TransactionAmt_to_mean_card4'] = train_test['TransactionAmt'] \/ train_test.groupby(['card4'])['TransactionAmt'].transform('mean')\n    train_test['New_TransactionAmt_to_std_card1'] = train_test['TransactionAmt'] \/ train_test.groupby(['card1'])['TransactionAmt'].transform('std')\n    train_test['TransactionAmt_to_std_card4'] = train_test['TransactionAmt'] \/ train_test.groupby(['card4'])['TransactionAmt'].transform('std')\n\n    train_test['New_id_02_to_mean_card1'] = train_test['id_02'] \/ train_test.groupby(['card1'])['id_02'].transform('mean')\n    train_test['New_id_02_to_mean_card4'] = train_test['id_02'] \/ train_test.groupby(['card4'])['id_02'].transform('mean')\n    train_test['New_id_02_to_std_card1'] = train_test['id_02'] \/ train_test.groupby(['card1'])['id_02'].transform('std')\n    train_test['New_id_02_to_std_card4'] = train_test['id_02'] \/ train_test.groupby(['card4'])['id_02'].transform('std')\n\n    train_test['New_D15_to_mean_card1'] = train_test['D15'] \/ train_test.groupby(['card1'])['D15'].transform('mean')\n    train_test['New_D15_to_mean_card4'] = train_test['D15'] \/ train_test.groupby(['card4'])['D15'].transform('mean')\n    train_test['New_D15_to_std_card1'] = train_test['D15'] \/ train_test.groupby(['card1'])['D15'].transform('std')\n    train_test['New_D15_to_std_card4'] = train_test['D15'] \/ train_test.groupby(['card4'])['D15'].transform('std')\n\n    train_test['New_D15_to_mean_addr1'] = train_test['D15'] \/ train_test.groupby(['addr1'])['D15'].transform('mean')\n    train_test['New_D15_to_mean_card4'] = train_test['D15'] \/ train_test.groupby(['card4'])['D15'].transform('mean')\n    train_test['New_D15_to_std_addr1'] = train_test['D15'] \/ train_test.groupby(['addr1'])['D15'].transform('std')\n    train_test['New_D15_to_std_card4'] = train_test['D15'] \/ train_test.groupby(['card4'])['D15'].transform('std')\n    \n    drop_col_list.extend(card_cols)\n    \n    # Frequency Encoding \n    fe_col_list=[\"New_TransactionAmt_Bin\",'card4','card6','P_emaildomain','R_emaildomain','DeviceType','DeviceInfo']+[c for c in train_test if c[0] == 'M']\n    for col in fe_col_list:\n        vc = train_test[col].value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = 'New_'+col+'_FE'\n        train_test[nm] = train_test[col].map(vc)\n        train_test[nm] = train_test[nm].astype('float32')\n\n    \n    print('DROPING UNNECESSARY FEATURES')\n    train_test=train_test.drop(drop_col_list, axis=1)\n    \n    print('APPLYING LABEL ENCODING TO CATEGORICAL FEATURES')\n    for col in train_test.columns:\n        if train_test[col].dtype == 'object':\n            le = LabelEncoder()\n            le.fit(list(train_test[col].astype(str).values))\n            train_test[col] = le.transform(list(train_test[col].astype(str).values))\n    \n    print('REDUCING MEMORY USAGE')\n    train_test = reduce_mem_usage(train_test)\n    \n    print('DATA IS READY TO MODELLING')\n    \n    return train_test","9d09e897":"def modeling(train_test,target):\n\n    train = train_test[train_test[target].notnull()]\n    test = train_test[train_test[target].isnull()]\n\n    folds = KFold(n_splits = 10, shuffle = True, random_state = 1001)\n\n    oof_preds = np.zeros(train.shape[0])\n    sub_preds = np.zeros(test.shape[0])\n    \n    feature_importance_df = pd.DataFrame()\n\n    features = [f for f in train.columns if f not in [target,'TransactionID','New_TransactionAmt_Bin','NewDate']]\n\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train[features], train[target])):\n        \n        start_time = time.time()\n        print('Training on fold {}'.format(n_fold + 1))\n\n        X_train, y_train = train[features].iloc[train_idx], train[target].iloc[train_idx]\n\n        X_valid, y_valid = train[features].iloc[valid_idx], train[target].iloc[valid_idx]\n        \n        params={'learning_rate': 0.01,\n        'objective': 'binary',\n        'metric': 'auc',\n        'num_threads': -1,\n        'num_leaves': 256,\n        'verbose': 1,\n        'random_state': 42,\n        'bagging_fraction': 1,\n        'feature_fraction': 0.85 }\n       \n        clf = LGBMClassifier(**params, n_estimators=1000) #categorical_feature = LGBM_cat_col_list\n\n        clf.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_valid, y_valid)], \n                eval_metric = 'auc', verbose = 200, early_stopping_rounds = 200)\n\n        #y_pred_valid\n        oof_preds[valid_idx] = clf.predict_proba(X_valid, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test[features], num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = features\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(y_valid, oof_preds[valid_idx]))) \n\n\n    print('Full AUC score %.6f' % roc_auc_score(train[target], oof_preds)) #y_pred_valid   \n\n    test[target] = sub_preds\n    test[['TransactionID', target]].to_csv(\"submission_lightgbm2.csv\", index= False)\n\n    display_importances(feature_importance_df)\n    \n    return feature_importance_df","018766bc":"def main():\n    with timer(\"Loading Data\"):\n        train_test = loading_data()\n    \n    with timer(\"Preprocessing Data\"):\n        train_test = processing_data(train_test)\n        \n    with timer(\"Modeling\"):\n        feat_importance = modeling(train_test ,'isFraud')","33b2808c":"if __name__ == \"__main__\":\n    with timer(\"Full model run\"):\n        main()","2edd4ee6":"## Exploring Target Features - isFraud\n","f38036c8":"### Context Manager","295830ef":"### D1-D15\n\n* The D Columns are \"time deltas\" from some point in the past.\n","4b68ac8d":"### P-emaildomain\n\n* I will group all e-mail domains by the respective enterprises.\n* Also, I will set as \"Others\" all values with less than 500 entries\n","5c86b743":"### P-emaildomain & R-emaildomain\n\n* We can see a very similar distribution in both email domain features.\n* We have high values in google and icloud frauds.\n","98975927":"### id1-id38\n\n* id1-id11 are numeric features\n* id12-id38 are categorical features.\n","568c366b":"# **EEE-CIS Fraud Detection** #\n\n* The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features.\n\n### What is Fraud Detection?\n\n* Fraud detection protects person information, assets, accounts and transactions through the real-time, near-real-time analysis of activities by users and other defined entities. It uses background server-based processes that examine users\u2019 and other defined entities\u2019 access and behavior patterns, and typically compares this information to a profile of what\u2019s expected.\n\n### Data Description\n\n* TransactionDT: Timedelta from a given reference datetime (not an actual timestamp)\n* TransactionAMT: Transaction payment amount in USD\n* ProductCD: Product code, the product for each transaction\n* card1 - card6: Payment card information, such as card type, card category, issue bank, country, etc.\n* addr: Address\n* dist: Distance\n* P_ and R_emaildomain: Purchaser and recipient email domain\n* C1-C14: Counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n* D1-D15: Timedelta, such as days between previous transaction, etc.\n* M1-M9: mMatch, such as names on card and address, etc.\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\n* Categorical Features: ProductCD, card1 - card6, addr1, addr2, P_emaildomain, R_emaildomain, M1 - M9\n\n### Identity Table\n\n* Variables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions. They're collected by Vesta\u2019s fraud protection system and digital security partners. (The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n* Categorical Features: DeviceType, DeviceInfo, id_12 - id_38","cf219d7a":"### V1-V339\n\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n* I will group the v features that have a similar number of nan observations.","999622d4":"## Exploring Continuous Features\n","529b1ef5":"### M1-M9\n\n* M1-M9 : match, such as names on card and address, etc.\n* All of the M features are categorical.\n* Values are T F or NaN except M4.\n* M4 feature appears to be different from others.\n","d628511c":"#### TransactionDT\n* TransactionDT : is a timedelta from a given reference datetime (not an actual timestamp).\n* TransactionDT is one of the features that can cause problems.\n* It seems as if there is a time difference between testing and train operations.","9301abc8":"### DeviceInfo\n\n* Provides information about device names.\n","25bff782":"### C1-C14\n\n* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is 0masked.\n* All of the C features are continuous.\n","e60a0418":"##### Feature Extraction Taking the start date \u20182015-04-22\u2019, constructed time variables. In discussions tab you should read an excellent solutions.","62be2ef6":"### ProductCD\n\n* W, C and R are the most frequent values.\n* 75.45% of observations belong to product W.\n* 1.97% of observations belong to product S.\n* Approximately 12% of transactions with product C are fraudulent.\n* Approximately 2% of transactions with product W are fraudulent.\n","bbb22565":"# Importing Libraries","713facd5":"## Exploring Categorical Features\n\nIn this section, I will examine the effect of categorical variables on fraud.\n\nAlthough some of the group features are categorical(like M, card, id_), I will examine them in a diffrent section.\n\n### Categorical Features\n* ProductCD\n* addr1 & addr2\n* P_emaildomain & R_emaildomain\n* DeviceType\n* DeviceInfo\n","01643a26":"#### dist1 & dist2\n* Perhaps this could be the distance of the transaction vs. the card owner's home\/work address.","599d769a":"# Exploratory Data Analysis (EDA)","6778f1ec":"#### TransactionAmt\n\n* TransactionAmt : The ammount of transaction.\n* I apply log transform in order to better show the distribution of data. Otherwise very large transactions skew the distribution.\n* The mean of the fraud transaction amount is larger than the mean of non - fraud transaction amount.\n* And also , the lowest and highest transaction amounts seem to be more likely to be fraudulent transactions.\n","ec368854":"# Reduce Memory Usage\n","78fcbc29":"# MODEL TUNING","2febaebe":"## Exploring Group Features (card, C, D, M, V, id )\n\n## card1-card6\n\n* The host of the competition stated that some of the features are categorical even if they look numerical like card features.\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n* card4 and card6 have 4 unique values, and the others more than 100\n* Except card1, card features have nan values \u200b\u200bso I will group them according to card1 and fill with the most common value.\n\n","204d1449":"### R-emaildomain\n\n* I will group all e-mail domains by the respective enterprises.\n* I will set as \"Others\" all values with less than 300 entries.\n","00014af4":"### DeviceType\n\n* Most of the fraudulent transactions were done by the mobile device.\n\n","0483a04e":"### addr1 - addr2\n\n* The host of the competition stated that these features are categorical even if they look numerical.\n","369fe1ed":"# Loading Data\n"}}