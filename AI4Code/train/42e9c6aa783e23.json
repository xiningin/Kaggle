{"cell_type":{"a1c3df87":"code","0bc2981a":"code","ce3b9244":"code","5ba535a2":"code","0e99543a":"code","b8f39aae":"code","684cc8dd":"code","119ae642":"code","58a7d678":"code","863cd258":"code","17717286":"code","d35bbb9d":"code","8a78e4d3":"code","87fc59f9":"code","9900a493":"code","72d1b57d":"code","db944741":"code","f5c2ba40":"code","d56bfd6c":"code","77af19be":"code","3c813da9":"code","459c8d47":"code","a250b33c":"code","22104ebf":"code","6355cf64":"code","d21338f2":"code","1353adc0":"code","6379b178":"code","d1e22280":"code","bf249313":"code","5e8be415":"code","bde64f98":"code","d25d590f":"code","b1449b0c":"code","fb02cbd6":"code","9d6fa02a":"code","86703f23":"code","7b587b97":"code","257ab13f":"code","e234aeb2":"code","415a4834":"code","8bfa8f9a":"code","5b150165":"code","250f4952":"code","c7dde9cf":"code","1d9ecf99":"code","51fa00cd":"code","da30d6b3":"code","209e6399":"code","4e87cec0":"code","e681adfd":"code","27db9d7b":"code","139e0288":"code","15dec5d7":"code","6cd362a0":"code","a120c9f5":"code","5a04371e":"code","875c5f7e":"code","692c53f9":"code","fbe0bb37":"code","820aea16":"code","03bf35e6":"code","c8ad9cc8":"code","a1a167d5":"code","e61420d6":"code","c9a6295c":"markdown","7b290f67":"markdown","0acd7c30":"markdown","7165ccc1":"markdown","8fb521aa":"markdown","6fc29651":"markdown","a5d58368":"markdown","e29bdebe":"markdown","02099151":"markdown","e1d3add9":"markdown","ee46acbf":"markdown","4e4a366d":"markdown","a7dcb7cf":"markdown","a54a45ab":"markdown","1e69fb01":"markdown","377eee7b":"markdown","91689ec9":"markdown","54adb057":"markdown","8ae543f6":"markdown","10903271":"markdown","3ebefb2b":"markdown","05793c02":"markdown","2b0b9cf3":"markdown","7726bf17":"markdown","b2852382":"markdown","d64862ca":"markdown","ccca8700":"markdown","1af027e2":"markdown","a1e0316c":"markdown"},"source":{"a1c3df87":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nimport time\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Configs\npd.options.display.float_format = '{:,.3f}'.format\nsns.set(style=\"whitegrid\")\nplt.style.use('seaborn')\nseed = 42\nnp.random.seed(seed)","0bc2981a":"sns.set(style=\"whitegrid\")\nplt.style.use('seaborn')","ce3b9244":"file_path = '\/kaggle\/input\/used-cars-price-prediction\/train-data.csv'\ndf_train = pd.read_csv(file_path)\nprint(\"Train DataSet = {} rows and {} columns\".format(df_train.shape[0], df_train.shape[1]), '\\n')\n\nquantitative = [f for f in df_train.columns if df_train.dtypes[f] != 'object']\n\nqualitative = [f for f in df_train.columns if df_train.dtypes[f] == 'object']\n\nprint(\"Qualitative Variables: (Numerics)\", \"\\n=>\", qualitative,\n      \"\\n\\nQuantitative Variable: (Strings)\\n=>\", quantitative)\n\ndf_train.head()","5ba535a2":"file_path = '\/kaggle\/input\/used-cars-price-prediction\/test-data.csv'\ndf_test = pd.read_csv(file_path)\nprint(\"Test DataSet = {} rows and {} columns\".format(df_test.shape[0], df_test.shape[1]))","0e99543a":"df_all = pd.concat([df_train, df_test]).reset_index(drop=True)\nprint(df_all.shape)\ndf_all.head()","b8f39aae":"def eda_categ_feat_desc_df(series_categorical):\n    \"\"\"Generate DataFrame with quantity and percentage of categorical series\n    @series_categorical = categorical series\n    \"\"\"\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    return val_concat","684cc8dd":"def eda_categ_feat_desc_plot(series_categorical, title = \"\"):\n    \"\"\"Generate 2 plots: barplot with quantity and pieplot with percentage. \n       @series_categorical: categorical series\n       @title: optional\n    \"\"\"\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    \n    fig, ax = plt.subplots(figsize = (15,4), ncols=2, nrows=1) # figsize = (width, height)\n    if(title != \"\"):\n        fig.suptitle(title, fontsize=18)\n        fig.subplots_adjust(top=0.8)\n\n    s = sns.barplot(x=series_name, y='quantity', data=val_concat, ax=ax[0])\n    for index, row in val_concat.iterrows():\n        s.text(row.name, row['quantity'], row['quantity'], color='black', ha=\"center\")\n\n    s2 = val_concat.plot.pie(y='percentage', autopct=lambda value: '{:.2f}%'.format(value),\n                             labels=val_concat[series_name].tolist(), legend=None, ax=ax[1],\n                             title=\"Percentage Plot\")\n\n    ax[1].set_ylabel('')\n    ax[0].set_title('Quantity Plot')\n\n    plt.show()","119ae642":"def eda_numerical_feat(series, title=\"\", number_format=\"\", with_label=True):\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 4), sharex=False)\n#     print(series.describe())\n    if(title != \"\"):\n        f.suptitle(title, fontsize=18)\n    sns.distplot(series, ax=ax1, rug=True)\n    sns.boxplot(series, ax=ax2)\n    ax1.set_title(\"distplot\")\n    ax2.set_title(\"boxplot\")\n    if(with_label):\n        describe = series.describe()\n        labels = { 'min': describe.loc['min'], 'max': describe.loc['max'], \n              'Q1': describe.loc['25%'], 'Q2': describe.loc['50%'],\n              'Q3': describe.loc['75%']}\n        if(number_format != \"\"):\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + number_format.format(v), ha='center', va='center', fontweight='bold',\n                         size=10, color='white', bbox=dict(facecolor='#445A64'))\n        else:\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + str(v), ha='center', va='center', fontweight='bold',\n                     size=10, color='white', bbox=dict(facecolor='#445A64'))\n    plt.show()","58a7d678":"def plot_model_score_regression(models_name_list, model_score_list, title=''):\n    fig = plt.figure(figsize=(15, 6))\n    ax = sns.pointplot( x = models_name_list, y = model_score_list, \n        markers=['o'], linestyles=['-'])\n    for i, score in enumerate(model_score_list):\n        ax.text(i, score + 0.002, '{:.4f}'.format(score),\n                horizontalalignment='left', size='large', \n                color='black', weight='semibold')\n    plt.ylabel('Score', size=20, labelpad=12)\n    plt.xlabel('Model', size=20, labelpad=12)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.xticks(rotation=70)\n    plt.title(title, size=20)\n    plt.show()","863cd258":"def describe_y_by_x_cat_boxplot(dtf, x_feat, y_target, title='', figsize=(15,5), rotatioon_degree=0):\n    the_title = title if title != '' else '{} by {}'.format(y_target, x_feat)\n    fig, ax1 = plt.subplots(figsize = figsize)\n    sns.boxplot(x=x_feat, y=y_target, data=df_train, ax=ax1)\n    ax1.set_title(the_title, fontsize=18)\n    plt.xticks(rotation=rotatioon_degree) # recomend 70\n    plt.show()","17717286":"df_missing = pd.concat([df_train.isnull().sum(), df_test.isnull().sum()], axis=1)\ndf_missing = df_missing.rename({0: 'train', 1: 'test'}, axis = 1).fillna(0)\ndf_missing['test'] = df_missing['test'].astype(int)\ndf_missing.T","d35bbb9d":"df_train.drop(['Unnamed: 0'], axis=1, inplace=True)\ndf_train.drop(['New_Price'],  axis=1, inplace=True)","8a78e4d3":"# get numeric part\nfor i in range(df_train.shape[0]):\n    try:\n        df_train.at[i, 'Name']        = df_train['Name'][i].split()[0] # Get First Name\n        df_train.at[i, 'Mileage'] = df_train['Mileage'][i].split()[0] # Get Number\n        df_train.at[i, 'Engine']     = df_train['Engine'][i].split()[0] # Get Number\n        df_train.at[i, 'Power']     = df_train['Power'][i].split()[0] # Get Number\n    except:\n        pass # is nan or string\n\n# # Engine :: Example: 998 CC\nreplace_engine = {'1798 CC': '1798', '72 CC': '72'}\ndf_train['Engine'] = df_train['Engine'].replace(replace_engine).astype(float)\nmedian_engine = df_train['Engine'].median()\ndf_train['Engine'] = df_train['Engine'].fillna(median_engine)\n\n# # Mileage :: Example: 26.6 km\/kg || 19.67 kmpl\ndf_train['Mileage'] = df_train['Mileage'].astype(float)\ndf_train['Mileage'] = df_train['Mileage'].fillna( df_train['Mileage'].median() )\n\n# # Power ::  Example: 58.16 bhp\nreplace_power = {'nan': '-1', 'null': '-1', '73 bhp': '-1', '41 bhp': '-1'}\ndf_train['Power'] = df_train['Power'].replace(replace_power)\nmedian_power = df_train['Power'].median()\ndf_train['Power'] = df_train['Power'].replace({-1: median_power}).fillna(median_power).astype(float)\ndf_train['Power'] = df_train['Power'].replace({-1: median_power})\n\n# # Seats :: float WARNING: There Are Seat=0 (Error)\nmedian_seats = df_train['Seats'].median()\ndf_train['Seats'] = df_train['Seats'].replace({0.0: median_seats}).fillna(median_seats).astype(float)\n\ndf_train.isnull().sum()","87fc59f9":"# After data cleaning\ndf_train.head()","9900a493":"# % by Name in DataSet, is 30 Names: Maruti: 20%, Hyundai: 10%, Toyota: 6% .....\neda_categ_feat_desc_df(df_train['Name']).T","72d1b57d":"# well distributed among locations\neda_categ_feat_desc_plot(df_train['Location'], 'Location')","db944741":"# Seats: 84% is 5; 11% is 7; the others together are 5%\neda_categ_feat_desc_df(df_train['Seats'])   ","f5c2ba40":"eda_categ_feat_desc_plot(df_train['Owner_Type'], 'Owner_Type')","d56bfd6c":"eda_numerical_feat(df_train['Kilometers_Driven'], 'Kilometers_Driven distribution')","77af19be":"# remove row with outiliers\ndf_train.drop(  df_train[df_train['Kilometers_Driven'] > 300000 ].index, inplace = True)","3c813da9":"eda_numerical_feat(df_train['Kilometers_Driven'], 'Kilometers_Driven distribution')","459c8d47":"eda_numerical_feat(df_train['Year'], 'Year distribution')","a250b33c":"eda_categ_feat_desc_plot(df_train['Fuel_Type'], 'Fuel Type')","22104ebf":"eda_categ_feat_desc_plot(df_train['Transmission'], 'Transmission')","6355cf64":"eda_numerical_feat(df_train['Mileage'], 'Mileage distribution')","d21338f2":"eda_numerical_feat(df_train['Engine'], 'Engine distribution')","1353adc0":"eda_numerical_feat(df_train['Power'], 'Power distribution')","6379b178":"# Remove Outiliers Rows with Price > 100\ndf_train.drop(  df_train[df_train['Price'] > 100 ].index, inplace = True)\neda_numerical_feat(df_train['Price'], 'Price distribution')","d1e22280":"# The bigger the year tends to be the higher the price\ndescribe_y_by_x_cat_boxplot(df_train, 'Year', 'Price', figsize=(18,5))","bf249313":"describe_y_by_x_cat_boxplot(df_train, 'Name', 'Price', figsize=(20,8), rotatioon_degree=65)","5e8be415":"fig, (ax1) = plt.subplots(figsize = (20,7))\n\nsns.boxplot(x=\"Location\", y=\"Price\", data=df_train, ax=ax1)\nax1.set_title('Price by Location')\nplt.show()","bde64f98":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(figsize = (15,10), ncols=2, nrows=2, sharex=False, sharey=False)\nfont_size = 14\nfig.suptitle('Price by Fuel_Type', fontsize=18)\n\nsns.boxplot(x=\"Fuel_Type\", y=\"Price\", data=df_train, ax=ax1)\nsns.distplot(df_train[(df_train.Fuel_Type == 'CNG')][\"Price\"],ax=ax2, hist=False, label='CNG')\nsns.distplot(df_train[(df_train.Fuel_Type == 'Diesel')]['Price'],ax=ax2, hist=False, label='Diesel')\nsns.distplot(df_train[(df_train.Fuel_Type == 'Petrol')]['Price'],ax=ax2, hist=False, label='Petrol')\nsns.distplot(df_train[(df_train.Fuel_Type == 'LPG')]['Price'],ax=ax2, hist=False, label='LPG')\nsns.distplot(df_train[(df_train.Fuel_Type == 'Electric')]['Price'],ax=ax2, hist=False, label='Electric')\n\n# Only Diesel and Petrol, cuz LPG, Eletric and CNG has few rows\ndf_real_fuel = df_train[ (df_train.Fuel_Type == 'Diesel') | (df_train.Fuel_Type == 'Petrol') ]\nsns.boxplot(x=\"Fuel_Type\", y=\"Price\", data=df_real_fuel, ax=ax3)\nsns.distplot(df_train[(df_train.Fuel_Type == 'Diesel')]['Price'],ax=ax4,hist=False, label='Diesel')\nsns.distplot(df_train[(df_train.Fuel_Type == 'Petrol')]['Price'],ax=ax4,hist=False, label='Petrol')\n\nax1.set_title('Price by Fuel_Type: All Fuel Type', fontsize=font_size)\nax2.set_title('Distribution of Price for Fuel_Type: All', fontsize=font_size)\nax3.set_title('Price by Fuel_Type: Only Diesel and Petrol', fontsize=font_size)\nax3.set_title('Distribution of Price for Fuel_Type: : Only Diesel and Petrol', fontsize=font_size)\n\nplt.show()","d25d590f":"ax = sns.scatterplot(x=\"Kilometers_Driven\", y=\"Price\", data=df_train)\nax.set_title(\"Price by Kilometers_Driven\")\nplt.show()","b1449b0c":"ax = sns.scatterplot(x=\"Mileage\", y=\"Price\", data=df_train)\nax.set_title(\"Price by Mileage\")\nplt.show()","fb02cbd6":"ax = sns.scatterplot(x=\"Engine\", y=\"Price\", data=df_train)\nax.set_title(\"Price by Engine\")\nplt.show()","9d6fa02a":"ax = sns.scatterplot(x=\"Power\", y=\"Price\", data=df_train)\nax.set_title(\"Price by Power\")\nplt.show()","86703f23":"fig, (ax1) = plt.subplots(figsize = (10,7))\n\nsns.boxplot(x=\"Seats\", y=\"Price\", data=df_train, ax=ax1)\nax1.set_title('Price by Seats')\nplt.show()","7b587b97":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(figsize = (17,11), ncols=2, nrows=2, sharex=False, sharey=False)\n\nsns.scatterplot(x=\"Engine\",  y=\"Price\", hue=\"Transmission\", data=df_train, ax=ax1)\nsns.scatterplot(x=\"Mileage\", y=\"Price\", hue=\"Transmission\", data=df_train, ax=ax2)\nsns.scatterplot(x=\"Power\",   y=\"Price\", hue=\"Transmission\", data=df_train, ax=ax3)\nsns.scatterplot(x=\"Kilometers_Driven\", y=\"Price\", hue=\"Transmission\", data=df_train, ax=ax4)\n\nfig.suptitle('Price by some Numerical Features and Transmission', fontsize=18)\nplt.show()","257ab13f":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(figsize = (13,11), ncols=2, nrows=3)\n\nsns.scatterplot(x=\"Power\", y=\"Price\", hue=\"Transmission\", data=df_train, ax=ax1)\nax2.remove()\n\nsns.scatterplot(x=\"Power\", y=\"Price\", hue=\"Owner_Type\", data=df_train, ax=ax3)\ndf_train_re = df_train[ (df_train['Owner_Type'] == 'Fourth & Above') | (df_train['Owner_Type'] == 'Third') ]\nsns.scatterplot(x=\"Power\", y=\"Price\", hue=\"Owner_Type\", data=df_train_re, ax=ax4, palette='Set2')\n\nsns.scatterplot(x=\"Power\", y=\"Price\", hue=\"Fuel_Type\", data=df_train, ax=ax5)\ndf_train_re = df_train[ (df_train['Fuel_Type'] == 'Electric') | (df_train['Fuel_Type'] == 'LPG') | (df_train['Fuel_Type'] == 'CNG')]\nsns.scatterplot(x=\"Power\", y=\"Price\", hue=\"Fuel_Type\", data=df_train_re, palette='Set2', ax=ax6)\n\n# Config Titles\nfig.suptitle('Price by Power with others features', fontsize=18)\nax1.set_title(\"Price by Power and Transmission\")\nax3.set_title(\"Price by Power and Ower Type: All\")\nax4.set_title(\"Price by Power and Ower Type: 3\u00b0, 4\u00b0 and others\")\nax5.set_title(\"Price by Power and Fuel Type: All\")\nax6.set_title(\"Price by Power and Fuel Type: CNG, PLG and ELetric\")\n\n# padding betwen axis\nfig.tight_layout(pad=0.4)\n\n# Correct height of suptitle\nplt.subplots_adjust(top=0.92)\n\nplt.show()","e234aeb2":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(figsize = (13,11), ncols=2, nrows=3)\nax2.remove()\n\nsns.scatterplot(x=\"Kilometers_Driven\", y=\"Price\", hue=\"Transmission\", data=df_train, ax=ax1)\n\nsns.scatterplot(x=\"Kilometers_Driven\", y=\"Price\", hue=\"Owner_Type\", data=df_train, ax=ax3)\ndf_train_re = df_train[ (df_train['Owner_Type'] == 'Fourth & Above') | (df_train['Owner_Type'] == 'Third') ]\nsns.scatterplot(x=\"Kilometers_Driven\", y=\"Price\", hue=\"Owner_Type\", data=df_train_re, ax=ax4, palette='Set2')\n\nsns.scatterplot(x=\"Kilometers_Driven\", y=\"Price\", hue=\"Fuel_Type\", data=df_train, ax=ax5)\ndf_train_re = df_train[ (df_train['Fuel_Type'] == 'Electric') | (df_train['Fuel_Type'] == 'LPG') | (df_train['Fuel_Type'] == 'CNG')]\nsns.scatterplot(x=\"Kilometers_Driven\", y=\"Price\", hue=\"Fuel_Type\", data=df_train_re, palette='Set2', ax=ax6)\n\n# Config Titles\nfig.suptitle('Price by Kilometers Driven with others features', fontsize=18)\nax3.set_title(\"Price by Kilometers Driven and Ower Type: All\")\nax4.set_title(\"Price by Kilometers Driven and Ower Type: 3\u00b0, 4\u00b0 and others\")\nax5.set_title(\"Price by Kilometers Driven and Fuel Type: All\")\nax6.set_title(\"Price by Kilometers Driven and Fuel Type: CNG, PLG and ELetric\")\n\n# padding betwen axis\nfig.tight_layout(pad=0.4)\n\n# Correct height of suptitle\nplt.subplots_adjust(top=0.92)\n\nplt.show()","415a4834":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(figsize = (13,11), ncols=2, nrows=3)\n\nsns.scatterplot(x=\"Mileage\", y=\"Price\", hue=\"Transmission\", data=df_train, ax=ax1)\n\nsns.scatterplot(x=\"Mileage\", y=\"Price\", hue=\"Owner_Type\", data=df_train, ax=ax3)\ndf_train_re = df_train[ (df_train['Owner_Type'] == 'Fourth & Above') | (df_train['Owner_Type'] == 'Third') ]\nsns.scatterplot(x=\"Mileage\", y=\"Price\", hue=\"Owner_Type\", data=df_train_re, ax=ax4, palette='Set2')\n\nsns.scatterplot(x=\"Mileage\", y=\"Price\", hue=\"Fuel_Type\", data=df_train, ax=ax5)\ndf_train_re = df_train[ (df_train['Fuel_Type'] == 'Electric') | (df_train['Fuel_Type'] == 'LPG') | (df_train['Fuel_Type'] == 'CNG')]\nsns.scatterplot(x=\"Mileage\", y=\"Price\", hue=\"Fuel_Type\", data=df_train_re, palette='Set2', ax=ax6)\n\n# Config Titles\nfig.suptitle('Price by Mileage with others features', fontsize=18)\nax3.set_title(\"Price by Mileage and Ower Type: All\")\nax4.set_title(\"Price by Mileage and Ower Type: 3\u00b0, 4\u00b0 and others\")\nax5.set_title(\"Price by Mileage and Fuel Type: All\")\nax6.set_title(\"Price by Mileage and Fuel Type: CNG, PLG and ELetric\")\n\n# padding betwen axis\nfig.tight_layout(pad=0.4)\n\n# Correct height of suptitle\nplt.subplots_adjust(top=0.92)\n\nplt.show()","8bfa8f9a":"df_train.head()","5b150165":"# Name\ndf_final_train = df_train.drop(['Name'],axis=1)\n\n# Location\ndf_location = pd.get_dummies(df_train['Location'],drop_first=True)\n\n# Fuel Type\ndf_fuel_typ = pd.get_dummies(df_train['Fuel_Type'],drop_first=True)\n\n# Transmission\ndf_final_train['Transmission'].replace( {'Manual': 0, 'Automatic': 1}, inplace=True) \n\n# Owener_Type\ndf_final_train['Owner_Type'].replace( {'First': 1, 'Second': 2, \"Third\": 3,\"Fourth & Above\":4}, inplace=True) \n\ndf_final_train = pd.concat([df_final_train, df_location, df_fuel_typ],axis=1).drop(['Location', 'Fuel_Type'],axis=1)\ndf_final_train.head()","250f4952":"for i in range(df_test.shape[0]):\n    try:\n        df_test.at[i, 'Name']        = df_test['Name'][i].split()[0] # Get First Name\n        df_test.at[i, 'Mileage'] = df_test['Mileage'][i].split()[0] # Get Number\n        df_test.at[i, 'Engine']     = df_test['Engine'][i].split()[0] # Get Number\n        df_test.at[i, 'Power']     = df_test['Power'][i].split()[0] # Get Number\n    except:\n        pass # is nan or string\n\n# # Engine :: 998 CC\n\nreplace_engine = {'1798 CC': '1798', '72 CC': '72'}\ndf_test['Engine'] = df_test['Engine'].replace(replace_engine).astype(float)\nmedian_engine = df_test['Engine'].median()\ndf_test['Engine'] = df_test['Engine'].fillna(median_engine)\n\n# # Mileage :: 26.6 km\/kg || 19.67 kmpl\ndf_test['Mileage'] = df_test['Mileage'].astype(float)\ndf_test['Mileage'] = df_test['Mileage'].fillna( df_test['Mileage'].median() )\n\n# # Power :: 58.16 bhp\nreplace_power = {'nan': '-1', 'null': '-1', '73 bhp': '-1', '41 bhp': '-1'}\ndf_test['Power'] = df_test['Power'].replace(replace_power)\nmedian_power = df_test['Power'].median()\ndf_test['Power'] = df_test['Power'].replace({-1: median_power}).fillna(median_power).astype(float)\ndf_test['Power'] = df_test['Power'].replace({-1: median_power})\n\n# # Seats :: float WARNING: There Are Seat=0 (Error)\nmedian_seats = df_test['Seats'].median()\ndf_test['Seats'] = df_test['Seats'].replace({0.0: median_seats}).fillna(median_seats).astype(float)\n\ndf_test.isnull().sum()","c7dde9cf":"# Name\ndf_final_test = df_test.drop(['Name','New_Price','Unnamed: 0'],axis=1)\n\n# Location\ndf_location = pd.get_dummies(df_test['Location'],drop_first=True)\n\n# Fuel Type\ndf_fuel_typ = pd.get_dummies(df_test['Fuel_Type'],drop_first=True)\n\n# Transmission\ndf_final_test['Transmission'].replace( {'Manual': 0, 'Automatic': 1}, inplace=True) \n\n# Owener_Type\ndf_final_test['Owner_Type'].replace( {'First': 1, 'Second': 2, \"Third\": 3,\"Fourth & Above\":4}, inplace=True) \n\ndf_final_test = pd.concat([df_final_test, df_location, df_fuel_typ],axis=1).drop(['Location', 'Fuel_Type'],axis=1)\ndf_final_test.head()","1d9ecf99":"df_final_train.head()","51fa00cd":"corr_matrix = df_final_train.corr()\nf, ax1 = plt.subplots(figsize=(18, 14), sharex=False)\n\nax1.set_title('Top Corr to {}'.format('\"Price\"'))\ncols_top = corr_matrix.sort_values(by=\"Price\", ascending=False)['Price'].index\n\ncm = np.corrcoef(df_final_train[cols_top].values.T)\nmask = np.zeros_like(cm)\nmask[np.triu_indices_from(mask)] = True\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                 annot_kws={'size': 14}, yticklabels=cols_top.values,\n                 xticklabels=cols_top.values, mask=mask, ax=ax1)\n\n# see the leftmost column, it's the correlations ordered from highest to lowest for Price","da30d6b3":"from sklearn.model_selection import train_test_split\n\n\nX = df_final_train.loc[:, df_final_train.columns != 'Price'].values\nY = df_final_train['Price'].values\n\nx_features = df_final_train.loc[:, df_final_train.columns != 'Price'].columns.tolist()\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","209e6399":"from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, RobustScaler, scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import ElasticNet, LassoCV, BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, ElasticNetCV, LinearRegression\nfrom sklearn.kernel_ridge import KernelRidge\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.svm import SVR\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor","4e87cec0":"# Setup cross validation folds\nkf = KFold(n_splits=4, random_state=42, shuffle=True)\n\n# Define error metrics\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=x_train):\n    rmse = np.sqrt(-cross_val_score(model, X, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","e681adfd":"# Create ML Models\n\n# Light Gradient Boosting Regressor\nlightgb_model = LGBMRegressor(objective='regression',  num_leaves=6, learning_rate=0.01,  n_estimators=7000,\n                       max_bin=200,  bagging_fraction=0.8, bagging_freq=4,  bagging_seed=8,\n                       feature_fraction=0.2, feature_fraction_seed=8, min_sum_hessian_in_leaf = 11,\n                       verbose=-1, random_state=42)\n\n# XGBoost Regressor\nxgboost_model = XGBRegressor(learning_rate=0.01, n_estimators=6000, max_depth=4, min_child_weight=0,\n                       gamma=0.6, subsample=0.7, colsample_bytree=0.7, objective='reg:squarederror',\n                       nthread=-1, scale_pos_weight=1, seed=42, reg_alpha=0.00006, random_state=42)\n\n# Linear Regressor\nlinear_model = LinearRegression()\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, \n                1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge_model = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Lasso Regressor\nlasso_alphas2 = [5e-05, 0.0001, 0.0008, 0.01, 0.1, 1]\nlasso_model = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=lasso_alphas2,\n                              random_state=42, cv=kf))\n\n# Elastic Net Regressor\nelastic_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\nelastic_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nelasticnet_model = make_pipeline(RobustScaler(),  \n                           ElasticNetCV(max_iter=1e7, alphas=elastic_alphas,\n                                        cv=kf, l1_ratio=elastic_l1ratio))\n\n# Kernel Ridge\nkeridge_model = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n# Support Vector Regressor\nsvm_model = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngboost_model = GradientBoostingRegressor(n_estimators=6000, learning_rate=0.01, max_depth=4, max_features='sqrt', \n                                min_samples_leaf=15, min_samples_split=10, loss='huber', random_state=42)  \n\n# Random Forest Regressor\nrandomforest_model = RandomForestRegressor(n_estimators=1200, max_depth=15, min_samples_split=5, min_samples_leaf=5,\n                          max_features=None, oob_score=True, random_state=42)\n\n# Neural Net\nneuralnet_model = MLPRegressor()\n\n# Extra Tree Regressor\nextratree_model = ExtraTreesRegressor()","27db9d7b":"regressor_models = {\n    'Linear': linear_model,\n    'Ridge': ridge_model,\n    'Lasso': lasso_model,\n    'KernelRidge': keridge_model,\n    'ElasticNet': elasticnet_model,\n    'SVM': svm_model,\n    'RandomForest': randomforest_model,\n    'ExtraTree': extratree_model,\n    'NeuralNet': neuralnet_model,\n    'GBoost': gboost_model,\n    'LightGB': lightgb_model,\n    'XGBoost': xgboost_model,\n}","139e0288":"## Cross Validation\n\ncv_scores = {}\nt_start = time.time()\n\nfor model_name, model in regressor_models.items():\n    print('{:17}'.format(model_name), end='')\n    t0 = time.time()\n    score = cv_rmse(model)\n    m, s = score.mean(), score.std()\n    cv_scores[model_name] = [m,s]\n    print('| MSE in CV | rmse_mean: {:11,.3f}, | rmse_std: {:9,.3f}  | took: {:9,.3f} s |'.format(m,s, time.time() - t0))\n    \nprint('\\nTime total to CrossValidation: took {:9,.3f} s'.format(time.time() - t_start)) # 200s\n\n# Show Sorted Train Scores DataFrame\ndf_cv = pd.DataFrame(data = cv_scores.values(), columns=['rmse_cv', 'std_cv'], index=cv_scores.keys())\ndf_cv = df_cv.sort_values(by='rmse_cv').reset_index().rename({'index': 'model'}, axis=1)\ndf_cv","15dec5d7":"# Def Stack Model: Stack up some the models above, optimized using one ml model\nstack_regressors = (regressor_models['XGBoost'],\n                    regressor_models['GBoost'],\n                    regressor_models['LightGB'],\n                    regressor_models['RandomForest'],\n                    regressor_models['ExtraTree']\n                   )\n\nstack_model = StackingCVRegressor(regressors = stack_regressors,\n                                meta_regressor = regressor_models['XGBoost'],\n                                use_features_in_secondary=True)\n\nregressor_models['Stack'] = stack_model","6cd362a0":"train_scores = {}\nt_start = time.time()\n\nfor model_name, model in regressor_models.items():\n    print('{:14}'.format(model_name), end='')\n    t0 = time.time()\n    if(model_name == 'Stack'):\n        model  = model.fit(np.array(x_train), np.array(y_train))\n        y_pred = model.predict(np.array(x_train))\n    else:\n        model  = model.fit( x_train, y_train )\n        y_pred = model.predict(x_train)\n    r2, mse = r2_score(y_train, y_pred), mean_squared_error(y_train, y_pred)\n    train_scores[model_name] = [r2, mse, np.sqrt(mse)]\n    text_print = '| Train | r2: {:5,.3f} | mse: {:7,.3f}  | took: {:9,.3f} s |'\n    print(text_print.format(r2, mse, time.time() - t0))\n    regressor_models[model_name] = model\n    \nprint('\\nTime total to Fit Models: took {:9,.3f} s'.format(time.time() - t_start)) # UsedCars: 686 s = 11min","a120c9f5":"# Blend Model is use a porcentage of some models mixing\nclass BlendModel:\n    \n    @classmethod\n    def predict(self, X):\n        return ((0.10 * regressor_models['LightGB'].predict(X)) + \\\n            (0.10 * regressor_models['GBoost'].predict(X)) + \\\n            (0.15 * regressor_models['XGBoost'].predict(X)) + \\\n            (0.10 * regressor_models['LightGB'].predict(X)) + \\\n            (0.20 * regressor_models['RandomForest'].predict(X)) + \\\n            (0.35 * regressor_models['Stack'].predict(np.array(X))))\n\nregressor_models['BlendModel'] = BlendModel()\ny_pred = BlendModel.predict(x_train)\nr2, mse = r2_score(y_train, y_pred), mean_squared_error(y_train, y_pred)\ntrain_scores['BlendModel'] = [r2, mse, np.sqrt(mse)]\nprint('RMSE score on train data to Blend Model:\\n\\t=>', np.sqrt(mse))","5a04371e":"from mlens.ensemble import SuperLearner\n\n# create a list of base-models\ndef get_models_to_super_leaner():\n    models = list()\n    models.append(regressor_models['LightGB'])\n    models.append(regressor_models['GBoost'])\n    models.append(regressor_models['XGBoost'])\n    models.append(regressor_models['LightGB'])\n    models.append(regressor_models['ExtraTree'])\n    models.append(regressor_models['RandomForest'])\n    models.append(regressor_models['KernelRidge'])\n    return models\n\n# create the super learner\ndef get_super_learner(X):\n    ensemble = SuperLearner(scorer=rmse, folds=5, shuffle=True, sample_size=len(X))\n    # add base models\n    models = get_models_to_super_leaner()\n    ensemble.add(models)\n    # add the meta model\n    ensemble.add_meta(LinearRegression())\n    return ensemble\n\n# key to regressros models\nmodel_name = 'SuperLeaner'\n\n# create the super learner\nensemble = get_super_learner(x_train)\n# fit the super learner\nt0 = time.time()\nensemble.fit(x_train, np.array(y_train)) # UsedCars: took 565s = 10min\n# pred and evaluate in train dataset\ny_pred = ensemble.predict(x_train)\nr2, mse = r2_score(y_train, y_pred), mean_squared_error(y_train, y_pred)\ntrain_scores[model_name] = [r2, mse, np.sqrt(mse)]\n# show results\ntext_print = '| Super Leaner in Train | r2: {:6,.3f}, | mse: {:9,.3f}  | took: {:9,.3f} s |\\n'\nprint(text_print.format(r2, mse, time.time() - t0))\n# set in dict regressors\nregressor_models[model_name] = ensemble\n# summarize base learners\nprint(ensemble.data)\n# evaluate meta model","875c5f7e":"# Show train_scores dataframe\ndf_train_scores = pd.DataFrame(data = train_scores.values(),index=train_scores.keys(), columns=['r2_train', 'mse_train', 'rmse_train'])\ndf_train_scores = df_train_scores.sort_values(by='r2_train', ascending=False).reset_index().rename({'index': 'model'}, axis=1)\ndf_train_scores","692c53f9":"test_scores = {}\n\n# predcit x_test to y_test and compare\nfor model_name, model in regressor_models.items():\n    if(model_name == 'Stack'):\n        y_pred = model.predict(np.array(x_test))\n    else:\n        y_pred = model.predict(x_test)\n    r2, mse = r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred)\n    test_scores[model_name] = [r2, mse, np.sqrt(mse)]\n    \n# Sort DF test scores\ndf_test_scores = pd.DataFrame(data = test_scores.values(), columns=['r2_test', 'mse_test', 'rmse_test'], index=test_scores.keys())\ndf_test_scores = df_test_scores.sort_values(by='r2_test', ascending=False).reset_index().rename({'index': 'model'}, axis=1)\ndf_test_scores","fbe0bb37":"# Include Blend in Train Scores\ndf_train_scores = pd.DataFrame(data = train_scores.values(),index=train_scores.keys(), columns=['r2_train', 'mse_train', 'rmse_train'])\ndf_train_scores = df_train_scores.sort_values(by='r2_train', ascending=False).reset_index().rename({'index': 'model'}, axis=1)\n\n# df_test_scores\ndf_cv2 = df_cv.merge(df_train_scores, how='right'  ,left_on='model', right_on='model')\ndf_final_scores = df_cv2.merge(df_test_scores, how='right' ,left_on='model', right_on='model')\n\ndf_final_scores.sort_values(by='mse_test')","820aea16":"plot_model_score_regression(list(test_scores.keys()), [r2 for r2, mse, rmse in test_scores.values()], 'Evaluate Models in Test: R2')","03bf35e6":"# To one of best models: GBoost\n\nplt.figure(figsize = (12,4))\nfeat_importances = pd.Series(regressor_models['XGBoost'].feature_importances_)#, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","c8ad9cc8":"from yellowbrick.model_selection import FeatureImportances, RFECV\n\n# FeatureImportances and RFECV to a good model, if put GBoost(the best) take a long time\n\nfig, (ax3,ax4) = plt.subplots(figsize = (15,5), ncols=2, sharex=False, sharey=False)\n\nthe_model = 'Linear'\nt_start = time.time()\n\nviz3 = FeatureImportances(regressor_models[the_model], ax=ax3, relative=False)\nviz3.fit(x_train, y_train)\nviz3.finalize()\n\nviz4 = RFECV(regressor_models[the_model], ax=ax4)\nviz4.fit(x_train, y_train)\nviz4.finalize()\n\nprint('Time total to RFECV to {} : took {:9,.3f} s'.format(the_model, time.time() - t_start))\n\nplt.show()","a1a167d5":"from yellowbrick.regressor import ResidualsPlot, PredictionError\nfrom yellowbrick.model_selection import FeatureImportances, RFECV\n\n# Can't use 'SuperLeaner' than, use the second place: GBoost\n\nfig, (ax1, ax2) = plt.subplots(figsize = (15,5), ncols=2)\n\nviz1 = ResidualsPlot(regressor_models['XGBoost'], ax=ax1)\nviz1.score(x_test, y_test)\nviz1.finalize()\n\nviz2 = PredictionError(regressor_models['XGBoost'], ax=ax2)\nviz2.score(x_test, y_test)  \nviz2.finalize()\n\nplt.show()","e61420d6":"from sklearn.metrics import mean_absolute_error, mean_squared_log_error\n\ny_pred = regressor_models['XGBoost'].predict(x_test)\nprint('The best Regressor Model to Test DataSet:')\nprint('MAE : {:14,.3f}'.format(mean_absolute_error(y_pred, y_test)))\nprint('MSE : {:14,.3f}'.format(mean_squared_error(y_pred, y_test)))\nprint('RMSE: {:14,.3f}'.format(np.sqrt(mean_squared_error(y_pred, y_test))))\nprint('R2  : {:14,.3f}'.format(r2_score(y_pred, y_test)))","c9a6295c":"## Import Libs and DataSet <a id='index01'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","7b290f67":"## Feature Importance <a id='index15'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","0acd7c30":"## Data Cleaning <a id='index03'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","7165ccc1":"Mileage, Engine, Power and Seats has some missing value. 'New Price' has to but it not will be used.","8fb521aa":"### Cross Validation <a id='index11'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","6fc29651":"<span style='font-size: 15pt'>Prepare Test DataSet<\/span>","a5d58368":"<span style='font-size: 15pt'>Drop Useless Columns of each pollutant<\/span>","e29bdebe":"### Bests Models <a id='index14'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","02099151":"### Target by cross Features <a id='index06'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","e1d3add9":"## Conclusion <a id='index25'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","ee46acbf":"## Snippets <a id='index02'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","4e4a366d":"## Split in train and Test <a id='index09'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","a7dcb7cf":"<span style='font-size: 15pt'>Data Cleaning Process<\/span>\n\n+ Get numeric part of MIleage, Engine and Power\n+ To Name, get first word\n+ Replace missing value with  medin values","a54a45ab":"The better model was SuperLeaner with the results above.\n\n---\n\nThis Kernel is still under development. I would highly appreciate your feedback for improvement and, of course, if you like it, please upvote it!\n\nPlease Upvote (It motivates me)","1e69fb01":"### Prepare ML Models and Training <a id='index33'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","377eee7b":"### EDA Conclusions <a id='index50'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n**Target by Feaute**\n\n+ Year: The bigger the year the higher the price tends to be\n+ Name: some names value more than others\n+ Location: Coimbatore and Bangalore are priced higher than others\n+ Fuel_Type: Diesel is more expensive than Petrol, and the others have few examples to check better\n+ Kilometers_Driven: Little Influence\n+ Milege: Little Influence\n+ Engine: Linear Infleunce, the higher, the more expensive the price\n+ Power: Linear Influence, the higher, the more expensive the price\n+ Seats: Having two seats has a greater influence on the price than the others\n\n**Target by cross Features**\n\n+ Transmission: It has an influence on Power, from Power 200 there is only automatic transmission and has the highest prices. It occurs in a similar way to Engine. Analyzing 4 numerical features (Engine, Power, Mileage, Kilometres_Drive) we see that being automatic transmission means having a great price\n\n...","91689ec9":"## Test Models <a id='index13'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","54adb057":"<span style='font-size: 15pt'>Prepare Train DataSet<\/span>","8ae543f6":"## Evaluate Best Model <a id='index20'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","10903271":"<h1 align=\"center\"> User Cars Cost: EDA and Regression <\/h1>\n\n<img src=\"https:\/\/mystrongad.com\/MTS_MillerToyota\/MTS_Interactive\/Used\/Used-Car-Toyota.png\" width=\"50%\" \/>\n\nCreated: 2020-09-01\n\nLast updated: 2020-09-04\n\nKaggle Kernel made by \ud83d\ude80 <a href=\"https:\/\/www.kaggle.com\/rafanthx13\"> Rafael Morais de Assis<\/a>\n\n## References\n\nSome other kernels that helped me with this project\n\nhttps:\/\/www.kaggle.com\/iabhishekmaurya\/used-car-price-prediction","3ebefb2b":"## Develop Models <a id='index10'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","05793c02":"## Table Of Contents (TOC) <a id=\"top\"><\/a>\n\n+ [Import Libs and DataSet](#index01) \n+ [Snippets](#index02)\n+ [Data Cleaning](#index03)\n+ [EDA](#index04)\n  - [Each feature Individually](#index04)\n  - [Target by Features](#index05)\n  - [Target by cross Features](#index06)\n  - [EDA conclusions](#index50)\n+ [Pre-Processing](#index07)\n+ [Correlations](#index08)\n+ [Split in train and Test](#index09)\n+ [Develop Models](#index10)\n  - [Prepare ML Models and Training](#index33)\n  - [Cross Validation](#index11)\n  - [Fit Models](#index12)\n  - [Test Models](#index13)\n  - [Bests Models](#index14)\n+ [Feature Importance](#index15)\n+ [Evaluate Best Model to Regression](#index20)\n+ [Conclusion](#index25)\n","2b0b9cf3":"## Correlations <a id='index08'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","7726bf17":"## EDA <a id='index04'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","b2852382":"### Target by Features <a id='index05'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","d64862ca":"## Pre-Processing <a id='index07'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","ccca8700":"### Each feature Individually","1af027e2":"## Fit Models <a id='index12'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","a1e0316c":"## Problem Description\n\nhttps:\/\/www.kaggle.com\/avikasliwal\/used-cars-price-prediction\/\n\n### Data Description\n\nDataSet of Used Cars, features and yout price\n\n### The Goal\n\nPredict price of used cars according to some characteristics"}}