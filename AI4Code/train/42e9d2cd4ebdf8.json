{"cell_type":{"30e9b321":"code","1c984eac":"code","afdb2660":"code","9c71071a":"code","82c18726":"code","8aa845d7":"code","82bfa145":"code","b1a53d49":"code","4558cf17":"code","cf7a4a17":"code","b242136c":"code","6e2a37cb":"code","acf6e4a4":"code","0370bd4f":"code","52c9c24b":"code","8aa69208":"code","6ca22a98":"code","01a01e36":"code","30a69071":"code","5fb8598b":"code","ac37e787":"code","1e39299b":"code","0a899dce":"code","e5af66a5":"code","a0491b58":"code","11b44d1b":"markdown","2fdd1d79":"markdown","794e1e26":"markdown"},"source":{"30e9b321":"import cupy as cp\nimport cudf\nimport cuml\nimport glob\nfrom tqdm import tqdm\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt","1c984eac":"PATH = \"\/kaggle\/input\/optiver-realized-volatility-prediction\"\n\n\ndef load_data(mode, path=\"\/kaggle\/input\/optiver-realized-volatility-prediction\"):\n    # mode = \"train\"\/\"test\"\n    file_name = f'{path}\/{mode}.csv'\n    return cudf.read_csv(file_name)\n\ndev_df = load_data(\"train\", path=PATH)\ndev_df.head()","afdb2660":"SCALE = 100\ndev_df[\"target\"] = SCALE*dev_df[\"target\"]\n\nstock_ids = dev_df[\"stock_id\"].unique()\nlen(stock_ids)","9c71071a":"order_book_training = glob.glob(f'{PATH}\/book_train.parquet\/*\/*')\norder_book_test = glob.glob(f'{PATH}\/book_test.parquet\/*\/*')\n\nlen(order_book_training), len(order_book_test)","82c18726":"trades_training = glob.glob(f'{PATH}\/trade_train.parquet\/*\/*')\ntrades_test = glob.glob(f'{PATH}\/trade_test.parquet\/*\/*')\n\nlen(trades_training), len(trades_test) ","8aa845d7":"%cd \/kaggle\/input\/rapids-kaggle-utils\/","82bfa145":"import cu_utils.transform as cutran\n\n\n\ndef log_diff(df, in_col, null_val):\n    df[\"logx\"] = df[in_col].log()\n    df[\"logx_shifted\"] = (df[[\"time_id\", \"logx\"]].groupby(\"time_id\", method='cudf')\n                             .apply_grouped(cutran.get_cu_shift_transform(shift_by=1, null_val=null_val),\n                                            incols={\"logx\": 'x'},\n                                            outcols=dict(y_out=cp.float32),\n                                            tpb=32)[\"y_out\"])\n    df[\"keep_row\"] = df[f\"logx_shifted\"] != null_val\n    return df[\"logx\"] - df[\"logx_shifted\"]\n\ndef realized_vol(log_return):\n    return cp.sqrt((log_return*log_return).sum())\n\ndef extract_raw_book_features(df, null_val=-9999):\n    df['wap1']=(df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    df['wap2']=(df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])     \n    df['wap3']=(df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    df['wap4']=(df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])                                                                                  \n    for n in [1,2,3,4]:\n        df[f\"log_return{n}\"] = log_diff(df, in_col=f\"wap{n}\", null_val=null_val)\n        df[f\"realized_vol{n}\"] = df[f\"log_return{n}\"]**2    \n    df['wap_balance'] = df['wap1'] - df['wap2']\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread1'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    df = df[df[\"keep_row\"]]\n    return df\n\n\ndef extract_raw_trade_features(df, null_val=-9999):\n    df[\"realized_vol_trade\"] = log_diff(df, in_col=f\"price\", null_val=null_val)**2\n    df[\"amount\"] = df['price']*df['size']\n    df = df[df[\"keep_row\"]]\n    return df\n\n\ndef agg(df, feature_dict):\n    agg_df = df.groupby(\"time_id\").agg(feature_dict).reset_index()\n    def f(x):\n        if x[1] == \"\":\n            return x[0]\n        return x[0] + \"_\" + x[1]\n    \n    agg_df.columns = [f(x) for x in agg_df.columns]\n    col_vol=[col for col in agg_df.columns if 'realized_vol' in col and ('mean' in col or 'sum' in col)]\n    agg_df[col_vol]=agg_df[col_vol].sqrt()\n    return agg_df    \n\n\ndef extract_book_stats(df):\n    feature_dict = {\n        'wap1': [\"sum\", \"std\"],\n        'wap2': [\"sum\", \"std\"],\n        'wap3': [\"sum\", \"std\"],\n        'wap4': [\"sum\", \"std\"],\n        'realized_vol1':[\"mean\"],\n        'realized_vol2':[\"mean\"],\n        'realized_vol3':[\"mean\"],\n        'realized_vol4':[\"mean\"],\n        'price_spread': [\"sum\", \"max\"],\n        'price_spread1': [\"sum\", \"max\"],\n        'wap_balance': [\"sum\", \"max\"],\n        'bid_spread': [\"sum\", \"max\"],\n        'ask_spread': [\"sum\", \"max\"],\n        'total_volume': [\"sum\", \"max\"],\n        'volume_imbalance': [\"sum\", \"max\"],\n        \"bid_ask_spread\":[\"sum\", \"max\"],\n    }\n    \n    return agg(df, feature_dict)\n    \ndef extract_book_stats_time(df):\n    feature_dict = {\n        'realized_vol1':[\"mean\"],\n        'realized_vol2':[\"mean\"],\n        'realized_vol3':[\"mean\"],\n        'realized_vol4':[\"mean\"],\n    }\n    \n    return agg(df, feature_dict)\n    \n    \ndef extract_trade_stats(df):\n    feature_dict = {\n        'realized_vol_trade': [\"mean\"],\n        'seconds_in_bucket':[\"nunique\"],\n        'size': [\"sum\",'max'],\n        'order_count': [\"sum\",'max'],\n        'amount':['sum','max'],\n    }\n    return agg(df, feature_dict)\n\ndef extract_trade_stats_time(df):\n    feature_dict = {\n        'realized_vol_trade': [\"mean\"],\n        'seconds_in_bucket':[\"nunique\"],\n        'size': [\"sum\"],\n        'amount':['sum'],\n        'order_count': [\"sum\"],\n    }\n    return agg(df, feature_dict)\n\ndef time_constraint_fe(df, stats_df, intervel_sec, fe_function, cols):\n    sub_df = df[(df[\"seconds_in_bucket\"] >= intervel_sec[0])&(df[\"seconds_in_bucket\"] <= intervel_sec[1])].reset_index(drop=True)\n    if sub_df.shape[0] > 0:\n        sub_stats = fe_function(sub_df)\n    else:\n        sub_stats = cudf.DataFrame(columns=cols)#\n    return stats_df.merge(sub_stats, on=\"time_id\", how=\"left\", suffixes=('', f'_{intervel_sec[0]}_{intervel_sec[1]}'))    \n    \n\ndef feature_engineering(book_path, trade_path):\n    book_df = cudf.read_parquet(book_path)\n    book_df = extract_raw_book_features(book_df)\n    book_stats = extract_book_stats(book_df)\n    #book_cols = book_stats.columns\n    book_cols_time=['realized_vol1_mean','realized_vol2_mean','realized_vol3_mean','realized_vol4_mean']+['time_id']\n    \n    trade_df = cudf.read_parquet(trade_path)\n    trade_df = extract_raw_trade_features(trade_df)\n    trade_stats = extract_trade_stats(trade_df)\n    #trade_cols = trade_stats.columns\n    trade_cols_time=['realized_vol_trade_mean','seconds_in_bucket_nunique','size_sum','order_count_sum','amount_sum']+['time_id']\n    for intervel_sec in [[0, 60],[60, 120],[120, 180],[180, 240],[240, 300],[300, 360],\n                        [360, 420],[420, 480],[480, 540],[540, 600]]:\n        book_stats = time_constraint_fe(book_df, book_stats, intervel_sec, extract_book_stats_time, book_cols_time) \n        trade_stats = time_constraint_fe(trade_df, trade_stats, intervel_sec, extract_trade_stats_time, trade_cols_time) \n    \n    return book_stats.merge(trade_stats, on=\"time_id\", how=\"left\")\n\n\ndef process_data(order_book_paths, trade_paths, stock_ids):\n    stock_dfs = []\n    for book_path, trade_path in tqdm(list(zip(order_book_paths, trade_paths))):\n        stock_id = int(book_path.split(\"=\")[1].split(\"\/\")[0])\n\n        df = feature_engineering(book_path, trade_path)\n        df[\"stock_id\"] = stock_id\n        stock_dfs.append(df)\n    return cudf.concat(stock_dfs)","b1a53d49":"train = process_data(order_book_training, trades_training, stock_ids)\ntest = process_data(order_book_test, trades_test, stock_ids)\ntrain.shape, test.shape","4558cf17":"def stock_time_fe(df):\n    cols = ['realized_vol1_mean', 'realized_vol2_mean', \n            #'realized_vol1_mean_180_600', 'realized_vol2_mean_180_600',\n            #'realized_vol1_mean_300_600', 'realized_vol2_mean_300_600', \n            #'realized_vol1_mean_420_600', 'realized_vol2_mean_420_600',\n           ]\n    tmp_df=df[~df[\"is_test\"]]\n    for agg_col in [\"stock_id\", \"time_id\"]:\n        for agg_func in [\"mean\", \"max\", \"std\", \"min\"]:\n            agg_df = tmp_df.groupby(agg_col)[cols].agg(agg_func)\n            agg_df.columns = [f\"{agg_col}_{agg_func}_{col}\" for col in agg_df.columns]\n            df = df.merge(agg_df.reset_index(), on=agg_col, how=\"left\")\n    \n    return df\n\ntrain[\"is_test\"] = False\ntest[\"is_test\"] = True\nall_df = train.append(test).reset_index(drop=True)\n\nall_df = stock_time_fe(all_df)\ntrain = all_df[~all_df[\"is_test\"]]\ntest = all_df[all_df[\"is_test\"]].to_pandas()\n","cf7a4a17":"train = dev_df.merge(train, on=[\"stock_id\", \"time_id\"], how=\"left\").to_pandas()\n\nnum_features = [col for col in list(train.columns)\n            if col not in {\"stock_id\", \"time_id\", \"target\", \"is_test\"}]\nlen(num_features)","b242136c":"end_list=[0, 60, 120, 180, 240, 300, 360, 420, 480, 540, 600]\nfor col_vol in ['realized_vol1_mean','realized_vol2_mean','realized_vol3_mean','realized_vol4_mean','realized_vol_trade_mean','seconds_in_bucket_nunique']:\n    for i in range(1,10):\n        k=10-i+1\n        N=k*(k+1)\/2\n        c=1\n        train[f'{col_vol}_{end_list[i-1]}_600']=train[f'{col_vol}_{end_list[i-1]}_{end_list[i]}']*c\/N\n        test[f'{col_vol}_{end_list[i-1]}_600']=test[f'{col_vol}_{end_list[i-1]}_{end_list[i]}']*c\/N\n        for j in range(i+1,11):\n            c+=1\n            train[f'{col_vol}_{end_list[i-1]}_600']+=train[f'{col_vol}_{end_list[j-1]}_{end_list[j]}']*c\/N\n            test[f'{col_vol}_{end_list[i-1]}_600']=test[f'{col_vol}_{end_list[i-1]}_{end_list[i]}']*c\/N\n  \n","6e2a37cb":"# replace by order sum (tau)\ntrain['size_tau'] = np.sqrt( 1\/ train['seconds_in_bucket_nunique'] )\ntest['size_tau'] = np.sqrt( 1\/ test['seconds_in_bucket_nunique'] )\n#train['size_tau_500'] = np.sqrt( 1\/ train['seconds_in_bucket_count_500'] )\n#test['size_tau_500'] = np.sqrt( 1\/ test['seconds_in_bucket_count_500'] )\ntrain['size_tau_420'] = np.sqrt( 1\/ train['seconds_in_bucket_nunique_420_600'] )\ntest['size_tau_420'] = np.sqrt( 1\/ test['seconds_in_bucket_nunique_420_600'] )\ntrain['size_tau_300'] = np.sqrt( 1\/ train['seconds_in_bucket_nunique_300_600'] )\ntest['size_tau_300'] = np.sqrt( 1\/ test['seconds_in_bucket_nunique_300_600'] )\ntrain['size_tau_240'] = np.sqrt( 1\/ train['seconds_in_bucket_nunique_240_600'] )\ntest['size_tau_240'] = np.sqrt( 1\/ test['seconds_in_bucket_nunique_240_600'] )\n#train['size_tau_100'] = np.sqrt( 1\/ train['seconds_in_bucket_count_100'] )\n#test['size_tau_100'] = np.sqrt( 1\/ test['seconds_in_bucket_count_100'] )","acf6e4a4":"train['size_tau2'] = np.sqrt( 1\/ train['order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1\/ test['order_count_sum'] )\n#train['size_tau2_500'] = np.sqrt( 0.16\/ train['order_count_sum'] )\n#test['size_tau2_500'] = np.sqrt( 0.16\/ test['order_count_sum'] )\ntrain['size_tau2_400'] = np.sqrt( 0.33\/ train['order_count_sum'] )\ntest['size_tau2_400'] = np.sqrt( 0.33\/ test['order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5\/ train['order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5\/ test['order_count_sum'] )\ntrain['size_tau2_200'] = np.sqrt( 0.66\/ train['order_count_sum'] )\ntest['size_tau2_200'] = np.sqrt( 0.66\/ test['order_count_sum'] )\n#train['size_tau2_100'] = np.sqrt( 0.83\/ train['order_count_sum'] )\n#test['size_tau2_100'] = np.sqrt( 0.83\/ test['order_count_sum'] )\n\n# delta tau\ntrain['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","0370bd4f":"import pandas as pd\nfrom sklearn.cluster import KMeans\n# making agg features\n\ntrain_p = dev_df.to_pandas()\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=5, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(5):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()","52c9c24b":"mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","8aa69208":"nnn = ['time_id',\n     'realized_vol1_mean_0c1',\n     'realized_vol1_mean_1c1',     \n     'realized_vol1_mean_3c1',\n     'realized_vol1_mean_4c1',     \n     #'realized_vol1_sum_6c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     #'total_volume_sum_6c1',\n     'size_sum_0c1',\n     'size_sum_1c1', \n     'size_sum_3c1',\n     'size_sum_4c1', \n     #'size_sum_6c1',\n     'amount_sum_0c1',\n     'amount_sum_1c1', \n     'amount_sum_3c1',\n     'amount_sum_4c1', \n     #'amount_sum_6c1',\n     'order_count_sum_0c1',\n     'order_count_sum_1c1',\n     'order_count_sum_3c1',\n     'order_count_sum_4c1',\n     #'order_count_sum_6c1',      \n     'price_spread_sum_0c1',\n     'price_spread_sum_1c1',\n     'price_spread_sum_3c1',\n     'price_spread_sum_4c1',\n     #'price_spread_sum_6c1',   \n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     #'bid_spread_sum_6c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     #'ask_spread_sum_6c1',   \n     'volume_imbalance_sum_0c1',\n     'volume_imbalance_sum_1c1',\n     'volume_imbalance_sum_3c1',\n     'volume_imbalance_sum_4c1',\n     #'volume_imbalance_sum_6c1',       \n     'bid_ask_spread_sum_0c1',\n     'bid_ask_spread_sum_1c1',\n     'bid_ask_spread_sum_3c1',\n     'bid_ask_spread_sum_4c1',\n     #'bid_ask_spread_sum_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     #'size_tau2_6c1',\n      ] \ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')\ntest = pd.merge(test,mat2[nnn],how='left',on='time_id')","6ca22a98":"import gc\ndel mat1,mat2\ngc.collect()","01a01e36":"def rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate(train, test, features):\n    # Hyperparammeters (just basic)\n    seed0=2021\n    params = {\n        'objective': 'rmse',\n        'boosting_type': 'gbdt',\n        'max_depth': -1,\n        'max_bin':100,\n        'min_data_in_leaf':500,\n        'learning_rate': 0.05,\n        'subsample': 0.72,\n        'subsample_freq': 4,\n        'feature_fraction': 0.5,\n        'lambda_l1': 0.5,\n        'lambda_l2': 1.0,\n        'categorical_column':[0],\n        'seed':seed0,\n        'feature_fraction_seed': seed0,\n        'bagging_seed': seed0,\n        'drop_seed': seed0,\n        'data_random_seed': seed0,\n        #\"tree_learner\": 'voting',\n        'device': \"gpu\",\n        'gpu_device_id':0,\n        'verbose': -1}\n    \n    # Split features and target\n    #features=[col for col in train.columns if col not in ['row_id', 'target', 'time_id','is_test']]\n    y = train['target']\n    # Transform stock id to a numeric value\n    #x['stock_id'] = x['stock_id'].astype(int)\n    #x_test['stock_id'] = x_test['stock_id'].astype(int)\n    \n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 57, shuffle = True)\n    # Iterate through each fold\n    #stack_weight=[0.225,0.225,0.225,0.1,0.225]\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                        num_boost_round=1000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=50,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        # Predict the test set\n        test_predictions += model.predict(test[features]) * 0.2#stack_weight[fold]\n        \n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    lgb.plot_importance(model,max_num_features=20)\n    # Return test predictions\n    return test_predictions","30a69071":"feature=['stock_id',\n 'wap1_sum',\n 'wap1_std',\n 'realized_vol2_mean',\n 'price_spread_sum',\n 'price_spread_max',\n 'price_spread1_sum',\n 'price_spread1_max',\n 'wap_balance_sum',\n 'wap_balance_max',\n 'bid_spread_sum',\n 'bid_spread_max',\n 'ask_spread_sum',\n 'ask_spread_max',\n 'total_volume_sum',\n 'total_volume_max',\n 'volume_imbalance_sum',\n 'volume_imbalance_max',\n 'bid_ask_spread_sum',\n 'bid_ask_spread_max',\n 'realized_vol1_mean',\n 'realized_vol1_mean_420_480',\n 'realized_vol1_mean_480_540',\n 'realized_vol1_mean_540_600',\n 'realized_vol_trade_mean',\n 'realized_vol1_mean_60_600',\n 'realized_vol1_mean_0_600',\n 'realized_vol1_mean_120_600',\n 'realized_vol1_mean_180_600',\n 'realized_vol1_mean_240_600',\n 'realized_vol1_mean_300_600',\n 'realized_vol1_mean_360_600',\n 'realized_vol1_mean_420_600',\n 'realized_vol1_mean_480_600',\n 'size_tau',\n 'size_tau_420',\n 'size_tau_300',\n 'size_tau_240',\n 'size_tau2',\n 'size_tau2_400',\n 'size_tau2_300',\n 'size_tau2_200',\n 'size_tau2_d',\n 'realized_vol1_mean_0c1',\n 'realized_vol1_mean_1c1',\n 'realized_vol1_mean_3c1',\n 'realized_vol1_mean_4c1',\n 'total_volume_sum_0c1',\n 'total_volume_sum_1c1',\n 'total_volume_sum_3c1',\n 'total_volume_sum_4c1',\n 'size_sum_0c1',\n 'size_sum_1c1',\n 'size_sum_3c1',\n 'size_sum_4c1',\n 'amount_sum_0c1',\n 'amount_sum_1c1',\n 'amount_sum_3c1',\n 'amount_sum_4c1',\n 'order_count_sum_0c1',\n 'order_count_sum_1c1',\n 'order_count_sum_3c1',\n 'order_count_sum_4c1',\n 'price_spread_sum_0c1',\n 'price_spread_sum_1c1',\n 'price_spread_sum_3c1',\n 'price_spread_sum_4c1',\n 'bid_spread_sum_0c1',\n 'bid_spread_sum_1c1',\n 'bid_spread_sum_3c1',\n 'bid_spread_sum_4c1',\n 'ask_spread_sum_0c1',\n 'ask_spread_sum_1c1',\n 'ask_spread_sum_3c1',\n 'ask_spread_sum_4c1',\n 'volume_imbalance_sum_0c1',\n 'volume_imbalance_sum_1c1',\n 'volume_imbalance_sum_3c1',\n 'volume_imbalance_sum_4c1',\n 'bid_ask_spread_sum_0c1',\n 'bid_ask_spread_sum_1c1',\n 'bid_ask_spread_sum_3c1',\n 'bid_ask_spread_sum_4c1',\n 'size_tau2_0c1',\n 'size_tau2_1c1',\n 'size_tau2_3c1',\n 'size_tau2_4c1']","5fb8598b":"# Traing and evaluate\ntest_predictions = train_and_evaluate(train, test,feature)\n# Save test predictions\ntest[\"row_id\"] = test[\"stock_id\"].astype(str) + \"-\" + test[\"time_id\"].astype(str) \ntest[\"target\"] = test_predictions\/SCALE","ac37e787":"train.shape,len(feature)","1e39299b":"%cd \/kaggle\/working","0a899dce":"sub_df = load_data(\"test\", path=PATH).merge(cudf.from_pandas(test[[\"row_id\", \"target\"]]), \n                                            on=\"row_id\", how=\"left\").fillna(0.0)\n\nsub_df.to_csv(\"submission.csv\", index=False, columns=[\"row_id\", \"target\"])","e5af66a5":"sub_df.head(3)\n","a0491b58":"feature=['stock_id',\n 'time_id',\n 'target',\n 'wap1_sum',\n 'wap1_std',\n 'wap2_sum',\n 'wap2_std',\n 'wap3_sum',\n 'wap3_std',\n 'wap4_sum',\n 'wap4_std',\n 'realized_vol1_mean',\n 'realized_vol2_mean',\n 'realized_vol3_mean',\n 'realized_vol4_mean',\n 'price_spread_sum',\n 'price_spread_max',\n 'price_spread1_sum',\n 'price_spread1_max',\n 'wap_balance_sum',\n 'wap_balance_max',\n 'bid_spread_sum',\n 'bid_spread_max',\n 'ask_spread_sum',\n 'ask_spread_max',\n 'total_volume_sum',\n 'total_volume_max',\n 'volume_imbalance_sum',\n 'volume_imbalance_max',\n 'bid_ask_spread_sum',\n 'bid_ask_spread_max',\n 'realized_vol1_mean_0_60',\n 'realized_vol2_mean_0_60',\n 'realized_vol3_mean_0_60',\n 'realized_vol4_mean_0_60',\n 'realized_vol1_mean_60_120',\n 'realized_vol2_mean_60_120',\n 'realized_vol3_mean_60_120',\n 'realized_vol4_mean_60_120',\n 'realized_vol1_mean_120_180',\n 'realized_vol2_mean_120_180',\n 'realized_vol3_mean_120_180',\n 'realized_vol4_mean_120_180',\n 'realized_vol1_mean_180_240',\n 'realized_vol2_mean_180_240',\n 'realized_vol3_mean_180_240',\n 'realized_vol4_mean_180_240',\n 'realized_vol1_mean_240_300',\n 'realized_vol2_mean_240_300',\n 'realized_vol3_mean_240_300',\n 'realized_vol4_mean_240_300',\n 'realized_vol1_mean_300_360',\n 'realized_vol2_mean_300_360',\n 'realized_vol3_mean_300_360',\n 'realized_vol4_mean_300_360',\n 'realized_vol1_mean_360_420',\n 'realized_vol2_mean_360_420',\n 'realized_vol3_mean_360_420',\n 'realized_vol4_mean_360_420',\n 'realized_vol1_mean_420_480',\n 'realized_vol2_mean_420_480',\n 'realized_vol3_mean_420_480',\n 'realized_vol4_mean_420_480',\n 'realized_vol1_mean_480_540',\n 'realized_vol2_mean_480_540',\n 'realized_vol3_mean_480_540',\n 'realized_vol4_mean_480_540',\n 'realized_vol1_mean_540_600',\n 'realized_vol2_mean_540_600',\n 'realized_vol3_mean_540_600',\n 'realized_vol4_mean_540_600',\n 'realized_vol_trade_mean',\n 'seconds_in_bucket_nunique',\n 'size_sum',\n 'size_max',\n 'order_count_sum',\n 'order_count_max',\n 'amount_sum',\n 'amount_max',\n 'realized_vol_trade_mean_0_60',\n 'seconds_in_bucket_nunique_0_60',\n 'size_sum_0_60',\n 'amount_sum_0_60',\n 'order_count_sum_0_60',\n 'realized_vol_trade_mean_60_120',\n 'seconds_in_bucket_nunique_60_120',\n 'size_sum_60_120',\n 'amount_sum_60_120',\n 'order_count_sum_60_120',\n 'realized_vol_trade_mean_120_180',\n 'seconds_in_bucket_nunique_120_180',\n 'size_sum_120_180',\n 'amount_sum_120_180',\n 'order_count_sum_120_180',\n 'realized_vol_trade_mean_180_240',\n 'seconds_in_bucket_nunique_180_240',\n 'size_sum_180_240',\n 'amount_sum_180_240',\n 'order_count_sum_180_240',\n 'realized_vol_trade_mean_240_300',\n 'seconds_in_bucket_nunique_240_300',\n 'size_sum_240_300',\n 'amount_sum_240_300',\n 'order_count_sum_240_300',\n 'realized_vol_trade_mean_300_360',\n 'seconds_in_bucket_nunique_300_360',\n 'size_sum_300_360',\n 'amount_sum_300_360',\n 'order_count_sum_300_360',\n 'realized_vol_trade_mean_360_420',\n 'seconds_in_bucket_nunique_360_420',\n 'size_sum_360_420',\n 'amount_sum_360_420',\n 'order_count_sum_360_420',\n 'realized_vol_trade_mean_420_480',\n 'seconds_in_bucket_nunique_420_480',\n 'size_sum_420_480',\n 'amount_sum_420_480',\n 'order_count_sum_420_480',\n 'realized_vol_trade_mean_480_540',\n 'seconds_in_bucket_nunique_480_540',\n 'size_sum_480_540',\n 'amount_sum_480_540',\n 'order_count_sum_480_540',\n 'realized_vol_trade_mean_540_600',\n 'seconds_in_bucket_nunique_540_600',\n 'size_sum_540_600',\n 'amount_sum_540_600',\n 'order_count_sum_540_600',\n 'is_test',\n 'stock_id_mean_realized_vol1_mean',\n 'stock_id_mean_realized_vol2_mean',\n 'stock_id_max_realized_vol1_mean',\n 'stock_id_max_realized_vol2_mean',\n 'stock_id_std_realized_vol1_mean',\n 'stock_id_std_realized_vol2_mean',\n 'stock_id_min_realized_vol1_mean',\n 'stock_id_min_realized_vol2_mean',\n 'time_id_mean_realized_vol1_mean',\n 'time_id_mean_realized_vol2_mean',\n 'time_id_max_realized_vol1_mean',\n 'time_id_max_realized_vol2_mean',\n 'time_id_std_realized_vol1_mean',\n 'time_id_std_realized_vol2_mean',\n 'time_id_min_realized_vol1_mean',\n 'time_id_min_realized_vol2_mean',\n 'realized_vol1_mean_60_600',\n 'realized_vol1_mean_0_600',\n 'realized_vol1_mean_120_600',\n 'realized_vol1_mean_180_600',\n 'realized_vol1_mean_240_600',\n 'realized_vol1_mean_300_600',\n 'realized_vol1_mean_360_600',\n 'realized_vol1_mean_420_600',\n 'realized_vol1_mean_480_600',\n 'realized_vol2_mean_0_600',\n 'realized_vol2_mean_60_600',\n 'realized_vol2_mean_120_600',\n 'realized_vol2_mean_180_600',\n 'realized_vol2_mean_240_600',\n 'realized_vol2_mean_300_600',\n 'realized_vol2_mean_360_600',\n 'realized_vol2_mean_420_600',\n 'realized_vol2_mean_480_600',\n 'realized_vol3_mean_0_600',\n 'realized_vol3_mean_60_600',\n 'realized_vol3_mean_120_600',\n 'realized_vol3_mean_180_600',\n 'realized_vol3_mean_240_600',\n 'realized_vol3_mean_300_600',\n 'realized_vol3_mean_360_600',\n 'realized_vol3_mean_420_600',\n 'realized_vol3_mean_480_600',\n 'realized_vol4_mean_0_600',\n 'realized_vol4_mean_60_600',\n 'realized_vol4_mean_120_600',\n 'realized_vol4_mean_180_600',\n 'realized_vol4_mean_240_600',\n 'realized_vol4_mean_300_600',\n 'realized_vol4_mean_360_600',\n 'realized_vol4_mean_420_600',\n 'realized_vol4_mean_480_600',\n 'realized_vol_trade_mean_0_600',\n 'realized_vol_trade_mean_60_600',\n 'realized_vol_trade_mean_120_600',\n 'realized_vol_trade_mean_180_600',\n 'realized_vol_trade_mean_240_600',\n 'realized_vol_trade_mean_300_600',\n 'realized_vol_trade_mean_360_600',\n 'realized_vol_trade_mean_420_600',\n 'realized_vol_trade_mean_480_600',\n 'size_tau',\n 'seconds_in_bucket_nunique_0_600',\n 'seconds_in_bucket_nunique_60_600',\n 'seconds_in_bucket_nunique_120_600',\n 'seconds_in_bucket_nunique_180_600',\n 'seconds_in_bucket_nunique_240_600',\n 'seconds_in_bucket_nunique_300_600',\n 'seconds_in_bucket_nunique_360_600',\n 'seconds_in_bucket_nunique_420_600',\n 'seconds_in_bucket_nunique_480_600',\n 'size_tau_420',\n 'size_tau_300',\n 'size_tau_240',\n 'size_tau2',\n 'size_tau2_400',\n 'size_tau2_300',\n 'size_tau2_200',\n 'size_tau2_d',\n 'realized_vol1_mean_0c1',\n 'realized_vol1_mean_1c1',\n 'realized_vol1_mean_3c1',\n 'realized_vol1_mean_4c1',\n 'total_volume_sum_0c1',\n 'total_volume_sum_1c1',\n 'total_volume_sum_3c1',\n 'total_volume_sum_4c1',\n 'size_sum_0c1',\n 'size_sum_1c1',\n 'size_sum_3c1',\n 'size_sum_4c1',\n 'amount_sum_0c1',\n 'amount_sum_1c1',\n 'amount_sum_3c1',\n 'amount_sum_4c1',\n 'order_count_sum_0c1',\n 'order_count_sum_1c1',\n 'order_count_sum_3c1',\n 'order_count_sum_4c1',\n 'price_spread_sum_0c1',\n 'price_spread_sum_1c1',\n 'price_spread_sum_3c1',\n 'price_spread_sum_4c1',\n 'bid_spread_sum_0c1',\n 'bid_spread_sum_1c1',\n 'bid_spread_sum_3c1',\n 'bid_spread_sum_4c1',\n 'ask_spread_sum_0c1',\n 'ask_spread_sum_1c1',\n 'ask_spread_sum_3c1',\n 'ask_spread_sum_4c1',\n 'volume_imbalance_sum_0c1',\n 'volume_imbalance_sum_1c1',\n 'volume_imbalance_sum_3c1',\n 'volume_imbalance_sum_4c1',\n 'bid_ask_spread_sum_0c1',\n 'bid_ask_spread_sum_1c1',\n 'bid_ask_spread_sum_3c1',\n 'bid_ask_spread_sum_4c1',\n 'size_tau2_0c1',\n 'size_tau2_1c1',\n 'size_tau2_3c1',\n 'size_tau2_4c1']","11b44d1b":"## Train LGBM model on GPU","2fdd1d79":"## Using rapids-kaggle-utils for missing cuDF aggregation functions","794e1e26":"# Accelerating Trading on GPU via RAPIDS"}}