{"cell_type":{"eb98bac3":"code","a6179064":"code","cab1e2a3":"code","cad24bff":"code","a95e7257":"code","d2bf4ab0":"code","7a09aa90":"code","458645b4":"code","dd406933":"code","5f1f397e":"code","9437c420":"code","e2d6cb7f":"code","d16872b6":"code","f2478ba0":"code","6caaa4f2":"code","5b142be3":"code","96dd3465":"code","4912b7e4":"code","07e861b8":"code","e7b0c1c7":"code","5b707b00":"code","b11e5ac9":"code","c1d1fdb8":"code","6adc24ac":"code","a2abaa32":"code","a90d0700":"code","102d5bdf":"code","8675accf":"code","023cc22b":"code","a9bad280":"code","851eb118":"code","f17bb6a7":"code","6186c79d":"code","6252ad2c":"code","c2568f10":"code","72a60e05":"code","af88ac19":"code","e6402307":"code","47cce667":"code","b8b19253":"code","49702e0b":"code","1836a543":"code","6c52ceae":"code","ebeb839b":"code","60683dfa":"code","0a8f4850":"code","977e4b36":"code","37b51ad5":"code","13fd3cc7":"code","a10c34b2":"code","27154a90":"code","e795eabd":"code","e9908acf":"code","0cd153d0":"code","57cd7579":"code","0c4b26e1":"code","c9615e2f":"code","152fc080":"code","a69205b7":"code","b7915b2d":"code","b6e427e8":"code","f18ec58f":"code","13fb4468":"code","cfd24c75":"code","b08bd0a0":"code","3fb8a275":"code","1b984010":"code","dbb659f2":"code","595f1f27":"code","5c75ca17":"code","5c31858b":"code","e92b23e6":"code","6638bb90":"code","d2eba68f":"code","d865d95f":"code","d96e91b7":"markdown","f58b9d93":"markdown","d8c72f00":"markdown","2237a262":"markdown","e906c054":"markdown","cc48bac1":"markdown","992b83e7":"markdown","6a2f82f9":"markdown","8d623386":"markdown","3e8fd88f":"markdown","c016bb28":"markdown","1970c6da":"markdown","019b066c":"markdown","5d253c97":"markdown"},"source":{"eb98bac3":"\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n\n\n\n\n# Necessary\/extra dependencies. \nimport os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom shutil import copyfile\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n#! conda install -c conda-forge gdcm -y\n#! conda install pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg\n#! conda install pillow\n#customize iPython writefile so we can write variables\nfrom IPython.core.magic import register_line_cell_magic\n\n\nimport pylab\n#import pillow\n#import gdcm\n#pydicom\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom fastai.imports import *\n#from fastai.medical.imaging import *\nfrom PIL import Image\n\n@register_line_cell_magic\ndef writetemplate(line, cell):\n    with open(line, 'w') as f:\n        f.write(cell.format(**globals()))\n        \n","a6179064":"# Download YOLOv5\n!git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n%cd yolov5\n# Install dependencies\n%pip install -qr requirements.txt  # install dependencies\n\n%cd ..\/\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","cab1e2a3":"# Install W&B \n!pip install -q --upgrade wandb\n# Login \nimport wandb\nwandb.login()","cad24bff":"\ndf=pd.read_csv('\/kaggle\/input\/df-train\/df_train.csv')\n\nTRAIN_PATH='\/kaggle\/input\/siim-covid19-resized-to-256px-jpg\/train\/'\n\n# TRAIN_PATH=  '\/kaggle\/working\/siim-covid19-resized-to-256px-jpg\/train\/'\n# Add absolute path\ndf['path'] = df.apply(lambda row: TRAIN_PATH+row.id+'.jpg', axis=1)","a95e7257":"df['path'][0]","d2bf4ab0":"labels = df[['Negative for Pneumonia','Typical Appearance','Indeterminate Appearance','Atypical Appearance']].values\nlabels = np.argmax(labels, axis=1)","7a09aa90":"df[['label_y','label_int']]","458645b4":"df['x_min'] = df.apply(lambda row: (row.x_min)\/row.w, axis =1)\ndf['y_min'] = df.apply(lambda row: (row.y_min)\/row.h, axis =1)\n\ndf['x_max'] = df.apply(lambda row: (row.x_max)\/row.w, axis =1)\ndf['y_max'] = df.apply(lambda row: (row.y_max)\/row.h, axis =1)\n\ndf['x_mid'] = df.apply(lambda row: (row.x_max+row.x_min)\/2, axis =1)\ndf['y_mid'] = df.apply(lambda row: (row.y_max+row.y_min)\/2, axis =1)\n\ndf['w'] = df.apply(lambda row: (row.x_max-row.x_min), axis =1)\ndf['h'] = df.apply(lambda row: (row.y_max-row.y_min), axis =1)\n\ndf['area'] = df['w']*df['h']\n","dd406933":"del df['class']\ndf['image_level'] = df.apply(lambda row: row.label.split(' ')[0], axis=1)\n\n","5f1f397e":"from sklearn.model_selection import train_test_split\n# Create train and validation split.\ntrain_df, valid_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df.image_level.values)\n\ntrain_df.loc[:, 'split'] = 'train'\nvalid_df.loc[:, 'split'] = 'valid'\n\ndf = pd.concat([train_df, valid_df]).reset_index(drop=True)","9437c420":"\n#TRAIN_PATH = 'input\/siim-covid19-resized-to-256px-jpg\/train\/'\nIMG_SIZE = 256\nBATCH_SIZE = 16\nEPOCHS = 10\n\n","e2d6cb7f":"#import pylibjpeg \nfrom fastai.imports import *\nfrom fastai.medical.imaging import *\ndef loadfilename(filename,voi_lut = True, fix_monochrome = True):\n    \n    \n    information={}\n\n    img = pydicom.read_file(filename)   \n\n\n    information['PatientID'] = img.PatientID\n\n    information['PatientName'] = img.PatientName\n\n    information['PatientSex'] = img.PatientSex\n\n    information['StudyID'] = img.StudyID\n\n    information['StudyDate'] = img.StudyDate\n\n    information['StudyTime'] = img.StudyTime\n    \n    if voi_lut:\n        img_data = apply_voi_lut(img.pixel_array, img)\n        \n    else:\n        img_data=img.pixel_array\n        \n    if fix_monochrome and img.PhotometricInterpretation == \"MONOCHROME1\":\n        img_data = np.amax(img_data) - img_data  \n\n    #print(np.max(img_data))\n    #print(np.min(img_data))\n\n    img_data=img_data-np.min(img_data)\n    img_data=img_data\/np.max(img_data)\n    img_data=(img_data * 255).astype(np.uint8)\n\n    # return information,img_data\n    return img_data","d16872b6":"# os.makedirs('siim-covid19-resized-to-256px-jpg\/train',exist_ok=True)\n\ndef create_dataset():\n    for i in  tqdm(range(len(df))):\n        row=df.loc[i]\n        img_name=row.id\n        study_id=(row.StudyInstanceUID)\n        dicom_path= (\"..\/input\/siim-covid19-detection\/train\/{}\".format(study_id))\n\n        path_x=(os.path.join(dicom_path,os.listdir(dicom_path)[0]))\n        img_path=os.path.join(path_x,os.listdir(path_x)[0])\n        #print(img_path)\n        #info,img=loadfilename(img_path)\n        img=loadfilename(img_path)\n        img_s = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n        #print('siim-covid19-resized-to-256px-jpg\/train\/'+str(img_name)+\".jpg\")\n        cv2.imwrite('siim-covid19-resized-to-256px-jpg\/train\/'+str(img_name)+\".jpg\", img_s)","f2478ba0":"# Load meta.csv file\n# Original dimensions are required to scale the bounding box coordinates appropriately.\nmeta_df = pd.read_csv('\/kaggle\/input\/siim-covid19-resized-to-256px-jpg\/meta.csv')\n\ntrain_meta_df = meta_df.loc[meta_df.split == 'train']\ntrain_meta_df = train_meta_df.drop('split', axis=1)\ntrain_meta_df.columns = ['id', 'dim0', 'dim1']\n\ntrain_meta_df.head(2)","6caaa4f2":"df = df.merge(train_meta_df, on='id',how=\"left\")\ndf.head(2)\n\ndf[['w','h','dim0','dim1']]","5b142be3":"print(f'Size of dataset: {len(df)}, training images: {len(train_df)}. validation images: {len(valid_df)}')","96dd3465":"os.makedirs('covid\/images\/train', exist_ok=True)\nos.makedirs('covid\/images\/valid', exist_ok=True)\n\nos.makedirs('covid\/labels\/train', exist_ok=True)\nos.makedirs('covid\/labels\/valid', exist_ok=True)\n\n! ls covid\/images","4912b7e4":"im_path='\/kaggle\/input\/siim-covid19-resized-to-256px-jpg\/train'\nim_path_list=(os.listdir(im_path))\nprint('b9175a64ad09.jpg' in im_path_list)","07e861b8":"# Get the raw bounding box by parsing the row value of the label column.\n# Ref: https:\/\/www.kaggle.com\/yujiariyasu\/plot-3positive-classes\ndef get_bbox(row):\n    bboxes = []\n    bbox = []\n    for i, l in enumerate(row.label.split(' ')):\n        if (i % 6 == 0) | (i % 6 == 1):\n            continue\n        bbox.append(float(l))\n        if i % 6 == 5:\n            bboxes.append(bbox)\n            bbox = []  \n            \n    return bboxes\n\n# Scale the bounding boxes according to the size of the resized image. \ndef scale_bbox(row, bboxes):\n    # Get scaling factor\n    scale_x = IMG_SIZE\/row.dim1\n    scale_y = IMG_SIZE\/row.dim0\n    \n    scaled_bboxes = []\n    for bbox in bboxes:\n        x = int(np.round(bbox[0]*scale_x, 4))\n        y = int(np.round(bbox[1]*scale_y, 4))\n        x1 = int(np.round(bbox[2]*(scale_x), 4))\n        y1= int(np.round(bbox[3]*scale_y, 4))\n\n        scaled_bboxes.append([x, y, x1, y1]) # xmin, ymin, xmax, ymax\n        \n    return scaled_bboxes\n\n# Convert the bounding boxes in YOLO format.\ndef get_yolo_format_bbox(img_w, img_h, bboxes):\n    yolo_boxes = []\n    for bbox in bboxes:\n        w = bbox[2] - bbox[0] # xmax - xmin\n        h = bbox[3] - bbox[1] # ymax - ymin\n        xc = bbox[0] + int(np.round(w\/2)) # xmin + width\/2\n        yc = bbox[1] + int(np.round(h\/2)) # ymin + height\/2\n        \n        yolo_boxes.append([xc\/img_w, yc\/img_h, w\/img_w, h\/img_h]) # x_center y_center width height\n    \n    return yolo_boxes","e7b0c1c7":"from shutil import copyfile\ndef create_img_dataset():\n    # Move the images to relevant split folder.\n    for i in tqdm(range(len(df))):\n        row = df.loc[i]\n        if row.split == 'train':\n            copyfile(row.path, f'covid\/images\/train\/{row.id}.jpg')\n        else:\n            copyfile(row.path, f'covid\/images\/valid\/{row.id}.jpg')","5b707b00":"create_img_dataset()","b11e5ac9":"# Prepare the txt files for bounding box\nfor i in tqdm(range(len(df))):\n    row = df.loc[i]\n    # Get image id\n    img_id = row.id\n    # Get split\n    split = row.split\n    # Get image-level label\n    label = row.image_level\n    \n    if row.split=='train':\n        file_name = f'covid\/labels\/train\/{row.id}.txt'\n    else:\n        file_name = f'covid\/labels\/valid\/{row.id}.txt'\n        \n    \n    if label=='opacity':\n        # Get bboxes\n        bboxes = get_bbox(row)\n        # Scale bounding boxes\n        scale_bboxes = scale_bbox(row, bboxes)\n        # Format for YOLOv5\n        yolo_bboxes = get_yolo_format_bbox(IMG_SIZE, IMG_SIZE, scale_bboxes)\n        \n        with open(file_name, 'w') as f:\n            for bbox in yolo_bboxes:\n                bbox = [1]+bbox\n                bbox = [str(i) for i in bbox]\n                bbox = ' '.join(bbox)\n                f.write(bbox)\n                f.write('\\n')","c1d1fdb8":"\n#! zip -r output.zip covid\n\n#! rm -rf covid\n\n","6adc24ac":"# Create .yaml file \nimport yaml","a2abaa32":"%cd yolov5","a90d0700":"(os.listdir('\/kaggle\/working\/yolov5\/data'))","102d5bdf":"data_yaml = dict(\n    train = '\/kaggle\/working\/covid\/images\/train',\n    val = '\/kaggle\/working\/covid\/images\/valid',\n    nc = 2,\n    names = ['none', 'opacity']\n)\n\n# Note that I am creating the file in the yolov5\/data\/ directory.\nwith open('\/kaggle\/working\/yolov5\/data\/data.yaml', 'w') as outfile:\n    yaml.dump(data_yaml, outfile, default_flow_style=True)\n    \n    \n","8675accf":"\n%cat \/kaggle\/working\/yolov5\/data\/data.yaml","023cc22b":"len(os.listdir('\/kaggle\/working\/covid\/images\/valid'))\n\n","a9bad280":"len(os.listdir('\/kaggle\/working\/covid\/images\/train'))","851eb118":"!python train.py --img {IMG_SIZE} \\\n                 --batch {BATCH_SIZE} \\\n                 --epochs {EPOCHS} \\\n                 --data data.yaml \\\n                 --weights yolov5m.pt \\\n                 --project kaggle-siim-covid19 \\\n                 --cache","f17bb6a7":"plt.figure(figsize = (20,20))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-siim-covid19\/exp\/labels.jpg'));","6186c79d":"!ls '\/kaggle\/working\/yolov5\/kaggle-siim-covid19\/exp'","6252ad2c":"TEST_PATH = '\/kaggle\/input\/siim-covid19-resized-to-256px-jpg\/test\/'","c2568f10":"weights_dir = 'kaggle-siim-covid19\/exp\/weights\/best.pt'","72a60e05":"%cd '\/kaggle\/working\/yolov5'","af88ac19":"\n\n#!python detect.py --weights {weights_dir} \\\n#                  --source {TEST_PATH} \\\n#                  --img {IMG_SIZE} \\\n#                  --conf 0.28 \\\n#                  --iou-thres 0.5 \\\n#                  --max-det 3 \\\n#                  --save-txt \\\n#                  --save-conf \\\n #                 --exist-ok\n","e6402307":"#pred_label_list=os.listdir('runs\/detect\/exp\/labels\/')","47cce667":"#print(f'Number of opacity predicted by YOLOv5: {len(pred_label_list)}')","b8b19253":"#!conda install '\/kaggle\/input\/pydicom-conda-helper\/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n#!conda install '\/kaggle\/input\/pydicom-conda-helper\/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n#!conda install '\/kaggle\/input\/pydicom-conda-helper\/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n#!conda install '\/kaggle\/input\/pydicom-conda-helper\/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n#!conda install '\/kaggle\/input\/pydicom-conda-helper\/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n#!conda install '\/kaggle\/input\/pydicom-conda-helper\/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y","49702e0b":"# Read the submisison file\nsub_df = pd.read_csv('\/kaggle\/input\/siim-covid19-detection\/sample_submission.csv')\nprint(len(sub_df))\nsub_df.head()","1836a543":"study_df = sub_df.loc[sub_df.id.str.contains('_study')]\nlen(study_df)","6c52ceae":"image_df = sub_df.loc[sub_df.id.str.contains('_image')]\nlen(image_df)\n\n","ebeb839b":"import glob\n\nfrom PIL import Image\n\nfrom shutil import copyfile\nfrom sklearn.model_selection import train_test_split\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut","60683dfa":"study_df.head()","0a8f4850":"image_df.head()","977e4b36":"meta_df=pd.read_csv('\/kaggle\/input\/siim-covid19-resized-to-256px-jpg\/meta.csv')\n\n","37b51ad5":"for study_dir in os.listdir('\/kaggle\/input\/siim-covid19-detection\/test'):\n    for series in os.listdir(f'\/kaggle\/input\/siim-covid19-detection\/test\/{study_dir}'):\n        for image in os.listdir(f'\/kaggle\/input\/siim-covid19-detection\/test\/{study_dir}\/{series}\/'):\n            image_id = image[:-4]\n            meta_df.loc[meta_df['image_id'] == image_id, 'study_id'] = study_dir\n        \nmeta_df.head()\n","13fd3cc7":"meta_df_test=meta_df.loc[meta_df['split']=='test']","a10c34b2":"del meta_df_test['split']","27154a90":"meta_df_test","e795eabd":"# YOLO_MODEL_PATH = '..\/input\/yolo-models\/yolov5s-e-100-img-512.pt'\nYOLO_MODEL_PATHS = 'kaggle-siim-covid19\/exp\/weights\/best.pt'\n\n\n!python detect.py --weights {weights_dir} \\\n                  --source {TEST_PATH} \\\n                  --img {IMG_SIZE} \\\n                  --conf 0.28 \\\n                  --iou-thres 0.5 \\\n                  --max-det 3 \\\n                  --save-txt \\\n                  --save-conf \\\n                  --exist-ok","e9908acf":"os.listdir('runs\/detect\/')","0cd153d0":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nimport random\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfiles = glob('runs\/detect\/exp\/*')\nfor _ in range(3):\n    row = 4\n    col = 3\n    grid_files = random.sample(files, row*col)\n    images     = []\n    for image_path in tqdm(grid_files):\n        img= cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        images.append(img)\n\n    fig = plt.figure(figsize=(col*5, row*5))\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(col, row),  # creates 2x2 grid of axes\n                     axes_pad=0.05,  # pad between axes in inch.\n                     )\n\n    for ax, im in zip(grid, images):\n        # Iterating over the grid returns the Axes.\n        ax.imshow(im)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","57cd7579":"PRED_PATH \/kaggle\/runs\/detect\/exp\/labels'\nprediction_files = os.listdir(PRED_PATH)\nprint(f'Number of opacity predicted by YOLOv5: {len(prediction_files)}')","0c4b26e1":"import tensorflow as tf\nimport gc\n\n","c9615e2f":"AUTOTUNE = tf.data.AUTOTUNE\n\nCONFIG = dict (\n    seed = 42,\n    num_labels = 4,\n    num_folds = 5,\n    img_width = 256,\n    img_height = 256,\n    batch_size = 8,\n    architecture = \"CNN\",\n    infra = \"GCP\",\n)","152fc080":"TEST_PATH = '\/kaggle\/input\/siim-covid19-resized-to-256px-jpg\/test\/'","a69205b7":"image_df['path'] = image_df.apply(lambda row: TEST_PATH+row.id.split('_')[0]+'.jpg', axis=1)\nimage_df = image_df.reset_index(drop=True)","b7915b2d":"image_df.head()","b6e427e8":"study_df = sub_df.loc[sub_df.id.str.contains('_study')]\nlen(study_df)","f18ec58f":"@tf.function\ndef decode_image(image):\n    # convert the compressed string to a 3D uint8 tensor\n    image = tf.image.decode_png(image, channels=3)\n    # Normalize image\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n@tf.function\ndef load_image(df_dict):\n    # Load image\n    image = tf.io.read_file(df_dict['path'])\n    image = decode_image(image)\n    \n    # Resize image\n    image = tf.image.resize(image, (CONFIG['img_height'], CONFIG['img_width']))\n    \n    return image\n\ntestloader = tf.data.Dataset.from_tensor_slices(dict(image_df))\n\ntestloader = (\n    testloader\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(CONFIG['batch_size'])\n    .prefetch(AUTOTUNE)\n)","13fb4468":"# Load Model\nSTUDY_MODEL_PATHS = '\/kaggle\/input\/studylevelmodel\/SIIM-Study-Level-model\/'\nstudy_models = os.listdir(STUDY_MODEL_PATHS)\nstudy_models","cfd24c75":"! pip install -q efficientnet\nfrom efficientnet.tfkeras import EfficientNetB5","b08bd0a0":"predictions = []\nfor model in study_models:\n    # Load model\n    tf.keras.backend.clear_session()\n    model = tf.keras.models.load_model(STUDY_MODEL_PATHS+model)\n    # Prediction\n    tmp = []\n    for img_batch in tqdm(testloader):\n        preds = model.predict(img_batch)\n        tmp.extend(preds)\n        \n    predictions.append(tmp)\n    \n    del model\n    _ = gc.collect()\n    \npredictions = np.mean(predictions, axis=0)","3fb8a275":"class_labels = ['0', '1', '2', '3']\nimage_df.loc[:, class_labels] = predictions\nimage_df.head()","1b984010":"class_to_id = { \n    'negative': 0,\n    'typical': 1,\n    'indeterminate': 2,\n    'atypical': 3}\nid_to_class  = {v:k for k, v in class_to_id.items()}\n\ndef get_study_prediction_string(preds, threshold=0):\n    string = ''\n    for idx in range(4):\n        conf =  preds[idx]\n        if conf>threshold:\n            string+=f'{id_to_class[idx]} {conf:0.2f} 0 0 1 1 '\n    string = string.strip()\n    return string","dbb659f2":"image_df.head()","595f1f27":"meta_df_test.head()","5c75ca17":"study_ids = []\npred_strings = []\n\nfor study_id, df in meta_df_test.groupby('study_id'):\n    # accumulate preds for diff images belonging to same study_id\n    tmp_pred = []\n    \n    df = df.reset_index(drop=True)\n    for image_id in df.image_id.values:\n        preds = image_df.loc[image_df.id == image_id+'_image'].values[0]\n        tmp_pred.append(preds[3:])\n    \n    preds = np.mean(tmp_pred, axis=0)\n    pred_string = get_study_prediction_string(preds)\n    pred_strings.append(pred_string)\n    \n    study_ids.append(f'{study_id}_study')\n    \nstudy_df = pd.DataFrame.from_dict({'id': study_ids, 'PredictionString': pred_strings})\nstudy_df.head()\n","5c31858b":"# The submisison requires xmin, ymin, xmax, ymax format. \n# YOLOv5 returns x_center, y_center, width, height\ndef correct_bbox_format(bboxes):\n    correct_bboxes = []\n    for b in bboxes:\n        xc, yc = int(np.round(b[0]*IMG_SIZE)), int(np.round(b[1]*IMG_SIZE))\n        w, h = int(np.round(b[2]*IMG_SIZE)), int(np.round(b[3]*IMG_SIZE))\n\n        xmin = xc - int(np.round(w\/2))\n        ymin = yc - int(np.round(h\/2))\n        xmax = xc + int(np.round(w\/2))\n        ymax = yc + int(np.round(h\/2))\n        \n        correct_bboxes.append([xmin, ymin, xmax, ymax])\n        \n    return correct_bboxes\n\ndef scale_bboxes_to_original(row, bboxes):\n    # Get scaling factor\n    scale_x = IMG_SIZE\/row.dim1\n    scale_y = IMG_SIZE\/row.dim0\n    \n    scaled_bboxes = []\n    for bbox in bboxes:\n        xmin, ymin, xmax, ymax = bbox\n        \n        xmin = int(np.round(xmin\/scale_x))\n        ymin = int(np.round(ymin\/scale_y))\n        xmax = int(np.round(xmax\/scale_x))\n        ymax = int(np.round(ymax\/scale_y))\n        \n        scaled_bboxes.append([xmin, ymin, xmax, ymax])\n        \n    return scaled_bboxes\n\n# Read the txt file generated by YOLOv5 during inference and extract \n# confidence and bounding box coordinates.\ndef get_conf_bboxes(file_path):\n    confidence = []\n    bboxes = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            preds = line.strip('\\n').split(' ')\n            preds = list(map(float, preds))\n            confidence.append(preds[-1])\n            bboxes.append(preds[1:-1])\n    return confidence, bboxes","e92b23e6":"image_pred_strings = []\nfor i in tqdm(range(len(image_df))):\n    row = meta_df_test.loc[i]\n    id_name = row.image_id\n    \n    if f'{id_name}.txt' in prediction_files:\n        # opacity label\n        confidence, bboxes = get_conf_bboxes(f'{PRED_PATH}\/{id_name}.txt')\n        bboxes = correct_bbox_format(bboxes)\n        ori_bboxes = scale_bboxes_to_original(row, bboxes)\n        \n        pred_string = ''\n        for j, conf in enumerate(confidence):\n            pred_string += f'opacity {conf} ' + ' '.join(map(str, ori_bboxes[j])) + ' '\n        image_pred_strings.append(pred_string[:-1]) \n    else:\n        image_pred_strings.append(\"none 1 0 0 1 1\")","6638bb90":"meta_df_test['PredictionString'] = image_pred_strings\nimage_df = meta_df_test[['image_id', 'PredictionString']]\nimage_df.insert(0, 'id', image_df.apply(lambda row: row.image_id+'_image', axis=1))\nimage_df = image_df.drop('image_id', axis=1)\nimage_df.head()","d2eba68f":"!rm -rf runs","d865d95f":"sub_df = pd.concat([study_df, image_df])\nsub_df.to_csv('\/kaggle\/working\/submission.csv', index=False)\nsub_df","d96e91b7":"Get Boxes","f58b9d93":"Create Images DataSets","d8c72f00":"Create Meta file for Test  dataset","2237a262":"Read Files","e906c054":"# Clone  Yolo  from GIT","cc48bac1":"Thanks to https:\/\/www.kaggle.com\/ayuraj\/submission-covid19\/data","992b83e7":"# Meta Files","6a2f82f9":"## Submission File","8d623386":"# # Run Detection","3e8fd88f":"# Load Model for Study _pred","c016bb28":"# Imports","1970c6da":"**Dowload data set in zip folder..!!**","019b066c":"# dim 0 -->h\n# dim 1 -->w","5d253c97":"## Train Yolo"}}