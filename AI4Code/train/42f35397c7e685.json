{"cell_type":{"74c3db38":"code","4dda0b7b":"code","3a0df927":"code","93204905":"code","599e8473":"code","460e703f":"code","fe7e0de7":"code","b8a1f252":"code","a9a3ef84":"code","cdf62349":"code","a7ddb083":"code","5597281e":"code","684fca2f":"code","a77d0bae":"code","8850b1cf":"code","692b5be0":"code","abf761b5":"code","5d16c9a2":"code","a3259c26":"code","251c2193":"code","beb180f2":"code","1369837e":"code","68429459":"code","065d73b7":"code","3132f50e":"code","507aa73f":"markdown","ef101f79":"markdown","974bc7ee":"markdown","98d96a80":"markdown"},"source":{"74c3db38":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         # print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4dda0b7b":"import tensorflow as tf\nfrom tensorflow.keras.layers import *\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom tqdm import tqdm","3a0df927":"PATH = '\/kaggle\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset\/'","93204905":"fish_dirs = [dir for dir in os.listdir(PATH) if os.path.isdir(os.path.join(PATH, dir))]","599e8473":"fish_dirs","460e703f":"def load_image(path, output=False):\n    \"\"\"\n    load_image - function which loads image and preprocess it.\n    \"\"\"\n    img = cv2.imread(path)\n    img = cv2.resize(img, (192, 192))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    if output:\n        _, img = cv2.threshold(img, 1, 255, cv2.THRESH_BINARY)\n    img = img.astype('float')\n    img \/= 255\n    return np.array(img)","fe7e0de7":"trainX = []\ntrainY = []","b8a1f252":"for dir in fish_dirs:\n    path = os.path.join(PATH, dir, dir)\n    path_gt = os.path.join(PATH, dir, dir + \" GT\")\n    for img in tqdm(os.listdir(path)):\n        trainX.append(load_image(os.path.join(path, img)))\n        trainY.append(load_image(os.path.join(path_gt, img), output=True))","a9a3ef84":"trainX = np.array(trainX)\ntrainY = np.array(trainY)","cdf62349":"from sklearn.model_selection import train_test_split","a7ddb083":"trainX, testX, trainY, testY = train_test_split(trainX, trainY, test_size=0.3, random_state=11)\ntestX, valX, testY, valY = train_test_split(testX, testY, test_size=0.1, random_state=11)","5597281e":"trainX.shape","684fca2f":"fig, ax = plt.subplots(1, 2, figsize=(10, 10))\nax[0].imshow(np.uint8(trainX[0] * 255), cmap='gray')\nax[1].imshow(np.uint8(trainY[0] * 255), cmap='gray')\nplt.show()","a77d0bae":"def conv_block(iters, filters, kernel, padding, activation, input_tensor):\n    for _ in range(iters):\n        input_tensor = Conv2D(filters, kernel, padding=padding, activation=activation, dilation_rate=2)(input_tensor)\n    return input_tensor","8850b1cf":"# UNet Implementation\ninputs = Input(shape=(192, 192, 1))\nconv_out1 = conv_block(2, 16, (3, 3), 'same', 'relu', inputs)\npooling1 = MaxPooling2D((2, 2))(conv_out1)\n\nconv_out2 = conv_block(2, 32, (3, 3), 'same', 'relu', pooling1)\npooling2 = MaxPooling2D((2, 2))(conv_out2)\n\nconv_out3 = conv_block(2, 64, (3, 3), 'same', 'relu', pooling2)\npooling3 = MaxPooling2D((2, 2))(conv_out3)\n\nconv_out4 = conv_block(2, 64, (3, 3), 'same', 'relu', pooling3)\n\nupsample1 = UpSampling2D((2, 2), interpolation='bilinear')(conv_out4)\nconcat1 = concatenate([conv_out3, upsample1], axis=3)\n\ndropout1 = Dropout(0.1)(concat1)\nconv_out5 = conv_block(2, 64, (3, 3), 'same', 'relu', dropout1)\n\nupsample2 = UpSampling2D((2, 2), interpolation='bilinear')(conv_out5)\nconcat2 = concatenate([conv_out2, upsample2], axis=3)\n\ndropout2 = Dropout(0.1)(concat2)\nconv_out6 = conv_block(2, 32, (3, 3), 'same', 'relu', dropout2)\n\nupsample3 = UpSampling2D((2, 2), interpolation='bilinear')(conv_out6)\nconcat3 = concatenate([conv_out1, upsample3], axis=3)\n\ndropout3 = Dropout(0.1)(concat3)\nconv_out7 = conv_block(2, 16, (3, 3), 'same', 'relu', dropout3)\nconv_out8 = Conv2D(1, (1, 1), activation='sigmoid', padding='same')(conv_out7)","692b5be0":"model = tf.keras.Model(inputs=[inputs], outputs=[conv_out8])","abf761b5":"model.summary()","5d16c9a2":"tf.keras.utils.plot_model(model, show_shapes=True)","a3259c26":"# Learning rate scheduler\ndef scheduler(epoch, lr):\n    if epoch == 20:\n        lr *= 0.1\n    return lr","251c2193":"reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)","beb180f2":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [\"accuracy\"])","1369837e":"model.fit(trainX, trainY, batch_size=8, epochs=25, validation_data=(valX, valY), callbacks=[reduce_lr])","68429459":"model.evaluate(testX, testY)","065d73b7":"idx = 0\n_, axis = plt.subplots(3, 5, figsize=(20, 20))\naxis = axis.flatten()\nfor ax in axis: \n    img = model.predict(np.expand_dims(testX[idx], 0))\n    img = np.concatenate((testX[idx], img.round().squeeze()), axis=1)\n    ax.imshow(img, cmap='gray')\n    idx += 1\nplt.show()","3132f50e":"model.save('unet.h5')","507aa73f":"# Loading images and preprocessing phase","ef101f79":"# Importing libraries","974bc7ee":"# Splitting data into training, testing and validation sets","98d96a80":"# Implementation of UNet"}}