{"cell_type":{"5805796a":"code","138f006b":"code","ce0eb397":"code","c58abfa2":"code","afdc0ee4":"markdown","15d028cd":"markdown","6939a6db":"markdown","7dd7941b":"markdown"},"source":{"5805796a":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedKFold, cross_validate\n\nprint(\"Loading and extracting data\")\nfilename = 'subject_01.npz'\nprint(f\"Loading {filename}\")\ndata = np.load('\/kaggle\/input\/visual-imagination-with-meg\/' + filename)\nX = data['X']  # timeseries: n_trials x n_sensors x n_timesteps\ny = data['y']  # class labels: n_trials\nprint(f\"X: {X.shape}\")\nprint(f\"y: {y.shape}\")\nprint(f\"n_trials: {X.shape[0]}\")\n\nt_min = 2.0  # sec.\nt_max = 4.0  # sec.\nfreq = 250.0  # Hz\n\nprint(f\"Extracting timeseries from {t_min}sec to {t_max}sec at {freq}Hz\")\nidx_min = int(t_min * freq)\nidx_max = int(t_max * freq)\nX_window = X[:, :, idx_min:idx_max]  # we use data from t_min to t_max\nprint(f\"X_window: {X_window.shape}\")","138f006b":"print(\"Concatenating the time-series of each trial into a vector\")\nX_window_time = X_window.reshape(X_window.shape[0], -1)\nprint(f\"X_window_time: {X_window_time.shape}\")\n\nclf = make_pipeline(StandardScaler(), LogisticRegression())\ncv = StratifiedKFold(n_splits=4)\n\nprint(\"Predicting what was imagined from the timeseries\")\nresults_time = cross_validate(clf, X=X_window_time, y=y, cv=cv, n_jobs=-1)\nprint(f\"Accuracy: {results_time['test_score'].mean()}\")","ce0eb397":"print(\"Installing pyRiemann to use Covariances and Riemannian distances on timeseries\")\n!pip install pyriemann","c58abfa2":"from pyriemann.estimation import Covariances\nfrom pyriemann.tangentspace import TangentSpace\n\nprint(\"Creating a pipeline that computes the (regularized) covariance matrix of each trial...\")\ncov = Covariances(estimator='oas')\nprint(\"...and approximates each covariance matrix into a vector through the Tangent Space...\")\nprint(\"...so that the Euclidean distance between vectors approximates the Riemannian distance between covariance matrices...\")\nts = TangentSpace()\nprint(\"...and feeds the vectors into a classifier\")\nclf = make_pipeline(cov, ts, LogisticRegression())\n\nprint(\"Predicting what was imagined from the timeseries\")\nresults_covariance = cross_validate(clf, X=X_window, y=y, cv=cv, n_jobs=-1, verbose=True)\nprint(f\"Accuracy: {results_covariance['test_score'].mean()}\")","afdc0ee4":"## Time-resolved decoding","15d028cd":"# Decoding Visual Imagination from MEG data: Time-resolved vs. Covariance\n\nA person imagines a familiar face or location (`y`) for 6 seconds while the brain activity is recorded from 306 sensors (`X`) with an MEG (magnetoencephalography) device. This is repeated 100-200 times (`n_trials`). The (concatenated) timeseries of one trial can be fed to a classifier to predict what was imagined - face or location - from just the MEG signal. Usually this approach, called time-resolved decoding, does not work. An alternative approach is to compute the covariance matrix between the sensors for each trial and then to predict what was imagined in that trial from the covariance matrix. Notice that covariance matrices live on a Riemannian manifold, so the distance between two covariance matrices is the Riemannian distance. We use [pyRiemann](https:\/\/github.com\/alexandrebarachant\/pyRiemann) to compute such quantities and do classification.\n","6939a6db":"## Loading and preparing data","7dd7941b":"## Covariance-based decoding"}}