{"cell_type":{"bcf6741e":"code","91a89180":"code","d91273b7":"code","a374cc00":"code","5d85b5f7":"code","3dbc5382":"code","41ac7b86":"code","02459ec4":"code","27b4fc2f":"code","4691f2a4":"code","49a73ea4":"code","c2cd74a0":"code","50e566a6":"code","cf59a655":"code","61e28cec":"code","12103c74":"code","12c55a90":"code","a331c019":"code","3bb00d39":"code","2404045b":"code","43244ae2":"code","938457a9":"code","af57a2dc":"code","7abb6c66":"code","2c2bc739":"code","d99644f8":"code","4d3c7afb":"code","622d8d21":"code","3342a3a9":"code","072e2d33":"code","6cbe053b":"code","8ac28f99":"code","70ddfb70":"code","ec6d7be5":"code","de089778":"code","5e519d5a":"code","8e969ee0":"code","343051d3":"code","63a071d2":"code","c4e26501":"code","354a16c6":"code","c2720a1e":"code","150e530c":"code","4da8a266":"code","bb19ffb8":"code","a4a61341":"code","275f3552":"code","f0297499":"code","c9f9605b":"code","9563141b":"markdown","556c5938":"markdown","6d5fb5c4":"markdown","ef1e7a5f":"markdown","eec6e02a":"markdown","5e53bf0f":"markdown","aad0e5b9":"markdown","3504177b":"markdown","30824db8":"markdown","d97a3c78":"markdown","36df4e9c":"markdown","126b1f17":"markdown","ca81265c":"markdown","f3eddbb7":"markdown","332c6b5d":"markdown","e86a82c0":"markdown","92de5f26":"markdown","01f85319":"markdown","06870e12":"markdown","1466d9f8":"markdown","570a7e76":"markdown"},"source":{"bcf6741e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","91a89180":"df_train = pd.read_csv(\"..\/input\/BBC News Train.csv\")\n","d91273b7":"df_train.head()","a374cc00":"df_train['category_id'] = df_train['Category'].factorize()[0]","5d85b5f7":"df_train.groupby('Category').category_id.count()","3dbc5382":"df_train.groupby('Category').category_id.count().plot.bar(ylim=0)","41ac7b86":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\nfeatures = tfidf.fit_transform(df_train.Text).toarray()\nlabels = df_train.category_id","02459ec4":"category_to_id = {'business':0, 'tech':1, 'politics':2, 'sport':3, 'entertainment':4}\nid_to_category = {0: 'business', 1: 'tech', 2: 'politics', 3: 'sport', 4: 'entertainment'}","27b4fc2f":"# Use chi-square analysis to find corelation between features (importantce of words) and labels(news category) \nfrom sklearn.feature_selection import chi2\n\nN = 3  # We are going to look for top 3 categories\n\n#For each category, find words that are highly corelated to it\nfor Category, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)                   # Do chi2 analyses of all items in this category\n  indices = np.argsort(features_chi2[0])                                  # Sorts the indices of features_chi2[0] - the chi-squared stats of each feature\n  feature_names = np.array(tfidf.get_feature_names())[indices]            # Converts indices to feature names ( in increasing order of chi-squared stat values)\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]         # List of single word features ( in increasing order of chi-squared stat values)\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]          # List for two-word features ( in increasing order of chi-squared stat values)\n  print(\"# '{}':\".format(Category))\n  print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:]))) # Print 3 unigrams with highest Chi squared stat\n  print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:]))) # Print 3 bigrams with highest Chi squared stat","4691f2a4":"##dimension reduction \nfrom sklearn.manifold import TSNE\n\n# Sampling a subset of our dataset because t-SNE is computationally expensive\nSAMPLE_SIZE = int(len(features) * 0.3)\nnp.random.seed(0)\nindices = np.random.choice(range(len(features)), size=SAMPLE_SIZE, replace=False)          # Randomly select 30 % of samples\nprojected_features = TSNE(n_components=2, random_state=0).fit_transform(features[indices]) # Array of all projected features of 30% of Randomly chosen samples ","49a73ea4":"colors = ['pink', 'green', 'midnightblue', 'orange', 'darkgrey']\n\n# Find points belonging to each category and plot them\nfor category, category_id in sorted(category_to_id.items()):\n    points = projected_features[(labels[indices] == category_id).values]\n    plt.scatter(points[:, 0], points[:, 1], s=30, c=colors[category_id], label=category)\nplt.title(\"tf-idf feature vector for each article, projected on 2 dimensions.\",\n          fontdict=dict(fontsize=15))\nplt.legend()","c2cd74a0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import cross_val_score\n\n\nmodels = [\n    \n    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n    MultinomialNB(),\n    LogisticRegression(random_state=0),\n    KNeighborsClassifier(n_neighbors=3)\n]","50e566a6":"CV = 5  # Cross Validate with 5 different folds of 20% data ( 80-20 split with 5 folds )\n\n#Create a data frame that will store the results for all 5 trials of the 3 different models\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = [] # Initially all entries are empty\n\n#For each Algorithm \nfor model in models:\n  model_name = model.__class__.__name__\n  # create 5 models with different 20% test sets, and store their accuracies\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  # Append all 5 accuracies into the entries list ( after all 3 models are run, there will be 3x5 = 15 entries)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))","cf59a655":"# Store the entries into the results dataframe and name its columns    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\nimport seaborn as sns\n\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)","61e28cec":"cv_df.groupby('model_name').accuracy.mean()","12103c74":"cv_df","12c55a90":"from sklearn.model_selection import train_test_split\n\nmodel = RandomForestClassifier()\n\n#Split Data \nX_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df_train.index, test_size=0.33, random_state=0)\n\n#Train Algorithm\nmodel.fit(X_train, y_train)\n\n# Make Predictions\ny_pred_proba = model.predict_proba(X_test)\ny_pred = model.predict(X_test)","a331c019":"category_id_df = df_train[['Category', 'category_id']].drop_duplicates().sort_values('category_id')","3bb00d39":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nconf_mat = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels=category_id_df.Category.values, yticklabels=category_id_df.Category.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')","2404045b":"from IPython.display import display\n\nfor predicted in category_id_df.category_id:\n   for actual in category_id_df.category_id:\n    if predicted != actual and conf_mat[actual, predicted] >= 2:\n      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n      display(df_train.loc[indices_test[(y_test == actual) & (y_pred == predicted)]]['Text'])\n      print('')","43244ae2":"model2 = RandomForestClassifier()\nmodel2.fit(features, labels)","938457a9":"text_submissions = pd.read_csv(\"..\/input\/BBC News Test.csv\")\nsubmissions = pd.read_csv(\"..\/input\/BBC News Sample Solution.csv\")\n","af57a2dc":"submissions.head()","7abb6c66":"text_submissions.head()","2c2bc739":"test_features = tfidf.transform(text_submissions.Text.tolist())\nprediction=model.predict(test_features)\nid_to_category = {0: 'business', 1: 'tech', 2: 'politics', 3: 'sport', 4: 'entertainment'}\nfor i in range(len(prediction)):\n    submissions.iloc[i,1] = id_to_category[prediction[i]]","d99644f8":"type(text_submissions.Text.tolist())","4d3c7afb":"# Convert submission dataframe to csv \n# you could use any filename. We choose submission here\nsubmissions.to_csv('submission.csv', index=False)","622d8d21":"import spacy\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\nfrom bs4 import BeautifulSoup\nimport unicodedata","3342a3a9":"import nltk\nnltk.download('all', halt_on_error=False)\nfrom nltk.corpus import sentiwordnet as swn","072e2d33":"nlp = spacy.load('en', parse = False, tag=False, entity=False)\ntokenizer = ToktokTokenizer()","6cbe053b":"def remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\ndef remove_special_characters(text):\n    text = re.sub(r'[^a-zA-z0-9\\s]', '', text)\n    return text\n\ndef lemmatize_text(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text\n\nstopword_list = nltk.corpus.stopwords.words('english')\nstopword_list.remove('no')\nstopword_list.remove('not')\n\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","8ac28f99":"CONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ndef expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n    \n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n                                      flags=re.IGNORECASE|re.DOTALL)\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match)\\\n                                if contraction_mapping.get(match)\\\n                                else contraction_mapping.get(match.lower())                       \n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n        \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text","70ddfb70":"def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n                     accented_char_removal=True, text_lower_case=True, \n                     text_lemmatization=True, special_char_removal=True, \n                     stopword_removal=True):\n    \n    normalized_corpus = []\n    for doc in corpus:        \n        if accented_char_removal:\n            doc = remove_accented_chars(doc)            \n        if contraction_expansion:\n            doc = expand_contractions(doc)           \n        if text_lower_case:\n            doc = doc.lower()        \n        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)          \n        special_char_pattern = re.compile(r'([{.(-)!}])')\n        doc = special_char_pattern.sub(\" \\\\1 \", doc)        \n        if text_lemmatization:\n            doc = lemmatize_text(doc)          \n        if special_char_removal:\n            doc = remove_special_characters(doc)        \n        doc = re.sub(' +', ' ', doc)        \n        if stopword_removal:\n            doc = remove_stopwords(doc, is_lower_case=text_lower_case)            \n        normalized_corpus.append(doc)        \n    return normalized_corpus","ec6d7be5":"text = df_train.iloc[0,1]\ntext","de089778":"nptext = np.array([text])\nnorm_articles = normalize_corpus(nptext)\nnorm_articles","5e519d5a":"import math","8e969ee0":"def analyze_sentiment_sentiwordnet_lexicon(article):\n    tagged_text = [(token.text, token.tag_) for token in nlp(article)]\n    pos_score = neg_score = token_count = obj_score = 0\n    for word, tag in tagged_text:\n        ss_set = None\n        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n#             print(ss_set)\n        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n#             ss_set.pos_score()*= 3\n        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n        if ss_set:\n            if 'JJ' in tag:\n                pos_score += ss_set.pos_score()*3\n                neg_score += ss_set.neg_score()*3\n#                 token_count +=3\n            elif 'RB' in tag:\n                pos_score += ss_set.pos_score()*2\n                neg_score += ss_set.neg_score()*2\n#                 token_count +=2\n            else:\n                pos_score += ss_set.pos_score()\n                neg_score +=ss_set.neg_score()\n                token_count += 1\n            obj_score += ss_set.obj_score()\n            token_count +=1\n    final_score = pos_score - neg_score\n    print(token_count+'the words which are considered')\n    norm_final_score = round(float(final_score) \/ token_count, 2)\n#     final_sentiment = math.exp(final_sentiment)\n#     final_sentiment = math.log(final_score)\n    final_sentiment = 'positive' if norm_final_score >= 0.05 else 'negative'\n    #################  NOTE  ##########################\n    # Please change this if statement to suit the needs because I've only done it using an arbitrary \n    #statement which might be wrong\n    #So please change the if statement\n    #Please please see if we can do it for a specific named entity \n#     (20*math.exp(norm_final_score)\/(2.35)) - 10\n    return final_sentiment, (norm_final_score), final_score","343051d3":"predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(article) for article in norm_articles]","63a071d2":"predicted_sentiments","c4e26501":"# Word cloud which is pretty good for representation\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nwordcloud = WordCloud(background_color= 'white', stopwords = stopword_list, max_words = 50, \n                     max_font_size = 45, random_state=42).generate(str(norm_articles))\nfig = plt.figure(1)\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","354a16c6":"# Tokenizes and builds a vocabulary\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\ncv=CountVectorizer(stop_words=stopword_list, max_features=10, ngram_range=(1,3))","c2720a1e":"#this can be put inside a loop to get key words for all articles\ncorpi = norm_articles #changing the number here will give us the key words for that specific article\nX=cv.fit_transform(corpi)\nlist(cv.vocabulary_.keys())[:10]","150e530c":"corpi","4da8a266":"from sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(X)\n# get feature names\nfeature_names=cv.get_feature_names()\n \n# fetch document for which keywords needs to be extracted\ndoc= norm_articles[0]\n \n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))","bb19ffb8":"#tf_idf sorting in descending order\nfrom scipy.sparse import coo_matrix\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n \ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n \n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results\n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n#extract only the top n; n here is 10\nkeywords=extract_topn_from_vector(feature_names,sorted_items,10)\n \n# now print the results\nprint(\"\\nAbstract:\")\nprint(doc)\nprint(\"\\nKeywords:\")\nfor k in keywords:\n    print(k,keywords[k])","a4a61341":"from collections import defaultdict, OrderedDict\nfrom math import sqrt\nfrom operator import itemgetter\nfrom spacy.tokens import Doc\nimport graphviz\nimport json\nimport logging\nimport networkx as nx\nimport os\nimport os.path\nimport re\nimport spacy\nimport string\nimport sys\nimport time\nimport unicodedata\n\n\n######################################################################\n## utility functions\n######################################################################\n\nPAT_FORWARD = re.compile(\"\\n\\-+ Forwarded message \\-+\\n\")\nPAT_REPLIED = re.compile(\"\\nOn.*\\d+.*\\n?wrote\\:\\n+\\>\")\nPAT_UNSUBSC = re.compile(\"\\n\\-+\\nTo unsubscribe,.*\\nFor additional commands,.*\")\n\n\ndef split_grafs (lines):\n    \"\"\"\n    segment raw text, given as a list of lines, into paragraphs\n    \"\"\"\n    graf = []\n\n    for line in lines:\n        line = line.strip()\n\n        if len(line) < 1:\n            if len(graf) > 0:\n                yield \"\\n\".join(graf)\n                graf = []\n        else:\n            graf.append(line)\n\n    if len(graf) > 0:\n        yield \"\\n\".join(graf)\n\n\ndef filter_quotes (text, is_email=True):\n    \"\"\"\n    filter the quoted text out of a message\n    \"\"\"\n    global PAT_FORWARD, PAT_REPLIED, PAT_UNSUBSC\n\n    if is_email:\n        text = filter(lambda x: x in string.printable, text)\n\n        # strip off quoted text in a forward\n        m = PAT_FORWARD.split(text, re.M)\n\n        if m and len(m) > 1:\n            text = m[0]\n\n        # strip off quoted text in a reply\n        m = PAT_REPLIED.split(text, re.M)\n\n        if m and len(m) > 1:\n            text = m[0]\n\n        # strip off any trailing unsubscription notice\n        m = PAT_UNSUBSC.split(text, re.M)\n\n        if m:\n            text = m[0]\n\n    # replace any remaining quoted text with blank lines\n    lines = []\n\n    for line in text.split(\"\\n\"):\n        if line.startswith(\">\"):\n            lines.append(\"\")\n        else:\n            lines.append(line)\n\n    return list(split_grafs(lines))\n\n\ndef maniacal_scrubber (text):\n    \"\"\"\n    it scrubs the garble from its stream...\n    or it gets the debugger again\n    \"\"\"\n    x = \" \".join(map(lambda s: s.strip(), text.split(\"\\n\"))).strip()\n\n    x = x.replace('\u201c', '\"').replace('\u201d', '\"')\n    x = x.replace(\"\u2018\", \"'\").replace(\"\u2019\", \"'\").replace(\"`\", \"'\")\n    x = x.replace(\"\u2026\", \"...\").replace(\"\u2013\", \"-\")\n\n    x = str(unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\"))\n\n    # some web content returns \"not string\" ?? ostensibly no longer\n    # possibl in Py 3.x but crazy \"mixed modes\" of character encodings\n    # have been found in the wild -- YMMV\n\n    try:\n        assert type(x).__name__ == \"str\"\n    except AssertionError:\n        print(\"not a string?\", type(line), line)\n\n    return x\n\n\ndef default_scrubber (text):\n    \"\"\"\n    remove spurious punctuation (for English)\n    \"\"\"\n    return text.lower().replace(\"'\", \"\")\n\n\n######################################################################\n## class definitions\n######################################################################\n\nclass CollectedPhrase:\n    \"\"\"\n    represents one phrase during the collection process\n    \"\"\"\n\n    def __init__ (self, chunk, scrubber):\n        self.sq_sum_rank = 0.0\n        self.non_lemma = 0\n        \n        self.chunk = chunk\n        self.text = scrubber(chunk.text)\n\n\n    def __repr__ (self):\n        return \"{:.4f} ({},{}) {} {}\".format(\n            self.rank, self.chunk.start, self.chunk.end, self.text, self.key\n        )\n\n\n    def range (self):\n        \"\"\"\n        generate the index range for the span of tokens in this phrase\n        \"\"\"\n        return range(self.chunk.start, self.chunk.end)\n\n\n    def set_key (self, compound_key):\n        \"\"\"\n        create a unique key for the the phrase based on its lemma components\n        \"\"\"\n        self.key = tuple(sorted(list(compound_key)))\n\n\n    def calc_rank (self):\n        \"\"\"\n        since noun chunking is greedy, we normalize the rank values\n        using a point estimate based on the number of non-lemma\n        tokens within the phrase\n        \"\"\"\n        chunk_len = self.chunk.end - self.chunk.start + 1\n        non_lemma_discount = chunk_len \/ (chunk_len + (2.0 * self.non_lemma) + 1.0)\n\n        # normalize the contributions of all the kept lemma tokens\n        # within the phrase using root mean square (RMS)\n\n        self.rank = sqrt(self.sq_sum_rank \/ (chunk_len + self.non_lemma)) * non_lemma_discount\n\n\nclass Phrase:\n    \"\"\"\n    represents one extracted phrase\n    \"\"\"\n\n    def __init__ (self, text, rank, count, phrase_list):\n        self.text = text\n        self.rank = rank\n        self.count = count\n        self.chunks = [p.chunk for p in phrase_list]\n\n\n    def __repr__ (self):\n        return self.text\n\n\nclass TextRank:\n    \"\"\"\n    Python impl of TextRank by Milhacea, et al., as a spaCy extension,\n    used to extract the top-ranked phrases from a text document\n    \"\"\"\n    _EDGE_WEIGHT = 1.0\n    _POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n    _TOKEN_LOOKBACK = 3\n    \n\n    def __init__ (\n            self,\n            edge_weight=_EDGE_WEIGHT,\n            logger=None,\n            pos_kept=_POS_KEPT,\n            scrubber=default_scrubber,\n            token_lookback=_TOKEN_LOOKBACK\n    ):\n        self.edge_weight = edge_weight\n        self.logger = logger\n        self.pos_kept = pos_kept\n        self.scrubber = scrubber\n        self.stopwords = defaultdict(list)\n        self.token_lookback = token_lookback\n\n        self.doc = None\n        self.reset()\n\n\n    def reset (self):\n        \"\"\"\n        initialize the data structures needed for extracting phrases\n        removing any state\n        \"\"\"\n        self.elapsed_time = 0.0\n        self.lemma_graph = nx.Graph()\n        self.phrases = defaultdict(list)\n        self.ranks = {}\n        self.seen_lemma = OrderedDict()\n\n\n    def load_stopwords (self, path=\"stop.json\"):\n        \"\"\"\n        load a list of \"stop words\" that get ignored when constructing\n        the lemma graph -- NB: be cautious when using this feature\n        \"\"\"\n        stop_path = None\n\n        # check if the path is fully qualified, or if the file is in\n        # the current working directory\n\n        if os.path.isfile(path):\n            stop_path = path\n        else:\n            cwd = os.getcwd()\n            stop_path = os.path.join(cwd, path)\n\n            if not os.path.isfile(stop_path):\n                loc = os.path.realpath(os.path.join(cwd, os.path.dirname(__file__)))\n                stop_path = os.path.join(loc, path)\n\n        try:\n            with open(stop_path, \"r\") as f:\n                data = json.load(f)\n\n                for lemma, pos_list in data.items():\n                    self.stopwords[lemma] = pos_list\n        except FileNotFoundError:\n            pass\n\n\n    def increment_edge (self, node0, node1):\n        \"\"\"\n        increment the weight for an edge between the two given nodes,\n        creating the edge first if needed\n        \"\"\"\n        if self.logger:\n            self.logger.debug(\"link {} {}\".format(node0, node1))\n    \n        if self.lemma_graph.has_edge(node0, node1):\n            self.lemma_graph[node0][node1][\"weight\"] += self.edge_weight\n        else:\n            self.lemma_graph.add_edge(node0, node1, weight=self.edge_weight)\n\n\n    def link_sentence (self, sent):\n        \"\"\"\n        link nodes and edges into the lemma graph for one parsed sentence\n        \"\"\"\n        visited_tokens = []\n        visited_nodes = []\n\n        for i in range(sent.start, sent.end):\n            token = self.doc[i]\n\n            if token.pos_ in self.pos_kept:\n                # skip any stop words...\n                lemma = token.lemma_.lower().strip()\n\n                if lemma in self.stopwords and token.pos_ in self.stopwords[lemma]:\n                    continue\n\n                # ...otherwise proceed\n                key = (token.lemma_, token.pos_)\n\n                if key not in self.seen_lemma:\n                    self.seen_lemma[key] = set([token.i])\n                else:\n                    self.seen_lemma[key].add(token.i)\n\n                node_id = list(self.seen_lemma.keys()).index(key)\n\n                if not node_id in self.lemma_graph:\n                    self.lemma_graph.add_node(node_id)\n\n                if self.logger:\n                    self.logger.debug(\"visit {} {}\".format(\n                        visited_tokens, visited_nodes\n                    ))\n                    self.logger.debug(\"range {}\".format(\n                        list(range(len(visited_tokens) - 1, -1, -1))\n                    ))\n            \n                for prev_token in range(len(visited_tokens) - 1, -1, -1):\n                    if self.logger:\n                        self.logger.debug(\"prev_tok {} {}\".format(\n                            prev_token, (token.i - visited_tokens[prev_token])\n                        ))\n                \n                    if (token.i - visited_tokens[prev_token]) <= self.token_lookback:\n                        self.increment_edge(node_id, visited_nodes[prev_token])\n                    else:\n                        break\n\n                if self.logger:\n                    self.logger.debug(\" -- {} {} {} {} {} {}\".format(\n                        token.i, token.text, token.lemma_, token.pos_, visited_tokens, visited_nodes\n                    ))\n\n                visited_tokens.append(token.i)\n                visited_nodes.append(node_id)\n\n\n    def collect_phrases (self, chunk):\n        \"\"\"\n        collect instances of phrases from the lemma graph\n        based on the given chunk\n        \"\"\"\n        phrase = CollectedPhrase(chunk, self.scrubber)\n        compound_key = set([])\n\n        for i in phrase.range():\n            token = self.doc[i]\n            key = (token.lemma_, token.pos_)\n        \n            if key in self.seen_lemma:\n                node_id = list(self.seen_lemma.keys()).index(key)\n                rank = self.ranks[node_id]\n                phrase.sq_sum_rank += rank\n                compound_key.add(key)\n        \n                if self.logger:\n                    self.logger.debug(\" {} {} {} {}\".format(\n                        token.lemma_, token.pos_, node_id, rank\n                    ))\n            else:\n                phrase.non_lemma += 1\n    \n        phrase.set_key(compound_key)\n        phrase.calc_rank()\n\n        self.phrases[phrase.key].append(phrase)\n\n        if self.logger:\n            self.logger.debug(phrase)\n\n\n    def calc_textrank (self):\n        \"\"\"\n        iterate through each sentence in the doc, constructing a lemma graph\n        then returning the top-ranked phrases\n        \"\"\"\n        self.reset()\n        t0 = time.time()\n\n        for sent in self.doc.sents:\n            self.link_sentence(sent)\n\n        if self.logger:\n            self.logger.debug(self.seen_lemma)\n\n        # to run the algorithm, we use PageRank \u2013 i.e., approximating\n        # eigenvalue centrality \u2013 to calculate ranks for each of the\n        # nodes in the lemma graph\n\n        self.ranks = nx.pagerank(self.lemma_graph)\n\n        # collect the top-ranked phrases based on both the noun chunks\n        # and the named entities\n\n        for chunk in self.doc.noun_chunks:\n            self.collect_phrases(chunk)\n\n        for ent in self.doc.ents:\n            self.collect_phrases(ent)\n\n        # since noun chunks can be expressed in different ways (e.g., may\n        # have articles or prepositions), we need to find a minimum span\n        # for each phrase based on combinations of lemmas\n\n        min_phrases = {}\n\n        for phrase_key, phrase_list in self.phrases.items():\n            phrase_list.sort(key=lambda p: p.rank, reverse=True)\n            best_phrase = phrase_list[0]\n            min_phrases[best_phrase.text] = (best_phrase.rank, len(phrase_list), phrase_key)\n\n        # yield results\n\n        results = sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True)\n\n        phrase_list = [\n            Phrase(p, r, c, self.phrases[k]) for p, (r, c, k) in results\n        ]\n\n        t1 = time.time()\n        self.elapsed_time = (t1 - t0) * 1000.0\n\n        return phrase_list\n\n\n    def write_dot (self, path=\"graph.dot\"):\n        \"\"\"\n        output the lemma graph in Dot file format\n        \"\"\"\n        keys = list(self.seen_lemma.keys())\n        dot = graphviz.Digraph()\n\n        for node_id in self.lemma_graph.nodes():\n            text = keys[node_id][0].lower()\n            rank = self.ranks[node_id]\n            label = \"{} ({:.4f})\".format(text, rank)\n            dot.node(str(node_id), label)\n\n        for edge in self.lemma_graph.edges():\n            dot.edge(str(edge[0]), str(edge[1]), constraint=\"false\")\n\n        with open(path, \"w\") as f:\n            f.write(dot.source)\n\n\n    def summary (self, limit_phrases=10, limit_sentences=4):\n        \"\"\"\n        run extractive summarization, based on vector distance \n        per sentence from the top-ranked phrases\n        \"\"\"\n        unit_vector = []\n\n        # construct a list of sentence boundaries with a phrase set\n        # for each (initialized to empty)\n\n        sent_bounds = [ [s.start, s.end, set([])] for s in self.doc.sents ]\n\n        # iterate through the top-ranked phrases, added them to the\n        # phrase vector for each sentence\n\n        phrase_id = 0\n\n        for p in self.doc._.phrases:\n            unit_vector.append(p.rank)\n\n            if self.logger:\n                self.logger.debug(\n                    \"{} {} {}\".format(phrase_id, p.text, p.rank)\n                )\n    \n            for chunk in p.chunks:\n                for sent_start, sent_end, sent_vector in sent_bounds:\n                    if chunk.start >= sent_start and chunk.start <= sent_end:\n                        sent_vector.add(phrase_id)\n\n                        if self.logger:\n                            self.logger.debug(\n                                \" {} {} {} {}\".format(sent_start, chunk.start, chunk.end, sent_end)\n                                )\n\n                        break\n\n            phrase_id += 1\n\n            if phrase_id == limit_phrases:\n                break\n\n        # construct a unit_vector for the top-ranked phrases, up to\n        # the requested limit\n\n        sum_ranks = sum(unit_vector)\n        unit_vector = [ rank\/sum_ranks for rank in unit_vector ]\n\n        # iterate through each sentence, calculating its euclidean\n        # distance from the unit vector\n\n        sent_rank = {}\n        sent_id = 0\n\n        for sent_start, sent_end, sent_vector in sent_bounds:\n            sum_sq = 0.0\n    \n            for phrase_id in range(len(unit_vector)):\n                if phrase_id not in sent_vector:\n                    sum_sq += unit_vector[phrase_id]**2.0\n\n            sent_rank[sent_id] = sqrt(sum_sq)\n            sent_id += 1\n\n        # extract the sentences with the lowest distance\n\n        sent_text = {}\n        sent_id = 0\n\n        for sent in self.doc.sents:\n            sent_text[sent_id] = sent\n            sent_id += 1\n\n        # yield results, up to the limit requested\n\n        num_sent = 0\n\n        for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n            yield sent_text[sent_id]\n            num_sent += 1\n\n            if num_sent == limit_sentences:\n                break\n\n\n    def PipelineComponent (self, doc):\n        \"\"\"\n        define a custom pipeline component for spaCy and extend the\n        Doc class to add TextRank\n        \"\"\"\n        self.doc = doc\n        Doc.set_extension(\"phrases\", force=True, default=[])\n        Doc.set_extension(\"textrank\", force=True, default=self)\n        doc._.phrases = self.calc_textrank()\n\n        return doc","275f3552":"length=\"1\"\ntr = TextRank()\nnlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\nrows=[]","f0297499":"doc = nlp(text)\nsent_bounds = [ [s.start, s.end, set([])] for s in doc.sents ]\nlimit_phrases = 4\nphrase_id = 0\nunit_vector = []\nfor p in doc._.phrases:\n#         print(phrase_id, p.text, p.rank)\n        \n        unit_vector.append(p.rank)\n    \n        for chunk in p.chunks:\n#             print(\" \", chunk.start, chunk.end)\n            \n            for sent_start, sent_end, sent_vector in sent_bounds:\n                if chunk.start >= sent_start and chunk.start <= sent_end:\n#                     print(\" \", sent_start, chunk.start, chunk.end, sent_end)\n                    sent_vector.add(phrase_id)\n                    break\n\n        phrase_id += 1\n\n        if phrase_id == limit_phrases:\n            break\n\nsum_ranks = sum(unit_vector)\nunit_vector = [ rank\/sum_ranks for rank in unit_vector ]\nfrom math import sqrt\nsent_rank = {}\nsent_id = 0\nfor sent_start, sent_end, sent_vector in sent_bounds:\n#         print(sent_vector)\n        sum_sq = 0.0\n    \n        for phrase_id in range(len(unit_vector)):\n#             print(phrase_id, unit_vector[phrase_id])\n        \n            if phrase_id not in sent_vector:\n                sum_sq += unit_vector[phrase_id]**2.0\n\n        sent_rank[sent_id] = sqrt(sum_sq)\n        sent_id += 1\nfrom operator import itemgetter\nsorted(sent_rank.items(), key=itemgetter(1))\nlimit_sentences = int(length)\nsent_text = {}\nsent_id = 0\nfinal_summary= \"Extractive Summary:\"\nfor sent in doc.sents:\n    sent_text[sent_id] = sent.text\n    sent_id += 1\nnum_sent = 0\nfor sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n        print(sent_id, sent_text[sent_id])\n        final_summary= final_summary + ' ' + sent_text[sent_id]\n        num_sent += 1\n    \n        if num_sent == limit_sentences:\n            rows.append([text, final_summary])\n         #   extractive_summary = pd.DataFrame({\n          #  'text': text,\n           # 'summary': summary,\n            #    index=[n]\n           # })\n            break\nprint(final_summary)","c9f9605b":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nfrom collections import defaultdict, OrderedDict\nfrom math import sqrt\nfrom operator import itemgetter\nfrom spacy.tokens import Doc\nimport graphviz\nimport json\nimport logging\nimport networkx as nx\nimport os\nimport os.path\nimport re\nimport spacy\nimport string\nimport sys\nimport time\nimport unicodedata\n\n\n######################################################################\n## utility functions\n######################################################################\n\nPAT_FORWARD = re.compile(\"\\n\\-+ Forwarded message \\-+\\n\")\nPAT_REPLIED = re.compile(\"\\nOn.*\\d+.*\\n?wrote\\:\\n+\\>\")\nPAT_UNSUBSC = re.compile(\"\\n\\-+\\nTo unsubscribe,.*\\nFor additional commands,.*\")\n\n\ndef split_grafs (lines):\n    \"\"\"\n    segment raw text, given as a list of lines, into paragraphs\n    \"\"\"\n    graf = []\n\n    for line in lines:\n        line = line.strip()\n\n        if len(line) < 1:\n            if len(graf) > 0:\n                yield \"\\n\".join(graf)\n                graf = []\n        else:\n            graf.append(line)\n\n    if len(graf) > 0:\n        yield \"\\n\".join(graf)\n\n\ndef filter_quotes (text, is_email=True):\n    \"\"\"\n    filter the quoted text out of a message\n    \"\"\"\n    global PAT_FORWARD, PAT_REPLIED, PAT_UNSUBSC\n\n    if is_email:\n        text = filter(lambda x: x in string.printable, text)\n\n        # strip off quoted text in a forward\n        m = PAT_FORWARD.split(text, re.M)\n\n        if m and len(m) > 1:\n            text = m[0]\n\n        # strip off quoted text in a reply\n        m = PAT_REPLIED.split(text, re.M)\n\n        if m and len(m) > 1:\n            text = m[0]\n\n        # strip off any trailing unsubscription notice\n        m = PAT_UNSUBSC.split(text, re.M)\n\n        if m:\n            text = m[0]\n\n    # replace any remaining quoted text with blank lines\n    lines = []\n\n    for line in text.split(\"\\n\"):\n        if line.startswith(\">\"):\n            lines.append(\"\")\n        else:\n            lines.append(line)\n\n    return list(split_grafs(lines))\n\n\ndef maniacal_scrubber (text):\n    \"\"\"\n    it scrubs the garble from its stream...\n    or it gets the debugger again\n    \"\"\"\n    x = \" \".join(map(lambda s: s.strip(), text.split(\"\\n\"))).strip()\n\n    x = x.replace('\u201c', '\"').replace('\u201d', '\"')\n    x = x.replace(\"\u2018\", \"'\").replace(\"\u2019\", \"'\").replace(\"`\", \"'\")\n    x = x.replace(\"\u2026\", \"...\").replace(\"\u2013\", \"-\")\n\n    x = str(unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\"))\n\n    # some web content returns \"not string\" ?? ostensibly no longer\n    # possibl in Py 3.x but crazy \"mixed modes\" of character encodings\n    # have been found in the wild -- YMMV\n\n    try:\n        assert type(x).__name__ == \"str\"\n    except AssertionError:\n        print(\"not a string?\", type(line), line)\n\n    return x\n\n\ndef default_scrubber (text):\n    \"\"\"\n    remove spurious punctuation (for English)\n    \"\"\"\n    return text.lower().replace(\"'\", \"\")\n\n\n######################################################################\n## class definitions\n######################################################################\n\nclass CollectedPhrase:\n    \"\"\"\n    represents one phrase during the collection process\n    \"\"\"\n\n    def __init__ (self, chunk, scrubber):\n        self.sq_sum_rank = 0.0\n        self.non_lemma = 0\n        \n        self.chunk = chunk\n        self.text = scrubber(chunk.text)\n\n\n    def __repr__ (self):\n        return \"{:.4f} ({},{}) {} {}\".format(\n            self.rank, self.chunk.start, self.chunk.end, self.text, self.key\n        )\n\n\n    def range (self):\n        \"\"\"\n        generate the index range for the span of tokens in this phrase\n        \"\"\"\n        return range(self.chunk.start, self.chunk.end)\n\n\n    def set_key (self, compound_key):\n        \"\"\"\n        create a unique key for the the phrase based on its lemma components\n        \"\"\"\n        self.key = tuple(sorted(list(compound_key)))\n\n\n    def calc_rank (self):\n        \"\"\"\n        since noun chunking is greedy, we normalize the rank values\n        using a point estimate based on the number of non-lemma\n        tokens within the phrase\n        \"\"\"\n        chunk_len = self.chunk.end - self.chunk.start + 1\n        non_lemma_discount = chunk_len \/ (chunk_len + (2.0 * self.non_lemma) + 1.0)\n\n        # normalize the contributions of all the kept lemma tokens\n        # within the phrase using root mean square (RMS)\n\n        self.rank = sqrt(self.sq_sum_rank \/ (chunk_len + self.non_lemma)) * non_lemma_discount\n\n\nclass Phrase:\n    \"\"\"\n    represents one extracted phrase\n    \"\"\"\n\n    def __init__ (self, text, rank, count, phrase_list):\n        self.text = text\n        self.rank = rank\n        self.count = count\n        self.chunks = [p.chunk for p in phrase_list]\n\n\n    def __repr__ (self):\n        return self.text\n\n\nclass TextRank:\n    \"\"\"\n    Python impl of TextRank by Milhacea, et al., as a spaCy extension,\n    used to extract the top-ranked phrases from a text document\n    \"\"\"\n    _EDGE_WEIGHT = 1.0\n    _POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n    _TOKEN_LOOKBACK = 3\n    \n\n    def __init__ (\n            self,\n            edge_weight=_EDGE_WEIGHT,\n            logger=None,\n            pos_kept=_POS_KEPT,\n            scrubber=default_scrubber,\n            token_lookback=_TOKEN_LOOKBACK\n    ):\n        self.edge_weight = edge_weight\n        self.logger = logger\n        self.pos_kept = pos_kept\n        self.scrubber = scrubber\n        self.stopwords = defaultdict(list)\n        self.token_lookback = token_lookback\n\n        self.doc = None\n        self.reset()\n\n\n    def reset (self):\n        \"\"\"\n        initialize the data structures needed for extracting phrases\n        removing any state\n        \"\"\"\n        self.elapsed_time = 0.0\n        self.lemma_graph = nx.Graph()\n        self.phrases = defaultdict(list)\n        self.ranks = {}\n        self.seen_lemma = OrderedDict()\n\n\n    def load_stopwords (self, path=\"stop.json\"):\n        \"\"\"\n        load a list of \"stop words\" that get ignored when constructing\n        the lemma graph -- NB: be cautious when using this feature\n        \"\"\"\n        stop_path = None\n\n        # check if the path is fully qualified, or if the file is in\n        # the current working directory\n\n        if os.path.isfile(path):\n            stop_path = path\n        else:\n            cwd = os.getcwd()\n            stop_path = os.path.join(cwd, path)\n\n            if not os.path.isfile(stop_path):\n                loc = os.path.realpath(os.path.join(cwd, os.path.dirname(__file__)))\n                stop_path = os.path.join(loc, path)\n\n        try:\n            with open(stop_path, \"r\") as f:\n                data = json.load(f)\n\n                for lemma, pos_list in data.items():\n                    self.stopwords[lemma] = pos_list\n        except FileNotFoundError:\n            pass\n\n\n    def increment_edge (self, node0, node1):\n        \"\"\"\n        increment the weight for an edge between the two given nodes,\n        creating the edge first if needed\n        \"\"\"\n        if self.logger:\n            self.logger.debug(\"link {} {}\".format(node0, node1))\n    \n        if self.lemma_graph.has_edge(node0, node1):\n            self.lemma_graph[node0][node1][\"weight\"] += self.edge_weight\n        else:\n            self.lemma_graph.add_edge(node0, node1, weight=self.edge_weight)\n\n\n    def link_sentence (self, sent):\n        \"\"\"\n        link nodes and edges into the lemma graph for one parsed sentence\n        \"\"\"\n        visited_tokens = []\n        visited_nodes = []\n\n        for i in range(sent.start, sent.end):\n            token = self.doc[i]\n\n            if token.pos_ in self.pos_kept:\n                # skip any stop words...\n                lemma = token.lemma_.lower().strip()\n\n                if lemma in self.stopwords and token.pos_ in self.stopwords[lemma]:\n                    continue\n\n                # ...otherwise proceed\n                key = (token.lemma_, token.pos_)\n\n                if key not in self.seen_lemma:\n                    self.seen_lemma[key] = set([token.i])\n                else:\n                    self.seen_lemma[key].add(token.i)\n\n                node_id = list(self.seen_lemma.keys()).index(key)\n\n                if not node_id in self.lemma_graph:\n                    self.lemma_graph.add_node(node_id)\n\n                if self.logger:\n                    self.logger.debug(\"visit {} {}\".format(\n                        visited_tokens, visited_nodes\n                    ))\n                    self.logger.debug(\"range {}\".format(\n                        list(range(len(visited_tokens) - 1, -1, -1))\n                    ))\n            \n                for prev_token in range(len(visited_tokens) - 1, -1, -1):\n                    if self.logger:\n                        self.logger.debug(\"prev_tok {} {}\".format(\n                            prev_token, (token.i - visited_tokens[prev_token])\n                        ))\n                \n                    if (token.i - visited_tokens[prev_token]) <= self.token_lookback:\n                        self.increment_edge(node_id, visited_nodes[prev_token])\n                    else:\n                        break\n\n                if self.logger:\n                    self.logger.debug(\" -- {} {} {} {} {} {}\".format(\n                        token.i, token.text, token.lemma_, token.pos_, visited_tokens, visited_nodes\n                    ))\n\n                visited_tokens.append(token.i)\n                visited_nodes.append(node_id)\n\n\n    def collect_phrases (self, chunk):\n        \"\"\"\n        collect instances of phrases from the lemma graph\n        based on the given chunk\n        \"\"\"\n        phrase = CollectedPhrase(chunk, self.scrubber)\n        compound_key = set([])\n\n        for i in phrase.range():\n            token = self.doc[i]\n            key = (token.lemma_, token.pos_)\n        \n            if key in self.seen_lemma:\n                node_id = list(self.seen_lemma.keys()).index(key)\n                rank = self.ranks[node_id]\n                phrase.sq_sum_rank += rank\n                compound_key.add(key)\n        \n                if self.logger:\n                    self.logger.debug(\" {} {} {} {}\".format(\n                        token.lemma_, token.pos_, node_id, rank\n                    ))\n            else:\n                phrase.non_lemma += 1\n    \n        phrase.set_key(compound_key)\n        phrase.calc_rank()\n\n        self.phrases[phrase.key].append(phrase)\n\n        if self.logger:\n            self.logger.debug(phrase)\n\n\n    def calc_textrank (self):\n        \"\"\"\n        iterate through each sentence in the doc, constructing a lemma graph\n        then returning the top-ranked phrases\n        \"\"\"\n        self.reset()\n        t0 = time.time()\n\n        for sent in self.doc.sents:\n            self.link_sentence(sent)\n\n        if self.logger:\n            self.logger.debug(self.seen_lemma)\n\n        # to run the algorithm, we use PageRank \u2013 i.e., approximating\n        # eigenvalue centrality \u2013 to calculate ranks for each of the\n        # nodes in the lemma graph\n\n        self.ranks = nx.pagerank(self.lemma_graph)\n\n        # collect the top-ranked phrases based on both the noun chunks\n        # and the named entities\n\n        for chunk in self.doc.noun_chunks:\n            self.collect_phrases(chunk)\n\n        for ent in self.doc.ents:\n            self.collect_phrases(ent)\n\n        # since noun chunks can be expressed in different ways (e.g., may\n        # have articles or prepositions), we need to find a minimum span\n        # for each phrase based on combinations of lemmas\n\n        min_phrases = {}\n\n        for phrase_key, phrase_list in self.phrases.items():\n            phrase_list.sort(key=lambda p: p.rank, reverse=True)\n            best_phrase = phrase_list[0]\n            min_phrases[best_phrase.text] = (best_phrase.rank, len(phrase_list), phrase_key)\n\n        # yield results\n\n        results = sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True)\n\n        phrase_list = [\n            Phrase(p, r, c, self.phrases[k]) for p, (r, c, k) in results\n        ]\n\n        t1 = time.time()\n        self.elapsed_time = (t1 - t0) * 1000.0\n\n        return phrase_list\n\n\n    def write_dot (self, path=\"graph.dot\"):\n        \"\"\"\n        output the lemma graph in Dot file format\n        \"\"\"\n        keys = list(self.seen_lemma.keys())\n        dot = graphviz.Digraph()\n\n        for node_id in self.lemma_graph.nodes():\n            text = keys[node_id][0].lower()\n            rank = self.ranks[node_id]\n            label = \"{} ({:.4f})\".format(text, rank)\n            dot.node(str(node_id), label)\n\n        for edge in self.lemma_graph.edges():\n            dot.edge(str(edge[0]), str(edge[1]), constraint=\"false\")\n\n        with open(path, \"w\") as f:\n            f.write(dot.source)\n\n\n    def summary (self, limit_phrases=10, limit_sentences=4):\n        \"\"\"\n        run extractive summarization, based on vector distance \n        per sentence from the top-ranked phrases\n        \"\"\"\n        unit_vector = []\n\n        # construct a list of sentence boundaries with a phrase set\n        # for each (initialized to empty)\n\n        sent_bounds = [ [s.start, s.end, set([])] for s in self.doc.sents ]\n\n        # iterate through the top-ranked phrases, added them to the\n        # phrase vector for each sentence\n\n        phrase_id = 0\n\n        for p in self.doc._.phrases:\n            unit_vector.append(p.rank)\n\n            if self.logger:\n                self.logger.debug(\n                    \"{} {} {}\".format(phrase_id, p.text, p.rank)\n                )\n    \n            for chunk in p.chunks:\n                for sent_start, sent_end, sent_vector in sent_bounds:\n                    if chunk.start >= sent_start and chunk.start <= sent_end:\n                        sent_vector.add(phrase_id)\n\n                        if self.logger:\n                            self.logger.debug(\n                                \" {} {} {} {}\".format(sent_start, chunk.start, chunk.end, sent_end)\n                                )\n\n                        break\n\n            phrase_id += 1\n\n            if phrase_id == limit_phrases:\n                break\n\n        # construct a unit_vector for the top-ranked phrases, up to\n        # the requested limit\n\n        sum_ranks = sum(unit_vector)\n        unit_vector = [ rank\/sum_ranks for rank in unit_vector ]\n\n        # iterate through each sentence, calculating its euclidean\n        # distance from the unit vector\n\n        sent_rank = {}\n        sent_id = 0\n\n        for sent_start, sent_end, sent_vector in sent_bounds:\n            sum_sq = 0.0\n    \n            for phrase_id in range(len(unit_vector)):\n                if phrase_id not in sent_vector:\n                    sum_sq += unit_vector[phrase_id]**2.0\n\n            sent_rank[sent_id] = sqrt(sum_sq)\n            sent_id += 1\n\n        # extract the sentences with the lowest distance\n\n        sent_text = {}\n        sent_id = 0\n\n        for sent in self.doc.sents:\n            sent_text[sent_id] = sent\n            sent_id += 1\n\n        # yield results, up to the limit requested\n\n        num_sent = 0\n\n        for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n            yield sent_text[sent_id]\n            num_sent += 1\n\n            if num_sent == limit_sentences:\n                break\n\n\n    def PipelineComponent (self, doc):\n        \"\"\"\n        define a custom pipeline component for spaCy and extend the\n        Doc class to add TextRank\n        \"\"\"\n        self.doc = doc\n        Doc.set_extension(\"phrases\", force=True, default=[])\n        Doc.set_extension(\"textrank\", force=True, default=self)\n        doc._.phrases = self.calc_textrank()\n\n        return doc\n\nsent_bounds = [ [s.start, s.end, set([])] for s in doc.sents ]\nlimit_phrases = 4\n\nphrase_id = 0\nunit_vector = []\n\nfor p in doc._.phrases:\n#     print(phrase_id, p.text, p.rank)\n    \n    unit_vector.append(p.rank)\n    \n    for chunk in p.chunks:\n#         print(\" \", chunk.start, chunk.end)\n        \n        for sent_start, sent_end, sent_vector in sent_bounds:\n            if chunk.start >= sent_start and chunk.start <= sent_end:\n#                 print(\" \", sent_start, chunk.start, chunk.end, sent_end)\n                sent_vector.add(phrase_id)\n                break\n\n    phrase_id += 1\n\n    if phrase_id == limit_phrases:\n        break\nsum_ranks = sum(unit_vector)\nunit_vector = [ rank\/sum_ranks for rank in unit_vector ]\n\nfrom math import sqrt\n\nsent_rank = {}\nsent_id = 0\n\nfor sent_start, sent_end, sent_vector in sent_bounds:\n#     print(sent_vector)\n    sum_sq = 0.0\n    \n    for phrase_id in range(len(unit_vector)):\n#         print(phrase_id, unit_vector[phrase_id])\n        \n        if phrase_id not in sent_vector:\n            sum_sq += unit_vector[phrase_id]**2.0\n\n    sent_rank[sent_id] = sqrt(sum_sq)\n    sent_id += 1\nfrom operator import itemgetter\n\nsorted(sent_rank.items(), key=itemgetter(1)) \nlimit_sentences = length\n\nsent_text = {}\nsent_id = 0\n\nfor sent in doc.sents:\n    sent_text[sent_id] = sent.text\n    sent_id += 1\n\nnum_sent = 0\n\nfor sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n    print(sent_text[sent_id])\n    num_sent += 1\n    \n    if num_sent == limit_sentences:\n        break","9563141b":"we can use the** chi-squared** test to find the terms are the most correlated with each of the categories:","556c5938":"Using off-the-shelf tools and simple models, we solved a complex task, that of document classification, which might have seemed daunting at first! To do so, we followed steps common to solving any task with machine learning:\n                1. Load and pre-process data.\n                2. Analyze patterns in the data, to gain insights.\n                3. Train different models, and rigorously evaluate each of them.\n                4. Interpret the trained model.","6d5fb5c4":"# The last code cell is the summarization one ignore other code cells","ef1e7a5f":"## Extractive method for GUI","eec6e02a":"**Conclusion**","5e53bf0f":"**Predictions of testing data**","aad0e5b9":"**Final submissions**","3504177b":"Given the very high accuracy of our model, almost all the predictions end up on the diagonal (predicted label = actual label), right where we want them to be!","30824db8":"**Model interpretation**","d97a3c78":"**Data analysis**","36df4e9c":"# Now the unsupervised sentiment analysis on this","126b1f17":"We can also use dimensionality reduction techniques, such** t-SNE** to project our high dimensional tf-idf features into a 2D plane, where they can be visualized.","ca81265c":"In this blog post, we will show how off-the-shelf ML tools can be used to automatically label news articles. The approach we\u2019ll describe can be used in any task related to processing text documents, and even to other types of ML tasks. We will also learn how data can be extracted and pre-processed, how we can make some initial observations about it, how to build ML models, and\u2014last but not least\u2014how to evaluate and interpret them.","f3eddbb7":"# Only keyword extraction is done here\n## It will be replaced by summarisation","332c6b5d":"**Data extraction and exploration**","e86a82c0":"The results for the **Random Forest** model show a large variance, the sign of a model that is overfitting to its training data. Running **cross-validation** is vital, because results from a single train\/test split might be misleading.We also notice that both **Multinomial Naive Bayes** and **Logistic Regression** perform extremely well, with **Logistic Regression** having a slight advantage with a median accuracy of around 97%","92de5f26":"**Comparing differencts models**","01f85319":"**Final model **","06870e12":"Most of these examples are at the intersection of the ground truth category and the one predicted,  which means even humans would struggle to classify them. This sort of noise is expected, and it is often unrealistic to aim for a model that achieves 100% accuracy.","1466d9f8":"It\u2019s not enough to have a model that performs well according to a given metric: we must also have a model that we can understand and whose results we can explain.We will start by training our model on part of the dataset, and then analyze the main sources of misclassification on the test set. One way to eliminate sources of error is to look at the confusion matrix, a matrix used to show the discrepancies between predicted and actual labels.","570a7e76":"We will calculate a measure called Term Frequency, Inverse Document Frequency, abbreviated to **tf-idf**. This statistic represents words\u2019 importance in each document. We use a word's frequency as a proxy for its importance: if \"football\" is mentioned 25 times in a document, it might be more important than if it was only mentioned once. We also use **the document frequency** (the number of documents containing a given word) as a measure of how common the word is. This minimizes the effect of **stop-words** such as pronouns, or domain-specific language that does not add much information (for example, a word such as \"news\" that might be present in most documents)."}}