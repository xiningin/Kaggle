{"cell_type":{"d154e669":"code","eaff2ee2":"code","06d11468":"code","19bd3a29":"code","bac9c347":"code","897f2f70":"code","6fd5ca16":"code","9930c735":"code","389c080c":"code","c8fe1dc3":"code","b872fe53":"code","e0da8847":"code","c0b644ed":"code","2da19e79":"code","d9eb903a":"code","d3c0fb9f":"markdown","ac49ca00":"markdown","045711ee":"markdown","e5d6553c":"markdown","ab036443":"markdown","939cfa0f":"markdown","53408425":"markdown","9b5f91f4":"markdown","446bcdb0":"markdown","0cdd724d":"markdown","65c53992":"markdown","003e2449":"markdown","1218530f":"markdown","1866a3eb":"markdown","076c9d51":"markdown","355a082a":"markdown","fb2f3831":"markdown","e4d473bc":"markdown","0088e0c4":"markdown","51126962":"markdown","38f67f93":"markdown","87196ff1":"markdown","e893deda":"markdown","63e5acf0":"markdown","598a6ec7":"markdown","8ff84216":"markdown","6aed8b43":"markdown","2e69af02":"markdown","d3fe1544":"markdown","7f65e799":"markdown"},"source":{"d154e669":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\n#import probscale\n# Any results you write to the current directory are saved as output.","eaff2ee2":"import json\n\nseason_1718 = json.load(open('..\/input\/datafilev2\/datafile\/season17-18\/season_stats.json', 'r'))\nprint('{}'.format(json.dumps(season_1718,indent=2))[:2500])","06d11468":"match_1718 = json.load(open('..\/input\/datafilev2\/datafile\/season17-18\/season_match_stats.json',  'r'))\nprint('{}'.format(json.dumps(match_1718,indent=2))[:2000])","19bd3a29":"# Each match has a dummy number in season_match_stats.json, first let's obtain the matches Liverpool played.\nmatch_Liv = []\nfor match, value in match_1718.items():\n    if 'Liverpool' in value.values():\n        match_Liv.append(match)\n        \nprint(len(match_Liv))\n\nseason_1718_Liv = dict((key,value) for key, value in season_1718.items() if key in match_Liv)\nprint (len(season_1718_Liv))\n#Now we've got a dic containing only Liverpool matches, next we split the data into with VDD and without VDD.\n#-----------------------------------------\n\nmatch_VDD = []\nfor i in match_Liv:\n    if 'Virgil van Dijk' in season_1718_Liv[i]['26']['Player_stats'] and season_1718_Liv[i]['26']['Player_stats']['Virgil van Dijk']['player_details']['player_position_info'] == 'DC':\n        match_VDD.append(i)\nprint (match_VDD)\n\n\"\"\"\nNow we that know which match VDD involved and which are not, I'm selecting which column\/fields would be meaningful for our analysis.\nNotice that in the original file the field is not listed if the value is zero, for example, if one team didn't score, there won't a\nfield name 'goals' under the 'team_details'.\n\"\"\"\ncolumns_to_used = ['total_offside', 'total_scoring_att', 'won_corners'\n                             ,'shot_off_target','blocked_scoring_att', 'ontarget_scoring_att', 'fk_foul_lost']\n\ndf = pd.DataFrame(columns = ['date', 'oppo','score', 'conceded', 'result','home\/away', 'team_rating'\n                             ,'possession_percentage', 'won_contest', 'total_tackle', 'total_pass', 'accurate_pass'\n                             ,'aerial_won','aerial_lost', 'total_offside', 'total_scoring_att', 'won_corners'\n                             ,'shot_off_target','blocked_scoring_att', 'ontarget_scoring_att', 'fk_foul_lost'\n                             \n                             ,'fk_foul_won','oppo_won_contest', 'oppo_total_pass', 'oppo_accurate_pass'\n                             ,'oppo_total_offside','oppo_scoring_attack', 'oppo_won_corners'\n                             ,'oppo_shot_off_target', 'oppo_blocked_scoring_att', 'oppo_ontarget_scoring_att'\n                             ,'oppo_team_rating','VDD_Start'])\n\n\ni = 0\nfor p in match_Liv:\n    if i < len(match_Liv):\n        \n        if list(season_1718_Liv[p])[0] == '26':\n            oppo_num = list(season_1718_Liv[p])[1]\n            df.loc[i,'oppo'] = season_1718_Liv[p][oppo_num]['team_details']['team_name']\n            df.loc[i,'home\/away'] = 'home'\n            \n        else:\n            oppo_num = list(season_1718_Liv[p])[0]\n            df.loc[i,'oppo'] = season_1718_Liv[p][oppo_num]['team_details']['team_name']\n            df.loc[i,'home\/away'] = 'away'\n          \n        df.loc[i, 'oppo_total_pass'] = season_1718_Liv[p][oppo_num]['aggregate_stats']['total_pass']\n        df.loc[i, 'oppo_accurate_pass'] = season_1718_Liv[p][oppo_num]['aggregate_stats']['accurate_pass']   \n        df.loc[i, 'oppo_scoring_attack'] = season_1718_Liv[p][oppo_num]['aggregate_stats']['total_scoring_att']\n        df.loc[i, 'fk_foul_won'] = season_1718_Liv[p][oppo_num]['aggregate_stats']['fk_foul_lost']\n        df.loc[i, 'oppo_won_contest'] = season_1718_Liv[p][oppo_num]['aggregate_stats']['won_contest']\n\n        if 'goals' not in season_1718_Liv[p][oppo_num]['aggregate_stats']:\n            df.loc[i, 'conceded'] = 0\n        else:\n            df.loc[i, 'conceded'] = season_1718_Liv[p][oppo_num]['aggregate_stats']['goals']\n            \n        if 'total_offside' not in season_1718_Liv[p][oppo_num]['aggregate_stats']:\n            df.loc[i, 'oppo_total_offside'] = 0\n        else:\n            df.loc[i, 'oppo_total_offside'] = season_1718_Liv[p][oppo_num]['aggregate_stats']['total_offside']\n        \n        if 'won_corners' not in season_1718_Liv[p][oppo_num]['aggregate_stats']:\n            df.loc[i, 'oppo_won_corners'] = 0\n        else:\n            df.loc[i, 'oppo_won_corners'] = season_1718_Liv[p][oppo_num]['aggregate_stats']['won_corners']\n        \n        if 'shot_off_target' not in season_1718_Liv[p][oppo_num]['aggregate_stats']:\n            df.loc[i, 'oppo_shot_off_target'] = 0\n        else:\n            df.loc[i, 'oppo_shot_off_target'] = season_1718_Liv[p][oppo_num]['aggregate_stats']['shot_off_target']\n    \n        if 'blocked_scoring_att' not in season_1718_Liv[p][oppo_num]['aggregate_stats']:\n            df.loc[i, 'oppo_blocked_scoring_att'] = 0\n        else:\n            df.loc[i, 'oppo_blocked_scoring_att'] = season_1718_Liv[p][oppo_num]['aggregate_stats']['blocked_scoring_att']\n        \n        if 'ontarget_scoring_att' not in season_1718_Liv[p][oppo_num]['aggregate_stats']:\n            df.loc[i, 'oppo_ontarget_scoring_att'] = 0\n        else:\n            df.loc[i, 'oppo_ontarget_scoring_att'] = season_1718_Liv[p][oppo_num]['aggregate_stats']['ontarget_scoring_att']\n        \n\n        df.loc[i, 'oppo_team_rating'] = round(float(season_1718_Liv[p][oppo_num]['team_details']['team_rating']), 2)\n        df.loc[i,'team_rating'] = round(float(season_1718_Liv[p]['26']['team_details']['team_rating']),2)\n        df.loc[i,'aerial_won'] = season_1718_Liv[p]['26']['aggregate_stats']['aerial_won']\n        df.loc[i,'total_tackle'] = season_1718_Liv[p]['26']['aggregate_stats']['total_tackle']\n        df.loc[i,'aerial_lost'] = season_1718_Liv[p]['26']['aggregate_stats']['aerial_lost']\n        df.loc[i,'possession_percentage'] = season_1718_Liv[p]['26']['aggregate_stats']['possession_percentage']\n        df.loc[i,'won_contest'] = season_1718_Liv[p]['26']['aggregate_stats']['won_contest']\n        df.loc[i,'total_pass'] = season_1718_Liv[p]['26']['aggregate_stats']['total_pass']\n        df.loc[i,'accurate_pass'] = season_1718_Liv[p]['26']['aggregate_stats']['accurate_pass']\n        df.loc[i,'date'] = season_1718_Liv[p]['26']['team_details']['date']      \n        \n        for beta in columns_to_used:\n            if beta not in season_1718_Liv[p]['26']['aggregate_stats']:\n                df.loc[i, beta] = 0\n            else:\n                df.loc[i, beta] = season_1718_Liv[p]['26']['aggregate_stats'][beta]\n                  \n        if 'goals' not in season_1718_Liv[p]['26']['aggregate_stats']:\n            df.loc[i,'score'] = 0\n        else:\n            df.loc[i,'score'] = season_1718_Liv[p]['26']['aggregate_stats']['goals']\n\n        if 'Virgil van Dijk' in season_1718_Liv[p]['26']['Player_stats'] and season_1718_Liv[p]['26']['Player_stats']['Virgil van Dijk']['player_details']['player_position_info'] == 'DC':\n            df.loc[i,'VDD_Start'] = 'Y'\n        else:\n            df.loc[i,'VDD_Start'] = 'N'\n        i = i +1\n        \nprint(df)\nprint(df.dtypes)\ndf.describe()","bac9c347":"from datetime import datetime, date\nimport datetime\n\n# I transfer the date first\ndf_liv = df\ndf_liv['date'] = pd.to_datetime(df_liv['date'], format= \"%d\/%m\/%Y\")\ndf_liv = df_liv.sort_values (by = 'date')\n\ndf_l = df_liv\ndf_l[['team_rating','possession_percentage', 'won_contest', 'total_tackle', 'total_pass', 'accurate_pass'\n                             ,'aerial_won','aerial_lost', 'total_offside', 'total_scoring_att', 'won_corners'\n                             ,'shot_off_target','blocked_scoring_att', 'ontarget_scoring_att', 'fk_foul_lost'\n                             \n                             ,'fk_foul_won','oppo_won_contest', 'oppo_total_pass', 'oppo_accurate_pass'\n                             ,'oppo_total_offside','oppo_scoring_attack', 'oppo_won_corners'\n                             ,'oppo_shot_off_target', 'oppo_blocked_scoring_att', 'oppo_ontarget_scoring_att'\n                             ,'oppo_team_rating','score', 'conceded']]= df_l[['team_rating'\n                             ,'possession_percentage', 'won_contest', 'total_tackle', 'total_pass', 'accurate_pass'\n                             ,'aerial_won','aerial_lost', 'total_offside', 'total_scoring_att', 'won_corners'\n                             ,'shot_off_target','blocked_scoring_att', 'ontarget_scoring_att', 'fk_foul_lost'\n                             \n                             ,'fk_foul_won','oppo_won_contest', 'oppo_total_pass', 'oppo_accurate_pass'\n                             ,'oppo_total_offside','oppo_scoring_attack', 'oppo_won_corners'\n                             ,'oppo_shot_off_target', 'oppo_blocked_scoring_att', 'oppo_ontarget_scoring_att'\n                             ,'oppo_team_rating','score', 'conceded']].apply(pd.to_numeric)\n\ndf_l['VDD_Start'] = df_l['VDD_Start'].astype('category')\ndf_l['date'] = pd.to_datetime(df_l['date'])\nfor x in range (len(df_l)):\n    if df_l.loc[x, 'score'] > df_l.loc[x, 'conceded']:\n        df_l.loc[x, 'result'] = 'W'\n    elif df_l.loc[x, 'score'] == df_l.loc[x, 'conceded']:\n        df_l.loc[x, 'result'] = 'D'\n    else:\n        df_l.loc[x, 'result'] = 'L'\n\nround_p = list(range(1, 39))\ndf_l['round'] = round_p\nprint(df_l.shape)\n\n# Let's check the first four rounds\ndf_l[:4]","897f2f70":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n#df_cor = df_l.iloc[:,5:]\ndf_cor = df_l\ncorrmat = df_cor.corr()\n\nplt.figure(figsize=(20, 12))\np =sns.heatmap(corrmat, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))","6fd5ca16":"sns.scatterplot(x='oppo_team_rating', y ='team_rating', data = df_l, marker=\"o\")","9930c735":"p = sns.jointplot('oppo_team_rating', 'team_rating', df_l, kind=\"reg\")","389c080c":"data_team_rating = np.array(df_l['team_rating'])\ndata_temp = data_team_rating[1:]\ndata_shift = data_team_rating[:-1]\nprint(np.corrcoef(data_temp, data_shift))\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nax.scatter(data_shift, data_temp)\nplt.xlabel(\"team_rating_last_match\")\nplt.ylabel(\"team_rating\")\nplt.show()","c8fe1dc3":"autocorrelation = []\n\nfor shift in range(1,15):\n    cor = np.corrcoef(data_team_rating[:-shift], data_team_rating[shift:])[0,1]\n    autocorrelation.append(cor)\n\ndf_autocorrelation = pd.DataFrame({'autocorrelation':autocorrelation})\ndf_autocorrelation['shift_count'] = list(range(1, 15))\n\nprint(df_autocorrelation)\nsns.lineplot(x='shift_count', y ='autocorrelation', data = df_autocorrelation, marker=\"o\")\n#df_autocorrelation['shift_count'] = list(range(1, 15))\n#print (df_autocorrelation)","b872fe53":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly.tools import FigureFactory as ff\n\n# Here I define how to plot the observed data points and the line.\ndef plot_line(X, y, theta, title1):\n    subs = 'subs'\n    p1 = go.Scatter(x = X, y = y,mode='markers', marker=dict(color='black'))\n    p2 = go.Scatter(x=np.array([min(X), max(X)]), \n                    y=np.array([[1,min(X)],[1,max(X)]]).dot(theta),\n                    mode='lines', line=dict(color='blue', width=3))\n    layout = go.Layout(\n                    title=title1,\n                    xaxis=dict(title='oppo_team_rating'),\n                    yaxis=dict(title='team_rating')\n                )\n    fig = go.Figure(data=[p1, p2], layout = layout)\n    py.iplot(fig)\n    \n    \n    \n# In the following I use the package (OLS) from sklearn to retrieve our parameters, denoted as theath_sklearn\nX = np.array(df_l['oppo_team_rating'])\nY = np.array(df_l['team_rating'])\n\nfrom sklearn import linear_model\nregr = linear_model.LinearRegression(normalize=True)\nregr.fit(X.reshape(-1, 1), Y)\ntheta_sklearn = [regr.intercept_,regr.coef_]\n\n\nprint('Theta for sklearn: ',theta_sklearn)\nplot_line(X,Y,theta_sklearn, 'sklearn')\n\n# Last but not least, the coefficient of determination\nmodel = regr.fit(X.reshape(-1, 1), Y)\nr_sq = model.score(X.reshape(-1, 1), Y)\nprint('Coefficient of determination:', r_sq)","e0da8847":"# By applying this I normailze the data in a range from -1 to 1.\ndef normalize(dataframe):\n    means = np.mean(dataframe, axis=0)\n    maxs = np.max(dataframe, axis=0)\n    mins = np.min(dataframe, axis=0)\n    return ((dataframe - means) \/ (maxs - mins), means, maxs, mins)\n\ndef normalize_data(matrix, means, maxs, mins):\n    return (matrix - means) \/ (maxs - mins)\n\n\"\"\"\nHere I use 'train', normally we will want to have train data for training the model and test data \nfor testing the model, we can simply seperate the data we have into these two groups, Otherwise how'd \nwe know if it's practical to use the model, but for the purpose of comparing gradient descent and OLS, \nI am using the whole dataset as our training data.\n\"\"\" \ndf_train, means,maxs,mins = normalize(df_l[['team_rating', 'oppo_team_rating']])\n\nx_train = np.array(df_train['oppo_team_rating'])\ny_train = np.array(df_train['team_rating'])\n\n\n\ndef cost_function(X, y, theta):\n    m = len(y)\n    return 1\/(2*m) * sum(np.square((np.dot(X,theta) - y)))\n\n#--------- cost_funtion = (1\/2n) * sum((b*x-y)^2) all x\n\n\ndef gradient_descent(X, y, alpha=0.1, num_iters=100):\n    theta = np.zeros(2)\n    X = np.column_stack(((np.ones(X.shape[0])), X))\n    m = len(y)\n    J_hist = np.zeros(num_iters)\n    theta_hist = np.zeros((num_iters,2))\n    for i in range(0,num_iters):\n        prediction = X.dot(theta)\n        \n        theta = theta - alpha*1\/m * X.T.dot((prediction-y))\n\n#------ m = theta_m - x times error, assuming y = m*x + b\n\n        J_hist[i] = cost_function(X, y, theta)\n        theta_hist[i] = theta.T\n        diff = 1 if i == 0 else J_hist[i-1] - J_hist[i]\n    return (theta, J_hist, theta_hist)\n\n# Prepare for the learning rate plot\ndef plot(X, y, mode='markers', title='Plot title', x_axis='X axis', y_axis='Y axis'):\n    p1 = go.Scatter(x = X, y = y,mode=mode, marker=dict(color='black'))\n    layout = go.Layout(\n                    title=title,\n                    xaxis=dict(title=x_axis),\n                    yaxis=dict(title=y_axis)\n                )\n    fig = go.Figure(data=[p1], layout=layout)\n    py.iplot(fig)","c0b644ed":"# Below I set the learning rate and iterations\ntheta_grad, J_hist, theta_hist = gradient_descent(x_train, y_train, 0.3,500)\ntheta_grad_large, J_hist_large, theta_hist_large = gradient_descent(x_train, y_train,3,40)\ntheta_grad_small, J_hist_small, theta_hist_small = gradient_descent(x_train, y_train, 0.03,20000)\n\nprint('theta_gradient_descent is', theta_grad)\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly.tools import FigureFactory as ff\n\nplot(np.arange(len(J_hist)),J_hist, mode='lines'\n     , title='Cost historic with learning rate = 0.3'\n     , x_axis='number of iteration'\n     , y_axis='Cost')\n\nplot(np.arange(len(J_hist)),J_hist_large, mode='lines'\n     , title='Cost historic with learning rate = 3'\n     , x_axis='number of iteration'\n     , y_axis='Cost')\n\nplot(np.arange(len(J_hist)),J_hist_small, mode='lines'\n     , title='Cost historic with learning rate = 0.03'\n     , x_axis='number of iteration'\n     , y_axis='Cost')\n\nimport math\ni = 1\nfor p in range(len(J_hist)):\n    if math.isclose(J_hist[p], 0, abs_tol = 0.00005):\n       print (i)\n       break\n    else: \n        i = i+1\n#print(J_hist[1:30])","2da19e79":"# Notice I define a function named plot_lines which plots the lines from gradient descent and OLS\ndef plot_lines(X, y, theta1, theta2, title1):\n    subs = 'subs'\n    p1 = go.Scatter(x = X, y = y,mode='markers', marker=dict(color='black'))\n    p2 = go.Scatter(x=np.array([min(X), max(X)]), \n                    y=np.array([[1,min(X)],[1,max(X)]]).dot(theta1),\n                    mode='lines', line=dict(color='red', width=3), name = 'Gradient Descent Line')\n    p3 = go.Scatter(x=np.array([min(X), max(X)]), \n                    y=np.array([[1,min(X)],[1,max(X)]]).dot(theta2),\n                    mode='lines', line=dict(color='blue', width=3), name = 'OLS Line')\n    \n    layout = go.Layout(\n                    title=title1,\n                    xaxis=dict(title='oppo_team_rating'),\n                    yaxis=dict(title='team_rating')\n                )\n    fig = go.Figure(data=[p1, p2, p3], layout = layout)\n    py.iplot(fig)\n    \n\nfrom sklearn import linear_model\nregr_n = linear_model.LinearRegression(normalize=True)\n\nregr_n.fit(x_train.reshape(-1, 1), y_train)\ntheta_sklearn_n = [regr_n.intercept_,regr.coef_]\nprint('Theta for gradient descent : ',theta_grad)\nprint('Theta for sklearn          : ',theta_sklearn_n)\n\nplot_lines(x_train,y_train,theta_grad, theta_sklearn_n, 'grad')\n#plot_line(x_train,y_train,theta_sklearn_n, 'sklearn')","d9eb903a":"sns.set_style(\"darkgrid\") \n\nx = df_l['result'].dropna()\ny = df_l['team_rating'].dropna()\nz = df_l['possession_percentage'].dropna()\np = df_l['won_contest'].dropna()\nt = df_l['total_tackle'].dropna()\npr = df_l['total_scoring_att'].dropna()\nr = df_l['won_corners']\nm = df_l['oppo_team_rating']\natr = df_l['oppo_scoring_attack']\nbo = df_l['fk_foul_lost']\nof = df_l['shot_off_target']\n\n\n, \np = sns.pairplot(pd.DataFrame(list(zip(r, m, y, z, pr, atr, bo, p, of, x)),\n                                   columns=['won_corners','oppo_team_rating', 'team_rating', 'possession_percentage', 'total_scoring_att', 'oppo_scoring_att', 'fk_foul_lost', 'won_contest', 'shot_off_target', 'result']), \n                 hue = 'result', palette= \"Set2\")\n","d3c0fb9f":"  *3.2. Transform data*\n\nHere I sovle the above problem, in practice you will also need to handle missing values and outliers, etc.\n\nOther things I did: Adding a column named 'round', coverted column 'VDD_Start' to category, input 'W', 'D' or 'L' in column 'result' where 'W' means win, 'D' means draw, 'L' means lose.\n\nIt all looks fine, looks like we can proceed with these data.","ac49ca00":"OK, so it looks like the autocorrelation is so small that we can ignore it in this case.","045711ee":"**4.3 Multiple Linear Regression**\n\nCongrats! You've come so far, here comes the dessert! \n\nSo far we are only considering the linear regression between 'team_rating' and 'oppo_team_rating', but what if there are some other variables affect the dependent variable? Like 'score', If that's the case, it's probably causing the errors! Remember those errors\/residuals? What if we can add more dependent vairable and make it contributes to the dependent variable effectively in a linear way.\n\nThat is the idea of multiple linear regression. It uses this formular, y = m1 * x1 + m2 * x2 + ... + b, where x1, x2, ... are dependent variables worth using, that's right, not all of them are worth using in the formular because some of them may be correlated with each other wihle some of them doesn't have an impact on the dependent variable. A method that's commonly used is called PCA, Principle Components Analysis.\n\nI will demonstrate this in the future.","e5d6553c":"**4. Data Analysis**\n\n **4.1. Find out which attribute correlates with team rating the most for Liverpool.**\n \n There are 34 columns\/attributes in the dataset and I want to find out which one assotiates with the team rating of Liverpool the best, that makes selecting one from 33. To be honest, I was trying to find out the one which affects the team rating first, notice that 'affecting' means causation, it's different from correlation. \n Now why I research for correlation instead of causation, it's because this is historical data, it's a non-experimental research, which means I can't manipulate the independent variables and find out which one changes 'team rating'. While in experimental researchs analysis like causation would be more convicing.\n \n In addition, we know some are not relevant, for example, date, round, etc. Moreover, to make things simpler, I'm only using a few attributes with best potential in my opinion. Again, please remember the purpose of this kernel, I'm not competing for the best model here.\n \n However, before we go to the 'real' analysis, we need to take a look at the data and understand each field first, this is crucial yet ignored by so many people!\n \n   **4.1.1 What are the types of the variables?**\n   \n   Basically we have two types of variables if we are only looking into the variable itself. We have continuous variable and categorical variable. While continuous variables can be further categorize as either interval or ratio variables, categorical variables can be further categorize as either nominal, ordinal or dichotomous.\n   \n   'VDD_Start' is a dichotomous variable as it has two categories, 'Y' and 'N'. Likewise, 'home\/away'.\n   \n   'result' is a nominal variable as it has more than two categories. Likewise, 'oppo'.\n   \n   'round' is an ordinal variable, the only difference between this one and nominal variable is that it can be ordered or ranked. Sure if we treat the result ranked from win to lose, then 'result' is an ordinal variable as well.\n   \n   \u2018score\u2019 is an interval variable, it's continuous while zero means no goal. Same as many others.\n   \n   'team_rating' is a ratio variable, notice the differences between this one and interval variable, ratio variable has no meaning of being zero and we can use the ratio of measurements. \n   \n   Why? It seems complicating things at first glance, but please think of these questions. Assuming everything else remains the same. Tell me how much has the team improved by scoring from 0 to 1? And how much has the team improved by team_rating from 6.5 to 7?\n   \n   Now let's perceive it from a different angle, we can also seperate variables into dependent variables and independent variables. Imagine we are carrying an experimental research, which means we are able to manipulate many other variables and record the team rating, in this case, all other variables are independent variables while 'team_rating' is dependent variable, under one condition: we have proved they are independent from each other.\n   \n   Important note here, this is called *univariate analysis*, we need to test the correlation between all the other variables to make sure they are indepent, if not then select those are. \n   \n   There is another type of analysis called *multivariate analysis*, in which we have more than one dependent variable.\n   \n   **4.1.2  Analysis Types**\n   \n   So what mentioned above covers it all? You wish...\n   \n   Basically we have these types of data analysis based on the purpose of it.\n   \n      Descriptive analytics answers \u201cwhat happened.\u201d\n\n      Diagnostic analytics seeks \u201cwhy it happened.\u201d\n    \n      Predictive analytics tells what is likely to happen next.\n    \n      Prescriptive analytics provides actions to take next.\n      \n      \n   **4.1.3  Correlation**\n   \n  'Find out which attribute correlates with team rating the most for Liverpool' belongs to descriptive analysis. To do this, let me show you the correlation among all numerical variables first. ","ab036443":"As the three graphs indicates, the bigger step it takes at each iteration (learing rate), the more iteration it needs to converge to the minimum of the cost, which is easy to understand I suppose.\n\nWhat we want is a learning rate that's neither too small nor too large which helps us find out the minimum of the cost after some iterations. If the learing rate is too big, it will never reach the objective, if the learing rate is too small, it simply wastes our time.\n\nWe can also tell the computer when to stop like if next iteration decreases the cost not over some number, a very small number, then we can call it off. Makes sense?\n\nAnyway, we have reached 0.005 for the cost after around 400 iteratoins, and we have our parameter as the two numbers show up in the output. Next let's compare it with OLS.\n","939cfa0f":"Other things we can do before multiple linear regression is to look at a pair plot, from this graph we can check if there's a relationship between some variables and how they distributed, it comes handy when you want to have a overview. Below I didn't plot much of the variables. ","53408425":"![image.png](attachment:image.png)","9b5f91f4":"There are other packages for Python as well, you can also refer to this [link](https:\/\/lectures.quantecon.org\/py\/ols.html). Notice that statsmodels.api is not working in this kernel cloud.","446bcdb0":"Now, have you ever wondered how the line was drew for the two variables? \n\n","0cdd724d":"Wait, I can only see one line there, where is the line generated by gradient descent? Please take a closer look, it is there! You can point it to the maximum or the minimum of the x axis. \n\nThat's right, they almost coincide. You can also see it in the output for the theta of them, which represents the intercept and slope calculated by both methods.","65c53992":"Welcome! Please make yourself comfortable, here's the menu. \n\nIn this Kernel I will cover the most important topics for any kinds of data analysis by demonstrating easy to understand real cases. You may wonder that must involve tons of topics as there are many types of data analysis depending on the goal of it.\n\nHold on, it's such a bore to simply put all the topics related together, don't you think? Instead, **this is about understanding the data and your objective at the beginning of the analysis, which is so fundamental yet being ignored by so many people**, even those who are building complicated models.\n\nTopics to be covered: *Data Analysis process, data types, correlation, simple linear regression and its assumptions, OLS, gradient descent.*\n\nYou don't need to be very familiar with data analysis or skilled with coding in order to understand the topics mentioned. All kinds of comments are welcome, please feel free to let me know if anything is confused, I didn't double check everything ; )","003e2449":"* Homoscedasticity. It describes a situation in which the error term (that is, the \u201cnoise\u201d or random disturbance in the relationship between the independent variables and the dependent variable) is the same across all values of the independent variables. We can see it from the join plot that this is the case, imagine we add some points on (6,6), that will violate this rule as the residuals in this area are so much bigger than the others. All in all, this assotiates with the confidence level of our prediction, that's right, prediction is just a prediction, how much can we count on it needs to be taken into consideration. We can check it by plotting the residual versus predicted values but I just skip it in this case. \n\n* Normality. This one is needed in many statistics test and analysis, we can never skip it. Calculation of confidence intervals and various significance tests for coefficients are all based on the assumptions of normally distributed errors. What if it isn't normally distributed, well we can always try to normalize the data, like many cases we saw in practice are log normal distributions, it's not difficult to transform the data into normal distribution.\n\nThese are the four pricipal assumptions for a linear regression. Keep in mind that this example only discuss simple regression, which means there is only one independent variable, however, for multiple linear regression which by the way it's a much common case, there are serval independent variables where **multicollinearity** matters, this will be discussed in future topics.\n\nFor more details please check this [link](http:\/\/people.duke.edu\/~rnau\/testing.htm)\n","1218530f":"But that's for another day. We have covered many just from a simple questions regarding correlation, I hope you find what you're looking for here, please let me know in the comments if you have anything to say. Stay tuned and have a nice day!","1866a3eb":"![image.png](attachment:image.png)","076c9d51":"At first glance it seems to be no outliers, but in practice we always have a hard time dealing with outliers, for more details on how to handle it please check this [link](https:\/\/www.itl.nist.gov\/div898\/handbook\/prc\/section1\/prc16.htm).\n\nWe can tell from this graph that the two variables have a similar scale and they are mostly distributed in the center, those with most distance from the line are both team ratings are relatively high or relatively low. For example, there is point where 'team_rating' is just above 7.5 and 'oppo_team_rating' is around 6.4, if 'team_rating' decreases to 7.2 or 'oppo_team_rating' decreases to 6, it will fall on the line.\n\nNevertheless, can we say this analysis is completed? Hell no...\n\n**4.2.1 Linear regression assumptions**\n\nLinear regression has serval important assumptions:\n* Linear relationship. If we can tell it from the scatter plot that they are not having a linear relationship, can we fit a linear model to them? Notice that there are quadratic regression, exponential regression, etc, linear regression is sensitive to outliers, so please make sure at least from the scatter plot that a straight line would fit the data well.\n\n* Little or no autocorrelation (serial correlation). Autocorrelaiton happens mostly in time series analysis, it means the correlation between the series and its past values. We need to make sure the variable itself doesnot change based on its past value, for example, a rainly day is more likely to be rainy tomorrow. Here, I checked the team rating, see if it's affected by past matches.","355a082a":"Our objective is to find out the point with the lowest sea level in the photo. Makes sense?\n\nGradient descent algorithm starts from a random point in the space captured in the photo and then it moves in a given distance towards our objectives at a time, then it checks if it reaches to the bottom (the lowest sea level) and if not, it moves again in the second iteration, it keeps moving until it reaches our objective point. It's like a step in the right direction at each iteration, that step is known as learning rate.\n\ncost function = 1\/2N * SSR \n\nN represents the number of data points\n\nYou see, the cost function of linear regression is almost the same as SSR, remember this one from the OLS? It makes perfect sense, right? What these two methods doing is the same, to minimize the SSR. BTW, it's multiplied by 1\/2N, 1\/2 is a simplification of the derivative while 1\/N is used to normalize it.\n\nThe following is to apply Gradien descent in the same linear regression and a normalization as mentioned in 4.2.2. What fun would it be if it's just for the theories and no demonstration?\n","fb2f3831":"\n**4.2.2 OLS**\n\nBack to where we started the simple linear regression, once we defined the fitted line as y = m*x + b + error, then our goal is to figure out the parameter m and b to make this line represent our observed data the best, which means we want all of our data points sit near to the line, which means we want the error term the least the better.\n\nHere comes OLS, Ordinary Least Square, it calculates the sum of squred residuals (error) and find out our parameters m and b to minimize this sum. \n\nWhat is Sum of squred residuals, AKA SSR, or ESS, or RSS. Well, we take the vertical distance from each observed data point and the line, square each distance (as you can imagine there are positive and negative numbers if we simply calculate y(i) - y_hat(i)), then we add it up. Simple, right?\n\nRemeber those assumptions for the linear regression? It means if they are not met then the fitted line generated by OLS is either biased or unefficient, that's why those assumptions matters. If they are met, then according to the[ Gauss\u2013Markov theorem](http:\/\/en.wikipedia.org\/wiki\/Gauss%E2%80%93Markov_theorem), the best linear unbiased estimator (BLUE) of the coefficients is given by the ordinary least squares (OLS) estimator. That's right, there are serval other methods to find out the parameters, especially when it comes to multiple linear regression.\n\nSuppose we have our model built through OLS or other methods, we will want to know how well our model fits the truth, this is known as the goodness of fit, which is determined by the **coefficient of determination** R-squared, which is the ratio of explained variance to the total variance of the dependent variable. \n\nAll in all, it is used to explain how much variability of one factor can be caused by its relationship to another factor. It ranges from 0 to 1, a good fit has an R-squared that is close to 1.\n\nBelow are the details of math:\n\nExplained variance, known as the explained sum of squares, denoted as SSE or ESS, we take the vertical distance between estimated data point (those on the line) and the mean of the observed data points, square it, then sum it up \n\nTotal variance, known as the total sum of squares, denoted as SST or TSS, we take the vertical distance between each observed data point and the mean of the observed data point, square it, then sume it up.\n\nSo basically we have this equation:\n![image.png](attachment:image.png)\n\n\nOK, enough for the theories, let's take a look at how it works.","e4d473bc":"That was plotting match to match, and the autocorrelation of y(t) and y(t-1) is called lag1, lag2 is for y(t) and y(t-2), which checks the match before the previous match and curent match, and so on.\n\nBelow I plotted from lag1 to lag14, sometimes it's called correlogram.","0088e0c4":"Less is more. After selecting and merging the data we have everything we want in one table, cool! \n\nOK, I admit that more than dropping data was involved in this operation, we have created new variables, renamed variable, combined data from two data sources, which are often viewed as transforming the data.\n\nNow notice that the data type are all object, which means you can't even order them by date or from highest to lowest, etc. the 'result' was not calculated neither. What's next then?","51126962":"I'll be using the data of English Premier League for the demonstration, for those who are familiar with football matches, congrats! But if that's not your case, don't worry as I will explain it to a extent.\n\nNow let's take a look at the original data.","38f67f93":"**3. Data Cleaning\/Scrubbing**\n\nAs mentioned above, we have many data at hand, it's just too many hence we need to ask ourselves what do we need? What are good data and poor data? **We only need those we can benifit from.** Of course my business goals are all related to Liverpool, therefore one dataset would be enough, but if I have more goals I'll need to prepare another.\n\nThis could be the most time consuming process during the whole analysis.\n\nSince we have the semi-structured data (Json) at hand, things can be a little tricky, but please rest assure that Pyhon is able to handle this type of data by treating it as a dictionary.\n\nNow, the following code is not short and nice as we expected, but I think codes are like piano scores, they are everywhere, if you don't know how just search for it, while technics and soul lie in the inference and the analysis. Tell me, if you're about to learn how to play the piano, would you spend more time on memorizing the sheets or practising?\n\nNotice that I'm not complicating things by writing long codes, I'm just not that skillful to pack everything nicely, you will see what I mean. \n\n*I didn't use multiple loops.*","87196ff1":"There are two files for each season, one contains the detailed information of each match including statistics for the whole team and each individual. The other one contains mainly the result of each match.\n\n**1. Define your questions **\n\nNow, let me tell you** what I want to achieve** when I first look into the data, since I'm a fan of Liverpool, here it goes:\n* Find out which attribute correlates with team rating the most for Liverpool.\n\n* Analyze how Liverpool players' stats changed after VDD came to Liverpool (Virgil Van Dijk, a defender who joined Liverpool on 2018 Jan)\n\n* Build a model that calculates the individual rating based on the stats available for each position (Apparently GK has different stats than field players, likewise center backs and strikers)\n\n* Catogorize Liverpool games, that means clustering. And try to use the attributes to identify different results, afterwards use this model to test\/compare matches from previous season, see what has Liverpool improved or fell behind.\n\n\nHere are the **basics you need to know in case you never watch football matches**\n* There are 11 players from each team including one Goal Keeper in the field, each team can use 3 subtitutes during one match. Each match lasts 90 minutes, after 90 mintues' play the referee will give some extra time to make up for the stoppage time during the match.\n\n* When the referee blows the whistle at the end of the game, if your team scores more than the oppornent you win. If equal, it's a draw.\n\n* There are 20 teams in the league playing against each other at home and away, which means 38 matches for each team in one season, win = 3 points, draw = 1, lose = 0. After all the matches played, the one team with most points win the title of the season.\n\n* English Premier League is the most developed football league in the world, 'developed' means history, which brings audience, which brings money and fame, which brings competition.\n\nNotice that for most cases we set our goals before collecting\/viewing the data.\n\n**2. Data Collection\/Extraction **\n\nIn this process you'll need to think about the data source and collection methods, which could cost a lot! In this particular case we are using external semi-structured data, it's quite common to use scrapy to fullfil it. I'm only showing you the result of it as this process was already covered.\n\nBy the way the data comes from [whoscored.com](https:\/\/www.whoscored.com\/)\n\nCongrats! You've just finished the Appetiser! Now shall we serve the main course?","e893deda":"Well, looks like it would fit in a linear regression. Notice a shift just happens, we are no long investigating the correlation, we are trying to predict the independent variable from the dependent variable. We just changed the analysis type from descriptive to predictive.\n\nSimple linear regression uses this formation: y = m*x + b + error, y is the dependent variable, x is the independent variable, m is the slope of the line, b is the intercept, error calculates the difference between the predictive value y hat and the observed value of the dependent variable y, error is also known as the residual.\n\nA more aggresive\/direct approach is to plot the regression line with seaborn as below shows.","63e5acf0":"Notice that the scale of the dependent variable and the independent variable are pretty much the same in this case, but if they are not (imagine we want to analysis the relationship between price and sales), we will want to perform normalization which transforms the dataset in some range, i.e., from 0 to 1, from -1 to 1, or standardization which transforms the dataset into a dataset with a mean equals to 1 and variance equals to zero.\n\nMeanwhile we have something called 'regulization' which is used to reduce overfitting, for more details please refer to this [link](https:\/\/maristie.com\/blog\/differences-between-normalization-standardization-and-regularization\/).","598a6ec7":"**4.2.3 Gradient Descent**\n\nThe well known Gradient Descent is a algorithm for finding the minimum of a function, but what does it have to do with linear regression? Here comes the cost function! The cost function is used to monitor the error in predictions of a machine learning model. That's right, machine learning models, not only linear regression, which makes the gradient descent **the most used learning algorithm** in machine learning! I could've skipped it given that it doesn't have an advantaged over OLS, but I wouldn't as it's so commonly used.\n\nSo, how does it work? Forget about calculus, just imagine the cost function as a valley, like this.\n\n","8ff84216":"  *3.1.   Drop data*","6aed8b43":"The following is a method to test the normality, it's called p-p plot, it shows us the cummulated probability of a variable, if it's strictly normally distributed the plot should fit in a straight line. You may want to pick up some attributes regarding normal distribution if you're not familar with it.\n\nThis method requires a package named probscale which is not installed in the cloud, I tried to install to but fail so I have to request the installation via Github. Anyway, that's why I can only show you what I get from my local notebook. I plot all the data regarding the two variables, the dots are not perfectly sitting in a straight line but it's good enough. Notice that there are many other ways to test normality.","2e69af02":"![image.png](attachment:image.png)","d3fe1544":"The number in each box represents the Pearson Correlation Coefficient between the two variables.\n\nThe math behind this: It's the covariance of the two variables divided by the standard deviation of the two variables. In this method, the correlation ranges from -1 to 1, which allows us to tell how strong the relationship it is between the two variables as the bigger the absolute it is the more correlated they are, while we can't tell it from covariance. That's why the first box is 1 as 'score' is 100% correlated with 'score' itself.\n\nBack to our questions, regarding the correlation coefficient, we can see that 'score' and 'ontarget_scoring_att' have the biggest positive number, 0.79 and 0.64, while 'oppo_team_rating' is the most negative one, -0.89. \n\nWe can easily interpret it as the following: The bigger number of 'score' the bigger number of 'team_rating' we find and vice versa, 'score' is having the most positive relationship with 'team_rating'.\n\nWhile the bigger 'oppo_team_rating' is, the smaller 'team_rating' is and vice versa. 'oppo_team_rating' is having the most negative relationship with 'team_rating'.\n\nI'm not saying what affects it the most because this is merely a correlation coefficient which doesn't tell us why it happens.\n\nTherefore, we can draw a conclusion here, 'oppo_team_rating' is the answer. But why don't we dig deeper?\n\n   **4.2 Simle Linear Regression**\n   \n   First, does it make sense? one team plays well and the one must suck? \n   Second, how are they distributed? Normally distributed or something else?\n   Third, is it possible to predict one from another, what kind of regrssion can we apply?\n\n\n   Regarding this kind of univariate analysis, normally we will want to visualize it first, scatter plot is a perfect option, then we can use box plot to check the outliers or empty records if necessary.\n","7f65e799":"**4.4 Other types of correlation**\n\nNow, suppose we wonder how VDD affects the team, we want to know about the correlation between 'team_rating' and 'VDD_Start', or even 'result' and 'VDD_Start', this involves a categorical variable 'VDD_Start' and the commonly used Pearson Correlation Coefficient is no longer useful in these cases. \n\nLuckily we have other types of correlation that serve this purpose, something like [this](https:\/\/medium.com\/@outside2SDs\/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365)\n![image.png](attachment:image.png)\n"}}