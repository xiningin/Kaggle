{"cell_type":{"c3f5dc19":"code","a2108ff3":"code","b4818a53":"code","adb9811e":"code","16f92067":"code","a47963a6":"code","71ef61cb":"code","32609336":"code","99373507":"code","89b0e55f":"code","83b55dac":"code","829201f1":"code","4bb13aca":"code","3f4aef9b":"code","45f65ec2":"code","d3d68fcb":"code","8bf6ce26":"code","86529d64":"code","807c0c90":"code","1d6f9c5d":"code","a37078ab":"code","bf703417":"code","30791273":"code","65239ff6":"code","21d1c9f2":"code","fa81ccfd":"code","1939de93":"code","ec0c9aab":"code","e874d029":"code","058040ce":"code","eb8ebb6d":"code","5815dc5c":"code","d39e5db0":"code","e41fae3f":"code","28efbe34":"code","ce905ae4":"code","5b260572":"code","786af9a3":"code","9bae7991":"code","ccc03443":"code","eacbdac4":"code","15c9d5cd":"code","73d92aeb":"code","37f2ba23":"code","35aaea28":"code","1f0eb1fe":"code","312722f3":"code","967f6a88":"markdown","59b0998c":"markdown","b486ff5f":"markdown","31724954":"markdown","ff28fcf5":"markdown","dcec0757":"markdown","7798c652":"markdown","8abd2861":"markdown","9f5678ea":"markdown","4164935e":"markdown","282262ba":"markdown","dd591b84":"markdown","efa5bb2f":"markdown","c7d2f564":"markdown","5a536743":"markdown","5200018b":"markdown","336c498b":"markdown","1ebe9df9":"markdown","74f07514":"markdown","c59bc11c":"markdown","529a7740":"markdown","81e80ac4":"markdown","8cb50e1d":"markdown","ab196c9d":"markdown","454af8af":"markdown","b84c4454":"markdown","47ab0aa7":"markdown","90fb10dd":"markdown","56355286":"markdown","4fb51bbe":"markdown","af51c7ea":"markdown","c09e7f57":"markdown","6405b226":"markdown","9ea5fd89":"markdown","5776b174":"markdown","608e1678":"markdown","071b8618":"markdown","ce1786ce":"markdown","dafcfce6":"markdown","5633d568":"markdown","f79f2e07":"markdown","ff9b21d2":"markdown","1f0e3412":"markdown","8e40bb06":"markdown","1145f548":"markdown","fe584a6d":"markdown","06a95e81":"markdown","7ac763c8":"markdown","d9ee906b":"markdown","2d2a9ec2":"markdown","bb1b242e":"markdown","2b658d63":"markdown","5ddd11b9":"markdown","0af394f4":"markdown","0d74cc2e":"markdown","3d7f1dbd":"markdown","8573036d":"markdown"},"source":{"c3f5dc19":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pylab as pylab\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport matplotlib as mpl\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\nimport os\n\n\nprint('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))\n\n","a2108ff3":"sns.set(style='white', context='notebook', palette='deep')\npylab.rcParams['figure.figsize'] = 12,8\nwarnings.filterwarnings('ignore')\nmpl.style.use('ggplot')\nsns.set_style('white')\n%matplotlib inline","b4818a53":"# import train and test to play with it\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","adb9811e":"type(train)","16f92067":"type(test)","a47963a6":"# Modify the graph above by assigning each species an individual color.\ng = sns.FacetGrid(train, hue=\"Survived\", col=\"Pclass\", margin_titles=True,\n                  palette={1:\"seagreen\", 0:\"gray\"})\ng=g.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend();","71ef61cb":"train.plot(kind='box', subplots=True, layout=(2,4), sharex=False, sharey=False)\nplt.figure()\n#This gives us a much clearer idea of the distribution of the input attributes:\n\n","32609336":"# To plot the species data using a box plot:\n\nsns.boxplot(x=\"Fare\", y=\"Age\", data=test )\nplt.show()","99373507":"# Use Seaborn's striplot to add data points on top of the box plot \n# Insert jitter=True so that the data points remain scattered and not piled into a verticle line.\n# Assign ax to each axis, so that each plot is ontop of the previous axis. \n\nax= sns.boxplot(x=\"Fare\", y=\"Age\", data=train)\nax= sns.stripplot(x=\"Fare\", y=\"Age\", data=train, jitter=True, edgecolor=\"gray\")\nplt.show()","89b0e55f":"# Tweek the plot above to change fill and border color color using ax.artists.\n# Assing ax.artists a variable name, and insert the box number into the corresponding brackets\n\nax= sns.boxplot(x=\"Fare\", y=\"Age\", data=train)\nax= sns.stripplot(x=\"Fare\", y=\"Age\", data=train, jitter=True, edgecolor=\"gray\")\n\nboxtwo = ax.artists[2]\nboxtwo.set_facecolor('red')\nboxtwo.set_edgecolor('black')\nboxthree=ax.artists[1]\nboxthree.set_facecolor('yellow')\nboxthree.set_edgecolor('black')\n\nplt.show()","83b55dac":"# histograms\ntrain.hist(figsize=(15,20))\nplt.figure()","829201f1":"train[\"Age\"].hist();","4bb13aca":"f,ax=plt.subplots(1,2,figsize=(20,10))\ntrain[train['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('Survived= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ntrain[train['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('Survived= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","3f4aef9b":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=train,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","45f65ec2":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex',hue='Survived',data=train,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","d3d68fcb":"\n# scatter plot matrix\npd.plotting.scatter_matrix(train,figsize=(10,10))\nplt.figure()","8bf6ce26":"# violinplots on petal-length for each species\nsns.violinplot(data=train,x=\"Fare\", y=\"Age\")","86529d64":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=train,split=True,ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=train,split=True,ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","807c0c90":"# Using seaborn pairplot to see the bivariate relation between each pair of features\nsns.pairplot(train, hue=\"Age\")","1d6f9c5d":"# updating the diagonal elements in a pairplot to show a kde\nsns.pairplot(train, hue=\"Age\",diag_kind=\"kde\")","a37078ab":"# seaborn's kdeplot, plots univariate or bivariate density estimates.\n#Size can be changed by tweeking the value used\nsns.FacetGrid(train, hue=\"Survived\", size=5).map(sns.kdeplot, \"Fare\").add_legend()\nplt.show()","bf703417":"# Use seaborn's jointplot to make a hexagonal bin plot\n#Set desired size and ratio and choose a color.\nsns.jointplot(x=\"Age\", y=\"Survived\", data=train, size=10,ratio=10, kind='hex',color='green')\nplt.show()","30791273":"# we will use seaborn jointplot shows bivariate scatterplots and univariate histograms with Kernel density \n# estimation in the same figure\nsns.jointplot(x=\"Age\", y=\"Fare\", data=train, size=6, kind='kde', color='#800000', space=0)","65239ff6":"plt.figure(figsize=(7,4)) \nsns.heatmap(train.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()","21d1c9f2":"sns.heatmap(train.corr(),annot=False,cmap='RdYlGn',linewidths=0.2)  \nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","fa81ccfd":"train['Pclass'].value_counts().plot(kind=\"bar\");","1939de93":"sns.factorplot('Pclass','Survived',hue='Sex',data=train)\nplt.show()","ec0c9aab":"f,ax=plt.subplots(1,3,figsize=(20,8))\nsns.distplot(train[train['Pclass']==1].Fare,ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(train[train['Pclass']==2].Fare,ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(train[train['Pclass']==3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()","e874d029":"# shape\nprint(train.shape)","058040ce":"#columns*rows\ntrain.size","eb8ebb6d":"train.isnull().sum()","5815dc5c":"# remove rows that have NA's\n#train = train.dropna()","d39e5db0":"print(train.info())","e41fae3f":"train['Age'].unique()","28efbe34":"train[\"Pclass\"].value_counts()\n","ce905ae4":"train.head(5) ","5b260572":"train.tail() ","786af9a3":"train.sample(5) ","9bae7991":"train.describe() ","ccc03443":"train.isnull().sum()","eacbdac4":"train.groupby('Pclass').count()","15c9d5cd":"train.columns","73d92aeb":"train.where(train['Age']==30)","37f2ba23":"train[train['Age']>7.2]","35aaea28":"# Seperating the data into dependent and independent variables\nX = train.iloc[:, :-1].values\ny = train.iloc[:, -1].values","1f0eb1fe":"cols = train.columns\nfeatures = cols[0:12]\nlabels = cols[4]\nprint(features)\nprint(labels)","312722f3":"X = train.iloc[:, :-1].values\ny = train.iloc[:, -1].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","967f6a88":"**<< Note >>**\n\n**Yellowbrick** is a suite of visual diagnostic tools called \u201cVisualizers\u201d that extend the Scikit-Learn API to allow human steering of the model selection process. In a nutshell, Yellowbrick combines scikit-learn with matplotlib in the best tradition of the scikit-learn documentation, but to produce visualizations for your models! ","59b0998c":"to print dataset **columns**, we can use columns atribute","b486ff5f":"how many NA elements in every column\n","31724954":"<a id=\"27\"><\/a> <br>\n###  6-2-9 andrews_curves","ff28fcf5":"for getting some information about the dataset you can use **info()** command","dcec0757":"**<< Note >>**\n>**Preprocessing and generation pipelines depend on a model type**","7798c652":"<a id=\"24\"><\/a> <br>\n### 6-2-6 pairplot","8abd2861":"### 5-5-1 Import","9f5678ea":"### 6-3-2 Explorer Dataset\n1- Dimensions of the dataset.\n\n2- Peek at the data itself.\n\n3- Statistical summary of all attributes.\n\n4- Breakdown of the data by the class variable.[7]\n\nDon\u2019t worry, each look at the data is **one command**. These are useful commands that you can use again and again on future projects.","4164935e":"<a id=\"3\"><\/a> <br>\n## 3- Problem Definition\nI think one of the important things when you start a new machine learning project is Defining your problem. that means you should understand business problem.( **Problem Formalization**)\n\nProblem Definition has four steps that have illustrated in the picture below:\n<img src=\"http:\/\/s8.picofile.com\/file\/8338227734\/ProblemDefination.png\">\n<a id=\"4\"><\/a> <br>\n### 3-1 Problem Feature\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. **On April 15, 1912**, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing **1502 out of 2224** passengers and crew. That's why the name DieTanic. This is a very unforgetable disaster that no one in the world can forget.\n\nIt took about $7.5 million to build the Titanic and it sunk under the ocean due to collision. The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle.\n\nwe will use the classic titanic data set. This dataset contains information about **11 different variables**:\n<img src=\"http:\/\/s9.picofile.com\/file\/8340453092\/Titanic_feature.png\" height=\"500\" width=\"500\">\n\n* Survival\n* Pclass\n* Name\n* Sex\n* Age\n* SibSp\n* Parch\n* Ticket\n* Fare\n* Cabin\n* Embarked\n\n\n### 3-3-1 Why use Titanic dataset?\n\n1- This is a good project because it is so well understood.\n\n2- Attributes are numeric and categorical so you have to figure out how to load and handle data.\n\n3- It is a ML problem, allowing you to practice with perhaps an easier type of supervised learning algorithm.\n\n4- we can define problem as clustering(unsupervised algorithm) project too.\n\n5- because we love   **Kaggle** :-) .\n\n<a id=\"5\"><\/a> <br>\n### 3-2 Aim\nIt is your job to predict if a passenger survived the sinking of the Titanic or not.  For each PassengerId in the test set, you must predict a 0 or 1 value for the Survived variable.\n\n<a id=\"6\"><\/a> <br>\n### 3-3 Variables\n\n1.  **Age** ==>> Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n2. **Sibsp** ==>> The dataset defines family relations in this way...\n\n    a. Sibling = brother, sister, stepbrother, stepsister\n\n    b. Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n3. **Parch** ==>> The dataset defines family relations in this way...\n\n    a. Parent = mother, father\n\n    b. Child = daughter, son, stepdaughter, stepson\n\n    c. Some children travelled only with a nanny, therefore parch=0 for them.\n\n4. **Pclass** ==>> A proxy for socio-economic status (SES)\n    * 1st = Upper\n    * 2nd = Middle\n    * 3rd = Lower\n5. **Embarked** ==>> nominal datatype \n6. **Name** ==>> nominal datatype . It could be used in feature engineering to derive the gender from title\n7. **Sex** ==>>  nominal datatype \n8. **Ticket** ==>> that have no impact on the outcome variable. Thus, they will be excluded from analysis\n9. **Cabin** ==>>  is a nominal datatype that can be used in feature engineering\n11. **Fare** ==>>  Indicating the fare\n12. **PassengerID ** ==>> have no impact on the outcome variable. Thus, it will be excluded from analysis\n11. **Survival** is ==>> **[dependent variable](http:\/\/www.dailysmarty.com\/posts\/difference-between-independent-and-dependent-variables-in-machine-learning)** , 0 or 1\n\n### 3-3-1  Types of Features\n\n### 3-3-1-1 Categorical\n\nA categorical variable is one that has two or more categories and each value in that feature can be categorised by them. for example, gender is a categorical variable having two categories (male and female). Now we cannot sort or give any ordering to such variables. They are also known as Nominal Variables.\n\n**Categorical Features in the dataset: Sex,Embarked.**\n\n### 3-3-1-2 Ordinal\nAn ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg: If we have a feature like Height with values Tall, Medium, Short, then Height is a ordinal variable. Here we can have a relative sort in the variable.\n\n**Ordinal Features in the dataset: PClass**\n\n### 3-3-1-3 Continous:\nA feature is said to be continous if it can take values between any two points or between the minimum or maximum values in the features column.\n\n**Continous Features in the dataset: Age**\n\n**<< Note >>**\n> You must answer the following question:\nHow does your company expact to use and benfit from your model.","282262ba":"\nWe can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\n\nYou should see 150 instances and 5 attributes:","dd591b84":"<a id=\"16\"><\/a> <br>\n## 6- Exploratory Data Analysis (EDA)\n In this section, you'll learn how to use graphical and numerical techniques to begin uncovering the structure of your data. \n \n* Which variables suggest interesting relationships?\n* Which observations are unusual?\n* Analysis of the features!\nBy the end of the section, you'll be able to answer these questions and more, while generating graphics that are both insightful and beautiful.  then We will review analytical and statistical operations:\n\n*   5-1 Data Collection\n*   5-2 Visualization\n*   5-3 Data Preprocessing\n*   5-4 Data Cleaning\n<img src=\"http:\/\/s9.picofile.com\/file\/8338476134\/EDA.png\">\n\n ","efa5bb2f":"to pop up 5 random rows from the data set, we can use **sample(5)**  function","c7d2f564":"### 5-5-2 Setup\n\nA few tiny adjustments for better **code readability**","5a536743":"you see number of unique item for Species with command below:","5200018b":"After loading the data via **pandas**, we should checkout what the content is, description and via the following:","336c498b":"**<< Note 2 >>**\nin pandas's data frame you can perform some query such as \"where\"","1ebe9df9":"<a id=\"7\"><\/a> <br>\n## 4- Inputs & Outputs\n<a id=\"8\"><\/a> <br>\n### 4-1 Inputs\n**Titanic** is a very popular **Machine Learning**   problem with a **multivariate data set** .\n<a id=\"9\"><\/a> <br>\n### 4-2 Outputs\nYour score is the percentage of passengers you correctly predict. This is known simply as \"accuracy\u201d.\n\n\nThe Outputs should have exactly **2 columns**:\n\nPassengerId (sorted in any order)\nSurvived (contains your binary predictions: 1 for survived, 0 for deceased)\n","74f07514":"<a id=\"17\"><\/a> <br>\n## 6-1 Data Collection\n**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\nHere data is collected by loading the training and testing datasets into Pandas DataFrames.\n","c59bc11c":"to check out last 5 row of the data set, we use tail() function","529a7740":"<a id=\"33\"><\/a> <br>\n## 7-2 Prepare Features & Targets\nFirst of all seperating the data into dependent(Feature) and independent(Target) variables.\n\n**<< Note 4 >>**\n* X==>>Feature\n* y==>>Target","81e80ac4":"## 7-3 Accuracy and precision\n* **precision** : \n\nIn pattern recognition, information retrieval and binary classification, precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, \n* **recall** : \n\nrecall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. \n* **F-score** :\n\nthe F1 score is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n\n**What is the difference between accuracy and precision?**\n\"Accuracy\" and \"precision\" are general terms throughout science. A good way to internalize the difference are the common \"bullseye diagrams\". In machine learning\/statistics as a whole, accuracy vs. precision is analogous to bias vs. variance.","8cb50e1d":"<a id=\"28\"><\/a> <br>\n### 6-2-10 Heatmap","ab196c9d":"<a id=\"30\"><\/a> <br>\n## 6-3 Data Preprocessing\n**Data preprocessing** refers to the transformations applied to our data before feeding it to the algorithm.\n \nData Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\nthere are plenty of steps for data preprocessing and we just listed some of them :\n* removing Target column (id)\n* Sampling (without replacement)\n* Making part of iris unbalanced and balancing (with undersampling and SMOTE)\n* Introducing missing values and treating them (replacing by average values)\n* Noise filtering\n* Data discretization\n* Normalization and standardization\n* PCA analysis\n* Feature selection (filter, embedded, wrapper)","454af8af":"<a id=\"33\"><\/a> <br>\n## 7-4 K-Nearest Neighbours\nIn **Machine Learning**, the **k-nearest neighbors algorithm** (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\nIn k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\nk-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.","b84c4454":"Note the diagonal grouping of some pairs of attributes. This suggests a high correlation and a predictable relationship.","47ab0aa7":"## 2-1 Machine Learning Workflow\n\n\n\nIf you have already read some [machine learning books](https:\/\/towardsdatascience.com\/list-of-free-must-read-machine-learning-books-89576749d2ff). You have noticed that there are different ways to stream data into machine learning.\n\nmost of these books share the following steps (checklist):\n*   Define the Problem(Look at the big picture)\n*   Specify Inputs & Outputs\n*   Data Collection\n*   Exploratory data analysis\n*   Data Preprocessing\n*   Model Design, Training, and Offline Evaluation\n*   Model Deployment, Online Evaluation, and Monitoring\n*   Model Maintenance, Diagnosis, and Retraining","90fb10dd":"to check the first 5 rows of the data set, we can use head(5).","56355286":"It looks like perhaps two of the input variables have a Gaussian distribution. This is useful to note as we can use algorithms that can exploit this assumption.\n\n","4fb51bbe":"to check out how many null info are on the dataset, we can use **isnull().sum()","af51c7ea":"<a id=\"23\"><\/a> <br>\n### 6-2-5 violinplots","c09e7f57":"as you can see in the below in python, it is so easy perform some query on the dataframe:","6405b226":"<a id=\"21\"><\/a> <br>\n### 6-2-3 Histogram\nWe can also create a **histogram** of each input variable to get an idea of the distribution.\n\n","9ea5fd89":"**<< Note 1 >>**\n\n* Each **row** is an observation (also known as : sample, example, instance, record)\n* Each **column** is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)","5776b174":"<a id=\"18\"><\/a> <br>\n## 6-2 Visualization\n**Data visualization**  is the presentation of data in a pictorial or graphical format. It enables decision makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns.\n\nWith interactive visualization, you can take the concept a step further by using technology to drill down into charts and graphs for more detail, interactively changing what data you see and how it\u2019s processed.[SAS]\n\n In this section see  **11 plots** with **matplotlib** and **seaborn** that is listed in the following picture:\n <img src=\"http:\/\/s8.picofile.com\/file\/8338475500\/visualization.jpg\" \/>\n","608e1678":"<a id=\"55\"><\/a> <br>\n\n-----------\n\n# 8- References\n1. [https:\/\/skymind.ai\/wiki\/machine-learning-workflow](https:\/\/skymind.ai\/wiki\/machine-learning-workflow)\n\n1. [Problem-define](https:\/\/machinelearningmastery.com\/machine-learning-in-python-step-by-step\/)\n\n1. [Sklearn](http:\/\/scikit-learn.org\/)\n\n1. [machine-learning-in-python-step-by-step](https:\/\/machinelearningmastery.com\/machine-learning-in-python-step-by-step\/)\n\n1. [Data Cleaning](http:\/\/wp.sigmod.org\/?p=2288)\n\n1. [competitive data science](https:\/\/www.coursera.org\/learn\/competitive-data-science\/)\n\n1. [Machine Learning Certification by Stanford University (Coursera)](https:\/\/www.coursera.org\/learn\/machine-learning\/)\n\n1. [Machine Learning A-Z\u2122: Hands-On Python & R In Data Science (Udemy)](https:\/\/www.udemy.com\/machinelearning\/)\n\n1. [Deep Learning Certification by Andrew Ng from deeplearning.ai (Coursera)](https:\/\/www.coursera.org\/specializations\/deep-learning)\n\n1. [Python for Data Science and Machine Learning Bootcamp (Udemy)](Python for Data Science and Machine Learning Bootcamp (Udemy))\n\n1. [Mathematics for Machine Learning by Imperial College London](https:\/\/www.coursera.org\/specializations\/mathematics-machine-learning)\n\n1. [Deep Learning A-Z\u2122: Hands-On Artificial Neural Networks](https:\/\/www.udemy.com\/deeplearning\/)\n\n1. [Complete Guide to TensorFlow for Deep Learning Tutorial with Python](https:\/\/www.udemy.com\/complete-guide-to-tensorflow-for-deep-learning-with-python\/)\n\n1. [Data Science and Machine Learning Tutorial with Python \u2013 Hands On](https:\/\/www.udemy.com\/data-science-and-machine-learning-with-python-hands-on\/)\n\n1. [Machine Learning Certification by University of Washington](https:\/\/www.coursera.org\/specializations\/machine-learning)\n\n1. [Data Science and Machine Learning Bootcamp with R](https:\/\/www.udemy.com\/data-science-and-machine-learning-bootcamp-with-r\/)\n\n1. [Creative Applications of Deep Learning with TensorFlow](https:\/\/www.class-central.com\/course\/kadenze-creative-applications-of-deep-learning-with-tensorflow-6679)\n\n1. [Neural Networks for Machine Learning](https:\/\/www.class-central.com\/mooc\/398\/coursera-neural-networks-for-machine-learning)\n\n1. [Practical Deep Learning For Coders, Part 1](https:\/\/www.class-central.com\/mooc\/7887\/practical-deep-learning-for-coders-part-1)\n\n1. [Machine Learning](https:\/\/www.cs.ox.ac.uk\/teaching\/courses\/2014-2015\/ml\/index.html)\n\n1. [https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic](https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic)\n\n1. [https:\/\/www.kaggle.com\/mrisdal\/exploring-survival-on-the-titanic](https:\/\/www.kaggle.com\/mrisdal\/exploring-survival-on-the-titanic)\n\n1. [https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)\n\n1. [https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)\n\n1. [https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n\n1. [Top 28 Cheat Sheets for Machine Learning](https:\/\/www.analyticsvidhya.com\/blog\/2017\/02\/top-28-cheat-sheets-for-machine-learning-data-science-probability-sql-big-data\/)\n-------------\n","071b8618":"<a id=\"12\"><\/a> <br>\n## 5-2 Kaggle Kernel\nKaggle kernel is an environment just like you use jupyter notebook, it's an **extension** of the where in you are able to carry out all the functions of jupyter notebooks plus it has some added tools like forking et al.","ce1786ce":" <a id=\"1\"><\/a> <br>\n## 1- Introduction\n## 1-1 Courses\n\n1. [Machine Learning Certification by Stanford University (Coursera)](https:\/\/www.coursera.org\/learn\/machine-learning\/)\n\n2. [Machine Learning A-Z\u2122: Hands-On Python & R In Data Science (Udemy)](https:\/\/www.udemy.com\/machinelearning\/)\n\n3. [Deep Learning Certification by Andrew Ng from deeplearning.ai (Coursera)](https:\/\/www.coursera.org\/specializations\/deep-learning)\n\n4. [Python for Data Science and Machine Learning Bootcamp (Udemy)](Python for Data Science and Machine Learning Bootcamp (Udemy))\n\n5. [Mathematics for Machine Learning by Imperial College London](https:\/\/www.coursera.org\/specializations\/mathematics-machine-learning)\n\n6. [Deep Learning A-Z\u2122: Hands-On Artificial Neural Networks](https:\/\/www.udemy.com\/deeplearning\/)\n\n7. [Complete Guide to TensorFlow for Deep Learning Tutorial with Python](https:\/\/www.udemy.com\/complete-guide-to-tensorflow-for-deep-learning-with-python\/)\n\n8. [Data Science and Machine Learning Tutorial with Python \u2013 Hands On](https:\/\/www.udemy.com\/data-science-and-machine-learning-with-python-hands-on\/)\n\n9. [Machine Learning Certification by University of Washington](https:\/\/www.coursera.org\/specializations\/machine-learning)\n\n10. [Data Science and Machine Learning Bootcamp with R](https:\/\/www.udemy.com\/data-science-and-machine-learning-bootcamp-with-r\/)\n11. [Creative Applications of Deep Learning with TensorFlow](https:\/\/www.class-central.com\/course\/kadenze-creative-applications-of-deep-learning-with-tensorflow-6679)\n12. [Neural Networks for Machine Learning](https:\/\/www.class-central.com\/mooc\/398\/coursera-neural-networks-for-machine-learning)\n13. [Practical Deep Learning For Coders, Part 1](https:\/\/www.class-central.com\/mooc\/7887\/practical-deep-learning-for-coders-part-1)\n14. [Machine Learning](https:\/\/www.cs.ox.ac.uk\/teaching\/courses\/2014-2015\/ml\/index.html)\n\n## 1-2 Kaggle kernels\n\n1. [https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic](https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic)\n\n2. [https:\/\/www.kaggle.com\/mrisdal\/exploring-survival-on-the-titanic](https:\/\/www.kaggle.com\/mrisdal\/exploring-survival-on-the-titanic)\n\n3. [https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)\n\n4. [https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)\n\n5. [https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n\n## 1-3 E-books\n1. [Probability and Statistics for Programmers](http:\/\/www.greenteapress.com\/thinkstats\/)\n2. [Bayesian Reasoning and Machine Learning](http:\/\/web4.cs.ucl.ac.uk\/staff\/D.Barber\/textbook\/091117.pdf)\n2. [An Introduction to Statistical Learning](http:\/\/www-bcf.usc.edu\/~gareth\/ISL\/)\n2. [Understanding Machine Learning](http:\/\/www.cs.huji.ac.il\/~shais\/UnderstandingMachineLearning\/index.html)\n2. [A Programmer\u2019s Guide to Data Mining](http:\/\/guidetodatamining.com\/)\n2. [Mining of Massive Datasets](http:\/\/infolab.stanford.edu\/~ullman\/mmds\/book.pdf)\n2. [A Brief Introduction to Neural Networks](http:\/\/www.dkriesel.com\/_media\/science\/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf)\n2. [Deep Learning](http:\/\/www.deeplearningbook.org\/)\n2. [Natural Language Processing with Python](https:\/\/www.researchgate.net\/publication\/220691633_Natural_Language_Processing_with_Python)\n2. [Machine Learning Yearning](http:\/\/www.mlyearning.org\/)\n\n## 1-4 Cheat Sheets\n\n1. [Quick Guide to learn Python for Data Science ](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/Data-Science-in-Python.pdf)\n1. [Python for Data Science Cheat sheet ](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/beginners_python_cheat_sheet.pdf)\n1. [Python For Data Science Cheat Sheet NumPy](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/Numpy_Python_Cheat_Sheet.pdf)\n1. [Exploratory Data Analysis in Python]()\n1. [Data Exploration using Pandas in Python](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/Data-Exploration-in-Python.pdf)\n1. [Data Visualisation in Python](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/data-visualisation-infographics1.jpg)\n1. [Python For Data Science Cheat Sheet Bokeh](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/Python_Bokeh_Cheat_Sheet.pdf)\n1. [Cheat Sheet: Scikit Learn ](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/Scikit-Learn-Infographic.pdf)\n1. [MLalgorithms CheatSheet](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/MLalgorithms-.pdf)\n1. [Probability Basics  Cheat Sheet ](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/probability_cheatsheet.pdf)\n\n","dafcfce6":"### 6-2-12 Factorplot","5633d568":"<a id=\"25\"><\/a> <br>\n###  6-2-7 kdeplot","f79f2e07":"<a id=\"15\"><\/a> <br>\n## 5-5 Loading Packages\n","ff9b21d2":"<a id=\"20\"><\/a> <br>\n### 6-2-2 Box\nIn descriptive statistics, a **box plot** or boxplot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram.[wikipedia]","1f0e3412":"<a id=\"2\"><\/a> <br>\n## 2- Machine Learning\nMachine Learning is a field of study that gives computers the ability to learn without being explicitly programmed.\n\n**Arthur\tSamuel, 1959**","8e40bb06":"to give a statistical summary about the dataset, we can use **describe()","1145f548":"<a id=\"22\"><\/a> <br>\n### 6-2-4 Multivariate Plots\nNow we can look at the interactions between the variables.\n\nFirst, let\u2019s look at scatterplots of all pairs of attributes. This can be helpful to spot structured relationships between input variables.","fe584a6d":"From the plot, we can see that the species setosa is separataed from the other two across all feature combinations\n\nWe can also replace the histograms shown in the diagonal of the pairplot by kde.","06a95e81":"<a id=\"32\"><\/a> <br>\n## 7- Model Deployment\nIn this section have been applied plenty of  ** learning algorithms** that play an important rule in your experiences and improve your knowledge in case of ML technique.\n\n> **<< Note 3 >>** : The results shown here may be slightly different for your analysis because, for example, the neural network algorithms use random number generators for fixing the initial value of the weights (starting points) of the neural networks, which often result in obtaining slightly different (local minima) solutions each time you run the analysis. Also note that changing the seed for the random number generator used to create the train, test, and validation samples can change your results.","7ac763c8":"<a id=\"26\"><\/a> <br>\n### 6-2-8 jointplot","d9ee906b":"Adapted from [mjbahmani's tutorial](https:\/\/www.kaggle.com\/mjbahmani\/the-data-scientist-s-toolbox-tutorial).\n\n## Notebook  Contents\n*   1-  [Introduction](#1)\n    *   1-  [Courses](#1)\n    *   1-  [Kaggle kernels](#1)\n    *   1-  [E-books](#1)\n    *   1-  [Cheat Sheet](#1)\n*   2- [Machine learning](#2)\n*       2- [Machine learning workflow](#2)\n*       2-1 [Real world Application Vs Competitions](#2)\n\n*   3- [Problem Definition](#3)\n*       3-1 [Problem feature](#4)\n*       3-2 [Aim](#5)\n*       3-3 [Variables](#6)\n*   4-[ Inputs & Outputs](#7)\n*   4-1 [Inputs ](#8)\n*   4-2 [Outputs](#9)\n*   5- [Installation](#10)\n*       5-1 [ jupyter notebook](#11)\n*       5-2[ kaggle kernel](#12)\n*       5-3 [Colab notebook](#13)\n*       5-4 [install python & packages](#14)\n*       5-5 [Loading Packages](#15)\n*   6- [Exploratory data analysis](#16)\n*       6-1 [Data Collection](#17)\n*       6-2 [Visualization](#18)\n*           6-2-1 [Scatter plot](#19)\n*           6-2-2 [Box](#20)\n*           6-2-3 [Histogram](#21)\n*           6-2-4 [Multivariate Plots](#22)\n*           6-2-5 [Violinplots](#23)\n*           6-2-6 [Pair plot](#24)\n*           6-2-7 [Kde plot](#25)\n*           6-2-8 [Joint plot](#26)\n*           6-2-9 [Andrews curves](#27)\n*           6-2-10 [Heatmap](#28)\n*           6-2-11 [Radviz](#29)\n*       6-3 [Data Preprocessing](#30)\n*       6-4 [Data Cleaning](#31)\n*   7- [Model Deployment](#32)\n*       7-1[ KNN](#33)\n*       7-2 [Radius Neighbors Classifier](#34)\n*       7-3 [Logistic Regression](#35)\n*       7-4 [Passive Aggressive Classifier](#36)\n*       7-5 [Naive Bayes](#37)\n*       7-6 [MultinomialNB](#38)\n*       7-7 [BernoulliNB](#39)\n*       7-8 [SVM](#40)\n*       7-9 [Nu-Support Vector Classification](#41)\n*       7-10 [Linear Support Vector Classification](#42)\n*       7-11 [Decision Tree](#43)\n*       7-12 [ExtraTreeClassifier](#44)\n*       7-13 [Neural network](#45)\n*            7-13-1 [What is a Perceptron?](#45)\n*       7-14 [RandomForest](#46)\n*       7-15 [Bagging classifier ](#47)\n*       7-16 [AdaBoost classifier](#48)\n*       7-17 [Gradient Boosting Classifier](#49)\n*       7-18 [Linear Discriminant Analysis](#50)\n*       7-19 [Quadratic Discriminant Analysis](#51)\n*       7-20 [Kmeans](#52)\n*       7-21 [Backpropagation](#53)\n*   8- [Conclusion](#54)\n*   9- [References](#55)","2d2a9ec2":"###  6-2-11 Bar Plot","bb1b242e":"## 6-3-1 Features\nFeatures:\n* numeric\n* categorical\n* ordinal\n* datetime\n* coordinates\n\nfind the type of features in titanic dataset\n<img src=\"http:\/\/s9.picofile.com\/file\/8339959442\/titanic.png\" height=\"700\" width=\"600\" \/>","2b658d63":"<a id=\"19\"><\/a> <br>\n### 6-2-1 Scatter plot\n\nScatter plot Purpose To identify the type of relationship (if any) between two quantitative variables\n\n\n","5ddd11b9":"<a id=\"31\"><\/a> <br>\n## 6-4 Data Cleaning\nWhen dealing with real-world data, dirty data is the norm rather than the exception. We continuously need to predict correct values, impute missing ones, and find links between various data artefacts such as schemas and records. We need to stop treating data cleaning as a piecemeal exercise (resolving different types of errors in isolation), and instead leverage all signals and resources (such as constraints, available statistics, and dictionaries) to accurately predict corrective actions.\n\nThe primary goal of data cleaning is to detect and remove errors and **anomalies** to increase the value of data in analytics and decision making. While it has been the focus of many researchers for several years, individual problems have been addressed separately. These include missing value imputation, outliers detection, transformations, integrity constraints violations detection and repair, consistent query answering, deduplication, and many other related problems such as profiling and constraints mining.[8]","0af394f4":"<a id=\"10\"><\/a> <br>\n## 5-Installation\n#### Windows:\n* Anaconda (from https:\/\/www.continuum.io) is a free Python distribution for SciPy stack. It is also available for Linux and Mac.\n* Canopy (https:\/\/www.enthought.com\/products\/canopy\/) is available as free as well as commercial distribution with full SciPy stack for Windows, Linux and Mac.\n* Python (x,y) is a free Python distribution with SciPy stack and Spyder IDE for Windows OS. (Downloadable from http:\/\/python-xy.github.io\/)\n\n#### Linux:\nPackage managers of respective Linux distributions are used to install one or more packages in SciPy stack.\n\nFor Ubuntu Users:\nsudo apt-get install python-numpy python-scipy python-matplotlibipythonipythonnotebook\npython-pandas python-sympy python-nose","0d74cc2e":"<a id=\"13\"><\/a> <br>\n## 5-3 Colab notebook\n**Colaboratory** is a research tool for machine learning education and research. It\u2019s a Jupyter notebook environment that requires no setup to use.\n### 5-3-1 What browsers are supported?\nColaboratory works with most major browsers, and is most thoroughly tested with desktop versions of Chrome and Firefox.\n### 5-3-2 Is it free to use?\nYes. Colaboratory is a research project that is free to use.\n### 5-3-3 What is the difference between Jupyter and Colaboratory?\nJupyter is the open source project on which Colaboratory is based. Colaboratory allows you to use and share Jupyter notebooks with others without having to download, install, or run anything on your own computer other than a browser.","3d7f1dbd":"### 6-2-13 distplot","8573036d":"## 7-1 Families of ML algorithms\nThere are several categories for machine learning algorithms, below are some of these categories:\n* Linear\n    * Linear Regression\n    * Logistic Regression\n    * Support Vector Machines\n* Tree-Based\n    * Decision Tree\n    * Random Forest\n    * GBDT\n* KNN\n* Neural Networks\n\n-----------------------------\nAnd if we  want to categorize ML algorithms with the type of learning, there are below type:\n* Classification\n\n    * k-Nearest \tNeighbors\n    * LinearRegression\n    * SVM\n    * DT \n    * NN\n    \n* clustering\n\n    * K-means\n    * HCA\n    * Expectation Maximization\n    \n* Visualization \tand\tdimensionality \treduction:\n\n    * Principal \tComponent \tAnalysis(PCA)\n    * Kernel PCA\n    * Locally -Linear\tEmbedding \t(LLE)\n    * t-distributed\tStochastic\tNeighbor\tEmbedding \t(t-SNE)\n    \n* Association \trule\tlearning\n\n    * Apriori\n    * Eclat\n* Semisupervised learning\n* Reinforcement Learning\n    * Q-learning\n* Batch learning & Online learning\n* Ensemble  Learning\n\n**<< Note >>**\n> Here is no method which outperforms all others for all tasks\n\n"}}