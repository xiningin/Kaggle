{"cell_type":{"9a659889":"code","71752be6":"code","af4eabf0":"code","fd1c59e7":"code","84735595":"code","12815c3c":"code","97f58958":"code","f8d35805":"code","6e9a033f":"markdown","26673153":"markdown","447d5676":"markdown","084f47a7":"markdown","37a523ab":"markdown","4a6f8f30":"markdown","bc7d3f03":"markdown","d402613f":"markdown"},"source":{"9a659889":"import tensorflow as tf\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\nprint('TensorFlow version:', tf.__version__)","71752be6":"(X_train , y_train) , (X_test , y_test) = tf.keras.datasets.mnist.load_data()\n\nX_train = np.reshape(X_train , (X_train.shape[0] , 28*28))\/255.\nX_test = np.reshape(X_test , (X_test.shape[0] , 28*28))\/255.\n\ny_train = tf.keras.utils.to_categorical(y_train)\ny_test = tf.keras.utils.to_categorical(y_test)\n\n\nprint(X_train.shape , y_train.shape)","af4eabf0":"model = tf.keras.models.Sequential([tf.keras.layers.Dense(16 , activation = 'relu' , input_shape = (784,)),\n                                    tf.keras.layers.Dense(10 , activation='softmax')])\n\nopt = tf.keras.optimizers.SGD(learning_rate=0.01)\n\nmodel.compile(loss = 'categorical_crossentropy' , optimizer = opt , metrics = ['accuracy'])\n\nmodel.summary()","fd1c59e7":"class CustomCallback(tf.keras.callbacks.Callback):\n    \n    \n    def __init__(self , frac):\n        \n        super(CustomCallback , self).__init__\n    \n        self.frac = frac\n\n        with open('log.txt' , 'w') as f:\n            f.write('start logging...\\n')\n\n\n    def on_train_end(self , logs = None):\n        \n        with open('log.txt' , 'a') as f:\n            f.write('End logging\\n')\n\n    def on_epoch_begin(self , epoch , logs = None):\n        \n        lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n        lr *=  self.frac\n        tf.keras.backend.set_value(self.model.optimizer.learning_rate , lr)\n        \n        with open('log.txt' , 'a') as f:\n            f.write('***** at epoch {:02d} , learning rate changed to {:.4f}\\n'.format(epoch , lr))\n\n         \n    def on_epoch_end(self , epoch , logs = None):\n        \n        val_acc = logs.get('val_accuracy')\n        train_acc = logs.get('accuracy')\n\n        with open('log.txt' , 'a') as f:\n            f.write('at epoch {:02d} , training accuracy : {:.3f} , validation accuracy : {:.3f}\\n'.format(epoch ,\n                                                                                                           train_acc ,\n                                                                                                           val_acc))\n","84735595":"myCallback = CustomCallback(0.9) ","12815c3c":"history = model.fit(X_train , y_train ,\n              validation_data = (X_test , y_test),\n              epochs = 10 , callbacks = [myCallback])","97f58958":"try:\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\nexcept KeyError:\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\nplt.title('Accuracy vs. epochs')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show()","f8d35805":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss') \nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show()","6e9a033f":"# building the model","26673153":"in this call back we will create a text file that contains training report (training and validation accuracy) \nand and also learning rate value in each epoch","447d5676":"# ploting Accuracy","084f47a7":"# ploting Loss","37a523ab":"# training the model","4a6f8f30":"# downloading and processing data","bc7d3f03":"# importing libraries","d402613f":"# Custom callback"}}