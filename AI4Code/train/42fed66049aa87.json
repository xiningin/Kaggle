{"cell_type":{"4aa816f0":"code","2ff232aa":"code","f985b516":"code","3f6427a2":"code","c5ceb05a":"code","c1e7bdd1":"code","aa56e92f":"code","e05b0c29":"code","89d4581a":"code","af7d8ca2":"code","e018c139":"code","54f79d93":"code","69899d43":"code","1e0adfcc":"code","fa45d28b":"code","630971de":"code","83a2dc59":"code","141fe244":"code","bbf50387":"code","38561d23":"code","51c2a030":"code","32f04b4c":"code","225ad611":"code","52a9fe6c":"code","ceed50df":"code","62a49c2e":"code","20df8bee":"code","9191605e":"code","9dafe1d7":"markdown","61a01a33":"markdown","24f0fa49":"markdown","6d7f310f":"markdown","2110ee3b":"markdown","a6fffc83":"markdown","04e3f0ea":"markdown"},"source":{"4aa816f0":"#Notebook\nimport os\nimport json\nfrom zipfile import ZipFile\nimport shutil\nfrom glob import glob\nfrom tqdm import tqdm\n\n#Data\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\n#Image\nimport cv2 as cv\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\n#keras\nimport tensorflow as tf\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model, load_model, save_model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","2ff232aa":"!nvidia-smi -L","f985b516":"IMG_SIZE = (256,256)","3f6427a2":"train_files = []\nmask_files = glob(\"..\/input\/lgg-mri-segmentation\/kaggle_3m\/*\/*_mask*\")\n\nfor f in tqdm(mask_files):\n    train_files.append(f.replace('_mask', ''))","c5ceb05a":"df = pd.DataFrame({\"image_path\": train_files, \"mask_path\":mask_files})","c1e7bdd1":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.15, shuffle = True)\ndf_train, df_val = train_test_split(df_train, test_size=0.15, shuffle = True)\nprint(df_train.values.shape)\nprint(df_val.values.shape)\nprint(df_test.values.shape)","aa56e92f":"def adjust(img,mask):\n    #normalization\n    img = img \/ 255.\n\n    #mask\n    mask = mask \/ 255.\n    mask[mask > 0.5] = 1\n    mask[mask <= 0.5] = 0\n\n    return (img, mask)","e05b0c29":"train_gen_aug = dict(rotation_range=0.2,\n                        width_shift_range=0.05,\n                        height_shift_range=0.05,\n                        shear_range=0.05,\n                        zoom_range=0.05,\n                        horizontal_flip=True,\n                        fill_mode='nearest')","89d4581a":"def train_generator(dataframe, batch_size, aug_dict, \n                    image_color_mode = \"rgb\",\n                    mask_color_mode = \"grayscale\",\n                    image_save_prefix = \"image\",\n                    mask_save_prefix = \"mask\",\n                    save_to_dir = None,\n                    target_size = (256,256),\n                    seed = 1):\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n\n  #image\n    image_generator = image_datagen.flow_from_dataframe(\n          dataframe,\n          x_col = \"image_path\",\n          target_size = target_size,\n          batch_size = batch_size,\n          class_mode = None,\n          color_mode = image_color_mode,\n          save_to_dir = save_to_dir,\n          save_prefix = image_save_prefix,\n          seed = seed )\n\n      #mask\n    mask_generator = mask_datagen.flow_from_dataframe(\n          dataframe,\n          x_col = \"mask_path\",\n          target_size = target_size,\n          batch_size = batch_size,\n          class_mode = None,\n          color_mode = mask_color_mode,\n          save_to_dir = save_to_dir,\n          save_prefix = mask_save_prefix,\n          seed = seed)\n  \n  #combining the generators to make a whole generator\n    train_gen = zip(image_generator, mask_generator)\n\n  #data adjustment\n    for (img, mask) in train_gen:\n        img, mask = adjust(img, mask)\n        yield (img,mask)","af7d8ca2":"def naive_inception_module(layer_in, filters):\n    f1 = f2 = f3 = filters\n    # 1x1 conv\n    conv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(layer_in)\n    # 3x3 conv\n    conv3 = Conv2D(f2, (3,3), padding='same', activation='relu')(layer_in)\n    # 5x5 conv\n    conv5 = Conv2D(f3, (5,5), padding='same', activation='relu')(layer_in)\n    # 7x7 conv\n    #   conv7 = Conv2D(f3, (7,7), padding='same', activation='relu')(layer_in)\n    # 3x3 max pooling\n    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n    # concatenate filters, assumes filters\/channels last\n    layer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n    return layer_out","e018c139":"def unet(input_size = (256,256,3)):\n    #Input Layer\n    inputs = Input(input_size)\n\n    #Encoder network\n    conv1 = naive_inception_module(inputs,16)\n    pool1 = MaxPooling2D(pool_size= (2,2))(conv1)\n\n    conv2 = naive_inception_module(pool1,32)\n    pool2 = MaxPooling2D(pool_size= (2,2))(conv2)\n\n    conv3 = naive_inception_module(pool2,64)\n    pool3 = MaxPooling2D(pool_size= (2,2))(conv3)\n\n    conv4 = naive_inception_module(pool3,128)\n    pool4 = MaxPooling2D(pool_size= (2,2))(conv4)\n\n    conv5 = naive_inception_module(pool4,256)\n\n    #decoder network\n    up6 = Conv2DTranspose(128, (2,2), strides = (2,2), padding = \"same\")(conv5)\n    up6 = concatenate([up6, conv4], axis = 3)\n    conv6 = naive_inception_module(up6, 128)\n\n    up7 = Conv2DTranspose(64, (2,2), strides = (2,2), padding = \"same\")(conv6)\n    up7 = concatenate([up7, conv3], axis = 3)\n    conv7 = naive_inception_module(up7, 64)\n\n    up8 = Conv2DTranspose(32, (2,2), strides = (2,2), padding = \"same\")(conv7)\n    up8 = concatenate([up8, conv2], axis = 3)\n    conv8 = naive_inception_module(up8, 32)\n\n    up9 = Conv2DTranspose(16, (2,2), strides = (2,2), padding = \"same\")(conv8)\n    up9 = concatenate([up9, conv1], axis = 3)\n    conv9 = naive_inception_module(up9, 16)\n\n    #output layer\n    conv10 = Conv2D(1, (1,1), activation= \"sigmoid\")(conv9)\n\n    model = Model(inputs = [inputs], outputs = [conv10])\n\n    return model","54f79d93":"BATCH_SIZE = 32","69899d43":"train_gen = train_generator(df_train, batch_size= BATCH_SIZE, \n                            aug_dict = train_gen_aug,\n                            target_size = IMG_SIZE)\ntest_gen = train_generator(df_val, batch_size = BATCH_SIZE,\n                           aug_dict = dict(),\n                           target_size = IMG_SIZE)\nmodel = unet((IMG_SIZE[0], IMG_SIZE[1], 3))","1e0adfcc":"EPOCHS = 150\nBATCH_SIZE = 32\nlearning_rate = 0.05","fa45d28b":"from keras.losses import binary_crossentropy\n\nbeta = 0.25\nalpha = 0.25\ngamma = 2\nepsilon = 1e-5\nsmooth = 1\n\n\nclass Semantic_loss_functions(object):\n    def __init__(self):\n        print (\"semantic loss functions initialized\")\n\n    def dice_coef(self, y_true, y_pred):\n        y_true_f = K.flatten(y_true)\n        y_pred_f = K.flatten(y_pred)\n        intersection = K.sum(y_true_f * y_pred_f)\n        return (2. * intersection + K.epsilon()) \/ (\n                    K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n\n    def sensitivity(self, y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        return true_positives \/ (possible_positives + K.epsilon())\n\n    def specificity(self, y_true, y_pred):\n        true_negatives = K.sum(\n            K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n        possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n        return true_negatives \/ (possible_negatives + K.epsilon())\n\n    def convert_to_logits(self, y_pred):\n        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(),\n                                  1 - tf.keras.backend.epsilon())\n        return tf.math.log(y_pred \/ (1 - y_pred))\n\n    def weighted_cross_entropyloss(self, y_true, y_pred):\n        y_pred = self.convert_to_logits(y_pred)\n        pos_weight = beta \/ (1 - beta)\n        loss = tf.nn.weighted_cross_entropy_with_logits(y_true,y_pred,pos_weight)\n        return tf.reduce_mean(loss)\n\n    def focal_loss_with_logits(self, logits, targets, alpha, gamma, y_pred):\n        weight_a = alpha * (1 - y_pred) ** gamma * targets\n        weight_b = (1 - alpha) * y_pred ** gamma * (1 - targets)\n\n        return (tf.math.log1p(tf.exp(-tf.abs(logits))) + tf.nn.relu(\n            -logits)) * (weight_a + weight_b) + logits * weight_b\n\n    def focal_loss(self, y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(),\n                                  1 - tf.keras.backend.epsilon())\n        logits = tf.math.log(y_pred \/ (1 - y_pred))\n\n        loss = self.focal_loss_with_logits(logits=logits, targets=y_true,\n                                      alpha=alpha, gamma=gamma, y_pred=y_pred)\n\n        return tf.reduce_mean(loss)\n\n    def depth_softmax(self, matrix):\n        sigmoid = lambda x: 1 \/ (1 + K.exp(-x))\n        sigmoided_matrix = sigmoid(matrix)\n        softmax_matrix = sigmoided_matrix \/ K.sum(sigmoided_matrix, axis=0)\n        return softmax_matrix\n\n    def dice_coefficient(self, y_true, y_pred):\n        smooth = 1.\n        y_true_f = K.flatten(y_true)\n        y_pred_f = K.flatten(y_pred)\n        intersection = K.sum(y_true_f * y_pred_f)\n        score = (2. * intersection + smooth) \/ (\n                    K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n        return score\n\n    def dice_loss(self, y_true, y_pred):\n        loss = 1 - self.dice_coefficient(y_true, y_pred)\n        return loss\n\n    def bce_dice_loss(self, y_true, y_pred):\n        loss = binary_crossentropy(y_true, y_pred) + \\\n               self.dice_loss(y_true, y_pred)\n        return loss \/ 2.0\n\n    def confusion(self, y_true, y_pred):\n        smooth = 1\n        y_pred_pos = K.clip(y_pred, 0, 1)\n        y_pred_neg = 1 - y_pred_pos\n        y_pos = K.clip(y_true, 0, 1)\n        y_neg = 1 - y_pos\n        tp = K.sum(y_pos * y_pred_pos)\n        fp = K.sum(y_neg * y_pred_pos)\n        fn = K.sum(y_pos * y_pred_neg)\n        prec = (tp + smooth) \/ (tp + fp + smooth)\n        recall = (tp + smooth) \/ (tp + fn + smooth)\n        return prec, recall\n\n    def true_positive(self, y_true, y_pred):\n        smooth = 1\n        y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n        y_pos = K.round(K.clip(y_true, 0, 1))\n        tp = (K.sum(y_pos * y_pred_pos) + smooth) \/ (K.sum(y_pos) + smooth)\n        return tp\n\n    def true_negative(self, y_true, y_pred):\n        smooth = 1\n        y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n        y_pred_neg = 1 - y_pred_pos\n        y_pos = K.round(K.clip(y_true, 0, 1))\n        y_neg = 1 - y_pos\n        tn = (K.sum(y_neg * y_pred_neg) + smooth) \/ (K.sum(y_neg) + smooth)\n        return tn\n\n    def tversky(self, y_true, y_pred):\n        y_true_pos = K.flatten(y_true)\n        y_pred_pos = K.flatten(y_pred)\n        true_pos = K.sum(y_true_pos * y_pred_pos)\n        false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n        false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n        alpha = 0.7\n        return (true_pos + smooth) \/ (true_pos + alpha * false_neg + (\n                    1 - alpha) * false_pos + smooth)\n\n    def tversky_loss(self, y_true, y_pred):\n        return 1 - self.tversky(y_true, y_pred)\n\n    def focal_tversky(self, y_true, y_pred):\n        pt_1 = self.tversky(y_true, y_pred)\n        gamma = 0.75\n        return K.pow((1 - pt_1), gamma)\n\n    def log_cosh_dice_loss(self, y_true, y_pred):\n        x = self.dice_loss(y_true, y_pred)\n        return tf.math.log((tf.exp(x) + tf.exp(-x)) \/ 2.0)\n    \n    def loss(self, y_true, y_pred):\n        return 0.5*s.focal_tversky(y_true, y_pred) + 0.2*self.weighted_cross_entropyloss(y_true, y_pred) + 0.3*surface_loss(y_true, y_pred) \n\ns = Semantic_loss_functions()","630971de":"decay_rate = learning_rate \/ EPOCHS\nopt = Adam(learning_rate=learning_rate, epsilon= 0.1, decay=decay_rate, amsgrad=False)","83a2dc59":"model.compile(optimizer = opt, loss = s.focal_tversky, metrics = [s.tversky, s.dice_coefficient, s.sensitivity, s.specificity ])","141fe244":"model.summary()","bbf50387":"#callbacks\nearlystopping = EarlyStopping(monitor='val_loss',\n                              mode='min', \n                              verbose=1, \n                              patience=20\n                             )\n# save the best model with lower validation loss\ncheckpointer = ModelCheckpoint(filepath=\"unet_2_12.hdf5\", \n                               verbose=1, \n                               save_best_only=True\n                              )\nreduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                              mode='min',\n                              verbose=1,\n                              patience=10,\n                              min_delta=0.0001,\n                              factor=0.2\n                             )","38561d23":"history = model.fit(train_gen,\n          steps_per_epoch = len(df_train)\/BATCH_SIZE,\n          epochs = EPOCHS,\n          callbacks = [checkpointer, earlystopping, reduce_lr],\n          validation_data = test_gen,\n          validation_steps= len(df_val)\/ BATCH_SIZE) ","51c2a030":"model.save_weights(\"unet_weights_2_12.h5\")","32f04b4c":"test_gen = train_generator(df_test, BATCH_SIZE,\n                                dict(),\n                                target_size=IMG_SIZE)\nresults = model.evaluate(test_gen, steps=len(df_test) \/ BATCH_SIZE)\nprint(\"Test loss: \",results[0])\nprint(\"Test tversky: \",results[1])\nprint(\"Test Dice Coefficent: \",results[2])\nprint(\"Test sensitivity: \",results[3])\nprint(\"Test specificity: \",results[4])","225ad611":"import cv2\nfor i in range(30):\n    index=np.random.randint(1,len(df_test.index))\n    img = cv.imread(df_test['image_path'].iloc[index])\n    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n    img = cv.resize(img ,IMG_SIZE)\n    img = img \/ 255\n    img = img[np.newaxis, :, :, :]\n    pred=model.predict(img)\n    img = np.squeeze(img)\n\n    original = img.copy()\n    fig, ax = plt.subplots(1,3,figsize = (15,5))\n    ax[0].imshow(original)\n    ax[0].set_xlabel('Original Image')\n    \n    main = original.copy()\n    label = cv.imread(df_test['mask_path'].iloc[index])\n    sample = np.array(np.squeeze(label), dtype = np.uint8)\n    contours, hier = cv.findContours(sample[:,:,0],cv.RETR_LIST,cv.CHAIN_APPROX_SIMPLE)\n    sample_over_gt = cv.drawContours(main, contours, -1,[0,255,0], thickness=-1)\n    ax[1].imshow(sample_over_gt)\n    ax[1].set_xlabel('Ground Truth')\n    \n    main = original.copy()\n    sample = np.array(np.squeeze(pred) > 0.5, dtype = np.uint8)\n    contours, hier = cv2.findContours(sample,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n    sample_over_pr = cv2.drawContours(main, contours, -1, [0,255,0], thickness=-1)\n    ax[2].imshow(sample_over_pr)\n    ax[2].set_xlabel(\"AI Prediction\")\n\nplt.show()","52a9fe6c":"def plot_segm_history(history,i,metrics=[\"iou\", \"val_iou\"], losses=[\"loss\", \"val_loss\"]):\n    \"\"\"[summary]\n    \n    Args:\n        history ([type]): [description]\n        metrics (list, optional): [description]. Defaults to [\"iou\", \"val_iou\"].\n        losses (list, optional): [description]. Defaults to [\"loss\", \"val_loss\"].\n    \"\"\"\n    # summarize history for iou\n    plt.figure(figsize=(12, 6))\n    for metric in metrics:\n        plt.plot(history.history[metric], linewidth=3)\n    plt.suptitle(\"metrics over epochs\", fontsize=20)\n    plt.ylabel(\"metric\", fontsize=20)\n    plt.xlabel(\"epoch\", fontsize=20)\n    # plt.yticks(np.arange(0.3, 1, step=0.02), fontsize=35)\n    # plt.xticks(fontsize=35)\n    plt.legend(metrics, loc=\"center right\", fontsize=15)\n    plt.savefig(f\"unet_metrics_{i}.png\")\n    plt.show()\n    # summarize history for loss\n    plt.figure(figsize=(12, 6))\n    for loss in losses:\n        plt.plot(history.history[loss], linewidth=3)\n    plt.suptitle(\"loss over epochs\", fontsize=20)\n    plt.ylabel(\"loss\", fontsize=20)\n    plt.xlabel(\"epoch\", fontsize=20)\n    # plt.yticks(np.arange(0, 0.2, step=0.005), fontsize=35)\n    # plt.xticks(fontsize=35)\n    plt.legend(losses, loc=\"center right\", fontsize=15)\n    plt.savefig(f\"unet_loss_{i}.png\")\n    plt.show()","ceed50df":"plot_segm_history(\n    history,0, # required - keras training history object\n    metrics=['dice_coefficient', 'val_dice_coefficient'], # optional - metrics names to plot\n    losses=['loss', 'val_loss']) # optional - loss names to plot","62a49c2e":"plot_segm_history(\n    history,1, # required - keras training history object\n    metrics=['sensitivity', 'val_sensitivity'], # optional - metrics names to plot\n    losses=['loss', 'val_loss']) # optional - loss names to plot","20df8bee":"plot_segm_history(\n    history,2, # required - keras training history object\n    metrics=['specificity', 'val_specificity'], # optional - metrics names to plot\n    losses=['loss', 'val_loss']) # optional - loss names to plot","9191605e":"plot_segm_history(\n    history,3, # required - keras training history object\n    metrics=['tversky', 'val_tversky'], # optional - metrics names to plot\n    losses=['loss', 'val_loss']) # optional - loss names to plot","9dafe1d7":"Making Train and valid dataframes\n\n","61a01a33":"If GPU ready or not","24f0fa49":"Importing Necessary Libraries","6d7f310f":"Metrics and Loss","2110ee3b":"Data Augmentor dict for train_generator","a6fffc83":"Training","04e3f0ea":"Callbacks"}}