{"cell_type":{"df6b5031":"code","67de6177":"code","c0cd46b1":"code","df2f871b":"code","fcb6ebe9":"code","50657706":"code","6e8bf3cb":"code","01e49022":"code","b86af1ee":"code","d3f5c76f":"code","54e3006d":"code","694d1d74":"code","aa12b9b8":"code","9aa6d0bc":"code","57683da1":"code","ddf96d4a":"code","1a7cc16a":"code","80966963":"code","208b2505":"code","9df2ef61":"code","e40a24ff":"code","9abed4c5":"code","613363d3":"code","a8514dae":"code","b03d3b89":"code","e8034b39":"code","d13b097e":"code","922205e1":"code","0ae04c9a":"code","0c79bcfa":"code","e7644804":"code","9d0b8f01":"code","31442c7a":"code","44083291":"code","21191a15":"code","058e8e92":"code","08bf9027":"code","89e42e69":"code","6ff95245":"code","51315377":"code","9b998b91":"code","501839c8":"code","31e7995d":"code","8b1261b0":"code","6c85c1a4":"code","180f38ca":"code","bc95773a":"code","765232fb":"code","2414159f":"markdown","5f741d78":"markdown","513233f3":"markdown","9e556f17":"markdown","81807978":"markdown","cad3d844":"markdown","2e48e050":"markdown","84f24f93":"markdown","e6f1910e":"markdown","817713df":"markdown","0d0280fc":"markdown","7a850f4d":"markdown","e9b0f642":"markdown","976b4e85":"markdown","4db0b179":"markdown","313823b8":"markdown","dcf2c759":"markdown","3569e84e":"markdown","c4becd7b":"markdown"},"source":{"df6b5031":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","67de6177":"df1= pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf2 = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","c0cd46b1":"#contains data from ID 0 to ID 891\n\ndf1['Age']=df1['Age'].round(2)\ndf1['Fare']=df1['Fare'].round(2)\ndf1","df2f871b":"# contains Data from ID 892 to ID 1309\n\ndf2['Age']=df2['Age'].round(2)\ndf2['Fare']=df2['Fare'].round(2)\ndf2","fcb6ebe9":"df1.info()","50657706":"df1.isnull().sum()","6e8bf3cb":"df2.isnull().sum()","01e49022":"sns.heatmap(df1.isnull(), yticklabels=False, cmap='plasma')","b86af1ee":"sns.heatmap(df2.isnull(), yticklabels=False, cmap='plasma')","d3f5c76f":"sns.countplot( x = 'Survived' ,data=df1 )","54e3006d":"# We can check which sex survived more\n\nsns.countplot( x = 'Survived' ,data=df1, hue= 'Sex' )","694d1d74":"\n# We can check survived person belongs to which P class\n\nsns.countplot( x = 'Survived' ,data=df1, hue= 'Pclass' )","aa12b9b8":"sns.distplot(df1['Age'].dropna(), bins=30)","9aa6d0bc":"df1['Age'].hist(bins=30,color='darkred')","57683da1":"sns.countplot(x='SibSp',data=df1)","ddf96d4a":"df1['Fare'].hist(bins=40,figsize=(8,4))","1a7cc16a":"fig, ax = plt.subplots(figsize=(8,4))\nsns.heatmap(df1.corr(), cmap='rainbow' , annot= True)","80966963":"df1.drop('Cabin', axis=1, inplace=True)\ndf2.drop('Cabin', axis=1, inplace=True)","208b2505":"a11 = df1[ (df1['Pclass'] == 1) & (df1['Sex'] == 'male') ].Age.mean()\na12 = df1[ (df1['Pclass'] == 1) & (df1['Sex'] == 'female') ].Age.mean()\na21 = df1[ (df1['Pclass'] == 2) & (df1['Sex'] == 'male') ].Age.mean()\na22 = df1[ (df1['Pclass'] == 2) & (df1['Sex'] == 'female') ].Age.mean()\na31 = df1[ (df1['Pclass'] == 3) & (df1['Sex'] == 'male') ].Age.mean()\na32 = df1[ (df1['Pclass'] == 3) & (df1['Sex'] == 'female') ].Age.mean()\n\nage_mean = [a11,a12,a21,a22,a31,a32]\nage_mean","9df2ef61":"## now we replace the mean age according to sex and Pclass with null value\n\ngrp = ['Pclass','Sex']\ndf1['Age'] = df1['Age'].fillna(df1.groupby(grp)['Age'].transform('mean'))\n","e40a24ff":"## check if there is any null value present in training dataset\n\ndf1['Age'].isnull().sum()","9abed4c5":"b11 = df2[ (df2['Pclass'] == 1) & (df1['Sex'] == 'male') ].Age.mean()\nb12 = df2[ (df2['Pclass'] == 1) & (df1['Sex'] == 'female') ].Age.mean()\nb21 = df2[ (df2['Pclass'] == 2) & (df1['Sex'] == 'male') ].Age.mean()\nb22 = df2[ (df2['Pclass'] == 2) & (df1['Sex'] == 'female') ].Age.mean()\nb31 = df2[ (df2['Pclass'] == 3) & (df1['Sex'] == 'male') ].Age.mean()\nb32 = df2[ (df2['Pclass'] == 3) & (df1['Sex'] == 'female') ].Age.mean()\n\nage_mean = [b11,b12,b21,b22,b31,b32]\nage_mean","613363d3":"grp = ['Pclass','Sex']\ndf2['Age'] = df2['Age'].fillna(df2.groupby(grp)['Age'].transform('mean'))\n","a8514dae":"df2['Age'].isnull().sum()","b03d3b89":"# Now we have null value in test data so we calculate mean of Training data (df1) and fill that null value with mean\ndf1.mean()","e8034b39":"df2.fillna(32.00, inplace=True)","d13b097e":"df2.isnull().sum()","922205e1":"## Now we check the same with the help of heat map\n\nsns.heatmap(df1.isnull(), yticklabels=False, cmap='plasma')","0ae04c9a":"sns.heatmap(df2.isnull(), yticklabels=False, cmap='plasma')\n","0c79bcfa":"## for training data\n\ndf1.info()","e7644804":"sex = pd.get_dummies(df1['Sex'],drop_first=True)\nembark = pd.get_dummies(df1['Embarked'],drop_first=True)","9d0b8f01":"df1.drop(['Sex', 'Embarked', 'Name', 'Ticket'], axis=1, inplace=True)","31442c7a":"df1 = pd.concat([df1, sex, embark], axis=1)\ndf1['Fare']=df1['Fare'].round(1)","44083291":"df1.head(3)","21191a15":"df2.info()","058e8e92":"sex1 = pd.get_dummies(df2['Sex'],drop_first=True)\nembark1 = pd.get_dummies(df2['Embarked'],drop_first=True)","08bf9027":"df2.drop(['Sex', 'Embarked', 'Name', 'Ticket'], axis=1, inplace=True)","89e42e69":"df2 = pd.concat([df2, sex1, embark1], axis=1)\ndf2['Fare']=df2['Fare'].round(1)","6ff95245":"df2.head(3)","51315377":"#import logistic regression\nfrom sklearn.linear_model import LogisticRegression # Logistic Regression\nfrom sklearn.model_selection import train_test_split #for split the data\nfrom sklearn.metrics import accuracy_score #for accuracy_score\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.svm import SVC, LinearSVC \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","9b998b91":"# Model Preprocessing.\nX_train = df1.drop(['Survived'], axis=1)\ny_train = df1['Survived']\nX_test  = df2.copy()\nX_train.shape, y_train.shape, X_test.shape","501839c8":"logmodel = LogisticRegression()\nlogmodel.fit(X_train , y_train)","31e7995d":"Y_pred = logmodel.predict(X_test)\n\nlogmodel.score(X_train, y_train)","8b1261b0":"# Random Forests\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(X_train, y_train)\n\nY_pred = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, y_train)","6c85c1a4":"# KNN\n\nknn = KNeighborsClassifier(n_neighbors = 3)\n\nknn.fit(X_train, y_train)\n\nY_pred = knn.predict(X_test)\n\nknn.score(X_train, y_train)","180f38ca":"\ngaussian = GaussianNB()\n\ngaussian.fit(X_train, y_train)\n\nY_pred = gaussian.predict(X_test)\n\ngaussian.score(X_train, y_train)\n","bc95773a":"# get Correlation Coefficient for each feature using Logistic Regression\ncoeff_df = pd.DataFrame(df1.columns.delete(0))\ncoeff_df.columns = ['Features']\ncoeff_df[\"Coefficient Estimate\"] = pd.Series(logmodel.coef_[0])\n\n# preview\ncoeff_df","765232fb":"submission = pd.DataFrame({\n        \"PassengerId\": df2[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False)","2414159f":"**Roughly 20 percent of the Age data is missing. The proportion of Age missing is likely small enough for reasonable replacement with some form of imputation. Looking at the Cabin column, it looks like we are just missing too much of that data to do something useful with at a basic level. We'll probably drop this later, or change it to another feature like \"Cabin Known: 1 or 0\"**","5f741d78":"## Now we apply machine learning algorithms","513233f3":"## Gaussian Processes Classifier","9e556f17":"### Train and Test Data","81807978":"As we can see, In Training data set(df1), 177 in Age column and 687 in cabin column have null values, and in Test Data set(df2) 86 in Age column and 327 in Cabin column and 1 in Fare Column have null values, Now we will check it using seaborn.\nWe can use seaborn to create a simple heatmap to see where we are missing data","cad3d844":"### Exploratory Data Analysis","2e48e050":"### Understand the DataSet","84f24f93":"#### Now we deal with Fare column as it also have one null value in test data","e6f1910e":"**We'll need to convert categorical features to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs.**","817713df":"### Now we deal with Age column\n\n**Now we will calcute mean age of Male and female in each Pclass and replace that mean age with all null values present in Age column**","0d0280fc":"### Now we do the same with Test data set","7a850f4d":"### First we deal with Cabin column \n\n#### We delete this column as this have too much null values and it does not affect survived data","e9b0f642":"## KNN","976b4e85":"## Logistic Regression","4db0b179":"### Converting Categorical Features","313823b8":"### Import all Libraries","dcf2c759":"## Data Cleaning ","3569e84e":"Now we do same for test data","c4becd7b":"## Random Forest"}}