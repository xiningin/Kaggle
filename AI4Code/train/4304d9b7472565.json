{"cell_type":{"1779328f":"code","5c701ff4":"code","6f0bea59":"code","e8fb0a61":"code","b36242e8":"code","3f49514a":"code","475049e0":"code","474bbf0e":"code","5bc5ef96":"code","b5cc22ea":"code","402ae60c":"code","4c31a589":"code","e74fa1bb":"code","719b2326":"markdown","f93b8394":"markdown","23b4e8f0":"markdown","c5498438":"markdown","35c349d0":"markdown","efc71594":"markdown","b9a038ff":"markdown","ad9d31c1":"markdown","629ec6e5":"markdown","8eed8111":"markdown","1733dafa":"markdown","9e4bb326":"markdown","d7c0f177":"markdown","572ae385":"markdown","07e98331":"markdown","a58a9ef8":"markdown"},"source":{"1779328f":"import os\nimport pandas as pd\n\ninput_io_dir=\"..\/input\/titanic\/\"\n\noriginal_train_data=pd.read_csv(input_io_dir+\"train.csv\")\noriginal_test_data=pd.read_csv(input_io_dir+\"test.csv\")\nprint('original_train_data',original_train_data.shape)\nprint('original_test_data',original_test_data.shape)","5c701ff4":"from sklearn import ensemble, model_selection, svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score,precision_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score,train_test_split\nfrom xgboost import XGBClassifier\n\ninput_io_dir='..\/input\/titanic-competition-feature-engineering-1\/'\n# Compare different models\ndef PrepareDataSets():\n    passengerId=pd.read_csv(input_io_dir+\"passengerId.csv\",header=None)\n    train_features=pd.read_csv(input_io_dir+\"train_features.csv\",header=0)\n    train_labels=pd.read_csv(input_io_dir+\"train_labels.csv\",header=None)\n    test_features=pd.read_csv(input_io_dir+\"test_features.csv\",header=0)\n    print('PrepareDataSets: passengerId loaded(%d)'% len(passengerId))\n    print('PrepareDataSets: train_features loaded(%d)'% len(train_features))\n    print('PrepareDataSets: train_labels loaded(%d)'% len(train_labels))\n    print('PrepareDataSets: test_features loaded(%d)'% len(test_features))\n    return passengerId,train_features,train_labels, test_features\n    \ndef ModelSelection(clf_list,name_list,train_features,train_labels,scoring='accuracy'):\n    best_score=0\n    for clf, name in zip(clf_list,name_list) :\n        scores = model_selection.cross_val_score(clf, train_features.values.astype(float), train_labels.values.ravel().astype(float), cv=10, scoring=scoring)  \n        print(\"ModelSelection: Scoring  %0.2f +\/- %0.2f (%s 95%% CI)\" % (scores.mean(), scores.std()*2, name))\n        reference_score=scores.mean()+scores.std()\n        if (reference_score>best_score):\n            best_clf=name\n            best_score=reference_score\n            learning_model=clf\n    print(\"ModelSelection: Best model - \"+best_clf)\n    return learning_model\n\ndef ConfigureLearningModelsForBinaryClassification():\n    xgb_clf = XGBClassifier(n_estimators=100,max_depth=40, random_state=42)\n    dt_clf = DecisionTreeClassifier(random_state=42)\n    rf_clf = ensemble.RandomForestClassifier(n_estimators=100, random_state=42)\n    et_clf = ensemble.ExtraTreesClassifier(n_estimators=100, random_state=42)\n    gb_clf = ensemble.GradientBoostingClassifier(n_estimators=100, random_state=42)\n    ada_clf = ensemble.AdaBoostClassifier(n_estimators=100, random_state=42)\n    svm_clf = svm.LinearSVC(C=0.1,random_state=42)\n    lg_clf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=400,random_state=42)\n    e_clf = ensemble.VotingClassifier(estimators=[('xgb', xgb_clf), ('dt', dt_clf),('rf',rf_clf), ('et',et_clf), ('gbc',gb_clf), ('ada',ada_clf), ('svm',svm_clf), ('lg',lg_clf)])\n    clf_list = [xgb_clf, dt_clf, rf_clf, et_clf, gb_clf, ada_clf, svm_clf,lg_clf,e_clf]\n    name_list = ['XGBoost', 'Decision Trees','Random Forest', 'Extra Trees', 'Gradient Boosted', 'AdaBoost', 'Support Vector Machine', 'LogisticRegression','Ensemble']\n    return clf_list,name_list\n\npassengerId,train_features,train_labels, test_features=PrepareDataSets()\nclf_list,name_list=ConfigureLearningModelsForBinaryClassification()\nlearning_model=ModelSelection(clf_list,name_list,train_features,train_labels)\n","6f0bea59":"import numpy as np\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n\n# Draw learning curve\ndef plot_learning_curve(ax,learning_model, title, X, y, ylim=None, cv=None, random_state=42,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    ax.set_title(title)\n    if ylim is not None:\n        ax.ylim(*ylim)\n    ax.set_xlabel(\"Training examples\")\n    ax.set_ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(learning_model, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,random_state=random_state)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    ax.legend(loc=\"best\")\n\ndef DrawLearningCurves(clf_list,name_list,train_features,train_labels,scoring='accuracy',cols=1,figsize=(20,20)):\n    rows=len(clf_list)\n    i=1\n    f = plt.figure(figsize=figsize)\n    for clf, name in zip(clf_list,name_list) :\n        ax=f.add_subplot(rows,cols,i)\n        plot_learning_curve(ax,clf,name,train_features.values.astype(float), train_labels.values.ravel().astype(float),cv=10)\n        i=i+1\n\npassengerId,train_features,train_labels, test_features=PrepareDataSets()\nclf_list,name_list=ConfigureLearningModelsForBinaryClassification()\nDrawLearningCurves(clf_list,name_list,train_features,train_labels,figsize=(16,60))","e8fb0a61":"from sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# Fine tune a model given a param_grid\ndef FineTuneLearningModel(learning_model, param_grid, train_features,train_labels,scoring='accuracy'):\n    grid_search = GridSearchCV(learning_model, param_grid, scoring,cv=10)\n    grid_search.fit(train_features.values.astype(float),train_labels.values.ravel().astype(float))\n    cvres = grid_search.cv_results_\n    for mean_score,std_score, params in zip(cvres[\"mean_test_score\"], cvres[\"std_test_score\"],cvres[\"params\"]):\n        print('FineTuneLearningModel:',mean_score,'+-',std_score, params)\n    print('FineTuneLearningModel: Best params - '+str(grid_search.best_params_))\n    return grid_search.best_estimator_\n# Let's run a iterative process in which we\npassengerId,train_features,train_labels, test_features=PrepareDataSets()\nlearning_model = XGBClassifier(objective='binary:logistic')\nprint('FineTuneLearningModel: round 1 - booster type')\nprint('---------------------------------------------')\nparam_grid = [\n    {'booster':['gbtree','gblinear'],'n_estimators': [10,30,100],'learning_rate':[0.1]}\n]\nlearning_model=FineTuneLearningModel(learning_model, param_grid,train_features,train_labels)\nprint('Current model params:'+str(learning_model))\nprint('FineTuneLearningModel: round 2 - complexity params')\nprint('--------------------------------------------------')\nparam_grid = [\n    {'max_depth':range(3,10,2),'min_child_weight':range(1,6,2),'gamma':[i\/10.0 for i in range(0,5)]}\n]\nlearning_model=FineTuneLearningModel(learning_model, param_grid,train_features,train_labels)\nprint('Current model params:'+str(learning_model))\nprint('FineTuneLearningModel: round 3 - robustness params')\nprint('--------------------------------------------------')\nparam_grid = [\n    { 'subsample':[1e-5,1e-2,0.1,0.2,0.5,0.8,1], 'colsample_bytree':[1e-5,1e-2,0.1,0.2,0.5,0.8,1]}\n]     \nlearning_model=FineTuneLearningModel(learning_model, param_grid,train_features,train_labels)\nprint('Current model params:'+str(learning_model))\nprint('FineTuneLearningModel: round 4 - regularisation')\nprint('-----------------------------------------------')\nparam_grid = [\n    { 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 10,50],'reg_lambda':[0.1,0.5, 1, 2,5,10,50]}\n]\nlearning_model=FineTuneLearningModel(learning_model, param_grid,train_features,train_labels)\nprint('Current model params:'+str(learning_model))\nprint('FineTuneLearningModel: round 5 - reduce learning rate as it prevents overfitting')\nprint('--------------------------------------------------------------------------------')\nparam_grid = [\n    { 'learning_rate': [1e-5,0.01],'n_estimators': [10,100,200,500,1000]}\n]\nlearning_model=FineTuneLearningModel(learning_model, param_grid,train_features,train_labels)\nprint('Current model params:'+str(learning_model))","b36242e8":"passengerId,train_features,train_labels, test_features=PrepareDataSets()\nclf_list=[]\nclf_list.append(learning_model)\nname_list=['XgBoost']\nDrawLearningCurves(clf_list,name_list,train_features,train_labels,figsize=(20,12))","3f49514a":"# Train and generate predictions\ndef TrainModelAndGeneratePredictionsOnTestSet(learning_model,train_features,train_labels,test_features, threshold=-1):\n    learning_model.fit(train_features.values.astype(float),train_labels.values.ravel().astype(float))\n    if threshold==-1:\n        predictions = learning_model.predict(test_features.values.astype(float))\n    else:\n        if hasattr(learning_model,\"decision_function\"):\n            y_scores=learning_model.decision_function(test_features.values.astype(float))\n        else:\n            y_proba=learning_model.predict_proba(test_features.values.astype(float))\n            y_scores=y_proba[:,1]\n        predictions=(y_scores>threshold).astype(float)\n    pred=pd.Series(predictions)\n    # Ensure no floats go out\n    return pred.apply(lambda x: 1 if x>0 else 0)\npredictions=TrainModelAndGeneratePredictionsOnTestSet(learning_model,train_features,train_labels,test_features)\nprint('TrainModelAndGeneratePredictionsOnTestSet: predictions ready')","475049e0":"print('TrainModelAndGeneratePredictionsOnTestSet:Feature importances',sorted(zip(learning_model.feature_importances_,train_features.columns), reverse=True))","474bbf0e":"def GenerateOutputFile(passengerId,predictions):\n    output = pd.DataFrame({ 'PassengerId': passengerId,\n                            'Survived': predictions })\n    output.to_csv(\"output.csv\", index=False)\n\npassengerId = original_test_data['PassengerId']\nGenerateOutputFile(passengerId,predictions)","5bc5ef96":"training_predictions = pd.DataFrame(learning_model.predict(train_features.values.astype(float)))\ntraining_predictions.iloc[:,0]=training_predictions.iloc[:,0].astype(int)","b5cc22ea":"result=original_train_data.join(training_predictions!=train_labels)\n","402ae60c":"result.rename(columns={0:'Error'},inplace=True)","4c31a589":"training_predict_error=result[result.Error==True]\ntraining_predict_error.head()","e74fa1bb":"training_predict_error.describe()","719b2326":"<a id=\"learning_strategy\"><\/a>\n## Defining a learning strategy\nFrom my experience, the most effective learning takes place when you combine theory, practice and reflection - see <a href=\"https:\/\/bit.ly\/2AkSCc8\" >Kolb's learning cycle<\/a> <br\/>\nLet's see what activities I will take on each of these dimensions:\n* Theory: Andrew NG course  provides a basic theorical basis for ML. Books, popular Kaggle kernels, and other internet resources will be useful to expand my knowledge both on learning algorithms and also in libraries and frameworks.\n* Practice: apply acquired knowledge to develop the best possible solution for the competition.\n* Reflection: writing this diary will force me to synthesise information and reflect on actions done.\n\nIterative development will be used to put in practice the strategy and to find the appropriate balance between theory and practice.","f93b8394":"Let's compare current values with early scores evaluation.\n* Training scores\n  * Model selection stage: 0.885\n  * Fine tuning stage: 0.840-0.845\n* Cross-validation scores\n  * Model selection stage: (0.785 - 0.825 - 0.870)\n  * Fine tuning stage: (0.800 - 0.835 - 0.875)\n \n  Great...our learning model is fine-tuned and, as planned, we exchanged some accuracy on the training scores to improve our cross-validation performance.  \n\nLet's now fit the model with the full training set and create predictions...","23b4e8f0":"<a id=\"feature_eng\"><\/a>\n## Feature Engineering\nThe objectives for this stage are:\n* Adjusting data types\n* Dealing with missing data\n* Scaling and encoding data\n* Add new features\n* Drop less relevant  features\n\nThe dataset preparation will consist of these steps:\n* Load training and test data\n* Extract labels from training data (Survived column)\n* Extract PassengerId from test data - required for the output generation step\n* Extend the feature set\n* Exclude features we don't like\n* Process data differently depending on its nature\n  * Numeric\n  * Categorical\n* Combine processed data and prepare output data sets\n  * Training data\n  * Training labels\n  * Test data\n\nThe following kernels illustrate the different feature engineering efforts taken:\n* <a href=\"https:\/\/www.kaggle.com\/sergioortiz\/titanic-competition-feature-engineering-1\">Iteration #1: initial feature engineering<\/a>  \n\nThese kernels are designed to provide output datasets so that results can be used in the following sections.","c5498438":"Good...notice though that models have a very similar performance.  \nHowever, a single indicator may not be rich enough to understand model performance.","35c349d0":"During this process it is important to review that scores are consistently improving. If not, check the  model params on previous stage to understand why this happened.  \nNow, we'll have a look at learning curves for the final model...","efc71594":"<a id=\"model_tuning\"><\/a>\n## Fine-tuning model parameters\nNext, we are going to set up the hyperparameters for the selected model - Xgboost.  \nRelevant sources of information for this are the parameter tuning section in <a href=\"https:\/\/bit.ly\/2Ry4Rrx\"> Xgboost official website<\/a> and any of the existing <a href=\"https:\/\/bit.ly\/2cIU6lv\">optimisation blogs<\/a> covering this learning model.  \nLet's build some code to support this process...","b9a038ff":"<a id=\"save_results\"><\/a>\n## Saving results\nFinally, we will dump results into a file so that we can submit predictions into the platform.","ad9d31c1":"<a id=\"conclusions\"><\/a>\n## Conclusions\nThat's it! We have an initial submission we can post directly into the competition.  \nI hope you enjoyed this learning journey and do not hesitate to comment and share any thoughts you may have!\n\nBest regards,\n\n\n","629ec6e5":"* **Good accuracy but overfitting**  \nThese models provide a very good and stable accuracy on the training set but do not generalise well. This is can noticed in the existence of a gap between the training and cross-validation scores.  \nThe training score is related with the potential performance of the model while the cross-validation scores defines how well it generalises, that is, how it performs with samples different from these in the training set.  \nCommon measures against overfitting are to increase training examples (not feasible), reduce model complexity (not nice as we would reduce our potential) or apply regularisation (preferred option).  \nLet's comment on each particular model:  \n  * Xgboost: despite not having the highest accuracy on the training set (0.885), it's on the top regarding cross-validation performance (0.785 - 0.825 - 0.870).\n  * Decision Trees: great accuracy (0.90) - significant performance gap when using cross-validation set (0.780 - 0.810 - 0.845).\n  * Random Forest:similar accuracy with DecisionTrees (0.90),  generalise better but not as good as Xgboost (0.785 - 0.820 - 0.855).\n  * Extra Trees: similar accuracy with DecisionTrees (0.90),  generalise better but not as good as Xgboost (0.780 - 0.815 - 0.855).\n* **Not as good accuracy but overfitting**  \n  The following models are similar to the former but present lower accuracy scores.\n  * Gradient boosted: potential accuracy scores not as good as previous models (0.855). Cross-validation scores behind training but close (0.780 - 0.820 - 0.855)\n  * Ensemble: good accuracy but behind previous models (0.875). Cross-validation scores not as good but and overfitting (0.785 - 0.825 - 0.870)\n* **Not as good accuracy with signs of underfitting**\n  * AdaBoost: potential accuracy scores not good but stable (0.825). Cross-validation scores (0.785 - 0.820 - 0.845) in similar average values - typical of  underfitting scenarios.\n  * Support vector machine: potential accuracy scores not good and not as stable as other models (0.825 - 0.830 - 0.835). Cross-validation scores in similar average values with pronounced differences in performance (0.800 - 0.830 - 0.860))\n  * Logistic regression: training set accuracy scores not good and slightly unstable (0.830 - 0.835 - 0.840). Cross-validation scores are among the highest and in similar average values as training scores (0.800 - 0.835 - 0.865)\n  \n In conclusion, logistic regression presents a great performance in the cross-validation set but reduced potential as its training scores are lower than top performing models and more unstable.  \n Therefore, we will take Xgboost as it presents similar cross-validation performance but higher accuracy on the training scores. Considering we will not be able to increase the training set, we will try to regularise the model to exchange some variance for more accuracy on the test set.\n  ","8eed8111":"# Titanic Competition: A learning diary\n\nHi there! This is Sergio and this is my first competition and practical exercise after taking <a href=\"https:\/\/bit.ly\/1IXp8Lg\">Andrew NG's ML course<\/a> and having a look at <a href=\"https:\/\/oreil.ly\/2nzmN8L\">Aur\u00e9lien G\u00e9ron's book<\/a>.  \n\nWhy am I writing a learning diary?  \nFirst and most important, as a reflection exercise to strengthen my learning. So, let me raise a caution note on the contents as I am not a ML expert but a beginner in this field. In addition to this, I would also like to connect with other people to share and contrast my views. Finally, after many years of practice, I also find this a 'humble exercise' of giving back to the community and supporting other learners.\n\nGreat! Let's start with the learning adventure...\n<hr\/>\n\n## Table of contents\n* <a href='#learning_strategy'>Defining a learning strategy<\/a>\n* <a href='#script_kernels'>Script kernels<\/a>\n* <a href='#data_load'>Loading data<\/a>\n* <a href='#data_exploration'>Exploring data<\/a>\n* <a href='#feature_eng'>Feature engineering<\/a>\n* <a href='#model_selection'>Playing with different models<\/a>\n* <a href='#model_performance'>Evaluating model performance<\/a>\n* <a href='#model_tuning'>Fine-tuning model parameters<\/a>\n* <a href='#save_results'>Saving results<\/a>\n* <a href='#conclusions'>Conclusions<\/a>\n<hr\/>\n","1733dafa":"<a id=\"data_load\"><\/a>\n## Loading data\nLet's start by loading data - both training and test sets.","9e4bb326":"<a id=\"model_selection\"><\/a>\n## Playing with different models\nDepending on the type of problem,  it is wise to initially evaluate how different learning models apply to your specific data set and circumstances.\nThe Titanic competition is basically a binary classification problem. Consequently, these machine learning models are good candidates:\n* DecisionTrees\n* RandomForests\n* XGBoostClassifier\n* GradientBoostingClassifier\n* AdaBoostClassifier\n* LogisticRegression\n* Ensemble of different models\n* Neural networks - to be addressed in next iterations! \n\nThe following code provides a general framework for model evaluation:","d7c0f177":"<a id=\"model_performance\"><\/a>\n## Evaluating model performance\n<a href=\"http:\/\/scikit-learn.org\/stable\/modules\/learning_curve.html#learning-curve\">Learning curves<\/a> are a valuable instrument to identify model performance. Particularly, to know whether the learning model is underfitting or overfitting.  \nLet's enrich our perspective on performance analysing the curves for each model.","572ae385":"Let's have a look at relative relevance of features for the learning model...","07e98331":"<a id=\"data_exploration\"><\/a>\n## Exploring data\nWhat I think are the main benefits of exploring data?\n* Classify available features - e.g. depending on its type data may need different processing\n* Identify the need to adapt\/correct data types, data values outliers, etc...\n* Identify missing data and take decisions (drop\/complete)\n* Identify trends and correlations\n* Identify opportunities to create new features from existing\n* Identify potential barriers to model training - e.g. uneven distribution of information between training and test sets\n\nThe following kernels illustrate the different data exploration processes taken:\n* <a href=\"https:\/\/www.kaggle.com\/sergioortiz\/titanic-competition-data-exploration-1\">Iteration #1: initial exploration<\/a>  \n* <a href=\"\">Iteration #2: further investigation (work in progress)<\/a>","a58a9ef8":"<a id=\"script_kernels\"><\/a>\n## Script kernels\n\nThe following kernels contain complete solution scripts for the competition:\n* <a href=\"https:\/\/www.kaggle.com\/sergioortiz\/titanic-competition-script-1\">Script #1: 0.79904<\/a>\n\nIn the next sections I will be presenting and commenting extracts from these solution scripts."}}