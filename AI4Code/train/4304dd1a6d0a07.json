{"cell_type":{"d1fbe68a":"code","1cb52dbc":"code","2f460d60":"code","e79df82a":"code","bc0572b3":"code","f127c91b":"code","8b3859f1":"code","5a92412d":"code","c103f0a0":"code","e62c9a9a":"code","7b471ec5":"code","8f327ea6":"code","f3fc0607":"code","5acd801d":"code","5be94e2f":"code","5f88a3f4":"code","aa25cc2e":"code","2b70d254":"code","84e6a4d7":"code","4165a26e":"code","df16beb9":"code","69d6846d":"code","98eb2eee":"code","f5e0b893":"code","c0c842c2":"code","a9b602f6":"markdown","9a976e3e":"markdown","ad8c7ecd":"markdown"},"source":{"d1fbe68a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime as datetime\nimport json\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn import model_selection, preprocessing, metrics\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport gc\ngc.enable()\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","1cb52dbc":"json_data=[\"device\",\"geoNetwork\",\"totals\",\"trafficSource\"]\n\ngc.enable()\nfeatures = ['channelGrouping', 'date', 'fullVisitorId', 'visitId',\\\n       'visitNumber', 'visitStartTime', 'device_browser',\\\n       'device_deviceCategory', 'device_isMobile', 'device_operatingSystem',\\\n       'geoNetwork_city', 'geoNetwork_continent', 'geoNetwork_country',\\\n       'geoNetwork_metro', 'geoNetwork_networkDomain', 'geoNetwork_region',\\\n       'geoNetwork_subContinent', 'totals_bounces', 'totals_hits',\\\n       'totals_newVisits', 'totals_pageviews', 'totals_transactionRevenue',\\\n       'trafficSource_adContent', 'trafficSource_campaign',\\\n       'trafficSource_isTrueDirect', 'trafficSource_keyword',\\\n       'trafficSource_medium', 'trafficSource_referralPath',\\\n       'trafficSource_source']\ndef load_df(csv_path='..\/input\/train_v2.csv'):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    ans = pd.DataFrame()\n    dfs = pd.read_csv(csv_path, sep=',',\n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                    chunksize = 100000)\n    for df in dfs:\n        df.reset_index(drop = True,inplace = True)\n        for column in JSON_COLUMNS:\n            column_as_df = json_normalize(df[column])\n            column_as_df.columns = [f\"{column}_{subcolumn}\" for subcolumn in column_as_df.columns]\n            df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n\n        print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n        use_df = df[features]\n        del df\n        gc.collect()\n        ans = pd.concat([ans, use_df], axis = 0).reset_index(drop = True)\n        print(ans.shape)\n    return ans\n\ntrain = load_df()\ntrain.shape","2f460d60":"train.head()","e79df82a":"test=load_df(csv_path='..\/input\/test_v2.csv')","bc0572b3":"train[\"totals_transactionRevenue\"] = train[\"totals_transactionRevenue\"].astype('float')\ntotal_revenue=train.groupby(\"fullVisitorId\")[\"totals_transactionRevenue\"].sum().reset_index()\nplt.figure(figsize=(8,6))\nplt.scatter(range(total_revenue.shape[0]), np.sort(np.log1p(total_revenue[\"totals_transactionRevenue\"].values)))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('TransactionRevenue', fontsize=12)\nplt.show()","f127c91b":"def chats(data):\n    trace = go.Bar(y=data.index[::-1],\n                   x=data.values[::-1],\n                   showlegend=False,\n                   orientation = 'h',\n    )\n    return trace\ndata=train.groupby(\"device_browser\")[\"totals_transactionRevenue\"].agg([\"size\",\"count\",\"mean\"])\ndata.columns=[\"count\", \"count of non-zero revenue\", \"mean\"]\ndata=data.sort_values(by=\"count\",ascending=False)\ntrace1=chats(data[\"count\"].head(10))\ntrace2=chats(data[\"count of non-zero revenue\"].head(10))\ntrace3=chats(data[\"mean\"].head(10))\n\n\ndata=train.groupby(\"device_deviceCategory\")[\"totals_transactionRevenue\"].agg([\"size\",\"count\",\"mean\"])\ndata.columns=[\"count\", \"count of non-zero revenue\", \"mean\"]\ndata=data.sort_values(by=\"count\",ascending=False)\ntrace4=chats(data[\"count\"].head(10))\ntrace5=chats(data[\"count of non-zero revenue\"].head(10))\ntrace6=chats(data[\"mean\"].head(10))\n\n\ndata=train.groupby(\"device_operatingSystem\")[\"totals_transactionRevenue\"].agg([\"size\",\"count\",\"mean\"])\ndata.columns=[\"count\", \"count of non-zero revenue\", \"mean\"]\ndata=data.sort_values(by=\"count\",ascending=False)\ntrace7=chats(data[\"count\"].head(10))\ntrace8=chats(data[\"count of non-zero revenue\"].head(10))\ntrace9=chats(data[\"mean\"].head(10))\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=3, vertical_spacing=0.04, \n                          subplot_titles=[\"Device Browser - Count\", \"Device Browser - Non-zero Revenue Count\", \"Device Browser - Mean Revenue\",\n                                          \"Device Category - Count\",  \"Device Category - Non-zero Revenue Count\", \"Device Category - Mean Revenue\", \n                                          \"Device OS - Count\", \"Device OS - Non-zero Revenue Count\", \"Device OS - Mean Revenue\"])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\nfig.append_trace(trace7, 3, 1)\nfig.append_trace(trace8, 3, 2)\nfig.append_trace(trace9, 3, 3)\n\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Device Plots\")\npy.iplot(fig, filename='device-plots')","8b3859f1":"train.head(10)","5a92412d":"#country\nnetwork_Country=train.groupby(\"geoNetwork_country\")[\"totals_transactionRevenue\"].agg([\"size\",\"count\",\"mean\"])\nnetwork_Country.columns=[\"Country_count\", \"Country_count of non-zero revenue\", \"mean\"]\nnetwork_Country=network_Country.sort_values(by=\"Country_count\",ascending=False)\ntrace1=chats(network_Country[\"Country_count\"].head(10))\ntrace2=chats(network_Country[\"Country_count of non-zero revenue\"].head(10))\ntrace3=chats(network_Country[\"mean\"].head(10))\n\n\n\n#continent\nnetwork_continent=train.groupby(\"geoNetwork_continent\")[\"totals_transactionRevenue\"].agg([\"size\",\"count\",\"mean\"])\nnetwork_continent.columns=[\"Continent_count\", \"Continent_count of non-zero revenue\", \"mean\"]\nnetwork_continent=network_continent.sort_values(by=\"Continent_count\",ascending=False)\ntrace4=chats(network_continent[\"Continent_count\"].head(10))\ntrace5=chats(network_continent[\"Continent_count of non-zero revenue\"].head(10))\ntrace6=chats(network_continent[\"mean\"].head(10))\n\n\nnetwork_continent=train.groupby(\"geoNetwork_networkDomain\")[\"totals_transactionRevenue\"].agg([\"size\",\"count\",\"mean\"])\nnetwork_continent.columns=[\"networkDomain_count\", \"networkDomain_count of non-zero revenue\", \"mean\"]\nnetwork_continent=network_continent.sort_values(by=\"networkDomain_count\",ascending=False)\ntrace7=chats(network_continent[\"networkDomain_count\"].head(10))\ntrace8=chats(network_continent[\"networkDomain_count of non-zero revenue\"].head(10))\ntrace9=chats(network_continent[\"mean\"].head(10))\n\nfig = tools.make_subplots(rows=3, cols=3, vertical_spacing=0.08, horizontal_spacing=0.15, \n                          subplot_titles=[\"Traffic Source - Count\", \"Traffic Source - Non-zero Revenue Count\", \"Traffic Source - Mean Revenue\",\n                                          \"Traffic Source Medium - Count\",  \"Traffic Source Medium - Non-zero Revenue Count\", \"Traffic Source Medium - Mean Revenue\"\n                                          ])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\nfig.append_trace(trace7, 3, 1)\nfig.append_trace(trace8, 3, 2)\nfig.append_trace(trace9, 3, 3)\n\nfig['layout'].update(height=1000, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Traffic Source Plots\")\npy.iplot(fig, filename='geoNetwork-plots')","c103f0a0":"#traffic_source\ntraffic_source=train.groupby(\"trafficSource_source\")[\"totals_transactionRevenue\"].agg([\"size\",\"count\",\"mean\"])\ntraffic_source.columns=[\"traffic_source_count\", \"traffic_source_count of non-zero revenue\", \"mean\"]\ntraffic_source=traffic_source.sort_values(by=\"traffic_source_count\",ascending=False)\ntrace1=chats(traffic_source[\"traffic_source_count\"].head(10))\ntrace2=chats(traffic_source[\"traffic_source_count of non-zero revenue\"].head(10))\ntrace3=chats(traffic_source[\"mean\"].head(10))\n\n\n\n#medium\ntraffic_medium=train.groupby(\"trafficSource_medium\")[\"totals_transactionRevenue\"].agg([\"size\",\"count\",\"mean\"])\ntraffic_medium.columns=[\"traffic_medium_count\", \"traffic_medium_count of non-zero revenue\", \"mean\"]\ntraffic_medium=traffic_medium.sort_values(by=\"traffic_medium_count\",ascending=False)\ntrace4=chats(traffic_medium[\"traffic_medium_count\"].head(10))\ntrace5=chats(traffic_medium[\"traffic_medium_count of non-zero revenue\"].head(10))\ntrace6=chats(traffic_medium[\"mean\"].head(10))\n\n\n\n\nfig = tools.make_subplots(rows=2, cols=3, vertical_spacing=0.08, horizontal_spacing=0.15, \n                          subplot_titles=[\"Traffic Source - Count\", \"Traffic Source - Non-zero Revenue Count\", \"Traffic Source - Mean Revenue\",\n                                          \"Traffic Source Medium - Count\",  \"Traffic Source Medium - Non-zero Revenue Count\", \"Traffic Source Medium - Mean Revenue\"\n                                          ])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\n\n\nfig['layout'].update(height=1000, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Traffic Source Plots\")\npy.iplot(fig, filename='geoNetwork-plots')","e62c9a9a":"import datetime\ndef scatter_plot(data):\n    trace = go.Scatter(\n        x=data.index[::-1],\n        y=data.values[::-1],\n        showlegend=False,\n        #mode = 'lines+markers',\n    )\n    return trace\ntrain['date'] = train['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\ndate_trans=train.groupby(\"date\")[\"totals_transactionRevenue\"].agg([\"size\",\"count\"])\ndate_trans.columns = [\"count\", \"count of non-zero revenue\"]\ndate_trans = date_trans.sort_index()\n\ntrace1=scatter_plot(date_trans[\"count\"])\ntrace2=scatter_plot(date_trans[\"count of non-zero revenue\"])\nfig = tools.make_subplots(rows=2, cols=1, vertical_spacing=0.08,\n                          subplot_titles=[\"Date - Count\", \"Date - Non-zero Revenue count\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 2, 1)\nfig['layout'].update(height=800, width=800, paper_bgcolor='rgb(233,233,233)', title=\"date-plots for count & Revenue\")\npy.iplot(fig, filename='date-plots')","7b471ec5":"print(\"Variables in train and not in test` : \", set(train.columns).difference(set(test.columns)))","8f327ea6":"train.shape,test.shape","f3fc0607":"train[\"totals_transactionRevenue\"].fillna(0, inplace=True)\ntrain_y = train[\"totals_transactionRevenue\"].values","5acd801d":"num_cols = [\"totals_hits\", \"totals_pageviews\", \"visitNumber\", \"visitStartTime\", 'totals_bounces',  'totals_newVisits']    \nfor col in num_cols:\n    train[col] = train[col].astype(float)\n    test[col] = test[col].astype(float)\nexcluded_features = [\n    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n    'visitId', 'visitStartTime', 'nb_sessions', 'max_visits','visit_time','totals_visits'\n]\n\ncategorical_features = [\n    _f for _f in train.columns\n    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n]\n\ncategorical_features","5be94e2f":"train_val=np.log1p(train_y)","5f88a3f4":"for f in categorical_features:\n    train[f], indexer = pd.factorize(train[f])\n    test[f] = indexer.get_indexer(test[f])\ntrain.head()","aa25cc2e":"train[\"totals_transactionRevenue\"].fillna(0, inplace=True)\ntrain_y = train[\"totals_transactionRevenue\"]\ntrain_val=np.log1p(train_y)","2b70d254":"train_x,test_x,val_x,val_y=train_test_split(train,train_val,test_size=0.2, random_state=42)\n","84e6a4d7":"trn_x=train_x[categorical_features+num_cols]\ntst_x=test_x[categorical_features+num_cols]\n","4165a26e":"test_X=test[categorical_features+num_cols]\ntest_X.head()","df16beb9":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\", \n        \"num_leaves\" : 30,\n        \"min_child_samples\" : 100,\n        \"learning_rate\" : 0.1,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=100)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, pred_val_y\n\n# Training the model #\npred_test, model, pred_val = run_lgb(trn_x, val_x,tst_x, val_y, test_X)","69d6846d":"from sklearn import metrics\npred_val[pred_val<0] = 0\nval_pred_df = pd.DataFrame({\"fullVisitorId\":test_x[\"fullVisitorId\"].values})\nval_pred_df[\"transactionRevenue\"] = test_x[\"totals_transactionRevenue\"].values\nval_pred_df[\"PredictedRevenue\"] = np.expm1(pred_val)\n#print(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values))))\nval_pred_df = val_pred_df.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\nprint(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values))))","98eb2eee":"submission = pd.DataFrame({\"fullVisitorId\":test[\"fullVisitorId\"].values})\npred_test[pred_test<0] = 0\nsubmission[\"PredictedLogRevenue\"] = np.expm1(pred_test)\nsubmission = submission.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsubmission.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsubmission[\"PredictedLogRevenue\"] = np.log1p(submission[\"PredictedLogRevenue\"])\nsubmission.to_csv(\"predicted_values.csv\", index=False)","f5e0b893":"submission.head(10)","c0c842c2":"fig, ax = plt.subplots(figsize=(18,20))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"Feature Importance\", fontsize=15)\nplt.show()","a9b602f6":"finding out the constant values and dropping them from the data","9a976e3e":"As the data contain some json formated columns, we can use the below code to convert such kind to CSV loaded formot.","ad8c7ecd":"The above figure shows the** 80\/20** rule to be **TRUE** as stated in the data"}}