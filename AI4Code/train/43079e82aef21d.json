{"cell_type":{"472c90e2":"code","fb60db13":"code","ea8a71b7":"code","6d191e67":"code","33fbda3c":"code","78e8ae20":"code","305490a7":"code","3b03c3be":"code","f2f01df8":"code","c79df17c":"code","65698ba9":"code","666ee760":"code","3a231a82":"code","ab729369":"code","6aabfdbe":"code","18a94cd3":"code","3fa90fcb":"code","d4102b2b":"markdown","7b6116dc":"markdown","efe7d091":"markdown","45b2d462":"markdown","c335b484":"markdown","d35fbf5f":"markdown","e1166fa0":"markdown","6b7aefe8":"markdown","2bfe23a5":"markdown","a0ef6642":"markdown","05d55c69":"markdown","27752a61":"markdown","43d76524":"markdown","c5fd5500":"markdown","06b4cf23":"markdown","371b6a04":"markdown","0230894c":"markdown","7fc5427b":"markdown"},"source":{"472c90e2":"#Follow Kaggle's way to load datasets.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fb60db13":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras as kr\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Activation, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.utils import np_utils\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","ea8a71b7":"#change the dataset into Pandas dataframe\nimport pandas as pd\ndataset = pd.read_csv(\"\/kaggle\/input\/chinese-mnist-digit-recognizer\/chineseMNIST.csv\")\ndataset","6d191e67":"#Let's start labelencoding!\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nencoded = le.fit_transform(dataset['character'].values)\ndecoded = le.inverse_transform(encoded)\ndataset['character2'] = encoded\nprint('This dataset has following true labels, ', le.classes_)","33fbda3c":"#Then, make a dictionary. \ndic={0:'\u4e00',1:'\u4e03',2:'\u4e07',3:'\u4e09',4:'\u4e5d',5:'\u4e8c',6:'\u4e94',7:'\u4ebf',8:'\u516b',9:'\u516d',10:'\u5341',11:'\u5343',12:'\u56db',13:'\u767e',14:'\u96f6'}","78e8ae20":"#Shuffle data :-)\ndataset_shuffled = dataset.sample(frac=1, random_state=0)\n#split the shuffled dataset into explanation data and target data.\nx =dataset_shuffled.iloc[:,0:4096]\ny =dataset_shuffled.iloc[:,[4098]]\n#Let's use sklearn's train_test_split function\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)","305490a7":"#normalizes data from 1 to 0. \nx_train = x_train.astype('float32')\/255\ny_train = y_train.astype('float32')\nx_test = x_test.astype('float32')\/255\ny_test = y_test.astype('float32')","3b03c3be":"#Check the shape of dataset' images\nimport numpy as np\nnp.sqrt(4096)","f2f01df8":"#import numpy and give a seed. \nimport numpy as np\nnp.random.seed(31)\n#Show 3 letter at random and convert them into gray scale letters. \nfor i in range(3):\n    plt.imshow(x_train.iloc[np.random.randint(0,15000)].values.reshape(64,64),cmap='Greys')\n    plt.show()","c79df17c":"#change data into numpy array\nx_train= x_train.to_numpy()\ny_train= y_train.to_numpy()\nx_test= x_test.to_numpy()\ny_test= y_test.to_numpy()\n\n#reshape train data and test data into 64 * 64 * 1channel\nx_train = x_train.reshape(x_train.shape[0], 64, 64, 1).astype('float32')\nx_test = x_test.reshape(x_test.shape[0], 64, 64, 1).astype('float32')","65698ba9":"#convert y_train and y_test into 15 categories\ny_train = kr.utils.to_categorical(y_train, 15)\ny_test = kr.utils.to_categorical(y_test, 15)\nnum_classes = y_train.shape[1]\nnum_classes","666ee760":"#Build an ordinary \"Deep Learning\" model with CNN and maxpooling by using Keras.\nmodel = Sequential()\nmodel.add(Conv2D(32, (5, 5), input_shape=(64, 64, 1), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n#Choose an optimizer and compile the model.\nmodel.compile(optimizer = Adam(learning_rate = 0.01), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n#And print the summary of the model.\nprint(model.summary())","3a231a82":"#model fitting\nmodel1 = model.fit(x_train, y_train,batch_size=128, epochs=20)","ab729369":"#Check this model by using two metrics, loss and accuracy.\nmetrics = ['loss', 'accuracy']\n#show the evaluation result by using matoplot.\nplt.figure(figsize=(10, 5))\n#Use \"For Loop\".\nfor i in range(len(metrics)):\n    metric = metrics[i]\n    #set subplots to show the result\n    plt.subplot(1, 2, i+1)\n    #Titles of subplots are \"loss\" and \"accuracy\"\n    plt.title(metric) \n    plt_train1 = model1.history[metric] \n\n    #plot them all\n    plt.plot(plt_train1, label='train1') \n    plt.legend() \nplt.show()","6aabfdbe":"i=100\n#Here is the prediction sample.\nplt.imshow(x_test[[i]].reshape(64,64),cmap='Greys')","18a94cd3":"#Let's predict.\nprediction=model.predict(x_test[[i]]) \nprediction","3fa90fcb":"#Let's check the result.\nprint(\"The answer is\",dic[np.argmax(prediction)],\". :-)\")","d4102b2b":"<font size=+3> That's enough! :-) <\/font>","7b6116dc":"<HR>\nBy the way, Let's check what kinda Chinese charactors the dataset has!\n<HR>","efe7d091":"#### I see :-) This dataset has 4,098 columns.\n#### 1) The label is the second from tha last column.\n#### 2) The last column is the charactor of each Chinese numeral.  \n#### 4,098 - 2 = 4,096. That means 64 x 64 = 4,096.\n#### Now I know it's a bit different from MNIST.","45b2d462":"## 9. Let's make use of the trained model and predict one case :-)","c335b484":"## 8. Check this model by using two metrics, loss and accuracy.","d35fbf5f":"<font size=+2>Correct! :-) <\/font>","e1166fa0":"## 1. Import libraries","6b7aefe8":"## 4.Let's make train data and test data!","2bfe23a5":"## 6. Let's check real images of the dataset.","a0ef6642":"## 6. Build an ordinary \"Deep Learning\" model with CNN and maxpooling by using TensorFlow Keras.","05d55c69":"# Let's enjoy <font color=\"Blue\">Chinese charactors' dataset<\/font>!","27752a61":"## 10. Let's check the result.","43d76524":"## 3. Let's start labelencoding.etc!","c5fd5500":"## 2. Read the dataset as Padas DataFrame","06b4cf23":"## 5. Check the shape of dataset' images","371b6a04":"### Now I know this dataset has 15 classifications.","0230894c":"## 7.Model Fitting","7fc5427b":"### Now I know this dataset has 64*64 images."}}