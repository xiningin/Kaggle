{"cell_type":{"54e3c968":"code","ab3900bb":"code","f3449003":"code","b909ff46":"code","51fe5f19":"code","8dda07dc":"code","f0e78dab":"code","4d3ef2a1":"code","1fe85631":"code","f90cdd25":"code","bd65e765":"code","7f2612b8":"code","72b28681":"code","dbd84e45":"code","b309a6d8":"code","6f9c5ee2":"code","b8a399da":"code","51430c0e":"code","95c05dc2":"code","05857a00":"code","59d00172":"code","1d451f2d":"code","48f3a11d":"code","2ffb7ad9":"code","491ad613":"code","5718f524":"code","663404ad":"code","23d72727":"code","81287f9c":"code","03560d7d":"code","da9c8841":"code","60088b08":"code","c0d96b61":"markdown","078d9826":"markdown","d45a1513":"markdown","84c1cbc0":"markdown","cfe14543":"markdown","5127a139":"markdown","3e273907":"markdown","f73795e6":"markdown","f6a72481":"markdown","62c370e1":"markdown","34a23098":"markdown","6d0b1661":"markdown","a4aff962":"markdown","20a88539":"markdown","abcc50ba":"markdown","9ab24049":"markdown","e9736b1d":"markdown","843ac817":"markdown","4e44603e":"markdown"},"source":{"54e3c968":"# Importing all required libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\n\n# Stats\nfrom scipy import stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# For ignoring the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport gc\ngc.enable()","ab3900bb":"# Fetching the data to pandas dataframes\ntrain = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/sample_submission.csv')","f3449003":"# Checking training data\ntrain.shape","b909ff46":"train.head()","51fe5f19":"# Checking the datatypes of columns\ntrain.dtypes","8dda07dc":"train.dtypes.value_counts()","f0e78dab":"train.isna().sum().sort_values(ascending = False).head()","4d3ef2a1":"# Checking if every row has unique id\nlen(train['ID_code'].unique())","1fe85631":"# Target column\ntrain['target'].value_counts()","f90cdd25":"ax = sns.countplot(x=\"target\",data=train)","bd65e765":"import tqdm\ntrain_enc = pd.DataFrame(index = train.index)\nfor col in tqdm.tqdm_notebook(train.columns):\n    train_enc[col] = train[col].factorize()[0]","7f2612b8":"feature_names = train.columns[2:]\nfeature_names\n","72b28681":"# Find skewed numerical features\nskew_features = train[feature_names].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","dbd84e45":"# Dropping ID and target columns\nz_score_calc = train.drop(columns=['ID_code', 'target'])\n# Calculating z score\nz = np.abs(stats.zscore(z_score_calc))\n# print(z)\nthreshold = 3\nprint(np.where(z > 4))","b309a6d8":"treated_data = train[(z < 4).all(axis=1)]","6f9c5ee2":"print(\"before treating outliers : {}\".format(train.shape))\nprint(\"after treating outliers : {}\".format(treated_data.shape))","b8a399da":"treated_data.columns","51430c0e":"# Creating the variables for model fitting\nX = treated_data.drop(columns=['ID_code', 'target'])\ny = treated_data['target']\n\n# Test variable\ntest = test.drop(columns=['ID_code'])","95c05dc2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .3, random_state=0)","05857a00":"## Import the random forest model.\nfrom sklearn.ensemble import RandomForestClassifier\n## This line instantiates the model. \nrf = RandomForestClassifier() \n## Fit the model on your training data.\nrf.fit(X_train, y_train) \n## And score it on your testing data.\nrf.score(X_test, y_test)","59d00172":"prediction_rf = rf.predict(test)","1d451f2d":"train.columns","48f3a11d":"submission=pd.DataFrame({\"ID_code\":sub['ID_code'],\n                         \"target\":prediction_rf})\nsubmission.to_csv('submission_rf.csv',index=False)","2ffb7ad9":"feature_importances = pd.DataFrame(rf.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)","491ad613":"feature_importances","5718f524":"feature_importances.median()","663404ad":"feature_importances.tail(15)","23d72727":"X_train.drop( ['var_38','var_158','var_73','var_14','var_10','var_84','var_61','var_103','var_185'],axis = 1,inplace = True )\nX_test.drop( ['var_38','var_158','var_73','var_14','var_10','var_84','var_61','var_103','var_185'],axis = 1 , inplace = True)\nX_train.head()\n","81287f9c":"rf.fit(X_train, y_train) \n## And score it on your testing data.\nrf.score(X_test, y_test)","03560d7d":"feature_selected_test = test.drop( ['var_38','var_158','var_73','var_14','var_10','var_84','var_61','var_103','var_185'],axis = 1)\nfeature_selected_test.head()","da9c8841":"prediction_rf = rf.predict(feature_selected_test)","60088b08":"submission=pd.DataFrame({\"ID_code\":sub['ID_code'],\n                         \"target\":prediction_rf})\nsubmission.to_csv('submission_rf2.csv',index=False)","c0d96b61":"# Outliers\n\nAs the data is huge, we will first calculate the z-score of the training data and then we will further explore if the value found is to be treated as an outlier or not","078d9826":"As there are not highly skewed features, we do not need to apply any transformation on the data","d45a1513":"# Exploratory data analysis","84c1cbc0":"# Introduction\n\nIn this project we have to identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted.\n\nEvaluation :\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\n","cfe14543":"There are no duplicate columns present in the data","5127a139":"### We have removed **27** rows having outliers\n\nWe will try fitting our model on treated and non treated data to see which one performs better and then we can try changing the threshold value for finding outliers","3e273907":"The training data has 202 columns and 200000 rows that will provide us a good amount of data for training our machine learning models","f73795e6":"As we can see the data is anonymized by the company and only target and Id are provided without anonymizing, we would need to explore the data and formulate certain hypothesis to de-anonymize the data and infer some valuable insights from the data that will help us predict the target value for test data","f6a72481":"As this is a banking related data, as expected there are no null values present in the data, that means the data is properly recorded by the bank without any failure","62c370e1":"## Let's explore the columns now","34a23098":"## Checking for missing values in the data","6d0b1661":"Removing the features that have importance less than 0.0038\n","a4aff962":"The first array gives the row numbers and the 2nd array gives the respective columns of the outliers","20a88539":"All the anonymized columns i.e var_0 to var_199 have float datatype, we will have to check the values of all the columns if they are actually a numeric type values or are encoded to values from a categorical type variable.<br>\nWe will then decode the values back to categorical features and that might improve the accuracy of our predictions","abcc50ba":"# Feature Skewness","9ab24049":"# Feature Selection","e9736b1d":"The target variable is highly imbalanced as value 0 has a much higher count then the value 1.<br>\nwe will need to consider this while doing cross validation of a model that balanced sample is taken for prediction and for cross validation","843ac817":"## Checking for duplicate columns\n\nFirstly we will encode each and every column using tqdm function which works as a pipeline operator for handing big amount of data.\n\nFun Fact : **tqdm means \"progress\" in Arabic (taqadum, \u062a\u0642\u062f\u0651\u0645) and is an abbreviation for \"I love you so much\" in Spanish (te quiero demasiado).**","4e44603e":"## Let's explore the training data first"}}