{"cell_type":{"7bd88c58":"code","1a6b3e6b":"code","5743a336":"code","febe2abd":"code","783fc6e3":"code","2e4a6973":"code","147f10f6":"code","0e563210":"code","30eab223":"code","19ceefb8":"code","a1bea6d7":"code","f94c6200":"code","10d12dad":"code","d2541a11":"code","c49abe06":"code","183b409a":"code","e334a1f2":"code","0e0cbb46":"code","9233a7d3":"code","840c96b8":"code","030c1123":"code","47df55d7":"code","3271a693":"code","952dd622":"code","b5236543":"code","a27d59e3":"code","d9b0cb65":"code","45afcab5":"code","c54529f8":"code","deec7c61":"code","1b93b397":"code","a363aebc":"code","39af5105":"code","2f0b6041":"code","e7be08c2":"code","e481f579":"code","5dfd7269":"code","986a60e4":"code","fe942004":"code","e8c6715c":"code","e9d29ceb":"code","2e88a878":"code","cf17cb0b":"code","0133f438":"code","2fd9e515":"code","fa145ea2":"code","f8b0af6a":"code","e7d27e94":"code","bb8df7c5":"code","7ec270b3":"code","be273578":"markdown","f0bb6551":"markdown","5e44f32e":"markdown","d0d9ff08":"markdown","abc9204b":"markdown","12bc9769":"markdown","9d047612":"markdown","3a70a7f0":"markdown","bfda29da":"markdown","cad9a364":"markdown","99341d67":"markdown","55f64771":"markdown","a00771c8":"markdown","88044f15":"markdown","5f11978f":"markdown","f35a8ae3":"markdown","d3105983":"markdown","fc15e4ea":"markdown","82aa07c4":"markdown","4966dd62":"markdown","718ca488":"markdown","bc6f105a":"markdown","85c4194d":"markdown","68db3baf":"markdown","b5c61b3d":"markdown","21ee693d":"markdown","d1363e20":"markdown","5a3f5931":"markdown","5b8d766c":"markdown","82a61af4":"markdown","8028bab5":"markdown","903be4d2":"markdown","c53094da":"markdown","72cffa00":"markdown","20fadd72":"markdown","a78f35ec":"markdown","e409e1a2":"markdown","02e2e494":"markdown","c1517372":"markdown","caa570d6":"markdown","2679c7d4":"markdown","66d7c40f":"markdown","7e383fa2":"markdown"},"source":{"7bd88c58":"import psutil\n# Getting % usage of virtual_memory ( 3rd field)\nprint('RAM memory % used:', psutil.virtual_memory()[2])","1a6b3e6b":"import pandas as pd\nimport numpy as np\nfrom numpy.random import default_rng\n\nimport os, glob\nfrom pathlib import Path\nfrom pprint import pprint\n\nfrom time import perf_counter as p_f\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GroupKFold, KFold\n\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\n\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb","5743a336":"data_path = \"..\/input\/optiver-realized-volatility-prediction\/\"","febe2abd":"time_series = pd.read_parquet(data_path + '\/book_train.parquet\/stock_id=0')\n\ntime_series.head()\n\ntime_series = time_series[time_series['time_id']<100]\n\ntime_series['row_id'] = [f\"0-{t_id}\" for t_id in time_series['time_id']]\n\ntime_series.drop(['time_id'], axis=1, inplace=True)\n\ntime_series.info()","783fc6e3":"def calc_wap1(box):\n    wap = (box['bid_price1'] * box['ask_size1'] +\n                                box['ask_price1'] * box['bid_size1']) \/ (\n                                       box['bid_size1']+ box['ask_size1'])\n    return wap\n\ndef calc_wap2(box):\n    wap = (box['bid_price2'] * box['ask_size2'] +\n                                box['ask_price2'] * box['bid_size2']) \/ (\n                                       box['bid_size2']+ box['ask_size2'])\n    return wap\n\ndef calc_wap3(box):\n    wap = (box['bid_price1'] * box['bid_size1'] +\n                                box['ask_price1'] * box['ask_size1']) \/ (\n                                       box['bid_size1']+ box['ask_size1'])\n    return wap\n\ndef calc_wap4(box):\n    wap = (box['bid_price2'] * box['bid_size2'] +\n                                box['ask_price2'] * box['ask_size2']) \/ (\n                                       box['bid_size2']+ box['ask_size2'])\n    return wap\n\n# log return\ndef log_return(list_stock_prices, fillna = True):\n    a = np.log(list_stock_prices).diff()\n    if(fillna):\n        return a.fillna(np.mean(a))\n    else:\n        return a\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef n_unique(series):\n    return len(np.unique(series))\n\n#f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\ndef n_above_mean(x):\n    m = np.mean(x)\n    return np.where(x > m)[0].size\n\n#f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\ndef n_below_mean(x):\n    m = np.mean(x)\n    return np.where(x < m)[0].size\n\n#df_max =  np.sum(np.diff(df_id['price'].values) > 0)\ndef n_pos_diff(x):\n    return np.sum(np.diff(x) > 0)\n\n#df_min =  np.sum(np.diff(df_id['price'].values) < 0)\ndef n_neg_diff(x):\n    return np.sum(np.diff(x) < 0)\n\n#abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))\n#abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))  \ndef med_abs_diff(x):\n    return np.median(np.abs( x - np.mean(x)))\n\n#iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n#iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\ndef iqpr(x):\n    return np.percentile(x,75) - np.percentile(x,25)\n\n#energy = np.mean(df_id['price'].values**2)\n#energy_v = np.sum(df_id['size'].values**2)\ndef mean_square(x):\n    return np.mean(x**2)","2e4a6973":"# weights - exponential decay\ndef calcExpWeights(t, halflife = 200):\n    t = np.array(t)\n    \n    # t will be 'age' measured relative to t0, t0 being the start of\n    # the second time box, at 'seconds_in_bucket' = 600s\n    t = 600 - t\n    tau = halflife\/np.log(2)\n    return np.exp(-t\/tau)\n\ndef weighted_average(pd_series,\n                     data_col, w_col,\n                     new_var_name,\n                     exp_w =False,\n                    sqrt = False):\n    data = pd_series[data_col]\n    w = pd_series[w_col]\n    \n    if(exp_w):\n        w = calcExpWeights(w)\n        \n        \n    ws = np.sum(data*w)\/np.sum(w) # weighted sum\n    \n    # root square of result\n    if(sqrt):\n        ws = ws**0.5\n    \n    res = {new_var_name: ws}\n    \n    return pd.Series(res)\n\n\n# calcExpWeights(np.arange(10), halflife=5)","147f10f6":"from scipy import stats\n\n# assume x-data is seconds_in_bucket column\ndef calcGradientR2(pd_series,\n                   y_col,\n                   new_var_name = 'var1'):\n    \n    x = pd_series['seconds_in_bucket']\n    y = pd_series[y_col]\n    \n    if(len(x) == 0 or len(y) ==0):\n        return 0\n    gradient, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n    \n    res = {f\"{new_var_name}_gradient\" : gradient,\n           f\"{new_var_name}_R2\" : r_value**2\n          }\n    \n    return pd.Series(res)","0e563210":"from scipy.interpolate import interp1d\nfrom scipy import stats\nfrom scipy.fft import rfft, irfft, rfftfreq\n\ndef getft_amplitudes(pd_series, y_col, component_list=None, aggf = [np.mean]):\n    \n    x = pd_series['seconds_in_bucket'].values\n    y = pd_series[y_col].values\n    \n    # extrapolate time-series to fill entire box 0-599s\n    # extrapolate by creating new data-points at beginning\/end of series\n    # having the same value as the closest known point\n    # to make the fourier coefficients comparable across all timeseries\n    if(x.min() != 0):\n        x = np.concatenate( ([0], x), axis=0)\n        y = np.concatenate( ([y[0]], y), axis=0)\n        \n    if(x.max() != 599):\n        x = np.concatenate( (x,[599]), axis=0)\n        y = np.concatenate( (y, [y[-1]]), axis=0)\n    \n    #print(y_detrend.mean()) # should be zero after removing the intercept and gradient\n    \n    # interpolate values to get a complete time-series\n    duration = x.max()\n    N = duration*2+1 # total number of samples\n    x_ip = np.linspace(0, duration, num=N, endpoint=True)\n    \n    f_ip = interp1d(x, y, kind=\"linear\")\n    y_ip = f_ip(x_ip) # interpolate values\n \n    gradient, intercept, r_value, p_value, std_err = stats.linregress(x_ip,y_ip)\n    y_detrend = y_ip - (x_ip*gradient + intercept)\n    \n    \n    # normalize signal\n    y_detrend = y_detrend\/(y_detrend.max()-y_detrend.min())\n    \n    yf = rfft(y_detrend)\n    xf = rfftfreq(N, 2)\n    \n    res = {}\n    if(component_list == None):\n        component_list = range(len(yf))\n        \n    # https:\/\/www.mathworks.com\/matlabcentral\/answers\/162846-amplitude-of-signal-after-fft-operation\n    amp = np.abs(yf)\/N\n        \n    for i in component_list:\n        var_name = f'{y_col}_fft{i}'\n        \n        res[var_name] = amp[i]\n        \n    for my_f in aggf:\n        var_name = f'{y_col}_fft_{my_f.__name__}'\n        res[var_name] = my_f(amp)\n        \n    return pd.Series(res)\n    \n\n#ex0 = pd.read_parquet(f\"{data_path}\/book_train.parquet\/stock_id=11\")\n#ex0_0 = ex0[ex0['time_id']==5]\n#res = (ex0.groupby('time_id')).apply(getft_amplitudes, 'bid_price1', [], [np.mean, np.max, np.argmax])\n#res","30eab223":"def flattenIndices(columns, suffix=None):\n    \n    new_colnames = []\n    for index in np.ravel(columns):\n        if type(index) is tuple:\n            varname, fname = index\n            cname = f\"{varname}_{fname}\"\n        else:\n            cname = f\"{index}\"\n            \n        if suffix is not None:\n            cname = f\"{cname}_{suffix}\"\n        new_colnames += [cname]\n            \n    return new_colnames","19ceefb8":"def replaceNaWithMean(df, col_list):\n    cols_with_na = df.columns[df.isnull().any()]\n    num_na = df.isnull().sum().sum()\n    num_notNa = df.count().sum()\n    num_tot = len(df.isnull())\n    \n    print(\"Number of na's:\", num_na, \"of\", num_na + num_notNa, \". Percentage (%) of na's=\", 100*num_na\/(num_notNa+num_na))\n    \n    for col in col_list:\n        df[col] = df.groupby(['stock_id'], sort=False)[col].apply(lambda x: x.fillna(x.mean()))\n    \n    #print(sorted(num_na[num_na>0]*100\/num_tot, reverse=True))\n    return df","a1bea6d7":"def aggregates_per_time_id(df_book_data, var_function_dict,\n                           # seconds_in_bucket is between 0 and 599 (inclusive) in original data\n                           #box_time_windows=[(0,599), (0,149), (150,299), (300,449), (450,599)]):\n                          #box_time_windows=[(0,599), (150,599), (300,599), (450, 599)]):\n                           box_time_windows=[(0,599), (0,99), (100,199), (200,299), (300,399), (400,499), (500,599)]):\n    \n    # bug resulting in NaN's somewhere in this code!\n    df_aggregates_time_id = pd.DataFrame()\n    df_aggregates_time_id['time_id'] = pd.Series(dtype=int)\n    \n    for tw in box_time_windows:\n        min_s, max_s = tw\n        idx = (df_book_data['seconds_in_bucket'] >= min_s) & (df_book_data['seconds_in_bucket'] <= max_s)\n        \n        res =  pd.DataFrame(df_book_data[idx].groupby(['time_id']).agg(var_function_dict)).reset_index(drop=True)\n        res.columns = flattenIndices(res.columns, suffix=f\"t{min_s}-{max_s}\")\n        \n        \n        res['time_id'] = pd.DataFrame(df_book_data[idx].groupby(['time_id']).agg({'time_id':np.mean})).reset_index(drop=True)\n        df_aggregates_time_id = pd.DataFrame(pd.merge(df_aggregates_time_id, res, how='outer', on='time_id')).reset_index(drop=True)\n\n    return df_aggregates_time_id\n\n#df_trade_data = pd.read_parquet(f\"{data_path}\/trade_train.parquet\/stock_id=103\")\n#f_dict = {'size' : np.sum,\n#         'order_count': np.sum}\n#res = aggregates_per_time_id(df_trade_data, f_dict,  box_time_windows=[(0,299),(300,599)])\n\n#idx = df_trade_data['time_id']==9664\n#df_trade_data = df_trade_data[idx]\n\n\n\n#idx = (df_trade_data['seconds_in_bucket'] >= 0) & (df_trade_data['seconds_in_bucket'] <= 300)\n#df_trade_data[idx].groupby(['time_id']).agg(f_dict)\n#res.tail()\n#df_trade_data","f94c6200":"def calcBookAggregatesForStock(si, file_path):\n    bdata = pd.read_parquet(file_path)\n    \n    # calculate statistics that will be used for aggregation\n    # and add them table\n    bdata['wap1'] = calc_wap1(bdata)\n    bdata['wap2'] = calc_wap2(bdata)\n    bdata['wap3'] = calc_wap3(bdata)\n    bdata['wap4'] = calc_wap4(bdata)\n    \n    # Calculate price\/bid spread\n    bdata['price_spread1'] = (bdata['ask_price1'] - bdata['bid_price1']) \/ ((bdata['ask_price1'] + bdata['bid_price1']) \/ 2)\n    bdata['price_spread2'] = (bdata['ask_price2'] - bdata['bid_price2']) \/ ((bdata['ask_price2'] + bdata['bid_price2']) \/ 2)\n    bdata['bid_spread'] = bdata['bid_price1'] - bdata['bid_price2']\n    bdata['ask_spread'] = bdata['ask_price1'] - bdata['ask_price2']\n    bdata[\"bid_ask_spread\"] = abs(bdata['bid_spread'] - bdata['ask_spread'])\n    \n    # volume\n    bdata['total_volume'] = (bdata['ask_size1'] + bdata['ask_size2']) + (bdata['bid_size1'] + bdata['bid_size2'])\n    bdata['volume_imbalance'] = abs((bdata['ask_size1'] + bdata['ask_size2']) - (bdata['bid_size1'] + bdata['bid_size2']))\n    bdata['volume1'] = bdata['bid_size1'] + bdata['ask_size1']\n    bdata['volume2'] = bdata['bid_size2'] + bdata['ask_size2']\n    \n    \n    # need to group by time_id to avoid calculating it across for different time_id's\n    bdata['log_return1'] = (bdata.groupby(['time_id'])['wap1'].apply(log_return))\n    bdata['log_return2'] = (bdata.groupby(['time_id'])['wap2'].apply(log_return))\n    bdata['log_return3'] = (bdata.groupby(['time_id'])['wap3'].apply(log_return))\n    bdata['log_return4'] = (bdata.groupby(['time_id'])['wap4'].apply(log_return))\n    \n    \n    bdata['sqlog_return1'] = bdata['log_return1']*bdata['log_return1']\n    bdata['sqlog_return2'] = bdata['log_return2']*bdata['log_return2']\n    bdata['sqlog_return3'] = bdata['log_return3']*bdata['log_return3']\n    bdata['sqlog_return4'] = bdata['log_return4']*bdata['log_return4']\n    \n    \n    # wap balance\n    bdata['wap_balance'] = abs(bdata['wap1'] - bdata['wap2'])\n    \n    # defines which aggregates to calculate\n    aggf_dict = { # [function_argument, [functions]]\n        'wap1': [np.sum, np.std],\n        'wap2': [np.sum, np.std],\n        'wap3': [np.sum, np.std],\n        'wap4': [np.sum, np.std],\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n        'sqlog_return1': [np.min, np.max],\n        'sqlog_return2': [np.min, np.max],\n        'sqlog_return3': [np.min, np.max],\n        'sqlog_return4': [np.min, np.max],\n        'wap_balance': [np.sum, np.max],\n        'price_spread1':[np.sum, np.max],\n        'price_spread2':[np.sum, np.max],\n        'bid_spread':[np.sum, np.max],\n        'ask_spread':[np.sum, np.max],\n        'total_volume':[np.sum, np.max],\n        'volume_imbalance':[np.sum, np.max],\n        \"bid_ask_spread\":[np.sum,  np.max] }\n    \n    \n    # functions to calculate for the different time windows\n    aggf_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility]\n    }\n    \n    \n    # aggregates over whole time window\n    res1 = aggregates_per_time_id(bdata, aggf_dict,\n                                  box_time_windows=[(0,599)])\n    \n    # aggregates over smaller time windows\n    res2 = aggregates_per_time_id(bdata, aggf_dict_time,\n                                  #box_time_windows=[(0,199), (200,399), (400,599)])\n                                  box_time_windows=[(0,99), (100,199), (200,299), (300,399), (400,499), (500,599)])\n    \n    \n    resAll = pd.merge(res1,res2, how='left', on=['time_id'])\n\n        \n    #\n    # aggregates requiring more than one argument \n    #\n    \n    # Find gradient of squared log-return. Note realized volatility = root-mean of squared-log-returns\n    reg_tmp = (bdata.groupby('time_id')).apply(calcGradientR2, 'sqlog_return1', 'sqlog_return1')\n    resAll = pd.merge(resAll, reg_tmp, on=[\"time_id\"], how=\"left\")\n    \n    reg_tmp = (bdata.groupby('time_id')).apply(calcGradientR2, 'sqlog_return2', 'sqlog_return2')\n    resAll = pd.merge(resAll, reg_tmp, on=[\"time_id\"], how=\"left\")\n    \n    reg_tmp = (bdata.groupby('time_id')).apply(calcGradientR2, 'sqlog_return3', 'sqlog_return3')\n    resAll = pd.merge(resAll, reg_tmp, on=[\"time_id\"], how=\"left\")\n    \n    reg_tmp = (bdata.groupby('time_id')).apply(calcGradientR2, 'sqlog_return4', 'sqlog_return4')\n    resAll = pd.merge(resAll, reg_tmp, on=[\"time_id\"], how=\"left\")\n    \n    \n    #\n    # fft of original features\n    #\n    \n    orig_features = ['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2', 'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2']\n    for col_name in orig_features:\n        res_tmp = (bdata.groupby('time_id')).apply(getft_amplitudes, col_name, [1,2,3], [np.mean, np.max, np.argmax])\n        resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n        \n        \n    # fft of sq-log return\n\n    res_tmp = (bdata.groupby('time_id')).apply(getft_amplitudes, 'sqlog_return1', [1,2,3], [np.mean, np.max, np.argmax])\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    res_tmp = (bdata.groupby('time_id')).apply(getft_amplitudes, 'sqlog_return2', [1,2,3], [np.mean, np.max, np.argmax])\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    \n    # fft of wap return\n    \n    res_tmp = (bdata.groupby('time_id')).apply(getft_amplitudes, 'wap1', [1,2,3], [np.mean, np.max, np.argmax])\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    res_tmp = (bdata.groupby('time_id')).apply(getft_amplitudes, 'wap2', [1,2,3], [np.mean, np.max, np.argmax])\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n        \n    # volume-weighted wap1\/2\n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'wap1', 'volume1', 'vw_wap1', exp_w=False, sqrt=False)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'wap2', 'volume2', 'vw_wap2', exp_w=False, sqrt=False)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\") \n                                                                   \n                                                                \n    # sqrt of volume-weighted sqlog-return 1\/2\n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'sqlog_return1', 'volume1', 'rvw_sqlog_return1', exp_w=False, sqrt=True)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'sqlog_return2', 'volume2', 'rvw_sqlog_return2', exp_w=False, sqrt=True)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    # sqrt of time-weighted sqlog-return 1\/2\n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'sqlog_return1', 'seconds_in_bucket', 'rtw_sqlog_return1', exp_w=True, sqrt=True)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'sqlog_return2', 'seconds_in_bucket', 'rtw_sqlog_return2', exp_w=True, sqrt=True)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    # time-weighted total_volume\n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'total_volume', 'seconds_in_bucket', 'tw_total_volume', exp_w=True, sqrt=False)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    # time-weighted volume_imbalance\n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'volume_imbalance', 'seconds_in_bucket', 'tw_volume_imbalance', exp_w=True, sqrt=False)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    # time-weighted price spread 1\/2\n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'price_spread1', 'seconds_in_bucket', 'tw_price_spread1', exp_w=True, sqrt=False)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'price_spread2', 'seconds_in_bucket', 'tw_price_spread2', exp_w=True, sqrt=False)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n     # add book prefix to all columns except time_id\n    resAll.columns = [(f\"book_{e}\" if e != \"time_id\" else e) for e in resAll.columns]\n    \n    \n    resAll['stock_id'] = f\"{si}\"\n    # to get correct time_id's,\n    resAll['row_id'] = [f\"{si}-{t_id:.0f}\" for t_id in resAll['time_id']]\n\n    return pd.DataFrame(resAll)\n\n#stats1 = calcBookAggregatesForStock(0, f\"{data_path}\/book_train.parquet\/stock_id=0\")\n#stats1.head()","10d12dad":"def calcTradeAggregatesForStock(si, file_path):\n    bdata = pd.read_parquet(file_path)\n    \n    #bdata['time_id'] = bdata['time_id'].astype(int)\n\n    # need to group by time_id to avoid calculating it across for different time_id's\n    bdata['log_return'] = (bdata.groupby(['time_id'])['price'].apply(log_return))\n    bdata['amount']=bdata['price']*bdata['size']\n    bdata['sqlog_return'] = bdata['log_return']*bdata['log_return']\n    \n    # Dict for aggregations\n    aggf_dict = {\n        'log_return':[realized_volatility],\n        'sqlog_return': [np.sum, np.max],\n        'seconds_in_bucket':[n_unique],\n        'size':[np.sum, np.max, med_abs_diff, iqpr],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max],\n        'price':[np.sum, n_above_mean, n_below_mean, n_pos_diff, n_neg_diff, med_abs_diff, iqpr]\n    }\n    \n    aggf_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[n_unique],\n        'size':[np.sum],\n        'order_count':[np.sum],\n    }\n\n    # aggregates over whole time window\n    res1 = aggregates_per_time_id(bdata, aggf_dict,\n                                  box_time_windows=[(0,599)])\n    \n    # aggregates over smaller time windows\n    res2 = aggregates_per_time_id(bdata, aggf_dict_time,\n                                  box_time_windows=[(0,99), (100,199), (200,299), (300,399), (400,499), (500,599)])\n    \n    \n   \n    # regress on square log-return\n    reg_tmp = (bdata.groupby('time_id')).apply(calcGradientR2, 'sqlog_return', 'sqlog_return')\n    res2 = pd.merge(res2, reg_tmp, on=[\"time_id\"], how=\"left\") \n    \n    resAll = pd.merge(res1,res2, how='left', on=['time_id'])\n    \n    \n    # functions requiring more than one argument\n    \n    #\n    # fft of original features\n    #\n    \n    orig_features = ['price', 'size', 'order_count']\n    for col_name in orig_features:\n        res_tmp = (bdata.groupby('time_id')).apply(getft_amplitudes, col_name, [], [np.mean, np.max, np.argmax])\n        resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n        \n        \n    # fft of sqlog return\n    \n    res_tmp = (bdata.groupby('time_id')).apply(getft_amplitudes, 'sqlog_return', [], [np.mean, np.max, np.argmax])\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    # fft of amount\n    res_tmp = (bdata.groupby('time_id')).apply(getft_amplitudes, 'amount', [], [np.mean, np.max, np.argmax])\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    # volume-weighted average price\n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'price', 'size', 'vw_price', exp_w=False, sqrt=False)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n\n    # sqrt of volume-weighted sqlog-return\n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'sqlog_return', 'size', 'rvw_sqlogreturn', exp_w=False, sqrt=True)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    # sqrt of time-weighted sqlog-return\n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'sqlog_return', 'seconds_in_bucket', 'rtw_sqlogreturn', exp_w=True, sqrt=True)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    # time-weighted size\n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'size', 'seconds_in_bucket', 'tw_size', exp_w=True, sqrt=False)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")\n    \n    # time-weighted order_count\n    res_tmp = (bdata.groupby('time_id')).apply(weighted_average, 'order_count', 'seconds_in_bucket', 'tw_order_count', exp_w=True, sqrt=False)\n    resAll = pd.merge(resAll, res_tmp, on=[\"time_id\"], how=\"left\")    \n    \n    # add trade prefix to all columns except time_id\n    resAll.columns = [(f\"trade_{e}\" if e != \"time_id\" else e) for e in resAll.columns]\n\n    resAll['stock_id'] = f\"{si}\"\n    # to get correct time_id's,\n    resAll['row_id'] = [f\"{si}-{t_id:.0f}\" for t_id in resAll['time_id']]\n\n    return pd.DataFrame(resAll)\n\n\n#stats2 = calcTradeAggregatesForStock(0, f\"{data_path}\/trade_train.parquet\/stock_id=0\")\n#stats2.head()","d2541a11":"def calcTimeStockAgg(df_data):\n    \n    # calculate for previous volatilty\n    vol_cols = [c for c in df_data.columns if ( \"volatility_t0-599\" in c) or ( \"volatility1\" in c)\n               or (\"wap1_sum_t0-599\" in c) or ('volume1_sum_t0-599' in c) or ('price_spread1_sum_t0-599' in c)]\n    \n    #print(sorted(vol_cols))\n    \n    # Group by the stock id\n    df_stock_id = df_data.groupby(['stock_id'])[vol_cols].agg(['mean', 'std']).reset_index()\n      \n    # Rename columns\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_for_stock_id')\n\n    # Group by the time id\n    df_time_id = df_data.groupby(['time_id'])[vol_cols].agg(['mean', 'std' ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_for_time_id')\n    \n    # Merge with original dataframe\n    df_data = pd.merge(df_data, df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__for_stock_id'])\n    df_data = pd.merge(df_data, df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__for_time_id'])\n    df_data.drop(['stock_id__for_stock_id', 'time_id__for_time_id'], axis = 1, inplace = True)\n    \n    return df_data","c49abe06":"from joblib import Parallel, delayed\n\ndef calcAggregatesParallell(stock_idx, is_train=True, is_book=True, n_jobs=-1):  \n    to_do = []\n    \n    dataset = \"train\" if is_train else \"test\"\n    \n    datalib = \"book\" if is_book else \"trade\"\n    \n    calcAggF = calcBookAggregatesForStock if is_book else calcTradeAggregatesForStock\n    \n    for si in stock_idx:\n        file_path = f\"{data_path}\/{datalib}_{dataset}.parquet\/stock_id={si}\"\n        to_do += [delayed(calcAggF)(si, file_path)]\n        \n    p_res = Parallel(n_jobs=n_jobs, verbose=3)(to_do)\n\n    df_boxagg = pd.concat(p_res, axis=0)\n    \n    df_boxagg['time_id'] = df_boxagg['time_id'].astype(int)\n    df_boxagg['stock_id'] = df_boxagg['stock_id'].astype(int)\n    df_boxagg.reset_index(inplace=True, drop=True)\n    \n    return df_boxagg\n\n\n# assumes data_path is set before\ndef calcAllAggregatesParallell(train_stock_idx, test_stock_idx):\n    \n    print(\"Calculating stats for book training data...\")\n    b_agg_train = calcAggregatesParallell(train_stock_idx, is_train = True, is_book=True)\n    \n    print(\"Calculating stats for trade training data...\")\n    t_agg_train = calcAggregatesParallell(train_stock_idx, is_train = True, is_book=False)\n    bt_agg_train = pd.merge(b_agg_train, t_agg_train, on = ['stock_id', 'time_id', 'row_id'], how='left')\n      \n    print(\"Calculating stats for book test data...\")\n    b_agg_test = calcAggregatesParallell(test_stock_idx, is_train = False, is_book=True)\n    \n    print(\"Calculating stats for trade test data...\")\n    t_agg_test = calcAggregatesParallell(test_stock_idx, is_train = False, is_book=False)\n    bt_agg_test = pd.merge(b_agg_test, t_agg_test, on = ['stock_id', 'time_id', 'row_id'], how='left')\n    \n    \n    print(\"Calculating stats over stock_id's\/time_id's... (not parallelized)\")\n    bt_agg_train = calcTimeStockAgg(bt_agg_train)\n    bt_agg_train['time_id'] = bt_agg_train['time_id'].astype(int)\n    bt_agg_train['stock_id'] = bt_agg_train['stock_id'].astype(int)\n    bt_agg_train.reset_index(inplace=True, drop=True)\n    \n    bt_agg_test = calcTimeStockAgg(bt_agg_test)\n    bt_agg_test['time_id'] = bt_agg_test['time_id'].astype(int)\n    bt_agg_test['stock_id'] = bt_agg_test['stock_id'].astype(int)\n    bt_agg_test.reset_index(inplace=True, drop=True)\n    \n    \n    print(\"Loading target value from training data and adding to full table... (not parallelized)\")\n    # add target values from training data\n    # (note: no target value in test to add)\n    train = pd.read_csv(data_path + 'train.csv')\n\n    # same datatype for join to work\n    train['time_id'] = train['time_id'] .astype(int)\n    train['stock_id'] = train['stock_id'].astype(int)\n    train.reset_index(inplace=True, drop=True)\n\n    bt_agg_train = pd.merge(bt_agg_train, train, on = ['stock_id', 'time_id'], how='left')\n    \n    print(bt_agg_train.head())\n    \n    \n    return (bt_agg_train, bt_agg_test)","183b409a":"# root mean square percentage error\ndef rmspe(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","e334a1f2":"book_train_path = f\"{data_path}\/book_train.parquet\"\nbook_test_path = f\"{data_path}\/book_test.parquet\"\n\ntrade_train_path = f\"{data_path}\/trade_train.parquet\"\ntrade_test_path = f\"{data_path}\/trade_test.parquet\"\n\n# find all stock id's\n# will be read as a category, use cat.categories for unique values\nbtrain_sidx = pd.read_parquet(book_train_path, columns = ['stock_id'])['stock_id'].cat.categories\nbtest_sidx = pd.read_parquet(book_test_path, columns = ['stock_id'])['stock_id'].cat.categories\n\nttrain_sidx = pd.read_parquet(trade_train_path, columns = ['stock_id'])['stock_id'].cat.categories\nttest_sidx = pd.read_parquet(trade_test_path, columns = ['stock_id'])['stock_id'].cat.categories\n\n# find all time id's\n# will be read as an int, use np.unique for unique values\nbtrain_tidx = np.unique(pd.read_parquet(book_train_path, columns = ['time_id'])['time_id'])\nbtest_tidx = np.unique(pd.read_parquet(book_test_path, columns = ['time_id'])['time_id'])\n\nttrain_tidx = np.unique(pd.read_parquet(trade_train_path, columns = ['time_id'])['time_id'])\nttest_tidx = np.unique(pd.read_parquet(trade_test_path, columns = ['time_id'])['time_id'])","0e0cbb46":"if(not np.array_equal(btrain_sidx, ttrain_sidx)):\n    print(\"Warning, stock indices are different in trade and book data for training set.\")\nelse:\n    print(\"Number of stock indices in book\/trade training set:\", len(ttrain_sidx))\n    \nif(not np.array_equal(btest_sidx, ttest_sidx)):\n    print(\"Warning, stock indices are different in trade and book data for test set.\")\nelse:\n    print(\"Number of stock indices in book\/trade test set:\", len(ttest_sidx))\n\nif(not np.array_equal(btrain_tidx, ttrain_tidx)):\n    print(\"Warning, time indices are different in trade and book data for training set.\")\nelse:\n    print(\"Number of time indices in book\/trade training set:\", len(ttrain_tidx))\nif(not np.array_equal(btest_tidx, ttest_tidx)):\n    print(\"Warning, time indices are different in trade and book data for test set.\")\nelse:\n    print(\"Number of time indices in book\/trade test set:\", len(ttest_tidx))","9233a7d3":"# Getting % usage of virtual_memory ( 3rd field)\nprint('RAM memory % used:', psutil.virtual_memory()[2])","840c96b8":"t0 = p_f()\n\n(all_agg_train, all_agg_test) = calcAllAggregatesParallell(btrain_sidx[:], btest_sidx) # returns (train, test) data\n\nprint(\"Time for pre-processing of train\/test, book\/trade data (h): \", (p_f()-t0)\/3600 ) # ","030c1123":"# Getting % usage of virtual_memory ( 3rd field)\nprint('RAM memory % used:', psutil.virtual_memory()[2])","47df55d7":"all_agg_train.head()","3271a693":"from sklearn.cluster import KMeans\n\ndef calcCorrClusterDict(df, n_clusters=6):\n    time_stock_target = df[['time_id', 'stock_id', 'target']]\n    train_p = time_stock_target.pivot(index='time_id', columns='stock_id', values='target')\n\n    corr = train_p.corr()\n    ids = corr.index\n    kmeans = KMeans(n_clusters=n_clusters, random_state=111, max_iter=500).fit(corr.values)\n    stock_clusters = kmeans.predict(corr.values)\n    \n    # print number of occurences of each cluster\n    #print(np.unique(stock_clusters, return_counts=True))\n\n    si_cluster_dict = { ids[i] : sc for i, sc in enumerate(stock_clusters)}\n    return si_cluster_dict\n\n\ndef calcMargClusterDict(df, n_clusters=6):\n    time_aggr_cols = [e for e in df.columns if \"for_stock_id\" in e] # marginal aggregates for stock_ids (values aggregated over all time-ids)\n    time_aggr_cols += ['stock_id']\n    cluster_data = df[time_aggr_cols].groupby(['stock_id']).agg(np.mean) # rows with same stock id have same values for these cols\n    \n    ids = cluster_data.index\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=111, max_iter=300).fit(cluster_data.values)\n    stock_clusters = kmeans.predict(cluster_data.values)\n    #print(np.unique(stock_clusters, return_counts=True))\n\n    si_cluster_dict = { ids[i] : sc for i, sc in enumerate(stock_clusters)}\n    return si_cluster_dict","952dd622":"# add feature describing which cluster the stock is in from k-means\n\nc_cluster_dict = calcCorrClusterDict(all_agg_train)\nm_cluster_dict = calcMargClusterDict(all_agg_train)\n\nall_agg_train['stock_corr_cluster_id'] = all_agg_train['stock_id'].map(c_cluster_dict)\nall_agg_train['stock_marg_cluster_id'] = all_agg_train['stock_id'].map(m_cluster_dict)\nall_agg_test['stock_corr_cluster_id'] = all_agg_test['stock_id'].map(c_cluster_dict)\nall_agg_test['stock_marg_cluster_id'] = all_agg_test['stock_id'].map(m_cluster_dict)","b5236543":"#!pip install umap-learn","a27d59e3":"import umap\n\ndef calcUMAPCoordsDict(df, n_components = 2, n_clusters=5, plot=True):\n    \n    time_aggr_cols = [e for e in df.columns if \"for_stock_id\" in e] # marginal aggregates for stock_ids (values aggregated over all time-ids)\n    time_aggr_cols += ['stock_id']\n    cluster_data = df[time_aggr_cols].groupby(['stock_id']).agg(np.mean) # rows with same stock id have same values for these cols\n    \n    ids = cluster_data.index\n    \n    umap_clst = umap.UMAP(n_components=n_components, n_neighbors=4, min_dist=0.05, random_state=111)\n    embedding = umap_clst.fit_transform(cluster_data)\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=111, max_iter=500).fit(embedding)\n    embedding_clusters = kmeans.predict(embedding)\n    \n    if(plot):\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.scatter(embedding[:,0], embedding[:,1], c = embedding_clusters)\n        \n    si_embedding_dict = {}\n    # embeddings\n    for ni in range(n_components):\n        si_embedding_dict[ni] = { ids[i] : emb[ni] for i, emb in enumerate(embedding)}\n    \n    # clusters\n    si_embedding_dict[n_components] = { ids[i] : cl for i, cl in enumerate(embedding_clusters)}\n    \n    # si_embedding_dict[0] will have first coords, si_embedding_dict[1] will have seconds coords, ...\n    return si_embedding_dict","d9b0cb65":"si_embedding_dict = calcUMAPCoordsDict(all_agg_train, plot=True)\nall_agg_train['stock_umap_coord0'] = all_agg_train['stock_id'].map(si_embedding_dict[0])\nall_agg_train['stock_umap_coord1'] = all_agg_train['stock_id'].map(si_embedding_dict[1])\nall_agg_train['stock_umap_cluster'] = all_agg_train['stock_id'].map(si_embedding_dict[2])\nall_agg_test['stock_umap_coord0'] = all_agg_test['stock_id'].map(si_embedding_dict[0])\nall_agg_test['stock_umap_coord1'] = all_agg_test['stock_id'].map(si_embedding_dict[1])\nall_agg_test['stock_umap_cluster'] = all_agg_test['stock_id'].map(si_embedding_dict[2])","45afcab5":"# Getting % usage of virtual_memory ( 3rd field)\nprint('RAM memory % used:', psutil.virtual_memory()[2])","c54529f8":"import pickle\n\nwith open('aggf.data', 'wb') as f:\n    pickle.dump((all_agg_train, all_agg_test), f)\n    \n# with open('aggf.data', 'rb') as f:\n#     (all_agg_train, all_agg_test) = pickle.load(f) ","deec7c61":"all_agg_train.head()","1b93b397":"all_agg_test.head()","a363aebc":"# replace na's with mean for variables such as mean price that does not depend on number of orders\n\n\"\"\"\ncols_mean = [e for e in all_agg_train.columns if not\n             ((\"size\" in e) or (\"count\" in e) or (\"unique\" in e) or (\"volume\") in e\n             or (\"n_above\" in e) or (\"n_below\" in e) or (\"n_pos\" in e) or (\"n_neg\" in e))]\n\"\"\"\n# note: this will set variables such as n_above_mean to their averages (over all timeid's for the respective stocks)\n# within their time-frame, while setting size\/order to the true value of zero\n#all_agg_train = replaceNaWithMean(all_agg_train, cols_mean)\n#all_agg_test = replaceNaWithMean(all_agg_test, cols_mean)\n\n# zero reasonable value for (all?) other Nas (i.e. order size being 0 since no orders, sum of price being 0 since to trades, standard deviation of price 0 since no data points\/deviations...)\nall_agg_train = all_agg_train.fillna(0)\nall_agg_test = all_agg_test.fillna(0)","39af5105":"# #!pip install pandas-downcast\n\n# import pdcast as pdc\n# import gc\n\n# all_agg_train = pdc.downcast(all_agg_train)\n# all_agg_test = pdc.downcast(all_agg_test)\n\n# gc.collect()","2f0b6041":"# Getting % usage of virtual_memory ( 3rd field)\nprint('RAM memory % used:', psutil.virtual_memory()[2])\nall_agg_train.info()","e7be08c2":"# Getting % usage of virtual_memory ( 3rd field)\nprint('RAM memory % used:', psutil.virtual_memory()[2])","e481f579":"from sklearn.utils import shuffle\nfrom sklearn.model_selection import GroupKFold, train_test_split\n\ndef train_valid_groupsplit(X_data, y_data, group_idx, random_state=111, ratio=4):\n    \n    n_splits = ratio+1 # ratio is num_train\/num_valid (must be integer)\n    gkfold = GroupKFold(n_splits = n_splits)\n    \n    X_shuffled, y_shuffled, groups_shuffled = shuffle(X_data, y_data, group_idx, random_state=random_state)\n\n    t_idx, v_idx = list(gkfold.split(X_shuffled, groups=groups_shuffled))[0] # no randomness in groupkfold\n\n    x_t = X_shuffled.iloc[t_idx]\n    x_v = X_shuffled.iloc[v_idx]\n\n    y_t = y_shuffled.iloc[t_idx]\n    y_v = y_shuffled.iloc[v_idx]\n    \n    return (x_t, x_v, y_t, y_v)\n\n#(x_tt, x_tv, y_tt, y_tv) = train_valid_groupsplit(all_agg_train.drop([\"time_id\", 'target', 'row_id'], axis=1), all_agg_train['target'], all_agg_train['time_id'])","5dfd7269":"from sklearn.model_selection import train_test_split\n\nX_data = all_agg_train.drop(['target', 'row_id'], axis=1)\ny_data = all_agg_train['target']\nX_test = all_agg_test.drop(['row_id'], axis=1)\n\n# x_tt, x_tv, y_tt, y_tv = train_valid_groupsplit(X_data, y_data, X_data['time_id'])\n# time_idxs_tt = x_tt['time_id']\n# time_idxs_tv = x_tv['time_id']\n\n# x_tt = x_tt.drop(['time_id'], axis=1)\n# x_tv = x_tv.drop(['time_id'], axis=1)\n\ncategorical_features = ['stock_umap_cluster', 'stock_corr_cluster_id', 'stock_marg_cluster_id', 'stock_id']","986a60e4":"lgb_paras0 = {\n    'num_iterations':2000,\n    'objective': 'rmse',  \n    'boosting_type': 'gbdt',\n    'num_leaves': 40,\n    'n_jobs': -1,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.6,\n    'bagging_fraction': 0.3,\n    'min_data_in_leaf': 70,\n    'max_depth': 3,\n    'verbose': -1,\n     'device' : \"cpu\"\n}\n\nlgb_paras1 = {\n        'num_iterations':2000,\n    'bagging_fraction': 0.21498006,\n 'boosting_type': 'gbdt',\n 'feature_fraction': 0.66163467,\n 'learning_rate': 0.03146706,\n 'max_depth': 4,\n 'min_data_in_leaf': 78,\n 'n_jobs': -1,\n 'num_leaves': 43,\n 'objective': 'rmse',\n 'verbose': -1}\n\nlgb_paras2 = {\n        'num_iterations':2000,\n    'bagging_fraction': [0.12037843],\n 'boosting_type': 'gbdt',\n 'feature_fraction': [0.59048959],\n 'learning_rate': [0.05567519],\n 'max_depth': 5,\n 'min_data_in_leaf': 79,\n 'n_jobs': -1,\n 'num_leaves': 35,\n 'objective': 'rmse',\n 'verbose': -1}\n\n\nlgb_paras3 = {\n        'num_iterations':2000,\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': 5,\n    'num_leaves' :45,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.6,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'n_jobs':-1,\n    'verbose': -1}\n\n\nlgb_paras4 = {\n        'num_iterations':2000,\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': 3,\n    'num_leaves' :45,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.01,\n    'subsample': 0.8,\n    'subsample_freq': 3,\n    'feature_fraction': 0.6,\n    'lambda_l1': 5.0,\n    'lambda_l2': 20.0,\n    'n_jobs':-1,\n    'verbose': -1}","fe942004":"from lightgbm import LGBMRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\nclass lgbEnsemble():\n    def __init__(self, n_folds = 5, lgb_paras={}):\n        self.n_folds_ = n_folds\n        self.lgb_paras_ = lgb_paras\n        return\n\n    def fit(self, X_data, y_data, group_idx, n_early_stopping=50, verbose_freq=50, categorical_features=[]):\n        kfold = GroupKFold(n_splits = self.n_folds_)\n        self.oof_predictions_ = np.zeros(X_data.shape[0])\n        \n        self.estimators_ = []\n        \n        self.weights = np.zeros(self.n_folds_)\n        \n        X_data, y_data, group_idx = shuffle(X_data, y_data, group_idx, random_state=111)\n        \n        for k, (t_idx, v_idx) in enumerate(kfold.split(X_data, groups=group_idx)):\n            \n            \n            x_tt, x_tv = X_data.iloc[t_idx], X_data.iloc[v_idx]\n            y_tt, y_tv = y_data.iloc[t_idx], y_data.iloc[v_idx]\n            \n            # to get rmspe errors:\n            train_weights = 1 \/ np.square(y_tt)\n            val_weights = 1 \/ np.square(y_tv)\n            feval_error = feval_rmspe\n            \n            train_dataset = lgb.Dataset(x_tt, y_tt, weight = train_weights, categorical_feature = categorical_features)\n            val_dataset = lgb.Dataset(x_tv, y_tv, weight = val_weights, categorical_feature = categorical_features)\n            model = lgb.train(params = self.lgb_paras_, \n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          early_stopping_rounds = n_early_stopping, \n                          verbose_eval = verbose_freq,\n                          feval = feval_rmspe)\n            \n            \n            \n            self.estimators_ += [model]\n            self.oof_predictions_[v_idx] = model.predict(x_tv)\n            \n        \n        my_score = rmspe(y_data, self.oof_predictions_)\n        self.oof_rmspe_ = my_score\n        print(f'Out-of-folds error is {my_score}')\n            \n        return self\n    \n    def predict(self, X_data):\n        \n        predictions = np.zeros(len(X_data))\n        \n        n_e = len(self.estimators_)\n        \n        for k, est in enumerate(self.estimators_):\n            predictions += est.predict(X_data)\/n_e\n        \n        return predictions","e8c6715c":"from catboost import CatBoostRegressor, Pool\n\nclass cbrEnsemble():\n    def __init__(self, n_folds = 5, cbr_paras={}):\n        self.n_folds_ = n_folds\n        self.cbr_paras_ = cbr_paras\n        return\n    \n    def fit(self, X_data, y_data, group_idx, n_early_stopping=50, verbose_freq=50, categorical_features=[]):\n        kfold = GroupKFold(n_splits = self.n_folds_)\n        self.oof_predictions_ = np.zeros(X_data.shape[0])\n        \n        self.estimators_ = []\n        \n        self.weights = np.zeros(self.n_folds_)\n        \n        \n        X_data, y_data, group_idx = shuffle(X_data, y_data, group_idx, random_state=111)\n        \n        \n        for k, (t_idx, v_idx) in enumerate(kfold.split(X_data, groups=group_idx)):\n            \n            \n            x_tt, x_tv = X_data.iloc[t_idx], X_data.iloc[v_idx]\n            y_tt, y_tv = y_data.iloc[t_idx], y_data.iloc[v_idx]\n            \n            train_weights = 1 \/ np.square(y_tt)\n            val_weights = 1 \/ np.square(y_tv)\n            feval_error = feval_rmspe\n            \n            \n            \n            train_dataset = Pool(x_tt,y_tt, weight=train_weights, cat_features = categorical_features)\n            val_dataset = Pool(x_tv, y_tv, weight = val_weights, cat_features = categorical_features)\n            \n            model = CatBoostRegressor(**self.cbr_paras_)\n            \n            model = model.fit(train_dataset,\n                          eval_set = val_dataset, \n                          early_stopping_rounds = n_early_stopping, \n                          verbose_eval = verbose_freq)\n            \n            \n            \n            self.estimators_ += [model]\n            self.oof_predictions_[v_idx] = model.predict(x_tv)\n            \n        \n        my_score = rmspe(y_data, self.oof_predictions_)\n        self.oof_rmspe_ = my_score\n        print(f'Out-of-folds error is {my_score}')\n            \n        return self\n    \n    def predict(self, X_data):\n        \n        predictions = np.zeros(len(X_data))\n        \n        n_e = len(self.estimators_)\n        \n        for k, est in enumerate(self.estimators_):\n            predictions += est.predict(X_data)\/n_e\n        \n        return predictions","e9d29ceb":"from sklearn.linear_model import LinearRegression\n\nclass lrStockEnsemble():\n    def __init__(self):\n        return\n    \n    def fit(self, X_data, y_data):\n        self.ensemble_dict_ = {}\n        stock_idxs = np.unique(X_data['stock_id'])\n        \n        for si in stock_idxs:\n            \n            rows = (X_data['stock_id']==si)\n            \n            X = X_data.loc[rows]\n            y = y_data.loc[rows]\n            \n            train_weights = 1 \/ np.square(y)\n            \n            model = LinearRegression()\n            model = model.fit(X, y,\n                             sample_weight=train_weights)\n            \n            self.ensemble_dict_[si] = model\n            \n        \n        \n            \n        return self\n    \n    def predict(self, X_data):\n        \n        predictions = np.zeros(len(X_data))\n\n        pred_f = lambda x: self.ensemble_dict_[x['stock_id']].predict(x.values.reshape(1, -1))[0]\n        return X_data.apply(pred_f, axis=1)\n    \n\n#lr_se = lrStockEnsemble()\n\n#lr_se.fit(x_tt, y_tt)\n#rmspe(y_tv, lr_se.predict(x_tv))","2e88a878":"from sklearn.linear_model import LinearRegression\n\nclass lrEnsemble():\n    def __init__(self, n_folds = 5):\n        self.n_folds_ = n_folds\n        return\n    \n    def fit(self, X_data, y_data, group_idx):\n        kfold = GroupKFold(n_splits = self.n_folds_)\n        self.oof_predictions_ = np.zeros(X_data.shape[0])\n        \n        self.estimators_ = []\n\n        X_data, y_data, group_idx = shuffle(X_data, y_data, group_idx, random_state=111)\n        \n        for k, (t_idx, v_idx) in enumerate(kfold.split(X_data, groups=group_idx)):\n            \n            x_tt, x_tv = X_data.iloc[t_idx], X_data.iloc[v_idx]\n            y_tt, y_tv = y_data.iloc[t_idx], y_data.iloc[v_idx]\n            \n            train_weights = 1 \/ np.square(y_tt)\n            \n            model = LinearRegression()\n            model = model.fit(x_tt, y_tt,\n                             sample_weight=train_weights)\n            \n            self.estimators_ += [model]\n            self.oof_predictions_[v_idx] = model.predict(x_tv)\n            \n        \n        my_score = rmspe(y_data, self.oof_predictions_)\n        self.oof_rmspe_ = my_score\n        print(f'Out-of-folds error is {my_score}')\n            \n        return self\n    \n    def predict(self, X_data):\n        \n        predictions = np.zeros(len(X_data))\n        \n        n_e = len(self.estimators_)\n        \n        for k, est in enumerate(self.estimators_):\n            predictions += est.predict(X_data)\/n_e\n        \n        return predictions","cf17cb0b":"class optModelStack():\n    def __init__(self, lgb_ens_parasets,\n                 cbr_ens_parasets,\n                 lgb_meta_paras,\n                 n_folds=5):\n        \n        \n        self.feature_names_ = {'lgb':[], 'cbr':[], 'lr':[]}\n        self.cbr_ensembles_ = []\n        self.lgb_ensembles_ = []\n        \n        n_ens = len(lgb_ens_parasets) + len(cbr_ens_parasets)\n        \n        # initiate lgbEnsembles\n        for i, paras in enumerate(lgb_ens_parasets):\n            self.lgb_ensembles_ += [lgbEnsemble(n_folds=n_folds, lgb_paras = paras)]\n            self.feature_names_['lgb'] += [f'lgb_f{i}']\n            \n            \n        for i, paras in enumerate(cbr_ens_parasets):\n            self.cbr_ensembles_ += [cbrEnsemble(n_folds=n_folds, cbr_paras = paras)]\n            self.feature_names_['cbr'] += [f'cbr_f{i}']\n            \n            \n        self.lr_ensembles_ = [lrEnsemble()]\n        self.feature_names_['lr'] += ['f0']\n        \n        self.meta_model_ = lgbEnsemble(n_folds=n_folds, lgb_paras = lgb_meta_paras)\n        \n        self.n_folds=n_folds\n        return\n    \n    def fit(self, X_data, y_data, group_idx, **extra_kwargs):\n        \n        self.oof_predictions_ = pd.DataFrame()\n        \n        for i, ens in enumerate(self.lgb_ensembles_):\n            ens.fit(X_data, y_data, group_idx, **extra_kwargs)\n            col_name = self.feature_names_['lgb'][i]\n            self.oof_predictions_[col_name] = ens.oof_predictions_\n            print(\"lgb\", i, \"error:\", ens.oof_rmspe_)\n            \n        for i, ens in enumerate(self.cbr_ensembles_):\n            ens.fit(X_data, y_data, group_idx, **extra_kwargs)\n            col_name = self.feature_names_['cbr'][i]\n            self.oof_predictions_[col_name] = ens.oof_predictions_\n            print(\"cbr\", i, \"error:\", ens.oof_rmspe_)\n            \n            \n        for i, ens in enumerate(self.lr_ensembles_):\n            ens.fit(X_data, y_data, group_idx)\n            col_name = self.feature_names_['lr'][i]\n            self.oof_predictions_[col_name] = ens.oof_predictions_\n            print(\"lr\", i, \"error:\", ens.oof_rmspe_)\n        \n        \n        # assumed rmspe error\n        sw = 1\/np.square(y_data)\n        self.oof_predictions_.set_index(X_data.index)\n        \n        X_data = pd.concat([X_data.reset_index(drop=True), self.oof_predictions_], axis=1)\n        \n        \n        self.meta_model_.fit(X_data, y_data, group_idx)\n        \n        return self\n    \n    def predict_L0(self, X_data):\n        predictions = pd.DataFrame()\n        \n        for i, ens in enumerate(self.lgb_ensembles_):\n            col_name = self.feature_names_['lgb'][i]\n            predictions[col_name] = ens.predict(X_data)\n        \n        for i, ens in enumerate(self.cbr_ensembles_):\n            col_name = self.feature_names_['cbr'][i]\n            predictions[col_name] = ens.predict(X_data)\n            \n        for i, ens in enumerate(self.lr_ensembles_):\n            col_name = self.feature_names_['lr'][i]\n            predictions[col_name] = ens.predict(X_data)\n            \n            \n        return predictions\n    \n    def predict(self, X_data):\n        res = self.predict_L0(X_data)\n        X_data = pd.concat([X_data.reset_index(drop=True), res], axis=1)\n        \n        return self.meta_model_.predict(X_data)","0133f438":"def evaluate_lr(X, y, group_idx, n_folds=5):\n    gkf = GroupKFold(n_splits=n_folds).split(X,y,groups=group_idx)\n    \n    X, y, group_idx = shuffle(X, y, group_idx, random_state=111)\n    \n    error = 0\n    for t_i, v_i in gkf:\n        model = LinearRegression().fit(X.iloc[t_i], y.iloc[t_i], sample_weight = 1\/np.square(y.iloc[t_i]))\n        error += rmspe(y.iloc[v_i], model.predict(X.iloc[v_i]))\/n_folds\n        \n    return error\n\n\ndef evaluate_knnr(X, y, group_idx, n_folds=5):\n    gkf = GroupKFold(n_splits=n_folds).split(X,y,groups=group_idx)\n    \n    X, y, group_idx = shuffle(X, y, group_idx, random_state=111)\n    \n    error = 0\n    for t_i, v_i in gkf:\n        model = KNeighborsRegressor(n_neighbors=10, weights='distance', metric='euclidean').fit(X.iloc[t_i], y.iloc[t_i])\n        error += rmspe(y.iloc[v_i], model.predict(X.iloc[v_i]))\/n_folds\n    return error","2fd9e515":"print('RAM memory % used:', psutil.virtual_memory()[2])\ncbr_paras0 = {'loss_function' : 'RMSE',\n             'iterations' : 600,\n              'learning_rate': 0.05,\n             'depth': 6,\n              'task_type': 'CPU',\n             'min_data_in_leaf': 3}","fa145ea2":"lgb_paralist = [lgb_paras0, lgb_paras1, lgb_paras2]\ncbr_paralist = [cbr_paras0]\n\nmodel_stack = optModelStack(lgb_paralist, cbr_paralist, lgb_paras3, n_folds=7)\n\nmodel_stack.fit(X_data.drop(['time_id'], axis=1), y_data, X_data['time_id'], verbose_freq=0, categorical_features=categorical_features)","f8b0af6a":"# Getting % usage of virtual_memory ( 3rd field)\nprint('RAM memory % used:', psutil.virtual_memory()[2])","e7d27e94":"# not needed?\n#test = pd.read_csv(data_path + 'test.csv')\n#test['time_id'] = test['time_id'] .astype(int)\n#test['stock_id'] = test['stock_id'].astype(int)\n#test.reset_index(inplace=True, drop=True)","bb8df7c5":"test_pred = model_stack.predict(X_test.drop(['time_id'], axis=1))\nall_agg_test['target'] = test_pred\nall_agg_test.head()","7ec270b3":"final_result = all_agg_test[['row_id', 'target']]\nfinal_result.to_csv('submission.csv',index = False)\nfinal_result.head()","be273578":"### Train ensemble\n\nTrain models for each stock id and calculate average oof-errors","f0bb6551":"This function will replace Na's in a dataframe with their means, for each column in col_list.\n\nMeans are calculated for the different stock-ids (averaged over time-id's).","5e44f32e":"## Links\n\n* https:\/\/www.kaggle.com\/tommy1028\/lightgbm-starter-with-feature-engineering-idea (some feature engineering ideas... with lightgbm\/crossvalidation for prediction)\n* https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data\/ (official tutorial notebook)\n* https:\/\/www.kaggle.com\/lucasmorin\/feature-engineering-aggregation-functions (feature engineering, lists many functions)\n* https:\/\/www.kaggle.com\/ragnar123\/optiver-realized-volatility-lgbm-baseline (group features by seconds in bucket)\n* https:\/\/www.kaggle.com\/endremoen\/feature-engineering-optimal-clustering (overview of clustering methods)\n* https:\/\/www.kaggle.com\/hiromasatabuchi\/baseline-simple-flow-with-lightgbm (good starting point)\n* https:\/\/www.kaggle.com\/monolith0456\/2xlgbm-fnn-ensemble (0.19... score)\n* https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/263308 (note groupkfold instead of kfold for better CV error, point 2)","d0d9ff08":"# Train models\n## Function definitions\n### Parameter sets","abc9204b":"### LightGBM ensemble","12bc9769":"## Train lightgbm model\n### Feature importances\n#### Calculate feature importances for model\n\nRemoved to reduce complexity of code, seems to be best to keep all...","9d047612":"### Load example data","3a70a7f0":"### Function for removing features that are highly correlated\nRemove variables with high Pearson-coefficients (absolute value).\n\nApproach: Find the pair of variables with highest absolute correlation, of these two, remove the one with highest mean absolute correlation to other variables.","bfda29da":"# Find collinearities and remove highly correlated variables\n\nRemoved to reduce computation time\/complexity of code.\n\nRemoving correlated variables seemed to result in worse results.\n\n## Function definitions\n### Calculate VIF\nRegresses one variable against the others. Variance inflation factor (VIF) is\n\n$VIF = 1\/(1-r^2)$\n\nwhere $r^2$ is R-squared value from linear regression.","cad9a364":"## Calculate predictions","99341d67":"## Set path to data","55f64771":"### Extract features, groupid's (time indices), and create train\/valid set for model training","a00771c8":"### Extract independent\/dependent variables for calculating correlations","88044f15":"## Define aggregates functions for trade","5f11978f":"## Aggregate group statistics over time and stock id's","f35a8ae3":"## Pandas helper functions\n\nThis function convert a pandas multi-index (created when applying different functions to columns with 'agg') to a list of column-names.","d3105983":"## Check % RAM used","fc15e4ea":"## Save result to file","82aa07c4":"## Read example timeseries","4966dd62":"## Downcast variables to save memory\n\nDoes it increase % of available RAM?","718ca488":"## Define aggregates functions for book","bc6f105a":"## Train model ensemble\n\nNotes:\n\n* baseline (folds=3, Na's replaced with zero)\n\nOut-of-folds RMSPE is 0.22493430359860675\n\nError on valid set is: 0.22943458641036366\n\n* Na's kept (folds=5)\n\nOut-of-folds RMSPE is 0.2259242998405427\n\nError on valid set is: 0.23030996419701785\n\n* add gradient features (folds=5)\n\nOut-of-folds RMSPE is 0.2252442009993441\n\nError on valid set is: 0.2292844069587652\n\n* add gradient features+ vw\/tw-features (folds=5)\n\nOut-of-folds RMSPE is 0.22541343505501404\nError on valid set is: 0.2289696589191299","85c4194d":"### Catboost Ensemble","68db3baf":"# Setup","b5c61b3d":"### Handle missing values\n\nAssume zero is a reasonable value for all na's. Na here (always? check this) means no data\/trades. Then order size can be set to 0 since no orders, sum of price=0 since to trades, standard deviation of price = 0 since no data points\/deviations...\n\nNote:\n\nlightgbm able to accept data with Na, will treat them as separate value. Could be better than setting Na's to zero.","21ee693d":"### LinearRegression ensemble","d1363e20":"### Model stack","5a3f5931":"## Calculate different statistics","5b8d766c":"## train_valid_groupsplit\n","82a61af4":"# Predict on test data and submit\n## Load test data","8028bab5":"## Remove variables with high internal correlations","903be4d2":"# New train\/valid split, keep selected columns","c53094da":"## Save result from preprocessing\nCommented out for now, saved files will be deleted when notebook is reset on kaggle.","72cffa00":"# Calculate statistics (pre-processing)\n## Find all stock id's in data","20fadd72":"## Calculate root-mean-square-percentage error\n\n$rmspe = \\sqrt{\\frac{1}{n} \\Sigma_{data} \\left( \\frac{y_{true} - y_{pred}}{y_{true}} \\right)^2}$\n\n$rmspe = \\sqrt{ \\frac{1}{n} \\Sigma_{data} \\left(1 - \\frac{y_{pred}}{y_{true}} \\right)^2}$","a78f35ec":"## Calculate statistics for book\/trade, training\/test set","e409e1a2":"### Gradient","02e2e494":"### Weighted averages","c1517372":"# Define functions","caa570d6":"## Collect, and parallelize all computations for the pre-processing","2679c7d4":"## Clustering","66d7c40f":"### Fourier amplitudes","7e383fa2":"## Aggregate values with same time id's"}}