{"cell_type":{"ea26f652":"code","6440d696":"code","b4fa22d4":"code","7cbe1a26":"code","f25ff63d":"code","9d1665b8":"code","721271cc":"code","078cec5a":"code","6632929d":"code","8e0426ae":"code","710c97ce":"code","1ca6380a":"code","f986fbc7":"code","1aa0409b":"code","ab664bc7":"code","9a410f58":"code","8515121d":"code","96af51df":"code","38cec640":"code","ff16d2b6":"code","c9b866a9":"code","3e2ddf75":"code","e1ef5d47":"code","04eefc78":"code","11429d48":"code","7ae5ec8a":"code","6bc12016":"code","5b62bf82":"code","6daa9875":"code","0d6519ad":"code","3225e408":"code","99c4ad4d":"code","cfc224bd":"markdown","a331c5df":"markdown","0fb4f851":"markdown","f2a9be5e":"markdown","a8f3c271":"markdown","ce0e208f":"markdown","e8ecd9ed":"markdown","0861b1ec":"markdown","d1a57e31":"markdown","abc6a5db":"markdown","c5a5d34e":"markdown","017c24d0":"markdown","e9cad21a":"markdown","48123110":"markdown","60364c75":"markdown","984ce4eb":"markdown","570ab626":"markdown"},"source":{"ea26f652":"import tensorflow as tf\n# detect and init the TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    tpu_strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", tpu_strategy.num_replicas_in_sync)","6440d696":"from pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tensorflow import keras \nfrom tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation \nfrom tensorflow.keras.layers import AveragePooling2D, Input, Flatten \nfrom tensorflow.keras.optimizers import Adam \nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler \nfrom tensorflow.keras.callbacks import ReduceLROnPlateau \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator \nfrom tensorflow.keras.regularizers import l2 \nfrom tensorflow.keras import backend as K \nfrom tensorflow.keras.models import Model \n\n%matplotlib inline","b4fa22d4":"DATA_PATH = Path(\"..\/input\/magnet-nasa\")\n\ndst = pd.read_csv(DATA_PATH \/ \"dst_labels.csv\")\ndst.timedelta = pd.to_timedelta(dst.timedelta)\ndst.set_index([\"period\", \"timedelta\"], inplace=True)\n\nsunspots = pd.read_csv(DATA_PATH \/ \"sunspots.csv\")\nsunspots.timedelta = pd.to_timedelta(sunspots.timedelta)\nsunspots.set_index([\"period\", \"timedelta\"], inplace=True)\n\nsolar_wind = pd.read_csv(DATA_PATH \/ \"solar_wind.csv\")\nsolar_wind.timedelta = pd.to_timedelta(solar_wind.timedelta)\nsolar_wind.set_index([\"period\", \"timedelta\"], inplace=True)","7cbe1a26":"print(\"Dst shape: \", dst.shape)\ndst.head()","f25ff63d":"dst.groupby(\"period\").describe()","9d1665b8":"print(\"Solar wind shape: \", solar_wind.shape)\nsolar_wind.head()","721271cc":"print(\"Sunspot shape: \", sunspots.shape)\nsunspots.head()","078cec5a":"solar_wind.groupby(\"period\").describe().T","6632929d":"sunspots.groupby(\"period\").describe().T","8e0426ae":"def show_raw_visualization(data):\n    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15), dpi=80)\n    for i, key in enumerate(data.columns):\n        t_data = data[key]\n        ax = t_data.plot(\n            ax=axes[i \/\/ 2, i % 2],\n            title=f\"{key.capitalize()}\",\n            rot=25,\n        )\n\n    fig.subplots_adjust(hspace=0.8)\n    plt.tight_layout()\n\n\ncols_to_plot = [\"bx_gse\", \"bx_gsm\", \"bt\", \"density\", \"speed\", \"temperature\"]\nshow_raw_visualization(solar_wind[cols_to_plot].iloc[:1000])","710c97ce":"solar_wind.isna().sum()","1ca6380a":"corr = solar_wind.join(sunspots).join(dst).fillna(method=\"ffill\").corr()\n\nplt.figure(figsize=(10, 5))\nplt.matshow(corr, fignum=1)\nplt.xticks(range(corr.shape[1]), corr.columns, fontsize=14, rotation=90)\nplt.gca().xaxis.tick_bottom()\nplt.yticks(range(corr.shape[1]), corr.columns, fontsize=14)\n\n\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title(\"Feature Correlation Heatmap\", fontsize=16)\nplt.show()","f986fbc7":"from numpy.random import seed\nfrom tensorflow.random import set_seed\n\nseed(2020)\nset_seed(2021)","1aa0409b":"from sklearn.preprocessing import StandardScaler,MinMaxScaler\n\n# subset of solar wind features to use for modeling\nSOLAR_WIND_FEATURES = [\n    \"bt\",\n    \"temperature\",\n    \"bx_gse\",\n    \"by_gse\",\n    \"bz_gse\",\n    \"speed\",\n    \"density\",\n]\n\n# all of the features we'll use, including sunspot numbers\nXCOLS = (\n    [col + \"_mean\" for col in SOLAR_WIND_FEATURES]\n    + [col + \"_std\" for col in SOLAR_WIND_FEATURES]\n    + [\"smoothed_ssn\"]\n)\n\n\ndef impute_features(feature_df):\n    \"\"\"Imputes data using the following methods:\n    - `smoothed_ssn`: forward fill\n    - `solar_wind`: interpolation\n    \"\"\"\n    # forward fill sunspot data for the rest of the month\n    feature_df.smoothed_ssn = feature_df.smoothed_ssn.fillna(method=\"ffill\")\n    # interpolate between missing solar wind values\n    feature_df = feature_df.interpolate()\n    return feature_df\n\n\ndef aggregate_hourly(feature_df, aggs=[\"mean\", \"std\"]):\n    \"\"\"Aggregates features to the floor of each hour using mean and standard deviation.\n    e.g. All values from \"11:00:00\" to \"11:59:00\" will be aggregated to \"11:00:00\".\n    \"\"\"\n    # group by the floor of each hour use timedelta index\n    agged = feature_df.groupby(\n        [\"period\", feature_df.index.get_level_values(1).floor(\"H\")]\n    ).agg(aggs)\n    # flatten hierachical column index\n    agged.columns = [\"_\".join(x) for x in agged.columns]\n    return agged\n\n\ndef preprocess_features(solar_wind, sunspots, scaler=None, subset=None):\n    \"\"\"\n    Preprocessing steps:\n        - Subset the data\n        - Aggregate hourly\n        - Join solar wind and sunspot data\n        - Scale using standard scaler\n        - Impute missing values\n    \"\"\"\n    # select features we want to use\n    if subset:\n        solar_wind = solar_wind[subset]\n\n    # aggregate solar wind data and join with sunspots\n    hourly_features = aggregate_hourly(solar_wind).join(sunspots)\n\n    # subtract mean and divide by standard deviation\n    if scaler is None:\n        scaler = StandardScaler()\n        scaler.fit(hourly_features)\n\n    normalized = pd.DataFrame(\n        scaler.transform(hourly_features),\n        index=hourly_features.index,\n        columns=hourly_features.columns,\n    )\n\n    # impute missing values\n    imputed = impute_features(normalized)\n\n    # we want to return the scaler object as well to use later during prediction\n    return imputed, scaler","ab664bc7":"features, scaler = preprocess_features(solar_wind, sunspots, subset=SOLAR_WIND_FEATURES)\nprint(features.shape)\nfeatures.head()","9a410f58":"YCOLS = [\"t0\", \"t1\"]\n\n\ndef process_labels(dst):\n    y = dst.copy()\n    y[\"t1\"] = y.groupby(\"period\").dst.shift(-1)\n    y.columns = YCOLS\n    return y\n\n\nlabels = process_labels(dst)\nlabels.head()","8515121d":"data = labels.join(features)\ndata.head()","96af51df":"def get_train_test_val(data, test_per_period, val_per_period):\n    \"\"\"Splits data across periods into train, test, and validation\"\"\"\n    # assign the last `test_per_period` rows from each period to test\n    test = data.groupby(\"period\").tail(test_per_period)\n    interim = data[~data.index.isin(test.index)]\n    # assign the last `val_per_period` from the remaining rows to validation\n    val = interim.groupby(\"period\").tail(val_per_period)\n    # the remaining rows are assigned to train\n    train = interim[~interim.index.isin(val.index)]\n    return train, test, val\n\n\n","38cec640":"train, test, val = get_train_test_val(data, test_per_period=6_000, val_per_period=3_000)","ff16d2b6":"ind = [0, 1, 2]\nnames = [\"train_a\", \"train_b\", \"train_c\"]\nwidth = 0.75\ntrain_cnts = [len(df) for _, df in train.groupby(\"period\")]\nval_cnts = [len(df) for _, df in val.groupby(\"period\")]\ntest_cnts = [len(df) for _, df in test.groupby(\"period\")]\n\np1 = plt.barh(ind, train_cnts, width)\np2 = plt.barh(ind, val_cnts, width, left=train_cnts)\np3 = plt.barh(ind, test_cnts, width, left=np.add(val_cnts, train_cnts).tolist())\n\nplt.yticks(ind, names)\nplt.ylabel(\"Period\")\nplt.xlabel(\"Hourly Timesteps\")\nplt.title(\"Train\/Validation\/Test Splits over Periods\", fontsize=16)\nplt.legend([\"Train\", \"Validation\", \"Test\"])","c9b866a9":"print(train.shape)\ntrain.head()","3e2ddf75":"print(test.shape)\ntest.head()","e1ef5d47":"print(val.shape)\nval.head()","04eefc78":"from keras import preprocessing\n\n\ndata_config = {\n    \"timesteps\": 34,\n    \"batch_size\": 34,\n}\n\n\ndef timeseries_dataset_from_df(df, batch_size):\n    dataset = None\n    timesteps = data_config[\"timesteps\"]\n\n    # iterate through periods\n    for _, period_df in df.groupby(\"period\"):\n        # realign features and labels so that first sequence of 32 is aligned with the 33rd target\n        inputs = period_df[XCOLS][:-timesteps]\n        outputs = period_df[YCOLS][timesteps:]\n\n        period_ds = preprocessing.timeseries_dataset_from_array(\n            inputs,\n            outputs,\n            timesteps,\n            batch_size=batch_size,\n        )\n\n        if dataset is None:\n            dataset = period_ds\n        else:\n            dataset = dataset.concatenate(period_ds)\n\n    return dataset\n\n\ntrain_ds = timeseries_dataset_from_df(train, data_config[\"batch_size\"])\nval_ds = timeseries_dataset_from_df(val, data_config[\"batch_size\"])\n\nprint(f\"Number of train batches: {len(train_ds)}\")\nprint(f\"Number of val batches: {len(val_ds)}\")","11429d48":"from tensorflow.keras.layers import Dense, LSTM,Bidirectional,Dropout,Embedding\nfrom tensorflow.keras.models import Sequential\nmodel_config = {\"n_epochs\": 15, \"n_neurons\": 256, \"dropout\": 0.1, \"stateful\": False}\n# define our model\ndef creat_model():\n    model = Sequential()\n    model.add(Input(shape=(data_config[\"timesteps\"],\n                                 len(XCOLS))))\n    model.add(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(256,dropout=0.3), return_sequences=True,\n                                         name=\"lstm1\"))\n#     model.add(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(256), return_sequences=True,dropout=0.1,\n#                                          name=\"lstm1.5\"))\n    model.add(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(256), name=\"lstm2\",))\n    model.add(tf.keras.layers.Dense(units=2))\n\n    return model\n\nwith tpu_strategy.scope():\n        model=creat_model()\n        model.compile(\n        loss=tf.keras.losses.MeanSquaredError(),\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n         )\n\nmodel.summary()","7ae5ec8a":"history = model.fit(\n        train_ds,\n        batch_size=data_config[\"batch_size\"],\n        epochs=model_config[\"n_epochs\"],\n        verbose=1,\n        shuffle=False,\n        validation_data=val_ds,\n    )\n\n","6bc12016":"for name, values in history.history.items():\n    plt.plot(values)","5b62bf82":"test_ds = timeseries_dataset_from_df(test, data_config[\"batch_size\"])","6daa9875":"mse = model.evaluate(test_ds)\nprint(f\"Test RMSE: {mse**.5:.2f}\")","0d6519ad":"import json\nimport pickle\nimport h5py \nsave_model=model.save(\"model.h5\")\n\nwith open(\"scaler.pck\", \"wb\") as f:\n    pickle.dump(scaler, f)\n\ndata_config[\"solar_wind_subset\"] = SOLAR_WIND_FEATURES\nprint(data_config)\nwith open(\"config.json\", \"w\") as f:\n    json.dump(data_config, f)","3225e408":"model = keras.models.load_model(\"model.h5\")","99c4ad4d":"%%writefile predict.py\n\nimport json\nimport pickle\nfrom typing import Tuple\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\n\n# Load in serialized model, config, and scaler\nmodel = keras.models.load_model(\"model.h5\")\n\nwith open(\"config.json\", \"r\") as f:\n    CONFIG = json.load(f)\n\nwith open(\"scaler.pck\", \"rb\") as f:\n    scaler = pickle.load(f)\n\n# Set global variables    \nTIMESTEPS = CONFIG[\"timesteps\"]\nSOLAR_WIND_FEATURES = [\n    \"bt\",\n    \"temperature\",\n    \"bx_gse\",\n    \"by_gse\",\n    \"bz_gse\",\n    \"speed\",\n    \"density\",\n]\nXCOLS = (\n    [col + \"_mean\" for col in SOLAR_WIND_FEATURES]\n    + [col + \"_std\" for col in SOLAR_WIND_FEATURES]\n    + [\"smoothed_ssn\"]\n)\n\n\n# Define functions for preprocessing\ndef impute_features(feature_df):\n    \"\"\"Imputes data using the following methods:\n    - `smoothed_ssn`: forward fill\n    - `solar_wind`: interpolation\n    \"\"\"\n    # forward fill sunspot data for the rest of the month\n    feature_df.smoothed_ssn = feature_df.smoothed_ssn.fillna(method=\"ffill\")\n    # interpolate between missing solar wind values\n    feature_df = feature_df.interpolate()\n    return feature_df\n\n\ndef aggregate_hourly(feature_df, aggs=[\"mean\", \"std\"]):\n    \"\"\"Aggregates features to the floor of each hour using mean and standard deviation.\n    e.g. All values from \"11:00:00\" to \"11:59:00\" will be aggregated to \"11:00:00\".\n    \"\"\"\n    # group by the floor of each hour use timedelta index\n    agged = feature_df.groupby([feature_df.index.floor(\"H\")]).agg(aggs)\n\n    # flatten hierachical column index\n    agged.columns = [\"_\".join(x) for x in agged.columns]\n    return agged\n\n\ndef preprocess_features(solar_wind, sunspots, scaler=None, subset=None):\n    \"\"\"\n    Preprocessing steps:\n        - Subset the data\n        - Aggregate hourly\n        - Join solar wind and sunspot data\n        - Scale using standard scaler\n        - Impute missing values\n    \"\"\"\n    # select features we want to use\n    if subset:\n        solar_wind = solar_wind[subset]\n\n    # aggregate solar wind data and join with sunspots\n    hourly_features = aggregate_hourly(solar_wind).join(sunspots)\n\n    # subtract mean and divide by standard deviation\n    if scaler is None:\n        scaler = StandardScaler()\n        scaler.fit(hourly_features)\n\n    normalized = pd.DataFrame(\n        scaler.transform(hourly_features),\n        index=hourly_features.index,\n        columns=hourly_features.columns,\n    )\n\n    # impute missing values\n    imputed = impute_features(normalized)\n\n    # we want to return the scaler object as well to use later during prediction\n    return imputed, scaler\n\n\n# THIS MUST BE DEFINED FOR YOUR SUBMISSION TO RUN\ndef predict_dst(\n    solar_wind_7d: pd.DataFrame,\n    satellite_positions_7d: pd.DataFrame,\n    latest_sunspot_number: float,\n) -> Tuple[float, float]:\n    \"\"\"\n    Take all of the data up until time t-1, and then make predictions for\n    times t and t+1.\n    Parameters\n    ----------\n    solar_wind_7d: pd.DataFrame\n        The last 7 days of satellite data up until (t - 1) minutes [exclusive of t]\n    satellite_positions_7d: pd.DataFrame\n        The last 7 days of satellite position data up until the present time [inclusive of t]\n    latest_sunspot_number: float\n        The latest monthly sunspot number (SSN) to be available\n    Returns\n    -------\n    predictions : Tuple[float, float]\n        A tuple of two predictions, for (t and t + 1 hour) respectively; these should\n        be between -2,000 and 500.\n    \"\"\"\n    # Re-format data to fit into our pipeline\n    sunspots = pd.DataFrame(index=solar_wind_7d.index, columns=[\"smoothed_ssn\"])\n    sunspots[\"smoothed_ssn\"] = latest_sunspot_number\n    \n    # Process our features and grab last 32 (timesteps) hours\n    features, s = preprocess_features(\n        solar_wind_7d, sunspots, scaler=scaler, subset=SOLAR_WIND_FEATURES\n    )\n    model_input = features[-TIMESTEPS:][XCOLS].values.reshape(\n        (1,TIMESTEPS, features.shape[1])\n    )\n    \n    # Make a prediction\n    prediction_at_t0, prediction_at_t1 = model.predict(model_input)[0]\n\n    # Optional check for unexpected values\n    if not np.isfinite(prediction_at_t0):\n        prediction_at_t0 = -12\n    if not np.isfinite(prediction_at_t1):\n        prediction_at_t1 = -12\n\n    return prediction_at_t0, prediction_at_t1","cfc224bd":"# Problem description\nThe goal of this challenge is to develop models for forecasting Dst (Disturbance Storm-Time Index) that 1) push the boundary of predictive performance 2) under operationally viable constraints 3) using specified real-time solar-wind data feeds. More information on the dataset, performance metric, and submission specifications is provided below.\n\nFinalists will be determined by performance on the private test set. These participants will then have the opportunity to submit their code to be audited using an out-of-sample verification set. The top 4 eligible teams that pass this final check will be awarded prizes.\n\n**You can find more information on** [DataDriven](https:\/\/www.drivendata.org\/competitions\/73\/noaa-magnetic-forecasting\/page\/279\/)","a331c5df":"## Evaluation","0fb4f851":"## Importing Dataset","f2a9be5e":"## Training Model","a8f3c271":"## Batch size and Time Steps","ce0e208f":"# Rule to get top 50\n\n1) **Persistence**\n![image.png](attachment:image.png)\n\nThe code submission was quite different and it took me one week to get hold of it, even in a later stage when you try to use bidirectional LSTM you will get the error, you need to note down what works for cloud computing what doesn't. I have gained better scores with multiple layers of the bidirectional model but they eventually failed in the submission area.\n\n2) **Look for a new model especially Medium Notebooks**\n\nIt took me 2 weeks just to implement these new experimental designs on these data sets, but it was worth it as I gain more knowledge on what works what doesn't. \n\n3) **Take a Break and reset your mindset**\n\nIf you keep on working on one thing, you will get frustrated and things will get hard. Try taking breaks.\n\n4) **It's better to find a dedicated teammate.**\n\nWe get better when we share our idea and try to implement them together.\n\n5) **If you are new to DataScience try to at least participate in the random competition.**\n\nParticipating in multiple competitions has helped me better understand models and real problems that can be solved with the models. I was amazed at how pre-trained models were better in performance than simple NN.\n","e8ecd9ed":"# If you Like my Work Do Upvote \ud83d\udc46 or Comment ","0861b1ec":"# Code","d1a57e31":"## Reqired file for Competition","abc6a5db":"# Task\nIn this challenge, your task is to develop models for forecasting Dst that push the boundary of predictive performance, under operationally viable constraints, using the real-time solar-wind (RTSW) data feeds from NOAA\u2019s DSCOVR and NASA\u2019s ACE satellites. Improved models can provide more advanced warning of geomagnetic storms and reduce errors in magnetic navigation systems.\n\nHead on over to the [Problem Description](https:\/\/www.drivendata.org\/competitions\/73\/noaa-magnetic-forecasting\/page\/279\/) to get started!","c5a5d34e":"## Using StandardScaler","017c24d0":"## Test, Train, Val","e9cad21a":"# Result\n\nI scored 40th rank in this DrivenData competition and my Jounery started with simple LSTM. You can look at all the versions. I will be relising GPU and CPU version of this notebook.\n![image.png](attachment:image.png)","48123110":"**This notebook was created by the help of** [LSTM Benchmark](https:\/\/www.drivendata.co\/blog\/model-geomagnetic-field-benchmark\/)","60364c75":"# Overview\n\n![Magnet](https:\/\/drivendata-public-assets.s3.amazonaws.com\/noaa-cover-img.png)\nHelp NOAA better forecast changes in Earth\u2019s magnetic field!\n\nThe efficient transfer of energy from solar wind into the Earth\u2019s magnetic field causes geomagnetic storms. The resulting variations in the magnetic field increase errors in magnetic navigation. The disturbance-storm-time index, or Dst, is a measure of the severity of the geomagnetic storm.\n\nAs a key specification of the magnetospheric dynamics, the Dst index is used to drive geomagnetic disturbance models such as NOAA\/NCEI\u2019s High Definition Geomagnetic Model - Real-Time (HDGM-RT). Additionally, magnetic surveyors, government agencies, academic institutions, satellite operators, and power grid operators use the Dst index to analyze the strength and duration of geomagnetic storms.\n\nEmpirical models have been proposed as early as in 1975 to forecast Dst solely from solar-wind observations at the Lagrangian (L1) position by satellites such as NOAA\u2019s Deep Space Climate Observatory (DSCOVR) or NASA's Advanced Composition Explorer (ACE). Over the past three decades, several models were proposed for solar wind forecasting of Dst, including empirical, physics-based, and machine learning approaches. While the ML models generally perform better than models based on the other approaches, there is still room to improve, especially when predicting extreme events. More importantly, we seek solutions that work on the raw, real-time data streams and are agnostic to sensor malfunctions and noise.","984ce4eb":"## Using TPU","570ab626":"## Saving Model and Params"}}