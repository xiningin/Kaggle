{"cell_type":{"6bbed636":"code","5611aadd":"code","a4c7dbaa":"code","2b7b705f":"code","d71abcd2":"code","69031173":"code","92cd93fc":"code","5031eb20":"code","17faf7df":"code","76ab77da":"code","60d5e22c":"code","8f8351e6":"code","a8abb994":"code","99ef4023":"code","d988e303":"code","3fdc09d5":"code","4d905ec8":"code","f2a8f9ac":"markdown","46983f1f":"markdown","c1d7871b":"markdown","322f700a":"markdown","9278c392":"markdown","6c953018":"markdown","a68b3811":"markdown"},"source":{"6bbed636":"# Loading all packages \nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport tensorflow_hub as hub\nimport tensorflow_text\nimport glob\nimport matplotlib.pyplot as plt\nimport scipy as sc\nimport warnings\nimport faiss  \nimport requests\nimport pickle\nfrom sklearn.metrics.pairwise import cosine_similarity\nplt.style.use('ggplot')\nimport re\n\nimport dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nimport pandas as pd\nfrom dash.dependencies import Input, Output, State\nfrom flask import Flask\nimport os\nimport requests\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport dash_bootstrap_components as dbc","5611aadd":"## Helper Functions\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        if 'abstract' in file:\n            features = [\n                file['paper_id'],\n                file['metadata']['title'],\n                format_authors(file['metadata']['authors']),\n                format_authors(file['metadata']['authors'], \n                               with_affiliation=True),\n                format_body(file['abstract']),\n                format_body(file['body_text']),\n                format_bib(file['bib_entries']),\n                file['metadata']['authors'],\n                file['bib_entries']\n            ]\n        else:\n            features = [\n                file['paper_id'],\n                file['metadata']['title'],\n                format_authors(file['metadata']['authors']),\n                format_authors(file['metadata']['authors'], \n                               with_affiliation=True),\n                format_body(file['body_text']),\n                format_body(file['body_text']),\n                format_bib(file['bib_entries']),\n                file['metadata']['authors'],\n                file['bib_entries']\n            \n            ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df\n\n","a4c7dbaa":"biorxiv_dir = '\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames))\n\nall_files = []\n\nfor filename in filenames:\n    filename = biorxiv_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files.append(file)\n","2b7b705f":"### Biorxiv: Generate CSV\n\ncleaned_files = []\n\nfor file in tqdm(all_files):\n    features = [\n        file['paper_id'],\n        file['metadata']['title'],\n        format_authors(file['metadata']['authors']),\n        format_authors(file['metadata']['authors'], \n                       with_affiliation=True),\n        format_body(file['abstract']),\n        format_body(file['body_text']),\n        format_bib(file['bib_entries']),\n        file['metadata']['authors'],\n        file['bib_entries']\n    ]\n    \n    cleaned_files.append(features)\n\ncol_names = [\n    'paper_id', \n    'title', \n    'authors',\n    'affiliations', \n    'abstract', \n    'text', \n    'bibliography',\n    'raw_authors',\n    'raw_bibliography'\n]\n\nclean_df = pd.DataFrame(cleaned_files, columns=col_names)\nclean_df.head()","d71abcd2":"#Reading all CSV files and Concatenating final result\npmc_dir = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/'\npmc_files = load_files(pmc_dir)\npmc_df = generate_clean_df(pmc_files)\n# pmc_df.to_csv('clean_pmc.csv', index=False)\n\n\npmc_dir_1 = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pmc_json\/'\npmc_files_1 = load_files(pmc_dir_1)\n\npmc_df_1 = generate_clean_df(pmc_files_1)\n# pmc_df.to_csv('clean_pmc.csv', index=False)\n\n\ncomm_dir = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pdf_json\/'\ncomm_files = load_files(comm_dir)\ncomm_df = generate_clean_df(comm_files)\n\n\n\ncomm_dir_1 = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pmc_json\/'\ncomm_files_1 = load_files(comm_dir_1)\ncomm_df_1 = generate_clean_df(comm_files_1)\n\n\n\nnoncomm_dir = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pdf_json\/'\nnoncomm_files = load_files(noncomm_dir)\nnoncomm_df = generate_clean_df(noncomm_files)\n\n\n\nnoncomm_dir_1 = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pmc_json\/'\nnoncomm_files_1 = load_files(noncomm_dir_1)\nnoncomm_df_1 = generate_clean_df(noncomm_files_1)\n\n\n\ndf_covid_new = pd.concat([clean_df,pmc_df,pmc_df_1,comm_df,comm_df_1,noncomm_df,noncomm_df_1],axis=0,ignore_index=True)\n","69031173":"#Reading Metadata file\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\n\ndf_covid_old = pd.merge(df_covid_new,meta_df[['sha','url']],left_on='paper_id',right_on='sha',how='left')\n\n## Saving the Doc information with their URLs.\ndf_covid_new[['paper_id','title','url']].to_csv('\/kaggle\/output\/df_docid_with_url.csv')\n","92cd93fc":"#Reading all CSV files and Concatenating final result\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\")","5031eb20":"## Creating a dictionary with Document as Key and Paragraphs as text\nshort_paragraph=[]\ndict1 = {}\nfor i in range(len(df_covid_old)):\n#     if dict1[df_covid.loc[i,'paper_id']] is not null:\n    dict1[df_covid_old.loc[i,'paper_id']] = re.split(r'(?:\\r?\\n){1,}', df_covid_old.loc[i,'text'])","17faf7df":"## Create Vector Embedding for all the Text Documents and storing in a dictionary with key as docId and values as paragraph embeddings\ndict_vector_old = {}\nfor key in list(dict1.keys()):\n    try:\n        dict_vector_old[key] = embed(dict1[key])\n        print(len(dict_vector_old))\n    except:\n        continue","76ab77da":"## Matching Vector and Text Documents (if embedding vector generation fails, we are ignoring the document)        \ndict1_old= {}\nfor key in list(dict_vector_old.keys()):\n    if key in dict1:\n        dict1_old[key]=dict1[key]\n    print(len(dict1_old))","60d5e22c":"## Storing the paragraph embeddings as pickle file for further use at \/kaggle\/output\/\nwith open('\/kaggle\/output\/dict1_text_v6.pickle', 'wb') as handle:\n    pickle.dump(dict1_old, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open('\/kaggle\/output\/dict_vector_v6.pickle', 'wb') as handle:\n    pickle.dump(dict_vector_old, handle, protocol=pickle.HIGHEST_PROTOCOL)","8f8351e6":"df_covid = df_covid_old\n\n## Reading the paragraph embeddings pickle files\nwith open('\/kaggle\/output\/dict1_text_v6.pickle', 'rb') as handle:\n    dict1_text_v1 = pickle.load(handle)\n\nwith open('\/kaggle\/output\/dict_vector_v6.pickle', 'rb') as handle:\n    dict_vector = pickle.load(handle)","a8abb994":"\n## Building the index for semantic search for faiss\nindex = faiss.IndexFlatL2(512)   # build the index\nfor vector in list(dict_vector.keys()):\n    index.add(dict_vector[vector].numpy())                  # add vectors to the index\n","99ef4023":"## Creating a list of documents with their docid and paragraph text to get the results\ntext=[]\ndocId=[]\nfor key in list(dict1_text_v1.keys()):\n\n    text.extend(dict1_text_v1[key])\n    doc=[key]*len(dict1_text_v1[key])\n    docId.extend(doc)\n    ","d988e303":"## Enter the required query to be searched upon\n## Below is the query to search \"Seasonality of transmission of corona virus\"\nsearch = [''' Seasonality of transmission of corona virus''']\n\n## Creating the embedding vectors for the query\npredictions = embed(search)\n\nk = 10                         # we want to see 10 nearest neighbors\nD, I = index.search(np.array(predictions,dtype='float32'), k) # sanity check\nprint(I)\nprint(D)\n\nfor i in range(k):\n    print(text[I[0][i]])\n    print(docId[I[0][i]])\n    print('\\n')\n    ","3fdc09d5":"# Supporting function for calculation of Text summarization using Extraction-based text summarization\nexternal_stylesheets=[dbc.themes.BOOTSTRAP]\n\nserver = Flask(__name__)\nserver.secret_key = os.environ.get('secret_key', 'secret')\napp = dash.Dash(name = __name__, server = server, external_stylesheets = external_stylesheets)\n#external_stylesheets = ['https:\/\/codepen.io\/chriddyp\/pen\/bWLwgP.css']\n\n#app.config['suppress_callback_exceptions'] = True\n\ndf = pd.read_csv('https:\/\/raw.githubusercontent.com\/rahulpoddar\/dash-deploy-exp\/master\/TASK1_annotated_1_v3.csv', encoding='latin1')\n\ntasks = df['Task Name'].unique().tolist()\n\ndef data_prep(inpt): \n    clean_data = []\n    article3 = ' '.join(inpt)\n    result=re.sub(\"\\d+\\.\", \" \", article3)\n    clean_data.append(result)\n            \n    clean_data = pd.DataFrame(clean_data)\n    clean_data.columns = ['Remediation']\n    clean_data['Remediation'] = clean_data['Remediation'].astype('str')\n\n    clean_data1 = clean_data['Remediation']\n    clean_data2 = []\n    regex = r\"(?<!\\d)[-,_;:()](?!\\d)\"\n    for i in range(1):\n        result2 = re.sub(regex,'',clean_data1.loc[i])\n        clean_data2.append(result2)\n    clean_data2 = pd.DataFrame(clean_data2)\n    clean_data2.columns = ['Remediation']\n    clean_data2['Remediation'] = clean_data2['Remediation'].astype('str')\n    \n    return (clean_data2)\n\ndef _create_dictionary_table(text_string) -> dict:\n   \n    # Removing stop words\n    stop_words = set(stopwords.words(\"english\"))\n        \n    words = word_tokenize(text_string)\n    \n    # Reducing words to their root form\n    stem = PorterStemmer()\n    \n    # Creating dictionary for the word frequency table\n    frequency_table = dict()\n    for wd in words:\n        wd = stem.stem(wd)\n        if wd in stop_words:\n            continue\n        if wd in frequency_table:\n            frequency_table[wd] += 1\n        else:\n            frequency_table[wd] = 1\n\n    return frequency_table\n\ndef _calculate_sentence_scores(sentences, frequency_table) -> dict:   \n\n    # Algorithm for scoring a sentence by its words\n    sentence_weight = dict()\n\n    for sentence in sentences:\n        sentence_wordcount = (len(word_tokenize(sentence)))\n        sentence_wordcount_without_stop_words = 0\n        for word_weight in frequency_table:\n            if word_weight in sentence.lower():\n                sentence_wordcount_without_stop_words += 1\n                if sentence[:7] in sentence_weight:\n                    sentence_weight[sentence[:7]] += frequency_table[word_weight]\n                else:\n                    sentence_weight[sentence[:7]] = frequency_table[word_weight]\n\n        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] \/(sentence_wordcount_without_stop_words)\n      \n    return sentence_weight\n\ndef _calculate_average_score(sentence_weight) -> int:\n   \n    # Calculating the average score for the sentences\n    sum_values = 0\n    for entry in sentence_weight:\n        sum_values += sentence_weight[entry]\n\n    # Getting sentence average value from source text\n    average_score = (sum_values \/ (len(sentence_weight)))\n\n    return average_score\n\ndef _get_article_summary(sentences, sentence_weight, threshold):\n    sentence_counter = 0\n    article_summary = ''\n\n    for sentence in sentences:\n        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n            article_summary += \" \" + sentence\n            sentence_counter += 1\n\n    return article_summary\n\ndef _run_article_summary(article):\n    \n    #creating a dictionary for the word frequency table\n    frequency_table = _create_dictionary_table(article)\n\n    #tokenizing the sentences\n    sentences = sent_tokenize(article)\n\n    #algorithm for scoring a sentence by its words\n    sentence_scores = _calculate_sentence_scores(sentences, frequency_table)\n\n    #getting the threshold\n    threshold = _calculate_average_score(sentence_scores)\n\n    #producing the summary\n    article_summary = _get_article_summary(sentences, sentence_scores, 1 * threshold)\n\n    return article_summary\n\ndef _output(inpt):\n    new = []\n    df = data_prep(inpt)\n    df_rem = df['Remediation']\n    #sentences = sent_tokenize(df_rem[0])\n    summary_results = _run_article_summary(df_rem[0])\n    new.append(summary_results)\n    return(new)\n'''\ndef generate_summary(task):\n    return 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'\n'''","4d905ec8":"def generate_table(dff):\n    rows = []\n    for i in range(len(dff)):\n        row = []\n        for col in ['Title', 'Output']:\n            value = dff.iloc[i][col]\n            url = dff.iloc[i]['URL']\n            if col == 'Title':\n                cell = html.Td(html.A(href=url, children = value))\n            else:\n                cell = html.Td(children = value)\n            row.append(cell)\n        rows.append(html.Tr(row))\n    return dbc.Table(\n        # Header\n        [html.Tr([html.Th(col,  style={'text-align':'center'}) for col in ['Title', 'Search Output']]) ] +\n        # Body\n        rows,\n        bordered=True,\n        dark=False,\n        hover=True,\n        responsive=True,\n        striped=True,\n    )\n\n\napp.layout = html.Div([\n        html.Div([\n        html.H1('COVID-19 Open Research Dataset Challenge (CORD-19)', style = {'margin-left': '10%', 'margin-top': '5%'}),\n        html.Hr(),\n        html.Div([\n        html.H3('Type a general query (e.g. \"What is Corona Virus?\"):'),\n        html.Br(),\n        dbc.Input(id = 'general-search', type = 'text', placeholder = 'Type a query', value = ''),\n        html.Br(),\n        dbc.Button(id='submit-button-state', n_clicks=0, children='Submit', color = \"primary\", className=\"mr-2\", style = {'margin-left': '46%'}),\n        ], style = {'width': '80%', 'margin': 'auto'}),\n        html.Hr(),\n        html.Div([html.H3('OR')],style = {'margin-left': '48%'}),\n        html.Hr(),\n        html.Div([\n        html.H3('Select a task:'),\n        dcc.Dropdown(\n        id='task-dropdown',\n        options=[\n            {'label': i, 'value': i} for i in tasks \n        ],\n        placeholder=\"Select a task\",\n    ),\n        html.Br(),\n        html.Div([\n                html.H3('Select a sub-task:'),\n                dcc.Dropdown(\n                        id='sub-task-dropdown',\n                        placeholder = \"Select a sub-task\",\n                        ),\n                ], id = 'sub-task'),\n                ], style = {'margin-left': '10%','margin-right': '10%'}),\n    ]),\n    html.Hr(),\n    html.Div([\n                html.H3('Response Summary'),\n                html.Div(id = 'task-summary'),\n                html.Div(id = 'query-summary')\n                        ], style = {'margin-left': '10%','margin-right': '10%'}),\n    html.Hr(),\n    html.Div([\n            html.H3('Search Results'),\n            html.Div(id = 'task-results'),\n            html.Div(id = 'query-results')\n            ], id = 'search-results-main', style = {'margin-left': '10%','margin-right': '10%'}),\n    html.Hr(),\n])\n\n@app.callback(\n    Output('sub-task-dropdown', 'options'),\n    [Input('task-dropdown', 'value')])\ndef set_subtask_options(selected_task):\n    if selected_task != None:\n        dff = df[df['Task Name'] == selected_task]\n        options = dff['Sub-tasks'].unique().tolist()\n        return [{'label': i, 'value': i} for i in options]\n    else:\n        return [{'label': i, 'value': i} for i in []]\n    \n@app.callback(\n    Output('sub-task-dropdown', 'value'),\n    [Input('sub-task-dropdown', 'options')])\ndef set_subtask_value(available_options):\n    if available_options != []:\n        return available_options[0]['value']\n    else:\n        return ''\n    \n@app.callback(\n    Output('task-summary', 'children'),\n    [Input('sub-task-dropdown', 'value')])\ndef update_taks_summary(value):\n    if value != '':\n        dff = df[df['Sub-tasks'] == value]\n        return _output(dff['Output'].tolist())[0]\n    else:\n        return ''\n\n\n@app.callback(\n    Output('task-results', 'children'),\n    [Input('sub-task-dropdown', 'value')])\ndef update_taks_results(value):\n    if value != '':\n        dff = df[df['Sub-tasks'] == value]\n        return generate_table(dff)\n    else:\n        return ''\n    \n@app.callback(\n        Output('query-results', 'children'),\n         [Input('submit-button-state', 'n_clicks')],\n         [State('general-search', 'value')]\n         )\ndef populate_search_results(n_clicks, value):\n    if value != '':\n        query = value\n        response = requests.post(\"https:\/\/nlp.biano-ai.com\/develop\/test\", json={\"texts\": [query]})\n        predictions = response.json()['predictions']\n        pred_df = pd.DataFrame(predictions[0])\n        pred_df.columns = ['Distance', 'Document ID', 'Output', 'Title', 'URL']\n        return generate_table(pred_df)\n    else:\n        return ''\n\n@app.callback(\n        Output('query-summary', 'children'),\n         [Input('submit-button-state', 'n_clicks')],\n         [State('general-search', 'value')]\n         )\ndef generate_search_summary(n_clicks, value):\n    if value != '':\n        query = value\n        response = requests.post(\"https:\/\/nlp.biano-ai.com\/develop\/test\", json={\"texts\": [query]})\n        predictions = response.json()['predictions']\n        pred_df = pd.DataFrame(predictions[0])\n        return _output(pred_df['text'].tolist())[0]\n    else:\n        return ''\n\nif __name__ == '__main__':\n    app.run_server(debug=True)","f2a8f9ac":"# 3. Text Summarization and Vizualization \n","46983f1f":"# METHODOLOGY :  <br>\n\n**1) CREATION OF EMBEDDINGS**\n\n         a. Parsed 59,000 documents (source: https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge)\n         b. Created embedding vectors for each paragraph (~10million), using universal sentence encoder, (https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/5)\n                  \n**2) SEMANTIC SIMILARITY MATCH**\n\n        a. Sub-task questions mined for answers using similarity match from \u201cfaiss\u201d library.\n        b. Reduce false positives by further refining search of articles in 2a which contains Covid-19 like terms (Coronavirus disease 2019, COVID-19, SARS-CoV-2, Severe acute respiratory syndrome coronavirus 2, 2019-nCoV, SARSr-CoV)\n        c. Reduce false positives by further validation from scientific scientists in Novartis based on context \n                  \n**3) TEXT SUMMARIZATION & VISUALIZATION**\n\n       a. Extraction-based text summarization performed on the results from 2c, in an automated way. \n       (source: https:\/\/blog.floydhub.com\/gentle-introduction-to-text-summarization-in-machine-learning\/)\n         i) Convert Paragraph to sentences and calculate word weightage.\n        ii) Calculate the average word weightage by dividing the sum of weightage by total number of words.\n       iii) Select the sentence with the highest average weight.\n       b. Create an API using dash in order to view the results and extend for more advanced search.**","c1d7871b":"OBJECTIVE\n# What do we know about vaccines and therapeutics?","322f700a":"# 1. Creating Embeddings","9278c392":"**SOLUTION SUMMARY**\n# Scalable, fast and efficient search developed and aided with domain experts. Created embedding from 60K journal articles, augmented with summarization on top relevant sections for task specific questions. \n\n# Scalable, to allow for more questions beyond tasks please use the intelligent search and summarization engine,https:\/\/dash-app-deploy.herokuapp.com\/","6c953018":"# RESULTS & OUTCOME: <br>\n**1) Summary:** \nUnfortunately, no drug or vaccine has yet been approved to treat coronavirus disease 2019 (COVID-19) . Favipiravir, ribavirin, remdesivir and galidesivir could be good candidates as potential antiviral agents for the treatment. 2 And many clinical trials on anti-HIV drugs, LPV\/r and experimental antiviral agent, remdesevir are in the development process in China (http:\/\/clinicaltrials.gov\/show\/NCT04261907, http:\/\/clinicaltrials.gov\/show\/NCT04255017). 2 There are reports that remdesevir and antimalarial agent, chloroquine effectively inhibited SARS-CoV-2 in vitro. 3 If these clinical studies are successful, they can provide us with more efficient treatment options and suggest better choices for COVID-19 treatment in high-risk groups (elderly patients or patients with underlying diseases). Results of rapid sequencing of 2019-nCoV, coupled with molecular modelling based on the genomes of related virus proteins, 1 have suggested a few compounds that are likely to be effective, including the anti-HIV lopinavir plus ritonavir combination. Small molecules, targeted at specific receptors present on immune cells (pattern recognition receptors (PRRs)), such as Toll-like receptors (TLRs), have the ability to trigger stronger immune responses (Demento et al., 2011) . The most studied molecules are agonists for TLRs, as for example CpG oligonucleotides (TLR 9), poly(I:C) (TLR 3) or imiquimod (TLR 7\/8), which have already been evaluated for their adjuvant properties in vaccines against malaria, hepatitis B, influenza, as well as in different therapeutic anticancer vaccines (Steinhagen et al., 2011) . it could be shown that Sinupret \u00ae produces clear antiviral effects when added to the tissue cultures directly after the infection. This represents a therapeutic treatment and it confirms early therapeutic effects. This screen identified approximately 50 putative antivirals that are currently in clinical development suggesting that a cure for SARS might be closer than once thought.\n\n \n\nAnimal models of SARS-CoV are important for the study of virus-host interactions. Cats, ferrets, and nonhuman primates have been experimentally infected with SARS-CoV. In addition, SARS-CoV-like viruses were isolated from palm civet cats and closely related rac-coon dogs, which are sold in markets in China (5) . All of these animal species are important for the in vivo study of SARS-CoV. However, a murine model is also necessary to evaluate antiviral agents, vaccines, and immune response. A wide range of animal species is susceptible to experimental infection with SARS-CoV, including rodents (mice and hamsters), carnivores (ferrets and cats), and nonhuman primates (cynomolgus and rhesus macaques, common marmosets, and African green monkeys) [3] [4] [5] [6] [7] [8] [9] [10] [11] . Adult mice infected with varying doses of SARS-CoV in the respiratory tract show no clinical signs of disease, although the virus replicates in respiratory tissues, peaking early after infection, with viral titres in the lungs reaching relatively high levels. The infection is accompanied by only mild infl ammatory changes of the respiratory tract. On the other hand, aged mice, as well as hamsters and ferrets, do show signs of clinical disease (weight loss and ruffl ed fur), albeit, in most cases, in the absence of the typical lung lesions seen in humans with SARS [4] [5] [6] .\n\n**2) NOVARTIS_COVID-19_SEARCH **: https:\/\/dash-app-deploy.herokuapp.com\/ <br>","a68b3811":"# 2. SEMANTIC SIMILARITY MATCH"}}