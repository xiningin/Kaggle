{"cell_type":{"3675449f":"code","5e14bee5":"code","9bb01f2c":"code","f82a607e":"code","438e6849":"code","0c02c0bd":"code","385232b0":"code","03d1e0af":"code","cded6392":"code","f0cbfcd6":"code","e569dc84":"code","65d70d55":"code","9218b010":"code","121a4102":"code","074515e6":"code","d6346f6d":"code","450b9425":"code","383c4252":"code","421ef7ea":"code","e1008c65":"code","e967cdcb":"code","e61a077b":"code","115b2ff5":"code","ebcb7a6d":"code","34064598":"code","1a5b4566":"code","29218376":"code","cec9e60d":"code","2137f341":"code","c463dfba":"code","ee4fbf81":"code","96a9f75d":"code","4af807dd":"code","befe8807":"code","e4f2034a":"code","f8c375fa":"code","213498bf":"code","d8892be0":"code","2978b8a7":"code","446ec203":"code","03b55761":"code","4f2dc51a":"code","3280eaf5":"code","d6b864fa":"code","22a7985c":"code","e17bd0a9":"code","dd24b205":"code","8652d156":"code","7fd195b3":"code","aeefba7d":"code","6ec4c71e":"markdown","f3d3ff6f":"markdown","e81d01ef":"markdown","1845e975":"markdown","79d6d71d":"markdown","de5aff79":"markdown"},"source":{"3675449f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e14bee5":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# %matplotlib inline will lead to static images of your plot embedded in the notebook\n\nfrom sklearn.ensemble import  RandomForestRegressor\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV","9bb01f2c":"df_train=pd.read_csv('..\/input\/bigmart-sales-data\/Train.csv')\ndf_train.sample(5)\n\n# I prefer sample() which gives random sample of data inspite of head() or tail()","f82a607e":"# Testing data \ndf_test=pd.read_csv('..\/input\/bigmart-sales-data\/Test.csv')\ndf_test.sample(5)","438e6849":"df_train.info()\n# checking for datatype and other info","0c02c0bd":"df_test.info()\n# checking for datatype and other info","385232b0":"df_train.isnull().sum()\n# checking for null values","03d1e0af":"df_test.isnull().sum()\n# checking for null values","cded6392":"# Total percentage of the missing values\nmissing_data = df_train.isnull().sum()\ntotal_percentage = (missing_data.sum()\/df_train.shape[0]) * 100\nprint(f'The total percentage of missing data is {round(total_percentage,2)}%')","f0cbfcd6":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent_total = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)*100\nmissing = pd.concat([total, percent_total], axis=1, keys=[\"Total\", \"Percentage\"])\nmissing_data = missing[missing['Total']>0]\nmissing_data","e569dc84":"# Plotting the percentage of missing values\nplt.figure(figsize=(5,5))\nsns.set(style=\"whitegrid\")\nsns.barplot(x=missing_data.index, y=missing_data['Percentage'], data = missing_data)\nplt.title('Percentage of missing data by feature')\nplt.xlabel('Features', fontsize=14)\nplt.ylabel('Percentage', fontsize=14)\nplt.show()","65d70d55":"## as Item_Weight is related to Item_Type thats why filling null values with mean specific to its item_type\nCategory_mean = df_train.groupby('Item_Type')['Item_Weight'].mean()\nfor i in range(len(Category_mean)):\n#     print(s.index[i], Category_mean[i])\n    c1 = (df_train['Item_Type']==Category_mean.index[i])&(df_train['Item_Weight'].isna()==True)\n    df_train['Item_Weight'] = np.select([c1], [Category_mean[i]], df_train['Item_Weight'] )\n    ","9218b010":"## as Item_Weight is related to Item_Type thats why filling null values with mean specific to its item_type\nCategory_mean_test = df_test.groupby('Item_Type')['Item_Weight'].mean()\nfor i in range(len(Category_mean_test)):\n#     print(s.index[i], Category_mean[i])\n    c1 = (df_test['Item_Type']==Category_mean_test.index[i])&(df_test['Item_Weight'].isna()==True)\n    df_test['Item_Weight'] = np.select([c1], [Category_mean_test[i]], df_test['Item_Weight'] )\n    ","121a4102":"# Treatment of null values in Outlet_Size column for training data\nfrom statistics import mode\ndf_train['Outlet_Size'].fillna(mode(df_train['Outlet_Size']),inplace=True)","074515e6":"# Treatment of null values in Outlet_Size column for testing data\nfrom statistics import mode\ndf_test['Outlet_Size'].fillna(mode(df_test['Outlet_Size']),inplace=True)","d6346f6d":"## croschecking for null value treatment\n\nprint('Null values in df_train[Item_Weight] : ',  df_train['Item_Weight'].isna().sum())\nprint('Null values in df_train[Outlet_Size] : ',  df_train['Outlet_Size'].isna().sum())","450b9425":"## croschecking for null value treatment\n\nprint('Null values in df_test[Item_Weight] : ',  df_test['Item_Weight'].isna().sum())\nprint('Null values in df_test[Outlet_Size] : ',  df_test['Outlet_Size'].isna().sum())","383c4252":"fig,ax=plt.subplots(2,3,figsize=(35,8))\nsns.countplot(df_train['Item_Fat_Content'],ax=ax[0,0])\nsns.countplot(df_train['Item_Type'],ax=ax[0,1])\nsns.countplot(df_train['Outlet_Size'],ax=ax[0,2])\nsns.countplot(df_train['Outlet_Location_Type'].value_counts(),ax=ax[1,0])\nsns.countplot(df_train['Outlet_Type'],ax=ax[1,1])","421ef7ea":"fig,ax=plt.subplots(2,3,figsize=(35,8))\nsns.distplot(df_train['Item_Weight'],ax=ax[0,0])\nsns.distplot(df_train['Item_Visibility'],ax=ax[0,1])\nsns.distplot(df_train['Item_MRP'],ax=ax[0,2])\nsns.distplot(df_train['Outlet_Establishment_Year'],ax=ax[1,0])\nsns.distplot(df_train['Item_Outlet_Sales'],ax=ax[1,1])","e1008c65":"plt.figure(figsize=(12,7))\nplt.xlabel(\"Item_Weight\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Item_Weight and Item_Outlet_Sales Analysis\")\nplt.plot(df_train.Item_Weight, df_train[\"Item_Outlet_Sales\"],'.', alpha = 0.3)","e967cdcb":"plt.figure(figsize=(12,7))\nplt.xlabel(\"Item_Visibility\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Item_Visibility and Item_Outlet_Sales Analysis\")\nplt.plot(df_train.Item_Visibility, df_train[\"Item_Outlet_Sales\"],'.', alpha = 0.3)","e61a077b":"plt.figure(figsize=(12,7))\nplt.xlabel(\"Item_MRP\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Item_MRP and Item_Outlet_Sales Analysis\")\nplt.plot(df_train.Item_MRP, df_train[\"Item_Outlet_Sales\"],'.', alpha = 0.3)","115b2ff5":"Outlet_Establishment_Year_pivot = \\\ndf_train.pivot_table(index='Outlet_Establishment_Year', values=\"Item_Outlet_Sales\", aggfunc=np.median)\n\nOutlet_Establishment_Year_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Outlet_Establishment_Year\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Impact of Outlet_Establishment_Year on Item_Outlet_Sales\")\nplt.xticks(rotation=0)\nplt.show()","ebcb7a6d":"import plotly.express as px\nfig=px.sunburst(df_train,path=['Outlet_Type','Item_Type'],values='Item_Outlet_Sales')","34064598":"px.sunburst(df_train,path=['Outlet_Location_Type','Outlet_Identifier','Outlet_Type'],values='Item_Outlet_Sales')","1a5b4566":"df_train.info()","29218376":"df_test.info()","cec9e60d":"df_train['Item_Identifier'].unique()\ndf_train.drop('Item_Identifier',axis=1,inplace=True)","2137f341":"df_test['Item_Identifier'].unique()\ndf_test.drop('Item_Identifier',axis=1,inplace=True)","c463dfba":"print('Values before Imputing numeric values:',df_train['Item_Fat_Content'].unique())\ndf_train['Item_Fat_Content']=df_train['Item_Fat_Content'].apply(lambda x: x.lower())\ndf_train['Item_Fat_Content']=df_train['Item_Fat_Content'].apply(lambda x:'lf' if x=='low fat' else x )\ndf_train['Item_Fat_Content']=df_train['Item_Fat_Content'].apply(lambda x:'reg' if x=='regular' else x )\ndf_train['Item_Fat_Content']=df_train['Item_Fat_Content'].map({\n    'lf':0,\n    'reg':1\n})\nprint('Values after Imputing numeric values:',df_train['Item_Fat_Content'].unique())","ee4fbf81":"print('Values before Imputing numeric values:',df_test['Item_Fat_Content'].unique())\ndf_test['Item_Fat_Content']=df_test['Item_Fat_Content'].apply(lambda x: x.lower())\ndf_test['Item_Fat_Content']=df_test['Item_Fat_Content'].apply(lambda x:'lf' if x=='low fat' else x )\ndf_test['Item_Fat_Content']=df_test['Item_Fat_Content'].apply(lambda x:'reg' if x=='regular' else x )\ndf_test['Item_Fat_Content']=df_test['Item_Fat_Content'].map({\n    'lf':0,\n    'reg':1\n})\nprint('Values after Imputing numeric values:',df_test['Item_Fat_Content'].unique())","96a9f75d":"print('Values before Imputing numeric values:', df_train['Outlet_Size'].unique())\ndf_train['Outlet_Size']=df_train['Outlet_Size'].map({ 'Medium':1,                            \n                                'High':2,\n                                'Small':3\n})\nprint('Values after Imputing numeric values:' ,df_train['Outlet_Size'].unique())","4af807dd":"print('Values before Imputing numeric values:', df_test['Outlet_Size'].unique())\ndf_test['Outlet_Size']=df_test['Outlet_Size'].map({ 'Medium':1,                            \n                                'High':2,\n                                'Small':3\n})\nprint('Values after Imputing numeric values:' ,df_test['Outlet_Size'].unique())","befe8807":"print('Values before Imputing :'  ,df_train['Outlet_Location_Type'].unique())\ndf_train['Outlet_Location_Type']=df_train['Outlet_Location_Type'].map({ 'Tier 1':1,\n                                'Tier 3':3,\n                                'Tier 2':2   \n})\nprint('Values after Imputing :'  ,df_train['Outlet_Location_Type'].unique())","e4f2034a":"print('Values before Imputing :'  ,df_test['Outlet_Location_Type'].unique())\ndf_test['Outlet_Location_Type']=df_test['Outlet_Location_Type'].map({ 'Tier 1':1,\n                                'Tier 3':3,\n                                'Tier 2':2   \n})\nprint('Values after Imputing :'  ,df_test['Outlet_Location_Type'].unique())","f8c375fa":"print(df_train['Outlet_Type'].unique())\ndf_train['Outlet_Type']=df_train['Outlet_Type'].map({ 'Supermarket Type1':1,\n                                'Supermarket Type2':2,\n                                'Grocery Store':3,\n                                'Supermarket Type3':4\n})\nprint('Values after Imputing:'  ,df_train['Outlet_Type'].unique())","213498bf":"print(df_test['Outlet_Type'].unique())\ndf_test['Outlet_Type']=df_test['Outlet_Type'].map({ 'Supermarket Type1':1,\n                                'Supermarket Type2':2,\n                                'Grocery Store':3,\n                                'Supermarket Type3':4\n})\nprint('Values after Imputing:'  ,df_test['Outlet_Type'].unique())","d8892be0":"df_train=pd.get_dummies(df_train, columns= ['Item_Type','Outlet_Identifier','Outlet_Type'],drop_first=True)\n","2978b8a7":"for col in df_train.iloc[:,0:7].columns:\n#     if type(col) !='str':\n    print(col)\n    sns.boxplot(x=df_train[col],data=df_train)\n    plt.show()","446ec203":"def boxoutlier(var):\n    for x in var.iloc[:,2:3].columns :        \n        Q1=var[x].quantile(0.25)\n        Q3=var[x].quantile(0.75)\n        IQR=Q3-Q1\n        Lower = Q1-(1.5*IQR)\n        Upper = Q3+(1.5*IQR)\n        var.loc[:,x]=np.where(var[x].values > Upper,Upper,var[x].values)\n        var.loc[:,x]=np.where(var[x].values < Lower,Lower,var[x].values)\n        \n    return var\ndf_train=boxoutlier(df_train)","03b55761":"for col in df_train.iloc[:,0:7].columns:\n    print(col)\n    sns.boxplot(x=df_train[col],data=df_train)\n    plt.show()","4f2dc51a":"df_test=pd.get_dummies(df_test, columns= ['Item_Type','Outlet_Identifier','Outlet_Type'],drop_first=True)","3280eaf5":"df_train.sample(5)","d6b864fa":"X=df_train.drop('Item_Outlet_Sales',axis=1)\nX.isnull().sum()","22a7985c":"y=df_train['Item_Outlet_Sales']","e17bd0a9":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=4 )\ndisplay(X_train.head(),y_train.head(),'Testing Data',X_test.head(),y_test.head())","dd24b205":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_test=sc.transform(X_test)\ndf_test=sc.transform(df_test)","8652d156":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics","7fd195b3":"clf = DecisionTreeRegressor() # defining Classifier\nparam_dist = {\n            'max_depth': [2,5,10,50,25,30,40,],\n             } #list of estimators i.e number of Tress to be Considered for Training\n\ndt_gs = GridSearchCV(clf, param_grid=param_dist, cv = 6) # CV = 5 data will be split into train & test folds 5 times\ndt_gs.fit(X_train, y_train) # Fitting Gridsearch to Trainig Data\n\n\npredict_Xtest_gcv=dt_gs.predict(X_test)\npredict_test_gcv=dt_gs.predict(df_test)\n\nfrom sklearn.metrics import mean_squared_error\nprint('mean_squared_error',mean_squared_error(y_test,predict_Xtest_gcv))\nprint('RMSE',np.sqrt(mean_squared_error(y_test,predict_Xtest_gcv)))","aeefba7d":"rf_c = RandomForestRegressor(n_estimators=50, max_depth=15, random_state = 47, min_samples_leaf = 10) \nrf_c.fit(X_train,y_train)\n\npredict_Xtest_rf=dt_gs.predict(X_test)\npredict_test_rf=dt_gs.predict(df_test)\n\nfrom sklearn.metrics import mean_squared_error\nprint('Random Forest ', 'mean_squared_error',mean_squared_error(y_test,predict_Xtest_rf))\nprint('Random Forest ','RMSE',np.sqrt(mean_squared_error(y_test,predict_Xtest_rf)))\n","6ec4c71e":"# Treating Categorical Columns\n","f3d3ff6f":"# **Treatment of Null_Values**","e81d01ef":"![image.png](attachment:image.png)","1845e975":"\n\n**Variable\t     Description**\nItem_Identifier\t Unique product ID\nItem_Weight\t     Weight of product\nItem_Fat_Content\tWhether the product is low fat or not\nItem_Visibility\tThe % of total display area of all products in a store allocated to the particular product\nItem_Type\t    The category to which the product belongs\nItem_MRP\t    Maximum Retail Price (list price) of the product\nOutlet_Identifier\tUnique store ID\nOutlet_Establishment_Year\tThe year in which store was established\nOutlet_Size\t    The size of the store in terms of ground area covered\nOutlet_Location_Type\tThe type of city in which the store is located\nOutlet_Type\t     Whether the outlet is just a grocery store or some sort of supermarket\nItem_Outlet_Sales\tSales of the product in the particular store. This is the outcome variable to be predicted.","79d6d71d":"# Data Visualisation\n","de5aff79":"# **Building a predictive model and predicting the sales of each product at a particular outlet**"}}