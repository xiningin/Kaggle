{"cell_type":{"35506961":"code","4f4b8bfd":"code","f3ecd319":"code","c5d3dc19":"code","75e4f8b2":"code","c191aacb":"code","78be27be":"code","2825976b":"code","0477b4d0":"code","8e79ba24":"code","a83951eb":"code","3ee04ed0":"code","c3ee3f2c":"code","40f0ab36":"code","d6cb23e1":"code","5001568e":"code","5a0ec97b":"code","36dc88a2":"code","c478c787":"code","0ecf2ce4":"code","0c1f40e7":"code","f2546fd8":"code","61e72ebd":"code","8a92d684":"code","06e605f2":"markdown","b53b48b7":"markdown","c877ea5b":"markdown","2833f62a":"markdown","611b5611":"markdown","901a9b74":"markdown","1b42da28":"markdown","bcd3bf31":"markdown","ff3ccacf":"markdown","2674fc27":"markdown","1a9be58f":"markdown"},"source":{"35506961":"import os #directory navigation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # data visualization\nfrom skimage.io import imread # data processing; images\nimport keras # deep learning\nfrom keras import Sequential # model building\nfrom keras.applications import MobileNetV2 # pretrained model\nfrom keras.layers import Dense # neural network layer\nfrom keras.preprocessing import image # data processing; images\nimport tensorflow as tf # machine learning; deep learning\nimport tensorflow.keras.layers as layers # model building\nimport warnings # what if","4f4b8bfd":"# Reproducibility\ndef set_seed(seed=42):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()","f3ecd319":"brain_df = pd.read_csv('..\/input\/brain-tumor\/Brain Tumor.csv', usecols=[0,1])\nbrain_df.head()","c5d3dc19":"# Check for null variables\nbrain_df.isnull().sum()","75e4f8b2":"# Check for imbalance\nbrain_df['Class'].value_counts()","c191aacb":"# Plot the value count\nsns.countplot(brain_df['Class'])","78be27be":"# Get image paths to build a dictionary for data generators\npath_list = []\nbase_path = '..\/input\/brain-tumor\/Brain Tumor\/Brain Tumor'\nfor entry in os.listdir(base_path):\n    path_list.append(os.path.join(base_path, entry))","2825976b":"# Create path dictionary and map it to brain_df['paths']\npaths_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in path_list}\nbrain_df['Path'] = brain_df['Image'].map(paths_dict.get)\nbrain_df.head()","0477b4d0":"# Plot few samples\nfor x in range(0,9):\n    plt.subplot(3,3,x+1)\n    # Remove x and y axis scales\n    plt.xticks([])\n    plt.yticks([])\n    img = imread(brain_df['Path'][x])\n    plt.imshow(img)\n    plt.xlabel(brain_df['Class'][x])","8e79ba24":"# Split brain_df into test and train lists for data generators\nbrain_df['split'] = np.random.randn(brain_df.shape[0], 1)\n\nmsk = np.random.rand(len(brain_df)) <= 0.8\n\ntrain_df = brain_df[msk]\ntest_df = brain_df[~msk]\ntrain_df.to_csv('brain_tumor_train.csv', index=False)\ntest_df.to_csv('brain_tumor_test.csv', index=False)\ntrain_list = train_df.values.tolist()\ntest_list = test_df.values.tolist()","a83951eb":"from random import shuffle\nimport cv2\ndef generator(samples, batch_size=32,shuffle_data=True):\n    \"\"\"\n    Yields the next training batch.\n    Suppose `samples` is an array [[image1_filename,label1], [image2_filename,label2],...].\n    \"\"\"\n    num_samples = len(samples)\n    while True: # Loop forever so the generator never terminates\n        shuffle(samples)\n\n        # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]\n        for offset in range(0, num_samples, batch_size):\n            # Get the samples you'll use in this batch\n            batch_samples = samples[offset:offset+batch_size]\n\n            # Initialise X_train and y_train arrays for this batch\n            X_train = []\n            y_train = []\n\n            # For each example\n            for batch_sample in batch_samples:\n                # Load image (X) and label (y)\n                label = batch_sample[1]\n                img_path = batch_sample[2]\n                img =  cv2.imread(img_path)\n                \n                # apply any kind of preprocessing\n                # img = cv2.resize(img,(resize,resize))\n                img = img.astype(np.float32)\n                # Add example to arrays\n                X_train.append(keras.applications.nasnet.preprocess_input(img))\n                y_train.append(label)\n\n            # Make sure they're numpy arrays (as opposed to lists)\n            X_train = np.array(X_train)\n            y_train = np.array(y_train)\n\n            # The generator-y part: yield the next training batch            \n            yield X_train, y_train","3ee04ed0":"# Create test and train generators\ntrain_generator = generator(train_list)\ntest_generator = generator(test_list)","c3ee3f2c":"# Resizing image (not used, since we are using generator with resize function)\n# from PIL.Image import open\n# brain_df['pixels']=brain_df['paths'].map(lambda x:np.asarray(open(x).resize((331,331))))","40f0ab36":"# CPU stats\n# import os, psutil  \n\n# def cpu_stats():\n#     pid = os.getpid()\n#     py = psutil.Process(pid)\n#     memory_use = py.memory_info()[0] \/ 2. ** 30\n#     return 'memory GB:' + str(np.round(memory_use, 2))","d6cb23e1":"model = Sequential([\n    # base\n    MobileNetV2(input_shape=(224, 224, 3),include_top=False, weights='imagenet'),\n    layers.GlobalAveragePooling2D(),\n    layers.Dropout(0.2),\n    layers.Dense(units=1, activation='sigmoid',name='preds'),   \n])\nmodel.layers[0].trainable= False\n# show model summary\nmodel.summary()","5001568e":"model.compile(\n    # Set the loss as binary_crossentropy\n    loss='binary_crossentropy',\n    # Set the optimizer to Adam\n    optimizer=keras.optimizers.Adam(epsilon=0.01),\n    # Set the metric as accuracy\n    metrics=['binary_accuracy']\n)","5a0ec97b":"# Measure memory consumption by file\n\n# import sys\n\n# # These are the usual ipython objects, including this one you are creating\n# ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# # Get a sorted list of the objects and their sizes\n# sorted([(x, sys.getsizeof(globals().get(x))\/1024**3) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], \n#        key=lambda x: x[1], reverse=True)\n","36dc88a2":"# Set parameters for model training\nbatch_size = 32\ntrain_size = len(train_list)\ntest_size = len(test_list)\nsteps_per_epoch = train_size\/\/batch_size\nvalidation_steps = test_size\/\/batch_size","c478c787":"# Use early stopping to cut resource wasting\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True,\n)","0ecf2ce4":"# Train the model\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = steps_per_epoch,\n    epochs=110,\n    validation_data=test_generator,\n    validation_steps = validation_steps,\n    verbose=1,\n    callbacks = [early_stopping]\n)\nmodel.save(\"model_brain_adam.h5\")\nprint(\"Saved model to disk\")","0c1f40e7":"# Graph loss and binary accuracy graphs\nhistory_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5\nhistory_df.loc[5:, ['loss', 'val_loss']].plot()\nhistory_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","f2546fd8":"# Evaluate samples using the model I've pretrained, saved, and loaded back\npretrained_cnn = keras.models.load_model('..\/input\/h5files\/model_brain_adam.h5')\neval_score = pretrained_cnn.evaluate(test_generator, steps = validation_steps)\n# print loss score\nprint('Eval loss:',eval_score[0])\n# print accuracy score\nprint('Eval accuracy:',eval_score[1])","61e72ebd":"# Output classification report and confusion matrix\nfrom sklearn.metrics import confusion_matrix , classification_report\n# Get predicted and true classes for our report and matrix\ny_pred = np.rint(pretrained_cnn.predict_generator(test_generator, steps = validation_steps)).astype(int)\ny_test = [i[1] for i in test_list[0:-2]]\ntarget_classes = ['No Tumor','Tumor']\n\nclassification_report(y_test , y_pred , output_dict = True\n                      , target_names=target_classes)","8a92d684":"confusion_matrix(y_test , y_pred ) ","06e605f2":"## Results","b53b48b7":"## Intro\nThe following model was built using Angie Ashraf's [approach](https:\/\/www.kaggle.com\/angieashraf\/89-brain-tumor-detection-using-dl).","c877ea5b":"### My Approach:\n1. Preliminary Data Analysis: check for null values, distribution, do basic plot analysis.\n2. Data Preprocessing: split data in train and test sets, create data generator function with image resizing, float32 and numpy array conversions.\n3. Building Model: use MobileNetV2 with global average pooling layer, dropout of 0.2 and top dense layer with sigmoid activation.\n4. Optimization: use binary cross entropy loss with adam optimizer.\n5. Training Model: use early stopping.\n6. Results: binary accuracy and loss graphs, classification report and confusion matrix","2833f62a":"## Data Preprocessing\n1. Data Cleaning. Data is clean; images are stored in one folder with feature and label details located in csv file.\n2. Data Integration. Data is coming from one source; no data integration techniques were applied.\n3. Data Transformation. Images were resized to 224x224 (below, in the generator function) for MobileNetV2 pretrained base, converted to float and numpy array format for CNN. For the future: data augmentation?\n4. Data Reduction. No data reduction techniques were used (ignoring image size reduction from 240x240 to 224x224). Furthermore, we cannot decrease the number of channels (could have been a possibility since images are almost black and white) due to the pretrained model that expects 3 channels as the input.\n5. Data discretization. Not applicable to images; the inputs are already discrete.","611b5611":"## Conclusion\nWe misclassified 43 images out of 693, with sensitivity of 94.006% and specificity of 94.272%.","901a9b74":"### Data Generator\nBelow is a generator function taken from [this article](https:\/\/medium.com\/@anuj_shah\/creating-custom-data-generator-for-training-deep-learning-models-part-2-be9ad08f3f0e). Used due to CPU limitations.","1b42da28":"## Building Model","bcd3bf31":"## Training Model","ff3ccacf":"## Optimization","2674fc27":"## Preliminary Data Analysis","1a9be58f":"### Libraries"}}