{"cell_type":{"724c31a3":"code","8e4cdf44":"code","8f352338":"code","4d4b3afc":"code","1eca627a":"code","c1db0999":"code","ed1f6458":"code","933374b6":"code","bb16eb8d":"code","75f63b9d":"code","5f065c04":"code","c0e6b8f0":"code","ad83d4d5":"code","b1444984":"code","5a4eed04":"code","6b2e67c3":"code","acff4a63":"code","058926d6":"code","9729ac91":"code","bbf569c2":"code","a74a21eb":"code","318746fb":"code","12aab012":"code","01eae289":"code","4207e4ab":"code","e752efab":"code","8ee3c649":"code","e04c1701":"code","a5dd24b2":"code","b5dd594e":"code","ea541cb4":"code","7a6ddca3":"code","d0ba5b07":"code","3b509a51":"code","ced09a0b":"code","e151e712":"code","ab3995f5":"code","97261c49":"code","9d90092c":"code","c878c132":"code","a5811bd5":"code","03918930":"code","8461e159":"code","1de03131":"code","e7a126d8":"code","5d0ad553":"code","01b9617c":"code","0f694b0f":"code","aced6897":"code","723cb394":"code","648eeb58":"code","9b22449d":"code","41c114a5":"code","9cf04a2a":"code","9ca4b8e5":"code","ce1a9a2c":"code","0049193e":"code","984afbae":"code","2fd9559f":"code","c663d8b8":"code","0ebfeb7c":"code","5a942a7c":"code","90d52ce6":"code","c86ad868":"code","4330d042":"code","40470715":"code","8b17a610":"markdown","716bca4c":"markdown","4c00bc86":"markdown","a9fac9cb":"markdown","3c71eefb":"markdown","8518d648":"markdown","24b10a57":"markdown","ee058988":"markdown","494fc4d0":"markdown","bd1ae87d":"markdown","5d00e768":"markdown","710ef905":"markdown","96c0257c":"markdown","f7ff0494":"markdown","fb49b1bb":"markdown","9e1310f7":"markdown","6c0a191e":"markdown","0a1a8dce":"markdown","ae017d4d":"markdown","96b6cee2":"markdown","4ebaefec":"markdown","04349a4c":"markdown","72c77e0f":"markdown","ab874ac9":"markdown","7f72d422":"markdown","4aaf353e":"markdown","597469d2":"markdown","50e67e2e":"markdown","82e1c29e":"markdown","10255c75":"markdown","1ec9f255":"markdown","7748b803":"markdown","b1fda8c2":"markdown","f7d2f0a4":"markdown","94df1d5b":"markdown","3ce57a77":"markdown","f49a6487":"markdown","44b76392":"markdown","d1b404e0":"markdown","fa8ec53b":"markdown","d25f1946":"markdown","d1992deb":"markdown","94913386":"markdown","20d6f815":"markdown","fa1163da":"markdown","b03f458d":"markdown","d3dfc2bb":"markdown","812b4730":"markdown"},"source":{"724c31a3":"!pip install tensorflow-gpu==2.0.0-alpha0","8e4cdf44":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport operator\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nfrom tensorflow.keras import layers\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', 1000)","8f352338":"print(tf.__version__)","4d4b3afc":"x = tf.random.uniform([3, 3])\n\nprint(\"Is there a GPU available: \"),\nprint(tf.test.is_gpu_available())\n\nprint(\"Is the Tensor on GPU #0:  \"),\nprint(x.device.endswith('GPU:0'))\n\nprint(\"Device name: {}\".format((x.device)))","1eca627a":"print(tf.executing_eagerly())","c1db0999":"print(tf.keras.__version__)","ed1f6458":"df  = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv('..\/input\/test.csv')\n","933374b6":"df[df.target==1].head(10)","bb16eb8d":"print(\"Number of questions: \", df.shape[0])","75f63b9d":"df.target.value_counts()","5f065c04":"print(\"Percentage of insincere questions: {}\".format(sum(df.target == 1)*100\/len(df.target)))","c0e6b8f0":"# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n","ad83d4d5":"plot_wordcloud(df[df.target == 0][\"question_text\"], title=\"Word Cloud of Sincere Questions\")","b1444984":"plot_wordcloud(df[df.target == 1][\"question_text\"], title=\"Word Cloud of Insincere Questions\")","5a4eed04":"stopwords = set(STOPWORDS)","6b2e67c3":"sincere_words = df[df.target==0].question_text.apply(lambda x: x.lower().split()).tolist()\ninsincere_words = df[df.target==1].question_text.apply(lambda x: x.lower().split()).tolist()\n\nsincere_words = [item for sublist in sincere_words for item in sublist if item not in stopwords]\ninsincere_words = [item for sublist in insincere_words  for item in sublist if item not in stopwords ]","acff4a63":"print('Number of sincere words',len(sincere_words))\nprint('Number of insincere words',len(insincere_words))","058926d6":"sincere_words_counter = Counter(sincere_words)\ninsincere_words_counter = Counter(insincere_words)","9729ac91":"most_common_sincere_words = sincere_words_counter.most_common()[:10]\nmost_common_sincere_words = pd.DataFrame(most_common_sincere_words)\nmost_common_sincere_words.columns = ['word', 'freq']\nmost_common_sincere_words['percentage'] = most_common_sincere_words.freq *100 \/ sum(most_common_sincere_words.freq)\nmost_common_sincere_words","bbf569c2":"most_common_insincere_words = insincere_words_counter.most_common()[:10]\nmost_common_insincere_words = pd.DataFrame(most_common_insincere_words)\nmost_common_insincere_words.columns = ['word', 'freq']\nmost_common_insincere_words['percentage'] = most_common_insincere_words.freq *100 \/ sum(most_common_insincere_words.freq)\nmost_common_insincere_words","a74a21eb":"def generate_ngrams(words, n):\n    \n    # Use the zip function to help us generate n-grams\n    # Concatentate the tokens into ngrams and return\n    ngrams = zip(*[words[i:] for i in range(n)])\n    return [\" \".join(ngram) for ngram in ngrams]","318746fb":"n = 3","12aab012":"sincere_ngram_counter = Counter(generate_ngrams(sincere_words, n))\ninsincere_ngram_counter = Counter(generate_ngrams(insincere_words, n))","01eae289":"most_common_sincere_ngram = sincere_ngram_counter.most_common()[:10]\nmost_common_sincere_ngram = pd.DataFrame(most_common_sincere_ngram)\nmost_common_sincere_ngram.columns = ['word', 'freq']\nmost_common_sincere_ngram['percentage'] = most_common_sincere_ngram.freq *100 \/ sum(most_common_sincere_ngram.freq)\nmost_common_sincere_ngram","4207e4ab":"most_common_insincere_ngram = insincere_ngram_counter.most_common()[:10]\nmost_common_insincere_ngram = pd.DataFrame(most_common_insincere_ngram)\nmost_common_insincere_ngram.columns = ['word', 'freq']\nmost_common_insincere_ngram['percentage'] = most_common_insincere_ngram.freq *100 \/ sum(most_common_insincere_ngram.freq)\nmost_common_insincere_ngram","e752efab":"# config values\nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use","8ee3c649":"X_train, X_test  = train_test_split(df, test_size=0.1, random_state=2019)\ny_train, y_test = X_train['target'].values, X_test['target'].values","e04c1701":"X_train = X_train['question_text'].fillna('_NA_').values\nX_test = X_test['question_text'].fillna('_NA_').values\nX_submission = df_test['question_text'].fillna('_NA_').values","a5dd24b2":"X_train.shape","b5dd594e":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\nX_submission = tokenizer.texts_to_sequences(X_submission)","ea541cb4":"X_train = pad_sequences(X_train, maxlen=maxlen)\nX_test = pad_sequences(X_test, maxlen=maxlen)\nX_submission = pad_sequences(X_submission, maxlen=maxlen)","7a6ddca3":"def data_prep(df):\n    print(\"Splitting dataframe with shape {} into training and test datasets\".format(df.shape))\n    X_train, X_test  = train_test_split(df, test_size=0.1, random_state=2019)\n    y_train, y_test = X_train['target'].values, X_test['target'].values\n    \n    print(\"Filling missing values\")\n    X_train = X_train['question_text'].fillna('_NA_').values\n    X_test = X_test['question_text'].fillna('_NA_').values\n    X_submission = df_test['question_text'].fillna('_NA_').values\n    \n    print(\"Tokenizing {} questions into words\".format(df.shape[0]))\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(X_train))\n    X_train = tokenizer.texts_to_sequences(X_train)\n    X_test = tokenizer.texts_to_sequences(X_test)\n    X_submission = tokenizer.texts_to_sequences(X_submission)\n    \n    print(\"Padding sequences for uniform dimensions\")\n    X_train = pad_sequences(X_train, maxlen=maxlen)\n    X_test = pad_sequences(X_test, maxlen=maxlen)\n    X_submission = pad_sequences(X_submission, maxlen=maxlen)\n    \n    print(\"Completed data preparation, returning training, test and submission datasets, split as dependent(X) and independent(Y) variables\")\n    \n    return X_train, X_test, y_train, y_test, X_submission","d0ba5b07":"model1 = Sequential()\nmodel1.add(Embedding(max_features, embed_size, input_length=maxlen))\nmodel1.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\nmodel1.add(GlobalMaxPool1D())\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(64, activation='relu'))\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(32, activation='relu'))\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(1, activation='sigmoid'))\nmodel1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel1.summary()","3b509a51":"%time model1.fit(X_train, y_train, batch_size=512, epochs=2, validation_data=(X_test, y_test), verbose = 1)","ced09a0b":"pred_test_y = model1.predict([X_test], batch_size=1024, verbose=1)\n","e151e712":"opt_prob = None\nf1_max = 0\n\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    f1 = metrics.f1_score(y_test, (pred_test_y > thresh).astype(int))\n    print('F1 score at threshold {} is {}'.format(thresh, f1))\n    \n    if f1 > f1_max:\n        f1_max = f1\n        opt_prob = thresh\n        \nprint('Optimal probabilty threshold is {} for maximum F1 score {}'.format(opt_prob, f1_max))","ab3995f5":"pred_submission_y = model1.predict([X_submission], batch_size=1024, verbose=1)\npred_submission_y = (pred_submission_y > opt_prob).astype(int)\n\ndf_submission = pd.DataFrame({'qid': df_test['qid'].values})\ndf_submission['prediction'] = pred_submission_y\n#df_submission.to_csv(\"submission.csv\", index=False)","97261c49":"def load_embed(file):\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    \n    if file == '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding=\"utf8\") if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n    return embeddings_index","9d90092c":"glove = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\nparagram =  '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\nwiki_news = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'","c878c132":"print(\"Extracting GloVe embedding\")\nembed_glove = load_embed(glove)\n#print(\"Extracting Paragram embedding\")\n#embed_paragram = load_embed(paragram)\n#print(\"Extracting FastText embedding\")\n#embed_fasttext = load_embed(wiki_news)","a5811bd5":"def build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","03918930":"def check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words \/ (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","8461e159":"vocab = build_vocab(df['question_text'])","1de03131":"print(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)\n#print(\"Paragram : \")\n#oov_paragram = check_coverage(vocab, embed_paragram)\n#print(\"FastText : \")\n#oov_fasttext = check_coverage(vocab, embed_fasttext)","e7a126d8":"type(embed_glove)","5d0ad553":"dict(list(embed_glove.items())[20:22])","01b9617c":"df['processed_question'] = df['question_text'].apply(lambda x: x.lower())","0f694b0f":"vocab_low = build_vocab(df['processed_question'])","aced6897":"print(\"Glove : \")\noov_glove = check_coverage(vocab_low, embed_glove)\n#print(\"Paragram : \")\n#oov_paragram = check_coverage(vocab_low, embed_paragram)\n#print(\"FastText : \")\n#oov_fasttext = check_coverage(vocab_low, embed_fasttext)","723cb394":"oov_glove[1:20]","648eeb58":"def add_lower(embedding, vocab):\n    count = 0\n    for word in vocab:\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\n    print(f\"Added {count} words to embedding\")","9b22449d":"print(\"Glove : \")\nadd_lower(embed_glove, vocab)\n#print(\"Paragram : \")\n#add_lower(embed_paragram, vocab)\n#print(\"FastText : \")\n#add_lower(embed_fasttext, vocab)","41c114a5":"print(\"Glove : \")\noov_glove = check_coverage(vocab_low, embed_glove)\n#print(\"Paragram : \")\n#oov_paragram = check_coverage(vocab_low, embed_paragram)\n#print(\"FastText : \")\n#oov_fasttext = check_coverage(vocab_low, embed_fasttext)","9cf04a2a":"oov_glove[1:20]","9ca4b8e5":"punctuations = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'","ce1a9a2c":"def clean_text(x):\n\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in punctuations:\n        x = x.replace(punct, '')\n    return x","0049193e":"df[\"processed_question\"] = df[\"processed_question\"].progress_apply(lambda x: clean_text(x))","984afbae":"vocab_low = build_vocab(df['processed_question'])","2fd9559f":"print(\"Glove : \")\noov_glove = check_coverage(vocab_low, embed_glove)\n#print(\"Paragram : \")\n#oov_paragram = check_coverage(vocab_low, embed_paragram)\n#print(\"FastText : \")\n#oov_fasttext = check_coverage(vocab_low, embed_fasttext)","c663d8b8":"df['question_text'] = df['processed_question']","0ebfeb7c":"X_train, X_test, y_train, y_test, X_submission = data_prep(df)","5a942a7c":"model1 = Sequential()\nmodel1.add(Embedding(max_features, embed_size, input_length=maxlen, weights = [embed_glove]))\nmodel1.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\nmodel1.add(GlobalMaxPool1D())\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(64, activation='relu'))\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(32, activation='relu'))\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(1, activation='sigmoid'))\nmodel1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel1.summary()","90d52ce6":"%time model1.fit(X_train, y_train, batch_size=512, epochs=5, validation_data=(X_test, y_test), verbose = 1)","c86ad868":"pred_test_y = model1.predict([X_test], batch_size=1024, verbose=1)\n","4330d042":"opt_prob = None\nf1_max = 0\n\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    f1 = metrics.f1_score(y_test, (pred_test_y > thresh).astype(int))\n    print('F1 score at threshold {} is {}'.format(thresh, f1))\n    \n    if f1 > f1_max:\n        f1_max = f1\n        opt_prob = thresh\n        \nprint('Optimal probabilty threshold is {} for maximum F1 score {}'.format(opt_prob, f1_max))","40470715":"pred_submission_y = model1.predict([X_submission], batch_size=1024, verbose=1)\npred_submission_y = (pred_submission_y > opt_prob).astype(int)\n\ndf_submission = pd.DataFrame({'qid': df_test['qid'].values})\ndf_submission['prediction'] = pred_submission_y\ndf_submission.to_csv(\"submission.csv\", index=False)","8b17a610":"## Missing value treatment\n- Fill missing questions with a string placeholder \"\\_NA\\_\"","716bca4c":"## Padding sequences for uniform dimensions","4c00bc86":"## Wordclouds for the question text corpus\n- This will show us the most frequently used words for sincere and insincere labeled questions","a9fac9cb":"# Further Improvements:\n- Optimizing LSTM hyperparameters\n- Optimizing LSTM network structure (adding LSTM, dense, maxpooling etc. layers)\n- Text processing to further improve embeddings coverage\n- Using all 3 embeddings together\/combining the weighted output of 3 LSTM models using each embedding","3c71eefb":"## Tokenizing the sentences to words","8518d648":"# Calculate optimal probability threshold for classification\n- Calculating best probability cut-off giving the highest F1 - Score","24b10a57":"# Submission","ee058988":"# Building Vocabulary and calculating coverage","494fc4d0":"## Removing special characters appropriately\n- This ensures better a match to embeddings","bd1ae87d":"## Function to load embeddings from file","5d00e768":"#### Check GPU detection for tensorflow","710ef905":"### Most common insincere words","96c0257c":"## Target variable response rate\n- __6.18 %__ questions are insincere in the provided dataset","f7ff0494":"### Most common sincere words","fb49b1bb":"# LSTM Text classification using Tensorflow 2.0 Alpha","9e1310f7":"# Utilizing embeddings in LSTM classifier\n- Following a similar model network structure as previous for comparable results","6c0a191e":"### Defining function for generating wordcloud","0a1a8dce":"### Wordcloud for sincere questions","ae017d4d":"# Data Preparation","96b6cee2":"#### Check Tensorflow Version","4ebaefec":"# Text pre-processing to improve coverage of embeddings","04349a4c":"# Calculate optimal probability threshold for classification","72c77e0f":"# Define LSTM layers and parameters\n- Bidirectional LSTM using CUDA GPU processing\n- No embeddings used currently\n- Two fully connected layers with dropout layers for reducing chances of overfit","ab874ac9":"# Environment checks","7f72d422":"# Loading embeddings","4aaf353e":"### Installing TF-gpu 2.0 Alpha","597469d2":"### Most common sincere n-grams","50e67e2e":"### Most common insincere n-grams\n- We observe a pattern of suggestive\/controversial n-grams amongst the insincere questions","82e1c29e":"#### Check if Eager execution is running\n- Eager execution enables a more interactive frontend to TensorFlow, the details of which we will discuss much later.\n- [Eager basics official guide](https:\/\/www.tensorflow.org\/tutorials\/eager\/eager_basics)","10255c75":"# Tensorflow 2.0 Alpha important information:\n- [Official TF2 Alpha](https:\/\/www.tensorflow.org\/alpha)\n- [New Features in TF 2](https:\/\/medium.com\/tensorflow\/whats-coming-in-tensorflow-2-0-d3663832e9b8)\n- [Standardizing on Keras for TF2](https:\/\/medium.com\/tensorflow\/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a)\n- [TF2 GPU installation guide](https:\/\/www.tensorflow.org\/install\/gpu)","1ec9f255":"## Data prep function for future use\n- Creating a function including all of the above data prep steps, to use for quick future data processing","7748b803":"# Fit LSTM model ","b1fda8c2":"# Parameters for preprocessing and algorithms","f7d2f0a4":"# Fit LSTM model ","94df1d5b":"# EDA on the Question Text corpus","3ce57a77":"# Submission","f49a6487":"### Function for generating n-grams","44b76392":"### Wordcloud for insincere questions\n- We observe that insincere questions contain highly debated and emotion based topics of Trump, Muslims, America, Russia, Obama, Liberal etc.","d1b404e0":"## Training and Test data split","fa8ec53b":"## Adding lower case words to embeddings if missing","d25f1946":"## Calculating coverage for each embedding","d1992deb":"# Predict using trained model","94913386":"## Word frequency and n-gram frequency\n- Frequency of words can show the most used words for sincere and insincere questions\n- N-grams can be useful in distinguishing sincere and insincere questions by identifying patterns in topics","20d6f815":"# Importing Libraries","fa1163da":"# Importing data","b03f458d":"## Bulding dataset vocabulary","d3dfc2bb":"# Predict using trained model","812b4730":"## Lower casing questions for uniform matching"}}