{"cell_type":{"e067e681":"code","09fa3e74":"code","c44e911c":"code","d34f9b5e":"code","321c287a":"code","fbb89168":"code","51e65ab3":"code","eadf9816":"code","4d19a15d":"code","a3ef9976":"code","0a28b20f":"code","70fe3e8a":"code","a45e3d67":"code","5d767f33":"code","658adbfe":"code","18fcac22":"code","99a8945b":"code","ec34035e":"code","8b07d615":"code","986b1157":"code","8b1e7326":"code","5a993988":"code","5f928c72":"code","5049d85c":"code","9f2882dc":"code","e15f7093":"code","b83983d0":"code","e478c575":"code","d4902c4b":"code","40b943e5":"code","33f48085":"code","b072fd29":"code","0d46de7e":"code","d801f6a4":"markdown","bd7d2594":"markdown","9a677a02":"markdown","70a1f295":"markdown","26a8ca05":"markdown"},"source":{"e067e681":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","09fa3e74":"import matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\n\n# import the train CSV into notebook\n\ndataset = pd.read_csv('..\/input\/train.csv')\nprint(dataset.shape)\ndataset.head()\n","c44e911c":"print(dataset.groupby('Survived').count())","d34f9b5e":"dataset['Family_size'] = dataset['SibSp'].values + dataset['Parch'].values\ndataset.tail()","321c287a":"# Some of the columns not much value added in this i.e., Name, Fare, Ticket etc., so we will remove them off the dataset\n\ndataset = dataset.drop(['Name', 'Ticket', 'Fare', 'Embarked', 'Cabin'], axis = 1)\nprint(dataset.shape)\ndataset.head()","fbb89168":"# move the Survived Column to the end\n\ndataset = dataset[['PassengerId','Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Family_size', 'Survived']]\ndataset.head()","51e65ab3":"# Check for any missing values from the dataset\n\ndataset.isna().any()\n","eadf9816":"dataset['Age'].isna().value_counts()","4d19a15d":"dataset.Age = dataset['Age'].fillna(method = 'ffill')","a3ef9976":"# convert the categorical features to numeric using Labelencoder\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ndataset['Sex'] = le.fit_transform(dataset['Sex'])\ndataset.head()\n","0a28b20f":"dataset['Age'].value_counts()","70fe3e8a":"a = map(lambda x : x\/\/15, dataset['Age'])\na = list(a)\nprint(len(a))","a45e3d67":"dataset['Age'] = pd.Series(a)\ndataset['Age']","5d767f33":"# Split the data as X, Y \n\nX = dataset.iloc[:, 0:7]\nY = dataset['Survived']\n\nprint(X.shape, Y.shape)\nX.head()","658adbfe":"from sklearn.feature_selection import chi2\n\nchi, p = chi2(X, Y)\nchi","18fcac22":"# now split the data into train and CV \n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_cv, y_train, y_cv = train_test_split(X, Y, train_size = 0.8)\nprint(x_train.shape, x_cv.shape)     # Size of Train and CV datasets\nprint(y_train.shape, y_cv.shape) ","99a8945b":"# Now use the Decission Tree to make the prediction\n\ndtree = DecisionTreeClassifier(criterion = 'gini',random_state = 42, min_samples_split = 2)\ndtree","ec34035e":"from sklearn.model_selection import GridSearchCV\n\ndepth = {'max_depth': [3, 4, 5, 6, 7]}\n\ngrid = GridSearchCV(estimator = dtree, param_grid = depth, scoring = 'accuracy', cv = 5, error_score = np.NaN)\ngrid","8b07d615":"# train the model with the CV for finding best fit hyperparameters\n\ngrid.fit(x_train, y_train)\n\nprint(grid.best_score_)\nprint(grid.best_params_)","986b1157":"# predict the CV error\n\ndt = DecisionTreeClassifier(max_depth = grid.best_params_['max_depth'], random_state = 42, min_samples_split = 2)\n\ndt.fit(x_train, y_train)\n\npred_cv = dt.predict(x_cv)\n\naccuracy = metrics.accuracy_score(y_cv, pred_cv)\nprint('The CV datset prediction accuracy is', accuracy)\n","8b1e7326":"print(dt.feature_importances_)","5a993988":"dict(zip(x_train.columns, dt.feature_importances_))","5f928c72":"# Testing it in test dataset\n\ntest_df = pd.read_csv('..\/input\/test.csv')\nprint(test_df.shape)\ntest_df.head()\n","5049d85c":"# Applying those preprocessing steps on the test data\n\ntest_df['Family_size'] = test_df['SibSp'].values + test_df['Parch'].values\n\ntest_df = test_df.drop(['Name', 'Ticket', 'Fare', 'Embarked', 'Cabin'], axis = 1)","9f2882dc":"test_df.isna().any()","e15f7093":"test_df.Age = test_df['Age'].fillna(method = 'ffill')\ntest_df.head()","b83983d0":"test_df['Sex'] = le.fit_transform(test_df['Sex'])\n\nb = map(lambda x : x\/\/15, test_df['Age'])\nb = pd.Series(a)\ntest_df['Age'] = b\n\ntest_df.head()","e478c575":"print(test_df.shape)","d4902c4b":"# Now our test dataset is ready and we can predict its Survival status from the above model\n\npred_test = dt.predict(test_df)\npred_test.shape\nprint(type(pred_test))","40b943e5":"c = dict(zip(test_df['PassengerId'], pred_test))\nc.items()","33f48085":"gender_submission1 = pd.DataFrame(data = list(c.items()), columns = ['PassengerId', 'Survived'])\ngender_submission1.head()","b072fd29":"print(os.listdir(\"..\/input\"))","0d46de7e":"gender_submission1.to_csv('submission.csv',encoding = 'utf-8')","d801f6a4":"Seems that there are 177 rows have missing values in the Age column. We can either drop them off or use any interpolation methods to fill the missing values.","bd7d2594":"Feaure  - SEX plays a critical role in building up the model which has highest feature importance value","9a677a02":"Age column which has more number of categorical values , will bin them into various buckets based on their values. \n\nAge         Group\n\n0 - 18       0\n19 - 35      1\n36 - 45      2\n45 - 55      3\n55+          4","70a1f295":"### From the above results this is not a massively imbalanced data. So there is no need of additional upsampling over the data again","26a8ca05":"Hyperparameter tuning using GridSearchCV method"}}