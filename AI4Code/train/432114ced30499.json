{"cell_type":{"16ba4cea":"code","2ee76e7f":"code","a0f10c9f":"code","a717d50c":"code","13cd8e4f":"code","83cb5a84":"code","975c187c":"code","9615d056":"code","6da163b9":"code","5ca9b71d":"markdown"},"source":{"16ba4cea":"import numpy as np\nimport pandas as pd\nimport os\n\nfrom timeit import default_timer as timer\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nimport torch.utils.data as data\nimport torchvision.models as models\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch\n\nfrom heng_s_utility_functions import *\nfrom heng_s_models_all import *\n\nPI = np.pi\nIMAGE_RGB_MEAN = [0.485, 0.456, 0.406]\nIMAGE_RGB_STD  = [0.229, 0.224, 0.225]\nDEFECT_COLOR = [(0,0,0),(0,0,255),(0,255,0),(255,0,0),(0,255,255)]","2ee76e7f":"SPLIT_DIR = '..\/input\/hengs-split'\nDATA_DIR = '..\/input\/severstal-steel-defect-detection'","a0f10c9f":"class Net(nn.Module):\n    def load_pretrain(self, skip, is_print=True):\n        conversion=copy.copy(CONVERSION)\n        for i in range(0,len(conversion)-8,4):\n            conversion[i] = 'block.' + conversion[i][5:]\n        load_pretrain(self, skip, pretrain_file=PRETRAIN_FILE, conversion=conversion, is_print=is_print)\n\n    def __init__(self, num_class=4, drop_connect_rate=0.2):\n        super(Net, self).__init__()\n\n        e = ResNet34()\n        self.block = nn.ModuleList([\n            e.block0,\n            e.block1,\n            e.block2,\n            e.block3,\n            e.block4,\n        ])\n        e = None  #dropped\n        self.feature = nn.Conv2d(512,32, kernel_size=1) #dummy conv for dim reduction\n        self.logit = nn.Conv2d(32,num_class, kernel_size=1)\n\n    def forward(self, x):\n        batch_size,C,H,W = x.shape\n\n        for i in range( len(self.block)):\n            x = self.block[i](x)\n            #print(i, x.shape)\n\n        x = F.dropout(x,0.5,training=self.training)\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = self.feature(x)\n        logit = self.logit(x)\n        return logit","a717d50c":"# Class which is used by the infor object in __get_item__\nclass Struct(object):\n    def __init__(self, is_copy=False, **kwargs):\n        self.add(is_copy, **kwargs)\n\n    def add(self, is_copy=False, **kwargs):\n        #self.__dict__.update(kwargs)\n\n        if is_copy == False:\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n        else:\n            for key, value in kwargs.items():\n                try:\n                    setattr(self, key, copy.deepcopy(value))\n                    #setattr(self, key, value.copy())\n                except Exception:\n                    setattr(self, key, value)\n\n    def __str__(self):\n        text =''\n        for k,v in self.__dict__.items():\n            text += '\\t%s : %s\\n'%(k, str(v))\n        return text\n\n# Creating masks\ndef run_length_decode(rle, height=256, width=1600, fill_value=1):\n    mask = np.zeros((height,width), np.float32)\n    if rle != '':\n        mask=mask.reshape(-1)\n        r = [int(r) for r in rle.split(' ')]\n        r = np.array(r).reshape(-1, 2)\n        for start,length in r:\n            start = start-1  #???? 0 or 1 index ???\n            mask[start:(start + length)] = fill_value\n        mask=mask.reshape(width, height).T\n    return mask\n\n# Collations\ndef null_collate(batch):\n    batch_size = len(batch)\n\n    input = []\n    truth_mask  = []\n    truth_label = []\n    infor = []\n    for b in range(batch_size):\n        input.append(batch[b][0])\n        truth_mask.append(batch[b][1])\n        infor.append(batch[b][2])\n\n        label = (batch[b][1].reshape(4,-1).sum(1)>8).astype(np.int32)\n        truth_label.append(label)\n\n\n    input = np.stack(input)\n    input = image_to_input(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n    input = torch.from_numpy(input).float()\n\n    truth_mask = np.stack(truth_mask)\n    truth_mask = (truth_mask>0.5).astype(np.float32)\n    truth_mask = torch.from_numpy(truth_mask).float()\n\n    truth_label = np.array(truth_label)\n    truth_label = torch.from_numpy(truth_label).float()\n\n    return input, truth_mask, truth_label, infor\n\n# Metric\ndef metric_hit(logit, truth, threshold=0.5):\n    batch_size,num_class, H,W = logit.shape\n\n    with torch.no_grad():\n        logit = logit.view(batch_size,num_class,-1)\n        truth = truth.view(batch_size,num_class,-1)\n\n        probability = torch.sigmoid(logit)\n        p = (probability>threshold).float()\n        t = (truth>0.5).float()\n\n        tp = ((p + t) == 2).float()  # True positives\n        tn = ((p + t) == 0).float()  # True negatives\n\n        tp = tp.sum(dim=[0,2])\n        tn = tn.sum(dim=[0,2])\n        num_pos = t.sum(dim=[0,2])\n        num_neg = batch_size*H*W - num_pos\n\n        tp = tp.data.cpu().numpy()\n        tn = tn.data.cpu().numpy().sum()\n        num_pos = num_pos.data.cpu().numpy()\n        num_neg = num_neg.data.cpu().numpy().sum()\n\n        tp = np.nan_to_num(tp\/(num_pos+1e-12),0)\n        tn = np.nan_to_num(tn\/(num_neg+1e-12),0)\n\n        tp = list(tp)\n        num_pos = list(num_pos)\n\n    return tn,tp, num_neg,num_pos\n\n# Loss\ndef criterion(logit, truth, weight=None):\n    batch_size,num_class, H,W = logit.shape\n    logit = logit.view(batch_size,num_class)\n    truth = truth.view(batch_size,num_class)\n    assert(logit.shape==truth.shape)\n\n    loss = F.binary_cross_entropy_with_logits(logit, truth, reduction='none')\n\n    if weight is None:\n        loss = loss.mean()\n\n    else:\n        pos = (truth>0.5).float()\n        neg = (truth<0.5).float()\n        pos_sum = pos.sum().item() + 1e-12\n        neg_sum = neg.sum().item() + 1e-12\n        loss = (weight[1]*pos*loss\/pos_sum + weight[0]*neg*loss\/neg_sum).sum()\n        #raise NotImplementedError\n\n    return loss\n\n# Learning Rate Adjustments\ndef adjust_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\ndef get_learning_rate(optimizer):\n    lr=[]\n    for param_group in optimizer.param_groups:\n       lr +=[ param_group['lr'] ]\n\n    assert(len(lr)==1) #we support only one param_group\n    lr = lr[0]\n    return lr\n\n# Learning Rate Schedule\nclass NullScheduler():\n    def __init__(self, lr=0.01 ):\n        super(NullScheduler, self).__init__()\n        self.lr    = lr\n        self.cycle = 0\n\n    def __call__(self, time):\n        return self.lr\n\n    def __str__(self):\n        string = 'NullScheduler\\n' \\\n                + 'lr=%0.5f '%(self.lr)\n        return string","13cd8e4f":"schduler = NullScheduler(lr=0.001)\nbatch_size = 4 #8\niter_accum = 8","83cb5a84":"class SteelDataset(Dataset):\n    def __init__(self, split, csv, mode, augment=None):\n#         import pdb; pdb.set_trace()\n        self.split   = split\n        self.csv     = csv\n        self.mode    = mode\n        self.augment = augment\n\n        self.uid = list(np.concatenate([np.load(SPLIT_DIR + '\/%s'%f , allow_pickle=True) for f in split]))\n        df = pd.concat([pd.read_csv(DATA_DIR + '\/%s'%f) for f in csv])\n        df.fillna('', inplace=True)\n        df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n        df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n        df = df_loc_by_list(df, 'ImageId_ClassId', [ u.split('\/')[-1] + '_%d'%c  for u in self.uid for c in [1,2,3,4] ])\n        self.df = df\n\n    def __str__(self):\n        num1 = (self.df['Class']==1).sum()\n        num2 = (self.df['Class']==2).sum()\n        num3 = (self.df['Class']==3).sum()\n        num4 = (self.df['Class']==4).sum()\n        pos1 = ((self.df['Class']==1) & (self.df['Label']==1)).sum()\n        pos2 = ((self.df['Class']==2) & (self.df['Label']==1)).sum()\n        pos3 = ((self.df['Class']==3) & (self.df['Label']==1)).sum()\n        pos4 = ((self.df['Class']==4) & (self.df['Label']==1)).sum()\n\n        length = len(self)\n        num = len(self)*4\n        pos = (self.df['Label']==1).sum()\n        neg = num-pos\n\n        #---\n\n        string  = ''\n        string += '\\tmode    = %s\\n'%self.mode\n        string += '\\tsplit   = %s\\n'%self.split\n        string += '\\tcsv     = %s\\n'%str(self.csv)\n        string += '\\t\\tlen   = %5d\\n'%len(self)\n        if self.mode == 'train':\n            string += '\\t\\tnum   = %5d\\n'%num\n            string += '\\t\\tneg   = %5d  %0.3f\\n'%(neg,neg\/num)\n            string += '\\t\\tpos   = %5d  %0.3f\\n'%(pos,pos\/num)\n            string += '\\t\\tpos1  = %5d  %0.3f  %0.3f\\n'%(pos1,pos1\/length,pos1\/pos)\n            string += '\\t\\tpos2  = %5d  %0.3f  %0.3f\\n'%(pos2,pos2\/length,pos2\/pos)\n            string += '\\t\\tpos3  = %5d  %0.3f  %0.3f\\n'%(pos3,pos3\/length,pos3\/pos)\n            string += '\\t\\tpos4  = %5d  %0.3f  %0.3f\\n'%(pos4,pos4\/length,pos4\/pos)\n        return string\n\n\n    def __len__(self):\n        return len(self.uid)\n\n\n    def __getitem__(self, index):\n        # print(index)\n        folder, image_id = self.uid[index].split('\/')\n\n        rle = [\n            self.df.loc[self.df['ImageId_ClassId']==image_id + '_1','EncodedPixels'].values[0],\n            self.df.loc[self.df['ImageId_ClassId']==image_id + '_2','EncodedPixels'].values[0],\n            self.df.loc[self.df['ImageId_ClassId']==image_id + '_3','EncodedPixels'].values[0],\n            self.df.loc[self.df['ImageId_ClassId']==image_id + '_4','EncodedPixels'].values[0],\n        ]\n        image = cv2.imread(DATA_DIR + '\/%s\/%s'%(folder,image_id), cv2.IMREAD_COLOR)\n        mask  = np.array([run_length_decode(r, height=256, width=1600, fill_value=1) for r in rle])\n\n        infor = Struct(\n            index    = index,\n            folder   = folder,\n            image_id = image_id,\n        )\n\n        if self.augment is None:\n            return image, mask, infor\n        else:\n            return self.augment(image, mask, infor)","975c187c":"def do_valid(net, valid_loader, displays=None):\n    valid_num  = np.zeros(6, np.float32)\n    valid_loss = np.zeros(6, np.float32)\n    \n    for t, (input, truth_mask, truth_label, infor) in enumerate(valid_loader):\n\n        #if b==5: break\n        net.eval()\n        input = input.cuda()\n        truth_mask  = truth_mask.cuda()\n        truth_label = truth_label.cuda()\n\n        with torch.no_grad():\n            logit = net(input) #data_parallel(net, input)\n            loss  = criterion(logit, truth_label)\n            tn,tp, num_neg,num_pos = metric_hit(logit, truth_label)\n\n\n            #zz=0\n        #---\n        batch_size = len(infor)\n        l = np.array([ loss.item(), tn,*tp])\n        n = np.array([ batch_size, num_neg,*num_pos])\n        valid_loss += l*n\n        valid_num  += n\n\n        #debug-----------------------------\n        if displays is not None:\n            probability = torch.sigmoid(logit)\n            image = input_to_image(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n\n            probability_label = probability.data.cpu().numpy()\n            truth_label = truth_label.data.cpu().numpy()\n            truth_mask  = truth_mask.data.cpu().numpy()\n\n            for b in range(0, batch_size, 4):\n                image_id = infor[b].image_id[:-4]\n                result = draw_predict_result_label(image[b], truth_mask[b], truth_label[b], probability_label[b], stack='vertical')\n                draw_shadow_text(result,'%05d    %s.jpg'%(valid_num[0]-batch_size+b, image_id),(5,24),0.75,[255,255,255],1)\n                image_show('result',result,resize=1)\n#                 cv2.imwrite(out_dir +'\/valid\/%s.png'%(image_id), result)\n#                 cv2.waitKey(1)\n                pass\n        #debug-----------------------------\n\n        #print(valid_loss)\n        print('\\r %8d \/%8d'%(valid_num[0], len(valid_loader.dataset)),end='',flush=True)\n\n        pass  #-- end of one data loader --\n    assert(valid_num[0] == len(valid_loader.dataset))\n    valid_loss = valid_loss\/valid_num\n\n    return valid_loss","9615d056":"def run_train():\n    batch_size = 4\n    \n    initial_checkpoint = \\\n    '\/root\/share\/project\/kaggle\/2019\/steel\/result1\/resnet34-cls-full-foldb0-0\/checkpoint\/00007500_model.pth'\n    \n    train_dataset = SteelDataset(\n        mode    = 'train',\n        csv     = ['train.csv',],\n        split   = ['train_b1_11568.npy',],\n        augment = train_augment,\n    )\n    train_loader  = DataLoader(\n        train_dataset,\n        #sampler     = BalanceClassSampler(train_dataset, 3*len(train_dataset)),\n        #sampler    = SequentialSampler(train_dataset),\n        sampler    = RandomSampler(train_dataset),\n        batch_size  = batch_size,\n        drop_last   = True,\n        num_workers = 2,\n        pin_memory  = True,\n        collate_fn  = null_collate\n    )\n\n    valid_dataset = SteelDataset(\n        mode    = 'train',\n        csv     = ['train.csv',],\n        split   = ['valid_b1_1000.npy',],\n        augment = valid_augment,\n    )\n    valid_loader = DataLoader(\n        valid_dataset,\n        sampler    = SequentialSampler(valid_dataset),\n        #sampler     = RandomSampler(valid_dataset),\n        batch_size  = 4,\n        drop_last   = False,\n        num_workers = 2,\n        pin_memory  = True,\n        collate_fn  = null_collate\n    )\n    \n    assert(len(train_dataset)>=batch_size)\n    \n    net = Net().cuda()\n    \n#     if initial_checkpoint is not None:\n#         state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n#         #for k in ['logit.weight','logit.bias']: state_dict.pop(k, None)\n\n#         net.load_state_dict(state_dict,strict=False)\n#     else:\n#         load_pretrain(net.e, skip=['logit'], is_print=False)\n        \n    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=schduler(0), momentum=0.9, weight_decay=0.0001)\n\n    num_iters   = 3000*1000\n    iter_smooth = 50\n    iter_log    = 500\n    iter_valid  = 1500\n    iter_save   = [0, num_iters-1]\\\n                   + list(range(0, num_iters, 1500))#1*1000\n\n    start_iter = 0\n    start_epoch= 0\n    rate       = 0\n    if initial_checkpoint is not None:\n        initial_optimizer = initial_checkpoint.replace('_model.pth','_optimizer.pth')\n        if os.path.exists(initial_optimizer):\n            checkpoint  = torch.load(initial_optimizer)\n            start_iter  = checkpoint['iter' ]\n            start_epoch = checkpoint['epoch']\n            #optimizer.load_state_dict(checkpoint['optimizer'])\n        pass\n    \n    train_loss = np.zeros(20,np.float32)\n    valid_loss = np.zeros(20,np.float32)\n    batch_loss = np.zeros(20,np.float32)\n    iter = 0\n    i    = 0\n    \n    start = timer()\n#     import pdb; pdb.set_trace()\n    while  iter<num_iters:\n        sum_train_loss = np.zeros(20,np.float32)\n        sum = np.zeros(20,np.float32)\n\n        optimizer.zero_grad()\n        for t, (input, truth_mask, truth_label, infor) in enumerate(train_loader):\n            batch_size = len(infor)\n            iter  = i + start_iter\n            epoch = (iter-start_iter)*batch_size\/len(train_dataset) + start_epoch\n            \n            # Weather to display images or not! While in validation loss\n            displays = None\n            #if 0:\n            if (iter % iter_valid==0):\n                valid_loss = do_valid(net, valid_loader, displays) # omitted outdir variable\n                #pass\n\n            if (iter % iter_log==0):\n                print('\\r',end='',flush=True)\n                asterisk = '*' if iter in iter_save else ' '\n                print('%0.5f  %5.1f%s %5.1f |  %5.3f   %4.2f [%4.2f,%4.2f,%4.2f,%4.2f]  |  %5.3f   %4.2f [%4.2f,%4.2f,%4.2f,%4.2f]  | %s' % (\\\n                         rate, iter\/1000, asterisk, epoch,\n                         *valid_loss[:6],\n                         *train_loss[:6],\n                         time_to_str((timer() - start),'min'))\n                )\n                print('\\n')\n                \n            #if 0:\n            if iter in iter_save:\n                torch.save(net.state_dict(),'..\/working\/%08d_model.pth'%(iter))\n                torch.save({\n                    #'optimizer': optimizer.state_dict(),\n                    'iter'     : iter,\n                    'epoch'    : epoch,\n                }, '..\/working\/%08d_optimizer.pth'%(iter))\n                pass\n\n            # learning rate schduler -------------\n            lr = schduler(iter)\n            if lr<0 : break\n            adjust_learning_rate(optimizer, lr)\n            rate = get_learning_rate(optimizer)\n            \n            net.train()\n            input = input.cuda()\n            truth_label = truth_label.cuda()\n            truth_mask  = truth_mask.cuda()\n\n            logit =  net(input) #data_parallel(net,input)  \n            loss = criterion(logit, truth_label)\n            tn,tp, num_neg,num_pos = metric_hit(logit, truth_label)\n            \n            (loss\/iter_accum).backward()\n            if (iter % iter_accum)==0:\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # print statistics  ------------\n            l = np.array([ loss.item(), tn,*tp ])\n            n = np.array([ batch_size, num_neg,*num_pos ])\n\n            batch_loss[:6] = l\n            sum_train_loss[:6] += l*n\n            sum[:6] += n\n            if iter%iter_smooth == 0:\n                train_loss = sum_train_loss\/(sum+1e-12)\n                sum_train_loss[...] = 0\n                sum[...]            = 0\n\n\n            print('\\r',end='',flush=True)\n            asterisk = ' '\n            print('%0.5f  %5.1f%s %5.1f |  %5.3f   %4.2f [%4.2f,%4.2f,%4.2f,%4.2f]  |  %5.3f   %4.2f [%4.2f,%4.2f,%4.2f,%4.2f]  | %s' % (\\\n                         rate, iter\/1000, asterisk, epoch,\n                         *valid_loss[:6],\n                         *batch_loss[:6],\n                         time_to_str((timer() - start),'min'))\n            , end='',flush=True)\n            i=i+1\n            \n            # debug-----------------------------\n            if 1:\n                for di in range(3):\n                    if (iter+di)%1000==0:\n\n                        probability = torch.sigmoid(logit)\n                        image = input_to_image(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n\n                        probability_label = probability.data.cpu().numpy()\n                        truth_label = truth_label.data.cpu().numpy()\n                        truth_mask  = truth_mask.data.cpu().numpy()\n\n\n                        for b in range(batch_size):\n                            result = draw_predict_result_label(image[b], truth_mask[b], truth_label[b], probability_label[b], stack='vertical')\n\n#                             image_show('result',result,resize=1)\n#                             cv2.imwrite('..\/working\/%05d.png'%(di*100+b), result)\n#                             cv2.waitKey(1)\n                            pass\n\n\n\n        pass  #-- end of one data loader --\n    pass #-- end of all iterations --","6da163b9":"print('rate     iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           ')\nprint('--------------------------------------------------------------------------------------------------------------------\\n')\nrun_train()","5ca9b71d":"**Heng's Starter code for training the classification model on Kaggle.**\n\nI have not run the kernel with GPU enabled because I do not have much of Kaggle GPU left as of now. So the kernel as expected is giving CUDA error.\nThis is just a simple kernel for training model on Kaggle easily. Made some minor changes to his code and seems like it will run fine here on kaggle.\nI have not tested the training time. It can exceed the 9 hour limit.\n\nThe kernel is based on Heng's starter kit version 20190910 you can find it [here](https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/discussion\/106462#latest-645576) .\nI have imported 2 utility scripts one for the utility functions with plotting code and another one is for model.\nYou can fork and edit the utility scripts and add the model classes as you feel like.\nThe model architecture can be changed from this kernel below by changing the Net() class.\n\nIf you face any problems or errors then feel free to comment them.\nAt last thank you very much [Heng](https:\/\/www.kaggle.com\/hengck23) and other leaderboard rankers for helping newbies like me."}}