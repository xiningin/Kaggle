{"cell_type":{"970ab5f0":"code","8bc56fda":"code","6faaae57":"code","cd259ed7":"code","423e206c":"code","5152fe55":"code","c905314e":"code","4382e281":"code","8d25eebc":"code","8d892332":"code","e9ff801f":"code","282122b1":"code","1aa8350c":"code","ecc48e73":"code","01e66f40":"code","6a2d4700":"code","3f471ca0":"code","1a6aca84":"code","da6131b9":"code","5523972d":"markdown","04a2aa78":"markdown","07012c98":"markdown","ed3d0b26":"markdown","b28e358e":"markdown","dc038d46":"markdown","df275644":"markdown","8c9d0e1a":"markdown","238cd755":"markdown"},"source":{"970ab5f0":"#data preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv('..\/input\/salary-data\/Salary_Data.csv')\nx = data.iloc[:, :-1].values\ny = data.iloc[:, 1].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 1\/3, random_state = 42)\n\n#No need to do feature scaling, as the library automatically takes care of this.\n\n#fitting regressor\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, Y_train)\n\n","8bc56fda":"#predicting the test set results\ny_pred = regressor.predict(X_test) #vector of all predictions of the dependent variable\ny_pred","6faaae57":"#visualize the training set results\nplt.scatter(X_train, Y_train, color = 'red')\nplt.plot(X_train, regressor.predict(X_train), color = 'blue')\nplt.title(\"years vs salary\")\nplt.xlabel(\"number of years\")\nplt.ylabel(\"salary (dollars)\")\nplt.show()","cd259ed7":"#visualizing test set results\nplt.scatter(X_test, Y_test, color = 'red')\nplt.plot(X_train, regressor.predict(X_train), color = 'blue') #this should not change as the regressor fitted to train set should be shown\nplt.title(\"years vs salary\")\nplt.xlabel(\"number of years\")\nplt.ylabel(\"salary (dollars)\")\nplt.show()","423e206c":"data2 = pd.read_csv('..\/input\/m-50-startups\/50_Startups.csv')\nx = data2.iloc[:, :-1]\ny = data2.iloc[:, 4]\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\nxtemp = x.iloc[:, 3]\nlabelencoder = LabelEncoder()\nxtemp = labelencoder.fit_transform(xtemp)\nxtemp = pd.DataFrame(to_categorical(xtemp))\n\nx = x.drop(['State'], axis = 1) #In pandas axis = 1 --> column\nx = pd.concat([x, xtemp], axis = 1)\n\n# x = x.iloc[:, :-1] This is to avoid dummay variable trap. However, the library already takes care of this, so no need.\n\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\n","5152fe55":"#Linear Regression\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, Y_train)\n\ny_pred = regressor.predict(X_test)\ny_pred, Y_test\n\nprint(x.shape[1])","c905314e":"#Find an optimal team of independent variables, so that each variable has significant impact on the prediction. \n# --> Backward elimination\n\nimport statsmodels.formula.api as sm\nx = np.array(x) #use numpy arrays instead of DataFrames for more useful functions. DataFrames are useful for preparing dataset\nx = np.append(np.ones((x.shape[0], 1), dtype = 'int'), x, axis = 1) #(x.shape[0], 1).astype(int) does not work\n#Above is done to add constant to the model, which is necessary for Ordinary Least Squares to work","4382e281":"x_opt = x[:, [0,1,2,3,4,5]] #needs to specify all the indexes, so that individual index is evaluated.\nregressor_OLS = sm.OLS(endog = y, exog = x_opt).fit()\nregressor_OLS.summary() #shows statistical summary","8d25eebc":"'''The significance value is set to 0.05 If P-value is lower than this, \nthen it is significant. If higher, it is less significant. Therefore, variables\nwith higher P_values need to be removed, as they do not have large impact. \nThis is called backward elimination. \n\nIn this case, as x4 has 0.990, it needs to be removed'''\n\nx_opt = x[:, [0,1,2,3,4,5]] #needs to specify all the indexes, so that individual index is evaluated.\nregressor_OLS = sm.OLS(endog = y, exog = x_opt).fit()\nregressor_OLS.summary() #shows statistical summary\n","8d892332":"\n\n\n#repeat the step --> remove the insignificant variable, fit it, repeat it.\n\nx_opt = x[:, [0,1,3,5]] #needs to specify all the indexes, so that individual index is evaluated.\nregressor_OLS = sm.OLS(endog = y, exog = x_opt).fit()\nregressor_OLS.summary() #shows statistical summary","e9ff801f":"x_opt = x[:, [0,1,3]] #needs to specify all the indexes, so that individual index is evaluated.\nregressor_OLS = sm.OLS(endog = y, exog = x_opt).fit()\nregressor_OLS.summary() #shows statistical summary","282122b1":"#To check if the team of variables are correct\nx_show = pd.DataFrame(x)\n# x_show\n\n#Therefore, only using these variables ","1aa8350c":"data3 = pd.read_csv('..\/input\/polynomial-position-salary-data\/Position_Salaries.csv')\nx = data3.iloc[:, 1:2].values #1:2 is done instead of only 1, because independent variable should be a matrix.\ny = data3.iloc[:, 2].values\n\n# X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n# Use whole dataset to train\n\ndata3\nplt.scatter(data3.iloc[:, 1], y)\nplt.title('Level vs Salary')\nplt.xlabel(\"Level\")\nplt.ylabel(\"Salary (dollrs)\")\nplt.show()","ecc48e73":"from sklearn.preprocessing import PolynomialFeatures\n#transforms the x matrix into a new matrix that has x1, x2, x3 --- columns\npoly_reg = PolynomialFeatures(degree = 2) #specify the degree -> how many terms\nx_poly = poly_reg.fit_transform(x)\nx_poly","01e66f40":"lin_reg = LinearRegression()\nlin_reg.fit(x_poly, y)\n\ny_pred = lin_reg.predict(poly_reg.fit_transform(x)) #this is used instead of x_poly, so that this model will work for any matrix input x \n\nplt.figure(2)\nplt.scatter(x, y)\nplt.plot(x, y_pred)\nplt.show()","6a2d4700":"#improving the model --> add degrees to make it more complex\n\npoly_reg = PolynomialFeatures(degree = 4) #specify the degree -> how many terms\nx_poly = poly_reg.fit_transform(x)\nx_poly\n\nlin_reg = LinearRegression()\nlin_reg.fit(x_poly, y)\n\ny_pred = lin_reg.predict(poly_reg.fit_transform(x)) #this is used instead of x_poly, so that this model will work for any matrix input x \n\n#This is to get a more continuous curve, by plotting more x values.\nx_grid = np.arange(min(x), max(x), 0.1) #0.1 --> increment \nx_grid = x_grid.reshape(x_grid.shape[0], 1)\n\nplt.figure(2)\nplt.scatter(x, y)\nplt.plot(x_grid, lin_reg.predict(poly_reg.fit_transform(x_grid)))\nplt.show()","3f471ca0":"data3 = pd.read_csv('..\/input\/polynomial-position-salary-data\/Position_Salaries.csv')\nx = data3.iloc[:, 1:2].values #1:2 is done instead of only 1, because independent variable should be a matrix.\ny = data3.iloc[:, 2].values\ny = y.reshape(y.shape[0], 1)\nx = x.reshape(x.shape[0], 1)\n\nprint(y.shape)\n# X_train, X_test, Y_train, Y_test = train_test_split()\n\n#svr does not have feature scaling built in\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nsc_y = StandardScaler()\nx = sc_x.fit_transform(x)\ny = sc_y.fit_transform(y)\n\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(x, y)\n\ny_pred = sc_y.inverse_transform(regressor.predict(sc_x.transform(np.array([[6.5]])))) #input scaled value, then inverse scale the predicted value\ny_pred","1a6aca84":"data3 = pd.read_csv('..\/input\/polynomial-position-salary-data\/Position_Salaries.csv')\nx = data3.iloc[:, 1:2].values #1:2 is done instead of only 1, because independent variable should be a matrix.\ny = data3.iloc[:, 2].values\n\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(x, y)\n\ny_pred = regressor.predict(np.array([[6.5]]))\n\nx_grid = np.arange(min(x), max(x), 0.01)\nx_grid = x_grid.reshape(len(x_grid), 1)\nplt.figure(3)\nplt.plot(x_grid, regressor.predict(x_grid))\nplt.show()\n\n#Notice how average is used to represent each interval.","da6131b9":"data3 = pd.read_csv('..\/input\/polynomial-position-salary-data\/Position_Salaries.csv')\nx = data3.iloc[:, 1:2].values #1:2 is done instead of only 1, because independent variable should be a matrix.\ny = data3.iloc[:, 2].values\n\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 5000, criterion = 'mse', random_state = 0) #can tune the n_estimators\nregressor.fit(x, y)\n\ny_pred = regressor.predict([[6.5]])\n\nx_grid = np.arange(min(x), max(x), 0.01)\nx_grid = x_grid.reshape(len(x_grid), 1)\nplt.plot(x_grid, regressor.predict(x_grid))\nplt.show()\nprint(y_pred)","5523972d":"![i10](https:\/\/i.imgur.com\/CdxakvJ.png)\nTake the average of each leaf, and assign that value to any coordinate that falls under any leaf.","04a2aa78":"Created by: Sangwook Cheon\n\nDate: Dec 23, 2018\n\nThis is step-by-step guide to Regression using scikit-learn, which I created for reference. I added some useful notes along the way to clarify things. I am excited to move onto more advanced concepts, such as deep learning, using frameworks like Keras and Tensorflow.\nThis notebook's content is from A-Z Datascience course, and I hope this will be useful to those who want to review materials covered, or anyone who wants to learn about the basics of regression.\n\n# Content:\n\n### 1. Simple Linear Regression\n### 2. Multiple Linear Regression\n### 3. Polynomial Regression\n### 4. Supper Vector Regression (SVR)\n### 5. Decision Tree Regression\n### 6. Random Forest Regression\n### 7. R-squared\/Adjusted R-squared\n_______________________________________________________\n_______\n\n# Simple Linear Regression  \n\n![i7](https:\/\/i.imgur.com\/LEgEZqA.png)","07012c98":"# Multiple linear regression\n![i8](https:\/\/i.imgur.com\/1YjjMvT.png)\n\n## Dummy variables\nNever include all dummy variable columns. Always omit one column (ex: if 9 columns, include 8), because including all leads to\n--> Dummy variable trap, which will disrupt the learning of the machine learning model.\n\n## 5 methods of building models:\n\n**1. All-in**\n\n*     \u2022 Putting all the variables into the equation.\n*     \u2022 Prepare the Backward Elimination\n\n**2. Backward Elimination**\n\n*     \u2022 Step 1: Select a significant level to stay in the model \n*     \u2022 Step 2: Fit the full model with all possible predictors\n*     \u2022 Step 3: Consider the predictor with the highest P-value. If P > Significance level, go to Step 4, otherwise go to FIN\n*     \u2022 Step 4: Remove the predictor\n*     \u2022 Step 5: Fit model without this variable. Go back to Step 3\n\n**3. Forward Selection**\n\n*     \u2022 Step 1: Select a significant level to enter the model \n*     \u2022 Step 2: Fit all simple regression models y ~ xn  Select the one with the lowest P-value.\n*     \u2022 Step 3: Keep this variavle and fit all possible models with one extra predictor added to the one(s) you already have\n*     \u2022 Step 4: Consider the predictor with the lowest P-value. If P > SL, go to STEP 3, other waise go to FIN\n\n**4. Bidirectional Elimination**\n\n*     \u2022 Step 1: Select a significance level to enter and to stay in the model (e.g: SLENTER = 0.05, SLSTAY = 0.05\n*     \u2022 Step 2: Perform the next step of Forward Selection (new variables must have P < SLENTER to enter)\n*     \u2022 Step 3: Perform ALL steps of Backward Elimination (old variables must have P < SLSTAY to stay)\n*     \u2022 Step 4: No new variables can enter and no old variables can exit. Until this happens,  repeat Step 2 and 3.\n\n**5. All Possible Models**\n\n*     \u2022 Step 1: Select a criterian of goodness of fit \n*     \u2022 Step 2: Construct All possible regression models 2^n - 1 total combinations\n*     \u2022 Step 3: Select the one with the best criterion.\n","ed3d0b26":"# Polynomial regression\ny = b0 + b1x1 + b2x1^2 --- bnx1^n","b28e358e":"# Support Vector Regression (SVR)\n![i8](https:\/\/i.imgur.com\/QDhMroy.png)","dc038d46":"# Random Forest Regression\n\n* Step 1: Pick at random K points from the training set\n* Step 2: Build the Decision Tree associated to these K points\n* Step 3: Choose the number of Decision trees to build, and repeat step 1 and 2\n* Step 4: For a new data point, make each Decision tree output a prediction, and assign the average of these values.\n\n## Forest --> A team of trees","df275644":"# R-Squared\n\nr^2 = 1 - SSres \/SStot\n\nSSres = SUM (yi - yihat)^2 --> Sum of residuals\n\nSStot = SUM (yi - yavg)^2 \n\nFind a best line that minizes R^2, and make it best compared to the average line. \n\n**Closer to 1, better. If smaller, worse**\n\n![i11](https:\/\/i.imgur.com\/4KGJmle.png)\n\n# Adjusted R-Squared\n\nThis takes care of adding non-meaningful regressors. It penalizes the model for adding a variable.  This metric can be used to assess the model accurately.\n![i12](https:\/\/i.imgur.com\/bqZZu05.png)\n\n## When doing backward elimination, check Adjusted R-Squared to see if removing a variable is beneficial to the model. If the Adjusted R-Squared grows, then removing it is a good idea.\n\n\tcoef\tstd err\tt\tP>|t|\t[0.025\t0.975]\nconst\t4.698e+04\t2689.933\t17.464\t0.000\t4.16e+04\t5.24e+04\nx1\t0.7966\t0.041\t19.266\t0.000\t0.713\t0.880\nx2\t0.0299\t0.016\t1.927\t0.060\t-0.001\t0.061\n\nWhen interpreting these coeffients, which is shown in the Statsmodel library, needs to be careful about units.\n\nThe coefficient part of the table shows how much impact a variable has on the independent variable **per unit**. If the variables are of same unit, then they can be compared, but if they are not, it is only valid to say \"one has more impact then the other per unit\"","8c9d0e1a":"In SVR, the objective is to make sure errors do not exceed the threshold, while in linear regression it is to minimize the error between prediction and data.","238cd755":"# Decision tree Regression\n![i9](https:\/\/i.imgur.com\/JZkzrXt.png)\n\nInformation Entropy --> tries to find an optimal way to split the dataset into leaves (each section is called a leaf)."}}