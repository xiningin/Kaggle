{"cell_type":{"ee31f1b1":"code","71ceccc5":"code","9bbb2b3c":"code","c0726508":"code","81a13693":"code","e7be8809":"code","18cb6aa5":"code","7bf10fa9":"code","2e89e193":"code","6b8e9838":"code","725d10c8":"code","dc5c5f6f":"code","a284c5c9":"code","adb59d37":"code","cd84558e":"code","d06f2b06":"code","27cb74ab":"code","d6a15ac3":"code","58310f69":"code","493db535":"code","2df52952":"code","a4b4c488":"code","f6e336ad":"code","a0927576":"code","aaad74a5":"code","da2bb955":"code","03ba3ee8":"code","c6fd53c9":"code","4b3b58e4":"code","2ea21c32":"code","0ac48a01":"code","5125935a":"code","f574c152":"markdown","fa55c5c9":"markdown","d3e3f20e":"markdown","a33e5e54":"markdown","cd26689c":"markdown","d65442f9":"markdown","70fa00ce":"markdown","837301ed":"markdown","2806c015":"markdown","c63f77ae":"markdown","5c4cf4c4":"markdown","d6092bab":"markdown","9dc340d5":"markdown","bd710ec9":"markdown","945ce2e5":"markdown","d2b3e335":"markdown","f5cb7ede":"markdown","c1fb91e9":"markdown","3fa86ddf":"markdown","02635eec":"markdown","aa52ee46":"markdown","addd9fb5":"markdown"},"source":{"ee31f1b1":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nSEED=23    #seed value for setting random state of classifiers","71ceccc5":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')","9bbb2b3c":"train_data.head()","c0726508":"train_data.info()","81a13693":"#Passenger Class\ntrain_data[['Pclass', 'Survived']].groupby('Pclass').Survived.mean()  \n#will give percentage of 1's (i.e. Survived) ","e7be8809":"#Sex\ntrain_data[['Sex', 'Survived']].groupby('Sex').Survived.mean()","18cb6aa5":"#Comparing Passenger class and Cabin status\n\nprint(train_data[train_data.Cabin.notnull()]\n      .groupby('Pclass')\n      .Pclass.count())                     #cabin number known","7bf10fa9":"print(train_data[train_data.Cabin.isnull()]\n      .groupby('Pclass')\n      .Pclass.count())                     #cabin number unknown","2e89e193":"#Modifying cabin column\ntrain_data.loc[train_data['Cabin'].notnull(), 'Cabin'] = 1\ntrain_data.loc[train_data['Cabin'].isnull(), 'Cabin'] = 0\ntrain_data.Cabin = train_data.Cabin.astype('int8')\ntrain_data.head()","6b8e9838":"#Creating NumFam and HasFam columns\ntrain_data['NumFam'] = train_data.SibSp + train_data.Parch #number of family members with them\ntrain_data['HasFam'] = train_data.NumFam > 0\ntrain_data.head()","725d10c8":"train_data.isna().any()  #checking which columns have null values","dc5c5f6f":"train_data[['Age', 'Embarked']].info()","a284c5c9":"#imputing 'Embarked' column\ntrain_data.Embarked.value_counts()","adb59d37":"#since 'S' (which stands for Southampton, England) is the most frequent value by a large margin, \n#we will fill that in for the missing values\ntrain_data.Embarked.fillna('S', inplace=True)\ntrain_data.Embarked.isna().any()","cd84558e":"#missing age will be filled with a random value ranging between M-S and M+S, \n#where M is the mean and S is the standard deviation of the available age values\n\nmean_age = train_data.Age.mean()\nstd_age = train_data.Age.std()\nnull_age_count = train_data.Age.isna().sum()\nprint(f'Mean age: {mean_age :.2f}, Standard deviation: {std_age :.2f}')\n\nrng = np.random.default_rng(seed=SEED)\nrng_ages = rng.integers(low = mean_age - std_age, high = mean_age + std_age + 1, size = null_age_count)\nprint(f'rng_ages sample: {rng_ages[:10]}, range: {mean_age - std_age :.2f} to {mean_age + std_age :.2f}')\n\ntrain_data.loc[train_data.Age.isna(), 'Age'] = rng_ages","d06f2b06":"print(f'Total age range: {train_data.Age.min()} to {train_data.Age.max()}')\n#oldest person is 80 years old, but the test data might have an older person, so we will set custom ranges for dividing the age\nbins = pd.IntervalIndex.from_tuples([(0, 18), (18, 36), (36,64), (64, 100)])\ntrain_data['AgeRange'] = pd.cut(train_data.Age, bins, ordered=True)\ntrain_data.AgeRange.value_counts()","27cb74ab":"train_data.head()","d6a15ac3":"#Converting categorical columns to numerical values for model-fitting\ntrain_data.HasFam = train_data.HasFam.astype('category').cat.codes  #1-has family, 0-alone\ntrain_data.Sex = train_data.Sex.astype('category').cat.codes  #1-male, 0-female\ntrain_data.Embarked = train_data.Embarked.astype('category').cat.codes  #C-0, Q-1, S-2\ntrain_data.AgeRange = train_data.AgeRange.astype('category').cat.codes  #0 to 3 for (0-18] to (64-100]\ntrain_data.head(10)","58310f69":"X = train_data.dropna(axis=0, subset=['Survived']) #Dropping rows with target value missing\ny = train_data.Survived #separating out the target column\n\n#removing the target and redundant features from training data\nX.drop(['Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare'], axis=1, inplace=True)","493db535":"X.head()","2df52952":"classifiers = [\n    LogisticRegression(solver='liblinear', multi_class='ovr', random_state=SEED),\n    SVC(random_state=SEED),\n    CategoricalNB(alpha=0.1),\n    RandomForestClassifier(n_estimators=150, max_depth=5, random_state=SEED),\n    GradientBoostingClassifier(n_estimators=150, learning_rate=1.0, max_depth=1, n_iter_no_change=5, random_state=SEED)\n]\n\nfor clf in classifiers:\n    print(f'{clf.__class__.__name__}: ',)\n    scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')  #5-fold cross-validation\n    print(f'Average accuracy: {(scores.mean() * 100): .4f}%\\n')","a4b4c488":"test_data.info()","f6e336ad":"#could have done this for training data too, but looking through the data \n#and transforming it step-by-step helped me understand the whole process\n\ndef transform(df):\n    \n    #Cabin numbers -> HasCabin status\n    df.loc[df['Cabin'].notnull(), 'Cabin'] = 1\n    df.loc[df['Cabin'].isnull(), 'Cabin'] = 0\n    df.Cabin = df.Cabin.astype('int8')\n    \n    #Family features\n    df['NumFam'] = df.SibSp + df.Parch\n    df['HasFam'] = df.NumFam > 0\n    \n    df.Embarked.fillna('S', inplace=True) #not required for test data\n    \n    #Imputing age values\n    mean_age = df.Age.mean()\n    std_age = df.Age.std()\n    null_age_count = df.Age.isna().sum()\n    rng = np.random.default_rng(seed=SEED)\n    rng_ages = rng.integers(low = mean_age - std_age, high = mean_age + std_age + 1, size = null_age_count)\n    df.loc[df.Age.isna(), 'Age'] = rng_ages\n    \n    #Age -> AgeRange\n    bins = pd.IntervalIndex.from_tuples([(0, 18), (18, 36), (36,64), (64, 100)])\n    df['AgeRange'] = pd.cut(df.Age, bins, ordered=True)\n    \n    #Converting categorical columns to numerical values for model-fitting\n    df.HasFam = df.HasFam.astype('category').cat.codes\n    df.Sex = df.Sex.astype('category').cat.codes\n    df.Embarked = df.Embarked.astype('category').cat.codes\n    df.AgeRange = df.AgeRange.astype('category').cat.codes\n    \n    df.drop(['Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare'], axis=1, inplace=True)\n    \n    return df\n","a0927576":"X_test = transform(test_data)\nX_test.head()","aaad74a5":"#fitting the classifiers to the training data\n#using test data for predictions\n#storing the predictions in a dataframe for voting procedure\n\npreds_df = pd.DataFrame(index=X_test.index)\n\nfor clf in classifiers:\n    clf.fit(X, y)\n    preds = clf.predict(X_test)\n    preds_df[clf.__class__.__name__] = preds\n","da2bb955":"preds_df.rename(columns={'LogisticRegression': 'LR',\n                        'CategoricalNB': 'CNB',\n                        'RandomForestClassifier': 'RFC',\n                        'GradientBoostingClassifier': 'GBC'},\n               inplace=True)\n\npreds_df.head(5)","03ba3ee8":"preds_df.corr()","c6fd53c9":"preds_df['majority5'] = preds_df.sum(axis=1) \/\/ 3  #majority of 3 out of 5\npreds_df.head()","4b3b58e4":"preds_df['majority3'] = preds_df[['SVC', 'RFC', 'GBC']].sum(axis=1) \/\/ 2  #majority of 2 out of 3\npreds_df.head()","2ea21c32":"preds_df['SVCplusGBC'] = preds_df[['SVC', 'GBC']].sum(axis=1)\npreds_df.loc[preds_df.SVCplusGBC == 2, 'SVCplusGBC'] \/\/= 2\npreds_df.head()","0ac48a01":"preds_df['SVCplusGBCplusRFC'] = preds_df[['SVC', 'GBC', 'RFC']].sum(axis=1)\npreds_df.loc[preds_df.SVCplusGBCplusRFC > 1, 'SVCplusGBCplusRFC'] \/\/= 2\npreds_df.head()","5125935a":"output = pd.DataFrame({'PassengerId': test_data.index, 'Survived' : preds_df.SVCplusGBCplusRFC})\noutput.to_csv('submission.csv', index=False)","f574c152":"A high percentage of 1st class passengers had cabins alloted to them. The remaining may or may not have cabins.  \nA very high percentage of 2nd and 3rd class passengers do not have a cabin number associated with them, which confirms the assumption that the majority of missing cabin values are because cabins were not alloted to these passengers. Thus, we can replace cabin numbers with categorical values to complement the passenger class.","fa55c5c9":"As expected, females have a very high survival rate compared to males.","d3e3f20e":"**Approach 1.1 - Majority voting with the top 3 performers - SVM, RFC, GBC**","a33e5e54":"**Handling missing values**","cd26689c":"**Approach 2.1: SVM + GBC + RFC**","d65442f9":"**Investigating importance of some of the original features**","70fa00ce":"**Generating predictions**  \nFirst we need to process the test data according to the transformations we applied to the training data.\n","837301ed":"**Approach 1: Majority-voting**","2806c015":"**Approach 2: SVM + GBC**  \nSVM and GBC performed best on the test set solo, we will accept that a passenger survived if either of them predict it.","c63f77ae":"Except CategoricalNB, the other four models are highly correlated. However, there is still some variation which may help us correct some inaccurate predictions by voting.","5c4cf4c4":"**Time to submit!**","d6092bab":"**Creating new features based on our intuition, and transforming some of the original ones to investigate them.**","9dc340d5":"* Embarked - only 2 missing values. We can fill in the most frequent value for the column without a significant effect on our models.\n* Age - 177 missing values, a significant number. But we know the feature is important so we don't want to lose that information by dropping the column. Two approaches are possible:\n   1. removing rows with missing values (again, some loss of information)\n   2. filling in missing values with some estimate of age (such as mean or median)  \n   Using both the approaches is viable by creating two different training sets.  \n   However, if age is missing in the test data, we cannot simply remove those rows. Since imputing the values is inevitable, we will go only for the second approach.","bd710ec9":"Converting Age to AgeRange (a categorical variable):\n* will separate the passengers into stages of human development (i.e., child, young adult, middle-aged, old-aged), which is more relevant compared to their exact age. For instance, children were given preference over adults while boarding lifeboats, but a 3-year old and a 12-year old would not be distinguished based on age.\n* will automatically handle outliers such as newborns and very old people, by making them part of a group\n* we do not want our models to learn based on the exact age since we have imputed a lot of values","945ce2e5":"**Results:** \n* SVM, GBC, SVM+GBC and SVM+GBC+RFC performed the best on the test set - 78.468%\n* RFC and majority-voting with top-3 performed slightly worse - 78.229%\n* Majority-voting (with all five classifiers) did not live up to my expectations - 77.99%\n\n**TODO:**\n* Implement One vs Rest voting mechanism\n* Explore other classifiers\n* Incorporate visualizations in data exploration and results\n* Clean up code using Pipelines","d2b3e335":"**Applying the concepts learned in the beginner Kaggle courses and borrowing from some excellent resources:**\n1. [https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier\/notebook](http:\/\/)\n2. [https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python](http:\/\/)\n3. [https:\/\/mlwave.com\/kaggle-ensembling-guide\/](http:\/\/)\n\n**Highlights:**\n* Basic data exploration and feature engineering\n* Model-fitting (SVM, Random forest, Gradient boosting)\n* Voting-based classifier to reduce error from individual models. ","f5cb7ede":"**All set for model fitting!**","c1fb91e9":"Looks like high-class passengers had a higher chance of surviving due to their location and the help\/preference given to them.","3fa86ddf":"Looks like not all predictions are unanimous.  \nWe can check how correlated the models are in their predictions.","02635eec":"**Some preliminary observations** from the data description and the head( ) & info( ) functions (with some previous knowledge of the actual event):\n1. SibSp (Sibling-Spouse) and Parch(Parent-Children) indicate whether a passenger is travelling with family. In case of a family, an adult would be travelling with their spouse and children, while a child would have siblings and parents. It seems best to combine the two for a new column having the total family size and a column indicating family\/alone status.\n2. Women and children were boarded first on the life boats during the incident. While the 'Sex' column has all the entries, 'Age' has some missing values. But it would be better to fill them somehow instead of dropping the column, since it seems like an important feature.\n3. 'Cabin' column has many missing values. Apparently 3rd class passengers were not given individual cabins and were located towards the bottom of the ship in large numbers, which hindered their chances of making it to safety. Thus, it feels like a feature worth investigating.","aa52ee46":"**Columns that we will not be considering in our feature set:**\n* Name - the title of a person can be extracted from the name, as has been done in the resources I referred. But that information is captured by the 'Sex', 'Age' and 'HasFam' columns. Rare titles (such as military ranks) will also not help generalize the models. Thus, I have chosen to drop the name column.\n* Fare - again, the information is captured by Pclass, Cabin and Embarked columns. 1st class passenger having cabins paid 8 to 10 times the fare paid by 3rd class passengers with no cabins.\n* Ticket - a string which does not capture any information related to the passenger on board","addd9fb5":"The balanced performance of the classifiers indicates that they might be correlated in their predictions. But we will still attempt to improve our accuracy by incorporating voting mechanisms."}}