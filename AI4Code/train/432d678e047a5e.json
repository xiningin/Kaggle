{"cell_type":{"cfad8f2a":"code","c33274c0":"code","d9a67c53":"code","c5eb756f":"code","4b501b32":"code","f54c4837":"code","9cfb90e5":"code","321a4b23":"code","8d906231":"code","c4bac028":"code","4fc81cd2":"code","e4c689ce":"code","4143f62e":"code","5a71d311":"code","9d5066d8":"code","179af089":"code","337d31e3":"code","a15f0f60":"code","91200248":"code","3e4c24d5":"code","ea8e264e":"markdown","43c682e0":"markdown","ff3d3014":"markdown","e9d0d7f8":"markdown","ba6290a6":"markdown","f8abdec6":"markdown","76c4479e":"markdown","46b24c68":"markdown","b26def9d":"markdown","aab4c37c":"markdown","935d1030":"markdown","7b5a5e5e":"markdown","c55a1c47":"markdown"},"source":{"cfad8f2a":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport seaborn as sns\nimport transformers\n\nimport nltk\nimport re\n\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nplt.style.use('seaborn')","c33274c0":"print(tf.__version__)\nprint(tf.config.list_physical_devices('GPU'))","d9a67c53":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c5eb756f":"dt= pd.read_excel('..\/input\/tunisian-texts\/tun.xlsx',header=0)","4b501b32":"dt","f54c4837":"def clean_text(text):\n    #Remove emojis and special chars\n    clean=text\n    reg = re.compile('\\\\.+?(?=\\B|$)')\n    clean = text.apply(lambda r: re.sub(reg, string=r, repl=''))\n    reg = re.compile('\\x89\u00db_')\n    clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n    reg = re.compile('\\&amp')\n    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='&'))\n    reg = re.compile('\\\\n')\n    clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n\n    #Remove hashtag symbol (#)\n    clean = clean.apply(lambda r: r.replace('#', ''))\n\n    #Remove user names\n    reg = re.compile('@[a-zA-Z0-9\\_]+')\n    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='@'))\n\n    #Remove URLs\n    reg = re.compile('https?\\S+(?=\\s|$)')\n    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='www'))\n\n    #Lowercase\n    clean = clean.apply(lambda r: r.lower())\n    return clean","9cfb90e5":"dt['clean'] = clean_text(dt['texts'])","321a4b23":"from transformers import TFXLNetModel, XLNetTokenizer","8d906231":"xlnet_model = 'xlnet-large-cased'\nxlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)","c4bac028":"def create_xlnet(mname):\n    \"\"\" Creates the model. It is composed of the XLNet main block and then\n    a classification head its added\n    \"\"\"\n    # Define token ids as inputs\n    word_inputs = tf.keras.Input(shape=(120,), name='word_inputs', dtype='int32')\n\n    # Call XLNet model\n    xlnet = TFXLNetModel.from_pretrained(mname)\n    xlnet_encodings = xlnet(word_inputs)[0]\n\n    # CLASSIFICATION HEAD \n    # Collect last step from last hidden state (CLS)\n    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n    # Apply dropout for regularization\n    doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)\n    # Final output \n    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(doc_encoding)\n\n    # Compile model\n    model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), ])\n\n    return model","4fc81cd2":"xlnet = create_xlnet(xlnet_model)","e4c689ce":"xlnet.summary()","4143f62e":"tweets = dt['clean']\nlabels = dt['data_labels']\n\nX_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.15, random_state=196)","5a71d311":"def get_inputs(tweets, tokenizer, max_len=120):\n    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in tweets]\n    inp_tok = np.array([a['input_ids'] for a in inps])\n    ids = np.array([a['attention_mask'] for a in inps])\n    segments = np.array([a['token_type_ids'] for a in inps])\n    return inp_tok, ids, segments\n\ndef warmup(epoch, lr):\n    \"\"\"Used for increasing the learning rate slowly, this tends to achieve better convergence.\n    However, as we are finetuning for few epoch it's not crucial.\n    \"\"\"\n    return max(lr +1e-6, 2e-5)\n\ndef plot_metrics(pred, true_labels):\n    \"\"\"Plots a ROC curve with the accuracy and the AUC\"\"\"\n    acc = accuracy_score(true_labels, np.array(pred.flatten() >= .5, dtype='int'))\n    fpr, tpr, thresholds = roc_curve(true_labels, pred)\n    auc = roc_auc_score(true_labels, pred)\n\n    fig, ax = plt.subplots(1, figsize=(8,8))\n    ax.plot(fpr, tpr, color='red')\n    ax.plot([0,1], [0,1], color='black', linestyle='--')\n    ax.set_title(f\"AUC: {auc}\\nACC: {acc}\");\n    return fig","9d5066d8":"inp_tok, ids, segments = get_inputs(X_train, xlnet_tokenizer)","179af089":"callbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, min_delta=0.02, restore_best_weights=True),\n    tf.keras.callbacks.LearningRateScheduler(warmup, verbose=0),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=1e-6, patience=2, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-6)\n]","337d31e3":"hist = xlnet.fit(x=inp_tok, y=y_train, epochs=1, batch_size=16, validation_split=.15, callbacks=callbacks)","a15f0f60":"text=['\u0627\u0644\u0645\u0646\u0648\u062c \u0628\u0627\u0647\u064a \u0628\u0631\u0634\u0627 \u0648\u0645\u062d\u0644\u0627\u0647']\ninp_tok, ids, segments = get_inputs(text, xlnet_tokenizer)\npreds = xlnet.predict(inp_tok, verbose=True)","91200248":"preds","3e4c24d5":"text=['\u0643\u0627\u0646\u062a \u0627\u0644\u062e\u062f\u0645\u0629 super!']\ninp_tok, ids, segments = get_inputs(text, xlnet_tokenizer)\npreds = xlnet.predict(inp_tok, verbose=True)\npreds","ea8e264e":"I used one epoch but ideally you can try 4","43c682e0":"** Text cleanning: **\n- remove strange characters\n- remove URLs (they doesn't tell us pretty much)\n- replace usernames for \"@\" character\n- remove line breaks","ff3d3014":"### Training","e9d0d7f8":"1. Import libraries","ba6290a6":"Clean and split the data","f8abdec6":"Hii everyone this is my first notebook please show your support and let me know if you have remarques or questions.\nEnjoy !!!! XD\n","76c4479e":"In this tutorial, I'll show you how to finetune the pretrained XLNet model with the huggingface library in order to achieve a sentiment analysis on Tunisian dataset already available on Kaggle.\n\nIn fact, Hugging Face initially supported only PyTorch, but now TF 2.0 is also well supported.\n\nFollowing is a general pipeline for any transformer model:\n**Tokenizer definition \u2192Tokenization of Documents \u2192Model Definition \u2192Model Training \u2192Inference**\n","46b24c68":"Next step is now to perform tokenization on documents. It can be performed either by encode() or encode_plus() method.\n\nXLNet requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n\n**input ids**: a sequence of integers identifying each input token to its index number in the XLNet tokenizer vocabulary\n\n**segment mask**: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long.(for two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence)\n\n**attention mask**: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we'll detail this in the next paragraph)\n\n**max_length**: Max length of any sentence to tokenize, its a hyperparameter. (originally BERT has 512 max length)\n\n**pad_to_max_length**: perform padding operation.\n\n![](http:\/\/)**add_special_tokens**: used to add special character like < cls >, < sep >,< unk >, etc","b26def9d":"**Tokenizer Definition**\n\nEvery transformer based model has a unique tokenization technique, unique use of special tokens. The transformer library takes care of this for us. It supports tokenization for every model which is associated with it.","aab4c37c":"This is the link to the dataset https:\/\/www.kaggle.com\/naim99\/tunisian-texts ","935d1030":"We are going to fine-tune a Pretrained transformer model on custom dataset.","7b5a5e5e":"Create the input data (tensors)","c55a1c47":"### Testing"}}