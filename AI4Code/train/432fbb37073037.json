{"cell_type":{"e3a8b102":"code","e454e8ef":"code","11d443cb":"code","cdc88759":"code","72de75a6":"code","9388b9aa":"code","6b2fabce":"code","4af5ecda":"code","b8c874d3":"code","f7275b9f":"code","7902c763":"code","65bead35":"code","863983e3":"code","67857604":"code","8a654dd7":"code","8da318c5":"code","557ca687":"code","0f629d79":"code","f07f44fd":"code","0e83a1f1":"code","e120a0a3":"code","f28672f1":"code","3cddd93a":"code","7b22f5d4":"code","e66b4191":"code","b9ce302b":"code","26de9eb2":"code","ac9f7d83":"code","78763d70":"code","93c51c86":"code","8b741e85":"code","e2e03514":"code","af721fe0":"code","d19bc5b6":"code","baf7e53b":"code","495fa3b1":"code","d4e78839":"code","e67aa10a":"code","bd6a6a3a":"code","1d859dfb":"code","606874c1":"code","300060b5":"code","f745a14d":"code","9c4aeea9":"code","17f01e2e":"code","ccf6e666":"code","8071214a":"code","fb38955f":"code","5b5e05e3":"code","19d3d6e6":"code","ee1feef1":"code","baecb09c":"code","e1012618":"code","36c12177":"code","dd432425":"code","52467e00":"code","31de2299":"code","a6e5666d":"code","e3992da4":"code","b3bddfe5":"code","eeeb93a4":"code","e673d7e4":"code","6d69c981":"code","2864ace5":"code","2f6466ee":"code","13ae48ab":"code","50e58f0a":"code","17df6068":"code","f6356e42":"code","b7f984fa":"code","8f2a37f6":"code","73eda3f7":"code","fd3d8b32":"code","78712d88":"code","b4ad0fb9":"code","60d653e8":"code","8f634942":"code","181d010b":"code","bae705cd":"code","a459a843":"code","da53c3c5":"code","331e7ce4":"code","316e2695":"markdown","bc15202f":"markdown","6cadbe9c":"markdown","62f0df40":"markdown","f60a6dd2":"markdown","83c24571":"markdown","de1819fd":"markdown","863dd0ef":"markdown","b999ef93":"markdown","6a7870d1":"markdown","d1f6d8cf":"markdown","1548a585":"markdown","7b796820":"markdown","059ae473":"markdown","0ab43d77":"markdown","5f90c914":"markdown","31619781":"markdown","bc1af180":"markdown","d8a628ae":"markdown","e57f94e9":"markdown","851118d9":"markdown","612ab0da":"markdown","8f62262e":"markdown","dbc8f75d":"markdown","0db0867e":"markdown","a47698ef":"markdown","b8a46a7d":"markdown","c9eb353a":"markdown","d8643e73":"markdown","b2910b9f":"markdown","30d9ab16":"markdown","74e34bde":"markdown","7d7b4417":"markdown","0b3393ea":"markdown","766d29e6":"markdown","e20e78c7":"markdown","a1c69e67":"markdown","14181362":"markdown"},"source":{"e3a8b102":"from warnings import filterwarnings\nfilterwarnings(\"ignore\")","e454e8ef":"import pandas as pd","11d443cb":"import numpy as np","cdc88759":"import matplotlib.pyplot as plt","72de75a6":"import seaborn as sns","9388b9aa":"import statsmodels.api as sm","6b2fabce":"import statsmodels.formula.api as smf","4af5ecda":"from sklearn.linear_model import LinearRegression","b8c874d3":"from sklearn.metrics import mean_squared_error,r2_score","f7275b9f":"from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,ShuffleSplit,GridSearchCV","7902c763":"from sklearn.decomposition import PCA","65bead35":"from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier","863983e3":"from sklearn.preprocessing import scale","67857604":"from sklearn import model_selection","8a654dd7":"from sklearn.svm import SVR","8da318c5":"import time","557ca687":"hts = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhts.head()","0f629d79":"hts.dropna(inplace=True)","f07f44fd":"one_hot_encoded = pd.get_dummies(hts[[\"League\",\"Division\",\"NewLeague\"]])\none_hot_encoded.head()","0e83a1f1":"new_hts = hts.drop([\"League\",\"Division\",\"NewLeague\",\"Salary\"],axis=1).astype(\"float64\")","e120a0a3":"X = pd.concat([new_hts,one_hot_encoded[[\"League_N\",\"Division_W\",\"NewLeague_N\"]]],axis=1)\nX.head()","f28672f1":"y = hts.Salary # Target-dependent variable","3cddd93a":"hts.shape","7b22f5d4":"#Independent Variables\nX.shape","e66b4191":"#Dependent Variables\ny.shape","b9ce302b":"X_train = X.iloc[:210]\nX_test = X.iloc[210:]\ny_train = y[:210]\ny_test = y[210:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","26de9eb2":"X_train = pd.DataFrame(X_train[\"Hits\"])\nX_test = pd.DataFrame(X_test[\"Hits\"])","ac9f7d83":"X_train","78763d70":"X_test[:10]","93c51c86":"svr_model = SVR(\"linear\").fit(X_train,y_train)","8b741e85":"svr_model","e2e03514":"print(\"y = {0} + {1} x\".format(svr_model.intercept_[0],\n                              svr_model.coef_[0][0]))","af721fe0":"print(\"Manual Calculation: \",svr_model.intercept_[0]+(svr_model.coef_[0][0]*X_train[\"Hits\"][0:1].item()),\n      \"\\nOutput of Model\",svr_model.predict(X_train[0:1]).item())","d19bc5b6":"y_pred=svr_model.predict(X_train)","baf7e53b":"plt.scatter(X_train,y_train)\nplt.plot(X_train,y_pred,c=\"r\")\nplt.title(\"Train Errors\")\nplt.show()","495fa3b1":"svr_model","d4e78839":"y_pred=svr_model.predict(X_train)","e67aa10a":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","bd6a6a3a":"r2_score(y_train,y_pred)","1d859dfb":"y_pred=svr_model.predict(X_test)","606874c1":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","300060b5":"r2_score(y_test,y_pred)","f745a14d":"svr_model","9c4aeea9":"svr_parameters = {\"C\": np.arange(0.1,3,0.1)}\nsvr_cv_model= GridSearchCV(svr_model,svr_parameters,cv=15).fit(X_train,y_train)","17f01e2e":"svr_cv_model.best_params_","ccf6e666":"svr_tuned = SVR(\"linear\",\n                C=pd.Series(svr_cv_model.best_params_)[0]).fit(X_train,y_train)","8071214a":"svr_tuned","fb38955f":"y_pred=svr_tuned.predict(X_train)","5b5e05e3":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","19d3d6e6":"r2_score(y_train,y_pred)","ee1feef1":"y_pred=svr_tuned.predict(X_test)","baecb09c":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","e1012618":"r2_score(y_test,y_pred)","36c12177":"hts = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhts.head()","dd432425":"hts.dropna(inplace=True)","52467e00":"one_hot_encoded = pd.get_dummies(hts[[\"League\",\"Division\",\"NewLeague\"]])\none_hot_encoded.head()","31de2299":"new_hts = hts.drop([\"League\",\"Division\",\"NewLeague\",\"Salary\"],axis=1).astype(\"float64\")","a6e5666d":"X = pd.concat([new_hts,one_hot_encoded[[\"League_N\",\"Division_W\",\"NewLeague_N\"]]],axis=1)\nX.head()","e3992da4":"y = hts.Salary # Target-dependent variable","b3bddfe5":"hts.shape","eeeb93a4":"#Independent Variables\nX.shape","e673d7e4":"#Dependent Variables\ny.shape","6d69c981":"X_train = X.iloc[:210]\nX_test = X.iloc[210:]\ny_train = y[:210]\ny_test = y[210:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","2864ace5":"SVR_Radial_Basis = SVR(\"rbf\").fit(X_train,y_train)","2f6466ee":"SVR_Radial_Basis","13ae48ab":"y_pred=SVR_Radial_Basis.predict(X_train)","50e58f0a":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","17df6068":"r2_score(y_train,y_pred)","f6356e42":"y_pred=SVR_Radial_Basis.predict(X_test)","b7f984fa":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","8f2a37f6":"r2_score(y_test,y_pred)","73eda3f7":"SVR_Radial_Basis","fd3d8b32":"svr_parameters = {\"C\": np.arange(0.2,10,0.1)}\nsvr_cv_model= GridSearchCV(SVR_Radial_Basis,svr_parameters,cv=15).fit(X_train,y_train)","78712d88":"svr_cv_model.best_params_","b4ad0fb9":"svr_tuned = SVR(\"rbf\",\n                C=pd.Series(svr_cv_model.best_params_)[0]).fit(X_train,y_train)","60d653e8":"svr_tuned","8f634942":"y_pred=svr_tuned.predict(X_train)","181d010b":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","bae705cd":"r2_score(y_train,y_pred)","a459a843":"y_pred=svr_tuned.predict(X_test)","da53c3c5":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","331e7ce4":"r2_score(y_test,y_pred)","316e2695":"## Importing Libraries","bc15202f":"## Resources","6cadbe9c":"### Model","62f0df40":"### Model Tuning","f60a6dd2":"**Created by Berkay Alan**\n\n**Non-Linear Models - Regression | Support Vector Regression(SVR)**\n\n**2 August 2021**","83c24571":"Support Vector Regression gives us the flexibility to define how much error is acceptable in our model and will find an appropriate line (or hyperplane in higher dimensions) to fit the data. The objective function of SVR is to minimize the coefficients \u2014 more specifically, the l2-norm of the coefficient vector \u2014 not the squared error. The error term is instead handled in the constraints, where we set the absolute error less than or equal to a specified margin, called the maximum error, **\u03f5 (epsilon)**. We can tune epsilon to gain the desired accuracy of our model.","de1819fd":"Illustrative example:\n\n![image-2.png](attachment:image-2.png)\n\nPhoto is cited by:https:\/\/towardsdatascience.com\/an-introduction-to-support-vector-regression-svr-a3ebc1672c2","863dd0ef":"![Screen%20Shot%202021-07-20%20at%2020.29.43.png](attachment:Screen%20Shot%202021-07-20%20at%2020.29.43.png)","b999ef93":"Now we will split our dataset as train and test set.","6a7870d1":"**For more Tutorial:** https:\/\/github.com\/berkayalan","d1f6d8cf":"We now have an additional **C** hyperparameter that we can tune. As C increases, our tolerance for points outside of \u03f5 also increases. As C approaches 0, the tolerance approaches 0 and the equation collapses into the simplified one.","1548a585":"### Prediction","7b796820":"### Model","059ae473":"We did all process with only **Hits** column. We can also try with all columns. But it takes more time nearly 30 min.","0ab43d77":"Now we will remove NA values.","5f90c914":"That's the formula to minimise:","31619781":"### Theory","bc1af180":"We will do **One Hot Encoding** to categorical columns.","d8a628ae":"Photo is cited by: https:\/\/towardsdatascience.com\/an-introduction-to-support-vector-regression-svr-a3ebc1672c2","e57f94e9":"- **The Elements of  Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Jerome Friedman -  Data Mining, Inference, and Prediction (Springer Series in Statistics) \n\n- [**Support Vector Regression Tutorial for Machine Learning**](https:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/support-vector-regression-tutorial-for-machine-learning\/)\n\n- [**An Introduction to Support Vector Regression (SVR)**](https:\/\/towardsdatascience.com\/an-introduction-to-support-vector-regression-svr-a3ebc1672c2)\n\n- [**SVM: Difference between Linear and Non-Linear Models**](https:\/\/www.aitude.com\/svm-difference-between-linear-and-non-linear-models\/)\n\n- [**Kernel Functions-Introduction to SVM Kernel & Examples**](https:\/\/data-flair.training\/blogs\/svm-kernel-functions\/)\n\n- [**Understanding Confusion Matrix**](https:\/\/towardsdatascience.com\/understanding-confusion-matrix-a9ad42dcfd62)","851118d9":"As we can see, we have data points that are outside the epsilon in sensitive tube. We care about the error for them and they will be measured as distance between the point and the tube. As such, we need to account for the possibility of errors that are larger than \u03f5. We can do this with slack variables.\n\nThe concept of **slack variables** is simple: for any value that falls outside of \u03f5, we can denote its deviation from the margin as \u03be.","612ab0da":"## Support Vector Regression(SVR)","8f62262e":"**<span style='color:Blue'> Check out My Github for other Regression Models  <\/span>**","dbc8f75d":"**Content**","0db0867e":"Now we will remove NA values.","a47698ef":"For a real world example, we will work with **Hitters** dataset.\n\nIt can be downloaded here: https:\/\/www.kaggle.com\/floser\/hitters","b8a46a7d":"We can write Support vector regression in linear regression form as below:","c9eb353a":"### Theory","d8643e73":"\n- Support Vector Regression(SVR) (Theory - Model- Tuning)\n- Non-Linear Support Vector Regression(SVR) (Theory - Model- Tuning)","b2910b9f":"We will do **One Hot Encoding** to categorical columns.","30d9ab16":"## Non-Linear Support Vector Regression(SVR)","74e34bde":"Now we will split our dataset as train and test set.","7d7b4417":"### Model Tuning","0b3393ea":"For a real world example, we will work with **Hitters** dataset.\n\nIt can be downloaded here: https:\/\/www.kaggle.com\/floser\/hitters","766d29e6":"When we cannot separate data with a straight line we use Non \u2013 Linear SVM. In this, we have **Kernel functions**. They transform non-linear spaces into linear spaces. It transforms data into another dimension so that the data can be classified.\n\nIt transforms two variables x and y into three variables along with z. Therefore, the data have plotted from 2-D space to 3-D space. Now we can easily classify the data by drawi","e20e78c7":"Let's control this formula.","a1c69e67":"### Prediction","14181362":"Github Repository Including:\n    \n- K - Nearest Neighbors(KNN) (Theory - Model- Tuning)\n- Regression(Decision) Trees (CART) (Theory - Model- Tuning)\n- Ensemble Learning - Bagged Trees(Bagging) (Theory - Model- Tuning)\n- Ensemble Learning - Random Forests (Theory - Model- Tuning)\n- Gradient Boosting Machines(GBM)  (Theory - Model- Tuning)\n- Light Gradient Boosting Machines(LGBM)  (Theory - Model- Tuning)\n- XGBoost(Extreme Gradient Boosting)  (Theory - Model- Tuning)\n- Catboost  (Theory - Model- Tuning)\n  \nCheck it out: https:\/\/github.com\/berkayalan\/Data-Science-Tutorials\/blob\/master\/Non-Linear%20Models%20-%20Regression.ipynb"}}