{"cell_type":{"a07926fd":"code","8b5c43b3":"code","62d6590d":"code","c7194ccd":"code","94ee994c":"code","785e84c7":"code","e88c2241":"code","da59b9b5":"code","2d486ab6":"code","75a679db":"code","5afb71b3":"code","38b0fdc6":"code","4f1f8aea":"code","22447aa8":"code","236ab81a":"code","26920451":"code","8d911d24":"code","1c375a3b":"code","ae9dce5f":"code","b7f39f52":"code","4921b3d0":"markdown","39dcf236":"markdown","95b785fb":"markdown","21b35421":"markdown","b4d06833":"markdown","a97fbc39":"markdown","fb65d56f":"markdown","f5f10f2b":"markdown","100a18ce":"markdown","48432425":"markdown","23d3741e":"markdown","7ba90ef1":"markdown"},"source":{"a07926fd":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\n\nfrom bayes_opt import BayesianOptimization\n\nfrom IPython.display import display\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline  ","8b5c43b3":"plt.style.use('ggplot')\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 1000)\npd.options.mode.use_inf_as_na = True","62d6590d":"#Load data\ndata_path = '..\/input\/learn-together\/'\n\ntrain_df = pd.read_csv(data_path + 'train.csv', index_col = 'Id')\ntest_df = pd.read_csv(data_path + 'test.csv', index_col = 'Id')","c7194ccd":"features = [f for f in list(train_df) if f!='Cover_Type']\ntarget = 'Cover_Type'","94ee994c":"stony_level = np.array([3,2,0,0,0,1,0,0,2,0,0,1,0,0,0,0,0,2,0,0,0,0,0,3,3,2,3,3,3,3,3,3,3,3,0,3,3,3,3,3])","785e84c7":"def feat_eng(df):\n    \n    df['Dist_To_Hydro'] = np.sqrt(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)\n    df['Log_Dist_To_Hydro'] = np.log( np.sqrt(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2) +1)\n    \n    df['Hydro_Fire_p'] = np.abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points'])\n    df['Hydro_Fire_n'] = np.abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n\n    df['Hydro_Road_p'] = np.abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n    df['Hydro_Road_n'] = np.abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n\n    df['Fire_Road_p'] = np.abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_n'] = np.abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n    \n    horiz_grp = [f for f in features if f.startswith('Horizontal')]\n    df['Horiz_Dist_Mean'] = df[horiz_grp].mean(axis = 1).round(2)\n    df['Horiz_Dist_Std'] = df[horiz_grp].std(axis = 1).round(2)\n    \n    df['Is_Overwater'] = df['Vertical_Distance_To_Hydrology'] > 0\n    \n    hill_grp = [f for f in features if f.startswith('Hill')]\n    df['Hillshade_Mean'] = df[hill_grp].mean(axis = 1).round(2)\n    df['Hillshade_Std'] = df[hill_grp].std(axis = 1).round(2)\n    \n    soil_grp = [f for f in features if f.startswith('Soil_Type')]\n    df['Stony_Level'] =  df[soil_grp]@stony_level\n    \n    df['Elevation_Adjusted'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n    df['Elevation'] = np.log(df['Elevation'])\n    \n    df['Aspect'] = df['Aspect'].astype(int) % 360\n    df['Sen_Aspect'] = np.sin(np.radians(df['Aspect']))\n    df['Cos_Aspect'] = np.cos(np.radians(df['Aspect']))\n    \n    df['Sen_Slope'] = np.sin(np.radians(df['Slope']))\n    df['Cos_Slope'] = np.cos(np.radians(df['Slope']))\n    return df\n    \ntrain_df = feat_eng(train_df)\ntest_df = feat_eng(test_df)","e88c2241":"features = [f for f in list(train_df) if f!='Cover_Type']\ntarget = 'Cover_Type'","da59b9b5":"X_train, X_val, Y_train, Y_val = train_test_split(train_df[features], train_df[target],\n                                                  test_size = 0.2, \n                                                  random_state = 42, \n                                                  stratify = train_df[target])\ntrain_idx = X_train.index\nval_idx = X_val.index","2d486ab6":"def Bayes_ExtraTrees(n_estimators,\n                     max_depth,\n                     min_samples_split,\n                     min_samples_leaf,\n                     max_features,\n                     bootstrap):\n    \n    n_estimators = int(n_estimators)\n    max_depth = int(max_depth)\n    min_samples_split = int(min_samples_split)\n    min_samples_leaf = int(min_samples_leaf)\n    bootstrap = bootstrap > 0.5\n    \n    assert type(n_estimators) == int\n    assert type(max_depth) == int\n    assert type(min_samples_split) == int\n    assert type(min_samples_leaf) == int\n    \n    preds = np.zeros(len(X_val))\n\n    etc = ExtraTreesClassifier(n_estimators=n_estimators,\n                               max_depth=max_depth,\n                               min_samples_split=min_samples_split,\n                               min_samples_leaf=min_samples_leaf,\n                               max_features=max_features,\n                               bootstrap=bootstrap,\n                               oob_score=bootstrap,\n                               n_jobs=6,\n                               random_state=42,\n                               verbose=0)\n    etc.fit(X_train, Y_train)\n    \n    preds = etc.predict(X_val)\n    \n    score = accuracy_score(Y_val, preds)\n    \n    return score\n","75a679db":"params = {'n_estimators' : (40, 1000),\n          'max_depth' : (10, 200),\n          'min_samples_split': (2,15),\n          'min_samples_leaf' : (2,15),\n          'max_features' : (.2,.8),\n          'bootstrap':(0,1)} \n\nExtraTreesBO = BayesianOptimization(Bayes_ExtraTrees, params, random_state = 42)\n\nprint(ExtraTreesBO.space.keys)\n","5afb71b3":"init_points = 15\nn_iter = 20","38b0fdc6":"print('--' * 100)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    ExtraTreesBO.maximize(init_points = init_points,\n                            n_iter = n_iter,\n                            acq = 'ucb',\n                            xi = 0.0,\n                            alpha = 1e-6)\n","4f1f8aea":"print(f'Best Accuracy Achieved: {ExtraTreesBO.max[\"target\"]}')\n","22447aa8":"X_train, X_val0, Y_train, Y_val0 = train_test_split(train_df[features], train_df[target],\n                                                  test_size = 0.05, \n                                                  random_state = 111, \n                                                  stratify = train_df[target])","236ab81a":"nfold = 11\nskf = StratifiedKFold(n_splits = nfold, shuffle = True, random_state = 42)\n\noof = np.zeros(len(train_df))\noof_probs = np.zeros((len(train_df),7))\n\naccuracies = []\n\ny_prob_et = np.zeros((len(test_df),7))\ny_val_prob = np.zeros((len(X_val0),7))\n\nfeature_importance_et = pd.DataFrame()","26920451":"for fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, Y_train)):\n    print(f'Computing fold {fold+1}...')\n    \n    etc = ExtraTreesClassifier(n_estimators = int(ExtraTreesBO.max['params']['n_estimators']),\n                               max_depth = int(ExtraTreesBO.max['params']['max_depth']),\n                               min_samples_split = int(ExtraTreesBO.max['params']['min_samples_split']),  \n                               min_samples_leaf = int(ExtraTreesBO.max['params']['min_samples_leaf']),\n                               max_features =  ExtraTreesBO.max['params']['max_features'],                 \n                               bootstrap = ExtraTreesBO.max['params']['bootstrap'] > 0.5,\n                               oob_score = ExtraTreesBO.max['params']['bootstrap'] > 0.5,\n                               n_jobs=6)\n   \n \n    X_trn, X_val = X_train.iloc[trn_idx, :], X_train.iloc[val_idx, :]\n    Y_trn, Y_val = Y_train.iloc[trn_idx], Y_train.iloc[val_idx]\n    etc.fit(X_trn, Y_trn)\n    \n    oof[val_idx] = etc.predict(X_val)\n    oof_probs[val_idx] = etc.predict_proba(X_val)\n    \n    accuracies.append(accuracy_score(Y_val, oof[val_idx]))\n    \n    print(f'Accuracy on Validation: {round(accuracies[fold],4)}')\n    if hasattr(etc, 'oob_score_'):\n        print(f'oob score: {round(etc.oob_score_,4)}')\n    \n    y_prob_et += etc.predict_proba(test_df[features])\/ skf.n_splits \n    y_val_prob += etc.predict_proba(X_val0)\/ skf.n_splits \n    \n    # Features imp\n    fold_importance_df = pd.DataFrame({'feature': features, 'importance': etc.feature_importances_, 'fold': nfold+1})\n    feature_importance_et = pd.concat([feature_importance_et, fold_importance_df], axis=0)\n    \n    del X_trn, X_val, Y_trn, Y_val\n\nprint(f\"Mean accuracy score: {round(np.mean(accuracies),4)}\")","8d911d24":"y_pred = np.argmax(y_prob_et, axis = 1)+1\ny_val_pred = np.argmax(y_val_prob, axis = 1)+1","1c375a3b":"print(pd.crosstab(Y_val0, y_val_pred, rownames=['Actual'], colnames=['Prediction']))\nprint(f\"accuracy: {accuracy_score(Y_val0, y_val_pred)}\")\nprint(f\"\\n {classification_report(Y_val0, y_val_pred)}\")","ae9dce5f":"sns.set_style('whitegrid')\n\ncols = (feature_importance_et[[\"feature\", \"importance\"]]\n    .groupby(\"feature\")\n    .mean()\n    .sort_values(by=\"importance\", ascending=False)[:30].index)\n\nbest_features = feature_importance_et.loc[feature_importance_et['feature'].isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n        edgecolor=('black'), linewidth=2, palette=\"colorblind\")\nplt.title('ExtraTreesClassifier Features Importance (averaged\/folds)', fontsize=15)\nplt.tight_layout()","b7f39f52":"test_df.reset_index(inplace=True)\nidx = test_df.Id\n\npreds = pd.DataFrame({'Id': idx,\n              'Cover_Type': y_pred})\npreds.to_csv('submission.csv', index = False)\npreds.head(10)","4921b3d0":"This is my very first notebook trying Bayesian Optimization implemented in bayes_opt. I am not an expert and I will try other bayesian optimization libraries in the future since I didn't find this one particularly flexible. Any suggestion is more than welcome!\n\nExtraTreesClassifier is my model of choice simply because it gave me best results so far in this competition.\n\n* [Feature Engineering](#fe)\n* [Bayesian Optimization Model](#bom)\n* [Stratified K-Fold](#skf)\n* [Feature Importance](#fi)\n* [Submission](#submission)","39dcf236":"<a id=\"fe\"><\/a>\n# Feature Engineering","95b785fb":"<a id=\"fi\"><\/a>\n# Features Importance","21b35421":"The bigger init_pts and n_iter the better! By laziness I chose these:","b4d06833":"Define the \"Black Box\" model to feed into BayesianOptimization","a97fbc39":"Define the parameters space and the BayesianOptimization model","fb65d56f":"<a id=\"skf\"><\/a>\n# Stratified K-Fold","f5f10f2b":"# Bayesian Optimization for Extra Trees Classifier","100a18ce":"<a id=\"bom\"><\/a>\n# Bayesian Optimization Model","48432425":"Make predictions","23d3741e":"Create a new (very small validation set) to use for scoring after Cross Validation","7ba90ef1":"<a id=\"submission\"><\/a>\n# Submission"}}