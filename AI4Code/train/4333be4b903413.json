{"cell_type":{"ad52b4f0":"code","3a29b390":"code","f11aab63":"code","d0168f85":"code","acd083b2":"code","37418aad":"code","bd93b694":"code","1351e1c5":"code","f1a3b544":"code","e8e243d3":"code","03fa0d3b":"code","6b507344":"code","28bccd4a":"code","fa46242a":"code","d5e6fbdc":"code","7707126e":"code","30ddd418":"code","c2676efa":"code","47a0d0d0":"code","1c24fb17":"code","4a26479e":"code","8e5f327f":"markdown","ae688cb0":"markdown","3568524d":"markdown","b68a9ad6":"markdown","0d437bee":"markdown","09658ccf":"markdown"},"source":{"ad52b4f0":"import lightgbm as lgbm\nimport xgboost as xgb\nimport catboost as cb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nimport category_encoders as ce\nimport shap\nimport matplotlib.pyplot as plt\nxgb.__version__\n%matplotlib inline\nshap.initjs()\n","3a29b390":"train = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/test.csv\")\nsample_sub = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\")","f11aab63":"cont_features = [col for col in train.columns if 'cont' in col]\ncat_features = [col for col in train.columns if 'cat' in col]\ntarget = 'target'\n\ny_train = train[target]\ntrain.drop(['id'], inplace=True, axis=1)\ntest.drop(['id'], inplace=True, axis=1)","d0168f85":"#frequency_encoding\nfor variable in cat_features:\n    count_dict = train[variable].value_counts().to_dict()\n    factor = 1.0 \/ sum(count_dict.values())\n    normalised_count_dict = {k: v * factor for k, v in count_dict.items()}\n    train[f'fe_{variable}'] = train[variable].map(normalised_count_dict)\n    test[f'fe_{variable}'] = test[variable].map(normalised_count_dict)\n\n#target_encoding\nfor variable in cat_features:\n    # create dictionary of category:mean values.\n    dict = train.groupby([variable])[target].mean().to_dict()\n    # apply the encoding to the train and test sets.\n    train[f'te_{variable}'] = train[variable].map(dict)\n    test[f'te_{variable}'] = test[variable].map(dict)\n    \n#label_encoding\nfor variable in cat_features:\n    le = LabelEncoder()\n    le.fit(train[variable])\n    train[f'le_{variable}'] = le.transform(train[variable])\n    test[f'le_{variable}'] = le.transform(test[variable])","acd083b2":"train.drop([target], inplace=True, axis=1)\n\ntrain.drop(cat_features, inplace=True, axis=1)\ntest.drop(cat_features, inplace=True, axis=1)","37418aad":"X_train = train\nX_test = test","bd93b694":"xgb_parameters = {\n        \"objective\": \"reg:squarederror\",\n        \"max_depth\": 10,\n        \"learning_rate\": 0.01,\n        \"colsample_bytree\": 0.5,\n        \"subsample\": 0.5,\n        \"reg_alpha\" : 6,\n        \"min_child_weight\": 100,\n        \"n_jobs\": 8,\n        \"seed\": 22,\n        'tree_method': \"gpu_hist\",\n        \"gpu_id\": 0,\n    }\n","1351e1c5":"kf = KFold(n_splits=5, shuffle=True, random_state=22)\noof = np.zeros(len(X_train))\nscore_list = []\nfold = 1\ntest_preds = []\ntest_df = xgb.DMatrix(X_test)\n\nfor train_index, test_index in kf.split(X_train):\n    Xoof_train, Xoof_val = X_train.iloc[train_index], X_train.iloc[test_index]\n    yoof_train, yoof_val = y_train.iloc[train_index], y_train.iloc[test_index]\n    \n    train_df = xgb.DMatrix(Xoof_train, label=yoof_train)\n    val_df = xgb.DMatrix(Xoof_val, label=yoof_val)\n\n    model = xgb.train(xgb_parameters, train_df, 3000)\n\n    yoof_pred = model.predict(val_df)\n    test_preds.append(model.predict(test_df))\n\n    oof[test_index] = yoof_pred\n    score = np.sqrt(mean_squared_error(yoof_val, yoof_pred))\n    score_list.append(score)\n    print(f\"RMSE Seed 22 Fold-{fold} : {score}\")\n    fold += 1\n\nprint(f\"Seed 22 folds average = {np.mean(score_list)} ({np.std(score_list)})\")\ntest_pred = np.mean(test_preds, axis=0)\ntest_pred_df = pd.DataFrame(test_pred, columns=['target'])","f1a3b544":"shap_preds = model.predict(test_df, pred_contribs=True)","e8e243d3":"X_test.shape","03fa0d3b":"shap.summary_plot(shap_preds[:,:-1], X_test, max_display=50)","6b507344":"shap.summary_plot(shap_preds[:,:-1], X_test, plot_type=\"bar\",  max_display=50)","28bccd4a":"%%time\nshap_interactions = model.predict(test_df, pred_interactions=True)","fa46242a":"cat_columns = [col for col in X_test.columns if 'cat' in col]","d5e6fbdc":"def plot_top_k_interactions(feature_names, shap_interactions, k):\n    # Get the mean absolute contribution for each feature interaction\n    aggregate_interactions = np.mean(np.abs(shap_interactions[:, :-1, :-1]), axis=0)\n    interactions = []\n    for i in range(aggregate_interactions.shape[0]):\n        for j in range(aggregate_interactions.shape[1]):\n            if j < i:\n                try:\n                    interactions.append((feature_names[i] + \"-\" + feature_names[j], aggregate_interactions[i][j] * 2))\n                except:\n                    pass\n    # sort by magnitude\n    interactions.sort(key=lambda x: x[1], reverse=True)\n    interaction_features, interaction_values = map(tuple, zip(*interactions))\n    print(interaction_features[:20])\n    plt.bar(interaction_features[:k], interaction_values[:k])\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n    \nplot_top_k_interactions(cat_columns, shap_interactions, 20)","7707126e":"train = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/test.csv\")\nsample_sub = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\")","30ddd418":"cont_features = [col for col in train.columns if 'cont' in col]\ncat_features = [col for col in train.columns if 'cat' in col]\ntarget = 'target'\n\ny_train = train[target]\ntrain.drop(['id'], inplace=True, axis=1)\ntest.drop(['id'], inplace=True, axis=1)","c2676efa":"cat_interactions = []\ncat_pairs = [(\"cat8\", \"cat0\"), (\"cat9\", \"cat8\"), (\"cat9\", \"cat5\"), (\"cat8\", \"cat5\"), (\"cat9\", \"cat0\")]\nfor pair in cat_pairs:\n    cat_interactions.append(f'{pair[0]}_{pair[1]}')\n    train[f'{pair[0]}_{pair[1]}'] = (train[pair[0]] + train[pair[1]]).astype(\"category\")\n    test[f'{pair[0]}_{pair[1]}'] = (test[pair[0]] + test[pair[1]]).astype(\"category\")","47a0d0d0":"#frequency_encoding\nfor variable in cat_features:\n    count_dict = train[variable].value_counts().to_dict()\n    factor = 1.0 \/ sum(count_dict.values())\n    normalised_count_dict = {k: v * factor for k, v in count_dict.items()}\n    train[f'fe_{variable}'] = train[variable].map(normalised_count_dict)\n    test[f'fe_{variable}'] = test[variable].map(normalised_count_dict)\n\n#target_encoding\nfor variable in cat_features:\n    # create dictionary of category:mean values.\n    dict = train.groupby([variable])[target].mean().to_dict()\n    # apply the encoding to the train and test sets.\n    train[f'te_{variable}'] = train[variable].map(dict)\n    test[f'te_{variable}'] = test[variable].map(dict)\n    \n#label_encoding\nfull_data = pd.concat([train,test], axis=0)\nfor variable in cat_features + cat_interactions:\n    le = LabelEncoder()\n    le.fit(full_data[variable])\n    train[f'le_{variable}'] = le.transform(train[variable])\n    test[f'le_{variable}'] = le.transform(test[variable])","1c24fb17":"train.drop([target], inplace=True, axis=1)\n\ntrain.drop(cat_features + cat_interactions, inplace=True, axis=1)\ntest.drop(cat_features + cat_interactions, inplace=True, axis=1)\n\nX_train = train\nX_test = test","4a26479e":"kf = KFold(n_splits=5, shuffle=True, random_state=22)\noof = np.zeros(len(X_train))\nscore_list = []\nfold = 1\ntest_preds = []\ntest_df = xgb.DMatrix(X_test, enable_categorical=True)\n\nfor train_index, test_index in kf.split(X_train):\n    Xoof_train, Xoof_val = X_train.iloc[train_index], X_train.iloc[test_index]\n    yoof_train, yoof_val = y_train.iloc[train_index], y_train.iloc[test_index]\n    \n    train_df = xgb.DMatrix(Xoof_train, label=yoof_train, enable_categorical=True)\n    val_df = xgb.DMatrix(Xoof_val, label=yoof_val, enable_categorical=True)\n\n    model = xgb.train(xgb_parameters, train_df, 3000)\n\n    yoof_pred = model.predict(val_df)\n    test_preds.append(model.predict(test_df))\n\n    oof[test_index] = yoof_pred\n    score = np.sqrt(mean_squared_error(yoof_val, yoof_pred))\n    score_list.append(score)\n    print(f\"RMSE Seed 22 Fold-{fold} : {score}\")\n    fold += 1\n\nprint(f\"Seed 22 folds average = {np.mean(score_list)} ({np.std(score_list)})\")\ntest_pred = np.mean(test_preds, axis=0)\ntest_pred_df = pd.DataFrame(test_pred, columns=['target'])","8e5f327f":"I want to focus on interactions between categorical features.","ae688cb0":"Folds average improved 0.8441617327137406 (0.0008574847655632837) =>  0.8440450262413028 (0.0008286076422571708)","3568524d":"5 most gain category interaction pairs are (\"cat8\", \"cat0\"), (\"cat9\", \"cat8\"), (\"cat9\", \"cat5\"), (\"cat8\", \"cat5\"), (\"cat9\", \"cat0\")","b68a9ad6":"We are starting from scratch to add interactions as new features","0d437bee":"This notebook uses [https:\/\/www.kaggle.com\/tunguz](http:\/\/) s [https:\/\/www.kaggle.com\/tunguz\/tps-02-21-feature-importance-with-xgboost-and-shap](http:\/\/)  work as base. \n","09658ccf":"Remind that these parameters are just default values and not tuned to have best score. "}}