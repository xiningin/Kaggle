{"cell_type":{"f12a2a8b":"code","e3e28cba":"code","0bbf5a44":"code","450a9b8d":"code","3e467c6a":"code","0ea08ecf":"code","f67c4d51":"code","0c82fb18":"code","61327f68":"code","e0f74de4":"code","8dd7b84d":"code","958a1d4e":"code","1eef466b":"code","339c7bf2":"code","9489056e":"code","8426d606":"code","06d7276c":"code","0be83fcf":"code","fdee4240":"code","57fe658f":"code","6ace2d46":"code","7207f77a":"code","004adcab":"code","150fcfb6":"code","01b5445d":"code","6988a657":"code","ac040531":"code","9c0cbbe8":"code","97e0845b":"code","769776f6":"code","fb4a5cf5":"code","fa8d718a":"code","0f9e8056":"code","f6f28f91":"code","22ef9271":"code","c39c575a":"code","36387877":"code","000169c4":"code","ac9b201e":"code","512fda42":"code","cef40db8":"code","eeaad4ea":"code","e01f8508":"code","3c402cc6":"code","0b7bfd83":"code","2069bd86":"code","eccaed38":"code","476656a9":"code","4e4209c4":"code","788fb767":"code","f102f6b6":"code","e2d06553":"code","0ac76ab5":"code","9e2bfc0b":"code","dc01804c":"code","41e049b7":"code","4fa31568":"code","9b3db7a0":"code","7d4717ba":"code","794c6af8":"code","547fc552":"code","aaf79569":"code","67c1b851":"code","2fb7f5a9":"code","b5cb9890":"code","72d62df7":"code","2a6c7630":"code","5aa090b3":"code","14b8cfda":"code","04838ed7":"code","f8e329b3":"code","7dfade7e":"code","95ec83c4":"code","d798d706":"code","bea0d6b4":"code","39dda13e":"code","1f3a42b9":"code","82305b69":"code","74ed19c9":"code","93ea27c1":"code","34231fe7":"code","bd5e540c":"code","383c20a2":"code","6561b05d":"code","ccdd8057":"code","8407a129":"code","7d189695":"code","097184b6":"code","9cf189c9":"code","8f37e8c1":"code","44ed2a7c":"code","d3d1f45a":"code","d0a10a18":"code","e617e6fc":"code","6a9746ea":"code","e99dc953":"code","70a0bfd9":"code","8e7ef428":"code","ba94318c":"code","2d34893d":"code","0b788d4a":"code","ebdd0f6e":"code","0dc6c022":"code","921dd946":"code","9ad01418":"code","fd18ca55":"code","1c5231f0":"code","2bef1733":"code","64130253":"code","4d841a27":"code","494eb90c":"markdown","a3a8cf4f":"markdown","2bffeefe":"markdown","83bc72f2":"markdown","524e2dc2":"markdown","650ea1b5":"markdown","3caf32f9":"markdown","8c8b77d5":"markdown","c26b9c91":"markdown","82d53ef9":"markdown","ffc0b33d":"markdown","e4dee763":"markdown","540a3a34":"markdown","072329e5":"markdown","0da174ce":"markdown","9b5ed8c3":"markdown","5e2b7b0e":"markdown","5d0dfce4":"markdown","83076951":"markdown","8ec4495e":"markdown","04b0e2d7":"markdown","2f98a329":"markdown","50b385f0":"markdown","6c442eed":"markdown"},"source":{"f12a2a8b":"# essential libraries\nimport numpy as np \nimport pandas as pd\n# for data visulization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Misc.\nimport os\nimport time\nimport gc\nimport random\nfrom scipy.stats import uniform\nimport warnings\n\n# for modeling estimators\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as gbm\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb\n\n#for data processing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import  OneHotEncoder as ohe\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.compose import ColumnTransformer as ct\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\n\n# for measuring performance\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\n\n#for tuning parameters\nfrom bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV\nfrom eli5.sklearn import PermutationImportance\n\n","e3e28cba":"print(os.listdir(\"..\/input\"))","0bbf5a44":"\n\n# Read the data csv files\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\npd.options.display.max_columns = 120\nids=test['Id']","450a9b8d":"train.head()","3e467c6a":"train.shape, test.shape","0ea08ecf":"test.head()","f67c4d51":"train.info() ","0c82fb18":"test.info() ","61327f68":"sns.countplot(x=\"abastaguano\",hue=\"Target\",data=train)","e0f74de4":"sns.countplot(x=\"tamhog\",hue=\"Target\",data=train)","8dd7b84d":" sns.countplot(x=\"r4t3\",hue=\"Target\",data=train)","958a1d4e":"sns.countplot(x=\"sanitario1\",hue=\"Target\",data=train)","1eef466b":"sns.countplot(x=\"hacdor\",hue=\"Target\",data=train)","339c7bf2":"sns.countplot(x=\"escolari\",hue=\"Target\",data=train)","9489056e":"# Checking the object variables in train data\ntrain.select_dtypes('object').head()","8426d606":"# Replace 'Yes' and 'No' with 0 and 1 for train data\n\nyes_no_map = {'no':0,'yes':1}\ntrain['dependency'] = train['dependency'].replace(yes_no_map).astype(np.float32)\ntrain['edjefe'] = train['edjefe'].replace(yes_no_map).astype(np.float32)\ntrain['edjefa'] = train['edjefa'].replace(yes_no_map).astype(np.float32)","06d7276c":"# Replace 'Yes' and 'No' with 0 and 1 for test data\n\nyes_no_map = {'no':0,'yes':1}\ntest['dependency'] = test['dependency'].replace(yes_no_map).astype(np.float32)\ntest['edjefe'] = test['edjefe'].replace(yes_no_map).astype(np.float32)\ntest['edjefa'] = test['edjefa'].replace(yes_no_map).astype(np.float32)","0be83fcf":"# First lets check the data details\ntrain[[\"dependency\",\"edjefe\",\"edjefa\"]].describe()","fdee4240":" # Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\n#Now list them sorted by the percent to know which all cols have missing \/ Null values\nmissing.sort_values('percent', ascending = False).head(10)","57fe658f":"# Fill in the null values for train data\n\ntrain['v18q1'] = train['v18q1'].fillna(0)\ntrain['v2a1'] = train['v2a1'].fillna(0)\ntrain['rez_esc'] = train['rez_esc'].fillna(0)\ntrain['rez_esc'] = train['rez_esc'].fillna(0)\n\ntrain['SQBmeaned'] = train['SQBmeaned'].fillna(0)\n\ntrain['meaneduc'] = train['meaneduc'].fillna(0)\n\n","6ace2d46":"# Fill in the null values for test data\n\ntest['rez_esc'] = test['rez_esc'].fillna(0)\ntest['SQBmeaned'] = test['SQBmeaned'].fillna(0)\ntest['meaneduc'] = test['meaneduc'].fillna(0)\n\ntest['v18q1'] = test['v18q1'].fillna(0)\ntest['v2a1'] = test['v2a1'].fillna(0)","7207f77a":"#Checking for missing values in TRAIN data again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","004adcab":"#Checking for missing values in TEST Data again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(test.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","150fcfb6":"# Drop the Id col as it is not nessecary \n\ntrain.drop(['Id','idhogar'], inplace = True, axis =1)\n\ntest.drop(['Id','idhogar'], inplace = True, axis =1)\n\ntrain.shape, test.shape","01b5445d":"y = train.iloc[:,140]\nX = train.iloc[:,1:141]\ny.shape, X.shape","6988a657":"my_imputer = SimpleImputer()\nX = my_imputer.fit_transform(X)\nscale = ss()\nX = scale.fit_transform(X)\n\nmy_imputer = SimpleImputer()\ntest = my_imputer.fit_transform(test)\nscale = ss()\ntest = scale.fit_transform(test)\n\nX.shape, y.shape,test.shape","ac040531":"X_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.3)","9c0cbbe8":"modeletc = ExtraTreesClassifier()\n","97e0845b":"start = time.time()\nmodeletc = modeletc.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","769776f6":"classes = modeletc.predict(X_test)\n\nclasses","fb4a5cf5":"(classes == y_test).sum()\/y_test.size\n","fa8d718a":"f1 = f1_score(y_test, classes, average='macro')\nf1","0f9e8056":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    ExtraTreesClassifier( ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {   'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n)","f6f28f91":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","22ef9271":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","c39c575a":"modeletcTuned=ExtraTreesClassifier(criterion=\"entropy\",\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","36387877":"start = time.time()\nmodeletcTuned = modeletcTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","000169c4":"yetc=modeletcTuned.predict(X_test)\nyetctest=modeletcTuned.predict(test)","ac9b201e":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","512fda42":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","cef40db8":"#  What all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","eeaad4ea":"modelknh = KNeighborsClassifier(n_neighbors=4)\nstart = time.time()\nmodelkngh = modelknh.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","e01f8508":"classes = modelknh.predict(X_test)\n\nclasses","3c402cc6":"(classes == y_test).sum()\/y_test.size ","0b7bfd83":"f1 = f1_score(y_test, classes, average='macro')\nf1","2069bd86":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    KNeighborsClassifier(\n       n_neighbors=4         # No need to tune this parameter value\n      ),\n    {\"metric\": [\"euclidean\", \"cityblock\"]},\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n   )","eccaed38":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","476656a9":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","4e4209c4":"modelneighTuned = KNeighborsClassifier(n_neighbors=4,\n               metric=\"cityblock\")","788fb767":"start = time.time()\nmodelneighTuned = modelneighTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","f102f6b6":"yneigh=modelneighTuned.predict(X_test)","e2d06553":"yneightest=modelneighTuned.predict(test)","0ac76ab5":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","9e2bfc0b":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","dc01804c":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","41e049b7":"modelgbm=gbm()\nstart = time.time()\nmodelgbm = modelgbm.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","4fa31568":"classes = modelgbm.predict(X_test)\n\nclasses","9b3db7a0":"(classes == y_test).sum()\/y_test.size ","7d4717ba":"f1 = f1_score(y_test, classes, average='macro')\nf1","794c6af8":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    gbm(\n               # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        \n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 2                # Number of cross-validation folds\n)","547fc552":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","aaf79569":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","67c1b851":"modelgbmTuned=gbm(\n               max_depth=81,\n               max_features=42,\n               min_weight_fraction_leaf=0.023286365925229927,\n               n_estimators=178)","2fb7f5a9":"start = time.time()\nmodelgbmTuned = modelgbmTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","b5cb9890":"ygbm=modelgbmTuned.predict(X_test)\nygbmtest=modelgbmTuned.predict(test)\n\n#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","72d62df7":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","2a6c7630":"modelxgb=XGBClassifier()\nstart = time.time()\nmodelxgb = modelxgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","5aa090b3":"classes = modelxgb.predict(X_test)\n\nclasses","14b8cfda":"(classes == y_test).sum()\/y_test.size ","04838ed7":"f1 = f1_score(y_test, classes, average='macro')\nf1","f8e329b3":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    XGBClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","7dfade7e":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","95ec83c4":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","d798d706":"modelxgbTuned=XGBClassifier(criterion=\"gini\",\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.05997,\n               n_estimators=500)","bea0d6b4":"start = time.time()\nmodelxgbTuned = modelxgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","39dda13e":"yxgb=modelxgbTuned.predict(X_test)\nyxgbtest=modelxgbTuned.predict(test)\n","1f3a42b9":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","82305b69":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","74ed19c9":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","93ea27c1":"modellgb = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)\n\nstart = time.time()\nmodellgb = modellgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","34231fe7":"classes = modellgb.predict(X_test)\n\nclasses","bd5e540c":"(classes == y_test).sum()\/y_test.size \n","383c20a2":"f1 = f1_score(y_test, classes, average='macro')\nf1","6561b05d":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    lgb.LGBMClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","ccdd8057":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","8407a129":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","7d189695":"modellgbTuned = lgb.LGBMClassifier(criterion=\"gini\",\n               max_depth=9,\n               max_features=18,\n               min_weight_fraction_leaf=0.09409158712324348,\n               n_estimators=492)","097184b6":"start = time.time()\nmodellgbTuned = modellgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","9cf189c9":"ylgb=modellgbTuned.predict(X_test)\nylgbtest=modellgbTuned.predict(test)\n\n#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","8f37e8c1":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","44ed2a7c":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","d3d1f45a":"modelrf = rf()\n\nstart = time.time()\nmodelrf = modelrf.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","d0a10a18":"classes = modelrf.predict(X_test)\n(classes == y_test).sum()\/y_test.size \n","e617e6fc":"f1 = f1_score(y_test, classes, average='macro')\nf1","6a9746ea":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    rf(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)\n","e99dc953":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","70a0bfd9":"modelrfTuned=rf(criterion=\"entropy\",\n               max_depth=77,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","8e7ef428":"start = time.time()\nmodelrfTuned = modelrfTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","ba94318c":"yrf=modelrfTuned.predict(X_test)\nyrf","2d34893d":"yrftest=modelrfTuned.predict(test)\nyrftest","0b788d4a":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","ebdd0f6e":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","0dc6c022":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","921dd946":"NewTrain = pd.DataFrame()\nNewTrain['yrf'] = yrf.tolist()\nNewTrain['yetc'] = yetc.tolist()\nNewTrain['yneigh'] = yneigh.tolist()\nNewTrain['ygbm'] = ygbm.tolist()\nNewTrain['yxgb'] = yxgb.tolist()\nNewTrain['ylgb'] = ylgb.tolist()\n\nNewTrain.head(5), NewTrain.shape","9ad01418":"NewTest = pd.DataFrame()\nNewTest['yrf'] = yrftest.tolist()\nNewTest['yetc'] = yetctest.tolist()\nNewTest['yneigh'] = yneightest.tolist()\nNewTest['ygbm'] = ygbmtest.tolist()\nNewTest['yxgb'] = yxgbtest.tolist()\nNewTest['ylgb'] = ylgbtest.tolist()\nNewTest.head(5), NewTest.shape","fd18ca55":"NewModel=rf(criterion=\"entropy\",\n               max_depth=77,\n               max_features=6,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","1c5231f0":"start = time.time()\nNewModel = NewModel.fit(NewTrain, y_test)\nend = time.time()\n(end-start)\/60","2bef1733":"ypredict=NewModel.predict(NewTest)\nylgbtest\n","64130253":"\nsubmit=pd.DataFrame({'Id': ids, 'Target': ylgbtest})\nsubmit.head(5)","4d841a27":"submit.to_csv('submit.csv', index=False)","494eb90c":"The dataset is a set of household characteristics from a representative sample of Costa Rican Households. The dataset has observations for each member of the household but the classification is done at the household level. That is, households cannot have two different classifications. Data is not presented at the household level so that participants can create their own household features from individual data.\n\nIt is a classification problem with poverty classified by four levels. \n\nCore Data fields\n\n\u2022\tId - a unique identifier for each row.\n\n\u2022\tTarget - the target is an ordinal variable indicating groups of income levels. \n\n1 = extreme poverty \n\n2 = moderate poverty \n\n3 = vulnerable households \n\n4 = non vulnerable households\n\n\u2022\tidhogar - this is a unique identifier for each household. This can be used to create household-wide features, etc. All rows in a given household will have a matching value for this identifier.\n\n\u2022\tparentesco1 - indicates if this person is the head of the household.\n\n\u2022\tThis data contains 142 total columns.\n\n","a3a8cf4f":"**Now let us tune the hyperparameters again using Baynesian Optimization**","2bffeefe":"**Let us explore the data first**","83bc72f2":"**Now we will tune the hyperparameters using Bayenesian Optimization**","524e2dc2":"Modelling with ExtraTreeClassifier","650ea1b5":"The following activities will be conducted:\n\na. Explore data and perform data visualization\n\nb. Fill in missing values (NULL values) either using mean or median (if the attribute is numeric) or most-frequently occurring value if the attribute is 'object' or categorical.\n\nb. Perform feature engineering, may be using some selected features and only from numeric features.\n\nc. Scale numeric features, AND IF REQUIRED, perform One HOT Encoding of categorical features\n\nd. IF number of features is very large, please do not forget to do PCA. \n\ne. Select some estimators for your work. May be select some (or all) of these:\n\n        GradientBoostingClassifier\n        RandomForestClassifier\n        KNeighborsClassifier\n        ExtraTreesClassifier\n        XGBoost\n        LightGBM\n   \n   First perform modeling with default parameter values and get accuracy.\n\nf. And then, perform tuning of parameters of selected estimators using Bayesian Optimization. \n","3caf32f9":"**Again tune the performance by setting hyperparameters using Baynesian Optimization**","8c8b77d5":"**Now let us tune the Hyperparameters using Baynesian Optimization**","c26b9c91":"**Now let us start modelling**","82d53ef9":"**Now performance tuning using Baynesian Optimization**","ffc0b33d":"**Spit the data now**","e4dee763":"**BUILDING A NEW DATASETS WITH Predicted Values using 6 models**","540a3a34":"**Now we will model using Random Forests**","072329e5":"**Costa Rican House Hold Poverty Level Assesment**","0da174ce":"**Now let us visualize the training data** ","9b5ed8c3":"**Now we will tune the hyperparameters using Baynesian Optimization**","5e2b7b0e":"**Now let us model using KNeighborsClassifier**","5d0dfce4":"**No let us model using GradientBoostingClassifier**","83076951":"**Now lets us model with the Light Gradient Booster**","8ec4495e":"**Now let us model with XGBClassifier**","04b0e2d7":"**Now let us check and prepare the data for modelling purpose**","2f98a329":"**Now we will convert these objects into numerical values**","50b385f0":"** Scaling numeric features **","6c442eed":"**Now let us divide the data into target and predictor variables**"}}