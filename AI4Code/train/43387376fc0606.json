{"cell_type":{"4161762b":"code","1696c4e2":"code","7fe92260":"code","bd979f7b":"code","c8b75a04":"code","0a2bb19b":"code","21218a9d":"code","a8765cc4":"code","50eb6f39":"code","ad7451fd":"code","cdc8aa70":"code","e788faac":"code","8a8a30b0":"code","187f45a2":"code","6b77859c":"code","eb7a154a":"code","07ae0e5f":"code","2e940be6":"code","b9738723":"code","fb1a2d73":"code","e5df81fb":"code","a2b2e208":"code","303f6850":"code","8b1cd792":"code","78216d4a":"code","5719f980":"code","d0bfa55a":"code","a66fcd37":"code","37924573":"code","43ff6ef2":"code","b5df857f":"code","e410c0ff":"code","bdaf91f8":"code","2b9f3f8f":"code","012848ff":"code","260ea853":"code","c9b820a7":"code","8d570aeb":"code","3baa0692":"code","281009ad":"code","5a25b604":"code","5e890e06":"code","e75777f3":"code","41ac414a":"code","dc710a83":"code","d2d710c4":"code","2c53eb2b":"code","7708679b":"code","13019933":"code","31b83917":"code","c6a36cb0":"code","cf2e98f4":"code","8534b9bf":"code","35f31e30":"code","0a4044d1":"code","6f8523dd":"code","b8a3f4e2":"code","d68b62c1":"code","2b7fb143":"code","67ad2c0c":"code","0f953abb":"code","e311f7f7":"code","5a555db5":"code","1376522f":"code","874212ae":"code","42ea355e":"code","696b9228":"code","9c72606e":"code","8a447786":"code","bac25b09":"code","d9e6891c":"code","403929a8":"code","b918cac2":"code","f6fda16d":"code","34bb0841":"code","4f4851c2":"code","c1ebf660":"code","a9660554":"code","b42b5be4":"code","e93f3303":"code","b227da30":"code","1d53c323":"code","683871dc":"code","fc45bf54":"code","f426a17d":"code","8dcdc07b":"code","506c0751":"code","b3e59b19":"code","8db94dd1":"code","a2f7201e":"code","3ebb9d34":"code","d3cf3149":"code","43202119":"markdown","75440f39":"markdown","45fbb8bb":"markdown","c1491e51":"markdown","18e2d95c":"markdown","b001f2f2":"markdown","f906774a":"markdown","e916040d":"markdown","7bc523f5":"markdown","0efd66a4":"markdown","d10391a0":"markdown","d4ee3615":"markdown","e85d6078":"markdown","5c047f2c":"markdown","b2fcd3c1":"markdown","5ca70a5f":"markdown","955e2122":"markdown","ef858079":"markdown","4cedc95a":"markdown","f4879dde":"markdown","69f3f83c":"markdown","fa6522b8":"markdown","e97a3489":"markdown","906c55a3":"markdown","2eb5c513":"markdown","ab2238b6":"markdown","fdda0825":"markdown","9dcec4cb":"markdown","1c74e3bc":"markdown","1272fde8":"markdown","ba2b7e2d":"markdown","561c96c4":"markdown","afc1965e":"markdown","82e5eb7a":"markdown","342dd487":"markdown","92b0e8c2":"markdown","811e5000":"markdown","7963b3e6":"markdown","8a329b8b":"markdown","ede19534":"markdown","8f863e35":"markdown","1b557e6d":"markdown","71117397":"markdown","718259e0":"markdown","703b12ea":"markdown","7b82bf26":"markdown","6d13f115":"markdown","db660a8a":"markdown","5540666b":"markdown","3a6c157d":"markdown","4b830664":"markdown","541b0e9b":"markdown","fbe729df":"markdown","be5c9a86":"markdown","38e7ed90":"markdown","7dc8ae60":"markdown","6d77e60e":"markdown","6cb47617":"markdown","de1ddc32":"markdown","77f8bbe8":"markdown","4a646a90":"markdown","d55fe5fb":"markdown","dba8a5c9":"markdown","9f1be9d3":"markdown","8cd18a94":"markdown","b59ed7dd":"markdown","c2f537ac":"markdown","9001862c":"markdown","02810806":"markdown","083f55ff":"markdown","d1c5bc31":"markdown","0a1d5685":"markdown","7117ca32":"markdown","635344d1":"markdown","50d7b572":"markdown","ff9abf5d":"markdown","ad32d974":"markdown","45ef158c":"markdown","eb900125":"markdown","3da78c83":"markdown","efd55b52":"markdown","d9124c1d":"markdown","68d2a2ec":"markdown","8e306be9":"markdown","c5deeeea":"markdown","a1cbbeed":"markdown","2744eeb0":"markdown","ecdca551":"markdown","904c46d3":"markdown","036d39da":"markdown","5057ced6":"markdown","6e16a2f2":"markdown","e2148472":"markdown","32c4ad8e":"markdown","bdadcbe8":"markdown","cefd82e2":"markdown"},"source":{"4161762b":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","1696c4e2":"# Importing all required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\nfrom sklearn.metrics import r2_score","7fe92260":"#Importing dataset\nbike = pd.read_csv('..\/input\/bike-sharing-dataset\/Bike_Sharing_BoomBikes_dataset.csv')\nbike.head()","bd979f7b":"# Check the descriptive Information in the dataset\nbike.info()","c8b75a04":"#Check the columns in the datset\nbike.columns","0a2bb19b":"# Check descriptive statistics in the dataset\nbike.describe()","21218a9d":"# Check the Shape of the dataset\nbike.shape","a8765cc4":"# Missing values check in the columns\nround(100*bike.isnull().sum()\/len(bike.index))","50eb6f39":"# Missing values check in the rows\nround(100* bike.isnull().sum(axis=1)\/len(bike.index))","ad7451fd":"# Create a dummy dataframe (copy of original bike df) for duplicate check\nbike_dp = bike\n\n# Checking for duplicates and dropping the entire duplicate row if any\nbike_dp.drop_duplicates(subset=None, inplace=True)\n\nbike_dp.shape","cdc8aa70":"# Checking the columns information before drop\nbike.columns","e788faac":"# Droping the unused features\nbike_1 = bike[['season', 'yr', 'mnth', 'holiday', 'weekday','workingday', 'weathersit', \n               'temp', 'hum', 'windspeed','cnt']]\nbike_1.info()","8a8a30b0":"# identify categorical variables\ncat_vars = ['season','mnth','holiday','weekday', 'workingday','weathersit']\n\n# identify numeric variables\nnum_vars = ['temp', 'hum','windspeed','cnt']","187f45a2":"# convert dtype of categorical variables\nbike_1[cat_vars] = bike_1[cat_vars].astype('category')","6b77859c":"# get insights of numeric variable\nbike_1.describe()","eb7a154a":"# get the insights of categorical variables\nbike_1.describe(include=['category'])","07ae0e5f":"# maped the season column according to descripttions\nbike_1['season'] = bike_1['season'].map({1:'spring', 2:'summer', 3:'fall', 4:'winter'})\n\n# maped the weekday column according to descriptin\nbike_1['weekday'] = bike_1['weekday'].map({0: 'Sun', 1: 'Mon', 2: 'Tue', 3: 'Wed', 4: 'Thu', 5: 'Fri', 6: 'Sat'})\n\n\n# maped mnth column values (1 to 12 ) as (jan to dec) respectively\nbike_1['mnth'] = bike_1['mnth'].map({1:'jan', 2:'feb', 3:'mar', 4:'apr', 5: 'may', 6: 'jun', 7: 'jul', 8: 'aug', 9: 'sep', 10: 'oct',\n                             11: 'nov', 12:'dec'})\n\n#  maped weathersit column\nbike_1['weathersit'] = bike_1['weathersit'].map({1: 'Clear_FewClouds', 2: 'Mist_Cloudy', 3: 'LightSnow_LightRain', 4: 'HeavyRain_IcePallets'})","2e940be6":"# Columns information\nbike_1.columns","b9738723":"# Check the data info before proceeding for analysis\nbike_1.info()","fb1a2d73":"# Visualizise the pattern of the target variable (demand) over period fo two years\nplt.figure(figsize=(15,5))\nplt.plot(bike_1.cnt)\nplt.show()","e5df81fb":"# selecting numerical variables\n\n# Box plot\ncol = 2\nrow = len(num_vars)\/\/col+1\n\nplt.figure(figsize=(12,8))\nplt.rc('font', size=12)\nfor i in list(enumerate(num_vars)):\n    plt.subplot(row, col, i[0]+1)\n    sns.boxplot(bike_1[i[1]])    \nplt.tight_layout()   \nplt.show()","a2b2e208":"##checking the pie chart distribution of categorical variables\nbike_piplot=bike_1[cat_vars]\nplt.figure(figsize=(15,15))\nplt.suptitle('pie distribution of categorical features', fontsize=20)\nfor i in range(1,bike_piplot.shape[1]+1):\n    plt.subplot(3,3,i)\n    f=plt.gca()\n    f.set_title(bike_piplot.columns.values[i-1])\n    values=bike_piplot.iloc[:,i-1].value_counts(normalize=True).values\n    index=bike_piplot.iloc[:,i-1].value_counts(normalize=True).index\n    plt.pie(values,labels=index,autopct='%1.1f%%')\n#plt.tight_layout()\nplt.show()","303f6850":"# Creating a new dataframe of numerical variables\nbike_num=bike_1[[ 'temp', 'hum', 'windspeed','cnt']]\n\nsns.pairplot(bike_num, diag_kind='kde')\nplt.show()","8b1cd792":"# checking dataset information \nbike_1.info()","78216d4a":"# Build boxplot of all categorical variables (before creating dummies) againt the target variable 'cnt' \n# to see how each of the predictor variable stackup against the target variable.\n\nplt.figure(figsize=(15, 8))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'season', y = 'cnt', data = bike_1)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'mnth', y = 'cnt', data = bike_1)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = bike_1)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'holiday', y = 'cnt', data = bike_1)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'weekday', y = 'cnt', data = bike_1)\nplt.subplot(2,3,6)\nsns.boxplot(x = 'workingday', y = 'cnt', data = bike_1)\n#plt.tight_layout(pad=1)\nplt.show()","5719f980":"# Check the correlation coefficients to see which variables are highly correlated.\n# (im considering only new dataframe variables (bike_1) that were chosen for analysis)\n\nsns.heatmap(bike_1.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","d0bfa55a":"# Check the datatypes before conversion of the variables\nbike_1.info()","a66fcd37":"# 1. Creating dummy variables\n# 2. Drop variable variables for which dummy was created\n# 3. Drop first dummy variable for each set of dummies created\n\nbike_1 = pd.get_dummies(bike_1, drop_first = True)\nbike_1.info()","37924573":"# Shape of the new data set\nbike_1.shape","43ff6ef2":"# We specify 'random_state' so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\ndf_train, df_test = train_test_split(bike_1, train_size = 0.70, test_size = 0.30, random_state = 333)","b5df857f":"# Checking the train dataset information\ndf_train.info()","e410c0ff":"# Checking the train dataset shape\ndf_train.shape","bdaf91f8":"# Checking the test dataset info\ndf_test.info()","2b9f3f8f":"# Checking the test dataset shape\ndf_test.shape","012848ff":"# instantiate and object\nscaler = MinMaxScaler()","260ea853":"#fit(): learns xmin, xmax\n#transform: (x-xmin)\/(xmax - xmin)\n#fit_transform ()","c9b820a7":"# Check vlaues before scaling\ndf_train.head()","8d570aeb":"# Checking Columns for infomration\ndf_train.columns","3baa0692":"# Apply scaler() to numerical variables\nnum_vars = ['temp','hum', 'windspeed','cnt']\n\n# Fit on data\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\ndf_train.head()","281009ad":"# After scaling checking descriptive statistics in the dataset\ndf_train.describe()","5a25b604":"# Check vlaues before scaling\ndf_test.head()","5e890e06":"# Checking Columns for infomration\ndf_test.columns","e75777f3":"# Apply scaler() to all numeric variables in test dataset. Note: we will only use scaler.transform, \n# as we want to use the metrics that the model learned from the training data to be applied on the test data. \n# In other words, we want to prevent the information leak from train to test dataset.\n\nnum_vars = ['temp', 'hum', 'windspeed','cnt']\n\ndf_test[num_vars] = scaler.transform(df_test[num_vars])","41ac414a":"# Check the test dataset\ndf_test.head()","dc710a83":"# Check the test dataset statistics\ndf_test.describe()","d2d710c4":"# Creating X and y data dataframe for train set\ny_train = df_train.pop('cnt')\nX_train =df_train\nX_train.head()","2c53eb2b":"# checking the y_train Data\ny_train.head()","7708679b":"# Creating X and y data dataframe for test set\ny_test = df_test.pop('cnt')\nX_test =df_test\nX_test.head()","13019933":"# checking the y_train Data\ny_test.head()","31b83917":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\n# running RFE\nrfe = RFE(lm, 15)             \nrfe = rfe.fit(X_train, y_train)","c6a36cb0":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","cf2e98f4":"# Select columns\ncol = X_train.columns[rfe.support_]\ncol","8534b9bf":"# Columns\nX_train.columns[~rfe.support_]","35f31e30":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","0a4044d1":"# Add a constant\nX_train_lm1 = sm.add_constant(X_train_rfe)\n\n# Create a first fitted model\nlr1 = sm.OLS(y_train, X_train_lm1).fit()\n\n#Check parameters \nlr1.params","6f8523dd":"# Summary of the linear regression model\nprint(lr1.summary())","b8a3f4e2":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","d68b62c1":"X_train_new = X_train_rfe.drop([\"mnth_may\"], axis = 1)","2b7fb143":"# Add a constant\nX_train_lm2 = sm.add_constant(X_train_new)\n\n# Create a fitted model\nlr2 = sm.OLS(y_train, X_train_lm2).fit()\n\n# Check parameters\nlr2.params","67ad2c0c":"# Summary of the linear regression model\nprint(lr2.summary())","0f953abb":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e311f7f7":"X_train_new = X_train_new.drop([\"hum\"], axis = 1)","5a555db5":"# Add a constant\nX_train_lm3 = sm.add_constant(X_train_new)\n\n# Create a fitted model\nlr3 = sm.OLS(y_train, X_train_lm3).fit()\n\n# Check parameters\nlr3.params","1376522f":"# Summary of the linear regression model\nprint(lr3.summary())","874212ae":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","42ea355e":"X_train_new = X_train_new.drop([\"season_fall\"], axis = 1)","696b9228":"# Add a constant\nX_train_lm4 = sm.add_constant(X_train_new)\n\n# Create a fitted model\nlr4 = sm.OLS(y_train, X_train_lm4).fit()\n\n# Check parameters\nlr4.params","9c72606e":"# Summary of the linear regression model\nprint(lr4.summary())","8a447786":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","bac25b09":"X_train_new = X_train_new.drop([\"mnth_oct\"], axis = 1)","d9e6891c":"# Add a constant\nX_train_lm5 = sm.add_constant(X_train_new)\n\n# Create a fitted model\nlr5 = sm.OLS(y_train, X_train_lm5).fit()\n\n# Check parameters\nlr5.params","403929a8":"# Summary of the linear regression model\nprint(lr5.summary())","b918cac2":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f6fda16d":"X_train_new = X_train_new.drop([\"mnth_mar\"], axis = 1)","34bb0841":"# Add a constant\nX_train_lm6 = sm.add_constant(X_train_new)\n\n# Create a fitted model\nlr6 = sm.OLS(y_train, X_train_lm6).fit()\n\n# Check parameters\nlr6.params","4f4851c2":"# Summary of the linear regression model\nprint(lr6.summary())","c1ebf660":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","a9660554":"# List down model varibales and its coefficients\n\n# assign final model to lm_final\nlm_final = lr6\n\n# list down and check variables of final model\nvar_final = list(lm_final.params.index)\nvar_final.remove('const')\nprint('Final Selected Variables:', var_final)\n\n# Print the coefficents of final varible\nprint('\\033[1m{:10s}\\033[0m'.format('\\nCoefficent for the variables are:'))\nprint(round(lm_final.params,3))","b42b5be4":"# predict traint dataset\ny_train_pred = lr6.predict(X_train_lm6)","e93f3303":"# Plot the histogram of the error terms\nres_train = y_train - y_train_pred\n\nfig = plt.figure()\nsns.distplot((res_train), bins = 20)\n# Plot heading\nfig.suptitle('Error Terms Train', fontsize = 20)                   \n# X-label\nplt.xlabel('Errors', fontsize = 18)                         ","b227da30":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure(figsize = (8,5))\nplt.scatter(y_train, res_train, alpha=.6)\n\n# Plot heading \nfig.suptitle('Error Terms Train', fontsize = 20)             \n\n# label\nplt.xlabel('y_train_pred', fontsize = 18)                          \nplt.ylabel('Residual', fontsize = 16)","1d53c323":"#Selecting the variables that were part of final model.\ncol1=X_train_new.columns\n\nX_test=X_test[col1]\n\n# Adding constant variable to test dataframe\nX_test_lm6 = sm.add_constant(X_test)\n\nX_test_lm6.info()","683871dc":"# predict test dataset\ny_test_pred = lr6.predict(X_test_lm6)","fc45bf54":"# Plot the histogram of the error terms\nres_test = y_test-y_test_pred\nplt.title('Error Terms Test', fontsize=16) \nsns.distplot(res_test)\nplt.show()","f426a17d":"# Get R-Squared fro test dataset\nr2_test = r2_score(y_test, y_pred = y_test_pred)\nprint('r2_test: ', round(r2_test,3))","8dcdc07b":"# We already have the value of R^2 (calculated in above step)\nr2=r2_test","506c0751":"# Get the shape of X_test\n\nX_test.shape","b3e59b19":"# n is number of rows in X\nn = X_test.shape[0]\n\n# Number of features (predictors, p) is the shape along axis 1\np = X_test.shape[1]\n\n# We find the Adjusted R-squared using the formula\nr2_test_adj = 1-(1-r2)*(n-1)\/(n-p-1)\nprint('r2_test_adj:', round(r2_test_adj,3))","8db94dd1":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, res_test, alpha=.5)\nfig.suptitle('Error Terms Test', fontsize = 20)             # Plot heading \nplt.xlabel('y_test_pred', fontsize = 18)                          # X-label\nplt.ylabel('Residual', fontsize = 16)","a2f7201e":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_test_pred)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_test_pred', fontsize = 16)      ","3ebb9d34":"# Print R Squared and adj. R Squared\nprint('R- Sqaured train: ', round(lm_final.rsquared,3), '  Adj. R-Squared train:', round(lm_final.rsquared_adj,3) )\nprint('R- Sqaured test : ', round(r2_test,2), '  Adj. R-Squared test :', round(r2_test_adj,3))","d3cf3149":"# Print the coefficents of final varible\nprint('\\033[1m{:10s}\\033[0m'.format('\\nCoefficent for the variables are:'))\nprint(round(lm_final.params,3))","43202119":"Dividing dataset into X and Y sets with respect to target variable 'cnt' for both train & test","75440f39":"### Model 6\n\nDropping the **'mnth_mar'** variable and Updating the Model","45fbb8bb":"Building Linear model using 'STATS MODEL':\n\nFit a regression line through the training data using `statsmodels`. Remember that in `statsmodels`, you need to explicitly fit a constant using `sm.add_constant(X)` because if we don't perform this step, `statsmodels` fits a regression line passing through the origin, by default.","c1491e51":"**Data Dictionary**:-\n\n  1. season : season_1:spring, season_2:summer, season_3:fall, season_4:winter\n  2. mnth : month ( 1 to 12)\n  3. weathersit : \n     - weathersit_1: Clear, Few clouds, Partly cloudy, Partly cloudy\n\t - weathersit_2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n\t - weathersit_3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\t - weathersit_4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n  4. weekday: day of the week \n     - ( weekday_1: Monday, weekday_2: Tuesday, weekday_3: Wednesday, weekday_4: Thursday, weekday_5: Friday, weekday_6: Saturday \n     \n \n \n`yr`, `holiday`, `workingday` are having binary values. So we will not map these columns","18e2d95c":"**Duplicate Check**","b001f2f2":"#### 2.1 Univariate Analysis","f906774a":"**Adjusted R^2 Value for TEST**\n\nFormula for Adjusted R^2","e916040d":"## Step 2: Data Visualisation\n\nWithout building any model or making any predictions, lets first look at the data by itself.\n\nI construct a data frame that summarizes the bike sharing cnt (count) base on the season, mnth, is it weekdays,  is it workingday, is it a holiday, and the type of weather, then calculating the mean of temperature, humidity, wind speed and other variables. \n\nThe purpose of this summarization is to find a general relationship between variables regardless of which year the data is from (since the data spans two years and the business is growing.)\n\nLet's now spend some time doing what is arguably the most important step - understanding the data.\n- If there is some obvious multicollinearity going on, this is the first place to catch it\n- Also to identify if some predictors directly have a strong association with the outcome variable\n","7bc523f5":"### Model 3\n\nDropping the **'hum'** variable and Updating the Model","0efd66a4":"### Model-2\n\nDropping the **'mnth_may'** variable and Updating the Model","d10391a0":"## Step 4: Data Modelling and Evaluation","d4ee3615":"#### Visualising Numeric Variables\n\nUsing the summarized data frame, we can visualize some of the features of the data and let's make a pairplot of all the numeric variables.\n","e85d6078":"### 3.4 Dividing into X and Y sets (for train & test set)","5c047f2c":"This time, we will be using the **LinearRegression function from SciKit Learn** for its compatibility with RFE (which is a utility from sklearn)","b2fcd3c1":"**INSIGHTS:-**\n\nThere were 6 categorical variables in the dataset. I used Box plot (refer the fig above) to study their effect on the dependent variable (`cnt`)\n\n- **SEASON**: High demand bike booking were happening in season_fall with a median of over 5000 booking. This was   followed by season_summer & season_winter with the total booking and very less demand in the spring season compared to others\n     - Indicates:- season can be a good predictor for the dependent variable\n     \n- **MNTH (MONTH)**: High demand bike booking were happening in the months june to sept with a median of over 4000 booking per month as compared to other months. \n     - Indicates: mnth(month) has some trend for bookings and can be a good predictor for the dependent variable*\n\n- **WEATHERSIT** : High demand bike booking were happening during weathersit_clear_fewclouds with a median of close to 5000 booking. This was followed by weathersit_MIst_cloudy with the total booking. \n     - Indicates:- weathersit does show some trend towards the bike bookings can be a good predictor for the dependent variable. \n\n- **HOLIDAY**: The bike booking were happening when it is not a holiday which means this data is clearly biased. \n    - Indicates:- holiday CANNOT be a good predictor for the dependent variable. \n    \n- **WEEKDAY**: The demand of the bike booking shows very close trend having their independent medians between 4000 to 5000. \n    - This variable can have some or no influence towards the predictor. Let the model will decide if this needs to be added or not. \n\n- **WORKINGDAY**: There is no significant change in bike demand with workign day and non working day.The bike booking were happening in \u2018workingday\u2019 with a median of close to 5000 booking. \n    - Indicates, workingday can be a good predictor for the dependent variable","5ca70a5f":"### 3.1 Create dummy variables for all the categorical vairables","955e2122":"#### Correlation Matrix","ef858079":"##### $R2adj.=1\u2212(1\u2212R2)\u2217 (n\u22121)\/(n\u2212p\u22121)$","4cedc95a":"### 4.2 Check the various assumptions","f4879dde":"## Step 3: Data Preparation","69f3f83c":"**INSIGHTS:**\n- We can colude that the model lr_6 fit isn't by chance, and has descent predictive power.","fa6522b8":"###### Test Dataset","e97a3489":"## Step 1: Reading and Understanding the Data","906c55a3":"**INSIGHTS:**\n- Observation from above boxplot, that two variables having few outliters\n    - hum \n    - windspeed","2eb5c513":"# Bike Sharing Demand (BoomBikes)\nPredicting bike sharing demand using Multiple Linear Regression model\n\n\n","ab2238b6":"It seems like the corresponding residual plot is reasonably random.","fdda0825":"### Model 5\n\nDropping the **'mnth_oct'** variable and Updating the Model","9dcec4cb":"#### Visualising Numeric Variables\n\nUsing the summarized data frame, we can visualize some of the features of the data and let's make a pairplot of all the numeric variables.\n","1c74e3bc":"**INSIGHTS:-**\n- We could see that the Residuals are normally distributed. Hence our assumption for Linear Regression is valid","1272fde8":"**3.4.1 Dividing into Xand Y sets for Test Set**","ba2b7e2d":"#### 2.2 Bi-Variate analysis","561c96c4":"**INSIGHTS:-**\n\nFrom correlation plot, we can observed that some features are positively correlated or some are negatively correlated to each other. The `temp` and `cnt` are highly positively correlated `(0.63)` to each other.\n\n- The heatmap clearly shows which all variable are multicollinear in nature, and which variable have high collinearity with the target variable\n- We will refer this map back-and-forth while building the linear model so as to validate different correlated values along with VIF & p-value, for identifying the correct variable to select\/eliminate from the model.","afc1965e":"**R^2 Value for TEST**","82e5eb7a":"> ###### Train R^2 :0.824 - Train Adjusted R^2 :0.821 \n> ###### Test R^2 :0.820 - Test Adjusted R^2 :0.812\n\nThis seems to be a **really good model** that can very well 'Generalize' various datasets","342dd487":"Now that we have fitted the model and checked the assumptions, it's time to go ahead and make predictions using the final model (lr6)","92b0e8c2":"## Step-5: Final Report","811e5000":"From R-Sqaured and adj R-Sqaured value of both train and test dataset we could conclude that the above variables can well explain more than 80% of bike demand.\n\nCoeffiencients of the variables explains the factors effecting the bike demand\n\n","7963b3e6":"**INSIGHT:**\n- `R2:0.829 and R2-adj:0.824`. All the variables are having `VIF <5`. Then we need to check `HIGH p-value` values where lessthan or eaual to `0.05`. \n- `mnth_oct` is insignificant in presence of other variables due to its HIGH p-value. so we can remove\/ drop this variable.","8a329b8b":"### 2.1 Performing EDA to understand various variables","ede19534":"Now that we have fitted the model and checked the assumptions, it's time to go ahead and make predictions using the final model (lr6)","8f863e35":"#### Import the required packages","1b557e6d":"**3.4.1 Dividing into Xand Y sets for Train Set**","71117397":"We will create DUMMY variables for 4 categorical variables 'mnth', 'weekday', 'season' & 'weathersit'.","718259e0":"**INSIGHTS:**\n\n- `mnth_may` is insignificant in presence of other variables due to its `HIGH p-values` (p-value >0.05). so we can remove\/ drop this variable\n- Once update the this vairable, we will check the VIF values for further updation of the model","703b12ea":"### 4.1 Create Linear Regression model using mixed approach","7b82bf26":"### 4.3 Check the Adjusted R-Square for both test and train data","6d13f115":"**Business Case:**\n\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n**Problem Statement:**   A US bike-sharing provider `BoomBikes` has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\nThe company wants to know:\n   - Which variables are significant in predicting the demand for shared bikes.\n   - How well those variables describe the bike demands\n\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. ","db660a8a":"**5.1. The equation of best fitted surface based on model lm_final**","5540666b":"**5.2 CONCLUSION:**\n\nAs per our final Model, the top 3 features contributing significantly towards explaining the demand of the shared bike\n\n- **Temperature (temp)** \n  - A coefficient value of \u20180.564\u2019 indicated that a unit increase in temp variable increases the demand\n- **weathersit_LightSnow_LightRain**\n  - A coefficient value of \u2018-0.307\u2019 indicated that, w.r.t Weathersit_Mist_cloudy, a unit increase in Weathersit_LightSnow_LightRain  variable decreases the demand\n- **Year (yr)**\n  - A coefficient value of \u20180.231\u2019 indicated that a unit increase in yr variable increases the bike demand \n  \n**SO IT IS RECOMMENDED TO GIVE THESE VARIABLES UTMOST IMPORTANCE WHILE PLANNING, TO ACHIEVE MAXIMUM DEBOOKING**","3a6c157d":"Based on the high level look at the data and the data dictionary, the following variables can be removed from further analysis:\n- instant : Index value, there is no use. so we can drop this column\n- dteday : This date columns, we have already seperate columns for 'year' & 'month', so we can drop this column. \n- casual & registered : casual + registered = cnt(count), we need total count of bikes are avaiable and not required specific category. so we can drop these two columns\n- temp and atemp are directly correlated among each other and We will use temp ","4b830664":"#### Model-1","541b0e9b":"Noticed, there are a few categorical variables as well. \nLet's make a boxplot for some of these variables to understand","fbe729df":"####  Visualising Catagorical Variables","be5c9a86":"**INSIGHTS**:-\n\n**Dataset has 730 rows and 16 columns**\n- All 15 columns are either float or integer type and One column is date type. \n- In the data, there seems to be some fields that are categorical in nature, but in integer\/float type.\n- Need to analyse & decide whether to convert them to categorical or treat as integer.","38e7ed90":"Checking the following below details in the dataset\n1. Missing values in the columns & Rows\n2. Duplicates","7dc8ae60":"In the Linear Regression, scaling doesn't impact on your model. It is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation.\n\nSo it is advised to use standardization or normalization so that the units of the coefficients obtained are all on the same scale. As you know, there are two common ways of rescaling:\n- Min-Max scaling (normalization): Between 0 and 1\n- Standardisation (mean-0, sigma-1) \n\nDuring EDA we could observe that there is different range of data in the data set. So it becomes important to scale the data. This time, we will use MinMax scaling.","6d77e60e":"**INSIGHTS:-**\n- It seems like the corresponding residual plot is reasonably random. \n- Also the error terms satisfies to have reasonably constant variance (homoscedasticity) ","6cb47617":"#### 1.2 Analysing and Cleaning of Unused Features","de1ddc32":"That there were NO DUPLICATE values in the dataset.","77f8bbe8":"**INSIGHTS:**\n- Seasons: Observation from pie chart- Business was operating similar days in all seasons.\n- Month: Observation from pie chart- Business was operating similar days in all months.\n- Holiday: Observation from pie chart-Business was operating in 2.9% days of holiday\n- weekdays: Observation from pie chart-Business was operating similar percentage in all weekdays.\n- Workingday: Observation from pie chart-Bisuness was operating in 68% in workign days and 31.6% in nonworking days.\n- Weathersit: Observation from pie chart- It is being observed that there is no data for 4th category of `weathersit i.e Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog`.May be the company is not operating on those days or there was no demand of bike.","4a646a90":"From the lm_final model summary, it is evident that all our coefficients are not equal to zero. which means We REJECT the NULL HYPOTHESIS\n\n**The final varibles are: 'yr','temp', 'windspeed', 'season_summer', 'season_winter', 'mnth_sep', 'weekday_Sat', 'workingday_1', 'weathersit_Mist_Cloudy', 'weathersit_LightSnow_LightRain'**","d55fe5fb":"### 3.2 Divide the data to train and test set\n\nDividing the dataset into two sets, Training (train) and Testing (test). \n\nTraining set will be used to train statistical models and estimate coefficients, while testing set will be used to validate the model we build with the training set. 70% of the complete data is partitioned into training set, sampled uniformly without replacement, and 30% is partitioned in to testing set. Sampling without replacement enables the model we build to extrapolate on the testing data, giving us a better sense of how our statistical models perform. \n- We will use train_test_split method from sklearn package","dba8a5c9":"#### 1.1 Data Quality Check","9f1be9d3":"Correlation matrix is tells about linear relationship between attributes and help us to build better models.","8cd18a94":"### Model 4\n\nDropping the **'season_fall'** variable and Updating the Model","b59ed7dd":"####  Residual Analysis \nError terms are normally distributed with mean zero (not X, Y)\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like","c2f537ac":"**INSIGHT:**\n- `R2:0.834 and R2-adj:0.830`. All the variables are having `p-value <0.05`. Then we need to check `HIGH VIF` values where morethan or eaual to `5`. \n- Identified three features where VIF value is high i.e: `temp -16.81` and `season_fall - 6.75`.\n- During the `EDA` analysis we found `temp` has a higly correlation with  `cnt` and based on general knowledge that temperature can be an important factor for a business like bike rentals, and wanted to retain 'tem\n- `season_fall` is insignificant in presence of other variables due to its HIGH VIF. so we can remove\/ drop this variable. \n- Even though the VIF of season_fall is second highest, we decided to drop 'season_fall' and not 'temp'.","9001862c":"#### Making Predictions using test dataset","02810806":"**Checking for Missing Values**","083f55ff":"$ cnt=0.084+(yr\u00d70.231)+(temp\u00d70.564)\u2212(windspeed\u00d70.155)+(season_summer\u00d70.083)+(season_winter\u00d70.129)+(mnth_sept\u00d70.095)+(weekday_sat\u00d70.057)+(workingday\u00d70.043)\u2212(weathersit_Mist_cloudy\u00d70.075)\u2212(weathersit_LightSnow_LightRain\u00d70.307)$","d1c5bc31":"**F Statistics**: F-Statistics is used for testing the overall significance of the Model: Higher the F-Statistics, more significant the Model is.\n  - F-statistic: 233.8\n  - Prob (F-statistic): 3.77e-181\n    \n\nThe F-Statistics value of 233 (which is greater than 1) and the p-value of '~0.0000' states that the overall model is significant","0a1d5685":"**Recusive feature elemination (RFE)**","7117ca32":"By preparing a correlation matrix, we can have a more straight forward view of what variables are strongly correlated and what is weakly correlated","635344d1":"**INSIGHTS:-**\n- Pair-Plot tells us that `temp` has highest positive correlation with target varaible(`cnt`) ","50d7b572":"###### Train Dataset","ff9abf5d":"Noticed, there are a few categorical variables as well. \nLet's make a boxplot for some of these variables to understand","ad32d974":"**INSIGHT:**\n- `R2:0.824 and R2-adj:0.821`. All the values are **within permissible range of below 5**\n- From the VIF calculation we could find that there is no multicollinearity existing between the predictor variables.\n\nThis model looks good, as there seems to be VERY LOW Multicollinearity between the predictors and the p-values for all the predictors seems to be significant\n- For now, we will consider this as our final model, unless the Test data metrics are not significantly close to this number","45ef158c":"**3.3.2 Testing Dataset Scalling** ","eb900125":"### Building a linear model\n\nFit a regression line through the training data using statsmodels. Remember that in statsmodels, need to explicitly fit a constant using sm.add_constant(X) because if we don't perform this step, statsmodels fits a regression line passing through the origin, by default","3da78c83":"**Hypothesis Testing**: It states that\n- $H0:B1=B2=...=Bn=0$\n- $H1: at least one Bi!=0$\n\nIt is evident that all our coefficients are not equal to zero, which means We REJECT the NULL HYPOTHESIS based on the final model","efd55b52":"**INSIGHTS:**\n\n- `R2:0.842 and R2-adj:0.837`. All the variables are having `p-value <0.05`. Then we need to check `HIGH VIF` values where morethan or eaual to `5`. \n- Identified three features where VIF value is high i.e: `temp -23.21`, `hum- 17.23` and `season_fall - 7.01`.\n- During the `EDA` analysis we found `temp` has a higly correlation with  `cnt` and based on general knowledge that temperature can be an important factor for a business like bike rentals, and wanted to retain 'tem\n- `hum` is insignificant in presence of other variables due to its `HIGH VIF`. so we can remove\/ drop this variable","d9124c1d":"### 3.3 Performing Scalling","68d2a2ec":"**INSIGHTS:**\n- Observed, there was growth over the period and recently we could see there is reduction in demand","8e306be9":"Preparing Linear Regression model using mixed approach","c5deeeea":"**RESULT COMPARISION**","a1cbbeed":"There is no missing values in the rows","2744eeb0":"**3.3.1 Training Dataset Scalling**","ecdca551":"### 2.2 Checking the correlation between the variables","904c46d3":"#### Business Goal:\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. ","036d39da":"**Final Model Variables:**","5057ced6":"There is no missing values in the columns","6e16a2f2":"Before diving into building statistical models:\n1. Create dummy variables for all the categorical vairables\n2. Divide the dataset into two sets, training and testing. \n3. performing scaling \n4. Dividing into X and Y sets for the model building","e2148472":"####  Visualising Catagorical Variables","32c4ad8e":"**INSIGHTS**-\n- We could see that the Residuals are normally distributed. Hence our assumption for Linear Regression is valid","bdadcbe8":"**INSIGHT:**\n- After dropping the variable `mnth_oct` there is no significant change in R-squared or adj. R2-squared. So decissionto drop the varibale is correct. \n- `R2:0.827 and R2-adj:0.823`. All the variables are having VIF Value < 5 and need to check 'p-value' \n- `mnth_mar` is having highest p-value. We will drop the variable, update the model and see the any impact in R-squared.","cefd82e2":"#### Making Predictions using test dataset"}}