{"cell_type":{"2a653348":"code","1a0415d1":"code","60e671b8":"code","7969fbc6":"code","3b08e406":"code","0babbc9d":"code","17e539f2":"code","e55122a3":"code","6b76412b":"code","eea8b1ff":"code","638e96b1":"code","131b5a54":"code","62940a60":"code","5cf4f125":"code","8d49a2b4":"code","fedd44cd":"code","94aeddc9":"code","386ecc4c":"code","297b79bd":"code","81c6d85a":"code","7275eb88":"code","ccbf0c24":"code","b796471a":"markdown","215c57a6":"markdown","c9be7a43":"markdown","6ff44adf":"markdown","efe8f496":"markdown","4be84004":"markdown","09a316cd":"markdown","777a90e6":"markdown","bad5f193":"markdown","ab5d0cb6":"markdown","d3f60ce1":"markdown","b2a5a2cb":"markdown","597b3163":"markdown","03ef0046":"markdown","8611284d":"markdown","c9a5aff3":"markdown"},"source":{"2a653348":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import *\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a0415d1":"train_data_full = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv') \nsubmission_data = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv') \nPRETRAINED_DIR = '..\/input\/roberta-transformers-pytorch\/distilroberta-base\/'","60e671b8":"train_data_full = train_data_full.fillna('')","7969fbc6":"train_data, val_data = train_test_split(train_data_full, test_size=0.1)\ntrain_data = train_data.reset_index(drop=True)\nval_data = val_data.reset_index(drop=True)","3b08e406":"###########################\n###########################\n\n### TEST ENV\n\n# train_data = train_data.head()\n# val_data = val_data.head()\n# test_data = test_data.head()\n\n###########################\n###########################","0babbc9d":"MAX_LEN = 96\nBATCH_SIZE = 32\nEPOCHS_NUM = 4\nDROPOUT_RATE = 0.2","17e539f2":"tokenizer = RobertaTokenizer.from_pretrained(PRETRAINED_DIR, lowercase=True, add_prefix_space=True)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","e55122a3":"# train set\ntrain_data_shape = train_data.shape[0]\ninput_ids = np.ones((train_data_shape,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((train_data_shape,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((train_data_shape,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((train_data_shape,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((train_data_shape,MAX_LEN),dtype='int32')\n\n# val set\nval_data_shape = val_data.shape[0]\ninput_ids_val = np.ones((val_data_shape,MAX_LEN),dtype='int32')\nattention_mask_val = np.zeros((val_data_shape,MAX_LEN),dtype='int32')\ntoken_type_ids_val = np.zeros((val_data_shape,MAX_LEN),dtype='int32')\nstart_tokens_val = np.zeros((val_data_shape,MAX_LEN),dtype='int32')\nend_tokens_val = np.zeros((val_data_shape,MAX_LEN),dtype='int32')\n\n# test set\ntest_data_shape = test_data.shape[0]\ninput_ids_test = np.ones((test_data_shape,MAX_LEN),dtype='int32')\nattention_mask_test = np.zeros((test_data_shape,MAX_LEN),dtype='int32')\ntoken_type_ids_test = np.zeros((test_data_shape,MAX_LEN),dtype='int32')\n\n# predykcja i walidacja\njac = [];\npreds_start = np.zeros((input_ids_test.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_test.shape[0],MAX_LEN))","6b76412b":"for k in range(train_data_shape):\n    text1 = \" \"+\" \".join(train_data.loc[k,'text'].split())\n    text2 = \" \".join(train_data.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    offsets = []; idx=0\n    for t in enc:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_data.loc[k,'sentiment']]\n    input_ids[k,:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","eea8b1ff":"for k in range(val_data_shape):\n    text1 = \" \"+\" \".join(val_data.loc[k,'text'].split())\n    text2 = \" \".join(val_data.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    offsets = []; idx=0\n    for t in enc:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[val_data.loc[k,'sentiment']]\n    input_ids_val[k,:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n    attention_mask_val[k,:len(enc)+5] = 1\n    if len(toks)>0:\n        start_tokens_val[k,toks[0]+1] = 1\n        end_tokens_val[k,toks[-1]+1] = 1","638e96b1":"for k in range(test_data_shape):\n    text1 = \" \"+\" \".join(test_data.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_data.loc[k,'sentiment']]\n    input_ids_test[k,:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n    attention_mask_test[k,:len(enc)+5] = 1","131b5a54":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","62940a60":"K.clear_session()\n\nids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\nmask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\ntokens = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\nconfig = RobertaConfig.from_pretrained(PRETRAINED_DIR)\nbert_model = TFRobertaModel.from_pretrained(PRETRAINED_DIR,config=config, from_pt=True)\nx = bert_model(ids,attention_mask=mask,token_type_ids=tokens)\nx1 = tf.keras.layers.Dropout(DROPOUT_RATE)(x[0])\nx1 = tf.keras.layers.BatchNormalization()(x1)\nx1 = tf.keras.layers.Conv1D(1,1)(x1)\nx1 = tf.keras.layers.Flatten()(x1)\nx1 = tf.keras.layers.Activation('softmax')(x1)\nx2 = tf.keras.layers.Dropout(DROPOUT_RATE)(x[0]) \nx2 = tf.keras.layers.BatchNormalization()(x2)\nx2 = tf.keras.layers.Conv1D(1,1)(x2)\nx2 = tf.keras.layers.Flatten()(x2)\nx2 = tf.keras.layers.Activation('softmax')(x2)\nmodel = tf.keras.models.Model(inputs=[ids, mask, tokens], outputs=[x1,x2])\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)","5cf4f125":"callbacks = tf.keras.callbacks.ModelCheckpoint('saved_model.h5', \n                                        monitor='val_loss', \n                                        verbose=1, \n                                        save_best_only=True, \n                                        save_weights_only=True, \n                                        save_freq='epoch')\n\nhistory = model.fit([input_ids, attention_mask, token_type_ids], [start_tokens, end_tokens], \n    epochs=EPOCHS_NUM, \n    batch_size=BATCH_SIZE, \n    verbose=1, \n    callbacks=[callbacks],\n    validation_data=([input_ids_val, attention_mask_val, token_type_ids_val], \n    [start_tokens_val, end_tokens_val]))","8d49a2b4":"model.load_weights('saved_model.h5')","fedd44cd":"preds_train = model.predict([input_ids,attention_mask,token_type_ids],verbose=1)\npreds_start_train = preds_train[0]\npreds_end_train = preds_train[1]","94aeddc9":"all_train = []\nall_jac = []\nfor k in range(input_ids.shape[0]):\n    a = np.argmax(preds_start_train[k,])\n    b = np.argmax(preds_end_train[k,])\n    if a>b: \n        st = train_data.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(train_data.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc[a:b+1])\n    all_train.append(st)\n    all_jac.append(jaccard(st,train_data.loc[k,'selected_text']))\nprint('Jaccard = {}'.format(np.mean(all_jac)))","386ecc4c":"preds = model.predict([input_ids_test,attention_mask_test,token_type_ids_test],verbose=1)\npreds_start = preds[0]\npreds_end = preds[1]","297b79bd":"all_test = []\nfor k in range(input_ids_test.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_data.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test_data.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc[a:b])\n    all_test.append(st)","81c6d85a":"submission_data['selected_text'] = all_test","7275eb88":"submission_data.head()","ccbf0c24":"submission_data.to_csv('submission.csv', index=False)","b796471a":"# Meta parametry","215c57a6":"# Wydzielenie zbioru walidacyjnego","c9be7a43":"# Douczenie modelu","6ff44adf":"# Scie\u017cki i wczytanie","efe8f496":"# Wczytanie distilberta i dodanie do architekutury ostatnich warstwy","4be84004":"# Tokenizowanie danych walidacyjnych","09a316cd":"# Wczytanie najlepszego modelu","777a90e6":"# Tokenizowanie danych treningowych","bad5f193":"# Predykcja dla danych testowych (submission)","ab5d0cb6":"# Predykcja na danych testowych do spradzenia J-score","d3f60ce1":"# Inicjalizacja tablic","b2a5a2cb":"# Szybki test env do sprawdzania sk\u0142adni przed commitem","597b3163":"# Submission","03ef0046":"Thanks to Chris Deotte for his amazing (https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705), which inspired my distilbert attempt, and to Abhishek Thakur for the tokenization (https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds)","8611284d":"# Wczytanie tokenizera","c9a5aff3":"# Tokenizowanie danych testowych (submission)"}}