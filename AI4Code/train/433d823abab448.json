{"cell_type":{"04713507":"code","5adfa965":"code","869d5bff":"code","5dbdf159":"code","7b983aff":"code","17e05f81":"code","fb1923a2":"code","7f44b67c":"code","c65b5bb1":"code","a6d704c4":"code","9654696f":"code","5e9bda5e":"code","3c6fa36b":"code","056354bf":"code","35732017":"code","f5bd65c4":"code","f48b2c0e":"code","544fc87e":"code","19b6d989":"code","17099d0f":"code","d3c93012":"code","af182693":"code","d05b604a":"code","28481952":"code","a280ad59":"code","d94ab85e":"code","ca110fdb":"code","f81eb205":"code","1ac82aa7":"code","16f57195":"code","074be1af":"code","6964d2f1":"code","ef43d654":"code","842dfee4":"code","a6aa5684":"code","b132851d":"code","a033323d":"code","44edddfd":"code","51130108":"code","5f3ad45b":"code","9dc1c963":"code","17dcb364":"code","4bf8428f":"code","2128120d":"code","36a86d79":"code","13a1c76e":"code","a2e2e0d5":"code","c0dbaf0d":"code","8fa85922":"code","f42526cc":"code","8e8d2a01":"code","fe1f4ced":"code","5dbdde98":"code","c709708e":"code","e085bddc":"markdown","a5e7f7a8":"markdown","a62a9b06":"markdown","8b932f34":"markdown","d0a899de":"markdown","ed05146f":"markdown","e3223139":"markdown","f6f200da":"markdown","188a6f16":"markdown","e8741cc0":"markdown","ea92e843":"markdown","34a4e7b2":"markdown","f8f84dc3":"markdown","99dea0f0":"markdown","569e71e4":"markdown","c75aadc1":"markdown","102a3710":"markdown"},"source":{"04713507":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5adfa965":"import nltk\nfrom nltk.corpus import stopwords\nimport re\nimport joblib\nfrom keras.preprocessing.text import Tokenizer\nimport gensim\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.layers import Embedding\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Dropout\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report","869d5bff":"df = pd.read_csv(\"\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\",encoding='latin-1',header=None)","5dbdf159":"df.head()","7b983aff":"columns=['target','ids','date','flag','user','text']\ndf.columns=columns","17e05f81":"df.head()","fb1923a2":"df.target.replace({0:'Negative',2:'Neutral',4:'Positive'},inplace=True)","7f44b67c":"df.head()","c65b5bb1":"stop_words=set(stopwords.words('english'))\nstop_words.remove('not')","a6d704c4":"\ncorpus=[]\nfor i in range(0,len(df)):\n    review=re.sub('@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+',' ',df['text'][i])\n    review=review.lower()\n    review=review.split()\n    review=[word for word in review if not word in stop_words]\n    review=' '.join(review)\n    corpus.append(review)","9654696f":"df.text=corpus","5e9bda5e":"df.head()","3c6fa36b":"from sklearn.model_selection import train_test_split\ntrain_df,test_df=train_test_split(df,test_size=0.20,random_state=123)","056354bf":"train_df.head()","35732017":"test_df.head()","f5bd65c4":"documents = [text.split() for text in train_df.text]","f48b2c0e":"w2v_model = gensim.models.word2vec.Word2Vec(size=300, \n                                            window=7, \n                                            min_count=10, \n                                            workers=8)","544fc87e":"w2v_model.build_vocab(documents)","19b6d989":"words = w2v_model.wv.vocab.keys()\nvocab_size = len(words)\nprint(\"Vocab size\", vocab_size)","17099d0f":"w2v_model.train(documents, total_examples=len(documents), epochs=30)","d3c93012":"w2v_model.wv.most_similar(\"good\")","af182693":"w2v_model.wv.most_similar(\"hate\")","d05b604a":"w2v_model.wv.most_similar(\"great\")","28481952":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_df.text)","a280ad59":"tokenizer.word_index","d94ab85e":"vocab_size=len(tokenizer.word_index)+1\nvocab_size","ca110fdb":"X_train = pad_sequences(tokenizer.texts_to_sequences(train_df.text), maxlen=300)\nX_train","f81eb205":"X_test = pad_sequences(tokenizer.texts_to_sequences(test_df.text), maxlen=300)\nX_test","1ac82aa7":"y_train=train_df.target\ny_train.head()","16f57195":"y_test=test_df.target\ny_test.head()","074be1af":"labelencoder = LabelEncoder()\ny_train = labelencoder.fit_transform(y_train)\ny_test=labelencoder.fit_transform(y_test)","6964d2f1":"y_train.shape","ef43d654":"y_test.shape","842dfee4":"embedding_matrix = np.zeros((vocab_size, 300))\nfor word, i in tokenizer.word_index.items():\n  if word in w2v_model.wv:\n    embedding_matrix[i] = w2v_model.wv[word]\nprint(embedding_matrix.shape)","a6aa5684":"embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)","b132851d":"model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","a033323d":"model.compile(loss='binary_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])","44edddfd":"model_history=model.fit(X_train, y_train,batch_size=1024,epochs=15,validation_split=0.1,verbose=1)","51130108":"acc = model_history.history['accuracy']\nval_acc = model_history.history['val_accuracy']\nloss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\nepochs=range(len(acc))","5f3ad45b":"plt.plot(epochs,acc,label='Trainin_acc',color='blue')\nplt.plot(epochs,val_acc,label='Validation_acc',color='red')\nplt.legend()\nplt.title(\"Training and Validation Accuracy\")","9dc1c963":"plt.plot(epochs,loss,label='Training_loss',color='blue')\nplt.plot(epochs,val_loss,label='Validation_loss',color='red')\nplt.legend()\nplt.title(\"Training and Validation loss\")","17dcb364":"def preprocess(text):\n    review=re.sub('@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+',' ',text)\n    review=review.lower()\n    review=review.split()\n    review=[word for word in review if not word in stop_words]\n    print(review)\n    review=pad_sequences(tokenizer.texts_to_sequences([review]), maxlen=300)\n    return review","4bf8428f":"def prediction(review):\n    review=preprocess(review)\n    score=model.predict(review)\n    score=score[0]\n    if score<0.4:\n        print(\"Negative\")\n    elif score>0.4 and score<0.6:\n        print(\"Neutral\")\n    else:\n        print(\"Positive\")\n    print(score)\n","2128120d":"prediction(\"the food is not bad\")","36a86d79":"prediction(\"the film was horrible\")","13a1c76e":"scores = model.predict(X_test, verbose=1, batch_size=1024)","a2e2e0d5":"scores","c0dbaf0d":"y_pred=np.where(scores>0.5,1,0)","8fa85922":"y_pred","f42526cc":"y_test","8e8d2a01":"\ncm=confusion_matrix(y_pred,y_test)\nprint(cm)","fe1f4ced":"print(accuracy_score(y_pred,y_test))","5dbdde98":"print(classification_report(y_test, y_pred))","c709708e":"joblib.dump(w2v_model,'word2vec.pkl')\njoblib.dump(tokenizer,'tokenizer.pkl')\njoblib.dump(model,'final_model.pkl')","e085bddc":"**Evaluation Using Confusion Matrix, accuracy_score and classification report**","a5e7f7a8":"**Preprocessing of tweets given by user**","a62a9b06":"**Word2Vec Model**","8b932f34":"**Fitting the Model**","d0a899de":"**Replacing the text column with preprocessed text**","ed05146f":"**Reading the Dataset**","e3223139":"**Replacing the target Values by Positive,Negative and Neutral**","f6f200da":"**Splitting the Data into Training and Test set**","188a6f16":"**Encoding the Categorical target into 0 and 1**","e8741cc0":"**Embedding Matrix**","ea92e843":"**In the Dataset we can see that there is no column names present so we add the column names of the Data.**","34a4e7b2":"**Data Preprocessing**\n\n1. Tweets contains a lot of emoticons, abbreviations and creative ways of expressing excitment such as long tailing (ex. happyyyy). We normalize\n   all letters to lowercase and remove any \u201d@USERNAME\u201d and \u201d#hashtag\u201d because they did not affect the sentiment of text.\n   \n2. Removing Stopwords\n3. Stemming\n","f8f84dc3":"**Saving the Trained Models**","99dea0f0":"**Prediction**","569e71e4":"**Importing Libraries and Packages**","c75aadc1":"**Build Model using LSTM**","102a3710":"**Compile Model**"}}