{"cell_type":{"41211699":"code","e7d08d66":"code","55768795":"code","5e9000b8":"code","1763e657":"code","e6e1e1c6":"code","ff0e3fe8":"code","a60f65fc":"code","9b64cee6":"code","f965c036":"code","58c9156d":"code","e2072115":"code","18aa74d4":"code","0dbde0ba":"code","01d1c9fb":"code","09c18a06":"markdown","2a92e454":"markdown","d4f8b876":"markdown","123d26a1":"markdown","6d7f6d9a":"markdown","7cff0065":"markdown","cbf6551d":"markdown","eb111e05":"markdown","ba84d8b1":"markdown","e698bb4d":"markdown","b989b979":"markdown","606f89d6":"markdown","85310706":"markdown"},"source":{"41211699":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport gc\n\npd.set_option('display.max_columns', 200)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e7d08d66":"# Lets import data sets\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","55768795":"print(train_df.head())\nprint(test_df.head())","5e9000b8":"predictors=train_df.columns[2:]\npredictors","1763e657":"pos=predictors[train_df[predictors].mean()>0]\nneg=predictors[train_df[predictors].mean()<=0]","e6e1e1c6":"idx = features = pos\nfor df in [train_df, test_df]:\n    df['sum_pos'] = df[idx].sum(axis=1)  \n    df['min_pos'] = df[idx].min(axis=1)\n    df['max_pos'] = df[idx].max(axis=1)\n    df['mean_pos'] = df[idx].mean(axis=1)\n    df['std_pos'] = df[idx].std(axis=1)\n    df['skew_pos'] = df[idx].skew(axis=1)\n    df['kurt_pos'] = df[idx].kurtosis(axis=1)\n    df['med_pos'] = df[idx].median(axis=1)","ff0e3fe8":"idx = features = neg\nfor df in [train_df, test_df]:\n    df['sum_neg'] = df[idx].sum(axis=1)  \n    df['min_neg'] = df[idx].min(axis=1)\n    df['max_neg'] = df[idx].max(axis=1)\n    df['mean_neg'] = df[idx].mean(axis=1)\n    df['std_neg'] = df[idx].std(axis=1)\n    df['skew_neg'] = df[idx].skew(axis=1)\n    df['kurt_neg'] = df[idx].kurtosis(axis=1)\n    df['med_neg'] = df[idx].median(axis=1)","a60f65fc":"train_df.head()","9b64cee6":"test_df.head()","f965c036":"param = {\n    'num_leaves': 25,\n     'max_bin': 60,\n     'min_data_in_leaf': 5,\n     'learning_rate': 0.010614430970330217,\n     'min_sum_hessian_in_leaf': 0.0093586657313989123,\n     'feature_fraction': 0.056701788569420042,\n     'lambda_l1': 0.060222413158420585,\n     'lambda_l2': 4.6580550589317573,\n     'min_gain_to_split': 0.29588543202055562,\n     'max_depth': 50,\n     'save_binary': True,\n     'seed': 1234,\n     'feature_fraction_seed': 1234,\n     'bagging_seed': 1234,\n     'drop_seed': 1234,\n     'data_random_seed': 1234,\n     'objective': 'binary',\n     'boosting_type': 'gbdt',\n     'verbose': 1,\n     'metric': 'auc',\n     'is_unbalance': True,\n     'boost_from_average': False\n}","58c9156d":"# number of folds\nnfold = 10 ","e2072115":"target = 'target'\npredictors = train_df.columns.values.tolist()[2:]","18aa74d4":"skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)\n\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nfold {}\".format(i))\n    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n                           label=train_df.iloc[train_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n                           label=train_df.iloc[valid_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    nround = 8000\n    clf = lgb.train(param, xg_train, nround, valid_sets = [xg_valid], verbose_eval=250)\n    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=nround) \n    \n    predictions += clf.predict(test_df[predictors], num_iteration=nround) \/ nfold\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.4f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))","0dbde0ba":"submission = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsubmission[\"target\"] = predictions\nsubmission[:10]","01d1c9fb":"submission.to_csv(\"LGMB_210_featutes.csv\", index=False)","09c18a06":"Lets extract predictors for some feature engineering","2a92e454":"Lets add features derived from row-wise summary of existing features with negative mean","d4f8b876":"Lets have a look of our data sets","123d26a1":"Now lets define parameters for LGBM","6d7f6d9a":"Lets submit our predictions","7cff0065":"Hope my work helps some kagglers to improve their solutions...\n### Thanks a lot","cbf6551d":"In this notebook, I will be using LGBM for prediction of our target variable. \nMost of the kagglers tried creating features from row-wise summary of existing features.\nI continued with the same approach but not for all features, some based on features with positive \nmean and some based on those features with negative mean...Let's start..\n","eb111e05":"Define our target variable and predictors","ba84d8b1":"Lets have a look of our data sets","e698bb4d":"Lets add features derived from row-wise summary of existing features with positive mean","b989b979":"Lets build model using LGBM","606f89d6":"Lets separate features with positive mean and negative mean values","85310706":"Create submission file and have a look of it..."}}