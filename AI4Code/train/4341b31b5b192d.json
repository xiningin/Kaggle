{"cell_type":{"150f8502":"code","b465ac0e":"code","70869c74":"code","ef5631fb":"code","df31c6e9":"code","e3edb48d":"code","2a5f3cf5":"code","671f7f56":"code","89417658":"code","82e00f43":"code","c6c37577":"code","ba3ac2bc":"code","12274435":"code","a7bc67e1":"code","caf0bebe":"code","6a4d7c3a":"code","20b87081":"code","d52b8fc3":"code","a71c0aba":"code","fd900f63":"code","bca64272":"markdown","6189db78":"markdown","6672db20":"markdown","68b792fd":"markdown"},"source":{"150f8502":"!pip install h3","b465ac0e":"import pandas as pd\nimport numpy as np\n\n# import seaborn as sns\n# import matplotlib\n#\n# from matplotlib import pyplot as plt\n\nimport gc\nimport math\nimport random\nimport mmh3\n\nfrom h3 import h3\n\nimport gzip\nfrom tqdm import tqdm\n\npd.set_option('display.max_columns', 250)  # or 1000\n# pd.set_option('display.max_rows', None)  # or 1000\n# pd.set_option('display.max_colwidth', -1)  # or 199","70869c74":"CITY = [\n    'Atlanta',\n    'Boston',\n    'Chicago',\n    'Philadelphia'\n]\n\nINPUTS = [\n    'RowId',\n    'IntersectionId',\n    'Latitude',\n    'Longitude',\n    'EntryStreetName',\n    'ExitStreetName',\n    'EntryHeading',\n    'ExitHeading',\n    'Hour',\n    'Weekend',\n    'Month',\n    'Path',\n    'City'\n]\n\nOUTPUTS = [\n    'TotalTimeStopped_p20',\n    'TotalTimeStopped_p40',\n    'TotalTimeStopped_p50',\n    'TotalTimeStopped_p60',\n    'TotalTimeStopped_p80',\n    'TimeFromFirstStop_p20',\n    'TimeFromFirstStop_p40',\n    'TimeFromFirstStop_p50',\n    'TimeFromFirstStop_p60',\n    'TimeFromFirstStop_p80',\n    'DistanceToFirstStop_p20',\n    'DistanceToFirstStop_p40',\n    'DistanceToFirstStop_p50',\n    'DistanceToFirstStop_p60',\n    'DistanceToFirstStop_p80'\n]\n\nLABELS = [\n    \"TotalTimeStopped_p20\",\n    \"TotalTimeStopped_p50\",\n    \"TotalTimeStopped_p80\",\n    \"DistanceToFirstStop_p20\",\n    \"DistanceToFirstStop_p50\",\n    \"DistanceToFirstStop_p80\"\n]","ef5631fb":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MaxAbsScaler\nfrom sklearn.impute import SimpleImputer\n","df31c6e9":"N_BINS = 10000\n\ndef get_id(value, n_components):\n    return mmh3.hash(value, signed=False) % n_components","e3edb48d":"h3_precision = 7\ncompass_brackets = {\n    \"N\": 0,\n    \"NE\": 1,\n    \"E\": 2,\n    \"SE\": 3,\n    \"S\": 4,\n    \"SW\": 5,\n    \"W\": 6,\n    \"NW\": 7\n}\n\nmissing_values = {\n    'EntryStreetName': 'MISSING',\n    'ExitStreetName': 'MISSING'\n}\n\nroad_encoding = {\n    'Road': 1,\n    'Street': 2,\n    'Avenue': 2,\n    'Drive': 3,\n    'Broad': 3,\n    'Boulevard': 4\n}\n\ncity_center_encoding = {\n    'Atlanta':  (math.radians(33.753746), math.radians(-84.386330)),\n    'Boston': (math.radians(42.361145), math.radians(-71.057083)),\n    'Chicago': (math.radians(41.881832), math.radians(-87.623177)),\n    'Philadelphia': (math.radians(39.952583), math.radians(-75.165222))\n}\n\nR = 6371  # radius of the earth in km\ndef get_geo_distance(lat1, lng1, lat2, lng2):\n    x = (lng2 - lng1) * math.cos(0.5*(lat2 + lat1))\n    y = lat2 - lat1\n    return R * math.sqrt(x*x + y*y)\n\ndef get_distance_from_city_center(lat1, lng1, city):\n    # already in radians, as stored as such in the hashset\n    lat2, lng2 = city_center_encoding[city]\n    \n    return get_geo_distance(math.radians(lat1), math.radians(lng1), lat2, lng2)\n\ndef get_road_type(street_name):\n    for road_type in road_encoding.keys():\n        if road_type in street_name:\n            return road_encoding[road_type]\n    return 0\n\ndef compute_rotation(entry_heading, exit_heading):\n    entry_idx = compass_brackets[entry_heading]\n    exit_idx = compass_brackets[exit_heading]\n    \n    return exit_idx - entry_idx + 8 if entry_idx > exit_idx else exit_idx - entry_idx\n\ndef get_hour_group(x):\n    if x < 8:\n         return \"midnight\"\n    elif x < 12:\n         return \"morning\"\n    elif x < 16:\n         return \"afternoon\"\n    elif x < 19:\n         return \"evening\"\n    else:\n         return \"midnight\"\n\ndef transform_dataframe(df, xf=None):\n    df = df.copy().fillna(value=missing_values)\n    \n    print('build distance from center')\n    intersections = df[['City', 'IntersectionId', 'Latitude', 'Longitude']].drop_duplicates()\n    intersections['DistanceFromCenter'] = intersections.apply(lambda x: get_distance_from_city_center(x[2], x[3], x[0]), axis=1)\n    \n    df = pd.merge(df, intersections[['City', 'IntersectionId', 'DistanceFromCenter']], on=['City', 'IntersectionId'])\n    \n    print('remove rare months')\n    df.loc[df['Month'] == 1, 'Month'] = 12\n    df.loc[df['Month'] == 5, 'Month'] = 6\n    \n    print('build cross-features')\n    df['f1'] = df['EntryStreetName'].map(lambda x: get_id(x, N_BINS))\n    df['f2'] = df['ExitStreetName'].map(lambda x: get_id(x, N_BINS))\n    df['f5'] = (df['City'] + ':' + df['IntersectionId'].map(str)).map(lambda x: get_id(x, N_BINS))\n    df['f7'] = df[['Latitude', 'Longitude']].apply(lambda x: h3.geo_to_h3(x[0], x[1], h3_precision), axis=1).map(lambda x: get_id(x, N_BINS))\n    df['f8'] = (df['EntryStreetName'] + ':' + df['ExitStreetName']).map(lambda x: get_id(x, N_BINS))\n    df['HourGroup'] = df['Hour'].map(lambda x: get_hour_group(x))\n    \n    df['f3'] = df['EntryHeading'] + ':' + df['ExitHeading']\n    df['f4'] = df[['EntryHeading', 'ExitHeading']].apply(lambda x: compute_rotation(x[0], x[1]), axis=1)\n    df['f6'] = df['Hour'].map(str) + ':' + df['Weekend'].map(str)\n    \n    df['EntryType'] = df['EntryStreetName'].map(lambda x: get_road_type(x))\n    df['ExitType'] = df['ExitStreetName'].map(lambda x: get_road_type(x))\n      \n    df[\"SameStreetExact\"] = (df[\"EntryStreetName\"] == df[\"ExitStreetName\"]).astype(int)\n    \n    if xf is None:\n        y = df[LABELS].values\n        df = df.drop(columns=OUTPUTS)\n        \n        print('build transformers')\n        \n        xf = ColumnTransformer(transformers=[\n            ('cat_02', OneHotEncoder(sparse=False, handle_unknown='ignore'), ['EntryHeading', 'ExitHeading', 'City', 'Month', 'f3', 'f4', 'f6', 'EntryType', 'ExitType', 'HourGroup']),\n            ('num_01', StandardScaler(), ['f4', 'Latitude', 'Longitude']),\n            ('num_02', SimpleImputer(strategy='constant', fill_value=0), ['Weekend', 'SameStreetExact']),\n            ('num_03', StandardScaler(), ['DistanceFromCenter'])\n        ], verbose=True)\n        xf.fit(df)\n    else:\n        y = None\n\n    print('apply transform')\n    X1 = xf.transform(df)\n    X2 = df[['f1', 'f2', 'f5', 'f7', 'f8']].values\n    ids = df['RowId'].values\n    \n    return X1, X2, y, ids, xf, df\n","2a5f3cf5":"import torch\n\nfrom torch.autograd import Variable \nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler","671f7f56":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = torch.device('cpu')\ndevice","89417658":"class EarlyStopManager:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, checkpoint_filename='checkpoint.pt'):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.checkpoint_filename = checkpoint_filename\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.checkpoint_filename)\n        self.val_loss_min = val_loss","82e00f43":"class MSEAccumulator:\n    def __init__(self):\n        self._n_samples = 0\n        self._sq_e = 0.0\n        \n    def add(self, n_samples, partial_mse):\n        self._n_samples += n_samples\n        self._sq_e += (partial_mse * n_samples)\n        \n    def mse(self):\n        return self._sq_e\/self._n_samples","c6c37577":"class MyDataset(Dataset):\n    def __init__(self, X1, X2, y, row_id):\n        if y is not None:\n            assert(X1.shape[0] == y.shape[0])\n            self._y = torch.from_numpy(y)\n        else:\n            self._y = torch.zeros(X1.shape[0], 6)\n        \n        assert(X1.shape[0] == row_id.shape[0])\n        assert(X2.shape[0] == row_id.shape[0])\n        self._X1 = X1\n        self._X2 = X2\n        self._row_id = torch.from_numpy(row_id)\n\n    def __len__(self):\n        return self._X1.shape[0]\n\n    def __getitem__(self, idx):       \n        return self._X1[idx], self._X2[idx], self._y[idx], self._row_id[idx]\n","ba3ac2bc":"class Net(torch.nn.Module):\n    def __init__(self, lin_input_size, embs_input_size, embs_input_cats, lin_output_size, embs_output_size):\n        super(Net, self).__init__()\n#         self.input1 = torch.nn.Linear(lin_input_size, lin_output_size)\n        self.input2 = torch.nn.Embedding(embs_input_size, embs_output_size)\n#         self.input2.weight.data.uniform_(-1.0\/embs_output_size, 1.0\/embs_output_size)\n        self.net = torch.nn.Sequential(\n#             torch.nn.BatchNorm1d(lin_input_size + (embs_output_size * embs_input_cats)),\n            torch.nn.Linear(lin_input_size + (embs_output_size * embs_input_cats), 256),\n            torch.nn.Tanh(),\n            torch.nn.Linear(256, 6),\n#             torch.nn.ReLU()\n        )\n        \n    def forward(self, x1, x2):\n#         print('x2\/1', x2.shape)\n        x2 = self.input2(x2)\n        batch_size, height, width = x2.shape\n        \n#         print('x2\/2', x2, x2.shape)\n        x2 = x2.reshape(batch_size, -1)\n#         print('x2\/3', x2, x2.shape)\n        \n#         print('x1\/1', x1.shape)\n#         x1 = self.input1(x1)\n#         print('x1\/2', x1.shape)\n        \n        x3 = torch.cat([x1, x2], 1)\n#         print('x3\/1', x3.shape)\n        x3 = self.net(x3)\n#         print('x3\/2', x3.shape)\n        return x3","12274435":"def run_training(lin_input_size, embs_input_cats, dl_train, dl_val, max_epochs=50):\n    model = Net(lin_input_size=lin_input_size, \n                embs_input_size=N_BINS,\n                embs_input_cats=embs_input_cats,\n                lin_output_size=128,\n                embs_output_size=64).to(device)\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr = 0.00005, momentum=0.9)\n    # optimizer = torch.optim.Adam(model.parameters(), lr=0.1) # lr=0.005) # , weight_decay=0.01)\n    \n    early_stop_manager = EarlyStopManager(verbose=True, patience=5)\n    for epoch in range(max_epochs):\n        model.train()\n        mse_acc_train = MSEAccumulator()\n        for ix, (_X1, _X2, _y, _row_id) in enumerate(dl_train):\n            _X1 = Variable(_X1).to(device).float()\n            _X2 = Variable(_X2).to(device)\n            _y = Variable(_y).to(device).float()\n\n            # zero parameters\n            optimizer.zero_grad() \n\n            # forward + loss + backward + step\n            _y_pred = model(_X1, _X2)\n            loss = criterion(_y_pred, _y)\n            loss.backward()\n            optimizer.step()\n\n            mse_acc_train.add(_y.shape[0], loss.item())\n            if (ix + 1) % 500 == 0:\n                print('train\/ epoch {}, batch {}, samples {}, loss {:.4f} {:.4f}'.format(\n                    epoch, \n                    ix, \n                    (epoch * train_data.shape[0]) + ((ix + 1) * _y.shape[0]),\n                    math.sqrt(loss.item()),\n                    math.sqrt(mse_acc_train.mse())\n                ))\n\n        model.eval()\n        mse_acc_val = MSEAccumulator()\n        for ix, (_X1, _X2, _y, _row_id) in enumerate(dl_val):\n            _X1 = Variable(_X1).to(device).float()\n            _X2 = Variable(_X2).to(device)\n            _y = Variable(_y).to(device).float()\n\n            _y_pred = model(_X1, _X2)\n            loss = criterion(_y_pred, _y)\n\n            mse_acc_val.add(_y.shape[0], loss.item())\n\n        train_loss = math.sqrt(mse_acc_train.mse())\n        valid_loss = math.sqrt(mse_acc_val.mse())\n        print('summary\/ epoch {}, train loss: {:.4f}, validation loss: {:.4f}'.format(\n            epoch, train_loss, valid_loss))\n\n        early_stop_manager(valid_loss, model)\n        if early_stop_manager.early_stop:\n            print('train\/ early stop!')\n            break\n\n    model.load_state_dict(torch.load('checkpoint.pt'))\n    \n    return model","a7bc67e1":"def write_submission(out_file, model, dl_test):    \n    for ix, (_X1, _X2, _, _row_id) in enumerate(dl_test):\n        _X1 = Variable(_X1).to(device).float()\n        _X2 = Variable(_X2).to(device)\n        _y_pred = model(_X1, _X2)\n\n        for row_id, preds in zip(_row_id, _y_pred):        \n            for pred_id, pred in enumerate(preds.cpu().detach().numpy()):\n    #             print(row_id, pred_id, preds, pred)\n                out_file.write('{}_{},{:.6f}\\n'.format(row_id, pred_id, max(pred, 0.0)))","caf0bebe":"def get_train_validation_splits(num_train):\n    indices = list(range(num_train))\n    np.random.shuffle(indices)\n    split = int(np.floor(0.10 * num_train))\n    # split = 75000\n    return indices[split:], indices[:split]\n# train_idx, valid_idx = indices[-1000:], indices[:1000]","6a4d7c3a":"def build_submission(train_data, test_data):\n    with open('submission.csv', 'w') as out_file:\n        out_file.write('TargetId,Target\\n')\n        for curr_city in CITY:\n            print('city: {}'.format(curr_city))\n            curr_train_data = train_data[train_data['City'] == curr_city]\n            curr_test_data = test_data[test_data['City'] == curr_city]\n\n            print(curr_train_data.shape, curr_test_data.shape)\n\n            X1_train, X2_train, y_train, ids_train, xf, train_df = transform_dataframe(curr_train_data)\n            print('train', X1_train.shape, X2_train.shape, y_train.shape, ids_train.shape)\n\n            X1_test, X2_test, y_test, ids_test, _, test_df = transform_dataframe(curr_test_data, xf)\n            print('test', X1_test.shape, X2_test.shape, y_test, ids_test.shape)\n\n            train_idx, valid_idx = get_train_validation_splits(X1_train.shape[0])\n            print('splits', len(train_idx), len(valid_idx))\n\n            ds_train = MyDataset(X1=X1_train, X2=X2_train, y=y_train, row_id=ids_train)\n            dl_train = DataLoader(ds_train, batch_size=128, sampler=SubsetRandomSampler(train_idx))\n            dl_val = DataLoader(ds_train, batch_size=128, sampler=SubsetRandomSampler(valid_idx))\n\n            print('start training')\n            model = run_training(lin_input_size=X1_train.shape[1],\n                                 embs_input_cats=X2_train.shape[1],\n                                 dl_train=dl_train,\n                                 dl_val=dl_val,\n                                 max_epochs=250)\n\n            ds_test = MyDataset(X1=X1_test, X2=X2_test, y=None, row_id=ids_test)\n            dl_test = DataLoader(ds_test, batch_size=1000, shuffle=False)\n\n            print('write submission file')\n            write_submission(out_file, model, dl_test)","20b87081":"DATA_FOLDER = '\/kaggle\/input\/bigquery-geotab-intersection-congestion\/'\n# DATA_FOLDER = 'bigquery-geotab-intersection-congestion\/'","d52b8fc3":"train_data = pd.read_csv(DATA_FOLDER + 'train.csv')\ntest_data = pd.read_csv(DATA_FOLDER + 'test.csv')\n\nprint(train_data.shape)\nprint(test_data.shape)","a71c0aba":"# Sample input dataset?\n\n# train_data = train_data.sample(n=10000)\n# test_data = test_data.sample(n=10000)\n\n# print(train_data.shape)\n# print(test_data.shape)","fd900f63":"%%time\nbuild_submission(train_data, test_data)","bca64272":"# Training","6189db78":"# Feature Engineering","6672db20":"#\u00a0Submission","68b792fd":"# PyTorch"}}