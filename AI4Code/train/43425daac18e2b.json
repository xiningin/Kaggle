{"cell_type":{"adcc0790":"code","2516d077":"code","47e0a611":"code","851f19f5":"code","5d7c1182":"code","da00022f":"code","66af543e":"code","1997456e":"code","61b7e444":"code","73bbd317":"code","5582134c":"code","5e919fff":"code","60044276":"code","601b85a2":"code","7a56d814":"code","c269b32f":"code","e882b7cb":"code","d92b9790":"code","053c5ff4":"code","94ca4c7b":"code","5c9898da":"code","a734a2ca":"code","c8733eda":"code","a549725a":"code","4fd847c9":"code","7291f239":"code","6111af56":"code","4e621804":"code","ccdcf3f8":"code","a00032cf":"code","5595430c":"code","88807c23":"code","58b5818e":"code","e563f755":"code","ed3b3050":"code","ae614834":"code","347e6872":"code","4e6ba275":"code","519f3519":"code","a8f9c706":"code","5b671cd2":"code","8a07b26f":"code","026c2d0c":"code","971e419c":"code","48564cdc":"code","99d5dd3a":"code","9cced917":"code","a7262fd0":"code","444ef285":"code","560c9fb6":"code","bf095c2d":"code","8d670fd4":"code","7688bae4":"code","741c3549":"code","cb8fb783":"code","9f6bc365":"code","a5b7c0f6":"code","6c761dea":"code","4ea0d9b0":"code","4116d42d":"code","0c4f107c":"code","34c34c70":"code","8070ec93":"code","21c93ca8":"code","ffb990e6":"code","1c06bd8b":"code","b2514321":"code","38cd0b62":"code","3eed3b2d":"code","3d97d67b":"code","a7dc5241":"code","d69843b9":"markdown","f2f0a7dd":"markdown","016cd0f1":"markdown","d0895859":"markdown","1323caf4":"markdown","7e5ee792":"markdown","d7a96da9":"markdown","4cd2d6f7":"markdown","5fb61c42":"markdown","7cbe127b":"markdown","2e0d84dd":"markdown","5d324f7b":"markdown","0c15e47d":"markdown","a2524959":"markdown","877a9126":"markdown","54f4e988":"markdown","1ee9bf88":"markdown","fd3f39e8":"markdown","0b709ac7":"markdown","a1c808d4":"markdown"},"source":{"adcc0790":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2516d077":"import re\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport scikitplot as skplt","47e0a611":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","851f19f5":"train.shape, test.shape","5d7c1182":"train.head() # Pclass - Ticket class, \n             # SibSp - # of siblings \/ spouses aboard the Titanic, \n             # Parch - # of parents \/ children aboard the Titanic,\n             # Embarked - Port of Embarked","da00022f":"np.unique(train[\"Pclass\"]), np.unique(train[\"SibSp\"]), np.unique(train[\"Parch\"]), train[\"Embarked\"].unique()","66af543e":"train[\"Cabin\"].unique()","1997456e":"# null values in the dataframe :\ntrain.isnull().sum()","61b7e444":"# removing null value of embarked by droppping it \ntrain = train.dropna(axis=0, subset=[\"Embarked\"])\ntrain.shape","73bbd317":"print(test.isnull().sum())","5582134c":"train.head()","5e919fff":"age = train[train[\"Survived\"]==1][\"Age\"]\nplt.hist(age,rwidth=0.85)","60044276":"age = train[train[\"Survived\"]==0][\"Age\"]\nplt.hist(age,rwidth=0.85)","601b85a2":"train[\"Sex\"].value_counts()","7a56d814":"male_survived = train[train[\"Sex\"]=='male'][\"Survived\"]\nfemale_survived = train[train[\"Sex\"]=='female'][\"Survived\"]","c269b32f":"# survival rate : \nprint(\"Survival rate for Men : {}\\nSurvival rate for Women : {}\".format(sum(male_survived)\/len(male_survived) *100, sum(female_survived)\/len(female_survived)*100))","e882b7cb":"# Let's check for the kids having age(<18)\n\nkids = train[train[\"Age\"]<=18][\"Survived\"]\nprint(\"Survival rate for kids : {}\".format(sum(kids)\/len(kids)*100))","d92b9790":"# Compairing the price of the passengers who has cabin and who doesn't have a cabin(their cabin are not known)\nticket_price_with_cabin = np.array(train[train['Cabin'].notnull()][\"Fare\"])\nticket_price_without_cabin = np.array(train[train['Cabin'].isnull()][\"Fare\"])","053c5ff4":"# Peoples without cabins: Lower class or Middle Class (Financial status)\n# People with cabin : Upper class (Financial status)\n\nprint(\"************ People without cabin **********************\")\nprint(\"Mean : {}\\tSD : {}\".format(ticket_price_without_cabin.mean(), ticket_price_without_cabin.std()))\nprint(\"Min : {}\\tMax : {}\".format(min(ticket_price_without_cabin), max(ticket_price_without_cabin)))\n\nprint(\"************ People with cabin ************************\")\nprint(\"Mean : {}\\tSD : {}\".format(ticket_price_with_cabin.mean(), ticket_price_with_cabin.std()))\nprint(\"Min : {}\\tMax : {}\".format(min(ticket_price_with_cabin), max(ticket_price_with_cabin)))","94ca4c7b":"plt.subplot(1, 2, 1)\nsb.distplot(ticket_price_without_cabin, kde=True)\n\nplt.subplot(1, 2, 2)\nsb.distplot(ticket_price_with_cabin, kde=True)\nplt.show()","5c9898da":"train[\"Pclass\"].value_counts() ","a734a2ca":"without_cabin  = dict(train[train['Cabin'].isnull()][\"Pclass\"].value_counts())\nwith_cabin = dict(train[train['Cabin'].notnull()][\"Pclass\"].value_counts())\nwithout_cabin = [without_cabin[1], without_cabin[2], without_cabin[3]]\nwith_cabin = [with_cabin[1], with_cabin[2], with_cabin[3]]\n\ndata = pd.DataFrame({\"With cabin\": with_cabin, \"Without cabin\": without_cabin}, index=[1,2,3])","c8733eda":"data.plot.bar()","a549725a":"# Are the cabin crew preferenced upper class people while saving them over middle and lower class people.\n\nupper_class = train[train[\"Pclass\"]==3][\"Survived\"]\nmiddle_class = train[train[\"Pclass\"]==2][\"Survived\"]\nlower_class = train[train[\"Pclass\"]==1][\"Survived\"]","4fd847c9":"upper = [upper_class.sum(), len(upper_class)-upper_class.sum()]\nmiddle = [middle_class.sum(), len(middle_class)-middle_class.sum()] \nlower = [lower_class.sum() , len(lower_class)-lower_class.sum()]\n\ndata = pd.DataFrame({\"upper\":upper,\"middle\":middle, \"lower\":lower})\n","7291f239":"data. plot.bar(figsize=(10,7))   # 0 - Not survived \n                                 # 1 - Survived","6111af56":"# Age binning\n# 1 : 1-10\n# 2 : 10-20 ......\ndef age(data):\n    arr = []\n    for i in data:\n        try:\n            arr.append(int(i \/ 10))\n        except:\n            arr.append(0)\n    return arr\ntrain[\"Age\"] = age(train[\"Age\"])\ntest[\"Age\"] = age(test[\"Age\"])","4e621804":"def fare(data):\n    fare = []\n    for i in data:\n        if 0<i<10: fare.append(1)\n        elif 10<i<30: fare.append(2)\n        elif 30<i<50: fare.append(3)\n        elif 50<i<100: fare.append(4)\n        else: fare.append(5)\n    return fare\ntrain[\"Fare\"] = fare(train[\"Fare\"])\ntest[\"Fare\"] = fare(test[\"Fare\"])","ccdcf3f8":"gender = {\"male\":1, \"female\":0}\nembarked = {'S':1, 'C':0, 'Q':2}\n\ntrain[\"Sex\"] = train[\"Sex\"].map(gender)\ntest[\"Sex\"] = test[\"Sex\"].map(gender)\n\ntrain[\"Embarked\"] = train[\"Embarked\"].map(embarked)\ntest[\"Embarked\"] = test[\"Embarked\"].map(embarked)","a00032cf":"train[\"Cabin\"] = train[\"Cabin\"].fillna(0)\ntest[\"Cabin\"] = test[\"Cabin\"].fillna(0)","5595430c":"def cabin_f(data):\n    cabin = []\n    cabinet_no= []\n    cabinet = {'A':1,'B':2,'C':3,'D':4,'E':5,\n              'F':6,'G':7,'T':8}\n    for i in data:\n        if i!=0:\n            cabin.append(cabinet[\"\".join(re.split(\"[^a-zA-Z]*\", i))[0]])\n            temp = re.split(\"[^0-9]\",i)[1]\n            if temp:\n                cabinet_no.append(int(temp))\n            else: cabinet_no.append(0)\n        else:\n            cabin.append(0)\n            cabinet_no.append(0)\n    return cabin, cabinet_no\n\ncabin, cabinet_no = cabin_f(train[\"Cabin\"])\ntrain[\"Cabin_type\"] = cabin\ntrain[\"Cabin_no\"] = cabinet_no\n\ncabin, cabinet_no = cabin_f(test[\"Cabin\"])\ntest[\"Cabin_type\"] = cabin\ntest[\"Cabinet_no\"] = cabinet_no\n\ntrain = train.drop(\"Cabin\", axis=1)\ntest = test.drop(\"Cabin\", axis=1)","88807c23":"train[\"Title\"] = train['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\ntrain[\"Title\"] = train[\"Title\"].replace(['Lady', 'Capt', 'Don', 'Col', 'Countess', 'Dona', 'Jonkheer', 'Dr', 'Major', 'Rev', 'Sir'], 'rare')\n\ntest[\"Title\"] = test['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\ntest[\"Title\"] = test[\"Title\"].replace(['Lady', 'Capt', 'Don', 'Col', 'Countess', 'Dona', 'Jonkheer', 'Dr', 'Major', 'Rev', 'Sir'], 'rare')\n\nmapper = {'rare':1, 'Mr':2, 'Mrs':3, 'Miss':4,'Master':5 }\n\ntrain['Title'] = train['Title'].map(mapper)\ntest['Title'] = test['Title'].map(mapper)\ntrain['Title'] = train['Title'].fillna(0)\ntest['Title'] = test['Title'].fillna(0)","58b5818e":"train[\"Family_member\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"Family_member\"] = test[\"SibSp\"] + test[\"Parch\"] +1","e563f755":"train[\"Alone\"] = [1 if i==1 else 0 for i in train[\"Family_member\"]]\ntest[\"Alone\"] = [1 if i==1 else 0 for i in test[\"Family_member\"]]","ed3b3050":"train.head()","ae614834":"data = train[[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Embarked\", \"Cabin_type\", \"Cabin_no\", \"Survived\"]]\n\nfig, ax = plt.subplots(figsize=(12, 8)) \nsb.heatmap(data.corr(),  ax= ax, annot= True)","347e6872":"train[\"Survived\"].value_counts()    # Not imbalanced looks fine","4e6ba275":"y = train[\"Survived\"]\n\ntest_id = test[\"PassengerId\"]\ntest = test.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=1)\ntrain = train.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Survived\"], axis=1)","519f3519":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, f1_score","a8f9c706":"from sklearn.preprocessing import StandardScaler \nscaler = StandardScaler()\ntrain = scaler.fit_transform(train)\nscaler = StandardScaler()\ntest = scaler.fit_transform(test)","5b671cd2":"xtrain, xtest, ytrain, ytest = train_test_split(train, y, train_size=0.80)\nxtrain.shape, xtest.shape","8a07b26f":"grid={\"C\":np.logspace(-3,3,7), \n      \"penalty\":['l1','l2'], \n      \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nmodel = LogisticRegression()\nmodel = GridSearchCV(model,grid,cv=10)\nmodel.fit(xtrain,ytrain)\n\nprint(\"tuned hpyerparameters :(best parameters) \",model.best_params_)\nprint(\"accuracy :\",model.best_score_)","026c2d0c":"model = LogisticRegression(C=0.01, penalty='l1', solver='liblinear')\nmodel.fit(xtrain, ytrain)\n\nprediction = model.predict(xtest)\n\nprint(\"Training accuracy : \",model.score(xtrain, ytrain))\nprint(\"Test accuracy : \",model.score(xtest, ytest))\nlr_acc = model.score(xtest, ytest)\nprint(\"F1 score : \", f1_score(prediction, ytest))\nprint(\"Confusion metrics : \\n\", confusion_matrix(ytest, prediction))\n","971e419c":"prob = model.predict_proba(xtest)\nskplt.metrics.plot_roc_curve(ytest, prob)\nplt.show()","48564cdc":"grid = { 'criterion':['gini','entropy'],'max_depth': np.arange(3, 25)}\n\nmodel = DecisionTreeClassifier()\nmodel = GridSearchCV(model,grid,cv=10)\nmodel.fit(xtrain,ytrain)\n\nprint(\"tuned hpyerparameters :(best parameters) \",model.best_params_)\nprint(\"accuracy :\",model.best_score_)","99d5dd3a":"model = DecisionTreeClassifier(criterion='entropy', max_depth=8)\nmodel.fit(xtrain, ytrain)\n\nprediction = model.predict(xtest)\n\nprint(\"Training accuracy : \",model.score(xtrain, ytrain))\nprint(\"Test accuracy : \",model.score(xtest, ytest))\ndt_acc = model.score(xtest, ytest)\nprint(\"F1 score : \", f1_score(prediction, ytest))\nprint(\"Confusion metrics : \\n\", confusion_matrix(ytest, prediction))","9cced917":"prob = model.predict_proba(xtest)\nskplt.metrics.plot_roc_curve(ytest, prob)\nplt.show()","a7262fd0":"\"\"\"grid={ \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\nmodel = RandomForestClassifier()\nmodel = GridSearchCV(model,grid,cv=10)\nmodel.fit(xtrain,ytrain)\n\nprint(\"tuned hpyerparameters :(best parameters) \",model.best_params_)\nprint(\"accuracy :\",model.best_score_)\"\"\"","444ef285":"model = RandomForestClassifier(criterion= 'gini', max_depth=3, max_features='auto', n_estimators= 200)\nmodel.fit(xtrain, ytrain)\n\nprediction = model.predict(xtest)\n\nprint(\"Training accuracy : \",model.score(xtrain, ytrain))\nprint(\"Test accuracy : \",model.score(xtest, ytest))\nrf_acc = model.score(xtest, ytest)\nprint(\"F1 score : \", f1_score(prediction, ytest))\nprint(\"Confusion metrics : \\n\", confusion_matrix(ytest, prediction))","560c9fb6":"prob = model.predict_proba(xtest)\nskplt.metrics.plot_roc_curve(ytest, prob)\nplt.show()","bf095c2d":"\"\"\"grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}\n\nmodel = SVC()\nmodel = GridSearchCV(model,grid,cv=10)\nmodel.fit(xtrain,ytrain)\n\nprint(\"tuned hpyerparameters :(best parameters) \",model.best_params_)\nprint(\"accuracy :\",model.best_score_)\"\"\"","8d670fd4":"model = SVC(C= 10, gamma=0.1, kernel='rbf')\nmodel.fit(xtrain, ytrain)\n\nprediction = model.predict(xtest)\n\nprint(\"Training accuracy : \",model.score(xtrain, ytrain))\nprint(\"Test accuracy : \",model.score(xtest, ytest))\nsvc_acc = model.score(xtest, ytest)\nprint(\"F1 score : \", f1_score(prediction, ytest))\nprint(\"Confusion metrics : \\n\", confusion_matrix(ytest, prediction))","7688bae4":"grid = {'n_neighbors':[3,5,11,19],'weights':['uniform', 'distance'], 'metric':['euclidean','manhattan']}\n\nmodel = KNeighborsClassifier()\nmodel = GridSearchCV(model,grid,cv=10)\nmodel.fit(xtrain,ytrain)\n\nprint(\"tuned hpyerparameters :(best parameters) \",model.best_params_)\nprint(\"accuracy :\",model.best_score_)","741c3549":"model = KNeighborsClassifier(metric='manhattan', n_neighbors=19, weights='distance')\nmodel.fit(xtrain, ytrain)\n\nprediction = model.predict(xtest)\n\nprint(\"Training accuracy : \",model.score(xtrain, ytrain))\nprint(\"Test accuracy : \",model.score(xtest, ytest))\nknn_acc = model.score(xtest, ytest)\nprint(\"F1 score : \", f1_score(prediction, ytest))\nprint(\"Confusion metrics : \\n\", confusion_matrix(ytest, prediction))","cb8fb783":"prob = model.predict_proba(xtest)\nskplt.metrics.plot_roc_curve(ytest, prob)\nplt.show()","9f6bc365":"\"\"\"grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"n_estimators\": [1, 2]\n             }\ntree = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"balanced\",max_depth = None)\n\nmodel = AdaBoostClassifier(base_estimator = tree)\nmodel = GridSearchCV(model, grid, scoring = 'roc_auc')\nmodel.fit(xtrain,ytrain)\n\nprint(\"tuned hpyerparameters :(best parameters) \",model.best_params_)\nprint(\"accuracy :\",model.best_score_)\"\"\"","a5b7c0f6":"tree = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"balanced\",max_depth = None)\n\nmodel = AdaBoostClassifier(base_estimator=tree)\nmodel.fit(xtrain, ytrain)\n\nprediction = model.predict(xtest)\n\nprint(\"Training accuracy : \",model.score(xtrain, ytrain))\nprint(\"Test accuracy : \",model.score(xtest, ytest))\nada_acc = model.score(xtest, ytest)\nprint(\"F1 score : \", f1_score(prediction, ytest))\nprint(\"Confusion metrics : \\n\", confusion_matrix(ytest, prediction))","6c761dea":"prob = model.predict_proba(xtest)\nskplt.metrics.plot_roc_curve(ytest, prob)\nplt.show()","4ea0d9b0":"\"\"\"grid = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.025, 0.075, 0.15, 0.2],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 8),\n    \"min_samples_leaf\": np.linspace(0.1, 0.5, 8),\n    \"max_depth\":[3,5,8],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"subsample\":[0.5, 0.8, 0.9,  1.0],\n    \"n_estimators\":[10]\n    }\n\nmodel = GridSearchCV(GradientBoostingClassifier(), grid, cv=10, n_jobs=-1)\nmodel.fit(xtrain,ytrain)\n\nprint(\"tuned hpyerparameters :(best parameters) \",model.best_params_)\nprint(\"accuracy :\",model.best_score_)\"\"\"","4116d42d":"model = GradientBoostingClassifier(criterion='friedman_mse', learning_rate= 0.15, \n                                   loss= 'deviance', max_depth= 8, max_features='sqrt', \n                                   min_samples_leaf= 0.15714285714285714, min_samples_split= 0.5, \n                                   n_estimators= 10, subsample=1.0)\nmodel.fit(xtrain, ytrain)\n\nprediction = model.predict(xtest)\n\nprint(\"Training accuracy : \",model.score(xtrain, ytrain))\nprint(\"Test accuracy : \",model.score(xtest, ytest))\ngb_acc = model.score(xtest, ytest)\nprint(\"F1 score : \", f1_score(prediction, ytest))\nprint(\"Confusion metrics : \\n\", confusion_matrix(ytest, prediction))","0c4f107c":"prob = model.predict_proba(xtest)\nskplt.metrics.plot_roc_curve(ytest, prob)\nplt.show()","34c34c70":"\"\"\"grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\nmodel =ExtraTreesClassifier()\n\nmodel = GridSearchCV(model,grid, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\nmodel.fit(xtrain,ytrain)\n\nprint(\"tuned hpyerparameters :(best parameters) \",model.best_params_)\nprint(\"accuracy :\",model.best_score_)\"\"\"","8070ec93":"model = ExtraTreesClassifier(bootstrap=False, criterion='gini', max_depth= None, \n                             max_features= 3, min_samples_leaf= 1, min_samples_split= 10, \n                             n_estimators= 300)\nmodel.fit(xtrain, ytrain)\n\nprediction = model.predict(xtest)\n\nprint(\"Training accuracy : \",model.score(xtrain, ytrain))\nprint(\"Test accuracy : \",model.score(xtest, ytest))\netc_acc = model.score(xtest, ytest)\nprint(\"F1 score : \", f1_score(prediction, ytest))\nprint(\"Confusion metrics : \\n\", confusion_matrix(ytest, prediction))","21c93ca8":"prob = model.predict_proba(xtest)\nskplt.metrics.plot_roc_curve(ytest, prob)\nplt.show()","ffb990e6":"param = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['binary:logistic'],\n              'learning_rate': [0.05], #so called `eta` value\n              'max_depth': [6],\n              'min_child_weight': [11],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [5], #number of trees, change it to 1000 for better results\n              'missing':[-999],\n              'seed': [1337]}\n\nmodel =XGBClassifier()\n\nmodel = GridSearchCV(model,grid, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\nmodel.fit(xtrain,ytrain)\n\nprint(\"tuned hpyerparameters :(best parameters) \",model.best_params_)\nprint(\"accuracy :\",model.best_score_)","1c06bd8b":"model = XGBClassifier(metric= 'euclidean', n_neighbors= 3, weights= 'uniform')\nmodel.fit(xtrain, ytrain)\n\nprediction = model.predict(xtest)\n\nprint(\"Training accuracy : \",model.score(xtrain, ytrain))\nprint(\"Test accuracy : \",model.score(xtest, ytest))\nxgb_acc = model.score(xtest, ytest)\nprint(\"F1 score : \", f1_score(prediction, ytest))\nprint(\"Confusion metrics : \\n\", confusion_matrix(ytest, prediction))\n","b2514321":"prob = model.predict_proba(xtest)\nskplt.metrics.plot_roc_curve(ytest, prob)\nplt.show()","38cd0b62":"# Compare accuracy : \nacc = [lr_acc, dt_acc, rf_acc, svc_acc, knn_acc, ada_acc, gb_acc, etc_acc, xgb_acc]","3eed3b2d":"plt.plot(acc,color='green', linestyle='dashed', linewidth = 3, \n         marker='o', markerfacecolor='blue', markersize=12)\nplt.xticks(np.arange(9), ('LR','DT','RF','SVC','KNN','AB','GB','ETC','XGB'))\n           #('LogiticRegression','DicisionTree','RandomForest','SVC','KNN','Adaboost','GradientBoost','ExtraTreeclassifier'))","3d97d67b":"model = XGBClassifier(objective = 'binary:logistic',eta=0.3, min_child_weight =1, subsample=1, colsample_by_tree=0.4,\n                      max_depth=9, learning_rate=0.03,metric= 'euclidean', n_neighbors= 3, weights= 'uniform',\n                     gamma=0, reg_lambda =2.8,reg_alpha=0, scale_pos_weight=1, n_estimator= 600 )\nmodel.fit(xtrain, ytrain)\n\nprediction = model.predict(xtest)\n\nprint(\"Training accuracy : \",model.score(xtrain, ytrain))\nprint(\"Test accuracy : \",model.score(xtest, ytest))\nxgb_acc = model.score(xtest, ytest)\nprint(\"F1 score : \", f1_score(prediction, ytest))\nprint(\"Confusion metrics : \\n\", confusion_matrix(ytest, prediction))","a7dc5241":"# highest I got highest by Adaboost on test data\nprediction = model.predict(test)\ndata = {\"PassengerId\": test_id, \"Survived\":prediction}\nresults = pd.DataFrame(data)\nresults.to_csv(\"ensemble_python_voting.csv\",index=False)\n","d69843b9":"Those who lost their live are commonly young generation of that time(20-40 year old).","f2f0a7dd":"# Age base survival analysis","016cd0f1":"# Correlation plot","d0895859":"# Hypothesis testing","1323caf4":"# So it is True Survival rate for womens and kids are higher than man's.\n# Well done gentleman","7e5ee792":"# ExtraTressClassifier","d7a96da9":"# This is the clear indication that people who buy ticket in cruise with the cabin are financially strong.","4cd2d6f7":"# Knn","5fb61c42":" # Gradient Boosting","7cbe127b":"# Conclusion:\n1. Ticket Fare is highly correlated with ticket class (it is negative becasue ticket class is reverse in order).\n2. Cabin type is also depend upon ticket type.\n3. Survival depend upon sex.\n\n","2e0d84dd":"# Feature engineering","5d324f7b":"### Class Category :\n1. 1st = Upper\n2. 2nd = Middle\n3. 3rd = Lower","0c15e47d":"# Gender Base analysis","a2524959":"# SVC","877a9126":"# Logistic Regression","54f4e988":"# Ada Boosting","1ee9bf88":"# Random Forest","fd3f39e8":"# People from the upper class are survied more than middle and lower class","0b709ac7":"# XGboost","a1c808d4":"# Decision tree"}}