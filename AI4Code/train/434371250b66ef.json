{"cell_type":{"a6c8c204":"code","639a9dbd":"code","8997fe4e":"code","31620046":"code","96d158e4":"code","80ebed3b":"code","f6e78951":"code","4ae87519":"code","1766740e":"code","a454a917":"code","460e7237":"code","8a7ee41b":"code","d37c64c3":"code","ca5b02a7":"code","700b643b":"code","f0c0cf93":"code","54ae05ba":"code","7afc1419":"code","4154b69e":"code","a35441bb":"code","02eb9e4b":"code","6b211895":"code","c9dcddbc":"code","37fb05bc":"code","0c9b669a":"code","16813627":"code","68bce8c4":"code","b025df0b":"code","28184daf":"code","4f9f5397":"code","ea26951d":"code","8d78d36f":"code","13d98711":"code","15a293be":"code","7635de06":"code","2fd5b7f1":"code","d9567c71":"code","35ad8212":"code","90231e62":"code","155716ac":"code","e7d8723f":"code","e59a77e3":"code","a04a1d8e":"code","33a06179":"code","887287bf":"code","4fe15c02":"code","cc3355cf":"code","ad00a6a7":"code","8bb5021f":"code","d584ba86":"code","ded7f091":"code","106aa268":"code","fb48cf33":"code","78d9a949":"code","8babd237":"code","2f7fd9e1":"markdown","fc290287":"markdown","43d76970":"markdown","8808f60c":"markdown","2fd3deb3":"markdown","a9140c5c":"markdown","ba73eb38":"markdown","9d704244":"markdown","f18b1581":"markdown","ce14aba8":"markdown","33451e5f":"markdown","dca689cd":"markdown","58e15682":"markdown","79bf914f":"markdown","682ab5f1":"markdown","ed604900":"markdown","241734e4":"markdown","067264f7":"markdown"},"source":{"a6c8c204":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","639a9dbd":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn import model_selection\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nsns.set(color_codes=True) # adds a nice background to the graphs\n%matplotlib inline","8997fe4e":"df = pd.read_csv('..\/input\/fetal-health-classification\/fetal_health.csv')\ndf.head()","31620046":"df.shape","96d158e4":"df.info()","80ebed3b":"df.isnull().sum()","f6e78951":"df.dtypes","4ae87519":"df.head(10).style.background_gradient(cmap=\"RdYlBu\")","1766740e":"sns.pairplot(df)","a454a917":"df.describe().T","460e7237":"df.skew()","8a7ee41b":"df[df.duplicated()]","d37c64c3":"df_dup = df.drop_duplicates(subset = None , keep = 'first', inplace = False)","ca5b02a7":"df_dup.shape","700b643b":"Target = df[\"fetal_health\"]","f0c0cf93":"corr = df.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(15, 15))\n    ax = sns.heatmap(corr,mask=mask,square=True,linewidths=2.5,cmap=\"viridis\",annot=True)","54ae05ba":"sns.countplot(Target)\nplt.show()","7afc1419":"print(\"Count of type 1.0 fetal health in the dataset \",len(df.loc[df[\"fetal_health\"]==1.0]))\nprint(\"Count of type 2.0 fetal health in the dataset \",len(df.loc[df[\"fetal_health\"]==2.0]))\nprint(\"Count of type 3.0 fetal health in the dataset \",len(df.loc[df[\"fetal_health\"]==3.0]))","4154b69e":"updated_cols = list(df.columns)\nfor column in updated_cols:\n    print(column,\" : \", len(df.loc[df[column]<0]))","a35441bb":"X = df_dup.iloc[:,:-1]\ny = df_dup.iloc[:,-1]","02eb9e4b":"y.value_counts()","6b211895":"scale = StandardScaler()\nX = scale.fit_transform(X)\nX = pd.DataFrame(X,columns=df_dup.iloc[:,:-1].columns)","c9dcddbc":"from imblearn.over_sampling import RandomOverSampler\nROS = RandomOverSampler(random_state=42)\nX_ros, y_ros = ROS.fit_resample(X,y)\nfrom collections import Counter\nprint('Resampled dataset shape %s' % Counter(y_ros))","37fb05bc":"import statsmodels.api as sm\nX = sm.add_constant(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10, test_size = 0.2)\n\nprint('X_train', X_train.shape)\nprint('y_train', y_train.shape)\n\nprint('X_test', X_test.shape)\nprint('y_test', y_test.shape)","0c9b669a":"print(\"{0:0.2f}% data is in training set\".format((len(X_train)\/len(df.index)) * 100))\nprint(\"{0:0.2f}% data is in test set\".format((len(X_test)\/len(y.index)) * 100))","16813627":"def get_train_report(model):\n    \n    train_pred = model.predict(X_train)\n    return(classification_report(y_train, train_pred))","68bce8c4":"def get_test_report(model):\n    test_pred = model.predict(X_test)\n    return(classification_report(y_test, test_pred))","b025df0b":"decision_tree_classification = DecisionTreeClassifier(criterion = 'entropy', random_state = 10)\ndecision_tree = decision_tree_classification.fit(X_train, y_train)\nfrom sklearn.metrics import classification_report","28184daf":"train_report = get_train_report(decision_tree)\nprint(train_report)","4f9f5397":"test_report = get_test_report(decision_tree)\nprint(test_report)","ea26951d":"dt_model = DecisionTreeClassifier(criterion = 'gini',\n                                  max_depth = 5,\n                                  min_samples_split = 4,\n                                  max_leaf_nodes = 6,\n                                  random_state = 10)\n\n# fit the model using fit() on train data\ndecision_tree = dt_model.fit(X_train, y_train)\ntrain_report = get_train_report(decision_tree)\n\n# print the performance measures\nprint('Train data:\\n', train_report)\ntest_report = get_test_report(decision_tree)\n\n# print the performance measures\nprint('Test data:\\n', test_report)","8d78d36f":"rf_classification = RandomForestClassifier(n_estimators = 10, random_state = 10)\n\n# use fit() to fit the model on the train set\nrf_model = rf_classification.fit(X_train, y_train)","13d98711":"train_report = get_train_report(rf_model)\nprint(train_report) ","15a293be":"test_report = get_test_report(rf_model)\nprint(test_report) ","7635de06":"important_features = pd.DataFrame({'Features': X_train.columns, \n                                   'Importance': rf_model.feature_importances_})\n\n# sort the dataframe in the descending order according to the feature importance\nimportant_features = important_features.sort_values('Importance', ascending = False)\n\n# create a barplot to visualize the features based on their importance\nsns.barplot(x = 'Importance', y = 'Features', data = important_features)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Feature Importance', fontsize = 15)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\n\n# display the plot\nplt.show()","2fd5b7f1":"from sklearn.metrics import confusion_matrix,roc_curve\nknn_classification = KNeighborsClassifier(n_neighbors = 3)\n\n# fit the model using fit() on train data\nknn_model = knn_classification.fit(X_train, y_train)","d9567c71":"train_report = get_train_report(knn_model)\nprint(train_report) ","35ad8212":"test_report = get_test_report(knn_model)\nprint(test_report)","90231e62":"tuned_paramaters = {'n_neighbors': np.arange(1, 25, 2),\n                   'metric': ['hamming','euclidean','manhattan','Chebyshev']}\n \n# instantiate the 'KNeighborsClassifier' \nknn_classification = KNeighborsClassifier()\n\nknn_grid = GridSearchCV(estimator = knn_classification, \n                        param_grid = tuned_paramaters, \n                        cv = 5, \n                        scoring = 'accuracy')\n\n# fit the model on X_train and y_train using fit()\nknn_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for KNN Classifier: ', knn_grid.best_params_, '\\n')","155716ac":"from sklearn.model_selection import cross_val_score\nerror_rate = []\n\n# use for loop to build a knn model for each K\nfor i in np.arange(1,25,2):\n    \n    # setup a knn classifier with k neighbors\n    # use the 'euclidean' metric \n    knn = KNeighborsClassifier(i, metric = 'euclidean')\n   \n    # fit the model using 'cross_val_score'\n    # pass the knn model as 'estimator'\n    # use 5-fold cross validation\n    score = cross_val_score(knn, X_train, y_train, cv = 5)\n    \n    # calculate the mean score\n    score = score.mean()\n    \n    # compute error rate \n    error_rate.append(1 - score)\n\n# plot the error_rate for different values of K \nplt.plot(range(1,25,2), error_rate)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Error Rate', fontsize = 15)\nplt.xlabel('K', fontsize = 15)\nplt.ylabel('Error Rate', fontsize = 15)\n\n# set the x-axis labels\nplt.xticks(np.arange(1, 25, step = 2))\n\n# plot a vertical line across the minimum error rate\nplt.axvline(x = 7, color = 'red')\n\n# display the plot\nplt.show()","e7d8723f":"train_report = get_train_report(knn_grid)\nprint(train_report) ","e59a77e3":"test_report = get_test_report(knn_grid)\nprint(test_report) ","a04a1d8e":"gnb = GaussianNB()\n\n# fit the model using fit() on train data\ngnb_model = gnb.fit(X_train, y_train)","33a06179":"test_report = get_test_report(gnb_model)\nprint(test_report) ","887287bf":"ada_model = AdaBoostClassifier(n_estimators = 40, random_state = 10)\nada_model.fit(X_train, y_train)","4fe15c02":"test_report = get_test_report(ada_model)\nprint(test_report)","cc3355cf":"gboost_model = GradientBoostingClassifier(n_estimators = 150, max_depth = 10, random_state = 10)\ngboost_model.fit(X_train, y_train)","ad00a6a7":"test_report = get_test_report(gboost_model)\nprint(test_report)","8bb5021f":"xgb_model = XGBClassifier(max_depth = 10, gamma = 1)\nxgb_model.fit(X_train, y_train)","d584ba86":"test_report = get_test_report(xgb_model)\nprint(test_report)","ded7f091":"#important_features = pd.DataFrame({'Features': X_train.columns, 'Importance': xgb_model.feature_importances_})\n#important_features = important_features.sort_values('Importance', ascending = False)\n\n#sns.barplot(x = 'Importance', y = 'Features', data = important_features)\n#plt.title('Feature Importance', fontsize = 15)\n#plt.xlabel('Importance', fontsize = 15)\n#plt.ylabel('Features', fontsize = 15)\n#plt.show()","106aa268":"svc_model = SVC(kernel='poly',probability=True)\nsvc_model.fit(X_train,y_train)","fb48cf33":"test_report = get_test_report(svc_model)\nprint(test_report)","78d9a949":"clf1 = KNeighborsClassifier(n_neighbors = 7 , weights = 'distance', metric='manhattan' )\nclf2 = GradientBoostingClassifier(n_estimators = 150,max_depth = 10,random_state=1)\n\nvotingclf = VotingClassifier(estimators=[('knn',clf1),('grb', clf2)],voting='hard')\nvotingclf = votingclf.fit(X_train,y_train)","8babd237":"test_report = get_test_report(votingclf)\nprint(test_report)","2f7fd9e1":"There is strong correlation between baseline value and histogram mode , histogram median and histogram mean.\n\nHistogram number of peaks and histogram width is also having good correlation.","fc290287":"Best parameters for KNN Classifier:  {'metric': 'manhattan', 'n_neighbors': 7}","43d76970":"Let's check if there is any duplicates in the dataset.","8808f60c":"We tried different algorithms for this dataset among them the boosting based algorithms i.e, XG Boost and Gradient boosting algorithms are performing best for the dataset with an accuracy of 94% on the test dataset and f1 score for XGBoost are 0.96 , 0.85 and 0.87 respectively for three different classes followed by Decision Tree Classifier without hypertuning it with an accuracy of 93% on the test data.","2fd3deb3":"Voting Classifier ","a9140c5c":"**Gradient Boosting Classifier**","ba73eb38":"From the above bar plot, we can see that short term variability is the most important feature in the dataset.","9d704244":"**Gaussian Naive Bayes**","f18b1581":"**Decision Tree Classifier**","ce14aba8":"Random sample Imputer for improving the class imbalance in the target column.","33451e5f":"**Adaboost Classifier**","dca689cd":"*PLEASE UPVOTE IF YOU LIKE THE ANALYSIS*","58e15682":"We can see that the optimal value of K = 7 obtained from the GridSearchCV results in a lowest error rate.","79bf914f":"**Random Forest**","682ab5f1":"**Support Vector Machine**","ed604900":"**K Nearest Neighbors**","241734e4":"**XG Boost Classifier**","067264f7":"Univariate Analysis"}}