{"cell_type":{"5645c41c":"code","d23727b0":"code","3aa64424":"code","97fc2dc4":"code","291ba37c":"code","73602992":"code","4d7e8609":"code","df0142e9":"code","1f960320":"code","1c1758cc":"code","977606bd":"code","d5535dc6":"code","67ceb425":"code","5bffc72b":"code","bc3b834e":"code","0df67bf5":"code","41f698de":"code","103d6687":"code","a8c80e62":"code","8ff15928":"code","244c6552":"code","45932ec4":"code","52c90cd6":"code","8adab197":"code","5a643e07":"code","ae797d55":"markdown","5aad5889":"markdown","34215ef0":"markdown","44e9d232":"markdown","113803f4":"markdown","25f54d5f":"markdown","aabf311e":"markdown","d93f00de":"markdown","d08aefd0":"markdown","b4d209d4":"markdown","a38c9915":"markdown","c344daae":"markdown","85cc0695":"markdown","d2b0948c":"markdown","a61bf14b":"markdown","6be5b7b1":"markdown","b55c3e5d":"markdown","6272a334":"markdown","364a1de7":"markdown","e92ca550":"markdown","6e1bbd50":"markdown","66ca10de":"markdown","dedbbcda":"markdown","df492f1d":"markdown","957f8427":"markdown","13d0104d":"markdown","e4b4e013":"markdown","aadb8679":"markdown","ac37e630":"markdown","cc85aa0c":"markdown"},"source":{"5645c41c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d23727b0":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","3aa64424":"train_data.head()","97fc2dc4":"train_data.info()","291ba37c":"# PassengerId Seems to be unique ID \n\n# Survived is the target variable\n\n# That leaves us with Pclass, Age, SibSp, Parch, Fare\n\nnum_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nfor col in num_cols:\n    print(train_data[col].value_counts())\n    print('\\n\\n')","73602992":"# Pclass seem to be more of categorical variable indicated in number field. That leaves us with 'Age', 'SibSp', 'Parch', 'Fare' numerical fields. \nfrom scipy import stats\nfrom collections import Counter\n\ndef detect_outliers_numerical_features(df, num_features):\n    \"\"\"\n    Take list of numerical features and populate the list of indices corresponding \n    to rows containing more than 2 outliers for each row from the given dataframe.\n    \"\"\"\n    \n    outlier_indices = []\n    \n    for feature in num_features:\n        ser = df[feature]\n        \n        # First quartile (Q1)\n        Q1 = np.nanpercentile(ser, 25) \n\n        # Third quartile (Q3) \n        Q3 = np.nanpercentile(ser, 75) \n\n        # Interquaritle range (IQR) \n        IQR = Q3 - Q1\n        \n        print('Feature is :{} and IQR is :{}'.format(feature,IQR))\n        # The default is the true IQR: (25, 75)\n        # IQR = stats.iqr(ser, nan_policy='omit') \n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_col_indices = df[(df[feature] < Q1 - outlier_step) | (df[feature] > Q3 + outlier_step )].index\n        \n        print(outlier_col_indices)\n        print('\\n\\n')\n        # Extend will keep adding the current list to the end of original list\n        outlier_indices.extend(outlier_col_indices)\n     \n    # select observations containing more than 2 outliers ( v > 2, means more than twice the occurence)\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > 2 )\n    \n    return multiple_outliers   \n        \n\ntrain_data_outlier_indices = detect_outliers_numerical_features(train_data, ['Age', 'SibSp', 'Parch', 'Fare'])\n\ntrain_data.loc[train_data_outlier_indices] ","4d7e8609":"# One way to handle outliers is ignore the records\n\n# Drop columns and Reset Index - Use the drop parameter to avoid the old index being added as a column\ntrain_data = train_data.drop(train_data_outlier_indices, axis=0).reset_index(drop=True)","df0142e9":"# Combine Train and Test data sets\n\ntrain_data['type'] = 'train'\ntest_data['type'] = 'test'\n\ntotal_df =  pd.concat(objs=[train_data, test_data], axis=0).reset_index(drop=True)","1f960320":"# Look for train data metrics \ntrain_data.info()\ntrain_data.isnull().sum()","1c1758cc":"# Fill empty\/missing values with nan\ntotal_df = total_df.fillna(np.nan)\n\n# Look for counts around nan entries\ntotal_df.isnull().sum()","977606bd":"# Summary of the training data (The below will only cover Numerical fields)\n\ntrain_data.describe()","d5535dc6":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(style='white', context='notebook', palette='deep')\n\n\n# Correlation Matrix between different Numerical fields \n\ncorr = sns.heatmap(train_data[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = plt.cm.RdBu)","67ceb425":"bar_sibsp = sns.catplot(x=\"SibSp\",y=\"Survived\", data=train_data, kind=\"bar\")\nbar_sibsp = bar_sibsp.set_ylabels(\"Survived\")","5bffc72b":"bar_parch = sns.factorplot(x=\"Parch\",y=\"Survived\", data=train_data, kind=\"bar\")\nbar_parch = bar_parch.set_ylabels(\"Survived\")","bc3b834e":"# Age has diverse values and hence we may have to draw a distribution plot\n\ndist_age = sns.FacetGrid(train_data, col='Survived', height=5, xlim=(0, 80))\ndist_age = dist_age.map(sns.distplot, \"Age\")","0df67bf5":"# Lets plot kde for Age vs Survived\/Non-Surived\n\nnon_survived = train_data[\"Age\"][(train_data[\"Survived\"] == 0) & (train_data[\"Age\"].notnull())]\nsurvived = train_data[\"Age\"][(train_data[\"Survived\"] == 1) & (train_data[\"Age\"].notnull())]\n                                 \nkde_age = sns.kdeplot(non_survived, color=\"Red\", shade = True)\nkde_age = sns.kdeplot(survived, ax = kde_age, color=\"Blue\", shade= True)\nkde_age.set_xlabel(\"Age\")\nkde_age.set_ylabel(\"Frequency\")\nkde_age = kde_age.legend([\"Not Survived\",\"Survived\"])","41f698de":"# Distribution plot for Fare\n\ndist_fare = sns.distplot(train_data[\"Fare\"], color=\"r\", label=\"Skewness : %.2f\"%(train_data[\"Fare\"].skew()))\ndist_fare = dist_fare.legend(loc=\"best\")","103d6687":"# Log function will tranform Fare values to reduce the skewness distribution\n\ntotal_df[\"Fare\"] = total_df[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n","a8c80e62":"# If we plot the distribution plot now to check for skewness. \n\ndist_fare = sns.distplot(total_df[\"Fare\"], color=\"r\", label=\"Skewness : %.2f\"%(total_df[\"Fare\"].skew()))\ndist_fare = dist_fare.legend(loc=\"best\")","8ff15928":"train_data.select_dtypes(exclude=['int64','float64']).info()","244c6552":"bar_s = sns.catplot(x=\"Sex\",y=\"Survived\", data=train_data, kind=\"bar\")\nbar_s = bar_s.set_ylabels(\"Survived\")","45932ec4":"bar_pclass = sns.catplot(x=\"Pclass\",y=\"Survived\", data=train_data, kind=\"bar\")\nbar_pclass = bar_pclass.set_ylabels(\"Survived\")","52c90cd6":"bar_pclass = sns.catplot(x=\"Pclass\",y=\"Survived\", hue='Sex', data=train_data, kind=\"bar\")\nbar_pclass = bar_pclass.set_ylabels(\"Survived\")","8adab197":"bar_embarked = sns.catplot(x=\"Embarked\",y=\"Survived\", data=train_data, kind=\"bar\")\nbar_embarked = bar_embarked.set_ylabels(\"Survived\")","5a643e07":"bar_em_p = sns.catplot(x=\"Embarked\", y=\"Survived\", hue=\"Pclass\", data=train_data, kind=\"bar\")\nbar_em_p = bar_em_p.set_ylabels(\"Survived\")","ae797d55":"Fare Distribution is skewed and may overweigh high values that could impact the model, hence we need to apply Log function to reduce the skewness. \n\nRead this article for more info https:\/\/towardsdatascience.com\/transforming-skewed-data-73da4c2d0d16\n\nLog function can be applied on cumulative dataset to bring all the values to same transformation when dealing with training the model vs making predictions. ","5aad5889":"Survival chances seem to indicate a trend based on Pclass. Female passengers have high chance of survival when compared to Male in all 3 Pclass categories.","34215ef0":"**1. Read Train and Test data sets**","44e9d232":"**Embarked (Port of Embarkation) - Values (C = Cherbourg, Q = Queenstown, S = Southampton)**","113803f4":"1.1 Identify Categorical and Numerical Features","25f54d5f":"**Age**","aabf311e":"2.2 Combine Train and Test data sets","d93f00de":"**Sex**","d08aefd0":"PassengerId is the uniqueID and will not be considered in the final features list. ","b4d209d4":"Very clear indication that Women have way better chance of survival and **Sex** column could be important to the model. ","a38c9915":"**Parch (# of parents \/ children aboard the Titanic)**","c344daae":"2.3 Correlation Mapping (With basic features)","85cc0695":"**Fare**","d2b0948c":"We can observe 2 peaks here. One between 0-5 (children) have better survival and the other one 20-40 (Adults) has less chance of survival when compared against the other target value. ","a61bf14b":"Data clearly indicates that as Pclass moves from 1 to 3, chance of survival reduces. \n\nWe can combine Pclass with Sex and see if we can find any interesting info","6be5b7b1":"This is far better skewness distribution than before. ","b55c3e5d":"2.3.1 Numerical Features","6272a334":"Both the graphs seem to look relatively same and replicate a gaussian distribution. Pasengers of ages between 20-35 seem to survive more than those of age 60-80. But you can say the same for the age group of non-survivors. \n\nThere could not be a direct correlation between Age vs Survived, but age categories more or less seem to have same progression both both target variables. \n","364a1de7":"Passengers who boarded from C (Cherbourg) seem to have better chance to survive. This could be because of the class they belong to. Let's look at the insights based on Survived vs (Pclass & Embarked)","e92ca550":"Total 11 outliers. Indices 27, 88 and 341 seem to be because of Fare. 745 seem to be because of the Age. Rest of them could be because of SibSp.  \n","6e1bbd50":"### Steps that need to be performed for a good baseline model \n1. **Read Train and Test data sets**\n\n\n2. **Exploratory Data Analysis**\n    * Find Outliers on training data\n    * Combine train\/test data & split them at the end (or) Write re-usable methods to handle train and test data\n    * Correlation Mapping (With basic features)\n    * Handle Null\/Missing values (Possibly add Null\/Non-Null Variable for each categorical feature)\n    * Handle Imputation technique on training data and apply the imputed values on Validation\/Test data sets. \n\n\n3. **Feature Engineering**\n    * Identify Latent Features\n    * Feature Selection (Based on Correlation and variance)\n\n\n4. **Models and Metrics**\n    * Split Train & Validation datasets using different K-Fold. \n    * Simple models\n    * Models with Hyperparameters\n    * Feature Selection (Based on model performance)\n    * Ensemble models\n    * Capture different metrics from the best ensemble model \n    * Confusion Matrix\n    * Pick the best model based on different metric (<i>precision_score vs accuracy_score vs recall_score vs f1_score<\/i>)\n\n\n5. **Predictions**\n    * Capture Training\/Validation Errors\n    * Feature Importance (SHAP, etc)\n    * Start making predictions ( Also metrics if the test data has target values) \n","66ca10de":"**2. Exploratory Data Analysis**\n\n2.1 Detect Outliers for numerical features","dedbbcda":"**2.3.2 Categorical Features**","df492f1d":"**Insights from missing values **\n\n1. Age and and Cabin seem to have lot of missing values\n2. All the Survived values seem to come from test data set and hence will be null anyway in the combined dataset. \n\n\nData analysis will be done on Train Data set but the missing values logic will be applied on the cumulative dataset.","957f8427":"Passengers with small families (Parch<=3) seem to have better chance to survive when compared to the ones with large Parch Value ( > 3) ","13d0104d":"**SibSp (# of siblings \/ spouses aboard the Titanic)**","e4b4e013":"**Pclass (Ticket Class) - Values (1 = 1st, 2 = 2nd, 3 = 3rd)**","aadb8679":"Fare seem to have a correlation with survived (Target Variable). We have to look at other features and build sub-features (Latent) and then check correlation again. ","ac37e630":"Passengers with SibSp with value more than 2 seem to have less chance of survival when compared to the ones with SibSp values less than or equal to 2. ","cc85aa0c":"Pclass seem to be more of categorical variable indicated in number field. We will also consider Pclass as categorical even though it is not mentioned above.  "}}