{"cell_type":{"37b7b33c":"code","35a2f1ba":"code","1a3b47de":"code","e8a62a64":"code","add79812":"code","641171d9":"code","0ec3034a":"code","946032e4":"code","9f36015b":"code","fa051000":"code","0231ab0f":"code","90a4a677":"code","4d821984":"code","03b5ee57":"code","401109d4":"code","b319e60f":"markdown","08269a66":"markdown","cbb4c26f":"markdown","4ccecf0a":"markdown","2559e7ed":"markdown","1527dfb9":"markdown","1c729431":"markdown","5dc2e00e":"markdown","31e2a94d":"markdown","1a590306":"markdown","2d7f15ad":"markdown","87951caa":"markdown","8913fef8":"markdown","179d8d7f":"markdown"},"source":{"37b7b33c":"import numpy as np\nfrom numpy.linalg import norm\nimport pickle\nfrom tqdm import tqdm, tqdm_notebook\nimport os\nimport time\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\nimport cv2","35a2f1ba":"model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3), pooling='max')","1a3b47de":"def extract_features(img_path, model):\n    \n    input_shape = (224, 224, 3)\n    img = image.load_img(img_path, target_size=(input_shape[0], input_shape[1]))\n    img_array = image.img_to_array(img)\n    \n    expanded_img_array = np.expand_dims(img_array, axis=0)\n    \n    preprocessed_img = preprocess_input(expanded_img_array)\n    \n    features = model.predict(preprocessed_img)\n    \n    normalized_features = features \/ norm(features)\n    \n    return normalized_features","e8a62a64":"features = extract_features('..\/input\/petfinder-pawpularity-score\/train\/0007de18844b0dbbb5e1f607da0606e0.jpg', model)[0]\nprint(len(features))","add79812":"extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\ndef get_file_list(root_dir):\n    file_list = []\n    counter = 1\n    for root, directories, filenames in os.walk(root_dir):\n        for filename in filenames:\n            if any(ext in filename for ext in extensions):\n                file_list.append(os.path.join(root, filename))\n                counter += 1\n    return file_list","641171d9":"# path to the datasets\nroot_dir = '..\/input\/petfinder-pawpularity-score\/train'\nfilenames = sorted(get_file_list(root_dir))","0ec3034a":"feature_list = []\nfor i in tqdm_notebook(range(len(filenames))):\n    feature_list.append(extract_features(filenames[i], model)[0])","946032e4":"neighbors = NearestNeighbors(n_neighbors=10, metric='euclidean').fit(feature_list)","9f36015b":"def plot_images(paths, title):\n    plt.figure(figsize=(20,20))\n    for i, path in enumerate(paths):\n        plt.subplot(1, len(paths), i+1)\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (512,512))\n        plt.imshow(img)\n        plt.axis('off')\n        plt.title(f'Distance: {title[i]:.2f}')\n    plt.show()","fa051000":"for i in range(30):\n    random_image_index = np.random.randint(0,len(filenames))\n    distances, indices = neighbors.kneighbors([feature_list[random_image_index]])\n    similar_image_paths = [filenames[random_image_index]] + [filenames[indices[0][i]] for i in range(1,10)]\n    plot_images(similar_image_paths, distances[0])","0231ab0f":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox","90a4a677":"# Perform PCA over the features\npca = PCA(n_components = 100)\nfeature_list_compressed = pca.fit_transform(feature_list)\n\n# Select random data points\nnumber_of_data_points = 1000\nrandom_indexes = np.random.randint(0,len(feature_list_compressed),number_of_data_points)\n\nselected_features = [feature_list_compressed[index] for index in random_indexes]\nselected_filenames = [filenames[index] for index in random_indexes]","4d821984":"images = []\nfor file in selected_filenames:\n    i = cv2.imread(file)\n    i = cv2.cvtColor(i, cv2.COLOR_BGR2RGB)\n    i = cv2.resize(i,(64,64))\n    images.append(i)  ","03b5ee57":"tsne_results = TSNE(n_components=2, metric='euclidean').fit_transform(selected_features)","401109d4":"fig, ax = plt.subplots(figsize=(20,20))\nartists = []\nfor xy, i in zip(tsne_results, images):\n    x0, y0 = xy\n    img = OffsetImage(i, zoom=0.6)\n    ab = AnnotationBbox(img, (x0, y0), xycoords='data', frameon=False)\n    artists.append(ax.add_artist(ab))\nax.update_datalim(tsne_results)\nax.autoscale()\nplt.axis('off')\nplt.show()  ","b319e60f":"Then, we provide the path to our dataset and call the function:","08269a66":"We now define a variable that will store all of the features, go through all filenames in the dataset, extract their features, and append them to the previously defined variable:","cbb4c26f":"Let\u2019s step up the game by visualizing the entire dataset! To do this, we need to reduce the dimensions of the feature vectors because it\u2019s not possible to plot a 2,048-dimension vector (the feature-length) in two dimensions (the paper). The t-distributed stochastic neighbor embedding (t-SNE) algorithm reduces the high dimensional feature vector to 2D, providing a bird\u2019s-eye view of the dataset, which is helpful in recognizing clusters and nearby images. t-SNE is difficult to scale to large datasets, so it is a good idea to reduce the dimensionality using Principal Component Analysis (PCA) and then call t-SNE:\n","4ccecf0a":"Because our aim is to search among millions of images, what we ideally need is a way to summarize the information contained in the millions of pixels in an image into a smaller representation (of say a few thousand dimensions), and have this summarized representation be close together for similar objects and further away for dissimilar items.\n\nCNNs take an image input and convert it into feature vectors of a thousand dimensions, which then act as input to a classifier that outputs the top identities to which the image might belong (say dog or cat). The feature vectors (also called embeddings or bottleneck features) are essentially a collection of a few thousand floating-point values. Going through the convolution and pooling layers in a CNN is basically an act of reduction, to filter the information contained in the image to its most important and salient constituents, which in turn form the bottleneck features. Training the CNN molds these values in such a way that items belonging to the same class have small Euclidean distance between them (or simply the square root of the sum of squares of the difference between corresponding values) and items from different classes are separated by larger distances. This is an important property that helps solve so many problems where a classifier can\u2019t be used, especially in unsupervised problems because of a lack of adequate labeled data.\n\n","2559e7ed":"# Similarity Search","1527dfb9":"# Reverse Image Search a.k.a. Instance Retrieval","1c729431":"# Work in Progress","5dc2e00e":"That\u2019s it! Let\u2019s see the feature-length that the model generates:","31e2a94d":"# Visualizing Image Clusters with t-SNE","1a590306":"# Feature Extraction\n","2d7f15ad":"Load the ResNet-50 model without the top classification layers, so we get only the bottleneck features. Then define a function that takes an image path, loads the image, resizes it to proper dimensions supported by ResNet-50, extracts the features, and then normalizes them:","87951caa":"The ResNet-50 model generated 2,048 features from the provided image. Each feature is a floating-point value between 0 and 1.","8913fef8":"Very interesting to see that we clearly have 2 main clusters between cats and dogs!","179d8d7f":"Given a photograph, our aim is to find another photo in our dataset similar to the current one.\nWe\u2019ll use Python\u2019s machine learning library scikit-learn for finding nearest neighbors of the query features; that is, features that represent a query image. We train a nearest-neighbor model to find the nearest ten neighbors based on Euclidean distance."}}