{"cell_type":{"0c1bb873":"code","72eb90fb":"code","efff241f":"code","5504e579":"code","5ee94b11":"code","4e1c4e8b":"code","d2a528b6":"code","baeb8fb1":"code","ae743ad0":"code","6d9dd9f9":"code","f29405b5":"code","e737f96b":"code","d46cde8c":"code","0297a9e2":"code","5c2552a3":"code","f30838fa":"code","f96a6078":"code","dbcacaf7":"code","f54c741e":"code","c9db9471":"code","5a29d7e2":"code","0d06a448":"code","21adf6c8":"code","753847af":"code","c9de796b":"code","42089452":"code","f807acfc":"code","c448fdb9":"code","4ec13490":"code","8c297ba6":"code","e3925374":"code","8d075524":"code","caee16c9":"code","b0f445cb":"code","c45627e7":"code","36815953":"code","28c1e77d":"code","4a3ce790":"code","de218945":"code","74557c74":"code","1ffa3039":"code","2cdb695c":"code","136f9302":"code","45e89a89":"markdown","b5a2cee0":"markdown","3662bd18":"markdown","e13ff1fd":"markdown","887e476b":"markdown","b6c2ee1a":"markdown","72d66750":"markdown","d092c665":"markdown","a64af9b4":"markdown","a8fdb30a":"markdown","2c1a3ca1":"markdown","313b15d7":"markdown","77db3ff9":"markdown","2d1f72a3":"markdown","1e1c2072":"markdown","0c3780e1":"markdown","4ea0b21e":"markdown","79d98ad2":"markdown","3beb84eb":"markdown","7fa4db73":"markdown","9d6aba66":"markdown","7f3e7c54":"markdown","564f9a30":"markdown","e4fd75e7":"markdown","413d2476":"markdown","2e488190":"markdown","cda549e2":"markdown","66a541fd":"markdown","3dc6924f":"markdown","f79e1024":"markdown","4df66f6e":"markdown","36abadfc":"markdown","28f2aded":"markdown","ebb0bc36":"markdown","f763f29d":"markdown","7dc977f6":"markdown","c59e8b06":"markdown","88bbe60c":"markdown","d2e8cce7":"markdown","51014012":"markdown"},"source":{"0c1bb873":"%%capture \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom pathlib import Path #flexible path files\nimport matplotlib.pyplot as plt #plotting\nimport seaborn as sns\nimport missingno as msno #library for missing values visualization\nimport warnings #ignoring warnings\nwarnings.filterwarnings('ignore')\nimport os\n\n%matplotlib inline","72eb90fb":"# Input data files are available in the \"..\/input\/\" directory.\n# Any results you write to the current directory are saved as output.import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","efff241f":"path = Path('\/kaggle\/input\/titanic')\ntrpath = path\/'train.csv'\ncvpath = path\/'test.csv'\n\ndf_train_raw = pd.read_csv(trpath)\ndf_test_raw = pd.read_csv(cvpath)\n\ndf_train = df_train_raw.copy(deep = True)\ndf_test  = df_test_raw.copy(deep = True)\n\ndata_cleaner = [df_train_raw, df_test_raw] #to clean both simultaneously","5504e579":"df_train.head(n=10)","5ee94b11":"df_train.info()","4e1c4e8b":"varnames = list(df_train.columns)\nfor name in varnames:\n    print(name+\": \",type(df_train.loc[1,name]))","d2a528b6":"print(\"Training Set\")\nprint(df_train.isnull().sum(axis=0))\nprint(\"Test Set\")\nprint(df_test.isnull().sum(axis=0))","baeb8fb1":"msno.matrix(df_train)","ae743ad0":"msno.bar(df_test)","6d9dd9f9":"print('Overall survival quota:')\ndf_train['Survived'].value_counts(normalize = True)","f29405b5":"plt.style.use('seaborn')","e737f96b":"fig = df_train.groupby('Survived')['Age'].plot.hist(histtype= 'bar',\n                                                    alpha = 0.7,\n                                                    fontsize = 14,\n                                                    figsize = [10,10])\nplt.legend(('Died','Survived'), fontsize = 13)\nplt.xlabel('Age', fontsize = 18)\nplt.ylabel('Count', fontsize = 18)\nplt.suptitle('Histogram of the ages of survivors and decased ones',fontsize =22)\nplt.show()","d46cde8c":"df_train['Family onboard'] = df_train['Parch'] + df_train['SibSp']\nplt.rcParams['figure.figsize'] = [20, 7]\n\nfig, axes = plt.subplots(nrows=1, ncols=3)\naxes[0].set_ylabel('Survival rate',fontsize = 18)\naxes = iter(axes.flatten())\n\ntitles = iter(['Family onboard','parents \/ children aboard','siblings \/ spouses aboard'])\nfamily_vars = ['Family onboard','Parch','SibSp']\n\n\nfor var in family_vars:\n    ax = next(axes)\n    df_train.groupby(var)['Survived'].value_counts(normalize = True).unstack().plot.bar(ax = ax, width = 0.85, fontsize = 14)\n    ax.set_xlabel(next(titles),fontsize = 18)\n    ax.legend(('Died','Survived'),fontsize = 13, loc = 'upper left')\n    plt.sca(ax)\n    plt.xticks(rotation=0)\n\nplt.suptitle('Survival rates over Number of relatives onboard',fontsize =22)\nplt.show()","0297a9e2":"fig = df_train.groupby(['Sex'])['Survived'].value_counts(normalize=True).unstack().plot.bar(figsize = [8,5], width = 0.5,fontsize = 14)\nplt.legend(('Died','Survived'),fontsize = 13, loc = 'upper left')\nplt.xlabel('Gender',fontsize =18)\nplt.xticks(rotation=0)\nplt.ylabel('Survival rate',fontsize = 18)\n\nplt.suptitle('Survival rates over Gender',fontsize =22)\nplt.show()","5c2552a3":"fig = df_train.groupby('Pclass')['Survived'].value_counts(normalize=True).unstack().plot.bar(figsize = [8,5], width = 0.5,fontsize = 14)\nplt.legend(('Died','Survived'),fontsize = 13, loc = 'upper left')\nplt.xlabel('Ticket Class',fontsize =18)\nplt.ylabel('Survival rate',fontsize = 18)\nplt.suptitle('Survival rate over Ticket class', fontsize = 22)\nplt.xticks(rotation=0)\nplt.show()","f30838fa":"df_train['Title'] = df_train['Name'].str.split(',',expand = True)[1].str.split('.',expand = True)[0].str.strip()\nvarnames = list(df_train.columns)\n    \nprint(\"Training set: \" ,list(df_train['Title'].unique()))    \ndf_test['Title'] = df_test['Name'].str.split(',',expand = True)[1].str.split('.',expand = True)[0].str.strip()\nprint(\"Test set: \" ,list(df_test['Title'].unique()))    \n","f96a6078":"def new_titles(df):\n    new_titles = dict()\n    assert 'Title' in df.columns\n    for key in df['Title'].unique():\n        females = ['Mrs','Miss','Ms','Mlle','Mme','Dona']\n        males = ['Mr','Don']\n        notable = ['Jonkheer','the Countess','Lady','Sir','Major','Col','Capt','Dr','Rev','Notable']\n        titles = [females,males,notable,'Master']\n        newtitles = ['Mrs','Mr','Notable','Master']\n        idx = [key in sublist for sublist in titles]\n        idx = np.where(idx)[0] \n        new_titles[key] = newtitles[idx[0]]\n    return new_titles\n\n\nnew_titles_dict = new_titles(df_train)\ndf_train['Title'] = df_train['Title'].replace(new_titles_dict)","dbcacaf7":"fig = df_train.groupby(['Title'])['Survived'].value_counts(normalize=True).unstack().plot.bar(figsize = [12,5], width = 0.7,fontsize = 14)\nplt.legend(('Died','Survived'),fontsize = 13, loc = 'upper left')\nplt.xlabel('Title',fontsize =16)\nplt.xticks(rotation=0)\n\nplt.suptitle('Survival rates over Title',fontsize =22)\nplt.show()","f54c741e":"df_train['Cabin'].fillna('Missing',inplace = True)\ndf_train['Cabin'] = df_train['Cabin'].str.split(r'(^[A-Z])',expand = True)[1]","c9db9471":"fig = df_train.groupby(['Cabin'])['Survived'].value_counts(normalize=True).unstack().plot.bar(figsize = [12,5], width = 0.9)\nplt.legend(('Died','Survived'),fontsize = 13, loc = 'upper left')\nplt.xlabel('Cabin Deck',fontsize =18)\nplt.suptitle('Survival rates over Cabin Deck',fontsize =22)\nplt.xticks(rotation=0)\nplt.show()","5a29d7e2":"fig = df_train.groupby(['Embarked'])['Survived'].value_counts(normalize=True).unstack().plot.bar(figsize = [10,5], width = 0.7)\nplt.legend(('Died','Survived'),fontsize = 13, loc = 'upper left')\nplt.xlabel('Embarking Port',fontsize =18)\nplt.suptitle('Survival rates over embarking port',fontsize =22)\nplt.xticks(rotation=0)\nplt.show()","0d06a448":"df_train.groupby(['Embarked'])['Pclass'].value_counts(normalize=True).unstack()","21adf6c8":"df_train.corr(method='pearson')['Age'].abs()","753847af":"def df_fill(datasets, mode):\n    assert mode =='median' or mode =='sampling'\n    datasets_cp =[]\n    np.random.seed(2)\n    varnames = ['Age','Fare']\n    for d in datasets:\n        df = d.copy(deep = True)\n        for var in varnames:\n            idx = df[var].isnull()\n            if idx.sum()>0:\n                if mode =='median':\n                    medians = df.groupby('Pclass')[var].median()\n                    for i,v in enumerate(idx):\n                        if v:\n                            df[var][i] = medians[df['Pclass'][i]]\n                else:\n                    g = df[idx==False].groupby('Pclass')[var]\n                    for i,v in enumerate(idx):\n                        if v:\n                            df[var][i] = np.random.choice((g.get_group(df['Pclass'][i])).values.flatten())\n    #Embarked                 \n        idx = df['Embarked'].isnull()\n        g = df[idx==False].groupby('Pclass')['Embarked']\n        for i,v in enumerate(idx):\n            if v:\n                df['Embarked'][i] = np.random.choice((g.get_group(df['Pclass'][i])).values.flatten())                   \n    #Cabin\n        df['Cabin'][df['Cabin'].isnull()]='Missing'\n        df['Cabin'] = df['Cabin'].str.split(r'(^[A-Z])',expand = True)[1]\n        datasets_cp.append(df)\n    return datasets_cp","c9de796b":"def prepare_data(datasets):\n        datasets_cp = []\n        for d in datasets:\n            df = d.copy(deep = True)\n            df['Family onboard'] = df['Parch'] + df['SibSp']\n            df['Title'] = df['Name'].str.split(',',expand = True)[1].str.split('.',expand = True)[0].str.strip()\n            new_titles_dict = new_titles(df)\n            df['Title'] = df['Title'].replace(new_titles_dict)\n            df.drop(columns = ['PassengerId','Name','Ticket'],axis = 1, inplace = True)\n            \n            datasets_cp.append(df)\n        return datasets_cp      ","42089452":"train,test =prepare_data(df_fill(data_cleaner,mode = 'sampling'))  \nprint(\"Training data:\")\nprint(train.isnull().sum())\nprint(\"Test data:\")\nprint(test.isnull().sum())","f807acfc":"ytrain = train['Survived']\nxtrain = train.drop('Survived',axis = 1)\nxtest = test","c448fdb9":"data = pd.concat([xtrain,xtest],copy =True)\nsex_mapping = {'male'  : 0,\n               'female': 1\n              }\n\ndata = pd.get_dummies(data,columns = ['Title', 'Cabin', 'Embarked'],drop_first = True)\ndata['Sex'] = data['Sex'].map(sex_mapping)\n\nm = xtrain.shape[0]\nx_train = data[:m].astype('float64')\nx_test = data[m:].astype('float64')\ny_train = ytrain.astype('int64')","4ec13490":"def normalize(df,cols,mu,sigma):\n    df[cols] = (df[cols]-mu)\/sigma\n    return df","8c297ba6":"x_train_mean, x_train_std = x_train[['Age','Fare']].mean(), x_train[['Age','Fare']].std()\nx_train = normalize(x_train,['Age','Fare'],x_train_mean, x_train_std)\nx_test = normalize(x_test,['Age','Fare'],x_train_mean, x_train_std)","e3925374":"from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier,RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.base import BaseEstimator, clone\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb","8d075524":"n_folds = 5\ndef acc_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(xtrain)\n    acc= cross_val_score(model, x_train, y_train, scoring=\"accuracy\", cv = kf)\n    return(acc)","caee16c9":"Adaboost = make_pipeline(RobustScaler(),\n                         AdaBoostClassifier(base_estimator=None,\n                                            n_estimators = 56,\n                                            learning_rate= 0.18,\n                                            algorithm='SAMME.R',\n                                            random_state = 1)\n                        )","b0f445cb":"GBoosting = make_pipeline(RobustScaler(), \n                          GradientBoostingClassifier(loss='deviance',\n                                                     learning_rate = 0.05,\n                                                     n_estimators = 56,\n                                                     min_samples_split = 9,\n                                                     min_samples_leaf = 2,\n                                                     max_depth = 4,\n                                                     random_state = 1,\n                                                     max_features = 9)\n                         )","c45627e7":"SVC =  make_pipeline(RobustScaler(), \n                     SVC(decision_function_shape = 'ovr',\n                         random_state = 1,\n                         max_iter = 14888,\n                         kernel = 'poly',\n                         degree = 2,\n                         coef0 = 0.49, \n                         C =  9.6)\n                     )","36815953":"RF = make_pipeline(RobustScaler(), \n                   RandomForestClassifier(criterion='gini', \n                                          n_estimators=364,\n                                          max_depth = 11,                    \n                                          min_samples_split=6,\n                                          min_samples_leaf=1,\n                                          max_features='auto',\n                                          oob_score=True,\n                                          random_state=1,\n                                          )\n                  )","28c1e77d":"xgbc = make_pipeline(RobustScaler(), \n                     xgb.XGBClassifier(n_estimators=121,\n                                       reg_lambda = 0.9,\n                                       reg_alpha = 0.5,\n                                       max_depth = 9,\n                                       learning_rate = 0.55,\n                                       gamma = 0.5,\n                                       colsample_bytree = 0.4,\n                                       coldsample_bynode = 0.15,\n                                       colsample_bylevel = 0.5)\n                    )","4a3ce790":"score = acc_cv(Adaboost)\nprint(\"Adaboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = acc_cv(GBoosting)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = acc_cv(SVC)\nprint(\"SVC  score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = acc_cv(RF)\nprint(\"Random Forest score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = acc_cv(xgbc)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","de218945":"class AveragingModels(BaseEstimator):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","74557c74":"%%capture\naveraged_models = AveragingModels(models = (Adaboost,SVC, GBoosting, RF,xgbc))\naveraged_models.fit(x_train, y_train)","1ffa3039":"train_pred = averaged_models.predict(x_train)\ntest_pred = averaged_models.predict(x_test)","2cdb695c":"train_pred = np.round(train_pred)\ntest_pred = np.round(test_pred)\n\nacc_averaged = np.round((train_pred==y_train).sum()\/train_pred.shape[0],5)\nprint(f\"Averaged models accuracy: {acc_averaged}\")","136f9302":"submission = pd.DataFrame()\nsubmission['PassengerId'] = df_test['PassengerId'].astype('int32')\nsubmission['Survived'] = test_pred\nsubmission['Survived'] = submission['Survived'].astype('int32')\nsubmission.to_csv('submission.csv',index=False)","45e89a89":"Let's first take a look at the first couple of rows of the training data, as well as the types of variables that the dataframe posesses and their corresponding value types.","b5a2cee0":"There are no more missing values in our data. The last step before training the model is to convert ordinal and nominal data to a trainable format, which is done with the help of dummy variables. To avoid colinearity, the first column of the generated dummy variables gets dropped. Additionally, the age and fare variables are normalized to minimize the impact of large values on the classification procedure.","3662bd18":"We see a clear trend that passengers with a family size between 1 and 3 had the higher the chance of survival. They are the only columns where the survivors are more than the deceased ones. Family size also combines the other 2 variables nicely and gives a more clear picture of the survival chances. Therefore, we conclude that this is an interesting feature to include in our training data.","e13ff1fd":"This notebook contains an approach to predict the survivors of the Titanic ship sinking using an ensemble of various classification techniques. The model achieves ~85% accuracy on the training set and ~80% on the test set. Every step in the process, from getting the data to predicting the survivors is thoroughly documented and explained. ","887e476b":"As expected, first class passengers have a higher survival rate, meaning they were either given priority during evacuation or they were closer to the lifeboats. THis can be double-checked through the cabin feature that will be discussed later. \n\nWe would now to check if the title name of a person can be useful in determining whether that person survived or not. This assumption stems from the idea that people of higher status could have been given higher priority during the ship's evacuation.  Therefore, we create a new variable called 'Title'.","b6c2ee1a":"Again, we see that different titles have different survival probabilities. A small surprising result is that people under the title 'Notable' have a low survival rate. One would expect that 'notable' people would travel first class and would therefore have a higher survival chance (see above), but is appears that this is not the case. This result indicates that the higher survival rates of the first class passengers' have to do with their positioning on the ship. We shall now examine that. To do that, we only keep the cabin deck portion of the 'cabin' variable and, since there are a lot of missing cabin information, the missing values are denoted as 'M' for missing. ","72d66750":"Finally, we initially thought that the embarking port should be irrelevant to the task. However, passengers that embarked the ship in Cherbourg were more likely to survive. An explanation for that could be that more rich people embarked the ship and were travelling in a better class.","d092c665":"Data file locations:","a64af9b4":"# 2. Undestanding the Data","a8fdb30a":"First, explore how to fill in the missing ages. Several strategies pinpoint to replace the missing values with the mean or median of the whole distribution, which in our eyes doesn't seem a good choice. Instead, let's look into the correlation of age with the other variables.","2c1a3ca1":"We are now going to ensure that there are no missing values in the dataset and prepare it for training our model. The 4 categories that have missing values in the train and test sets are:\n* Age \n* Cabin \n* Embarked \n* Fare\n\nIn order to ease the documents' readability, any extra variables created above will be recreated here from scratch and will be encapsulated in a function. This is done to make it easier to the reader to find all feature engineering procedures in one place.","313b15d7":"The variables included in the data are:\n\n* ***PassengerId***: Passenger index\n* Survived: Whether the passenger in the accident. Possible values:\n    * 0 = died , \n    * 1 = survived\n* ***Pclass***: Passenger class. Possible values:\n    * 1 = First class\n    * 2 = Second class\n    * 3 = Third class\n* ***Name***: Passenger name\n* ***Sex***: Passenger gender. Possible values:\n    * male\n    * female\n* ***Age***: Passenger Age\n* ***SibSp***: Number of siblings\/spouses on board\n* ***Parch***: Number of parents\/children on board\n* ***Ticket***: Ticket number\n* ***Fare***: Ticket cost\n* ***Cabin***: Cabin number\n* ***Embarked***: Port of Embarkation. Possible values:\n    * C = Cherbourg\n    * Q = Queenstown\n    * S = Southampton","77db3ff9":"We see that the strongest correlation of the variable age is with the variable Pclass (passenger class). Therefore, it is appropriate to use this information in order to sample the missing ages according to the pclass. We can either take the median of each Pclass group or sample a random value from that group. We are going to try both and see which one yields better results. Sampling from a distribution, however, seems like the more viable option, since we have a lot of missing values to replace and setting all of them to the same value would skew the distribution massively. For the other missing variables ( Fare, Embarked), the analysis we performed above leads us to believe that sampling according to the passengers' class is a viable method.","2d1f72a3":"We can see that there are 2 major categories with missing data, as well as a couple of missing values in other two.\n\n**Major missing value variables: **\n* Age\n* Cabin\n\n**Minor missing value variables: **\n* Fare\n* Embarked","1e1c2072":"We also see that female passengers had a higher chance of survival than male ones. It was expected that females and children would be more likely to survive, as the evacuation protocol of the ship was instructing accordingly. Let us now compare the survival chances and the passengers' ticket class.","0c3780e1":"The model idea is to create 5 individual classifiers and use them as an ensemble to create the final prediction. After experimenting with various classifiers, the 5 with the best performance were chosen. These include:\n* Adaboost\n* Gradient Boosting Classifier\n* SVC\n* Random Forest Classifier\n* XGBoost Classifier\n\nThe performance of each classifier is evaluated using a 5-fold cross validation strategy on the training set, where the accuracy of the classification is measured. This procedure is wrapped in the function acc_cv.","4ea0b21e":"# 5. Model Training","79d98ad2":"**Random Forest**","3beb84eb":"Our classifier has achieved an accuracy of ~88% percent on the training set, which is higher than the perfomance of any single classifier.","7fa4db73":"Since we are averaging over 5 binary classification predictions, a strategy is needed that will decide which passengers will be marked as survivors and which not. After experimenting with different strategies, it has been decided that a passenger is marked as a survivor if the majority of the classifiers have predicted so. ","9d6aba66":"We can now check the survival rates for each title to see if there is some useful information here.","7f3e7c54":"Before we start cleaning up the data, it is important to see which variables are of relevance, which can be ignored  and what is the most appropriate way to fill in the missing values. As we can see in the charts above, there are 3 variables with missing values in the training set(Age,Cabin and Embarked) and only 2 in the test set (Age,Cabin). In the test set, there is also 1 fare entry missing, which we will fill later on. We shall now try and decide what we are going to do with those values.","564f9a30":"It is very important to understand whether and where there are missing values in the data (both train and test). This will help us determine a strategy for filling in the missing values.","e4fd75e7":"**Gradient Boosting**","413d2476":"# 4. Data Cleaning","2e488190":"**XGB**","cda549e2":"**Adaboost**","66a541fd":"Some of these titles can be grouped up, since they mean the same thing. For example, \"Mrs\", \"Miss\", \"Ms\" will be grouped together under the label \"Mrs\". There are also some titles that appear to actually be a name instead of a title (Mlle, Mme, Dona) that will also be mapped to the same value. \"Don\" is probably an abbreviation to a male name and will be mapped to \"Mr\". The rest of the titles denote nobility, military or clergy service and doctors. To avoid sparse categories, they are all grouped under the title 'Notable'. Finally, 'Master' is kept as a standalone title that was given to men under 26 years of age.","3dc6924f":"# 1. Reading the data and setting up the environment","f79e1024":"We see that the ages distribution between those who survived and those who did not is similar. We see, however, that more young-aged passengers were saved. This was expected, since it lines up with ship evacuation policies. Other than that, age is probably not a major factor that determined who survived the accident.","4df66f6e":"**SVC**","36abadfc":"To average the results of the models, a new class was created.","28f2aded":"Let's now explore the impact that the amount of relatives on board had on survival. For that, we create a new feature called 'Family onboard', which is the sum of parents\/children\/siblings\/spouses (variables Parch and SibSp).","ebb0bc36":"**Scores**","f763f29d":"Now we can read the data into Pandas dataframes. A copy of the original data is kept should we require it later. Both training and test datasets are put together in a list so that we can iterate over both at the same time during data cleaning. ","7dc977f6":"**Setting up plotting parameters:**","c59e8b06":"To find the optimal parameters for each classifier, the RandomizedSearchCV function from the sklearn library was used to find the parameters that perform best on the cross-validated training set. The parameter tuning code has been omitted to ease readability. Every classifier is preceded by the RobustScaler() function that transforms the data to reduce outliers' impact.","88bbe60c":"The first step to analyzing the data is to load all the libraries we are going to use. This is performed at the start so that we can know at any point which libraries are loaded in the notebook. ","d2e8cce7":"We see that the cabin decks have different survival rates. As for the ones where the data was missing, the rates line up with the overall survival rate of the ship (~68%-32%).\n\n","51014012":"# 3. Exploratory Data Analysis"}}