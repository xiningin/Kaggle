{"cell_type":{"69dcd432":"code","0790db63":"code","0b053b65":"code","1f5d727f":"code","5e64a196":"code","7911c580":"code","610543e2":"code","12a1fc13":"code","31d6d191":"code","d8595fe1":"code","c27b6cf0":"code","dd6462d7":"code","fa9d019f":"code","0e0e24e7":"code","7b2f277a":"code","d21857ab":"code","65467a2a":"code","f4e1c338":"code","04b622fd":"code","f6c9e345":"code","4ef57c9d":"code","5a1768c0":"code","64372d56":"markdown","a7d2c831":"markdown","d37e1777":"markdown"},"source":{"69dcd432":"import numpy as np\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport shap\nfrom sklearn import preprocessing","0790db63":"plt.style.use('seaborn')\nplt.rcParams['figure.figsize'] = 12, 7","0b053b65":"def make_test_train(df, split=0.80):\n    # Label encode the assetCode feature\n    X = df[df.universe==1]\n    le = preprocessing.LabelEncoder()\n    X = X.assign(assetCode = le.fit_transform(X.assetCode))\n    \n    # split test and train\n    train_ct = int(X.shape[0]*split)\n    y_train, y_test = X['returnsOpenNextMktres10'][:train_ct], X['returnsOpenNextMktres10'][train_ct:]\n    X = X.drop(['time', 'returnsOpenNextMktres10'], axis=1)\n    X_train, X_test = X.iloc[:train_ct,], X.iloc[train_ct:,]\n    return X, X_train, X_test, y_train, y_test","1f5d727f":"def make_lgb(X_train, X_test, y_train, y_test, categorical_cols = ['assetCode']):\n    # Set up LightGBM data structures\n    train_cols = X_train.columns.tolist()\n    dtrain = lgb.Dataset(X_train.values, y_train, feature_name=train_cols, categorical_feature=categorical_cols)\n    dvalid = lgb.Dataset(X_test.values, y_test, feature_name=train_cols, categorical_feature=categorical_cols)\n    return dtrain, dvalid","5e64a196":"# Set up the LightGBM params\nlgb_params = dict(\n    objective='regression_l1', learning_rate=0.1, num_leaves=127, max_depth=-1, bagging_fraction=0.75,\n    bagging_freq=2, feature_fraction=0.5, lambda_l1=1.0, seed=1015\n)","7911c580":"# @marketneutral: Create some random assetCodes (this is a nice snippet from McKinney, Wes. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython (p. 340). O'Reilly Media. Kindle Edition.)\nimport random; random.seed(0)\nimport string\nnum_stocks = 1250\ndef rands(n):\n    choices = string.ascii_uppercase\n    return ''.join([random.choice(choices) for _ in range(n)])\nassetCodes = np.array([rands(5) for _ in range(num_stocks)])","610543e2":"# @marketneutral:  Spoof intraday and overnight returns\ndays_in_year = 260\ntotal_days = days_in_year*7\non_vol_frac = 0.2  # overnight volatility fraction\n\nannualized_vol = 0.20\nopen_to_close_returns = np.random.normal(0.0, scale=annualized_vol*(1-on_vol_frac)\/np.sqrt(days_in_year), size=(total_days, num_stocks))\nclose_to_open_returns = np.random.normal(0, scale=annualized_vol*(on_vol_frac)\/np.sqrt(days_in_year), size=(total_days, num_stocks))\nopen_to_open_returns = close_to_open_returns + open_to_close_returns \nclose_to_close_returns = close_to_open_returns + np.roll(open_to_close_returns, -1)\n\n# Make price series\nprices_close = 100*np.cumprod(1+close_to_close_returns, axis=0)\nprices_open = prices_close*(1+close_to_open_returns)","12a1fc13":"import itertools\n\n# Make into a DataFrame\ndates = pd.date_range(end=pd.Timestamp('2017-12-31'), periods=total_days)\nspoofed_df = pd.DataFrame(\n    data={'close': prices_close.flatten('F'), 'open': prices_open.flatten('F')},\n    index = pd.MultiIndex.from_tuples(\n        list(itertools.product(assetCodes, dates)), names=('assetCode', 'time')\n    )\n)\nspoofed_df['universe'] = 1.0","31d6d191":"spoofed_df.head()","d8595fe1":"spoofed_df = spoofed_df.reset_index().sort_values(['assetCode','time']).set_index(['assetCode', 'time'])","c27b6cf0":"# @marketneutral: make sure we did the open\/close transform properly. Looks good.\nspoofed_df.loc['MYNBI', ['open', 'close']]['1Q2013'].plot();","dd6462d7":"#  @marketneutral: Make the \"return\" based features\n\nspoofed_df = spoofed_df.assign(\n     returnsClosePrevRaw1 = spoofed_df.groupby(level='assetCode').\n     apply(lambda x: x.close\/x.close.shift(1) -1)\n     .reset_index(0, drop=True)\n)\n\nspoofed_df = spoofed_df.assign(\n     returnsOpenPrevRaw1 = spoofed_df.groupby(level='assetCode').\n     apply(lambda x: x.open\/x.open.shift(1) -1)\n     .reset_index(0, drop=True)\n)\n\nspoofed_df = spoofed_df.assign(\n     returnsOpenPrevRaw10 = spoofed_df.groupby(level='assetCode').\n     apply(lambda x: (x.open\/x.open.shift(10)) - 1)\n     .reset_index(0, drop=True)\n)\n\nspoofed_df = spoofed_df.assign(\n     returnsClosePrevRaw10 = spoofed_df.groupby(level='assetCode').\n     apply(lambda x: x.close\/x.close.shift(10)-1)\n     .reset_index(0, drop=True)\n)\n","fa9d019f":"#  @marketneutral: Make the target variable\nspoofed_df = spoofed_df.assign(\n    returnsOpenNextMktres10 = spoofed_df.groupby(level='assetCode').\n    apply(lambda x: (x.open.shift(-10)\/x.open)-1)\n    .reset_index(0, drop=True)\n)","0e0e24e7":"# @marketneutral: Drop the edges where we don't have data to make returns\nspoofed_df = spoofed_df.reset_index().dropna()","7b2f277a":"spoofed_df.head()","d21857ab":"#Derived from Advances in Financial Machine Learning, pp 116-117, Marcos Lopez de Prado\n\nfrom sklearn.model_selection._split import KFold\nimport datetime as dt\n\ndef featImpMDA_regress(reg,X,y,cv):\n    # feat importance based on OOS score reduction\n    print('start MDA',dt.datetime.now())\n    from sklearn.metrics import mean_squared_error\n    cvGen=KFold(n_splits=cv)\n    scr0,scr1=pd.Series(),pd.DataFrame(columns=X.columns)\n    for i,(train,test) in enumerate(cvGen.split(X=X)):\n        print('   Split',i+1)\n        X0,y0=X.iloc[train,:],y.iloc[train]\n        X1,y1=X.iloc[test,:],y.iloc[test]\n        fit=reg.fit(X=X0,y=y0)\n        pred=fit.predict(X1)\n        scr0.loc[i]=mean_squared_error(y1,pred)\n        for j in X.columns:\n            X1_=X1.copy(deep=True)\n            np.random.shuffle(X1_[j].values) # permutation of a single column\n            pred=fit.predict(X1_)\n            scr1.loc[i,j]=mean_squared_error(y1,pred)\n    imp=scr1.subtract(scr0,axis=0) #Increases in error\n    imp=pd.concat({'mean':imp.mean(),'std':imp.std()*imp.shape[0]**-.5},axis=1)\n    print('end MDA',dt.datetime.now())\n    return imp","65467a2a":"# From rfpimp: https:\/\/pypi.org\/project\/rfpimp\/\n\ndef plot_importances(df_importances, save=None, xrot=0, tickstep=3,\n                     label_fontsize=12,\n                     figsize=None, scalefig=(1.0, 1.0), show=True):\n    \"\"\"\n    Given an array or data frame of importances, plot a horizontal bar chart\n    showing the importance values.\n\n    :param df_importances: A data frame with Feature, Importance columns\n    :type df_importances: pd.DataFrame\n    :param save: A filename identifying where to save the image.\n    :param xrot: Degrees to rotate importance (X axis) labels\n    :type xrot: int\n    :param tickstep: How many ticks to skip in X axis\n    :type tickstep: int\n    :param label_fontsize:  The font size for the column names and x ticks\n    :type label_fontsize:  int\n    :param figsize: Specify width and height of image (width,height)\n    :type figsize: 2-tuple of floats\n    :param scalefig: Scale width and height of image (widthscale,heightscale)\n    :type scalefig: 2-tuple of floats\n    :param show: Execute plt.show() if true (default is True). Sometimes\n                 we want to draw multiple things before calling plt.show()\n    :type show: bool\n    :return: None\n\n    SAMPLE CODE\n\n    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True)\n    X_train, y_train = ..., ...\n    rf.fit(X_train, y_train)\n    imp = importances(rf, X_test, y_test)\n    plot_importances(imp)\n    \"\"\"\n    I = df_importances\n\n    if figsize:\n        fig = plt.figure(figsize=figsize)\n    elif scalefig:\n        fig = plt.figure()\n        w, h = fig.get_size_inches()\n        fig.set_size_inches(w * scalefig[0], h * scalefig[1], forward=True)\n    else:\n        fig = plt.figure()\n    ax = plt.gca()\n    labels = []\n    for col in I.index:\n        if isinstance(col, list):\n            labels.append('\\n'.join(col))\n        else:\n            labels.append(col)\n\n    for tick in ax.get_xticklabels():\n        tick.set_size(label_fontsize)\n    for tick in ax.get_yticklabels():\n        tick.set_size(label_fontsize)\n    ax.barh(np.arange(len(I.index)), I.Importance, height=0.6, tick_label=labels)\n\n    # rotate x-ticks\n    if xrot is not None:\n        plt.xticks(rotation=xrot)\n\n    # xticks freq\n    xticks = ax.get_xticks()\n    nticks = len(xticks)\n    new_ticks = xticks[np.arange(0, nticks, step=tickstep)]\n    ax.set_xticks(new_ticks)\n\n    if save:\n        plt.savefig(save, bbox_inches=\"tight\", pad_inches=0.03)\n    if show:\n        plt.show()","f4e1c338":"def make_regress(df):\n    # Label encode the assetCode feature\n    X = df[df.universe==1]\n    le = preprocessing.LabelEncoder()\n    X = X.assign(assetCode = le.fit_transform(X.assetCode))\n  \n    y = X['returnsOpenNextMktres10']\n    X = X.drop(['time', 'returnsOpenNextMktres10'], axis=1)\n    \n    return X, y","04b622fd":"X_r,y_r=make_regress(spoofed_df)","f6c9e345":"regress = lgb.LGBMModel(objective='regression_l1', learning_rate=0.1, num_leaves=127, max_depth=-1, bagging_fraction=0.75,\n    bagging_freq=2, feature_fraction=0.5, lambda_l1=1.0, seed=1015)","4ef57c9d":"mda_imps_r=featImpMDA_regress(regress,X_r,y_r,cv=10)","5a1768c0":"import matplotlib\n%matplotlib inline\nm_imps=mda_imps_r.reset_index().rename(index=int,columns={\"index\":\"Feature\",'mean':'Importance'}, inplace=False).set_index('Feature')\nm_imps=m_imps.sort_values(by='Importance')\nplot_importances(m_imps,scalefig=(1.5,1.0))","64372d56":"\n## Spoof Completely Random Dataset\n\nCreate a random DataFrame similar to the `market_data_df` DataFrame.","a7d2c831":"assetCode is the least important feature (among all the market features).","d37e1777":"## assetCode with MDA using random data\n\nIn the kernel \"The fallacy of encoding assetCode\", @marketneutral makes the point that encoding assetCode and using either of the lightGBM built in feature importances (split and gain) results in assetCode being a significant feature. The advice was \"Don't encode assetCode\".\n\nIn this kernel, we experiment with using MDA feature importances as described in \"Advances in Financial Machine Learning\" by Marcos Lopez de Prado.\n\nWe use the code from @marketneutral's kernel to generate random data for the test.\n\nUsing this process, assetCode has no importance."}}