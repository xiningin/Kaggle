{"cell_type":{"87e7525a":"code","46af6aba":"code","844b9d65":"code","fd656a92":"code","c6b97d25":"code","73dbf37a":"code","bba50010":"code","a9d98fc6":"code","3b3f2929":"code","f73d41fc":"code","ea29e1f0":"code","ec4d3aeb":"code","da882bb4":"code","149a170c":"code","32b1e551":"code","b5f7a159":"code","1ae0fd29":"code","9ec3012f":"code","ca017117":"code","0d10dc17":"code","7358998f":"code","c6eb8a45":"code","f68d37f5":"code","64def90e":"code","52fec6b5":"code","abbdd8de":"code","419ce0b2":"code","93d436c2":"code","e9aae309":"code","dc0ed023":"code","9556f5f8":"code","c50637a5":"code","068d6ef6":"code","6587ebc0":"code","0ca45c2c":"code","5ba81e2a":"code","a403711e":"code","0364fc77":"code","bc005403":"code","99641862":"code","37780bf9":"code","5eec57d8":"code","a00896e0":"code","f490726b":"code","061514a9":"code","ebdbfdb8":"code","70db3105":"code","393ef9b4":"markdown","17c2eac9":"markdown","28e257a2":"markdown","7a8ae8df":"markdown","7cd34c7e":"markdown","4103f4dc":"markdown","4ebdb4e1":"markdown","502ecb24":"markdown","c4eaacc2":"markdown","4e6473a5":"markdown","3fcb38f5":"markdown","4c97538d":"markdown","002786ae":"markdown","5d0860ec":"markdown","5a1ac0ae":"markdown","a178dfc5":"markdown","5b8d030f":"markdown","e469dd36":"markdown","db2fcd3e":"markdown","71544d75":"markdown","7e7c1ae0":"markdown","33936f61":"markdown","c3a4addf":"markdown","6185eb89":"markdown","d5e6d1ff":"markdown","65137c87":"markdown","b617b731":"markdown","dcf0fdae":"markdown","88d418b2":"markdown","4a7759fd":"markdown","75bc4971":"markdown","20de37cf":"markdown","968bc173":"markdown","d8cb7e1f":"markdown","5ff66463":"markdown","81a7135b":"markdown"},"source":{"87e7525a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","46af6aba":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","844b9d65":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","fd656a92":"print(test_data.keys())\nprint(train_data.keys())","c6b97d25":"#print(train_data.dtypes)\ntypes_train = train_data.dtypes\nprint(types_train)\nnum_types = types_train[types_train == float]\n#print(num_types)","73dbf37a":"train_data.describe()","bba50010":"test_data.describe()","a9d98fc6":"print(\"Train data Frame\")\ntrain_data.count()\nprint(train_data.isnull().sum())","3b3f2929":"print(test_data.isnull().sum())","f73d41fc":"train_data.drop(labels = [\"Cabin\",\"Ticket\"], axis = 1, inplace=True)\ntest_data.drop(labels = [\"Cabin\",\"Ticket\"], axis = 1, inplace=True)\nprint(set(train_data[\"Embarked\"]))\nprint(set(train_data[\"Sex\"]))","ea29e1f0":"#sns.distplot(test_data[\"Age\"])\ncopy = train_data.copy()\ncopy.dropna(inplace=True)\nsns.distplot(copy[\"Age\"])","ec4d3aeb":"train_data[\"Age\"].fillna(train_data[\"Age\"].median(), inplace = True)\ntest_data[\"Age\"].fillna(train_data[\"Age\"].median(), inplace = True)\ntest_data[\"Fare\"].fillna(train_data[\"Fare\"].mean(), inplace = True)\ntrain_data.dropna(inplace=True)\n#test_data.dropna(inplace=True)\ntest_data.count()","da882bb4":"train_data.head()","149a170c":"test_data.head()","32b1e551":"sns.barplot(x=\"Sex\", y=\"Survived\", data=train_data)\nplt.title(\"Distribution based on gender\")\nplt.show()\ntotal_survived_females = train_data.loc[train_data[\"Sex\"] == \"female\"][\"Survived\"].sum()\ntotal_survived_males = train_data.loc[train_data[\"Sex\"] == \"male\"][\"Survived\"].sum()\ntotal_survivals = total_survived_females + total_survived_males\n\nprint(\"Total people survived is: \" + str((total_survived_females + total_survived_males)))\nprint(\"Proportion of Females who survived:\") \nprint(total_survived_females\/(total_survived_females + total_survived_males))\nprint(\"Proportion of Males who survived:\")\nprint(total_survived_males\/(total_survived_females + total_survived_males))\n","b5f7a159":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nwomen.head(20)\nrate_women = sum(women)\/len(women)\nprint(\"% of women who survived: \", rate_women)","1ae0fd29":"sns.barplot(x=\"Pclass\", y=\"Survived\", data = train_data)\nplt.title(\"Distribution of Survival based on Class\")\nplt.show()","9ec3012f":"total_survived_one = train_data[train_data.Pclass == 1][\"Survived\"].sum()\ntotal_survived_two = train_data[train_data.Pclass == 2][\"Survived\"].sum()\ntotal_survived_three = train_data[train_data.Pclass == 3][\"Survived\"].sum()\ntotal_survived = total_survived_one + total_survived_two + total_survived_three\nprint(f\"Total survivors {total_survived}\")\nprint(f\"Proportion of first class passengers amongst survivors: {total_survived_one\/total_survived}\")\nprint(f\"Proportion of second class passengers amongst survivors: {total_survived_two\/total_survived}\")\nprint(f\"Proportion of third class passengers amongst survivors: {total_survived_three\/total_survived}\")","ca017117":"sns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data = train_data)\nplt.title(\"Survival Rate based on Gender and Class\")\nplt.show()","0d10dc17":"sns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data = train_data)\nplt.title(\"Survival Rate based on Gender and Class\")\nplt.show()","7358998f":"survived_ages = train_data[train_data.Survived == 1][\"Age\"]\nnot_survived_ages = train_data[train_data.Survived == 0][\"Age\"]\nplt.subplot(1,2,1)\nsns.distplot(survived_ages, kde = False)\nplt.axis([0, 100, 0, 150])\nplt.title(\"Survived\")\nplt.subplot(1,2,2)\nsns.distplot(not_survived_ages, kde = False)\nplt.title(\"Deceased\")\nplt.axis([0, 100, 0, 150])\nplt.show()","c6eb8a45":"sns.stripplot(x = \"Survived\", y = \"Age\", data = train_data, jitter = True)","f68d37f5":"sns.pairplot(train_data)","64def90e":"train_data.sample(5)","52fec6b5":"print(set(train_data[\"Embarked\"]))\nprint(set(train_data[\"Sex\"]))","abbdd8de":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train_data[\"Sex\"])\ntrain_data[\"Sex\"] = le.transform(train_data[\"Sex\"])\ntest_data[\"Sex\"] = le.transform(test_data[\"Sex\"])\nle_embarked = LabelEncoder()\nle_embarked.fit(train_data[\"Embarked\"])\ntrain_data[\"Embarked\"] = le_embarked.transform(train_data[\"Embarked\"])\ntest_data[\"Embarked\"] = le_embarked.transform(test_data[\"Embarked\"])\ntrain_data.sample(5)","419ce0b2":"train_data[\"FamSize\"] = train_data[\"SibSp\"] + train_data[\"Parch\"] + 1\ntest_data[\"FamSize\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1","93d436c2":"train_data[\"IsAlone\"] = train_data.FamSize.apply(lambda x: 1 if x == 1 else 0)\ntest_data[\"IsAlone\"] = test_data[\"FamSize\"].apply(lambda x: 1 if x == 1 else 0)\ntest_data.head(5)","e9aae309":"train_data[\"Title\"] = train_data[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\ntest_data[\"Title\"] = test_data[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\ntrain_data.head(5)","dc0ed023":"test_data.head(5)","9556f5f8":"titles = set(train_data[\"Title\"])\nprint(titles)","c50637a5":"from collections import Counter\ntitle_counts = Counter(train_data[\"Title\"])\n#for title, count in title_counts.items():\n#    print(title, count)\ntitle_df = pd.DataFrame.from_dict(title_counts, orient='index').reset_index()\nprint(title_df)","068d6ef6":"title_replacements = {\"Mlle\": \"Other\", \"Major\": \"Other\", \"Col\": \"Other\", \"Sir\": \"Other\", \"Don\": \"Other\", \"Mme\": \"Other\",\n          \"Jonkheer\": \"Other\", \"Lady\": \"Other\", \"Capt\": \"Other\", \"Countess\": \"Other\", \"Ms\": \"Other\", \"Dona\": \"Other\"}\n\ntrain_data[\"Title\"].replace(title_replacements, inplace=True)\ntest_data[\"Title\"].replace(title_replacements, inplace=True)\nfrom collections import Counter\ntitle_counts = Counter(train_data[\"Title\"])\n#for title, count in title_counts.items():\n#    print(title, count)\ntitle_df = pd.DataFrame.from_dict(title_counts, orient='index').reset_index()\nprint(title_df)","6587ebc0":"le_title = LabelEncoder()\nle_title.fit(train_data[\"Title\"])\ntrain_data[\"Title\"] = le_title.transform(train_data[\"Title\"])\ntest_data[\"Title\"] = le_title.transform(test_data[\"Title\"])\ntrain_data.drop(\"Name\",  axis=1 , inplace = True)\ntest_data.drop(\"Name\",  axis=1 , inplace = True)\ntrain_data.sample(5)","0ca45c2c":"test_data.sample(5)","5ba81e2a":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nages_train = np.array(train_data[\"Age\"]).reshape(-1,1)\nfares_train = np.array(train_data[\"Fare\"]).reshape(-1,1)\nages_test = np.array(test_data[\"Age\"]).reshape(-1,1)\nfares_test = np.array(test_data[\"Fare\"]).reshape(-1,1)\ntrain_data[\"Age\"] = scaler.fit_transform(ages_train)\ntest_data[\"Age\"] = scaler.fit_transform(ages_test)\ntrain_data[\"Fare\"] = scaler.fit_transform(fares_train)\ntest_data[\"Fare\"] = scaler.fit_transform(fares_test)","a403711e":"train_data.sample(5)\ntrain_data.dtypes","0364fc77":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","bc005403":"from sklearn.metrics import make_scorer, accuracy_score","99641862":"from sklearn.model_selection import GridSearchCV","37780bf9":"X_train = train_data.drop(labels=[\"PassengerId\",\"Survived\"], axis = 1)\ny_train = train_data[\"Survived\"]\nX_test = test_data.drop([\"PassengerId\"], axis = 1)\nX_train.sample(5)","5eec57d8":"from sklearn.model_selection import train_test_split\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state = 0)","a00896e0":"svc_clf = SVC()\nparameters_svc = {\"kernel\": [\"rbf\", \"linear\"], \"probability\": [True, False], \"verbose\": [True, False]}\ngrid_svc = GridSearchCV(svc_clf, parameters_svc, scoring = make_scorer(accuracy_score))\ngrid_svc.fit(X_training, y_training)\nsvc_clf = grid_svc.best_estimator_\nsvc_clf.fit(X_training, y_training)\npred_svc = svc_clf.predict(X_valid)\nacc_svc = accuracy_score(y_valid, pred_svc)\nprint(\"The Score for SVC is: \" + str(acc_svc))","f490726b":"linsvc_clf = LinearSVC()\n\nparameters_linsvc = {\"multi_class\": [\"ovr\", \"crammer_singer\"], \"fit_intercept\": [True, False], \"max_iter\": [100, 500, 1000, 1500]}\n\ngrid_linsvc = GridSearchCV(linsvc_clf, parameters_linsvc, scoring=make_scorer(accuracy_score))\ngrid_linsvc.fit(X_training, y_training)\n\nlinsvc_clf = grid_linsvc.best_estimator_\n\nlinsvc_clf.fit(X_training, y_training)\npred_linsvc = linsvc_clf.predict(X_valid)\nacc_linsvc = accuracy_score(y_valid, pred_linsvc)\n\nprint(\"The Score for LinearSVC is: \" + str(acc_linsvc))","061514a9":"rf_clf = RandomForestClassifier()\n\nparameters_rf = {\"n_estimators\": [4, 5, 6, 7, 8, 9, 10, 15], \"criterion\": [\"gini\", \"entropy\"], \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n                 \"max_depth\": [2, 3, 5, 10], \"min_samples_split\": [2, 3, 5, 10]}\n\ngrid_rf = GridSearchCV(rf_clf, parameters_rf, scoring=make_scorer(accuracy_score))\ngrid_rf.fit(X_training, y_training)\n\nrf_clf = grid_rf.best_estimator_\n\nrf_clf.fit(X_training, y_training)\npred_rf = rf_clf.predict(X_valid)\nacc_rf = accuracy_score(y_valid, pred_rf)\n\nprint(\"The Score for Random Forest is: \" + str(acc_rf))","ebdbfdb8":"model_performance = pd.DataFrame({\"Model\":[\"SVC\", \"Linear SVC\", \"Random Forest\"],\"Accuracy\":[acc_svc, acc_linsvc, acc_rf]})\nmodel_performance.sort_values(by=\"Accuracy\",ascending = False)","70db3105":"print(test_data.shape)\nsvc_clf.fit(X_train, y_train)\nsubmission_predictions = svc_clf.predict(X_test)\nsubmission = pd.DataFrame({\"PassengerId\": test_data[\"PassengerId\"], \"Survived\": submission_predictions})\nsubmission.to_csv(\"titanic.csv\", index = False)\nprint(submission.shape)","393ef9b4":"## 9. Submission ","17c2eac9":"We may also come up with an IsAlone feature, indicating whether the passenger is alone on the ship or not","28e257a2":"## 2. Loading and Viewing Data Set\nWe'll use pandas to load train and test data that we'll later use to train our model and measure its performace. Before we begin, we should take a look at the data we'll be dealing with, the names of columns and their types.","7a8ae8df":"We may also derive insights from the name, not the actual names themselves but the title. We can extract the respective titles from the names and encode them like we did with Sex or Embarked.","7cd34c7e":"### SVC Model ","4103f4dc":"importing sklearn models","4ebdb4e1":"class","502ecb24":"Let's look at the various titles","c4eaacc2":"We see that gender appears to be a very good feature for the model, let's try to gain similar insights on the class feature and determine its importance in the model","4e6473a5":"gender","3fcb38f5":"Finally we plot pairplots for all features of the training dataset that shows correlation between the training features","4c97538d":"## 6. Feature Rescaling","002786ae":"### Random Forest Model","5d0860ec":"### Creating Synthetic Features\nSometimes synthesized features may improve the predictions of our ML models.","5a1ac0ae":"It is evident that class also plays a role in survival of the passenger, since passengers in class 1 are more likely to survive","a178dfc5":"Nice! Now let's look at the imputed data","5b8d030f":"There are three values for Embarked: C, S, Q and two values for Sex: female, male","e469dd36":"---\n**NOTE**\n\nSince the distribution is skewed, it is recommended to replace null values with median instead of mean since large values on one end greatly impact the mean, as opposed to the median, which is less impacted.\n\n---","db2fcd3e":"## 8. Evaluating model performances","71544d75":"## Validation data set\nSince we plan to run different models on our dataset, it is advisable to further split the training data into training and validation sets. Moreover a validation set would ensure that the models don't overfit the data. We can use sklearn's train_test_split function to create a validation set. We would use the validation set to test the accuracy of our models and to predict the best performing model.","7e7c1ae0":"Since the distribution of this feature is skewed towards the right, we will replace null values with the median of the feature for most accuracy\n","33936f61":"To evaluate the performance of our model, we will use the make_scorer and accuracy_score functions from sklearn.metrics","c3a4addf":"## 5. Feature Engineering \nBecause Sex and Embarked features contain categorical data, these features are needed to be converted to numerical values based on their categorical values. We will use One-Hot Encoding for this transformation","6185eb89":"---\n**NOTE**\n\nThese statistics only represent the survivors whereas the graph represents all passengers of the ship including the dead\n\n---","d5e6d1ff":"Defining features in training\/test set","65137c87":"## 7. Modelling, Optimization and Prediction","b617b731":"We can combine SibSp and Parch into one synthetic feature called FamSize, which indicates the number of family members on board for each passenger","dcf0fdae":"## 1. Importing libraries and packages\nWe'll import these packages to manipulate the data and visualise the features as well as measure the performance of our models","88d418b2":"We can also use GridSearch coss validation to find the optimal parameters for the model we choose to work with and use to predict on our testing set","4a7759fd":"## 3. Imputation of data\nIn this part, we focus on removing NaN data from the test and training data","75bc4971":"### Linear SVC Model ","20de37cf":"Age","968bc173":"If we look at Age and Fare features, we see their values are significantly larger in comparison to those of other features. This may cause problems during modelling, since it may make these features seem more important than others. It would be beneficial to scale these features so that they are more representative. We may use one of many scaling libraries provided by scikitlearn, I will be using StandardScalar","d8cb7e1f":"We're left with just one step before we proceed with modelling our data. We see Age and Fare values are too large as compared to the rest of our data, which might affect our models' performance. We intend to use StandardScalar library to standardise the training and test dataframes","5ff66463":"It appears that younger passengers are more likely to survive as seen in the strip-plot and histogram, even though their likelihood of survival is not substantially larger than that of older passengers","81a7135b":"## 4. Data Visualisation\nOne of the most important steps of the EDA process, data visualisation helps us in understanding data and deriving insights by plotting various graphs. We're able to assess trends and general association of features.  <TODO Something about- entropy and information gain>"}}