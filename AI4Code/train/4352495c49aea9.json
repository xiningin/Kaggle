{"cell_type":{"92998e13":"code","c1a4fe63":"code","d91382af":"code","2f8d563b":"code","0121260e":"code","c6b68a02":"code","379b0b89":"code","7d3e8b88":"code","f1b7171e":"code","c26cd6dd":"code","861baa83":"code","76d06add":"code","2dd0d52f":"code","ccb1acee":"code","0ca0893d":"code","30bcc4d8":"code","2b1fca32":"code","6a094b08":"code","393a88fd":"code","7750a569":"code","c409ca50":"code","ef477478":"code","824b5354":"code","6c0682ea":"code","0fd511fa":"code","0e3ebdc7":"code","f0bdde33":"code","20140365":"code","e623e120":"code","bb7e90f2":"code","fa887a33":"code","cff32d30":"code","d45be60a":"code","2094aa7b":"code","3a09d4cf":"code","54db3eac":"code","cfd83733":"code","0afc5ea6":"code","48ba2172":"code","11aa6c10":"code","68419c30":"code","e23871b8":"code","8a66208f":"code","ce093e33":"code","1a2b8379":"code","679182f8":"code","8715e782":"code","b7036ca1":"code","5e1f99e2":"code","22fe07e2":"code","54047cb1":"code","eff0c363":"code","91308bc5":"code","69cccc45":"code","9414a303":"code","ffa9362e":"code","0be29b74":"code","aa3dff3e":"code","3654d02c":"code","7cbe4a4f":"code","c28e49f7":"code","f2c3d9b2":"code","ea379ae7":"code","28b2e7b0":"code","401f9e6b":"code","749273c0":"code","75c1308e":"code","ac5abda5":"code","03a15e69":"code","0d3fc39c":"code","d2454d1a":"code","39373d52":"code","85e5fdea":"code","5003def7":"code","4e3093d1":"code","ecd3e89f":"code","9ac59ce8":"code","cc08b65b":"code","81c7c0f7":"code","6cb61a4a":"code","8b4e945d":"code","19fee5dc":"code","e06491bd":"code","2bb450b4":"code","92fc5c00":"code","595cb9b1":"code","646257c6":"code","91cd17c8":"code","1817b847":"code","c5f29d9a":"code","c98e11fa":"code","d7636e62":"code","d778d968":"code","034d19ce":"code","2291fded":"code","017f280d":"code","696e19b2":"code","ef85f45b":"code","db0d93e8":"markdown","99fff92b":"markdown","9a12c4c3":"markdown","b695508f":"markdown","4cacaacf":"markdown","9538b33c":"markdown","3ada84b4":"markdown","1ab4008e":"markdown","2ab94216":"markdown","39d6b022":"markdown","2d46471f":"markdown","25496916":"markdown","bd5789d0":"markdown","11f16494":"markdown","22008fe0":"markdown","93246565":"markdown","23e21982":"markdown","79623374":"markdown","49ef316b":"markdown","6b6a9def":"markdown","78501197":"markdown","30f42a94":"markdown","6cbc482d":"markdown","af374385":"markdown","ac602e24":"markdown","f82e875d":"markdown","32c16da6":"markdown","89f4ef37":"markdown","aa88ddbd":"markdown","daf22cf9":"markdown","621a916b":"markdown","ea224c48":"markdown","d1dae813":"markdown","29567530":"markdown","250b8bf8":"markdown","006ee454":"markdown","3f92a377":"markdown","559dfee5":"markdown","412f8d1b":"markdown","ba4ff451":"markdown","d210e0ae":"markdown","f3dd728d":"markdown","a5cc86af":"markdown","5c7be006":"markdown","893c8bf5":"markdown","30071203":"markdown","c90707ac":"markdown","a8604aa0":"markdown","203f6e9d":"markdown","5225d12f":"markdown","7ec010d5":"markdown","c8004e75":"markdown","fff7621a":"markdown","c9962b4a":"markdown","2445948a":"markdown","bf4cfd45":"markdown","9df91f21":"markdown","d0b7d495":"markdown","1c07e41f":"markdown","3982c462":"markdown","375544ca":"markdown","f97279d8":"markdown","600c4a0e":"markdown","c3b34363":"markdown","90467840":"markdown","2f4e907a":"markdown","96c8c7e4":"markdown","094d81af":"markdown","6bae155e":"markdown","3b7de62d":"markdown","d102ffa3":"markdown","c832cc8e":"markdown","c5d5ed3e":"markdown","8cd4bfa3":"markdown","d10a6055":"markdown","7f4fd057":"markdown"},"source":{"92998e13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1a4fe63":"#General\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport shap \n#Preprocessing\nfrom itertools import cycle\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom scipy import stats, interp\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_predict\nfrom sklearn.feature_selection import RFECV, RFE\nfrom imblearn.under_sampling import RandomUnderSampler \nfrom imblearn.over_sampling import SMOTE\n#Models\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, label_binarize\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier, plot_importance\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.cluster import KMeans \n#Model_Evaluation\nimport optuna\nfrom sklearn.metrics import roc_curve, auc, make_scorer, accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import f1_score, plot_confusion_matrix, classification_report, average_precision_score\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve\n#Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\n#Setting_Parameters\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set()\nstyle.use('ggplot')\npd.set_option(\"display.max_rows\", 100)\n%matplotlib inline \nplt.rcParams[\"figure.figsize\"] = (12, 8)\ncmap = LinearSegmentedColormap.from_list(\"\", ['#FFFFFF',\"#FFF5BD\",\"#FF4646\",'#E41A1C',\"#960018\"])","d91382af":"!pip install xlrd\n!pip install openpyxl\n","2f8d563b":"def reduce_mem_usage(train_data):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    start_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n\n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n\n    end_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return train_data\n","0121260e":"%%time\ndf = pd.read_excel('\/kaggle\/input\/mobilechurndataxlsx\/mobile-churn-data.xlsx')\ndf.head()","c6b68a02":"print('Feature names: \\n',df.columns)","379b0b89":"df.drop(['year', 'user_account_id'], axis=1, inplace=True)","7d3e8b88":"df.describe().transpose()","f1b7171e":"reduce_mem_usage(df)","c26cd6dd":"null = df.isnull().sum().sum()\nprint('-'*40)\nprint('Null values check')\nprint('-'*40)\nprint('There is {} null values in the Data Frame'.format(null))","861baa83":"def correlation_heatmap(df, fig=20, annot=False, filtering=None,sort=True,name='Title', title='Pearson Correlation of Features'):\n    \"\"\"\n    Create a correlation matrix with respect to the most correlated variables, and the ability \n    to filter the correlation matrix by pourcentage between 0-1\"\"\"\n    if sort == True:\n        cols1=[]\n        cols2=[]\n        value=[]\n        matrix = df.corr()\n        for i in range(len(matrix.columns)):\n            for j in range(i):\n                cols1.append(matrix.columns[i])\n                cols2.append(matrix.columns[j])\n                value.append(matrix.iloc[i,j])\n        new_df = pd.DataFrame({'column_name_1': cols1, 'column_name_2': cols2, 'corr':value})\n        new_df.sort_values('corr',ascending=False, inplace=True)\n    \n        if filtering != None:\n            new_df = new_df[(abs(new_df['corr']) >= filtering)]\n    \n        all_cols = []\n        cols = []\n        for index, row in new_df.iterrows():\n            all_cols.append(row.column_name_1)\n            all_cols.append(row.column_name_2)\n        for col in all_cols:\n            if col not in cols:\n                cols.append(col)\n        final = df[cols]\n    else:\n        final = df.copy()\n    _ , ax = plt.subplots(figsize =(fig, fig))\n    colormap = sns.diverging_palette(240, 10,n=9, as_cmap = True)\n    \n    _ = sns.heatmap(\n        final.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=annot, \n        linewidths=0.1,\n        vmin=-1, \n        vmax=1.0, \n        linecolor='white',\n        annot_kws={'fontsize':8 }\n    )\n    \n    plt.title(title, y=1.05, size=15)\n    if name == 'title':\n        name = str(tittle)\n    plt.savefig(name+\".png\",transparent=True)\n    ","76d06add":"correlation_heatmap(df, sort=True, title='Preason Correlation Matrix', name='corr_all_feat')","2dd0d52f":"correlation_heatmap(df, fig=15,sort=True, name='corr_filtrd_feat', annot=True, filtering=0.85, title='Pearson Correlation Matrix')","ccb1acee":"cols1=[]\ncols2=[]\nvalue=[]\nmatrix = df.corr()\nfor i in range(len(matrix.columns)):\n    for j in range(i):\n        cols1.append(matrix.columns[i])\n        cols2.append(matrix.columns[j])\n        value.append(matrix.iloc[i,j])\nnew_df = pd.DataFrame({'Feature Name 1': cols1, 'Feature Name 2': cols2, 'Correlation':value})\nnew_df.sort_values('Correlation',ascending=False, inplace=True)\nnew_df.reset_index(drop=True, inplace=True)\nnew_df = new_df[new_df['Correlation']>0.85]\nnew_df['Feature Name 1'].unique()","0ca0893d":"cols1=[]\nmatrix = df.corr()\nfor i in range(len(matrix.columns)):\n    for j in range(i):\n        if abs(matrix.iloc[i,j]) >= 0.85:\n            cols1.append(matrix.columns[i])\ndropped = df.drop(cols1, axis=1)\ncorrelation_heatmap(dropped)","30bcc4d8":"dropped.shape","2b1fca32":"from matplotlib import cm, colors\ncmap = cm.get_cmap('Set1', 10)    # PiYG\n\nfor i in range(cmap.N):\n    rgba = cmap(i)\n    # rgb2hex accepts rgb or rgba\n    print(colors.rgb2hex(rgba))\n","6a094b08":"dropped.churn = dropped.churn.map({0:'No', 1:'Yes'})\ndropped.head()","393a88fd":"f, axes = plt.subplots(1,2,figsize=(14,5))\nsns.distplot(dropped.user_no_outgoing_activity_in_days, color=\"#dcae52\", ax=axes[0])\naxes[0].set_title(\"'user_no_outgoing_activity_in_days' distribution\")\naxes[1] = sns.distplot(dropped[dropped.user_no_outgoing_activity_in_days < 365]['user_no_outgoing_activity_in_days'], color=\"#dcae52\")\naxes[1].set_title(\"'user_no_outgoing_activity_in_days' new distribution\")\nplt.show()\nf.savefig(\"user_no_outgoing_activity_in_days_DIST.png\")\ndropping = dropped[dropped.user_no_outgoing_activity_in_days > 365].shape[0]\nprint('There were {} records dropped'.format(dropping))\n#dropped = dropped[dropped.user_no_outgoing_activity_in_days < 365]","7750a569":"f, ax = plt.subplots(1,figsize=(7,5))\nax = sns.distplot(dropped.user_spendings, color=\"#e41a1c\")\nax.set( xlim=(-50,800),title='user_spendings')\nf.savefig(\"user_spendings_DIST_2.png\")","c409ca50":"f, ax = plt.subplots(1,figsize=(7,5))\nax = sns.distplot(dropped.user_lifetime, color=\"#e41a1c\")\nax.set(title='user_lifetime')\nf.savefig(\"user_lifetime_DIST.png\")\nplt.show()","ef477478":"pourc = pd.DataFrame(dropped['user_intake']).value_counts()\/len(dropped['user_intake'])*100\npourc = pd.DataFrame(pourc, columns=['pourcentage']).round().reset_index()\n#pourc.churn = pourc.churn.map({0:'No', 1:'Yes'})\nbar,ax = plt.subplots(1, figsize=(7,5))\nax = sns.barplot(data = pourc, x='user_intake', y='pourcentage', ci=None ,orient='v' )\nax.set(ylim=(0, 105))\nax.set_title(\"user intake\", fontsize=15)\nax.set_xlabel (\"user_intake\",fontsize=13)\nax.set_ylabel (\"Percentage %\",fontsize=13)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2-0.09,rect.get_height()+0.9,\"%.1f%%\"% rect.get_height(), weight='bold',fontsize=12 )\nbar.savefig('user_intake.png',transparent=True)","824b5354":"ax = sns.lmplot(x='user_spendings' , y='calls_outgoing_spendings', data=dropped , hue='churn', palette='Set1', height=4, aspect=1.5,legend_out=False,scatter_kws={\"s\": 50, \"alpha\":.3} )\nax.set(xlim=(-50, 800), ylim=(-50,600), title='user_spendings & calls_outgoing_spendings')\nplt.show()\nax.savefig(\"user_spendings_VS_calls_outgoing_spendings.png\",transparent=True)","6c0682ea":"spending = dropped[['user_spendings', 'calls_outgoing_spendings', 'sms_outgoing_spendings','gprs_spendings', 'churn']]\nsns.pairplot(spending, hue='churn', size=2.5)","0fd511fa":"ax = sns.lmplot(x='user_account_balance_last' , y='reloads_sum', data=dropped , hue='churn', palette='Set1', height=4, aspect=1.5,legend_out=False,scatter_kws={\"s\": 50, \"alpha\":.3} )\nax.set(xlim=(-50, 1400), ylim=(-50,1600), title='user_account_balance_last & reloads_sum')\nplt.show()\nax.savefig(\"user_account_balance_last_VS_reloads_sum.png\")","0e3ebdc7":"f, ax = plt.subplots()\nsns.boxplot(dropped.churn,dropped.user_no_outgoing_activity_in_days, ax=ax)\nax.set(ylim=(-5,80), title='User last balance vs churn')\nplt.show()","f0bdde33":"cat = dropped[['user_has_outgoing_calls', 'user_has_outgoing_sms', 'user_use_gprs', 'user_does_reload', 'churn']]\n#maping the columns\ncat.user_has_outgoing_calls = cat.user_has_outgoing_calls.map({0:'No', 1:'Yes'})\ncat.user_has_outgoing_sms = cat.user_has_outgoing_sms.map({0:'No', 1:'Yes'})\ncat.user_use_gprs = cat.user_use_gprs.map({0:'No', 1:'Yes'})\ncat.user_does_reload = cat.user_does_reload.map({0:'No', 1:'Yes'})\ncat['indx'] = cat.index\n#Pivot_tables\nuser_has_outgoing_calls = cat.pivot_table('indx',index=['user_has_outgoing_calls'],columns='churn',aggfunc='count')\nuser_has_outgoing_sms  = cat.pivot_table('indx',index=['user_has_outgoing_sms'],columns='churn',aggfunc='count')\nuser_use_gprs = cat.pivot_table('indx',index=['user_use_gprs'],columns='churn',aggfunc='count')\nuser_does_reload = cat.pivot_table('indx',index=['user_does_reload'],columns='churn',aggfunc='count')","20140365":"f, axes = plt.subplots(1,4,figsize=(12,3),sharey=True)\nf.subplots_adjust(wspace=0)\nf.suptitle('Diffirent customer services')\ntables = [user_has_outgoing_calls, user_has_outgoing_sms, user_use_gprs, user_does_reload]\ni = 0\nfor table in tables:\n    table.plot(kind='bar', ax=axes[i], stacked=True)\n    axes[i].tick_params(axis='y', labelsize=7)\n    axes[i].set(ylim=(0,70000))\n    if i < 3 :\n        axes[i].get_legend().remove()\n    i+=1\nplt.show()\nf.savefig(\"Diffirent customer services.png\")","e623e120":"dropped.churn = dropped.churn.map({'No':0, 'Yes':1})\ndropped.head()","bb7e90f2":"dropped.churn.value_counts()","fa887a33":"%%time\nrfm = dropped[['reloads_inactive_days','reloads_count','reloads_sum']]\nrfm.columns = ['recency', 'frequency', 'monetary']\nrfm.iloc[200:206].to_excel('RFM.xlsx')\nrfm.iloc[200:206]\n","cff32d30":"scaler = StandardScaler()\nrfm_scaled = scaler.fit_transform(rfm)\nrfm = pd.DataFrame(rfm_scaled, columns=rfm.columns)\nrfm.iloc[200:206].to_excel('Scaled_RFM.xlsx')\nrfm.iloc[200:206]","d45be60a":"%%time\nrfm_segmentation = rfm.copy()\nNc = range(1, 10)\nkmeans = [KMeans(n_clusters=i) for i in Nc]\nscore = [kmeans[i].fit(rfm_segmentation).score(rfm_segmentation) for i in range(len(kmeans))]\nf, ax = plt.subplots(1, figsize=(7,5))\nax.plot(Nc,score)\nax.set_title('Elbow Curve')\nax.set_xlabel('Number of clusters')\nax.set_ylabel('Score')\nf.savefig(\"Elbow_RFM.png\")\nplt.show()","2094aa7b":"kmeans = KMeans(n_clusters=3, random_state=0).fit(rfm_segmentation)","3a09d4cf":"rfm_segmentation['cluster'] = kmeans.labels_\ndropped['cluster'] = kmeans.labels_\nnew = dropped.copy()\nrfm_segmentation.reset_index()\nnew.head()","54db3eac":"\n\nrfm_segmentation_plot = rfm_segmentation[(np.abs(stats.zscore(rfm_segmentation)) < 3).all(axis=1)]\n\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18,4))\n#f.suptitle('Rcency, Frequency & Monetary', fontsize=14)\n\nsns.boxplot(rfm_segmentation_plot.cluster, rfm_segmentation_plot.recency,palette=\"Set1\", ax=ax1)\nax1.set_title('Recency', fontsize=12)\n\nsns.boxplot(rfm_segmentation_plot.cluster,rfm_segmentation_plot.frequency,palette=\"Set1\", ax=ax2)\nax2.set_title('Frequency', fontsize=12)\n\nsns.boxplot(rfm_segmentation_plot.cluster,rfm_segmentation_plot.monetary,palette=\"Set1\", ax=ax3)\nax3.set_title('Monetary', fontsize=12)\n\nf.savefig(\"Box_RFM.png\")\nplt.show()","cfd83733":"f, (ax1, ax2) = plt.subplots(1,2,figsize=(20,8))\nf.suptitle('Visualizing Clusters', fontsize=14)\n\nsns.scatterplot(rfm_segmentation_plot.frequency ,rfm_segmentation_plot.recency,hue = rfm_segmentation_plot.cluster,palette=\"Set1\",s=100, ax=ax1)\nax1.set_title('Recency x Frequency', fontsize=14)\n\nsns.scatterplot(rfm_segmentation_plot.frequency ,rfm_segmentation_plot.monetary,hue = rfm_segmentation_plot.cluster,palette=\"Set1\",s=100, ax=ax2)\nax2.set_title('Frequency x Monetary', fontsize=14)\n\nplt.show()","0afc5ea6":"new_group = new.copy()\nnew_group.churn = new.churn.map({0:'No', 1:'Yes'})\nnew_group.reset_index(inplace=True)\ngroup = new_group.pivot_table('index',index='cluster',columns='churn',aggfunc='count')\ngroup = group.div(group.sum(axis=1), axis=0).round(2)*100\ndisplay(group)\nf ,ax = plt.subplots(1, figsize=(7,5))\ngroup.plot(kind='bar',ax=ax)\nax.set(ylim=(0, 110))\nax.set_title('Churn percentage',fontsize=14)\nax.set_ylabel('Percentage %',fontsize=12)\nax.set_xlabel('Cluster',fontsize=12)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2-0.09,rect.get_height()+0.9,\"%.1f%%\"% rect.get_height(), weight='bold',fontsize=10 )\nf.savefig(\"Churn_RFM.png\")","48ba2172":"new_group.drop('index', axis=1, inplace=True)","11aa6c10":"print('There were {} records deleted !'.format(new[new['cluster'] == 1 ].shape[0]))\nnew = new[new['cluster'] != 1]\nnew.churn = new.churn.map({0:'No', 1:'Yes'})","68419c30":"new_churn = new.churn\nnew_X = new[new.columns.difference(['churn'])]\ncols = new_X.columns\nscaler = StandardScaler()\nnew_X = scaler.fit_transform(new_X)\nnew_X = pd.DataFrame(new_X, columns=cols)\nnew_X.head()","e23871b8":"new_X.sample(5, axis=1).sample(5, axis=0).to_excel('Standardize_X.xlsx')","8a66208f":"new_churn.head()","ce093e33":"X_train, X_test, y_train, y_test = train_test_split( new_X, new_churn, test_size=0.2, random_state=23, stratify=new_churn)\nname = []\nshape = []\ndata = {'X_train':X_train, 'X_test':X_test, 'y_train':y_train, 'y_test':y_test}\nfor key, value in data.items():\n    name.append(key)\n    shape.append(len(value))\nsize = pd.DataFrame({'Data':name, 'Size':shape})\nsize.to_excel('Size.xlsx')\nsize","1a2b8379":"pourc = pd.DataFrame(y_train).value_counts()\/len(y_train)*100\npourc = pd.DataFrame(pourc, columns=['pourcentage']).round().reset_index()\n#pourc.churn = pourc.churn.map({0:'No', 1:'Yes'})\nf,ax = plt.subplots(1, figsize=(7,5))\nax = sns.barplot(data = pourc, x='churn', y='pourcentage', ci=None ,orient='v' )\nax.set(ylim=(0, 100))\nax.set_title(\"Churn Distribution\", fontsize=15)\nax.set_xlabel (\"Churn\",fontsize=13)\nax.set_ylabel (\"Percentage %\",fontsize=13)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2-0.09,rect.get_height()+0.9,\"%.1f%%\"% rect.get_height(), weight='bold',fontsize=15 )\nf.savefig(\"Churn_imbalance.png\")","679182f8":"smote = SMOTE(sampling_strategy=0.6, random_state=23)\nX_sm, y_sm = smote.fit_resample(X_train,y_train)\ny_sm.shape\n","8715e782":"under = RandomUnderSampler(random_state=23)\n\nX, y = under.fit_resample(X_sm, y_sm)","b7036ca1":"print('y_train = {}, y_sm = {}, y = {} '.format(y_train.shape[0],y_sm.shape[0],y.shape[0]))","5e1f99e2":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18,4))\n#f.suptitle('Rcency, Frequency and Monetary Box-Plots', fontsize=18)\n\npourc = new[['churn']].value_counts()\/len(new)*100\npourc = pd.DataFrame(pourc, columns=['pourcentage']).round().reset_index()\n#pourc.churn = pourc.churn.map({0:'No', 1:'Yes'})\nsns.barplot(data = pourc, x='churn', y='pourcentage', ci=None ,orient='v' ,ax=ax1)\nax1.set(ylim=(0, 100))\nax1.set_title(\"Before Resampling\", fontsize=12)\nax1.set_xlabel (\"Churn\",fontsize=10)\nax1.set_ylabel (\"Percentage %\",fontsize=10)\nfor rect in ax1.patches:\n    ax1.text (rect.get_x() + rect.get_width() \/ 2-0.09,rect.get_height()+0.9,\"%.1f%%\"% rect.get_height(), weight='bold',fontsize=10 )\n\npourc2 = y_sm.value_counts()\/len(y_sm)*100\npourc2 = pd.DataFrame(pourc2).round().reset_index()\n#pourc2['index'] = pourc2['index'].map({0:'No', 1:'Yes'})\nsns.barplot(data = pourc2, y='churn', x='index', ci=None ,orient='v' ,ax=ax2)\nax2.set(ylim=(0, 100))\nax2.set_title(\"After SMOTE Oversampling\", fontsize=12)\nax2.set_xlabel (\"Churn\",fontsize=10)\nax2.set_ylabel (\"Percentage %\",fontsize=10)\nfor rect in ax2.patches:\n    ax2.text (rect.get_x() + rect.get_width() \/ 2-0.09,rect.get_height()+0.9,\"%.1f%%\"% rect.get_height(), weight='bold',fontsize=10 )\n\npourc3 = y.value_counts()\/len(y)*100\npourc3 = pd.DataFrame(pourc3).round().reset_index()\n#pourc3['index'] = pourc3['index'].map({0:'No', 1:'Yes'})\nsns.barplot(data = pourc3, y='churn', x='index', ci=None ,orient='v',ax=ax3 )\nax3.set(ylim=(0, 100))\nax3.set_title(\"After Random Undersampling\", fontsize=12)\nax3.set_xticklabels(['No','Yes'])\nax3.set_xlabel (\"Churn\",fontsize=10)\nax3.set_ylabel (\"Percentage %\",fontsize=10)\nfor rect in ax3.patches:\n    ax3.text (rect.get_x() + rect.get_width() \/ 2-0.09,rect.get_height()+0.9,\"%.1f%%\"% rect.get_height(), weight='bold',fontsize=10 )\n\nf.savefig(\"Resampling_imbalance.png\")    \nplt.show()","22fe07e2":"y = y.map({'No':0, 'Yes':1})\ny_test = y_test.map({'No':0, 'Yes':1})","54047cb1":"%%time\n\ncv = KFold(n_splits=5, shuffle=True, random_state=23)\nclf = RandomForestClassifier(n_jobs=-1) \nrfecv = RFECV(estimator=clf, step=1, cv=cv,scoring='recall', n_jobs=-1)   #5-fold cross-validation\nrfecv = rfecv.fit(X, y)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', X.columns[rfecv.support_])","eff0c363":"%%time\n# Create the RFE object and rank each pixel\nclf_rf_3 = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf_3, n_features_to_select=37, step=1)\nrfe = rfe.fit(X, y)\nprint('Chosen best 42 feature by rfe:',X.columns[rfe.support_])","91308bc5":"keep_cols = ['calls_outgoing_count', 'calls_outgoing_duration',\n       'calls_outgoing_duration_max', 'calls_outgoing_inactive_days',\n       'calls_outgoing_spendings', 'calls_outgoing_spendings_max',\n       'calls_outgoing_to_abroad_count', 'calls_outgoing_to_abroad_duration',\n       'calls_outgoing_to_abroad_spendings', 'calls_outgoing_to_offnet_count',\n       'calls_outgoing_to_offnet_spendings', 'gprs_inactive_days',\n       'last_100_calls_outgoing_to_abroad_duration',\n       'last_100_calls_outgoing_to_onnet_duration', 'last_100_gprs_usage',\n       'last_100_reloads_count', 'last_100_reloads_sum',\n       'last_100_sms_outgoing_to_abroad_count', 'month', 'reloads_count',\n       'reloads_inactive_days', 'reloads_sum', 'sms_incoming_count',\n       'sms_incoming_from_abroad_count', 'sms_outgoing_count',\n       'sms_outgoing_inactive_days', 'sms_outgoing_spendings',\n       'sms_outgoing_spendings_max', 'sms_outgoing_to_abroad_count',\n       'sms_outgoing_to_abroad_spendings', 'sms_outgoing_to_onnet_count',\n       'user_account_balance_last', 'user_has_outgoing_sms', 'user_intake',\n       'user_lifetime', 'user_no_outgoing_activity_in_days', 'user_spendings']\nlen(keep_cols)","69cccc45":"X = X[keep_cols]\nX_test = X_test[keep_cols]","9414a303":"%%time\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    #gaussian_process.GaussianProcessClassifier(copy_X_train=False),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(solver='liblinear'),\n    #linear_model.PassiveAggressiveClassifier(),\n    #linear_model.RidgeClassifierCV(),\n    #linear_model.SGDClassifier(),\n    #linear_model.Perceptron(),\n    \n    #Navies Bayes\n    #naive_bayes.BernoulliNB(),\n    #naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    #neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    #svm.SVC(probability=True),\n    #svm.NuSVC(probability=True),\n    #svm.LinearSVC(),\n    \n    #Trees    \n    #tree.DecisionTreeClassifier(),\n    tree.DecisionTreeClassifier(),\n    \n    #Discriminant Analysis\n    #discriminant_analysis.LinearDiscriminantAnalysis(),\n    #discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier(),\n    \n    #LightGBM\n    LGBMClassifier(n_jobs=-1),\n    ]\n\n\n#split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['Name','Accuracy','Recall', 'Precision', 'F1_score']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\n#MLA_predict = y.copy()\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'Name'] = MLA_name\n    #MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    #scoring = ['accuracy','precision', 'recall', 'f1_weighted']\n    scoring = {'accuracy' : make_scorer(accuracy_score), \n           'precision' : make_scorer(precision_score),\n           'recall' : make_scorer(recall_score), \n           'f1_score' : make_scorer(f1_score)}\n    \n    cv_results = model_selection.cross_validate(alg, X, y,cv  = cv_split, scoring=scoring, n_jobs=-1)\n    MLA_compare.loc[row_index, 'Fit Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'Accuracy'] = cv_results['test_accuracy'].mean()\n    #print(cv_results['test_accuracy'].mean())\n    MLA_compare.loc[row_index, 'Recall'] = cv_results['test_recall'].mean()\n    MLA_compare.loc[row_index, 'Recall_STD'] = cv_results['test_recall'].std()\n    MLA_compare.loc[row_index, 'Precision'] = cv_results['test_precision'].mean()\n    MLA_compare.loc[row_index, 'F1_score'] = cv_results['test_f1_score'].mean()\n    #MLA_compare.loc[row_index, 'Parameters'] = str(alg.get_params())\n    #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    #MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_accuracy'].std()*3   #let's know the worst that can happen!\n    print(cv_results['fit_time'].mean(), '==>  ',MLA_name, 'is done!')\n\n    \n\n    #save MLA predictions - see section 6 for usage\n    #alg.fit(X, y)\n    #y_predict = alg.predict(X_test)\n    #print(classification_report(y_test, y_predict))\n    #print('-'*25)\n    \n    row_index+=1\n    \n#print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['Recall'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict","ffa9362e":"MLA_compare.sort_values(by = ['F1_score'], ascending = False, inplace = True)\nMLA_compare.to_excel('MLA_compare.xlsx')\nMLA_compare","0be29b74":"%%time\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10,9))\ncmap = LinearSegmentedColormap.from_list(\"\", ['#FFFFFF',\"#FFF5BD\",\"#FF4646\",'#E41A1C',\"#960018\"])\n\nmodels = {ax1: ensemble.ExtraTreesClassifier(),\n          ax2: ensemble.RandomForestClassifier(),\n          ax3 : LGBMClassifier(n_jobs=-1),\n          ax4 : XGBClassifier(n_jobs=-1, verbosity=0)\n         }\nfor ax, model in models.items():\n    ax.grid(False)\n    mod = model.fit(X,y)\n    #y_pred = pd.Series(mod.predict(X_test)).map({0:'No', 1:'Yes'}).to_numpy()\n    plot_confusion_matrix(mod, \n                          X_test, \n                          y_test, \n                          ax=ax, \n                          cmap=cmap,\n                         display_labels=['No', 'Yes'],\n                         normalize='true')\n    ax.set_title(str(model.__class__.__name__), fontsize=12)\nf.savefig(\"ML_Confusion_matrices.png\")     \nplt.show()\n","aa3dff3e":"%%time\n#import cudf\n\n# Created the Xgboost specific DMatrix data format from the numpy array to optimise memory consumption\ndtrain = xgb.DMatrix(X, label=y)\ndvalid = xgb.DMatrix(X_test, label=y_test)\n\ndef objective(trial):\n    \n# params specifies the XGBoost hyperparameters to be tuned\n    param = {\n        #\"silent\": 1,\n        \"objective\": \"binary:logistic\",\n        \"eval_metric\": \"auc\",\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\",\n                                                         #\"gblinear\",\n                                                         #\"dart\"\n                                                        ]),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 1.0),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 1.0),\n    }\n\n    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n        param[\"eta\"] = trial.suggest_loguniform(\"eta\", 1e-8, 1.0)\n        param[\"gamma\"] = trial.suggest_loguniform(\"gamma\", 1e-8, 1.0)\n        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n    #if param[\"booster\"] == \"dart\":\n        #param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n        #param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n        #param[\"rate_drop\"] = trial.suggest_loguniform(\"rate_drop\", 1e-8, 1.0)\n        #param[\"skip_drop\"] = trial.suggest_loguniform(\"skip_drop\", 1e-8, 1.0)\n    \n    bst = xgb.train(param, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n# trials will be evaluated based on their accuracy on the test set\n    recall = sklearn.metrics.recall_score(y_test, pred_labels)\n    return recall\n\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective,n_trials=500) \n#You can increase n_trials parameter\n\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","3654d02c":"best = {'booster': 'gbtree', 'lambda': 3.028014963599024e-07, 'alpha': 6.505761650849223e-07, 'max_depth': 4, 'eta': 0.036103094695571915, 'gamma': 0.01039365714088514, 'grow_policy': 'lossguide'}","7cbe4a4f":"%time \n#best_params['objective'] = 'binary:logistic'\n# Fit the XGBoost classifier with optimal hyperparameters\nXGB = XGBClassifier(**best,\n                    verbosity = 0,\n                    n_jobs=-1)\nXGB.fit(X, y)  #Used the whole training data","c28e49f7":"y_predict = XGB.predict(X_test)\ny_true = pd.Series(y_test).map({0:'No', 1:'Yes'}).to_numpy()\ny_pred = pd.Series(y_predict).map({0:'No', 1:'Yes'}).to_numpy()\nprint(classification_report(y_true, y_pred))\n","f2c3d9b2":"%%time\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n#f.suptitle('Hyperparamter results comparison', fontsize=18)\n\nmodels = {ax1: XGBClassifier(verbosity = 0, n_jobs=-1),\n          ax2: XGB\n         }\nfor ax, model in models.items():\n    ax.grid(False)\n    mod = model.fit(X,y)\n    #y_pred = pd.Series(mod.predict(X_test)).map({0:'No', 1:'Yes'}).to_numpy()\n    plot_confusion_matrix(mod, \n                          X_test, \n                          y_test, \n                          ax=ax, \n                          cmap=cmap,\n                         display_labels=['No', 'Yes'],\n                         normalize='true')\n    ax1.set_title('XGBClassifier before Hyperparameter', fontsize=12)\n    ax2.set_title('XGBClassifier after Hyperparameter', fontsize=12)\n\nf.savefig(\"Hyperparameter.png\")\nplt.show()\n","ea379ae7":"y_prob = XGB.predict_proba(X_test)\n    \nfpr_t, tpr_t, t_t = roc_curve(y_test, y_prob[:, 1])\n\naverage_precision = average_precision_score(y_test, y_prob[:, 1])\nprecision, recall, _ = precision_recall_curve(y_test, y_prob[:, 1])\nroc_auc = auc(fpr_t, tpr_t)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\n\nf, (ax1, ax2) = plt.subplots(1,2,figsize=(12,5))\n#f.suptitle('ROC and Precision-Recall Curves', fontsize=15)\n\n#ROC\nax1.plot([0, 1], [0, 1], linestyle = '--', lw = 2,label='Baseline', color = 'grey', alpha= 1)\nax1.step(fpr_t, tpr_t, lw=2, alpha=0.8,where='post', label='Test ROC ', color = '#E60F30')\nax1.fill_between(fpr_t, tpr_t, step='post', alpha=0.2,color='#E60F30', label='AUC = %0.2f' % (roc_auc))\nax1.set_xlim([-0.01, 1.01])\nax1.set_ylim([-0.01, 1.01])\nax1.set_ylabel('True Positive Rate')\nax1.set_xlabel('False Positive Rate')\nax1.legend(loc=\"lower right\",prop={'size': 8})\nax1.set_title('Receiver operating characteristic (ROC) curve', fontsize=12)\n\n#Precision-Recall\nax2.plot([0, 1], [no_skill, no_skill], linestyle='--', label='Baseline', alpha=1, color = 'grey')\nax2.step(recall, precision, color='#243BBD', alpha=0.8,where='post', label='XGBoost')\nax2.fill_between(recall, precision, step='post', alpha=0.2,color='#243BBD', label='Average Precision-Recall Score ={0:0.2f}'.format(average_precision))\nax2.set_ylabel('Precision')\nax2.set_xlabel('Recall')\nax2.set_xlim([-0.01, 1.01])\nax2.set_ylim([-0.01, 1.01])\nax2.set_title('Precision-Recall curve:', fontsize=12)\nax2.legend(loc=\"lower right\",prop={'size': 8})\n\nf.savefig(\"Roc_Recall-Precision.png\")    \nplt.show()","28b2e7b0":"XGB = XGBClassifier(**best).fit(X,y)\nplot_importance(XGB,max_num_features=10)\nplt.show()","401f9e6b":"explainer = shap.TreeExplainer(XGB)\nshap_values = explainer(X)","749273c0":"shap.summary_plot(shap_values, X, plot_type=\"bar\", max_display=10)\nplt.savefig(\"SHAP_bar.png\")","75c1308e":"\n#shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n# visualize the first prediction's explanation\nshap.plots.waterfall(shap_values[100])","ac5abda5":"shap.initjs()\nshap.plots.force(shap_values[1420])\n","03a15e69":"shap.plots.force(shap_values[6311])","0d3fc39c":"shap.plots.beeswarm(shap_values)\nplt.savefig(\"SHAP_summary.png\")","d2454d1a":"XGB.predict(X_test.iloc[100:101,:])","39373d52":"shap.plots.scatter(shap_values[:,\"user_no_outgoing_activity_in_days\"], color=shap_values)","85e5fdea":"\nshap.summary_plot(shap_values, X)\n","5003def7":"cmap = LinearSegmentedColormap.from_list(\"\",['#FFFFFF', '#ffdd6b', '#dcae52', '#af7132'])\ncmap","4e3093d1":"import matplotlib as mpl\nCOLOR = 'white'\nmpl.rcParams['text.color'] = COLOR\nmpl.rcParams['axes.labelcolor'] = COLOR\nmpl.rcParams['xtick.color'] = COLOR\nmpl.rcParams['ytick.color'] = COLOR\nmpl.rcParams['axes.grid'] = False\nmpl.rcParams['savefig.transparent'] = True\npal = dict(Yes=\"#af7132\", No=\"#ffdd6b\")\npal1 = ['#ffdd6b', '#dcae52', '#af7132']\nsns.set_palette(pal1)","ecd3e89f":"f, axes = plt.subplots(1,2,figsize=(14,5))\nsns.distplot(dropped.user_no_outgoing_activity_in_days, color=\"#dcae52\", ax=axes[0])\naxes[0].set_title(\"'user_no_outgoing_activity_in_days' distribution\")\naxes[1] = sns.distplot(dropped[dropped.user_no_outgoing_activity_in_days < 365]['user_no_outgoing_activity_in_days'], color=\"#dcae52\")\naxes[1].set_title(\"'user_no_outgoing_activity_in_days' new distribution\")\nplt.show()\nf.savefig(\"user_no_outgoing_activity_in_days_DIST.png\")\ndropping = dropped[dropped.user_no_outgoing_activity_in_days > 365].shape[0]\nprint('There were {} records dropped'.format(dropping))\n#dropped = dropped[dropped.user_no_outgoing_activity_in_days < 365]","9ac59ce8":"f, axes = plt.subplots(1,4,figsize=(12,3),sharey=True)\nf.subplots_adjust(wspace=0)\nf.suptitle('Diffirent customer services')\ntables = [user_has_outgoing_calls, user_has_outgoing_sms, user_use_gprs, user_does_reload]\ni = 0\nfor table in tables:\n    table.plot(kind='bar', ax=axes[i], stacked=True,color=pal)\n    axes[i].tick_params(axis='y', labelsize=7)\n    axes[i].set(ylim=(0,70000))\n    axes[i].legend(fancybox=True, framealpha=0,title=\"Churn\")\n    if i < 3 :\n        axes[i].get_legend().remove()\n    i+=1\nplt.show()\nf.savefig(\"Diffirent customer services.png\")","cc08b65b":"%%time\nrfm_segmentation = rfm.copy()\nNc = range(1, 10)\nkmeans = [KMeans(n_clusters=i) for i in Nc]\nscore = [kmeans[i].fit(rfm_segmentation).score(rfm_segmentation) for i in range(len(kmeans))]\nf, ax = plt.subplots(1, figsize=(7,5))\nax.plot(Nc,score, linewidth=2.5)\nax.set_title('Elbow Curve')\nax.set_xlabel('Number of clusters')\nax.set_ylabel('Score')\nax.grid(axis = 'x', linestyle = '--', linewidth = 0.4)\nf.savefig(\"Elbow_RFM.png\")\nplt.show()","81c7c0f7":"pal2 = ['#ffdd6b', '#af7132']\nsns.set_palette(pal2)","6cb61a4a":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18,4))\n#f.suptitle('Rcency, Frequency and Monetary Box-Plots', fontsize=18)\n\npourc = new[['churn']].value_counts()\/len(new)*100\npourc = pd.DataFrame(pourc, columns=['pourcentage']).round().reset_index()\n#pourc.churn = pourc.churn.map({0:'No', 1:'Yes'})\nsns.barplot(data = pourc, x='churn', y='pourcentage', ci=None ,orient='v' ,ax=ax1)\nax1.set(ylim=(0, 100))\nax1.set_title(\"Before Resampling\", fontsize=12)\nax1.set_xlabel (\"Churn\",fontsize=10)\nax1.set_ylabel (\"Percentage %\",fontsize=10)\nfor rect in ax1.patches:\n    ax1.text (rect.get_x() + rect.get_width() \/ 2-0.09,rect.get_height()+0.9,\"%.1f%%\"% rect.get_height(), weight='bold',fontsize=10 )\n\npourc2 = y_sm.value_counts()\/len(y_sm)*100\npourc2 = pd.DataFrame(pourc2).round().reset_index()\n#pourc2['index'] = pourc2['index'].map({0:'No', 1:'Yes'})\nsns.barplot(data = pourc2, y='churn', x='index', ci=None ,orient='v' ,ax=ax2)\nax2.set(ylim=(0, 100))\nax2.set_title(\"After SMOTE Oversampling\", fontsize=12)\nax2.set_xlabel (\"Churn\",fontsize=10)\nax2.set_ylabel (\"Percentage %\",fontsize=10)\nfor rect in ax2.patches:\n    ax2.text (rect.get_x() + rect.get_width() \/ 2-0.09,rect.get_height()+0.9,\"%.1f%%\"% rect.get_height(), weight='bold',fontsize=10 )\n\npourc3 = y.value_counts()\/len(y)*100\npourc3 = pd.DataFrame(pourc3).round().reset_index()\n#pourc3['index'] = pourc3['index'].map({0:'No', 1:'Yes'})\nsns.barplot(data = pourc3, y='churn', x='index', ci=None ,orient='v',ax=ax3 )\nax3.set(ylim=(0, 100))\nax3.set_title(\"After Random Undersampling\", fontsize=12)\nax3.set_xticklabels(['No','Yes'])\nax3.set_xlabel (\"Churn\",fontsize=10)\nax3.set_ylabel (\"Percentage %\",fontsize=10)\nfor rect in ax3.patches:\n    ax3.text (rect.get_x() + rect.get_width() \/ 2-0.09,rect.get_height()+0.9,\"%.1f%%\"% rect.get_height(), weight='bold',fontsize=10 )\n\nf.savefig(\"Resampling_imbalance.png\")    \nplt.show()","8b4e945d":"%%time\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10,9))\n#cmap = LinearSegmentedColormap.from_list(\"\", ['#FFFFFF',\"#FFF5BD\",\"#FF4646\",'#E41A1C',\"#960018\"])\n\nmodels = {ax1: ensemble.ExtraTreesClassifier(),\n          ax2: ensemble.RandomForestClassifier(),\n          ax3 : LGBMClassifier(n_jobs=-1),\n          ax4 : XGBClassifier(n_jobs=-1, verbosity=0)\n         }\nfor ax, model in models.items():\n    ax.grid(False)\n    mod = model.fit(X,y)\n    #y_pred = pd.Series(mod.predict(X_test)).map({0:'No', 1:'Yes'}).to_numpy()\n    plot_confusion_matrix(mod, \n                          X_test, \n                          y_test, \n                          ax=ax, \n                          cmap=cmap,\n                         display_labels=['No', 'Yes'],\n                         normalize='true')\n    ax.set_title(str(model.__class__.__name__), fontsize=12)\nf.savefig(\"ML_Confusion_matrices.png\")     \nplt.show()\n","19fee5dc":"%%time\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n#f.suptitle('Hyperparamter results comparison', fontsize=18)\n\nmodels = {ax1: XGBClassifier(verbosity = 0, n_jobs=-1),\n          ax2: XGB\n         }\nfor ax, model in models.items():\n    ax.grid(False)\n    mod = model.fit(X,y)\n    #y_pred = pd.Series(mod.predict(X_test)).map({0:'No', 1:'Yes'}).to_numpy()\n    plot_confusion_matrix(mod, \n                          X_test, \n                          y_test, \n                          ax=ax, \n                          cmap=cmap,\n                         display_labels=['No', 'Yes'],\n                         normalize='true')\n    ax1.set_title('XGBClassifier before Hyperparameter', fontsize=12)\n    ax2.set_title('XGBClassifier after Hyperparameter', fontsize=12)\n\nf.savefig(\"Hyperparameter.png\")\nplt.show()\n","e06491bd":"y_prob = XGB.predict_proba(X_test)\n    \nfpr_t, tpr_t, t_t = roc_curve(y_test, y_prob[:, 1])\n\naverage_precision = average_precision_score(y_test, y_prob[:, 1])\nprecision, recall, _ = precision_recall_curve(y_test, y_prob[:, 1])\nroc_auc = auc(fpr_t, tpr_t)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\n\nf, (ax1, ax2) = plt.subplots(1,2,figsize=(12,5))\n#f.suptitle('ROC and Precision-Recall Curves', fontsize=15)\n\n#ROC\nax1.plot([0, 1], [0, 1], linestyle = '--', lw = 2,label='Baseline', color = 'grey', alpha= 1)\nax1.step(fpr_t, tpr_t, lw=2, alpha=0.93,where='post', label='Test ROC ', color = '#ffdd6b')\nax1.fill_between(fpr_t, tpr_t, step='post', alpha=0.2,color='#ffdd6b', label='AUC = %0.2f' % (roc_auc))\nax1.set_xlim([-0.01, 1.01])\nax1.set_ylim([-0.01, 1.01])\nax1.set_ylabel('True Positive Rate')\nax1.set_xlabel('False Positive Rate')\nax1.legend(loc=\"lower right\",prop={'size': 8},fancybox=True, framealpha=0)\nax1.set_title('Receiver operating characteristic (ROC) curve', fontsize=12)\n\n#Precision-Recall\nax2.plot([0, 1], [no_skill, no_skill], linestyle='--', label='Baseline', alpha=1, color = 'grey')\nax2.step(recall, precision, color='#af7132', alpha=0.93,where='post', label='XGBoost')\nax2.fill_between(recall, precision, step='post', alpha=0.2,color='#af7132', label='Average Precision-Recall Score ={0:0.2f}'.format(average_precision))\nax2.set_ylabel('Precision')\nax2.set_xlabel('Recall')\nax2.set_xlim([-0.01, 1.01])\nax2.set_ylim([-0.01, 1.01])\nax2.set_title('Precision-Recall curve:', fontsize=12)\nax2.legend(loc=\"lower right\",prop={'size': 8},fancybox=True, framealpha=0)\nf.savefig(\"Roc_Recall-Precision.png\")    \nplt.show()\n","2bb450b4":"shap.summary_plot(shap_values, X, plot_type=\"bar\", color='#dcae52', max_display=10,show=False, axis_color='#FFFFFF')\nplt.savefig(\"SHAP_bar.png\")","92fc5c00":"shap.plots.bar(shap_values, show=False)\nplt.savefig(\"SHAP_bar.png\")","595cb9b1":"summ = LinearSegmentedColormap.from_list(\"\",['#fff380','#af7817'])\nsumm","646257c6":"import inspect\n\nprint(inspect.getargspec(shap.plots.waterfall))","91cd17c8":"shap.summary_plot(shap_values,X, show=False,axis_color='#FFFFFF', max_display=10, sort=True, auto_size_plot=True, cmap = summ)\nplt.savefig(\"SHAP_summary.png\")","1817b847":"shap.force_plot(shap_values[1420],plot_cmap=['#af7817','#eac117'])","c5f29d9a":"shap.plots.force(shap_values[6311],plot_cmap=[ '#af7817','#eac117'])","c98e11fa":"shap.summary_plot(shap_values,X, show=False,axis_color='#FFFFFF', max_display=10, sort=True, auto_size_plot=True, cmap = summ)","d7636e62":"shap.plots.waterfall(shap_values[100])","d778d968":"res_hard = pd.DataFrame(cv_results)\nres_hard.mean()","034d19ce":"%%time\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\nscoring = ['accuracy','precision_weighted', 'recall_weighted', 'f1_weighted']\ncv_results = model_selection.cross_validate(vote_hard, X, y,cv  = cv_split, scoring=scoring, n_jobs=-1)\nprint(cv_results['fit_time'].mean(), '==>  ', 'is done!')","2291fded":"%time\nfrom sklearn.ensemble import VotingClassifier\nvote_soft = VotingClassifier(estimators=[('XGB', XGB),\n                                    ('RF', RF),\n                                    ('ET', ET)],\n                        voting='soft')\nvote_soft.fit(X, y)  #Used the whole training data\ny_pred = vote_soft.predict(X_test)\ny_true = pd.Series(y_test).map({0:'No', 1:'Yes'}).to_numpy()\ny_pred = pd.Series(y_predict).map({0:'No', 1:'Yes'}).to_numpy()\nskplt.metrics.plot_confusion_matrix(\n    y_true, \n    y_pred,\n    figsize=(12,12),\n    normalize=True,\n    cmap = 'Reds')\n","017f280d":"%%time\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\nscoring = ['accuracy','precision_weighted', 'recall_weighted', 'f1_weighted']\ncv_results = model_selection.cross_validate(vote_soft, X, y,cv  = cv_split, scoring=scoring, n_jobs=-1)\nprint(cv_results['fit_time'].mean(), '==>  ', 'is done!')","696e19b2":"res_soft = pd.DataFrame(cv_results)\nres_soft.mean()","ef85f45b":"# compare hard voting to standalone classifiers\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn import ensemble\nfrom matplotlib import pyplot\n \n\n# get a voting ensemble of models\ndef get_voting():\n    # define the base models\n    models = list()\n    models.append(('XGB', XGBClassifier()))\n    models.append(('RF', ensemble.RandomForestClassifier()))\n    models.append(('GB', ensemble.GradientBoostingClassifier()))\n    models.append(('ET', ensemble.ExtraTreesClassifier()))\n    #models.append(('LGBM', LGBMClassifier(n_jobs=-1)))\n    # define the voting ensemble\n    Ensemble = VotingClassifier(estimators=models, voting='hard')\n    return Ensemble\n \n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    models['XGB'] = XGBClassifier()\n    models['RF'] = ensemble.RandomForestClassifier()\n    models['GB'] = ensemble.GradientBoostingClassifier()\n    models['ET'] = ensemble.ExtraTreesClassifier()\n    #models['LGBM'] = LGBMClassifier(n_jobs=-1)\n    models['hard_voting'] = get_voting()\n    return models\n \n# evaluate a give model using cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='recall', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n \n# define dataset\n#X, y = get_dataset()\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","db0d93e8":"We want to get the right number of cluster for K-means so we neeed to loop from 1 to 10 number of cluster and check score.\nElbow method is used to represnt that. ","99fff92b":">XGB 0.912 (0.007)\n>RF 0.899 (0.008)\n>GB 0.875 (0.009)\n>ET 0.913 (0.006)","9a12c4c3":"#### Comparing churn in the three clusters","b695508f":"#### SMOTE","4cacaacf":"We conclude that:\n   * Churn happens more often in the cluster number 1, then number 0 and number 2 respectively,these results match with the previous results.\n   * Cluster number 2 is the most profitable cluster and it includes the most loyal clients.\n   * Cluster number 1 is the least profitable cluster and it includes the occasional clients.\n> Thus we will eliminate the cluster number 1.","9538b33c":"#### Feature Importance ","3ada84b4":"### Resampling","1ab4008e":"### Recursive feature elimination with cross validation and random forest classification (RFECV)","2ab94216":"##### The best model is an XGBoost model.\n##### **Note**: we can see that Ensemble models preform way better than classical models","39d6b022":"Imbalanced classification involves developing predictive models on classification datasets that have a severe class imbalance.\n\nThe challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important.\n\nOne approach to addressing imbalanced datasets is to oversample the minority class. The simplest approach involves duplicating examples in the minority class, although these examples don\u2019t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE for short.","2d46471f":"### Checking for Class Imbalances","25496916":"> Acquiring a new custommer costs **5 times** than kepping an existing one\n##### The goal here is to improve the model in in the 'Recall' score in order to detect the maximum number of churners.","bd5789d0":"This creates a new column called cluster which has cluster number for each row respectively and adds it to the original data","11f16494":"* The correlation matrix of the dropped columns shows that every column has at least one or more columns in which the correlation exceeds 85%.","22008fe0":"#### Confusion Matrix","93246565":"* There is a positive correlation between user_spendings and calls_outgoing_spendings, which suggests that outgoing calls are one of the main services that drives the spending habits of the custommers.\n* The more the user spends the less probable that he will churn and vice-versa.","23e21982":"The goal of the clustering is to determine and keep the clients that are most profitable and to drop the occasional clients that barely add value to the company.","79623374":"#### 1. Over-Sampling (SMOTE)","49ef316b":"This section is added because my presentation in power point has a dark theme and gold colors.","6b6a9def":"#### 2. Under-Sampling","78501197":"\n\nRFM is a method used for analyzing customer value. It is commonly used in database marketing and direct marketing and has received particular attention in retail and professional services industries\n\nRFM stands for the three dimensions:\n\n    Recency \u2013 How recently did the customer purchase?\n    Frequency \u2013 How often do they purchase?\n    Monetary Value \u2013 How much do they spend?\n\n   ","30f42a94":"#### Intrepreting the results","6cbc482d":"Feature selection is the process of identifying a subset of important features\/attributes (such as a column in tabular data) that are most relevant to the modeling and business objectives of the problem. It essentially aids in the extraction of the most meaningful inputs from the data.","af374385":"### From previous results, the best params are :","ac602e24":"Due to the large number fo features (46), the visual exploratory data analysis will focus on the most prominent features.","f82e875d":"The new model has improved in the Recall score of the positive class 'Yes', **Although** it has lost some Accuarcy and Precision which is understandable and does not affect much the study.","32c16da6":"#### Inspecting the classification report","89f4ef37":"# Model Validation","aa88ddbd":"#### Correlation matrix of the data","daf22cf9":"#### Visualizing groups ","621a916b":"# Model Selection","ea224c48":"### Choosing the best model by Cross-Validation","d1dae813":"### Correlation Inspection ","29567530":"There is a clear imbalance in the target variable, we will have to rectify this imbalance with resampling techniques","250b8bf8":"### SHAP Values","006ee454":"### Train-Test split","3f92a377":"# Presentation","559dfee5":"#### Correlation matrix after filtering","412f8d1b":"### Checking for null values ","ba4ff451":"# RFM Analysis","d210e0ae":"the optimal k = 3","f3dd728d":"**37** is the ideal number of features which was determined by the RFECV algorithm","a5cc86af":"* There are some clients with no activity for more than 3 years (1200 - 1400 days).\n* Any Custommer who doesn't show any activity sign whatsoever for more than one year is considered as non active custommer, and therefore is excluded from the study. ","5c7be006":"# Exploratory Data Analysis","893c8bf5":"#### Analysing the user_no_outgoing_activity_in_days column","30071203":"* There are numerous columns with high correlation in the data.","c90707ac":"This section is under construction !","a8604aa0":"#### Fitting data in Kmeans (K = 3).","203f6e9d":"## Voting Classifires","5225d12f":"Box-Plot of the RFM analysis","7ec010d5":"* Most of the users spend from 0 to 100.","c8004e75":"%%time\nfrom sklearn.ensemble import VotingClassifier\nvote_hard = VotingClassifier(estimators=[('XGB', xgb),\n                                    ('RF', RF),\n                                    ('ET', ET)],\n                        voting='hard')\nvote_hard.fit(X, y)  #Used the whole training data\ny_pred = vote_hard.predict(X_test)\ny_true = pd.Series(y_test).map({0:'No', 1:'Yes'}).to_numpy()\ny_pred = pd.Series(y_predict).map({0:'No', 1:'Yes'}).to_numpy()\nskplt.metrics.plot_confusion_matrix(\n    y_true, \n    y_pred,\n    figsize=(12,12),\n    normalize=True,\n    cmap = 'Reds')\n","fff7621a":"### Univariate","c9962b4a":"#### Scaling the RFM Table","2445948a":"### Hyperparameter Tuning ","bf4cfd45":"### Creating the RFM table","9df91f21":"### Clustering RFM with K-Means","d0b7d495":"SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n\nSpecifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.\n\n**NOTE** : As mentioned in the paper, it is believed that SMOTE performs better when combined with undersampling of the majority class, such as random undersampling.","1c07e41f":"## Feature Selection ","3982c462":"# Data Preprocessing","375544ca":"## Dealing with calss imbalance","f97279d8":"As expected, the previous results are confirmed, thus, we will be using XGBoost model.","600c4a0e":"#### Fitting and Applying the elbow method to determine the numbeer of clusters","c3b34363":"* The churners stay inactive more than the non-churners.","90467840":"### Standardizing the Data ","2f4e907a":"* If the user has NO outgoing calls, he is more likely to churn, which means that 'user_has_outgoing_calls' is good for separating churners from no-churners\n* If the user has outgoing sms, he isn't likely to churn.\n* If the user uses gprs, he isn't likely to churn\n* The 'user_does_reload' variable doesn't separate well the target variable 'churn'","96c8c7e4":"* As expected, if the user has a relatively high balance in his account, he is less likely to churn.\n* So the lesser balance account and reloads sum, the more likely that the user will churn.","094d81af":"#### Fitting the best parameters model","6bae155e":"* In the **Recency** Box-Plot, the cluster number 1 has the highest value which means that those clients didn't do any transaction in a long time when compared to the other two clusters.\n* In the **Frequency** Box-Plot, the cluster number 1 has the lowest frequency value.\n* In the **Monetary** Box-Plot, the cluster number 2 has the highest value between the three which means that this group is the most profitable between the three groups.\n\nAcording to recency, frequency and monetary box-plots, the clusters number 0 and 2 are the most profitable clusters, thus, we will eliminate cluster number 1.","3b7de62d":"plt.clf()\nx = shap.plots.bar(shap_values, X)\nplt.savefig(\"SHAP_bar.png\")\n","d102ffa3":"From previous trials, the best columns are:","c832cc8e":"#### Correlation matrix of filtered columns (85%) \n","c5d5ed3e":"Basically, it uses one of the classification methods (random forest in our example), assign weights to each of features. Whose absolute weights are the smallest are pruned from the current set features, in contrast with RFF, we will not only find best features but we also find how many features do we need for best accuracy.","8cd4bfa3":"* After dropping the correlated columns, the new data seems to be clear of extreem correlations.","d10a6055":"#### Comparing top models on test data using confusion matrix","7f4fd057":"The rate of churned customers captured by the model is satisfying"}}