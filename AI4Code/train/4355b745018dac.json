{"cell_type":{"3f0f0da0":"code","984de5a6":"code","037c56b8":"code","2964f6d5":"code","d21ce5a0":"code","d9d1a32f":"code","6a6537e1":"code","d225cbd4":"code","803f2b4f":"code","17199ce3":"code","6236a603":"code","2a89cfcd":"code","14cc55b9":"code","1367449f":"code","62335cbc":"code","b06c858c":"code","98f234b3":"code","0488b1a7":"code","1a1abd4a":"code","e6ef6839":"code","a3b61c1b":"code","f5c8e469":"code","80e82470":"code","bd517f10":"code","d368aafe":"code","a9323a28":"code","361980ec":"code","75dec209":"code","9da55c6b":"code","563aa3a7":"code","fb937a1d":"code","6d2a825a":"code","5c30be25":"code","8d7eff51":"code","244acfc4":"code","bb3050bf":"code","e4b59905":"code","b0eac745":"code","8d6ad492":"code","2b107724":"code","5b858e1d":"code","9ffbf7cf":"code","f1c6cb55":"code","f8d42b39":"code","8dc4c2b0":"code","159f36e7":"code","fff5904b":"code","2e7462dd":"code","67889fdd":"code","37457473":"code","f3c8b2fe":"code","811077ee":"code","ec7cd913":"code","f721de6c":"code","3f79d9a6":"code","5c9b814d":"code","0d23d70d":"code","d533d389":"code","a6f1735a":"code","a2751ff7":"code","682ab5b5":"code","1e94b695":"code","9d5164dd":"code","f6eaedbf":"code","7844261c":"code","4afd3a11":"code","8963bb73":"code","b56e315e":"code","b049851f":"code","20972a6a":"code","4eb1cffd":"code","f4b3f31a":"code","98bebe29":"code","343dc05c":"code","940aa4e7":"code","8c021e24":"code","795eb4f7":"code","0b241904":"code","43e43757":"code","1a104bd0":"code","b8109002":"code","bd7723ea":"code","a7e12232":"code","a0a3ac2d":"code","7dae7482":"code","b2f67e23":"code","522ce8a8":"code","49bd396f":"code","9b1162d8":"code","5bf0e8ca":"code","b9e733b4":"code","10003776":"code","b70ba64f":"code","64c8cd8e":"code","93718e98":"code","83b3e25c":"code","0789b32d":"code","20e362cb":"code","525893a5":"code","f02f15cd":"code","80612582":"code","f7f21daa":"code","b9285ea6":"markdown","036347b3":"markdown","02e354fb":"markdown","1b4c8f8d":"markdown","0b5249a4":"markdown","cf72016c":"markdown","cd344ef7":"markdown","3ac7c384":"markdown","9d27b79e":"markdown","8fc51a57":"markdown"},"source":{"3f0f0da0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","984de5a6":"import matplotlib.pylab as plt\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datetime\nfrom matplotlib import pyplot as plt\nwarnings.simplefilter(\"ignore\")\nplt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]","037c56b8":"pd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\npd.set_option('display.width', 200)\npd.set_option('display.max_rows', None)","2964f6d5":"train_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntest_transaction  = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_transaction.csv\")\n\ntest_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_identity.csv\")\ntrain_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")\n\nsample_submission = pd.read_csv(\"..\/input\/ieee-fraud-detection\/sample_submission.csv\")","d21ce5a0":"print('train_transaction shape is {}'.format(train_transaction.shape))\nprint('test_transaction shape is {}'.format(test_transaction.shape))\nprint('train_identity shape is {}'.format(train_identity.shape))\nprint('test_identity shape is {}'.format(test_identity.shape))","d9d1a32f":"# trainleri birle\u015ftirdik:\ntrain = pd.merge(train_transaction,train_identity, on=\"TransactionID\", how=\"left\")\ntrain.head()","6a6537e1":"# testleri birle\u015ftirdik:\ntest = pd.merge(test_transaction,test_identity, on=\"TransactionID\", how=\"left\")\ntest.head()","d225cbd4":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","803f2b4f":"train = reduce_mem_usage(train)\n\ntest = reduce_mem_usage(test)","17199ce3":"del train_identity, train_transaction, test_identity, test_transaction","6236a603":"train.shape","2a89cfcd":"print(f'There are {train.isnull().any().sum()} columns in train dataset with missing values.')","14cc55b9":"train.isnull().any().sum()\/train.shape[1]","1367449f":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\nprint(f'There are {len(one_value_cols)} columns in train dataset with one unique value.')\nprint(f'There are {len(one_value_cols_test)} columns in test dataset with one unique value.')","62335cbc":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    \"\"\"\n\n    Veri setindeki kategorik, numerik ve kategorik fakat kardinal de\u011fi\u015fkenlerin isimlerini verir.\n    Not: Kategorik de\u011fi\u015fkenlerin i\u00e7erisine numerik g\u00f6r\u00fcn\u00fcml\u00fc kategorik de\u011fi\u015fkenler de dahildir.\n\n    Parameters\n    ------\n        dataframe: dataframe\n                De\u011fi\u015fken isimleri al\u0131nmak istenilen dataframe\n        cat_th: int, optional\n                numerik fakat kategorik olan de\u011fi\u015fkenler i\u00e7in s\u0131n\u0131f e\u015fik de\u011feri\n        car_th: int, optinal\n                kategorik fakat kardinal de\u011fi\u015fkenler i\u00e7in s\u0131n\u0131f e\u015fik de\u011feri\n\n    Returns\n    ------\n        cat_cols: list\n                Kategorik de\u011fi\u015fken listesi\n        num_cols: list\n                Numerik de\u011fi\u015fken listesi\n        cat_but_car: list\n                Kategorik g\u00f6r\u00fcn\u00fcml\u00fc kardinal de\u011fi\u015fken listesi\n\n    Examples\n    ------\n        import seaborn as sns\n        df = sns.load_dataset(\"iris\")\n        print(grab_col_names(df))\n\n\n    Notes\n    ------\n        cat_cols + num_cols + cat_but_car = toplam de\u011fi\u015fken say\u0131s\u0131\n        num_but_cat cat_cols'un i\u00e7erisinde.\n        Return olan 3 liste toplam\u0131 toplam de\u011fi\u015fken say\u0131s\u0131na e\u015fittir: cat_cols + num_cols + cat_but_car = de\u011fi\u015fken say\u0131s\u0131\n\n    \"\"\"\n\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car","b06c858c":"# eksiklik oran\u0131\n(train.isnull().sum() \/ train.shape[0] * 100).sort_values(ascending=False)","98f234b3":"cat_cols, num_cols, cat_but_car = grab_col_names(train)","0488b1a7":"cat_cols, num_cols, cat_but_car = grab_col_names(test)","1a1abd4a":"train['isFraud'].value_counts(normalize=True)","e6ef6839":"sns.countplot(x=\"isFraud\", data=train).set_title('Distribution of Target')\nplt.show()","a3b61c1b":"round((len(train[train['isFraud']==1])\/len(train))*100,2)","f5c8e469":"train.columns","80e82470":"train['TransactionAmt'].isnull().any()","bd517f10":"train['DeviceType'].isnull().any()","d368aafe":"sns.countplot(train['DeviceType'])","a9323a28":"sns.countplot(test['DeviceType'])","361980ec":"sns.boxplot(train['TransactionAmt'])\nplt.show()","75dec209":"def target_summary_with_num(dataframe, target, numerical_col):\n    print(dataframe.groupby(target).agg({numerical_col: \"mean\"}), end=\"\\n\\n\\n\")","9da55c6b":"target_summary_with_num(train, \"isFraud\", \"TransactionAmt\")","563aa3a7":"### TransactionDT ###\n\nplt.figure(figsize=(15,5))\nsns.distplot(train[\"TransactionDT\"])\nsns.distplot(test[\"TransactionDT\"])\nplt.title('train vs test TransactionDT distribution')\nplt.show()\n","fb937a1d":"# Notice the minimum value of TransactionDT is 86400 that happens to be the number of seconds of 1 day so we can assume that the unit of the column is second*\n# TransactionDT data of train and test set are from different distribution as the graphs don't overlap.\n# The data spreads across 396 days which is about 13 months\n# Looking at the peaks at both ends, they're likely to be associated with a festive season, Black Friday or Christmas maybe","6d2a825a":"#YEAR\ntrain.groupby('DeviceInfo').agg({'TransactionDT':'min'}).sort_values('TransactionDT').head(20)\n# A quick check shows that the Samsung Galaxy S8 ***(SAMSUNG SM-G892A Build\/NRD90M)*** was released in the US the earliest on 21st April 2017 (Wikipedia). Since there's transactions made by this phone the 2nd day of the data's time, this data apparently can only be as old as ***April of 2017***\n#SAMSUNG S8'in \u00e7\u0131k\u0131\u015f tarihi*","5c30be25":"#DATE\nnp.max(test['TransactionDT'])\/86400 - np.min(train['TransactionDT'])\/8640","8d7eff51":"google trend fotosu gelecek. https:\/\/trends.google.com\/trends\/explore?cat=18&date=2017-04-01%202018-12-31&geo=US","244acfc4":"# g\u00fcn baz\u0131nda transaction da\u011f\u0131l\u0131m\u0131\n# How about plotting the TransactionDT day wise?\nplt.figure(figsize=(20,10))\ntrain_1, test_1 = plt.hist(np.ceil(train['TransactionDT']\/86400), bins=182), plt.hist(np.ceil(test['TransactionDT']\/86400), bins=182)\nplt.show()","bb3050bf":"# **Assuming we're right that the peaks are associated with the Black Friday season. Let's see what are the dates we're looking at**\ntrain_1[1][:182][train_1[0]> 6000]","e4b59905":"# Since 2017's Black Friday was 24th November that happens to be tally with the numbers above. Let us just assume that the data begins on November 1st of 2017, Let's have a look at the 2nd peak in the test data\ntest_peaks = test_1[1][:182][test_1[0]> 5000]\ntest_peaks","b0eac745":"[datetime.date(2017,11,1) + datetime.timedelta(days=x) for x in test_peaks.tolist()]\n# [datetime.date(2018, 11, 23),\n#  datetime.date(2018, 11, 26),\n#  datetime.date(2018, 11, 27),\n#  datetime.date(2018, 12, 1)]\n# *23rd November 2018 is Black Friday, and 26th November 2018 is Cyber Monday, how cool is that**\n","8d6ad492":"### PRODUCTCD ###\ntrain['ProductCD'].isnull().any()","2b107724":"## let's look at the unique values\ntrain['ProductCD'].value_counts()","5b858e1d":"# For now we don't know exactly what these values represent.\n\ntrain.groupby('ProductCD') \\\n    ['TransactionID'].count() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(15, 3),\n         title='Count of Observations by ProductCD')\nplt.show()\n\ntrain.groupby('ProductCD')['isFraud'] \\\n    .mean() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(15, 3),\n         title='Percentage of Fraud by ProductCD')\nplt.show()","9ffbf7cf":"train.groupby('ProductCD')['isFraud'] \\\n    .value_counts() \\\n    .sort_values(ascending=False) \\\n    .plot(kind='bar',\n          figsize=(15, 3),\n         title='Percentage of Fraud-NoNFraud by ProductCD')\nplt.show()","f1c6cb55":"# plot to see if any pattern in product type and fraud\nsns.countplot(x='ProductCD',hue='isFraud',data=train)\nplt.show()\n\n# fraudlar genelde: W ve C ","f8d42b39":"### CARD1-CARD6 ###\n\n# We are told these are all categorical, even though some appear numeric.\n\ncard_cols = [c for c in train.columns if 'card' in c]\ntrain[card_cols].head()\n\nprint(train.dtypes)\n","8dc4c2b0":"color_idx = 0\nfor c in card_cols:\n    if train[c].dtype in ['float16','int16']:\n        train[c].plot(kind='hist',\n                      title=c,\n                      bins=50,\n                      figsize=(15, 2),\n                      color=color_pal[color_idx])\n    color_idx += 1\n    plt.show()\n    \n# Looking at the amount of unique values, I suppose card1 could be the Issuer Identification Number (IIN), encoded as a categorical variable with LabelEncoder.\n#\n# Furthermore, card3 or card5 could be the issue country of the card.\n#\n# We already know that card4 contains the card type (visa, mastercard, etc.), and card6 contains the card category (credit, debit, etc.).\n#\n# As there are a total of 6 card-variables, the question is what the other 2 variables are. Some options include:\n# Individual account identifier\n# Some kind of card identifier (e.g. Gold Card, Platinum Card, etc.)\n# With some pivot analysis we should be able to verify some of these hypotheses. E.g. all cards of bank X should be issued in country Y.\n#  'Card 1' column in Data Dictionary is given as Categorical but it is behaving like Continuous Data.\n# Having '13553' unique Value","159f36e7":"train_transaction_fr = train.loc[train['isFraud'] == 1]\ntrain_transaction_nofr = train.loc[train['isFraud'] == 0]\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 8))\ntrain_transaction_fr.groupby('card4')['card4'].count().plot(kind='barh', ax=ax1, title='Count of card4 fraud')\ntrain_transaction_nofr.groupby('card4')['card4'].count().plot(kind='barh', ax=ax2, title='Count of card4 non-fraud')\ntrain_transaction_fr.groupby('card6')['card6'].count().plot(kind='barh', ax=ax3, title='Count of card6 fraud')\ntrain_transaction_nofr.groupby('card6')['card6'].count().plot(kind='barh', ax=ax4, title='Count of card6 non-fraud')\nplt.show()","fff5904b":"train.card6.value_counts()","2e7462dd":"def target_summary_with_cat(dataframe, target, categorical_col):\n    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")","67889fdd":"target_summary_with_cat(train, \"isFraud\", \"card6\")","37457473":"### addr1 & addr2 ###\n# The data description states that these are categorical even though they look numeric. Could they be the address value?","f3c8b2fe":"print(' addr1 - has {} NA values'.format(train['addr1'].isna().sum()))","811077ee":"print(' addr2 - has {} NA values'.format(train['addr2'].isna().sum()))","ec7cd913":"train['addr1'].plot(kind='hist', bins=500, figsize=(15, 2), title='addr1 distribution')\nplt.show()","f721de6c":"train['addr2'].plot(kind='hist', bins=500, figsize=(15, 2), title='addr2 distribution')\nplt.show()\n","3f79d9a6":"# Unique count of Regions:\ntrain.addr1.nunique()","5c9b814d":"# To find top 10 regions by count of transaction.\naddr1_df = pd.DataFrame(train.addr1.value_counts())\naddr1_df = addr1_df.rename_axis(\"region\").reset_index()\naddr1_df = addr1_df.sort_values(by=[\"addr1\"], ascending=False).head(10)","0d23d70d":"top_region_df = train[train[\"addr1\"].isin(list(addr1_df[\"region\"]))]","d533d389":"def countplot_viz(\n    data,\n    xcolumn,\n    xlabel,\n    ylabel,\n    title,\n    hue=None,\n    fontsize_label=16,\n    fontsize_title=20,\n    fontsize_text=12,\n    rotation=45,\n    figsize_x=12,\n    figsize_y=5,\n    palette=\"mako\",\n):\n    \"\"\"\n    This function gets a Python Pandas dataframe and visualize a countplot.\n    :param data: Dataframe to be analyze\n    :param xcolumn: This column designates x axis column.\n    :param xlabel: It designates name of x axis column.\n    :param ylabel: It designates name of y axis column.\n    :param title: This column designates name of graph.\n    :param hue: Name of variables in `data` or vector data, optional Inputs for plotting long-form data.\n    :param fontsize_label: It designates label size.\n    :param fontsize_title: It designates title size.\n    :param rotation: It designates rotation of graph.\n    :param palette: It designates colors of graph.\n    :return: This function doesn't return anything.\n    \"\"\"\n    plt.figure(figsize=(figsize_x,figsize_y))\n    \n    g = sns.countplot(x=xcolumn, data=data, hue=hue, palette=palette)\n    g.set_title(title, fontsize=19)\n    g.set_xlabel(xlabel, fontsize=17)\n    g.set_ylabel(ylabel, fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(), rotation=40, ha=\"right\")\n    plt.tight_layout()\n    for p in g.patches:\n        height = p.get_height()\n        g.text(\n            p.get_x() + p.get_width() \/ 2.0,\n            height + 3,\n            \"{:1}\".format(height),\n            ha=\"center\",\n            fontsize=fontsize_text,\n        )    \n    if hue != None:\n        g.legend(bbox_to_anchor=(1, 1), loc=1, borderaxespad=0)","a6f1735a":"countplot_viz(\n    top_region_df,\n    \"addr1\",\n    \"Transaction Regions\",\n    \"Freq\",\n    \"Regions Distribution\",\n    palette=\"rocket\",)","a2751ff7":"# Unique count of Country:\ntrain.addr2.nunique()","682ab5b5":"# To find top 5 countries by count of transaction.\naddr2_df = pd.DataFrame(train.addr2.value_counts())\naddr2_df = addr2_df.rename_axis(\"country\").reset_index()\naddr2_df = addr2_df.sort_values(by=[\"addr2\"], ascending=False).head()\ntop_country_df = train[\n    train[\"addr2\"].isin(list(addr2_df[\"country\"]))]","1e94b695":"countplot_viz(\n    top_country_df,\n    \"addr2\",\n    \"Transaction Countries\",\n    \"Freq\",\n    \"Country Distribution\",\n    palette=\"rocket_r\",)","9d5164dd":"# Observation:\n# addr1:\n# There are 332 unique regions in dataframe. We show top 10 regions that have most transactions.\n# addr2:\n# There are 74 unique countries in dataframe. We show top 5 countries that have most transactions.","f6eaedbf":"### dist1 & dist2 ###\n\n# tahmin: card sahibi ve i\u015flem yap\u0131lan yer aras\u0131ndaki uzakl\u0131k\n# Plotting with logx to better show the distribution. Possibly this could be the distance of the transaction vs. the card owner's home\/work address. This is just a guess.\n\ntrain['dist1'].plot(kind='hist',\n                                bins=5000,\n                                figsize=(15, 2),\n                                title='dist1 distribution',\n                                color=color_pal[1],\n                                logx=True)\nplt.show()","7844261c":"train['dist2'].plot(kind='hist',\n                                bins=5000,\n                                figsize=(15, 2),\n                                title='dist2 distribution',\n                                color=color_pal[1],\n                                logx=True)\nplt.show()\n","4afd3a11":"### C1 - C14 ###\n# Because we are provided many numerical columns, we can create a pairplot to plot feature interactions. I know these plots can be hard to read, but it is helpful for gaining intution about potential feature interactions and if certain features have more variance than others.\nc_cols = [c for c in train if c[0] == 'C']\ntrain[c_cols].head()","8963bb73":"# Sample 500 fraud and 500 non-fraud examples to plot\nsampled_train = pd.concat([train.loc[train['isFraud'] == 0].sample(500),\n          train.loc[train['isFraud'] == 1].sample(500)])\nsns.pairplot(sampled_train,\n             hue='isFraud',\n            vars=c_cols)\nplt.show()","b56e315e":"### D1-D9 ###  \nprint(d_cols)","b049851f":"print(train[d_cols].dtypes)","20972a6a":"# Similarly for features D1-D9. In these plots we can see some linear and non-linear interactions between features. We may want to create additional features using these interactions if we think it would help our model better find relationship between fraud and non-fraud observations.\n\n#d_cols = [c for c in train if c[0] == 'D']\ntrain[d_cols].head()\n\nsns.pairplot(sampled_train,\n             hue='isFraud',\n            vars=d_cols)\nplt.show()","4eb1cffd":"d_cols = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15']","f4b3f31a":"### M1-M9 ###\n# Values are T F or NaN\n# Column M4 appears to be different with values like M2 and M0\n\nm_cols = [c for c in train if c[0] == 'M']\ntrain[m_cols].head()\n\n(train[m_cols] == 'T').sum().plot(kind='bar',\n                                              title='Count of T by M column',\n                                              figsize=(15, 2),\n                                              color=color_pal[3])\nplt.show()\n","98bebe29":"(train[m_cols] == 'F').sum().plot(kind='bar',\n                                              title='Count of F by M column',\n                                              figsize=(15, 2),\n                                              color=color_pal[4])\nplt.show()\n","343dc05c":"(train[m_cols].isna()).sum().plot(kind='bar',\n                                              title='Count of NaN by M column',\n                                              figsize=(15, 2),\n                                              color=color_pal[0])\nplt.show()\n\n","940aa4e7":"for i in m_cols:\n    target_summary_with_cat(train, \"isFraud\", i)","8c021e24":"# Looking at M4 column since it is different than the others\ntrain.groupby('M4')['TransactionID'] \\\n    .count() \\\n    .plot(kind='bar',\n          title='Count of values for M4',\n          figsize=(15, 3))\nplt.show()","795eb4f7":"### V1 - V339 ###\n# Lots of 1s 0s and Nans, some larger values\nv_cols = [c for c in train if c[0] == 'V']\ntrain[v_cols].head()\ntrain['P_emaildomain'].value_counts()\nsns.countplot(x='P_emaildomain',hue='isFraud',data=train)\nplt.show()","0b241904":"train[v_cols].describe()","43e43757":"train['v_mean'] = train[v_cols].mean(axis=1)\n\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15, 6))\ntrain.loc[train['isFraud'] == 1]['v_mean'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='log transformed mean of V columns - Fraud',\n          ax=ax1)\ntrain.loc[train['isFraud'] == 0]['v_mean'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='log transformed mean of V columns - Not Fraud',\n          color=color_pal[5],\n          ax=ax2)\nplt.show()","1a104bd0":"### Device type ###\ntrain.groupby('DeviceType') \\\n    .mean()['isFraud'] \\\n    .sort_values() \\\n    .plot(kind='barh',\n          figsize=(15, 5),\n          title='Percentage of Fraud by Device Type')\nplt.show()","b8109002":"train[\"DeviceType\"].value_counts()","bd7723ea":"train.groupby('DeviceInfo') \\\n    .count()['TransactionID'] \\\n    .sort_values(ascending=False) \\\n    .head(20) \\\n    .plot(kind='barh', figsize=(15, 5), title='Top 20 Devices in Train')\nplt.show()","a7e12232":"train['R_emaildomain'].value_counts()\nsns.countplot(x='R_emaildomain',hue='isFraud',data=train)\nplt.show()","a0a3ac2d":"train['P_emaildomain'].value_counts()\nsns.countplot(x='P_emaildomain',hue='isFraud',data=train)\nplt.show()","7dae7482":"# compile list of C columns\nc_list = []\nfor i in range(1,15):\n    c_list.append('C' + str(i))","b2f67e23":"# compile list of D columns\nd_list = []\nfor i in range(1,16):\n    d_list.append('D' + str(i))","522ce8a8":"# compile list of V columns\nv_list = []\nfor i in range(1,340):\n    v_list.append('V' + str(i))","49bd396f":"# compile list of id columns\nid_list = []\nfor i in range(1,12):\n    ii = str(i).zfill(2)\n    id_list.append('id_' + str(ii))\nid_list.extend(['id_13','id_14','id_17','id_18','id_19','id_20','id_21',\\\n                'id_22','id_24','id_25','id_26','id_32'])","9b1162d8":"# create list of columns label for correlation check\ncolums_for_corr = ['isFraud','TransactionAmt','card1','card2','card3','card5','addr1','addr2','dist1','dist2']\ncolums_for_corr.extend(c_list)\ncolums_for_corr.extend(d_list)\ncolums_for_corr.extend(v_list)\ncolums_for_corr.extend(id_list)","5bf0e8ca":"corr_matrix = train[colums_for_corr].corr()","b9e733b4":"#Hedef \u00f6zellik ile g\u00fc\u00e7l\u00fc bir korelasyona sahip hi\u00e7bir \u00f6zellik yok gibi g\u00f6r\u00fcn\u00fcyor, bu y\u00fczden onu devralmak i\u00e7in elimizden geldi\u011fince \u00e7ok \u00f6zelli\u011fi tutmam\u0131z gerekiyor\u00b6 \n#Daha fazla \u00f6zelli\u011fi silmeyece\u011fiz \n#Kategorik \u00f6zellikleri kodlamak i\u00e7in verileri tekrar birle\u015ftirme olacak","10003776":"# Let's plot a heatmap to have a sight about correlation. Generally speaking, C and D show a high internal correlation. The same for V, but with some exceptions.\n\nsns.set(rc={'figure.figsize':(20,20)})\nsns.heatmap(corr_matrix, cmap=\"RdBu\")\nplt.show()","b70ba64f":"# From the global correlation matrix, let's extract just the isFraud column and see labels with correlation higher than 0.20. (also correlation < - 0.2 would be interesting, but the minimum value encountered was about -0.14.\n\nisFraud_corr = corr_matrix['isFraud']\ncolumns_with_correlation = isFraud_corr.loc[isFraud_corr >= 0.2]\nprint(columns_with_correlation.sort_values(ascending=False))","64c8cd8e":"# For categorical variables, let's explore via graph the correlation between labels and isFraud.\n# First of all, let's create a function that counts the 1 and 0 values in isFraud for each label. Please note that this function neglect all labels that have at least one value with isFraud=0 but no one with isFraud=1. At this stage, this is not a high limitation as we are interested to check correlation with isFraud=1.\n\ndef check_corr_fraud(df, col):\n    fr1 = (df.loc[df['isFraud'] == 1][col].value_counts())\n    fr0 = (df.loc[df['isFraud'] == 0][col].value_counts())\n\n    fr = pd.merge(fr1, fr0, how='left',\n                  left_on=fr1.index,\n                  right_on=fr0.index)\n\n    fr = fr.rename(columns={'key_0': col,\n                            f'{col}_x': 'Fraud',\n                            f'{col}_y': 'Not Fraud'})\n\n    fr['tot count'] = fr['Fraud'] + fr['Not Fraud']\n\n    fr['% Fraud'] = 100 * np.divide(fr['Fraud'], fr['tot count'])\n\n    fr['% Not Fraud'] = 100 * np.divide(fr['Not Fraud'], fr['tot count'])\n\n    return fr\n","93718e98":"# Let's prepare also a function ready to make plots for categorical variables. Note that the ylim property was set to [0,100], in order to avoid misleading charts. The large function is to make a larger graph (for many categorical variables).\n\ndef plot_corr_fraud(df, col):\n    a = check_corr_fraud(train, col)\n\n    sns.set(rc={'figure.figsize': (11.7, 8.27)})\n    plt.bar(a[col], a['% Fraud'])\n    plt.ylim([0, 100])\n    plt.ylabel('% Fraud', fontsize=12)\n    plt.xlabel(col, fontsize=12)\n    plt.xticks(rotation=90)\n    plt.subplots_adjust(bottom=0.4)\n\n    return","83b3e25c":"def plot_corr_fraud_large(df, col):\n    a = check_corr_fraud(train, col)\n\n    sns.set(rc={'figure.figsize': (101.7, 8.27)})\n    plt.bar(a[col], a['% Fraud'])\n    plt.ylim([0, 100])\n    plt.ylabel('% Fraud', fontsize=12)\n    plt.xlabel(col, fontsize=12)\n    plt.xticks(rotation=90)\n    plt.subplots_adjust(bottom=0.4)\n\n    return","0789b32d":"# In the following, I will select just the graphs with relevant correlation between labels and isFraud=1.\n\nplot_corr_fraud(train,'P_emaildomain')\nplt.show()\n","20e362cb":"plot_corr_fraud(train,'R_emaildomain')\nplt.show()\n","525893a5":"# Some email domains look to be more exposed to fraudulent transactions.\n\nplot_corr_fraud_large(train,'id_31')\nplt.show()","f02f15cd":"# Some browser look to be more exposed to fraudulent transactions.\n\nplot_corr_fraud_large(train,'DeviceInfo')\nplt.show()","80612582":"# Baz\u0131 cihazlar doland\u0131r\u0131c\u0131l\u0131k i\u015flemlerine daha fazla maruz kal\u0131yor gibi g\u00f6r\u00fcn\u00fcyor.\n\n# Conclusion\n# From numerical variables, some from the V group showed an appreciable correlation with the isFraud label.\n#From categorical variables, emaildomain and DeviceInfo showed an appreciable correlation with the isFraud label.","f7f21daa":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_addr2'] = train['D15'] \/ train.groupby(['addr2'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_addr2'] = train['D15'] \/ train.groupby(['addr2'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_addr2'] = test['D15'] \/ test.groupby(['addr2'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_addr2'] = test['D15'] \/ test.groupby(['addr2'])['D15'].transform('std')\ntrain[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\ntest[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\ntest[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)","b9285ea6":"aralar\u0131nda anlaml\u0131 bir fark var. M1*","036347b3":"en b\u00fcy\u00fck fraud oran\u0131: credit","02e354fb":" Conclusion: Based on the patterns found within the data, together with insights provided by Google Trends, the data's start date is likely 2017\/11\/1","1b4c8f8d":"*  W has the most number of observations, C the least.\n*  ProductCD C has the most fraud with >11%\n*  ProductCD W has the least with ~2%","0b5249a4":"############################ FEATURE ENGINEERING ############################","cf72016c":"############################ EDA ############################","cd344ef7":"Kolonlar\u0131n %95'i eksik.","3ac7c384":"# CORRELAT\u0130ON","9d27b79e":"86400: 2 tarih aras\u0131nda zaman aral\u0131\u011f\u0131d\u0131r. Bu zaman aral\u0131\u011f\u0131n\u0131 saniye olarak varsay\u0131yoruz. Do\u011frulamak i\u00e7in google trendi baz ald\u0131k. Zaman aral\u0131\u011f\u0131n\u0131 tarihe \u00e7evirdi\u011fimizde fraud peak'i ve black friday peak'i benzer.","8fc51a57":"veriseti dengesiz bir target."}}