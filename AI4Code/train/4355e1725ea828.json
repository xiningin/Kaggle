{"cell_type":{"04ce827d":"code","d5ccd07b":"code","b42b2cab":"code","6933da86":"code","e536b648":"code","360f9c02":"code","8dd9309b":"code","6343bb23":"code","00560a3b":"code","770c618f":"code","70a5a3cb":"markdown","68ba3fb2":"markdown","689a1dc2":"markdown","290ed69e":"markdown","affac6e1":"markdown","2055c9e5":"markdown","0af7e268":"markdown","9b596802":"markdown","596b6bdb":"markdown","c12d0ea9":"markdown","5f160a39":"markdown","32ac70cd":"markdown","6f7fc267":"markdown","f5415805":"markdown","2d1bfefd":"markdown","1bb146c6":"markdown"},"source":{"04ce827d":"# importing the libraries\n\nimport torch\n# Variable object automatically does require_autograd = True\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets","d5ccd07b":"# torchvision.datasets has preloaded common datasets, ready to use. \ntrain_dataset = dsets.MNIST(root = '.\/data', train = True, transform = transforms.ToTensor(), download = True)\ntest_dataset = dsets.MNIST(root = '.\/data', train = False, transform = transforms.ToTensor(), download = True)","b42b2cab":"# Hyper-parameters settings\nbatch_size = 100\nn_iters = 3000\nepochs = n_iters \/ (len(train_dataset)\/ batch_size)\n\ninput_dim = 784\noutput_dim = 10\nlr_rate = 0.001","6933da86":"train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\ntest_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)","e536b648":"class a(torch.nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(a, self).__init__()\n        self.linear = torch.nn.Linear(input_dim, output_dim)\n        \n    def forward(self, x):\n        outputs = self.linear(x)\n        return outputs","360f9c02":"model = a(input_dim, output_dim)","8dd9309b":"# loss Class \ncriterion = torch.nn.CrossEntropyLoss() # computes softmax and then the cross entropy","6343bb23":"optimizer = torch.optim.SGD(model.parameters(), lr = lr_rate)","00560a3b":"# Training the model\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        \n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i + 1) % 100 == 0:\n            print('Epoch: [%d\/%d], Step: [%d\/%d], Loss: %.4f' %(epoch + 1, num_epochs, i+ 1, len(train_dataset)\/\/batch_size, loss.item()))\n    ","770c618f":"# Test the model\ncorrect = 0\ntotal = 0\n\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, 28*28))\n    outputs = model(images)\n    \n    _, predicted = torch.max(outputs.data, 1)\n    \n    total += labels.size(0)\n    \n    correct += (predicted == labels).sum()\n    \nprint('Accuracy of the model on the 10.000 test images: %d ' %(100 * correct \/ total))\n    ","70a5a3cb":"### Step 4 -> Instantiating the Model Class","68ba3fb2":"In the upcoming post, I'll discuss **Feed Forward Neural Networks**. Stay tuned!","689a1dc2":"### Step 2 -> Making the dataset iterable using DataLoader","290ed69e":"Here you<br>\nClass name can be anything. <br>\nIt can be logistic regression, it can be 'A', can be 'xyz' etc.","affac6e1":"### Step 3 -> Creating the Deep Learning Model Class","2055c9e5":"### Step 1 -> Loading and Preparing the Dataset","0af7e268":"### Evaluating the Model","9b596802":"Since the model isn't computationally expensive, you can run the notebook in 'cpu' device mode. **However, for Neural Networks you would require to switch to 'gpu' mode.**","596b6bdb":"Notebook Details:-<br>\n1. Framework Used -> PyTorch\n2. Dataset Used -> MNIST","c12d0ea9":"Steps\n \n1. Loading and Preparation of Dataset\n2. Making the dataset iterable\n3. Create Model Class\n4. Instantiate Model Class\n5. Instantiate Loss Class\n6. Instantiate Optimizer Class\n7. Train Model","5f160a39":"### Step 7 -> Training the Model","32ac70cd":"In this step, you load the dataset and do preparations on it (transformations) so that it can be used by the Deep Learning Models.<br>\nIn your case, we are using MNIST dataset which is provided by the PyTorch library and it is preprocessed. We need not to make much changes. <br>\nHowever, in most of the cases, you would require to implement the Dataset class describing how your dataset would be handled.","6f7fc267":"### Welcome to the Part-3 of PyTorch Series. <br>\n## Logistic Regression using PyTorch","f5415805":"### Step 6 -> Instantiate the Optimizer Class","2d1bfefd":"Link to the Previous Notebook -> https:\/\/www.kaggle.com\/superficiallybot\/getting-started-with-pytorch-series-part-2","1bb146c6":"### Step 5 -> Instantiating the Loss Class"}}