{"cell_type":{"1ed43b3a":"code","17ae6899":"code","d54c563a":"code","47dd8f63":"code","0fc19fac":"code","0ff1b795":"code","5a593042":"code","6665be08":"code","b66a8685":"code","916623bb":"code","5870a0f3":"code","f19c5bd6":"code","cc463a29":"code","9475d290":"code","a329dddf":"code","0b784625":"code","10eb8c00":"code","287ef393":"code","bd624553":"code","cb47a56f":"code","24d33c7f":"code","76dd62ea":"code","eb69ce0f":"code","92f9fc55":"code","347709fa":"code","ce5383ba":"code","d97f7b85":"code","fecca688":"code","1d7436a4":"code","8578c8ce":"code","9a956c38":"code","a7503866":"code","8563ceaa":"code","bd9473ec":"code","98251834":"code","979ba069":"code","a3ac9512":"markdown","4e5ffdf6":"markdown","90d3ee9c":"markdown","18dc296a":"markdown","e0c07e10":"markdown","c1c50e26":"markdown","e2eb9c4d":"markdown","7f6848c8":"markdown","19b56771":"markdown","3d03b642":"markdown","27f1a164":"markdown","a09dddd6":"markdown","125ce909":"markdown","385aadd9":"markdown","9dfa96ae":"markdown","ecfd9cce":"markdown"},"source":{"1ed43b3a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","17ae6899":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nimport torch\n\nimport warnings\nwarnings.simplefilter('ignore')","d54c563a":"import os\nprint(os.listdir(\"..\/input\/nlp-getting-started\"))","47dd8f63":"train_tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_tweet=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","0fc19fac":"print('There are {} rows and {} columns in train'.format(train_tweet.shape[0],train_tweet.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test_tweet.shape[0],test_tweet.shape[1]))","0ff1b795":"train_tweet.sample(5)","5a593042":"## Disaster tweets\ntrain_tweet[train_tweet['target']==1]['text'].values[:3]","6665be08":"## Non disaster tweets\ntrain_tweet[train_tweet['target']==0]['text'].values[:3]","b66a8685":"## check for the missing values in train and test dataset\ntrain_tweet.isnull().sum()","916623bb":"test_tweet.isnull().sum()","5870a0f3":"## target distribution in our training set\nsns.barplot(train_tweet['target'].value_counts().index,train_tweet['target'].value_counts())","f19c5bd6":"## Check for the most frequently used keywords in each type of tweets\n\n## disaster tweets\n\nsns.barplot(y=train_tweet[train_tweet['target']==1]['keyword'].value_counts()[:10].index,x=train_tweet[train_tweet['target']==1]['keyword'].value_counts()[:10],orient='h')","cc463a29":"## Non Disaster tweets\n\nsns.barplot(y=train_tweet[train_tweet['target']==0]['keyword'].value_counts()[:10].index,x=train_tweet[train_tweet['target']==0]['keyword'].value_counts()[:10],orient='h')","9475d290":"# Text Preprocessing ( Helper function)\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\n","a329dddf":"# Applying the cleaning function to both test and training datasets\ntrain_tweet['text'] = train_tweet['text'].apply(lambda x: clean_text(x))\ntest_tweet['text'] = test_tweet['text'].apply(lambda x: clean_text(x))","0b784625":"# A disaster tweet\ndisaster_tweets = train_tweet[train_tweet['target']==1]['text']\n\n#not a disaster tweet\nnon_disaster_tweets = train_tweet[train_tweet['target']==0]['text']\n\nfrom wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=[26, 18])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","10eb8c00":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_tweet[train_tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train_tweet[train_tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Number of Words in a tweet')\nplt.show()","287ef393":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_tweet[train_tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train_tweet[train_tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","bd624553":"train_tweet['text'] ","cb47a56f":"# text preprocessing function\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","24d33c7f":"# Applying the cleaning function to both test and training datasets\ntrain_tweet['text'] = train_tweet['text'].apply(lambda x: text_preprocessing(x))\ntest_tweet['text'] = test_tweet['text'].apply(lambda x: text_preprocessing(x))","76dd62ea":"train_tweet['text'][:5]","eb69ce0f":"## Applying Countvectorizer\n# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=5000,ngram_range=(1,3))","92f9fc55":"train_vectors = cv.fit_transform(train_tweet['text'])\ntest_vectors = cv.transform(test_tweet[\"text\"])","347709fa":"train_vectors.shape","ce5383ba":"cv.get_feature_names()[:20]","d97f7b85":"## train-test split\n\n## Divide the dataset into Train and Test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_vectors, train_tweet['target'], test_size=0.33, random_state=0)","fecca688":"## applied with TfidfVectorizer\n\n## TFidf Vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_v=TfidfVectorizer(max_features=5000,ngram_range=(1,3))\n\ntrain_vectors_tfidf = tfidf_v.fit_transform(train_tweet['text'])\ntest_vectors_tfidf = tfidf_v.transform(test_tweet[\"text\"])\n\nX_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(train_vectors_tfidf, train_tweet['target'], test_size=0.33, random_state=0)\n","1d7436a4":"\nimport matplotlib.pyplot as plt\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    See full source and example: \n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","8578c8ce":"from sklearn import metrics\nimport numpy as np\nimport itertools","9a956c38":"from sklearn.naive_bayes import MultinomialNB\nclassifier_count=MultinomialNB()\n\n\n## applied with countVectorizer\nclassifier_count.fit(X_train, y_train)\npred = classifier_count.predict(X_test)\nscore = metrics.accuracy_score(y_test, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = metrics.confusion_matrix(y_test, pred)\nplot_confusion_matrix(cm, classes=['NON_DISASTER', 'DISASTER'])","a7503866":"from sklearn.naive_bayes import MultinomialNB\nclassifier_tfidf=MultinomialNB()\n\n## applied with TfidfVectorizer\nclassifier_tfidf.fit(X_train_tfidf, y_train_tfidf)\npred = classifier_tfidf.predict(X_test_tfidf)\nscore = metrics.accuracy_score(y_test_tfidf, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = metrics.confusion_matrix(y_test_tfidf, pred)\nplot_confusion_matrix(cm, classes=['NON_DISASTER', 'DISASTER'])","8563ceaa":"from sklearn.linear_model import PassiveAggressiveClassifier\nlinear_clf = PassiveAggressiveClassifier()\n\n\n## applied with countVectorizer\nlinear_clf.fit(X_train, y_train)\npred = linear_clf.predict(X_test)\nscore = metrics.accuracy_score(y_test, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = metrics.confusion_matrix(y_test, pred)\nplot_confusion_matrix(cm, classes=['NON_DISASTER', 'DISASTER'])","bd9473ec":"from sklearn.linear_model import PassiveAggressiveClassifier\nlinear_clf = PassiveAggressiveClassifier()\n\n## applied with TfidfVectorizer\nlinear_clf.fit(X_train_tfidf, y_train_tfidf)\npred = linear_clf.predict(X_test_tfidf)\nscore = metrics.accuracy_score(y_test_tfidf, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = metrics.confusion_matrix(y_test, pred)\nplot_confusion_matrix(cm, classes=['NON_DISASTER', 'DISASTER'])","98251834":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","979ba069":"\n## file path = ..\/input\/nlp-getting-started\/sample_submission.csv\nsubmission_file_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\ntest_vectors=test_vectors_tfidf\nsubmission(submission_file_path,classifier_tfidf,test_vectors)","a3ac9512":"We can see there are lot of miisng values  in the location column for both train and test data. Since we only be making use of text column for our analyssi there is no point in imputing the missing values at this point.","4e5ffdf6":"**Number of words and the length of the tweets does not provide any concrete information with respect to the tweet type**","90d3ee9c":"Let's create a WOR CLOUD to check the most frequently occuring words with respect to both Disaster and Non-Disaster tweets","18dc296a":"**From above samples we have validated that our train set contains the valid disaster and non-disaster tweets**","e0c07e10":"#### Distribution of number of characters for each tweet type","c1c50e26":"\n## Passive Aggressive Classifier Algorithm","e2eb9c4d":"#### Distribution of number of words for each tweet type","7f6848c8":"Still we can see our unnecessary words that is stop words that are not useful in classfication task. We can will remove them in our next preprocessing step .","19b56771":"## Submission","3d03b642":"### **MultinomialNB**","27f1a164":"let's check what features does our dataset contains and dimension of our trrain and test dataset","a09dddd6":"Time to explore some meta information for different tweets and see fi we are able to figure out any differences with respect to these properties.","125ce909":"*How disaster and non-disaster tweets look like in our train set (Validating from a sample of tweets)*","385aadd9":"### Exploratory Data Analysis","9dfa96ae":"### Countvectorizer","ecfd9cce":"## Data Preparation (Textual Data)"}}