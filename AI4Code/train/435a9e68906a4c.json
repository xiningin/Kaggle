{"cell_type":{"f108c946":"code","b145d1b0":"code","9e7d9dff":"code","2ba580f0":"code","354af27f":"code","abfa4042":"code","69f3010f":"code","68a8ae30":"code","0d7f4a78":"code","d1119f20":"code","c08cca9b":"code","fde4dbe6":"code","62bea5cb":"code","c0ab3ec1":"code","1d116e18":"code","cf8f5d1e":"markdown","7d5c3c10":"markdown","59a35dc9":"markdown","b1c83af4":"markdown","725810bf":"markdown","6e5d2031":"markdown","7e2e907c":"markdown","507d7184":"markdown","7a8023a9":"markdown","a9f8e3b6":"markdown","fb6d46ce":"markdown","69ba2cd9":"markdown","f84aeaf3":"markdown","5caf8030":"markdown"},"source":{"f108c946":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b145d1b0":"import pandas as pd\nimport pandas_profiling as pdp\nimport numpy as np\nnp.random.seed(2)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nimport keras\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator","9e7d9dff":"!ls ..\/input\/digit-recognizer\ntrain = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nsample_submission = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\n\ny_train = train[\"label\"]\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"],axis = 1) \n\n# free some space\ndel train \ng = sns.countplot(y_train)\ny_train.value_counts()","2ba580f0":"X_train.isnull().any().describe()\ntest.isnull().any().describe() \n#unique:\u7a2e\u985e top:\u6700\u983b\u5024 freq:\u6700\u983b\u5024\u306e\u51fa\u73fe\u56de\u6570","354af27f":"# 0~1\u6b63\u5247\u5316\nX_train = X_train \/ 255.0\ntest = test \/ 255.0\n\n# 28\u00d728 \u306e\u30c1\u30e3\u30cd\u30eb1(grayscale)\u306breshape\n# Reshape:Keras\u306e\u5165\u529b\u30c7\u30fc\u30bf\u306e\u5f62\u5f0f\u306f(\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3001\u6a2a\u5e45\u3001\u7e26\u5e45\u3001\u30c1\u30e3\u30cd\u30eb\u6570)\u3067\u3042\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u3067\u3001reshape()\u3092\u4f7f\u3063\u3066\u5f62\u5f0f\u3092\u5909\u63db\u3059\u308b\u3002\n# 784\u6b21\u5143\u3042\u3063\u305f\u3082\u306e\u309228px*28px\u306b\u3059\u308b\u3002\n# MNIST\u306e\u753b\u50cf\u306f\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u306a\u306e\u3067\u3001\u30c1\u30e3\u30cd\u30eb\u6570\u306f1\u3002RGB\u753b\u50cf\u306e\u5834\u5408\u306f\u30c1\u30e3\u30cd\u30eb\u6570\u306f3\u306a\u306e\u3067784px\u306e\u30d9\u30af\u30c8\u30eb\u309228x28x3\u306e3D\u884c\u5217\u306breshape\u3059\u308b\u3002\nX_train = X_train.values.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)\n\n# 0~9\u306e10\u5206\u5272\u3059\u308b\ny_train = to_categorical(y_train, num_classes=10)\n\n# \u5206\u5272\u3059\u308b\nrandom_seed = 2\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=random_seed)","abfa4042":"#\u753b\u50cf\u78ba\u8a8d\nprint(X_train.shape)\ntest_show = plt.imshow(X_train[4][:,:,0])","69f3010f":"# keras\u3067\u30e2\u30c7\u30eb\u69cb\u7bc9\nmodel = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same', activation='relu', input_shape=(28, 28, 1)))\nmodel.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))","68a8ae30":"# \u30b3\u30f3\u30d1\u30a4\u30eb\u3001\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u7b49\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b9a\u7fa9\u3059\u308b\nmodel.compile(optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), loss='categorical_crossentropy', metrics=['accuracy'])\n# learning rate \u3092\u8a2d\u5b9a\u3059\u308b\nreduce_lr = ReduceLROnPlateau(monitor='val_acc',\n                              patience=3,\n                              verbose=1,\n                              factor=0.5,\n                              min_lr=0.00001)\nepochs = 20\nbatch_size = 86\nearly_stopping = EarlyStopping(patience=0, verbose=1) ","0d7f4a78":"# augmantation.\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","d1119f20":"# \u5b66\u7fd2\n\nhistory = model.fit_generator(\n    datagen.flow(X_train, y_train, batch_size=batch_size),\n    epochs=epochs,\n    validation_data=(X_test, y_test),\n    verbose=1,\n    steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n    callbacks=[reduce_lr]\n)","c08cca9b":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","fde4dbe6":"# Look at confusion matrix \n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\ny_pred = model.predict(X_test)\n# Convert predictions classes to one hot vectors \ny_pred_classes = np.argmax(y_pred,axis = 1) \n# Convert validation observations to one hot vectors\ny_true = np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_true, y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","62bea5cb":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (y_pred_classes - y_true != 0)\n\ny_pred_classes_errors = y_pred_classes[errors]\ny_pred_errors = y_pred[errors]\ny_true_errors = y_true[errors]\nX_test_errors = X_test[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# \u4e88\u6e2c\u3057\u305f\u6570\u5b57\u304c\u9593\u9055\u3063\u3066\u3044\u308b\u78ba\u7387\ny_pred_errors_prob = np.max(y_pred_errors,axis = 1)\n\n# \u8aa4\u5dee\u96c6\u5408\u5185\u306e\u771f\u306e\u5024\u306e\u4e88\u6e2c\u78ba\u7387\ntrue_prob_errors = np.diagonal(np.take(y_pred_errors, y_true_errors, axis=1))\n\n# \u4e88\u6e2c\u3055\u308c\u305f\u30e9\u30d9\u30eb\u3068\u771f\u306e\u30e9\u30d9\u30eb\u306e\u78ba\u7387\u306e\u5dee\ndelta_pred_true_errors = y_pred_errors_prob - true_prob_errors\n\n# the delta prob errors\u3000\u306e sort list\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_test_errors, y_pred_classes_errors,y_true_errors)","c0ab3ec1":"# predict results\nresults = model.predict(test)\n\n# 10\u30e9\u30d9\u30eb\u304b\u3089\u4e00\u756a\u3067\u304b\u3044\u306e\u3092\u9078\u3076\uff08axis=1\u306f\u305f\u304f\u3055\u3093\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u5b58\u5728\u3059\u308b\u70ba\u3001\u4e00\u5c64\u843d\u3068\u3057\u3066\u305d\u308c\u305e\u308c\u306e\u30c7\u30fc\u30bf\u304b\u3089\u898b\u308b\uff09\nresults = np.argmax(results,axis = 1)\n\n#pandas\u3067\u6574\u5f62\nresults = pd.Series(results,name=\"Label\")\n\nresults","1d116e18":"submission = pd.concat([pd.Series(range(1,28001), name=\"ImageId\"), results], axis=1)\nsubmission.to_csv('sample_submission.csv', index=False)","cf8f5d1e":"\u7b2c\uff11\u306f\u3001\u7573\u307f\u8fbc\u307f\uff08Conv2D\uff09\u5c64\u3067\u3042\u308b\u3002\u3053\u308c\u306f\u5b66\u7fd2\u53ef\u80fd\u306a\u30d5\u30a3\u30eb\u30bf\u306e\u30bb\u30c3\u30c8\u306e\u3088\u3046\u306a\u3082\u306e\u3002\u6700\u521d\u306e2\u3064\u306eConv2D\u306b32\u500b\u306e\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u3001\u6700\u5f8c\u306e2\u3064\u306b64\u500b\u306e\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u8a2d\u5b9a\u3002\n\u305d\u308c\u305e\u308c\u306e\u30d5\u30a3\u30eb\u30bf\u306f\u30ab\u30fc\u30cd\u30eb\u30d5\u30a3\u30eb\u30bf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u306e\u4e00\u90e8\uff08\u30ab\u30fc\u30cd\u30eb\u30b5\u30a4\u30ba\u3067\u5b9a\u7fa9\u3055\u308c\u308b\uff09\u3092\u5909\u63db\u3057\u307e\u3059\u3002\u30ab\u30fc\u30cd\u30eb\u30d5\u30a3\u30eb\u30bf\u884c\u5217\u306f\u753b\u50cf\u5168\u4f53\u306b\u9069\u7528\u3055\u308c\u307e\u3059\u3002\u30d5\u30a3\u30eb\u30bf\u306f\u753b\u50cf\u306e\u5909\u63db\u3068\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n<br>\n\nCNN\u306e2\u756a\u76ee\u306e\u91cd\u8981\u306a\u30ec\u30a4\u30e4\u30fc\u306f\u30d7\u30fc\u30ea\u30f3\u30b0\uff08MaxPool2D\uff09\u30ec\u30a4\u30e4\u30fc\u3067\u3059\u3002\u3053\u306e\u5c64\u306f\u5358\u7d14\u306b\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30d5\u30a3\u30eb\u30bf\u3068\u3057\u3066\u6a5f\u80fd\u3057\u307e\u3059\u3002\u96a3\u63a5\u3059\u308b2\u3064\u306e\u30d4\u30af\u30bb\u30eb\u3092\u898b\u3066\u3001\u6700\u5927\u5024\u3092\u9078\u3073\u307e\u3059\u3002\u8a08\u7b97\u30b3\u30b9\u30c8\u3092\u524a\u6e1b\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u3001\u307e\u305f\u3001\u3042\u308b\u7a0b\u5ea6\u30aa\u30fc\u30d0\u30fc\u30d5\u30a3\u30c3\u30c8\u3092\u6e1b\u3089\u3059\u305f\u3081\u306b\u3082\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002\u30d7\u30fc\u30ea\u30f3\u30b0\u30b5\u30a4\u30ba\u3092\u9078\u629e\u3059\u308b\u3002\n<br>\n\nDropout\u306f\u6b63\u5247\u5316\u306e\u624b\u6cd5\u3067\u3042\u308a\u3001\u5b66\u7fd2\u30b5\u30f3\u30d7\u30eb\u3054\u3068\u306b\u5c64\u5185\u306e\u30ce\u30fc\u30c9\u306e\u5272\u5408\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u7121\u8996\u3059\u308b\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e00\u90e8\u306e\u30ce\u30fc\u30c9\u304c\u30e9\u30f3\u30c0\u30e0\u306b\u524a\u9664\u3055\u308c\u3001\u30aa\u30fc\u30d0\u30fc\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3092\u6e1b\u5c11\u3055\u305b\u308b\u3002\n<br>\n\nrelu\u306f\u6d3b\u6027\u5316\u95a2\u6570max(0,x)\u3002\u6d3b\u6027\u5316\u95a2\u6570\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u975e\u7dda\u5f62\u6027\u3092\u4ed8\u52a0\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u308b\u3002\n<br>\n\nFlatten\u306f\u3001\u6700\u7d42\u7684\u306a\u7279\u5fb4\u30de\u30c3\u30d7\u30921\u3064\u306e1\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3002\u3044\u304f\u3064\u304b\u306e\u7573\u307f\u8fbc\u307f\/\u30de\u30af\u30b9\u30d7\u30fc\u30eb\u30ec\u30a4\u30e4\u30fc\u306e\u5f8c\u306b\u5b8c\u5168\u306b\u63a5\u7d9a\u3055\u308c\u305f\u30ec\u30a4\u30e4\u30fc\u3092\u5229\u7528\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u3067\u3059\u3002\u3053\u308c\u306f\u3001\u524d\u306e\u7573\u307f\u8fbc\u307f\u30ec\u30a4\u30e4\u30fc\u306e\u5c40\u6240\u7684\u306a\u7279\u5fb4\u3092\u3059\u3079\u3066\u7d50\u5408\u3057\u307e\u3059\u3002\n<br>\n\n\u6700\u7d42\u7684\u306b\u306f\u30012\u3064\u306e\u5b8c\u5168\u306b\u63a5\u7d9a\u3055\u308c\u305f(Dense)\u5c64\u3067\u7279\u5fb4\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u308c\u306f\u4eba\u5de5\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af(ANN)\u5206\u985e\u5668\u306b\u3059\u304e\u307e\u305b\u3093\u3002\u6700\u5f8c\u306e\u5c64(Dense(10, activation=\"softmax\"))\u3067\u306f\u3001\u5404\u30af\u30e9\u30b9\u306e\u78ba\u7387\u5206\u5e03\u3092\u30cd\u30c3\u30c8\u51fa\u529b\u3057\u3066\u3044\u307e\u3059\u3002","7d5c3c10":"compile:\u5b66\u7fd2\u306e\u8a2d\u5b9a\u3001loss:\u640d\u5931\u95a2\u6570\u3001optimizer:\u6700\u9069\u5316\u624b\u6cd5\u3001metrics:\u6307\u6a19\u624b\u6cd5","59a35dc9":"## 2.2 Check for null and missing values","b1c83af4":"# 2.Date Preparation","725810bf":"## 3.1 Define the Model","6e5d2031":"# 3. CNN","7e2e907c":"# 1.Introduction\ntensorflow\u306ebackend\u3067\u3042\u308bkeras\u3092\u7528\u3044\u308b\u3002","507d7184":"## 4.2 Confusion Matrix","7a8023a9":"# 4.Evaluate the model","a9f8e3b6":"## 2.3 Normalizaiton\n## 2.4 Reshape\n## 2.5 Label encoding\n## 2.6 Split training and test set ","fb6d46ce":"## 3.3 Data augmentation","69ba2cd9":"## 3.2 Set the optimizer and annealer","f84aeaf3":"## 4.1 Trainning amd validation data","5caf8030":"## 2.1 Load Data"}}