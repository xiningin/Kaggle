{"cell_type":{"09da799e":"code","1074d3b7":"code","8f80745c":"code","89a2e1ac":"code","5a11c158":"code","541e4550":"code","cc801def":"code","0f0ed492":"code","b0a445bf":"code","abaf31c0":"code","5e678d3a":"code","d2e3eda4":"code","351a1710":"code","07935842":"code","bf31b3b9":"code","de5a9ba1":"code","3fb08c3d":"code","30270d87":"code","6d1761d9":"code","0b258391":"code","f817cc96":"code","78950c33":"code","92362c9d":"code","cf52935d":"code","4fde5245":"code","d720532b":"code","11ba3dd7":"code","e9b0a2c0":"code","b3942b44":"code","20ddbe77":"code","2f330e5e":"code","8ea08a9b":"code","c3d373fd":"code","c9cbc8b6":"code","f70898ea":"code","d7f5d8e7":"code","85006271":"code","4a422ef9":"code","3b41bcc5":"code","10751d31":"code","2db597d4":"code","6ecec43a":"code","d83f1162":"code","aec8a3c1":"code","7b91edd3":"code","a6750553":"code","eb42e9b4":"code","64e3c2ec":"code","663b7444":"code","91848934":"code","0523e5ee":"code","5360cbd3":"code","4562200b":"code","adc5847d":"code","afebb855":"code","60c0570d":"code","846ad095":"code","794caf49":"code","99c7506b":"code","a0b22b8d":"code","4bbe8153":"code","b1d9eabb":"code","0b93d09b":"code","d8251440":"code","a2102222":"code","a7725cd5":"code","bf001546":"code","d9d3fbc3":"code","3f62b7fc":"code","d1736ac5":"code","c6a7b133":"code","f93eeace":"code","04f19549":"code","6ae1b997":"code","932e00bc":"code","027f1b21":"code","4cb75d1a":"code","e7809c7f":"code","9cd59bf9":"code","e3e49863":"markdown","b88162a1":"markdown","5479ec3f":"markdown","a2a3fd4a":"markdown","335b1f4e":"markdown","f0478427":"markdown","a71cf178":"markdown","2ad3d583":"markdown","e813a2dd":"markdown","8d5995d7":"markdown","0a1a2a8c":"markdown","6ca24ee3":"markdown","ca365529":"markdown","47491f29":"markdown","6dbd1273":"markdown","f325bf12":"markdown","6eef7841":"markdown","e219c571":"markdown","064ef5c7":"markdown","fda3e0ba":"markdown","d9a1099a":"markdown","323f0bab":"markdown","a2a792dc":"markdown","48b57356":"markdown","80ffd61f":"markdown","32743f77":"markdown","a47e643a":"markdown","d8d99473":"markdown","dacc9473":"markdown","76c06ec2":"markdown","67873899":"markdown","308901bd":"markdown","0b7a880e":"markdown","34469708":"markdown","a564cce0":"markdown","932de66a":"markdown","2c2db7bb":"markdown","69fc6ddd":"markdown","97f91d9c":"markdown","570b55ed":"markdown","439dbb37":"markdown","e3e995dc":"markdown","1addc399":"markdown","0ce1ef7b":"markdown","8a1e098b":"markdown","b1dcfcce":"markdown","e01a37f2":"markdown","f943098c":"markdown","e6426b44":"markdown","c1975c11":"markdown","f276b52b":"markdown","f8fd6ab6":"markdown","9b234b28":"markdown","655a74b3":"markdown","1ac99021":"markdown","378748be":"markdown","113f5794":"markdown","e8432b40":"markdown","c5c8407a":"markdown","4a8f7d16":"markdown"},"source":{"09da799e":"# Computational imports\nimport numpy as np   # Library for n-dimensional arrays\nimport pandas as pd  # Library for dataframes (structured data)\n\n# Plotting imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Set seeds to make the experiment more reproducible.\nfrom numpy.random import seed\nseed(1)\n\n# Allows us to see more information regarding the DataFrame\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 500)","1074d3b7":"!pip install openpyxl","8f80745c":"path = '..\/input\/flight-fare-prediction-mh\/Data_Train.xlsx'\ntrain_data = pd.read_excel(path)","89a2e1ac":"train_data.head()","5a11c158":"train_data.info()","541e4550":"train_data[\"Duration\"].value_counts()","cc801def":"train_data.shape","0f0ed492":"train_data.dropna(inplace = True)","b0a445bf":"train_data.shape","abaf31c0":"train_data.isnull().sum()","5e678d3a":"train_data[\"Journey_day\"] = pd.to_datetime(train_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day","d2e3eda4":"train_data[\"Journey_month\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month","351a1710":"train_data.head()","07935842":"train_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)","bf31b3b9":"# Departure time is when a plane leaves the gate. \n# Similar to Date_of_Journey we can extract values from Dep_Time\n\n# Extracting Hours\ntrain_data[\"Dep_hour\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Dep_min\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.minute\n\n# Now we can drop Dep_Time as it is of no use\ntrain_data.drop([\"Dep_Time\"], axis = 1, inplace = True)","de5a9ba1":"train_data.head()","3fb08c3d":"# Arrival time is when the plane pulls up to the gate.\n# Similar to Date_of_Journey we can extract values from Arrival_Time\n\n# Extracting Hours\ntrain_data[\"Arrival_hour\"] = pd.to_datetime(train_data.Arrival_Time).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Arrival_min\"] = pd.to_datetime(train_data.Arrival_Time).dt.minute\n\n# Now we can drop Arrival_Time as it is of no use\ntrain_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)","30270d87":"train_data.head()","6d1761d9":"# Time taken by plane to reach destination is called Duration\n# It is the differnce betwwen Departure Time and Arrival time\n\n\n# Assigning and converting Duration column into list\nduration = list(train_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration","0b258391":"# Adding duration_hours and duration_mins list to train_data dataframe\n\ntrain_data[\"Duration_hours\"] = duration_hours\ntrain_data[\"Duration_mins\"] = duration_mins","f817cc96":"train_data.drop([\"Duration\"], axis = 1, inplace = True)","78950c33":"train_data.head()","92362c9d":"train_data[\"Airline\"].value_counts()","cf52935d":"# From graph we can see that Jet Airways Business have the highest Price.\n# Apart from the first Airline almost all are having similar median\n\n# Airline vs Price\nsns.catplot(y = \"Price\", x = \"Airline\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 6, aspect = 3)\nplt.show()","4fde5245":"Airline = train_data[[\"Airline\"]]\n\nAirline = pd.get_dummies(Airline, drop_first= True) \n\nAirline.head(10)","d720532b":"train_data[\"Source\"].value_counts()","11ba3dd7":"# Source vs Price\n\nsns.catplot(y = \"Price\", x = \"Source\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 4, aspect = 3)\nplt.show()","e9b0a2c0":"# As Source is Nominal Categorical data we will perform OneHotEncoding\n\nSource = train_data[[\"Source\"]]\n\nSource = pd.get_dummies(Source, drop_first= True)\n\nSource.head()","b3942b44":"train_data[\"Destination\"].value_counts()","20ddbe77":"# As Destination is Nominal Categorical data we will perform OneHotEncoding\n\nDestination = train_data[[\"Destination\"]]\n\nDestination = pd.get_dummies(Destination, drop_first = True)\n\nDestination.head()","2f330e5e":"train_data[\"Route\"]","8ea08a9b":"# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\n\ntrain_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)","c3d373fd":"train_data[\"Total_Stops\"].value_counts()","c9cbc8b6":"train_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)","f70898ea":"train_data.head()","d7f5d8e7":"# Concatenate dataframe --> train_data + Airline + Source + Destination\n\ndata_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)","85006271":"data_train.head()","4a422ef9":"data_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","3b41bcc5":"data_train.head()","10751d31":"data_train.shape","2db597d4":"path = \"..\/input\/flight-fare-prediction-mh\/Test_set.xlsx\"\ntest_data = pd.read_excel(path)","6ecec43a":"test_data.head()","d83f1162":"# Preprocessing\n\nprint(\"Test data Info\")\nprint(\"-\"*75)\nprint(test_data.info())\n\nprint()\nprint()\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest_data.dropna(inplace = True)\nprint(test_data.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest_data[\"Journey_day\"] = pd.to_datetime(test_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\ntest_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\ntest_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\ntest_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\ntest_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest_data[\"Duration_hours\"] = duration_hours\ntest_data[\"Duration_mins\"] = duration_mins\ntest_data.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test_data[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test_data[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test_data[\"Source\"].value_counts())\nSource = pd.get_dummies(test_data[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test_data[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test_data + Airline + Source + Destination\ndata_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n\ndata_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", data_test.shape)\n\n","aec8a3c1":"data_test.head()","7b91edd3":"data_train.shape","a6750553":"data_train.columns","eb42e9b4":"X = data_train.loc[:, ['Total_Stops', 'Journey_day', 'Journey_month', 'Dep_hour',\n       'Dep_min', 'Arrival_hour', 'Arrival_min', 'Duration_hours',\n       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n       'Airline_Jet Airways', 'Airline_Jet Airways Business',\n       'Airline_Multiple carriers',\n       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\n       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',\n       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\n       'Destination_Kolkata', 'Destination_New Delhi']]\nX.head()","64e3c2ec":"y = data_train.iloc[:, 1]\ny.head()","663b7444":"# Finds correlation between Independent and dependent attributes\n\nplt.figure(figsize = (18,18))\nsns.heatmap(train_data.corr(), annot = True, cmap = \"RdYlGn\")\n\nplt.show()","91848934":"# Important feature using ExtraTreesRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X, y)","0523e5ee":"print(selection.feature_importances_)","5360cbd3":"#plot graph of feature importances for better visualization\n\nplt.figure(figsize = (12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()\n","4562200b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","adc5847d":"from xgboost import XGBRegressor \nfrom sklearn.ensemble import RandomForestRegressor\n\nreg_rf = RandomForestRegressor()\nxgboost_model = XGBRegressor()\n\nreg_rf.fit(X_train, y_train)\nxgboost_model.fit(X_train, y_train)","afebb855":"y_pred_reg_rf = reg_rf.predict(X_test)\ny_pred_xgboost = xgboost_model.predict(X_test)","60c0570d":"print(reg_rf.score(X_train, y_train))\nprint(xgboost_model.score(X_train, y_train))","846ad095":"print(reg_rf.score(X_test, y_pred_reg_rf))\nprint(xgboost_model.score(X_test, y_pred_xgboost))","794caf49":"sns.distplot(y_test-y_pred_reg_rf)\nplt.show()","99c7506b":"sns.distplot(y_test-y_pred_xgboost)\nplt.show()","a0b22b8d":"plt.scatter(y_test, y_pred_reg_rf, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","4bbe8153":"plt.scatter(y_test, y_pred_xgboost, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","b1d9eabb":"from sklearn import metrics","0b93d09b":"print('RandomForest')\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred_reg_rf))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred_reg_rf))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_reg_rf)))\n\nprint('XGBoost')\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred_xgboost))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred_xgboost))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_xgboost)))","d8251440":"metrics.r2_score(y_test, y_pred_reg_rf)","a2102222":"metrics.r2_score(y_test, y_pred_xgboost)","a7725cd5":"from sklearn.model_selection import RandomizedSearchCV","bf001546":"# Create the random grid for the XGBoost model\n\nparams = {\n \"learning_rate\" : [0.05,0.10,0.15,0.20,0.25,0.30],\n \"max_depth\" : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\": [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n}","d9d3fbc3":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\nxgb_model_tuned = RandomizedSearchCV(estimator = xgboost_model, param_distributions = params, scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","3f62b7fc":"xgb_model_tuned.fit(X_train,y_train)","d1736ac5":"xgb_model_tuned.best_params_","c6a7b133":"prediction = xgb_model_tuned.predict(X_test)","f93eeace":"plt.figure(figsize = (8,8))\nsns.distplot(y_test-prediction)\nplt.show()","04f19549":"plt.figure(figsize = (8,8))\nplt.scatter(y_test, prediction, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","6ae1b997":"# MAE: 1135.7739136142043\n# MSE: 3313395.5274770306\n# RMSE: 1820.2734760131596\n\nprint('MAE:', metrics.mean_absolute_error(y_test, prediction))\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))","932e00bc":"metrics.r2_score(y_test, prediction)","027f1b21":"import pickle\n# open a file, where you ant to store the data\nfile = open('flight_rf.pkl', 'wb') # wb is write and binary mode\n\n# dump information to that file\npickle.dump(xgb_model_tuned, file)","4cb75d1a":"model = open('flight_rf.pkl','rb')\nxgboost = pickle.load(model)","e7809c7f":"y_prediction = xgboost.predict(X_test)","9cd59bf9":"metrics.r2_score(y_test, y_prediction)","e3e49863":"We notice that the scatter plot for the XGBoost model is much tighter.","b88162a1":"---","5479ec3f":"## Hyperparameter Tuning\n\n\n* Choose following method for hyperparameter tuning\n    1. **RandomizedSearchCV** --> Fast\n    2. **GridSearchCV**\n* Assign hyperparameters in form of dictionery\n* Fit the model\n* Check best paramters and best score","a2a3fd4a":"We will use pandas get dummies method to create a one hot encoding for the Airlines Column.\n\nNote that we are selecting the Destination column with [\"Destination\"] brackets. This is so that Airline creates a new DataFrame and not a Series.\n\nDrop_first to true drops the first column of the one hot encoding since we do not need it. If everything else is 0, that means the column dropped is 1. We do not need to explictely have it in the DataFrame","335b1f4e":"The value counts method allows us to get unique values that exist in a specific column. In this case, we will get the unique values and count of the Duration column.","f0478427":"Finally, the MAE and the MSE is lower, but the RMSE is slightly higher.\n\nBy analzing these graphs and metrics, we can actually say that the tuning optimized the model for the better.","a71cf178":"### Metrics to decide on which model to use","2ad3d583":"As you can see, it is extremely easy to save and load a trained model and to use it for future predictions. No need to traine everytime!","e813a2dd":"## Importing dataset\n\n1. Since data is in form of excel file we have to use pandas read_excel to load the data\n2. After loading it is important to check the complete information of data as it can indication many of the hidden infomation such as null values in a column or a row\n3. Check whether any null values are there or not. if it is present then following can be done,\n    1. Imputing data using Imputation method in sklearn\n    2. Filling NaN values with mean, median and mode using fillna() method\n4. Describe data --> which can give statistical analysis","8d5995d7":"Now don't forget to concatenate (merge horizontally in this case) all the one-hot encodings and the DataFrames. ","0a1a2a8c":"We will use pandas get dummies method to create a one hot encoding for the Airlines Column.\n\nNote that we are selecting the Source column with [\"Source\"] brackets. This is so that Airline creates a new DataFrame and not a Series.\n\nDrop_first to true drops the first column of the one hot encoding since we do not need it. If everything else is 0, that means the column dropped is 1. We do not need to explictely have it in the DataFrame","6ca24ee3":"In this column, the categories have a ranking or an order. Non-stop -> 1 Stop -> ...\nThis means we have to use a Ordinal Encoding. We can choose to use LabelEncoder from sklearn, but in this case, we can simply use the pandas replace method to replace the strings with correspoding ordinal encodings.","ca365529":"Let us used our tuned model to predict the Target price and see if it does better than our untuned model.","47491f29":"First Step will be to transform Date_of_Journey column into two different columns representing the day and the month. This makes it easier for the model to grasp and make relations between the input and the output.","6dbd1273":"With this heat map, you can see that the # stops and durations hours are the most correlated to the Target Price. This makes total sense, the more stops and the longer the flight generally entails a higher price range. (There is obviously cases where MANY stops could reduce the price, but in general, more stops and necessary connecting flights entails a higher price due to the need to keep the employees and airplane ready for the remaining connecting flights. More time in air --> More $$$ needed)","f325bf12":"Drop [\"Airline\", \"Source\", \"Destination\"], since they have been one-hot encoded.","6eef7841":"We will use pandas get dummies method to create a one hot encoding for the Airlines Column.\n\nNote that we are selecting the Airline column with [\"Airline\"] brackets. This is so that Airline creates a new DataFrame and not a Series.\n\nDrop_first to true drops the first column of the one hot encoding since we do not need it. If everything else is 0, that means the column dropped is 1. We do not need to explictely have it in the DataFrame","e219c571":"Let's check the shape to see if we removed a row... Indeed we did. ","064ef5c7":"## Feature Selection\n\nFinding out the best feature which will contribute and have good relation with target variable.\nFollowing are some of the feature selection methods,\n\n\n1. <span style=\"color: purple;\">**heatmap**<\/span>\n2. <span style=\"color: purple;\">**feature_importance_**<\/span>\n3. <span style=\"color: purple;\">**SelectKBest**<\/span>","fda3e0ba":"## Fitting model using Random Forest\n\n1. Split dataset into train and test set in order to prediction with respect to X_test\n2. If needed do scaling of data\n    * Scaling is not done here\n3. Import model\n4. Fit the data\n5. Predict with respect to X_test\n6. In regression check **RSME** Score\n7. Plot graph","d9a1099a":"You can also use the ExtraTressRegressor from sklearn which will allows you to easily see what are the important features for the Target Price.","323f0bab":"For example, Airline column here has categrical data that do not really have any order (order meaning there is no ranking between the different categories such as grades)","a2a792dc":"It is useful to use .info() method to quickly have a glance on the general information about the DataFrame. It displays info such as the type of the columnd and also the # of non-null count. In this case there is 10683 entries and for each coloumn we have 10683 non-null count. This means no column has any missing values.","48b57356":"Since we have converted Date_of_Journey column into integers, Now we can drop as it is of no use.","80ffd61f":"From description we can see that Date_of_Journey is a object data type,\\\nTherefore, we have to convert this datatype into timestamp so as to use this column properly for prediction\n\nFor this we require pandas **to_datetime** to convert object data type to datetime dtype.\n\n<span style=\"color: red;\">**.dt.day method will extract only day of that date**<\/span>\\\n<span style=\"color: red;\">**.dt.month method will extract only month of that date**<\/span>","32743f77":"## Final Remarks\nThank you for going through this notebook. Please feel free to show support and comment on the notebooks with advice or improvements. If you found it useful, please let me know as well :)","a47e643a":"Already the dist plot looks MUCH better","d8d99473":"## Save the model to reuse it again","dacc9473":"There's various ways to save the model. We decided to go forward with pickling. It is very easy and straighforward. ","76c06ec2":"---","67873899":"The duration columns is a bit trickier because we cannot directly transform it to a datetime object column. That is because the pd.to_datetime() cannot automaticaly transform `2h 50m` to a datetime object. That is why we will need to do it manually with string operations.","308901bd":"---","0b7a880e":"### Scatter plot of Target and Predicted","34469708":"### Heatmap\nHeatmaps allows you to easily see the correlation between the different features. This can:\n\n1) Allow you to see which features affect the target varaible the most.\n\n2) Allow you to see features that you can remove since they are strongly correlated. Fewer the # features, the better it is for us.","a564cce0":"---","932de66a":"XGBoost outperformed the RandomForest on all three metrics. I decided to proceed with the XGBoost model for the hyperparameter tuning.","2c2db7bb":"# Introduction \n`V1.0.0`\n### Who am I\nJust a fellow Kaggle learner. I was creating this Notebook as practice and thought it could be useful to some others \n### Who is this for\nThis Notebook is for people that learn from examples. Forget the boring lectures and follow along for some fun\/instructive time :)\n### What can I learn here\nYou learn all the basics needed to create a rudimentary XGBoost model with hyperparameter tuning. I go over a multitude of steps with explanations. Hopefully with these building blocks,you can go ahead and build much more complex models.\n\n### Things to remember\n+ Please Upvote\/Like the Notebook so other people can learn from it\n+ Feel free to give any recommendations\/changes. \n+ I will be continuously updating the notebook. Look forward to many more upcoming changes in the future.\n\n### You can also refer to these notebooks that have helped me as well:\n+ https:\/\/www.kaggle.com\/anshigupta01\/flight-price-prediction\n\n+ https:\/\/www.kaggle.com\/mragpavank\/flight-price-prediction\n","69fc6ddd":"For the departure and the arrival time (they are given as hour and minutes),first we transform the column into a datetime object column and then we will make two new features for each and they will represent the hour and the minute. Again, this is so the model has an easy time making a connection between the input and the desired output. Feeding it straight datetime features without seprating them into hours and minutes will create problems and unnecesary issues.","97f91d9c":"## Test set\nFor the test set, it is super easy. You apply everything that you did on the training set, but now on the test set.","570b55ed":"### Analyzing tuned model","439dbb37":"### Distplot of Target and predicted","e3e995dc":"The following code allows us to quickly check if there is any NaN values for each column. In our case, there is no NaN values... Life is good :)","1addc399":"---","0ce1ef7b":"Let's obtain the path and read the excel file with pandas read excel method.","8a1e098b":"## Handling Categorical Data\nSo that the model can understand categorical data, we must transform them in a numerical form. There is various ways to do that. \n\nSome of them categorical data are,\n1. <span style=\"color: blue;\">**Nominal data**<\/span> --> data are not in any order --> <span style=\"color: green;\">**OneHotEncoder**<\/span> is used in this case\n2. <span style=\"color: blue;\">**Ordinal data**<\/span> --> data are in order --> <span style=\"color: green;\">**LabelEncoder**<\/span> is used in this case","b1dcfcce":"Don't forget to remove the Duration column after creating the two new duration feature column (Duration_hours and Duration_mins)","e01a37f2":"---","f943098c":"---","e6426b44":"We notice that the disth plot for the RandomForest seems tigher and a bit less spread. However, notice that the XGBoost graph spreads to only -5000 to 5000 while as the RandomForest spreads to -10000 to 10000","c1975c11":"The best is to plot it using a simple barh plot. As expected from the heatmap, # stops and total duration are the the most important features.","f276b52b":"Analyzing the data, we notiuce that the Additional_Info column has mostly no information. We decide to drop it.\n\nAlso we notice that the Route and the Total_stops are strongly correlated. We chose to keep Total_Stops and drop Route.","f8fd6ab6":"---","9b234b28":"You can print it, but it isn't the pretties.","655a74b3":"We can check the best parameters by accessing the following attribute:","1ac99021":"To take care of Null values, you can choose to drop the row containing null values:","378748be":"We can use the .head() method to obtain the first 5 rows of the DataFrame.","113f5794":"## Exploratory Data Analysis\nIn this section, we will more deeply explore the data and analyse how we can transform them so that they can be used in our XGBoost Model.","e8432b40":"The shape attribute accesses a tuple representing the row and column #. The first number is the row and the second is the column. Note that it is not a function but an attribute (variable for the DataFrame).","c5c8407a":"Even the scatter plot looks much tighter.","4a8f7d16":"# Flight Price Prediction\n---"}}