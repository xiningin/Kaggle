{"cell_type":{"de62e2ae":"code","b8bbe4a5":"code","5c7ebc39":"code","f2b986da":"code","c7befbfa":"code","a03ea91a":"code","29c6159b":"code","fa87afa7":"code","55f7738b":"code","f5dc3231":"code","3f2ecd17":"code","37a9d239":"code","10513e73":"code","13003177":"code","85d5e370":"code","765e381e":"code","19544b21":"code","0bd174fe":"code","1947933a":"code","9757b6bc":"code","a0329a94":"code","a82f800c":"code","6d1f3b81":"code","6f14df01":"code","b0e5e994":"code","81d49765":"code","289b6719":"code","ea12bca9":"code","20c1df8f":"code","1e9a0b27":"code","d88f91a7":"code","9d812bcc":"code","7666f29f":"code","486d0449":"code","b027a212":"code","6c2c182f":"code","8f0f41cc":"code","12442586":"code","cb1ef3dc":"code","07d47d31":"markdown","cfce4980":"markdown","4a7def04":"markdown","a667c9fc":"markdown","742d76c6":"markdown","ef85ca65":"markdown","3f866816":"markdown","16c807e5":"markdown","9a0fbe10":"markdown","b9e51256":"markdown","b9811173":"markdown","4b946278":"markdown","090116c6":"markdown","492d71ea":"markdown","981d3aee":"markdown","5a4496e7":"markdown","eec163e3":"markdown","3abe7f34":"markdown"},"source":{"de62e2ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b8bbe4a5":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n%matplotlib inline","5c7ebc39":"data = pd.read_csv(\"..\/input\/creditcard.csv\")\ndata.head()","f2b986da":"sns.countplot(x='Class', data=data)","c7befbfa":"# Standardizing the amount column\nfrom sklearn.preprocessing import StandardScaler\n\ndata['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n\ndata = data.drop(['Time', 'Amount'], axis=1)\ndata.head()","a03ea91a":"import random\n\nX = data.iloc[:, data.columns != 'Class']\ny = data.iloc[:, data.columns == 'Class']\n\nfraud_instances = data[data['Class']==1]['Class'].count()\nsampled_non_fraud_data = data[data.Class == 0].iloc[random.sample(list(data[data.Class == 0].index), fraud_instances)]","29c6159b":"fraud_data = data[data.Class == 1]\nsampled_data = pd.concat([sampled_non_fraud_data, fraud_data])\n\nsampled_data = sampled_data.sample(frac=1)\nX_sampled = sampled_data.iloc[:, sampled_data.columns != 'Class']\ny_sampled = sampled_data.iloc[:, sampled_data.columns == 'Class']\n\n# Showing ratio\nprint(\"Percentage of normal transactions: \", len(sampled_data[sampled_data.Class == 0])\/len(sampled_data))\nprint(\"Percentage of fraud transactions: \", len(sampled_data[sampled_data.Class == 1])\/len(sampled_data))\nprint(\"Total number of transactions in resampled data: \", len(sampled_data))","fa87afa7":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n\nprint(\"Number transactions train dataset: \", len(X_train))\nprint(\"Number transactions test dataset: \", len(X_test))\nprint(\"Total number of transactions: \", len(X_train)+len(X_test))\n\n# Undersampled dataset\nX_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_sampled\n                                                                                                   ,y_sampled\n                                                                                                   ,test_size = 0.3\n                                                                                                   ,random_state = 0)\nprint(\"\")\nprint(\"Number transactions train dataset: \", len(X_train_undersample))\nprint(\"Number transactions test dataset: \", len(X_test_undersample))\nprint(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))","55f7738b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report, precision_score ","f5dc3231":"def printing_Kfold_scores(x_train_data,y_train_data):\n    fold = KFold(n_splits=5,shuffle=False) \n\n    # Different C parameters\n    c_param_range = [0.01,0.1,1,10,100]\n\n    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n    results_table['C_parameter'] = c_param_range\n    j = 0\n    for c_param in c_param_range:\n        print('-------------------------------------------')\n        print('C parameter: ', c_param)\n        print('-------------------------------------------')\n        print('')\n\n        recall_accs = []\n        for iteration, (train_indices, val_indices) in enumerate(fold.split(y_train_data)):\n            \n            # Train the logistic regression with multiple c parameter\n            lr = LogisticRegression(C=c_param, penalty='l1')\n            lr.fit(x_train_data.iloc[train_indices, :], y_train_data.iloc[train_indices, :])\n            \n            y_predict_undersample = lr.predict(x_train_data.iloc[val_indices])\n            \n            recall_acc = recall_score(y_train_data.iloc[val_indices], y_predict_undersample)\n            recall_accs.append(recall_acc)\n            print('Iteration ', iteration,': recall score = ', recall_acc)\n        results_table.loc[j, 'Mean recall score'] = np.mean(recall_accs)\n        j += 1\n        print('')\n        print('Mean recall score ', np.mean(recall_accs))\n        print('')\n    return results_table\n        ","3f2ecd17":"param_df = printing_Kfold_scores(X_train_undersample,y_train_undersample)","37a9d239":"param_df","10513e73":"optimum_c = param_df[param_df['Mean recall score'] == param_df['Mean recall score'].max()].C_parameter","13003177":"optimum_c","85d5e370":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        1#print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","765e381e":"lr = LogisticRegression(C=float(optimum_c), penalty='l1')\nlr.fit(X_train_undersample, y_train_undersample)\n\ny_pred_undersample = lr.predict(X_test_undersample)\n\ncnf = confusion_matrix(y_test_undersample, y_pred_undersample)\nnp.printoptions(precision=2)\nprint(\"Recall metric in the testing dataset: \", cnf[1,1]\/(cnf[1,0]+cnf[1,1]))","19544b21":"# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","0bd174fe":"lr = LogisticRegression(C=float(optimum_c))\nlr.fit(X_train_undersample, y_train_undersample)\npreds_on_whole = lr.predict(X_test)\n\ncnf = confusion_matrix(y_test, preds_on_whole)\nprint(\"Recall: {}\".format(cnf[1][1]\/(cnf[1][1]+cnf[1][0])))","1947933a":"class_names = [0, 1]\nplt.figure()\nplot_confusion_matrix(cnf, class_names, title='Confusion Matrix')","9757b6bc":"preds_on_whole = lr.predict(X)\n\ncnf = confusion_matrix(y, preds_on_whole)\nprint(\"Recall: {}\".format(cnf[1][1]\/(cnf[1][1]+cnf[1][0])))","a0329a94":"class_names = [0, 1]\nplt.figure()\nplot_confusion_matrix(cnf, class_names, title='Confusion Matrix')","a82f800c":"lr = LogisticRegression(C=float(optimum_c), penalty='l1')\ny_pred_undersample_score = lr.fit(X_train_undersample, y_train_undersample).decision_function(X_test_undersample)\nfpr, tpr, thresholds = roc_curve(y_test_undersample.values, y_pred_undersample_score)\nroc_auc = auc(fpr, tpr)","6d1f3b81":"plt.title(\"TPR vs FPR of the data\")\nplt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","6f14df01":"best_c = printing_Kfold_scores(X_train,y_train)","b0e5e994":"lr = LogisticRegression(C = best_c.iloc[0, 0], penalty = 'l1')\nlr.fit(X_train,y_train)\ny_pred_undersample = lr.predict(X_test)\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test,y_pred_undersample)\nnp.set_printoptions(precision=2)\n\nprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]\/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","81d49765":"lr = LogisticRegression(C=0.01, penalty='l1')\nlr.fit(X_train_undersample, y_train_undersample)\n\npred_prob = lr.predict_proba(X_test_undersample)\n\nthresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n\nplt.figure(figsize=(10, 10))\n\nj=1\nfor i in thresholds:\n    y_test_prediction_recall = pred_prob[:, 1] > i\n    plt.subplot(3, 3, j)\n    \n    j += 1\n    \n    cnf = confusion_matrix(y_test_undersample, y_test_prediction_recall)\n    np.set_printoptions(precision=2)\n\n    print(\"Recall metric in the testing dataset: \", cnf[1,1]\/(cnf[1,0]+cnf[1,1]))\n\n    # Plot non-normalized confusion matrix\n    class_names = [0,1]\n    plot_confusion_matrix(cnf\n                          , classes=class_names\n                          , title='Threshold >= %s'%i)","289b6719":"from itertools import cycle\n\nlr = LogisticRegression(C = 0.01, penalty = 'l1')\nlr.fit(X_train_undersample,y_train_undersample.values.ravel())\ny_pred_undersample_proba = lr.predict_proba(X_test_undersample.values)\n\nthresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\ncolors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'red', 'yellow', 'green', 'blue','black'])\n\nplt.figure(figsize=(5,5))\n\nfor i,color in zip(thresholds,colors):\n    y_test_predictions_prob = y_pred_undersample_proba[:,1] > i\n    \n    precision, recall, thresholds = precision_recall_curve(y_test_undersample,y_test_predictions_prob)\n    print(precision, recall, thresholds)\n    \n    # Plot Precision-Recall curve\n    plt.plot(recall, precision, color=color,\n                 label='Threshold: %s'%i)\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    plt.title('Precision-Recall example')\n    plt.legend(loc=\"lower left\")","ea12bca9":"print(\"X Train Shape : {}\".format(X_train.shape))\nprint(\"y Train Shape : {}\".format(y_train.shape))\n\nprint(\"X Test Shape : {}\".format(X_test.shape))\nprint(\"Y Test Shape : {}\".format(y_test.shape))\n","20c1df8f":"from collections import Counter\n\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=42)\nX_resample, y_resample = sm.fit_resample(X_train, y_train)\n\nprint(\"Y Size: {}\".format(Counter(y_resample)))\nprint(\"X Size: {}\".format(X_resample.shape))","1e9a0b27":"lr = LogisticRegression(C=0.01, penalty='l1')\n\nlr.fit(X_resample, y_resample)\npredictions = lr.predict(X_test)\n\ncnf = confusion_matrix(y_test, predictions)\n\nplot_confusion_matrix(cnf, [0, 1])","d88f91a7":"print(\"Recall: {}\".format(recall_score(y_test, predictions)))\nprint(\"Precision: {}\".format(precision_score(y_test, predictions)))","9d812bcc":"from sklearn.model_selection import GridSearchCV","7666f29f":"params = {\n          'C': np.linspace(.01, 10),\n          'penalty': ['l1', 'l2']\n         }\n\nlr = LogisticRegression()\nclf = GridSearchCV(lr, params, cv=5, verbose=5, n_jobs=3)\nclf.fit(X_sampled, y_sampled)","486d0449":"clf.best_params_","b027a212":"lr = LogisticRegression(C=2, penalty='l1')\n\nlr.fit(X_resample, y_resample)\npredictions = lr.predict(X_test)\n\ncnf = confusion_matrix(y_test, predictions)\n\nplot_confusion_matrix(cnf, [0, 1])\n\nprint(\"Recall: {}\".format(recall_score(y_test, predictions)))\nprint(\"Precision: {}\".format(precision_score(y_test, predictions)))","6c2c182f":"from sklearn.ensemble import RandomForestClassifier","8f0f41cc":"rf = RandomForestClassifier()\n\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\nclf = GridSearchCV(rf, param_grid, cv=5, verbose=5, n_jobs=10)\nclf.fit(X_sampled, y_sampled)","12442586":"optimum_params = dict(clf.best_params_)","cb1ef3dc":"lr = RandomForestClassifier(criterion= 'entropy',max_depth= 7,max_features='auto',n_estimators= 200)\n\nlr.fit(X_sampled, y_sampled)\npredictions = lr.predict(X_test)\n\ncnf = confusion_matrix(y_test, predictions)\n\nplot_confusion_matrix(cnf, [0, 1])\n\nprint(\"Recall: {}\".format(recall_score(y_test, predictions)))\nprint(\"Precision: {}\".format(precision_score(y_test, predictions)))","07d47d31":"# Making Predictions on the whole of Test Dataset.","cfce4980":"# Undersampling the data","4a7def04":"We get a 93% recall\non the undersampled dataset.","a667c9fc":"# Splitting data into train and test set.","742d76c6":"# Logistic Regression Classifier on the Skewed Data\n\nWe'll do the KFold validation on the entire dataset and see what all inconsistencies creep up.","ef85ca65":"# SMOTE on the Imbalanced Data","3f866816":"# Logistic Regression with KFold Cross Validation\n","16c807e5":"The metric is plotted by changin the thresholds of the probability predictions. We then plot the values TPR vs FPR. We can see that as FPR increases we keep on classifying non-frauds as frauds but our true-positive rates keep on increasing. We need to do make a trade-off of how much misclassifying of non-fraud as fraud we can sacrifice to increase the recall.","9a0fbe10":"# Changing Classification Threshold\nIf we use predict_prob() instead of the predict() method, we'll get the probability of the particular classification. If we use a threshold to control the probability, we can control the precision and the recall.","b9e51256":"# Whole dataset","b9811173":"Use the best param to predict the values on the test set","4b946278":"# Kernel Summary\n\nBasically there is no feature engineering to be done as the features are given as PCA of the original feature. The only important thing to note here is that we need to sample the data to have equal number of both the classes.\n\n* Working with skewed datasets with disproportionate classes\n* Different types of sampling undersampling, oversampling, SMOTE\n* Confusion Matrix and applications - plotting the matrix using imshow\n* Metrics to use : Recall, TPR, FPR, AOC Curve\n* Precision Vs. Recall Trade-off\n* Training on disproportionate datasets and doing KFold for recall\n","090116c6":"# ROC Curve","492d71ea":"# Grid Searching for Best Params (Logistic Regression)","981d3aee":"# Random Forest","5a4496e7":"# Setting input and target variable with resampling","eec163e3":"* TPR = True positive rate: That is the recall value. The number of positive classification divided by the true number of postive classification. In this scenario it would be the number of correctly classified fraud cases by the sum of correctly classified fraud cases and the incorrectly labeled fraud cases\n* FPR = False Postive rate: The number of not fraud by the sum of the number of not fraud and the incorrectly classified not fraud.\n* AUC = is the area under the curve of TPR vs FPR","3abe7f34":"Since the data is highly disproportionate, even just simply predicting the score would give us a high accuracy score. This can be solved by changing the metric of performance\n* Confusion Matrix\n* F1 Score \n* ROC Curve\n* Kappa\n\nResampling data\n* Processes data to have 50-50 ratio\n* Undersampling \n* Oversampling\n"}}