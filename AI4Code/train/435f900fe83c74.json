{"cell_type":{"79685dc1":"code","f71fb815":"code","64d4bd0b":"code","bf2770d9":"code","56a7e857":"code","53770eaa":"code","9a3ac2c9":"code","83c3482d":"code","503d9da3":"code","b3fabdf7":"code","06469c69":"code","c1a9e1be":"code","b07bcd66":"code","042bada4":"code","19557124":"markdown"},"source":{"79685dc1":"!pip install python-docx \n!pip install nltk","f71fb815":"# Importing The Required Libraries\n\nimport docx\nimport numpy as np\nimport collections\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\nfrom matplotlib import rcParams\nfrom nltk.corpus import stopwords  \nfrom nltk.tokenize import word_tokenize  \nfrom wordcloud import WordCloud, STOPWORDS\n\n%matplotlib inline","64d4bd0b":"# Convert into a Python List\ndoc = docx.Document('..\/input\/budget-speech\/budget_speech.docx')\ndata = [p.text for p in doc.paragraphs if p.text]   ","bf2770d9":"# Lower Casing\n\ndata1 = list(map(lambda x:x.lower(),data))","56a7e857":"# Split Scentences Into Words\n\ndata2 = [word for line in data1 for word in line.split()]","53770eaa":"# Remove Stopwords\n  \nstop_words_list = set(stopwords.words('english'))  \n  \n# word_tokens = word_tokenize(example_sent)  \n  \ndata_stop_wpords = [w for w in data2 if not w in stop_words_list]  \n  \ndata3 = []  \n  \nfor w in data2:  \n    if w not in stop_words_list:  \n        data3.append(w) ","9a3ac2c9":"# Frequent Words\n\nprint(Counter(word for word in data3).most_common(20))","83c3482d":"# Drop Unuseful Frequent Words from Top 20 Most Frequent Words: also, would, provide, one\n\ndrop_words = [\"also\", \"would\", \"provide\", \"one\"]\n\ndata4 = [word for word in data3 if word not in drop_words]","503d9da3":"# Rare Words\n\nprint(Counter(word for word in data4).most_common()[-20:])","b3fabdf7":"import nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n# wordnet_map = [(\"N\", wordnet.NOUN), \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV]\npos_tagged_text = nltk.pos_tag(data4)\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n\n\ndata5 = []\n\nfor word,pos in pos_tagged_text:\n    a = lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN))\n    data5.append(a)\n\n# def lemmatize_words(data5):\n#     pos_tagged_text = nltk.pos_tag(text.split())\n#     return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\n# df[\"text_lemmatized\"] = df[\"text\"].apply(lambda text: lemmatize_words(text))\n# df.head()","06469c69":"def plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(40, 30))\n    # Display image\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");","c1a9e1be":"data6 = (\" \").join(data5)\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', \n                      colormap='Pastel1', collocations=False).generate(data6)\nplot_cloud(wordcloud)","b07bcd66":"# Frequent Words\ncounted_words = collections.Counter(data5)\n\nwords = []\ncounts = []\nfor word, count in counted_words.most_common(10):\n    words.append(word)\n    counts.append(count)\n    \nprint(words,counts)","042bada4":"colors = cm.rainbow(np.linspace(0, 1, 10))\nrcParams['figure.figsize'] = 20, 8\n\nfig, ax = plt.subplots()\nax.tick_params(axis='y', which='major', labelsize=20)\nax.tick_params(axis='y', which='minor', labelsize=30)\n\nplt.title('Top words from the Budget Speech vs Frequency', fontsize=25)\nplt.xlabel('Frequency', fontsize=20)\nplt.ylabel('Words', fontsize=20)\nplt.barh(words, counts, color=colors)\nplt.show()","19557124":"ToDo: Sentiment Analysis"}}