{"cell_type":{"ee4cfac9":"code","1b93a7b9":"code","8c408876":"code","881b654a":"code","08732c42":"code","55f9d986":"code","7b09bc50":"code","5abb26a3":"code","c5b98aa2":"code","cbff5bb2":"code","7705f8cb":"code","1485f72a":"code","e4d90f67":"code","044889c3":"code","c1b101a8":"code","d19285b5":"code","f1510316":"code","5363ca87":"code","5362dbe0":"code","817b41ac":"code","c8c9e92c":"code","d01dbe62":"code","734db0a7":"code","15f1fe6d":"code","dd5e9552":"code","d1760dab":"markdown","6455a078":"markdown","cbedb0e2":"markdown","1cdca015":"markdown","f561b262":"markdown","f393fd32":"markdown","98d6df14":"markdown","a2f79762":"markdown"},"source":{"ee4cfac9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport time\n\nimport sklearn\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\n# scipy sparse matrix\nimport scipy\nfrom scipy.sparse import hstack\n\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nfrom nltk.stem.snowball import SnowballStemmer\nwnl = SnowballStemmer('english')\n\nimport collections\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b93a7b9":"# train_df = pd.read_csv('..\/input\/parsed-dmoz-dataset\/train_50000.csv', na_filter=False)\n# test_df = pd.read_csv('..\/input\/parsed-dmoz-dataset\/test_50000.csv', na_filter=False)\n\ndf = pd.read_csv('\/kaggle\/input\/parsed-dmoz-dataset\/fw_bw_max_match.csv', converters={'maxmatch': eval}, na_filter=False)\n# df.columns = [\"index\", \"url\", \"label\"]\ndf['maxmatch'] = list(map(lambda x: ' '.join(x), df['maxmatch']))\ndf['url'] = df['domain'].str.cat(df['domain_suffix'], sep=' ')\ndf['url'] = df['url'].str.cat(df['maxmatch'], sep=' ')\ndf['url'] = df['url'].str.cat(df['uri'], sep=' ')\ndf['label'] = df['class']\ndf.head(5)","8c408876":"train_df = df.sample(frac=0.8,random_state=200)[:50000] #random state is a seed value\ntest_df = df.drop(train_df.index).sample(frac=0.8,random_state=200)[:10000]","881b654a":"test_df.head(5)","08732c42":"unique_labels = set(train_df['label'])\nlabel2idx = {k: i for i, k in enumerate(unique_labels)}\n\nlabel2idx = {k: 0 for k, i in label2idx.items()}\nlabel2idx['Adult'] = 1\n\nidx2label = {i: k for k, i in label2idx.items()}\n\nidx2label[0] = 'Normal'","55f9d986":"print(label2idx, idx2label, sep='\\n')","7b09bc50":"def preprocess_on_sentences(data):\n    data['_s'] = '<cls> <s> '\n    data['_e'] = ' <\/s>'\n    sentences = data['_s'].str.cat(data['url'], sep=' ', na_rep='?')\n#     sentences = sentences.str.cat(data['uri'], sep=' ', na_rep='?')\n    sentences = sentences.str.cat(data['_e'], sep=' ')\n    sentences = sentences.tolist()\n    return sentences\n\ntrain_urls = preprocess_on_sentences(train_df)\ntrain_labels = train_df['label'].map(label2idx).to_list()\n\ntest_urls = preprocess_on_sentences(test_df)\ntest_labels = test_df['label'].map(label2idx).to_list()","5abb26a3":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.utils.data.sampler import RandomSampler, SequentialSampler, WeightedRandomSampler\n\nfrom transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","c5b98aa2":"tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')","cbff5bb2":"train_encodings = tokenizer(train_urls, return_tensors='pt', padding=True, truncation=True, max_length=80)\ntest_encodings = tokenizer(test_urls, return_tensors='pt', padding=True, truncation=True, max_length=80)","7705f8cb":"class Toydataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.encodings.input_ids[idx]), torch.tensor(self.labels[idx])\n\n    def __len__(self):\n        return len(self.labels)","1485f72a":"train_dataset = Toydataset(train_encodings, train_labels)\ntest_dataset = Toydataset(test_encodings, test_labels)","e4d90f67":"counter = collections.Counter(list(map(lambda x: int(x[1]), train_dataset)))\nweights = [100.\/counter[label] for label in list(map(lambda x: int(x[1]), train_dataset))]\n\nsampler = WeightedRandomSampler(torch.DoubleTensor(weights), len(weights), replacement=True)","044889c3":"train_dataloader = DataLoader(train_dataset, batch_size=64, sampler=sampler)\ntest_dataloader = DataLoader(test_dataset, batch_size=32)","c1b101a8":"model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=len(idx2label))\n# model = XLNetForSequenceClassification.from_pretrained('..\/input\/url-classification-without-webpage-xlnet\/xlnet_e7.model')","d19285b5":"_ = model.to(device)","f1510316":"train_losses, test_losses = [], []\ntrain_accs, test_accs = [], []\n\nlrs = [1e-5, 1e-5, 1e-6, 1e-6, 1e-6, 5e-7, 5e-7, 1e-7]\nepoch = len(lrs)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, sampler=sampler)\n\nfor e_ in range(epoch):\n    model.train()\n    optim = AdamW(model.parameters(), lr=lrs[e_])\n    y_pred, y_true = [], []\n    correct, loss_total = 0, 0\n    for step, batch in enumerate(train_dataloader):\n        optim.zero_grad()\n        batch = tuple(t.to(device) for t in batch)\n        sent, labels = batch\n        outputs = model(sent, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n        \n        loss_total += loss\n        \n        loss.backward()\n        optim.step()\n        \n        pred = logits.argmax(dim=1)\n        correct += torch.sum(pred == labels)\n        y_pred.extend(pred.tolist())\n        y_true.extend(labels.tolist())\n        \n        if step % 200 == 0:\n            print(f'{e_+1} - {step}\/{len(train_dataloader)}, loss: {loss_total \/ step}')\n        \n    print(f'\\nEpoch {e_+1}, Avg loss: {loss_total \/ len(train_dataloader)}\\n')\n    model.save_pretrained(f'.\/xlnet_e{e_+1}.model')\n    \n    train_losses.append((loss_total \/ len(train_dataloader)).item())\n    train_accs.append(f1_score(y_true, y_pred, average='macro'))\n    \n    print(classification_report(y_true, y_pred))\n    \n    model.eval()\n    y_pred, y_true = [], []\n    correct, loss_total = 0, 0\n    for batch in tqdm(test_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        sent, labels = batch\n        with torch.no_grad():\n            outputs = model(sent, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n        \n        loss_total += loss\n        pred = logits.argmax(dim=1)\n        correct += torch.sum(pred == labels)\n        y_pred.extend(pred.tolist())\n        y_true.extend(labels.tolist())\n    \n    test_losses.append((loss_total \/ len(test_dataloader)).item())\n    test_accs.append(f1_score(y_true, y_pred, average='macro'))\n    print(classification_report(y_true, y_pred))","5363ca87":"plt.plot(train_losses)\nplt.plot(test_losses)\nplt.show()","5362dbe0":"plt.plot(train_accs)\nplt.plot(test_accs)\nplt.show()","817b41ac":"print(train_losses)\nprint(test_losses)\nprint(train_accs)\nprint(test_accs)","c8c9e92c":"model.eval()\ny_pred, y_true = [], []\ncorrect, loss_total = 0, 0\nfor batch in tqdm(test_dataloader):\n    batch = tuple(t.to(device) for t in batch)\n    sent, labels = batch\n    with torch.no_grad():\n        outputs = model(sent, labels=labels)\n    loss = outputs.loss\n    logits = outputs.logits\n\n    loss_total += loss\n    pred = logits.argmax(dim=1)\n    correct += torch.sum(pred == labels)\n    y_pred.extend(pred.tolist())\n    y_true.extend(labels.tolist())","d01dbe62":"print(classification_report(y_true, y_pred))","734db0a7":"sklearn.metrics.roc_auc_score(y_true, y_pred)","15f1fe6d":"# test_dataloader = DataLoader(test_dataset, batch_size=16)\n\n# model.eval()\n# y_pred, y_true = [], []\n# correct, loss_total = 0, 0\n# for batch in tqdm(test_dataloader):\n#     input_ids = batch['input_ids'].to(device)\n#     attention_mask = batch['attention_mask'].to(device)\n#     labels = batch['labels'].to(device)\n#     outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n#     loss, logits = outputs.loss, outputs.logits\n#     loss_total += loss\n#     pred = logits.argmax(dim=1)\n#     correct += torch.sum(pred == labels)\n#     y_pred.extend(pred.tolist())\n#     y_true.extend(labels.tolist())\n    \n# print(f'[TEST] Accuracy {correct \/ len(test_dataloader)}, loss: {loss_total \/ len(test_dataloader)}')","dd5e9552":"# print(classification_report(y_true, y_pred))","d1760dab":"# Import pretrained XLNet","6455a078":"# And special tokens","cbedb0e2":"## Dealing with label\n\nLabels are not integer, so we need to transform it.","1cdca015":"# Classification Report","f561b262":"# Make it a dataset","f393fd32":"# Reading Data","98d6df14":"# Bert Tokenizer","a2f79762":"# Model"}}