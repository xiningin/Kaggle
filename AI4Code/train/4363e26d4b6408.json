{"cell_type":{"be55fe3f":"code","ffcf9c2a":"code","81202363":"code","6f87f6ce":"code","6241575f":"code","e5251b08":"code","018fd15b":"code","85010fec":"code","8785cd88":"code","faff3196":"code","80acdead":"code","902478d0":"code","7e4956da":"code","17236a98":"code","e646e119":"code","bb1f1311":"code","adfd53f4":"code","6d308a3d":"code","91d1e7fa":"code","9af1614e":"code","0a8735de":"code","8ecd60b7":"code","6d0eada5":"markdown","fb5ceeab":"markdown","927e0710":"markdown","ff3e640e":"markdown"},"source":{"be55fe3f":"import warnings\nwarnings. filterwarnings('ignore')","ffcf9c2a":"!pip install transformers\n# !pip install wandb -q","81202363":"VERSION = \"nightly\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version $VERSION","6f87f6ce":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nimport re\nimport string\nimport gc\nimport unicodedata\nimport os\nimport time\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n\n# Importing the T5 modules from huggingface\/transformers\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration","6241575f":"# Checking out the GPU we have access to. This is output is from the google colab version. \n# !nvidia-smi","e5251b08":"# # Setting up the device for GPU usage\n# from torch import cuda\n# device = 'cuda' if cuda.is_available() else 'cpu'\n\n# Preparing for TPU usage\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils","018fd15b":"# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n\nclass CustomDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, source_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.source_len = source_len\n        self.question = self.data.question\n        self.answer = self.data.answer\n\n    def __len__(self):\n        return len(self.question)\n\n    def __getitem__(self, index):\n        question = str(self.question[index])\n        question = 'nq question: '+' '.join(question.split())\n        answer = ' <sep> '.join(self.answer[index]) + \" <\/s>\"\n        answer = ' '.join(answer.split())\n\n        # print(question,\":\",answer)\n        \n        source = self.tokenizer.batch_encode_plus(\n            [question], \n            max_length= self.source_len,\n            add_special_tokens=True,\n            return_special_tokens_mask=True,\n            truncation=True, \n            pad_to_max_length=True,\n            return_tensors='pt')\n        \n        target = self.tokenizer.batch_encode_plus(\n            [answer], \n            max_length= self.source_len, \n            add_special_tokens=True,\n            return_special_tokens_mask=True,\n            truncation=True,\n            pad_to_max_length=True,\n            return_tensors='pt')\n        \n        source_ids = source['input_ids'].squeeze()\n        source_mask = source['attention_mask'].squeeze()\n        target_ids = target['input_ids'].squeeze()\n        target_mask = target['attention_mask'].squeeze()\n\n        return {\n            'source_ids': source_ids.to(dtype=torch.long), \n            'source_mask': source_mask.to(dtype=torch.long), \n            'target_ids': target_ids.to(dtype=torch.long),\n            'target_ids_y': target_ids.to(dtype=torch.long)\n        }","85010fec":"# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n\ndef train(epoch, tokenizer, model, device, loader, optimizer):\n    ## Trains\n    train_start = time.time()\n    model.train()\n    para_train_loader = pl.ParallelLoader(loader, [device]).per_device_loader(device)\n    for _,data in enumerate(para_train_loader, 0):\n        y = data['target_ids'].to(device, dtype = torch.long)\n        y_ids = y[:, :-1].contiguous()\n        lm_labels = y[:, 1:].clone().detach()\n        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n        ids = data['source_ids'].to(device, dtype = torch.long)\n        mask = data['source_mask'].to(device, dtype = torch.long)\n        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n        loss = outputs[0]\n        if _%500==0:\n            # master_print will only print once (not from all 8 cores)\n            print('[xla:{}], Loss={:.3f}'.format(\n                    xm.get_ordinal(), loss.item()))\n            xm.master_print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n#         optimizer.step()\n        xm.optimizer_step(optimizer)\n        # xm.mark_step()\n    elapsed_train_time = time.time() - train_start\n    xm.master_print(\"finished training. Train time was:\", elapsed_train_time) ","8785cd88":"def validate(epoch, tokenizer, model, device, loader):\n    \n    valid_start = time.time()\n    model.eval()\n    predictions = []\n    actuals = []\n    with torch.no_grad():\n        para_train_loader = pl.ParallelLoader(loader, [device]).per_device_loader(device)\n        for _, data in enumerate(para_train_loader, 0):\n            y = data['target_ids'].to(device, dtype = torch.long)\n            ids = data['source_ids'].to(device, dtype = torch.long)\n            mask = data['source_mask'].to(device, dtype = torch.long)\n                \n            generated_ids = model.generate(\n                input_ids = ids,\n                attention_mask = mask, \n                max_length=150, \n                repetition_penalty=2.5, \n                length_penalty=1.0, \n                early_stopping=True\n                )\n            \n            preds = [tokenizer.decode(g) for g in generated_ids]\n            target = [tokenizer.decode(t) for t in y]\n            \n            if _%100==0:\n                xm.master_print(f'Completed {_}')\n\n            predictions.extend(preds)\n            actuals.extend(target)\n            \n    elapsed_valid_time = time.time() - valid_start\n    xm.master_print(\"finished Valid. Train time was:\", elapsed_valid_time)\n            \n    return predictions, actuals","faff3196":"def map_fn(index, flags):\n\n    # Set random seeds and deterministic pytorch for reproducibility\n    torch.manual_seed(flags['seed']) # pytorch random seed\n    np.random.seed(flags['seed']) # numpy random seed\n#     torch.backends.cudnn.deterministic = True\n\n    device = xm.xla_device()\n    \n    if not xm.is_master_ordinal():\n        xm.rendezvous('download_only_once')\n    \n    # tokenzier for encoding the text\n    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", eos_token='<\/s>', sep_token='<sep>')\n    \n    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n    # Further this model is sent to device (GPU\/TPU) for using the hardware.\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n    \n    if xm.is_master_ordinal():\n        xm.rendezvous('download_only_once')\n\n    # Creation of Dataset and Dataloader\n    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n    train_size = 0.95\n    train_df = pd.read_json(\"\/kaggle\/input\/naturalqa\/nq-open-master\/NQ-open.train.jsonl\", orient='columns', lines=True)\n    train_df = train_df[:4000]\n    train_dataset=train_df.sample(frac=train_size, random_state = flags['seed']).reset_index(drop=True)\n    val_dataset=train_df.drop(train_dataset.index).reset_index(drop=True)\n    val_dataset = train_df[:50]\n    xm.master_print(\"FULL Dataset: {}\".format(train_df.shape))\n    xm.master_print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n    xm.master_print(\"TEST Dataset: {}\".format(val_dataset.shape))\n    \n    # Creating the Training and Validation dataset for further creation of Dataloader\n    training_set = CustomDataset(train_dataset, tokenizer, flags['max_len'])\n    val_set = CustomDataset(val_dataset, tokenizer, flags['max_len'])\n    \n    # defining data samplers and loaders \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          training_set,\n          num_replicas=xm.xrt_world_size(), # tell PyTorch how many devices (TPU cores) we are using for training\n          rank=xm.get_ordinal(), # tell PyTorch which device (core) we are on currently\n          shuffle=True)\n    \n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          val_set,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n    training_loader = DataLoader(training_set,\n                                batch_size = flags['batch_size'],\n                                sampler = train_sampler,\n                                num_workers =  flags['num_workers'],\n                                drop_last = True)\n    val_loader = DataLoader(val_set,\n                            batch_size = flags['batch_size'],\n                            sampler = valid_sampler,\n                            num_workers =  flags['num_workers'],\n                            drop_last = True)\n\n    #Send model to TPU device\n    model = model.to(device)\n    xm.master_print('done loading model')\n\n    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n    optimizer = torch.optim.Adam(params =  model.parameters(), lr=flags['learning_rate'])\n\n    #Training loop\n    xm.master_print('training on train dataset')\n\n    for epoch in range(flags['num_epochs']):\n        gc.collect()\n        train(epoch, tokenizer, model, device, training_loader, optimizer)\n\n    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n    # Saving the dataframe as predictions.csv\n    xm.master_print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n\n    val_dataset['predictions'] = predictions\n    val_dataset['answer'] = actuals\n    \n    #9: Results from TPU\n    xm.mesh_reduce\n    \n    gc.collect()","80acdead":"flags = {}\nflags['batch_size'] = 8\nflags['num_workers'] = 8\nflags['num_epochs'] = 2\nflags['seed'] = 1234\nflags['max_len'] = 512\nflags['learning_rate'] = 1e-4 * xm.xrt_world_size()\n\nxmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')","902478d0":"for i in range(10):\n    print(val_dataset['question'][i], \"\\nActual Answer: \", val_dataset['answer'][i],\"\\nPredicted Answer: \", val_dataset['predictions'][i], '\\n')","7e4956da":" val_dataset.to_csv('predictions.csv')","17236a98":"val_dataset.query('predictions==answer')","e646e119":"val_dataset['predictions'] = val_dataset['predictions'].apply(lambda s: '[ '+ s.replace('<sep>', ',') + ' ]')\nval_dataset['predictions'].head(10)","bb1f1311":"\"\"\"Evaluation utilities.\"\"\"\n\ndef normalize_answer(s):\n    \"\"\"Normalize answer.\"\"\"\n    s = unicodedata.normalize(\"NFD\", s)\n\n    def remove_articles(text):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef exact_match_score(prediction, ground_truth):\n    return normalize_answer(prediction) == normalize_answer(ground_truth)\n\n\ndef regex_match_score(prediction, ground_truth):\n    try:\n        regex = re.compile(ground_truth,\n                       flags=re.IGNORECASE + re.UNICODE + re.MULTILINE)\n        return regex.match(prediction) is not None\n    except re.error:\n        return False\n\ndef metric_max_over_ground_truths(metric_fn, prediction,\n                                  ground_truths):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)","adfd53f4":"val_dataset['exact_match'] = val_dataset.apply(lambda row: exact_match_score(row['predictions'], row['answer']), axis=1)\nval_dataset['regex_match'] = val_dataset.apply(lambda row: regex_match_score(row['predictions'], row['answer']), axis=1)","6d308a3d":"val_dataset.head()","91d1e7fa":"val_dataset[val_dataset['exact_match']]","9af1614e":"# Step 1: Get the credential from the Cloud SDK\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()","0a8735de":"# Step 2: Set the credentials\nuser_secrets.set_tensorflow_credential(user_credential)","8ecd60b7":"# Step 3: Use a familiar call to get the GCS path of the dataset\nfrom kaggle_datasets import KaggleDatasets\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()","6d0eada5":"## DataProcessing","fb5ceeab":"## Imports","927e0710":"## Evaluation:","ff3e640e":"## Training and Validation"}}