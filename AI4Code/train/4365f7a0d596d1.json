{"cell_type":{"ab19ca11":"code","a61a9afb":"code","9c0be964":"code","403a4531":"code","fad33244":"code","2d313c89":"code","4f37e0a1":"code","0b42ce5a":"code","21ab7a2f":"code","17b6b52d":"code","78bf9a5c":"code","71241934":"code","0546be33":"code","3f46c981":"code","cbb9035f":"code","82d39269":"code","b8882b7b":"code","50534806":"code","122f4bea":"code","56991e1a":"code","fd70187b":"code","8b977775":"code","bdd8c916":"code","930c1d8f":"code","71aa36dd":"code","066b2e49":"code","6de1f771":"code","b2f2499c":"code","7504cb66":"code","863ebec1":"code","02a80fe8":"code","12e26540":"code","5de21034":"code","28afe06a":"code","e8d5a6a3":"code","12a94c6b":"code","307992df":"code","2cde9078":"code","668c86ed":"code","71dc7286":"code","9e8a30c2":"code","4593e4f9":"code","684bc17d":"code","a9c0830a":"code","c5decce2":"code","dfd8eb93":"code","8add404f":"code","e1f02e71":"code","850d3694":"code","9b4c29ba":"code","fb9f03dd":"code","ff32ef44":"code","ef3f4e19":"code","b6199a8c":"code","bd25b0a6":"code","eec33645":"code","a234aa4e":"code","cbe6f27c":"code","77d396de":"code","70f132d5":"code","4b42fbfb":"code","426006a0":"code","f3aa405d":"code","b33f903a":"code","36f28e3c":"code","2d803ea0":"code","73f20dc6":"code","968e31f8":"code","6889acd0":"code","ba4ad3c2":"code","cc195814":"code","3f99639d":"code","07802924":"markdown","01f48b3f":"markdown","831f29ed":"markdown","f4fd104f":"markdown","a9ce7eb7":"markdown","b457b7aa":"markdown","918e7fb0":"markdown","27538962":"markdown","6d589db9":"markdown","71dc3f44":"markdown","fb21856b":"markdown","f5985076":"markdown","e0fff5cf":"markdown","1690c18c":"markdown","8cf1d04d":"markdown","22051d45":"markdown","eee0a611":"markdown","bde3a780":"markdown","6085ab3d":"markdown","d5fade26":"markdown","2432999b":"markdown"},"source":{"ab19ca11":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a61a9afb":"# functions for reading in embedding data and\n# tokenizing and processing sequences with padding and\n# function for plotting model accuracy and loss\n# modify line.split to line.split(\" \") as 300D contains spaces\n\nimport os\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# vectorizer and sequence function\n# takes in raw text and labels\n# params for max sequence length and max words\n# default arg for Shuffle=True to randomise data\n# returns tokenizer object. x_train,y_train, x_val,y_val\ndef tokenize_and_sequence(texts, labels, max_len, max_words, validation_samples, shuffle=True):\n    #initialise tokenizer with num_words param\n    tokenizer = Tokenizer(num_words=max_words)\n    tokenizer.fit_on_texts(texts)\n    # convert texts to sequences\n    sequences = tokenizer.texts_to_sequences(texts)\n    # generate work index\n    word_index = tokenizer.word_index\n    # print top words count\n    print('{} of unique tokens found'.format(len(word_index)))\n    # pad sequences using max_len param\n    data = pad_sequences(sequences, maxlen=max_len)\n    # convert list of labels into numpy array\n    labels = np.asarray(labels)\n    # print shape of text and label tensors\n    print('data tensor shape: {}\\nlabel tensor shape:{}'.format(data.shape, labels.shape))\n\n    # shuffle data=True as labels are ordered\n    # randomise data to vary class distribution\n    if shuffle:\n        # get length of data sequence and create array\n        indices = np.arange(data.shape[0])\n        np.random.shuffle(indices)\n        # shuffle data and labels\n        data = data[indices]\n        labels = labels[indices]\n    else:\n        pass\n\n    # split training data into training and validation splits\n    # split using validation length\n    # validation split\n    x_val = data[:validation_samples]\n    y_val = labels[:validation_samples]\n    # training split\n    x_train = data[validation_samples:]\n    y_train = labels[validation_samples:]\n\n    # return tokenizer, word_index, training and validation data\n    return tokenizer, word_index, x_train, y_train, x_val, y_val\n\n\n# function to lpad pretrained glove embeddings\n# takes in embedding dim for variable embedding sizes\n# and base directory as well as txt file\n# embedding dim should match the file name dimension\n# and max words and word_index for embedding features\ndef load_glove(base_directory, f_name, max_words, word_index, embedding_dim=None):\n    # check file name ends in .txt\n    # read file name embedding value if not specified\n    if f_name[-4:] == '.txt':\n        # check embedding value\n        if embedding_dim is not None:\n            dim = f_name[-8:-5]\n            dim = int(dim)\n            embedding_dim = dim\n        else:\n            # assuming dimension is not none for manual input\n            pass\n        # continue\n\n        # create embedding dictionary\n        embeddings_index = {}\n        # open embeddings file\n        try:\n            f = open(os.path.join(base_directory, f_name))\n            # iterate over lines and split on individual words\n            # split coefficient of word values\n            # map words and coefficients to embeddings dictionary\n            for line in f:\n                values = line.split(\" \") # returns list of [word, coeff]\n                word = values[0] # gets first list element\n                coeff = np.asarray(values[1:], dtype='float32')  # slice coefficiennt value array from remainder of list\n                # assign mapping to dictionary\n                embeddings_index[word] = coeff\n            f.close()\n        except IOError:\n            print('cannot read file. check file paths')\n\n        # prepare glove word-embedding matrix\n        # create empty embedding tensor\n        embedding_matrix = np.zeros((max_words,embedding_dim ))\n        # map the top words of the data into the glove embedding matrix\n        # words not found from the data in glove will be zeroed\n        for word, i in word_index.items():\n            if i < max_words:\n                embedding_vector = embeddings_index.get(word)\n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n\n        # return embedding matrix\n        return embedding_matrix\n\n\n# function to visualise keras model history metrics\n# function takes in acc, val_acc, loss, val_loss for model params\n# range is defined by epochs in range len(acc)\n\nimport matplotlib.pyplot as plt\n\ndef plot_training_and_validation(acc, val_acc, loss, val_loss):\n    epochs = range(1, len(acc) + 1)\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.figure()\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    plt.show()\n\n# end","9c0be964":"base_dir ='..\/input'\nprint(base_dir)\n# list files in current directory\nprint(os.listdir(base_dir))","403a4531":"# set train and test data set paths\ntrain_path = os.path.join(base_dir, 'train.csv')\ntest_path = os.path.join(base_dir, 'test.csv')\nprint(train_path, test_path)","fad33244":"# set embedding file path\nprint(os.listdir(os.path.join(base_dir, 'embeddings')))","2d313c89":"glove_file = 'glove.840B.300d.txt'\nbase_embedding_dir = os.path.join(base_dir, 'embeddings\/glove.840B.300d')\nprint(os.path.join(base_embedding_dir, glove_file))","4f37e0a1":"# load data into DataFrames\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)","0b42ce5a":"# verify and inspect DataFrames\ntrain_df.head()","21ab7a2f":"test_df.head()","17b6b52d":"# simple statistics of training data\ntrain_df.info()","78bf9a5c":"train_df.shape","71241934":"# check for null values in labels in training data\ntrain_df.isnull().any()","0546be33":"# define variable to index data frame question column\nquestions = 'question_text'","3f46c981":"# find maximum sequence length of questions\nnp.max(train_df[questions].apply(lambda x: len(x.split())))","cbb9035f":"# split labels from training data into numpy array\ny_train = train_df['target'].values\ny_train = np.asarray(y_train)\nprint(type(y_train))","82d39269":"# verify label array shape\ny_train.shape","b8882b7b":"# inspect label\ny_train[0]","50534806":"# extract questions from Series objects of train and test data\nquestions_train = train_df[questions]\nquestions_test = test_df[questions]","122f4bea":"# transforms Series into lists\nx_train = list(questions_train)\nx_test = list(questions_test)","56991e1a":"# verify and inspect data\nprint(x_train[0], y_train[0], type(x_train), len(x_train))\nprint(x_test[0], len(x_test))","fd70187b":"# define a 10% validation split from training data\nvalidation_samples = int((len(x_train) \/\/ 10))","8b977775":"# verify 90:10 split\nprint(len(x_train), validation_samples)","bdd8c916":"# define max sequence length and total dictionary words\nmax_len = 100\nmax_words = 10000","930c1d8f":"# Vectorize training data and return tokenizer and word_index as well as validation splits\ntokenizer, word_index, X_train, Y_train, x_val, y_val = tokenize_and_sequence(\n    x_train, y_train, max_len=max_len, max_words=max_words, validation_samples=validation_samples, shuffle=False)","71aa36dd":"# verify train and validation text and labels\nprint('training:',X_train.shape, Y_train.shape, '\\nvalidation:', x_val.shape, y_val.shape)","066b2e49":"# define embedding dimension\nembedding_dim = 300","6de1f771":"# load in glove embedding using custom function from earlier\n# function takes as input the raw file, word_index returned from the tokenizer and max_words\nglove_embedding_300d = load_glove('..\/input\/embeddings\/glove.840B.300d\/', glove_file, max_words=max_words, word_index=word_index, embedding_dim=embedding_dim)\n# *error in loading* needs investigation \n# * 300D needs line.split(' ') compared to smaller dimensions","b2f2499c":"# verify embeddings loaded correctly\nglove_embedding_300d.shape","7504cb66":"# import layers\nfrom keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\nfrom keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\nfrom keras.models import Sequential","863ebec1":"# import AUC ROC metrics from sklearn\nfrom sklearn.metrics import roc_auc_score","02a80fe8":"# define model architecture\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_len))\nmodel.add(Conv1D(64, 3, activation='relu'))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(MaxPooling1D(4))\nmodel.add(CuDNNGRU(64))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","12e26540":"# load pre-trained Glove embeddings in the first layer\nmodel.layers[0].set_weights([glove_embedding_300d])\n# freeze embedding layer weights\nmodel.layers[0].trainable = False\n# compile model with adam optimizer\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","5de21034":"# fit model and train on training data and validate on validation samples\n# train for 5 epochs to establish baseline overfitting model\n# saves results to histroy object\nhistory = model.fit(X_train, Y_train, epochs=5, batch_size=128, validation_data=(x_val, y_val))","28afe06a":"# save model\nmodel.save('cnn_cudnngru_300d.h5')","e8d5a6a3":"# define plotting metrics\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n# plot model training and validation accuracy and loss\nplot_training_and_validation(acc, val_acc, loss, val_loss)","12a94c6b":"# further enhancements to be made and model checkpointing\n# plots show model performs the best at epoch 3","307992df":"y_hat = model.predict(x_val)","2cde9078":"# print auc roc score\n\"{:0.2f}\".format(roc_auc_score(y_val, y_hat)*100.0)","668c86ed":"# we first need to tokenize and pad the raw text from the test data\nsequences_test = tokenizer.texts_to_sequences(x_test)","71dc7286":"test_data = pad_sequences(sequences_test, maxlen=max_len)","9e8a30c2":"test_data.shape","4593e4f9":"# verify test data sample\ntest_data[0]","684bc17d":"# iterate over test sequences and predict y_hat values\ny_hat = model.predict(test_data)","a9c0830a":"y_hat.shape","c5decce2":"y_hat","dfd8eb93":"predictions = (np.array(y_hat) > 0.5).astype(np.int)","8add404f":"# create dataframe for precitions\nsubmit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": predictions.flatten()})","e1f02e71":"submit_df.head()","850d3694":"# save as csv for submission\n# submit_df.to_csv('submission.csv', index=False)","9b4c29ba":"# functions for reading in embedding data and\n# tokenizing and processing sequences with padding and\n# function for plotting model accuracy and loss\n# modify line.split to line.split(\" \") as 300D contains spaces\n\nimport os\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# vectorizer and sequence function\n# takes in raw text and labels\n# params for max sequence length and max words\n# default arg for Shuffle=True to randomise data\n# returns tokenizer object. x_train,y_train, x_val,y_val\ndef tokenize_and_sequence(full_texts, texts, labels, max_len, max_words, validation_samples, shuffle=True):\n    #initialise tokenizer with num_words param\n    tokenizer = Tokenizer(num_words=max_words)\n    tokenizer.fit_on_texts(full_texts)\n    # convert texts to sequences\n    sequences = tokenizer.texts_to_sequences(texts)\n    # generate work index\n    word_index = tokenizer.word_index\n    # print top words count\n    print('{} of unique tokens found'.format(len(word_index)))\n    # pad sequences using max_len param\n    data = pad_sequences(sequences, maxlen=max_len)\n    # convert list of labels into numpy array\n    labels = np.asarray(labels)\n    # print shape of text and label tensors\n    print('data tensor shape: {}\\nlabel tensor shape:{}'.format(data.shape, labels.shape))\n\n    # shuffle data=True as labels are ordered\n    # randomise data to vary class distribution\n    if shuffle:\n        # get length of data sequence and create array\n        indices = np.arange(data.shape[0])\n        np.random.shuffle(indices)\n        # shuffle data and labels\n        data = data[indices]\n        labels = labels[indices]\n    else:\n        pass\n\n    # split training data into training and validation splits\n    # split using validation length\n    # validation split\n    x_val = data[:validation_samples]\n    y_val = labels[:validation_samples]\n    # training split\n    x_train = data[validation_samples:]\n    y_train = labels[validation_samples:]\n\n    # return tokenizer, word_index, training and validation data\n    return tokenizer, word_index, x_train, y_train, x_val, y_val\n\n\n# function to lpad pretrained glove embeddings\n# takes in embedding dim for variable embedding sizes\n# and base directory as well as txt file\n# embedding dim should match the file name dimension\n# and max words and word_index for embedding features\ndef load_glove(base_directory, f_name, max_words, word_index, embedding_dim=None):\n    # check file name ends in .txt\n    # read file name embedding value if not specified\n    if f_name[-4:] == '.txt':\n        # check embedding value\n        if embedding_dim is not None:\n            dim = f_name[-8:-5]\n            dim = int(dim)\n            embedding_dim = dim\n        else:\n            # assuming dimension is not none for manual input\n            pass\n        # continue\n\n        # create embedding dictionary\n        embeddings_index = {}\n        # open embeddings file\n        try:\n            f = open(os.path.join(base_directory, f_name))\n            # iterate over lines and split on individual words\n            # split coefficient of word values\n            # map words and coefficients to embeddings dictionary\n            for line in f:\n                values = line.split(\" \") # returns list of [word, coeff]\n                word = values[0] # gets first list element\n                coeff = np.asarray(values[1:], dtype='float32')  # slice coefficiennt value array from remainder of list\n                # assign mapping to dictionary\n                embeddings_index[word] = coeff\n            f.close()\n        except IOError:\n            print('cannot read file. check file paths')\n\n        # prepare glove word-embedding matrix\n        # create empty embedding tensor\n        embedding_matrix = np.zeros((max_words,embedding_dim ))\n        # map the top words of the data into the glove embedding matrix\n        # words not found from the data in glove will be zeroed\n        for word, i in word_index.items():\n            if i < max_words:\n                embedding_vector = embeddings_index.get(word)\n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n\n        # return embedding matrix\n        return embedding_matrix\n\n\n# function to visualise keras model history metrics\n# function takes in acc, val_acc, loss, val_loss for model params\n# range is defined by epochs in range len(acc)\n\nimport matplotlib.pyplot as plt\n\ndef plot_training_and_validation(acc, val_acc, loss, val_loss):\n    epochs = range(1, len(acc) + 1)\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.figure()\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    plt.show()\n\n# end","fb9f03dd":"full_texts = x_train + x_test","ff32ef44":"max_len = 100\nmax_words = 10000","ef3f4e19":"# vectorize training data\n# Vectorize training data and return tokenizer and word_index as well as validation splits\ntokenizer, word_index, X_train, Y_train, x_val, y_val = tokenize_and_sequence(\n    full_texts, x_train, y_train, max_len=max_len, max_words=max_words, validation_samples=validation_samples, shuffle=False)","b6199a8c":"# verify train and validation text and labels\nprint('training:',X_train.shape, Y_train.shape, '\\nvalidation:', x_val.shape, y_val.shape)","bd25b0a6":"# define embedding dimension\nembedding_dim = 300","eec33645":"# load in glove embedding using custom function from earlier\n# function takes as input the raw file, word_index returned from the tokenizer and max_words\nglove_embedding_300d = load_glove('..\/input\/embeddings\/glove.840B.300d\/', glove_file, max_words=max_words, word_index=word_index, embedding_dim=embedding_dim)\n# * 300D needs line.split(' ') compared to smaller dimensions","a234aa4e":"# verify embeddings loaded correctly\nglove_embedding_300d.shape","cbe6f27c":"# import keras layers\nimport keras","77d396de":"# define custom ROC callback\n# import AUC ROC metrics from sklearn\nfrom sklearn.metrics import roc_auc_score\n\n# define class for ROC AUC callback with simple name modifications\n# credit to https:\/\/www.kaggle.com\/yekenot\nclass roc_auc_validation(keras.callbacks.Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n        self.interval = interval\n        self.x_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.x_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","70f132d5":"# import keras layers \nfrom keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU, CuDNNLSTM\nfrom keras.layers import Dropout, Dense, Activation, Flatten,Conv1D, Bidirectional, SpatialDropout1D, BatchNormalization\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop ","4b42fbfb":"# define model architecture\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_len))\nmodel.add(SpatialDropout1D(0.2)) # add spatial dropout\nmodel.add(Conv1D(64, 5, activation='relu')) # increase kernel size to 5\nmodel.add(MaxPooling1D(4))\nmodel.add(BatchNormalization()) # add batch normalization\nmodel.add(Dropout(0.1))\n# modify to CuDNNGRU\n#model.add(GRU(64, dropout=0.1, recurrent_dropout=0.5)) # defaults inclide tanh activation\nmodel.add(CuDNNGRU(64)) # does not have a dropout or recurrent dropout param\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","426006a0":"# define callbacks\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau","f3aa405d":"# initialise customer roc callback\nroc_callback = roc_auc_validation(validation_data=(x_val, y_val), interval=1)","b33f903a":"# define early stopping and reduce lr callbacks\ncallback_list = [keras.callbacks.EarlyStopping(monitor='acc', patience=1),\n                 keras.callbacks.ModelCheckpoint(filepath='baseline_plus_.h5', monitor='val_loss',\n                                                 save_best_only=True)]","36f28e3c":"# add roc to callbacks list\ncallback_list.append(roc_callback)","2d803ea0":"callback_list","73f20dc6":"# load pre-trained Glove embeddings in the first layer\nmodel.layers[0].set_weights([glove_embedding_300d])\n# freeze embedding layer weights\nmodel.layers[0].trainable = False\n# compile model with adam optimizer\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","968e31f8":"# fit model and train on training data and validate on validation samples\n# train for 5 epochs to establish baseline overfitting model\n# saves results to histroy object\nhistory = model.fit(X_train, Y_train, epochs=20, batch_size=512, callbacks=callback_list,validation_data=(x_val, y_val))","6889acd0":"# evaluate model on test set and submit results\n# we first need to tokenize and pad the raw text from the test data\nsequences_test = tokenizer.texts_to_sequences(x_test)\ntest_data = pad_sequences(sequences_test, maxlen=max_len)\n# iterate over test sequences and predict y_hat values\ny_hat = model.predict(test_data)","ba4ad3c2":"predictions = (np.array(y_hat) > 0.5).astype(np.int)","cc195814":"# create dataframe for precitions\nsubmit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": predictions.flatten()})","3f99639d":"submit_df.to_csv('submission.csv', index=False)","07802924":"# CNN-GRU Model\nFor my first submission I will develop a simple CNN-GRU model based on my ongoing learning of model architectures for various text classification problems. \n\nThis notebook will follow a typical deep learning workflow with an initial EDA from which we select important parameters such as max sequence length and max number of words. We then vectorize both training and test data followed by model training and evaluation and possibly model tuning if results are lower than expected. Finally, I post my first submission and review the model for further enhancements.","01f48b3f":"94% under the curve is not a bad starting point. \n\nDefinitely better scores to gain using advanced model tuning and hyperparameters.","831f29ed":"# 2.2 Evaluate on Test set and submit\nLet's evaluate the model on the test set, round the predicted values as per requirements of the competition, and submit the csv","f4fd104f":"## 3.3 Load GloVe embeddings","a9ce7eb7":"A max sequence length of 134 means a good max_len parameter would be 100 as in previous experiments that establish a good baseline","b457b7aa":"# 3. Baseline ++","918e7fb0":"# 1.3 - Vectorization\nTokenize and pad text sequences","27538962":"# Model Results\nPlot model training and validation performance","6d589db9":"# 1 - Data exploration\nLoading data and simple EDA","71dc3f44":"## 3.1 Redefine custom helper functions","fb21856b":"# 1.1 - Loading in Training and Test data\nSet directory paths for data as well as GloVe embeddings","f5985076":"# Improving upon the baseline model\nIn this experiment we increase the model complexity while modifying the vecotrization process slightly by fitting on test and train texts. \n\nWe also implement callbacks, dropout, batch normalisation as well as early stopping.","e0fff5cf":"# 1.2 - Split text and labels ready for vectorization\nSplit training text and labels into lists, split test text into a list","1690c18c":"Custom helper functions for reading in glove embeddings as well as vectorizing training data","8cf1d04d":"Let's use the AUC ROC score metrics for a slightly better understanding of model performance.\n\nThe training and validation performance remains consistent but let's review it on the validation data and plot the AUC graph for a better representation of model generalization","22051d45":"## 3.2 Vectorize data\nWe fit our tokenizer on both the training and test sets to capture as many words as possible when passing to the embeddings","eee0a611":"## 3.4 Model Architecture\n## Baseline++\n\nWe take the same core design of the baseline model of a CNN-GRU but add features such as Dropout, Batch normalization.\n\nWe optimise the model with callbacks and monitor progress to compare if the modifications in input sequence length and size improves our results.","bde3a780":"# 2 - Model Architecture","6085ab3d":"# 1.4 - Load Glove Embeddings\nLoad 300D pre-trained embeddings","d5fade26":"## 3.5 Evaluate and submit test results","2432999b":"# 2.1 CNN-GRU Baseline\nDesign a simple model using a 1D convolution layer followed by a GRU layer to establish a benchmark performance score for further improvements.\n\nUsing the pre-trained embeddings with weights frozen to prevent re-training of word vectors during model training.\n\nTraining for 5 epochs on batch sizes of 128\n\nenhancements -\nUsing a CuDNNGRU layer for GPU acceleration as well as SpatialDropout1D of 0.2 for the convolution layer\n\n## Design\nA sequential model with the following layers:\n\nEmbedding(dimension=200)\nConv1D(64, 3, 'relu') 64 convolutions with a kernel size of 3, can be extended up to 7\nSpatialDropout1D(0.2)\nMaxPooling1D(4) standard practive following convolutions\nGRU(64, dropout=0.1, recurrent_dropout=0.5) *using a layer dropout of 10% and a recurent unit dropout of 50%, as seen in research to return good performance*\nDense(1, activation='sigmoid') dense classifier layer of six outputs"}}