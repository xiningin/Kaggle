{"cell_type":{"8d86765c":"code","0d43ab8c":"code","72827982":"code","51f8ac5b":"code","8d9d4213":"code","75e0bc35":"code","44a76e81":"code","2c097c98":"code","a6b27315":"code","bfafea81":"code","014e2adc":"code","c5e8af47":"code","3d1fd926":"code","f2ffe6b2":"code","9c62f90e":"code","7ad8e973":"code","87bf9029":"code","99504026":"code","9ed58c99":"code","b17e91cb":"code","ceeb9464":"code","5842ad55":"code","368750db":"code","034520b7":"code","d87bfd2a":"code","1aa0c436":"code","fcaf151f":"code","5723fff6":"code","94a0046d":"code","75768eab":"code","8f8269db":"code","cc78f85b":"code","6dd1b7d9":"code","4faf1823":"code","e8e8adc8":"code","c27d585e":"code","2408d890":"code","d2adb89b":"code","4c3e5d91":"code","d7b3ad53":"code","dd046af7":"code","039d56c1":"code","d441a820":"code","949698af":"code","dbf04461":"code","ff806cf1":"code","196a427e":"code","48fa02ea":"code","35df4eec":"code","3fd10146":"code","90001954":"code","eeae12e6":"code","e2cad71c":"code","dd9d4016":"code","e4a85c05":"code","976c7d76":"code","430cd9b5":"code","62c04e87":"code","b6667b06":"code","1753a8f1":"code","9bb7ce82":"code","04b38a27":"code","e44dc49d":"code","3972da1a":"code","06a008aa":"code","f39f4d66":"code","a903cb25":"code","edb1ff82":"code","0d700954":"code","95c65f37":"code","434d784f":"code","969fb69a":"code","beb2aa1c":"code","d4f9a797":"code","21ce49bf":"code","35b13fe7":"code","ea342712":"code","3c8850b8":"code","ed24eef3":"code","f62be280":"code","39961d87":"code","64363bc7":"code","51611621":"code","05f2ba6a":"code","c59e879c":"code","89a575a8":"code","e25e11a7":"code","f7dc4457":"code","a49e87c5":"code","86212a48":"code","172cca87":"code","6c3b605a":"code","edef9a54":"code","c8d3567b":"code","f33bce80":"code","69126fbb":"code","b861f1ae":"code","10f377f6":"code","b37d17db":"code","01e79199":"code","323d727b":"code","44a810e6":"code","b99ebb89":"code","ad2c0bee":"code","4cb46ed7":"markdown","82c834cd":"markdown","43941d5d":"markdown","e0b6ce72":"markdown","0187c97d":"markdown","e57b70e4":"markdown","5bd3b682":"markdown","324eba31":"markdown","ab0ea80f":"markdown","6900552e":"markdown","2c494810":"markdown","419728fe":"markdown","c7cbcd10":"markdown","14df6990":"markdown","272e6e9e":"markdown","6d18a069":"markdown","e05cdb2b":"markdown","318d8a4f":"markdown","a0263907":"markdown","a7f6382f":"markdown","91117d14":"markdown","5d440f99":"markdown","dfc901ce":"markdown","b132e45e":"markdown","0272f37a":"markdown","9aca3811":"markdown","e6146195":"markdown","504b1ea9":"markdown","67f3eed9":"markdown","c48b09be":"markdown","3466b8cd":"markdown","87e1bfb3":"markdown","0a566484":"markdown","82173bf1":"markdown","b9601481":"markdown","c9db53af":"markdown"},"source":{"8d86765c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0d43ab8c":"#load the data\nspotify_song_data= pd.read_csv(\"..\/input\/19000-spotify-songs\/song_data.csv\")\nspotify_song_info= pd.read_csv(\"..\/input\/19000-spotify-songs\/song_info.csv\") ","72827982":"song_info=spotify_song_info.copy()","51f8ac5b":"song_data=spotify_song_data.copy()\nsong_data.head(3)","8d9d4213":"song_data.shape","75e0bc35":"song_data.columns[song_data.isnull().any()]","44a76e81":"song_data.isnull().sum()","2c097c98":"song_data.info()","a6b27315":"song_data.song_duration_ms= song_data.song_duration_ms.astype(float)\nsong_data.time_signature= song_data.time_signature.astype(float)\nsong_data.audio_mode= song_data.audio_mode.astype(float)","bfafea81":"song_data.describe()","014e2adc":"song_data[\"popularity\"]= [ 1 if i>=66.5 else 0 for i in song_data.song_popularity ]\nsong_data[\"popularity\"].value_counts()","c5e8af47":"#popular songs' data\na=song_data[song_data[\"popularity\"]==1]\na.describe()","3d1fd926":"song_data.corr()","f2ffe6b2":"f,ax = plt.subplots(figsize=(12, 12))\nmask = np.zeros_like(song_data.corr())\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(song_data.corr(), annot=True, linewidths=0.4,linecolor=\"white\", fmt= '.1f',ax=ax,cmap=\"Blues\",mask=mask)\nplt.show() ","9c62f90e":"def bar_plot(variable):\n    \n    var=song_data[variable]\n    var_value= var.value_counts()\n    \n    #visualize\n    plt.figure(figsize=(9,3))\n    plt.bar(var_value.index,var_value,color=\"orange\")\n    plt.xticks(var_value.index,var_value.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable,var_value))\n","7ad8e973":"category1 = [\"popularity\",\"key\",\"audio_mode\",\"time_signature\"]\nfor c in category1:\n    bar_plot(c)","87bf9029":"# key vs popularity\nsong_data[[\"key\",\"popularity\"]].groupby([\"key\"], as_index = False).mean().sort_values(by=\"popularity\",ascending = False)","99504026":"# audio_mode vs popularity\nsong_data[[\"audio_mode\",\"popularity\"]].groupby([\"audio_mode\"], as_index = False).mean().sort_values(by=\"popularity\",ascending = False)","9ed58c99":"# time_signature vs popularity\nsong_data[[\"time_signature\",\"popularity\"]].groupby([\"time_signature\"], as_index = False).mean().sort_values(by=\"popularity\",ascending = False)","b17e91cb":"from collections import Counter\ndef detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index #filtre\n        # store indeces\n        outlier_indices.extend(outlier_list_col) #The extend() extends the list by adding all items of a list (passed as an argument) to the end.\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2) \n    \n    return multiple_outliers","ceeb9464":"song_data.loc[detect_outliers(song_data,[\"song_popularity\",\"song_duration_ms\",\"danceability\",\"energy\",\"instrumentalness\",\"liveness\",\"loudness\",\"speechiness\",\"audio_valence\"])]","5842ad55":"# drop outliers\nsong_data = song_data.drop(detect_outliers(song_data,[\"song_popularity\",\"song_duration_ms\",\"danceability\",\"energy\",\"instrumentalness\",\"liveness\",\"loudness\",\"speechiness\",\"audio_valence\"]),axis = 0).reset_index(drop = True)","368750db":"song_data[song_data[\"audio_mode\"].isnull()]","034520b7":"g = sns.factorplot(x = \"key\", y = \"popularity\", data = song_data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Popularity Probability\")\nplt.show()","d87bfd2a":"g = sns.factorplot(x = \"audio_mode\", y = \"popularity\", data = song_data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Popularity Probability\")\nplt.show()","1aa0c436":"g = sns.factorplot(x = \"time_signature\", y = \"popularity\", data = song_data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Popularity Probability\")\nplt.show()","fcaf151f":"g = sns.FacetGrid(song_data, row = \"audio_mode\", col = \"popularity\", size = 4)\ng.map(sns.barplot, \"key\", \"acousticness\")\ng.add_legend()\nplt.show()","5723fff6":"g = sns.FacetGrid(song_data, row = \"audio_mode\", col = \"popularity\", size = 4)\ng.map(sns.barplot, \"key\", \"danceability\",color=\"purple\")\ng.add_legend()\nplt.show()","94a0046d":"g = sns.FacetGrid(song_data, row = \"audio_mode\", col = \"popularity\", size = 4)\ng.map(sns.barplot, \"key\", \"instrumentalness\",color=\"green\")\ng.add_legend()\nplt.show()","75768eab":"g = sns.FacetGrid(song_data, row = \"audio_mode\", col = \"popularity\", size = 4)\ng.map(sns.barplot, \"key\", \"loudness\",color=\"orange\")\ng.add_legend()\nplt.show()","8f8269db":"f, axes = plt.subplots(3, 5, figsize=(12, 12))\nsns.distplot( song_data[\"song_duration_ms\"] , color=\"teal\", ax=axes[0, 0])\nsns.distplot( song_data[\"instrumentalness\"] , color=\"teal\", ax=axes[0, 1])\nsns.distplot( song_data[\"acousticness\"] , color=\"teal\", ax=axes[0, 2])\nsns.distplot( song_data[\"danceability\"] , color=\"teal\", ax=axes[0, 3])\nsns.distplot( song_data[\"energy\"] , color=\"teal\", ax=axes[0, 4])\nsns.distplot( song_data[\"song_popularity\"] , color=\"teal\", ax=axes[1, 0])\nsns.distplot( song_data[\"key\"] , color=\"teal\", ax=axes[1, 1])\nsns.distplot( song_data[\"liveness\"] , color=\"teal\", ax=axes[1, 2])\nsns.distplot( song_data[\"loudness\"] , color=\"teal\", ax=axes[1, 3])\nsns.distplot( song_data[\"audio_mode\"] , color=\"teal\", ax=axes[1, 4])\nsns.distplot( song_data[\"tempo\"] , color=\"teal\", ax=axes[2, 0])\nsns.distplot( song_data[\"speechiness\"] , color=\"teal\", ax=axes[2, 1])\nsns.distplot( song_data[\"time_signature\"] , color=\"teal\", ax=axes[2, 2])\nsns.distplot( song_data[\"audio_valence\"] , color=\"teal\", ax=axes[2, 3])\nf.delaxes(axes[2][4])\nplt.show()","cc78f85b":"g = sns.FacetGrid(song_data, col = \"popularity\")\ng.map(sns.distplot, \"acousticness\", bins = 25)\nplt.show()","6dd1b7d9":"g = sns.FacetGrid(song_data, col = \"popularity\")\ng.map(sns.distplot, \"danceability\", bins = 25)\nplt.show()","4faf1823":"g = sns.FacetGrid(song_data, col = \"popularity\")\ng.map(sns.distplot, \"loudness\", bins = 25)\nplt.show()","e8e8adc8":"g = sns.FacetGrid(song_data, col = \"popularity\")\ng.map(sns.distplot, \"instrumentalness\", bins = 25)\nplt.show()","c27d585e":"song_data3=song_data.copy()\nsong_data3[\"song_audio_valence\"]= [ \"Happy\" if i>=0.5 else \"Sad\" for i in song_data.audio_valence ]\nsong_data3[\"song_audio_valence\"].value_counts()","2408d890":"song_data1=song_data3[song_data3[\"song_popularity\"]>66.5]\nsong_data1[\"song_audio_valence\"]= [ \"Happy\" if i>=0.5 else \"Sad\" for i in song_data1.audio_valence ]\nsong_data1[\"song_audio_valence\"].value_counts()","d2adb89b":"song_data2_new=song_data1[song_data1[\"song_popularity\"]>90]\nsong_data2_new[\"song_audio_valence\"]= [ \"Happy\" if i>=0.5 else \"Sad\" for i in song_data2_new.audio_valence ]\nsong_data2_new[\"song_audio_valence\"].value_counts()","4c3e5d91":"song_data2= song_data2_new[song_data2_new.popularity==1]\na=song_data2.iloc[:,1]\na.to_numpy()\nb=song_data2.iloc[:,14]\nb.to_numpy()\nplt.figure(figsize=[8,8])\nmarkerline, stemlines, baseline = plt.stem(\n    a, b, linefmt='grey', markerfmt='D', bottom=0.5)\nmarkerline.set_markerfacecolor('none')\nplt.xlabel(\"Popularity\")\nplt.ylabel(\"Audio Valance\")\nplt.show()","d7b3ad53":"new_data = pd.concat([song_info, song_data1],axis=1)\nnew_data=new_data[new_data[\"song_popularity\"]>90]","dd046af7":"# Top 500 Playlists Gender\nplt.figure(figsize=(12,5))\nnew_data= song_info['playlist'].head(500)\ng = sns.countplot(new_data, palette=\"icefire\")\nplt.title(\"Top 500 Genres\")\nplt.show()","039d56c1":"spotify_song_data[\"popularity\"]= [ 1 if i>=66.5 else 0 for i in spotify_song_data.song_popularity ]\nspotify_song_data[\"popularity\"].value_counts()","d441a820":"data_plr = pd.concat([spotify_song_data.popularity,spotify_song_data.song_name],axis=1)\ndata_plr.head()","949698af":"song_data['song_name'].value_counts(dropna=False)\nsong_data['song_name'].dropna(inplace=True)","dbf04461":"import re\nimport nltk \nimport nltk as nlp\n\nnltk.download(\"stopwords\") \nfrom nltk.corpus import stopwords","ff806cf1":"Song_Name = [ word for word in spotify_song_data.song_name if not word in set(stopwords.words(\"english\"))]\nlemma = nlp.WordNetLemmatizer()\nSong_Name = [ lemma.lemmatize(word) for word in Song_Name] ","196a427e":"Song_Name_list = []\nfor Song_Name in data_plr.song_name:\n    Song_Name = re.sub(\"[^a-zA-Z]\",\" \",Song_Name)\n    Song_Name = Song_Name.lower() \n    Song_Name = nltk.word_tokenize(Song_Name)\n    lemma = nlp.WordNetLemmatizer()\n    Song_Name = [ lemma.lemmatize(word) for word in Song_Name]\n    Song_Name = \" \".join(Song_Name)\n    Song_Name_list.append(Song_Name)","48fa02ea":"# %% bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\nmax_features = 10\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\nsparce_matrix = count_vectorizer.fit_transform(Song_Name_list).toarray()  \nprint(\"en sik kullanilan {} kelimeler: {}\".format(max_features,count_vectorizer.get_feature_names()))","35df4eec":" a = count_vectorizer.get_feature_names()","3fd10146":"df=pd.DataFrame(Song_Name_list,columns=['Names'])","90001954":"artist= song_info.artist_name.tolist()\nname_count = Counter(artist)         \nmost_common_names2 = name_count.most_common(10) \nmost_common_names2\n","eeae12e6":"from textblob import TextBlob\ndata_plr['sentiment'] = data_plr['song_name'].map(lambda text: TextBlob(text).sentiment.polarity)","e2cad71c":"data_plr.head()","dd9d4016":"import numpy as np\ncut = pd.cut(\n    data_plr['sentiment'],\n    [-np.inf, -.01, .01, np.inf],\n    labels=['negative', 'neutral', 'positive']\n)\ndata_plr['polarity'] = cut.values\ndata_plr[['polarity','sentiment']].head()","e4a85c05":"plt.figure(figsize=(5,5))\ndata= data_plr.polarity\ng = sns.countplot(data, palette=\"Set3\")\nplt.title(\"Songs' Polarity\")\nplt.show()","976c7d76":"song_data =pd.concat([song_data,data_plr.sentiment],axis=1)\nsong_data.head(3)","430cd9b5":"song_data[\"key\"] = song_data[\"key\"].astype(\"category\")\nsong_data = pd.get_dummies(song_data, columns=[\"key\"])\nsong_data.head()","62c04e87":"song_data[\"audio_mode\"] = song_data[\"audio_mode\"].astype(\"category\")\nsong_data = pd.get_dummies(song_data, columns=[\"audio_mode\"])\nsong_data.head()","b6667b06":"song_data[\"time_signature\"] = song_data[\"time_signature\"].astype(\"category\")\nsong_data = pd.get_dummies(song_data, columns=[\"time_signature\"])\nsong_data.head()","1753a8f1":"song_data.drop([\"song_popularity\",\"song_name\"],axis=1,inplace=True)","9bb7ce82":"song_data.columns[song_data.isnull().any()]","04b38a27":"#fill nan values\nsong_data['song_duration_ms'] = song_data['song_duration_ms'].fillna(np.mean(song_data['song_duration_ms']))\nsong_data['acousticness'] = song_data['acousticness'].fillna(np.mean(song_data['acousticness']))\nsong_data['danceability'] = song_data['danceability'].fillna(np.mean(song_data['danceability']))\nsong_data['energy'] = song_data['energy'].fillna(np.mean(song_data['energy']))\nsong_data['instrumentalness'] = song_data['instrumentalness'].fillna(np.mean(song_data['instrumentalness']))\nsong_data['liveness'] = song_data['liveness'].fillna(np.mean(song_data['liveness']))\nsong_data['loudness'] = song_data['loudness'].fillna(np.mean(song_data['loudness']))\nsong_data['speechiness'] = song_data['speechiness'].fillna(np.mean(song_data['speechiness']))\nsong_data['tempo'] = song_data['tempo'].fillna(np.mean(song_data['tempo']))\nsong_data['audio_valence'] = song_data['audio_valence'].fillna(np.mean(song_data['audio_valence']))\nsong_data['popularity'] = song_data['popularity'].fillna(np.mean(song_data['popularity']))","e44dc49d":"song_data.columns[song_data.isnull().any()]","3972da1a":"def change_type(var):\n    song_data[var] = song_data[var].astype(int)","06a008aa":"column= [\"sentiment\",\"key_0.0\",\"key_1.0\",\"key_2.0\",\"key_3.0\",\"key_4.0\",\"key_5.0\",\"key_6.0\",\"key_7.0\",\"key_8.0\",\"key_9.0\",\"key_10.0\",\"key_11.0\",\"audio_mode_0.0\",\"audio_mode_1.0\",\"time_signature_0.0\",\"time_signature_1.0\",\"time_signature_3.0\",\"time_signature_4.0\",\"time_signature_5.0\"]\nfor i in column:\n    change_type(i)","f39f4d66":"#data preparation\ny = song_data[\"popularity\"].values\nx_data=song_data.drop([\"popularity\"],axis=1)\n#normalization\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.astype(int).T\ny_test = y_test.astype(int).T\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","a903cb25":"# parameter initialize and sigmoid function\ndef initialize_weights_and_bias(dimension):\n    w=np.full((dimension,1),0.01)\n    b=0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1\/(1+ np.exp(-z))\n    return y_head","edb1ff82":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] \n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 \n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","0d700954":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","95c65f37":"def predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","434d784f":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension)\n    #update\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate =0.01, num_iterations = 200)","969fb69a":"x,y = song_data.loc[:,song_data.columns != 'popularity'], song_data.loc[:,'popularity']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)\ny=y.astype(int)\ny_train= y_train.astype(int)\ny_test= y_test.astype(int)","beb2aa1c":"# ROC Curve with logistic regression\nfrom sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred_prob = logreg.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr,color=\"red\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()","d4f9a797":"from sklearn.model_selection import cross_val_score\nk = 10\ncv_result = cross_val_score(logreg,x_train,y_train,cv=k)\ncross_val_log=np.sum(cv_result)\/k\nprint('Cross_val Scores: ',cv_result)\nprint('Cross_val scores average: ',np.sum(cv_result)\/k)","21ce49bf":"#GridSearchCV with Logreg\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,param_grid,cv=3)\nlogreg_cv.fit(x_train,y_train)\nprint(\"Tuned hyperparameters : {}\".format(logreg_cv.best_params_))\nprint(\"Best Accuracy: {}\".format(logreg_cv.best_score_))\n##numpy.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None, axis=0)","35b13fe7":"Logistic_score=logreg_cv.best_score_\nCrossVal_Logistic_score=cross_val_log","ea342712":"x= ['song_duration_ms', 'acousticness', 'danceability',\n     'energy', 'instrumentalness', 'key', 'liveness',\n     'loudness', 'audio_mode', 'speechiness', 'tempo', \n     'time_signature', 'audio_valence']\ny= ['popularity']","3c8850b8":"# KNN prediction\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = song_data.loc[:,song_data.columns != 'popularity'], song_data.loc[:,'popularity']\ny=y.astype(int)\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))","ed24eef3":"#KNN Test\nknn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint('With KNN (K=3) train accuracy is: ',knn.score(x_train,y_train))\nprint('With KNN (K=3) test accuracy is: ',knn.score(x_test,y_test))","f62be280":"neig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n\nfor i, k in enumerate(neig):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    train_accuracy.append(knn.score(x_train, y_train))\n    test_accuracy.append(knn.score(x_test, y_test))\n\nplt.figure(figsize=[10,6])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('Knn k value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","39961d87":"from sklearn.model_selection import cross_val_score\nk = 10\ncv_result = cross_val_score(knn,x_train,y_train,cv=k)  \ncv_result_knn=np.sum(cv_result)\/k\nprint('Cross_val Scores: ',cv_result)\nprint('Cross_val scores average: ',np.sum(cv_result)\/k)","64363bc7":"from sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) \nknn_cv.fit(x,y)\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best accuracy: {}\".format(knn_cv.best_score_))","51611621":"KKN_Score= max(test_accuracy)\nCrossVal_KKN_Score=cv_result_knn","05f2ba6a":"from sklearn.svm import SVC\nsvm= SVC(random_state=1)  #kernel='rbf'\nsvm.fit(x_train,y_train)\nprint(\"Train accuracy of svm algo:\",svm.score(x_train,y_train))\nprint(\"Test accuracy of svm algo:\",svm.score(x_test,y_test))","c59e879c":"from sklearn.model_selection import cross_val_score\nk = 10\ncv_result = cross_val_score(svm,x_train,y_train,cv=k) \ncv_result_svm= np.sum(cv_result)\/k\nprint('Cross_val Scores: ',cv_result)\nprint('Cross_val scores average: ',np.sum(cv_result)\/k)","89a575a8":"SVM_score= svm.score(x_test,y_test)\nCrossVal_SVM_score=cv_result_svm","e25e11a7":"from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nsteps = [('scalar', StandardScaler()),\n         ('SVM', SVC())]\npipeline = Pipeline(steps)\nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\ncv = GridSearchCV(pipeline,param_grid=parameters,cv=10)\ncv.fit(x_train,y_train)\ny_pred = cv.predict(x_test)\n\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\nprint(\"Test accuracy: {}\".format(cv.score(x_test, y_test)))","f7dc4457":"from sklearn.naive_bayes import GaussianNB\nnb= GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"Train accuracy of naive bayes:\",nb.score(x_train,y_train))\nprint(\"Test accuracy of naive bayes:\",nb.score(x_test,y_test))\n","a49e87c5":"Naive_bayes_score=nb.score(x_test,y_test)","86212a48":"from sklearn.metrics import accuracy_score,recall_score,precision_score,confusion_matrix,f1_score\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt= DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny_pred=dt.predict(x_test)\nDecisionTree_score=dt.score(x_test,y_test)\nprint(\"Train ccuracy of decision tree:\",dt.score(x_train,y_train))\nprint(\"Test accuracy of decision tree:\",dt.score(x_test,y_test))","172cca87":"from sklearn.model_selection import cross_val_score\nk =10\ncv_result = cross_val_score(dt,x_train,y_train,cv=k) # uses R^2 as score \nprint('Cross_val Scores: ',cv_result)\nprint('Cross_val scores average: ',np.sum(cv_result)\/k)","6c3b605a":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=150,random_state = 3)\nrf.fit(x_train,y_train)\nprint(\"Train ccuracy of random forest\",rf.score(x_train,y_train))\nprint(\"Test accuracy of random forest\",rf.score(x_test,y_test))\nRandomForestClassifier_score=rf.score(x_test,y_test)\ny_pred=rf.predict(x_test)\nt_true=y_test","edef9a54":"from sklearn.model_selection import cross_val_score\nk = 10\ncv_result = cross_val_score(rf,x_train,y_train,cv=k) # uses R^2 as score \ncv_result_randomforest=np.sum(cv_result)\/k\nprint('Cross_val Scores: ',cv_result)\nprint('Cross_val scores average: ',np.sum(cv_result)\/k)","c8d3567b":"CrossVal_RandomForestClassifier_score=cv_result_randomforest","f33bce80":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state = 4)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',cm)\nprint('Classification report: \\n',classification_report(y_test,y_pred))","69126fbb":"sns.heatmap(cm,annot=True,fmt=\"d\") \nplt.show()","b861f1ae":"#Voting Classifier\nfrom sklearn.ensemble import VotingClassifier\nensemble=VotingClassifier(estimators=[('Random Forest', rf), ('Logistic Regression', logreg)], \n                       voting='soft', weights=[2,1]).fit(x_train,y_train)\nprint('The train accuracy for Random Forest and Logistic Regression is:',ensemble.score(x_train,y_train))\nprint('The test accuracy for Random Forest and Logistic Regression is:',ensemble.score(x_test,y_test))","10f377f6":"from sklearn.model_selection import cross_val_score\nk = 5\ncv_result = cross_val_score(ensemble,x_train,y_train,cv=k) # uses R^2 as score \nprint('Cross_val Scores: ',cv_result)\nprint('Cross_val scores average: ',np.sum(cv_result)\/k)","b37d17db":"# plot feature importance manually\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom matplotlib import pyplot\n\n# fit model no training data\nmodel = XGBClassifier()\nmodel.fit(x_train, y_train)\n# feature importance\nprint(model.feature_importances_)\n# plot\n#pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)\n#pyplot.show()","01e79199":"from numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\n\nmodel = XGBClassifier()\nmodel.fit(x_train, y_train)\n\n# plot feature importance\nax = plot_importance(model)\nfig = ax.figure\nfig.set_size_inches(10, 7)\n#plot_importance(model)\npyplot.show()","323d727b":"from numpy import sort\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\n\nmodel = XGBClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\nthresholds = sort(model.feature_importances_) # Fit model using each importance as a threshold\n\nfor thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(model, threshold=thresh, prefit=True) # if prefit=True, you should call transform directly.\n    select_X_train = selection.transform(x_train)\n    # train model \n    selection_model = XGBClassifier()\n    selection_model.fit(select_X_train, y_train)\n    # evaluate model\n    select_X_test = selection.transform(x_test)\n    y_pred = selection_model.predict(select_X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))","44a810e6":"model_performances=pd.DataFrame({'Model':['RandomForestClassifier','SVM','DesicionTreeClassifier','K-NearestNeighbors','LogisticRegession','NaiveBayes'],\n                                 'Accuracy':[RandomForestClassifier_score,SVM_score,DecisionTree_score,KKN_Score,Logistic_score,Naive_bayes_score]})\nmodel_performances.sort_values(by = \"Accuracy\",ascending=False)","b99ebb89":"model_list= list(model_performances['Model'].unique())\naccuracy_list= list(model_performances['Accuracy'].sort_values(ascending=False))\nf,ax = plt.subplots(figsize = (4,6))\nsns.barplot(x=accuracy_list,y=model_list,color='green',alpha = 0.5)\nax.set(xlabel='Test Accuracies', ylabel='Models')\nplt.show()\n","ad2c0bee":"labels = ['RF','SVM', 'K-NN', 'LR']\naccuracy = [0.897531,0.885054, 0.789488,0.710711]\nvalidation = [0.878750, 0.875631, 0.714693, 0.710711]\n\nx = np.arange(len(labels))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(12, 5))\nrects1 = ax.bar(x - width\/2,accuracy, width, label='Accuracy')\nrects2 = ax.bar(x + width\/2, validation, width, label='Validation')\n\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\ndef autolabel(rects):\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(0, 2),\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n        \nautolabel(rects1)\nautolabel(rects2)\nfig.tight_layout()\nplt.show()","4cb46ed7":"<a id=\"0\"><\/a> <br>\n\n> ## Introduction\n- Currently, being able to predict that something might be popular beforehand is an important research subject for every industry. It also has recently became a very important subject for the growing and competitive music industry as well. Since wide use of digital music platforms (Spotify, Billboard, Lastfm), data can be easily reached and the listening behaviors of the listeners can be easily observed. This provides convenience in forecasting techniques and it is also frequently used in recommendation systems.","82c834cd":"- Since we set our threshold such that 70% of the songs in our dataset are named as not popular and 30% are named as popular, 70% accuracy can be achieved by predicting all 0s. In order to capture this, we should consider the precision and recall values. As we see in confusion matrix, model predicted \u201c695 popular\u201d songs and \u201c2596 unpopular\u201d songs correct.\n","43941d5d":"- 60's and 70's rock music seems more popular in this data.","e0b6ce72":"- Checked popularity rating of songs that have been popular in the last 10 years in Spotify and took the mean value of them (66.5) . According to this value, the songs has above this rating could remain on the top lists for a long time. If song_popularity is higher than 66.5 (this is about 30% percent of data) we labeled it \"1\" and if is not we labeled it \"0\". So we have \"1\" for the popular songs and \"0\" for the unpopular ones.","0187c97d":"**Confusion Matrix with Random Forest**","e57b70e4":"- LR is one of the basic classi\ufb01cation method is used prediction of categorical variables.Our problem has two possible outputs popular(1) and unpopular(0) which is suitable for binary logistic regression. Since it is a probability value that we want to get from the problem, we obtained a value between [0,1] using the sigmoid function.  \n\u03b1(z) = 1\/(1+e-z)\n- Binary cross entropy is used for the loss function and gradient descent for the update the parameters.","5bd3b682":" <a id=\"3b\"><\/a> \n## Analysis","324eba31":"**ROC Curve with Logistic Regression**","ab0ea80f":"> <a id=\"10\"><\/a> <br>\n**Random Forest Classifier**\n- RF is one of the most popular ensemble learning method in machine learning not only gives good results even without hyperparameter optimization\u00a0but also can use both classification and regression problems. But the common problem of the traditional decision trees is over-fitting. In order to avoid overfitting, random forest models select and train hundreds of different sub-samples (multiple deep decision trees) randomly and reduce the variance. We used 100 estimators and random state 3 gave the about % 89 accuracy. ","6900552e":" <a id=\"3c\"><\/a> \n## Feature Engineering","2c494810":"## Why Do Some Songs Become Popular?","419728fe":"> ## Features Distribution\n","c7cbcd10":" <a id=\"3a\"><\/a> \n## Cleaning Data","14df6990":"**Top 500**","272e6e9e":"<a id=\"2\"><\/a> \n## Import Libraries","6d18a069":"<a id=\"6\"><\/a> <br>\n**KNN Algorithm**","e05cdb2b":"**All Songs**","318d8a4f":"## Basic Data Analysis","a0263907":"- SVM is an effective and simple method mostly used in classification problems. The aim of the SVM algorithm is to find a hyperplane in an N-dimensional space (N \u2014 the number of features) that distinctly classifies the data points. C and Gamma are the parameters for a nonlinear support vector machine (SVM) with a Gaussian radial basis function kernel.","a7f6382f":"<a id=\"4a\"><\/a> <br>\n## Models","91117d14":"- Data distribution of songs display today's songs features like dancebility, energy, loudness and tempo are quite high. People like fast and loud music.\n- According to instrumentalness, liveness and speechness, most of the songs are not live performances and they have lyrics.\n- Keys like 0,1,5,6 and 11 seems more effective in songs. And if key== 0 or 1 or 6 song has more chance to be populer.\n- Time_signure is mostly 4 and 5 in both populer and general data.\n- If danceability>0.6 song has more chance to be popular.\n- If loudness > -10 song has more chance to be popular.\n","5d440f99":"> ## People like Happy Songs or Sad Songs?\n- As we see in the feature explanations, audio valance describes the musical positiveness conveyed by a track (like sad or happiness - between 0 to 1). We supposed our threshold is 0.5.\n- With this threshold we have happy songs more in the general data and the numbers are pretty close in the popular songs, but when we looked at top 500, we can say that negative songs are twice as much as positives.","dfc901ce":"<a id=\"8\"><\/a> <br>\n**Naive Bayes**","b132e45e":"<a id=\"12\"><\/a> <br>\n## Comparison Of Performance","0272f37a":"<a id=\"4\"><\/a> <br>\n**Logistic Regression**","9aca3811":"- Correlation between loudness and energy is 0.8 which is strong and correlation between loudness and accusticness is 0.6 which is moderate. Except two of them all the correlations are quite low. When we compare the correlation between song_popularity and all other features, we don't see a strong correlation (a linear relationship) that gives us a clear information about popularity. Accusticness,danceability and loudness seems to have correlation with popularity feature(0.10) and istrumentalness has 0.20.","e6146195":" <a id=\"3\"><\/a> \n ## Data Preparation and Analysis","504b1ea9":"**Populer Songs**","67f3eed9":"<a id=\"11\"><\/a> <br>\n## FEATURE IMPORTANCE","c48b09be":"## Categorical Variable Analysis","3466b8cd":"# <a id=\"1\"><\/a> <br>\n> ## Feature Explanations\n- Dataset contains 19.000 songs and has 15 features like duration ms, key, audio mode, acousticness, danceability, energy and so on .\n- duration_ms: The duration of the track in milliseconds.\n- key: The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C\u266f\/D\u266d, 2 = D, and so on. If no key was detected, the value is -1.\n- audio_mode: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n- time_signature: An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).\n- acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n- danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n- energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. \n- instrumentalness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n- loudness: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\n- speechiness: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music.Values below 0.33 most likely represent music and other non-speech-like tracks.\n- audio_valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n- tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. \n- song_popularity: Song ratings of spotify audience.\n- liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live.","87e1bfb3":"<a id=\"13\"><\/a> <br>\n## Conclusion\n\nFirst we tried to predict popular songs using audio features then we added song name texts\u2019 polarity to it and tried to improve our model. We also fitted the model using each importance as a threshold. Although accusticness is the most important of these features\u00a0did not lead us to a strong result.  \nWe had 18835 songs available. Decision Tree algorithms which mainly given better results when we don\u2019t have so much data got the best result with RF. There were no strong linear correlations in our data, so linear methods did not fit well. In LR, We built the model using the gradient decent and this gave the best result in 200 iterations which was a moderate result. As in many popularity studies, we achieved the second best result with SVM. Adding Polarity to features value has almost not changed the result at all .\nFor the future work, the following questions can be asked. Nowadays, the industrialization of popular music has became more common all over the world and most musicians are using computers when they create their songs. So are songs started to be more similar and less unique? Does these affect our predictions in a positive way? With industrialization, prediction of the popular and trendy items are getting easier? Could the prediction increase if we had data with all the songs are created by a computer?","0a566484":"<a id=\"7\"><\/a> <br>\n**SVM**","82173bf1":"<a id=\"9\"><\/a> <br>\n**Decision Tree Classifier**","b9601481":"**SENTIMENTAL ANALYSIS**","c9db53af":"- [Introduction](#0)\n- [Feature explanations](#1)\n- [Import Libraries](#2)\n- [Data Preparation and Analysis](#3)\n    * [Cleaning the data](#3a)\n    * [Analysis](#3b)\n    * [Feature Engineering](#3c)\n- [Models](#4a)\n    * [Logistic Regression](#4)\n    * [Ridge and Lasso Regression](#5)\n    * [KNN Algorithm](#6)\n    * [SVM](#7)\n    * [Naive Bayes](#8)\n    * [Decision Tree Classifier](#9)\n    * [Random Forest Classifier](#10)\n    * [Feature Importance](#11)\n    * [Comparison Of Performance](#12)\n    * [Conclusion](#13)\n"}}