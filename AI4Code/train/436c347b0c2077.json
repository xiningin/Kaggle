{"cell_type":{"565d1b29":"code","367840dd":"code","fb1f12c3":"code","13dff834":"code","0f56b6dc":"code","fbb2254b":"code","e047573a":"code","f946d8e1":"code","8ec10a63":"code","18348d58":"code","80fa65d2":"code","c00d9fbb":"code","190a319d":"code","ee7b408b":"code","643d323f":"code","d2d1c4ad":"code","f4746a2c":"code","a80efff5":"code","336da1e3":"code","bfbeeb39":"code","a82d4ce7":"code","faa58c75":"code","3589bc89":"code","66bc312d":"code","289b2f94":"code","2bbe807f":"code","6799d632":"code","60dbdde8":"code","5aa5aeb4":"code","7ab390f1":"code","97a6a814":"code","76a96b24":"code","f7665bca":"markdown","76308b60":"markdown","143f41b4":"markdown","70ac0ac9":"markdown","601b24c0":"markdown","b7cd48ea":"markdown","5bdd188a":"markdown","aa66f11d":"markdown","b46db96b":"markdown","276f7000":"markdown","49fccd16":"markdown","20e503a5":"markdown","ded89372":"markdown","485be743":"markdown","c8d024be":"markdown","1ed02089":"markdown","d4e6e544":"markdown","f07ae2f1":"markdown","4146f7b9":"markdown","487c7c79":"markdown","028894cb":"markdown","b70c76ed":"markdown","c7c52609":"markdown","00783972":"markdown","e6176f62":"markdown","d781788d":"markdown","5426a6f3":"markdown","313fcfdd":"markdown","989cea6f":"markdown","e43076e9":"markdown","8a42326e":"markdown","ed5aacf2":"markdown"},"source":{"565d1b29":"import os\nimport pandas as pd \nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.linear_model import Lars\nfrom sklearn.linear_model import LassoLars\nfrom sklearn.linear_model import PassiveAggressiveRegressor\nfrom sklearn.linear_model import RANSACRegressor\nfrom sklearn.linear_model import SGDRegressor\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import LSTM\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\n\n# Set all options\n%matplotlib inline\nplt.style.use('seaborn-notebook')\nplt.rcParams[\"figure.figsize\"] = (20, 5)\npd.options.display.float_format = '{:20,.4f}'.format\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nsns.set(context=\"paper\", font=\"monospace\")","367840dd":"def nulls_summary(dataframe):\n  '''\n  Helper function that gives a quick summary of null values in a dataframe\n  Arguments\n  =========\n  dataframe: pandas dataframe\n  target: target column in classification\n  '''\n  print('\\nMissing values per column')\n  print('=========================')\n\n  print(dataframe.isnull().sum(axis=0))\n\n  print('\\nMissing values per row')\n  print('======================')\n\n  print('All data set :')\n  print(pd.DataFrame(dataframe.isnull().sum(axis=1), columns = ['nb_nans']).nb_nans.value_counts())","fb1f12c3":"DATA_DIR = '\/kaggle\/input\/household'\nDATA_DIR2= '\/kaggle\/input\/household-daily'\nINPUT_FILENAME = 'household_power_consumption.txt' \nRANDOM_SEED = 42\nNB_MIN_PER_HOUR = (24 * 60)","13dff834":"# load all data\ndf = pd.read_csv(os.path.join(DATA_DIR, INPUT_FILENAME), sep=';', header=0, low_memory=False, infer_datetime_format=True, parse_dates={'datetime':[0,1]}, index_col=['datetime'])\n# summarize\nprint(df.shape)\nprint(df.head())","0f56b6dc":"df.info(verbose=True, null_counts=True)","fbb2254b":"# df.loc['2007-04-28'] # Missing values '?' specified in description \ndf = df.replace('?', np.nan)","e047573a":"nulls_summary(df) \n# 25 979 missing rows","f946d8e1":"df = df.astype('float')\ndf.info(verbose=True, null_counts=True)","8ec10a63":"df['Sub_metering_4'] = (df['Global_active_power'] * 1000 \/ 60) - (df['Sub_metering_1'] + df['Sub_metering_2'] + df['Sub_metering_3'])","18348d58":"renaming_cols_dict = {'Sub_metering_1': 'Active_energy_kitchen', \n                      'Sub_metering_2': 'Active_energy_laundry',\n                      'Sub_metering_3': 'Active_energy_climate_control',\n                      'Sub_metering_4': 'Active_energy_remain'}\ndf = df.rename(columns=renaming_cols_dict)","80fa65d2":"plt.rcParams[\"figure.figsize\"] = (20, 10)\nplt.rcParams['axes.titlesize'] = 20\nplt.figure()\nfor i in range(len(df.columns)):\n\tplt.subplot(len(df.columns), 1, i+1)\n\tname = df.columns[i]\n\tplt.plot(df[name])\n\tplt.title(name, y=0, )\nplt.show()","c00d9fbb":"years = ['2007', '2008', '2009', '2010']\n\nplt.figure()\nfor i in range(len(years)):\n\t# prepare subplot\n\tax = plt.subplot(len(years), 1, i+1)\n\t# determine the year to plot\n\tyear = years[i]\n\t# get all observations for the year\n\tresult = df[str(year)]\n\t# plot the active power for the year\n\tplt.plot(result['Global_active_power'])\n\t# add a title to the subplot\n\tplt.title(str(year), y=0, loc='left')\nplt.show()","190a319d":"months = [x for x in range(1, 13)]\nfor i in range(len(months)):\n\t# prepare subplot\n\tax = plt.subplot(len(months), 1, i+1)\n\t# determine the month to plot\n\tmonth = '2007-' + str(months[i])\n\t# get all observations for the month\n\tresult = df[month]\n\t# plot the active power for the month\n\tplt.plot(result['Global_active_power'])\n\t# add a title to the subplot\n\tplt.title(month, y=0, loc='left')\nplt.show()","ee7b408b":"# daily line plots\ndays = [x for x in range(1, 20)] # 20 first days of a month\nplt.figure()\nfor i in range(len(days)):\n\t# prepare subplot\n\tax = plt.subplot(len(days), 1, i+1)\n\t# determine the day to plot\n\tday = '2007-01-' + str(days[i])\n\t# get all observations for the day\n\tresult = df[day]\n\t# plot the active power for the day\n\tplt.plot(result['Global_active_power'])\n\t# add a title to the subplot\n\tplt.title(day, y=0, loc='left')\nplt.show()","643d323f":"plt.rcParams[\"figure.figsize\"] = (20, 10)\nplt.rcParams['axes.titlesize'] = 15\nfor i in range(len(df.columns)):\n\tplt.subplot(len(df.columns), 1, i+1)\n\tname = df.columns[i]\n\tdf[name].hist(bins=100)\n\tplt.title(name, y=0)\nplt.show()","d2d1c4ad":"# plot active power for each year\nyears = ['2007', '2008', '2009', '2010']\nfor i in range(len(years)):\n\t# prepare subplot\n\tax = plt.subplot(len(years), 1, i+1)\n\t# determine the year to plot\n\tyear = years[i]\n\t# get all observations for the year\n\tresult = df[str(year)]\n\t# plot the active power for the year\n\tresult['Global_active_power'].hist(bins=100)\n\t# zoom in on the distribution\n\tax.set_xlim(0, 5)\n\t# add a title to the subplot\n\tplt.title(str(year), y=0, loc='right')\nplt.show()","f4746a2c":"for i in range(len(months)):\n\t# prepare subplot\n\tax = plt.subplot(len(months), 1, i+1)\n\t# determine the month to plot\n\tmonth = '2007-' + str(months[i])\n\t# get all observations for the month\n\tresult = df[month]\n\t# plot the active power for the month\n\tresult['Global_active_power'].hist(bins=100)\n\t# zoom in on the distribution\n\tax.set_xlim(0, 5)\n\t# add a title to the subplot\n\tplt.title(month, y=0, loc='right')\nplt.show()","a80efff5":"\"\"\"\ndf = pd.read_csv(os.path.join(DATA_DIR, INPUT_FILENAME), sep=';', header=0, low_memory=False, infer_datetime_format=True, parse_dates={'datetime':[0,1]}, index_col=['datetime'])\n# replace invalid values with nan\nprint(df.shape)\ndf = df.replace('?', np.nan)\n# cast numercial columns to float \ndf = df.astype('float')\ndf['Sub_metering_4'] = (df['Global_active_power'] * 1000 \/ 60) - (df['Sub_metering_1'] + df['Sub_metering_2'] + df['Sub_metering_3'])\nrenaming_cols_dict = {'Sub_metering_1': 'Active_energy_kitchen', \n                      'Sub_metering_2': 'Active_energy_laundry',\n                      'Sub_metering_3': 'Active_energy_climate_control',\n                      'Sub_metering_4': 'Active_energy_remain'}\ndf = df.rename(columns=renaming_cols_dict)\n\n\n# df = df.iloc[:10000, ]\n# fill missing values with a value at the same time one day ago\ndef fill_missing(df):\n    one_day = 60 * 24\n    for row in range(df.shape[0]):\n        for col in range(df.shape[1]):\n            if np.isnan(df.iloc[row, col]):\n                df.iloc[row, col] = df.iloc[row - one_day, col]\n# fill missing\nfill_missing(df)\n\n# resample data to daily\ndaily_groups = df.resample('D')\ndaily_data = daily_groups.sum()\n# summarize\nprint(daily_data.shape)\nprint(daily_data.head())\n# save\ndaily_data.to_csv('household_power_consumption_days.csv')\n\"\"\"","336da1e3":"# evaluate one or more weekly forecasts against expected values\ndef evaluate_forecasts(actual, predicted):\n\tscores = list()\n\t# calculate an RMSE score for each day\n\tfor i in range(actual.shape[1]):\n\t\t# calculate mse\n\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n\t\t# calculate rmse\n\t\trmse = sqrt(mse)\n\t\t# store\n\t\tscores.append(rmse)\n\t# calculate overall RMSE\n\ts = 0\n\tfor row in range(actual.shape[0]):\n\t\tfor col in range(actual.shape[1]):\n\t\t\ts += (actual[row, col] - predicted[row, col])**2\n\tscore = sqrt(s \/ (actual.shape[0] * actual.shape[1]))\n\treturn score, scores","bfbeeb39":"(train_start_date, train_end_date) = \"2006-12-17\", \"2010-01-02\"\n(test_start_date, test_end_date) = \"2010-01-03\", \"2010-11-20\"\n\n# split a univariate dataset into train\/test sets\ndef split_dataset(data):\n\t# split into standard weeks\n\ttrain, test = data.loc[train_start_date : train_end_date], data.loc[test_start_date : test_end_date]\n\t# restructure into windows of weekly data\n\ttrain = np.array(np.split(train, len(train)\/7)) # 159 weeks\n\ttest = np.array(np.split(test, len(test)\/7)) # 46 weeks\n\treturn train, test","a82d4ce7":"# convert history into inputs and outputs\ndef to_supervised(history, output_ix):\n\tX, y = list(), list()\n\t# step over the entire history one time step at a time\n\tfor i in range(len(history)-1):\n\t\tX.append(history[i][:,0]) # prior week of observations\n\t\ty.append(history[i + 1][output_ix,0]) # specific output day\n\treturn np.array(X), np.array(y)","faa58c75":"# fit a model and make a forecast\ndef sklearn_predict(model, history):\n\tyhat_sequence = list()\n\t# fit a model for each forecast day\n\tfor i in range(7):\n\t\t# prepare data\n\t\ttrain_x, train_y = to_supervised(history, i)\n\t\t# make pipeline\n\t\tpipeline = make_pipeline(model)\n\t\t# fit the model\n\t\tpipeline.fit(train_x, train_y)\n\t\t# forecast\n\t\tx_input = np.array(train_x[-1, :]).reshape(1,7)\n\t\tyhat = pipeline.predict(x_input)[0]\n\t\t# store\n\t\tyhat_sequence.append(yhat)\n\treturn yhat_sequence","3589bc89":"# evaluate a single model\ndef evaluate_model(model, train, test):\n\t# history is a list of weekly data\n\thistory = [x for x in train]\n\t# walk-forward validation over each week\n\tpredictions = list()\n\tfor i in range(len(test)):\n\t\t# predict the week\n\t\tyhat_sequence = sklearn_predict(model, history)\n\t\t# store the predictions\n\t\tpredictions.append(yhat_sequence)\n\t\t# get real observation and add to history for predicting the next week\n\t\thistory.append(test[i, :])\n\tpredictions = np.array(predictions)\n\t# evaluate predictions days for each week\n\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n\treturn score, scores\n\n# display the performance of a model as a single line\ndef summarize_scores(name, score, scores):\n\ts_scores = ', '.join(['%.1f' % s for s in scores])\n\tprint('%s: [%.3f] %s' % (name, score, s_scores))","66bc312d":"# create a feature preparation pipeline for a model\ndef make_pipeline(model):\n\tsteps = list()\n\t# standardization\n\tsteps.append(('standardize', StandardScaler()))\n\t# the model\n\tsteps.append(('model', model))\n\t# create pipeline\n\tpipeline = Pipeline(steps=steps)\n\treturn pipeline\n","289b2f94":"# prepare a list of ml models\ndef get_models(models=dict()):\n\t# linear models\n\tmodels['lr'] = LinearRegression()\n\tmodels['lasso'] = Lasso()\n\tmodels['ridge'] = Ridge()\n\tmodels['en'] = ElasticNet()\n\tmodels['huber'] = HuberRegressor()\n\tmodels['lars'] = Lars()\n\tmodels['llars'] = LassoLars()\n\tmodels['pa'] = PassiveAggressiveRegressor(max_iter=1000, tol=1e-3)\n\tmodels['ranscac'] = RANSACRegressor()\n\tmodels['sgd'] = SGDRegressor(max_iter=1000, tol=1e-3)\n\tprint('Defined %d models' % len(models))\n\treturn models","2bbe807f":"# load the new file\ndf = pd.read_csv('\/kaggle\/input\/householddaily\/household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n# split into train and test\ntrain, test = split_dataset(df)\n# prepare the models to evaluate\nmodels = get_models()\n# evaluate each model\ndays = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\nfor name, model in models.items():\n\t# evaluate and get scores\n\tscore, scores = evaluate_model(model, train, test)\n\t# summarize scores\n\tsummarize_scores(name, score, scores)\n\t# plot scores\n\tplt.plot(days, scores, marker='o', label=name)\n# show plot\nplt.legend()\nplt.show()","6799d632":"# convert history into inputs and outputs\ndef to_supervised(train, n_input, n_out=7):\n    # flatten data\n    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n    X, y = list(), list()\n    in_start = 0\n    # step over the entire history one time step at a time\n    for _ in range(len(data)):\n        # define the end of the input sequence\n        in_end = in_start + n_input\n        out_end = in_end + n_out\n        # ensure we have enough data for this instance\n        if out_end < len(data):\n            X.append(data[in_start:in_end, :])\n            y.append(data[in_end:out_end, 0])\n        # move along one time step\n        in_start += 1\n    return np.array(X), np.array(y)","60dbdde8":"# make a forecast\ndef forecast(model, history, n_input):\n    # flatten data\n    data = np.array(history)\n    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n    # retrieve last observations for input data\n    input_x = data[-n_input:, :]\n    # reshape into [1, n_input, n]\n    input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n    # forecast the next week\n    yhat = model.predict(input_x, verbose=0)\n    # we only want the vector forecast\n    yhat = yhat[0]\n    return yhat","5aa5aeb4":"# convert history into inputs and outputs\ndef to_supervised(train, n_input, n_out=7):\n    # flatten data\n    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n    X, y = list(), list()\n    in_start = 0\n    # step over the entire history one time step at a time\n    for _ in range(len(data)):\n        # define the end of the input sequence\n        in_end = in_start + n_input\n        out_end = in_end + n_out\n        # ensure we have enough data for this instance\n        if out_end < len(data):\n            X.append(data[in_start:in_end, :])\n            y.append(data[in_end:out_end, 0])\n        # move along one time step\n        in_start += 1\n    return np.array(X), np.array(y)\n \n# train the model\ndef build_model(train, n_input):\n    # prepare data\n    train_x, train_y = to_supervised(train, n_input)\n    # define parameters\n    verbose, epochs, batch_size = 0, 50, 16\n    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n    # reshape output into [samples, timesteps, features]\n    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n    model.add(RepeatVector(n_outputs))\n    model.add(LSTM(200, activation='relu', return_sequences=True))\n    model.add(TimeDistributed(Dense(100, activation='relu')))\n    model.add(TimeDistributed(Dense(1)))\n    model.compile(loss='mse', optimizer='adam')\n    # fit network\n    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    return model\n \n# make a forecast\ndef forecast(model, history, n_input):\n    # flatten data\n    data = np.array(history)\n    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n    # retrieve last observations for input data\n    input_x = data[-n_input:, :]\n    # reshape into [1, n_input, n]\n    input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n    # forecast the next week\n    yhat = model.predict(input_x, verbose=0)\n    # we only want the vector forecast\n    yhat = yhat[0]\n    return yhat\n\n","7ab390f1":"# evaluate a single model\ndef evaluate_model(train, test, n_input):\n    # fit model\n    model = build_model(train, n_input)\n    # history is a list of weekly data\n    history = [x for x in train]\n    # walk-forward validation over each week\n    predictions = list()\n    for i in range(len(test)):\n        # predict the week\n        yhat_sequence = forecast(model, history, n_input)\n        # store the predictions\n        predictions.append(yhat_sequence)\n        # get real observation and add to history for predicting the next week\n        history.append(test[i, :])\n    # evaluate predictions days for each week\n    predictions = np.array(predictions)\n    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n    return score, scores","97a6a814":"# load the new file\ndf = pd.read_csv('\/kaggle\/input\/householddaily\/household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n# split into train and test\ntrain, test = split_dataset(df)","76a96b24":"# evaluate model and get scores\nn_input = 14\nscore, scores = evaluate_model(train, test, n_input)\n\n# summarize scores\nsummarize_scores('lstm', score, scores)\n\n# plot scores\ndays = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\nplt.plot(days, scores, marker='o', label='lstm')\nplt.show()","f7665bca":"## Create the column remain of sub-metering\nThe remaining watt-hours can be calculated from the active energy by first converting the active energy to watt-hours then subtracting the other sub-metered active energy in watt-hours, as follows:\nsub_metering_remainder = (global_active_power * 1000 \/ 60) - (sub_metering_1 + sub_metering_2 + sub_metering_3)","76308b60":"## Time Series distributions","143f41b4":"# Household Power Consumption Dataset\nThe Household Power Consumption dataset is a multivariate time series dataset that describes the electricity consumption for a single household over four years.\n\nThe data was collected between December 2006 and November 2010 and observations of power consumption within the household were collected every minute.\n\nIt is a multivariate series comprised of seven variables (besides the date and time); they are:\n\n* global_active_power: The total active power consumed by the household (kilowatts).\n* global_reactive_power: The total reactive power consumed by the household (kilowatts).\n* voltage: Average voltage (volts).\n* global_intensity: Average current intensity (amps).\n* sub_metering_1: Active energy for kitchen (watt-hours of active energy).\n* sub_metering_2: Active energy for laundry (watt-hours of active energy).\n* sub_metering_3: Active energy for climate control systems (watt-hours of active energy).\n\n\n**Note:** In general terms, the active energy is the real power consumed by the household, whereas the reactive energy is the unused power in the lines.\n\n","70ac0ac9":"## 2. Modeling","601b24c0":"### 2.1 Evaluation metric\n\n* Each forecast will be composed of seven values, one for each day of the week ahead.\n* It is common with multi-step forecasting problems to evaluate each forecasted time step separately. This is useful for testing effecienty on a specific day as well as comparing models performance on different lead days (perf on day+1 vs. perf on day+5 for exemple).\n* We will use Root Mean Squared Error (RMSE) as a metric for evaluating forecast error for each single day.","b7cd48ea":"### Consumption per minute ","5bdd188a":"## Load data & quality check","aa66f11d":"## 1. Feature engineering\nWe will reload the initial data and apply: \n* Necassary transformations used in EDA\n* Fill missing values with a value at the same time one day ago\n\n**NB:** filling the missing values takes alot of time to run hense the commented code. By the end of this step we store daily consumption in a csv file that we use later on for modeling","b46db96b":"* The distribution of active power appears to be bi-modal, meaning it looks like it has two mean groups of observations.\n* We can investigate this further by looking at the distribution of active power consumption for the four full years of data.","276f7000":"### 2.3 Direct Multi-Step Forecasting\n\nWe are interested in forecasting seven days, this would require preparing seven different models, each specialized for forecasting a different day.Each model would predict a specific day of the standard week, e.g. Monday.\n\nWe create two specific functions specific to this method: \n* **to_supervised()** to prepare the data, such as the prior week of observations, used as input and an observation from a specific day in the following week used as the output.\n* **sklearn_predict()** function to create a new dataset and a new model for each day in the one-week forecast.","49fccd16":"# II. Machine Learning\nProblem definition: **Given recent power consumption, what is the expected power consumption for the week ahead?**\n\n* A predictive model will forecast the total active power for each day over the next seven days.\n* Technically, this is a multi-variate and multi-step time series forecasting model.\n* The results would be useful in planning electricity demand for a specific household\n\n","20e503a5":"### Build and evaluate model Encoder-Decoder LSTM","ded89372":"## Explore Global active power","485be743":"* We can see some common gross patterns across the years, such as around Feb-Mar and around Aug-Sept where we see a marked decrease in consumption\n* We also seem to see a downward trend over the summer months (middle of the year in the northern hemisphere) and perhaps more consumption in the winter months towards the edges of the plots. These may show an annual seasonal pattern in consumption.\n* We spot some missing data in the last year","c8d024be":"# I. EDA","1ed02089":"### Monthly consumption","d4e6e544":"* Running the example creates an image with 12 plots, one for each month in 2007.\n* We can see generally the same data distribution each month. The axes for the plots appear to align (given the similar scales), and we can see that the peaks are shifted down in the warmer northern hemisphere months and shifted up for the colder months.\n* We can also see a thicker or more prominent tail toward larger kilowatt values for the cooler months of December through to March.","f07ae2f1":"## Format all columns as float","4146f7b9":"## Useful functions","487c7c79":"### 2.5 Pipeline preparation","028894cb":"## 2.7 Encoder-Decoder LSTM Model With Multivariate Input and Recursive Multi-Step Forecasting\nIt involves making a prediction for one time step, taking the prediction, and feeding it into the model as an input in order to predict the subsequent time step. This process is repeated until the desired number of steps have been forecasted.\n\n**NB:** We re-define certain function to adapt to the recursive approach ","b70c76ed":"### 2.6 Linear models with Direct Multi-Step approach\n","c7c52609":"## Missing values","00783972":"## Rename columns with more significant names","e6176f62":"### 2.2 Train and Test Sets\n* We will use the first three years of data for training predictive models and the final year for evaluating models.\n* The data in a given dataset will be divided into standard weeks. These are weeks that begin on a Sunday and end on a Saturday.\n* We will split the data into standard weeks: \n    - **Test:** The first Sunday for 2010 was January 3rd and the closest final Saturday in the data is November 20th --> **46 standard weeks of test data**\n    - **Train:** The daily data starts in late 2006 and the first Sunday is December the 17th. The rest of the data untill the first date of test is for training --> **159 full standard weeks of train data**","d781788d":"* Running the example creates a single plot with four figures, one for each of the years between 2007 to 2010.\n* We can see that the distribution of active power consumption across those years looks very similar. The distribution is indeed bimodal with one peak around 0.3 KW and perhaps another around 1.3 KW.\n* There is a long tail on the distribution to higher kilowatt values. It might open the door to notions of discretizing the data and separating it into peak 1, peak 2 or long tail. These groups or clusters for usage on a day or hour may be helpful in developing a predictive model.","5426a6f3":"* There is commonality across the days; for example, many days consumption starts early morning, around 6-7AM.\n* For the first week of the year, the morning peaks are delayed which sugests an extension of the end of the year vacation   \n* Some days show a drop in consumption in the middle of the day, which might make sense if most occupants are out of the house.\n* We do see some strong overnight consumption on some days, that in a northern hemisphere January may match up with a heating system being used.\n* Time of year, specifically the season and the weather that it brings, will be an important factor in modeling this data, as would be expected.","313fcfdd":"* Global active decrease in hot weather specifically months 8 and 9, years 2008 -- 2010\n* Increase of energy climate control consumption first semester of 2009, possible new equipement \n* Decrease in remain energy matching with climate change increase ","989cea6f":"### Data Set Information\n\nThis archive contains 2075259 measurements gathered in a house located in Sceaux (7km of Paris, France) between December 2006 and November 2010 (47 months).\nNotes:\n* 1.(global_active_power*1000\/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\n* 2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.","e43076e9":"### 2.4 Walk-Forward Validation\nModels will be evaluated using a scheme called walk-forward validation.This is where a model is required to make a one week prediction, then the actual data for that week is made available to the model so that it can be used as the basis for making a prediction on the subsequent week. ","8a42326e":"## Plot each of the variables","ed5aacf2":"* We can see the sign-wave of power consumption of the days within each month. This is good as we would expect some kind of daily pattern in power consumption.\n* We can see that there are stretches of days with very minimal consumption, such as in August and in April. These may represent vacation periods where the home was unoccupied and where power consumption was minimal."}}