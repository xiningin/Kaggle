{"cell_type":{"890e3628":"code","dadea81a":"code","dca46c20":"code","f24e61f3":"code","0975d792":"code","55c19ead":"code","4b688ba4":"code","66ef1364":"code","cd2a0482":"code","c12f4a8a":"code","7ab058cb":"code","de57b1ef":"code","3d43ab44":"code","657a9413":"code","da0ed57f":"code","1b1f78ac":"code","5b35b47d":"code","d42102b2":"code","c5b52c48":"code","c5c52f64":"code","b19984b4":"code","8133571b":"code","58821fc7":"code","14f2547a":"code","e65b43c0":"code","ba57f2d9":"code","7a4d5fdf":"code","6a4d1519":"code","8df0bc19":"code","f209ceb1":"code","9fd07ac2":"code","d7e5c9e6":"markdown","ca283751":"markdown","14b04901":"markdown","ee993313":"markdown","d2b830b2":"markdown","9c32d2f8":"markdown","0583419c":"markdown","bb2398b1":"markdown","08eab434":"markdown","5a94cc62":"markdown","aa8db8c6":"markdown","a2d3d47c":"markdown","7a34b4e1":"markdown","c3b0fb6e":"markdown","f8aa54fa":"markdown","4a0621d4":"markdown","b1e7762b":"markdown"},"source":{"890e3628":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem import WordNetLemmatizer\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\nfrom sklearn.svm import SVC, LinearSVC\n\nimport re,string\nfrom wordcloud import WordCloud,STOPWORDS","dadea81a":"data = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\nprint(data.shape)\ndata.head()","dca46c20":"#Summary of the dataset\ndata.describe()","f24e61f3":"#Class Distrubution\ndata['sentiment'].value_counts()","0975d792":"## 0 as Negative and 1 as Positive\ndata.sentiment = data.sentiment.apply(lambda x: 0 if x=='negative' else 1)","55c19ead":"## Indirect features\neng_stopwords = set(stopwords.words(\"english\"))\n\ndata[\"count_words_title\"] = data[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ndata[\"count_stopwords\"] = data[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\ndata['count_word'] = data[\"review\"].apply(lambda x: len(str(x).split()))\ndata['count_unique_word'] = data[\"review\"].apply(lambda x: len(set(str(x).split())))\ndata[\"count_punctuations\"] = data[\"review\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ndata['word_unique_percent'] = data['count_unique_word'] * 100 \/ data['count_word']\ndata['punct_percent'] = data['count_punctuations'] * 100 \/ data['count_word']\n","4b688ba4":"## Reordering the columns \ndata = data[['review', 'count_words_title', 'count_stopwords',\n             'count_word', 'count_unique_word', 'count_punctuations',\n             'word_unique_percent', 'punct_percent','sentiment']]\ndata.head()","66ef1364":"plt.hist(data[data['sentiment']==0]['count_word'], range=(0,1500), color='red', edgecolor='black', \n         label='positive reviews', alpha=0.5)\nplt.hist(data[data['sentiment']==1]['count_word'], range=(0,1500), color='green', edgecolor='black', \n         label='negative reviews', alpha=0.1)\n\nplt.title('Word Count Distribution')\nplt.xlabel('Word Count')\nplt.legend()\nplt.show()","cd2a0482":"# Removing all punctuations from Text\nmapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\nPUNCT_TO_REMOVE = string.punctuation # '!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~'\neng_stopwords = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndef clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in eng_stopwords])\n\ndef word_replace(text):\n    return text.replace('<br \/>','')\n\ndef lemmatize_words(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ndef remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ndef preprocess(text):\n    text = clean_contractions(text, mapping)\n    text = text.lower()\n    text = word_replace(text)\n    text = remove_urls(text)\n    text = remove_html(text)\n    text = remove_stopwords(text)\n    text = remove_punctuation(text)\n    text = lemmatize_words(text)\n    \n    return text","c12f4a8a":"data[\"reviews_preprocessed\"]=data[\"review\"].apply(lambda text: preprocess(text))\ndata.head()","7ab058cb":"# Positive Reviews.\nplt.figure(figsize=(15, 15))\nwc = WordCloud(max_words=200, width=1000, height=500, stopwords=STOPWORDS).generate(\" \".join(data[data.sentiment==1].reviews_preprocessed))\nplt.imshow(wc, interpolation='bilinear')","de57b1ef":"# Negative Reviews.\nplt.figure(figsize=(15, 15))\nwc = WordCloud(max_words=200, width=1000, height=500, stopwords=STOPWORDS).generate(\" \".join(data[data.sentiment==0].reviews_preprocessed))\nplt.imshow(wc, interpolation='bilinear')","3d43ab44":"def metrics(model, x , y):\n    y_pred = model.predict(x)\n    acc = accuracy_score(y, y_pred)\n    f1 = f1_score(y, y_pred)\n    print(\"\\nAccuracy: \", round(acc,3))\n    print(\"\\nF1 Score: \", round(f1,3))\n    \n    cm=confusion_matrix(y, y_pred)\n    plt.figure(figsize=(4, 4))\n    sns.heatmap(cm, annot=True, cmap='coolwarm', xticklabels=[0,1], fmt='d', annot_kws={\"fontsize\":19})\n    plt.xlabel(\"Predicted\", fontsize=16)\n    plt.ylabel(\"Actual\", fontsize=16)\n    plt.show()","657a9413":"X = data[['count_words_title', 'count_stopwords',\n        'count_word', 'count_unique_word', 'count_punctuations',\n        'word_unique_percent', 'punct_percent']]\n\ny = data['sentiment']","da0ed57f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n[i.shape for i in [X_train, X_test, y_train, y_test]]","1b1f78ac":"linear_svc = LinearSVC(penalty='l2', dual=False)\nlinear_svc.fit(X_train, y_train)\nmetrics(linear_svc, X_test, y_test)","5b35b47d":"#dtype: string\ntexts = ' '.join(data['reviews_preprocessed'])\ntexts_to_list = texts.split(\" \")","d42102b2":"def draw_n_gram(texts_to_list, i):\n    n_gram = (pd.Series(nltk.ngrams(texts_to_list, i)).value_counts())[:11]\n    n_gram_df = pd.DataFrame(n_gram)\n    n_gram_df = n_gram_df.reset_index()\n    n_gram_df = n_gram_df.rename(columns={\"index\": \"word\", 0: \"count\"})\n    print(n_gram_df.head(10))\n    plt.figure(figsize=(10,5))\n    return sns.barplot(x='count', y='word', data=n_gram_df, palette=\"Blues_d\")","c5b52c48":"draw_n_gram(texts_to_list, 1)","c5c52f64":"draw_n_gram(texts_to_list, 2)","b19984b4":"draw_n_gram(texts_to_list, 3)","8133571b":"draw_n_gram(texts_to_list, 4)","58821fc7":"X_train, X_test, y_train, y_test = train_test_split(data['reviews_preprocessed'], data['sentiment'], test_size=0.2, random_state=0)\n[i.shape for i in [X_train, X_test, y_train, y_test]]","14f2547a":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 4),\n    max_features=8000\n)\n\nword_vectorizer.fit(data['reviews_preprocessed'])\n\ntfidf_train = word_vectorizer.transform(X_train)\ntfidf_test = word_vectorizer.transform(X_test)\n\nprint('Shape of tfidf_train:', tfidf_train.shape)\nprint('Shape of tfidf_test:', tfidf_test.shape)","e65b43c0":"word_vectorizer2 = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 4),\n    max_features=None\n)\n\nword_vectorizer2.fit(data['reviews_preprocessed'])\n\ntfidf_train2 = word_vectorizer2.transform(X_train)\ntfidf_test2 = word_vectorizer2.transform(X_test)\n\nprint('Shape of tfidf_train:', tfidf_train2.shape)\nprint('Shape of tfidf_test:', tfidf_test2.shape)","ba57f2d9":"linear_svc = LinearSVC(penalty='l2', dual=False)\nlinear_svc.fit(tfidf_train, y_train)\nmetrics(linear_svc, tfidf_test, y_test)","7a4d5fdf":"linear_svc = LinearSVC(penalty='l2', dual=False)\nlinear_svc.fit(tfidf_train2, y_train)\nmetrics(linear_svc, tfidf_test2, y_test)","6a4d1519":"cv=CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',\n                   ngram_range=(1,3),max_features=10000)\ncv.fit(data['reviews_preprocessed'])\ncv_train = cv.transform(X_train)\ncv_test = cv.transform(X_test)\nprint('Shape of cv_train:', cv_train.shape)\nprint('Shape of cv_test:', cv_test.shape)","8df0bc19":"cv4=CountVectorizer(analyzer='word', token_pattern = r'\\w{1,}',\n                    ngram_range=(1,4),max_features=10000)\ncv4.fit(data['reviews_preprocessed'])\ncv4_train = cv4.transform(X_train)\ncv4_test = cv4.transform(X_test)\nprint('Shape of cv_train:', cv4_train.shape)\nprint('Shape of cv_test:', cv4_test.shape)","f209ceb1":"linear_svc = LinearSVC(C=0.5, dual=False, random_state=42)\nlinear_svc.fit(cv_train, y_train)\n\nmetrics(linear_svc, cv_test, y_test)","9fd07ac2":"linear_svc = LinearSVC(penalty='l2', dual=False, random_state=42)\nlinear_svc.fit(cv4_train, y_train)\n\nmetrics(linear_svc, cv4_test, y_test)","d7e5c9e6":"<h1>Feature Engineering<\/h1>\n\n### Indirect features:\n- count of sentences\n- count of title words\n- count of stop words\n- count of words\n- count\/percentage of unique words\n- count\/percentage of punctuations","ca283751":"- From these word clouds, we are not able to judge any starling differences in both the sentiments by looking at words. We don\u2019t see usage of extreme negative connotation or abusive language used while writing negative reviews.","14b04901":"## Quadgram Analysis","ee993313":"## Text Preprocessing of Reviews","d2b830b2":"### Change Target variable","9c32d2f8":"## Bigram Analysis","0583419c":"- As expected, this model is giving us poor accuracy of 58% as we depicted in EDA. Indirect features have very similar trends and patterns across both the classes, we have seen in EDA portion.","bb2398b1":"## Unigram Analysis","08eab434":"# Model based on Indirect Features","5a94cc62":"**Problem Statement:**\n\nIn this, we have to predict the number of positive and negative reviews based on sentiments by using different classification models.","aa8db8c6":"### Utility Function","a2d3d47c":"## Trigram Analysis","7a34b4e1":"# Sentiment Analysis of IMDB Movie Reviews","c3b0fb6e":"### **2) Count Vectorizer-** \n","f8aa54fa":"<b> In machine learning task, cleaning or pre-processing the data is as important as model building if not more. And when it comes to unstructured data like text, this process is of most importance. IMDB reviews are posted by users manually, so we observe high usage of contractions and chat words in it. Also, some reviews are collected from other sites, so we also observe usage of many HTML tags in dataset.<\/b>\n\n**a. Clean Contractions or Chat Words:**\nAs this is manually entered reviews, people do use a lot of abbreviated words in chat and so it is important for us to expand all such chat words and contractions used. I\u2019ve used list of slangs and contractions from repo.\n\n**b. Lower Casing** Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way. This is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts \/ tfidf values.\n\n**c. Removal Of Stop Words**\nStopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. These stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language is,\n\n**d. Lemmatization**\nLemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language. As a result, this one is generally slower than stemming process. I\u2019m using standard WordNetLemmatizer for work.\n\n**e. Removal Of Urls & HTML Tags:**\nWe found large usage of HTML tags in dataset. To make sense of dataset, such tags to be removed.\n\n**f. Removal Of Punctuations** In this process, we remove the punctuations (!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~) from the text data. This is a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way. Note of caution- This process has to be performed after removal of HTML tags else some standard tags of HTML will partially get removed in this process and afterwards HTML removal process will not give suitable results.","4a0621d4":"# Word Cloud","b1e7762b":"## N-gram Analysis\n- The order that words are used in text is not random. In English, for example, you can say \"the red apple\" but not \"apple red the\". The general idea is that you can look at each pair (or double, triple etc.) of words that occur next to each other. In a sufficently-large corpus, you're likely to see \"the red\" and \"red apple\" several times, but less likely to see \"apple red\" and \"red the\". This is useful to know if, for example, you're trying to figure out what someone is more likely to say to help decide between possible output for an automatic speech recognition system. These co-occuring words are known as \"n-grams\", where \"n\" is a number saying how long a string of words you considered."}}