{"cell_type":{"1a061ba9":"code","ee9cbfee":"code","65775bea":"code","8bec680f":"code","d757aae8":"code","cb9fef0a":"code","3c3ce433":"code","146c4c9b":"code","5b703156":"code","67a5aa10":"code","3003fdcc":"code","d78b29a6":"code","6af55454":"code","bcc273c4":"code","8b54a0bd":"code","0578b20e":"code","926c59fb":"code","effcaa51":"code","ecd2c2e6":"code","5a01701a":"code","af99a0df":"code","c108db37":"code","5ea2f727":"code","1eba066d":"code","07d1561d":"code","c0e9ca52":"code","5aacfd32":"code","58efe416":"code","1eadf731":"markdown","0c9a9a6e":"markdown","3484e474":"markdown","171bcc89":"markdown","8b69a8ee":"markdown","b17eb401":"markdown","334ba09f":"markdown","f2719135":"markdown","6fb14ab4":"markdown","82552260":"markdown","e6d996cb":"markdown","bb9f53be":"markdown","ceee9567":"markdown","f9ef7ed0":"markdown","ab308c25":"markdown","18760f8e":"markdown","262c2291":"markdown","1120dbe9":"markdown","19819704":"markdown","1805f3a0":"markdown","8e7ec444":"markdown","a453e4c9":"markdown","061de33f":"markdown","b0ebf070":"markdown","2d2054ce":"markdown","8e4deb49":"markdown","add14fde":"markdown","ef69ac69":"markdown","0a78d64a":"markdown","5ab7d6c5":"markdown","89a6d2d8":"markdown","cd51a464":"markdown","f1017f38":"markdown","88ec4acc":"markdown","3160bd02":"markdown","51b60dc7":"markdown","c7cdda0e":"markdown","b0aae6c1":"markdown","b411c454":"markdown"},"source":{"1a061ba9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","ee9cbfee":"# Basic\nimport numpy as np\nimport pandas as pd\n\n# Set random seed\nnp.random.seed(1)\n\n# Plottiing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Label encoding\nfrom keras.utils.np_utils import to_categorical\n\n# Train val split\nfrom sklearn.model_selection import train_test_split\n\n# Modelling\nfrom keras import models\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# Evaluation\nfrom sklearn.metrics import confusion_matrix","65775bea":"X_train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nX_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\nprint('Shape of the training data: ', X_train.shape)\nprint('Shape of the test data: ', X_test.shape)","8bec680f":"X_train.head(1)","d757aae8":"# Extracting out y_train\n\ny_train = X_train['label']\n\n# Dropping the label from X_train\n\nX_train.drop(labels = ['label'], axis=1, inplace=True)","cb9fef0a":"#---- CHECK FOR NULL VALUES ----#\n\nprint('Null values in training data: ',X_train.isna().any().sum() == 'True')\nprint('Null values in test data: ',X_test.isna().any().sum() == 'True')\n\n\n#---- NORMALIZATION ----#\n\nX_train = X_train \/ 255.0\nX_test = X_test \/ 255.0\n\n\n#---- VISUALISE IMAGES ----#\n\nx = X_train[:10]\nx = x.values.reshape(x.shape[0], 28, 28)\n\nfor i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.axis('off')\n    plt.imshow(x[i],cmap=plt.cm.binary)\n\n    \n#---- RESHAPE DATA ----#\n\nX_train = X_train.values.reshape(-1, 28, 28, 1)\nX_test = X_test.values.reshape(-1, 28, 28, 1)\n\n\n#---- CLASS IMBALANCE ----#\n\nprint('\\nCount for each class')\nprint(y_train.value_counts())\nsns.countplot(y_train)\n","3c3ce433":"#---- LABEL ENCODING ----#\n\ny_train = to_categorical(y_train, num_classes=10)\n\n\n#---- TRAIN-VALIDATION SPLIT ----#\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1)","146c4c9b":"#---- CNN MODEL ----#\n\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\n\nmodel.summary()","5b703156":"#---- OPTIMIZER ----#\n\noptimizer = Adam(learning_rate=0.001, epsilon=1e-07)\n\n#---- COMPILING ----#\n\nmodel.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])\n\n#---- LEARNING RaTE REDUCTION ----#\n\nearlyStopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=0, mode='auto')\nmcp = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_accuracy', mode='auto')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='auto')\n\n","67a5aa10":"datagen = ImageDataGenerator(\n    rotation_range=5,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=5,\n    zoom_range=0.1)\n\ndatagen.fit(X_train)","3003fdcc":"history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=64),\n                              epochs = 100, \n                              validation_data = (X_val,y_val),\n                              verbose = 1, \n                              steps_per_epoch=X_train.shape[0]\/\/64, \n                             callbacks = [earlyStopping, mcp, reduce_lr_loss])","d78b29a6":"plt.figure(figsize=(6, 4))\nplt.plot(history.history['loss'], color='b', label=\"Training loss\")\nplt.plot(history.history['val_loss'], color='r', label=\"validation loss\")\nplt.legend(loc='best')\nplt.title('Training loss and Validation Loss across epochs')\nplt.show()\n\nplt.figure(figsize=(6, 4))\nplt.plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nplt.plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nplt.legend(loc='best')\nplt.title('Training loss and Validation Accuracy across epochs')\nplt.show()","6af55454":"y_pred = model.predict(X_val)\ny_pred = np.argmax(y_pred, axis=1)\ny_val_test = np.argmax(y_val, axis=1)\ncm = confusion_matrix(y_val_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g', cbar=False)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for predicted and true labels')\nplt.show()","bcc273c4":"model.load_weights(filepath = '.mdl_wts.hdf5')","8b54a0bd":"scores = model.evaluate(X_val, y_val, callbacks = [earlyStopping, mcp, reduce_lr_loss])","0578b20e":"rows = 2\ncols = 10\n\nplt.figure(figsize=(16, 4))\nfor i in range(0,20):\n    ax = plt.subplot(rows, cols, i+1)\n    plt.axis('off')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.imshow(X_val[i].reshape(28, 28), cmap=plt.cm.binary)\n    plt.title('Predicted: {}\\nTrue: {}'.format(y_pred[i], y_val_test[i]))\n","926c59fb":"#---- FINDING OUT THE IMAGES THAT WERE INCORRECTLY PREDICTED ----#\n\nls = np.array(y_pred - y_val_test) # Subtract true values from predicted --> All non-zero results were incorrectly predicted\n\nnonzero_pred = np.nonzero(ls)[0]\n\nrows = 3\ncols = (int)(np.ceil(nonzero_pred.size\/3))\n\nplt.figure(figsize=(10, 7))\nfor i in range(0, nonzero_pred.size):\n    ax = plt.subplot(rows, cols, i+1)\n    plt.axis('off')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.imshow(X_val[nonzero_pred[i]].reshape(28, 28), cmap=plt.cm.binary)\n    plt.title('Predicted: {}\\nTrue: {}'.format(y_pred[nonzero_pred[i]], y_val_test[nonzero_pred[i]]))","effcaa51":"test = X_test[0:10,:,:,:]\ntest_pred = np.argmax(model.predict(X_test), axis=1)\n\nplt.figure(figsize=(18,1))\nfor i in range(0,10):\n    ax = plt.subplot(1, 10, i+1)\n    plt.axis('off')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.imshow(test[i].reshape(28, 28), cmap=plt.cm.binary)\n    plt.title('Predicted: {}'.format(test_pred[i]))","ecd2c2e6":"img_tensor = X_test[5].reshape(-1, 28, 28, 1)","5a01701a":"for layer in model.layers:\n    if'conv' in layer.name:\n        filters, biases = layer.get_weights()\n        \n        print('Layer: ', layer.name, filters.shape)\n        \n        f_min, f_max = filters.min(), filters.max()\n        filters = (filters - f_min) \/ (f_max - f_min)\n        \n        print('Filter size: (', filters.shape[0], ',', filters.shape[1], ')')\n        print('Channels in this layer: ', filters.shape[2])\n        print('Number of filters: ', filters.shape[3])\n        \n        count = 1\n        plt.figure(figsize = (18, 4))\n        \n        # Plotting the first channel of every filter\n        for i in range(filters.shape[3]):\n            \n            ax= plt.subplot(4, filters.shape[3]\/4, count)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            plt.imshow(filters[:,:,0, i], cmap=plt.cm.binary)\n            count+=1\n            \n        plt.show()\n        ","af99a0df":"# Extract outputs of top 8 layers\nlayer_outputs = [layer.output for layer in model.layers[0:8]]\n\n# Create a model that will return these outputs given the model input\nactivation_model = models.Model(inputs = model.input, outputs = layer_outputs)","c108db37":"activations = activation_model.predict(img_tensor)","5ea2f727":"first_layer_activation = activations[0]\nprint(first_layer_activation.shape)","1eba066d":"ax = plt.subplot(1, 2, 1)\nax.set_xticks([])\nax.set_yticks([])\nplt.imshow(first_layer_activation[0,:,:,0], cmap = plt.cm.binary)\nplt.title('1st Filter')\n\nax = plt.subplot(1, 2, 2)\nax.set_xticks([])\nax.set_yticks([])\nplt.imshow(first_layer_activation[0,:,:,22], cmap = plt.cm.binary)\nplt.title('23rd Filter')\n    \nplt.show()","07d1561d":"layer_names = []\nfor layer in model.layers[:8]:\n    layer_names.append(layer.name)\n\nfor activation_layer, layer_name in zip(activations, layer_names):\n\n    n_features = activation_layer.shape[3]\n    feat_per_row = 16\n    rows = n_features\/\/feat_per_row\n    size = activation_layer.shape[1]\n    \n    print(layer_name)\n    plt.figure(figsize=(20, rows))\n    for i in range(n_features):\n        ax = plt.subplot(rows, feat_per_row, i+1)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        plt.imshow(activation_layer[:,:,:,i].reshape(size, size))#, cmap = plt.cm.binary)\n    \n    plt.show()","c0e9ca52":"# predict the results\n\nresults = np.argmax(model.predict(X_test), axis=1)\n\nresults = pd.Series(results, name = \"Label\")\n\nresults.head(2)","5aacfd32":"# Submission \n\nsubmission = pd.concat([pd.Series(range(1, 28001), name = \"ImageId\"), results], axis = 1)\n\nsubmission.head(2)","58efe416":"submission.to_csv(\"CNN_MNIST_results.csv\",index=False)","1eadf731":"Below, I have extracted out the top 8 layer outputs of the model. Then I have created another activation model with the original model input and the layer outputs as *layer_outputs*. ","0c9a9a6e":"* The number of images in our training set is 42000 and test set is 28000.\n\n* The size for each image is 28 x 28 pixels (784 pixels in total).\n\n* The first column in the dataset is the label, i.e., the number that is in the image. We will separate it out to form our y_train.","3484e474":"#### *DATA AUGMENTATION*\n\nWith the aim to generalize the model as much as possible, we employ this method. Our ImageDataGenerator takes the original images and applies a series of random transformations and **replaces** the original dataset with a **new transformed** dataset. ImageDataGenerator is an on-the-fly data augmentation as it takes images in batches and sends the batch out to train the model and repeats that until the images are exhausted. \n\nTransformations I have used:\n1. Random Rotations\n2. Width shifts\n3. Height shifts\n4. Shear in the CCW direction\n5. Random Zoom","171bcc89":"# MNIST --> CNN --> Visualisation (Filters and Feature Maps) \n\nHey guys!\n\nI have done very basic projects in Image Recognition in the past that involved making the model and simply predicting results. \n\nMaking a model seems very easy at times but a lot of data preprocessing, parameter tuning for the model, hyper parameter tuning, is involved in the process along with the learning process of the model that is just hidden in plain sight. \n\nThe aim of this notebook was to understand the ***inner working of Convolutional Neural Network*** in image recognition. Hyper Parameter tuning, parameter tuning for the construction of the model has not been dealt with in detail in this notebook. \n\nNOTE: The notebook might not have 0.9957 as a validation accuracy. I assure you I did end up with that accuracy a couple of times. With multiple runs, the model trains in the range (0.9951 to 0.9963).","8b69a8ee":"The first 10 images have been correctly predicted as we can see above. \n\nNow, we will choose a particular image which we will use in the later sub-section to view the features learnt by the model. Click [here to go to that section](#4.5).\n\nThe image we have chosen is the 6th image: '7'.","b17eb401":"### *ARCHITECTURE*\n\nLet us discuss the **significance of different layers** that are used in a convolutional neural network.\n\n**1. Convolutional Layer:** The aim is to extract and learn high level features of an image, a way that would not require us to hard code the features for recognition later on. How do we achieve this? \n\nThe dimension of our image is (28, 28, 1). Our first convolutional layer, for example, creates filters of size (5, 5, 1). These filters are applied on the image such that, the first position of the filter would be on the top left corner of the image. *Convolution* will be carried out (element wise multiplications (25 here) and then added) to give the value of the first pixel for the processed image (***feature map***). Like this, the filter keeps moving throughout the image to give us a final feature map that can be used for further training.\n\nNote: Using n number of filters (5, 5, 1) gives us an output volume of (28, 28, 1, n). \n\n*How it replicates the human eye?* Spatial variations are retained better when we use higher number of filters. COnvolution layers tend to extract high level features that the humans subconsciously register when they look at an image. \n","334ba09f":"<a id='5'><\/a>\n\n## 5. SUBMISSION","f2719135":"- We have applied *max-min normalization* to the values of the filters to make it easy to visualise the filters.\n\n- We have only visualised the first channel of every filter across all the convolutional layers. Visualising every channel would have been a tedious task.\n\n- These filters are learnable filters that learn some visual feature as the model gets trained over images across epochs.","6fb14ab4":"**4. Fully Connected Layers (Flatten, Dense):** The fature map generated by the above layers is flattenned out and then the neural network simply learns a linear\/non-linear function that helps the model recognize the image. ","82552260":"Here I will use the *img_tensor* to visualise the feature maps for intermediate activation layers. \n\nLet us first check out the channels of the first activation layer. ","e6d996cb":"### *ALL ACTIVATION LAYERS*","bb9f53be":"I have read a lot about what happens under the hood in these models but have never tried out for myself. I am trying it out below.\nCheers to first times!\n\nThis section contains the following sub-sections:\n\n1. [Predicted Validation Set Images](#4.1)\n2. [Errors in Prediction](#4.2)\n3. [Predict Test Images](#4.3)\n4. [Filters](#4.4)\n5. [Activation Layers](#4.5)\n\nClick on any section you want to view!","ceee9567":"<a id='4'><\/a>\n\n## 4. VISUALIZATION","f9ef7ed0":"### *PREPROCESSING STEPS*\n\n1. **Check for null values in both the training and test data**\n   * This is to check if there is a null image lying in the dataset.\n\n\n2. **Normalizing data**\n    * We generally normalize the data of any image between 0 to 1 for both greyscale and RGB images. This is done so the CNN we use to create a model processes inputs with small input values optimising the time and space required to train the model.\n\n\n3. **Reshape the data (Visualise few images)**\n    * We have received the data for an image in a 1D array. We need to change it to a *4D array*.\n    * 2 dimensions will be for the image: 28 x 28 pixels.\n    * 3rd dimension will be the channel for the image. The *MNIST images are in grayscale* so the channel will be 1. If it were an RGB image, the channel would be 3.\n    * 4th dimension signifies the number of images in our dataset. (Think of it as images being stacked on top of each other and to be sent to the model for training.)\n\n\n4. **Check for class imbalance**\n    * Class imbalance can be a real trouble when training a model. Some class that is really underrepresented will not be correctly predicted in the result either. (Much like how under representation in society works! Came as a fleeting thought while reading about it!)\n    * One of the best methods to solve this problem is *Image Augmentation* through different Image Augmentation techniques for the unbalanced classes.\n    * It might not be required here since this is a standard dataset.\n    * Who knows though? Read on to find out!\n\n\n5. **Label Encoding to One Hot Encoding format**\n    * It is expected to keep the labels in a one hot encoded format by NN. \n\n\n6. Split training data into training and validation data\n    * You know it! Training dataset to train the model and validation data to validate the model. \n    * Pretty intuitive!","ab308c25":"<a id='4.3'><\/a>\n\n### 4.3 PREDICT TEST IMAGES","18760f8e":"It was quite a long notebook. I was actually studying through this notebook,thus a little theory too.\n\nThanks for going through it all! \nHope you liked it!\n\n***Please comment below if you liked it or if you have questions, I would love to discuss about it.***\n\n***Some upvotes would really be a good motivation for me. Thank you!***","262c2291":"#### *RESULTS*\n\n| Architecture | Callbacks | Training Loss | Training Accuracy | Validation Loss | Validation Accuracy | Remarks |\n| ---- | ---- | ---- | ---- | ---- | ---- | ----|\n|32-64-64-128 | None | 0.0189 | 0.9948 | 0.0248 | 0.9938|\n|32-64-128 | None | 0.0232 | 0.9936 | 0.0209 | 0.9948 | Converging took a long time |\n|32-64-128-256 | None | 0.0212 | 0.9947 | 0.0403 | 0.9938 |\n|32-64-128-256 | ReduceLROnPlateau | 0.0054 | 0.9983 | 0.0205 | 0.9950 | Monitor: val_loss |\n|32-64-128-256 | ReduceLROnPlateau, ModelCheckpoint |  0.0074 | 0.9977 | 0.0213 | 0.9948 | Monitor: val_accuracy |\n|32-64-128-256 | ReduceLROnPlateau, ModelCheckpoint, EarlyStopping | 0.0100 | 0.9971 | 0.0181 | 0.9948 | Monitor: val_loss |\n|32-64-128-256 | ReduceLROnPlateau, ModelCheckpoint, EarlyStopping | 0.0134 | 0.9960 | 0.0169 | 0.9957| Monitor: val_accuracy (***FINAL***)|\n\n","1120dbe9":"We can see that this layer has tried to understand the shape of the numberin the image. \n\nLet us now visualise all the feature maps across the first 8 layers.","19819704":"The first activation layer is a convolutional layer that has:\n\n1. 1 channel\n2. Image size (28 x 28)\n3. 32 filters\n\n--> (1, 28, 28, 32)","1805f3a0":"To find out the images that were wrongly predicted, I have followed the following steps:\n\n- Subtract the predicted labels from the true labels. The correctly predicted labels will result to 0 while the incorrect predictions will gie a non zero value.\n- Extract the indices of the non-zero values. \n- Use these same indices to display the images from the validation set of images.\n- Display the predicted and true labels. ","8e7ec444":"This section prints out the validation set images that were given as an input along with the predicted and true labels. \n\nThe validation accuracy of the model is 0.9957, thus it is most likely that we will see all correctly identified labels. \n\nWe have will have some errors though, to see that go to the [next section](#4.2).","a453e4c9":"<a id='3'><\/a>\n\n## 3. MODEL EVALUATION","061de33f":"Predicting the activation layer feature maps using the *img_tensor* below.","b0ebf070":"**Optimizer:** Optimizers are algorithms that reduce the losses in the model by changing the learning rate and weights (binds the loss function and model parameters). \n\nFor example, let's say that you are playing a blindfold game with your friend. The rules of the game are: one person has to be blindfolded, the sighted person has to instruct that person to follow the path (blindfolded person cannot see the path) and reach the end in the least time possible. In this scenario, when you friend will be shouting out instructions to you and and lead you to the end, that is exactly what optimizers do (albeit, converging in the *shortest time* is not a concern here!).","2d2054ce":"We do not have the true labels for the test set so we will just see what predictions we get.","8e4deb49":"**3. Dropout:** This layer randomly sets inputs to 0 to prevent the model form overfitting. This help the model to generalize it. \n","add14fde":"The best validation accuracy I could attain was in the last setup of the model and callbacks. ","ef69ac69":"<a id='4.5'><\/a>\n\n### 4.5 ACTIVATION LAYERS","0a78d64a":"**EarlyStopping:** This callback will stop training when the monitored metric (here, val_accuracy) has stopped changing (here, mode = 'auto' which means 'max' (increasing)) for a 'patience' number of epochs (here, 10).\n\n**ReductLROnPlateau:** Sometimes, model benefit when learning rate is decreased after it stagnates. This callback lets you reduce the learning rate by a factor (here, 0.1) if the monitored metric (here, val_accuracy) has stopped changing (here, mode = 'auto' which means 'mas' (increasing)) for a 'patience' number of epochs (here, 7).\n\n**ModelCheckpoint:** This callback helps you save the model of the weights of the model depending on your choice.","5ab7d6c5":"This notebook goes through the following parts below. Click on any section if you wish to move on to that.\n\n1. [Data Preprocessing](#1)\n2. [Model Construction](#2)\n3. [Model Evaluation](#3)\n4. [Visualization](#4)\n    1. Predicted Validation Set Images\n    2. Errors in Prediction\n    3. Predict Test Images\n    4. Filters\n    5. Activation Layers\n\n\n5. [Submission of results](#5)","89a6d2d8":"***CONCLUDING THE SECTION***\n\n* We have successfully worked on every step.\n\n* It will be good to note that the classes are almost balanced. \n\n*We can move on the next section without worries!*","cd51a464":"**2. Max Pooling:** This layer aims to *reduce the spatial size* of the feature map. This dimensionality reduction helps to make use of *less computation power*. Dominant features are also extracted by this layer. \n\nThis layer extracts the maximum value from the area covered by the kernel on the image.","f1017f38":"#### *OBSERVATION*\n\n- The first convolutional activation layer retains the shape of the number. Almost all the information of the image is preserved. \n\n- The second convolutional activation layer is a little more abstract. The entire information of the image is not retained. We can see that it has tried to capture higher features of the image: straight lines, edges, angles.\n\n- The third and fourth convolutional activation layers are even more abstract. The higher features are captured here in the most rudimentary form. Layers here are more about the class of the images (that is, the numbers 0, 1, 2 ... 9) than the image itself. \n\n- A lot of feature maps in the last layers are not activating which signifies that it has no new feature to learn.","88ec4acc":"<a id='4.1'><\/a>\n\n### 4.1 PREDICTED VALIDATION SET IMAGES","3160bd02":"<a id='4.2'><\/a>\n\n### 4.2 ERRORS IN PREDICTION","51b60dc7":"<a id='1'><\/a>\n\n## 1. DATA PREPROCESSING","c7cdda0e":"<a id='4.4'><\/a>\n\n### 4.4 FILTERS","b0aae6c1":"Metrics of the final model when tested on the validation dataset can be seen below. ","b411c454":"<a id='2'><\/a>\n\n## 2. MODEL CONSTRUCTION"}}