{"cell_type":{"944fb5b1":"code","b7734988":"code","c96fad65":"code","0b98f473":"code","75a7c726":"code","1a888aa5":"code","992c3c9f":"code","1ae6e0ab":"code","7c941694":"code","7a2f2f1d":"code","56f21e13":"markdown","0c431ee5":"markdown","88c2ed51":"markdown","280f7aa5":"markdown","a4f3c659":"markdown"},"source":{"944fb5b1":"!pip install git+https:\/\/github.com\/ucals\/hamiltorch.git","b7734988":"from datetime import date\nimport hamiltorch\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom tqdm import tqdm","c96fad65":"val_ratio = 0.1\nbatch_size = 100\nnum_epochs = 400\nmodel_file = f'model-{date.today():}.pth'\nlearning_rate = 1e-3","0b98f473":"class UserDataset(Dataset):\n    def __init__(self, transform=None):\n        self.transform = transform\n        self.dfu = pd.read_csv(\n            '..\/input\/movielens-1m\/ml-1m\/users.dat', \n            delimiter='::', header=None)\n        self.dfu.columns = ['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code']\n        self.dfr = pd.read_csv(\n            '..\/input\/movielens-1m\/ml-1m\/ratings.dat', \n            delimiter='::', header=None)\n        self.dfr.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']\n\n    def __len__(self):\n        return len(self.dfu)\n\n    def __getitem__(self, item):\n        user = self.dfu.iloc[item]\n        ratings = self.dfr[self.dfr['UserID'] == user['UserID']].\\\n            drop(columns=['UserID', 'Timestamp'])\n\n        sample = {\n            'user_data': user,\n            'ratings': ratings,\n            'num_ratings': len(ratings)\n        }\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample","75a7c726":"# helper transform \nclass ToTensor:\n    def __call__(self, sample):\n        return {\n            'user_id': sample['user_data']['UserID'],\n            'ratings': torch.from_numpy(sample['ratings'].values),\n            'num_ratings': sample['num_ratings']\n        }","1a888aa5":"# helper function to collate batches with irregular sizes\ndef collate_fn(batch):\n    user_ids = []\n    ratings = []\n    num_ratings = []\n    for sample in batch:\n        user_ids.append(sample['user_id'])\n        ratings.append(sample['ratings'])\n        num_ratings.append(sample['num_ratings'])\n\n    return {\n        'user_id': torch.tensor(user_ids),\n        'ratings': torch.cat(ratings, dim=0),\n        'num_ratings': torch.tensor(num_ratings)\n    }","992c3c9f":"class Model(nn.Module):\n    def __init__(self, num_movies=3953, num_latent=10):\n        super(Model, self).__init__()\n        self.num_movies = num_movies\n        self.num_latent = num_latent\n        self.emb = nn.Embedding(num_movies, 8)\n        self.fc1 = nn.Linear(8, 100)\n        self.fc2 = nn.Linear(100, num_latent * 2)\n        self.fc3 = nn.Linear(num_latent, 100)\n        self.fc4 = nn.Linear(100, num_movies)\n        self.bn1 = nn.BatchNorm1d(100)\n        self.bn2 = nn.BatchNorm1d(num_latent * 2)\n        self.bn3 = nn.BatchNorm1d(100)\n        self.bn4 = nn.BatchNorm1d(num_movies)\n        self.relu = nn.ReLU()\n\n    def encode(self, ratings, num_ratings):\n        emb = self.emb(ratings[:, 0].long())\n        x = emb * ratings[:, 1].view(-1, 1)\n\n        stack = []\n        i0 = 0\n        for i in range(num_ratings.size(0)):\n            i1 = i0 + num_ratings[i]\n            slc = x[i0:i1]\n            assert slc.size(0) == num_ratings[i]\n            stack.append(slc.sum(dim=0))\n            i0 = i1\n\n        x = torch.stack(stack, dim=0)\n        assert x.size() == (num_ratings.size(0), 8)\n\n        x = self.bn1(self.relu(self.fc1(x)))\n        x = self.bn2(self.relu(self.fc2(x)))\n\n        mu = x[:, :self.num_latent]\n        log_var = x[:, self.num_latent:]\n        return mu, log_var\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(log_var\/2)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        x = self.bn3(self.relu(self.fc3(z)))\n        x = self.bn4(self.relu(self.fc4(x)))\n        return x\n\n    def forward(self, ratings, num_ratings, use_hmc=False, num_samples=10,\n                num_steps=20, device=None):\n        mu, log_var = self.encode(ratings, num_ratings)\n\n        if not use_hmc:\n            z = self.reparameterize(mu, log_var)\n            x_hat = self.decode(z)\n            return x_hat, mu, log_var\n\n        else:\n            # This Hamiltonian Monte Carlo implementation is too slow\n            # TODO: figure out a way to accelerate this sampling\n            if device is None:\n                device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n                \n            x_hat = []\n            std = torch.exp(log_var\/2)\n            for i in range(mu.size(0)):\n                params_init = torch.zeros(self.num_latent).to(device)\n                params_hmc = hamiltorch.sample(\n                    log_prob_func=log_prob_func(mu[i], std[i]),\n                    params_init=params_init,\n                    num_samples=num_samples + 1,\n                    step_size=0.3,\n                    num_steps_per_sample=num_steps,\n                    silent=True\n                )\n                zs = torch.stack(params_hmc[1:], dim=0)\n                x_hats = self.decode(zs).mean(dim=0)\n                x_hat.append(x_hats)\n\n            x_hat = torch.stack(x_hat, dim=0)\n            assert x_hat.size() == (mu.size(0), self.num_movies)\n            return x_hat, mu, log_var\n\n\ndef log_prob_func(mean, stddev):\n    def lpf(params):\n        return torch.distributions.Normal(mean, stddev).log_prob(params).sum()\n\n    return lpf","1ae6e0ab":"# helper functions\ndef mask(x_hat, x, num_ratings):\n    # filter ratings, to calculate loss only in the observations\n    stack = []\n    i0 = 0\n    for i in range(x_hat.size(0)):\n        i1 = i0 + num_ratings[i]\n        movie_ids = x[i0:i1, 0].long()\n        assert movie_ids.size(0) == num_ratings[i]\n        i0 = i1\n\n        preds = x_hat[i, movie_ids]\n        stack.append(preds)\n\n    x_pred = torch.cat(stack, dim=0)\n    x_true = x[:, 1]\n    return x_pred, x_true\n\n\ndef custom_loss(x_hat, x, num_ratings, mu, log_var):\n    x_pred, x_true = mask(x_hat, x, num_ratings)\n\n    # now, calculate the loss\n    criterion = torch.nn.MSELoss()\n    mse = criterion(x_pred, x_true)\n    kld = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n    return mse + kld, mse","7c941694":"# load data and initialize\ndataset = UserDataset(transform=ToTensor())\n\nval_size = int(len(dataset) * val_ratio)\ntrain_size = len(dataset) - val_size\ntrainset, valset = random_split(dataset, [train_size, val_size])\ndataloaders = {\n    'train': DataLoader(trainset, batch_size=batch_size, shuffle=True,\n                        collate_fn=collate_fn),\n    'val': DataLoader(valset, batch_size=batch_size, shuffle=False,\n                      collate_fn=collate_fn)\n}\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = Model().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nbest_mse = np.inf","7a2f2f1d":"# training loop\nfor epoch in range(num_epochs):\n    # Each epoch has a training and validation phase\n    for phase in ['train', 'val']:\n        if phase == 'train':\n            model.train()  # Set model to training mode\n        else:\n            model.eval()   # Set model to evaluate mode\n\n        running_loss = 0.0\n        running_mse = 0.0\n        running_preds = 0\n        bar = tqdm(dataloaders[phase])\n        bar.set_description(f'Epoch {epoch} {phase}'.ljust(20))\n\n        for batch in bar:\n            x = batch['ratings'].float().to(device)\n            num_ratings = batch['num_ratings'].to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward\n            # track history if only in train\n            with torch.set_grad_enabled(phase == 'train'):\n                x_hat, mu, log_var = model(x, num_ratings)\n                loss, mse = custom_loss(x_hat, x, num_ratings, mu, log_var)\n\n                # backward + optimize only if in training phase\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n            running_loss += loss.item() * x.size(0)\n            running_mse += mse.item() * x.size(0)\n            running_preds += x.size(0)\n\n            bar.set_postfix(loss=f'{running_loss \/ running_preds:0.6f}',\n                            mse=f'{running_mse \/ running_preds:0.6f}')\n\n        epoch_mse = running_mse \/ running_preds\n        if epoch_mse < best_mse:\n            best_mse = epoch_mse\n            torch.save(model.state_dict(), model_file)","56f21e13":"# 5. Training","0c431ee5":"# 3. Dataset interfance","88c2ed51":"# 2. Imports and global variables","280f7aa5":"# 1. Partial VAE for Hybrid Recommender System\n\nThis code reproduces the paper [Partial VAE for Hybrid Recommender System](https:\/\/www.semanticscholar.org\/paper\/Partial-VAE-for-Hybrid-Recommender-System-Ma-Gong\/e81b4bc889e99ad3ebd588eef12a0216a015d7bf).","a4f3c659":"# 4. Model"}}