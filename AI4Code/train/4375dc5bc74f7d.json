{"cell_type":{"61878a6d":"code","9567311a":"code","e5d95dc3":"code","02b16b77":"code","6d334435":"code","caf9962d":"code","3395ab50":"code","ded276f3":"code","8e4de83c":"code","c77e4300":"code","332a3710":"code","22faa573":"code","19c30bd9":"code","6213a358":"code","e54f401a":"code","2995c3b9":"code","61526199":"code","e39e24ab":"code","5dcd823b":"code","4666e14d":"code","9139df87":"code","fa78ab40":"code","82ff7ef7":"code","809e2a97":"code","9ea1c96f":"code","b9bcf746":"code","55208985":"code","27439645":"code","6381b69f":"markdown","0f303620":"markdown","e64b810c":"markdown","ed274e45":"markdown","094ea397":"markdown"},"source":{"61878a6d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9567311a":"train_sales = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nprint(f\"train shape {train_sales.shape}\")\nsubmission_file = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sample_submission.csv')\nprint(f\"submission_file shape {submission_file.shape}\")","e5d95dc3":"days = range(1, 1913 + 1)\ntime_series_columns = [f'd_{i}' for i in days]\n\ntime_series_data = train_sales[time_series_columns]","02b16b77":"print(train_sales.columns)\nid_df_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']","6d334435":"train_sales[id_df_columns].nunique()","caf9962d":"train_sales[id_df_columns + time_series_columns].head(2)\n","3395ab50":"##opt - drop out test set rows\n# train_sales = train_sales[id_df_columns + time_series_columns]","ded276f3":"# https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else: df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","8e4de83c":"# ## reduce memory usage. There's an imporved version of this function that also saves data as categoircals type, but that can affect joins if not handled explicitly\n# train_sales = reduce_mem_usage(train_sales)\n\n### we know the max range of the sales cols, let's just set them all to int 16 (some are int8 , but that doesn't matter if we 'll cast it)\ndisplay(train_sales.info())\ntrain_sales[time_series_columns] = train_sales[time_series_columns].astype(np.int16)","c77e4300":"# train_sales[id_df_columns] = train_sales[id_df_columns].astype('category')\ndisplay(train_sales.info())","332a3710":"train_sales.dtypes","22faa573":"train_sales","19c30bd9":"submission_file","6213a358":"time_series_data","e54f401a":"calendar = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/calendar.csv\",parse_dates=[\"date\"])\nprint(calendar.shape)\nprices = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\")\nprint(prices.shape)","2995c3b9":"calendar.tail(3)","61526199":"#no need to keep the textual weekday name, we have it from wday + data. Saturday = 1,Sunday\t2, Friday\t7\ncalendar.drop(\"weekday\",axis=1,inplace=True)\n\n## we  drop the prefix from the calendar date\/d column for easy merging with sales data. .\ncalendar[\"d\"] = calendar[\"d\"].replace(\"d_\",\"\",regex=True).astype(int)\ncalendar","e39e24ab":"prices","5dcd823b":"print(f\"After reshaping to 1 row per id per day\/date, we would have: {train_sales.shape[0]*time_series_data.shape[1]} rows\")\n## 58 million rows. many sparse likely","4666e14d":"%%time\npd.wide_to_long(train_sales.head(3),\"d_\",i=id_df_columns,j=\"sales\").reset_index()","9139df87":"stores_list = list(set(train_sales[\"store_id\"]))\nstores_list","fa78ab40":"%%time\n### reshape incrementally - hopefully this will help with memory errors\ndfs= []\nfor st in stores_list:  \n    df = train_sales.loc[train_sales[\"store_id\"]==st]#.head()\n    dfs.append(pd.wide_to_long(df,\"d_\",i=id_df_columns,j=\"day\").reset_index())\n    \ndf = pd.concat(dfs)\ndf.rename(columns={\"d_\":\"sales\"})\ndel(dfs)\nprint(df.shape)\ndf","82ff7ef7":"df.tail()","809e2a97":"# %%time\n# train_sales = pd.wide_to_long(train_sales,\"d_\",i=id_df_columns,j=\"sales\").reset_index()\n# print(train_sales.shape)\n# train_sales","9ea1c96f":"df.to_csv(\"sales_basic_v1_all.csv.gz\",index=False,compression=\"gzip\")","b9bcf746":"validation_ids = train_sales['id'].values\nevaluation_ids = [i.replace('validation', 'evaluation') for i in validation_ids]\n","55208985":"ids = np.concatenate([validation_ids, evaluation_ids])\n","27439645":"predictions = pd.DataFrame(ids, columns=['id'])\nforecast = pd.concat([forecast] * 2).reset_index(drop=True)\npredictions = pd.concat([predictions, forecast], axis=1)\npredictions.to_csv('submission.csv', index=False)","6381b69f":"### Reshape the data, basic EDA & a baseline model\n* Naive baseline - mean on that day of week per store, or the last sale value\n* reshape from columnar (wide format) to long\n\n* Some eda code borrowed from here: https:\/\/www.kaggle.com\/rdizzl3\/eda-and-baseline-model\n\n\n* reshaping note: some items may only start to be sold after a certain date - would be best to cutoff them off before that to avoid noise in the model. e.g. take first index\/col >0 as start ? \n\n* Issue: currently, notebook crashes when reshaping\/pivoting :|. ","0f303620":"## Reshape sales data to long format\n* Also join with calendar data\n\n* The IDs being set to categorical type slows this down immensely\n\n* due to memory - we may wish to split this into sub frames, them concat them. e.g. split by state or store_id?\n","e64b810c":"## Metadata","ed274e45":"##### Predictions\n* We need to provide predictions for the next 28 days for each of the series. For the validation series that is days 1914 - 1941 and for the evaluation that is days 1942 - 1969.\n* https:\/\/www.kaggle.com\/rdizzl3\/eda-and-baseline-model","094ea397":"####### We are given previous data days sales in the sales_train_validation dataset.\n\n* d_1914 - d_1941 represents the validation row\n* d_1942 - d_1969 represents the evaluation rows.\n    * WE could drop them from the pivoting data (leave ot for testing prediction rows) , or leave them in then split later for easy creation of test set data in smae format"}}