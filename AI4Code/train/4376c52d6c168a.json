{"cell_type":{"d7cbaa47":"code","15eb7433":"code","51ef4214":"code","75780010":"code","1fbd5dd4":"code","51036211":"code","61c2c115":"code","011091b5":"code","de2944ca":"code","caf76f2b":"code","e7d9a7ac":"markdown","307fb0c3":"markdown","4af18711":"markdown","7d167402":"markdown","cf3b15e9":"markdown"},"source":{"d7cbaa47":"# DATA preprocessing\n\n# libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport numpy as np\nimport time\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nimport math\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GroupKFold, RepeatedStratifiedKFold, cross_validate, StratifiedShuffleSplit\nfrom sklearn import metrics\n\n# DATA LOADING\nfrom sklearn.neural_network import MLPClassifier\n\npath = '\/kaggle\/input\/stock-market-prediction\/'\npathOutput = '.\/'\ncompletoTrain='dataset_train_validation.csv'\ncompletoTest='dataset_test.csv'\n\n# INPUT\ntrain = pd.read_csv(os.path.join(path, completoTrain))\ntest = pd.read_csv(os.path.join(path, completoTest))\n\n# Se separan los resultados reales con los que comparar\ncolumns =  [col for col in test.columns if col not in ['company', 'age', 'market', 'TARGET']]\nsubmission = test[columns]\nsolucion = test['TARGET']\n\n####################### ELIMINACION de filas con null en la columna TARGET\ndef filter_rows_by_values(df, col, values):\n    return df[~df[col].isin(values)]\n\ntrain = filter_rows_by_values(train, \"TARGET\", [\"null\"])\n\n\n########################## NUEVAS FEATURES ###########################\n######33###### RSI ###################\ndef relative_strength_idx(df, n=14):\n    close = df['close']\n    delta = close.diff()\n    delta = delta[1:]\n    pricesUp = delta.copy()\n    pricesDown = delta.copy()\n    pricesUp[pricesUp < 0] = 0\n    pricesDown[pricesDown > 0] = 0\n    rollUp = pricesUp.rolling(n).mean()\n    rollDown = pricesDown.abs().rolling(n).mean()\n    rs = rollUp \/ rollDown\n    rsi = 100.0 - (100.0 \/ (1.0 + rs))\n    return rsi\n\n################################# NUEVAS FEATURES TRAIN\ntrain['close_lag'] = train['close'].shift(1)\ntrain['RSI'] = relative_strength_idx(train).fillna(0)\ntrain = train.fillna(0)\n\n################################## NUEVAS FEATURES TEST\n# all the same for the test data\ntest['close_lag'] = test['close'].shift(1)\ntest['RSI'] = relative_strength_idx(test).fillna(0)\ntest = test.fillna(0)\n\n#######################################\n\n# Se eliminan filas que tengan alg\u00fan nulo\ntrain=train.dropna(axis=0, how=\"any\")\n","15eb7433":"import math \n\nNUMBER_OF_AGES_IN_ERA = 5\n\nageMin= train['age'].min()\nageMax= train['age'].max()\n\n# S\u00f3lo se toma la parte entera de la divisi\u00f3n. Cada era es un bloque de ages\nnumberOfEras=(ageMax-ageMin) \/\/ NUMBER_OF_AGES_IN_ERA\n\nmaxAgeOfvalidEras = numberOfEras * NUMBER_OF_AGES_IN_ERA + ageMin\n\n# Se toma el 70% (aprox.) de eras para train y el resto para validaci\u00f3n\nfraccion_train = 0.7  # Fracci\u00f3n de datos usada para entrenar\neraMaxTrain=math.trunc(numberOfEras * fraccion_train)\n\n# Eras: \n# - Train: 0 to eraMaxTrain\n# - Validation: \"eraMaxTrain + 1\" to numberOfEras\n\n########### SE CREA UNA NUEVA COLUMNA, QUE INDICA LA ERA\ntrain['era']=train['age'] \/\/ NUMBER_OF_AGES_IN_ERA\n\n#########################################\n# Se fraccionan los datos de train en: train + validaci\u00f3n\naux=train\ntrain=aux[aux['age'] <= int(eraMaxTrain * NUMBER_OF_AGES_IN_ERA)]\nvalidacion=aux[aux['age'] > int(eraMaxTrain * NUMBER_OF_AGES_IN_ERA)]\nprint(\"Filas de train: \")\nprint(train.shape[0])\nprint(\"Filas de validaci\u00f3n: \")\nprint(validacion.shape[0])\n\nprint(\"FIN TROCEO\")\n\n######################## PARA TEST, SE GENERA LA COLUMNA ERA\n# S\u00f3lo se toma la parte entera de la divisi\u00f3n. Cada era es un bloque de ages\nageMin= test['age'].min()\nageMax= test['age'].max()\nnumberOfEras=(ageMax-ageMin) \/\/ NUMBER_OF_AGES_IN_ERA\nmaxAgeOfvalidEras = numberOfEras * numberOfEras + ageMin\n\n# Se eliminan las filas sobrantes\ntest=test.iloc[int(maxAgeOfvalidEras):, :]\n\n# SE CREA UNA NUEVA COLUMNA, QUE INDICA LA ERA\ntest['era']=test['age'] \/\/ NUMBER_OF_AGES_IN_ERA\n","51ef4214":"print(\"Neutralizing to risky features\")\n# getting the per era correlation of each feature vs the target\nall_feature_corrs = train.groupby(\"era\").apply(\n    lambda d: d[columns].corrwith(d[\"TARGET\"])\n)\n\ndef get_biggest_change_features(corrs, n):\n    all_eras = corrs.index.sort_values()\n    h1_eras = all_eras[:len(all_eras) \/\/ 2]\n    h2_eras = all_eras[len(all_eras) \/\/ 2:]\n\n    h1_corr_means = corrs.loc[h1_eras, :].mean()\n    h2_corr_means = corrs.loc[h2_eras, :].mean()\n\n    corr_diffs = h2_corr_means - h1_corr_means\n    worst_n = corr_diffs.abs().sort_values(ascending=False).head(n).index.tolist()\n    return worst_n\n\nimport os\nimport requests\nimport numpy as np\nimport pandas as pd\nimport scipy\nfrom pathlib import Path\nimport json\nfrom scipy.stats import skew, kurtosis\ndef neutralize(df,\n               columns,\n               neutralizers=None,\n               proportion=1.0,\n               normalize=True,\n               era_col=\"era\"):\n    if neutralizers is None:\n        neutralizers = []\n    unique_eras = df[era_col].unique()\n    computed = []\n    for u in unique_eras:\n        df_era = df[df[era_col] == u]\n        scores = df_era[columns].values\n        if normalize:\n            scores2 = []\n            for x in scores.T:\n                x = (scipy.stats.rankdata(x, method='ordinal') - .5) \/ len(x)\n                x = scipy.stats.norm.ppf(x)\n                scores2.append(x)\n            scores = np.array(scores2).T\n        exposures = df_era[neutralizers].values\n\n        scores -= proportion * exposures.dot(\n            np.linalg.pinv(exposures.astype(np.float32)).dot(scores.astype(np.float32)))\n\n        scores \/= scores.std(ddof=0)\n\n        computed.append(scores)\n\n    return pd.DataFrame(np.concatenate(computed),\n                        columns=columns,\n                        index=df.index)\n\n\n# find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\nriskiest_features = get_biggest_change_features(all_feature_corrs, 50)\n\nprint(\"riskiest_features: \")\nprint(riskiest_features)\n\n# neutralize our predictions to the riskiest features\ntrain['TARGET'] = neutralize(\n    df=train,\n    columns=['TARGET'],\n    neutralizers=riskiest_features,\n    proportion=0.5,\n    normalize=\"True\",\n    era_col=\"era\",\n)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntrain[\"TARGET\"] = scaler.fit_transform(train[[\"TARGET\"]]) # transform back to 0-1\n\nprint(\"FIN DE NEUTRALIZACI\u00d3N\")\n\n\n################# Se separa en features y target\ncolumns =  [col for col in train.columns if col not in ['company', 'age', 'market', 'TARGET']]\ntrain_X = train[columns]\ntrain_y = train['TARGET']\nvalid_X = validacion[columns]\nvalid_y = validacion['TARGET']\n\n","75780010":"import numpy as np\nfrom scipy.stats import spearmanr\n\ndef feature_exposures(df): \n    columns = [col for col in df.columns if col not in ['company', 'age', 'market', 'TARGET']] \n    train_X = df[columns] \n    train_y = df['TARGET'] \n    feature_names = columns \n    exposures = [] \n    for f in feature_names: \n        fe = spearmanr(train_y, train_X[f])[0]\n        exposures.append(fe) \n    return np.array(exposures)\n\ndef max_feature_exposure(df): \n    return np.max(np.abs(feature_exposures(df)))\n\ndef feature_exposure(df): \n    return np.sqrt(np.mean(np.square(feature_exposures(df))))\n\n###################### Se imprimen los resultados \nprint(\"N\u00famero de ceros y unos: \")\nprint( train['TARGET'].value_counts() )\nprint(\"max_feature_exposure: \" + str(max_feature_exposure(train)))\nprint(\"feature_exposure: \" + str(feature_exposure(train)))","1fbd5dd4":"# compute feature exposure\nfes = feature_exposures(train)\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib_venn import venn2\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('fivethirtyeight')\npd.options.display.max_columns = None\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfes_df = pd.DataFrame()\nfes_df['features'] = np.array(columns)\nfes_df['features'] = fes_df['features'].apply(lambda x : str(x).split('_')[-1])\nfes_df['corr_to_target'] = fes\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 5))\nsns.barplot(x='features', y='corr_to_target', data=fes_df, ax=ax)","51036211":"\n### Feature neutralization:\n#https:\/\/www.kaggle.com\/yamqwe\/g-research-avoid-overfit-feature-neutralization\/\n","61c2c115":"columns = [col for col in train_X.columns if col not in ['company', 'age', 'market', 'TARGET']] \n\n# code to feature neutralize\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm.auto import tqdm\n\ndef neutralize_series(series : pd.Series, by : pd.Series, proportion=0.1):\n    \"\"\"\n    neutralize pandas series (originally from the Numerai Tournament)\n    \"\"\"\n    scores = np.nan_to_num(series.values).reshape(-1, 1)\n    exposures = np.nan_to_num(by.values).reshape(-1, 1)\n    exposures = np.hstack((exposures, np.array([np.mean(np.nan_to_num(series.values))] * len(exposures)).reshape(-1, 1)))\n    correction = proportion * (exposures.dot(np.linalg.lstsq(exposures, scores)[0]))\n    corrected_scores = scores - correction\n    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n    return neutralized\n\nfor f in train_X[columns]:\n        \n    # neut\n    neut = neutralize_series(train_X[f], train_y, proportion=1.0)\n    \n    # verify\n    original_corr = np.corrcoef(np.nan_to_num(train_X[f].values), np.nan_to_num(train_y.values))[0, 1]\n    neut_corr = np.corrcoef(neut, np.nan_to_num(train_y.values))[0, 1]\n    f_corr = np.corrcoef(np.nan_to_num(train_X[f].values), neut)[0, 1]\n    #print('{}: original corr to target={:.3f}, corr to target after neut={:.3f}, corr with old and neut feat={:.3f}'.format(f,\n    #    original_corr, neut_corr, f_corr))\n    \n    # assign\n    #train_X[f] = neut.values\n    \n    # print(f)\n    # print(train_X[f])","011091b5":"# MODEL TRAINING\n###################### MODELO LGBM ######################\nfolds = GroupKFold(n_splits=5)\nparams = {'objective': 'binary',\n          'learning_rate': 0.02,\n          \"boosting_type\": \"gbdt\",\n          \"metric\": 'precision',\n          'n_jobs': -1,\n          'min_data_in_leaf': 32,\n          'num_leaves': 1024,\n          }\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(train_X, train_y, groups=train['company'])):\n    print(f'Fold {fold_n} started at {time.ctime()}')\n    X_train, X_valid = train_X[columns].iloc[train_index], train_X[columns].iloc[valid_index]\n    y_train, y_valid = train_y.iloc[train_index], train_y.iloc[valid_index]\n\n    model = lgb.LGBMClassifier(**params, n_estimators=50)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)])\n#####################################################\n\n\n#################### SE DIBUJAN LAS FEATURES POR IMPORTANCIA #################\nfeature_importance = pd.DataFrame()\nfold_importance = pd.DataFrame()\nfold_importance[\"feature\"] = columns\nfold_importance[\"importance\"] = model.feature_importances_\nfold_importance[\"fold\"] = fold_n + 1\nfeature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\nfeature_importance[\"importance\"] \/= 5\n# Se pintan las primeras 50 features\ncols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\n\nbest_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\nplt.figure(figsize=(16, 12));\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\nplt.title('LGB Features (avg over folds)');\n# plt.show(block=False)\n# plt.pause(5)\nplt.savefig(pathOutput + \"BOLSA_feature_importances.png\")\n# plt.close()\n###################","de2944ca":"\n##################### VALIDACI\u00d3N ###################\nprint(\"COMIENZO DE VALIDACI\u00d3N\")\nscore = metrics.mean_absolute_error(valid_y, model.predict(valid_X))\nprint('CV score: {0:.4f}.'.format(score))\nprint(\"FIN DE VALIDACI\u00d3N\")\n###############################################","caf76f2b":"# PREDICCI\u00d3N independiente\n\n# Eliminaci\u00f3n de filas a null\ntest = filter_rows_by_values(test, \"TARGET\", [\"null\"])\n\nsubmission = test[columns]\nsolucion = test['TARGET']\n\n# S\u00f3lo aplicable si hay sigmoid en la \u00faltima capa\nprediccion = (model.predict(submission) > 0.5).astype(\"int32\")\nsubmission['TARGET']=prediccion\n\n# Se a\u00f1ade la empresa en la primera columna y se guarda en Excel\nsubmission=submission.join(test['company'])\nsubmission.set_index(submission.pop('company'), inplace=True)\nsubmission.reset_index(inplace=True)\nsubmission.to_csv(os.path.join(pathOutput, 'Bolsa_DL_submission.csv'), index=False)\n\n# PRECISION independiente\na=solucion\nb=prediccion\nTP=sum(1 for x,y in zip(a,b) if (x == y and y == 1))\nTPandFP=sum(b)\nprecision= TP \/ TPandFP\nprint(\"TP: \", TP)\nprint(\"TP + FP: \", TPandFP)\nprint(\"---->>>>>> PRECISION (TP\/(TP+FP)) FOR TEST DATASET: {0:.2f}% <<<<<<------\".format(precision * 100))\n\n# Tasa de desbalanceo\nift_mayoritaria = test[test.TARGET == False]\nift_minoritaria = test[test.TARGET == True]\ntasaDesbalanceo = round(ift_mayoritaria.shape[0] \/ ift_minoritaria.shape[0], 2)\nprint(\"Tasa de desbalanceo = \" + str(ift_mayoritaria.shape[0]) + \"\/\" + str(\n        ift_minoritaria.shape[0]) + \" = \" + str(tasaDesbalanceo))\n\nprint(\"Tasa de mejora de precisi\u00f3n respecto a random: \",\n              round(precision \/ (1\/(1+tasaDesbalanceo)), 2))\n\nprint(\"END\")","e7d9a7ac":"\n\n### Feature exposures:\n\nsource: https:\/\/forum.numer.ai\/t\/model-diagnostics-feature-exposure\/899\n\n","307fb0c3":"### Data are splitted in eras of time (block of ages):","4af18711":"### Neutralizing:\n\nSource: https:\/\/wandb.ai\/parmarsuraj99\/massive_nmr\/reports\/A-Super-Easy-Guide-to-the-Super-Massive-Numerai-Dataset--VmlldzoxMTM4OTU2","7d167402":"\n### Independent prediction:\n\n","cf3b15e9":"### Feature correlation to target:"}}