{"cell_type":{"37c9e1ae":"code","588c1e2c":"code","ce7d1f64":"code","cbe05c5a":"code","ac5d33d4":"markdown","e099227e":"markdown","ade69742":"markdown","727c77d2":"markdown"},"source":{"37c9e1ae":"# Import libraries\n\nimport numpy as np","588c1e2c":"# Model parameters\ninput_size = 3 # number of features\nlayers = [4, 3] # number of neurons in the 1st and 2nd hidden layer\noutput_size = 2 # number of output labels","ce7d1f64":"def softmax(a):\n    \"\"\"\n        Define the softmax activation for an input 'a'\n    \"\"\"\n    e_pa = np.exp(a)\n    ans = e_pa\/np.sum(e_pa, axis = 1, keepdims = True)\n    return ans","cbe05c5a":"class NeuralNetwork:\n    \n    def __init__(self, input_size, layers, output_size):\n        np.random.seed(19)\n        \n        # Initialize for all weights and bias paramters to be added\n        params = {}\n        \n        # initialize the weights and bias associated with the input and the first hidden layer\n        params['W1'] = np.random.randn(input_size, layers[0])\n        params['b1'] = np.zeros((1, layers[0]))\n        \n        # weights and bias for the 1st hidden layer and 2nd hidden layer\n        params['W2'] = np.random.randn(layers[0], layers[1])\n        params['b2'] = np.zeros((1, layers[1]))\n        \n        # weights and bias for the 2nd hidden layer and output layer\n        params['W2'] = np.random.randn(layers[1], output_size)\n        params['b2'] = np.zeros((1, output_size))\n        \n        self.params = params\n        \n        \n    def forward(self, X):\n        \"\"\"\n            Calculate softmax activation for the weighted sums for each layer and implement\n            forward propagation.\n        \"\"\"\n        W1, W2, W3 = self.params['W1'], self.params['W2'], self.params['W3']\n        b1, b2, b3 = self.params['b1'], self.params['b2'], self.params['b3']\n        \n        z1 = np.dot(x, W1) + b1\n        a1 = softmaz(z1)\n        \n        z2 = np.dot(a1, W2) + b2\n        a2 = softmax(z2)\n        \n        z3 = np.dot(a2, W3) + b3\n        y_ = softmax(z3) # The final output after forward propagation for 3 layers","ac5d33d4":"## Implementation","e099227e":"# Multilayer Perceptron\/ Feed-forward architecture\n\n## Implementing a 3-layer neural network from scratch\n\nIn this notebook, we will start with the basics of deep learning and implement a simple 3-layer MLP from scratch using Python. Later, we will use this network for training and prediction purposes.","ade69742":"# Part 1 - Neural Architecture\n\n**One Layer Network**\n\n![](https:\/\/images.deepai.org\/glossary-terms\/perceptron-6168423.jpg)\n\nThis is the simplest unit perceptron with 1 output unit and no hidden units.\nThe number of layers in a neural network increases with the increase in the number of hidden layers.\nNumber of layers = Number of hidden layers + 1 (for the output layer)\n\n**Our Three Layer Network**\n\n![](https:\/\/cdn-media-1.freecodecamp.org\/images\/FDWrPCgJTJbH3MPUSyT0tgG2Zi2TYczZDOAj)\n\nIn a 3 layer network, we will have 2 hidden layers in our network as seen above. We will be defining the architecture of our MLP in the next section and it will consist of two hidden layers.\n\nAlong with the input data and output labels, we will also have weights and bias terms associated with each layer and neuron. For the 3-layer network, we will have three weights matrices associated with the input and 1st hidden layer, the 1st hidden layer and the 2nd hidden layer, and the 2nd hidden layer and output.\n\nThe size of the weights matrix is given as (m,n) where m is the number of neurons in the previous layer and n is the number of neurons in the current layer.\n\nFor example, the 1st weight matrix (W1) is of size (3,4) as 3 is the number of input neurons and 4 is the neurons in the 1st hidden layer. Similarly, W2 = (4,3), and W3 = (3,2).\n\nAlso, the size for the bias terms is (1, n). For example, b1 = (1,3), b2 = (1,4), and b3 = (1,3).","727c77d2":"## Part 2 - Forward Propagation\n\n![](https:\/\/miro.medium.com\/max\/875\/1*ts5LSdtkfSsMYS7M0X84Tw.gif)\n\nAfter we have defined the architecture, the weights and bias associated with each layer, we now move on to an important part of the neural network: forward propagation. Neural networks are arranged layer-after-layer so that the input can move through them to give us some output. So, we will need to define a function to pass on the inputs along with the weights and bias to the next layer.\n\nAt each neuron in a hidden or output layer, the processing happens in two steps:\n- Preactivation: it is a weighted sum of inputs i.e. the linear transformation of weights w.r.t to inputs available. Based on this aggregated sum and activation function the neuron makes a decision whether to pass this information further or not.\n- Activation: the calculated weighted sum of inputs is passed to the activation function. An activation function is a mathematical function which adds non-linearity to the network. There are four commonly used and popular activation functions \u2014 sigmoid, hyperbolic tangent(tanh), ReLU and Softmax.\n\nThe output will be calculated as \n\n\\begin{equation}\n    z = \\Sigma w_i * x_i + b\\\\\n    h = \\sigma(z)\n\\end{equation}\n\nUsing vectorization, the equation for activation will look like:\n\\begin{equation}\nz_1 = A^{[i]}.W^{[i+1]} + b\n\\end{equation}"}}