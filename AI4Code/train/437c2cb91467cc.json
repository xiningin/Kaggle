{"cell_type":{"3864a592":"code","f6330750":"code","abc792c7":"code","0b73cef6":"code","d29ea8ab":"code","1473af7d":"code","28b0bd8c":"code","4f6385e0":"code","3a260672":"code","38cb9df3":"code","77bfd50d":"code","dc23e9b8":"code","c739f4cc":"code","49cc6bb2":"code","0910b6a8":"code","cf9ed581":"markdown","fabd4041":"markdown","e3425d74":"markdown","0d754059":"markdown","e3ec3667":"markdown","0b897892":"markdown","6c909257":"markdown","88d6fb23":"markdown","ea569c49":"markdown","f6ddca9f":"markdown","193412fe":"markdown","04f3fa5b":"markdown","fbcfcfe4":"markdown","2eefd899":"markdown","7a4e4c6c":"markdown"},"source":{"3864a592":"# Data Manipulation & Visualization\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns # used for plot interactive graph. \nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\nimport pickle as pk\nfrom scipy import sparse as sp\n\n# Text Manipulation\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom gensim.models import Phrases\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nimport gensim\nimport pyLDAvis.gensim\npyLDAvis.enable_notebook()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n# Machine Learning\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix,mean_squared_error,mean_absolute_error,log_loss,accuracy_score,classification_report\nfrom sklearn.metrics import precision_score\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing","f6330750":"df = pd.read_csv('..\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')\ndf.head()","abc792c7":"plt.figure(figsize=(8,7))\nsns.countplot(data=df,x=\"Rating\",edgecolor='black',linewidth=3)\nplt.title('Rating distribution',size=17)\nplt.show()","0b73cef6":"docs= np.array(df['Review'])","d29ea8ab":"def docs_preprocessor(docs):\n    tokenizer = RegexpTokenizer(r'\\w+')\n    \n    for idx in range(len(docs)):\n        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n\n    # Remove numbers, but not words that contain numbers.\n    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n    \n    # Remove words that are only one character.\n    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n    \n    # Lemmatize all words in documents.\n    lemmatizer = WordNetLemmatizer()\n    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n  \n    return docs\n\ndocs = docs_preprocessor(docs)\n","1473af7d":"# Add bigrams and trigrams to docs (only ones that appear 10 times or more).\nbigram = Phrases(docs, min_count=10)\ntrigram = Phrases(bigram[docs])\n\nfor idx in range(len(docs)):\n    for token in bigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)\n    for token in trigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)","28b0bd8c":"# Create a dictionary representation of the documents.\ndictionary = Dictionary(docs)\nprint('Number of unique words in initital documents:', len(dictionary))\n\n# Filter out words that occur less than 10 documents, or more than 20% of the documents.\ndictionary.filter_extremes(no_below=10, no_above=0.2)\nprint('Number of unique words after removing rare and common words:', len(dictionary))","4f6385e0":"corpus = [dictionary.doc2bow(doc) for doc in docs]\nprint('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","3a260672":"# Set training parameters.\nnum_topics = 4\nchunksize = 500 # size of the doc looked at every pass\npasses = 20 # number of passes through documents\niterations = 400\neval_every = 1  # Don't evaluate model perplexity, takes too much time.\n\n# Make a index to word dictionary.\ntemp = dictionary[0] # load dictionary\nid2word = dictionary.id2token\n\n%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n                       alpha='auto', eta='auto', \\\n                       iterations=iterations, num_topics=num_topics, \\\n                       passes=passes, eval_every=eval_every,random_state=12)\n","38cb9df3":"pyLDAvis.gensim.prepare(model, corpus, dictionary)","77bfd50d":"def explore_topic(lda_model, topic_number, topn, output=True):\n    \"\"\"\n    accept a ldamodel, atopic number and topn vocabs of interest\n    prints a formatted list of the topn terms\n    \"\"\"\n    terms = []\n    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n        terms += [term]\n        if output:\n            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n    \n    return terms","dc23e9b8":"topic_summaries = []\nprint(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\nfor i in range(num_topics):\n    print('Topic '+str(i)+' |---------------------\\n')\n    tmp = explore_topic(model,topic_number=i, topn=5, output=True )\n#     print tmp[:5]\n    topic_summaries += [tmp[:5]]","c739f4cc":"# attach topics to df\nall_topics = model.get_document_topics(corpus, minimum_probability=0.0)\nall_topics_csr = gensim.matutils.corpus2csc(all_topics)\nall_topics_numpy = all_topics_csr.T.toarray()\ndf['Topic'] = all_topics_numpy.argmax(axis=1)\n\n# plot topics distribution by rating\nplt.figure(figsize=(8,7))\nsns.countplot(data=df,x=\"Rating\",hue=\"Topic\",edgecolor=\"black\",linewidth=3)\nplt.legend(['Top hotels',\"Resort Hotels\",\"Worst Hotels\",\"Business Hotels\"])\nplt.title('Topics Distribution by rating',size=18)\nplt.show()\n\n# plot topics distribution \nplt.figure(figsize=(8,7))\nax=sns.countplot(data=df,x=\"Topic\",edgecolor=\"black\",linewidth=3)\nax.set_xticklabels(['Top hotels',\"Resort Hotels\",\"Worst Hotels\",\"Business Hotels\"])\nplt.title('Topics Distribution',size=18)\nplt.show()","49cc6bb2":"from sklearn.model_selection import train_test_split\n\nX = df['Review']\ny = df['Rating']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n","0910b6a8":"xgb = Pipeline([('vect', TfidfVectorizer()),\n               ('tfidf', TfidfTransformer()),\n               ('clf', XGBClassifier(objective=\"multi:softmax\",n_estimators=300,learning_rate=0.01))\n              ])\n\nxgb.fit(X_train, y_train)\n\ny_pred = xgb.predict(X_test)\n\nprint(\"_\"*25+\"Classification Report\"+\"_\"*25)\nprint(classification_report(y_pred,y_test,zero_division=0))\nprint(\"_\"*25+\"Evaluation Metrics\"+\"_\"*25)\nprint(\"\\n\")\nprint(\"Accuracy: %f\" % accuracy_score(y_pred,y_test))\nprint(\"Weighted Precision :%f\" % precision_score(y_pred,y_test,average=\"weighted\"))\nprint(\"MAE :%f\" % mean_absolute_error(y_pred,y_test))\nprint(\"RMSE :%f\" % mean_squared_error(y_pred,y_test,squared=False))\n\n\nplt.figure(figsize=(8,7))\ncm=confusion_matrix(y_pred,y_test)\ng=sns.heatmap(cm,annot=True,fmt='d',linewidths=1,linecolor='black',\n                  annot_kws={\"size\":14},cmap='Blues',cbar=False)\n\nplt.xlabel('Actual',size=16)\nplt.ylabel('Predicted',size=16)\nplt.title('Confusion Matrix \\n XGB Classifier',size=16)\nplt.show()","cf9ed581":"<a id='topic'><\/a>\n# Topic modelling on reviews","fabd4041":"# Rating distribution in dataset","e3425d74":"## Let's see topic distribution by rating\n**Draw your conclusions:**","0d754059":"### Pre-process and vectorize review","e3ec3667":"#### We can say that rating distribution is \"left-skewed\" since we have more 4-5 stars ratings in our dataset","0b897892":"![Tripadvisor_lockup_horizontal_secondary_registered.svg](attachment:Tripadvisor_lockup_horizontal_secondary_registered.svg)\n## Hotels play a crucial role in traveling and with the increased access to information new pathways of selecting the best ones emerged. \n\n# Challenges\n\n* [Topic Modelling on reviews](#topic)\n* [Explore Key Aspects that make hotels good or bad](#eda)\n* [Predict review rating](#model)","6c909257":"## Training LDA...\n\nLDA is an unsupervised technique, meaning that we don't know prior to running the model how many topics exits in our corpus. Four topic can be a good choice to start,and see if it is the optimal number that would separate topics the most. \n\nNext we use pyLDAvis tool to visualize LDA result:","88d6fb23":"<a id='eda'><\/a>\n# Explore key aspects that make hotels good or bad","ea569c49":"<a id='model'><\/a>\n# Predict review rating","f6ddca9f":"**Suggested Metrics:**\n* MAE\n* RMSE","193412fe":"**The four topics:**\n* Topic 0 : \"Top Hotels\/Comfortable Hotels\"\n**Includes words like: (minute walk,walking distance, comfortable, city ,staff friendly) that make us think that this topic it's related with City Hotels, close to center city and comfortable ( Seems with an higher rating )**\n\n* Topic 1 : \"Resort Hotels\"\n**Includes words like: (beach,resort, punta cana, trip ,beautiful, ocean). It's surely related with Resort hotels.**\n\n* Topic 2 : \"Worst Hotels\"\n**Includes words like: (desk,problem, asked ,told, check,dirty,loud). that make us think that this topic can be related with lower rating hotels, and seems that those problems are related with reservation problems,dirtiness and loudness, basically the worst hotels.**\n\n* Topic 3 : \"Business Hotels\"\n**Includes words like: (coffee,continental breakfast,sitting_area). This topic seems to refer to business-class hotels or something like that.**","04f3fa5b":"### Remove rare and common tokens:\n**Filtering out words that occur less than 10 documents and more than 20% of the documents we ended up with about 23% of original words**","fbcfcfe4":"# Dataset","2eefd899":"**What do we see here?**\n\nThe left panel, labeld Intertopic Distance Map,represent different topics and the distance between them. Similar topics appear closer and the dissimilar topics farther. The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus. An individual topic may be selected for closer scrutiny by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left.\n\nThe right panel, include the bar chart of the top 30 terms. When no topic is selected in the plot on the left, the bar chart shows the top-30 most \"salient\" terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics. Selecting each topic on the right, modifies the bar chart to show the \"relevant\" terms for the selected topic. Relevence is defined as in footer 2 and can be tuned by parameter  \u03bb , smaller  \u03bb  gives higher weight to the term's distinctiveness while larger  \u03bb s corresponds to probablity of the term occurance per topics.\n\nTherefore, to get a better sense of terms per topic we'll use  \u03bb =0.","7a4e4c6c":"### First of all let's look at the terms that appear more in each topic"}}