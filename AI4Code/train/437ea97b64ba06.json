{"cell_type":{"1c9c6974":"code","7c538d0f":"code","0a3cc7f1":"code","64a7e89b":"code","0907922c":"code","e72bb9aa":"code","7ee02919":"code","99980fa8":"code","0e8d0585":"code","593a9574":"code","83b6008d":"code","af5dfa42":"code","2a0694e1":"code","63307ad4":"code","52121922":"code","729007e6":"code","b899854a":"code","372a1a72":"code","80e01416":"code","1c2f4c36":"code","2ae9aba9":"code","c841ef6c":"code","cc0ac990":"code","4b38262b":"code","e3ca64c3":"code","88e655a2":"code","9f683a11":"code","9905ddab":"code","e65f14f2":"code","78a2ac9f":"code","4bbb2780":"code","8997f744":"code","0877f873":"code","24319fc5":"code","62c1c7ae":"code","00551a64":"code","60d4ec5c":"code","18409b42":"code","466a7a7b":"code","d8505416":"code","19aaa881":"code","9190a6d2":"code","ea775f93":"code","7d595df1":"code","bc40e20e":"code","05390c27":"code","0f5f2c94":"code","897e0580":"code","539af9cc":"code","2c20baf8":"code","3ee71aa3":"code","cf436429":"code","d736831c":"code","351bfedf":"code","75b6784c":"code","123c9a81":"code","ca17f0a4":"code","992037ce":"code","563ff33a":"code","5f9a5072":"code","2226e2a9":"code","3f5baab9":"code","1e719308":"code","4fb602c2":"code","0c164e05":"code","b6c93981":"code","93a2878f":"code","6f1169f0":"code","c6edcb69":"code","c5a0918c":"code","3dd60af2":"code","bbf968b5":"code","8cf8acd4":"code","36f7e906":"code","faff678c":"code","bbe62fd2":"markdown","3037923b":"markdown","93f4b296":"markdown","73b3a9de":"markdown","57b866e3":"markdown","8ee5cf5a":"markdown","bf1b85b2":"markdown","1c3eef44":"markdown","564aa4e9":"markdown","f5270de9":"markdown","b7ab16dc":"markdown","fe6996be":"markdown","048fe063":"markdown","011dd1fb":"markdown","77071d49":"markdown","50f82220":"markdown","4176a497":"markdown","8d39bc89":"markdown","d1a52dfe":"markdown","8d955bdf":"markdown","7fd2ce64":"markdown","f27d791f":"markdown","3d51b626":"markdown","b61c368e":"markdown","ccc82e43":"markdown","a4a0f6ed":"markdown","c8d9c9a7":"markdown","172b704f":"markdown","a0b2bd02":"markdown"},"source":{"1c9c6974":"import pandas as pd     \nimport numpy as np\nimport re\nimport io\nimport requests\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","7c538d0f":"# read train and test dataset\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# use to index the submittion file\nPassengerId = test['PassengerId']\n\n# process train and test dataset at the same time\nall_data = pd.concat([train, test], ignore_index = True)","0a3cc7f1":"url=\"https:\/\/github.com\/thisisjasonjafari\/my-datascientise-handcode\/raw\/master\/005-datavisualization\/titanic.csv\"\ns=requests.get(url).content\nc=pd.read_csv(io.StringIO(s.decode('utf-8')))\n \ntest_labels = c\ntest_cp = pd.read_csv('..\/input\/titanic\/test.csv')\nwarnings.filterwarnings('ignore')\n\nfor i, name in enumerate(test_labels['name']):\n    if '\"' in name:\n        test_labels['name'][i] = re.sub('\"', '', name)\n        \nfor i, name in enumerate(test_cp['Name']):\n    if '\"' in name:\n        test_cp['Name'][i] = re.sub('\"', '', name)\n        \nsurvived = []\nfor name in test_cp['Name']:\n    survived.append(int(test_labels.loc[test_labels['name'] == name]['survived'].values[-1]))","64a7e89b":"train.head()","0907922c":"train.info()","e72bb9aa":"test.info()","7ee02919":"# correlation map\nplt.figure(figsize=(12, 8))\nsns.set(font_scale=1.5)\nsns.heatmap(train.corr(), cmap='coolwarm', annot=True, annot_kws={'size':15})\nplt.show()","99980fa8":"# the number of survival and death\ntrain['Survived'].value_counts()","0e8d0585":"# Pclass\uff1athe higher the class, the higher the survival rate\nsns.barplot(x = 'Pclass', y = 'Survived', data = train)","593a9574":"# Sex\uff1afemale is more likely to survive than male\nsns.barplot(x = 'Sex', y = 'Survived', data = train)","83b6008d":"# Age\uff1aunder 15-year-old there is an apparent differency of survival rate in the same age\nfacet = sns.FacetGrid(train, hue = 'Survived', aspect = 2)\nfacet.map(sns.kdeplot, 'Age', shade = True)\nfacet.set(Xlim = (0, train['Age'].max()))\nfacet.add_legend()\nplt.xlabel('Age')\nplt.ylabel('density')","af5dfa42":"# SibSp\uff1athe more medium the number of sublings, the higher survival rate\nsns.barplot(x = 'SibSp', y = 'Survived', data = train)","2a0694e1":"# Parch\uff1athe more medium the number of parents and children, the higher survival rate\nsns.barplot(x = 'Parch', y = 'Survived', data = train)","63307ad4":"# Embarked\uff1aC has the highest survival rate\nsns.countplot('Embarked', hue = 'Survived', data = train)","52121922":"# surname\nall_data['Surname'] = all_data['Name'].apply(lambda x : x.split(',')[0].strip())","729007e6":"# Name Title\nall_data['Title'] = all_data['Name'].apply(lambda x:x.split(',')[1].split('.')[0].strip())\nprint(set(all_data['Title']))","b899854a":"Title_Dict = {}\nTitle_Dict.update(dict.fromkeys(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer'))\nTitle_Dict.update(dict.fromkeys(['Don', 'Sir', 'the Countess', 'Dona', 'Lady'], 'Royalty'))\nTitle_Dict.update(dict.fromkeys(['Mme', 'Ms', 'Mrs'], 'Mrs'))\nTitle_Dict.update(dict.fromkeys(['Mlle', 'Miss'], 'Miss'))\nTitle_Dict.update(dict.fromkeys(['Mr'], 'Mr'))\nTitle_Dict.update(dict.fromkeys(['Master', 'Jonkheer'], 'Master'))","372a1a72":"all_data['Title'] = all_data['Title'].map(Title_Dict)\nsns.barplot(x = 'Title', y = 'Survived', data = all_data)","80e01416":"# show the deck using cabin number\n# E D B is more likely to survive\nall_data['Cabin'] = all_data['Cabin'].fillna('Unknown')\nall_data['Deck'] = all_data['Cabin'].str.get(0)\nsns.barplot(x = 'Deck', y = 'Survived', data = all_data) # is the same as all_data[all_data['Survived'].isna() == False]","1c2f4c36":"# TicketGroup\nTicket_Count = dict(all_data['Ticket'].value_counts())\nall_data['TicketGroup'] = all_data['Ticket'].apply(lambda x : Ticket_Count[x])\nsns.barplot(x = 'TicketGroup', y = 'Survived', data = all_data)","2ae9aba9":"# FamilySize\nall_data['FamilySize'] = all_data['SibSp'] + all_data['Parch'] + 1\nsns.barplot(x = 'FamilySize', y = 'Survived', data = all_data)","c841ef6c":"# optimization 1st: IsAlone\nall_data['IsAlone'] = 0\nall_data.loc[all_data['FamilySize'] == 1, 'IsAlone'] = 1","cc0ac990":"# Embarked\uff1aonly two missing values, and set to C according to Pclass\uff0cEmbarked\nall_data[all_data['Embarked'].isnull()]","4b38262b":"all_data.groupby(by = ['Pclass', 'Embarked']).Fare.median()","e3ca64c3":"all_data['Embarked'] = all_data['Embarked'].fillna('C')","88e655a2":"# Fare\uff1athere is only one missing value, and calculate the median value of fare according to Embarked and Pclass\nall_data[all_data['Fare'].isnull()]","9f683a11":"fare = all_data[(all_data['Embarked'] == 'S') & (all_data['Pclass'] == 3)].Fare.median()\nall_data['Fare'] = all_data['Fare'].fillna(fare)","9905ddab":"all_data[all_data['Fare'].isnull()]","e65f14f2":"# optimization 1st: change to a new feature\nall_data['CategoricalFare'] = pd.qcut(all_data['Fare'], 4)","78a2ac9f":"# Age\uff1athe missing rate of Age is quite high, so use random forest to fill it based on Sex, Title and Pclass\nall_data[all_data['Age'].isnull()]","4bbb2780":"guess_ages = np.zeros((2,3))\nsex = ['female', 'male']\nfor i in range(0, 2):\n    for j in range(0, 3):\n        guess_data = all_data[(all_data['Sex'] == sex[i]) & (all_data['Pclass'] == j + 1)]['Age'].dropna()\n\n        age_guess = guess_data.median()\n\n        # Convert random age float to nearest .5 age\n        guess_ages[i, j] = int(age_guess \/ 0.5 + 0.5) * 0.5\n            \nfor i in range(0, 2):\n    for j in range(0, 3):\n        all_data.loc[(all_data.Age.isnull()) & (all_data.Sex == sex[i]) & (all_data.Pclass == j + 1), 'Age'] = guess_ages[i,j]\n\nall_data['Age'] = all_data['Age'].astype(int)","8997f744":"all_data['CategoricalAge'] = pd.cut(all_data['Age'], 5)","0877f873":"all_data.head(3)","24319fc5":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n\nall_features = ['Pclass', 'Sex', 'CategoricalAge', 'SibSp', 'Parch', 'CategoricalFare',\n                    'Embarked', 'Surname', 'Title', 'Deck', 'TicketGroup', 'FamilySize', 'IsAlone']","62c1c7ae":"num_att = ['Pclass', 'SibSp', 'Parch', 'FamilySize']\ncat_att1 =  ['CategoricalAge', 'CategoricalFare', 'Deck', 'TicketGroup']\ncat_att2 = ['Sex', 'Embarked', 'Surname', 'Title', 'IsAlone']\ndrop_att = list(set(all_features) - set(num_att+cat_att1+cat_att2))\n\ntransform_pipline = ColumnTransformer([\n    (\"toDrop\",'drop', drop_att),\n    (\"numerical\",StandardScaler(), num_att),\n    (\"categorical1\",OrdinalEncoder(), cat_att1),\n    (\"categorical2\",OneHotEncoder(), cat_att2)\n])\n\ntransform_pipline.fit(all_data)\nall_data_prep = transform_pipline.transform(all_data)","00551a64":"# # correlation map\n# plt.figure(figsize=(14, 12))\n# sns.set(font_scale=1.5)\n# sns.heatmap(all_data_prep.astype(float).corr(), cmap='coolwarm', annot=True, annot_kws={'size':15})\n# plt.show()","60d4ec5c":"# train = all_data[all_data['Survived'].notnull()]\n\n# X_train = train.values[:, 1:]\n# y_train = train.values[:, 0]","18409b42":"# from sklearn.ensemble import RandomForestClassifier\n\n# random_forest= RandomForestClassifier(n_estimators=100,\n#                              max_features='auto',\n#                              criterion='entropy',\n#                              max_depth=10)\n# random_forest.fit(X_train, y_train)\n\n# random_forest.score(X_train, y_train)\n# acc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\n# print(round(acc_random_forest,2,), \"%\")","466a7a7b":"# importances = pd.DataFrame({'feature': train.iloc[:, 1:].columns,'importance': np.round(random_forest.feature_importances_, 3)})\n# importances = importances.sort_values('importance', ascending=False).set_index('feature')\n# importances","d8505416":"# all_data = all_data[['Survived', 'Sex', 'Surname', 'Title', 'Pclass', 'TicketGroup', 'FamilySize', 'CategoricalAge', 'CategoricalFare']]","19aaa881":"train = all_data[all_data['Survived'].notnull()]\ntest = all_data[all_data['Survived'].isnull()].drop('Survived', axis = 1)\n\ntest = test.iloc[:, 1:].reset_index(drop=True)\nX_train = train.iloc[:, 2:]\ny_train = train.iloc[:, 1]","9190a6d2":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import GridSearchCV","ea775f93":"# feature selection and transformer\nnum_att = ['Pclass','FamilySize']\ncat_att1 =  ['CategoricalAge', 'CategoricalFare']\ncat_att2 = ['Sex', 'Embarked', 'Surname']\ndrop_att = list(set(all_features) - set(num_att+cat_att1+cat_att2))\n\ntransformer_RF = ColumnTransformer([\n    (\"toDrop\",'drop', drop_att),\n    (\"numerical\",StandardScaler(), num_att),\n    (\"categorical1\",OrdinalEncoder(), cat_att1),\n    (\"categorical2\",OneHotEncoder(handle_unknown='ignore'), cat_att2)\n])","7d595df1":"# build and train a model\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators=100,\n                                       max_features='auto',\n                                       criterion='entropy',\n                                       max_depth=6,\n                                       random_state=5)","bc40e20e":"# make a pipeline\nrandom_forest_pipe = Pipeline([('RandomForest transformer', transformer_RF), \n                               ('Random Forest', random_forest)])\n\nrandom_forest_pipe.fit(X_train, y_train)\ny_prediction = random_forest_pipe.predict(test)\n\nacc_random_forest_pipe = round(random_forest_pipe.score(X_train,y_train) * 100, 2)\nprint(round(acc_random_forest_pipe,2,), \"%\")","05390c27":"# submit\nsubmission = pd.DataFrame({'PassengerId' : PassengerId, 'Survived' : y_prediction.astype(np.int32)})\nsubmission.to_csv(r\"submission_RandomForest.csv\", index = False)\n\nprint(metrics.accuracy_score(survived, y_prediction))","0f5f2c94":"# feature selection and transformer\nnum_att = ['Pclass', 'SibSp', 'Parch', 'FamilySize']\ncat_att1 =  ['CategoricalAge', 'CategoricalFare', 'Deck', 'TicketGroup']\ncat_att2 = ['Sex', 'Embarked', 'Surname', 'Title', 'IsAlone']\ndrop_att = list(set(all_features) - set(num_att+cat_att1+cat_att2))\n\ntransformer_GBT = ColumnTransformer([\n    (\"toDrop\",'drop', drop_att),\n    (\"numerical\",StandardScaler(), num_att),\n    (\"categorical1\",OrdinalEncoder(), cat_att1),\n    (\"categorical2\",OneHotEncoder(handle_unknown='ignore'), cat_att2)\n])","897e0580":"# build and train a model\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbt = GradientBoostingClassifier(n_estimators=100,\n                                 learning_rate=1.0,\n                                 max_depth=1,\n                                 random_state=100)","539af9cc":"# make a pipeline\ngradient_boosting_pipe = Pipeline([('GradientBoosting transformer', transformer_GBT), \n                  ('Gradient Boosting', gbt)])\n\ngradient_boosting_pipe.fit(X_train, y_train)\ny_prediction= gradient_boosting_pipe.predict(test)\n\nacc_gradient_boosting_pipe = round(gradient_boosting_pipe.score(X_train, y_train) * 100, 2)\nprint(round(acc_gradient_boosting_pipe,2,), \"%\")","2c20baf8":"submission = pd.DataFrame({'PassengerId' : PassengerId, 'Survived' : y_prediction.astype(np.int32)})\nsubmission.to_csv(r\"submission_GradientBoosting.csv\", index = False)\n# real score: 0.75837\n\nprint(metrics.accuracy_score(survived, y_prediction))","3ee71aa3":"# feature selection and transformer\nnum_att = ['Pclass']\ncat_att1 =  ['CategoricalAge', 'CategoricalFare']\ncat_att2 = ['Sex', 'Surname', 'Title']\ndrop_att = list(set(all_features) - set(num_att+cat_att1+cat_att2))\n\ntransformer_LR = ColumnTransformer([\n    (\"toDrop\",'drop', drop_att),\n    (\"numerical\",StandardScaler(), num_att),\n    (\"categorical1\",OrdinalEncoder(), cat_att1),\n    (\"categorical2\",OneHotEncoder(handle_unknown='ignore'), cat_att2)\n])","cf436429":"# build and train a model\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state=0)","d736831c":"# make a pipeline\nlogistic_regression_pipe = Pipeline([('LogisticRegression transformer', transformer_LR), \n                  ('Logistic Regression', logreg)])\n\nlogistic_regression_pipe.fit(X_train, y_train)\ny_prediction = logistic_regression_pipe.predict(test)\n\nacc_logistic_regression_pipe = round(cross_val_score(logistic_regression_pipe,X_train,y_train,cv=10,scoring='accuracy').mean() * 100, 2)\nprint(round(acc_logistic_regression_pipe,2,), \"%\")","351bfedf":"submission = pd.DataFrame({'PassengerId' : PassengerId, 'Survived' : y_prediction.astype(np.int32)})\nsubmission.to_csv(r\"submission_LogisticRegression.csv\", index = False)\n# real score: 0.74880\n\nprint(metrics.accuracy_score(survived, y_prediction))","75b6784c":"# feature selection and transformer\nnum_att = ['Pclass', 'SibSp', 'Parch', 'FamilySize']\ncat_att1 =  ['CategoricalAge', 'CategoricalFare', 'Deck', 'TicketGroup']\ncat_att2 = ['Sex', 'Embarked', 'Surname', 'Title', 'IsAlone']\ndrop_att = list(set(all_features) - set(num_att+cat_att1+cat_att2))\n\ntransformer_DT = ColumnTransformer([\n    (\"toDrop\",'drop', drop_att),\n    (\"numerical\",StandardScaler(), num_att),\n    (\"categorical1\",OrdinalEncoder(), cat_att1),\n    (\"categorical2\",OneHotEncoder(handle_unknown='ignore'), cat_att2)\n])","123c9a81":"# build and train a model\nfrom sklearn.tree import DecisionTreeClassifier\n\n# X_train_prep = transformer_DT.fit_transform(X_train)\n\n# criterion = [\"gini\", \"entropy\"]\n# max_depth = list(range(1,12,1))\n# max_features = list(range(1, 20))\n# splitter = [\"best\",\"random\"]\n# min_samples_split = [2,3]\n# hyperparams = {'criterion': criterion, 'max_depth': max_depth, 'max_features': max_features, \n#                'splitter': splitter, 'min_samples_split': min_samples_split}\n\n# gd = GridSearchCV(estimator = DecisionTreeClassifier(random_state=1), param_grid = hyperparams, verbose=0, \n#                 cv=10, scoring = \"roc_auc\")\n\n# gd.fit(X_train_prep, y_train)\n# print(gd.best_score_)\n# print(gd.best_estimator_)\n\ndecision_tree = DecisionTreeClassifier(criterion='entropy',\n                                       max_depth=10,\n                                       max_features='auto',\n                                       splitter='random',\n                                       random_state=3)","ca17f0a4":"# make a pipeline\ndecision_tree_pipe = Pipeline([('DecisionTree transformer', transformer_DT), \n                  ('Decision Tree', decision_tree)])\n\ndecision_tree_pipe.fit(X_train, y_train)\ny_prediction = decision_tree_pipe.predict(test)\n\nacc_decision_tree_pipe = round(decision_tree_pipe.score(X_train, y_train) * 100, 2)\nprint(round(acc_decision_tree_pipe,2,), \"%\")","992037ce":"submission = pd.DataFrame({'PassengerId' : PassengerId, 'Survived' : y_prediction.astype(np.int32)})\nsubmission.to_csv(r\"submission_DecisionTree.csv\", index = False)\n\nprint(metrics.accuracy_score(survived, y_prediction))","563ff33a":"# feature selection and transformer\nnum_att = ['Pclass']\ncat_att1 =  ['CategoricalAge',  'TicketGroup']\ncat_att2 = ['Surname', 'Title', 'IsAlone']\ndrop_att = list(set(all_features) - set(num_att+cat_att1+cat_att2))\n\ntransformer_XGB = ColumnTransformer([\n    (\"toDrop\",'drop', drop_att),\n    (\"numerical\",StandardScaler(), num_att),\n    (\"categorical1\",OrdinalEncoder(), cat_att1),\n    (\"categorical2\",OneHotEncoder(handle_unknown='ignore'), cat_att2)\n])","5f9a5072":"# build and train a model\nfrom xgboost import XGBClassifier\n\nparams_xgb = {'colsample_bylevel': 0.7,\n              'learning_rate': 0.03,\n              'max_depth': 3, \n              'n_estimators': 400,\n              'reg_lambda': 15,\n              'subsample': 0.5}\n\nxgb = XGBClassifier(**params_xgb, random_state=0)","2226e2a9":"# make a pipeline\nxgboost_pipe = Pipeline([('XGBoost transformer', transformer_XGB), \n                  ('XGBoost', xgb)])\n\nxgboost_pipe.fit(X_train, y_train)\ny_prediction = xgboost_pipe.predict(test)\n\nacc_xgboost_pipe = round(cross_val_score(xgboost_pipe,X_train,y_train,cv=10,scoring='accuracy').mean() * 100, 2)\nprint(round(acc_xgboost_pipe,2,), \"%\")","3f5baab9":"submission = pd.DataFrame({'PassengerId' : PassengerId, 'Survived' : y_prediction.astype(np.int32)})\nsubmission.to_csv(r\"submission_XGBoost.csv\", index = False)\n# real score: 0.76315\n\nprint(metrics.accuracy_score(survived, y_prediction))","1e719308":"# feature selection and transformer\nnum_att = ['Pclass', 'FamilySize']\ncat_att1 =  ['CategoricalAge', 'CategoricalFare', 'Deck', 'TicketGroup']\ncat_att2 = ['Sex', 'Embarked', 'Surname', 'Title', 'IsAlone']\ndrop_att = list(set(all_features) - set(num_att+cat_att1+cat_att2))\n\ntransformer_SVM = ColumnTransformer([\n    (\"toDrop\",'drop', drop_att),\n    (\"numerical\",StandardScaler(), num_att),\n    (\"categorical1\",OrdinalEncoder(), cat_att1),\n    (\"categorical2\",OneHotEncoder(handle_unknown='ignore'), cat_att2)\n])","4fb602c2":"# build and train a model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\n\nsvm = LinearSVC(C=0.1, loss = 'hinge', max_iter=100000, random_state=0)","0c164e05":"# make a pipeline\nsvm_pipe = Pipeline([('SVM transformer', transformer_SVM), \n                  ('SVM', svm)])\n\nsvm_pipe.fit(X_train, y_train)\ny_prediction= svm_pipe.predict(test)\n\nacc_svm_pipe = round(svm_pipe.score(X_train, y_train) * 100, 2)\nprint(round(acc_svm_pipe,2,), \"%\")","b6c93981":"submission = pd.DataFrame({'PassengerId' : PassengerId, 'Survived' : y_prediction.astype(np.int32)})\nsubmission.to_csv(r\"submission_SVM.csv\", index = False)\n# real score: 0.77033\n\nprint(metrics.accuracy_score(survived, y_prediction))","93a2878f":"# feature selection and transformer\nnum_att = []\ncat_att1 =  ['CategoricalAge', 'CategoricalFare']\ncat_att2 = ['Sex', 'IsAlone']\ndrop_att = list(set(all_features) - set(num_att+cat_att1+cat_att2))\n\ntransformer_NB = ColumnTransformer([\n    (\"toDrop\",'drop', drop_att),\n    (\"numerical\",StandardScaler(), num_att),\n    (\"categorical1\",OrdinalEncoder(), cat_att1),\n    (\"categorical2\",OneHotEncoder(handle_unknown='ignore'), cat_att2)\n])","6f1169f0":"# build and train a model\nfrom sklearn.naive_bayes import CategoricalNB\n\nnaive_bayes = CategoricalNB(alpha=0.05)","c6edcb69":"# make a pipeline\nnaive_bayes_pipe = Pipeline([('NaiveBayes transformer', transformer_NB), \n                  ('Naive Bayes', naive_bayes)])\n\nnaive_bayes_pipe.fit(X_train, y_train)\ny_prediction= naive_bayes_pipe.predict(test)\n\nacc_naive_bayes_pipe = round(cross_val_score(naive_bayes_pipe,X_train,y_train,cv=10,scoring='accuracy').mean() * 100, 2)\nprint(round(acc_naive_bayes_pipe,2,), \"%\")","c5a0918c":"# submit\nsubmission = pd.DataFrame({'PassengerId' : PassengerId, 'Survived' : y_prediction.astype(np.int32)})\nsubmission.to_csv(r\"submission_naive_bayes.csv\", index = False)\n# real score: 0.75837\n\nprint(metrics.accuracy_score(survived, y_prediction))","3dd60af2":"from sklearn.ensemble import VotingClassifier","bbf968b5":"voting_all = VotingClassifier(\n    estimators = [('Random Forest', random_forest_pipe),\n                  ('Gradient Boosting', gradient_boosting_pipe),\n                  ('Logistic Regression', logistic_regression_pipe),\n                  ('Decision Tree', decision_tree_pipe),\n                  ('Xgboost', xgboost_pipe),\n                  ('Support Vector Machine', svm_pipe),\n                  ('Naive Bayes', naive_bayes_pipe),],\n    voting = 'hard'\n)\n\nvoting_all.fit(X_train, y_train)\ny_prediction= voting_all.predict(test)\n\nacc_voting_all = round(voting_all.score(X_train,y_train) * 100, 2)\nprint(round(acc_voting_all,2,), \"%\")","8cf8acd4":"# submit\nsubmission = pd.DataFrame({'PassengerId' : PassengerId, 'Survived' : y_prediction.astype(np.int32)})\nsubmission.to_csv(r\"submission_voting_RF_NB.csv\", index = False)\n# real score: 0.75837\n\nprint(metrics.accuracy_score(survived, y_prediction))","36f7e906":"voting_RF_NB = VotingClassifier(\n    estimators = [('Naive Bayes',naive_bayes_pipe),('Decision Tree',random_forest_pipe)],\n    voting = 'hard'\n)\n\nvoting_RF_NB.fit(X_train, y_train)\ny_prediction= voting_RF_NB.predict(test)\n\nacc_voting_RF_NB = round(cross_val_score(voting_RF_NB,X_train,y_train,cv=10,scoring='accuracy').mean() * 100, 2)\nprint(round(acc_voting_RF_NB,2,), \"%\")","faff678c":"# submit\nsubmission = pd.DataFrame({'PassengerId' : PassengerId, 'Survived' : y_prediction.astype(np.int32)})\nsubmission.to_csv(r\"submission_voting_RF_NB.csv\", index = False)\n# real score: 0.75837\n\nprint(metrics.accuracy_score(survived, y_prediction))","bbe62fd2":"### (7) naive bayes","3037923b":"# 4 feature extraction","93f4b296":"## 5.1 train model","73b3a9de":"### (6) support vector machine","57b866e3":"# Introduction\n* This is my first practice project for landing in Kaggle\n* I have seen more than 10 notebooks from other great kagglers, thanks and admires\n* Notice: train for analysis, test for evaluation, but process both at the same time\n* Refered to these great kernels which I upvoted, thanks and regards:\n\n    1. [Titanic to beginner](https:\/\/www.kaggle.com\/adamml\/titanic-to-beginner)\n    2. [Getting Started with Titanic using neural network](https:\/\/www.kaggle.com\/theblackmamba31\/getting-started-with-titanic-using-neural-network\/notebook)\n    3. [Titanic Modeling: NBayes + DTree - 0.8253](https:\/\/www.kaggle.com\/mkulio\/titanic-modeling-nbayes-dtree-0-8253#Adding-new-features)\n    4. [Titanic Survival Prediction](https:\/\/www.kaggle.com\/vaishnavikhilari\/titanic-survival-prediction#7.-Model)\n    5. [A Data Science Framework: To Achieve 99% Accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy#Step-7:-Optimize-and-Strategize)","8ee5cf5a":"### (4) Decision Tree","bf1b85b2":"## 2.1 overview","1c3eef44":"### (1) random forest","564aa4e9":"### (1) voting all","f5270de9":"## 2.2 the relationships between every features and survival","b7ab16dc":"# 6 ensemble models","fe6996be":"## 3.4 show the correlations","048fe063":"* Notice all the features and their missing values","011dd1fb":"There's a problem when trying to encode the 'Surname' since there's some unknown values in test set.\n\nSo when the ColumnTransformer encodes both of the train and test set, there will be an improvement.\n\nBut since we are going to make a more practical model which means we may meet a different surname in the future, so we just ignore those unknown values with \"handle_unknown='ignore'\".","77071d49":"# 1 Load Data","50f82220":"# Background\n\n### The Challenge\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. In this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n### Overview\n\nThe data has been split into two groups:\n* training set (train.csv)\n* test set (test.csv)\n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n\n### Data Dictionary\n\n|Variable|Definition|Key|\n|:-|:-|:-|\n|survival|Survival|0 = No, 1 = Yes|\n|pclass|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd|\n|sex|Sex||\n|Age|Age in years||\n|sibsp|# of siblings \/ spouses aboard the Titanic||\n|parch|# of parents \/ children aboard the Titanic||\n|ticket|Ticket number||\n|fare|Passenger fare||\n|cabin|Cabin number||\n|embarked|Port of Embarkation||","4176a497":"## 3.3 transform features","8d39bc89":"## 5.2 Predict Data","d1a52dfe":"# 3 Process Data","8d955bdf":"### (5) Xgboost","7fd2ce64":"### (2) GradientBoosting","f27d791f":"create a ColumnTransformer for each model, so that we can have different features for different model\n\nthen, make a pipeline for each 'ColumnTransformer and model' pair in order to make voting","3d51b626":"1. random_forest 2. gbt 3. logreg 4. decision_tree 5. xgb 6. svm 7. native_bayes","b61c368e":"## 3.2 fill missing values","ccc82e43":"### (3) logistic regression","a4a0f6ed":"## 3.1 create new features with name, family and ticket","c8d9c9a7":"# 2 Analyse Data","172b704f":"### (2) voting Random Forest and Naive Bayes","a0b2bd02":"# 5 Build Model"}}