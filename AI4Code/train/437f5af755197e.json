{"cell_type":{"3fa733a4":"code","ca5d8fad":"code","5de07671":"code","797f557b":"code","8d67b2cd":"code","de647a86":"code","c3fa2fb7":"code","f60b62e3":"code","747da5f6":"code","dd44d9c0":"code","0afdd3b5":"code","0b69ecbb":"code","49a9ad68":"code","81ea8123":"code","20c12f2f":"code","9a00a23f":"code","daa83424":"code","317aab4e":"code","2c5ccb7a":"code","cd577407":"code","4540fc69":"code","5819c9e1":"code","e60171cc":"code","45625de0":"code","f4ccdeec":"code","8002eaff":"code","16da8991":"code","b6cef0bd":"code","419f3a84":"code","5c380d71":"code","c5a83f1a":"code","552edd71":"code","6f7e93d1":"code","2283a3f8":"code","968b98b7":"code","b18d1022":"code","27ad77e5":"code","c0fdd932":"code","a901c61b":"code","78e00c0a":"code","da04544d":"code","4ecd3597":"code","abdebbe5":"code","cf22db8a":"code","fe382c93":"code","65dcb29a":"code","37b925f6":"code","5cfc02ae":"code","1a9b8290":"code","76d5a4a8":"code","6c7a44d1":"code","2dd2f02d":"code","44ca64d2":"code","e6e255a4":"code","352ce606":"code","271fea03":"code","b7447a21":"code","f076ce46":"code","7cefbff4":"code","b587a977":"code","ed848b65":"code","d6cf9560":"code","6a3cfc25":"code","f3138ade":"code","5f69ee2f":"code","5db5251e":"code","80e5284a":"code","32ed56e6":"code","203b82d9":"code","610e9abb":"code","6ca39604":"code","6612d542":"code","9a0cae3a":"code","0f03d77c":"code","f63074a0":"code","47f640e5":"code","f5b86f38":"code","ce281f43":"code","86a72215":"code","f4810b9e":"code","723bd927":"code","2b0a00be":"code","7c09a3a7":"code","3effa010":"code","9aaab106":"code","90e9dd77":"code","1a00f3f1":"code","2dfc114e":"code","e3540681":"code","0d46557c":"code","003f360f":"code","c1bb839b":"code","898add8c":"code","63231a68":"code","810096b1":"code","d29f672e":"code","9590b1f5":"code","0e6c86f1":"code","2eb267e3":"code","960a386c":"code","866ed207":"code","75377ee4":"code","dd493a42":"markdown","c7ba9a41":"markdown","94bd06c4":"markdown","3622bfc2":"markdown","6bd27ad7":"markdown","ccf3cd0f":"markdown","25e5b133":"markdown","91bde8d6":"markdown","22ece3d6":"markdown","04b6445a":"markdown","53b7db7e":"markdown","3f90e882":"markdown","c3c0c726":"markdown","d1c582f2":"markdown","92024037":"markdown","3167b4d9":"markdown","c67a615b":"markdown","f416d671":"markdown","7b5ed369":"markdown","b2433e57":"markdown","54163e6b":"markdown","9038785d":"markdown","bd77d059":"markdown","01041d18":"markdown","0eaea9fb":"markdown","2378100f":"markdown","fccc1936":"markdown","bf4e184e":"markdown","e2e5095d":"markdown","3e64a8fc":"markdown","5a3cbca5":"markdown","38f9b5a1":"markdown","17e38099":"markdown","4df2648d":"markdown","e0a00ee9":"markdown","6e80566a":"markdown","3fb1112b":"markdown","eb683b73":"markdown","8004a05f":"markdown","0ce78254":"markdown","8ec71a87":"markdown","641b2ee0":"markdown","4db94f4e":"markdown","6794b698":"markdown","77921829":"markdown","6350aa9a":"markdown","6db9f549":"markdown","8afc830a":"markdown","cbf44c3a":"markdown","34f3819b":"markdown","7ac40411":"markdown","74be034d":"markdown","a369c746":"markdown","184deabd":"markdown","48432d7e":"markdown","f04dbc2b":"markdown","c1769980":"markdown","70cadf13":"markdown","815a533e":"markdown","9f83442e":"markdown","6507b132":"markdown","10d07ad8":"markdown","f18e5b4e":"markdown","16328413":"markdown","e4c04311":"markdown","c9eb50d0":"markdown","e5295a80":"markdown","d4168e0f":"markdown","1e2cd304":"markdown","e5b60891":"markdown","4417d202":"markdown","8cb75273":"markdown","017fe1bc":"markdown","5e077c11":"markdown","5f1360d0":"markdown","c575345f":"markdown","1d8c23fe":"markdown","4d6c7dbe":"markdown","765f394d":"markdown"},"source":{"3fa733a4":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import plot_tree\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix,roc_curve, roc_auc_score, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder","ca5d8fad":"wine = pd.read_csv('..\/input\/beginner-datasets\/beginner_datasets\/wine.csv')\n\nwine # display the first and last 5 rows","5de07671":"wine.info() # we would like to see if we have null values in the df","797f557b":"wine.shape # 6497 rows and 13 columns","8d67b2cd":"wine.isnull().sum() #for NaN","de647a86":"wine.duplicated().sum() # duplicates may point on the same wine, it can make our model unstable","c3fa2fb7":"wine = wine.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)","f60b62e3":"wine.shape # 5320 rows and 13 columns","747da5f6":"count_cat = wine.nunique() # it shows us how much unique examples we have in each feature","dd44d9c0":"count_cat","0afdd3b5":"wine[\"type\"].value_counts() # it let us understand the number of the examples we have in the dataset","0b69ecbb":"# The distribution of the examples in our dataset\nwine[\"type\"].value_counts(\"white\")*100 # in %\nwine[\"type\"].value_counts(\"red\")*100 # in %","49a9ad68":"plt.figure(figsize = (10,5))\nsns.countplot(x = wine['type']); # we will plot it for better illustration","81ea8123":"# Pie chart\nlabels = ['White', 'Red'] # every label represnts a type\npie_x = 0.753886 # the distribution\npie_y = 0.246114 # the distribution\nsizes = [pie_x,pie_y] # we have two types and their distribution\nexplode = (0, 0.15) # the distance between the pic slicers\ncol_pie = [\"silver\",\"red\"] # for the colors\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels,colors=col_pie, autopct='%1.1f%%',\n        shadow=True, startangle=93)\n# equal aspect ratio ensures that pie is drawn as a circle\nplt.title(\"Types division:\", size=20, color=\"blue\")\nax1.axis('equal')  \nplt.tight_layout()\nplt.show()","20c12f2f":"# another plot to show us more intersting details about the dataset\nplt.figure(figsize = (16,6))\nplt.title(\"The quality distribution of the wine according to it's type\", size=18, color='b')\nsns.countplot(wine['quality'], hue = wine['type']);","9a00a23f":"wine[\"quality\"].value_counts() # the amount of wine we have from each quality number","daa83424":"red_per_quality = wine.loc[wine[\"type\"]=='red'][\"quality\"].value_counts() # only the red wine, the sum is sorted by quality\nwhite_per_quality = wine.loc[wine[\"type\"]=='white'][\"quality\"].value_counts() # only white...\nquality_dist = pd.DataFrame({ # new df for our two new series\n        'red': red_per_quality,\n        'white':white_per_quality})\nquality_dist.fillna(value = 0, inplace=True) # if we dont have any data = we dont't have wine from that quality\n\nquality_dist","317aab4e":"red_wine = wine.loc[wine[\"type\"]=='red'] # we chose only red wine\nred_wine_5 = red_wine.loc[red_wine[\"quality\"]==6] # we chose only quality 6\nred_wine_5","2c5ccb7a":"df_types = wine.groupby('type') #by type\ndf_types","cd577407":"df_types.quality.mean() # the mean of the quality for each type","4540fc69":"encoder = LabelEncoder()\nwine['type'] = encoder.fit_transform(wine['type'])\n# red = 0\n# white = 1","5819c9e1":"wine[\"type\"].value_counts()","e60171cc":"wine","45625de0":"#I want to know the correalation between each feature\nplt.figure(figsize=(20, 10))\nsns.heatmap(wine.corr(), square=True, annot=True, cmap=\"Blues\");","f4ccdeec":"wine.groupby('type')['total sulfur dioxide'].plot(title=\"Type correlation with total sulfur dioxide\", legend=True);\n# the most important thing here is to watch the difference between the red and white wine\n# we can also see that there are a few outliers ","8002eaff":"wine.groupby('type')['free sulfur dioxide'].plot(title=\"Type correlation with free sulfur dioxide\", legend=True);","16da8991":"wine.groupby('type')['residual sugar'].plot(title=\"Type correlation with residual sugar\", legend=True);","b6cef0bd":"wine.describe() # more info about our dataset before the models part","419f3a84":"features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\nss = StandardScaler()\ndf = wine\ndf[features] = ss.fit_transform(df[features])\ndf.head(3)","5c380d71":"#df['type']=df['type'].map({'red':0,'white':1})\n#df","c5a83f1a":"target=df['type'] # the feature we would like to predict, the type of the wine\ndata=df.drop(['type'], axis = 1) # we will drop y from x, because we want to predict it\nX_train,X_test,y_train,y_test = train_test_split(data,target,random_state=0) #train\\test split","552edd71":"X_train.shape # 3990 rows, 12 columns","6f7e93d1":"X_test.shape # 1330 rows, 12 columns","2283a3f8":"y_train.shape # 3990 rows, one column","968b98b7":"y_test.shape # 1330 rows, one columns","b18d1022":"# our options: 'most_frequent', 'stratified', 'uniform', 'constant', 'prior')\n# I chose the best dummy classifier from all those options (please, believe me)\n\ndm = DummyClassifier(strategy='most_frequent', random_state=0)\n# random_state = 0 assure the result will be the same every time we run the model (no random results)\ndm.fit(X_train,y_train) # training the model\ndm","27ad77e5":"y_pred = dm.predict(X_test) # predict the result","c0fdd932":"print(classification_report(y_test,y_pred))","a901c61b":"cm_dm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_dm,annot=True,annot_kws = {'size':15},fmt=\".0f\")\nplt.xlabel(\"Predict\")\nplt.ylabel(\"Actual\") #up and right = FP , down and right = FN","78e00c0a":"fpr, tpr, _= roc_curve(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc), color='r')\nplt.box(False)\nplt.title('ROC CURVE Dummy Classifier')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n\ndummy_auc = round(auc,3)*100\n \nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","da04544d":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn","4ecd3597":"y_pred = knn.predict(X_test)","abdebbe5":"print (classification_report(y_test, y_pred))","cf22db8a":"k_range = list(range(3,51)) # we will test k values in range 3-50\nweight_op = ['uniform', 'distance'] # we will test the knn methods uniform & distance\nd = {'n_neighbors' :k_range, 'weights': weight_op}","fe382c93":"grid_temp = GridSearchCV(knn, d, cv=10, scoring='accuracy') # we chose model, d(range, methods), num of cv groups and scoring method\ngrid_temp.fit(data, target)\nprint(\"score:\",grid_temp.best_score_,\" params:\",grid_temp.best_params_)","65dcb29a":"knn = KNeighborsClassifier(n_neighbors = 6, weights = 'distance')\nknn.fit(X_train,y_train)\nknn","37b925f6":"y_pred = knn.predict(X_test)\ny_pred","5cfc02ae":"print (classification_report(y_test,y_pred))","1a9b8290":"cm_knn = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_knn,annot=True,annot_kws = {'size':15},fmt=\".0f\")\nplt.xlabel(\"Predict\")\nplt.ylabel(\"Actual\") #up and right = FP , down and right = FN","76d5a4a8":"fpr, tpr, _= roc_curve(y_test, y_pred)\nauc= roc_auc_score(y_test, y_pred)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc), color='g')\nplt.box(False)\nplt.title('ROC CURVE KNN')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n\nknn_auc = round(auc,3)*100\n\nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","6c7a44d1":"dct = DecisionTreeClassifier(random_state=0) # gini and best are the default\ndct = dct.fit(X_train,y_train)\ndct","2dd2f02d":"y_pred = dct.predict(X_test)","44ca64d2":"print(classification_report(y_test,y_pred))","e6e255a4":"path = dct.cost_complexity_pruning_path(X_train,y_train)\nalphas = path.ccp_alphas\nimpurities = path.impurities\n# it returns the effective alphas and the corresponding total leaf impurities at each step of the pruning process\nalphas","352ce606":"d = {'ccp_alpha':alphas}\nd","271fea03":"fig, ax = plt.subplots()\nax.plot(alphas[:-1], impurities[:-1], marker = 'o', drawstyle = \"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\");\n#  as alpha increases, more of the tree is pruned, which increases the total impurity of its leaves","b7447a21":"grid_dct = GridSearchCV(dct, d, cv=10, scoring='accuracy')\ngrid_dct.fit(data, target)\nprint(\"score:\", grid_dct.best_score_, \" params:\", grid_dct.best_params_)","f076ce46":"dct = DecisionTreeClassifier(ccp_alpha = 0.0004761904761904761)\ndct = dct.fit(X_train, y_train)\ndct","7cefbff4":"y_pred = dct.predict(X_test)\ny_pred","b587a977":"print(classification_report(y_test,y_pred))","ed848b65":"cm_dct = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_dct,annot=True,annot_kws = {'size':15},fmt=\".0f\")\nplt.xlabel(\"Predict\")\nplt.ylabel(\"Actual\") #up and right = FP , down and right = FN","d6cf9560":"plt.figure(figsize=(15,7.5))\nplot_tree(dct,filled=True);","6a3cfc25":"fpr, tpr, _= roc_curve(y_test, y_pred)\nauc= roc_auc_score(y_test, y_pred)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc), color='b')\nplt.box(False)\nplt.title('ROC CURVE Decision Tree')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n\ndct_auc = round(auc,3)*100\n \nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","f3138ade":"lr = LogisticRegression(random_state=0)\nlr.fit(X_train, y_train)\npredictions = lr.predict(X_test)","5f69ee2f":"print(classification_report(y_test,predictions))","5db5251e":"cm = confusion_matrix(y_test, predictions)\nsns.heatmap(cm,annot=True,annot_kws = {'size':15},fmt=\".0f\")\nplt.xlabel(\"Predict\")\nplt.ylabel(\"Actual\") #up and right = FP , down and right = FN","80e5284a":"print(metrics.accuracy_score(y_test, predictions))","32ed56e6":"fpr, tpr, _= roc_curve(y_test, predictions)\nauc= roc_auc_score(y_test, predictions)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc), color='orange')\nplt.box(False)\nplt.title('ROC CURVE Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n\nlr_auc = round(auc,3)*100\n \nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","203b82d9":"print(\"The score for the models:\\n\")\nprint(\"Dummy Classifer:      \",dummy_auc,\"%\")\nprint(\"KNN:                  \",knn_auc,\"%\")\nprint(\"Decision Tree:        \",dct_auc,\"%\")\nprint(\"Logistic Regression:  \",lr_auc,\"%\")","610e9abb":"import warnings\nimport plotly.offline as pyo\n\n# new models:\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom xgboost import XGBClassifier\n\n# A model that I learned by myself: CatBoost + Plotly\n\nfrom catboost import CatBoostClassifier\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport plotly.express as px\n\n# Clustering:\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import Pipeline\n\n#PCA:\n\nfrom sklearn.decomposition import PCA\n\n#Scaling:\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler","6ca39604":"wine_df = pd.read_csv('..\/input\/beginner-datasets\/beginner_datasets\/wine.csv')\n\nwine_df.sample(n = 5, random_state = 123).sort_values(by = 'type') # 5 random samples from the data","6612d542":"wine_df[wine_df['type']=='red'].describe() # for red wine","9a0cae3a":"wine_df[wine_df['type']=='white'].describe() # for white wine","0f03d77c":"wine_df.info(memory_usage = \"deep\")","f63074a0":"corr = wine_df\n\nmapper = {'red':1, 'white':0}\ncorr['type'] = corr.type.map(mapper)\n\nplt.figure(figsize=(16, 8))\nsns.heatmap(corr.corr(), square=False, annot=True);","47f640e5":"# A function to show the labels\ndef num_to_name(label):\n    labeled = label.copy()\n    mapping = {1:'red', 0:'white'}\n    labeled = label.map(mapping)\n    return labeled","f5b86f38":"target = wine_df['type'] # the feature we would like to predict, the type of the wine\ndata = wine_df.drop(['type'], axis = 1) # we will drop y from x, because we want to predict it","ce281f43":"X_train,X_test,y_train,y_test = train_test_split(data, target, random_state=18) # test size = 0.25 as default","86a72215":"# we will scale the data after we splited it to train and test\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f4810b9e":"# I want to understand how reduction of features will harm the understanding of the data:\n\npca = PCA() # all 784 features\npca.fit(X_train)\n\nexp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\nfig = px.area(\n    title = \"Explained variance as a function of the number of dimensions:\",\n    x=range(1, exp_var_cumul.shape[0] + 1),\n    y=exp_var_cumul * 100,\n    labels={\"x\": \"# of Features\", \"y\": \"Explained Variance\"},\n    width = 1000 ,\n    height = 500\n)\nfig.show()","723bd927":"pca = PCA(n_components=0.95) # minimum 95 % of explained variance\nX_train_reduced = pca.fit_transform(X_train) # fit + transform to X_train\nX_test_reduced = pca.transform(X_test) # transform (WITHOUT FIT) to X_test\npca.n_components_ # number of features we will use","2b0a00be":"# A 3D graph which shows the data distribution\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\n\nfig = px.scatter_3d(\n    X_train_reduced, x=0, y=1, z=2, color = num_to_name(y_train),\n    color_discrete_sequence=[\"red\", \"grey\"],\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n)\nfig.show()","7c09a3a7":"# 2D version:\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter(\n    X_train_reduced, x=0, y=1, color = num_to_name(y_train),\n    color_discrete_sequence=[\"red\", \"grey\"],\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2'}\n)\nfig.show()","3effa010":"X_train = pd.DataFrame(X_train_reduced)\nX_test = pd.DataFrame(X_test_reduced)","9aaab106":"# params from last semester\n\nknn = KNeighborsClassifier(n_neighbors = 6, weights = 'distance')\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nknn_acc = accuracy_score(y_test, y_pred)\nknn_acc","90e9dd77":"print (classification_report(y_test, y_pred))","1a00f3f1":"# from last semester\n\nlr = LogisticRegression(solver = 'lbfgs')\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nlr_acc = accuracy_score(y_test, y_pred)\nlr_acc","2dfc114e":"print (classification_report(y_test, y_pred))","e3540681":"# we didnt use it last semester\n\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nrf_acc = accuracy_score(y_test, y_pred)\nrf_acc","0d46557c":"print (classification_report(y_test, y_pred))","003f360f":"ada = AdaBoostClassifier()\nada.fit(X_train, y_train)\ny_pred = ada.predict(X_test)\nada_acc = accuracy_score(y_test, y_pred)\nada_acc","c1bb839b":"print (classification_report(y_test, y_pred))","898add8c":"xgb = XGBClassifier(use_label_encoder = False)\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\nxgb_acc = accuracy_score(y_test, y_pred)\nxgb_acc","63231a68":"print (classification_report(y_test, y_pred))","810096b1":"cat = CatBoostClassifier(logging_level='Silent')\ncat.fit(X_train, y_train)\ny_pred = cat.predict(X_test)\ncat_acc = accuracy_score(y_test, y_pred)\ncat_acc","d29f672e":"print (classification_report(y_test, y_pred))","9590b1f5":"pipe = Pipeline(steps=[('CatBoost', cat)]) # because it doesnt work without pipeline using\nsearch = GridSearchCV(pipe, param_grid={'CatBoost__iterations':[100,250,500]}, cv=3) # cv on iterations\nsearch.fit(X_train, y_train) # apply cv on train, unless, the test is useless\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","0e6c86f1":"y_pred = search.predict(X_test)\ncat_acc = accuracy_score(y_test, y_pred)\ncat_acc # catboost accuracy after cv & superficial gs","2eb267e3":"acc_list = {'Model':  ['KNN', 'Logistic Regression', 'Random Forest', 'AdaBoost', 'XGBoost', 'CatBoost'],\n        'Accuracy': [knn_acc,lr_acc,rf_acc,ada_acc,xgb_acc,cat_acc],\n        }","960a386c":"fig = go.Figure(data=[\n    go.Bar(name='train set', x=acc_list['Model'], y=acc_list['Accuracy'],text=np.round(acc_list['Accuracy'],2),textposition='outside',marker_color=['darkgreen', 'blue', 'red', 'purple', 'orange', 'yellow']\n          ),\n])\nfig.update_layout(barmode='group',title_text='Accuracy Comparison On Different Models',yaxis=dict(\n        title='Accuracy'))\nfig.show()","866ed207":"# confusion matrix for our best model\n\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6,6))\nplt.imshow(cm, interpolation='nearest', cmap = plt.cm.seismic)\nplt.title('Confusion matrix - CatBoost Classifier', size = 15)\nplt.colorbar()\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, ['Red','White'], rotation=45, size = 12)\nplt.yticks(tick_marks, ['Red','White'], size = 12)\nplt.tight_layout()\nplt.ylabel('Actual label', size = 15)\nplt.xlabel('Predicted label', size = 15)\nwidth, height = cm.shape\nfor x in range(width):\n    for y in range(height):\n        plt.annotate(str(cm[x][y]), xy=(y, x), \n        horizontalalignment='center',\n        verticalalignment='center')","75377ee4":"# ROC curve for our best model:\n\nfpr, tpr, _= roc_curve(y_test, y_pred)\nauc= roc_auc_score(y_test, y_pred)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc), color='cyan')\nplt.box(False)\nplt.title('ROC CURVE CatBoost', size=15)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n\ndct_auc = round(auc,3)*100\n \nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","dd493a42":"## ROC Curve","c7ba9a41":"![image.png](attachment:image.png)","94bd06c4":"##### First of all, we would like to show some of the models we used last semester because we did a dimensionality reduction by PCA and probably the results of the models will change as a result. It will also help us make a comparison based on the models we used last semester and the models we use now.","3622bfc2":"## ROC Curve","6bd27ad7":"# KNN - K-Nearest Neighbors","ccf3cd0f":"##### We will present a comparison of the results of the models by graph so that we can see which of them was the most successful.","25e5b133":"<a name='bookmark' \/>","91bde8d6":"We can also take only the red wine which it's quality is 6:","22ece3d6":"Here's a reminder for a confusion matrix values:","04b6445a":"It seems that we have much more white wine than red wine. It will be usefull for our dummy classifier.","53b7db7e":"# To sum up, CatBoost was the best model : 99.5 %","3f90e882":"### Training - Testing split","c3c0c726":"#### The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset.\n\n\n##### For illustration:","d1c582f2":"## AdaBoost:","92024037":"### Quick review:\n##### Link to the dataset: https:\/\/www.kaggle.com\/ahmettezcantekin\/beginner-datasets\nI Chose wine as my dataset. I would like to create a model which will predict the type of the wine(red\/white), according to it's features.\nAt first, I thought that this model might be a little stupid. We have eyes, we also have mouth. There is no problem to recognize the difference between white and red wine. However, a model has no senses. It can't taste the wine or see it color. The idea that we can use only data to predict at first the color of the wine, and after it to connect person to specific wine, is a big thing. This model may looks a little boring to you, but to me it is the first step of a long journey.\n\nFirstly, we will upload the dataset:","3167b4d9":"We can also display this by a dataframe:","c67a615b":"## ROC Curve","f416d671":"\u03b1 (alpha) is a tuning parameter that we finding using cross validation.","7b5ed369":"##### Updating X_train & X_test after dimentionality reduction:","b2433e57":"#### A statistical model that describes a possible relationship between a qualitative \/ categorical variable, known as the \"explained variable\", and other variables called \"explanatory variables\". The explanatory variables can be qualitative or quantitative. The model makes it possible to estimate the extent of the effect of a change in its value on each of the variables that explain the value of the explained variable. In other words, the model makes it possible to estimate correlations between the explanatory variables and the explained variable.","54163e6b":"# Logistic Regression","9038785d":"#### A graph showing the performance of an ambiguous classifier, due to the decision threshold set for it. The curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) under different acceptance thresholds.","bd77d059":"## Dimensionality Reduction by PCA:","01041d18":"# Conclusion","0eaea9fb":"## Random Forest:","2378100f":"At first, we had 6,497 rows. Now we have 5,320 rows.","fccc1936":"![image.png](attachment:image.png)","bf4e184e":"##### Choosing our PCA value:","e2e5095d":"## We had 98.7 % from KNN last semester. Now we have 99.5 % of success by using PCA and ensemble learning algorithm - CatBoost.","3e64a8fc":"The default split is 75% train , 25% test.","5a3cbca5":"As we can see, the grid search found the ideal k (6) and knn strategy (distance) for us. Let's use them for better performance for our model.","38f9b5a1":"## It can be concluded that KNN is the most accurate model for our dataset, with 98.7 percentages of success.","17e38099":"Let's try something else, we will groupby the wine into type groups to see which one has a greater quality:","4df2648d":"![image.png](attachment:image.png)","e0a00ee9":"# DATASET : Wine Type Classification","6e80566a":"# Models","3fb1112b":"Heatmap is a super important tool, because it tells us the bigger story.\nAs we can see, total sulfur dioxide has a huge effect on the type of the wine (0.7 of correalation). free sulfur dioxide takes the 2nd place with 0.47 of correalation. After it, the residual sugar with 0.35 of correalation.\nAnother intersting thing is the fact that the quality of the wine has a connection to the type of the wine. It will be intresting to find out which one of the types has the higher quality.","eb683b73":"## KNN:","8004a05f":"### Cross validation:","0ce78254":"## CatBoost:","8ec71a87":"#### A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. Criterion is the function to measure the quality of a split. Supported criteria are \u201cgini\u201d for the Gini impurity and \u201centropy\u201d for the information gain. Splitter is the strategy used to choose the split at each node. Supported strategies are \u201cbest\u201d to choose the best split and \u201crandom\u201d to choose the best random split.","641b2ee0":"XGBoost works as newton raphson in function space unlike gradient boosting that works as gradient descent in function space, a second order taylor's approximation is used in the loss function to make the connection to newton raphson method.","4db94f4e":"##### It is easy to see that the CatBoost model was the most successful model. This is very gratifying because it teaches us about the strength that ensemble learning models have.","6794b698":"CatBoost is an open-source software library developed by Yandex. It provides a gradient boosting framework which attempts to solve for Categorical features using a permutation driven alternative compared to the classical algorithm.","77921829":"Let's take a look at the decision tree of our model:","6350aa9a":"It let us understand how much \"categories\" we have for each column.","6db9f549":"# Wine Classifier - PCA & Ensemble Learning\n#####  @ Haim Goldfisher","8afc830a":"## KNN was the most accurate model for this dataset, with 98.7 % of success.","cbf44c3a":"\n\n# Data Visualization: Improvement Part","34f3819b":"## Cross Valdation on the best model by Pipeline & Grid Search","7ac40411":"\n## If you like to jump into the improved model, click here <a href=#bookmark>Data Visualization - Semester B<\/a>","74be034d":"Explantion for the features:\n- **fixed acidity:** most acids involved with wine or fixed or nonvolatile (do not evaporate readily).\n- **volatile acidity:** the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste.\n- **citric acid:** found in small quantities, citric acid can add 'freshness' and flavor to wines.\n- **residual sugar:** the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram\/liter and wines with greater than 45 grams\/liter are considered sweet\n- **chlorides:** the amount of salt in the wine.\n- **free sulfur dioxide:** the free form of $SO_2$ exists in equilibrium between molecular $SO_2$ (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine.\n- **total sulfur dioxide:** amount of free and bound forms of $SO_2$; in low concentrations, $SO_2$ is mostly undetectable in wine, but at free $SO_2$ concentrations over 50 ppm, $SO_2$ becomes evident in the nose and taste of wine.\n- **density:** the density of water is close to that of water depending on the percent alcohol and sugar content.\n- **pH:** describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale.\n- **sulphates:** a wine additive which can contribute to sulfur dioxide gas $(SO_2)$ levels, wich acts as an antimicrobial and antioxidant.\n- **alcohol:** the percent alcohol content of the wine.\n- **quality:** score between 0 and 10.\n- **type:** red\/white.\n","a369c746":"We have 1,177 duplicated rows, we can infer that it referrence to the same wine. Let's drop those rows for better results for our model.","184deabd":"As we can see, the white wine that we have in the dataset, has a little bit better quality than the red wine.","48432d7e":"![image.png](attachment:image.png)","f04dbc2b":"### Training \/ Testing Split:","c1769980":"##### We would like to use PCA for dimensionality reduction:\n##### Principal Component Analysis (PCA) is a classical technique in statistical data analysis, feature extraction and data reduction, aiming at explaining observed signals as a linear combination of orthogonal principal components.","70cadf13":"##### We will convert 'red'\/'white' wine type to 1\/0:","815a533e":"We can see that we have some quality for each wine. Let's take a look on it:","9f83442e":"## XGBoosting:","6507b132":"## Logistic Regression:","10d07ad8":"#### We can see that the reduction of the dimentions didn't hurt the understanding of the data. We can use 9 dimentions instead of 12 and still have more than 95% of explained data.","f18e5b4e":"### Data\/Target Split:","16328413":"We would like to see the impact of total sulfur dioxide, free sulfur dioxide and residual sugar on the wine's type. Let's go back to the graphs.","e4c04311":"# Decision Tree","c9eb50d0":"As we can see, the grid search found the optimal alpha for us. We will use that alpha to improve our model results","e5295a80":"#### The output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors","d4168e0f":"# Conclusion","1e2cd304":"# Scaling","e5b60891":"The most frequent strategy goes always after the majority. In our case, that every wine is white, because 75% of the wine is white.","4417d202":"## ROC Curve","8cb75273":"## Data Processing","017fe1bc":"# Dummy Classifier","5e077c11":"AdaBoost, short for Adaptive Boosting. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. In some problems it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner.","5f1360d0":"#### By using new tools and new models, we would like to get a better result than 98.7% of success.","c575345f":"#### Our model must be better than this model.","1d8c23fe":"##### Describe per type:","4d6c7dbe":"### Cross validation:","765f394d":"![image.png](attachment:image.png)"}}