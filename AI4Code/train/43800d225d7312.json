{"cell_type":{"00523dd8":"code","82fd70db":"code","839410a5":"code","04dcc1dc":"code","d1f3a878":"code","eaaf2e5e":"code","2c05b3ae":"code","ab6316ef":"code","3cf4d598":"code","b225d43f":"code","a54dc8fc":"code","8bc2dc03":"code","79059ea3":"code","89f86636":"code","18db7f8d":"code","c02bd6b4":"markdown","cf3e69fc":"markdown","2a13309f":"markdown","75f81023":"markdown","40a03de4":"markdown","c70f8fcc":"markdown","77aafcc9":"markdown","ab9e482c":"markdown","1dfaa8a7":"markdown","5e82527f":"markdown","670ab408":"markdown","68797b1a":"markdown","a9c911e9":"markdown","f1b37814":"markdown","f882ee81":"markdown","a6802f93":"markdown","81725292":"markdown","9515dfad":"markdown","09c87059":"markdown"},"source":{"00523dd8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.image as implt\nimport os\nimport seaborn as sns\nimport cv2 as cv\n\nfrom PIL import Image\n\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')","82fd70db":"train_path = \"..\/input\/genderdetectionface\/dataset1\/train\"\ntest_path = \"..\/input\/genderdetectionface\/dataset1\/test\"\n\ntrain_woman = sorted(os.listdir(train_path +'\/woman'))\ntrain_man =  sorted(os.listdir(train_path +'\/man'))\n\ntest_woman = sorted(os.listdir(test_path +'\/woman'))\ntest_man =  sorted(os.listdir(test_path +'\/man'))\n","839410a5":"category_names = sorted(os.listdir(train_path))\nimg_pr_cat = []\nfor category in category_names:\n    folder = train_path + '\/' + category\n    img_pr_cat.append(len(os.listdir(folder)))\nsns.barplot(x=category_names, y=img_pr_cat).set_title(\"Number of training images:\")","04dcc1dc":"category_names = sorted(os.listdir(test_path))\nimg_pr_cat = []\nfor category in category_names:\n    folder = test_path + '\/' + category\n    img_pr_cat.append(len(os.listdir(folder)))\nsns.barplot(x=category_names, y=img_pr_cat).set_title(\"Number of test images:\")\n","d1f3a878":"img_1 = implt.imread(train_path +'\/woman\/face_389.jpg')\nimg_2 = implt.imread(train_path +'\/man\/face_486.jpg')\n\nplt.subplot(1, 2, 1)\nplt.title('woman')\nplt.imshow(img_1)       \nplt.subplot(1, 2, 2)\nplt.title('man')\nplt.imshow(img_2) ","eaaf2e5e":"img_size = 50\nwomen_faces = []\nmen_faces = [] \nlabel = []\n\nfor i in train_woman:\n        if os.path.isfile(train_path +'\/woman\/'+ i):\n            faces = Image.open(train_path +'\/woman\/'+ i).convert('L') #converting grey scale            \n            faces = faces.resize((img_size,img_size), Image.ANTIALIAS) #resizing to 50,50\n            faces = np.asarray(faces)\/255.0 #normalizing images\n            women_faces.append(faces)  \n            label.append(1) #label 1 for women\n \nfor i in train_man:\n        if os.path.isfile(train_path+'\/man\/'+ i):\n            faces = Image.open(train_path+'\/man\/'+ i).convert('L')\n            faces = faces.resize((img_size,img_size), Image.ANTIALIAS)\n            faces = np.asarray(faces)\/255.0 #normalizing images\n            men_faces.append(faces)  \n            label.append(0) #label 0 for men          \n           \nx_train = np.concatenate((women_faces,men_faces),axis=0) # training dataset\nx_train_label = np.asarray(label)# label array containing 0 and 1\nx_train_label = x_train_label.reshape(x_train_label.shape[0],1)\n\nprint(\"women_faces:\",np.shape(women_faces) , \"men_faces:\",np.shape(men_faces))\nprint(\"train_dataset:\",np.shape(x_train), \"train_values:\",np.shape(x_train_label))","2c05b3ae":"img_size = 50\nwomen_faces = []\nmen_faces = [] \nlabel = []\n\nfor i in test_woman:\n        if os.path.isfile(test_path +'\/woman\/'+ i):\n            faces = Image.open(test_path +'\/woman\/'+ i).convert('L')            \n            faces = faces.resize((img_size,img_size), Image.ANTIALIAS)\n            faces = np.asarray(faces)\/255.0\n            women_faces.append(faces)  \n            label.append(1)     \n \nfor i in test_man:\n        if os.path.isfile(test_path+'\/man\/'+ i):\n            faces = Image.open(test_path+'\/man\/'+ i).convert('L')\n            faces = faces.resize((img_size,img_size), Image.ANTIALIAS)\n            faces = np.asarray(faces)\/255.0            \n            men_faces.append(faces)\n            label.append(0)                       \n\nx_test = np.concatenate((women_faces,men_faces),axis=0) # test dataset\nx_test_label = np.asarray(label) # corresponding labels\nx_test_label = x_test_label.reshape(x_test_label.shape[0],1)\n\nprint(\"women_faces:\",np.shape(women_faces), \"men_faces:\",np.shape(men_faces))\nprint(\"test_dataset:\",np.shape(x_test), \"test_values:\",np.shape(x_test_label))","ab6316ef":"x = np.concatenate((x_train,x_test),axis=0) #train_data\ny = np.concatenate((x_train_label,x_test_label),axis=0) #test data\nx = x.reshape(x.shape[0],x.shape[1]*x.shape[2]) #flatten 3D image array to 2D\nprint(\"images:\",np.shape(x), \"labels:\",np.shape(y))","3cf4d598":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]\n\nprint(\"train number:\",number_of_train, \"test number:\",number_of_test)","b225d43f":"x_train = X_train.T\nx_test = X_test.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","a54dc8fc":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b\n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b    \n    y_head = sigmoid(z)    \n    loss = -(1-y_train)*np.log(1-y_head)-y_train*np.log(y_head)        \n    cost = (np.sum(loss))\/x_train.shape[1]  # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                   # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients\n\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 50 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is woman (y_head=1),\n    # if z is smaller than 0.5, our prediction is man (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction","8bc2dc03":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # 2500\n    w,b = initialize_weights_and_bias(dimension)\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    train_acc_lr = round((100 - np.mean(np.abs(y_prediction_train - y_train)) * 100),2)\n    test_acc_lr = round((100 - np.mean(np.abs(y_prediction_test - y_test)) * 100),2)\n    # Print train\/test Errors\n    print(\"train accuracy: %\", train_acc_lr)\n    print(\"test accuracy: %\", test_acc_lr)\n    return train_acc_lr, test_acc_lr\n    \n\ntrain_acc_lr, test_acc_lr = logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 500)\n#you can adjust learning_rate and num_iteration to check how the result is affected\n#(for learning rate, try exponentially lower values:0.001 etc.) \n","79059ea3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogreg = LogisticRegression()\ntest_acc_logregsk = round(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)* 100, 2)\ntrain_acc_logregsk = round(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)* 100, 2)","89f86636":"from sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\ntest_acc_perceptron = round(perceptron.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)* 100, 2)\ntrain_acc_perceptron = round(perceptron.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)* 100, 2)","18db7f8d":"models = pd.DataFrame({\n    'Model': ['LR without sklearn','LR with sklearn', 'Perceptron'],\n    'Train Score': [train_acc_lr, train_acc_logregsk, train_acc_perceptron],\n    'Test Score': [test_acc_lr, test_acc_logregsk, test_acc_perceptron]})\nmodels.sort_values(by='Test Score', ascending=False)","c02bd6b4":"Next step, we need to determine the amount of data for **train** and **test**. You can modify **test_size** and see how it affects the accuracy. Let's split!","cf3e69fc":"This notebook is based on:\n* Images Dataset of .jpg files\n* Logistic Regression without sklearn\n* Logistic Regression with sklearn","2a13309f":"The accuracy percentages are as depicted on the table:","75f81023":"Scaling down the train set images, we have:\n* women_faces and men_faces arrays of (800,50,50) sizes ---> total train_set (1600,50,50)\n* Label array of corresponding label values, label 1 for women, label 0 for men ---> total train_set_label (1600,1) ","40a03de4":"<a id=\"**3. Processing Dataset**\"><\/a> <br>\n# **3. Processing Dataset**\n\nNow we need to modify images. The dataset contains different sizes of RGB color images. First we should resize all the images, second sonvert images to grayscale. Depending on purpose, you can go for RGB images. But grey scale has just one dimension while RGB image has 3, and helps you to avoid false classification and complexities.","c70f8fcc":"We're concatenating image arrays and labels, and flattening 'x':","77aafcc9":"<a id=\"**5. Logistic Regression with sklearn**\"><\/a> <br>\n# **5. Logistic Regression with sklearn**","ab9e482c":"Below, we're visualizing number of train and test images using barplot:","1dfaa8a7":"<a id=\"**4. Logistic Regression without sklearn**\"><\/a> <br>\n# **4. Logistic Regression without sklearn**","5e82527f":"![](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1547672259\/2_i1cdwq.png)","670ab408":"The whole picture:","68797b1a":"\"**logistic_regression**\" uses following functions:\n* **initialize_weights_and_bias :** has initial values of weights and bias which will be updated later\n* **sigmoid :** activation function that limit the output to a range between 0 and 1\n* **forward_backward propogation :** is used to calculate cost function(error) and gradient descent(to learn proper weights and bias values that minimize the error)\n* **update :** updating learning parameters 'w' and 'b' to find best values of them for better training\n* **predict :** uses x_test as input for forward propogation\n","a9c911e9":"We will have the accuracy values of Logistic Regression using sklearn. And bonus, I will compare accuracies of two different Linear Models with LR w\/o sklearn:\n* Logistic Regression\n* Perceptron","f1b37814":"<a id=\"**2. Exploring the Data**\"><\/a> <br>\n# **2. Exploring the Dataset**\n\nWe begin with importing dataset; train and test folders.","f882ee81":"Then we need to take transpose of all matrices. The purpose of it, quoted from [here](https:\/\/www.quora.com\/Why-do-we-transpose-matrices-in-machine-learning) is: \"In python, it is often the case that transposing will enable you to have the data in a given shape that might make it easier to use whatever framework or algorithm\"","a6802f93":"<a id=\"1. ****Importing Libraries****\"><\/a> <br>\n# 1. ****Importing Libraries****\n \nFirst thing we need to import some libraries.","81725292":"Same process with the test set images, we have:\n* women_faces and men_faces arrays of (170,50,50) sizes ---> total test_set (340,50,50)\n* Label array of corresponding label values, label 1 for women, label 0 for men ---> total test_set_label (340,1) ","9515dfad":"Our [dataset](https:\/\/www.kaggle.com\/gmlmrinalini\/genderdetectionface) has 1600 train images and 340 test images. Let's have a look at sample images of different sizes:","09c87059":"# **Gender Classification using Logistic Regression(Beginner)**\n\nIn this Notebook, genders will be classified using face dataset containing face images of men and women. \nI hope you find this useful. This is a beginner's first kernel, any suggestions and upvotes are appreciated :)"}}