{"cell_type":{"c0c36449":"code","4963a153":"code","1bfd8817":"code","e0aee7ed":"code","db2874cf":"code","d0dcd1f4":"code","28e04957":"code","4041d227":"code","b87232b5":"code","f94cebce":"code","f55161aa":"code","8f906882":"code","a1292434":"code","2d898c29":"code","5f4470e7":"code","2f95a953":"code","f215d5a9":"code","d5ec4c05":"code","81ee4a95":"code","f762a90b":"code","ad106feb":"code","7d1b1bdb":"code","03e7a8ad":"code","c15769a8":"code","f03b2b13":"code","ea99808d":"code","fb1b704e":"code","b60ab6da":"code","5347f346":"code","6dc46013":"code","806adc04":"code","d54b4106":"code","b41550b7":"code","b24bdec0":"code","f9f207d9":"code","fbbfd865":"code","1ab76035":"code","98347f1e":"code","527854b9":"code","0f3ea0ac":"code","9515628e":"code","9e3e9da9":"code","29e11f74":"code","ead0cf4b":"code","c678ca7e":"code","dfd5b30b":"code","fc02c166":"code","ed352858":"code","072be77a":"code","4a21456c":"code","cffac8e8":"code","35913e96":"code","d9b86ead":"code","93aa11eb":"code","7f3dd41d":"code","65d836bd":"code","a55b7a96":"code","57dedad4":"markdown","8f116ea6":"markdown","f88d51ed":"markdown","50840c23":"markdown","becbfbcc":"markdown","9eda61c0":"markdown","8ba6c4c5":"markdown","4e2b4577":"markdown","ef49127d":"markdown","16a6dd8a":"markdown","8b645428":"markdown","f66a905b":"markdown","ea429f93":"markdown","caf01bb4":"markdown","e1e13968":"markdown","b16c4bda":"markdown","baec157e":"markdown","8ea8ce4c":"markdown","5259e8ab":"markdown","dd41cf91":"markdown","f781c726":"markdown","18d6fa33":"markdown","fc965d57":"markdown","ef71bc2c":"markdown","aa35521d":"markdown","9d87e391":"markdown","09ed5268":"markdown"},"source":{"c0c36449":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4963a153":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom contextlib import contextmanager\nimport multiprocessing as mp\nfrom functools import partial\nfrom scipy.stats import kurtosis, iqr, skew\nimport lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n","1bfd8817":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","e0aee7ed":"def plot_stats(feature,label_rotation=False,horizontal_layout=True):\n    temp = df[feature].value_counts()\n    df1 = pd.DataFrame({feature: temp.index,'Number of contracts': temp.values})\n\n    # Calculate the percentage of target=1 per category value\n    cat_perc = df[[feature, 'TARGET']].groupby([feature],as_index=False).mean()\n    cat_perc.sort_values(by='TARGET', ascending=False, inplace=True)\n    \n    if(horizontal_layout):\n        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\n    else:\n        fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(12,14))\n    sns.set_color_codes(\"pastel\")\n    s = sns.barplot(ax=ax1, x = feature, y=\"Number of contracts\",data=df1)\n    if(label_rotation):\n        s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    \n    s = sns.barplot(ax=ax2, x = feature, y='TARGET', order=cat_perc[feature], data=cat_perc)\n    if(label_rotation):\n        s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.ylabel('Percent of target with value 1 [%]', fontsize=10)\n    plt.tick_params(axis='both', which='major', labelsize=10)\n\n    plt.show();","db2874cf":"def plot_distribution_comp(var,nrow=2):\n    \n    i = 0\n    t1 = df.loc[df['TARGET'] != 0]\n    t0 = df.loc[df['TARGET'] == 0]\n\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(nrow,2,figsize=(12,6*nrow))\n\n    for feature in var:\n        i += 1\n        plt.subplot(nrow,2,i)\n        sns.kdeplot(t1[feature], bw=0.5,label=\"TARGET = 1\")\n        sns.kdeplot(t0[feature], bw=0.5,label=\"TARGET = 0\")\n        plt.ylabel('Density plot', fontsize=12)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();","d0dcd1f4":"df = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_train.csv')\ntest =  pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_test.csv')\ndf.head()","28e04957":"missing_values_table(df)","4041d227":"df.dtypes.value_counts()","b87232b5":"df['TARGET'].value_counts()","f94cebce":"df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","f55161aa":"df.select_dtypes(['float64','int64']).describe()","8f906882":"df.head()","a1292434":"df.columns.tolist()","2d898c29":"plot_stats('CODE_GENDER')","5f4470e7":"plot_stats('FLAG_OWN_CAR')","2f95a953":"plot_stats('FLAG_OWN_REALTY')","f215d5a9":"plot_stats('CNT_CHILDREN')","d5ec4c05":"plot_stats('NAME_INCOME_TYPE',True)","81ee4a95":"plot_stats('NAME_FAMILY_STATUS',True)","f762a90b":"plot_stats('NAME_HOUSING_TYPE',True)","ad106feb":"plot_stats('OCCUPATION_TYPE',True)","7d1b1bdb":"plot_stats('CNT_FAM_MEMBERS',True)","03e7a8ad":"plot_stats('REG_CITY_NOT_LIVE_CITY')\nplot_stats('REG_CITY_NOT_WORK_CITY')","c15769a8":"var = ['AMT_ANNUITY','AMT_GOODS_PRICE','DAYS_EMPLOYED', 'DAYS_REGISTRATION','DAYS_BIRTH','DAYS_ID_PUBLISH']\nplot_distribution_comp(var,nrow=3)","f03b2b13":"df['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","ea99808d":"df['CNT_CHILDREN'].plot.hist(title = 'CNT_CHILDREN');\nplt.xlabel('CNT_CHILDREN');","fb1b704e":"df['AMT_ANNUITY'].plot.hist(title = 'AMT_ANNUITY');\nplt.xlabel('AMT_ANNUITY');","b60ab6da":"df['CNT_FAM_MEMBERS'].plot.hist(title = 'CNT_FAM_MEMBERS');\nplt.xlabel('CNT_FAM_MEMBERS');","5347f346":"df = df[df['DAYS_EMPLOYED'] != 365243]\ndf = df[df['CNT_CHILDREN']<4]\ndf = df[df['AMT_ANNUITY']<120000]\ndf = df[df['CNT_FAM_MEMBERS']<7.5]\n\n\n","6dc46013":"correlations = df.corr()['TARGET'].sort_values()\ncorrelations","806adc04":"df.NAME_INCOME_TYPE.value_counts()","d54b4106":"df = df.append(test)\ndf['DAYS_BIRTH'] = abs(df['DAYS_BIRTH'])\ndf['family_members_more7']= np.where(df['CNT_FAM_MEMBERS']>7,1,0)\ndf['islowskilled_labour']= np.where(df['OCCUPATION_TYPE']=='Low-skill Laborers',1,0)\ndf['is_Maternity_leave']= np.where(df['NAME_INCOME_TYPE'] =='Maternity leave' ,1,0)\ndf['is_unemployed']= np.where(df['NAME_INCOME_TYPE'] =='Unemployed' ,1,0)\n\ndf['cnt_childern_more6']= np.where(df['CNT_CHILDREN'] > 6,1,0)\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(df['DAYS_BIRTH'] \/ 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","b41550b7":"df.family_members_more7.value_counts()","b24bdec0":"def get_age_label(days_birth):\n    \"\"\" Return the age group label (int). \"\"\"\n    age_years = -days_birth \/ 365\n    if age_years < 27: return 1\n    elif age_years < 40: return 2\n    elif age_years < 50: return 3\n    elif age_years < 65: return 4\n    elif age_years < 99: return 5\n    else: return 0","f9f207d9":"df = df[df['CODE_GENDER']!='XNA']","fbbfd865":"df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\ndf['CREDIT_INCOME_PERCENT'] = df['AMT_CREDIT'] \/ df['AMT_INCOME_TOTAL']\ndf['ANNUITY_INCOME_PERCENT'] = df['AMT_ANNUITY'] \/ df['AMT_INCOME_TOTAL']\ndf['CREDIT_TERM'] = df['AMT_ANNUITY'] \/ df['AMT_CREDIT']\ndf['DAYS_EMPLOYED_PERCENT'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\ndf['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] \/ df['AMT_GOODS_PRICE']\ndf['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['DAYS_EMPLOYED']\ndf['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['DAYS_BIRTH']\ndf['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] \/ df['DAYS_BIRTH']\ndf['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] \/ df['DAYS_BIRTH']\ndf['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] \/ df['DAYS_EMPLOYED']\ndf['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] \/ df['DAYS_BIRTH']\nfor function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n    feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n    df[feature_name] = eval('np.{}'.format(function_name))(\n        df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\ndf['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\ndf['CREDIT_LENGTH'] = df['AMT_CREDIT'] \/ df['AMT_ANNUITY']","1ab76035":"external_sources = df[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\nimputer = SimpleImputer(strategy = 'median')\nexternal_sources = imputer.fit_transform(external_sources)\nexternal_sources= pd.DataFrame(external_sources, columns = ['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH'])\n\nexternal_sources['SK_ID_CURR'] = df['SK_ID_CURR'].tolist()\ndf = df.merge(external_sources, on = 'SK_ID_CURR', how = 'left')\n","98347f1e":"drop_list = [\n        'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'HOUR_APPR_PROCESS_START',\n        'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'FLAG_PHONE',\n        'FLAG_OWN_REALTY', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n        'REG_CITY_NOT_WORK_CITY', 'OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n        'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_YEAR', \n        'COMMONAREA_MODE', 'NONLIVINGAREA_MODE', 'ELEVATORS_MODE', 'NONLIVINGAREA_AVG',\n        'FLOORSMIN_MEDI', 'LANDAREA_MODE', 'NONLIVINGAREA_MEDI', 'LIVINGAPARTMENTS_MODE',\n        'FLOORSMIN_AVG', 'LANDAREA_AVG', 'FLOORSMIN_MODE', 'LANDAREA_MEDI',\n        'COMMONAREA_MEDI', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'BASEMENTAREA_AVG',\n        'BASEMENTAREA_MODE', 'NONLIVINGAPARTMENTS_MEDI', 'BASEMENTAREA_MEDI', \n        'LIVINGAPARTMENTS_AVG', 'ELEVATORS_AVG', 'YEARS_BUILD_MEDI', 'ENTRANCES_MODE',\n        'NONLIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'LIVINGAPARTMENTS_MEDI',\n        'YEARS_BUILD_MODE', 'YEARS_BEGINEXPLUATATION_AVG', 'ELEVATORS_MEDI', 'LIVINGAREA_MEDI',\n        'YEARS_BEGINEXPLUATATION_MODE', 'NONLIVINGAPARTMENTS_AVG', 'HOUSETYPE_MODE',\n        'FONDKAPREMONT_MODE', 'EMERGENCYSTATE_MODE'   ,'OWN_CAR_AGE',\n'CAR_TO_EMPLOYED_RATIO', \n'CAR_TO_BIRTH_RATIO', \n'EXT_SOURCES_PROD', \n'APARTMENTS_AVG', \n'APARTMENTS_MODE',\n'APARTMENTS_MEDI',\n'ENTRANCES_AVG', \n'ENTRANCES_MEDI', \n'LIVINGAREA_AVG', \n'FLOORSMAX_MEDI', \n'FLOORSMAX_AVG', \n'FLOORSMAX_MODE', \n'YEARS_BEGINEXPLUATATION_MEDI', \n'TOTALAREA_MODE'\n    ]\ndf.drop(drop_list,axis=1,inplace=True)","527854b9":"le = LabelEncoder()\nfor col in df:\n    if df[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(df[col].unique())) <= 2:\n            # Train on the training data\n            le.fit_transform(df[col])\ndf = pd.get_dummies(df)","0f3ea0ac":"df = df.replace([np.inf, -np.inf], np.nan)","9515628e":"train = df[df['TARGET'].notnull()]\ntest = df[df['TARGET'].isnull()]\nX,y = train.drop(['TARGET','SK_ID_CURR'],axis=1),train['TARGET'].tolist()\nprint(\"Train\/valid shape: {}, test shape: {}\".format(train.shape, test.shape))","9e3e9da9":"print(X.shape)\nprint(len(y))","29e11f74":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=314,stratify=y)","ead0cf4b":"fit_params={\"early_stopping_rounds\":30, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_test,y_test)],\n            'eval_names': ['valid'],\n            'verbose': 100,\n            'categorical_feature': 'auto'}","c678ca7e":"def learning_rate_010_decay_power_099(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\ndef learning_rate_010_decay_power_0995(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\ndef learning_rate_005_decay_power_099(current_iter):\n    base_learning_rate = 0.05\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3","dfd5b30b":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nparam_test ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}","fc02c166":"#This parameter defines the number of HP points to be tested\nn_HP_points_to_test = 100\n\n\n#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\nclf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n","ed352858":"gs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=n_HP_points_to_test,\n    scoring='roc_auc',\n    cv=3,\n    refit=True,\n    random_state=314,\n    verbose=True)\n","072be77a":"opt_parameters = {'colsample_bytree': 0.9234, 'min_child_samples': 399, 'min_child_weight': 0.1, 'num_leaves': 13, 'reg_alpha': 2, 'reg_lambda': 5, 'subsample': 0.855,'imbalanced':True}","4a21456c":"\n\nX_train.columns =  [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\nX_test.columns =  [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]\n\nclf_final = lgb.LGBMClassifier(**clf.get_params())\n#set optimal parameters\nclf_final.set_params(**opt_parameters)\n\n#Train the final model with learning rate decay\nclf_final.fit(X_train, y_train, **fit_params, callbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])","cffac8e8":"feat_imp = pd.Series(clf_final.feature_importances_, index=train.drop(['SK_ID_CURR', 'TARGET'], axis=1).columns)\nfeat_imp.nlargest(20).plot(kind='barh', figsize=(8,10))","35913e96":"# X = X.replace([np.inf, -np.inf], np.nan)\n# X.fillna(0, inplace=True)","d9b86ead":"# from imblearn.over_sampling import SMOTE\n# X_resampled, y_resampled = SMOTE().fit_sample(X, y)","93aa11eb":"# X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.20, random_state=314,stratify=y_resampled)","7f3dd41d":"# X_train.columns =  [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n# X_test.columns =  [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]\n","65d836bd":"# clf_final = lgb.LGBMClassifier(**clf.get_params())\n# #set optimal parameters\n# clf_final.set_params(**opt_parameters)\n\n# #Train the final model with learning rate decay\n# clf_final.fit(X_train, y_train, **fit_params, callbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])","a55b7a96":"probabilities = clf_final.predict_proba(test.drop(['SK_ID_CURR','TARGET'], axis=1))\nsubmission = pd.DataFrame({\n    'SK_ID_CURR': test['SK_ID_CURR'],\n    'TARGET':     [ row[1] for row in probabilities]\n})\nsubmission.to_csv(\"submission.csv\", index=False)","57dedad4":"data is imbalanced","8f116ea6":"Both people with rented apartment and who lives with their parents are more likely to struggle in returning the loan","f88d51ed":"let's explore distribution for some appealing columns","50840c23":"This feature is not actually interesting as it contibute the same towards return\/not return the loan","becbfbcc":"# Getting data","9eda61c0":"# EDA","8ba6c4c5":"Most of the categorical variables are having 2 unique variables so its ok to use label encoding with columns which has 2 unique variables and one hot encoding with the rest.","4e2b4577":"## Anomality handling","ef49127d":"We can try to merge both Civil marriage and Married under on variable later","16a6dd8a":"## Trying Upsampling the minority class","8b645428":"The number of female clients is almost double the number of male clients. Looking to the percent of defaulted credits, males have a higher chance of not returning their loans.\n\nalso there's a strange value 'XNA' , you will need to remove later","f66a905b":"## Learning rate callbacks","ea429f93":"We can try to bin the days birth to ranges maybe according to years so that it could be easier for the model we use","caf01bb4":"People who doesn't own a car are more likely to not return their loan","e1e13968":"# Imports","b16c4bda":"So more childern contibute alot in the way on not returning the loans","baec157e":"\n# Set up HyperParameter search space\n\nWe use random search, which is more flexible and more efficient than a grid search\n","8ea8ce4c":"# Modelling","5259e8ab":"More family memembers contibutes alot towards non returning of loans","dd41cf91":"# Feature engineering","f781c726":"### Let's explore some continuous features aganist their target","18d6fa33":"Those feature are somehow equally distributed for target = 0 or target =1","fc965d57":"# Encoding","ef71bc2c":"low skill laborers flag can be added ","aa35521d":"It seems that both unemployed and people who take Maternity leave are struggling in returning their loans, we can add a flag Unemployed\/Maternity leave later in feature engineering part","9d87e391":"# Correlations","09ed5268":"strong negative correlation "}}