{"cell_type":{"32c38fe0":"code","782825d8":"code","a7a05457":"code","f8adbc76":"code","39bf46fc":"code","fed34600":"code","6e78b3a7":"code","58288f3f":"code","d204ca0d":"code","e3c2d09e":"markdown"},"source":{"32c38fe0":"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom datetime import datetime\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom torch.cuda.amp import autocast, GradScaler\nimport pandas as pd\nimport gc","782825d8":"class CFG:\n    device = torch.device(\"cuda:0\")\n    amp = False\n    weights = {\n        \"swap_weight\": 0.75,\n        \"cat_weight\": 10\/24,\n        \"cont_weight\": 14\/24,\n        \"mask_weight\": 2\n    }\n    accum_grad = 1\n    clip_grad_norm = None\n    num_epochs = 2000\n    train_bs = 384\n    val_bs = 1024\n    test_bs = 1024\n    init_lr = 3e-4\n    lr_decay = 0.998\n    swap_probas = sum([[p] * r for p, r in zip([.95, .4, .7, .9, .9, .9, .9, .9, .9, .9, .25], [  2,  2,  2,  4,  4,  4,  8,  8,  7, 15,  14])], [])\n    resume = None\n    epoch_per_train = 1000\n    is_commit = True","a7a05457":"def preprocess_data(train_df, test_df):\n    cat_features = [feature for feature in train_df.columns if (\"cat\" in feature)]\n    cont_features = [feature for feature in train_df.columns if (\"cont\" in feature)]\n    X_conts = np.vstack([train_df[cont_features].values, test_df[cont_features].values])\n    X_conts = (X_conts - X_conts.mean(0))\/X_conts.std(0)\n    X_cats = np.vstack([train_df[cat_features].values, test_df[cat_features].values])\n    ohe = OneHotEncoder(sparse=False)\n    X_cats = ohe.fit_transform(X_cats)\n    X = np.hstack([X_cats, X_conts])\n    y = train_df[\"target\"].values.reshape(-1, 1)\n    return X, y, X_cats.shape[1], X_conts.shape[1]","f8adbc76":"class DTADataset(Dataset):\n    def __init__(self, x, is_sparse=False):\n        super().__init__()\n        self.x = x.astype(\"float32\")\n        self.is_sparse = is_sparse\n    \n    def __len__(self):\n        return self.x.shape[0]\n    \n    def __getitem__(self, index):\n        ele = self.x[index]\n        if (self.is_sparse):\n            ele = ele.toarray().squeeze()\n        return ele","39bf46fc":"class SwapNoiseMasker():\n    def __init__(self, probas):\n        self.probas = torch.from_numpy(np.array(probas))\n    \n    def apply(self, X):\n        should_swap = torch.bernoulli(self.probas.to(X.device) * torch.ones((X.shape)).to(X.device))\n        corrupted_X = torch.where(should_swap == 1, X[torch.randperm(X.shape[0])], X)\n        mask = (corrupted_X != X).float()\n        return corrupted_X, mask","fed34600":"class TransformerEncoder(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout, feedforward_dim):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.linear_1 = nn.Linear(embed_dim, feedforward_dim)\n        self.linear_2 = nn.Linear(feedforward_dim, embed_dim)\n        self.layernorm_1 = nn.LayerNorm(embed_dim)\n        self.layernorm_2 = nn.LayerNorm(embed_dim)\n    \n    def forward(self, x_in):\n        attn_out, _ = self.attn(x_in, x_in, x_in)\n        x = self.layernorm_1(x_in + attn_out)\n        ff_out = self.linear_2(torch.nn.functional.relu(self.linear_1(x)))\n        x = self.layernorm_2(x + ff_out)\n        return x\n    \nclass DenoiseTransformerAutoEncoder(nn.Module):\n    def __init__(self, \n            num_inputs,\n            hidden_size=1024, \n            num_subspaces=8,\n            embed_dim=128, \n            num_heads=8, \n            dropout=0, \n            feedforward_dim=512):\n        super().__init__()\n        assert hidden_size == embed_dim * num_subspaces\n        self.num_subspaces = num_subspaces\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        \n        self.excite = nn.Linear(in_features=num_inputs, out_features=hidden_size)\n        self.encoder_1 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        self.encoder_2 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        self.encoder_3 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        \n        self.mask_predictor = nn.Linear(in_features=hidden_size, out_features=num_inputs)\n        self.reconstructor = nn.Linear(in_features=hidden_size + num_inputs, out_features=num_inputs)\n    \n    def divide(self, x):\n        batch_size = x.shape[0]\n        x = x.reshape((batch_size, self.num_subspaces, self.embed_dim)).permute((1, 0, 2))\n        return x\n    \n    def combine(self, x):\n        batch_size = x.shape[1]\n        x = x.permute((1, 0, 2)).reshape(batch_size, -1)\n        return x\n    \n    def extract_feature(self, x):\n        x = torch.nn.functional.relu(self.excite(x))\n        x = self.divide(x)\n        x1 = self.encoder_1(x)\n        x2 = self.encoder_2(x1)\n        x3 = self.encoder_3(x2)\n        features = torch.cat([self.combine(x1), self.combine(x2), self.combine(x3)], dim=-1)\n        return features\n        \n    def forward(self, x):\n        x = torch.nn.functional.relu(self.excite(x))\n        x = self.divide(x)\n        x1 = self.encoder_1(x)\n        x2 = self.encoder_2(x1)\n        x3 = self.encoder_3(x2)\n        x = self.combine(x3)\n        \n        predicted_mask = self.mask_predictor(x)\n        x_reconstructed = self.reconstructor(torch.cat([x, predicted_mask], axis=-1))\n        return {\n            \"out_encoder_1\": x1,\n            \"out_encoder_2\": x2,\n            \"out_encoder_3\": x3,\n            \"predicted_mask\": predicted_mask,\n            \"x_reconstructed\": x_reconstructed\n        }","6e78b3a7":"class MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val, count=1):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += count\n        metric[\"avg\"] = metric[\"val\"] \/ metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )\n    \n    def __getitem__(self, metric_name):\n        metric = self.metrics[metric_name]\n        return metric[\"avg\"]","58288f3f":"def loss_fn(out_dict, gt_x, gt_mask, num_cats, num_conts, weights={}):\n    x_cats, x_conts = torch.split(out_dict[\"x_reconstructed\"], [num_cats, num_conts], dim=1)\n    y_cats, y_conts = torch.split(gt_x, [num_cats, num_conts], dim=1)\n    w_cats, w_conts = torch.split(gt_mask * weights[\"swap_weight\"] + (1 - gt_mask) * (1 - weights[\"swap_weight\"]), [num_cats, num_conts], dim=1)\n    \n    cat_loss = weights[\"cat_weight\"] * (w_cats * nn.BCEWithLogitsLoss(reduction=\"none\")(x_cats, y_cats))\n    cont_loss = weights[\"cont_weight\"] * (w_conts * nn.MSELoss(reduction=\"none\")(x_conts, y_conts))\n    \n    reconstruction_loss = cat_loss.mean() + cont_loss.mean()\n    mask_loss = weights[\"mask_weight\"] * nn.BCEWithLogitsLoss(reduction=\"mean\")(out_dict[\"predicted_mask\"], gt_mask)\n    return reconstruction_loss + mask_loss\n\ndef competition_metric(y_true, y_pred):\n    return np.sqrt(np.mean((y_true - y_pred)**2))\n\ndef train_fn(epoch, model, train_loader, criterion, optimizer, noisy_maker, device, scaler, num_cats, num_conts):\n    model.train()\n    metric_monitor = MetricMonitor()\n    stream = tqdm(enumerate(train_loader), total=len(train_loader))\n    for i, data in stream:\n        x = data\n        x = x.to(device)\n        x_noise, mask = noisy_maker.apply(x)\n        if (CFG.amp):\n            with autocast():\n                out_dict = model(x_noise)\n                loss = criterion(out_dict, x, mask, num_cats, num_conts, weights=CFG.weights)\n            metric_monitor.update(\"Loss\", loss.item())\n            if (CFG.accum_grad > 1):\n                loss \/= CFG.accum_grad\n            scaler.scale(loss).backward()\n            if (((i + 1)%CFG.accum_grad == 0) or ((i + 1) == len(train_loader))):\n                if (CFG.clip_grad_norm):\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.clip_grad_norm)\n                scaler.step(optimizer)\n                optimizer.zero_grad()\n                scaler.update()\n        else:\n            out_dict = model(x_noise)\n            loss = criterion(out_dict, x, mask, num_cats, num_conts, weights=CFG.weights)\n            metric_monitor.update(\"Loss\", loss.item())\n            if (CFG.accum_grad > 1):\n                loss \/= CFG.accum_grad\n            loss.backward()\n            if (((i + 1)%CFG.accum_grad == 0) or ((i + 1) == len(train_loader))):\n                if (CFG.clip_grad_norm):\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.clip_grad_norm)\n                optimizer.step()\n                optimizer.zero_grad()\n        stream.set_description(\n             \"Epoch: {epoch}. Train. {metric_monitor} | lr = {lr}\".format(epoch=epoch, metric_monitor=metric_monitor, lr=optimizer.param_groups[0]['lr'])\n         )\n    return metric_monitor[\"Loss\"]\n\ndef eval_fn(epoch, model, val_loader, metric, device, train_df, num_cats, num_conts):\n    print(f\"Epoch {epoch} Validation: \")\n    target = train_df[\"target\"].values\n    all_features = extract_features(model, val_loader, device, num_cats, num_conts)\n    ridge_oof_preds = np.zeros((train_df.shape[0],))\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n        X_train, y_train, X_val, y_val = all_features[train_idx], target[train_idx], all_features[val_idx], target[val_idx]\n        ridge_model = Ridge(alpha=0.01, random_state=42)\n        ridge_model.fit(\n            X_train.numpy().cpu(),\n            y_train,\n        )\n        ridge_oof_pred = ridge_model.predict(X_val.numpy().cpu())\n        print(f\"Ridge's Fold {fold} competition metric: \", metric(y_val, ridge_oof_pred))\n        ridge_oof_preds[val_idx] = ridge_oof_pred\n        del ridge_model, X_train, y_train, X_val, y_val\n        gc.collect()\n    torch.save(torch.from_numpy(all_features), \".\/all_features.pt\")\n    print(\"Hix\")\n    del all_features\n    gc.collect()\n    score = metric(target, ridge_oof_preds)\n    print(f\"RMSE Epoch {epoch}: \", score)\n    return score\n        \ndef extract_features(model, test_loader, device, num_cats, num_conts):\n    model.eval()\n    all_features = []\n    stream = tqdm(enumerate(test_loader), total=len(test_loader))\n    for i, data in stream:\n        x = data\n        x = x.to(device)\n        with torch.no_grad():\n            features = model.extract_feature(x)\n            all_features.append(features.detach())\n        stream.set_description(\n             \"Extracting Features. \"\n         )\n    all_features = torch.cat(all_features, dim=0)\n    return all_features","d204ca0d":"if not CFG.is_commit:\n    train_df = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\n    test_df = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\n    submission_df = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\n    X, y, num_cats, num_conts = preprocess_data(train_df, test_df)\n    train_dataset = DTADataset(X)\n    train_loader = DataLoader(train_dataset, batch_size=CFG.train_bs, num_workers=4, shuffle=True, pin_memory=True, drop_last=True)\n    model = DenoiseTransformerAutoEncoder(\n                num_inputs=X.shape[1],  \n                hidden_size=1024, \n                num_subspaces=8,\n                embed_dim=128, \n                num_heads=8, \n                dropout=0,\n                feedforward_dim=512).to(CFG.device)\n    noisy_maker = SwapNoiseMasker(CFG.swap_probas)\n    criterion = loss_fn\n    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.init_lr)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=CFG.lr_decay)\n    scaler = GradScaler() if (CFG.amp) else None\n    best_score = float(\"+inf\")\n    start_epoch = -1\n    if (CFG.resume is not None):\n        ckpt = torch.load(CFG.resume)\n        model.load_state_dict(ckpt[\"model\"])\n        optimizer.load_state_dict(ckpt[\"optimizer\"])\n        if ((scheduler is not None) and (ckpt[\"scheduler\"] is not None)):\n            scheduler.load_state_dict(ckpt[\"scheduler\"])\n        if ((scaler is not None) and (ckpt[\"scaler\"] is not None)):\n            scaler.load_state_dict(ckpt[\"scaler\"])\n        best_score = ckpt[\"best_score\"]\n        start_epoch = ckpt[\"epoch\"]\n    start_epoch += 1\n    for epoch in range(start_epoch, start_epoch + CFG.epoch_per_train):\n        train_loss = train_fn(epoch, model, train_loader, criterion, optimizer, noisy_maker, CFG.device, scaler, num_cats, num_conts)\n        if (scheduler is not None):\n            scheduler.step()\n        torch.save(model.state_dict(), f\".\/dae_model_{epoch}.pt\")\n    # -----------------------------------------Evaluation (comment to avoid out of ram)---------------------------------------------\n    #     score = eval_fn(epoch, model, val_loader, competition_metric, CFG.device, train_df, num_cats, num_conts)\n    #     if (score < best_score):\n    #         print(f\"Epoch {epoch} improve from {best_score} to {score} saving model checkpoints\")\n    #         best_score = score\n    #         torch.save({\n    #                 \"optimizer\": optimizer.state_dict(),\n    #                 \"scheduler\": scheduler.state_dict() if (scheduler is not None) else None,\n    #                 \"model\": model.state_dict(),\n    #                 \"best_score\": best_score,\n    #                 \"epoch\": epoch,\n    #                 \"scaler\": scaler.state_dict() if (scaler is not None) else None,\n    #             }, f\".\/best_dae_model.pt\"\n    #         )\n    torch.save({\n            \"optimizer\": optimizer.state_dict(),\n            \"scheduler\": scheduler.state_dict() if (scheduler is not None) else None,\n            \"model\": model.state_dict(),\n            \"best_score\": best_score,\n            \"scaler\": scaler.state_dict() if (scaler is not None) else None,\n            \"epoch\": epoch\n        }, f\".\/resume_dae_model_{start_epoch + CFG.epoch_per_train}.pt\"\n    )\n    # ----------------------------------------Feature Extraction---------------------------------------------------\n    # model.load_state_dict(torch.load(f\".\/resume_dae_model_2000.pt\")[\"model\"])\n    # test_loader = DataLoader(DTADataset(X), batch_size=CFG.test_bs, num_workers=4, shuffle=False, pin_memory=True)\n    # all_features = extract_features(model, test_loader, CFG.device, num_cats, num_conts)\n    # torch.save(all_features, \"dae_features.pt\")","e3c2d09e":"### **Code train Denoise Transformer AutoEncoder**\n- Reference: https:\/\/github.com\/ryancheunggit\/Denoise-Transformer-AutoEncoder\n- Recommend train 2000 epochs to get best features (train in 18h GPU kaggle)."}}