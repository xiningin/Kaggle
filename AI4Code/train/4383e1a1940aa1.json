{"cell_type":{"b8dc6410":"code","b5282c7d":"code","babfb3f3":"code","ab4088c1":"code","eea223c8":"code","ce4e9287":"code","edd960cc":"code","d9a6a81c":"code","b5e33bd2":"code","c08c036e":"code","d8d75aa2":"code","3103c6bd":"code","4a5631a8":"code","181226ff":"code","cfb2fc53":"code","a59f3319":"code","fe2c9673":"code","ae1e7e9e":"code","0bc899d0":"code","02ac251d":"code","49c0ab68":"code","2b94221e":"code","d5a061c0":"code","327dfd70":"code","a3c46389":"code","8c14d43e":"code","c9ca1d47":"markdown","351c45f7":"markdown","123c51d9":"markdown","66069036":"markdown","b7bda4a4":"markdown","4ecd3b7c":"markdown","e471ea7f":"markdown","16115d09":"markdown","446ab04e":"markdown","28d760f1":"markdown","c333c364":"markdown","b4602056":"markdown","4f41f8a6":"markdown","5a5a2e48":"markdown","cbb5e5ac":"markdown","0eb1b3ee":"markdown","d53cac2a":"markdown","e1f31f01":"markdown"},"source":{"b8dc6410":"!git clone --depth 1 https:\/\/github.com\/location-competition\/indoor-location-competition-20 indoor_location_competition_20\n!rm -rf indoor_location_competition_20\/data","b5282c7d":"import multiprocessing\nimport numpy as np\nimport pandas as pd\nimport scipy.interpolate\nimport scipy.sparse\nfrom tqdm import tqdm\n\nfrom indoor_location_competition_20.io_f import read_data_file\nimport indoor_location_competition_20.compute_f as compute_f","babfb3f3":"INPUT_PATH = '..\/input\/indoor-location-navigation'","ab4088c1":"def compute_rel_positions(acce_datas, ahrs_datas):\n    step_timestamps, step_indexs, step_acce_max_mins = compute_f.compute_steps(acce_datas)\n    headings = compute_f.compute_headings(ahrs_datas)\n    stride_lengths = compute_f.compute_stride_length(step_acce_max_mins)\n    step_headings = compute_f.compute_step_heading(step_timestamps, headings)\n    rel_positions = compute_f.compute_rel_positions(stride_lengths, step_headings)\n    return rel_positions","eea223c8":"import math\n\norder = 6\n# fs = 50.0  # sample rate, Hz\n# cutoff = 3\n\nfs = 100\ncutoff = 3.667  # desired cutoff frequency of the filter, Hz\n\nstep_distance = 0.75\nw_height = 1.7\nm_trans = -2","ce4e9287":"from scipy.signal import butter, lfilter\n\ndef butter_lowpass(cutoff, fs, order=5):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff \/ nyq\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    return b, a\n\ndef butter_lowpass_filter(data, cutoff, fs, order=5):\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    y = lfilter(b, a, data)\n    return y","edd960cc":"def peak_accel_threshold(data, timestamps, threshold):\n    d_acc = []\n    last_state = 'below'\n    crest_troughs = 0\n    crossings = []\n\n    for i, datum in enumerate(data):\n        \n        current_state = last_state\n        if datum < threshold:\n            current_state = 'below'\n        elif datum > threshold:\n            current_state = 'above'\n\n        if current_state is not last_state:\n            if current_state is 'above':\n                crossing = [timestamps[i], threshold]\n                crossings.append(crossing)\n            else:\n                crossing = [timestamps[i], threshold]\n                crossings.append(crossing)\n\n            crest_troughs += 1\n        last_state = current_state\n    return np.array(crossings)","d9a6a81c":"def steps_compute_rel_positions(sample_file):\n    \n    mix_acce = np.sqrt(sample_file.acce[:,1:2]**2 + sample_file.acce[:,2:3]**2 + sample_file.acce[:,3:4]**2)\n    mix_acce = np.concatenate([sample_file.acce[:,0:1], mix_acce], 1)\n    mix_df = pd.DataFrame(mix_acce)\n    mix_df.columns = [\"timestamp\",\"acce\"]\n    \n    filtered = butter_lowpass_filter(mix_df[\"acce\"], cutoff, fs, order)\n\n    threshold = filtered.mean() * 1.1\n    crossings = peak_accel_threshold(filtered, mix_df[\"timestamp\"], threshold)\n\n    step_sum = len(crossings)\/2\n    distance = w_height * 0.4 * step_sum\n\n    mag_df = pd.DataFrame(sample_file.magn)\n    mag_df.columns = [\"timestamp\",\"x\",\"y\",\"z\"]\n    \n    acce_df = pd.DataFrame(sample_file.acce)\n    acce_df.columns = [\"timestamp\",\"ax\",\"ay\",\"az\"]\n    \n    mag_df = pd.merge(mag_df,acce_df,on=\"timestamp\")\n    mag_df.dropna()\n    \n    time_di_list = []\n\n    for i in mag_df.iterrows():\n\n        gx,gy,gz = i[1][1],i[1][2],i[1][3]\n        ax,ay,az = i[1][4],i[1][5],i[1][6]\n\n        roll = math.atan2(ay,az)\n        pitch = math.atan2(-1*ax , (ay * math.sin(roll) + az * math.cos(roll)))\n\n        q = m_trans - math.degrees(math.atan2(\n            (gz*math.sin(roll)-gy*math.cos(roll)),(gx*math.cos(pitch) + gy*math.sin(roll)*math.sin(pitch) + gz*math.sin(pitch)*math.cos(roll))\n        )) -90\n        if q <= 0:\n            q += 360\n        time_di_list.append((i[1][0],q))\n\n    d_list = [x[1] for x in time_di_list]\n    \n    steps = []\n    step_time = []\n    di_dict = dict(time_di_list)\n\n    for n,i in enumerate(crossings[:,:1]):\n        if n % 2 == 1:\n            continue\n        direct_now = di_dict[i[0]]\n        dx = math.sin(math.radians(direct_now))\n        dy = math.cos(math.radians(direct_now))\n#         print(int(n\/2+1),\"\u6b69\u76ee\/x:\",dx,\"\/y:\",dy,\"\/\u89d2\u5ea6\uff1a\",direct_now)\n        steps.append((i[0],dx,dy))\n        step_time.append(i[0])\n    \n        step_dtime = np.diff(step_time)\/1000\n        step_dtime = step_dtime.tolist()\n        step_dtime.insert(0,5)\n        \n        rel_position = []\n\n        wp_idx = 0\n#         print(\"WP:\",round(sample_file.waypoint[0,1],3),round(sample_file.waypoint[0,2],3),sample_file.waypoint[0,0])\n#         print(\"------------------\")\n        for p,i in enumerate(steps):\n            step_distance = 0\n            if step_dtime[p] >= 1:\n                step_distance = w_height*0.25\n            elif step_dtime[p] >= 0.75:\n                step_distance = w_height*0.3\n            elif step_dtime[p] >= 0.5:\n                step_distance = w_height*0.4\n            elif step_dtime[p] >= 0.35:\n                step_distance = w_height*0.45\n            elif step_dtime[p] >= 0.2:\n                step_distance = w_height*0.5\n            else:\n                step_distance = w_height*0.4\n\n#             step_x += i[1]*step_distance\n#             step_y += i[2]*step_distance\n            \n            rel_position.append([i[0], i[1]*step_distance, i[2]*step_distance])\n#     print(rel_position)\n    \n    return np.array(rel_position)","b5e33bd2":"def correct_path(args):\n    path, path_df = args\n    \n    T_ref  = path_df['timestamp'].values\n    xy_hat = path_df[['x', 'y']].values\n    \n    example = read_data_file(f'{INPUT_PATH}\/test\/{path}.txt')\n\n    rel_positions1 = compute_rel_positions(example.acce, example.ahrs)\n    rel_positions2 = steps_compute_rel_positions(example)\n    rel1 = rel_positions1.copy()\n    rel2 = rel_positions2.copy()\n    rel1[:,1:] = rel_positions1[:,1:] \/ 2\n    rel2[:,1:] = rel_positions2[:,1:] \/ 2\n    rel_positions = np.vstack([rel1,rel2])\n    rel_positions = rel_positions[np.argsort(rel_positions[:, 0])]\n    \n    if T_ref[-1] > rel_positions[-1, 0]:\n        rel_positions = [np.array([[0, 0, 0]]), rel_positions, np.array([[T_ref[-1], 0, 0]])]\n    else:\n        rel_positions = [np.array([[0, 0, 0]]), rel_positions]\n    rel_positions = np.concatenate(rel_positions)\n    \n    T_rel = rel_positions[:, 0]\n    delta_xy_hat = np.diff(scipy.interpolate.interp1d(T_rel, np.cumsum(rel_positions[:, 1:3], axis=0), axis=0)(T_ref), axis=0)\n\n    N = xy_hat.shape[0]\n    delta_t = np.diff(T_ref)\n    alpha = (8.1)**(-2) * np.ones(N)\n    beta  = (0.3 + 0.3 * 1e-3 * delta_t)**(-2)\n    A = scipy.sparse.spdiags(alpha, [0], N, N)\n    B = scipy.sparse.spdiags( beta, [0], N-1, N-1)\n    D = scipy.sparse.spdiags(np.stack([-np.ones(N), np.ones(N)]), [0, 1], N-1, N)\n\n    Q = A + (D.T @ B @ D)\n    c = (A @ xy_hat) + (D.T @ (B @ delta_xy_hat))\n    xy_star = scipy.sparse.linalg.spsolve(Q, c)\n\n    return pd.DataFrame({\n        'site_path_timestamp' : path_df['site_path_timestamp'],\n        'floor' : path_df['floor'],\n        'x' : xy_star[:, 0],\n        'y' : xy_star[:, 1],\n    })","c08c036e":"sub = pd.read_csv('..\/input\/indoor-loc-and-nav-subs\/submission_4475.csv')  # ('..\/input\/simple-99-accurate-floor-model\/submission.csv')\ntmp = sub['site_path_timestamp'].apply(lambda s : pd.Series(s.split('_')))\nsub['site'] = tmp[0]\nsub['path'] = tmp[1]\nsub['timestamp'] = tmp[2].astype(float)\n\nprocesses = multiprocessing.cpu_count()\nwith multiprocessing.Pool(processes=processes) as pool:\n    dfs = pool.imap_unordered(correct_path, sub.groupby('path'))\n    dfs = tqdm(dfs)\n    dfs = list(dfs)\nsub = pd.concat(dfs).sort_values('site_path_timestamp')\nsub.to_csv('first-submission.csv', index=False)","d8d75aa2":"import json\nimport re\nimport gc\nimport pickle\nimport itertools\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\nfrom datetime import datetime as dt\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport datetime\nts_conv = np.vectorize(datetime.datetime.fromtimestamp) # ut(10 digit) -> date\n\n# pandas settings -----------------------------------------\npd.set_option(\"display.max_colwidth\", 100)\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\npd.options.display.float_format = '{:,.5f}'.format\n\n# Graph drawing -------------------------------------------\nimport matplotlib\nfrom matplotlib import font_manager\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom matplotlib import rc\nfrom matplotlib_venn import venn2, venn2_circles\nfrom matplotlib import animation as ani\nfrom IPython.display import Image\nfrom pylab import imread\n\nplt.rcParams[\"patch.force_edgecolor\"] = True\nfrom IPython.display import display # Allows the use of display() for DataFrames\nimport seaborn as sns\nsns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\nsns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\nred = sns.xkcd_rgb[\"light red\"]\ngreen = sns.xkcd_rgb[\"medium green\"]\nblue = sns.xkcd_rgb[\"denim blue\"]\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n# ML -------------------------------------------\nfrom sklearn.preprocessing import LabelEncoder\n\n\nimport dill\nfrom collections import defaultdict, OrderedDict\nfrom scipy.spatial import distance","3103c6bd":"def unpickle(filename):\n    with open(filename, 'rb') as fo:\n        p = pickle.load(fo)\n    return p\n\ndef to_pickle(filename, obj):\n    with open(filename, 'wb') as f:\n        pickle.dump(obj, f, -1)\n\n\n\nclass FeatureStore():\n    \n    # necessayr to re-check\n    floor_convert = {'1F' :  0, '2F' : 1, '3F' : 2, '4F' : 3, '5F' : 4, \n                     '6F' : 5, '7F' : 6, '8F' : 7, '9F' : 8,\n                     'B'  : -1, 'B1' : -1, 'B2' : -2, 'B3' : -3, \n                     'BF' : -1, 'BM' : -1, \n                     'F1' : 0, 'F2' : 1, 'F3' : 2, 'F4' : 3, 'F5' : 4, \n                     'F6' : 5, 'F7' : 6, 'F8' : 7, 'F9' : 8, 'F10': 9,\n                     'L1' : 0, 'L2' : 1, 'L3' : 2, 'L4' : 3, 'L5' : 4, \n                     'L6' : 5, 'L7' : 6, 'L8' : 7, 'L9' : 8, 'L10': 9, \n                     'L11': 10,\n                     'G'  : 0, 'LG1': 0, 'LG2': 1, 'LM' : 0, 'M'  : 0, \n                     'P1' : 0, 'P2' : 1,}\n    \n    df_types = ['accelerometer',\n                'accelerometer_uncalibrated',\n                'beacon',\n                'gyroscope',\n                'gyroscope_uncalibrated',\n                'magnetic_field',\n                'magnetic_field_uncalibrated',\n                'rotation_vector',\n                'waypoint',\n                'wifi']\n    \n    # https:\/\/github.com\/location-competition\/indoor-location-competition-20\n    df_type_cols = {'accelerometer': [\"timestamp\", \"x\", \"y\", \"z\", \"accuracy\"],\n                'accelerometer_uncalibrated': [\"timestamp\", \"x\", \"y\", \"z\", \n                                               \"x2\", \"y2\", \"z2\", \"accuracy\" ],\n                'beacon': [\"timestamp\", \"uuid\", \"major_id\", \"minor_id\", \"tx_power\", \n                           \"rssi\", \"distance\", \"mac_addr\", \"timestamp2\"],\n                'gyroscope': [\"timestamp\", \"x\", \"y\", \"z\", \"accuracy\"],\n                'gyroscope_uncalibrated': [\"timestamp\", \"x\", \"y\", \"z\", \n                                           \"x2\", \"y2\", \"z2\", \"accuracy\" ],\n                'magnetic_field': [\"timestamp\", \"x\", \"y\", \"z\", \"accuracy\"],\n                'magnetic_field_uncalibrated': [\"timestamp\", \"x\", \"y\", \"z\", \n                                                \"x2\", \"y2\", \"z2\", \"accuracy\" ],\n                'rotation_vector': [\"timestamp\", \"x\", \"y\", \"z\", \"accuracy\"],\n                'waypoint': [\"timestamp\", \"x\", \"y\"],\n                'wifi': [\"timestamp\", \"ssid\", \"bssid\",\"rssi\",\"frequency\",\n                         \"last_seen_timestamp\",]}\n\n    dtype_dict = {}\n    dtype_dict[\"accelerometer\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \"z\":float, \n                                   \"accuracy\":int}\n    dtype_dict[\"accelerometer_uncalibrated\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \n                                                \"z\":float, \"x2\":float, \"y2\":float, \n                                                \"z2\":float, \"accuracy\":int}\n    dtype_dict[\"beacon\"] = {\"timestamp\":int, \"uuid\":str, \"major_id\":str, \n                            \"minor_id\":str, \"tx_power\":int,  \"rssi\":int, \n                            \"distance\":float, \"mac_addr\":str, \"timestamp2\":int}\n    dtype_dict[\"gyroscope\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \"z\":float, \n                               \"accuracy\":int}\n    dtype_dict[\"gyroscope_uncalibrated\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \n                                            \"z\":float, \"x2\":float, \"y2\":float, \n                                            \"z2\":float, \"accuracy\":int}\n    dtype_dict[\"magnetic_field\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \n                                    \"z\":float, \"accuracy\":int}\n    dtype_dict[\"magnetic_field_uncalibrated\"] = {\"timestamp\":int, \"x\":float, \n                                                 \"y\":float, \"z\":float, \"x2\":float, \n                                                 \"y2\":float, \"z2\":float, \"accuracy\":int}\n    dtype_dict[\"rotation_vector\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \n                                     \"z\":float, \"accuracy\":int}\n    dtype_dict[\"waypoint\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \"z\":float}\n    dtype_dict[\"wifi\"] = {\"timestamp\":int, \"ssid\":str, \"bssid\":str,\n                          \"rssi\":int,\"frequency\":int, \"last_seen_timestamp\":int}\n\n    def __init__(self, site_id, floor, path_id, \n                 input_path=\"..\/input\/indoor-location-navigation\/\",\n                 save_path=\"..\/mid\"):\n        self.site_id = site_id.strip()\n        self.floor = floor.strip()\n        self.n_floor = self.floor_convert[self.floor]\n        self.path_id = path_id.strip()\n        \n        self.input_path = input_path\n        assert Path(input_path).exists(), f\"input_path do not exist: {input_path}\"\n        \n        self.save_path = save_path\n        Path(save_path).mkdir(parents=True, exist_ok=True)\n        \n        self.site_info = SiteInfo(site_id=self.site_id, floor=self.floor, input_path=self.input_path)\n        \n    def _flatten(self, l):\n        return list(itertools.chain.from_iterable(l))\n    \n    def multi_line_spliter(self, s):\n        matches = re.finditer(\"TYPE_\", s)\n        matches_positions = [match.start() for match in matches]\n        split_idx = [0] + [matches_positions[i]-14 for i in range(1, len(matches_positions))] + [len(s)]\n        return [s[split_idx[i]:split_idx[i+1]] for i in range(len(split_idx)-1)]\n    \n    def load_df(self, ):\n        path = str(Path(self.input_path)\/f\"train\/{self.site_id}\/{self.floor}\/{self.path_id}.txt\")\n        with open(path) as f:\n            data = f.readlines()\n        \n        modified_data = []\n        for s in data:\n            if s.count(\"TYPE_\")>1:\n                lines = self.multi_line_spliter(s)\n                modified_data.extend(lines)\n            else:\n                modified_data.append(s)\n        del data\n        self.meta_info_len = len([d for d in modified_data if d[0]==\"#\"])\n        self.meta_info_df = pd.DataFrame([m.replace(\"\\n\", \"\").split(\":\") \n                                          for m in self._flatten([d.split(\"\\t\") \n                                                                  for d in modified_data if d[0]==\"#\"]) if m!=\"#\"])\n\n        data_df = pd.DataFrame([d.replace(\"\\n\", \"\").split(\"\\t\") for d in modified_data if d[0]!=\"#\"])\n        for dt in self.df_types:\n            # select data type\n            df_s = data_df[data_df[1]==f\"TYPE_{dt.upper()}\"]\n            if len(df_s)==0:\n                setattr(self, dt, pd.DataFrame(columns=self.df_type_cols[dt]))\n            else:\n                # remove empty cols\n                na_info = df_s.isna().sum(axis=0) == len(df_s)\n                df_s = df_s[[i for i in na_info[na_info==False].index if i!=1]].reset_index(drop=True)\n                \n                if len(df_s.columns)!=len(self.df_type_cols[dt]):\n                    df_s.columns = self.df_type_cols[dt][:len(df_s.columns)]\n                else:\n                    df_s.columns = self.df_type_cols[dt]\n            \n                # set dtype          \n                for c in df_s.columns:\n                    df_s[c] = df_s[c].astype(self.dtype_dict[dt][c])\n                                     \n                # set DataFrame to attr\n                setattr(self, dt, df_s)\n    \n    def get_site_info(self, keep_raw=False):\n        self.site_info.get_site_info(keep_raw=keep_raw)\n            \n    def load_all_data(self, keep_raw=False):     \n        self.load_df()\n        self.get_site_info(keep_raw=keep_raw)\n        \n    def __getitem__(self, item):\n        if item in self.df_types:\n            return getattr(self, item)\n        else:\n            return None\n    \n    def save(self, ):\n        # to be implemented\n        pass\n    \n    \nclass SiteInfo():\n    def __init__(self, site_id, floor, input_path=\"..\/input\/indoor-location-navigation\/\"):\n        self.site_id = site_id\n        self.floor = floor\n        self.input_path = input_path\n        assert Path(input_path).exists(), f\"input_path do not exist: {input_path}\"\n        \n    def get_site_info(self, keep_raw=False):\n        floor_info_path = f\"{self.input_path}\/metadata\/{self.site_id}\/{self.floor}\/floor_info.json\"\n        with open(floor_info_path, \"r\") as f:\n            self.floor_info = json.loads(f.read())\n            self.site_height = self.floor_info[\"map_info\"][\"height\"]\n            self.site_width = self.floor_info[\"map_info\"][\"width\"]\n            if not keep_raw:\n                del self.floor_info\n            \n        geojson_map_path = f\"{self.input_path}\/metadata\/{self.site_id}\/{self.floor}\/geojson_map.json\"\n        with open(geojson_map_path, \"r\") as f:\n            self.geojson_map = json.loads(f.read())\n            self.map_type = self.geojson_map[\"type\"]\n            self.features = self.geojson_map[\"features\"]\n            \n            self.floor_coordinates = self.features[0][\"geometry\"][\"coordinates\"]\n            self.store_coordinates = [self.features[i][\"geometry\"][\"coordinates\"] \n                                          for i in range(1, len(self.features))]\n                \n            if not keep_raw:\n                del self.geojson_map\n    \n    def show_site_image(self):\n        path = f\"{self.input_path}\/metadata\/{self.site_id}\/{self.floor}\/floor_image.png\"\n        plt.imshow(imread(path), extent=[0, self.site_width, 0, self.site_height])\n\n    def draw_polygon(self, size=8, only_floor=False):\n\n        fig = plt.figure()\n        ax = plt.subplot(111)\n            \n        xmax, xmin, ymax, ymin = self._draw(self.floor_coordinates, ax, calc_minmax=True)\n        if not only_floor:\n            self._draw(self.store_coordinates, ax, fill=True)\n        plt.legend([])\n        \n        xrange = xmax - xmin\n        yrange = ymax - ymin\n        ratio = yrange \/ xrange\n        \n        self.x_size = size\n        self.y_size = size*ratio\n\n        fig.set_figwidth(size)\n        fig.set_figheight(size*ratio)\n        # plt.show()\n        return ax\n        \n    def _draw(self, coordinates, ax, fill=False, calc_minmax=False):\n        xmax, ymax = -np.inf, -np.inf\n        xmin, ymin = np.inf, np.inf\n        for i in range(len(coordinates)):\n            ndim = np.ndim(coordinates[i])\n            if ndim==2:\n                corrd_df = pd.DataFrame(coordinates[i])\n                if fill:\n                    ax.fill(corrd_df[0], corrd_df[1], alpha=0.7)\n                else:\n                    corrd_df.plot.line(x=0, y=1, style=\"-\", ax=ax)\n                        \n                if calc_minmax:\n                    xmax = max(xmax, corrd_df[0].max())\n                    xmin = min(xmin, corrd_df[0].min())\n\n                    ymax = max(ymax, corrd_df[1].max())\n                    ymin = min(ymin, corrd_df[1].min())\n            elif ndim==3:\n                for j in range(len(coordinates[i])):\n                    corrd_df = pd.DataFrame(coordinates[i][j])\n                    if fill:\n                        ax.fill(corrd_df[0], corrd_df[1], alpha=0.6)\n                    else:\n                        corrd_df.plot.line(x=0, y=1, style=\"-\", ax=ax)\n                        \n                    if calc_minmax:\n                        xmax = max(xmax, corrd_df[0].max())\n                        xmin = min(xmin, corrd_df[0].min())\n\n                        ymax = max(ymax, corrd_df[1].max())\n                        ymin = min(ymin, corrd_df[1].min())\n            else:\n                assert False, f\"ndim of coordinates should be 2 or 3: {ndim}\"\n        if calc_minmax:\n            return xmax, xmin, ymax, ymin\n        else:\n            return None","4a5631a8":"# train_meta_data\ntrain_meta = glob(\"..\/input\/indoor-location-navigation\/train\/*\/*\/*\")\ntrain_meta_org = pd.DataFrame(train_meta)\ntrain_meta = train_meta_org[0].str.split(\"\/\", expand=True)[[4, 5, 6]]\ntrain_meta.columns = [\"site_id\", \"floor\", \"path_id\"]\ntrain_meta[\"path_id\"] = train_meta[\"path_id\"].str.replace(\".txt\", \"\")\ntrain_meta[\"path\"] = train_meta_org[0]\n#train_meta.head()","181226ff":"def pickle_dump_dill(obj, path):\n    with open(path, mode='wb') as f:\n        dill.dump(obj, f)\n\n\ndef pickle_load_dill(path):\n    with open(path, mode='rb') as f:\n        data = dill.load(f)\n        return data","cfb2fc53":"sample_sub = pd.read_csv('..\/input\/indoor-location-navigation\/sample_submission.csv')\ntest_sites = sample_sub.site_path_timestamp.apply(lambda x: pd.Series(x.split(\"_\")))[0].unique().tolist()\n\ntest_meta = sample_sub[\"site_path_timestamp\"].apply(\n    lambda x: pd.Series(x.split(\"_\")))\ntest_meta.columns = [\"site_id\", \"path_id\", \"timestamp\"]\ntest_meta=test_meta.drop('timestamp', axis=1)\ntest_meta = test_meta.drop_duplicates(subset=[\"site_id\", \"path_id\"]).reset_index(drop=True)","a59f3319":"create_train_meta_sub=False\nif create_train_meta_sub:\n    train_meta_sub=train_meta[train_meta['site_id'].isin(test_sites)].reset_index(drop=True)\n    train_meta_sub['start_time']=0\n    train_meta_sub['end_time']=0\n    train_meta_sub['start_wp_time']=0\n    train_meta_sub['start_wp_x']=0\n    train_meta_sub['start_wp_y']=0\n    train_meta_sub['end_wp_time']=0\n    train_meta_sub['end_wp_x']=0\n    train_meta_sub['end_wp_y']=0\n    train_meta_sub['n_floor']=0\n    for i in tqdm(range(len(train_meta_sub))):\n        t = train_meta_sub.iloc[i]\n        n_floor = FeatureStore.floor_convert[t.floor]\n        feature = FeatureStore(\n            site_id=t.site_id, floor=t.floor, path_id=t.path_id)\n        feature.load_all_data() \n        start_time=int(feature.meta_info_df[feature.meta_info_df[0]=='startTime'][1])\n        end_time=int(feature.meta_info_df[feature.meta_info_df[0]=='endTime'][1])\n        train_meta_sub.loc[i,'start_time']=start_time\n        train_meta_sub.loc[i,'start_wp_time']=feature.waypoint.iloc[0]['timestamp']\n        train_meta_sub.loc[i,'start_wp_x']=feature.waypoint.iloc[0]['x']\n        train_meta_sub.loc[i,'start_wp_y']=feature.waypoint.iloc[0]['y']\n        train_meta_sub.loc[i,'end_time']=end_time\n        train_meta_sub.loc[i,'end_wp_time']=feature.waypoint.iloc[-1]['timestamp']\n        train_meta_sub.loc[i,'end_wp_x']=feature.waypoint.iloc[-1]['x']\n        train_meta_sub.loc[i,'end_wp_y']=feature.waypoint.iloc[-1]['y']\n        train_meta_sub.loc[i,'n_floor']=feature.n_floor\n    train_meta_sub.to_csv('train_meta_sub.csv', index=False)\nelse:\n    train_meta_sub = pd.read_csv('..\/input\/indoor-public\/train_meta_sub.csv')","fe2c9673":"train_meta_sub[train_meta_sub.site_id=='5d2709b303f801723c327472'][['path_id','site_id','n_floor','start_time','start_wp_x','start_wp_y','end_time','end_wp_x','end_wp_y']].sort_values(['site_id','n_floor','start_time'])[:50]","ae1e7e9e":"import seaborn as sns\nfor test_site in test_sites:\n    plt.figure()\n    plt.title(test_site)\n    sns.boxplot(x='floor', y='start_time', data=train_meta_sub[train_meta_sub.site_id==test_site])","0bc899d0":"def read_txt(file):\n    with open(file) as f:\n        txt = f.readlines()\n\n    modified_data = []\n    for s in txt:\n        if s.count(\"TYPE_\") > 1:\n            lines = multi_line_spliter(s)\n            modified_data.extend(lines)\n        else:\n            modified_data.append(s)\n    return modified_data\n\n\ndef _flatten(l):\n    return list(itertools.chain.from_iterable(l))\n\n\ndef get_feature_test(site_id, path_id, input_path, sample_sub):\n    file = f\"{input_path}\/test\/{path_id}.txt\"\n    content = read_txt(file)\n    data_df = pd.DataFrame([d.replace(\"\\n\", \"\").split(\"\\t\")\n                            for d in content if d[0] != \"#\"])\n    data_dict = OrderedDict()\n    for dt in FeatureStore.df_types:\n        # select data type\n        df_s = data_df[data_df[1] == f\"TYPE_{dt.upper()}\"]\n        if len(df_s) == 0:\n            setattr(data_dict, dt, pd.DataFrame(\n                columns=FeatureStore.df_type_cols[dt]))\n        else:\n            # remove empty cols\n            na_info = df_s.isna().sum(axis=0) == len(df_s)\n            df_s = df_s[[i for i in na_info[na_info ==\n                                            False].index if i != 1]].reset_index(drop=True)\n\n            if len(df_s.columns) != len(FeatureStore.df_type_cols[dt]):\n                df_s.columns = FeatureStore.df_type_cols[dt][:len(\n                    df_s.columns)]\n            else:\n                df_s.columns = FeatureStore.df_type_cols[dt]\n\n            # set dtype\n            for c in df_s.columns:\n                df_s[c] = df_s[c].astype(FeatureStore.dtype_dict[dt][c])\n            setattr(data_dict, dt, df_s)\n    data_dict.meta_info_df = pd.DataFrame([m.replace(\"\\n\", \"\").split(\":\")\n                                           for m in _flatten([d.split(\"\\t\")\n                                                              for d in content if d[0] == \"#\"]) if m != \"#\"])\n    startTime_ind = int(np.where(data_dict.meta_info_df[0] == 'startTime')[0])\n    endTime_ind = int(np.where(data_dict.meta_info_df[0] == 'endTime')[0])\n    data_dict.meta_info_df.loc[startTime_ind,\n                               1] = data_dict.meta_info_df.loc[startTime_ind+1, 0]\n    data_dict.meta_info_df.loc[endTime_ind,\n                               1] = data_dict.meta_info_df.loc[endTime_ind+1, 0]\n\n    data_dict.waypoint['timestamp'] = sample_sub[sample_sub.path_id ==\n                                                 path_id].timestamp.values.astype(int)\n    data_dict.waypoint['x'] = 0\n    data_dict.waypoint['y'] = 0\n    data_dict.n_floor = 0\n    data_dict.site_id = site_id\n    return data_dict","02ac251d":"def leak_postprocessing(submission_df,train_meta, postprocess_start=True, postprocess_end=True, postprocess_floor=True,start_threshold=5500,end_threshold=5500):\n    out_df=submission_df.copy()\n    out_df[[\"site_id\", \"path_id\", \"timestamp\"]] = out_df[\"site_path_timestamp\"].apply(\n        lambda x: pd.Series(x.split(\"_\")))\n    start_counter = 0\n    end_counter = 0\n    floor_counter = 0\n    input_path='\/kaggle\/input\/indoor-location-navigation\/'\n    sample_sub = pd.read_csv(f\"{input_path}\/sample_submission.csv\")\n    sample_sub = sample_sub[\"site_path_timestamp\"].apply(\n        lambda x: pd.Series(x.split(\"_\")))\n    sample_sub.columns = [\"site_id\", \"path_id\", \"timestamp\"]\n    out_df_unique=out_df.drop_duplicates(\n    subset=[\"site_id\", \"path_id\"]).reset_index(drop=True)\n    for i in tqdm(range(len(out_df_unique.path_id))):\n        t = out_df_unique.iloc[i]\n        site_id=t.site_id\n        path_id=t.path_id\n        feature = get_feature_test(site_id, path_id, input_path, sample_sub)\n        if feature.meta_info_df[feature.meta_info_df[0] == 'startTime'][1].values == None:\n            start_time = int(np.nanmin([feature.accelerometer.timestamp.min(\n            ), feature.wifi.timestamp.min(), feature.beacon.timestamp.min()]))\n        else:\n            start_time = int(\n                feature.meta_info_df[feature.meta_info_df[0] == 'startTime'][1])\n        if (len(feature.meta_info_df[feature.meta_info_df[0] == 'endTime']) == 0) or (feature.meta_info_df[feature.meta_info_df[0] == 'endTime'][1].values == None):\n            end_time = int(np.nanmax([feature.accelerometer.timestamp.max(\n            ), feature.wifi.timestamp.max(), feature.beacon.timestamp.max()]))\n        else:\n            end_time = int(\n                feature.meta_info_df[feature.meta_info_df[0] == 'endTime'][1])\n        if len(feature.beacon) > 0:\n            gap = feature.beacon.loc[0, 'timestamp2'] - \\\n                feature.beacon.loc[0, 'timestamp']\n        else:\n            gap = (feature.wifi.last_seen_timestamp.values -\n                   feature.wifi.timestamp.values).max()+210.14426803816337  # from mean gap\n        site_id = feature.site_id\n        train_meta_site = train_meta[train_meta.site_id == site_id]\n        \n        #postprocess start point based on leakage\n        train_meta_site_end = train_meta_site[(\n            start_time+gap) > train_meta_site.end_time]\n        if len(train_meta_site_end) > 0:\n            nearest_endpoint = train_meta_site_end.loc[train_meta_site_end.end_time.idxmax(\n            )]\n            if postprocess_start and (start_time + gap - nearest_endpoint.end_time < start_threshold):\n                out_df.loc[(out_df.path_id == path_id) & (out_df.timestamp == \n                    out_df[out_df.path_id == path_id].timestamp.min()), 'x'] = nearest_endpoint.end_wp_x\n                out_df.loc[(out_df.path_id == path_id) & (out_df.timestamp == \n                    out_df[out_df.path_id == path_id].timestamp.min()), 'y'] = nearest_endpoint.end_wp_y\n                start_counter += 1\n        \n        #postprocess end point based on leakage\n        train_meta_site_start = train_meta_site[train_meta_site.start_time > (\n            end_time+gap)]\n        if len(train_meta_site_start) > 0:\n            nearest_startpoint = train_meta_site_start.loc[train_meta_site_start.start_time.idxmin(\n            )]\n            if postprocess_end and (nearest_startpoint.start_time - end_time - gap < end_threshold):\n                out_df.loc[(out_df.path_id == path_id) & (out_df.timestamp == \n                    out_df[out_df.path_id == path_id].timestamp.max()), 'x'] = nearest_startpoint.start_wp_x\n                out_df.loc[(out_df.path_id == path_id) & (out_df.timestamp == \n                    out_df[out_df.path_id == path_id].timestamp.max()), 'y'] = nearest_startpoint.start_wp_y\n                end_counter += 1\n                \n        #postprocess floor based on leakage\n        if postprocess_floor:\n            if (len(train_meta_site_end) > 0) and (len(train_meta_site_start) > 0) and (nearest_endpoint.n_floor == nearest_startpoint.n_floor):\n                out_df.loc[(out_df.path_id == path_id),\n                                  'floor'] = nearest_endpoint.n_floor\n                floor_counter += (out_df.path_id == path_id).sum()\n\n            # uncomment this section if you want to postprocess all floor predictions\n            # elif (len(train_meta_site_end) > 0) and (len(train_meta_site_start) > 0):\n            #     diff_start_time = start_time - nearest_endpoint.end_time\n            #     diff_end_time = nearest_startpoint.start_time - end_time\n            #     if diff_start_time < diff_end_time:\n            #         out_df.loc[(out_df.path_id == path_id),\n            #                           'floor'] = nearest_endpoint.n_floor\n            #         floor_counter += (out_df.path_id == path_id).sum()\n            #     if diff_end_time < diff_start_time:\n            #         out_df.loc[(out_df.path_id == path_id),\n            #                           'floor'] = nearest_startpoint.n_floor\n            #         floor_counter += (out_df.path_id == path_id).sum()\n            # elif len(train_meta_site_end) > 0:\n            #     out_df.loc[(out_df.path_id == path_id),\n            #                       'floor'] = nearest_endpoint.n_floor\n            #     floor_counter += (out_df.path_id == path_id).sum()\n            # elif len(train_meta_site_start) > 0:\n            #     out_df.loc[(out_df.path_id == path_id),\n            #                       'floor'] = nearest_startpoint.n_floor\n            #     floor_counter += (out_df.path_id == path_id).sum()\n            \n    print(str(start_counter) + ' start points are postprocessed.')\n    print(str(end_counter) + ' end points are postprocessed.')\n    print(str(floor_counter) + ' floors are postprocessed.')\n    out_df = out_df.drop(\n        [\"site_id\", \"path_id\", \"timestamp\"], axis=1)\n    return out_df","49c0ab68":"# submission_df = pd.read_csv('..\/input\/3-3-g6-indoor-navigation-snap-to-grid\/submission_snap_to_grid.csv')\nsubmission_df = pd.read_csv('first-submission.csv')","2b94221e":"submission_df_leak_start = leak_postprocessing(submission_df,train_meta_sub, postprocess_start=True, postprocess_end=False, postprocess_floor=False)\nsubmission_df_leak_start.to_csv(\n    'submission_df_leak_start.csv', index=False)","d5a061c0":"submission_df_leak_end = leak_postprocessing(submission_df,train_meta_sub, postprocess_start=False, postprocess_end=True, postprocess_floor=False)\nsubmission_df_leak_end.to_csv(\n    'submission_df_leak_end.csv', index=False)","327dfd70":"submission_df_leak_floor = leak_postprocessing(submission_df,train_meta_sub, postprocess_start=False, postprocess_end=False, postprocess_floor=True)\nsubmission_df_leak_floor.to_csv(\n    'submission_df_leak_floor.csv', index=False)","a3c46389":"submission_df_leak_all = leak_postprocessing(submission_df,train_meta_sub, postprocess_start=True, postprocess_end=True, postprocess_floor=True)\nsubmission_df_leak_all.to_csv(\n    'submission_df_leak_all.csv', index=False)","8c14d43e":"submission_df_leak_start_end = leak_postprocessing(submission_df,train_meta_sub, postprocess_start=True, postprocess_end=True, postprocess_floor=False)\nsubmission_df_leak_start_end.to_csv(\n    'submission_df_leak_start_end.csv', index=False)","c9ca1d47":"### Solution follows","351c45f7":"**The blending method is to halve the stride length and adopt all steps, the number of steps will be doubled, but here only the movement distance is required**","123c51d9":"You can see the raw timestamps have information for the floor in training dataset.","66069036":"I postprocessed all data here.","b7bda4a4":"# Get first and last waypoints in train dataset","4ecd3b7c":"The LB Scores are as follows\n* Original: 4.789\n* Start point: 4.751\n* End point: 4.750\n* Floor: 4.995\n* All: 4.924\n* Start and end point: 4.718\n\nAlthough the postprocessing floor makes the LB score a bit lower, it is surprising that the score is comparable even if about 80% of floor predictions were replaced with those without any sensor information. If I uncomment the section in postprocessing function and postprocess all floor predictions, the LB score becomes 8.7xx. It is still not so bad, if I consider the fact that LB score < 15 means my floor prediction error < 1.\n\nAny comments are welcome.","e471ea7f":"# Postprocessing based on leaked feature.","16115d09":"This notebook demonstrates a post-processing strategy for the\n[Indoor Location & Navigation](https:\/\/www.kaggle.com\/c\/indoor-location-navigation)\ncompetition.\n\nTo combine machine learning (wifi features) predictions with sensor data (acceleration, attitude heading),\nI defined cost function as follows,\n$$\nL(X_{1:N}) = \\sum_{i=1}^{N} \\alpha_i \\| X_i - \\hat{X}_i \\|^2 + \\sum_{i=1}^{N-1} \\beta_i \\| (X_{i+1} - X_{i}) - \\Delta \\hat{X}_i \\|^2\n$$\nwhere $\\hat{X}_i$ is absolute position predicted by machine learning and $\\Delta \\hat{X}_i$ is relative position predicted by sensor data.\n\nSince the cost function is quadratic, the optimal $X$ is solved by linear equation $Q X = c$\n, where $Q$ and $c$ are derived from above cost function.\nBecause the matrix $Q$ is tridiagonal,\neach machine learning prediction is corrected by *all* machine learning predictions and sensor data.\n\nThe optimal hyperparameters ($\\alpha$ and $\\beta$) can be estimated by expected error of machine learning and sensor data,\nor just tuned by public score.","446ab04e":"I postprocessed start and end point here.","28d760f1":"You can see the last waypoints of some paths are exactly the same as the first waypoints of other paths in training dataset.","c333c364":"### Disclaimer\n* Credits go to https:\/\/www.kaggle.com\/museas for revisiting the cost minimisation solution. \n* I came across this method from https:\/\/www.kaggle.com\/mehrankazeminia\/1-3-indoor-navigation-cost-minimization-floor, which I thank here for their contribution.\n* The submission file used was from the best public ensemble, published in this notebook: https:\/\/www.kaggle.com\/saurabhbagchi\/ensembling-best-performing-notebooksCredits go to https:\/\/www.kaggle.com\/museas for revisiting the cost minimisation solution. I came across this method from https:\/\/www.kaggle.com\/mehrankazeminia\/1-3-indoor-navigation-cost-minimization-floor, which I thank here for their contribution","b4602056":"\n\n**The step estimation module provided by the host is great, but sometimes it points to strange positions. That of the host uses TYPE_ROTATION_VECTOR. Therefore, by using it in combination with the estimation using TYPE_MAGNETIC_FIELD, the walking direction can be brought closer to the accurate one.**\n\n\n\nScore improved 0.018 in this sub\n\n4.545 \u2192\u30004.527","4f41f8a6":"I postprocessed start waypoints only here.","5a5a2e48":"I postprocessed end waypoints only here.","cbb5e5ac":"# Postprocessing with leaked feature\n\nIn the discussion section, I proposed [the possiility of the leakage caused by raw timestamps](https:\/\/www.kaggle.com\/c\/indoor-location-navigation\/discussion\/228898).\n\nIn short.\n* The last waypoints of some paths are exactly the same as the first waypoints of other paths. I think this is because they are divided from a single measurement.\n* The raw timestamps have information for the floor. I think this is because the measurements took place in floor by floor manner.\n\nIn this notebook, I demonstrate the existance of the leakage by presenting the results of postprocessing based on the raw timestamps  and not on any sensor information are actually good or even better.\n\nThe algorithm of postprocessing is simple.\n* For x and y of the first waypoint, I use those of the temporally nearest endpoint in training dataset.\n* For x and y of the last waypoint, I use those of the temporally nearest startpoint in training dataset.\n* For floor of all waypoints, I use those of the temporally nearest endpoint and startpoint in training dataset if they match each other.\n\nI have not decided whether I use this leakage. Should I try to create the model valid for real life or just to win this competition? I hope the competition host recreated the test dataset without any leakages.\n\nI use some codes, data, and ideas from following notebooks. Thank you very much.\n* https:\/\/www.kaggle.com\/kenmatsu4\/feature-store-for-indoor-location-navigation\n* https:\/\/www.kaggle.com\/mehrankazeminia\/3-3-g6-indoor-navigation-snap-to-grid\n* https:\/\/www.kaggle.com\/jiweiliu\/fix-the-timestamps-of-test-data-using-dask\n\n[update] I found a bug in calculating the raw time stamp and fix it. The score changed worse a little bit.\n[update2] I found another bug in getting test data function and fix it. Then the score changed better a little bit.","0eb1b3ee":"I postprocessed floor only here","d53cac2a":"## References\n+ [Simple 99% Accurate Floor Model](https:\/\/www.kaggle.com\/nigelhenry\/simple-99-accurate-floor-model)\n+ [Indoor Location Competition 2.0 (Sample Data and Code)](https:\/\/github.com\/location-competition\/indoor-location-competition-20)","e1f31f01":"I used the submission file of the current best public notebook for demonstration."}}