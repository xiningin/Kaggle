{"cell_type":{"bb1990bf":"code","256c8865":"code","665ddbc0":"code","9acdb229":"code","40692acc":"code","9bd9c48e":"code","723512b7":"code","cc1a262a":"markdown","cf8265f4":"markdown","f5ca828f":"markdown","a7318ec2":"markdown","75b9af56":"markdown","604ded6b":"markdown","b93228a1":"markdown","0f7c549f":"markdown"},"source":{"bb1990bf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# datset\nfrom sklearn.datasets import load_boston\nX, y = load_boston(return_X_y=True)\n\nnp.random.seed(123)","256c8865":"# Boston Dataset\nm, n = X.shape\nprint(f\"num of samples: {m}\\nnum of feats: {n}\")","665ddbc0":"# return covariance_matrix\ndef get_covar_matrix(dataset_X, is_col_standardized=False):\n    \n    if type(dataset_X) != np.ndarray: raise Exception(\"X must be a numpy array\")\n    \n    if is_col_standardized is False:\n        # column standardize `dataset_X` (w\/ matrix operations)\n        n_means   = np.mean(X, axis=0)\n        n_stds    = np.std(X, axis=0)\n        dataset_X = (X-n_means) \/ (n_stds) # automatic broadcasting\n    \n    # use col-standardized formula\n    num_samples, _ = dataset_X.shape\n    covar_mat = (1\/num_samples) * (dataset_X.T.dot(dataset_X))\n    \n    return covar_mat","9acdb229":"covar_mat = get_covar_matrix(X, is_col_standardized=False)\ncovar_mat.shape","40692acc":"# dont use matshow\nplt.imshow(covar_mat)\n\nplt.xlabel(\"feature indies\")\nplt.ylabel(\"feature indies\")\nplt.title(\"Covariance matrix\")\nplt.colorbar()\nplt.show()","9bd9c48e":"# 1. STANDARDIZE\nfrom sklearn.preprocessing import StandardScaler\nstandardizer = StandardScaler()\n\nX_std = standardizer.fit_transform(X)\ncov_mat = np.cov(X_std.T)","723512b7":"plt.figure(figsize=(10,10))\nsns.set(font_scale=1)\n\nlabels = [f\"feat{i}\" for i in range(0, 13)]\n\nhm = sns.heatmap(\n        cov_mat,\n        cbar=True,\n        annot=True,\n        square=True,\n        fmt='.2f',\n        annot_kws={'size': 7},\n        yticklabels= labels,\n        xticklabels= labels,\n        cmap='viridis'\n    )\n\n\nplt.title('Covariance matrix showing correlation coefficients\\n')\nplt.show()","cc1a262a":"## Using Library: One-Liner w\/ Scikit, Numpy and Seaborn","cf8265f4":"**Implement co-var matrix**","f5ca828f":"## Co-variance Matrix","a7318ec2":"**Visualize**\n\n> Large values are yellow \/ lighter green","75b9af56":"# CO-VARIANCE MATRIX","604ded6b":"> - Note principal diagonal elements have large values (because, each feature is highly correlated w\/ itself) \n> - Feature `9` and `8` are highly correlated\n> - Half of the values are duplicates","b93228a1":"Diven a dataset with $m$ samples(rows) and $n$ features(cols),\n\n> Co-variance matrix represents co-variance of all features against all features.\n\nAs a result, it is square matrix of dims $nxn$. Each cell of matrix represents covariance of two features\n\n$$covar(X_1,X_2) = \\frac{1}{m}\\sum{(\\mu_{X_1} - x_{1_i})(\\mu_{X_2} - x_{2_i})}$$\n\n**Note:** \n\n> - When all features in dataset are **column-standardized** i.e converted to $N(0,1)$,\n> $$covar(X_1,X_2) = \\frac{1}{m}\\sum{(\\mu_{X_1} - x_{1_i})(\\mu_{X_2} - x_{2_i})} = \\frac{1}{m}\\sum{x_{1_i}*x_{2_i}}$$\n> $$\\Rightarrow covar(X_1,X_2) = \\frac{1}{m} ( {X_1}^T \\cdot X_2 )$$\n\nNote how it becomes a simple dot product","0f7c549f":"**Variance**\n\n> On average, how much the datapoints are away from mean\n\n$$var(X) = \\frac{1}{m}\\sum{(\\mu_X - x_i)^{2}} \\, \\, \\,  \\text{where \"m\" is number of samples}$$\n\nNote that it is squared because we are more focused on magnitude of difference. *Not direction*.\n\n**Co-variance**\n\n> Explained concisely [here](https:\/\/www.kaggle.com\/l0new0lf\/02-04-relationship-between-two-features). Used to find relationship between features\n\n$$covar(X,Y) = \\frac{1}{m}\\sum{(\\mu_X - x_i)(\\mu_Y - y_i)}$$\n\nNote that it is **not** squared because more focused on *direction as well as magnitude* of difference"}}