{"cell_type":{"e2cbdd9d":"code","66231a8c":"code","b6dbff8a":"code","17c99b55":"code","f097febd":"code","33101e46":"code","9643f535":"code","419590a9":"code","68b852db":"code","04421930":"code","d4e20865":"code","4f9e60d3":"code","479f00c7":"code","3bfaf4d5":"code","ac4ec9e7":"code","ad379c8f":"code","ae12d2f2":"code","d503a71a":"code","cdf6d2d3":"code","40a155fb":"code","7277ef8d":"code","90960327":"code","4f3591e4":"code","ca76c11c":"code","f03b6a3f":"code","b25c55a1":"code","dd4f95b9":"code","1f0a6890":"code","af5dfe6b":"code","d8930555":"code","1340f439":"code","67ce70e1":"code","998f53ee":"code","9069de15":"code","c826fd07":"code","79156e45":"code","a843ac25":"code","ad668691":"code","2d90004b":"code","04b8b440":"code","af23d57d":"code","09daa051":"code","ec329439":"code","325b19be":"code","9392d5b7":"code","d11a7d4c":"code","65bfe576":"code","245bb311":"code","8d7dbde2":"code","fce0eb92":"code","064cd62e":"code","29bb588b":"code","370bfb57":"code","76d2c150":"code","1176c9d1":"code","149b157b":"code","f14150e9":"code","b26aedcf":"code","773071a5":"code","6c8528af":"code","ca64d860":"code","5c702d6b":"code","23b5c2b8":"code","648c5ee9":"code","5493238d":"code","2e9fa206":"code","948c839b":"code","97098190":"code","72dfbb82":"code","2cf1d148":"code","f5e55d6a":"code","b0b2d169":"code","4ad6c214":"code","c4cbc6c0":"code","a64d9504":"code","8bb07c28":"code","c4c4ee89":"code","ac7a6eb9":"code","617c61a6":"code","f17e1b3c":"code","c0f1991d":"code","5efd2b0f":"code","4f0823b6":"code","dd0b2f8e":"markdown","f75e93be":"markdown","8a68bf47":"markdown","c4187e78":"markdown","41b0e544":"markdown","f15f2cf3":"markdown","0bbac17e":"markdown","7763fa02":"markdown","a7dba3a2":"markdown","9cef3f5b":"markdown","5bd95881":"markdown","0bd07479":"markdown","740cb868":"markdown","df7769c5":"markdown","1885b3b3":"markdown","a004c703":"markdown","fbd03c78":"markdown","02098f18":"markdown","fd15f206":"markdown","c925d25a":"markdown","62210823":"markdown","ad64a09a":"markdown","fbf4afd1":"markdown","93f81284":"markdown","03e6cef6":"markdown","9595c99d":"markdown","4b92f119":"markdown","17ea51ef":"markdown","6f14bac7":"markdown","74742255":"markdown","a0a2d331":"markdown","9e6344a2":"markdown","2b3b3507":"markdown","807324af":"markdown","79035d47":"markdown","c1039016":"markdown","07ec0040":"markdown","1cd47055":"markdown","9076c59f":"markdown","b13cdced":"markdown","c7338566":"markdown","178ed9e9":"markdown","924a715c":"markdown","edc46738":"markdown","cbfa6f87":"markdown","391ab8f4":"markdown","d9ac7baf":"markdown","616cc57e":"markdown"},"source":{"e2cbdd9d":"# !pip install fastai\n# import libraries\nimport missingno as msno\nimport fastai\nfrom fastai import *\nfrom fastai.text import * \nimport pandas as pd\nimport numpy as np\nfrom functools import partial\nimport io\nimport os","66231a8c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.express as px\nimport cufflinks as cf \nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)  \nimport plotly.figure_factory as ff\nimport random\n","b6dbff8a":"def random_colors(number_of_colors):\n    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n                 for i in range(number_of_colors)]\n    return color","17c99b55":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint('Training Set Shape = {}'.format(train.shape))\nprint('Test Set Shape = {}'.format(test.shape))","f097febd":"train.columns\n","33101e46":"train.head()","9643f535":"test.columns","419590a9":"test.head()","68b852db":"train.info()\n","04421930":"test.info()","d4e20865":"train.describe()","4f9e60d3":"test.describe()","479f00c7":"train.isnull().sum()","3bfaf4d5":"test.isnull().sum()","ac4ec9e7":"msno.matrix(train)\n","ad379c8f":"msno.heatmap(train)","ae12d2f2":"msno.bar(train, color = 'b', figsize = (10,8))","d503a71a":"msno.matrix(test)\n","cdf6d2d3":"msno.heatmap(test)","40a155fb":"msno.bar(test, color = 'b', figsize = (10,8))","7277ef8d":"species_count = train['target'].value_counts()\ndata = [go.Bar(\n    x = species_count.index,\n    y = species_count.values,\n    marker = dict(color = random_colors(3),line=dict(color='#000000', width=2))\n)]\n\nlayout = go.Layout(\n   {\n      \"title\":\"Disaster Analyis\",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","90960327":"trace = go.Pie(labels = list(train.target.unique()), values = list(train.target.value_counts()),\n                            hole = 0.2,\n               marker=dict(colors = random_colors(2), \n                           line=dict(color='#000000', width=2)\n                           ))\ndata = [trace]\nlayout = go.Layout(\n   {\n      \"title\":\"Disaster Analysis \",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","4f3591e4":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train[(train['target'] != 0)][\"text\"], title=\"Word Cloud for Negative Text\")","ca76c11c":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train[(train['target'] != 1)][\"text\"], title=\"Word Cloud for Positive Text\")","f03b6a3f":"from collections import defaultdict\ntrain1_df = train[(train['target'] != 0)]\ntrain0_df = train[(train['target'] != 1)]\n\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ))\n    return trace\n\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), random_colors(50))\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), random_colors(50))\n\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of Positive Text\", \n                                          \"Frequent words of Negative Text\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\niplot(fig)","b25c55a1":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), random_colors(50))\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50),random_colors(50))\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of Positive Text\", \n                                          \"Frequent bigrams of Negative Text\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\niplot(fig)","dd4f95b9":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), random_colors(50))\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), random_colors(50))\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.2,\n                          subplot_titles=[\"Frequent trigrams of Positive Text\", \n                                          \"Frequent trigrams of Negative Text\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\niplot(fig)","1f0a6890":"import string\n## Number of words in the text ##\ntrain[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\ntrain[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\ntrain[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\ntrain[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntrain[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\ntrain[\"num_punctuations\"] =train[\"text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntrain[\"num_punctuations\"] =train[\"text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntrain[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntrain[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntrain[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","af5dfe6b":"cols = \"num_words\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","d8930555":"Present = train[(train['target'] != 1)]\nNot_Present = train[(train['target'] != 0)]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","1340f439":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","67ce70e1":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","998f53ee":"cols = \"mean_word_len\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\ndata = [trace0]\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","9069de15":"Present = train[(train['target'] != 1)]\nNot_Present = train[(train['target'] != 0)]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","c826fd07":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","79156e45":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","a843ac25":"cols = \"num_words_title\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","ad668691":"Present = train[(train['target'] != 1)]\nNot_Present = train[(train['target'] != 0)]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","2d90004b":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","04b8b440":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","af23d57d":"cols = \"num_words_upper\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","09daa051":"Present = train[(train['target'] != 1)]\nNot_Present = train[(train['target'] != 0)]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","ec329439":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","325b19be":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","9392d5b7":"cols = \"num_punctuations\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","d11a7d4c":"Present = train[(train['target'] != 1)]\nNot_Present = train[(train['target'] != 0)]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","65bfe576":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","245bb311":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","8d7dbde2":"cols = \"num_stopwords\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","fce0eb92":"Present = train[(train['target'] != 1)]\nNot_Present = train[(train['target'] != 0)]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","064cd62e":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","29bb588b":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","370bfb57":"cols = \"num_chars\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","76d2c150":"Present = train[(train['target'] != 1)]\nNot_Present = train[(train['target'] != 0)]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","1176c9d1":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","149b157b":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","f14150e9":"cols = \"num_unique_words\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","b26aedcf":"Present = train[(train['target'] != 1)]\nNot_Present = train[(train['target'] != 0)]\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Positive', 'Negative']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","773071a5":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","6c8528af":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","ca64d860":"EMOTICONS = {\n    u\":\u2011\\)\":\"Happy face or smiley\",\n    u\":\\)\":\"Happy face or smiley\",\n    u\":-\\]\":\"Happy face or smiley\",\n    u\":\\]\":\"Happy face or smiley\",\n    u\":-3\":\"Happy face smiley\",\n    u\":3\":\"Happy face smiley\",\n    u\":->\":\"Happy face smiley\",\n    u\":>\":\"Happy face smiley\",\n    u\"8-\\)\":\"Happy face smiley\",\n    u\":o\\)\":\"Happy face smiley\",\n    u\":-\\}\":\"Happy face smiley\",\n    u\":\\}\":\"Happy face smiley\",\n    u\":-\\)\":\"Happy face smiley\",\n    u\":c\\)\":\"Happy face smiley\",\n    u\":\\^\\)\":\"Happy face smiley\",\n    u\"=\\]\":\"Happy face smiley\",\n    u\"=\\)\":\"Happy face smiley\",\n    u\":\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\":D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n    u\"X\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\":\"Very happy\",\n    u\":\u2011\\(\":\"Frown, sad, andry or pouting\",\n    u\":-\\(\":\"Frown, sad, andry or pouting\",\n    u\":\\(\":\"Frown, sad, andry or pouting\",\n    u\":\u2011c\":\"Frown, sad, andry or pouting\",\n    u\":c\":\"Frown, sad, andry or pouting\",\n    u\":\u2011<\":\"Frown, sad, andry or pouting\",\n    u\":<\":\"Frown, sad, andry or pouting\",\n    u\":\u2011\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\[\":\"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n    u\">:\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\{\":\"Frown, sad, andry or pouting\",\n    u\":@\":\"Frown, sad, andry or pouting\",\n    u\">:\\(\":\"Frown, sad, andry or pouting\",\n    u\":'\u2011\\(\":\"Crying\",\n    u\":'\\(\":\"Crying\",\n    u\":'\u2011\\)\":\"Tears of happiness\",\n    u\":'\\)\":\"Tears of happiness\",\n    u\"D\u2011':\":\"Horror\",\n    u\"D:<\":\"Disgust\",\n    u\"D:\":\"Sadness\",\n    u\"D8\":\"Great dismay\",\n    u\"D;\":\"Great dismay\",\n    u\"D=\":\"Great dismay\",\n    u\"DX\":\"Great dismay\",\n    u\":\u2011O\":\"Surprise\",\n    u\":O\":\"Surprise\",\n    u\":\u2011o\":\"Surprise\",\n    u\":o\":\"Surprise\",\n    u\":-0\":\"Shock\",\n    u\"8\u20110\":\"Yawn\",\n    u\">:O\":\"Yawn\",\n    u\":-\\*\":\"Kiss\",\n    u\":\\*\":\"Kiss\",\n    u\":X\":\"Kiss\",\n    u\";\u2011\\)\":\"Wink or smirk\",\n    u\";\\)\":\"Wink or smirk\",\n    u\"\\*-\\)\":\"Wink or smirk\",\n    u\"\\*\\)\":\"Wink or smirk\",\n    u\";\u2011\\]\":\"Wink or smirk\",\n    u\";\\]\":\"Wink or smirk\",\n    u\";\\^\\)\":\"Wink or smirk\",\n    u\":\u2011,\":\"Wink or smirk\",\n    u\";D\":\"Wink or smirk\",\n    u\":\u2011P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X\u2011P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u2011\u00de\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u00de\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u2011\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":\u2011\\|\":\"Straight face\",\n    u\":\\|\":\"Straight face\",\n    u\":$\":\"Embarrassed or blushing\",\n    u\":\u2011x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":\u2011#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":\u2011&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:\u2011\\)\":\"Angel, saint or innocent\",\n    u\"O:\\)\":\"Angel, saint or innocent\",\n    u\"0:\u20113\":\"Angel, saint or innocent\",\n    u\"0:3\":\"Angel, saint or innocent\",\n    u\"0:\u2011\\)\":\"Angel, saint or innocent\",\n    u\"0:\\)\":\"Angel, saint or innocent\",\n    u\":\u2011b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n    u\">:\u2011\\)\":\"Evil or devilish\",\n    u\">:\\)\":\"Evil or devilish\",\n    u\"\\}:\u2011\\)\":\"Evil or devilish\",\n    u\"\\}:\\)\":\"Evil or devilish\",\n    u\"3:\u2011\\)\":\"Evil or devilish\",\n    u\"3:\\)\":\"Evil or devilish\",\n    u\">;\\)\":\"Evil or devilish\",\n    u\"\\|;\u2011\\)\":\"Cool\",\n    u\"\\|\u2011O\":\"Bored\",\n    u\":\u2011J\":\"Tongue-in-cheek\",\n    u\"#\u2011\\)\":\"Party all night\",\n    u\"%\u2011\\)\":\"Drunk or confused\",\n    u\"%\\)\":\"Drunk or confused\",\n    u\":-###..\":\"Being sick\",\n    u\":###..\":\"Being sick\",\n    u\"<:\u2011\\|\":\"Dump\",\n    u\"\\(>_<\\)\":\"Troubled\",\n    u\"\\(>_<\\)>\":\"Troubled\",\n    u\"\\(';'\\)\":\"Baby\",\n    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(\u30fb\\.\u30fb;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\":\"Sleeping\",\n    u\"\\(\\^_-\\)\":\"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n    u\"\\(\\+o\\+\\)\":\"Confused\",\n    u\"\\(o\\|o\\)\":\"Ultraman\",\n    u\"\\^_\\^\":\"Joyful\",\n    u\"\\(\\^_\\^\\)\/\":\"Joyful\",\n    u\"\\(\\^O\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(\\^o\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\":\"Sad or Crying\",\n    u\"\\(\/_;\\)\":\"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n    u\"\\(;_;\":\"Sad of Crying\",\n    u\"\\(;_:\\)\":\"Sad or Crying\",\n    u\"\\(;O;\\)\":\"Sad or Crying\",\n    u\"\\(:_;\\)\":\"Sad or Crying\",\n    u\"\\(ToT\\)\":\"Sad or Crying\",\n    u\";_;\":\"Sad or Crying\",\n    u\";-;\":\"Sad or Crying\",\n    u\";n;\":\"Sad or Crying\",\n    u\";;\":\"Sad or Crying\",\n    u\"Q\\.Q\":\"Sad or Crying\",\n    u\"T\\.T\":\"Sad or Crying\",\n    u\"QQ\":\"Sad or Crying\",\n    u\"Q_Q\":\"Sad or Crying\",\n    u\"\\(-\\.-\\)\":\"Shame\",\n    u\"\\(-_-\\)\":\"Shame\",\n    u\"\\(\u4e00\u4e00\\)\":\"Shame\",\n    u\"\\(\uff1b\u4e00_\u4e00\\)\":\"Shame\",\n    u\"\\(=_=\\)\":\"Tired\",\n    u\"\\(=\\^\\\u00b7\\^=\\)\":\"cat\",\n    u\"\\(=\\^\\\u00b7\\\u00b7\\^=\\)\":\"cat\",\n    u\"=_\\^=\t\":\"cat\",\n    u\"\\(\\.\\.\\)\":\"Looking down\",\n    u\"\\(\\._\\.\\)\":\"Looking down\",\n    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n    u\"\\(\\\u30fb\\\u30fb?\":\"Confusion\",\n    u\"\\(?_?\\)\":\"Confusion\",\n    u\">\\^_\\^<\":\"Normal Laugh\",\n    u\"<\\^!\\^>\":\"Normal Laugh\",\n    u\"\\^\/\\^\":\"Normal Laugh\",\n    u\"\\\uff08\\*\\^_\\^\\*\uff09\" :\"Normal Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n    u\"\\(\\^\u2014\\^\\\uff09\":\"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n    u\"\\\uff08\\^\u2014\\^\\\uff09\":\"Waving\",\n    u\"\\(;_;\\)\/~~~\":\"Waving\",\n    u\"\\(\\^\\.\\^\\)\/~~~\":\"Waving\",\n    u\"\\(-_-\\)\/~~~ \\($\\\u00b7\\\u00b7\\)\/~~~\":\"Waving\",\n    u\"\\(T_T\\)\/~~~\":\"Waving\",\n    u\"\\(ToT\\)\/~~~\":\"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n    u\"\\(\\*_\\*\\)\":\"Amazed\",\n    u\"\\(\\*_\\*;\":\"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n    u'\\(-\"-\\)':\"Worried\",\n    u\"\\(\u30fc\u30fc;\\)\":\"Worried\",\n    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n    u\"\\(\\\uff3e\uff56\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\\uff3e\uff55\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n    u\"\\(\\^O\\^\\)\":\"Happy\",\n    u\"\\(\\^o\\^\\)\":\"Happy\",\n    u\"\\)\\^o\\^\\(\":\"Happy\",\n    u\":O o_O\":\"Surprised\",\n    u\"o_0\":\"Surprised\",\n    u\"o\\.O\":\"Surpised\",\n    u\"\\(o\\.o\\)\":\"Surprised\",\n    u\"oO\":\"Surprised\",\n    u\"\\(\\*\uffe3m\uffe3\\)\":\"Dissatisfied\",\n    u\"\\(\u2018A`\\)\":\"Snubbed or Deflated\"\n}","5c702d6b":"from collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport string\nlemmatizer = WordNetLemmatizer()\ncnt = Counter()\nPUNCT_TO_REMOVE = string.punctuation\nSTOPWORDS = set(stopwords.words('english'))\nn_rare_words = 10\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndef remove_rarewords(text):\n    \"\"\"custom function to remove the rare words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ndef lemmatize_words(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n\ndef remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', string)\n\ndef remove_emoticons(text):\n    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n    return emoticon_pattern.sub(r'', text)\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ndef remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ntrain[\"text\"] = train[\"text\"].str.lower()\ntrain[\"text\"] = train[\"text\"].apply(lambda text: remove_punctuation(text))\ntrain[\"text\"] = train[\"text\"].apply(lambda text: remove_stopwords(text))\ntrain[\"text\"] = train[\"text\"].apply(lambda text: remove_emoji(text))\ntrain[\"text\"] = train[\"text\"].apply(lambda text: remove_emoticons(text))\ntrain[\"text\"] = train[\"text\"].apply(lambda text: remove_urls(text))\ntrain[\"text\"] = train[\"text\"].apply(lambda text: remove_html(text))\n\nfor text in train[\"text\"].values:\n    for word in text.split():\n        cnt[word] += 1\n    \nFREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n\ntrain[\"text\"] = train[\"text\"].apply(lambda text: remove_freqwords(text))\n\nRAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\ntrain[\"text\"] = train[\"text\"].apply(lambda text: remove_rarewords(text))\ntrain[\"text\"] = train[\"text\"].apply(lambda text: stem_words(text))\ntrain[\"text\"] = train[\"text\"].apply(lambda text: lemmatize_words(text))","23b5c2b8":"train.head()","648c5ee9":"text = train[[\"text\",\"target\"]]","5493238d":"data_lm = (TextList.from_df(text)\n           #Inputs: all the text files in path\n            .split_by_rand_pct(0.20)\n           #We randomly split and keep 20% for validation\n            .label_for_lm()           \n           #We want to do a language model so we label accordingly\n            .databunch(bs=128))\ndata_lm.save('tmp_lm')","2e9fa206":"data_lm.show_batch()","948c839b":"# Language model AWD_LSTM\nlearn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)","97098190":"print('Model Summary:')\nprint(learn.layer_groups)","72dfbb82":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","2cf1d148":"learn.fit_one_cycle(10, 1e-2)\nlearn.save('lm_hyper')","f5e55d6a":"learn.unfreeze()\nlearn.fit_one_cycle(10, 1e-3)","b0b2d169":"learn.save_encoder('ft_enc')\n","4ad6c214":"data_clas = (TextList.from_df(train, cols=[\"text\"], vocab=data_lm.vocab)\n             .split_by_rand_pct(0.3)\n             .label_from_df('target')\n             .databunch(bs=128))\n\ndata_clas.save('tmp_class')\n","c4cbc6c0":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n","a64d9504":"learn.load_encoder('ft_enc')","8bb07c28":"learn.freeze_to(-1)\nlearn.summary()","c4c4ee89":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","ac7a6eb9":"learn.fit_one_cycle(20, 1e-1)","617c61a6":"learn.save('stage1')","f17e1b3c":"learn.fit_one_cycle(20,1e-1)\nlearn.save('stage2')","c0f1991d":"from fastai.vision import ClassificationInterpretation\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(6,6), dpi=60)","5efd2b0f":"interp = TextClassificationInterpretation.from_learner(learn)\ninterp.show_top_losses(10)","4f0823b6":"learn.export()\nlearn.model_dir = \"\/kaggle\/working\"\nlearn.save(\"stage-1\",return_path=True)","dd0b2f8e":"<font color=\"blue\" size=+2.5><b> 3. Train a Text Language Model <\/b><\/font>","f75e93be":"<a id=\"6.2\"><\/a>\n<font color=\"blue\" size=+2.5><b>Data Exploration<\/b><\/font>","8a68bf47":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> N-gram Analysis <\/b><\/font>\n","c4187e78":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of Unique words <\/b><\/font>","41b0e544":"<a id=\"6.5\"><\/a>\n<font color=\"blue\" size=+2.5><b> Target Class Distribution <\/b><\/font>","f15f2cf3":"<a id=\"15\"><\/a>\n<font color=\"blue\" size=+2.5><b> 4.2 Loading Text Classification Model <\/b><\/font>\n","0bbac17e":"\n\n<a id=\"1\"><\/a>\n<font color=\"blue\" size=+2.5><b>Introduction<\/b><\/font>\n","7763fa02":"**Lets train our language model. First, we call lr_find to analyze and find an optimal learning rate for our problem, then we fit or train the model for a few epochs. Finally we unfreeze the model and runs it for a few more epochs. So we have a encoder trained and ready to be used for our classifier and it is recorded on disk.**","a7dba3a2":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of Char <\/b><\/font>","9cef3f5b":"***Create a databunch for a text language model to get the data ready for training a language model. The text will be processed, tokenized and numericalized by a default processor, if you want to apply a customized tokenizer or vocab, you just need to create them.***","5bd95881":"<a id=\"8\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.3 Model Loading For Language Model training <\/b><\/font>\n","0bd07479":"<a id=\"13\"><\/a>\n<font color=\"blue\" size=+2.5><b> 4. Building and Training a Text Classifier <\/b><\/font>","740cb868":"<a id=\"6.4\"><\/a>\n<font color=\"blue\" size=+2.5><b> Visualisation of Missing Values <\/b><\/font>","df7769c5":"<a id=\"5\"><\/a>\n\n\n<font color=\"blue\" size=+2.5><b>2. Library<\/b><\/font>","1885b3b3":"<a id=\"24\"><\/a>\n<font color=\"blue\" size=+2.5><b>5.4 Sources<\/b><\/font>\n<br\/>\n* [Fastai MOOC](https:\/\/course.fast.ai\/)\n* [Fastai library](https:\/\/docs.fast.ai\/)","a004c703":"<a id=\"6.1\"><\/a>\n<font color=\"blue\" size=+2.5><b>Data Loading<\/b><\/font>","fbd03c78":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>Objective  <\/center><\/h2>\n\nGoal of this kernel is following:\n- Basic Exploratory Data Analysis on Text Data.\n- Data Cleaning of Text Data.\n- Learn how to train FastAI  ULMFIT on custom text data.\n- Learn how to implement language model on custom data.\n- Learn how to use Transfer Learning to get better accuracy.\n- Provide Perfect Guide for all the tips and trick to implement Text Classification model and get better accuracy as a Beginner.\n","02098f18":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of Stopwords <\/b><\/font>","fd15f206":"<a id=\"14\"><\/a>\n<font color=\"blue\" size=+2.5><b> 4.1 Loading Data For Text Classification <\/b><\/font>\n","c925d25a":"<a id=\"4\"><\/a>\n<font color=\"blue\" size=+2.5><b>2.1 Installation<\/b><\/font>\n* Numpy\n* Pandas\n* Matplotlib\n* Fastai","62210823":"**Now, the training cycle is repeated: lr_find, freeze except last layer,..., unfreeze the model and saving the final trained model.**","ad64a09a":"<a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center> Table of content <\/center><\/h2>\n\n<font color=\"blue\" size=+1><b>Introduction<\/b><\/font>\n* [1. What is Sentiment Analysis ?](#2)\n* [2. What is Transfer Learning ?](#3)    \n\n<font color=\"blue\" size=+1><b>Library<\/b><\/font>\n* [1. Installation](#4)\n* [2. Import Libraries ](#5)\n    \n<font color=\"blue\" size=+1><b> Data Exploration and Data Cleaning <\/b><\/font>\n* [1. load Data](#6.1)\n* [2. Data Exploration ](#6.2)\n* [3. Check Null Value](#6.3)\n* [4. Visualisation of missing Values ](#6.4)\n* [5. Target class Distribution](#6.5)\n* [6. Common Words Analysis ](#6.6)\n* [7. Data Cleaning ](#6.7) \n   \n<font color=\"blue\" size=+1><b> Train a text language model <\/b><\/font>\n* [1. Data Loading ](#6)\n* [2. Data Explorations ](#8)\n* [3. Model Loading For Language Model training ](#7)\n* [4. Training Language Model ](#9)\n* [5. Model Summary ](#10)\n* [6. Finding LR ](#11)\n* [7. Hyper Parameter Tuning ](#12)\n* [8. Saving Model ](#13)\n\n<font color=\"blue\" size=+1><b> Building a Text Classifier <\/b><\/font>\n* [1. Data Loading ](#14)\n* [2. Data Explorations ](#15)\n* [3. Model Loading ](#16)\n* [4. Training Model ](#17)\n* [5. Model Summary ](#18)\n* [6. Finding LR ](#19)\n* [7. Hyper Parameter Tuning ](#20)\n* [8. Saving Model ](#21)\n\n<font color=\"blue\" size=+1><b> Others <\/b><\/font>\n* [1. Interpret the results](#22)\n* [2. Prediction Using Trained Model](#23)\n* [3. Save and Load Model](#24)\n* [4. Sources](#25)","fbf4afd1":"<a id=\"2\"><\/a>\n<font color=\"blue\" size=+2.5><b>1.2  What is Sentiment Analysis ?<\/b><\/font>\n<br\/>\n<br\/>\n![image.png](attachment:image.png)\n\n<br\/>\n**Sentiment analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.**","93f81284":"![image.png](attachment:image.png)","03e6cef6":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Data Preprocessing and Data Cleaning <\/b><\/font>\n","9595c99d":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Word Cloud for a Target Variable <\/b><\/font>\n","4b92f119":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of words <\/b><\/font>","17ea51ef":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of words Upper <\/b><\/font>","6f14bac7":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Mean words length<\/b><\/font>","74742255":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>References and Credits<\/center><\/h2>\n\nThis notebook wouldn't have been possible without these following resources.This kernel includes codes and ideas from kernels below. If this kernel helps you, please upvote their work as well.\n\n* [Improve your Score with some Text Preprocessing](https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-some-text-preprocessing)\n* [A Real Disaster - Leaked Label](https:\/\/www.kaggle.com\/szelee\/a-real-disaster-leaked-label)\n* [Natural Language Processing (NLP) \ud83e\uddfe for Beginners](https:\/\/www.kaggle.com\/faressayah\/natural-language-processing-nlp-for-beginners)\n* [Disaster NLP: Keras BERT using TFHub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub)\n* [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert)\n* [Basic EDA,Cleaning and GloVe](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)","a0a2d331":"<font size=\"+3\" color=blue><b> <center><u>Tweets Classification(EDA , Cleaning , FastAI)<\/u><\/center><\/b><\/font>","9e6344a2":"<a id=\"11\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.6 HyperParameter Tuning For Model Training <\/b><\/font>","2b3b3507":"**we will have to load the encoder previously trained (the language model).**","807324af":"<a id=\"12\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.7 Saving Model After Training <\/b><\/font>","79035d47":"<a id=\"3\"><\/a>\n<font color=\"blue\" size=+2.5><b>1.3  What is Language Model ?<\/b><\/font>\n<br\/>\n<br\/>\n\n![image.png](attachment:image.png)\n**A language model learns to predict the probability of a sequence of words. But why do we need to learn the probability of words? Let\u2019s understand that with an example.**\n\nI\u2019m sure you have used Google Translate at some point. We all use it to translate one language to another for varying reasons. This is an example of a popular NLP application called Machine Translation.\n\nIn Machine Translation, you take in a bunch of words from a language and convert these words into another language. Now, there can be many potential translations that a system might give you and you will want to compute the probability of each of these translations to understand which one is the most accurate.\n[Source](https:\/\/www.analyticsvidhya.com\/blog\/2019\/08\/comprehensive-guide-language-model-nlp-python-code\/)","c1039016":"<a id=\"10\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.5 Finding LR <\/b><\/font>\n","07ec0040":"<a id=\"22\"><\/a>\n<font color=\"blue\" size=+2.5><b>5.1 Interpret the results<\/b><\/font>\n<br\/>","1cd47055":"<a id=\"6.6\"><\/a>\n<font color=\"blue\" size=+2.5><b> Keyword Analysis <\/b><\/font>","9076c59f":"<a id=\"6.3\"><\/a>\n<font color=\"blue\" size=+2.5><b>Checking the Null Values <\/b><\/font>","b13cdced":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of in Title <\/b><\/font>","c7338566":"<a id=\"5\"><\/a>\n<font color=\"blue\" size=+2.5><b>2.2 Library Import<\/b><\/font>","178ed9e9":"<a id=\"9\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.4 Model Summary <\/b><\/font>\n","924a715c":"Now we can create a language model based on the architecture \n[AWD_LSTM](https:\/\/docs.fast.ai\/text.models.html#AWD_LSTM)","edc46738":"<a id=\"23\"><\/a>\n<font color=\"blue\" size=+2.5><b>5.3 Save and Load Model<\/b><\/font>\n<br\/>\n","cbfa6f87":"<a id=\"18\"><\/a>\n<font color=\"blue\" size=+2.5><b>Feedback and Support<\/b><\/font>\n<br\/>\n* Your feedback is much appreciated\n* Please UPVOTE if you LIKE this notebook\n* Comment if you have any doubts or you found any errors in the notebook","391ab8f4":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Feature Engineering <\/b><\/font>\n\n* Number of words in the text\n* Number of unique words in the text\n* Number of characters in the text\n* Number of stopwords\n* Number of punctuations\n* Number of upper case words\n* Number of title case words\n* Average length of the words","d9ac7baf":"<font size=\"+2\" color=blue ><b>Please Upvote my kernel and keep it in your favourite section if you think it is helpful.<\/b><\/font>","616cc57e":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Number of Punc <\/b><\/font>"}}