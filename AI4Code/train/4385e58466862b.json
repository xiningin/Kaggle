{"cell_type":{"809f1fbb":"code","afe1cdb6":"code","7bdeb909":"code","35db27c7":"code","d4b940fb":"code","20d3b9f7":"code","626a7576":"code","7fa4ba47":"code","0436ddeb":"code","70d01d37":"code","3a1743e3":"code","af1472da":"code","1351bd09":"code","1e55f441":"code","5271c052":"code","41351fda":"code","95b27d05":"code","c81ce0a9":"code","d2f3e88c":"code","1fba3dd4":"code","7cae29d8":"code","b7b381ed":"code","dbc550a6":"code","87da86c9":"code","d888798f":"code","26859730":"code","b698f92d":"code","40102e05":"code","2df38106":"code","10e769dc":"code","dedef7d2":"code","3d701478":"code","18f09da7":"code","13d2ae02":"code","e9bd207d":"code","20dadc99":"code","4a9e2327":"code","c8ff712b":"code","68f84444":"code","d3f6a616":"code","12e93100":"code","7370c183":"code","72d7e683":"code","7b921cc3":"markdown","d2b890ee":"markdown","da9da6c7":"markdown","ea4131a0":"markdown","0d9a9318":"markdown","91fba623":"markdown","2de3f349":"markdown","aa723d34":"markdown","908a0e23":"markdown","b5dce7fb":"markdown","cd654f1a":"markdown"},"source":{"809f1fbb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","afe1cdb6":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\")\n\n#basics\nimport numpy as np \nimport pandas as pd \nimport lightgbm as lgbm\nimport xgboost as xgb\n\n#methods n algo\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import confusion_matrix,classification_report\nimport lightgbm as lgbm\nimport xgboost as xgb\n\n#plots and other utilities\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n#pd.set_option('max_columns',100)\nimport plotly.express as ex\n\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nfrom pandas.plotting import parallel_coordinates\nimport pandas_profiling\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nplt.rc('figure',figsize=(17,13))","7bdeb909":"#comment this after installation of sweetviz (running second time)\n!pip install sweetviz","35db27c7":"# \/kaggle\/input\/ml2020spring-hw2\/data\/X_train\n# \/kaggle\/input\/ml2020spring-hw2\/data\/X_test\n# \/kaggle\/input\/ml2020spring-hw2\/data\/Y_train\ntrain_full_data = pd.read_csv('..\/input\/ml2020spring-hw2\/data\/train.csv')\ntrain_full_data.replace('\\s+','',regex=True,inplace=True) \ntrain_full_label = train_full_data.y\ntrain_full_data.drop(columns=['id' , 'y'],inplace=True)\ntest_data = pd.read_csv('..\/input\/ml2020spring-hw2\/data\/test_no_label.csv')\ntest_data.drop(columns = ['id'], inplace = True)\ntest_data.replace('\\s+','',regex=True,inplace=True) \nprint(train_full_data[train_full_data['migration prev res in sunbelt'] == ' ?'])","d4b940fb":"train_full_data.columns\nprint(train_full_data.shape)\nprint(train_full_label.shape)\n# Index(['age', 'class of worker', 'detailed industry recode',\n#        'detailed occupation recode', 'education', 'wage per hour',\n#        'enroll in edu inst last wk', 'marital stat', 'major industry code',\n#        'major occupation code', 'race', 'hispanic origin', 'sex',\n#        'member of a labor union', 'reason for unemployment',\n#        'full or part time employment stat', 'capital gains', 'capital losses',\n#        'dividends from stocks', 'tax filer stat',\n#        'region of previous residence', 'state of previous residence',\n#        'detailed household and family stat',\n#        'detailed household summary in household',\n#        'migration code-change in msa', 'migration code-change in reg',\n#        'migration code-move within reg', 'live in this house 1 year ago',\n#        'migration prev res in sunbelt', 'num persons worked for employer',\n#        'family members under 18', 'country of birth father',\n#        'country of birth mother', 'country of birth self', 'citizenship',\n#        'own business or self employed',\n#        'fill inc questionnaire for veteran's admin', 'veterans benefits',\n#        'weeks worked in year', 'year'],\n#       dtype='object')","20d3b9f7":"# print(train_full_data.isnull().sum())\nprint(train_full_data.dtypes)\n# train_full_data.replace(' ?' , np.nan,inplace = True)\ntrain_full_data.replace('?' , np.nan,inplace = True)\ntest_data.replace('?' , np.nan,inplace = True)\nprint(train_full_data.isnull().sum())\nprint(train_full_data.dtypes)","626a7576":"print(train_full_data[train_full_data['migration prev res in sunbelt'] == np.nan])\n# print(train_full_data )\n# print(train_full_data.head() , train_full_data.tail())","7fa4ba47":"train_full_data.describe()","0436ddeb":"a = train_full_data['migration prev res in sunbelt'].value_counts()\nprint(a.keys()[0])","70d01d37":"def countTheMost(col_list):\n    return col_list.value_counts().index[0]\n\nprint(train_full_data['migration code-change in reg'].isnull())\n# migration code-change in msa\nprint(train_full_data['migration code-change in reg'].isnull().sum())\nNan_replace_value = countTheMost(train_full_data['migration code-change in reg'])\nprint(Nan_replace_value)\ntrain_full_data['migration code-change in reg'] = \\\n    train_full_data['migration code-change in reg'].fillna(Nan_replace_value)\nprint(train_full_data['migration code-change in reg'].isnull().sum())\n#test\n# for col_name in train_full_data.columns :\n#     countTheMost(train_full_data[col_name])","3a1743e3":"# dispose of Nan data\nfor col_name in train_full_data.columns :\n    #train data\n    col_list = train_full_data[col_name]\n    Nan_replace_value = countTheMost(col_list)\n    train_full_data[col_name]= train_full_data[col_name].fillna(Nan_replace_value)\n    #test data\n    col_list = test_data[col_name]\n    Nan_replace_value = countTheMost(col_list)\n    test_data[col_name]= test_data[col_name].fillna(Nan_replace_value)\n#     if train_full_data[col_name].isnull().sum()!=0:\n#         print(col_name)\n#         print(type(train_full_data[col_name][0]))","af1472da":"for col_name in train_full_data.columns :\n    if train_full_data[col_name].isnull().sum()!=0 or test_data[col_name].isnull().sum()!=0 :\n        print (col_name)","1351bd09":"print(type(train_full_data['enroll in edu inst last wk'][0]))\nprint((train_full_data['enroll in edu inst last wk'][0]))","1e55f441":"def featureCatalogize(df_col):\n    '''\n    input : a column needed to be catalogized of the datafrmae\n    output: numeric list , char2num , num2char\n    '''\n    label_mapping = {label:idx for idx,label in enumerate(np.unique(df_col))}\n    inv_label_mapping = {v:k for k,v in label_mapping.items()}\n    label_col = df_col.map(label_mapping)\n    return label_col, label_mapping, inv_label_mapping\n\nname2num_mapping_dict = dict()\nnum2name_mapping_dict = dict()\n\nfor col_name in train_full_data.columns:\n#     print(train_full_data[col_name])\n    if isinstance(train_full_data[col_name][0],str):\n        print('col name:%s'%col_name )\n        print('type:%s'%type(train_full_data[col_name][0]))\n        train_full_data[col_name],name2num_mapping_dict[col_name],num2name_mapping_dict[col_name] \\\n            = featureCatalogize(train_full_data[col_name])\n        test_data[col_name] = test_data[col_name].map(name2num_mapping_dict[col_name])\n\n\n\nfor col_name in train_full_data.columns:\n#     print(train_full_data[col_name])\n    if isinstance(train_full_data[col_name][0],str):\n        print('col name:%s'%col_name )\n        print('type:%s'%type(train_full_data[col_name][0]))\n        train_full_data[col_name],name2num_mapping_dict[col_name],num2name_mapping_dict[col_name] \\\n            = featureCatalogize(train_full_data[col_name])\n        test_data[col_name] = test_data[col_name].map(name2num_mapping_dict[col_name])\n\n        ","5271c052":"print(name2num_mapping_dict)\nprint(num2name_mapping_dict)","41351fda":"train_full_data.describe()","95b27d05":"corr_mat = train_full_data.corr()\ndef dropCorrHeat(corr):\n    plt.matshow(corr_mat, cmap='plasma')\n    col_name =  train_full_data.columns\n    plt.yticks(np.arange(0, len(col_name), 1),col_name) \n    plt.colorbar()\n    # plt.xticks(np.arange(0, len(col_name), 1),col_name)\n    plt.show() \ndropCorrHeat(corr_mat)","c81ce0a9":"# #analyzing data using sweetviz (automated EDA)\n# import sweetviz as sv\n# data_report = sv.analyze(train_full_data)\n# data_report.show_html('Analysis.html')\n# # show analysis result \n# from IPython.display import IFrame\n# IFrame(src = 'Analysis.html',width=1000,height=600)","d2f3e88c":"drop_list = ['migration code-move within reg','detailed household summary in household']\ntrain_full_data = train_full_data.drop(columns = drop_list)\ntest_data = test_data.drop(columns = drop_list)","1fba3dd4":"need_list = ['country of birth father', 'country of birth mother', 'country of birth self', 'citizenship']\ngroup_data = train_full_data[need_list]\nfrom sklearn.decomposition import PCA\ndataset = group_data.values\nprint(dataset.shape)","7cae29d8":"def bestPCnum(dataset , needplot = 0):\n    len = dataset.shape[1]\n    pca_transformer = PCA(n_components=len).fit(dataset)\n    PCA_contirbution = pca_transformer.explained_variance_ratio_\n    pre = 0\n    res = -1 \n    PCA_contirbution_sum = np.empty(0) \n    flag = 0 \n    for i in range(0 ,  PCA_contirbution.shape[0]):\n        pre = pre + PCA_contirbution[i] \n        PCA_contirbution_sum=np.append(PCA_contirbution_sum , pre) \n        if pre >= 0.85:\n            print(\"sum contribution: %lf,idx:%d\" % (pre,i) ) \n            if not flag:\n                res = i \n                flag = 1 \n    if needplot:\n        plt.figure(1) \n        plt.plot(np.arange(1, PCA_contirbution.shape[0]+1 ,1 ) , PCA_contirbution)\n        plt.xlabel('PCA_n')\n        plt.ylabel('PCA_contribution') \n        plt.show() \n        plt.figure(2)\n        plt.plot(np.arange(1, PCA_contirbution.shape [0] + 1, 1), PCA_contirbution_sum)\n        plt.xlabel('PCA_n')\n        plt.ylabel('PCA_contribution_sum')\n        plt.show()\n        print(PCA_contirbution_sum) \n    return res ;\n\ndef dataPCADealing(dataset):\n    pc_n = bestPCnum(dataset, needplot = 1) \n    pca_transformer = PCA(n_components=pc_n).fit(dataset)\n    pca_dataset = pca_transformer.transform(dataset)\n    return pca_dataset , pca_transformer\n\ndef pcaAnalysis(dataset ,n_pc = 4):\n    pca_dataset ,pca_transformer= dataPCADealing(dataset)\n    plt.matshow(pca_transformer.components_[0:n_pc], cmap='plasma')\n    origin_feature_name = [\"value\",\"sex\",\"homeown\",\"income\",\"age\",\"pmethod1\",\"pmethod2\",\"pmethod3\"]\n    pc_name = np.empty(0) \n    for i in range( 0 , n_pc):\n        name = 'component'+str(i+1) \n        pc_name = np.append(pc_name , name) \n    plt.yticks(np.arange(0, n_pc, 1),\n               pc_name) \n    plt.colorbar()\n    plt.xticks(range(len(origin_feature_name)), origin_feature_name, rotation=60, ha='left')\n    plt.show() ","b7b381ed":"#result : 1 is enough\npca_dataset , pca_transformer = dataPCADealing(dataset)\nprint(pca_dataset.shape)\ntrain_full_data['country'] = pca_dataset\ntrain_full_data = train_full_data.drop(columns = need_list)\ntest_data['country'] = pca_transformer.transform(test_data[need_list])\ntest_data = test_data.drop(columns = need_list)","dbc550a6":"corr_mat = train_full_data.corr()\ndef dropCorrHeat(corr):\n    plt.matshow(corr_mat, cmap='plasma')\n    col_name =  train_full_data.columns\n    plt.yticks(np.arange(0, len(col_name), 1),col_name) \n    plt.colorbar()\n    # plt.xticks(np.arange(0, len(col_name), 1),col_name)\n    plt.show() \ndropCorrHeat(corr_mat)","87da86c9":"test_data['country'] = test_data['country'].astype(int)\ntrain_full_data['country'] = train_full_data['country'].astype(int)","d888798f":"#final result\ncorr_mat = train_full_data.corr()\ndropCorrHeat(corr_mat)","26859730":"print(test_data.columns.shape)\nprint(train_full_data.columns.shape)","b698f92d":"# model = xgb.XGBClassifier(**{'colsample_bylevel': 0.7, 'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 1000,\n#                                  'reg_lambda': 15,'eval_metric': 'error','subsample': 0.5}).fit(train_full_data.values, train_full_label)","40102e05":"# test_y = model.predict(test_data.values)\n# test_y = test_y.replace('50000+.', 1).astype(int)\n# test_y = test_y.replace('-50000', 0).astype(int)","2df38106":"# output = pd.DataFrame({'id': range(0 , len(test_y)), 'label': test_y})\n# output = output.replace('50000+.', 1)\n# output = output.replace('-50000', 0)\n# output.to_csv('my_submission.csv', index=False)","10e769dc":"# train_full_data","dedef7d2":"train_full_data['label'] = train_full_label\ntrain_full_data['label'].replace(\"50000+.\",1,inplace = True)\ntrain_full_data['label'].replace('-50000.',0,inplace = True)","3d701478":"# train_full_data['ageCateg'] = pd.qcut(train_full_data['age'], 5)\n# print(train_full_data.ageCateg)\n#  [(-0.001, 25.0] < (25.0, 35.0] < (35.0, 44.0] < (44.0, 56.0] < (56.0, 90.0]]\ndef ageCut(train):\n    train.loc [train ['age'] <= 25, 'age'] = 0\n    train.loc [(train ['age'] > 25.0) & (train ['age'] <= 35.0), 'age'] = 1\n    train.loc [(train ['age'] > 35.0) & (train ['age'] <= 44.0), 'age'] = 2\n    train.loc [(train ['age'] > 44.0) & (train ['age'] <= 56.0), 'age'] = 3\n    train.loc [(train ['age'] > 56.0) & (train ['age'] <= 90.0), 'age'] = 4\n    \nageCut(train_full_data)\nageCut(test_data)","18f09da7":"# train_full_data = train_full_data.drop(columns='ageCateg')\n# test_data = test_data.drop(columns='ageCateg')\nprint(test_data.describe().shape)\nprint(train_full_data.describe().shape)","13d2ae02":"# train_full_data.to_csv('my_dataset.csv', index=False)\ntest_data.to_csv('my_testset.csv', index=False)","e9bd207d":"class BayesClassifier:\n    def __init__(self,pd_dataset = None,continous_data_list=None):\n        self.discreteMemoryMap = dict()\n        self.continuousDataSet=set()\n        if continous_data_list!=None:\n            for col_name in continous_data_list:\n                self.continuousDataSet.add(col_name)\n        self.pdDataset = copy.deepcopy(pd_dataset)\n        self.classNum = dict() # possible number of Discrete data's category\n        for col_name in self.pdDataset.columns:\n            if col_name not in self.continuousDataSet:\n                num = len(np.unique(self.pdDataset[col_name]))\n                self.classNum[col_name]=num\n                self.discreteMemoryMap[col_name]=dict()\n\n    def priorPossContinous(self,feature_name,feature_val,class_flag):\n        pd_need = self.pdDataset[self.pdDataset[feature_name] == feature_val \\\n                                 & self.pdDataset.label == class_flag]\n        mean_val = pd_need[feature_name].mean()\n        std_val = pd_need[feature_name].std()\n        prio_poss = np.exp( (-1*(( feature_val - mean_val )**2) ) \/ (2 * (std_val**2) ) ) \/ (np.sqrt(2*np.pi)*mean_val)\n        return np.log(prio_poss)\n\n    def priorPossDiscrete(self,feature_name,feature_val,class_flag):\n        if feature_val in self.discreteMemoryMap[feature_name].keys():\n            if class_flag in self.discreteMemoryMap[feature_name][feature_val].keys():\n                return self.discreteMemoryMap[feature_name][feature_val][class_flag]\n        else:\n            self.discreteMemoryMap[feature_name][feature_val] = dict()\n\n        pd_cond = self.pdDataset [(self.pdDataset [feature_name] == feature_val) \\\n                                  & (self.pdDataset.label == class_flag)]\n        pd_tot = self.pdDataset[self.pdDataset.label == class_flag]\n        prio_poss = ( pd_cond.shape[0] + 1 ) \/ ( pd_tot.shape[0] + self.classNum[feature_name] )\n        self.discreteMemoryMap[feature_name][feature_val][class_flag] = np.log(prio_poss)\n        val=np.log(prio_poss)\n        self.discreteMemoryMap[feature_name][feature_val][class_flag] = val\n        return val\n\n\n    def predict(self,pd_tesst_dataset):\n        col_list = pd_tesst_dataset.columns\n        stastic_pos_poss = np.log(self.pdDataset.label.sum())\n        stastic_neg_poss = np.log(self.pdDataset.shape[0] - stastic_pos_poss)\n        label_result = []\n        for i in range(0 , pd_tesst_dataset.shape[0]):\n            row_data = pd_tesst_dataset.loc[i][:]\n            pos_poss = stastic_pos_poss\n            neg_poss = stastic_neg_poss\n            for col_name in col_list:\n                col_val = row_data[col_name]\n                if col_name in self.continuousDataSet:\n                    temp_pos_poss = self.priorPossContinous(col_name, col_val, 1)\n                    temp_neg_poss = self.priorPossContinous(col_name, col_val, 0)\n                else:\n                    temp_pos_poss = self.priorPossDiscrete(col_name, col_val, 1)\n                    temp_neg_poss = self.priorPossDiscrete(col_name, col_val, 0)\n                pos_poss = pos_poss + temp_pos_poss\n                neg_poss = neg_poss + temp_neg_poss\n\n            if pos_poss > neg_poss:\n                label_result = np.append(label_result , 1)\n            else:\n                label_result = np.append(label_result , 0)\n        return label_result\n        print('Finish prediction!')\n\n","20dadc99":"def result2CSV(result_label,name=None):\n    output = pd.DataFrame({'id': range(0, len(result_label)), 'label': result_label})\n    output['label'] = output['label'].astype(int)\n    if name == None:\n        output.to_csv('my_submission.csv', index=False)\n    else:\n        output.to_csv(name+'.csv', index=False)","4a9e2327":"# print(predict_result)","c8ff712b":"# bc_model=BayesClassifier(pd_dataset=train_full_data,continous_data_list=None)\n# predict_result = bc_model.predict(pd_tesst_dataset = test_data)\n# result2CSV(predict_result,name = 'BayesClassifier_result')","68f84444":"class LogisticRegression:\n    def __init__(self,pd_dataset = None,non_order_data_list=None):\n        self.nonOrderList = non_order_data_list\n        self.pdDataset = copy.deepcopy(pd_dataset)\n        self.label=pd_dataset.label\n        self.columns = pd_dataset.columns\n        self.pdDataset.drop(columns=['label'], inplace=True)\n        # one_hot_code = pd.get_dummies(self.pdDataset[non_order_data_list[0]])\n        self.coderDict = dict()\n        coder=OneHotEncoder()\n        hot_code_lsit=np.empty([self.pdDataset.shape[0],0])\n        for col_name in non_order_data_list:\n            coder.fit(self.pdDataset[col_name].unique().reshape(-1,1))\n            self.coderDict[col_name]=copy.deepcopy(coder)\n            temp_list=coder.transform(self.pdDataset[col_name].values.reshape(-1,1)).toarray()\n            hot_code_lsit=np.concatenate([hot_code_lsit,temp_list],axis=1)\n        self.pdDataset.drop(columns=non_order_data_list,inplace = True)\n        self.pdDataset=np.concatenate([self.pdDataset.values,hot_code_lsit],axis=1)\n        # print(self.pdDataset.shape)\n\n    def oneHotCoder(self,data):\n        # print('---------------------------')\n        # one_hot_code = pd.get_dummies(data [self.nonOrderList[0]])\n        hot_code_list=np.empty([data.shape[0],0])\n        for col_name in self.nonOrderList:\n            coder=self.coderDict[col_name]\n            one_hot_code_temp = coder.transform(data[col_name].values.reshape(-1,1)).toarray()\n            hot_code_list=np.concatenate([hot_code_list,one_hot_code_temp],axis=1)\n        data.drop(columns=self.nonOrderList, inplace=True)\n        res=np.concatenate([data.values,hot_code_list],axis=1)\n        # print(res.shape)\n        return res\n\n    def train(self,lamada=None):\n        mat_X=np.asmatrix(np.concatenate([self.pdDataset , np.ones([self.pdDataset.shape[0],1])],axis=1))\n        if lamada==None:\n            dot_inv=np.linalg.pinv(np.dot(mat_X.T,mat_X))\n            self.arg_val = np.dot(np.dot(dot_inv, mat_X.T), self.label)\n        else:\n            symm_mat=np.zeros([mat_X.shape[1],mat_X.shape[1]])\n            for i in range(1,symm_mat.shape[0]-1):\n                symm_mat[i][i]=1\n            symm_mat=lamada*symm_mat\n            dot_inv = np.linalg.inv(np.dot(mat_X.T, mat_X)+symm_mat)\n            self.arg_val=np.dot(np.dot(dot_inv,mat_X.T),self.label)\n\n    def predict(self,data):\n        self.train(lamada=10)\n        one_hot_data=self.oneHotCoder(data)\n        pred_result=[]\n        for i in range(0,one_hot_data.shape[0]):\n            feature_val=np.append(one_hot_data[i][:].reshape(-1,1),1)\n            res=np.dot(feature_val,np.asarray(self.arg_val).reshape(-1,1))\n            if res>0.5:\n                res=1\n            else:\n                res=0\n            pred_result=np.append(pred_result,res)\n        return pred_result","d3f6a616":"non_col=['class of worker','detailed industry recode',\n             'detailed occupation recode','marital stat','major industry code',\n             'marital stat','major industry code','reason for unemployment'\n             ,'full or part time employment stat','tax filer stat',\n             'region of previous residence','own business or self employed'\n             ,'veterans benefits','country','migration code-change in msa'\n             ,'migration code-change in reg']","12e93100":"import copy\nfrom sklearn.preprocessing import OneHotEncoder","7370c183":"train_full_data['label'] = train_full_label\ntrain_full_data['label'].replace(\"50000+.\",1,inplace = True)\ntrain_full_data['label'].replace('-50000.',0,inplace = True)","72d7e683":"lr = LogisticRegression(pd_dataset=train_full_data,non_order_data_list=non_col)\nlg_result=lr.predict(test_data)\nresult2CSV(lg_result,'logistics_regression_result')","7b921cc3":"# Mapping and cutting ","d2b890ee":"# **Deal with Nan data**","da9da6c7":"# discriminative method--logistics regression","ea4131a0":"# Deal with the first and the second group\n\nsimply drop one feature for each of them","0d9a9318":"# generative method -- Naive Bayes Classifiers","91fba623":"# **Feature engineering**\n**1. Correlationship Test**\n\n**2. Forest test the importance of varieties**\n\n**3.New features contruction**","2de3f349":"**It is notable that only 3 groups of features need to be dealt with**\n\n1.  'detailed household and family stat','detailed household summary in household' , 2 in total\n\n2. 'migration code-change in reg','migration code-move within reg' , 2 in total\n\n3. 'country of birth father', 'country of birth mother', 'country of birth self', 'citizenship', 4 in total","aa723d34":"# Raw method handwriting\n\n1. generative method -- Naive Bayes Classifiers\n\n2. discriminative method -- logistics regression","908a0e23":"# Deal with the third group\n\n1. PCA might work well because of high correlationship in this group","b5dce7fb":"# Feature selection\n\n1. Forest selection method\n\n2. whatever try it first","cd654f1a":"# Model construction\n\n1.xgboost"}}