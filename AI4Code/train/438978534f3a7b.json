{"cell_type":{"8f9cd4f1":"code","7a9cf1a0":"code","f3e83b04":"code","4eb745b6":"code","6cff864c":"code","6cbd8311":"code","5c2be42e":"code","22125c36":"code","b3568493":"code","d22cc839":"code","6f852cb0":"code","83501c53":"code","9dc0f112":"code","9f38f435":"code","910c187f":"code","aa4a8c68":"code","7d9eea6c":"code","f6895b7c":"code","9c24fe33":"code","f3dd6844":"code","f63b17df":"code","69ee0f09":"code","a6466fa2":"code","05f6d752":"code","0ca5ea6a":"code","53dbec12":"code","8ce0c762":"markdown","e00aecf8":"markdown","39f6c996":"markdown","b8fafb48":"markdown","44994b16":"markdown","37c57a11":"markdown","cd4e4af1":"markdown","f5964cd5":"markdown","0565b048":"markdown","c7973347":"markdown","1821cd28":"markdown","cbd9efdc":"markdown","7886a798":"markdown","f62bcc47":"markdown","8546ef41":"markdown"},"source":{"8f9cd4f1":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS","7a9cf1a0":"import os\nemails = pd.read_csv('..\/input\/emails.csv')\nprint(emails.shape)\nemails.head()","f3e83b04":"out = None\nfor emails in pd.read_csv('..\/input\/emails.csv', chunksize = 10000):\n    if out is None:\n        out = emails.copy()\n    else:\n        out.append(emails)\n    out.head()","4eb745b6":"print(out.shape)","6cff864c":"print (out.info())","6cbd8311":"# extract to, from and body from each email\ndef email_parsing(raw_message):\n    lines = raw_message.split('\\n')\n    email = {}\n    message = ''\n    keys_to_extract = ['from', 'to']\n    for line in lines:\n        if ':' not in line:\n            message += line.strip()\n            email['body'] = message\n        else:\n            pairs = line.split(':')\n            key = pairs[0].lower()\n            val = pairs[1].strip()\n            if key in keys_to_extract:\n                email[key] = val\n    return email","5c2be42e":"# compile all extracted data into a single dictionary\ndef emails_parsing(messages):\n    emails = [email_parsing(message) for message in messages]\n    return {\n        'body': result_append(emails, 'body'), \n        'to': result_append(emails, 'to'), \n        'from_': result_append(emails, 'from')\n    }","22125c36":"# append all key results into single list\ndef result_append(emails, key):\n    results = []\n    for email in emails:\n        if key not in email:\n            results.append('')\n        else:\n            results.append(email[key])\n    return results","b3568493":"# create a new dataframe for extracted data\nextracted_data = pd.DataFrame(emails_parsing(out['message']))","d22cc839":"# check any NaN values\nextracted_data.isnull().any()","6f852cb0":"# drop empty values\n# extracted_data.drop(extracted_data.query(\"body == '' | to == '' | from_ == ''\").index, inplace = True)","83501c53":"stopwords = ENGLISH_STOP_WORDS.union(['ect', 'hou', 'com', 'recipient'])\nvect = TfidfVectorizer(analyzer = 'word', stop_words = stopwords, max_df = 0.5, min_df = 2)","9dc0f112":"X = vect.fit_transform(extracted_data.body)","9f38f435":"X_dense = X.todense()\ncoords = PCA(n_components=2).fit_transform(X_dense)\nplt.scatter(coords[:, 0], coords[:, 1], c = 'm')","910c187f":"features = vect.get_feature_names()","aa4a8c68":"def top_msg_features(X, features, row_id, top_words = 25):\n    row = np.squeeze(X[row_id].toarray())\n    return top_tfidf_features(row, features, top_words)","7d9eea6c":"def top_tfidf_features(row, features, top_words = 20):\n\t# argsort produces the indices that orders the row by tf-idf value\n\t# reverse them (into descending order) and select the top words\n    top_ids = np.argsort(row)[::-1][:top_words]\n    top_features = [(features[i], row[i]) for i in top_ids]\n    df = pd.DataFrame(top_features, columns=['features', 'score'])\n    return df","f6895b7c":"def top_msg_features(X, features, row_id, top_words = 25):\n    row = np.squeeze(X[row_id].toarray())\n    return top_tfidf_features(row, features, top_words)","9c24fe33":"def top_terms(X, features, grp_ids = None, min_tfidf = 0.1, top_words = 25):\n    if grp_ids:\n        msgs = X[grp_ids].toarray()\n    else:\n        msgs = X.toarray()\n    msgs[msgs < min_tfidf] = 0\n    # calculate the mean of each column across the selected rows (which results in a single row of tf-idf values)\n    tfidf_means = np.mean(msgs, axis = 0)\n    # then pass on to the previous func for picking out the top n words\n    return top_tfidf_features(tfidf_means, features, top_words)","f3dd6844":"# create classifier with 3 clusters and 100 iterations\nclf = KMeans(n_clusters = 3, max_iter = 100, init = 'k-means++', n_init = 1)\nlabels = clf.fit_predict(X)","f63b17df":"X_dense = X.todense()\npca = PCA(n_components = 2).fit(X_dense)\ncoords = pca.transform(X_dense)","69ee0f09":"label_colours = ['c', 'm', 'r']\ncolours = [label_colours[i] for i in labels]\nplt.scatter(coords[:, 0], coords[:, 1], c = colours)","a6466fa2":"def top_words_per_cluster(X, y, features, min_tfidf = 0.1, top_words = 25):\n    dfs = []\n\n    labels = np.unique(y)\n    for label in labels:\n        ids = np.where(y == label) \n        features_df = top_terms(X, features, ids, min_tfidf = min_tfidf, top_words = top_words)\n        features_df.label = label\n        dfs.append(features_df)\n    return dfs","05f6d752":"def plot_top_words(dfs):\n    fig = plt.figure(figsize = (11, 8), facecolor = \"w\")\n    x = np.arange(len(dfs[0]))\n    for i, df in enumerate(dfs):\n        ax = fig.add_subplot(1, len(dfs), i+1)\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n        ax.set_frame_on(False)\n        ax.get_xaxis().tick_bottom()\n        ax.get_yaxis().tick_left()\n        ax.set_xlabel(\"Tf-Idf Score\", labelpad = 16, fontsize = 13)\n        ax.set_title(\"cluster = \" + str(df.label), fontsize = 15)\n        ax.ticklabel_format(axis = 'x', style = 'sci', scilimits = (-2, 2))\n        ax.barh(x, df.score, align = 'center', color = '#7530FF')\n        ax.set_yticks(x)\n        ax.set_ylim([-1, x[-1] + 1])\n        yticks = ax.set_yticklabels(df.features)\n        plt.subplots_adjust(bottom = 0.09, right = 0.97, left = 0.15, top = 0.95, wspace = 0.52)\n    plt.show()","0ca5ea6a":"tops = top_words_per_cluster(X, labels, features, 0.1, 25)","53dbec12":"plot_top_words(tops)","8ce0c762":"\"grp_ids\" takes a list of row indices which pick out some particular messages that you want to inspect providing \"None\" indicates, that you are interested in all messages (emails).","e00aecf8":"Now as it's known which emails the model assigned to each cluster, it's possible to extract the top words per cluster.","39f6c996":"# K-Means Clustering","b8fafb48":"# Clustering Enron Dataset","44994b16":"The next step is to calculate the average tf-idf score of all words across all emails, in other words the average per column of a tf-idf matrix","37c57a11":"A simple machine learning approach to investigate the Enron email dataset by applying k-means algorithm to cluster the unlabeled emails where it classifies emails based on their message body.","cd4e4af1":"Hence, the output of a tf-idf is a sparse matrix which doesnt support matrix operations, so a single row need[](http:\/\/) first to be converted into dense format in order to apply top_tfidf_features function.","f5964cd5":"T[](http:\/\/)he following[](http:\/\/) function top_tfidf_features[](http:\/\/) takes a single row of the tf-idf matrix (corresponding to a particular message), and return the n highest scoring words (or more generally tokens or features).","0565b048":"K-means clustering is one of the simplest and popular clustering algorithms that stores k centroids that it uses to define clusters.","c7973347":"Now the email bodies are converted into a document-term matrix to visualise this matrix we need to create 2d representation of the DTM (document-term matrix)","1821cd28":"To visualise it, we need to make 2d coordinates from the sparse matrix.","cbd9efdc":"To this point emails has no null records, yet its well-known that better encoding of categorical data means better performance as many machine learning algorithms cannot operate on label data directly, this means categorical data must be converted to a numerical form.","7886a798":"The next step is to find out the top keywords of each email, this can be done by creating a variable (features) that holds a list of all the words in the tf-idfs vocabulary in the same order as the columns in the matrix.","f62bcc47":"Instead of loading in all +500k emails, as it's hard to fit such data within a PC's memory, the dataset will be chunked into segments with approximately 1[](http:\/\/)0k each.","8546ef41":"Now the out dataframe has 10k messages separated into 2 columns \"file\" and \"message\", before working with this dataset the body message should be parsed into key-value pairs where only the sender, receiver and email body data will be extracted from the body message."}}