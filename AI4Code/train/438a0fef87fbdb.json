{"cell_type":{"cd3c7906":"code","e49986ff":"code","3c6c83c5":"code","3c803764":"code","f756b64f":"code","491f5d2f":"code","fb1b6f27":"code","f0160e52":"code","949810e8":"code","d52a447a":"code","9aef0d6f":"code","c7f38b2f":"code","9dd4bd4a":"code","16c20bae":"code","c13458be":"code","7464401e":"code","82933aca":"code","7952d655":"code","c1d3b321":"code","abc9f73d":"code","8bba0dad":"code","a8796c76":"code","cb007e1a":"code","05e20008":"code","50134c53":"code","f8c45934":"code","c9ca6abc":"code","c7341ed3":"code","ae149bf6":"code","8e6e5949":"code","638b9060":"code","ca9576e0":"code","3ee0c6f6":"code","101fe7db":"code","13c8897c":"code","21caf6eb":"code","a093448b":"code","d16b7171":"code","648de04e":"code","cf7036ba":"code","2d514763":"code","c2b6a41d":"code","1621e48c":"code","1976cf7b":"code","21022cb3":"code","e5316fbc":"code","bd3a21a7":"code","1555cd11":"code","03c172be":"code","21c1593e":"code","4e807b98":"code","5fc840b7":"code","71681ff0":"code","ccd1bb3f":"code","16bc4885":"code","5ad7be22":"markdown","2ae54a0e":"markdown","e729aa84":"markdown","46418982":"markdown","d5b0d7f6":"markdown","f05d3642":"markdown","fe3ff1b7":"markdown","2fab9485":"markdown","50f19106":"markdown","129a5533":"markdown","c46d9f03":"markdown","27032a1b":"markdown","12b9865a":"markdown","80f582ab":"markdown","48df0ecc":"markdown","18c1d5f4":"markdown","ab9e9f64":"markdown","dbfb2fa9":"markdown","e1c3927d":"markdown","4820fed8":"markdown","919a55c2":"markdown","efa1ede3":"markdown","f19bcb2e":"markdown","6c4c555f":"markdown","a73eef88":"markdown","6bfd292a":"markdown","00148671":"markdown","69f834e4":"markdown","685dd50b":"markdown","5801584b":"markdown","eb90d59c":"markdown","a4ef7b1f":"markdown","21a24264":"markdown","ded0bf69":"markdown"},"source":{"cd3c7906":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e49986ff":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt \nimport seaborn as sns ","3c6c83c5":"X=pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv',index_col='id')\nprint(X.shape)\nX.head(1)","3c803764":"X.describe()# print the numerical field information discription","f756b64f":"sns.distplot(X.age)","491f5d2f":"X.dtypes","fb1b6f27":"X.isnull().sum()","f0160e52":"X.bmi[X.bmi.isnull()]=X.bmi.mean()#X.dropna()\nX.bmi.isnull().sum()","949810e8":"X.gender","d52a447a":"X.age=X.age.astype('int64')\nX.age.dtype\nX['gender'].replace({'Male':3,'Female':1,'Other':2},inplace=True)\nX['gender']=X['gender'].astype('uint8')\nX.gender.head(5)","9aef0d6f":"X.hypertension.head(5)","c7f38b2f":"X.hypertension=X.hypertension.astype('uint8')\nX.heart_disease=X.heart_disease.astype('uint8')","9dd4bd4a":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nX['smoking_status'] = label_encoder.fit_transform(X['smoking_status'])\nX.smoking_status=X.smoking_status.astype('uint8')\nX['smoking_status']","16c20bae":"X.ever_married[X.ever_married=='Yes']=1\nX.ever_married[X.ever_married=='No']=0\nX.ever_married=X.ever_married.astype('uint8')","c13458be":"X.Residence_type[X.Residence_type=='Rural']=1\nX.Residence_type[X.Residence_type=='Urban']=0\nX.Residence_type=X.Residence_type.astype('uint8')","7464401e":"print(X.work_type[X.work_type=='Govt_job'].count())#657 22 2925 819 687\nprint(X.work_type[X.work_type=='Never_worked'].count())\nprint(X.work_type[X.work_type=='Private'].count())\nprint(X.work_type[X.work_type=='Self-employed'].count())\nprint(X.work_type[X.work_type=='children'].count())","82933aca":"pd.get_dummies(X['work_type']).head(5)","7952d655":"#get_dummies has actually the same effect as compare to oneHoeEncoder\nX=pd.concat((X,pd.get_dummies(X['work_type'])),axis=1)\nX.drop('work_type',axis=1,inplace=True)\nX.head(1)\n\n#pd.DataFrame(OH_encoder.fit_transform(X[col_of_one])).shape\n#col_of_one=['hypertension','heart_disease','ever_married','work_type','Residence_type']\n# OH_encoder.categories_\n    #addX.concat(keep,axis=1)","c1d3b321":"X.shape,X.dtypes","abc9f73d":"plt.figure(figsize=(15,15))\nsns.heatmap(X.corr(),annot=True,cmap='coolwarm')","8bba0dad":"y=X['stroke']\nX.drop('stroke',axis=1,inplace=True)","a8796c76":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX=scaler.fit_transform(X)","cb007e1a":"from sklearn.model_selection import train_test_split\ntrain_x,val_x,train_y,val_y=train_test_split(X,y,test_size=0.25, random_state=1)","05e20008":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import metrics","50134c53":"X.shape[1]","f8c45934":"def model():\n    model=Sequential()\n    model.add(Dense(32,input_dim=(14),activation='relu'))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(8, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer ='adam', loss='binary_crossentropy',metrics=['binary_accuracy'])\n    return model","c9ca6abc":"model=model()\nmodel.summary()","c7341ed3":"history=model.fit(train_x,train_y,batch_size=128,epochs=20)#,validation_data=(val_x,val_y)","ae149bf6":"from sklearn.metrics import (classification_report, accuracy_score, precision_score, recall_score, f1_score,confusion_matrix)","8e6e5949":"from sklearn.metrics import f1_score\nmodely=model.predict_classes(val_x)\nf1 = f1_score(val_y,modely)\nprint(modely)\nf1","638b9060":"print(classification_report(val_y,modely))\nprint(confusion_matrix(val_y,modely))","ca9576e0":"from sklearn.ensemble import RandomForestClassifier#RandomForestClassifier","3ee0c6f6":"rf = RandomForestClassifier(random_state=42)\nrf.fit(train_x, train_y)\nrf_pred = rf.predict(val_x)\nprint(confusion_matrix(val_y, rf_pred))\nprint(classification_report(val_y,rf_pred ))","101fe7db":"from sklearn.linear_model import LogisticRegression","13c8897c":"lr=LogisticRegression(random_state=42)\nlr.fit(train_x, train_y)\ny_pred_lr = lr.predict(val_x)\nprint(confusion_matrix(val_y, rf_pred))\nprint(classification_report(val_y,rf_pred ))","21caf6eb":"history.history.keys()","a093448b":"# summarize history for accuracy\nplt.plot(history.history['loss'])\nplt.plot(history.history['binary_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['loss','binary_accuracy'], loc='upper left')\nplt.show()\n# summarize history for loss","d16b7171":"from imblearn.over_sampling import SMOTE\nsm = SMOTE()\nX_oversampled, y_oversampled = sm.fit_resample(X, y)\n\n#sns.countplot(x = y_oversampled, data = df)\n#plt.savefig('stroke_oversampled.png')","648de04e":"X_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, test_size = 0.2, random_state = 42)","cf7036ba":"history2=model.fit(X_train,y_train,batch_size=128,epochs=20)","2d514763":"plt.plot(history2.history['loss'])\nplt.plot(history2.history['binary_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['loss','binary_accuracy'], loc='upper left')\nplt.show()\n# summarize history for loss","c2b6a41d":"modely=model.predict_classes(X_test)\nprint(classification_report(y_test,modely))\nprint(confusion_matrix(y_test,modely))","1621e48c":"modely=model.predict_classes(val_x)\nprint(classification_report(val_y,modely))\nprint(confusion_matrix(val_y,modely))","1976cf7b":"from sklearn.neighbors import KNeighborsClassifier","21022cb3":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn_pred = knn.predict(X_test)\nprint(confusion_matrix(y_test,knn_pred ))\nprint(classification_report(y_test,knn_pred))","e5316fbc":"knn_pred = knn.predict(val_x)\nprint(confusion_matrix(val_y,knn_pred ))\nprint(classification_report(val_y,knn_pred))","bd3a21a7":"from sklearn.tree import DecisionTreeClassifier","1555cd11":"#Decision Tree\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ndt_pred = dt.predict(X_test)\nprint(confusion_matrix(y_test,dt_pred ))\nprint(classification_report(y_test, dt_pred))","03c172be":"dt_pred = dt.predict(val_x)\nprint(confusion_matrix(val_y,dt_pred ))\nprint(classification_report(val_y, dt_pred))","21c1593e":"from sklearn.ensemble import RandomForestClassifier#RandomForestClassifier","4e807b98":"rf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_train)\nprint(confusion_matrix(y_train, rf_pred))\nprint(classification_report(y_train,rf_pred ))","5fc840b7":"rf_pred = rf.predict(val_x)\nprint(confusion_matrix(val_y, rf_pred))\nprint(classification_report(val_y,rf_pred ))","71681ff0":"from sklearn.linear_model import LogisticRegression","ccd1bb3f":"lr=LogisticRegression(random_state=42)\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_train)\nprint(confusion_matrix(y_train, y_pred_lr))\nprint(classification_report(y_train,y_pred_lr ))","16bc4885":"y_pred_lr = lr.predict(val_x)\nprint(confusion_matrix(val_y, y_pred_lr))\nprint(classification_report(val_y,y_pred_lr ))","5ad7be22":"The result of oversampling data ","2ae54a0e":"**1. id**: unique identifier<br\/>\nThe id field is use to identify the record of the row, so we we'll get rid of this column because it don't any relation with probability of having stroke.<br\/>\n**2. gender** : \"Male\", \"Female\" or \"Other\"<br\/>\ngender has three type, we should set it's type to categorical data type,*nominal type*.<br\/>\n**3. age**: age of the patient<br\/>\nObviously, it's a numerical data type,*ratio type*.<br\/>\n**4. hypertension**: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension<br\/>\nIt's also typically *numerical data type*.<br\/>\n**5. heart_disease**: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease<br\/>\nThis is boolean data stucture. We set it to categorical data type,*nominal type*.<br\/>\n**6. ever_married**: \"No\" or \"Yes\"<br\/>\nIt's same,as well.Set to Categorical type.*Nominal type*.<br\/>\n**7. work_type**: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"<br\/>\nEach field has different meaning and not numerical data type, set all of the to *Nominal data type*.<br\/>\n**8. Residence_type**: \"Rural\" or \"Urban\"<br\/>\ntwo type, set to categorical data type,*nominal type*.<br\/>\n**9. avg_glucose_level**: average glucose level in blood<br\/>\n*Numerical data type*.<br\/>\n**10. bmi**: body mass index<br\/>\n*Numerical data type*.<br\/>\n**11. smoking_status**: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*<br\/>\nThe frequence of smoking status, it tell the extent of information on how a person smoking frequence. Set it to *Ordinal data type*.<br\/>\n**12. stroke**: 1 if the patient had a stroke or 0 if not<br\/>\nthe y result field.","e729aa84":"We tested regression, random forest, ann, KNeighbor and other methods, and also dealt with uneven data distribution and based on the results of the above algorithm.\n\nWe choose random forest classifier as our prediction model. Because it shows high scores on every metric of classification performance","46418982":"The accuracy is pretty high, but f1 score is totally a disaster! And it also means that our model will not work for the unhealthy people who are really in need for treatment!","d5b0d7f6":"We use ann, random forest, and regression models to make predictions","f05d3642":"The above answer of the result seem to predict all in the negetive answer to get the high preditoin, which is not a good strategy.","fe3ff1b7":"We use min max scaling to transform inot 1~0 in order to let computer compute well","2fab9485":"the above acuracy rate of the true positive rate(also known as recall,sensitivity) of the above model were all 0, which mean they have same prolem as the first model.","50f19106":"The result of oversampling data ","129a5533":"# Depolyment","c46d9f03":"**RandomForest model**","27032a1b":"**Regression model**","12b9865a":"For data pre-processing, we quantify the text data in the data set <br\/>\nFor example: gender, smoking, whether you are married, whether you live in a country or a city... etc. <br\/>\nWe present it digitally for subsequent processing. <br\/>","80f582ab":"The result of oversampling data ","48df0ecc":"The result of the using oversampling data is quite good. it shows 85% of true positive rate!\n<br>Thus, let's see how good is our new model predict on the original data.","18c1d5f4":"# **Understanding Data**","ab9e9f64":"According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. <BR>So, if we can know more information about stroke, we can use the information to prevent it. Let people to understand what kind of conditions will lead to high probability to get stroke.\n<br>\nwhy do we need to predict whether a person have stroke or not?<br>\n*because medical resources are limited, if we can prepare in advance, we can save more resouce, cost and time. ","dbfb2fa9":"fit data by oversampling data.","e1c3927d":"The result of original data predicted by the model built by oversampling data.","4820fed8":"So let's use it on the other model.","919a55c2":"We think it may be because the data set is unbalanced, so we use the method of SMOTE to balance the data set and get good results. <br\/>\nWe got this result in line with our original intention to predict the probability of stroke!","efa1ede3":"unfortunately,all the variable in the heatmap do not show some important message,because there is no variables that have high correlatinship with the stroke variable, and other correlationship that have a high value with other variable are all just common sense.Thus, we take all the feature as input data to build our model.","f19bcb2e":"# **Business Understanding**","6c4c555f":"So we need to balance our data set in order to repair our work.<br>\nwe will do oversampleing method to balance the dataset in the below section.","a73eef88":"The result of original data predicted by the model built by oversampling data. ","6bfd292a":"# Modeling","00148671":"The result of original data predicted by the model built by oversampling data.","69f834e4":"The result of original data predicted by the model built by oversampling data.","685dd50b":"In general, we would say if the correlation between two variables is bigger than **0.7**, it will then, have a **high correlationship**\uff1b<BR>\n    between **0.7 to 0.3**, will say it has **median correlations** and <BR>\n    if lower than **0.3**, we say that it has **low correlations**.","5801584b":"it also show big improvement on the tp rate result!","eb90d59c":"**ANN model**","a4ef7b1f":"The result of oversampling data ","21a24264":"# Evaluation","ded0bf69":"# **Data Preparation**"}}