{"cell_type":{"7b2956e4":"code","1b9655e6":"code","d66ad393":"code","f77f6ad1":"code","a5d8d787":"code","f42b5802":"code","a4c80271":"code","a1cd5ab0":"code","daf028e6":"code","6e823d26":"code","ca97936f":"code","0f44d1a4":"code","b6b71573":"code","95e46cf3":"code","a4917214":"code","9e8b0794":"code","249d96f3":"code","2a94da7e":"code","337a1cf9":"code","d42872d0":"code","46e27473":"code","2e300689":"code","582d810f":"code","d7214c48":"markdown","94364d59":"markdown","0f882ba3":"markdown","703b8c5a":"markdown","5b4dfea0":"markdown","1b0d4678":"markdown","388fc885":"markdown","52c65137":"markdown","868fbfa9":"markdown","55a8f9b3":"markdown","a323971a":"markdown","ba9594c7":"markdown","529cd161":"markdown","b00c3ed0":"markdown","be4953b0":"markdown"},"source":{"7b2956e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1b9655e6":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Flatten,Conv2D,MaxPool2D,Dropout\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","d66ad393":"train_df=pd.read_csv('\/kaggle\/input\/sign-language-mnist\/sign_mnist_train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/sign-language-mnist\/sign_mnist_test.csv')","f77f6ad1":"train_df.info()","a5d8d787":"test_df.info()","f42b5802":"train_df.describe()","a4c80271":"train_df.head(6)","a1cd5ab0":"train_label=train_df['label']\ntrain_label.head()\ntrainset=train_df.drop(['label'],axis=1)\ntrainset.head()","daf028e6":"X_train = trainset.values\nX_train = trainset.values.reshape(-1,28,28,1)\nprint(X_train.shape)","6e823d26":"test_label=test_df['label']\nX_test=test_df.drop(['label'],axis=1)\nprint(X_test.shape)\nX_test.head()","ca97936f":"from sklearn.preprocessing import LabelBinarizer\nlb=LabelBinarizer()\ny_train=lb.fit_transform(train_label)\ny_test=lb.fit_transform(test_label)","0f44d1a4":"y_train","b6b71573":"X_test=X_test.values.reshape(-1,28,28,1)","95e46cf3":"print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)","a4917214":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                  rotation_range = 0,\n                                  height_shift_range=0.2,\n                                  width_shift_range=0.2,\n                                  shear_range=0,\n                                  zoom_range=0.2,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest')\n\nX_test=X_test\/255","9e8b0794":"fig,axe=plt.subplots(2,2)\nfig.suptitle('Preview of dataset')\naxe[0,0].imshow(X_train[0].reshape(28,28),cmap='gray')\naxe[0,0].set_title('label: 3  letter: C')\naxe[0,1].imshow(X_train[1].reshape(28,28),cmap='gray')\naxe[0,1].set_title('label: 6  letter: F')\naxe[1,0].imshow(X_train[2].reshape(28,28),cmap='gray')\naxe[1,0].set_title('label: 2  letter: B')\naxe[1,1].imshow(X_train[4].reshape(28,28),cmap='gray')\naxe[1,1].set_title('label: 13  letter: M')","249d96f3":"sns.countplot(train_label)\nplt.title(\"Frequency of each label\")","2a94da7e":"model=Sequential()\nmodel.add(Conv2D(128,kernel_size=(5,5),\n                 strides=1,padding='same',activation='relu',input_shape=(28,28,1)))\nmodel.add(MaxPool2D(pool_size=(3,3),strides=2,padding='same'))\nmodel.add(Conv2D(64,kernel_size=(2,2),\n                strides=1,activation='relu',padding='same'))\nmodel.add(MaxPool2D((2,2),2,padding='same'))\nmodel.add(Conv2D(32,kernel_size=(2,2),\n                strides=1,activation='relu',padding='same'))\nmodel.add(MaxPool2D((2,2),2,padding='same'))\n          \nmodel.add(Flatten())","337a1cf9":"model.add(Dense(units=512,activation='relu'))\nmodel.add(Dropout(rate=0.25))\nmodel.add(Dense(units=24,activation='softmax'))\nmodel.summary()\n","d42872d0":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","46e27473":"model.fit(train_datagen.flow(X_train,y_train,batch_size=200),\n         epochs = 35,\n          validation_data=(X_test,y_test),\n          shuffle=1\n         )","2e300689":"(ls,acc)=model.evaluate(x=X_test,y=y_test)","582d810f":"print('MODEL ACCURACY = {}%'.format(acc*100))","d7214c48":"**Convolution layers**\n\nConv layer 1 -- UNITS - 128  KERNEL SIZE - 5 * 5   STRIDE LENGTH - 1   ACTIVATION - ReLu\n\nConv layer 2 -- UNITS - 64   KERNEL SIZE - 3 * 3   STRIDE LENGTH - 1   ACTIVATION - ReLu\n\nConv layer 3 -- UNITS - 32   KERNEL SIZE - 2 * 2   STRIDE LENGTH - 1   ACTIVATION - ReLu\n\n\n\n\nMaxPool layer 1 -- MAX POOL WINDOW - 3 * 3   STRIDE - 2\n\nMaxPool layer 2 -- MAX POOL WINDOW - 2 * 2   STRIDE - 2\n\nMaxPool layer 3 -- MAX POOL WINDOW - 2 * 2   STRIDE - 2","94364d59":"**Preview of the images in the training dataset**","0f882ba3":"# **Loading and Preprocessing the dataset**\nThe dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST handwritten digit dataset but otherwise similar with a header row of label, pixel1,pixel2\u2026.pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255. \n","703b8c5a":"Converting the dataframe to numpy array type to be used while training the CNN.\nThe array is converted from  1-D to 3-D which is the required input to the first layer of the CNN.\nSimilar preprocessing is done to the test dataframe.","5b4dfea0":"**Augmenting the image dataset to generate new data**\n\nImageDataGenerator package from keras.preprocessing.image allows to add different distortions to image dataset by providing random rotation, zoom in\/out , height or width scaling etc to images pixel by pixel.\n\nHere is the package details https:\/\/keras.io\/preprocessing\/image\/\n\nThe image dataset in also normalised here using the rescale parameter which divides each pixel by 255 such that the pixel values range between 0 to 1.","1b0d4678":"**Training the model**","388fc885":"# **Visualization of the Dataset**","52c65137":"**Converting the integer labels to binary form**\n\nThe label dataframe consist of single values from 1 to 24 for each individual picture. The CNN output layer will be of 24 nodes since it has 24 different labels as a multi label classifier. Hence each integer is encoded in a binary array of size 24 with the corresponding label being 1 and all other labels are 0. Such as if y=4 the the array is [0 0 0 1 0 0.....0].\nThe LabelBinarizer package from sklearn.preprocessing is used for that. The document link is https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelBinarizer.html","868fbfa9":"**Frequency plot of the labels**","55a8f9b3":"# **Building the CNN Model**\n\nThe model consist of :\n1. Three convolution layer each followed bt MaxPooling for better feature capture\n2. A dense layer of 512 units\n3. The output layer with 24 units for 24 different classes","a323971a":"#  **Importing Important Packages**","ba9594c7":"**Dense and output layers**","529cd161":"The train_df dataset consit of 1st column representing labels 1 to 24.\nThe label is loaded in a seperate dataframe called 'train_label' and the 'label' column is dropped from the original training dataframe which now consist of only 784 pixel values for each image.","b00c3ed0":"# **American Sign Language Recognition Using CNN**\n\nCommunication is an important part of our lives. Deaf and dumb people being unable to speak and listen, experience a lot of problems while communicating with normal people. There are many ways by which people with these disabilities try to communicate. One of the most prominent ways is the use of sign language, i.e. hand gestures. It is necessary to develop an application for recognizing gestures and actions of sign language so that deaf and dumb people can communicate easily with even those who don\u2019t understand sign language. The objective of this work is to take an elementary step in breaking the barrier in communication between the normal people and deaf and dumb people with the help of sign language.\n\nAmerican Sign Language (ASL) is a complete, natural language that has the same linguistic properties as spoken languages, with grammar that differs from English. ASL is expressed by movements of the hands and face. It is the primary language of many North Americans who are deaf and hard of hearing, and is used by many hearing people as well.\n![![NIDCD-ASL-hands-2019.jpg](attachment:NIDCD-ASL-hands-2019.jpg)]\n\n","be4953b0":"**Evaluating the model**"}}