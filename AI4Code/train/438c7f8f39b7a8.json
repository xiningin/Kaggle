{"cell_type":{"c901399c":"code","fdf4b0d7":"code","fc8bf483":"code","c9b3de03":"code","6f9901ba":"code","3bb122e8":"code","0a41453c":"code","e498a6dc":"markdown","42779fb7":"markdown","22da2760":"markdown","550655cb":"markdown","f342be7c":"markdown","95e01087":"markdown","eb6a0bfe":"markdown","25cc7ff1":"markdown"},"source":{"c901399c":"import sklearn\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score\nfrom sklearn.datasets import load_iris\nimport xgboost\n","fdf4b0d7":"random_state = 69\ndataset = load_iris()\nx = dataset.data\ny = dataset.target \nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=random_state)","fc8bf483":"dmatrix_train = xgboost.DMatrix(x_train, label=y_train)\ndmatrix_test = xgboost.DMatrix(x_test, label=y_test)","c9b3de03":"param = { 'max_depth': 3, 'eta': 0.8, 'silent': 1, 'objective': 'multi:softprob', 'num_class': 3} \nnum_round = 20","6f9901ba":"xgb_train = xgboost.train(param,dmatrix_train,num_round)\npredictions = xgb_train.predict(dmatrix_test)\n","3bb122e8":"best_predictions = np.asarray( [ np.argmax(linha) for linha in predictions])\nbest_predictions","0a41453c":"precision_score(y_test, best_predictions, average='macro')","e498a6dc":"**Para o XGBoost funcionar como voc\u00ea realmente deseja, \u00e9 preciso definar alguns par\u00e2metros.**\n\n*  **max_depth** Profundidade m\u00e1xima de cada \u00e1rvore.\n*  **eta** Etapa de treinamento para cada itera\u00e7\u00e3o.\n* **silent** Modo silencioso.\n* **objective** Avalia\u00e7\u00e3o de erro para treinamento multiclasse e etc.\n* **num_class** N\u00famero de classes que existem neste conjunto de dados.* ('setosa', 'versicolor', 'virginica')*\n* **num_round** N\u00famero de itera\u00e7\u00f5es de treinamento.","42779fb7":"# Setup","22da2760":"Aqui temos as tr\u00eas classes** 0,1,2**. Para cada linha, voc\u00ea precisar escolher as com o maior n\u00edvel de probabilidade.","550655cb":"**Train** e *predict...*","f342be7c":"Criando DMatrix, ela trabalha diretamente com matrizes do numpy","95e01087":"# XGBoost - Extreme Gradient Boosting - Estudo Base\nO XGBoost \u00e9 uma implementa\u00e7\u00e3o de \u00e1rvores de decis\u00e3o, projetadas para ganho de perfomance no aprendizado de m\u00e1quina.\n\nEsse pacote foi desenvolvido inicialmente pela [Tianq Chen](https:\/\/scholar.google.com.br\/scholar?q=Tianqi+Chen+DMLC&hl=pt-BR&as_sdt=0&as_vis=1&oi=scholart) como parte da DMLC (*Distributed Machine Learning Community*), e tem como objetivo ser extremamente r\u00e1pido, escal\u00e1vel e port\u00e1til.  ","eb6a0bfe":"**Verificando a precis\u00e3o das previs\u00f5es feitas:**","25cc7ff1":"**Refer\u00eancias**\n* https:\/\/www.kaggle.com\/rozester\/xgboost-example-python\n* https:\/\/www.kdnuggets.com\/2017\/03\/simple-xgboost-tutorial-iris-dataset.html\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_iris.html\n* https:\/\/www.datacamp.com\/community\/tutorials\/xgboost-in-python\n* http:\/\/suruchifialoke.com\/2016-10-13-machine-learning-tutorial-iris-classification\/"}}