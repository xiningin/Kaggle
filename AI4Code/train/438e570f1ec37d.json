{"cell_type":{"19739d78":"code","8eef7fe2":"code","1a9df98d":"code","005f9a90":"code","273fcc0f":"code","4f52e9a0":"code","0c609fdf":"code","a3095099":"code","db93ccdc":"code","31abdc41":"code","dca4a0b8":"code","6735b0ff":"code","1e689210":"code","c95ea588":"code","ba794a77":"code","10b5062b":"code","d527c1c9":"code","a9b12d5d":"code","6b59798e":"code","8d287b1b":"code","8ebe5718":"code","dca51372":"code","35e2af7b":"code","d388cc7a":"code","e83fa3ef":"code","949a81ee":"code","01f3ea6e":"code","c68d3e06":"code","9040ec48":"code","7601d6fb":"code","3603367d":"markdown","c0dba923":"markdown","9d4954e5":"markdown","2fc07f9f":"markdown","aa108cd3":"markdown","bf1486f6":"markdown","1585fa60":"markdown","9470beac":"markdown","895d6a68":"markdown","1a09035d":"markdown","e41a1292":"markdown","5aeda66d":"markdown","96ed8a46":"markdown","208875b3":"markdown","5809aecb":"markdown","685137cf":"markdown","40e69afc":"markdown","22694a5f":"markdown","01eb3af7":"markdown"},"source":{"19739d78":"import os\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import kurtosis, skew\nfrom matplotlib.offsetbox import AnchoredText\nimport random\nfrom matplotlib.ticker import MaxNLocator\nimport pylab as p\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8eef7fe2":"train_ = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv', index_col='id')\ntest = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv', index_col='id')\n\nsubmission= pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-feb-2021\/sample_submission.csv', index_col='id')","1a9df98d":"train = train_.copy()","005f9a90":"print('Train data of shape {}'.format(train.shape))\ndisplay(train.head())\nprint('Test data of shape {}'.format(test.shape))\ndisplay(test.head())","273fcc0f":"display(train.describe().T)","4f52e9a0":"target = train.pop('target')","0c609fdf":"cat_features =[]\nnum_features =[]\n\nfor col in train.columns:\n    if train[col].dtype=='object':\n        cat_features.append(col)\n    else:\n        num_features.append(col)\nprint('Catagoric features: ', cat_features)\nprint('Numerical features: ', num_features)","a3095099":"print('Number of NA values in train data is {}'.format(train.isna().sum().sum()))\nprint('Number of NA values in test data is {}'.format(test.isna().sum().sum()))","db93ccdc":"for col in cat_features:\n    print('{} unique values in {}'.format(train[col].nunique(), col))","31abdc41":"# train_data\nunique_cat_train = []\nfor col in train[cat_features]:\n    unique_train = train[col].nunique()  \n    dict1 ={\n        'Features' : col,\n        'Unique cats (train)': unique_train,        \n    }\n    unique_cat_train.append(dict1)\nDF1 = pd.DataFrame(unique_cat_train, index=None).sort_values(by='Unique cats (train)',ascending=False)\n\n# test_data\nunique_cat_test = []\nfor col in test[cat_features]:\n    unique_test = test[col].nunique()    \n    dict2 ={\n        'Features' : col,\n        'Unique cats (test)': unique_test,        \n    }\n    unique_cat_test.append(dict2)\nDF2 = pd.DataFrame(unique_cat_test, index=None).sort_values(by='Unique cats (test)',ascending=False)\n\npd.merge(DF1, DF2, how='outer', on=['Features']).style.format(None, na_rep=\"-\")","dca4a0b8":"def count_plot_pct(data, features, titleText):\n    i = 1\n    plt.figure()\n    fig, ax = plt.subplots(5, 2,figsize=(18, 22))\n    fig.subplots_adjust(top=0.95)\n    for feature in features:\n        total = float(len(data)) \n        plt.subplot(5, 2, i)\n        ax = sns.countplot(x=feature, palette='coolwarm', data=data)        \n        ylabels = ['{:.0f}'.format(x) + 'K' for x in ax.get_yticks()\/1000]\n        ax.set_yticklabels(ylabels)\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x()+p.get_width()\/2.,height + 3,'{:1.2f} %'.format((height\/total)*100),ha=\"center\")\n        i += 1\n    plt.suptitle(titleText ,fontsize = 24)\n    plt.show()    \n    ","6735b0ff":"count_plot_pct(train, cat_features, 'Train data Categorical features: distribution of each category')","1e689210":"count_plot_pct(test, cat_features, 'Test data Categorical features: percentage of each category')","c95ea588":"# Install additional data-visualization library\n!pip install ptitprince","ba794a77":"import ptitprince as pt\ndef raincloud_plot(data, features, titleText='Title'):    \n    i = 1\n    plt.figure()\n    fig, ax = plt.subplots(5, 2,figsize=(18, 28))\n    fig.subplots_adjust(top=0.95)\n    for feature in features:\n        plt.subplot(5, 2, i)\n        ax = pt.RainCloud(x = data[feature], y = target, \n                  data = train, \n                  width_viol = 0.8,\n                  width_box = 0.4,\n                  orient = 'h',\n                  move = 0.0)\n        i += 1\n    plt.suptitle(titleText ,fontsize=24)\n    plt.show()","10b5062b":"raincloud_plot(train, cat_features, 'Raincloud plot: train data, categorical features')","d527c1c9":"def sns_box_plot1(data, features, titleText='Title'):\n    i = 1\n    \n    L = len(num_features)\n    nrow= int(np.ceil(L\/3))\n    ncol= 3\n    \n    remove_last= (nrow * ncol) - L\n    \n    fig, ax = plt.subplots(nrow, ncol,figsize=(18, 22))\n    ax.flat[-remove_last].set_visible(False)\n    fig.subplots_adjust(top=0.95) \n    \n    for feature in features:\n        plt.subplot(5, 3, i)\n        ax = sns.boxplot(x=data[feature], palette='coolwarm')\n        ax = sns.violinplot(x=data[feature], inner=None, palette='viridis')\n        plt.xlabel(feature, fontsize=10)\n        i += 1\n    plt.suptitle(titleText, fontsize=24,)\n    plt.show()","a9b12d5d":"sns_box_plot1(train, num_features, 'Box+violin plots: train data, numerical features')","6b59798e":"plt.figure()\nfig, ax = plt.subplots(4, 3,figsize=(20, 20))\nfig.subplots_adjust(top=0.95)\ni = 1\nfor feature in num_features:\n    plt.subplot(5, 3, i)\n    ax = sns.histplot(train[feature],color=\"cyan\", kde=True,bins=120, label='train')\n    ax = sns.histplot(test[feature], color=\"red\", kde=True,bins=120, label='test')\n    ylabels = ['{:.0f}'.format(x) + 'K' for x in ax.get_yticks()\/1000]\n    ax.set_yticklabels(ylabels)\n    plt.xlabel(feature, fontsize=9)\n    plt.legend()\n    i += 1\nplt.suptitle('Histogram of numerical features', fontsize=20)\nplt.show()","8d287b1b":"plt.figure()\nfig, ax = plt.subplots(4, 3,figsize=(20, 20))\nfig.subplots_adjust(top=0.95)\ni = 1\nfor feature in num_features:\n    plt.subplot(5, 3, i)\n    sns.scatterplot(data= train, x=train[feature], y=target, color=\"blue\", label='train')\n    plt.xlabel(feature, fontsize=9);\n    plt.legend()\n    i += 1\nplt.suptitle('Scatter plot of numerical features', fontsize=20)\nplt.show()","8ebe5718":"plt.figure(figsize=(12, 8))\nax = sns.kdeplot(target, shade=True, color='black', edgecolor='red', alpha=0.85)\nplt.title('Target Distribution', fontsize=20)","dca51372":"kurt = []\nske = []\nfor cont in num_features:\n    x = train[cont]     \n    kurt.append(kurtosis(x))\n    ske.append(skew(x))\n    \nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111)\nax.plot(num_features, kurt, '*', markersize = 15, color= 'red', label=\"kurtosis\")\nax.plot(num_features, ske, 'o', markersize = 15, color='blue',label=\"skewness\" )\nax.hlines(y=0, xmin=0, xmax=13, colors='black', linestyles='solid', label='Normal-dist')\nax.set_xlabel('Numerical features', fontsize=12)\nax.set_ylabel('Skewness & kurtosis', fontsize=12)\nax.set_title('Skewness and kurtosis of the numerical features', fontsize=20)\nax.grid()\nax.legend()\nplt.show()","35e2af7b":"corr = train_.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(14, 10))\ncmap = sns.diverging_palette(230, 0, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, vmin=-.1, center=0, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": 0.75})","d388cc7a":"corr = pd.concat((train_[cat_features], target), axis=1).apply(lambda x : pd.factorize(x)[0]).corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(12, 10))\ncmap = sns.diverging_palette(230, 0, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, vmin=-.1, center=0, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": 0.75})","e83fa3ef":"def cont_feature_engg_plot(cont, titleText, data=train):    \n    L = len(num_features)\n    nrow= int(np.ceil(L\/3))\n    ncol= 3\n    \n    remove_last= (nrow * ncol) - L\n    \n    fig, ax = plt.subplots(nrow, ncol,figsize=(18, 22))\n    ax.flat[-remove_last].set_visible(False)\n    fig.subplots_adjust(top=0.92)\n    i = 1\n    for feature in num_features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.kdeplot((data[cont] + data[feature]), shade=True, color='#D5DBDB', edgecolor='black',\n                         alpha=0.9, label= str(cont) +'+'+ str(feature))\n        \n        # target is scaled only for plotting purpose. We are interested in the shape of the \n        # engineered feature in relation to the target \n        ax = sns.kdeplot(target\/7, shade=True, color='red', edgecolor='black', label= 'target')\n        ax.set_xlabel(None)\n                \n        # correlation value\n        correlation = np.round(np.array(train[[cont, feature]].corr())[0][1], 3)\n        \n        # Anchored text for the correlation value between the two features\n        at = AnchoredText(correlation,\n                  prop=dict(size=15), frameon=True, loc='upper left')\n        at.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\n        ax.add_artist(at)\n        ax.legend()\n     \n        i += 1\n    plt.suptitle(titleText ,fontsize = 20)\n    plt.show()    ","949a81ee":"cont_feature_engg_plot('cont0', titleText='Shape (kde-plot) of engineered features: cont{0} + cont{x}'\n                       '\\n(comparision with target)\\n[upper left corner: corr b\/n feats]', data=train)","01f3ea6e":"# for cont in num_features[1:]:    \n#     cont_feature_engg_plot(cont, titleText='Shape (kde-plot) of engineered features:' + cont + ' + cont{x}'\n#                        '\\n(comparision with target)\\n[upper left corner: corr b\/n feats]', data=train)\n    ","c68d3e06":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nle_train = train.copy()\nle_test = test.copy()\n\nfor col in cat_features:\n    le_train[col] = le.fit_transform(train[col])\n    le_test[col] = le.transform(test[col])\ntrain = le_train\ntest = le_test","9040ec48":"# the following two code snippets are adapted from the \"feature engineering kaggle min-course\"\n\nfrom sklearn.feature_selection import mutual_info_regression\nfeatures = train.dtypes == int\n\ndef make_mi_scores(train, y, discrete_features):\n    mi_scores = mutual_info_regression(train, target, discrete_features=features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=train.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(train, target, features)\nmi_scores","7601d6fb":"def plot_utility_scores(scores):\n    y = scores.sort_values(ascending=True)\n    width = np.arange(len(y))\n    ticks = list(y.index)\n    plt.barh(width, y, color='red', alpha=0.3)\n    plt.yticks(width, ticks)\n    #plt.grid()\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_utility_scores(mi_scores)","3603367d":"## Unique values in categorical features (train vs test)\n\n<div class=\"alert alert-block alert-danger\">  \n>>> Cat6 in train data has one more catagory than test data!!\n<\/div>\n","c0dba923":"### Thank you very much for reading this notebook!","9d4954e5":"## No null-values in the data","2fc07f9f":"## Uncomment and run the cell below to plot the rest of the features (cont{x} + cont{y})","aa108cd3":"\n# Tabular Playground Series: Feb 2021\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25225\/logos\/header.png?t=2021-01-27-17-34-26)\n\n## Introduction:\n\nStarting from January this year, the kaggle competition team is offering a month-long tabulary playground competitions. This series aims to bridge between inclass competition and featured competitions with a friendly and approachable datasets.\n\nFor this month, kaggle is offering a dataset which is synthetic but based on a real dataset and generated using a CTGAN. The original dataset, this synthetic dataset is derived from, deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\n\nThe data has: \n\n* 10 categorical variables: **cat0** to **cat9**\n* 14 continuous variables: **cont0** to **cont13**\n* 1 numericat **target** column\n\nFiles provides:\n\n- train.csv - the training data with the target column\n- test.csv - the test set; you will be predicting the target for each row in this file\n- sample_submission.csv - a sample submission file in the correct format\n\nThe goal of the competition is to predict a continuous **target** based on the given categorical and continuous features. However, the goal of **this notebook** is to explore (EDA) and visualize the given data. And when possible try to discover (engineer) *potentially usefull* features for further data modelling and prediction.","bf1486f6":"# Set-up","1585fa60":"### Test data categorical features","9470beac":"# Feature engineering: numerical features\n(See a sample plot of engineered features)","895d6a68":"# Explore the data","1a09035d":"## Categorical features\n","e41a1292":"# Skewness & Kurtosis","5aeda66d":"# Target variable distribution","96ed8a46":"## Continuous feature cont{0} added to cont{x}\n(How does the resulting kde-shape look like compared to target?)\n","208875b3":"# Load the data","5809aecb":"## Numerical features \n","685137cf":"### Train data categorical features","40e69afc":"## Mutual information regression\n\nAs a beginner, Kaggle learn is a great source of information and knowledge for me. In the course [Feature Engineering](https:\/\/www.kaggle.com\/learn\/feature-engineering), the description of mutual information reads like this: \n> Mutual information describes relationships in terms of uncertainty. The mutual information (MI) between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If you knew the value of a feature, how much more confident would you be about the target?\n\nSo from the MI score knowing the values of cont8, cat1, cont0 and cat9 will increase confidence level of your target value associated with these features.   ","22694a5f":"# Correlation matrix","01eb3af7":"# Data Visualization"}}