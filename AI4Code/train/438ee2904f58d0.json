{"cell_type":{"a1fc4ca5":"code","d88c46fa":"code","4d4face6":"code","9c2e3180":"code","3dabde06":"code","85bd8b5e":"code","bafc007d":"code","c280af60":"code","9dc6091b":"code","f031e79a":"code","f7d455f3":"code","7bb3e8e2":"code","11fef796":"code","d0aa2921":"code","a54d11ee":"code","b3cd0177":"code","072943ef":"code","0f08371b":"code","1fd4c8a1":"code","a860f579":"code","f6e4a549":"code","856faa18":"code","6ffdf721":"code","258036c6":"code","54b950dd":"code","57dde39f":"code","6da862c1":"code","a9d94fe4":"code","202051e0":"code","46aef4d7":"code","92a786e5":"code","a753b9d2":"code","c3020184":"code","f4f40ee9":"code","18f1001b":"code","656499e4":"code","cbcf0b73":"code","ca6c6d99":"code","fcdcadac":"code","fbef1b38":"code","038767be":"code","7712683c":"code","246f6b0e":"code","3cd0d8b5":"code","9d855a7a":"code","5271e57f":"code","f4c8475c":"code","f8f6d7bb":"code","28aa4ad7":"code","4adb7dd6":"code","dcbe1d66":"code","387b9e5b":"code","a2750371":"code","8f1333d6":"code","a08e348b":"code","8647fd56":"code","d6d36a4b":"code","2f39c942":"code","e23987c2":"code","6e44e770":"code","2edbdcc3":"code","65596e2f":"code","44b6c92e":"code","823fe68a":"code","3ff6ece4":"code","87fedeeb":"code","247de9ed":"code","a6c95e29":"code","7361e808":"code","eff61d40":"code","f9c99a8f":"code","3b0de780":"code","2dbb7a27":"code","930acb27":"code","5db787de":"code","15d0a7d4":"code","1436a7d7":"code","ef6ac90d":"code","5be7df52":"code","7891a0a5":"code","b1d22c63":"code","eaf668a9":"code","95cf9389":"code","82ae2d41":"code","7805daab":"code","9a16aebc":"code","ece31f4c":"code","a1ff298d":"code","f519f822":"code","9823f26a":"code","a7f040dc":"code","1c42e407":"code","a1eaa1e9":"code","b4734808":"code","d0d70042":"code","05d0b541":"code","22696359":"code","9a1e533c":"code","17c526e2":"markdown","fe81eb2d":"markdown","1144010e":"markdown","02970ffd":"markdown","6447bc88":"markdown","7656e2d3":"markdown","1ce2e728":"markdown","d8c827df":"markdown","c01c798d":"markdown","b03b020b":"markdown","3beea883":"markdown","adf03661":"markdown","74173220":"markdown","b258c4e2":"markdown","4ddf123c":"markdown","1258f50a":"markdown","bad1279e":"markdown","c109e6ea":"markdown","ec6b1790":"markdown","c7acb790":"markdown","e3afbac0":"markdown","3eae2d9b":"markdown","95e6b6ff":"markdown","cf94a061":"markdown","8b373712":"markdown","2cf084a9":"markdown","bc782e94":"markdown","0b6d435a":"markdown","258f4765":"markdown","267be134":"markdown","3b36eda2":"markdown","5113128a":"markdown","0bae143c":"markdown","66e61df8":"markdown","cf038675":"markdown","146aabd6":"markdown","eb8bdde1":"markdown","7663d39c":"markdown"},"source":{"a1fc4ca5":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport nltk\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score, roc_auc_score\nfrom sklearn.pipeline import make_pipeline\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")","d88c46fa":"train  = pd.read_csv('..\/input\/tweeter-hate-speech-sentiment-analysis\/train_tweets.csv')\ntest = pd.read_csv('..\/input\/tweeter-hate-speech-sentiment-analysis\/test_tweets.csv')","4d4face6":"train.head(5)","9c2e3180":"test.head(5)","3dabde06":"train.info()","85bd8b5e":"test.info()","bafc007d":"train.shape\n","c280af60":"test.shape\n","9dc6091b":"train['tweet'].isna().sum()","f031e79a":"train['label'].value_counts() #count number of  positive and negative  tweet","f7d455f3":"train['length'] = train['tweet'].apply(len)\nfig1 = sns.barplot('label','length',data = train,palette='PRGn')\nplt.title('Average Word Length vs label')\nplot = fig1.get_figure()\nplot.savefig('Barplot.png')","7bb3e8e2":"# Count positive negative label\nfig2 = sns.countplot(x= 'label',data = train)\nplt.title('Label Counts')\nplot = fig2.get_figure()\nplot.savefig('Count Plot.png')","11fef796":"#count of words disrtibution\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\ntrain['length'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='word count',\n    linecolor='black',\n    yTitle='count',\n    title='Review Text Word Count Distribution')","d0aa2921":"def vectorization(table):\n    #CountVectorizer will convert a collection of text documents to a matrix of token counts\n    #Produces a sparse representation of the counts \n    #Initialize\n    vector = CountVectorizer()\n    #We fit and transform the vector created\n    frequency_matrix = vector.fit_transform(table.tweet)\n    #Sum all the frequencies for each word\n    sum_frequencies = np.sum(frequency_matrix, axis=0)\n    #Now we use squeeze to remove single-dimensional entries from the shape of an array that we got from applying np.asarray to\n    #the sum of frequencies.\n    frequency = np.squeeze(np.asarray(sum_frequencies))\n    #Now we get into a dataframe all the frequencies and the words that they correspond to\n    frequency_df = pd.DataFrame([frequency], columns=vector.get_feature_names()).transpose()\n    return frequency_df","a54d11ee":"def graph(word_frequency, sent):\n    labels = word_frequency[0][1:51].index\n    title = \"Word Frequency for %s\" %sent\n    #Plot the figures\n    plt.figure(figsize=(10,5))\n    plt.bar(np.arange(50), word_frequency[0][1:51], width = 0.8, color = sns.color_palette(\"bwr\"), alpha=0.5, \n            edgecolor = \"black\", capsize=8, linewidth=1);\n    plt.xticks(np.arange(50), labels, rotation=90, size=14);\n    plt.xlabel(\"50 more frequent words\", size=14);\n    plt.ylabel(\"Frequency\", size=14);\n    #plt.title('Word Frequency for %s', size=18) %sent;\n    plt.title(title, size=18)\n    plt.grid(False);\n    plt.gca().spines[\"top\"].set_visible(False);\n    plt.gca().spines[\"right\"].set_visible(False);\n    plt.show()","b3cd0177":"word_frequency = vectorization(train).sort_values(0, ascending = False)\n#word_frequency_pos = vectorization(training_data[training_data['label'] == '1']).sort_values(0, ascending = False)\n#word_frequency_neg = vectorization(training_data[training_data['label'] == '1']).sort_values(0, ascending = False)\n\n    #Graph with frequency words all, positive and negative tweets and get the frequency\ngraph(word_frequency, 'all')\n#graph(word_frequency_pos, 'positive')\n#graph(word_frequency_neg, 'negative')","072943ef":"word_frequency_pos = vectorization(train[train['label'] == 0]).sort_values(0, ascending = False)\nword_frequency_neg = vectorization(train[train['label'] == 1]).sort_values(0, ascending = False)\n\ngraph(word_frequency_pos, 'positive')\ngraph(word_frequency_neg, 'negative')","0f08371b":"def regression_graph(table):\n    table = table[1:]\n    #We set the style of seaborn\n    sns.set_style(\"whitegrid\")   \n    #Initialize the figure\n    plt.figure(figsize=(6,6))\n    \n    #we obtain the points from matplotlib scatter\n    points = plt.scatter(table[\"Positive\"], table[\"Negative\"], c=table[\"Positive\"], s=75, cmap=\"bwr\")\n    #graph the colorbar\n    plt.colorbar(points)\n    #we graph the regplot from seaborn\n    sns.regplot(x=\"Positive\", y=\"Negative\",fit_reg=False, scatter=False, color=\".1\", data=table)\n    plt.xlabel(\"Frequency for Positive Tweets\", size=14)\n    plt.ylabel(\"Frequency for Negative Tweets\", size=14)\n    plt.title(\"Word frequency in Positive vs. Negative Tweets\", size=14)\n    plt.grid(False)\n    sns.despine()","1fd4c8a1":"table_regression = pd.concat([word_frequency_pos, word_frequency_neg], axis=1, sort=False)\ntable_regression.columns = [\"Positive\", \"Negative\"]\nregression_graph(table_regression)","a860f579":"#function to drop unwanted features\n\ndef drop_features(features,data):\n    data.drop(features,inplace=True,axis=1)","f6e4a549":"train.shape, test.shape","856faa18":"train.drop('id',inplace = True, axis = 1)\ntrain","6ffdf721":"test.drop('id',inplace = True, axis = 1)\ntest","258036c6":"from textblob import TextBlob\ndef getSubjectivity(tweet) :\n    return TextBlob(tweet).sentiment.subjectivity\ndef getPolarity(tweet) :\n    return TextBlob(tweet).sentiment.polarity\ndef getAnalysis(score) :\n    return 'Neutral' if (score == 0) else ('Negative' if (score < 0) else 'Positive')","54b950dd":"train['Subjectivity'] = train['tweet'].apply(getSubjectivity)\ntrain['Polarity'] = train['tweet'].apply(getPolarity)\ntrain['Analysis'] = train['Polarity'].apply(getAnalysis)\n\ntrain","57dde39f":"import re\ndef process_tweet(tweet):\n    return \" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",tweet.lower()).split())","6da862c1":"train['processed_tweets'] = train['tweet'].apply(process_tweet)","a9d94fe4":"train.head(5)","202051e0":"normal_words = ' '.join([word for word in train['processed_tweets'][train['label'] == 0]])\nwordcloud = WordCloud(width = 800, height = 500, max_font_size = 110).generate(normal_words)\nprint('Normal words')\nplt.figure(figsize= (12,8))\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","46aef4d7":"negative_words = ' '.join([word for word in train['processed_tweets'][train['label'] == 1]])\nwordcloud = WordCloud(width = 800, height = 500, max_font_size = 110).generate(negative_words)\nprint('Negative words')\nplt.figure(figsize= (12,8))\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","92a786e5":"train.info()","a753b9d2":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train[\"processed_tweets\"], train[\"label\"], test_size = 0.2, random_state = 42)","c3020184":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\ncount_vect = CountVectorizer(stop_words='english')\ntransformer = TfidfTransformer(norm='l2',sublinear_tf=True)","f4f40ee9":"x_train_counts = count_vect.fit_transform(x_train)\nx_train_tfidf = transformer.fit_transform(x_train_counts)","18f1001b":"print(x_train_counts.shape)\nprint(x_train_tfidf.shape)","656499e4":"x_train_counts","cbcf0b73":"x_test_counts = count_vect.transform(x_test)\nx_test_tfidf = transformer.transform(x_test_counts)","ca6c6d99":"print(x_test_counts.shape)\nprint(x_test_tfidf.shape)","fcdcadac":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(x_train_tfidf,y_train)","fbef1b38":"predictions = model.predict(x_test_tfidf)","038767be":"from sklearn.metrics import confusion_matrix,f1_score\nfrom sklearn import metrics\ncm = confusion_matrix(y_test,predictions)","7712683c":"print(cm)","246f6b0e":"print(metrics.classification_report(y_test,predictions))","3cd0d8b5":"rf_f1=f1_score(y_test,predictions)\nrf_f1","9d855a7a":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, predictions)","5271e57f":"from sklearn.metrics import precision_score,recall_score,f1_score\nf1_score(y_test,predictions)","f4c8475c":"precision_score(y_test,predictions)","f8f6d7bb":"recall_score(y_test, predictions)","28aa4ad7":"# jaccard_index\nfrom sklearn.metrics import jaccard_score\njaccard_score(predictions,y_test)","4adb7dd6":"# Log Loss\nfrom sklearn.metrics import log_loss\nlog_loss(y_test,predictions)","dcbe1d66":"# ROC_AUC_Score\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,predictions)","387b9e5b":"from sklearn import svm\nlin_clf = svm.LinearSVC()\nlin_clf.fit(x_train_tfidf,y_train)","a2750371":"predict_svm = lin_clf.predict(x_test_tfidf)","8f1333d6":"from sklearn.metrics import confusion_matrix,f1_score\nconfusion_matrix(y_test,predict_svm)","a08e348b":"svm_f1=f1_score(y_test,predict_svm)\nsvm_f1","8647fd56":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(train[\"tweet\"], train[\"label\"], test_size = 0.20, random_state = 42)","d6d36a4b":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\ncount_vect = CountVectorizer(stop_words='english')\ntransformer = TfidfTransformer(sublinear_tf=True)\n\nX_train_cnt = count_vect.fit_transform(X_train)\nX_train_TF = transformer.fit_transform(X_train_cnt)\nprint(X_train_cnt.shape)\nprint(X_train_TF.shape)\n\nX_test_cnt = count_vect.transform(X_test)\nX_test_TF = transformer.transform(X_test_cnt)\nprint(X_test_cnt.shape)\nprint(X_test_TF.shape)","2f39c942":"from sklearn.ensemble import RandomForestClassifier as RFClass\nranForModel = RFClass(n_estimators=10, criterion = \"entropy\")\nranForModel.fit(X_train_TF, Y_train)","e23987c2":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report as class_re\nfrom sklearn.metrics import confusion_matrix as c_m\nranForPredict = ranForModel.predict(X_test_TF)\nprint(\"Predicted Class:\",ranForPredict)\nprint(\"Confusion Matrix:\\n\",c_m(Y_test ,ranForPredict))\nprint(\"Accuracy:\", accuracy_score(Y_test ,ranForPredict))\nprint(\"F_score:\", f1_score(Y_test ,ranForPredict))\nprint(\"Classification Report:\\n\",class_re(Y_test ,ranForPredict))","6e44e770":"from wordcloud import WordCloud as WC\nWordsInAllTweets = ''.join([words for words in train['tweet']])\nwordcloud = WC(width = 1000, height = 600, random_state = 42, max_font_size = 120).generate(WordsInAllTweets)\n\n\npos_train = train[train['Analysis'] == \"Positive\"]\nWordsInPosTweets = ''.join([words for words in pos_train['tweet']])\npos_wordcloud = WC(width = 1000, height = 600, random_state = 42, max_font_size = 120).generate(WordsInPosTweets)\n\nneg_train = train[train['Analysis'] == \"Negative\"]\nWordsInNegTweets = ''.join([words for words in neg_train['tweet']])\nneg_wordcloud = WC(width = 1000, height = 600, random_state = 42, max_font_size = 120).generate(WordsInNegTweets)\n\nneu_train = train[train['Analysis'] == \"Neutral\"]\nWordsInNeuTweets = ''.join([words for words in neu_train['tweet']])\nneu_wordcloud = WC(width = 1000, height = 600, random_state = 42, max_font_size = 120).generate(WordsInNeuTweets)","2edbdcc3":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score as acc_score\nfrom sklearn.metrics import classification_report as class_re\nfrom sklearn.preprocessing import StandardScaler\n\ntrain.drop('tweet',inplace = True, axis = 1)\ntrain.Analysis = train.Analysis.map({\"Neutral\":0, \"Negative\":-1, \"Positive\":+1})\ncol_names = [\"Subjectivity\", \"Polarity\", \"Analysis\"]\ntarget_name = [\"label\"]\n\n\nX = train[col_names]\nX_std_scal = StandardScaler().fit_transform(X)\nY = train[target_name]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_std_scal, Y, test_size = 0.20, random_state = 42)\n\nDTreeClass = DecisionTreeClassifier(criterion = \"entropy\", random_state = 42, max_depth = 7)\nDTreeClass.fit(X_train, Y_train)\nY_pred = DTreeClass.predict(X_test)\nY_scored = DTreeClass.score(X,Y)\nY_scored2 = DTreeClass.score(X_train,Y_train)\nY_scored3 = DTreeClass.score(X_test,Y_test)\n\n\nprint(\"Classification Report:\\n\",class_re(Y_test, Y_pred))\nprint(\"Confusion Matrix:\\n\",c_m(Y_test ,Y_pred))\nprint(\"F_score:\", f1_score(Y_test ,Y_pred))\nprint(\"Accuracy:\", acc_score(Y_test, Y_pred))\nprint(\"Predicted Class:\",Y_pred)\nprint(\"Scored Class (From all data):\",Y_scored)\nprint(\"Scored Class (From training data):\",Y_scored2)\nprint(\"Scored Class (From testing data):\",Y_scored3)\n","65596e2f":"import pandas_profiling\nimport re\nimport matplotlib.pyplot as plt \nimport seaborn as sb\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport unidecode\nfrom wordcloud import WordCloud\nfrom nltk.stem import WordNetLemmatizer \nnltk.download('wordnet')\nfrom nltk.stem import PorterStemmer\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize \nimport matplotlib.animation as animation\nimport operator\nimport plotly.express as px\nfrom collections import Counter\n%matplotlib inline","44b6c92e":"from xgboost import XGBClassifier\nmodel_bow = XGBClassifier(random_state=22,learning_rate=0.9)\nmodel_bow.fit(x_train_tfidf,y_train)","823fe68a":"predict_xgb = model_bow.predict(x_test_tfidf)","3ff6ece4":"from sklearn.metrics import confusion_matrix,f1_score\nconfusion_matrix(y_test,predict_xgb)","87fedeeb":"xgb_f1=f1_score(y_test,predict_xgb)\nxgb_f1","247de9ed":"results = {'RandomForest':rf_f1, 'XgBoost':xgb_f1,'SVM':svm_f1}  \ndf = pd.DataFrame(results, index =['f1Score']) \ndf","a6c95e29":"X_train, X_val, y_train, y_val = train_test_split(train['processed_tweets'], train['label'], random_state = 0)\nX_train.shape, X_val.shape","7361e808":"#Code for removing slang words\nd = {'luv':'love','wud':'would','lyk':'like','wateva':'whatever','ttyl':'talk to you later',\n               'kul':'cool','fyn':'fine','omg':'oh my god!','fam':'family','bruh':'brother',\n               'cud':'could','fud':'food'} ## Need a huge dictionary\nwords = \"I luv myself\"\nwords = words.split()\nreformed = [d[word] if word in d else word for word in words]\nreformed = \" \".join(reformed)\nreformed","eff61d40":"train  = pd.read_csv(\"..\/input\/tweeter-hate-speech-sentiment-analysis\/train_tweets.csv\")\ntest = pd.read_csv(\"..\/input\/tweeter-hate-speech-sentiment-analysis\/test_tweets.csv\")","f9c99a8f":"df = train.append(test, ignore_index = True)\ndf.shape","3b0de780":"df.sample(5)","2dbb7a27":"#Code to remove @\ndf['clean_tweet'] = df['tweet'].apply(lambda x : ' '.join([tweet for tweet in x.split()if not tweet.startswith(\"@\")]))\ndf.head()","930acb27":"#Removing numbers\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([tweet for tweet in x.split() if not tweet == '\\d*']))\ndf.head()\n","5db787de":"#Removing all the greek characters using unidecode library\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([unidecode.unidecode(word) for word in x.split()])) \ndf.head(5)","15d0a7d4":"#Removing the word 'hmm' and it's variants\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if not word == 'h(m)+' ]))\ndf.head()","1436a7d7":"#Code for removing slang words\nd = {'luv':'love','wud':'would','lyk':'like','wateva':'whatever','ttyl':'talk to you later',\n               'kul':'cool','fyn':'fine','omg':'oh my god!','fam':'family','bruh':'brother',\n               'cud':'could','fud':'food'} ## Need a huge dictionary\nwords = \"I luv myself\"\nwords = words.split()\nreformed = [d[word] if word in d else word for word in words]\nreformed = \" \".join(reformed)\nreformed","ef6ac90d":"df['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join(d[word] if word in d else word for word in x.split()))\ndf.head(5)","5be7df52":"#Finding words with # attached to it\ndf['#'] = df['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if word.startswith('#')]))\ndf.head()\n","7891a0a5":"train['cleaned_tweet'] = train.tweet.apply(lambda x: ' '.join([word for word in x.split() if not word.startswith('@')]))\ntest['cleaned_tweet'] = test.tweet.apply(lambda x: ' '.join([word for word in x.split() if not word.startswith('@')]))","b1d22c63":"#Select all words from normal tweet\nnormal_words = ' '.join([word for word in train['cleaned_tweet'][train['label'] == 0]])\n#Collect all hashtags\npos_htag = [htag for htag in normal_words.split() if htag.startswith('#')]\n#Remove hashtag symbol (#)\npos_htag = [pos_htag[i][1:] for i in range(len(pos_htag))]\n#Count frequency of each word\npos_htag_freqcount = nltk.FreqDist(pos_htag)\npos_htag_df = pd.DataFrame({'Hashtag' : list(pos_htag_freqcount.keys()),\n                            'Count' : list(pos_htag_freqcount.values())})","eaf668a9":"#Select top 20 most frequent hashtags and plot them   \nmost_frequent = pos_htag_df.nlargest(columns=\"Count\", n = 20) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=most_frequent, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","95cf9389":"#Repeat same steps for negative tweets\nnegative_words = ' '.join([word for word in train['cleaned_tweet'][train['label'] == 1]])\nneg_htag = [htag for htag in negative_words.split() if htag.startswith('#')]\nneg_htag = [neg_htag[i][1:] for i in range(len(neg_htag))]\nneg_htag_freqcount = nltk.FreqDist(neg_htag)\nneg_htag_df = pd.DataFrame({'Hashtag' : list(neg_htag_freqcount.keys()),\n                            'Count' : list(neg_htag_freqcount.values())})","82ae2d41":"most_frequent = neg_htag_df.nlargest(columns=\"Count\", n = 20) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=most_frequent, x= \"Hashtag\", y = \"Count\")\nplt.show()","7805daab":"normal_words = ' '.join([word for word in train['tweet'][train['label'] == 0]])\nwordcloud = WordCloud(width = 800, height = 500, max_font_size = 110).generate(normal_words)\nprint('Normal words')\nplt.figure(figsize= (12,8))\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","9a16aebc":"negative_words = ' '.join([word for word in train['tweet'][train['label'] == 1]])\nwordcloud = WordCloud(width = 800, height = 500, max_font_size = 110).generate(negative_words)\nprint('Negative words')\nplt.figure(figsize= (12,8))\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","ece31f4c":"vect = CountVectorizer().fit(X_train)\nvect","a1ff298d":"print('Total features =', len(vect.get_feature_names()))\nprint(vect.get_feature_names()[::5000])","f519f822":"X_train_vectorized = vect.transform(X_train)\nX_train_vectorized","9823f26a":"model = MultinomialNB()\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_val))\nprint('F1 :', f1_score(y_val, pred))","a7f040dc":"model = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_val))\nprint('F1 :', f1_score(y_val, pred))","1c42e407":"# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\nvect = TfidfVectorizer().fit(X_train)\nprint('Total Features =', len(vect.get_feature_names()))\nX_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_val))\nprint('F1: ', f1_score(y_val, pred))","a1eaa1e9":"vect = CountVectorizer(min_df = 2, ngram_range = (1,2)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nprint('Total Features =', len(vect.get_feature_names()))\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_val))\nprint('F1: ', f1_score(y_val, pred))","b4734808":"pipe = make_pipeline(CountVectorizer(), LogisticRegression())\nparam_grid = {\"logisticregression__C\": [0.01, 0.1, 1, 10, 50, 100],\n              \"countvectorizer__min_df\": [1,2,3],\n              \"countvectorizer__ngram_range\": [(1,1), (1,2), (1,3)]}\ngrid = GridSearchCV(pipe, param_grid, cv = 5, scoring = 'f1', n_jobs = -1)\ngrid.fit(X_train, y_train)\nprint('Best parameters:', grid.best_params_)","d0d70042":"vect = CountVectorizer(min_df = 1, ngram_range = (1,1)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nprint('Total Features =', len(vect.get_feature_names()))\n\nmodel = LogisticRegression(C = 10)\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_val))\nprint('F1: ', f1_score(y_val, pred))","05d0b541":"print('Fraction of racist\/sexist tweet in train data :', train.label.sum()\/len(train))\nprint('Fraction of racist\/sexist tweet predicted by model :', pred.sum()\/len(pred))","22696359":"pred_prob = model.predict_proba(vect.transform(X_val))\npred = np.where(pred_prob[:,1] > 0.35, 1, 0)\nprint('Fraction of racist\/sexist tweet predicted by model :', sum(pred)\/len(pred))\nprint('F1: ', f1_score(y_val, pred))","9a1e533c":"feature_names = np.array(vect.get_feature_names())\nsorted_coef_index = model.coef_[0].argsort()\nprint('Smallest_coefs :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest_coefs :\\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","17c526e2":"# Normal Tweets","fe81eb2d":"# Data Visualisation","1144010e":"# Conclusion","02970ffd":"# Sentiment Analysis","6447bc88":"### Racist\/Sexist Tweets","7656e2d3":"# Exploratory Data Analysis","1ce2e728":"# Decision Tree Classifier","d8c827df":"### Tfidf","c01c798d":"Fraction is very less. Lets change the default predict probability.","b03b020b":"# What is Sentimental Analysis..?? <br>\n\n**A good sentiment analysis tool will tell you whether content that someone has shared online (like a Tweet, post, or comment, etc) expresses positive, negative, or neutral opinions.** <br>\n**This is done using algorithms that perform text analyses, computational linguistics, and natural language processing to understand the sentiment attached to an author\u2019s words.** <br>\n\n**Analyzing patterns in sentiment from customer feedback allows you to identify what your customers like (and dislike).** <br>\n**And this is key to creating an amazing user experience.**<br>","3beea883":"# Objective of Twitter Sentiment Analysis : <br>\n**Hate Speech in the form of racism and sexism has become a nuisance on twitter and it is important to segregate these sort of tweets from the rest.** <br>\n**In this problem, we provide Twitter data that has both normal and hate tweets.** <br>\n**Your task is to identify the tweets which have a positive connotation from the tweets that have a negative one**.","adf03661":"# Comparision Random Forest VS XGBoost Vs SVM","74173220":"![image.png](attachment:72eaa1a9-9472-4d6b-b348-58fd483c8add.png)","b258c4e2":"# Towards Conclusion","4ddf123c":"# Data Cleaning","1258f50a":"Logistic Regression performed well then Naive Bayes for the default parameters. Thus, we will be using only Logistic Regression ahead.\n\nLets now rescale the data using tf-idf","bad1279e":"**Note :-** <br>\n**From both plots, we can conclude that hashtags are very important for sentiment analysis and should not be ignored.**","c109e6ea":"Words used like love, friend, happy are used in normal tweets whereas racist\/sexist can be found in words like trump, black, politics etc.","ec6b1790":"# Applying Bag-of-Words","c7acb790":"# Racist\/Sexist Tweets","e3afbac0":"# Importing All Important Libraries","3eae2d9b":"# Train Test Split","95e6b6ff":"### min_df & n-grams","cf94a061":"# Support Vector Classifier","8b373712":"# Random Forest Classifier","2cf084a9":"# Twitter Sentiment Analysis <br>\n\n**Detect hate speech (racist\/sexist) in tweets.**","bc782e94":"# CountVectorizer","0b6d435a":"  # Content of this NoteBook <br>  \n  **1. Topic.** <br>\n**2. Presenter Detail.** <br>\n**3. Project Detail.** <br>\n**4. Importing All Important Libraries.** <br>\n**5. Loading the Data.** <br>\n**6. Exploratory Data Analysis.** <br>\n**7. Data Visualisation.** <br>\n**8. Data Cleaning.** <br>\n**9. Sentiment Analysis.** <br>\n**10. Normal Tweets.** <br>\n**11. Racist \/ Sexist Tweets.** <br>\n**12 Visualization of common words tweets.** <br>\n**13.Train_Test_Split.** <br>\n**14. Count Vectorizer.** <br>\n**15. Random Forest Classifier.** <br>\n**16. Decision Tree Classifier.** <br>\n**17. Hashtags.** <br>\n**18. XGB Classifier.** <br>\n**19. Support Vector Classifier.** <br>\n**20. Comparision Random Forest VS XGBoost Vs SVC.** <br>\n**21. Applying Bag of Words.** <br>\n**22. Positive, Neutral And Negative Sentiment.** <br>\n**23 Visualization of common words tweets.** <br>\n**24. Normal Tweets.** <br>\n**25. Racist\/Sexist Tweets.** <br>\n**26 CountVectorizer.** <br>\n**27. Naive Bayes.** <br>\n**28. Logistic Regression.** <br>\n**29. Tfidf.** <br>\n**30. min_df & n-grams.** <br>\n**31. Hyper parameter tuning.** <br>\n**32. Towards Conclusion.** <br>\n**33. Conclusion.**  <br>","258f4765":"### Hyper parameter tuning","267be134":"# Loading the Data Set","3b36eda2":"# The smallest coefficients are indicating to normal tweets to the model whereas the largest coeeficients are indicative for racist\/sexist tweets.","5113128a":"# XGBoost Classifier","0bae143c":"tf-idf not performed well for this data.","66e61df8":"### Logistic Regression","cf038675":"## Bag-of-Words with more than one word (n-grams)","146aabd6":"# Hashtags","eb8bdde1":"Using hyperparameter tuning and probability method, we were able to improve our model score by 5%.\n\nLets look at largest and smallest coefficients that our model used.","7663d39c":"### Naive Bayes"}}