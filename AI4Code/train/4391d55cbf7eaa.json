{"cell_type":{"7d84f6f7":"code","14d4cc99":"code","0b8d4719":"code","82fb0a8f":"code","8fedb49b":"code","07afbd0d":"code","aed322f4":"code","055c5b8a":"code","263e7153":"code","f81175ad":"code","4523b6b9":"code","208cc8b2":"code","c3f2ed29":"code","7f4d7861":"code","bd2c5df4":"code","49475cb0":"code","90ee405c":"code","59abaa20":"code","3f2964f2":"code","ed188226":"code","37cc63fd":"code","5618e911":"code","c446e43d":"code","26074894":"code","a5b42f0e":"code","5006d6b5":"code","89e577a5":"code","4dc0d174":"code","a09a2e2c":"code","5af4fe05":"code","98d8418e":"code","3e4c11c0":"code","1f2e3052":"code","bd322cd3":"code","d0ca479d":"code","3134caad":"code","2853725d":"code","cd2a135b":"code","2bbd30c4":"code","601bd0c3":"code","bc00938b":"markdown","9d44134a":"markdown","11740989":"markdown","b750e552":"markdown","a09b081a":"markdown","fa44ee77":"markdown","ab7ab5b2":"markdown","cc3d5a8e":"markdown","11ed565f":"markdown","88595cd9":"markdown","4abd8135":"markdown","75a87641":"markdown","c30771ec":"markdown","f28d8d31":"markdown","56a85edd":"markdown","93be75c8":"markdown","108c674e":"markdown","757a1d11":"markdown","1a0b8b12":"markdown","3ff5b5e9":"markdown","78865049":"markdown","5903dc71":"markdown","acbe7a9c":"markdown","031d7b39":"markdown","b2abe207":"markdown","41212877":"markdown","ee65b04f":"markdown","ec833afd":"markdown","acb17c33":"markdown","6158055f":"markdown"},"source":{"7d84f6f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14d4cc99":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport timeit\n\nstart_time_total = timeit.default_timer()\n\nSEED = 1","0b8d4719":"lol_data = pd.read_csv('..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv', index_col='gameId')\nlol_data.head()","82fb0a8f":"lol_data.info()","8fedb49b":"sns.boxplot(x=\"blueWins\", y=\"blueGoldDiff\", data=lol_data)","07afbd0d":"sns.boxplot(x=\"blueWins\", y=\"blueExperienceDiff\", data=lol_data)","aed322f4":"y = lol_data.blueWins\nX = lol_data.drop(columns='blueWins')","055c5b8a":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=SEED)\nrf_model = RandomForestClassifier(random_state=SEED)\nrf_model.fit(train_X, train_y)\npred = rf_model.predict(val_X)\nbaseline_score = accuracy_score(val_y, pred)\nprint('Accuracy: %.2f%%' %(baseline_score*100))","263e7153":"r = lol_data.drop('blueWins', axis=1).corr()\nplt.figure(figsize=(20, 12))\nsns.heatmap(r, annot=True, fmt='.2f', center= 0)","f81175ad":"redundant_data = ['redFirstBlood', 'redKills', 'redDeaths', 'redGoldDiff', 'redExperienceDiff', 'redGoldPerMin', 'redCSPerMin', 'blueGoldPerMin', 'blueCSPerMin']\nclean_data = lol_data.drop(redundant_data, axis=1)\n\nr = clean_data.drop('blueWins', axis=1).corr()\nplt.figure(figsize=(20, 12))\nsns.heatmap(r, annot=True, fmt='.2f', center= 0);","4523b6b9":"y_clean = clean_data.blueWins\nX_clean = clean_data.drop(columns='blueWins')\ntrain_X, val_X, train_y, val_y = train_test_split(X_clean, y_clean, random_state=SEED)\nrf_model = RandomForestClassifier(random_state=SEED)\nrf_model.fit(train_X, train_y)\npred = rf_model.predict(val_X)\nscore = accuracy_score(val_y, pred)\nprint('Accuracy: %.2f%% vs baseline accuracy: %.2f%%' %(score*100, baseline_score*100))","208cc8b2":"import eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import SelectFromModel\n\nstart_time = timeit.default_timer()\nperm = PermutationImportance(rf_model).fit(val_X, val_y)\nelapsed = timeit.default_timer() - start_time\nprint('Elapsed time: %s s' %elapsed)\neli5.show_weights(perm, feature_names=val_X.columns.tolist(), top=None)","c3f2ed29":"loss = clean_data[clean_data.blueWins==0]\nwon = clean_data[clean_data.blueWins==1]\nloss = loss.drop(columns='blueWins')\nwon = won.drop(columns='blueWins')\nloss.head()","7f4d7861":"won.head()","bd2c5df4":"import shap\n\ndef shap_row(df, model, row_to_show=0):\n    data_for_prediction = df.iloc[row_to_show]\n    data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n    model.predict_proba(data_for_prediction_array)\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(data_for_prediction)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","49475cb0":"plot = shap_row(loss, rf_model, 1)\nplot","90ee405c":"plot = shap_row(won, rf_model, 1)\nplot","59abaa20":"loss_small_diff = loss[(abs(loss.blueGoldDiff)<500) & (abs(loss.blueExperienceDiff)<500)]\nloss_small_diff.head()","3f2964f2":"plot = shap_row(loss_small_diff, rf_model, 0)\nplot","ed188226":"start_time = timeit.default_timer()\n\nexplainer = shap.TreeExplainer(rf_model)\nshap_values_all = explainer.shap_values(val_X)\nshap.summary_plot(shap_values_all[0], val_X)\n\nelapsed = timeit.default_timer() - start_time\nprint('Elapsed time: %s s' %elapsed)","37cc63fd":"from sklearn.feature_selection import SelectKBest, f_classif\n\ndef k_features(model, train_X, val_X, train_y, val_y, k=1):\n    selector = SelectKBest(f_classif, k=k)\n    X_new = selector.fit_transform(train_X, train_y)\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                     index=train_X.index, \n                                     columns=train_X.columns)\n    selected_cols = selected_features.columns[selected_features.var() != 0]\n\n    kbest_X = train_X[selected_cols]\n    kval_X = val_X[selected_cols]\n\n    model.fit(kbest_X, train_y)\n    pred = model.predict(kval_X)\n    score = accuracy_score(val_y, pred)\n    return score\n\ndef plot_results(results):\n    plt.plot(list(results.keys()), list(results.values()))\n    plt.xlabel('# of features')\n    plt.ylabel('Score (accuracy)')\n    plt.show()\n    \ndef scores(results):\n    key_min = min(results.keys(), key=(lambda k: results[k]))\n    key_max = max(results.keys(), key=(lambda k: results[k]))\n    \n    print('Highest score at %d features of %.2f%%' %(key_max, results[key_max]*100))\n    print('Lowest score at %d features of %.2f%%' %(key_min, results[key_min]*100))\n    return key_max","5618e911":"start_time = timeit.default_timer()\nresults_rf = {}\n\nrf_model = RandomForestClassifier(random_state=SEED)\n\nfor i in range(1, len(train_X.columns)): # len(train_X.columns)\n    score = k_features(rf_model, train_X, val_X, train_y, val_y, i)\n#     print('Accuracy: %.2f%% %s' %(score*100, selected_cols.tolist()))\n    results_rf[i] = score\n    \nplot_results(results_rf)\nelapsed = timeit.default_timer() - start_time\nprint('Elapsed time: %.2f s' %elapsed)","c446e43d":"k_num = scores(results_rf)\nselector = SelectKBest(f_classif, k=k_num)\nX_new = selector.fit_transform(train_X, train_y)\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=train_X.index, \n                                 columns=train_X.columns)\nselected_cols = selected_features.columns[selected_features.var() != 0]\n\nkbest_X = train_X[selected_cols]\nkval_X = val_X[selected_cols]\n\nrf_model.fit(kbest_X, train_y)\npred = rf_model.predict(kval_X)\nscore = accuracy_score(val_y, pred)\nprint('\\nAccuracy: %.2f%% vs the baseline score %.2f%% \\n\\nBest Features using RandomForestClassifier: %s' %(score*100, baseline_score*100, selected_cols.tolist()))\nnot_selected = selected_features.columns[selected_features.var() == 0]\nprint('\\nFeatures not selected in RandomForestClassifier: %s' %not_selected.tolist())","26074894":"from xgboost import XGBClassifier\n\nxgb_model = XGBClassifier()\nxgb_model.fit(train_X, train_y)\npred = xgb_model.predict(val_X)\nscore = accuracy_score(val_y, pred)\nprint('Accuracy: %.2f%% vs the baseline score of %.2f%%' %(score*100, baseline_score*100))\n\nperm = PermutationImportance(xgb_model).fit(val_X, val_y)\neli5.show_weights(perm, feature_names=val_X.columns.tolist())","a5b42f0e":"start_time = timeit.default_timer()\nresults_xgb = {}\n\nfor i in range(1, len(train_X.columns)): # len(train_X.columns)\n    score = k_features(xgb_model, train_X, val_X, train_y, val_y, i)\n#     print('Accuracy: %.2f%% %s' %(score*100, selected_cols.tolist()))\n    results_xgb[i] = score\n\n# print(results_xgb)\nplot_results(results_xgb)\nelapsed = timeit.default_timer() - start_time\nprint('Elapsed time: %.2f s' %elapsed)","5006d6b5":"k_num = scores(results_xgb)\nselector = SelectKBest(f_classif, k=k_num)\nX_new = selector.fit_transform(train_X, train_y)\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=train_X.index, \n                                 columns=train_X.columns)\nselected_cols = selected_features.columns[selected_features.var() != 0]\n\nkbest_X = train_X[selected_cols]\nkval_X = val_X[selected_cols]\n\nxgb_model.fit(kbest_X, train_y)\npred = xgb_model.predict(kval_X)\nscore = accuracy_score(val_y, pred)\nprint('\\nAccuracy: %.2f%% vs the baseline score %.2f%% \\n\\nBest features using XGBoost: %s' %(score*100, baseline_score*100, selected_cols.tolist()))\nnot_selected = selected_features.columns[selected_features.var() == 0]\nprint('\\nFeatures not selected in XGBClassifier: %s' %not_selected.tolist())","89e577a5":"# Helper function\ndef make_diffs(df):\n    df['killDiff'] = df['blueKills'] - df['redKills']\n    df['assistDiff'] = df['blueAssists'] - df['redAssists']\n    df['avgLevelDiff'] = df['blueAvgLevel'] - df['redAvgLevel']\n    df['eliteMonstersDiff'] = df['blueEliteMonsters'] - df['redEliteMonsters']\n    df['towersDiff'] = df['blueTowersDestroyed'] - df['redTowersDestroyed']\n    df['wardsPlacedDiff'] = df['blueWardsPlaced'] - df['redWardsPlaced']\n    df['wardsDestroyedDiff'] = df['blueWardsDestroyed'] - df['redWardsDestroyed']\n    df['minionsDiff'] = df['blueTotalMinionsKilled'] - df['redTotalMinionsKilled']\n    df['jungleMinionsDiff'] = df['blueTotalJungleMinionsKilled'] - df['redTotalJungleMinionsKilled']\n    df.head()\n    return df\n\nclean_data = lol_data.copy()\ny = clean_data.blueWins\n# clean_data.columns\nclean_data = clean_data.drop(columns='blueWins', axis=1)\nclean_data.info()","4dc0d174":"clean_data = make_diffs(clean_data)\nprint(clean_data.columns.tolist())","a09a2e2c":"r = clean_data.corr()\nplt.figure(figsize=(20, 12))\nsns.heatmap(r, annot=True, fmt='.2f', center= 0)","5af4fe05":"correlations = ['redFirstBlood', 'redKills', 'redDeaths', 'blueCSPerMin', 'blueGoldPerMin', 'blueTotalGold', \n                'redGoldDiff', 'redExperienceDiff', 'redTotalGold', 'redCSPerMin', 'redGoldPerMin', \n                'blueTotalExperience', 'redTotalExperience']\n\nother_data = ['blueWardsPlaced', 'blueWardsDestroyed', 'blueKills', 'blueDeaths', 'blueAssists', \n              'blueEliteMonsters', 'blueDragons', 'blueHeralds', 'blueTowersDestroyed', \n              'blueAvgLevel', 'blueTotalMinionsKilled', 'blueTotalJungleMinionsKilled', \n              'redWardsPlaced', 'redWardsDestroyed', 'redAssists', 'redEliteMonsters', \n              'redDragons', 'redHeralds', 'redTowersDestroyed', 'redAvgLevel', \n              'redTotalMinionsKilled', 'redTotalJungleMinionsKilled']\nteam_diff = clean_data.drop(correlations, axis=1)\nteam_diff.head()","98d8418e":"r = team_diff.corr()\nplt.figure(figsize=(20, 12))\nsns.heatmap(r, annot=True, fmt='.2f', center= 0)","3e4c11c0":"train_X, val_X, train_y, val_y = train_test_split(team_diff, y, random_state=SEED)\ndiff_model = RandomForestClassifier(random_state=SEED)\ndiff_model.fit(train_X, train_y)\npred = diff_model.predict(val_X)\nscore = accuracy_score(val_y, pred)\nprint('Accuracy: %.2f%% vs the baseline score %.2f%%' %(score*100, baseline_score*100))","1f2e3052":"perm = PermutationImportance(diff_model).fit(val_X, val_y)\neli5.show_weights(perm, feature_names=val_X.columns.tolist(), top=None)","bd322cd3":"sel = SelectFromModel(perm, threshold=0.02, prefit=True)\nX_trans = sel.transform(val_X)\nX_trans","d0ca479d":"pi_features = team_diff.filter(['blueGoldDiff', 'killDiff', 'blueExperienceDiff', 'jungleMinionsDiff', \n                                'redTotalMinionsKilled', 'redEliteMonsters', 'redTotalJungleMinionsKilled', \n                                'blueDeaths', 'blueDragons'], axis=1)\ntpi_X, vpi_X, tpi_y, vpi_y = train_test_split(pi_features, y, random_state=SEED)\npi_model = RandomForestClassifier(random_state=SEED)\npi_model.fit(tpi_X, tpi_y)\npred = pi_model.predict(vpi_X)\nscore = accuracy_score(vpi_y, pred)\nprint('Accuracy: %.2f%% vs the baseline score %.2f%%' %(score*100, baseline_score*100))","3134caad":"start_time = timeit.default_timer()\n\nexplainer = shap.TreeExplainer(diff_model)\nshap_values_all = explainer.shap_values(val_X)\nshap.summary_plot(shap_values_all[0], val_X)\n\nelapsed = timeit.default_timer() - start_time\nprint('Elapsed time: %s s' %elapsed)","2853725d":"start_time = timeit.default_timer()\nresults_rf = {}\n\nfor i in range(1, len(train_X.columns)): # len(train_X.columns)\n    score = k_features(diff_model, train_X, val_X, train_y, val_y, i)\n#     print('Accuracy: %.2f%% %s' %(score*100, selected_cols.tolist()))\n    results_rf[i] = score\n    \nplot_results(results_rf)\nelapsed = timeit.default_timer() - start_time\nprint('Elapsed time: %.2f s' %elapsed)","cd2a135b":"k_num = scores(results_rf)\nselector = SelectKBest(f_classif, k=k_num)\nX_new = selector.fit_transform(train_X, train_y)\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=train_X.index, \n                                 columns=train_X.columns)\nselected_cols = selected_features.columns[selected_features.var() != 0]\n\nkbest_X = train_X[selected_cols]\nkval_X = val_X[selected_cols]\n\ndiff_model.fit(kbest_X, train_y)\npred = diff_model.predict(kval_X)","2bbd30c4":"score = accuracy_score(val_y, pred)\nprint('Accuracy: %.2f%% vs a baseline score of %.2f%%\\n\\nBest Features using RandomForestClassifier: %s' %(score*100, baseline_score*100, selected_cols.tolist()))\nnot_selected = selected_features.columns[selected_features.var() == 0]\nprint('\\nFeatures not selected in RandomForestClassifier: %s' %not_selected.tolist())","601bd0c3":"end_time_total = timeit.default_timer() - start_time_total\nprint('Elapsed time: %.2f s' %end_time_total)","bc00938b":"# Taking it one step further\n\nRed and blue's total gold and total experience are basically red and blue's gold and experience differences subtracted from each other. Thus, they can be removed. Looking at the difference in each team's jungle minions killed, assists and kills, etc. could be useful. Starting at the original data set then removing and adding features that were used in calculating differences:","9d44134a":"## SHAP values\n\nSeparate the rows where blue wins and the ones where blue loses:","11740989":"Below shows the SHAP values of the first row from the table above. In general, when the gold\/experience differences are small, the total gold and total experience both become an important feature determining the outcome of the game.","b750e552":"Then, looking at the correlation between the columns listed above and removing blueTotalGold and redTotalGold since they're what make up blueGoldDiff, as well as those that perfectly correlate with each other:","a09b081a":"This is a helper function that shows the SHAP value plot for a specific row:","fa44ee77":"Choosing the features that Permutation Importance thinks are the best (this will change depending on the seed):","ab7ab5b2":"# Choosing the features","cc3d5a8e":"## Permutation importance for reduced dataset","11ed565f":"The correlations after the perfectly correlated columns were removed:","88595cd9":"# Setup\n\nAs always, read the data file and look at the data to determine whether or not it needs to be cleaned up before doing any predictions. The SEED is for the random_state parameter in RandomForesClassifier.","4abd8135":"What features become important when the gold difference and experience difference are both small?","75a87641":"And for a winning team:","c30771ec":"## Permutation importance\n\nThe most important features, according to permutation importance, determining whether or not blue wins are:","f28d8d31":"Create all the possible differences, minus the difference in number of dragons and heralds which gets combined as the different between elite monsters, all the columns are:","56a85edd":"A few helper functions to help out in the next section. Click to unhide:","93be75c8":"Using **SelectKBest** to help pick out the best features with ANOVA F-value for classification:","108c674e":"## SelectKBest for reduced dataset","757a1d11":"There should no longer be any 1s or -1s unless it's on the diagonal. Then, the accuracy of the cleaned data vs the baseline accuracy:","1a0b8b12":"## SHAP values for reduced dataset","3ff5b5e9":"RandomForestClassifier will be the model used to predict whether or not blue wins. Without any data manipulation the accuracy is:","78865049":"It looks like there are no categorical features so we can proceed with training the model.","5903dc71":"## SelectKBest with XGBoost","acbe7a9c":"For a losing team, let's look at the SHAP values that determined why they lost:","031d7b39":"The best and worst accuracy scores are shown below as well as the number of features used in the model. This is again compared with the baseline accuracy score.","b2abe207":"This took a few minutes (~3 mins) to run on my PC because there were almost 10k rows and a lot of features but it shows which features push the outcome towards a loss or a win:","41212877":"Will XGBoost give better accuracy? For XGBoost, without selecting the k best features, the accuracy is:","ee65b04f":"After running SelectKBest from 1 to the total number of features, XGBoost was faster and had better accuracy with less features. While RandomForestModel was slower and had better accuracy with more features.","ec833afd":"My theory is that the gold and experience differences will be the most important features in determining whether blue wins or not. But, when the gold and experience differences are almost negligible, other features start becoming important in predicting which team will win.","acb17c33":"## SelectKBest with RandomForestClassifier","6158055f":"The above accuracy will be the baseline and will be referred to as comparison in later predictions. To improve the score, start with a heatmap showing all the correlations. The features that have extremely high correlation should be dropped:"}}