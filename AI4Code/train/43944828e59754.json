{"cell_type":{"5cec3f2d":"code","c696f36f":"code","bcb9ad8b":"code","dc9ae8b4":"code","378ef8a8":"code","4b48d94b":"code","198d983b":"code","0ff684c3":"code","f533d3bd":"code","9c13f884":"code","b263b18a":"markdown","fb729af0":"markdown","64c15457":"markdown","7e72e6b3":"markdown","34dbaffa":"markdown","3a8da1e8":"markdown","47f9c138":"markdown","8aaaee6e":"markdown","c15f6fc3":"markdown","503dd61a":"markdown","c04f6248":"markdown","09809a76":"markdown","22adbeb7":"markdown","f437cb91":"markdown","528bfc3b":"markdown","6f8e6e69":"markdown"},"source":{"5cec3f2d":"# We need to install GPy and GPyOpt for tunning\n!pip install GPy\n!pip install GPyOpt","c696f36f":"# Importing\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras as K\nimport GPy\nimport GPyOpt\nfrom GPyOpt.methods import BayesianOptimization\nfrom tensorflow.keras.datasets import mnist","bcb9ad8b":"# Importing datasets from MNIST and perform a simple data preprocessing\n# Getting dataset from the mnist\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\n# Flatten data\nm, h, w = X_train.shape\nmv, _, _ = X_test.shape\nX_train = X_train.reshape(m, h * w)\nX_test = X_test.reshape(mv, h * w)\n\n# Data Normalization\nX_train = X_train \/ 255\nX_test = X_test \/ 255\n\n# Labels Encoding\nY_train_oh = K.utils.to_categorical(Y_train)\nY_test_oh = K.utils.to_categorical(Y_test)","dc9ae8b4":"def get_model(lr, beta1, beta2, nodes):\n    \"\"\"Creates a model base on the provided parameters\n    Arguments:\n        lr {float} -- Is the learning rate for Adam optimizer\n        beta1 {float} -- Is the exponential decay rate\n        beta2 {float} -- Is the exponential decay rate\n        nodes {tuple} -- Contains the number of units in each layer\n    Returns:\n        keras.model -- A keras model\n    \"\"\"\n    inputs = K.Input(\n        shape=(784,),\n        name=\"inputs_layer\"\n    )\n    layer = K.layers.Dense(\n        nodes[0],\n        activation=\"relu\",\n        name=\"first_layer\"\n    )(inputs)\n    layer = K.layers.Dense(\n        nodes[1],\n        activation=\"relu\",\n        name=\"second_layer\"\n    )(layer)\n    layer = K.layers.Dense(\n        nodes[2],\n        activation=\"relu\",\n        name=\"third_layer\"\n    )(layer)\n    outputs = K.layers.Dense(\n        10,\n        activation=\"softmax\",\n        name=\"last_layer\"\n    )(layer)\n    model = K.Model(inputs, outputs)\n\n    optimizer = K.optimizers.Adam(\n        lr=lr,\n        beta_1=beta1,\n        beta_2=beta2\n    )\n    model.compile(\n        loss=\"categorical_crossentropy\",\n        optimizer=optimizer,\n        metrics=['accuracy']\n    )\n    return model","378ef8a8":"def fit_model(hyperparams):\n    \"\"\"Fit the model, black-box function\n    \n    Arguments:\n        hyperparams {list} -- Contains the passed parameters\n    Returns:\n        float -- Accuracy\n    \"\"\"\n    lr = hyperparams[0][0]\n    beta1 = hyperparams[0][1]\n    beta2 = hyperparams[0][2]\n    batch_size = int(hyperparams[0][3])\n    epochs = int(hyperparams[0][4])\n    nodes = (\n        int(hyperparams[0][5]),\n        int(hyperparams[0][6]), \n        int(hyperparams[0][7])\n    )\n\n    model = get_model(lr, beta1, beta2, nodes)\n\n    callback = []\n\n    early = K.callbacks.EarlyStopping(monitor='loss', patience=3)\n    callback.append(early)\n\n    filepath = \"lr_{}-beta1_{}-beta2_{}-batchsize_{}-epochs_{}-nodes_{}.{}.{}\".format(\n        lr.round(4),\n        beta1.round(4),\n        beta2.round(4),\n        batch_size,\n        epochs,\n        nodes[0],\n        nodes[1],\n        nodes[2]\n    )\n    best = K.callbacks.ModelCheckpoint(\n        filepath,\n        save_best_only=True,\n        monitor='loss'\n    )\n    callback.append(best)\n\n    blackbox = model.fit(\n        X_train,\n        Y_train_oh,\n        batch_size=batch_size,\n        epochs=epochs,\n        validation_split=0.15,\n        verbose=0,\n        callbacks=callback\n    )\n    loss = blackbox.history['val_loss'][-1]\n    return loss","4b48d94b":"# Space domain\nlr_domain = (0.00001, 0.1)\nbeta1_domain = (0.9, 0.9999)\nbeta2_domain = (0.9, 0.9999)\nbatch_domain = [25, 50, 75, 100, 125, 150, 200]\nepochs_domain = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\nfirst_domain = [16, 32, 64, 128, 256, 512]\nsecond_domain = [16, 32, 64, 128, 256, 512]\nthird_domain = [16, 32, 64, 128, 256, 512]\n\n# Space bounds\nbounds = [\n    {\"name\": \"lr\", \"type\": \"continuous\", \"domain\": lr_domain},\n    {\"name\": \"beta1\", \"type\": \"continuous\", \"domain\": beta1_domain},\n    {\"name\": \"beta2\", \"type\": \"continuous\", \"domain\": beta2_domain},\n    {\"name\": \"batch_size\", \"type\": \"discrete\", \"domain\": batch_domain},\n    {\"name\": \"epochs\", \"type\": \"discrete\", \"domain\": epochs_domain},\n    {\"name\": \"first\", \"type\": \"discrete\", \"domain\": first_domain},\n    {\"name\": \"second\", \"type\": \"discrete\", \"domain\": second_domain},\n    {\"name\": \"third\", \"type\": \"discrete\", \"domain\": third_domain},\n]","198d983b":"# Initialize Bayesian optimization\nb_optimization = BayesianOptimization(\n    fit_model,\n    domain=bounds,\n    model_type=\"GP\",\n    initial_design_numdata=1,\n    acquisition_type=\"EI\",\n    maximize=True,\n    verbosity=True\n)\n# Run optimization\nb_optimization.run_optimization(max_iter=30)","0ff684c3":"b_optimization.plot_convergence()","f533d3bd":"b_optimization.save_report('report_1.txt')","9c13f884":"# Optimization result\n!cat report_1.txt","b263b18a":"$$\n\\begin{pmatrix}\n    f(x_{1})\\\\\n    f(x_{2})\\\\\n    \\vdots\\\\\n    f(x_{n})\n\\end{pmatrix}\n\\thicksim\\mathscr{N}\n\\begin{pmatrix}\n    \\begin{pmatrix}\n        \\mu_{1}\\\\\n        \\mu_{2}\\\\\n        \\vdots\\\\\n        \\mu_{n}\n    \\end{pmatrix}\n    ,\n    \\begin{pmatrix}\n        \\sigma_{11}&\\sigma_{12}&\\cdots&\\sigma_{1n}\\\\\n        \\sigma_{21}&\\sigma_{22}&\\cdots&\\sigma_{2n}\\\\\n        \\vdots&\\vdots&\\ddots\\\\\n        \\sigma_{n}&\\sigma_{n2}&\\cdots&\\sigma_{nn}\\\\\n    \\end{pmatrix}\n\\end{pmatrix}\n$$","fb729af0":"Our neural network is a simple model with 3 hidden layers each layer contain a n number of units and use the ReLU activation function. the output layer is a Softmax layer with 10 categories to predict. the model use Adam optimization with learning rate $\\lambda$, two exponential decay rate $\\beta_1$ and $\\beta_2$","64c15457":"But for multivariate Gaussian we extend the \u03bc and \u03c3 to a high dimension vector space.","7e72e6b3":"$$\n\\begin{pmatrix}\n    f\\\\\n    f_{*}\n\\end{pmatrix}\n= \\mathscr{N}\n\\begin{pmatrix}\n    \\begin{bmatrix}\n        \\mu\\\\\n        \\mu_{*}\n    \\end{bmatrix},\n    \\begin{pmatrix}\n        K&&K_{*}\\\\\n        K_{*}^{T}&&K_{**}\n    \\end{pmatrix}\n\\end{pmatrix}\n$$","34dbaffa":"## Gaussian Process\n\nStarting from the Gaussian distribution which is a Normal distribution defined by two parameters, the mean $\\mu$ to express our expectation, and the standard deviation $\\sigma$ to show how much we are certain about this expectation.","3a8da1e8":"The Gaussian Process is a Gaussian distribution over a function, the function is maps a specific domain X to another domain Y, with a mean vector contains the expected output of that function and the \u03a3 covariance describe how much we are close to the truth. so as much we are close to the truth Y in our expectation as much the \u03c3 is close to zero.","47f9c138":"## Abstract\n\nFinding the right number of layers, or the number of nodes in each layer, either a lot of other hyper-parameters, it could be a headache. Until now in my learning journey I used to think about it as a strike of luck to find the right fit. But today we will build a deep neural network (not much deep) in order to solve a classic computer vision problem to recognize a handwritten digits from the MNIST datasets. But first we need to figure out the math behind the whole concept of how to optimize parameter using Gaussian Process, and Bayesian Optimization.","8aaaee6e":"$$\\mathit{K}(x_{i}, x_{j})=\\sigma_{f}^{2}.exp\\lgroup-\\frac{1}{2l^2}.(x_{i}, x_{j})^{T}.(x_{i}, x_{j})\\rgroup$$","c15f6fc3":"$$x\\thicksim\\mathscr{N}(\\mu,\\sigma)$$","503dd61a":"## Building black-box function\nThe black-box function is a function that takes the tuned hyper-parameters and give a significant metrics in order to evaluation acquisition. so what could be better than the accuracy to evaluate a model. Our hyper-parameters are:\n- learning rate: is an important parameters has a significant impact on the yield of our model.\n_The learning rate is perhaps the most important hyper-parameter. If you have time to tune only one hyper-parameter, tune the learning rate. Deep learning_\n- $\\beta_1$: The exponential decay rate for the first moment estimates.\n- $\\beta_2$: The exponential decay rate for the second-moment estimates\n- nodes: Is the number of nodes in each hidden layer, the number of nodes reflect the capacity of the model.\n_We can control whether a model is more likely to overfit or underfit by altering its capacity_\n- batch_size: Is our way to control stability of the model\n- epochs: As the number of epochs increases, more number of times the weight are changed in the neural network and the curve goes from underfitting to optimal to overfitting curve.","c04f6248":"Based on the assumption that values close to each other in the X domain could be close in mapped co-domain Y. So building a new covariance matrix \u03a3 could construct a new function in the way that neighbor values have larger correlation than values with larger distance between them.\n\nTo build the new covariance matrix we need a Kernel which would be responsible of assigning a high correlation based on distance between values. There is a lot of kernel: Radial Basis Function, Squared Exponential Kernel \u2026\n\nUse the Radial Basis Function is my pick for today and here the function:","09809a76":"## Bayesian optimization\nWhen we don\u2019t have an explicit expression for a particular function, sampling at random points is the only way for evaluation of this function. In case the sampling is very expensive we need a clever optimization technique as Bayesian optimization which attempt the global optimum in few steps. A surrogate model such us Gaussian Process is useful in order to measure outcomes easily.\n\nThe Bayesian optimization is an iterative process which plan the next move based on the actual improvement. This suggestions is done by an Acquisition Function and we can see a lot of them as maximum probability improvement (MPI), or Expected improvement (EI):\n","22adbeb7":"$$x\\thicksim\\mathscr{N}(\\vec{\\mu},\\Sigma)$$","f437cb91":"## Introduction\nI remember when I started studying machine learning and saw this countless parameters, the first question jumped in my head, How can I find the exact needed value? at first I thought its a secret recipe private to a experienced deep learning practitioner. But as I move on in my learning journey at [Holberton School](http:\/\/holbertonschool.com), I found out a beautiful and clever technique that I would like to share it with you.","528bfc3b":"Now we need to get help from GPy and GPyOpt to optimize our model based on the surrogate model Gaussian Process, and using the Expectation Improvement acquisition function.","6f8e6e69":"The Gaussian process are used to find a prior distribution of the function that could be describe our data conditional on the observed samples in form of posterior functions that fit the data. Let\u2019s say we have some outputs of function f and we want to find the unknown outputs $f_*$."}}