{"cell_type":{"af9f502f":"code","4a54697f":"code","1b59b498":"code","464ac34a":"code","fcfed2e2":"code","ca2ac59e":"code","7f712234":"code","b4f9a2b3":"code","46747993":"code","b3be99d7":"code","ae5c9c30":"code","b5508459":"code","01df1fab":"code","8f580281":"code","3dcdf32d":"code","6737e566":"code","02f2811c":"code","acd4b34d":"code","dda3c88c":"code","4695292e":"code","62aa4148":"code","2fb3c310":"code","5d0549f1":"code","a5f12d37":"code","2304766d":"code","4b9a9c78":"code","b918ea19":"code","314c5bfc":"code","582ed539":"markdown","64b63e3a":"markdown","7f873a39":"markdown","bcdd381f":"markdown","5d9e0794":"markdown","c33bffac":"markdown","457cd88d":"markdown","a44db9e2":"markdown","f2558dc2":"markdown","794c78d8":"markdown","37914f14":"markdown","c5e931dc":"markdown","4ba7cd61":"markdown","96802b59":"markdown","e04c79b6":"markdown","84ea9edc":"markdown","290cbb2a":"markdown","51a318e2":"markdown","b087015d":"markdown","2119b6ea":"markdown","30c861cd":"markdown","43ab7ed7":"markdown","bdc84848":"markdown","03007a2a":"markdown","673e0648":"markdown","0c1ad307":"markdown","d4c3b4ce":"markdown","bf5548f1":"markdown","9d40d51b":"markdown","ca418486":"markdown","c68c3a8b":"markdown","f644b367":"markdown","da351705":"markdown","1b72e04d":"markdown","f5a879d1":"markdown","3b461755":"markdown","ab491627":"markdown","ccb2452d":"markdown","a6e3ed6f":"markdown","2df02ee0":"markdown"},"source":{"af9f502f":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nrandom_state = 123","4a54697f":"df = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\", index_col=\"customerID\")\ndf.head()","1b59b498":"df[\"TotalCharges\"] = df[\"TotalCharges\"].apply(pd.to_numeric, errors='coerce')\ndf[\"SeniorCitizen\"] = df[\"SeniorCitizen\"].apply(lambda x: \"Yes\" if x == 0 else \"No\")","464ac34a":"df.dtypes","fcfed2e2":"df.isnull().sum()","ca2ac59e":"df = df.dropna()","7f712234":"df.isnull().sum()","b4f9a2b3":"for column in df.select_dtypes(\"number\").columns:\n    df.pivot(columns=\"Churn\")[column].plot.hist(alpha=0.5)\n    plt.title(column)\n    plt.show()","46747993":"for column in df.select_dtypes(\"object\").columns.drop(\"Churn\"):\n    df.pivot(columns=\"Churn\")[column].apply(pd.value_counts).plot.bar()\n    plt.title(column)\n    plt.show()","b3be99d7":"X = df.drop(columns=[\"Churn\"])\ny = df[\"Churn\"]","ae5c9c30":"scaler = StandardScaler()\nX[X.select_dtypes(\"number\").columns] = scaler.fit_transform(X.select_dtypes(\"number\"))","b5508459":"ordEnc = OrdinalEncoder(dtype=np.int)\nX[X.select_dtypes(\"object\").columns] = ordEnc.fit_transform(X.select_dtypes(\"object\"))","01df1fab":"labEnc = LabelEncoder()\ny = labEnc.fit_transform(y)","8f580281":"estimator = LogisticRegression(random_state=random_state)\nrfecv = RFECV(estimator=estimator, cv=StratifiedKFold(10, random_state=random_state, shuffle=True), scoring=\"accuracy\")\nrfecv.fit(X, y)","3dcdf32d":"plt.figure(figsize=(8, 6))\nplt.plot(range(1, len(rfecv.grid_scores_)+1), rfecv.grid_scores_)\nplt.grid()\nplt.xticks(range(1, X.shape[1]+1))\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"CV Score\")\nplt.title(\"Recursive Feature Elimination (RFE)\")\nplt.show()\n\nprint(\"The optimal number of features: {}\".format(rfecv.n_features_))","6737e566":"X_rfe = X.iloc[:, rfecv.support_]","02f2811c":"print(\"\\\"X\\\" dimension: {}\".format(X.shape))\nprint(\"\\\"X\\\" column list:\", X.columns.tolist())\nprint(\"\\\"X_rfe\\\" dimension: {}\".format(X_rfe.shape))\nprint(\"\\\"X_rfe\\\" column list:\", X_rfe.columns.tolist())","acd4b34d":"X_train, X_test, X_rfe_train, X_rfe_test, y_train, y_test = train_test_split(X, X_rfe, y, \n                                                                             train_size=0.8, \n                                                                             stratify=y,\n                                                                             random_state=random_state)\nprint(\"Train size: {}\".format(len(y_train)))\nprint(\"Test size: {}\".format(len(y_test)))","dda3c88c":"clf_keys = [\"Logistic Regression\", \"Support Vector Machine\", \"Naive Bayes\", \"k-Nearest Neighbors\",\n            \"Stochastic Gradient Descent\", \"Decision Tree\", \"AdaBoost\", \"Multi-layer Perceptron\"]\nclf_values = [LogisticRegression(random_state=random_state), SVC(kernel=\"linear\", random_state=random_state),\n              GaussianNB(), KNeighborsClassifier(), SGDClassifier(random_state=random_state),\n              DecisionTreeClassifier(random_state=random_state), AdaBoostClassifier(random_state=random_state), \n              MLPClassifier(random_state=random_state, max_iter=1000)]\nclf_rfe_keys = [\"Logistic Regression\", \"Support Vector Machine\", \"Naive Bayes\", \"k-Nearest Neighbors\",\n                \"Stochastic Gradient Descent\", \"Decision Tree\", \"AdaBoost\", \"Multi-layer Perceptron\"]\nclf_rfe_values = [LogisticRegression(random_state=random_state), SVC(kernel=\"linear\",random_state=random_state),\n                  GaussianNB(), KNeighborsClassifier(), SGDClassifier(random_state=random_state),\n                  DecisionTreeClassifier(random_state=random_state), AdaBoostClassifier(random_state=random_state), \n                  MLPClassifier(random_state=random_state, max_iter=1000)]\nclfs = dict(zip(clf_keys, clf_values))\nclfs_rfe = dict(zip(clf_rfe_keys, clf_rfe_values))\n\n# Original dataset\nprint(\"Model training using original data: started!\")\nfor clf_name, clf in clfs.items():\n    clf.fit(X_train, y_train)\n    clfs[clf_name] = clf\n    print(clf_name, \"training: done!\")\nprint(\"Model training using original data: done!\\n\")\n\n# Feature-selected dataset\nprint(\"Model training using feature-selected data: started!\")\nfor clf_rfe_name, clf_rfe in clfs_rfe.items():\n    clf_rfe.fit(X_rfe_train, y_train)\n    clfs_rfe[clf_rfe_name] = clf_rfe\n    print(clf_rfe_name, \"training: done!\")\nprint(\"Model training using feature-selected data: done!\")","4695292e":"# Original dataset\nacc = []\nfor clf_name, clf in clfs.items():\n    y_pred = clf.predict(X_test)\n    acc.append(accuracy_score(y_test, y_pred))\n\n# Feature selected dataset\nacc_rfe = []\nfor clf_rfe_name, clf_rfe in clfs_rfe.items():\n    y_rfe_pred = clf_rfe.predict(X_rfe_test)\n    acc_rfe.append(accuracy_score(y_test, y_rfe_pred))\n    \nacc_all = pd.DataFrame({\"Original dataset\": acc, \"Feature-selected dataset\": acc_rfe},\n                       index=clf_keys)\nacc_all","62aa4148":"print(\"Accuracy\\n\" + acc_all.mean().to_string())\n\nax = acc_all.plot.bar(figsize=(10, 8))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height().round(3)), (p.get_x()*0.985, p.get_height()*1.002))\nplt.ylim((0.7, 0.82))\nplt.xticks(rotation=90)\nplt.title(\"All Classifier Accuracies\")\nplt.grid()\nplt.show()","2fb3c310":"scoring = [\"accuracy\", \"roc_auc\"]\n\nscores = []\n# Original dataset\nprint(\"Cross-validation on original data: started!\")\nfor clf_name, clf in clfs.items():\n    score = pd.DataFrame(cross_validate(clf, X, y, cv=StratifiedKFold(10, random_state=random_state, shuffle=True), scoring=scoring)).mean()\n    scores.append(score)\n    print(clf_name, \"cross-validation: done!\")\ncv_scores = pd.concat(scores, axis=1).rename(columns=dict(zip(range(len(clf_keys)), clf_keys)))\nprint(\"Cross-validation on original data: done!\\n\")\n\nscores = []\n# Feature-selected dataset\nprint(\"Cross-validation on feature-selected data: started!\")\nfor clf_name, clf in clfs_rfe.items():\n    score = pd.DataFrame(cross_validate(clf, X_rfe, y, cv=StratifiedKFold(10, random_state=random_state, shuffle=True), scoring=scoring)).mean()\n    scores.append(score)\n    print(clf_name, \"cross-validation: done!\")\ncv_scores_rfe = pd.concat(scores, axis=1).rename(columns=dict(zip(range(len(clf_keys)), clf_keys)))\nprint(\"Cross-validation on feature-selected data: done!\")","5d0549f1":"# Accuracy\ncv_acc_all = pd.concat([cv_scores.loc[\"test_accuracy\"].rename(\"Original data\"), cv_scores_rfe.loc[\"test_accuracy\"].rename(\"Feature-selected data\")], \n                       axis=1)\n\nprint(\"Cross-validation accuracy\\n\" + cv_acc_all.mean().to_string())\nax = cv_acc_all.plot.bar(figsize=(10, 8))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height().round(3)), (p.get_x()*0.985, p.get_height()*1.003))\nplt.xticks(rotation=90)\nplt.ylim((0.7, 0.82))\nplt.title(\"Cross-validation Accuracy\")\nplt.grid()\nplt.legend()\nplt.show()","a5f12d37":"# ROC AUC\ncv_roc_auc_all = pd.concat([cv_scores.loc[\"test_roc_auc\"].rename(\"Original data\"), cv_scores_rfe.loc[\"test_roc_auc\"].rename(\"Feature-selected data\")], \n                           axis=1)\n\nprint(\"Cross-validation ROC AUC score\\n\" + cv_roc_auc_all.mean().to_string())\nax = cv_roc_auc_all.plot.bar(figsize=(10, 8))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height().round(3)), (p.get_x()*0.985, p.get_height()*1.003))\nplt.xticks(rotation=90)\nplt.ylim((0.63, 0.88))\nplt.title(\"Cross-validation ROC AUC Score\")\nplt.grid()\nplt.legend()\nplt.show()","2304766d":"# Fit time\ncv_fit_time_all = pd.concat([cv_scores.loc[\"fit_time\"].rename(\"Original data\"), cv_scores_rfe.loc[\"fit_time\"].rename(\"Feature-selected data\")], \n                           axis=1)\n\nprint(\"Cross-validation fit time\\n\" + cv_fit_time_all.mean().to_string())\nax = cv_fit_time_all.plot.bar(figsize=(10, 8))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height().round(3)), (p.get_x()*0.985, p.get_height()*1.003))\nplt.xticks(rotation=90)\nplt.yscale(\"log\")\nplt.title(\"Cross-validation Fit Time\")\nplt.grid()\nplt.legend()\nplt.show()","4b9a9c78":"importance = abs(clfs[\"Logistic Regression\"].coef_[0])\nplt.barh(X.columns.values[importance.argsort()], importance[importance.argsort()])\nplt.title(\"Logistic Regression - Feature Importance (Original Data)\")\nplt.grid()\nplt.show()\n\nimportance_rfe = abs(clfs_rfe[\"Logistic Regression\"].coef_[0])\nplt.barh(X_rfe.columns.values[importance_rfe.argsort()], importance_rfe[importance_rfe.argsort()])\nplt.title(\"Logistic Regression - Feature Importance (Feature-selected Data)\")\nplt.grid()\nplt.show()","b918ea19":"importance = clfs[\"AdaBoost\"].feature_importances_\nplt.barh(X.columns.values[importance.argsort()], importance[importance.argsort()])\nplt.title(\"AdaBoost - Feature Importance (Original Data)\")\nplt.grid()\nplt.show()\n\nimportance_rfe = clfs_rfe[\"AdaBoost\"].feature_importances_\nplt.barh(X_rfe.columns.values[importance_rfe.argsort()], importance_rfe[importance_rfe.argsort()])\nplt.title(\"AdaBoost - Feature Importance (Feature-selected Data)\")\nplt.grid()\nplt.show()","314c5bfc":"importance = abs(clfs[\"Support Vector Machine\"].coef_[0])\nplt.barh(X.columns.values[importance.argsort()], importance[importance.argsort()])\nplt.title(\"Support Vectore Machine - Feature Importance (Original Data)\")\nplt.grid()\nplt.show()\n\nimportance_rfe = abs(clfs_rfe[\"Support Vector Machine\"].coef_[0])\nplt.barh(X_rfe.columns.values[importance_rfe.argsort()], importance_rfe[importance_rfe.argsort()])\nplt.title(\"Support Vectore Machine - Feature Importance (Feature-selected Data)\")\nplt.grid()\nplt.show()","582ed539":"Find the feature importance of the predictive model that has been made. In this case, use Logistic Regression because it has the highest accuracy among all models.","64b63e3a":"<a id='Feature-Importance'><\/a>\n# E. Feature Importance","7f873a39":"Remove any rows that have NaN value.","bcdd381f":"Split the feature-selected DataFrame into train and test set. Also, do the same thing on the original DataFrame.","5d9e0794":"From the steps above, the data is reduced to only 9 features from 19 features in the original data.\n\nNow, let's compare their performance on various machine learning models.","c33bffac":"<a id='Feature-Engineering-and-Selection'><\/a>\n# B. Feature Engineering and Selection","457cd88d":"The top 5 important features of both AdaBoost models are slightly different. AdaBoost classifier that was trained on original data includes \"PaymentMethod\" on the fifth rank of its feature importance, while this feature is not selected during the RFE step. The rest of these important features are similar in both models.","a44db9e2":"Top 5 important features of both Logistic Regression models are the same (\"tenure\", \"PhoneService\", \"Contract\", \"TotalCharges\", and \"MonthlyCharges\"). The rest of these important features are quite the same in both models.","f2558dc2":"Do some changes on \"SeniorCitizen\" and \"TotalCharges\" data type to make them appropriate.","794c78d8":"Also, don't forget to encode the label.","37914f14":"Do feature selection by using recursive feature elimination (RFE). Use Logistic Regression classifier as the estimator, and set the fold (k) for cross-validation to 10.","c5e931dc":"Use StandarScaler to standardize all numerical features, so their mean and standard deviation are zero and one, respectively.","4ba7cd61":"<a id='Build-Some-ML-Models'><\/a>\n# C. Build Some ML Models","96802b59":"Make histogram for every numeric features against churn.","e04c79b6":"Also find the feature importance of Support Vector Machine.","84ea9edc":"Let's check the data type.","290cbb2a":"Then, make bar plot for every categorical features against churn.","51a318e2":"<a id='Summary'><\/a>\n# F. Summary","b087015d":"From the accuracy result, the mean accuracy of feature-selected data is 0.75% higher than the mean accuracy of the original data. The best accuracy here is Logistic Regression model trained on feature-selected data with 80.4% accuracy. Multi-layer Perceptron accuracy got the highest improvement by 2.8% with training on feature-selected data. Both SVM and AdaBoost accuracies of feature-selected data are slightly lower (only 0.1% lower) than the accuracies of original data. Remember, feature-selected data only has **9 features** while original data has 19 features.\n\nThe models that have the best ROC AUC score are Logistic Regression and AdaBoost with an ROC AUC score of 0.844. The ROC AUC result is not much different from the accuracy result. But there are some classifiers (Logistic Regression, Naive Bayes, and AdaBoost) that have slightly lower ROC AUC score of feature-selected data than the ROC AUC score of original data.\n\nAll models that were trained on feature-selected data have faster fit time than the one that was trained on original data. It is obviously because the number of features trained on those models.","2119b6ea":"# **Recursive Feature Elimination (RFE) to Predict Customer Churn**","30c861cd":"Let's dig down how recursive feature elimination can be useful to reduce data with many features, and this feature-selected data will still give similar performance result (or even higher) on various machine learning models compared to the original data.","43ab7ed7":"## Contents\n1. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n2. [Feature Engineering and Selection](#Feature-Engineering-and-Selection)\n3. [Build Some ML Models](#Build-Some-ML-Models)\n4. [Model Evaluation](#Model-Evaluation)\n5. [Feature Importance](#Feature-Importance)\n6. [Summary](#Summary)","bdc84848":"Separate features and label column into two variables, X and y.","03007a2a":"From these three models, \"tenure\", \"MonthlyCharges\", and \"TotalCharges\" are always appeared on the top 5 important features of each model.","673e0648":"Let's visualize cross-validation accuracy, ROC AUC score, and fit time results.","0c1ad307":"Recursive feature elimination (RFE) is very useful to select only necessary features, save the training time, and still get similar accuracy, or even higher than the original data. RFE is popular because it is easy to configure and use and because it is effective at selecting those features (columns) in a training dataset that are more or most relevant in predicting the target variable. The feature importance of feature-selected data is also still preserved and is quite the same with original data based on the observation above.","d4c3b4ce":"Make a line plot of number of selected features against cross-validation score. Then, print the optimal number of features.","bf5548f1":"From the result above, the mean accuracy of feature-selected data is slightly higher (0.3% higher) than the mean accuracy of the original data. The model that has the best accuracy is Support Vector Machine trained on feature-selected data with 79.6% accuracy. Multi-layer Perceptron accuracy improved by 2.3% with training on feature-selected data. But, there are some classifiers (Naive Bayes, k-Nearest Neighbors, Stochastic Gradient Descent, and AdaBoost) that don't get the advantage from training on feature-selected data.\n\nTo ensure this result, evaluate the model by using cross-validation.","9d40d51b":"Import the dataset.","ca418486":"Let's try these following classifiers to make the machine learning model, and compare their performance for the original and feature-selected dataset.\n* Logistic Regression\n* Support Vector Machine (linear kernel)\n* Naive Bayes\n* k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Decision Tree\n* AdaBoost\n* Multi-layer Perceptron","c68c3a8b":"Compare the dimension of DataFrame \"X\" and \"X_rfe\".","f644b367":"Check the accuracy of these two models, for now.","da351705":"Make a new DataFrame called \"X_rfe\" that contains selected features.","1b72e04d":"To validate the accuracy result and evaluate the performance of these two models furthermore, do k-fold cross-validation with $k = 10$ on the whole dataset.\nMetrics to validate are: accuracy, and ROC AUC score.","f5a879d1":"Let's check the feature importance of AdaBoost classifier for comparison.","3b461755":"<a id='Model-Evaluation'><\/a>\n# D. Model Evaluation","ab491627":"Encode each categorical feature by using ordinal encoder.","ccb2452d":"Make a bar plot of all accuracy results to visualize them.","a6e3ed6f":"The top 5 important features of both Support Vector Machine models are kind of different. SVM classifier that was trained on original data placed \"tenure\" on the fifth rank, while the other model placed \"tenure\" on the first rank.","2df02ee0":"<a id='Exploratory-Data-Analysis'><\/a>\n# A. Exploratory Data Analysis"}}