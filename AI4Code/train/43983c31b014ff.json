{"cell_type":{"437f0f89":"code","192dee0a":"code","75c90e58":"code","6608925b":"code","eb8989d4":"code","bcbdded5":"code","f4bba07b":"code","f40badae":"code","f58831bb":"code","34b87ecd":"code","c295739c":"code","42b243e1":"code","12865a23":"code","316b5f2b":"code","de5e7e4e":"code","c45bd971":"code","906b175b":"code","eb269921":"code","a3a1d86b":"code","4ede7312":"code","d95f5575":"code","1d475d3d":"code","fd6bb57b":"code","157bda68":"code","b7ad6242":"code","ccfb87b8":"code","0d77281a":"code","384b474e":"code","f0d8cf48":"code","dc3a83ea":"code","6d605be4":"code","8bb2053c":"code","2054d3d9":"code","078bde6f":"code","ada3eaef":"code","9567e47b":"code","f3b1b7a4":"code","86f7f3fd":"code","511e44f1":"code","2b4ada12":"code","8e529dd0":"code","173021a7":"code","f92f28a1":"code","a8cb713f":"code","724059b9":"code","b919197b":"code","59f32685":"markdown","826ad870":"markdown","54de17ef":"markdown","0c8a1dc4":"markdown","8f7ae4b7":"markdown","9379d9f4":"markdown","5e69fa23":"markdown","c3e04ef7":"markdown","b8802401":"markdown","6d1e92d0":"markdown","a609bbc4":"markdown","9e9cd78d":"markdown","923d9635":"markdown","edc7b3d7":"markdown","d2efe409":"markdown","08f27c93":"markdown","5e6cfa8e":"markdown","7952d324":"markdown","92cfbbca":"markdown","7b9cdb68":"markdown","cd04b58c":"markdown","0df75968":"markdown","6e9a2598":"markdown","0635ae3d":"markdown","092833a6":"markdown","582705ce":"markdown","506037f9":"markdown","a78862a3":"markdown","61186e01":"markdown","00a911f2":"markdown","ee27fea9":"markdown","b8414867":"markdown","3dc55b1a":"markdown","08ede682":"markdown","4e3a6195":"markdown","7e4620eb":"markdown","6d6f71ed":"markdown"},"source":{"437f0f89":"!pip install pyspark","192dee0a":"from pyspark import SparkContext\nsc = SparkContext.getOrCreate()\nprint(sc)","75c90e58":"rdd=sc.emptyRDD() \nrdd.isEmpty()","6608925b":"a = sc.parallelize([])\na.isEmpty()","eb8989d4":"mixRDD =sc.parallelize([True, [11,22,33,44,55], (10,20,30,40,50)], 3)\nmixRDD.collect()","bcbdded5":"mixRDD.getNumPartitions()","f4bba07b":"mixRDD.glom().collect()","f40badae":"mixRDD.setName('MyRDD')","f58831bb":"a= sc.parallelize(range(1,1000), 4)\nprint(a.getNumPartitions())\nprint(\"\\n\")\nprint(a.glom().take(1))\nprint(\"\\n\")\nprint(a.glom().max()) #to get max partitions data means last one partition(3)\nprint(\"\\n\")\nprint(a.glom().min()) #to get min partitions data means first one partition(0)\nprint(\"\\n\")","34b87ecd":"b =sc.parallelize([12,21,23,43,1,22,11,45,56])\nb.takeOrdered(4, key=lambda x: -x)","c295739c":"c=sc.parallelize(range(1,1000), 5)\nc.getNumPartitions()","42b243e1":"d= c.repartition(7)\nd.getNumPartitions()","12865a23":"e= d.repartition(4)\ne.getNumPartitions()","316b5f2b":"e.coalesce(6)\ne.getNumPartitions()","de5e7e4e":"e.coalesce(2)\ne.getNumPartitions()","c45bd971":"e.saveAsTextFile(\".\/sampletext\")","906b175b":"e.toDebugString()","eb269921":"i =sc.parallelize([1,9,5,6,7,8])\ni.reduce(lambda a,b:a+b)","a3a1d86b":"i.reduce(lambda a,b:a*b)","4ede7312":"word =['how', 'are', 'you', 'hey', 'hi']\nwordRDD=sc.parallelize(word)\nwordRDD.collect()","d95f5575":"# defining functions\ndef start_h(word):\n    return word[0].lower().startswith('h')","1d475d3d":"wordRDD.filter(start_h).collect() # it will return all words starts with 'h'","fd6bb57b":"def toUpper(s):\n    return s.upper()","157bda68":"x=['navin', 'kumar', 'pal']\nx1=sc.parallelize(x)\nk3=x1.map(toUpper)\nk3.collect()","b7ad6242":"k3=x1.map(lambda k:k.upper())\nk3.collect()","ccfb87b8":"demo = sc.textFile(\"..\/input\/titanic\/test.csv\")\ndemo.collect()","0d77281a":"demo.flatMap(lambda x:x.split(\",\")).map(lambda x: (x,1)).reduceByKey(lambda a,b:a+ b).collect()","384b474e":"t1=(['navin', 121223], ['kumar',3000])\nt1rdd=sc.parallelize(t1,2)\nprint(t1rdd.collect())\nprint(t1rdd.getNumPartitions())\nt1rdd.collectAsMap()","f0d8cf48":"C=((201, \"pune\"), (301, \"kol\"),(201, \"Mum\"), (402, \"Jaipur\"),(505,\"RTM\"))\nD=sc.parallelize(C)\nA=((201, \"navin\"), (301, \"kumar\" ), (402, \"pal\"),(603,'kk'))\nB=sc.parallelize(A)\ntupleJoin=B.join(D)\ntupleJoin.collect()","dc3a83ea":"tupleLeftJoin=B.leftOuterJoin(D)\ntupleLeftJoin.collect()","6d605be4":"sc.parallelize([5,6,7]).map(lambda x:[x,x,x]).collect()","8bb2053c":"sc.parallelize([5,6,7]).map(lambda x:[[x,x,x],[x*x*x],[x+5]]).collect()","2054d3d9":"input1 = sc.parallelize([\"apple\", \"banana\", \"pineapple\"])\nprint(input1.collect())\ninput1.count()","078bde6f":"a=sc.parallelize([5,5,6,7,8,8,8,9]).countByValue()\na","ada3eaef":"i =sc.textFile(\"..\/input\/titanic\/train.csv\",4)\ni.collect()","9567e47b":"j=i.map(lambda x:x.split(\",\"))\nk=j.map(lambda field:(field[5], field[1]))\nk.collect()","f3b1b7a4":"L=k.groupByKey()\nfor j in L.collect():\n    print([j[0],list(j[1])])","86f7f3fd":"print(L.toDebugString())\nL.getNumPartitions()","511e44f1":"# creating datframe\nx=[('Designation', 'Salary'), ('AM', '50000'), ('AM', '50000'), ('SSE', '30000'), ('SSE', '30000'), ('Lead', '40000'), ('Lead', '35000'), ('ASE', '15000'), ('ASE', '15000'), ('SE', '22000'), ('SE', '25000'), ('SE', '25000'), ('ASE', '20000'), ('ASE', '18000'), ('ASE', '15000'), ('ASE', '18000')]\nk=sc.parallelize(x)\nk.collect()","2b4ada12":"k.reduceByKey(lambda x,y:int(int(x)+int(y))).collect()","8e529dd0":" k.reduceByKey(lambda x,y:int(int(x)+int(y))).sortByKey().collect()","173021a7":"for i in k.collect():\n    print (i)","f92f28a1":"k.countByKey()","a8cb713f":" k.countByValue()","724059b9":" k.groupByKey().distinct().count()","b919197b":"k.sortByKey().collect()","59f32685":"Creating RDD from File\n\ntextFile method reads froom csv as well as text","826ad870":"<a id=\"7d\"><\/a>\n### d. Creating Flat Maps","54de17ef":"With coalesce we cannot increase the partitions","0c8a1dc4":"Expand the output to see the results","8f7ae4b7":"<a id=\"5a\"><\/a>\n### a. Fetch Ordered Elements","9379d9f4":"<a id=\"7c\"><\/a>\n### c. Applying Function as Filter","5e69fa23":"<a id=\"4d\"><\/a>\n### d. Collecting the Data with Partitions","c3e04ef7":"Finally you can check the lineage","b8802401":"<a id=\"7g\"><\/a>\n### g. Grouping on RDD","6d1e92d0":"<a id=\"6a\"><\/a>\n### a. Saving RDD to a Text File","a609bbc4":"<a id=\"4\"><\/a>\n## 4. Creating RDDs\nWe can RDDs in many different ways lets go through few of them","9e9cd78d":"<a id=\"2\"><\/a>\n## 2. Using Spark in Python\n\nThe first step in using Spark is connecting to a cluster.\n\nIn practice, the cluster will be hosted on a remote machine that's connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called worker. The master sends the workers data and calculations to run, and they send their results back to the master.\n\nWhen just getting started with Spark it's simpler to just run a cluster locally. Thus for now, instead of connecting to another computer, all computations will be run on Kaggle's servers in a simulated cluster.\n\nCreating the connection is as simple as creating an instance of the SparkContext class. The class constructor takes a few optional arguments that allow us to specify the attributes of the cluster we're connecting to.","923d9635":"<a id=\"5\"><\/a>\n## 5. Ordering and Repartitioning RDD","edc7b3d7":"# KEY NOTE\n\nThis notebook is guide to core pyspark datatype RDD. Since most technologies and industries now a days are working on cloud, it is good to have knowledge on how to apply ML and Datascience when it comes to \"Big Data\". I will be modifying the code and content already available so that we can use it with simple dataset like \"Titanic\" which is the hello world of ML.\n\nThese are just my personal notes. I am sharing these so that it helps others too, who are trying to learn the similar concepts. Any Feedback is appreciated.\n\nIf you havent checked the Spark DataFrame notebook, please feel free to do so [here](https:\/\/www.kaggle.com\/amritvirsinghx\/scalable-data-science-pyspark-nb1)","d2efe409":"<a id=\"3\"><\/a>\n# 3. Spark Core RDD\n\nRDD stands for Resilient Distributed Dataset, these are the elements that run and operate on multiple nodes to do parallel processing on a cluster. RDDs are immutable elements, which means once you create an RDD you cannot change it. RDDs are fault tolerant as well, hence in case of any failure, they recover automatically. You can apply multiple operations on these RDDs to achieve a certain task.\n\n![RDD](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-forum-message-attachments\/o\/inbox%2F1991843%2Fece9429ac833006eda20f0aabede3860%2FCapture.JPG?generation=1601101924589580&alt=media)\n\n\nTo apply operations on these RDD's, there are two ways \u2212\n\n- Transformation\n- Action\n\n**Transformation** \u2212 These are the operations, which are applied on a RDD to create a new RDD. Filter, groupBy and map are the examples of transformations.\n\n**Action** \u2212 These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver.\n\nWe can create initialize spark core RDD context as follows:","08f27c93":"<a id=\"1\"><\/a>\n## 1. What is Spark?\n\nSpark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n\nAs each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\n\nHowever, with greater computing power comes greater complexity.\n\nIf you are deciding whether or not Spark is the best solution for your problem you can consider questions like:\n* Is my data too big to work with on a single machine?\n* Can my calculations be easily parallelized?\n\n![Spark Logo](https:\/\/www.lwindia.com\/images\/Cloudera-Landing-Page-Banner.jpg)","5e6cfa8e":"The output will be saved in kaggle's output directory, notice how the file is saved in partitions as per the number of partitions of RDD","7952d324":"<a id=\"7h\"><\/a>\n### h. Reducing by Key and Functions","92cfbbca":"<a id=\"7a\"><\/a>\n### a. Performing Reduce","7b9cdb68":"<a id=\"7e\"><\/a>\n### e. Perfoming Joins on RDD","cd04b58c":"<a id=\"4c\"><\/a>\n### c. Checking Number of Partitions","0df75968":"<a id=\"7\"><\/a>\n## 7. Performing Operations on RDDs\nWe can perform two kind of operations on RDD:\n\n### i. Transformation\nTransformations are kind of operations which will transform your RDD data from one form to another. And when you apply this operation on any RDD, you will get a new RDD with transformed data (RDDs in Spark are immutable). Operations like map, filter, flatMap are transformations.\n\nNow there is a point to be noted here and that is when you apply the transformation on any RDD it will not perform the operation immediately. It will create a DAG(Directed Acyclic Graph) using the applied operation, source RDD and function used for transformation. And it will keep on building this graph using the references till you apply any action operation on the last lined up RDD. That is why the **transformation in Spark are lazy**.\nSpark has certain operations which can be performed on RDD. An operation is a method, which can be applied on a RDD to accomplish certain task. RDD supports two types of operations, which are Action and Transformation. An operation can be something as simple as sorting, filtering and summarizing data\n\nFurther transformations are of two types:\n\n- Narrow transformation\n\nIn Narrow transformation, all the elements that are required to compute the records in single partition live in the single partition of parent RDD. A limited subset of partition is used to calculate the result. Narrow transformations are the result of map(), filter().\n\n- Wide transformation\n\nIn wide transformation, all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. The partition may live in many partitions of parent RDD. Wide transformations are the result of groupbyKey and reducebyKey.\n\n### ii. Actions\nTransformations create RDDs from each other, but when we want to work with the actual dataset, at that point action is performed. When the action is triggered after the result, new RDD is not formed like transformation. Thus, actions are RDD operations that give non-RDD values. The values of action are stored to drivers or to the external storage system. It brings laziness of RDD into motion.\n\nSpark drivers and external storage system store the value of action. It brings laziness of RDD into motion.\nAn action is one of the ways of sending data from Executer to the driver. Executors are agents that are responsible for executing a task. While the driver is a JVM process that coordinates workers and execution of the task","6e9a2598":"These are some of the core operations that we perform on RDDs for further operations like querying and manipulating data we have RDD abstractions called dataframes, these are kind of similar to pd dataframes and can can also be convertedd back and forth to pandas dataframes, most data science opertations are performed with the use of Spark Dynamic frames. \n\nTo know more bout them checkout this [notebook](https:\/\/www.kaggle.com\/amritvirsinghx\/scalable-data-science-pyspark-nb1)\n\nOne last thing, if we want to ingest real time data and perform actions on streaming data we have something called spark streaming context. We ll cover it up going further, if the implementation is possible through kaggle notebooks.\n\nDanke!","0635ae3d":"Expand the output to see the result","092833a6":"<a id=\"4f\"><\/a>\n### f. Using Range to Create RDD","582705ce":"<a id=\"5b\"><\/a>\n### b. Repartitions and Coalesce","506037f9":"<a id=\"6\"><\/a>\n## 6. Saving and Debugging","a78862a3":"We can increase or decrease number of partitions with repartition method","61186e01":"Ouput DAG can be viewed on Spark console on the port which is configured. A sample DAG is as follows:\n\n![DAG](https:\/\/1.bp.blogspot.com\/-OYuEUWP8UZo\/XZQ4ZOF8ApI\/AAAAAAAADGc\/_nwzjUP8BHIEFr0FNHy3Vt55xeJYBsfdwCLcBGAsYHQ\/s640\/sp_1.png)\n\nTo know more about DAGs follow this [link](https:\/\/databricks.com\/blog\/2015\/06\/22\/understanding-your-spark-application-through-visualization.html)","00a911f2":"<a id=\"4b\"><\/a>\n### b. Creating RDD with Partitions","ee27fea9":"<a id=\"7b\"><\/a>\n### b. Creating User Defined Functions (UDF)","b8414867":"<a id=\"8\"><\/a>\n## 8. Epilogue","3dc55b1a":"<a id=\"4a\"><\/a>\n### a. Creating an Empty RDD","08ede682":"<a id=\"4e\"><\/a>\n### e. Setting Name of RDD","4e3a6195":"<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Navigation<\/h3>\n\n[1. What is Spark?](#1)     \n[2. Using Spark in Python](#2)    \n[3. Spark Core RDD](#3)    \n[4. Creating RDDs](#4)    \n&nbsp;&nbsp;&nbsp;&nbsp;[a. Creating an Empty RDD](#4a)   \n&nbsp;&nbsp;&nbsp;&nbsp;[b. Creating RDD with Partitions](#4b)       \n&nbsp;&nbsp;&nbsp;&nbsp;[c. Checking Number of Partitions](#4c)   \n&nbsp;&nbsp;&nbsp;&nbsp;[d. Collecting the Data with Partition](#4d)   \n&nbsp;&nbsp;&nbsp;&nbsp;[e. Setting Name of RDD](#4e)       \n&nbsp;&nbsp;&nbsp;&nbsp;[f. Using Range to Create RDD](#4f)  \n[5. Ordering and Repartitioning RDD](#5)  \n&nbsp;&nbsp;&nbsp;&nbsp;[a. Fetch Ordered Elements](#5a)   \n&nbsp;&nbsp;&nbsp;&nbsp;[b. Repartitions and Coalesce](#5b)       \n[6. Saving and Debugging](#6)     \n&nbsp;&nbsp;&nbsp;&nbsp;[a. Saving RDD to a Text File](#6a)   \n&nbsp;&nbsp;&nbsp;&nbsp;[b. Checking Lineage of RDD](#6b)  \n[7. Performing Operations on RDD](#7)         \n&nbsp;&nbsp;&nbsp;&nbsp;[a. Performing Reduce](#7a)   \n&nbsp;&nbsp;&nbsp;&nbsp;[b. Creating User Defined Functions (UDF)](#7b)    \n&nbsp;&nbsp;&nbsp;&nbsp;[c. Applying Function as Filter](#7c)      \n&nbsp;&nbsp;&nbsp;&nbsp;[d. Creating Flat Maps](#7d)     \n&nbsp;&nbsp;&nbsp;&nbsp;[e. Performing Joins on RDD](#7e)       \n&nbsp;&nbsp;&nbsp;&nbsp;[f. Listing and Replication](#7f)      \n&nbsp;&nbsp;&nbsp;&nbsp;[g. Grouping on RDD](#7g)     \n&nbsp;&nbsp;&nbsp;&nbsp;[h. Reducing by Key and Functions](#7h)       \n[8. Epilogue](#8)   ","7e4620eb":"<a id=\"6b\"><\/a>\n### b. Checking Lineage of RDD","6d6f71ed":"<a id=\"7f\"><\/a>\n### f. Listing and Replication"}}