{"cell_type":{"14655ff1":"code","6e2194bd":"code","94f1be98":"code","3b977346":"code","49a513f4":"code","14db9444":"code","de45e26e":"code","c8338302":"code","b0694d85":"code","c6d9136a":"code","80ed98ea":"code","6765fbaf":"code","26538e2b":"code","d5409e6e":"code","4fc47e7a":"code","c9d5366b":"code","54729f31":"code","dfbbf7bf":"code","7c142387":"code","cb89e5d9":"code","4f1ca934":"code","27d0ea4f":"code","63da9145":"code","67f83ee2":"code","c859eff8":"code","4729a4fc":"code","205b09b0":"code","e9bfaf58":"code","bee36337":"code","f5899845":"code","05fe0747":"code","dd0c4b60":"code","747665b8":"code","922c7760":"code","d1349cb8":"code","71f308c8":"code","59a8caae":"code","d7c3a54f":"code","e0669e6f":"code","eb1f63fe":"code","1475339e":"code","b44c8af9":"markdown","9ab755da":"markdown","449573ac":"markdown","7511806f":"markdown","6480669a":"markdown","f88095b7":"markdown","b5f792fc":"markdown","bb4f3e0e":"markdown"},"source":{"14655ff1":"import warnings; warnings.filterwarnings(\"ignore\")\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(\"ignore\",category=DataConversionWarning)\nimport numpy,pandas,pylab,seaborn,time,os\npylab.style.use('seaborn-pastel')\npath='..\/input\/wholesale-customers-data-set\/'\nfile='Wholesale customers data.csv'\ncmap=seaborn.cubehelix_palette(2,start=.1,rot=-.25,as_cmap=True)\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\nos.listdir('..\/input')","6e2194bd":"# https:\/\/github.com\/udacity\/machine-learning\/blob\/master\/projects\/customer_segments\/visuals.py\ndef pca_results(good_data,pca):\n    dimensions=['Dimension {}'.format(i) \n                for i in range(1,len(pca.components_)+1)]\n    components=pandas.DataFrame(numpy.round(pca.components_,4),\n                                columns=good_data.keys())\n    components.index=dimensions\n    ratios=pca.explained_variance_ratio_.reshape(len(pca.components_),1)\n    variance_ratios=pandas.DataFrame(numpy.round(ratios,4),\n                                     columns=['Explained Variance'])\n    variance_ratios.index=dimensions \n    fig,ax=pylab.subplots(figsize=(11,5))\n    components.plot(ax=ax,kind='bar');\n    ax.set_ylabel(\"Feature Weights\",fontsize=12)\n    ax.set_xticklabels(dimensions,rotation=0)\n    for i,ev in enumerate(pca.explained_variance_ratio_):\n        ax.text(i-0.40,ax.get_ylim()[1]+0.05,\n                \"Explained Variance \\n %.4f\"%(ev))\n    pylab.grid(); pylab.show()\n    return pandas.concat([variance_ratios,components],axis=int(1))\ndef cluster_results(reduced_data,preds,centers,pca_samples,alg):\n    predictions=pandas.DataFrame(preds,columns=['Cluster'])\n    plot_data=pandas.concat([predictions,reduced_data],axis=int(1))\n    fig,ax=pylab.subplots(figsize=(11,7))\n    for i,cluster in plot_data.groupby('Cluster'):   \n        cluster.plot(ax=ax,kind='scatter',\n                     x='Dimension 1',y='Dimension 2',\n                     alpha=0.8,edgecolors='slategray',\n                     c=cmap((i)*1.0\/(len(centers)-1)),\n                     label='Cluster %i'%(i),s=30);\n    for i,c in enumerate(centers):\n        ax.scatter(x=c[0],y=c[1],color='white',edgecolors='black',\n                   alpha=1,linewidth=2,marker='o',s=250);\n        ax.scatter(x=c[0],y=c[1],marker='$%d$'%(i),\n                   alpha=1,s=120,color='black');\n        ax.scatter(x=pca_samples[:,0],y=pca_samples[:,1],s=150,\n                   linewidth=4,color='#ff5a8b',marker='x');\n    ti=\"%s Cluster Learning on PCA-Reduced Data\\n\"+\\\n       \"Centroids Marked by Numbers\\n\"+\\\n       \"Transformed Sample Data Marked by Red Crosses\"\n    ax.set_title(ti%alg,fontsize=15); pylab.show()\ndef biplot(good_data,reduced_data,pca):\n    fig,ax=pylab.subplots(figsize=(11,7))\n    x=reduced_data.loc[:,'Dimension 1']\n    y=reduced_data.loc[:,'Dimension 2']\n    ax.scatter(x=x,y=y,facecolors='slategray',\n               edgecolors=cmap((x*y)**2),s=70,alpha=.4)\n    feature_vectors=pca.components_.T\n    arrow_size,text_pos=7.,5.\n    for i,v in enumerate(feature_vectors):\n        ax.arrow(0,0,arrow_size*v[0],arrow_size*v[1],\n                 head_width=.2,head_length=.2,\n                 linewidth=2,color='#ff5a8b')\n        ax.text(v[0]*text_pos,v[1]*text_pos,\n                good_data.columns[i],color='black',\n                ha='center',va='center',fontsize=12)\n    ax.set_xlabel(\"Dimension 1\",fontsize=10)\n    ax.set_ylabel(\"Dimension 2\",fontsize=10)\n    ax.set_title(\"PC plane with original feature projections.\",\n                 fontsize=15); pylab.show()\ndef channel_results(reduced_data,outliers,pca_samples):\n    full_data=pandas.read_csv(path+file)\n    channel=pandas.DataFrame(full_data['Channel'],columns=['Channel'])\n    channel=channel.drop(channel.index[outliers]).reset_index(drop=True)\n    labeled=pandas.concat([reduced_data,channel],axis=int(1))\n    fig,ax=pylab.subplots(figsize=(11,5))\n    labels=['Hotel\/Restaurant\/Cafe','Retailer']\n    grouped=labeled.groupby('Channel')\n    for i,channel in grouped:   \n        channel.plot(ax=ax,kind='scatter',\n                     x='Dimension 1',y='Dimension 2',\n                     color=cmap((i-1)*1.0\/2),label=labels[i-1],s=30)  \n    for i,sample in enumerate(pca_samples):\n        ax.scatter(x=sample[0],y=sample[1],s=230,linewidth=3,\n                   color='black',marker='o',facecolors='none');\n        ax.scatter(x=sample[0]+0.25,y=sample[1]+0.3,\n                   marker='$%d$'%(i),alpha=1,s=200,color='black')\n    ti=\"PCA-Reduced Data Labeled by 'Channel'\\n\"+\\\n       \"Transformed Sample Data Circled\"\n    ax.set_title(ti,fontsize=15); pylab.show()","94f1be98":"def split_fit():\n    X_train,X_test,y_train,y_test=\\\n    train_test_split(new_data,target,test_size=0.25,random_state=1)\n    print (\"Training and testing splitting was successful.\")\n    regressor1=DecisionTreeRegressor(random_state=1)\n    regressor1.fit(X_train,y_train)\n    regressor2=RandomForestRegressor(random_state=1)\n    regressor2.fit(X_train,y_train)\n    score1=regressor1.score(X_test,y_test)\n    score2=regressor2.score(X_test,y_test)\n    st1=\"Decision Tree Regressor. \"\n    st2=\"Random Forest Regressor. \"\n    st3=\"The score of the prediction using the testing set is {}.\"\n    print(st1+st3.format(score1)); print(st2+st3.format(score2))\n    return regressor1,regressor2\ndef plot_feature_importance(f,regressor1,regressor2):\n    D1=dict(zip(new_data,regressor1.feature_importances_))\n    D2=dict(zip(new_data,regressor2.feature_importances_))\n    pylab.figure(figsize=(11,5)); \n    pylab.subplot(121); pylab.grid()\n    pylab.bar(range(len(D1)),D1.values(),alpha=.5,\n              color='slategrey',align='center')\n    pylab.xticks(range(len(D1)),D1.keys(),fontsize=7)\n    st='Experiment with \"%s\": Feature Importances, \\n'\n    pylab.title(st%f+'Decision Tree Regressor')\n    pylab.subplot(122); pylab.grid()\n    pylab.bar(range(len(D2)),D2.values(),alpha=.5,\n              color='slategrey',align='center')\n    pylab.xticks(range(len(D2)),D2.keys(),fontsize=7)\n    pylab.title(st%f+'Random Forest Regressor'); pylab.show()","3b977346":"data=pandas.read_csv(path+file)\ndata.drop(['Region','Channel'],axis=1,inplace=True)\ndata.columns=['Fresh','Milk','Grocery',\n              'Frozen','Detergents_Paper','Delicatessen']\nti=\"The dataset has {} samples with {} features each.\"\nprint(ti.format(*data.shape)); data.describe().T","49a513f4":"data.plot.area(stacked=False,figsize=(11,5))\npylab.grid(); pylab.show()","14db9444":"# selecting samples\nindices=[23,25,27]\ndef hex_to_rgb(chex):\n        chex=chex.lstrip('#'); n=len(chex)\n        return tuple(int(chex[i:i+n\/\/3],16)\/255. for i in range(0,n,n\/\/3))\nsamples=pandas.DataFrame(data.loc[indices],\n                         columns=data.keys()).reset_index(drop=True)\nsamples.index=['C0','C1','C2']; samples.loc['Mean']=samples.mean()\nprint (\"Chosen samples of the wholesale customers' dataset:\"); samples\ncols=['#1b2c45','#5a8bbd','#008b8b','#ff5a8b']\nsamples.T.plot(figsize=(11,5),color=[hex_to_rgb(el) for el in cols])\npylab.grid(); pylab.show()","de45e26e":"pylab.figure(figsize=(11,5))\np_samples=samples.iloc[:].T.apply(lambda x:100.0*x\/x.sum())\nseaborn.heatmap(p_samples,cmap=cmap,annot=True,\n                annot_kws={\"color\":\"White\",\"size\":20},fmt='.1f')\nti=\"Product categories in percentages for the sample customers\"\npylab.title(ti,fontsize=15)\npylab.xticks(ha='center',fontsize=12)\npylab.yticks(fontsize=10); pylab.show()","c8338302":"new_data=data.drop('Delicatessen',axis=int(1))\ntarget=data['Delicatessen']\nnew_data_target=pandas.concat([new_data,target],axis=int(1))\nprint (\"The correlation table for the choosen feature 'Delicatessen'\")\npearson=new_data_target.corr(method='pearson')\ncorr_with_delicatessen=pearson.iloc[int(-1)][:-int(1)]\ncorr_with_delicatessen[abs(corr_with_delicatessen).argsort()[::-int(1)]]","b0694d85":"pylab.figure(figsize=(11,5))\nseaborn.distplot(target,color='#1b2c45',bins=200,\n                 hist_kws={'color':'SlateGrey'})\npylab.xlabel(\"Delicatessen\",fontsize=15)\npylab.title(\"Customers' Annual Spending\",fontsize=12)\npylab.grid(); pylab.show()","c6d9136a":"regressor1,regressor2=split_fit()\nplot_feature_importance('Delicatessen',regressor1,regressor2)","80ed98ea":"new_data=data.drop('Grocery',axis=int(1)); target=data['Grocery']\nnew_data_target=pandas.concat([new_data,target],axis=int(1))\nprint (\"The correlation table for the choosen feature 'Grocery'\")\npearson=new_data_target.corr(method='pearson')\ncorr_with_grocery=pearson.iloc[int(-1)][:-int(1)]\ncorr_with_grocery[abs(corr_with_grocery).argsort()[::-int(1)]]","6765fbaf":"pylab.figure(figsize=(11,5))\nseaborn.distplot(target,color='#1b2c45',bins=200,\n                 hist_kws={'color':'SlateGrey'})\npylab.xlabel(\"Grocery\",fontsize=15)\npylab.title(\"Customers' Annual Spending\",fontsize=12)\npylab.grid(); pylab.show()","26538e2b":"regressor1,regressor2=split_fit()\nplot_feature_importance('Grocery',regressor1,regressor2)","d5409e6e":"axes=pandas.plotting.scatter_matrix(data,alpha=.3,figsize=(11,10),\n                                    diagonal='hist',c='#1b2c45',\n                                    hist_kwds={'color':'SlateGrey','bins':100})\ncorr=data.corr().values\nfor i,j in zip(*numpy.triu_indices_from(axes,k=1)):\n    axes[i,j].annotate(\"%.3f\" %corr[i,j],(.8,.8),fontsize=12,\n                       xycoords='axes fraction',ha='center',va='center')\npylab.grid(); pylab.show()","4fc47e7a":"print (\"Pearson correlation coefficients\")\ndata.corr(method='pearson')","c9d5366b":"pylab.figure(figsize=(11,5)); mu,sigma=3.,1.\ns=numpy.random.lognormal(mu,sigma,1000)\ncount,bins,ignored=pylab.hist(s,200,align='mid',density=True,\n                              alpha=.5,color='slategrey')\nx=numpy.linspace(min(bins),max(bins),10000)\npdf=(numpy.exp(-(numpy.log(x)-mu)**2\/(2*sigma**2))\\\n     \/(x*sigma*numpy.sqrt(2*numpy.pi)))\npylab.plot(x,pdf,linewidth=2,color='#1b2c45')\npylab.axis('tight'); pylab.grid()\npylab.title('Example of a Random Log-Normal Distribution',\n            fontsize=15); pylab.show()","54729f31":"log_data=numpy.log(data); log_samples=numpy.log(samples)\ncorr=data.corr().values\naxes=pandas.plotting.scatter_matrix(log_data,alpha=.3,figsize=(9,8),\n                                    diagonal='hist',c='#1b2c45',\n                                    hist_kwds={'color':'SlateGrey','bins':100})\nfor i,j in zip(*numpy.triu_indices_from(axes,k=1)):\n    axes[i,j].annotate(\"%.3f\" %corr[i,j],(.2,.2),fontsize=12,\n                       xycoords='axes fraction',ha='center',va='center')\npylab.grid(); pylab.show()","dfbbf7bf":"log_samples","7c142387":"print (\"Pearson correlation coefficients\")\nlog_data.corr(method='pearson')","cb89e5d9":"for feature in log_data.keys():\n    Q1,Q3=numpy.percentile(log_data[feature],25),\\\n          numpy.percentile(log_data[feature],75)\n    step=(Q3-Q1)*1.5\n    st=\"Data points considered outliers for the feature '{}':\"\n    print(st.format(feature))\n    display(log_data[~((log_data[feature]>=Q1-step)&(log_data[feature]<=Q3+step))])\noutliers=[65,66,75,128,154]\nprint(\"Data outliers: '{}'\".format(outliers))\ngood_data=log_data.drop(log_data.index[outliers]).reset_index(drop=True)","4f1ca934":"out_percentiles=data.rank(pct=True).iloc[outliers]\npylab.figure(figsize=(11,5))\npylab.xticks(fontsize=10); pylab.yticks(fontsize=10)\nseaborn.heatmap(out_percentiles,annot=True,cmap=cmap,\n                annot_kws={'color':'White','size':'20'})\npylab.title('Multiple Outliers Heatmap',fontsize=15); pylab.show()","27d0ea4f":"pca=PCA(n_components=len(good_data.keys())).fit(good_data)\npca_samples=pca.transform(log_samples) \npca_result=pca_results(good_data,pca); pca_result","63da9145":"pylab.figure(figsize=(11,4))\npylab.xlabel('Dimension',fontsize=10)\npylab.plot(numpy.arange(1,7),\n           numpy.cumsum(pca.explained_variance_ratio_),\n           '-o',c='#1b2c45')\npylab.title('Explained Variance Ratio',fontsize=15)\npylab.grid(); pylab.show()\nprint (\"The first and second principal components explained {}%\"\\\n.format(sum(pca_result['Explained Variance'][:int(2)])*100.0))\nprint (\"The first four principal components explained {}%\"\\\n.format(sum(pca_result['Explained Variance'][:int(4)])*100.0))","67f83ee2":"pandas.DataFrame(numpy.round(pca_samples,4),\n                 columns=pca_result.index.values,\n                 index=['C0','C1','C2','Mean'])","c859eff8":"pca=PCA(n_components=2).fit(good_data)\npca_samples=pca.transform(log_samples)\nreduced_data=pandas.DataFrame(pca.transform(good_data),\n                              columns=['Dimension 1','Dimension 2'])\npandas.DataFrame(numpy.round(pca_samples,4),\n                 columns=['Dimension 1','Dimension 2'],\n                 index=['C0','C1','C2','Mean'])","4729a4fc":"biplot(good_data,reduced_data,pca)","205b09b0":"for n in list(range(2,12)):\n    clusterer=KMeans(n_clusters=n).fit(reduced_data)\n    preds=clusterer.predict(reduced_data)\n    centers=clusterer.cluster_centers_\n    sample_preds=clusterer.predict(pca_samples)\n    score=silhouette_score(reduced_data,preds)\n    st=\"For number of clusters = {}, the silhouette_score is : {}\"\n    print(st.format(n,score))","e9bfaf58":"for n in list(range(2,12)):\n    clusterer2=GaussianMixture(n_components=n,\n                               covariance_type='full').fit(reduced_data)\n    preds2=clusterer2.predict(reduced_data); centers2=clusterer2.means_\n    sample_preds2=clusterer2.predict(pca_samples)\n    score2=silhouette_score(reduced_data,preds2,metric='mahalanobis')\n    print(\"For number of clusters = {}, the silhouette_score is : {}\"\\\n          .format(n,score2))","bee36337":"n,k=300,6; kmeans_train_times=[]; gmm_train_times=[]\nfor k in range(2,k+1):\n    cum_time=0.\n    for i in range(n):\n        start=time.time()\n        KMeans(n_clusters=k).fit(reduced_data)\n        cum_time+=(time.time()-start)\n    kmeans_train_times.append([k,cum_time\/n])\nkm_df=pandas.DataFrame(kmeans_train_times,\n                       columns=['KM_Clusters','KM_Time'])\nfor k in range(2,k+1):\n    cum_time=0.\n    for i in range(n):\n        start=time.time()\n        GaussianMixture(n_components=k).fit(reduced_data)\n        cum_time+=(time.time()-start)\n    gmm_train_times.append([k,cum_time\/n])\ngmm_df=pandas.DataFrame(gmm_train_times,\n                        columns=['GMM_Components','GMM_Time'])\ntimes_df=km_df.join(gmm_df)\npylab.figure(figsize=(11,4))\npylab.plot(times_df.GMM_Components,times_df.GMM_Time*1000.,\n           '-o',c='#5a8bbd',label='GMM Train Time')\npylab.plot(times_df.GMM_Components,times_df.KM_Time*1000.,\n           '-o',c='#008b8b',label='Kmeans Train Time')\npylab.ylabel('Train time (in millisec.)',fontsize=10)\npylab.xlabel('Cluster\/Components Used in Training',fontsize=10)\nst='Training Time for Different Cluster\/Component Sizes\\n'+\\\n   'Averaged Over {} Runs Per Size'\npylab.title(st.format(n),fontsize=15)\npylab.legend(loc='best'); pylab.grid(); pylab.show()","f5899845":"clusterer=KMeans(n_clusters=2).fit(reduced_data)\nsample_preds=clusterer.predict(pca_samples)\npreds=clusterer.predict(reduced_data)\ncenters=clusterer.cluster_centers_\nscore=silhouette_score(reduced_data,preds)\ncluster_results(reduced_data,preds,centers,pca_samples,'K-Means')","05fe0747":"clusterer2=GaussianMixture(n_components=2,\n                           covariance_type='full').fit(reduced_data)\nsample_preds2=clusterer2.predict(pca_samples)\npreds2=clusterer2.predict(reduced_data)\ncenters2=clusterer2.means_; score2=silhouette_score(reduced_data,preds2)\ncluster_results(reduced_data,preds2,centers2,\n                pca_samples,'Gaussian Mixture Model')","dd0c4b60":"print (\"Data Means\")\npandas.DataFrame(numpy.mean(data)).T","747665b8":"log_centers=pca.inverse_transform(centers)\ntrue_centers=numpy.exp(log_centers)\nsegments=['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers=pandas.DataFrame(numpy.round(true_centers),\n                              columns=data.keys(),index=segments)\nprint (\"K-Means True Centers\"); true_centers","922c7760":"true_centers=true_centers.append(data.describe().loc['mean'])\ntrue_centers.plot(kind='bar',figsize=(11,5))\npylab.title('Comparing K-Means Centers with Data Means',fontsize=15)\npylab.xticks(rotation=0,fontsize=10); pylab.grid(); pylab.show()","d1349cb8":"log_centers2=pca.inverse_transform(centers2)\ntrue_centers2=numpy.exp(log_centers2)\nsegments2=['Segment {}'.format(i) for i in range(0,len(centers2))]\ntrue_centers2=pandas.DataFrame(numpy.round(true_centers2),\n                               columns=data.keys(),index=segments2)\nprint (\"Gaussian Mixture Model True Centers\"); true_centers2","71f308c8":"true_centers2=true_centers2.append(data.describe().loc['mean'])\ntrue_centers2.plot(kind='bar',figsize=(11,5))\nst='Comparing Gaussian Mixture Model Centers with Data Means'\npylab.title(st,fontsize=15)\npylab.xticks(rotation=0,fontsize=10); pylab.grid(); pylab.show()","59a8caae":"clusters=pandas.DataFrame(2*[['','']],\n                          columns=['K-Means','Gaussian Mixture'],\n                          index=['Segment 0','Segment 1'])\nif true_centers['Fresh']['Segment 0']>true_centers['Fresh']['Segment 1']:\n    clusters['K-Means']['Segment 0']='HoReCa'\n    clusters['K-Means']['Segment 1']='Retail'\nelse: \n    clusters['K-Means']['Segment 0']='Retail'\n    clusters['K-Means']['Segment 1']='HoReCa'\nif true_centers2['Fresh']['Segment 0']>true_centers2['Fresh']['Segment 1']:\n    clusters['Gaussian Mixture']['Segment 0']='HoReCa'\n    clusters['Gaussian Mixture']['Segment 1']='Retail'\nelse: \n    clusters['Gaussian Mixture']['Segment 0']='Retail'\n    clusters['Gaussian Mixture']['Segment 1']='HoReCa'\nclusters","d7c3a54f":"clusters2=pandas.DataFrame(3*[['','']],\n                           columns=['K-Means','Gaussian Mixture'],\n                           index=['C0','C1','C2'])\nclusters2['Cluster']=['Retail','Retail','HoReCa']\nfor el in ['K-Means','Gaussian Mixture']:\n    if clusters[el].values[0]=='Retail': \n        clusters2[el]=['Segment 0','Segment 0','Segment 1']\n    else: \n        clusters2[el]=['Segment 1','Segment 1','Segment 0']\nclusters2","e0669e6f":"st1=\"Sample point {} predicted to be in Cluster {}\"\nst2=\"The distance between sample point {} and center of Cluster {}:\"\nprint (\"K-Means\")\nfor i,pred in enumerate(sample_preds):\n    print (st1.format(i,pred))\n    print (st2.format(i,pred))\n    print (samples.iloc[i]-true_centers.iloc[pred])\nprint (\"\\n\\nGaussian Mixture Model\")\nfor i,pred in enumerate(sample_preds2):\n    print (st1.format(i,pred))\n    print (st2.format(i,pred))\n    print (samples.iloc[i]-true_centers2.iloc[pred])","eb1f63fe":"channel_results(reduced_data,outliers,pca_samples[:3])","1475339e":"channel_data=pandas.read_csv(path+file)\nchannel_data=channel_data.drop(\n    channel_data.index[outliers]).reset_index(drop=True)\nif clusters2['K-Means'].values[0]=='Segment 0': v11,v12=0,1\nelse: v11,v12=1,0\nif clusters2['Gaussian Mixture'].values[0]=='Segment 0': v21,v22=0,1\nelse: v21,v22=1,0\ndf=numpy.where(channel_data['Channel']==2,v11,v12)\npercentage_kmeans=sum(df==preds)\/float(len(preds))\ndf2=numpy.where(channel_data['Channel']==2,v21,v22)\npercentage_gaussian_mixture=sum(df2==preds2)\/float(len(preds2))\nprint ('K-Means')\nprint ('Percentage of correctly classified customers: {:.2%}'\\\n.format(percentage_kmeans))\nprint ('\\nGaussian Mixture Model')\nprint ('Percentage of correctly classified customers: {:.2%}'\\\n.format(percentage_gaussian_mixture))","b44c8af9":"[Interactive Version with SageMathCell](https:\/\/olgabelitskaya.github.io\/MLE_ND_P3_SMC.html)\n# Python Modules & Helpful Functions","9ab755da":"# Feature Transformation","449573ac":"# Creating Clusters","7511806f":"# Data","6480669a":"### Experiment with Grocery","f88095b7":"# Outlier Detection","b5f792fc":"# Feature Relevance\n### Experiment with Delicatessen","bb4f3e0e":"### Feature Distributions"}}