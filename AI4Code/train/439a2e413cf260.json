{"cell_type":{"8ba50db7":"code","ec0a92af":"code","dbded98a":"code","7e292a87":"code","d27b43d9":"code","93b4a7ac":"code","556f8412":"code","8b8fa48f":"code","0408c2e6":"code","f21f44a1":"code","fc2d85f0":"code","15942910":"code","704d3592":"code","60962e83":"code","2406f1b0":"code","cf505d2a":"code","5468fabe":"code","16d3c998":"code","47311395":"code","8aa5a2f7":"code","72e4e189":"code","7d43fea1":"code","503fe047":"code","a803e9c9":"markdown","5bc0a691":"markdown"},"source":{"8ba50db7":"#Kutuphanelerin yuklenmesi\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport gc\n","ec0a92af":"#Previous application\nimport pandas as pd\nprevious_application = pd.read_csv(\"..\/input\/home-credit-default-risk\/previous_application.csv\")\n\n#Verideki ilk 5 g\u00f6zlem\npd.set_option('display.max_columns', None) \nprevious_application.head(2) \n","dbded98a":"#Verideki tekil g\u00f6zlemlerin say\u0131s\u0131\nprevious_application.nunique()\n","7e292a87":"#Verinin boyutu\nprevious_application.shape\n","d27b43d9":"#Verinin hakk\u0131nda bilgiler\nprevious_application.info()","93b4a7ac":"#Previous application tablosundaki kategorik de\u011fi\u015fkenlerin tutulmas\u0131\ncat_cols = [col for col in previous_application.columns if previous_application[col].nunique() < 30]\nprint(\"kategorik degisken say\u0131s\u0131 : \" , len(cat_cols))\n\n#Verinin kategorik de\u011fi\u015fkenlerinin hedef de\u011fi\u015fkene gore durumlar\u0131\ndef cat_summary(dataframe,target, noc=30):\n    print(\"CATEGORICAL FEATURE ANALYSIS\",end=\"\\n\\n\")\n    var_count = 0\n    vars_more_classes = []\n    for var in cat_cols:\n        if dataframe[var].nunique() <= noc:  # s\u0131n\u0131f say\u0131s\u0131na g\u00f6re se\u00e7\n            print(var, \": has\",dataframe[var].nunique(), \"unique category\",\"\\t-\",str(dataframe[var].dtypes),end=\"\\n\\n\")\n            print(pd.DataFrame({var: dataframe[var].value_counts(),\n                                \"Count\": len(dataframe[var]),\n                                \"Ratio\": 100 * dataframe[var].value_counts() \/ len(dataframe),\n                                \"TARGET_MEAN\": dataframe.groupby(var)[target].mean()}),end=\"\\n\\n\\n\")\n            var_count += 1\n            \n            print(\"\\n\\n\")\n        else:\n            vars_more_classes.append(dataframe[var].name)\n    print('%d categorical variables have been described' % var_count, end=\"\\n\\n\")\n    print('There are', len(vars_more_classes), \"variables have more than\", noc, \"classes\", end=\"\\n\\n\")\n    print('Variable names have more than %d classes:' % noc, end=\"\\n\\n\")\n    print(vars_more_classes)","556f8412":"#Histogram cizdirilmesi icin say\u0131sal degiskenlerin secilmesi\n\nnum_cols = [col for col in previous_application.columns if previous_application[col].dtypes != 'O' and col not in \"Id\"\n           and previous_application[col].nunique() > 30]\nprint('Say\u0131sal de\u011fi\u015fken say\u0131s\u0131: ', len(num_cols))\n\n\n#Say\u0131sal degiskenlerin histogram\u0131na bak\u0131lmas\u0131n\u0131 sa\u011flayan fonksiyon.\ndef hist_for_nums(data, numeric_cols):\n    col_counter = 0\n    data = data.copy()\n    for col in numeric_cols:\n        data[col].hist(bins=20)\n        plt.xlabel(col)\n        plt.title(col)\n        plt.show()\n        col_counter += 1\n    print(col_counter, \"variables have been plotted\")\n\n\nhist_for_nums(previous_application, num_cols)","8b8fa48f":"# One-hot encoding for categorical columns with get_dummies\ndef one_hot_encoder(df, nan_as_category = False):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n","0408c2e6":"\n# Preprocess application_train.csv and application_test.csv\ndef application_train_test(num_rows = None, nan_as_category = False):\n    # Read data and merge\n    df = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv', nrows= num_rows)\n    test_df = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv', nrows= num_rows)\n    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n    df = df.append(test_df).reset_index()\n    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n    df = df[df['CODE_GENDER'] != 'XNA']\n    \n    # Categorical features with Binary encode (0 or 1; two categories)\n    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    # Categorical features with One-Hot encode\n    df, cat_cols = one_hot_encoder(df, nan_as_category)\n    \n    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n    \n\n    # Some simple new features (percentages)\n    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\n    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] \/ df['AMT_CREDIT']\n    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] \/ df['CNT_FAM_MEMBERS']\n    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] \/ df['AMT_INCOME_TOTAL']\n    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] \/ df['AMT_CREDIT']\n    \n    #yeniler\n    \n    df[\"app_1\"] = df[\"EXT_SOURCE_1\"] * df[\"OBS_30_CNT_SOCIAL_CIRCLE\"]\n    df[\"app_2\"] = df[\"EXT_SOURCE_1\"] * df[\"AMT_INCOME_TOTAL\"]\n    df[\"app_3\"] = df[\"NONLIVINGAPARTMENTS_AVG\"] * df[\"OWN_CAR_AGE\"]\n    df[\"app_4\"] = df[\"EXT_SOURCE_2\"] * df[\"OBS_30_CNT_SOCIAL_CIRCLE\"]\n    df[\"app_5\"] = df[\"EXT_SOURCE_3\"] * df[\"OBS_30_CNT_SOCIAL_CIRCLE\"]\n    df[\"app_6\"] = (df[\"AMT_CREDIT\"] * df[\"AMT_ANNUITY\"]) * (df[\"AMT_INCOME_TOTAL\"])\n    \n    del test_df\n    gc.collect()\n    return df","f21f44a1":"\ndef previous_applications(num_rows = None, nan_as_category = False):\n    prev = pd.read_csv('..\/input\/home-credit-default-risk\/previous_application.csv', nrows = num_rows)\n    prev, cat_cols = one_hot_encoder(prev, nan_as_category= False)\n    # Days 365.243 values -> nan\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n    # Add feature: value ask \/ value received percentage\n    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] \/ prev['AMT_CREDIT']\n    \n    prev[\"new_1\"] = (prev.AMT_DOWN_PAYMENT * prev.RATE_DOWN_PAYMENT)\n    prev[\"new_2\"] = (prev.AMT_DOWN_PAYMENT * prev.AMT_CREDIT)\n    prev[\"new_3\"] = (prev.AMT_APPLICATION *  prev.AMT_GOODS_PRICE)\n    prev[\"new_4\"] = (prev.AMT_DOWN_PAYMENT * prev.AMT_APPLICATION)\n    prev[\"new_5\"] = (prev.AMT_DOWN_PAYMENT * prev.AMT_ANNUITY)\n   \n    # Previous applications numeric features\n    num_aggregations = {\n        'AMT_ANNUITY': ['mean'],\n        'AMT_APPLICATION': ['mean'],\n        'AMT_CREDIT': ['mean'],\n        'APP_CREDIT_PERC': ['mean'],\n        'AMT_DOWN_PAYMENT': ['mean'],\n        'AMT_GOODS_PRICE': ['mean'],\n        'HOUR_APPR_PROCESS_START': ['mean'],\n        'RATE_DOWN_PAYMENT': ['mean'],\n        'DAYS_DECISION': ['max'],\n        'CNT_PAYMENT': ['mean'],\n\n    }\n    # Previous applications categorical features\n    cat_aggregations = {}\n    for cat in cat_cols:\n        cat_aggregations[cat] = ['mean']\n    \n    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n    # Previous Applications: Approved Applications - only numerical features\n    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n    # Previous Applications: Refused Applications - only numerical features\n    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n    del refused, refused_agg, approved, approved_agg, prev\n    gc.collect()\n    return prev_agg","fc2d85f0":"#Tablolar\u0131n birlestirilmesi\n\ndf = application_train_test()\nprev = previous_applications()\nprint(\"Previous applications df shape:\", prev.shape)\ndf = df.join(prev, how='left', on='SK_ID_CURR')\n#del prev\ngc.collect()","15942910":"#Kategorik de\u011fiskenlerin s\u0131n\u0131flar\u0131\nprev_orj = pd.read_csv(\"..\/input\/home-credit-default-risk\/previous_application.csv\")\nprev_cat_cols = [col for col in prev_orj.columns if prev_orj[col].nunique() < 30]\n\n\nfor i in prev_cat_cols:\n    print(\"*\" * 100)\n    print(i)\n    print(prev_orj[i].unique())","704d3592":"#application_train ve previous_application tablolar\u0131n\u0131n \u00f6zetleri(iki tablodaki say\u0131sal degiskenlerin \"TARGET\"e gore durumlar\u0131)\ndef target_summary_with_nums(data, target):\n    num_names = [col for col in data.columns if len(data[col].unique()) > 30\n                 and data[col].dtypes != 'O']\n\n    for var in num_names:\n        print(data.groupby(target).agg({var: np.median}), end=\"\\n\\n\\n\")\n    \ntarget_summary_with_nums(df , \"TARGET\")","60962e83":"#Numerik ve kategorik degiskenlerin tespiti ve tutulmas\u0131\ndef get_categorical_and_numeric_columns(dataframe, exit_columns, number_of_unique_classes=10):\n    \"\"\"\n    -> Kategorik ve say\u0131sal de\u011fi\u015fkenleri belirler.\n\n    :param dataframe: \u0130\u015flem yap\u0131lacak dataframe\n    :param exit_columns: Dikkate al\u0131nmayacak de\u011fi\u015fken ismi\n    :param number_of_unique_classes: De\u011fi\u015fkenlerin s\u0131n\u0131flar\u0131n\u0131n frekans s\u0131n\u0131r\u0131\n    :return: \u0130lk de\u011fer olarak kategorik s\u0131n\u0131flar\u0131n ad\u0131n\u0131, ikinci de\u011fer olarak say\u0131sal de\u011fi\u015fkenlerin ad\u0131n\u0131 d\u00f6nd\u00fcr\u00fcr.\n\n    \"\"\"\n\n    categorical_columns = [col for col in dataframe.columns\n                           if len(dataframe[col].unique()) <= number_of_unique_classes]\n\n    numeric_columns = [col for col in dataframe.columns if len(dataframe[col].unique()) > number_of_unique_classes\n                       and dataframe[col].dtype != \"O\"\n                       and col not in exit_columns]\n\n    return categorical_columns, numeric_columns","2406f1b0":"#Kategorik de\u011fi\u015fken analizi(previous_applcation - TARGET)\nprev_target = pd.concat([prev , df.TARGET] , axis = 1)\nprev_target.shape\n\n#Kategorik de\u011fi\u015fken analizi\ncat_cols = [col for col in prev_target.columns if prev_target[col].nunique() < 10]\nprint(\"kategorik degisken say\u0131s\u0131 : \" , len(cat_cols))\n\n#get_categorical_and_numeric_columns(prev_target , \"TARGET\" )\n\ncat_summary(prev_target , \"TARGET\")","cf505d2a":"#Nadirlik durumunun incelenmesi\ndef rare_analyser(dataframe, categorical_columns, target, rare_perc):\n    \"\"\"\n     Data frame de\u011fi\u015fkenlerinin herhangi bir s\u0131n\u0131f\u0131, verilen e\u015fik de\u011ferden d\u00fc\u015f\u00fck frekansa sahipse bu de\u011fi\u015fkenleri g\u00f6sterir.\n    :param dataframe: \u0130\u015flem yap\u0131lacak dataframe\n    :param categorical_columns: Rare analizi yap\u0131lacak kategorik de\u011fi\u015fken adlar\u0131\n    :param target: Analizi yap\u0131lacak hedef de\u011fi\u015fken ad\u0131\n    :param rare_perc: Rare i\u00e7in s\u0131n\u0131r de\u011fer. Alt\u0131nda olanlar rare kategorisine girer.\n    :return:\n    \"\"\"\n    rare_columns = [col for col in categorical_columns\n                    if (dataframe[col].value_counts() \/ len(dataframe) < rare_perc).any(axis=None)]\n\n    for var in rare_columns:\n        print(var, \" : \", len(dataframe[var].value_counts()))\n\n        print(pd.DataFrame({\"COUNT\": dataframe[var].value_counts(),\n                            \"RATIO\": dataframe[var].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(var)[target].mean(),\n                            \"TARGET_MEDIAN\": dataframe.groupby(var)[target].median()}),\n              end=\"\\n\\n\\n\")\n\n    print(len(rare_columns), \" adet rare s\u0131n\u0131fa sahip de\u011fi\u015fken var.\")\n    \nrare_analyser(prev_target ,cat_cols , \"TARGET\" , 0.1)","5468fabe":"#Ayk\u0131r\u0131 degerlerin s\u0131n\u0131rlar\u0131n\u0131n belirlenmesi\ndef outlier_thresholds(dataframe, variable, low_quantile=0.05, up_quantile=0.95):\n    \"\"\"\n    -> Verilen de\u011ferin alt ve \u00fcst ayk\u0131r\u0131 de\u011ferlerini hesaplar ve d\u00f6nd\u00fcr\u00fcr.\n    :param dataframe: \u0130\u015flem yap\u0131lacak dataframe\n    :param variable: Ayk\u0131r\u0131 de\u011feri yakalanacak de\u011fi\u015fkenin ad\u0131\n    :param low_quantile: Alt e\u015fik de\u011ferin hesaplanmas\u0131 i\u00e7in bak\u0131lan quantile de\u011feri\n    :param up_quantile: \u00dcst e\u015fik de\u011ferin hesaplanmas\u0131 i\u00e7in bak\u0131lan quantile de\u011feri\n    :return: \u0130lk de\u011fer olarak verilen de\u011fi\u015fkenin alt s\u0131n\u0131r de\u011ferini, ikinci de\u011fer olarak \u00fcst s\u0131n\u0131r de\u011ferini d\u00f6nd\u00fcr\u00fcr\n    \"\"\"\n    quantile_one = dataframe[variable].quantile(low_quantile)\n\n    quantile_three = dataframe[variable].quantile(up_quantile)\n\n    interquantile_range = quantile_three - quantile_one\n\n    up_limit = quantile_three + 1.5 * interquantile_range\n\n    low_limit = quantile_one - 1.5 * interquantile_range\n\n    return low_limit, up_limit\n","16d3c998":"#Ayk\u0131r\u0131 degerlerin bask\u0131lanmas\u0131\ndef replace_with_thresholds(dataframe, numeric_columns):\n    \"\"\"\n    Bask\u0131lama y\u00f6ntemi\n    Silmemenin en iyi alternatifidir.\n    Loc kullan\u0131ld\u0131\u011f\u0131ndan dataframe i\u00e7inde i\u015flemi uygular.\n    :param dataframe: \u0130\u015flem yap\u0131lacak dataframe\n    :param numeric_columns: Ayk\u0131r\u0131 de\u011ferleri bask\u0131lanacak say\u0131sal de\u011fi\u015fkenlerin adlar\u0131\n    \"\"\"\n    for variable in numeric_columns:\n        low_limit, up_limit = outlier_thresholds(dataframe, variable)\n\n        dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n\n        dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n","47311395":"#Bask\u0131lama i\u015flemi i\u00e7in numerik degi\u015fkenlerin tespiti\nnum_cols = [col for col in prev_target.columns if len(prev_target[col].unique()) > 30\n                 and prev_target[col].dtypes != 'O']\n#Bask\u0131lama isleminin ger\u00e7eklestirilmesi\nreplace_with_thresholds(prev_target, num_cols )","8aa5a2f7":"#Say\u0131sal degiskenlerin(previous_application) TARGET'e gore durumlar\u0131 \ndef target_summary_with_nums(data, target):\n    num_names = [col for col in data.columns if len(data[col].unique()) > 30\n                 and data[col].dtypes != 'O']\n\n    for var in num_names:\n        print(data.groupby(target).agg({var: np.mean}), end=\"\\n\\n\\n\")\n    \ntarget_summary_with_nums(prev_target , \"TARGET\")","72e4e189":"#NUMER\u0130K DEG\u0130SKENLER\u0130N TARGET DEG\u0130SKEN\u0130NE GORE DURUMLARI (previous_application )\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nnum_cols_prev = [col for col in prev_target.columns if prev_target[col].nunique() > 30 ]\n\nlen(num_cols_prev)\n\n#Tum degiskenlerin target degiskenine gore gorsellestirilmesi\nimport seaborn as sns\n\nfor i in num_cols_prev:\n    \n    sns.catplot(x = \"TARGET\"  , y= i ,data = prev_target , kind=\"violin\")\n    ","7d43fea1":"pd.set_option('display.max_rows', None) \nprev_target.corr()","503fe047":"df['NEW_DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']","a803e9c9":"**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**","5bc0a691":"**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"}}