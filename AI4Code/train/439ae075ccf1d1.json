{"cell_type":{"696940f1":"code","3277f61c":"code","a8cb267b":"code","f09c0b18":"code","f4462b0c":"code","beb17761":"code","5d99eb94":"code","0fc4f423":"code","5a7f1411":"code","cc2d892c":"code","654a22dc":"code","26af94b2":"code","63bd0f3a":"code","60c28609":"code","71180721":"code","94ab645b":"code","95a0c510":"code","636d800d":"code","0780c35d":"code","705dbe15":"code","d4212403":"code","b95f1bad":"code","14607499":"code","f6841f8f":"code","134a970d":"code","5c15f21e":"code","4e6b4339":"code","84582499":"code","336c035e":"code","51ab4e25":"code","4e4f0803":"code","fbd3c2dd":"code","a651616f":"code","39673b51":"code","c5c1a973":"code","23c2ec2c":"code","c1ab2cc5":"code","9aba76f4":"code","4d062856":"code","b070d244":"code","55e18d6a":"code","db0b3868":"code","66bdfd6b":"code","ca9fd789":"code","40bb2da7":"code","e7d266ce":"code","fe38645c":"code","c3448dbf":"code","f6448fa9":"code","b40bf775":"code","c04d434a":"code","64628138":"code","4b6a9421":"code","0aa654fb":"code","fa974d84":"code","2bdb017f":"code","95e14dea":"code","137b38ed":"code","d2c4ddd7":"code","63cf3ecd":"code","1e25dfd6":"code","6fd2e767":"code","b7cf7894":"code","5f56b6ea":"code","153c1f79":"code","a13a8a82":"code","fa952ff8":"code","7b7b4854":"code","411c56af":"code","ffa57060":"code","70350d71":"code","60d49436":"code","114eff07":"code","d3984511":"code","c28873c2":"code","0ead82fd":"code","0eb64fab":"code","77f0f1b5":"code","774079ed":"code","9153a7b9":"code","bdd59707":"code","4916bb0f":"code","85944fd0":"code","b9577aa3":"code","9c805bed":"code","91a1f681":"code","15f2caad":"code","f3609543":"code","4effa981":"code","4a1829bc":"code","cd8d06a6":"code","0c413dc3":"code","f7da9579":"code","36686c3b":"code","1955a346":"code","9d55cfa9":"code","29a25600":"code","c05a9709":"code","a3843824":"code","8cec03ae":"code","d2129388":"code","f85cab6b":"code","12e85ebf":"code","14526059":"code","6c8dc385":"code","9727dfbf":"code","3f6819df":"code","d1237594":"code","81ebd455":"code","0895ad5f":"code","d9834726":"code","7b9ea071":"code","9ba8bb57":"code","da8bb605":"code","eaeca32d":"code","a5c04df5":"code","33ec389f":"markdown","a6c75a10":"markdown","58ec6782":"markdown","7c922399":"markdown","9f692df6":"markdown","063f6743":"markdown","a8fcd368":"markdown","3f929b53":"markdown","c74e6c00":"markdown","2f8adab8":"markdown","0df14280":"markdown","a51b9c25":"markdown","32e244a6":"markdown","9e69fbf4":"markdown","c26ed069":"markdown","7ff9caaf":"markdown","cbd7b690":"markdown","c9e454b9":"markdown","82fe92ac":"markdown","3928664b":"markdown","edb7efa4":"markdown","1ee61108":"markdown","bd38b1ce":"markdown","6ffeb143":"markdown","3a4f7361":"markdown","269b615a":"markdown","15052010":"markdown","6e877b8b":"markdown","1e18821f":"markdown","45154b44":"markdown","a72a6f02":"markdown","8aefe91a":"markdown","7a305f64":"markdown","383bfbbe":"markdown","b658414f":"markdown","4589a627":"markdown","b74c74ba":"markdown","7f7be679":"markdown","430913c8":"markdown","ba3617d3":"markdown","d5286058":"markdown","85b2e7bf":"markdown","bdd5cc29":"markdown","4a46c191":"markdown","b0ed1a15":"markdown","7497af6c":"markdown","311c2c1d":"markdown","afc9edba":"markdown","55c2205b":"markdown","040623ff":"markdown","ff033580":"markdown","33248e09":"markdown","86cdca18":"markdown","80935e2b":"markdown","f0a359e3":"markdown","580cabeb":"markdown","09ca340d":"markdown","c7cc0f60":"markdown","a6f76541":"markdown","bd9b856c":"markdown","091e33e8":"markdown","4b2aeae4":"markdown","08d9d5ff":"markdown","64d0e8ec":"markdown","f6a1d0c9":"markdown","9ec6b2fc":"markdown","7947c7d0":"markdown","1da4e42a":"markdown","924e8ef7":"markdown","f398adf4":"markdown","8d8862dd":"markdown","dbf97367":"markdown","9a1b2c6c":"markdown","c473f90e":"markdown","d8a3de43":"markdown","2c70e077":"markdown","ed82d625":"markdown","7abd7dc0":"markdown","f82fecf1":"markdown","9076cd62":"markdown","6b6d9e4e":"markdown","223f8300":"markdown","47600a25":"markdown","614213f2":"markdown","a6c472fa":"markdown","bec10c50":"markdown","a70f8f01":"markdown"},"source":{"696940f1":"# pip install xgboost --upgrade","3277f61c":"# to check version \nimport xgboost as xgb\nxgb.__version__","a8cb267b":"# import os\n# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n# import tensorflow as tf","f09c0b18":"# from tensorflow.python.client import device_lib\n# print(device_lib.list_local_devices())","f4462b0c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge , Lasso, LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import KFold, train_test_split, cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nfrom keras.optimizers import Adam\nimport tensorflow as tf\nimport time\n\n# To ignore unwanted warnings\nimport warnings\nwarnings.filterwarnings('ignore')","beb17761":"df_sales_train0 = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\ndf_shops = pd.read_csv('..\/input\/predict-future-sales-translated-dataset\/shops_en.csv')\ndf_items = pd.read_csv('..\/input\/predict-future-sales-translated-dataset\/items_en.csv')\ndf_catog = pd.read_csv('..\/input\/predict-future-sales-translated-dataset\/item_categories_en.csv')\ndf = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","5d99eb94":"display(df_sales_train0.head())\ndf_sales_train0.shape","0fc4f423":"df_sales_train0['date_block_num'].nunique()","5a7f1411":"df_sales_train0['shop_id'].nunique()","cc2d892c":"df_sales_train0['item_id'].nunique()","654a22dc":"df_catog.head()","26af94b2":"df_catog['item_category_name'].nunique()","63bd0f3a":"df_items.head()","60c28609":"df_items['item_name'].nunique()","71180721":"df_shops.head()","94ab645b":"df_shops['shop_name'].nunique()","95a0c510":"test.head()","636d800d":"df.head()","0780c35d":"df_sales_train0.shape","705dbe15":"#this figure before filtering train data by taking only item_id and shop_id that exists in the test data  \n\nZ = df_sales_train0.groupby('date_block_num').agg({'item_cnt_day': sum}).reset_index()\nfig, ax = plt.subplots(ncols=1, sharey=True, figsize = (20,10))\nsns.barplot(data=Z, x='date_block_num', y='item_cnt_day', ax = ax, palette=\"BrBG\")\nplt.title('Total Sales Per Month', fontsize=25)\nplt.xlabel('Months', fontsize=25)\nplt.ylabel('Sales', fontsize=25);\n","d4212403":"#also this figure before filtering train data by taking only item_id and shop_id that exists in the test data  \n\nprice_per_month = df_sales_train0.groupby('date_block_num').agg({'item_price': 'mean'}).reset_index()\nfig, ax = plt.subplots(ncols=1, sharey=True, figsize = (20,10))\nsns.barplot(data=price_per_month, x='date_block_num', y='item_price', ax = ax, palette=\"BrBG\")\nplt.title('Average Items Price Per Month', fontsize=25)\nplt.xlabel('Months', fontsize=25)\nplt.ylabel('Price', fontsize=25);","b95f1bad":"sns.boxplot(df_sales_train0['item_price'])","14607499":"sns.boxplot(df_sales_train0['item_cnt_day'])","f6841f8f":"#removing item_cnt_day bigger than 1001 and item_price bigger than 100000\ndf_sales_train0 = df_sales_train0[df_sales_train0['item_cnt_day'] < 1001]\ndf_sales_train0 = df_sales_train0[df_sales_train0['item_price'] < 100000]","134a970d":"from itertools import product\ndf_sales_train = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = df_sales_train0[df_sales_train0['date_block_num'] == i]\n    df_sales_train.append(np.array(list(product([i], sales['shop_id'].unique(), sales['item_id'].unique())), dtype='int16'))\n    \ndf_sales_train = pd.DataFrame(np.vstack(df_sales_train), columns=cols)","5c15f21e":"df_sales_train0['revenue'] = df_sales_train0['item_price'] *  df_sales_train0['item_cnt_day']","4e6b4339":"agg = df_sales_train0.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\nagg.columns = ['item_cnt_month']\nagg.reset_index(inplace=True)\nmerge1 = pd.merge(df_sales_train, agg, on=cols, how='left')","84582499":"test['date_block_num'] = 34","336c035e":"test_shop_ids = test['shop_id'].unique()\ntest_item_ids = test['item_id'].unique()\n# Only shops that exist in test set.\nmerge1 = merge1[merge1['shop_id'].isin(test_shop_ids)]\n# Only items that exist in test set.\nmerge1 = merge1[merge1['item_id'].isin(test_item_ids)]\nmerge1.reset_index(inplace=True, drop=True)","51ab4e25":"merge1 = pd.concat([merge1, test], ignore_index=True, sort=False, keys=['date_block_num','shop_id','item_id'])\nmerge1.fillna(0, inplace=True)","4e4f0803":"merge1 = pd.merge(merge1, df_shops, on=['shop_id'], how='left')\nmerge1 = pd.merge(merge1, df_items, on=['item_id'], how='left')\nmerge1 = pd.merge(merge1, df_catog, on=['item_category_id'], how='left')","fbd3c2dd":"merge1.loc[merge1.shop_id == 0, 'shop_id'] = 57\nmerge1.loc[merge1.shop_id == 1, 'shop_id'] = 58\nmerge1.loc[merge1.shop_id == 10, 'shop_id'] = 11","a651616f":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","39673b51":"merge1 = lag_feature(merge1, [1,2,3,6,12], 'item_cnt_month')  # Create months lags","c5c1a973":"CPI_inflation = [0.97,0.56,0.34,0.51,0.66,0.42,0.82,0.14,0.21,0.57,0.57,0.50,\n                 0.59,0.70,1.02,0.90,0.90,0.62,0.49,0.24,0.65,0.82,1.28,2.62,\n                 3.85,2.22,1.21,0.46,0.35,0.19,0.80,0.35,0.57,0.74,0.75]\nmerge1['CPI_inflation'] = 0\nfor i, value in enumerate(CPI_inflation):\n    merge1['CPI_inflation'][merge1['date_block_num'] == i ] = value\n\nmerge1 = lag_feature(merge1, [1,2,3,6,12], 'CPI_inflation')  # Creating lags for CPI_inflation","23c2ec2c":"agg = merge1.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\nagg.columns = [ 'date_avg_item_cnt' ]\nagg.reset_index(inplace=True)\n\nmerge1 = pd.merge(merge1, agg, on=['date_block_num'], how='left')\nmerge1 = lag_feature(merge1, [1], 'date_avg_item_cnt')\n# merge1.drop(['date_avg_item_cnt'], axis=1, inplace=True)","c1ab2cc5":"agg = merge1.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\nagg.columns = [ 'date_item_avg_item_cnt' ]\nagg.reset_index(inplace=True)\n\nmerge1 = pd.merge(merge1, agg, on=['date_block_num','item_id'], how='left')\nmerge1 = lag_feature(merge1, [1,2,3,6,12], 'date_item_avg_item_cnt')\n# merge1.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)","9aba76f4":"agg = merge1.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\nagg.columns = [ 'date_shop_avg_item_cnt' ]\nagg.reset_index(inplace=True)\n\nmerge1 = pd.merge(merge1, agg, on=['date_block_num','shop_id'], how='left')\nmerge1 = lag_feature(merge1, [1,2,3,6,12], 'date_shop_avg_item_cnt')\n# merge1.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\n","4d062856":"agg = merge1.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\nagg.columns = [ 'date_cat_avg_item_cnt' ]\nagg.reset_index(inplace=True)\n\nmerge1 = pd.merge(merge1, agg, on=['date_block_num','item_category_id'], how='left')\nmerge1 = lag_feature(merge1, [1], 'date_cat_avg_item_cnt')\n# merge1.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\n","b070d244":"agg = merge1.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\nagg.columns = ['date_shop_cat_avg_item_cnt']\nagg.reset_index(inplace=True)\n\nmerge1 = pd.merge(merge1, agg, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmerge1 = lag_feature(merge1, [1], 'date_shop_cat_avg_item_cnt')\n# merge1.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\n","55e18d6a":"agg =df_sales_train0.groupby(['item_id']).agg({'item_price': ['mean']})\nagg.columns = ['item_avg_item_price']\nagg.reset_index(inplace=True)\nmerge1 = pd.merge(merge1, agg, on=['item_id'], how='left')","db0b3868":"agg = df_sales_train0.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\nagg.columns = ['date_item_avg_item_price']\nagg.reset_index(inplace=True)\n\nmerge1 = pd.merge(merge1, agg, on=['date_block_num','item_id'], how='left')\nlags = [1,2,3,4,5,6]\nmerge1 = lag_feature(merge1, lags, 'date_item_avg_item_price')","66bdfd6b":"for i in lags:\n    merge1['delta_price_lag_'+str(i)] = \\\n        (merge1['date_item_avg_item_price_lag_'+str(i)] - merge1['item_avg_item_price']) \/ merge1['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmerge1['delta_price_lag'] = merge1.apply(select_trend, axis=1)\nmerge1['delta_price_lag'].fillna(0, inplace=True)","ca9fd789":"fetures_to_drop = []\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmerge1.drop(fetures_to_drop, axis=1, inplace=True)\n","40bb2da7":"agg = df_sales_train0.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\nagg.columns = ['date_shop_revenue']\nagg.reset_index(inplace=True)\n\nmerge1 = pd.merge(merge1, agg, on=['date_block_num','shop_id'], how='left')","e7d266ce":"agg = merge1.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\nagg.columns = ['shop_avg_revenue']\nagg.reset_index(inplace=True)\n\nmerge1 = pd.merge(merge1, agg, on=['shop_id'], how='left')","fe38645c":"merge1['delta_revenue'] = (merge1['date_shop_revenue'] - merge1['shop_avg_revenue']) \/ merge1['shop_avg_revenue']\n\nmerge1 = lag_feature(merge1, [1], 'delta_revenue')\n","c3448dbf":"merge1['month'] = merge1['date_block_num'] % 12","f6448fa9":"days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmerge1['days'] = merge1['month'].map(days)","b40bf775":"cache = {}\nmerge1['item_shop_last_sale'] = -1\nfor idx, row in merge1.iterrows():    \n    key = str(row['item_id'])+' '+str(row['shop_id'])\n    if key not in cache:\n        if row['item_cnt_month']!=0:\n            cache[key] = row['date_block_num']\n    else:\n        last_date_block_num = cache[key]\n        merge1.at[idx, 'item_shop_last_sale'] = row['date_block_num'] - last_date_block_num\n        cache[key] = row['date_block_num']  ","c04d434a":"cache = {}\nmerge1['item_last_sale'] = -1\nfor idx, row in merge1.iterrows():    \n    key = row['item_id']\n    if key not in cache:\n        if row['item_cnt_month']!=0:\n            cache[key] = row['date_block_num']\n    else:\n        last_date_block_num = cache[key]\n        if row['date_block_num']>last_date_block_num:\n            merge1.at[idx, 'item_last_sale'] = row['date_block_num'] - last_date_block_num\n            cache[key] = row['date_block_num']   ","64628138":"merge1['item_shop_first_sale'] = merge1['date_block_num'] - merge1.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmerge1['item_first_sale'] = merge1['date_block_num'] - merge1.groupby('item_id')['date_block_num'].transform('min')","4b6a9421":"merge1['gdp'] = 0\nmerge1['gdp'][merge1['date_block_num'] < 25 ] = 14101\nmerge1['gdp'][merge1['date_block_num'] > 24 ] = 9314","0aa654fb":"merge1.isnull().sum().sum()","fa974d84":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)\n            elif('CPI_inflation' in col):\n                df[col].fillna(0, inplace=True)        \n    return df","2bdb017f":"merge1 = fill_na(merge1)","95e14dea":"merge1['shop_name'].unique()","137b38ed":"merge1['shop_name'].replace('! Yakutsk Ordzhonikidze, 56 Franc' , 'Yakutsk Ordzhonikidze, 56' , inplace=True) \nmerge1['shop_name'].replace('! Yakutsk TC \"Central\" Franc' , 'Yakutsk TC \"Central\"' , inplace=True) \nmerge1['shop_name'].replace('St. Petersburg TK \"Nevsky Center\"' , 'Petersburg TK \"Nevsky Center\"' , inplace=True) \nmerge1['shop_name'].replace('Shop Online Emergencies' , 'online Shop Emergencies' , inplace=True) \nmerge1['shop_name'].replace('Digital storage 1C-line' , 'online Digital storage 1C-line' , inplace=True)\nmerge1['shop_name'].replace('Zhukovsky Street. Chkalov 39m?' , 'Zhukovsky Street. Chkalov 39m\u00b2' , inplace=True)","d2c4ddd7":"merge1['city'] = merge1['shop_name'].str.split(' ').map(lambda x: x[0])","63cf3ecd":"merge1['item_category_name'].unique()","1e25dfd6":"merge1['split'] = merge1['item_category_name'].str.split('-')\nmerge1['category_type'] = merge1['split'].map(lambda x: x[0].strip())\nmerge1['category_subtype'] = merge1['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nmerge1.drop(columns=['item_category_name' , 'split'] , inplace=True , axis=1)","6fd2e767":"merge1['category_subtype'].replace('Blu','BluRay' , inplace=True)\nmerge1['category_type'].replace('Movies','Movie' , inplace=True)","b7cf7894":"merge1['category_type'].unique()","5f56b6ea":"merge1['category_subtype'].unique()","153c1f79":"# merge1.to_csv('merge1_Nodummies.csv')","a13a8a82":"# merge1 = pd.read_csv('merge1_Nodummies.csv' , index_col='Unnamed: 0')","fa952ff8":"# merge1_Nodummies = pd.read_csv('merge1_Nodummies.csv' , index_col='Unnamed: 0')","7b7b4854":"merge1_Nodummies - merge1.copy()","411c56af":"merge1_Nodummies = merge1_Nodummies[merge1_Nodummies['date_block_num'] < 34]\nmerge1_Nodummies = merge1_Nodummies[merge1_Nodummies['item_cnt_month'] > 0]","ffa57060":"import plotly.express as px\npie_fig = merge1_Nodummies.groupby('category_type').agg({'item_cnt_month': sum}).\\\nsort_values(by='item_cnt_month',ascending=False).reset_index()\nfig = px.pie(pie_fig, values='item_cnt_month', names='category_type', title='Percentage of Sales Per Category')\nfig.show()","70350d71":"import plotly.express as px\nsub_pie = merge1_Nodummies.groupby('category_subtype').agg({'item_cnt_month': sum}).\\\nsort_values(by='item_cnt_month',ascending=False).reset_index()\nfig = px.pie(sub_pie, values='item_cnt_month', names='category_subtype', title='Percentage of Sales Per Sub Category')\nfig.show()","60d49436":"Z = merge1_Nodummies.groupby('category_type').agg({'item_cnt_month': sum}).sort_values(by='item_cnt_month',ascending=False).reset_index()\nfig, ax = plt.subplots(ncols=1, sharey=True, figsize = (20,16))\nsns.barplot(data=Z, x='item_cnt_month', y='category_type', palette=\"magma\",orient='h')\nplt.yticks()\nplt.ylabel('')\nplt.xlabel('')\nplt.title('Total Categories Sales', fontsize=25)\nplt.xticks([i for i in range(0, 350000, 20000)], fontsize=14);","114eff07":"Z = merge1_Nodummies.groupby('category_subtype').agg({'item_cnt_month': sum}).sort_values(by='item_cnt_month',ascending=False).reset_index()\nfig, ax = plt.subplots(ncols=1, sharey=True, figsize = (20,16))\nsns.barplot(data=Z, x='item_cnt_month', y='category_subtype', palette=\"magma\",orient='h')\nplt.yticks(fontsize=16)\nplt.ylabel('')\nplt.xlabel('')\nplt.title('Total Sub Categories Sales', fontsize=25)\nplt.xticks([i for i in range(0, 240000, 20000)], fontsize=16);","d3984511":"Z = merge1_Nodummies.groupby('city').agg({'item_cnt_month': sum}).sort_values('item_cnt_month', ascending=False).reset_index()\nfig, ax = plt.subplots(ncols=1, sharey=True, figsize = (20,14))\nsns.barplot(data=Z, x='item_cnt_month', y='city', palette=\"gist_earth\",orient='h')\nplt.ylabel('')\nplt.xlabel('')\nplt.title('Number of Sales Per City', fontsize=25)\nplt.yticks(fontsize=16)\nplt.xticks([i for i in range(0, 600000, 30000)]);","c28873c2":"p = merge1_Nodummies.groupby('date_block_num').agg({'item_cnt_month': sum}).reset_index() \ng = sns.relplot(x=\"date_block_num\", y=\"item_cnt_month\",palette=[\"b\", \"r\"], ci=None, kind=\"line\", data=p)\ng.fig.set_size_inches(15,8)\nplt.title('Monthly items sales for 3 years')\nplt.xlabel('Months')\nplt.ylabel('Sales')\nplt.xticks([i for i in range(0, 35)]);","0ead82fd":"Z = merge1_Nodummies.groupby('date_block_num').agg({'item_cnt_month': sum}).reset_index()\nfig, ax = plt.subplots(ncols=1, sharey=True, figsize = (20,10))\nsns.barplot(data=Z, x='date_block_num', y='item_cnt_month', ax = ax, palette=\"BrBG\")\nplt.title('Total Sales Per Month', fontsize=25)\nplt.xlabel('Months', fontsize=25)\nplt.ylabel('Sales', fontsize=25);","0eb64fab":"Z = merge1_Nodummies.groupby('date_block_num').agg({'item_avg_item_price': sum}).reset_index()\nfig, ax = plt.subplots(figsize = (20,10))\nsns.barplot(data=Z, x='date_block_num', y='item_avg_item_price', ax = ax, palette=\"BrBG\")\nax.set_yticklabels(['{:,}'.format(int(x)) for x in ax.get_yticks().tolist()])\nplt.title('Total Prices Per Month', fontsize=25)\nplt.xlabel('Months', fontsize=25)\nplt.ylabel('Price', fontsize=25);","77f0f1b5":"Z = merge1.groupby('date_block_num').agg({'item_id': 'nunique'}).reset_index()\nfig, ax = plt.subplots(figsize = (20,10))\nsns.barplot(data=Z, x='date_block_num', y='item_id', ax = ax, palette=\"cool_r\")\nplt.title('Number of Unique Items', fontsize=25)\nplt.xlabel('Months', fontsize=25)\nplt.ylabel('Count', fontsize=25);","774079ed":"Z = merge1_Nodummies.groupby('shop_name').agg({'item_cnt_month': sum})\\\n    .sort_values(by='item_cnt_month', ascending=False).reset_index()\nfig, ax = plt.subplots(figsize = (22,18))\nsns.barplot(data=Z, x='item_cnt_month', y='shop_name', palette=\"copper\",orient='h', ax=ax)\nplt.title('Shops Sales', fontsize=25)\nplt.xlabel('')\nplt.ylabel('')\nplt.yticks(fontsize=18)\nplt.xticks([i for i in range(0, 170000, 10000)], fontsize=16);","9153a7b9":"merge1 = pd.get_dummies(merge1, columns=['city','category_type','category_subtype'],drop_first=True)","bdd59707":"# del df_catog\n# del df_items\n# del df_shops\n# del df_sales_train0\n# del merge2\n# del merge1_Nodummies","4916bb0f":"merge1.isnull().sum().sort_values().tail(10)","85944fd0":"merge1.drop([\n             'date_shop_cat_avg_item_cnt' ,\n             'date_avg_item_cnt' ,\n             'date_item_avg_item_cnt',\n             'date_shop_avg_item_cnt',\n             'date_cat_avg_item_cnt','shop_name',\n             'ID',\n             'item_avg_item_price',\n             'date_item_avg_item_price',\n             'date_shop_revenue',\n             'shop_avg_revenue',\n             'delta_revenue',\n             'delta_revenue_lag_1'\n             ], axis=1, inplace=True)","b9577aa3":"merge1 = merge1[merge1['date_block_num'] > 11]","9c805bed":"# Top Correlations\n\npercent=0.20 \ncor_train=merge1.corr()\nhigh_corre = cor_train.index[abs(cor_train[\"item_cnt_month\"])>percent]\n\n#to sort columns from highest correlation with item_cnt_month\nsorted_cols = cor_train.nlargest(len(high_corre),\n'item_cnt_month')['item_cnt_month'].index \n\nplt.figure(figsize=(15,13))\nsns.set(font_scale=1.5)\n\n#plot heatmap with only the top features\nnr_corr_matrix = sns.heatmap(merge1[sorted_cols].corr(),\nannot=True,cmap=\"BrBG\",square=True, annot_kws={'size':14})","91a1f681":"for col in merge1.columns:\n    if col == 'date_shop_revenue':\n        merge1[col] = merge1[col].astype('float64')\n    elif col == 'item_cnt_month':\n        merge1[col] = merge1[col].astype('float32')\n    elif merge1[col].dtype == 'float64':\n        merge1[col] = merge1[col].astype('float16')\n    elif col == 'item_id':\n        merge1[col] = merge1[col].astype('int16')\n    elif merge1[col].dtype == 'int64':\n        merge1[col] = merge1[col].astype('int8')","15f2caad":"merge1.isnull().sum().sort_values().tail(10)","f3609543":"merge1['item_cnt_month'] = merge1['item_cnt_month'].clip(0,20)","4effa981":"X_plot= merge1[merge1['date_block_num'] < 34]","4a1829bc":"X = merge1[merge1['date_block_num'] < 34].drop(['item_cnt_month', 'item_name'], axis=1)\ny = merge1[merge1['date_block_num']< 34]['item_cnt_month']","cd8d06a6":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=.1, random_state = 42)","0c413dc3":"testing = merge1[merge1['date_block_num'] == 34].drop(['item_cnt_month', 'item_name'], axis=1)","f7da9579":"del X\ndel y\ndel Z\ndel cor_train\ndel pie_fig\ndel sub_pie\ndel p\n# del cache\n# del agg\n# del df_sales_train","36686c3b":"import gc\ngc.collect()","1955a346":"# test size = .1\n# shuffle = True\n# random_state =42 \n# xgboost version = '1.1.0'\n# google colab\n\n##### Model\n# ts = time.time()\n\n# xgb_model = xgb.XGBRegressor(eta=0.01,\n#                                  max_depth=11,n_estimators=1400,\n#                                  alpha=2,\n#                                  n_jobs=-1,\n#                                  tree_method='gpu_hist'\n#                                  )\n\n# xgb_hist = xgb_model.fit(X_train,y_train,\n#                          eval_set=[(X_train,y_train),(X_test,y_test)],\n#                          eval_metric='rmse',\n#                          early_stopping_rounds=10)\n\n# time.time() - ts\n\n# results:\n# [0]\tvalidation_0-rmse:1.54692\tvalidation_1-rmse:1.52665\n# [1399]\tvalidation_0-rmse:0.73776\tvalidation_1-rmse:0.88586\n# Train Score: 0.7750457101368959\n# Test Score : 0.6667570149949471\n# Kaggle Score 0.89995","9d55cfa9":"ts = time.time()\n\nxgb_model = xgb.XGBRegressor(eta=0.01,\n                                 max_depth=11,n_estimators=1400,\n                                 alpha=2,\n                                 n_jobs=-1,\n                                 tree_method='gpu_hist'\n                                 )\n\nxgb_hist = xgb_model.fit(X_train,y_train,\n                         eval_set=[(X_train,y_train),(X_test,y_test)],\n                         eval_metric='rmse',\n                         early_stopping_rounds=10)\n\n\n# tree_method='gpu_hist', gpu_id=0\ntime.time() - ts","29a25600":"y_predV = xgb_model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, y_predV)))\ny_pred = xgb_model.predict(testing)","c05a9709":"xgb_model.get_params","a3843824":"print('Train Score:', xgb_model.score(X_train, y_train))\nprint('Test Score :', xgb_model.score(X_test, y_test))","8cec03ae":"# cv=KFold(n_splits=5, shuffle=True, random_state=1)\n# cross_val_score(xgb_model, X, y, cv=cv).mean()","d2129388":"features_importance = xgb_model.get_booster().get_fscore()\nf_results = pd.DataFrame(features_importance.items(), columns=['feature_name', 'fscore'])\nf_results.sort_values(by='fscore',ascending=False,inplace=True)\nf_results.reset_index(inplace=True)\ntop_features = f_results['feature_name']","f85cab6b":"top_features","12e85ebf":"results = xgb_model.evals_result()\nepochs = len(results['validation_0']['rmse'])\nx_axis = range(0, epochs)","14526059":"fig, ax = plt.subplots(figsize=(12,6))\nax.plot(x_axis, results['validation_0']['rmse'], label='Train')\nax.plot(x_axis, results['validation_1']['rmse'], label='Test')\nax.legend()\nplt.ylabel('rmse')\nplt.title('XGBoost')\nplt.show()","6c8dc385":"fig, ax = plt.subplots(figsize=(14,33))\nxgb.plot_importance(xgb_model, ax);","9727dfbf":"#here we will get item_name from date_block_number 33 (month 33) \ny_predV_Df_test = pd.DataFrame({'item_id' : X_test['item_id'],'shop_id' : X_test['shop_id'] ,'item_cnt_month' : y_predV  }) \ntop_10_items = y_predV_Df_test.sort_values(by='item_cnt_month' , ascending=False).head(20)\ntop_10_items['date_block_num'] = 33 \nitem_name = []\nfor i in top_10_items['item_id'].values:\n    \n    top = pd.merge(top_10_items,merge1[['item_name' , 'item_id','shop_id','date_block_num']],on=['item_id','shop_id','date_block_num'],how='left')#\n    \ntop['date_block_num'] = 34","3f6819df":"top.head(10)","d1237594":"y_predV_Df = pd.DataFrame({'date_block_num' : X_test['date_block_num'] ,'item_cnt_month' : y_predV  })","81ebd455":"y_test_Df = pd.DataFrame({'date_block_num' : X_test['date_block_num'] ,'item_cnt_month' : y_test  })","0895ad5f":"fig , ax = plt.subplots(ncols=1 , figsize=(16,8))\nxl = y_test_Df.groupby('date_block_num').agg({'item_cnt_month': sum}).reset_index()\ny_predsum = y_predV_Df.groupby('date_block_num').agg({'item_cnt_month': sum}).reset_index()\nxl.plot(x='date_block_num',y='item_cnt_month' , kind='line' , ax=ax ,  linewidth=2 , c='b')\ny_predsum.plot(x='date_block_num',y='item_cnt_month' , kind='line' , ax=ax , linewidth=2 , c='orange' )\nplt.legend(['Actual' , 'Predicted'])\nplt.xlabel('Months' , fontsize=25)\n# y_predsum.plot(x='date_block_num',y='item_cnt_month', kind='line' , ax=ax)\nax.set_xticks([i for i in range(12, 35)]);\n# ax.set_yticks([i for i in range(0, 20000, 3000)]);","d9834726":"y_predDf = pd.DataFrame({'date_block_num' : 34 ,'item_cnt_month' : y_pred  })\ny_predsum = y_predDf.groupby('date_block_num').agg({'item_cnt_month': sum}).reset_index()\nxl = X_plot.groupby('date_block_num').agg({'item_cnt_month': sum}).reset_index()\nmk = pd.concat([xl, y_predsum])","7b9ea071":"fig , ax = plt.subplots(ncols=1 , figsize=(16,8))\nxl = X_plot.groupby('date_block_num').agg({'item_cnt_month': sum}).reset_index()\nmk.plot(x='date_block_num',y='item_cnt_month' , kind='line' , ax=ax ,  linewidth=2 , c='orange')\nxl.plot(x='date_block_num',y='item_cnt_month' , kind='line' , ax=ax , linewidth=2 , c='b' )\nax.legend(['Test' , 'Train'])\nplt.xlabel('Months' , fontsize=20)\nax.set_xticks([i for i in range(12, 35)]);","9ba8bb57":"fig, ax = plt.subplots(figsize = (20,10))\nsns.barplot(data=mk, x='date_block_num', y='item_cnt_month', ax = ax, palette=\"BrBG\")\nplt.title('Total Sales Per Month', fontsize=25)\nplt.xlabel('Months', fontsize=25)\nplt.ylabel('Sales', fontsize=25);","da8bb605":"import lightgbm as lgb\n\nts = time.time()\ntrain_data = lgb.Dataset(data=X_train, label=y_train)\nvalid_data = lgb.Dataset(data=X_test, label=y_test)\n\ntime.time() - ts\n    \nparams = {'num_leaves': 2000, 'max_depth': 19, 'max_bin': 107, 'n_estimators': 1100,\n          'bagging_freq': 1, 'bagging_fraction': 0.7135681370918421, \n          'feature_fraction': 0.49446461478601994, 'min_data_in_leaf': 88, \n          'learning_rate': 0.01, 'num_threads': 3, \n          'min_sum_hessian_in_leaf': 6,\n         \n          'verbosity' : 1,\n          'boost_from_average' : 'true',\n          'boost' : 'gbdt',\n          'metric' : 'rmse',}\nlgb_model = lgb.train(params, train_data, valid_sets=[train_data, valid_data], verbose_eval=1, num_boost_round=20)\n\n############################################\n# Kaggle Score : 0.90560\n# [1]\ttraining's rmse: 1.54864\tvalid_1's rmse: 1.52789\n# [1100]\ttraining's rmse: 0.809505\tvalid_1's rmse: 0.887232","eaeca32d":"y_predV = lgb_model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, y_predV)))\ny_pred = lgb_model.predict(testing)","a5c04df5":"thesubmission = df.copy()\nthesubmission['item_cnt_month'] = y_pred.clip(0,20)\nthesubmission.to_csv('Xgboostlastone.csv', index=False)\nthesubmission['item_cnt_month'].head()","33ec389f":"Create a new column \"date_shop_avg_item_cnt\" wich will take the average for each shop sales per month ","a6c75a10":"Monthly Sales per city ","58ec6782":"In the evaluation section from the data provider they stated that the target in the test data is clipped to (0,20) so we did the same thing in the train data to get better results, we tried modeling without clipping and the results were bad, it seems clipping is better at handling outliers <br>\n**Reference:** https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales\/overview\/evaluation\n ","7c922399":"## Lightgbm","9f692df6":"<a id='the_destination_12'><\/a>\n\n# 12.Evaluation","063f6743":"<a id='the_destination_4'><\/a>\n# 4. Exploring the target","a8fcd368":"<a id='the_destination_6'><\/a>\n\n# 6. Feature Engineering","3f929b53":"<a id='the_destination_7'><\/a>\n\n### 7. Fill lags missing values","c74e6c00":"<a id='the_destination_2'><\/a>\n## import required libraries","2f8adab8":"fix spelling ","0df14280":"Monthly Sales per category type ","a51b9c25":"#### The notebook is classified to the following sections:\n<a href='#the_destination_1'>1. Problem Statement<\/a><br>\n<a href='#the_destination_2'>2. Import Data<\/a><br>\n-   <a href='#the_destination_2.1'>2.1 Data Dictionary<\/a><br>\n\n<a href='#the_destination_3'>3. Exploratory Data Analysis (EDA)<\/a><br>\n<a href='#the_destination_4'>4. Exploring the target<\/a><br>\n<a href='#the_destination_5'>5. Detecting Outliers<\/a><br>\n<a href='#the_destination_6'>6. Feature Engineering<\/a><br>\n-   <a href='#the_destination_6.1'>6.1 merging test with train<\/a><br>\n\n<a href='#the_destination_7'>7 Fill lags missing values<\/a><br>\n<a href='#the_destination_8'>8. visualization<\/a><br>\n<a href='#the_destination_9'>9. Dummies<\/a><br>\n\n<a href='#the_destination_10'>10. Modeling<\/a>\n\n   -   <a href='#the_destination_10.1'>10.1 Xgboost<\/a>\n\n   -   <a href='#the_destination_10.2'>10.2 Sequential model<\/a>\n\n   -   <a href='#the_destination_10.3'>10.3 LSTM<\/a>\n\n   -   <a href='#the_destination_10.4'>10.4 Random Forest Regressor Model<\/a>\n\n   -   <a href='#the_destination_10.5'>10.5 LinearRegression<\/a>\n\n   -   <a href='#the_destination_10.6'>10.6 Ridge<\/a>\n\n   -   <a href='#the_destination_10.7'>10.7 Build Logistic Regression Model<\/a>\n\n          \n\n<a href='#the_destination_11'>11. Submission<\/a><br>\n<a href='#the_destination_13'>12. Evaluation<\/a><br>\n<a href='#the_destination_13'>13. Conclusion and Recommendations<\/a><br>\n<a href='#the_destination_14'>14. References<\/a><br>","32e244a6":"Create a new columns : \n - \"category_type\" which will extract category_type from item_category_name\n - \"category_subtype\" which will extract category_subtype from item_category_name","9e69fbf4":"This graph shows change of sales per month \n\n","c26ed069":"Create a new column \"date_avg_item_cnt\" wich will take the average of all items sales per month","7ff9caaf":"Datasets : https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales\/data <br>\nCPI: Consumer price index : https:\/\/www.inflation.eu\/inflation-rates\/russia\/historic-inflation\/cpi-inflation-russia.aspx <br>\nRussia GDP Per Capita : https:\/\/www.macrotrends.net\/countries\/RUS\/russia\/gdp-per-capita","cbd7b690":"### Ploting the train and predicted test","c9e454b9":"--------------------","82fe92ac":"--------------------","3928664b":"Because the data in time series structure, Here we are creating a function which will create a lag columns to see how the data changes from time to time","edb7efa4":"<a id='the_destination_9'><\/a>\n# 9. Dummies\n##### Creating Dummies For Categorical Columns.","1ee61108":"<br><br><br>**To use tensorflow in CPU uncomment the code below, And for GPU leave it commented**","bd38b1ce":"--------------------","6ffeb143":"<a id='the_destination_3'><\/a>\n# 3. EDA","3a4f7361":"to save memory uncomment the cell below , which will delete unneeded variables","269b615a":"### Compairing the actual values with predicted values","15052010":"Create a 'date_block_num' column in test dataset, And all of it will be 34 because all the test data in the same month ","6e877b8b":"Create a new column \"item_shop_last_sale\" which will count duration since last sale for each item and shop","1e18821f":"Our data is too big so to save memory and modeling time we will change types for all columns ","45154b44":"<a id='the_destination_10.1'><\/a>\n\n### 10.1 Xgboost","a72a6f02":"This data gathered from russian stores, So here we are adding a new column 'CPI_inflation' which contains monthly CPI inflation in russia from 2013 to the end of 2015 (CPI: Consumer price index)<br>","8aefe91a":"<a id='the_destination_8'><\/a>\n\n# 8. Visualization","7a305f64":"--------------------","383bfbbe":"**Importing the dataset**","b658414f":"Create a new lag columns which will contains the percent of the difference between average item price in lag months and average price for each item ","4589a627":"Here are the predicted top 10 items we expect to have high sales in the next month","b74c74ba":"Here we are merging train and test data ","7f7be679":"Create a new column \"item_last_sale\" which will count duration since last sale for each item","430913c8":"Here submissions are evaluated by root mean squared error (RMSE). <br>\nTrue target values are clipped into [0,20] range. <br>\n<br>\nBest model<br>\nModel:             XGBoost<br>\nRuntime:           3 Minutes<br>\nkaggle Score(RMSE):       0.89\n","ba3617d3":"Create a new column \"city\" from the first word in shop names","d5286058":"<a id='the_destination_13'><\/a>\n\n# 13. Conclusion and Recommendations","85b2e7bf":"### also Ploting the train and predicted test","bdd5cc29":"## Visualizing XGBoost results","4a46c191":"--------------------","b0ed1a15":"This data gathered from russian stores, So here we are adding a new column 'gdp' which contains yearly gdp in russia from 2013 to the end of 2015 (gdp: Gross domestic product)<br>\n","7497af6c":"# <center>Forecast Future Sales    <\/center>","311c2c1d":"### Install XGBOOST 1.1.0","afc9edba":"Create a new column \"date_cat_avg_item_cnt\" wich will take the average for each category sales per month ","55c2205b":"### Saving the data  after we fineshed feature engineering ","040623ff":"Create a new column \"date_item_avg_item_price\" wich will take the average price for each item per month","ff033580":"all the first words in shop names contains city names so here we will fix spaces and special characters","33248e09":"Create a new column \"date_shop_revenue\" wich will take the sum of revenue for each shop per month","86cdca18":"<a id='the_destination_14'><\/a>\n\n# 14. References","80935e2b":"<a id='the_destination_2.1'><\/a>\n#### 2.1 Data Dictionary\n\n|Feature|Dataset|Description|\n|-------|---|---|\n|ID|test|an Id that represents a (Shop, Item) tuple within the test set.| \n|Shop_id|df_shops\/test\/df_sales_train0|unique identifier of a shop.| \n|item_id|df_items\/test\/df_sales_train0|unique identifier of a product.| \n|item_category_id |df_catog\/df_sales_train0\/df_items|unique identifier of item category.| \n|item_cnt_day|df_sales_train0|number of products sold. You are predicting a monthly amount of this measure.| \n|item_price|df_sales_train0|current price of an item.| \n|date|df_sales_train0|date in format dd\/mm\/yyyy.| \n|date_block_num|df_sales_train0|a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33.| \n|item_name|df_sales_train0\/df_items|name of item.|\n|shop_name|df_sales_train0\/df_shops|name of shop.|\n|item_category_name|df_sales_train0\/df_catog|name of item category.|","f0a359e3":"<a id='the_destination_1'><\/a>\n# 1. Problem Statement","580cabeb":"Create a 'revenue' column from multiplying 'item_price' with 'item_cnt_day'","09ca340d":"Create a new column \"month\" wich will take month number from date_block_num","c7cc0f60":"we will remove first 12 months because we are using 12 as lag","a6f76541":"transform our data from daily to monthly by using grouby ","bd9b856c":"Create a new lag columns which will contains the percent of the difference between average shop revenue in lag months and sum of revenue for each shop per month","091e33e8":"drop unnecessary columns","4b2aeae4":"we don't need all lags columns we create it to get delta here we will drop it ","08d9d5ff":"Merging our dataframe with shops and items and categories datasets ","64d0e8ec":"<a id='the_destination_11'><\/a>\n\n## 11. Submission","f6a1d0c9":"<a id='the_destination_5'><\/a>\n### 5. Detecting Outliers","9ec6b2fc":"We are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Notice that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.","7947c7d0":"Create a new column \"date_item_avg_item_cnt\" wich will take the average for each item sales per month ","1da4e42a":"--------------------","924e8ef7":"Gathering all combinations of : [date_block_num , shop_id , item_id] because we will transform our data from daily to monthly","f398adf4":"<a id='the_destination_6.1'><\/a>\n\n#### 6.1 merging test with train","8d8862dd":"Create a new column \"date_shop_revenue\" wich will take the avreage of revenue for each shop","dbf97367":"Correct duplicate values","9a1b2c6c":"<a id='the_destination_10'><\/a>\n\n# 10. Modeling","c473f90e":"### Best Submission parameters","d8a3de43":"Monthly Sales per category subtype ","2c70e077":"--------------------","ed82d625":"## After testing a lot of models, and after parameters tuning, we found that XGBoost is the best model","7abd7dc0":"Create a new column \"date_shop_cat_avg_item_cnt\" wich will take the average for each shop with category sales per month ","f82fecf1":"<a id='the_destination_2'><\/a>\n# 2. Importing Datasets","9076cd62":"Create a new column \"item_avg_item_price\" wich will take the average price for each item ","6b6d9e4e":"After we saw that the sales well increase in November, we advise the shops to expect a lot of customers ,\nalso after analyzing the data we noticed that over 65% of the sales come from entertainment sector, also our model expect that 4 of the top 10 selling items in the next month will come from the entertainment sector\nSo we recommend shops in the entertainment sector to be ready for high demand from customers","223f8300":"filter our data to select only sales for visualization","47600a25":"--------------------","614213f2":"Create a new column \"days\" wich will take Number of days in a month. \nThere are no leap years.\n\n","a6c472fa":"Here we are filtering the train data to only keep 'shop_id' and 'item_id' that exist in test data","bec10c50":"Create a new columns :\n-   \"item_shop_first_sale\" which will count duration since first sale for each item and shop\n\n-   \"item_first_sale\" which will count duration since first sale for each item ","a70f8f01":"Monthly Sales per category type "}}