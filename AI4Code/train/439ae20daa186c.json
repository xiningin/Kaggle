{"cell_type":{"fde47b45":"code","68497f1e":"code","2f14060a":"code","7c919872":"code","ef5dc5c7":"code","d9e081a3":"code","f98c7168":"code","3e789d91":"code","311f6c25":"code","6339ad6a":"code","9de41261":"code","923ce113":"code","a9da456f":"code","7be392da":"code","90444ac8":"code","978b9b9c":"code","814005ef":"code","b91e7769":"code","43e53e83":"code","93febe82":"code","c02ab83c":"code","0c1953e0":"code","0d2aa352":"code","a6379fb7":"code","9539bee5":"markdown","f2e35a80":"markdown","dee5e75b":"markdown","1c5c4c77":"markdown","4df85f58":"markdown","9e2363b3":"markdown","318fd052":"markdown"},"source":{"fde47b45":"import numpy as np \nimport pandas as pd \nimport bz2\nimport gc\nimport re\nimport os\nprint(os.listdir(\"..\/input\"))","68497f1e":"train_file = bz2.BZ2File('..\/input\/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('..\/input\/test.ft.txt.bz2')","2f14060a":"train_file_lines = train_file.readlines()\ntest_file_lines = test_file.readlines()","7c919872":"del train_file, test_file\ngc.collect()","ef5dc5c7":"train_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]","d9e081a3":"train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n    \ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n                                                       \nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])","f98c7168":"del train_file_lines, test_file_lines\ntrain_sentences[0]","3e789d91":"gc.collect()","311f6c25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\n\nfrom wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output","6339ad6a":"# Create train and test dataframes\nNa_train = {'Sentence': train_sentences, 'Label': train_labels}\nNav_train = pd.DataFrame(Na_train)\n\nNa_test = {'Sentence': test_sentences, 'Label': test_labels}\nNav_test = pd.DataFrame(Na_test)\n\nNav_train.head()\n\nNav_train = Nav_train.head(900)\nNav_test = Nav_test.head(100)\n\n","9de41261":"#train_pos = Nav_train[Nav_train['Label'] == 1]\n#train_pos = Nav_train['Sentence']\n#train_neg = Nav_train[Nav_train['Label'] == 0]\n#train_neg = Nav_train['Sentence']\n\ntest_pos = Nav_test[Nav_test['Label'] == 1]\ntest_pos = Nav_test['Sentence']\ntest_neg = Nav_test[Nav_test['Label'] == 0]\ntest_neg = Nav_test['Sentence']","923ce113":"del Na_train, Na_test, train_sentences, train_labels\ngc.collect()","a9da456f":"                \nsents = []\nalll = []\nstopwords_set = set(stopwords.words(\"english\"))\n\nfor index, row in Nav_train.iterrows():\n    words_filtered = [e.lower() for e in row.Sentence.split() if len(e) >= 3]\n    words_cleaned = [word for word in words_filtered\n        if 'http' not in word\n        and not word.startswith('@')\n        and not word.startswith('#')\n        and word != 'RT']\n    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n    sents.append((words_without_stopwords, row.Label))\n    alll.extend(words_without_stopwords )\n    ","7be392da":"def get_word_features(wordlist):\n    wordlist = nltk.FreqDist(wordlist)\n    features = wordlist.keys()\n    return features\n","90444ac8":"#teee = get_words_in_tweets(sents)\n","978b9b9c":"\nw_features = get_word_features(alll)\n# TESTING BELOW\n","814005ef":"\ndef extract_features(document):\n    document_words = set(document)\n    features = {}\n    for word in w_features:\n        features['contains(%s)' % word] = (word in document_words)\n    return features","b91e7769":"# Training the Naive Bayes classifier\ntraining_set = nltk.classify.apply_features(extract_features,sents)\n","43e53e83":"classifier = nltk.NaiveBayesClassifier.train(training_set)\n","93febe82":"train_pos = Nav_train[Nav_train['Label'] == 1]\ntrain_pos = train_pos['Sentence']\ntrain_neg = Nav_train[Nav_train['Label'] == 0]\ntrain_neg = train_neg['Sentence']\ntest_pos = Nav_test[Nav_test['Label'] == 1]\ntest_pos = test_pos['Sentence']\ntest_neg = Nav_test[Nav_test['Label'] == 0]\ntest_neg = test_neg['Sentence']","c02ab83c":"test_neg.head(40)","0c1953e0":"neg_cnt = 0\npos_cnt = 0\nfor obj in test_neg: \n    res =  classifier.classify(extract_features(obj.split()))\n    if(res == 0): \n        neg_cnt = neg_cnt + 1\nfor obj in test_pos: \n    res =  classifier.classify(extract_features(obj.split()))\n    if(res == 1): \n        pos_cnt = pos_cnt + 1\n        \nprint('[Negative]: %s\/%s '  % (len(test_neg),neg_cnt))        \nprint('[Positive]: %s\/%s '  % (len(test_pos),pos_cnt))  \n\n\n\n","0d2aa352":"#aa = classifier.evaluate(Nav_test['Sentence'],Nav_test['Label'])\nacccc= ((neg_cnt+pos_cnt)\/(len(test_neg)+len(test_pos))) * 100\nprint(\"Accuracy by nltk classifier is\", acccc)","a6379fb7":"print(test_neg.loc[52])\nclassifier.classify(extract_features(test_neg.loc[52].split()))","9539bee5":"# Read & Preprocess data","f2e35a80":"\n## Cleaning and Feature Extraction","dee5e75b":"### Separate Positive and Negative tweets","1c5c4c77":"## Create Lists containing Train & Test sentences","4df85f58":"-# Extracting word features\ndef get_words_in_tweets(tweets):\n    alll = []\n    for (words, sentiment) in tweets:\n        alll.extend(words)\n    return alll\n","9e2363b3":"## Convert from raw binary strings to strings that can be parsed","318fd052":"# 1: Using NLTK Naive Bayes"}}