{"cell_type":{"80af23e5":"code","f42f973d":"code","cf06f2f0":"code","94d737a4":"code","bde0c206":"code","497fcec2":"code","43c2fbf8":"code","f236b1ca":"code","b6889912":"code","c69da54d":"code","aba362a5":"code","d3d954c5":"code","6bf04dbf":"code","563df47e":"code","4ae3cf13":"code","4ee2a9ea":"code","ad4ea333":"code","2d959517":"code","7143ae18":"code","3052cffb":"code","2b19a483":"code","eb889bfd":"code","ecdcae04":"code","ec435838":"code","ae726dfd":"code","c474edba":"code","1ca814cd":"code","27b5c642":"code","9c049703":"code","4d941d35":"code","e0ba03b4":"code","b3f99501":"code","b3369d5f":"markdown","27186731":"markdown","24659770":"markdown","5fe11df2":"markdown","dee17bdf":"markdown","82cc0173":"markdown","2af428e7":"markdown","8c6c5aa2":"markdown","940f5a04":"markdown","a804a76d":"markdown","7b3e5830":"markdown","c02aa6a8":"markdown","bcaed9c3":"markdown","95145fdf":"markdown","8ae62a3d":"markdown","c4672f63":"markdown","a39f1822":"markdown","f897f1ef":"markdown","ce9777fb":"markdown","1e11c253":"markdown","c30acf5b":"markdown","68e31891":"markdown","24f34932":"markdown","eb49ced7":"markdown","3503a6ab":"markdown","a0f712db":"markdown"},"source":{"80af23e5":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f42f973d":"import pandas as pd\n\nbiorxiv = pd.read_csv('\/kaggle\/input\/cordcsvs\/biorxiv_medrxiv.csv')\n\nnoncomm_sub = pd.read_csv('\/kaggle\/input\/cordcsvs\/noncomm_use_subset.csv')\n\ncomm_use_sub =  pd.read_csv('\/kaggle\/input\/cordcsvs\/comm_use_subset.csv')\n\nmetadata =  pd.read_csv('\/kaggle\/input\/cordcsvs\/metadata.csv')\n\ncustom_license =  pd.read_csv('\/kaggle\/input\/cordcsvs\/custom_license.csv')\n\npieces = (biorxiv, noncomm_sub, comm_use_sub, custom_license)\n\ndf_minus_metadata = pd.concat(pieces, ignore_index = True)\n\ndf_minus_metadata.head(10)","cf06f2f0":"# Thanks, Andy White! https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions\ndef get_terms():\n    term_list = ['covid',\n                    'coronavirus disease 19',\n                    'sars cov 2', # Note that search function replaces '-' with ' '\n                    '2019 ncov',\n                    '2019ncov',\n                    r'2019 n cov\\b',\n                    r'2019n cov\\b',\n                    'ncov 2019',\n                    r'\\bn cov 2019',\n                    'coronavirus 2019',\n                    'wuhan pneumonia',\n                    'wuhan virus',\n                    'wuhan coronavirus',\n                    r'coronavirus 2\\b']\n    return term_list","94d737a4":"!pip install fuzzywuzzy","bde0c206":"!pip install python-levenshtein","497fcec2":"from fuzzywuzzy import fuzz \nfrom fuzzywuzzy import process \n\ntext_df = df_minus_metadata\n\ntext_df['paragraphs'] = text_df['text'].str.split('\\n\\n')\n\ncombo_dict = {'paper_id':[], 'title':[], 'authors':[], 'term':[],'partial_ratio':[], 'paragraphs':[]}\n\n","43c2fbf8":"term_list = get_terms()\n\nfor ind in text_df.index:\n    title = text_df['title'][ind]\n    paras = text_df['paragraphs'][ind]\n    paper_id = text_df['paper_id'][ind]\n    authors = text_df['authors'][ind]\n    text = text_df['text'][ind]\n    affils = text_df['affiliations'][ind]\n    abstract = text_df['abstract'][ind]\n    bib = text_df['bibliography'][ind]\n    raw_auth = text_df['raw_authors'][ind]\n    raw_bib = text_df['raw_bibliography'][ind]\n    \n    for term in term_list:\n        for p in paras:\n            if len(term) < len(p):\n                partial_ratio = fuzz.partial_ratio(term, p)\n\n                if partial_ratio >= 90:\n                    combo_dict[\"title\"].append(title)\n                    combo_dict[\"paper_id\"].append(paper_id)\n                    combo_dict[\"authors\"].append(authors)\n                    combo_dict[\"paragraphs\"].append(p)\n                    combo_dict[\"term\"].append(term)\n                    combo_dict[\"partial_ratio\"].append(partial_ratio)","f236b1ca":"results_df = pd.DataFrame.from_dict(combo_dict)\n\nresults_df = results_df.drop_duplicates()\nresults_df.head(5)","b6889912":"# due to computational limitations of notebook, uploaded as input data\nresults_df.to_csv('relevant_covidpapers.csv')","c69da54d":"dataset = pd.read_csv('\/kaggle\/input\/labeled-relevant-papers\/relevant_covidpapers_labeled.csv')\ndataset = dataset[['paragraphs', 'label (1-11)']]\n\ndataset.rename(columns = {'paragraphs':'paragraph','label (1-11)':'label'}, inplace = True)\n\ndataset = dataset.dropna()\ndataset.head(10)","aba362a5":"import matplotlib.pyplot as plt\ndataset.groupby('label').count().plot.bar(ylim=0)\nplt.show()","d3d954c5":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(400)\nimport nltk\nnltk.download('wordnet')","6bf04dbf":"X = dataset['paragraph']\ny = dataset['label']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","563df47e":"def lemmatize_stemming(text):\n    stemmer = PorterStemmer()\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\n# Tokenize and lemmatize\ndef preprocess(text):\n    result=[]\n    for token in gensim.utils.simple_preprocess(text) :\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n            \n    return result","4ae3cf13":"processed_docs = []\n\nfor doc in X_train:\n    processed_docs.append(preprocess(doc))\n\ndictionary = gensim.corpora.Dictionary(processed_docs)","4ee2a9ea":"count = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","ad4ea333":"'''\nOPTIONAL STEP\nRemove very rare and very common words:\n\n- words appearing less than 15 times\n- words appearing in more than 10% of all documents\n'''\n\ndictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)","2d959517":"'''\nCreate the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\nwords and how many times those words appear. Save this to 'bow_corpus'\n'''\nbow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]","7143ae18":"'''\nTrain your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n'''\n# TODO\nlda_model =  gensim.models.LdaMulticore(bow_corpus, \n                                   num_topics = 10, \n                                   id2word = dictionary,                                    \n                                   passes = 10,\n                                   workers = 2)","3052cffb":"for idx, topic in lda_model.print_topics(-1):\n    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n    print(\"\\n\")","2b19a483":"num = 0\nunseen_document = X_test.iloc[num]\nprint(unseen_document)","eb889bfd":"# Data preprocessing step for the unseen document\nbow_vector = dictionary.doc2bow(preprocess(unseen_document))\n\nfor index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))","ecdcae04":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport operator\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sqlite3 import Error\nfrom sklearn.ensemble import RandomForestClassifier\nimport sqlite3\nimport pickle","ec435838":"X = dataset['paragraph']\ny = dataset['label']","ae726dfd":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","c474edba":"stemmer = PorterStemmer()\nwords = stopwords.words(\"english\")\ndataset['cleaned'] = dataset['paragraph'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())","1ca814cd":"vectorizer = TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))\nfinal_features = vectorizer.fit_transform(dataset['cleaned']).toarray()\nfinal_features.shape # (373, 7475) so out of 373 paragraphs we got 7475 features","27b5c642":"#first we split our dataset into testing and training set:\n# this block is to split the dataset into training and testing set \nX = dataset['cleaned']\nY = dataset['label']\nindices = dataset.index.values\n\nX_train, X_test,indices_train,indices_test = train_test_split(X, indices, test_size=0.25, random_state=42)\n\ny_train, y_test = Y[indices_train],  Y[indices_test]\n\n# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 42)\n\n\n\n# instead of doing these steps one at a time, we can use a pipeline to complete them all at once\npipeline = Pipeline([('vect', vectorizer),\n                     ('chi',  SelectKBest(chi2, k=1200)),\n                     ('clf', RandomForestClassifier())])\n# fitting our model and save it in a pickle for later use\nmodel = pipeline.fit(X_train, y_train)\nwith open('RandomForest.pickle', 'wb') as f:\n    pickle.dump(model, f)\nytest = np.array(y_test)\n# confusion matrix and classification report(precision, recall, F1-score)\nprint(classification_report(ytest, model.predict(X_test)))\nprint(confusion_matrix(ytest, model.predict(X_test)))","9c049703":"dataset.loc[indices_test,'pred_test'] = model.predict(X_test)\ndataset","4d941d35":"import pandas as pd\ndf = pd.read_csv('\/kaggle\/input\/labeled-relevant-papers\/relevant_covidpapers_labeled.csv')\ndf = df[['paragraphs', 'label (1-11)']]\n\ndf.rename(columns = {'paragraphs':'paragraph','label (1-11)':'label'}, inplace = True)\n\ndf = df.dropna()\ndf.head(10)\n\nimport itertools\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as pyplot\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.preprocessing import text, sequence\nfrom keras import utils, optimizers, models\n\n\n\nfrom keras import backend as K\ntf.random.set_seed(1234)\n\n# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n# K.set_session(sess)\n\ntrain_size = int(len(df) * .7)\ntrain_posts = df['paragraph'][:train_size]\ntrain_tags = df['label'][:train_size]\n\ntest_posts = df['paragraph'][train_size:]\ntest_tags = df['label'][train_size:]\n\nmax_words = 1000\ntokenize = text.Tokenizer(num_words=max_words, char_level=False)\ntokenize.fit_on_texts(train_posts)  # only fit on train\n\nx_train = tokenize.texts_to_matrix(train_posts)\nx_test = tokenize.texts_to_matrix(test_posts)\n\nencoder = LabelEncoder()\nencoder.fit(train_tags)\ny_train = encoder.transform(train_tags)\ny_test = encoder.transform(test_tags)\n\nnum_classes = np.max(y_train) + 1\ny_train = utils.to_categorical(y_train, num_classes)\ny_test = utils.to_categorical(y_test, num_classes)\n\nbatch_size = 32\nepochs = 300\n\n# Build the model\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\nrmsprop = optimizers.rmsprop(lr=0.001)\nsgd = optimizers.sgd(lr=0.001)\nadam = optimizers.adam(lr=0.001)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_split=0.1)\n\ntrain_acc = model.evaluate(x_train, y_train, verbose=0)\ntest_acc = model.evaluate(x_test, y_test, verbose=0)\n","e0ba03b4":"pyplot.subplot(211)\npyplot.title('Loss')\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Accuracy')\npyplot.plot(history.history['accuracy'], label='train')\npyplot.plot(history.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()","b3f99501":"# # serialize model to JSON\/save\n# model_json = model.to_json()\n# with open(\"BoW_ann_model.json\", \"w+\") as json_file:\n#     json_file.write(model_json)\n# # serialize weights to HDF5\n# model.save_weights(\"model.h5\")\n# print(\"Saved model to disk\")\n\n\nscore = model.evaluate(x_test, y_test,\n                       batch_size=batch_size, verbose=1)\nprint('Test accuracy:', score[1])","b3369d5f":"It appears that the topic with therapy, image, approval, chest, and anti matches this paragraph the best to a score of 41.8% \"accuracy.\"","27186731":"## Introduction \n\nThis submission is for the task, \u201cWhat do we know about therapeutics and vaccines?\u201d\n\nScientific information concerning COVID-19 is expanding rapidly and changing daily.   There is no such thing as a definitive study or publication on the therapeutic management of COVID-19 post infection or vaccination to prevent it. Rather, there are multiple incremental \u201cpieces\u201d of information scattered across many studies that in combination might provide new insights for scientific researchers. \n\nThe successful identification, integration, and use of published literature with  these \u201cpieces\u201d of information  will advance the field and move us faster to the discovery of therapies or a vaccine.   Identifying and managing these pieces of scientific information is possible via the use of Natural Language Processing (NLP) and Artificial Intelligence (AI) techniques.   Once a successful model based on NLP and AI techniques has been developed, it can be run again and again,  as the literature database on COVID-19 continues to grow. We are fortunate that as members of the biotechnology industry, we are able to investigate the data from a scientific lens, which helps us to identify ways to label datasets with our domain knowledge, a key driver for successful modeling.","24659770":"The key takeaway is that for each arbitrary topic, because the topic numbers aren't the same as the numbers we set in our labels, you get a grouping of the most simplest form of the important words for that cluster. It is up to the human to try to match each cluster to 1 of our 11 focus areas. It's a bit difficult to tell for this which leads us to conclude we could have benefitted from even stricter sampling of paragraphs.","5fe11df2":"# Abstract\nWe present an approach for narrowing down the dataset to relevant COVID-19 papers using fuzzy string matching comparing keywords with matches in paragraphs via the fuzzywuzzy library. Through utilizing a targeted dataset, manual labeling of 355 paragraphs, each from a separate paper, and training an Artificial Neural Network (ANN), specifically Multilayer Perceptron utilizing Stochastic Gradient Descent, we were able to achieve multiclass classification accuracy of 65.2%","dee17bdf":"# Model 3: Training a Multilayer Perceptron on the Bag-of-Words textual representation\n\nThis is an Artificial Neural Network (ANN) that uses a feedforward and a backpropagation process in order to update weights of the hidden layers. We selected the Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.001 to get our best result.\n\nMore information on ANNs and SGD can be found at: https:\/\/medium.com\/@AI_with_Kain\/understanding-of-multilayer-perceptron-mlp-8f179c4a135f and https:\/\/scikit-learn.org\/stable\/modules\/sgd.html ","82cc0173":"Below, we've commented out code that will save the model to your disk, but we also display the **accuracy** of the model, which was **65.2%**, much better than the 50% we were getting with the One Vs. Rest Logistic Regression. This is actually our best result.","2af428e7":"When we graph the distribution of labels. The distribution is not uniform. Right off the bat, we're probably not going to see good predictions for certain topics that have few samples.","8c6c5aa2":"Install the fuzzywuzzy and python-levenshtein libraries:","940f5a04":"Our first step was to filter papers from a list of relevant terms combined with sentiment analysis to create a model that could predict which papers were most relevant to vaccine and therapeutic development, our Task\u2019s focus area. We did not invest much time in researching and implementing a machine-based mechanism for labeling papers with relevance to our understanding of the Task\u2019s goal, so we attempted to label the papers manually gave each paper a score from 1-5 with 1 correlating to a negative sentiment and 5 being a positive sentiment. We recognized this was far from perfect as it was hard to control for bias in the labeling and was constrained by time available for human effort. However, we believed it was critical to know the type and variety of papers in the database to identify possible ways to maximize model performance, i.e. don\u2019t rely solely on the application of AI methods --- know your data. With about 200 papers labeled, the data was split into testing and training sets and inputted into a One Vs. Rest Logistic Regression model, which with ~50% accuracy wasn\u2019t able to predict relevant papers particularly well.\n\nOur subsequent approach emerged from us realizing we might be able to improve the model by improving the data we\u2019re feeding into it. So we did another round of manual labeling, this time labeling ~350 papers that most aligned to one of our Task\u2019s 11 focus areas (we split the first question into 2 separate focus areas). The distribution of papers aligned to the 11 focus area was non-uniform, which likely affected the model's ability to predict as it's highly dependent on the number of samples for each focus area. ","a804a76d":"The juicy bit of fuzzywuzzy. This takes a bit of time to run so you can view the dataset in the input data as relevant_covidpapers.csv under fuzzywuzzy-output:","7b3e5830":"The result is an accuracy of 62%","c02aa6a8":"## Modeling \n\nFor our model, we experimented with several text classification models to effectively predict upon our categorized data. We experimented with a Doc2Vec model, Bag-of-Words (BoW) model with Latent Dirichlet Allocation (LDA), Bag-of-Words (BoW) Artificial Neural Network(ANN), and a Random Forest model. The strategy was to see if we could utilize our targeted dataset to provide a more accurate base for training the model upon, since all paragraphs will have some sort of relation to coronavirus, covid-19, etc. We will showcase the LDA + BoW model and our winner, the BoW ANN. ","bcaed9c3":"# Model 2: Random Forest + TF-IDF (Term Frequency-Inverse Document Frequency)\nTF-IDF attempts to identify in all of your words in the dataset, how important a word is to a document in a collection or corpus. We use this instead of the Bag-of-Words for the Random Forest model.","95145fdf":"### Accuracy: 65.2%","8ae62a3d":"# Model 1: LDA with Bag-of-Words (BoW)\nThe thought process behind using LDA with BoW would be to try to better visualize which words are grouped together before taking a document and calculating the fit for each group and perhaps get a more accurate model than simply using BoW.","c4672f63":"Looking at an example of what the truncated words would look like in their most clean form:","a39f1822":"# Obtaining a targeted dataset: Python fuzzywuzzy\nThe **fuzzwuzzy** package is developed by Seatgeek, the company most known for their platform for reselling tickets to sporting events. They leveraged the traditional Levenshtein Distance calculations to form a basis for determining string similarity. There are several scores, but the one we found that would best suit our needs was the Partial Ratio metric, which compares the length of the shortest string against all n-length substrings of the larger string and returns the highest match score, with 100 being a perfect match. We used a threshold score of 90 and returned matching paragraphs to each of the following terms: `'covid'`, `'coronavirus disease 19'`, `'sars cov 2'`, `'2019 ncov'`, `'2019ncov'`, `r'2019 n cov\\b'`, `r'2019n cov\\b'`, `'ncov 2019'`, `r'\\bn cov 2019'`, `'coronavirus 2019'`, `'wuhan pneumonia'`, `'wuhan virus'`, `'wuhan coronavirus'`, `r'coronavirus 2\\b'`. The `r''` pattern just denotes that it\u2019s utilizing a regex pattern to search. Special thanks to Andy White for sharing his notebook on which relevant terms were worthy of searching (https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions)\n\nThe result of running the below code using the fuzzywuzzy package is a list of list of 355 matching paragraphs that match the search terms we searched for. This dataset is called `relevant_covidpapers_labeled.csv` and it is the dataset we utilized in our successful approach. Below is the code for that process:\n","f897f1ef":"First, we imported the CORD-19 jSON files that we had converted to csv for each topic area. Special thanks to xhlulu (https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv) for sharing the notebook on how to get from jSON to csv.  ","ce9777fb":"\n## Problem Statement and Approach \n\nThere are many possible NLP and AI approaches  to extract insights from the literature by topic or subject matter.  In fact, the number of options available can be a challenge with respect to rapid identification and selection of the best approach or highest performing model, measured as the ability of an algorithm to identify high value papers or ones that contain \u201cpieces\u201d of information that will advance development of therapies to treat COVID-19 or the creation of a vaccine to prevent it. \n","1e11c253":"Create a new column in our data called `paragraphs` and then split the text in `text` into paragraphs. Create `combo_dict` which will store our data before we convert dataframe to csv for manual labeling.","c30acf5b":"Below is the code for plotting the train vs. test loss curves and accuracy. You can see that over the course of 300 epochs, the test curve closely follows the train curve, a good sign of the validity of the accuracy of the model when applied on a test dataset.","68e31891":"Let's try one of our test documents are try to predict which topic it falls into:","24f34932":"## Results\n\nWe found that our best results were from the deep learning model trained on the vectors generated via the Bag of Words text representation with an **accuracy of 65.2 percent**. The second best model was our Random Forest model with TF-IDF which had an accuracy of 62 percent. Perhaps we can glean the most insight from our LDA + Bag of Words model because the output of LDA shows us the groupings of words. Upon examination, these groupings were not easily matched to each of the questions listed in our Kaggle task. Thus, our initial insight is that perhaps we could have benefitted from further refining the data, as well as increasing the number of observations in our data. We discuss further suggestions for model improvement in our next section.\n\n## Discussion of Model Improvement\n\nWe noticed that our best scores for models were associated with the most general sections of the text in papers ( for example the introductory paragraph, or the abstract). The issues with this is that the best opportunity for scientific insights are likely to be later in the paper such as in the results or conclusion sections. Therefore, we propose to link these later sections of a given paper with the high scoring intro paragraphs in a future model. In this way we can capture the appropriate high level ideas (those from the introduction) as well as the details needed for scientific insights (those from the results or conclusions sections). \n\nWe also observed that this COVID-19 literature dataset did not appear to have as much definitive literature on vaccines and therapeutics compared to some of the other Tasks' topics like transmission and disease characteristics. The volume of relevant data is a factor in model performance, and given our Task focus areas on vaccines and therapeutics, we believe more vaccine and therapeutic related literature would improve the model performance. We also did not discover highly relevant papers amongst those we reviewed that stood out as a \"gold standard\" to use in our training data or as a reference in labeling the papers.\n\n\n## Key Takeaway\n\nOur approach was not intended to identify specific vaccines and treatments for COVID-19 directly. Our goal was to identify the most relevant literature for each focus area in our task. We do think our effort yielded a valuable result by furthering this hybrid human\/machine approach, one that relies on creating labeled data to have higher quality inputs into a more targeted literature search engine. We demonstrated as a proof of concept that having basic scientific literacy to determine relevance of a training set was good enough to build a model that improved predictive accuracy of relevant literature. We think further iterations of this approach, such as by crowdsourcing the labeling effort from a scientific community, could yield even more accurate and valuable literature identification.\n\nFrom a technical perspective, using the fuzzywuzzy partial ratio metric can help to identify matches with greater flexibility than `regex`, something we think the community should consider as `regex` only finds exact matches while fuzzywuzzy can account for mispellings in the document, or multi-word terms that may be rearranged in different orders but mean the same thing. (cheeseburger vs. burger with cheese). Finally, even with a loosely labeled dataset, we find that the deep learning approach is extremely powerful in mapping those nuanced associations that may not immediately be clear simply on the textual representations alone.","eb49ced7":"# Labeling Data:\nWe took our output from fuzzywuzzy and created labels based on the focus areas for our task:\n\n1 = \"Effectiveness of drugs being developed and tried to treat COVID-19 patients.\"\n2 = \"Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\"\n3 = \"Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\"\n4 = \"Exploration of use of best animal models and their predictive value for a human vaccine.\"\n5 = \"Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\"\n6 = \"Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\"\n7 = \"Efforts targeted at a universal coronavirus vaccine.\"\n8 = \"Efforts to develop animal models and standardize challenge studies.\"\n9 = \"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers.\"\n10 = \"Approaches to evaluate risk for enhanced disease after vaccination\"\n11 = \"Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]\"\n\nWe went through each paragraph and labeled to the best of our abilities which focus area each paragraph corresponded to. For those that did not match any, we labeled it as 0. The resulting dataset is `relevant_covidpapers_labeled.csv`","3503a6ab":"We tested a number of models, and those with low performance were quickly eliminated. Other models, discussed in the section below, were pursued with more promising results. The results were also more useful from this approach as we could predict papers that would be more relevant for the specific focus area.","a0f712db":"Define our list of terms to search for:"}}