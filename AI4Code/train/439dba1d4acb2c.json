{"cell_type":{"50cec794":"code","9e265ad6":"code","a6496641":"code","e6bf2886":"code","a59d87e2":"code","0a638058":"code","6c5e872a":"code","eb4a1922":"code","d7875452":"code","783c3f0a":"code","d6e295e3":"code","5bf91c42":"code","bf22ade3":"code","61a36595":"code","1f71f320":"code","0c93a8ef":"code","a7fe9ff7":"code","cf67f1f8":"code","9ef69838":"code","3515a3a6":"code","2628901a":"code","c34fffec":"code","1625fa34":"code","8cf9acd0":"code","1715c333":"markdown","86f6fd78":"markdown","d068ac3f":"markdown","097bad5d":"markdown","f306a279":"markdown","ded45ba5":"markdown"},"source":{"50cec794":"import numpy as np\nimport pandas as pd\nfrom fastai.text import *\nfrom fastai.callbacks import *","9e265ad6":"path = Path('..\/input\/nlp-getting-started')\npath.ls()","a6496641":"train = pd.read_csv(path\/'train.csv')\ntest = pd.read_csv(path\/'test.csv')","e6bf2886":"data_lm = (TextList.from_df(pd.concat([train[['text']], test[['text']]], ignore_index=True, axis=0))\n           .split_by_rand_pct(0.15)\n           .label_for_lm()\n           .databunch(bs=128))","a59d87e2":"data_lm.show_batch()","0a638058":"## create lm learner with pre-trained model\nlearn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)","6c5e872a":"learn.lr_find()\nlearn.recorder.plot(suggestion = True)","eb4a1922":"learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))","d7875452":"callback = SaveModelCallback(learn,monitor=\"accuracy\", mode=\"max\", name=\"best_lang_model\")","783c3f0a":"learn.unfreeze()\nlearn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7), callbacks=[callback])","d6e295e3":"learn.save('fine_tuned')\nlearn.save_encoder('fine_tuned_enc')","5bf91c42":"df = train[['text', 'target']]","bf22ade3":"df_test = test[['text']]","61a36595":"data_clas = (TextList.from_df(df, vocab=data_lm.vocab)\n             #.split_none()\n             .split_by_rand_pct(0.1)\n             .label_from_df('target')\n             .add_test(TextList.from_df(df_test, vocab=data_lm.vocab))\n             .databunch(bs=128))","1f71f320":"## check test set looks ok\ndata_clas.show_batch(ds_type=DatasetType.Test)","0c93a8ef":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, FBeta(beta=1)])\nlearn.load_encoder('fine_tuned_enc')","a7fe9ff7":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","cf67f1f8":"learn.fit_one_cycle(1, 1e-3, moms=(0.8,0.7))","9ef69838":"## unfreeze the last 2 layers and train for 1 cycle\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-3\/(2.6**4),1e-2), moms=(0.8,0.7))","3515a3a6":"## unfreeze the last 3 layers and train for 1 cycle\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3), moms=(0.8,0.7))","2628901a":"callbacks = SaveModelCallback(learn,monitor=\"accuracy\", mode=\"max\", name=\"best_classification_model\")","c34fffec":"## unfreeze all and train for 2 cycles\nlearn.unfreeze()\nlearn.fit_one_cycle(15, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7), callbacks=[callbacks])","1625fa34":"preds, _ = learn.get_preds(ds_type=DatasetType.Test,  ordered=True)\npreds = preds.argmax(dim=-1)\n\nid = test['id']","8cf9acd0":"my_submission = pd.DataFrame({'id': id, 'target': preds})\nmy_submission.to_csv('submission.csv', index=False)","1715c333":"Importing the libraries","86f6fd78":"Save the language model and the encoder too","d068ac3f":"We will use both our training and test text for our language model. Pay attention to the fact that we are not taking any labels here. We are not building a classifier now rather we are buliding a language model where we are using a pretrained architecture trained on the Wikitext-103 and then training it on our tweet data.\n\nThe paragraph given below is taken from [Understanding building blocks of ULMFIT](https:\/\/medium.com\/mlreview\/understanding-building-blocks-of-ulmfit-818d3775325b#:~:text=High%20level%20idea%20of%20ULMFIT,learning%20rates%20in%20multiple%20stages)\n> High level idea of ULMFIT is to train a language model using a very large corpus like Wikitext-103 (103M tokens), then to take this pretrained model\u2019s encoder and combine it with a custom head model, e.g. for classification, and to do the good old fine tuning using discriminative learning rates in multiple stages carefully.\nArchitecture that ULMFIT uses for it\u2019s language modeling task is an [AWD-LSTM](https:\/\/arxiv.org\/pdf\/1708.02182.pdf). The name is an abbreviation of ASGD Weight-Dropped LSTM.\n\nRefer to this paper of you want to read more about ULMFiT : https:\/\/arxiv.org\/abs\/1801.06146\n\nULMFiT brought the concept of transfer learning in Computer Vision to NLP","097bad5d":"Building the Classifier with the same encoder from the language model. ","f306a279":"Using callbacks to select the best classification model out of the given epochs","ded45ba5":"Creating a Text Databunch for our classifier. We are taking 10% as validation set and keeping the vocabulary same as the language model databunch"}}