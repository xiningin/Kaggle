{"cell_type":{"dc0685bb":"code","864c109f":"code","9cc75e57":"code","831e6c89":"code","e23f1264":"code","e99ad55a":"code","e1e3dac3":"code","ef0751b0":"code","27f24612":"code","0cbb2736":"code","1f681b4d":"code","491786af":"code","d39e6ff5":"code","988940c2":"markdown","c47189c0":"markdown","8170b8a9":"markdown","6154c9d9":"markdown"},"source":{"dc0685bb":"import numpy as np\nimport pandas as pd\ndf = pd.read_csv(\"..\/input\/water-potability\/water_potability.csv\")","864c109f":"df.isna().sum()","9cc75e57":"df = df.dropna()\ndf.isna().sum()","831e6c89":"df.head(5)","e23f1264":"import seaborn as sns\nimport matplotlib.pyplot as plt\nax = sns.countplot(df.Potability,label=\"Count\") \nnot_potable, potable = df.Potability.value_counts()\nprint('Number of Potable: ',potable)\nprint('Number of Not Potable : ',not_potable)","e99ad55a":"X = df.drop(columns = [\"Potability\"])\ny = df.Potability","e1e3dac3":"data_dia = y\ndata = X\ndata_n_2 = (data - data.mean()) \/ (data.std())              \ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"Potability\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"Potability\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","ef0751b0":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(X.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","27f24612":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\naccuricies = []\nn_estimators = []\nmax_acc = 0\nmax_i = 0\nfor i in range(1,100):\n    clf_rf = RandomForestClassifier(random_state=42, n_estimators=i)      \n    clr_rf = clf_rf.fit(x_train,y_train)\n    ac = accuracy_score(y_test,clf_rf.predict(x_test))\n    if ac > max_acc:\n        max_acc = ac\n        max_i = i\n    accuricies.append(ac)\n    n_estimators.append(i)\nplt.plot(n_estimators, accuricies)\nprint(\"Max : \", max_acc, max_i)","0cbb2736":"import seaborn as sns\nimport matplotlib.pyplot as plt\nax = sns.countplot(y_train,label=\"Count\") \nnot_potable, potable = y_train.value_counts()\nprint('Number of Potable: ',potable)\nprint('Number of Not Potable : ',not_potable)","1f681b4d":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 42)\nx_train_res, y_train_res = sm.fit_resample(x_train, y_train.ravel())","491786af":"import seaborn as sns\nimport matplotlib.pyplot as plt\nax = sns.countplot(y_train_res,label=\"Count\") \nnot_potable, potable = y_train_res.value_counts()\nprint('Number of Potable: ',potable)\nprint('Number of Not Potable : ',not_potable)","d39e6ff5":"accuricies = []\nn_estimators = []\nmax_acc = 0\nmax_i = 0\nfor i in range(1,100):\n    clf_rf = RandomForestClassifier(random_state=42, n_estimators=i)      \n    clr_rf = clf_rf.fit(x_train_res,y_train_res)\n    ac = accuracy_score(y_test,clf_rf.predict(x_test))\n    if ac > max_acc:\n        max_acc = ac\n        max_i = i\n    accuricies.append(ac)\n    n_estimators.append(i)\nplt.plot(n_estimators, accuricies)\nprint(\"Max : \", max_acc, max_i)","988940c2":"**We reached 0.7 accuracy with n = 34. Next, We will try SMOTE beacuse our data unbalanced**","c47189c0":"**From this figure we can say data not good for training. If we look Sulfate and Organic_carbon feature medians nearly same for both models maybe we can drop this features**","8170b8a9":"**There is no correlacition between features. Our data not good for training but we can not drop features because we have very less feature**","6154c9d9":"**Conclusion: We reached 0.7 acc but I think this data not appropiriate for classification**"}}