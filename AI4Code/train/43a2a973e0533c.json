{"cell_type":{"a370205e":"code","9c8d4def":"code","6ffa583b":"code","877909cf":"code","34c28f2c":"code","fde27e60":"code","a7a80aa0":"code","1d3fc7d6":"code","3b1a243a":"code","3828927b":"code","82ed7aaf":"code","aea389ca":"code","a53a7960":"code","98a2c151":"code","a0ff37d0":"code","b6311013":"code","a48a903c":"code","38c3aacb":"code","8c455686":"code","611fa44e":"code","b565afca":"code","43b2cecf":"code","ef284953":"code","0a5ff969":"code","f1eb9de3":"code","b40916f4":"code","a16dfc66":"code","3458ddb3":"code","cb251e8c":"code","f2910f0f":"code","0c86077b":"code","19d09e7a":"code","72609c0c":"code","11dc01b8":"code","e6d2853a":"code","c2208cfc":"code","fd3a6d6c":"code","227eeccb":"code","a6113d5d":"code","67b02167":"code","5f45974d":"code","d28d029f":"code","1265b134":"code","1f843f9a":"code","63e26101":"code","b6ed0a31":"code","5f68b1f9":"code","c89eb319":"code","74550199":"code","6b9e45f5":"code","248224b8":"code","24366d0f":"code","6863e59f":"code","11a907ac":"code","935b8b40":"code","049d202d":"code","322f31af":"code","d8145e51":"code","3783e6e9":"code","af7b2124":"code","a255631f":"code","c937bbce":"code","a41ab595":"code","373fc457":"code","e00b3784":"code","26f23607":"code","b2a63e42":"code","4e441361":"code","f9974bb3":"code","1fcf06ca":"code","24d6af84":"code","bea2a085":"code","6b079751":"code","ea951a5e":"code","57727a65":"code","a321b809":"code","e7d070d2":"code","49b4ae9f":"code","8a7d4e12":"code","b70b1864":"code","5eb058b0":"code","6ef0e1ba":"code","9d4be8aa":"markdown","5b44063b":"markdown","20110760":"markdown","fad55ac1":"markdown","04e13eab":"markdown","fd6a01c0":"markdown","79e00aca":"markdown","666b8fd6":"markdown","ef4208b0":"markdown","b038cffb":"markdown","c72fcef8":"markdown","2b84eff8":"markdown","8a0f0108":"markdown","423d9513":"markdown","4c4ae18d":"markdown","0f4687f0":"markdown","5f3ea576":"markdown","56c0e4a0":"markdown","417f788d":"markdown","ace61c35":"markdown","b825b8d6":"markdown","7a041ef3":"markdown","e22b5d50":"markdown","58771dfb":"markdown","fd0fae10":"markdown","7bc060df":"markdown","ebe2e0ec":"markdown","0a3a6316":"markdown","01768115":"markdown","10e0345a":"markdown","f7a4fde3":"markdown","c81b956c":"markdown","f6c7bdd1":"markdown","b3d38097":"markdown","84f3d6de":"markdown","654305d6":"markdown","5a84ad50":"markdown","239df282":"markdown","9441d336":"markdown","7accc963":"markdown","c2f86373":"markdown","efcff512":"markdown","9102f195":"markdown","353040b0":"markdown","7086422f":"markdown","88af2ccc":"markdown","de1b5d17":"markdown","333f033e":"markdown","27768147":"markdown","16ede79c":"markdown","077bb838":"markdown","cbfdc309":"markdown","9ee89a10":"markdown","aec18e2f":"markdown","e71b27e0":"markdown","ec6408b6":"markdown","3e3a7e04":"markdown","a6e69a5c":"markdown","3de6154b":"markdown","1e9a999f":"markdown","3cb1bf50":"markdown","e91018d1":"markdown","d16ad2ac":"markdown","a648305b":"markdown","a2a0d158":"markdown","1bccbdd9":"markdown","ba3fa28b":"markdown","207977d5":"markdown","f8bde30a":"markdown","008b95f4":"markdown","b4aec0a9":"markdown","ea05e61f":"markdown","6837f15d":"markdown","bc84e0d2":"markdown","afd2d91e":"markdown","ff6c92c1":"markdown","4211b33a":"markdown","50cb5e31":"markdown","d586d85e":"markdown","b15386f6":"markdown","b249ba09":"markdown","928ce2ec":"markdown","8dc6a7e7":"markdown","5a92febc":"markdown","d64e3606":"markdown","ad4fe32d":"markdown","c92bd9b4":"markdown","b88afd5d":"markdown","60aaeeec":"markdown","7810eb99":"markdown","1e749ee1":"markdown","516fca60":"markdown","1b82fafc":"markdown","1754e0eb":"markdown","3351d515":"markdown","3beb6b2d":"markdown","1fde587e":"markdown","cf9cc2b6":"markdown","73678a8c":"markdown","d929ed0f":"markdown","d23256b5":"markdown","e5498dbd":"markdown","b88cc2fb":"markdown","a97ff686":"markdown","e97ee529":"markdown","d6e5fbee":"markdown","a2b6a277":"markdown","dec79a32":"markdown","3fabea4e":"markdown","8055dce1":"markdown","4054e510":"markdown","61dc8e52":"markdown","b37ce1de":"markdown","6a13520a":"markdown","d77f932c":"markdown","081bc76c":"markdown","c1e270b0":"markdown","a15e4c6c":"markdown","fa679c40":"markdown","16e5234b":"markdown","6451d134":"markdown","7fce2e32":"markdown","57bdba9e":"markdown","21c8e2b4":"markdown","50527e15":"markdown","4463040d":"markdown","88b04513":"markdown","3f768187":"markdown","10ab5dd6":"markdown","4cf0934b":"markdown","35019f1d":"markdown","90149c7f":"markdown","ac555e1c":"markdown","ff6b1bb0":"markdown","aba2500b":"markdown","d747128b":"markdown","8bf017c6":"markdown","156c7743":"markdown","f17fdae1":"markdown","835c9cc9":"markdown","fad9014e":"markdown","f83fb3e5":"markdown","ecf58156":"markdown","3614625a":"markdown","99e5a116":"markdown","e5380aad":"markdown","9aab6c04":"markdown","9eb9a77a":"markdown","d8c43404":"markdown","7d76109f":"markdown","1d3a1cd0":"markdown","566348c9":"markdown","ce042f66":"markdown","a2619730":"markdown","67533626":"markdown","4fcc6158":"markdown","324ea7f0":"markdown","4398cccc":"markdown","2c3de0a6":"markdown","1be60298":"markdown","e5603319":"markdown","837bd9af":"markdown","2f97d77a":"markdown","672977ac":"markdown","22d9a324":"markdown","ea50b441":"markdown","a6b83f67":"markdown","41c2327c":"markdown","37ec72bd":"markdown","cfb98006":"markdown","5d230799":"markdown"},"source":{"a370205e":"from sklearn.base import clone\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, Normalizer\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\n# Local application\nimport utilidad_grupor as utils","9c8d4def":"seed = 27912","6ffa583b":"filepath = \"..\/input\/breast-cancer-wisconsin-data\/data.csv\"\n\nindex = \"id\"\ntarget = \"diagnosis\"\n\ndata = utils.load_data(filepath, index, target)","877909cf":"data.sample(5, random_state=seed)","34c28f2c":"del data['Unnamed: 32']","fde27e60":"data.sample(5, random_state=seed)","a7a80aa0":"data.diagnosis.unique()","1d3fc7d6":"data['diagnosis']=data['diagnosis'].map({'M':1,'B':0})","3b1a243a":"(X, y) = utils.divide_dataset(data, target=\"diagnosis\")","3828927b":"X.sample(5, random_state=seed)","82ed7aaf":"y.sample(5, random_state=seed)","aea389ca":"train_size = 0.7\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","a53a7960":"train = utils.join_dataset(X_train, y_train)\ntest = utils.join_dataset(X_test, y_test)","98a2c151":"train.sample(5, random_state=seed)","a0ff37d0":"from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\npreproc = ColumnTransformer([(\"\", \"drop\", [\"concavity_mean\", \"compactness_mean\", \"concavity_se\", \"compactness_se\", \"concavity_worst\", \"compactness_worst\", \"area_mean\", \"perimeter_mean\", \"area_se\", \"perimeter_se\", \"area_worst\", \"perimeter_worst\"])], remainder=\"passthrough\")","b6311013":"normalizacion = Normalizer()","a48a903c":"n_neighbors = 5\nweights = 'distance'\n\nk_neighbors_model = make_pipeline(\n        preproc,\n        normalizacion,\n        KNeighborsClassifier(n_neighbors, weights=weights)\n)","38c3aacb":"decision_tree_model = make_pipeline(\n        preproc,\n        DecisionTreeClassifier(random_state=seed, \n                               max_depth=3,\n                               criterion='entropy',\n                               ccp_alpha=0.1,\n                               min_samples_leaf=25)\n) ","8c455686":"adaboost_model = AdaBoostClassifier(random_state=seed)\n\nadaboost_model = make_pipeline(\n        preproc,\n        AdaBoostClassifier(random_state=seed, base_estimator=DecisionTreeClassifier(max_depth=1))\n)","611fa44e":"bagging_model = make_pipeline(\n        preproc,\n        BaggingClassifier(random_state=seed, base_estimator=DecisionTreeClassifier(max_depth=None))\n)","b565afca":"random_forest_model = make_pipeline(\n        preproc,\n        RandomForestClassifier(random_state=seed)\n)","43b2cecf":"gradient_boosting_model = make_pipeline(\n        preproc,\n        GradientBoostingClassifier(random_state=seed)\n)","ef284953":"hist_gradient_boosting_model = make_pipeline(\n        preproc,\n        HistGradientBoostingClassifier(random_state=seed)\n)","0a5ff969":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=seed)","f1eb9de3":"model = decision_tree_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","b40916f4":"model = k_neighbors_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","a16dfc66":"model = adaboost_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","3458ddb3":"model = bagging_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","cb251e8c":"model = random_forest_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","f2910f0f":"model = gradient_boosting_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","0c86077b":"model = hist_gradient_boosting_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","19d09e7a":"scoring = [\"accuracy\",\"recall\"]","72609c0c":"estimator = decision_tree_model\n\nmax_depth = [3, 4, 5, 6, 7]\nccp_alpha = [0, 0.05, 0.1, 0.2]\ncriterion = ['entropy', 'gini']\nmin_samples_leaf = [5, 10, 15, 20, 25]\n\ndecision_tree_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv,\n                                        scoring=scoring, refit=\"recall\",\n                                        decisiontreeclassifier__max_depth=max_depth,\n                                        decisiontreeclassifier__criterion=criterion,\n                                        decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                        decisiontreeclassifier__min_samples_leaf=min_samples_leaf)","11dc01b8":"estimator = k_neighbors_model\n\nweights = [\"uniform\", \"distance\"]\nn_neighbors = [3, 5, 7, 9, 11, 13, 15]\n\n\nk_neighbors_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                        kneighborsclassifier__weights=weights,\n                                        kneighborsclassifier__n_neighbors=n_neighbors)","e6d2853a":"estimator = adaboost_model\n\ncriterion = [\"gini\", \"entropy\"]\nlearning_rate = [0.25, 0.5, 0.75, 1]\nmin_samples_split = [2, 5, 10, 20, 30]\n\nadaboost_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                        adaboostclassifier__base_estimator__min_samples_split=min_samples_split,\n                                        adaboostclassifier__base_estimator__criterion=criterion,\n                                        adaboostclassifier__learning_rate=learning_rate)","c2208cfc":"estimator = bagging_model\n\nmax_samples = [0.25, 0.5, 0.75, 1]\nmax_features = [0.25, 0.5, 0.75, 1]\n\nbagging_clf = utils.optimize_params(estimator,\n                                     X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                     baggingclassifier__max_samples=max_samples,\n                                     baggingclassifier__max_features=max_features)","fd3a6d6c":"estimator = random_forest_model\n\nmax_samples = [0.25, 0.5, 0.75]\nmax_features = ['sqrt', 'log2']\ncriterion = [\"gini\", \"entropy\"]\n\nrandom_forest_clf = utils.optimize_params(estimator,\n                                               X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                               randomforestclassifier__max_samples=max_samples,\n                                               randomforestclassifier__criterion=criterion,\n                                               randomforestclassifier__max_features=max_features)","227eeccb":"estimator = gradient_boosting_model\n\nlearning_rate = [0.025, 0.05, 0.1]\nmax_depth = [1, 3, 5, 7]\n\n\ngradient_boosting_clf = utils.optimize_params(estimator,\n                                               X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                               gradientboostingclassifier__learning_rate = learning_rate,\n                                               gradientboostingclassifier__max_depth=max_depth)","a6113d5d":"estimator = hist_gradient_boosting_model\n\nmin_samples_leaf = [10, 20, 30, 40]\nlearning_rate = [0.05, 0.1, 0.15]\n\nhist_gradient_boosting_clf = utils.optimize_params(estimator,\n                                                  X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                                  histgradientboostingclassifier__learning_rate=learning_rate,\n                                                  histgradientboostingclassifier__min_samples_leaf=min_samples_leaf)","67b02167":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","5f45974d":"utils.evaluate_estimators(estimators, X_test, y_test)","d28d029f":"random_state = 27912","1265b134":"filepath = \"..\/input\/pima-indians-diabetes-database\/diabetes.csv\"\n\ntarget = \"Outcome\"\n\ndata = utils.load_data(filepath, None, target)\n\ntrain, test = train_test_split(data, train_size=0.7, stratify=data[target], random_state=random_state)\n\ntrain_X, train_y = utils.divide_dataset(train, target)\ntest_X, test_y = utils.divide_dataset(test, target)\n\ntrain.sample(5)\n","1f843f9a":"mean_imp = SimpleImputer(missing_values=0)\nremove_columns = ColumnTransformer([('a', 'drop', ['Insulin', 'SkinThickness']),\n                                    ('b', mean_imp, ['Glucose', 'BloodPressure', 'BMI'])],\n                                    remainder='passthrough')\nremove_columns.fit_transform(train_X)\nremove_columns.fit_transform(test_X)\n\ntrain_X.sample(5)\n","63e26101":"cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=random_state)","b6ed0a31":"k_neighbors_classifier = KNeighborsClassifier()\nn_neighbors = np.arange(1, 16)\nweights = [\"uniform\", \"distance\"]\n\nk_neighbors_clf = utils.optimize_params(k_neighbors_classifier, train_X, train_y, cv=cv, scoring=\"recall\", weights=weights, n_neighbors=n_neighbors)","5f68b1f9":"decision_tree_classifier = DecisionTreeClassifier(random_state=random_state)\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [None, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmin_samples_leaf = [5, 6, 7, 8, 9]\nccp_alpha = [0.0, 0.1, 0.2, 0.3, 0.4]\n\ndecision_tree_clf = utils.optimize_params(decision_tree_classifier,\n                                          train_X, train_y, cv,\n                                          scoring=\"recall\",\n                                          criterion=criterion,\n                                          max_depth=max_depth,\n                                          min_samples_leaf=min_samples_leaf,\n                                          ccp_alpha=ccp_alpha)\n\n","c89eb319":"base_estimator = DecisionTreeClassifier(random_state=random_state)\n\nadaboost_classifier = AdaBoostClassifier(random_state=random_state, base_estimator=base_estimator)\n\nlearning_rate = [0.95, 1.0]\nn_estimators = [50, 75]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\nadaboost_clf = utils.optimize_params(adaboost_classifier,\n                                     train_X, train_y, cv,\n                                     scoring=\"recall\",\n                                     learning_rate=learning_rate,\n                                     n_estimators=n_estimators,\n                                     base_estimator__criterion=criterion,\n                                     base_estimator__max_depth=max_depth,\n                                     base_estimator__ccp_alpha=ccp_alpha)","74550199":"base_estimator = DecisionTreeClassifier(random_state=random_state)\n\nbagging_classifier = BaggingClassifier(random_state=random_state, base_estimator=base_estimator)\n\nn_estimators = [100, 250, 500]\ncriterion = [\"gini\", \"entropy\"]\n\nbagging_clf = utils.optimize_params(bagging_classifier,\n                                    train_X, train_y, cv,\n                                    scoring=\"recall\",\n                                    n_estimators=n_estimators,\n                                    base_estimator__criterion=criterion)","6b9e45f5":"random_forest_classifier = RandomForestClassifier(random_state=random_state)\n\nn_estimators = [100, 150]\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\n\nrandom_forest_clf = utils.optimize_params(random_forest_classifier,\n                                          train_X, train_y, cv,\n                                          scoring=\"recall\",\n                                          n_estimators=n_estimators,\n                                          criterion=criterion,\n                                          max_features=max_features)","248224b8":"gradient_boosting_classifier = GradientBoostingClassifier(random_state=random_state)\n\nlearning_rate = [0.01, 0.05, 0.1]\nn_estimators = [100, 200]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\ngradient_boosting_clf = utils.optimize_params(gradient_boosting_classifier,\n                                          train_X, train_y, cv,\n                                          scoring=\"recall\",\n                                          learning_rate=learning_rate,\n                                          n_estimators=n_estimators,\n                                          criterion=criterion,\n                                          max_depth=max_depth,\n                                          ccp_alpha=ccp_alpha)","24366d0f":"histogram_gradient_boosting_classifier = HistGradientBoostingClassifier(random_state=random_state)\n\nlearning_rate = [0.01, 0.02, 0.03, 0.04, 0.05]\nmax_leaf_nodes = [15, 31, 65, 127]\n\nhist_gradient_boosting_clf = utils.optimize_params(histogram_gradient_boosting_classifier,\n                                                   train_X, train_y, cv,\n                                                   scoring=\"recall\",\n                                                   learning_rate=learning_rate,\n                                                   max_leaf_nodes=max_leaf_nodes)","6863e59f":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","11a907ac":"utils.evaluate_estimators(estimators, test_X, test_y)","935b8b40":"import numpy as np \nimport pandas as pd \nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n#Preprocesamiento de datos\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n#Creaci\u00f3n de modelos y b\u00fasqueda de hiperpar\u00e1metros\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\n#Validaci\u00f3n y visualizaci\u00f3n de m\u00e9tricas\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, precision_score, recall_score, auc\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\n\nimport utilidad_grupor as utils","049d202d":"seed = 27912","322f31af":"df = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\ndataset = df.values\ndf.sample(5, random_state=seed)","d8145e51":"df.shape","3783e6e9":"df.isnull().sum()","af7b2124":"names = list(df.columns)\nx = df[names[1:]]\ny = df['class']","a255631f":"#a\u00f1adimos la seed al random state para evitar sesgo y que los experimentos sean reproducibles\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=seed)","c937bbce":"train = utils.join_dataset(x_train, y_train)\ntest = utils.join_dataset(x_test, y_test)","a41ab595":"# Arreglados Data Leaks\n\ncolors = ('#EF8787','#9CF29C')\npalette = sns.set_palette(sns.color_palette(colors))\n\nutils.plot_class_distribution(train, 'class', [\"Venenoso\", \"Comestible\"], colors, 'Mushroom Class Distribution')","373fc457":"# Arreglados Data Leaks\n\nutils.plot_feature_distribution(train, 'class', train.columns, palette)","e00b3784":"# Arreglados Data Leaks\n\nlabelencoder=LabelEncoder()\ntrain_enc = train.copy()\nfor column in train.columns:\n    train_enc[column] = labelencoder.fit_transform(train[column])","26f23607":"plt.figure(figsize=(16,16))\nsns.heatmap(train_enc.corr(),linewidths=.1,annot=True, cmap=\"magma\")","b2a63e42":"#A\u00f1adimos pipeline para preprocesamiento \n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\npreproc = ColumnTransformer([(\"\", \"drop\", [\"bruises\",\"gill-color\",\"veil-type\"])], remainder=\"passthrough\")","4e441361":"preproc.fit_transform(x_train)\npreproc.fit_transform(x_test)","f9974bb3":"#Train\nohe_x_train = OneHotEncoder(drop='first').fit(x_train)\nohe_x_train = ohe_x_train.transform(x_train).toarray()\n\naux = y_train.values.reshape(-1, 1)\nohe_y_train = OneHotEncoder(drop='first').fit(aux)\nohe_y_train = ohe_y_train.transform(aux).toarray()\nohe_y_train = ohe_y_train.flatten()\n\n#Test\nohe_x = OneHotEncoder(drop='first').fit(x_test)\nohe_x = ohe_x.transform(x_test).toarray()\n\naux = y_test.values.reshape(-1, 1)\nohe_y = OneHotEncoder(drop='first').fit(aux)\nohe_y = ohe_y.transform(aux).toarray()\nohe_y = ohe_y.flatten()","1fcf06ca":"models = ['LogisticRegression','NaiveBayes','RandomForest','KNearestNeighbors']\n\nscores = [None] * len(models)","24d6af84":"from sklearn.model_selection import cross_val_score\n\nlr = LogisticRegression()\nlr.fit(ohe_x_train, ohe_y_train)\ny_pred = lr.predict(ohe_x)\naccuracy = lr.score(ohe_x, ohe_y)\n\nutils.show_results(models, scores, ohe_x, ohe_y, lr, y_pred,\"LogisticRegression\")","bea2a085":"score_list = cross_val_score(lr,ohe_x,ohe_y, cv=10)\nscore = np.mean(score_list)\nprint (score)\n\n# we swap the score obtained before with the cross_val_score\nscores[0] = score","6b079751":"nb = GaussianNB()\nnb.fit(ohe_x_train, ohe_y_train)\npreds= nb.predict(ohe_x)\nutils.show_results(models, scores, ohe_x, ohe_y, nb, preds,\"NaiveBayes\")","ea951a5e":"score_list = cross_val_score(nb,ohe_x,ohe_y, cv=5)\nprint(score)\nscore = np.mean(score_list)\n# we swap the score obtained before with the cross_val_score\nscores[1] = score","57727a65":"print(score_list)","a321b809":"cv_split = TimeSeriesSplit(n_splits=5)","e7d070d2":"rf = RandomForestClassifier(random_state=1)\nrf_params = {\n    'model__n_estimators': list(range(25,251,25)),\n    'model__max_features': list(np.arange(0.1,0.36,0.05))\n}\nrf_pipe = Pipeline([\n    ('scale', StandardScaler()),\n    ('model', rf)\n])\ngridsearch_rf = GridSearchCV(estimator=rf_pipe,\n                          param_grid = rf_params,\n                          cv = cv_split,\n                         )\ngridsearch_rf.fit(ohe_x_train, ohe_y_train)","49b4ae9f":"rf_best_model = gridsearch_rf.best_estimator_\npreds = rf_best_model.predict(ohe_x)\nutils.show_results(models, scores, ohe_x, ohe_y, rf_best_model, preds,'RandomForest')","8a7d4e12":"knn = KNeighborsClassifier()\nknn_params = {\n    'n_neighbors': list(range(4,10)),\n    'weights': ['uniform','distance']\n}\n\ngridsearch_knn = GridSearchCV(knn,\n                          param_grid = knn_params,\n                          cv = cv_split,\n                         )\ngridsearch_knn.fit(ohe_x_train, ohe_y_train)","b70b1864":"knn_best_model = gridsearch_knn.best_estimator_\npreds = knn_best_model.predict(ohe_x)\nutils.show_results(models, scores, ohe_x, ohe_y, knn_best_model, preds,'KNearestNeighbors')","5eb058b0":"utils.plot_accuracy(models, scores)","6ef0e1ba":"palette = sns.set_palette(sns.color_palette('Set1')) #just to define de plot palette\n\nfor i in range(0,5):\n    random.seed(i)\n    randlist = list(names[x] for x in random.sample(range(0,21),k=5))\n    rand_df = train[randlist]\n    rand_df = pd.get_dummies(rand_df)\n\n    utils.plot_roc(rand_df, ohe_y_train)\n    \nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\nplt.gcf().set_size_inches(15,10)","9d4be8aa":"Ahora cargaremos el conjunto de datos, que es el conjunto [Pima Indian Diabetes](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database). Con el objetivo de evaluar los modelos que obtendremos finalmente, reservaremos un $30\\%$ de los ejemplos para realizar una validaci\u00f3n *holdout*.","5b44063b":"## 5.3. Adaptative Boosting (AdaBoost)","20110760":"En este caso parece claro que cuantos m\u00e1s clasificadores se usen, mejor resultado se alcanza, aunque la mejora entre emplear $250$ o $500$ es casi inapreciable por lo que puede no merecer la pena. Una vez m\u00e1s, usando entrop\u00eda conseguimos mejores resultados. Tambi\u00e9n es interesante observar que se ha conseguido un $100\\%$ clasificando los datos de entrenamiento, lo cual indica un gran sobreajuste a estos.","fad55ac1":"Nuestra variable clase, como vemos, se llama `class`. Lo que haremos ser\u00e1 separar esta variable de las variables predictoras","04e13eab":"#\u00a04. Evaluaci\u00f3n de modelos","fd6a01c0":"Como averiguamos en la pr\u00e1ctica anterior, existen multitud de valores perdidos (codificados como $0$), que debemos tratar. Para ello, imputaremos por la media en las variables `Glucose`, `BloodPressure` y `BMI`; y eliminaremos las variables `Insulin` y `SkinThickness` que tienen un porcentaje demasiado alto de valores perdidos como para ser significativas.\n\nYa que dicho proceso no depende del algoritmo que vayamos a usar, lo aplicaremos directamente sobre el conjunto de entrenamiento a priori una sola vez, con el objetivo de ahorrar tiempo entrenando.","79e00aca":"En esta selecci\u00f3n de modelos trataremos de optimizar los par\u00e1metros que consideramos son m\u00e1s relevantes en Bagging.\n\nLos hiperpar\u00e1metros elegidos para ser optimizados han sido:\n* `max_samples`: N\u00famero de instancias del conjunto de entrenamiento necesarias para entrenar cada estimador.\n* `max_features`: N\u00famero de variables predictoras de nuestro conjunto de entrenamiento necesarias para entrenar cada estimador.\n\nAmbas variables toman por defecto el valor 1.","666b8fd6":"## 5.3. Random Forest","ef4208b0":"## 5.1. \u00c1rbol de decisi\u00f3n","b038cffb":"De esta manera, podremos ver las variables que se encuentran m\u00e1s correladas unas con otras. A simple vista, encontramos diversas parejas de variables que est\u00e1n altamente relacionadas unas con otras.\n\n* `veil-color y gill-attachment` con una correlaci\u00f3n del 89%\n* `ring-type y bruises` con una correlaci\u00f3n del 69%\n* `ring-type y gill-color` con una correlaci\u00f3n del 63%\n* `spore-print-color y gill-size` con una correlaci\u00f3n del 62%\n\nDe este estudio extraemos que las variables `bruises y gill-color` dependen ambas de `ring-type`, y consideramos que podr\u00edan ser eliminadas. Pese a que en el procesamiento no se haya llevado a cabo dicha acci\u00f3n, la implementaremos.\n\nComo hemos mencionado anteriormente, la variable `veil-type` ser\u00e1 eliminada puesto que no aporta ninguna informaci\u00f3n relevante.","c72fcef8":"En esta pr\u00e1ctica estudiaremos los modelos m\u00e1s utilizados en `scikit-learn` para conocer los distintos hiperpar\u00e1metros que los configuran y estudiar los clasificadores resultantes. Adem\u00e1s, veremos m\u00e9todos de selecci\u00f3n de modelos orientados a obtener una configuraci\u00f3n \u00f3ptima de hiperpar\u00e1metros.","2b84eff8":"Como hemos comentado previamente, este algoritmo emplea \u00e1rboles distintos con mucho sobreajuste, por lo que no resultar\u00eda de importancia controlar los hiperpar\u00e1metros de \u00e9stos \u00e1rboles como hicimos en AdaBoost. Nos limitaremos a controlar los siguientes hiperpar\u00e1metros referentes a los Random Forest:\n* `max_samples`: N\u00famero de instancias del conjunto de entrenamiento necesarias para entrenar cada estimador (\u00e1rbol).\n* `max_features`: N\u00famero m\u00e1ximo de variables para considerar al buscar la mejor divisi\u00f3n al entrenar los \u00e1rboles. Al utilizar `sqrt` estamos indicando que `max_features=sqrt(n_features)`. Lo mismo para `log2`, tal que `max_features=log2(n_features)`\n* `criterion`: Funci\u00f3n para medir la calidad de las particiones del \u00e1rbol. Usaremos `entropy` para la ganancia de informaci\u00f3n y el \u00edndice `gini`.","8a0f0108":"Podemos observar que los mejores resultados se dan cuando se aplica pospoda, usando como criterio `entropy`. Tambi\u00e9n observamos que el aumento del n\u00famero de clasificadores de $50$ a $75$ no tiene demasiado impacto, por lo que entre estas dos opciones elegiremos la de menor complejidad.\n\n","423d9513":"Para realizar este proceso, haremos uso de un **Pipeline**. Este Pipeline ser\u00e1 el encargado de aplicar las transformaciones que hemos decidido a nuestro conjunto de datos.\n\nEliminaremos las siguientes columnas (variables):\n* Aquellas relacionadas con los `concave points`: `concavity_mean, compactness_mean, concavity_se, compactness_se, concavity_worst y compactness_worst`.\n* Aquellas relacionadas con `radius`, es decir: `area_mean, perimeter_mean, area_se, perimeter_se, area_worst y perimeter_worst`.\n\nPorque son variables que dependen directamente de las 2 que vamos a dejar en nuestro conjunto de datos: `radius y concave points`.","4c4ae18d":"Lo primero de todo ser\u00e1 cargar las librer\u00edas para que est\u00e9n disponibles posteriormente:","0f4687f0":"El algoritmo Histogram Gradient Boosting es una optimizaci\u00f3n de *Gradient Boosting* que discretiza el conjunto de datos de entrada con el fin de poder trabajar con un mayor n\u00famero de instancias en un tiempo razonable.\n\nAl contrario que en otros modelos, Histogram Gradient Boosting realiza su propia discretizaci\u00f3n, por lo que no tendremos que proporcion\u00e1rsela mediante el **Pipeline**.","5f3ea576":"Empezamos mostrando las variables predictoras:","56c0e4a0":"Vamos a usar los hiperpar\u00e1metros por defecto para configurar este estimador:","417f788d":"Como podemos observar, en t\u00e9rminos de *recall*, los modelos que proporcionan mejores resultados para este problema son el \u00e1rbol de decisi\u00f3n y el AdaBoost; y el peor el Vecinos m\u00e1s cercanos. Si nos fijamos en otras m\u00e9tricas como el tiempo necesario para entrenamiento e inferencia, el \u00e1rbol de decisi\u00f3n vuelve a destacar.\n\nAnte estos resultados, debemos priorizar el **\u00e1rbol de decisi\u00f3n** al conseguir un buen resultado y ser m\u00e1s simple.","ace61c35":"Vamos a configurar un estimador tipo Random Forests usando los hiperpar\u00e1metros por defecto:","b825b8d6":"Tanto KNN como RandomForests nos dar\u00e1n una precisi\u00f3n casi perfecta. Incluso si las caracter\u00edsticas del conjunto de datos permiten tener f\u00e1cilmente altas precisiones.\n\nSiempre es importante procesar correctamente los datos y ajustar los modelos correctamente, entendiendo lo que est\u00e1 haciendo el programa en lugar de solo enfocarse en obtener mejores m\u00e9tricas.\n\nAl hacer esto, sabremos si nuestros scores son correctos o si estamos haciendo algo mal.","7a041ef3":"Como podemos observar, se obtiene un mejor resultado con un valor de `n_neighbors` alto (11) y con una distribuci\u00f3n de `weights` uniforme.","e22b5d50":"A continuaci\u00f3n, separaremos en dos subconjuntos nuestro conjunto de datos inicial, uno con las variables predictoras (`X`) y otro con la variable objetivo (`y`). ","58771dfb":"## 4.5. Random Forests","fd0fae10":"---","7bc060df":"En esta selecci\u00f3n de modelos trataremos de optimizar los par\u00e1metros que consideramos son m\u00e1s relevantes en AdaBoost.\n\nLos hiperpar\u00e1metros elegidos para ser optimizados han sido:\n* `criterion`: Funci\u00f3n para medir la calidad de las particiones del \u00e1rbol. Usaremos `entropy` para la ganancia de informaci\u00f3n y el \u00edndice `gini`.\n* `learning_rate`: Nos indica la contribuci\u00f3n de los estimadores basada en el valor de esta variable, por defecto a 1.\n* `min_samples_split`: M\u00ednimo n\u00famero de instancias requeridas para dividir un nodo interno, este hiperpar\u00e1metro ser\u00e1 usado por los \u00e1rboles que conforman nuestro modelo de AdaBoost.","ebe2e0ec":"### B\u00fasqueda Grid\nAhora separamos los datos en carpetas para la b\u00fasqueda **Grid**","0a3a6316":"Como las variables de nuestro conjunto de datos son categ\u00f3ricas, llevaremos a cabo un mapa de calor para observar la relaci\u00f3n entre estas variables.\n\nAntes, deberemos crear un `LabelEncoder` que transforme nuestras variables con un valor entre $0$ y el $ nclases - 1 $.","01768115":"A la hora de evaluar los distintos modelos, utilizaremos una t\u00e9cnica conocida como **validaci\u00f3n cruzada**. En esta, se separa el conjunto de datos en `k` particiones y se repite `k` veces el proceso de aprendizaje y validaci\u00f3n, pero utilizando cada vez una combinaci\u00f3n \u00fanica de `k-1` muestras para entrenar y la restante para validar. De este modo, obtendremos unos valores fiables de sesgo empleando solamente el conjunto de entrenamiento.\n\nEn este caso vamos a evaluar nuestros clasificadores y el tipo de validaci\u00f3n cruzada a utilizar usando una `5*10-cv` estratificada:","10e0345a":"Un Random Forest es un conjunto (ensemble) de \u00e1rboles de decisi\u00f3n combinados con bagging. Al usar bagging, lo que en realidad est\u00e1 pasando, es que distintos \u00e1rboles ven distintas porciones de los datos. Ning\u00fan \u00e1rbol ve todos los datos de entrenamiento. Esto hace que cada \u00e1rbol se entrene con distintas muestras de datos para un mismo problema. De esta forma, al combinar sus resultados, unos errores se compensan con otros y tenemos una predicci\u00f3n que generaliza mejor.","f7a4fde3":"En este algoritmo al igual que en el resto basados en \u00e1rboles probaremos cu\u00e1l de los criterios para medir la calidad de una partici\u00f3n es mejor. Adem\u00e1s, al tomar para cada \u00e1rbol un subconjunto aleatorio de caracter\u00edsticas, deberemos elegir cu\u00e1ntas tomar. El muestreo nuevamente ser\u00e1 con reemplazo. Por lo tanto los hiperpar\u00e1metros que variaremos ser\u00e1n:\n\n* `criterion`: `gini` o `entropy`.\n\n* `max_features`: $\\sqrt{\\mathit{n\\_features}}$ o $log_2(\\mathit{n\\_features})$.\n\n* `n_estimators`: el n\u00famero de clasificadores base.","c81b956c":"Fijaremos tamb\u00eden una semilla aleatoria para que los experimentos sean reproducibles:","f6c7bdd1":"## 3.4. Bootstrap Aggregating (Bagging)","b3d38097":"De esta manera, podremos apreciar los valores de los hiperpar\u00e1metros que optimizan las m\u00e9tricas del modelo seleccionado, en este caso `max_features = 0.75` y `max_samples = 0.5`","84f3d6de":"Como hemos comentado anteriormente, ser\u00e1 recomendable eliminar la variable que no aporta informaci\u00f3n `veil-type` y las dos variables que van relacionadas con `ring-type`, como son `bruises y gill-color`.","654305d6":"En cuanto al criterio seleccionado, vemos que en este caso el \u00edndice gini ha conseguido darnos mejores resultados en nuestro modelo junto a una tasa de aprendizaje (learning rate) que mantiene su valor por defecto de 1.\n\nConociendo el algoritmo AdaBoost sabemos que \u00e9ste trabaja con clasificadores 1R, los cuales reciben una inmensa cantidad de instancias, no obstante es interesante remarcar que el hiperpar\u00e1metro `min_samples_split` para los nodos internos ha resultado no ser relevante para el problema.","5a84ad50":"## 5.4. KNN","239df282":"# Breast Cancer Wisconsin","9441d336":"En esta selecci\u00f3n de modelos trataremos de optimizar los par\u00e1metros que consideramos son m\u00e1s relevantes en el algoritmo de vecinos m\u00e1s cercanos.\n\nLos hiperpar\u00e1metros elegidos para ser optimizados han sido:\n* `weights`: Funci\u00f3n de peso de los vecinos, que pueden ser uniformes o por distancias.\n* `n_neighbors`: N\u00famero de vecinos m\u00e1s cercanos para elegir la clase del que se estudia. Un gran n\u00famero de vecinos podr\u00eda llevar al modelo a parecerse a un `ZeroR`, y un peque\u00f1o n\u00famero de vecinos podr\u00eda causarnos un gran sobreajuste. Es por ello que este hiperpar\u00e1metro es el m\u00e1s importante a la hora de seleccionar nuestro modelo de vecinos m\u00e1s cercanos.","7accc963":"### 2.1.2. Normalizaci\u00f3n","c2f86373":"Podemos apreciar as\u00ed, como distintas variables predictoras nos dar\u00e1n mas informaci\u00f3n que otras. Podemos ver tambi\u00e9n que solo hay un tipo de **`veil-type`** en esa variable, por lo que en el posterior **preprocesamiento de datos** que llevemos a cabo, eliminaremos esa variable","efcff512":"## 3.6. Gradient Tree Boosting (Gradient Boosting)","9102f195":"# Pima Diabetes","353040b0":"Las conclusiones que podemos obtener son que la tasa de aprendizaje es un factor relevante a la hora de seleccionar un modelo, y que un \u00e1rbol con un n\u00famero de instancias en cada nodo hoja nos proporciona mejores resultados.","7086422f":"Por \u00faltimo, compararemos los resultados de cada modelo con los hiperpar\u00e1metros aprendidos y entrenados con la totalidad del conjunto de entrenamiento, contra el conjunto de test que hab\u00edamos reservado al principio.","88af2ccc":"Gradient Boosting es una generalizaci\u00f3n de los algoritmos de Boosting con la capacidad de optimizar cualquier tipo de funci\u00f3n p\u00e9rdida, generando un conjunto de estimadores de forma secuencial. No todos estos estimadores tendr\u00e1n la misma importancia, puesto que en cada iteraci\u00f3n solo se tomar\u00e1n en cuenta a los modelos anteriores.","de1b5d17":"Hay muchos modelos diferentes y muchas variaciones diferentes de cada modelo, para elegir los que tendr\u00e1n un mejor rendimiento, debemos considerar cu\u00e1l se ajustar\u00e1 mejor a nuestro conjunto de datos.\n\nNuestro conjunto de datos est\u00e1 equilibrado, la entrada y la salida son categ\u00f3ricas y tenemos alrededor de 8.000 instancias. Teniendo esto en cuenta usaremos los siguientes modelos:\n\n* Regresi\u00f3n Log\u00edstica\n* Naive Bayes\n* Random Forest\n* KNN","333f033e":"## 3.1. Vecinos m\u00e1s cercanos","27768147":"En esta selecci\u00f3n de modelos trataremos de optimizar los par\u00e1metros que consideramos son m\u00e1s relevantes en Gradient Boosting.\n\nLos hiperpar\u00e1metros elegidos para ser optimizados han sido:\n* `learning_rate`: Tasa de aprendizaje usada como factor multiplicativo para los nodos hoja. Toma un valor por defecto de 0.1.\n* `max_depth`: Profundidad m\u00e1xima de cada \u00e1rbol (desde la ra\u00edz al nodo m\u00e1s profundo). Este par\u00e1metro no se comprueba por defecto, pero puede resultar de inter\u00e9s estudiarlo.\n","16ede79c":"Para realizar este proceso, haremos uso de un **Pipeline**. Este Pipeline ser\u00e1 el encargado de aplicar las transformaciones que hemos decidido a nuestro conjunto de datos.","077bb838":"Pasemos ahora a la selecci\u00f3n de modelos, donde emplearemos para cada uno el **Pipeline** definido.","cbfdc309":"## 5.2. Vecinos m\u00e1s cercanos","9ee89a10":"# Conclusiones","aec18e2f":"#\u00a05. Selecci\u00f3n de modelos","e71b27e0":"# Conclusiones","ec6408b6":"#\u00a02. Preprocesamiento de datos","3e3a7e04":"Como podemos apreciar, RandomForest y KNN son los que nos proporcionan una mayor precisi\u00f3n cuando los enfrentamos contra el conjunto de test que reservamos previamente.","a6e69a5c":"## 5.1. Regresi\u00f3n Log\u00edstica","3de6154b":"## 4.4. Bootstrap Aggregating (*Bagging*)","1e9a999f":"## 3.5. Random Forests","3cb1bf50":"##\u00a03.5. Bootstrap Aggregating (*Bagging*)","e91018d1":"# Conclusiones","d16ad2ac":"#\u00a02. Carga de datos","a648305b":"## 5.4. Bootstrap Aggregating (Bagging)","a2a0d158":"Nuevamente podemos ver c\u00f3mo despu\u00e9s de usar la validaci\u00f3n cruzada, nuestra precisi\u00f3n ha disminuido esta vez hasta casi un 85%.","1bccbdd9":"##\u00a03.2. \u00c1rboles de decisi\u00f3n","ba3fa28b":"\n\nVamos a configurar este estimador usando los hiperpar\u00e1metros por defecto:","207977d5":"Despu\u00e9s de esta b\u00fasqueda **grid** podemos pensar que independientemente de la b\u00fasqueda de hiperpar\u00e1metros y el ajuste de las variables el modelo es propenso a obtener precisiones muy altas.","f8bde30a":"## 4.1. \u00c1rbol de decisi\u00f3n","008b95f4":"Este conjunto de datos est\u00e1 compuesto por 30 variables predictoras que en realidad se agrupan en 10 ya que estas se ven divididas entre `_mean, _se y _worst`, una variable `Unnamed` que ser\u00e1 tratada posteriormente y la variable clase `diagnosis` que nos indicar\u00e1 si el tumor es **maligno** (m) o **benigno** (b).\n\nLas variables predictoras, por tanto, son:\n* `radius, perimeter, area, compactness`: relacionadas con el tama\u00f1o del tumor\n* `symmetry`: se refiere a la simetr\u00eda en la forma del tumor\n* `smoothness`: variaci\u00f3n en la longitud de los tama\u00f1os\n* `concavity, concave_points`: relacionados con la concavidad del tumor\n* `texture`: referente a la textura del tumor (en escala de grises)\n* `fractal_dimension`","b4aec0a9":"# 4. Construcci\u00f3n y validaci\u00f3n del modelo final","ea05e61f":"En esta selecci\u00f3n de modelos trataremos de optimizar los par\u00e1metros que consideramos son m\u00e1s relevantes en \u00e1rboles de decisi\u00f3n.\n\nLos hiperpar\u00e1metros elegidos para ser optimizados han sido:\n* `max_depth`: Profundidades del \u00e1rbol que se estudiar\u00e1n para evitar un posible sobreajuste y garantizar un buen resultado.\n* `ccp_alpha`: Par\u00e1metros de complejidad usado para una posibilidad de poda posterior con el m\u00ednimo coste computacional\n* `criterion`: Funci\u00f3n para medir la calidad de las particiones del \u00e1rbol. Usaremos `entropy` para la ganancia de informaci\u00f3n y el \u00edndice `gini`.\n* `min_samples_leaf`: M\u00ednimo n\u00famero de instancias para poder tener en cuenta un nodo hoja, lo cual afecta al suavizado del modelo que sea elegido. ","6837f15d":"Los resultados indican que a mayor n\u00famero de \u00e1rboles mejor resultado, como era de esperar. Tambi\u00e9n podemos observar que esta vez el criterio `gini` se impone a la entrop\u00eda (`entropy`). En cuanto al n\u00famero de caracter\u00edsticas seleccionadas, debido al bajo n\u00famero de caracter\u00edsticas totales con los que contamos en este conjunto de datos, ambos m\u00e9todos son en esencia id\u00e9nticos y la ligera ventaja que encontramos que tiene la ra\u00edz cuadrada es seguramente debido al azar. Por \u00faltimo, observamos que se ha sobreajustado a los datos de entrenamiento, consiguiendo un $100\\%$ para estos.","bc84e0d2":"## 4.2. Vecinos m\u00e1s cercanos","afd2d91e":"Para este algoritmo probaremos distintas opciones para los hiperpar\u00e1metros:\n\n* `criterion`: el criterio para hacer las particiones (`gini` o `entropy`).\n\n* `max_depth`: la profundidad m\u00e1xima del \u00e1rbol (sin limitar o de $2$ a $10$).\n\n* `min_samples_leaf`: el n\u00famero m\u00ednimo de ejemplos por hoja. Probaremos a partir de $5$ para evitar sobreajuste.\n\n* `ccp_alpha`: par\u00e1metro para controlar la pospoda del \u00e1rbol.","ff6c92c1":"Lo primero de todo es cargar las librer\u00edas para que est\u00e9n disponibles posteriormente:","4211b33a":"Nuevamente, vamos a asegurarnos de que el conjunto de datos se ha dividido correctamente. Para ello visualizaremos el conjunto de datos de entrenamiento, observando que la variable clase tambi\u00e9n aparece al final del conjunto:","50cb5e31":"No es necesario normalizar los datos en este caso, puesto que nuestras variables son categ\u00f3ricas","d586d85e":"Antes de comenzar el an\u00e1lisis exploratorio de los datos, dividiremos nuestro conjunto de datos en otros dos subconjuntos, uno de entrenamiento y otro de prueba, con los siguientes porcentajes:\n\n* Conjunto de entrenamiento: **70%**\n* Conjunto de prueba: **30%**\n\nMediante este proceso, nos aseguraremos de que los resultados posteriores del proceso de validaci\u00f3n han sido obtenidos de una manera correcta.","b15386f6":"## 3.7. Histogram-Based Gradient Boosting (Histogram Gradient Boosting)\n\n","b249ba09":"# 3. Modelos de clasificaci\u00f3n supervisada","928ce2ec":"Ahora que somos capaces de evaluar correctamente los clasificadores, es importante decidir una estrategia que nos permita encontrar una configuraci\u00f3n \u00f3ptima de los hiperpar\u00e1metros. Para ello haremos uso de una b\u00fasqueda **Grid** para explorar las posibles combinaciones de hiperpar\u00e1metros y obtener el mejor modelo.","8dc6a7e7":"Para AdaBoost, deberemos decidir los par\u00e1metros del propio algoritmo as\u00ed como los del clasificador usado como base, que ser\u00e1 un \u00e1rbol de decisi\u00f3n de poca profundidad. Estos ser\u00e1n:\n\n* `learning_rate`: la tasa de aprendizaje.\n\n* `n_estimators`: probaremos con $50$ y $75$ clasificadores.\n\n* Y para el \u00e1rbol de decisi\u00f3n:\n    * `criterion`: si usar `gini` o `entropia` para tomar la decisi\u00f3n de particionar.\n\n    * `max_depth`: la profundidad m\u00e1xima del \u00e1rbol, que deber\u00e1 ser peque\u00f1a.\n\n    * `ccp_alpha`: el par\u00e1metro de penalizaci\u00f3n para la pospoda.","5a92febc":"Seguidamente, lo que haremos ser\u00e1 unir los conjuntos `X_train` e `y_train` para obtener el conjunto de datos de entrenamiento.\nHaremos los mismo para `X_test` e `y_test`, juntando as\u00ed el conjunto de datos de test.","d64e3606":"Este algoritmo es considerado perozoso puesto que computa los par\u00e1metros necesarios para la clasificaci\u00f3n durante inferencia. La clasificaci\u00f3n de este modelo consistir\u00e1 en asignar a la instancia de entrada la clase mayoritaria de los vecinos m\u00e1s cercanos.\n\nEncontramos 2 principales par\u00e1metros a la hora de definir nuestro modelo de vecinos mas cercanos, el n\u00famero de vecinos `n_neighbours` que se dejar\u00e1 por defecto en un valor moderado como puede ser 5, y los pesos `weights`, donde consideraremos que todos los vecinos tienen la misma importancia, y por tanto, el mismo peso. Dejaremos los valores de esta variable por defecto.\n\nPor \u00faltimo, al crear el **Pipeline** de este algoritmo, deberemos pasarle las variables normalizadas como hemos definido previamente en el preprocesamiento de datos, de esta manera escalaremos las variables predictoras en proporciones similares que ser\u00e1n tratadas por los pesos.","ad4fe32d":"Hemos visto c\u00f3mo todos los modelos probados pueden obtener altas precisiones y lo poco que var\u00edan las precisiones entre los diferentes par\u00e1metros (ilustrado con el ejemplo KNN).\n\nPara comprender finalmente lo f\u00e1cil que podemos lograr altas precisiones, trazaremos varias curvas ROC para determinados subgrupos de caracter\u00edsticas aleatorias.","c92bd9b4":"En esta etapa limpiaremos y organizaremos los datos de manera adecuada para entrenar a nuestro modelo bas\u00e1ndonos en las observaci\u00f3n que hemos realizado en el an\u00e1lisis exploratorio de datos previo. Por ello, en este conjunto de datos nos centraremos en la selecci\u00f3n de variables adecuadas para conseguir reducir el n\u00famero de estas.","b88afd5d":"## 3.3. Adaptative Boosting (AdaBoost)","60aaeeec":"Adem\u00e1s de usar estas dos m\u00e9tricas, emplearemos el m\u00e9todo refit que nos servir\u00e1 para elegir cu\u00e1l es el mejor modelo una vez acabada la b\u00fasqueda **Grid** y volver a entrenar con \u00e9l tratando de mejorar, en este caso, su **recall**.","7810eb99":"## 4.6. Gradient Tree Boosting (Gradient Boosting)","1e749ee1":"Este \u00faltimo algoritmo se diferencia del anterior en que se reduce el n\u00famero de umbrales que se prueban para hacer las particiones mediante el uso de histogramas, lo que lo hace bastante m\u00e1s r\u00e1pido. En esta ocasi\u00f3n estudiaremos los par\u00e1metros:\n\n* `learning_rate`: la tasa de aprendizaje. En esta ocasi\u00f3n es viable usar valores m\u00e1s bajos debido al menor tiempo de entrenamiento con respecto a *Gradient Boosting*.\n\n* `max_leaf_nodes`: esta vez usaremos el m\u00e1ximo n\u00famero de nodos hoja: otra forma de controlar el tama\u00f1o de los \u00e1rboles en lugar de por su profundidad m\u00e1xima.","516fca60":"Una vez hemos cargado el conjunto de datos, mostraremos 5 registros aleatorios mediante la funci\u00f3n `sample` para comprobar que el proceso ha sido realizado correctamente","1b82fafc":"Como nuestro conjunto de datos est\u00e1 formado por variables categ\u00f3ricas, para trabajar con ellos debemos codificarlos. A la hora de realizar este proceso, debemos tener en cuenta que para algunos modelos, codificar los valores directamente como n\u00fameros pueden crear sesgo hacia las variables con mayor valor.\n\nPara evitar eso, haremos uso de `OneHotEncoder`","1754e0eb":"Seguidamente, lo que haremos ser\u00e1 unir los conjuntos `X_train` e `y_train` para obtener el conjunto de datos de entrenamiento. Haremos los mismo para `X_test` e `y_test`, juntando as\u00ed el conjunto de datos de test.","3351d515":"Con el prop\u00f3sito de usar m\u00e9tricas como el `recall` en el apartado de selecci\u00f3n de modelos, es necesario convertir nuestra variable clase que resulta ser categ\u00f3rica a una con valores num\u00e9ricos.\n\nPara ello elegiremos que la clase **M** (malign) pasar\u00e1 a tomar un valor de 1, y la clase **B** (benign) pasar\u00e1 a tomar un valor de 0.\n\nMas adelante veremos que dicho cambio se ha realizado correctamente.","3beb6b2d":"El modelo seleccionado utilizar\u00e1 un 50% de la instancias para entrenar los \u00e1rboles, junto con un n\u00famero m\u00e1ximo de variables guiado por `sqrt`, y haciendo uso de un criterio basado en el \u00edndice `gini`, garantizando as\u00ed un modelo con un recall y una accuracy adecuados para nuestro problema.","1fde587e":"## 4.7. Histogram-Based Gradient Boosting (Histogram Gradient Boosting)","cf9cc2b6":"# Mushroom Classification","73678a8c":"# 1. Preliminares","d929ed0f":"# 5. Selecci\u00f3n de modelos","d23256b5":"Para este algoritmo variaremos dos de los hiperpar\u00e1metros principales:\n\n* `n_neighbors`: la cantidad de ejemplos que se consideran para clasificar. Lo variaremos de $1$ a $15$\n\n* `weights`: el peso de cada ejemplo. Probaremos con pesos uniformes o proporcionales a la inversa de la distancia.","e5498dbd":"Podemos observar que existen diversas configuraciones de par\u00e1metros que nos dan la misma puntuaci\u00f3n m\u00e1xima. En este caso tomaremos la opci\u00f3n de no limitar la profundidad del \u00e1rbol excepto por obligar a que las hojas tengan al menos $5$ ejemplos para evitar sobreajustar totalmente.","b88cc2fb":"## 5.2. Naive Bayes","a97ff686":"## 3.1. Vecinos m\u00e1s cercanos","e97ee529":"Veamos ahora una serie de diagramas de barras que nos relacionar\u00e1n la distribuci\u00f3n de cada variable predictora con sus posibles valores junto a la variable clase, y el n\u00famero de ejemplos de las mismas","d6e5fbee":"Como podemos observar, tenemos una columna, la \u00faltima, cuyo nombre es `Unnamed` y todo el contenido de sus filas `NaN`. Esto es debido a que en la declaraci\u00f3n de las columnas en el archivo `csv` hay una `,` sobrante al final de la l\u00ednea, lo que hace que se cree una columna sin nombres y con valores inexistentes.\n\nEsto significa que antes de continuar trabajando con nuestro conjunto de datos, debemos borrar esa columna.","a2b6a277":"Hemos obtenido un 100% de precisi\u00f3n, por lo que podr\u00edamos estar teniendo un problema de sobreajuste. Para evaluar mejor los modelos de ahora en adelante, usaremos validaci\u00f3n cruzada. Al hacerlo, es probable que obtengamos peores resultados, pero estos ser\u00e1n m\u00e1s fiables.","dec79a32":"De acuerdo con estos resultados, podemos concluir que los **\u00e1rboles de decisi\u00f3n** y los ensembles como **Random Forest, Gradient Boosting** e **Histogram Gradient Boosting** son los clasificadores que nos ofrecen un mejor rendimiento en t\u00e9rminos de **recall** en nuestro conjunto de datos `wisconsin`.\n\nNo obstante, pese a centrarnos en el **accuracy** y sobre todo en el **recall**, es importante destacar y tener en cuenta otros factores como el tiempo de aprendizaje y de inferencia.\n\nSi bien los ensembles mencionados anteriormente mejoran ligeramente el recall y la precisi\u00f3n cuando los enfrentamos contra el conjunto de test, los **\u00e1rboles de decisi\u00f3n** tienen un tiempo de aprendizaje e inferencia mucho menor que el resto de modelos de ensembles estudiados.\n\nPor ello, antes de decantarnos por un modelo u otro para nuestro problema, debemos tener en cuenta si el aumento de coste computacional necesario para emplear un modelo m\u00e1s complejo compensa la ligera mejora que este supone frente a modelos m\u00e1s sencillos.\n\nEn el caso del problema que estudiamos, al ser una mejor\u00eda tan liviana, no nos resulta necesario aumentar el tiempo de aprendizaje e inferencia de ensembles como Random Forest, Gradient Boosting e Histogram Gradient Boosting, por lo que el modelo seleccionado finalmente han sido los **\u00c1rboles de Decisi\u00f3n**.","3fabea4e":"En primer lugar, definimos una funci\u00f3n para mostrar los resultados obtenido por cada uno de los modelos.","8055dce1":"Y a continuaci\u00f3n la variable clase:","4054e510":"Los algoritmos basados en inducci\u00f3n de \u00e1rboles de decisi\u00f3n representan los modelos mediante un conjunto de reglas.\n\nLos hiperpar\u00e1metros que a nuestro parecer resultan m\u00e1s interesantes para este modelo ser\u00e1n `max_depth`, que tomar\u00e1 un valor de 3 y se referir\u00e1 a la profundidad del arbol (evitaremos sobreajuste con este valor), y `min_samples_split`, que tomar\u00e1 un valor de 20, lo que significa que las hojas deber\u00e1n tener un m\u00ednimo de 20 instancias para evitar, de nuevo, un posible sobreajuste.\n\nEs necesario comentar que posteriormente, en el apartado de evaluaci\u00f3n de modelos, estudiaremos de mejor forma estos hiperpar\u00e1metros.","61dc8e52":"Como podemos observar, los mejores resultados se han obtenido con distinto n\u00famero m\u00e1ximo de hojas de los \u00e1rboles, por lo que podemos deducir que este hiperpar\u00e1metro no es el m\u00e1s importante. Se usar\u00e1 por tanto el modelo de menor complejidad. Sin embargo, la tasa de aprendizaje que ha dado mejores resultados ha sido $0.03$.","b37ce1de":"Este algoritmo propone un entrenamiento de una serie de clasificadores de manera iterativa, de modo que cada nuevo clasificador se enfoque en los datos que fueron err\u00f3neamente clasificados por su predecesor, de esta forma el algoritmo se adapta y logra obtener mejores resultados.\n\nLos hiperpar\u00e1metros m\u00e1s relevantes a tener en cuenta ser\u00e1n `base_estimator`, que nos indica el estimador que utilizar\u00e1 el ensemble (en este caso un \u00c1rbol de decisi\u00f3n), el n\u00famero de estimadores `n_estimators`, o la tasa de aprendizaje que nos ayudar\u00e1 a controlar la contribuci\u00f3n de cada estimador `learning_rate`.\n\nEs necesario comentar que posteriormente, en el apartado de evaluaci\u00f3n de modelos, estudiaremos de mejor forma estos hiperpar\u00e1metros.","6a13520a":"## 3.8. Histogram-Based Gradient Boosting (*Histogram Gradient Boosting*)","d77f932c":"Bagging es un ensemble cuya estrategia es utilizar una funci\u00f3n de aprendizaje y obtener modelos diversos entre s\u00ed para reducir el error obtenido mediante varianza. Mediante un voto de estos modelos se obtendr\u00e1 el ensemble.\n\nLos hiperpar\u00e1metros m\u00e1s relevantes a tener en cuenta ser\u00e1n `base_estimator`, que nos indica el estimador que utilizar\u00e1 el ensemble (en este caso un \u00c1rbol de decisi\u00f3n) o el n\u00famero de estimadores `n_estimators`. Para un muestreo con reemplazo, emplearemos el par\u00e1metro `bootstrap_features=True`.defecto, `random_state=None`).\n\nEs necesario comentar que posteriormente, en el apartado de evaluaci\u00f3n de modelos, estudiaremos de mejor forma estos hiperpar\u00e1metros.","081bc76c":"# 6. \u00bfPrecisi\u00f3n del 100%?","c1e270b0":"## 4.2. Codificaci\u00f3n de los datos","a15e4c6c":"Comenzemos mostrando la distribuci\u00f3n de la clase","fa679c40":"Una vez podamos a\u00f1adir al **Pipeline** el preprocesamiento de datos obtenido, podemos pasar a tratar los modelos de clasificaci\u00f3n para nuestro conjunto de datos.","16e5234b":"Como podemos observar, el problema est\u00e1 cerca de ser balanceado, pero encontramos alg\u00fan ejemplo m\u00e1s de setas venenosas que comestibles","6451d134":"---","7fce2e32":"## 4.1. Eliminacion de variables","57bdba9e":"**Librer\u00edas usadas** \n\n* [Numpy](https:\/\/numpy.org\/): Estudiar y trabajar con los datos\n* [Pandas](https:\/\/pandas.pydata.org\/): Trabajr con el conjunto de datos\n* [Sklearn](https:\/\/scikit-learn.org\/stable\/): Crear y evaluar los modelos\n* [Seaborn](https:\/\/seaborn.pydata.org\/): Visualizar gr\u00e1ficas con los datos\n* [Matplotlib](https:\/\/matplotlib.org\/): Visualizar gr\u00e1ficas con los datos\n","21c8e2b4":"Definimos las 2 m\u00e9tricas que usaremos para seleccionar nuestros modelos, como son el `accuracy` y el `recall`. A la hora de elegir los mejores modelos, seleccionaremos aquellos que logren un mejor resultado en recall, el ratio de verdaderos positivos y falsos negativos obtenido mediante $\\frac {tp}{(tp+fn)}$","50527e15":"## 3.4. Adaptative Boosting (*AdaBoost*)","4463040d":"Mostramos el tama\u00f1o del conjunto de datos","88b04513":"El an\u00e1lisis exploratorio de datos es un paso fundamental a la hora de comprender los datos con los que vamos a trabajar.\n\nEl objetivo de este an\u00e1lisis es explorar, describir y visualizar la naturaleza de los datos recogidos mediante la aplicaci\u00f3n de t\u00e9cnicas simples de resumen de datos y m\u00e9todos gr\u00e1ficos, para observar las posibles relaciones entre las variables de nuestro conjunto de datos.","3f768187":"Vamos a configurar nuestro \u00e1rbol de decisi\u00f3n con lo mencionado anteriormente:","10ab5dd6":"## 4.3. Adaptative Boosting (*AdaBoost*)","4cf0934b":"### 2.1.1. Eliminaci\u00f3n de variables","35019f1d":"## 3.7. Gradient Tree Boosting (*Gradient Boosting*)","90149c7f":"# 2. Carga de datos","ac555e1c":"Ser\u00e1 necesario **normalizar** el conjunto de datos a la hora de definir los modelos a emplear, de forma que todas las variables predictoras tengan el mismo peso.\n\nPara ello definiremos una variable `normalizacion` que haga uso de la funci\u00f3n `Normalizer()`, la cual normaliza individualmente los datos a una norma unitaria, justo lo que buscamos.","ff6b1bb0":"Cargamos el conjunto de datos y visualizamos sus elementos","aba2500b":"En la libreta en la cual nos hemos basado, se pasaba directamente a realizar un an\u00e1lisis exploratorio de los datos teniendo en cuenta la totalidad del conjunto de datos.\n\nEsto significa que exploraba con el mismo conjunto de datos con los que luego iba a validar el modelo, causando as\u00ed un **Data Leak**. Para solucionar este problema, hemos decidido dividir primero nuestro conjunto de datos en dos subconjuntos de entrenamiento y test.\n\nDe esta manera, trabajaremos en el an\u00e1lisis exploratorio con este conjunto de entrenamiento, y el de test permanecer\u00e1 sin usarse hasta el apartado de selecci\u00f3n de modelos.","d747128b":"Por tanto, antes de comenzar el an\u00e1lisis exploratorio de los datos, dividiremos nuestro conjunto de datos en otros dos subconjuntos, uno de entrenamiento y otro de prueba, con los siguientes porcentajes:\n\n* Conjunto de entrenamiento: **80%**\n* Conjunto de prueba: **20%**\n\nMediante este proceso, nos aseguraremos de que los resultados posteriores del proceso de validaci\u00f3n han sido obtenidos de una manera correcta.","8bf017c6":"## 5.7. Histogram-Based Gradient Boosting (Histogram Gradient Boosting)\n\n","156c7743":"Lo siguiente que debemos hacer es fijar una semilla con el objetivo de que todo lo que hagamos sea reproducible.","f17fdae1":"Fijamos tambi\u00e9n una semilla para que los experimentos sean reproducibles","835c9cc9":"Una vez que en el apartado anterior hemos obtenido mediante la b\u00fasqueda **grid** todos los modelos, podemos emplearlos contra el conjunto de test que reservamos al principio del documento para evaluar cu\u00e1l nos proporciona mejores resultados en las m\u00e9tricas elegidas.","fad9014e":"Comenzamos cargando el conjunto de datos `wisconsin`:","f83fb3e5":"La tasa de aprendizaje (learning rate), se ha dejado a 0.025, mientras que la profundidad del \u00e1rbol parece no ser demasiado importante. Puesto que toma un valor de 3, podemos extraer que \u00e1rboles m\u00e1s peque\u00f1os trabajan mejor con nuestro algoritmos que \u00e1rboles que puedan tener una mayor profundidad y por tanto un mayor sobreajuste con nuestro conjunto de entrenamiento.","ecf58156":"Una vez se ha realizado la b\u00fasqueda **Grid**, vemos que el modelo que nos proporciona mejores resultados cuenta con un `ccp_alpha` de 0, lo que nos indica que una posible post poda no ayudar\u00eda a mejorar el modelo. Adem\u00e1s vemos que el criterio elegido es el de entrop\u00eda que trata de maximizar la ganancia de informaci\u00f3n, una profundidad del \u00e1rbol de 3, y un n\u00famero m\u00ednimo de ejemplos por nodo hoja de 10.","3614625a":"Una vez tenemos bien definidos nuestros conjuntos de entrenamiento y prueba, podemos aplicar el **preprocesamiento de datos** definido en la pr\u00e1ctica anterior.","99e5a116":"Vamos a estudiar el siguiente kernel [Mushroom Classification & why it's easy to 100%ac](https:\/\/www.kaggle.com\/arevel\/mushroom-classification-why-it-s-easy-to-100-ac) realizando las modificaciones que consideramos necesarias:\n* Traducci\u00f3n de la libreta.\n* Uso de un script de utilidades con el c\u00f3digo necesario para plotear gr\u00e1ficas y generar los modelos.\n* Arreglo de **Data Leaks** encontrados.","e5380aad":"En esta selecci\u00f3n de modelos trataremos de optimizar los par\u00e1metros que consideramos son m\u00e1s relevantes en Histogram Gradient Boosting.\n\nLos hiperpar\u00e1metros elegidos para ser optimizados han sido:\n* `min_samples_leaf`: M\u00ednimo n\u00famero de instancias para poder tener en cuenta un nodo hoja, lo cual afecta al suavizado del modelo que sea elegido. \n* `learning_rate`: Tasa de aprendizaje usada como factor multiplicativo para los nodos hoja. Toma un valor por defecto de 0.1.\n","9aab6c04":"##\u00a02.1. Preprocesamiento de datos","9eb9a77a":"En esta libreta trabajaremos con distintos tipos de modelos que evaluaremos posteriormente para tratar de clasificar las setas en dos clases:\n* Comestibles (edible) clasificadas como `e`.\n* Venenosas (poisonous) clasificadas como `p`.","d8c43404":"Comprobamos si hay valores nulos","7d76109f":"# 4. Preprocesamiento de los datos","1d3a1cd0":"Para el caso del *bagging* aprovecharemos los hiperpar\u00e1metros por defecto (queremos muestreo con reemplazo y la totalidad de las caracter\u00edsticas), y solo modificaremos el n\u00famero de clasificadores (`n_estimators`) y el criterio del \u00e1rbol de decisi\u00f3n usado como base (`criterion`), que esta vez interesa que sea profundo por lo que mantendremos los par\u00e1metros relacionados con la prepoda (`max_depth`, `min_samples_leaf`, etc) o pospoda (`ccp_alpha`) en su valor por defecto.\n","566348c9":"# 1. Preliminares","ce042f66":"# 3. An\u00e1lisis exploratorio de los datos","a2619730":"Comprobaremos que se hayan separado correctamente:","67533626":"Podemos observar c\u00f3mo la precisi\u00f3n del 99% obtenida previamente no era fiable.","4fcc6158":"La estrategia seleccionada utiliza una distribuci\u00f3n de pesos uniforme, y un n\u00famero de vecinos muy peque\u00f1o: 3. Es interesante que \u00e9ste sea el n\u00famero de vecinos, puesto que es el menor de los proporcionados y hemos comentado que un peque\u00f1o n\u00famero de vecinos podr\u00eda causarnos sobreajuste.","324ea7f0":"Comprobamos que se ha borrado correctamente haciendo uso del m\u00e9todo `sample` de nuevo:","4398cccc":"Para este algoritmo, tendremos en cuenta al igual que con el resto de *ensembles* el n\u00famero de clasificadores y la tasa de aprendizaje. Para los \u00e1rboles usados como base intentaremos ajustar la profundidad m\u00e1xima y el par\u00e1metro de complejidad para la poda. Al estar hablando ahora de \u00e1rboles de regresi\u00f3n en lugar de clasificaci\u00f3n usaremos como criterio el error cuadrado medio con y sin la mejora de Friedman. Los par\u00e1metros del algoritmo que variaremos, por tanto, son:\n\n* `criterion`\n* `n_estimators`\n* `learning_rate`\n* `max_depth`\n* `ccp_alpha`","2c3de0a6":"# 1. Preliminares\n\nPrimero cargaremos toda la funcionalidad que necesitaremos:\n* Los distintos algoritmos que aprenderemos (`AdaBoostClassifier`, \n`BaggingClassifier`, \n`GradientBoostingClassifier`, \n`HistGradientBoostingClassifier`, \n`RandomForestClassifier`, \n`KNeighborsClassifier` y \n`DecisionTreeClassifier`)\n\n* Funcionalidad relacionada con la selecci\u00f3n y evaluaci\u00f3n de modelos (`RepeatedStratifiedKFold` y \n`train_test_split`)","1be60298":"Si nos fijamos en los mejores modelos, encontramos que la tasa de aprendizaje preferible de entre las probadas es $0.05$. Adem\u00e1s un mayor n\u00famero de \u00e1rboles mejora los resultados como viene siendo habitual. Ambos criterios de calidad de las particiones probados no presentan una diferencia significativa en los resultados. Los \u00e1rboles que mejor han funcionado han sido los de profundidad $3$ sin pospoda (`ccp_alpha` $=0$).","e5603319":"# 3. Selecci\u00f3n de modelos\n\nEn esta pr\u00e1ctica, aplicaremos distintos algoritmos de aprendizaje de clasificadores con el objetivo de encontrar el que mejor funciona para el problema en cuesti\u00f3n (lograr predecir que un paciente tiene diabetes). Estos son:\n\n* Vecinos m\u00e1s cercanos\n\n* \u00c1rbol de decisi\u00f3n\n\n* AdaBoost\n\n* Bagging\n\n* Random Forests\n\n* Gradient Boosting\n\n* Histogram Gradient Boosting\n\nPor cada algoritmo, buscaremos los hiperpar\u00e1metros que mejor funcionen mediante el algoritmo de b\u00fasqueda en Grid. La evaluaci\u00f3n de los modelos obtenidos se har\u00e1 mediante una $10 \\times 5$ validaci\u00f3n cruzada, usando como m\u00e9trica de rendimiento el *recall*, al tratarse de un problema desbalanceado.","837bd9af":"## 5.5. Random Forests","2f97d77a":"# 5. Construcci\u00f3n y validaci\u00f3n del modelo final","672977ac":"# Pr\u00e1ctica 2: Aprendizaje y selecci\u00f3n de modelos de clasificaci\u00f3n\n\n### Miner\u00eda de Datos: Curso acad\u00e9mico 2020-2021\n\n###\u00a0Profesorado:\n\n* Juan Carlos Alfaro Jim\u00e9nez\n* Jos\u00e9 Antonio G\u00e1mez Mart\u00edn\n\n### Realizado por:\n\n* Antonio Beltr\u00e1n Navarro\n* Ram\u00f3n Jes\u00fas Mart\u00ednez S\u00e1nchez\n\n\\* Adaptado de las pr\u00e1cticas de Jacinto Arias Mart\u00ednez y Enrique Gonz\u00e1lez Rodrigo","22d9a324":"\nVamos a configurar un modelo AdaBoost sencillo que utilice los hiperpar\u00e1metros por defecto:","ea50b441":"Veamos, para terminar, un resumen de la precisi\u00f3n obtenida por cada uno de los modelos que hemos estudiado para resolver nuestro problema.","a6b83f67":"## 5.6. Gradient Tree Boosting (Gradient Boosting)","41c2327c":"## 3.2. \u00c1rbol de decisi\u00f3n","37ec72bd":"## 3.6. Random Forests","cfb98006":"\nVamos a configurar un ensemble tipo Bagging utilizando los hiperpar\u00e1metros por defecto:","5d230799":"Es el momento de definir los diferentes modelos que utilizaremos en esta pr\u00e1ctica. Cada uno de estos modelos contar\u00e1 con el preprocesamiento de datos definido previamente haciendo uso del **Pipeline**.\n\nComentaremos los par\u00e1metros de cada modelo m\u00e1s importantes para nuestro problema y que utilizaremos posteriormente mediante un proceso de validaci\u00f3n cruzada."}}