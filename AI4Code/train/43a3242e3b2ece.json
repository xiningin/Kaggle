{"cell_type":{"b5123c6a":"code","32362a6a":"code","340648e2":"code","6fa16cb5":"code","50390ba9":"code","4f51ffb6":"code","393fa221":"code","92aa9383":"code","56b9aca3":"code","1b1add6e":"code","5034296b":"code","fc768378":"code","52ce3cae":"code","de455733":"code","8cbc8625":"code","f78be702":"code","8748f438":"code","3f9b6632":"code","5bbc5cef":"code","45cf3129":"code","0ac56f57":"code","bf9dc330":"code","79b7f473":"code","c48cd7e6":"code","4aec8bda":"code","e3d8057f":"code","7ad4422a":"code","7101bcf6":"code","763cf3f1":"code","6c6a5ed4":"code","8b9080f3":"code","f8674f88":"code","4f84928a":"code","60af3fdc":"code","8911b4d3":"code","93000236":"code","345ec69a":"code","b9f811dd":"code","7d464a12":"code","a2486262":"code","c6261dbd":"code","4e456aa4":"code","f95185e6":"code","692515c0":"code","9385c5bd":"code","66ce47f7":"code","7400ff99":"code","0609ff32":"code","5be902fc":"code","3ac62392":"code","13b741ad":"code","6d2b5407":"code","57125bec":"code","25780230":"code","da253eab":"code","8739c63b":"code","46e8c9a1":"code","102d34c2":"code","0ebd689f":"code","a05668d2":"code","e83a65cf":"code","8ce79265":"code","aa8fe0e7":"code","c5f855ae":"code","eca47412":"code","f931188d":"code","05a83206":"code","ad0ee6aa":"code","a36c7fbc":"code","44a7812b":"code","88045d5d":"code","2814ac1c":"code","a0da3f68":"code","d0fa00ed":"code","de65b7fb":"code","0c647519":"code","9638b021":"code","7aad80ab":"code","bb732e42":"code","288984e9":"code","6603f3c1":"code","fb758370":"code","1b82741b":"code","3dc674f1":"code","06d57c98":"code","ae0b39e6":"code","106a58a3":"code","48d413b4":"code","48e840d0":"code","003f892f":"code","72f67d53":"code","cd674346":"code","99bdcd5b":"code","992393e5":"code","be4e9420":"code","6393e549":"code","7abb5728":"code","9b2837b4":"code","177bc334":"code","bfbbf33a":"code","b889df04":"code","cb814c37":"code","20bf4c64":"code","e03e1132":"code","2f2d73a2":"code","0eb2ec7b":"code","5bfa30f0":"code","5b7fddaa":"code","af6be093":"code","ad77860d":"code","595ba43a":"code","3dd91f85":"code","a68753b1":"code","734a2083":"code","a3f1dfd7":"code","e7cc5c53":"code","f68dfb2e":"code","100703c1":"markdown","6edd6c4e":"markdown","eace339e":"markdown","23fd9f32":"markdown","bd9ccb9c":"markdown","1574f235":"markdown","fd169a7c":"markdown","cac47585":"markdown","416c0d26":"markdown","5466db1d":"markdown","5840a7c8":"markdown","b796810f":"markdown","a0dce64c":"markdown","3c6eb4e3":"markdown","dfc226cd":"markdown","d5f39d67":"markdown","8ce76509":"markdown","4bf9ca1d":"markdown","34032754":"markdown","04d4c4fe":"markdown","7eab7bd0":"markdown","0ac113ba":"markdown","8e4fdcf6":"markdown","c281bcb4":"markdown","57405a76":"markdown","f7f6036c":"markdown","4532c5c6":"markdown","4a4df37f":"markdown","d8f8d1d5":"markdown","07281c76":"markdown","686b284d":"markdown","d57a8edb":"markdown","68c117ff":"markdown","f2797735":"markdown","1e270041":"markdown","117515b8":"markdown","ed4b250a":"markdown","c5b1de1c":"markdown","2f26cdb3":"markdown","02af75f1":"markdown","ce4916a2":"markdown","486d1c1c":"markdown","7b75ebfe":"markdown","70195e79":"markdown","3928ad63":"markdown","9a5d779b":"markdown","cb84dbdf":"markdown","fcf1e750":"markdown","11792f72":"markdown","0203fc34":"markdown","20973ac7":"markdown","cca874d2":"markdown","cb967788":"markdown","afea85da":"markdown","4ff845c3":"markdown","a5daffb6":"markdown","a8840668":"markdown","5d151211":"markdown","36e23754":"markdown","4824e946":"markdown","03757bd3":"markdown","ffc10ebc":"markdown","f3c54dbd":"markdown","17821c2a":"markdown","ff350505":"markdown","1dee653e":"markdown","1eb34794":"markdown","0d02ecc2":"markdown","65601107":"markdown","e13a9b3f":"markdown","0795c1c8":"markdown","a1c53827":"markdown","cf138c2b":"markdown","a99fa6cb":"markdown","f934df83":"markdown","e6e84b09":"markdown","82159491":"markdown","c4643275":"markdown","e3d8385a":"markdown","88dd8215":"markdown","0ae97424":"markdown","c8dd3f1f":"markdown","a89bff35":"markdown","4c569818":"markdown","d0f486a0":"markdown"},"source":{"b5123c6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32362a6a":"!pip install pycaret-nightly\n!pip install missingno","340648e2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn import preprocessing\nimport umap\nimport umap.plot\n\nimport warnings\nwarnings.filterwarnings('ignore')","6fa16cb5":"def get_clf_eval(y_test, pred=None, pred_proba=None):\n    confusion = confusion_matrix( y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test , pred)\n    recall = recall_score(y_test , pred)\n    f1 = f1_score(y_test,pred)\n\n    roc_auc = roc_auc_score(y_test, pred_proba)\n    print('confusion matrix')\n    print(confusion)\n\n    # ROC-AUC print \n    print('accuracy: {0:.4f}, precision: {1:.4f}, recall: {2:.4f},\\\n    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))\n    return confusion","50390ba9":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings","4f51ffb6":"def outlier_iqr(data):\n    q1,q3 = np.percentile(data,[25,75])\n    iqr = q3-q1\n    lower = q1-(iqr*1.5)\n    upper = q3+(iqr*1.5)\n    return np.where((data>upper)|(data<lower))","393fa221":"def encode_features(dataDF,feat_list):\n    for feature in feat_list:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(dataDF[feature])\n        dataDF[feature] = le.transform(dataDF[feature])\n        \n    return dataDF","92aa9383":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission_data = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ntitanic_df = pd.concat([train_data, test_data], ignore_index = True, sort = False)\ntr_idx = titanic_df['Survived'].notnull()","56b9aca3":"titanic_df.head(3).T.style.set_properties(**{'background-color': 'lightyellow',\n                           'color': 'darkblack',\n                           'border-color': 'darkblack'})","1b1add6e":"titanic_df.shape","5034296b":"titanic_df.drop(['PassengerId'],axis=1,inplace=True)","fc768378":"titanic_df.info()","52ce3cae":"print(plt.style.available)","de455733":"total_cnt = titanic_df['Survived'].count()\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nf, ax = plt.subplots(1, 2, figsize = (18, 8))\ntitanic_df['Survived'].value_counts().plot.pie(explode = [0, 0.1], \n                                               autopct = '%1.1f%%', \n                                               ax = ax[0],\n                                               shadow = True,\n                                               colors = ['grey', 'green'])\nax[0].set_title('Survived %')\nax[0].set_ylabel('')\nsns.countplot('Survived', data = titanic_df, ax = ax[1], palette='Blues_r')\nax[1].set_title('Survived')\nfor p in ax[1].patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax[1].text(x + width \/ 2, height + 10, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=20)\nsns.despine()","8cbc8625":"import missingno as msno\nmsno.matrix(titanic_df.drop(['Survived'],axis=1))","f78be702":"titanic_df.drop(['Survived'],axis=1).isnull().sum()","8748f438":"titanic_df['Cabin'].unique()","3f9b6632":"titanic_df['Cabin'].isnull().sum()","5bbc5cef":"titanic_df['Has_Cabin'] = titanic_df['Cabin'].isnull().astype(int)","45cf3129":"total_cnt = titanic_df['Survived'].count()\nrcParams['figure.figsize'] = 12,8\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nax = sns.countplot(x=\"Has_Cabin\",\n                   hue=\"Survived\", \n                   data=titanic_df,\n                   palette = 'Blues_r')\nax.set_title('Survived Count\/Rate')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 10, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=20)\nsns.despine()","0ac56f57":"rcParams['figure.figsize'] = 10,7\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\ntitanic_df['Cabin'] = titanic_df['Cabin'].fillna('N')\ntitanic_df['Cabin_label'] = titanic_df['Cabin'].str.get(0)\nax = sns.barplot(x = 'Cabin_label', y = 'Survived', data = titanic_df, palette = 'Blues_r',ci=False)\nsns.despine()","bf9dc330":"rcParams['figure.figsize'] = 20,10\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nax = sns.countplot(x=\"Cabin_label\", hue=\"Survived\", data=titanic_df,palette = 'Blues_r')\nax.set_title('Survived Rate')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 7, f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=20)\nsns.despine()","79b7f473":"rcParams['figure.figsize'] = 20,10\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nax = sns.countplot(x='Parch',hue ='Survived',data=titanic_df,palette=\"Blues_r\")\nax.set_title('Survived Rate')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 7, f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=20)\nsns.despine()","c48cd7e6":"rcParams['figure.figsize'] = 20,10\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nax = sns.countplot(x='SibSp',hue ='Survived',data=titanic_df,palette=\"Blues_r\")\nax.set_title('Survived Rate')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 7, f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=20)\nsns.despine()","4aec8bda":"titanic_df['FamilySize'] = titanic_df['SibSp'] + titanic_df['Parch'] + 1","e3d8057f":"rcParams['figure.figsize'] = 20,10\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nax = sns.countplot(x='FamilySize',hue ='Survived',data=titanic_df,palette=\"Blues_r\")\nax.set_title('Survived Rate')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 7, f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=20)\nsns.despine()","7ad4422a":"titanic_df['IsAlone'] = 0\ntitanic_df.loc[titanic_df['FamilySize'] == 1, 'IsAlone'] = 1","7101bcf6":"rcParams['figure.figsize'] = 10,6\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nax = sns.countplot(x='IsAlone',hue ='Survived',data=titanic_df,palette=\"Blues_r\")\nax.set_title('Survived Rate')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 7, f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=25)\nsns.despine()","763cf3f1":"titanic_df['Name'].unique()[:5]","6c6a5ed4":"import re\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\ntitanic_df['Title'] = titanic_df['Name'].apply(get_title)","8b9080f3":"titanic_df['Title'].unique()","f8674f88":"titanic_df['Title'] = titanic_df['Title'].replace(\n       ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], \n       'Rare')\n\ntitanic_df['Title'] = titanic_df['Title'].replace('Mlle', 'Miss')\ntitanic_df['Title'] = titanic_df['Title'].replace('Ms', 'Miss')\ntitanic_df['Title'] = titanic_df['Title'].replace('Mme', 'Mrs')\ntitanic_df['Title'].unique()","4f84928a":"rcParams['figure.figsize'] = 20,10\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nax = sns.countplot(x='Title',hue ='Survived',data=titanic_df,palette=\"Blues_r\")\nax.set_title('Survived Rate')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 7, f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=25)\nsns.despine()","60af3fdc":"rcParams['figure.figsize'] = 20,15\ntitles = titanic_df['Title'].unique()\nplt.subplots_adjust(hspace=1.5)\nidx = 1\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\n\nfor title in titles:\n    plt.subplot(3,2,idx)\n    ax = sns.histplot(x='Age',data=titanic_df[titanic_df['Title']== title],hue ='Survived',palette=\"Blues_d\",kde=True)\n    ax.set_title(title)\n    sns.despine()\n    idx = idx + 1","8911b4d3":"titanic_df['Age'].isnull().sum()","93000236":"titanic_df['Has_Age'] = titanic_df['Age'].isnull().astype(int)","345ec69a":"rcParams['figure.figsize'] = 10,6\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nax = sns.countplot(x='Has_Age',hue ='Survived',data=titanic_df,palette=\"Blues_r\")\nplt.legend(loc = 'upper right')\nax.set_title('Survived Rate')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 10,f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=25)\nsns.despine()","b9f811dd":"rcParams['figure.figsize'] = 12,7\nsns.set_palette(\"bright\")\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nax = sns.histplot(x=\"Age\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['Age'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['Age'].mean(), 60, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()","7d464a12":"mean = titanic_df['Age'].mean()\nstd = titanic_df['Age'].std()\nskew = titanic_df['Age'].skew()\nprint('Age : mean: {0:.4f}, std: {1:.4f}, skew: {2:.4f}'.format(mean, std, skew))","a2486262":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntitanic_df[['Age_mean']] = imp.fit_transform(titanic_df[['Age']])","c6261dbd":"rcParams['figure.figsize'] = 20,6\nsns.set_palette(\"bright\")\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nplt.subplot(1,2,1)\nax = sns.histplot(x=\"Age\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['Age'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['Age'].mean(), 60, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()\nplt.subplot(1,2,2)\nax = sns.histplot(x=\"Age_mean\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['Age_mean'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['Age_mean'].mean(), 60, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()","4e456aa4":"mean = titanic_df['Age_mean'].mean()\nstd = titanic_df['Age_mean'].std()\nskew = titanic_df['Age_mean'].skew()\nprint('Age_mean : mean: {0:.4f}, std: {1:.4f}, skew: {2:.4f}'.format(mean, std, skew))","f95185e6":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=10, random_state=0)\ntitanic_df[['Age_iter']] = imp.fit_transform(titanic_df[['Age']])","692515c0":"rcParams['figure.figsize'] = 20,6\nsns.set_palette(\"bright\")\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nplt.subplot(1,2,1)\nax = sns.histplot(x=\"Age\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['Age'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['Age'].mean(), 60, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()\nplt.subplot(1,2,2)\nax = sns.histplot(x=\"Age_iter\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['Age_iter'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['Age_iter'].mean(), 60, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()","9385c5bd":"mean = titanic_df['Age_iter'].mean()\nstd = titanic_df['Age_iter'].std()\nskew = titanic_df['Age_iter'].skew()\nprint('Age_iter : mean: {0:.4f}, std: {1:.4f}, skew: {2:.4f}'.format(mean, std, skew))","66ce47f7":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\ntitanic_df[['Age_knn']] = imputer.fit_transform(titanic_df[['Age']])","7400ff99":"rcParams['figure.figsize'] = 20,6\nsns.set_palette(\"bright\")\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nplt.subplot(1,2,1)\nax = sns.histplot(x=\"Age\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['Age'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['Age'].mean(), 60, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()\nplt.subplot(1,2,2)\nax = sns.histplot(x=\"Age_knn\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['Age_knn'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['Age_knn'].mean(), 60, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()","0609ff32":"mean = titanic_df['Age_knn'].mean()\nstd = titanic_df['Age_knn'].std()\nskew = titanic_df['Age_knn'].skew()\nprint('Age_knn : mean: {0:.4f}, std: {1:.4f}, skew: {2:.4f}'.format(mean, std, skew))","5be902fc":"titanic_df['new_Age'] = titanic_df['Age']","3ac62392":"for title in titles:   \n    t_mean = titanic_df[titanic_df['Title']== title]\n    print('{} mean ===> {}'.format(title, t_mean['new_Age'].mean()))","13b741ad":"titanic_df['new_Age'].fillna(titanic_df.groupby('Title')['new_Age'].transform('mean'), inplace=True)","6d2b5407":"rcParams['figure.figsize'] = 20,6\nsns.set_palette(\"bright\")\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nplt.subplot(1,2,1)\nax = sns.histplot(x=\"Age\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['Age'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['Age'].mean(), 60, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()\nplt.subplot(1,2,2)\nax = sns.histplot(x=\"new_Age\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['new_Age'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['new_Age'].mean(), 60, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()","57125bec":"mean = titanic_df['new_Age'].mean()\nstd = titanic_df['new_Age'].std()\nskew = titanic_df['new_Age'].skew()\nprint('new_Age : mean: {0:.4f}, std: {1:.4f}, skew: {2:.4f}'.format(mean, std, skew))","25780230":"from sklearn.preprocessing import RobustScaler\nrobuster = RobustScaler()\ntitanic_df['Age_knn'] = robuster.fit_transform(titanic_df[['Age_knn']])","da253eab":"rcParams['figure.figsize'] = 20,6\nsns.set_palette(\"bright\")\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nplt.subplot(1,2,1)\nax = sns.histplot(x=\"Age\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['Age'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['Age'].mean(), 60, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()\nplt.subplot(1,2,2)\nax = sns.histplot(x=\"Age_knn\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['Age_knn'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['Age_knn'].mean(), 60, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()","8739c63b":"mean = titanic_df['Age_knn'].mean()\nstd = titanic_df['Age_knn'].std()\nskew = titanic_df['Age_knn'].skew()\nprint('Age_knn : mean: {0:.4f}, std: {1:.4f}, skew: {2:.4f}'.format(mean, std, skew))","46e8c9a1":"titanic_df.drop(['Age','Age_mean','Age_iter','new_Age'],axis=1,inplace=True)","102d34c2":"rcParams['figure.figsize'] = 12,7\nsns.set_palette(\"bright\")\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nax = sns.histplot(x=\"Fare\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['Fare'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['Fare'].mean(), 90, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()","0ebd689f":"mean = titanic_df['Fare'].mean()\nstd = titanic_df['Fare'].std()\nskew = titanic_df['Fare'].skew()\nprint('Fare : mean: {0:.4f}, std: {1:.4f}, skew: {2:.4f}'.format(mean, std, skew))","a05668d2":"from sklearn.preprocessing import QuantileTransformer\ntransformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\ntitanic_df['Fare'] = transformer.fit_transform(titanic_df[['Fare']])","e83a65cf":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntitanic_df[['Fare']] = imp.fit_transform(titanic_df[['Fare']])","8ce79265":"rcParams['figure.figsize'] = 12,7\nsns.set_palette(\"bright\")\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nax = sns.histplot(x=\"Fare\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nplt.axvline(x=titanic_df['Fare'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(0, 100, \"Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\nsns.despine()","aa8fe0e7":"titanic_df['Fare_class'] = pd.qcut(titanic_df['Fare'], 5, labels=['F1', 'F2', 'F3','F4','F5' ])","c5f855ae":"rcParams['figure.figsize'] = 12,7\nsns.set_palette(\"bright\")\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nax = sns.histplot(x=\"Fare_class\", hue=\"Survived\", data=titanic_df,palette = 'Blues_d',kde=True)\nsns.despine()","eca47412":"titanic_df['Fare_class'] = titanic_df['Fare_class'].replace({'F1':1,'F2':2,'F3':3,'F4':4,'F5':5})","f931188d":"rcParams['figure.figsize'] = 12,7\nsns.set_palette(\"bright\")\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nax = sns.countplot(x='Embarked',hue = 'Survived',data=titanic_df,palette=\"Blues_d\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width() \/ 2., height + 3, f'{height \/ total_cnt * 100:2.1f}%', ha = 'center', size = 25)\nsns.despine()","05a83206":"rcParams['figure.figsize'] = 12,7\nsns.set_palette(\"bright\")\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nax = sns.countplot(x='Embarked',hue = 'Sex',data=titanic_df,palette=\"Blues_d\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width() \/ 2., height + 3, f'{height \/ total_cnt * 100:2.1f}%', ha = 'center', size = 25)\nsns.despine()","ad0ee6aa":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ntitanic_df[['Embarked']] = imp.fit_transform(titanic_df[['Embarked']])","a36c7fbc":"import missingno as msno\nmsno.matrix(titanic_df.drop('Survived',axis=1))\nprint('Number of Missing Values in Dataset ',titanic_df.drop('Survived',axis=1).isnull().sum().sum())","44a7812b":"from sklearn.decomposition import PCA","88045d5d":"features = [\"Sex\",\"Age_knn\",\"FamilySize\",\"IsAlone\",'Embarked','Cabin_label']\ntitanic_copy = titanic_df[tr_idx].copy()\ny_copy = titanic_copy.pop(\"Survived\")\nX_copy = titanic_copy.loc[:, features]\nencode_features(X_copy,['Sex', 'Embarked','Cabin_label'])\npca, X_pca, loadings = apply_pca(X_copy)\nprint(loadings)","2814ac1c":"import plotly.express as px\nfig = px.histogram(X_pca.melt(), color=\"variable\", \n                   marginal=\"box\",\n                   barmode =\"overlay\",\n                   histnorm ='density'\n                  )  \nfig.update_layout(\n    title_font_color=\"black\",\n    legend_title_font_color=\"green\",\n    title={\n        'text': \"PCA Histogram\",\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n)","a0da3f68":"pc1_outlier_idx = list(outlier_iqr(X_pca['PC1'])[0])","d0fa00ed":"component = \"PC1\"\n\ndef highlight_min(s, props=''):\n    return np.where(s == np.nanmin(s.values), props, '')\n\ntrain_data.iloc[pc1_outlier_idx,:].style.set_properties(**{'background-color': 'Grey',\n                            'color': 'white',\n                            'border-color': 'darkblack'})","de65b7fb":"titanic_df = pd.get_dummies(titanic_df, columns = ['Title','Sex', 'Embarked','Cabin_label'],drop_first=True)\ntitanic_df.head().T.style.set_properties(**{'background-color': 'lightyellow',\n                           'color': 'darkblack',\n                           'border-color': 'darkblack'})","0c647519":"corr=titanic_df.corr().round(1)\n\nsns.set(font_scale=1.15)\nplt.figure(figsize=(14, 10))\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,annot=True,cmap='Blues',mask=mask,cbar=True)\nplt.title('Correlation Plot')","9638b021":"abs(corr['Survived']).sort_values()[:-1].plot.barh()\nplt.gca().set_facecolor('#FFFFFF')","7aad80ab":"def drop_features(df):\n    df.drop(['Name','Ticket','SibSp','Parch','Fare_class',\n             'Cabin','Cabin_label_G','Cabin_label_T',\n             'Cabin_label_F','FamilySize','Embarked_Q','Title_Rare'],\n            axis=1,\n            inplace=True)\n    return df\n\ntitanic_df = drop_features(titanic_df)","bb732e42":"corr=titanic_df.corr().round(1)\n\nsns.set(font_scale=1.15)\nplt.figure(figsize=(14, 10))\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,annot=True,cmap='Blues',mask=mask,cbar=True)\nplt.title('Correlation Plot')","288984e9":"abs(corr['Survived']).sort_values()[:-1].plot.barh()\nplt.gca().set_facecolor('#FFFFFF')","6603f3c1":"sns.set(font_scale=2)\nplt.figure(figsize=(14, 10))\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nsns.pairplot(titanic_df,kind = 'reg',corner = True,palette ='Blues',hue='Survived' )","fb758370":"tr_idx = titanic_df['Survived'].notnull()\ny_titanic_df = titanic_df[tr_idx]['Survived']\nX_titanic_df= titanic_df[tr_idx].drop('Survived',axis=1)\nX_test_df = titanic_df[~tr_idx].drop('Survived',axis=1)","1b82741b":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val=train_test_split(X_titanic_df, y_titanic_df, \\\n                                                  test_size=0.2, random_state=11)","3dc674f1":"y_train.names='Survived'\ntrain_data = pd.concat([X_train,y_train],axis=1)\ny_val.names='Survived'\nval_data = pd.concat([X_val,y_val],axis=1)","06d57c98":"tr_idx = titanic_df['Survived'].notnull()\ntrain_final = titanic_df[tr_idx]","ae0b39e6":"X_train.shape","106a58a3":"mapper = umap.UMAP().fit(X_train)\numap.plot.points(mapper, labels=y_train, theme='fire')","48d413b4":"all_cols = [cname for cname in X_titanic_df.columns]","48e840d0":"from pycaret.classification import *\nclf1 = setup(data = train_final, \n             target = 'Survived',\n             preprocess = False,\n             numeric_features = all_cols,\n             silent=True)","003f892f":"top5 = compare_models(sort='Accuracy',n_select = 5,\n                      exclude = ['knn', 'svm','ridge','nb','dummy','qda','xgboost']\n                     )","72f67d53":"top5","cd674346":"catboost = create_model('catboost')\nrf = create_model('rf')\nlightgbm = create_model('lightgbm')\n#mlp = create_model('mlp')\ngbc = create_model('gbc')\nlda = create_model('lda')\nlr = create_model('lr')","99bdcd5b":"interpret_model(catboost)","992393e5":"interpret_model(rf)","be4e9420":"interpret_model(lightgbm)","6393e549":"tuned_rf = tune_model(rf, optimize = 'Accuracy',early_stopping = True)","7abb5728":"tuned_lightgbm = tune_model(lightgbm, optimize = 'Accuracy',early_stopping = True)","9b2837b4":"tuned_catboost = tune_model(catboost, optimize = 'Accuracy',early_stopping = True)","177bc334":"tuned_gbc = tune_model(gbc, optimize = 'Accuracy',early_stopping = True)","bfbbf33a":"tuned_lda = tune_model(lda, optimize = 'Accuracy',early_stopping = True)","b889df04":"tuned_lr = tune_model(lr, optimize = 'Accuracy',early_stopping = True)","cb814c37":"# stack top5 models\nstack_model = stack_models(estimator_list = [lr,rf,lightgbm,catboost,gbc,lda], meta_model = top5[0] ,optimize = 'Accuracy')","20bf4c64":"plt.figure(figsize=(10, 10))\nplot_model(stack_model, plot='boundary')","e03e1132":"plt.figure(figsize=(10, 10))\nplot_model(stack_model, plot = 'auc')","2f2d73a2":"plt.figure(figsize=(8, 8))\nplot_model(stack_model, plot='confusion_matrix')","0eb2ec7b":"blend_soft = blend_models(estimator_list = [lr,rf,lightgbm,catboost,gbc,lda], optimize = 'Accuracy',method = 'soft')","5bfa30f0":"plt.figure(figsize=(10, 10))\nplot_model(blend_soft, plot='boundary')","5b7fddaa":"plt.figure(figsize=(10, 10))\nplot_model(blend_soft, plot = 'auc')","af6be093":"plt.figure(figsize=(8, 8))\nplot_model(blend_soft, plot='confusion_matrix')","ad77860d":"blend_hard = blend_models(estimator_list = [lr,rf,lightgbm,catboost,gbc,lda], optimize = 'Accuracy',method = 'hard')","595ba43a":"plt.figure(figsize=(10, 10))\nplot_model(blend_hard, plot='boundary')","3dd91f85":"plt.figure(figsize=(8, 8))\nplot_model(blend_hard, plot='confusion_matrix')","a68753b1":"cali_model = calibrate_model(blend_soft)","734a2083":"final_model = finalize_model(cali_model)","a3f1dfd7":"plt.figure(figsize=(8, 8))\nplot_model(final_model, plot='boundary')","e7cc5c53":"plt.figure(figsize=(8, 8))\nplot_model(final_model, plot='confusion_matrix')","f68dfb2e":"last_prediction = final_model.predict(X_test_df)\nsubmission_data['Survived'] = last_prediction.astype(int)\nsubmission_data.to_csv('submission.csv', index = False)","100703c1":"The first projects most people start with are probably titanic. However, As I think, titatic datasets are not easy.\nThe reason for thinking like this is as follows.\n* Missing Values: There are many missing values of features that are considered important for survivor judgment, such as Age and Cabin.\n* Small Dataset: Because the dataset is small, it seems difficult to train the model sufficiently.\n\nIn this notebook, we will focus on imputation of missing values and ensemble methods to improve performance.\nIn addition, we will create new derivative variables if necessary for each feature.","6edd6c4e":"------------------------------------------------\n## Parch ( Number of parents\/children )","eace339e":"---------------------------------------\n## Embarked","23fd9f32":"**Let's check out outliers in PC 1**","bd9ccb9c":"<span style=\"color:Blue\"> Observation and Decision:\n    \n* There is a large correlation between FamilySize and SibSp and Parch. Since the derived variable FamilySize is made of SibSp and Parch, SibSp and Parch are removed.\n* The relationship between Cabin and Has_Cabin is high. Therefore, the derived variable Has_Cabin is left and Cabin is removed.\n* The relationship between Fare and Fare_class is high. Fare is selected because skewness is removed by nonlinear transform of the Fare feature.\n* There are many features that are not related to the survived value.","1574f235":"## Creating Models\n\n> This function trains and evaluates the performance of a given estimator using cross validation. The output of this function is a score grid with CV scores by fold. \n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","fd169a7c":"--------------------------------------------------------------------------------------------------\n# Detecting Outliers by PCA\n\n![](https:\/\/miro.medium.com\/max\/602\/0*PnqMbZEdnuL9yHuo.png)\n\nPicture Credit: https:\/\/miro.medium.com\n\nThe more features, the higher the dimension. When projecting to a lower dimension through PCA, new insights can be gained. PCA can effectively detect outliers.\n\nPC 1 has the largest variance in the dataset distribution. That is, the outlier in PC 1 is very likely to be real outlier","cac47585":"-------------------------------------------------------------------------\n## Age","416c0d26":"## Checking the final model","5466db1d":"--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n## Has_Age ( Derived variable )\n\n**Question: Does the survival rate make a difference with and without age records?|**","5840a7c8":"Let's check the correlation between the target value (Suvived) and other features.","b796810f":"______________________________________________\n## Alone ( Derived variable )","a0dce64c":"### An Extension To Imputation\n\n> Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing.\n\n![](https:\/\/i.imgur.com\/UWOyg4a.png)\n\n> In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries.\n> \n> In some cases, this will meaningfully improve results. In other cases, it doesn't help at all.\n\nRef: https:\/\/www.kaggle.com\/alexisbcook\/missing-values","3c6eb4e3":"PassengerId has nothing to do with survior. It can be removed immediately.","dfc226cd":"## Soft Blending","d5f39d67":"### Missing Values\n\n> For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and\/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. \n\nReference: https:\/\/scikit-learn.org\/stable\/","8ce76509":"<span style=\"color:Blue\"> Observation:\n* Among the features, if you look at Fare and Age_knn, the features are spread in a wide distribution of importance, and the colors are also spread from blue to red. \n* Each model is learning with the importance of different features. The diversity of these models seems to increase the performance of the ensemble model.\n* Title_Mr and Sex_male play an important role in how the model learns.","4bf9ca1d":"**Question: Does the number of accompanying family members affect the survival rate??**","34032754":"<span style=\"color:Blue\"> Observation:\n    \n* Among the passengers who boarded at S port, the proportion of males is higher than that of other ports.","04d4c4fe":"**OK! Looking at the heatmap and pairplot above, it seems that the features are properly selected.**","7eab7bd0":"### Multivariate feature imputation\n\n> A more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.\n\nRef: https:\/\/scikit-learn.org\/stable\/","0ac113ba":"-------------------------------------------------------------\n# Selecting Features\n\nFeatures that are not helpful in judging the above heatmap and survivors, or that have other derived variables, will be removed.","8e4fdcf6":"### Scaling \nLooking at the above result, the skewness was 0.4559, not skewed to one side. Let's try linear scaling.\n\nSelect RobustScaler during linear scaling. In this way, the influence of outliers can be minimized.\n\n> This is a technique that minimizes the influence of outliers.  \n> Since the median and IQR (interquartile range) are used, it can be confirmed that the same values are more widely distributed after standardization when compared with the StandardScaler.\n> \n> $\ud835\udc3c\ud835\udc44\ud835\udc45=\ud835\udc443\u2212\ud835\udc441$: That is, it deals with values in the 25th and 75th percentiles.\n\nIf you want to know more about Scaling, please refer to the notebook below.\n\n[NotebooK](https:\/\/www.kaggle.com\/ohseokkim\/preprocessing-linear-nonlinear-scaling)","c281bcb4":"## Choosing top models\n\n> This function trains and evaluates the performance of a given estimator using cross validation. The output of this function is a score grid with CV scores by fold. \n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","57405a76":"In machine learning, it is important to determine the boundary. In particular, in tree-based models, it is more important to determine the boundary, because the process of creating a new leaf in the tree is also the process of determining the boundary.\nLooking at the above picture again, there are many overlapping points with the green dot indicating the survior and the blue dot indicating the non-survior. Determining the boundary in this situation would be a very difficult task.\nIf the feature engineer work was done well, the distribution of the two points to determine the boundary would have been well divided. However, the titanic dataset is difficult to do with some missing values \u200b\u200band a small dataset.\n\nLet's look again at the picture above.\nBoundary is not very clean. It can be judged that overfitting has occurred, and therefore it can be judged that it does not have generality.\n\nIf you want to know more about overfitting, please refer to the notebook below.\n\n[Notebook](https:\/\/www.kaggle.com\/ohseokkim\/overfitting-and-underfitting-eda)","f7f6036c":"## Stacking","4532c5c6":"Considering above results, the soft blending model seems appropriate among ensemble models. Therefore, we use this model to make the final prediction with the test dataset.","4a4df37f":"---------------------------------------------------\n# Ensemble\n\n![](https:\/\/media3.giphy.com\/media\/26xBvMWzk7FQr54Sk\/giphy.gif)\n\nPicture Credit: https:\/\/media3.giphy.com\n\n\n> Empirically, ensembles tend to yield better results when there is a significant diversity among the models.Many ensemble methods, therefore, seek to promote diversity among the models they combine.Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees).Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity. It is possible to increase diversity in the training stage of the model using correlation for regression tasks or using information measures such as cross entropy for classification tasks\n\nRef: https:\/\/en.wikipedia.org\/","d8f8d1d5":"# Checking Target Value Imbalace","07281c76":"## Cabin\n\n![](https:\/\/www.retrograph.com\/wp-content\/uploads\/thumbnails\/s5RGL181.jpg)\n\nPicture Credit: https:\/\/www.retrograph.com\n\nLooking at the picture above, it can be hypothesized that the survival rate will be different depending on the location of the cabin.\n\nIf you are the captain or sailor of titanic, you will be able to explain the relationship between cabin and survival rate well. If you have some kind of domain knowledge, you will be able to process that feature well. However, in the absence of such domain knowledge, we must examine the corresponding features in detail and process them to be suitable for machine learning.","686b284d":"<span style=\"color:Blue\"> Observation:\n\nThe mortality rate is higher in the case of Mr. I think it will help with learning.","d57a8edb":"**Let's check the correlation of each feature.**","68c117ff":"------------------------------------------------------\n# Encoding\n\nLet's perform encoding on categorical features.\n\nWhen only tree-based models are used, label encoding is sufficient. However, we will use one-hot encoding for model extension in the future.\n\nIf you want to know more about the encoding of categorical features, please refer to the notebook below.\n\n[Notebook](https:\/\/www.kaggle.com\/ohseokkim\/preprocessing-encoding-categorical-data)","f2797735":"-------------------------------------------------------------------------------------------------\n## Calibrating the final model\n\n> This function calibrates the probability of a given estimator using isotonic or logistic regression. \n","1e270041":"English honistic looks. Let's check some more.","117515b8":"-------------------------------------\n# Checking features","ed4b250a":"It seems that the Boundary is set properly.","c5b1de1c":"<span style=\"color:Blue\"> Observation:\n* In the case of Mr, the number of survivors is small.\n* In the case of Mrs and Miss, there are many survivors.\n\nI think it will be helpful in judging survivors using this.\nHowever, it seems difficult to find the relationship between age and title from the above distributions. Therefore, it seems difficult to use this to fill in the missing values of age.","2f26cdb3":"### Nearest neighbors imputation\n\n> The KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors. Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed. When the number of available neighbors is less than n_neighbors and there are no defined distances to the training set, the training set average for that feature is used during imputation. If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation. If a feature is always missing in training, it is removed during transform. \n\nRef: https:\/\/scikit-learn.org\/stable\/","02af75f1":"---------------------------------------\n# Checking Last Results","ce4916a2":"## Checking and Handling Missing Values","486d1c1c":"# Tuning Hyperparameters\n\n> This function tunes the hyperparameters of a given estimator. The output of this function is a score grid with CV scores by fold of the best selected model based on optimize parameter. \n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","7b75ebfe":"Looking at the skewness, it is skewed to one side.","70195e79":"<span style=\"color:Blue\"> Observation:\n\n* Cases with cabins have more survivors compared to cases without cabins. It is likely that the new derived variable will be helpful in the classification of survivors.","3928ad63":"As shown in the figure above, when viewed in two dimensions, there are quite a few areas where the survivors and the dead overlap. First, let's feel for a moment that the task of classification is a difficult task before learning the model. Our model does this difficult job!\n\n**Thanks models!**","9a5d779b":"--------------------------\n# Introduction\n\n![](https:\/\/c.tenor.com\/csSu8i3jaRQAAAAC\/titanic-sinking.gif)\n\nPicture Credit: https:\/\/c.tenor.com\n\n","cb84dbdf":"The training dataset has 16 dimensions. To show the approximate distribution of the training dataset preprocessed above, let's reduce the dimension to two dimensions and draw it.","fcf1e750":"# Utility Function","11792f72":"---------------------------------------------\n## Cabin_Label ( Derived variable )","0203fc34":"The first letter is an uppercase letter of the alphabet. Let's analyze it a bit more using this.","20973ac7":"## Hard Blending","cca874d2":"Let's try Bining.","cb967788":"**OK! There is no missing values at the train dataset.**","afea85da":"Compared to the soft blending model, the boundary does not look clean.","4ff845c3":"-------------------------------------------------\n## FamilySize ( Derived variable )","a5daffb6":"<span style=\"color:Blue\"> Observation:\n\nThose who were alone died more than those who were not alone. The derived feature seems to be helpful for model training.","a8840668":"Although the survivor is small, the imbalance is not large enough for over\/under sampling.\nIf you want to know more about over\/under sampling, please refer to the notebook below.\n\n[Over\/Under sampling](https:\/\/www.kaggle.com\/ohseokkim\/preprocessing-resolving-imbalance-by-sampling)","5d151211":"Let's check the distribution first to fill in the appropriate values.","36e23754":"<span style=\"color:Blue\"> Observation:\n    \nMissing values are 1014. This is used to determine whether the cabin is owned or not. To do this, create a new derived variable.","4824e946":"-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n## Fare\n\nFirst, let's check the distribution for that value.","03757bd3":"### Filling using other features\n\nAnother conceivable strategy is to use other features to fill in the missing values \u200b\u200bof the Age feature. Consider how to fill the Age feature using the Title feature above.","ffc10ebc":"## Setting up models\n\n> This function trains and evaluates performance of all estimators available in the model library using cross validation. The output of this function is a score grid with average cross validated scores.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","f3c54dbd":"--------------------------------------------\n# EDA","17821c2a":"<span style=\"color:Blue\"> Observation:\n    \n* More than the case where Age is not missing.\n* Cases in which age is not missed have a higher survival rate than cases in which age is omitted.","ff350505":"------------------------------------------------------------------------\n## Has_Cabin ( Derived variable )\n\n**Question: Is there a difference in the survival rate between passengers with and without cabin?**","1dee653e":"------------------------------------------------------------------------------------------\n# Checking Missing Value Again\nFinally, let's check the missing values.","1eb34794":"---------------------------------------------------------------\n# Setting up","0d02ecc2":"---------------------------------------\n# Checking Correlation","65601107":"----------------------------------------------------------------------\n## SibSp ( Number of siblings\/spouses )","e13a9b3f":"----------------------------------------------------------------------------\n# Visualizing Training Dataset after Dimension Reduction","0795c1c8":"**Let imputation be done with KNN. The remaining Age-related features are removed.**","a1c53827":"### Univariate feature imputation\n\n> The SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.\n\nRef: https:\/\/scikit-learn.org\/stable\/","cf138c2b":"## Checking Data Type","a99fa6cb":"**It is skewed to one side. Consider nonlinear scaling. In this case, we will use QuantileTransformer.**\n\n> The quantile function ranks or smooths out the relationship between observations and can be mapped onto other distributions, such as the uniform or normal distribution.\n\nIf you want to know more about Scaling, please refer to the notebook below.\n\n[NotebooK](https:\/\/www.kaggle.com\/ohseokkim\/preprocessing-linear-nonlinear-scaling)","f934df83":"<span style=\"color:Blue\"> Observation:\n    \nThere are missing values for Age, Cabin, Fare, and Embarked features. In particular, there are many missing values for Age and Cabin features. Let's think about how to handle these missing values.","e6e84b09":"# Loading Dataset","82159491":"# Interpreting Models\n\nThis function analyzes the predictions generated from a trained model. Most plots in this function are implemented based on the SHAP (SHapley Additive exPlanations).\n\n> SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions\n\nRef: https:\/\/shap.readthedocs.io\/en\/latest\/\n\n**If you want to know more about feature importance and SHAP, please refer to the notebook below.**\n\n[Notebook](https:\/\/www.kaggle.com\/ohseokkim\/explaning-machine-by-feature-importnace)","c4643275":"<span style=\"color:Blue\"> Observation:\n* The Sage family started from S port, there was no Cabin, and the ages were not recorded, and they appear to be a poor and pitiful family with a pclass 3 rating.\n* All three females in this family using the Miss title have died.\n\n**The last sad news is that the training dataset is small, so it seems difficult to remove even if the above data are outliers.    \nNevertheless, fortunately, this problem is not a regression problem, but a classification problem. If it is a regression problem, outliers should be removed.**","e3d8385a":"Let's impute missing value for Embarked feature. The strategy for Embarked's missing values is to choose 'most_frequent'.","88dd8215":"________________________________________________________\n## Name\n\nIt seems difficult to find the feature directly related to the survivor.","0ae97424":"--------------------------------------------------\n# Finalizing the last model\n> This function trains a given estimator on the entire dataset including the holdout set.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html\n\nThe blend_soft model is selected based on the above result. Finally, the model is tuned with the entire dataset.\n","c8dd3f1f":"## Imputting Missing Values","a89bff35":"<span style=\"color:Blue\"> Observation:\n    \n* Many passengers on board at S port died.\n* For passengers boarding at port C, the survival rate is higher than the mortality rate.","4c569818":"<span style=\"color:Blue\"> Observation:\n\nWhen FamilySize is 1, the survival rate is significantly lower than in other cases. I think it will be helpful when the model is learning.","d0f486a0":"# Spliting Train\/Validation\/Test Data"}}