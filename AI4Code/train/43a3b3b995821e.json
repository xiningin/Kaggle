{"cell_type":{"16b5bc04":"code","4d5b62d1":"code","3f7ce57b":"code","7f2ca9e9":"code","3789c8da":"code","e360a7e6":"code","a6e23f0e":"code","a45ec861":"code","97976380":"code","18dbe6a1":"code","9b00b5d2":"code","43928aa0":"code","64c1eb05":"code","05e9ab9b":"code","f57b14cd":"code","d4b6f1df":"code","f1ee11f5":"code","d43e9624":"code","94269a96":"code","3f0a05fe":"code","77074e9d":"code","bb0673b4":"code","94b7450e":"code","1292e469":"code","61d8ce07":"code","1bd0783d":"code","392a3f2b":"code","db38eefa":"markdown","3536fb79":"markdown","346a8508":"markdown","1134b5ce":"markdown","ce534cc6":"markdown","0b12a86c":"markdown","df8da4a1":"markdown","286d9020":"markdown","f5beebac":"markdown","47d34bc6":"markdown","12dde445":"markdown","5ad30ce1":"markdown","9799a282":"markdown","51674f6b":"markdown","f72fc951":"markdown"},"source":{"16b5bc04":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nimport tensorflow as tf\n\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau","4d5b62d1":"train = pd.read_csv(\"\/kaggle\/input\/sign-language-mnist\/sign_mnist_train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/sign-language-mnist\/sign_mnist_test.csv\")","3f7ce57b":"train.head()","7f2ca9e9":"print(\"Shape of train set: \", train.shape, '\\n'\n      \"Shape of test set: \", test.shape)","3789c8da":"print(\"Corrupted pixels and labels in train set: \", train.isna().sum().sum(), '\\n'\n        \"Corrupted pixels and labels in test set: \", test.isna().sum().sum())","e360a7e6":"plt.figure(figsize = (6, 6))\nsns.countplot(x = train['label'])","a6e23f0e":"plt.figure(figsize = (6, 6))\nsns.countplot(x = test['label'])","a45ec861":"dataset = train.append(test)","97976380":"X = dataset.drop(['label'], axis = 1)\ny = dataset['label']\n\nXTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size = 0.3, train_size = 0.7, random_state = 21, shuffle = True)\ny = yTest.copy()","18dbe6a1":"plt.figure(figsize = (6, 6))\nsns.countplot(x = yTrain)","9b00b5d2":"plt.figure(figsize = (6, 6))\nsns.countplot(x = yTest)","43928aa0":"numClasses = len(yTrain.unique()) + 1 # Dataset is missing class 9","64c1eb05":"yTrain = to_categorical(yTrain, numClasses)\nyTest = to_categorical(yTest, numClasses)","05e9ab9b":"XTrain = XTrain \/ 255.0\nXTest = XTest \/ 255.0","f57b14cd":"XTrain = np.array(XTrain).reshape(-1, 28, 28, 1)\nXTest = np.array(XTest).reshape(-1, 28, 28, 1)","d4b6f1df":"fig = plt.figure(figsize = (8, 8))\nrows, columns = 4, 4\n\nfor i in range(1, (rows * columns) + 1):\n    imageIndex = np.random.randint(0, XTrain.shape[0])\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(XTrain[imageIndex], cmap = 'gray')\n    \nplt.show()\n","f1ee11f5":"dataGenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=14,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.09, # Randomly zoom image \n        width_shift_range=0.14,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.14,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False,   # randomly flip images\n        brightness_range = (0.8, 1.0),  # brightness of image\n        rescale = 1 \/ 255.0)  # Normalization output\n                                \ndataGenerator.fit(XTrain)","d43e9624":"class CustomEarlyStopping(tf.keras.callbacks.Callback):\n    \n    def __init__(self, threshold):\n        self.threshold = threshold\n        \n    def on_epoch_end(self, epoch, logs = None): \n        if(logs.get('val_accuracy') >= self.threshold):\n            print(\"Reached\", self.threshold, \"% accuracy. Stopping learning!\")\n            self.model.stop_training = True\n            \nearlyStopping = CustomEarlyStopping(0.997)","94269a96":"learningRateReduction = ReduceLROnPlateau(monitor='loss', patience = 1, verbose=1,factor=0.75, min_lr=0.00000000001)","3f0a05fe":"model = Sequential()\n\nmodel.add(Conv2D(64, (3,3), strides = 1, padding = 'same', activation = 'relu', input_shape = (28,28,1)))\nmodel.add(Dropout(0.05))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2), strides = 2, padding = 'valid'))\n\nmodel.add(Conv2D(32, (3,3), strides = 1, padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.15))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2), strides = 2, padding = 'valid'))\n\nmodel.add(Conv2D(16, (3,3), strides = 1, padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.05))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2), strides = 1, padding = 'valid'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units = 64, activation = 'relu'))\nmodel.add(Dropout(0.05))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(units = 128, activation = 'relu'))\nmodel.add(Dropout(0.05))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(units = 64, activation = 'relu'))\nmodel.add(Dropout(0.05))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(units = numClasses, activation = 'softmax'))\n\nmodel.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.005) , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()","77074e9d":"history = model.fit(dataGenerator.flow(XTrain, yTrain, batch_size = 16) ,epochs = 80, \n                    validation_data = (XTest, yTest), callbacks = [learningRateReduction])","bb0673b4":"plt.figure(figsize = (8, 8))\nplt.plot(history.history[\"accuracy\"], color = 'red', linewidth = 3)\nplt.plot(history.history[\"val_accuracy\"], color = 'green', linewidth = 3)\nplt.title(\"Model accuracy\", fontsize = 22)\nplt.ylabel(\"accuracy\", fontsize = 18)\nplt.xlabel(\"Epoch\", fontsize = 18)\nplt.legend([\"Train accuracy\", \"Test accuracy\"], loc = 'lower right')","94b7450e":"plt.figure(figsize = (8, 8))\nplt.plot(history.history[\"loss\"], color = 'red', linewidth = 3)\nplt.plot(history.history[\"val_loss\"], color = 'green', linewidth = 3)\nplt.title(\"Model loss\", fontsize = 22)\nplt.ylabel(\"loss\", fontsize = 18)\nplt.xlabel(\"Epoch\", fontsize = 18)\nplt.legend([\"Train loss\", \"Test loss\"], loc = 'upper right')","1292e469":"predictions = model.predict(XTest)\npredictions = np.argmax(predictions, axis = 1)\n\nconfusionMatrix = confusion_matrix(y, predictions)\n\nplt.figure(figsize = (12, 9))\nsns.heatmap(confusionMatrix, cmap= \"Blues\", linecolor = 'black', linewidth = 1, annot = True, fmt='')","61d8ce07":"badlyClassified = np.where(predictions != y)","1bd0783d":"badlyClassified = np.array(badlyClassified).tolist()\nbadlyClassified = [item for sublist in badlyClassified for item in sublist]","392a3f2b":"fig = plt.figure(figsize = (21, 21))\nrows, columns = 1, 7\n\nfor i in range(0, len(badlyClassified)):\n    fig.add_subplot(rows, columns,  i + 1)\n    plt.axis('off')\n    plt.title(\"Predicted class: {} \\n Actual class: {}\".format(predictions[badlyClassified[i]], np.argmax(yTest[badlyClassified[i]], axis = 0)))\n    plt.imshow(XTest[badlyClassified[i]], cmap = 'gray')\n    \nplt.show()","db38eefa":"# Data normalization","3536fb79":"# Analysis","346a8508":"# Split dataset to XTrain, yTrain, XTest, yTest and y which we will use for confusion matrix","1134b5ce":"### Before we start making the model, let's think about what we want to achieve. We need to find a golden mean between accuracy, model speed (total number of multiplications) and learning speed.\n### In the model below, I care only about the accuracy and speed of the model.","ce534cc6":"# Training the model","0b12a86c":"# Lets plot some training examples","df8da4a1":"### Early stopping","286d9020":"### Data augmentation","f5beebac":"# Convert yTrain and yTest to one hot representation for softmax","47d34bc6":"# Importing librays","12dde445":"# Data preprocessing","5ad30ce1":"### Reduce learning rate","9799a282":"# Loading dataset","51674f6b":"# Check for corrupted data","f72fc951":"# Convert XTrain and XTest to (numberOfTrainingExamples x imageShape) where image shape is (28 x 28 x 1). 1 becouse our images are in grayscale"}}