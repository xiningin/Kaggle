{"cell_type":{"ffad39e8":"code","d7ffe067":"code","66d08d98":"code","efd32cb2":"code","6432b53c":"code","b90fc48d":"code","a234eccc":"code","44a3af7e":"code","d422d808":"code","74158f6d":"code","b29f2159":"code","f4fb3c64":"code","b1e40e7b":"code","c0794870":"code","d7103f06":"code","2704ac25":"code","9971ece6":"code","0387ee85":"code","57eba3bb":"code","dc090150":"code","221cb20d":"code","b9e6af1a":"code","01c1cc06":"code","329bb04f":"code","e90e72a6":"code","44be640c":"code","9d312eb6":"code","1542f1e2":"code","3803359e":"code","525fa76a":"code","1a0f764e":"code","793e8a0d":"code","413857f1":"code","04120dce":"code","6a60d290":"code","b594d3f1":"code","23468959":"code","f66bc79a":"code","a62f0767":"code","d4139d22":"markdown","4e8650d6":"markdown","6c311baf":"markdown","6d23de23":"markdown","924186c6":"markdown","c4408f51":"markdown","3cd92f4b":"markdown","2ee166f8":"markdown","3dc45dc7":"markdown","1bfd364c":"markdown","0ae0e3d9":"markdown","c7484c12":"markdown","56936eea":"markdown","5b185234":"markdown","567bc0ca":"markdown","f8b0361c":"markdown","bd70e3bc":"markdown","bf509e05":"markdown","dd6e13b4":"markdown","6d7f6aa6":"markdown","b76ea823":"markdown","3748c08f":"markdown","63d578ca":"markdown","9084b7ae":"markdown","128c52a8":"markdown","420805eb":"markdown","b2cc57c9":"markdown","49404a32":"markdown","8198ae07":"markdown","29c806dd":"markdown","3cb12a11":"markdown","693ef8c0":"markdown","f729ad30":"markdown","4f18a1a0":"markdown","8572e607":"markdown","538d42ff":"markdown","14c57cb6":"markdown","9c3f43fc":"markdown","6e080266":"markdown","50a1f294":"markdown","29ec80fe":"markdown","405766f6":"markdown","227c1122":"markdown","21c68a9d":"markdown","99f3c46b":"markdown","d94668ea":"markdown","37777640":"markdown","c785c779":"markdown","71dab5ba":"markdown","f449c697":"markdown","3d5026d7":"markdown","a10cff1b":"markdown","20fd2749":"markdown","b33c797b":"markdown","b2a2a8e3":"markdown","19686ac2":"markdown"},"source":{"ffad39e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7ffe067":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","66d08d98":"print('This is the shape of training data: ' + str(train.shape))\nprint('This is the shape of testing data: ' + str(test.shape))","efd32cb2":"print('These are the columns in training data: ' + str(train.columns))","6432b53c":"X_train, y_train = train.drop('label', axis=1), train['label']","b90fc48d":"random_digit = X_train.iloc[23] # selecting a random index\nrandom_label = y_train.iloc[23] # taking label of the same digit\n\n# As discussed earlier, the X_train has 784 columns and each has a pixel density value in it. \n# We will need to transform it into a 24 x 24 image to plot it.\nimport numpy as np\nrandom_digit = random_digit.values.reshape(28, 28)\n\n# Let's plot it now\nprint('This is the plot:')\n\nimport matplotlib.pyplot as plt\nplt.imshow(random_digit, cmap='binary')\nplt.show()\nprint('This is the label: ' + str(random_label))","a234eccc":"\n# Before we do anything with the data, we should have a validation set. To see how well we are doing.\nfrom sklearn.model_selection import train_test_split\n\nX_training, X_validation, y_training, y_validation = train_test_split(X_train, y_train, test_size = 0.2, random_state = 43)","44a3af7e":"# For this purpose we will need to transform our target values a little bit.\n# We need the target value to be either True or False. (It's 9 \/ It's not 9)\n\n# The following lines will transform the target values from both testing and training set into True\/False\ny_is9_training = (y_training == 9)\ny_is9_validation = (y_validation == 9)\n\nprint(len(y_is9_training))\nprint(len(y_is9_validation))","d422d808":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=20)\n\n# Training the model\nsgd_clf.fit(X_training, y_is9_training)","74158f6d":"prediction = sgd_clf.predict(X_validation)","b29f2159":"print(\"Correctly predicted: \" + str(sum(prediction == y_is9_validation)))\nprint(\"Total entries: \" + str(len(y_is9_validation)))\nprint(\"Accuracy: \" + str((sum(prediction == y_is9_validation) \/ len(y_is9_validation))*100) + \" %\")","f4fb3c64":"from sklearn.model_selection import cross_val_score\n\ncross_val_score(sgd_clf, X_validation, y_is9_validation, cv=3, scoring='accuracy')","b1e40e7b":"from sklearn.metrics import confusion_matrix\n\n# Takes actual values and the predicted values.\nconfusion_matrix(y_is9_validation, prediction)","c0794870":"# Scikit learn porvies several functions to compute classifier metrics, including precision and recall.\n\nfrom sklearn.metrics import precision_score, recall_score\n\nprecision_score = precision_score(y_is9_validation, prediction)\nrecall_score = recall_score(y_is9_validation, prediction)","d7103f06":"print(\"This is the precision score: \" + str(precision_score))\nprint(\"This is the recall score: \" + str(recall_score))","2704ac25":"# To compute the F1 score, simply call the f1_score() function provided by sklearn.\nfrom sklearn.metrics import f1_score\n\nf1_score(y_is9_validation, prediction)","9971ece6":"from sklearn.model_selection import cross_val_predict\n\ny_scores = cross_val_predict(sgd_clf, X_validation, y_is9_validation, cv=3, method='decision_function') # returns decision score for each instance in X_validation","0387ee85":"y_scores","57eba3bb":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_is9_validation, y_scores)","dc090150":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n    plt.legend(loc=\"center right\", fontsize=16)\n    plt.xlabel(\"Threshold\", fontsize=16)\n    plt.grid(True) \n\nplt.figure(figsize=(10, 4))                                                                  # Not shown\n\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","221cb20d":"def plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.axis([0, 1, 0, 1])\n    plt.grid(True)\n\nplt.figure(figsize=(10, 4))      \nplot_precision_vs_recall(precisions, recalls)\nplt.show()","b9e6af1a":"X_validation.shape","01c1cc06":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_is9_validation, y_scores)","329bb04f":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16)\n    plt.ylabel('True Positive Rate (Recall)', fontsize=16)\n    plt.grid(True)\n\nplt.figure(figsize=(10, 4))\nplot_roc_curve(fpr, tpr)\nplt.plot([4.837e-3, 4.837e-3], [0., 0.4368], \"r:\")\nplt.plot([0.0, 4.837e-3], [0.4368, 0.4368], \"r:\")\nplt.plot([4.837e-3], [0.4368], \"ro\")\nplt.show()","e90e72a6":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_is9_validation, y_scores)","44be640c":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(random_state = 43)\ny_probas_forest = cross_val_predict(rf_clf, X_validation, y_is9_validation, cv = 3, method = \"predict_proba\") # The random forest classifier does not have the decision_function, it has the predict_proba which works in the similar way. You can find all this information by going through the documentation.","9d312eb6":"y_probas_forest #gives you the score of -ve and +ve classes respectively.","1542f1e2":"y_scores_forest = y_probas_forest[:, 1] # you want scores only for the positive class.\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_is9_validation, y_scores_forest)","3803359e":"plt.figure(figsize=(10, 4))  \nplt.plot(fpr, tpr, 'b:', label = \"sgd\")\nplot_roc_curve(fpr_forest, tpr_forest, \"RF\")\nplt.legend(loc = \"center\")\nplt.show()","525fa76a":"# Finding out the AUC in the case of random forest.\nroc_auc_score(y_is9_validation, y_scores_forest)","1a0f764e":"from sklearn.svm import SVC\n\nsvm_clf = SVC()\nsvm_clf.fit(X_train, y_train)","793e8a0d":"random_digit = X_validation.iloc[3]\nprediction = svm_clf.predict([random_digit])\nlabel = y_validation.iloc[3]\n\nprint('This is the prediction: ' + str(prediction))\nprint('This is the label: ' + str(label))","413857f1":"# Let's run a SGD classifer(which is a multi class classifier) and see what the decision_function returns.\n\nsgd_clf.fit(X_train, y_train)\nprediction = sgd_clf.predict([random_digit])\nlabel = y_validation.iloc[3]","04120dce":"print('This is the prediction: ' + str(prediction))\nprint('This is the label: ' + str(label))","6a60d290":"sgd_clf.decision_function([random_digit])","b594d3f1":"cross_val_score(sgd_clf, X_train, y_train, cv = 3, scoring = 'accuracy')","23468959":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float64))","f66bc79a":"cross_val_score(sgd_clf, X_train_scaled, y_train, cv = 3, scoring = 'accuracy')","a62f0767":"# The score looks much better now, let's do some error analysis\n\ny_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)","d4139d22":"##### This score tells us that when the model claims that the digit it 9, it is correct 98%. Moreover, it detects 9's only 82% times.","4e8650d6":"##### Just by looking at the plot, we can tell that the random forest performs better than the stochastic gradient descent.","6c311baf":"## The moment of truth!","6d23de23":"## Let's plot and see one digit","924186c6":"# Understanding the dataset","c4408f51":"### Take this scenario:\n#### Your friend has 100 candies of colors red and green in a bag and you can't see them. He asks you to guess the number of pink coloured candies in the bag (He knows there are none).\n#### You make a guess, let's say, 30% of the candies are pink.\n#### He opens the bag, and guess what? You were 70% correct.\n\n### What is the takeaway here? \n## Never take accuracy as a performance measure for classifiers","3cd92f4b":"#### The score tells you that random forest is indeed a better model for this binary classifier.","2ee166f8":"# How does a Binary Classifier work?","3dc45dc7":"#### Each row in a confusion matrix represents an actual class, while each column represents a predicted class. \n\n#### By just looking at the table, you can tell that 467 times your model did the right prediction at figuring out that the digit was actually 9. And at the same time, 354 times did the right prediction that the digit was not 9.\n\n#### In a confusion matrix, the diagnal cells(I have made them green) always tells you how right the model was. Whereas all the other cells tells you how wrong the model was.\n\n#### Here we were finding just yes or no (It's 9 and It's not 9) hence there are only two columns and two rows. The number of columns and rows are exactly equal to the number of possible predictions.","1bfd364c":"### Every classifier makes it's decision whether to classify an instance as true or false based on a threshold value and a decision score. If the decision score is higher than the threshold value then it classifies the class as True and vice versa.\n\n### You can find out the decision score for each instance using the decision_function() provided by sklearn.\n","0ae0e3d9":"##### There are a total of 42,000 training images and 28,000 testing images.","c7484c12":"# Getting the data","56936eea":"##### Now we will write a model that can predict if the digit is 9 or not.\n##### I chose RandomForestClassifer for this task (No reason honestly, get something else if you feel like). \n","5b185234":"#### You can easily tell the performance of any classifier by looking at a confusion matrix table. It just tells you how many times the model did the right thing and how many times it did not.\n#### Look at the following table for instance:\n\n![Confusion matrix](https:\/\/i.imgur.com\/qR9rDRo.png)\n##### Not 9 - Negative\n##### Is 9 - Positive\n\n\n","567bc0ca":"## The ROC Curve (Reciever Operating Characteristic curve) helps you decide which classifier performs better on your data.\n### The ROC curve plots the TPR or the true positive rate(recall) against the FPR or the false positive rate.\n### As we read, true positive rate (recall) is how many true positives could we find from all the actual positives. Similarly false positive rate is just the opposite, How many negative instances were incorrectly classified as positives.\n\n### To conclude, ROC plots TPR against FPR.\n#### And, \n![](https:\/\/i.imgur.com\/jd6gKuA.png)\n### Hence we can say ROC plots TPR against 1 - TNR.\n### As we read, TPR is basically recall and it's also called sensitivity. At the same time TNR is called specificity. Hence we can say:\n## ROC curve plots sensitivity against (1 - specificity)","f8b0361c":"# Multiclass Classification","bd70e3bc":"## Now let's train a RandomForestClassifier and compare it's ROC Curve and ROC AUC score to those of the SGDClassifier","bf509e05":"### 2. Recall\n#### Also knows as sesitivity or the true positive rate (TPR), This is the ratio of positive instance that are correctly detected by the classifier.\n![](https:\/\/i.imgur.com\/fYNaiIl.png)\n\n##### Out of the total positives (not the predicted, but the actual), how many were identified as positive.","dd6e13b4":"### How to find out the confusion matrix values for our prediction?\n\n#### Scikit learn provides a function that takes in your model as well as validation sets for train and test and gives you the confusion matrix values. Have a look:","6d7f6aa6":"# Writing a model to predict a single digit - Starting with binary classifier","b76ea823":"### The best and renouned way of finding the performance of a classifier is having a confusion matrix","3748c08f":"##### It's too good to be true. Well, yeah it's a hoax. Sorry to break the bubble.","63d578ca":"## Precision and Recall\n#### The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. This is where precision and recall come in the picture.","9084b7ae":"## Confusion Matrix","128c52a8":"#### When dealing with multi class classifiers, the decision_function returns one value per class.","420805eb":"### Important Note: You can't have high values for both precision and recall. There's this concept of precision\/recall trade-off which means that if precision goes up then recall goes down and vice versa. You have to decide which one to choose and it totally depends on the type of problem that you are working on. In the case where you do not want to have any False positives choose high precision and in the case where you do not want to have any False Negatives choose high recall.","b2cc57c9":"## In-progress","49404a32":"#### The goal is to write a model that can identify a single digit. Let it be digit 9.","8198ae07":"# Cross-validation of the single digit classifier","29c806dd":"##### From this plot we can see that as the value of threshold increases, the value of recall decreases and the value of precision increases.","3cb12a11":"## New to classification problems?\n### Let's crack it together, Here we try to understand a basic workflow of how to tackle such problems and things that you need to keep in mind in such scenarios. It's the second step after regression problems, you learn regression and then figure out not every prediction is a number, It can be a yes or a no! \n### If you are in that situation, here's the place which might give you some insight.\n### I hope someone who is starting out finds it useful.\n### We will be talking about the MNIST dataset,which contains many different pictures of handwritten digits. It's a great dataset for people starting out and is considered as the \"Hello World\" of classification problems. It's pretty intuitive too, it widened horizons for me, how easy things can be even when they look like rocket science.\n### About the dataset: \n#### There are a total of 784 columns where each represents a single pixel of the image (It's a 28 x 28 pixel image, so a total of 784 pixels), each column simply represents one pixel's density ranging from 0 to 255 (white to black).\n#### And obviously, each row represents a single picture.\n\n##### Side note: This notebook takes reference from the great book called \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" in some places, it's a highly recommended book. Just give it a read if you haven't yet.","693ef8c0":"### Now that we have values of precision, recall and threshold plotted against each other, the question that you have to ask yourself is how much precision and recall do you want? Do you want the classifier to find out maximum number of actual positives then go for high recall, if you want more true positives (If you are fine with leaving some actual positives but want all that you find to be actually true) then go for higher precision.\n\n### Suppose you want a classifier that has 90% precision, then you can look at the first plot and figure out the value of threshold and return the predicions.","f729ad30":"### Let's try to plot Precision against Recall","4f18a1a0":"#### 93% correct? Really? Let's do it a couple of more times and get the average.","8572e607":"##### The matrix tells us the 7525 times our model was able to tell that the digit was 9, 708 times it could tell the it was not 9.\n\n##### True Negative = 7525, False Positive = 14, False Positive = 153, True Positive = 708\n\n#### A perfect classifier would have nonzero values only on its main diagonal (top left to bottom right)","538d42ff":"#### Well, scaling the training data might improve the score.","14c57cb6":"##### Note that when you train a multiclass classifier and not a binary classifier then scikit does not have to run OvR or OvO because of obvious reasons. They a Multi class classifiers and can handle multiple classes.","9c3f43fc":"##### These were all the techniques that you can use to select a binary classifier and to tune it. We saw how to train a binary classifier, choose an appropriate metric for our task, evaluate the classifiers using cross-validation, select the precision\/recall trade off that fits our model, and then finally use ROC curves and ROC AUC scores to compare various models.","6e080266":"##### Let's see how we did!","50a1f294":"# Why is Cross-validation different for classification problems?","29ec80fe":"#### Let's have a cross val score for this sgd classifer","405766f6":"# What are good ways of measuring performance of classification models?","227c1122":"# The ROC Curve","21c68a9d":"##### Now that we are done with theory, let's see how can we put this to use using scikitlearn.","99f3c46b":"##### The first column is the label which tells us which digit is it, and rest 784 columns represents each pixel of the image.","d94668ea":"## What to do with these decision scores and how to find out the threshold value?\n\n### With these scores, you can use the precision_recall_curve() function to compute precision and recall for all possible thresholds.","37777640":"### Okay, so this is all you need to do to tune a classifier. But how do you decide which classifier to use in the first place?","c785c779":"##### We can see that the SGD classifier did not do very good job at prediction, but by looking at the decision score we can figure out how things are working under the hood. There is a list of decision scores for each value and the highest score wins that the model spits out that value.","71dab5ba":"### 1. Precision\n#### Precision is nothing but the accuracy of the positive predictions.\n##### Out of the total positive predictions, how many were correct.\n![](https:\/\/i.imgur.com\/YO5pjnF.png)\n","f449c697":"## Okay, we have the images(pixels) and the labels. What to do next?\n### One would say let's grab a model from sklearn train it on this data. I thought the same, But that's not how it is done.\n### Let's start slow and make a smaller model that can predict a single digit. (It will tell that - Yes, it is the digit or no, it's not the one)\n### These are called binary classifiers, basically a classifier that can answer either true or false.\n","3d5026d7":"### Take this example to understand precision and recall in a better way:\n#### Suppose we are identifying dogs and cats from a set of 100 images. Imagine there are 75 dogs and 25 cats. We predict that there are 50 dogs and out of those only 43 turn out to be actually dogs.\n#### The precision here will be how many dogs could we predict correctly (43) divided by the number of predictions where we thought it's a dog (50).\n#### The recall here will be how many dogs could we predict correctly (43) divided by actual dogs in the dataset (75).","a10cff1b":"### It is often convenient to combine precision and recall into a single metric called the F1 score, in particular if you need a simple way to compare two classifiers.\n### The F1 score is the harmonic mean of precision and recall.\n\n![](https:\/\/i.imgur.com\/LvmL6wi.png)","20fd2749":"### Using Binary Classifier for Multiclass Classification\n#### We studied how does a binary classifier work. For multiclass classifier, one approach can be to make 10 binary classifiers for each digit and then to classify an image, you get the decision score from each classifier for that image and you select the class whose classifier outputs the highest score.\n##### This is called the One vs rest strategy (OvR)\n#### Another strategy is to train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distingush 0s and 2s and so on. If there are N classes then you need to train N x (N - 1) \/ 2 classifiers. When you want to classify an image, you have to run the image through all the classifiers and see which class wins the most duels.\n##### This is called the one-vs-one strategy (OvO).\n\n### But, like most of the times, you don't have to worry about the technicalities and just use sklearn. Some classifiers are strictly binary classifiers (like SVM, Logistic Regression) and when sklearn detects that you are trying to use a binary classifier for a multi class classification, it automatically runs the OvR or OvO, depending on the algorithm.","b33c797b":"##### From this plot we can notice that the value of Precision starts to gradually decrease after around 80% recall. You have to choose the value of threshold before this point, 70% would be a good place.","b2a2a8e3":"## How to compare two classifiers by looking at the ROC curve?\n### By measuring the AUC (area under the curve). A perfect classifer will have a ROC AUC equal to 1, whereas a purely random classifier will have ROC AUC equal to 0.5.\n### Scikit-learn provides a function to compute the ROC AUC.","19686ac2":"## Split the training features and the target features for the training."}}