{"cell_type":{"90772d16":"code","8750fa6e":"code","d8d4038b":"code","66664ebd":"code","1cc49d33":"code","d5d1afb5":"code","9fc3a93d":"code","a0e4b11d":"code","97db2532":"code","900ccf60":"code","7fcf4a24":"code","157d2de3":"code","09a1c332":"code","0303ae8f":"code","7968ca4f":"code","2911fb17":"code","a61c17cb":"code","6afe3264":"code","91dc3ea3":"code","fef25c2e":"code","3df90921":"code","d410984c":"code","a6541971":"code","b56ade64":"code","769f55d8":"code","4cc18860":"code","19c97280":"code","ec361739":"code","397a4df7":"code","562ca47c":"code","42a51276":"code","227e3565":"code","5124a69d":"code","ae8f7e66":"code","b1561dfe":"code","08d1b132":"code","eecdf103":"code","f718e413":"code","92ef8644":"code","97f10c3a":"code","3e279448":"code","123e239c":"code","293585ae":"code","d1f17f1f":"code","51f9463d":"code","31cb484f":"code","7ab82b9d":"code","70afca1e":"code","727fc933":"code","b62fe2c2":"code","e9fe8efb":"code","63c0d077":"code","f7e1ca16":"code","5943395f":"code","e280418a":"code","f5e36b83":"code","08217a4c":"code","f68eed3a":"code","ec86ab60":"code","cf9f57a2":"code","b3da111f":"code","94e14ccf":"code","a0dc5150":"code","acc10977":"code","0765f1a6":"code","3e2df0c2":"code","8a70819e":"code","94a912fe":"code","d70daa47":"code","4d1976c6":"code","1d4e976e":"code","d171156e":"code","17f616d4":"code","e7a856bf":"code","f432763d":"code","369f64e8":"code","e73cb6e5":"code","bdbe0ce2":"code","d60b66d7":"code","91d3def2":"code","9640104b":"code","a10ad4b9":"code","df0c044c":"code","7b03b3eb":"code","740b75f2":"code","0ebe3e30":"code","994ab724":"code","e2af5eee":"code","524b63c7":"code","9e067db7":"code","982dcc98":"code","860d7aac":"code","715dd8e4":"code","341e71f6":"code","e23927b4":"code","66da7473":"code","699f0dfe":"code","27822617":"code","ae7df589":"code","dc33aac3":"code","3abdab3f":"code","8ac3d4a1":"code","f397993e":"code","71345145":"code","c4ba0fd6":"code","89721949":"code","6e447924":"code","905477b5":"code","d7b8a862":"code","2491594a":"code","b4a564f3":"code","6f388928":"code","163036c1":"code","fcccacf3":"code","97facbcc":"code","5025cbf9":"code","67701978":"code","c87e9c50":"code","bb18b53a":"code","64710226":"code","8a144972":"code","4ba14d5d":"code","0cf79459":"code","1cf26ff5":"code","3dcf7ed1":"code","f7054e7c":"code","53538910":"code","4c00d3f1":"code","28ed4c14":"code","0f4e8df7":"code","c921a308":"code","2853c7b6":"code","a140e23d":"code","2fca4faf":"code","4eee96af":"code","d4bbc8e4":"code","22ac40fc":"code","358f5bdc":"code","e640d954":"code","901b30db":"code","af587a63":"code","9c49b00a":"code","144a5e94":"code","16497eb5":"code","013314f6":"code","1c8a8b82":"code","e2ce41c1":"code","28272489":"code","258e5cc8":"code","78c6f406":"code","4094eb47":"code","572e362c":"code","4ecab71b":"code","206e3b6d":"code","2b1e7dc4":"code","01b63910":"code","7c2711f8":"code","71e0f7ae":"code","dbbb5775":"code","352c4000":"code","ed46e811":"code","1a173aac":"code","2b70f447":"code","8dea655e":"code","c20369b6":"code","be0af46b":"code","5a3f30b9":"code","536c9035":"code","fce5b45f":"code","112c4e85":"code","f540ae16":"code","c4fcc6e6":"code","e8247153":"code","ae0ee502":"code","359b59a1":"code","65c4de91":"code","b6afa820":"code","5ba0fcb0":"code","f1e51098":"code","8efbbde1":"code","b5fc2d87":"code","44c69214":"code","ef5e0832":"code","27b5411a":"code","53cd2f2f":"code","3ef4d847":"code","f9be285b":"code","29be6ae5":"code","82558c1c":"code","3d810bb5":"code","a5d32356":"code","b8e1f0cc":"code","c3aa4423":"code","65f7cb58":"code","1c70bafd":"code","aea01adf":"code","2115a592":"code","7fd09ea0":"code","27facb9f":"code","eaa3301c":"code","1b47d90c":"code","7262ace1":"code","c1e6ed71":"code","7a15e333":"code","22eb8d12":"code","83f94b24":"code","558d7e90":"code","50b57797":"code","203390e1":"code","a1c5c5a6":"code","2e845db1":"code","948791ef":"code","fe6f6aef":"code","1ae3d92f":"code","9273c7d6":"code","f5a534be":"code","3e581d09":"code","32a758cc":"code","758f278c":"code","c76ee187":"code","db9149e0":"code","4915e045":"code","b99b1dc7":"code","2fb292ff":"code","26e995f6":"code","c3f28390":"code","69ca411a":"code","059e6737":"code","7f55e026":"code","902e0cf8":"code","d02f8a7f":"code","058bb35f":"code","2da20294":"code","169996bc":"code","d3fbec3b":"code","b66b7799":"code","669793bc":"code","5a4034d2":"code","deef1d24":"code","95a0487b":"code","b28bdb11":"code","ba73adfa":"code","759ef026":"code","77e58957":"markdown","ea1e9e74":"markdown","47be3d3e":"markdown","1ddde479":"markdown","e85397c4":"markdown","9c7cc16e":"markdown","8601d6ff":"markdown","741a6856":"markdown","106c8166":"markdown","527bd4b2":"markdown","55ca214f":"markdown","776eb50c":"markdown","c7fca8e1":"markdown","ff579c6d":"markdown","b46bf54c":"markdown","936ccacb":"markdown","9f4decb1":"markdown","e244f0f5":"markdown","dd7850dc":"markdown","05d7172b":"markdown","6b27af66":"markdown","0b77c454":"markdown","7f1121a3":"markdown","8c046642":"markdown","edbbbaa2":"markdown","3ee14134":"markdown","b059a04f":"markdown","63205c68":"markdown","d42986a5":"markdown","e0bbef6c":"markdown","8afd2049":"markdown","eaf30ab0":"markdown","c0fb8a51":"markdown","0ec6be41":"markdown","2ed3135d":"markdown","89f81ba1":"markdown","3c9eb980":"markdown","2e8bd5a5":"markdown","115de88f":"markdown","f0e7d928":"markdown","3de67e14":"markdown","13d7de87":"markdown","2aaf2a85":"markdown","cd5fe6ff":"markdown","c4ac890e":"markdown","7276f9f9":"markdown","4751fe3d":"markdown","787f6d19":"markdown"},"source":{"90772d16":"from subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Import the necessary packages\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter(action =\"ignore\")\n\nfrom collections import Counter\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\nfrom scikitplot.plotters import plot_learning_curve\nfrom mlxtend.plotting import plot_learning_curves\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nimport seaborn as sns\n\n# Algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import VotingClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import mean_squared_error","8750fa6e":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest  = pd.read_csv('..\/input\/titanic\/test.csv')","d8d4038b":"train.head()","66664ebd":"train.columns.values","1cc49d33":"test.head()","d5d1afb5":"test.columns.values","9fc3a93d":"# Analyse statically insight of train data\ntrain.describe()","a0e4b11d":"# Analyse statically insight of test data\ntest.describe()","97db2532":"train.info()","900ccf60":"test.info()","7fcf4a24":"print(f\"The train data size: {train.shape}\")\nprint(f\"The test data size: {test.shape}\")","157d2de3":"diff_train_test = set(train.columns) - set(test.columns)\ndiff_train_test","09a1c332":"train[\"Survived\"].describe()","0303ae8f":"total_survived= train[\"Survived\"].sum()\ntotal_no_survived = 891 - total_survived\n\nplt.figure(figsize = (10,5))\nplt.subplot(121)\nsns.countplot(x=\"Survived\", data=train)\nplt.title(\"Survival count\")\n\nplt.subplot(122)\nplt.pie([total_no_survived, total_survived], labels=[\"No Survived\",\"Survived\"], autopct=\"%1.0f%%\")\nplt.title(\"Survival rate\") \n\nplt.show()","7968ca4f":"numeric_data=train.select_dtypes(exclude=\"object\")\nnumeric_corr=numeric_data.corr()\nf,ax=plt.subplots(figsize=(17,1))\nsns.heatmap(numeric_corr.sort_values(by=[\"Survived\"], ascending=False).head(1), cmap=\"Greens\")\nplt.title(\" Numerical features correlation with the survival\", weight='bold', fontsize=18, color=\"green\")\nplt.xticks(weight=\"bold\")\nplt.yticks(weight=\"bold\", color=\"green\", rotation=0)\n\n\nplt.show()","2911fb17":"Num_feature=numeric_corr[\"Survived\"].sort_values(ascending=False).head(10).to_frame()\n\ncm = sns.light_palette(\"green\", as_cmap=True)\n\nstyle = Num_feature.style.background_gradient(cmap=cm)\nstyle","a61c17cb":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train[\"Survived\"].to_frame()\n\n#Combine train and test sets\nconcat_data = pd.concat((train, test), sort=False).reset_index(drop=True)\n#Drop the target \"Survived\" and Id columns\nconcat_data.drop([\"Survived\"], axis=1, inplace=True)\nconcat_data.drop([\"PassengerId\"], axis=1, inplace=True)\nprint(\"Total size is :\",concat_data.shape)","6afe3264":"concat_data.head()","91dc3ea3":"concat_data.tail()","fef25c2e":"concat_data.info()","3df90921":"# Count the null columns\nnull_columns = concat_data.columns[concat_data.isnull().any()]\nconcat_data[null_columns].isnull().sum()","d410984c":"# Outlier detection \ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        \n        # 1st quartile (25%)\n        Q1 = np.percentile(concat_data[col], 25)\n        \n        # 3rd quartile (75%)\n        Q3 = np.percentile(concat_data[col],75)\n        \n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = concat_data[(concat_data[col] < Q1 - outlier_step) | \n                              (concat_data[col] > Q3 + outlier_step )].index\n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n   \n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)  \n\n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(concat_data,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","a6541971":"# Drop outliers\ndf = concat_data.drop(Outliers_to_drop)","b56ade64":"print(f\"The full data size: {df.shape}\")","769f55d8":"# Count the null columns\nnull_columns = df.columns[df.isnull().any()]\nprint(f\"Number of null columns in concatenated data:\\n{df[null_columns].isnull().sum()}\")","4cc18860":"numeric_features = df.select_dtypes(include=[np.number])\ncategorical_features = df.select_dtypes(exclude=[np.number])\n\nprint(f\"Numerical features size: {df.shape}\")\nprint(f\"Categorical features size: {categorical_features.shape}\")","19c97280":"categ =  [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\nnumic = [\"Fare\", \"Age\"]\n\n#Distribution\nfig = plt.figure(figsize=(30, 20))\nfor i in range (0,len(categ)):\n    fig.add_subplot(3,3,i+1)\n    sns.countplot(x=categ[i], data=train);  \n\nfor col in numic:\n    fig.add_subplot(3,3,i + 2)\n    sns.distplot(df[col].dropna());\n    i += 1\n    \nplt.show()\nfig.clear()","ec361739":"numeric_features = df.select_dtypes(include=[np.number])\ncorr_numeric_features = numeric_features.corr()\ncorr_numeric_features","397a4df7":"print(f\"Numerical features: {numeric_features.shape}\")","562ca47c":"unique_list_numeric_features = [(item, np.count_nonzero(df[item].unique())) for item in numeric_features]\nunique_list_numeric_features","42a51276":"# Corralation between Numeric features \ncorr_numeric_features = numeric_features.corr()\n\n#Using Pearson Correlation\nsns.heatmap(corr_numeric_features, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap=\"Greens\")\n\nplt.show()","227e3565":"# Count the null columns' numeric_features in data set\nnull_columns_numeric_features = numeric_features.columns[numeric_features.isnull().any()]\nprint(f\"Missing values in numerical features: \\n{numeric_features[null_columns_numeric_features].isnull().sum()}\")","5124a69d":"highly_correlated_visualization = sns.heatmap(train[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Survived\"]].corr(), annot=True, fmt = \".2f\", cmap = \"Greens\")","ae8f7e66":"unique_list_numeric_features = [(item, np.count_nonzero(df[item].unique())) for item in numeric_features]\nunique_list_categorical_features = [(item, np.count_nonzero(df[item].unique())) for item in                                                  categorical_features]\n\nprint(f\"List of unique numerical features:\\n{unique_list_numeric_features}\")\nprint(f\"List of unique categorical features:\\n{unique_list_categorical_features}\")","b1561dfe":"# Corralation between Numeric features \ncorr_numeric_features = numeric_features.corr()\n\n#Using Pearson Correlation\n\nsns.heatmap(corr_numeric_features, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap=\"Greens\")\n\nplt.show()","08d1b132":"df[[\"Name\"]]","eecdf103":"# Count the null in \"Name\" column\nnull_name_columns = df[\"Name\"].isnull().sum()\nnull_name_columns","f718e413":"def GetTitle(name):\n    first_name_with_title = name.split(\",\")[1]\n    title = first_name_with_title.split(\".\")[0]\n    title = title.strip().lower()\n    return title","92ef8644":"df[\"Name\"].map(lambda x : GetTitle(x))","97f10c3a":"df[\"Name\"].map(lambda x : GetTitle(x)).unique()","3e279448":"def GetTitle(name):\n    title_group = {\n        'mr': \"Mr\"\n        , 'mrs': \"Mrs\"\n        , 'miss': \"Miss\"\n        , 'master': \"master\"\n        , 'don': \"Sir\"\n        , 'rev': \"Sir\"\n        , 'dr': \"Officer\"\n        , 'mme': \"Mrs\"\n        , 'ms': \"Mrs\"\n        ,'major': \"Officer\"\n        , 'lady': \"Lady\"\n        , 'sir': \"Sir\"\n        , 'mlle': \"Miss\"\n        , 'col': \"Officer\"\n        , 'capt': \"Officer\"\n        , 'the countess': \"Lady\"\n        ,'jonkheer': \"Sir\"\n        , 'dona': \"Lady\"\n    }\n    first_name_with_title = name.split(\",\")[1]\n    title = first_name_with_title.split(\".\")[0]\n    title = title.strip().lower()\n    return title_group[title]","123e239c":"df[\"Name\"].map(lambda x: GetTitle(x))","293585ae":"df[\"Title\"] = df[\"Name\"].map(lambda x: GetTitle(x))\ndf[\"Title\"]","d1f17f1f":"title_age_median = df.groupby(df[\"Title\"]).Age.transform(\"median\")\ntitle_age_median","51f9463d":"df[[\"Sex\"]]","31cb484f":"Survived = [\"Survived\"]\nNot_Survived = [\"Not Survived\"]\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\nwomen = train[train[\"Sex\"] == \"female\"]\nmen = train[train[\"Sex\"] == \"male\"]\n\nax = sns.distplot(women[women[\"Survived\"] == 1].Age.dropna(), bins=18, label=Survived, ax=axes[0], kde=False, color=\"#006666\")\nax = sns.distplot(women[women[\"Survived\"] == 0].Age.dropna(), bins=40, label=Not_Survived, ax=axes[0], kde=False, color=\"#00b3b3\")\nax.legend()\nax.set_title(\"Female\")\n\nax = sns.distplot(men[men[\"Survived\"] == 1].Age.dropna(), bins=18, label=Survived, ax=axes[1], kde=False, color=\"#006666\")\nax = sns.distplot(men[men[\"Survived\"] == 0].Age.dropna(), bins=40, label=Not_Survived, ax=axes[1], kde=False, color=\"#00b3b3\")\nax.legend()\n_ = ax.set_title(\"Male\")\n\nplt.show()","7ab82b9d":"# Count the null in \"Sex\" column\nnull_sex_columns = df[\"Sex\"].isnull().sum()\nnull_sex_columns","70afca1e":"def getGender(gender):\n    if (gender == \"male\"):\n        return 0\n    else:\n        return 1","727fc933":"df[\"Sex\"] = df[\"Sex\"].map(lambda x: getGender(x))","b62fe2c2":"by_age_gender = df.groupby([\"Age\", \"Sex\"])\nage_gen_sz = by_age_gender.size().unstack()\nage_gen_sz.plot(kind=\"bar\", figsize=(30,12), color=[\"#80CBC4\", \"#00897B\"]);","e9fe8efb":"df[[\"Sex\"]]","63c0d077":"df[[\"Age\"]]","f7e1ca16":"# Count the null in \"Age\" column\nnull_age_columns = df[\"Age\"].isnull().sum()\nnull_age_columns","5943395f":"df[\"Age\"].fillna(title_age_median, inplace = True)","e280418a":"df[[\"Age\"]]","f5e36b83":"df[\"AgeState\"] = np.where(df[\"Age\"] >= 18, \"Adult\",\"Child\")\ndf[\"AgeState\"]","08217a4c":"df[\"AgeState\"].value_counts()","f68eed3a":"df.assign(AgeState = lambda x: np.where(x.Age >= 18, \"Adult\", \"Child\"))","ec86ab60":"plt.figure(figsize= (10 ,5))\nsns.barplot(data=train, x=\"Pclass\", y=\"Survived\",ci=None, color=\"#00897B\")\n\nplt.show()","cf9f57a2":"df[[\"Parch\"]]","b3da111f":"# Count the null in \"Parch\" column\nnull_parch_columns = df[\"Parch\"].isnull().sum()\nnull_parch_columns","94e14ccf":"df[[\"Ticket\"]]","a0dc5150":"# Count the null in \"Ticket\" column\nnull_ticket_columns = df[\"Parch\"].isnull().sum()\nnull_ticket_columns","acc10977":"df[[\"Fare\"]]","0765f1a6":"sns.distplot(df[\"Fare\"], color=\"#00897B\")","3e2df0c2":"# Count the null in \"Fare\" column\nnull_fare_columns = df[\"Fare\"].isnull().sum()\nnull_fare_columns","8a70819e":"median_fare = df.loc[(df[\"Pclass\"] == 3) & (df[\"Embarked\"] == \"S\"), [\"Fare\"]].median()\nprint(median_fare)\nprint(type(median_fare))\nprint(median_fare[0])","94a912fe":"df[\"Fare\"].fillna(median_fare[0] , inplace = True )","d70daa47":"pd.qcut(df[\"Fare\"], q=4)","4d1976c6":"pd.qcut(df[\"Fare\"], q = 4, labels = [\"very_low\", \"low\", \"high\", \"very_high\"])","1d4e976e":"df[\"Fare_Bin\"] = pd.qcut(df[\"Fare\"], q = 4, labels = [\"very_low\", \"low\", \"high\", \"very_high\"])","d171156e":"df[[\"Cabin\"]]","17f616d4":"# Count the null in \"Cabin\" column\nnull_cabin_columns = df[\"Cabin\"].isnull().sum()\nnull_cabin_columns","e7a856bf":"df.Cabin.value_counts(normalize=True)","f432763d":"df[\"Cabin\"].unique()","369f64e8":"df.loc[df[\"Cabin\"] == \"T\"]","e73cb6e5":"df.loc[df[\"Cabin\"] == \"T\", \"Cabin\"] = np.NaN","bdbe0ce2":"df[\"Cabin\"].unique()","d60b66d7":"def Get_Deck(cabin):\n    return np.where(pd.notnull(cabin), str(cabin)[0].upper(), \"Z\")","91d3def2":"df[\"Deck\"] = df[\"Cabin\"].map(lambda x : Get_Deck(x))","9640104b":"df[\"Deck\"].value_counts()","a10ad4b9":"df[[\"Embarked\"]]","df0c044c":"FaceGrid = sns.FacetGrid(train, row=\"Embarked\", size=4.5, aspect=1.6)\nFaceGrid.map(sns.pointplot, \"Pclass\", \"Survived\", palette=None, order=None, hue_order=None, color=\"#00897B\")\nFaceGrid.add_legend()\nplt.show()","7b03b3eb":"# Count the null in \"Embarked\" column\nnull_embarked_columns = df[\"Embarked\"].isnull().sum()\nnull_embarked_columns","740b75f2":"df.groupby([\"Pclass\", \"Embarked\"]).agg({\"Fare\": [\"median\"]}).unstack()","0ebe3e30":"df[\"Embarked\"].fillna(\"C\", inplace = True )","994ab724":"# Check the null in \"Embarked\" column\nnull_embarked_columns = df[\"Embarked\"].isnull().sum()\nnull_embarked_columns","e2af5eee":"df.drop([\"Cabin\", \"Name\", \"Ticket\", \"SibSp\", \"Parch\", \"Sex\"], axis=1, inplace=True) ","524b63c7":"# Check the null columns in concat_data\nnull_columns = df.columns[df.isnull().any()]\ndf[null_columns].isnull().sum()","9e067db7":"print(f\"The Shape of all data: {df.shape}\")","982dcc98":"# One hot encoding:\nfinal_df = pd.get_dummies(df, columns = [\"Deck\", \"Pclass\", \"Title\", \"Fare_Bin\", \"Embarked\", \"AgeState\"])","860d7aac":"final_df.columns","715dd8e4":"print(f\"The shape of the original dataset: {df.shape}\")\nprint(f\"Encoded dataset shape: {final_df.shape}\")\nprint(f\"We have: {final_df.shape[1] - df.shape[1]} new encoded features\")","341e71f6":"TrainData = final_df[:ntrain] \nTestData = final_df[ntrain:]","e23927b4":"TrainData.shape, TestData.shape","66da7473":"TrainData.info()","699f0dfe":"TestData.info()","27822617":"print(f\"Encoded dataset shape: {final_df.shape}\")","ae7df589":"target = train[[\"Survived\"]]","dc33aac3":"print(\"We make sure that both train and target sets have the same row number:\")\nprint(f\"Train: {TrainData.shape[0]} rows\")\nprint(f\"Target: {target.shape[0]} rows\")","3abdab3f":"# Remove any duplicated column names\nfinal_df = final_df.loc[:,~final_df.columns.duplicated()]","8ac3d4a1":"x = TrainData\ny = np.array(target)","f397993e":"from sklearn.model_selection import train_test_split\n# Split the data set into train and test sets \nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","71345145":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","c4ba0fd6":"scaler = MinMaxScaler()\n\n# transform \"x_train\"\nx_train = scaler.fit_transform(x_train)\n# transform \"x_test\"\nx_test = scaler.transform(x_test)\n#Transform the test set\nX_test= scaler.transform(TestData)","89721949":"# Baseline model of gradient boosting classifier with default parameters:\ngbc = GradientBoostingClassifier()\ngbc_mod = gbc.fit(x_train, y_train)\nprint(f\"Baseline gradient boosting classifier: {round(gbc_mod.score(x_test, y_test), 3)}\")\n\npred_gbc = gbc_mod.predict(x_test)","6e447924":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, pred_gbc)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint(f\"Area Under Curve for Gradient Boosting Classifier: {round(roc_auc, 3)}\")","905477b5":"cv_method = StratifiedKFold(n_splits=3, \n                            random_state=42\n                            )","d7b8a862":"# Cross validate Gradient Boosting Classifier model\nscores_GBC = cross_val_score(gbc, x_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Gradient Boosting Classifier model:\\n{scores_GBC}\")\nprint(f\"CrossValMeans: {round(scores_GBC.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_GBC.std(), 3)}\")","2491594a":"params_GBC = {\"loss\": [\"deviance\"],\n              \"learning_rate\": [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1], \n              \"n_estimators\": [200],\n              \"max_depth\": [3, 5, 8],\n              \"min_samples_split\": np.linspace(0.1, 1.0, 10, endpoint=True),\n              \"min_samples_leaf\": np.linspace(0.1, 0.5, 5, endpoint=True),\n              \"max_features\": [\"log2\",\"sqrt\"],\n              \"criterion\": [\"friedman_mse\", \"mae\"]\n              }","b4a564f3":"GridSearchCV_GBC = GridSearchCV(estimator=GradientBoostingClassifier(), \n                                param_grid=params_GBC, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","6f388928":"# Fit model with train data\nGridSearchCV_GBC.fit(x_train, y_train);","163036c1":"# Get the best estimator values.\nbest_estimator_GBC = GridSearchCV_GBC.best_estimator_\nprint(f\"Best estimator values for GBC model:\\n{best_estimator_GBC}\")","fcccacf3":"# Get the best parameter values.\nbest_params_GBC = GridSearchCV_GBC.best_params_\nprint(f\"Best parameter values for GBC model:\\n{best_params_GBC}\")","97facbcc":"# Best score for GBC by using the best_score attribute.\nbest_score_GBC = GridSearchCV_GBC.best_score_\nprint(f\"Best score value foe GBC model: {round(best_score_GBC, 3)}\")","5025cbf9":"# Test with new parameter for GBC model\ngbc = GradientBoostingClassifier(criterion=\"friedman_mse\", learning_rate=1, loss=\"deviance\", max_depth=5, max_features=\"log2\", min_samples_leaf=0.2, min_samples_split=0.5, n_estimators=200, random_state=42)\ngbc_mod = gbc.fit(x_train, y_train)\npred_gbc = gbc_mod.predict(x_test)\n\nmse_gbc = mean_squared_error(y_test, pred_gbc)\nrmse_gbc = np.sqrt(mean_squared_error(y_test, pred_gbc))\nscore_gbc_train = gbc_mod.score(x_train, y_train)\nscore_gbc_test = gbc_mod.score(x_test, y_test)","67701978":"print(f\"Mean Square Error for Gradient Boosting Classifier = {round(mse_gbc, 3)}\")\nprint(f\"Root Mean Square Error for Gradient Boosting Classifier = {round(rmse_gbc, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_gbc_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_gbc_test, 3)}\")","c87e9c50":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_gbc))","bb18b53a":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_gbc))","64710226":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_gbc), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Survived\", \"No Survived\"]);","8a144972":"# Baseline model of XGBoost classifier with default parameters:\nXGBoost_classifier = XGBClassifier()\nXGBoost_classifier_mod = XGBoost_classifier.fit(x_train, y_train)\nprint(f\"Baseline XGBoost classifier: {round(XGBoost_classifier_mod.score(x_test, y_test), 3)}\")\n\npred_XGBoost_classifier = XGBoost_classifier_mod.predict(x_test)","4ba14d5d":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, pred_XGBoost_classifier)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint(f\"Area Under Curve for XGBoost Classifier: {round(roc_auc, 3)}\")","0cf79459":"# Cross validate  XGBoost Classifier model\nscores_XGBoost = cross_val_score(XGBoost_classifier, x_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for XGBoost Classifier model:\\n{scores_XGBoost}\")\nprint(f\"CrossValMeans: {round(scores_XGBoost.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_XGBoost.std(), 3)}\")","1cf26ff5":"params_XGBoost = {\"learning_rate\": [0.01, 0.02, 0.05, 0.1], \n              \"max_depth\": [1, 3, 5],\n              \"min_child_weight\": [1, 3, 5],\n              \"subsample\": [0.6, 0.7, 1],\n              \"colsample_bytree\": [1]\n              }","3dcf7ed1":"GridSearchCV_XGBoost = GridSearchCV(estimator=XGBClassifier(), \n                                param_grid=params_XGBoost, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","f7054e7c":"# Fit model with train data\nGridSearchCV_XGBoost.fit(x_train, y_train);","53538910":"best_estimator_XGBoost = GridSearchCV_XGBoost.best_estimator_\nprint(f\"Best estimator value for XGboost model:\\n{best_estimator_XGBoost}\")","4c00d3f1":"best_params_XGBoost = GridSearchCV_XGBoost.best_params_\nprint(f\"Best parameter values for XGBoost model:\\n{best_params_XGBoost}\")","28ed4c14":"best_score_XGBoost = GridSearchCV_XGBoost.best_score_\nprint(f\"Best score for XGBoost model: {round(best_score_XGBoost, 3)}\")","0f4e8df7":"# The grid search returns the following as the best parameter set\nXGBoost_classifier = XGBClassifier(colsample_bytree=1, learning_rate=0.05, max_depth=5, min_child_weight=1, subsample=0.6, random_state=42)\nXGBoost_classifier_mod = XGBoost_classifier.fit(x_train, y_train)\npred_XGBoost_classifier = XGBoost_classifier_mod.predict(x_test)\n\nmse_gbc = mean_squared_error(y_test, pred_XGBoost_classifier)\nrmse_gbc = np.sqrt(mean_squared_error(y_test, pred_XGBoost_classifier))\nscore_XGBoost_classifier_train = XGBoost_classifier_mod.score(x_train, y_train)\nscore_XGBoost_classifier_test = XGBoost_classifier_mod.score(x_test, y_test)","c921a308":"print(f\"Mean Square Error for XGBoost Classifier = {round(mse_gbc, 3)}\")\nprint(f\"Root Mean Square Error for XGBoost Classifier = {round(rmse_gbc, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_XGBoost_classifier_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_XGBoost_classifier_test, 3)}\")","2853c7b6":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_XGBoost_classifier))","a140e23d":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_XGBoost_classifier))","2fca4faf":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_XGBoost_classifier), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Survived\", \"No Survived\"]);","4eee96af":"# Baseline model of Adaptive Boosting with default parameters:\nadaboost = AdaBoostClassifier()\nadaboost_mod = adaboost.fit(x_train, y_train)\nprint(f\"Baseline Adaptive Boosting: {round(adaboost_mod.score(x_test, y_test), 3)}\")\n\npred_adaboost = adaboost_mod.predict(x_test)","d4bbc8e4":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, pred_adaboost)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint(f\"Area Under Curve for Adaptive Boosting: {roc_auc}\")","22ac40fc":"# Cross validate Adaptive Boosting model\nscores_Adaptive = cross_val_score(adaboost, x_train, y_train, cv = 3, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Adaptive Boosting model:\\n{scores_Adaptive}\")\nprint(f\"CrossValMeans: {round(scores_Adaptive.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_Adaptive.std(), 3)}\")","358f5bdc":"params_adaboost = {\"learning_rate\": [0.001, 0.01, 0.5, 0.1], \n              \"n_estimators\": [750],\n              \"algorithm\": [\"SAMME.R\"]\n              }","e640d954":"GridSearchCV_adaboost = GridSearchCV(estimator=AdaBoostClassifier(), \n                                param_grid=params_adaboost, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","901b30db":"# Fit model with train data\nGridSearchCV_adaboost.fit(x_train, y_train);","af587a63":"best_estimator_adaboost = GridSearchCV_adaboost.best_estimator_\nprint(f\"Best estimatore for Adaboost model:\\n{best_estimator_adaboost}\")","9c49b00a":"best_params_adaboost = GridSearchCV_adaboost.best_params_\nprint(f\"Best parameter values for Adaboost model:\\n{best_params_adaboost}\")","144a5e94":"best_score_adaboost = GridSearchCV_adaboost.best_score_\nprint(f\"Best score for Adaboost model: {round(best_score_adaboost, 3)}\")","16497eb5":"# The grid search returns the following as the best parameter set\nadaboost = AdaBoostClassifier(algorithm=\"SAMME.R\", learning_rate=0.5, n_estimators=750, random_state=42)\nadaboost_mod = adaboost.fit(x_train, y_train)\npred_adaboost = adaboost_mod.predict(x_test)\n\nmse_adaboost = mean_squared_error(y_test, pred_adaboost)\nrmse_adaboost = np.sqrt(mean_squared_error(y_test, pred_adaboost))\nscore_adaboost_train = adaboost_mod.score(x_train, y_train)\nscore_adaboost_train = adaboost_mod.score(x_test, y_test)","013314f6":"print(f\"Mean Square Error for Adaptive Boosting = {round(mse_adaboost, 3)}\")\nprint(f\"Root Mean Square Error for Adaptive Boosting = {round(rmse_adaboost, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_adaboost_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_adaboost_train, 3)}\")","1c8a8b82":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_adaboost))","e2ce41c1":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_adaboost))","28272489":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_adaboost), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Survived\", \"No Survived\"]);","258e5cc8":"# Baseline model of Logistic Regression with default parameters:\n\nlogistic_regression = linear_model.LogisticRegression()\nlogistic_regression_mod = logistic_regression.fit(x_train, y_train)\nprint(f\"Baseline Logistic Regression: {round(logistic_regression_mod.score(x_test, y_test), 3)}\")\n\npred_logistic_regression = logistic_regression_mod.predict(x_test)","78c6f406":"y_pred_proba = logistic_regression.predict_proba(x_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr, tpr, color=\"#00897B\", label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\n\nplt.show()","4094eb47":"# Cross validate Logistic Regression model\nscores_Logistic = cross_val_score(logistic_regression, x_train, y_train, cv =cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Logistic Regression model:\\n{scores_Logistic}\")\nprint(f\"CrossValMeans: {round(scores_Logistic.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_Logistic.std(), 3)}\")","572e362c":"params_LR = {\"tol\": [0.0001,0.0002,0.0003],\n            \"C\": [0.01, 0.1, 1, 10, 100],\n            \"intercept_scaling\": [1, 2, 3, 4]\n              }","4ecab71b":"GridSearchCV_LR = GridSearchCV(estimator=linear_model.LogisticRegression(), \n                                param_grid=params_LR, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","206e3b6d":"# Fit model with train data\nGridSearchCV_LR.fit(x_train, y_train);","2b1e7dc4":"best_estimator_LR = GridSearchCV_LR.best_estimator_\nprint(f\"Best estimator for LR model:\\n{best_estimator_LR}\")","01b63910":"best_params_LR = GridSearchCV_LR.best_params_\nprint(f\"Best parameter values for LR model:\\n{best_params_LR}\")","7c2711f8":"print(f\"Best score for LR model: {round(GridSearchCV_LR.best_score_, 3)}\")","71e0f7ae":"# The grid search returns the following as the best parameter set\nlogistic_regression = linear_model.LogisticRegression(C=1, intercept_scaling=1, tol=0.0001, penalty=\"l2\", solver=\"liblinear\", random_state=42)\nlogistic_regression_mod = logistic_regression.fit(x_train, y_train)\npred_logistic_regression = logistic_regression_mod.predict(x_test)\n\nmse_logistic_regression = mean_squared_error(y_test, pred_logistic_regression)\nrmse_logistic_regression = np.sqrt(mean_squared_error(y_test, pred_logistic_regression))\nscore_logistic_regression_train = logistic_regression_mod.score(x_train, y_train)\nscore_logistic_regression_test = logistic_regression_mod.score(x_test, y_test)","dbbb5775":"print(f\"Mean Square Error for Logistic Regression = {round(mse_logistic_regression, 3)}\")\nprint(f\"Root Mean Square Error for Logistic Regression = {round(rmse_logistic_regression, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_logistic_regression_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_logistic_regression_test, 3)}\")","352c4000":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_logistic_regression))","ed46e811":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_logistic_regression))","1a173aac":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_logistic_regression), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Survived\", \"No Survived\"]);","2b70f447":"# Baseline model of Stochastic Gradient Descent with default parameters:\n\nsgd = linear_model.SGDClassifier()\nsgd_mod = sgd.fit(x_train, y_train)\nprint(f\"Baseline Stochastic Gradient Descent: {round(sgd_mod.score(x_test, y_test), 3)}\")\n\npred_sgd = sgd_mod.predict(x_test)","8dea655e":"# Cross validate Stochastic Gradient Descent model\nscores_SGD = cross_val_score(sgd, x_train, y_train, cv = 3, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Stochastic Gradient Descent model:\\n{scores_SGD}\")\nprint(f\"CrossValMeans: {round(scores_SGD.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_SGD.std(), 3)}\")","c20369b6":"params_SGD = {\"alpha\": [0.001, 0.005, 0.01, 0.05, 0.1, 1], \n             \"max_iter\": [1, 3, 5, 7], \n             \"tol\": [0.0001,0.0002,0.0003]\n              }","be0af46b":"GridSearchCV_SGD = GridSearchCV(estimator=linear_model.SGDClassifier(), \n                                param_grid=params_SGD, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","5a3f30b9":"# Fit model with train data\nGridSearchCV_SGD.fit(x_train, y_train);","536c9035":"best_estimator_SGD = GridSearchCV_SGD.best_estimator_\nprint(f\"Best estimator for SGD model:\\n{best_estimator_SGD}\")","fce5b45f":"best_params_SGD = GridSearchCV_SGD.best_params_\nprint(f\"Best parameter values for SGD model:\\n{best_params_SGD}\")","112c4e85":"best_score_SGD = GridSearchCV_SGD.best_score_\nprint(f\"Best score for SGD model: {round(best_score_SGD, 3)}\")","f540ae16":"sgd = linear_model.SGDClassifier(alpha=0.005, max_iter=7, tol=0.0003, random_state=42)\nsgd_mod = sgd.fit(x_train, y_train)\npred_sgd = sgd_mod.predict(x_test)\n\nmse_sgd = mean_squared_error(y_test, pred_sgd)\nrmse_sgd = np.sqrt(mean_squared_error(y_test, pred_sgd))\nscore_sgd_train = sgd_mod.score(x_train, y_train)\nscore_sgd_test = sgd_mod.score(x_test, y_test)","c4fcc6e6":"print(f\"Mean Square Error for Stochastic Gradient Descent = {round(mse_sgd, 3)}\")\nprint(f\"Root Mean Square Error for Stochastic Gradient Descent = {round(rmse_sgd, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_sgd_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_sgd_test, 3)}\")","e8247153":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_sgd))","ae0ee502":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_sgd))","359b59a1":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_sgd), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Survived\", \"No Survived\"]);","65c4de91":"random_forest = RandomForestClassifier()\nrandom_forest_mod = random_forest.fit(x_train, y_train)\nprint(f\"Baseline Random Forest: {round(random_forest_mod.score(x_test, y_test), 3)}\")\n\npred_random_forest = random_forest_mod.predict(x_test)","b6afa820":"# Cross validate Random forest model\nscores_RF = cross_val_score(random_forest, x_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Random forest model:\\n{scores_RF}\")\nprint(f\"CrossValMeans: {round(scores_RF.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_RF.std(), 3)}\")","5ba0fcb0":"params_RF = {\"min_samples_split\": [2, 6, 20],\n              \"min_samples_leaf\": [1, 4, 16],\n              \"n_estimators\" :[100,200,300,400],\n              \"criterion\": [\"gini\"]             \n              }","f1e51098":"GridSearchCV_RF = GridSearchCV(estimator=RandomForestClassifier(), \n                                param_grid=params_RF, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","8efbbde1":"# Fit model with train data\nGridSearchCV_RF.fit(x_train, y_train);","b5fc2d87":"best_estimator_RF = GridSearchCV_RF.best_estimator_\nprint(f\"Best estimator for RF model:\\n{best_estimator_RF}\")","44c69214":"best_params_RF = GridSearchCV_RF.best_params_\nprint(f\"Best parameter values for RF model:\\n{best_params_RF}\")","ef5e0832":"best_score_RF = GridSearchCV_RF.best_score_\nprint(f\"Best score for RF model: {round(best_score_RF, 3)}\")","27b5411a":"random_forest = RandomForestClassifier(criterion=\"gini\", n_estimators=200, min_samples_leaf=1, min_samples_split=6, random_state=42)\nrandom_forest_mod = random_forest.fit(x_train, y_train)\npred_random_forest = random_forest_mod.predict(x_test)\n\nmse_random_forest = mean_squared_error(y_test, pred_random_forest)\nrmse_random_forest = np.sqrt(mean_squared_error(y_test, pred_random_forest))\nscore_random_forest_train = random_forest_mod.score(x_train, y_train)\nscore_random_forest_test = random_forest_mod.score(x_test, y_test)","53cd2f2f":"print(f\"Mean Square Error for Random Forest = {round(mse_random_forest, 3)}\")\nprint(f\"Root Mean Square Error for Random Forest = {round(rmse_random_forest, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_random_forest_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_random_forest_test, 3)}\")","3ef4d847":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_random_forest))","f9be285b":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_random_forest))","29be6ae5":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_random_forest), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Survived\", \"No Survived\"]);","82558c1c":"svc = SVC()\nsvc_mod = svc.fit(x_train, y_train)\nprint(f\"Baseline Support Vector Machine: {round(svc_mod.score(x_test, y_test), 3)}\")\n\npred_svc = svc_mod.predict(x_test)","3d810bb5":"# Cross validate SVC model\nscores_SVC = cross_val_score(svc, x_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for SVC model:\\n{scores_SVC}\")\nprint(f\"CrossValMeans: {round(scores_SVC.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_SVC.std(), 3)}\")","a5d32356":"params_SVC = {\"C\": [0.1, 1, 10, 100, 1000],  \n              \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001], \n              \"kernel\": [\"rbf\"]\n              }","b8e1f0cc":"GridSearchCV_SVC = GridSearchCV(estimator=SVC(), \n                                param_grid=params_SVC, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=-1,\n                                refit = True,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","c3aa4423":"# Fit model with train data\nGridSearchCV_SVC.fit(x_train, y_train);","65f7cb58":"best_estimator_SVC = GridSearchCV_SVC.best_estimator_\nprint(f\"Best estimator for SVC model:\\n{best_estimator_SVC}\")","1c70bafd":"best_params_SVC = GridSearchCV_SVC.best_params_\nprint(f\"Best parameter values:\\n{best_params_SVC}\")","aea01adf":"best_score_SVC = GridSearchCV_SVC.best_score_\nprint(f\"Best score for SVC model: {round(best_score_SVC, 3)}\")","2115a592":"svc = SVC(C=1000, gamma=0.01, kernel=\"rbf\" , random_state=42)\nsvc_mod = svc.fit(x_train, y_train)\npred_svc = svc_mod.predict(x_test)\n\nmse_svc = mean_squared_error(y_test, pred_svc)\nrmse_svc = np.sqrt(mean_squared_error(y_test, pred_svc))\nscore_svc_train = svc_mod.score(x_train, y_train)\nscore_svc_test = svc_mod.score(x_test, y_test)","7fd09ea0":"print(f\"Mean Square Error for Linear Support Vector Machine = {round(mse_svc, 3)}\")\nprint(f\"Root Mean Square Error for Linear Support Vector Machine = {round(rmse_svc, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_svc_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_svc_test, 3)}\")","27facb9f":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_svc))","eaa3301c":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_svc))","1b47d90c":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_svc), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Survived\", \"No Survived\"]);","7262ace1":"decision_tree = DecisionTreeClassifier(random_state= 42)\ndecision_tree_mod = decision_tree.fit(x_train, y_train)\nprint(f\"Baseline Decision Tree: {round(decision_tree_mod.score(x_test, y_test), 3)}\")\n\npred_decision_tree = decision_tree_mod.predict(x_test)","c1e6ed71":"# Cross validate Decision Tree model\nscores_DT = cross_val_score(decision_tree, x_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Decision Tree model:\\n{scores_DT}\")\nprint(f\"CrossValMeans: {round(scores_DT.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_DT.std(), 3)}\")","7a15e333":"params_DT = {\"criterion\": [\"gini\", \"entropy\"],\n             \"max_depth\": [1, 2, 3, 4, 5, 6, 7, 8],\n             \"min_samples_split\": [2, 3]}","22eb8d12":"GridSearchCV_DT = GridSearchCV(estimator=DecisionTreeClassifier(), \n                                param_grid=params_DT, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=-1,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","83f94b24":"# Fit model with train data\nGridSearchCV_DT.fit(x_train, y_train);","558d7e90":"best_estimator_DT = GridSearchCV_DT.best_estimator_\nprint(f\"Best estimator for DT model:\\n{best_estimator_DT}\")","50b57797":"best_params_DT = GridSearchCV_DT.best_params_\nprint(f\"Best parameter values:\\n{best_params_DT}\")","203390e1":"best_score_DT = GridSearchCV_DT.best_score_\nprint(f\"Best score for DT model: {round(best_score_DT, 3)}\")","a1c5c5a6":"decision_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4, min_impurity_split=2, min_samples_leaf=0.4, random_state=42)\ndecision_tree_mod = decision_tree.fit(x_train, y_train)\npred_decision_tree = decision_tree_mod.predict(x_test)\n\nmse_decision_tree = mean_squared_error(y_test, pred_decision_tree)\nrmse_decision_tree = np.sqrt(mean_squared_error(y_test, pred_decision_tree))\nscore_decision_tree_train = decision_tree_mod.score(x_train, y_train)\nscore_decision_tree_test = decision_tree_mod.score(x_test, y_test)","2e845db1":"print(f\"Mean Square Error for Decision Tree = {round(mse_decision_tree, 3)}\")\nprint(f\"Root Mean Square Error for Decision Tree = {round(rmse_decision_tree, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_decision_tree_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_decision_tree_test, 3)}\")","948791ef":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_decision_tree))","fe6f6aef":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_decision_tree))","1ae3d92f":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_decision_tree), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Survived\", \"No Survived\"]);","9273c7d6":"gaussianNB = GaussianNB()\ngaussianNB_mod = gaussianNB.fit(x_train, y_train)\nprint(f\"Baseline Gaussin Navie Bayes: {round(gaussianNB_mod.score(x_test, y_test), 3)}\")\n\npred_gaussianNB = random_forest_mod.predict(x_test)","f5a534be":"# Cross validate Gaussian Naive Bayes model\nscores_GNB = cross_val_score(gaussianNB, x_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Gaussian Naive Bayes model:\\n{scores_GNB}\")\nprint(f\"CrossValMeans: {round(scores_GNB.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_GNB.std(), 3)}\")","3e581d09":"params_GNB = {\"C\": [0.1,0.25,0.5,1],\n              \"gamma\": [0.1,0.5,0.8,1.0],\n              \"kernel\": [\"rbf\",\"linear\"]}","32a758cc":"GridSearchCV_GNB = GridSearchCV(estimator=svm.SVC(), \n                                param_grid=params_GNB, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=-1,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","758f278c":"# Fit model with train data\nGridSearchCV_GNB.fit(x_train, y_train);","c76ee187":"best_estimator_GNB = GridSearchCV_GNB.best_estimator_\nprint(f\"Best estimator for DT model:\\n{best_estimator_GNB}\")","db9149e0":"best_params_GNB = GridSearchCV_GNB.best_params_\nprint(f\"Best parameter values:\\n{best_params_GNB}\")","4915e045":"best_score_GNB = GridSearchCV_GNB.best_score_\nprint(f\"Best score for GNB model: {round(best_score_GNB, 3)}\")","b99b1dc7":"mse_gaussianNB = mean_squared_error(y_test, pred_gaussianNB)\nrmse_gaussianNB = np.sqrt(mean_squared_error(y_test, pred_gaussianNB))\nscore_gaussianNB_train = gaussianNB_mod.score(x_train, y_train)\nscore_gaussianNB_test = gaussianNB_mod.score(x_test, y_test)","2fb292ff":"print(f\"Mean Square Error for Gaussian Naive Bayes = {round(mse_gaussianNB, 3)}\")\nprint(f\"Root Mean Square Error for Gaussian Naive Bayes = {round(rmse_gaussianNB, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_gaussianNB_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_gaussianNB_test, 3)}\")","26e995f6":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_gaussianNB))","c3f28390":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_gaussianNB))","69ca411a":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_gaussianNB), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Survived\", \"No Survived\"]);","059e6737":"feat_labels = [\"Deck\", \"Pclass\", \"Title\", \"Fare_Bin\", \"Embarked\", \"AgeState\"]\n\n# Print the name and gini importance of each feature\nfor feature in zip(feat_labels, XGBoost_classifier_mod.feature_importances_):\n    print(f\"Feature importances:\\n{feature}\")","7f55e026":"# plot feature importance\nfrom xgboost import plot_importance\n\nplot_importance(XGBoost_classifier_mod, color=\"#00897B\")\nplt.title(\"Feature importances in XGBoost classifier model\")\nplt.show()","902e0cf8":"# Plot learning curve\ndef plot_learning_curve(estimator, title, x, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n        \n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, x, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"#80CBC4\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"#00897B\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","d02f8a7f":"# Gradient Boosting Classifier\nplot_learning_curve(GridSearchCV_GBC.best_estimator_,title = \"Gradient Boosting Classifier learning curve\", x = x_train, y = y_train, cv = cv_method);","058bb35f":"# XGBoost Classifier\nplot_learning_curve(GridSearchCV_XGBoost.best_estimator_,title = \"XGBoost Classifier learning curve\", x = x_train, y = y_train, cv = cv_method);","2da20294":"# Adaptive Boosting\nplot_learning_curve(GridSearchCV_adaboost.best_estimator_,title = \"Adaptive Boosting learning curve\", x = x_train, y = y_train, cv = cv_method);","169996bc":"# Logistic Regression\nplot_learning_curve(GridSearchCV_LR.best_estimator_,title = \"Logistict Regression learning curve\", x = x_train, y = y_train, cv = cv_method);","d3fbec3b":"# Stochastic Gradient Descent\nplot_learning_curve(GridSearchCV_SGD.best_estimator_,title = \"Stochastic Gradient Descent learning curve\", x = x_train, y = y_train, cv = cv_method);","b66b7799":"# Random forest\nplot_learning_curve(GridSearchCV_RF.best_estimator_,title = \"Random Forest learning curve\", x = x_train, y = y_train, cv = cv_method);","669793bc":"# Support Vector Machine\nplot_learning_curve(GridSearchCV_SVC.best_estimator_,title = \"Support Vector Machine learning curve\", x = x_train, y = y_train, cv = cv_method);","5a4034d2":"# Decision Tree\nplot_learning_curve(GridSearchCV_DT.best_estimator_,title = \"Decision Tree learning curve\", x = x_train, y = y_train, cv = cv_method);","deef1d24":"# Gaussian Naive Bayes\nplot_learning_curve(GridSearchCV_LR.best_estimator_,title = \"Gaussian Naive Bayes learning curve\", x = x_train, y = y_train, cv = cv_method);","95a0487b":"results = pd.DataFrame({\n                        \"Model\": [\"Gradient Boosting Classifier\",\n                                    \"XGBoost Classifier\",\n                                    \"Adaptive Boosting\",\n                                    \"Logistic Regression\",\n                                    \"Stochastic Gradient Descent\",\n                                    \"Random Forest\",\n                                    \"Support Vector Machine\",\n                                    \"Decision Tree\",\n                                    \"Gaussian Naive Bayes\"],\n                        \"Score\": [gbc_mod.score(x_train, y_train),\n                                    XGBoost_classifier_mod.score(x_train, y_train),\n                                    adaboost_mod.score(x_train, y_train),\n                                    logistic_regression_mod.score(x_train, y_train),\n                                    sgd_mod.score(x_train, y_train),\n                                    random_forest_mod.score(x_train, y_train),\n                                    svc_mod.score(x_train, y_train),\n                                    decision_tree_mod.score(x_train, y_train),\n                                    gaussianNB_mod.score(x_train, y_train)]\n                        })\nresult_df = results.sort_values(by=\"Score\", ascending=False)\nresult_df = result_df.set_index(\"Score\")\nresult_df.head(10)","b28bdb11":"stack = StackingCVClassifier(classifiers=[random_forest, adaboost, svc, logistic_regression, gbc],\n                            meta_classifier=XGBoost_classifier,\n                            random_state=42)\n\nstack_mod = stack.fit(x_train, y_train.ravel())\nstack_pred = stack_mod.predict(x_test)\n\nprint(f\"Root Mean Square Error test for STACKING CV Classifier: {round(np.sqrt(mean_squared_error(y_test, stack_pred)), 3)}\")","ba73adfa":"test[\"PassengerId\"].value_counts()","759ef026":"# final = (final1 + final2 + final3)\n\nFinal_Submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": stack_mod.predict(X_test)})\n\nFinal_Submission.to_csv(\"Final_Submission.csv\", index=False)\nFinal_Submission.head(10)","77e58957":"* Working with \"Sex\" column","ea1e9e74":"* Gaussian Naive Bayes","47be3d3e":"### Filling missing values in every column.","1ddde479":"* Adaptive Boosting with HyperParameter Tuning","e85397c4":"### AUC (Area Under Curve) as the evaluation metric is a good way for our binary classification problem for Gradient Boosting Classifier","9c7cc16e":"### Working with non-numeric Features","8601d6ff":"### PROJECT CONTENT\n* EXPLORATORY DATA ANALYSIS\n* DATA CLEANING\n* FEATURE ENGINEERING\n* ENCODING CATEGORICAL FEATURES\n* WORKING WITH OUTLIERS\n* MACHINE LEARNING","741a6856":"* XGBoost Classifier with HyperParameter Tuning","106c8166":"### Submission to Kaggle","527bd4b2":"* Working with \"Embarked\" column","55ca214f":"### Scale the data by creating an instance of the scaler and scaling it:","776eb50c":"### Let's have a look first at the correlation between numerical features and the target \"Survived\"","c7fca8e1":"* Working with \"Fare\" column","ff579c6d":"* Logistic Regression model and tuning","b46bf54c":"* Gradient Boosting Classifier with HyperParameter Tuning","936ccacb":"### Lets check that test dataset has all the columns in train dataset except Survived\n","9f4decb1":"* Decision Tree model","e244f0f5":"*Thank you for your consideration.*","dd7850dc":"* Support Vector Machine model","05d7172b":"### The principal of this challenge is, Predict survival on the Titanic. So get the info about the column of \"Survived\":\n","6b27af66":"### Stacking: to avoid fitting on the same data twice , and is effective in reducing overfitting.","0b77c454":"### Second, we pass the GradientBoostingClassifier() and params_GBC as the model and the parameter dictionary into the GridSearchCV function.","7f1121a3":"* Random Forest model and tunig","8c046642":"### Define numeric features and categorical features in concatenate dataset.","edbbbaa2":"### Before cleaning the data, we zoom at the features with missing values, those missing values won't be treated equally. we will use the forward fill method to fill them.","3ee14134":"## Which is the best Model ?","b059a04f":"### Working with outlier","63205c68":"### It's hyperparameter tuning time. First, we need to define a dictionary of GBC parameters for the grid search.","d42986a5":"### Plotting the learning curve","e0bbef6c":"### AUC (Area Under Curve) as the evaluation metric for our binary classification problem for XGBoost Classifier","8afd2049":"### Working with Numerical","eaf30ab0":"### We are done with the cleaning and feature engineering. Now, we split the combined dataset to the original train and test sets","c0fb8a51":"### Encoding categorical features: Convert categorical variables into dummy variable","0ec6be41":"#### To improve this score(0.81), we want to search the set of \"hyperparameters\" by using common approach \"Grid search\". GridSearch exhaustively searches through all possible combinations of hyperparameters during training the phase. Before we proceed further, we shall cover other cross-validation (CV) methods since tuning hyperparameters via grid search is usually cross-validated to avoid overfitting. we are using the StratifiedKFold function with a stratified 10-fold (n_splits = 3) cross-validation. \n*NOTE: For accelerating the running GridSearchCV we set: n-splits=3, n_jobs=2*","2ed3135d":"### Define classifier\u2019s feature importances","89f81ba1":"* Stochastic Gradient Descent model","3c9eb980":"### Machine Learning\n#### We start machine learning by setting the features and target:\n\n1. Features: x\n2. Target: y","2e8bd5a5":"*Welcome to my kernel*\n### Before starting, I'd like to mention for solving this competition, Two tutorial helped me:\n*Resource:*\n* https:\/\/www.dataquest.io\/blog\/learning-curves-machine-learning\/\n* Doing Data Science with Python by Abhishek Kumar","115de88f":"### Building Machine Learning Models","f0e7d928":"### But before going any futher, we start by cleaning the data from missing values.","3de67e14":"* Working with \"Age\" column","13d7de87":"* Working with \"Parch\" column","2aaf2a85":"* Working with \"Pclass\" column","cd5fe6ff":"* Working with \"Cabin\" column","c4ac890e":"* Working with \"Ticket\" column","7276f9f9":"### Features engineering","4751fe3d":"### Generates a plot of a classifier\u2019s feature importances\n","787f6d19":"* Working with Name column"}}