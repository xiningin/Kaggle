{"cell_type":{"6ab75bf0":"code","7c292314":"code","bcafef5c":"code","634a4249":"code","a1c05dbf":"code","717cb8f1":"code","3f2dc67c":"code","dd64ca06":"code","6bc746a1":"code","a5b6e6bc":"code","a8a697ff":"code","417ccdb0":"code","cfbf2abe":"code","5d745c85":"code","74425885":"code","fbb327d8":"markdown","67d0015c":"markdown","e15ce798":"markdown","db482464":"markdown","a66d7f05":"markdown","47afbbff":"markdown","90b28b6b":"markdown"},"source":{"6ab75bf0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, KFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\n\nimport optuna\n\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7c292314":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\", low_memory=False)\n\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\", low_memory=False)\n\ntrain.info(memory_usage=\"deep\")# just to get the idea of how much memory is being consumed in accessing this data frame","bcafef5c":"test.info(memory_usage=\"deep\")# just to get the idea of how much memory is being consumed in accessing this data frame","634a4249":"colors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","a1c05dbf":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","717cb8f1":"train.describe().T","3f2dc67c":"train.isna().sum().sum(), test.isna().sum().sum()","dd64ca06":"train[\"loss\"].value_counts()","6bc746a1":"fig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.bar(train[\"loss\"].value_counts().sort_index().index,\n              train[\"loss\"].value_counts().sort_index().values,\n              color=colors,\n              edgecolor=\"black\")\nax.set_title(\"Loss (target) distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Loss (target) value\", fontsize=14, labelpad=10)\n\n# ax.bar_label(bars, [f\"{x:2.2f}%\" for x in train[\"loss\"].value_counts().sort_index().values\/(len(train)\/100)],\n#                  padding=5, fontsize=10, rotation=90)\n\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","a5b6e6bc":"train.nunique().sort_values().head() # just to get the idea whether we could make any columns categorical or not.","a8a697ff":"sca = StandardScaler()\nX = pd.DataFrame(sca.fit_transform(train.drop([\"id\", \"loss\"], axis=1)),\n                 columns=train.drop([\"id\", \"loss\"], axis=1).columns) # to also include the column in the df\n\nX_test = pd.DataFrame(sca.transform(test.drop(\"id\", axis=1)),\n                      columns=test.drop([\"id\"], axis=1).columns)# to also include the column in the df\n\ny = train[\"loss\"].copy()","417ccdb0":"X.describe().T # the new scaled X","cfbf2abe":"X_test.describe() # ok","5d745c85":"target_bin_edges = np.histogram_bin_edges(train[\"loss\"], bins=10)\n# Above one divides the feature into 10 equal sized bins\ntarget_bin_edges[0] = -np.inf\n# making the the first bin negative infinite\ntarget_bin_edges[-1] = np.inf\n# making the last bin as infinite\nprint(target_bin_edges)\ntarget_bins = pd.cut(train[\"loss\"], target_bin_edges, labels=np.arange(10))\ntarget_bins.value_counts()","74425885":"%%time\n\n# Splitting data into train and valid folds using target bins for stratification\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=123)\n\nfor train_idx, valid_idx in split.split(X, target_bins):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n# Setting optuna verbosity to show only warning messages\n# If the line is uncommeted each iteration results will be shown\n# optuna.logging.set_verbosity(optuna.logging.WARNING)\n\ntime_limit = 3600 * 4\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid,\n                                                    y_train, y_valid),\n               n_trials = 100,\n               timeout=time_limit\n              )\n\n# Showing optimization results\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","fbb327d8":"### Hyperparameter Optimization","67d0015c":"We can see that as the number is inceasing the number of ocurrences of it are decreasing.","e15ce798":"### Target value analysis","db482464":"### Pie Chart","a66d7f05":"Okay, we see that the values are scaled and there is not a vast difference in the max values of diff features.","47afbbff":"### Scaling","90b28b6b":"We can see that there are negative as well as positive values in the whole dataset."}}