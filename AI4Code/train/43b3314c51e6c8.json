{"cell_type":{"c6baf1ed":"code","b89d22c3":"code","ba401bef":"code","46f4a6b7":"code","65f6fde9":"code","3ae6df95":"code","49eb765e":"code","e57db261":"code","f51ce33a":"code","6f81f072":"code","c874d016":"code","45e38e93":"code","448a2f78":"code","244c3a6a":"code","13c74967":"code","977a00c8":"code","64fa58b9":"code","3cc5a6f6":"code","07b57223":"code","c2fcbd55":"code","6c3a257a":"code","42c0aaf3":"code","8df9f9bc":"code","d7434f43":"code","4cf179fd":"code","e7825caa":"code","107f1269":"code","e491ac73":"code","0259b1d2":"code","46fe121b":"code","7d62a4a1":"code","00160d56":"code","bbf2a133":"code","79db2bd9":"code","24106810":"markdown","be3db309":"markdown","ddaa635d":"markdown","981b05e4":"markdown","6fe150db":"markdown","41257c82":"markdown","57e40b84":"markdown"},"source":{"c6baf1ed":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","b89d22c3":"def read_train():\n    train=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","ba401bef":"def jaccard_improve(str1, str2): \n    str1=str1.lower()\n    str2=str2.lower()    \n    index=str1.find(str2) \n    text1=str1[:index]\n    #print(text1)\n    text2=str1[index:].replace(str2,'')\n    words1=text1.split()\n    words2=text2.split()\n    #print(words1[-3:])\n\n    if len(words1)>len(words2):\n        words1=words1[-3:]\n        mod_text=\" \".join(words1)+\" \"+ str2\n    else:\n        words2=words2[0:2]\n        mod_text=str2+\" \"+\" \".join(words2)\n    return mod_text ","46f4a6b7":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split())  \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","65f6fde9":"print(len(train_df))\n#train_df1=train_df","3ae6df95":"train_df['selected_text_mod']=train_df['selected_text']\ntrain_df['mod']=0","49eb765e":"# train_df['mod']=0\n# for index,row in train_df.iterrows():\n#     #print(row['text'])\n#     #print(row['selected_text'])\n#     res1=jaccard(row['text'],row['selected_text_mod'])\n#     res2=jaccard(row['text'],row['selected_text'])\n    \n#     if res1<0.5 and row['mod']==0:\n#         mod_text=jaccard_improve(row['text'],row['selected_text'])\n#         train_df.at[index,'mod']=1\n#         train_df.at[index,'selected_text']=mod_text\n# #         print('____________1')\n# #         print(mod_text)\n# #         print(row['text'])\n# #         print(row['selected_text'])\n# #         print('____________2')\n#         res2=jaccard(row['text'],mod_text)\n#     else:\n#         train_df.at[index,'selected_text']=row['selected_text_mod']\n    \n#     train_df.at[index,'score1']=res1\n#     train_df.at[index,'score2']=res2\n    \n#     #print(res1)\n    \n#     #print(res1)\n#     #train_df.at[index,'score']=res1\n ","e57db261":"# print(len(train_df[train_df.score1!=train_df.score2]))\n\n# train_df[train_df.score1!=train_df.score2]\n\n# #print(len(train_df[train_df.score>0.9]))\n# train_df2=train_df[train_df.score>0.9]","f51ce33a":"#train_df = train_df2","6f81f072":"MAX_LEN = 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","c874d016":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')","45e38e93":"print(input_ids.shape)\nprint(attention_mask.shape)\nprint(token_type_ids.shape)\nprint(start_tokens.shape)\nprint(end_tokens.shape)","448a2f78":" \n\n# for k in range(train_df.shape[0]):\n    \n#     # FIND OVERLAP\n#     text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n#     text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n#     idx = text1.find(text2)\n#     #text1='rx th as'\n#     chars = np.zeros((len(text1)))\n#     print(\"========1\")\n#     print(chars)\n#     chars[idx:idx+len(text2)]=1\n#     print(chars)\n#     print(idx)\n#     print(text1)\n#     print(text2)\n#     print(len(text2))\n#     enc = tokenizer.encode(text1) \n#     print(enc)\n#     print(text1)\n#     if text1[idx-1]==' ': chars[idx-1] = 1 \n#     print(chars)\n\n#     offsets = []; idx=0    \n#     for t in enc.ids:\n#         w = tokenizer.decode([t])\n#         #print(w)\n#         #print(len(w))\n#         offsets.append((idx,idx+len(w)))\n#         idx += len(w)\n#     #print(offsets)\n        \n        \n#     #offsets.append((idx,idx+len(w)))\n#     #idx += len(w)\n        \n#     # START END TOKENS\n#     toks = []\n#     for i,(a,b) in enumerate(offsets):\n#         #print(a,b)\n#         sm = np.sum(chars[a:b])\n#         #print(chars[a:b])\n#         #print(sm)\n#         if sm>0: toks.append(i) \n\n#     s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n#     input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n#     attention_mask[k,:len(enc.ids)+5] = 1\n#     if len(toks)>0:\n#         start_tokens[k,toks[0]+1] = 1\n#         end_tokens[k,toks[-1]+1] = 1            \n            \n#     print(\"========21\")   \n#     print(enc.ids)\n#     print(enc)    \n#     print(text1)\n#     print(text2)\n    \n#     print(offsets)    \n#     print(chars)\n#     print(len(chars))\n#     print(toks)\n#     print(len(toks))\n\n#     print(input_ids[k,:] )\n#     print(s_tok)\n#     print( start_tokens[k,])\n#     print( end_tokens[k,])\n#     print(attention_mask[k,:])\n#     print(toks)\n#     print([0] + enc.ids + [2,2] + [s_tok] + [2])\n#     print(\"========2\")\n    \n    ","244c3a6a":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","13c74967":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","977a00c8":"def scheduler(epoch):\n    return 3e-5 * 0.2**epoch","64fa58b9":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    #x1 = tf.keras.layers.ReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    #x2 = tf.keras.layers.ReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)    \n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model\n","3cc5a6f6":"n_splits = 5","07b57223":"# #input_ids\n# #train_df.sentiment.values\n\n# print(len(train_df))\n\n# train_df1=train_df[:1000]\n# print(len(train_df1))\n\n# input_ids1=input_ids[:1000]\n\n","c2fcbd55":"\n# skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n# for fold,(idxT,idxV) in enumerate(skf.split(input_ids1,train_df1.sentiment.values)):\n#     print(idxV)\n#     print(len(idxV))","6c3a257a":"# jac = []; VER='v6'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n# oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n# oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n\n\n# skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n\n# #for fold,(idxT,idxV) in enumerate(skf.split(input_ids1,train_df1.sentiment.values)):\n# for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n\n#     print('#'*25)\n#     print('### FOLD %i'%(fold+1))\n#     print('#'*25)\n    \n#     K.clear_session()\n#     model = build_model()\n        \n#     reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\n#     sv = tf.keras.callbacks.ModelCheckpoint(\n#         '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, \n#         save_best_only=True,\n#         save_weights_only=True, mode='auto', save_freq='epoch')\n        \n#     hist = model.fit([input_ids[idxT,], attention_mask[idxT,], \n#                       token_type_ids[idxT,]], [start_tokens[idxT,], \n#                                                end_tokens[idxT,]], \n#                         epochs=5, batch_size=8, verbose=DISPLAY, \n#                      callbacks=[sv, reduce_lr],\n#         validation_data=([input_ids[idxV,],attention_mask[idxV,],\n#                           token_type_ids[idxV,]], \n#         [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n#     print('Loading model...')\n#     model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    \n#     print('Predicting OOF...')\n#     oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n     \n    \n#     # DISPLAY FOLD JACCARD\n#     all = []\n#     for k in idxV:\n#         a = np.argmax(oof_start[k,])\n#         b = np.argmax(oof_end[k,])\n#         if a>b: \n#             st = train_df.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n#         else:\n#             text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n#             enc = tokenizer.encode(text1)\n#             st = tokenizer.decode(enc.ids[a-1:b])\n#         all.append(jaccard(st,train_df.loc[k,'selected_text']))\n#     jac.append(np.mean(all))\n#     print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n#     print(oof_start[idxV,])\n#     print(oof_end[idxV,])\n    \n     \n   \n","42c0aaf3":" \n\n# # DISPLAY FOLD JACCARD\n# all = []\n# for k in idxV:\n#     a = np.argmax(oof_start[k,])\n#     b = np.argmax(oof_end[k,])\n#     if a>b: \n#         st = train_df.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n#     else:\n#         text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n#         enc = tokenizer.encode(text1)\n#         st = tokenizer.decode(enc.ids[a-1:b])\n#     all.append(jaccard(st,train_df.loc[k,'selected_text']))\n# jac.append(np.mean(all))\n# print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n# # print(oof_start[idxV,])\n# # print(oof_end[idxV,])\n\n ","8df9f9bc":"#train_df.loc[10,'text']\n\n\n#train_df.reset_index(inplace = True) \n#train_df","d7434f43":"preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(5):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    #model.load_weights('..\/input\/m6aprila\/v6-roberta-%i.h5'%i)\n    model.load_weights('..\/input\/model8\/v8-roberta-%i.h5'%i)\n\n    #model.load_weights('v5-roberta-%i.h5'%i)\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/n_splits\n    preds_end += preds[1]\/n_splits","4cf179fd":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_df.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])          \n        st1=st.strip()\n        if st1=='****' or  st1=='****!' or st1=='****!' or st1=='****!' or st1=='****,' or  st1=='****,' or  st1=='****.' or st1=='****.':\n            #print(st1.strip())\n            #print(text1)\n            st=text1\n        elif st1=='(good':   \n            st='good'\n        elif st1=='__joy':   \n            st='joy'           \n    all.append(st)","e7825caa":"# import pandas as pd\n# # train_df.to_csv('train_df.csv',index=False)\n\n# test_df=pd.read_csv('..\/input\/submission\/submission_v2.csv')\n\n# # for index,row in test_df.iterrows():\n# #     row['selected_text']\n","107f1269":"# i=0\n# for index,row in test_df.iterrows():\n#     #print(row['selected_text'])\n#     if len(row['selected_text'])>100:\n#         #print(row['selected_text'])\n#         test_df.at[index,'selected_text']=''\n#         i=i+1\n# print(i)","e491ac73":"test_df['selected_text'] = all\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)\n \n","0259b1d2":"\n\n# from distutils.dir_util import copy_tree\n# todir='\/kaggle\/working'\n# fromdirc='..\/input\/tweet-sentiment-extraction'\n# copy_tree(fromdirc,todir)\n\n","46fe121b":"# import os\n# os.chdir('\/kaggle')\n# os.getcwd()\n# os.listdir()","7d62a4a1":"# # import os\n# # os.getcwd()\n\n# import shutil\n# source='\/kaggle\/test1.csv'\n# destination='\/kaggle\/working\/test1.csv'\n# dest = shutil.copyfile(source, destination)\n","00160d56":"# import os\n# import shutil\n# for subdir, dirs, files in os.walk('\/kaggle\/working\/'):\n#     for file in files:\n#         if '.h5' in file:\n#           #print(file) #file\n#           source='\/kaggle\/working'+str('\/')+file\n#           destination='..\/input\/model4'+str('\/')+file\n#           destination='\/kaggle'+str('\/')+file\n#           print(source)  \n#           #print(destination)  \n#           dest = shutil.copyfile(source, destination) ","bbf2a133":"# import os\n# for subdir, dirs, files in os.walk('..\/input\/model4\/'):\n#     for file in files:\n#       print(file) #file","79db2bd9":"# from IPython.display import FileLink, FileLinks\n# FileLinks('.') #lists all downloadable files on server","24106810":"# Load  data and libraries","be3db309":"# Model","ddaa635d":"# Train\nWe will skip this stage and load already trained model","981b05e4":"# TensorFlow roBERTa + CNN head - LB   v2","6fe150db":"# Data preproccesing","41257c82":"# Inference","57e40b84":"Hello everyone! \n\n1. 1. 1. This kernel is based on [Al-Kharba Kiram](https:\/\/www.kaggle.com\/al0kharba\/tensorflow-roberta-0-712\/output).  \n\n "}}