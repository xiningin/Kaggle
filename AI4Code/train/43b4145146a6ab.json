{"cell_type":{"e4320c4a":"code","7873ce11":"code","1713b95a":"code","bdfbd6cc":"code","eb23ec96":"code","a4434984":"code","dbdfb4d3":"code","28cb8374":"code","1afac37f":"code","9cb1460a":"code","9ee2b003":"markdown","80fb7dd2":"markdown","e6353bcd":"markdown","f7adc106":"markdown","f3a2b443":"markdown"},"source":{"e4320c4a":"# Pull in some tools we'll need.\nimport codecs\nimport glob\nimport gensim\n%pylab inline","7873ce11":"# Create a list of all of our book files.\nbook_filenames = sorted(glob.glob(\"..\/input\/*.rtf\"))\nprint(\"Found books:\")\nbook_filenames","1713b95a":"# Read each book into the book_corpus, doing some cleanup along the way.\nbook_corpus = []\nfor book_filename in book_filenames:\n    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n        book_corpus.append(\n            gensim.models.doc2vec.TaggedDocument(\n                gensim.utils.simple_preprocess( # Clean the text with simple_preprocess\n                    book_file.read()),\n                    [\"{}\".format(book_filename)])) # Tag each book with its filename","bdfbd6cc":"# Set up the model.\nmodel = gensim.models.Doc2Vec(vector_size = 300, \n                              min_count = 3, \n                              epochs = 100)","eb23ec96":"model.build_vocab(book_corpus)\nprint(\"model's vocabulary length:\", len(model.wv.vocab))","a4434984":"model.train(book_corpus,\n            total_examples=model.corpus_count,\n            epochs=model.epochs)","dbdfb4d3":"model.docvecs.most_similar(12) #The_Adventures_of_Tom_Sawyer_by_Mark_Twain","28cb8374":"model.docvecs.most_similar(11) # The_Adventures_of_Sherlock_Holmes_by_Arthur_Conan_Doyle.rtf","1afac37f":"model.docvecs.most_similar(16) # The_Prince_by_Nicolo_Machiavelli.rtf","9cb1460a":"model.wv.most_similar(\"monster\")","9ee2b003":"Below we will find the books that the neural net thinks are most similar to the given one. You can put any number you like (between 0 and 19) inside the parentheses. The numbers correspond to the order of the books inside the input folder (under \"Data\", in the right sidebar).\n\nThe output will show you pairs which contain the book's identifier and also a similarity score. (The list will be sorted by similarity.)","80fb7dd2":"Incidentally, in deciding which *documents* are most similar, the neural net has also made inferences about which *words* are most similar. You can explore what it thinks by putting different words inside the quote marks below.\n\nAs with documents, it outputs both words and similarity scores. Can you identify a threshhold score above which the similarity seems reliable, and below which it's just grasping at straws?\n\nKeep in mind that it won't know anything about words that don't appear in its corpus. If you enter a word it doesn't know about, it will throw an error. That's fine; you can just try a different word and rerun the cell.","e6353bcd":"# Comparing Project Gutenberg's 20 Most Popular Books","f7adc106":"In this analysis, we will use Doc2Vec to determine which of Project Gutenberg's 20 most popular books are most conceptually similar.","f3a2b443":"The next bit of code is where we build our model -- the fun part!\n\nWe set up our model with parameters that affect how it learns. In theory\nwe can set those numbers to any value we like, although in practice some will produce more useful results than others. This is more of an art than a science, and experimentation is helpful here.\n\nThe first time you run this notebook, use the defaults so you can see how it works. But after that, go ahead and set these values to any number you like. (Afterward, rerun this code block and the ones after it to see the data update.)\n\n**vector_size** is how many dimensions our idea space has. More dimensions lets you capture more possible concepts, but runs the risk that no books are is similar to any others (the space is too big, so everything is far apart).\n\n**min_count** lets you ignore infrequent words (any word which is in the corpus less than min_count times). These are hard for the neural net to understand since it doesn't have much data about them. Setting min_count low will mean including rare words and ending up with garbage ideas about them, but setting it high will mean throwing out data you could have used.\n\n**epochs** is how many times to run over the training data. Too few will be inaccurate but too many will be slow and have diminishing returns.\n\nWant all the programmer details (including many other parameters you can set)? Check out the [gensim documentation](https:\/\/radimrehurek.com\/gensim\/models\/doc2vec.html#gensim.models.doc2vec.Doc2Vec).\n"}}