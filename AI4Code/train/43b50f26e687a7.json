{"cell_type":{"250211c1":"code","7cb56f9b":"code","9344319a":"code","8a7ce391":"code","fcfbe6ba":"code","5a2b096d":"code","078540e5":"code","b251de00":"code","72899b18":"code","1b7435d9":"code","f6ff363e":"code","7258c848":"code","9b1c0fb5":"code","7586de5f":"code","1b032467":"code","863ba845":"code","d26a2bf4":"code","aad8aee1":"code","f122c566":"code","8b9499a3":"code","608d8453":"code","ed68e9a0":"code","6e24f255":"code","c7643d20":"code","625cf90b":"code","ca180a8e":"code","0eab3d65":"code","eb8180ed":"code","2bafa206":"code","ea6afb39":"code","3ff6244a":"code","bd615250":"code","1f7e9cb2":"code","cf679a32":"code","cda10f28":"code","d116ed6a":"code","3c236e6b":"code","a127e4f9":"code","a8e4816b":"code","6a4a7d67":"code","d8465207":"code","d54a6c79":"code","d9699bd2":"code","a2ced3e1":"code","0313e74d":"code","4acf9ab7":"code","e97e6728":"code","143cb227":"code","b587b7f7":"code","f062f291":"code","4199c4cd":"code","02a80deb":"code","85ede939":"code","77f35f38":"markdown","9b248b85":"markdown","a7f82c4e":"markdown","cc1740c0":"markdown","84f3bff2":"markdown","97cc38df":"markdown"},"source":{"250211c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7cb56f9b":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')","9344319a":"# Read the full of data and display some rows\ndf_ratings_all = pd.read_csv('\/kaggle\/input\/the-movies-dataset\/ratings.csv', usecols = ['userId','movieId', 'rating'])\ndf_ratings = df_ratings_all[:1000000]\ndf_ratings.sample(10)","8a7ce391":"# Check if there is any None value or duplicated line\nmissing_data = sum(df_ratings.isnull().any())\nduplicated_data = sum(df_ratings.duplicated(['userId','movieId','rating']))\nprint(\"Missing data: {} and duplicated data: {}\".format(missing_data, duplicated_data))\ndf_ratings.dtypes","fcfbe6ba":"# Split dataset as %80 train and %20 test\ndf_train = df_ratings.sample(frac=0.8)\ndf_test = df_ratings.drop(df_train.index)\nprint(\"Train data: {}, test data: {}\".format(len(df_train), len(df_test)))","5a2b096d":"# Visualize the data as a bar chart according to count of ratings\nfig, ax = plt.subplots()\nsns.countplot(df_train.rating)\nplt.title('Distribution if rating over training dataset', fontsize=10)\nax.set_ylabel('No. of Ratings (Million)')\nplt.show()","078540e5":"# Visualize first 100 entry for which movie is rated by which user while the color intensity of the marks represent the rating score.\nx = df_train.head(100).movieId\ny = df_train.head(100).userId\n\nplt.scatter(x, y, c=df_train.head(100).rating, alpha=0.5)\nplt.colorbar();  \nplt.xlabel(\"Movie IDs\")\nplt.ylabel(\"User IDs\")\nplt.show()","b251de00":"#Define the constants for the each filteres input\nCONSTANTS_RECOMENDATION={\"USER_ID_MODEL\":1,\n                         \"MOVIE_ID_MODEL\":260,\n                         \"ITEM_MOVIE\":\"Rocky III\",\n                         \"CONTENT_MOVIE\":\"Toy Story\"\n}\nCONSTANTS_RECOMENDATION[\"CONTENT_MOVIE\"]","72899b18":"from surprise import Reader, Dataset,SVD\nfrom surprise.model_selection import cross_validate\n# Define the format\nreader = Reader(rating_scale=(0.5, 5.0))\ndata = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\ndata\n","1b7435d9":"svd_benchmark = []\n\nalgorithm =  SVD(n_epochs = 15, lr_all=0.01 ) # epc=15 ,lr=0.01test_rmse 0.867418\n\n# Perform cross validation\nresults = cross_validate(algorithm, data, measures=['RMSE','MAE'], cv=5, verbose=True)\n\n# Get results & append algorithm name\ntmp = pd.DataFrame.from_dict(results).mean(axis=0)\ntmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]]))\nsvd_benchmark.append(tmp)\nprint(svd_benchmark)","f6ff363e":"from surprise.model_selection import train_test_split\nfrom surprise import accuracy\ntrainset, testset = train_test_split(data,test_size=.25)\nalgorithm.fit(trainset)\npredictions = algorithm.test(testset)\naccuracy.rmse(predictions)","7258c848":"def get_collabration_based_model_recommendations(userId, movieId):\n    return algorithm.predict(userId, movieId).est","9b1c0fb5":"get_collabration_based_model_recommendations(CONSTANTS_RECOMENDATION[\"USER_ID_MODEL\"],CONSTANTS_RECOMENDATION[\"MOVIE_ID_MODEL\"]) # try 260 2767 258 2761940 1628","7586de5f":"# Read required files\ncredits = pd.read_csv('\/kaggle\/input\/the-movies-dataset\/credits.csv')\nprint(\"Type of credit columns: \\n\", credits.dtypes)\nmovies = pd.read_csv('\/kaggle\/input\/the-movies-dataset\/movies_metadata.csv')\nprint(\"Type of movies columns: \\n\", movies.dtypes)\nkeywords = pd.read_csv('\/kaggle\/input\/the-movies-dataset\/keywords.csv')\nprint(\"Type of keywords columns: \\n\", keywords.dtypes)","1b032467":"# Merge these tree file to the one dataframe. But id of movie column is object but for other files movie id column is int64.\n# Convert these int64 to string then merge\nkeywords['id'] = keywords['id'].astype('str')\ncredits['id'] = credits['id'].astype('str')\ndf_merge = (credits.merge(movies,on='id')).merge(keywords,on='id')\ndf = df_merge[:10000]\ndf.dtypes\n# print(movies.shape)\n# print(credits.shape)\n# print(keywords.shape)\n","863ba845":"# Choose which columns will be used. And drop the null values.\ndf = df[['title','cast', 'crew', 'genres','keywords','original_language']]\nprint(sum(df.isnull().any()))\ndf.dropna(inplace=True)\nprint(sum(df.isnull().any()))\ndf.head()","d26a2bf4":"# Data maybe consist of lower\/upper case problems.\n# Can solve this problem using regular expirations\nimport unicodedata\nimport re\ndef clean_data(value):\n    value =  unicodedata.normalize('NFD', value).encode('ascii', 'ignore').decode('ascii')\n    x = str.lower(re.sub(r\"[^a-zA-Z0-9]+\", '_', value.strip().replace(\" \",\"_\")))\n    if(x != '_'):\n        return x \n    else:\n        return \" \" ","aad8aee1":"df.cast","f122c566":"import ast\ndf['cast'] = df['cast'].apply(lambda s: list(ast.literal_eval(s)))\ndf['cast'] = df['cast'].apply((lambda cast : [clean_data(actor['name']) for actor in cast]))\ndf.cast","8b9499a3":"print(df.title)\ndf['title'] = df['title'].apply((lambda title: clean_data(title)))\ndf.title","608d8453":"print(df.genres)\ndf['genres'] = df['genres'].apply(lambda s: list(ast.literal_eval(s)))\ndf['genres'] = df['genres'].apply((lambda genres : [clean_data(genre['name']) for genre in genres]))\ndf.genres","ed68e9a0":"print(df.keywords)\ndf['keywords'] = df['keywords'].apply(lambda s: list(ast.literal_eval(s)))\ndf['keywords'] = df['keywords'].apply((lambda keywords : [clean_data(key['name']) for key in keywords[:5]]))\ndf.keywords","6e24f255":"# The other crew members except director may mislead the model. So select only director name\nprint(df.crew)\ndf['crew'] = df['crew'].apply(lambda s: list(ast.literal_eval(s)))\ndf['crew'] = df['crew'].apply((lambda crew : [clean_data(member['name']) for member in crew if member[\"job\"] == \"Director\"]))\ndf.crew","c7643d20":"print(df.original_language)\n# Dont change anything here\ndf.title","625cf90b":"# Now cumulate all the words on a column, say bag of words :)\ntest_csv = pd.read_csv('..\/input\/imdb-movie-reviews-dataset\/test_data (1).csv') # path to file\ntrain_csv = pd.read_csv('..\/input\/imdb-movie-reviews-dataset\/train_data (1).csv') # path to file\ndf['bag_of_words'] = df['cast'] + df['keywords'] + df['genres'] + df['crew'] \ndf['bag_of_words'] = df['bag_of_words'].apply(lambda x: ' '.join(x)) + \" \" + df['title']\ndf.bag_of_words.head()\n","ca180a8e":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df.bag_of_words)\ntfidf.get_feature_names()","0eab3d65":"# Compute cosine similarity between all movie-descriptions\nsimilarity = cosine_similarity(tfidf_matrix)\n# Remove self-similarity from matrix\nsimilarity -= np.eye(similarity.shape[0])","eb8180ed":"def get_content_based_recommendations(title):\n    # Get the index of given movie (if exist)\n    index = df.reset_index(drop=True)[df.title == clean_data(title)].index\n    if len(index) > 0:\n        index = index[0]\n        n_plot = 10\n        # Get indices and scores of similar movies\n        similar_movies_index = np.argsort(similarity[index])[::-1][:n_plot]\n        similar_movies_score = np.sort(similarity[index])[::-1][:n_plot]\n\n        # Get titles of similar movies\n        similar_movie_titles = df.iloc[similar_movies_index].index\n        return [df.iloc[index].title for index in similar_movies_index]\n    else:\n        return None","2bafa206":"get_content_based_recommendations(CONSTANTS_RECOMENDATION[\"CONTENT_MOVIE\"])","ea6afb39":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics","3ff6244a":"train_X = train_csv['0']   # '0' corresponds to Texts\/Reviews\ntrain_y = train_csv['1']   # '1' corresponds to Label (1 - positive and 0 - negative)\ntest_X = test_csv['0']\ntest_y = test_csv['1']","bd615250":"# loading TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer()\nX_train_tfid = tfidf_vectorizer.fit_transform(train_X)\nX_test_tfid = tfidf_vectorizer.transform(test_X)","1f7e9cb2":"#Analying train and test data with use of naive_bayes classifier\nnaive_bayes_classifier_tfidf = MultinomialNB()\nnaive_bayes_classifier_tfidf.fit(X_train_tfid,train_y)","cf679a32":"y_pred_tfidf= naive_bayes_classifier_tfidf.predict(X_test_tfid)\n# compute the performance measures\n\nprint(\"accuracy of TfIdfVectorizer:\")\n\nprint(metrics.classification_report(test_y, y_pred_tfidf,\n                                            target_names=['Positive', 'Negative']))","cda10f28":"from sklearn.decomposition import TruncatedSVD\nmovies_item=movies\ndf_ratings_small=pd.read_csv('\/kaggle\/input\/the-movies-dataset\/ratings_small.csv')\ndf_ratings_small['movieId'] = df_ratings_small['movieId'].astype('str')\ndf_ratings_mer = df_ratings_small.merge(movies_item[['title','id']], left_on='movieId', right_on='id')\ndf_ratings_mer = df_ratings_mer[['userId','movieId','rating', 'title']]\n","d116ed6a":"print(len(df_ratings))\nprint(len(df_ratings_mer))\ndf_ratings_mer.dtypes\ndf_ratings_mer.shape\ndf_ratings_mer","3c236e6b":"rating_cross = df_ratings_mer.pivot_table(values=\"rating\",index=\"userId\",columns=\"title\",fill_value=0)\nrating_cross.sample(10)\n","a127e4f9":"train_matrix = rating_cross.T","a8e4816b":"SVD_Truncated = TruncatedSVD(n_components=12, random_state=5)\nresultant_matrix = SVD_Truncated.fit_transform(train_matrix)\nresultant_matrix.shape","6a4a7d67":"### correlation matrix\ncorr_mat = np.corrcoef(resultant_matrix)\ncorr_mat.shape","d8465207":"col_idx = rating_cross.columns.get_loc(CONSTANTS_RECOMENDATION[\"ITEM_MOVIE\"])\ncorr_specific = corr_mat[col_idx]\npd.DataFrame({'corr_specific':corr_specific, 'Movies': rating_cross.columns})\\\n.sort_values('corr_specific', ascending=False)\\\n.head(10)","d54a6c79":"reader = Reader(rating_scale=(0.5, 5.0))\nTruncated_svd =  SVD(n_epochs = 19)\ndata_trun_test = Dataset.load_from_df(df_ratings_mer[['userId', 'movieId', 'rating']], reader)","d9699bd2":"trun_svd = []\n\n# Perform cross validation\nresult = cross_validate(Truncated_svd, data_trun_test, measures=['RMSE','MAE'], cv=5, verbose=True)\n\n# Get results & append algorithm name\ntrun_svd = pd.DataFrame.from_dict(result).mean(axis=0)\ntrun_svd = trun_svd.append(pd.Series([str(Truncated_svd).split(' ')[0].split('.')[-1]]))\ntrun_svd.append(trun_svd)\nprint(trun_svd)","a2ced3e1":"df_ratings_all['movieId'] = df_ratings_all['movieId'].astype('str')\ndf_ratings_merge = df_ratings_all.merge(movies[['title','id']], left_on='movieId', right_on='id')\ndf_ratings_merge = df_ratings_merge[['userId','movieId', 'rating', 'title']]\ndf_ratings_merge","0313e74d":"print(len(df_ratings_all))\nprint(len(df_ratings_merge))","4acf9ab7":"#Clean the title of new merged data\ndf_ratings_merge_with_different_title = df_ratings_merge\ndf_ratings_merge_with_different_title['title'] = df_ratings_merge_with_different_title['title'].apply(lambda x: clean_data(x))\nprint(df_ratings_merge_with_different_title)","e97e6728":"def get_content_based_recommendation_for_user(userId):\n    # Check if user id exist\n    # if not exist show some of populars\n    \n    # Get highest ranking score of this user\n    highest_ranked_movies = df_ratings_merge[df_ratings_merge['userId'] == userId]\n    highest_ranked_movies.sort_values(by=['rating'], ascending=False)\n    #print(highest_ranked_movies)\n    similar_movies = set()\n    for i in range(len(highest_ranked_movies)):\n        current_similar_movies = get_content_based_recommendations(highest_ranked_movies.iloc[i].title)\n        if current_similar_movies == None:\n             continue\n        for movie_title in current_similar_movies:\n            similar_movies.add(movie_title)\n    \n    result_df = pd.DataFrame(similar_movies, columns=['title'])\n    \n    without_duplicate = df_ratings_merge_with_different_title.drop_duplicates(subset='title', keep='first', inplace = False)    \n    result_df = result_df.merge(without_duplicate, on = 'title')\n    \n    return result_df","143cb227":"def get_recommendations_hybrid_content_model(userId):\n    content_based = get_content_based_recommendation_for_user(userId)\n    content_based = content_based[['title','movieId']]\n    \n    predicted_rating = []\n    for i in range(len(content_based)):\n        ratings = get_collabration_based_model_recommendations(userId, int(content_based.iloc[i].movieId))\n        predicted_rating.append(ratings)\n    content_based['predicted_rating'] = predicted_rating\n    content_based = content_based.sort_values(by=['predicted_rating'], ascending=False)\n\n    return content_based[:10]","b587b7f7":"get_recommendations_hybrid_content_model(1)","f062f291":"corr_mat -= np.eye(corr_mat.shape[0])\ndef get_item_based_recommendations(title):\n    # Get the index of given movie (if exist)\n    index = df.reset_index(drop=True)[df.title == title].index\n    if len(index) > 0:\n        index = index[0]\n        n_plot = 10\n        # Get indices and scores of similar movies\n        similar_movies_index = np.argsort(corr_mat[index])[::-1][:n_plot]\n        similar_movies_score = np.sort(corr_mat[index])[::-1][:n_plot]\n\n        # Get titles of similar movies\n        similar_movie_titles = df.iloc[similar_movies_index].index\n        return [df.iloc[index].title for index in similar_movies_index]\n    else:\n        return None","4199c4cd":"def get_item_based_recommendation_for_user(userId):\n    # Check if user id exist\n    # if not exist show some of populars\n    \n    # Get highest ranking score of this user\n    highest_ranked_movies = df_ratings_merge[df_ratings_merge['userId'] == userId]\n    highest_ranked_movies.sort_values(by=['rating'], ascending=False)\n    #print(highest_ranked_movies)\n    similar_movies = set()\n    for i in range(len(highest_ranked_movies)):\n        current_similar_movies = get_item_based_recommendations(highest_ranked_movies.iloc[i].title)\n        if current_similar_movies == None:\n             continue\n        for movie_title in current_similar_movies:\n            similar_movies.add(movie_title)\n    \n    result_df = pd.DataFrame(similar_movies, columns=['title'])\n    \n    without_duplicate = df_ratings_merge_with_different_title.drop_duplicates(subset='title', keep='first', inplace = False)    \n    result_df = result_df.merge(without_duplicate, on = 'title')\n    \n    return result_df","02a80deb":"def get_recommendations_hybrid_item_model(userId):\n    item_based = get_item_based_recommendation_for_user(userId)\n    item_based = item_based[['title','movieId']]\n    \n    predicted_rating = []\n    for i in range(len(item_based)):\n        ratings = get_collabration_based_model_recommendations(userId, int(item_based.iloc[i].movieId))\n        predicted_rating.append(ratings)\n    item_based['predicted_rating'] = predicted_rating\n    item_based = item_based.sort_values(by=['predicted_rating'], ascending=False)\n\n    return item_based[:10]","85ede939":"get_recommendations_hybrid_item_model(1)","77f35f38":"# Item-based-Recomendation","9b248b85":"# Hybrid System (Content,Model)","a7f82c4e":"# Content-based Recommendation","cc1740c0":"# Hybrid System (Item,Model)","84f3bff2":"# **Collabrative Model Filtering**","97cc38df":" ## Extracting the KeyWords From Bag Of Words using TD-IDF"}}