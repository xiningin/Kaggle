{"cell_type":{"56c1d782":"code","e0922f3e":"code","e3b885d8":"code","4410b44d":"code","d3be3829":"code","6aee0209":"code","c61fb20d":"code","3d1e42f0":"code","af590092":"code","6297926c":"code","ad2dc121":"code","fad5df34":"code","8738e9fa":"code","d6376468":"code","f060d856":"code","1ae4b3e9":"code","5c85f926":"code","07618287":"code","1fada172":"code","3a564678":"code","814a0429":"code","ca5f409e":"code","bbd6f513":"code","4f968eb5":"code","03018d15":"code","836c2b02":"code","ec765a6e":"code","2a9e857e":"code","414cbfb9":"code","acf16e4a":"code","68e34c46":"code","abe74c48":"markdown","ac006a9a":"markdown","ca12ead6":"markdown","ef0cc776":"markdown","35448440":"markdown","b42a687d":"markdown","60905829":"markdown","6b76882e":"markdown","cbfe2a61":"markdown","2d881fca":"markdown","cf50823a":"markdown","7dd18f5b":"markdown","2a48c98b":"markdown","e877cb28":"markdown","3741bce8":"markdown","9615a15e":"markdown","9a993426":"markdown","f1b04d93":"markdown"},"source":{"56c1d782":"import numpy as np\nimport pandas as pd\n\nfrom mlens.ensemble import SuperLearner\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import RidgeClassifier, Perceptron, PassiveAggressiveClassifier\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning","e0922f3e":"# To see optuna progress you need to comment this row\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)","e3b885d8":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","4410b44d":"train.head()","d3be3829":"for col in train.columns:\n    print(col, str(round(100* train[col].isnull().sum() \/ len(train), 2)) + '%')","6aee0209":"train['LastName'] = train['Name'].str.split(',', expand=True)[0]\ntest['LastName'] = test['Name'].str.split(',', expand=True)[0]\nds = pd.concat([train, test])\n\nsur = list()\ndied = list()\n\nfor index, row in ds.iterrows():\n    s = ds[(ds['LastName']==row['LastName']) & (ds['Survived']==1)]\n    d = ds[(ds['LastName']==row['LastName']) & (ds['Survived']==0)]\n    s=len(s)\n    if row['Survived'] == 1:\n        s-=1\n    d=len(d)\n    if row['Survived'] == 0:\n        d-=1\n    sur.append(s)\n    died.append(d)\n    \nds['FamilySurvived'] = sur\nds['FamilyDied'] = died\nds['FamilySize'] = ds['SibSp'] + ds['Parch'] + 1\nds['IsAlone'] = 0\nds.loc[ds['FamilySize'] == 1, 'IsAlone'] = 1\nds['Fare'] = ds['Fare'].fillna(train['Fare'].median())\nds['Embarked'] = ds['Embarked'].fillna('Q')\n\ntrain = ds[ds['Survived'].notnull()]\ntest = ds[ds['Survived'].isnull()]\ntest = test.drop(['Survived'], axis=1)\n\ntrain['rich_woman'] = 0\ntest['rich_woman'] = 0\ntrain['men_3'] = 0\ntest['men_3'] = 0\n\ntrain.loc[(train['Pclass']<=2) & (train['Sex']=='female'), 'rich_woman'] = 1\ntest.loc[(test['Pclass']<=2) & (test['Sex']=='female'), 'rich_woman'] = 1\ntrain.loc[(train['Pclass']==3) & (train['Sex']=='male'), 'men_3'] = 1\ntest.loc[(test['Pclass']==3) & (test['Sex']=='male'), 'men_3'] = 1\n\ntrain['rich_woman'] = train['rich_woman'].astype(np.int8)\ntest['rich_woman'] = test['rich_woman'].astype(np.int8)\n\ntrain[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train['Cabin']])\ntest['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in test['Cabin']])\n\nfor cat in ['Pclass', 'Sex', 'Embarked', 'Cabin']:\n    train = pd.concat([train, pd.get_dummies(train[cat], prefix=cat)], axis=1)\n    train = train.drop([cat], axis=1)\n    test = pd.concat([test, pd.get_dummies(test[cat], prefix=cat)], axis=1)\n    test = test.drop([cat], axis=1)\n    \ntrain = train.drop(['PassengerId', 'Ticket', 'LastName', 'SibSp', 'Parch', 'Sex_male', 'Name'], axis=1)\ntest =  test.drop(['PassengerId', 'Ticket', 'LastName', 'SibSp', 'Parch', 'Sex_male', 'Name'], axis=1)\n\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\n\ntrain.head()","c61fb20d":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"Age\", \n    points='all',\n    title='Age & Survived box plot',\n    width=700,\n    height=500\n)\n\nfig.show()","3d1e42f0":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"Fare\", \n    points='all',\n    title='Fare & Survived box plot',\n    width=700,\n    height=500\n)\n\nfig.show()","af590092":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"FamilySize\", \n    points='all',\n    title='Family Size & Survived box plot',\n    width=700,\n    height=500\n)\n\nfig.show()","6297926c":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"FamilyDied\", \n    points='all',\n    title='Family Died & Survived box plot',\n    width=700,\n    height=500\n)\n\nfig.show()","ad2dc121":"f = plt.figure(\n    figsize=(19, 15)\n)\n\nplt.matshow(\n    train.corr(), \n    fignum=f.number\n)\n\nplt.xticks(\n    range(train.shape[1]), \n    train.columns, \n    fontsize=14, \n    rotation=75\n)\n\nplt.yticks(\n    range(train.shape[1]), \n    train.columns, \n    fontsize=14\n)\n\ncb = plt.colorbar()\ncb.ax.tick_params(\n    labelsize=14\n)","fad5df34":"train.head()","8738e9fa":"y = train['Survived']\nX = train.drop(['Survived', 'Cabin_T'], axis=1)\nX_test = test.copy()\n\nX, X_val, y, y_val = train_test_split(X, y, random_state=0, test_size=0.2, shuffle=False)","d6376468":"class Optimizer:\n    def __init__(self, metric, trials=30):\n        self.metric = metric\n        self.trials = trials\n        self.sampler = TPESampler(seed=666)\n        \n    def objective(self, trial):\n        model = create_model(trial)\n        model.fit(X, y)\n        preds = model.predict(X_val)\n        if self.metric == 'acc':\n            return accuracy_score(y_val, preds)\n        else:\n            return f1_score(y_val, preds)\n            \n    def optimize(self):\n        study = optuna.create_study(direction=\"maximize\", sampler=self.sampler)\n        study.optimize(self.objective, n_trials=self.trials)\n        return study.best_params","f060d856":"rf = RandomForestClassifier(random_state=666)\nrf.fit(X, y)\npreds = rf.predict(X_val)\n\nprint('Random Forest accuracy: ', accuracy_score(y_val, preds))\nprint('Random Forest f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    model = RandomForestClassifier(\n        min_samples_leaf=min_samples_leaf, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nrf_f1_params = optimizer.optimize()\nrf_f1_params['random_state'] = 666\nrf_f1 = RandomForestClassifier(**rf_f1_params)\nrf_f1.fit(X, y)\npreds = rf_f1.predict(X_val)\n\nprint('Optimized on F1 score')\nprint('Optimized Random Forest: ', accuracy_score(y_val, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nrf_acc_params = optimizer.optimize()\nrf_acc_params['random_state'] = 666\nrf_acc = RandomForestClassifier(**rf_acc_params)\nrf_acc.fit(X, y)\npreds = rf_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized Random Forest: ', accuracy_score(y_val, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_val, preds))","1ae4b3e9":"xgb = XGBClassifier(random_state=666)\nxgb.fit(X, y)\npreds = xgb.predict(X_val)\n\nprint('XGBoost accuracy: ', accuracy_score(y_val, preds))\nprint('XGBoost f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    gamma = trial.suggest_uniform('gamma', 0.0000001, 1)\n    model = XGBClassifier(\n        learning_rate=learning_rate, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        gamma=gamma, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nxgb_f1_params = optimizer.optimize()\nxgb_f1_params['random_state'] = 666\nxgb_f1 = XGBClassifier(**xgb_f1_params)\nxgb_f1.fit(X, y)\npreds = xgb_f1.predict(X_val)\n\nprint('Optimized on F1 score')\nprint('Optimized XGBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized XGBoost f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nxgb_acc_params = optimizer.optimize()\nxgb_acc_params['random_state'] = 666\nxgb_acc = XGBClassifier(**xgb_acc_params)\nxgb_acc.fit(X, y)\npreds = xgb_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized XGBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized XGBoost f1-score: ', f1_score(y_val, preds))","5c85f926":"lgb = LGBMClassifier(random_state=666)\nlgb.fit(X, y)\npreds = lgb.predict(X_val)\n\nprint('LightGBM accuracy: ', accuracy_score(y_val, preds))\nprint('LightGBM f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    num_leaves = trial.suggest_int(\"num_leaves\", 2, 3000)\n    min_child_samples = trial.suggest_int('min_child_samples', 3, 200)\n    model = LGBMClassifier(\n        learning_rate=learning_rate, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        num_leaves=num_leaves, \n        min_child_samples=min_child_samples,\n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nlgb_f1_params = optimizer.optimize()\nlgb_f1_params['random_state'] = 666\nlgb_f1 = LGBMClassifier(**lgb_f1_params)\nlgb_f1.fit(X, y)\npreds = lgb_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized LightGBM accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized LightGBM f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nlgb_acc_params = optimizer.optimize()\nlgb_acc_params['random_state'] = 666\nlgb_acc = LGBMClassifier(**lgb_acc_params)\nlgb_acc.fit(X, y)\npreds = lgb_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized LightGBM accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized LightGBM f1-score: ', f1_score(y_val, preds))","07618287":"lr = LogisticRegression(random_state=666)\nlr.fit(X, y)\npreds = lr.predict(X_val)\n\nprint('Logistic Regression: ', accuracy_score(y_val, preds))\nprint('Logistic Regression f1-score: ', f1_score(y_val, preds))","1fada172":"dt = DecisionTreeClassifier(random_state=666)\ndt.fit(X, y)\npreds = dt.predict(X_val)\n\nprint('Decision Tree accuracy: ', accuracy_score(y_val, preds))\nprint('Decision Tree f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n    min_weight_fraction_leaf = trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n    model = DecisionTreeClassifier(\n        min_samples_split=min_samples_split, \n        min_weight_fraction_leaf=min_weight_fraction_leaf, \n        max_depth=max_depth, \n        min_samples_leaf=min_samples_leaf, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\ndt_f1_params = optimizer.optimize()\ndt_f1_params['random_state'] = 666\ndt_f1 = DecisionTreeClassifier(**dt_f1_params)\ndt_f1.fit(X, y)\npreds = dt_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized Decision Tree accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Decision Tree f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\ndt_acc_params = optimizer.optimize()\ndt_acc_params['random_state'] = 666\ndt_acc = DecisionTreeClassifier(**dt_acc_params)\ndt_acc.fit(X, y)\npreds = dt_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized Decision Tree accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Decision Tree f1-score: ', f1_score(y_val, preds))","3a564678":"bc = BaggingClassifier(random_state=666)\nbc.fit(X, y)\npreds = bc.predict(X_val)\n\nprint('Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Bagging Classifier f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int('n_estimators', 2, 200)\n    max_samples = trial.suggest_int('max_samples', 1, 100)\n    model = BaggingClassifier(\n        n_estimators=n_estimators, \n        max_samples=max_samples, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nbc_f1_params = optimizer.optimize()\nbc_f1_params['random_state'] = 666\nbc_f1 = BaggingClassifier(**bc_f1_params)\nbc_f1.fit(X, y)\npreds = bc_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Bagging Classifier f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nbc_acc_params = optimizer.optimize()\nbc_acc_params['random_state'] = 666\nbc_acc = BaggingClassifier(**bc_acc_params)\nbc_acc.fit(X, y)\npreds = bc_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Bagging Classifier f1-score: ', f1_score(y_val, preds))","814a0429":"knn = KNeighborsClassifier()\nknn.fit(X, y)\npreds = knn.predict(X_val)\n\nprint('KNN accuracy: ', accuracy_score(y_val, preds))\nprint('KNN f1-score: ', f1_score(y_val, preds))\n\nsampler = TPESampler(seed=0)\ndef create_model(trial):\n    n_neighbors = trial.suggest_int(\"n_neighbors\", 2, 25)\n    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n    return model\n\noptimizer = Optimizer('f1')\nknn_f1_params = optimizer.optimize()\nknn_f1 = KNeighborsClassifier(**knn_f1_params)\nknn_f1.fit(X, y)\npreds = knn_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized KNN accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized KNN f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nknn_acc_params = optimizer.optimize()\nknn_acc = KNeighborsClassifier(**knn_acc_params)\nknn_acc.fit(X, y)\npreds = knn_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized KNN accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized KNN f1-score: ', f1_score(y_val, preds))","ca5f409e":"abc = AdaBoostClassifier(random_state=666)\nabc.fit(X, y)\npreds = abc.predict(X_val)\n\nprint('AdaBoost accuracy: ', accuracy_score(y_val, preds))\nprint('AdaBoost f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0005, 1.0)\n    model = AdaBoostClassifier(\n        n_estimators=n_estimators, \n        learning_rate=learning_rate, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nabc_f1_params = optimizer.optimize()\nabc_f1_params['random_state'] = 666\nabc_f1 = AdaBoostClassifier(**abc_f1_params)\nabc_f1.fit(X, y)\npreds = abc_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized AdaBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized AdaBoost f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nabc_acc_params = optimizer.optimize()\nabc_acc_params['random_state'] = 666\nabc_acc = AdaBoostClassifier(**abc_acc_params)\nabc_acc.fit(X, y)\npreds = abc_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized AdaBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized AdaBoost f1-score: ', f1_score(y_val, preds))","bbd6f513":"et = ExtraTreesClassifier(random_state=666)\net.fit(X, y)\npreds = et.predict(X_val)\n\nprint('ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\nprint('ExtraTreesClassifier f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    model = ExtraTreesClassifier(\n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        random_state=0\n    )\n    return model\n\noptimizer = Optimizer('f1')\net_f1_params = optimizer.optimize()\net_f1_params['random_state'] = 666\net_f1 = ExtraTreesClassifier(**et_f1_params)\net_f1.fit(X, y)\npreds = et_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized ExtraTreesClassifier f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\net_acc_params = optimizer.optimize()\net_acc_params['random_state'] = 666\net_acc = ExtraTreesClassifier(**et_acc_params)\net_acc.fit(X, y)\npreds = et_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized ExtraTreesClassifier f1-score: ', f1_score(y_val, preds))","4f968eb5":"model = SuperLearner(\n    folds=5, \n    random_state=666\n)\n\nmodel.add(\n    [\n        bc, \n        lgb, \n        xgb, \n        rf, \n        dt, \n        knn\n    ]\n)\n\nmodel.add_meta(\n    LogisticRegression()\n)\n\nmodel.fit(X, y)\n\npreds = model.predict(X_val)\n\nprint('SuperLearner accuracy: ', accuracy_score(y_val, preds))\nprint('SuperLearner f1-score: ', f1_score(y_val, preds))","03018d15":"mdict = {\n    'RF': RandomForestClassifier(random_state=666),\n    'XGB': XGBClassifier(random_state=666),\n    'LGBM': LGBMClassifier(random_state=666),\n    'DT': DecisionTreeClassifier(random_state=666),\n    'KNN': KNeighborsClassifier(),\n    'BC': BaggingClassifier(random_state=666),\n    'OARF': RandomForestClassifier(**rf_acc_params),\n    'OFRF': RandomForestClassifier(**rf_f1_params),\n    'OAXGB': XGBClassifier(**xgb_acc_params),\n    'OFXGB': XGBClassifier(**xgb_f1_params),\n    'OALGBM': LGBMClassifier(**lgb_acc_params),\n    'OFLGBM': LGBMClassifier(**lgb_f1_params),\n    'OADT': DecisionTreeClassifier(**dt_acc_params),\n    'OFDT': DecisionTreeClassifier(**dt_f1_params),\n    'OAKNN': KNeighborsClassifier(**knn_acc_params),\n    'OFKNN': KNeighborsClassifier(**knn_f1_params),\n    'OABC': BaggingClassifier(**bc_acc_params),\n    'OFBC': BaggingClassifier(**bc_f1_params),\n    'OAABC': AdaBoostClassifier(**abc_acc_params),\n    'OFABC': AdaBoostClassifier(**abc_f1_params),\n    'OAET': ExtraTreesClassifier(**et_acc_params),\n    'OFET': ExtraTreesClassifier(**et_f1_params),\n    'LR': LogisticRegression(random_state=666),\n    'ABC': AdaBoostClassifier(random_state=666),\n    'SGD': SGDClassifier(random_state=666), \n    'ET': ExtraTreesClassifier(random_state=666),\n    'MLP': MLPClassifier(random_state=666),\n    'GB': GradientBoostingClassifier(random_state=666),\n    'RDG': RidgeClassifier(random_state=666),\n    'PCP': Perceptron(random_state=666),\n    'PAC': PassiveAggressiveClassifier(random_state=666)\n}","836c2b02":"def create_model(trial):\n    model_names = list()\n    models_list = [\n        'RF', 'XGB', 'LGBM', 'DT', \n        'KNN', 'BC', 'OARF', 'OFRF', \n        'OAXGB', 'OFXGB', 'OALGBM', \n        'OFLGBM', 'OADT', 'OFDT', \n        'OAKNN', 'OFKNN', 'OABC', \n        'OFBC', 'OAABC', 'OFABC', \n        'OAET', 'OFET', 'LR', \n        'ABC', 'SGD', 'ET', \n        'MLP', 'GB', 'RDG', \n        'PCP', 'PAC'\n    ]\n    \n    head_list = [\n        'RF', 'XGB', 'LGBM', 'DT', \n        'KNN', 'BC', 'LR', 'ABC', \n        'SGD', 'ET', 'MLP', 'GB', \n        'RDG', 'PCP', 'PAC'\n    ]\n    n_models = trial.suggest_int(\"n_models\", 2, 6)\n    for i in range(n_models):\n        model_item = trial.suggest_categorical('model_{}'.format(i), models_list)\n        if model_item not in model_names:\n            model_names.append(model_item)\n    \n    folds = trial.suggest_int(\"folds\", 2, 6)\n    \n    model = SuperLearner(\n        folds=folds, \n        random_state=666\n    )\n    \n    models = [\n        mdict[item] for item in model_names\n    ]\n    model.add(models)\n    head = trial.suggest_categorical('head', head_list)\n    model.add_meta(\n        mdict[head]\n    )\n        \n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X, y)\n    preds = model.predict(X_val)\n    score = accuracy_score(y_val, preds)\n    return score\n\nstudy = optuna.create_study(\n    direction=\"maximize\", \n    sampler=sampler\n)\nstudy.optimize(\n    objective, \n    n_trials=50\n)","ec765a6e":"params = study.best_params\n\nhead = params['head']\nfolds = params['folds']\ndel params['head'], params['n_models'], params['folds']\nresult = list()\nfor key, value in params.items():\n    if value not in result:\n        result.append(value)\n        \nresult","2a9e857e":"model = SuperLearner(\n    folds=folds, \n    random_state=666\n)\n\nmodels = [\n    mdict[item] for item in result\n]\nmodel.add(models)\nmodel.add_meta(mdict[head])\n\nmodel.fit(X, y)\n\npreds = model.predict(X_val)\n\nprint('Optimized SuperLearner accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized SuperLearner f1-score: ', f1_score(y_val, preds))","414cbfb9":"preds = model.predict(X_test)\npreds = preds.astype(np.int16)","acf16e4a":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = preds\nsubmission.to_csv('submission.csv', index=False)","68e34c46":"submission.head()","abe74c48":"<a id=\"2\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>2. Single models training and optimization<center><h2>","ac006a9a":"<a id=\"4\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>4. Final submission<center><h2>","ca12ead6":"Lets create some separate single models and check accuracy score. We also try to optimize every single model using optuna framework. As we can see we can get some better results with it.","ef0cc776":"#### In this notebook I will not focus on preprocessing and feature engineering steps, just show how to build your efficient ensemble in few lines of code. I use almost the same features as in the most of kernels in current competition.","35448440":"We can see from training set that almost all people with Age higher than 63 years didn't survive. Can use these information in modeling post processing.","b42a687d":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:Black; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h3>\n\n* [1. Feature engineering](#1)\n* [2. Single models training and optimization](#2)\n* [3. SuperLearner training and optimization](#3)\n* [4. Final submission](#4)\n","60905829":"We are going to use our single models in the first layer and LogisticRegressor as metalearner.","6b76882e":"Let's do some visualization.","cbfe2a61":"Here is some basic preprocessing to get fast training and test datasets.","2d881fca":"Let's optimize SuperLearner","cf50823a":"Another one thing. People with family size more than 7 didn't survive.","7dd18f5b":"Lets create train and test dataset and create holdout set for validation.","2a48c98b":"<a id=\"3\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>3. SuperLearner training and optimization<center><h2>","e877cb28":"Now we will create ensemble model named SuperLearner from mlens package. For details check https:\/\/machinelearningmastery.com\/super-learner-ensemble-in-python\/","3741bce8":"<h1><center>Titanic Classification<\/center><\/h1>\n\n<center><img src=\"https:\/\/asset.kompas.com\/crops\/6ZgUuMr3vydyyEwFHPKZ3Fzx4B8=\/15x3:786x517\/750x500\/data\/photo\/2017\/04\/15\/4106667456.jpg\"><\/center>","9615a15e":"<a id=\"1\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>1. Feature engineering<center><h2>","9a993426":"As we can see we improved our best single score only in a few lines of code. Feel free to add new features and try different models inside superlearner.","f1b04d93":"Lets see percent of NaNs for every column in training set"}}