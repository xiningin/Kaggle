{"cell_type":{"05de25fc":"code","3216c8f5":"code","251a50f4":"code","ac4a2ba6":"code","ba0aeb5a":"code","087b09f9":"code","50a585d8":"code","27034bef":"code","b30e0adf":"code","78f80eec":"code","ddb85534":"code","0cb2bd68":"code","56492f2c":"code","707bfbbc":"code","9a94d325":"code","3845df03":"code","654cdb1a":"code","945cb68a":"code","239abeeb":"code","efc7fe02":"code","fe6e8e91":"code","988a96d0":"code","02ae97c0":"code","60383c67":"code","9bcfe4d1":"code","fe925eea":"code","db62b6ac":"code","9d572ab4":"code","1f67c9d8":"code","9cc16fdb":"code","540f9485":"markdown","a65286fa":"markdown","ee4d5d02":"markdown","a73d75b9":"markdown","cc86a9ac":"markdown","52bbccdd":"markdown","6bfd305e":"markdown","5fe11ac9":"markdown","33be7d25":"markdown","f6e9cade":"markdown","70819ec0":"markdown","74dc67bc":"markdown","de93dc66":"markdown","9b7979db":"markdown","0f3a502c":"markdown"},"source":{"05de25fc":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport nltk\nimport nltk.sentiment\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, cohen_kappa_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, GroupShuffleSplit\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.svm import LinearSVC\n","3216c8f5":"train = pd.read_csv('..\/input\/train.tsv', sep='\\t')\ntrain.head()","251a50f4":"sns.countplot(x='Sentiment', data=train)","ac4a2ba6":"train['word_count'] = train['Phrase'].apply(lambda x: len(x.split()))\nsns.boxplot(x='Sentiment', y='word_count', data=train)","ba0aeb5a":"train['avg_word_lenght'] = train['Phrase'].apply(lambda x:np.mean([len(word) for word in x.split()]))\nsns.boxplot(x='Sentiment', y='avg_word_lenght', data=train)","087b09f9":"group_split = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\ntrain_index, validation_index = list(group_split.split(train['PhraseId'],\n                                    y=train['Sentiment'], groups=train['SentenceId']))[0]","50a585d8":"train, validation = train.iloc[train_index], train.iloc[validation_index]","27034bef":"log_reg_countvec_pl = Pipeline([\n        ('vectorizer', CountVectorizer(analyzer='word',ngram_range=[1,3], stop_words=nltk.corpus.stopwords.words('english'))),\n        ('clf',LogisticRegression())\n])","b30e0adf":"log_reg_countvec_pl.fit(train['Phrase'], train['Sentiment'])\naccuracy_score(validation['Sentiment'], log_reg_countvec_pl.predict(validation['Phrase']))","78f80eec":"#concating tran and validation to calculate the features only once\ndata = pd.concat([train, validation], keys=['train', 'validation'])","ddb85534":"#tokenizing the phrases to use it as input to calculate another features\ndata['tokenized_words'] = data['Phrase'].apply(nltk.word_tokenize)\n# mark words after negation word with _NEG tag\ndata['negated_phrase_tokenized'] = data['tokenized_words'].apply(nltk.sentiment.util.mark_negation)\ndata['negated_phrase'] = data['negated_phrase_tokenized'].apply(lambda x: \" \".join(x))\n#returns 1 if the text contains negation word\ndata['negated_flag'] = (data['tokenized_words'].apply(nltk.sentiment.vader.negated)).astype('int8')","0cb2bd68":"# get_numeric_data = preprocessing.FunctionTransformer(lambda a: a[['negated_flag']], validate=False)\nget_text_data = preprocessing.FunctionTransformer(lambda a: a['negated_phrase'], validate=False)","56492f2c":"lg_pl = Pipeline([\n        ('union', FeatureUnion( #unites both text and numeric arrays into one array\n            transformer_list = [\n                ('text_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', CountVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n             ]\n        )), \n        ('clf',LogisticRegression(penalty ='l1'))\n    ])","707bfbbc":"lg_pl.fit(data.loc['train'], data.loc['train']['Sentiment'])\naccuracy_score(data.loc['validation']['Sentiment'], lg_pl.predict(data.loc['validation']))","9a94d325":"lg_pl = Pipeline([\n        ('union', FeatureUnion( #unites both text and numeric arrays into one array\n            transformer_list = [\n                ('text_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', CountVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n                ('tfidf', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n                ('char_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(analyzer='char',ngram_range=[3,5], \\\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n             ]\n        )), \n        ('clf',LogisticRegression(penalty ='l1'))\n])","3845df03":"lg_pl.fit(data.loc['train'], data.loc['train']['Sentiment'])\naccuracy_score(data.loc['validation']['Sentiment'], lg_pl.predict(data.loc['validation']))","654cdb1a":"train_data = data.loc['train']\ntrain_index, validation_index = list(group_split.split(train_data['PhraseId'],\n                                    y=train_data['Sentiment'], groups=train_data['SentenceId']))[0]\ntrain, validation_stack = train_data.iloc[train_index], train_data.iloc[validation_index]","945cb68a":"nb_pl = Pipeline([\n        ('union', FeatureUnion( #unites both text and numeric arrays into one array\n            transformer_list = [\n                ('text_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', CountVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n                ('tfidf', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n                ('char_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(analyzer='char',ngram_range=[3,5], \\\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n             ]\n        )), \n        ('clf',MultinomialNB())\n    ])","239abeeb":"nb_pl.fit(train, train['Sentiment'])","efc7fe02":"svc_pl = Pipeline([\n        ('union', FeatureUnion( #unites both text and numeric arrays into one array\n            transformer_list = [\n                ('text_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', CountVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n                ('tfidf', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n                ('char_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(analyzer='char',ngram_range=[3,5], \\\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n             ]\n        )), \n        ('clf',LinearSVC())\n])","fe6e8e91":"svc_pl.fit(train, train['Sentiment'])","988a96d0":"lg_stack = LogisticRegression()","02ae97c0":"lg_stack.fit(\n    np.column_stack(\n        (\n            nb_pl.predict_proba(validation_stack),\n            lg_pl.predict_proba(validation_stack),\n            svc_pl.decision_function(validation_stack)\n        )\n    )\n    ,validation_stack['Sentiment']\n)","60383c67":"accuracy_score(data.loc['validation']['Sentiment'], lg_stack.predict(\n    np.column_stack(\n        (\n            nb_pl.predict_proba(data.loc['validation']),\n            lg_pl.predict_proba(data.loc['validation']),\n            svc_pl.decision_function(data.loc['validation'])\n\n            \n        )\n    )\n))","9bcfe4d1":"test = pd.read_csv('..\/input\/test.tsv', sep='\\t')","fe925eea":"#tokenizing the phrases to use it as input to calculate another features\ntest['tokenized_words'] = test['Phrase'].apply(nltk.word_tokenize)\n# mark words after negation word with _NEG tag\ntest['negated_phrase_tokenized'] = test['tokenized_words'].apply(nltk.sentiment.util.mark_negation)\ntest['negated_phrase'] = test['negated_phrase_tokenized'].apply(lambda x: \" \".join(x))\n#returns 1 if the text contains negation word\ntest['negated_flag'] = (test['tokenized_words'].apply(nltk.sentiment.vader.negated)).astype('int8')","db62b6ac":"lg_pl.fit(data.loc['train'], data.loc['train']['Sentiment'])\nnb_pl.fit(data.loc['train'], data.loc['train']['Sentiment'])\nsvc_pl.fit(data.loc['train'], data.loc['train']['Sentiment'])","9d572ab4":"lg_stack.fit(\n    np.column_stack(\n        (\n            nb_pl.predict_proba(data.loc['validation']),\n            lg_pl.predict_proba(data.loc['validation']),\n            svc_pl.decision_function(data.loc['validation'])\n        )\n    )\n    ,data.loc['validation']['Sentiment']\n)","1f67c9d8":"test['Sentiment'] = lg_stack.predict(\n                        np.column_stack(\n                            (\n                                nb_pl.predict_proba(test),\n                                lg_pl.predict_proba(test),\n                                svc_pl.decision_function(test)\n                            )\n                        )\n                    )","9cc16fdb":"test[['PhraseId', \"Sentiment\"]].to_csv('test_predictions.csv', index=False)","540f9485":"## Base Model","a65286fa":"** Average Word Length versus Sentiment **","ee4d5d02":"**Logistic Regression Model**","a73d75b9":"* **using count features**","cc86a9ac":"** The Target Class Distribution **\n","52bbccdd":"## EDA","6bfd305e":" ** Adding negation Tag to the words **","5fe11ac9":"\n**Splitting data into train, validation sets** using GroupShuffleSplit using sentieceId to split on it  instead of  train_test_split as the sntence is split into different phrases and phrases sentiment may vary so I think it is better to to evaluate the model using complete sentences in the train or the validation.","33be7d25":"**Combinng the propabilities of the three models**","f6e9cade":"** Logistic Regression with Negation Marked Text **","70819ec0":"** Retraining with the complete train data **","74dc67bc":"## Stacking Different Models","de93dc66":"## Test Predictions ","9b7979db":"**  Word Count against Sentiment **","0f3a502c":"** Adding Character N-Gram **"}}