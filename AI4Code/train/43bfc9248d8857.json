{"cell_type":{"25c3f7ef":"code","d0b33a57":"code","083ba9d4":"code","cf7ee8ce":"code","1a7f0570":"code","3036cf7f":"code","fbf9962f":"code","2c715295":"code","aca893d4":"code","c3532ed4":"code","65a5c123":"code","150881c6":"code","30bec56a":"code","9d618a64":"code","d26040d9":"code","90b7c271":"code","0df40f29":"code","8926ea7a":"code","8283bb72":"code","8d0e7f25":"code","12f837a2":"code","8afbae77":"code","5658aa32":"code","fe462cc9":"code","2bcbb198":"code","9ff4af16":"code","06b966de":"code","65b5b32d":"code","1edf8638":"code","338d560f":"code","07495686":"code","cb764324":"code","2bd3e6a2":"code","55f0d742":"code","6c8fbc4e":"code","6e6214dc":"code","419a21b1":"code","9af9c9ed":"code","31050130":"code","85820fc3":"code","399702ee":"code","398771ff":"code","53093495":"code","97c93ae6":"code","ce982514":"code","419034cf":"code","0ee7edbf":"code","d5f70967":"code","f5901d9a":"code","a2a789f8":"code","2b189666":"code","73e2b8d9":"code","0c40a83e":"code","3230fa93":"code","da40d94d":"code","7db58485":"code","c7317fa5":"code","4926e832":"code","735de549":"code","c7aad0eb":"code","84a7d587":"markdown","654d111e":"markdown","def9b3f0":"markdown","ffa52b2d":"markdown","a602b70d":"markdown","c452e15a":"markdown","ddd9807c":"markdown","c1dda1bd":"markdown","4e2e045e":"markdown","53ed084a":"markdown","024ce0f7":"markdown","c7a5a7a2":"markdown","60ff54c8":"markdown","094f0bc3":"markdown","9cf389f9":"markdown","ae09f7b8":"markdown","47a48e0b":"markdown","f89da71c":"markdown","26660992":"markdown","f717452d":"markdown","4b1451a2":"markdown"},"source":{"25c3f7ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d0b33a57":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nfrom tqdm import tqdm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport sqlite3\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report\nfrom prettytable import PrettyTable","083ba9d4":"df = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndf.head()","cf7ee8ce":"sns.countplot(df['is_sarcastic'])","1a7f0570":"df.isna().sum()","3036cf7f":"\ndef cleaner(phrase):\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can't\", 'can not', phrase)\n  \n  # general\n    phrase = re.sub(r\"n\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'re'\",\" are\", phrase)\n    phrase = re.sub(r\"\\'s\",\" is\", phrase)\n    phrase = re.sub(r\"\\'ll\",\" will\", phrase)\n    phrase = re.sub(r\"\\'d\",\" would\", phrase)\n    phrase = re.sub(r\"\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'ve\",\" have\", phrase)\n    phrase = re.sub(r\"\\'m\",\" am\", phrase)\n    \n    return phrase","fbf9962f":"from bs4 import BeautifulSoup\nfrom tqdm import tqdm\nimport re\n\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords","2c715295":"stop = set(stopwords.words('english'))\nlen(stop)","aca893d4":"cleaned_title = []\n\nfor sentance in tqdm(df['headline'].values):\n    sentance = str(sentance)\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = cleaner(sentance)\n    sentance = re.sub(r'[?|!|\\'|\"|#|+]', r'', sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stop)\n    cleaned_title.append(sentance.strip())","c3532ed4":"df['headline'] = cleaned_title","65a5c123":"from wordcloud import WordCloud,STOPWORDS","150881c6":"plt.figure(figsize = (20,20)) # Text that is Not Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 0].headline))\nplt.imshow(wc , interpolation = 'bilinear')","30bec56a":"plt.figure(figsize = (20,20)) # Text that is Not Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 1].headline))\nplt.imshow(wc , interpolation = 'bilinear')","9d618a64":"import plotly.offline as pyoff\nimport plotly.graph_objs as go\nfrom nltk.util import ngrams\nimport re\nimport unicodedata\nimport nltk","d26040d9":"def basic_clean(text):\n  \"\"\"\n  A simple function to clean up the data. All the words that\n  are not designated as a stop word is then lemmatized after\n  encoding and basic regex parsing are performed.\n  \"\"\"\n  wnl = nltk.stem.WordNetLemmatizer()\n  stopwords = nltk.corpus.stopwords.words('english')\n  text = (unicodedata.normalize('NFKD', text)\n    .encode('ascii', 'ignore')\n    .decode('utf-8', 'ignore')\n    .lower())\n  words = re.sub(r'[^\\w\\s]', '', text).split()\n  return [wnl.lemmatize(word) for word in words if word not in stopwords]","90b7c271":"sarcastic_words = basic_clean(''.join(str(df[df['is_sarcastic']==1]['headline'].tolist())))","0df40f29":"bigram_sarcastic=(pd.Series(nltk.ngrams(sarcastic_words, 2)).value_counts())[:30]\n\nbigram_sarcastic=pd.DataFrame(bigram_sarcastic)","8926ea7a":"bigram_sarcastic['idx']=bigram_sarcastic.index","8283bb72":"bigram_sarcastic['idx'] = bigram_sarcastic.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+')',axis=1)","8d0e7f25":"plot_data = [\n    go.Bar(\n        x=bigram_sarcastic['idx'],\n        y=bigram_sarcastic[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'Red'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 bi-grams from Sarcastic News',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","12f837a2":"trigram_sarcastic=(pd.Series(nltk.ngrams(sarcastic_words, 3)).value_counts())[:30]\n\ntrigram_sarcastic=pd.DataFrame(trigram_sarcastic)\n\ntrigram_sarcastic","8afbae77":"trigram_sarcastic['idx']=trigram_sarcastic.index\n\ntrigram_sarcastic['idx'] = trigram_sarcastic.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+', ' + x['idx'][2]+')',axis=1)","5658aa32":"plot_data = [\n    go.Bar(\n        x=trigram_sarcastic['idx'],\n        y=trigram_sarcastic[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'Red'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 tri-grams from Sarcastic News',\n        yaxis_title='Count',\n        xaxis_title='tri-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","fe462cc9":"not_sarcasm = basic_clean(''.join(str(df[df['is_sarcastic']==0]['headline'].tolist())))","2bcbb198":"bigram_notsarcastic=(pd.Series(nltk.ngrams(not_sarcasm, 2)).value_counts())[:30]\n\nbigram_notsarcastic=pd.DataFrame(bigram_notsarcastic)\n\nbigram_notsarcastic","9ff4af16":"bigram_notsarcastic['idx']=bigram_notsarcastic.index\n\nbigram_notsarcastic['idx'] = bigram_notsarcastic.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+')',axis=1)","06b966de":"plot_data = [\n    go.Bar(\n        x=bigram_notsarcastic['idx'],\n        y=bigram_notsarcastic[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'Green'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 bi-grams from Not Sarcasm News',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","65b5b32d":"trigram_notsarcastic=(pd.Series(nltk.ngrams(not_sarcasm, 3)).value_counts())[:30]\n\ntrigram_notsarcastic=pd.DataFrame(trigram_notsarcastic)\n\ntrigram_notsarcastic['idx']=trigram_notsarcastic.index\n\ntrigram_notsarcastic['idx'] = trigram_notsarcastic.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+', ' + x['idx'][2]+')',axis=1)","1edf8638":"plot_data = [\n    go.Bar(\n        x=trigram_notsarcastic['idx'],\n        y=trigram_notsarcastic[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'Green'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 tri-grams from Non Sarcastic News',\n        yaxis_title='Count',\n        xaxis_title='tri-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","338d560f":"import gensim","07495686":"X = []\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nfor par in df['headline'].values:\n    tmp = []\n    sentences = nltk.sent_tokenize(par)\n    for sent in sentences:\n        sent = sent.lower()\n        tokens = tokenizer.tokenize(sent)\n        filtered_words = [w.strip() for w in tokens if w not in stop and len(w) > 1]\n        tmp.extend(filtered_words)\n    X.append(tmp)","cb764324":"w2v_model = gensim.models.Word2Vec(sentences=X, size=150, window=5, min_count=2)","2bd3e6a2":"w2v_model.wv.most_similar(positive = 'trump')","55f0d742":"w2v_model.wv.most_similar(positive = 'research')","6c8fbc4e":"w2v_model.wv.most_similar(positive = 'study')","6e6214dc":"w2v_model.wv.similarity('trump', 'fake')","419a21b1":"w2v_model.wv.doesnt_match(['trump', 'hillary', 'pence'])","9af9c9ed":"w2v_model.wv.most_similar(positive = ['research','study'], negative=['harvard'], topn=3)","31050130":"X = df['headline']\ny = df['is_sarcastic']","85820fc3":"from sklearn.model_selection import train_test_split\nX_Train, X_test, y_Train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","399702ee":"X_train, X_cross, y_train, y_cross = train_test_split(X_Train, y_Train, test_size=0.1, random_state=42)","398771ff":"tf_idf=TfidfVectorizer(min_df=4,use_idf=True,ngram_range=(1,3))\ntf_idf.fit(X_train)\nTrain_TFIDF = tf_idf.transform(X_train)\nCrossVal_TFIDF = tf_idf.transform(X_cross)\nTest_TFIDF= tf_idf.transform(X_test)","53093495":"from sklearn.naive_bayes import MultinomialNB","97c93ae6":"alpha_set=[0.0001,0.001,0.01,0.1,1,10,100,1000]\n\nTrain_AUC_TFIDF = []\nCrossVal_AUC_TFIDF = []\n\n\nfor i in alpha_set:\n    naive_b=MultinomialNB(alpha=i)\n    naive_b.fit(Train_TFIDF, y_train)\n    Train_y_pred =  naive_b.predict(Train_TFIDF)\n    Train_AUC_TFIDF.append(roc_auc_score(y_train,Train_y_pred))\n    CrossVal_y_pred =  naive_b.predict(CrossVal_TFIDF)\n    CrossVal_AUC_TFIDF.append(roc_auc_score(y_cross,CrossVal_y_pred))","ce982514":"Alpha_set=[]\nfor i in range(len(alpha_set)):\n    Alpha_set.append(np.math.log(alpha_set[i]))","419034cf":"plt.plot(Alpha_set, Train_AUC_TFIDF, label='Train AUC')\nplt.scatter(Alpha_set, Train_AUC_TFIDF)\nplt.plot(Alpha_set, CrossVal_AUC_TFIDF, label='CrossVal AUC')\nplt.scatter(Alpha_set, CrossVal_AUC_TFIDF)\nplt.legend()\nplt.xlabel(\"alpha : hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()","0ee7edbf":"optimal_alpha=alpha_set[CrossVal_AUC_TFIDF.index(max(CrossVal_AUC_TFIDF))]\nprint(optimal_alpha)","d5f70967":"Classifier2 = MultinomialNB(alpha=optimal_alpha)\nClassifier2.fit(Train_TFIDF, y_train)","f5901d9a":"from sklearn.metrics import accuracy_score\nprint (\"Accuracy on Train is: \", accuracy_score(y_train,Classifier2.predict(Train_TFIDF)))\n\nprint (\"Accuracy on Test is: \", accuracy_score(y_test,Classifier2.predict(Test_TFIDF)))","a2a789f8":"from sklearn import metrics\nprint(metrics.classification_report(y_test,Classifier2.predict(Test_TFIDF)))","2b189666":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(Classifier2, Test_TFIDF, y_test ,display_labels=['0','1'],cmap=\"Blues\",values_format = '')","73e2b8d9":"tf_idf=TfidfVectorizer(min_df=4,use_idf=True,ngram_range=(1,2))\ntf_idf.fit(X_train)\nTrain_TFIDF = tf_idf.transform(X_train)\nCrossVal_TFIDF = tf_idf.transform(X_cross)\nTest_TFIDF= tf_idf.transform(X_test)","0c40a83e":"from sklearn.linear_model import LogisticRegression","3230fa93":"c=[0.0001,0.001,0.01,0.1,1,10,100,1000]\nTrain_AUC_TFIDF = []\nCrossVal_AUC_TFIDF = []\nfor i in c:\n    logreg = LogisticRegression(C=i,penalty='l2')\n    logreg.fit(Train_TFIDF, y_train)\n    Train_y_pred =  logreg.predict(Train_TFIDF)\n    Train_AUC_TFIDF.append(roc_auc_score(y_train ,Train_y_pred))\n    CrossVal_y_pred =  logreg.predict(CrossVal_TFIDF)\n    CrossVal_AUC_TFIDF.append(roc_auc_score(y_cross,CrossVal_y_pred))","da40d94d":"C=[]\nfor i in range(len(c)):\n    C.append(np.math.log(c[i]))","7db58485":"plt.plot(C, Train_AUC_TFIDF, label='Train AUC')\nplt.scatter(C, Train_AUC_TFIDF)\nplt.plot(C, CrossVal_AUC_TFIDF, label='CrossVal AUC')\nplt.scatter(C, CrossVal_AUC_TFIDF)\nplt.legend()\nplt.xlabel(\"lambda : hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()","c7317fa5":"optimal_inverse_lambda=c[CrossVal_AUC_TFIDF.index(max(CrossVal_AUC_TFIDF))]\nprint(pow(optimal_inverse_lambda,-1))","4926e832":"Classifier=LogisticRegression(C=optimal_inverse_lambda,penalty='l2')\nClassifier.fit(Train_TFIDF, y_train)","735de549":"print (\"Accuracy on Train is: \", accuracy_score(y_train,Classifier.predict(Train_TFIDF)))\nprint (\"Accuracy on Test is: \", accuracy_score(y_test,Classifier.predict(Test_TFIDF)))","c7aad0eb":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(Classifier, Test_TFIDF, y_test ,display_labels=['0','1'],cmap=\"Blues\",values_format = '')","84a7d587":"Both the plots look similar till lambda = 0, after which the the model starts overfitting to the train data.","654d111e":"**Please upvote the notebook if you liked. Thanks for the time**","def9b3f0":"![](https:\/\/quicksilver.scoopwhoop.com\/unsafe\/960x500\/center\/https:\/\/s3.scoopwhoop.com\/anj\/chb\/267436620.jpg)","ffa52b2d":"**The dataset consists about 28000 text data points where each data category belongs to 2 category - Sarcastic or Not Sarcastic**\n\n**We shall also have deep dive into basic EDA using bi-grams and tri-grams. \nWe shall also build a Word2Vec model for inferences**\n\n**We will use two models for making predictions - Logistic Regression and Naive Bayes. We will then compare their results and see which performs better.**","a602b70d":"**This accuracy is pretty good for a model built on simple ML methods**","c452e15a":"Naive observations","ddd9807c":"**Logistic Regression**","c1dda1bd":"Model Building","4e2e045e":"**Finding Bi-grams and Tri-grams for Sarcastic headlines**","53ed084a":"**SOME BASIC EDA**","024ce0f7":"Word 2 Vec","c7a5a7a2":"'Research' is to what as 'study' is to 'harvard'","60ff54c8":"Kudos - **https:\/\/www.kaggle.com\/sreshta140\/is-it-authentic-or-not\/notebook**","094f0bc3":"![](https:\/\/images-na.ssl-images-amazon.com\/images\/I\/81vF8+5-AcL.png)","9cf389f9":"No surprise","ae09f7b8":"**Bi-grams and Tri-grams for non-sarcastic headlines**","47a48e0b":"No clear signs of overfitting observed","f89da71c":"**No surprise ** 2**","26660992":"**Data Cleaning**","f717452d":"**Model Building**","4b1451a2":"First we tokenize the words of the entire corpus. "}}