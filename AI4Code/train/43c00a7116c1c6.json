{"cell_type":{"a207b4c5":"code","f0242889":"code","1cc41e0c":"code","53f01c3c":"code","85847507":"code","dc4270c0":"code","f61222df":"code","39d68947":"code","9b6a7763":"code","9fc93228":"code","d91db648":"code","be1beb3f":"markdown","db2862c6":"markdown","ef0894ec":"markdown","2b696118":"markdown","b098cb54":"markdown","b64a3850":"markdown","02d1eed2":"markdown","754ef5e5":"markdown","8c2a24ee":"markdown","b88d9356":"markdown","51b9f3e4":"markdown","db4fa268":"markdown","b30f69bb":"markdown","18ec411c":"markdown","368aaeaf":"markdown"},"source":{"a207b4c5":"import tensorflow as tf\nimport numpy as np\nimport os\nimport pandas as pd","f0242889":"path = \"..\/input\/anime-names-and-image-generation\/final_names.csv\"\n\nnames_df = pd.read_csv(path)\nnames_ls = list(names_df[\"0\"])\nprint(\"Number of names: \",len(names_ls))\n\nnames_df.head()","1cc41e0c":"text = \"\"\nfor name in names_ls:\n    text+=name\n    \nunique_chars = list(set(text))\nnum_chars = len(unique_chars)\nprint(\"Number of unique characters: \",num_chars)\n\nchar_to_id = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=unique_chars)\nid_to_char = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=char_to_id.get_vocabulary(), invert=True)\n\nchar_ls = tf.strings.unicode_split(names_ls,input_encoding=\"UTF-8\")\nname_byte_ls = list(tf.strings.reduce_join(char_ls, axis=-1))\nid_ls = list(char_to_id(char_ls))\nall_ids = char_to_id(tf.strings.unicode_split(text, 'UTF-8'))\n\nprint(\"Example name: \",name_byte_ls[0])\nprint(\"Split: \",char_ls[0])\nprint(\"It's encoding: \",id_ls[0])\n\nprint(\"Number of chars in all names: \",tf.shape(all_ids)[0])\n\nlen(char_to_id.get_vocabulary())","53f01c3c":"padded_id = tf.keras.preprocessing.sequence.pad_sequences(id_ls,padding = \"post\")\nprint(\"Padded ids shape: \",padded_id.shape)","85847507":"seq_length = padded_id.shape[1]\nBATCH_SIZE = 64\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nSHUFFLE = 60000\nids_dataset = tf.data.Dataset.from_tensor_slices(padded_id)\n\ndef io_split(id_arr):\n    inp = id_arr[:-1]\n    out = id_arr[1:]\n    return inp,out\nids_dataset = ids_dataset.map(io_split)\nids_dataset = ids_dataset.shuffle(SHUFFLE).batch(BATCH_SIZE,drop_remainder=True).prefetch(AUTOTUNE)\n\nids_dataset","dc4270c0":"print(char_to_id.get_vocabulary())","f61222df":"rnn_units = 1024\nembedding_dim = 256\n\nclass GenerateModel(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, rnn_units):\n        super().__init__(self)\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(rnn_units,\n                                   return_sequences=True, \n                                   return_state=True)\n        self.dense = tf.keras.layers.Dense(vocab_size)\n\n    def call(self, inputs, states=None, return_state=False, training=False):\n        x = inputs\n        x = self.embedding(x, training=training)\n        if states is None:\n            states = self.gru.get_initial_state(x)\n        x, states = self.gru(x, initial_state=states, training=training)\n        x = self.dense(x, training=training)\n\n        if return_state:\n            return x, states\n        else: \n            return x\n        \nmodel = GenerateModel(vocab_size=len(char_to_id.get_vocabulary()),embedding_dim=embedding_dim,rnn_units=rnn_units)\n\n\nfor input_batch, target_batch in ids_dataset.take(1):\n    batch_pred = model(input_batch)\n    print(batch_pred.shape)\n    \nprint(\"MODEL SUMMARY\")\nmodel.summary()","39d68947":"loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer='adam', loss=loss)\n\ncheckpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","9b6a7763":"EPOCHS = 10\nhistory = model.fit(ids_dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","9fc93228":"class OutputStep(tf.keras.Model):\n    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n        super().__init__()\n        self.temperature=temperature\n        self.model = model\n        self.id_to_char = id_to_char\n        self.char_to_id = char_to_id\n\n    \n        skip_ids = self.char_to_id(['','?',\"#\",\"-\",'[UNK]'])[:, None]\n        sparse_mask = tf.SparseTensor(\n            values=[-float('inf')]*len(skip_ids),\n            indices = skip_ids,\n            dense_shape=[len(char_to_id.get_vocabulary())]) \n        sparse_mask = tf.sparse.reorder(sparse_mask)\n        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n\n    @tf.function\n    def generate_one_step(self, inputs, states=None):\n        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n        input_ids = self.char_to_id(input_chars).to_tensor()\n\n    \n        predicted_logits, states =  self.model(inputs=input_ids, states=states, \n                                          return_state=True)\n   \n        predicted_logits = predicted_logits[:, -1, :]\n        predicted_logits = predicted_logits\/self.temperature\n  \n        predicted_logits = predicted_logits + self.prediction_mask\n\n    \n        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n\n\n        predicted_chars = self.id_to_char(predicted_ids)\n\n        return predicted_chars, states\n    \noutput_model = OutputStep(model, id_to_char, char_to_id)","d91db648":"req_len = 15\nstart_letter = \"A\"\n\nstates = None\nnext_char = tf.constant([start_letter])\nresult = [next_char]\n\nfor n in range(req_len):\n    next_char, states = output_model.generate_one_step(next_char, states=states)\n    result.append(next_char)\n\nresult = tf.strings.join(result)\nprint(result[0].numpy().decode('utf-8'))","be1beb3f":"# Preprocessing Data","db2862c6":"# Preparing Model ","ef0894ec":"# GENERATING NAME","2b696118":"# TRAINING","b098cb54":"### Below `OutputStep()` Model will generate a letter in each call.\n* I wll append the generated letter to a empty string till required length of name is reached","b64a3850":"## How is the generated Anime Name\ud83d\ude01.\n### Hope you got some valuable learning from this notebook.\n### Happy Coding\u2764.","02d1eed2":"# Introduction","754ef5e5":"1. Give `req_len` and `start_letter` as input and get final anime name as output","8c2a24ee":"# Final Anime Name","b88d9356":"### In this following notebook I will walk you through steps needed to generate name from given dataset. \n#### I used [Anime names and Images](https:\/\/www.kaggle.com\/shanmukh05\/anime-names-and-image-generation) dataset for this task.\n*   This is basically a RNN network, that takes input a list of encoded words and outputs a letter.We can get any length of Anime name you want.\n\n## **Let's dive in**","51b9f3e4":"[](http:\/\/)","db4fa268":"# Data Pipeline","b30f69bb":"### Padding sequences","18ec411c":"**In this below cell, I have done two things**\n* Converting every anime name into id's\n* Converting the id's back to name\nAbove two steps are done using `tf.keras.layers.experimental.preprocessing.StringLookup`","368aaeaf":"# Loading Data"}}