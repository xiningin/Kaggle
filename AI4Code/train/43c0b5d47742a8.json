{"cell_type":{"2123a7c9":"code","9ae3760a":"code","31d339d0":"code","cdea0172":"code","12aaf0ae":"code","de798181":"code","e8880f1f":"code","303be3ce":"code","d92cdb57":"code","dca65acc":"code","1e511b65":"code","66381474":"code","2fcedd56":"code","f823d1c3":"code","b0256f84":"code","03b1ffee":"code","9c55100c":"code","5fefcf8b":"code","571e0e27":"code","1068646a":"code","15fdf994":"code","b77781e7":"code","7e329859":"code","14df4998":"code","e3ae3097":"code","4a229eb3":"code","20347ddf":"code","3eaa8fcb":"code","45961b93":"code","a8815842":"code","eef6284e":"code","e50d2d66":"code","30b90c32":"code","81b3696b":"code","a76fc98a":"code","366c809f":"code","8c99e91b":"code","1b00d252":"code","3d070ae6":"code","f536d332":"code","45bedd83":"code","ff7d0641":"code","d1b47c5f":"code","e4d1ce92":"code","9706cf08":"code","4f724d17":"markdown","fb4030dd":"markdown","bed7df8b":"markdown","00f909fd":"markdown","06825f3a":"markdown","67632299":"markdown","ec0ec2b9":"markdown","35bd2ae0":"markdown","625ec269":"markdown","5431bfbf":"markdown","1c50fb14":"markdown","fc766a07":"markdown","361b67ea":"markdown","e0e98150":"markdown","141f20c7":"markdown","c149d0b9":"markdown","214720c1":"markdown","969cc7d7":"markdown","7aa830f8":"markdown","3e67084b":"markdown"},"source":{"2123a7c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ae3760a":"# Importing the required libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","31d339d0":"# Importing the csv file\ndf = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","cdea0172":"# Checking the first five rows of the dataset\ndf.head()","12aaf0ae":"# Checking the shape of the dataset\ndf.shape","de798181":"# Checking the basic overview of the dataset\ndf.info()","e8880f1f":"# Checking for null values in the dataset\ndf.isnull().sum()","303be3ce":"# Checking if there are null values in the form of question mark\ndf[df=='?'].count()","d92cdb57":"# The categorical columns in the dataset, except the target variable are these\ndf_cat = df[['sex','exng','caa','cp','fbs','restecg','slp','thall']]","dca65acc":"# Changing the datatypes of the categorical variables which are coded\ndf['sex'] = df['sex'].astype('object')\ndf['exng']=df['exng'].astype('object')\ndf['caa']=df['caa'].astype('object')\ndf['cp'] = df['cp'].astype('object')\ndf['fbs']= df['fbs'].astype('object')\ndf['restecg'] = df['restecg'].astype('object')\ndf['slp'] = df['slp'].astype('object')\ndf['thall'] = df['thall'].astype('object')\ndf['output'] = df['output'].astype('object')","1e511b65":"# The numerical columns in the dataset are these\ndf_num = df[['age','trtbps','chol','thalachh','oldpeak']]","66381474":"# Checking for the percentage of output column\nplt.rcParams['figure.figsize'] = 8,4\nfig,ax = plt.subplots(1,2)\ndf['output'].value_counts().plot(kind='bar',rot=0, cmap='summer',ax=ax[0])\ndf['output'].value_counts().plot(kind='pie',cmap='icefire',ax=ax[1],autopct='%.1f%%')\nplt.show()","2fcedd56":"# Five point summary of numerical columns\ndf_num.describe()","f823d1c3":"# Checking the distribution of the numerical columns\nplt.rcParams['figure.figsize']= 16,6\ni=1\nfor col in df_num:\n    fig,ax= plt.subplots(1,3)\n    sns.boxplot(df_num[col],ax=ax[0],palette='Greens_r')\n    sns.distplot(df_num[col],ax=ax[1],color='green')\n    sns.violinplot(df_num[col],ax=ax[2],palette='Greens_r')\n    i+=1\n    plt.show()","b0256f84":"# Univariate analysis of categorical features\nplt.rcParams['figure.figsize']= 12,6\nfor col in df_cat:\n    print('\\n',col,':')\n    fig,ax = plt.subplots(1,2)\n    df_cat[col].value_counts().plot(kind='bar',ax=ax[0],rot=0, cmap='Set2')\n    df_cat[col].value_counts().plot(kind='pie',autopct='%.1f%%',ax=ax[1], cmap='Set3')\n    plt.show()\nplt.show()","03b1ffee":"# Numerical columns with output\nfor col in df_num:\n    fig,ax= plt.subplots(1,2)\n    sns.boxplot(df['output'], df_num[col],ax=ax[0],palette='spring')\n    sns.violinplot(df['output'], df_num[col],ax=ax[1],palette='spring')\n    plt.show()","9c55100c":"# Categorical columns with Output\nplt.rcParams['figure.figsize']= 10,4\nfor col in df_cat:\n    print(col,'Vs','output:')\n    fig,ax = plt.subplots(1,2)\n    sns.countplot(df_cat[col],hue= df['output'],ax=ax[0],palette='autumn')\n    ax_1= pd.crosstab(df_cat[col],df['output']).apply(lambda r: r\/r.sum()*100, axis=1).plot(kind='bar',stacked=True,ax=ax[1],rot=0,cmap='summer')\n    for rec in ax_1.patches:\n        height = rec.get_height()\n        ax_1.text(rec.get_x() + rec.get_width() \/ 2, \n              rec.get_y() + height \/ 2,\n              \"{:.0f}%\".format(height),\n              ha='center', \n              va='bottom')\n    plt.show()","5fefcf8b":"# Correlation plot\nsns.heatmap(df.corr(),annot=True, cmap='coolwarm')\nplt.show()","571e0e27":"from scipy.stats import chi2_contingency\nfor col in df_cat:\n    print(col,'Vs','Output')\n    print(chi2_contingency(pd.crosstab(df_cat[col],df['output'])))\n    print('\\n')","1068646a":"df = df.drop('fbs',axis=1)","15fdf994":"# For the numerical features, we first perform the shapiro test\nimport scipy.stats as st\nfrom scipy.stats import shapiro\nfor i in df_num:\n    print(i,'Vs','output')\n    st1= df[df['output']==0][i]\n    st2= df[df['output']==1][i]\n    print(shapiro(st1))\n    print(shapiro(st2))\n    print('\\n')","b77781e7":"from scipy.stats import mannwhitneyu\nfor i in df_num:\n    z=df[df['output']==1][i]\n    w=df[df['output']==0][i]\n    print('ManwhitneyU test for %s with admit pvalue is:'%i,mannwhitneyu(z,w)[1])\n    print('\\n')","7e329859":"# Transformation\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\nfor i in df.select_dtypes(np.number):\n    df[i]= pt.fit_transform(df[[i]])","14df4998":"for i in df.columns:\n    df[i]=df[i].astype('int64')","e3ae3097":"X = df.drop('output',axis=1)\ny = df['output']","4a229eb3":"from sklearn.model_selection import train_test_split\nxtrain , xtest , ytrain , ytest = train_test_split(X,y,test_size=0.3,random_state=42)","20347ddf":"xtrain.shape , xtest.shape , ytrain.shape , ytest.shape","3eaa8fcb":"# Scaling the train and test set seperately\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nfor i in xtrain.columns:\n    xtrain[i] = sc.fit_transform(xtrain[[i]])\nfor i in xtest.columns:\n    xtest[i] = sc.fit_transform(xtest[[i]])","45961b93":"from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve, recall_score, f1_score, accuracy_score, precision_score","a8815842":"def models(model,xtrain,xtest,ytrain,ytest):\n    md = model.fit(xtrain,ytrain)\n    ypred = md.predict(xtest)\n    yprob = md.predict_proba(xtest)[:,1]\n    print(classification_report(ytest,ypred))\n    print('Train Accuracy:', md.score(xtrain,ytrain),'\\nTest Accuracy',md.score(xtest,ytest))\n    print('AUC Score:', roc_auc_score(ytest,yprob))\n    print('Confusion Matrix:\\n',confusion_matrix(ytest,ypred))","eef6284e":"# Logistic Regression Model\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='sag')\nmodels(lr,xtrain,xtest,ytrain,ytest)","e50d2d66":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n    {'penalty' : ['l1', 'l2'],\n    'C' : np.logspace(-4, 4, 20),\n    'solver' : ['liblinear']}]\ngs= GridSearchCV(LogisticRegression(),\n                      param_grid = param_grid,\n                      cv=3,\n                      n_jobs=-1,\n                      verbose=3)\ngs.fit(xtrain,ytrain)\nprint('Best parameters for Decision Tree Classifier: ', gs.best_params_, '\\n')","30b90c32":"lr_t = LogisticRegression(C=0.0001,penalty='l2',solver='liblinear')\nmodels(lr_t,xtrain,xtest,ytrain,ytest)","81b3696b":"# KNeighbors Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nmodels(knn,xtrain,xtest,ytrain,ytest)","a76fc98a":"from sklearn.model_selection import GridSearchCV\ng= {'n_neighbors' : range(2,20)}\ngs= GridSearchCV(KNeighborsClassifier(),\n                      param_grid =g,\n                      cv=3,\n                      n_jobs=-1,\n                      verbose=3)\ngs.fit(xtrain,ytrain)\nprint('Best parameters for Decision Tree Classifier: ', gs.best_params_, '\\n')","366c809f":"knn_t = KNeighborsClassifier(n_neighbors=19)\nmodels(knn_t,xtrain,xtest,ytrain,ytest)","8c99e91b":"# Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\nmodels(dt,xtrain,xtest,ytrain,ytest)","1b00d252":"# Finding best parameters from randomized search cv to avoid overfitting\nfrom sklearn.model_selection import GridSearchCV\ng= {'criterion': ['entropy', 'gini'],\n                     'max_depth': range(2, 20),\n                     'min_samples_split': range(2,10)}\ngs= GridSearchCV(DecisionTreeClassifier(),\n                      param_grid =g,\n                      cv=3,\n                      n_jobs=-1,\n                      verbose=3)\ngs.fit(xtrain,ytrain)\nprint('Best parameters for Decision Tree Classifier: ', gs.best_params_, '\\n')","3d070ae6":"dt_t = DecisionTreeClassifier(min_samples_split=2, max_depth=3,criterion='entropy')\nmodels(dt_t,xtrain,xtest,ytrain,ytest)","f536d332":"# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nmodels(rf,xtrain,xtest,ytrain,ytest)","45bedd83":"# Ada Boost Classifier\nfrom sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier()\nmodels(adb,xtrain,xtest,ytrain,ytest)","ff7d0641":"final = pd.DataFrame({'Models':['Logistic Regression','KNeighborsClassifier' ,'AdaBoostClassifier','DecisionTreeClassifier']\n                     ,'Accuracy':[accuracy_score(ytest,lr_t.predict(xtest)),accuracy_score(ytest,knn_t.predict(xtest))\n                                 ,accuracy_score(ytest,adb.predict(xtest)),accuracy_score(ytest,dt_t.predict(xtest))]\n                     ,'AUC Score':[roc_auc_score(ytest,lr_t.predict_proba(xtest)[:,1]),roc_auc_score(ytest,\n                                                                                                    knn_t.predict_proba(xtest)[:,1])\n                                  , roc_auc_score(ytest,adb.predict_proba(xtest)[:,1]), roc_auc_score(ytest,dt_t.predict_proba(xtest)[:,1])]\n                     ,'Recall Score':[recall_score(ytest,lr_t.predict(xtest)), recall_score(ytest,knn_t.predict(xtest))\n                                     ,recall_score(ytest,adb.predict(xtest)), recall_score(ytest,dt_t.predict(xtest))]\n                      ,'Precision Score':[precision_score(ytest,lr_t.predict(xtest)), precision_score(ytest,knn_t.predict(xtest))\n                                     ,precision_score(ytest,adb.predict(xtest)), precision_score(ytest,dt_t.predict(xtest))]\n                     ,'F1 Score':[f1_score(ytest,lr_t.predict(xtest)), f1_score(ytest,knn_t.predict(xtest))\n                                     ,f1_score(ytest,adb.predict(xtest)),precision_score(ytest,dt_t.predict(xtest))]\n                     })\nfinal","d1b47c5f":"feat_imp = pd.DataFrame(columns={'Features':X.columns,'Importance': dt_t.feature_importances_})","e4d1ce92":"important_features = pd.DataFrame({'Features': xtrain.columns, \n                                   'Importance': adb.feature_importances_})\n\n# print the dataframe\nimportant_features.sort_values(by='Importance', ascending=False, inplace=True)\nimportant_features","9706cf08":"sns.barplot(x = 'Importance', y = 'Features', data = important_features)\nplt.title('Feature Importance', fontsize = 15)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\nplt.show()","4f724d17":"We can see from the feature importance graph that, the caa feature is having more influence in the output column followed by cp and slp, sex and thalachh features. Even old peak,thall, age,restecg, exng seems to have some effect on Output. The trtbps and chol are not influencing the output feature much. \n\nWe saw from bivariate that when the number of major vessels(caa value) is too small and when too large it was having more chances of getting heart attack. Also the chest pain type is having high influence on Output, so whether the person will get heart attack or not is mainly determined by the number of major vessels and the type of chest pain that person gets.\n\nThe slp value is also having very high influence and an interesting feature is the sex feature, where we saw from bivariate analysis that the gender type 1 is specifically having more chances of getting heart attack.\n\nThe normal heart rate is also influencing very much, we saw that when that person is having higher heart rate then that person has more chances of getting heart attack and we also saw that when the heart rate is very less, then that person does not have a chance of getting heart attack at all.\n\nSo, any health industry should focus on these 4 features to determine if the person will get heart attack or not.","fb4030dd":"#### Bivariate Analysis","bed7df8b":"We can see that, age is varying between 29 and 77. The mean and median are moreover equal, so age is not skewed.\n\nThe bp level varies between 94 and 200. We can observe that few people have very high bp, but the mean is normal.\n\nWe can see that cholestrol varies between 126 and 564. The mean and median are different\nSo, we can tell that chol distribution is skewed. And also the standard deviation value is very high,\nso there can be outliers in this column.\n\nThe thalachh column is varying between 71 to 202. We can observe that, few people are \nhaving very high heart beat rates, which could be risky. The mean and median are also very much different, so the column is skewed.\nThe mean is little higher than usual.\n\nOld peak is between 0 and 6.2 , there is not much info given about this feature.\nBut we can tell from mean and median that , it is skewed.\n\n","00f909fd":"In the sex column, we do not know which is Male and which is female\nWe have data more about gender type 1 compared to gender type 2\nGender type 1 is more than double gender type 2\n\nIn the exercise induced angina column, we can see that only very less percentage of people\n32 % of them has got pain in the chest due to some physical activity\n\nIn the caa column, we can see 0th value of caa is taking more than 50%\n\nIn the cp column, we see that around 50% percentage of people have typical angina\n\nThe fasting blood sugar column says, around 85% of people's blood sugar is normal\nand only 15% of people have fasting blood sugar higher than normal value\n\nFrom restecg, we see that, around 50% of the people's ecg is normal\n\nIn slp column , the values 1 and 2 are contributing more\n\nThe thallium stress test tells that the value 2 is more compared to other values","06825f3a":"### Hypothesis testings","67632299":"Normality test\n\nNull hypothesis: Data is not skewed; skewness=0\n\nAlternate hypothesis: Data is skewed; skewness!=0","ec0ec2b9":"We can see that there are no question marks as well.","35bd2ae0":"So, fbs column does not have an effect on the output column, so we can drop it.","625ec269":"There is no multicollinearity between the features.\n","5431bfbf":"We can see in the Age column that, there are more number of people in the age group of\n50-60.\n\nWe can see in the trtbps column that, most people's bp is in the range between 120 and 140\nwhich is the normal range, but there are also plenty of people with high bp value\n\nMost of the people have cholestrol in the range 200-300 which is little high than normal cholestrol value\nBut since most of the data is about 50-70 age people, this is the trend we are getting\n\nThe thalachh column is having a peak between the range 150-175\nwhich is high for people of the age group 50-70\n\nPeople with value 0 of old peak is more compared to other values.\n\nWe can see that, the numerical columns trtbps, chol, thalachh and oldpeak has outliers. It will not be wise to drop the outliers so we can transform them.","1c50fb14":"We can see that there is no imbalance in our dataset.\n\nSo, we could see from our target variable that , there are more number of people, that is around 54% of people have high chances of having a heart attack.\nLet's see how it is getting affected based on the other features.","fc766a07":"When comparing age with the output we can see an anomaly that\nPeople between age 50-65 have less chances of getting a heart attack\nWhereas, for having chances of getting heart attack, it is uniformly distributed\nfor all the ages between 40-70.\n\nWith trtbps with output, we can see that people with high bp have less\nchances of getting heart attack, whereas, those with bp between 120 and\n140 are having higher chances.\n\nFrom cholestrol with output distribution we can see that, the people\nwith high cholestrol, that is more than 400 are surely having chances \nof getting heart attack\n\nWhen comparing thalachh with output we can see that, people with less heart rates\nhave very less chances of getting heart attack, and similarly the people with higher\nheart rates are having more chances of getting heart attack\n\nFrom old peak with output we can see that, the people with old peak value 0 are\nhaving more chances of getting heart attack, also we can see that, as old peak value\nincreases, the chance of getting heart attack is reduced.","361b67ea":"### My first dataset in kaggle","e0e98150":"### Model Building","141f20c7":"When comparing sex with the output, we can see that, the gender 0 is having \nvery high chance of getting heart attack, while the gender 1 is having around 50% \nchance of getting the heart attack\n\nComparing exng with output, we can see an anomaly that, of the people who does\nnot get pain due to physical activity are having more chance of getting the heart attack\n\nThe caa with output is also having an anomaly, when the number of vessels is very less\nthat is when 0, the chances of getting heart attack are around 75% and also\nwhen the number of major vessels is 4, there is 80% chance for getting heart attack\nThe intermediate number of vessels are having less percentage of people getting heart attack\n\nWe can see from cp that, the atypical angina is having more than 80% of people getting heart attack\nFollowed by people with non-anginal and asymptomatic, where both of them are having\nmore than 70% chance of getting heart attack\n\nThe fasting blood sugar is not seeming to have an effect on output, as \npeople with blood sugar and also people without blood sugar is also having\nalmost equal chances of getting heart attack\n\nWhen comparing restecg with output, we can see that, people with normal ecg \nis also having around 46% chances of getting heart attack. Also, when there is abnormality\nin the restecg, there is around 63% chance of getting heart attack.\n\nWhile comparing slp with output, we can see that, the slp value 2 is having very high\nchance of getting heart attack, the other two slope values too have some significant\nchance of getting heart attack\n\nFrom thallium stress test result we can see that, the value 2 is having 78% chance\nof getting heart attack and also value 0 is having 50% chance, the other two values\nare having less chances.","c149d0b9":"So, other than fbs all the other columns has an effect on the target variable. So, we can build the model.","214720c1":"**Univariate Analysis**","969cc7d7":"#### Categorical Features\nNull hypothesis: The feature does not have effect on Output\n\nAlternate hypothesis: The feature has an effect on Output","7aa830f8":"We can see that, there are no null values in our dataset","3e67084b":"Numerical Columns\n\nNull hypothesis: The feature does not have an effect on Output\n\nAlternate hypothesis: The feature has an effect on Output"}}