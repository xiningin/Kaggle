{"cell_type":{"9380c01b":"code","62c4e878":"code","b9503b69":"code","1ecec833":"code","929b39dc":"code","33efda16":"code","be40c547":"code","b8cd61db":"code","e45f17b4":"code","94d1947c":"code","10816f43":"code","c8082e82":"code","b4b1647e":"markdown","8a4a425a":"markdown","5b22aa48":"markdown","30327f23":"markdown","ff0a67dd":"markdown","1de0cb93":"markdown","7c555d98":"markdown","503ea5dd":"markdown","7fb14265":"markdown","c6d7f619":"markdown","d5c1a480":"markdown","0539c1e6":"markdown","39b614c4":"markdown"},"source":{"9380c01b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical # convert to one-hot-encoding\nfrom keras.preprocessing.image import ImageDataGenerator","62c4e878":"# init random seed\nnp.random.seed(1)\n\n# Load the data from csv to dataframe\nX_raw = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\nX_test_raw = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\ny = X_raw[\"label\"] #ground truth\nX = X_raw.drop(labels = [\"label\"],axis = 1) \n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=0)\n\nWIDTH=28\nHEIGHT=28\nNUM_CLASSES=10\n\nX_train = X_train.values\ny_train = y_train.values\n\nX_valid = X_valid.values\ny_valid = y_valid.values\n\n# create one-hot vector of the ground truth\ny_train_oh = to_categorical(y_train, num_classes = NUM_CLASSES)\ny_valid_oh = to_categorical(y_valid, num_classes = NUM_CLASSES)\n\nX_test = X_test_raw.values","b9503b69":"plt.subplot(1,2,1)\nsns.countplot(y_train).set_title('y_train')\nplt.subplot(1,2,2)\nsns.countplot(y_valid).set_title('y_valid')\nprint('Missing value X_raw: ' + str(X_raw.isnull().values.any()))","1ecec833":"import math\ndef showImg(X,y,n_row=4, n_col=4):\n    plt.figure()\n    for i in range(n_row*n_col):\n        plt.subplot(n_row,n_col,i+1)\n        plt.tight_layout()\n        plt.imshow(X[i].reshape(WIDTH,HEIGHT),cmap='gray')\n        plt.title(y[i])\n        plt.axis('off')\n        \nshowImg(X_train,y_train,4,4)","929b39dc":"X_train.max()","33efda16":"def scaleData(X):\n    n_max = X_train.max()\n    X = X\/n_max\n    return X  \ndef reshape_channel(X):\n    return np.expand_dims(X.reshape(-1,HEIGHT,WIDTH),-1)\n\ndef preprocessData(X):\n    X = scaleData(X)\n    X = reshape_channel(X)\n    return X","be40c547":"from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\noptimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.9999, amsgrad=True)\n\nmodel = Sequential()\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'same',activation ='relu',use_bias=True,input_shape = (HEIGHT,WIDTH,1)))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'same',activation ='relu',use_bias=True))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'same',activation ='relu',use_bias=True))\nmodel.add(MaxPool2D(pool_size=(3,3), strides=(3,3)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'same',activation ='relu',use_bias=True))\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'same',activation ='relu',use_bias=True))\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'same',activation ='relu',use_bias=True))\nmodel.add(MaxPool2D(pool_size=(5,5), strides=(5,5)))\nmodel.add(Flatten())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation = \"relu\"))\nmodel.add(Dense(64, activation = \"relu\"))\nmodel.add(Dense(NUM_CLASSES, activation = \"softmax\"))\n# model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.compile(optimizer ='adam', loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()","b8cd61db":"import math\nfrom keras.callbacks import LearningRateScheduler\n\n\nn_epoch=50\ndrop=0.5\nepoch_drop = 8\ninitial_lrate = 0.002\nmin_lrate = 0.00005\n\ndef step_decay(epoch):\n    lrate = initial_lrate*math.pow(drop,math.floor((1+epoch)\/epoch_drop))\n    if(lrate<min_lrate):\n        lrate=min_lrate\n    \n    print('learning rate at epoch ' +str(epoch+1) +': ' + str(lrate))\n    return lrate\n\nlrate = LearningRateScheduler(step_decay)","e45f17b4":"from keras.preprocessing.image import ImageDataGenerator\n# create data generator\ndatagen = ImageDataGenerator(\n    rotation_range=10,\n    zoom_range = 0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n)\n# create iterator\n\nit_train = datagen.flow(preprocessData(X_train), y_train_oh)\nit_valid = datagen.flow(preprocessData(X_valid), y_valid_oh)","94d1947c":"hist = model.fit_generator(it_train,validation_data=it_valid,callbacks=[lrate],epochs=n_epoch)\n# hist = model.fit(preprocessData(X_train),y_train_oh,validation_data=(preprocessData(X_valid),y_valid_oh),callbacks=[lrate],epochs=n_epoch)\n# hist = model.fit(preprocessData(X_train),y_train_oh,validation_data=(preprocessData(X_valid),y_valid_oh),epochs=50)","10816f43":"y_pred = model.predict(preprocessData(X_test))\ny_pred = np.argmax(y_pred,axis = 1)\n\n# X_test_raw\nshowImg(X_test,y_pred,4,4)\nsubmission = pd.DataFrame({'ImageId':range(1,28001),'Label':y_pred})\nsubmission.to_csv('submission.csv',index=False)","c8082e82":"print(hist.history.keys())\nfig,(ax1,ax2,ax3) = plt.subplots(3,1,figsize=(15,30))\n\nax1.plot(np.arange(1,len(hist.history['accuracy'])+1,1),hist.history['accuracy'],label='train accuracy')\nax1.plot(np.arange(1,len(hist.history['val_accuracy'])+1,1),hist.history['val_accuracy'],label='validation accuracy')\nax1.grid()\nax1.legend()\n\nax2.plot(np.arange(1,len(hist.history['loss'])+1,1),hist.history['loss'],label='train loss')\nax2.plot(np.arange(1,len(hist.history['val_loss'])+1,1),hist.history['val_loss'],label='validation loss')\nax2.grid()\nax2.legend()\n\nax3.plot(np.arange(1,len(hist.history['lr'])+1,1),hist.history['lr'],label='learning rate')\nax3.grid()\nax3.legend()","b4b1647e":"## 5.1. Define \"learning rate scheduler\" to optimize learning rate base on epoch\n![](https:\/\/miro.medium.com\/max\/334\/1*iYWyu8hemMyaBlK6V-2vqg.png)\nIn training deep networks, it is helpful to reduce the learning rate as the number of training epochs increases. This is based on the intuition that with a high learning rate, the deep learning model would possess high kinetic energy. As a result, it\u2019s parameter vector bounces around chaotically. Thus, it\u2019s unable to settle down into deeper and narrower parts of the loss function (local minima). If the learning rate, on the other hand, was very small, the system then would have low kinetic energy. Thus, it would settle down into shallow and narrower parts of the loss function (false minima)\n\nThe above figure depicts that a high learning rate will lead to random to and fro moment of the vector around local minima while a slow learning rate results in getting stuck into false minima. Thus, knowing when to decay the learning rate can be hard to find out.\n\nWe base our experiment on the principle of step decay. Here, we reduce the learning rate by a constant factor every few epochs. Typical values might be reducing the learning rate by half every 5 epochs, or by 0.1 every 20 epochs. These numbers depend heavily on the type of problem and the model. One heuristic you may see in practice is to watch the validation error while training with a fixed learning rate, and reduce the learning rate by a constant (e.g. 0.5) whenever the validation error stops improving.\n\nIn practice, step decay is preferred as it\u2019s easier to interpret hyperparameters like fraction of decay and the step timings in units of epochs. Also, it\u2019s found to provide stabilization to the value of learning rate which in turn helps the stochastic gradient descent to exhibit fast convergence and a high rate of success.","8a4a425a":"# 7. Visualize learning\n- Visualize the learning history","5b22aa48":"# 4. Define model (CNN)\n- Use Conv2D (3x3) and (5x5) to extract feature.\n- Include bias and padding in the convolution.\n- Add drop out in the middle to avoid overfitting.","30327f23":"## 2.3 Find out max value of all pixel","ff0a67dd":"## 2.2 Draw image of train data and it's ground truth","1de0cb93":"## 2.1 Review some statistic on training data","7c555d98":"## 5.3. Fit the model","503ea5dd":"# 3. Preprocess data\n- Scaling data\n- Reshape data to NUM_SAMPLE x WIDTH x HEIGHT","7fb14265":"# 2. Explore data\nReview training data to findout what we need to preprocess on this dataset","c6d7f619":"## 5.2. Augmetation","d5c1a480":"# 5. Train model","0539c1e6":"# 1. Load data\n- Load data from csv to dataframe and transform it to numpy array\n- Calculate onehot vector of y (ground truth)","39b614c4":"# 6. Predict data and create submission\n- Predict test data and generate submission to submit to the leaderboard.\n- Show test image and it's prediction to have a quick check of accuracy."}}