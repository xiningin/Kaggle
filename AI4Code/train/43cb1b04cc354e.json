{"cell_type":{"272dc538":"code","233d7782":"code","e03acb4a":"code","1dbd9a73":"code","33b276f6":"code","0f49c34a":"code","d4f2b4cc":"code","da0859bf":"code","8778d90b":"code","2db0071c":"code","9ee29c54":"code","7d634393":"code","cce30faa":"code","c562c6c9":"code","78bb09c0":"code","5a98fd7a":"code","19f14f08":"code","d5b6147a":"code","39d8e7da":"code","e2dc12b0":"code","95f2e1b0":"code","42251be0":"code","1527e0e9":"code","f719004f":"code","1fc9fce8":"code","b22e0b15":"code","4944b369":"code","f3e502da":"code","59205375":"code","a2e45ccd":"code","129bbb04":"code","d3f3369d":"code","09d21644":"code","01b4b642":"code","eba3514e":"code","56201f67":"code","4e281adc":"code","bc77c6bc":"code","9a975d83":"code","8b5ee3ea":"markdown","f5a5d75c":"markdown","59073227":"markdown","908cf2ab":"markdown","ade6d6c7":"markdown","226f872e":"markdown","acb5a3ef":"markdown","2d6d9eb6":"markdown","1ab13a28":"markdown","75323fcd":"markdown","516b4de3":"markdown","596b5027":"markdown","e4264fc8":"markdown","319d4b5f":"markdown","7fa013eb":"markdown","1d56193d":"markdown","45900f47":"markdown","88e14fce":"markdown","19313759":"markdown","453d7d65":"markdown","2651ed1a":"markdown","0e82496e":"markdown","daaf5eea":"markdown","1bdc17da":"markdown","52036ac9":"markdown","c05edcc5":"markdown","e9ce5507":"markdown","4e7f1b15":"markdown","66cc229f":"markdown","99017284":"markdown","e6bbfed0":"markdown","9a8b4068":"markdown","27259711":"markdown","da829689":"markdown","22cce117":"markdown","8bce76a3":"markdown","9a029fd2":"markdown","c76b9427":"markdown","155f3f57":"markdown","8aeaf2a8":"markdown","ecca3cd4":"markdown","fd4be285":"markdown","58373675":"markdown","50089d1a":"markdown","c24082e9":"markdown","c7821d9e":"markdown","cf1dc7a5":"markdown","0d87a11d":"markdown","7ca7027c":"markdown","e1762e4b":"markdown"},"source":{"272dc538":"pip install feature_engine","233d7782":"# Analysis Tools\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import norm\n\n# Plotting Design Settings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \"#E2E2E2\", 'grid.color': '#B0B0B0', 'patch.edgecolor': '#B0B0B0', 'font.sans-serif': 'Verdana'})\nsns.set_palette('twilight')\n\n# Feature Engineering\nfrom feature_engine.encoding import OneHotEncoder as fe_OneHotEncoder, OrdinalEncoder\nfrom feature_engine.outliers import Winsorizer\nfrom feature_engine.selection import DropConstantFeatures\nfrom collections import defaultdict\nfrom sklearn.model_selection import cross_val_score, cross_validate, train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Modeling\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n\n# Scoring\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\n\n\n# Mute warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","e03acb4a":"data = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndata.shape","1dbd9a73":"data.head()","33b276f6":"data.describe()","0f49c34a":"# What type of variables are in this dataset\ndata.dtypes","d4f2b4cc":"# Number of unique values in each variable\nfor var in data:\n    print(var, ': ', data[var].nunique())","da0859bf":"print('# of Missing Values:  {}'.format(data.isnull().sum().sum()))","8778d90b":"print('# of Duplicates: {}'.format(len(data[data.duplicated()])))","2db0071c":"# Note that im making 'FastingBS' and the target 'HeartDisease' categorical for now,\n# this will make it easier for analysing them as there both low cardinality\ndata['FastingBS'] = data['FastingBS'].astype(str)\ndata['HeartDisease'] = data['HeartDisease'].astype(str)\n\n\ncategorical = data.select_dtypes(include=object).columns\n\nnumerical = data.select_dtypes(exclude=object).columns","9ee29c54":"# Plotting Categoricals\nwith plt.rc_context(rc = {'axes.labelsize': 13, 'ytick.labelsize': 11}):\n    fig, ax = plt.subplots(2, 3, figsize = (13, 7))\n\n    for idx, (var, axes) in list(enumerate(zip(categorical, ax.flatten()))):\n        sns.countplot(ax=axes, x=data[var])\n    ### Removing empty figures\n    else:\n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n\n    plt.tight_layout()\n    plt.show()\n    \n    \n    \n\n# Plotting Numericals\nfor var in numerical:\n    plt.figure(figsize=(16,7))\n    plt.xticks(rotation='90')\n    \n    sns.countplot(x=data[var], palette=\"twilight\")\n    \n    plt.show()\n    \n    \n    # Find a way to reduce number of ticks on x-axis!!!","7d634393":"# Visualizing relationship with target variable\nwith plt.rc_context(rc = {'axes.labelsize': 13, 'ytick.labelsize': 11}):\n    fig, ax = plt.subplots(2, 3, figsize = (13, 7))\n\n    for idx, (var, axes) in list(enumerate(zip(categorical, ax.flatten()))):\n        sns.histplot(ax=axes, data=data, x=data[var], hue='HeartDisease', multiple='dodge', shrink=0.8)\n    ### Removing empty figures\n    else:\n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n\n    plt.tight_layout()\n    plt.show()","cce30faa":"# Visualizing relationship with target variable\nwith plt.rc_context(rc = {'axes.labelsize': 13, 'ytick.labelsize': 11}):\n    fig, ax = plt.subplots(2, 3, figsize = (13, 7))\n\n    for idx, (var, axes) in list(enumerate(zip(numerical, ax.flatten()))):\n        sns.histplot(ax=axes, data=data, x=data[var], hue='HeartDisease', multiple='dodge', shrink=0.8)\n    ### Removing empty figures\n    else:\n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n\n    plt.tight_layout()\n    plt.show()","c562c6c9":"# Plotting pairwise relationships of the numerical features to 'HearDisease'\nplt.figure(figsize=(13,7))\nsns.pairplot(data, hue='HeartDisease')\nplt.show()","78bb09c0":"# Visualizing the outlier boundaries with boxplots\nwith plt.rc_context(rc = {'axes.labelsize': 13, 'ytick.labelsize': 11}):\n    fig, ax = plt.subplots(2, 3, figsize = (13, 8))\n\n    for idx, (var, axes) in list(enumerate(zip(numerical, ax.flatten()))):\n        sns.boxplot(ax=axes, y=data[var], data=data)\n    ### Removing empty figures\n    else:\n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n\n    plt.tight_layout()\n    plt.show()","5a98fd7a":"# Looking at the upper and lower limit outlier boundaries of each feature\ndef find_boundaries(df, var, distance):\n\n    IQR = df[var].quantile(0.75) - df[var].quantile(0.25)\n\n    lower_boundary = df[var].quantile(0.25) - (IQR * distance)  # you can choose your own distance, i went with 1.5\n    upper_boundary = df[var].quantile(0.75) + (IQR * distance)\n\n    return upper_boundary, lower_boundary\n\n# find limits for each variable\n\nfor var in numerical:\n    upper_limit, lower_limit = find_boundaries(data, var, 1.5)\n    print(f'{var}s lower and upper limit is {lower_limit},{upper_limit}')\n\n","19f14f08":"# Looking at the outlier relationship with the target\nfig1 , axes = plt.subplots(nrows=5 ,ncols=2, figsize=(15,40))\n\nsns.swarmplot(x=data['HeartDisease'], y=data['Age'], ax=axes[0,0], data=data, size=8)\nsns.boxplot(x=data['HeartDisease'], y=data['Age'], ax=axes[0,1], data=data)\n\nsns.swarmplot(x=data['HeartDisease'], y=data['RestingBP'], ax=axes[1,0], data=data, size=8)\nsns.boxplot(x=data['HeartDisease'], y=data['RestingBP'], ax=axes[1,1], data=data)\n\nsns.swarmplot(x=data['HeartDisease'], y=data['Cholesterol'], ax=axes[2,0], data=data, size=8)\nsns.boxplot(x=data['HeartDisease'], y=data['Cholesterol'], ax=axes[2,1], data=data)\n\nsns.swarmplot(x=data['HeartDisease'], y=data['MaxHR'], ax=axes[3,0] , data=data, size=8)\nsns.boxplot(x=data['HeartDisease'], y=data['MaxHR'], ax=axes[3,1] , data=data)\n\nsns.swarmplot(x=data['HeartDisease'], y=data['Oldpeak'], ax=axes[4,0], data=data, size=8)\nsns.boxplot(x=data['HeartDisease'], y=data['Oldpeak'], ax=axes[4,1], data=data)","d5b6147a":"# I converted it to a string awhile back for easier analysis on other variables, switching it back to integer now\ndata['HeartDisease'] = data['HeartDisease'].astype(int)\n\nsns.countplot(x=data['HeartDisease'])","39d8e7da":"X = data.drop([\"HeartDisease\"], axis=1)\ny = data[\"HeartDisease\"]\n\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=42)\n\nXtrain.shape, Xtest.shape, ytrain.shape, ytest.shape","e2dc12b0":"# Capping all the values above the upper and lower limit threshold\nwindsoriser = Winsorizer(capping_method='iqr',\n                          tail='both',\n                          fold=1.5,\n                          variables=['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n                        )\n\nXtrain_capped = windsoriser.fit_transform(Xtrain)\nXtest_capped = windsoriser.transform(Xtest)\n\nXtrain_capped.shape","95f2e1b0":"# Looking at the position where the min cap was\nwindsoriser.left_tail_caps_","42251be0":"# Looking at the position where the max cap was\nwindsoriser.right_tail_caps_","1527e0e9":"# Finds all features with 97% or more of its feature space being one value\nconstant = DropConstantFeatures(tol=0.97, variables=None, missing_values='raise')\n\nXtrain_noConst = constant.fit_transform(Xtrain_capped)\nXtest_noConst = constant.transform(Xtest_capped)\n\nprint(f'There are {len(constant.features_to_drop_)} number of Constant\/Quasi-Constant features to drop')","f719004f":"# Visualizing the correlations\ncorrmat = Xtrain_noConst.corr(method='pearson')\n#cmap = sns.diverging_palette(250, 2500, as_cmap=True)\ncmap = sns.diverging_palette(240, 10, n=1, as_cmap=True)\nfig, ax = plt.subplots()\nfig.set_size_inches(10,8)\nsns.heatmap(corrmat, cmap=cmap)","1fc9fce8":"encoder = fe_OneHotEncoder(\n                            top_categories=None,\n                            variables=['Sex', 'ChestPainType', 'FastingBS',\n                                       'RestingECG', 'ExerciseAngina', 'ST_Slope'],\n                            drop_last=True)  # to return k-1, false to return k\n\n\nencoder.fit(Xtrain)\nXtrain_enc = encoder.transform(Xtrain_noConst)\nXtest_enc = encoder.transform(Xtest_noConst)\n","b22e0b15":"Xtrain_enc.head()","4944b369":"scaler = MinMaxScaler()\nXtrain_scaled = scaler.fit_transform(Xtrain_enc)\nXtest_scaled = scaler.transform(Xtest_enc)","f3e502da":"dt = DecisionTreeClassifier(random_state=101)\ndt_model = dt.fit(Xtrain_scaled, ytrain)\n\ndt_y_pred = dt.predict(Xtest_scaled)\n\n\nprint ('Accuracy score : {}'.format(round(accuracy_score(ytest, dt_y_pred),4)))\nprint('F1 score: {}'.format(round(f1_score(ytest, dt_y_pred),4)))\nprint(\"ROC-AUC score: {}\".format(round(roc_auc_score(ytest, dt_y_pred),4)))","59205375":"rf = RandomForestClassifier()\n\nn_estimators_space = list(range(1, 50))\nparam_grid_rf = {'n_estimators': n_estimators_space}\n\nrf_cv = GridSearchCV(rf, param_grid_rf, cv=5)\nrf_cv.fit(Xtrain_scaled, ytrain)\n\nrf_y_pred = rf_cv.predict(Xtest_scaled)\n\n\n# Best params for knn\nprint(\"Best number of estimators: {}\".format(rf_cv.best_params_))\nprint()\n\n# Look at predictions on training and validation set\nprint ('Accuracy score : {}'.format(round(accuracy_score(ytest, rf_y_pred),4)))\nprint('F1 score: {}'.format(round(f1_score(ytest, rf_y_pred),4)))\nprint(\"AUC score: {}\".format(round(roc_auc_score(ytest, rf_y_pred),4)))","a2e45ccd":"gbc = GradientBoostingClassifier()\n\nn_estimators_space = list(range(1, 50))\nparam_grid_gbc = {'n_estimators': n_estimators_space}\n\ngbc_cv = GridSearchCV(rf, param_grid_gbc, cv=5)\ngbc_cv.fit(Xtrain_scaled, ytrain)\n\ngbc_y_pred = gbc_cv.predict(Xtest_scaled)\n\n# Best params for knn\nprint(\"Best number of estimators: {}\".format(gbc_cv.best_params_))\nprint()\n\n\n# Look at predictions on training and validation set\nprint ('Accuracy score : {}'.format(round(accuracy_score(ytest, gbc_y_pred),4)))\nprint('F1 score: {}'.format(round(f1_score(ytest, gbc_y_pred),4)))\nprint(\"AUC score: {}\".format(round(roc_auc_score(ytest, gbc_y_pred),4)))","129bbb04":"knn = KNeighborsClassifier()\n\nn_neighbors_space = list(range(1, 11))\nparam_grid_knn = {'n_neighbors': n_neighbors_space}\n\nknn_cv = GridSearchCV(knn, param_grid_knn, cv=5)\nknn_model = knn_cv.fit(Xtrain_scaled,ytrain)\n\nknn_y_pred = knn_cv.predict(Xtest_scaled)\n\n# Best params for knn\nprint(\"Tuned k Nearest Algorithm Parameters: {}\".format(knn_cv.best_params_))\nprint()\n\n# Look at predictions on training and validation set\nprint ('Accuracy score : {}'.format(round(accuracy_score(ytest, knn_y_pred),4)))\nprint('F1 score: {}'.format(round(f1_score(ytest, knn_y_pred),4)))\nprint(\"AUC score: {}\".format(round(roc_auc_score(ytest, knn_y_pred),4)))","d3f3369d":"svc = SVC(random_state=101)\n\nsvc_model = svc.fit(Xtrain_scaled, ytrain)\n\nsvc_y_pred = svc.predict(Xtest_scaled)\n\n# Look at predictions on training and validation set\nprint ('Accuracy score : {}'.format(round(accuracy_score(ytest, svc_y_pred),4)))\nprint('F1 score: {}'.format(round(f1_score(ytest, svc_y_pred),4)))\nprint(\"AUC score: {}\".format(round(roc_auc_score(ytest, svc_y_pred),4)))","09d21644":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nestimators = [\n    ('Random_forest', rf),\n    ('Gradient_boosting', gbc),\n    ('KNN', knn),\n]\n\nmodel_stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\nmodel_stack.fit(Xtrain_scaled, ytrain)\n\nstack_y_pred = model_stack.predict(Xtest_scaled)\n\n\n# Look at predictions on training and validation set\nprint ('Accuracy score : {}'.format(round(accuracy_score(ytest, stack_y_pred),4)))\nprint('F1 score: {}'.format(round(f1_score(ytest, stack_y_pred),4)))\nprint(\"AUC score: {}\".format(round(roc_auc_score(ytest, stack_y_pred),4)))","01b4b642":"print ('Decision Tree Accuracy score : {}'.format(round(accuracy_score(ytest, dt_y_pred),4)))\nprint ('Random Forest Accuracy score : {}'.format(round(accuracy_score(ytest, rf_y_pred),4)))\nprint ('Gradient Boosting Accuracy score : {}'.format(round(accuracy_score(ytest, gbc_y_pred),4)))\nprint ('KNN Accuracy score : {}'.format(round(accuracy_score(ytest, knn_y_pred),4)))\nprint ('SVC Accuracy score : {}'.format(round(accuracy_score(ytest, svc_y_pred),4)))\nprint ('Stacked Accuracy score : {}'.format(round(accuracy_score(ytest, stack_y_pred),4)))","eba3514e":"acc = pd.DataFrame({\n    \"algorithms\": ['Decision Tree','Random Forest', 'Gradient Boost', 'KNN', 'SVC', 'Stacked Model'],\n    \"accuracy\": [accuracy_score(ytest, dt_y_pred),accuracy_score(ytest, rf_y_pred),\n                 accuracy_score(ytest, gbc_y_pred),accuracy_score(ytest, knn_y_pred),\n                 accuracy_score(ytest, svc_y_pred), accuracy_score(ytest, stack_y_pred)]\n})\n\nplt.figure(figsize=(10,5))\nplt.xlim(0.7)\nsns.barplot(x='accuracy', y='algorithms', data=acc)","56201f67":"print('Decision Tree F1 score: {}'.format(round(f1_score(ytest, dt_y_pred),4)))\nprint('Random Forest F1 score: {}'.format(round(f1_score(ytest, rf_y_pred),4)))\nprint('Gradient Boosting F1 score: {}'.format(round(f1_score(ytest, gbc_y_pred),4)))\nprint('KNN F1 score: {}'.format(round(f1_score(ytest, knn_y_pred),4)))\nprint('SVC F1 score: {}'.format(round(f1_score(ytest, svc_y_pred),4)))\nprint('Stacked F1 score: {}'.format(round(f1_score(ytest, stack_y_pred),4)))","4e281adc":"f1 = pd.DataFrame({\n    \"algorithms\": ['Decision Tree','Random Forest', 'Gradient Boost', 'KNN', 'SVC', 'Stacked Model'],\n    \"f1score\": [f1_score(ytest, dt_y_pred),f1_score(ytest, rf_y_pred),\n                 f1_score(ytest, gbc_y_pred),f1_score(ytest, knn_y_pred),\n                 f1_score(ytest, svc_y_pred), accuracy_score(ytest, stack_y_pred)]\n})\n\nplt.figure(figsize=(10,5))\nplt.xlim(0.7)\nsns.barplot(x='f1score', y='algorithms', data=f1)","bc77c6bc":"print(\"Decision Tree AUC score: {}\".format(round(roc_auc_score(ytest, dt_y_pred),4)))\nprint(\"Random Forest AUC score: {}\".format(round(roc_auc_score(ytest, rf_y_pred),4)))\nprint(\"Gradient Boost AUC score: {}\".format(round(roc_auc_score(ytest, gbc_y_pred),4)))\nprint(\"KNN AUC score: {}\".format(round(roc_auc_score(ytest, knn_y_pred),4)))\nprint(\"SVC AUC score: {}\".format(round(roc_auc_score(ytest, svc_y_pred),4)))\nprint(\"Stacked AUC score: {}\".format(round(roc_auc_score(ytest, stack_y_pred),4)))","9a975d83":"rocauc = pd.DataFrame({\n    \"algorithms\": ['Decision Tree','Random Forest', 'Gradient Boost', 'KNN', 'SVC', 'Stacked Model'],\n    \"roc_auc\": [roc_auc_score(ytest, dt_y_pred),roc_auc_score(ytest, rf_y_pred),\n                 roc_auc_score(ytest, gbc_y_pred),roc_auc_score(ytest, knn_y_pred),\n                 roc_auc_score(ytest, svc_y_pred), accuracy_score(ytest, stack_y_pred)]\n})\n\nplt.figure(figsize=(10,5))\nplt.xlim(0.7)\nsns.barplot(x='roc_auc', y='algorithms', data=rocauc)","8b5ee3ea":"**From the models i tested it seems that the Random Forest and Gradient Boosting models have the best prediction rate**","f5a5d75c":"### Encoding Categorical Variables","59073227":"**Gradient Boosting Classifier**","908cf2ab":"I will be using a few scoring systems in this notebook for fun, which are\n* Accuracy Score - tests how good our model is at predicting the correct category (classes or labels)\n* F1 Score -  defined as the harmonic mean between precision and recall, and is used to evaluate binary classification systems\n* ROC-AUC Score - (Receiver Operating Characteristic)-(Area Under ROC Curve), its a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting","ade6d6c7":"I read through a lot of peoples notebooks, and theres to many people to thank for pointers and ideas, I definetly do have to thank Suprema tism and his notebook tutorials on modeling using seaborn and matplotlib though, they were a great help! Heres a link to his notebook https:\/\/www.kaggle.com\/suprematism\/house-prices-advanced-visualisation","226f872e":"The target is fairly normalized i.e. isnt skewed in one category, this is good","acb5a3ef":"## Modeling","2d6d9eb6":"### Missing Values","1ab13a28":"This data is really clean, normally you would need to split categorical further into ordinal and nominal, and numerical further into discrete and continuous. All the categoricals in the list above are ordinal, and all the numericals in the other list are continuous, this is great!","75323fcd":"There arent any duplicate variables, thats good","516b4de3":"**Suport Vector Clustering (SVC)**","596b5027":"Things to notice\n* Ages 55 and up are more likely to contracting heart disease\n* Majority of the values of Choleterol that are 0 have heart disease\n* People with a Max Heart Rate of around 130 or lower are more prone to contract heart disease\n* People with an Oldpeak of around 0.4 or higher are more prone to contract heart disease","e4264fc8":"**Random Forest Classifier**","319d4b5f":"### Scaling","7fa013eb":"### Comparison Between Models","1d56193d":"### Analyzing Features","45900f47":"Things to notice\n* More Men than women contract heart disease, (take into account theres more men in this survey though)\n* An asymptomatic (ASY) type of chest pain is more likley to contract heart disease\n* A fasting blook pressure of over 120 mg\/dl (1) is more likely to contract heart disese\n* An exercise-included anigna (Y) is more likely to contract heart disease\n* an upsloping (Up) slope of peak exercise is more likely to not contract heart disease, while a flat (Flat) slope of peak exercise is more likely to contract heart disease","88e14fce":"Things to notice\n* Roughly around the ages of 51 to 63 is when your most likely to contract heart disease\n* Roughly around a resting blood pressure of 120 to 145 is when your most likely to contract heart disease\n* Cholesterol is skewed by the variables at 0, but the average Cholesterol number for contracting it looks to be around 100 to 300\n* Roughly around a Max Heart Rate of 116 to 143 is when you most likely to contract heart disease\n* Roughly around an Oldpeak of 0 to 2 is when your most likely to contract heart disease","19313759":"### Conclusion","453d7d65":"No missing values, that makes my life easier :)","2651ed1a":"**Stacked Models**","0e82496e":"I will be capping the variables lying outside the minimum and maximum range of the IQR, which is IQR * 1.5 . Censoring, or capping, means capping the maximum and or minimum of a distribution at an arbitrary value. In other words, values bigger or smaller than the arbitrarily determined ones arent dropped, just censored.","daaf5eea":"### Creating Target","1bdc17da":"**Before Building and making predictions on any model, Its important to understand whether its a Regression model which seeks to predict a continuous quantity i.e. weight, hight, salary etc. or a Classification model which seeks to predict some class label quantity i.e. Male or Female, or Pass or Fail. For this dataset ill be using a Classification model because the target value is binary, with two values showing either (1) the participant got heart disease, or (0) they didnt.**","52036ac9":"**Scoring Measures**","c05edcc5":"### Basic Models","e9ce5507":"**Accuracy**","4e7f1b15":"All the categorical features are ordinal, so there wont be any outliers. Im just going to look at the outliers of the numerical featuers.","66cc229f":"**Decision Tree Classifier**","99017284":"# An analysis on important contributors to your health and how much they play a role into contracting Heart Disease or not","e6bbfed0":"## Table Of Contents\n* Data Analysis\n* Feature Engineering\n* Plotting|","9a8b4068":"### Correlations","27259711":"Things to notice\n* Ther seems to be no linear relationships with the numerical independent variables and the target HeartDisease\n* Only one value of RestingBP is 0\n* There seems to be a correlation with attracting Heart Disease if your Max Heart Rate (MaxHR) is around 140 or lower","da829689":"Things to Notice\n* Average Age is around 46 to 60\n* Average RestingBP is around 123 to 140\n* Average Cholesterol is around 180 to 2080\n* Average MaxHR is around 120 to 158\n* Average Oldpeak is around 0 to 0.8\n\nAnother thing to point out is that all the participants with a cholesterol of 0 are outliers, so ill deal with them after i split the dataset and take care of all outliers.","22cce117":"### Looking further into the numerical variables","8bce76a3":"Things to be aware of about this survey group\n\n* 'Sex' - More men than women\n* 'ChestPainType' - Most participants were Asymptomatic to Chest pain\n* 'FastingBS' - Most participants had fasting blood sugar < 120 mg\/dl (milligrams per deciliter)\n* 'RestingECG' - Most participants resting electrocardiogram results were Normal\n* 'ST_Slope' - Most participants had a downsloping peak exercise ST segment\n* 'Age' - All participants are between the ages of 28 and 77\n* 'Cholesteral' - I assume a lot of participants didnt give this imformation, because its impossible to have a cholesterol level of 0","9a029fd2":"There doesnt seem to be any positive correlations with the independent variables and target","c76b9427":"### Looking at Outliers","155f3f57":"Always split before you do any data pre-processing. Performing pre-processing before splitting will mean that information from your test set will be present during training, causing a data leak. ","8aeaf2a8":"**F1 Score**","ecca3cd4":"### Im playing around with a lot of plotting techniques, also using Decision Tree, Random Forest, Gradient Boosting, KNN, SVC, and Stacking models with a few hyperparameter tuning techniques at the end. ","fd4be285":"**K-Nearest Neighbors (KNN)**","58373675":"### Duplicate Variables","50089d1a":"### Outliers","c24082e9":"### Constant and Quasi-Constant Variables","c7821d9e":"### Splitting Features into categorical and numerical","cf1dc7a5":"I will be using Feature Engines fe_OneHotEncoder() because it keeps the name of the feature and value name after it is split, which makes it a lot easier to analyse. I also doesnt turn into a numpy array, so i wont have to return it back into a pandas dataframe","0d87a11d":"### Looking further into the categorical variables","7ca7027c":"**ROC-AUC Score**","e1762e4b":"### Splitting Dataset into Train and Test sets"}}