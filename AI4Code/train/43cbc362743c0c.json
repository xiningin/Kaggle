{"cell_type":{"13689747":"code","2a509acc":"code","01424d25":"code","6aef853f":"code","3ec6d014":"code","55f25047":"code","2d3fab1a":"code","3a767144":"code","a2582513":"code","11286d05":"code","10883e7d":"code","a5ef7f19":"code","452bdc07":"code","8322688a":"code","8a5d6b68":"code","239cbc18":"code","63cc4987":"code","694ee152":"code","50c47f97":"code","239f4d0c":"code","92222624":"code","7182dbfa":"code","94f3627c":"code","8588414c":"code","5070336d":"code","43325532":"code","8fe71472":"code","1c7b8aa2":"code","742157a3":"code","abed7387":"markdown","247d6c2e":"markdown","6654cb6b":"markdown","a46cfb08":"markdown","4d6685b1":"markdown","b4337cb8":"markdown","64eb0865":"markdown","64386ba0":"markdown","c7beb7b6":"markdown","dc82f4a0":"markdown","9a091673":"markdown","9840a446":"markdown","06129faa":"markdown","598c27f7":"markdown","008ce54f":"markdown"},"source":{"13689747":"import os\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport cv2 as cv\nfrom numpy.random import seed\nseed(45)\nimport pickle\nimport matplotlib.image as mpimg\n\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom glob import glob \n%matplotlib inline","2a509acc":"train_dirname = '..\/input\/hcd-cropped\/train'","01424d25":"train_labels = pd.read_csv('..\/input\/hcd-cropped\/train_labels.csv')\ntrain_labels.head()","6aef853f":"train_labels['label'].value_counts()","3ec6d014":"# Display a DataFrame showing the proportion of observations with each \n# possible of the target variable (which is label). \n(train_labels.label.value_counts() \/ len(train_labels)).to_frame()","55f25047":"train_labels.info()","2d3fab1a":"train_path = \"..\/input\/hcd-cropped\/train\"\nprint('Training Images:', len(os.listdir(train_path)))\n\nsample = train_labels.sample(n=16).reset_index()\n\nplt.figure(figsize=(16,16))\n\nfor i, row in sample.iterrows():\n\n    img = mpimg.imread(f'..\/input\/hcd-cropped\/train\/{row.id}.tif')    \n    label = row.label\n\n    plt.subplot(4,4,i+1)\n    plt.imshow(img)\n    ## class 0 is negative, and class 1 is positive\n    plt.text(0, -5, f'Class {label}', color='k')\n        \n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","3a767144":"IMG_SIZE = 32\nIMG_CHANNELS = 3\n#TRAIN_SIZE=80000\nTRAIN_SIZE = 89117\nBATCH_SIZE = 128\nEPOCHS = 30","a2582513":"train_neg = train_labels[train_labels['label']==0].sample(TRAIN_SIZE,random_state=45)\ntrain_pos = train_labels[train_labels['label']==1]\n\ntrain_data = pd.concat([train_neg, train_pos], axis=0).reset_index(drop=True)\n\ntrain_data = shuffle(train_data)","11286d05":"train_data['label'].value_counts()","10883e7d":"def append_ext(fn):\n    return fn+\".tif\"","a5ef7f19":"#y = train_data['label']\n#train_df, val_df = train_test_split(train_data, test_size=0.3, random_state=45, stratify=y)\n#y = val_df['label']\n#val_df, test_df = train_test_split(val_df, test_size=0.5, random_state=45, stratify=y)\n#print(train_df.shape)\n#print(val_df.shape)\n#print(test_df.shape)","452bdc07":"y = train_data['label']\ntrain_df, valid_df = train_test_split(train_data, test_size=0.2, random_state=45, stratify=y)\n\nprint(train_df.shape)\nprint(valid_df.shape)","8322688a":"train_df['id'] = train_df['id'].apply(append_ext)\nvalid_df['id'] = valid_df['id'].apply(append_ext)\ntrain_df.head()","8a5d6b68":"#train_datagen = ImageDataGenerator(rescale=1\/255)\n#valid_datagen = ImageDataGenerator(rescale=1\/255)\n\n## add image augmentation\ntrain_datagen = ImageDataGenerator(\n    rescale = 1\/255, \n    rotation_range = 45,\n    width_shift_range = 0.2, \n    height_shift_range = 0.2, \n    shear_range = 0.2, \n    zoom_range = 0.2, \n    horizontal_flip = True, \n    vertical_flip = True \n)\nvalid_datagen = ImageDataGenerator(rescale=1\/255)","239cbc18":"BATCH_SIZE = 128\ntrain_path = '..\/input\/hcd-cropped\/train'\ntrain_df['label'] = train_df['label'].astype(str)\nvalid_df['label'] = valid_df['label'].astype(str)\n\ntrain_loader = train_datagen.flow_from_dataframe(\n    dataframe = train_df,\n    directory = train_path,\n    x_col = 'id',\n    y_col = 'label',\n    batch_size = BATCH_SIZE,\n    seed = 1,\n    shuffle = True,\n    class_mode = 'categorical',\n    target_size = (32,32)\n)\n\nvalid_loader = valid_datagen.flow_from_dataframe(\n    dataframe = valid_df,\n    directory = train_path,\n    x_col = 'id',\n    y_col = 'label',\n    batch_size = BATCH_SIZE,\n    seed = 1,\n    shuffle = True,\n    class_mode = 'categorical',\n    target_size = (32,32)\n)","63cc4987":"TR_STEPS = len(train_loader)\nVA_STEPS = len(valid_loader)\n\nprint(TR_STEPS)\nprint(VA_STEPS)","694ee152":"cnn = Sequential([\n    Conv2D(16, (3,3), activation = 'relu', padding = 'same', input_shape=(32,32,3)),\n    Conv2D(16, (3,3), activation = 'relu', padding = 'same'),\n    Conv2D(16, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.25),\n    BatchNormalization(),\n\n    Conv2D(32, (3,3), activation = 'relu', padding = 'same'),\n    Conv2D(32, (3,3), activation = 'relu', padding = 'same'),\n    Conv2D(32, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.25),\n    BatchNormalization(),\n    \n    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.5),\n    BatchNormalization(),\n    \n    Flatten(),\n    \n    Dense(128, activation='relu'),\n    Dropout(0.6),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(32, activation='relu'),\n    Dropout(0.4),\n    Dense(8, activation='relu'),\n    Dropout(0.25),\n    BatchNormalization(),\n    Dense(2, activation='softmax')\n])\n\ncnn.summary()","50c47f97":"opt = tf.keras.optimizers.Adam(0.001)\ncnn.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.AUC()])","239f4d0c":"%%time \n\nh1 = cnn.fit(\n    x = train_loader, \n    steps_per_epoch = TR_STEPS, \n    epochs = 30,\n    validation_data = valid_loader, \n    validation_steps = VA_STEPS, \n    verbose = 1,\n    use_multiprocessing = True,\n    workers = 8\n)","92222624":"history = h1.history\nprint(history.keys())","7182dbfa":"epoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,3,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,3,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.subplot(1,3,3)\nplt.plot(epoch_range, history['auc'], label='Training')\nplt.plot(epoch_range, history['val_auc'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('AUC'); plt.title('AUC')\nplt.legend()\nplt.tight_layout()\nplt.show()","94f3627c":"#Training Run 2\ntf.keras.backend.set_value(cnn.optimizer.learning_rate, 0.0001)","8588414c":"%%time \n\nh2 = cnn.fit(\n    x = train_loader, \n    steps_per_epoch = TR_STEPS, \n    epochs = 30,\n    validation_data = valid_loader, \n    validation_steps = VA_STEPS, \n    verbose = 1,\n    use_multiprocessing = True,\n    workers = 8\n\n)","5070336d":"for k in history.keys():\n    history[k] += h2.history[k]\n\nepoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,3,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,3,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.subplot(1,3,3)\nplt.plot(epoch_range, history['auc'], label='Training')\nplt.plot(epoch_range, history['val_auc'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('AUC'); plt.title('AUC')\nplt.legend()\nplt.tight_layout()\nplt.show()","43325532":"#Training Run 3\ntf.keras.backend.set_value(cnn.optimizer.learning_rate, 0.00001)","8fe71472":"%%time \n\nh3 = cnn.fit(\n    x = train_loader, \n    steps_per_epoch = TR_STEPS, \n    epochs = 30,\n    validation_data = valid_loader, \n    validation_steps = VA_STEPS, \n    verbose = 1,\n    use_multiprocessing = True,\n    workers = 8\n\n)","1c7b8aa2":"for k in history.keys():\n    history[k] += h3.history[k]\n\nepoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,3,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,3,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.subplot(1,3,3)\nplt.plot(epoch_range, history['auc'], label='Training')\nplt.plot(epoch_range, history['val_auc'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('AUC'); plt.title('AUC')\nplt.legend()\nplt.tight_layout()\nplt.show()","742157a3":"cnn.save('cancer_detection_model_crop_v07.h5')\npickle.dump(history, open(f'cancer_detection_history_crop_v07.pkl', 'wb'))","abed7387":"Data is not entirely balanced, there is more negative samples than positive, by about 30 percent","247d6c2e":"## Train Network","6654cb6b":"### Save Model and History","a46cfb08":"## Create a CNN training model","4d6685b1":"### Create image generators for MobileNetV2","b4337cb8":"## Image generators for the simple CNN model","64eb0865":"# Dataset exploration","64386ba0":"# View Sample Images","c7beb7b6":"# Splitting dataset","dc82f4a0":"# Setting up learning constants","9a091673":"## Balancing the dataset","9840a446":"## Training Run 1","06129faa":"## ","598c27f7":"## Splitting the dataset","008ce54f":"# Label distribution"}}