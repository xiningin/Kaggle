{"cell_type":{"4e527e78":"code","a7c4b6ab":"code","f3fbabd7":"code","cd9cdbc9":"code","8ed6576a":"code","9cdfa7d4":"code","579cd811":"code","e8eb95ee":"code","1ae82e58":"code","681a434d":"code","905a7e02":"code","d5d57a31":"code","134b476a":"code","24ebce92":"code","26199a4c":"code","7068cf8e":"code","eedc2ca7":"code","46fdb68c":"code","a10f96c8":"code","1ef8a681":"code","fb1aba01":"code","4fcb10b1":"code","96b4fdf1":"code","06261d2c":"code","c0963638":"code","b2933439":"code","413053bf":"code","12560aa3":"code","69289ce7":"code","4108f7a7":"code","c3d4a2c8":"code","b205ecee":"code","70e61015":"code","0083a72f":"code","2a738602":"code","ab5a50be":"code","5a356f26":"code","d03e0a83":"code","4f1d265c":"code","6185f191":"code","432b3a9f":"markdown"},"source":{"4e527e78":"!conda install -c conda-forge folium=0.5.0 --yes # comment\/uncomment if not yet installed.\n!conda install -c conda-forge geopy --yes        # comment\/uncomment if not yet installed\n\nimport numpy as np # library to handle data in a vectorized manner\nimport pandas as pd # library for data analsysis\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport json # library to handle JSON files\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\n\n# import k-means from clustering stage\nfrom sklearn.cluster import KMeans\nimport folium # map rendering library\nfrom sklearn.preprocessing import StandardScaler #we are going to use this to find the optimal k for clustering\nfrom sklearn.metrics import silhouette_score #once again when finding the optimal k\n\nimport requests # library to handle requests\nimport bs4 as bs\nimport urllib.request\n\nprint('Libraries imported.')","a7c4b6ab":"url   = \"https:\/\/en.wikipedia.org\/wiki\/List_of_districts_of_Istanbul\"","f3fbabd7":"from IPython.display import HTML\nimport base64\n\n# Extra Helper scripts to generate download links for saved dataframes in csv format.\ndef create_download_link( df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\npage  = urllib.request.urlopen(url).read()\nsoup  = bs.BeautifulSoup(page,'lxml')\ntable = soup.find(\"table\",class_=\"wikitable\")\nheader = [head.findAll(text=True)[0].strip() for head in table.find_all(\"th\")]\ndata   = [[td.findAll(text=True)[0].strip() for td in tr.find_all(\"td\")]\n          for tr in table.find_all(\"tr\")]\ndata    = [row for row in data if len(row) == 6] #I have 6 columns\n\nraww_df= pd.DataFrame(data,columns=header)\nraww_df.to_csv('istanbul_district_income_density_population_area.csv',index=False)\n\nraw_df = pd.DataFrame(data,columns=header)\nraw_df = raw_df[:-4] #removing last 4 rows of unneccessary info.\nraw_df = raw_df.drop(raw_df.columns[[4,5]], axis=1) #removing last 2 columns of unneccessary info.\nraw_df = raw_df.replace(',','', regex=True) #removing commas from numbers\nraw_df['Population (2019)'].astype(str).astype(float)\nraw_df['Area (km\u00b2)'].astype(str).astype(float)\nraw_df['Density (per km\u00b2)'].astype(str).astype(float) #converting from string to float\nraw_df=raw_df.sort_values(by=['District','Population (2019)','Area (km\u00b2)','Density (per km\u00b2)'], ascending=[1,1,1,1]).reset_index(drop=True)\n\nprint(raw_df.info(verbose=True))","cd9cdbc9":"create_download_link(raww_df,\"Istanbul District List 2019(income_density_population_area)\",\"istanbul_district_income_density_population_area_2019.csv\")","8ed6576a":"create_download_link(raw_df,\"Istanbul District List 2019(population_area_density)\",\"istanbul_district_population_area_density_2019\")","9cdfa7d4":"raw_df.head()","579cd811":"geolocator = Nominatim(user_agent='My-Notebook')\nlokasyon=[]\nlat=[]\nlon=[]\nfor x in range(0, len(raw_df)):#some district names like \u015eile gets mistaken for Chile so we add Istanbul next to them.\n    lokasyon.append(raw_df.loc[x,'District']+', Istanbul')\nfor r in range(0, len(raw_df)):\n    location = geolocator.geocode(lokasyon[r])\n    lat.append(location.latitude)\n    lon.append(location.longitude)","e8eb95ee":"map(float, lat)#converting to float\nmap(float, lon)\nraw_df = raw_df.assign(Latitude = lat) #adding lat and lon values for districts\nraw_df = raw_df.assign(Longitude = lon)\nraw_df = raw_df.drop(raw_df.columns[[1,2,3]], axis=1) #should've done this earlier, but firmly decided not to use population now, so, off they go.","1ae82e58":"raw_df.head()","681a434d":"raw_df.to_csv('istanbul_district_latlon.csv',index=False) #storing into a csv for proper use, whatever that means.","905a7e02":"create_download_link(raw_df,\"Istanbul District List 2019(district_lat_lon)\",\"istanbul_district_latlon.csv\")","d5d57a31":"ist = pd.read_csv('istanbul_district_latlon.csv')","134b476a":"#getting the coordinates for Istanbul for map creation\naddress = 'Istanbul Turkey'\n\nlocationist = geolocator.geocode(address)\nlatitudist = locationist.latitude\nlongitudist = locationist.longitude\nprint('The geograpical coordinates for Istanbul are {}, {}.'.format(latitudist, longitudist))","24ebce92":"#creating a map of Istanbul with Districts\nmap_istanbul = folium.Map(location=[latitudist, longitudist], zoom_start=10)\n\n# add markers to map\nfor lat, lng, label in zip(ist['Latitude'], ist['Longitude'], ist['District']):\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=6,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#87cefa',\n        fill_opacity=0.5,\n    ).add_to(map_istanbul)\nmap_istanbul","26199a4c":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nCLIENT_ID = user_secrets.get_secret(\"Foursquare ID\")\nCLIENT_SECRET = user_secrets.get_secret(\"Foursquare secret\")\nVERSION = '20180604'\nLIMIT = 50","7068cf8e":"\n# Get the venue names and store it in a dataframe\ndef getNearbyVenues(names, latitudes, longitudes, radius=20000):#20km radius to filter every possible option since we are dealing with districts.\n                                                                #Since we also deal with islands 20km is overkill, but would give an idea anyway.\n    venues_list=[]\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        # creating API request url\n        url = 'https:\/\/api.foursquare.com\/v2\/venues\/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            radius, \n            LIMIT)\n            \n        # GET request\n        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n        \n        # return only relevant information for each nearby venue\n        venues_list.append([(\n            name, \n            lat, \n            lng, \n            v['venue']['name'], \n            v['venue']['location']['lat'], \n            v['venue']['location']['lng'],  \n            v['venue']['categories'][0]['name']) for v in results])\n\n    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n    nearby_venues.columns = ['District', \n                  'District Latitude', \n                  'District Longitude', \n                  'Venue', \n                  'Venue Latitude', \n                  'Venue Longitude', \n                  'Venue Category']\n    return(nearby_venues)","eedc2ca7":"ist_venues=getNearbyVenues(names=ist['District'], latitudes=ist['Latitude'],longitudes=ist['Longitude'])","46fdb68c":"#filtering results with parks only\nist_park=ist_venues[ist_venues['Venue Category'].str.contains('Park')]\n#and forests\nist_orman=ist_venues[ist_venues['Venue Category'].str.contains('Forest')] \n#and gardens\nist_bahce=ist_venues[ist_venues['Venue Category'].str.contains('Garden')] \n#and dogruns eventghough you may not be a dog person\nist_dog=ist_venues[ist_venues['Venue Category'].str.contains('Dog')] \n#and farms considering they are places that you can refresh\nist_cift=ist_venues[ist_venues['Venue Category'].str.contains('Farm')] \n#and parkours where you can take a walk that may be alongside a greenspace\nist_parkur=ist_venues[ist_venues['Venue Category'].str.startswith('Field')]\n#I am pushin the boundaries and counting mountain tops as green spaces too.\nist_dag=ist_venues[ist_venues['Venue Category'].str.contains('Mountain')] \n#In the context of green spaces, cemeteries are also in the mix, but I assume no one would go to a cemetery to get a breath of fresh air.\n\nist_green=ist_park #creating a dataframe where all of them are listed.\nist_green=ist_green.append(ist_orman)\nist_green=ist_green.append(ist_bahce)\nist_green=ist_green.append(ist_dag)\nist_green=ist_green.append(ist_cift)\nist_green=ist_green.append(ist_dog)\nist_green=ist_green.append(ist_parkur)","a10f96c8":"#reseting the indexing for the dataframe\nist_green=ist_green.sort_values(by=['District','District Latitude','District Longitude','Venue','Venue Latitude','Venue Longitude','Venue Category'], ascending=[1,1,1,1,1,1,1]).reset_index(drop=True)","1ef8a681":"ist_green.to_csv('istanbul_greenspaces.csv',index=False) #storing into a csv for proper use, whatever that means.","fb1aba01":"ist_green.head()","4fcb10b1":"create_download_link(ist_green,\"Istanbul District List 2019(district_nearbygreen20km_venue_lat_lon)\",\"istanbul_greenspaces.csv\")","96b4fdf1":"print('There are {} unique venues.'.format(len(ist_green['Venue'].unique()))) #No of unique green spaces","06261d2c":"#lets see the map where these greens are located\nfor lat, lng, label in zip(ist_green['Venue Latitude'], ist_green['Venue Longitude'], ist_green['Venue']):\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=6,\n        popup=label,\n        color='green',\n        fill=True,\n        fill_color='#87cefa',\n        fill_opacity=0.5,\n    ).add_to(map_istanbul)\nmap_istanbul","c0963638":"#The venues are not unique to their respective districts, lets see which district has more greenspace.\n\nplt.figure(figsize=(10,10), dpi = 100)\nplt.title('Number of Green Spaces for each District in Istanbul')\n#On x-axis\nplt.xlabel('No.of Greenspaces', fontsize = 15)\n#On y-axis\nplt.ylabel('District Name', fontsize=15)\n#giving a bar plot\nist_green.groupby('District')['Venue'].count().sort_values(ascending=True).plot(kind='barh',color='green')\n#displays the plot\nplt.show()","b2933439":"#We can see from our graph that Adalar, \u00c7atalca and Maltepe are the most green Districts\n#While a total of 11 districts from Gaziosmanpa\u015fa and G\u00fcng\u00f6ren are the least green.","413053bf":"# one hot encoding\nist_onehot = pd.get_dummies(ist_green[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n\n# add district column back to dataframe\nist_onehot['District'] = ist_green['District'] \n\n# move district column to the first column\nfixed_columns = [ist_onehot.columns[-1]] + list(ist_onehot.columns[:-1])\nist_onehot = ist_onehot[fixed_columns]\n\n# Regroup rows by district and mean of frequency occurrence per category.\nist_grouped = ist_onehot.groupby('District').mean().reset_index()","12560aa3":"ist_grouped.to_csv('istanbul_greenspaces_freq_district.csv',index=False) #storing into a csv for proper use, whatever that means.","69289ce7":"#lets check the optimal value of k we should use in order to obtain ideal clustering\nist_grouped_clustering =ist_grouped.drop('District', 1)\nist_std = StandardScaler().fit_transform(ist_grouped_clustering)\nsse = []\nsil = []\nlist_k = list(range(2, 12))\n\nfor k in list_k:\n    km = KMeans(n_clusters=k, random_state=1)\n    km.fit(ist_std)\n    sse.append(km.inertia_)\n    sil.append(silhouette_score(ist_std, km.labels_))\n\nfig, ax1 = plt.subplots(figsize=(6, 6))\n\nax2 = ax1.twinx()\nax1.plot(list_k, sse, 'bo-')\nax2.plot(list_k, sil, 'rd-')\n\nax1.set_xlabel(r'Number of clusters *k*')\nax1.set_ylabel('Sum of inner squared distance', color='b')\nax2.set_ylabel('Silhouette score', color='r')\n\nplt.show()","4108f7a7":"# We can see k=6 is best.\nkclusters = 6\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=1).fit(ist_grouped_clustering)\n# check cluster labels generated for each row in the dataframe\nprint(kmeans.labels_[0:10])","c3d4a2c8":"ist_sorted = ist.set_index(\"District\")\nist_merged = ist_grouped.set_index(\"District\")\nist_merged['Cluster Labels'] = kmeans.labels_\nist_merged = ist_merged.join(ist_sorted)","b205ecee":"# creating the map for clusters\nmap_clusters = folium.Map(location=[latitudist, longitudist], tiles=\"Openstreetmap\", zoom_start=10)\n\n\n# set color scheme for the clusters\nx = np.arange(kclusters)\nys = [i+x+(i*x)**2 for i in range(kclusters)]\ncolors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\nrainbow = [colors.rgb2hex(i) for i in colors_array]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, poi, cluster in zip(ist_merged['Latitude'], ist_merged['Longitude'], ist_merged.index.values,kmeans.labels_):\n    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n    folium.CircleMarker(\n        [lat, lon],\n        radius=10,\n        popup=label,\n        color=rainbow[cluster-1],\n        fill=True,\n        fill_color=rainbow[cluster-1],\n        fill_opacity=1).add_to(map_clusters)   \nmap_clusters","70e61015":"ist_merged.loc[ist_merged['Cluster Labels'] == 0, ist_merged.columns[[2] + list(range(0, ist_merged.shape[1]))]]\n#The cluster where there is access to a forest.","0083a72f":"ist_merged.loc[ist_merged['Cluster Labels'] == 1, ist_merged.columns[[2] + list(range(0, ist_merged.shape[1]))]]\n#The cluster where there is only access to mainly a Park and a Field","2a738602":"ist_merged.loc[ist_merged['Cluster Labels'] == 2, ist_merged.columns[[2] + list(range(0, ist_merged.shape[1]))]]\n#The the cluster where there is every category but a mountain top","ab5a50be":"ist_merged.loc[ist_merged['Cluster Labels'] == 3, ist_merged.columns[[2] + list(range(0, ist_merged.shape[1]))]]\n#The the cluster where there is only access to a Botanical Garden","5a356f26":"ist_merged.loc[ist_merged['Cluster Labels'] == 4, ist_merged.columns[[2] + list(range(0, ist_merged.shape[1]))]]\n#The the cluster where there is access to a mountain top, field and a park","d03e0a83":"ist_merged.loc[ist_merged['Cluster Labels'] == 5, ist_merged.columns[[2] + list(range(0, ist_merged.shape[1]))]]\n#The the cluster where there is access to a garden.","4f1d265c":"#We can understand from the clusters that the most variety of green spaces are in cluster2\n#And the least variety is in cluster 3 and 5 districts Beylikd\u00fcz\u00fc and Arnavutk\u00f6y","6185f191":"#From our clustering map and bar graph consisting the access to number of green spaces\n#the ideal district for green space variety and quantity would be \u00c7atalca.\n#The least ideal would either be Arnavutk\u00f6y or Beylikd\u00fcz\u00fc since they only have access to\n#a single botanical garden and a garden.","432b3a9f":"\n# Abstract\n\nA colleague has reached out to me for advice on a nice place to live in Istanbul. I have interpreted this request as my mission to analyze which district of Istanbul has access to more and various green spaces within a 20km distance. I have used geopy, foursquare API and a wikipedia table to scrape and manipulate data of district names, name of green spaces, their longitude and latitude values. I have used k-means algorithm to cluster the variety and used my data frame to count the number of green spaces each district has access to. After analysis, I have found that **\u00c7atalca** is the district with access to most green spaces with a wide variety and the least desirable districts are **Beylikd\u00fcz\u00fc** and **Arnavutk\u00f6y** (see my article: [The quest for a breath of fresh air](https:\/\/www.linkedin.com\/pulse\/quest-breath-fresh-air-analysis-districts-istanbul-access-sa\u011f\u0131ro\u011flu\/) and the full repo:[github_repo](https:\/\/github.com\/cansagiroglu\/Coursera_Capstone\/tree\/master\/Project_TBON)\n\n# Discussion\nThe data I have gathered with the Foursquare API is a user based collection of data, meaning the amount of data is limited with the user input, considering Foursquare lost its popularity by about 2015 or so (although a lot of the apps and services we render today use their API service) the data gathered from their resource may not have been an accurate description of the great outdoors I was looking for; thus the 20km radius I have set in order to get a variety of places. I do not believe this is of significant importance while deriving an analysis but a minor detail that needs to be mentioned.\n\nThe location data that I have gathered consisting of latitude and longitude is approximately at the center of each district, this does not necessarily mean that an individual would reside at the center of each district, once again, this is the main reason why I introduced the 20 km radius. One solution to this could have easily been using neighbourhoods rather than districts but since Istanbul has very little amount of green spaces, the analysis would have painted a darker picture than we have already produced.\n\nRandom state and number of clusters when using the k-means algorithm could have been varied to observe the effect they have on the clustering mechanism, since the access to number of green spaces and variety didn't necessarily concur with one another in my results; but nevertheless the produced results are a good representation."}}