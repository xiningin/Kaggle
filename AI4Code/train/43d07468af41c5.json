{"cell_type":{"3c8f6501":"code","8c1da638":"code","c943e165":"code","7d9c11c3":"code","914a2ff9":"code","e4a785c9":"code","6bde8b2f":"code","ce8f90a4":"code","990eb6a0":"code","c772cd19":"code","d6f20297":"code","12989af2":"code","f711abcb":"code","9420ab57":"code","1e9f8957":"code","a6a70f4b":"code","90cf7d24":"code","3544b8e1":"code","b977c109":"code","f37fa899":"code","c0259b50":"code","2ebbd915":"code","9c4b3e5c":"code","afd9f8bc":"code","e4aac2a3":"code","01fe1eda":"code","15a5faa6":"code","2fc33aa7":"code","9acf464f":"code","c885812f":"code","3b62284a":"code","cfebdf3d":"code","a51af454":"code","f09f3c1f":"code","2b0f544a":"code","1ea7db18":"code","1a41ce1c":"code","21acbdac":"code","9e50c32a":"code","a3d96d2b":"code","0a88950b":"markdown","6c7fdcbb":"markdown","65af710f":"markdown","723ab1f2":"markdown","49c97371":"markdown","0f956a64":"markdown","0d4489ff":"markdown","ffd342be":"markdown","f77da843":"markdown","f4b6c405":"markdown","897d6a2b":"markdown"},"source":{"3c8f6501":"import numpy as np \nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","8c1da638":"df=pd.read_csv('..\/input\/diabetes-data-set\/diabetes.csv')","c943e165":"df.head( )","7d9c11c3":"df.info()","914a2ff9":"df.describe()","e4a785c9":"df.count()","6bde8b2f":"df.sum()","ce8f90a4":"df.isnull()","990eb6a0":"df['DiabetesPedigreeFunction'].plot.hist()","c772cd19":"df['Age'].plot.hist()","d6f20297":"sns.scatterplot(x='DiabetesPedigreeFunction', y='Age', data=df)","12989af2":"sns.pairplot(df)","f711abcb":"import missingno as msno\nmsno.matrix(df)\n","9420ab57":"a = 4  # number of rows\nb = 4  # number of columns\nc = 1  # initialize plot counter\nfig = plt.figure(figsize=(35,32))\nfor i in df:\n    if df[i].dtype=='float64':\n        plt.subplot(a, b, c)\n        sns.distplot(df[i])\n        c = c+1\n    else:\n        continue\nplt.tight_layout()\nplt.show()       \n","1e9f8957":"#Soure: https:\/\/www.kaggle.com\/momincks\/rain-tomorrow-in-aus-by-xgboostclassifier : Do check him out.\n\ndf_dateplot = df.iloc[-950:,:]\nplt.figure(figsize=[20,5])\nplt.plot(df_dateplot['DiabetesPedigreeFunction'],df_dateplot['Age'],color='blue',linewidth=1, label= 'Outcome')\nplt.plot(df_dateplot['DiabetesPedigreeFunction'],df_dateplot['Age'],color='red',linewidth=1, label= 'Outcome')\nplt.fill_between(df_dateplot['DiabetesPedigreeFunction'],df_dateplot['Outcome'],df_dateplot['Outcome'], facecolor = '#EBF78F')\nplt.title('MinTemp vs MaxTemp by Date')\nplt.legend(loc='lower left', frameon=False)\nplt.show()\n\n","a6a70f4b":"df_dateplot = df.iloc[-950:,:]\nplt.figure(figsize=[20,5])\nplt.plot(df_dateplot['DiabetesPedigreeFunction'],df_dateplot['Outcome'],color='violet', linewidth=2, label= 'WindGustSpeed')\nplt.legend(loc='upper left', frameon=False)\nplt.title('WindGustSpeed by DiabetesPedigreeFunction')\nplt.show()\n","90cf7d24":"sns.pairplot(data=df)\nplt.show()\n","3544b8e1":"from sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n# abnormal = 1 and normal = 0\ndf ['DiabetesPedigreeFunction'] = [1 if i == 'Abnormal' else 0 for i in df.loc[:,'Age']]\nx,y = df.loc[:,(df.columns != 'DiabetesPedigreeFunction') & (df.columns != 'Age')], df.loc[:,'Outcome']\n","b977c109":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=42)\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred_prob = logreg.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()\n","f37fa899":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\ngrid = {'n_neighbors': np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(x,y)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))\n","c0259b50":"\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nsteps = [('scalar', StandardScaler()),\n         ('SVM', SVC())]\npipeline = Pipeline(steps)\nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 1)\ncv = GridSearchCV(pipeline,param_grid=parameters,cv=3)\ncv.fit(x_train,y_train)\n\ny_pred = cv.predict(x_test)\n\nprint(\"Accuracy: {}\".format(cv.score(x_test, y_test)))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n","2ebbd915":"plt.figure(figsize=(7,4)) \nsns.heatmap(df.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()\n","9c4b3e5c":"\nfrom sklearn.linear_model import LinearRegression\nX_train = np.array(df.iloc[:, :-1].values)\ny_train = np.array(df.iloc[:, 1].values)\nX_test = np.array(df.iloc[:, :-1].values)\ny_test = np.array(df.iloc[:, 1].values)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\n\nplt.plot(X_train, model.predict(X_train), color='green')\nplt.show()\nprint(accuracy)\n","afd9f8bc":"from sklearn.linear_model import  TheilSenRegressor\nmodel = TheilSenRegressor()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)\n","e4aac2a3":"from sklearn.linear_model import  RANSACRegressor\nmodel = RANSACRegressor()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)\n","01fe1eda":"from sklearn.linear_model import  RANSACRegressor\nmodel = RANSACRegressor()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)","15a5faa6":"from sklearn.linear_model import  HuberRegressor\nmodel = HuberRegressor()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)\n","2fc33aa7":"training_size = int(len(df)*0.80)\ndata_len = len(df)\n\ntrain, test = df[0:training_size],df[training_size:data_len]","9acf464f":"print(\"Training Size --> \", training_size)\nprint(\"total length of data --> \", data_len)\nprint(\"Train length --> \", len(train))\nprint(\"Test length --> \", len(test))","c885812f":"# the part of data that we will use as training.\ntrain = train.loc[:, [\"DiabetesPedigreeFunction\"]].values\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\ntrain_scaled = scaler.fit_transform(train)","3b62284a":"end_len = len(train_scaled)\nX_train = []\ny_train = []\ntimesteps = 40\n\nfor i in range(timesteps, end_len):\n    X_train.append(train_scaled[i - timesteps:i, 0])\n    y_train.append(train_scaled[i, 0])\nX_train, y_train = np.array(X_train), np.array(y_train)","cfebdf3d":"X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nprint(\"X_train --> \", X_train.shape)\nprint(\"y_train shape --> \", y_train.shape)","a51af454":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout","f09f3c1f":"regressor = Sequential()\n\nregressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1],1)))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units = 50))\nregressor.add(Dropout(0.2))\n\nregressor.add(Dense(units = 1))","2b0f544a":"regressor.compile(optimizer= \"adam\", loss = \"mean_squared_error\")","1ea7db18":"epochs = 100 \nbatch_size = 20","1a41ce1c":"regressor.fit(X_train, y_train, epochs = epochs, batch_size = batch_size)","21acbdac":"real_price = test.loc[:, [\"DiabetesPedigreeFunction\"]].values\nprint(\"Real Price Shape --> \", real_price.shape)","9e50c32a":"dataset_total = pd.concat((df[\"DiabetesPedigreeFunction\"], test[\"DiabetesPedigreeFunction\"]), axis = 0)\ninputs = dataset_total[len(dataset_total) - len(test) - timesteps:].values.reshape(-1,1)\ninputs = scaler.transform(inputs)","a3d96d2b":"X_test = []\n\nfor i in range(timesteps, 412):\n    X_test.append(inputs[i-timesteps:i, 0])\nX_test = np.array(X_test)\n\nprint(\"X_test shape --> \", X_test.shape)","0a88950b":"# \u062a\u062d\u0645\u064a\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a","6c7fdcbb":" # SVM, pre-process and pipeline","65af710f":"# Here we can make the test and train :","723ab1f2":"#  ROC Curve with logistic regression","49c97371":"# \u0627\u0633\u062a\u062f\u0639\u0627\u0621 \u0645\u0643\u062a\u0627\u0628\u0627\u062a \u0627\u0644\u062a\u0649 \u0646\u0631\u064a\u062f\u0647\u0627 \u0644\u0644\u0639\u0645\u0644","0f956a64":"# RANSAC Regressor","0d4489ff":"# TheilSen Regressor","ffd342be":"# \u0628\u0639\u0636 \u0627\u0644\u062a\u0635\u0648\u064a\u0631\u0627\u062a \u0644\u0627\u0638\u0647\u0627\u0631 \u0628\u0639\u0636 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0648\u0641\u0647\u0645\u0647\u0627 ","f77da843":"# Huber Regressor","f4b6c405":"# \u0639\u0631\u0636 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a","897d6a2b":"# HYPERPARAMETER TUNING:"}}