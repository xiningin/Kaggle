{"cell_type":{"213a1f8f":"code","74315fce":"code","1b3e6de6":"code","ad35565a":"code","397267ac":"code","0d81a81c":"code","3c9ee29b":"code","95dec6aa":"code","a6ce7e11":"code","394bdb16":"code","f0ba1fec":"code","a12bbaf4":"code","6105fb6f":"code","b061afd9":"code","b95c47a2":"code","36eaaefa":"code","ea99839e":"code","06d574d7":"code","afe4b88c":"code","eac0d005":"code","92693274":"code","054fe11e":"code","15cd8ddc":"code","65b8c444":"markdown","a6e8948d":"markdown","f126be8d":"markdown","9dbc4389":"markdown","7c1b9b85":"markdown","473cdd6b":"markdown","48fff8ee":"markdown","8ba06a7b":"markdown","d5aaadbc":"markdown","96e9b29c":"markdown","816cfe17":"markdown","1b261743":"markdown","5fe09348":"markdown","d2e779c8":"markdown","3556e696":"markdown","6107aa3b":"markdown","4b8e79ee":"markdown","679abbec":"markdown","41819ebf":"markdown","1769ea72":"markdown","34e7bd6d":"markdown","ff4aecf4":"markdown","29c5037e":"markdown","29ac06a6":"markdown","56f57a7f":"markdown","3b8ee300":"markdown","48e84a12":"markdown","2dcbc0cf":"markdown","683e1915":"markdown","c7f811b3":"markdown","f7daecdc":"markdown","07994b0b":"markdown","7ec10c28":"markdown","1e1a27c9":"markdown","20452a83":"markdown","20f87316":"markdown","5e129a51":"markdown","fadfe52e":"markdown"},"source":{"213a1f8f":"#Libraries for manipulating data\nimport numpy as np\nimport pandas as pd\n\n#Libraries for visualizaiton\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport wandb\n\n#Libraries and tools for deep learning \nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom transformers import TrainingArguments, Trainer","74315fce":"#Download datasets\n!pip install -q gdown\n!gdown --id --folder 1cmrCgvpowRN4ySol4J8ZT0oVh_0v75Q_","1b3e6de6":"#Read file and seperate each line and convert it to a DataFrame\ndef read_file(path, label):\n    df = pd.read_csv(path, delimiter = \"\\n\", names=[\"text\", 'label'])\n    df['label'] = label\n    return df","ad35565a":"#Read data from files\ndf_rumi_small = read_file('.\/dataset\/small\/poetry_rumi.txt', 'Rumi')\ndf_saadi_small = read_file('.\/dataset\/small\/poetry_saadi.txt', 'Saadi')\n\n#Shuffle poets into one DataFrame\ndf_small = pd.concat([df_rumi_small, df_saadi_small], ignore_index=True)\ndf = df_small.sample(frac=1).reset_index(drop=True)\ndf.head()","397267ac":"sns.catplot(data=df, x='label', kind='count')\nplt.show()","0d81a81c":"#Count spaces plus one for number of words\ndf['word_count'] = df['text'].str.count(' ') + 1\n\nsns.histplot(data=df, x='word_count', hue='label', kde=True)\nplt.show()","3c9ee29b":"from transformers import BertForSequenceClassification, BertTokenizer\n\n#Load the model\nmodel_name = \"bert-base-multilingual-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)","95dec6aa":"#Encode the label\ndf['label'] = pd.factorize(df['label'])[0]\n\n#Split dataset\nX = list(df['text'])\ny = list(df['label'])\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\n#Tokenize dataset\nX_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\nX_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)","a6ce7e11":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, label=None):\n        self.encodings = encodings\n        self.label = label\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.label:\n            item[\"label\"] = torch.tensor(self.label[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\ntrain_dataset = Dataset(X_train_tokenized, y_train)\nval_dataset = Dataset(X_val_tokenized, y_val)","394bdb16":"def compute_metrics(p):\n    preds, labels = p\n    preds = np.argmax(preds, axis=1)\n\n    accuracy = accuracy_score(y_true=labels, y_pred=preds)\n    f1 = f1_score(y_true=labels, y_pred=preds)\n\n    return {\"accuracy\":accuracy, \"f1\": f1}","f0ba1fec":"args = TrainingArguments(\n    output_dir=\"output\/bert\",\n    run_name=\"BERT Model\",\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=100,\n    per_device_eval_batch_size=100,\n    num_train_epochs=5,\n    logging_steps = 10,\n    seed=0,\n)\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\nwandb.finish()","a12bbaf4":"from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n\n#Load the model\nmodel_name = \"xlm-roberta-base\"\ntokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\nmodel = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)","6105fb6f":"X = list(df['text'])\ny = list(df['label'])\n\n#Split and tokenize dataset\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\nX_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=128)\nX_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=128)\n\ntrain_dataset = Dataset(X_train_tokenized, y_train)\nval_dataset = Dataset(X_val_tokenized, y_val)","b061afd9":"args = TrainingArguments(\n    output_dir=\"output\/xlmroberta\",\n    run_name=\"XLMRoBERTa Model\",\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=5,\n    logging_steps =10,\n    seed=0,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\nwandb.finish()","b95c47a2":"from transformers import DistilBertForSequenceClassification, DistilBertTokenizer \n\n#Load the model\nmodel_name = \"distilbert-base-multilingual-cased\"\ntokenizer = DistilBertTokenizer.from_pretrained(model_name)\nmodel = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)","36eaaefa":"X = list(df['text'])\ny = list(df['label'])\n\n#Split and tokenize dataset\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\nX_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\nX_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n\ntrain_dataset = Dataset(X_train_tokenized, y_train)\nval_dataset = Dataset(X_val_tokenized, y_val)","ea99839e":"args = TrainingArguments(\n    output_dir=\"output\/distilbert\",\n    run_name=\"DistilBERT Model\",\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=100,\n    per_device_eval_batch_size=100,\n    num_train_epochs=5,\n    logging_steps =10,\n    seed=0,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\nwandb.finish()","06d574d7":"df_rumi_org = read_file('.\/dataset\/org\/poetry_rumi.txt', 'Rumi')\ndf_saadi_org = read_file('.\/dataset\/org\/poetry_saadi.txt', 'Saadi')\n\ndf_org = pd.concat([df_rumi_org, df_saadi_org], ignore_index=True)\ndf = df_org.sample(frac=1).reset_index(drop=True)\ndf.head()","afe4b88c":"sns.catplot(data=df, x='label', kind='count')\nplt.show()","eac0d005":"df['word_count'] = df['text'].str.count(' ') + 1\n\nsns.histplot(data=df, x='word_count', hue='label', kde=True)\nplt.show()","92693274":"model_name = \"bert-base-multilingual-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\ndf['label'] = pd.factorize(df['label'])[0]\nX = list(df['text'])\ny = list(df['label'])\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\nX_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\nX_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n\ntrain_dataset = Dataset(X_train_tokenized, y_train)\nval_dataset = Dataset(X_val_tokenized, y_val)\n\nargs = TrainingArguments(\n    output_dir=\"output_org\/bert\",\n    run_name=\"BERT Model\",\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=256,\n    per_device_eval_batch_size=256,\n    num_train_epochs=5,\n    logging_steps = 10,\n    seed=0,\n)\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\nwandb.finish()","054fe11e":"#Load the model\nmodel_name = \"xlm-roberta-base\"\ntokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\nmodel = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n#Tokenize data\nX_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\nX_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n\ntrain_dataset = Dataset(X_train_tokenized, y_train)\nval_dataset = Dataset(X_val_tokenized, y_val)\n\nargs = TrainingArguments(\n    output_dir=\"output_org\/xlmroberta\",\n    run_name=\"XLMRoberta Model\",\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=256,\n    per_device_eval_batch_size=256,\n    num_train_epochs=5,\n    logging_steps = 10,\n    seed=0,\n)\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\nwandb.finish()","15cd8ddc":"model_name = \"distilbert-base-multilingual-cased\"\ntokenizer = DistilBertTokenizer.from_pretrained(model_name)\nmodel = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\nX_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\nX_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n\ntrain_dataset = Dataset(X_train_tokenized, y_train)\nval_dataset = Dataset(X_val_tokenized, y_val)\n\nargs = TrainingArguments(\n    output_dir=\"output_org\/distilbert\",\n    run_name=\"Distil BERT Model\",\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=256,\n    per_device_eval_batch_size=256,\n    num_train_epochs=5,\n    logging_steps = 10,\n    seed=0,\n)\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\nwandb.finish()","65b8c444":"### Loading Pretrained Model","a6e8948d":"# Original Dataset Classification\nWe saw result of three models on small dataset, so it this section we tend to test them for the original dataset.","f126be8d":"## DistrilBERT\nThird model is DistrilBERT. Main purpose of this model was reduce required resources rather than better performance. The can be found in [this link](https:\/\/huggingface.co\/distilbert-base-multilingual-cased). We assume that this model will be faster to train but has lower accuracy. Let's see the results!","9dbc4389":"In the rest of this notebook, [HuggingFace](https:\/\/huggingface.co\/)'s Models will help us to classify our poem! First model is:\n## BERT\nBidirectional Encoder Representations from Transformers (BERT for short) was introduced in [this paper](https:\/\/arxiv.org\/abs\/1810.04805) and we will use [this model](https:\/\/huggingface.co\/bert-base-multilingual-uncased) for our purpose.","7c1b9b85":"### Train Model\n\nNow the pretrain model is loaded and ready for learning Persian literature!\nBefore train it on dataset, we will write a function for compute evaluation metrics.","473cdd6b":"OK, almost done! Next code cell will define arguments for learning such as number of epochs and batch size, etc. then we will watch the model learning!","48fff8ee":"Both values of accuracy and loss are near each other so we can be sure that the model is not overfitted. The value of accuracy is about 0.8 which is, agian, lower than BERT but this time, consumed time is roughly the same. Probably this model would do better than BERT for more epoch (we can conclude that from the learning curves) but for 5 epochs its performance is not accetpable.","8ba06a7b":"![image.png](attachment:6f4fa668-397d-4981-a24f-0818fde51817.png)","d5aaadbc":"DistrilBERT had a great performance! It reached about 0.86 accuracy and F1 Score which is higher than RoBERTa but lower than BERT. Also its losses does not show any noticable sign of overfitting. On the other hand, DistrilBERT was a appreicable advantage! Average runtime for each epoch is about 15 seconds which is way lower than two other models.","96e9b29c":"How about each hemistich? What is the distribution of their number of words?","816cfe17":"![image.png](attachment:ccaef5e1-49a8-499f-af8c-2b1cb9e1e1eb.png)","1b261743":"As we expect, the accuracy and F1 socre got better due to larger dataset. Loss values are near each other and it suggest not high varicane and good generaliziation of model. Average time for each epoch was about 60 seconds. Let's see two other models.","5fe09348":"## Understanding Dataset \nLet us load the data and get a gist of it!","d2e779c8":"# Small Dataset Classification\nThere are two files `\/dataset\/small\/poetry_rumi.txt` and `\/dataset\/small\/poetry_saadi.txt`. Each of them has 1000 lines of text that we want to feed to our models as training and validation data.","3556e696":"# Installing and Importing","6107aa3b":"## XLMRoBERTa\n\nSecond model is XLMRoBERTa which is multilingual version of RoBERTa. This model was introduced in [this](https:\/\/arxiv.org\/abs\/1911.02116) and the correspondent model is [this model](https:\/\/huggingface.co\/xlm-roberta-base). This model is very heavier than BERT, it will take more time but is expected to return better results. ","4b8e79ee":"## DistilBERT","679abbec":"First, we need to load pretrained model from `transformers` library","41819ebf":"In `PyTorch` there is need to define our own dataset class, next few lines do it for us:","1769ea72":"![image.png](attachment:eb3f13bb-3b5f-4eb6-bdf7-f3f29859e618.png)","34e7bd6d":"BERT did a real good job! The accuracy and F1 socre reached 0.86 so the model has low bias. Difference between validation loss and training loss is a little bit large which suggests that the model is prone to overfit and need more data. Average runtime for each epoch was about 30 seconds.\nLet us see the next model and compare it with BERT!","ff4aecf4":"Next let us see each poet has what fraction of data:","29c5037e":"The data set need just a little bit of preprocssing, encoding the poet names, spliting for validation, and tokenizing each poem into its word. Next code cell will do them for us.","29ac06a6":"### Preprocess Data","56f57a7f":"### Loading Pretrained Model","3b8ee300":"## Conclusion\nFrom the above results we can conclude three main points:\n1. For limited resource, DistrilBERT is best. It will consume low amount of time and has nice performance.\n2. For small dataset, RoBERTa is not a good choice at all! It consume a lot of time and also it cannot reduce loss values.\n3. For high accuracy on small dataset, BERT is great. It will reach about 0.9 accuracy and of course it can go higher with better hyperparameters. ","48e84a12":"![image.png](attachment:a08e656a-2cdb-4ea4-bf42-2a3b44225908.png)","2dcbc0cf":"### Loading Pretrained Model","683e1915":"Despite the expectations, the accuracy was lower than BERT it may be beacause of small dataset and in the original dataset it probably does better than 0.8. Two loss values are very near each other and this is a sign of proper abbility for generalization of this model, and as we expect this model consume 40 seconds on average for each epoch.\n\nThere is also a third model so let's meet DistrilBERT.","c7f811b3":"## Conclusion\nSo following points can be inferred from results:\n1. BERT can have acceptable results for both small and large datasets but for large datasets, it could be time consuming.\n2. For using RoBERTa you need a lot of time and data, huge (not just large dataset)! In relativly small datasets it does not provide proper results. (It is possible that XLMRoBERTa does not support Persian good enough but unfortunately there is not multilingual RoBERTa model since now).\n3. DistilBERT is best for limited time resource. It can reach good accuracy in a little amount of time.","f7daecdc":"![image.png](attachment:37e1daa2-0d5c-4688-af56-1d7c7af3a9cb.png)","07994b0b":"![image.png](attachment:e9a04e7c-1618-47bb-b3ef-10720e564868.png)","7ec10c28":"## XLMRoBERTa","1e1a27c9":"Again, best part of DistrilBERT is its time, its roughly the half of two other models! About the accuracy and F1 score is not bad and its accuracy reached 0.9, maybe with more balanced data its F1 score could reach higher. Loss values are roughly the same and it has low variance and could learn better with more epochs. ","20452a83":"## Understanding Dataset ","20f87316":"# BERT","5e129a51":"In this notebook, various model of transformers family will be used for classifying Persain poems specifically Rumi and Saadi poets.\nWe have two datasets, one small version of them containing 1000 hemistiches, and one original version containing all of their poets.","fadfe52e":"### Preprocess Data"}}