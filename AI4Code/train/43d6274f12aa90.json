{"cell_type":{"ce23372b":"code","a4f2584a":"code","8f5947f8":"code","dfb86e97":"code","d8c8e213":"code","2dfb8782":"code","ed9966f3":"code","69b27251":"code","2a721e67":"code","21fe2ada":"code","f9229fda":"code","aa484116":"code","954d070b":"code","e1670be6":"code","4cf6c651":"code","1fa2b7b1":"code","a1541b84":"code","8b55b17b":"code","c1000dea":"code","c67d21bf":"code","c00814e9":"code","734f8972":"code","cc23e924":"markdown","d06555df":"markdown","c3e6d7a6":"markdown","344a6232":"markdown","44d74119":"markdown"},"source":{"ce23372b":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom nltk.corpus import stopwords\nimport re\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","a4f2584a":"df = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndf.head()","8f5947f8":"df = df[[\"is_sarcastic\" , \"headline\"]]\nprint (df.isna().sum())\nimport seaborn as sns\nsns.countplot(\"is_sarcastic\",data=df);","dfb86e97":"replace_list = {r\"i'm\": 'i am',\n                r\"'re\": ' are',\n                r\"let\u2019s\": 'let us',\n                r\"'s\":  ' is',\n                r\"'ve\": ' have',\n                r\"can't\": 'can not',\n                r\"cannot\": 'can not',\n                r\"shan\u2019t\": 'shall not',\n                r\"n't\": ' not',\n                r\"'d\": ' would',\n                r\"'ll\": ' will',\n                r\"'scuse\": 'excuse',\n                ',': ' ,',\n                '.': ' .',\n                '!': ' !',\n                '!!': ' !',\n                '!!!': ' !',\n                '?': ' ?',\n                '??': ' ?',\n                '???': ' ?',\n                '\\s+': ' '}\n\nimport string\nfrom string import punctuation\nstop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)\n\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n\ndef text_preprocess (x):\n    processed_feature = re.sub(r'\\W', ' ', str(x))\n\n    # Remove all single characters\n    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n\n    # Remove single characters from the start\n    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n\n    # Substituting multiple spaces with single space\n    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n\n    # Removing prefixed 'b'\n    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n\n    # Converting to Lowercase\n    processed_feature = processed_feature.lower()\n    \n    #remove square brackets\n    processed_feature = re.sub('\\[[^]]*\\]', '', processed_feature)\n    \n    # remove url\n    processed_feature = re.sub(r'http\\S+', '', processed_feature)\n    \n    \n    # remove stopwords\n    processed_feature = remove_stopwords(processed_feature)\n    \n    \n    for s in replace_list:\n        processed_feature = processed_feature.replace(s, replace_list[s])\n    #text = ' '.join(text.split())\n    \n    return processed_feature\n\ndf['headline'] = df['headline'].apply(text_preprocess)\ndf.sample()","d8c8e213":"from wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (20,20)) # Text that is Not Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 0].headline))\nplt.imshow(wc , interpolation = 'bilinear')","2dfb8782":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(df['headline'].values)\nX = tokenizer.texts_to_sequences(df['headline'].values)\nX = pad_sequences(X)\nprint (X.shape)","ed9966f3":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","69b27251":"Y = pd.get_dummies(df['is_sarcastic']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","2a721e67":"batch_size = 128\nhistory = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data=(X_test,Y_test))","21fe2ada":"# Let's obtain our predictions on our test dataset\npredictions = model.predict(X_test)\npreds = [np.argmax(y) for y in predictions]\n# and the true predictions as vector\ny_test = [np.argmax(y) for y in Y_test]\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,preds))\nprint(classification_report(y_test,preds))\nprint(\"Accuracy {0:.2f}%\".format(100*accuracy_score(y_test, preds)))","f9229fda":"history.history","aa484116":"import matplotlib.pyplot as plt\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\nfig.suptitle(\"Performance \")\nax1.plot(history.history['accuracy'])\nax1.plot(history.history['val_accuracy'])\nvline_cut = np.where(history.history['val_accuracy'] == np.max(history.history['val_accuracy']))[0][0]\nax1.axvline(x=vline_cut, color='k', linestyle='--')\nax1.set_title(\"Model Accuracy\")\nax1.legend(['train', 'test'])\n\nax2.plot(history.history['loss'])\nax2.plot(history.history['val_loss'])\nvline_cut = np.where(history.history['val_loss'] == np.min(history.history['val_loss']))[0][0]\nax2.axvline(x=vline_cut, color='k', linestyle='--')\nax2.set_title(\"Model Loss\")\nax2.legend(['train', 'test'])\nplt.show()","954d070b":"# !wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n# !unzip -q glove.6B.zip","e1670be6":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","4cf6c651":"path_to_glove_file = \"\/kaggle\/input\/glove6b\/glove.6B.100d.txt\"\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))","1fa2b7b1":"vocab_size = len(tokenizer.word_index) + 1\nprint (vocab_size)","a1541b84":"num_tokens = vocab_size + 2\nembedding_dim = 100\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","8b55b17b":"embed_dim = 100\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(num_tokens, embed_dim, weights = [embedding_matrix]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","c1000dea":"Y = pd.get_dummies(df['is_sarcastic']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","c67d21bf":"batch_size = 128\nhistory = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data=(X_test,Y_test))","c00814e9":"import matplotlib.pyplot as plt\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\nfig.suptitle(\"Performance with glove\")\nax1.plot(history.history['accuracy'])\nax1.plot(history.history['val_accuracy'])\nvline_cut = np.where(history.history['val_accuracy'] == np.max(history.history['val_accuracy']))[0][0]\nax1.axvline(x=vline_cut, color='k', linestyle='--')\nax1.set_title(\"Model Accuracy\")\nax1.legend(['train', 'test'])\n\nax2.plot(history.history['loss'])\nax2.plot(history.history['val_loss'])\nvline_cut = np.where(history.history['val_loss'] == np.min(history.history['val_loss']))[0][0]\nax2.axvline(x=vline_cut, color='k', linestyle='--')\nax2.set_title(\"Model Loss\")\nax2.legend(['train', 'test'])\nplt.show()","734f8972":"# Let's obtain our predictions on our test dataset\npredictions = model.predict(X_test)\npreds = [np.argmax(y) for y in predictions]\n# and the true predictions as vector\ny_test = [np.argmax(y) for y in Y_test]\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,preds))\nprint(classification_report(y_test,preds))\nprint(\"Accuracy {0:.2f}%\".format(100*accuracy_score(y_test, preds)))","cc23e924":"### Model","d06555df":"### Training","c3e6d7a6":"### Tokenizer","344a6232":"Now, let's prepare a corresponding embedding matrix that we can use in a Keras Embedding layer. It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary.","44d74119":"### Validation"}}