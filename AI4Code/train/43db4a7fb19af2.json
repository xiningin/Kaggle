{"cell_type":{"36645288":"code","3278b3fd":"code","94a0fe9d":"code","f930c728":"code","d99cd6b5":"code","860c04c6":"code","661667c0":"code","89051102":"code","d1c51586":"code","f2da18fc":"code","bb2eb587":"code","72a0c1fd":"code","3b132e27":"code","2acdfaf9":"code","f3496214":"markdown","c12b8a87":"markdown","599bf3ac":"markdown","0241a5fb":"markdown","e6bbecf7":"markdown","75874212":"markdown","1ab5257e":"markdown","2ad845ea":"markdown","6269bc1b":"markdown","e9fd08a4":"markdown","434c3a0d":"markdown","0c0ba845":"markdown","db1d3de1":"markdown","7aefb56f":"markdown","9afac533":"markdown","f186afbe":"markdown","5227f769":"markdown"},"source":{"36645288":"import numpy as np\nimport pandas as pd\nfrom pandas import Series,DataFrame\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.model_selection import StratifiedKFold","3278b3fd":" pd.set_option('display.width', 1000) \n\npd.set_option('display.max_rows', 200) \n\npd.set_option('display.max_columns', 200) ","94a0fe9d":"train = pd.read_csv('..\/input\/train.csv')\n\ntest = pd.read_csv('..\/input\/test.csv')\n","f930c728":"\nsum_id = test['Id']\ndel test['Id']\n\nY = train.Target.values.astype(int)\n\ndel train['Target']\n\nall_data = pd.concat((train.loc[:,'v2a1':'agesq'],\n                      test.loc[:,'v2a1':'agesq']))\ndel all_data['idhogar']","d99cd6b5":"#------------ fill NaNs --------------\n\nall_data.isnull().any()\n\nall_data[\"v2a1\"].fillna(all_data[\"v2a1\"].median(), inplace=True)\nall_data[\"v18q1\"].fillna(0, inplace=True)\nall_data[\"rez_esc\"].fillna(0, inplace=True)\nall_data[\"meaneduc\"].fillna(all_data[\"meaneduc\"].median(), inplace=True)\nall_data[\"SQBmeaned\"].fillna(all_data[\"SQBmeaned\"].median(), inplace=True)","860c04c6":"#------------- digitalizing -----------\n\nall_data.loc[all_data[\"dependency\"]==\"yes\",\"dependency\"]=0.25      \nall_data.loc[all_data[\"dependency\"]==\"no\",\"dependency\"]=8\nall_data.loc[all_data[\"edjefe\"]==\"yes\",\"edjefe\"]=1\nall_data.loc[all_data[\"edjefe\"]==\"no\",\"edjefe\"]=0      \nall_data.loc[all_data[\"edjefa\"]==\"yes\",\"edjefa\"]=1\nall_data.loc[all_data[\"edjefa\"]==\"no\",\"edjefa\"]=0  \n\nall_data['dependency'] = all_data['dependency'].astype('float')\nall_data['edjefe'] = all_data['edjefe'].astype('float')\nall_data['edjefa'] = all_data['edjefa'].astype('float')\n\n\ntrain = all_data[:train.shape[0]]\ntest = all_data[train.shape[0]:]","661667c0":"#------------------ Predicting ------------------------------------------------\n\n\n#---------- 1 RandomForest ----------------- Score: 0.366\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(train, Y)\n\n\nX_train, X_val, y_train,y_val = train_test_split(train,Y,test_size=0.3, random_state=42) \nprint('trainning accuracy\uff1a\\n',random_forest.score(X_train, y_train))\nprint('validation accuracy\uff1a\\n',random_forest.score(X_val, y_val))\n\nprint('RandomForest Accuracy\uff1a\\n',random_forest.score(train, Y))\n\npred_RF = random_forest.predict(test)\n\nsol_RF = pd.DataFrame({'Id':sum_id.values, 'Target':pred_RF}) \n\nsol_RF.to_csv('pred_RF.csv',index=None) ","89051102":"#---------- 2 DecisionTree ----------------- Score: 0.352\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nDT=DecisionTreeClassifier()\n\nDT.fit(train,Y)\nX_train, X_val, y_train,y_val = train_test_split(train,Y,test_size=0.3, random_state=42)\n\nprint('Accuracy on training\uff1a\\n',DT.score(X_train, y_train))\nprint('Accuracy on validation\uff1a\\n',DT.score(X_val, y_val))\nprint('DecisionTree Accuracy\uff1a\\n',DT.score(train, Y))\n\npred_DT = (DT.predict(test))\n\nsol_DT = pd.DataFrame({'Id':sum_id.values, 'Target':pred_DT}) \n\nsol_DT.to_csv('pred_DT.csv',index=None) \n","d1c51586":"#---------- 3 LogisticRegression ----------------- Score: 0.253\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\n\nLR.fit(train, Y)\nX_train, X_val, y_train,y_val = train_test_split(train,Y,test_size=0.3, random_state=42) \nprint('Accuracy on training\uff1a\\n',LR.score(X_train, y_train)) \nprint('Accuracy on validation\uff1a\\n',LR.score(X_val, y_val))\nprint('LogisticRegression Accuracy\uff1a\\n',LR.score(train, Y))\n\npred = LR.predict(test)\n\npred = pd.DataFrame({'Id':sum_id.values, 'Target':pred}) \n\npred.to_csv('pred_LR.csv',index=None) \n","f2da18fc":"#---------- 4 kNN ----------------- Score: 0.308\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\n\nknn.fit(train, Y)\nX_train, X_val, y_train,y_val = train_test_split(train,Y,test_size=0.3, random_state=42) \n\nprint('Accuracy on training\uff1a\\n',knn.score(X_train, y_train)) \nprint('Accuracy on validation\uff1a\\n',knn.score(X_val, y_val))\nprint('kNN Accuracy\uff1a\\n',knn.score(train, Y))\n\npred = knn.predict(test)\n\npred = pd.DataFrame({'Id':sum_id.values, 'Target':pred}) \n\npred.to_csv('pred_kNN.csv',index=None)","bb2eb587":"#---------- 5 NaiveBayes Gaussian ----------------- Score: 0.373\n \nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\n\ngaussian.fit(train, Y)\nX_train, X_val, y_train,y_val = train_test_split(train,Y,test_size=0.3, random_state=42) \n\nprint('Accuracy on training\uff1a\\n',gaussian.score(X_train, y_train)) \nprint('Accuracy on validation\uff1a\\n',gaussian.score(X_val, y_val))\nprint('gaussian Accuracy\uff1a\\n',gaussian.score(train, Y))\n\npred_NB = gaussian.predict(test)\n\nsol_NB = pd.DataFrame({'Id':sum_id.values, 'Target':pred_NB})\n\nsol_NB.to_csv('pred_NaiveBayes.csv',index=None) \n","72a0c1fd":"#---------- 6 LinearRegression ----------------- Grade: 0.346\n# doesn't output int\nfrom sklearn.linear_model import LinearRegression\n\nLR = LinearRegression()\n\nLR.fit(train, Y)\nX_train, X_val, y_train,y_val = train_test_split(train,Y,test_size=0.3, random_state=42) \n\nprint('Accuracy on training\uff1a\\n',LR.score(X_train, y_train)) \nprint('Accuracy on validation\uff1a\\n',LR.score(X_val, y_val))\nprint('LinearRegression Accuracy\uff1a\\n',LR.score(train, Y))\n\npred = LR.predict(test)\n  \npred = pd.DataFrame({'Id':sum_id.values, 'Target':pred}) \n\npred.loc[pred[\"Target\"] < 1.5,\"Target\"] = 1\npred.loc[(1.5 <= pred[\"Target\"]) & (pred[\"Target\"] < 2.5),\"Target\"] = 2\npred.loc[(2.5 <= pred[\"Target\"]) & ( pred[\"Target\"] < 3.5),\"Target\"] = 3\npred.loc[3.5 <= pred[\"Target\"],\"Target\"] = 4\npred['Target'] = pred['Target'].astype('int')\npred.to_csv('pred_Linear.csv',index=None) \n","3b132e27":"#========================== lasso ridge xgb ============================== # Score: 0.363\n\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, train, Y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)\n\nmodel_ridge = Ridge()\n\nalphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]\n\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation - Just Do It\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\n\nprint(cv_ridge.min())\n\nmodel_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(train, Y)\n\nprint(rmse_cv(model_lasso).mean())\n\ncoef = pd.Series(model_lasso.coef_, index = train.columns)\n\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n\nimp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")\n\n#let's look at the residuals as well:\nmatplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model_lasso.predict(train), \"true\":Y})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\n\n#-------------- xgboosting -------------\n\nimport xgboost as xgb\n\ndtrain = xgb.DMatrix(train, label = Y)\ndtest = xgb.DMatrix(test)\n\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n\nmodel.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n\nmodel_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\nmodel_xgb.fit(train, Y)\n\nxgb_preds = model_xgb.predict(test)\nlasso_preds = model_lasso.predict(test)\n\npredictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})\npredictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")\n\npreds = 0.3*lasso_preds + 0.7*xgb_preds\n\nsolution = pd.DataFrame({\"Id\":sum_id.values, \"Target\":preds})\nsolution.loc[solution[\"Target\"] < 1.5,\"Target\"] = 1\nsolution.loc[(1.5 <= solution[\"Target\"]) & (solution[\"Target\"] < 2.5),\"Target\"] = 2\nsolution.loc[(2.5 <= solution[\"Target\"]) & ( solution[\"Target\"] < 3.5),\"Target\"] = 3\nsolution.loc[3.5 <= solution[\"Target\"],\"Target\"] = 4\nsolution['Target'] = solution['Target'].astype('int')\nsolution.to_csv(\"ridge_sol.csv\", index = False) ","2acdfaf9":"#===================== Lightgbm ========================================= Score: 0.424\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\nclf = lgb.LGBMClassifier(class_weight='balanced', boosting_type='dart',\n                         drop_rate=0.9, min_data_in_leaf=100, \n                         max_bin=255,\n                         n_estimators=500,\n                         bagging_fraction=0.01,\n                         min_sum_hessian_in_leaf=1,\n                         importance_type='gain',\n                         learning_rate=0.1, \n                         max_depth=-1, \n                         num_leaves=31)\nkf = StratifiedKFold(n_splits=5, shuffle=True)\n# partially based on https:\/\/www.kaggle.com\/c0conuts\/xgb-k-folds-fastai-pca\nY = pd.Series(Y)\npredicts = []\n\nfor train_index, test_index in kf.split(train, Y):\n    print(\"###\")\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = Y.iloc[train_index], Y.iloc[test_index]\n    clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n            early_stopping_rounds=20)\n    predicts.append(clf.predict(test))\n    \npredict = pd.DataFrame(np.array(sum_id),\n                             columns=['Id'],\n                             index=test.index)\npredict['Target'] = np.array(predicts).mean(axis=0).round().astype(int)\npredict.to_csv('predict.csv', index = False)","f3496214":"**3. LogisticRegression**","c12b8a87":"**5. NaiveBayes**","599bf3ac":"**Thanks if you read it through,  please leave a comment if you have any question, any feedback will be appreciated!**","0241a5fb":"This is a comprehensive kernel including **data cleansing** , **traditional machine learning** algorithms, **xgboost**  combined with lasso and **lightgbm**.\n\nBut there won't be too much feature engineering and any other tricks. Here I just skip them and the farthest I  go is just to make the raw data work for algorithm. Still feature engineering is absolutely when you need to get a higher score    : )\n","e6bbecf7":" **2. Decision Tree**","75874212":"> fill NaNs with median and 0","1ab5257e":"> **import lib**\n\nWe import the libraries we need here all at once.","2ad845ea":"> **Predicting**\n\nHere are some traditional mechine learning algorithm applying on this problem.","6269bc1b":">** Data Cleansing**\n\nThis data cleaning part is kind of raw, it only include filling NaNs and type tranforming.\n\nAnd I found a curious question that deleting redundant variables like 'SQB's will decrease the below algorithms' score. So I am gonna let it be : ).","e9fd08a4":"> **Set**\n\nJust to show the data more completely in interactive environment.","434c3a0d":"> There are some variables are 'object' type,we need to transform it into int or float.","0c0ba845":"**6. LinearRegression**\n\nApplying LinearRegression on this issue this way won't work as a classifier and it generate floats.\n\nSo I manually transform the result into int type.\n\nIt is a kind of indirect way.","db1d3de1":"> **XGBoost combined with lasso**","7aefb56f":"**1.RandomForest**","9afac533":"> **Lightgbm**\n\nThis LGB with early-stopping is from[https:\/\/www.kaggle.com\/ischurov\/more-feature-eng-lgb-5-fold-early-stopping](http:\/\/)\n\nThanks to his work.","f186afbe":"**4. kNN** ","5227f769":"> **Read data file**"}}