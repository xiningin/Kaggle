{"cell_type":{"d8d9c262":"code","d0d4c596":"code","015992b8":"code","b68daf98":"code","d1136428":"code","067a424e":"code","3fbd9802":"code","0e4931e1":"code","67d8e692":"code","bb2fb2e0":"code","55e6002c":"code","928b1ba2":"code","35708778":"code","d8de54c7":"code","756ea618":"code","da57c662":"code","97a7fd3d":"code","91f9b85b":"code","faaff0d8":"code","f822fe46":"code","58db54be":"code","2d124005":"code","99293526":"code","d39d2034":"code","1eebbe0f":"code","62532aea":"code","6036e3c2":"code","7c94cba0":"code","74982fe4":"code","a0f62c49":"code","3eb44c41":"code","1be99c1d":"code","ddf92a14":"code","30116f41":"code","ebca1ba9":"code","b9aa3cc2":"code","ca2870d8":"code","ad130353":"code","ddfc1e8e":"code","a3d0d77c":"code","81ae00ac":"code","3cd11ae7":"code","77cec9e3":"code","05e295d0":"code","03c7a2d3":"code","f9ac27ca":"code","a5c266cb":"code","a3a6dcb6":"code","8b5f9f0c":"code","6b9b74f0":"code","90bb74df":"code","5640dae2":"code","1a302f80":"code","52b03d8c":"code","764f6445":"code","f812f7cf":"markdown","f7d15e07":"markdown","4955b319":"markdown","4e328fa1":"markdown","f58cd717":"markdown","07cfd784":"markdown","4fe2e6b7":"markdown","f51f3272":"markdown","cc4d97dc":"markdown","bd6aa88e":"markdown","d1e759dd":"markdown","439f6aad":"markdown","ef3f2f41":"markdown","cbfcf281":"markdown","299c5254":"markdown","8e1887a5":"markdown","7732ca18":"markdown","9b8caf6d":"markdown","65a1fe6b":"markdown","24fcbe9d":"markdown","8d256e0f":"markdown","e2f6713c":"markdown","5e59a5fc":"markdown","ecfc8191":"markdown","d05be3e4":"markdown","78e631f8":"markdown","dc7a0651":"markdown","ddf12db4":"markdown","9254af9f":"markdown"},"source":{"d8d9c262":"import warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","d0d4c596":"warnings.filterwarnings('ignore') #shutting down warnings","015992b8":"from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n","b68daf98":"iris = datasets.load_iris()","d1136428":"#Two classes are versicolor and virignica\n#Two Features are Sepal Width and Petal length\nX, y = iris.data[50:, [1,2]], iris.target[50:]","067a424e":"#let's convert two classes into numerical labels\n\nle = LabelEncoder()\ny = le.fit_transform(y)","3fbd9802":"np.unique(y)","0e4931e1":"#splitting the data\n#Here I am using stratification it will make sure that training and test set have same proportion of classes\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 1, stratify = y)","67d8e692":"#lets train three classifier, logistic regression, decision tree, k-nearest neighbors\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline","bb2fb2e0":"clf1 = LogisticRegression(\n        penalty='l2',\n        C = 0.001,\n        random_state = 1\n)\n\nclf2 = DecisionTreeClassifier(\n            max_depth=1,\n            criterion='entropy',\n            random_state=0\n)\n\nclf3 = KNeighborsClassifier(\n        n_neighbors=1,\n        p = 2,\n        metric='minkowski'\n)","55e6002c":"pipe1 = make_pipeline(StandardScaler(), clf1) #it is important to standarize data before LogisticRegression","928b1ba2":"pipe2 = make_pipeline(StandardScaler(), clf3) #it is important to standarize data before using KNN classification","35708778":"\nclf_labels = ['Logistic Regression', 'Decision Tree', 'KNN']\n\n\nfor  label, clf in zip(clf_labels, [pipe1, clf2, pipe2]):\n    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc')\n    \n    print(\"ROC AUC: {:.2f} (+\/- {:.2f}) {}\".format(np.mean(scores), np.std(scores), label))\n    ","d8de54c7":"from sklearn.ensemble import VotingClassifier","756ea618":"en = VotingClassifier(\n    estimators=[('m1',pipe1),('m2',clf2),('m3',pipe2)],\n    voting='soft'\n)","da57c662":"clf_labels  += ['Ensemble']","97a7fd3d":"for label, clf in zip(clf_labels, [pipe1, clf1, pipe2, en]):\n    scores = cross_val_score(estimator=clf, scoring='roc_auc', cv = 10, X = X_train, y = y_train)\n    print(\"ROC AUC: {:.2f} (+\/- {:.2f}) {}\".format(np.mean(scores), np.std(scores), label))","91f9b85b":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc","faaff0d8":"all_clf = [pipe1, clf2, pipe2, en]\n\ncolors = ['black', 'orange', 'blue', 'green']\nlinestyles = [':', '--', '-.', '-']\n\nfor clf, label, clr, ls in zip(all_clf, clf_labels, colors, linestyles):\n    #assuming the label of positive class is 1 remember that roc curve is tpr vs fpr\n    \n    probas = clf.fit(X_train, y_train).predict_proba(X_test)[:,1]\n    fpr, tpr, threshods = roc_curve(y_true=y_test, y_score=probas)\n    \n    roc_auc = auc(x=fpr, y = tpr)\n    \n    plt.plot(fpr, tpr, color = clr, linestyle=ls, label = '{} (auc = {:.2f})'.format(label, roc_auc))\n\nplt.legend(loc = 'lower right')\nplt.plot([0,1],[0,1], linestyle = '--', color = 'gray', linewidth = 2)\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.grid(alpha = 0.5)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","f822fe46":"#we will standarize our data for visualization to be on same scale even though our pipeline have it for the two models.\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\n\nfrom itertools import product #it does cross product \/ cartesian product\n\nx1_min, x1_max = X_train_std[:,0].min() - 1, X_train_std[:,0].max() + 1\nx2_min, x2_max = X_train_std[:,1].min() - 1, X_train_std[:,1].max() + 1\n\nxx1, xx2  = np.meshgrid(\n            np.arange(x1_min, x1_max , 0.1),\n            np.arange(x2_min, x2_max, 0.1)\n)","58db54be":"#let's make subplots for our 2 X 2 figure first\n\nfig, axis = plt.subplots(ncols=2, nrows=2, sharex='col', sharey='row' ,figsize = (7,5))\n\nfor idx, clf, tt in zip(product([0,1],[0,1]), all_clf, clf_labels):\n    clf.fit(X_train_std, y_train) #fitting our model\n    \n    Z = clf.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    \n    #since tt is a tupe with (x, y)\n    axis[idx[0], idx[1]].contourf(xx1, xx2, Z, alpha = 0.3)\n    axis[idx[0], idx[1]].scatter(X_train_std[y_train==0, 0], X_train_std[y_train==0, 1], c = 'blue', marker = '^', s = 50)\n    axis[idx[0], idx[1]].scatter(X_train_std[y_train==1,0], X_train_std[y_train==1, 1], c = 'green', marker = 'o', s = 50)\n    axis[idx[0], idx[1]].set_title(tt)\n\nplt.text(-3.5, -4.7, s = 'Sepal width [standarized]', ha = 'center', va = 'center', fontsize = 12)\nplt.text(-12.5, 4.5, s = 'Petal length [standarized]', ha = 'center', va = 'center', fontsize = 12, rotation = 90)\nplt.show()","2d124005":"#let's tune this ensemble model, but for that first let's know what the variable get called\nen.get_params()","99293526":"from sklearn.model_selection import GridSearchCV","d39d2034":"params = {\n    'm1__logisticregression__C': [0.001, 0.1, 100.0],\n    'm2__max_depth':[1,2]\n}","1eebbe0f":"grid = GridSearchCV(estimator=en, cv=10, scoring='roc_auc', param_grid=params)","62532aea":"grid.fit(X_train, y_train)","6036e3c2":"grid.best_score_","7c94cba0":"grid.best_params_","74982fe4":"df_wine = pd.read_csv('..\/input\/wine-data\/wine.data', header = None)","a0f62c49":"df_wine.columns = [\n    'class label',\n    'Alcohol',\n    'Malic Acid',\n    'Ash',\n    'Alcalinity of ash',\n    'Magnesium',\n    'Total phenols',\n    'Flavanoids',\n    'Nonflavanoid phenols',\n    'Proanthocyanins',\n    'Color intensity',\n    'Hue',\n    '0D280\/0D315 of diluted wines',\n    'Proline'\n]","3eb44c41":"df_wine.head()","1be99c1d":"#here we will only consider wine class 2 and 3\ndf_wine = df_wine[df_wine['class label'] != 1]\n\n#here we will only consider two features Alochol, OD280\/0D315 of diluted wines\ny = df_wine['class label'].values\nX = df_wine[['Alcohol', '0D280\/0D315 of diluted wines']].values","ddf92a14":"#let's label encode our class labels\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ny = le.fit_transform(y)","30116f41":"np.bincount(y), np.unique(y)","ebca1ba9":"#let's split our data in 80:20 ration for training and testing set\nfrom sklearn.model_selection import train_test_split","b9aa3cc2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1, stratify = y)","ca2870d8":"from sklearn.ensemble import BaggingClassifier","ad130353":"#we will be using decision tree with no pruning for our Bagging classifier\ntree = DecisionTreeClassifier(\n        criterion='entropy',\n        random_state = 1,\n        max_depth = None\n)","ddfc1e8e":"'''\n    Here, we will be making 500 estimator,\n    each estimator will train for random sample with replacement (Bootstrap = True),\n    each estimator will train for all feature without replacement (Bootstrap = False)\n    \n    See Bagging Classifier doc for understanding max_sample\n    and max_feature, it is differnt for int value and float value\n    for int, max_feature means max_feature for sampling.\n    For float, max_feature means max_feature * X.shape[1] for sampling\n    similary for max_sample\n    for int; max_sample\n    for Float; max_sample * X.shape[0]\n\n'''\n\n\nbag = BaggingClassifier(\n        base_estimator=tree,\n        n_estimators=500,\n        max_samples=1.0,\n        max_features=1.0,\n        bootstrap=True,\n        bootstrap_features=False,\n        n_jobs=-1,\n        random_state=1\n)","a3d0d77c":"from sklearn.metrics import accuracy_score","81ae00ac":"tree  = tree.fit(X_train, y_train) #fitting our decision tree first","3cd11ae7":"y_train_pred = tree.predict(X_train) #how model perform on already seen data\ny_test_pred = tree.predict(X_test)   #how model perform on unseen data\n\ntree_training_set_accuracy = accuracy_score(y_pred=y_train_pred, y_true=y_train)\ntree_testing_set_accuracy = accuracy_score(y_pred=y_test_pred, y_true=y_test)\n\nprint('Decision tree test\/train score {:.3f}\/{:.3f}'.format(tree_training_set_accuracy, tree_testing_set_accuracy))","77cec9e3":"bag.fit(X_train, y_train)","05e295d0":"y_train_pred = bag.predict(X_train)\ny_test_pred = bag.predict(X_test)\n\nbag_training_set_accuracy = accuracy_score(y_true=y_train, y_pred=y_train_pred)\nbag_testing_set_accuracy  = accuracy_score(y_true=y_test, y_pred = y_test_pred)\n\nprint('Bagging train\/test accuracies: {:.3f}\/{:.3f}'.format(bag_training_set_accuracy, bag_testing_set_accuracy))","03c7a2d3":"#let's see decision region for both the models\n\nx1_min, x1_max = X_train[:,0].min() - 1, X_train[:,0].max() + 1\nx2_min, x2_max = X_train[:,1].min() - 1, X_train[:,1].max() + 1\n\nxx1, xx2 = np.meshgrid(\n                np.arange(x1_min, x1_max, 0.1),\n                np.arange(x2_min, x2_max, 0.1)\n)","f9ac27ca":"fig, ax = plt.subplots(nrows=1, ncols=2, sharex='col', sharey='row', figsize = (8,3))\n\nfor idx, clf, tt in zip([0,1], [tree, bag], ['Decision tree', 'Bagging']):\n    Z = clf.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    \n    ax[idx].contourf(xx1, xx2, Z, alpha = 0.3)\n    ax[idx].scatter(X_train[y_train == 0, 0], X_train[y_train==0, 1], c = 'blue', marker = '^')\n    ax[idx].scatter(X_train[y_train == 1, 0], X_train[y_train == 1,1], c = 'green', marker = 'o')\n    ax[idx].set_title(tt)\n\nax[0].set_ylabel('Alcohol', fontsize = 12)\nplt.text(10.2, -1.2, s = '0d280\/0d315 of dilute wines', ha = 'center', va = 'center', fontsize = 12)\nplt.show()","a5c266cb":"from sklearn.ensemble import AdaBoostClassifier","a3a6dcb6":"#let's use Decision tree for our base estimator, remeber decision stump\n#creating decision tree stump having max_depth 1 (weak learner)\n\ntree = DecisionTreeClassifier(\n        criterion='entropy',\n        random_state =1 ,\n        max_depth = 1\n\n)","8b5f9f0c":"ada = AdaBoostClassifier(\n        base_estimator=tree,\n        n_estimators=500,\n        learning_rate=0.1,\n        random_state=1\n)","6b9b74f0":"#let's train our tree model for comparison \ntree = tree.fit(X_train, y_train)","90bb74df":"y_train_pred = tree.predict(X_train) #seeing how good model generalizes over seen data.\ny_test_pred =  tree.predict(X_test)  #seeing how good model generalizes over unseen data.\n\ntree_training_set_accuracy = accuracy_score(y_true=y_train, y_pred=y_train_pred)\ntree_test_set_accuracy  = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n\nprint('Decision Tree Stump Train\/Test accuracies {:.3f}\/{:.3f}'.format(tree_training_set_accuracy, tree_test_set_accuracy))","5640dae2":"ada = ada.fit(X_train, y_train)","1a302f80":"y_train_pred = ada.predict(X_train) #seeing how good model generalizes over seen data.\ny_test_pred = ada.predict(X_test)   #seeing how good model generalizes over unseen data.\n\nada_training_set_accuracy = accuracy_score(y_train, y_train_pred)\nada_testing_set_accuracy = accuracy_score(y_test, y_test_pred)\n\nprint('AdaBoost Train\/Test accuracies: {:.3f}\/{:.3f}'.format(ada_training_set_accuracy, ada_testing_set_accuracy))","52b03d8c":"#let's check decision region for both the models\n\nx1_min, x1_max = X_train[:,0].min() - 1, X_train[:,0].max() + 1\nx2_min, x2_max = X_train[:,1].min() - 1, X_train[:,1].max() + 1\n\nxx, yy = np.meshgrid(\n            np.arange(x1_min, x1_max, 0.1),\n            np.arange(x2_min, x2_max, 0.1)\n)","764f6445":"fig, ax = plt.subplots(ncols=2, nrows=1, sharex='col', sharey='row', figsize = (8,3))\n\nfor idx, clf, tt in zip([0,1], [tree, ada], ['Decision Tree','Ada Boost']):\n    Z = clf.predict(np.array([xx.ravel(), yy.ravel()]).T)\n    Z = Z.reshape(xx.shape)\n    \n    ax[idx].contourf(xx, yy, Z, alpha = 0.3)\n    ax[idx].scatter(X_train[y_train==0,0], X_train[y_train==0,1], c= 'blue', marker = '^')\n    ax[idx].scatter(X_train[y_train==1,0], X_train[y_train==1, 1],  c= 'green', marker = 'o')\n    ax[idx].set_title(tt)\n\nax[0].set_ylabel('Alchol', fontsize = 12)\nplt.text(10.2, -0.5, s= '0D280\/0D315 of diluted wines', ha='center', va='center', fontsize = 12)\nplt.show()","f812f7cf":"**Please Upvote if you like this notebook**","f7d15e07":"**Note-**<br>\nBoosting can lead to decrease in bias as well as variance compared to bagging models. In practice however, boosting algorithms such as AdaBoost are also known for their high variance, i.e, the tendancy to overfit the training data.","4955b319":"Consider watching statsquest youtube video on adaboost for better understanding of the concept","4e328fa1":"*Ensemble can be done by combining different classifier models or by using same collection of models and training them with different subsets of training set.*","f58cd717":"**Steps in AdaBoost**\n\n1. Assign equal weight to each sample that will be equal to $ \\frac {1} {Total Number of samples} $\n\n\n2. Calculate Total Error for misclassified sample by the model.  $ TE = \\sum _{j=1} ^{j=m} w _j $\n    \n    here, m is total number of misclassified sample <br> $ w _j $ is weight of each misclassified sample\n    \n \n3. Calculate Weighted Error rate $ \\alpha _j $ (also called as Amount of say)  $ \\alpha _j = \\frac {1} {2} \\log \\frac {1-TE} {TE} $\n\n\n4. Now we will update the weight for each sample\n\n    For Misclassified sample, $ New Weight = Weight * e ^{Amount of Say} $, It will increase the weight for misclassified sample<br>\n    For Correct classif. sample, $ New Weight = Weight * e ^ {Amount of Say} $, It will decrease the weight for correct classified sample.\n    \n    \n5. We will normalize the weight so that it added to 1. $ w = \\frac {w} {\\sum w} $\n\n6. Compute the final prediction: $ ypred = (\\sum _{j=1} ^{m} (\\alpha _j * predict(C _j, X)) > 0) $, m is the total number of samples.\n ","07cfd784":"As we can observe in the above figure the decision boudary made by our ensemble classifier looks like hybrid of individual classifier.","4fe2e6b7":"**Bagging can improve the accuracy of unstable models and decrease the degree of overfitting**<br>\n\nBy default bagging models have low bias and low variance, so bagging is good for complex model that get overfitted easily","f51f3272":"By looking at the decision regions, we can see that the decision boundary of the AdaBoost model is substantially more complex than the decision boundary of the decision stump. In addition, we note that AdaBoost model seperates the feature space very similarly to the bagging classifier that we trained in the previous section.","cc4d97dc":"## Learning with ensembles","bd6aa88e":"As we can observe in the resulting ROC, the ensemble also performs well on the test set (ROC AUC = 0.95). Also we can observe that the logistic regression classifier performs similary well on the same dataset, which is probably due to the high variance (in this case senstivity of how we split the dataset) given small size of the dataset","d1e759dd":"As we can observe above our ensemble model perform better than any individual model","439f6aad":"In Bagging instead of using the same training set to fit the individual classifiers in the ensemble, we draw *bootstrap samples* (with replacement, can also carry some duplicates) from the initial training set, which is why bagging is also know as **Bootstrap Aggregatror**. \n\nAggregator here means that we club the result from individual classifier in ensemble to make the decision bacially voting.\n\n*Note:- Random Forest is a special case of Bagging where we also use random feature subsets for fitting the individual decision trees*","ef3f2f41":"# AdaBoost (Adaptive Boosting)\n\n**Making Weak learner into Strong Learner**","cbfcf281":"We will be using Iris Data Set to learn the ensemble models.","299c5254":"## Evaluating and tuning the ensemble classifier","8e1887a5":"In this notebook we will learn about followings:-\n\n- Make predictions on majority voting.\n- Use bagging to reduce overfitting by drawing random combinations of the training set with repetition\n- Apply boosting to build powerful models from *weak learners* that learn from their mistakes","7732ca18":"The goal of **ensemble methods** is to combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier alone.\n\n**Majority Voting** - It simply means that we select the class label that has been predicted by the majority of classifiers, i.e, received more than 50% of the votes. Also the term *majority voting* referes to binary classes only. However it is easy to generalize the majority voting priniciple to multi-class setting, which is called **plurality voting**. (here we select the class label that received the most vote(mode)).","9b8caf6d":"# Ensemble Learning","65a1fe6b":"You can take a look at more boosting techniques like :-\n\n- Gradient Boost\n- Extreme Gradient Boost\n- Light Gradient Boost\n\n\n**NOTE --------------**\n\nEnsemble learning increases the computational complexity compared to individual classifiers. So In practice it will not be coming in a lot of use since we need cost efficient model. But this approach can be good for competition like kaggle and others where we need to outperform other by achieveing the highest accuracy possible on unseen data.","24fcbe9d":"We will be using only two features and two classes (binary), for understanding. The concept will be generalize to any number of features.","8d256e0f":"## Combining classifiers via majority vote (also plurality voting)","e2f6713c":"Here we will check how good our model generalizes over an unseen data (test data)","5e59a5fc":"Remember above (in Bagging) when we train the same tree with max_depth None (Unpruned) i.e tree achieve maximum depth and achieve 100% accuracy on training data. But here we can seen that our model is underfit (High Bias) because of max_depth 1 (pruned) (Decision Stump are weak learner remember).\n\nNow let's seen how our weak model will perform in AdaBoost","ecfc8191":"We can observe that bagging classifier perfrom much better than single Decision tree, although both have training accuracy as 100%, bagging also perform better on testing set with accuracy of 91.7% which is more than Decision Tree. Thus our variance of bagging classifier is by default less. (Bagging classifier generalizes better)","d05be3e4":"# Bagging","78e631f8":"In boosting, the ensemble consists of very simple base classifiers, also often referred to as **weak learners**, which often only have a slight performance advantage over random guessing - a typical example of a weak learner is a decision tree stump.\n\n**Decision Tree Stump** - It is a one level decision tree. i.e a decision tree with one internal nodes which is immediately connect to the terminal nodes. A decision stumps make a predicition based on the values of just a single input feature. Basically a stump is tree with one node and two leaves\n\nThe key concept behind boosting is to focus on training samples that are hard to classify, i.e, to let the weak learners subsequentally learn from misclassified training samples to improve the performance of the ensemble.\n\n**Idea behind AdaBoost**\n\n- AdaBoost combines a lot of **weak learners** to make classifications. The weak learner are always stumps (Decision Stump)\n- Some stump get more say (Amount of say) in the classification than others.\n- Each stump is made by taking the previous stumps mistakes into account.\n\n**Basic different b\/w Adaboost and other ensemble model**\n\n- In other ensemble model every estimator has an equal weightage on voting for classification, but in adaboost some estimator (Decision stump) have more weightage in the voting.\n- Also order of implementation of estimator is important in adaboost because the error that the first stump makes influences how the second stump is made and so on. On the other hand the order of implementation is not important in other ensemble model.","dc7a0651":"Remeber that we don't used pruning for our decision tree, therefore will our model capture every details of data and make the model complex. It can be evident from the accuray score that our model is overfitted, 100% score for training set and only 83% for testing set. Thus our model have high variance.\n\nNow let's see how our bagging model with same base estimator performs","ddf12db4":"We can observe from the above diagram that decison boundary is more smoother in bagging classifier than in Decision tree thus our Bagging generalize better than decision tree on unseen data.\n\n**Note :-**\nThe bagging algorithm can be an effective approach to reduce the variance of a model. However bagging is ineffective in reducing model bias, i.e models that are too simple to capture the trend in the data well. This is why we want to perform bagging on ensembe of classifiers with low bias(Decision tree), for example unpruned decision tree.","9254af9f":"As we can see, the AdaBoost model predicts all class labels of the training set correctly and also shows a slightly improved test set performance compared to the decision tree stump. However, we also see that we introduce <u>additional variance by our attempt to reduce the model bias - a higher gap between training and test performance.<\/u>"}}