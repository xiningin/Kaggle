{"cell_type":{"43d26220":"code","9fcc7138":"code","85e2ec77":"code","db072e86":"code","dcecb0ca":"code","8763ba1e":"code","99c53b64":"code","3447beb5":"code","75791441":"code","1bd7f254":"code","56c82f9e":"code","2d222b9f":"code","9e89e7b3":"code","ec07d50d":"code","d1522ce0":"code","db4c7418":"code","ca159595":"code","438b9dfe":"code","fac0138c":"code","e59362a9":"code","d7fa7a61":"code","4180f131":"code","6b38ae74":"code","668386a2":"code","e9d4ac26":"code","34379c07":"code","7f6594f7":"code","52569322":"code","a66c5ea6":"code","c475fcfd":"code","c849e786":"code","3839c1cc":"code","d3ca1b99":"code","f7a289f3":"code","f661cb93":"code","8ce32adf":"code","a9986203":"code","f6a97bd7":"code","d06d500b":"code","bcc19558":"code","28d336dc":"code","29b57d24":"code","6a4bfdc2":"code","15c8f278":"code","6003687c":"code","ba50fb05":"code","de593809":"code","e1d35cfe":"code","4e99cc9e":"code","33389f87":"code","37369610":"code","79b7cb42":"code","ededa462":"code","4c55f2d5":"code","f9e5952a":"markdown"},"source":{"43d26220":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9fcc7138":"#reading the dataset","85e2ec77":"df = pd.read_csv(\"\/kaggle\/input\/insurance-premium-prediction\/insurance.csv\")","db072e86":"#exploring the size of dataset","dcecb0ca":"df.shape","8763ba1e":"#viewing the head of dataset","99c53b64":"df.head(25)","3447beb5":"#getting information about the dataset","75791441":"df.info()","1bd7f254":"#descrbing the dataset\/statistical view on the dataset","56c82f9e":"df.describe(include = \"all\")","2d222b9f":"#looking for null values in the dataset","9e89e7b3":"df.isna().sum()","ec07d50d":"#checking for duplicates\ndf.duplicated().sum()","d1522ce0":"df = df.drop_duplicates(keep='first')\ndf","db4c7418":"df.shape","ca159595":"#data preprocessing","438b9dfe":"#1.data_cleaning","fac0138c":"#getting column headings\ndf.columns","e59362a9":"#first we separate the features\/columns with non-numerical values to encode it.\n\ndf_category_columns = df.select_dtypes(exclude = np.number).columns","d7fa7a61":"df_category_columns","4180f131":"# we go one by one on the features and see the data it contains\ndf[\"sex\"].value_counts()","6b38ae74":"#encoding simply by replacing the catagory values into numerical for model\ndf[\"sex\"] = df[\"sex\"].replace({\"male\":0,\"female\":1})","668386a2":"#encoded feature\ndf[\"sex\"].head()","e9d4ac26":"#next feature to be encoded is \"smoker\"\ndf[\"smoker\"].value_counts()","34379c07":"#encoding simply by replacing the catagory values into numerical for model\ndf[\"smoker\"] = df[\"smoker\"].replace({\"yes\":1,\"no\":0})\ndf['smoker']","7f6594f7":"#next feature to be encoded is \"smoker\"\ndf[\"region\"].value_counts()","52569322":"#Import library:\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n#New variable for outlet\nvar_mod = ['region']\nle = LabelEncoder()\nfor i in var_mod:\n    df[i] = le.fit_transform(df[i])","a66c5ea6":"df['region']","c475fcfd":"df","c849e786":"df= pd.get_dummies(df, columns=[\"region\"])","3839c1cc":"df","d3ca1b99":"#checking whether all the features consist only numerical data by calling the features consist only numerical values\ndf_number_columns = df.select_dtypes(include = np.number).columns\ndf_number_columns","f7a289f3":"import seaborn as sns","f661cb93":"sns.heatmap(df.corr(), annot=True)","8ce32adf":"#data_exploration\/overview of data graphically\nsns.pairplot(df)","a9986203":"#Check for Multi Collinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor \n\n# the independent variables set \nX = df.select_dtypes(include=np.number).drop(columns=[\"expenses\"])\n  \n# VIF dataframe \nvif_data = pd.DataFrame() \nvif_data[\"feature\"] = X.columns \n  \n# calculating VIF for each feature \nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) \n                          for i in range(len(X.columns))] \n  \nprint(vif_data)","f6a97bd7":"#since we have already reduced features so it may not be neccessary to ommit the features or we can drop the feature with VIF more than 10\ndel df['region_2']","d06d500b":"df","bcc19558":"#saviong the preprocessed data in new file.csv\ndf.to_csv(\"insurance_prem_pred_preprocess.csv\",index = False)","28d336dc":"#reading the preprocessed data\ndf_preprocessed = pd.read_csv(\"insurance_prem_pred_preprocess.csv\")","29b57d24":"df_preprocessed.head(20)","6a4bfdc2":"#now we create a train test slpit to build, validate our model\nimport sklearn\n\nfrom sklearn.model_selection import train_test_split","15c8f278":"#determing the input\/independant features\n\nX = df_preprocessed.drop(columns =\"expenses\")\n\n#determing the output\/dependant\/target feature\n\ny = df_preprocessed[\"expenses\"]\n","6003687c":"#spliting the test data with 33%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","ba50fb05":"print (X_train.shape)\nprint (y_train.shape)\nprint (X_test.shape)\nprint (y_test.shape)","de593809":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()","e1d35cfe":"model.fit(X_train,y_train)","4e99cc9e":"model.coef_","33389f87":"model.intercept_","37369610":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","79b7cb42":"from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.model_selection import cross_val_score","ededa462":"#error metrics on train data\npred_train = model.predict(X_train)\nprint(\"Mean Absolute Error of train data = \",mean_absolute_error(y_train,pred_train))\nprint(\"RMSE of train data = \",np.sqrt(mean_squared_error(y_train, pred_train)))\nscore = cross_val_score(model, X_train, y_train, cv = 10, scoring = \"neg_root_mean_squared_error\" )\nprint(\"Cross validation Score  = \",np.mean(np.abs(score)))\nprint(\"Mean Absolute Percentage Error of train data = \", mean_absolute_percentage_error(y_train, pred_train))","4c55f2d5":"#error metics on test data\npred_test = model.predict(X_test)\npred_test = abs(pred_test)\nprint(\"Mean Absolute Error of train data = \",mean_absolute_error(y_test,pred_test))\nprint(\"RMSE of train data = \",np.sqrt(mean_squared_error(y_test,pred_test)))\nscore = cross_val_score(model, X_test, y_test, cv = 10, scoring = \"neg_root_mean_squared_error\" )\nprint(\"Cross validation Score  = \",np.mean(np.abs(score)))\nprint(\"Mean Absolute Percentage Error of train data = \", mean_absolute_percentage_error(y_test,pred_test))","f9e5952a":"#insights from the above graph\n\n#1.the expenses ranges between 1k to 60k\n\n#2.people with bmi range between 25 to 45 have max expenses\n\n#3.people with 0 children have more expense\n\n#4.people who doesnt smoke have high expenses\n\n#5.only few have higher expenses"}}