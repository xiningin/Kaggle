{"cell_type":{"b04b0ffa":"code","716a43a9":"code","5327cf51":"code","56f15b7e":"code","f0547f47":"code","027db1e8":"code","d52ab16a":"code","f0cc40a8":"code","3ffe6ae0":"code","3b7f8c58":"code","0463b334":"code","fdddcca2":"code","15d976b8":"code","00e445b4":"code","a3b6244d":"code","22b9bd59":"code","61aaad39":"code","54b0f120":"code","49c940f2":"code","4db26180":"code","10c3b94c":"code","382e451b":"code","def85f85":"code","db5a7ec4":"code","b00de241":"code","9a29635e":"code","37e06d2b":"code","7fca2319":"code","2d241b09":"code","cd32f2fa":"code","1983bf48":"code","8dd22990":"code","ed6963a3":"code","00c5f062":"code","5cda2d97":"code","26313941":"code","1231e997":"code","72e19687":"code","826eb19f":"code","7226d88e":"code","79892a7e":"code","dcdeb441":"code","261af6ad":"code","9370cae1":"code","4e3f5819":"code","e10075c1":"markdown","49548dc8":"markdown","9f581763":"markdown","675adf78":"markdown","056a71b0":"markdown","5bc42c41":"markdown","0801040b":"markdown","c71225bb":"markdown","416c263c":"markdown","be8228d3":"markdown","6b569a5d":"markdown","d94a383f":"markdown","5a87459c":"markdown","3b55c705":"markdown","b74e56aa":"markdown","13e56290":"markdown","11806c03":"markdown","f004b647":"markdown","2739519a":"markdown","f1fee56d":"markdown","b07919c0":"markdown","bcbccb7c":"markdown","fc2a7ab6":"markdown","ab8a6702":"markdown","a57aea79":"markdown"},"source":{"b04b0ffa":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport tensorflow_hub as hub\nimport os\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\nfrom keras import optimizers\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \n","716a43a9":"print(\"Version \", tf.__version__)\nprint(\"Eager mode:\", tf.executing_eagerly())\nprint(\"Hub version: \", hub.__version__)\nprint(\"GPU is\",\"available\" if tf.test.is_gpu_available() else\"Not Available\")","5327cf51":"#!\/usr\/bin\/python\n\nimport os, sys\n\n# Create new Train and val folders\n\nbase_dir = 'kaggle\/input\/RiceLeafs'\ntrain_path = '\/kaggle\/input\/RiceLeafs\/train'\nval_path = 'kaggle\/input\/RiceLeafs\/validation\/'\n\ncolumn_names = os.listdir(train_path)\nfor i in column_names:\n    os.makedirs(f'..\/kaggle\/output\/train\/{i}')\n    os.makedirs(f'..\/kaggle\/output\/validation\/{i}')\n\nout_path = '..\/kaggle\/output\/train\/'\n\n\n","56f15b7e":"from PIL import Image\ndef resize(input_path,folder,column_name):\n    dirs = os.listdir(input_path)\n    for item in dirs:\n        item_path = input_path +'\/' +item\n        if os.path.isfile(item_path):\n            #print('CHECK')\n            im = Image.open(item_path)\n\n            # Check whether the specified \n            # path exists or not \n            outpath = f'\/kaggle\/kaggle\/output\/{folder}\/{column_name}'\n            temp_out_path = outpath+'\/'+item\n            f, e = os.path.splitext(temp_out_path)\n\n            imResize = im.resize((255,255), Image.ANTIALIAS)\n            #print('CHECK 3')\n            imResize.save(f + '.jpg', 'JPEG', quality=90)\n\n\n\n","f0547f47":"input_path = '..\/input\/RiceLeafs\/train\/Healthy'\nfolder = 'train'\ncolumn_name = 'Healthy'\nresize(input_path,folder,column_name)\n\ninput_path = '..\/input\/RiceLeafs\/train\/BrownSpot'\nfolder = 'train'\ncolumn_name = 'BrownSpot'\nresize(input_path,folder,column_name)\n\ninput_path = '..\/input\/RiceLeafs\/train\/Hispa'\nfolder = 'train'\ncolumn_name = 'Hispa'\nresize(input_path,folder,column_name)\n\ninput_path = '..\/input\/RiceLeafs\/train\/LeafBlast'\nfolder = 'train'\ncolumn_name = 'LeafBlast'\nresize(input_path,folder,column_name)\n\nprint('Done with train resizing')","027db1e8":"## VALIDATION\ninput_path = '..\/input\/RiceLeafs\/validation\/Healthy'\nfolder = 'validation'\ncolumn_name = 'Healthy'\nresize(input_path,folder,column_name)\n\ninput_path = '..\/input\/RiceLeafs\/validation\/BrownSpot'\nfolder = 'validation'\ncolumn_name = 'BrownSpot'\nresize(input_path,folder,column_name)\n\ninput_path = '..\/input\/RiceLeafs\/validation\/Hispa'\nfolder = 'validation'\ncolumn_name = 'Hispa'\nresize(input_path,folder,column_name)\n\ninput_path = '..\/input\/RiceLeafs\/validation\/LeafBlast'\nfolder = 'validation'\ncolumn_name = 'LeafBlast'\nresize(input_path,folder,column_name)\n\nprint('Done with Validation resizing')","d52ab16a":"os.path.exists('\/kaggle\/kaggle\/output\/validation\/Healthy\/')","f0cc40a8":"os.path.exists('\/kaggle\/kaggle\/output\/train\/')\nos.path.exists('\/kaggle\/kaggle\/output\/validation\/')\n","3ffe6ae0":"os.listdir('\/kaggle\/kaggle\/output\/train\/BrownSpot\/')","3b7f8c58":"data_dir = os.path.join(os.path.dirname('\/kaggle\/kaggle\/'), 'output')","0463b334":"# Use this if you avoided the resizing\ndata_dir = os.path.join(os.path.dirname('\/output\/'), 'RiceLeafs')","fdddcca2":"train_dir = os.path.join(data_dir, 'train')\ntrain_BrownSpot_dir = os.path.join(train_dir, 'BrownSpot')\ntrain_Healthy_dir = os.path.join(train_dir, 'Healthy')\ntrain_Hispa_dir = os.path.join(train_dir, 'Hispa')\ntrain_LeafBlast_dir = os.path.join(train_dir, 'LeafBlast')\n\n\nvalidation_dir = os.path.join(data_dir, 'validation')\nvalidation_BrownSpot_dir = os.path.join(validation_dir, 'BrownSpot')\nvalidation_Healthy_dir = os.path.join(validation_dir, 'Healthy')\nvalidation_Hispa_dir = os.path.join(validation_dir, 'Hispa')\nvalidation_LeafBlast_dir = os.path.join(validation_dir, 'LeafBlast')","15d976b8":"train_BrownSpot_names = os.listdir(train_BrownSpot_dir)\nprint(train_BrownSpot_names[:10])\n\ntrain_Healthy_names =  os.listdir(train_Healthy_dir)\nprint(train_Healthy_names[:10])\n\ntrain_Hispa_names = os.listdir(train_Hispa_dir)\nprint(train_Hispa_names[:10])\n\ntrain_LeafBlast_names =  os.listdir(train_LeafBlast_dir)\nprint(train_LeafBlast_names[:10])","00e445b4":"\nimport time\nimport os\nfrom os.path import exists\n\ndef count(dir, counter=0):\n    \"returns number of files in dir and subdirs\"\n    for pack in os.walk(dir):\n        for f in pack[2]:\n            counter += 1\n    return dir + \" : \" + str(counter) + \" files\"\n\nprint('total images for training :', count(train_dir))\nprint('total images for validation :', count(validation_dir))","a3b6244d":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n\n# Parameters for our graph; we'll outpu images in a 4x4 configuration\nnrows = 4\nncols = 4\n\n# for iternating over images\npic_index = 0","22b9bd59":"# Set up matplotlib fig, and size it to fit 4x4 pics\n\nfig = plt.gcf()\nfig.set_size_inches(ncols *4, nrows*4)\n\npic_index += 8\nnext_BrownSpot_pix = [os.path.join(train_BrownSpot_dir, fname)\n                for fname in train_BrownSpot_names[pic_index-8:pic_index]]\nfor i, img_path in enumerate(next_BrownSpot_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows,ncols,i +1)\n  #sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","61aaad39":"# Set up matplotlib fig, and size it to fit 4x4 pics\n\nfig = plt.gcf()\nfig.set_size_inches(ncols *4, nrows*4)\n\npic_index += 8\nnext_Healthy_pix = [os.path.join(train_Healthy_dir, fname)\n                for fname in train_Healthy_names[pic_index-8:pic_index]]\n\n\nfor i, img_path in enumerate(next_Healthy_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows,ncols,i +1)\n  #sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","54b0f120":"# Set up matplotlib fig, and size it to fit 4x4 pics\n\nfig = plt.gcf()\nfig.set_size_inches(ncols *4, nrows*4)\n\npic_index += 8\n\nnext_Hispa_pix = [os.path.join(train_Hispa_dir, fname)\n                for fname in train_Hispa_names[pic_index-8:pic_index]]\n\n\nfor i, img_path in enumerate(next_Hispa_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows,ncols,i +1)\n  #sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","49c940f2":"# Set up matplotlib fig, and size it to fit 4x4 pics\n\nfig = plt.gcf()\nfig.set_size_inches(ncols *4, nrows*4)\n\npic_index += 8\n\nnext_LeafBlast_pix = [os.path.join(train_LeafBlast_dir, fname)\n                for fname in train_LeafBlast_names[pic_index-8:pic_index]]\n\nfor i, img_path in enumerate(next_LeafBlast_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows,ncols,i +1)\n  #sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","4db26180":"IMAGE_SHAPE = (244, 244)\nBATCH_SIZE = 64 #@param {type:\"integer\"}","10c3b94c":"# Inputs are suitably resized for the selected module. Dataset augmentation (i.e., random distortions of an image each time it is read) improves training, esp. when fine-tuning.\n\nvalidation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.\/255)\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_dir, \n    shuffle=False, \n    seed=42,\n    color_mode=\"rgb\", \n    class_mode=\"categorical\",\n    target_size=IMAGE_SHAPE,\n    batch_size=BATCH_SIZE)\n\ndo_data_augmentation = True #@param {type:\"boolean\"}\nif do_data_augmentation:\n  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n      rescale = 1.\/255,\n      rotation_range=40,\n      horizontal_flip=True,\n      width_shift_range=0.2, \n      height_shift_range=0.2,\n      shear_range=0.2, \n      zoom_range=0.2,\n      fill_mode='nearest' )\nelse:\n  train_datagen = validation_datagen\n  \ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,  \n    shuffle=True, \n    seed=42,\n    color_mode=\"rgb\", \n    class_mode=\"categorical\",\n    target_size=IMAGE_SHAPE,\n    batch_size=BATCH_SIZE)","382e451b":"train_generator.num_classes","def85f85":"class MyCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self,epoch,log = {}):\n    if(log.get('accuracy')> 0.99):\n      if(log.get('val_accuracy')>0.99):\n        print(\"\\n Reached 99% Accuracy for both train and val.\")\n        self.model.stop_training = True\n\ncallbacks = MyCallback()","db5a7ec4":"\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(16,(3,3),activation = 'relu',input_shape = (244,244,3)),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(32,(3,3),activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(64,(3,3),activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128,activation = 'relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(256,activation = 'relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(4,activation = 'softmax')\n\n],    name = 'Conv2D_Model')\n\nmodel.summary()","b00de241":"LEARNING_RATE = 0.001 #@param {type:\"number\"}\n\nmodel.compile(optimizer = tf.keras.optimizers.Adam(),\n              loss = 'categorical_crossentropy',\n              metrics = ['accuracy'])","9a29635e":"EPOCHS=10 #@param {type:\"integer\"}\n\nhistory = model.fit_generator(\n        train_generator,\n        steps_per_epoch=train_generator.samples\/\/train_generator.batch_size,\n        epochs=EPOCHS,\n        validation_data=validation_generator,\n        callbacks = [callbacks],\n        validation_steps=validation_generator.samples\/\/validation_generator.batch_size)","37e06d2b":"import matplotlib.pylab as plt\nimport numpy as np\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.ylabel(\"Accuracy (training and validation)\")\nplt.xlabel(\"Training Steps\")\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.ylabel(\"Loss (training and validation)\")\nplt.xlabel(\"Training Steps\")\nplt.show()","7fca2319":"import numpy as np\nimport random\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\n\n\n# Lets define a new Model that will take an image as an input and will output \n# the intermediate representations for all layers in the previous model after \n# the first\n\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n\n# Visualization_model = Model(img_input,successive_outputs)\nvisualization_model = tf.keras.models.Model(inputs = model.input,\n                                            outputs = successive_outputs)\n\n# Lets prepare a random input image form the training set.\n\nBrownSpot_img_files = [os.path.join(train_BrownSpot_dir, f) for f in train_BrownSpot_names]\nHealthy_files = [os.path.join(train_Healthy_dir, f) for f in train_Healthy_names]\nimg_path = random.choice(BrownSpot_img_files + Healthy_files)\n\n\nimg = load_img(img_path,target_size = (244,244)) # This is a PIL image\nx = img_to_array(img)  # Numpy array with shape (244,244,3)\nx = x.reshape((1,) + x.shape) # Numpy array with shape (1,244,244,3)\n\n# Rescale by 1\/255\nx \/=255\n\n\n# Let's run our image through our network, thus obtaining all\n# Intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers so we can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers[1:]]\n\n\n# Now lets display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n  if len(feature_map.shape) == 4:\n    # Just do this for the conv\/maxpool layers, for the fully-connected layers\n    n_features = feature_map.shape[-1] # number of features in feature map\n    # The feature map has shape (1,size,size,n_features)\n    size = feature_map.shape[1]\n    # We will title our images in this matrix\n    display_grid = np.zeros((size, size* n_features))\n    for i in range(n_features):\n      # Post process the feature to make it visibly palatable\n      x = feature_map[0,:,:,i]\n      x -= x.mean()\n      x \/= x.std()\n      x *= 64\n      x+= 128\n      x = np.clip(x,0,255).astype('uint8')\n      # We'll tile each filter into this big horizontal grid\n      display_grid[:,i*size:(i+1)*size] = x\n    # Display the grid\n    scale = 20. \/ n_features\n    plt.figure(figsize=(scale*n_features,scale))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid,aspect = 'auto', cmap = 'viridis')","2d241b09":"!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/mledu-datasets\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n    -O \/tmp\/inception_v3_weights_tf.dim_ordering_tf_kernels.notop.h5","cd32f2fa":"from tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nlocal_weights_file = '\/tmp\/inception_v3_weights_tf.dim_ordering_tf_kernels.notop.h5'\n\npre_trained_model = InceptionV3(\n                                input_shape = (244,244,3),\n                                include_top= False,\n                                weights = None\n)\n\npre_trained_model.load_weights(local_weights_file)\n\nfor layer in pre_trained_model.layers:\n    layer.trainable = False\n    \n","1983bf48":"last_layer = pre_trained_model.get_layer('mixed7')\nprint(f'The shape of the last layer is {last_layer.output_shape}')\noutput_layer = last_layer.output","8dd22990":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\nx = tf.keras.layers.Flatten()(output_layer)\nx = tf.keras.layers.Dense(512, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.4)(x)\n#x = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(4, activation='softmax')(x)\n\nmodel = Model(pre_trained_model.input, x,name=\"RiceLeafs_Inception_model\")","ed6963a3":"LEARNING_RATE = 0.001 #@param {type:\"number\"}\n\nmodel.compile(optimizer = tf.keras.optimizers.RMSprop(lr = LEARNING_RATE),\n              loss = 'categorical_crossentropy',\n              metrics = ['accuracy'])","00c5f062":"EPOCHS=10 #@param {type:\"integer\"}\n\nhistory = model.fit_generator(\n        train_generator,\n        steps_per_epoch=train_generator.samples\/\/train_generator.batch_size,\n        epochs=EPOCHS,\n        validation_data=validation_generator,\n        callbacks = [callbacks],\n        validation_steps=validation_generator.samples\/\/validation_generator.batch_size)","5cda2d97":"import matplotlib.pylab as plt\nimport numpy as np\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.ylabel(\"Accuracy (training and validation)\")\nplt.xlabel(\"Training Steps\")\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.ylabel(\"Loss (training and validation)\")\nplt.xlabel(\"Training Steps\")\nplt.show()","26313941":"import tensorflow as tf\nimport tensorflow_hub as hub\nmodel = tf.keras.Sequential([\nhub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b7\/feature-vector\/1\"),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(512, activation='relu'),\n\n  tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n])\n\nmodel.build([None, 244, 244, 3])","1231e997":"#Compile model specifying the optimizer learning rate\n\nLEARNING_RATE = 0.0001 #@param {type:\"number\"}\n\nmodel.compile(\n   optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n   loss='categorical_crossentropy',\n   metrics=['accuracy'])","72e19687":"EPOCHS=10 #@param {type:\"integer\"}\n\nhistory = model.fit_generator(\n        train_generator,\n        steps_per_epoch=train_generator.samples\/\/train_generator.batch_size,\n        epochs=EPOCHS,\n        validation_data=validation_generator,\n        #callbacks = [callbacks],\n        validation_steps=validation_generator.samples\/\/validation_generator.batch_size)","826eb19f":"import matplotlib.pylab as plt\nimport numpy as np\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.ylabel(\"Accuracy (training and validation)\")\nplt.xlabel(\"Training Steps\")\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.ylabel(\"Loss (training and validation)\")\nplt.xlabel(\"Training Steps\")\nplt.show()","7226d88e":"IMAGE_SHAPE[0]","79892a7e":"# Import OpenCV\nimport cv2\n\n# Utility\nimport itertools\nimport random\nfrom collections import Counter\nfrom glob import iglob\n\n\ndef load_image(filename):\n    img = cv2.imread(os.path.join(data_dir, validation_dir, filename))\n    img = cv2.resize(img,(IMAGE_SHAPE[0], IMAGE_SHAPE[1]) )\n    img = img \/255\n    \n    return img\n\n\ndef predict(image):\n    probabilities = model.predict(np.asarray([img]))[0]\n    class_idx = np.argmax(probabilities)\n    \n    return {classes[class_idx]: probabilities[class_idx]}","dcdeb441":"for idx, filename in enumerate(random.sample(validation_generator.filenames, 5)):\n    print(\"SOURCE: class: %s, file: %s\" % (os.path.split(filename)[0], filename))\n    \n    img = load_image(filename)\n    prediction = predict(img)\n    print(\"PREDICTED: class: %s, confidence: %f\" % (list(prediction.keys())[0], list(prediction.values())[0]))\n    plt.imshow(img)\n    plt.figure(idx)    \n    plt.show()","261af6ad":"import time\nt = time.time()\n\nexport_path = \"\/tmp\/saved_models\/{}\".format(int(t))\ntf.keras.experimental.export_saved_model(model, export_path)\n\nexport_path","9370cae1":"# Now confirm that we can reload it, and it still gives the same results\nreloaded = tf.keras.experimental.load_from_saved_model(export_path, custom_objects={'KerasLayer':hub.KerasLayer}) # custom_objects depends on model","4e3f5819":"# convert the model to TFLite\n!mkdir \"tflite_models\"\nTFLITE_MODEL = \"tflite_models\/rice_leaf_disease.tflite\"\n\n\n# Get the concrete function from the Keras model.\nrun_model = tf.function(lambda x : reloaded(x))\n\n# Save the concrete function.\nconcrete_func = run_model.get_concrete_function(\n    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype)\n)\n\n# Convert the model to standard TensorFlow Lite model\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\nconverted_tflite_model = converter.convert()\nopen(TFLITE_MODEL, \"wb\").write(converted_tflite_model)","e10075c1":"### 2.Model - Inception <a name=\"InceptionV3\"><\/a>","49548dc8":"## Viewing Images  <a name=\"ViewingImages\"><\/a>","9f581763":"## Image Count <a name=\"ImageCount\"><\/a>","675adf78":"# Split into Training and Validation  <a name=\"Split\"><\/a>","056a71b0":"# Export as TensorFlowLITE <a name=\"TFLITE\"><\/a>","5bc42c41":"# Importing Libraries <a name=\"ImportingLibraries\"><\/a>","0801040b":"#### Downloading Weights <a name=\"DownloadWeights\"><\/a>","c71225bb":"# Rice Crop Disease Detection using TensorFlow\n\n## Table of Contents\n- [Importing Libraries](#ImportingLibraries)\n- [Loading Dataset](#LoadingDataset)\n- [Resizing Image](#Resize)\n- [Split into Training and Validation](#Split)\n- [Image Count](#ImageCount)\n- [Viewing Images](#ViewingImages)\n    - [BrownSpot](#BrownSpot)\n    - [Healthy](#Healthy)\n    - [Hispa](#Hispa)\n    - [LeafBlast](#LeafBlast)\n- [Data Augmentation and Generators](#DataAugAndGen)\n- [Callback](#Callback)\n- [Models](#Models)\n    - [1. Model - Conv2D](#Conv2D)\n        - [Metrics](#MetricsConv2D)\n        - [Observing the Convolutions](#ObservingConv2D)\n    - [2. Model - InceptionV3](#InceptionV3)\n        - [Metrics](#MetricInceptionv3)\n    - [3. Model - EfficientNet](#EfficientNet)\n        - [Metrics](#MetricsEfficientv2)\n- [Export as TensorFlow LITE](#TFLITE)","416c263c":"### 3. Model - EfficientNet v2 <a name=\"EfficientNet\"><\/a>","be8228d3":"### Metrics <a name=\"MetricsConv2D\"><\/a>","6b569a5d":"### BrownSpot <a name=\"BrownSpot\"><\/a>","d94a383f":"### LeafBlast <a name=\"LeafBlast\"><\/a>","5a87459c":"# Resizing Image [OPTIONAL]  <a name=\"Resize\"><\/a>","3b55c705":"### Hispa <a name=\"Hispa\"><\/a>","b74e56aa":"# Models <a name=\"Model\"><\/a>","13e56290":"### 1. Model - Conv2D <a name=\"Conv2D\"><\/a>","11806c03":"## Callback <a name=\"Callback\"><\/a>","f004b647":"#### Observing the Convolutions  <a name=\"ObservingConv2D\"><\/a>","2739519a":"#### Metrics <a name=\"MetricsInceptionv3\"><\/a>","f1fee56d":"### Metrics <a name=\"MetricsEfficientv2\"><\/a>","b07919c0":"### Healthy <a name=\"Healthy\"><\/a>","bcbccb7c":"#### TensorFlow Hub Dataset\n- [EfficientNet B7](https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b7\/feature-vector\/1)","fc2a7ab6":"# Predict","ab8a6702":"# Loading Dataset <a name=\"LoadingDataset\"><\/a>","a57aea79":"# Data Augmentation and Generators <a name=\"DataAugAndGen\"><\/a>"}}