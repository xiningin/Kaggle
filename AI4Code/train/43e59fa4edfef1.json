{"cell_type":{"976cbd06":"code","0467213e":"code","a0249597":"code","4be2bdb2":"code","d05a6a8a":"code","589054d9":"code","da8441a1":"code","7240e78e":"code","dfb8bf6e":"code","78f91250":"code","20826baa":"code","e867a0e2":"code","3a4e63fc":"code","5377e18c":"code","8a5c1e3e":"code","275a1ad2":"code","797abef9":"code","b088c238":"code","be3393e2":"code","8ccb0156":"code","bffec327":"code","370c8a21":"code","dfa24edc":"code","553ffebf":"code","61bc4f30":"code","95c8ac8e":"code","b0190157":"code","52326548":"code","06baed12":"code","448bd367":"code","4053ee38":"code","5e3a70cf":"code","23d1dc42":"code","53a60812":"code","e102e7cd":"code","3c5a5312":"code","aaff008b":"code","a0b87c8e":"code","f05cf662":"code","54672716":"code","f39371f3":"code","2686fb6f":"code","331e8d4f":"code","ba016882":"code","be2b433b":"code","138cebde":"code","c77d0e5f":"code","73101916":"code","cf8d74f9":"code","f1943f34":"code","78106379":"code","d2b68ea8":"code","03e38a87":"markdown","77009ee8":"markdown","b352d783":"markdown","ca33cedd":"markdown","e2b4e58d":"markdown","8d13eb52":"markdown","f7fc550a":"markdown","e0df0cf1":"markdown","c409a3e1":"markdown","ca6cfca2":"markdown","dd0f3824":"markdown","390174b8":"markdown","aa3b3255":"markdown","8b61aeeb":"markdown","5cb82cee":"markdown","def82f15":"markdown","d5e5087c":"markdown","08577bd3":"markdown","8bd72cdf":"markdown","e00b57a2":"markdown","f8a6fc97":"markdown","40a79510":"markdown","82815e4f":"markdown","6fd82a6b":"markdown"},"source":{"976cbd06":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0467213e":"!pip install h5py","a0249597":"from matplotlib import pyplot as plt\n%matplotlib inline","4be2bdb2":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsubmit = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","d05a6a8a":"print(train.shape)\nprint(train.describe())\nprint(train.info())","589054d9":"train.describe(include='O')","da8441a1":"int_columns = train.select_dtypes(exclude='object').columns\nint_columns","7240e78e":"obj_columns = train.select_dtypes(include='object').columns\nobj_columns","dfb8bf6e":"select_columns = int_columns[1:-1]\nprint(select_columns)\nprint(len(select_columns))","78f91250":"train[select_columns].isnull().any()","20826baa":"# imputer\nfrom sklearn.impute import SimpleImputer\nprint(train[select_columns])\n\nimp_mean = SimpleImputer()\ntrain_imp = pd.DataFrame(imp_mean.fit_transform(train[select_columns]))\ntrain_imp.columns = select_columns\n\ntest_imp = pd.DataFrame(imp_mean.transform(test[select_columns]))\ntest_imp.columns = select_columns\n\nprint(train_imp)","e867a0e2":"train_imp.isnull().any()","3a4e63fc":"# select_columns = [col for col in select_columns if train[col].isnull().any() == False]\n# print(select_columns)\n# print(len(select_columns))","5377e18c":"train_obj = train[obj_columns].fillna('nan')\nprint(train_obj.describe(include='O'))\n\ntest_obj = test[obj_columns].fillna('nan')\nprint(test_obj.describe(include='O'))","8a5c1e3e":"train_categorize = pd.get_dummies(train_obj)\nprint(train_categorize)\n\ntest_categorize = pd.get_dummies(test_obj)\nprint(test_categorize)","275a1ad2":"# X = train[select_columns]\n# _X = pd.concat([train_imp, train_categorize], axis=1)\ny = train['SalePrice']\n\n# _X_val = pd.concat([test_imp, test_categorize], axis=1)\n# X_val = test[select_columns]\n\nfrom sklearn.preprocessing import StandardScaler\n\n## StanderdScaler\nsc = StandardScaler()\ntrain_std = pd.DataFrame(sc.fit_transform(train_imp))\ntrain_std.columns = train_imp.columns\n\ntest_std = pd.DataFrame(sc.fit_transform(test_imp))\ntest_std.columns = test_imp.columns\n","797abef9":"import category_encoders as ce\n\n# OneHotEncode\u3057\u305f\u3044\u5217\u3092\u6307\u5b9a\u3002Null\u3084\u4e0d\u660e\u306e\u5834\u5408\u306e\u88dc\u5b8c\u65b9\u6cd5\u3082\u6307\u5b9a\u3002\nce_ohe = ce.OneHotEncoder(handle_unknown='impute')\n\n# pd.DataFrame\u3092\u305d\u306e\u307e\u307e\u7a81\u3063\u8fbc\u3080\ntrain_categorize = ce_ohe.fit_transform(train_obj)\ntest_categorize = ce_ohe.transform(test_obj)\n\nprint(train_categorize)\nprint(test_categorize)\n\nX = pd.concat([train_std, train_categorize], axis=1)\nX_val = pd.concat([test_std, test_categorize], axis=1)\n# X = pd.concat([train_std, train_obj], axis=1)\n# X_val = pd.concat([test_std, test_obj], axis=1)","b088c238":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)","be3393e2":"X_train","8ccb0156":"names = []\nnames_h5 = []\npd.options.display.float_format = '{:.2f}'.format","bffec327":"X_train['KitchenAbvGr']","370c8a21":"from sklearn.linear_model import LinearRegression\n\nsimple_lr = LinearRegression()\n\nX_train_simple = X_train['KitchenAbvGr'].values.reshape(-1, 1)\nX_test_simple = X_test['KitchenAbvGr'].values.reshape(-1, 1)\n\nsimple_lr.fit(X_train_simple, y_train)\n","dfa24edc":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nimport pickle\n\ny_pred = simple_lr.predict(X_test_simple)\n\ndef lr_scores(model_name, model, y_test, y_pred):\n    # MAE \/ Mean Absolute Error ...\u5e73\u5747\u7d76\u5bfe\u8aa4\u5dee\n    mae = mean_absolute_error(y_test, y_pred)\n\n    # MSE \/ Mean Squared Error ...\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\n    mse = mean_squared_error(y_test, y_pred)\n\n    # RMSE \/ Root Mean Squared Error...\u4e8c\u4e57\u5e73\u5747\u5e73\u65b9\u6839\u8aa4\u5dee\n    rmse = np.sqrt(mse)\n\n    # R2_score \/ \u6c7a\u5b9a\u4fc2\u6570...1\u306b\u8fd1\u3044\u65b9\u304c\u3088\u3044\u3002\u30de\u30a4\u30ca\u30b9\u3082\u3042\u308b\n    r2 = r2_score(y_test, y_pred)\n    \n    linear_predict = {'name': model_name, 'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2}\n    print(linear_predict)\n\n    names.append(model_name)\n    with open(model_name + '.pickle', mode='wb') as fp:\n        pickle.dump(model, fp)\n\ndef lr_scores_h5(model_name, model, y_test, y_pred):\n    # MAE \/ Mean Absolute Error ...\u5e73\u5747\u7d76\u5bfe\u8aa4\u5dee\n    mae = mean_absolute_error(y_test, y_pred)\n\n    # MSE \/ Mean Squared Error ...\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\n    mse = mean_squared_error(y_test, y_pred)\n\n    # RMSE \/ Root Mean Squared Error...\u4e8c\u4e57\u5e73\u5747\u5e73\u65b9\u6839\u8aa4\u5dee\n    rmse = np.sqrt(mse)\n\n    # R2_score \/ \u6c7a\u5b9a\u4fc2\u6570...1\u306b\u8fd1\u3044\u65b9\u304c\u3088\u3044\u3002\u30de\u30a4\u30ca\u30b9\u3082\u3042\u308b\n    r2 = r2_score(y_test, y_pred)\n    \n    linear_predict = {'name': model_name, 'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2}\n    print(linear_predict)\n\n    names_h5.append(model_name)\n    model.save(model_name + '.h5')\n\nlr_scores('Simple Linear Regressor', simple_lr, y_test, y_pred)","553ffebf":"multiple_lr = LinearRegression()\nmultiple_lr.fit(X_train, y_train)\n\ny_pred = multiple_lr.predict(X_test)\nlr_scores('Multiple Linear Regressor', multiple_lr, y_test, y_pred)","61bc4f30":"from sklearn.linear_model import SGDRegressor\n\nsgd = SGDRegressor(eta0 = 0.0001, max_iter = 100)\nsgd.fit(X_train, y_train)\n\ny_pred = sgd.predict(X_test)\nlr_scores('SGD Regressor', sgd, y_test, y_pred)","95c8ac8e":"from sklearn.linear_model import Lasso\n\nlasso = Lasso()\nlasso.fit(X_train, y_train)\n\ny_pred = lasso.predict(X_test)\nlr_scores('Lasso', lasso, y_test, y_pred)","b0190157":"from sklearn.linear_model import ElasticNet\n\neln = ElasticNet()\neln.fit(X_train, y_train)\n\ny_pred = eln.predict(X_test)\nlr_scores('ElasticNet', eln, y_test, y_pred)","52326548":"from sklearn.linear_model import Ridge\n\nridge = Ridge()\nridge.fit(X_train, y_train)\n\ny_pred = ridge.predict(X_test)\nlr_scores('Ridge Regressor', ridge, y_test, y_pred)","06baed12":"from sklearn.svm import SVR\n\nsvr_l = SVR(kernel='linear')\nsvr_l.fit(X_train, y_train)\n\ny_pred = svr_l.predict(X_test)\nlr_scores('SVM linear', svr_l, y_test, y_pred)","448bd367":"from sklearn.svm import SVR\n\nsvr_r = SVR(kernel='rbf')\nsvr_r.fit(X_train, y_train)\n\ny_pred = svr_r.predict(X_test)\nlr_scores('SVM rbf', svr_r, y_test, y_pred)","4053ee38":"from sklearn.ensemble import RandomForestRegressor\n\nrandom_forest = RandomForestRegressor(random_state=1, n_estimators=10)\nrandom_forest.fit(X_train, y_train)\n\ny_pred = random_forest.predict(X_test)\nlr_scores('Random Forest Regressor', random_forest, y_test, y_pred)","5e3a70cf":"from sklearn.ensemble import GradientBoostingRegressor\n\ngradient_boost = GradientBoostingRegressor(random_state=1, n_estimators=10)\ngradient_boost.fit(X_train, y_train)\n\ny_pred = gradient_boost.predict(X_test)\nlr_scores('Gradient Boosting Regressor', gradient_boost, y_test, y_pred)","23d1dc42":"# \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u9806\u306b\u5b9f\u884c\nresult = []\n\nfor name in names:\n    with open(name + '.pickle', 'rb') as fp:\n        reg = pickle.load(fp)\n    \n    if name == 'Simple Linear Regressor':\n        y_pred = reg.predict(X_test_simple)\n    else:\n        y_pred = reg.predict(X_test)\n\n    # MAE\n    mae = mean_absolute_error(y_test, y_pred)\n\n    # MSE\n    mse = mean_squared_error(y_test, y_pred)\n\n    # RMSE\n    rmse = np.sqrt(mse)\n\n    # R2_score\n    r2 = r2_score(y_test, y_pred)\n\n    result.append([mae, mse, rmse, r2])\n    \n    print(name)\n\ndf_result = pd.DataFrame(result, columns=['mae', 'mse', 'rmse', 'r2'], index = names)\ndf_result","53a60812":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100)\nxgb.fit(X_train, y_train)","e102e7cd":"y_pred = xgb.predict(X_test)\n\nlr_scores('XGBoost Regressor', xgb, y_test, y_pred)","3c5a5312":"from lightgbm import LGBMRegressor\n\nlgb = LGBMRegressor(max_depth=3, learning_rate=0.1, n_estimators=100)\nlgb.fit(X_train, y_train)\n","aaff008b":"y_pred = lgb.predict(X_test)\n\nlr_scores('LightGBM Regressor', lgb, y_test, y_pred)","a0b87c8e":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n\nparams = {'max_depth': list(range(2, 10)), 'random_state': [0], 'n_estimators': list(range(50, 200, 50))}\n\n# XGBoost\nxgb = XGBRegressor()\n\nreg_cv = GridSearchCV(xgb, params, cv=folds, return_train_score=True)\nreg_cv.fit(X, y)","f05cf662":"reg_cv.best_params_\ny_pred = reg_cv.predict(X_test)\nlr_scores('XGBoost(GCV)', reg_cv, y_test, y_pred)\n","54672716":"# LightGBM\nlgb = LGBMRegressor()\n\nreg_cv = GridSearchCV(lgb, params, cv=folds, return_train_score=True)\nreg_cv.fit(X, y)","f39371f3":"reg_cv.best_params_\ny_pred = reg_cv.predict(X_test)\nlr_scores('LightGBM(GCV)', reg_cv, y_test, y_pred)\n","2686fb6f":"# \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u9806\u306b\u5b9f\u884c\nresult = []\n\nfor name in names:\n    with open(name + '.pickle', 'rb') as fp:\n        reg = pickle.load(fp)\n    \n    if name == 'Simple Linear Regressor':\n        y_pred = reg.predict(X_test_simple)\n    else:\n        y_pred = reg.predict(X_test)\n\n    # MAE\n    mae = mean_absolute_error(y_test, y_pred)\n\n    # MSE\n    mse = mean_squared_error(y_test, y_pred)\n\n    # RMSE\n    rmse = np.sqrt(mse)\n\n    # R2_score\n    r2 = r2_score(y_test, y_pred)\n\n    result.append([mae, mse, rmse, r2])\n    \n    print(name)\n\ndf_result = pd.DataFrame(result, columns=['mae', 'mse', 'rmse', 'r2'], index = names)\ndf_result\n","331e8d4f":"batch_size = 64\nn_epochs = 100","ba016882":"import tensorflow as tf\nfrom tensorflow import keras\n\nmodel = keras.Sequential([\n    keras.layers.Dense(512, activation='relu', input_shape=[X.shape[1]]),\n    keras.layers.Dense(512, activation='relu'),\n    keras.layers.Dense(1)\n])\n","be2b433b":"optimizer = tf.keras.optimizers.RMSprop(0.001)\n\nmodel.compile(optimizer=optimizer, \n              loss='mse',\n              metrics=['mae', 'mse'])\n\nmodel.summary()","138cebde":"model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size)\n","c77d0e5f":"y_pred = model.predict(X_test)\nlr_scores_h5('TensorFlow', model, y_test, y_pred)\n","73101916":"from keras.models import load_model\n\n# \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u9806\u306b\u5b9f\u884c\nresult = []\n\nfor name in names_h5:\n    reg = load_model(name + '.h5')\n    \n    y_pred = reg.predict(X_test)\n\n    # MAE\n    mae = mean_absolute_error(y_test, y_pred)\n\n    # MSE\n    mse = mean_squared_error(y_test, y_pred)\n\n    # RMSE\n    rmse = np.sqrt(mse)\n\n    # R2_score\n    r2 = r2_score(y_test, y_pred)\n\n    result.append([mae, mse, rmse, r2])\n    \n    print(name)\n\ndf_result = pd.DataFrame(result, columns=['mae', 'mse', 'rmse', 'r2'], index = names_h5)\ndf_result\n","cf8d74f9":"submit","f1943f34":"with open('XGBoost(GCV).pickle', 'rb') as fp:\n    reg = pickle.load(fp)\n\ny_pred_final = reg.predict(X_val)\ny_pred_final","78106379":"submit['SalePrice'] = y_pred_final","d2b68ea8":"submit.to_csv('submit.csv', index=None)","03e38a87":"### Tensorflow","77009ee8":"## SVR(kernel='linear')\n","b352d783":"## SGD Regressor\n","ca33cedd":"## Ensemble Regressors\n","e2b4e58d":"### LightGBM for sklearn","8d13eb52":"## Analyze Data \/ \u30c7\u30fc\u30bf\u5206\u6790","f7fc550a":"## sklearn's regression models \/ sklearn\u306e\u56de\u5e30\u30e2\u30c7\u30eb\n- Linear Regressor \/ \u7dda\u5f62\u56de\u5e30\n- SGD Regressor\n- Lasso\n- Elastic Net\n- Ridge Regressor\n- Support Vector Regressor(kernel='linear')\n- Support Vector Regressor(kernel='rbf')\n- Ensemble Regressors\n","e0df0cf1":"## SVR(kernel='rbf')\n","c409a3e1":"## Linear Regression \/ \u7dda\u5f62\u56de\u5e30","ca6cfca2":"### XGBoost for sklearn","dd0f3824":"### \u307e\u3068\u3081\u3066\u5206\u985e\u5668\u691c\u8a3c","390174b8":"### Multiple Linear Regression \/ \u91cd\u56de\u5e30\u5206\u6790","aa3b3255":"### Grid search","8b61aeeb":"## Ridge Regressor\n","5cb82cee":"## Lasso\n","def82f15":"### Simple Linear Regression \/ \u5358\u56de\u5e30\u5206\u6790","d5e5087c":"## Nural Network\n- Tensorflow\n- PyTorch","08577bd3":"## Preparing Data \/ \u30c7\u30fc\u30bf\u6e96\u5099","8bd72cdf":"## Cross Validation and Grid Search \/ \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3068\u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1","e00b57a2":"## GBDT(Gradient Boosting Decision Tree)\n- XGBoost\n- LightGBM","f8a6fc97":"## Elastic Net\n","40a79510":"## make submit file","82815e4f":"### Random Forest Regressor","6fd82a6b":"### Gradient Boosting Regressor"}}