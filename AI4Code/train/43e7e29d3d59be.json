{"cell_type":{"e80e9963":"code","ddf0dcaa":"code","efd666ab":"code","8cb343da":"code","55d83ee7":"code","7e1b63f2":"code","763d70a7":"code","8a253e25":"code","90a47295":"code","319afc0d":"code","6eb6c4f6":"code","ebdb4941":"code","0428e6be":"code","9d59fc02":"code","78553558":"code","5a92d3b2":"code","8918df12":"code","a750538e":"code","fbacaad1":"code","c0cfb5ea":"code","503809e5":"code","cf400c1c":"code","5b7f14a3":"code","fdd5bf49":"code","44b0094a":"code","649a8b92":"code","dff09772":"code","0da22835":"code","9e5f7a6d":"code","223b962f":"markdown","f7305c1f":"markdown","9e793a12":"markdown","f10f1ecc":"markdown","ec836610":"markdown","b5fdedea":"markdown","bb78d68e":"markdown","531986c7":"markdown","4a0d8db9":"markdown","fc9609d7":"markdown","38bfd619":"markdown","57f85b48":"markdown","fe4b090d":"markdown"},"source":{"e80e9963":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn import tree","ddf0dcaa":"data = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndata.head()","efd666ab":"print(data.info())","8cb343da":"data.quality.unique()","55d83ee7":"map = {\n    3: 0,\n    4: 1,\n    5: 2,\n    6: 3,\n    7: 4,\n    8: 5\n}\ndata.quality = data.quality.map(map)","7e1b63f2":"data.quality.unique()","763d70a7":"df_train = data.head(1000)\ndf_label = df_train['quality']\ndf_train = df_train.drop('quality', axis = 1)","8a253e25":"df_train.head()","90a47295":"df_test = data.tail(599)\ntest_label = df_test['quality']\ndf_test= df_test.drop('quality', axis = 1)","319afc0d":"train_acc = [0.5]\ntest_acc = [0.5]\nfor depth in range(1, 20):\n    clf = tree.DecisionTreeClassifier(max_depth = depth)\n    \n    clf.fit(df_train, df_label)\n    \n    train_pred = clf.predict(df_train)\n    acc_train = metrics.accuracy_score(df_label, train_pred)\n    \n    test_pred = clf.predict(df_test)\n    acc_test = metrics.accuracy_score(test_label, test_pred)\n    \n    train_acc.append(acc_train)\n    test_acc.append(acc_test)","6eb6c4f6":"plt.figure(figsize=(10,5))\nsns.set_style('whitegrid')\nplt.plot(train_acc, label = 'train accuracy')\nplt.plot(test_acc, label = 'test accuracy')\nplt.legend(loc='upper left', prop = {'size': 15})\nplt.xticks(range(0, 20, 5))\nplt.xlabel('max_depth', size = 20)\nplt.ylabel('accuracy', size = 20)\nplt.show()","ebdb4941":"#Applying KFold Cross validation\n#create a new column kfold with entries -1\ndata['kfold'] = -1\n#Shuffle data\ndata = data.sample(frac = 1).reset_index(drop = True)\n#Split data into 5 folds\nkf = model_selection.KFold(n_splits = 5)\nfor fold, (t, v) in enumerate(kf.split(X=data)):\n    data.loc[v, 'kfold'] = fold\n#Saving data for further use\ndata.to_csv('t_fold.csv', index = False)","0428e6be":"def check(fold):\n    df = pd.read_csv('.\/t_fold.csv')\n    df_train = df[df.kfold != fold].reset_index(drop = True)\n    df_test = df[df.kfold == fold].reset_index(drop = True)  \n    \n    y_train = df_train.quality.values\n    x_train = df_train.drop('quality', axis = 1).values\n    \n    y_valid = df_test.quality.values\n    x_valid = df_test.drop('quality', axis = 1).values\n    \n    ktrain_acc = [0.5]\n    ktest_acc = [0.5]\n    for depth in range(1, 20):\n        clf = tree.DecisionTreeClassifier(max_depth = depth)\n\n        clf.fit(x_train, y_train)\n\n        train_pred = clf.predict(x_train)\n        acc_train = metrics.accuracy_score(y_train, train_pred)\n\n        test_pred = clf.predict(x_valid)\n        acc_test = metrics.accuracy_score(y_valid, test_pred)\n\n        ktrain_acc.append(acc_train)\n        ktest_acc.append(acc_test)\n    plt.figure(figsize=(10,5))\n    sns.set_style('whitegrid')\n    plt.plot(ktrain_acc, label = 'train accuracy')\n    plt.plot(ktest_acc, label = 'test accuracy')\n    plt.legend(loc='upper left', prop = {'size': 15})\n    plt.xticks(range(0, 20, 5))\n    plt.xlabel('max_depth', size = 20)\n    plt.ylabel('accuracy', size = 20)\n    plt.show()","9d59fc02":"#Taking fold 0 as test set and rest as training set\ncheck(fold = 0)","78553558":"#Taking fold 1 as test set and rest as training set\ncheck(fold = 1)","5a92d3b2":"#Taking fold 2 as test set and rest as training set\ncheck(fold = 2)","8918df12":"#Taking fold 3 as test set and rest as training set\ncheck(fold = 3)","a750538e":"#Taking fold 4 as test set and rest as training set\ncheck(fold = 4)","fbacaad1":"X = pd.read_csv('.\/t_fold.csv')\ny = X.quality.values\nX = X.drop('quality', axis = 1).values\nclf = tree.DecisionTreeClassifier(max_depth = 20)\nscores = model_selection.cross_val_score(clf, X, y, cv=5)\nprint(scores)","c0cfb5ea":"#Calculating the average\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","503809e5":"sns.countplot(data['quality'])","cf400c1c":"#Applying stratied k-fold\ny_data = data.quality.values\nkf = model_selection.StratifiedKFold(n_splits = 5)\nfor fold, (t, v) in enumerate(kf.split(X=data, y = y_data)):\n    data.loc[v, 'kfold'] = fold\ndata.to_csv('st_fold.csv', index = False)","5b7f14a3":"def check_stratified(fold):\n    df = pd.read_csv('.\/st_fold.csv')\n    df_train = df[df.kfold != fold].reset_index(drop = True)\n    df_test = df[df.kfold == fold].reset_index(drop = True)  \n    \n    y_train = df_train.quality.values\n    x_train = df_train.drop('quality', axis = 1).values\n    \n    y_valid = df_test.quality.values\n    x_valid = df_test.drop('quality', axis = 1).values\n    \n    sktrain_acc = [0.5]\n    sktest_acc = [0.5]\n    for depth in range(1, 20):\n        clf = tree.DecisionTreeClassifier(max_depth = depth)\n\n        clf.fit(x_train, y_train)\n\n        train_pred = clf.predict(x_train)\n        acc_train = metrics.accuracy_score(y_train, train_pred)\n\n        test_pred = clf.predict(x_valid)\n        acc_test = metrics.accuracy_score(y_valid, test_pred)\n\n        sktrain_acc.append(acc_train)\n        sktest_acc.append(acc_test)\n    plt.figure(figsize=(10,5))\n    sns.set_style('whitegrid')\n    plt.plot(sktrain_acc, label = 'train accuracy')\n    plt.plot(sktest_acc, label = 'test accuracy')\n    plt.legend(loc='upper left', prop = {'size': 15})\n    plt.xticks(range(0, 20, 5))\n    plt.xlabel('max_depth', size = 20)\n    plt.ylabel('accuracy', size = 20)\n    plt.show()","fdd5bf49":"#Taking fold 0 as test set and rest as training set\ncheck_stratified(fold = 0)","44b0094a":"#Taking fold 1 as test set and rest as training set\ncheck_stratified(fold = 1)","649a8b92":"#Taking fold 2 as test set and rest as training set\ncheck_stratified(fold = 2)","dff09772":"#Taking fold 3 as test set and rest as training set\ncheck_stratified(fold = 3)","0da22835":"#Taking fold 4 as test set and rest as training set\ncheck_stratified(fold = 4)","9e5f7a6d":"X = pd.read_csv('.\/st_fold.csv')\ny = X.quality.values\nX = X.drop('quality', axis = 1).values\nclf = tree.DecisionTreeClassifier(max_depth = 20)\nscores = model_selection.cross_val_score(clf, X, y, cv=5)\nprint(scores)\n#Calculating the average\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","223b962f":"You can see from above graph visualisation and the accuracy score that stratified k fold produced much better result than the k fold technique of model validation. Thus whenever there is an uneven distribution of targets choose stratified k-fold instead of simple k-fold validation.","f7305c1f":"# K-Fold Cross Validation:\nIt involves randomly k-fold CV dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the machine learing model is fit on the remaining k - 1 folds.\n\nThis procedure is repeated k times; each time, a different group of observations is treated as a validation set.\n![image.png](attachment:image.png)\n\nImage Source: [https:\/\/towardsdatascience.com\/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85](http:\/\/)","9e793a12":"# Model Validation:\n\nFor simple model selection technique I have divided the data(that contains 1599 entries) into train and test set. Train test containing first 1000 entries and test set containing last 599 entries. We will train the model on first 1000 entries and check the model by predicting and comparing with test set.\n\n![](http:\/\/)![image.png](attachment:image.png)","f10f1ecc":"In this notebook only DecisionTreeClassifier is used to show how the model accuracy varies with respect to different model validation methods.","ec836610":"From the graph plotted above it is quite clear that the model fails in predicting most real world data accurately, with the highest accuracy of approx 0.57 at the max depth of 5. The model overfits as the train accuracy is much higher than the test accuracy.","b5fdedea":"Since the quality varies from 3 to 8, let's map these quantities in range 0 to 5 for better predictions.","bb78d68e":"# Introduction:\n\n\nModel Validation: The process of determining the degree to which the model corresponds to the real system is referred to as model validation. It represents how your model acts to the real world data and helps in determining how good the model is trained.\nSteps involved in model selection:\n* Reserve a sample data set\n* Train the model using the remaining part of the dataset\n* Use the reserve sample of the test (validation) set to test the effectiveness of your model\u2019s performance.\n\n","531986c7":"# Stratified K Fold \n\nFrom the Graph above it is quite clear that the data is skewed for a classification problem as there is very less data availble for the wine with quality index 0. Wine with quality index 1 and 5 also have a little sample, while the wine with quality index 2 and 3 have a huge amount of samples availble. For this classification purpose, simple kfold validation doesn't produced good results. So we move to another cross validation technique called stratified k fold.\n\nThis cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n![image.png](attachment:image.png)\n\n\nThe implementation is designed to:\n\n* Generate test sets such that all contain the same distribution of classes, or as close as possible.\n* Be invariant to class label: relabelling y = [\"Happy\", \"Sad\"] to y = [1, 0] should not change the indices generated.\n* Preserve order dependencies in the dataset ordering, when shuffle=False: all samples from class k in some test set were contiguous in y, or separated in y by samples from classes other than k.\n* Generate test sets where the smallest and largest differ by at most one sample.\n\nSource: [https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html](http:\/\/)\n","4a0d8db9":"# Cross Validation:\n\nCross validation is a technique in the process of building any machine learning model which ensures that the modle fit the data accurately and doesn't overfit the data. \n\n\nIn this notebook we will only look at K-Fold cross validation and startified K-Fold cross validation.","fc9609d7":"The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. The code below shows the result at the max depth of 12 and the accuracy is at differnt folds taken one at a time and the the average is calculated.","38bfd619":"First we will see the accuracy by taking different training and validation sets and its accuracy with the increasing depth of decision tree.","57f85b48":"But still we can see the problem of overfitting, let's try to improve the model further by trying another validation technique","fe4b090d":"This notebook is based on the **'Red Wine Quality dataset'** which contanis only physicochemical (inputs) and sensory (the output) data for red wine.\nContent\n\nFor more information, read [Cortez et al., 2009].\n\nInput variables (based on physicochemical tests):\n\n1 - fixed acidity\n\n2 - volatile acidity\n\n3 - citric acid\n\n4 - residual sugar\n\n5 - chlorides\n\n6 - free sulfur dioxide\n\n7 - total sulfur dioxide\n\n8 - density\n\n9 - pH\n\n10 - sulphates\n\n11 - alcohol\n\nOutput variable (based on sensory data):\n\n12 - quality (score between 0 and 10) "}}