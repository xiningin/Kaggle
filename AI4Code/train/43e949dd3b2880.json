{"cell_type":{"0daec166":"code","6af2ad84":"code","5c71df4b":"code","4daa9964":"code","bd6e6db4":"code","7c4f55fe":"code","a60ba38f":"code","2ed752d9":"code","ac99dbac":"code","9419fec1":"code","08715d2c":"code","e50e51f7":"code","b989573c":"code","a4f0bec3":"code","3cb74875":"code","92fc4929":"code","5b1d2f91":"code","cf5dedeb":"code","414a5608":"code","89a7464d":"code","b4beb58e":"code","c41338a3":"code","38191ca2":"code","25c1ed6d":"code","2ac4e2a4":"code","276face2":"code","c86d9da7":"code","744f6f53":"code","5d6ef64c":"code","d6f17e86":"code","fdd71fac":"code","5bd781db":"code","60580714":"code","b481b12c":"code","6f55542d":"code","36a72c61":"code","43ce381b":"code","26daa6fe":"code","69791f8e":"markdown","71fff3e7":"markdown","44092df5":"markdown","0669e52a":"markdown","6c1866df":"markdown","fb83c58f":"markdown","14314061":"markdown","38e0e058":"markdown","14dd4771":"markdown","b8bedaca":"markdown","f061e729":"markdown","f062fd5e":"markdown","1975a483":"markdown","946ed8f6":"markdown","c7b2be94":"markdown","c18e30e5":"markdown","00980d13":"markdown","d76d4695":"markdown","721bad3c":"markdown","dd157482":"markdown","aa7a9cfd":"markdown","37d14675":"markdown","950e97e7":"markdown","048ddc99":"markdown","f46726fa":"markdown","67154a64":"markdown","494c15bd":"markdown","ab4215e7":"markdown","8b757bb4":"markdown","f747bcdb":"markdown","c573015f":"markdown","c9d58b7f":"markdown","d682ddff":"markdown","063751ac":"markdown","d89c5403":"markdown","c2d2fecc":"markdown","7b0e21cd":"markdown","4a6c5bd2":"markdown","bff21ba4":"markdown","3d49bf3c":"markdown","9beb6b72":"markdown","eec65563":"markdown","2349a54c":"markdown","64cbe569":"markdown","166b448b":"markdown","f28c5e66":"markdown"},"source":{"0daec166":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom datetime import datetime\n\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","6af2ad84":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(\"Train set size:\", train.shape)\nprint(\"Test set size:\", test.shape)","5c71df4b":"train_ID = train['Id']\ntest_ID = test['Id']\n\n# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","4daa9964":"corr = train.corr()\nplt.subplots(figsize=(13,10))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)","bd6e6db4":"train.get_dtype_counts()","7c4f55fe":"# Heatmap between top 10 correlated variables with SalePrice\ncorrMatrix=train[[\"SalePrice\",\"OverallQual\",\"GrLivArea\",\"GarageCars\",\n                  \"GarageArea\",\"GarageYrBlt\",\"TotalBsmtSF\",\"1stFlrSF\",\"FullBath\",\n                  \"TotRmsAbvGrd\",\"YearBuilt\",\"YearRemodAdd\"]].corr()\n\nsns.set(font_scale=1.10)\nplt.figure(figsize=(10, 10))\n\nsns.heatmap(corrMatrix, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='viridis',linecolor=\"white\")\nplt.title('Correlation between features');","a60ba38f":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","2ed752d9":"print(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","ac99dbac":"sns.factorplot(\"Fireplaces\",\"SalePrice\",data=train,hue=\"FireplaceQu\");","9419fec1":"labels = train[\"MSZoning\"].unique()\nsizes = train[\"MSZoning\"].value_counts().values\nexplode=[0.1,0,0,0,0]\nparcent = 100.*sizes\/sizes.sum()\nlabels = ['{0} - {1:1.1f} %'.format(i,j) for i,j in zip(labels, parcent)]\n\ncolors = ['yellowgreen', 'gold', 'lightblue', 'lightcoral','blue']\npatches, texts= plt.pie(sizes, colors=colors,explode=explode,\n                        shadow=True,startangle=90)\nplt.legend(patches, labels, loc=\"best\")\n\nplt.title(\"Zoning Classification\")\nplt.show()","08715d2c":"train1 = train\ntrain1['SalePriceSF'] = train['SalePrice']\/train['GrLivArea']\nplt.hist(train['SalePriceSF'], bins=15,color=\"blue\")\nplt.title(\"Sale Price per Square Foot\")\nplt.ylabel('Number of Sales')\nplt.xlabel('Price per square feet');","e50e51f7":"train1['ConstructionAge'] = train['YrSold'] - train['YearBuilt']\nplt.scatter(train1['ConstructionAge'], train['SalePriceSF'])\nplt.ylabel('Price per square foot (in dollars)')\nplt.xlabel(\"Construction Age of house\");","b989573c":"sns.stripplot(x=\"HeatingQC\", y=\"SalePrice\",data=train,hue='CentralAir',jitter=True,split=True)\nplt.title(\"Sale Price vs Heating Quality\");","a4f0bec3":"sns.factorplot(\"KitchenAbvGr\",\"SalePrice\",data=train,hue=\"KitchenQual\")\nplt.title(\"Sale Price vs Kitchen\");","3cb74875":"plt.style.use('seaborn')\nplt.scatter(train.GrLivArea, train.SalePrice)","92fc4929":"# Deleting outliers\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)","5b1d2f91":"y = train['SalePrice']\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=stats.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=stats.lognorm)","cf5dedeb":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","414a5608":"y = train.SalePrice.reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test","89a7464d":"features = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)","b4beb58e":"# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)\n\nfeatures['Functional'] = features['Functional'].fillna('Typ')\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")","c41338a3":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\n","38191ca2":"features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n","25c1ed6d":"objects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\n\nfeatures.update(features[objects].fillna('None'))","2ac4e2a4":"features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","276face2":"# Filling in the rest of the NA's\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)\n\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","c86d9da7":"features = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n\nfeatures['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n# simplified features\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","744f6f53":"print(features.shape)\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nprint(final_features.shape)\n\nX = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(X):, :]\n\nprint('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)","5d6ef64c":"outliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])\n\noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_sub = X_sub.drop(overfit, axis=1).copy()\n\nprint('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)","d6f17e86":"kfolds = KFold(n_splits=8, shuffle=True, random_state=42)","fdd71fac":"# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n\n# build our model scoring function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=kfolds))\n    return (rmse)","5bd781db":"# setup models    \nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# Ridge Regressor\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n# Lasso Regressor\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=alphas2,\n                              random_state=42, cv=kfolds))\n\n# Elasticnet Regressor\nelasticnet = make_pipeline(RobustScaler(),\n                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n                                        cv=kfolds, l1_ratio=e_l1ratio))\n                                        \n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(),\n                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\n                                   \n# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       #min_data_in_leaf=2,\n                                       #min_sum_hessian_in_leaf=11\n                                       )\n                                       \n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n                                            gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","60580714":"print('TEST score on CV')\n\nscores = {}\n\nscore = cv_rmse(ridge)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()),)\nscores['ridge'] = (score.mean(), score.std())\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), )\nscores['lasso'] = (score.mean(), score.std())\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()),)\nscores['elasticnet'] = (score.mean(), score.std())\n\nscore = cv_rmse(svr)\nprint(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()),)\nscores['svr'] = (score.mean(), score.std())\n\nscore = cv_rmse(lightgbm)\nprint(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), )\nscores['lgbm'] = (score.mean(), score.std())\n\nscore = cv_rmse(gbr)\nprint(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()),)\nscores['gbr'] = (score.mean(), score.std())\n\nscore = cv_rmse(xgboost)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), )\nscores['xgboost'] = (score.mean(), score.std())","b481b12c":"print('START Fit')\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\nprint(datetime.now(), 'elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nprint(datetime.now(), 'lasso')\nlasso_model_full_data = lasso.fit(X, y)\nprint(datetime.now(), 'ridge')\nridge_model_full_data = ridge.fit(X, y)\nprint(datetime.now(), 'svr')\nsvr_model_full_data = svr.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","6f55542d":"def blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.1 * lasso_model_full_data.predict(X)) + \\\n            (0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.25 * stack_gen_model.predict(np.array(X))))\n            \n\n# Get final precitions from the blended model\nblended_score = rmsle(y, blend_models_predict(X))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","36a72c61":"print('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_sub)))\n\n# this kernel gives lower score\n# let's up it by mixing with the top kernels\n\nprint('Blend with Top Kernals submissions', datetime.now(),)\nsub_1 = pd.read_csv('..\/input\/top-10-0-10943-stacking-mice-and-brutal-force\/House_Prices_submit.csv')\nsub_2 = pd.read_csv('..\/input\/hybrid-svm-benchmark-approach-0-11180-lb-top-2\/hybrid_solution.csv')\nsub_3 = pd.read_csv('..\/input\/lasso-model-for-regression-problem\/lasso_sol22_Median.csv')\n\nsubmission.iloc[:,1] = np.floor((0.25 * np.floor(np.expm1(blend_models_predict(X_sub)))) + \n                                (0.25 * sub_1.iloc[:,1]) + \n                                (0.25 * sub_2.iloc[:,1]) + \n                                (0.25 * sub_3.iloc[:,1]))","43ce381b":"# Brutal approach to deal with predictions close to outer range \nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\n\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint('Save submission', datetime.now(),)","26daa6fe":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Various Models', size=20)\n\nplt.show()","69791f8e":"Price of house goes down with its age.","71fff3e7":"#### 8. Building , remodelling years and age of house","44092df5":"# Blend models and get predictions","0669e52a":"#### Setup cross validation method","6c1866df":"#### 7. SalePrice per square foot","fb83c58f":"Inspirations are drawn from various Kaggle notebooks but majorly motivation is from the following :\n\n1. https:\/\/www.kaggle.com\/poonaml\/house-prices-data-exploration-and-visualisation\n\n2. https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition\n \n3. https:\/\/www.kaggle.com\/itslek\/blend-stack-lr-gb-0-10649-house-prices-v57\/data?scriptVersionId=11189608\n \n4. https:\/\/www.kaggle.com\/hemingwei\/top-2-from-laurenstc-on-house-price-prediction\/notebook\n\n5. https:\/\/www.kaggle.com\/jesucristo\/1-house-prices-solution-top-1\n\n##### Credit for image to https:\/\/www.vancouverrealestatepodcast.com\/tag\/housing-price-prediction-2019\/","14314061":"Zoning is also interesting","38e0e058":"### Deal with predictions close to outer range ","14dd4771":"#### 1. Correlation among variables","b8bedaca":"# Model","f061e729":"### Creating Dummies\n\nSince sklearn lm.fit() does not accept strings we have to convert our objects to dummy variables.","f062fd5e":"### Features Simplication\n","1975a483":"# Acknowledgments","946ed8f6":"### NA's\nLet's figure out what NA's excist, sort them by categories and impute them in the best possible way.","c7b2be94":"#### Get cross validation scores for each model","c18e30e5":"### Identify the best performing model","00980d13":"#### Defining model scoring function ","d76d4695":"We can observe from the graph above that the blended model far outperforms the other models, with an RMSLE of 0.0170 .  This is the model which will be used for making the final predictions.","721bad3c":"### Concatenation\nTo keep consistency between test and train features we concatenate the two sets while remembering the index so we can split it later again.","dd157482":"#### Setting Up Models","aa7a9cfd":"### Overfitting prevention\n","37d14675":"# Data Cleaning and Pre-processing","950e97e7":"# House Prices : Visualization & Prediction\n### Predict sales prices and practice different machine learning regressors. \n##### ( \u2b50\ufe0f Upvote my Notebook \u2014 it helps! )\n\n<img src=\"https:\/\/www.vancouverrealestatepodcast.com\/wp-content\/uploads\/2018\/10\/Detached-home-prices.jpg\" alt=\"Smiley face\" height=\"50%\" width=\"70%\">\n\n\n\n\nGetting started with competitive data science can be quite intimidating. So I build this notebook for quick overview on the Advanced Regression Techniques competition. If there is interest, I\u2019m happy to do deep dives into the intuition behind the feature engineering and models used in this kernel.\n\nI encourage you to fork this kernel, play with the code and enter the competition. Good luck!\n\n**Competition Description**\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. Each row in the dataset describes the characteristics of a house. This competition challenges you to predict the final price of each home.\n\n**Executive Summary**\n\nI started this competition by just focusing on getting a good understanding of the dataset. The EDA & Visualizations are included to allow developers to dive into analysis of dataset. \n\n\n### Key features of the model training process in this kernel\n\n- Cross Validation: Using 10-fold cross-validation\n- Models: On each run of cross-validation tried fitting following models :-\n    - Elastic model\n    - Lasso regression model \n    - XGBoost model\n    - LGBM model\n    - SVR model\n    - Ridge model\n    - GBR model\n- Stacking : Trained a meta StackingCVRegressor optimized using xgboost\n- Blending: All models trained will overfit the training data to varying degrees. Therefore, to make final predictions, I blended their predictions together to get more robust predictions.\n","048ddc99":"The lotfrontage averages differ a lot per neighborhood so let's impute with the median per neighborhood.","f46726fa":"### Blend with Top Kernals submissions","67154a64":"Having Heating & AC arrangements definitely escalates price of house.","494c15bd":"There seems to be clear evidence of right-skewedness in the target variable. We can correct this with a simple log transformation.","ab4215e7":"#### 3. Heatmap","8b757bb4":"My GitHub Project Link - https:\/\/github.com\/RohitLearner\/House-Prices-Visualization-Prediction . ","f747bcdb":"### Outlier","c573015f":"#### 9. Heating and AC arrangements","c9d58b7f":"Let's impute all incongruencies with the most likely value.","d682ddff":"If you really enjoyed above kernel, then you might want to take a close look at following interesting kernels  :-\n\n1. Flight Crash Investigation - https:\/\/www.kaggle.com\/iamrohitsingh\/flight-crash-investigation\/\n\n2. Titanic : Visualization & Prediction - https:\/\/www.kaggle.com\/iamrohitsingh\/titanic-visualization-prediction\n\n3. YouTube India Data Exploration - https:\/\/www.kaggle.com\/iamrohitsingh\/youtube-india-trending-data-exploration","063751ac":"#### 2. Various datatypes columns","d89c5403":"#### 5. Fireplaces Variable Factor","c2d2fecc":"For the rest we will just use a loop to impute 'None' value","7b0e21cd":"Having 2 fireplaces increases house price and fireplace of Excellent quality is a big plus","4a6c5bd2":"The rest can be safely imputed with 0 since this means that the property is not present in the house.","bff21ba4":"### Basic Visualization","3d49bf3c":"# Train models","9beb6b72":"### Fit the models","eec65563":"#### 10. Kitchen Quality","2349a54c":"# Feature Engineering","64cbe569":"#### 4. SalePrice: the variable we're trying to predict","166b448b":"#### 6. MSZoning","f28c5e66":"Having only one Kitchen of Ex (Excellent) quality hikes house price like anything."}}