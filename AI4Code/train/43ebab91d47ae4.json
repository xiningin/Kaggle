{"cell_type":{"f3b20e62":"code","28ac1b6b":"code","5ac74d0d":"code","58d61b38":"code","0a046ca5":"code","758761f1":"code","6d799728":"code","f85ee065":"code","88e9c4fa":"code","ebb7e608":"code","6ad4a666":"code","e31937f5":"code","3ac7a95b":"code","aa519e0a":"code","2d0b1e36":"code","cb4eb447":"code","2665de46":"code","14f2021f":"code","aa505656":"code","bcbdb3fd":"code","b521f1cf":"code","db98de12":"code","87661a41":"code","a1f09567":"code","eeb53040":"code","d97eb2ac":"code","5426d8e9":"code","ddd154bc":"code","2b2f9fc6":"code","a8ff1cf5":"code","32667f41":"code","b39a5e91":"code","f156079f":"markdown","599e02b8":"markdown","f713fec9":"markdown","424880ee":"markdown","dfe3fd59":"markdown"},"source":{"f3b20e62":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport lightgbm as lgb\nplt.style.use('ggplot')\nsns.set(font_scale=1)\npd.set_option('display.max_columns', 500)","28ac1b6b":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","5ac74d0d":"features = [c for c in train.columns if c not in ['ID_code', 'target']]\nfeatures_t = [c for c in test.columns if c not in ['ID_code']]\ny = train.target","58d61b38":"train.head()","0a046ca5":"unique = []\nfor col in train.columns:\n    if col == 'ID_code' or col == 'target' : pass\n    else:\n        unique.append([len(train[col].unique()),col])\n    gc.collect()\nunique = sorted(unique)","758761f1":"unique[:][:10]","6d799728":"def plot_col(train, y=y, rng=5):\n    for i in range(rng):\n        plt.figure(figsize=(20,8))\n        if 'target' not in train.columns:\n            train['target'] = y\n        else: pass\n        fig, axis = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n        sns.distplot(train[unique[i][1]].loc[train['target'] == 0], kde=True, label='target 0',ax=axis[0])\n        sns.distplot(train[unique[i][1]].loc[train['target'] == 1], kde=True, label='target 1',ax=axis[0])\n        sns.boxplot(train[unique[i][1]], ax = axis[1])\n        plt.title('{}'.format(unique[i][1]))\n        plt.legend()\n        plt.show()","f85ee065":"%%time\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\ndef clusters(train,y):\n    distance = []\n    for cluster in range(2,11,1):\n        plt.figure(figsize=(10,7))\n        print('Started checking {}'.format(cluster))\n        kmeans = KMeans(cluster, random_state=2702)\n        labels = kmeans.fit_predict(train)\n        plt.title('Clusters {}'.format(cluster))\n        sns.countplot(labels, hue=y)\n        plt.show()\n        distance.append(kmeans.inertia_)\n        gc.collect()\n    return distance\ntrain_dist = clusters(train[features],train.target)","88e9c4fa":"plt.figure(figsize=(15,6))\nplt.plot(train_dist, 'go--')","ebb7e608":"def plot_data(train, test):\n    vlas = train.columns.values\n    vlas_t = test.columns.values\n    plt.figure(figsize=(20,8))\n    plt.title(\"Distribution of mean values per row in the train and test set\")\n    sns.distplot(train[vlas].mean(axis=1),color=\"green\", kde=True,bins=100, label='train')\n    sns.distplot(test[vlas_t].mean(axis=1), color='red', kde=True, bins=100, label='test')\n    plt.legend()\n    plt.show()\n    \n    plt.figure(figsize=(20,8))\n    plt.title(\"Distribution of std values per row in the train and test set\")\n    sns.distplot(train[vlas].std(axis=1),color=\"green\", kde=True,bins=100, label='train')\n    sns.distplot(test[vlas_t].std(axis=1), color='red', kde=True, bins=100, label='test')\n    plt.legend()\n    plt.show()\n    \n    plt.figure(figsize=(20,8))\n    plt.title(\"Distribution of max values per row in the train and test set\")\n    sns.distplot(train[vlas].max(axis=1), color=\"green\", kde=True, bins=100, label='train')\n    sns.distplot(test[vlas_t].max(axis=1), color='red', kde=True, bins=100, label='test')\n    plt.legend()\n    plt.show()\n    \n    plt.figure(figsize=(20,8))\n    plt.title(\"Distribution of min values per row in the train and test set\")\n    sns.distplot(train[vlas].min(axis=1), color=\"green\", kde=True, bins=100, label='train')\n    sns.distplot(test[vlas_t].min(axis=1), color='red', kde=True, bins=100, label='test')\n    plt.legend()\n    plt.show()\n    \nplot_data(train[features], test[features_t])","6ad4a666":"plot_col(train)","e31937f5":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler(copy=False)\ntrain_sc = pd.DataFrame(sc.fit_transform(train[features]), columns=features)\ntest_sc = pd.DataFrame(sc.fit_transform(test[features_t]), columns=features_t)\ngc.collect()","3ac7a95b":"plot_data(train_sc, test_sc)","aa519e0a":"plot_col(train_sc)","2d0b1e36":"from sklearn.preprocessing import MinMaxScaler\nmn = MinMaxScaler(copy=False)\ntrain_mn = pd.DataFrame(mn.fit_transform(train[features]), columns=features)\ntest_mn = pd.DataFrame(mn.fit_transform(test[features_t]), columns=features_t)\ngc.collect()","cb4eb447":"plot_data(train_mn, test_mn)","2665de46":"plot_col(train_mn)","14f2021f":"from sklearn.preprocessing import QuantileTransformer\nqt = QuantileTransformer(n_quantiles = 200)\ntrain_qt = pd.DataFrame(mn.fit_transform(train[features]), columns=features)\ntest_qt = pd.DataFrame(mn.fit_transform(test[features_t]), columns=features_t)\ngc.collect()","aa505656":"plot_data(train_qt, test_qt)","bcbdb3fd":"from sklearn.preprocessing import RobustScaler\nrs = RobustScaler(copy=False)\ntrain_rs = pd.DataFrame(rs.fit_transform(train[features]), columns=features)\ntest_rs = pd.DataFrame(rs.fit_transform(test[features_t]), columns=features_t)\ngc.collect()","b521f1cf":"plot_data(train_rs, test_rs)","db98de12":"plot_col(train_rs)","87661a41":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport eli5\nX_train, X_test, y_train, y_test = train_test_split(\n    train[features], y, test_size=0.3,stratify = y,  random_state=2701)\nmodel = lgb.LGBMClassifier(\n        n_estimators = 5000,\n        learning_rate= 0.1,\n        metric='auc',\n        )\n\nmodel.fit(X_train, y_train)\neli5.explain_weights(model)","a1f09567":"pred = model.predict(X_test)\nprint(f'AUC: {roc_auc_score(pred, y_test)}')","eeb53040":"X_train, X_test, y_train, y_test = train_test_split(\n    train_sc[features], y, test_size=0.3,stratify = y,  random_state=2701)\nmodel.fit(X_train, y_train)\neli5.explain_weights(model)","d97eb2ac":"pred = model.predict(X_test)\nprint(f'AUC: {roc_auc_score(pred, y_test)}')","5426d8e9":"X_train, X_test, y_train, y_test = train_test_split(\n    train_qt[features], y, test_size=0.3,stratify = y,  random_state=2701)\nmodel.fit(X_train, y_train)\neli5.explain_weights(model)","ddd154bc":"X_train, X_test, y_train, y_test = train_test_split(\n    train_rs[features], y, test_size=0.3,stratify = y,  random_state=2701)\nmodel.fit(X_train, y_train)\neli5.explain_weights(model)","2b2f9fc6":"pred = model.predict(X_test)\nprint(f'AUC: {roc_auc_score(pred, y_test)}')","a8ff1cf5":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.335,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.041,\n    'learning_rate': 0.0083,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': -1\n}","32667f41":"from sklearn.model_selection import StratifiedKFold\nnum_folds = 11\n\nfolds = StratifiedKFold(n_splits=num_folds, shuffle=False, random_state=2702)\noof = np.zeros(len(train))\npredictions = np.zeros(len(y))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, y.values)):\n    print(\"Fold idx:{}\".format(fold_ + 1))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=y.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=y.iloc[val_idx])\n    \n    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=2000, early_stopping_rounds = 4000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y, oof)))","b39a5e91":"sub = pd.DataFrame({\"ID_code\": test.ID_code.values})\nsub[\"target\"] = predictions\nsub.to_csv('submission.csv', index=False)","f156079f":"**For examples, set the number of columns for visualization to 5, and we will build histograms and boxplots to look at the outliers**","599e02b8":"**\u0421reate a base model to test it on all types of transformations, and find out the importance of features\n**","f713fec9":"**Look at the number of unique entries in the columns, and sort to check if the data can be coded by category.\n**","424880ee":"**Make a function to draw the parameters, such as mean, standard deviation, minimum and maximum, to see how the transformations change our data**","dfe3fd59":"**Check how many clusters can be distinguished in the data. This can be done by plotting the distance of the points, and in the place of the \"break\" of this graph there should be an optimal number of clasetters\n**"}}