{"cell_type":{"5a59cf0f":"code","8fb8050a":"code","0582bc63":"code","bb435d89":"code","822d943b":"code","30b54200":"code","4fde8d4a":"code","849360ac":"code","12591bc0":"code","dc0e3e09":"code","02600562":"code","900684a8":"code","29f1f66a":"code","c1bc9d62":"markdown","42a8b1fe":"markdown","72459277":"markdown","06370366":"markdown","735c4588":"markdown","23b7befa":"markdown","d72ee938":"markdown"},"source":{"5a59cf0f":"import os\nimport math\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom itertools import chain, combinations\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, cross_val_score, GridSearchCV\n\nfrom sklearn.covariance import EllipticEnvelope\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n\nfrom sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score\nfrom sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, plot_roc_curve\n\nnp.set_printoptions(suppress=True)\nsns.set()\n\ntry:\n    os.environ['KAGGLE_DATA_PROXY_TOKEN']\nexcept KeyError:\n    path = \"creditcard.csv\"\nelse:\n    path = \"\/kaggle\/input\/creditcardfraud\/creditcard.csv\"","8fb8050a":"data = pd.read_csv(path)","0582bc63":"data","bb435d89":"data.isna().any().any()","822d943b":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_ix, test_ix in split.split(data, data[\"Class\"]):\n    df_train = data.iloc[train_ix]\n    df_test = data.iloc[test_ix]","30b54200":"X = df_train.drop([\"Class\"], axis=1)\n[variance_inflation_factor(X.values, i) for i in range(len(X.columns))] ","4fde8d4a":"df_train = df_train.drop([\"Amount\"], axis=1)\ndf_test = df_test.drop([\"Amount\"], axis=1)","849360ac":"fig, axs = plt.subplots(6, 5, figsize = (30, 45))\nfig.subplots_adjust(hspace=0.3, wspace=0.3)\n\nfor ix in range(len(df_train.drop([\"Class\"], axis=1).columns)):\n    col = df_train.columns[ix]\n    ax = axs.flat[ix]\n    \n    sns.boxenplot(x=\"Class\", y=col, data=df_train, ax=ax)\n    \n#     label = ax.xaxis.get_label()\n    ax.set_xlabel(col, fontsize=22)\n    \n    for tick in ax.xaxis.get_major_ticks():\n        tick.label.set_fontsize(20)\n\n    for tick in ax.yaxis.get_major_ticks():\n        tick.label.set_fontsize(15)","12591bc0":"fig, axs = plt.subplots(15, 2, figsize = (40, 70))\nfig.subplots_adjust(hspace=0.3, wspace=0.3)\n\nfor ix in range(len(df_train.drop([\"Class\"], axis=1).columns)):\n    col = df_train.columns[ix]\n    ax = axs.flat[ix]\n    \n    sns.distplot(df_train[col], kde=True, ax=ax)\n    \n    ax.set_xlabel(col, fontsize=22)\n    \n    for tick in ax.xaxis.get_major_ticks():\n        tick.label.set_fontsize(20)\n\n    for tick in ax.yaxis.get_major_ticks():\n        tick.label.set_fontsize(15)","dc0e3e09":"cols = [\"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\"]","02600562":"# def powerset(l):\n#     return chain.from_iterable(combinations(l, x) for x in range(1, len(l) + 1))","900684a8":"y_test = df_test[\"Class\"]\n\noutlier_clf = EllipticEnvelope()\noutlier_clf.fit(df_train[cols])\noutliers = outlier_clf.predict(df_test[cols])\nmap_vals = {1 : 0, -1 : 1}\npred = np.vectorize(map_vals.get)(outliers)\nconfusion_matrix(y_test, pred, normalize=\"true\")","29f1f66a":"y_test = df_test[\"Class\"]\nmodel = QuadraticDiscriminantAnalysis(reg_param=0.65) \n\nmodel.fit(df_train[cols], df_train[\"Class\"])\npred = model.predict(df_test[cols])\nconfusion_matrix(y_test, pred, normalize=\"true\")","c1bc9d62":"# Credit Card Fraud\n\n## Dataset description\nhttps:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n## Result\n\nMy imagined scenario was that preferably close to no fraud should go unidentified while the number of transactions falsely classified as frauds should not take to much resources - e.g. workers checking them. \nThis means to reach as high a recall as possible while maintaining a tolerable false positive rate.\n\nIt turned to be **very** hard to raise the recall and not raise the FPR to intolerable numbers (which I set to < 90% and > 15%, respectively). Other tested models were KNN and random forest. Both did very well in terms of recall but not in terms of FPR. Changing the threshold for the random forest messed up either recall or FPR.\nChanging dimensionality, feature selection, reduction, over- and undersampling did not help either or only a little. KNN drastically and random forest completely overfitted the training data, which I failed to manage as well by tweaking hyper parameters.\n  \nThe notebook only shows the end result of a variety of examinations, tested models and approaches. Since the assumption of normal distribution holds approximately for the selected features I tried EllipticEnvelope and QDA, and they were by far the only models that managed to get an ok recall and FPR, ~ 91% and ~ 10%, respectively, for EE. Checking one out of ten transactions if probably still way too expensive. QDA has ~ 89% recall but only < 1% FPR, which provides a more realistic result.","42a8b1fe":"Drop that one collinear feature.","72459277":"Choose features where the classes look well separable and approximately normally distributed.","06370366":"Are there missing values?","735c4588":"## Assumption of model and distribution\nNot knowing the original features and the result of the PCA still being high-dimensional makes assuming a distribution and a good model challenging.  ","23b7befa":"With almost all features we seem to have a multivariate normal distribution. So, to **detect outliers \/ anomalies** next we should try Elliptic Envelope.","d72ee938":"Examine how the values for the features are distributed for both classes:"}}