{"cell_type":{"c8d74c2a":"code","e3f2503f":"code","1df18f95":"code","e0e2563d":"code","2c22c347":"code","faccf0bb":"code","89c1eb9c":"code","54b167b7":"code","507aaf88":"code","ab68df5c":"code","a3d9751c":"code","50c0627a":"code","4982da26":"code","46656e3d":"code","68f928fd":"code","32addfb9":"code","6d6aff95":"code","37026dd7":"code","f8331c84":"code","b67e7c72":"code","dbc851df":"code","bcd3952e":"code","b36828c9":"code","624d0482":"markdown","8c7f82c1":"markdown","353a1e9e":"markdown","ad8cd48d":"markdown","cdf5d9ac":"markdown","219f8fc6":"markdown"},"source":{"c8d74c2a":"# IMPORTING NECESSARY MODULES FOR DATA ANALYSIS AND PREDICTIVE MODELLING\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport lightgbm as lgb\nimport re\nimport gc\nimport os\nimport psutil\nimport humanize\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML, display, clear_output\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","e3f2503f":"print(os.listdir(\"..\/input\"))","1df18f95":"DIR = '..\/input\/test\/'\nprint(len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))","e0e2563d":"TrainDataPath = '..\/input\/train.csv'\nTestDataPath = '..\/input\/test\/seg_00030f.csv' # Randomly taking a sample test data\nSubDataPath = '..\/input\/sample_submission.csv'\n\n# Loading the Training Dataset and Submission File\nTrainData = pd.read_csv(TrainDataPath, nrows=10000000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\nTestData = pd.read_csv(TestDataPath, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\nSubData = pd.read_csv(SubDataPath)","2c22c347":"print(\"Training Dataset Shape:\")\nprint(TrainData.shape)\nprint(\"\\n\")\nprint(\"Training Dataset Columns\/Features:\")\nprint(TrainData.dtypes)\nTrainData.head()","faccf0bb":"print(\"Test Dataset Shape:\")\nprint(TestData.shape)\nprint(\"\\n\")\nprint(\"Test Dataset Columns\/Features:\")\nprint(TestData.dtypes)\nTestData.head()","89c1eb9c":"print(\"Submission Dataset Shape:\")\nprint(SubData.shape)\nprint(\"\\n\")\nprint(\"Submission Dataset Columns\/Features:\")\nprint(SubData.dtypes)\nSubData.head()","54b167b7":"# checking missing data percentage in train data\ntotal = TrainData.isnull().sum().sort_values(ascending = False)\npercent = (TrainData.isnull().sum()\/TrainData.isnull().count()*100).sort_values(ascending = False)\nmissing_TrainData  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_TrainData.head(10)","507aaf88":"# checking missing data percentage in test data\ntotal = TestData.isnull().sum().sort_values(ascending = False)\npercent = (TestData.isnull().sum()\/TestData.isnull().count()*100).sort_values(ascending = False)\nmissing_TrainData  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_TrainData.head(10)","ab68df5c":"def printmemusage():\n process = psutil.Process(os.getpid())\n print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\nprintmemusage()","a3d9751c":"def plot_bar_counts_categorical(data_se, title, figsize, sort_by_counts=False):\n    info = data_se.value_counts()\n    info_norm = data_se.value_counts(normalize=True)\n    categories = info.index.values\n    counts = info.values\n    counts_norm = info_norm.values\n    fig, ax = plt.subplots(figsize=figsize)\n    if data_se.dtype in ['object']:\n        if sort_by_counts == False:\n            inds = categories.argsort()\n            counts = counts[inds]\n            counts_norm = counts_norm[inds]\n            categories = categories[inds]\n        ax = sns.barplot(counts, categories, orient = \"h\", ax=ax)\n        ax.set(xlabel=\"count\", ylabel=data_se.name)\n        ax.set_title(\"Distribution of \" + title)\n        for n, da in enumerate(counts):\n            ax.text(da, n, str(da)+ \",  \" + str(round(counts_norm[n]*100,2)) + \" %\", fontsize=10, va='center')\n    else:\n        inds = categories.argsort()\n        counts_sorted = counts[inds]\n        counts_norm_sorted = counts_norm[inds]\n        ax = sns.barplot(categories, counts, orient = \"v\", ax=ax)\n        ax.set(xlabel=data_se.name, ylabel='count')\n        ax.set_title(\"Distribution of \" + title)\n        for n, da in enumerate(counts_sorted):\n            ax.text(n, da, str(da)+ \",  \" + str(round(counts_norm_sorted[n]*100,2)) + \" %\", fontsize=10, ha='center')","50c0627a":"def count_plot_by_hue(data_se, hue_se, title, figsize, sort_by_counts=False):\n    if sort_by_counts == False:\n        order = data_se.unique()\n        order.sort()\n    else:\n        order = data_se.value_counts().index.values\n    off_hue = hue_se.nunique()\n    off = len(order)\n    fig, ax = plt.subplots(figsize=figsize)\n    ax = sns.countplot(y=data_se, hue=hue_se, order=order, ax=ax)\n    ax.set_title(title)\n    patches = ax.patches\n    for i, p in enumerate(ax.patches):\n        x=p.get_bbox().get_points()[1,0]\n        y=p.get_bbox().get_points()[:,1]\n        total = x\n        p = i\n        q = i\n        while(q < (off_hue*off)):\n            p = p - off\n            if p >= 0:\n                total = total + (patches[p].get_bbox().get_points()[1,0] if not np.isnan(patches[p].get_bbox().get_points()[1,0]) else 0)\n            else:\n                q = q + off\n                if q < (off*off_hue):\n                    total = total + (patches[q].get_bbox().get_points()[1,0] if not np.isnan(patches[q].get_bbox().get_points()[1,0]) else 0)\n       \n        perc = str(round(100*(x\/total), 2)) + \" %\"\n        \n        if not np.isnan(x):\n            ax.text(x, y.mean(), str(int(x)) + \",  \" + perc, va='center')\n    plt.show()","4982da26":"def show_unique(data_se):\n    display(HTML('<h5><font color=\"green\"> Shape Of Dataset Is: ' + str(data_se.shape) + '<\/font><\/h5>'))\n    for i in data_se.columns:\n        if data_se[i].nunique() == data_se.shape[0]:\n            display(HTML('<font color=\"red\"> ATTENTION!!! ' + str(i+' --> '+str(data_se[i].nunique())) + '<\/font>'))\n        elif (data_se[i].nunique() == 1):\n            display(HTML('<font color=\"Blue\"> ATTENTION!!! ' + str(i+' --> '+str(data_se[i].nunique())) + '<\/font>'))\n        else:\n            print(i+' -->', data_se[i].nunique())","46656e3d":"def show_countplot(data_se):\n    display(HTML('<h2><font color=\"blue\"> Dataset CountPlot Visualization: <\/font><\/h2>'))\n    for i in data_se.columns:\n        if (data_se[i].nunique() <= 10):\n            plot_bar_counts_categorical(data_se[i].astype(str), 'Dataset Column: '+ i, (15,7))\n        elif (data_se[i].nunique() > 10 and data_se[i].nunique() <= 20):\n            plot_bar_counts_categorical(data_se[i].astype(str), 'Dataset Column: '+ i, (15,12))\n        else:\n            print('Columns do not fit in display '+i+' -->', data_se[i].nunique())","68f928fd":"gc.collect() # Python garbage collection module for dereferencing the memory pointers and making memory available for better usage","32addfb9":"TrainData.head()","6d6aff95":"TestData.head()","37026dd7":"# show_unique function shows the no of unique values present in the each column of the dataset\nshow_unique(TrainData)","f8331c84":"show_unique(TestData)","b67e7c72":"# TRAIN DATA HeatMap\nf,ax = plt.subplots(figsize=(10, 5))\nsns.heatmap(TrainData.corr(), annot=True, linewidths=.2, fmt= '.1f',ax=ax,cmap='Blues')","dbc851df":"plt.figure(figsize=(18, 5))\nsns.distplot((TrainData[\"acoustic_data\"]))\nplt.title('TRAIN DATA')\nplt.show()\n\nplt.figure(figsize=(18, 5))\nsns.distplot((TrainData[\"time_to_failure\"]))\nplt.title('TRAIN DATA')\nplt.show()","bcd3952e":"fig, ax = plt.subplots(2,1, figsize=(18,12))\nax[1].plot(TrainData.index.values, TrainData.time_to_failure.values, c=\"darkred\")\nax[1].set_title(\"Quaketime of 10 Mio rows\")\nax[1].set_xlabel(\"Index\")\nax[1].set_ylabel(\"Quaketime in ms\");\nax[0].plot(TrainData.index.values, TrainData.acoustic_data.values, c=\"mediumseagreen\")\nax[0].set_title(\"Signal of 10 Mio rows\")\nax[0].set_xlabel(\"Index\")\nax[0].set_ylabel(\"Acoustic Signal\");","b36828c9":"fig, ax = plt.subplots(3,1,figsize=(18,18))\nax[0].plot(TrainData.index.values[0:50000], TrainData.time_to_failure.values[0:50000], c=\"Red\")\nax[0].set_xlabel(\"Index\")\nax[0].set_ylabel(\"Time to quake\")\nax[0].set_title(\"How does the second quaketime pattern look like?\")\nax[1].plot(TrainData.index.values[0:49999], np.diff(TrainData.time_to_failure.values[0:50000]))\nax[1].set_xlabel(\"Index\")\nax[1].set_ylabel(\"Difference between quaketimes\")\nax[1].set_title(\"Are the jumps always the same?\")\nax[2].plot(TrainData.index.values[0:4000], TrainData.time_to_failure.values[0:4000])\nax[2].set_xlabel(\"Index from 0 to 4000\")\nax[2].set_ylabel(\"Quaketime\")\nax[2].set_title(\"How does the quaketime changes within the first block?\");","624d0482":"# From above code patch we can see that we have around 2624 test files ","8c7f82c1":"# I Will Be Soon Updating The Whole Notebook.","353a1e9e":"# Please Upvote, Your Support Is Very Much Required.","ad8cd48d":"# HELPER FUNCTIONS","cdf5d9ac":"### We can't see any correlation among the columns ","219f8fc6":"# Ok Now We Should Start Getting Some Insights About the Data"}}