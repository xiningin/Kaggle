{"cell_type":{"6bc05468":"code","919eef48":"code","74acb47b":"code","b17d3ecc":"code","be644b43":"code","ae3c3d8b":"code","762ff945":"code","8a87af2b":"code","dbd05b00":"code","61c796b4":"code","5edac345":"code","d382a32d":"code","1ee6e242":"code","a2cba263":"code","a3826305":"code","e1e1d7b8":"code","b0a2dd83":"code","04c395d7":"code","4990ced8":"code","4b6e95e0":"code","92a390a1":"code","fb6b6ae0":"code","9a9f54b4":"code","9be2c231":"code","969bf670":"code","b0291879":"code","bc80522b":"code","23baa592":"code","0342dc27":"code","d9522c85":"code","8c3908df":"code","8175e6a6":"code","55eb6ecc":"code","99c13191":"code","be73213e":"code","180525de":"code","32035b05":"code","0fd06d8c":"code","31b0eafe":"code","57ef1488":"code","58556f20":"code","2f763cda":"code","7000d327":"code","cc8b009e":"code","9e147e08":"code","275c24b2":"code","f6db5d2b":"code","6b7f503e":"code","c8bd63b6":"code","66e41546":"code","1548ca9c":"code","0e78436f":"code","ee3bd05f":"code","dbc7f7e7":"code","db7ff668":"code","74a33be4":"code","c28ab348":"code","bb370a36":"code","7249f7be":"code","42c4e80a":"code","689829bc":"code","0f9752f2":"code","bae418ef":"markdown","88683d88":"markdown","576076a2":"markdown","42efd7ac":"markdown","17dc6356":"markdown","f0b855a2":"markdown","3f59f42f":"markdown","4bf00fb4":"markdown","f7c1ca1d":"markdown","a31c089a":"markdown","d41ab69a":"markdown","400d44da":"markdown","46fa8524":"markdown","79491e49":"markdown","e3521c32":"markdown","b0357bb7":"markdown","f7a0d35c":"markdown","52fb3ca2":"markdown","e99db6db":"markdown","f6996de8":"markdown","14037e36":"markdown","6f81ba4c":"markdown","a3a0bb86":"markdown","bd71aa31":"markdown","860ca1b9":"markdown","490eb387":"markdown","47a4b69f":"markdown","ddc1a5a9":"markdown","f5c34037":"markdown","ebb961f6":"markdown","c942c666":"markdown","74c0f956":"markdown","e30e1596":"markdown","49d8ec85":"markdown","9cf1ec81":"markdown","b190e4b6":"markdown","ce635df3":"markdown","f80693a3":"markdown","a92ab53b":"markdown","d3faf0fc":"markdown","5ed902ad":"markdown","28f02f24":"markdown","ec4972ea":"markdown","1c019212":"markdown","9a02ca0c":"markdown","9949cdca":"markdown","8017648c":"markdown","545fb60a":"markdown"},"source":{"6bc05468":"import numpy as np\nimport pandas as pd\nimport os\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn \nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))","919eef48":"DATA_FOLDER = '\/kaggle\/input\/house-prices-advanced-regression-techniques'\ntrain = pd.read_csv(os.path.join(DATA_FOLDER,'train.csv'))\ntest = pd.read_csv(os.path.join(DATA_FOLDER,'test.csv'))","74acb47b":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","b17d3ecc":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","be644b43":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n","ae3c3d8b":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","762ff945":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","8a87af2b":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","dbd05b00":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data","61c796b4":"f, ax = plt.subplots(figsize=(10,8))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","5edac345":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","d382a32d":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","1ee6e242":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","a2cba263":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","a3826305":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","e1e1d7b8":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","b0a2dd83":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","04c395d7":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","4990ced8":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","4b6e95e0":"all_data = all_data.drop(['Utilities'], axis=1)","92a390a1":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","fb6b6ae0":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","9a9f54b4":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","9be2c231":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","969bf670":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","b0291879":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","bc80522b":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","23baa592":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","0342dc27":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","d9522c85":"all_data.columns","8c3908df":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","8175e6a6":"eps = 0.125\n# Adding total above ground \nall_data['TotalAboveGroundSF'] = all_data['1stFlrSF'] + all_data['2ndFlrSF']\n# All bathrooms\nall_data['AllBath'] = all_data['FullBath'] + all_data['HalfBath']\nall_data['BsmtAllBath'] = all_data['BsmtFullBath'] + all_data['BsmtHalfBath']\nall_data['FullAllBath'] = all_data['AllBath'] + all_data['BsmtAllBath']\n# Average Bedroom\nall_data['AverageBedroomSF'] = all_data['TotalAboveGroundSF'] \/ (all_data['BedroomAbvGr'] + eps)\n# Bedroom \/ Bathroom ratio\nall_data['BedroomBathroomRatio'] = all_data['BedroomAbvGr'] \/ (all_data['AllBath'] + eps)\n    ","55eb6ecc":"def build_agg_values(df, group, feature, criteria):\n    temp = df.groupby(group)[feature].agg([criteria]).rename({criteria:f'{feature}_{group}_{criteria}'},axis=1)\n    df = pd.merge(df,temp,on=group,how='left')\n    return df","99c13191":"all_data = build_agg_values(all_data, 'BedroomAbvGr', 'TotalAboveGroundSF', 'mean')\nall_data = build_agg_values(all_data, 'AllBath', 'TotalAboveGroundSF', 'mean')\nall_data = build_agg_values(all_data, 'Neighborhood', 'AllBath', 'mean')\nall_data = build_agg_values(all_data, 'Neighborhood', '1stFlrSF', 'mean')","be73213e":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","180525de":"f, ax = plt.subplots(figsize=(14, 10))\nplt.xticks(rotation='90')\nsns.barplot(x=skewness.index, y=skewness.Skew)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","32035b05":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)","0fd06d8c":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","31b0eafe":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","57ef1488":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","58556f20":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","2f763cda":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","7000d327":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","cc8b009e":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","9e147e08":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","275c24b2":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","f6db5d2b":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","6b7f503e":"lasso_score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(lasso_score.mean(), lasso_score.std()))","c8bd63b6":"enet_score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(enet_score.mean(), enet_score.std()))","66e41546":"kr_score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(kr_score.mean(), kr_score.std()))","1548ca9c":"gb_score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(gb_score.mean(), gb_score.std()))","0e78436f":"xgb_score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(xgb_score.mean(), xgb_score.std()))","ee3bd05f":"lgb_score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(lgb_score.mean(), lgb_score.std()))","dbc7f7e7":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","db7ff668":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","74a33be4":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","c28ab348":"averaged_models.fit(train.values, y_train)\naveraged_models_train_pred = averaged_models.predict(train.values)\naveraged_pred = np.expm1(averaged_models.predict(test.values))\nprint(rmsle(y_train, averaged_models_train_pred))","bb370a36":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = averaged_pred\nsub.to_csv('averaged_submission.csv',index=False)","7249f7be":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","42c4e80a":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","689829bc":"weighted_average = averaged_pred * 0.70 + xgb_pred * 0.15 + lgb_pred * 0.15","0f9752f2":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = weighted_average\nsub.to_csv('weighted_average_submission.csv',index=False)","bae418ef":"### Apply models","88683d88":"## Load the data","576076a2":"For masonry type and area we fill with `None` and 0 respectivelly for missing values.","42efd7ac":"### Get Dummy categorical variables","17dc6356":"Let's also plot the skeweness factor.","f0b855a2":"## Ensemble models\n\n### Average model\n\nWe average the prediction of several models.","3f59f42f":"There are two obvious outliers - with relativelly small price for a given living area. We just remove these two outliers.","4bf00fb4":"### Imputing missing variables\n\nWe fill with `None` all missing `PoolQC` since missing data means most probably that there is no Pool. Most of the properties will not have a Pool so the assumption is most probably correct.","f7c1ca1d":"Lasso is very sensitive to outliers. We apply a RobustScaler before.","a31c089a":"#### More features","d41ab69a":"Same for `Exterior1st`, `Exterior2nd`, `SaleType`.","400d44da":"## Base models","46fa8524":"## Outliers\n\nWe visualize the scatter plot of `SalePrice` (which is the target feature) vs. `GrLivArea` to spot outliers.","79491e49":"# Analysis preparation\n\n## Load the packages","e3521c32":"#### Label encoding few variables\n\nWe apply label encoding for few of the variables.","b0357bb7":"We fill with mode all missing data for Zoning.","f7a0d35c":"We also look to the correlation map to see which features are mostly correlated with `SalesPrice`.","52fb3ca2":"We set as string - to be treated as categorical values few of the features:\n\n* Building class;  \n* Overall condition;  \n* Year of the transaction;    \n* Month of the transaction;  \n","e99db6db":"# Models","f6996de8":"For garages, we fill with `None` missing data, because we suppose that missing data means no garrage present. We apply this raationale for both the categorial and numerical values.","14037e36":"# Data exploration","6f81ba4c":"We use mode imputation for missing `KitchenQual` data.","a3a0bb86":"We use mean imputation for missing `Electrical` data.","bd71aa31":"Same rationale for basement related features. We fill with 0 the values for numerical features (because missing values means probably missing basement and therefore we can agree all these values ore 0). For categorical features, we fill all missing with None.","860ca1b9":"## Glimpse and prepare the data\n\nWe look here to the data shape and we simply set aside the data IDs.","490eb387":"Submission with the weighted average of the simple average of 4 models predictions and prediction of XGB and LGB.","47a4b69f":"## Features engineering\n\nWe apply features engineering transform to all data.\n\nWe concatenate train and test.","ddc1a5a9":"For dimmension of the `LotFrontage`, we suppose that on the same neighborhood we will have similar values for `LotFrontage` therefore we fill with the median value for the neighborhood.","f5c34037":"We set back the train and test set.","ebb961f6":"We drop `Utilities` feature.","c942c666":"# Introduction\n\n\n## Acknowledgments\n\nThis Kernel (for training purposes only) is heavelly based on [Stacked Regression: Top 4% on Leaderboard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) Kernel by Serigne.\n\n## Data fields\nHere is a brief version of the data description information.\n\n* <font color='blue'>SalePrice<\/font>: the property's sale price in dollars. This is the target variable that you're trying to predict.\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: USD alue of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale  ","74c0f956":"We treated all missing values.","e30e1596":"ElasticNet is very sensitive to outliers as well. We apply a RobustScaler before.","49d8ec85":"We check now if there are any other missing data.","9cf1ec81":"## Log-transform of the target variable\n\nWe correct the distribution of the target variable by applying log1p transform to all values. After prediction, we apply back this transformation.","b190e4b6":"We also plot this distribution.","ce635df3":"We fill all missing data for `Functional` with `Typ`.","f80693a3":"### Skewed features\n\n\nFor the skewed features (numerical features with skewed distribution of the values) we want to apply a log1p transformation.  \n\n\nFirst, we detect these skewed features.","a92ab53b":"#### Add Total SqFt feature\n\nWe add an additional features, for total area of the house.","d3faf0fc":"##  More features engineering","5ed902ad":"### Missing data\n\nWe show the distribution of the missing data. We order features by percent of missing data and we show the 20 features with largest percent of missing data.","28f02f24":"Submission with the prediction of average models (Elastic Net, Gradient Boosting, Kernel Ridge and Lasso).","ec4972ea":"Same treatment for `MiscFeature` - missing data means that there are no miscelaneous features present.","1c019212":"### Validation function","9a02ca0c":"#### Apply BoxCox transform to skewed variables","9949cdca":"We fill with 0 the missing values for `MSSubClass` - the value of building class.","8017648c":"If we have missing data for `Alley`, we reckon that there is no alley so again we fill it with None. Same treatment for `Fence` and `FireplaceQu` (quality).","545fb60a":"## Target variable"}}