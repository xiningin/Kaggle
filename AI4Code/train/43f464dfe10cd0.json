{"cell_type":{"728e670c":"code","31afe133":"code","b8006a26":"code","64330972":"code","e7329083":"code","c4a3eab3":"code","8494ee56":"code","6b6200d1":"code","b114476f":"code","52e4202e":"code","bc59311e":"code","5732a209":"code","db2e2574":"code","2f407690":"code","301464b5":"code","1d0de3e8":"code","2894058e":"code","899a7296":"code","27592622":"code","b7da8fd4":"code","134d6fad":"code","5650c866":"code","5b96ef21":"code","b250c469":"code","f1b1e205":"markdown","efd74c2c":"markdown","3d3af2cb":"markdown","537df076":"markdown","53ecec4c":"markdown","724e7108":"markdown","00e9d7d3":"markdown","b2c21950":"markdown","fcad6728":"markdown","bd48f948":"markdown","5c65e101":"markdown","83a7ac8d":"markdown","a13ec182":"markdown","7b6f595d":"markdown","798e2711":"markdown","a283d5df":"markdown","43ed9d5a":"markdown","09c17764":"markdown","18835e26":"markdown","7833b4b6":"markdown","1dc7952e":"markdown","db9df832":"markdown"},"source":{"728e670c":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np, pandas as pd\nimport seaborn as sns\nimport os","31afe133":"ds_dir = '\/kaggle\/input\/kannada-mnist\/kannada_mnist_datataset_paper\/Kannada_MNIST_datataset_paper\/Kannada_MNIST_npz\/Kannada_MNIST'\n\nX_train = np.load(os.path.join(ds_dir,'X_kannada_MNIST_train.npz'))['arr_0']\nX_test = np.load(os.path.join(ds_dir,'X_kannada_MNIST_test.npz'))['arr_0']\ny_train = np.load(os.path.join(ds_dir,'y_kannada_MNIST_train.npz'))['arr_0']\ny_test = np.load(os.path.join(ds_dir,'y_kannada_MNIST_test.npz'))['arr_0']\n\nprint(X_train.shape, X_test.shape)\nprint(y_train.shape, y_test.shape)","b8006a26":"def plot_random_digit():\n    random_index = np.random.randint(0,X_train.shape[0])\n    plt.imshow(X_train[random_index], cmap='BuPu_r')\n    plt.title(y_train[random_index])\n    plt.axis(\"Off\")","64330972":"plt.figure(figsize=[2,2])\nplot_random_digit()","e7329083":"plt.figure(figsize=[10,6])\nfor i in range(50):\n    plt.subplot(5, 10, i+1)\n    plt.axis('Off')\n    if i < 10:\n        plt.title(y_train[i])\n    plt.imshow(X_train[i], cmap='BuPu_r')","c4a3eab3":"X_train_reshape = X_train.reshape(X_train.shape[0], 784)\nX_test_reshape = X_test.reshape(X_test.shape[0], 784)\nX_train_reshape.shape, X_test_reshape.shape","8494ee56":"from sklearn.linear_model import LogisticRegression\nlr1 = LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\")\n\n# Fitting on first 10000 records for faster training  \nlr1.fit(X_train_reshape[:10000], y_train[:10000])","6b6200d1":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\ny_train_pred = lr1.predict(X_train_reshape[:10000])","b114476f":"cm = confusion_matrix(y_train[:10000], y_train_pred[:10000])\n\nplt.figure(figsize=[7,6])\nsns.heatmap(cm, cmap=\"Reds\", annot=True, fmt='.0f')\nplt.show()","52e4202e":"print(\"Accuracy: \"+ str(accuracy_score(y_train[:10000], y_train_pred)))\nprint(classification_report(y_train[:10000], y_train_pred))","bc59311e":"y_series = pd.Series(y_train[:10000])\ny_miss = y_series[~(y_series==y_train_pred)]\nprint(y_miss.shape)","5732a209":"y_miss_samp = y_miss.sample(9)\n\n%matplotlib inline\nplt.figure(figsize=[6,6])\nfor i in range(len(y_miss_samp)):\n    plt.subplot(3,3,i+1)\n    plt.title(\"True:\"+str(y_miss_samp.values[i])+\", Pred:\"+str(y_train_pred[y_miss_samp.index][i]))\n    plt.imshow(X_train[y_miss_samp.index[i]],cmap='BuPu_r')\n    plt.axis('Off')","db2e2574":"y_test_pred = lr1.predict(X_test_reshape)\ncm_test = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=[7,6])\nsns.heatmap(cm_test, cmap=\"Reds\", annot=True, fmt='.0f')\nplt.show()","2f407690":"print(\"Accuracy: \"+ str(accuracy_score(y_test, y_test_pred)))\nprint(classification_report(y_test, y_test_pred))","301464b5":"lr1.coef_.shape","1d0de3e8":"plt.figure(figsize=[3,3])\ncoefs = lr1.coef_[0].reshape(28,28)\nplt.imshow(coefs,cmap=\"RdYlGn\",vmin=-np.max(coefs),vmax=np.max(coefs)) #setting mid point to 0\nplt.show()","2894058e":"plt.figure(figsize=[10,4])\nfor i in range(10):\n    plt.subplot(2,5,i+1)\n    plt.title(str(i))\n    coefs = lr1.coef_[i].reshape(28,28)\n    plt.imshow(coefs,cmap=\"RdYlGn\",vmin=-np.max(coefs),vmax=np.max(coefs)) #getting the midpoint to 0\n    plt.axis('Off')\nplt.suptitle('Pixel Heatmap: One-vs-Rest Formulation')","899a7296":"lr2 = LogisticRegression(random_state=42, multi_class=\"multinomial\", solver=\"sag\")\nlr2.fit(X_train_reshape[:10000], y_train[:10000])","27592622":"y_test_pred = lr2.predict(X_test_reshape)\n\ncm_test = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=[7,6])\nsns.heatmap(cm_test, cmap=\"Reds\", annot=True, fmt='.0f')\nplt.show()","b7da8fd4":"print(\"Accuracy: \" + str(accuracy_score(y_test, y_test_pred)))\nprint(classification_report(y_test, y_test_pred))","134d6fad":"plt.figure(figsize=[3,3])\ncoefs = lr2.coef_[0].reshape(28,28)\nplt.imshow(coefs,cmap=\"RdYlGn\",vmin=-np.max(coefs),vmax=np.max(coefs)) #setting mid point to 0\nplt.show()","5650c866":"plt.figure(figsize=[10,4])\nfor i in range(10):\n    plt.subplot(2,5,i+1), plt.title(str(i))\n    coefs = lr2.coef_[i].reshape(28,28)\n    plt.imshow(coefs,cmap=\"RdYlGn\",vmin=-np.max(coefs),vmax=np.max(coefs)) #getting midpoint to 0\nplt.suptitle('Pixel Heatmap: Multinomial Formulation')","5b96ef21":"plt.figure(figsize=(10, 4))\nfor i in range(10):\n    plt.subplot(2,5,i+1), plt.title(i)\n    plt.imshow(np.mean(X_train[y_train==i],axis=0),cmap='gray')\nplt.suptitle('Mean images for each digit')","b250c469":"plt.figure(figsize=(10, 4))\nfor i in range(10):\n    plt.subplot(2,5,i+1), plt.title(i)\n    plt.imshow(np.mean(X_train[y_train==i],axis=0),cmap='gray')\n    plt.axis('Off')\nplt.suptitle('Mean images for each digit')\n\nplt.figure(figsize=[10,4])\nfor i in range(10):\n    plt.subplot(2,5,i+1), plt.title(str(i))\n    coefs = lr1.coef_[i].reshape(28,28)\n    plt.imshow(coefs,cmap=\"RdYlGn\",vmin=-np.max(coefs),vmax=np.max(coefs)) #getting the midpoint to 0\n    plt.axis('Off')\nplt.suptitle('Pixel Heatmap: One-vs-Rest Formulation')\n\nplt.figure(figsize=[10,4])\nfor i in range(10):\n    plt.subplot(2,5,i+1), plt.title(str(i))\n    coefs = lr2.coef_[i].reshape(28,28)\n    plt.imshow(coefs,cmap=\"RdYlGn\",vmin=-np.max(coefs),vmax=np.max(coefs)) #getting midpoint to 0\n    plt.axis('Off')\nplt.suptitle('Pixel Heatmap: Multinomial Formulation')\n\n","f1b1e205":"Have a good look at these heatmaps. This will reveal what the model has learnt.\nBe mindful that we have 'One vs. Rest' formulation, especially when comparing with heatmaps of other digits.\n\n## Now, let's build a model using the multinomial scheme.\n- We need to specify the `multi_class` parameter as \"multinomial\"\n- The 'liblinear' solver doesn't support this, so we choose the \"sag\" solver.","efd74c2c":"#### Assessing performance on the test set\nPlotting the confusion matrix","3d3af2cb":"***\n## Building and understanding the Logistic regression model\n\nLet's build a Logistic regression model for our multiclass classification problem.   \n\n**Note that we'll not be focusing on getting the best possible performance. Instead, we'll focus on how to understand what the model has learnt. Which is more interesting to assess when the model isn't working extremely well. :) **\n\n\nA logistic regression model will be easy and interesting to analyse the coefficients to understand what the model has learnt.  \nThe formulation of a multi-class classification can be done in a couple of ways in SciKit-learn. They are - \n\n- One vs Rest\n- Multinomial\n\n**1. One vs Rest: **  \n\nAlso known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. One advantage of this approach is its interpretability.\n\nSince each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.\n\nFor our case, it would mean building 10 different classifiers.\n\nRead more about it here:  \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.multiclass.OneVsRestClassifier.html\n\n\n**2. Multinomial: **  \n\nIn this strategy, we model the logarithm of the probability of seeing a given output using the linear predictor.  \nFor `multinomial` the loss minimised is the multinomial loss fit across the entire probability distribution. \nThe softmax function is used to find the predicted probability of each class.\n\nRead more about this here:  \nhttps:\/\/en.wikipedia.org\/wiki\/Multinomial_logistic_regression#As_a_log-linear_model  \n\n  \n  \n**Note**: This distinction is important, and needs you to intrepret the coefficients differently for the models.\n\n### First, let's built our model using the One vs. Rest scheme","537df076":"How different\/similar is this to the heatmap from the OVR model?  \nLet's make the heatmaps for all pixels.\n\n#### Making such pixel heatmaps for all the digits","53ecec4c":"#### That's VERY high training accuracy! Overfitting?\n\nAlso, looks like the model is **NOT** very confused between 3 and 7, 6 and 9, at least not on the train set.\n\n### Error Analysis: Checking out the mis-classified cases\nWe'll covert to a Pandas series for ease of indexing, isolate the misclassification cases, plot some examples.","724e7108":"As someone who is not good at reading Kannada script, to me the symbols seem somewhat similar for - \n - 3 and 7 \n - 6 and 9  \n\nAt the onset, I would expect that the predictors could be somewhat confused between these pairs. Although this isn't necessarily true - maybe our model can identify the digits better than I can. \n\n## Reshaping the datasets for predictive model building\n\nThe individual examples are 28 X 28. For most predictive modeling methods in scikit learn, we need to get flatten the examples to a 1D array.  \nWe'll use the reshape method of numpy arrays.\n","00e9d7d3":"### Understanding the contribution of each pixel\n\nWe have 784 coefficients for each label - coefficients for each pixel. \n\nNow, a positive coefficient would mean what makes this label what it is! But, if 3 labels have similar presence in particular pixel, the coefficients for all 3 may have similar values.\n\n#### Extracting the pixel coefficients and plotting on a heatmap for the label 0","b2c21950":"## Exercise - \nYou've seen the heatmaps for the OVR method, as well as the multinomial method. And you also have the average image for each label.  \n\n- Compare and contrast the heatmaps with the mean images.\n- What do think is going on? Can you try to understand what the models have learnt for each digit? \n- Why are the models not performing so well on certain digits? Can the heatmap help understand?\n\n### Possible next steps for those interesting in trying out more things -\n\nI suggest you try the following - \n1. Use Logistic regression with regularization (ridge, lasso, elasticnet) and hyper-parameter optimization using cross validation to reduce overfitting.\n2. Use SVD\/PCA to denoise and reconstruct the original data; follow it up with a tuned Logistic regression model.\n\nWell, that's all for our little demo here! I'll soon share more demos with different modeling techniques, ways to interpret them and more experiments with the same dataset with supervised and unsupervised techniques.  \n\n\n\n### Found this interesting? Stay tuned for more such demos.\n\n\n#### Do share your remarks\/comments\/suggestions!","fcad6728":"### How does these heatmaps compare to the mean images for each label?","bd48f948":"We have 60K train cases, 10K test cases.  \nEach example is a 28 x 28 matrix representing the greyscale values. \n\n***\n## Visualizing the digits data\n#### Function to plot one random digit along with its label","5c65e101":"Can you see why the model was confused?  \nLet's see how the model fares on the test set.\n\n#### Confusion matrix on the test set\n\nMaking predictions on the test data, and plotting the confusion matrix.","83a7ac8d":"## Plotting them all together - have a good look.","a13ec182":"#### Looking at the confusion matrix and the classification report -\n\nRecall is least for 3, 7 - model is confused between them significantly. Similarly, there is confusion between 4 and 5. Also, many 0s are mistaken for 1 and 3.\n\nOkay! So it looks like the performance has fallen sharply on the test set. There's a very good chance we're overfitting on the train set.  \n\nWe acknowledge that the model could be improved.  \n\nBut, let's not worry about that for now. **Let's focus on the way to understand what the model learnt. **\n***\n## Model interpretation\n### Understanding the contribution of each pixel\n\nThe coefficients we learnt right now for each pixel, are based on the One vs Rest scheme.  \n\nLet's go ahead and analyze the coefficients for our OVR model.\n","7b6f595d":"## Loading the train and test datasets  \nData files are numpy arrays stored as compresses Numpy files, we'll load them using numpy's `load` method","798e2711":"#### 11 cases were misclassified\n- Studying some cases\n- Picking 9 random cases - we'll plot the digits, along with the true and predicted labels","a283d5df":"Execute the cell multiple times to see random examples.  \n#### Looking at 50 samples at one go","43ed9d5a":"We have 784 coefficients for each label - coefficients for each pixel.  \n\nA positive coefficient means a high value on that pixel increases the chances of this label, **compared to ALL other classes**. \nThe coefficients therefore tell us how this pixel differentiates this label from all the other labels together.\n\n#### Extracting the pixel coefficients and plotting on a heatmap for the label 0","09c17764":"# Kannada MNIST Data - Modeling, Interpreting, Visualization, Model understanding  \n\n\nThe Kannada MNIST dataset is a great recent work ([details here](https:\/\/arxiv.org\/abs\/1908.01242)), and I'm delighted that it is available to the public on Kaggle as well. I'm sure pretty soon the community here would be posting state of the art accuracy numbers on this dataset. Which is why, I'm doing something different.   \n\nInstead, we will try to visualize, try to see what the model sees, assess things pixel by pixel. Our goal would be interpretability. I'll start with the 'simplest', easiest to interpret algorithm in this notebook. Hopefully, I'll post results with other modeling techniques in later kernels\/notebooks.\n\n**To reiterate and clarify: I will not be focusing on getting best possible performance. Rather, I'll focus on visualizing the output, making sense of the model, and understanding where it failed and why. Which is more interesting to assess when the model isn't working extremely well. :) **","18835e26":"Plotting the confusion matrix","7833b4b6":"#### Importing our usual, reliable friends","1dc7952e":"I've used a divergent colour scheme to differentiate between the positive and negative signs. \n\nIn the image above, pixels with green colour are positive value pixels. The image tells us that values in certain pixels help classify the digit as 0. As expected, the red colour in the center indicates that presence of values in that range means lower chances of the digit being a zero. Yellow is close to 0 - meaning the pixel doesn't help differentiate in any way.\n\n#### Making such pixel heatmaps for all the digits","db9df832":"#### Assessing performance on the train set\n\nThe predictions of the model for the training data"}}