{"cell_type":{"cfb920a0":"code","56f2cb87":"code","88c34fd9":"code","94b67aa1":"code","37690f7a":"code","d50cde3f":"code","6b93d6d9":"code","1450e9a6":"code","11b6614e":"code","5dc8a236":"code","c9aba7aa":"code","0d9eb6d6":"code","492ef32a":"code","c3f80c55":"code","19337928":"code","f5b53bb4":"code","cbfbb024":"code","2f3953e5":"code","ba9d634d":"code","0fe900bb":"code","48e84f71":"code","1762bdd9":"code","a0cd25b8":"code","5aa9fc3f":"code","01a4eb71":"code","536626e1":"code","e50cfd88":"markdown","4636588f":"markdown","c73213f9":"markdown","4c367f7c":"markdown","a5ddccda":"markdown","0b26f3a1":"markdown","556d301c":"markdown","3d3490f5":"markdown","ee6a0aea":"markdown","75dbf6bb":"markdown","367ece7d":"markdown","3eb8c170":"markdown","48fa09f6":"markdown","827ded9b":"markdown","4b36b3a1":"markdown"},"source":{"cfb920a0":"from kaggle_datasets import KaggleDatasets\nimport math, re, os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom functools import partial\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nimport cv2\nprint(\"Tensorflow version \" + tf.__version__)","56f2cb87":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","88c34fd9":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path() # Google Cloud Storage (CGS) Bucket\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [512, 512]\nCLASSES = ['0', '1', '2', '3', '4']\nEPOCHS = 15","94b67aa1":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # konverterer image 255 RGB values til floats i [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image","37690f7a":"def read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), #Her betyder tf.string at det er en byte-string\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum","d50cde3f":"#train_df = tf.io.gfile.glob(GCS_PATH + 'train.csv')\n#display(train_df.head())","6b93d6d9":"def read_dataset(tfrecords, labeled = True, ordered = False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n        # Ved at slukke for sortering, \u00f8ger vi hastigheden.\n        \n    dataset = tf.data.TFRecordDataset(\n        tfrecords, num_parallel_reads=AUTOTUNE\n    ) # Her blandes der automatisk adskillige filer til at blive l\u00e6st p\u00e5 samme tid.\n    \n    dataset = dataset.with_options(ignore_order)\n    # Bruger dataen s\u00e5 snart den kommer ind, istedet for at sortere den.\n    \n    dataset = dataset.map(\n        partial(read_tfrecord, labeled=labeled),\n        num_parallel_calls=AUTOTUNE)\n    return dataset\n        ","1450e9a6":"train_tfrecords, valid_tfrecords = train_test_split(\n    tf.io.gfile.glob(GCS_PATH + '\/train_tfrecords\/ld_train*.tfrec'),\n    test_size=0.30, random_state = 42\n)\n\n\ntrain_images = GCS_PATH + '\/train_images\/'\n\ntest_tfrecords = tf.io.gfile.glob(GCS_PATH + '\/test_tfrecords\/ld_test*.tfrec')","11b6614e":"#ImageDataGenerator(\n                  #  rotation_range = 30,\n                  #  width_shift_range = 0.2,\n                  #  height_shift_range = 0.2,\n                  #  shear_range = 0.2,\n                  #  zoom_range = 0.2,\n                  #  brightness_range = [0.5,1.5],\n                   # horizontal_flip = True,\n                   # vertical_flip = True,\n                   # fill_mode = 'nearest'\n#)","5dc8a236":"#def data_augmentation(image, label):\n   # image = tf.image.random_brightness(image, 0.2)\n    #image = tf.image.random_contrast(image, 0.2, 0.4)\n    #image = tf.image.random_flip_left_right(image)\n   # image = tf.image.random_flip_up_down(image)\n   # return image, label","c9aba7aa":"def data_augmentation(image, label):\n   \n    return image, label","0d9eb6d6":"def get_train_dataset():\n    dataset = read_dataset(train_tfrecords, labeled = True)  \n    dataset = dataset.map(data_augmentation, num_parallel_calls = AUTOTUNE)  \n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","492ef32a":"def get_valid_dataset():\n    dataset = read_dataset(valid_tfrecords, labeled = True, ordered = True)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","c3f80c55":"def get_test_dataset(ordered = False):\n    dataset = read_dataset(test_tfrecords, labeled = False, ordered = True)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","19337928":"def count_data(tfrecords):\n    num = [int(re.compile(r\"-([0-9]*)\\.\").search(tfrecord).group(1))\n           for tfrecord in tfrecords]\n    return np.sum(num)","f5b53bb4":"num_train_images = count_data(train_tfrecords)\nnum_valid_images = count_data(valid_tfrecords)\nnum_test_images = count_data(test_tfrecords)\n\nprint(\"Datas\u00e6t: {} billeder til tr\u00e6ning, {} billeder til validering, {} (unlabeled) test billeder\".format(\nnum_train_images, num_valid_images, num_test_images))","cbfbb024":"#plt.figure(figsize = (20,20))\n#for i in range(20):\n   # plt.subplot(4,5,i+1)\n   # img = cv2.imread(train_images + images[i])\n    #img = cv2.imread(tf.io.gfile.glob(GCS_PATH + '\/train_images\/10*.jpg'))\n   # img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    #plt.imshow(img)\n   # plt.title(data[str(labels[i])])","2f3953e5":"with strategy.scope():\n        \n    adjust_img_layer = tf.keras.layers.Lambda(\n        tf.keras.applications.resnet50.preprocess_input,\n        input_shape=[IMAGE_SIZE, IMAGE_SIZE, 3])\n        \n    base_model = tf.keras.applications.ResNet50(\n        weights = \"imagenet\",\n        include_top = False)\n    base_model.trainable = False\n        \n    model = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(renorm = True),\n        adjust_img_layer,\n        base_model,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(8, activation = \"relu\"),\n        #tf.keras.layers.BatchNormalization(renorm=True),\n        tf.keras.layers.Dense(len(CLASSES), activation = 'softmax')\n    ])\n    model.compile(\n        optimizer = tf.keras.optimizers.Adam(\n            learning_rate = 0.001),\n        loss = \"sparse_categorical_crossentropy\",\n        metrics = [\"sparse_categorical_accuracy\"])","ba9d634d":"train_dataset = get_train_dataset()\nvalid_dataset = get_valid_dataset()","0fe900bb":"# Stopper tr\u00e6ningen n\u00e5r validation loss metric er stoppet med at falde i 5 epochs.\nearly_stopping = EarlyStopping(monitor = 'val_loss',\n                               patience = 5,\n                               mode = 'min',\n                               restore_best_weights = True)\n\n# Gemmer modellen med det maksimale validerings pr\u00e6cision, virker ikke?\ncheckpoint = ModelCheckpoint('best_model.hdf5', \n                             monitor = 'val_sparse_categorical_accuracy',\n                             verbose = 1,\n                             mode = 'max', \n                             save_best_only = True)\n# Reducerer l\u00e6rings raten\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n                              factor = 0.2,\n                              patience = 2,\n                              mode = 'min',\n                              verbose = 1)","48e84f71":"epoch_steps = num_train_images \/\/ BATCH_SIZE\nvalid_steps = num_valid_images \/\/ BATCH_SIZE","1762bdd9":"history = model.fit(train_dataset,\n                    validation_data = valid_dataset,\n                    epochs = EPOCHS,\n                    steps_per_epoch = epoch_steps, \n                    validation_steps = valid_steps, \n                    callbacks = [early_stopping, reduce_lr]\n                   )","a0cd25b8":"model.summary()","5aa9fc3f":"print(history.history.keys())","01a4eb71":"acc = history.history['sparse_categorical_accuracy']\nval_acc = history.history['val_sparse_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'c-', label='Training sparse categorical accuracy')\nplt.plot(epochs, val_acc, 'y-', label='Validation sparse categorical accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'c-', label='Training Loss')\nplt.plot(epochs, val_loss, 'y-', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","536626e1":"plot_model(model, show_shapes = True)","e50cfd88":"Nu s\u00e6tter vi vores \"features\" som \"image\", og vores label som \"target\".\n\nEndnu mere vigtigt at forklare, s\u00e5 er datas\u00e6ttet allerede blevet formatteret til TFRecords for os (fra kaggles side).\nTFRecords er en serialisering af datas\u00e6ttet, s\u00e5 datas\u00e6ttet omdannes til bytes. Derfor skal vores data omdannes til byte-strings f\u00f8r de kan komme i en TFRecord.\n\nEfterf\u00f8lgende kan man konvertere bytestrings tilbage til tensors","4636588f":"# Resultats historie:","c73213f9":"## Data Augmentation\n\nHvis man selv udf\u00f8rte data augmentation p\u00e5 billederne ned til mindste detalje, ville man bruge keras ImageDataGenerator.\n\nJeg bruger dog en simpel metode f\u00f8rst.","4c367f7c":"# Setup","a5ddccda":"Nu s\u00e6tter vi vores konfigurerings variabler og bestemmer hvordan vi skal h\u00e5ndtere vores billeder i datas\u00e6ttet:\n\nTPU'er l\u00e6ser data fra Google Cloud Storage (GCS) buckets.","0b26f3a1":"## Training & Validation datas\u00e6t","556d301c":"## Lad os f\u00e5 antal p\u00e5 dataen:","3d3490f5":"Nu g\u00f8r jeg brug af SciKit Learns \"train_test_split\" funktion til at adskille datas\u00e6ttet i et tr\u00e6nings s\u00e6t, og et s\u00e6t til validering.\n\nHvis jeg havde tid, ville jeg have gjort brug af K-fold Cross Validation i stedet, men det er noget mere kompliceret.","ee6a0aea":"* ResNet50 - image_size = (512, 512), batch_size = 128 - 0.6901\n\n\n### Med Callbacks\/flere augmentationer:\n\n\n* ResNet50 - image_size = (512, 512), batch_size = 128 - 0.7169\n* EfficientNetB3 - image_size = (512, 512), batch_size = 128 - 0.61584\n* ResNet50 - Ekstra augmentation (\u00e6ndring af contrast + brightness) - image_size = (512, 512), batch_size = 128 - 0.6486 - endnu v\u00e6rre.\n\n\n### Skiftet train\/test split til 15% + Accuracy metric i stedet for sparse categorical accuracy: \n\n* EfficientNetB3 - image_size = (512, 512), batch_size = 128 - 0.61968","75dbf6bb":"Men vi er s\u00e5 heldige at den f\u00f8lgende \"dataset.prefetch(AUTO)\" funktion g\u00f8r dette for os, \"gratis\" ved brug af TPU.","367ece7d":"# EDA?","3eb8c170":"Nu ser vi om vi kan f\u00e5 forbindelse\/ se vores TPU'er:","48fa09f6":"# Valg af model","827ded9b":"# Tr\u00e6ning af model","4b36b3a1":"Nu fremviser vi l\u00e6rings kurven + loss funktion for at evaluere vores model"}}