{"cell_type":{"97204d93":"code","67c94d3f":"code","2de1a2e0":"code","76395dcb":"code","ba3dec7c":"code","0f5da147":"code","22ad7603":"code","c9df1ce9":"code","d0704d1f":"code","a1ed782a":"code","3f8a62a2":"code","370c42d2":"code","a396aa85":"code","25f9c275":"code","0fa22720":"code","52ba1b34":"code","c1d2a9b7":"code","861ceba0":"code","c13d1dfe":"code","2088f881":"code","9de5a72a":"code","baa4b3d3":"code","a4c3387e":"code","8c774758":"code","3a21d7a2":"code","9c9f8f46":"code","e85636b4":"code","4c15b1c2":"code","e91c06a2":"code","ca6e27f7":"code","77774243":"code","35c1fede":"code","90d21810":"code","f13d1f04":"code","bdf68402":"code","836e9000":"code","129e3a73":"code","c07a764c":"code","9118f8c2":"code","af78b643":"code","128de0a8":"code","d7c24b7c":"code","bd475fb5":"code","73dc81ee":"code","eb746a83":"code","7092090b":"code","977e3c20":"code","a1cec5e9":"code","0bedd59f":"code","cb07b27c":"code","0e06699e":"code","7e93a1e2":"code","535c3dfe":"code","fef422b1":"code","8e4dddf4":"code","301f700a":"code","859a6e33":"code","e4af9dc4":"code","89365e11":"code","3e6a69b4":"markdown","ed17241f":"markdown","a324c021":"markdown","2f158800":"markdown","57d7a545":"markdown","d6a99459":"markdown","34e7b779":"markdown","ef58ceb8":"markdown","f007da53":"markdown","5c26caf5":"markdown","b16583af":"markdown","e6f2eb69":"markdown","1c2ba86f":"markdown","fea31312":"markdown","bc41689f":"markdown","d68168d5":"markdown","7ebd8c69":"markdown","a85e11db":"markdown","03e412a0":"markdown","4a13d160":"markdown","87ad0fe8":"markdown","23202254":"markdown","2f1152b1":"markdown","df7b0a39":"markdown","303e6361":"markdown"},"source":{"97204d93":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","67c94d3f":"import re\nimport string\n\n# Reset the output dimensions\nimport matplotlib.pyplot as plt\n\nfrom sklearn import decomposition\nfrom sklearn.svm import LinearSVC, NuSVC, SVC\nfrom sklearn.metrics import f1_score, accuracy_score, hamming_loss\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n\nfrom skmultilearn.problem_transform import BinaryRelevance, LabelPowerset\n\nfrom scipy import linalg\n\nfrom collections import Counter\n\nimport pickle\n\nimport nltk\nnltk.download('wordnet')\nfrom nltk import stem\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n\nfrom gensim import matutils, models\nfrom gensim.models import Word2Vec\n\nimport scipy.sparse\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport seaborn as sns\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\n\nplt.rcParams['figure.figsize'] = [24, 12]\nplt.style.use('seaborn-darkgrid')","2de1a2e0":"# Detect hardware, return appropriate distribution strategy\n\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS available: \", strategy.num_replicas_in_sync)","76395dcb":"train = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis\/sample_submission.csv')\ntrain.columns = train.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\ntest.columns = test.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n\nprint('Train Data shape: ', train.shape, 'Test Data shape: ', test.shape)\n\ntrain.head(10)","ba3dec7c":"# word_count\ntrain['word_count'] = train['tweet'].apply(lambda x: len(str(x).split()))\ntest['word_count'] = test['tweet'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ntrain['unique_word_count'] = train['tweet'].apply(lambda x: len(set(str(x).split())))\ntest['unique_word_count'] = test['tweet'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ntrain['stop_word_count'] = train['tweet'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest['stop_word_count'] = test['tweet'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# url_count\ntrain['url_count'] = train['tweet'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ntest['url_count'] = test['tweet'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\ntrain['mean_word_length'] = train['tweet'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest['mean_word_length'] = test['tweet'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ntrain['char_count'] = train['tweet'].apply(lambda x: len(str(x)))\ntest['char_count'] = test['tweet'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ntrain['punctuation_count'] = train['tweet'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest['punctuation_count'] = test['tweet'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ntrain['hashtag_count'] = train['tweet'].apply(lambda x: len([c for c in str(x) if c == '#']))\ntest['hashtag_count'] = test['tweet'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ntrain['mention_count'] = train['tweet'].apply(lambda x: len([c for c in str(x) if c == '@']))\ntest['mention_count'] = test['tweet'].apply(lambda x: len([c for c in str(x) if c == '@']))","0f5da147":"METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n        'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\n\nTWEETS = train['label'] == 1\n\nfig, axes = plt.subplots(ncols = 2, nrows = len(METAFEATURES), figsize = (20, 50), dpi = 100)\n\nfor i, feature in enumerate(METAFEATURES):\n    sns.distplot(train.loc[~TWEETS][feature], label = 'Positive', ax = axes[i][0], color = 'green')\n    sns.distplot(train.loc[TWEETS][feature], label = 'Negative', ax = axes[i][0], color = 'red')\n\n    sns.distplot(train[feature], label = 'Training', ax = axes[i][1])\n    sns.distplot(test[feature], label = 'Test', ax = axes[i][1])\n  \n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].tick_params(axis = 'x', labelsize = 12)\n        axes[i][j].tick_params(axis = 'y', labelsize = 12)\n        axes[i][j].legend()\n  \n    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize = 13)\n    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize = 13)\n\nplt.show()","22ad7603":"fig, axes = plt.subplots(ncols = 2, figsize = (17, 4), dpi = 100)\nplt.tight_layout()\n\ntrain.groupby('label').count()['id'].plot(kind = 'pie', ax = axes[0], labels = ['Negative (92.9%)', 'Positive (7.1%)'])\nsns.countplot(x = train['label'], hue = train['label'], ax = axes[1])\n\naxes[0].set_ylabel('')\naxes[1].set_ylabel('')\naxes[1].set_xticklabels(['Negative (29720)', 'Positive (2242)'])\naxes[0].tick_params(axis = 'x', labelsize = 15)\naxes[0].tick_params(axis = 'y', labelsize = 15)\naxes[1].tick_params(axis = 'x', labelsize = 15)\naxes[1].tick_params(axis = 'y', labelsize = 15)\n\naxes[0].set_title('Label Distribution in Training Set', fontsize = 13)\naxes[1].set_title('Label Count in Training Set', fontsize = 13)\n\nplt.show()","c9df1ce9":"def remove_stopwords(string):\n    word_list = [word.lower() for word in string.split()]\n    stopwords_list = list(stopwords.words(\"english\"))\n    for word in word_list:\n        if word in stopwords_list:\n            word_list.remove(word)\n    return ' '.join(word_list)","d0704d1f":"train['tweet_length'] = train['tweet'].apply(len)\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub('\\\\n',' ',str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r'\\W',' ',str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r'https\\s+|www.\\s+',r'', str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r'http\\s+|www.\\s+',r'', str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+',' ',str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r'\\^[a-zA-Z]\\s+',' ',str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r'\\s+',' ',str(x)))\ntrain['tweet'] = train['tweet'].str.lower()\n\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\\u2019\", \"\\'\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"n\\\u2019t\", \" not\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'s\", \" is\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\\u2019d\", \" would\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\d\", \" would\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"[0-9]\", \"digit\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r'[?|!|\\'|\"|#]',r'', str(x)))\ntrain['tweet'] = train['tweet'].map(lambda x: re.sub(r'[.|,|)|(|\\|\/]',r' ', str(x)))\ntrain['tweet'] = train['tweet'].apply(lambda x: remove_stopwords(x))\n\ntest['tweet'] = test['tweet'].map(lambda x: re.sub('\\\\n',' ',str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r'\\W',' ',str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+',' ',str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r'\\^[a-zA-Z]\\s+',' ',str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r'\\s+',' ',str(x)))\ntest['tweet'] = test['tweet'].str.lower()\n\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\\u2019\", \"\\'\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"n\\\u2019t\", \" not\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'s\", \" is\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\\u2019d\", \" would\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\d\", \" would\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"[0-9]\", \"digit\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r'[?|!|\\'|\"|#]',r'', str(x)))\ntest['tweet'] = test['tweet'].map(lambda x: re.sub(r'[.|,|)|(|\\|\/]',r' ', str(x)))\ntest['tweet'] = test['tweet'].apply(lambda x: remove_stopwords(x))","a1ed782a":"def clean_text(text):\n    '''make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    #text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('user ', '', text)\n    text = re.sub('amp ', '', text)\n    text = re.sub('like ', '', text)\n    text = re.sub('new ', '', text)\n    text = re.sub('people ', '', text)\n    text = re.sub('bihday', 'birthday', text)\n    text = re.sub('allahsoil', 'allah soil', text)\n    return text","3f8a62a2":"train['tweet'] = train['tweet'].apply(lambda x: clean_text(x))\ntest['tweet'] = test['tweet'].apply(lambda x: clean_text(x))","370c42d2":"train_data = train.copy()\n\nlabel = {0: 'A', 1: 'B'}\ntrain['label'] = train['label'].map(label)\ntrain = train.drop('id', axis = 1)\n\ntrain = pd.get_dummies(train, columns = ['label'])\ntrain.head()","a396aa85":"categories = ['label_A', 'label_B']\n\ntrain_dict = {}\n\nfor column in categories:\n    a = train.loc[train[column] == 1, 'tweet'].tolist()\n    train_dict[column] = ' '.join(a)","25f9c275":"# We can either keep it in dictionary format or put it into a pandas dataframe\n\ndata_df = pd.DataFrame(train_dict.items())\ndata_df.columns = ['index', 'tweet']\ndata_df = data_df.set_index('index')\ndata_df = data_df.sort_index()\ndata_df.head()","0fa22720":"data_df = pd.DataFrame(data_df['tweet'].apply(lambda x: clean_text(x)))\ndata_clean = data_df.copy()\ndata_df.head()","52ba1b34":"cv = CountVectorizer(stop_words = 'english')\ndata_cv = cv.fit_transform(data_df['tweet'])\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns = cv.get_feature_names())\ndata_dtm.index = data_df.index\ndata_dtm = data_dtm.transpose()\ndata_dtm.head()","c1d2a9b7":"# Find the top 30 words on each category\n\ntop_dict = {}\nfor c in data_dtm.columns:\n    top = data_dtm[c].sort_values(ascending = False).head(30)\n    top_dict[c]= list(zip(top.index, top.values))\n\ntop_dict","861ceba0":"# Print the top 15 words said by each category\n\nfor category, top_words in top_dict.items():\n    print(category + \":\")\n    print(', '.join([word for word, count in top_words[0:14]]))\n    print('-----------------------------------------------------------------------------------------------------------------------')","c13d1dfe":"# Let's first pull out the top words for each category\n\nwords = []\nfor category in data_dtm.columns:\n    top = [word for (word, count) in top_dict[category]]\n    for t in top:\n        words.append(t)\n        \nwords\n\n# Let's aggregate this list and identify the most common words along with how many routines they occur in\nCounter(words).most_common()","2088f881":"data_dtm","9de5a72a":"# Find the bottom 200 words in each category\n\nbottom_dict = {}\nfor c in data_dtm.columns:\n    bottom = data_dtm[c].sort_values(ascending = True).head(200)\n    bottom_dict[c]= list(zip(bottom.index, bottom.values))\n\n# Let's first pull out the bottom words for each category\n\nbottom_words = []\nfor category in data_dtm.columns:\n    bottom = [word for (word, count) in bottom_dict[category]]\n    for b in bottom:\n        bottom_words.append(b)\n\nCounter(bottom_words).most_common()","baa4b3d3":"# Let's make some word clouds!\n\nstop_words = text.ENGLISH_STOP_WORDS\n\nwc = WordCloud(stopwords = stop_words, background_color = \"white\", colormap = \"Dark2\", max_font_size = 150, random_state = 42)","a4c3387e":"# Create subplots for each category\n\nfor index, description in enumerate(data_dtm.columns):\n    wc.generate(data_clean.tweet[description])\n    \n    plt.subplot(1, 2, index + 1)\n    plt.imshow(wc, interpolation = \"bilinear\")\n    plt.axis(\"off\")\n    plt.title(categories[index])\n    \nplt.show()","8c774758":"# Find the number of unique words that each category has\n\n# Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once\nunique_list = []\nfor category in data_dtm.columns:\n    uniques = data_dtm[category].to_numpy().nonzero()[0].size\n    unique_list.append(uniques)\n\n# Create a new dataframe that contains this unique word count\ndata_words = pd.DataFrame(list(zip(categories, unique_list)), columns = ['category', 'unique_words'])\ndata_unique_sort = data_words.sort_values(by = 'unique_words')\ndata_unique_sort","3a21d7a2":"y_pos = np.arange(len(data_words))\n\nplt.figure(figsize = (6, 8))\nplt.barh(y_pos, data_unique_sort.unique_words, align = 'center')\nplt.yticks(y_pos, data_unique_sort.category)\nplt.title('Number of Unique Words', fontsize = 20)\nplt.show()","9c9f8f46":"Train, Test = train_test_split(train_data.drop('id', axis = 1), test_size = 0.25, random_state = 22) # Splits Dataset into Training and Testing set\nprint(\"Train Data size:\", len(Train))\nprint(\"Test Data size\", len(Test))","e85636b4":"Train.head()","4c15b1c2":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(Train.tweet)\n\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary Size :\", vocab_size)","e91c06a2":"MAX_LENGTH = 30\n\nfrom keras.preprocessing.sequence import pad_sequences\n\nX_train = pad_sequences(tokenizer.texts_to_sequences(Train.tweet), maxlen = MAX_LENGTH)\nX_test = pad_sequences(tokenizer.texts_to_sequences(Test.tweet),\n                       maxlen = MAX_LENGTH)\n\nprint(\"Training X Shape:\",X_train.shape)\nprint(\"Testing X Shape:\",X_test.shape)","ca6e27f7":"# Get all the train labels\n\nlabels = Train.tweet.unique().tolist()","77774243":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\nencoder.fit(Train.label.to_list())\n\ny_train = encoder.transform(Train.label.to_list())\ny_test = encoder.transform(Test.label.to_list())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","35c1fede":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove.6B.zip","90d21810":"GLOVE_EMB = '\/kaggle\/working\/glove.6B.300d.txt'\nBATCH_SIZE = 1024\nEPOCHS = 15\nMODEL_PATH = '\/kaggle\/working\/best_model.hdf5'","f13d1f04":"embeddings_index = {}\n\nf = open(GLOVE_EMB)\nfor line in f:\n    values = line.split()\n    word = value = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","bdf68402":"EMBEDDING_DIM = 300\n\nembedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","836e9000":"import tensorflow as tf\n\nembedding_layer = tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, weights = [embedding_matrix], input_length = MAX_LENGTH, trainable = False)","129e3a73":"from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint","c07a764c":"sequence_input = Input(shape = (MAX_LENGTH,), dtype = 'int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation = 'relu')(x)\nx = Bidirectional(LSTM(64, dropout = 0.2, recurrent_dropout = 0.2))(x)\nx = Dense(512, activation = 'relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation = 'relu')(x)\noutputs = Dense(1, activation = 'sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, outputs)","9118f8c2":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nLR = 1e-3\n\nmodel.compile(optimizer = Adam(learning_rate = LR), loss = 'binary_crossentropy', metrics = ['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor = 0.1, min_lr = 0.01, monitor = 'val_loss', verbose = 1)","af78b643":"print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")","128de0a8":"history = model.fit(X_train, y_train, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = (X_test, y_test), callbacks = [ReduceLROnPlateau])","d7c24b7c":"s, (at, al) = plt.subplots(2,1)\nat.plot(history.history['accuracy'], c = 'b')\nat.plot(history.history['val_accuracy'], c ='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['LSTM_train', 'LSTM_val'], loc ='upper left')\n\nal.plot(history.history['loss'], c ='m')\nal.plot(history.history['val_loss'], c ='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')","bd475fb5":"def decode_sentiment(score):\n\n    return 1 if score > 0.5 else 0\n\nscores = model.predict(X_test, verbose = 1, batch_size = 10000)\ny_pred_1d = [decode_sentiment(score) for score in scores]","73dc81ee":"import itertools\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap = cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize = 13)\n    plt.yticks(tick_marks, classes, fontsize = 13)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment = \"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize = 17)\n    plt.xlabel('Predicted label', fontsize = 17)","eb746a83":"cnf_matrix = confusion_matrix(Test.label.to_list(), y_pred_1d)\nplt.figure(figsize = (6,6))\nplot_confusion_matrix(cnf_matrix, classes = Test.label.unique(), title = \"Confusion matrix\")\nplt.show()","7092090b":"print(classification_report(list(Test.label), y_pred_1d))","977e3c20":"results = pd.DataFrame()\n\nY_test = pad_sequences(tokenizer.texts_to_sequences(test.tweet), maxlen = MAX_LENGTH)\n\n\nfinal_scores = model.predict(Y_test, verbose = 1, batch_size = 10000)\ny_pred_1d = [decode_sentiment(score) for score in final_scores]\n\nresults['id'] = test['id'].tolist()\nresults['label'] = y_pred_1d\nresults.to_csv('tweets_v1.csv', index = False)\nresults.head()","a1cec5e9":"import tensorflow as tf\nimport transformers","0bedd59f":"# Maximum sequence size for BERT is 512\n\ndef regular_encode(texts, tokenizer, maxlen = 512):\n    enc_di = tokenizer.batch_encode_plus(texts, return_attention_masks = False, return_token_type_ids = False, pad_to_max_length = True, max_length = maxlen)\n    return np.array(enc_di['input_ids'])","cb07b27c":"#bert large uncased pretrained tokenizer\n\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')","0e06699e":"X_train, X_test, y_train, y_test = train_test_split(train_data['tweet'], train_data['label'], random_state = 22, test_size = 0.1)","7e93a1e2":"#tokenizing the tweets' descriptions and converting the categories into one hot vectors using tf.keras.utils.to_categorical\n\nXtrain_encoded = regular_encode(X_train.astype('str'), tokenizer, maxlen = 128)\nytrain_encoded = tf.keras.utils.to_categorical(y_train, num_classes = 2, dtype = 'int32')\nXtest_encoded = regular_encode(X_test.astype('str'), tokenizer, maxlen = 128)\nytest_encoded = tf.keras.utils.to_categorical(y_test, num_classes = 2, dtype = 'int32')","535c3dfe":"def build_model(transformer, loss = 'categorical_crossentropy', max_len = 512):\n    input_word_ids = tf.keras.layers.Input(shape = (max_len,), dtype = tf.int32, name = \"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n\n    #adding dropout layer\n    x = tf.keras.layers.Dropout(0.40)(cls_token)\n\n    #using a dense layer of 2 neurons as the number of unique categories is 2. \n    out = tf.keras.layers.Dense(2, activation = 'softmax')(x)\n\n    model = tf.keras.Model(inputs = input_word_ids, outputs = out)\n    model.compile(tf.keras.optimizers.Adam(lr = 3e-5), loss = loss, metrics = ['accuracy'])\n    return model","fef422b1":"#building the model on tpu\n\nwith strategy.scope():\n    transformer_layer = transformers.TFAutoModel.from_pretrained('bert-large-uncased')\n    model = build_model(transformer_layer, max_len = 128)\nmodel.summary()","8e4dddf4":"#creating the training and testing dataset.\n\nBATCH_SIZE = 32*strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE \ntrain_dataset = (tf.data.Dataset.from_tensor_slices((Xtrain_encoded, ytrain_encoded)).repeat().shuffle(2048).batch(BATCH_SIZE).prefetch(AUTO))\ntest_dataset = (tf.data.Dataset.from_tensor_slices(Xtest_encoded).batch(BATCH_SIZE))","301f700a":"#training for 10 epochs\n\nn_steps = Xtrain_encoded.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(train_dataset, steps_per_epoch = n_steps, epochs = 20)","859a6e33":"#making predictions \n\npreds = model.predict(test_dataset, verbose = 1)\n\n#converting the one hot vector output to a linear numpy array.\npred_classes = np.argmax(preds, axis = 1)","e4af9dc4":"Test = test['tweet']\nTestEncoded = regular_encode(Test.astype('str'), tokenizer, maxlen = 128)\nTestDataset = (tf.data.Dataset.from_tensor_slices(TestEncoded).batch(BATCH_SIZE))\n\n#making predictions\nPreds = model.predict(TestDataset, verbose = 1)\n\n#converting the one hot vector output to a linear numpy array.\npredClasses = np.argmax(Preds, axis = 1)\npredClasses","89365e11":"results = pd.DataFrame()\n\nresults['id'] = test['id'].tolist()\nresults['label'] = predClasses\nresults.to_csv('tweets_BERT_v4.csv', index = False)\nresults.head()","3e6a69b4":"## Keras Tokenizer\n\nThis class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n\n- **fit_on_texts** : Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. \n\n##### So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).","ed17241f":"##### Bert large uncased\n\n- 24-layer, 1024-hidden, 16-heads, 340M parameters\n- Trained on lower-cased English text","a324c021":"Next, we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight vector from the loaded GloVe embedding.\n\nThe result is a matrix of weights only for words we will see during training.","2f158800":"# Twitter Sentiment Analysis using Part 1: **Glove Embeddings** and Part 2: **BERT model**","57d7a545":"##### The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n\n##### Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist\/sexist and label '0' denotes the tweet is not racist\/sexist, your objective is to predict the labels on the test dataset.\n\n \n\n## Motivation\n\nHate  speech  is  an  unfortunately  common  occurrence  on  the  Internet.  Often social media sites like Facebook and Twitter face the problem of identifying and censoring  problematic  posts  while weighing the right to freedom of speech. The  importance  of  detecting  and  moderating hate  speech  is  evident  from  the  strong  connection between hate speech and actual hate crimes. Early identification of users promoting  hate  speech  could  enable  outreach  programs that attempt to prevent an escalation from speech to action. Sites such as Twitter and Facebook have been seeking  to  actively  combat  hate  speech. In spite of these reasons, NLP research on hate speech has been very limited, primarily due to the lack of a general definition of hate speech, an analysis of its demographic influences, and an investigation of the most effective features.\n\n \n\n## Data\n\nOur overall collection of tweets was split in the ratio of 65:35 into training and testing data. Out of the testing data, 30% is public and the rest is private.\n\n \n\n### Data Files\n \n- train.csv - For training the models, we provide a labelled dataset of 31,962 tweets. The dataset is provided in the form of a csv file with each line    storing a tweet id, its label and the tweet.\n\n- test_tweets.csv - The test data file contains only tweet ids and the tweet text with each tweet in a new line.\n \n\n### Submission Details\n\nThe following 2 files are to be uploaded:\n\n1. test_predictions.csv - This should contain the 0\/1 label for the tweets in test_tweets.csv, in the same order corresponding to the tweets in test_tweets.csv. Each 0\/1 label should be in a new line.\n \n2. A .zip file of source code - The code should produce the output file submitted and must be properly commented.\n \n\n## Evaluation Metric:\n\nThe metric used for evaluating the performance of classification model would be F1-Score.\nThe metric can be understood as -\n\nTrue Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.\n\nTrue Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.\n\nFalse Positives (FP) \u2013 When actual class is no and predicted class is yes.\n\nFalse Negatives (FN) \u2013 When actual class is yes but predicted class in no.\n\nPrecision = TP\/TP+FP\n\nRecall = TP\/TP+FN\n\n### F1 Score = 2*(Recall * Precision) \/ (Recall + Precision)\n\n- F1 is usually more useful than accuracy, especially if for an uneven class distribution.","d6a99459":"# Fetch Glove embeddings","34e7b779":"# Predictions on unseen data","ef58ceb8":"Next, we need to load the entire GloVe word embedding file into memory as a dictionary of word to embedding array.","f007da53":"# Model Building","5c26caf5":"- **max_len**: Maximum sequence size for BERT is 512\n- **batch_encode_plus**: It will generate a dictionary which contains the input_ids, token_type_ids and the attention_mask as list for each input sentence","b16583af":"### What are word embeddings?\n\n##### \"Word embeddings\" are a family of natural language processing techniques aiming at mapping semantic meaning into a geometric space. This is done by associating a numeric vector to every word in a dictionary, such that the distance (e.g. L2 distance or more commonly cosine distance) between any two vectors would capture part of the semantic relationship between the two associated words. The geometric space formed by these vectors is called an embedding space.\n\n##### For instance, \"coconut\" and \"polar bear\" are words that are semantically quite different, so a reasonable embedding space would represent them as vectors that would be very far apart. But \"kitchen\" and \"dinner\" are related words, so they should be embedded close to each other.","e6f2eb69":"- ReduceLROnPlateau: Reduce learning rate when a metric has stopped improving","1c2ba86f":"### KERAS pad_sequences\n\n###### This function transforms a list (of length num_samples) of sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, or the length of the longest sequence in the list.\n- Sequences that are shorter than num_timesteps are padded with value until they are num_timesteps long.\n- Sequences longer than num_timesteps are truncated so that they fit the desired length.\n\n###### The position where padding or truncation happens is determined by the arguments padding and truncating, respectively. Pre-padding or removing values from the beginning of the sequence is the default.","fea31312":"# Final Submission","bc41689f":"# Classification Matrix","d68168d5":"- Bidirectional LSTM\n##### The basic idea of bidirectional recurrent neural nets is to present each training sequence forwards and backwards to two separate recurrent nets, both of which are connected to the same output layer. \u2026 This means that for every point in a given sequence, the BRNN has complete, sequential information about all points before and after it. Also, because the net is free to use as much or as little of this context as necessary, there is no need to find a (task-dependent) time-window or target delay size.\n\n\n- Dropout\n##### It is a technique where randomly selected neurons are ignored during training. They are \u201cdropped-out\u201d randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n\n\n- SpatialDropout1D\n##### This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements","7ebd8c69":"# Splitting the data into Train an Test","a85e11db":"# KERAS","03e412a0":"# Import and load the dataset","4a13d160":"#### NOTE: At this point, we could go on and create word clouds. However, by looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so let's figure that out\n\n### Look at the most common top words add them to the stop word list","87ad0fe8":"### Wordclouds for positive and negative tweets","23202254":"# Data Preprocessing and Cleaning","2f1152b1":"## Model Performance","df7b0a39":"# Exploratory Data Analysis","303e6361":"# BERT Model"}}