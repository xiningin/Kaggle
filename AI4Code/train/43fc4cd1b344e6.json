{"cell_type":{"633ebbdf":"code","0bb7ab8d":"code","a6bfb7a7":"code","38d08f88":"code","e9c73fe8":"code","9196a922":"code","cce8cd0e":"code","0b31da38":"code","ab777dcb":"code","e656e3a0":"code","0445c9cb":"code","57c7c45e":"code","8e23fd66":"code","5fba90f3":"code","6d9aba2f":"code","9adc807d":"code","66ad3b6c":"code","7a9a983d":"code","7f42cb1a":"code","414c5581":"code","2b07fa6b":"code","d04fe72f":"code","ba0f1cbd":"code","adedefd1":"code","4c36eb60":"code","0cbf34a6":"code","65d2ccf6":"code","07d611e9":"code","0951f456":"code","0ead7300":"code","1db82567":"code","a9db1000":"code","d4bcdab5":"markdown","f191345b":"markdown","c8f32b66":"markdown","7f0f76ea":"markdown","3eca1373":"markdown","de4d159b":"markdown","6ce4549d":"markdown","e5d03544":"markdown","79619c99":"markdown","686a9ae8":"markdown","10295c46":"markdown","fa82f225":"markdown","d1fa4fcf":"markdown","ce0c0846":"markdown","83bd20e5":"markdown","1ccf7a25":"markdown","9ec91079":"markdown","beccc88d":"markdown","2c4fe173":"markdown","c95e0c2e":"markdown","e67ce033":"markdown","41f05ae9":"markdown","674776ac":"markdown"},"source":{"633ebbdf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn import base","0bb7ab8d":"df_train=pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ndf_test=pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')","a6bfb7a7":"print('train data set has got {} rows and {} columns'.format(df_train.shape[0],df_train.shape[1]))\nprint('test data set has got {} rows and {} columns'.format(df_test.shape[0],df_test.shape[1]))\n","38d08f88":"df_train.head()","e9c73fe8":"df_train.info()","9196a922":"X=df_train.drop(['target'],axis=1)\ny=df_train['target']\n#X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)","cce8cd0e":"x=y.value_counts()\nplt.bar(x.index,x)\nplt.gca().set_xticks([0,1])\nplt.title('distribution of target variable')\nplt.show()","0b31da38":"from sklearn.preprocessing import LabelEncoder","ab777dcb":"%%time\n\ntrain=pd.DataFrame()\nlabel=LabelEncoder()\nfor c in  X.columns:\n    if(X[c].dtype=='object'):\n        train[c]=label.fit_transform(X[c])\n    else:\n        train[c]=X[c]\n        \ntrain.head(3)    ","e656e3a0":"\nprint('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))\n","0445c9cb":"def logistic(X,y):\n    X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)\n    lr=LogisticRegression()\n    lr.fit(X_train,y_train)\n    y_pre=lr.predict(X_test)\n    print('Accuracy : ',accuracy_score(y_test,y_pre))\n","57c7c45e":"logistic(train,y)","8e23fd66":"#train=pd.get_dummies(X).astype(np.int8)\n#print('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))\n\n","5fba90f3":"from sklearn.preprocessing import OneHotEncoder","6d9aba2f":"%%time \n\none=OneHotEncoder()\n\none.fit(X)\ntrain=one.transform(X)\n\nprint('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))\n\n","9adc807d":"logistic(train,y)","66ad3b6c":"from sklearn.feature_extraction import FeatureHasher","7a9a983d":"%%time\n\nX_train_hash=X.copy()\nfor c in X.columns:\n    X_train_hash[c]=X[c].astype('str')      \nhashing=FeatureHasher(input_type='string')\ntrain=hashing.transform(X_train_hash.values)","7f42cb1a":"\nprint('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))\n\n","414c5581":"logistic(train,y)","2b07fa6b":"%%time\n\nX_train_stat=X.copy()\nfor c in X_train_stat.columns:\n    if(X_train_stat[c].dtype=='object'):\n        X_train_stat[c]=X_train_stat[c].astype('category')\n        counts=X_train_stat[c].value_counts()\n        counts=counts.sort_index()\n        counts=counts.fillna(0)\n        counts += np.random.rand(len(counts))\/1000\n        X_train_stat[c].cat.categories=counts\n    \n        \n        ","d04fe72f":"X_train_stat.head(3)","ba0f1cbd":"print('train data set has got {} rows and {} columns'.format(X_train_stat.shape[0],X_train_stat.shape[1]))\n        ","adedefd1":"logistic(X_train_stat,y)","4c36eb60":"%%time\n\nX_train_cyclic=X.copy()\ncolumns=['day','month']\nfor col in columns:\n    X_train_cyclic[col+'_sin']=np.sin((2*np.pi*X_train_cyclic[col])\/max(X_train_cyclic[col]))\n    X_train_cyclic[col+'_cos']=np.cos((2*np.pi*X_train_cyclic[col])\/max(X_train_cyclic[col]))\nX_train_cyclic=X_train_cyclic.drop(columns,axis=1)\n\nX_train_cyclic[['day_sin','day_cos']].head(3)","0cbf34a6":"one=OneHotEncoder()\n\none.fit(X_train_cyclic)\ntrain=one.transform(X_train_cyclic)\n\nprint('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))\n","65d2ccf6":"logistic(train,y)","07d611e9":"%%time\n\nX_target=df_train.copy()\nX_target['day']=X_target['day'].astype('object')\nX_target['month']=X_target['month'].astype('object')\nfor col in X_target.columns:\n    if (X_target[col].dtype=='object'):\n        target= dict ( X_target.groupby(col)['target'].agg('sum')\/X_target.groupby(col)['target'].agg('count'))\n        X_target[col]=X_target[col].replace(target).values\n        \n    \n    \n\nX_target.head(4)","0951f456":"logistic(X_target.drop('target',axis=1),y)","0ead7300":"X['target']=y\ncols=X.drop(['target','id'],axis=1).columns","1db82567":"%%time\n\nX_fold=X.copy()\nX_fold[['ord_0','day','month']]=X_fold[['ord_0','day','month']].astype('object')\nX_fold[['bin_3','bin_4']]=X_fold[['bin_3','bin_4']].replace({'Y':1,'N':0,'T':1,\"F\":0})\nkf = KFold(n_splits = 5, shuffle = False, random_state=2019)\nfor train_ind,val_ind in kf.split(X):\n    for col in cols:\n        if(X_fold[col].dtype=='object'):\n            replaced=dict(X.iloc[train_ind][[col,'target']].groupby(col)['target'].mean())\n            X_fold.loc[val_ind,col]=X_fold.iloc[val_ind][col].replace(replaced).values\n\n            ","a9db1000":"X_fold.head()","d4bcdab5":"## Method 3 : Feature hashing (a.k.a the hashing trick)  <a id='3'><\/a>","f191345b":"![](https:\/\/miro.medium.com\/max\/1955\/1*ZKD4eZXzd_FdN0SQDszFVQ.png)","c8f32b66":"# Summary <a id='5'><\/a>\n\nHere you can see the summary of our model performance against each of the encoding techniques we have used.\nIt is clear that OnHotEncoder together with cyclic feature encoding yielded maximum accuracy.\n\n<table style=\"width : 50%\">\n    <tr>\n    <th>Encoding<\/th>\n    <th>Score<\/th>\n    <th>Wall time<\/th>\n    <\/tr>\n    <tr>\n    <td>Label Encoding<\/td>\n    <td>0.692<\/td>\n    <td> 973 ms<\/td>\n    <\/tr>\n    <tr>\n    <td>OnHotEncoder<\/td>\n    <td>0.759<\/td>\n    <td>1.84 s<\/td>\n    <\/tr>\n    <tr>\n    <td>Feature Hashing<\/td>\n    <td>0.751<\/td>\n    <td>4.96 s<\/td>\n    <\/tr>\n    <tr>\n    <td>Dataset statistic encoding<\/td>\n    <td>0.694<\/td>\n    <td>894 ms<\/td>\n    <\/tr>\n    <\/tr>\n    <tr>\n    <td>Cyclic + OnHotEncoding<\/td>\n    <td>0.759<\/td>\n    <td>431 ms<\/td>\n    <\/tr>\n    <\/tr>\n    <tr>\n    <td>Target encoding<\/td>\n    <td>0.694<\/td>\n    <td>2min 5s<\/td>\n    <\/tr>\n    \n<\/table>\n    ","7f0f76ea":"## Method 1: Label encoding <a id='1'><\/a>\nIn this method we change every categorical data to a number.That is each type will be subtuted by a number.for example we will substitute 1 for Grandmaster,2 for master ,3 for expert etc..\nFor implementing this we will first import *Labelencoder* from  *sklearn* module.","3eca1373":"Encoding for India = [Number of true targets under the label India\/ Total Number of targets under the label India] \nwhich is 2\/3 = 0.66\n\n<table style=\"width : 20%\">\n    <tr>\n    <th>Country<\/th>\n    <th>Target<\/th>\n    <\/tr>\n    <tr>\n    <td>India<\/td>\n    <td>0.66<\/td>\n    <\/tr>\n    <tr>\n    <td>China<\/td>\n    <td>0.5<\/td>\n    <\/tr>\n<\/table>\n\n","de4d159b":"Feature hashing is a very cool technique to represent categories in a \u201cone hot encoding style\u201d as a sparse matrix but with a much lower dimensions. In feature hashing we apply a hashing function to the category and then represent it by its indices. for example, if we choose a dimension of 5 to represent \u201cNew York\u201d we will calculate H(New York) mod 5 = 3 (for example) so New York representation will be (0,0,1,0,0).","6ce4549d":"- **Method 1 :** [Label encoding](#1)\n- **Method 2 :** [OnHot encoding](#2)\n- **Method 3 :** [Feature Hashing](#3)\n- **Method 4 :** [Encoding categories with dataset statistics](#4)\n- **Cyclic features :** [Encoding cyclic features](#6)\n- **Method 5:** [Target Encoding](#7)\n- **Method 6 :** [K-Fold target encoding](#8)\n- **Summary :** [Summary of model performance](#5)","e5d03544":"## Encoding cyclic features  <a id='6'><\/a>\n![](https:\/\/miro.medium.com\/max\/343\/1*70cevmU8wNggGJEdLam1lw.png)\n\nSome of our features are cyclic in nature.ie day,month etc.\n\nA common method for encoding cyclical data is to transform the data into two dimensions using a sine and consine transformation.\n\n","79619c99":" Now we will try to give our models a numeric representation for every category with a small number of columns but with an encoding that will put similar categories close to each other. The easiest way to do it is replace every category with the number of times that we saw it in the dataset. This way if New York and New Jersey are both big cities, they will probably both appear many times in our dataset and the model will know that they are similar.","686a9ae8":"**You can try two or more of this approaches together,and encode the dataset in suitable way to acheive higher accuracy**\n","10295c46":"Now we will do these three steps to label encode our data:\n- Initialize the labelencoder class\n- Call the fit() method to fit the data\n- Transform data to labelencoded data","fa82f225":"This produces output as a pandas dataframe.Alternatively we can use *OneHotEncoder()* method available in* sklearn* to convert out data to on-hot encoded data.But this method produces a sparse metrix.The advantage of this methos is that is uses very less memory\/cpu resourses.\nTo do that,we need to :\n- Import OneHotEncoder from sklean.preprocessing\n- Initialize the OneHotEncoder\n- Fit and then transform our data","d1fa4fcf":"### Defining the train and target","ce0c0846":"Before getting into encoding,I will just breif you with types data variables present in this data:\n- **Binary data** : A  binary variable a variable that has only 2 values..ie 0\/1\n- **Categorical data** : A categorical variable is a variable that can take some limited number of values.for example,day of the week.It can be one of 1,2,3,4,5,6,7 only.\n- **Ordinal data** : An ordinal variable is a categorical variable that has some order associated with it.for example,the ratings that are given to a movie by a user.\n- **Nominal data** :  Nominal value is a variable that has no numerical importance,such as occupation,person name etc..\n- **Timeseries data** : Time series data has a temporal value attached to it, so this would be something like a date or a time stamp that you can look for trends in time.\n\n","83bd20e5":"## Method 2 : On hot encoding  <a id='2'><\/a>\nOur second method is encoding each category as a one hot encoding (OHE) vector (or dummy variables). OHE is a representation method that takes each category value and turns it into a binary vector of size |i|(number of values in category i) where all columns are equal to zero besides the category column. Here is a little example:   \n\n\n![](https:\/\/miro.medium.com\/max\/878\/1*WXpoiS7HXRC-uwJPYsy1Dg.png)\n\nTo implement on-hot encoding we will use *get_dummies()* function in *pandas*.\n\n","1ccf7a25":"Here you can see the label encoded output train data.We will check the shape of train data now and verify that there is no change in the number of columns.","9ec91079":"## Method 5 : Target encoding <a id='7'><\/a>\n \t\t\nTarget-based encoding is numerization of categorical variables via target. In this method, we replace the categorical variable with just one new numerical variable and replace each category of the categorical variable with its corresponding probability of the target (if categorical) or average of the target (if numerical). The main drawbacks of this method are its dependency to the distribution of the target, and its lower predictability power compare to the binary encoding method.\n\nfor example,\n<table style=\"width : 20%\">\n    <tr>\n    <th>Country<\/th>\n    <th>Target<\/th>\n    <\/tr>\n    <tr>\n    <td>India<\/td>\n    <td>1<\/td>\n    <\/tr>\n    <tr>\n    <td>China<\/td>\n    <td>0<\/td>\n    <\/tr>\n    <tr>\n    <td>India<\/td>\n    <td>0<\/td>\n    <\/tr>\n    <tr>\n    <td>China<\/td>\n    <td>1<\/td>\n    <\/tr>\n    <\/tr>\n    <tr>\n    <td>India<\/td>\n    <td>1<\/td>\n    <\/tr>\n<\/table>\n\n","beccc88d":"## Importing required libraries","2c4fe173":"# Encoding Techniques","c95e0c2e":"Now we will use OnHotEncoder to encode other variables,then feed the data to our model.","e67ce033":"### Logistic regression","41f05ae9":"## Method 4 :Encoding categories with dataset statistics  <a id='4'><\/a>","674776ac":"### K-Fold target encoding <a id='8' ><\/a>\n\nk-fold target encoding can be applied to reduce the overfitting. In this method, we divide the dataset into the k-folds, here we consider 5 folds. Fig.3 shows the first round of the 5 fold cross-validation. We calculate mean-target for fold 2, 3, 4 and 5 and we use the calculated values, mean_A = 0.556 and mean_B = 0.285 to estimate mean encoding for the fold-1."}}