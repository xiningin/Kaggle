{"cell_type":{"4d4ae1ec":"code","f604bebd":"code","62338759":"code","a1ba9ef0":"code","185cf187":"code","b0b4efe0":"code","1f9a5163":"code","960c4da8":"code","a9d0a28c":"code","f1453b44":"code","7a089f14":"code","11c6f5cc":"code","b3ca9199":"code","3c3b28ba":"code","c21d07d7":"code","19cbcf54":"code","6c48703a":"code","cfff5561":"code","c706a1cf":"code","b3267e90":"code","b93e8e55":"code","f248644b":"code","95e57e5a":"code","52351c74":"code","24b7c0b7":"code","784d4599":"code","a48b08f7":"code","7ffdc134":"code","2c0336e0":"code","1fc15597":"code","3451a063":"code","0d1cf972":"markdown","84ab48bd":"markdown","7bf59f99":"markdown","aa90066f":"markdown","5a619c57":"markdown","5880237f":"markdown","0f5f2c40":"markdown","3f288ff2":"markdown","57e4b71c":"markdown","c07be12b":"markdown","17ae9c4d":"markdown","a8dde0a5":"markdown","07dc91d9":"markdown","3ac11849":"markdown","175d7f21":"markdown","f48e68e4":"markdown","effd7a1c":"markdown","60156dc5":"markdown","37001e99":"markdown","ab854e3d":"markdown","8065d850":"markdown","cdd67190":"markdown","8167ce50":"markdown","9a16f34a":"markdown","914dc5a6":"markdown","cbe9f146":"markdown","40596563":"markdown","1981c91a":"markdown","67531c26":"markdown"},"source":{"4d4ae1ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom plotly.subplots import make_subplots #Visualizations\nimport plotly.graph_objects as go #visualizations\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n#read CSV and check\ndf=pd.read_csv('..\/input\/retail-store-sales-transactions\/scanner_data.csv')\ndf.head()\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f604bebd":"df.duplicated().sum() #finds duplicayet rows","62338759":"df.isnull().sum() #Finds any null values","a1ba9ef0":"df.drop('Unnamed: 0', axis=1, inplace=True)\ndf.head()","185cf187":"df.dtypes","b0b4efe0":"df['Date'] = pd.to_datetime(df['Date'])\ndf.dtypes","1f9a5163":"df.sort_values(by=['Date'])","960c4da8":"print(df['Date'].nunique())","a9d0a28c":"pd.date_range(start = '2017-01-02', end = '2018-01-01' ).difference(df['Date'])","f1453b44":"#Create new Dataframes containing individual breakdowns of time period and sales amount\nsales_time = df.sort_values('Date').copy()\nsales_time['Quarter']= df.Date.dt.quarter\nsales_time['Month']= df.Date.dt.month\nsales_time['Week']= df.Date.dt.isocalendar().week\nsales_time['Day of Week']= df.Date.dt.dayofweek #Monday = 0","7a089f14":"sales_time.head()","11c6f5cc":"sales_by_quarter= sales_time.groupby(['Quarter']).agg({'Sales_Amount':'sum'}).reset_index()\nsales_by_Month= sales_time.groupby(['Month']).agg({'Sales_Amount':'sum'}).reset_index()\nsales_by_week= sales_time.groupby(['Week']).agg({'Sales_Amount':'sum'}).reset_index()\nsales_by_weekday= sales_time.groupby(['Day of Week']).agg({'Sales_Amount':'sum'}).reset_index()","b3ca9199":"fig=make_subplots(rows=2,cols=2, subplot_titles=('Sales by Quarter', 'Sales by Month', 'Sales By Week', 'Sales By Day of Week (0=Monday)'))\nfig.add_trace(go.Bar(x=sales_by_quarter['Quarter'], y=sales_by_quarter['Sales_Amount']), row=1, col=1)\nfig.add_trace(go.Bar(x=sales_by_Month['Month'], y=sales_by_Month['Sales_Amount']), row=1, col=2)\nfig.add_trace(go.Bar(x=sales_by_week['Week'], y=sales_by_week['Sales_Amount']), row=2, col=1)\nfig.add_trace(go.Bar(x=sales_by_weekday['Day of Week'], y=sales_by_week['Sales_Amount']), row=2, col=2)\nfig.update_layout(showlegend=False, title_text=\"Sales Amounts in Currency Over different Periods\")","3c3b28ba":"#Groups SKU's amd gives us total sales and quanitity sold for each\nsku= df.groupby(['SKU']).agg({'Sales_Amount':'sum', 'Quantity':'sum'}).reset_index()\nsku.head()","c21d07d7":"len(sku)","19cbcf54":"sku.sort_values(['Quantity'], ascending=False).head(10)","6c48703a":"sku.sort_values(['Sales_Amount']).head(10)","cfff5561":"transactions=sales_time.sort_values('Transaction_ID')\ndf.Transaction_ID.nunique()","c706a1cf":"transactions_by_quarter= sales_time.groupby(['Quarter']).agg({'Transaction_ID':'nunique'}).reset_index()\ntransactions_by_Month= sales_time.groupby(['Month']).agg({'Transaction_ID':'nunique'}).reset_index()\ntransactions_by_week= sales_time.groupby(['Week']).agg({'Transaction_ID':'nunique'}).reset_index()\ntransactions_by_weekday= sales_time.groupby(['Day of Week']).agg({'Transaction_ID':'nunique'}).reset_index()","b3267e90":"fig2=make_subplots(rows=2,cols=2, subplot_titles=('Transactions by Quarter', 'Transactions by Month', 'Transactions By Week', 'Transactions By Day of Week (0=Monday)'))\nfig2.add_trace(go.Bar(x=transactions_by_quarter['Quarter'], y=transactions_by_quarter['Transaction_ID']), row=1, col=1)\nfig2.add_trace(go.Bar(x=transactions_by_Month['Month'], y=transactions_by_Month['Transaction_ID']), row=1, col=2)\nfig2.add_trace(go.Bar(x=transactions_by_week['Week'], y=transactions_by_week['Transaction_ID']), row=2, col=1)\nfig2.add_trace(go.Bar(x=transactions_by_weekday['Day of Week'], y=transactions_by_week['Transaction_ID']), row=2, col=2)\nfig2.update_layout(showlegend=False, title_text=\"Total Transactions Over different Periods\")","b93e8e55":"fig=make_subplots(rows=2,cols=2, subplot_titles=('Quarterly', 'Monthly', 'Weekly','Day of Week 0=Monday'),\n                                specs=[[{\"secondary_y\": True}, {\"secondary_y\": True}],\n                                       [{\"secondary_y\": True}, {\"secondary_y\": True}]])\n\n#Quarterly\nfig.add_trace(go.Bar(x=sales_by_quarter['Quarter'], y=sales_by_quarter['Sales_Amount'],name='Sale Amount'), row=1, col=1, secondary_y=False)\nfig.add_trace(go.Line(x=transactions_by_quarter['Quarter'], y=transactions_by_quarter['Transaction_ID'],name='Transaction Total'), row=1, col=1, secondary_y=True,)\n#Monthly\nfig.add_trace(go.Bar(x=sales_by_Month['Month'], y=sales_by_Month['Sales_Amount'],name='Sale Amount'), row=1, col=2,secondary_y=False)\nfig.add_trace(go.Line(x=transactions_by_Month['Month'], y=transactions_by_Month['Transaction_ID'],name='Transaction Total'), row=1, col=2,secondary_y=True,)\n#Weekly\nfig.add_trace(go.Bar(x=sales_by_week['Week'], y=sales_by_week['Sales_Amount'],name='Sale Amount'), row=2, col=1,secondary_y=False)\nfig.add_trace(go.Line(x=transactions_by_week['Week'], y=transactions_by_week['Transaction_ID'],name='Transaction Total'), row=2, col=1,secondary_y=True,)\n#By Day of Week\nfig.add_trace(go.Bar(x=sales_by_weekday['Day of Week'], y=sales_by_week['Sales_Amount'],name='Sale Amount'), row=2, col=2,secondary_y=False)\nfig.add_trace(go.Line(x=transactions_by_weekday['Day of Week'], y=transactions_by_week['Transaction_ID'],name='Transaction Total'), row=2, col=2,secondary_y=True,)","f248644b":"grouped=sales_time.groupby(['Week', 'SKU']).agg({'Quantity':['sum']})\nprint(grouped.head())\nprint(grouped.tail())","95e57e5a":"pivot=grouped.pivot_table(values='Quantity', index='SKU', columns='Week')\npivot.head()","52351c74":"#Check datatypes have been preserved\npivot.dtypes","24b7c0b7":"pivot=pivot.replace(np.nan, 0)\npivot.head()","784d4599":"#Must set index to SKU\npivot.reset_index(inplace=True)\n#Creates new Dataframe using Index from Pivot\nstats=pd.DataFrame(index=pivot.index)\nstats['SKU']=pivot['SKU'] #Copies SKUs into column\nstats['total_sold']=pivot.sum(axis=1) #Finds total sold of SKU for the year\nstats['average']=pivot.mean(axis=1) #Gives average of units sold of SKU for weekly period\nstats['std_dev']=pivot.std(axis=1) #Gives the standard deviation of quantity sold in a week\nstats","a48b08f7":"#Check your data matches\nstats2=stats.copy()\nprint(df['Quantity'].sum())\nprint(stats2['total_sold'].sum())","7ffdc134":"#Create separate dataframe for SKUs\nskuCat=pd.DataFrame()\nskuCat['SKU']=df['SKU']\nskuCat['SKU_Category']=df['SKU_Category']","2c0336e0":"#Drop duplicates and sort values to match out stats dataframe\nskuCat.drop_duplicates(inplace=True)\nskuCat.sort_values('SKU', inplace=True)\nstats2.sort_values('SKU', inplace=True)","1fc15597":"print(skuCat)\nprint(stats2)","3451a063":"stats=pd.merge(stats2, skuCat, how='outer', on='SKU')\nstats.head()","0d1cf972":"## Using Plotly, we will create a subplot view of sales amount over these time frames","84ab48bd":"### Total Sku's = 5242, as we expected from data source","7bf59f99":"## Transactions over time appear very similar to Sales Amount over time. \n### We can add these 2 sets of charts together to better understand the relationship, with transactions as line charts","aa90066f":"### Top 10 Best selling items this year by quantity","5a619c57":"### That looks good\n\n### Lets turn this into a pivot table to grab some statistics","5880237f":"### Lets replace those NAN values with zeros","0f5f2c40":"### Merge this info using Full outer join back to stats dataframe\n","3f288ff2":"# Set Data for Sales Amount Tables","57e4b71c":"### Indeed there is 1 year of data, beginning on 2017-01-02 and ending 2018-01-01 <br> A full Calendar year","c07be12b":"### Those missing dates are 2017-03-28 & 2017-12-26","17ae9c4d":"### We will create new dataframes similarly to Sales Amount data","a8dde0a5":"### Nice\n\n## The data we recieved states this is 1 year of data, lets check that","07dc91d9":"### Worst 10 selling items this year by quantity","3ac11849":"### With this data:\n1. We have an over view of trends in sales amount, as well as quantity of products sold\n2. We also have an statistics table where we can further investigate which items are popular and how often they sell!\n","175d7f21":"# Lets look into SKU'S \n1. How many Sku's are there?\n2. What are the most popular items, by quantity?\n3. What item gave the most revenue?","f48e68e4":"### Make sure everything matches\n\n","effd7a1c":"### Lets see this information graphed to see when the store is busiest with customers","60156dc5":"### Lets add in the SKU category back into the","37001e99":"# Clean it up, check it out for clarity","ab854e3d":"### Total transactions for the year is 64,682, as expected","8065d850":"### Nice, no dupes or missing values\n\n### Lets drop 'Unnamed: 0' columns and go into checking data types","cdd67190":"### Here we see that there are only 363 dates recorded <br>\nWe are missing 2 days, what are they?","8167ce50":"### Lets turn Date column into DateTime","9a16f34a":"# With the above data we can visualize a quick idea of the spread of sales\n## Some insights\n1. Mondays are typically low earning days\n2. Sales amount rose in the 4th quarter, as supported by rising Sales Amount through weeks","914dc5a6":"# Lets breakdown some informative data on the actual SKU products\n### First we will isolate SKUs and Quantity and turn it into a pivot table, broken down by week","cbe9f146":"# Let us begin to gather Sales Data\n## We will then order subplots over different periods to view Sales Amounts\n## Sales by Quarter, Month, Week, and day\n- Both Sales Amount and Individual Transactions","40596563":"# Lets explore the data with respect to Transactions","1981c91a":"## We can the relationship between Sales Amount and total transactions in that same period","67531c26":"### Now we can see what categories these products belong to!"}}