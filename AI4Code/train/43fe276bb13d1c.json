{"cell_type":{"029daef7":"code","c7442d51":"code","c34d5406":"code","7cfc79dd":"code","19712815":"code","00623ebc":"code","d891ba25":"code","eb5fc2ad":"code","2773e942":"code","65d835b2":"code","33bd00f2":"code","8a13a233":"code","d510ece6":"code","2e943bb0":"code","88bb42c9":"code","692d2c77":"code","49517fd2":"code","ffd91c47":"code","9057a077":"code","44620a94":"code","b5fa0348":"code","60470611":"code","7835f365":"code","aa1884b2":"code","271c7393":"code","bba4b557":"code","000031c7":"code","ebab2daf":"code","ac61288b":"code","27219bdf":"code","2cc2459c":"code","87311087":"code","5748ea15":"code","912eba14":"code","2b6ebad3":"code","c5c0f679":"code","0dbfe16c":"code","12f8574e":"code","cd0db3d6":"code","ea4864de":"code","bbbaaedb":"code","63f9feb0":"code","bfff43f7":"code","5eb3d349":"code","68e879f8":"code","461d001d":"code","cfe1070b":"code","3731bafa":"markdown","f5457c3d":"markdown","0ff5d6f6":"markdown","fcc5e65a":"markdown","d4640b5e":"markdown","a2b2a629":"markdown","9e78a085":"markdown","16dca8e3":"markdown","d88332e3":"markdown"},"source":{"029daef7":"pip install google.colab","c7442d51":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom google.colab.patches import cv2_imshow\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization","c34d5406":"path = '..\/input\/rating-opencv-emotion-images\/Images\/train'","7cfc79dd":"\ntraining_generator = ImageDataGenerator(rescale=1.\/255,\n                                         rotation_range=7,\n                                         horizontal_flip=True,\n                                         zoom_range=0.2)\ntraining_dataset = training_generator.flow_from_directory('..\/input\/rating-opencv-emotion-images\/Images\/train',\n                                                              target_size = (48, 48),\n                                                              batch_size = 16,\n                                                              class_mode = 'categorical',\n                                                              shuffle = True)","19712815":"training_dataset.classes","00623ebc":"training_dataset.class_indices","d891ba25":"sns.countplot(x = training_dataset.classes)","eb5fc2ad":"folder_names = os.listdir(path)\nprint(folder_names)","2773e942":"def plot_examples(folder_names, selected_folder, path):    \n    \n    fig, axs = plt.subplots(1, 5, figsize=(25, 12))\n    fig.subplots_adjust(hspace = .2, wspace=.2)\n    axs = axs.ravel()\n    file_names = os.listdir(path + '\/' + selected_folder)\n    for i in range(5):\n        print(file_names[i])\n        img = cv2.imread(path + '\/' + selected_folder + '\/' + file_names[i])\n        axs[i].imshow(img)\n        axs[i].set_xticklabels([])\n        axs[i].set_yticklabels([])\n        \nplot_examples(folder_names,'Neutral', path)\nplot_examples(folder_names,'Sad', path)\nplot_examples(folder_names,'Surprise', path)\nplot_examples(folder_names,'Fear', path)\nplot_examples(folder_names,'Angry', path)\nplot_examples(folder_names,'Disgust', path)\nplot_examples(folder_names,'Happy', path)","65d835b2":"test_generator = ImageDataGenerator(rescale=1.\/255)\ntest_dataset = test_generator.flow_from_directory('..\/input\/rating-opencv-emotion-images\/Images\/validation',\n                                                  target_size = (48, 48),\n                                                  batch_size = 1,  \n                                                  class_mode = 'categorical',\n                                                  shuffle = False)","33bd00f2":"number_classes = 7\ndetectors_number = 32 \nwidth, height = 48, 48\nepochs = 50\nnetwork = Sequential()\n\nnetwork.add(Conv2D(filters=number_classes, kernel_size=(3,3), activation='relu', padding='same', input_shape=(width, height, 3)))\nnetwork.add(BatchNormalization())  \nnetwork.add(Conv2D(filters=number_classes, kernel_size=(3,3), activation='relu', padding='same'))\nnetwork.add(BatchNormalization())\nnetwork.add(MaxPooling2D(pool_size=(2,2)))\nnetwork.add(Dropout(0.2))\n                            \nnetwork.add(Conv2D(filters=2*number_classes, kernel_size=(3,3), activation='relu', padding='same'))\nnetwork.add(BatchNormalization())\nnetwork.add(Conv2D(filters=2*number_classes, kernel_size=(3,3), activation='relu', padding='same'))\nnetwork.add(BatchNormalization())\nnetwork.add(MaxPooling2D(pool_size=(2,2)))\nnetwork.add(Dropout(0.2))\n\nnetwork.add(Conv2D(filters=2*2*number_classes, kernel_size=(3,3), activation='relu', padding='same'))\nnetwork.add(BatchNormalization())\nnetwork.add(Conv2D(filters=2*2*number_classes, kernel_size=(3,3), activation='relu', padding='same'))\nnetwork.add(BatchNormalization())\nnetwork.add(MaxPooling2D(pool_size=(2,2)))\nnetwork.add(Dropout(0.2))\n\nnetwork.add(Conv2D(filters=2*2*2*number_classes, kernel_size=(3,3), activation='relu', padding='same'))\nnetwork.add(BatchNormalization())\nnetwork.add(Conv2D(filters=2*2*2*number_classes, kernel_size=(3,3), activation='relu', padding='same'))\nnetwork.add(BatchNormalization())\nnetwork.add(MaxPooling2D(pool_size=(2,2)))\nnetwork.add(Dropout(0.2))\n\nnetwork.add(Flatten())\n\nnetwork.add(Dense(units=2 * number_classes, activation='relu'))\nnetwork.add(BatchNormalization())\nnetwork.add(Dropout(0.2))\n\nnetwork.add(Dense(units=2 * number_classes, activation='relu'))\nnetwork.add(BatchNormalization())\nnetwork.add(Dropout(0.2))\n\nnetwork.add(Dense(units=number_classes, activation='softmax'))\nprint(network.summary())","8a13a233":"network.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","d510ece6":"network.fit(training_dataset, epochs=epochs)   ","2e943bb0":"network.evaluate(test_dataset)","88bb42c9":"forecasts = network.predict(test_dataset)\nforecasts","692d2c77":"forecasts = np.argmax(forecasts, axis = 1)\nforecasts","49517fd2":"test_dataset.classes","ffd91c47":"from sklearn.metrics import accuracy_score\naccuracy_score(test_dataset.classes, forecasts)","9057a077":"test_dataset.class_indices","44620a94":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(test_dataset.classes, forecasts)\ncm","b5fa0348":"sns.heatmap(cm, annot=True);","60470611":"from sklearn.metrics import classification_report\nprint(classification_report(test_dataset.classes, forecasts))","7835f365":"image = cv2.imread('..\/input\/celebridades\/Jennifer-Lawrence-2.jpg')\ncv2_imshow(image)","aa1884b2":"image.shape","271c7393":"detector_face = cv2.CascadeClassifier('..\/input\/casdadefrontal\/haarcascade_frontalface_default.xml')","bba4b557":"original_image = image.copy()\ndeteccoes = detector_face.detectMultiScale(original_image)","000031c7":"deteccoes","ebab2daf":"roi = image[60:280 + 120, 90:380 + 120]\ncv2_imshow(roi)","ac61288b":"roi.shape","27219bdf":"roi = cv2.resize(roi, (48,48))\ncv2_imshow(roi)","2cc2459c":"roi.shape","87311087":"roi","5748ea15":"roi = roi \/ 255\nroi","912eba14":"roi.shape","2b6ebad3":"roi = np.expand_dims(roi, axis = 0)\nroi.shape","c5c0f679":"probs = network.predict(roi)\nprobs","0dbfe16c":"forecasts = np.argmax(probs)\nforecasts","12f8574e":"test_dataset.class_indices","cd0db3d6":"image = cv2.imread('..\/input\/celebridades\/17-impresionantes-datos-de-celebridades-que-te-de-2-19568-1493914661-8_dblbig.jpg')\ncv2_imshow(image)","ea4864de":"original_image = image.copy()\ndeteccoes = detector_face.detectMultiScale(original_image)\nroi = image[30:200 + 100, 60:250 + 100]\ncv2_imshow(roi)","bbbaaedb":"roi = cv2.resize(roi, (48,48))\nroi = roi \/ 255\nroi = np.expand_dims(roi, axis = 0)\nprobs = network.predict(roi)\nprobs","63f9feb0":"forecasts = np.argmax(probs)\nforecasts","bfff43f7":"image = cv2.imread('..\/input\/multiple\/emocoes-trabalho.jpg')\ncv2_imshow(image)","5eb3d349":"deteccoes = detector_face.detectMultiScale(image)\ndeteccoes","68e879f8":"test_dataset.class_indices.keys()","461d001d":"emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']","cfe1070b":"for (x, y, w, h) in deteccoes:\n \n  cv2.rectangle(image, (x, y), (x + w, y + h), (0,255,0), 1)\n  roi = image[y:y + h, x:x + w]\n \n  roi = cv2.resize(roi, (48,48))\n\n  roi = roi \/ 255\n  roi = np.expand_dims(roi, axis = 0)\n\n  forecasts = network.predict(roi)\n  \n  cv2.putText(image, emotions[np.argmax(forecasts)], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2, cv2.LINE_AA)\ncv2_imshow(image)","3731bafa":"# Multiple image classification","f5457c3d":"# Single image classification","0ff5d6f6":"# Classification\u00b6 2","fcc5e65a":"# Import from libraries","d4640b5e":"# **If you find this notebook useful, support with an upvote** \ud83d\udc4d\u00b6\u00b6","a2b2a629":"# Neural network evaluation","9e78a085":"# Construction and training of the neural network","16dca8e3":"# Training and testing bases","d88332e3":"# **Conclusion**\n\n**One of the great challenges of this database is that the attributes are very unbalanced, making us have to run more times in the training of the algorithm.**"}}