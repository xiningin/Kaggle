{"cell_type":{"89d0b3b2":"code","fb6a3c5f":"code","694df9ca":"code","a1f4a6c8":"code","8968e36c":"code","6c0df2b5":"code","df21a25a":"code","87be5588":"code","1453bd74":"code","cb695ba3":"code","f452e8c5":"code","bddfd331":"code","8867dd87":"code","0a561915":"code","53494335":"code","c5788b5c":"code","6f78a311":"code","72d18d2d":"code","fa5b7c16":"code","8215cbd5":"code","e444b596":"code","c4d171d1":"code","7ba8d0d7":"code","f4964486":"code","d6992bfd":"code","7c8e967a":"code","9eed134f":"code","146d5d72":"code","bd2d5ab7":"code","06ef42b5":"code","b00a5d92":"code","bca44e93":"code","8f725788":"code","592763b5":"code","513aef31":"code","bb84278e":"code","ce36703c":"code","ab236a6b":"code","bc9bfc80":"code","b554dfeb":"code","2561b8d8":"code","12ea0b3c":"code","1987e259":"code","9ac2524b":"code","e4b8e8e4":"code","ec6f4e71":"markdown","5f888fa5":"markdown","db4ed3fc":"markdown","72fbaac3":"markdown","f8e671e2":"markdown","7609fb6b":"markdown","199bc525":"markdown","974b0174":"markdown","8b57e8d5":"markdown","16192ed3":"markdown","ef3c38bd":"markdown","10728fb1":"markdown","906f2316":"markdown","a941ff2d":"markdown","9cc7b93e":"markdown","6b661791":"markdown","3d40abca":"markdown","5d44d9aa":"markdown","c826a3ce":"markdown","8531fc37":"markdown","8708f15e":"markdown","4430be10":"markdown","a2215f47":"markdown","ecb33da8":"markdown","6ef5388b":"markdown","522a0176":"markdown","a0898351":"markdown","c7f9c91c":"markdown","c9fee582":"markdown","a62f9aa2":"markdown","6b4bd4a2":"markdown","24b625ad":"markdown","7d3bda9c":"markdown","dcf0d576":"markdown","be09cf7e":"markdown"},"source":{"89d0b3b2":"# Base Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Setting the seaborn style\nsns.set_style(\"darkgrid\")\n\n# SciKit Learn Library\nimport sklearn\n\n# For Spliting the Dataset\nfrom sklearn.model_selection import train_test_split\n\n# For Scaling the numeric predictors\nfrom sklearn.preprocessing import StandardScaler\n\n# For GridSearch()\nfrom sklearn.model_selection import GridSearchCV\n\n# For Creating K-folds\nfrom sklearn.model_selection import KFold\n\n# For Classification Report\nfrom sklearn.metrics import classification_report\n\nfrom sklearn import metrics\n\n# supress warnings\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\n# CatBoost library\nimport catboost\n","fb6a3c5f":"# Importing data for Analysis\nsubmission = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nvalidation = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndataset = pd.read_csv(\"..\/input\/titanic\/train.csv\")","694df9ca":"# Dropping irrelevant columns form train and test set\ndataset = dataset.drop(['PassengerId','Name','Ticket','Cabin'],axis = 1)\nvalidation = submission.drop(['PassengerId','Name','Ticket','Cabin'],axis = 1)","a1f4a6c8":"# Let's view the dataset\ndataset.head()","8968e36c":"dataset.info()","6c0df2b5":"# Countplot: to check the balance of Survived Class\nsns.countplot(dataset['Survived'])\nplt.title(\"Survived Class Histogram\")\nplt.show\n\n# Displaying the Counts\ndataset['Survived'].value_counts()","df21a25a":"# Countplot: Survived vs Sex\nsns.countplot(dataset['Sex'], hue=dataset['Survived'])\nplt.title(\"Sex: Survived vs Dead\")\nplt.show()","87be5588":"# Countplot: Survived vs Passenger Class\nsns.countplot(dataset['Pclass'], hue=dataset['Survived'])\nplt.title(\"Passenger Class: Survived vs Dead\")\nplt.show()","1453bd74":"# Countplot: Survived vs Embarked\n\n# Increasing figure size\nplt.figure(figsize=(12,6))\n\n# Initiating first figure\nplt.figure(1)\n\n# Countplot - Embarked: Survive vs Dead\nplt.subplot(1,2,1)\nsns.countplot(dataset['Embarked'], hue=dataset['Survived'])\nplt.title(\"Embarked: Survived vs Dead\")\n\n# Barplot - Embarked Survival Rate\nplt.subplot(1,2,2)\nsns.barplot(x = 'Embarked', y = 'Survived', data = dataset)\nplt.title(\"Embarked Survival Rate\")\nplt.show()","cb695ba3":"# Spliting Dataset into Train and Test\n\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(dataset, train_size = 0.7, test_size = 0.3, random_state = 0)","f452e8c5":"# Increasing figure size\nplt.figure(figsize=(12,6))\n\n# Initiating first figure\nplt.figure(1)\n\n# Boxplot - Age\nplt.subplot(1,2,1)\nsns.boxplot(y = df_train['Age'])\nplt.title(\"Age - Train\")\n\n# Boxplot - Fare\nplt.subplot(1,2,2)\nsns.boxplot(y = df_train['Fare'])\nplt.title(\"Fare - Train\")\n\nplt.show()","bddfd331":"# Increasing figure size\nplt.figure(figsize=(12,6))\n\n# Initiating first figure\nplt.figure(1)\n\n# Boxplot - Age\nplt.subplot(1,2,1)\nsns.boxplot(y = df_test['Age'])\nplt.title(\"Age - Test\")\n\n# Boxplot - Fare\nplt.subplot(1,2,2)\nsns.boxplot(y = df_test['Fare'])\nplt.title(\"Fare - Test\")\n\nplt.show()","8867dd87":"df_train['Age'].describe()","0a561915":"df_train['Fare'].describe()","53494335":"df_test['Age'].describe()","c5788b5c":"df_test['Fare'].describe()","6f78a311":"# Removing Outliers from Age\nQ1 = df_train['Age'].quantile(0.25)\nQ3 = df_train['Age'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Removing Outliers from Train\ndf_train['Age'] = df_train['Age'].drop(df_train[(df_train['Age'] < (Q1 - 1.5 * IQR)) | (df_train['Age'] > (Q3 + 1.5 * IQR))].index)","72d18d2d":"# Removing Outliers from Fare\nQ1 = df_train['Fare'].quantile(0.25)\nQ3 = df_train['Fare'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Removing Outliers from Train\ndf_train['Fare'] = df_train['Fare'].drop(df_train[(df_train['Fare'] < (Q1 - 1.5 * IQR)) | (df_train['Fare'] > (Q3 + 1.5 * IQR))].index)","fa5b7c16":"# Removing Outliers from Age\nQ1 = df_test['Age'].quantile(0.25)\nQ3 = df_test['Age'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Removing Outliers from Test\ndf_test['Age'] = df_test['Age'].drop(df_test[(df_test['Age'] < (Q1 - 1.5 * IQR)) | (df_test['Age'] > (Q3 + 1.5 * IQR))].index)","8215cbd5":"# Removing Outliers from Fare\nQ1 = df_test['Fare'].quantile(0.25)\nQ3 = df_test['Fare'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Removing Outliers from Train\ndf_test['Fare'] = df_test['Fare'].drop(df_test[(df_test['Fare'] < (Q1 - 1.5 * IQR)) | (df_test['Fare'] > (Q3 + 1.5 * IQR))].index)","e444b596":"# Increasing figure size\nplt.figure(figsize=(12,6))\n\n# Initiating first figure\nplt.figure(1)\n\n# Boxplot - Age\nplt.subplot(1,2,1)\nsns.boxplot(y = df_train['Age'])\nplt.title(\"Age\")\n\n# Boxplot - Fare\nplt.subplot(1,2,2)\nsns.boxplot(y = df_train['Fare'])\nplt.title(\"Fare\")\n\nplt.show()","c4d171d1":"# Increasing figure size\nplt.figure(figsize=(12,6))\n\n# Initiating first figure\nplt.figure(1)\n\n# Boxplot - Age\nplt.subplot(1,2,1)\nsns.boxplot(y = df_test['Age'])\nplt.title(\"Age\")\n\n# Boxplot - Fare\nplt.subplot(1,2,2)\nsns.boxplot(y = df_test['Fare'])\nplt.title(\"Fare\")\n\nplt.show()","7ba8d0d7":"# Printing how many missing values are there in each column in Train\ndf_train.isnull().sum()","f4964486":"# Printing how many missing values are there in each column in Test\ndf_test.isnull().sum()","d6992bfd":"# Imputing missing values in Age and Fare with Mean of Columns - Train\ndf_train['Age'].fillna(df_train['Age'].mean(), inplace=True)\ndf_train['Fare'].fillna(df_train['Fare'].mean(), inplace=True)","7c8e967a":"# Imputing missing values in Age and Fare with Mean of Columns - Test\ndf_test['Age'].fillna(df_test['Age'].mean(), inplace=True)\ndf_test['Fare'].fillna(df_test['Fare'].mean(), inplace=True)","9eed134f":"# For Embarked we will be dropping the rows with missing values\ndf_train = df_train.dropna(axis = 0,subset=['Embarked'])\ndf_test = df_test.dropna(axis = 0,subset=['Embarked'])","146d5d72":"# First we will create Column family_size basis SibSp and Parch\ndf_train['family_size'] = df_train.SibSp + df_train.Parch+1\ndf_test['family_size'] = df_test.SibSp + df_test.Parch+1\nvalidation['family_size'] = validation.SibSp + validation.Parch+1","bd2d5ab7":"# From family_size we will derive family_group\ndef family_group(size):\n    b = ''\n    if (size <= 1):\n        b = 'alone'\n    elif (size <= 4):\n        b = 'small'\n    else:\n        b = 'large'\n    return b\n\n# Creating family_group by its size (in both dataset and Submission)\ndf_train['family_group'] = df_train['family_size'].map(family_group)\ndf_test['family_group'] = df_test['family_size'].map(family_group)\nvalidation['family_group'] = validation['family_size'].map(family_group)\n\n# Drop family_size\ndf_train.drop(columns=['family_size'], inplace = True)\ndf_test.drop(columns=['family_size'], inplace = True)\nvalidation.drop(columns=['family_size'], inplace = True)","06ef42b5":"# Function to bin Passengers basis their Age\ndef age_group(age):\n    ag_grp= ''\n    \n    if age <= 8:\n        ag_grp = \"child\"\n    elif age <= 19:\n        ag_grp = \"teenager\"\n    elif age <= 40:\n        ag_grp = \"Adult\"\n    elif age <= 60:\n        ag_grp = \"Senior\"\n    else:\n        ag_grp = \"Old\"\n    return ag_grp\n\ndf_train['age_group'] = df_train['Age'].map(age_group)\ndf_test['age_group'] = df_test['Age'].map(age_group)\nvalidation['age_group'] = submission['Age'].map(age_group)\n\n# Drop Age columns\ndf_train.drop(columns=['Age'], inplace = True)\ndf_test.drop(columns=['Age'], inplace = True)\nvalidation.drop(columns=['Age'], inplace = True)","b00a5d92":"# Let's look our dataset again\ndf_train.head()","bca44e93":"# First, let's understand which columns need treatment\ndf_train.info()","8f725788":"# Scaling Numerical Features\nfrom sklearn.preprocessing import StandardScaler\n\n# Creating StandardScaler Object\nsc = StandardScaler()\n\n# Selecting Numeric Columns\nnumeric_vars = ['Pclass','SibSp','Parch','Fare']\n\n# Scaling Train Dataframe\ndf_train[numeric_vars] = sc.fit_transform(df_train[numeric_vars])\n\n# Transforming Test Dataframe\ndf_test[numeric_vars] = sc.fit_transform(df_test[numeric_vars])","592763b5":"# Assigning Predictors and Response in X and y\ny_train = df_train.pop('Survived')\nX_train = df_train\n\ny_test = df_test.pop('Survived')\nX_test = df_test","513aef31":"# Collecting 'Column Indexes' of all the Categorical Columns in X_train\ncategorical_features_indices = np.where(X_train.dtypes == np.object)[0]\n\n# Printing Categorical Features\ncategorical_features_indices","bb84278e":"# Initialising Base CatBoost Model\n\n# 1. Base Model\ncb = catboost.CatBoostClassifier(loss_function='Logloss',\n                         eval_metric='Logloss',\n                         boosting_type='Ordered', # use permutations\n                         random_seed=2405, \n                         use_best_model=True,\n                         one_hot_max_size = 6,\n                         silent=True)\n\n\n\n# 2. Fitting the Model\ncb.fit(X_train,y_train,cat_features=categorical_features_indices, eval_set=(X_test, y_test))\n\n# 3. Initial Prediction of Results\ny_pred = cb.predict(X_test)\n\n# 4. Predicting Probabilites\ny_pred_proba = cb.predict_proba(X_test)\n\n# 5. Printing Classification Report\nprint(classification_report(y_test, y_pred))\n","ce36703c":"# Printing ROC-AUC score\nfrom sklearn import metrics\nmetrics.roc_auc_score(y_test,y_pred_proba[:,1])\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred_proba[:,1])\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(12,8))\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","ab236a6b":"# Creating a Parameter Grid to be Tuned\nparam_grid = {'depth':[1,3,5,10],\n              'iterations':[100,200,300],\n              'learning_rate':[0.1,0.03,0.01], \n              'l2_leaf_reg':[1,3,5],\n              'border_count':[5,10,20]\n          }\n\n# Creating Hyper-param tuned model with 5 models\n#from sklearn.model_selection import GridSearchCV\n\nmodel_cv = GridSearchCV(estimator = cb, \n                        param_grid = param_grid,\n                        cv = 5, \n                        verbose = 1) ","bc9bfc80":"# Fitting the Hyperparameter-tuned Model\n#model_cv.fit(X_train,y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test))","b554dfeb":"# Getting list of best parameters\n#model_cv.best_params_","2561b8d8":"# Making the Final CatBoost model\ncb_final = catboost.CatBoostClassifier(\n                         loss_function='Logloss',\n                         eval_metric='Logloss',\n                         boosting_type='Ordered', # use permutations\n                         random_seed=240, \n                         use_best_model=True,\n                         one_hot_max_size = 5,\n                         silent=True,\n                         depth = 3,\n                         iterations = 300,\n                         learning_rate = 0.03, \n                         l2_leaf_reg = 5,\n                         border_count = 5\n                        )","12ea0b3c":"# Fitting the Final Model, Final Prediction and Classification report\ncb_final.fit(X_train,y_train,cat_features=categorical_features_indices, eval_set=(X_test, y_test))\n\n# Final Prediction of Results\ny_pred = cb_final.predict(X_test)\n\n# Final Prediction of Probabilities\ny_pred_proba = cb_final.predict_proba(X_test)\n\n# Printing Classification Report\nprint(classification_report(y_test, y_pred))","1987e259":"# Printing ROC-AUC score\nfrom sklearn import metrics\nmetrics.roc_auc_score(y_test,y_pred_proba[:,1])\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred_proba[:,1])\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(12,8))\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","9ac2524b":"# Predicting final results\ny_pred = cb_final.predict(validation)\n\n# Creating final Submission\nsubmission = pd.DataFrame({\n        \"PassengerId\": submission[\"PassengerId\"],\n        \"Survived\": y_pred\n    })","e4b8e8e4":"#submission.to_csv('mycsvfile.csv',index=False)","ec6f4e71":"**4. Understanding the Dataset in bit more detail**","5f888fa5":"From the above histogram we can conclude that the **Dataset is fairly balanced**","db4ed3fc":"**Titanic: Machine Learning from Disaster**\n\nFor the above problem statement, I have used CatBoost algorithm. **Catboost** achieves **better results** when you work with datasets where **categorical features play a large role**. CatBoost is an open-sourced machine learning algorithm from Yandex. It works with diverse data-types to help solve a wide range of problems that businesses face today. To top it up, it provides best-in-class accuracy.\n\n**Advantages of CatBoost Library**\n\n- **Performance- ** CatBoost provides state of the art results and it is competitive with any leading machine learning algorithm on the performance front.\n\n\n- **Handling Categorical features automatically- ** We can use CatBoost without any explicit pre-processing to convert categories into numbers. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features.\n\n\n- **Robust-** It reduces the need for extensive hyper-parameter tuning and lower the chances of overfitting also which leads to more generalized models. Although, CatBoost has multiple parameters to tune and it contains parameters like the number of trees, learning rate, regularization, tree depth, fold size, bagging temperature and others. You can read about all these parameters here.\n\n**Table of Content**\n\n1. Importing Libraries\n\n2. Loading Data\n\n3. Re-defining dataset with relevant columns\n\n4. Deep dive into Dataset\n\n5. Exploratory Data Analysis\n\n6. Spliting the datasest into Train and Test set\n\n7. Detect and Remove Outliers in Age and Fare (numerical) columns\n\n8. Imputing missing values\n\n9. Feature Engineering\n\n10. Scaling Numerical columns and Building Base CatBoost Model\n\n11. Hyper-Parameter tuning\n\n12. CatBoost - Final Model, Prediction and Classification\/ROC-AUC Report\n\n13. Submission","72fbaac3":"**12. CatBoost: Final Model, Predict and Classification Report** ","f8e671e2":"*Fare Column still have few Outliers that we can treat later*","7609fb6b":"**7. Detecting and Removing Outliers, using Boxplot, in Age and Fare columns**","199bc525":"Inital base-model produced an **accuracy of 81%**. We will improve the accuracy by fine-tuning our model ","974b0174":"**8. Imputing missing values in Train and Test**\n\n- *For Age and Fare - we will Impute the missing value with the Mean of Column*\n\n- *For Embarked - we will simply drop the missing values*","8b57e8d5":"From the above we can conclude that **females** were more **likely to survive** than **males**","16192ed3":"**First Class** passengers were **more likely to survive** as compared to **second or third class passenger**","ef3c38bd":"**7.3. Removing Outliers from train dataset**","10728fb1":"***Form the above we can see, that Age column has 177 missing values. These missing values constitute 20% of the Age column. Hence, it's not advisable to simply drop them, rather we will impute the missing values with the 'Mean' of column after the outliers (if any) are removed***\n\n***Also Embarked is another column that has 2 missing values***","906f2316":"**10. CatBoost - BaseModel, Predict and Classification Report **\n\nFor this dataset, I will be using **CatBoost** to predict the outcome whether a passenger will survive the crash or not. \n**Catboost** achieves **better results** when you look at datasets where **categorical features play a large role**.\n\nTo get the best out of Catboost, we would need to understand and tune some of the parameters of this amazing model. Below is a list of parameters that I think needs special attention\n\n**Important Parameters:**\n\n- **cat_features: ** This parameter is a must in order to leverage Catboost preprocessing of categorical features, if you encode the categorical features yourself and don\u2019t pass the columns indices as 'cat_features' you are missing the essence of Catboost. This parameter takes in the **indices** of Categorical Columns\n\n\n- **one_hot_max_size: ** As mentioned before, Catboost uses  one-hot encoding for all features with at most 'one_hot_max_size' unique values. In our case, the categorical features have at most **5 unique values**, so we will set one_hot_max_size as 5\n\n\n- **learning_rate & n_estimators: ** **Smaller learning_rate** and **higher n_estimators** are needed to utilize the model. Usually, the approach is to **start with a relative high learning_rate**, tune other parameters and then **decrease the learning_rate while increasing n_estimators**\n\n\n- **depth: ** Depth of the base trees, this parameter has an high impact on training time.\n\n\n- **l2_leaf_reg: ** L2 regularization coefficient\n\n\n- **random_strength: ** Every split gets a score and random_strength is adding some randomness to the score, it helps to reduce overfitting.\n\n\n- **use_best_model: ** if set to True, this parameter will make the model prevent overfitting\n\n\n- **loss_function: ** For 2-class classification use 'LogLoss'. For Multiclass use 'MultiClass'\n\n\n- **border_count: ** The number of splits for numerical features. Allowed values are integers from 1 to 255 inclusively.\n\n**Attention: Do not use one-hot encoding during preprocessing. This affects both the training speed and the resulting quality.**","a941ff2d":"**10.2. Assigning Predictors in X and Response in y**","9cc7b93e":"**6. Spliting the dataset into Train and Test set**","6b661791":"**5. Exploratory Data Analysis**","3d40abca":"**11. Hyper Parameter Tuning using GridSearchCV**","5d44d9aa":"**3. Dropping Columns not suitable for Analysis**","c826a3ce":"**Printing and Plotting ROC-AUC Score and Curve**","8531fc37":"**Let's make a Boxplot to ensure that Outliers have been properly removed - Train**","8708f15e":"**Let's make a Boxplot to ensure that Outliers have been properly removed - Test**","4430be10":"**7.1. BoxPlot of Age and Fare - Train Dataset**","a2215f47":"**7.2. BoxPlot of Age and Fare - Test Dataset**","ecb33da8":"**Observation:** Most of people who **survived** the disaster, embarked from **Cherbourg**. Whereas, a large proportion of people who **didn't survived** emarked from **Southampton** ","6ef5388b":"*From the above we can conclude that both 'Age' and 'Fare' columns are riddled with Outliers which need to be removed. We will be using IQR technique to treat these Outliers*","522a0176":"**10.3. Passing Indexes of all the Categorical features for train set ONLY**","a0898351":"**Below step is really important, as it's mandatory in CatBoost to pass the indexes of all the categorical columns in our dataset**","c7f9c91c":"**Printing and Plotting ROC-AUC Score and Curve**","c9fee582":"**7.4. Removing Outliers from test dataset**","a62f9aa2":"**10.1. Scaling Numerical Features**","6b4bd4a2":"*To get more insights about the Outliers in Age and Fare Columns, we will use describe() in conjuction to boxplot*","24b625ad":"**9. Feature Engineering**\n\nIn this section we will add **two** new columns\n\n- **family_group** which is a **categorical column** and will be derived using columns **SibSp** and **Parch**\n\n- **age_group** which again is a **categorical column** and used to bin the passengers basis their age","7d3bda9c":"**13. Submission**","dcf0d576":"**2. Loading Data**","be09cf7e":"**1. Importing Libraries**"}}