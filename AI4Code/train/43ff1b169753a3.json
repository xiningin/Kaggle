{"cell_type":{"adc6c232":"code","67f35445":"code","d364d260":"code","a2342a29":"code","8466e30b":"code","1c81f468":"code","698490a6":"code","2f0692f9":"code","c059c5ce":"code","94b268e0":"code","f4574436":"code","cd9d4a48":"code","a335bfbd":"code","45833d21":"code","662e2d13":"code","452a5d40":"code","07ca4613":"markdown","30ef940e":"markdown","2adadfe7":"markdown","6fb741fd":"markdown","81e5fe48":"markdown","d4a6fd52":"markdown","f4a16f2e":"markdown","ec2a9d16":"markdown","bc3171a5":"markdown","4f94abd8":"markdown","1a222a39":"markdown","7f83f544":"markdown","a06b38c7":"markdown","ceb07f6f":"markdown","3ba2abf4":"markdown","b688f002":"markdown","bd4d0ba3":"markdown","5376746e":"markdown","c3c68f70":"markdown","8cf57627":"markdown","f82eb870":"markdown"},"source":{"adc6c232":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67f35445":"import tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\n","d364d260":"tf.__version__","a2342a29":"train_datagen = ImageDataGenerator(\n        rescale=1.\/255, #we get all the pixel values between 0 and 1\n        shear_range=0.2, \n        zoom_range=0.2,\n        horizontal_flip=True)\n\ntraining_set = train_datagen.flow_from_directory(\n        '..\/input\/dogs-cats-images\/dataset\/training_set',\n        target_size=(64, 64),# final size of our images when they will be fed into CNN\n        batch_size=32,# how many images we want to have in each batch, 32 is default value\n        class_mode='binary') # binary outcome, cat or dog\n\n# connect the dataset\n\n#import the training set, and creating batches and resizing the images","8466e30b":"test_datagen = ImageDataGenerator(rescale=1.\/255)\n#we don't want touch test images but we need to rescale\n#no transformation for the test\n\ntest_set = test_datagen.flow_from_directory(\n        '..\/input\/dogs-cats-images\/dataset\/test_set',\n        target_size=(64, 64),\n        batch_size=32,\n        class_mode='binary')","1c81f468":"cnn=tf.keras.models.Sequential()","698490a6":"cnn.add(tf.keras.layers.Conv2D(filters=32,\n                               kernel_size=3,\n                               activation = \"relu\",\n                               input_shape=[64,64,3]))\n\n","2f0692f9":"cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))","c059c5ce":"cnn.add(tf.keras.layers.Conv2D(filters=32,\n                               kernel_size=3,\n                               activation = \"relu\"))\n\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))","94b268e0":"cnn.add(tf.keras.layers.Flatten())","f4574436":"cnn.add(tf.keras.layers.Dense(units=128, activation = \"relu\"))","cd9d4a48":"cnn.add(tf.keras.layers.Dense(units=1, activation = \"sigmoid\"))\n#binary classification ( 0 or 1 )","a335bfbd":"cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","45833d21":"cnn.fit(x=training_set, validation_data=test_set, epochs = 25) # try different epochs\n","662e2d13":"import numpy as np\nfrom keras.preprocessing import image\n\ntest_image = image.load_img(path=\"..\/input\/single-prediction\/cat_or_dog_1.jpg\",\n                            target_size=(64,64))\n# ! Important! - image have to have the same size as used during the training\n\n#We need to convert the image into the array (because the prediction required the 2D array)\ntest_image=image.img_to_array(test_image)\n\n# add an extra \"fake\" dimension: batch_size\ntest_image=np.expand_dims(test_image, axis=0) # having batch as the first dimension\n\nresult = cnn.predict(test_image)\n\n# how to differentiate what is a dog(1) or a cat(0)\n\ntraining_set.class_indices\n\nif result[0][0]==1:\n    prediction = \"dog\"\nelse:\n    prediction = \"cat\"\n#first accessing the batch and then accessing the single element of the batch","452a5d40":"print(prediction)","07ca4613":"## Training the CNN on the Training Set and evaluating it on the Test Set","30ef940e":" ## Step 2 - Pooling","2adadfe7":"# Part 2 - Building the CNN","6fb741fd":"## Add a second convolutional layer","81e5fe48":"# Step 3 - Flattening\nwe do flatten results (of pulling and convolutions) into one dimensional vector","d4a6fd52":"# Step 4 - Full Connection","f4a16f2e":"Transformation on all images - to avoid overfitting \n If we don't apply those transformations, when training the CNN on the training set  we will get a huge difference between the accuracy on the training set\nand the one on the test set\n\nOn the evaluation set, we will get very high accuracy on the training set, close to 98 % a\n and much lower accuracy on the test - called \"overfitting\"\n\n - What are those transformations: it is some simple geometrical transformations\n or some zooms or some rotation on the images, horizontal flips\n\n We are going to do a series of transformation to modify the images and get them augmented\n\n- Image Augmentation - which consists of transforming our images of the training set so that   your CNN model does not overlearned , is not overtrained on the existing images. because by applying those transformations we will get new images  we augment the diversity of the training set images\n\n- We are going to use from ImageDataGenerator: sheer_range, zoom_range and horizontal_flip","ec2a9d16":"### Preprocessing the Test Set\n","bc3171a5":"train_datagen is an instance of that image data generator clas in which represents\nthe tool that will apply all the transformations on the images of the training set\n\n- sheer_range: Float. Shear Intensity (Shear angle in counter-clockwise direction in degrees)\n- zoom_range: Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range].\n-  horizontal_flip:Boolean. Randomly flip inputs horizontally.\n\n rescale: rescaling factor. Defaults to None. \nIf None or 0, no rescaling is applied,\n#otherwise we multiply the data by the value provided (after applying all other transformations). it will apply feature scaling to each and every single one of your pixels by dividing their value by 255. just like normalization","4f94abd8":"### Initialising the CNN","1a222a39":"# Part 3 - Training the CNN","7f83f544":"We will apply a max pooling to obtain the pooled feature map.\n\nArguments:\n- pool_size (2x2 square)\n- strides (recommended: 2) sliding two by two pixels\n- padding - when we reach the border with extra two empty cells, with a valid padding, we will just ignore empty cells, and with the same padding, we will add an extra column with fake pixels that are equal to 0","a06b38c7":"## Compiling the CNN","ceb07f6f":"### Preprocessing the Training Set\n","3ba2abf4":"# Part 4 - Making a single prediction","b688f002":"## Step 1 - Convolution","bd4d0ba3":"Arguments:\n- units - number of hidden neurons (128) have a larger number of neurons\n- activation - rectifier activation function","5376746e":"add the first convolutional level. \n\nArguments:\n- filters - Integer, the dimensionality of the output space (the number of output filter in the convolution) feature detectors. Classic Architecture: \n    First Convolutional layer - 32, Second convolutional layer : 32\n- kernel_size - An inteeger or tuple of 2 integers, specifying the height and width of the 2D Convolution window, the number of rows and columns of Feature Detector (3x3) \n-no default value of the activation, because on the general rule, as long as we have not reached the output layer, we rather want to get a rectified activation function.We will use the \"RELU\"(rectifier activation function) parameters\n- input_shape - when you add your very first layer, whether it is convolutional or a dense layer, you have to specify the input shape of your inputs. (64,64,3) for colorful images, (64,64,1) - if we worked with black-and-white images","c3c68f70":"## Importing Libraries","8cf57627":"# Part 1 - Data Preprocessing","f82eb870":"# Step 5 - Output Layer"}}