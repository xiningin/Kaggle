{"cell_type":{"01a41f2b":"code","4d4f27fe":"code","44991795":"code","ef870387":"code","8c84e449":"code","21ac9e7b":"code","0ae4a783":"code","394a73f6":"code","926dac30":"code","0605492e":"code","80b46271":"code","1cb54080":"markdown"},"source":{"01a41f2b":"import transformers\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import SequentialSampler\nfrom transformers import AutoTokenizer","4d4f27fe":"class configuration:\n    finetuned_robertabase_path=\"..\/input\/robertabasemodelweights\/clrp-robertabase-modelweights\"\n    finetuned_robertalarge_path=\"..\/input\/robertalargemodelweights\"\n    robertabase_tokenizer=\"..\/input\/roberta-base\"\n    robertalarge_tokenizer=\"..\/input\/robertalarge\"\n    batch_size = 16\n    device = 'cuda'\n    max_len = 256\n    contest_data=\"..\/input\/commonlitreadabilityprize\/test.csv\"\n    model_count=5","44991795":"scaler = torch.cuda.amp.GradScaler() \ndevice = torch.device(configuration.device if torch.cuda.is_available() else 'cpu')\nprint(f\"using device {torch.cuda.get_device_name(0)}\")","ef870387":"test = pd.read_csv(configuration.contest_data)","8c84e449":"def convert_examples_to_features(text, tokenizer, max_len):\n\n    tok = tokenizer.encode_plus(\n        text, \n        max_length=configuration.max_len, \n        truncation=True,\n        padding='max_length',\n    )\n    return tok\n\n\nclass CLRPDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.tolist()\n        if not is_test:\n            self.targets = self.data.target.tolist()\n            \n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt = self.excerpts[item]\n            label = self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.float),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","21ac9e7b":"class AttentionHead(nn.Module):\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n        return context_vector\n\nclass CLRPModel(nn.Module):\n\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n        x = self.head(transformer_out.last_hidden_state)\n        x = self.linear(x)\n        return x","0ae4a783":"robertabase_tokenizer=AutoTokenizer.from_pretrained(configuration.robertabase_tokenizer)\nrobertalarge_tokenizer=AutoTokenizer.from_pretrained(configuration.robertalarge_tokenizer)","394a73f6":"roberta_base_predictions = []\n\nfor model_num in range(configuration.model_count):\n    print(f'Model #{model_num+1}\/{configuration.model_count}')\n    test_ds = CLRPDataset(data=test, tokenizer=robertabase_tokenizer, max_len=configuration.max_len, is_test=True)\n    test_sampler = SequentialSampler(test_ds)\n    test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=configuration.batch_size)\n    model = torch.load(f'{configuration.finetuned_robertabase_path}\/model_{model_num}.bin').to(configuration.device)\n\n    all_preds = []\n    model.eval()\n\n    for step,batch in enumerate(test_dataloader):\n        sent_id, mask = batch['input_ids'].to(configuration.device), batch['attention_mask'].to(configuration.device)\n        with torch.no_grad():\n            preds = model(sent_id, mask)\n            all_preds += preds.flatten().cpu().tolist()\n    \n    roberta_base_predictions.append(all_preds)\n\n","926dac30":"\nroberta_large_predictions = []\n\nfor model_num in range(configuration.model_count):\n    print(f'Model #{model_num+1}\/{configuration.model_count}')\n    test_ds = CLRPDataset(data=test, tokenizer=robertalarge_tokenizer, max_len=configuration.max_len, is_test=True)\n    test_sampler = SequentialSampler(test_ds)\n    test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=configuration.batch_size)\n    model = torch.load(f'{configuration.finetuned_robertalarge_path}\/model_{model_num}.bin').to(configuration.device)\n\n    all_preds = []\n    model.eval()\n\n    for step,batch in enumerate(test_dataloader):\n        sent_id, mask = batch['input_ids'].to(configuration.device), batch['attention_mask'].to(configuration.device)\n        with torch.no_grad():\n            preds = model(sent_id, mask)\n            all_preds += preds.flatten().cpu().tolist()\n    \n    roberta_large_predictions.append(all_preds)","0605492e":"roberta_base_predictions = np.array(roberta_base_predictions)\nroberta_large_predictions = np.array(roberta_large_predictions)\n\n   #each model weighted equally\nallpreds= (roberta_large_predictions[0] * .10) + (roberta_large_predictions[1] * .10) + (roberta_large_predictions[2] * .10) + \\\n    (roberta_large_predictions[3] * .10) + (roberta_large_predictions[4] * .10) + (roberta_base_predictions[0] * .10) + \\\n    + (roberta_base_predictions[1] * .10) + (roberta_base_predictions[2] * .10) + (roberta_base_predictions[3] * .10) + \\\n   (roberta_base_predictions[4] * .10)\n\n","80b46271":"result_df = pd.DataFrame(\n    {\n        'id': test.id,\n        'target': allpreds\n    })\n\nprint(result_df)\nresult_df.to_csv('submission.csv', index=False)","1cb54080":"**Solution Overview:**\n\nTrain Roberta-Base and RobertaLarge models on the contest data along with supplmemental sources similar to that data.  Fine tune the models using cross-validation folds. Inference weights all 10 models (two trained models * five fine-tuned models [five folds] per model) equally.\n\n**Notebook Sequence:**\n* [Train Roberta Base Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-train-robertabase-maskedlm-model)\n* [Train Roberta Large Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-train-robertalarge-masked-lm-model\/)\n* [Fine Tune Trained Roberta Base Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-finetune-trained-robertabase)\n* [Fine Tune Trained Roberta Large Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-finetune-trained-robertalarge)\n* [Inference Notebook  -- **This Notebook**](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-inference-robertabase-robertalarge-ensemble)"}}