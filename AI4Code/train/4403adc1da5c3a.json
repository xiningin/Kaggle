{"cell_type":{"958bfc5f":"code","7142e040":"code","70e15ff9":"code","e70dfce7":"code","aeb4c106":"code","95abb907":"code","36390fc9":"code","4e74a82b":"code","6f0620fd":"code","4e45b0f9":"code","99799b62":"code","2247041f":"code","6fe8752f":"code","a953bce3":"code","ee7a9e03":"code","51cc1163":"code","887eae0b":"code","fc4f9e2b":"code","c8bd1590":"code","c88dd962":"code","f7dd54a7":"code","6ce7d6e7":"markdown","4d9e66eb":"markdown","7607235b":"markdown","1497b5f9":"markdown","b8e9b06f":"markdown","bd207b54":"markdown","dbeb7ab5":"markdown","b01700cf":"markdown","71618e0a":"markdown"},"source":{"958bfc5f":"#load data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncalendar = pd.read_csv('..\/input\/m5-forecasting-accuracy\/calendar.csv')\nsales = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nprice = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sell_prices.csv')","7142e040":"calendar.head()","70e15ff9":"calendar.shape","e70dfce7":"sales.tail()","aeb4c106":"sales.shape","95abb907":"price.head()","36390fc9":"price.shape","4e74a82b":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \nfrom math import sqrt\nfrom numpy import concatenate\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import LSTM\n\n\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = concat(cols, axis=1)\n\tagg.columns = names\n\t#drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg\n\n\ndays = range(1, 1913 + 1)\ntime_series_columns = [f'd_{i}' for i in days]\n\ntime_series_data = sales[time_series_columns]","6f0620fd":"time_series_data.head()","4e45b0f9":"# n_features = X.shape[2]\n# # define model\n# model = Sequential()\n# model.add(LSTM(200, activation='relu', input_shape=(n_steps_in, n_features)))\n# model.add(RepeatVector(n_steps_out))\n# model.add(LSTM(200, activation='relu', return_sequences=True))\n# model.add(TimeDistributed(Dense(n_features)))\n# model.compile(optimizer='adam', loss='mse')\n# # fit model\n# model.fit(X, y, batch_size=16, epochs=300, verbose=0)\n# # demonstrate prediction\n# x_input = X[-1].reshape((1, n_steps_in, n_features))\n# yhat = model.predict(x_input, verbose=0)\n# print(yhat)","99799b62":"# # multivariate output stacked lstm example\n# from numpy import array\n# from numpy import hstack\n# from keras.models import Sequential\n# from keras.layers import LSTM\n# from keras.layers import Dense\n\n# # split a multivariate sequence into samples\n# def split_sequences(sequences, n_steps):\n# \tX, y = list(), list()\n# \tfor i in range(len(sequences)):\n# \t\t# find the end of this pattern\n# \t\tend_ix = i + n_steps\n# \t\t# check if we are beyond the dataset\n# \t\tif end_ix > len(sequences)-1:\n# \t\t\tbreak\n# \t\t# gather input and output parts of the pattern\n# \t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n# \t\tX.append(seq_x)\n# \t\ty.append(seq_y)\n# \treturn array(X), array(y)\n\n# # define input sequence\n# # for i in range(100):\n# #     'in_seqi%'%i = array(time_series_data.iloc[i, :])\n# # in_seq2 = array(time_series_data.iloc[1, :])\n# # out_seq = array(time_series_data.iloc[2, :])\n# # # convert to [rows, columns] structure\n# # # in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n# # in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n# # out_seq = out_seq.reshape((len(out_seq), 1))\n# # horizontally stack columns\n# dataset = time_series_data.iloc[:100, :]\n# dataset = array(dataset)\n# dataset = dataset.reshape(1913, 100)\n# # choose a number of time steps\n# n_steps = 28\n# # convert into input\/output\n# X, y = split_sequences(dataset, n_steps)\n# # the dataset knows the number of features, e.g. 2\n# for i in range(len(X)):\n# \tprint(X[i], y[i])","2247041f":"# n_features = X.shape[2]\n# # define model\n# model = Sequential()\n# model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n# model.add(LSTM(100, activation='relu'))\n# model.add(Dense(n_features))\n# model.compile(optimizer='adam', loss='mse')\n# # fit model\n# model.fit(X, y, epochs=400, verbose=0)\n# # demonstrate prediction\n# # x_input = array([[70,75,145], [80,85,165], [90,95,185]])\n# # x_input = x_input.reshape((1, n_steps, n_features))\n# yhat = model.predict(X[-28:].reshape((28, n_steps, n_features)), verbose=0)\n# print(yhat)","6fe8752f":"# yhat = model.predict(X[0:2].reshape((2, n_steps, n_features)), verbose=0)\n# print(yhat)","a953bce3":"len(time_series_data)","ee7a9e03":"import gc\ntimes = pd.DataFrame()\ndef model_fit(train_X, train_y, test_X, test_y):\n    model = Sequential()\n    model.add(LSTM(64, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\n    model.add(LSTM(48, return_sequences=True))\n    #model.add(LSTM(128, return_sequences=True))\n    #model.add(Dropout(0.3))\n    model.add(LSTM(32))\n    model.add(Dense(1, activation='relu'))\n    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=['mae'])\n    model.fit(train_X, train_y,batch_size=16,epochs=0,validation_data=(test_X, test_y), verbose=2, shuffle=False)\n    return model\n\ndef train():\n    for i in range(30490):\n        a = time_series_data.iloc[i, :]\n        values = np.array(a)\n        # integer encode direction\n        # ensure all data is float\n        a = values.reshape(-1,1)\n        values = a.astype('float32')\n        del a\n        # normalize features\n        scaler = MinMaxScaler(feature_range=(0, 1))\n        scaled = scaler.fit_transform(values)\n        reframed = series_to_supervised(scaled, 14, 56)\n        del scaled\n        a, b = reframed.iloc[:, :14],reframed.iloc[:, 41]\n        del reframed\n        reframed1 = pd.concat([a,b],axis=1)\n        values = reframed1.values\n        del reframed1\n        n_train_hours = 1475\n        train = values[:n_train_hours, :]\n        test = values[n_train_hours:1816, :]\n        validation = values[1816:, :]\n\n        # split into input and outputs\n        train_X, train_y = train[:, :-1], train[:, -1]\n        test_X, test_y = test[:, :-1], test[:, -1]\n        vali_X, vali_y = validation[:, :-1], validation[:, -1]\n        # reshape input to be 3D [samples, timesteps, features]\n        train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n        test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n        vali_X = vali_X.reshape((vali_X.shape[0], 1, vali_X.shape[1]))\n        # fit model\n        model = model_fit(train_X, train_y, test_X, test_y)\n        # make a prediction\n        vali_hat = model.predict(vali_X)\n        del model\n        vali_X = vali_X.reshape((vali_X.shape[0], vali_X.shape[2]))\n        # invert scaling for forecast\n        vali_yhat = concatenate((vali_hat, vali_X[:, 1:]), axis=1)\n        vali_yhat = scaler.inverse_transform(vali_yhat)\n        vali_yhat = vali_yhat[:,0]\n\n        times['row_%s'%i]=pd.Series(vali_yhat)\n        gc.collect()\n    return times\n\ntimes = train()\ntimes.to_csv('times.csv', index=False)","51cc1163":"print('Done !!')","887eae0b":"times.head()","fc4f9e2b":"times.shape","c8bd1590":"time_new = times.T\ndf1 = time_new.iloc[:, :28]\ndf2 = time_new.iloc[:, :28]\ndf2.columns = [f'F{i}' for i in range(1, df2.shape[1] + 1)]\ndf1.columns = [f'F{i}' for i in range(1, df1.shape[1] + 1)]\npre = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sample_submission.csv')\nref = pd.concat([df1,df2])\nref.shape\nref = ref.reset_index()\ndel ref['index']\n\nref.insert(0, 'id', pre['id'])\n\nnum = ref._get_numeric_data()\nnum[num < 0] = 0\nref.to_csv('submission.csv', index=False)\nref.head()","c88dd962":"ref.shape","f7dd54a7":"# ################\n# # Code from OP #\n# ################\n# import numpy as np\n# def random_sample(len_timeseries=3000):\n#     Nchoice = 600\n#     x1 = np.cos(np.arange(0,len_timeseries)\/float(1.0 + np.random.choice(Nchoice)))\n#     x2 = np.cos(np.arange(0,len_timeseries)\/float(1.0 + np.random.choice(Nchoice)))\n#     x3 = np.sin(np.arange(0,len_timeseries)\/float(1.0 + np.random.choice(Nchoice)))\n#     x4 = np.sin(np.arange(0,len_timeseries)\/float(1.0 + np.random.choice(Nchoice)))\n#     y1 = np.random.random(len_timeseries)\n#     y2 = np.random.random(len_timeseries)\n#     y3 = np.random.random(len_timeseries)\n#     for t in range(3,len_timeseries):\n#         ## the output time series depend on input as follows: \n#         y1[t] = x1[t-2] \n#         y2[t] = x2[t-1]*x3[t-2]\n#         y3[t] = x4[t-3]\n#     y = np.array([y1,y2,y3]).T\n#     X = np.array([x1,x2,x3,x4]).T\n#     return y, X\n# def generate_data(Nsequence = 1000):\n#     X_train = []\n#     y_train = []\n#     for isequence in range(Nsequence):\n#         y, X = random_sample()\n#         X_train.append(X)\n#         y_train.append(y)\n#     return np.array(X_train),np.array(y_train)\n\n# Nsequence = 100\n# prop = 0.5\n# Ntrain = int(Nsequence*prop)\n# X, y = generate_data(Nsequence)\n# X_train = X[:Ntrain,:,:]\n# X_test  = X[Ntrain:,:,:]\n# y_train = y[:Ntrain,:,:]\n# y_test  = y[Ntrain:,:,:] \n\n# #X.shape = (N sequence, length of time series, N input features)\n# #y.shape = (N sequence, length of time series, N targets)\n# print(X.shape, y.shape)\n# # (100, 3000, 4) (100, 3000, 3)\n\n# ####################\n# # Cutting function #\n# ####################\n# def stateful_cut(arr, batch_size, T_after_cut):\n#     if len(arr.shape) != 3:\n#         # N: Independent sample size,\n#         # T: Time length,\n#         # m: Dimension\n#         print(\"ERROR: please format arr as a (N, T, m) array.\")\n\n#     N = arr.shape[0]\n#     T = arr.shape[1]\n\n#     # We need T_after_cut * nb_cuts = T\n#     nb_cuts = int(T \/ T_after_cut)\n#     if nb_cuts * T_after_cut != T:\n#         print(\"ERROR: T_after_cut must divide T\")\n\n#     # We need batch_size * nb_reset = N\n#     # If nb_reset = 1, we only reset after the whole epoch, so no need to reset\n#     nb_reset = int(N \/ batch_size)\n#     if nb_reset * batch_size != N:\n#         print(\"ERROR: batch_size must divide N\")\n\n#     # Cutting (technical)\n#     cut1 = np.split(arr, nb_reset, axis=0)\n#     cut2 = [np.split(x, nb_cuts, axis=1) for x in cut1]\n#     cut3 = [np.concatenate(x) for x in cut2]\n#     cut4 = np.concatenate(cut3)\n#     return(cut4)\n\n# #############\n# # Main code #\n# #############\n# from keras.models import Sequential\n# from keras.layers import Dense, LSTM, TimeDistributed\n# import matplotlib.pyplot as plt\n# import matplotlib.patches as mpatches\n\n# ##\n# # Data\n# ##\n# N = X_train.shape[0] # size of samples\n# T = X_train.shape[1] # length of each time series\n# batch_size = N # number of time series considered together: batch_size | N\n# T_after_cut = 100 # length of each cut part of the time series: T_after_cut | T\n# dim_in = X_train.shape[2] # dimension of input time series\n# dim_out = y_train.shape[2] # dimension of output time series\n\n# inputs, outputs, inputs_test, outputs_test = \\\n#   [stateful_cut(arr, batch_size, T_after_cut) for arr in \\\n#   [X_train, y_train, X_test, y_test]]\n\n# ##\n# # Model\n# ##\n# nb_units = 10\n\n# model = Sequential()\n# model.add(LSTM(batch_input_shape=(batch_size, None, dim_in),\n#                return_sequences=True, units=nb_units, stateful=True))\n# model.add(TimeDistributed(Dense(activation='linear', units=dim_out)))\n# model.compile(loss = 'mse', optimizer = 'rmsprop')\n\n# ##\n# # Training\n# ##\n# epochs = 100\n\n# nb_reset = int(N \/ batch_size)\n# if nb_reset > 1:\n#     print(\"ERROR: We need to reset states when batch_size < N\")\n\n# # When nb_reset = 1, we do not need to reinitialize states\n# history = model.fit(inputs, outputs, epochs = epochs, \n#                     batch_size = batch_size, shuffle=False,\n#                     validation_data=(inputs_test, outputs_test))\n\n# def plotting(history):\n#     plt.plot(history.history['loss'], color = \"red\")\n#     plt.plot(history.history['val_loss'], color = \"blue\")\n#     red_patch = mpatches.Patch(color='red', label='Training')\n#     blue_patch = mpatches.Patch(color='blue', label='Test')\n#     plt.legend(handles=[red_patch, blue_patch])\n#     plt.xlabel('Epochs')\n#     plt.ylabel('MSE loss')\n#     plt.show()\n\n# plt.figure(figsize=(10,8))\n# plotting(history) # Evolution of training\/test loss\n\n# ##\n# # Visual checking for a time series\n# ##\n# ## Mime model which is stateless but containing stateful weights\n# model_stateless = Sequential()\n# model_stateless.add(LSTM(input_shape=(None, dim_in),\n#                return_sequences=True, units=nb_units))\n# model_stateless.add(TimeDistributed(Dense(activation='linear', units=dim_out)))\n# model_stateless.compile(loss = 'mse', optimizer = 'rmsprop')\n# model_stateless.set_weights(model.get_weights())\n\n# ## Prediction of a new set\n# i = 0 # time series selected (between 0 and N-1)\n# x = X_train[i]\n# y = y_train[i]\n# y_hat = model_stateless.predict(np.array([x]))[0]\n\n# for dim in range(3): # dim = 0 for y1 ; dim = 1 for y2 ; dim = 2 for y3.\n#     plt.figure(figsize=(10,8))\n#     plt.plot(range(T), y[:,dim])\n#     plt.plot(range(T), y_hat[:,dim])\n#     plt.show()\n\n# ## Conclusion: works almost perfectly.","6ce7d6e7":"   According to https:\/\/mk0mcompetitiont8ake.kinstacdn.com\/wp-content\/uploads\/2020\/02\/M5-Competitors-Guide_Final-1.pdf, the dataset involves the unit sales of **3,075** products, classified in **3 product categories** (Hobbies, Foods, and Household) and **7 product departments**, in which the above-mentioned categories are disaggregated. The products are sold across 10 stores, located in 3 States (CA, TX, and WI). ","4d9e66eb":"The historical data range from 2011-01-29 to 2016-06-19. Thus, the products have a (maximum) selling history of 1,941 days \/ 5.4 years (test data of h=28 days not included).","7607235b":"## price.csv detail\n**Contains information about the price of the products sold per store and date.**\n1. store_id: The id of the store where the product is sold.\n1. item_id: The id of the product.\n1. wm_yr_wk: The id of the week.\n1. sell_price: The price of the product for the given week\/store. The price is provided per week (average across seven days). If not available, this means that the product was not sold during the examined week. Note that although prices are constant at weekly basis, they may change through time (both training and test set). ","1497b5f9":"# Visulazation","b8e9b06f":"# LSTM time series forecasting model","bd207b54":"# Simple EDA","dbeb7ab5":"**M5 competion is a muiti-step ahead forecasting problem.**\n\nHere I will show your guys simple EDA and LSTM model","b01700cf":"## calendar.csv detail\n**Contains information about the dates the products are sold.**\n1. date: The date in a \u201cy-m-d\u201d format.\n1. wm_yr_wk: The id of the week the date belongs to.\n1. weekday: The type of the day (Saturday, Sunday, \u2026, Friday).\n1. wday: The id of the weekday, starting from Saturday.\n1. month: The month of the date.\n1. year: The year of the date.\n1. event_name_1: If the date includes an event, the name of this event.\n1. event_type_1: If the date includes an event, the type of this event.\n1. event_name_2: If the date includes a second event, the name of this event.\n1. event_type_2: If the date includes a second event, the type of this event.\n1. snap_CA, snap_TX, and snap_WI: A binary variable (0 or 1) indicating whether the stores of CA,TX or WI allow SNAP2 purchases on the examined date. 1 indicates that SNAP purchases areallowed.\n","71618e0a":"## sales_train_validation.csv detail\n**Contains the historical daily unit sales data per product and store.**\n1. item_id: The id of the product.\n1. dept_id: The id of the department the product belongs to.\n1. cat_id: The id of the category the product belongs to.\n1. store_id: The id of the store where the product is sold.\n1. state_id: The State where the store is located.\n1. d_1, d_2, \u2026, d_i, \u2026 d_1941: The number of units sold at day i, starting from 2011-01-29."}}