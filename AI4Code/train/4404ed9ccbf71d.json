{"cell_type":{"a01871aa":"code","7c146f39":"code","e85b9b65":"code","cd0fb6e2":"code","a4a8196a":"code","304679c8":"code","3b723722":"code","e368c9bb":"code","2cf685a1":"code","5f49d50e":"code","e540bc3a":"code","a94793fe":"code","88de6510":"code","0a062d70":"code","0c6c65b9":"code","3bca73d1":"code","0e783da1":"code","1b1599cf":"code","bbcb11ad":"code","3234806c":"code","5945cfcc":"code","7d23c09c":"code","204e1313":"code","db34a4a3":"code","5e8a288f":"code","13f654bc":"code","3f28ca18":"code","eb828756":"code","6f54b355":"code","e2bb32e4":"code","2f3912b0":"code","da7dccec":"code","df64dc26":"code","128616e4":"code","e6165421":"code","d0d3ea47":"code","766faf8b":"code","21c47a4e":"code","9fbe8fac":"code","e14f8bed":"code","138faca6":"code","8e61dc28":"code","c4fbe810":"code","cbe6f47b":"code","8ad9f2d4":"code","5609af70":"code","00f03477":"code","76f9bd84":"code","0493ddee":"code","bfac956d":"code","55f0db50":"code","eff7b0ba":"code","2f8c6983":"code","67cc98ca":"code","d1041474":"code","3fddd52e":"code","19e0dab6":"code","1834428e":"code","58ef24ea":"code","db2d933c":"code","7e71c5e7":"code","68ab1c9f":"code","3617259b":"code","9c2b3ebd":"code","1e953f9e":"code","28c30d39":"code","2670a796":"code","b0733760":"code","f5e03f16":"code","5e6aed1b":"code","286b65ac":"code","458ed123":"code","b3910561":"code","4d96b041":"code","ee69daed":"code","aae7908a":"code","88c903b1":"code","d87b4e51":"code","3583c81b":"code","2f7f0f03":"code","02c4e745":"code","71c78286":"code","9d39694e":"code","a3a7344e":"code","b5ea897d":"code","e6470157":"code","e86e34a7":"code","6352d2e8":"code","c379b7de":"code","e040a03c":"code","b5bb21ff":"code","685eeca3":"code","7e8a7516":"code","08f94ebe":"code","db26e76d":"code","8add6f04":"code","8cb6f542":"code","fd31ba10":"code","c8037ed1":"code","f21bb41d":"code","eb5032fd":"code","78187661":"code","9ab6f0c5":"code","4b1fcdf5":"code","b356ceaf":"code","ef68ebad":"code","661343fb":"code","3ceb5250":"code","cae4aee6":"code","1c9211ac":"markdown","25998902":"markdown","76618450":"markdown","53e15411":"markdown","4a3c1276":"markdown","489d706f":"markdown","eddc3ccf":"markdown","f71d638e":"markdown","5b043538":"markdown","c075b846":"markdown","e743b2ec":"markdown","676f24a9":"markdown","d9056de8":"markdown","b3226e41":"markdown","64a6ab15":"markdown","7368332d":"markdown","b2768e97":"markdown","c6609fcf":"markdown","01b3eb55":"markdown","405fcd93":"markdown","dab886e4":"markdown","7cce2e22":"markdown","14327b3b":"markdown","e23c3c20":"markdown","2ebbefff":"markdown","5ec9053c":"markdown","88979024":"markdown","c7f0160d":"markdown","f4527472":"markdown","ace9f753":"markdown","974790f6":"markdown","c6a01606":"markdown","3f24af65":"markdown","7d4914bc":"markdown","187c3206":"markdown","0b3c56c8":"markdown","5f426015":"markdown","4fa3e4d7":"markdown","6658ed6d":"markdown","b4d466eb":"markdown","010d8e01":"markdown","57729953":"markdown","8869d5ae":"markdown","50464237":"markdown","c435d27d":"markdown","0e8f001b":"markdown","041f05dc":"markdown","4eabbdd8":"markdown","091478d2":"markdown","c5a42dd3":"markdown","435241e2":"markdown","456ddc7d":"markdown","6d963d2e":"markdown"},"source":{"a01871aa":"import pandas as pd\nimport numpy as np\nimport os\nprint(os.listdir(\"..\/input\"))\n","7c146f39":"import matplotlib.pyplot as plt","e85b9b65":"data=pd.read_csv('..\/input\/train.csv')\ndata_pred=pd.read_csv('..\/input\/test.csv')","cd0fb6e2":"data.describe()","a4a8196a":"c=data['target'].value_counts()\nprint('% of 0 ---> ', c[0]\/(c[0]+c[1]))\nprint('% of 1 ---> ', c[1]\/(c[0]+c[1]))","304679c8":"data['target'].hist()","3b723722":"data[data.columns[2:102]].plot(kind='box', figsize=[15,4], title='Non standarized values')\ndata[data.columns[103:]].plot(kind='box', figsize=[15,4], title='Non standarized values')\n","e368c9bb":"import seaborn as sns","2cf685a1":"values=data.columns.drop(['ID_code', 'target'])\nplt.figure(figsize=(20,10))\nfor val in values:\n    sns.distplot(data[val], hist=False)\n\nplt.title('Density non Stadarized Data')\nplt.xlabel('features')\nplt.ylabel('density')\n","5f49d50e":"val_max=pd.DataFrame(data=data.max(), columns=['max'])\nval_max=val_max[2:]\nval_max['var']=val_max.index\nval_max[['var', 'max']].head()","e540bc3a":"val_max['max'].plot(kind='hist', title='Max values distribution')","a94793fe":"val_max['max'].plot(kind='box', title='Max values distribution')","88de6510":"data.kurt().head(10)\nKur_max=pd.DataFrame(data=(data.kurtosis()) , columns=['Kurtosis'])\nKur_max['var']=Kur_max.index\nKur_max.sort_values('Kurtosis', ascending=False).head()\n","0a062d70":"features=data.drop(columns=['ID_code', 'target'])","0c6c65b9":"correlations = data.corr().unstack().sort_values(ascending=True)\n","3bca73d1":"cor_abs=correlations.abs().reset_index()","0e783da1":"cor_abs=cor_abs[cor_abs['level_0']!=cor_abs['level_1']]\n","1b1599cf":"cor_abs=cor_abs.set_axis(['level_0', 'level_1', 'cor'],axis=1, inplace=False)\n","bbcb11ad":"cor_abs.tail(10)","3234806c":"corr=data.corr()\nplt.figure(figsize=(17,12))\n\nsns.heatmap(corr, cmap='coolwarm')","5945cfcc":"from sklearn.model_selection import train_test_split","7d23c09c":"train, test=train_test_split(data, test_size=0.25)","204e1313":"train.head()","db34a4a3":"x=train[train.columns[2:202]]\ny_train=train[train.columns[1:2]]","5e8a288f":"xt=test[test.columns[2:202]]\ny_test=test[test.columns[1:2]]","13f654bc":"from sklearn import preprocessing\nstd=preprocessing.StandardScaler()","3f28ca18":"x_names=x.columns","eb828756":"x_tr=std.fit_transform(x)\nx_train=pd.DataFrame(x_tr, columns=x_names)","6f54b355":"xts=std.fit_transform(xt)\nx_test=pd.DataFrame(xts, columns=x_names)","e2bb32e4":"data[data.columns[2:102]].plot(kind='box', figsize=[15,4], title='Non standarized values')\nx_train[x_train.columns[:100]].plot(kind='box', figsize=[15,4], title='Standarized values')\ndata[data.columns[103:]].plot(kind='box', figsize=[15,4], title='Non standarized values')\nx_train[x_train.columns[101:]].plot(kind='box', figsize=[15,4], title='Standarized values')","2f3912b0":"values=data.columns.drop(['ID_code', 'target'])\nplt.figure(figsize=(20,10))\nfor val in values:\n    sns.distplot(data[val], hist=False)\nplt.title('Density non Stadarized Data')\nplt.xlabel('features')\nplt.ylabel('density')\n\nplt.figure(figsize=(20,10))\nfor val in values:\n    sns.distplot(x_train[val], hist=False)\nplt.title('Density Stadarized Data')\nplt.xlabel('features')\nplt.ylabel('density')\n","da7dccec":"from sklearn.decomposition import PCA\n#import mglearn","df64dc26":"array=x_train.values\n","128616e4":"pca=PCA(n_components=3)\npca.fit(array)\nthreeD=pca.transform(array)\nthreeD\n","e6165421":"three_Df = pd.DataFrame(data = threeD, columns = ['PCA1', 'PCA2', 'PCA3']) ","d0d3ea47":"df_pca = pd.concat([three_Df, y_train], axis = 1)","766faf8b":"df_pca.head()","21c47a4e":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import animation","9fbe8fac":"\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(111, projection='3d') \nax.set_xlabel('PCA1', fontsize = 15)\nax.set_ylabel('PCA2', fontsize = 15)\nax.set_zlabel('PCA3', fontsize = 15)\nax.set_title('3 component PCA', fontsize = 20)\ntargets = [0,1]\ncolors = ['r','g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = df_pca['target'] == target\n    ax.scatter(df_pca.loc[indicesToKeep, 'PCA1']\n               , df_pca.loc[indicesToKeep, 'PCA2']\n               , df_pca.loc[indicesToKeep, 'PCA3']\n               , c = color\n               , s = 50)\n\nax.legend(targets)\nax.grid()\n\nax.view_init(azim=50)\n\n    \n","e14f8bed":"pca.explained_variance_ratio_","138faca6":"from sklearn.linear_model import Lasso","8e61dc28":"from scipy.stats import uniform","c4fbe810":"alphas=uniform.rvs(loc=0, scale=0.2, size=30)\nalphas","cbe6f47b":"alphas=[0.12301225, 0.14288355, 0.18551073, 0.05006723, 0.0333933 ,\n       0.03646111, 0.04268822, 0.10610886, 0.19878154, 0.01463984,\n       0.09548202, 0.13826288, 0.12977404, 0.06173418, 0.09480236,\n       0.15044969, 0.05521685, 0.00238981, 0.13915425, 0.15324187,\n       0.18726584, 0.0666834 , 0.01948747, 0.02757435, 0.13793408,\n       0.09817728, 0.02072232, 0.1429758 , 0.11844789, 0.04484972]","8ad9f2d4":"from sklearn.model_selection import GridSearchCV\nmodel=Lasso()\ngrid=GridSearchCV(estimator=model,param_grid=dict(alpha=alphas), cv=10)","5609af70":"grid.fit(x_train, y_train)","00f03477":"print('Best alpha--->', grid.best_params_)\nprint('Best score--->', grid.best_score_)","76f9bd84":"model_lasso=Lasso(alpha=0.00238981)\nmodel_lasso.fit(x_train, y_train)","0493ddee":"lasso_cf=list(model_lasso.coef_)\nfeature_names=x_train.columns.values.tolist()\ncoef_lasso=pd.DataFrame({'feature': feature_names, 'Coef':lasso_cf})\nfeatures_filter=coef_lasso[coef_lasso['Coef']!=0]\nfeatures_sel=features_filter['feature'].tolist()\nprint(features_sel)\nlen(features_sel)\n","bfac956d":"x=train[train.columns[2:102]]\n","55f0db50":"grid.fit(x, y_train)","eff7b0ba":"print('Best alpha--->', grid.best_params_)\nprint('Best score--->', grid.best_score_)","2f8c6983":"model_lasso=Lasso(alpha=0.00238981)\nmodel_lasso.fit(x, y_train)","67cc98ca":"lasso_cf=list(model_lasso.coef_)\nfeature_names=x.columns.values.tolist()\ncoef_lasso=pd.DataFrame({'feature': feature_names, 'Coef':lasso_cf})\nfeatures_filter=coef_lasso[coef_lasso['Coef']!=0]\nfeatures_sel=features_filter['feature'].tolist()\nprint(features_sel)\nlen(features_sel)","d1041474":"x_lasso_non=x_train[features_sel]\nx_lasso_non.head()","3fddd52e":"xtest_lasso_non=x_test[features_sel]\n","19e0dab6":"x_lasso=x_train[features_sel]\nx_lasso.columns","1834428e":"xtest_lasso=x_test[features_sel]\n","58ef24ea":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix","db2d933c":"log_reg=LogisticRegression()\nlog_reg1=LogisticRegression()\nlog_reg2=LogisticRegression()\nlog_reg3=LogisticRegression()\n","7e71c5e7":"xtot_non=train[train.columns[2:202]]\nxtot_non.head()","68ab1c9f":"log_reg.fit(xtot_non,y_train)","3617259b":"log_reg1.fit(x_train,y_train)","9c2b3ebd":"log_reg2.fit(x_lasso , y_train)","1e953f9e":"log_reg3.fit(x_lasso_non , y_train)","28c30d39":"y_pred=log_reg.predict(xt)","2670a796":"y_pred1=log_reg1.predict(x_test)","b0733760":"y_pred2=log_reg2.predict(xtest_lasso)","f5e03f16":"y_pred3=log_reg3.predict(xtest_lasso_non)","5e6aed1b":"print('score of 200 features normalized----->', log_reg1.score(x_test,y_test))\nprint('score of 200 features NO normalized----->', log_reg.score(xt,y_test))\n\nprint('score of 48 features normalizer------>', log_reg2.score(xtest_lasso,y_test))\nprint('score of 48 features NO normalizer--->', log_reg3.score(xtest_lasso_non,y_test))","286b65ac":"conf_matrix1=confusion_matrix(y_test, y_pred1)\nconf_matrix2=confusion_matrix(y_test, y_pred2)\nconf_matrix3=confusion_matrix(y_test, y_pred3)","458ed123":"tp1=conf_matrix1[0,0]+conf_matrix1[1,1]\ntp2=conf_matrix2[0,0]+conf_matrix2[1,1]\ntp3=conf_matrix3[0,0]+conf_matrix3[1,1]\nfp1=conf_matrix1[0,1]+conf_matrix1[1,0]\nfp2=conf_matrix2[0,1]+conf_matrix2[1,0]\nfp3=conf_matrix3[0,1]+conf_matrix3[1,0]","b3910561":"print('True predictions 200 features normalized---->',tp1)\nprint('True predictions 48 features normalized----->',tp2)\nprint('True predictions 48 features NO normalized-->',tp3)\n\nprint('False predictions 200 features normalized--->',fp1)\nprint('False predictions 48 features normalized---->',fp2)\nprint('False predictions 48 features NO normalized->',fp3)\n\n\n\n","4d96b041":"import tensorflow as tf\nfrom tensorflow import keras","ee69daed":"from tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import backend as k\nfrom tensorflow.keras.models import model_from_json\n\n\n\n","aae7908a":"features=xtot_non.shape[1]","88c903b1":"data=xtot_non.as_matrix()\nlab=y_train.as_matrix()\nlabel=to_categorical(lab)\ndata_test=xt.as_matrix()\nlab_test=y_test.as_matrix()\nlabel_test=to_categorical(lab_test)\n\n","d87b4e51":"model=tf.keras.Sequential()\nk.clear_session()\n\nmodel.add(layers.Dense(400, activation='relu', input_shape=(features,)))\nmodel.add(layers.Dense(400, activation='relu'))\nmodel.add(layers.Dense(200, activation='relu'))\nmodel.add(layers.Dense(100, activation='relu'))\nmodel.add(layers.Dense(2, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit(data, label, epochs=25, batch_size=512)","3583c81b":"model.evaluate(data_test, label_test, batch_size=512)","2f7f0f03":"features1=x_train.shape[1]","02c4e745":"data1=x_train.as_matrix()\nlab=y_train.as_matrix()\nlabel=to_categorical(lab)","71c78286":"data1_test=x_test.as_matrix()\nlab_test=y_test.as_matrix()\nlabel_test=to_categorical(lab_test)","9d39694e":"model1=tf.keras.Sequential()","a3a7344e":"k.clear_session()","b5ea897d":"model1.add(layers.Dense(800, activation='relu', input_shape=(features1,)))\nmodel1.add(layers.Dense(800, activation='relu'))\nmodel1.add(layers.Dense(400, activation='relu'))\nmodel1.add(layers.Dense(200, activation='relu'))\nmodel1.add(layers.Dense(2, activation='softmax'))","e6470157":"model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","e86e34a7":"model1.fit(data1,label, epochs=300, batch_size=512)","6352d2e8":"model1.evaluate(data1_test, label_test, batch_size=512)","c379b7de":"model_js=model1.to_json()\nwith open(\"model.json1\", \"w\") as json_file:\n    json_file.write(model_js)\n# serialize weights to HDF5\nmodel1.save_weights(\"model1.h5\")\nprint(\"Saved model to disk\")","e040a03c":"features2=x_lasso.shape[1]\n\ndata2=x_lasso.as_matrix()\ndata2_test=xtest_lasso.as_matrix()\n\nmodel2=tf.keras.Sequential()\nk.clear_session()\n\nmodel2.add(layers.Dense(400, activation='relu', input_shape=(features2,)))\nmodel2.add(layers.Dense(400, activation='relu'))\nmodel2.add(layers.Dense(200, activation='relu'))\nmodel2.add(layers.Dense(100, activation='relu'))\nmodel2.add(layers.Dense(2, activation='softmax'))\n\nmodel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel2.fit(data2, label, epochs=25, batch_size=512)","b5bb21ff":"model2.evaluate(data2_test, label_test, batch_size=512)","685eeca3":"model_js=model2.to_json()\nwith open(\"model.json2\", \"w\") as json_file:\n    json_file.write(model_js)\n# serialize weights to HDF5\nmodel2.save_weights(\"model2.h5\")\nprint(\"Saved model to disk\")","7e8a7516":"features3=x_lasso_non.shape[1]\n\ndata3=x_lasso_non.as_matrix()\ndata3_test=xtest_lasso_non.as_matrix()\n\nmodel3=tf.keras.Sequential()\nk.clear_session()\n\nmodel3.add(layers.Dense(400, activation='relu', input_shape=(features3,)))\nmodel3.add(layers.Dense(400, activation='relu'))\nmodel3.add(layers.Dense(200, activation='relu'))\nmodel3.add(layers.Dense(100, activation='relu'))\nmodel3.add(layers.Dense(2, activation='softmax'))\n\nmodel3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel3.fit(data3, label, epochs=25, batch_size=512)","08f94ebe":"model3.evaluate(data3_test, label_test, batch_size=512)","db26e76d":"model_js=model3.to_json()\nwith open(\"model.json3\", \"w\") as json_file:\n    json_file.write(model_js)\n# serialize weights to HDF5\nmodel3.save_weights(\"model3.h5\")\nprint(\"Saved model to disk\")","8add6f04":"from sklearn.ensemble import RandomForestClassifier as rfc\nmodel=rfc(n_jobs=2, random_state=0)\nmodel.fit(x_train, y_train)","8cb6f542":"x_train.shape","fd31ba10":"from sklearn.metrics import accuracy_score\naccuracy_score(y_train, model.predict(x_train))","c8037ed1":"y_pred=model.predict(xt)\naccuracy_score(y_test, y_pred)","f21bb41d":"\nx_pred=data_pred[data_pred.columns[1:201]]\nx_var=x_pred.columns","eb5032fd":"\nx_norm=std.fit_transform(x_pred)\nx_norm=pd.DataFrame(x_norm, columns=x_var)","78187661":"prediction=log_reg1.predict(x_norm)","9ab6f0c5":"prediction=pd.DataFrame(data=prediction , columns=['target'])\nprediction.head()","4b1fcdf5":"ID_code=[]\n\nfor i in range(len(prediction)):\n    s=str(i)\n#    t=str(prediction[i])\n    line='test_'+s\n    ID_code.append(line)\n    ","b356ceaf":"ID=pd.DataFrame(data=ID_code, columns=['ID_code'])","ef68ebad":"ID.head()","661343fb":"ID['target']=prediction.target\nID['target'].hist()","3ceb5250":"p=ID['target'].value_counts()\nprint('% of 0 ---> ', p[0]\/(p[0]+p[1]))\nprint('% of 1 ---> ', p[1]\/(p[0]+p[1]))","cae4aee6":"ID.to_csv('submission.csv', index=False)","1c9211ac":"We very can verify how the distribution of the feature approaches each other, with similar data ranges and close to a normal data distribution","25998902":"# load json and create model\njson_file = open('model.json1', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"model1.h5\")\nprint(\"Loaded model from disk\")\n ","76618450":"First I will split my data set into train and test","53e15411":"prediction2=model2.predict(data2_test, batch_size=256)","4a3c1276":"At Santander our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\n\nOur data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n\nIn this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.","489d706f":"## PCA","eddc3ccf":"x_train and x_test ----> 200 features NORMALIZED","f71d638e":"# Data Standarization","5b043538":"# Santander Customer Transaction Prediction","c075b846":"No hay valores Null en ninguna de las variables","e743b2ec":"After several attempts over different model algorithms feeding with a combination of data sets, we didn't get a relevant variation of accuracy during the valuation process. The best accuracy refers to the simplest model, a logistic regression trained wit 200 normalized features.","676f24a9":"## Multicorrelation Analytic","d9056de8":"## Forward Squential Feature Selector","b3226e41":"# EDA","64a6ab15":"# Train\/Test Spliting","7368332d":"x_arr=np.asarray(x_train)\ny_arr=np.asarray(y_train['target'])\nfeatures=x_train.columns\ny_arr","b2768e97":"Target is a binary variable that indicates if the tansaction was made or not","c6609fcf":"x_lasso_non and xtest_lasso_non ----> 87 features Non NORMALIZED following lasso reduction model of features","01b3eb55":"Box plot helps to understand the data distribution among the diferente Features, there are big data range defereences that could affect some algorithm trainings.","405fcd93":"# Final Conclusion","dab886e4":"data_pred.info(verbose=True, null_counts=True)","7cce2e22":"The most correlated pairs of values does not approach to 1, they show a relatively low correlation, therefore 'features' does not have a big multicorrelation","14327b3b":"x_lasso and xtest_lasso ----> 87 features NORMALIZED","e23c3c20":"data.shape\nlab.shape","2ebbefff":"prediction3=model3.predict(data3_test, batch_size=256)","5ec9053c":"loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","88979024":"features.head()","c7f0160d":"prediction1=model1.predict(data1_test, batch_size=256)","f4527472":"\n\nprint('best combination (ACC: %.3f): %s\\n' % (feature_selector.k_score_, feature_selector.k_feature_idx_))\nprint('all subsets:\\n', feature_selector.subsets_)\nplot_sfs(feature_selector.get_metric_dict(), kind='std_err');","ace9f753":"I tried the forward Sequential Feature Selector with an iteration from 50 to 200 features, Unfortunately, the training process took hours without concluyentes results.","974790f6":"data.info(verbose=True, null_counts=True)","c6a01606":"That was something expected, the large number of features and the non-correlation. Dimensionality reduction to 3 var does not show any independent cluster of values.","3f24af65":"## --- EDA Data Standarized vs Non Standarized","7d4914bc":"I just keep the value of alphas bolcked copy\/paste, if not each time we run uniform.rvs the alphas array will change and therefore the result for best alpha.","187c3206":"## Aplico Regresion logistica con las nuevas variables.","0b3c56c8":"Features selection is far from easy, as we demonstrated there isn't a correlation among variables.\nI tried several methods to evaluate different possibilities of featuring reduction and as we will see none of them gives concluded results.","5f426015":"Alpha coeficiente evaluation for lasso regression. The Cross Validation algorithm GridSearchCV will iterate the Lasso algorith for diferent values of Alpha. In this case GridSearch will test a randon number of alpha values ","4fa3e4d7":"# Deep Learning con Keras","6658ed6d":"# Random Forest Classifier","b4d466eb":"# Features Selection.","010d8e01":"data.head()\n","57729953":"## We have fiferent Feature data-sets to be tested","8869d5ae":"Fitting nomalized data -----> x_train, y_train, x_test","50464237":"feature_selector.fit(x_arr, y_arr, custom_feature_names=features)","c435d27d":"#### MODEL2 trained with 48 features Normalized","0e8f001b":"loaded_model.evaluate(data_test, label_test, batch_size=10)","041f05dc":"#### MODEL3 trained with 48 features NON Normalized","4eabbdd8":"model_lasso.coef_\n","091478d2":"## Lasso Regession an Features selection.","c5a42dd3":"Very poor accuracy I will not use random forest","435241e2":"from mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=4)\nfeature_selector = sfs(estimator=knn,  \n           k_features=(50,200),\n           forward=True,\n           verbose=2,\n           scoring='accuracy',\n           cv=4)","456ddc7d":"I have to remove from corr_abs those values pairs wich are equals, this twins pairs values does not offers any information their correlation is obviously 1","6d963d2e":"#### Same Lasso Algorithm but with Non normalized features."}}