{"cell_type":{"f9f4b880":"code","2bd99e27":"code","c6e5b034":"code","95e9ac0b":"code","11bec227":"code","751bad20":"code","3044307d":"code","d391fe94":"code","581a4793":"code","97584895":"code","853866df":"code","db879637":"code","825b6979":"code","0c1d8f05":"code","532756b0":"markdown","494e2295":"markdown"},"source":{"f9f4b880":"import os\nimport gc\nimport sys\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datatable as dt\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\nimport pickle\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import KFold, StratifiedKFold","2bd99e27":"folder_path = '..\/input\/jane-street-market-prediction\/'\nsample = pd.read_csv(folder_path + 'example_sample_submission.csv')\ntest_data = pd.read_csv(folder_path + 'example_test.csv')","c6e5b034":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","95e9ac0b":"features = [f'feature_{i}' for i in range(130)]\n\nconfig = {\n    \"epochs\":100,\n    \"train_batch_size\":1024,\n    \"valid_batch_size\":1024,\n    \"test_batch_size\":64,\n    \"nfolds\":5, \n    \"learning_rate\":0.0005,\n    \n    'encoder_input':len(features),\n    \"input_size1\":len(features), #raw input\n    \"input_size2\":128, #encoded input\n    'output_size':5,\n}\n\ndata_path = '..\/input\/jsmp-pytorch-bottelneck-model-train'\ntrain_data_mean = pd.read_csv(f\"{data_path}\/train_data_mean.csv\").to_numpy()","11bec227":"class GaussianNoise(nn.Module):\n    \n    def __init__(self,device,sigma=0.1, is_relative_detach=True):\n        super().__init__()\n        self.sigma = sigma\n        self.is_relative_detach = is_relative_detach\n        self.noise = torch.tensor(0,dtype=torch.float).to(device)\n\n    def forward(self, x):\n        if self.training and self.sigma != 0:\n            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n            sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n            x = x + sampled_noise\n        return x \n    \nclass Autoencoder(nn.Module):\n    def __init__(self,input_size,gaussian_noise,noise_level= 0.1):\n        super(Autoencoder,self).__init__()\n        \n        #encoder\n        self.noise_level = noise_level\n        self.gaussian_noise = gaussian_noise\n        self.layer1 = self.batch_linear(input_size,768,nn.ReLU)\n        self.layer2 = self.batch_linear(768,768,nn.ReLU)\n        self.layer3 = self.batch_linear(768,128,nn.ReLU)\n\n        #decoder\n        self.layer4 = self.batch_linear(128,768,nn.ReLU)\n        self.layer5 = self.batch_linear(768,768,nn.ReLU)\n        self.layer6 = self.batch_linear(768,input_size)\n                \n    def swap_noise(self,x):\n        batch_size = x.shape[0]\n        num_columns = x.shape[1]\n\n        random_rows = torch.randint(low = 0,high = batch_size,size=(batch_size,))\n        t = x[random_rows]\n        random_swap = torch.rand(num_columns) < self.noise_level\n        x[:,random_swap] = t[:,random_swap]\n        return x\n    \n    def batch_linear(self,inp,out,activation=None):\n        if activation:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Linear(inp,out),activation())\n        else:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Linear(inp,out))\n            \n    def forward(self,x):\n        x = self.gaussian_noise(x)\n#         x = self.swap_noise(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        \n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.layer6(x)\n        \n        return x\n    \n    def get_encoder(self,x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        \n        return x","751bad20":"class Model(nn.Module):\n    def __init__(self,input_size1,input_size2,output_size):\n        super(Model,self).__init__()\n        \n        #fc1\n        self.layer1 = self.batch_linear_drop(input_size1,256,0.3,activation=nn.ELU)\n        self.layer2 = self.batch_linear(256,128,activation= nn.ELU)\n        \n        #resblock1\n        self.layer3 = self.batch_linear_drop(input_size1+128,256,0.1,nn.ReLU)\n        \n        #resblock2\n        self.layer4 = self.batch_linear_drop(input_size1+384,256,0.1,nn.ELU)\n        self.layer5 = self.batch_linear(256,128,nn.ReLU)\n        \n        #resblock3\n        self.layer6 = self.batch_linear_drop(384,256,0.1,nn.ELU)\n        self.layer7 = self.batch_linear(256,128,nn.ReLU)\n        \n        #resblock4\n        self.layer8 = self.batch_linear_drop(512,256,0.1,nn.ELU)\n        self.layer9 = self.batch_linear(256,128,nn.ReLU)\n        \n        #resblock5\n        self.layer10 = self.batch_linear_drop(384,256,0.1,nn.ELU)\n        self.layer11 = self.batch_linear(256,128,nn.ReLU)\n                \n        #fc2\n        self.layer12 = self.batch_linear(768,256,nn.SELU)\n        self.layer13 = self.batch_linear(256,128,nn.SELU)\n        self.layer14 = nn.Sequential(nn.BatchNorm1d(128),nn.Linear(128,output_size))\n        \n    def batch_linear_drop(self,inp,out,drop,activation=None):\n        if activation:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Dropout(drop),nn.Linear(inp,out),activation())\n        else:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Dropout(drop),nn.Linear(inp,out))\n            \n    def batch_linear(self,inp,out,activation=None):\n        if activation:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Linear(inp,out),activation())\n        else:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Linear(inp,out))\n    \n    def forward(self,input1,input2):\n        #fc1\n        x1 = self.layer1(input1)\n        x1 = self.layer2(x1)\n        \n        #resblock1\n        x2 = torch.cat([input1,x1],1)\n        x2 = self.layer3(x2)\n        \n        #resblock2\n        x3 = torch.cat([input1,x1,x2],1)\n        x3 = self.layer4(x3)\n        x3 = self.layer5(x3)\n        \n        #resblock3\n        x4 = torch.cat([x2,x3],1)\n        x4 = self.layer6(x4)\n        x4 = self.layer7(x4)\n        \n        #resblock4\n        x5 = torch.cat([x2,x3,x4],1)\n        x5 = self.layer8(x5)\n        x5 = self.layer9(x5)\n        \n        #resblock5\n        x6 = torch.cat([x3,x4,x5],1)\n        x6 = self.layer10(x6)\n        x6 = self.layer11(x6)\n        \n        #fc2\n        x7 = torch.cat([x1,x2,x3,x5,x6],1)\n        x7 = self.layer12(x7)\n        x7 = self.layer13(x7)\n        x7 = self.layer14(x7)\n        \n        return x7\n\n# class Model(nn.Module):\n#     def __init__(self,input_size1,input_size2,output_size):\n#         super(Model,self).__init__()\n#         total_input_size = input_size1+input_size2\n#         hidden_size = 256\n        \n#         self.layer1 = self.batch_linear_drop(total_input_size,hidden_size,0.2,nn.LeakyReLU)\n#         self.layer2 = self.batch_linear_drop(total_input_size+hidden_size,hidden_size,0.2,nn.LeakyReLU)\n#         self.layer3 = self.batch_linear_drop(2*hidden_size,hidden_size,0.2,nn.LeakyReLU)\n#         self.layer4 = self.batch_linear_drop(2*hidden_size,hidden_size,0.2,nn.LeakyReLU)\n#         self.layer5 = self.batch_linear_drop(2*hidden_size,hidden_size,0.2,nn.LeakyReLU)\n#         self.layer6 = self.batch_linear_drop(2*hidden_size,hidden_size,0.2,nn.LeakyReLU)\n#         self.layer7 = self.batch_linear_drop(2*hidden_size,hidden_size,0.2,nn.LeakyReLU)\n#         self.layer8 = self.batch_linear(2*hidden_size,output_size)\n        \n#     def batch_linear_drop(self,inp,out,drop,activation=None):\n#         if activation:\n#             return nn.Sequential(nn.BatchNorm1d(inp),nn.Dropout(drop),nn.Linear(inp,out),activation())\n#         else:\n#             return nn.Sequential(nn.BatchNorm1d(inp),nn.Dropout(drop),nn.Linear(inp,out))\n            \n#     def batch_linear(self,inp,out,activation=None):\n#         if activation:\n#             return nn.Sequential(nn.BatchNorm1d(inp),nn.Linear(inp,out),activation())\n#         else:\n#             return nn.Sequential(nn.BatchNorm1d(inp),nn.Linear(inp,out))\n    \n#     def forward(self,input1,input2):\n#         x1 = torch.cat([input1,input2],1)\n#         x1 = self.layer1(x1)\n        \n#         x2 = torch.cat([x1,input1,input2],1)\n#         x2 = self.layer2(x2)\n        \n#         x3 = torch.cat([x2,x1],1)\n#         x3 = self.layer3(x3)\n        \n#         x4 = torch.cat([x3,x2],1)\n#         x4 = self.layer4(x4)\n        \n#         x5 = torch.cat([x4,x3],1)\n#         x5 = self.layer5(x5)\n        \n#         x6 = torch.cat([x5,x4],1)\n#         x6 = self.layer6(x6)\n        \n#         x7 = torch.cat([x6,x5],1)\n#         x7 = self.layer7(x7)\n        \n#         x8 = torch.cat([x7,x6],1)\n#         x8 = self.layer8(x8)\n        \n#         return x8","3044307d":"#loading models and scaler\ndata_path = '..\/input\/jsmp-pytorch-bottelneck-model-train'\n# scaler = pickle.load(open(f'{data_path}\/scaler.pkl','rb'))\nmodels = list()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nfor i in range(config['nfolds']):\n    model = Model(config['input_size1'],config['input_size2'],config['output_size'])\n    model.load_state_dict(torch.load(f\"{data_path}\/model{i}.bin\",map_location=device))\n    model.to(device)\n    model.eval()\n    models.append(model)\n\ngaussian_noise = GaussianNoise(device)\nencoder = Autoencoder(config['input_size1'],gaussian_noise)\nencoder.load_state_dict(torch.load(f'{data_path}\/encoder.bin',map_location=device))\nencoder.to(device)\nencoder.eval();","d391fe94":"def inference(test):\n    all_prediction = np.zeros((test.shape[0],5))\n    inputs = torch.tensor(test,dtype=torch.float)\n\n    for model in models:\n        inputs = inputs.to(device,dtype=torch.float)\n        encoder_inp = encoder.get_encoder(inputs)\n        outputs = model(inputs,encoder_inp) \n        all_prediction += outputs.sigmoid().detach().cpu().numpy()\n\n    return all_prediction\/len(models)","581a4793":"test_data = pd.read_csv(folder_path + 'example_test.csv')\ntest_data.fillna(0,inplace=True)\ntest_data = test_data[features].to_numpy()\n# test_data = scaler.transform(test_data[features].to_numpy())\npredictions = inference(test_data)\npredictions = predictions.mean(axis=1)\nsns.distplot(predictions);","97584895":"import janestreet\nenv = janestreet.make_env()\niter_test = env.iter_test()","853866df":"%%time\nall_predictions = list()\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    if test_df['weight'].item() != 0:\n        test_df.fillna(0,inplace=True)\n        predictions = inference(test_df[features].to_numpy())\n        prediction = np.mean(predictions)\n        all_predictions.append(prediction)\n        sample_prediction_df.action =  np.where(prediction >= 0.5, 1, 0).astype(int)\n    else:\n        sample_prediction_df.action = 0\n        \n    env.predict(sample_prediction_df)","db879637":"# env.predict(sample)","825b6979":"submission = pd.read_csv('.\/submission.csv')\nsubmission.head()","0c1d8f05":"plt.figure(figsize=(15,7))\nplt.subplot(121)\nsns.distplot(all_predictions)\nplt.title(\"Distplot of all the prediction\")\nplt.subplot(122)\nsns.countplot(submission.action)\nplt.title(\"Count plot of action in prediction\")\nplt.show()","532756b0":"## Importing Libraries \ud83d\udcd7","494e2295":"## JSMP: Pytorch Model- Inference\n\nThis Notebook uses pytorch to make inference using autoencoder model.<br\/>\n\nI will release training code soon."}}