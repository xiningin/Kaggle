{"cell_type":{"a1db63fb":"code","0bde7367":"code","50351c36":"code","40dadc83":"code","03a91b55":"code","3228dce4":"code","820f71d2":"code","46d3f57f":"code","830c1b7d":"code","ce82da3c":"code","0bc9601d":"code","f2cc5f35":"code","24aee0b8":"code","f937e27c":"code","ceb4fe45":"markdown"},"source":{"a1db63fb":"!pip install -q vit-keras","0bde7367":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import layers","50351c36":"from vit_keras import vit","40dadc83":"FOLD = 0\nN_SPLITS = 5","03a91b55":"class CustomDataset(tf.keras.utils.Sequence):\n    def __init__(self, df, directory, \n                 batch_size=32, \n                 random_state=1127802825, \n                 shuffle=True, target=True, ext='.npy'):\n        np.random.seed(random_state)\n        \n        self.directory = directory\n        self.df = df\n        self.shuffle = shuffle\n        self.target = target\n        self.batch_size = batch_size\n        self.ext = ext\n        \n        self.on_epoch_end()\n    \n    def __len__(self):\n        return np.ceil(self.df.shape[0] \/ self.batch_size).astype(int)\n    \n    def __getitem__(self, idx):\n        start_idx = idx * self.batch_size\n        batch = self.df[start_idx: start_idx + self.batch_size]\n        \n        signals = []\n\n        for fname in batch.id:\n            path = os.path.join(self.directory, fname + self.ext)\n            data = np.load(path)\n            signals.append(data)\n        \n        signals = np.stack(signals).astype('float32')\n        signals = tf.pad(signals, tf.constant([[0, 0], [2, 3,], [0, 0]]), \"SYMMETRIC\")\n        signals = tf.tile(tf.expand_dims(signals, axis=-1), multiples=[1,1,1,3])\n        \n        if self.target:\n            return signals, batch.target.values\n        else:\n            return signals\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)","3228dce4":"vit_model = vit.vit_b16(\n        image_size = (32, 128),\n        activation = 'softmax',\n        pretrained = True,\n        include_top = False,\n        pretrained_top = False,\n        classes = 2)","820f71d2":"def build_model():\n    inputs = layers.Input(shape=(32, 128, 3))\n\n    x = vit_model(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = layers.Dense(128, activation = tfa.activations.gelu)(x)\n    x = layers.Dense(1, activation=\"sigmoid\", name=\"sigmoid\")(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=x)\n    \n    return model","46d3f57f":"train = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/training_labels.csv')\ntrain.head()","830c1b7d":"cv = StratifiedKFold(n_splits=N_SPLITS, random_state=1127802825, shuffle=True)\ncv_splits = cv.split(X=train, y=train['target'].values)\nfor _fold, (train_idx, valid_idx) in enumerate(cv_splits):\n    if _fold == FOLD:\n        break\n\ntrain_df = train.iloc[train_idx, :]\nvalid_df = train.iloc[valid_idx, :]","ce82da3c":"train_dset = CustomDataset(\n    train_df, '..\/input\/g2net-n-mels-128-train-images', batch_size=64)\n\nvalid_dset = CustomDataset(\n    valid_df, '..\/input\/g2net-n-mels-128-train-images', batch_size=64, shuffle=False)\n\nsample = next(iter(train_dset))\nfor item in sample:\n    print(item.shape)","0bc9601d":"model = build_model()\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n              loss=\"binary_crossentropy\", \n              metrics=[tf.keras.metrics.AUC()])\nmodel.summary()","f2cc5f35":"ckpt = tf.keras.callbacks.ModelCheckpoint(\n    \"model_weights.h5\", save_best_only=True, save_weights_only=True,\n)\n\ntrain_history = model.fit(\n    train_dset, \n    epochs=8,\n    validation_data=valid_dset,\n    callbacks=[ckpt],\n    verbose=1\n)","24aee0b8":"model.load_weights('model_weights.h5')","f937e27c":"sub = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/sample_submission.csv')\n\ntest_dset = CustomDataset(\n    sub, \"..\/input\/g2net-n-mels-128-test-images\", batch_size=64, target=False, shuffle=False)\n\ny_pred = model.predict(test_dset, verbose=1)\nsub['target'] = y_pred\nsub.to_csv(f'vit_sub_{FOLD}.csv', index=False)","ceb4fe45":"# Starter using the Vision Transformer (ViT)\n\nWhat Transformer does:\n- dividing the spectrogram into patches\n- build patch embeddings\n- attention between different patches\n\nSince the default ViT needs 16x16 patches, the final images are padded...\n\nReference:\n\n- Yasufumi Nakama's (@yasufuminakama) spectrogram preprocessing notebooks and datasets:\n    * Train: [Notebook](https:\/\/www.kaggle.com\/yasufuminakama\/g2net-spectrogram-generation-train), [Dataset](https:\/\/www.kaggle.com\/yasufuminakama\/g2net-n-mels-128-train-images)\n    * Test: [Notebook](https:\/\/www.kaggle.com\/yasufuminakama\/g2net-spectrogram-generation-test), [Dataset](https:\/\/www.kaggle.com\/yasufuminakama\/g2net-n-mels-128-test-images)\n- @xhlulu 's pipeline: https:\/\/www.kaggle.com\/xhlulu\/g2net-rnn-starter-from-spectrogram"}}