{"cell_type":{"6c5d893a":"code","349af407":"code","57399c7e":"code","0841cabc":"code","eea0ceaa":"code","cb52e1c7":"code","1ca31d46":"code","e09e5922":"code","7c0d00ca":"code","b3dd53a6":"code","d37a403e":"code","a3350a10":"code","2104196f":"code","3696dabc":"code","39ce120c":"code","da40dbb6":"code","bf177bd7":"code","21a11434":"code","8438d7d1":"code","c4c2ac6b":"code","e089c053":"code","e72bcf51":"code","09b9b7f7":"code","89efcbdd":"code","9f9910f0":"code","9aa73b01":"code","1cb98f4f":"code","48aa4efd":"markdown","5a692359":"markdown","a486729a":"markdown","c023da5b":"markdown","cf99f62c":"markdown","c133779e":"markdown","7215b1a5":"markdown","60bb5761":"markdown","5bb75767":"markdown","5b934f3f":"markdown","4cbd9c3f":"markdown","726dfc0e":"markdown","5b9a5a04":"markdown","ab4e0b87":"markdown","214cb80c":"markdown","9a987b78":"markdown","dfe52503":"markdown","62c90ce3":"markdown","9ab1b658":"markdown","b11bdf2d":"markdown","a64fad72":"markdown","c9e5cb4e":"markdown","80b9e694":"markdown","935815dd":"markdown","474c1eba":"markdown"},"source":{"6c5d893a":"from google.colab import drive\ndrive.mount('\/content\/drive', force_remount=True)","349af407":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, BatchNormalization, Dropout, Flatten, LeakyReLU\nfrom tensorflow.keras.utils import to_categorical","57399c7e":"print(tf.__version__)","0841cabc":"import h5py\n\n# Open the file as read only\n# User can make changes in the path as required\nh5f = h5py.File('\/content\/drive\/MyDrive\/ColabNotebooks\/SVHN_single_grey1.h5', 'r')\n\n# Load the training and the test set\nX_train = h5f['X_train'][:]\ny_train = h5f['y_train'][:]\nX_test = h5f['X_test'][:]\ny_test = h5f['y_test'][:]\n\n\n# Close this file\nh5f.close()","eea0ceaa":"len(X_train), len(X_test)","cb52e1c7":"# visualizing the first 10 images in the dataset and their labels\nplt.figure(figsize=(10, 1))\n\nfor i in range(10):\n    plt.subplot(1, 10, i+1)\n    plt.imshow(X_train[i], cmap=\"gray\") # write the function to visualize images\n    plt.axis('off')\n\nplt.show()\nprint('label for each of the above image: %s' % (y_train[0:10]))","1ca31d46":"# Shape of the images and the first image\n\nprint(\"Shape:\", X_train[0].shape)\nprint()\nprint(\"First image:\\n\", X_train[0])","e09e5922":"# Reshaping the dataset to flatten them. Remember that we always have to give a 4D array as input to CNNs\n\nX_train = X_train.reshape(X_train.shape[0], 32,32,1)\nX_test = X_test.reshape(X_test.shape[0], 32,32,1)","7c0d00ca":"# Normalize inputs from 0-255 to 0-1\n\nX_train = X_train \/ 255.0\nX_test = X_test \/ 255.0","b3dd53a6":"# New shape \n\nprint('Training set:', X_train.shape, y_train.shape)\nprint('Test set:', X_test.shape, y_test.shape)","d37a403e":"#Write the function and appropriate variable name to one-hot encode the output\n\ny_train_encoded = tf.keras.utils.to_categorical(y_train)\ny_test_encoded = tf.keras.utils.to_categorical(y_test)\n\n#test labels\ny_test","a3350a10":"#Fixing the seed for random number generators\nnp.random.seed(42)\nimport random\nrandom.seed(42)\ntf.random.set_seed(42)","2104196f":"#Importing losses and optimizers modules\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import optimizers\n\n#Define the function\ndef cnn_model_1():\n    model = Sequential() \n    #Add layers as per the architecture mentioned above in the same sequence\n    model.add(Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\", input_shape=(32, 32, 1)))\n    model.add(LeakyReLU(0.1))\n    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\"))\n    model.add(LeakyReLU(0.1))\n    model.add(MaxPool2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(32))\n    model.add(LeakyReLU(0.1))\n    model.add(Dense(10, activation='softmax'))\n    #declare adam optimizer with learning rate of 0.001 \n    adam = optimizers.Adam(learning_rate = 0.001)\n    \n    #compile the model\n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer=adam, \n        metrics=['accuracy']\n    )\n    \n    return model","3696dabc":"#Build the model\nmodel_1 = cnn_model_1()","39ce120c":"#Print the model summary\nmodel_1.summary()","da40dbb6":"# Fit the model\nhistory_model_1 = model_1.fit(\n            X_train, y_train_encoded,\n            epochs=20,\n            validation_split=0.2,\n            batch_size = 32,\n            verbose=1)","bf177bd7":"# plotting the accuracies\n\ndict_hist = history_model_1.history\nlist_ep = [i for i in range(1,21)]\n\nplt.figure(figsize = (8,8))\nplt.plot(list_ep,dict_hist['accuracy'],ls = '--', label = 'accuracy')\nplt.plot(list_ep,dict_hist['val_accuracy'],ls = '--', label = 'val_accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show()","21a11434":"#Clearing backend\nfrom tensorflow.keras import backend\nbackend.clear_session()","8438d7d1":"#Fixing the seed for random number generators\nnp.random.seed(42)\nimport random\nrandom.seed(42)\ntf.random.set_seed(42)","c4c2ac6b":"#Define the function\ndef cnn_model_2():\n    model = Sequential() \n    #Add layers as per the architecture mentioned above in the same sequence\n    model.add(Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\", input_shape=(32, 32, 1)))\n    model.add(LeakyReLU(0.1))\n    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\"))\n    model.add(LeakyReLU(0.1))\n    model.add(MaxPool2D(pool_size=(2, 2)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\"))\n    model.add(LeakyReLU(0.1))\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\"))\n    model.add(LeakyReLU(0.1))\n    model.add(MaxPool2D(pool_size=(2, 2)))\n    model.add(BatchNormalization())\n    model.add(Flatten())\n    model.add(Dense(32))\n    model.add(LeakyReLU(0.1))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n\n    #declare adam optimizer with learning rate of 0.001 \n    adam = optimizers.Adam(learning_rate = 0.001)\n    \n    #compile the model\n    model.compile(loss = 'categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n    \n    return model","e089c053":"# Build the model\nmodel_2 = cnn_model_2()","e72bcf51":"#Print the summary\nmodel_2.summary()","09b9b7f7":"# Fit the model\nhistory_model_2 = model_2.fit(\n            X_train, y_train_encoded,\n            epochs=30,\n            validation_split=0.2,\n            batch_size = 128,\n            verbose=1)","89efcbdd":"# plotting the accuracies\n\ndict_hist = history_model_2.history\nlist_ep = [i for i in range(1,31)]\n\nplt.figure(figsize = (8,8))\nplt.plot(list_ep,dict_hist['accuracy'],ls = '--', label = 'accuracy')\nplt.plot(list_ep,dict_hist['val_accuracy'],ls = '--', label = 'val_accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show()","9f9910f0":"#Make prediction on the test data using model_2 \ntest_pred = model_2.predict(X_test)\n\ntest_pred = np.argmax(test_pred, axis=-1)","9aa73b01":"#Converting each entry to single label from one-hot encoded vector\ny_test = np.argmax(y_test, axis=-1)","1cb98f4f":"#importing required functions\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n#Printing the classification report\nprint(classification_report(y_test, test_pred))\n\n#Plotting the heatmap using confusion matrix\ncm = confusion_matrix(y_test, test_pred)\nplt.figure(figsize=(8,5))\nsns.heatmap(cm, annot=True,  fmt='.0f')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","48aa4efd":"## **Importing libraries**","5a692359":"## **Predictions on the test data**\n\n- Make predictions on the test set using the second model\n- Print the obtained results using classification report and the confusion matrix\n- Final observations from the obtained results","a486729a":"#### Use One-hot encode the labels in the target variable y_train and y_test","c023da5b":"Let's check the number of images in the training and testing data.","cf99f62c":"Let's build another model and see if we can get a better model with generalized performance.\n\nFirst, we need to clear the previous model's history from the keras backend. Also, let's fix the seed again after clearing the backend.","c133779e":"**Observations:________**\n- The second model with dropout layers seems to have reduced the overfitting as compared to the previous model but still the validation data ccuracy is around 5% lower than train data accuracy.\n- The overal accuracy of this model is better than the 1st model.\n- The Training accuracy is increasing with the increase in epochs. It has a rapid increase up to around 5 epochs and then seems to have very lower increase.\n- The validation accuracy is bit fluctuating but overall it is also increasing with the increase in epochs. Same as training accuracy, validation accuracy also has a rapid increase up to around 2 epochs and then has lower increase, almost like a constant.\n- The model is giving around ~90% accuracy at 30 epochs. At 30 epochs, the accuracy of the model on the training data is about 94% and the validation accuracy is ~90%.\n- The validation accuracy and train accuracy still has a about 5% difference. ","7215b1a5":"## **Data Preparation**\n\n- Print the first image in the train image and figure out the shape of the images\n- Reshape the train and the test dataset to make them fit the first convolutional operation that we will create later. Figure out the required shape\n- Normalize the train and the test dataset by dividing by 255\n- Print the new shapes of the train and the test set\n- One-hot encode the target variable","60bb5761":"- Notice that each entry of y_test is a one-hot encoded vector instead of a single label.","5bb75767":"#### Build and train a CNN model as per the above mentioned architecture **","5b934f3f":"### **Model Architecture**\n- Write a function that returns a sequential model with the following architecture\n - First Convolutional layer with **16 filters and kernel size of 3x3**. Use the **'same' padding** and provide an **apt input shape**\n - Add a **LeakyRelu layer** with the **slope equal to 0.1**\n - Second Convolutional layer with **32 filters and kernel size of 3x3 with 'same' padding**\n - Another **LeakyRelu** with the **slope equal to 0.1**\n - A **max-pooling layer** with a **pool size of 2x2**\n - **Flatten** the output from the previous layer\n - Add a **dense layer with 32 nodes**\n - Add a **LeakyRelu layer with slope equal to 0.1**\n - Add the final **output layer with nodes equal to the number of classes** and **softmax activation**\n - Compile the model with the **categorical_crossentropy loss, adam optimizers (learning_rate = 0.001), and accuracy metric**. Do not fit the model here, just return the compiled model\n- Call the function and store the model in a new variable\n- Print the summary of the model.\n- Fit the model on the train data with a **validation split of 0.2, batch size = 32, verbose = 1, and 20 epochs**. Store the model building history to use later for visualization.","4cbd9c3f":"- There are 42,000 images in the training data and 18,000 images in the testing data. ","726dfc0e":"Let us check for the version of TensorFlow.","5b9a5a04":"## **Model Building**\n\nNow that we have done data preprocessing, let's build a CNN model.","ab4e0b87":"**Note:** Earlier, we noticed that each entry of the test data is a one-hot encoded vector but to print the classification report and confusion matrix, we must convert each entry of y_test to a single label.","214cb80c":"#### Final observations on the performance of the model on the test data ","9a987b78":"**Observations:**\n- We can see from the above plot that the model has done poorly on the validation data. The model is overfitting the training data. \n- The validation accuracy has become more or less constant after 2 epochs.\n- The incresing rate of accuracy is lower afte 2 epochs.","dfe52503":"### **Plotting the validation and training accuracies**","62c90ce3":"## **Mount the drive**\nLet us start by mounting the drive and importing the necessary libraries.","9ab1b658":"### **Plotting the validation and training accuracies**","b11bdf2d":"#### **Build and train the second CNN model as per the above mentioned architecture **","a64fad72":"#### **Observations:**\n- The model is giving about **91% accuracy on the test data** which is **comparable to the accuracy of the validation data.** This implies that the model is giving a generalized performance.\n- The **recall has a very high range** (88-94)% which implies that the **model is good at identifying most of the objects.** Model is able to identify about 94% of image 0 but can only identify only ~88% of image 3 and 8**.\n- Here also lowest image classes 0and 8 has the lowesta acuracy which means the model needs improvements in order to distinguish between 3 and 8.\n- Overall the model could distiguish individual digits well.\n- Also, considering the overall accuracy and recall values, we can say that CNN works better than ANN when identifying digits in the images.","c9e5cb4e":"## **Visualizing images**\n- Use X_train to visualize the first 10 images\n- Use Y_train to print the first 10 labels","80b9e694":"## **Load the dataset**\n- Let us now load the dataset that is available as a .h5 file.\n- Split the data into train and the test dataset","935815dd":"# **Project - Convolutional Neural Networks: Street View Housing Number Digit Recognition**\n\nOne of the most interesting tasks in deep learning is to recognize objects in natural scenes. The ability to process visual information using machine learning algorithms can be very useful as demonstrated in various applications.\n\nThe SVHN dataset contains over 600,000 labeled digits cropped from street level photos. It is one of the most popular image recognition datasets. It has been used in neural networks created by Google to improve map quality by automatically transcribing the address numbers from a patch of pixels. The transcribed number with a known street address helps pinpoint the location of the building it represents. \n\n----------------\n### **Objective:**\n----------------\n\nBuild a CNN model that can identify the digits in the images.\n\n-------------\n### **Dataset**\n-------------\nHere, we will use a subset of the original data to save some computation time. The dataset is provided as a .h5 file. The basic preprocessing steps have been done.","474c1eba":"### **Second Model Architecture**\n- Write a function that returns a sequential model with the following architecture\n - First Convolutional layer with **16 filters and kernel size of 3x3**. Use the **'same' padding** and provide an **apt input shape**\n - Add a **LeakyRelu layer** with the **slope equal to 0.1**\n - Second Convolutional layer with **32 filters and kernel size of 3x3 with 'same' padding**\n - Add **LeakyRelu** with the **slope equal to 0.1**\n - Add a **max-pooling layer** with a **pool size of 2x2**\n - Add a **BatchNormalization layer**\n - Third Convolutional layer with **32 filters and kernel size of 3x3 with 'same' padding**\n - Add a **LeakyRelu layer with slope equal to 0.1**\n - Fourth Convolutional layer **64 filters and kernel size of 3x3 with 'same' padding** \n - Add a **LeakyRelu layer with slope equal to 0.1**\n - Add a **max-pooling layer** with a **pool size of 2x2**\n - Add a **BatchNormalization layer**\n - **Flatten** the output from the previous layer\n - Add a **dense layer with 32 nodes**\n - Add a **LeakyRelu layer with slope equal to 0.1**\n - Add a **dropout layer with rate equal to 0.5**\n - Add the final **output layer with nodes equal to the number of classes** and **softmax activation**\n - Compile the model with the **categorical_crossentropy loss, adam optimizers (learning_rate = 0.001), and accuracy metric**. Do not fit the model here, just return the compiled model\n- Call the function and store the model in a new variable\n- Print the summary of the model.\n- Fit the model on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and 30 epochs**. Store the model building history to use later for visualization."}}