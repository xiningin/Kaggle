{"cell_type":{"15f80f14":"code","8e87cb91":"code","72b566e9":"code","7c55cba8":"code","ed27cec6":"code","e3f5fe1e":"code","3e48bd03":"code","ff55eb50":"code","3c9fb511":"code","b69ff53a":"code","f36c3032":"code","2f697f24":"code","ff097fbe":"code","ce0137e3":"code","2ab8f53d":"code","781f1051":"code","a403c213":"code","9923fe2f":"code","a40ecb7c":"code","7c9435b1":"code","5dee1a15":"code","56494600":"code","ce6ba2a3":"code","6eb0e345":"code","505324d9":"code","bed33df7":"code","0ecd75aa":"code","4eb571c3":"code","b40a2df9":"code","15d2bee9":"code","29339fdc":"code","9faba97c":"code","f62053d0":"code","f7183579":"code","af20fa29":"code","d41ffe42":"code","f78f6949":"code","e305d7b0":"code","fc99b96d":"markdown","23bd6ab5":"markdown","74b9f01a":"markdown","91c7fa9a":"markdown","83311ee3":"markdown","36679f20":"markdown","03492c88":"markdown","a9789713":"markdown","cd1d30a9":"markdown","887d2b80":"markdown","2ed77b96":"markdown","d79a8936":"markdown","572fa908":"markdown","df1156f2":"markdown","3c00abf9":"markdown","70ca08a4":"markdown","41aa9de7":"markdown","7ed2a307":"markdown","e00f1798":"markdown","774edad8":"markdown","5510ba93":"markdown","a55caafa":"markdown","6d93bfbc":"markdown","49356053":"markdown","c9265d2f":"markdown","416f2dfb":"markdown","ad8b1633":"markdown","cf831d63":"markdown","e243b88a":"markdown","4880ccbe":"markdown","8b8e8dcf":"markdown"},"source":{"15f80f14":"import warnings\nwarnings.filterwarnings('ignore')","8e87cb91":"#importing libraries\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,classification_report\nfrom sklearn import tree\nimport graphviz\nimport random \nrandom.seed(3)","72b566e9":"heart_data = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\nheart_data.head()","7c55cba8":"X_train = heart_data.drop('target',axis = 1)\ny_train = heart_data['target']\nX_train,X_test,y_train,y_test = train_test_split(X_train,y_train,random_state = 3,test_size = 0.2)\nclf_randomForest = RandomForestClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(X_train,y_train)","ed27cec6":"print(accuracy_score(y_test,clf_randomForest.predict(X_test)))\nprint(classification_report(y_test,clf_randomForest.predict(X_test)))","e3f5fe1e":"from IPython.display import Image\nfrom subprocess import call\ntree_graph = tree.export_graphviz(clf_randomForest.estimators_[0], out_file='tree.dot', feature_names=X_train.columns.tolist(),proportion = True,rounded = True,filled = True,precision = 2)\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\nImage(filename = 'tree.png')","3e48bd03":"#permutation importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(clf_randomForest, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","ff55eb50":"#partial dependence plot\n\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=clf_randomForest, dataset=X_test, model_features=X_test.columns.tolist(), feature='thalach')\n# plot it\npdp.pdp_plot(pdp_goals, 'thalach')\nplt.show()","3c9fb511":"#partial dependence plot\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=clf_randomForest, dataset=X_test, model_features=X_test.columns.tolist(), feature='ca')\n# plot it\npdp.pdp_plot(pdp_goals, 'ca')\nplt.show()","b69ff53a":"#partial dependence plot\nfeatures_to_plot = ['ca', 'thalach']\ninter1  =  pdp.pdp_interact(model=clf_randomForest, dataset=X_test, model_features=X_test.columns.tolist(), features=features_to_plot)\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","f36c3032":"#shap\nrow_to_show = 4\ndata_for_prediction = X_test.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\nclf_randomForest.predict_proba(data_for_prediction_array)","2f697f24":"import shap  # package used to calculate Shap values\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(clf_randomForest)\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)","ff097fbe":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","ce0137e3":"#Dependence Contribution Plots\nshap_values = explainer.shap_values(X_train)\nshap.dependence_plot('thalach',shap_values[1], X_train, interaction_index=\"ca\")","2ab8f53d":"#summary plot\nshap_values = explainer.shap_values(X_train)\nshap.summary_plot(shap_values[1], X_train,auto_size_plot=False)","781f1051":"\nshap.force_plot(explainer.expected_value[0],shap_values[0] , X_train)","a403c213":"import lime\nimport lime.lime_tabular\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train.astype(int).values,mode='classification',training_labels=y_train,feature_names=X_train.columns.tolist(),class_names=['true','false'])\n#Let's take a look for the 100th row\ni = 1\nexp = explainer.explain_instance(X_train.loc[i,X_train.columns.tolist()].astype(int).values, clf_randomForest.predict_proba, num_features=13)","9923fe2f":"exp.show_in_notebook(show_table=True)","a40ecb7c":"from lime import submodular_pick\n# SP-LIME returns exaplanations on a sample set to provide a non redundant global decision boundary of original model\nsp_obj = submodular_pick.SubmodularPick(explainer, X_train.values, clf_randomForest.predict_proba, num_features=13,num_exps_desired=3)\n[exp.show_in_notebook() for exp in sp_obj.sp_explanations]","7c9435b1":"import os\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\nfrom keras.models import load_model\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras import models\nfrom keras.applications import VGG16\nfrom keras import backend as K\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nimport cv2","5dee1a15":"\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","56494600":"#Visualize Intermediate activation \n\nmodel.load_weights('..\/input\/catndog\/model.h5')\nimg_path = '..\/input\/cnn-image\/dog.jpeg'\n\n#Preprocesses the image into a 4D tensor\nimg = image.load_img(img_path, target_size=(128, 128))\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\nimg_tensor \/= 255.\n\n\nplt.imshow(img_tensor[0])\nplt.show()","ce6ba2a3":"#Visualize Intermediate activation \n\nlayer_outputs = [layer.output for layer in model.layers[:8]]\nactivation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n\nactivations = activation_model.predict(img_tensor)\n\nfirst_layer_activation = activations[0]\nprint(first_layer_activation.shape)","6eb0e345":"#Visualize Intermediate activation\nplt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')","505324d9":"#Visualize Intermediate activation\nplt.matshow(first_layer_activation[0, :, :, 7], cmap='viridis')","bed33df7":"#Visualize Intermediate activation\n\nlayer_names = []\nfor layer in model.layers[:8]:\n    layer_names.append(layer.name)\n\nimages_per_row = 16\n\n# Now let's display our feature maps\nfor layer_name, layer_activation in zip(layer_names, activations):\n    # This is the number of features in the feature map\n    n_features = layer_activation.shape[-1]\n\n    # The feature map has shape (1, size, size, n_features)\n    size = layer_activation.shape[1]\n\n    # We will tile the activation channels in this matrix\n    n_cols = n_features \/\/ images_per_row\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n    # We'll tile each filter into this big horizontal grid\n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,:, :,col * images_per_row + row]\n            # Post-process the feature to make it visually palatable\n            channel_image -= channel_image.mean()\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size,row * size : (row + 1) * size] = channel_image\n\n    # Display the grid\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n    \nplt.show()\n","0ecd75aa":"model = VGG16(weights='imagenet',include_top=False)\nlayer_name = 'block3_conv1'\nfilter_index = 0\nlayer_output = model.get_layer(layer_name).output\nloss = K.mean(layer_output[:, :, :, filter_index])","4eb571c3":"# The call to `gradients` returns a list of tensors (of size 1 in this case)\n# hence we only keep the first element -- which is a tensor.\ngrads = K.gradients(loss, model.input)[0]","b40a2df9":"# We add 1e-5 before dividing so as to avoid accidentally dividing by 0.\ngrads \/= (K.sqrt(K.mean(K.square(grads))) + 1e-5)","15d2bee9":"iterate = K.function([model.input], [loss, grads])\nloss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))])","29339fdc":"# We start from a gray image with some noise\ninput_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128.\n# Run gradient ascent for 40 steps\nstep = 1.  # this is the magnitude of each gradient update\nfor i in range(40):\n    # Compute the loss value and gradient value\n    loss_value, grads_value = iterate([input_img_data])\n    # Here we adjust the input image in the direction that maximizes the loss\n    input_img_data += grads_value * step","9faba97c":"def deprocess_image(x):\n    # normalize tensor: center on 0., ensure std is 0.1\n    x -= x.mean()\n    x \/= (x.std() + 1e-5)\n    x *= 0.1\n\n    # clip to [0, 1]\n    x += 0.5\n    x = np.clip(x, 0, 1)\n\n    # convert to RGB array\n    x *= 255\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","f62053d0":"def generate_pattern(layer_name, filter_index, size=150):\n    # Build a loss function that maximizes the activation\n    # of the nth filter of the layer considered.\n    layer_output = model.get_layer(layer_name).output\n    loss = K.mean(layer_output[:, :, :, filter_index])\n    # Compute the gradient of the input picture wrt this loss\n    grads = K.gradients(loss, model.input)[0]\n    # Normalization trick: we normalize the gradient\n    grads \/= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n    # This function returns the loss and grads given the input picture\n    iterate = K.function([model.input], [loss, grads])\n    # We start from a gray image with some noise\n    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n    # Run gradient ascent for 40 steps\n    step = 1.\n    for i in range(40):\n        loss_value, grads_value = iterate([input_img_data])\n        input_img_data += grads_value * step\n        \n    img = input_img_data[0]\n    return deprocess_image(img)","f7183579":"plt.imshow(generate_pattern('block3_conv1', 1))\nplt.show()","af20fa29":"for layer_name in ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1']:\n    size = 64\n    margin = 5\n\n    # This a empty (black) image where we will store our results.\n    results = np.zeros((8 * size + 7 * margin, 8 * size + 7 * margin, 3))\n\n    for i in range(8):  # iterate over the rows of our results grid\n        for j in range(8):  # iterate over the columns of our results grid\n            # Generate the pattern for filter `i + (j * 8)` in `layer_name`\n            filter_img = generate_pattern(layer_name, i + (j * 8), size=size)\n\n            # Put the result in the square `(i, j)` of the results grid\n            horizontal_start = i * size + i * margin\n            horizontal_end = horizontal_start + size\n            vertical_start = j * size + j * margin\n            vertical_end = vertical_start + size\n            results[horizontal_start: horizontal_end, vertical_start: vertical_end, :] = filter_img\n\n    # Display the results grid\n    plt.figure(figsize=(20, 20))\n    plt.imshow(np.array(results,np.int32))\n    plt.show()","d41ffe42":"#heatmaps of class activation\nK.clear_session()\nmodel = VGG16(weights='imagenet')","f78f6949":"#heatmaps of class activation\nimg_path = '..\/input\/cnn-image\/elephant.jpeg'\n# `img` is a PIL image of size 224x224\nimg = image.load_img(img_path, target_size=(224, 224))\n# `x` is a float32 Numpy array of shape (224, 224, 3)\nx = image.img_to_array(img)\n# We add a dimension to transform our array into a \"batch\"\n# of size (1, 224, 224, 3)\nx = np.expand_dims(x, axis=0)\n# Finally we preprocess the batch\n# (this does channel-wise color normalization)\nx = preprocess_input(x)\npreds = model.predict(x)\nprint('Predicted:', decode_predictions(preds, top=3)[0])\nnp.argmax(preds[0])\n# This is the \"african elephant\" entry in the prediction vector\nafrican_elephant_output = model.output[:, 386]\n# The is the output feature map of the `block5_conv3` layer,\n# the last convolutional layer in VGG16\nlast_conv_layer = model.get_layer('block5_conv3')\n# This is the gradient of the \"african elephant\" class with regard to\n# the output feature map of `block5_conv3`\ngrads = K.gradients(african_elephant_output, last_conv_layer.output)[0]\n# This is a vector of shape (512,), where each entry\n# is the mean intensity of the gradient over a specific feature map channel\npooled_grads = K.mean(grads, axis=(0, 1, 2))\n# This function allows us to access the values of the quantities we just defined:\n# `pooled_grads` and the output feature map of `block5_conv3`,\n# given a sample image\niterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n# These are the values of these two quantities, as Numpy arrays,\n# given our sample image of two elephants\npooled_grads_value, conv_layer_output_value = iterate([x])\n# We multiply each channel in the feature map array\n# by \"how important this channel is\" with regard to the elephant class\nfor i in range(512):\n    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n# The channel-wise mean of the resulting feature map\n# is our heatmap of class activation\nheatmap = np.mean(conv_layer_output_value, axis=-1)\nheatmap = np.maximum(heatmap, 0)\nheatmap \/= np.max(heatmap)\nplt.matshow(heatmap)\nplt.show()","e305d7b0":"# We use cv2 to load the original image\nimg = cv2.imread(img_path)\nplt.imshow(img)\nplt.show()\n# We resize the heatmap to have the same size as the original image\nheatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n# We convert the heatmap to RGB\nheatmap = np.uint8(255 * heatmap)\n# We apply the heatmap to the original image\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n# 0.6 here is a heatmap intensity factor\nsuperimposed_img = heatmap * 0.6 + img\nplt.imshow(np.array(superimposed_img,np.int32))\nplt.show()\n\n\n\n","fc99b96d":"<h1>Why this dataset ?<\/h1>   \n\nI selected this [**Heart Disease UCI**](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci) dataset because it is easy to relate why we need to look inside a model or need to explain our model's working. Here if our model predicts that a person is having heart disease then we should explain why. To know that \"why\", we need to look inside our model and get to know on what factor it is giving that result. Now as we are clear that model explainability is important, let's dive into it.","23bd6ab5":"<h1>LIME<\/h1>  \nLIME is model-agnostic, meaning that it can be applied to any machine learning model. The technique attempts to understand the model by perturbing the input of data samples and understanding how the predictions change. LIME provides local model interpretability. LIME modifies a single data sample by tweaking the feature values and observes the resulting impact on the output. Often, this is also related to what humans are interested in when observing the output of a model. You can read more [here](https:\/\/towardsdatascience.com\/understanding-model-predictions-with-lime-a582fdff3a3b).\n\n<h2>Advantages<\/h2>\n* Even if you replace the underlying machine learning model, you can still use the same local, interpretable model for explanation.   \n* LIME is one of the few methods that works for tabular data, text and images.    \n* LIME is implemented in Python (lime and Skater) and R (lime package and iml package) and is very easy to use.   \n* The explanations created with local surrogate models can use other features than the original model. * This can be a big advantage over other methods, especially if the original features cannot bet interpreted. \n\n<h2>Disadvantages<\/h2>  \n* The instability of the explanations.Instability means that it is difficult to trust the explanations, and you should be very critical.\n* For each application you have to try different kernel settings and see for yourself if the explanations make sense.   \n* The complexity of the explanation model has to be defined in advance. \n","74b9f01a":"Let's draw 2D Partial dependence plot and check interactions between **ca** and **thalach**.","91c7fa9a":"We can see that having major vessels (ca) =  0 and maximum heart rate (thalach) above 150 increase a person's chance of having heart disease very much. Whereas having 2 major vessels and maximum heart rate less 110 is the most safe.","83311ee3":"<h2>Content that helped me to put together this kernel aka References :)<\/h2>\n\nhttps:\/\/www.oreilly.com\/library\/view\/deep-learning-with\/9781617294433\/     \nhttps:\/\/christophm.github.io\/interpretable-ml-book\/     \nhttps:\/\/www.kaggle.com\/learn\/machine-learning-explainability       \nAnd Our dear friend [Google](https:\/\/www.google.com\/) :)\n","36679f20":"<h1>Partial Dependence Plot<\/h1>   \n\nIn short :-    \n* Used to check *how* a feature affects the model.\n* It is also done after training the model and validation data should be used.\n\n<h2>Process<\/h2> \n1. We will aim one feature at a time.\n1. Select a row of features.\n1. First slightly decrease the value of the feature and note the accuracy of that row.\n1. Then slightly increase the value of that feature and again note the accuracy of that row.\n1. Now we do the same above mentioned steps but on multiple rows and average of them is taken.\n1. Then we will plot the average predicted outcome on vertical axis.\n\n<h2>Advantages<\/h2>    \n\n* The computation of partial dependence plots is intuitive: The partial dependence function at a particular feature value represents the average prediction if we force all data points to assume that feature value. In my experience, lay people usually understand the idea of PDPs quickly.\n\n* In the uncorrelated case, the interpretation is clear: The partial dependence plot shows how the average prediction in your dataset changes when the j-th feature is changed.\n\n* Partial dependence plots are easy to implement.\n\n<h2>Disadvantages<\/h2>\n\n* The realistic maximum number of features in a partial dependence function is two. This is not the fault of PDPs, but of the 2-dimensional representation (paper or screen) and also of our inability to imagine more than 3 dimensions.    \n* The assumption of independence is the biggest issue with PD plots. It is assumed that the feature(s) for which the partial dependence is computed are not correlated with other features.\n\n* Some PD plots do not show the feature distribution. Omitting the distribution can be misleading, because you might overinterpret regions with almost no data. \n\n\n\n","03492c88":"<h2>Visualizing intermediate convnet outputs (intermediate activations)<\/h2>   \n\nVisualizing intermediate activations consists of displaying the feature maps that are output by various convolution and pooling layers in a network, given a certain input. This gives a view into how an input is decomposed into the different filters learned by the network.","a9789713":"<h1>Permutation Importance<\/h1>   \n\nKeeping it short :-\n* Used to check importance of a feature to a model in predicting target value.\n* It is done after training the model and validation data should be used.       \n\n<h2>Steps<\/h2>\n1. Check accuracy of model on validation data.\n2. Randomly shuffle a feature and put it back.\n3. Now again check accuracy of the model on that data.\n4. Compare it with the accuracy before shuffling.\n    * Decrease in accuracy will show importance of a feature.\n5. Now undo the shuffling of that feature and put it back. \n6. Repeat above process for every feature\n\n<h2>Advantages<\/h2>   \n* It does not require retraining  the model.\n* It automatically takes into account all interactions with other features. By permuting the feature you also destroy the interaction effects with other features. This means that the permutation feature importance takes into account both the main feature effect and the interaction effects on model performance.\n* It provides highly compressed, global insight into the model\u2019s behavior.\n\n<h2>Disadvantages<\/h2>   \n* It is linked to the error of the model. Therefore it is not useful when you want to check how robust is you model's output when you manipulate a feature. At that time you will not be interested in how much      model performance decreases when you permute a feature.\n* You need access to the true outcome. If someone only provides you with the model and unlabeled data but not the true outcome you cannot compute the permutation feature importance.\n\n* The permutation of features produces unlikely data instances when two or more features are correlated. When they are positively correlated (like height and weight of a person) and we shuffle one of the features, we create new instances that are unlikely or even physically impossible (2 meter person weighing 30 kg for example), yet I use these new instances to measure the importance. Therefore, If features are correlated, the permutation feature importance can be biased by unrealistic data instances.\n\n* It depends on shuffling the feature, which adds randomness to the measurement. When the permutation is repeated, the results might vary greatly. Repeating the permutation and averaging the importance measures over repetitions stabilizes the measure, but increases the time of computation.\n\n\n\n\n\n","cd1d30a9":"Values towards top are most important and here it show that **thalach** (person's maximum heart rate achieved) and **ca** (number of magor vessels) are the two most important features. Go through [this](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4468223\/), they have explained it well and our result also seems reasonable according to that.    \n\n<h2>How to interpret the values :-<\/h2>  \nThe first number in each row shows how much model performance decreased with a random shuffling and there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the \u00b1 measures how performance varied from one-reshuffling to the next. There are also some negative values so what about them. When result on shuffle data happen to be more accurate than the real data , we encounter negative values. This usually happens with small dataset like our dataset.\n","887d2b80":"<h2>Different Filters learned by Model<\/h2>","2ed77b96":"There are various techniques for interpretation but we will discuss following :- \n\n* <h3>Visualizing intermediate convnet outputs (intermediate activations)<\/h3>\u2014Useful for understanding how successive convnet layers transform their input, and for getting a first idea of the meaning of individual convnet filters.\n* <h3>Visualizing convnets filters<\/h3>\u2014Useful for understanding precisely what visual pattern or concept each filter in a convnet is receptive to.\n* <h3>Visualizing heatmaps of class activation in an image<\/h3>\u2014Useful for understanding which parts of an image were identified as belonging to a given class, thus allowing you to localize objects in images.     \nMore you can read in this [book](https:\/\/www.oreilly.com\/library\/view\/deep-learning-with\/9781617294433\/)","d79a8936":"<h1>SHAP<\/h1>   \n\nUsed to see impact of each feature on prediction. To know what made a model to predict a certain value and which feature is contributing how much in that decision.\n\n<h2>Advantages<\/h2>   \n* The difference between the prediction and the average prediction is fairly distributed among the feature values of the instance\n* The Shapley value allows contrastive explanations. Instead of comparing a prediction to the average prediction of the entire dataset, you could compare it to a subset or even to a single data point.\n* The Shapley value is the only explanation method with a solid theory. The axioms \u2013 efficiency, symmetry, dummy, additivity \u2013 give the explanation a reasonable foundation.\n\n<h2>Disadvantages<\/h2>       \n* The Shapley value requires a lot of computing time. In 99.9% of real-world problems, only the approximate solution is feasible. An exact computation of the Shapley value is computationally expensive because there are 2k possible coalitions of the feature values and the \u201cabsence\u201d of a feature has to be simulated by drawing random instances, which increases the variance for the estimate of the Shapley values estimation.\n* The Shapley value returns a simple value per feature, but no prediction model like LIME. This means it cannot be used to make statements about changes in prediction for changes in the input, such as: \u201cIf I were to earn \u20ac300 more a year, my credit score would increase by 5 points.\u201d  \nthe \n* Shapley value method suffers from inclusion of unrealistic data instances when features are correlated.\n\n\n","572fa908":"<h1>About our dataset :- <\/h1>\n\n* **age:** The person's age in years    \n* **sex:** The person's sex (1 = male, 0 = female)\n* **cp:** The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n* **trestbps:** The person's resting blood pressure (mm Hg on admission to the hospital)\n* **chol:** The person's cholesterol measurement in mg\/dl\n* **fbs:** The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n* **restecg:** Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n*  **thalach:** The person's maximum heart rate achieved\n*  **exang:** Exercise induced angina (1 = yes; 0 = no)\n*  **oldpeak:** ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot.)\n*  **slope:** the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n*  **ca:** The number of major vessels (0-3)\n*  **thal:** A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n*  **target:** Heart disease (0 = no, 1 = yes)\n\n[Source](https:\/\/www.kaggle.com\/tentotheminus9\/what-causes-heart-disease-explaining-the-model)\n","df1156f2":"<h2>Dependence Contribution Plots<\/h2>","3c00abf9":"This visualisation technique answers two important questions:       \n    Why did the network think this image contained an African elephant?     \n    Where is the African elephant located in the picture?     \n    \nwe can see that model identify shape of the elephant and it is more focused on the ears of the elephant, maybe  it thinks that this is the main feature of an African elephant.\n\n","70ca08a4":"<h2>Please let me know your views on this kernel. All kinds of suggestions are welcome. If you like this kernel, I would appreciate if you could upvote it.  <\/h2> ","41aa9de7":"Our model's performance seems good but here we are not focused on improving the results of the model. We want to know why our model is taking a certain decision and what features are contributing and how much they are contributing to make that decision. ","7ed2a307":"<h2>Thanks<\/h2> [Dan Becker](https:\/\/www.kaggle.com\/dansbecker) for creating such a good course on [Machine learning explainability](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability).   \n<h2>Thanks<\/h2> [kaggle](https:\/\/www.kaggle.com\/) for this challenge because in the process of creating this kernel I learned a lot. Kaggle community is great to learn data science by practical approach. ","e00f1798":"Output value is 0.65 more than base value i.e 0.5062. This person is having high chance of heart disease. We can see that major vessels (**ca**) = 0 , **oldpeak** (ST depression) = 0 , maximum heart rate (**thalach**)  186 (which is > 120), **age** = 51 (seems reasonable)  and **thal**(blood disorder) = 2(normal) etc. are contributing to increase the chance of having a heart disease. And on the other side **exang**( Exercise induced angina) = 1(true), **cp** (chest pain) = 0 (normal) and **restecg** (Resting electrocardiographic measurement) = 0 (normal) are decreasing chance of having a heart disease which seems reasonable. [Check this.](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4468223\/)","774edad8":"<h2>Visualizing convnet filters<\/h2>        \n\nAnother easy thing to do to inspect the filters learned by convnets is to display the visual pattern that each filter is meant to respond to. This can be done with gradient ascent in input space: applying gradient descent to the value of the input image of a convnet so as to maximize the response of a specific filter, starting from a blank input image. The resulting input image would be one that the chosen filter is maximally responsive to.","5510ba93":"Above plots are self explainatory. We can easily understand which values are contributing towards what.","a55caafa":"<h2>Visualizing heatmaps of class activation in an image<\/h2>\n\nA \"class activation\" heatmap is a 2D grid of scores associated with an specific output class, computed for every location in any input image, indicating how important each location is with respect to the class considered.","6d93bfbc":"<h1>What we are going to discuss in this kernel :- <\/h1>\n\n* **Permutation Importance**   \n* **Partial Dependence Plots**\n* **SHAP (SHapley Additive exPlanations)**\n* **LIME** \n* **CNN**\n* **References**","49356053":"In above graph y-axis indicates change in prediction and x-axis are values of **thalach**. Blue shaded area indicates. Maximum heart rate of a person till 140 have very less affect on increasing his chance of having a heart disease or have very less affect. Around 160 chance of having a heart disease increase and after that it is same.","c9265d2f":"This time as number of major vessel (ca) increases, it decreases chance of having a heart disease. Other thing to note is having one, two, three number of major vessels have similiar affect. So something is better than nothing :)  ","416f2dfb":"<h2>Aggregated force_plot<\/h2>","ad8b1633":"<h1>Let's Start<\/h1>\n\nYou trained a model and it is performing great. Now question is why it is performing that way. What are the features it is dependent on. Considering machine learning models black box is not the way to go. We have to learn the methods to get insights of our models. In this kernel we will discuss some models and how to get their insigts. There are different methods to get information about your model's dependences and we will discuss some of those methods.      \nNext thing is why would anyone want to know the insights of the model. There are various reasons like it helps in developing trust on the model, we learn what to tweak to improve our results and above all it is fun to know how the model is doing such a great job :).\n","cf831d63":"<h3>Our Image<\/h3>","e243b88a":"<h2>Visualizations of intermediate activations<\/h2>","4880ccbe":"<h1>Convolution Neural Networks<\/h1>  \n\nDeep learning models are strongly considered black box because we think it's difficult to know why the deep learning model is taking a certain decision. What features it is depending the most and how is it using different features. But this is not the case, we can also look inside a deep learning model. Here now we will look into a convolution neural network. We will check what features it rely on and what factors are affecting it.  ","8b8e8dcf":"In the above plot on x-axis we have SHAP value which show impact on the model. Color of the dot show value of a certain feature. We can easily interpret it. Having high value of **ca** and **oldpeak** decreases the chance of having a heart disease. High value of **thalach** increases the chance of having a heart disease."}}