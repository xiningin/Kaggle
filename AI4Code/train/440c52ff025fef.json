{"cell_type":{"7f84029d":"code","d89a59a0":"code","12304490":"code","f5d56219":"code","5279cf77":"code","e2e89779":"code","51ae78b4":"code","5b3b73a7":"code","b7fed9f5":"code","2e15dd50":"code","aa3bd831":"code","c82889ae":"code","0ddf009f":"code","731e6a6d":"code","066aa978":"code","afc251b4":"code","4731b032":"code","ff5747f2":"code","4b7e1b3d":"code","00c16de3":"code","eeec40d5":"code","080bf777":"code","fa47db69":"code","11e00861":"code","7af20107":"code","4f9b5d0f":"code","1587704d":"code","a284e5fb":"code","0d97c743":"code","c70a2276":"code","6e5a69f0":"code","24e12d34":"code","0b7d87b0":"code","ed9424eb":"code","c7068842":"code","249f6129":"code","15be93ac":"code","6798cddf":"code","3f05ada5":"code","de9b710d":"code","13ca65ab":"code","1e455132":"code","c1f2a768":"code","0e79ef2d":"code","a0119911":"code","a3542a29":"code","8be37aa8":"code","e4942198":"code","fe722ec3":"code","b0e4a3d7":"code","18734092":"code","e58d5e09":"code","4ebfb61a":"code","8416462d":"code","62e6098c":"code","de396620":"code","443fe0b5":"code","82845aa3":"code","58ec7a3a":"code","9248c04c":"code","01704e23":"code","5092fc4d":"code","c8615260":"code","52a7140e":"code","0029a19e":"code","e7f37c50":"code","e80a03b7":"code","25f8f8e3":"code","450da438":"code","2792ef27":"code","bc15edf5":"code","507b317b":"code","db37c2a6":"code","3f84167f":"code","9a53bfa6":"code","cdf34e1e":"code","e3385773":"code","bcf4e2c9":"code","9826b8fb":"code","f4bfa3fb":"code","a9b8c3fb":"code","3dff94ef":"code","3adeeedc":"code","a68a8254":"code","2d852ab1":"code","32868323":"code","7e909c2a":"code","c439d2ac":"code","fd256a53":"code","6979358a":"code","a15ac4cd":"code","a79e9a34":"code","1fa65356":"code","89def0b3":"code","86e3459f":"code","aee9cf48":"code","5be73d26":"code","258f5add":"code","dd4ebf04":"code","db68d82a":"code","09085e7f":"code","8a72622c":"code","c3f1ed3e":"code","c2206166":"code","b6792060":"code","54578435":"code","6a6089c2":"code","49312ede":"code","9a7317ad":"code","285366ac":"markdown","76919e68":"markdown","9f48eef3":"markdown","d7929932":"markdown","1937ec5f":"markdown","7b2a7dee":"markdown","6bf0e626":"markdown","d5da07e9":"markdown","d22525d2":"markdown","68928ac5":"markdown","f1b055fe":"markdown","b8dfc70b":"markdown","fdb286da":"markdown","40bc122c":"markdown","ab008071":"markdown"},"source":{"7f84029d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.tabular import *\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nimport gc\ngc.collect()\n\nimport re\nimport os\nimport re\nimport gc\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom bayes_opt import BayesianOptimization\nfrom lightgbm import LGBMRegressor\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.stem.snowball import EnglishStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom functools import lru_cache\nfrom tqdm import tqdm as tqdm\nfrom fastai.text import *\nfrom fastai.metrics import *","d89a59a0":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 42\nseed_everything(SEED)","12304490":"train = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")\ntest = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")\nsub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")","f5d56219":"train.shape, test.shape, sub.shape","5279cf77":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\n', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df","e2e89779":"target_cols_questions = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written']\n\ntarget_cols_answers = ['answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\n\ntargets = target_cols_questions + target_cols_answers\n\ninput_columns = ['question_title', 'question_body', 'answer']","51ae78b4":"train = clean_data(train, ['answer', 'question_body', 'question_title'])\ntest = clean_data(test, ['answer', 'question_body', 'question_title'])","5b3b73a7":"find = re.compile(r\"^[^.]*\")\n\ntrain['netloc_1'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_1'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntrain['netloc_2'] = train['question_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_2'] = test['question_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntrain['netloc_3'] = train['answer_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_3'] = test['answer_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])","b7fed9f5":"train = train[input_columns + targets]\ntest = test[input_columns]","2e15dd50":"train, val = train_test_split(train, test_size=0.2, shuffle=True, random_state=42)","aa3bd831":"train.shape, val.shape","c82889ae":"!pip install ..\/input\/sacremoses\/sacremoses-master\/\n!pip install ..\/input\/transformers\/transformers-master\/","0ddf009f":"!ls ..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased","731e6a6d":"from collections import defaultdict\nfrom dataclasses import dataclass\nimport functools\nimport gc\nimport itertools\nimport json\nfrom multiprocessing import Pool\nimport os\nfrom pathlib import Path\nimport random\nimport re\nimport shutil\nimport subprocess\nimport time\nfrom typing import Callable, Dict, List, Generator, Tuple\nfrom os.path import join as path_join\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json._json import JsonReader\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, Subset, DataLoader\n\nfrom transformers import BertTokenizer, AdamW, BertModel, BertPreTrainedModel, BertConfig\nfrom transformers.optimization import get_linear_schedule_with_warmup","066aa978":"# Creating a config object to store task specific information\nclass Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n        \nconfig = Config(\n    testing=False,\n    seed = 42,\n    roberta_model_name='bert-base-uncased', # can also be exchnaged with roberta-large \n    use_fp16=False,\n    bs=16, \n    max_seq_len=128, \n    hidden_dropout_prob=.25,\n    hidden_size=768, # 1024 for roberta-large\n    start_tok = \"[CLS]\",\n    end_tok = \"[SEP]\",\n)","afc251b4":"# forward tokenizer\n\nclass FastAiRobertaTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs): \n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len \n    def __call__(self, *args, **kwargs): \n        return self \n    def tokenizer(self, t:str) -> List[str]: \n        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n        return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok]","4731b032":"# backward tokenizer\n\nclass FastAiRobertaTokenizerBackward(BaseTokenizer):\n    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs): \n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len \n    def __call__(self, *args, **kwargs): \n        return self \n    def tokenizer(self, t:str) -> List[str]: \n        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n        return [config.end_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.start_tok]","ff5747f2":"# create fastai tokenizer for roberta\nbert_tok = BertTokenizer.from_pretrained('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased-vocab.txt')\n\nfastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(bert_tok, max_seq_len=config.max_seq_len), \n                             pre_rules=[], post_rules=[])\n\nfastai_tokenizer_bwd = Tokenizer(tok_func=FastAiRobertaTokenizerBackward(bert_tok, max_seq_len=config.max_seq_len), \n                             pre_rules=[], post_rules=[])","4b7e1b3d":"# create fastai vocabulary for roberta\npath = Path()\nbert_tok.save_vocabulary(path)\n   \nfastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","00c16de3":"databunch = TextDataBunch.from_df(\".\", train, val, test,\n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=input_columns,\n                  label_cols=targets,\n                  bs=16,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\ndatabunch.save('databunch.pkl')","eeec40d5":"databunch = load_data(path, 'databunch.pkl', bs=16)","080bf777":"databunch.show_batch()","fa47db69":"start_time = time.time()\n\nseed = 42\n\nnum_labels = len(targets)\nn_epochs = 3\nlr = 2e-5\nwarmup = 0.05\nbatch_size = 16\naccumulation_steps = 4\n\nbert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/bert_config.json'\n\nbert_model = 'bert-base-uncased'\ndo_lower_case = 'uncased' in bert_model\ndevice = torch.device('cuda')\n\noutput_model_file = 'bert_pytorch.bin'\noutput_optimizer_file = 'bert_pytorch_optimizer.bin'\noutput_amp_file = 'bert_pytorch_amp.bin'\n\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","11e00861":"class BertForSequenceClassification(BertPreTrainedModel):\n    r\"\"\"\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification\/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n    \"\"\"\n    def __init__(self, config):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask,\n                            inputs_embeds=inputs_embeds)\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        return logits","7af20107":"loss_func = nn.BCEWithLogitsLoss()","4f9b5d0f":"bert_config = BertConfig.from_json_file(bert_model_config)\nbert_config.num_labels = len(targets)\n\nmodel_path = os.path.join('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/')\n\nmodel = BertForSequenceClassification.from_pretrained(model_path, config=bert_config)\nlearn_bert = Learner(databunch, model, loss_func=loss_func, model_dir='\/temp\/model')","1587704d":"def bert_clas_split(self) -> List[nn.Module]:\n    \n    bert = model.bert\n    embedder = bert.embeddings\n    pooler = bert.pooler\n    encoder = bert.encoder\n    classifier = [model.dropout, model.classifier]\n    n = len(encoder.layer)\/\/3\n    print(n)\n    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n    return groups","a284e5fb":"x = bert_clas_split(model)","0d97c743":"learn_bert.layer_groups","c70a2276":"learn_bert","6e5a69f0":"learn_bert.split([x[2],  x[4],  x[5]])","24e12d34":"learn_bert.freeze()","0b7d87b0":"learn_bert.lr_find()","ed9424eb":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport matplotlib.style as style\nstyle.use('seaborn-poster')\nstyle.use('ggplot')","c7068842":"learn_bert.recorder.plot(suggestion=True)","249f6129":"learn_bert.fit_one_cycle(7, max_lr=slice(1e-3, 1e-2), moms=(0.8,0.7), pct_start=0.2, wd =0.1)","15be93ac":"learn_bert.save('head-1')","6798cddf":"learn_bert.freeze_to(-2)\nlearn_bert.fit_one_cycle(7, max_lr=slice(1e-4, 1e-3), moms=(0.8,0.7), pct_start=0.4, wd =0.1)","3f05ada5":"learn_bert.save('head-2')","de9b710d":"learn_bert.freeze_to(-3)\nlearn_bert.fit_one_cycle(7, max_lr=slice(1e-5, 1e-4), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","13ca65ab":"learn_bert.unfreeze()\nlearn_bert.lr_find()\nlearn_bert.recorder.plot(suggestion=True)","1e455132":"learn_bert.fit_one_cycle(12, slice(1e-5, 1e-4), moms=(0.8,0.7), pct_start=0.4, wd =0.1)","c1f2a768":"bs, bptt = 32, 80\n\ndata_lm = TextLMDataBunch.from_df('.', train, val, test,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=['question_title', 'question_body', 'answer'],\n                  label_cols=targets,\n                  bs=bs,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\ndata_lm.save('data_lm.pkl')","0e79ef2d":"path = \".\"\ndata_lm = load_data(path, 'data_lm.pkl', bs=bs, bptt=bptt)","a0119911":"path = \".\"\ndata_bwd = load_data(path, 'data_lm.pkl', bs=bs, bptt = bptt, backwards=True)","a3542a29":"data_lm.show_batch()","8be37aa8":"data_bwd.show_batch()","e4942198":"awd_lstm_lm_config = dict( emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=False, bidir=False, output_p=0.1,\n                          hidden_p=0.15, input_p=0.25, embed_p=0.02, weight_p=0.2, tie_weights=True, out_bias=True)","fe722ec3":"awd_lstm_clas_config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=False, bidir=False, output_p=0.4,\n                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5)","b0e4a3d7":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5,\n                               config=awd_lstm_lm_config, pretrained = False)\nlearn = learn.to_fp16(clip=0.1)","18734092":"fnames = ['..\/input\/awd-lstm\/lstm_wt103.pth','..\/input\/awd-lstm\/itos_wt103.pkl']\nlearn.load_pretrained(*fnames, strict=False)\nlearn.freeze()","e58d5e09":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","4ebfb61a":"learn.fit_one_cycle(2, max_lr=slice(5e-3, 5e-2), moms=(0.8, 0.7), pct_start=0.3, wd =0.1)","8416462d":"learn.save('fit_head')","62e6098c":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","de396620":"learn.fit_one_cycle(10, max_lr = slice(1e-4, 1e-3), moms=(0.8, 0.7), pct_start=0.3, wd =0.1)","443fe0b5":"learn.recorder.plot_losses()","82845aa3":"learn.save('fine-tuned')\nlearn.load('fine-tuned')\nlearn.save_encoder('fine-tuned-fwd')","58ec7a3a":"learn = language_model_learner(data_bwd, AWD_LSTM, drop_mult=0.5,\n                               config=awd_lstm_lm_config, pretrained = False)\nlearn = learn.to_fp16(clip=0.1)","9248c04c":"fnames = ['..\/input\/awd-lstm\/lstm_wt103.pth','..\/input\/awd-lstm\/itos_wt103.pkl']\nlearn.load_pretrained(*fnames, strict=False)\nlearn.freeze()","01704e23":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","5092fc4d":"learn.fit_one_cycle(2, max_lr=slice(5e-2, 1e-1), moms=(0.8, 0.7), pct_start=0.3, wd =0.1)","c8615260":"learn.save('fit_head-bwd')","52a7140e":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","0029a19e":"learn.fit_one_cycle(10, max_lr = slice(1e-4, 1e-3), moms=(0.8, 0.7), pct_start=0.3, wd =0.1)","e7f37c50":"learn.recorder.plot_losses()","e80a03b7":"learn.save('fine-tuned-bwd')\nlearn.load('fine-tuned-bwd')\nlearn.save_encoder('fine-tuned-bwd')","25f8f8e3":"text_cols = ['question_title', \"question_body\", 'answer']","450da438":"data_cls = TextClasDataBunch.from_df('.', train, val, test, vocab = data_lm.vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=text_cols,\n                  label_cols=targets,\n                  bs=bs,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\ndata_cls.save('data_cls.pkl')","2792ef27":"data_cls = load_data(path, 'data_cls.pkl', bs=bs)","bc15edf5":"data_cls.show_batch()","507b317b":"data_cls_bwd = load_data(path, 'data_cls.pkl', bs=bs, backwards=True)","db37c2a6":"data_cls_bwd.show_batch()","3f84167f":"learn = text_classifier_learner(data_cls, AWD_LSTM, drop_mult=0.5,config=awd_lstm_clas_config, pretrained = False, loss_func=loss_func)\nlearn.load_encoder('fine-tuned-fwd')\nlearn = learn.to_fp16(clip=0.1)\n#learn.loss_func = L1LossFlat()\nfnames = ['..\/input\/awd-lstm\/lstm_wt103.pth','..\/input\/awd-lstm\/itos_wt103.pkl']\nlearn.load_pretrained(*fnames, strict=False)\nlearn.freeze()","9a53bfa6":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","cdf34e1e":"learn.fit_one_cycle(2, max_lr=slice(1e-2, 1e-1), moms=(0.8, 0.7), pct_start=0.3, wd =0.1)","e3385773":"learn.save('first-head')\nlearn.load('first-head')","bcf4e2c9":"learn.freeze_to(-2)\nlearn.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","9826b8fb":"learn.save('second')\nlearn.load('second')","f4bfa3fb":"learn.freeze_to(-3)\nlearn.fit_one_cycle(2, slice(1e-4\/(2.6**4),1e-4), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","a9b8c3fb":"learn.save('third')\nlearn.load('third')","3dff94ef":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","3adeeedc":"learn.fit_one_cycle(7, slice(1e-4\/(2.6**4),1e-4), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","a68a8254":"learn.recorder.plot_losses()","2d852ab1":"learn.save('fwd-cls')","32868323":"learn_bwd = text_classifier_learner(data_cls_bwd, AWD_LSTM, drop_mult=0.5, config=awd_lstm_clas_config, loss_func=loss_func,\n                                    pretrained = False)\nlearn_bwd.load_encoder('fine-tuned-bwd')\nlearn_bwd = learn_bwd.to_fp16(clip=0.1)","7e909c2a":"fnames = ['..\/input\/awd-lstm\/lstm_wt103.pth','..\/input\/awd-lstm\/itos_wt103.pkl']\nlearn_bwd.load_pretrained(*fnames, strict=False)\nlearn_bwd.freeze()","c439d2ac":"learn_bwd.lr_find()\nlearn_bwd.recorder.plot(suggestion=True)","fd256a53":"learn_bwd.fit_one_cycle(2, max_lr=slice(5e-2, 1e-1), moms=(0.8, 0.7), pct_start=0.3, wd =0.1)","6979358a":"learn_bwd.save('first-head-bwd')\nlearn_bwd.load('first-head-bwd')","a15ac4cd":"learn_bwd.freeze_to(-2)\nlearn_bwd.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","a79e9a34":"learn_bwd.save('second-bwd')\nlearn_bwd.load('second-bwd')","1fa65356":"learn_bwd.freeze_to(-3)\nlearn_bwd.fit_one_cycle(2, slice(1e-5\/(2.6**4),1e-5), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","89def0b3":"learn_bwd.save('third-bwd')\nlearn_bwd.load('third-bwd')","86e3459f":"learn_bwd.unfreeze()\nlearn_bwd.lr_find()\nlearn_bwd.recorder.plot(suggestion=True)","aee9cf48":"learn_bwd.fit_one_cycle(7, slice(1e-5\/(2.6**4),1e-5), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","5be73d26":"learn_bwd.recorder.plot_losses()","258f5add":"learn_bwd.save('bwd-cls')","dd4ebf04":"def get_ordered_preds(learn_bert, ds_type, preds):\n  np.random.seed(42)\n  sampler = [i for i in learn_bert.data.dl(ds_type).sampler]\n  reverse_sampler = np.argsort(sampler)\n  preds = [p[reverse_sampler] for p in preds]\n  return preds","db68d82a":"test_raw_preds = learn_bert.get_preds(ds_type=DatasetType.Test)\ntest_preds_bert = get_ordered_preds(learn_bert, DatasetType.Test, test_raw_preds)","09085e7f":"pred_fwd_test, lbl_fwd_test = learn.get_preds(ds_type=DatasetType.Test,ordered=True)\npred_bwd_test, lbl_bwd_test = learn_bwd.get_preds(ds_type=DatasetType.Test,ordered=True)","8a72622c":"type(pred_fwd_test)","c3f1ed3e":"test_preds_bert = torch.FloatTensor(test_preds_bert[0])","c2206166":"final_preds_test = (0.3*pred_fwd_test + 0.3*pred_bwd_test + 0.4*test_preds_bert)","b6792060":"sub.iloc[:, 1:] = final_preds_test.numpy()\nsub.to_csv('submission.csv', index=False)\nsub.head()","54578435":"fig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(targets):\n    ax = axes[i]\n    sns.distplot(train[col], label=col, bins=bins, ax=ax, color='blue')\n    sns.distplot(sub[col], label=col, bins=bins, ax=ax, color='orange')\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\nplt.tight_layout()\nplt.show()\nplt.close()","6a6089c2":"# y_train = train[targets].values\n\n# for column_ind in range(30):\n#     curr_column = y_train[:, column_ind]\n#     values = np.unique(curr_column)\n#     map_quantiles = []\n#     for val in values:\n#         occurrence = np.mean(curr_column == val)\n#         cummulative = sum(el['occurrence'] for el in map_quantiles)\n#         map_quantiles.append({'value': val, 'occurrence': occurrence, 'cummulative': cummulative})\n            \n#     for quant in map_quantiles:\n#         pred_col = test_preds_bert[0][:, column_ind]\n#         q1, q2 = np.quantile(pred_col, quant['cummulative']), np.quantile(pred_col, min(quant['cummulative'] + quant['occurrence'], 1))\n#         pred_col[(pred_col >= q1) & (pred_col <= q2)] = quant['value']\n#         test_preds_bert[0][:, column_ind] = pred_col","49312ede":"# sub.iloc[:, 1:] = test_preds_bert[0].numpy()\n# sub.to_csv('submission.csv', index=False)\n# sub.head()","9a7317ad":"# fig, axes = plt.subplots(6, 5, figsize=(18, 15))\n# axes = axes.ravel()\n# bins = np.linspace(0, 1, 20)\n\n# for i, col in enumerate(targets):\n#     ax = axes[i]\n#     sns.distplot(train[col], label=col, bins=bins, ax=ax, color='blue')\n#     sns.distplot(sub[col], label=col, bins=bins, ax=ax, color='orange')\n#     # ax.set_title(col)\n#     ax.set_xlim([0, 1])\n# plt.tight_layout()\n# plt.show()\n# plt.close()","285366ac":"## Build BERT Model","76919e68":"## Forward Training","9f48eef3":"## Backward Training\n","d7929932":"# Prediction","1937ec5f":"### Forward Training","7b2a7dee":"# train-val-test split","6bf0e626":"# Modelling","d5da07e9":"# Import data and Libraries","d22525d2":"# Backward Training","68928ac5":"## Setup model","f1b055fe":"# Forward Training","b8dfc70b":"# Transformers","fdb286da":"## Cleaning the data","40bc122c":"# Splitting the model","ab008071":"# Fastai - ULMFiT"}}