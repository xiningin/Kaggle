{"cell_type":{"ec99c399":"code","bca18d4d":"code","3f549fa3":"code","c16cf773":"code","d07f029b":"code","3bbe7151":"code","ecfe0d7c":"code","9920b8d9":"code","5d8f1e09":"code","275c0612":"code","d49daafc":"code","411c5c48":"code","1dcd78a3":"code","f7a2dda0":"code","1a52d590":"code","4d829a8a":"code","32f682a5":"code","86d21ad8":"code","601581a2":"code","ac52b9c2":"code","922e5f68":"code","f3d34936":"code","639ac4a8":"code","c65f3aab":"code","f58dd580":"code","c7b5ec7f":"code","3aa4dbba":"code","5c730e25":"code","549dbe0c":"code","bb0c7d8a":"code","d61a200e":"code","43eba325":"code","bba9e8cb":"code","68b59499":"code","83be3ca7":"code","5fcc20ab":"code","d04983a3":"code","ed0cf928":"code","ee8cc763":"code","3330f2c6":"code","a41206c8":"code","0218f216":"code","17a1b9b3":"code","b02b43f5":"code","659f42ae":"code","95bd7d6c":"code","29ec06d4":"markdown","dd7def52":"markdown","a78db416":"markdown","2a47d8c6":"markdown","8c1d6eb3":"markdown","6eae7eb1":"markdown","8779d43e":"markdown","1aff70ae":"markdown","9ed31856":"markdown","6d1a95e4":"markdown","a866459a":"markdown","409ee231":"markdown","0664a3b1":"markdown","ce09267e":"markdown"},"source":{"ec99c399":"#Import Packages\nimport sys\n# Set the environment path\nsys.path.append(\"..\/..\/\")  \nimport os\nfrom collections import Counter\nimport math\nimport numpy as np\nimport pandas as pd\nimport gensim\nfrom gensim.models.doc2vec import LabeledSentence\nfrom gensim.models.doc2vec import TaggedDocument\nfrom gensim.models import Doc2Vec\nfrom scipy.spatial import distance\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport spacy\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nprint(\"System version: {}\".format(sys.version))\nprint(\"Gensim version: {}\".format(gensim.__version__))\n","bca18d4d":"root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={'pubmed_id': str,'Microsoft Academic Paper ID': str, 'doi': str})\nmeta_df.head()","3f549fa3":"import glob\nall_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nprint('number of current articles : ',len(all_json))","c16cf773":"import json\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            #print(content)\n            self.paper_id = content['paper_id']\n            #self. = content['publish_time']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\n#print(first_row)","d07f029b":"dict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'title': [], 'journal': [], 'publish_time' : [] , 'cord_uid' : [] , 'who_covidence_id': [], 'url' : []  }\nfor idx, entry in enumerate(all_json):\n    if idx>400:   #Process only 300 files \n        break\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')    \n    try:\n        content = FileReader(entry)\n    except Exception as e:\n        continue  # invalid paper format, skip\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['abstract'].append(content.abstract)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['body_text'].append(content.body_text)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    #print(meta_data['publish_time'])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \n    # add doi\n    dict_['doi'].append(meta_data['doi'].values[0])\n    \n    #print(meta_data['publish_time'])\n    \n    dict_['publish_time'].append(meta_data['publish_time'].values[0])\n    dict_['cord_uid'].append(meta_data['cord_uid'].values[0])\n    dict_['who_covidence_id'].append(meta_data['who_covidence_id'].values[0])\n    dict_['url'].append(meta_data['url'].values[0])\n    #dict_['s2_id'].append(meta_data['s2_id'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text', 'title', 'journal', 'publish_time' , 'cord_uid', 'who_covidence_id', 'url' ])\ndf_covid.count()","3bbe7151":"number_of_articles  = 100 #sample files with doc2vec similarity threshold \nthreshold = 0.4 #threshold for doc2vec cosin similarity \nstart_kaggle_index= 0 #start index for particular kaggle query\nend_kaggle_index = 5 #index for particular kaggle search query\nsubset_df_covid = df_covid.iloc[:number_of_articles,:] #only look at the 100 articles because the vec2wrod model consumes lots of memory.  \nsubset_df_covid = subset_df_covid[['paper_id','body_text','abstract']]\nsubset_df_covid.head()\n","ecfe0d7c":"print('Number of articles with no bodies',subset_df_covid['body_text'].isna().sum()) #no article with zero text","9920b8d9":"def apply_all_sent_token(doc):\n    list_all_tokens = []\n    all_sent = [nltk.word_tokenize(sents) for sents in doc]\n    for one_sent in all_sent:\n        filtered_tokens = []\n        filtered_tokens =[words for words in one_sent if words not in stop_words]\n        custom_removed_tokens = [words for words in filtered_tokens if words not in ['.','shown','fig.','figure','fig']]\n        custom_removed_tokens = [words for words in custom_removed_tokens if len(words) > 2]\n        list_all_tokens.append(custom_removed_tokens)\n    return list_all_tokens","5d8f1e09":"def split_sentences(article):\n    import nltk.data\n    import re\n    tokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n    list_df = []\n    marked_sentences = '***'.join(tokenizer.tokenize(article))\n    marked_sentences = re.sub(r\"(\\[[0-9]*\\])\",'',marked_sentences) #remove the citations \n    marked_sentences = re.sub(r\"(\\([0-9]*\\))\",'',marked_sentences)\n    list_df  = marked_sentences.split('***')\n    return list_df","275c0612":"def preprocses_sentences(df):\n    df_sent = df['body_text'].apply(lambda x: split_sentences(x)) #check abstract istead of abstract body-text\n    df_lower = df_sent.apply(lambda doc: [sent.lower() if type(sent) == str else sent for sent in doc])\n    df_clean = df_lower.apply(lambda doc: [re.sub(r\"[^.a-z]\",' ',sent) for sent in doc]) #remove punctation\n    df_token = df_clean.apply(lambda doc:apply_all_sent_token(doc)) #tokenize all the sentences \n    preprocess_df=pd.concat([df,df_sent],axis = 1)\n    preprocess_df=pd.concat([preprocess_df, df_token], axis=1)\n    preprocess_df.columns=['paper_id','original_body', 'abstract','sentences','token_sentences']\n    return preprocess_df","d49daafc":"def prepare_kaggle_df():\n    kaggle_df = pd.DataFrame()\n   \n     \n    kaggle_group2 = [ \n                     'Detection Method in Diagnosing SARS-COV-2 with antibodies', \n                     'Sample in Diagnosing SARS-COV-2 with antibodie'\n                     'Sample size in Diagnosing SARS-COV-2 with antibodies',       \n                     'Measure in evidence in Diagnosing SARS-COV-2 with antibodies',\n                     'Speed of assay in Diagnosing SARS-COV-2 with antibodies',\n                     'FDA approval in Diagnosing SARS-COV-2 with antibodies'\n        \n        \n                     'Detection Method in Diagnosing SARS-COV-2 with Nucleic-acid based technigues',\n                     'Sample in Diagnosing SARS-COV-2 with Nucleic-acid based technigues'\n                     'Sample szie in Diagnosing SARS-COV-2 with Nucleic-acid based technigues',  \n                     'Measure of evidence in Diagnosing SARS-COV-2 with Nucleic-acid based technigues',\n                     'Speed of assay in  Diagnosing SARS-COV-2 with Nucleic-acid based technigues',\n                     'FDA approval in Diagnosing SARS-COV-2 with Nucleic-acid based technigues',\n                    \n        \n        \n                     'Detection Method in Development of a point-of-care test and rapid bed-side tests',\n                     'Sample in Development of a point-of-care test and and rapid bed-side tests',\n                     'Sample Size in Development of a point-of-care test and and rapid bed-side tests',\n                     'Measure of evidence Development of a point-of-care test and rapid bed-side tests',\n                     'Speed of assay in Development of a point-of-care test and rapid bed-side test',\n                     'FDA approval in Development of a point-of-care and rapid bed-side test',\n                                \n                     \n                    ]           \n                     \n                    \n    kaggle_tokens=[nltk.word_tokenize(sents) for sents in kaggle_group2]\n    kaggle_df['sentences']=[kaggle_group2]\n    kaggle_df['token_sentences']=[kaggle_tokens]\n    #print(kaggle_df.sentences[0])\n    #print(len(kaggle_df.token_sentences.values))\n    #print(len(kaggle_df.token_sentences.values.tolist()))\n    return kaggle_df","411c5c48":"def preprocess_data(df,number_of_artciles):\n    preprocess_df=preprocses_sentences(df) \n    kaggle_df = prepare_kaggle_df()\n    preprocess_df.head()\n    kaggle_df.head()\n    return preprocess_df,kaggle_df\n","1dcd78a3":"def generate_article_df(one_article_df):\n    all_articles_relevant_sentences_list = []\n    article_df = pd.DataFrame()\n    article_df['sentences'] = one_article_df['sentences']\n    article_df['token_sentences'] =  one_article_df['token_sentences']\n    return article_df","f7a2dda0":"def transform_kaggle(kaggle_df, start_kaggle_index, end_kaggle_index):\n    kaggle_list = [] \n    sent_list = []\n    for index in range(len(kaggle_df.token_sentences.iloc[0])):\n        kaggle_list.append(kaggle_df.token_sentences.apply(lambda x: x[index]))\n    for index in range(len(kaggle_df.sentences.iloc[0])):\n        sent_list.append(kaggle_df.sentences.apply(lambda x: x[index]))\n    \n    df = pd.DataFrame()    \n    df = pd.concat(kaggle_list, axis=1,ignore_index=True)\n    df2=pd.concat(sent_list,axis=1,ignore_index=True)\n    df=df.transpose()\n    df2=df2.transpose()\n    kaggle_transformed_df = pd.concat([df2,df],axis=1)\n    kaggle_transformed_df.columns=['sentences','token_sentences']\n    kaggle_transformed_df = kaggle_transformed_df.loc[start_kaggle_index:end_kaggle_index]\n\n    return kaggle_transformed_df","1a52d590":"def prepare_doc2vec_tagged_documents(article_df, transformed_kaggle_df):\n    article_sentences = article_df[['sentences']] \n    article_sentences_list = article_sentences.values.flatten().tolist()\n\n \n    kaggle_sentences = transformed_kaggle_df[['sentences']]\n    kaggle_sentences_list = kaggle_sentences.values.flatten().tolist()\n   \n    all_sentences_list = article_sentences_list + kaggle_sentences_list\n\n\n    corpus = all_sentences_list\n    len(corpus)\n\n    # Produce dictionary of sentence to id\n    sentence_id = {sent : i for i, sent in enumerate(corpus)} #\n    \n    # Assign id to sentences\n    article_df['qid1'] = article_df['sentences'].apply(lambda row : sentence_id[row])\n    transformed_kaggle_df['qid2'] = transformed_kaggle_df['sentences'].apply(lambda row : sentence_id[row])\n    \n     # Doc2vec requires data as Tagged Documents with the tokenized sentence and the sentence id\n    article_df['labeled_tokens_article'] = article_df.apply(lambda x: TaggedDocument(x.token_sentences, str(x.qid1)), axis=1)\n    transformed_kaggle_df['labeled_tokens_kaggle'] = transformed_kaggle_df.apply(lambda x: TaggedDocument(x.token_sentences, str(x.qid2)), axis=1)\n    \n    #Get all Tagged Documents\n    labeled_article_sentences = article_df[['labeled_tokens_article']]  \n    labeled_article_sentences_list = labeled_article_sentences.values.flatten().tolist()\n    #print('num of one article sentences',len(all_sentences))\n    labeled_kaggle_sentences = transformed_kaggle_df[['labeled_tokens_kaggle']]\n    labeled_kaggle_sentences_list = labeled_kaggle_sentences.values.flatten().tolist()\n    labeled_sentences= labeled_article_sentences_list + labeled_kaggle_sentences_list\n    \n    return labeled_sentences","4d829a8a":"def train_doc2vec_model(labeled_sentences):\n    model = Doc2Vec(labeled_sentences, dm=1, min_count=1, window=10,negative=5, vector_size=100, epochs=30)\n    \n    # Train our model for 20 epochs\n    for epoch in range(30):\n        model.train(labeled_sentences, epochs=model.epochs, total_examples=model.corpus_count)\n        \n    return model","32f682a5":"def doc2vec_cosine_similarity(article_df,transformed_kaggle_df,doc2vec_model, start_kaggle_index ,end_kaggle_index):\n    for index in range(start_kaggle_index,end_kaggle_index):\n        name = \"task_\" + str(index+1)\n        kaggle_token = transformed_kaggle_df.loc[index]['token_sentences']\n        article_df[name] = article_df.apply(lambda x: doc2vec_model.wv.n_similarity(x.token_sentences,kaggle_token) if len(x.token_sentences) != 0 else 0, axis = 1)\n    return article_df\n","86d21ad8":"def doc2vec_per_article(one_article_df,transformed_kaggle_df,threshold, start_kaggle_index ,end_kaggle_index):\n    article_df = generate_article_df(one_article_df)\n    labeled_sentences = prepare_doc2vec_tagged_documents(article_df,transformed_kaggle_df)\n    doc2vec_model = train_doc2vec_model(labeled_sentences)\n    transformed_article_df=doc2vec_cosine_similarity(article_df,transformed_kaggle_df,doc2vec_model,start_kaggle_index ,end_kaggle_index)\n    relevant_sentences_one_article= find_top_relevant_sentences(transformed_article_df,transformed_kaggle_df,threshold, start_kaggle_index ,end_kaggle_index) #put the threshold \n    return relevant_sentences_one_article","601581a2":"def word2vec_per_article(one_article_df,transformed_kaggle_df,threshold,start_kaggle_index ,end_kaggle_index):\n    article_df = generate_article_df(one_article_df)\n    sentence_embeddings_list,kaggle_embeddings_list= generate_word_embeddings(article_df,transformed_kaggle_df)\n    transformed_article_df=generate_cosin_similarity(article_df,sentence_embeddings_list,kaggle_embeddings_list)\n    relevant_sentences_one_article = find_top_relevant_sentences(transformed_article_df,transformed_kaggle_df,threshold, start_kaggle_index ,end_kaggle_index)\n    return relevant_sentences_one_article\n    ","ac52b9c2":"def find_top_relevant_sentences(article_df,kaggle_transformed_df,threshold,start_kaggle_index ,end_kaggle_index):\n    relevant_sentences_df = pd.DataFrame()\n    deduplicated_relevante_top_sentences = pd.DataFrame()\n    final_relevant_sentences_list = []\n    tmp_final=pd.DataFrame(columns={'sentences','score'})\n    for index in range(start_kaggle_index,end_kaggle_index): \n        name = \"task_\" + str(index+1)\n        tmp_df=article_df.sort_values(name, inplace = False, ascending=False).head(5) #sorting and select the top 5 sentences\n        tmp2_df = tmp_df[['sentences',name]]\n        tmp2_df = tmp2_df.rename(columns={name : \"score\"})\n        tmp_final=tmp_final.append(tmp2_df, ignore_index=True) #concatinate all the sentences and score and scores\n    relevant_sentences_df=tmp_final.sort_values('score',inplace = False, ascending = False) #sorting\n    deduplicated_relevante_top_sentences = relevant_sentences_df.drop_duplicates('sentences',keep='first') #drop duplicates\n    deduplicated_relevante_top_sentences['score'] = pd.to_numeric(deduplicated_relevante_top_sentences['score'],errors='coerce')\n    final_relevante_sentences=deduplicated_relevante_top_sentences[deduplicated_relevante_top_sentences.score > threshold]\n    return final_relevante_sentences \n","922e5f68":"def find_relevant_sentences_all_articles(preprocess_df,kaggle_df, threshold, start_kaggle_index,end_kaggle_index):\n    all_relevant_sentences_list = []\n    transformed_kaggle_df = transform_kaggle(kaggle_df,start_kaggle_index,end_kaggle_index)\n    for index in range(len(preprocess_df)):\n        one_article_relevant_sentences_list = []\n        one_article_df=preprocess_df.iloc[index]\n        #relevant_sentences_one_article=doc2vec_per_article(one_article_df,transformed_kaggle_df,threshold, start_kaggle_index ,end_kaggle_index)\n        relevant_sentences_one_article = word2vec_per_article(one_article_df,transformed_kaggle_df,threshold,start_kaggle_index ,end_kaggle_index) #using word2vec instead of doc2vec\n        for i in range(len(relevant_sentences_one_article)):\n            one_article_relevant_sentences_list.append(relevant_sentences_one_article.iloc[i]['sentences'])\n        all_relevant_sentences_list.append(one_article_relevant_sentences_list)\n    preprocess_df['extracted_relevant_sentences_doc2word_group5'] = all_relevant_sentences_list\n    return preprocess_df","f3d34936":"from gensim.models.keyedvectors import KeyedVectors\nimport word2vec\nword2vec_model = KeyedVectors.load_word2vec_format('\/kaggle\/input\/word2vecgooglenewsmodel\/GoogleNews-vectors-negative300.bin', binary =True)","639ac4a8":"def generate_word_embeddings(transformed_article_df,transformed_kaggle_df):   \n    all_sentences = transformed_article_df[['token_sentences']]\n    all_sentences_list = all_sentences.values.flatten().tolist()\n    #print(len(all_sentences_list))\n    kaggle_sentences = transformed_kaggle_df[['token_sentences']]\n    #print(kaggle_sentences)\n    kaggle_sentenes_list = kaggle_sentences.values.flatten().tolist()\n    #print(len(kaggle_sentenes_list))\n    all_sentences = all_sentences_list + kaggle_sentenes_list\n    #print(len(all_sentences))\n    document_frequency_dict, num_documnets = get_document_frequency(all_sentences)\n    sentence_embeddings_list = average_word_embedding_cosine_similarity(transformed_article_df, word2vec_model,document_frequency_dict,num_documnets)\n    kaggle_embeddings_list = average_word_embedding_cosine_similarity(transformed_kaggle_df, word2vec_model,document_frequency_dict,num_documnets)\n    #print('sentence_embeddings ' ,len(sentence_embeddings_list))\n    #print(len(kaggle_embeddings_list))\n    return sentence_embeddings_list,kaggle_embeddings_list","c65f3aab":"def generate_cosin_similarity(transformed_article_df,sentence_embeddings_list,kaggle_embeddings_list):\n    for index in range(len(kaggle_embeddings_list)): #len(kaggle_embeddings_list)\n        cosine_similarity = []\n        name = \"task_\" + str(index+1)\n        for index2 in range(len(sentence_embeddings_list)):\n            one_sentence_embeddings = sentence_embeddings_list[index2]\n            kaggle_embeddings = kaggle_embeddings_list[index]\n            if sum(one_sentence_embeddings) != 0 and sum(kaggle_embeddings)!= 0:\n                #print(calculate_modified_cosine_similarity(one_sentence_embeddings,kaggle_embeddings))\n                cosine_similarity.append(calculate_cosine_similarity(one_sentence_embeddings,kaggle_embeddings))\n            else: \n                #print(0)\n                cosine_similarity.append(0)\n        #print(len(cosine_similarity))\n        transformed_article_df[name]= cosine_similarity \n    #cosine_df[name]=pd.Series(cosine_similarity) \n    return transformed_article_df","f58dd580":"def get_document_frequency(all_sentences_list):\n    \"\"\"Iterate through all sentences in dataframe and create a dictionary \n    mapping tokens to the number of sentences in our corpus they appear in\n    \n    Args:\n        df (pandas dataframe): dataframe of sentence pairs with their similarity scores\n        \n    Returns:\n        document_frequency_dict (dictionary): mapping from tokens to number of sentences they appear in\n        n (int): number of sentences in the corpus\n    \"\"\"\n    document_frequency_dict = {}\n    all_sentences = []\n    all_sentences = all_sentences_list     \n    n = len(all_sentences)\n\n    for s in all_sentences:\n        for token in set(s):\n            document_frequency_dict[token] = document_frequency_dict.get(token, 0) + 1\n\n    return document_frequency_dict, n","c7b5ec7f":"def average_word_embedding_cosine_similarity(df, embedding_model, document_frequencies,num_documnets, rm_stopwords=False):\n    \"\"\"Calculate the cosine similarity between TF-IDF weighted averaged embeddings\n    \n    Args:\n        df (pandas dataframe): dataframe as provided by the nlp_utils\n        embedding_model: word embedding model\n        rm_stopwords (bool): whether to remove stop words (True) or not (False)\n    \n    Returns:\n        list: predicted values for sentence similarity of test set examples\n    \"\"\"\n   \n    sentence_embedding = df.apply(lambda x: average_sentence_embedding(x.token_sentences, embedding_model,document_frequencies, num_documnets), axis=1)\n     \n    return sentence_embedding.tolist()","3aa4dbba":"def calculate_cosine_similarity(embedding1, embedding2):\n    \"\"\"Calculate cosine similarity between two embedding vectors\n    \n    Args:\n        embedding1 (list): embedding for the first sentence\n        embedding2 (list): embedding for the second sentence\n    \n    Returns:\n        list: cosine similarity value between the two embeddings\n    \"\"\"\n    # distance.cosine calculates cosine DISTANCE, so we need to\n    # return 1 - distance to get cosine similarity\n    cosine_similarity = 1 - distance.cosine(embedding1, embedding2)\n    return cosine_similarity","5c730e25":"def average_sentence_embedding(tokens, embedding_model,document_frequencies, num_documents):\n    \"\"\"Calculate TF-IDF weighted average embedding for a sentence\n    \n    Args:\n        tokens (list): list of tokens in a sentence\n        embedding_model: model to use for word embedding (word2vec, glove, fastText, etc.)\n    \n    Returns:\n        list: vector representing the sentence\n    \"\"\"\n    # Throw away tokens that are not in the embedding model\n    tokens = [i for i in tokens if i in embedding_model]\n\n    if len(tokens) == 0:\n        return []\n\n    # We will weight by TF-IDF. The TF part is calculated by:\n    # (# of times term appears \/ total terms in sentence)\n    count = Counter(tokens)\n    token_list = list(count)\n    term_frequency = [count[i] \/ len(tokens) for i in token_list]\n\n    #print(num_documents)\n    # Now for the IDF part: LOG(# documents \/ # documents with term in it)\n    inv_doc_frequency = [\n        math.log(num_documents \/ (document_frequencies.get(i, 0) + 1)) for i in count\n    ]\n\n    # Put the TF-IDF together and produce the weighted average of vector embeddings\n    word_embeddings = [embedding_model[token] for token in token_list]\n    weights = [term_frequency[i] * inv_doc_frequency[i] for i in range(len(token_list))]\n    return list(np.average(word_embeddings, weights=weights, axis=0))","549dbe0c":"preprocess_df, kaggle_df = preprocess_data(subset_df_covid,number_of_articles)\npreprocess_df.head()","bb0c7d8a":"final_preprocess_df = find_relevant_sentences_all_articles(preprocess_df,kaggle_df,threshold,start_kaggle_index ,end_kaggle_index)\nfinal_preprocess_df.head()","d61a200e":"#save the file to be processed in a VM machine for importing transformers\n#final_preprocess_df.to_csv('.\/method3_word2vec.csv',index= False) #method1 and method2 are already saved","43eba325":"#!pip install transformers","bba9e8cb":"#def BERT_Qquestion_Answer(question,text):\n    \n#    from transformers import BertTokenizer, BertForQuestionAnswering\n#    import torch\n#    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n#    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n    \n#    input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\n#    import re\n#    #print(input_text)\n#    encoding = tokenizer.encode_plus(\n#                    input_text,                      # Sentence to encode.\n#                    max_length = 512,           # Pad & truncate all sentences.\n#                    pad_to_max_length = True,\n#                    return_attention_mask = True,   # Construct attn. masks.\n    \n#    )\n\n#    input_ids, token_type_ids = encoding[\"input_ids\"], encoding[\"token_type_ids\"]\n#    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n \n#    # Find the tokens with the highest `start` and `end` scores.\n#    answer_start = torch.argmax(start_scores)\n#    answer_end = torch.argmax(end_scores)\n    \n\n    # Get the string versions of the input tokens.\n#    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n    # Start with the first token.\n #   answer = tokens[answer_start]\n\n    # Select the remaining answer tokens and join them with whitespace.\n #   for i in range(answer_start + 1, answer_end + 1):\n        \n        # If it's a subword token, then recombine it with the previous token.\n#        if tokens[i][0:2] == '##':\n #           answer += tokens[i][2:]\n        \n        # Otherwise, add a space then the token.\n  #      else:\n   #         answer += ' ' + tokens[i]\n\n        \n   # return answer","68b59499":"#new_df  = pd.read_csv('.\/method1_BERT_result.csv') #load the data from files\n#new_df.count() #400 files for training\n#sentence_list = preprocess_df.extracted_relevant_sentences_doc2word_group5.apply(lambda x:x)\n#question_list = ['what is the study type?','what detection method is used?','what is the sample size?','what sample is obtained?',\n#                 'what are the measures of evidence?','what is the speed of assay?','Is it FDA approval?']\n\n#all_answer_list = []\n#for qindex in range(len(question_list)):\n#    answer_list = list()\n#    for index in range(len(sentence_list)):\n#        answer=BERT_Qquestion_Answer(question_list[qindex],sentence_list[index])\n#        #print('answer',answer)\n#        answer_list.append(answer)\n#    all_answer_list.append(answer_list)","83be3ca7":"#new_df['study_type'] = all_answer_list[0]\n#new_df['method'] = all_answer_list[1]\n#new_df['sample_size'] = all_answer_list[2]\n#new_df['Sample_obtained'] = all_answer_list[3]\n#new_df['measure'] = all_answer_list[4]\n#new_df['assay'] = all_answer_list[5]\n#new_df['FDA'] = all_answer_list[6]","5fcc20ab":"#new_df.to_csv('method3_BERT_result.csv') #save to files to be loaded again for generating final results - this part was done in a Linux VM environment","d04983a3":"df_target = pd.DataFrame(columns = ['Publication Date','Study', 'Study Link', 'Journal','Study Type', ' Detected Method', 'Sample Obtained','Sample Size','Measure of Evidence','Speed of Assay','FDA Approval','DOI CORD_UID'])","ed0cf928":"pd.read_csv('..\/input\/bertfinalqaresults\/method1_BERT_result.csv')","ee8cc763":"df_method1_BERT_result = pd.read_csv('..\/input\/bertfinalqaresults\/method1_BERT_result.csv')\ndf_method1 = df_covid.loc[:100,:]\ndf_method1=df_method1.merge(df_method1_BERT_result, on = 'paper_id', how='inner')\ndf_target1=pd.concat([df_method1.publish_time,df_method1.title,df_method1.url,df_method1.journal,df_method1.study_type,df_method1.method,df_method1.sample_obtained,df_method1.sample_size,df_method1.measure,df_method1.assay,df_method1.FDA,df_method1.doi],axis=1)\ndf_target1 = df_target1.replace(np.nan,'')\ndf_target1.columns = ['Publication Date','Study', 'Study Link', 'Journal','Study Type', ' Detected Method', 'Sample Obtained','Sample Size','Measure of Evidence','Speed of Assay','FDA Approval','DOI CORD_UID']\ndf_target1","3330f2c6":"df_target1.to_csv('.\/Group6_Diagnostics.target_table1.csv')","a41206c8":"df_method2_BERT_result = pd.read_csv('\/kaggle\/input\/bertfinalqaresults\/method2_BERT_result.csv')\ndf_method2 = df_covid.loc[101:200,:]\ndf_method2=df_method2.merge(df_method2_BERT_result, on = 'paper_id', how='inner')\ndf_target2=pd.concat([df_method2.publish_time,df_method2.title,df_method2.url,df_method2.journal,df_method2.study_type,df_method2.method,df_method2.sample_obtained,df_method2.sample_size,df_method2.measure,df_method2.assay,df_method2.FDA,df_method2.doi],axis=1)\ndf_target2 = df_target2.replace(np.nan,'')\ndf_target2.columns = ['Publication Date','Study', 'Study Link', 'Journal','Study Type', ' Detected Method', 'Sample Obtained','Sample Size','Measure of Evidence','Speed of Assay','FDA Approval','DOI CORD_UID']\ndf_target2","0218f216":"df_target2 = pd.read_csv('\/kaggle\/input\/expectedoutput\/Group6_Diagnostics_target_table2.csv', index_col=0)  #this file was generated by my notebook\ndf_target2","17a1b9b3":"df_target2.to_csv('.\/Group6_Diagnostics_target_table2.csv') ","b02b43f5":"df_method3_BERT_result = pd.read_csv('\/kaggle\/input\/bertfinalqaresults\/method3_BERT_result.csv', index_col=0)\ndf_method3 = df_covid.loc[200:300,:]\ndf_method3=df_method3.merge(df_method3_BERT_result, on = 'paper_id', how='inner')\ndf_target3=pd.concat([df_method3.publish_time,df_method3.title,df_method3.url,df_method3.journal,df_method3.study_type,df_method3.method,df_method3.Sample_obtained,df_method3.sample_size,df_method3.measure,df_method3.assay,df_method3.FDA,df_method3.doi],axis=1)\ndf_target3= df_target3.replace(np.nan,'')\ndf_target3.columns = ['Publication Date','Study', 'Study Link', 'Journal','Study Type', ' Detected Method', 'Sample Obtained','Sample Size','Measure of Evidence','Speed of Assay','FDA Approval','DOI CORD_UID']\ndf_target3","659f42ae":"df_target3 = pd.read_csv('\/kaggle\/input\/expectedoutput\/Group6_Diagnostics_target_table3.csv', index_col=0) #this file was generated by my notebook\ndf_target3","95bd7d6c":"df_target3.to_csv('.\/Group6_Diagnostics_target_table3.csv')","29ec06d4":"# Preprocess Data","dd7def52":"# Results","a78db416":"## Run Methods","2a47d8c6":"# Round2 Group6 Diagnostics Methods for COVID-19\n\nThis notebook provides the steps and solution to address the task group5  (material studies) of round 2 for [COVID-19 Open Research Dataset Challenge (CORD-19)](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge).\n\n## Methodolgy \n\nThe dataset of this challenge is the masive volumes of unstructed articles related to Covid-19 research. During round 1, many work have demonstrated various solutions for analysing and classifying the articles types and abstract contents. Most tasks of round 2 focus on **'Information Retreival'** from NLP approaches, meaning that the solution must either answer specific questions or retrieve relevant information for particular search quireis related to the Covid-19 documents.  \n\nTo address the group6 task,addressing diagnostics studies related to COVID-19, this notebook extracts the relevannt information from the bodies of the articles rather than their abstracts, because such information may not provided at that level, plus many articles lack abstracts.\n\n\nThe steps to apply 'Information Extraction' are implemented as follows: \n> 1. Load Data: modify the code sumbitted for one of the participants from round 1 to load the data from kaggle input. \n> 1. Preprocess Data: split the artciles bodies into sentences, remove stop and custom words, tokenize each sentence.\n> 1. Word2Vec and Doc2Vec Cosin Similarity: apply both word2vec doc2vec between kaggle group6 search terms and sentences to generate word embeddings, calculate cosin similarity, rank the relevant sentences with relevant key search and threshold - the result of word2vec was slightly better. \n> 1. BERT Pre-train Question Answering: Load the BERT fine-pre-train models, structure question-sentences pairs to extract the answers (run this part on a VM machines)\n> 1. Result: Repeat the above steps for 100 samples of articles for each diagnostics methods, join with orginal tables and generate csv files \n> 1. Future Work: While word2vec generated fair word embeddings for this task, the word embeddings technigues can improve by using transformer language models. For the future work, the author plans to continue this research by applying other approaches for knowledge extraction such as [Microsoft Covid-19 Azure cognitive search](https:\/\/covid19search.azurewebsites.net\/) and [Google REALM](https:\/\/kentonl.com\/pub\/gltpc.2020.pdf)\n* \n\n\n","8c1d6eb3":"# Load Data\n\n\n\n","6eae7eb1":"# BERT Pre-trained Question-Answer Model\n","8779d43e":"# Word2Vec","1aff70ae":"# Doc2Vec","9ed31856":"# Note: \nTransformers cannot be installed in this notebook due to some weired behaviour of the environment. I saved the current result into a csv file and ran the BERT Question-Answer function at the Linux VM with the following settings: \n\n","6d1a95e4":"## Target_Table3: Diagnosing SARS-COV-2 with antibodies","a866459a":"Load result csv file and join with original dataframe and generate the target files","409ee231":"> This notebook was generated by Dr. Lida Ghahremanlou, NLP specialist and Data Scientist @ Microsoft UK","0664a3b1":"These settings are important for processing the articles for different methods. \n","ce09267e":"## Target_Table2: Diagnosing SARS-COV-2 with Nucleic-acid based tech"}}