{"cell_type":{"98655715":"code","cc8e732e":"code","11681227":"code","509a4a29":"code","35676b16":"code","0d1cab12":"code","ef5bf8b8":"code","08f8bdcb":"code","979204fd":"code","fe0275c9":"code","5c2539b6":"code","0c2dc13b":"code","e2bec0df":"code","ecb48880":"code","80ce1089":"code","2772eed3":"code","aa109647":"code","e5b65341":"code","0bc29178":"code","0f16ecb5":"code","f88af94c":"code","3cc97656":"code","e0c77fd7":"code","a9b8ba1e":"code","15d31355":"code","59b70fd8":"code","c6ff3895":"code","d5d7a379":"code","1156cf4d":"code","2752faa8":"code","0c463f26":"code","32743a75":"code","9d6575a0":"code","62b0b721":"code","0434473c":"code","c8a73331":"code","144dfbc0":"code","a20aa091":"code","f5a5accb":"code","047b04a1":"code","3ccb60cd":"code","96451218":"code","200d28e5":"code","d61e1ecc":"code","7c0afddf":"code","29690391":"code","bf8c0e6e":"code","7202df55":"code","0d0432ff":"code","d68ca219":"code","5469c162":"code","3b4dcdde":"code","4bfada00":"code","5eda2ce7":"code","1ce319b7":"code","020e6d48":"code","18a21340":"code","42e2371c":"code","8bd3457c":"markdown","c6a96d1d":"markdown","00a5040a":"markdown","e3eaa006":"markdown","5c2b8340":"markdown","aa4cb189":"markdown","787dded4":"markdown","52811d2c":"markdown","6aed4363":"markdown","f5d559fb":"markdown","3fbf94bd":"markdown","65a5783b":"markdown","cececa9b":"markdown","9672b26e":"markdown","495e8fbb":"markdown","21913481":"markdown","16487376":"markdown","05298325":"markdown"},"source":{"98655715":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc8e732e":"df=pd.read_csv(\"..\/input\/heart-disease-prediction-using-differenttechniques\/heart.csv\")","11681227":"df.head()","509a4a29":"df.tail()","35676b16":"df.shape","0d1cab12":"df.info()","ef5bf8b8":"df.describe()","08f8bdcb":"df2=df","979204fd":"#!pip install dtale","fe0275c9":"#import dtale","5c2539b6":"#dtale.show(df2)","0c2dc13b":"df.corr()","e2bec0df":"# firstly install klib\n#!pip install klib","ecb48880":"#import klib.describe ","80ce1089":"#df3=df","2772eed3":"#klib.corr_mat(df3)","aa109647":"#klib.corr_plot(df3)","e5b65341":"#klib.dist_plot(df3)","0bc29178":"#import klib.clean","0f16ecb5":"#klib.data_cleaning(df3)","f88af94c":"for i in df.columns:\n    for each in df[i].values:\n        \n        if each > 1 or each < -1:\n            df[i] = (df[i] - np.min(df[i]))\/(np.max(df[i]) - np.min(df[i]))\n        else:\n            pass","3cc97656":"y = df[\"target\"]\nX = df.drop([\"target\"],axis = 1)","e0c77fd7":"from sklearn.model_selection import train_test_split","a9b8ba1e":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25,random_state = 0)","15d31355":"y_train","59b70fd8":"print(X_train.shape)\nprint(y_train.shape)","c6ff3895":"from sklearn.metrics import r2_score, mean_squared_error, confusion_matrix, roc_curve, classification_report\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom collections import Counter\nfrom sklearn.metrics import r2_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","d5d7a379":"log_reg=LogisticRegression(random_state=0,max_iter=900)","1156cf4d":"log_reg.fit(X_train,y_train)","2752faa8":"y_pred=log_reg.predict(X_test)","0c463f26":"log_reg.score(X_test,y_test)","32743a75":"params = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\nlr_model = LogisticRegression(random_state = 0)\nlr_cv = GridSearchCV(lr_model,params,cv = 5).fit(X_train,y_train)\nlr_cv.best_params_","9d6575a0":"log_reg2 = LogisticRegression(C = 1,random_state = 0)\nlog_reg2.fit(X_train,y_train)","62b0b721":"log_reg2.predict(X_test)","0434473c":"log_reg2.score(X_test,y_test)","c8a73331":"knn = KNeighborsClassifier(n_neighbors = 2).fit(X_train,y_train)\nknn.score(X_test,y_test)","144dfbc0":"params = {\"n_neighbors\": range(1,50)}\nknn_model = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn_model, params, cv = 10, n_jobs = -1).fit(X_train,y_train)\nknn_cv.best_params_","a20aa091":"knn_tuned = KNeighborsClassifier(n_neighbors = 5).fit(X_train,y_train)\nknn_tuned.score(X_test,y_test)","f5a5accb":"tree = DecisionTreeClassifier(random_state = 0).fit(X_train,y_train)\ntree.score(X_test,y_test)","047b04a1":"params = {\"max_depth\": range(1,10),\n            \"min_samples_split\" : list(range(2,50))}\ntree_model = DecisionTreeClassifier(random_state = 0)\ntree_cv = GridSearchCV(tree_model, params, cv = 10, n_jobs = -1).fit(X_train,y_train)\ntree_cv.best_params_","3ccb60cd":"tree_tuned = DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 7, min_samples_split = 2,random_state = 42).fit(X_train,y_train)\ntree_tuned.score(X_test,y_test)","96451218":"rf = RandomForestClassifier(random_state = 0).fit(X_train,y_train)\nrf.score(X_test,y_test)","200d28e5":"params = {\"max_depth\": range(1,30),\n         \"min_samples_split\" : [2,3,5,6,7,10,11,15],\n         \"min_samples_leaf\": [2,5,7,8,9],\n         \"max_features\" : [2,3,5,7,9,12,15],\n         }\nrf_model = DecisionTreeClassifier(random_state = 0)\nrf_cv = GridSearchCV(rf_model, params, cv = 10, n_jobs = -1).fit(X_train,y_train)\nrf_cv.best_params_","d61e1ecc":"rf_tuned = RandomForestClassifier(max_depth = 5, max_features  = 9, min_samples_leaf = 8, min_samples_split = 2,random_state = 0).fit(X_train,y_train)\nrf_tuned.score(X_test,y_test)","7c0afddf":"svm = SVC(random_state = 42).fit(X_train,y_train)\nsvm.score(X_test,y_test)","29690391":"params = {\"C\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100],\n             \"gamma\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100]}\nsvm_model = SVC(random_state = 42)\nsvm_cv = GridSearchCV(svm_model,params,cv = 10,n_jobs = -1).fit(X_train,y_train)\nsvm_cv.best_params_","bf8c0e6e":"svm_tuned = SVC(C = 10, degree = 1, gamma = 0.1, random_state = 42).fit(X_train,y_train)\nsvm_tuned.score(X_test,y_test)","7202df55":"gbm = GradientBoostingClassifier(random_state = 42).fit(X_train,y_train)\ngbm.score(X_test,y_test)","0d0432ff":"gbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,1000],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\ngbm_model = GradientBoostingClassifier(random_state = 42)\ngbm_cv = GridSearchCV(gbm_model, gbm_params, cv = 10, n_jobs = -1).fit(X_train,y_train)\ngbm_cv.best_params_","d68ca219":"gbm_tuned = GradientBoostingClassifier(max_depth = 2, learning_rate = 0.01, min_samples_split = 2, n_estimators = 100, random_state = 42).fit(X_train,y_train)\ngbm_tuned.score(X_test,y_test)","5469c162":"ada = AdaBoostClassifier(random_state = 42).fit(X_train,y_train)\nada.score(X_test,y_test)","3b4dcdde":"params = {\"n_estimators\":[10,100,200,300,500,1000],\"learning_rate\":[0.0001,0.001,0.01,0.1,0.2,0.3,0.7]}\nada_model = AdaBoostClassifier(random_state = 42)\nada_cv = GridSearchCV(ada_model,params,cv = 10,n_jobs = -1).fit(X_train,y_train)\nada_cv.best_params_","4bfada00":"ada_tuned = AdaBoostClassifier(learning_rate = 0.01,n_estimators = 1000,random_state = 42).fit(X_train,y_train)\nada_tuned.score(X_test,y_test)","5eda2ce7":"bag = BaggingClassifier(random_state = 42).fit(X_train,y_train)\nbag.score(X_test,y_test)","1ce319b7":"params = {\"n_estimators\": range(1,50)}\nbag_model = BaggingClassifier(random_state = 42)\nbag_cv = GridSearchCV(bag_model, params, cv = 10, n_jobs = -1).fit(X_train,y_train)\nbag_cv.best_params_","020e6d48":"bag_tuned = BaggingClassifier(n_estimators = 45,random_state = 42).fit(X_train,y_train)\nbag_tuned.score(X_test,y_test)","18a21340":"from matplotlib import pyplot as plt\nimport seaborn as sns","42e2371c":"pred_list = [log_reg2,knn_tuned,tree_tuned,rf_tuned,gbm_tuned,svm_tuned,ada_tuned,bag_tuned]\n\nfor i in pred_list:\n    print(\"Score : \",i.score(X_test,y_test))\n    y_pred = i.predict(X_test)\n    sns.heatmap(confusion_matrix(y_test,y_pred),annot = True)\n    plt.xlabel(\"Y_pred\")\n    plt.ylabel(\"Y_test\")\n    plt.title(i)\n    plt.show()","8bd3457c":"# SVM","c6a96d1d":"# dtale lib is very help full for automation EDA and very fast","00a5040a":"# Random Forest","e3eaa006":"# GBM","5c2b8340":"# Now Use Klib lib for Check for data and also See Plots and Preprossing.\n# This Lib also help to perform auto EDA and its also very quick and fast lib to perform some specific task","aa4cb189":"# Normalization","787dded4":"# Now Import Some important libs","52811d2c":"# Logistic Regression","6aed4363":"# Machine Learning (Classification)\nLogistic Regression\n\nKNN(K Neighbors)\n\nDecision Tree\n\nRandom Forests\n\nSVM\n\nGBM (Gradient Boosting Machine)\n\nAdaBoost\n\nBagging","f5d559fb":"# Bagging","3fbf94bd":"# Now Train Test Split","65a5783b":"# Now check Correlation","cececa9b":"# Adaboost","9672b26e":"# Decision Tree","495e8fbb":"# Compare Algorithms","21913481":"# KNN (K Neighbors)","16487376":"# klib.describe - functions for visualizing datasets\n- klib.cat_plot(df) # returns a visualization of the number and frequency of categorical features\n- klib.corr_mat(df) # returns a color-encoded correlation matrix\n- klib.corr_plot(df) # returns a color-encoded heatmap, ideal for correlations\n- klib.dist_plot(df) # returns a distribution plot for every numeric feature\n- klib.missingval_plot(df) # returns a figure containing information about missing values\n\n# klib.clean - functions for cleaning datasets\n- klib.data_cleaning(df) # performs datacleaning (drop duplicates & empty rows\/cols, adjust dtypes,...)\n- klib.clean_column_names(df) # cleans and standardizes column names, also called inside data_cleaning()\n- klib.convert_datatypes(df) # converts existing to more efficient dtypes, also called inside data_cleaning()\n- klib.drop_missing(df) # drops missing values, also called in data_cleaning()\n- klib.mv_col_handling(df) # drops features with high ratio of missing vals based on informational content\n- klib.pool_duplicate_subsets(df) # pools subset of cols based on duplicates with min. loss of information\n\n# klib.preprocess - functions for data preprocessing (feature selection, scaling, ...)\n- klib.train_dev_test_split(df) # splits a dataset and a label into train, optionally dev and test sets\n- klib.feature_selection_pipe() # provides common operations for feature selection\n- klib.num_pipe() # provides common operations for preprocessing of numerical data\n- klib.cat_pipe() # provides common operations for preprocessing of categorical data\n- klib.preprocess.ColumnSelector() # selects num or cat columns, ideal for a Feature Union or Pipeline\n- klib.preprocess.PipeInfo() # prints out the shape of the data at the specified step of a Pipeline","05298325":"# GridSearchCV"}}