{"cell_type":{"ba4b9c8a":"code","1335506e":"code","7255ef7f":"code","95dda4b5":"code","149879fe":"code","121fcbfa":"code","f15c9597":"code","4fae27d8":"code","b92843b8":"code","5566c029":"code","7def0657":"code","4c0ddcf4":"code","d11cf08d":"code","1c27e894":"code","60d5278e":"code","9cbd7a49":"code","e6edc810":"code","ff2f8f2d":"code","28f60aa7":"code","a52c71ad":"code","ac9f4cf7":"code","dbee64ce":"code","6e1e1700":"code","e42341a2":"code","6962ba63":"code","91d24fad":"code","a46d1c8c":"code","c13a12af":"code","8b447661":"code","441401ce":"code","dfb21473":"code","0c4a6abe":"code","da8b0919":"code","8020b523":"code","abccc80b":"code","f8925c1d":"code","554733f0":"code","e6a14ac8":"code","f2113cc2":"code","59229f6c":"code","45b0da6f":"code","23e53c29":"code","158db402":"code","caf50469":"markdown","cd547d44":"markdown","09be7a4d":"markdown","e55685f9":"markdown","98b319db":"markdown","a9c0ede3":"markdown","0fdfe4bc":"markdown","c1c51dbb":"markdown","e313b5dd":"markdown","7eb3e650":"markdown","e6ff2fe4":"markdown"},"source":{"ba4b9c8a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split,GridSearchCV  \nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom bayes_opt import BayesianOptimization\nfrom lightgbm import LGBMModel,LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport gc \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))","1335506e":"traindata = pd.read_csv(\"..\/input\/train.csv\")\ntestdata = pd.read_csv(\"..\/input\/test.csv\")\nprint(traindata.shape)\nprint(testdata.shape)","7255ef7f":"traindata.head()","95dda4b5":"testdata.head()","149879fe":"\nmaster_test_id = testdata['ID_code']\n\ntraindata.drop(['ID_code'],axis=1,inplace=True)\ntestdata.drop(['ID_code'],axis=1,inplace=True)","121fcbfa":"a = traindata[traindata.target == 1].target.sum()\nprint('Percentage of target variables with label =1 is: ',a*100\/traindata.shape[0])","f15c9597":"uni = (traindata.nunique()).sort_values()\nprint(uni)\n\n#here we see that there is no variable which is binary.","4fae27d8":"#checked correlation for all but no luck\n\n'''\ncorrmat = traindata.iloc[:,1:199].corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);\n'''","b92843b8":"#checked for duplicated rows but no duplicates found\n\n'''\n\nsam = traindata.append(testdata,sort=False)\nsam.drop('target',axis=1,inplace=True)\n\ndf = sam[sam.duplicated()]  # checks duplicate rows considering all columns\ndf\n'''","5566c029":"traindata.head(3)","7def0657":"testdata.head(2)","4c0ddcf4":"y_master_train = traindata['target']\n#traindata.drop(['target'],axis=1,inplace=True)\ntestdata['target'] = 'test_data'\ntotaldata = pd.concat([traindata, testdata])  #concatenation will automatically match columns and append\ntotaldata.head()","d11cf08d":"totaldata.shape","1c27e894":"y_master_train.value_counts(normalize=True)   #checking proportion of different ratings","60d5278e":"#Plotting boxplots of 5 variables\nm=1\nplt.figure(figsize = (20,20))\nfor i in totaldata.columns[1:6]:\n    plt.subplot(3,4,m)\n    sns.boxplot(totaldata[i])\n    m = m+1","9cbd7a49":"a=list(totaldata.columns)\na.remove(\"target\")\n\ndef outlier_treatment(data,cols):\n    data_X = data.copy()\n    \n    for i in cols:\n        a = data_X[data_X['target']!='test_data'][i].quantile([0.25,0.75]).values  #doing only on train data\n        p_cap = a[1] + 1.5*(a[1]-a[0])\n        p_clip = a[0] - 1.5*(a[1]-a[0])\n        data_X[i][data_X[i] <= p_clip] = p_clip\n        data_X[i][data_X[i] >= p_cap] = p_cap\n    \n  \n    return data_X\n\n#totaldata = outlier_treatment(totaldata,a)","e6edc810":"from scipy.stats import skew\ndef skew_treatment(data):\n    data_X = data.copy()\n    #finding skewness of all variables\n    col = data_X.columns\n    skewed_feats = data_X[col].apply(lambda x: skew(x.dropna()))\n    #adjusting features having skewness >0.75\n    skewed_feats = skewed_feats[skewed_feats > 0.75]\n    skewed_feats = skewed_feats.index\n    data_X[skewed_feats] = np.log1p(data_X[skewed_feats])\n    \n    return data_X","ff2f8f2d":"#totaldata.iloc[:,1:] = skew_treatment(totaldata.iloc[:,1:])","28f60aa7":"from sklearn.model_selection import train_test_split\n\ntrain_data = (totaldata[totaldata['target']!='test_data']).drop(['target'],axis=1)\ntest_data = (totaldata[totaldata['target']=='test_data']).drop(['target'],axis=1)\n#  split X between training and testing set\nx_train, x_test, y_train, y_test = train_test_split(train_data,y_master_train, test_size=0.25, shuffle=True,stratify=y_master_train)","a52c71ad":"'''\n\n#PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=50)\npca = pca.fit(x_train)\nprincipalComponents_train = pca.transform(x_train)\nprincipalComponents_test = pca.transform(x_test)\nx_train_pca = pd.DataFrame(principalComponents_train)\nx_test_pca = pd.DataFrame(principalComponents_test)\n\n'''\n\n#Not using PCA as it decreased the AUC score from 89.4 to 72 with all other things kept constant","ac9f4cf7":"del totaldata\ndel traindata\ndel testdata\ndel uni\ngc.collect();","dbee64ce":"del train_data\ndel test_data\ngc.collect();","6e1e1700":"y_train.value_counts(normalize=True)","e42341a2":"def model2_lgbm(num_leaves,  # int\n    learning_rate,  \n    feature_fraction,\n    lambda_l1,\n    lambda_l2,\n    min_gain_to_split,\n    max_depth,n_estimators):\n    \n    clf = LGBMClassifier( n_estimators=int(n_estimators),\n                         num_leaves = int(num_leaves),\n                         learning_rate=learning_rate,\n                         feature_fraction=feature_fraction,\n                         lambda_l1=lambda_l1,\n                        lambda_l2=lambda_l2,\n                        min_gain_to_split=min_gain_to_split,\n                        max_depth=int(max_depth),\n                     eval_metric='auc'\n                     \n            )        \n\n    clf.fit(x_train, y_train, \n                eval_set=[(x_test,y_test)],early_stopping_rounds=200,eval_metric='auc',verbose=False\n               )\n    \n    a = clf.best_score_['valid_0']['auc']\n    \n    return a","6962ba63":"bounds_lgbm = {\n    'max_depth': (10, 15),\n    'num_leaves':(5,40),\n    'learning_rate':(0.01,0.1),\n    'feature_fraction':(0.7,1),\n    'lambda_l1': (0, 8.0), \n    'lambda_l2': (0, 8.0), \n    'min_gain_to_split': (0, 1.0),\n    'n_estimators':(2000,5000)\n}","91d24fad":"from bayes_opt import BayesianOptimization\n\nLGB_BO = BayesianOptimization(model2_lgbm, bounds_lgbm)\n\ninit_points = 5\nn_iter = 15\n\n\nprint('-' * 130)\n\nLGB_BO.maximize(init_points=init_points, n_iter=n_iter)","a46d1c8c":"print(LGB_BO.max['target'])\nprint(LGB_BO.max['params'])","c13a12af":"sdsd","8b447661":"def model_1_catb( iterations,learning_rate,depth,l2_leaf_reg):\n        catb = CatBoostClassifier(\n            iterations=int(iterations),\n           #cat_features=cat_col,\n            learning_rate=learning_rate,\n            depth = int(depth),\n            l2_leaf_reg = l2_leaf_reg,\n            #subsample=subsample,  #can't be trained for catboost using bayesian opt\n            early_stopping_rounds=50,\n          #  colsample_bylevel = colsample_bylevel,  #can't be trained on GPU(but only on CPU) for catboost using bayesian opt\n          # max_leaves = int(max_leaves),  can't be trained on CPU\n            eval_metric='AUC',\n           task_type='GPU',\n           #verbose=30\n        )\n        catb.fit(\n            x_train, y_train,\n            eval_set=(x_test, y_test),verbose=10\n        )\n       # print('Model is fitted: ' + str(catb.is_fitted()))\n        #print('Model params:')\n        #print(catb.get_params())\n        a = catb.get_best_score()\n        return a['validation_0']['AUC']","441401ce":"bounds_catb = {\n    'iterations': (30, 150), \n    'learning_rate': (0.05, 0.9),  \n    'depth': (6, 15),\n    'l2_leaf_reg': (0,5),    \n  # 'subsample': (0.75,1),\n   # 'colsample_bylevel': (0.75, 1), \n   # 'max_leaves': (5,40)\n}","dfb21473":"from bayes_opt import BayesianOptimization\n\nCATB_BO = BayesianOptimization(model_1_catb, bounds_catb)\n\ninit_points = 3\nn_iter = 10\n\n\nprint('-' * 130)\n\n#CATB_BO.maximize(init_points=init_points, n_iter=n_iter)\n\nprint(CATB_BO.max['target'])\nprint(CATB_BO.max['params'])","0c4a6abe":"# NOTE: Here I have shown how to make prediction on test set made from train data split\n# To make prediction on unknown test set just replace X_train with full train data and \n# X_test with unknown test data\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom vecstack import stacking     #this is a library made for stacking by a Kaggler\nfrom catboost import CatBoostClassifier\n\nmodels = [\n    #KNeighborsClassifier(n_neighbors=5,\n                       # n_jobs=-1),\n    CatBoostClassifier(\n    learning_rate=0.05,\n    depth = 10,\n    rsm = 0.7, loss_function = 'Logloss', logging_level='Verbose', eval_metric='AUC',iterations = 300,),\n        \n    #RandomForestClassifier(random_state=0, n_jobs=-1, \n                           #n_estimators=100, max_depth=3),\n        \n    XGBClassifier(n_estimators=2000, reg_alpha = 0.01, objective= 'rank:pairwise',silent=False)\n]\n\n\nS_train, S_test = stacking(models,                   \n                           x_train, y_train, x_test,   \n                           regression=False, \n     \n                           mode='oof_pred_bag', \n       \n                           needs_proba=False,\n         \n                           save_dir=None, \n            \n                           metric=roc_auc_score, \n    \n                           n_folds=4, \n                 \n                           stratified=True,\n            \n                           shuffle=True)\n\n#meta model\nmodel = XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n                      n_estimators=100, max_depth=3)\n    \nmodel = model.fit(S_train, y_train)\ny_pred = model.predict(S_test)\nprint('Final prediction score: [%.8f]' % roc_auc_score(y_test, y_pred))","da8b0919":"a = np.ravel(y_test)\nb = np.ravel(y_pred)\nroc_auc_score(a,b)","8020b523":"from catboost import CatBoostClassifier\n\nmodel = CatBoostClassifier(\n    random_seed=63,\n    iterations=300,\n    learning_rate=0.05,\n    depth=10,\n    loss_function='Logloss',\n    rsm = 0.7,\n    od_type='Iter',\n    od_wait=20,\n    eval_metric = 'AUC',\n)\nmodel.fit(\n    x_train, y_train,\n    logging_level='Silent',\n    eval_set=(x_test, y_test),\n    plot=True\n)","abccc80b":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nxgb = XGBClassifier(n_estimators=2000, reg_alpha = 0.01)\nrf = RandomForestClassifier()\nextraT = ExtraTreesClassifier()\n\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n    \n    \nstacked_averaged_models = StackingAveragedModels(base_models = (extraT, rf, xgb),\n                                                 meta_model = lasso)","f8925c1d":"#stacked_averaged_models.fit(x_train, y_train)\n#y_pred = stacked_averaged_models.predict(x_test)","554733f0":"def Stacking(model,train,y,test,n_fold):\n    folds=StratifiedKFold(n_splits=n_fold,random_state=1)\n    test_pred=np.empty((test.shape[0],1),float)\n    train_pred=np.empty((0,1),float)\n    for train_indices,val_indices in folds.split(train,y.values):\n        x_train,x_val=train.iloc[train_indices],train.iloc[val_indices]\n        y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]\n\n        model.fit(X=x_train,y=y_train)\n        train_pred=np.append(train_pred,model.predict(x_val))\n        test_pred=np.append(test_pred,model.predict(test))\n    return test_pred.reshape(-1,1),train_pred","e6a14ac8":"#lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nxgb = XGBClassifier(n_estimators=100, reg_alpha = 0.01)\nrf = RandomForestClassifier()\n\n\ntest_pred1 ,train_pred1=Stacking(model=xgb,n_fold=5, train=x_train,test=x_test,y=y_train)\n\ntrain_pred1=pd.DataFrame(train_pred1)\ntest_pred1=pd.DataFrame(test_pred1)","f2113cc2":"test_pred2 ,train_pred2=Stacking(model=rf,n_fold=10,train=x_train,test=x_test,y=y_train)\n\ntrain_pred2=pd.DataFrame(train_pred2)\ntest_pred2=pd.DataFrame(test_pred2)","59229f6c":"df = pd.concat([train_pred1, train_pred2], axis=1)\ndf_test = pd.concat([test_pred1, test_pred2], axis=1)\n\nmodel = ExtraTreesClassifier()\nmodel.fit(df,y_train)\nmodel.score(df_test, y_test)","45b0da6f":"roc_auc_score(y_test, lgb_pred)","23e53c29":"y_pred = clf.predict(test_data)","158db402":"sub = pd.DataFrame(data = testid,columns =['ID_code'])\nsub['target'] = y_pred\nsub.to_csv('submission.csv', index=False)","caf50469":"# Catboost bayesian optimization","cd547d44":"\n#following params are from 1st run of bayesian optimization\nparam = {\n    'feature_fraction': 0.8197428551123196, 'lambda_l1': 7.075054502660179, 'lambda_l2': 7.820448238204753,\n    'learning_rate': 0.05831167983832596, \n    'max_depth': 14.497149517724528, 'min_gain_to_split': 0.31541832302278316, \n    'n_estimators': 2778.5508893048313, 'num_leaves': 5.258308295984117\n}\nclf = LGBMClassifier( n_estimators=int(param['n_estimators']),\n                         num_leaves = int(param['num_leaves']),\n                         learning_rate=param['learning_rate'],\n                         feature_fraction=param['feature_fraction'],\n                         lambda_l1=param['lambda_l1'],\n                        lambda_l2=param['lambda_l2'],\n                        min_gain_to_split=param['min_gain_to_split'],\n                        max_depth=int(param['max_depth']),\n                     eval_metric='auc'\n                    )        \n\nclf.fit(x_train, y_train, \n            eval_set=[(x_test,y_test)],early_stopping_rounds=200,eval_metric='auc',verbose=False\n           )\n\na = clf.best_score_['valid_0']['auc']\nprint(a)","09be7a4d":"# Light GBM standalone","e55685f9":"Modelling:","98b319db":"Used XGB, got 0.889 auc score. Commenting now to end execution of whole program faster.","a9c0ede3":"#reducing y=0 labels from training set\ntotaldata = totaldata.reset_index(drop=True)\ny_master_train = y_master_train.reset_index(drop=True)\n\n#get training data and then shuffle and get some random permutation of observations\nntrain = int(y_master_train.shape[0])\nremove_n = int(ntrain*0.6)\ndrop_indices = np.random.choice(y_master_train[y_master_train==0].index, remove_n, replace=False)\nprint('Shape of training data before dropping rows having 0 labels: ', y_master_train.shape)\ntotaldata = totaldata.drop(drop_indices, axis=0)\ny_master_train = y_master_train.drop(drop_indices)\nprint('Shape of training data after dropping rows having 0 labels: ',y_master_train.shape)\n\n#checking proportion of different classes in y\ny_master_train.value_counts(normalize=True)","0fdfe4bc":"# Things I tried and few learnings:\n\n1) Hyper-parameter tuning is very important in this exercise, since there is nothing much that can be done on data processing part\n\n2) I tried PCA, undersampling and outlier treatment, but didn't give any significant improvement.\n\n3) I tried NN, Light GBM, Catboost, XGBoost models of which Light GBM and Catboost were top two. So I tried tuning their parameters using Bayesian optimization. \n* **Catboost:** Very few parameters of Catboost can be trained using Bayesian optimization due to internal creation of process. Without GPU, the run time was very large. SO I tried using GPU and only 3-4 params can be put in Bayesian optimization using that. It didn't give as good AUC as light GBM.\n* **Light GBM:** It can be run without GPU and run time is also less. Tried many hyper-parameters under tuning and got combination which gave 89.4 AUC score on validation data during hyp-param tuning. Number of estimators was one important param.\n\n","c1c51dbb":"Different variables have different scaling and are very slightly skewed. We wil apply transformation to variables having skewness > 0.75","e313b5dd":"# Light GBM bayesian optimization","7eb3e650":"I tried both oversampling and skewness treatment but the model was not performing any better so I am not using them in final code. It is written above if you want to try that piece of code.","e6ff2fe4":"Now since the data is unbalanced, we can try oversampleing and undersampling:\n    1. Undersampling\nI  am not trying oversamling since the data is already huge, and oversamling will slow down the entire excecution."}}