{"cell_type":{"106403d6":"code","58c9bbb3":"code","b4b62307":"code","b658adbd":"code","79dd40f7":"code","27c90c1c":"code","275b275b":"code","8e239c92":"code","3fdb8602":"code","ddbaf3ac":"code","750403e5":"code","8f03f2d3":"code","6050bda0":"code","5c765edf":"code","28f93441":"code","2c77ebe6":"code","5b1274b0":"code","75dcda66":"markdown","ce9173e2":"markdown","f19b8332":"markdown","759190c2":"markdown","67f6859b":"markdown","ccd3216a":"markdown","882484ac":"markdown","4ecd1fe6":"markdown","49e258be":"markdown","43284468":"markdown","463686b6":"markdown","b225c6c4":"markdown","ad34d69d":"markdown","45f10b7e":"markdown","aa57e4c7":"markdown","c8de365f":"markdown","37381399":"markdown"},"source":{"106403d6":"import pandas as pd\nimport numpy as np\nimport pickle\nimport itertools\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport dateutil.easter as easter\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom datetime import datetime, date, timedelta\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold, TimeSeriesSplit\nfrom sklearn.linear_model import LinearRegression, HuberRegressor, Ridge, Lasso\n","58c9bbb3":"original_train_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\noriginal_test_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n\ngdp_df.set_index('year', inplace=True)\n\n# The dates are read as strings and must be converted\nfor df in [original_train_df, original_test_df]:\n    df['date'] = pd.to_datetime(df.date)\noriginal_train_df.head(2)","b4b62307":"def smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) \/ (y_true + np.abs(y_pred)) * 200\n","b658adbd":"# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n        \n    new_df = pd.DataFrame({'gdp': np.log(df.apply(get_gdp, axis=1)),\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 3):\n        new_df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'hat_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Hat']\n        new_df[f'hat_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Hat']\n\n    return new_df\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer(original_test_df)\n\nfeatures = test_df.columns\n\nfor df in [train_df, test_df]:\n    df[features] = df[features].astype(np.float32)\nprint(list(features))","79dd40f7":"def fit_model(X_tr, X_va=None, outliers=False):\n    \"\"\"Scale the data, fit a model, plot the training history and validate the model\"\"\"\n    start_time = datetime.now()\n\n    # Preprocess the data\n    X_tr_f = X_tr[features]\n    preproc = StandardScaler()\n    X_tr_f = preproc.fit_transform(X_tr_f)\n    y_tr = X_tr.num_sold.values.reshape(-1, 1)\n    \n    # Train the model\n    #model = LinearRegression()\n    #model = HuberRegressor(epsilon=1.20, max_iter=500)\n    model = Ridge(alpha=1.0)\n    model.fit(X_tr_f, np.log(y_tr).ravel())\n\n    if X_va is not None:\n        # Preprocess the validation data\n        X_va_f = X_va[features]\n        X_va_f = preproc.transform(X_va_f)\n        y_va = X_va.num_sold.values.reshape(-1, 1)\n\n        # Inference for validation\n        y_va_pred = np.exp(model.predict(X_va_f)).reshape(-1, 1)\n        oof.update(pd.Series(y_va_pred.ravel(), index=X_va.index))\n        \n        # Evaluation: Execution time and SMAPE\n        smape_before_correction = np.mean(smape_loss(y_va, y_va_pred))\n        #y_va_pred *= LOSS_CORRECTION\n        smape = np.mean(smape_loss(y_va, y_va_pred))\n        print(f\"Fold {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]}\"\n              f\" | SMAPE: {smape:.5f}   (before correction: {smape_before_correction:.5f})\")\n        score_list.append(smape)\n        \n        # Plot y_true vs. y_pred\n        if fold == 0:\n            plt.figure(figsize=(10, 10))\n            plt.scatter(y_va, y_va_pred, s=1, color='r')\n            #plt.scatter(np.log(y_va), np.log(y_va_pred), s=1, color='g')\n            plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n            plt.gca().set_aspect('equal')\n            plt.xlabel('y_true')\n            plt.ylabel('y_pred')\n            plt.title('OOF Predictions')\n            plt.show()\n        \n    return preproc, model\n\npreproc, model = fit_model(train_df)\n\n# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\ndef plot_five_years_combination(engineer, country='Norway', store='KaggleMart', product='Kaggle Hat'):\n    demo_df = pd.DataFrame({'row_id': 0,\n                            'date': pd.date_range('2015-01-01', '2019-12-31', freq='D'),\n                            'country': country,\n                            'store': store,\n                            'product': product})\n    demo_df.set_index('date', inplace=True, drop=False)\n    demo_df = engineer(demo_df)\n    demo_df['num_sold'] = np.exp(model.predict(preproc.transform(demo_df[features])))\n    plt.figure(figsize=(20, 6))\n    plt.plot(np.arange(len(demo_df)), demo_df.num_sold, label='prediction')\n    train_subset = train_df[(original_train_df.country == country) & (original_train_df.store == store) & (original_train_df['product'] == product)]\n    plt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\n    plt.legend()\n    plt.title('Predictions and true num_sold for five years')\n    plt.show()\n\nplot_five_years_combination(engineer)","27c90c1c":"train_df['pred'] = np.exp(model.predict(preproc.transform(train_df[features])))\nby_date = train_df.groupby(train_df['date'])\nresiduals = (by_date.pred.sum() - by_date.num_sold.sum()) \/ (by_date.pred.sum() + by_date.num_sold.sum()) * 200\n\n# Plot all residuals (four-year range, sum of all products)\ndef plot_all_residuals(residuals):\n    plt.figure(figsize=(20,6))\n    plt.scatter(residuals.index,\n                residuals,\n                s=1, color='k')\n    plt.vlines(pd.date_range('2015-01-01', '2019-01-01', freq='M'),\n               plt.ylim()[0], plt.ylim()[1], alpha=0.5)\n    plt.vlines(pd.date_range('2015-01-01', '2019-01-01', freq='Y'),\n               plt.ylim()[0], plt.ylim()[1], alpha=0.5)\n    plt.title('Residuals for four years')\n    plt.show()\n    \nplot_all_residuals(residuals)\n\n# Plot residuals for interesting intervals\ndef plot_around(residuals, m, d, w):\n    \"\"\"Plot residuals in an interval of with 2*w around month=m and day=d\"\"\"\n    plt.figure()\n    plt.title(f\"Residuals around m={m} d={d}\")\n    for y in np.arange(2015, 2020):\n        d0 = pd.Timestamp(date(y, m, d))\n        residual_range = residuals[(residuals.index > d0 - timedelta(w)) & \n                                   (residuals.index < d0 + timedelta(w))]\n        plt.plot([(r - d0).days for r in residual_range.index], residual_range, label=str(y))\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True)) # only integer labels\n    plt.legend()\n    plt.show()\n\nplot_around(residuals, 1, 1, 20) # end of year peak\nplot_around(residuals, 5, 1, 50) # three moveable peaks depending on Easter\n#plot_around(residuals, 5, 21, 10) # zoom-in\n#plot_around(residuals, 5, 31, 15) # zoom-in\nplot_around(residuals, 6, 10, 10) # first half of June (with overlay of Pentecost in 2017)\nplot_around(residuals, 6, 30, 10) # moveable peak end of June\nplot_around(residuals, 11, 5, 10) # moveable peak beginning of November","275b275b":"# Feature engineering for holidays\ndef engineer_more(df):\n    \"\"\"Return a new dataframe with more engineered features\"\"\"\n    new_df = engineer(df)\n\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"n-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"f-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in range(1, 14)}),\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(1, 10)}),\n                        pd.DataFrame({f\"s-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in range(1, 15)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10))}), #  + list(range(17, 25))\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in list(range(19, 26))})],\n                       axis=1)\n    \n    # June and July\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in list(range(8, 14))}),\n                        #pd.DataFrame({f\"june{d}\":\n                        #              (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Norway')\n                        #              for d in list(range(22, 31))}),\n                        #pd.DataFrame({f\"july{d}\":\n                        #              (df.date.dt.month == 7) & (df.date.dt.day == d) & (df.country == 'Norway')\n                        #              for d in list(range(1, 3))})],\n                       ],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(-4, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(0, 9))})],\n                       axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in list(range(6, 14))})],\n                       axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})],\n                       axis=1)\n    \n    return new_df.astype(np.float32)\n\ntrain_df = engineer_more(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer_more(original_test_df)\n\nfeatures = list(test_df.columns)\nprint(list(features))\n","8e239c92":"preproc, model = fit_model(train_df)\ntrain_df['pred'] = np.exp(model.predict(preproc.transform(train_df[features])))\nwith open('train_pred.pickle', 'wb') as handle: pickle.dump(train_df.pred, handle) # save residuals for further analysis\nby_date = train_df.groupby(train_df['date'])\nresiduals = (by_date.pred.sum() - by_date.num_sold.sum()) \/ (by_date.pred.sum() + by_date.num_sold.sum()) * 200\n\n# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\nplot_five_years_combination(engineer_more)\n\n# Plot all residuals (four-year range, sum of all products)\nplot_all_residuals(residuals)\n\n# Plot residuals for interesting intervals\nplot_around(residuals, 1, 1, 20) # end of year peak\nplot_around(residuals, 5, 1, 50) # three moveable peaks depending on Easter\n#plot_around(residuals, 5, 21, 10) # zoom-in\n#plot_around(residuals, 5, 31, 15) # zoom-in\nplot_around(residuals, 6, 10, 10) # first half of June (with overlay of Pentecost in 2017)\nplot_around(residuals, 6, 30, 10) # moveable peak end of June\nplot_around(residuals, 11, 5, 10) # moveable peak beginning of November\n","3fdb8602":"residuals = np.log(train_df.pred) - np.log(train_df.num_sold)\nplt.figure(figsize=(18, 4))\nplt.scatter(np.arange(len(residuals)), residuals, s=1)\nplt.title('All residuals by row number')\nplt.ylabel('residual')\nplt.show()\nplt.figure(figsize=(18, 4))\nplt.hist(residuals, bins=200)\nplt.title('Histogram of all residuals')\nplt.show()\nprint(f\"Standard deviation of log residuals: {residuals.std():.3f}\")\n","ddbaf3ac":"\ntrain_df['dayfix'] = train_df.date.dt.dayofyear\ntrain_df.loc[(train_df.date.dt.year != 2016) & (train_df.date.dt.month >= 3), 'dayfix'] += 1\n\nfrom scipy.stats import norm\nprint(\"Look for residuals beyond\", norm.ppf([0.5\/365, 364.5\/365]))\n\nrr = residuals.groupby(train_df.dayfix).mean()\nrrstd = rr.std()\nprint(f\"Standard deviation when grouped by dayofyear: {rrstd:.5f}\")\nrrdf = pd.DataFrame({'residual': rr, 'z_score': rr \/ rrstd, 'date': pd.date_range('2016-01-01', '2016-12-31')})\nrrdf[rrdf.z_score.abs() > 3]","750403e5":"# Candidate country-specific holidays\nrr = residuals.groupby([original_train_df.country, train_df.dayfix]).mean()\nrrstd = rr.std()\nprint(f\"Standard deviation when grouped by country and dayofyear: {rrstd:.5f}\")\nrrdf = pd.DataFrame({'residual': rr, 'z_score': rr \/ rrstd, 'date': np.datetime64('2015-12-31') + pd.to_timedelta(rr.index.get_level_values(1), 'D')})\nrrdf[rrdf.z_score.abs() > 3]","8f03f2d3":"#%%time\nRUNS = 1 # should be 1. increase the number of runs only if you want see how the result depends on the random seed\nOUTLIERS = True\nTRAIN_VAL_CUT = datetime(2018, 1, 1)\nLOSS_CORRECTION = 1\n\n# Make the results reproducible\nnp.random.seed(202100)\n\ntotal_start_time = datetime.now()\noof = pd.Series(0.0, index=train_df.index)\nscore_list = []\nfor run in range(RUNS):\n    #kf = GroupKFold(n_splits=4)\n    kf = TimeSeriesSplit(n_splits=4)\n    #for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, groups=train_df.date.dt.year)):\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n        X_tr = train_df.iloc[train_idx]\n        X_va = train_df.iloc[val_idx]\n        print(f\"Fold {run}.{fold}\")\n        preproc, model = fit_model(X_tr, X_va)\n\nprint(f\"Average SMAPE: {sum(score_list) \/ len(score_list):.5f}\")\nwith open('oof.pickle', 'wb') as handle: pickle.dump(oof, handle)","6050bda0":"# Fit the model on the complete training data\ntrain_idx = np.arange(len(train_df))\nX_tr = train_df.iloc[train_idx]\npreproc, model = fit_model(X_tr, None)\n\nplot_five_years_combination(engineer_more) # Quick check for debugging\n\n# Inference for test\ntest_pred_list = []\ntest_pred_list.append(np.exp(model.predict(preproc.transform(test_df[features]))) * LOSS_CORRECTION)\n\n# Create the submission file\nsub = original_test_df[['row_id']].copy()\nsub['num_sold'] = sum(test_pred_list) \/ len(test_pred_list)\nsub.to_csv('submission_linear_model.csv', index=False)\n\n# Plot the distribution of the test predictions\nplt.figure(figsize=(16,3))\nplt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, label='Training')\nplt.hist(sub['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, rwidth=0.5, label='Test predictions')\nplt.xlabel('num_sold')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\nsub","5c765edf":"# Create a rounded submission file\nsub_rounded = sub.copy()\nsub_rounded['num_sold'] = sub_rounded['num_sold'].round()\nsub_rounded.to_csv('submission_linear_model_rounded.csv', index=False)\nsub_rounded","28f93441":"w = pd.Series(model.coef_, features) # weights of the linear regression\nws = w \/ preproc.scale_ # weight as it would be applied to the original feature (before scaling)\n\ndef plot_feature_weights_numbered(prefix):\n    prefix_features = [f for f in features if f.startswith(prefix)]\n    plt.figure(figsize=(12, 2))\n    plt.bar([int(f[len(prefix):]) for f in prefix_features], ws[prefix_features])\n    plt.title(f'Feature weights for {prefix}')\n    plt.ylabel('weight')\n    plt.xlabel('day')\n    plt.show()\n    \nplot_feature_weights_numbered('easter')\nplot_feature_weights_numbered('dec')\nplot_feature_weights_numbered('jan')\n","2c77ebe6":"ws_sorted = ws.iloc[np.argsort(-np.abs(ws))]\nws_plot = ws_sorted.head(30)\n\nplt.figure(figsize=(9, len(ws_plot) \/ 3))\nplt.barh(np.arange(len(ws_plot)), ws_plot, color=ws_plot.apply(lambda ws: 'b' if ws >= 0 else 'y'))\nplt.yticks(np.arange(len(ws_plot)), ws_plot.index)\nplt.gca().invert_yaxis()\nplt.title('Most important features')\nplt.show()","5b1274b0":"gdp_exponent = ws['gdp']\ngdp_exponent","75dcda66":"Results of the sensitivity analysis with respect to the regularization parameter alpha are shown below. Note that higher alpha will lead to a less sensitivity of the target on the feature change:\n\nalpha=1: CV=4.33623\n\nalpha=10: CV=4.30701\n\nalpha=100: CV=5.23354\n\nalpha=20: CV=4.35293\n\nalpha=11: CV=4.30860\n\nalpha=5: CV=4.31184\n\nalph=9: CV=4.30614\n\n**alpha=8: CV=4.30613**\n\nalpha=7: CV=4.30696\n\nConsider TimeSeriesSplit instead of GroupKFold\n\nalpha=1: CV=4.94179\n\nalpha=8: CV=5.16081\n\nalpha=0.1: CV=4.91815\n\nalpha=0.01: CV=5.52044\n\nalpha=0.2: CV=4.91937\n\nalpha=0.08: CV=4.91837\n\n**alpha=0.11: CV=4.91810**\n\nalpha=0.12: CV=4.91794\n\nalpha=0.15: CV=4.91854\n\nalpha=0.13: CV=4.91821","ce9173e2":"# Residuals of the simple model\n\nNow we plot the residuals of the simple model. These diagrams show us where the holidays are:\n- End of year peak\n- Three movable holidays depending on the full moon (Easter)\n- First half of May, second half of May\n- Beginning of June, end of June\n- Beginning of November\n","f19b8332":"# Simple feature engineering (without holidays)\n\nIn this simple model, we consider the following:\n- country, store, product\n- weekdays\n- seasonal variations per product as a Fourier series with wavelengths from 1 year down to 18 days\n- country's GDP\n\nThe residuals of this simple model will permit us to understand the effect of holidays.","759190c2":"In the next step, we compute the mean residual for all the 366 days and search for days with unusually high residuals. But what is \"unusually high\"? In statistical tests we often aim at a significance level of 0.05. But the significance level corresponds to the rate of false positives, and when testing 366 days, I want to get at most 1 false positive holiday. At a significance level of 1\/365, we need to find residuals which are higher than three times the standard deviation.\n","67f6859b":"# Feature importance\n\nThe coefficients (weights) of a linear model show us the importance of features. Let's start by displaying the weights of the days around Easter and the end of the year:\n","ccd3216a":"# More feature engineering (advanced model)","882484ac":"# Training with validation\n\nWe train on the years 2015 - 2017 and validate on 2018. For the validation, we show\n- The execution time and the SMAPE\n- A scatterplot y_true vs. y_pred (ideally all points should lie near the diagonal)\n","4ecd1fe6":"# Systematic analysis of the residuals\n\nLooking at the residual plots has brought us quite far, but it's not enough. We'll now do a more systematic analysis of the residuals. We start by computing all the residuals, this time defined as the difference between the log of the prediction and the log of the true value.\n\nWe see that these residuals are normally distributed with center 0 and standard deviation 0.053.","49e258be":"Credit: https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model","43284468":"The table above shows us one potential holiday: October 21. But wait: There may be country-specific holidays. Repeating the same procedure for residuals grouped by country and day gives some more candidates:","463686b6":"# Linear model for the January TPS\n\nScikit-learn doesn't offer SMAPE as a loss function. As a workaround, I'm training for Huber loss with a transformed target, apply a correction factor, and we'll see how far we'll get.\n\nThe **transformed target** for the regression is the log of the sales numbers. This has several benefits:\n- MAE of the log is a good approximation for SMAPE. Why? SMAPE is a relative error which can be approximated by abs(y_pred - y_true) \/ y_true = abs(y_pred\/y_true - 1). For small errors, we may approximate abs(y_pred\/y_true - 1) by abs(log(y_pred\/y_true)) = abs(log(y_pred) - log(y_true)) = MAE(log()). A relative error resembles an absolute error of the logarithm of the target. In a [diagram](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298473) of the two functions, the difference is barely noticeable.\n- While the regression can output negative predictions, exp(regression_output) is always positive.\n- With this transformed target, we can easily fit an exponential growth rate.\n- Most other effects will be multiplicative as well: hat sales on a Sunday in Finland will be average hat sales multiplied by a Sunday factor multiplied by a Finland factor\n\n\nWe proceed in **two steps**:\n1. We first create a simple model without holidays. Plotting the residuals of this simple model helps identify the holidays.\n2. We then create an advanced model with features for all the holidays.\n\nThe notebook goes together with the [EDA notebook](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-01-eda-which-makes-sense), which visualizes the various seasonal effects and the differences in growth rate.\n\nBug reports: Please report all bugs in the comments section of the notebook.\n\nRelease notes:\n- V2: Modified yearly growth\n- V3: No growth from 2018 to 2019\n- V4: Added Easter feature\n- V5: Various optimizations\n- V6: Without loss correction factor\n- V7: More holidays\n- V8: Added GDP feature, thanks to [@carlmcbrideellis](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298911)'s [dataset](https:\/\/www.kaggle.com\/carlmcbrideellis\/gdp-20152019-finland-norway-and-sweden)\n- V9: Added dayofdataset feature\n- V10: Taking the ceiling of all predictions as @remekkinas [suggests](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/299162)\n- V11: log of GDP makes more sense because we predict the log of the target\n- V14: Ridge ([proposed by @paddykb](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/299296#1641253)), rounding (as [proposed by @remekkinas](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/299162) and demonstrated in @fergusfindley's [ensembling and rounding techniques comparison](https:\/\/www.kaggle.com\/fergusfindley\/ensembling-and-rounding-techniques-comparison)), feature importance, different holiday length \n- V15: GroupKFold, save oof and residuals, GDP exponent, shorter Fourier transformation\n- V16: more features, systematic residual analysis\n","b225c6c4":"The weights have a clear real-world interpretation: If, for instance, the weekend feature `wd56` has a weight of $0.225$, this means that weekend sales are $e^{0.225} = 1.25$ times as high as non-weekend sales. Or sales on the 1st of January with a weight of $0.447$ are $e^{0.447} = 1.56$ times as high as if there were no end-of-year holiday.\n\nThe final display of this notebook shows the weights of the 30 most important features. Of course, you could as well show the 30 least important features and perhaps eliminate some of them.","ad34d69d":"# Inference and submission\n\nWe are saving two submission files:\n- The real-valued predictions of the regression. You can use these values for blending.\n- The predictions rounded to the nearest integer.","45f10b7e":"# Residuals of the advanced model\n\nThe diagrams show that the residuals are much smaller now and that we have implemented the most important holidays correctly.","aa57e4c7":"# GDP exponent\n\nThe weight of the GDP feature, which is 1.212, has a special significance: It means that the sales of every year are proportional to `GDP ** 1.212`. We'll use this exponent in the [LightGBM Quickstart notebook](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-06-lightgbm-quickstart).","c8de365f":"Engineering the features for these holidays is a task for a future version of this notebook (or for your own model). And the other open task is the search for movable holidays (e.g. based on the first Sunday of November). We can apply the same statistical test to find movable holidays, we only need to correctly group the residuals.","37381399":"# Training the simple model (without holidays)\n\nWe train the model on the full training data. Cross-validation will come later; we first want to see the residual diagrams.\n\nAs a first quick check, we plot the predictions for the combination `country='Norway', store='KaggleMart', product='Kaggle Hat'`.\n"}}