{"cell_type":{"6c346a52":"code","3d744723":"code","6d027c11":"code","a863bea8":"code","0f76b18e":"code","6333312c":"code","b136c8ef":"code","ecc76b40":"code","9f9db8c0":"code","dbfff930":"code","daa15370":"code","6bc64c7d":"code","375dda59":"code","29f58315":"code","8cf69236":"code","8dd7747e":"code","a8cfb708":"code","b60a491e":"code","2f163bc9":"code","6ff1d258":"code","70be3e64":"code","d8c63f25":"code","7eeceee4":"code","2b22a1c0":"code","695aba60":"code","405c6527":"code","95762795":"code","ee85cdab":"code","90311170":"code","c66b34cc":"code","c7cb52af":"code","f678b38d":"code","26a2ff2b":"code","1c777416":"code","aec5acba":"code","f335b1df":"code","73aeebb8":"code","82ad6f49":"code","b6672d53":"code","78cc896f":"code","e2acc4d4":"code","809ecbe9":"code","9fa7c4b5":"code","97c7cbb8":"code","1777d183":"code","b9d457ba":"code","77eeb530":"code","666c6ea4":"code","c57da2b1":"code","8cdf26ab":"code","3f905cd0":"code","be3ea3c7":"code","3214e8ab":"code","2b744b5d":"code","1fc94c85":"code","4e2e234f":"code","baab7c9d":"code","951f0c27":"code","0e4f863e":"code","a19641f7":"code","85e48d51":"code","461e667b":"code","a279f5e0":"code","b0324b69":"code","a89f64f6":"code","7c5eb083":"code","8177f6d5":"code","96d67cc4":"code","24e28c4c":"markdown","09bef5e4":"markdown","9e3b6406":"markdown","52766aeb":"markdown","437735bf":"markdown","245eea9a":"markdown","f0103c3c":"markdown","0ea6de9c":"markdown","82a293b2":"markdown","3c68d4b4":"markdown","ce90be14":"markdown","df257bd8":"markdown","82fbdef3":"markdown","9b952646":"markdown","351cc12e":"markdown","157d9a48":"markdown","be6d2aa1":"markdown","d51ea452":"markdown","589a397f":"markdown","2a98b908":"markdown","6ae6f140":"markdown","06757919":"markdown","dc47ba53":"markdown","57cbed4b":"markdown","c55e4988":"markdown","6ed1a0eb":"markdown","e5fd7890":"markdown","611e025f":"markdown","ec7c1439":"markdown","46e9e108":"markdown","6a775df1":"markdown","14941d2b":"markdown","d8b07a08":"markdown","30ae7506":"markdown","37be7283":"markdown","872eb6d4":"markdown","7c2a2465":"markdown","890e9a27":"markdown","0b38d827":"markdown","7433f946":"markdown","12d610cb":"markdown","743206ab":"markdown","99fdeebb":"markdown","7bc27571":"markdown","62a7c236":"markdown","ad82024f":"markdown","ea313d0c":"markdown","b02d90bb":"markdown","8edac577":"markdown","134e8a51":"markdown","8d77b044":"markdown","5cbddb82":"markdown","aa34ef95":"markdown","8f2a3fe8":"markdown","5d61fd2d":"markdown","b0a2055f":"markdown","15c5c86b":"markdown","b5326171":"markdown","76911daf":"markdown","5c22c1ff":"markdown","7baf92a4":"markdown","2fa6c7a3":"markdown","782ed069":"markdown","3ec86295":"markdown","c782e4e7":"markdown","01f19819":"markdown","e12839c5":"markdown","4be13a28":"markdown","138b264b":"markdown","5dff720f":"markdown","748cef67":"markdown","ab24cf96":"markdown","bff65b39":"markdown","31a5dc20":"markdown","44894482":"markdown","37325ece":"markdown","adbfeae6":"markdown","1e534d17":"markdown","140f1749":"markdown","b26d9893":"markdown","615890c9":"markdown","fcda89ed":"markdown","3fb25765":"markdown","d322c614":"markdown","03b04e0c":"markdown"},"source":{"6c346a52":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings #to remove warning from the notebook\nwarnings.filterwarnings(action='ignore')","3d744723":"#loading dataset\nname= ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndf = pd.read_csv(filepath_or_buffer=\"..\/input\/boston-house-prices\/housing.csv\",delim_whitespace=True,names=name)\ndf.head()","6d027c11":"#shape of our dataset\ndf.shape","a863bea8":"#information about the data\ndf.info()","0f76b18e":"#checking for missing data\ndf.isnull().sum()\n#there is no missing value in the data","6333312c":"plt.figure(figsize=(12,12))\nsns.heatmap(data=df.corr().round(2),annot=True,cmap='coolwarm',linewidths=0.2,square=True)","b136c8ef":"#since some of these features shows quite good and very good correlation with our predictive variable Houese Price(MEDV)\ndf1 = df[['RM','TAX','PTRATIO','LSTAT','MEDV']]\ndf1.head()","ecc76b40":"sns.pairplot(data=df1)","9f9db8c0":"#description about data\ndesc = df1.describe().round(2)\ndesc","dbfff930":"#Box Plot and Distribution Plot for Dependent variable MEDV\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,2,1)\nsns.boxplot(df1.MEDV,color='#005030')\nplt.title('Box Plot of MEDV')\n\nplt.subplot(1,2,2)\nsns.distplot(a=df1.MEDV,color='#500050')\nplt.title('Distribution Plot of MEDV')\nplt.show()","daa15370":"MEDV_Q3 = desc['MEDV']['75%']\nMEDV_Q1 = desc['MEDV']['25%']\nMEDV_IQR = MEDV_Q3 - MEDV_Q1\nMEDV_UV = MEDV_Q3 + 1.5*MEDV_IQR\nMEDV_LV = MEDV_Q1 - 1.5*MEDV_IQR\n\ndf1[df1['MEDV']<MEDV_LV]","6bc64c7d":"df1[df1['MEDV']>MEDV_UV].sort_values(by=['MEDV','RM'])","375dda59":"print(f'Shape of dataset before remving Outliers: {df1.shape}')\ndf2 = df1[~(df1['MEDV']==50)]\nprint(f'Shape of dataset after remving Outliers: {df2.shape}')","29f58315":"#Box Plot, Distribution Plot and Scatter Plot for TAX\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,3,1)\nsns.boxplot(df2.TAX,color='#005030')\nplt.title('Box Plot of TAX')\n\nplt.subplot(1,3,2)\nsns.distplot(a=df2.TAX,color='#500050')\nplt.title('Distribution Plot of TAX')\n\nplt.subplot(1,3,3)\nsns.scatterplot(df2.TAX,df2.MEDV)\nplt.title('Scatter Plot of TAX vs MEDV')\n\nplt.show()","8cf69236":"temp_df = df2[df1['TAX']>600].sort_values(by=['RM','MEDV'])\ntemp_df.shape","8dd7747e":"temp_df","a8cfb708":"temp_df.describe()","b60a491e":"TAX_10 = df2[(df2['TAX']<600) & (df2['LSTAT']>=0) & (df2['LSTAT']<10)]['TAX'].mean()\nTAX_20 = df2[(df2['TAX']<600) & (df2['LSTAT']>=10) & (df2['LSTAT']<20)]['TAX'].mean()\nTAX_30 = df2[(df2['TAX']<600) & (df2['LSTAT']>=20) & (df2['LSTAT']<30)]['TAX'].mean()\nTAX_40 = df2[(df2['TAX']<600) & (df2['LSTAT']>=30)]['TAX'].mean()\n\nindexes = list(df2.index)\nfor i in indexes:\n    if df2['TAX'][i] > 600:\n        if (0 <= df2['LSTAT'][i] < 10):\n            df2.at[i,'TAX'] = TAX_10\n        elif (10 <= df2['LSTAT'][i] < 20):\n            df2.at[i,'TAX'] = TAX_20\n        elif (20 <= df2['LSTAT'][i] < 30):\n            df2.at[i,'TAX'] = TAX_30\n        elif (df2['LSTAT'][i] >30):\n            df2.at[i,'TAX'] = TAX_40\n\nprint('Values imputed successfully')","2f163bc9":"#This show all those extreme TAX values are replaced successfully\ndf2[df2['TAX']>600]['TAX'].count()","6ff1d258":"sns.distplot(a=df2.TAX,color='#500050')\nplt.title('Distribution Plot of TAX after replacing extreme values')\nplt.show()","70be3e64":"#Box Plot, Distribution Plot and Scatter Plot for PTRATIO\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,3,1)\nsns.boxplot(df2.PTRATIO,color='#005030')\nplt.title('Box Plot of PTRATIO')\n\nplt.subplot(1,3,2)\nsns.distplot(a=df2.PTRATIO,color='#500050')\nplt.title('Distribution Plot of PTRATIO')\n\nplt.subplot(1,3,3)\nsns.scatterplot(df2.PTRATIO,df2.MEDV)\nplt.title('Scatter Plot of PTRATIO vs MEDV')\n\nplt.show()","d8c63f25":"df2[df2['PTRATIO']<14].sort_values(by=['LSTAT','MEDV'])","7eeceee4":"#Box Plot, Distribution Plot and Scatter Plot for LSTAT\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,3,1)\nsns.boxplot(df2.LSTAT,color='#005030')\nplt.title('Box Plot of LSTAT')\n\nplt.subplot(1,3,2)\nsns.distplot(a=df2.LSTAT,color='#500050')\nplt.title('Distribution Plot of LSTAT')\n\nplt.subplot(1,3,3)\nsns.scatterplot(df2.LSTAT,df2.MEDV)\nplt.title('Scatter Plot of LSTAT vs MEDV')\n\nplt.show()","2b22a1c0":"LSTAT_Q3 = desc['LSTAT']['75%']\nLSTAT_Q1 = desc['LSTAT']['25%']\nLSTAT_IQR = LSTAT_Q3 - LSTAT_Q1\nLSTAT_UV = LSTAT_Q3 + 1.5*LSTAT_IQR\nLSTAT_LV = LSTAT_Q1 - 1.5*LSTAT_IQR\n\ndf2[df2['LSTAT']>LSTAT_UV].sort_values(by='LSTAT')","695aba60":"#Box Plot, Distribution Plot and Scatter Plot for RM\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,3,1)\nsns.boxplot(df2.RM,color='#005030')\nplt.title('Box Plot of MEDV')\n\nplt.subplot(1,3,2)\nsns.distplot(a=df2.RM,color='#500050')\nplt.title('Distribution Plot of MEDV')\n\nplt.subplot(1,3,3)\nsns.scatterplot(df2.RM,df2.MEDV)\nplt.title('Scatter Plot of RM vs MEDV')\n\nplt.show()","405c6527":"RM_Q3 = desc['RM']['75%']\nRM_Q1 = desc['RM']['25%']\nRM_IQR = RM_Q3 - RM_Q1\nRM_UV = RM_Q3 + 1.5*RM_IQR\nRM_LV = RM_Q1 - 1.5*RM_IQR\n\ndf2[df2['RM']<RM_LV].sort_values(by=['RM','MEDV'])","95762795":"print(f'Shape of dataset before removing data points: {df2.shape}')\ndf3 = df2.drop(axis=0,index=[365,367])\nprint(f'Shape of dataset before removing data points: {df3.shape}')","ee85cdab":"df3[df3['RM']>RM_UV].sort_values(by=['RM','MEDV'])","90311170":"print(f'Shape of dataset before removing data points: {df3.shape}')\ndf3 = df3.drop(axis=0,index=[364])\nprint(f'Shape of dataset before removing data points: {df3.shape}')","c66b34cc":"#Now will split our dataset into Dependent variable and Independent variable\n\nX = df3.iloc[:,0:4].values\ny = df3.iloc[:,-1:].values","c7cb52af":"print(f\"Shape of Dependent Variable X = {X.shape}\")\nprint(f\"Shape of Independent Variable y = {y.shape}\")","f678b38d":"def FeatureScaling(X):\n    \"\"\"\n    is function takes an array as an input, which needs to be scaled down.\n    Apply Standardization technique to it and scale down the features with mean = 0 and standard deviation = 1\n    \n    Input <- 2 dimensional numpy array\n    Returns -> Numpy array after applying Feature Scaling\n    \"\"\"\n    mean = np.mean(X,axis=0)\n    std = np.std(X,axis=0)\n    for i in range(X.shape[1]):\n        X[:,i] = (X[:,i]-mean[i])\/std[i]\n\n    return X","26a2ff2b":"X = FeatureScaling(X)","1c777416":"m,n = X.shape\nX = np.append(arr=np.ones((m,1)),values=X,axis=1)","aec5acba":"#Now we will spit our data into Train set and Test Set\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state = 42)\n\nprint(f\"Shape of X_train = {X_train.shape}\")\nprint(f\"Shape of X_test = {X_test.shape}\")\nprint(f\"Shape of y_train = {y_train.shape}\")\nprint(f\"Shape of y_test = {y_test.shape}\")","f335b1df":"#ComputeCost function determines the cost (sum of squared errors) \n\ndef ComputeCost(X,y,theta):\n    \"\"\"\n    This function takes three inputs and uses the Cost Function to determine the cost (basically error of prediction vs\n    actual values)\n    Cost Function: Sum of square of error in predicted values divided by number of data points in the set\n    J = 1\/(2*m) *  Summation(Square(Predicted values - Actual values))\n    \n    Input <- Take three numoy array X,y and theta\n    Return -> The cost calculated from the Cost Function\n    \"\"\"\n    m=X.shape[0] #number of data points in the set\n    J = (1\/(2*m)) * np.sum((X.dot(theta) - y)**2)\n    return J","73aeebb8":"#Gradient Descent Algorithm to minimize the Cost and find best parameters in order to get best line for our dataset\n\ndef GradientDescent(X,y,theta,alpha,no_of_iters):\n    \"\"\"\n    Gradient Descent Algorithm to minimize the Cost\n    \n    Input <- X, y and theta are numpy arrays\n            X -> Independent Variables\/ Features\n            y -> Dependent\/ Target Variable\n            theta -> Parameters \n            alpha -> Learning Rate i.e. size of each steps we take\n            no_of_iters -> Number of iterations we want to perform\n    \n    Return -> theta (numpy array) which are the best parameters for our dataset to fit a linear line\n             and Cost Computed (numpy array) for each iteration\n    \"\"\"\n    m=X.shape[0]\n    J_Cost = []\n    for i in range(no_of_iters):\n        error = np.dot(X.transpose(),(X.dot(theta)-y))\n        theta = theta - alpha * (1\/m) * error\n        J_Cost.append(ComputeCost(X,y,theta))\n    \n    return theta, np.array(J_Cost)","82ad6f49":"iters = 1000\n\nalpha1 = 0.001\ntheta1 = np.zeros((X_train.shape[1],1))\ntheta1, J_Costs1 = GradientDescent(X_train,y_train,theta1,alpha1,iters)\n\nalpha2 = 0.003\ntheta2 = np.zeros((X_train.shape[1],1))\ntheta2, J_Costs2 = GradientDescent(X_train,y_train,theta2,alpha2,iters)\n\nalpha3 = 0.01\ntheta3 = np.zeros((X_train.shape[1],1))\ntheta3, J_Costs3 = GradientDescent(X_train,y_train,theta3,alpha3,iters)\n\nalpha4 = 0.03\ntheta4 = np.zeros((X_train.shape[1],1))\ntheta4, J_Costs4 = GradientDescent(X_train,y_train,theta4,alpha4,iters)","b6672d53":"plt.figure(figsize=(8,5))\nplt.plot(J_Costs1,label = 'alpha = 0.001')\nplt.plot(J_Costs2,label = 'alpha = 0.003')\nplt.plot(J_Costs3,label = 'alpha = 0.01')\nplt.plot(J_Costs4,label = 'alpha = 0.03')\nplt.title('Convergence of Gradient Descent for different values of alpha')\nplt.xlabel('No. of iterations')\nplt.ylabel('Cost')\nplt.legend()\nplt.show()","78cc896f":"theta4","e2acc4d4":"def Predict(X,theta):\n    \"\"\"\n    This function predicts the result for the unseen data\n    \"\"\"\n    y_pred = X.dot(theta)\n    return y_pred","809ecbe9":"y_pred = Predict(X_test,theta4)\ny_pred[:5]","9fa7c4b5":"plt.scatter(x=y_test,y=y_pred,alpha=0.5)\nplt.xlabel('y_test',size=12)\nplt.ylabel('y_pred',size=12)\nplt.title('Predicited Values vs Original Values (Test Set)',size=15)\nplt.show()","97c7cbb8":"sns.residplot(y_pred,(y_pred-y_test))\nplt.xlabel('Predicited Values',size=12)\nplt.ylabel(\"Residues\",size=12)\nplt.title('Residual Plot',size=15)\nplt.show()","1777d183":"sns.distplot(y_pred-y_test)\nplt.xlabel('Residual',size=12)\nplt.ylabel('Frquency',size=12)\nplt.title('Distribution of Residuals',size=15)\nplt.show()","b9d457ba":"from sklearn import metrics\nr2= metrics.r2_score(y_test,y_pred)\nN,p = X_test.shape\nadj_r2 = 1-((1-r2)*(N-1))\/(N-p-1)\nprint(f'R^2 = {r2}')\nprint(f'Adjusted R^2 = {adj_r2}')","77eeb530":"from sklearn import metrics\nmse = metrics.mean_squared_error(y_test,y_pred)\nmae = metrics.mean_absolute_error(y_test,y_pred)\nrmse = np.sqrt(metrics.mean_squared_error(y_test,y_pred))\nprint(f'Mean Squared Error: {mse}',f'Mean Absolute Error: {mae}',f'Root Mean Squared Error: {rmse}',sep='\\n')","666c6ea4":"#coefficients of regression model\ncoeff=np.array([y for x in theta4 for y in x]).round(2)\nfeatures=['Bias','RM','TAX','PTRATIO','LSTAT']\neqn = 'MEDV = '\nfor f,c in zip(features,coeff):\n    eqn+=f\" + ({c} * {f})\";\n\nprint(eqn)","c57da2b1":"sns.barplot(x=features,y=coeff)\nplt.ylim([-5,25])\nplt.xlabel('Coefficient Names',size=12)\nplt.ylabel('Coefficient Values',size=12)\nplt.title('Visualising Regression Coefficients',size=15)\nplt.show()","8cdf26ab":"X_dt = df3.iloc[:,:-1].values\ny_dt = df3.iloc[:,-1].values","3f905cd0":"from sklearn.model_selection import train_test_split\nX_train_dt,X_test_dt,y_train_dt,y_test_dt = train_test_split(X_dt,y_dt,test_size=0.2,random_state=42)\n\nprint(f\"Shape of X_train_dt = {X_train_dt.shape}\")\nprint(f\"Shape of X_test_dt = {X_test_dt.shape}\")\nprint(f\"Shape of y_train_dt = {y_train_dt.shape}\")\nprint(f\"Shape of y_test_dt = {y_test_dt.shape}\")","be3ea3c7":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\ndt.fit(X_train_dt,y_train_dt)","3214e8ab":"y_pred_dt = dt.predict(X_test_dt)\ny_pred_dt[:5]","2b744b5d":"plt.scatter(x=y_test_dt,y=y_pred_dt,alpha=0.5)\nplt.xlabel('y_test',size=12)\nplt.ylabel('y_pred',size=12)\nplt.title('Predicited Values vs Original Values (Test Set)',size=15)\nplt.show()","1fc94c85":"sns.residplot(y_pred_dt,(y_pred_dt-y_test_dt))\nplt.xlabel('Predicited Values',size=12)\nplt.ylabel(\"Residues\",size=12)\nplt.title('Residual Plot',size=15)\nplt.show()","4e2e234f":"sns.distplot(y_pred_dt-y_test_dt)\nplt.xlabel('Residual',size=12)\nplt.ylabel('Frquency',size=12)\nplt.title('Distribution of Residuals',size=15)\nplt.show()","baab7c9d":"from sklearn import metrics\nr2_dt= metrics.r2_score(y_test_dt,y_pred_dt)\nN,p = X_test_dt.shape\nadj_r2_dt = 1-((1-r2_dt)*(N-1))\/(N-p-1)\nprint(f'R^2 = {r2_dt}')\nprint(f'Adjusted R^2 = {adj_r2_dt}')","951f0c27":"from sklearn import metrics\nmse_dt = metrics.mean_squared_error(y_test_dt,y_pred_dt)\nmae_dt = metrics.mean_absolute_error(y_test_dt,y_pred_dt)\nrmse_dt = np.sqrt(metrics.mean_squared_error(y_test_dt,y_pred_dt))\nprint(f'Mean Squared Error: {mse_dt}',f'Mean Absolute Error: {mae_dt}',f'Root Mean Squared Error: {rmse_dt}',sep='\\n')","0e4f863e":"X_rf = df3.iloc[:,:-1].values\ny_rf = df3.iloc[:,-1].values","a19641f7":"from sklearn.model_selection import train_test_split\nX_train_rf,X_test_rf,y_train_rf,y_test_rf = train_test_split(X_rf,y_rf,test_size=0.2,random_state=42)\n\nprint(f\"Shape of X_train_rf = {X_train_rf.shape}\")\nprint(f\"Shape of X_test_rf = {X_test_rf.shape}\")\nprint(f\"Shape of y_train_rf = {y_train_rf.shape}\")\nprint(f\"Shape of y_test_rf = {y_test_rf.shape}\")","85e48d51":"warnings.filterwarnings(action='ignore')\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state=42)\nrf.fit(X_train_rf,y_train_rf)","461e667b":"y_pred_rf = rf.predict(X_test_rf)\ny_pred_rf[:5]","a279f5e0":"plt.scatter(x=y_test_rf,y=y_pred_rf,alpha=0.5)\nplt.xlabel('y_test',size=12)\nplt.ylabel('y_pred',size=12)\nplt.title('Predicited Values vs Original Values (Test Set)',size=15)\nplt.show()","b0324b69":"sns.residplot(y_pred_rf,(y_pred_rf-y_test_rf))\nplt.xlabel('Predicited Values',size=12)\nplt.ylabel(\"Residues\",size=12)\nplt.title('Residual Plot',size=15)\nplt.show()","a89f64f6":"sns.distplot(y_pred_rf-y_test_rf)\nplt.xlabel('Residual',size=12)\nplt.ylabel('Frquency',size=12)\nplt.title('Distribution of Residuals',size=15)\nplt.show()","7c5eb083":"from sklearn import metrics\nr2_rf= metrics.r2_score(y_test_rf,y_pred_rf)\nN,p = X_test_dt.shape\nadj_r2_rf = 1-((1-r2_rf)*(N-1))\/(N-p-1)\nprint(f'R^2 = {r2_rf}')\nprint(f'Adjusted R^2 = {adj_r2_rf}')\n","8177f6d5":"from sklearn import metrics\nmse_rf = metrics.mean_squared_error(y_test_rf,y_pred_rf)\nmae_rf = metrics.mean_absolute_error(y_test_rf,y_pred_rf)\nrmse_rf = np.sqrt(metrics.mean_squared_error(y_test_rf,y_pred_rf))\nprint(f'Mean Squared Error: {mse_rf}',f'Mean Absolute Error: {mae_rf}',f'Root Mean Squared Error: {rmse_rf}',sep='\\n')","96d67cc4":"results=pd.DataFrame({'Linear Regression':[r2,adj_r2],'Decision Tree':[r2_dt,adj_r2_dt],\n                      'Random Forest':[r2_rf,adj_r2_rf]},index=['R square','Adj R square'])\n\nresults.plot(kind='bar',alpha=0.7,grid=True,title='Interpreting Results',rot=0,figsize=(10,5),colormap='jet')\nresults","24e28c4c":"**Observations:**\n- *Distribution of Residuals Plot* shows residuals are normally distributed.\n\n\n- From above *Residual Plot*, I do not found any significant pattern in residues (errors or predicition).\n\n\n- I can conclude that our model is neither under fitting nor over fitting the data.","09bef5e4":"Here we are going to import our **Boston House Price** dataset and will see how it looks o_o","9e3b6406":"- Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. \n\n\n- Few advantages of Feature Scaling the data are as follows:\n    - It makes training of model faster.\n    - It prevents the model from getting stuck in local optima.\n\n\n- Here, we are using Standard Scalar which will scale Independent variables such that distribution is now centred around 0, with a Standard Deviation of 1.","52766aeb":"**Observations:**\n- I am more concerned about two data points (row index - 365 & 367) where MEDV is higher while RM is very low, though RM and MEDV are positively correlated.\n- Also for these two data points TAX and PTRATIO are above 50% of data points respectively, though both are negatively correlated to MEDV.\n- For rest data points, I don't see any unusual behaviour.\n\n**Conclusion:**\n- I am going to delete those two data points (row index - 365 & 367) as it may influence the prediction capability of our model.\n- Also I am going to keep all other points.","437735bf":"## 5.3 Model Interpretation","245eea9a":"**Observations:**\n- For house prices = 50, it is observed that number of Room ranges from 5 to 9 (approx.) which is quite unusual.\n- Also for these houses TAX ranges from low to high.\n- For houses price between 37 to less than 50, RM is higher than 75% of the total data points. Since RM is positively correlated to MEDV, so this could be reason for little higher house prices.\n- Also for these houses PTRATIO and LSTAT lies in 25% - 50% of the total observation respectively. Since PTRATIO and LSAT are negatively correlated to MEDV so this could be reason for little higher house prices.\n\n**Conclusion:**\n- I am going to DROP ALL entries whose MEDV = 50 because I feel these entries are outliers and can create problem in having good predicitions.\n- I am going to keep all entries having MEDV between 37 to less than 50, since I could not observe any unusual behaviour for them.","f0103c3c":"We can see in the difference of shape of dataset after removing two data points (outliers).","0ea6de9c":"First we are importing all the important libraries we are going to use in this project and if we need any other library, we will import it at that time only.","82a293b2":"This data set has 14 features and 506 rows i.e. details of 506 houses.","3c68d4b4":"Predict fucntion predicts the house price i.e. MEDV on the new unseen data using the regression coefficients i.e. theta.","ce90be14":"\n**Observations:**\n- *Distribution of Residuals Plot* shows residuals are normally distributed.\n\n\n- From above *Residual Plot*, I do not found any significant pattern in residues (errors or predicition).\n\n\n- I can conclude that our model is neither under fitting nor over fitting the data.\n","df257bd8":"We used the Decision Tree Regressor to predict the House Price on the Test Set.","82fbdef3":"# Boston House Price Prediction","9b952646":"# 5. Multiple Linear Regression\n##### Here we are building Multiple Linear Regression from Scratch","351cc12e":"## 7.2 Model Interpretation","157d9a48":"From above three figures we can observe that:\n- LSTAT is  normally distributed and skewed to right.\n- There are some high LSTAT values in the dataset which we will analyse.","be6d2aa1":"There are total 132 entries in TAX mostly having value 666 which I thinks is a *DEVIL'S* number. Now lets deep dive inside them.","d51ea452":"## 4. Splitting Dataset into Train and Test Set","589a397f":"From above *Evaluation Metrices*, we can notice that Root Mean Squared Error is low for our Random Forest Model and that is good thing for us. Also all these error scores are less then Decision Tree and Linear Regression.\n\n**Conclusion:**\n- Random Forest Model gives better R square than both the models we made earlier, it means that this model is able to predict house prices more accurate than previous both models and we may use this model for predicting the house prices.\n\n---","2a98b908":"We have divided the dataset in Training set and Test set","6ae6f140":"# 3. Univariate and Multivariate Analysis","06757919":"We fiitted a Decision Tree Regressor with default parameters.","dc47ba53":"## 3.3 PTRATIO","57cbed4b":"## 2.1 Loading Dataset","c55e4988":"We need to add a variable for **Bias** also. So, we are adding a new column of 1's in X as the fist column. ","6ed1a0eb":"Set of Independent variables X is now scaled down.","e5fd7890":"**Observations:**\n- For these two low house prices, we can see that TAX = 666 which is very high for a house with approx 5 rooms.\n- For these two low house prices, we can see that LSTAT is also  high.\n\n**Conclusion:**\n- Since both TAX and LSTAT are negatively correlated to MEDV which means higher the TAX and LSTAT lower will be the house price and vica-versa.\n- I find it meaningful to have such low house prices.\n- Therefore, I will keep these data points.","611e025f":"## 3.5 RM","ec7c1439":"This shows that those values are replaced succesfully :)","46e9e108":"**Observations:**\n- In the above data points, I am more concerned about one data point only (row index - 364) where MEDV is very low while RM is very high, though RM and MEDV are positively correlated.\n- Also for this data point LSTAT is low and MEDV is also low, though both are negatively correlated.\n- For rest data points, I don't see any unusual behaviour.\n\n**Conclusion:**\n- I am going to delete the data point (row index - 364) as I believe this could be human error while inputting the data.\n- Also I am going to keep all other points.","6a775df1":"## 6.2 Model Interpretation","14941d2b":"We fiitted a Random Forest Regressor with default parameters.","d8b07a08":"**Observations:**\n- PTRATIO for all above data points is same.\n- RM and MEDV is increasing simultaneously, as RM and MEDV are positively correlated, which is fine.\n- As LSTAT increases MEDV decreases, which follows negative correlation.\n\n**Conclusion:**\n- I don't observe any unusual behaviour for these data points. Therefore, I will keep them.","30ae7506":"**Observations:**\n\n - **MEDV = (21.74 * Bias) + (2.74 * RM) + (-1.06 * TAX) + (-1.93 * PTRATIO) + (-3.03 * LSTAT)**\n\n\n- From above equation we can conclude that, for 1 unit increase in RM the House Price will go up by 2.74 units and vica-versa, considering other factors remaining constant.\n\n\n- Also for 1 unit increase in TAX the House Price will go down by 1.06 units and vica-versa, considering other factors remaining constant.\n\n\n- Also for 1 unit increase in PTRATIO the House Price will go down by 1.93 units and vica-versa, considering other factors remaining constant.\n\n\n- Also for 1 unit increase in LSTAT the House Price will go down by 3.03 units and vica-versa, considering other factors remaining constant.\n\n\n(Above four observations are quite meaningful also, since RM is positively correlated to MEDV and TAX, PRTATIO & LSTAT are negatively correlated to MEDV.)\n\n**Conclusion:**\n\n- *As we know, as the number of rooms increases price of the house increases. Whereas if the number of lower class people is high in a region (LSTAT) or if the student-teacher ratio is bigger (PTRATIO) i.e. less number of teachers for more number of students or if TAX rate is more, obiously House price will gp down.*\n\n\n- Our multiple regression model does not explains the data perfectly (as R sqare value is 0.77) but it still it explains the good relationship of House Price (i.e. MEDV) and other factors affecting the price.\n\n\n- We will fit few more models on this dataset and at the end will choose the model which explains the data best among all models.\n---","37be7283":"These 5x5 figures above helps us to understand how data in each variable (feature) is distributed with itself and with others.\n\n**Observations**\n- As we can see that RM, LSTAT and MEDV are quite normally distributed.\n\n\n- Also we can see that RM and LSTAT shows kind of good Linear relationship with MEDV.\n\n\n- There seems to have presence of some outliers in the dataset, we will study about them in some time.","872eb6d4":"From above two figures we can see observe that:\n- MEDV is normally distributed\n- It contains some extreme values which could be potential outliers\n\nNext we are going to observe data points which lies outside wiskers.\n\n*Q3 + 1.5 * IQR*  <  **Potential Outliers**  <  *Q1 - 1.5 * IQR*\n- Q3 -> Quartile 3, Under which 75% of data lies\n- Q1 -> Quartile 1, Under which 25% of data lies\n- IQR -> Inter-Quartile Range, Q3 - Q1","7c2a2465":"*In this project we are going to use Machine Learning to predict the house prices of city named Boston in US.*\n\n*The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970.*\n\n*There are several features given for a house and we have to predicts its value as accurate as possible.*","890e9a27":"- *As from the above figure it is clearly visible that R square and Adjusted R square value for* **Random Forest** *Model is highest among all three models we used for predicting house price.*\n\n\n- *We can say that it will be good to use Random Forest model for this dataset as it will help in predicitng house prices best without overfitting.*\n\n\n- *Also, this data was collected many years ago and many things has changed since then, so it may not be feasible to implement this model for current house price predictions of Boston. Other new features must be taken into consideration and then we can make a better model for current scenario.*\n\n\n- *Though this data is enough to learn and understand how to predicit House Price frpm given number of features using Machine Learning.*","0b38d827":"From above three figures we can observe that:\n- PTRATIO is NOT normally distributed\n- There are few low PRATIO values in the dataset which is bothering me.","7433f946":"In the above scatter plot we can see that the diagonal line is not that straight, which represents the differences in the actual and predictions.","12d610cb":"**Observations:**\n- RM for these entries lies between 3.5 to 8.78.\n- PTRATIO for almost all of these entries is same and equal to 20.20.\n- LSTAT for these entries lies between 2.96 to 37.97.\n- MEDV for these entries lies between 5 to 29.80.\n- All these observations are very unusual, it seems impossible to have such high TAX values for all these houses.\n- These values most likely missing values which were imputed casually by someone.\n\n**Conclusion:**\n- Since LSTAT is most correlated to TAX as seen above in Heatmap, so I am going to replace those 132 TAX values with mean of remaining TAX values dividing in some intervals with the help of LSTAT.\n- Interval 1: TAX_10 -> Replacing extreme TAX values having LSTAT is between 0 to 10 with mean of other TAX values whose LSTAT is between 0 to 10.\n- Interval 2: TAX_20 -> Replacing extreme TAX values having LSTAT is between 10 to 20 with mean of other TAX values whose LSTAT is between 10 to 20.\n- Interval 3: TAX_30 -> Replacing extreme TAX values having LSTAT is between 20 to 30 with mean of other TAX values whose LSTAT is between 20 to 30.\n- Interval 4: TAX_40 -> Replacing extreme TAX values having LSTAT >= 30 with mean of other TAX values whose LSTAT >= 30.","743206ab":"# 6. Decision Tree\n#### We will be using sklearn lirbrary to build Decision Tree model on the dataset. ","99fdeebb":"First, we have divided our data into two sets:\n\n**X** contains all independent variables\n\n**y** contains independent variable MEDV","7bc27571":"In the above scatter plot we can see that the diagonal line is not that straight, which represents the differences in the actual and predictions.","62a7c236":"This is our Gradient Descent Algorithm which will minimize the *Error in Prediction*.\n\nBasically, it will find the best coefficients **theta** for our data which will represt Best Linear Line for our data.   ","ad82024f":"We can see that all features in the dataset are numeric type either float or int. There is no categorical variable, which makes our life little easier here :)","ea313d0c":"**Boston House Price** dataset has 14 features and their description is given as follows:\n- CRIM     per capita crime rate by town\n- ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n- INDUS    proportion of non-retail business acres per town\n- CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n- NOX      nitric oxides concentration (parts per 10 million)\n- RM       average number of rooms per dwelling\n- AGE      proportion of owner-occupied units built prior to 1940\n- DIS      weighted distances to five Boston employment centres\n- RAD      index of accessibility to radial highways\n- TAX      full-value property-tax rate per dollar 10,000.\n- PTRATIO  pupil-teacher ratio by town\n- B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n- LSTAT    % lower status of the population\n- MEDV     Median value of owner-occupied homes in $1000's\n\nHere main thing to notice is that **MEDV** is the outcome variable which we need to predict and all other variables are predictor variables.","b02d90bb":"## 5.2 Model Evaluation","8edac577":"## 3.1 MEDV","134e8a51":"**Observations:**\n- From above data, I observed that since LSAT value for these 7 houses is high resulting in low MEDV, which follows the negative correaltion and is True.\n- RM is low and TAX is little higher which means low MEDV and which is True.\n\n**Conclusion:**\n- I don't find any strong reason  to exclude these data points. Therefore, I will keep this data also for our model","8d77b044":"- Now we run the Gradient Descent Algorithm using different **learning rate** *alpha*. Number of iterations we will be performing = 1000\n\n\n- After that we will see what is *best learing rate* for our algorithm by visualizing the results.\n\n\n- Finally we will get best *theta*, which represents the best linear line for our data.","5cbddb82":"# 0. Overview\n\nBelow is the overview of the whole project, what all things we will be doing, step wise.\n\n\n- 1. Importing Libraries\n\n\n- 2. Exploring Dataset\n    - 2.1. We will be importing the dataset using Pandas library.\n    - 2.2. Finding variables which are useful for prediction.\n\n\n- 3. Univariate and Multivariate Analysis  \n    - 3.1 MEDV\n    - 3.2 TAX\n    - 3.3 PTRATIO\n    - 3.4 LSTAT\n    - 3.5 RM\n\n\n- 4. Splitting Dataset into Train and Test Set\n\n\n- 5. Multiple Linear Regression\n    - 5.1 Model Prepration\n    - 5.2 Model Evaluation\n    - 5.3 Model Interpretation\n\n\n- 6. Decision Tree\n    - 6.1 Model Prepration\n    - 6.2 Model Interpretation\n\n\n- 7. Random Forest\n    - 7.1 Model Prepration\n    - 7.2 Model Interpretation\n\n\n- 8. Conclusion\n---","aa34ef95":"## 5.1 Model Prepration","8f2a3fe8":"We noticed that there are *No Missing* values in the dataset which again reduced our work load. Cheers!","5d61fd2d":"# 7. Random Forest\n#### We will be using sklearn lirbrary to build Random Forest model on the dataset. ","b0a2055f":"## 3.4 LSTAT","15c5c86b":" # Thank You\nPlease leave your valuable feedback on this","b5326171":"We can see in the difference of shape of dataset after removing one data point (outlier).\n\n---\n\nNow, we are done with univariate and multivariate analysis and I feel data is ready to put into the **Black Box** i.e. model.\n\nBut before doing that we need to split our data into Training set and Test set and then we will make our model on Training set and test its accracy on Test set.","76911daf":"**Observations:**\n- *Distribution of Residuals Plot* shows residuals are quite normally distributed.\n\n\n- From above *Residual Plot*, I do not found any significant pattern in residues (errors or predicition).\n\n\n- I can conclude that our model is neither under fitting nor over fitting the data.","5c22c1ff":"*R square* value above is calcualted on Test Set, though it is not very good but still its better than our Decision Tree Score. This means that Random Forest model fits better than Decision Tree Model.","7baf92a4":"From above three figures we can observe that:\n- RM is normally distributed .\n- There are some low and high RM values in the dataset which we will analyse.\n- Scatter plot of RM vs MEDV show good Positive Linear Relationship.","2fa6c7a3":"As we can see that we have deleted 16 rows from out dataset having MEDV = 50","782ed069":"# 1. Importing Libraries","3ec86295":"From above *Evaluation Metrices*, we can notice that Root Mean Squared Error is low for our Multiple Regression Model and that is good thing for us.","c782e4e7":"**Observations:**\n- We can see that for ***alpha = 0.03***, Gradient Descent algorithm converges to minimum much faster than for any other value of alpha (taken).\n\n\n- We can see that Gradient Descent algorithm converged to minimum Cost somewhere before 50 iterations for *alpha = 0.03*.\n\n\n- Gradient Descent convergenced fastest for *alpha = 0.03 -> 0.01 -> 0.003 -> 0.001*.\n\n\n- Thus, the best value of *alpha = 0.03* and corrosponding to it we will get best *theta* which is equal to '*theta4*.","01f19819":"Now we have created a new dataset consisting of only those variables which we selected after analysing Heatmap.","e12839c5":"# 8. Conclusion\n\n*Since we have made three models for our dataset and all of them have some difference in their prediction capability, now it's time to wrap up all we learned and reach to our final conclusion.*\n\n*So guyz it's Show time!*","4be13a28":"*R square* value above is calcualted on Test Set, though it is not very good but still its better than our Multiple Linear Regression Score. This means that Decision Tree model fits better than Multiple Linear Regression Model.","138b264b":"*R square* value above is calcualted on Test Set, though it is not very good but still it explains quite good linear relationship among independent variable and dependent variables.","5dff720f":"We used the Random Forest Regressor to predict the House Price on the Test Set.","748cef67":"From above *Evaluation Metrices*, we can notice that Root Mean Squared Error is low for our Decision Tree Model and that is good thing for us. Also all these error scores are less then Linear Regression Model.\n\n**Conclusion:**\n- Decision Tree Model gives better R square than one we got in Linear Regression, it means that this model is able to predict house prices more accurate than Linear Regression and we may use this model for predicting the house prices.\n\n\n- We will try one more model Random Forest before reaching to our final conclusion.\n---","ab24cf96":"Predicted value for Test Set is saved in *y_pred* successfully.","bff65b39":"## 2.2 Finding variables which are useful for prediction","31a5dc20":"Above table displays measures of central tendency like Mean, Median (50%) etc. We can see number of entries for each variable which is same as 506.\n\n**Observations**\n- Maximum value in MEDV and LSTAT are much higher than 75% of data points, which is kind of alarming situtaion for me.\n\n\n- We will study each of the feature seprately and see how data is distributed and if there are any outliers or not.","44894482":"Above is the value of theta corrosponding to alpha = 0.03","37325ece":"Here we can see that we have split the data into Training Set (80% of total data) and Test Set (20% of total data)","adbfeae6":"We have divided the dataset in Training set and Test set","1e534d17":"This is the function which computes the Cost of sum of squared errors of our Multiple Linear Regression function.","140f1749":"From above three figures we can observe that:\n- TAX is NOT normally distributed\n- Though Boxplot does not show any outlier but there are some extreme TAX values in the dataset which is bothering me.\n- Also from the scatter plot we can observe that for these extreme TAX values, MEDV ranges from low to high.","b26d9893":"## 3.2 TAX","615890c9":"## 7.1 Model Prepration","fcda89ed":"The Big colorful picture above which is called *Heatmap* helps us to understand how features are correlated to each other.\n- Postive sign implies postive correlation between two features whereas Negative sign implies negative correlation between two features.\n\n\n- I am here interested to know which features have good correlation with our dependent variable MEDV and can help in having good predictions.\n\n\n- I observed that INDUS, RM, TAX, PTRATIO and LSTAT shows some good correaltion with MEDV and I am interested to know more about them.\n\n\n- However I noticed that INDUS shows good correlation with TAX and LSAT which is a pain point for us :(\n  \n  because it leads to **Multicollinearity**. So I decided NOT to consider this feature and do further analysis with other 5 remaining features.","3fb25765":"# 2. Exploring Dataset","d322c614":"In the above scatter plot we can see that the diagonal line is not that straight, which represents the differences in the actual and predictions.","03b04e0c":"## 6.1 Model Prepration"}}