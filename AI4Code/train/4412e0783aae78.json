{"cell_type":{"10b340a4":"code","9c3af1b3":"code","82b3eff4":"code","50c6aafa":"code","11323aec":"code","8e61f25a":"code","3696d362":"code","28e92401":"code","5ec46adb":"code","7960bd10":"code","85459f44":"code","01deab21":"code","4ad9a44e":"code","d4f3ecd4":"code","f5f1f3ac":"code","85c5d43f":"code","bfbc5cd2":"code","205e800d":"code","b20d5d09":"code","e84f7ee3":"code","ac66976e":"code","439d2ed7":"code","edf6f469":"code","f5f5cbb4":"code","3d503278":"code","b8c497a1":"code","0e900806":"code","2f0ee6be":"code","a988dfee":"code","45a3ae46":"markdown","d2714e15":"markdown","b1997c3d":"markdown","12cd5fab":"markdown","d181e2a7":"markdown","46381418":"markdown","445735a5":"markdown","b2b5d51d":"markdown","24bcc2c9":"markdown","1cd07924":"markdown","6e2335e0":"markdown","ca4424ff":"markdown","c76e193a":"markdown","c9d2dca8":"markdown"},"source":{"10b340a4":"import math\nimport pandas as pd\nimport numpy as np\n\nimport lightgbm as lgb\nimport time\nimport datetime\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import f1_score\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\npd.set_option(\"display.precision\", 8)\n\nDIR_INPUT = '\/kaggle\/input\/liverpool-ion-switching'","9c3af1b3":"train_df = pd.read_csv(DIR_INPUT + '\/train.csv')\ntrain_df.shape","82b3eff4":"train_df.head()","50c6aafa":"train_df.info()","11323aec":"train_df.describe()","8e61f25a":"train_df['time'].diff().min(), train_df['time'].diff().max()","3696d362":"train_df['open_channels'].value_counts()","28e92401":"train_df.iloc[0:500000]['open_channels'].value_counts()","5ec46adb":"train_df['signal'].min(), train_df['signal'].max(), train_df['signal'].mean()","7960bd10":"fig = go.Figure(data=[\n    go.Scatter(x=train_df.iloc[100000:125000]['time'], y=train_df.iloc[100000:125000]['signal'], name='Signal'),\n])\n\nfig.update_layout(title='Signal (part of batch #0)')\nfig.show()","85459f44":"batch = train_df.iloc[2200000:2202000]\nbatch.reset_index(drop=True, inplace=True)\n\ndata=[\n    go.Scatter(x=batch.index, y=batch['signal'], name='Signal'),\n]\n\nfor i in range(11):\n    ocx = batch[batch['open_channels'] == i]\n    data.append(go.Scatter(x=ocx.index, y=[i for _ in range(len(ocx))], name='OC: {}'.format(i), marker=dict(size=4), mode=\"markers\"))\n\nfig = go.Figure(data)\n\nfig.update_layout(title='Signal (part of batch #4)')\nfig.show()","01deab21":"fig = go.Figure(data=[\n    go.Bar(x=list(range(11)), y=train_df['open_channels'].value_counts(sort=False).values)\n])\n\nfig.update_layout(title='Target (open_channels) distribution')\nfig.show()","4ad9a44e":"fig = make_subplots(rows=3, cols=4,  subplot_titles=[\"Batch #{}\".format(i) for i in range(10)])\ni = 0\nfor row in range(1, 4):\n    for col in range(1, 5):\n        data = train_df.iloc[(i * 500000):((i+1) * 500000 + 1)]['open_channels'].value_counts(sort=False).values\n        fig.add_trace(go.Bar(x=list(range(11)), y=data), row=row, col=col)\n        \n        i += 1\n\n\nfig.update_layout(title_text=\"Target distribution in different batches\", showlegend=False)\nfig.show()","d4f3ecd4":"window_sizes = [10, 50, 100, 1000]\n\nfor window in window_sizes:\n    train_df[\"rolling_mean_\" + str(window)] = train_df['signal'].rolling(window=window).mean()\n    train_df[\"rolling_std_\" + str(window)] = train_df['signal'].rolling(window=window).std()","f5f1f3ac":"fig, ax = plt.subplots(len(window_sizes),1,figsize=(20, 6 * len(window_sizes)))\n\nn = 0\nfor col in train_df.columns.values:\n    if \"rolling_\" in col:\n        if \"mean\" in col:\n            mean_df = train_df.iloc[2200000:2210000][col]\n            ax[n].plot(mean_df, label=col, color=\"mediumseagreen\")\n        if \"std\" in col:\n            std = train_df.iloc[2200000:2210000][col].values\n            ax[n].fill_between(mean_df.index.values,\n                               mean_df.values-std, mean_df.values+std,\n                               facecolor='lightgreen',\n                               alpha = 0.5, label=col)\n            ax[n].legend()\n            n+=1","85c5d43f":"train_df = pd.read_csv(DIR_INPUT + '\/train.csv')\ntrain_df.shape","bfbc5cd2":"window_sizes = [10, 25, 50, 100, 500, 1000, 5000, 10000, 25000]\n\nfor window in window_sizes:\n    train_df[\"rolling_mean_\" + str(window)] = train_df['signal'].rolling(window=window).mean()\n    train_df[\"rolling_std_\" + str(window)] = train_df['signal'].rolling(window=window).std()\n    train_df[\"rolling_var_\" + str(window)] = train_df['signal'].rolling(window=window).var()\n    train_df[\"rolling_min_\" + str(window)] = train_df['signal'].rolling(window=window).min()\n    train_df[\"rolling_max_\" + str(window)] = train_df['signal'].rolling(window=window).max()\n    \n    train_df[\"rolling_min_max_ratio_\" + str(window)] = train_df[\"rolling_min_\" + str(window)] \/ train_df[\"rolling_max_\" + str(window)]\n    train_df[\"rolling_min_max_diff_\" + str(window)] = train_df[\"rolling_max_\" + str(window)] - train_df[\"rolling_min_\" + str(window)]\n    \n    a = (train_df['signal'] - train_df['rolling_min_' + str(window)]) \/ (train_df['rolling_max_' + str(window)] - train_df['rolling_min_' + str(window)])\n    train_df[\"norm_\" + str(window)] = a * (np.floor(train_df['rolling_max_' + str(window)]) - np.ceil(train_df['rolling_min_' + str(window)]))\n    \ntrain_df = train_df.replace([np.inf, -np.inf], np.nan)    \ntrain_df.fillna(0, inplace=True)\n\ntrain_y = train_df['open_channels']\ntrain_x = train_df.drop(columns=['time', 'open_channels'])\n\ndel train_df","205e800d":"scaler = StandardScaler()\nscaler.fit(train_x)\ntrain_x_scaled = pd.DataFrame(scaler.transform(train_x), columns=train_x.columns)\n\ndel train_x","b20d5d09":"test_df = pd.read_csv(DIR_INPUT + '\/test.csv')\ntest_df.drop(columns=['time'], inplace=True)\ntest_df.shape","e84f7ee3":"for window in window_sizes:\n    test_df[\"rolling_mean_\" + str(window)] = test_df['signal'].rolling(window=window).mean()\n    test_df[\"rolling_std_\" + str(window)] = test_df['signal'].rolling(window=window).std()\n    test_df[\"rolling_var_\" + str(window)] = test_df['signal'].rolling(window=window).var()\n    test_df[\"rolling_min_\" + str(window)] = test_df['signal'].rolling(window=window).min()\n    test_df[\"rolling_max_\" + str(window)] = test_df['signal'].rolling(window=window).max()\n    \n    test_df[\"rolling_min_max_ratio_\" + str(window)] = test_df[\"rolling_min_\" + str(window)] \/ test_df[\"rolling_max_\" + str(window)]\n    test_df[\"rolling_min_max_diff_\" + str(window)] = test_df[\"rolling_max_\" + str(window)] - test_df[\"rolling_min_\" + str(window)]\n\n    \n    a = (test_df['signal'] - test_df['rolling_min_' + str(window)]) \/ (test_df['rolling_max_' + str(window)] - test_df['rolling_min_' + str(window)])\n    test_df[\"norm_\" + str(window)] = a * (np.floor(test_df['rolling_max_' + str(window)]) - np.ceil(test_df['rolling_min_' + str(window)]))\n\ntest_df = test_df.replace([np.inf, -np.inf], np.nan)    \ntest_df.fillna(0, inplace=True)\n","ac66976e":"test_x_scaled = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns)\ndel test_df","439d2ed7":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n\nparams = {'num_leaves': 128,\n          'min_data_in_leaf': 64,\n          'objective': 'huber',\n          'max_depth': -1,\n          'learning_rate': 0.005,\n          \"boosting\": \"gbdt\",\n          \"bagging_freq\": 5,\n          \"bagging_fraction\": 0.8,\n          \"bagging_seed\": 11,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1,\n          'reg_lambda': 0.3\n         }\n","edf6f469":"oof = np.zeros(len(train_x_scaled))\nprediction = np.zeros(len(test_x_scaled))\nscores = []\n\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(train_x_scaled)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = train_x_scaled.iloc[train_index], train_x_scaled.iloc[valid_index]\n    y_train, y_valid = train_y.iloc[train_index], train_y.iloc[valid_index]\n    \n    model = lgb.LGBMRegressor(**params, n_estimators = 6000, n_jobs = -1)\n    model.fit(X_train, y_train, \n            eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n            verbose=500, early_stopping_rounds=200)\n\n    y_pred_valid = model.predict(X_valid)\n    y_pred = model.predict(test_x_scaled, num_iteration=model.best_iteration_)\n\n    oof[valid_index] = y_pred_valid.reshape(-1,)\n    scores.append(mean_absolute_error(y_valid, y_pred_valid))\n\n    prediction += y_pred\n\nprediction \/= n_fold","f5f5cbb4":"from functools import partial\nimport scipy as sp\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize F1 (Macro) score\n    # https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _f1_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n        return -f1_score(y, X_p, average = 'macro')\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._f1_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","3d503278":"optR = OptimizedRounder()\noptR.fit(oof.reshape(-1,), train_y)\ncoefficients = optR.coefficients()\nprint(coefficients)","b8c497a1":"f1_score(train_y, np.round(oof), average = 'macro')","0e900806":"opt_preds = optR.predict(oof.reshape(-1,), coefficients)\nf1_score(train_y, opt_preds, average = 'macro')","2f0ee6be":"prediction[prediction <= coefficients[0]] = 0\nprediction[np.where(np.logical_and(prediction > coefficients[0], prediction <= coefficients[1]))] = 1\nprediction[np.where(np.logical_and(prediction > coefficients[1], prediction <= coefficients[2]))] = 2\nprediction[np.where(np.logical_and(prediction > coefficients[2], prediction <= coefficients[3]))] = 3\nprediction[np.where(np.logical_and(prediction > coefficients[3], prediction <= coefficients[4]))] = 4\nprediction[np.where(np.logical_and(prediction > coefficients[4], prediction <= coefficients[5]))] = 5\nprediction[np.where(np.logical_and(prediction > coefficients[5], prediction <= coefficients[6]))] = 6\nprediction[np.where(np.logical_and(prediction > coefficients[6], prediction <= coefficients[7]))] = 7\nprediction[np.where(np.logical_and(prediction > coefficients[7], prediction <= coefficients[8]))] = 8\nprediction[np.where(np.logical_and(prediction > coefficients[8], prediction <= coefficients[9]))] = 9\nprediction[prediction > coefficients[9]] = 10","a988dfee":"sample_df = pd.read_csv(DIR_INPUT + \"\/sample_submission.csv\", dtype={'time':str})\n\nsample_df['open_channels'] = prediction.astype(np.int)\nsample_df.to_csv(\"submission.csv\", index=False, float_format='%.4f')","45a3ae46":"## Rolling features","d2714e15":"# Model","b1997c3d":"### We fit the optimizedRounder with our training data. We will then use those optimized coeffients for our test predictions","12cd5fab":"# Signal","d181e2a7":"# Regression with Optimized Rounder\n\nThis kernel is based on the great EDA and LightGBM kernel from [pestipeti](https:\/\/www.kaggle.com\/pestipeti).\n\nhttps:\/\/www.kaggle.com\/pestipeti\/eda-ion-switching\n\nInstead of performing a basic round prior to submission, I am implementing a class which optimizes rounding coefficients based on f1 score. This method was used in DSB 2019 for most public kernels, courtesy of [artgor](https:\/\/www.kaggle.com\/artgor) and his 'quick and dirty regression' kernel.\n\nhttps:\/\/www.kaggle.com\/artgor\/quick-and-dirty-regression","46381418":"# Target distribution","445735a5":"### OptimizedRounder class to choose thesholds that result in best F1 score based on validation set","b2b5d51d":"# Statistics","24bcc2c9":"# Train data\n\n> In this competition, you will be predicting the number of open_channels present, based on electrophysiological signal data.\n>\n> **IMPORTANT**: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.\n\n## `time`\nWe have 5M rows in the train dataset. According to the data description we have 50 seconds long 10kHz samples (500,000 rows per batch).\nSo we have 10 batches. The data in a batch is continuous, but discontinuous between batches.\n\n## `signal`\n\n\n## `open_channels`\nPredictions have 11 possible values of open_channels: 0-10.","1cd07924":"## Model Training","6e2335e0":"## Target distribution in different batches","ca4424ff":"### F1 Score on validation set with standard numpy round() function","c76e193a":"### F1 Score on validation set using optimized rounder","c9d2dca8":"## Rounding and Prediction"}}