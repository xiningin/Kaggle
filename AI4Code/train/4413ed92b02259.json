{"cell_type":{"3472ed65":"code","16576bd8":"code","cc124477":"code","2ae2e37d":"code","dbc3a3db":"code","45f8acc0":"code","63bbcd85":"code","6a58d8e4":"code","6e3cad5a":"code","0d49e8be":"code","54ec162a":"code","5db98353":"code","51c1bbec":"code","076d3114":"code","40e43521":"code","4fdd7c5d":"code","f87e5ee1":"code","d8c70c71":"code","539782f0":"code","3d5e8ab8":"code","aa81c31e":"code","7b4f3702":"code","42e053b7":"code","c1548a8e":"code","d0191287":"code","ef1971fd":"code","5b4cbc1b":"code","cbd82fe5":"code","e9f1ed91":"code","806d9eae":"code","87b60f6b":"code","4780e6e5":"code","08d0761c":"code","b9946f68":"code","d986f9e9":"markdown","50e84805":"markdown","d2be39d8":"markdown","7e5520cf":"markdown","a096c635":"markdown","f3c8a5e7":"markdown","ab2a53a2":"markdown","13cf772f":"markdown","aa3f7aff":"markdown","f38f42b5":"markdown","d8824210":"markdown","462e790b":"markdown","ffca7dfc":"markdown","eeb472dd":"markdown","63484eca":"markdown"},"source":{"3472ed65":"import pandas as pd\nimport numpy as np\nmain_file_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv' # this is the path to the Iowa data that you will use\ndata = pd.read_csv(main_file_path)\n\n# Run this code block with the control-enter keys on your keyboard. Or click the blue botton on the left\ndata.shape","16576bd8":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nplt.figure(figsize=(12,6))\n#sns.heatmap(data.corr(),cmap=\"coolwarm\",annot=True)\n","cc124477":"data.info()\n\n","2ae2e37d":"data = data.drop(['Alley','PoolQC','Fence','MiscFeature','Id'],axis=1)\ndata.shape","dbc3a3db":"data.describe()","45f8acc0":"\n#corr_matrix = data.corr()\n#corr_matrix['SalePrice'].sort_values(ascending=False)\n","63bbcd85":"'''\nimport pandas as pd\ndf = pd.DataFrame(corr_matrix[\"SalePrice\"])\ndf = df.drop(df[df.SalePrice>0.6].index)\nlow_corr_attribs = list(df.index)\nlow_corr_attribs.extend(['YearBuilt','YearRemodAdd'])\ndata_filter = data.drop(low_corr_attribs,axis=1)\ndata_filter.shape\n'''","6a58d8e4":"import matplotlib.pyplot as plt\ndata.hist(bins=100,figsize=(50,25))\nplt.show()","6e3cad5a":"'''\noutliers_list = ['2ndFlrSF','3SsnPorch','BsmtHalfBath','BsmtUnfSF','GarageArea']\n#Calculate first and third quartile\noutli\nfirst_quart = data_filter['GarageArea'].describe()['25%']\nthird_quart = data_filter['GarageArea'].describe()['75%']\n#Interquartile range\niqr = third_quart-first_quart\n#Remove outliers\ndata_filter=data_filter[(data_filter['GarageArea']>(first_quart-3*iqr))&(data_filter['GarageArea']<(third_quart+3*iqr))]\nplt.hist(data_filter['GarageArea'])\n'''","0d49e8be":"#housing = train_set.drop('SalePrice',axis=1)\n#housing_labels = train_set['SalePrice'].copy()\n#housing.head()\nhousing= data.drop('SalePrice',axis=1)\nhousing_labels = data['SalePrice'].copy()\nhousing.shape","54ec162a":"\n\n#housing_tr = DataFrameImputer().fit_transform(housing)\n","5db98353":"\nnum_attribs = list(housing.select_dtypes(include=np.number))\ncat_attribs = list(housing.select_dtypes(exclude=np.number))\n","51c1bbec":"from sklearn.preprocessing import StandardScaler,Imputer\nfrom sklearn.pipeline import Pipeline,FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator,TransformerMixin):\n    def __init__(self,attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self,X, y=None):\n        return self\n    def transform(self,X):\n        return X[self.attribute_names].values\n\nclass Imputer_Cat(BaseEstimator,TransformerMixin):\n    def __init__(self,cat_attribs):\n        self.cat_attribs = cat_attribs\n    def fit(self,X,y=None):\n        \n        return self\n        \n    def transform(self,X,y=None):\n        for c in self.cat_attribs:\n            X[c].fillna(X[c].value_counts().index[0])\n        return (X)\n    \nclass CustomLabelEncoder(BaseEstimator,TransformerMixin):\n    def __init__(self,cat_attribs):\n        self.cat_attribs = cat_attribs\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        return pd.get_dummies(X[self.cat_attribs]).values\n        ","076d3114":"\nnum_pipeline = Pipeline([\n    ('selector',DataFrameSelector(num_attribs)),\n    ('imputer',Imputer(strategy='median')),\n    ('std_scaler',StandardScaler()),\n])\n\ncat_pipeline = Pipeline([\n    #('selector',DataFrameSelector(cat_attribs)),\n    ('imputer',Imputer_Cat(cat_attribs)),\n    ('custom_label_encoder',CustomLabelEncoder(cat_attribs))\n])\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('num_pipeline',num_pipeline),\n    ('cat_pipeline',cat_pipeline),\n])\n\nhousing_prepared = full_pipeline.fit_transform(housing)\ncat_values = list(pd.get_dummies(housing[cat_attribs]).columns)\nhousing_prepared_columns = num_attribs+cat_values\nhousing_prepared = pd.DataFrame(housing_prepared,columns=housing_prepared_columns)","40e43521":"#housing_prepared.hist(bins=100,figsize=[100,50])","4fdd7c5d":"data_corr = pd.concat([housing_prepared,housing_labels],axis=1)\ncorr_matrix = data_corr.corr()\ncorr_matrix['SalePrice'].sort_values(ascending=False)","f87e5ee1":"'''\nimport pandas as pd\ndf = pd.DataFrame(corr_matrix[\"SalePrice\"])\ndf = df.drop(df[df.SalePrice>0.6].index)\nlow_corr_attribs = list(df.index)\nlow_corr_attribs.extend(['YearBuilt','YearRemodAdd'])\ndata_filter = data.drop(low_corr_attribs,axis=1)\ndata_filter.shape\n'''","d8c70c71":"from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared,housing_labels)\n\nhousing_pred_forest = forest_reg.predict(housing_prepared)\nplt.scatter(housing_pred_forest,housing_labels.values,c=['g','r'])\nplt.show()\n\nforest_mse = mean_squared_error(housing_labels,housing_pred_forest)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","539782f0":"gradient_reg = GradientBoostingRegressor()\ngradient_reg.fit(housing_prepared,housing_labels)\n\nhousing_pred_gradient = gradient_reg.predict(housing_prepared)\n\ngradient_mse = mean_squared_error(housing_labels,housing_pred_gradient)\ngradient_rmse = np.sqrt(gradient_mse)\ngradient_rmse","3d5e8ab8":"\nfrom sklearn.model_selection import cross_val_score\nforest_scores = cross_val_score(forest_reg,housing_prepared,housing_labels,\n                        scoring='neg_mean_squared_error',cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndef display_scores(scores):\n    print('scores',scores)\n    print('mean',scores.mean())\n    print('Standard deviation:',scores.std())\ndisplay_scores(forest_rmse_scores)\n\n","aa81c31e":"gradient_scores = cross_val_score(gradient_reg,housing_prepared,housing_labels,\n                                 scoring='neg_mean_squared_error',cv=10)\ngradient_rmse_scores = np.sqrt(-gradient_scores)\ndisplay_scores(gradient_rmse_scores)\n","7b4f3702":"loss = ['ls','lad','huber']\nn_estimators = [100,500,900,1100,1500]\nmax_depth = [2,3,5,10,15]\nmin_samples_leaf = [1,2,3,6,8]\nmin_samples_split = [2,4,6,10]\nmax_features = ['auto','sqrt','log2',None]\nhyperparameter_grid = {'loss': loss,\n                      'n_estimators':n_estimators,\n                      'max_depth': max_depth,\n                      'min_samples_leaf': min_samples_leaf,\n                      'min_samples_split': min_samples_split,\n                      'max_features': max_features}\n","42e053b7":"from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n\nrandom_cv = RandomizedSearchCV( estimator = gradient_reg,\n                              param_distributions=hyperparameter_grid,\n                              cv=4, n_iter = 25,\n                              scoring = 'neg_mean_squared_error',\n                              n_jobs = 1, verbose =1,\n                              return_train_score = True,\n                              random_state=42)\nrandom_cv.fit(housing_prepared,housing_labels)\n","c1548a8e":"random_cv.best_params_","d0191287":"trees_grid = {'n_estimators':[100,300,500],'max_features':[30,90,None]}\n              \nmodel = GradientBoostingRegressor(n_estimators=1500,loss='lad',max_depth=5,min_samples_leaf=1,min_samples_split = 10,max_features='sqrt',random_state=42)\n\ngrid_search = GridSearchCV(estimator = model,param_grid=trees_grid,cv=5,scoring ='neg_mean_squared_error')\ngrid_search.fit(housing_prepared,housing_labels)","ef1971fd":"results= pd.DataFrame(grid_search.cv_results_)\n\n#figsize(8,8)\nplt.style.use('fivethirtyeight')\nplt.plot(results['param_n_estimators'],-1*results['mean_test_score'],label = 'Testing Error')\nplt.plot(results['param_n_estimators'],-1*results['mean_train_score'],label ='Training Error')\n","5b4cbc1b":"grid_search.best_estimator_","cbd82fe5":"test_file_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv' # this is the path to the Iowa data that you will use\ntest = pd.read_csv(test_file_path)\ntest.shape","e9f1ed91":"#test_filter = test.drop(low_corr_attribs,axis=1)\n#test_filter.shape","806d9eae":"test_prepared = full_pipeline.transform(test)\ntest_prepared.shape","87b60f6b":"num_of_diffs = 0 \nnet_set_remove =[]\nfor c in cat_attribs:\n    test_set = set(test[c].values)\n    train_set = set(housing[c].values)\n    net_set = [x for x in train_set if x not in test_set]\n    net_set_remove.extend(net_set)\n    #num_of_diffs += len(net_set_remove)\nprint(len(net_set_remove))","4780e6e5":"\nzero_features = np.zeros(shape=(1459,16))\ntest_final = np.concatenate((test_prepared,zero_features),axis=1)\ntest_final.shape\n","08d0761c":"final_model = grid_search.best_estimator_\nfinal_predictions = final_model.predict(test_final)\nfinal_mse = mean_squared_error(housing_labels[:-1],final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","b9946f68":"my_submission = pd.DataFrame({'Id':test.Id,'SalePrice':final_predictions})\nmy_submission.to_csv('submission.csv',index=False)","d986f9e9":"From above histogram , some things to note:\n- need to remove outliers in GarageArea,2nd\n- many attributes have different scales -> featuring scaling\n- many histograms are tail heavy-> transform to have bell-shapes distribution","50e84805":"# Hyperparameter tuning","d2be39d8":"Remove columns with very low data","7e5520cf":"First, remove low correlation attributions\n","a096c635":"Now combine all in one","f3c8a5e7":"# Evaluate your system on test set","ab2a53a2":"Filter numeric attributes and catergorical attributes\n","13cf772f":"# Remove Outliers\n","aa3f7aff":"Plot to discover the distribution shape","f38f42b5":"\n**If you have any questions or hit any problems, come to the [Learn Discussion](https:\/\/www.kaggle.com\/learn-forum) for help. **\n\n**Return to [ML Course Index](https:\/\/www.kaggle.com\/learn\/machine-learning)**","d8824210":"Add cat_features who is not test_prepared by using zeros","462e790b":"the model performs good.\n\nNow lets check on cross-validation","ffca7dfc":"# Introduction\n**This will be your workspace for the [Machine Learning course](https:\/\/www.kaggle.com\/learn\/machine-learning).**\n\nYou will need to translate the concepts to work with the data in this notebook, the Iowa data. Each page in the Machine Learning course includes instructions for what code to write at that step in the course.\n\n# Write Your Code Below","eeb472dd":"Fill Nan value in the data","63484eca":"Create some classes for data preprocessing"}}