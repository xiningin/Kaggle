{"cell_type":{"2f2103d8":"code","6d1dd698":"code","65ae442f":"code","c23a18c0":"code","960b3e74":"code","1a12acc9":"code","8d779ce2":"code","6c405fa2":"code","fd5dc894":"code","999fced9":"code","777efbff":"code","b3f8e3ee":"code","d5b0c06f":"code","714bf1ab":"code","6ae05554":"code","ad7603a4":"code","c2ba8a79":"code","c3b0483e":"code","a263bed5":"code","a862e5a4":"code","34a12663":"code","25b15784":"code","77e09a07":"code","594baedd":"code","2388c8ff":"code","9b2cc602":"code","67f926fc":"code","1f62a880":"code","90431c7b":"markdown","ae243131":"markdown","d64d179e":"markdown","7d5911e4":"markdown","f254b151":"markdown","f8a74953":"markdown","ea7f9914":"markdown","d3859db4":"markdown","339d69b2":"markdown","732d24b4":"markdown","4d114a9c":"markdown","9956a6e6":"markdown","619157c9":"markdown","9ab20ac5":"markdown","624cae73":"markdown","3ac7b7c6":"markdown","0914b416":"markdown"},"source":{"2f2103d8":"%matplotlib inline\nimport numpy as np \nimport pandas as pd\nimport os\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras.models import Model","6d1dd698":"# number of class\nn_class = 5\n\n# path to kaggle dataset\nroot_path = \"\/kaggle\/input\/kneeoa\/\"\n\n# list of folders\nfolder_list = os.listdir(root_path)\nimage_path_list = []\nlabel_list = []\n\n# for each folder, get the image path and labels\nfor folder in folder_list:\n    for label in range(n_class):\n        \n        # get all the images path inside the current folder\n        image_list = os.listdir(f\"{root_path}{folder}\/{label}\")\n        # add to the image path list\n        image_path_list += [ f\"{root_path}{folder}\/{label}\/\"+ path for path in image_list]\n        \n        # add labels to the label list\n        label_list += [label] * len(image_list)\n\n# convert to dataframe\ndf_train_kaggle = pd.DataFrame({\"filepath\" : image_path_list, \"label\": label_list})","65ae442f":"df_train_kaggle.shape","c23a18c0":"df_train_kaggle.label.value_counts().plot.bar()\nplt.xlabel(\"label\")\nplt.ylabel(\"count\")","960b3e74":"# train data generator object\ntrain_aug = ImageDataGenerator(rescale=1.\/255)\n\n# validation data generator object\nvalid_aug = ImageDataGenerator(rescale=1.\/255)","1a12acc9":"# create train generator\ntrain_generator = train_aug.flow_from_dataframe(\ndataframe=df_train_kaggle,\ndirectory=None,\nx_col=\"filepath\",\ny_col=\"label\",\nbatch_size=32,\nseed=42,\nshuffle=True,\nclass_mode=\"raw\",\ntarget_size=(224,224))","8d779ce2":"# download data from shared google drive link\n!pip install gdown\n!gdown --id \"1NdDqPK4NLn2aV8ZdF5ilux1sfG6IyebC\"","6c405fa2":"# unzip the data\n!unzip -q -o \/kaggle\/working\/KneeXray.zip -d \/kaggle\/working\/","fd5dc894":"# read Train.csv file which contains image names and labels and preprocess them\ncompi_root_path= \"\/kaggle\/working\/KneeXray\/\"\ndf_val_compi = pd.read_csv(compi_root_path + \"Train.csv\")\n\n# add absolute path to the image names\ndf_val_compi[\"filename\"] = df_val_compi.filename.apply(lambda x: compi_root_path+\"train\/\" + x)\ndf_val_compi.head()","999fced9":"# class count of compitition dataset\ndf_val_compi.label.value_counts().plot.bar()\nplt.xlabel(\"label\")\nplt.ylabel(\"count\")","777efbff":"# create validation generator\nvalid_generator = valid_aug.flow_from_dataframe( \ndataframe= df_val_compi,\nx_col= \"filename\",\ny_col= \"label\",\nbatch_size= 32,\nseed= 42,\nshuffle= True,\nclass_mode= \"raw\",\ntarget_size= (224,224))","b3f8e3ee":"xception = Xception(weights=\"imagenet\",)\nx=  xception.layers[-3].output\n\nx = tf.keras.layers.Conv2D(filters= 1024, kernel_size= 3, padding= \"same\")(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Activation(\"relu\")(x)\n\nx = tf.keras.layers.Conv2D(filters= 256, kernel_size= 3, padding= \"same\")(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Activation(\"relu\")(x)\n\nx = tf.keras.layers.Conv2D(filters= 64, kernel_size= 3, padding= \"same\")(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Activation(\"relu\")(x)\n\nx = tf.keras.layers.Conv2D(filters= n_class, kernel_size= 3, padding= \"same\")(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Activation(\"relu\")(x)\n\nGAP = tf.keras.layers.GlobalAveragePooling2D()(x)\npred = tf.keras.activations.softmax(GAP)\n\nxception_model = Model(inputs=xception.input,outputs=pred)","d5b0c06f":"# compile\nxception_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001,decay=0.0001),\n                 metrics=[\"acc\"],\n                 loss= tf.keras.losses.sparse_categorical_crossentropy)\n\n# callbacks and checkpoints\ncheckpoint_path = \"xception_best.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\nmy_callbacks = [\n               ModelCheckpoint(checkpoint_path,\n                               monitor = 'val_acc',\n                               verbose = 1,\n                               save_weights_only=True,\n                               save_best_only = True,\n                               mode=\"max\"),\n              EarlyStopping(monitor='val_loss',\n                            patience=5,\n                            verbose=0),\n              ReduceLROnPlateau(monitor='val_loss',\n                                patience=5,\n                                verbose=1)\n]","714bf1ab":"from sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 classes= np.unique(df_train_kaggle.label.values),\n                                                 y= df_train_kaggle.label.values)\nclass_weights = dict(enumerate(class_weights))","6ae05554":"# train the model\nxception_model.fit(\n        train_generator,\n        epochs=100,\n        validation_data=valid_generator,\n        callbacks=[my_callbacks],\n        class_weight=class_weights)\n\n# load best saved weights\nxception_model.load_weights(checkpoint_path)","ad7603a4":"# train and validation split on competition data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(df_val_compi,\n                                   test_size=0.1,\n                                   random_state=42,\n                                   stratify= df_val_compi.label)","c2ba8a79":"train_generator = train_aug.flow_from_dataframe(\ndataframe = X_train,\nx_col=\"filename\",\ny_col=\"label\",\nbatch_size=32,\nseed=42,\nshuffle=True,\nclass_mode=\"raw\",\ntarget_size=(224,224))","c3b0483e":"valid_generator = valid_aug.flow_from_dataframe( \ndataframe=X_test,\nx_col=\"filename\",\ny_col=\"label\",\nbatch_size=32,\nseed=42,\nshuffle=True,\nclass_mode=\"raw\",\ntarget_size=(224,224))","a263bed5":"# number of steps to consider 1 as  epoch\nSTEP_SIZE_TRAIN = train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID =valid_generator.n\/\/valid_generator.batch_size","a862e5a4":"# kick off training\nxception_model.fit(\n        train_generator,\n        steps_per_epoch=STEP_SIZE_TRAIN,\n        epochs=50,\n        validation_data=valid_generator,\n        validation_steps=STEP_SIZE_VALID,callbacks=[my_callbacks])","34a12663":"# load best saved weights\nxception_model.load_weights(checkpoint_path)","25b15784":"from sklearn.metrics import precision_recall_curve\ntarget_shape = 224\nBATCH_SIZE = 1\n\n# test generator\ncompi_gen = valid_aug.flow_from_dataframe(dataframe= X_test,\n                            x_col= \"filename\",\n                            class_mode=None,\n                            target_size= (target_shape, target_shape),\n                            shuffle= False,\n                            batch_size= BATCH_SIZE\n                            )","77e09a07":"# prediction on train data\npredicition_compi = xception_model.predict(compi_gen, steps= compi_gen.n\/ BATCH_SIZE, verbose= 1)","594baedd":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nclass_prediction_compi =  np.argmax(predicition_compi, axis= 1)\ncm = confusion_matrix(X_test.label, class_prediction_compi, labels=[0, 1, 2, 3, 4])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=[0, 1, 2, 3, 4])\ndisp.plot()","2388c8ff":"# read the csv file\ntest = pd.read_csv(compi_root_path + \"Test.csv\")\n\n# create test generator\ntest_generator = valid_aug.flow_from_dataframe(\ndataframe= test,\ndirectory= compi_root_path + \"test\",\nx_col= \"filename\",\ny_col= None,\nbatch_size= 1,\nseed= 42,\nshuffle= False,\nclass_mode= None,\ntarget_size= (224,224))\n\n# number of steps to consider 1 epoch\nSTEP_SIZE_TEST=test_generator.n\/\/test_generator.batch_size","9b2cc602":"# make prediction and create dataframe out of it\npred = xception_model.predict(test_generator,steps=STEP_SIZE_TEST,verbose=1)\ndf_submit = pd.DataFrame({\"label\":np.argmax(pred, axis= 1)})\ndf_submit[\"label\"].value_counts()","67f926fc":"import sys\nimport shutil\n\n# Get directory name\nmydir = \"\/kaggle\/working\"\n\ntry:\n    shutil.rmtree(mydir)\nexcept OSError as e:\n    print(\"Error: %s - %s.\" % (e.filename, e.strerror))","1f62a880":"xception_model.save_weights(\"knee_xray_Xceptionnet_GPA.h5\")\ndf_submit.to_csv(\"submission.csv\",index=False)","90431c7b":"## Making prediction on test set (to make submission)\nFinally we save the predictions on disk in CSV format","ae243131":"### Weighting classes\nAs we have unevenly class distibution, we will weight them based on the number of samples","d64d179e":"### Short description\nThis notebook is a part of [Data Sprint #35: Osteoarthritis Knee X-ray](https:\/\/dphi.tech\/challenges\/data-sprint-35-osteoarthritis-knee-x-ray\/81\/leaderboard\/datathon\/) challenge hosted on [dphi.tech](https:\/\/dphi.tech\/)","7d5911e4":"# Train\nLets roll","f254b151":"### Save best weights and output prediction file","f8a74953":"### Observation\nAs our dataset is imbalanced, we will balance our class by weighting majority class less and minoiry class more","ea7f9914":"# Create validation dataframe using compitition dataset.\nWe will download compition dataset from gdrive and use it as validation set to validated against kaggle dataset","d3859db4":"### Observation\nwe have total of 9786 images in kaggle dataset. We will use data to train the deep learning model\n# Lets look at class distribution","339d69b2":"## Importing dependencies","732d24b4":"# Confusion Matrix\nAs our data set is imbalaned, lets see where is our model making mistakes. I encourage to you to take initative for bringing FPs and FNs down.","4d114a9c":"The submission results in 96.8% on public leaderboard.\n\n**Suggestion to improve the score**\n* Using right data augmentations\n* Using different model architecture\n* Ensembling and stacking\n* Using pretrained model trained on xray images","9956a6e6":"# Basic Idea\nThe Basic idea is to use external data present in kaggle here: [Kaggle: Knee Osteoarthritis Dataset with KL Grading - 2018](https:\/\/www.kaggle.com\/tommyngx\/kneeoa)\n\nAs we have labels for train, validation and test, we will combine all splits into one and test it on dataset provided by the compitition team, this will make sure kaggle dataset and compitition dataset has same data distribution.\n\nIf train(kaggle dataset) and test (compition dataset) data has same distribution then their metric score should be roughy be the same (accuracy score in our case).","619157c9":"# Read And Combining train dataset (kaggle dataset)","9ab20ac5":"### Clearing the working directory\nBecause if don't, \"output\" tabl will show only images","624cae73":"### Observation\nAs we can see train and validation accuracy is pretty close, which proves kaggle and competition data has come from the same distribution and we can freely use it to experiment with.\n\n## Retraining last trained model on competition data\nAs we have used competition data as validation set previously, we will use it as train set now (and some part of it as validation set) hoping this additional training would give our model new information to perform better.","3ac7b7c6":"### Model Architecture\nHere we will be using Xception by google. (I encourage you to try different architectures)","0914b416":"## DataGenerator train and validation\nWe will use kaggle dataset as train set and compitition dataset as validation set. If train and validation metric is similar, it shows their distribution is similar and hence we can use kaggle dataset as well."}}