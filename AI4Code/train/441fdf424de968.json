{"cell_type":{"91785dc7":"code","a92055d5":"code","46832af7":"code","9172b9e8":"code","599c12ef":"code","be314f4c":"code","a6af65cc":"code","d39f80e8":"code","a385261b":"code","f6c7c442":"code","cab37653":"code","6a3826d6":"code","a84d0ffb":"code","2db1835f":"code","b4fe89a6":"code","94b738ac":"code","0e7376bc":"code","7c4f0f89":"code","d9c0ceb0":"code","f9804e30":"code","5cb67c09":"code","42187333":"code","4f76c0f5":"code","629130c0":"code","40a4f144":"code","b9c3c57f":"code","51a6e9c1":"code","e8e5f87e":"code","f58f0428":"code","75c2a81e":"code","ccfd55d2":"code","f4cd6123":"code","dcf6d573":"code","e2028a96":"code","af41e7e6":"code","0eff13fe":"code","98435043":"code","6eb8839f":"code","c86483f4":"code","e7c4c70c":"markdown","687c9fee":"markdown","b4b764cb":"markdown","d55cc6d7":"markdown","53f464c4":"markdown","28e7b138":"markdown","f5e37a3e":"markdown","6ed99680":"markdown","62ebd4cb":"markdown","24646d86":"markdown","fc2db781":"markdown","a6012bd9":"markdown","11927d62":"markdown","2873e6c0":"markdown"},"source":{"91785dc7":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re \nfrom scipy import sparse\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n\nfrom sklearn.linear_model import Ridge\n","a92055d5":"df = pd.read_csv('..\/input\/jigsaw-regression-based-data\/train_data_version2.csv')\ndf.head()","46832af7":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","9172b9e8":"df = df.dropna(axis = 0)","599c12ef":"df.info()","be314f4c":"vec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5), max_features = 46000)\nvec.fit(df['text'])","a6af65cc":"from gensim.models import KeyedVectors, FastText\n\nfmodel = FastText.load('..\/input\/jigsaw-regression-based-data\/FastText-jigsaw-256D\/Jigsaw-Fasttext-Word-Embeddings-256D.bin')","d39f80e8":"from scipy.sparse import hstack\n\ndef splitter(text):\n    tokens = []\n    \n    for word in text.split(' '):\n        tokens.append(word)\n    \n    return tokens\n\ndef vectorizer(text):\n    tokens = splitter(text)\n    \n    x1 = vec.transform([text]).toarray()\n    x2 = np.mean(fmodel.wv[tokens], axis = 0).reshape(1, -1)\n    x = np.concatenate([x1, x2], axis = -1).astype(np.float16)\n    del x1\n    del x2 \n    \n    return x   ","a385261b":"X_list = []\n\nfor text in df.text:\n    X_list.append(vectorizer(text))","f6c7c442":"!pip install -U sentence-transformers","cab37653":"# from sentence_transformers import SentenceTransformer\n# model = SentenceTransformer('all-MiniLM-L6-v2')\n\n#Our sentences we like to encode\n# sentences = ['This framework generates embeddings for each input sentence',\n#     'Sentences are passed as a list of string.', \n#     'The quick brown fox jumps over the lazy dog.']\n\n\n\nsentences = df['text'].values\n# print(len(ssentences))\n\n#Sentences are encoded by calling model.encode()\n# embeddings = model.encode(sentences)\n\n# #Print the embeddings\n# for sentence, embedding in zip(sentences, embeddings):\n#     print(\"Sentence:\", sentence)\n#     print(\"Embedding:\", embedding)\n#     print(\"\")\nprint(embeddings.shape)","6a3826d6":"EMB_DIM = len(vec.vocabulary_) + 256","a84d0ffb":"X_np = np.array(X_list).reshape(-1, EMB_DIM)","2db1835f":"from scipy import sparse\n\nX = sparse.csr_matrix(X_np)\ndel X_np","b4fe89a6":"fast_X = X.todense()\nfast_X.shape","94b738ac":"final = np.concatenate((fast_X, embeddings), axis=1)\nfinal.shape","0e7376bc":"%%time\nmodel = Ridge(alpha=0.5)\nmodel.fit(X, df['y'])","7c4f0f89":"%%time\nl_model = Ridge(alpha=1.)\nl_model.fit(X, df['y'])","d9c0ceb0":"%%time\nd_model = Ridge(alpha=1.5)\nd_model.fit(X, df['y'])","f9804e30":"%%time\ns_model = Ridge(alpha=2.)\ns_model.fit(X, df['y'])","5cb67c09":"df_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")","42187333":"df_val.head()","4f76c0f5":"tqdm.pandas()\ndf_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\ndf_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)","629130c0":"X_less_toxic_temp = []\n\nfor text in df_val.less_toxic:\n    X_less_toxic_temp.append(vectorizer(text))\n\nX_less_toxic_temp = np.array(X_less_toxic_temp).reshape(-1, EMB_DIM)\nX_less_toxic = sparse.csr_matrix(X_less_toxic_temp)\n\ndel X_less_toxic_temp\n    \nX_more_toxic_temp = []\n\nfor text in df_val.more_toxic:\n    X_more_toxic_temp.append(vectorizer(text))\n    \nX_more_toxic_temp = np.array(X_more_toxic_temp).reshape(-1, EMB_DIM)\nX_more_toxic = sparse.csr_matrix(X_more_toxic_temp)\n\ndel X_more_toxic_temp","40a4f144":"p1 = model.predict(X_less_toxic)\np2 = model.predict(X_more_toxic)","b9c3c57f":"# Validation Accuracy\n(p1 < p2).mean()","51a6e9c1":"p1 = l_model.predict(X_less_toxic)\np2 = l_model.predict(X_more_toxic)\n# Validation Accuracy\n(p1 < p2).mean()","e8e5f87e":"p1 = d_model.predict(X_less_toxic)\np2 = d_model.predict(X_more_toxic)\n# Validation Accuracy\n(p1 < p2).mean()","f58f0428":"p1 = s_model.predict(X_less_toxic)\np2 = s_model.predict(X_more_toxic)\n# Validation Accuracy\n(p1 < p2).mean()","75c2a81e":"df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","ccfd55d2":"tqdm.pandas()\ndf_sub['text'] = df_sub['text'].progress_apply(text_cleaning)","f4cd6123":"X_sub_temp = []\nfor text in df_sub.text:\n    X_sub_temp.append(vectorizer(text))\n    \nX_sub_temp = np.array(X_sub_temp).reshape(-1, EMB_DIM)\nX_test = sparse.csr_matrix(X_sub_temp)\n\ndel X_sub_temp","dcf6d573":"p3 = model.predict(X_test)","e2028a96":"p4 = l_model.predict(X_test)\np5 = s_model.predict(X_test)\np6 = d_model.predict(X_test)","af41e7e6":"df_sub['score'] = (p3 + p4 + p5 + p6) \/ 4.","0eff13fe":"df_sub['score'].count()","98435043":"df_sub['score'] = df_sub['score'] ","6eb8839f":"df_sub['score'].nunique()","c86483f4":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","e7c4c70c":"# Prepare validation data","687c9fee":"# TF-IDF","b4b764cb":"<h2>Prepare submission file<\/h2>","d55cc6d7":"# Prepare submission data ","53f464c4":"Ensembling the Ridge Regression models\n","28e7b138":"In below code cell we are creating data which we will feed in model","f5e37a3e":"# Preparing Data For training \n\nWe used 46000 Features of TFIDF Word Representations, 256 Dimensional FastText Word Embeddings (Trained on Toxic Comments) and then concatenate them to make final dataset for Training a model ","6ed99680":"<h2>Text cleaning<\/h2>","62ebd4cb":"Preparing Submission data","24646d86":"<h2>Text cleaning<\/h2>","fc2db781":"## Prediction","a6012bd9":"<h3>Text Cleaning<\/h3>","11927d62":"<h1>Fit Ridge<\/h1>","2873e6c0":"# Loading Data "}}