{"cell_type":{"3303ad62":"code","02946ab7":"code","bb187438":"code","7aa6c998":"code","913c4833":"code","2db875b9":"code","90898eb6":"code","a90aa2e6":"code","d6d1c931":"code","e9283fd7":"code","3f886f30":"code","a43e0059":"code","ee972a09":"code","d18994ca":"code","db4498a5":"code","0bf5c516":"code","15008eee":"code","479718b0":"markdown","e2c630e0":"markdown","e20555da":"markdown","aa49a334":"markdown","389ba490":"markdown","4a87fb25":"markdown","89fe6372":"markdown","ee7a608b":"markdown","89e57e35":"markdown","2368dfe7":"markdown","40053789":"markdown","2a44c74f":"markdown","621eeb3f":"markdown","21464ba4":"markdown","8df2d859":"markdown","d700418d":"markdown","4ff2a8ed":"markdown","afe3e553":"markdown"},"source":{"3303ad62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","02946ab7":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error","bb187438":"housing_train_file_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\nhousing_test_file_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\n\nhousing_train_data = pd.read_csv(housing_train_file_path)\nhousing_test_data = pd.read_csv(housing_test_file_path)\n\nhousing_train_data.columns","7aa6c998":"outofbox_numerical_features = ['BedroomAbvGr', 'FullBath', 'LotArea', 'YearBuilt']","913c4833":"housing_train_data['TotalBathroom'] = \\\n    housing_train_data['BsmtFullBath'] + \\\n    housing_train_data['BsmtHalfBath'] + \\\n    housing_train_data['FullBath'] + \\\n    housing_train_data['HalfBath']\n\nhousing_train_data['TotalSF'] = \\\n    housing_train_data['TotalBsmtSF'] + \\\n    housing_train_data['1stFlrSF'] + \\\n    housing_train_data['2ndFlrSF']\n\ncustom_numerical_features = ['BedroomAbvGr', 'TotalBathroom', 'TotalSF', 'LotArea', 'YearBuilt']","2db875b9":"housing_train_data['HasAlley'] = housing_train_data['Alley'].fillna('None')\n\nAlleyLookUp = {'Grvl' : True, 'Pave': True, 'None': False}\n\nhousing_train_data['HasAlley'] = housing_train_data['HasAlley'].map(AlleyLookUp)\n\nprint(housing_train_data['HasAlley'])\n\nhousing_train_data['HasPool'] = housing_train_data['PoolArea']\n\ncustom_categorical_features = ['BedroomAbvGr', 'LotArea', 'YearBuilt', 'HasAlley', 'HasPool']","90898eb6":"full_features = set( outofbox_numerical_features + custom_numerical_features + custom_categorical_features )\n\nX = housing_train_data[full_features]\n\n# if we encounter 0 values, NaN, NA, or null, let's simply replace with the average values\nX.fillna(X.mean())\n\ny = housing_train_data.SalePrice","a90aa2e6":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","d6d1c931":"housing_model = DecisionTreeRegressor(random_state=1)\n\nhousing_model.fit(train_X, train_y)","e9283fd7":"val_predictions = housing_model.predict(val_X)\n\n# print the top few validation predictions\nprint(val_predictions[:5])\n\n# print the top few actual prices from validation data\nprint(val_y.head().tolist())","3f886f30":"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\nfor max_leaf_nodes in [5, 25, 50, 100, 250, 500]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","a43e0059":"best_tree_size = 50","ee972a09":"optimized_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=0)\n\noptimized_model.fit(X, y)\n\nval_predictions = optimized_model.predict(val_X)\nval_mae = mean_absolute_error(val_y, val_predictions)\n\nprint(\"Validation MAE for Optimized Decision Tree Model: {:.2f}\".format(val_mae))","d18994ca":"rf_model = RandomForestRegressor(random_state=1)\n\nrf_model.fit(X, y)\n\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(val_y, rf_val_predictions)\n\nprint(\"Validation MAE for Random Forest Model: {:.2f}\".format(rf_val_mae))","db4498a5":"# update the features set for test data\nhousing_test_data['TotalBathroom'] = \\\n    housing_test_data['BsmtFullBath'] + \\\n    housing_test_data['BsmtHalfBath'] + \\\n    housing_test_data['FullBath'] + \\\n    housing_test_data['HalfBath']\n\nhousing_test_data['TotalSF'] = \\\n    housing_test_data['TotalBsmtSF'] + \\\n    housing_test_data['1stFlrSF'] + \\\n    housing_test_data['2ndFlrSF']\n\nhousing_test_data['HasAlley'] = housing_test_data['Alley'].fillna('None')\n\nAlleyLookUp = {'Grvl' : True, 'Pave': True, 'None': False}\n\nhousing_test_data['HasAlley'] = housing_test_data['HasAlley'].map(AlleyLookUp)\n\nhousing_test_data['HasPool'] = housing_test_data['PoolArea']","0bf5c516":"test_X = housing_test_data[full_features]\n\n# if we encounter 0 values, NaN, NA, or null, let's simply replace with the average values\ntest_X = test_X.fillna(test_X.mean())\n\nrf_test_predictions = rf_model.predict(test_X)","15008eee":"output = pd.DataFrame({'Id': housing_test_data.Id,\n                       'SalePrice': rf_test_predictions})\n\noutput['SalePrice'] = output['SalePrice'].round(decimals=2)\n\noutput.to_csv('submission.csv', index=False)\n\noutput","479718b0":"Step 4: Optimize fit with tree depth.\n\nThese results don't look great, let's see if we can improve them by controlling tree depth.\n\nWe can use a utility function to help compare Mean Absolute Error scores from different values for max_leaf_nodes.\n\nAnd then compare scores to find the ideal tree size.","e2c630e0":"The best tree size for this model appears to be 50.","e20555da":"We'll run the predictions over this test set.","aa49a334":"Let's take some of the numerical columns from the data set.","389ba490":"Let's create some features for categorical variables, such as the presence of an alley or a pool.\n\n    Alley: Type of alley access to property\n\n       Grvl\tGravel\n       Pave\tPaved\n       NA \tNo alley access\n\nIn this case, the existence of these features are represented by strings, whereas the lack thereof is represented by NA values. We can replace the NA values with 0, and then use a dict to convert that set of categorical data.\n\n    PoolArea: Pool area in square feet\n       \nIn the case of pools, the existence of these features are represented by non-zero integers, which will evaluate to true.\n","4a87fb25":"Let's create some standard features based on the columns, for example, total number of bathrooms.\n\nAs per the documentation, we can sum the following values to get total number of bathrooms.\n\n    \"BsmtFullBath: Basement full bathrooms\n\n    BsmtHalfBath: Basement half bathrooms\n\n    FullBath: Full bathrooms above grade\n\n    HalfBath: Half baths above grade\"\n    \nWe'll do the same for square footage.","89fe6372":"We want a set of the columns to use as features in our regression model. Let's create a few lists of features, to see what works best.\n\n","ee7a608b":"Lastly, all the features we've used in the lists above. As well as the SalePrice from our training set.\n\nWe'll use a set here to only get the distinct features.","89e57e35":"Import the housing training and testing data csv files, in to a pandas data frame. Inspect the training data.","2368dfe7":"Step 1: Split the Data.","40053789":"Step 6: Compare to Random Forest Regression Model","2a44c74f":"Step 2: Create and fit the Model, using the training data.","621eeb3f":"Unfortunately, all of our efforts around fitting our Decision Tree Model were in vain, as the Random Forest Model performed much better.\n\nWe'll use that model to submit for the competition.\n\nRemember, we'll be using the test data set to submit, we'll have to recreate our custom features in the test data set.","21464ba4":"Now we can fit the model using these feature sets, to determine the ideal tree size. ","8df2d859":"Import the necessary modues from the sklearn package.","d700418d":"Step 5: Determine final MAE\n\nLet's use the full training data now that we've fit the model and determined tree size.\n\nWe'll use this model to determine our MAE.","4ff2a8ed":"Output to a .csv file for submission","afe3e553":"Step 3: Make predictions with the validation data.\n\nWe can print the first few predictions, as well as the actual sales price."}}