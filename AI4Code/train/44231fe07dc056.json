{"cell_type":{"f5d62ac8":"code","6bbfa2f5":"code","726f8f55":"code","3a867670":"code","7bdd97d5":"code","2806d9b2":"code","0de327a7":"code","d7bec6ab":"code","ea100449":"code","df9a8a71":"code","41d7dcd4":"code","6f4286c0":"code","0a193208":"code","24a1b8d4":"code","74057fd7":"code","f721abd7":"code","3db26ad6":"code","6faf4466":"code","a6de4843":"code","d696a0f5":"code","fd1cef1e":"code","4dfc2cf2":"code","a4c37767":"code","f0ab74a9":"code","e55bfa98":"code","f5795993":"code","93704e1e":"code","d7b667f0":"code","c5088d44":"code","eb39f404":"code","c66caaac":"code","a66b3823":"code","c83f05f8":"code","cfed38bf":"code","5e98d5e0":"code","09f7385a":"code","e5ee3a0f":"markdown","8a1a5397":"markdown","7b550b67":"markdown","96279833":"markdown","d8cbfaec":"markdown","40797e6a":"markdown","b9cca32d":"markdown","9264d6e3":"markdown","608d1c26":"markdown","c37fe0ef":"markdown","de557d58":"markdown","4c8fe7e5":"markdown","c887acb5":"markdown","7ec71d6b":"markdown","21781a60":"markdown","99bdd65f":"markdown","9db3f27e":"markdown","3d0b4f03":"markdown","2078bf09":"markdown","5a31c663":"markdown","f88e8a9c":"markdown","cc8b0724":"markdown","ff90ab29":"markdown","a44745aa":"markdown","203d0539":"markdown","741822fe":"markdown","16556fe4":"markdown","f23f4a79":"markdown","58a26e9e":"markdown","9729652d":"markdown","d7cb72c3":"markdown","aa5b7b7e":"markdown","dd844d22":"markdown","b041cf6e":"markdown","a490264b":"markdown","3ef51ca8":"markdown","79d4b455":"markdown"},"source":{"f5d62ac8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy, matplotlib.pyplot as plt, IPython.display as ipd, sklearn\nimport os\nimport librosa\nimport librosa.display\nfrom ggplot import *\nimport seaborn as sns","6bbfa2f5":"df=pd.read_csv(\"..\/input\/gtzan-dataset-music-genre-classification\/Data\/features_30_sec.csv\")\ndf.iloc[:,:20].head()","726f8f55":"df.iloc[:,20:40].head()","3a867670":"df.iloc[:,40:60].head()","7bdd97d5":"df[\"chroma_stft_mean\"].describe()","2806d9b2":"y, sr = librosa.load(\"..\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/blues\/blues.00000.wav\")\nipd.Audio(y, rate=sr)","0de327a7":"S = np.abs(librosa.stft(y))\nchroma = librosa.feature.chroma_stft(S=S, sr=sr)\nS = np.abs(librosa.stft(y, n_fft=4096))**2\nchroma = librosa.feature.chroma_stft(S=S, sr=sr)\nfig, ax = plt.subplots()\nimg = librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', ax=ax)\nfig.colorbar(img, ax=ax)\nax.set(title='Chromagram')\nplt.show()","d7bec6ab":"np.mean(chroma)","ea100449":"df[\"rms_mean\"].describe()","df9a8a71":"df[\"spectral_centroid_mean\"].describe()","41d7dcd4":"x, sr = librosa.load('..\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/jazz\/jazz.00016.wav')\nipd.Audio(x, rate=sr)","6f4286c0":"spectral_centroids = librosa.feature.spectral_centroid(y, sr=sr)[0]\nframes = range(len(spectral_centroids))\nt = librosa.frames_to_time(frames)\ndef normalize(y_, axis=0):\n    return sklearn.preprocessing.minmax_scale(y_, axis=axis)\nlibrosa.display.waveplot(x, sr=sr, alpha=0.4)\nplt.plot(t, normalize(spectral_centroids), color='r') # normalize for visualization purposes\nplt.show()\n","0a193208":"spectral_bandwidth_2 = librosa.feature.spectral_bandwidth(x+0.01, sr=sr)[0]\nspectral_bandwidth_3 = librosa.feature.spectral_bandwidth(x+0.01, sr=sr, p=3)[0]\nspectral_bandwidth_4 = librosa.feature.spectral_bandwidth(x+0.01, sr=sr, p=4)[0]\nlibrosa.display.waveplot(x, sr=sr, alpha=0.4)\nplt.plot(t, normalize(spectral_bandwidth_2), color='r')\nplt.plot(t, normalize(spectral_bandwidth_3), color='g')\nplt.plot(t, normalize(spectral_bandwidth_4), color='y')\nplt.legend(('p = 2', 'p = 3', 'p = 4'))\nplt.show()","24a1b8d4":"df=df[['filename', 'chroma_stft_mean', 'chroma_stft_var', 'rms_mean', 'rms_var', 'spectral_centroid_mean',\n      'spectral_centroid_var', 'spectral_bandwidth_mean', 'spectral_bandwidth_var', 'label']]\ndf.isnull().any()","74057fd7":"df.describe()","f721abd7":"p = ggplot(df, aes(x='label', y='chroma_stft_mean', color='label')) +\\\n    geom_point() +\\\n    scale_color_brewer(type='diverging', palette=4) +\\\n    xlab(\"Genre\") + ylab(\"Chroma Stft Mean\") + ggtitle(\"Song Chroma Mean by Genre\")\np","3db26ad6":"reggae_out=df[df['label']=='reggae']\nreggae_out=reggae_out[reggae_out['chroma_stft_mean']>0.55]\nreggae_out","6faf4466":"x, sr = librosa.load('..\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/reggae\/reggae.00051.wav')\nipd.Audio(x, rate=sr)","a6de4843":"x, sr = librosa.load('..\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/reggae\/reggae.00086.wav')\nipd.Audio(x, rate=sr)","d696a0f5":"df=df.drop(851)\ndf=df.drop(886)","fd1cef1e":"outlier=df[df['label']=='classical']\noutlier=outlier[outlier['chroma_stft_mean']>0.4]\nx, sr = librosa.load('..\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/classical\/classical.00080.wav')\nipd.Audio(x, rate=sr)","4dfc2cf2":"outlier=df[df['label']=='disco']\noutlier=outlier[outlier['chroma_stft_mean']<0.3]\nx, sr = librosa.load('..\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/disco\/disco.00047.wav')\nipd.Audio(x, rate=sr)","a4c37767":"df=df.drop(347)","f0ab74a9":"ggplot(df, aes(x='spectral_centroid_mean', color='label')) +\\\n    geom_density(alpha=0.95) +\\\n    facet_wrap(\"label\") +\\\n    xlab(\"Spectral Centroid Mean\") + ggtitle(\"Song Spectral Centroid Mean by Genre\")","e55bfa98":"ggplot(df, aes(x='spectral_centroid_mean', y='chroma_stft_mean', color='label')) +\\\n    geom_point(alpha=0.80) +\\\n    xlab(\"Spectral Centroid Mean\") + ylab(\"Chroma Stft Mean\") + ggtitle(\"Scatterplot for Spectral Centroid vs. Chroma Stft Mean\")","f5795993":"df_no_var=df[['filename', 'chroma_stft_mean', 'rms_mean', 'spectral_centroid_mean', 'spectral_bandwidth_mean', 'label']]\ncorr = df_no_var.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","93704e1e":"df_no_var['chroma_stft_mean']=df_no_var['chroma_stft_mean']\/df_no_var['chroma_stft_mean'].max()\ndf_no_var['rms_mean']=df_no_var['rms_mean']\/df_no_var['rms_mean'].max()\ndf_no_var['spectral_centroid_mean']=df_no_var['spectral_centroid_mean']\/df_no_var['spectral_centroid_mean'].max()\ndf_no_var['spectral_bandwidth_mean']=df_no_var['spectral_bandwidth_mean']\/df_no_var['spectral_bandwidth_mean'].max()\ndf_no_var.describe()","d7b667f0":"df2=pd.read_csv(\"..\/input\/additionaldata1\/additional_df.csv\")\nprint(df2.shape)\nprint(df2.isnull().any())\ndf2.head()","c5088d44":"df2.groupby(['label']).size()","eb39f404":"p = ggplot(df2, aes(x='label', y='chroma_stft_mean', color='label')) +\\\n    geom_point() +\\\n    scale_color_brewer(type='diverging', palette=4) +\\\n    xlab(\"Genre\") + ylab(\"Chroma Stft Mean\") + ggtitle(\"Song Chroma Mean by Genre\")\np","c66caaac":"rock=df2[df2['label']=='rock']\nrock=rock[rock['chroma_stft_mean']<0.55]\nrock","a66b3823":"df2=df2.drop(784)","c83f05f8":"df_no_var['dataset'] = \"Source 1\"\ndf2['dataset'] = \"Source 2\"\n\ndf2['chroma_stft_mean']=df2['chroma_stft_mean']\/df2['chroma_stft_mean'].max()\ndf2['rms_mean']=df2['rms_mean']\/df2['rms_mean'].max()\ndf2['spectral_centroid_mean']=df2['spectral_centroid_mean']\/df2['spectral_centroid_mean'].max()\ndf2['spectral_bandwidth_mean']=df2['spectral_bandwidth_mean']\/df2['spectral_bandwidth_mean'].max()\n\nmaster_df = pd.concat([df_no_var, df2])\nmaster_df","cfed38bf":"ggplot(master_df, aes(x='spectral_centroid_mean', color='dataset')) +\\\n    geom_density(alpha=0.95) +\\\n    facet_wrap(\"label\") +\\\n    xlab(\"Spectral Centroid Mean\") + ggtitle(\"Comparing spectral centroid distributions for different data sources\")","5e98d5e0":"ggplot(master_df, aes(x='chroma_stft_mean', color='dataset')) +\\\n    geom_density(alpha=0.95) +\\\n    facet_wrap(\"label\") +\\\n    xlab(\"Chroma Mean\") + ggtitle(\"Comparing chroma means for different data sources\")","09f7385a":"# import pandas as pd\n# import numpy as np\n# import librosa\n# import os\n# import math\n\n# list_=[]\n\n# for dirname, _, filenames in os.walk('E:\/code\/470w30'):\n#     for filename in filenames:\n        \n#         path_=(os.path.join(dirname, filename))\n#         label=(dirname.split('\\\\')[1])\n#         y, sr = librosa.load(path_)\n#         S = np.abs(librosa.stft(y))\n#         chroma = librosa.feature.chroma_stft(S=S, sr=sr)\n#         chroma_mean = (np.mean(chroma))\n        \n#         rms=math.sqrt(np.mean(y*y))\n        \n#         spectral_centroids = librosa.feature.spectral_centroid(y, sr=sr)[0]\n#         spectral_mean = (np.mean(spectral_centroids))\n        \n#         spectral_bandwidth_2 = librosa.feature.spectral_bandwidth(y+0.01, sr=sr)[0]\n#         spectral_bandwidth_2 = np.mean(spectral_bandwidth_2)\n#         spectral_bandwidth_3 = librosa.feature.spectral_bandwidth(y+0.01, sr=sr, p=3)[0]\n#         spectral_bandwidth_3 = np.mean(spectral_bandwidth_3)\n#         spectral_bandwidth_4 = librosa.feature.spectral_bandwidth(y+0.01, sr=sr, p=4)[0]\n#         spectral_bandwidth_4 = np.mean(spectral_bandwidth_4)\n#         band_mean = np.mean(np.array([spectral_bandwidth_2, spectral_bandwidth_3, spectral_bandwidth_4]))\n        \n#         list_.append([filename, chroma_mean, rms, spectral_mean, band_mean, label])\n        \n# df_=pd.DataFrame(list_, columns=['filename', 'chroma_stft_mean', 'rms_mean', 'spectral_centroid_mean', 'spectral_bandwidth_mean', 'label'])\n\n# df_.to_csv(\"additional_df.csv\", index=False)","e5ee3a0f":"From this chart we can note a couple of things --\n* Our variables are definitely all on different scales. Our max values range from 0.027 to 3,036,843, with spectral variables being the ones with high values.\n* There is a lot of variation for spectral centroid and not much for chroma stft or rms.\n* There doesn't seem to much skew to our data as our mean and max don't seem too far apart for any of our variables.\n\nLet's do some visualizations.","8a1a5397":"The main difference in the two datasets is the difference in size for labels, with country only having 66 songs while blues has 124. I am aiming to continue adding songs to this dataset as the semester goes on so I will have this fixed soon.","7b550b67":"# Examining data sources\n\n## Data Source 1\n\nFor this project, we were given an initial data source from Kaggle organized by Andrada Olteanu from Romania.\nThe dataset, known as the [GTZAN Dataset](https:\/\/www.kaggle.com\/andradaolteanu\/gtzan-dataset-music-genre-classification), is a collection of 30 second snippets from songs, with a 100 songs for 10 genres equalling a total of 1000 songs.\n\nThe dataset also contains features of audio analysis for each of the songs in .csv form that we will be using to explore the data.\n\nWhile Andrada organized, uploaded, and generated features from the song snippets, the original data was recorded by G. Tzanetakis and P. Cook in _IEEE Transactions on Audio and Speech Processing_ in 2002, a journal for the \"science, technologies and applications that relate to the analysis, coding, enhancement, recognition and synthesis of audio, music, speech and language\".\nThe songs were all pulled from personal CDs, radio, microphone recordings, in order to represent a variety of recording conditions, and are all 22050Hz Mono 16-bit audio files in .wav format.\n\n## Data Source 2\n\nWe also added in our own dataset of songs downloaded from Youtube Playlists by genre using a command line program. These are upwards of 60+ songs for each genre and are also all 22050Hz Mono 16-bit audio files in .wav format. These are all also cut down to only 30 second snippets using the Digital Audio Workstation _Ableton_.\n\nA lot of these songs are similar to each other in time released, as for example many of the rap songs from the same playlist came out around similar times. To get a more well rounded dataset, I will soon download more playlists from older or newer music from the genre. Our existing GTZAN dataset has mostly old music, as it is from 2002.\n\nThe features for this data will be generated by us later on in the exploration and encorporated into the GTZAN Dataset.","96279833":"The other features we will not use as there are no standardized ways of computing them, so we cannot recreate them for our new songs.\n\nLet's check for missingness.","d8cbfaec":"This looks good! No missing values for any of our columns of interest.\n\nNext let's look at some tables and graphs.","40797e6a":"This gives us additional information about each of the songs in our data. The features are taking the mean and variance across all of this data.\n\n[Spectral Bandwidth](https:\/\/musicinformationretrieval.com\/spectral_features.html) is similar to above, just with a weighted p to provide different measures of spectral analysis.","b9cca32d":"## Dataset Source 2","9264d6e3":"The other outliers investigated proved to be correctly sorted.","608d1c26":"To make this heatmap simpler, I only visualized the means for each of our variables.\nWe see all of our variables are pretty strongly correlated together, especially our two spectral centroids.\nThis could potentially cause problems for us in the future and it might be better to only use one of them.\n\nFor recoding variables, we could divide each category by the max of that category to get a final percent value.","c37fe0ef":"This dataset has 788 rows while our previous had 1000.","de557d58":"# Purpose\n\nOur research question was the following: __Can we create a machine learning model that can classify songs in their respective categories to a reasonable accuracy?__\n\nThe end goal of this project will be to use a pre-trained machine learning model to classify the audio files directly, and either compare it's performance to this model or use them together to create a hybrid model.\n\nHowever, for the purpose of feature exploration, it will be more useful to explore the features in a tabular state.","4c8fe7e5":"I won't play this file as about 5 seconds the file glitches very loud for the rest of the recording -- no wonder it showed up so far away from our other values. I am going to remove this one as well.","c887acb5":"Let's begin by printing the head of our entire dataframe. It is wide so it'll have to be printed out 20 columns at a time.","7ec71d6b":"This looks very similar to our above plot with the Chroma Mean variable as they all follow very similar distributions. Let's see if we graph a scatterplot with both as the x and y values.","21781a60":"[RMS level][3] (root mean squared) is fairly simple -- it is just __proportional to the amount of energy over a period of time in the signal.__ This can be used to distinguish songs that are louder from each other.\n\n[3]: https:\/\/en.wikipedia.org\/wiki\/Audio_power#Continuous_power_and_%22RMS_power%22\n\nThis RMS value is generated using numpy and is a fairly simple calculation (again, root mean squared).\n","99bdd65f":"I am not sure if that qualifies as disco -- I would call that more like a Disney song or something? I am going to remove it.","9db3f27e":"In music, the term [chroma feature][1] relates to the twelve different pitch classes. Chroma-based featurescan be used to categorize music with meaningful pitches (usually into 12 scales) and whose tuning can equated to the equal-tempered scale.\n\n[1]: https:\/\/en.wikipedia.org\/wiki\/Chroma_feature\n\n\nIn our dataset, Andrada used the [librosa][2] package, a python package for music and audio analysis, to extract features from each of our song snippets.\n\n[2]: https:\/\/librosa.org\/doc\/latest\/index.html\n\nLet's generate a Chromagram from one of our blues songs using [librosa][2].","3d0b4f03":"That concludes our outliers. Next let's view some graphs for the spectral centroid statistics.","2078bf09":"Our offending files are 51 and 86. Let's have a listen to them.","5a31c663":"# Offline Audio Feature Extraction","f88e8a9c":"## Data Source 1","cc8b0724":"We see a surprisingly similar graph to what we saw before, with classical being the lowest and rock\/metal being the highest.","ff90ab29":"Now that we have established which features are of interest to us and possible to replicate from our first dataset, we can generate them from the songs we downloaded in our second dataset. I did this offline as uploading all of the songs to Kaggle would have taken too long.\n\nThe code for that is at the bottom of the page and commented out if you are interested.\n\nFor this case, we will quickly examine the second dataset source similarly to how we did the first, looking at descriptive statistics generated in the same way as the above features and looking for outliers to see if I made any mistakes or the playlists had any mislabeled songs.","a44745aa":"For our generated features, the mean and variance was taken across the entire generated array.","203d0539":"So -- our values aren't as linear as I thought. There is a loosely linear correlation between the two but it is not super strong.\n\nSpeaking of correlation, let's do a heatmap to see how correlated our variables are to each other.","741822fe":"While that Metallica one is correct, the Helen O'Connel is one from the Jazz category and got incorrectly sorted.","16556fe4":"We see here that majority of our genres follow a very similar distribution, with the main differences being in pop and rock -- most likely because the Youtube playlists I downloaded for these genres were majority songs from 2020.\nIn order to fix this, I'll add in other playlists that are from older songs to more fit the curve from the older dataset. \n\nI also was unable to find any good Metal playlists as most of them were just the same songs in the rock playlist, so this is something I need to go back to and really search for.","f23f4a79":"That is a lot of columns!\n\nJust doing a simple inspection of our data, we see it is 1000 rows by 60 columns. There are quite a lot of features that were generated for us by Andrada (58 in total and all numerical), so lets quickly move through them to get a better sense of our data.\n\nIf we want encorporate files from our second data source, we should know how each of these variables were calculated as well so we can generate them for our own songs.","58a26e9e":"Our differences are just exacerbated here, as for almost every category they are higher up on the chroma mean scale. Possibly the quality from Youtube is better than the quality from the GTZAN dataset, as those songs were all pulled from old personal CDs, radio, microphone recordings.","9729652d":"This one just contains a lot of silence, but that is just how the song sounds. It shouldn't affect our label too much.","d7cb72c3":"Exploring these outliers was useful for reggae. Let's check out some others.","aa5b7b7e":"The [spectral centroid](https:\/\/en.wikipedia.org\/wiki\/Spectral_centroid) is a measure used to characterise an audio spectrum by finding its center of mass. It is also connected to the brightness of a sound, which refers to the higher mid and treble parts of the frequency.\n\nWe can use the librosa package again to compute this frequency that the energy of the spectrum centers upon.\n\nLet's start by loading in one of our jazz songs.","dd844d22":"Next we will find the spectral centroids per each frame in an audio signal, compute time for visualization, and then plot.","b041cf6e":"So.. this doesn't sound like reggae. This is a lot more electronic music. I am going to drop this out of the dataset as it seems to be mislabeled.","a490264b":"One last thing I wanted to examine is how these two Data sources compare side by side using a density plot.","3ef51ca8":"# Concluding Thoughts\n\n* We have successfully identified our variables of interest for our data through research and exploration of our features.\n* We cleaned our data by picking out outliers and mislabeled values through using visualizations as well normalizing our features.\n* We encorporated our own data and merged the two dataframes together and compared the differences between them.\n\n\nMoving forward, this should set the base for a good baseline model to compare against a pre-trained neural network and to potentially merge together.","79d4b455":"So we can see there's definitely a difference across our Genres when comparing the Chroma mean. Metal has the highest while classical has the lowest.\nReggae has a few interesting outliers that go pretty far up as well. Let's look at those."}}