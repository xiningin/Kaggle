{"cell_type":{"94dd51a1":"code","daa4082d":"code","9d933212":"code","c013108c":"code","112dbf3a":"code","2486753a":"code","dfb1312d":"code","bad4ba0a":"code","3bc8013c":"code","2d4562f4":"code","940923a6":"code","394aab34":"markdown","55fed8b3":"markdown","44b0cbde":"markdown","4ca4bd91":"markdown","2648ad5c":"markdown","09d5688d":"markdown","f8fa4c9d":"markdown","ebc9212d":"markdown","d064511a":"markdown"},"source":{"94dd51a1":"# !pip freeze > '..\/working\/requirements.txt'","daa4082d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm # Prints a progress bar (0 to 100%) when doing training\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9d933212":"df=pd.read_csv(r\"\/kaggle\/input\/iris\/Iris.csv\")\nprint(\"Shape of the input dataframe: \",df.shape,\"\\n\")","c013108c":"print(\"The column names are:\")\nfor i,colName in enumerate(df.columns):\n    print(\"\\t\",i,colName)\nprint(\"\\n\")","112dbf3a":"print(\"There are \",len(df.Species.unique()),\" unique species in this dataset.\")\nprint(\"They are rows 1-50,51-100,101-150\")","2486753a":"if \"Id\" in df.columns:\n    df = df.drop(['Id'],axis=1)\n    \n    \ntarget = df['Species']\ns = set()\nfor val in target:\n    s.add(val)\ns = list(s)\n\n\nrows = list(range(100,150))\nprint(\"There are \",len(df.Species.unique()),\" unique species in this dataset.\")\nif (df.shape[0]==150):\n    df = df.drop(df.index[rows])\nprint(\"There are now \",len(df.Species.unique()),\" unique species in this dataset.\")    \ndfBackup=df.copy()","dfb1312d":"x = df['SepalLengthCm']\ny = df['PetalLengthCm']\n\nsetosa_x = x[:50]\nsetosa_y = y[:50]\n\nversicolor_x = x[50:]\nversicolor_y = y[50:]\n\nplt.figure(figsize=(8,6))\nplt.scatter(setosa_x,setosa_y,marker='+',color='green')\nplt.scatter(versicolor_x,versicolor_y,marker='_',color='red')\nplt.show()","bad4ba0a":"df=dfBackup.copy()\n## Here we drop the other two features that we will not use for this SVM\n# There should be 4 features and the target vector\nif \"SepalWidthCm\" and \"PetalWidthCm\" in df.columns:    \n    df = df.drop(['SepalWidthCm','PetalWidthCm'],axis=1)\n# There should now be 2 features and the target vector   \nprint(\"After dropping 2 features, column names in df\",df.columns,\"\\n\")\n\nY = []\ntarget = df['Species']\nfor val in target:\n    if(val == 'Iris-setosa'):\n        Y.append(-1)\n    else:\n        Y.append(1)\nif \"Species\" in df.columns:   \n    df = df.drop(['Species'],axis=1)\nprint(\"After dropping target column, the remaining features\/column names in df are: \",df.columns)    \nX = df.values.tolist()\n\n## Shuffle and split the data into training and test set\nX, Y = shuffle(X,Y)\nx_train = []\ny_train = []\nx_test = []\ny_test = []\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.9)\n\nx_train = np.array(x_train)\ny_train = np.array(y_train)\nx_test = np.array(x_test)\ny_test = np.array(y_test)\n\ny_train = y_train.reshape(90,1)\ny_test = y_test.reshape(10,1)","3bc8013c":"## Support Vector Machine \n\ntrain_f1 = x_train[:,0]\ntrain_f2 = x_train[:,1]\n\ntrain_f1 = train_f1.reshape(90,1)\ntrain_f2 = train_f2.reshape(90,1)\n\nw1 = np.zeros((90,1))\nw2 = np.zeros((90,1))\n\nimport time\nalpha = 0.0001\nepochNum=10000\nclassification=[]\nmisclassification=[]\nfor i in tqdm(range(epochNum)):\n    epochs = i+1    \n    y = w1 * train_f1 + w2 * train_f2\n    # Prediction multiplied by target\n    prod = y * y_train\n    count = 0\n    correctClassification=0\n    incorrectClassification=0\n    for val in prod:\n        # If no misclassification\n        if(val >= 1):\n            correctClassification=correctClassification+1\n            cost = 0\n            w1 = w1 - alpha * (2 * 1\/epochs * w1)\n            w2 = w2 - alpha * (2 * 1\/epochs * w2)\n        # If misclassification    \n        else:\n            incorrectClassification=incorrectClassification+1\n            cost = 1 - val \n            w1 = w1 + alpha * (train_f1[count] * y_train[count] - 2 * 1\/epochs * w1)\n            w2 = w2 + alpha * (train_f2[count] * y_train[count] - 2 * 1\/epochs * w2)\n        count += 1\n    classification.append(correctClassification)\n    misclassification.append(incorrectClassification)\nw1Backup=w1.copy()\nw2Backup=w2.copy()","2d4562f4":"fig, axs = plt.subplots(2, 1, figsize=(8, 6), tight_layout=True)\n# plt.subplot(2,1,1)\naxs[0].set_title(\"Number of Correct Classifications against Epochs\")\naxs[0].plot(classification)\n# plt.subplot(2,1,2)\naxs[1].set_title(\"Number of Misclassifications against Epochs\")\naxs[1].plot(misclassification)\n# plt.subplots_adjust(top=0.9)","940923a6":"w1=w1Backup.copy()\nw2=w2Backup.copy()\n\n## Clip the weights \nindex = list(range(10,90))\nw1 = np.delete(w1,index)\nw2 = np.delete(w2,index)\nprint(\"Before reshape: \",w1.shape)\nw1 = w1.reshape(10,1)\nw2 = w2.reshape(10,1)\nprint(\"After reshape: \",w1.shape)\n\n## Extract the test data features \ntest_f1 = x_test[:,0]\ntest_f2 = x_test[:,1]\ntest_f1 = test_f1.reshape(10,1)\ntest_f2 = test_f2.reshape(10,1)\n\n## Make predictions using the trained weight vectors\ny_pred = w1 * test_f1 + w2 * test_f2\npredictions = []\n\nfor val in y_pred:\n    if(val > 1):\n        predictions.append(1)\n    else:\n        predictions.append(-1)\n\n# Using an accuracy score function from : sklearn.metrics.accuracy_score\nprint(\"The final score as a percentage is: \",accuracy_score(y_test,predictions)*100,\"%\")\nprint(\"This concludes the SVM for the IRIS dataset\")","394aab34":"## Two questions need verifying\n1. Why during training do we reduce the regularization parameter in using the inverse of the number of epochs\n2. Why are all weights equal? Why can we clip the weight vector to be the size of the test vector?","55fed8b3":"# Let's first read in the Iris dataset","44b0cbde":"##  Using the new vectors computed to make predictions on the test dataset\n* We reduce the size of the weight vector from 90 samples to 10 samples\n* The reason for this reduction is due to the test set being 10 samples","4ca4bd91":"# There are 2 features, we split the data out into these 2 variables\n\n**Recollecting that we are using a 90:10 train:test split. This means that 90 samples\/rows will be used for the training data. The remaining 10 rows\/samples will be used for testing.**\n*  $\\alpha$ is the learning rate and is set to 0.0001\n* Epochs are the number of iterations used to converge to the final solution\n* $\\lambda$ represents the regularization parameter. This will reduce as the number of epochs increase by $\\lambda$=1\/epochs\n\n\n## When do we update regularization parameter and\/or the loss function\n* It must be noted that if there is **no misclassification** (i.e. the prediction is greater than 1), then we only update the term with the regularization parameter\n* If there **is a misclassification**, we update both the loss function and the term with the regularization parameter","2648ad5c":"## Observations\n* We are only updating the information relating to the vectors (i.e. w1 and w2) - this is where the name comes from \"**Support Vector** Machines\"\n* w1 and w2 are initialized as being '0' vectors. \n* The only reason we have the weights as a vector of 90 samples is due to the features being 90 samples.\n* Based on the previous 2 points, this explains why the weight vectors have a constant value and we can clip them (numpy.delete) to make predictions","09d5688d":"# This is an adaption of the blog post entitled:\n*  **Support Vector Machine \u2014 Introduction to Machine Learning Algorithms**\n* **URL:** [link](https:\/\/towardsdatascience.com\/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47) \n","f8fa4c9d":"## We drop rows 100 to 150 since this represent the third species in the datset\n","ebc9212d":"## We will create a \"90:10\" \"train:test\" split after dropping two features (SepalWidthCm and PetalWidthCm) that will not be considered\n","d064511a":"## Let's plot the sepal length and petal length \n\n\n**we are only going to consider these two features making this a binary classification problem**"}}