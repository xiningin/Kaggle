{"cell_type":{"cbecb65d":"code","6cdee7e7":"code","63a96b2f":"code","ce7eb815":"code","029b0093":"code","a3087793":"code","6ff05513":"code","c54fc176":"code","358fdb77":"code","4f72645b":"code","c5379fbc":"code","3db5dbb7":"code","37fad7ad":"code","f1024854":"code","c9aed2c8":"code","4c034063":"code","86735682":"code","9d3cbee6":"code","1b584355":"code","c971e9a6":"code","9aa89d2a":"code","9d828f79":"code","85416007":"code","735dec9e":"code","ba186f05":"code","779d20aa":"code","8c662181":"code","716c225b":"code","60b1a591":"code","e8f3a6cf":"code","ba5f501a":"code","d1d0073c":"code","70c3fc0d":"code","0383b579":"code","bdc8c61f":"code","f9e6901e":"code","4817d296":"code","eb660e8f":"code","548ac3e5":"code","47ce9dc3":"code","2a6f87af":"code","2fe4ddda":"code","9082c78f":"code","99dd7aa2":"markdown","4556a005":"markdown","f7014040":"markdown","30bd4b51":"markdown","9b8c9b46":"markdown","2bff55ea":"markdown","81327d19":"markdown","f52e275d":"markdown","cf5d4f0b":"markdown","0583ac38":"markdown","1b6332fa":"markdown","5a383802":"markdown","229515e7":"markdown","16ec9d8b":"markdown","da11e12f":"markdown","570e5b08":"markdown","70c7dc86":"markdown","610648ed":"markdown","7a3e323a":"markdown","c5dcdb52":"markdown","6b5b2407":"markdown","abb22c9c":"markdown","c40edcbc":"markdown","8e6a1951":"markdown","1faa33d5":"markdown","b3540c52":"markdown","d76c169b":"markdown","7d47ff11":"markdown","298bd597":"markdown","b094a859":"markdown"},"source":{"cbecb65d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\nhouse = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\nprint('house shape:',house.shape)\nprint('test shape:',test.shape)\noriginal_test = test","6cdee7e7":"pd.set_option('display.max_columns', None)\nhouse.head(6)","63a96b2f":"display(house.select_dtypes('object').head(6))","ce7eb815":"house.dtypes.value_counts()\ncategorical_columns = house.select_dtypes('object').columns\nprint(len(house.select_dtypes('object').columns),'categorical columns:')\nprint(list(house.select_dtypes('object').columns),'\\n')\nprint(len(house.columns)-len(house.select_dtypes('object').columns),'numerical columns (including sales price):')\nprint([i for i in list(house.columns) if i not in list(house.select_dtypes('object').columns)])","029b0093":"display(house._get_numeric_data().describe().round(decimals=1))","a3087793":"display(house.select_dtypes('object').describe())\n# display(house._get_numeric_data().columns.str.contains('Yr|Year'))","6ff05513":"print('univariate analysis on distribution of categorical variables...')\ncolumns = list(house.select_dtypes('object').columns)\nplt.figure(figsize=[16,30])\nfor i in range(len(house.select_dtypes('object').columns)):\n    ax = plt.subplot(11,4,i+1)\n    group = house[columns[i]].value_counts()\n    plt.barh(group.index,(group.values\/len(house)),)\n    plt.title(columns[i])\n    ax.set_xlim([0,1])\n    box = ax.get_position()\n    box.y1 = box.y1 - 0.0075 \n    ax.set_position(box)","c54fc176":"print('bivariate analysis on categorical variables to dependent variables')\ncolumns = list(house.select_dtypes('object').columns)\nplt.figure(figsize=[16,30])\nfor i in range(len(house.select_dtypes('object').columns)):\n    ax = plt.subplot(11,4,i+1)\n    sns.boxplot(data=house,y=columns[i],x='SalePrice')\n    plt.title(columns[i])\n#     ax.set_xlim([0,1])","358fdb77":"print('Analyze:\\n','1: outliers\\n','2: skewed variables\\n','The outliers and the skewed variables must be addressed accordingly...')\ncolumns = list(house.select_dtypes(exclude='object').columns)\nplt.figure(figsize=[16,30])\nfor i in range(len(house.select_dtypes(exclude='object').columns)):\n    try:\n        ax = plt.subplot(10,4,i+1)\n        plt.scatter(house[columns[i]],house.SalePrice,alpha=0.15)\n        plt.title(columns[i])\n        box = ax.get_position()\n        box.y1 = box.y1 - 0.01 \n        ax.set_position(box)\n    except:\n        pass\nplt.show()","4f72645b":"original_house = house\nprint('filtering outliers...')\n\nold = house.LotArea\nhouse = house.drop(house['LotFrontage'][house['LotFrontage']>280].index)\nprint('rows deleted so far:',str(len(original_house)-len(house)))\nhouse = house.drop(house['LotArea'][house['LotArea']>50000].index)\nprint('rows deleted so far:',str(len(original_house)-len(house)))\nhouse = house.drop(house['BsmtFinSF1'][house['BsmtFinSF1']>4000].index)\nprint('rows deleted so far:',str(len(original_house)-len(house)))\nhouse = house.drop(house['BsmtFinSF2'][house['BsmtFinSF2']>1400].index)\nprint('rows deleted so far:',str(len(original_house)-len(house)))\nhouse = house.drop(house['TotalBsmtSF'][house['TotalBsmtSF']>5800].index)\nprint('rows deleted so far:',str(len(original_house)-len(house)))\nhouse = house.drop(house['1stFlrSF'][house['1stFlrSF']>4000].index)\nprint('rows deleted so far:',str(len(original_house)-len(house)))\nhouse = house.drop(house['GrLivArea'][house['GrLivArea']>4000].index)\nprint('rows deleted so far:',str(len(original_house)-len(house)))\nhouse = house.drop(house['PoolArea'][house['PoolArea']>400].index)\nprint('rows deleted so far:',str(len(original_house)-len(house)))\nhouse = house.drop(house['MiscVal'][house['MiscVal']>3000].index)\nprint('rows deleted so far:',str(len(original_house)-len(house)))","c5379fbc":"plt.figure(figsize=[11,4])\nplt.subplot(1,2,1)\nplt.title('Sale Price')\nplt.hist(house.SalePrice,bins=40)\n\nplt.subplot(1,2,2)\nplt.title('Log of Sale Price')\nlog = house.SalePrice.apply(lambda x: np.log(x))\nplt.hist(log,bins=40)\nhouse.SalePrice = log","3db5dbb7":"print('Transformed skewed variables (seen on bivariate plot) to be more symmetric')\n\nplt.figure(figsize=[11,4])\nplt.subplot(1,2,1)\nplt.title('Before')\nplt.scatter(house.LotArea,house.SalePrice,alpha=0.25)\nplt.ylabel('log(SalePrice)')\nplt.xlabel('LotArea')\n\nlog_columns = ['LotFrontage','LotArea','BsmtFinSF1','BsmtFinSF2','MasVnrArea','BsmtUnfSF','TotalBsmtSF','1stFlrSF','GrLivArea','WoodDeckSF','OpenPorchSF']\nfor i in log_columns:\n    try:\n        house[i] = house[i].apply(lambda x: np.log(x) if x!=0 else x)\n        test[i] = test[i].apply(lambda x: np.log(x) if x!=0 else x)\n    except:\n        print('failed log on',house[i])\n\nplt.figure(figsize=[11,4])\nplt.subplot(1,2,2)\nplt.title('After')\nplt.scatter(house.LotArea,house.SalePrice,alpha=0.25)\nplt.xlabel('LotArea')\n","37fad7ad":"print('Transformed year columns to age-based (relative to year sold)')\n\nplt.figure(figsize=[11,4])\nplt.subplot(1,2,1)\nplt.title('Before')\nplt.scatter(house.YearBuilt,house.SalePrice,alpha=0.25)\nplt.ylabel('log(SalePrice)')\nplt.xlabel('YearBuilt')\n\nyear = ['YearBuilt','YearRemodAdd','GarageYrBlt']\nfor i in year:\n    house[i] = house['YrSold'] - house[i]\n    test[i] = test['YrSold'] - test[i]\n\nplt.subplot(1,2,2)\nplt.title('After')\nplt.scatter(house.YearBuilt,house.SalePrice,alpha=0.25)\nplt.xlabel('YearBuilt (now age)')","f1024854":"print('here to doublecheck for any skewed distribution in variables...')\ncolumns = list(house.select_dtypes(exclude='object').columns)\nplt.figure(figsize=[16,30])\nfor i in range(len(house.select_dtypes(exclude='object').columns)):\n    try:\n        ax = plt.subplot(10,4,i+1)\n        plt.title(columns[i])\n        box = ax.get_position()\n        box.y1 = box.y1 - 0.01 \n        ax.set_position(box)\n        sns.distplot(house[columns[i]])\n    except:\n        pass\nplt.show()","c9aed2c8":"correlation = house.corr()\n\nplt.subplots(figsize=(14,12))\nplt.title('Correlation of numerical attributes', size=16)\nsns.heatmap(correlation)","4c034063":"decision_point = 0.1\nprint('selecting variables with correlation of more than',decision_point)\ncorrelation[correlation.SalePrice>decision_point].SalePrice.sort_values(ascending=False)\n\ncolumns = correlation[correlation.SalePrice > decision_point].SalePrice.sort_values(ascending=False).index\ncolumns_below_correlation_threshold = correlation[correlation.SalePrice<=decision_point].SalePrice.sort_values(ascending=False).index","86735682":"print('plotting the qualified correlated columns...')\nplt.figure(figsize=[16,30])\nfor i in range(len(columns)):\n    try:\n        ax = plt.subplot(10,4,i+1)\n        plt.scatter(house[columns[i]],house.SalePrice,alpha=0.15)\n        plt.title(columns[i]+' (cor: '+str(round(correlation[correlation.index==columns[i]]['SalePrice'].values[0],2))+')')\n        box = ax.get_position()\n        box.y1 = box.y1 - 0.01 \n        ax.set_position(box)\n    except:\n        pass\nplt.show()","9d3cbee6":"threshold = 0.7\nprint('checking for multi-colinearity above',threshold,'if found, choose only the more highly correlated column')\ncross_correlation = house[columns[1:]].corr()\nplt.subplots(figsize=(10,8))\nplt.title('Correlation of numerical attributes (high correlation to SalePrice)', size=16)\nsns.heatmap(pd.DataFrame(np.where(np.array(cross_correlation)>threshold,1,0),columns=cross_correlation.columns,index=cross_correlation.columns))","1b584355":"co_correlation = ['TotRmsAbvGrd','GarageArea','LotFrontage']\nprint('dropping multicolinear columns for good:\\n',['TotRmsAbvGrd','GarageArea','LotFrontage'])\nhouse = house.drop(columns=co_correlation)\n\nnum_columns = list(columns)\nfor i in num_columns: \n    if i in co_correlation: num_columns.remove(i)\n    if i == 'SalePrice': num_columns.remove(i)\nprint('the numerical columns to keep are:\\n',num_columns)","c971e9a6":"print('one last glance at numerical variables to be good to go...')\nhouse[num_columns].describe()","9aa89d2a":"def missing_values_table(df):\n    mis_val=df.isnull().sum()    \n    mis_val_perc=100*df.isnull().sum()\/len(df)\n    mis_val_table=pd.concat([mis_val, mis_val_perc], axis=1) \n    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n    print (\"Your selected data frame has \" + str(df.shape[1]) + \" columns.\\n\"+\"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n \" columns that have missing values.\")\n    return mis_val_table_ren_columns\n\nmiss = missing_values_table(house.select_dtypes(exclude='object'))\nprint(miss,'\\n')\n\nmiss = missing_values_table(test.select_dtypes(exclude='object'))\nprint('Test:')\nprint(miss,'\\n')\nprint('Missing data is few in the dataset. We will just impute with ''median'' values later in modelling')","9d828f79":"def missing_values_table(df):\n    mis_val=df.isnull().sum()    \n    mis_val_perc=100*df.isnull().sum()\/len(df)\n    mis_val_table=pd.concat([mis_val, mis_val_perc], axis=1) \n    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n    print (\"Your selected data frame has \" + str(df.shape[1]) + \" columns.\\n\"+\"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n \" columns that have missing values.\")\n    return mis_val_table_ren_columns\n\nmiss = missing_values_table(house.select_dtypes('object'))\nprint(miss[miss['% of Total Values']>15],'\\n')\n\nmiss = missing_values_table(test.select_dtypes('object'))\nprint('Test:')\nprint(miss[miss['% of Total Values']>15],'\\n')\nprint('Many missing values in the categorical columns')","85416007":"cat_cols_fill_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n                     'GarageCond', 'GarageQual', 'GarageFinish', 'GarageType',\n                     'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond',\n                     'MasVnrType']\nfor cat in cat_cols_fill_none:\n    house[cat] = house[cat].fillna(\"None\")\n    test[cat] = test[cat].fillna('None')\nprint('Replaced columns with ''None''as many of the columns are nan because they don''t exist in those houses')","735dec9e":"def missing_values_table(df):\n    mis_val=df.isnull().sum()    \n    mis_val_perc=100*df.isnull().sum()\/len(df)\n    mis_val_table=pd.concat([mis_val, mis_val_perc], axis=1) \n    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n    print (\"Your selected data frame has \" + str(df.shape[1]) + \" columns.\\n\"+\"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n \" columns that have missing values.\")\n    return mis_val_table_ren_columns\n\nmiss = missing_values_table(house.select_dtypes('object'))\nprint('Train:')\nprint(miss[miss['% of Total Values']>15],'\\n')\n\nmiss = missing_values_table(test.select_dtypes('object'))\nprint('Test:')\nprint(miss[miss['% of Total Values']>15],'\\n')","ba186f05":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle_count = 0\n\n# house.Alley = house.Alley.apply(lambda x: str(x))\nfor col in house:\n    if house[col].dtype == 'object' and house[col].nunique() <= 2:\n        le.fit(house[col])\n        house[col] = le.transform(house[col])\n        le_count += 1\nprint('%d columns encoded.' % le_count)\n\nprint('house shape:',house.shape)\nprint('test shape:',test.shape)","779d20aa":"house = pd.get_dummies(house,drop_first=True)\ntest = pd.get_dummies(test,drop_first=True)","8c662181":"print('house shape:',house.shape)\nprint('test shape:',test.shape)\n\ntrain = house\ntrain_label = train.SalePrice\ntrain, test = house.align(test,join='inner', axis = 1)\ntrain['SalePrice'] = train_label\n\nprint('training features shape:',train.shape)\nprint('testing features shape:',test.shape)\n","716c225b":"train_knn = train","60b1a591":"from scipy.stats import ttest_ind\np_stack = []\ncol_stack = []\np_stack_qualified = []\ncol_stack_qualified = []\nk=1\nfor j in categorical_columns:\n    variable = j\n    cat_columns = [i for i in train.columns if variable in i]\n    for i in cat_columns:\n        zstat, pval = ttest_ind(train.SalePrice[train[i]==0],train.SalePrice[train[i]==1])\n        if pval < 0.05:\n            p_stack_qualified.append(pval)\n            col_stack_qualified.append(i)\n        k+=1\nplt.figure(figsize=[8,25])\nplt.plot(p_stack_qualified,col_stack_qualified)\nplt.title('p-value of all the one-hot-encoded columns')","e8f3a6cf":"correlation = house[col_stack_qualified].corr()\nplt.figure(figsize=[18,18])\ndt_heatmap = pd.DataFrame(np.where(np.logical_or(np.array(correlation)>0.7,np.array(correlation)<-0.7),1,0),columns=correlation.columns,index=correlation.columns)\nsns.heatmap(dt_heatmap)\nprint('Notice highly correlated categorical variables...')","ba5f501a":"col_dic =dict(list(zip(col_stack_qualified,p_stack_qualified)))\ntemp_dict = {}\ncat_column = []\nlist_=dt_heatmap.index\np=0\nfor i in dt_heatmap.index:\n    col_compare = dt_heatmap[i][dt_heatmap[i]==1].index\n    if dt_heatmap[i][dt_heatmap[i]==1].index.nunique()==1: cat_column.append(i)\n    else:\n        for k in range(dt_heatmap[i][dt_heatmap[i]==1].index.nunique()): #2\n            temp_dict[col_compare[k]]=col_dic[col_compare[k]]\n        if min(temp_dict, key=temp_dict.get) == i: cat_column.append(i)\n    temp_dict = {}\n    p+=1\nprint('deleted',len(dt_heatmap.index)-len(cat_column),'columns that show multi-collinearity and the other colinear variable had more significant p-value')","d1d0073c":"correlation = house[cat_column].corr()\nplt.figure(figsize=[18,18])\ndt_heatmap = pd.DataFrame(np.where(np.logical_or(np.array(correlation)>0.7,np.array(correlation)<-0.7),1,0),columns=correlation.columns,index=correlation.columns)\nsns.heatmap(dt_heatmap)\nprint('No more multi-colinearity...')","70c3fc0d":"from sklearn.model_selection import train_test_split\ny = train['SalePrice']\nx = train[num_columns + cat_column]\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state = 6)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy = 'median')\nimputer.fit(x_train)\nx_train = imputer.transform(x_train)\nx_test = imputer.transform (x_test)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nmlr = LinearRegression()\nmlr.fit(x_train,y_train)\nprint('AdjR2 for train',mlr.score(x_train, y_train))\nprint('AdjR2 for test',mlr.score(x_test, y_test))\ny_pred = mlr.predict(x_test)\nmean = np.exp(house.SalePrice.mean())\nprint('Mean:',mean)\ny_test = np.exp(y_test)\ny_pred = np.exp(y_pred)\nprint('RMS Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('+- of',str(round(100*np.sqrt(metrics.mean_squared_error(y_test, y_pred))\/mean,0))+'%')","0383b579":"hi = pd.DataFrame(list(zip(num_columns + col_stack_qualified,list(mlr.coef_))),columns=['columns','coefficient'])\n# hi['log_coefficient']=hi.coefficient.apply(lambda x: np.log(x))\npd.set_option('display.max_columns', None)\nplt.figure(figsize=[8,29])\nplt.plot(hi['coefficient'],hi['columns'])\nplt.xlabel('coefficients')","bdc8c61f":"df = pd.DataFrame({'Actual': y_test, 'Predicted':y_pred})\ndf1 = df.head(20)\ndf1.plot(kind='bar',figsize=(16,10))","f9e6901e":"y_train = train['SalePrice']\nx_train = train[num_columns + cat_column]\nx_test = test[num_columns + cat_column]\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy = 'median')\nimputer.fit(x_train)\nx_train = imputer.transform(x_train)\nx_test = imputer.transform (x_test)\n\nfrom sklearn.linear_model import LinearRegression\nmlr = LinearRegression()\nmlr.fit(x_train,y_train)\ny_pred = mlr.predict(x_test)\ny_pred = np.exp(y_pred)\n\nprint('AdjR2 for train',mlr.score(x_train, y_train))\npd.DataFrame(list(zip(list(original_test['Id']),list(y_pred))),columns=['Id','SalePrice']).to_csv('results_linear.csv',index=False)","4817d296":"hi = pd.DataFrame(list(zip(num_columns + col_stack_qualified,list(mlr.coef_))),columns=['columns','coefficient'])\n# hi['log_coefficient']=hi.coefficient.apply(lambda x: np.log(x))\nplt.figure(figsize=[8,20])\npd.set_option('display.max_columns', None)\nplt.plot(hi['coefficient'],hi['columns'])","eb660e8f":"from sklearn.model_selection import train_test_split\ny = train['SalePrice'] #undo the ln function used above\nx = train[num_columns + cat_column]\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state = 6)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nscalery = StandardScaler()\ny_train = scalery.fit_transform(y_train.values.reshape(-1,1))\ny_test = scalery.transform(y_test.values.reshape(-1,1))\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy = 'median')\nimputer.fit(x_train)\nx_train = imputer.transform(x_train)\nx_test = imputer.transform (x_test)","548ac3e5":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn import metrics\n\nscore = []\nfor i in range(2,40):\n    classifier = KNeighborsRegressor(i, weights = 'distance')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    score.append(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n\nplt.figure(figsize=[16,7])\nplt.subplot(1,2,1)\nplt.title('Distance')\nplt.plot(range(2,40),score)\n\nuniform_score = []\nfor i in range(2,40):\n    classifier = KNeighborsRegressor(i, weights = 'uniform')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    uniform_score.append(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n\nplt.subplot(1,2,2)\nplt.title('Uniform')\nplt.plot(range(2,40),uniform_score)","47ce9dc3":"neighbors = score.index(min(score))+2\nclassifier =KNeighborsRegressor(neighbors, weights = 'distance')\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\nprint('Best number of neighbors',neighbors)\n\ny_pred = scalery.inverse_transform(y_pred)\ny_test = scalery.inverse_transform(y_test)\nmean = np.exp(house.SalePrice.mean())\ny_test = np.exp(y_test)\ny_pred = np.exp(y_pred)\n\nprint('Mean:',mean)\nprint('RMS Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('+- of',str(round(100*np.sqrt(metrics.mean_squared_error(y_test, y_pred))\/mean,0))+'%')\n","2a6f87af":"print('some preperation to look at our prediction errors...')\nnew_y_test = []\nfor i in y_test:\n    new_y_test.append(i[0])\ny_test = new_y_test   \n\nnew_y_pred = []\nfor i in list(y_pred):\n    new_y_pred.append(i[0])\ny_pred = list(new_y_pred)    ","2fe4ddda":"df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf1 = df.head(20)\ndf1.plot(kind='bar',figsize=(16,10))","9082c78f":"#now we fit all the features\ny = train['SalePrice'] #undo the ln function used above\nx = train[num_columns + cat_column]\ncompetition_data = test[num_columns + cat_column]\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx = scaler.fit_transform(x)\ncompetition_data = scaler.transform(competition_data)\nscalery = StandardScaler()\ny = scalery.fit_transform(y.values.reshape(-1,1))\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan,strategy='median')\nx = imputer.fit_transform(x)\ncompetition_data = imputer.transform(competition_data)\n\nclassifier = KNeighborsRegressor(neighbors,weights = 'distance')\nclassifier.fit(x,y)\ny_pred = classifier.predict(competition_data)\ny_pred = scalery.inverse_transform(y_pred)\ny_pred = np.exp(y_pred)\nanswer = []\nfor i in y_pred:\n    answer.append(i[0])\npd.DataFrame(list(zip(list(original_test['Id']),list(answer))),columns=['Id','SalePrice']).to_csv('results_knn.csv',index=False)","99dd7aa2":"# Data Processing (Reduce Skew on x)","4556a005":"* Now we finalized our values with lowest Root Mean Square Error","f7014040":"* We try to fit our data with KNN and check for the best number of neighbors\n* KNN works on distance metrics so it is advised to perform normalization on y too, which is not done in linear regression","30bd4b51":"# Column types","9b8c9b46":"# Linear Regression","2bff55ea":"Looking at the correlation chart, we ought to drop columns that is below a correlation value","81327d19":"# Multi-colinearity (Numerical)","f52e275d":"# Housing Prices\n\nHi guys, I made an attempt to analyze the dataset and built 2 machine learning models to predict the Sale Price of houses. Enjoy! Working on this, I have studied many notebooks & other websites. So yes, it is not 100% original, but I'll be grateful to get any pointers from you guys...","cf5d4f0b":"# Data Processing (Feature Engineer)","0583ac38":"1. Save dataframe in case we use another model which don't require the steps necessary for linear model","1b6332fa":"# More Visualization on Numerical variables...","5a383802":"# K Nearest Neighbours","229515e7":"Below we add 'columns_miss' for columns with a lot of nan, the fact that they are nan could be useful for model.","16ec9d8b":"# Addressing Missing data (Numerical)","da11e12f":"# Multi-colinearity (Categorical)","570e5b08":"# Statistics of columns","70c7dc86":"# Data Processing (Reduce Skew on y)","610648ed":"# One Hot Encode (Categorical)","7a3e323a":"Among the well-correlated variables. Check cross-correlation with other features. If found, check if they correlate well to SalePrice, if they do, you have to choose the best correlated one.","c5dcdb52":"# Label Encode (Categorical)","6b5b2407":"* There are negatively\/positively skewed columns.\n* for negatively skewed, we apply log function\n* for positively skewed, we apply exp function\n","abb22c9c":"# 1 - Quick glance at data","c40edcbc":"# Data Processing (Dependant Variables - Filter Outliers)","8e6a1951":"* Linear model is unimpressive. With values going to negative and values too big in magnitude. We can explore other models like KNN\n* But lets run the same steps on the whole train dataset and apply it on test data for submission.","1faa33d5":"* Outliers in numerical data relation","b3540c52":"# Visualization (43 categorical, 38 numerical columns) ","d76c169b":"look for multicollinearity within","7d47ff11":"* Now we fit the 'test.csv' data into our model for submission!","298bd597":"# Correlation Study (Numerical Data)","b094a859":"Looking at the heat plot for cross correlation. We have keep the former, drop the latter as the former is better correlated to the SalePrice:\n* GrLivArea \/ TotRmsAbvGrd\n* GarageCars \/ GarageArea\n* LotArea \/ LotFrontage"}}