{"cell_type":{"e14281ef":"code","e92a12a9":"code","76bbf306":"code","1ba1608f":"code","5efa3028":"code","0cfd17e5":"code","edaf1f78":"code","3ff2ae74":"code","854bdc71":"code","1b25d008":"code","484732ac":"code","f35af3d4":"code","30b735e9":"code","77b89277":"code","a34838f9":"code","5d501b78":"code","1b9b5463":"code","14175986":"code","80f2569a":"code","d8725d04":"code","32ac3e00":"markdown"},"source":{"e14281ef":"# installing haystack\n! pip install git+https:\/\/github.com\/deepset-ai\/haystack.git\n    \n# Installing Elasticsearch\n! wget https:\/\/artifacts.elastic.co\/downloads\/elasticsearch\/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n! chown -R daemon:daemon elasticsearch-7.6.2","e92a12a9":"# General libraries\nimport re, os, string, random, requests\nimport pandas as pd\nfrom subprocess import Popen, PIPE, STDOUT\nfrom tqdm import tqdm\n\n# Haystack importings\nfrom haystack import Finder\nfrom haystack.reader.farm import FARMReader\nfrom haystack.utils import print_answers\nfrom haystack.document_store.elasticsearch import ElasticsearchDocumentStore\nfrom haystack.retriever.sparse import ElasticsearchRetriever","76bbf306":"# Starting ElasticSearch server as daemon\nes_server = Popen(['elasticsearch-7.6.2\/bin\/elasticsearch'],\n                   stdout=PIPE, stderr=STDOUT,\n                   preexec_fn=lambda: os.setuid(1)  # as daemon\n                  )\n\n# wait until ElasticSearch has started\n! sleep 30","1ba1608f":"def get_index(n):\n    \"\"\"Return a random string of length n\"\"\"\n    letters = string.ascii_lowercase\n    result_str = ''.join(random.choice(letters) for i in range(n))\n    return result_str","5efa3028":"def trim_doc(doc):\n    \"\"\"Trim doc with respect to the boundary of a sentence.\"\"\"\n    \n    trimmedText = []\n    charCount = 0\n    for sentence in doc.split('.'):\n        if charCount < DOC_THRESHOLD:\n            charCount+=len(sentence.strip())\n            trimmedText.append(sentence)\n\n    finalText = \".\".join(trimmedText)\n    \n    return finalText","0cfd17e5":"def clean_text(text):\n    \"\"\"Doc cleaning\"\"\"\n    \n    # Lowering text\n    text = text.lower()\n    \n    # Removing punctuation\n    text = \"\".join([c for c in text if c not in PUNCTUATION])\n    \n    # Removing whitespace and newlines\n    text = re.sub('\\s+',' ',text)\n    \n    # Trimming doc\n    text = trim_doc(text)\n    return text","edaf1f78":"# Constants\nES_INDEX = get_index(10) # Elastic Search DB index name\nPUNCTUATION = \"\"\"!\"#$%&'()*+,-\/:;<=>?@[\\]^_`{|}~\"\"\" # excluding . (full-stop) from the set of punctuations\nDOC_THRESHOLD = 10000 # character limit for a doc\nTOP_K_RETRIEVER = 10 # top k documents to analyze further for a given query\nTOP_K_READER = 5 # top k number of answers to return\nBASE_URL = \"http:\/\/localhost:9200\/\"+ES_INDEX+\"\/_doc\/\"","3ff2ae74":"data = pd.read_csv(\"\/kaggle\/input\/nips-papers-1987-2019-updated\/papers.csv\")\ndata.head()","854bdc71":"data.shape","1b25d008":"data.dropna(subset=['full_text'], inplace=True)\ndata.shape","484732ac":"# Structuring data to haystack required format\n# Format: [{'text': 'paper_content', 'meta':{'name':'title'}}]\ndocs = []\ncorpora = []\ndoc_len = []\n\nfor index, row in tqdm(data.iterrows()):\n    dicts = {}\n    dicts['text'] = clean_text(row['full_text'])\n    doc_len.append(len(dicts['text']))\n    corpora.append(dicts['text'])\n    dicts['meta'] = {}\n    dicts['meta']['name'] = clean_text(row['title'])\n    docs.append(dicts)","f35af3d4":"# Average characters in a document after trimming\nsum(doc_len)\/len(docs)","30b735e9":"# Be careful while overwriting data on the same ES index\ndocument_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=ES_INDEX)","77b89277":"# Now, let's write the dicts containing documents to our DB.\ndocument_store.write_documents(docs)","a34838f9":"# Instantiating ES retriever \nretriever = ElasticsearchRetriever(document_store=document_store)","5d501b78":"# Initializing reader on the top of roberta-base-squad2 pre-trained model, which will be downloaded on the first run\n# Here, we can set the size of context window for our answers and use the GPU if available\n\nreader = FARMReader(model_name_or_path=\"ahotrod\/albert_xxlargev1_squad2_512\",use_gpu=True, context_window_size=500)","1b9b5463":"# Fitting reader and retriever to Finder\nfinder = Finder(reader, retriever)","14175986":"# Question prediction with TOP_K_RETRIEVER and TOP_K_READER\nquestion = \"What is the use of CNN?\"\nprediction = finder.get_answers(question=question, top_k_retriever=TOP_K_RETRIEVER, top_k_reader=TOP_K_READER)","80f2569a":"# Printing answers with minimal detail\n# details = minimal | medium | all\n\nprint_answers(prediction, details=\"minimal\")","d8725d04":"pd.DataFrame(prediction['answers'])","32ac3e00":"## Amazing \ud83d\udd25\nThe question was, **What is the use of CNN?**.\n\nAs we all know, the CNN (ConvNet\/Convolutional Neural Network) algorithm deals with the image data. It looks like our QA system has answered our query very well. Yayyy!! \ud83e\udd73 \ud83c\udf89\ud83c\udf89\ud83c\udf89\n![](https:\/\/media.tenor.com\/images\/5a2d3ba3504d3f48da005d9fe6b52110\/tenor.gif)\n\nDon't forget to upvote the notebook, if you like my work. Let me know your feedback in the comment section below. \ud83d\ude0a\n\n### #StaySafe"}}