{"cell_type":{"8101903a":"code","f4dbae02":"code","84a125f8":"code","f784376d":"code","e4f14f9d":"code","d46c044e":"code","c69c745a":"code","3eb5b529":"code","90ef5ca6":"code","7e083c44":"code","88da5f7c":"code","e078b84b":"code","cb957be5":"code","c157d4ac":"code","2b1f769e":"code","0fab21fb":"code","2c70f63a":"code","dc2ebbb7":"code","895d41d7":"code","3bdcc4c1":"code","a3a1161a":"code","36c1d88e":"code","ca80fb61":"code","0690e9e4":"code","a7431d1e":"code","e45a9704":"code","f3e913f0":"code","e44c13e3":"code","4705467f":"code","0065f108":"code","ca1d8690":"code","51818b14":"code","faeccac4":"code","a1319690":"code","a0d2ba57":"code","f5a0ae8a":"code","0547736f":"code","8caeb816":"code","f478da5e":"code","702c726b":"code","0c39f280":"code","d5f1e78b":"code","b786483c":"code","27e7ce33":"markdown","fd5fad5b":"markdown","4d55eda5":"markdown","aa93d05c":"markdown","2bc53f5c":"markdown","5aee425b":"markdown","2b705c8e":"markdown","d653edd3":"markdown","1d20f3c7":"markdown"},"source":{"8101903a":"#import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\nfrom numpy import NaN\npd.set_option('display.max_rows', 70)\npd.set_option('display.max_columns', 70)\npd.set_option('display.width', 100)\n","f4dbae02":"prop16 = pd.read_csv(\"..\/input\/zillow-prize-1\/properties_2016.csv\")\nprop17 = pd.read_csv(\"..\/input\/zillow-prize-1\/properties_2017.csv\")\nsmplsub = pd.read_csv(\"..\/input\/zillow-prize-1\/sample_submission.csv\")\ntrain16 = pd.read_csv(\"..\/input\/zillow-prize-1\/train_2016_v2.csv\")\ntrain17 = pd.read_csv(\"..\/input\/zillow-prize-1\/train_2017.csv\")","84a125f8":"prop16.head()","f784376d":"prop17.head()","e4f14f9d":"train16.head()","d46c044e":"train17.head()","c69c745a":"smplsub.head()","3eb5b529":"#function to get all info in one go\ndef full_info(df):\n    df_column=[]\n    df_dtype=[]\n    df_null=[]\n    df_nullc=[]\n    df_mean=[]\n    df_median=[]\n    df_std=[]\n    df_min=[]\n    df_max=[]\n    df_uniq=[]\n    for col in df.columns: \n        df_column.append( col)\n        df_dtype.append( df[col].dtype)\n        df_null.append( round(100 * df[col].isnull().sum(axis=0)\/len(df[col]),2))\n        df_nullc.append( df[col].isnull().sum(axis=0))\n        df_uniq.append( df[col].nunique()) if df[col].dtype == 'object' else df_uniq.append( NaN)\n        df_mean.append(  '{0:.2f}'.format(df[col].mean())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_mean.append( NaN)\n        df_median.append( '{0:.2f}'.format(df[col].median())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_median.append( NaN)\n        df_std.append( '{0:.2f}'.format(df[col].std())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_std.append( NaN)\n        df_max.append( '{0:.2f}'.format(df[col].max())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_max.append( NaN)\n        df_min.append( '{0:.2f}'.format(df[col].min())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_min.append( NaN)\n    return pd.DataFrame(data = {'ColName': df_column, 'ColType': df_dtype, 'NullCnt': df_nullc, 'NullCntPrcntg': df_null,  'Min': df_min, 'Max': df_max, 'Mean': df_mean, 'Med': df_median, 'Std': df_std, 'UniqCnt': df_uniq})","90ef5ca6":"prop16_Info = full_info(prop16)\nprop16_Info.sort_values(by=['NullCnt'], ascending=False, inplace=True, ignore_index=True)\nprop16_Info","7e083c44":"full_info(train16)","88da5f7c":"print('size of properties_2016.csv: ', prop16.shape)\nprint('size of train_2016_v2.csv: ', train16.shape)\nprint('size of properties_2017.csv: ', prop17.shape)\nprint('size of train_2017.csv: ', train17.shape)","e078b84b":"unique_props = len(train16['parcelid'].unique())\nmultiple_sales = len(train16) - unique_props\nprint('number of unique sales: ', unique_props)\nprint('Number of duplicate: ', multiple_sales)","cb957be5":"# lets visualize the Null Count percentage graphically\nprop16_Info.plot.bar(x = 'ColName', y = 'NullCnt', figsize=(25, 6),rot=90, title='Missing (null) Feature Values')\nplt.show()","c157d4ac":"# interactive feature transfromation.\nprop16['prop_age'] = 2018 - prop16['yearbuilt']  # property age\nprop16['has_basement'] = prop16['basementsqft'].apply(lambda x: 0 if np.isnan(x) else 1).astype(float)\nprop16['has_pool'] = prop16[['poolcnt','poolsizesum','pooltypeid10','pooltypeid2','pooltypeid7']].apply(lambda x: 1 if(np.all(pd.notnull(x[1]))) else 0, axis = 1)\nprop16['has_patio_yard'] = prop16['yardbuildingsqft17'].apply(lambda x: 0 if np.isnan(x) else 1).astype(float)\nprop16['has_starage_yard'] = prop16['yardbuildingsqft26'].apply(lambda x: 0 if np.isnan(x) else 1).astype(float)\nprop16['has_garage'] = prop16['garagecarcnt'].apply(lambda x: 0 if np.isnan(x) else 1).astype(float)","2b1f769e":"# some nan features actually make sense, lets fill them with 0\nprop16.yardbuildingsqft17.fillna(0, inplace=True)\nprop16.yardbuildingsqft26.fillna(0, inplace=True)\nprop16.basementsqft.fillna(0, inplace=True)\nprop16.poolcnt.fillna(0, inplace=True)\nprop16.poolsizesum.fillna(0, inplace=True)\nprop16.pooltypeid10.fillna(0, inplace=True)\nprop16.pooltypeid2.fillna(0, inplace=True)\nprop16.pooltypeid7.fillna(0, inplace=True)\nprop16.garagecarcnt.fillna(0, inplace=True)","0fab21fb":"# drop columns with data that has > 90% null\nprop16_trim = prop16.drop(prop16_Info[(prop16_Info.NullCntPrcntg>=90)].ColName.values.tolist(),axis=1)\nprop16_trim","2c70f63a":"prop16_trim.select_dtypes(include=['object']).columns","dc2ebbb7":"# these object dtype not categorical. can be ignored.\nprop16_trim[['propertycountylandusecode', 'propertyzoningdesc']]","895d41d7":"# drop the object dtype columns\nprop16_trim=prop16_trim.drop(['propertycountylandusecode', 'propertyzoningdesc'],axis=1)","3bdcc4c1":"# lets fill rest of the NaNs with medians\nprop16_median_imputed = prop16_trim.fillna(prop16_trim.median())\nprop16_median_imputed","a3a1161a":"# lets concatenated both property and train data\ntrain16_merge = pd.merge(prop16_median_imputed, train16, on='parcelid', how='inner')\ntrain16_merge","36c1d88e":" train16_merge=train16_merge.drop(['parcelid'],axis=1)","ca80fb61":"# lets check the correlation of the feature to target\nfrom yellowbrick.target.feature_correlation import feature_correlation\nX, y = train16_merge.drop(columns =[ 'logerror', 'transactiondate', 'fireplacecnt']), train16_merge['logerror']\n\nfeatures = np.array(train16_merge.drop(columns = [ 'logerror', 'transactiondate', 'fireplacecnt']).columns)\nfig, ax = plt.subplots(figsize=(10,18))\nvisualizer = feature_correlation(X, y, labels=features, sort= True, color='gray', show=True, ax=ax)\nplt.show()","0690e9e4":"# import LightGBM Libraries\nimport lightgbm as lgb\nimport random","a7431d1e":"#LightGBM accepts numphy array as input\nx_train = train16_merge.drop(columns =[ 'logerror', 'transactiondate', 'fireplacecnt']).values.astype(np.float32) # np array\ny_train = train16_merge['logerror'].values.astype(np.float32)  # np array\nx_test = train16_merge.drop([ 'logerror', 'transactiondate', 'fireplacecnt'], axis=1).values.astype(np.float32)  # np array\ntrain_columns = train16_merge.drop(columns = [ 'logerror', 'transactiondate', 'fireplacecnt']).columns ","e45a9704":"# manually added the features as the numbers in some features is not acceptable for lgb\nd_train = lgb.Dataset(x_train, y_train, feature_name=['airconditioningtypeid', 'bathroomcnt', 'bedroomcnt',\n        'buildingqualitytypeid', 'calculatedbathnbr',\n       'calculatedfinishedsquarefeet', 'finishedsquarefeet', 'fips',\n       'fullbathcnt', 'garagecarcnt', 'garagetotalsqft',\n       'heatingorsystemtypeid', 'latitude', 'longitude', 'lotsizesquarefeet',\n       'poolcnt', 'pooltypeid', 'propertylandusetypeid',\n       'rawcensustractandblock', 'regionidcity', 'regionidcounty',\n       'regionidneighborhood', 'regionidzip', 'roomcnt', 'threequarterbathnbr',\n       'unitcnt', 'yearbuilt', 'numberofstories', 'structuretaxvaluedollarcnt',\n       'taxvaluedollarcnt', 'assessmentyear', 'landtaxvaluedollarcnt',\n       'taxamount', 'censustractandblock', 'prop_age', 'has_basement',\n       'has_pool', 'has_patio_yard', 'has_starage_yard','has_garage'])  # lightgbm data model","f3e913f0":"# lgb hyper parameters\nparams = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.01  # shrinkage_rate 0.0021 grid search = 0.01\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'mae'  # l1\nparams['sub_feature'] = 0.5  # feature_fraction\nparams['bagging_fraction'] = 0.85  # sub_row\nparams['num_leaves'] = 512  # num_leaf\nparams['min_data'] = 500  # min_data_in_leaf\nparams['min_hessian'] = 0.05  # min_sum_hessian_in_leaf\nparams['verbose'] = 0\nparams['feature_fraction_seed'] = 2\nparams['bagging_seed'] = 3\n#params['n_estimators'] = 10  # grid search\nparams['colsample_bytree'] = 0.85\nparams['num_leaves'] = 22\nparams['subsample'] = 0.7\n\nnp.random.seed(0)\nrandom.seed(0)","e44c13e3":"# lgb train\nclf = lgb.train(params, d_train, 430)","4705467f":"# lgb predict\np_test = clf.predict(x_test)\npd.DataFrame(p_test).head()","0065f108":"# lgb feature importance\nlgb.plot_importance(clf, figsize=(20,20))\nplt.show()","ca1d8690":"# lgb tree plot\nimport os\nos.environ[\"PATH\"] += os.pathsep + '\/opt\/anaconda3\/lib\/python3.7\/site-packages\/sphinx\/templates\/graphviz'\nlgb.plot_tree(clf, figsize=(100,40))\nplt.show()","51818b14":"# import the library\nimport xgboost as xgb","faeccac4":"#XGBoost instead accepts dataframe as input\nx_train_xgb = train16_merge.drop(columns =[ 'logerror', 'transactiondate', 'fireplacecnt']) \ny_train_xgb = train16_merge['logerror']\nx_test_xgb = train16_merge.drop([ 'logerror', 'transactiondate', 'fireplacecnt'], axis=1)\ntrain_columns_xgb = train16_merge.drop(columns = [ 'logerror', 'transactiondate', 'fireplacecnt']).columns ","a1319690":"# xgb hyperparameters\n\ny_mean = np.mean(y_train_xgb)\nxgb_params1 = {\n    'eta' : 0.04,  # 0.037 grid search = .04\n    'max_depth' : 6,  #5\n    'subsample' : 0.80,\n    'objective' : 'reg:linear',\n    'eval_metric' : 'mae',\n    'lambda' : 0.8,\n    'alpha' : 0.4,\n    'base_score' : y_mean,\n    'silent' : 1,\n    'min_child_weight': 5  # grid search\n}","a0d2ba57":"dtrain1 = xgb.DMatrix(x_train_xgb, y_train_xgb, feature_names=train_columns_xgb)\ndtest1 = xgb.DMatrix(x_test_xgb)","f5a0ae8a":"# training\nmodel1 = xgb.train(dict(xgb_params1, silent=1), dtrain1, num_boost_round=150)","0547736f":"# xgb feature importance\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model1, max_num_features=50, height=0.8, ax=ax)\nplt.show()","8caeb816":"fig, ax = plt.subplots(figsize=(100, 60))\nxgb.plot_tree(model1, num_trees=4, ax=ax)\nplt.show()","f478da5e":"# xgb2 hyperparameters\nxgb_params2 = {\n    'eta': 0.05,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:squarederror',\n    'silent': 1,\n    'seed' : 0\n}\ndtrain2 = xgb.DMatrix(x_train_xgb, y_train_xgb, feature_names=train_columns)\ndtest2 = xgb.DMatrix(x_test_xgb)","702c726b":"# training\nmodel2 = xgb.train(dict(xgb_params2, silent=0), dtrain2, num_boost_round=50)","0c39f280":"# predict\nxgb_pred2 = model2.predict(dtest2)\nxgb_pred2","d5f1e78b":"# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model2, max_num_features=50, height=0.8, ax=ax)\nplt.show()","b786483c":"fig, ax = plt.subplots(figsize=(100, 60))\nxgb.plot_tree(model2, num_trees=4, ax=ax)\nplt.show()","27e7ce33":"## 6. LightGBM model","fd5fad5b":"#### *Please upvote if any of this useful*","4d55eda5":"## 3. Feature & Observation Analysis","aa93d05c":"## 7. **XGBoost model**","2bc53f5c":"## 2. Load Data","5aee425b":"## 1. Import Libraries","2b705c8e":"## 4. Feature Engineering","d653edd3":"## 8. XGBoost with a variation","1d20f3c7":"## 5. Correlation"}}