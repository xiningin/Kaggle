{"cell_type":{"02299c91":"code","f76d9164":"code","0efd09fe":"code","7bf71c0b":"code","aed3daf2":"code","fc88f72a":"code","4c7dacae":"code","b2e3ecc1":"code","e8842244":"code","c53fe8a7":"code","32bc90bc":"code","2002e757":"code","ff7ca367":"code","081408e2":"code","38bf21f0":"code","8ab19d84":"code","21faff54":"markdown","08b79685":"markdown","19729bd5":"markdown","401627a1":"markdown","e9d88850":"markdown","075d6b89":"markdown","194fb23b":"markdown","4d514410":"markdown","d7117e24":"markdown","bfefa67b":"markdown","38586b13":"markdown","16b8523f":"markdown","c2228ce9":"markdown"},"source":{"02299c91":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f76d9164":"# Importing pandas and numpy\nimport pandas as pd\nimport numpy as np\n\n# Reading the csv file into a pandas DataFrame\ndata = pd.read_csv('\/kaggle\/input\/student-data\/student_data.csv')\n\n# Printing out the first 10 rows of our data\ndata.head(10)","0efd09fe":"# Importing matplotlib\nimport matplotlib.pyplot as plt\n\n# Function to help us plot\ndef plot_points(data):\n    X = np.array(data[[\"gre\",\"gpa\"]])\n    y = np.array(data[\"admit\"])\n    admitted = X[np.argwhere(y==1)]\n    rejected = X[np.argwhere(y==0)]\n    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'red', edgecolor = 'k')\n    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')\n    plt.xlabel('Test (GRE)')\n    plt.ylabel('Grades (GPA)')\n    \n# Plotting the points\nplot_points(data)\nplt.show()","7bf71c0b":"# Separating the ranks\ndata_rank1 = data[data[\"rank\"]==1]\ndata_rank2 = data[data[\"rank\"]==2]\ndata_rank3 = data[data[\"rank\"]==3]\ndata_rank4 = data[data[\"rank\"]==4]\n\n# Plotting the graphs\nplot_points(data_rank1)\nplt.title(\"Rank 1\")\nplt.show()\nplot_points(data_rank2)\nplt.title(\"Rank 2\")\nplt.show()\nplot_points(data_rank3)\nplt.title(\"Rank 3\")\nplt.show()\nplot_points(data_rank4)\nplt.title(\"Rank 4\")\nplt.show()","aed3daf2":"pd.concat([data, pd.get_dummies(data['rank'], prefix='rank')], axis=1)","fc88f72a":"data","4c7dacae":"pd.get_dummies(data['rank'], prefix='rank')","b2e3ecc1":"# Make dummy variables for rank\none_hot_data = pd.concat([data, pd.get_dummies(data['rank'], prefix='rank')], axis=1)\n\n# Drop the previous rank column\none_hot_data = one_hot_data.drop('rank', axis=1)\n\n# Print the first 10 rows of our data\none_hot_data.head(10)","e8842244":"one_hot_data.describe()","c53fe8a7":"# Copying our data\nprocessed_data = one_hot_data[:]\n\n# Scaling the columns\nprocessed_data['gre'] = processed_data['gre']\/800\nprocessed_data['gpa'] = processed_data['gpa']\/4.0\nprocessed_data.head(10)","32bc90bc":"train_data = processed_data.sample(frac=0.9, random_state=25)\ntest_data = processed_data.drop(train_data.index)","2002e757":"import numpy as np\nimport keras\nfrom keras.utils import np_utils\n# Separate data and one-hot encode the output\n# Note: We're also turning the data into numpy arrays, in order to train the model in Keras\n\nxtrain = np.array(train_data.drop('admit', axis=1))\nytrain = np_utils.to_categorical(train_data['admit'], 2)\n\nxtest = np.array(test_data.drop('admit', axis=1))\nytest = np_utils.to_categorical(test_data['admit'], 2)\n\nprint(\"xtrain:\", xtrain[:10], '\\n')\nprint(\"ytrain:\", ytrain[:10])","ff7ca367":"xtrain.shape","081408e2":"# Imports\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\n# from keras.optimizers import SGD\nfrom keras.utils import np_utils\n\n# Building the model\nmodel = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(6,)))\nmodel.add(Dropout(0.2)) #26\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2, activation='sigmoid'))\n\n# Compiling the model\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","38bf21f0":"# Training the model\nmodel.fit(xtrain, ytrain, epochs=500, batch_size=100, verbose=True)","8ab19d84":"# Evaluating the model on the training and testing set\ntrain_score = model.evaluate(xtrain, ytrain)\nprint(\"\\n Training Accuracy:\", train_score[1])\ntest_score = model.evaluate(xtest, ytest)\nprint(\"\\n Testing Accuracy:\", test_score[1])","21faff54":"## Task 5\n## Defining the model architecture\nHere's where we use Keras to build our neural network.\n\n1. Make a sequential model with 128 nodes on first layer with an activation function of `relu`. \n2. Add a `Dropout` layer with 0.2\n3. Add a `Dense` layer with 64 nodes along with `relu` as activation function.\n4. Add a `Dropout` layer with 0.1\n5. Add a `Dense` layer with 2 nodes with a activation function of softmax.\n6. Compile the model with `binary_crossentropy`, optimzer as `adam` and metrics as `accuracy`.","08b79685":"This looks more promising, as it seems that the lower the rank, the higher the acceptance rate. Let's use the rank as one of our inputs. In order to do this, we should one-hot encode it.\n\n## Task-2\n## One-hot encoding the rank\nFor this, please use the `get_dummies` function in pandas.","19729bd5":"# Predicting Student Admissions with Neural Networks in Keras\nIn this notebook, we predict student admissions to graduate school at UCLA based on three pieces of data:\n- GRE Scores (Test)\n- GPA Scores (Grades)\n- Class rank (1-4)\n\nThe dataset originally came from here: http:\/\/www.ats.ucla.edu\/\n\n## Loading the data\nTo load the data and format it nicely, we will use two very useful packages called Pandas and Numpy. You can read on the documentation here:\n- https:\/\/pandas.pydata.org\/pandas-docs\/stable\/\n- https:\/\/docs.scipy.org\/","401627a1":"Roughly, it looks like the students with high scores in the grades and test passed, while the ones with low scores didn't, but the data is not as nicely separable as we hoped it would. Maybe it would help to take the rank into account? Let's make 4 plots, each one for each rank.","e9d88850":"## Plotting the data\n\nFirst let's make a plot of our data to see how it looks. In order to have a 2D plot, let's ingore the rank.","075d6b89":"# Task 1\n## Loading the data\nRead the data file named: \"student_data.csv\"\n","194fb23b":"## Task 3\n## Scaling the data\nThe next step is to scale the data. We notice that the range for grades is 1.0-4.0, whereas the range for test scores is roughly 200-800, which is much larger. This means our data is skewed, and that makes it hard for a neural network to handle. Let's fit our two features into a range of 0-1, by dividing the grades by 4.0, and the test score by 800.\n\nOr you can also use `Min Max Scaler`","4d514410":"## Task 4\n## Splitting the data into Training and Testing","d7117e24":"## Training the model\nTrain the model by using `fit` function giving inputs as `xtrain`, `ytrain`, `epochs = 200`, `batch_size = 100` and `verbose = True`","bfefa67b":"## Scoring the model\n\nEvaluate the model by using `model.evaluate` function on\n1. Training Set by giving `xtrain` and `ytrain`\n2. Testing Set by giving `xtest` and `ytest`","38586b13":"## Challenge: Play with the parameters!\nYou can see that we made several decisions in our training. For instance, the number of layers, the sizes of the layers, the number of epochs, etc.\nIt's your turn to play with parameters! Can you improve the accuracy? The following are other suggestions for these parameters. We'll learn the definitions later in the class:\n- Activation function: relu and sigmoid\n- Loss function: categorical_crossentropy, mean_squared_error\n- Optimizer: rmsprop, adam, adadelta","16b8523f":"## Splitting the data into features and targets (labels)\nNow, as a final step before the training, we'll split the data into features (X) and targets (y).\n\nAlso, in Keras, we need to one-hot encode the output. We'll do this with the `to_categorical function`.","c2228ce9":"In order to test our algorithm, we'll split the data into a Training and a Testing set. The size of the testing set will be 10% of the total data."}}