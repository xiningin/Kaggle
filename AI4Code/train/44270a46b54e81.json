{"cell_type":{"b387e495":"code","59519f3a":"code","6e3a0405":"code","e8f38349":"code","dc05cad1":"code","e878a540":"code","2132b3dc":"code","062f00c0":"code","22feb2e2":"code","bd5daafe":"code","091b2285":"code","4ddf2d68":"code","95c2272c":"code","52a2fa92":"code","ef362d8e":"code","98228505":"code","61baa6ed":"code","6344fb08":"code","316c1a88":"code","37bfe269":"code","890526ce":"code","947436ff":"markdown","7db1d570":"markdown","7cac77b9":"markdown","ef5728d3":"markdown","cc18bb81":"markdown","20a227b6":"markdown","4bcde477":"markdown","ce60a1a5":"markdown","fc37917e":"markdown","c052110c":"markdown","04e65a27":"markdown","4909e560":"markdown","f5be28e5":"markdown"},"source":{"b387e495":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport json\nimport glob\nimport re\nfrom tqdm import tqdm","59519f3a":"!pip install -U spacy[transformers]","6e3a0405":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n\n# does not lowercase the text\ndef clean_text2(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt))","e8f38349":"df = pd.read_csv(\"..\/input\/coleridgeinitiative-show-us-the-data\/train.csv\")","dc05cad1":"df.head()","e878a540":"df.shape","2132b3dc":"# number of unique labels\nlen(df.cleaned_label.unique()) ","062f00c0":"# create a subset for quick demo\nsample = df.sample(500)\nsample.shape","22feb2e2":"# get positive and negative examples for entities\nPOSITIVE_DATA = []\nNEGATIVE_DATA = []\nfor idx,row in tqdm(sample.iterrows()):\n    pub = \"..\/input\/coleridgeinitiative-show-us-the-data\/train\/\" + row.Id + \".json\"\n    f = open(pub)  \n    data = json.load(f)\n    paper_text = str([sec['text'] for sec in data]).strip(\"[\").strip(\"]\")\n    sentences = paper_text.split(\".\")\n    for sentence in sentences:\n        sentence2 = clean_text(sentence) # use given clean_text to find cleaned_label\n        a = re.search(row.cleaned_label,sentence2)\n        if  a != None: # if label is found, make it a positive example\n            POSITIVE_DATA.append((clean_text2(sentence),{\"entities\":[(a.span()[0],a.span()[1],\"DATASET\")]}))\n        else: # if label is not found, make it a negative example\n            if len(clean_text2(sentence))>20: # greater than 20 chars\n                NEGATIVE_DATA.append((clean_text2(sentence),{\"entities\":[(0,0,\"DATASET\")]}))","bd5daafe":"POSITIVE_DATA[0:10]","091b2285":"len(POSITIVE_DATA)","4ddf2d68":"len(NEGATIVE_DATA)","95c2272c":"import random\nNEG_SAMPLE = random.choices(NEGATIVE_DATA, k=2000) # downsampling negative class","52a2fa92":"TRAIN_DATA = np.array(POSITIVE_DATA + NEG_SAMPLE) # our train data is positive + negative examples\nnp.random.shuffle(TRAIN_DATA) # shuffle the train data\nlen(TRAIN_DATA) # total examples in train data","ef362d8e":"import spacy\nfrom spacy.tokens import DocBin\n\nnlp = spacy.blank(\"en\") # load a new spacy model\ndb = DocBin() # create a DocBin object\n\nfor text, annot in tqdm(TRAIN_DATA): # data in previous format\n    doc = nlp.make_doc(text) # create doc object from text\n    ents = []\n    for start, end, label in annot[\"entities\"]: # add character indexes\n        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n        if span is None:\n            pass\n        else:\n            ents.append(span)\n    doc.ents = ents # label the text with the ents\n    db.add(doc)\n\ndb.to_disk(\".\/train.spacy\") # save the docbin object","98228505":"# step1: Get baseconfig file from https:\/\/spacy.io\/usage\/training#quickstart\n!cp \"..\/input\/spacybaseconfigcfg\/base_config.cfg\" .\/","61baa6ed":"# step2: initialize the base config file. \n# Config file contains the training settings. \n# Init with spacy init initializes it with most common settings\n!python -m spacy init fill-config base_config.cfg config.cfg","6344fb08":"# step3: train using spacy train command\n!python -m spacy train config.cfg --output .\/output --paths.train .\/train.spacy --paths.dev .\/train.spacy --gpu-id 0","316c1a88":"from thinc.api import set_gpu_allocator, require_gpu\nset_gpu_allocator(\"pytorch\")\nrequire_gpu(0)\n# Use spacy.load to load your custom model\ncustom_ner_model = spacy.load(\".\/output\/model-best\") # output model is stored as \"model-best\" and \"model-last\"","37bfe269":"test_pubs = glob.glob(\"..\/input\/coleridgeinitiative-show-us-the-data\/test\/*.json\")","890526ce":"from spacy import displacy\n\nfor index, pub in enumerate(test_pubs):\n    f = open(pub)\n    data = json.load(f)\n    paper_text = str([sec['text'] for sec in data]).strip(\"[\").strip(\"]\")\n    sentences = paper_text.split(\".\")\n    for sentence in sentences:\n        sentence = clean_text2(sentence)\n        doc = custom_ner_model(sentence)\n        if len(doc.ents) > 0:\n            displacy.render(doc, style=\"ent\", jupyter=True)\n        ","947436ff":"## We have an IMBALANCED CLASS problem.\n#### For brevity, let's downsample negative class to 2000 examples","7db1d570":"# References\n1. https:\/\/spacy.io\/usage\/training","7cac77b9":"## Spacy 3.0 uses DocBin format - convert train set to this format\n####  DocBin is highly efficient serializable format used by spaCy3.0 \nUse below converter to change above train_set into new format","ef5728d3":"## Predefined function for prepropossing\n\nFor preprocessing, we stick to the given function which replaces anything apart from letters and digits with a ' '. However, for training our spaCy model, we do not lowercase the text","cc18bb81":"# Train the spaCy transformer model\nhttps:\/\/spacy.io\/usage\/training#quickstart","20a227b6":"# Read train csv and create a sample (for faster demo)","4bcde477":"## Install Spacy 3.0.+ Transformers","ce60a1a5":"#### This is my first notebook on Kaggle. Your feedback and suggestions would be appreciated! - Shivam","fc37917e":"## Create the training dataset by marking entries","c052110c":"### Explaining Training Pipeline Variables\n\n- E is epochs\n- Loss Transformer\n- Loss NER\n- ENTS_F is f score\n- ENTS_P is precision\n- ENTS_R is recall\n- Score is to score the model (in order to pick best model later)","04e65a27":"<h1 style=\"background-color:DodgerBlue; color:white\" >Custom NER using Spacy 3.0+<\/h1>\n\nRecently, in my work, I did custom NER using production-level NLP library called spaCy.  \n\nUtilizing that experience, this notebook aims to train a custom NER transformer-based model to detect datasets as entities. For achieving this, we require spaCy 3.0+.\n\nThe whole process is quite straightforward:\n1. Make your training dataset by marking entities in it. spaCy 3.0 requires DocBin format. \n    - For our problem, the training labels help us mark the entities. (the **positive examples**)\n    - Rest lines could be our **negative examples** with start and end indexes of entity has 0,0\n    - **Caution:** In this competition, train data is not exhaustively labeled. That means, we have some positive examples inside the examples that we mark as negative. You would ideally want to increase the class-prior weight of the positive examples we already know.\n2. Initialize spacy with a config file (**spacy init** command)\n3. Train spacy model using the settings mentioned in config file (**spacy train** command)\n4. Load the model and use it like any other spacy pipeline (**spacy.load()** command)\n","4909e560":"**Note:**  This notebook uses internet, therefore, you cannot submit it as submission. However, you can take the trained model and use it make your submissions.","f5be28e5":"# Load the custom NER model and predict."}}