{"cell_type":{"a7bbb1ba":"code","e53a6ec5":"code","c3f9cf5c":"code","97310f8f":"code","38c5a147":"code","f4b3ac63":"code","417ac2b4":"code","de78f2b7":"code","5f9139b6":"code","c169b333":"code","5936e026":"code","02ca0eea":"code","2882e24a":"code","ea84df2c":"code","15a9e2eb":"code","3b1375b2":"code","bb379d6f":"code","3fc5d1b0":"code","835ed685":"code","c7c8db51":"code","01d0ea5f":"code","caf56899":"code","3d88683f":"code","2653e5bf":"markdown","d4334df9":"markdown","b1e0c8b1":"markdown","f01564c0":"markdown","2faca519":"markdown","f51dde59":"markdown","88c64bdb":"markdown","3eb9ada8":"markdown","f8e29e26":"markdown","74923eab":"markdown","47ea81a9":"markdown","cbc4d450":"markdown","32c402be":"markdown","8ec1167e":"markdown","c6b4b1d8":"markdown","8fbf26cf":"markdown","24f2382e":"markdown","13bd5ccf":"markdown","46553226":"markdown","579c1e25":"markdown"},"source":{"a7bbb1ba":"import numpy as np\nimport pandas as pd\nsample = pd.read_csv(\"\/kaggle\/input\/traffic\/DataSet\/train13519.csv\")\nsample","e53a6ec5":"import glob\n\ntrain = {}\n\nfor file in glob.iglob(\"\/kaggle\/input\/traffic\/DataSet\/*.csv\"):\n    id = file.split(\"\/\")[-1].replace(\"train\", \"\").replace(\".csv\", \"\")\n    train[id] = pd.read_csv(file)","c3f9cf5c":"n_out_steps = 12\nn_in_steps = 3 * n_out_steps\nn_locations = len(train)","97310f8f":"#jump = 488\nbatches = []\nfor id in train:\n    n = train[id].index.size\n    jump = round(n\/2)\n    start = 0\n    batch_i = -1\n    while start < round(n-.5*jump):\n        batch_i += 1\n        batch = train[id].iloc[list(range(start, start+jump))][\"hourly_traffic_count\"]\n        start += jump\n        if len(batches) < batch_i + 1:\n            batches.append([batch])\n        else:\n            batches[batch_i].append(batch)\nfor batch_i, batch in enumerate(batches):\n    batches[batch_i] = list(zip(*batch))","38c5a147":"len(batches), len(batches[0]), len(batches[0][0])","f4b3ac63":"X = []\nY = []\nfor batch_i, batch in enumerate(batches):\n    x = []\n    y = []\n    for t in range(n_in_steps, len(batch)-n_out_steps):\n        _x = batch[t-n_in_steps:t]\n        x.append(np.array(_x))\n        _y = np.sum(batch[t:t+n_out_steps], axis=0)\n        y.append(np.array(_y))\n    X.extend(x)\n    Y.extend(y)","417ac2b4":"len(X), len(X[1]), len(X[0][0]), len(Y), len(Y[0])","de78f2b7":"train_X = [x for i, x in enumerate(X) if i % 2 == 0]\ntrain_Y = [x for i, x in enumerate(Y) if i % 2 == 0]\ntest_X = [x for i, x in enumerate(X) if i % 2 == 1]\ntest_Y = [x for i, x in enumerate(Y) if i % 2 == 1]\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nnormalizer_x = MinMaxScaler()\nnormalizer_x.fit(np.array(train_X).flatten().reshape(-1,1))\nnormalizer_y = MinMaxScaler()\nnormalizer_y.fit(np.array(train_Y).flatten().reshape(-1,1))\ntrain_X = normalizer_x.transform(np.array(train_X).flatten().reshape(-1,1)).reshape(len(train_X), len(train_X[0]), len(train_X[0][0]))\ntrain_Y = normalizer_y.transform(np.array(train_Y).flatten().reshape(-1,1)).reshape(len(train_Y), len(train_Y[0]))\ntest_X = normalizer_x.transform(np.array(test_X).flatten().reshape(-1,1)).reshape(len(test_X), len(test_X[0]), len(test_X[0][0]))\ntest_Y = normalizer_y.transform(np.array(test_Y).flatten().reshape(-1,1)).reshape(len(test_Y), len(test_Y[0]))","5f9139b6":"len(train_X), len(train_X[1]), len(train_X[1][1]), len(train_Y), len(train_Y[1])","c169b333":"from tensorflow import keras\nimport math\n\ndef create_lstm_model():\n    inputs = keras.layers.Input(shape=(n_in_steps, n_locations))\n    l = inputs\n    l = keras.layers.LSTM(n_locations)(l)\n    l = keras.layers.Dense(n_locations)(l)\n    outputs = l\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(\n        optimizer=keras.optimizers.Nadam(learning_rate=0.001),\n        loss=\"mean_absolute_error\",\n        metrics=[\"mean_absolute_percentage_error\", \"mean_squared_error\"]\n    )\n    return model","5936e026":"lstm = create_lstm_model()\nlstm.summary()","02ca0eea":"import time\ntime1 = time.time()\nlstm = create_lstm_model()\nlstm.fit(np.array(train_X), np.array(train_Y), epochs=10, verbose=0)\ntime2 = time.time()\nprint(\"Execution time:\", time2-time1)","2882e24a":"lstm.evaluate(np.array(test_X), np.array(test_Y))","ea84df2c":"lstm_pred = lstm.predict(np.array(test_X))\nlstm_pred = normalizer_y.inverse_transform(lstm_pred)\nnp.round(lstm_pred[50] * 1 + 0).astype(np.int)","15a9e2eb":"import copy\ntrue = np.array(copy.deepcopy(test_Y))\ntrue = normalizer_y.inverse_transform(true)\nnp.round(true[50]).astype(np.int)","3b1375b2":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(16, 4), dpi=100)\n\nlocation_id = 0\nplot_pred = np.round(np.array([round(x[location_id]) for x in lstm_pred[:700]]) * 1 - 0).astype(np.int)\nplot_true = [round(x[location_id]) for x in true[:700]]\nplt.subplot(131)\nplt.title(\"LSTM\")\nplt.plot(plot_pred, label='pred')\nplt.plot(plot_true, label='true')\nplt.legend()\nplt.grid(True)\n\nlocation_id = 10\nplot_pred = np.round(np.array([round(x[location_id]) for x in lstm_pred[:700]]) * 1 - 0).astype(np.int)\nplot_true = [round(x[location_id]) for x in true[:700]]\nplt.subplot(132)\nplt.title(\"LSTM\")\nplt.plot(plot_pred, label='pred')\nplt.plot(plot_true, label='true')\nplt.legend()\nplt.grid(True)\n\nlocation_id = 20\nplot_pred = np.round(np.array([round(x[location_id]) for x in lstm_pred[:700]]) * 1 - 0).astype(np.int)\nplot_true = [round(x[location_id]) for x in true[:700]]\nplt.subplot(133)\nplt.title(\"LSTM\")\nplt.plot(plot_pred, label='pred')\nplt.plot(plot_true, label='true')\nplt.legend()\nplt.grid(True)\n\nplt.show()","bb379d6f":"from tensorflow import keras\nimport tensorflow.keras.backend as kb\nimport math\nimport tensorflow as tf\n\ndef sparse_regularizer(x):\n    gamma = tf.convert_to_tensor(.1)\n    rho = tf.convert_to_tensor(.001)\n    rho2 = tf.reduce_mean(tf.math.abs(x))\n    y = tf.math.add(tf.math.multiply(rho, tf.math.log(tf.math.divide(rho, rho2))), tf.math.multiply(tf.math.subtract(tf.constant(1.0), rho), tf.math.log(tf.math.divide(tf.math.subtract(tf.constant(1.0), rho), tf.math.subtract(tf.constant(1.0), rho2)))))\n    return y\n\nclass SparseRegLayer(keras.layers.Layer):\n    def __init__(self, layer):\n        keras.layers.Layer.__init__(self)\n        self.layer = layer\n        \n    def call(self, inputs):\n        x = self.layer(inputs)\n        self.add_loss(sparse_regularizer(x))\n        return x\n\ndef create_autoencoder(input_size, hidden_size):\n    inputs = keras.layers.Input(shape=(input_size,))\n    l = inputs\n    l = SparseRegLayer(keras.layers.Dense(hidden_size))(l)\n    l = keras.layers.Dense(input_size)(l)\n    outputs = l\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(\n        optimizer=keras.optimizers.Nadam(learning_rate=0.001),\n        loss=\"mean_absolute_error\"\n    )\n    return model\n\ndef create_sae_model(hidden_sizes):\n    inputs = keras.layers.Input(shape=(n_in_steps, n_locations))\n    l = inputs\n    ll = keras.layers.Flatten()\n    l = ll(l)\n    hidden = []\n    prev_size = n_in_steps * n_locations\n    X = np.array(train_X)\n    for hidden_size in hidden_sizes:\n        model1 = create_autoencoder(prev_size, hidden_size)\n        X = ll(X)\n        model1.fit(X, X, epochs=10, verbose=0)\n        ll = model1.layers[1]\n        hidden.append(ll)\n        l = keras.layers.Dense(hidden_size, activation=\"relu\")(l)\n        prev_size = hidden_size\n    l = keras.layers.Dense(n_locations)(l)\n    outputs = l\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    for layer_i, layer in enumerate(model.layers[2:-1]):\n        layer.set_weights(hidden[layer_i].get_weights())\n    model.compile(\n        optimizer=keras.optimizers.Nadam(learning_rate=0.001),\n        loss=\"mean_absolute_error\",\n        metrics=[\"mean_absolute_percentage_error\", \"mean_squared_error\"]\n    )\n    return model","3fc5d1b0":"model = create_sae_model([100,100])\nmodel.summary()","835ed685":"import time\ntime1 = time.time()\nmodel = create_sae_model([100, 100])\nmodel.fit(np.array(train_X), np.array(train_Y), epochs=10, verbose=0)\ntime2 = time.time()\nprint(\"Execution time:\", time2-time1)","c7c8db51":"np.array(model.evaluate(np.array(test_X), np.array(test_Y)))","01d0ea5f":"pred = model.predict(np.array(test_X))\npred = normalizer_y.inverse_transform(pred)\nnp.round(pred[50]).astype(np.int)","caf56899":"import copy\ntrue = np.array(copy.deepcopy(test_Y))\ntrue = normalizer_y.inverse_transform(true)\nnp.round(true[50]).astype(np.int)","3d88683f":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(16, 8), dpi=100)\n\nlocation_id = 0\n\nplot_pred = np.round(np.array([round(x[location_id]) for x in pred[:700]]) * 1 - 0).astype(np.int)\nplot_true = [round(x[location_id]) for x in true[:700]]\nplt.subplot(231)\nplt.title(\"Stacked Autoencoder\")\nplt.plot(plot_pred, label='pred')\nplt.plot(plot_true, label='true')\nplt.legend()\nplt.grid(True)\n\nplot_pred = np.round(np.array([round(x[location_id]) for x in lstm_pred[:700]]) * 1 - 0).astype(np.int)\nplt.subplot(234)\nplt.title(\"LSTM\")\nplt.plot(plot_pred, label='pred')\nplt.plot(plot_true, label='true')\nplt.legend()\nplt.grid(True)\n\nlocation_id = 3\n\nplot_pred = np.round(np.array([round(x[location_id]) for x in pred[:700]]) * 1 - 0).astype(np.int)\nplot_true = [round(x[location_id]) for x in true[:700]]\nplt.subplot(232)\nplt.title(\"Stacked Autoencoder\")\nplt.plot(plot_pred, label='pred')\nplt.plot(plot_true, label='true')\nplt.legend()\nplt.grid(True)\n\nplot_pred = np.round(np.array([round(x[location_id]) for x in lstm_pred[:700]]) * 1 - 0).astype(np.int)\nplt.subplot(235)\nplt.title(\"LSTM\")\nplt.plot(plot_pred, label='pred')\nplt.plot(plot_true, label='true')\nplt.legend()\nplt.grid(True)\n\nlocation_id = 4\n\nplot_pred = np.round(np.array([round(x[location_id]) for x in pred[:700]]) * 1 - 0).astype(np.int)\nplot_true = [round(x[location_id]) for x in true[:700]]\nplt.subplot(233)\nplt.title(\"Stacked Autoencoder\")\nplt.plot(plot_pred, label='pred')\nplt.plot(plot_true, label='true')\nplt.legend()\nplt.grid(True)\n\nplot_pred = np.round(np.array([round(x[location_id]) for x in lstm_pred[:700]]) * 1 - 0).astype(np.int)\nplt.subplot(236)\nplt.title(\"LSTM\")\nplt.plot(plot_pred, label='pred')\nplt.plot(plot_true, label='true')\nplt.legend()\nplt.grid(True)\n\nplt.show()\n","2653e5bf":"Calculate prediction on the test set","d4334df9":"Evaluation: MAE, MRE, RMSE.","b1e0c8b1":"# Stacked Autoencoder vs LSTM on traffic flow prediction","f01564c0":"Training","2faca519":"# LSTM model.","f51dde59":"Load the whole data set.","88c64bdb":"# Stacked Autoencoder","3eb9ada8":"Train","f8e29e26":"Plot the results","74923eab":"We will test two kinds of models to predict traffic flow in an urban area. The dataset consists of observations of number of vehicles running the road. The observations are made in multiple locations and colected in 5 minutes intervals.\n\nWe are going to test the LSTM model, as well as the Stacked Autoencoder model.\n\nAs the training set, we will use time series of the data form the last 3 hours, from all the locations, to predict the overall traffic in each location in the next hour.","47ea81a9":"Evaluation: MAE, MRE, RMSE.","cbc4d450":"Scale test set true values","32c402be":"Calculate prediction on the test set","8ec1167e":"Load a sample of the data set.","c6b4b1d8":"Transform batches into actual train set.","8fbf26cf":"Plot","24f2382e":"Scale the test set true values.","13bd5ccf":"Calculate input \/ output sizes.","46553226":"Split the batches into train and test sets.","579c1e25":"Transform the data set into batches of time series. Each time step contains the data from all the locations."}}