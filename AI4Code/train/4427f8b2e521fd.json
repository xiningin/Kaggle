{"cell_type":{"a048d8f3":"code","7083f738":"code","24c9f44a":"code","46c4066e":"code","576fb44f":"code","4ca66510":"code","c93fee39":"code","84646e1d":"code","03997c3f":"code","dbe6188e":"code","87ee6ca0":"code","2f40678d":"code","8b01adeb":"code","125aacd3":"code","248a7582":"code","33e172a3":"code","4ca3c9b0":"code","f82f78da":"code","1a9dddf5":"code","28e9e221":"code","d4bf4f8d":"code","27a31565":"code","15a92a69":"code","a7de6cb9":"code","d61df993":"code","f5db4120":"code","6406cd94":"code","0d5c4c7b":"markdown","acab7807":"markdown","c97d3b9e":"markdown","0d7136b0":"markdown","ed413c5e":"markdown","17036840":"markdown","a6cd5cdb":"markdown","f3ecfe60":"markdown","fc649c4e":"markdown","4b392668":"markdown","922b7827":"markdown","63cc1f81":"markdown","28346688":"markdown","be7f4c02":"markdown","caedce1d":"markdown","65acea81":"markdown","4fd39571":"markdown","b15451dd":"markdown","2a255d45":"markdown","22a72522":"markdown","b2b60a4f":"markdown","e22d6272":"markdown","1f33cdc7":"markdown","29f48f49":"markdown","4385fa0d":"markdown"},"source":{"a048d8f3":"from google.cloud import bigquery\nfrom bq_helper import BigQueryHelper\nimport pandas as pd\n\n# This establishes an authenticated session and prepares a reference to the dataset that lives in BigQuery.\nbq_assistant = BigQueryHelper(\"bigquery-public-data\", \"ethereum_blockchain\")\n\n# This initiates a google big query client without a reference to a specific dataset\nclient = bigquery.Client()","7083f738":"bq_assistant.list_tables()","24c9f44a":"# Quering GBQ with BigQueryHelper\n    \n    # BigQueryHelper Functions\n# Queries dataset and returns a panda dataframe\n##df = bq_assistant.query_to_pandas(query)\n\n# Queries dataset and returns a panda dataframe + allows to set a max scan limit\n##df = bq_assistant.query_to_pandas_safe(query, max_gb_scanned=40)\n\n# Lists all tables in the dataset\n##bq_assistant.list_tables()\n\n# Shows the head of a specific table\n##bq_assistant.head(\"table_name\", num_rows=3)\n\n# Shows details about colums \n##bq_assistant.table_schema(\"table_name\")\n\n# check estimated size of a query\n##bq_assistant.estimate_query_size(query)\n\n    # other usefull functions\n# Print size of dataframe\n##print('Size of dataframe: {} Bytes'.format(int(df.memory_usage(index=True, deep=True).sum())))\n","46c4066e":"# Quering GBQ at Kaggle with GBQ client (without BigQueryHelper)\n##query_job = client.query(query)\n\n##iterator = query_job.result(timeout=30)\n##rows = list(iterator)\n\n# Transform the rows into a nice pandas dataframe\n##df = pd.DataFrame(data=[list(x.values()) for x in rows], columns=list(rows[0].keys()))\n\n# Look at the first 10\n##df.head(10)","576fb44f":"# Query examples\n\nmin_block_number = 5100000\nmax_block_number = 5200000   #usually 6400000\n\n# select distinct transaction senders within a given range of blocks \nquery_1 = \"\"\"\nSELECT DISTINCT\n    from_address AS sender, block_number AS block_number\nFROM\n    `bigquery-public-data.ethereum_blockchain.transactions`\nWHERE\n    block_number > %d\n    AND\n    block_number < %d\n\"\"\"\n\n# select distinct transaction senders within a given range of blocks\nquery_2 = \"\"\"\nSELECT DISTINCT\n    to_address AS receipient, block_number AS block_number\nFROM\n    `bigquery-public-data.ethereum_blockchain.transactions`\nWHERE\n    block_number > %d\n    AND\n    block_number < %d\n\"\"\"\n\n# select transaction senders that are contracts \nquery_3 = \"\"\"\nSELECT\n    DISTINCT address AS sc_address, is_erc20, is_erc721, block_number AS block_number\nFROM\n    `bigquery-public-data.ethereum_blockchain.contracts`\nWHERE\n    block_number > %d\n    AND\n    block_number < %d\n\"\"\"","4ca66510":"# query_1: unique senders\nunique_senders = bq_assistant.query_to_pandas_safe(query_1 % (min_block_number, max_block_number), max_gb_scanned=52)\nprint(\"Retrieved \" + str(len(unique_senders)) + \" unique_senders.\")\nprint(unique_senders.head(10))","c93fee39":"# query_2: unique recipients\nunique_receipients = bq_assistant.query_to_pandas_safe(query_2 % (min_block_number, max_block_number), max_gb_scanned=52)\nprint(\"Retrieved \" + str(len(unique_receipients)) + \" unique_receipients.\")\nprint(unique_receipients.head(10))","84646e1d":"# query_3: unique contracts\nunique_contracts = bq_assistant.query_to_pandas_safe(query_3 % (min_block_number, max_block_number), max_gb_scanned=52)\nprint(\"Retrieved \" + str(len(unique_contracts)) + \" unique_contracts.\")\nprint(unique_contracts.head(10))","03997c3f":"from google.cloud import bigquery\nfrom bq_helper import BigQueryHelper\nimport pandas as pd\n\nbq_assistant = BigQueryHelper(\"bigquery-public-data\", \"ethereum_blockchain\")\nclient = bigquery.Client()\n\nmin_block_number = 5100000\nmax_block_number = 6400000\n\n# find average values and sort\nquery = \"\"\"\nSELECT\n  address, SUM(n_updates) AS updates\nFROM\n(\n  SELECT\n      address, COUNT(*) AS n_updates\n  FROM\n  (\n  SELECT DISTINCT\n    from_address AS address, block_number AS block_number\n  FROM\n    `bigquery-public-data.ethereum_blockchain.transactions`\n  WHERE\n    block_number > %d\n    AND\n    block_number < %d\n  )\n  GROUP BY \n    address\n\n  UNION ALL\n\n  SELECT \n      address AS address, COUNT(*) AS n_updates\n  FROM\n  (\n  SELECT DISTINCT\n    to_address AS address, block_number AS block_number\n  FROM\n    `bigquery-public-data.ethereum_blockchain.transactions`\n  WHERE\n    block_number > %d\n    AND\n    block_number < %d\n  )\n  GROUP BY \n    address\n)\nWHERE\n  n_updates >= 5\n  AND\n  address IS NOT NULL\nGROUP BY \n  address\nORDER BY \n  updates DESC\n\"\"\"\n\nmost_populars = bq_assistant.query_to_pandas_safe(query % (min_block_number, max_block_number, min_block_number, max_block_number), max_gb_scanned=100)\nprint(\"Retrieved \" + str(len(most_populars)) + \" accounts.\")\nblocks_int = max_block_number - min_block_number\nmost_populars = most_populars.sort_values(by='updates', ascending=False)\nmost_populars[\"probability\"] = most_populars[\"updates\"] \/ (blocks_int*1.0)\nprint(most_populars.head(10))","dbe6188e":"from scipy.optimize import curve_fit\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef func_powerlaw(x, m, c, c0):\n    return c0 + x**m * c\n\nblocks_int = max_block_number - min_block_number\n\n# Compute probabilities\nmost_populars[\"probability\"] = most_populars[\"updates\"] \/ (blocks_int*1.0)\nmost_populars[\"idxs\"] = range(1, len(most_populars) + 1)\n\n# Fit curve\nsol = curve_fit(func_powerlaw, most_populars[\"idxs\"], most_populars[\"probability\"], p0 = np.asarray([float(-1),float(10**5),0]))\nfitted_func = func_powerlaw(most_populars[\"idxs\"], sol[0][0], sol[0][1], sol[0][2])\nprint(\"Fit with values {} {} {}\".format(sol[0][0], sol[0][1], sol[0][2]))\n\n# Plot fit vs samples (only for the first 2000)\nplt.rcParams.update({'font.size': 20})\nplt.figure(figsize=(10,5))\nplt.loglog(most_populars[\"probability\"].tolist()[1:10000],'o')\nplt.loglog(fitted_func.tolist()[1:10000])\nplt.xlabel(\"Account index (by descending popularity)\")\nplt.ylabel(\"Relative frequency [1\/block]\")\nplt.show()","87ee6ca0":"import matplotlib.pyplot as plt\nimport numpy as np\n\nquery = \"\"\"\nSELECT \n      timestamp, number\n    FROM\n      `bigquery-public-data.ethereum_blockchain.blocks`\nINNER JOIN \n(\n    SELECT DISTINCT\n              from_address AS address, block_number AS block_number\n            FROM\n              `bigquery-public-data.ethereum_blockchain.transactions`\n            WHERE\n              from_address = '%s'\n              AND\n              block_number > %d\n              AND\n              block_number < %d\n\n    UNION DISTINCT\n\n    SELECT DISTINCT\n              to_address AS address, block_number AS block_number\n            FROM\n              `bigquery-public-data.ethereum_blockchain.transactions`\n            WHERE\n              to_address = '%s'\n              AND\n              block_number > %d\n              AND\n              block_number < %d\n) as InnerTable\nON \n    `bigquery-public-data.ethereum_blockchain.blocks`.number = InnerTable.block_number;\n\"\"\"\n\n# CryptoKitties address\nadx_1 = most_populars.iloc[4].address \ntransax_1 = bq_assistant.query_to_pandas_safe(query % (adx_1, min_block_number, max_block_number, adx_1, min_block_number, max_block_number), max_gb_scanned=60)\nprint(\"Retrieved \" + str(len(transax_1)) + \" blocks for account %s.\" % (adx_1) )\ntransax_1.sort_values(by=\"number\", ascending=True, inplace=True)\n    \n# Bittrex address    \nadx_2 = most_populars.iloc[3].address \ntransax_2 = bq_assistant.query_to_pandas_safe(query % (adx_2, min_block_number, max_block_number, adx_2, min_block_number, max_block_number), max_gb_scanned=60)\nprint(\"Retrieved \" + str(len(transax_2)) + \" blocks for account %s.\" % (adx_2) )\ntransax_2.sort_values(by=\"number\", ascending=True, inplace=True)\n    \ntransax = list()\ntransax.append(transax_1)\ntransax.append(transax_2)","2f40678d":"# plot the Empirical CDF\n\nplt.figure(figsize=(15,5))\nfor t in transax:\n    t.sort_values(by=\"number\", inplace=True)\n    tx_d = t.diff()\n    tx_d = tx_d.iloc[1:]\n    count = np.sort(tx_d[\"number\"].values)\n    cdf = np.arange(len(count)+1)\/float(len(count))\n    plt.plot(count, cdf[:-1])\n\nplt.axis([0, 20, 0, 1])\nplt.xlabel(\"Number of blocks without updates [n]\")\nplt.ylabel(\"Empirical CDF ( Pr[x <= n] )\")\nplt.show()","8b01adeb":"# activity during time\ntxp_1 = transax_1[\"timestamp\"].groupby(transax_1[\"timestamp\"].dt.floor('d')).size().reset_index(name='CryptoKitties')\ntxp_2 = transax_2[\"timestamp\"].groupby(transax_2[\"timestamp\"].dt.floor('d')).size().reset_index(name='Bittrex')\ntxp_1 = txp_1[2:-2]\ntxp_2 = txp_2[2:-2]\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8))\nax = txp_1.plot(x=\"timestamp\", y=\"CryptoKitties\", ax=ax)\nax = txp_2.plot(x=\"timestamp\", y=\"Bittrex\", ax=ax)\nplt.ylabel(\"Active blocks\/day\")\n\n# patterns\nf = plt.figure(figsize=(15,5))\nax = f.add_subplot(121)\nax2 = f.add_subplot(122)\n\nplt.subplot(1, 2, 1)\ntxp_1 = transax_1[\"timestamp\"].groupby(transax_1[\"timestamp\"].dt.day_name()).count().sort_values()\ntxp_1 \/= sum(txp_1)\ntxp_1.plot(kind=\"bar\", ax=ax)\nplt.xlabel(\"Day of the week\")\nplt.ylabel(\"Normalized count\")\nplt.title(\"CryptoKitties\")\n\nplt.subplot(1, 2, 2)\ntxp_2 = transax_2[\"timestamp\"].groupby(transax_2[\"timestamp\"].dt.day_name()).count().sort_values()\ntxp_2 \/= sum(txp_2)\ntxp_2.plot(kind=\"bar\", ax=ax2)\nplt.xlabel(\"Day of the week\")\nplt.ylabel(\"Normalized count\")\nplt.title(\"Bittrex\")","125aacd3":"# 0 Getting started\nfrom google.cloud import bigquery\nfrom bq_helper import BigQueryHelper\nimport pandas as pd\n\nbq_assistant = BigQueryHelper(\"bigquery-public-data\", \"ethereum_blockchain\")","248a7582":"# 1.1 Query for unique deployers ordered by contract count including first and last creation timestamps\nquery_deployers = \"\"\"\nSELECT\n    DISTINCT from_address AS unique_deployer,\n    COUNT(1) AS contracts_deployed,\n    MIN(block_timestamp) AS first_event,\n    MAX(block_timestamp) AS last_event,\n    DATE_DIFF(DATE(MAX(block_timestamp)),DATE(MIN(block_timestamp)),day) AS days_active\nFROM   `bigquery-public-data.ethereum_blockchain.transactions`\n        WHERE\n        \n        block_number < 6400000 \n        AND\n        block_number > 5100000 \n        AND\n        to_address IS null\n        AND\n        receipt_status = 1\n    GROUP BY\n        from_address\n    HAVING\n        COUNT(1) > 4\n    ORDER BY\n        contracts_deployed DESC\n\"\"\"\n\ndeployers = bq_assistant.query_to_pandas_safe(query_deployers, max_gb_scanned=100)\ndeployers.describe()","33e172a3":"# 1.1 continued: Summary of unique deployers ordered by number of deployed contracts (limited to 95th percentile)\ndeployers2 = deployers[deployers.contracts_deployed < deployers.contracts_deployed.quantile(.95)]\ndeployers2.describe()","4ca3c9b0":"# 1.1 continued: Graph for unique deployers ordered by number of deployed contracts (limited to 95th percentile)\nimport matplotlib.pyplot as plt\n\nfig,ax = plt.subplots()\nplt.plot(deployers2.contracts_deployed)\nplt.title(\"Contracts created per deployer \\n limited to the 95th percentile\", y=1.01, fontsize=20)\nplt.ylabel(\"Contracts created\", labelpad=15)\nplt.xlabel(\"Unique deployers\", labelpad=15)\nplt.legend()\n","f82f78da":"# 1.2 Querying unique vs duplicate bytecodes in contract creation\nquery_dupl_bytecode = \"\"\"\nSELECT DISTINCT creator, COUNT(occurrence) as duplicates_creator, SUM(occurrence) AS duplicates, SUM(uniques) AS uniques\nFROM (\n  SELECT\n    DISTINCT(output) AS bytecode,\n    from_address,\n    CASE\n      WHEN trace_address IS NOT NULL THEN 'contract'\n    ELSE\n    'user'\n  END\n    AS creator,\n    CASE\n      WHEN COUNT(*) > 1 THEN COUNT(*)\n  END\n    AS occurrence,\n    CASE\n      WHEN COUNT(*) = 1 THEN 1\n  END\n    AS uniques\n  FROM\n    `bigquery-public-data.crypto_ethereum.traces`\n  WHERE\n    trace_type = 'create'\n    AND status = 1\n    AND block_number > 5400000\n    AND block_number < 6100000\n  GROUP BY\n    from_address,\n    bytecode,\n    creator)\n    GROUP BY creator\n\"\"\"\n\ndupl_bytecode = bq_assistant.query_to_pandas_safe(query_dupl_bytecode, max_gb_scanned=500)\nprint(dupl_bytecode)","1a9dddf5":"# 1.3 Querying user-created contract deployments over time\nimport matplotlib.pyplot as plt\n\nquery_contract_growth = \"\"\"\nWITH\n  a AS (\n  SELECT\n    DATE(block_timestamp) AS date,\n    COUNT(*) AS contracts_creation\n  FROM\n    `bigquery-public-data.crypto_ethereum.traces` AS traces\n  WHERE\n    trace_type = 'create'\n    AND trace_address IS NULL\n  GROUP BY\n    date),\n  b AS (\n  SELECT\n    date,\n    SUM(contracts_creation) OVER (ORDER BY date) AS ccc,\n    LEAD(date, 1) OVER (ORDER BY date) AS next_date\n  FROM\n    a\n  ORDER BY\n    date),\n  calendar AS (\n  SELECT\n    date\n  FROM\n    UNNEST(GENERATE_DATE_ARRAY('2015-07-30', CURRENT_DATE())) AS date),\n  c AS (\n  SELECT\n    calendar.date,\n    ccc\n  FROM\n    b\n  JOIN\n    calendar\n  ON\n    b.date <= calendar.date\n    AND calendar.date < b.next_date\n  ORDER BY\n    calendar.date)\nSELECT\n  DATE,\n  ccc AS cumulative_contract_creation\nFROM\n  c\nORDER BY\n  date desc\n\"\"\"\n\ncontract_growth = bq_assistant.query_to_pandas_safe(query_contract_growth, max_gb_scanned=500)\n\ncontract_growth.plot(x='DATE', y='cumulative_contract_creation', kind='line', \n        figsize=(10, 8), legend=False, style='c-')\nplt.title(\"Cumulative user-created contract deployment \\n from 30.70.2015 until today\", y=1.01, fontsize=20)\nplt.axvline(pd.Timestamp('2018-09-27'),color='black',linestyle='--', label='After the crypto bubble \"explosion\"')\nplt.axvline(pd.Timestamp('2016-06-18'),color='green',linestyle='--', label='DAO Hack')\nplt.ylabel(\"count of user-created contracts (in million)\", labelpad=15)\nplt.xlabel(\"time\", labelpad=15)\nplt.legend()","28e9e221":"# 2.1 Query for heavy users (preliminary analysis of users with 200 to 2000 updates)\nmin1 = 200\nmax1 = 2000\nmin2 = 100\nmax2 = 1000\nmin3 = 400\nmax3 = 4000\nmin4 = 50\nmax4 = 4000\nquery_accounts_updates = \"\"\"\nSELECT \n    address, \n    SUM(n_updates) AS updates,\n    first_txn,\n    last_txn,\n    DATE_DIFF(DATE(last_txn),DATE(first_txn),day) AS days_active,\n    (CASE WHEN address IN (SELECT DISTINCT address FROM `bigquery-public-data.crypto_ethereum.contracts`) THEN TRUE\n    ELSE FALSE END) AS IsContract\nFROM \n(\n    SELECT \n        address, \n        count(*) AS n_updates, \n        MIN(block_timestamp) AS first_txn,\n        MAX(block_timestamp) AS last_txn\n        FROM\n        (\n            SELECT \n                DISTINCT from_address AS address, \n                block_number AS block_number, \n                block_timestamp AS block_timestamp\n                FROM \n                    `bigquery-public-data.ethereum_blockchain.transactions`\n                    WHERE \n                    block_number < 6400000 \n                    AND \n                    block_number > 5100000\n        )\n    GROUP BY address\n    UNION ALL\n    SELECT \n        address AS address, \n        count(*) AS n_updates,\n        MIN(block_timestamp) AS first_txn,\n        MAX(block_timestamp) AS last_txn\n        FROM \n        (\n            SELECT \n                DISTINCT to_address AS address, \n                block_number AS block_number, \n                block_timestamp AS block_timestamp\n                FROM \n                    `bigquery-public-data.ethereum_blockchain.transactions`\n                    WHERE \n                    block_number < 6400000 \n                    AND \n                    block_number > 5100000\n        )\n        GROUP BY address\n)\nWHERE n_updates >= %s \nAND\nn_updates <= %s\nAND\naddress IS NOT NULL\nGROUP BY \n    address,\n    first_txn,\n    last_txn,\n    IsContract\nORDER BY \n    updates DESC\n\"\"\"\n\naccounts_updates1 = bq_assistant.query_to_pandas_safe(query_accounts_updates % (min1, max1), max_gb_scanned=65)\nheavy_users1 = accounts_updates1.loc[accounts_updates1['IsContract'] == False]\nheavy_users1.describe()","d4bf4f8d":"# 2.1 continued: Histrogram to show the distribution of heavy users by updates (200 to 2000 updates)\nimport matplotlib.pyplot as plt\n\naccounts_updates2 = bq_assistant.query_to_pandas_safe(query_accounts_updates % (min2, max2), max_gb_scanned=65)\nheavy_users2 = accounts_updates2.loc[accounts_updates2['IsContract'] == False]\naccounts_updates3 = bq_assistant.query_to_pandas_safe(query_accounts_updates % (min3, max3), max_gb_scanned=65)\nheavy_users3 = accounts_updates3.loc[accounts_updates3['IsContract'] == False]\naccounts_updates4 = bq_assistant.query_to_pandas_safe(query_accounts_updates % (min4, max4), max_gb_scanned=65)\nheavy_users4 = accounts_updates4.loc[accounts_updates4['IsContract'] == False]\n\nplt.gcf().set_size_inches(10.5, 10.5)\n\n#subplot with 200 to 2000 updates\nplt.subplot(221)\nplt.plot(heavy_users1.updates)\nplt.title('200 to 2000 updates')\n\n#subplot with 100 to 1000 updates\nplt.subplot(222)\nplt.plot(heavy_users2.updates)\nplt.title('100 to 1000 updates')\n\n#subplot with 400 to 4000 updates\nplt.subplot(223)\nplt.plot(heavy_users3.updates)\nplt.title('400 to 4000 updates')\n\n#subplot with 50 to 4000 updates\nplt.subplot(224)\nplt.plot(heavy_users4.updates)\nplt.title('50 to 4000 updates')\n\nplt.show()","27a31565":"# 2.2: Bargraph of users vs contracts by udpates (200 to 2000 updates)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.gcf().set_size_inches(9,9)\n\n#bar position\nr = [0,1,2,3,4,5]\n\n#bar labels\nlabels = ['200-499','500-799','800-1099','1100-1399','1400-1699','1700-2000']\n\n#split data by IsContract\nusers = accounts_updates1.loc[accounts_updates1['IsContract'] == False]\ncontracts = accounts_updates1.loc[accounts_updates1['IsContract'] == True]\n\n#group splitted data by update counts\nusers_by = users.groupby('updates').size().reset_index(name='counts')\ncontracts_by = contracts.groupby('updates').size().reset_index(name='counts')\n\n#split data into selected columns\nusers_bars = [users_by.loc[users_by['updates']< 500]['counts'].sum(),users_by.loc[(users_by['updates']>= 500) & (users_by['updates'] < 800)]['counts'].sum(),users_by.loc[(users_by['updates']>= 800) & (users_by['updates'] < 1100)]['counts'].sum(),users_by.loc[(users_by['updates']>= 1100) & (users_by['updates'] < 1400)]['counts'].sum(),users_by.loc[(users_by['updates']>= 1400) & (users_by['updates'] < 1700)]['counts'].sum(), users_by.loc[users_by['updates'] >= 1700]['counts'].sum()]\ncontracts_bars = [contracts_by.loc[contracts_by['updates']< 500]['counts'].sum(),users_by.loc[(users_by['updates']>= 500) & (users_by['updates'] < 800)]['counts'].sum(),contracts_by.loc[(contracts_by['updates']>= 800) & (contracts_by['updates'] < 1100)]['counts'].sum(),users_by.loc[(users_by['updates']>= 1100) & (users_by['updates'] < 1400)]['counts'].sum(),users_by.loc[(users_by['updates']>= 1400) & (users_by['updates'] < 1700)]['counts'].sum(), contracts_by.loc[contracts_by['updates'] >= 1700]['counts'].sum()]\n\n#plot stacked bargraph\nplt.bar(r, users_bars, color='#7f6d5f', edgecolor='white', width=1, label = 'users')\nplt.bar(r, contracts_bars, bottom=users_bars, color='#557f2d', edgecolor='white', width=1,label = 'contracts')\nplt.xticks(r, labels, fontweight='bold')\nplt.title('Stacked barplot showing users and contracts by updates (200 to 2000 updates)')\nplt.ylabel(\"Number of users\", labelpad=15)\nplt.xlabel(\"Update range\", labelpad=15)\nplt.legend()\nplt.show()","15a92a69":"# 3.1 All Investors ordered by tokens (extensive query explanation below)\n\nquery_token_investors = \"\"\"\nWITH\n  #date of first transfer per token (used in every transfer query to select transfers within x days from first transfer --> ICO date)\n  first_trf AS(\n  SELECT\n    DISTINCT tr.token_address AS token,\n    MIN(tr.block_timestamp) AS time\n  FROM\n    `bigquery-public-data.ethereum_blockchain.token_transfers` AS tr\n  WHERE\n    tr.block_number BETWEEN 510000\n    AND 6400000\n  GROUP BY\n    tr.token_address)\n  ,\n    #issuer per token (issuer: most occuring from_address per token within 15 days of first token transfer)\n  issuer AS (\n    #select from_address with highest occurrence (occurrence = max_count) per token --> issuer\n  SELECT\n    DISTINCT tt.token_address AS token1,\n    tt.from_address AS issuer1,\n    COUNT(tt.from_address) AS c1\n  FROM\n    `bigquery-public-data.ethereum_blockchain.token_transfers` AS tt\n  INNER JOIN (\n      #select max_count per token\n    SELECT\n      token_address,\n      MAX(c) AS maxCount\n    FROM (\n        #count occurrence of from_addresses (potential issuer) per token\n      SELECT\n        token_address,\n        from_address,\n        COUNT(from_address) AS c\n      FROM\n        `bigquery-public-data.ethereum_blockchain.token_transfers`\n      JOIN\n        first_trf\n      ON\n        token_address = first_trf.token\n      WHERE\n        CAST(block_timestamp AS date) <= DATE_ADD(CAST(first_trf.time AS date),INTERVAL 15 DAY)\n        AND block_number BETWEEN 5100000\n        AND 6400000\n      GROUP BY\n        token_address,\n        from_address)AS tt1\n    GROUP BY\n      token_address) AS groupedtt\n  ON\n    tt.token_address = groupedtt.token_address\n  JOIN\n    first_trf\n  ON\n    tt.token_address = first_trf.token\n  WHERE\n    CAST(tt.block_timestamp AS date) <= DATE_ADD(CAST(first_trf.time AS date),INTERVAL 15 DAY)\n    AND tt.block_number BETWEEN 5100000\n    AND 6400000\n  GROUP BY\n    tt.token_address,\n    tt.from_address,\n    groupedtt.maxCount\n  HAVING\n    groupedtt.maxCount = c1\n    AND c1 > 1)\n  #select all investors who have once sent a transaction to that contract (cross-check)\nSELECT\n  investor,\n  token_contract,\n  token_issuer,\n  first_transfer\nFROM (\n    #select all investors (investors =^ to_addresses that received a token transfer from the respective issuer)\n  SELECT\n    DISTINCT t.to_address AS investor,\n    symbol,\n    issuer.token1 AS token_contract,\n    issuer.issuer1 AS token_issuer,\n    first_trf.time AS first_transfer\n  FROM\n    `bigquery-public-data.ethereum_blockchain.token_transfers` AS t\n  JOIN\n    issuer\n  ON\n    issuer.issuer1 = t.from_address\n    AND issuer.token1 = t.token_address\n  JOIN\n    `bigquery-public-data.ethereum_blockchain.tokens`\n  ON\n    address = t.token_address\n  JOIN\n    first_trf\n  ON\n    first_trf.token = issuer.token1\n  WHERE\n    CAST(t.block_timestamp AS date) <= DATE_ADD(CAST(first_trf.time AS date),INTERVAL 15 DAY)\n    AND t.block_number BETWEEN 5100000\n    AND 6400000\n  GROUP BY\n    investor,\n    token_issuer,\n    token1,\n    symbol,\n    first_trf.time)\n  #crosscheck that investor has once made txn to token_contract\nINNER JOIN\n  `bigquery-public-data.ethereum_blockchain.transactions`\nON\n  investor = from_address\nWHERE\n  to_address = token_contract\nGROUP BY\n  token_contract,\n  investor,\n  token_issuer,\n  first_transfer\nORDER BY\n  token_contract ASC\n  \"\"\"\n\ntoken_investors = bq_assistant.query_to_pandas_safe(query_token_investors, max_gb_scanned=500)\ntok_inv = token_investors.head(5)\ntok_inv.append(token_investors.tail(5))","a7de6cb9":"# 3.2 Unique investors who have invested in at least 5 different tokens\nimport matplotlib.pyplot as plt\ninvestments_per_investor = token_investors.groupby('investor').size().reset_index(name='investment_count')\ninv_five = investments_per_investor.loc[investments_per_investor['investment_count'] >= 5].groupby('investment_count').size().reset_index(name = 'investors')\ninv_five.describe()","d61df993":"# 3.2 continued: Bargraph of investments per investor\nimport matplotlib.pyplot as plt\n\nplt.gcf().set_size_inches(9,9)\n\n#subplot with investment_count >= 5\nplt.subplot(211)\nplt.bar(inv_five.investment_count,inv_five.investors)\nplt.title('Investors per Investment count (count >= 5)', y=1.01, fontsize=15)\nplt.ylabel('Investors per investment_count',labelpad=15, fontsize=12)\n\n#subplot with investment_count >= 35\nplt.subplot(212)\nplt.bar(inv_five.loc[inv_five['investment_count'] >= 35].investment_count, inv_five.loc[inv_five['investment_count'] >= 35].investors)\nplt.title('Investors per Investment count (count >= 35)', y=1.01, fontsize=15)\nplt.xlabel('Investment_count', fontsize=12)\nplt.ylabel('Investors per investment_count',labelpad=15, fontsize=12)\nplt.show()","f5db4120":"# 3.3 Token investors by invesment count (>=5) and by eth.invested\nquery_eth_by_investor = \"\"\"\nWITH\n  first_trf AS(\n  SELECT\n    DISTINCT tr.token_address AS token,\n    MIN(tr.block_timestamp) AS time\n  FROM\n    `bigquery-public-data.ethereum_blockchain.token_transfers` AS tr\n  WHERE\n    tr.block_number BETWEEN 510000\n    AND 6400000\n  GROUP BY\n    tr.token_address)\n  #issuer per token (issuer =^ most occuring from_address per token within 15 days of first token transfer)\n  ,\n  issuer AS (\n    #select from_address with highest occurrence (occurrence = max_count) per token --> issuer\n  SELECT\n    DISTINCT tt.token_address AS token1,\n    tt.from_address AS issuer1,\n    COUNT(tt.from_address) AS c1\n  FROM\n    `bigquery-public-data.ethereum_blockchain.token_transfers` AS tt\n  INNER JOIN (\n      #select max_count per token\n    SELECT\n      token_address,\n      MAX(c) AS maxCount\n    FROM (\n        #count occurrence of from_addresses (potential issuer) per token\n      SELECT\n        token_address,\n        from_address,\n        COUNT(from_address) AS c\n      FROM\n        `bigquery-public-data.ethereum_blockchain.token_transfers`\n      JOIN\n        first_trf\n      ON\n        token_address = first_trf.token\n      WHERE\n        CAST(block_timestamp AS date) <= DATE_ADD(CAST(first_trf.time AS date),INTERVAL 15 DAY)\n        AND block_number BETWEEN 5100000\n        AND 6400000\n      GROUP BY\n        token_address,\n        from_address)AS tt1\n    GROUP BY\n      token_address) AS groupedtt\n  ON\n    tt.token_address = groupedtt.token_address\n  JOIN\n    first_trf\n  ON\n    tt.token_address = first_trf.token\n  WHERE\n    CAST(tt.block_timestamp AS date) <= DATE_ADD(CAST(first_trf.time AS date),INTERVAL 15 DAY)\n    AND tt.block_number BETWEEN 5100000\n    AND 6400000\n  GROUP BY\n    tt.token_address,\n    tt.from_address,\n    groupedtt.maxCount\n  HAVING\n    groupedtt.maxCount = c1\n    AND c1 > 1)\n  #select all investors who have once sent a transaction to that contract (cross-check)\nSELECT\n  DISTINCT investor,\n  COUNT(token_contract) as investments,\n  sum(value)\/power(10,18) as eth_invested\nFROM\n(SELECT\n  investor,\n  token_contract,\n  token_issuer,\n  first_transfer,\n  value\nFROM (\n    #select all investors (investors =^ to_addresses that received a token transfer from the respective issuer)\n  SELECT\n    DISTINCT t.to_address AS investor,\n    symbol,\n    issuer.token1 AS token_contract,\n    issuer.issuer1 AS token_issuer,\n    first_trf.time AS first_transfer\n  FROM\n    `bigquery-public-data.ethereum_blockchain.token_transfers` AS t\n  JOIN\n    issuer\n  ON\n    issuer.issuer1 = t.from_address\n    AND issuer.token1 = t.token_address\n  JOIN\n    `bigquery-public-data.ethereum_blockchain.tokens`\n  ON\n    address = t.token_address\n  JOIN\n    first_trf\n  ON\n    first_trf.token = issuer.token1\n  WHERE\n    CAST(t.block_timestamp AS date) <= DATE_ADD(CAST(first_trf.time AS date),INTERVAL 15 DAY)\n    AND t.block_number BETWEEN 5100000\n    AND 6400000\n  GROUP BY\n    investor,\n    token_issuer,\n    token1,\n    symbol,\n    first_trf.time)\n  #crosscheck that investor has once made txn to token_contract\nINNER JOIN\n  `bigquery-public-data.ethereum_blockchain.transactions`\nON\n  investor = from_address\nWHERE\n  to_address = token_contract\nGROUP BY\n  token_contract,\n  investor,\n  token_issuer,\n  first_transfer,\n  value)\nGROUP BY\n  investor\nHAVING\n  investments >= 2\nORDER BY\n  eth_invested DESC\n  \"\"\"\neth_by_investor = bq_assistant.query_to_pandas(query_eth_by_investor)\neth_by_investor.head()","6406cd94":"# 3.3 continued: Plots of token investors by investment value\nimport matplotlib.pyplot as plt\n\nplt.gcf().set_size_inches(9,9)\n\n#round all investments to nearest float with one decimal place and group data by count of eth invested\ninv_value_grouped = eth_by_investor.round(1).groupby('eth_invested').size().reset_index(name = 'count')\n\n#subplot including all ether values invested\nplt.subplot(211)\nplt.plot(inv_value_grouped.eth_invested)\nplt.title('Investors per Investment Value', y=1.01, fontsize=15)\nplt.ylabel('Investment value (per unique investor)',labelpad=15, fontsize=12)\n\n#limit dataset to ether value from 1 to 100\ninv_value_lim = inv_value_grouped.loc[(inv_value_grouped['eth_invested']>= 1) & (inv_value_grouped['eth_invested'] < 100)]\n\n#subplot including ether values invested from 1 to 100\nplt.subplot(212)\nplt.plot(inv_value_lim.eth_invested)\nplt.title('Investors per Investment Value (1 to 100)', y=1.01, fontsize=15)\nplt.xlabel('Investors', fontsize=12)\nplt.ylabel('Investment value (per unique investor)',labelpad=15, fontsize=12)\n\nplt.show()","0d5c4c7b":"This plot illustrates the development trajectory of user-created contracts between July 30th 2015 and today. The red line signifies June 18th, 2016, the date of the DAO Hack. The black line represents September 27th, 2018, the approximate date at which the crypto bubble exploded.","acab7807":"We plot the relative frequency of accounts that are modified at least once per hour (there are approximately 10k of them in the main network). Their relative frequency doesn't follow a \"simple\" power-law, because the 20 most popular accounts have \"equal power\", i.e. they show similar activity. A broken power-law would be more appropriate, as we have observed in a recent paper. In that document, you can find more insights about how to use the statistics of accounts to design better protocols.","c97d3b9e":"To help you understand the previous query, here is a small explanation of our assumptions and approach:\n\nFirstly, with investors we mean people who sent ether to a token contract within its ICO time period.\nSince we cannot find out the **ICO dates** of each token using GBQ data, we assume that it is always the **first 15 days after the first token_transfer**.\n\nTo find out the investors of each token, we assumed that each token has a **'token-issuer'**, an address that sends out tokens to the users who invested. We assumed that within the ICO time period, **the address that sent out the most token transfers** is the token-issuer.\n\nTo get the **investors**, now list **every address that received a token transfer from the token-issuer within the ICO time period**.\n\nTo make sure that we only get investors who actually sent ether to the token, we cross-check the investors for having sent an external transaction to the respective token.\n\n*Note: the 0x00 addresses signify that the token is mintable since mintable tokens use the 0x00 address to mint \/ burn tokens. This is a useful insight for future analyses of mintable vs. non-mintable tokens.*","0d7136b0":"To arrive at the stacked barplot, we first split the existing data by 'IsContract' into users and contracts. Subsequently, we grouped the data by 'updates' and then split this data again by bin-sizes. For each bin-size, we computed the total count of users \/ contracts within that range and then stacked the bars on top of each other.\n\nWe now continue by examining the next user group: the token investors.","ed413c5e":"If GoogleBigQueryHelper for whatever reason does not work, you can also query the data set with the big query client. Below you find the respective code.","17036840":"Now, lets try different queries to get a first overview of the database We limit our query to blocks after the crypto bubble exploded.","a6cd5cdb":"From the results, it is possible to extract several interesting statistics. For example, the cumulative density function (CDF) of number of blocks between two consecutive updates to the account. The knowledge of the CDF is important to design and evaluate the performances of blockchain lightweight protocols. For example, we can decide to sync our blockchain client periodically, and tune the period based on the statistics of the account that we are interested in.","f3ecfe60":"The barplots above show the number of investors (y-axis) per investment-count (x-axis). The first barplot shows a range of investment_count between a restricted minimum of 5 and the maximum of 127 investments. The lower plot shows a range from 35 to 127 to show the distribution of investors at the higher end.","fc649c4e":"We, the ethereum smart contract project group from the [TIM](https:\/\/www.professors.wi.tum.de\/tim\/home\/) Institute at the Technical University of Munich, show how to extract meaningful statistics of accounts of the Ethereum main network, from the data set hosted by Google BigQuery. This kernel is structured as follows.\n\nTable of content:\n1. Setting up Google Big Query and first queries\n2. Identifying unique users\n3. Creating a data set comprising all users' transactions\n4. Identifying different types of users\n\n*In progress:*\n* *Creating a data set comprising different exogenous shocks (scams, regulations, forks, updates)*\n* *Analyzing users reactions to exogenous shocks*\n* *Contract to contract transactions (difficult to implement)*","4b392668":"bq_helper is planned to be deprecated. Please consider using from google.cloud import bigquery for reproducible kernels. See GBQ Documentation for details.","922b7827":"Here is an overview of different helpful functions comming along with the GBQ Helper.","63cc1f81":"# 4. Identifying different types of users\n\nTo gain a more thorough understanding of the composition of the ethereum community, we divide unique users into the following groups:\n\n1. **Deployers**: addresses that have created contracts (includes both users and contracts)\n2. **Heavy users**: categorising unique users by updates (= mention in txn as from_address or to_address)\n3. **Investors**: accounts that participated in several ICOs\n\nSubsequently, we use these groups to analyse the behaviour, interrelation and development of the community.\n\nThe timeframe we observe is between block 5,100,000 (February 16th, 2018) and block 6,400,000 (September 26th, 2018) which limits the study to a range of blocks after the \"explosion\" of the crypto bubble.\n\nSeveral queries, especially of the contract deployers were taken in part from the publicly available master's thesis:\n\n> A. Voulgari, 2019, \"Ethereum Analytics\", ETH Z\u00fcrich","28346688":"We continue with the range of 200 to 2000 updates.\nTo be able to put into relation the distribution of user updates with contract updates, we compare users and contracts in a stacked barplot.","be7f4c02":"Looking at numbers of contracts_deployed below the 95th percentile to roughly removes the largest outliers.","caedce1d":"The plots above illustrate the number of token investors per ETH value invested. The invested values are rounded to floats with one decimal place. While the first plot has no restriction, the second one looks only at investments between 1 and 100 ether.\n\nThat's it for now.","65acea81":"# 2. Identifying unique users\n\nOk, we obtained the list of most popular addresses and printed the top ten. A quick check of the addresses on a blockchain explorer (https:\/\/etherscan.io\/) tells us that they are all associated with currency exchanges or popular decentralized applications (dApps). We might expect that the accounts follow a power-law, where the most \"active\" accounts are owned by centralized exchanges. Is it true?","4fd39571":"# 1. Setting up Google Big Query (GBQ) and first queries\nFirst, we have to import all necessary libraries and initialize a GBQ client and BigQueryHelper instance.\n\nBigQueryHelper makes querying GBQ even easier on Kaggle. It has a function to automatically load queries into a pandas dataframe and provides an option to set a query limit. You can find it documentation [here](https:\/\/github.com\/SohierDane\/BigQuery_Helper).\n\nAs suggested [here](https:\/\/www.kaggle.com\/mrisdal\/visualizing-average-ether-costs-over-time?utm_medium=partner&utm_source=cloud&utm_campaign=big+data+blog+ethereum), we set a safety limit to control the quota of free queries provided by Kaggle. For this kernel, you will read a maximim of 120 GB from BigQuery. As the database size is constantly increasing, in the future you will incur in a error, if the amount of scanned data is greater than max_gb_scanned=40.","b15451dd":"In the next step, we look at unique and duplicate bytecodes created by contract deployers.","2a255d45":"The previous query returns all investors with the amount invested and the count of tokens that he invested in. It contains a total of 770533 unique investors with a mean (max) of 4 (282) investments and 0.1 (6000) ETH invested.","22a72522":"todos:\n\n* Welche user nutzen welche Smart Contracts (wie gro\u00df ist das DS)\n* Liste an Smart Contracts\/ICO die Scams waren (au\u00dferhalb GBQ) (andere au\u00dfergew\u00f6hnliche Events)\n* Wenn Anfragen zu gro\u00df sind, dann Test DS erstellen","b2b60a4f":"We can also look for specific activity patterns. For example, in the figure below we show that, even if the two accounts show the same relative frequency on average, their activity is not stationary.\n\nOther question: on which day of the week are the two accounts more active? The bar figures show that CryptoKitties, that is a game, is particularly used during the weekends. On the other hand, Bittrex is active on Mondays and Tuesdays and on Friday, that is quite expected for a trading platform.","e22d6272":"This query was executed via the 'traces' dataset which contains both user-created contracts and contract-created contracts. When the trace_address is null the creator is a user, because the trace was executed directly, not through another trace.\n\nWe only considered bytecodes as duplicates when they exactly match each other without accounting for ones that are very similar. Even so, the result shows that only 2620 contracts were responsible for the creation of 764,634 contracts.\n\nTo get a better grasp of the contract deployers, we next have a look at contract creation by non-contracts only over time.","1f33cdc7":"The above table summarises the data of contract deployers by number of contracts deployed. The large discrepancy in contracts deployed and the relatively small timeframes give rise to question the uniqueness of these contracts.\nAs a result, we first analyse this data in a graph and afterwards find out the uniqueness of the contract bytecode.","29f48f49":"In the previous query, we used a range of 200 to 2000 updates because preliminary analyses revealed that the spread of users per update_count above and below that range distorts the data and produces a large amount of outliers.\n\nTo illustrate this spread, the following query shows the ranges from:\n* 200 to 2000 updates\n* 100 to 1000 updates\n* 400 to 4000 updates\n* 50 to 4000 updates\n\nTo create the different subsets, we created variables at the start of the previous query which are now inserted into the query using the query_to_pandas_safe function.","4385fa0d":"# 3. Statistics of specific accounts\n\nWe can also observe the activity of specific accounts. For example, the following query returns the block numbers (and their timestamp) at which an account has been modified. We apply it to two addresses: one associated with CryptoKitties dApp, and one with Bittrex exchange, that are characterized by similar relative frequency of updates."}}