{"cell_type":{"33d2d647":"code","fbd2809f":"code","f62a773a":"code","7928ba50":"markdown"},"source":{"33d2d647":"import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import GroupKFold\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import mixed_precision\nfrom tensorflow.keras import backend as K\nfrom tqdm.notebook import tqdm\nimport random\nimport warnings\nimport gc\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 100)","fbd2809f":"# Function to get hardware strategy\ndef get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        policy = mixed_precision.Policy('mixed_bfloat16')\n        mixed_precision.set_global_policy(policy)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    return tpu, strategy\n\ntpu, strategy = get_hardware_strategy()\n# Configuration\nEPOCHS = 15\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\n# Model Seed \nMODEL_SEED = 42\n# Learning rate\nLR = 0.0008\n# Folds\nFOLDS = 5\n# Verbosity\nVERBOSE = 2\n# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE","f62a773a":"# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \ndef correlationLoss(x, y, axis = -2):\n    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n    while trying to have the same mean and variance\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = tf.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis = axis)\n    ysum = tf.reduce_sum(y, axis = axis)\n    xmean = xsum \/ n\n    ymean = ysum \/ n\n    xsqsum = tf.reduce_sum(tf.math.squared_difference(x, xmean), axis = axis)\n    ysqsum = tf.reduce_sum(tf.math.squared_difference(y, ymean), axis = axis)\n    cov = tf.reduce_sum((x - xmean) * (y - ymean), axis = axis)\n    corr = cov \/ tf.sqrt(xsqsum * ysqsum)\n    sqdif = tf.reduce_sum(tf.math.squared_difference(x, y), axis = axis) \/ n \/ tf.sqrt(ysqsum \/ n)\n    return tf.convert_to_tensor(K.mean(tf.constant(1.0, dtype = x.dtype) - corr + (0.01 * sqdif)))\n    \ndef build_model(shape, steps):\n    with strategy.scope(): \n        def fc_block(x, units):\n            x = tf.keras.layers.Dropout(0.35)(x)\n            x = tf.keras.layers.Dense(units, activation = 'relu')(x)\n            return x\n        \n        inp = tf.keras.layers.Input((shape))\n        x = fc_block(inp, units = 768)\n        x = fc_block(x, units = 384)\n        x = fc_block(x, units = 192)\n        output = tf.keras.layers.Dense(1, activation = 'linear')(x)\n        model = tf.keras.models.Model(inputs = [inp], outputs = [output])\n        scheduler = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate = LR, decay_steps = steps, end_learning_rate = 0.000005)\n        opt = tf.keras.optimizers.Adam(learning_rate = scheduler)\n        model.compile(\n            optimizer = opt,\n            loss = [tf.keras.losses.MeanSquaredError()],\n        )\n        return model\n\n# Custom callback for mean pearson correlation coefficient with early stopping\nclass mpcc_metric(tf.keras.callbacks.Callback):\n    def __init__(self, validation_data, indices, targets, patience = 7):\n        super(mpcc_metric, self).__init__()\n        self.patience = patience\n        # Store best weights\n        self.best_weights = None\n        self.x_val, self.y_val = validation_data\n        self.indices = indices\n        self.targets = targets\n        \n    def on_train_begin(self, logs = None):\n        self.wait = 0\n        self.stopped_epoch = 0\n        self.best = 0\n        \n    def on_epoch_end(self, epoch, logs = None):\n        prediction = self.model.predict(self.x_val).astype(np.float32).reshape(-1)\n        p_corr_co = []\n        for i in range(len(self.targets)):\n            p_corr_co.append(pearsonr(self.targets[i], prediction[self.indices[i]])[0])\n        p_corr_co_score = np.average(np.array(p_corr_co))\n        print(\"Mean Pearson correlation coefficient  - epoch: {:d} - score: {:.6f}\".format(epoch + 1, p_corr_co_score))\n        if p_corr_co_score > self.best:\n            print('Validation score improved!')\n            self.best = p_corr_co_score\n            self.wait = 0\n            self.best_weights = self.model.get_weights()\n        else:\n            self.wait += 1\n        if self.wait >= self.patience:\n            self.stopped_epoch = epoch\n            self.model.stop_training = True\n            print('Restoring the best weights from the best epoch')\n            self.model.set_weights(self.best_weights)\n            print('Saving best weights to disk')\n            self.model.save_weights('simple_fc.h5')\n    def on_train_end(self, logs = None):\n        if self.stopped_epoch > 0:\n            print('Epoch %05d: Early Stopping' % (self.stopped_epoch + 1))\n\n# Calculate pearson correlation coefficient\ndef pearson_coef(data):\n    return data.corr()['target']['prediction']\n\n# Calculate mean pearson correlation coefficient\ndef comp_metric(valid_df):\n    return np.mean(valid_df.groupby(['time_id']).apply(pearson_coef))\n\n# Function to train and evaluate\ndef train_and_evaluate():\n    # Seed everything\n    seed_everything(MODEL_SEED)\n    # Read data\n    train = pd.read_pickle('..\/input\/ubiquant-market-prediction-half-precision-pickle\/train.pkl')\n    # Feature list\n    features = [col for col in train.columns if col not in ['row_id', 'time_id', 'investment_id', 'target']]\n    # Some feature engineering\n    # Get the correlations with the target to encode time_id\n    corr1 = train[features[0:100] + ['target']].corr()['target'].reset_index()\n    corr2 = train[features[100:200] + ['target']].corr()['target'].reset_index()\n    corr3 = train[features[200:] + ['target']].corr()['target'].reset_index()\n    corr = pd.concat([corr1, corr2, corr3], axis = 0, ignore_index = True)\n    corr['target'] = abs(corr['target'])\n    corr.sort_values('target', ascending = False, inplace = True)\n    best_corr = corr.iloc[3:103, 0].to_list()\n    del corr1, corr2, corr3, corr\n    # Add time id related features (market general features to relate time_ids)\n    time_id_features = []\n    for col in tqdm(best_corr):\n        mapper = train.groupby(['time_id'])[col].mean().to_dict()\n        train[f'time_id_{col}'] = train['time_id'].map(mapper)\n        train[f'time_id_{col}'] = train[f'time_id_{col}'].astype(np.float16)\n        time_id_features.append(f'time_id_{col}')\n    print(f'We added {len(time_id_features)} features related to time_id')\n    # Update feature list\n    features += time_id_features\n    np.save('features.npy', np.array(features))\n    np.save('best_corr.npy', np.array(best_corr))\n    # Store out of folds predictions\n    oof_predictions = np.zeros(len(train))\n    # Initiate GroupKFold (all investment_id should be in the same fold, we want to predict new investment_id)\n    kfold = GroupKFold(n_splits = FOLDS)\n    # Create groups based on time_id\n    train.loc[(train['time_id'] >= 0) & (train['time_id'] < 280), 'group'] = 0\n    train.loc[(train['time_id'] >= 280) & (train['time_id'] < 585), 'group'] = 1\n    train.loc[(train['time_id'] >= 585) & (train['time_id'] < 825), 'group'] = 2\n    train.loc[(train['time_id'] >= 825) & (train['time_id'] < 1030), 'group'] = 3\n    train.loc[(train['time_id'] >= 1030) & (train['time_id'] < 1400), 'group'] = 4\n    train['group'] = train['group'].astype(np.int16)\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, groups = train['group'])):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train[features].loc[trn_ind], train[features].loc[val_ind]\n        y_train, y_val = train['target'].loc[trn_ind], train['target'].loc[val_ind]\n        # Reset keras session and tpu\n        K.clear_session()\n        if tpu:\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n        n_training_rows = x_train.shape[0]\n        n_validation_rows = x_val.shape[0]\n        STEPS_PER_EPOCH = n_training_rows  \/\/ BATCH_SIZE\n        # Build simple fc model\n        print('Building model...')\n        model = build_model(len(features), STEPS_PER_EPOCH * EPOCHS)\n        print(f'Training with {n_training_rows} rows')\n        print(f'Validating with {n_validation_rows} rows')\n        print(f'Training model with {len(features)} features...')\n        # Callbacks\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n            f'simple_fc_dnn_{fold + 1}.h5', \n            monitor = 'val_loss', \n            verbose = VERBOSE, \n            save_best_only = True,\n            save_weights_only = True, \n            mode = 'min', \n            save_freq = 'epoch'\n        )\n        # Train and evaluate\n        history = model.fit(\n            x = x_train,\n            y = y_train,\n            batch_size = BATCH_SIZE,\n            epochs = EPOCHS,\n            verbose = VERBOSE,\n            callbacks = [checkpoint],\n            validation_data = (x_val, y_val),\n        )\n        # Predict validation set\n        val_pred = model.predict(x_val, batch_size = BATCH_SIZE).astype(np.float32).reshape(-1)\n        # Add validation prediction to out of folds array\n        oof_predictions[val_ind] = val_pred\n    # Compute out of folds Pearson Correlation Coefficient (for each time_id)\n    oof_df = pd.DataFrame({'time_id': train['time_id'], 'target': train['target'], 'prediction': oof_predictions})\n    # Save out of folds csv for blending\n    oof_df.to_csv('simple_fc_dnn.csv', index = False)\n    score = comp_metric(oof_df)\n    print(f'Our out of folds mean pearson correlation coefficient is {score}')    \n    \ntrain_and_evaluate()","7928ba50":"# Quick Notes\n* This is a time series problem, we should split the data based on time. In my experiments it does not provide good cv and lb\n* Best version is splitting data with GroupKFold\n* With this in mind, im still looking for a good cv strategy that can fit models in the correct spot\n* It seems the data is already normalized\n* The competition metric is the mean pearson correlation for each time_id, previous notebook computes the metric at the end of each epoch but for speed reason I only compute it at the end of the training\n* The dataset is very large, use pickle, parquet or others format to save memory\n* Probably a sequence model is much better than a fully connected model (RNN, CNN, Transformer)\n\nExperiment configs:\n\nKFOLD_STRAT -> GroupKFold using investiment_id CV -> 0.1936 LB -> 0.144 EPOCHS -> 15\n\nKFOLD_STRAT -> GroupKFold using investiment_id CV -> 0.1822 LB -> 0.144 EPOCHS -> 8\n\nKFOLD_STRAT -> GroupKFold using time_id CV -> 0.1461 LB -> 0.145 EPOCHS -> 15\n\nLast experiment seems to be the best way of validation, cv lb gap is small"}}