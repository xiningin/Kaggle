{"cell_type":{"af81f83c":"code","a69c5502":"code","8d8924f8":"code","c2b2d466":"code","4dd811cf":"code","09f395a8":"code","435de58c":"code","49588ea0":"code","fdb35ca0":"code","28f50be5":"code","c9499bb6":"code","e2bb203d":"code","9e82c67c":"code","a14c27c0":"markdown","79efbee2":"markdown","3d6ef1e7":"markdown","318056be":"markdown","c7df765f":"markdown","b4a1b41a":"markdown","cce6a175":"markdown"},"source":{"af81f83c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a69c5502":"x = np.load(\"..\/input\/sign-language-digits-dataset\/X.npy\")\ny = np.load(\"..\/input\/sign-language-digits-dataset\/Y.npy\")","8d8924f8":"x.shape","c2b2d466":"y.shape","4dd811cf":"plt.figure(figsize = (4,4))\nplt.imshow(x[4])\nplt.axis(\"off\")\nplt.show()","09f395a8":"plt.subplot(1,2,1)\nplt.imshow(x[500])\nplt.axis(\"off\")\nplt.subplot(1,2,2)\nax2 = plt.imshow(x[1234])\nplt.axis(\"off\")\nplt.show()","435de58c":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.15, random_state = 42)","49588ea0":"x_test.shape","fdb35ca0":"x_train.shape","28f50be5":"x_train = x_train.reshape(x_train.shape[0], 64*64)\nx_test = x_test.reshape(x_test.shape[0], 64*64)","c9499bb6":"x_train.shape","e2bb203d":"from sklearn.model_selection import cross_val_score\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\n\n\nmodel = Sequential()\nmodel.add(Dense(units = 200, kernel_initializer = \"uniform\", activation = \"relu\", \n                input_dim = x_train.shape[1]))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(units = 100, kernel_initializer= \"uniform\", activation = \"relu\"))\nmodel.add(Dense(units = 50, kernel_initializer= \"uniform\", activation = \"relu\"))\nmodel.add(Dense(units = 25, kernel_initializer= \"uniform\", activation = \"relu\"))\nmodel.add(Dense(units = 10, kernel_initializer= \"uniform\", activation = \"softmax\"))\noptimizer = Adam(lr = 0.001, beta_1= 0.9, beta_2 = 0.999)\nmodel.compile(optimizer = optimizer, loss = \"binary_crossentropy\", \n              metrics = [\"accuracy\", \"AUC\"])\n\nmodel.fit(x_train, y_train, epochs= 800, batch_size = 150, verbose = 1 )","9e82c67c":"from sklearn.metrics import accuracy_score\ny_pred = model.predict_classes(x_test)\ny_test_classes = np.argmax(y_test, axis = 1)\nac = accuracy_score(y_test_classes, y_pred)\nprint(\"Accuracy of the NN Model is : \", ac,\"%\")","a14c27c0":"<a id = 2 ><\/a><br>\n# Shaping the Data\n\n- After train-test split, we should reshape our pictures as 2D matrices to use them as \"neural network\" imputs.\n- Let's code the reshape.","79efbee2":"# Introduction\n1. [Have a look to the data](#1)\n1. [Shaping the Data](#2)\n1. [Get Ready to Keras - ANN Classifier](#3)\n1. [Conclusion](#4)","3d6ef1e7":"<a id = 4><\/a>\n# Conclusion\n- Now we can see the test accuracy here.\n- This can be increased by tuning the parameters or adding\/dropping layers or increasing\/decreasing neural network nodes as well. \n- Thanks for your attention, I am looking forward to discuss in comments.","318056be":"- Looks like we have 2062 pictures in a 64x64 matrix shape, and we have their corresponding labes as \"y\".\n- Let's divide the data to train and test.","c7df765f":"- After watching the output, we can see at the bottom that accuracy of training algorithm is 92.35% and AUC is 0.99.\n- Now we are going to test the model with our spared test dataset (x_test, y_test)","b4a1b41a":"<a id = 1 ><\/a><br>\n\n- Let's get a view to the data and handle it.","cce6a175":"<a id = 3><\/a>\n\n# Get Ready to Build the Neural Network\n\n- After preprocessing the images as 2D arrays, we are now ready to use them in Neural Network. \n- We are going to use Keras library for building neural network. We are going to build a five-layered network here. Nodes in every layer will progressively decrease about a half. \n- We are going to use activation functions as \"relu\" : \"Rectified Linear Unit\", to increase non-linearity which is commonly seen in realty.\n- We'll initialize weights as uniform. (kernel_initializer = \"uniform\")\n- We'll use Dropout technic for trying to avoid overfitting as randomly ignoring %15 of nodes in the 1st hidden layer.\n- At the final, we are going to use \"softmax\" activation function which can be simplified as sigmoid function for multiclass problems.\n- After ensembling the layers we are finally compile our network with learning rate optimizer of \"Adam\" (adaptive moment estimation).\n- We can change number of epochs and batch size ofcourse. Now, we are going to set them as 800 and 150."}}