{"cell_type":{"f428be95":"code","a0f72f10":"code","3e330cd2":"code","88343dc9":"code","ca3d5994":"code","873df9b9":"code","156aa0cf":"code","a1d13203":"code","a0588c60":"code","eba83b41":"code","ca130f4d":"code","f3dc83c5":"markdown","0a8c5952":"markdown","55e8b033":"markdown"},"source":{"f428be95":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout\nfrom keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nfrom math import floor\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import StratifiedKFold\nimport pickle\nfrom keras.layers import LeakyReLU\nLeakyReLU = LeakyReLU(alpha=0.1)\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", None)","a0f72f10":"# Make scorer accuracy\nscore_acc = make_scorer(accuracy_score)\n\n# Load dataset\ntrainSet = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\n\n# Feature generation: training data\ntrain = trainSet.drop(columns=['Name', 'Ticket', 'Cabin'])\ntrain = train.dropna(axis=0)\ntrain = pd.get_dummies(train)\n\ntrain.head()","3e330cd2":"# train validation split\nX_train, X_val, y_train, y_val = train_test_split(train.drop(columns=['PassengerId','Survived'], axis=0),\n                                                  train['Survived'],\n                                                  test_size=0.2, random_state=111,\n                                                  stratify=train['Survived'])","88343dc9":"# Set seed\nfrom numpy.random import seed\nseed(123)\n\nimport os\nos.environ['PYTHONHASHSEED']=str(123) \n\nimport random\nrandom.seed(123)\n\nimport tensorflow as tf\ntf.random.set_seed(123)","ca3d5994":"# Create function\ndef nn_cl_bo(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n        \n    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n                   'elu', 'exponential', LeakyReLU,'relu']\n    \n    neurons = round(neurons)\n    activation = activationL[round(activation)]\n    batch_size = round(batch_size)\n    epochs = round(epochs)\n    \n    def nn_cl_fun():\n        opt = Adam(lr = learning_rate)\n        \n        nn = Sequential()\n        nn.add(Dense(neurons, input_dim=10, activation=activation))\n        nn.add(Dense(neurons, activation=activation))\n        nn.add(Dense(1, activation='sigmoid'))\n        nn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n        return nn\n    \n    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,\n                         verbose=0)\n    \n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n    #score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold).mean()\n    \n    return score","873df9b9":"params_nn ={\n    'neurons': (10, 100),\n    'activation':(0, 9),\n    'optimizer':(0,7),\n    'learning_rate':(0.01, 1),\n    'batch_size':(200, 1000),\n    'epochs':(20, 100)\n}\n\n# Run Bayesian Optimization\nnn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=111)\nnn_bo.maximize(init_points=25, n_iter=4)","156aa0cf":"params_nn_ = nn_bo.max['params']\n\nactivationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n               'elu', 'exponential', LeakyReLU,'relu']\nparams_nn_['activation'] = activationL[round(params_nn_['activation'])]\n\nparams_nn_","a1d13203":"# Create function\ndef nn_cl_bo2(neurons, activation, optimizer, learning_rate, batch_size, epochs,\n              layers1, layers2, normalization, dropout, dropout_rate):\n    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n        \n    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n                   'elu', 'exponential', LeakyReLU,'relu']\n        \n    neurons = round(neurons)\n    activation = activationL[round(activation)]\n    optimizer = optimizerD[optimizerL[round(optimizer)]]\n    batch_size = round(batch_size)\n    epochs = round(epochs)\n    layers1 = round(layers1)\n    layers2 = round(layers2)\n        \n    def nn_cl_fun():\n        nn = Sequential()\n        nn.add(Dense(neurons, input_dim=10, activation=activation))\n        if normalization > 0.5:\n            nn.add(BatchNormalization())\n        for i in range(layers1):\n            nn.add(Dense(neurons, activation=activation))\n        if dropout > 0.5:\n            nn.add(Dropout(dropout_rate, seed=123))\n        for i in range(layers2):\n            nn.add(Dense(neurons, activation=activation))\n        nn.add(Dense(1, activation='sigmoid'))\n        nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        return nn\n        \n    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n    \n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n    \n    return score","a0588c60":"params_nn2 ={\n    'neurons': (10, 100),\n    'activation':(0, 9),\n    'optimizer':(0,7),\n    'learning_rate':(0.01, 1),\n    'batch_size':(200, 1000),\n    'epochs':(20, 100),\n    'layers1':(1,3),\n    'layers2':(1,3),\n    'normalization':(0,1),\n    'dropout':(0,1),\n    'dropout_rate':(0,0.3)\n}\n\n# Run Bayesian Optimization\nnn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\nnn_bo.maximize(init_points=25, n_iter=4)","eba83b41":"params_nn_ = nn_bo.max['params']\n\nlearning_rate = params_nn_['learning_rate']\nactivationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n               'elu', 'exponential', LeakyReLU,'relu']\nparams_nn_['activation'] = activationL[round(params_nn_['activation'])]\n\nparams_nn_['batch_size'] = round(params_nn_['batch_size'])\nparams_nn_['epochs'] = round(params_nn_['epochs'])\nparams_nn_['layers1'] = round(params_nn_['layers1'])\nparams_nn_['layers2'] = round(params_nn_['layers2'])\nparams_nn_['neurons'] = round(params_nn_['neurons'])\n\noptimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\noptimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n             'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n             'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n             'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\nparams_nn_['optimizer'] = optimizerD[optimizerL[round(params_nn_['optimizer'])]]\n\nparams_nn_","ca130f4d":"# Fitting Neural Network\ndef nn_cl_fun():\n    nn = Sequential()\n    nn.add(Dense(params_nn_['neurons'], input_dim=10, activation=params_nn_['activation']))\n    if params_nn_['normalization'] > 0.5:\n        nn.add(BatchNormalization())\n    for i in range(params_nn_['layers1']):\n        nn.add(Dense(params_nn_['neurons'], activation=params_nn_['activation']))\n    if params_nn_['dropout'] > 0.5:\n        nn.add(Dropout(params_nn_['dropout_rate'], seed=123))\n    for i in range(params_nn_['layers2']):\n        nn.add(Dense(params_nn_['neurons'], activation=params_nn_['activation']))\n    nn.add(Dense(1, activation='sigmoid'))\n    nn.compile(loss='binary_crossentropy', optimizer=params_nn_['optimizer'], metrics=['accuracy'])\n    return nn\n        \nes = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\nnn = KerasClassifier(build_fn=nn_cl_fun, epochs=params_nn_['epochs'], batch_size=params_nn_['batch_size'],\n                         verbose=0)\n \nnn.fit(X_train, y_train, validation_data=(X_val, y_val), verbose=1)","f3dc83c5":"# Tuning the Layers","0a8c5952":"# Tuning the Hyperparameters","55e8b033":"# Neural Network (Deep Learning)"}}