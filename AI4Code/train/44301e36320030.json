{"cell_type":{"46528f02":"code","a2431ec5":"code","f409bdf7":"code","e93a253d":"code","d3adfb59":"code","d9815b3b":"code","a0a63129":"code","0930b9e9":"code","45b6344c":"code","c31736f1":"code","212b81cd":"code","49ea8d4b":"code","7b33698c":"code","33f19921":"code","e7210ff9":"code","9e995886":"code","b8f0fb8f":"code","6fef823f":"code","8fbcee32":"code","23503d46":"code","f59274ea":"code","11fd21f7":"code","a2142545":"code","fd55af93":"code","fafec697":"code","8ab86570":"code","f6e260af":"code","e2d2a543":"code","86409384":"code","026ec16e":"code","d15b11d7":"code","67ab7765":"code","4637ac6b":"code","1346c41e":"code","2a480bec":"code","08229cd7":"code","f580fa69":"code","3ec2439b":"code","4f4b2d28":"code","d6c40bb6":"code","b7af93c8":"code","9e51ff87":"code","a1a1d382":"code","6d38bf47":"code","d2e5df35":"code","c0afd613":"markdown","e94d8bd0":"markdown","ced24384":"markdown","6461adf0":"markdown","3b8bf536":"markdown"},"source":{"46528f02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved _as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2431ec5":"# Loading Dataset\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","f409bdf7":"train_df.head()","e93a253d":"train_df.info()","d3adfb59":"# Handling missing value\ntrain_df.isnull().sum()","d9815b3b":"# Replace values in 'Age' column with mean value\ntrain_df['Age'].fillna(train_df['Age'].mean(), inplace=True)","a0a63129":"train_df['Embarked'].value_counts()","0930b9e9":"# Find the mode value of 'Embarked' column\n# The mode of a set of values is the value that appears most often. It can be multiple values.\ntrain_df['Embarked'].mode()","45b6344c":"# Replace the missing values in 'Embarked' column with mode\ntrain_df.fillna(train_df['Embarked'].mode()[0], inplace=True)","c31736f1":"# Drop the 'Cabin' column from the dataframe\ntrain_df = train_df.drop(columns='Cabin', axis=1)","212b81cd":"train_df.isnull().sum()","49ea8d4b":"train_df.describe()","7b33698c":"train_df['Survived'].value_counts()","33f19921":"import seaborn as sns","e7210ff9":"sns.set()\nsns.countplot(train_df['Survived'])","9e995886":"train_df['Sex'].value_counts()","b8f0fb8f":"sns.countplot('Sex', data=train_df)","6fef823f":"# Number of survivor gender wise\nsns.countplot('Sex', hue='Survived', data=train_df)","8fbcee32":"sns.countplot(train_df['Pclass'])","23503d46":"# Number of survior Pclass wise\nsns.countplot('Pclass', hue='Survived', data=train_df)","f59274ea":"train_df.info()","11fd21f7":"train_df.Sex.value_counts()","a2142545":"train_df.Embarked.value_counts()","fd55af93":"train_df.replace({\n    'Sex': {'male':0, 'female':1},\n    'Embarked': {'S': 0, 'C': 1, 'Q': 2 }\n}, inplace=True)","fafec697":"train_df.head()","8ab86570":"train_df.columns","f6e260af":"columns_list = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\nsns.heatmap(train_df[columns_list].corr(), annot=True, fmt='.2f')","e2d2a543":"X = train_df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Survived'], axis=1)\nY = train_df['Survived']","86409384":"print(X)\nprint(Y)","026ec16e":"# Split into training and testing\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=2)","d15b11d7":"# Feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","67ab7765":"print(x_train)","4637ac6b":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf.fit(x_train, y_train)","1346c41e":"y_pred = clf.predict(x_test)","2a480bec":"# Making Confusion metrix and calculating accuracy score\nacList = []\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacList.append(ac)\nprint(cm)\nprint(ac)","08229cd7":"# Finding the optimal number of neighbors\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor neighbors in range(3,10):\n    clf = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(3,10)), list1)\nplt.show()","f580fa69":"cls = KNeighborsClassifier(n_neighbors=3)\ncls.fit(x_train, y_train)","3ec2439b":"y_pred = cls.predict(x_test)","4f4b2d28":"cm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacList.append(ac)\nprint(cm)\nprint(ac)","d6c40bb6":"# Findeing the optimal number of n_estimators\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1=[]\nfor c in [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n    clf = SVC(C = c, random_state=0, kernel = 'rbf')\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    list1.append(accuracy_score(y_test, y_pred))\nplt.plot([0.5, 0.6, 0.7, 0.8, 0.9, 1.0], list1)","b7af93c8":"clf = SVC(C = 0.7, random_state=0, kernel = 'rbf')\nclf.fit(x_train, y_train)","9e51ff87":"y_pred = clf.predict(x_test)\nprint(y_pred)","a1a1d382":"cm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nacList.append(ac)","6d38bf47":"mylist = [\"Logistic Regression\", \"KNearestNeighbours\",\"SupportVector\"]","d2e5df35":"plt.rcParams['figure.figsize'] = 10, 6\nsns.set_style('darkgrid')\nax = sns.barplot(x=mylist, y=acList, palette='rocket', saturation=1.5)\nplt.xlabel('Classifier Models', fontsize=20)\nplt.ylabel('% of Accuracy', fontsize=20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","c0afd613":"Model Evaluation","e94d8bd0":"LOGISTIC REGRESSION","ced24384":"SUPPORT VECTOR CLASSIFIER","6461adf0":"Encoding categorical columns\n","3b8bf536":"K NEAREST NEIGHBOR\n"}}