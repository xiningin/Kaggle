{"cell_type":{"6c8af621":"code","5abd5895":"code","7a5ded28":"code","22fd81f0":"code","f0e9ed84":"code","33569da9":"code","918262ab":"code","e1b27148":"code","0fc9dd79":"code","cb5c0d83":"code","f8d619a8":"code","83654445":"code","286897ff":"code","4938aac9":"code","c7708d49":"code","99ea9a90":"code","8606f19f":"code","5f9c3b2f":"markdown","f40f4916":"markdown","a95ab409":"markdown","28059f45":"markdown","e2756bf9":"markdown","2b809a48":"markdown","06904db9":"markdown","7bed068d":"markdown","69b8cbe3":"markdown","6684dccf":"markdown","525418bd":"markdown","07402279":"markdown","6e19ba2b":"markdown","220770a4":"markdown","bd52cd42":"markdown","17b4412f":"markdown","6ffc6958":"markdown"},"source":{"6c8af621":"import sys\nimport scipy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pandas import read_csv\nimport datetime\nimport sklearn\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import SimpleRNN\nfrom tensorflow.keras import initializers","5abd5895":"url = \"..\/input\/apple-aapl-historical-stock-data\/HistoricalQuotes.csv\"\n#url = \"E:\\ENSIAS\\Courses\\IA\\TPs\\IMDB Dataset.csv\"\n#names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\ndataset = read_csv(url)","7a5ded28":"print(dataset.shape)\n# Head\nprint(dataset.head(dataset.size))\n# descriptions\nprint('description de la base')\nprint(dataset.describe())\nprint(dataset.dtypes)","22fd81f0":"for h in dataset.columns:\n    if h == \" Volume\" or h == \"Date\":\n        continue\n    for x in range(len(dataset[h])):\n        dataset[h][x] = dataset[h][x][2:]\n    dataset[h] = dataset[h].astype(float)\n","f0e9ed84":"for i in range(0,1259):\n    dataset.iloc[i,:],dataset.iloc[2517-i] = dataset.iloc[2517-i],dataset.iloc[i,:]","33569da9":"Dates = dataset[\"Date\"]\ndataset[\"Date\"] = pd.to_datetime(dataset[\"Date\"])\ndataset.set_index(keys='Date', inplace=True)","918262ab":"plt.figure(figsize=(15,15))\nplt.plot(dataset.index.date, dataset[' Close\/Last'])\nplt.xlabel(\"date\")\nplt.ylabel(\"Low\")\nplt.title(\"Closing price 03\/1\/2010 - 02\/28\/2020\")","e1b27148":"def search_date_index(datetime_object):\n        s = 0 \n        for i in range(0,len(dataset.index.date)):\n            if dataset.index.date[i] == datetime_object:\n                s = 1 \n                break\n        date_index = i\n        if s == 0 :\n             print(\"date not found !\")\n             \n             return None\n        \n        print(\"date index found :\\n\")\n        return i","0fc9dd79":"print(search_date_index(datetime.date(2018, 11, 30)))","cb5c0d83":"#search_date_index expects a datetime object formatted as follows : datetime.date(Year(from 2010 to 2020), month(from 1 to 12),day(from 1 to 31)) for instance datetime.date(2010, 3, 1), Note there are some date records that are note available in the dataset.\n# this function simply look from the index by which you will be splitting the data let (into training and testing) training data will be indexed from 0 to the returned value of this function, while testing will be indexed from the retuned value to 2517\ndate_index = search_date_index(datetime.date(2018,11,30))\nplt.figure(figsize=(15,15))\nplt.plot(dataset.index.date[0:date_index], dataset[' Close\/Last'][0:date_index])\nplt.xlabel(\"date\")\nplt.ylabel(\"Low\")\nplt.title(\"Lowest Price 03\/1\/2010 - 11\/30\/2018\")","f8d619a8":"x_train = dataset[[\" Volume\",\" Open\",\" High\",\" Low\"]]\ny_train = dataset[[\" Close\/Last\"]]  \nx_test = x_train[date_index:]\ny_test = y_train[date_index:]\nx_train = x_train[0:date_index]\ny_train = y_train[0:date_index]\n\nx_train = np.array(x_train)\nx_test = np.array(x_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n","83654445":"scaler = MinMaxScaler()\nprint(scaler.fit(x_train))\nx_train = scaler.transform(x_train)\nprint(scaler.fit(x_test))\nx_test = scaler.transform(x_test)\nprint(\"trainnig set scaled:\",x_train)\nprint(\"test set scaled:\",x_test)\n","286897ff":"processed_x_train = [None]*date_index\nfor i in range(0,date_index):\n    processed_x_train[i] = [[x_train[i][0]],[x_train[i][1]],[x_train[i][2]],[x_train[i][3]]]\nprocessed_x_test = [None]*(2518-date_index)\nfor i in range(0,2518-date_index):\n    processed_x_test[i] = [[x_test[i][0]],[x_test[i][1]],[x_test[i][2]],[x_test[i][3]]]    \nprocessed_x_test = np.array(processed_x_test)\nprocessed_x_train = np.array(processed_x_train)\nprint(processed_x_train.shape)\nprint(processed_x_test.shape)\n","4938aac9":"model = keras.Sequential()\ninitializer = tf.keras.initializers.RandomUniform(minval=0., maxval=100)\nmodel.add(layers.SimpleRNN(15,input_shape=(4,1),kernel_initializer=initializer))\nmodel.add(layers.Dense(1))\nprint(model.summary())\n\n#model.add(layers.RNN(cell))\n#model.summary()\n\n","c7708d49":"loss = tf.keras.losses.MeanSquaredError()\noptim = tf.keras.optimizers.SGD(learning_rate=1e-3)\n\n#metrics = [\"MeanSquaredError\"]\nbatch_size = date_index + 1 \nepochs = 500\nmodel.compile(loss=loss,optimizer=optim)\nhistory = model.fit(processed_x_train,y_train,batch_size=batch_size,epochs=epochs)\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\n\n","99ea9a90":"predictions = model.predict(processed_x_train,batch_size=date_index+1)\nplt.figure(figsize=(15,15))\nplt.plot(dataset.index.date[0:date_index], dataset[' Close\/Last'][0:date_index],label='Real values')\nplt.plot(dataset.index.date[0:date_index], predictions,label='Predicted values')\nplt.xlabel(\"date\")\nplt.ylabel(\"Closing price\")\nplt.title(\"Closing price 03\/1\/2010\"+\"   01\/6\/2018\")\nplt.legend()","8606f19f":"predictions = model.predict(processed_x_test)\nplt.figure(figsize=(15,15))\nplt.plot(dataset.index.date[date_index:], dataset[' Close\/Last'][date_index:],label='Real values')\nplt.plot(dataset.index.date[date_index:], predictions,label='Predicted values')\nplt.xlabel(\"date\")\nplt.ylabel(\"Closing price\")\nplt.title(\"Closing price 01\/6\/2018\"+ \"  02\/28\/2020\")\nplt.legend()","5f9c3b2f":"Next we normalize data according to the minimum and maximum value.","f40f4916":"# **3. Modeling**\nNext we build an neural network model made up of 3 layers (an input layer who gives (4,1) dimensional vector, a seconds layers who produces 15 ouputs each output results from an RNN cell, an output layer produces the final prediction) ","a95ab409":"Next we swap symmetric rows with the respect to the 1258 (the middle), we do this to have rows ordered correctly starting from index 0 (the oldest records) to 2517 (latest record)","28059f45":"We want the index corresponding to the date 01\/06\/2015","e2756bf9":"Then we import the data","2b809a48":"Next we split the data into training and testing","06904db9":"Finally, when it comes down to expose the model to unseen data, we can notice that it can barely predicts some trends in the data (futur increase or decrease), However the real values are far from reached. We can see that both curves are somewhat unscaled, both curves (real values and predicted values) follow approximately the same overall evolution. We could translate one over the other one by 150$ units (approximately). It think this problem is due to the fact that I scaled the training data and testing data seperately, this could lead to having some training data value similar to the testing ones, thus the model would assign the same range of training closing price values to the testing closing price values. Tell me what do you think down below.","7bed068d":"Visualization of the evolution of closing price, we notice periodicity in terms of how the closing price evolves over the years. The cycles of highs and lows repeats itself every four years\/ three years and half. The general trend is that the closing price goes up from cycle to cycle. The seasonality is that the closing price peaks every two years and half since 2010   ","69b8cbe3":"Let's now pick an index from the dataset in order to split the data, the training data indexes will range from 0 to date_index, while the testing data will have its range from date_index to 2517. If we know a date but don't know it's index, we call the function written above. ","6684dccf":"# **2. Data pre-processing**\nThe code written in the next cell will just convert the columns \"Close\/Last,\"Open\", \"High\", \"Low\" from strings into numerical typ. We had to eliminate those \"$\" signs.","525418bd":"First we import all the necessary packages","07402279":"Next we write a simple function that looks for the date index in the dataset, this would be useful later on when we will split the dataset into training and testing. In fact we will split the dataset using an index. If we only know a certain date by which we want to perfom the split, we can use the following function to get its index.","6e19ba2b":"Firstly,we will proceed with a couple preprocessing steps, the brut data shown below clearly needs data type transformations, we could also swap rows to order them in a way to follow a chronological order this is already the case with this dataset but it's ordered inversely so we could swap rows the other way in order to have 03\/01\/2010 at the index 0 ect.., this is particularly important because we are processing a time series, we must provide data that is chronogically ordered to the predictive algorithm. ","220770a4":"# **1. Introduction**\nThis notebook is intended for beginners to get started with using RNN cells with tensorflow. The main aim is to predict how will stock prices evolve at the end of the trading day, based on previous evolutions, and of course if we could predict the exact value of closing prices.","bd52cd42":"We notice that the model fits to some extent the training data, but significantly stagnate when it comes to correctly fitting the latest records. You comment down below about a potential reasons for this.","17b4412f":"Next we reshape the data into 3 dimensional tensors","6ffc6958":"We are now in the phase of training, we learn 271 parameter to be able predict correctly the trend in the data. We notice a smooth decrease of the loss function."}}