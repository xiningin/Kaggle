{"cell_type":{"3700cbc9":"code","006e98ce":"code","743feffe":"code","a0496875":"code","3a23deff":"code","a1edcf1b":"code","0e112987":"code","4a115451":"code","b15f9cfb":"code","20c194a3":"code","cfcbc99c":"code","5179ab10":"code","6a9d1c4c":"code","74524da8":"code","482f427b":"code","6736e08a":"code","3690fece":"code","b91d4310":"code","0070645b":"markdown","13f95b24":"markdown","3465e716":"markdown","71667d63":"markdown","4f1b3d66":"markdown","275dc58d":"markdown","c3a957d2":"markdown","0d51819b":"markdown","b2971c86":"markdown","bc7fa4d5":"markdown","449f33fc":"markdown","1a439137":"markdown"},"source":{"3700cbc9":"# source from https:\/\/www.kaggle.com\/infinitewing\/for-fun\nimport pandas as pd; import numpy as np\nkernel_df = pd.read_csv('..\/input\/for-fun\/kernel_df.csv')\nkernel_df['final_score'] = (kernel_df['kernel_plb_score'] \n                            + kernel_df['kernel_valid_oof_score']\n                            + kernel_df['kernel_valid_preds_score']) \/ 3\nkernel_df['plb_rank'] = kernel_df['kernel_plb_score'].rank(ascending=False)\nkernel_df['valid_oof_rank'] = kernel_df['kernel_valid_oof_score'].rank(ascending=False)\nkernel_df['valid_test_rank'] = kernel_df['kernel_valid_preds_score'].rank(ascending=False)\nkernel_df['final_rank'] = kernel_df['final_score'].rank(ascending=False)\nprint(kernel_df)\nkernel_df.to_csv('kernel_df.csv',index=False)","006e98ce":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import VarianceThreshold\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('..\/input\/instant-gratification\/train.csv')\ntest = pd.read_csv('..\/input\/instant-gratification\/test.csv')\n","743feffe":"cols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\nmagic_turtles = len(train['wheezy-copper-turtle-magic'].unique())\nprint('wheezy-copper-turtle-magic has {} unique values.'.format(magic_turtles))\n\nuseful_cols_count = np.zeros(magic_turtles)\nuseful_cols_count = useful_cols_count.astype('int32')\nfor i in range(magic_turtles):\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    train_data2 = VarianceThreshold(threshold=2).fit_transform(train2[cols])\n    train2 = train2[cols].values\n    \n    useful_cols_count[i] = train_data2.shape[1]\n    '''\n    # check if the useful cols in test and train are the same\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    test_data2 = VarianceThreshold(threshold=2).fit_transform(test2[cols])\n    test2 = test2[cols].values\n    \n    print(train2.shape[1])\n    print(train_data2.shape[1])\n    for _i in range(train_data2.shape[1]):\n        for _j in range(train2.shape[1]):\n            if(np.sum(train_data2[:,_i] - train2[:,_j]) == 0):\n                if(np.sum(test_data2[:,_i] - test2[:,_j]) == 0):\n                    print('train_data2[:,{}] == train2[:,{}]'.format(_i, _j))\n                break\n    '''","a0496875":"df = pd.DataFrame()\ndf['wheezy-copper-turtle-magic'] = np.array([i for i in range(magic_turtles)])\ndf['useful_cols_count'] = useful_cols_count\n#print(df)\nprint(df['useful_cols_count'].describe())\n    ","3a23deff":"df['total_train_rows'] = [0 for _ in range(magic_turtles)]\ndf['total_test_rows'] = [0 for _ in range(magic_turtles)]\nfor i in range(magic_turtles):\n    total_train_rows = train[train['wheezy-copper-turtle-magic'] == i]['wheezy-copper-turtle-magic'].count()\n    total_test_rows = test[test['wheezy-copper-turtle-magic'] == i]['wheezy-copper-turtle-magic'].count()\n    df.loc[df['wheezy-copper-turtle-magic'] == i, ['total_train_rows']] = total_train_rows\n    df.loc[df['wheezy-copper-turtle-magic'] == i, ['total_test_rows']] = total_test_rows\nprint(df.head(10))\nprint(df['total_train_rows'].sum() == train.shape[0])\nprint(df['total_test_rows'].sum() == test.shape[0])\nprint(df[['total_train_rows','total_test_rows']].sum())\n\n","a1edcf1b":"from sklearn.datasets import make_classification\n\ntrain_for_valid = False\ntest_for_valid = False\nfor i, row in df.iterrows():\n    if(i%50 == 0): print(i)\n    np.random.seed(520999+i)\n    useful_cols_count = row['useful_cols_count']\n    total_train_rows = row['total_train_rows']\n    total_test_rows = row['total_test_rows']\n    X, y = make_classification(n_samples=total_train_rows+total_test_rows, n_features=255, \\\n                               n_informative=useful_cols_count, n_redundant=0, \\\n                               n_clusters_per_class=3, \\\n                               random_state=3228+i, shuffle=True,  \\\n                               flip_y=0.05)\n    Xy = np.zeros((total_train_rows+total_test_rows, 257))\n    Xy[:,:-2] = X\n    Xy[:,-2] = i # represent 'wheezy-copper-turtle-magic'\n    Xy[:,-1] = y\n    if(train_for_valid is False):\n        train_for_valid = Xy[:total_train_rows,:]\n        test_for_valid = Xy[total_train_rows:,:]\n    else:\n        train_for_valid = np.concatenate((train_for_valid, Xy[:total_train_rows,:]))\n        test_for_valid = np.concatenate((test_for_valid, Xy[total_train_rows:,:]))\nprint(train_for_valid.shape)\nprint(test_for_valid.shape)","0e112987":"train_valid_df = pd.DataFrame(train_for_valid, columns=cols+['wheezy-copper-turtle-magic', 'target'])\ntest_valid_df = pd.DataFrame(test_for_valid, columns=cols+['wheezy-copper-turtle-magic', 'target'])\n\ntrain_valid_df['wheezy-copper-turtle-magic'] = train_valid_df['wheezy-copper-turtle-magic'].astype('int32')\ntrain_valid_df['target'] = train_valid_df['target'].astype('int32')\ntest_valid_df['wheezy-copper-turtle-magic'] = test_valid_df['wheezy-copper-turtle-magic'].astype('int32')\ntest_valid_df['target'] = test_valid_df['target'].astype('int32')\nprint(train_valid_df.head(2))\nprint(test_valid_df.head(2))","4a115451":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\ndef qda(train, test):\n    oof = np.zeros(len(train))\n    preds = np.zeros(len(test))\n    aucs = np.zeros(512)\n    for i in range(512):\n        #if(i%50 == 0): print(i)\n        train2 = train[train['wheezy-copper-turtle-magic']==i]\n        test2 = test[test['wheezy-copper-turtle-magic']==i]\n        idx1 = train2.index; idx2 = test2.index\n        train2.reset_index(drop=True,inplace=True)\n\n        data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n        data2 = VarianceThreshold(threshold=2).fit_transform(data[cols])\n\n        train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n\n        skf = StratifiedKFold(n_splits=11, random_state=42)\n        for train_index, test_index in skf.split(train2, train2['target']):\n\n            clf = QuadraticDiscriminantAnalysis(0.1)\n            clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n            oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n            preds[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n        aucs[i] = roc_auc_score(train['target'][idx1], oof[idx1])\n    auc = roc_auc_score(train['target'], oof)\n    print('QDA AUC: {}'.format(round(auc,5)))\n    aucs_df = pd.DataFrame(aucs,columns = ['auc'])\n    print(aucs_df.describe())\n    return oof, preds\ndef qda_pseudo(train, test):\n    oof = np.zeros(len(train))\n    preds = np.zeros(len(test))\n    aucs = np.zeros(512)\n\n    # BUILD 512 SEPARATE MODELS\n    for k in range(512):\n        # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n        train2 = train[train['wheezy-copper-turtle-magic']==k] \n        train2p = train2.copy(); idx1 = train2.index \n        test2 = test[test['wheezy-copper-turtle-magic']==k]\n\n        # ADD PSEUDO LABELED DATA\n        test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n        test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n        test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n        train2p = pd.concat([train2p,test2p],axis=0)\n        train2p.reset_index(drop=True,inplace=True)\n\n        # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n        sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n        train3p = sel.transform(train2p[cols])\n        train3 = sel.transform(train2[cols])\n        test3 = sel.transform(test2[cols])\n\n        # STRATIFIED K FOLD\n        skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n        for train_index, test_index in skf.split(train3p, train2p['target']):\n            test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n\n            # MODEL AND PREDICT WITH QDA\n            clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n            clf.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n            oof[idx1[test_index3]] = clf.predict_proba(train3[test_index3,:])[:,1]\n            preds[test2.index] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n        aucs[k] = roc_auc_score(train['target'][idx1], oof[idx1])\n        #if k%64==0: print(k)\n\n    # PRINT CV AUC\n    auc = roc_auc_score(train['target'],oof)\n    print('Pseudo Labeled QDA scores CV =',round(auc,5))\n    aucs_df = pd.DataFrame(aucs,columns = ['auc'])\n    print(aucs_df.describe())\n    return oof, preds\n","b15f9cfb":"oof, preds = qda(train, test)\ntest['target'] = preds\noof, preds = qda_pseudo(train, test)","20c194a3":"oof, preds = qda(train_valid_df, test_valid_df)\noriginal_target = test_valid_df['target'].values.copy()\ntest_valid_df['target'] = preds\noof, preds = qda_pseudo(train_valid_df, test_valid_df)\ntest_valid_df['target'] = original_target","cfcbc99c":"# The original LB score is 0.9659\nauc = roc_auc_score(test_valid_df['target'], preds)\nprint('AUC: {}'.format(round(auc,5)))","5179ab10":"\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression\n\ndef log(train, test):\n    oof = np.zeros(len(train))\n    preds = np.zeros(len(test))\n    aucs = np.zeros(512)\n    for i in range(512):\n        train2 = train[train['wheezy-copper-turtle-magic']==i]\n        test2 = test[test['wheezy-copper-turtle-magic']==i]\n        idx1 = train2.index; idx2 = test2.index\n        train2.reset_index(drop=True,inplace=True)\n\n        # Adding quadratic polynomial features can help linear model such as Logistic Regression learn better\n        poly = PolynomialFeatures(degree=2)\n        sc = StandardScaler()\n        data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n        data2 = poly.fit_transform(sc.fit_transform(VarianceThreshold(threshold=2).fit_transform(data[cols])))\n        train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n\n        # STRATIFIED K FOLD\n        skf = StratifiedKFold(n_splits=11, random_state=42)\n        for train_index, test_index in skf.split(train2, train2['target']):\n\n            clf = LogisticRegression(solver='liblinear',penalty='l2',C=0.001,tol=0.0001,random_state=0,max_iter=1000,n_jobs=-1)\n            clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n            oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n            preds[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n        aucs[i] = roc_auc_score(train['target'][idx1], oof[idx1])\n    auc = roc_auc_score(train['target'], oof)\n    print('LOG AUC: {}'.format(round(auc,5)))\n    aucs_df = pd.DataFrame(aucs,columns = ['auc'])\n    print(aucs_df.describe())\n    return oof, preds","6a9d1c4c":"oof, preds = log(train, test)\noof, preds = log(train_valid_df, test_valid_df)\n\nauc = roc_auc_score(test_valid_df['target'], preds)\nprint('AUC: {}'.format(round(auc,5)))","74524da8":"from sklearn import svm, neighbors, linear_model, neural_network\nfrom sklearn.svm import NuSVC\nfrom sklearn.decomposition import PCA\n\ndef nusvc(train, test):\n    oof = np.zeros(len(train))\n    preds = np.zeros(len(test))\n    aucs = np.zeros(512)\n    cols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\n    for i in range(512):\n        train2 = train[train['wheezy-copper-turtle-magic']==i]\n        test2 = test[test['wheezy-copper-turtle-magic']==i]\n        idx1 = train2.index; idx2 = test2.index\n        train2.reset_index(drop=True,inplace=True)\n\n        data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n        data2 = StandardScaler().fit_transform(PCA(n_components=40, random_state=4).fit_transform(data[cols]))\n        train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n\n        # STRATIFIED K FOLD (Using splits=25 scores 0.002 better but is slower)\n        skf = StratifiedKFold(n_splits=5, random_state=42)\n        for train_index, test_index in skf.split(train2, train2['target']):\n\n            clf = NuSVC(probability=True, kernel='poly', degree=4, gamma='auto', random_state=4, nu=0.59, coef0=0.053)\n            clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n            oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n            preds[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n        aucs[i] = roc_auc_score(train['target'][idx1], oof[idx1])\n    auc = roc_auc_score(train['target'], oof)\n    print('NUSVC AUC: {}'.format(round(auc,5)))\n    aucs_df = pd.DataFrame(aucs,columns = ['auc'])\n    print(aucs_df.describe())\n    return oof, preds","482f427b":"oof, preds = nusvc(train, test)\noof, preds = nusvc(train_valid_df, test_valid_df)\n\nauc = roc_auc_score(test_valid_df['target'], preds)\nprint('AUC: {}'.format(round(auc,5)))","6736e08a":"from sklearn.covariance import EmpiricalCovariance\nfrom sklearn.covariance import GraphicalLasso\nfrom sklearn.mixture import GaussianMixture\ndef get_mean_cov(x,y):\n    model = GraphicalLasso()\n    ones = (y==1).astype(bool)\n    x2 = x[ones]\n    model.fit(x2)\n    p1 = model.precision_\n    m1 = model.location_\n    \n    onesb = (y==0).astype(bool)\n    x2b = x[onesb]\n    model.fit(x2b)\n    p2 = model.precision_\n    m2 = model.location_\n    \n    ms = np.stack([m1,m2])\n    ps = np.stack([p1,p2])\n    return ms,ps\n\ndef gmm(train, test):\n    oof = np.zeros(len(train))\n    preds = np.zeros(len(test))\n    aucs = np.zeros(512)\n    cols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n    # BUILD 512 SEPARATE MODELS\n    for i in (range(512)):\n        # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n        train2 = train[train['wheezy-copper-turtle-magic']==i]\n        test2 = test[test['wheezy-copper-turtle-magic']==i]\n        idx1 = train2.index; idx2 = test2.index\n        train2.reset_index(drop=True,inplace=True)\n\n        # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n        sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n        train3 = sel.transform(train2[cols])\n        test3 = sel.transform(test2[cols])\n\n        # STRATIFIED K-FOLD\n        skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n        for train_index, test_index in skf.split(train3, train2['target']):\n\n            # MODEL AND PREDICT WITH QDA\n            ms, ps = get_mean_cov(train3[train_index,:],train2.loc[train_index]['target'].values)\n            gm = GaussianMixture(n_components=2, init_params='random', covariance_type='full', tol=0.001,reg_covar=0.001, max_iter=100, n_init=1,means_init=ms, precisions_init=ps)\n            gm.fit(np.concatenate([train3[train_index,:],test3],axis = 0))\n            oof[idx1[test_index]] = gm.predict_proba(train3[test_index,:])[:,0]\n            preds[idx2] += gm.predict_proba(test3)[:,0] \/ skf.n_splits\n        aucs[i] = roc_auc_score(train['target'][idx1], oof[idx1])\n    auc = roc_auc_score(train['target'], oof)\n    print('GMM AUC: {}'.format(round(auc,5)))\n    aucs_df = pd.DataFrame(aucs,columns = ['auc'])\n    print(aucs_df.describe())\n    return oof, preds","3690fece":"oof, preds = gmm(train, test)\noof, preds = gmm(train_valid_df, test_valid_df)\n\nauc = roc_auc_score(test_valid_df['target'], preds)\nprint('AUC: {}'.format(round(auc,5)))","b91d4310":"train_valid_df.to_csv('train_valid_df.csv', index=False)\ntest_valid_df.to_csv('test_valid_df.csv', index=False)","0070645b":"#### Previous work\nAs we known from some past posts, the train-test data are generated by sklearn.datasets.make_classification()\nLet's see the parameters information from official doc from scikit-learn.\n\n> sklearn.datasets.make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)[source]\n\nFor these parameters, we can know that the overall flip_y would be 0.05, n_informative is between 33 to 47 (see below), n_redundant is 0, n_repeated is 0.","13f95b24":"### This notebook is aimed to create an useable offline train-test files for validation purpose by using make_classification with appropriate parameters.\n\n* 2019-06-20 v16 update, add high-scores publick kernel validation result. The code will release after competition end.\n* 2019-06-18 v14 update, add comparison of GMM-EM model\n* 2019-06-17 v13 update, add comparison of PCA+NuSVC model\n* 2019-06-16 v12 update, fixed some problem\n* 2019-06-16 v8 update, n_clusters_per_class=2\n* 2019-06-16 v7 update, check pseudo labeling works or not\n* 2019-06-15 v6 update, check redundant will make the prediction different or not, results shows that n_redundant probably should be 0\n\nAs the result, the QDA auc scores (original data V.S. our data that created by make_classification) looks almost the same (dif < 0.0005), the std is nearly equal, and the 25%, 50%, 75% auc score is also very close. So I think it's good to use it to double-check your method is overfitting the LB or not.\n\n*(But be aware the parameters are tuned by myself, so you may change the parameter settings to have a more accurate train-test files.)*\n\nThe following is this kernels outline.\n\n+ High-scores publick kernel validation result (new)\n+ Previous work\n+ Check the n_informative range\n+ Create train-test data by ourself\n+ Use QDA model to check the performance is equal to current CV\/Public LB or not\n","3465e716":"Now we can knows that n_informative is probably between 33 to 47, it's a great start!","71667d63":"#### PCA + NuSVC\n*Code from: https:\/\/www.kaggle.com\/tunguz\/pca-nusvc-knn\/code*","4f1b3d66":"Now we can test our model\/algorithm with the file we create!\nWish you can find some insights or idea from this kernel.","275dc58d":"#### High-scores publick kernel validation result\n","c3a957d2":"#### Check the n_informative range\nLet's start some data analyze","0d51819b":"#### Use QDA model to check the performance is equal to current CV\/Public LB or not\nNow we use QDA model to test is our created data can has almost same result against original data or not. \n*The QDA model is based on this kernel:\nhttps:\/\/www.kaggle.com\/speedwagon\/quadratic-discriminant-analysis","b2971c86":"#### GMM\n*Code from: https:\/\/www.kaggle.com\/christofhenkel\/graphicallasso-gaussianmixture*","bc7fa4d5":"As we can see, the auc scores looks almost the same (dif < 0.0005), the std is nearly equal, and the 25%, 50%, 75% auc score is also very close. \nLet's test some more popular model before we save file to local.\n#### PolynomialFeatures + LogisticRegression\n*Code from: https:\/\/www.kaggle.com\/gogo827jz\/pseudo-labelled-polylr-and-qda*","449f33fc":"Now we know how many rows need to be create in each subset! Let's start create our offline train-test datas. \n<font color=\"#a11\">Noted that I test the parameter setting to make the QDA model has the (almost) same cv and test result. (compared to QDA model that use original train-test data). So you may change the parameter setting on you own.<\/font>","1a439137":"#### Create train-test data by ourself\nBefore we start creat train-test data by ourself, we need to know how many rows need to be create in each subset(i.e. groupby(n_informative))."}}