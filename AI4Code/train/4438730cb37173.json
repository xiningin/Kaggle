{"cell_type":{"96facd03":"code","90f42c8e":"code","6d1f6a71":"code","2fb1df57":"code","70047fa8":"code","04dae57c":"code","f167d6a4":"code","d99ae060":"code","81f94e2e":"code","62df3bc6":"code","30a3bac4":"code","29d099c8":"code","9f43916f":"code","88ff02ca":"code","ca181a8c":"code","7511cb75":"code","e4d1a8ec":"code","6e750411":"code","ffa1fa08":"code","1c1b4e86":"code","2919418e":"code","2582fc4c":"code","a1be1a5d":"code","a11c687c":"code","d004f726":"code","e3493e56":"code","9c1b94cd":"code","5b90b530":"code","df1d105d":"code","4c036e48":"code","160ae59c":"code","02f1b2c2":"code","de68f2c8":"code","791dc37b":"code","1ac49d59":"code","05688f6a":"code","a7b31b1a":"code","6ee486f2":"code","3a225dbc":"code","7f6e88a8":"code","4c29d927":"code","c78f2831":"code","691b2021":"code","604b10dc":"code","4c212437":"code","aa5375d1":"code","f2c53e05":"code","ffc9c43f":"code","c894f780":"code","4bccb365":"code","070751de":"code","01a609c8":"code","1a45bbfb":"code","a60ea8d8":"code","031a8c33":"code","64352de5":"code","27ad5fb5":"code","8d7bc029":"code","401342ec":"code","95650994":"code","0c416172":"code","c7e96c0e":"code","ab8422be":"code","8c5e5844":"code","24e8f53a":"code","3ca9fa8c":"markdown","114d322d":"markdown","e8ff165b":"markdown","327d3889":"markdown","58beb6e0":"markdown","9966239a":"markdown","e61770f7":"markdown","4e238836":"markdown","23b16b76":"markdown","68f1bf45":"markdown","e655dbf8":"markdown","12c3a390":"markdown","c062160e":"markdown","fbbe3e21":"markdown","5926ddc7":"markdown","b70fbb33":"markdown","06e46835":"markdown","1c1ec8b6":"markdown","cf0ce32c":"markdown","1d474a4e":"markdown","68d72dda":"markdown","02656836":"markdown","ff0d2ecb":"markdown","3ca46fb2":"markdown","dae38944":"markdown","52b32c5d":"markdown","e1987629":"markdown","71261e67":"markdown","34cd456b":"markdown","dd6de10a":"markdown","e0fd98d9":"markdown","9ceea16e":"markdown","5defb52d":"markdown","46b67452":"markdown","5d0dda48":"markdown","7fa65e5d":"markdown","6c2461c4":"markdown","9822e30e":"markdown","52d73277":"markdown","3387b7b4":"markdown","2dd26010":"markdown","74d7dddc":"markdown","cecc877f":"markdown","b1bb449e":"markdown","24e79a32":"markdown","3c25c391":"markdown","68ed6c2c":"markdown","194b0263":"markdown","707fd0bf":"markdown","67632a84":"markdown","b8110cf1":"markdown","a71fa0df":"markdown","74f49897":"markdown","a8520428":"markdown","94d7bd43":"markdown","b37f056b":"markdown","7cc6fed3":"markdown","cc2e2462":"markdown","205c0831":"markdown","c0f94182":"markdown","af832a71":"markdown"},"source":{"96facd03":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","90f42c8e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sb\nimport matplotlib.pyplot as pl\n\n%matplotlib inline\n\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","6d1f6a71":"train.head()","2fb1df57":"test.head()","70047fa8":"train.shape","04dae57c":"test.shape","f167d6a4":"train.info()","d99ae060":"test.info()","81f94e2e":"train.isnull().sum()","62df3bc6":"test.isnull().sum()","30a3bac4":"def grafico(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar', stacked=True, figsize=(10,5))","29d099c8":"grafico('Sex')","9f43916f":"grafico('Pclass')","88ff02ca":"grafico('SibSp')","ca181a8c":"grafico('Parch')","7511cb75":"grafico('Embarked')","e4d1a8ec":"train_test = [train, test]\nfor dataset in train_test:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","6e750411":"train['Title'].value_counts()","ffa1fa08":"test['Title'].value_counts()","1c1b4e86":"title_map = {\"Mr\": 0,\n            \"Miss\": 1,\n            \"Mrs\": 2,\n            \"Master\": 3,\n            \"Dr\": 3,\n            \"Rev\": 3,\n            \"Col\": 3,\n            \"Major\": 3,\n            \"Mlle\": 3,\n            \"Ms\": 3,\n            \"Don\": 3,\n            \"Lady\": 3,\n            \"Jonkheer\": 3,\n            \"Countess\": 3,\n            \"Mme\": 3,\n            \"Sir\": 3,\n            \"Capt\": 3}\nfor dataset in train_test:\n    dataset['Title'] = dataset['Title'].map(title_map)","2919418e":"test[\"Title\"].fillna(0, inplace=True)","2582fc4c":"train.isnull().sum()","a1be1a5d":"train.head()","a11c687c":"grafico('Title')","d004f726":"train.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","e3493e56":"test.head()","9c1b94cd":"train.head()","5b90b530":"sex_map = {\"male\": 0, \"female\": 1}\nfor dataset in train_test:\n    dataset['Sex'] = dataset['Sex'].map(sex_map)","df1d105d":"grafico('Sex')","4c036e48":"train.head(100)","160ae59c":"train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntest[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)","02f1b2c2":"facet = sb.FacetGrid(train, hue=\"Survived\", aspect=4)\nfacet.map(sb.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\npl.show()","de68f2c8":"for dataset in train_test:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] =0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 62), 'Age'] = 4","791dc37b":"grafico('Age')","1ac49d59":"Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1,Pclass2,Pclass3])\ndf.index = ['1st class', '2nd class', '3rd class']\ndf.plot(kind='bar', stacked=True, figsize=(10,5))","05688f6a":"for dataset in train_test:\n    dataset['Embarked'] =  dataset['Embarked'].fillna('S')","a7b31b1a":"train.head()","6ee486f2":"emb_map = {\"S\": 0,\n           \"C\": 1,\n           \"Q\": 2}\nfor dataset in train_test:\n    dataset['Embarked'] = dataset['Embarked'].map(emb_map)","3a225dbc":"train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntest[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)","7f6e88a8":"for dataset in train_test:\n    dataset.loc[ dataset['Fare'] <= 17, 'Fare'] =0\n    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2\n    dataset.loc[(dataset['Fare'] > 100), 'Fare'] = 3","4c29d927":"train.Cabin.value_counts()","c78f2831":"for dataset in train_test:\n    dataset['Cabin'] = dataset['Cabin'].str[:1]","691b2021":"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\nPclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\nPclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\ndf = pd.DataFrame([Pclass1,Pclass2,Pclass3])\ndf.index = ['1st class', '2nd class', '3rd class']\ndf.plot(kind='bar', stacked=True, figsize=(10,5))","604b10dc":"cab_map = {\"A\": 0,\n           \"B\": 0.4,\n           \"C\": 0.8,\n           \"D\": 1.2,\n           \"E\": 1.6,\n           \"F\": 2.0,\n           \"G\": 2.4,\n           \"T\": 2.8}\nfor dataset in train_test:\n    dataset['Cabin'] = dataset['Cabin'].map(cab_map)","4c212437":"train[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\ntest[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)","aa5375d1":"train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"]  + 1\ntest[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"]  + 1","f2c53e05":"facet = sb.FacetGrid(train, hue=\"Survived\", aspect=4)\nfacet.map(sb.kdeplot, 'FamilySize', shade= True)\nfacet.set(xlim=(0, train['FamilySize'].max()))\nfacet.add_legend()","ffc9c43f":"family_map = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\nfor dataset in train_test:\n    dataset['FamilySize'] = dataset['FamilySize'].map(family_map)","c894f780":"train.head()","4bccb365":"test.head()","070751de":"features_drop = ['Ticket', 'SibSp', 'Parch']\ntrain = train.drop(features_drop, axis=1)\ntest = test.drop(features_drop, axis=1)\ntrain = train.drop(['PassengerId'], axis=1)\ntrain_data = train.drop('Survived', axis=1)\ntarget = train['Survived']\n\ntrain_data.shape, target.shape","01a609c8":"train_data.head(10)","1a45bbfb":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn import linear_model\nfrom sklearn.svm import SVC\n\nimport numpy as np","a60ea8d8":"train.info()","031a8c33":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","64352de5":"clf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100, 2)","27ad5fb5":"clf = DecisionTreeClassifier()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100, 2)","8d7bc029":"clf = RandomForestClassifier(n_estimators=13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100, 2)","401342ec":"clf = GaussianNB()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100, 2)","95650994":"clf = SVC()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100,2)","0c416172":"clf = QuadraticDiscriminantAnalysis()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100,2)","c7e96c0e":"clf = linear_model.LinearRegression()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1)\nprint(score)\nround(np.mean(score)*100,2)","ab8422be":"clf = SVC()\n\nclf.fit(train_data, target)\n\ntest_data = test.drop(\"PassengerId\", axis=1).copy()\n\nprediction = clf.predict(test_data)\n\ntest_data2 = pd.read_csv('\/kaggle\/input\/testes\/teste.csv')\nprediction2 = clf.predict(test_data2)\n\nprint(prediction2)","8c5e5844":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": prediction\n    })\n\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index=False)","24e8f53a":"submission = pd.read_csv('\/kaggle\/working\/submission.csv')\nsubmission.head()","3ca9fa8c":"> ### A seguir, substitu\u00edmos os valores 'male' e 'female' da *feature* 'Sex' por 0 e 1 respectivamente, para criarmos o gr\u00e1fico de sobreviventes entre homens e mulheres:","114d322d":"> ### Importando classificadores:","e8ff165b":"> ### A seguir, \u00e9 preenchido os valores vazios das tarifas com a m\u00e9dia de cada conjunto:","327d3889":"> ###  Transformando os t\u00edtulos em representa\u00e7\u00e3o num\u00e9rica:","58beb6e0":"# 3. DATA PREPARATION\n\n> * Criar classes baseados nos pronomes de tratamento;\n> * Preencher os valores vazios das idades, baseados nos nas m\u00e9dias de idade dos pronomes de tratamento;\n> * Substituir valores da *feature* 'Sex' para valores num\u00e9ricos;\n> * Criar classes para as idades.","9966239a":"# 6. DEPLOYMENT\n\n> ### Criamos um pequeno *dataset* com os dados dos integrantes do grupo do projeto para o algoritmo ser executado e testarmos o seu funcionamento.\n","e61770f7":"> * Gr\u00e1fico baseado no sexo:","4e238836":"> ## Naive Bayes","23b16b76":"> ### Abaixo mostra os valores atribu\u00eddos para a *feature* 'Title' do conjunto de treino:","68f1bf45":">## Support Vector Machine (SVM):","e655dbf8":"> * Gr\u00e1fico baseado na classe de embarque:","12c3a390":"> *  Contagem dos t\u00edtulos presentes no conjunto de treino:","c062160e":"> ### E para finalizar a prepara\u00e7\u00e3o de dados do *dataset*, foi definido que:\n> * As *features* 'SibSP' e 'Parch' v\u00e3o ser *dropadas*, pois  n\u00e3o s\u00e3o mais necess\u00e1rias;\n> * As *features* 'Ticket' e 'PassengerId' n\u00e3o ser\u00e3o importantes para nossa an\u00e1lise;\n> * E no conjunto de treino a *feature* 'Survived' vai ser eliminada.","fbbe3e21":"> ## Regress\u00e3o Linear:","5926ddc7":"> * Gr\u00e1fico baseado no n\u00famero de irm\u00e3os\/c\u00f4njuges a bordo:","b70fbb33":"> ## \u00c1rvore de Decis\u00e3o:","06e46835":"## 1.2. Objetivo do Projeto\n\n>### Construir um algoritmo de *Machine Learning* para prever o \u00edndice de sobreviv\u00eancia dos passageiros do *Titanic*, que tenha pelo menos 80% de acur\u00e1cia, baseadas nas *features* dos *datasets* disponibilizados no desafio do *[Kaggle](https:\/\/www.kaggle.com\/c\/titanic)*.","1c1ec8b6":"> * Gr\u00e1fico baseado em qual porto o navio embarcou:","cf0ce32c":"> * Abaixo mostra as informa\u00e7\u00f5es de cada *features* do conjunto de teste:","1d474a4e":"> ### Nesta etapa, utiliza-se a *feature* 'Name' para criar a *feature* 'Title', que nada mais \u00e9 do que a extra\u00e7\u00e3o do t\u00edtulo dos nomes da pessoas,isto \u00e9, os pronomes de tratamento, por exemplo: Mr., Miss., etc.:\n","68d72dda":"> * Abaixo apresenta a quantidade de dados nulos do conjunto de teste:\n","02656836":"> ### \u00c9 feito um *mapping* das cabines substituindo elas por valores n\u00famericos que variam de 0.4 em 0.4:","ff0d2ecb":"> * Abaixo mostra a dimens\u00e3o do conjunto de dados de treino:\n>> - 891 registros (linhas);\n>> - 12  *features* (colunas).\n","3ca46fb2":"> ## QDA:","dae38944":"> ### Foram definidos alguns classificadores que ser\u00e3o utilizados para analisarmos qual ser\u00e1 o melhor m\u00e9todo a ser aplicado no algoritmo, baseado no resultado da apura\u00e7\u00e3o *SCORE*:\n> * Cross Validation com K-fold;\n> * KNN;\n> * \u00c1rvore de Decis\u00e3o; \n> * Florestas Aleat\u00f3rias (Random Forest);\n> * Naive Bayes;\n> * Support Vector Machine (SVM);\n> * QDA; \n> * Regress\u00e3o Linear.","52b32c5d":"# 1. BUSINESS UNDERSTANDING\n\n## 1.1. Problem\u00e1tica\n>### Os motivos que contribu\u00edram para o naufr\u00e1gio foram: fatores naturais, como o clima; e causas humanas, como neglig\u00eancia, pois n\u00e3o haviam botes salva-vidas suficientes para os passageiros e tripulantes e muitos dos botes salva-vidas n\u00e3o estavam com a sua capacidade m\u00e1xima de pessoas a bordo, e se estivessem seria poss\u00edvel salvar 53% dos passageiros, mas apenas 32% deles sobreviveram.\n>### Embora aqueles que escaparam com vida tiveram sua sorte, alguns grupos de pessoas eram mais propensos a escaparem da morte do que outros. A maioria do sobreviventes eram mulheres, crian\u00e7as e passageiros da 1\u00aa Classe, deixando evidente que existe algum padr\u00e3o que pode ser extra\u00eddo dos dados brutos, que ser\u00e1 apresentado ao longo do projeto.\n\n>### Ent\u00e3o, eis que surge a quest\u00e3o: Quais eram as caracter\u00edsticas das pessoas que sobreviveram ao desastre? Haveria um padr\u00e3o entre as caracter\u00edsticas dos sobreviventes?  Por que certas pessoas sobreviveram e outras n\u00e3o?\n<p align=\"center\">\n  <img width=\"550\" height=\"250\" src=\"https:\/\/digitalks.com.br\/wp-content\/uploads\/2018\/08\/blockchain-marketing-digital.png\">\n<\/p>\n","e1987629":"## 1.3. Metodologia\n\n> ### Ser\u00e1 utilizada o CRISP-DM (*Cross Industry Standard Process for Data Mining*), \u00e9 uma metodologia de processo de minera\u00e7\u00e3o de dados, capaz de transformar os dados em conhecimento e informa\u00e7\u00f5es para estrat\u00e9gias de neg\u00f3cio.\n> ## Classificador de dados utilizado no *Machine Learning*:\n> * Cross Validation com K-fold;\n> * KNN;\n> * \u00c1rvore de Decis\u00e3o; \n> * Florestas Aleat\u00f3rias (Random Forest);\n> * Naive Bayes;\n> * Support Vector Machine (SVM);\n> * QDA; \n> * Regress\u00e3o Linear.\n\n> ## Tecnologias utilizadas:\n\n> ### Ambiente de desenvolvimento:\n>> * Jupyter Notebook - Servidor do Kaggle\n\n> ### Linguagem de programa\u00e7\u00e3o:\n>> *  Python\n\n> ### Bibliotecas:\n>> *  Pandas\n>> *  Numpy\n>> *  Seaborn\n>> *  Matplotlib\n>> *  SciKit Learn\n\n> ### Formato dos *datasets*:\n>> *  .csv (valores separados por v\u00edrgulas)","71261e67":"# 2. DATA UNDERSTANDING\n> * Importar as bibliotecas utilizadas;\n> * Importar os *datasets* utilizando a biblioteca Pandas;\n> * Analisar os *datasets*.","34cd456b":"> ### Arquivos disponibilizados pelo *[Kaggle](https:\/\/www.kaggle.com\/c\/titanic)*:\n","dd6de10a":"> ### Foi criada uma faixa de valores para a tarifa:\n| Faixa valores| Classe |\n|--------------|--------|\n| 0 - 17       | 0      |\n| 18 - 30      | 1      |\n| 31 - 100     | 2      |\n| > 100        | 3      |","e0fd98d9":"> * O resultado apresentado acima, mostra que dos 5 integrantes do grupo, baseado nas *features*, apenas 1 integrante sobreviveria.","9ceea16e":" # PROJETO TITANIC","5defb52d":"> * **Conjunto de teste**\n>> #### A fun\u00e7\u00e3o abaixo, nos mostra as 5 primeiras linhas do *Test Dataset*.","46b67452":"> ### Aqui come\u00e7a a an\u00e1lise relacionado ao tamanho da fam\u00edlia dos passeiros:","5d0dda48":"> * Contagem de cada titulo presente no conjunto de teste:","7fa65e5d":"# 5. EVALUATION\n> * Ser\u00e1 verificado se os resultados foram atingidos baseados no objetivo definido do projeto: **atingir pelo menos 80% de acur\u00e1cia.**","6c2461c4":"> ### Nesta etapa, um gr\u00e1fico \u00e9 plotado para analisar os lugares de embarque das pessoas:","9822e30e":"> * Gr\u00e1fico baseado no n\u00famero de pais\/filhos a bordo:","52d73277":"> ### Ent\u00e3o, criamos classes e classificamos as idades por faixas et\u00e1rias:\n| Faixa et\u00e1ria | Classe |\n|--------------|--------|\n| 0 - 16       | 0      |\n| 17 - 26      | 1      |\n| 27 - 36      | 2      |\n| 37 - 62      | 3      |\n| Maior que 62 | 4      |\n \n","3387b7b4":"> ## KNN:","2dd26010":"> ### Aqui tamb\u00e9m \u00e9 feito um mapping do tamanho das fam\u00edlias, substituindo elas por valores n\u00famericos que variam de 0.4 em 0.4:","74d7dddc":"<p align=\"center\">\n  <img width=\"550\" height=\"250\" src=\"https:\/\/canalhistoria.pt\/wp-content\/uploads\/2016\/05\/1.Portada.jpg\">\n<\/p>\n\n### O naufr\u00e1gio do RMS Titanic \u00e9 uma das trag\u00e9dias mais famosas da hist\u00f3ria, originando diversos livros, filmes e afins. \u00c9 v\u00e1lido lembrar que a hist\u00f3ria narrada de forma c\u00e9lebre por James Cameron em seu filme de 1997, ilustra perfeitamente o motivo deste desafio. Vamos come\u00e7ar com uma breve perspectiva sobre o tema: O Titanic foi um navio de passageiros constru\u00eddo nos estaleiros da Harland and Wolff durante o per\u00edodo de mar\u00e7o de 1909 a maio de 1911 em Belfast no Reino Unido. Naquela \u00e9poca, a constru\u00e7\u00e3o do Titanic levou cerca de 2 anos e custou 7,5 milh\u00f5es de d\u00f3lares. A embarca\u00e7\u00e3o partiu em sua viagem inaugural de Southampton para Nova Iorque em 10 de abril de 1912, com passagem em Cherbourg-Octeville na Fran\u00e7a e em Queenstown na Irlanda. Devido a sua excelente projeta\u00e7\u00e3o, gerou boatos de que a embarca\u00e7\u00e3o seria \"inafund\u00e1vel\", por\u00e9m, \u00e0s 23h40min do dia 14 de abril, a embarca\u00e7\u00e3o se chocou contra um iceberg. Em 15 de abril de 1912, o Titanic afundou matando 1.502 dos 2.224 passageiros e tripulantes, ou seja, apenas 32% desses passageiros sobreviveram ao naufr\u00e1gio, tornando assim o maior desastre mar\u00edtimo em tempos de paz da hist\u00f3ria.\n\n","cecc877f":"> ### Analisando o *dataset*:\n\n> * **Conjunto de treinamento**\n>> #### A fun\u00e7\u00e3o abaixo, nos mostra as 5 primeiras linhas do *Train Dataset*.","b1bb449e":"> ### Aqui foi feito um *substring* utilizando apenas a primeira posi\u00e7\u00e3o da cabine, ou seja, apenas as letras:","24e79a32":"> ### Vamos retirar a *feature* 'Name' de ambos os conjuntos:","3c25c391":"> * Mais do que 50% de cada classe veio do porto 'S', ent\u00e3o est\u00e1 sendo preenchido os vazios com 'S'.","68ed6c2c":"> ### Importa\u00e7\u00e3o de bibliotecas e defini\u00e7\u00e3o de v\u00e1riaveis de **treino** e de **teste** com a biblioteca *pandas*, utilizando os caminhos dos arquivos mostrados no c\u00f3digo anterior.","194b0263":"> * O gr\u00e1fico abaixo mostra a quantidade de passageiros em determinadas se\u00e7\u00f5es de cabines baseada nas suas classes:","707fd0bf":"> ### Os *datasets* possuem uma quantidade de dados nulos (sem valor) em algumas *features*.\n> *  Abaixo apresenta a quantidade de dados nulos do conjunto de treino:\n","67632a84":"# 4. MODELING","b8110cf1":"> ### A seguir, foram escolhidos alguns gr\u00e1ficos a serem plotados para a visualiza\u00e7\u00e3o de como as *features* se comportam:","a71fa0df":"> * Abaixo mostra as informa\u00e7\u00f5es de cada *features* do conjunto de treino:","74f49897":"> * Assim criando apenas 5 classes, vamos obter o resultado apenas das 5 classes.","a8520428":"> ### Nesta etapa, \u00e9 feito um mapping dos lugares de embarque para n\u00fameros, transformando as letras em representa\u00e7\u00e3o n\u00famerica, isto \u00e9, atribu\u00edmos aos valores da feature 'Embarked' os numeros 0, 1 e 2:","94d7bd43":"> ### Por fim, executamos o algoritmo no conjunto de teste do projeto para prever se cada um dos passeiros do Titanic sobreviveria \u00e0 trag\u00e9dia ou n\u00e3o, baseado na an\u00e1lise, defini\u00e7\u00e3o das t\u00e9cnicas e *datasets* disponibilizados pelo desafio do [Kaggle](https:\/\/www.kaggle.com\/c\/titanic).\n> ### O classificador escolhido para fazer a predi\u00e7\u00e3o do conjunto de dados foi o **Support Vector Machine (SVM)**, no qual obtivemos o *score* mais alto, de 83,5.  ","b37f056b":"> ### Como existem v\u00e1rias idades presentes nos conjuntos de dados, se cri\u00e1ssemos um gr\u00e1fico n\u00e3o teria como saber qual a idade que mais sobreviveu \u00e0 trag\u00e9dia, pois ter\u00edamos muitos resultados...","7cc6fed3":"> ## Florestas Aleat\u00f3rias (Random Forest):","cc2e2462":"> * Abaixo mostra a dimens\u00e3o do conjunto de dados de teste:\n>> - 418 registros (linhas);\n>> - 11  *features* (colunas).\n\n> Obs: Tem 1 coluna a menos, pois \u00e9 a *feature* que indica se o passageiro sobreviveu (1) ou n\u00e3o (0) e n\u00e3o deve estar no conjunto de testes, pois \u00e9 a previs\u00e3o que o algoritmo nos mostrar\u00e1.\n","205c0831":"> ## Cross Validation com K-fold:","c0f94182":"> ### Desta forma, tornamos o gr\u00e1fico mais limpo e f\u00e1cil de ser interpretado:","af832a71":"> ### A partir disto, vamos criar um algoritmo capaz de dizer se uma pessoa sobreviveria \u00e0 trag\u00e9dia do Titanic, baseado em *Features* que o pr\u00f3prio desafio nos disponibiliza atrav\u00e9s de um algoritmo de *Machine Learning*."}}