{"cell_type":{"fa9dbe1f":"code","910bfd79":"code","3e795c02":"code","86fdc926":"code","cccbb41f":"code","7e0e90db":"code","c9ca5922":"code","4fdd1b7d":"code","5707b8e4":"code","8e263550":"code","a2fc7196":"code","4f294df6":"code","9b61c969":"code","7b973f84":"code","d81cc1e0":"code","125844d7":"code","76b604c3":"code","11d57a64":"code","00c41db9":"code","09fc773d":"code","19d878d1":"code","05a39c39":"code","aa0ef776":"code","9ea1f4df":"code","8eb5eefe":"code","a1469a7c":"code","d00937fc":"code","633320c5":"code","e7d2823d":"code","5587a49b":"markdown","815cc616":"markdown","10f3960b":"markdown","f34374e7":"markdown","2fb1f8ea":"markdown","c3c7da56":"markdown","5966c9a2":"markdown","f3310e76":"markdown","a8532ba5":"markdown","c3892c33":"markdown","0a8b580b":"markdown","794f58e9":"markdown","c9b1ce70":"markdown","f17b1baf":"markdown","4d2fd16b":"markdown","9b7e2095":"markdown","c7ea19af":"markdown"},"source":{"fa9dbe1f":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import FeatureAgglomeration\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nDF_PATH = '\/kaggle\/input\/tse-marketing-econometrics-project-2'\npd.set_option('display.max_columns', 100)","910bfd79":"df_train_start = pd.read_csv(os.path.join(DF_PATH, \"Train_transactions.csv\"))\ny_train = pd.read_csv(os.path.join(DF_PATH, \"Train_customers_repurchase.csv\"))\ndf_test = pd.read_csv(os.path.join(DF_PATH, \"Test_transactions.csv\"))\n\ndf_train = df_train_start.merge(y_train, on=\"id_client\")\nassert len(df_train) == len(df_train_start)\ndf_train.head()","3e795c02":"def skew_correction(df):\n    # Skew correction\n    # compute skewness\n    skewed_feats = df.select_dtypes(include=np.number).drop(columns=[\"payment_gift\", \"payment_cheque\", \"payment_cash\", \"payment_card\"]).apply(lambda x: skew(x.dropna()))  \n    skewed_feats = skewed_feats[skewed_feats > 0.75]\n    skewed_feats = skewed_feats.index\n    if \"id_client\" in skewed_feats:\n        skewed_feats = list(set(skewed_feats) - set([\"id_client\"]))\n    df.loc[:, tuple(skewed_feats)] = np.log1p(np.asarray(df[skewed_feats] + 0.001, dtype=float))\n       \ndef data_agg(data):\n    \n    data_agg = data.groupby(\"id_client\", as_index=False).agg({\"transaction_date\": [\"max\", \"min\", \"count\"], \n                                                              \"item_count\": [\"sum\", \"mean\", \"std\"], \n                                                              \"gross_amount\": [\"sum\", \"mean\", \"std\"], \n                                                              \"discount_amount\": [\"sum\", \"mean\", \"std\"],\n                                                              \"payment_gift\": [\"sum\", \"mean\", \"std\"], \n                                                              \"payment_cheque\": [\"sum\", \"mean\", \"std\"],\n                                                              \"payment_cash\": [\"sum\", \"mean\", \"std\"],\n                                                              \"payment_card\": [\"sum\", \"mean\", \"std\"], \n                                                              \"avg_item_price\": ['sum', 'mean', 'std'],\n                                                              \"stores_nb\": [\"count\", pd.Series.mode],\n                                                              }) \\\n                                                            .fillna(0)\n    return data_agg\n\ndef flatten_cols(data):\n    data.columns = [\"_\".join(col_name).rstrip('_') \n                                for col_name in \n                                data.columns.to_flat_index()]\ndef build_pipeline():\n    estimators = []\n    estimators.append(('standardize', RobustScaler()))\n    pipeline = Pipeline(estimators, verbose=True)\n    return pipeline\n\ndef run_pipeline(data, pipeline):\n    cols = [\"item_count\", \"gross_amount\", \"discount_amount\", \"avg_item_price\"]\n    data_pipeline = pd.DataFrame(pipeline.fit_transform(data[cols]),\n                                 index=data.index,\n                                 columns=cols)\n    data.loc[:, cols] = data_pipeline\n    return data\n\n\ndef prep_data(data):\n    ## Numerical columns\n    data.transaction_date = pd.to_datetime(data.transaction_date)\n\n    \n    data[['id_client', 'stores_nb', \"multicard\"]] = data[['id_client', 'stores_nb', 'multicard']].astype(object)\n    \n    if 'repurchase' in data.columns:\n        data[['repurchase']] = data[['repurchase']].astype(object)\n    ##\n    data[\"avg_item_price\"] = (data[\"gross_amount\"] \/ data[\"item_count\"]).replace(np.inf, 0).replace(-np.inf, 0)\n     ## Unskewing\n    print(\"Unskewing...\")\n    skew_correction(data)\n    \n    ## Pipeline Standardize\n    print(\"Running pipeline...\")\n    pipeline = build_pipeline()\n    run_pipeline(data, pipeline)\n    \n    ## Aggregated\n    print(\"Aggregating data...\")\n    data_num_grouped = data_agg(data)\n    flatten_cols(data_num_grouped)\n\n    \n    cat_cols = data.select_dtypes(include=object).columns\n    data_cat = data[list(cat_cols)]\n    data_cat.drop_duplicates(subset=\"id_client\", inplace=True)\n\n    assert len(data_num_grouped) == len(data_cat)\n    \n    res = pd.merge(data_cat, data_num_grouped, on=\"id_client\")\n    return res\n\ndf_train = prep_data(df_train)\ndf_test = prep_data(df_test)","86fdc926":"def replace_zip_code(x):\n    try:\n        x = int(x)\n    except ValueError:\n        return x\n    except TypeError:\n        x = np.round(x[0]\/1000)\n        x = str(int(x * 1000))\n        return x\n    else:\n        x = np.round(x\/1000)\n        x = str(int(x * 1000))\n        return x\n            \ndf_train.zip_code = df_train.zip_code.apply(lambda x: replace_zip_code(x))\ndf_test.zip_code = df_test.zip_code.apply(lambda x: replace_zip_code(x))","cccbb41f":"for col in [\"stores_nb\", \"stores_nb_mode\"]:\n    df_train[col] = df_train[col].apply(lambda x: replace_zip_code(x))\n    df_test[col] = df_test[col].apply(lambda x: replace_zip_code(x))","7e0e90db":"from fuzzywuzzy import process\n\ndef clean_emails(df):\n    ### To do fuzzy matching on names\n    df.email_domain = df.email_domain.apply(lambda x: x.split(\".\")[0] if isinstance(x, str) else x)\n    good_domains = df.email_domain.value_counts()[(df.email_domain.value_counts()  > 100)].index\n    print(good_domains)\n    def fuzzy_match(x):\n        try:\n            x_split = x.split('.')[0]\n        except AttributeError:\n            return \"residual\"\n        else:\n            try:\n                highest, ratio = process.extractOne(x_split, good_domains)\n                print(x, highest, ratio)\n            except TypeError:\n                return \"residual\"\n            else:\n                if ratio > 65:\n                    return highest\n                else:\n                    return \"residual\"\n\n    df.loc[~df.email_domain.isin(good_domains), \"email_domain\"] = df.loc[~df.email_domain.isin(good_domains), \n                                                                         \"email_domain\"].apply(lambda x: fuzzy_match(x))\n    return good_domains\n\ngood_domains = clean_emails(df_train)","c9ca5922":"def fuzzy_match(x, good_domains,):\n    try:\n        x_split = x.split('.')[0]\n    except AttributeError:\n        return \"residual\"\n    else:\n        try:\n            highest, ratio = process.extractOne(x_split, good_domains)\n            print(x, highest, ratio)\n        except TypeError:\n            return \"residual\"\n        else:\n            if ratio > 65:\n                return highest\n            else:\n                return \"residual\"\n            \n\ndf_test.email_domain = df_test.email_domain.apply(lambda x: x.split(\".\")[0] if isinstance(x, str) else x)\ndf_test.loc[~df_test.email_domain.isin(good_domains), \"email_domain\"] = df_test.loc[~df_test.email_domain.isin(good_domains), \n                                                                                    \"email_domain\"].apply(lambda x: fuzzy_match(x,good_domains))","4fdd1b7d":"import re\n    \ndef delta_time(x):\n    if x==\"0\":\n        return np.nan\n    elif not re.match(\"\\d{2}\/\\d{2}\/\\d{4}\", x):\n        print(x)\n        return np.nan\n    else:\n        x = pd.to_datetime(x, format=\"%d\/%m\/%Y\")\n        delta = pd.to_datetime(\"01\/09\/2020\", format=\"%d\/%m\/%Y\") - x\n        return delta.days\n\ndef has_sub(x):\n    if x==\"0\":\n        x = 0\n    else: \n        x = 1\n    return x\n\ndf_train.card_subscription = df_train.card_subscription.fillna(\"0\").apply(lambda x: delta_time(x))\ndf_test.card_subscription = df_test.card_subscription.fillna(\"0\").apply(lambda x: delta_time(x))\ndf_train[\"has_subscription\"] = df_train.card_subscription.fillna(\"0\").apply(lambda x: has_sub(x))\ndf_test[\"has_subscription\"] = df_test.card_subscription.fillna(\"0\").apply(lambda x: has_sub(x))","5707b8e4":"df_train.loc[df_train.price_segmentation=='08', \"price_segmentation\"] = np.nan","8e263550":"for df in [df_train, df_test]:\n    df.multicard.fillna(\"nan\", inplace=True)","a2fc7196":"class LabelEncoderExt(object):\n    def __init__(self):\n        \"\"\"\n        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n        \"\"\"\n        self.label_encoder = LabelEncoder()\n        # self.classes_ = self.label_encoder.classes_\n\n    def fit(self, data_list):\n        \"\"\"\n        This will fit the encoder for all the unique values and introduce unknown value\n        :param data_list: A list of string\n        :return: self\n        \"\"\"\n        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n        self.classes_ = self.label_encoder.classes_\n\n        return self\n\n    def transform(self, data_list):\n        \"\"\"\n        This will transform the data_list to id list where the new values get assigned to Unknown class\n        :param data_list:\n        :return:\n        \"\"\"\n        new_data_list = list(data_list)\n        for unique_item in np.unique(data_list):\n            if unique_item not in self.label_encoder.classes_:\n                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n\n        return self.label_encoder.transform(new_data_list)","4f294df6":"def label_encoder(df, estimated_var, lbe_dict):\n    if estimated_var == \"stores_nb\":\n        lbe = lbe_dict[\"stores_nb_mode\"]\n        df[estimated_var] = lbe.transform(df[estimated_var].astype(str))  \n    else:\n        lbe = LabelEncoderExt()\n        lbe.fit(df[estimated_var].astype(str))\n        df[estimated_var] = lbe.transform(df[estimated_var].astype(str))\n    \n    return lbe\n    ","9b61c969":"lbe_dict = {}\nfor col in [\"email_domain\", \"zip_code\", \"civility\", \"price_segmentation\", \"stores_nb_mode\", \"stores_nb\", \"multicard\"]:\n    lbe_dict[col] = label_encoder(df_train, col, lbe_dict)\n    df_test[col] = lbe_dict[col].transform(df_test[col].astype(str))\n    print(col)","7b973f84":"last_month = pd.to_datetime(\"01\/07\/2020\", format=\"%d\/%m\/%Y\")\ndf_train[\"last_month\"] = np.where(df_train[\"transaction_date_max\"].dt.tz_localize(None) < last_month, 0, 1)\ndf_test[\"last_month\"] = np.where(df_test[\"transaction_date_max\"].dt.tz_localize(None) < last_month, 0, 1)","d81cc1e0":"df_train[\"transaction_date_max\"] = (pd.to_datetime(\"01\/09\/2020\", format=\"%d\/%m\/%Y\") - df_train[\"transaction_date_max\"].dt.tz_localize(None)).dt.days\ndf_test[\"transaction_date_max\"] = (pd.to_datetime(\"01\/09\/2020\", format=\"%d\/%m\/%Y\") - df_test[\"transaction_date_max\"].dt.tz_localize(None)).dt.days\n\ndf_train[\"transaction_date_min\"] = (pd.to_datetime(\"01\/09\/2020\", format=\"%d\/%m\/%Y\") - df_train[\"transaction_date_min\"].dt.tz_localize(None)).dt.days\ndf_test[\"transaction_date_min\"] = (pd.to_datetime(\"01\/09\/2020\", format=\"%d\/%m\/%Y\") - df_test[\"transaction_date_min\"].dt.tz_localize(None)).dt.days","125844d7":"df_train.transaction_date_max = (df_train.transaction_date_max - df_train.transaction_date_max.min()) \/ (df_train.transaction_date_max.max() - df_train.transaction_date_max.min())\ndf_test.transaction_date_max = (df_test.transaction_date_max - df_test.transaction_date_max.min()) \/ (df_test.transaction_date_max.max() - df_test.transaction_date_max.min())\n\ndf_train.transaction_date_min = (df_train.transaction_date_min - df_train.transaction_date_min.min()) \/ (df_train.transaction_date_min.max() - df_train.transaction_date_min.min())\ndf_test.transaction_date_min = (df_test.transaction_date_min - df_test.transaction_date_min.min()) \/ (df_test.transaction_date_min.max() - df_test.transaction_date_min.min())","76b604c3":"email_domains = [\"email_domain\"]\nCAT_COLS = [\"civility\", \"zip_code\", \"card_subscription\", \n            \"price_segmentation\", \"id_client\", \"stores_nb_mode\",\n            \"stores_nb\", \"has_subscription\", \"multicard\", \n            \"last_month\"] + email_domains\n\ndf_train = df_train.astype({col: \"category\" for col in CAT_COLS})\ndf_test = df_test.astype({col: \"category\" for col in CAT_COLS})","11d57a64":"df_train.head()","00c41db9":"from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, log_loss\nimport catboost as cbt\n\n\ndf_to_use = df_train\n#df_to_use = pd.concat([df_to_use[df_to_use.repurchase==1], df_to_use[df_to_use.repurchase==0].sample(50000)]).reset_index(drop=True)\n\nX = df_to_use.drop(columns=['repurchase', 'id_client', 'card_subscription',])\n# X = df_to_use[col_to_keep]\ny = df_to_use.repurchase.astype(int)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=123,)","09fc773d":"cat_cols = list(X_train.select_dtypes(\"category\").columns)\nmodel = cbt.CatBoostClassifier(cat_features=cat_cols,\n                               iterations=10000,\n                               learning_rate=0.01, \n                               objective=\"Logloss\",\n                               task_type=\"GPU\", \n                               early_stopping_rounds=80)\nmodel.fit(X_train, y_train, eval_set=(X_valid, y_valid), verbose=50)","19d878d1":"feat_imp = model.get_feature_importance(prettified=True)\nfeat_imp","05a39c39":"features_to_drop = list(feat_imp.iloc[-10:, :][\"Feature Id\"].values)\n\nX = df_to_use.drop(columns=['repurchase', 'id_client', 'card_subscription'] + features_to_drop)\ny = df_to_use.repurchase.astype(int)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=123)","aa0ef776":"from lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,  ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nimport xgboost as xgb\nimport lightgbm as lgb\n\n\nNFOLDS = 5\nSEED = 0\n\nkf = StratifiedKFold(n_splits = NFOLDS, shuffle=True, random_state=SEED)\ncat_cols = list(X_train.select_dtypes(\"category\").columns)\n\nX_test = df_test.drop(columns=[\"id_client\", \"card_subscription\"] + features_to_drop)\n\n\ndef one_hot_encode(X, ohe=None):\n    X_cat = X.select_dtypes(include=\"category\")\n    X_not_cat = X.select_dtypes(exclude=\"category\")\n    if ohe is None:\n        ohe = OneHotEncoder(handle_unknown=\"ignore\")\n        X_cat = pd.DataFrame(ohe.fit_transform(X_cat).todense(), \n                             columns=ohe.get_feature_names(X_cat.columns))\n    else:\n        X_cat = pd.DataFrame(ohe.transform(X_cat).todense(), \n                             columns=ohe.get_feature_names(X_cat.columns))\n    X_final = pd.concat([X_cat, X_not_cat.reset_index(drop=True)], axis=1)\n        \n    return X_final, ohe \n\n\nclass SklearnWrapper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n        self.ohe = None\n        \n    def train(self, x_train, y_train):\n        x_train_ohe, self.ohe = one_hot_encode(x_train, self.ohe) \n        x_train_no_nan = x_train_ohe.fillna(x_train_ohe.mean())\n        self.clf.fit(x_train_no_nan, y_train)\n\n    def predict(self, x):\n        x_ohe, self.ohe = one_hot_encode(x, self.ohe)\n        x_no_nan = x_ohe.fillna(x_ohe.mean())\n        return self.clf.predict_proba(x_no_nan)[:,1]\n\nclass CatboostWrapper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_seed'] = seed\n        params['cat_features'] = cat_cols\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict_proba(x)[:,1]\n        \nclass LightGBMWrapper(object):\n    def __init__(self, clf, seed=0, params=None):\n        self.params = params\n        self.params['feature_fraction_seed'] = seed\n        self.params['bagging_seed'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        \n        dtrain = lgb.Dataset(x_train, label=y_train,\n                     feature_name=list(x_train.columns),\n                     categorical_feature=cat_cols, \n                     free_raw_data=False\n                    )\n        self.model = lgb.train(self.params, \n                               dtrain, \n                               num_boost_round = 1000)\n\n    def predict(self, x):\n        return self.model.predict(x)\n\n\nclass XgbWrapper(object):\n    def __init__(self, seed=0, params=None):\n        self.param = params\n        self.param['seed'] = seed\n        self.nrounds = params.pop('nrounds', 250)\n        self.ohe = None\n        \n    def train(self, x_train, y_train):\n        x_train_ohe, self.ohe = one_hot_encode(x_train, self.ohe) \n        dtrain = xgb.DMatrix(x_train_ohe, label=y_train)\n        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n\n    def predict(self, x):\n        x_ohe, self.ohe = one_hot_encode(x, self.ohe) \n        return self.gbdt.predict(xgb.DMatrix(x_ohe))\n    \n    \ndef get_oof(clf):\n    oof_train = np.zeros((len(X),))\n    oof_test = np.zeros((len(X_test),))\n    oof_test_skf = np.empty((NFOLDS, len(X_test)))\n\n    for i, (train_index, test_index) in enumerate(kf.split(X, y)):\n        x_tr = X.loc[train_index]\n        y_tr = y.loc[train_index]\n        x_te = X.loc[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        print(f\"fold {i+1}\/{NFOLDS}\", log_loss(y.loc[test_index], oof_train[test_index]))\n        oof_test_skf[i, :] = clf.predict(X_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n","9ea1f4df":"rf_params = {\n    'n_jobs': 16,\n    'n_estimators': 500,\n    'max_features': 0.8,\n    'max_depth': 12,\n    'min_samples_leaf': 2,\n}\n\nxgb_params = {\n    'subsample': 0.85,\n    'learning_rate': 0.02,\n    'objective': 'binary:logistic',\n    'max_depth': 6,\n    'nrounds': 2500,\n    'tree_method': 'gpu_hist'  # quote if no gpu\n}\n\ncatboost_params = {\n    'iterations': 3000,\n    'learning_rate': 0.02,\n    'objective': 'Logloss',\n    'task_type': \"GPU\",  # quote if no gpu\n    'verbose': 0\n}\n\nadb_params = {'n_estimators': 1000,\n              'learning_rate': 0.2}\n\nlightgbm_params = {\n    'subsample': 0.8,\n    'reg_lambda': 0.8,\n    'reg_alpha': 0.25,\n    'min_child_weight': 3,\n    'max_depth': 12,\n    'learning_rate': 0.1,\n    'gamma': 0.1,\n    'colsample_bytree': 0.5,\n    'objective': 'binary', \n    'metric': 'binary_logloss',\n    'device': 'gpu',  # quote if no gpu\n    'gpu_platform_id': 0,  # quote if no gpu\n    'gpu_device_id': 0  # quote if no gpu\n}","8eb5eefe":"xg = XgbWrapper(seed=SEED, params=xgb_params)\ncb = CatboostWrapper(clf= cbt.CatBoostClassifier, seed = SEED, params=catboost_params)\nlg = LightGBMWrapper(clf = LGBMClassifier, seed = SEED, params = lightgbm_params)","a1469a7c":"xg_oof_train, xg_oof_test = get_oof(xg)\nprint(\"XGB done\")\ncb_oof_train, cb_oof_test = get_oof(cb)\nprint(\"CB done\")\nlg_oof_train, lg_oof_test = get_oof(lg)\nprint(\"LG done\")","d00937fc":"print(\"XG-CV: {}\".format((log_loss(y, xg_oof_train))))\nprint(\"CB-CV: {}\".format((log_loss(y, cb_oof_train))))\nprint(\"LG-CV: {}\".format((log_loss(y, lg_oof_train))))","633320c5":"weights = [3, 1, 4]\nX_cat = np.concatenate((xg_oof_train, lg_oof_train, cb_oof_train), axis=1) * weights\nX_test_cat = np.concatenate((xg_oof_test, lg_oof_test, cb_oof_test), axis=1) * weights\n\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(X_cat,y)","e7d2823d":"# y_pred = logistic_regression.predict_proba(X_test_cat)[:, 1]\ny_pred = X_test_cat.sum(axis=1) \/ np.sum(weights)\npredd = pd.concat([df_test.id_client, pd.DataFrame(y_pred)], axis=1)\npredd.columns = [\"id_client\", \"repurchase_proba\"]\npredd.to_csv(\"submission_ensemble.csv\", index=False)","5587a49b":"## Card Subscription\n\nCreate a subscription dummy","815cc616":"## Price Segmentation\n\nRemove typo","10f3960b":"### Zip Code\n\nReplacing zip codes with departements","f34374e7":"## Load Data","2fb1f8ea":"## Model\n\n### Train test split","c3c7da56":"## Time Variables\n\nCmpute a Last month dummy: if the client has purchased in July","5966c9a2":"### Features Selection","f3310e76":"## Categorical Data","a8532ba5":"## Label Encoding","c3892c33":"## Multicard\n\nAdd a Missing value category","0a8b580b":"## Email Domains\n\nClean email domains with fuzzy matching to delete typos","794f58e9":"# Propensity Score","c9b1ce70":"## Import Modules","f17b1baf":"## Stores ID\n\nDo the same for the store ID","4d2fd16b":"## Aggregation Pipeline","9b7e2095":"Categorical variables to Categorical pandas types","c7ea19af":"MinMax Scale the Time Data"}}