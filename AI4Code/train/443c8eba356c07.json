{"cell_type":{"ddab26e5":"code","60682f0e":"code","65c75055":"code","4cff455c":"code","391affec":"code","b686b599":"code","afa1c37e":"code","616c9f48":"code","6a8bcf12":"code","8ee00df6":"code","a87473d3":"code","d4827566":"code","e080b74d":"code","d4952055":"code","f666ce1e":"code","7ec50cc8":"code","0f5ecdaf":"code","23608bf2":"code","4dc57250":"code","7f9f391b":"code","4c6e0fc9":"code","1755ddc2":"code","a9576bb4":"code","56cfa4d8":"code","cf8c42f1":"code","90b8d6d0":"code","07df5298":"code","3a6acf06":"code","81b468c2":"code","38692f6b":"code","05556391":"code","3d5ab292":"code","3cca64ea":"code","0cade507":"code","78738c13":"code","1c76d487":"code","5d1035e6":"code","6e8cb869":"code","be24f808":"code","d6f83bd9":"code","b149ef55":"code","c134ae23":"code","b093277e":"code","f07f66f6":"code","2931a455":"code","692e86d7":"code","6b05f355":"code","ff4397a9":"code","5d5309ba":"code","7473f21f":"code","ce8f8ce8":"code","a4de4c76":"code","9e2d41f6":"code","73b9f6eb":"code","4c2aa921":"markdown","c1761a6f":"markdown","32a45b5b":"markdown","ed31ea11":"markdown","d0c584bc":"markdown","d3a8f19e":"markdown","e403ba78":"markdown","c243ce39":"markdown","fbe50692":"markdown","cd901943":"markdown","b708b792":"markdown","bb43b149":"markdown","726e3bd2":"markdown","c7d48bbc":"markdown","922abb5c":"markdown","0563f5bd":"markdown","2f8dbc06":"markdown","4320f2d2":"markdown","25f0291f":"markdown","3f861c65":"markdown","5d41f75b":"markdown","fbb0aeca":"markdown","ce77b03d":"markdown","580e4a1c":"markdown","bece6698":"markdown","caecdcf1":"markdown","b45cec70":"markdown","baba2832":"markdown","e675ea32":"markdown","a5e3e727":"markdown"},"source":{"ddab26e5":"%%HTML\n<style type=\"text\/css\">\n\ndiv.h2 {\n    background-color: white; \n    color: #2C3E50; \n    padding: auto;\n    margin:5px,\n    font-size: 24px; \n}\nspan.high_all{\n    font-weight: bold;\n    #background-color: #ffffb3;\n    font-size: 16px; \n}\nspan.high_prod{\n    font-weight: bold;\n    color: #45a1a1;\n    font-size: 16px; \n}\nspan.high_expl{\n    font-weight: bold;\n    color: #a1a112;\n    font-size: 16px; \n}\nspan.high {\n    background-color: #f5f5ef; \n    color: #000000; \n    font-size: 16px; \n    margin-top: 1px;\n}\nspan.high_p {\n    background-color: #a6d9d9; \n    color: #000000;  \n    font-size: 16px; \n    margin-top: 1px;\n}\nspan.high_e {\n    background-color: #f2f28c; \n    color: #000000;  \n    font-size: 16px; \n    margin-top: 1px;\n}\n\nspan.high_1{    \n    background-color: #E8B151; \n    color: #37322D; \n    font-size: 36px; \n    max-width: 1500px; \n    margin-top: 1px;}\n\np.normal {\n    padding-bottom: 3px;\n    color: #383b40; \n    font-size: 16px;\n    display: table;\n    margin:auto;\n    #border-bottom:1px dotted;\n}\np.heading1 {\n    padding-bottom: 3px;\n    border-bottom: 1px solid #355f96;\n    color: #355f96; \/* #b54002; *\/\n    font-size: 30px;\n    display: inline-block;\n    text-align: center;\n\n}\np.heading2 {\n    padding-bottom: 3px;\n    border-bottom: 1px solid #355f96;\n    color: #355f96; \/* #b54002; *\/\n    font-size: 20px;\n    display: inline-block;\n\n}\ndiv.a {\n  text-indent: 50px;\n}\ndiv.h {\n  text-align: center;\n}\ndiv.p {\n  text-align: justify;\n  content: \"\";\n  display: inline-block;\n  width: 100%;\n}\n<\/style>","60682f0e":"import numpy as np\nimport pandas as pd\nimport re\nfrom itertools import chain\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport matplotlib.ticker as mtick\nimport seaborn as sns\n\n# load 2020 data\ndf_2020 = pd.read_csv('\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv', low_memory=False)\n\n# 2019 data\ndf_2019 = pd.read_csv(\"..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv\", low_memory=False)\n\n# 2018 data\ndf_2018 = pd.read_csv(\"..\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv\", low_memory=False)\n\n# 2017 data\ndf_2017 = pd.read_csv(\"..\/input\/kaggle-survey-2017\/multipleChoiceResponses.csv\",encoding='ISO-8859-1', low_memory=False)\n\n\n\n#schema_2019 = pd.read_csv('\/kaggle\/input\/kaggle-survey-2019\/survey_schema.csv')\n\n# capture column names; remove col names from main data \ndf_2020_cols = df_2020.loc[0,:]\ndf_2020.drop(0, inplace=True)\n\n# capture column names; remove col names from main data \ndf_2019_cols = df_2019.loc[0,:]\ndf_2019.drop(0, inplace=True)\n\ndf_2018_cols = df_2018.loc[0,:]\ndf_2018.drop(0, inplace=True)\n\n# new variables\ndf_2020.loc[:,'All'] = 'All'\ndf_2020_cols = df_2020_cols.append(pd.Series('All', index=['All']))\n\n# explorers vs production\ndf_2020.loc[df_2020.loc[:,'Q22'].isin(['We are exploring ML methods (and may one day put a model into production)',\n                                       'We use ML methods for generating insights (but do not put working models into production)']),'q22_groups'] = 'Explorers'\ndf_2020.loc[df_2020.loc[:,'Q22'].isin(['We have well established ML methods (i.e., models in production for more than 2 years)',\n                                       'We recently started using ML methods (i.e., models in production for less than 2 years)']),'q22_groups'] = 'Producers'\ndf_2020_cols = df_2020_cols.append(pd.Series('q22_groups', index=['q22_groups']))\n\n# 2019\ndf_2019.loc[df_2019.loc[:,'Q8'].isin(['We are exploring ML methods (and may one day put a model into production)',\n                                       'We use ML methods for generating insights (but do not put working models into production)']),'Q8_groups'] = 'Explorers'\ndf_2019.loc[df_2019.loc[:,'Q8'].isin(['We have well established ML methods (i.e., models in production for more than 2 years)',\n                                       'We recently started using ML methods (i.e., models in production for less than 2 years)']),'Q8_groups'] = 'Producers'\ndf_2019_cols = df_2019_cols.append(pd.Series('Q8_groups', index=['Q8_groups']))\n\n# 2018\ndf_2018.loc[df_2018.loc[:,'Q10'].isin(['We are exploring ML methods (and may one day put a model into production)',\n                                       'We use ML methods for generating insights (but do not put working models into production)']),'Q10_groups'] = 'Explorers'\ndf_2018.loc[df_2018.loc[:,'Q10'].isin(['We have well established ML methods (i.e., models in production for more than 2 years)',\n                                       'We recently started using ML methods (i.e., models in production for less than 2 years)']),'Q10_groups'] = 'Producers'\ndf_2018_cols = df_2018_cols.append(pd.Series('Q10_groups', index=['Q10_groups']))\n\n\n# income recode\ndf_2020.loc[:,'Q24r']=df_2020.loc[:,'Q24'].astype(str).replace(regex=r'^\\$0', value='0') \\\n    .replace(regex=r'^> \\$500,000', value='500,000-').apply(lambda x: x.split('-')[0]).str.replace(r'\\,','').astype(float)\n\ndf_2020.loc[(df_2020['Q24r']>=0) & (df_2020['Q24r']<50000),'Q24rec'] = 'a) bellow 50k USD'\ndf_2020.loc[(df_2020['Q24r']>=50000) & (df_2020['Q24r']<100000),'Q24rec'] = 'b) 50k-100k USD'\ndf_2020.loc[(df_2020['Q24r']>=100000) & (df_2020['Q24r']<150000),'Q24rec'] = 'c) 100k-150k USD'\ndf_2020.loc[(df_2020['Q24r']>=150000) & (df_2020['Q24r']<200000),'Q24rec'] = 'c) 150k-200k USD'\ndf_2020.loc[(df_2020['Q24r']>=200000),'Q24rec'] = 'd) >=200k USD'","65c75055":"plt.style.use('default')\n## helper functions\n## tables - counts and %\ndef tab_counts_perc(df, schema, q_prefix, multi=True):\n    cnt=0\n    # find columns by prefix\n    if multi:\n        for ele in df_cols.index:\n            mtch = re.match(q_prefix, ele)\n            if (mtch): \n                cnt=cnt+1\n                \n                agg = df.groupby(ele).agg({\n                        ele: 'count'})\n                if q_prefix.find(\"_\")>-1:\n                    fin_name = q_prefix[:q_prefix.find(\"_\")]\n                else:\n                    fin_name = q_prefix\n                agg.columns = [fin_name]\n                if cnt==1:\n                    df_fin = agg\n                else:\n                    df_fin = pd.concat([df_fin, agg])\n                #print(fin_name)\n    else:\n        fin_name = q_prefix\n        df_fin = df.groupby(q_prefix).agg({q_prefix: 'count'})\n        \n    #df_fin['percent'] = df_fin[fin_name]\/int(schema.loc[1,fin_name])\n    df_fin['percent'] = df_fin[q_prefix].apply(lambda x: (x\/df_fin[q_prefix].sum()*100).round(1))\n    df_fin.index.name = schema[fin_name][0]\n\n    return(df_fin)\n\n\ndef tab_multi_cross_perc(df, schema, q_prefix, col_var, multi=True):\n    cnt=0\n    lst_row_names = []\n    # find columns by prefix\n    if multi:\n        for ele in schema.index:\n            mtch = re.match(q_prefix, ele)\n            if (mtch): \n                cnt=cnt+1\n                valid_cols=pd.crosstab(df[ele],df[col_var]).columns\n                if cnt==1:\n                    all_cols=valid_cols\n                elif (int(cnt) > int(1)) and (len(pd.crosstab(df[ele],df[col_var]).columns) > len(valid_cols)):\n                    all_cols=valid_cols\n                    \n                agg = pd.crosstab(df[ele],df[col_var]).apply(lambda r: r\/ tab_counts_perc(df, schema, col_var, multi=False)[col_var][valid_cols].values, axis=1)\n\n                if q_prefix.find(\"_\")>-1:\n                    #print(q_prefix[:q_prefix.find(\"_\")])\n                    fin_name = q_prefix[:q_prefix.find(\"_\")]\n                else:\n                    fin_name = q_prefix\n                #print(fin_name)\n                #print(agg.values)\n                #print(agg.index)\n                #print(agg.index.name)\n                lst_row_names.append(list(agg.index.values))\n                #agg.index.name = [fin_name]\n                \n                if cnt==1:\n                    df_fin = pd.DataFrame(agg.values,columns=valid_cols)\n                else:\n                    df_fin = pd.concat([df_fin, pd.DataFrame(agg.values, columns=valid_cols)])\n                    #df_fin = df_fin.append(pd.DataFrame(agg.values))\n                #print(fin_name)\n    else:\n        fin_name = q_prefix\n        valid_cols=pd.crosstab(df[q_prefix],df[col_var]).columns\n        agg = pd.crosstab(df[q_prefix],df[col_var]).apply(lambda r: r\/ tab_counts_perc(df, schema, col_var, multi=False)[col_var][valid_cols].values, axis=1)\n        df_fin = pd.DataFrame(agg.values,columns=agg.columns)\n        lst_row_names.append(list(agg.index.values))\n        \n    #df_fin.index.name = schema[fin_name][0]\n    #df_fin.columns = all_cols #list(set(df.loc[df[col_var].notna(),col_var]))\n    df_fin.index = list(chain(*lst_row_names))\n    return(df_fin)\n\n## cross tab - counts only\ndef tab_cross_cnt(df, schema, q_prefix, col_var, multi=True):\n    df_cross = pd.crosstab(df[q_prefix],df[col_var])\n    df_cross.index.name = schema[q_prefix][0]\n    return(df_cross)\n\n## cross tab - %\ndef tab_cross_perc(df, schema, q_prefix, col_var, multi=True):\n    valid_cols=pd.crosstab(df[q_prefix],df[col_var]).columns\n    df_cross = pd.crosstab(df[q_prefix],df[col_var]).apply(lambda r: r\/ tab_counts_perc(df, schema, col_var, multi=False)[col_var][valid_cols].values, axis=1)\n    df_cross.index.name = schema[q_prefix][0]\n    return(df_cross)\n\n\n\n\n# custom function to return only % target values - by category\ndef select_indicator_by_group(main_df, quest_no, lst_exact_match, exact_match_label, find_word, lst_words, words_label, has_col_item, label_col=None,\n                                has_recode=None):\n    if (has_col_item):\n        df_q = main_df.loc[(main_df.loc[:,'question_number']==quest_no) & (main_df.loc[:,'column_name']==label_col),:].copy()  # target question\n    else:\n        df_q = main_df.loc[main_df.loc[:,'question_number']==quest_no,:].copy()  # target question\n    df_q.loc[:,'response_value'].fillna('No Response', inplace=True)\n    df_ans = df_q['response_value'].value_counts() \/ df_q.shape[0]  # results % values\n    \n    if(has_recode):\n        df_cross = pd.crosstab(df_q['response_recode'], df_q['primary_industry']).apply(lambda r: r\/r.sum(), axis=0)\n    else:\n        df_cross = pd.crosstab(df_q['response_value'], df_q['primary_industry']).apply(lambda r: r\/r.sum(), axis=0)\n        \n    n_e=0\n    for exact_m in lst_exact_match:\n        n_e+=1\n        exact_mtch = df_cross.loc[df_cross.index==exact_m,:].copy()\n        exact_mtch.loc[:,'indicator'] = exact_match_label[n_e-1]\n        if (n_e==1): exact_mtch_fin=exact_mtch\n        else: exact_mtch_fin=exact_mtch_fin.append(exact_mtch)\n    \n    if (find_word):\n        n_w = 0\n        for word in lst_words:\n            n_w += 1\n            find_wrd = df_cross.loc[df_cross.index.str.contains(word,regex=True),:].copy()\n            d_fin = pd.DataFrame(find_wrd.sum()).transpose()\n            d_fin.loc[:,'indicator'] = words_label[n_w-1]\n            if (n_w==1): d_fin2 = exact_mtch.append(d_fin)\n            else: d_fin2 = d_fin2.append(d_fin)\n    else:\n        d_fin2 = exact_mtch_fin\n\n    return(d_fin2.reset_index())\n\n\n   \ndef highlight_max(s):\n    '''\n    highlight the maximum in a Series.\n    '''\n    is_max = s == s.max()\n    return ['background-color: #fff5cc' if v else '' for v in is_max]\n\ndef title_style():\n    return [{\n    'selector': 'caption',\n    'props': [\n        ('color', '#496FA0'),\n        ('font-size', '16px')\n    ]\n}]\n\n\ndef delta_cross_table(df, q_prx, head_var, q_tit, multi_val=False):\n    dlt_df = tab_multi_cross_perc(df, df_2020_cols, q_prefix=q_prx, col_var=head_var, multi=multi_val)\n    dlt_df.loc[:,'delta'] = dlt_df.loc[:,'Producers'] - dlt_df.loc[:,'Explorers']\n    dlt_df.columns.name=''\n    dlt_df = dlt_df.sort_values('delta', ascending=False)\n    return(dlt_df)\n\ndef apply_style_tab(df, q_tit):\n    dff = df.style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                    .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(q_tit) \\\n                    .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())\n    return(dff)\n\n\n## change position of y series for scatter pplot - not to overlap labels\ndef position_series_labels(df, target_value):\n    use_itemy = df\n    label_height = 0.03\n\n    y2_df = pd.DataFrame(use_itemy[target_value])\n    y2_df.sort_values(by=[target_value], inplace=True)\n\n    y2_names = y2_df.index\n    y2 = (y2_df[target_value].values)\n\n    y_fst = y2[0]-0.006\n    ser_y2_dup =[]\n    ser_y2_dup.append(y_fst)\n    ser_y2_dup.extend(y2[1:])\n\n    dif_y2 = []\n    dif_y2.append(ser_y2_dup[0])\n    dif_y2.extend(np.array(ser_y2_dup)[1:] - np.array(ser_y2_dup)[:-1])\n\n    err=0\n\n    while err < 20:\n        n = -1\n        err = err + 1\n\n        for i in dif_y2:\n            #print(i)\n            n = n+1\n            if (n == 0): \n                ser_y2_dup[n] = ser_y2_dup[n]\n            else:\n                if (i < label_height):\n                    #print(i)\n                    ser_y2_dup[n] = ser_y2_dup[n] + (label_height - dif_y2[n])\n                    dif_y2[n] = round(ser_y2_dup[n] - ser_y2_dup[n-1],4)  # update dif value\n                else:\n                    ser_y2_dup[n] = ser_y2_dup[n]\n\n    final_frame = pd.DataFrame({'index' : y2_names, 'values_y' : ser_y2_dup})\n    \n    final_join = use_itemy.join(final_frame.set_index('index'))\n                        \n    return(final_join['values_y'].values)","4cff455c":"prop_q22 = pd.DataFrame(df_2020['Q22'].value_counts() \/ df_2020['Q22'].value_counts().sum()).sort_index(ascending=False)\nprop_q22['Does your current employer incorporate machine learning methods into their business?'] = prop_q22.index\nprop_q22.style.apply(lambda x: ['background: #a6d9d9' if x.name in ['We have well established ML methods (i.e., models in production for more than 2 years)',\n                                                                   'We recently started using ML methods (i.e., models in production for less than 2 years)'] \n                                \n                              else ('background: #f2f28c' if x.name in ['We are exploring ML methods (and may one day put a model into production)',\n                                                                   'We use ML methods for generating insights (but do not put working models into production)']  \n                                    else 'background: #f5f5f0')\n                            for i in x], \n                   axis=1).format(\"{:.1%}\",  subset=['Q22']).set_properties(**{'width': '210px', 'height': '9px', 'font-size': '10pt'}).hide_index()","391affec":"q22_gr = tab_counts_perc(df_2020, df_2020_cols ,\"q22_groups\", multi=False).sort_values(by='percent', ascending=False)\nq8_gr = tab_counts_perc(df_2019, df_2019_cols ,\"Q8_groups\", multi=False).sort_values(by='percent', ascending=False)\nq10_gr = tab_counts_perc(df_2018, df_2018_cols ,\"Q10_groups\", multi=False).sort_values(by='percent', ascending=False)\nq22_gr.index.name='2020'\nq8_gr.index.name='2019'\nq10_gr.index.name = '2018'\nq22_evo = pd.concat([q10_gr['percent']\/100, q8_gr['percent']\/100, q22_gr['percent']\/100], axis=1)\nq22_evo.columns = ['2018', '2019', '2020']\nq22_evo.style.apply(lambda x: ['background: #a6d9d9' if x.name in ['Producers'] \n                              else ('background: #f2f28c' if x.name in ['Explorers'] else '')\n                            for i in x], axis=1) \\\n        .set_caption('Explorers vs Producers evolution').format(\"{:.1%}\").set_properties(**{'width': '160px', 'height': '9px', 'font-size': '10pt'})","b686b599":"class create_basket_var:\n    def __init__(self, q_prefix, q_prefix_short, cross_q, df, df_cols, limit):\n        self.q_prefix = q_prefix\n        self.q_prefix_short = q_prefix_short\n        self.cross_q = cross_q\n        self.new_df=df\n        self.df_cols=df_cols\n        \n        \n        var_list = [x for x in df_cols.index if re.match(r'^{0}.+$'.format(q_prefix),x)]\n        self.new_df['{0}_ans_basket'.format(q_prefix_short)] = df.loc[:,var_list].astype(str).apply(lambda x: '|'.join(x), axis=1) \\\n            .replace(regex=r'nan', value='').replace(regex=r'\\|+', value='|').replace(regex=r'\\|', value='\\n')\n        self.new_df['{0}_ans_cnt'.format(q_prefix_short)] = df.loc[:,var_list].count(axis=1)\n        self.df_cols = df_cols.append(pd.Series('{0} answer basket'.format(q_prefix_short), index=['{0}_ans_basket'.format(q_prefix_short)]))\n        \n        check_lst = var_list\n        check_lst.append('{0}_ans_basket'.format(q_prefix_short))\n        check_lst.append('{0}_ans_cnt'.format(q_prefix_short))\n\n        # include in analisys only if sample >= 30 cases\n        lst_keep = list(self.new_df.loc[self.new_df['{0}_ans_cnt'.format(self.q_prefix_short)]>1,'{0}_ans_basket'.format(q_prefix_short)].value_counts()[self.new_df['{0}_ans_basket'.format(q_prefix_short)].value_counts()>=30].index)\n        try:\n            lst_keep.remove('\\n') \n        except:\n            pass\n        try:\n            lst_keep.remove('\\nNone') \n        except:\n            pass\n        self.lst_keep=lst_keep[0:limit]  # select top n baskets\n        #print(lst_keep)\n        #print(len(self.lst_keep))\n\n    def cross_df(self):\n        cross_df = tab_multi_cross_perc(self.new_df.loc[(self.new_df['{0}_ans_cnt'.format(self.q_prefix_short)]>1) & \n                                                        (self.new_df['{0}_ans_basket'.format(self.q_prefix_short)].isin(self.lst_keep)),:], self.df_cols, q_prefix=self.cross_q, col_var='{0}_ans_basket'.format(self.q_prefix_short), multi=False).sort_index(axis=0).sort_index(axis=1)\n        #print(self.new_df['{0}_ans_basket'.format(self.q_prefix_short)].value_counts()[35:70])\n        return(cross_df)\n    \n    def counts_answer_basket(self):\n        return(self.new_df.loc[(self.new_df['{0}_ans_cnt'.format(self.q_prefix_short)]>1) & (self.new_df['{0}_ans_basket'.format(self.q_prefix_short)].isin(self.lst_keep)),\n                        '{0}_ans_basket'.format(self.q_prefix_short)].value_counts())\n    def counts_total(self):\n        return(self.new_df.loc[:,'{0}_ans_cnt'.format(self.q_prefix_short)].value_counts())\n    \n    def counts_more_than_one(self):\n        return(self.new_df.loc[(self.new_df['{0}_ans_cnt'.format(self.q_prefix_short)]>1),\n                        '{0}_ans_basket'.format(self.q_prefix_short)].value_counts())","afa1c37e":"# STACKED BAR function\n# color codes\ncolor_dict={'Producers': '#40a6bf', 'Explorers': '#f9f986',\n            'Alteryx ': \"#99ccff\", \n            'Google Data Studio': \"#0059b3\",\n            'Microsoft Power BI': \"#ffcc00\", \n            'Qlik': \"#00802b\",\n            'Salesforce':\"#0099e6\", \n            'Tableau': \"#b30000\", \n            'Other': \"#a3a375\",\n           'Bash': '#1f1f14',\n           'C': '#4d4dff',\n           'C++': '#ccddff',\n           'Java': '#ffff33',\n           'Javascript': \"#ffcc00\",\n           'Julia': '#d65cad',\n           'MATLAB': \"#008080\",\n           'None': '#ebebe0',\n           'Python': '#005580',\n           'R': '#ffff66',\n           'SQL': '#ff704d',\n           'Swift': \"#ff6600\",\n           'Microsoft Access ': '#ff8000',\n           'Microsoft Azure Data Lake Storage ': '#ffbf00',\n           'Microsoft SQL Server ': '#ffff66',\n           'MongoDB ': '#70db70',\n           'MySQL ': '#0080ff',\n           'Oracle Database ': '#ff3333',\n           'PostgresSQL ': '#4d6680',\n           'SQLite ': '#80e5ff'}\n\ndef bar_stacked(txt_index, plotdata, title, xlab, ylab, bar_orient='bar', add_rectang=False, categ_prop=None):\n    plt.style.use('seaborn-bright')\n    N = len(txt_index)\n    if(bar_orient=='bar'):\n        fsize = (25,8.5)\n        if not categ_prop is None:\n            fig, (ax, ax0) = plt.subplots(nrows=2, ncols=1, gridspec_kw={'height_ratios': [4, 1]}) \n            plt.subplots_adjust(hspace=0.7)\n        else:\n            fig, ax = plt.subplots()\n    elif(bar_orient=='barh'):\n        fsize = (12,14)\n        if not categ_prop is None:\n            fig, (ax, ax0) = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [4, 1]}) \n            plt.subplots_adjust(hspace=0.5)\n        else:\n            fig, ax = plt.subplots()\n        \n    sort_lst = list(plotdata.transpose().index.values)\n    try:\n        sort_lst.remove(\"Other\")\n    except:\n        pass\n    \n    if 'Explorers' in sort_lst and bar_orient=='barh':\n        use_data = plotdata.sort_values(by=sort_lst, ascending=False)\n        if not categ_prop is None:\n            use_data0 = categ_prop.sort_index(axis=0, level=sort_lst)\n    else:\n        use_data = plotdata.sort_values(by=sort_lst, ascending=True)\n        if not categ_prop is None:\n            use_data0 = categ_prop.sort_index(axis=0, level=sort_lst)\n    \n    #ax=use_data.plot(kind=bar_orient,stacked=True,color=color_dict, figsize=fsize, width=0.95, alpha=0.70)\n    ax=use_data.plot(ax=ax, kind=bar_orient,stacked=True,color=color_dict, figsize=fsize, width=0.95, alpha=0.70); #axes[0,0].set_title('A')\n    #ax.plot(data=use_data, kind=bar_orient, stacked=True,color=color_dict, figsize=fsize, width=0.95, alpha=0.70)\n    ax.set_title(title+'\\n', fontsize=18, color='#5c5c3d', fontweight='bold')\n    ax.set_xlabel(xlab, fontsize=14, color='#5c5c3d')\n    ax.set_ylabel(ylab, fontsize=14, color='#5c5c3d')\n    ax.spines['bottom'].set_color('#000000')\n    ax.spines['top'].set_visible(False) \n    ax.spines['right'].set_visible(False) \n    ax.spines['left'].set_visible(False) \n    #ax.set_xticklabels(['0%','20%','40%','60%','80%','100%'])\n    if(bar_orient=='bar'):\n        ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n    elif(bar_orient=='barh'):\n        ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n        \n    for tick in ax.get_xticklabels():\n                tick.set_rotation(0)\n        \n    #print(len(sort_lst))\n    if len(sort_lst)>=8:\n        pos_x_legend = -0.05\n    else:\n        pos_x_legend = 0.05\n    ax.legend(ncol=len(list(plotdata.transpose().index.values)), \n              loc='upper center', fontsize=12, borderaxespad = -1 ) # bbox_to_anchor=(pos_x_legend, 1),\n    \n    if not categ_prop is None:          \n        ax0=use_data0.plot(kind=bar_orient,stacked=False,color=\"#d6d6c2\", figsize=fsize, width=0.95, alpha=0.70)\n        ax0.spines[\"top\"].set_visible(False)\n        ax0.spines[\"left\"].set_visible(False)\n        ax0.spines[\"right\"].set_visible(False)\n        ax0.set_ylabel('# observations', fontsize=14, color='#5c5c3d')\n        for tick in ax0.get_xticklabels():\n            tick.set_rotation(0)\n        plt.setp(ax0.get_xticklabels(), visible=False)\n        plt.setp(ax0.get_yticklabels(), visible=False)\n\n    if not categ_prop is None:\n        tar_ax = [ax,ax0]\n    else:\n        tar_ax = [ax]\n    cnt = -1\n    for ax in tar_ax:\n        cnt+=1\n        for rect in ax.patches:\n            # Find where everything is located\n            height = rect.get_height()\n            width = rect.get_width()\n            x = rect.get_x()\n            y = rect.get_y()\n            prev_y = y\n            \n            # The height of the bar is the data value [bar] | width [barh]\n            if(bar_orient=='bar'):\n                if cnt==0:\n                    label_text = f'{height*100:.0f}%'  # f'{height:.2f}' \n                else:\n                    label_text = f'{height:.0f}'\n            elif(bar_orient=='barh'):\n                if cnt==0:\n                    label_text = f'{width*100:.0f}%'  # f'{height:.2f}' \n                else:\n                    label_text = f'{width:.0f}'\n\n            # ax.text(x, y, text)\n            label_x = x + width \/ 2\n            label_y = y + height \/ 2\n\n            # plot only when height is greater than specified value\n            if height > 0.1:\n                ax.text(label_x, label_y, label_text, ha='center', va='center', fontsize=14, color='#0f0f0a', fontweight='bold', alpha=0.4)\n\n    #plt.setp([a.get_xticklabels() for a in fig.axes[:-1]], visible=True)","616c9f48":"q23_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q23_Part', col_var='q22_groups')\nq23_22group.loc[:,'delta'] = q23_22group.loc[:,'Producers'] - q23_22group.loc[:,'Explorers']\nq23_22group.columns.name =''\nq23_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q23 main work activities\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","6a8bcf12":"# Q23 main work activities - producers\/explorers\nq23basket_obj = create_basket_var('Q23_Part', 'q23', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=10)\nq23_by_q22gr = q23basket_obj.cross_df()\nq23_prop = q23basket_obj.counts_answer_basket()\nbar_stacked(q23_by_q22gr.transpose().index, q23_by_q22gr.transpose(), \n            'Main work activities - producers vs explorers distribution','%','Work activities mix', 'barh', False, q23_prop)","8ee00df6":"q3_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q3', col_var='q22_groups', multi=False)\nq3_22group.loc[:,'delta'] = q3_22group.loc[:,'Producers'] - q3_22group.loc[:,'Explorers']\nq3_22group.columns.name=''\npd.concat([q3_22group.sort_values('delta', ascending=False).head(), q3_22group.sort_values('delta', ascending=False).tail()]) \\\n    .style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']).format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']) \\\n    .set_caption(\"Q3 Country\") \\\n    .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","a87473d3":"q5_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q5', col_var='q22_groups', multi=False)\nq5_22group.loc[:,'delta'] = q5_22group.loc[:,'Producers'] - q5_22group.loc[:,'Explorers']\n\nq5_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']).format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']) \\\n    .set_caption(\"Q5 Title\").set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())\n\n","d4827566":"q6_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q6', col_var='q22_groups', multi=False)\nq6_22group.loc[:,'delta'] = q6_22group.loc[:,'Producers'] - q6_22group.loc[:,'Explorers']\nq6_22group.columns.name=''\nq6_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']).format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']) \\\n.set_caption(\"Q6 Coding experience\").set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","e080b74d":"\nq15_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q15', col_var='q22_groups', multi=False)\nq15_22group.loc[:,'delta'] = q15_22group.loc[:,'Producers'] - q15_22group.loc[:,'Explorers']\nq15_22group.columns.name=''\nq15_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']).format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']) \\\n.set_caption(\"Q15 ML experience\").set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())\n","d4952055":"q11_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q11', col_var='q22_groups', multi=False)\nq11_22group.loc[:,'delta'] = q11_22group.loc[:,'Producers'] - q11_22group.loc[:,'Explorers']\nq11_22group.columns.name=''\nq11_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']).format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']) \\\n    .set_caption(\"Q11 Clomputing platform\").set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","f666ce1e":"q20_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q20', col_var='q22_groups', multi=False)\nq20_22group.loc[:,'delta'] = q20_22group.loc[:,'Producers'] - q20_22group.loc[:,'Explorers']\nq20_22group.columns.name=''\nq20_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q20 Company size\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())\n","7ec50cc8":"q21_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q21', col_var='q22_groups', multi=False)\nq21_22group.loc[:,'delta'] = q21_22group.loc[:,'Producers'] - q21_22group.loc[:,'Explorers']\nq21_22group.columns.name=''\nq21_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q21 Data science team size\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","0f5ecdaf":"q24_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q24rec', col_var='q22_groups', multi=False)\nq24_22group.loc[:,'delta'] = q24_22group.loc[:,'Producers'] - q24_22group.loc[:,'Explorers']\nq24_22group.columns.name='salary ranges'\nq24_22group.style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q24 Salary\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())\n","23608bf2":"q25_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q25', col_var='q22_groups', multi=False)\nq25_22group.loc[:,'delta'] = q25_22group.loc[:,'Producers'] - q25_22group.loc[:,'Explorers']\nq25_22group.insert(0,'',q25_22group.index)\n\nq25_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q25 Spend on ML\") \\\n                .set_table_styles(title_style()) \\\n                .hide_index() \\\n                .set_properties(**{'width': '160px', 'height': '16px', 'font-size': '10pt'})","4dc57250":"q36_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q36', col_var='q22_groups', multi=True)\nq36_22group.loc[:,'delta'] = q36_22group.loc[:,'Producers'] - q36_22group.loc[:,'Explorers']\nq36_22group.columns.name=''\nq36_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q36 Public share\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())\n","7f9f391b":"# Q36 public share - producers\/explorers\nq36basket_obj = create_basket_var('Q36_Part', 'q36', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=15)\nq36_by_q22gr = q36basket_obj.cross_df()\nq36_prop = q36basket_obj.counts_answer_basket()\nbar_stacked(q36_by_q22gr.transpose().index, q36_by_q22gr.transpose(), \n            'Public share platforms - producers vs explorers distribution','','%', 'bar', False, q36_prop)","4c6e0fc9":"q37_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q37', col_var='q22_groups', multi=True)\nq37_22group.loc[:,'delta'] = q37_22group.loc[:,'Producers'] - q37_22group.loc[:,'Explorers']\nq37_22group.columns.name=''\nq37_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q37 Platforms used for data science courses\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())\n","1755ddc2":"# Q37 data science courses platforms used - producers\/explorers\nq37basket_obj = create_basket_var('Q37_Part', 'q37', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=15)\nq37_by_q22gr_expl = q37basket_obj.cross_df()\nq37_prop = q37basket_obj.counts_answer_basket()\nbar_stacked(q37_by_q22gr_expl.transpose().index, q37_by_q22gr_expl.transpose(), \n            'Platforms used for data science courses','%',\n            ' ', 'barh', False, q37_prop)","a9576bb4":"q39_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q39', col_var='q22_groups', multi=True)\nq39_22group.loc[:,'delta'] = q39_22group.loc[:,'Producers'] - q39_22group.loc[:,'Explorers']\nq39_22group.columns.name = ''\nq39_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q39 Media sources of information on data science topics\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())\n","56cfa4d8":"# Q39 media sources of information - producers\/explorers\nq39basket_obj = create_basket_var('Q39_Part', 'q39', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=15)\nq39_by_q22gr_expl = q39basket_obj.cross_df()\nq39_prop = q39basket_obj.counts_answer_basket()\nbar_stacked(q39_by_q22gr_expl.transpose().index, q39_by_q22gr_expl.transpose(), \n            'Media sources of information','%',\n            ' ', 'barh', False, q39_prop)","cf8c42f1":"q7_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q7_Part', col_var='q22_groups')\nq7_22group.loc[:,'delta'] = q7_22group.loc[:,'Producers'] - q7_22group.loc[:,'Explorers']\nq7_22group.columns.name=''\nq7_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q7 Programming languages\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","90b8d6d0":"# Q7 prog lang - producers\/explorers\nq7basket_obj_ = create_basket_var('Q7_Part', 'q7', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=15)\nq7_by_q22gr = q7basket_obj_.cross_df()\nq7_prop_ = q7basket_obj_.counts_answer_basket()\nbar_stacked(q7_by_q22gr.transpose().index, q7_by_q22gr.transpose(), \n            'Programming languages usage basket',' ','%', 'bar',add_rectang=False, categ_prop=q7_prop_)","07df5298":"# Q7 Programming languages do you use on a regular basis vs Q8 Programming language recommended to aspiring data scientist\nq7basket_obj = create_basket_var('Q7_Part', 'q7', 'Q8', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=15)\nq7basket_df = q7basket_obj.cross_df()\nq7_prop = q7basket_obj.counts_answer_basket()\nbar_stacked(q7basket_df.transpose().index, q7basket_df.transpose(), 'Language recommended to aspiring DS by people using multiple programming languages on a regular basis',\n            '','Programing laguage recommended to aspiring data scientist', 'bar', add_rectang=False, categ_prop=q7_prop)","3a6acf06":"q14_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q14_Part', col_var='q22_groups')\nq14_22group.loc[:,'delta'] = q14_22group.loc[:,'Producers'] - q14_22group.loc[:,'Explorers']\nq14_22group.columns.name=''\nq14_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q14 Data visualization libraries\/tools used\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","81b468c2":"q16_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q16_Part', col_var='q22_groups')\nq16_22group.loc[:,'delta'] = q16_22group.loc[:,'Producers'] - q16_22group.loc[:,'Explorers']\nq16_22group.columns.name=''\nq16_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q16 ML Frameworks used on a regular bases\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","38692f6b":"# Q16 ML frameworks used on a regular basis - producers\/explorers\nq16basket_obj = create_basket_var('Q16_Part', 'q16', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=20)\nq16_by_q22gr_expl = q16basket_obj.cross_df()\nq16_prop = q16basket_obj.counts_answer_basket()\nbar_stacked(q16_by_q22gr_expl.transpose().index, q16_by_q22gr_expl.transpose(), \n            'ML frameworks used - top 20 combinations',' ',\n            '%', 'bar', False, q16_prop)","05556391":"q17_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q17_Part', col_var='q22_groups')\nq17_22group.loc[:,'delta'] = q17_22group.loc[:,'Producers'] - q17_22group.loc[:,'Explorers']\nq17_22group.columns.name=''\nq17_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q17 ML Algorithms used\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","3d5ab292":"# Q17 ML algorithms used on a regular basis - producers\/explorers\nq17basket_obj = create_basket_var('Q17_Part', 'q17', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=10)\nq17_by_q22gr_expl = q17basket_obj.cross_df()\nq17_prop = q17basket_obj.counts_answer_basket()\nbar_stacked(q17_by_q22gr_expl.transpose().index, q17_by_q22gr_expl.transpose(), \n            'ML algorithms used - top 10 combinations','%',\n            ' ', 'barh', False, q17_prop)","3cca64ea":"q18_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q18_Part', col_var='q22_groups')\nq18_22group.loc[:,'delta'] = q18_22group.loc[:,'Producers'] - q18_22group.loc[:,'Explorers']\nq18_22group.columns.name=''\nq18_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q18 Computer vision methods used\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","0cade507":"q19_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q19_Part', col_var='q22_groups')\nq19_22group.loc[:,'delta'] = q19_22group.loc[:,'Producers'] - q19_22group.loc[:,'Explorers']\nq19_22group.columns.name=''\nq19_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q19 NLP methods used\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","78738c13":"q38_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q38', col_var='q22_groups', multi=False)\nq38_22group.loc[:,'delta'] = q38_22group.loc[:,'Producers'] - q38_22group.loc[:,'Explorers']\nq38_22group.columns.name=''\nq38_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q38 Primary analysis tool\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","1c76d487":"q9_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q9_Part', col_var='q22_groups')\nq9_22group.loc[:,'delta'] = q9_22group.loc[:,'Producers'] - q9_22group.loc[:,'Explorers']\nq9_22group.columns.name=''\nq9_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q9 IDE's used\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","5d1035e6":"# Q9 IDE's used on a regular basis vs producers\/explorers\nq9basket_obj = create_basket_var('Q9_Part', 'q9', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=15)\nq9_by_q22gr_expl = q9basket_obj.cross_df()\nq9_prop = q9basket_obj.counts_answer_basket()\nbar_stacked(q9_by_q22gr_expl.transpose().index, q9_by_q22gr_expl.transpose(), \n            'Multiple IDE users','%','', 'barh', False, q9_prop)","6e8cb869":"q10_22group = tab_multi_cross_perc(df_2020, df_2020_cols, q_prefix='Q10_Part', col_var='q22_groups')\nq10_22group.loc[:,'delta'] = q10_22group.loc[:,'Producers'] - q10_22group.loc[:,'Explorers']\nq10_22group.columns.name=''\nq10_22group.sort_values('delta', ascending=False).style.bar(subset=['delta'], align='mid', color=['#EDA39A', '#99DAAE']) \\\n                .format(\"{:.1%}\", subset=['Producers', 'Explorers','delta']).set_caption(\"Q10 hosted notebooks used\") \\\n                .set_properties(**{'width': '100px', 'height': '25px', 'font-size': '10pt'}).set_table_styles(title_style())","be24f808":"q12_22group = delta_cross_table(df=df_2020, q_prx = 'Q12', head_var='q22_groups', q_tit=\"Q12 specialized hardware used used\", multi_val=True)\napply_style_tab(q12_22group, q_tit=\"Q12 specialized hardware used used\")","d6f83bd9":"q26a_22group = delta_cross_table(df_2020,q_prx = 'Q26_A_', head_var='q22_groups', q_tit=\"Q26A cloud computing platform used\", multi_val=True)\napply_style_tab(q26a_22group, q_tit=\"Q26A cloud computing platform used\")","b149ef55":"# Q26 cloud computing platforms used on a regular basis - producers\/explorers\nq26basket_obj = create_basket_var('Q26_A_Part', 'q26', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=10)\nq26_by_q22gr_expl = q26basket_obj.cross_df()\nq26_prop = q26basket_obj.counts_answer_basket()\nbar_stacked(q26_by_q22gr_expl.transpose().index, q26_by_q22gr_expl.transpose(), \n            'Cloud computing platforms used - producers vs explorers distribution',' ',\n            '%', 'bar', False, q26_prop)","c134ae23":"q27a_22group = delta_cross_table(df_2020,q_prx = 'Q27_A_Part', head_var='q22_groups', q_tit=\"Q27A Cloud computing products used\", multi_val=True)\napply_style_tab(q27a_22group, q_tit=\"Q27A cloud computing products used\")","b093277e":"# Q27 cloud computing peoducts used on a regular basis - producers\/explorers\nq27basket_obj = create_basket_var('Q27_A_Part', 'q27', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=15)\nq27_by_q22gr_expl = q27basket_obj.cross_df()\nq27_prop = q27basket_obj.counts_answer_basket()\nbar_stacked(q27_by_q22gr_expl.transpose().index, q27_by_q22gr_expl.transpose(), \n            'Cloud computing products used - producers vs explorers distribution','%',' ', \n            'barh', False,q27_prop)","f07f66f6":"q28a_22group = delta_cross_table(df_2020, q_prx = 'Q28_A_Part', head_var='q22_groups', q_tit=\"Q28A ML products used\", multi_val=True)\napply_style_tab(q28a_22group, q_tit=\"Q28A ML products used\")","2931a455":"q29a_22group = delta_cross_table(df_2020,q_prx = 'Q29_A_Part', head_var='q22_groups', q_tit=\"Q29A big data products used\", multi_val=True)\napply_style_tab(q29a_22group, q_tit=\"Q29A Big data products used\")","692e86d7":"# Q29 big data products used on a regular basis vs Q30 big data product used most often\nq29basket_obj = create_basket_var('Q29_A_Part', 'q29', 'Q30', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=20)\nq29_by_q30 = q29basket_obj.cross_df()\nbar_stacked(q29_by_q30.transpose().index, q29_by_q30.transpose(), 'Big data products used on a regular basis vs. most often used',\n            'Big data products used on a regular basis','Big data product used most often', 'bar')","6b05f355":"# Q29A big dats tools - producers\/explorers\nq29basket_obj = create_basket_var('Q29_A_Part', 'q29', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=15)\nq29_by_q22gr = q29basket_obj.cross_df()\n\nq29_prop = q29basket_obj.counts_answer_basket()\nbar_stacked(q29_by_q22gr.transpose().index, q29_by_q22gr.transpose(), \n            'Big data tools usage mix - producers vs explorers distribution',' ','%', 'bar', add_rectang=False, categ_prop=q29_prop)","ff4397a9":"q31a_22group = delta_cross_table(df_2020,q_prx = 'Q31_A_Part', head_var='q22_groups', q_tit=\"Q31A BI tools used\", multi_val=True)\napply_style_tab(q31a_22group, q_tit=\"Q31A BI used used\")","5d5309ba":"# Q31 BI tools used on a regular basis vs Q32 BI tool used most often\nq31basket_obj = create_basket_var('Q31_A_Part', 'q31', 'Q32', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=20)\nq31_by_q32 = q31basket_obj.cross_df()\nbar_stacked(q31_by_q32.transpose().index, q31_by_q32.transpose(), 'Prefered tool when using multiple BI tools','BI tools usage mix','More often BI tool used',\n            'bar', add_rectang=False, categ_prop=None)","7473f21f":"# Q31 BI tools - producers\/explorers\nq31basket_obj_ = create_basket_var('Q31_A_Part', 'q31', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=15)\nq31_by_q22gr = q31basket_obj_.cross_df()\n\nq31_prop = q31basket_obj_.counts_answer_basket() #\/ q31basket_obj_.counts_answer_basket().sum()\nbar_stacked(q31_by_q22gr.transpose().index, q31_by_q22gr.transpose(), \n            'BI tools usage mix - producers vs explorers distribution',' ','%', 'bar', add_rectang=False, categ_prop=q31_prop)","ce8f8ce8":"# Q33A AutoML tools - producers\/explorers\nq33basket_obj = create_basket_var('Q33_A_Part', 'q33', 'q22_groups', df_2020.loc[df_2020.loc[:,'q22_groups'].isin(['Producers','Explorers']),:].copy(), df_2020_cols, limit=15)\nq33_by_q22gr = q33basket_obj.cross_df()\nq33_prop = q33basket_obj.counts_answer_basket()\nbar_stacked(q33_by_q22gr.transpose().index, q33_by_q22gr.transpose(), \n            'AutoML tools - producers vs explorers distribution','%','AutoMl usage tool mix', 'barh', False, q33_prop)","a4de4c76":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy.linalg import svd\n\n\nclass CA(object):\n    \"\"\"Simple corresondence analysis.\n    \n    Inputs\n    ------\n    ct : array_like\n      Two-way contingency table. If `ct` is a pandas DataFrame object,\n      the index and column values are used for plotting.\n    Notes\n    -----\n    The implementation follows that presented in 'Correspondence\n    Analysis in R, with Two- and Three-dimensional Graphics: The ca\n    Package,' Journal of Statistical Software, May 2007, Volume 20,\n    Issue 3.\n    \"\"\"\n\n    def __init__(self, ct):\n        self.rows = ct.index.values if hasattr(ct, 'index') else None\n        self.cols = ct.columns.values if hasattr(ct, 'columns') else None\n        \n        # contingency table\n        N = np.matrix(ct, dtype=float)\n\n        # correspondence matrix from contingency table\n        P = N \/ N.sum()\n\n        # row and column marginal totals of P as vectors\n        r = P.sum(axis=1)\n        c = P.sum(axis=0).T\n\n        # diagonal matrices of row\/column sums\n        D_r_rsq = np.diag(1. \/ np.sqrt(r.A1))\n        D_c_rsq = np.diag(1. \/ np.sqrt(c.A1))\n\n        # the matrix of standarized residuals\n        S = D_r_rsq * (P - r * c.T) * D_c_rsq\n\n        # compute the SVD\n        U, D_a, V = svd(S, full_matrices=False)\n        D_a = np.asmatrix(np.diag(D_a))\n        V = V.T\n\n        # principal coordinates of rows\n        F = D_r_rsq * U * D_a\n\n        # principal coordinates of columns\n        G = D_c_rsq * V * D_a\n\n        # standard coordinates of rows\n        X = D_r_rsq * U\n\n        # standard coordinates of columns\n        Y = D_c_rsq * V\n\n        # the total variance of the data matrix\n        inertia = sum([(P[i,j] - r[i,0] * c[j,0])**2 \/ (r[i,0] * c[j,0])\n                       for i in range(N.shape[0])\n                       for j in range(N.shape[1])])\n\n        self.F = F.A\n        self.G = G.A\n        self.X = X.A\n        self.Y = Y.A\n        self.inertia = inertia\n        self.eigenvals = np.diag(D_a)**2\n\n    def plot_ca(self):\n        \"\"\"Plot the first and second dimensions.\"\"\"\n        xmin, xmax = None, None\n        ymin, ymax = None, None\n        fig, ax = plt.subplots(1,1,figsize=(8,10))\n        if self.rows is not None:\n            for i, t in enumerate(self.rows):\n                x, y = self.F[i,0], self.F[i,1]\n                plt.text(x, y, t, va='center', ha='center', color='#7d7d4f', size=9)\n                xmin = min(x, xmin if xmin else x)\n                xmax = max(x, xmax if xmax else x)\n                ymin = min(y, ymin if ymin else y)\n                ymax = max(y, ymax if ymax else y)\n        else:\n            plt.plot(self.F[:, 0], self.F[:, 1], 'ro')  #plt\n\n        if self.cols is not None:\n            for i, t in enumerate(self.cols):\n                x, y = self.G[i,0], self.G[i,1]\n                plt.text(x, y, t, va='center', ha='center', color='#a81608', fontweight='bold', size=14)\n                xmin = min(x, xmin if xmin else x)\n                xmax = max(x, xmax if xmax else x)\n                ymin = min(y, ymin if ymin else y)\n                ymax = max(y, ymax if ymax else y)\n        else:\n            plt.plot(self.G[:, 0], self.G[:, 1], 'bs')\n            \n\n        if xmin and xmax:\n            pad = (xmax - xmin) * 0.05\n            plt.xlim(xmin - pad, xmax + pad)\n        if ymin and ymax:\n            pad = (ymax - ymin) * 0.05\n            plt.ylim(ymin - pad, ymax + pad)\n\n        plt.grid(color='#f5f5ef', linestyle=\"-\")\n        #plt.xlabel('Dim 1' + str(self.inertia))\n        #plt.ylabel('Dim 2' + str(self.eigenvals))\n        ax.axes.xaxis.set_visible(False)\n        ax.axes.yaxis.set_visible(False)\n        \n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_color(\"#e2e2d0\")\n        ax.spines['left'].set_visible(False)\n        #ax.spines['bottom'].set_visible(False)\n        ax.spines['bottom'].set_color('#e2e2d0')\n        ","9e2d41f6":"# general profile\nq5_22group_use = pd.concat([q5_22group.sort_values('delta', ascending=False).head(1),q5_22group.sort_values('delta', ascending=False).tail(1)])\nq5_22group_use.index = '' + q5_22group_use.index\n\nq6_22group_use = pd.concat([q6_22group.sort_values('delta', ascending=False).head(1),q6_22group.sort_values('delta', ascending=False).tail(1)])\nq6_22group_use.index = 'Code exp: ' + q6_22group_use.index\n\nq15_22group_use = pd.concat([q15_22group.sort_values('delta', ascending=False).head(1),q15_22group.sort_values('delta', ascending=False).tail(1)]) \nq15_22group_use.index = 'ML exp: ' + q15_22group_use.index\n\nq20_22group_use = pd.concat([q20_22group.sort_values('delta', ascending=False).head(1),q20_22group.sort_values('delta', ascending=False).tail(1)]) \nq20_22group_use.index = 'Comp.size: ' + q20_22group_use.index\n\nq21_22group_use = pd.concat([q21_22group.sort_values('delta', ascending=False).head(1),q21_22group.sort_values('delta', ascending=False).tail(1)]) \nq21_22group_use.index = 'Team size: ' + q21_22group_use.index\n\nq24_22group_use = pd.concat([q24_22group.sort_values('delta', ascending=False).head(1),q24_22group.sort_values('delta', ascending=False).tail(1)]) \nq24_22group_use.index = 'Salary: ' + q24_22group_use.index\n\n\n\nq11_22group_use = q11_22group.loc[~q11_22group.index.isin(['Other','None']),:]\nq11_22group_use.index = '' + q11_22group_use.index\n\nq23_22group_use = pd.concat([q23_22group.sort_values('delta', ascending=False).head(1),q23_22group.sort_values('delta', ascending=False).tail(1)]) \nq23_22group_use.index = ' ' + q23_22group_use.index\n\nq36_22group_use = pd.concat([q36_22group.sort_values('delta', ascending=False).head(1),q36_22group.sort_values('delta', ascending=False).tail(1)]) \nq36_22group_use.index = 'Public share: ' + q36_22group_use.index\n\nq37_22group_use = pd.concat([q37_22group.sort_values('delta', ascending=False).head(1),q37_22group.sort_values('delta', ascending=False).tail(1)]) \nq37_22group_use.index = 'Courses: ' + q37_22group_use.index\n\nq39_22group_use = pd.concat([q39_22group.sort_values('delta', ascending=False).head(1),q39_22group.sort_values('delta', ascending=False).tail(1)]) \nq39_22group_use.index = 'Info sources: ' + q39_22group_use.index\n\n\np1_df = pd.concat([q5_22group_use,q6_22group_use,q15_22group_use,q20_22group_use,q21_22group_use,q24_22group_use,\n                   q11_22group_use,q23_22group_use, q36_22group_use,q37_22group_use]) #q39_22group_use\n\np1_df['indicator'] = p1_df.index\n\np1_df_afc = pd.DataFrame(p1_df[['Explorers','Producers']].reset_index().copy())\n\ndf = p1_df_afc.iloc[:,:]\ndf = df.set_index('index')\n\nca = CA(df)\nca.plot_ca()\nplt.title('General profile map')\nplt.show()","73b9f6eb":"q7_22group_use = pd.concat([q7_22group.sort_values('delta', ascending=False).head(1),q7_22group.sort_values('delta', ascending=False).tail(1)]) \nq7_22group_use.index = 'Prog.lang: ' + q7_22group_use.index\n\nq14_22group_use = pd.concat([q14_22group.sort_values('delta', ascending=False).head(1),q14_22group.sort_values('delta', ascending=False).tail(1)]) \nq14_22group_use.index = 'Viz.libraries: ' + q14_22group_use.index\n\nq17_22group_use = pd.concat([q17_22group.sort_values('delta', ascending=False).head(1),q17_22group.sort_values('delta', ascending=False).tail(1)]) \nq17_22group_use.index = 'ML algorithms: ' + q17_22group_use.index + '\\n'\n\nq18_22group_use = pd.concat([q18_22group.sort_values('delta', ascending=False).head(1),q18_22group.sort_values('delta', ascending=False).tail(1)]) \nq18_22group_use.index = 'Computer vision: ' + q18_22group_use.index\n\nq19_22group_use = pd.concat([q19_22group.sort_values('delta', ascending=False).head(1),q19_22group.sort_values('delta', ascending=False).tail(1)]) \nq19_22group_use.index = 'NLP: ' + q19_22group_use.index\n\nq38_22group_use = pd.concat([q38_22group.sort_values('delta', ascending=False).head(1),q38_22group.sort_values('delta', ascending=False).tail(1)]) \nq38_22group_use.index = 'Main tool: ' + q38_22group_use.index\n\n\np2_df = pd.concat([q7_22group_use,q14_22group_use,q17_22group_use, q18_22group_use, q19_22group_use, q38_22group_use]) \np2_df['indicator'] = p2_df.index\n\n\np2_df_afc = pd.DataFrame(p2_df[['Explorers','Producers']].reset_index().copy())\n\n#df = p2_df_afc.iloc[:,:]\n#df = df.set_index('index')\n#ca = CA(df)\n#ca.plot_ca()\n#plt.show()\n\nq9_22group_use = pd.concat([q9_22group.sort_values('delta', ascending=False).head(1),q9_22group.sort_values('delta', ascending=False).tail(1)]) \nq9_22group_use.index = 'IDE: ' + q9_22group_use.index\n\nq10_22group_use = pd.concat([q10_22group.sort_values('delta', ascending=False).head(1),q10_22group.sort_values('delta', ascending=False).tail(1)]) \nq10_22group_use.index = 'Hosted notebooks: ' + q10_22group_use.index\n\nq12_22group_use = pd.concat([q12_22group.sort_values('delta', ascending=False).head(1),q12_22group.sort_values('delta', ascending=False).tail(1)]) \nq12_22group_use.index = 'Hosted notebooks: ' + q12_22group_use.index\n\nq26a_22group_use = pd.concat([q26a_22group.sort_values('delta', ascending=False).head(1),q26a_22group.sort_values('delta', ascending=False).tail(1)]) \nq26a_22group_use.index = 'Cloud platforms: ' + q26a_22group_use.index\n\nq27a_22group_use = pd.concat([q27a_22group.sort_values('delta', ascending=False).head(1),q27a_22group.sort_values('delta', ascending=False).tail(1)]) \nq27a_22group_use.index = 'Cloud products: ' + q27a_22group_use.index\n\nq28a_22group_use = pd.concat([q28a_22group.sort_values('delta', ascending=False).head(1),q28a_22group.sort_values('delta', ascending=False).tail(1)]) \nq28a_22group_use.index = 'ML products: ' + q28a_22group_use.index\n\nq29a_22group_use = pd.concat([q29a_22group.sort_values('delta', ascending=False).head(1),q29a_22group.sort_values('delta', ascending=False).tail(1)]) \nq29a_22group_use.index = 'Big data: ' + q29a_22group_use.index\n\nq31a_22group_use = pd.concat([q31a_22group.sort_values('delta', ascending=False).head(1),q31a_22group.sort_values('delta', ascending=False).tail(1)]) \nq31a_22group_use.index = 'BI: ' + q31a_22group_use.index\n\n#q33a_22group_use = pd.concat([q33a_22group.sort_values('delta', ascending=False).head(2),q33a_22group.sort_values('delta', ascending=False).tail(2)]) \n#q33a_22group_use.index = 'Auto ML: ' + q33a_22group_use.index\n\n#q34a_22group_use = pd.concat([q34a_22group.sort_values('delta', ascending=False).head(2),q34a_22group.sort_values('delta', ascending=False).tail(2)]) \n#q34a_22group_use.index = 'Auto ML: ' + q34a_22group_use.index\n\n#q35a_22group_use = pd.concat([q35a_22group.sort_values('delta', ascending=False).head(1),q35a_22group.sort_values('delta', ascending=False).tail(1)]) \n#q35a_22group_use.index = 'Experiments: ' + q35a_22group_use.index\n\n\np3_df = pd.concat([q9_22group_use, q10_22group_use, q12_22group_use, q26a_22group_use, q27a_22group_use, q28a_22group_use, q29a_22group_use,\n                   q31a_22group_use])\np3_df['indicator'] = p3_df.index\np3_df_afc = pd.DataFrame(p3_df[['Explorers','Producers']].reset_index().copy())\n\ndf = pd.concat([p2_df_afc,p3_df_afc])\n\n#df = p3_df_afc.iloc[:,:]\ndf = df.set_index('index')\n\nca = CA(df)\nca.plot_ca()\nplt.title('Skills and infrastructure map')\nplt.show()","4c2aa921":"<p class='normal'>While R and SQL is the most popular choice to use together, <span class='high_prod'>70% of producers<\/span> are using Python,Bash or Python,SQL,Bash or Python,Bash,SQL,R. Either way, <span class='high_all'>Python and Bash<\/span> seem to be present in producers' routine usage of programming languages.<\/p>","c1761a6f":"<p class='normal'>Generally BI tools are more used by explorers. One exception could be Tableau, that seems to be used a bit more by producers.<\/p>","32a45b5b":"<p class='normal'>Among ML users of big data infrastructure, it was interesting to see how different big data products compete with each other. For example, MySQL is quite frequent as main big data product used, but tends to lower its' share when used togheter with PostgresSQL.<\/p>","ed31ea11":"<p class='normal'> <span class='high_all'>Blogs<\/span> and <span class='high_all'>Journal publications<\/span> are the sources of information used in a higher degree by <span class='high_prod'>producers<\/span> when compared to explorers.<\/p>","d0c584bc":"<p class='normal'>In general, computer vision methods and NLP methos are used more by producers than explorers.<\/p>","d3a8f19e":"<p class='normal'><span class='high_expl'>Explorers<\/span> share their work more on <span class='high_all'>Kaggle<\/span>, while <span class='high_prod'>producers<\/span> either <span class='high_all'>don't share their work publicly<\/span> or they share it on <span class='high_all'>GitHub<\/span>.<\/p>","e403ba78":"<p class='normal'><span class='high_prod'>Producers<\/span> have <span class='high_all'>more coding experience<\/span> and especially <span class='high_all'>more ML experience<\/span> (biggest gap between the two groups is on less than 1 year ML experience category, where producers have an advance of <span class='high_all'>+15pp<\/span> vs. explorers. Producers are also using more cloud computing platforms. <\/p>","c243ce39":"<p class='normal'> Coursera is the preferred platform used for data science courses by producers compared to explorers. Interestingly when looking at the answers 'baskets': <span class='high_all'>Coursera & Fast.ai<\/span> combination is the most popular choice among <span class='high_prod'>producers<\/span> (<span class='high_all'>78%<\/span> of respondents that chose this combination are producers).<\/p>","fbe50692":"<p class='normal'>I've also checked overall (Explorers and Producers combined) what would be the recommended programming laguage to an aspiring data scientist based on the most frequent choices. Python is definitely leading the recommended programming language to be learned first and  it's interesting that even one third of users that are solely using R and SQL are also recommending Python to aspiring data scientists. It is also interesting to see that the most frequent combination of programing languages used is Python, SQL and Javascript (881 cases).<\/p>","cd901943":"<p class ='normal'><span class='high_all'>Bash, Python<\/span> and <span class='high_all'>SQL<\/span> are in a higher degree used by <span class='high_prod'>producers<\/span>. We also observe that <span class='high_expl'>explorers<\/span> are also using Javascript, C or Matlab more than producers are.<\/p>","b708b792":"<p class='normal'><span class='high_all'>Xgboost<\/span> is very popular among <span class='high_prod'>producers<\/span> which are using this framework by <span class='high_all'>+14pp<\/span> more than explorers are using it (must be working great in production).<\/p>","bb43b149":"<p class='heading2'>Proportion of target population<\/p>\n\n<p class='normal'>About two thirds of survey participants fall into these two categories - overlapping the profesionally engaged population.<\/p>","726e3bd2":"<a id='general_profile'><\/a>\n<p class='heading2'>Profile<\/p>\n\n<p class='normal'>Concerning the main work activities, <span class='high_all'>experimentation, improving product & workflow and prototyping<\/span> are the three key aspects that are differentiating Producers from Explorers.<\/p>","c7d48bbc":"<p class='normal'>References<\/p>\n\n[Correspondence analysis implementation in python](https:\/\/okomestudio.net\/biboroku\/2014\/05\/brand-positioning-by-correspondence-analysis\/)","922abb5c":"<p class ='normal'><span class='high_all'>Seaborn, Plotly<\/span> and <span class='high_all'>Matplotlib<\/span> visualization libraries are in higher use by <span class='high_prod'>producers<\/span> than by explorers.<\/p>","0563f5bd":"<p class='normal'>We can also observe what are the main work activities combined. The categories are order by highest share of producers and we can also see the absolute frequency of each group in grey, on the right side - to assess the popularity of each 'basket' of answers. Producers are generally doing more activities, the number of activities in each basket is higher when we have a higher proportion of producers, while the most popular basket is a combination of analyzing data and building\/running the data infrastructure,\n<\/p>","2f8dbc06":"<p class='normal'> There is a higher share of producers in US compared to other countries.<\/p>","4320f2d2":"<p class='normal'>The proportion of explorers and producers is relativelly stable over time - each group representing around one half of ML users population.<\/p>","25f0291f":"<p class='normal'>Second map summarizes the producers' interest toward cloud computing platforms, gradient boosting machines in particular or Bash programming language.<\/p>","3f861c65":"<p class='normal'>Interestingly the machine learning engineers doesn't show to be greatly concentrated in the producers group and neither data engineers, however the <span class='high_all'>data scientist<\/span> job shows the biggest gap between the two groups (<span class='high_all'>+15pp<\/span> for producers vs. explorers)<\/p>\n","5d41f75b":"<a id='skills'><\/a>\n<p class='heading2'>Skills<\/p>","fbb0aeca":"<p class='heading1'>Handling machine learning methods<\/p>\n\n\nThere is generally large interest surrounding data science, machine learning domain as a result of its large scale popularity.\nHowever, the domain is quite complex and there are many multidisciplinary facets involved when trying to put toghether all the necessary pieces needed for the entire \"machine\" to work well.\n\nMy interest and objective in this analysis is to identify and profile the kaggle survey respondents who brought models in production in contrast with professionals that are analyzing data or using ML methods, but haven't deployed their work in production. In the first case, the work is operationalized and implemented in production, while the latter category has either no need to deploy the model (when the final 'product' is packed as insights helping business decision makers) or they weren't succesfull in deploying the model in production for various reasons. \n\nBy profiling the two groups, we meet a secondary objective, which is to better understand and be able to assess professionals, teams or enterprises maturity in using ML methods.\n\n\nSurvey question 'Q22 Does your current employer incorporate machine learning methods into their business' marks off these two populations that are both using machine learning methods, but with different approach and purpose.\n\nBased on Q22 answers, the two groups have been segmented as such:\n* <p class='normal'><span class='high_expl'>EXPLORERS<\/span> - respondents choosing one of the two options below:<\/p>\n    <div class ='a'><span class='high_expl'>We are exploring ML methods (and may one day put a model into production)<\/span><\/div>\n    <div class ='a'><span class='high_expl'>We use ML methods for generating insights (but do not put working models into production)<\/span><\/div>\n\n\n* <p class='normal'><span class='high_prod'>PRODUCERS<\/span> - respondents choosing one of the two options below: <\/p>\n    <div class ='a'><span class='high_prod'>We have well established ML methods (i.e., models in production for more than 2 years)<\/span><\/div>\n    <div class ='a'><span class='high_prod'>We recently started using ML methods (i.e., models in production for less than 2 years)<\/span><\/div>\n\nThe analysis explores 2020 survey data and highlights the differences between the two groups. This notebook is structured around three main areas:\n    <div class ='a'>[General profile](#general_profile)<\/div>\n    <div class ='a'>[Skills](#skills)<\/div>\n    <div class ='a'>[Infrastructure used](#infrastructure)<\/div>\n\nNote: I have exluded from the analysis those who answered at Q22 that they are not using ML methods or do not know if their company has incorporated ML methods.","ce77b03d":"<p class=normal>Producers display a higher usage of algoritms and frameworks vs explorers, but <span class='high_all'>gradient boosting algorithms<\/span> are by far the most used solution in production environments.<\/p>","580e4a1c":"<p class='normal'>I hope this analysis brought more clarity in the way ML methods are being used and happy production or exploration for all ML users!<\/p>","bece6698":"<p class='normal'> Amazon Web Serices are preferred by people with models in production.<\/p>","caecdcf1":"<p class='heading2'>Conclusion<\/p>\n<p class='normal'>To conclude, the two maps below are summarizing the main differences between explorers and producers. The visuals are produced by a correspondence analysis taking as input the highest dissimilarities of the analyzed indicators between the two populations.<\/p>\n<p class='normal'>The separation between the two groups is mainly loaded on X axis. We can observe in the general profile map, the items revolving around <span class='high_prod'>producers<\/span> are related to higher salary ranges, bigger team size, bigger company size, more experience with code and ML and closely related to Data science job title.<\/p>","b45cec70":"<p class='normal'><span class='high_prod'>Producers<\/span> are definitely <span class='high_all'>employed in bigger companies<\/span> and are part of <span class='high_all'>bigger data science workload teams<\/span>: producers are included in teams of +20 members by +22pp vs explorers, while almost 30% of producers work in companies with more than 10k employees (+15pp vs. explorers). Producers are also <span class='high_all'>earning higher salaries<\/span> compared to explorers.<\/p>","baba2832":"<a id='infrastructure'><\/a>\n<p class='heading2'>Infrastructure used<\/p>","e675ea32":"**<p class=normal><span class='high_all'>Going in production requires more resources<\/span>, more infrastructure and running services, therefore more spending on ML products - about <span class='high_prod'>15% of all producers<\/span> have spend <span class='high_prod'>over 100k USD<\/span><\/p>**","a5e3e727":"<p class='normal'>Jupiter, VSCode and Vim\/Emacs combined usage is mostly preferred by producers, while Jupyter and PyCharm is the most popular choice in general.<\/p>"}}