{"cell_type":{"48140f19":"code","89d92965":"code","527f104d":"code","a9e78b83":"code","82d95d22":"code","5e54ae9a":"code","b7609e73":"code","9a14a0e5":"code","3b858190":"code","8a0048c1":"code","9c5b5b4d":"code","9155788e":"code","e96693aa":"code","cdb399e3":"code","5eb8b638":"code","da3632f4":"code","e84d86d4":"code","144e021a":"code","352dc6de":"code","95eab008":"code","44558e93":"code","47cedc00":"code","b8ea5d77":"code","7be92927":"code","4d94d277":"code","68844898":"code","c86ca4db":"code","03d1637d":"code","cf074123":"code","d15f53fe":"code","a517060a":"code","10555ebb":"code","2c0fac8f":"code","113d1aea":"code","d9ef8e1b":"code","87feff48":"code","32ad18ef":"code","bfdb4525":"code","862b9fac":"code","a308622b":"code","4d38b7a9":"code","65ab81a6":"code","ccb0cb41":"code","93c5157b":"code","c6c9da49":"code","4105449a":"markdown","966c636d":"markdown","fb801852":"markdown","4f455bb2":"markdown","669c119d":"markdown","315f46f6":"markdown","84b3123b":"markdown","2a689be4":"markdown","4e71d2e0":"markdown","cf9857c4":"markdown","fb5f86f2":"markdown","1a77752e":"markdown","97b62465":"markdown","a6dbc002":"markdown","b062f550":"markdown","a22a2b55":"markdown","bf7184a4":"markdown","0e12b54a":"markdown","f721c73b":"markdown","ebf46525":"markdown","d0bf471d":"markdown","35c9e8d2":"markdown","94d0a94d":"markdown","0908e13d":"markdown","e6a2534a":"markdown","c47f2cc5":"markdown","e8214a27":"markdown","7c5e0afc":"markdown","17af462d":"markdown","0517ebeb":"markdown","f22dabbe":"markdown","364492ff":"markdown","55d0621b":"markdown","09804f98":"markdown","a6e913b7":"markdown","b394b5a0":"markdown","1f9b7597":"markdown","08293703":"markdown","c45840be":"markdown"},"source":{"48140f19":"import numpy as np\nimport pandas as pd \nimport os\nimport cv2\nimport skimage.transform\nimport skimage.io\nimport matplotlib.pyplot as plt\nfrom random import seed\nfrom random import random\nfrom statistics import mean\nfrom shutil import move\n\ndata_dir1 = '\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/sunflower'\ndata_dir2 = '\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/dandelion'\ndata_dir3 = '\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/rose'\n\ndef ambilGambar(folder):\n    images = []\n    labels = []\n    x = 0\n    if folder == data_dir1:\n        for i in range(100):\n            y = 0\n            labels.append(y)\n    elif folder == data_dir2:\n        for i in range(100):\n            y = 1\n            labels.append(y)\n    elif folder == data_dir3:\n        for i in range(100):\n            y = 2\n            labels.append(y)\n    \n    for filename in os.listdir(folder):\n        img = cv2.imread(os.path.join(folder,filename))\n        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = cv2.resize(img, (320,240), interpolation = cv2.INTER_AREA) \n        if img is not None:\n            images.append(img)\n        x += 1\n        if x == 100: break\n    \n    return images, labels\n\nimage_sun, label_sun = ambilGambar(data_dir1)\nimage_dan, label_dan = ambilGambar(data_dir2)\nimage_ros, label_ros = ambilGambar(data_dir3)\n\nimage = image_sun + image_dan + image_ros\ntarget = label_sun + label_dan + label_ros\n\nprint(len(image))\nprint(len(target))","89d92965":"plt.figure(figsize = (20,20))\nfor i in range(3):\n    img = image[100*i]\n    plt.subplot(1,3,i+1)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(\"kelas ke-\" + str(target[100*i]))","527f104d":"def sigmoid(x):\n    return 1.0 \/(1.0+np.exp(-x))","a9e78b83":"from random import seed\nfrom random import random\nseed(1)\n \ndef initBobot(layer):\n    nLayer = len(layer)\n    bias = [np.random.randn(y,1) for y in layer[1:]]\n    w = [np.random.randn(y,x) for x,y in zip(layer[:-1], layer[1:])]\n    return [bias, w, nLayer]","82d95d22":"def cost_func(a, y):\n    return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))","5e54ae9a":"def feedforward(x):\n    weight = x\n    weightNet = [x]\n    dotProd = []\n    for b, w in zip(Theta[0], Theta[1]):\n        dots = np.dot(w, weight) + b\n        dotProd.append(dots)\n        weight = sigmoid(dots)\n        weightNet.append(weight)\n        \n    return dotProd, weightNet","b7609e73":"def backpropagation(x,y):\n    deltaBias = [np.zeros(b.shape) for b in Theta[0]]\n    deltaW = [np.zeros(w.shape) for w in Theta[1]]\n    \n    dotProd, weight = feedforward(x)\n    \n    loss = cost_func(weight[-1], y)\n    delta_cost = weight[-1] - y\n    \n    delta = delta_cost\n    \n    deltaBias[-1] = delta\n    deltaW[-1]= np.dot(delta, weight[-2].T)\n    \n    for x in range(2, Theta[2]):\n        dots = dotProd[-x]\n        deltaA = delta_sigmoid(dots)\n        delta = np.dot(Theta[1][-x + 1].T, delta) * deltaA\n        deltaBias[-x] = delta\n        deltaW[-x] = np.dot(delta, weight[-x - 1].T)\n        \n    return(loss, deltaBias, deltaW)","9a14a0e5":"def prediksi(X):\n    prediksi = np.array([])\n    labels = [\"sunflower\",\"dandelion\",\"rose\"]\n    for x in X:\n        dots, weight = feedforward(x)\n        prediksi = np.append(prediksi, np.argmax(weight[-1]))\n    prediksi = np.array([labels[int(p)] for p in prediksi])\n    return prediksi","3b858190":"def akurasi(X, y):\n    tmp = 0\n    for x, _y in zip(X, y):\n        dots, weight = feedforward(x)\n        \n        if np.argmax(weight[-1]) == np.argmax(_y):\n            tmp += 1\n    ak = (float(tmp) \/ X.shape[0]) * 100\n    return ak","8a0048c1":"from sklearn.model_selection import train_test_split as split\ndef train_test_split (X, Y, trainSize):\n    X_train, X_test, Y_train, Y_test = split(X, Y, train_size = trainSize)\n    return X_train, X_test, Y_train, Y_test","9c5b5b4d":"def one_hot_encode(target):\n    kelas = np.max(target) + 1\n    encoded = np.eye(kelas)[target]\n    return encoded","9155788e":"def normalisasiPixel(data):\n    return data\/255","e96693aa":"image = np.array(image)\nimage = normalisasiPixel(image)\nimage.shape","cdb399e3":"nNeurons = image.shape[1] * image.shape[2] * image.shape[3]\nnNeurons","5eb8b638":"X_train, X_test, Y_train, Y_test = train_test_split(image, target, 0.8)","da3632f4":"X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, 0.8)","e84d86d4":"X_train_CNN = X_train\nX_test_CNN = X_test\nX_val_CNN = X_val\nY_train_CNN = Y_train\nY_test_CNN = Y_test\nY_val_CNN = Y_val","144e021a":"def reshape_x(data,a):\n    data = data.reshape(-1,a,1)\n    return data","352dc6de":"X_train = reshape_x(X_train, nNeurons)\nX_test = reshape_x(X_test, nNeurons)\nX_val = reshape_x(X_val, nNeurons)","95eab008":"Y_train = np.array(Y_train)\ny_train_enc = one_hot_encode(Y_train)","44558e93":"y_train_enc = y_train_enc.reshape(-1, 3, 1)","47cedc00":"alpha = 0.1\nepoch = 300\nTheta = initBobot([X_train[0].size, 128, y_train_enc[0].size])\nlen(Theta)","b8ea5d77":"def getBatch(X, y, size):\n    for idx in range(0, X.shape[0], size):\n        batch = zip(X[idx:idx+size],\n                   y[idx:idx+size])\n        yield batch","7be92927":"def delta_sigmoid(x):\n    return sigmoid(x) * (1- sigmoid(x))","4d94d277":"batch_size = 32\nnBatch = int(X_train.shape[0] \/ batch_size)","68844898":"Y_val = np.array(Y_val)\ny_val_enc = one_hot_encode(Y_val)\ny_val_enc = y_val_enc.reshape(-1, 3, 1)","c86ca4db":"historyLoss = []\nhistoryAcc = []\nfor j in range(epoch):\n    batch_iter = getBatch(X_train, y_train_enc, batch_size)\n    for i in range(nBatch):\n        batch = next(batch_iter)\n        deltaBias = [np.zeros(b.shape) for b in Theta[0]]\n        deltaW = [np.zeros(w.shape) for w in Theta[1]]\n        for batch_X, batch_Y in batch:\n            loss, delta_deltaBias, delta_deltaW = backpropagation(batch_X, batch_Y)\n            deltaBias = [db + ddb for db, ddb in zip(deltaBias, delta_deltaBias)]\n            deltaW = [dw + ddw for dw, ddw in zip(deltaW, delta_deltaBias)]\n    Theta[1] = [w - (alpha\/batch_size)*delw for w, delw in zip(Theta[1], deltaW)]\n    Theta[0] = [b - (alpha\/batch_size)*delb for b, delb in zip(Theta[0], deltaBias)]\n    historyLoss.append(loss)\n    acc = akurasi(X_val, y_val_enc)\n    historyAcc.append(acc)\n    print(\"\\nEpoch : %d\\tLoss: %f\\tAkurasi: %f\\n\"%(j, loss, acc))","03d1637d":"Y_test_arr = np.array(Y_test)\ny_test_enc = one_hot_encode(Y_test_arr)\ny_test_enc = y_test_enc.reshape(-1, 3, 1)","cf074123":"akurasi(X_test, y_test_enc)","d15f53fe":"print(prediksi(X_test))","a517060a":"plt.plot(range(len(historyLoss)),historyLoss)\nplt.xlabel('epoch')\nplt.show()","10555ebb":"plt.plot(range(len(historyAcc)),historyAcc)\nplt.xlabel('epoch')\nplt.show()","2c0fac8f":"len(Theta[0])","113d1aea":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D,MaxPool2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator","d9ef8e1b":"from keras.utils.np_utils import to_categorical\nY_train_CNN = to_categorical(Y_train_CNN,num_classes = 3)\nY_test_CNN = to_categorical(Y_test_CNN,num_classes = 3)\nY_val_CNN = to_categorical(Y_val_CNN,num_classes = 3)","87feff48":"plt.figure(figsize = (20,20))\nfor i in range(3):\n    img = X_train_CNN[64*i]\n    plt.subplot(1,3,i+1)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(Y_train_CNN[64*i])\nplt.show()","32ad18ef":"panjang = 320\nlebar = 240","bfdb4525":"model = Sequential()\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),padding=\"Same\",activation=\"relu\" , input_shape = (lebar,panjang,3)))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(256,activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(3,activation=\"softmax\"))\n\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])","862b9fac":"model.compile(loss='categorical_crossentropy',\n              optimizer=Adam(lr=0.001),\n              metrics=['accuracy'])","a308622b":"epoch = 300\nbatch_size = 32","4d38b7a9":"datagen = ImageDataGenerator(\n    featurewise_center=False,\n    samplewise_center=False, \n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    rotation_range=60,\n    zoom_range = 0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    fill_mode = \"reflect\"\n    ) \ndatagen.fit(X_train_CNN)","65ab81a6":"history = model.fit_generator(datagen.flow(X_train_CNN,Y_train_CNN,batch_size=batch_size),epochs= epoch,validation_data=(X_val_CNN,Y_val_CNN),\n                              steps_per_epoch=X_train_CNN.shape[0] \/\/ batch_size)","ccb0cb41":"print(\"Test Accuracy: {0:.2f}%\".format(model.evaluate(X_test_CNN,Y_test_CNN)[1]*100))","93c5157b":"plt.plot(range(len(history.history['accuracy'])),history.history['accuracy'])\nplt.xlabel('epoch')\nplt.show()","c6c9da49":"plt.plot(range(len(history.history['loss'])),history.history['loss'])\nplt.xlabel('epoch')\nplt.show()","4105449a":"Validasi data digunakan untuk menghitung akurasi tiap epoch sehingga tidak menggunakan data test","966c636d":"# Akurasi\nSama seperti prediksi, dimana nilai dari bobot dikenai fungsi feedforward hingga output layer pada image yang ada. Namun, jika sudah diketahui hasilnya maka akan dibandingkan dengan nilai label aslinya. Kemudian nilai tersebut dijumlahkan apabila benar dan diratarata dengan jumlah data yang ada dan dikali 100%.","fb801852":"Data image dan target di split menggunakan fungsi yang telah didefinisikan sebelumnya. Train data = 0.8 -> 80% , Test data= 0.2 -> 20%","4f455bb2":"# B. MULTI LAYER PERCEPTRON","669c119d":"Contoh gambar tiap kelas yang berhasil di load","315f46f6":"# Normalisasi Pixel\nNilai dari tiap pixel diubah menjadi range 0 hingga 1. Dibagi 255 dikarenakan array dari image tersebut hingga 255 saja nilainya.","84b3123b":"Sehingga setelah di split semua akan menghasilkan. Terdapat 300 data, maka :\nTrain Data = 0.8x0.8x300 = 192\nValidasi Data = 0.2x0.8x300 = 40\nTest Data = 0.2 x 300 = 60","2a689be4":"# Plot Loss untuk tiap EPOCH\nloss dari tiap epoch disimpan dalam historyLoss","4e71d2e0":"Fungsi Aktivasi dari convolutional layer hingga hidden layer yaitu menggunakan ReLU, sedangkan untuk output layer fungsi aktivasinya adalah SOFTMAX.\n\nRELU (X) = max (0,X)","cf9857c4":"# **A. LOAD DATASET**\nSaya menggunakan kaggle notebook sehingga direktori file berada di cloud kaggle. Lalu menggunakan library openCV untuk membaca pixel tiap gambar.\nGambar yang di load disimpan di dalam list images dan diberi batasan 100 gambar. Dikarenakan terdapat 3 bunga, sehingga fungsi load gambar dipanggil tiap folder bunga (Sunflower, Dandelion, dan Rose) dan diberi label tiap folder. Oleh karena itu didapatkan total 300 gambar[](http:\/\/)","fb5f86f2":"# Fungsi Sigmoid\nFungsi aktivasi **sigmoid** untuk normalisasi nilai.","1a77752e":"Learning rate = 0.1 dan jumlah epoch = epoch = 300.","97b62465":"**Data dibagi menjadi beberapa batch untuk mempercepat proses kalkulasi**","a6dbc002":"# Plot Akurasi untuk tiap EPOCH\nAkurasi dari tiap epoch disimpan dalam historyAcc yang dihasilkan dari fungsi akurasi dengan input data validasi yang telah digenerate sebelumnya.","b062f550":"# **UAS IMPLEMENTASI**\n# **FAQIH ETHANA PRABANDARU**\n# **17\/409433\/PA\/17740**","a22a2b55":"# PLOT ACCURACY dan LOSS","bf7184a4":"# Compile Model\nModel di compile menggunakan optimasi Adam dengan learning rate 0.001 dan loss functionnya menggunakan categorical cross entropy. Dan akurasi disimpan dalam matrik Accuracy","0e12b54a":"# Architecture\n1. Convolutional layer dengan ukuran kernel (3,3) dengan banyak filter 64\n2. Max pooling layer dengan pool size(2,2)\n3. Flatten layer ntuk mengubah data menjadi bentuk 1 Dimensi\n4. 1 Hidden layer pada fully connected layer dengan neuron berjumlah 256\n5. 1 Output layer dengan jumlah neuron sebanyak 3","f721c73b":"Menggenerate batch dari data tensor image dengan augmentasi data secara real-time.","ebf46525":"# C. Convolutional Neural Network","d0bf471d":"Mengubah target menjadi categorical yaitu berbentuk one hot encoding","35c9e8d2":"# Cost Function\nFungsi dari cost\/loss disini menggunakan **Sigmoid cross entropy loss** dikarenakan data berupa kategorikal. ","94d0a94d":"# Inisialisasi Bobot\nBobot diinisialisasi menggunakan distribusi Gaussian dengan mean 0, dan variance 1.\nlayer didefinisikan sebagai list dari jumlah neuron di input, hidden, dan output.\nJumlah hidden layer juga dapat didapatkan dari panjang list dikurangi 2 dimana 2 tersebut yaitu input dan output layer.","0908e13d":"# PROSES TRAINING DATA\nData yang di train adalah data train dengan epoch 300. Kemudian untuk menghitung akurasi tiap epoch digunakan data validasi.","e6a2534a":"nNeurons merupakan jumlah neuron pada input layer (Jumlah pixel pada gambar -> Panjang * Lebar * 3)","c47f2cc5":"Contoh Gambar","e8214a27":"Kemudian dilakukan inisialisasi bobot dengan jumlah neuron pada input layer = jumlah pixel\/nneurons\/230400, hidden layer = 100, output layer = 3. Hasil dari inisialisasi menghasilkan bobot pada bias (Theta[0]), bobot pada neuron (Theta[1]), dan jumlah layernya (Theta[2])","7c5e0afc":"Menyimpan variabel untuk panjang dan lebar gambar sesuai perintah. (320x240)","17af462d":"# Import Library yang diperlukan","0517ebeb":"Menyimpan variabel image dan label pada train, test dan validasi untuk digunakan pada CNN sehingga data yang digunakan sama untuk membandingkan akurasi yang dihasilkan.","f22dabbe":"# Fungsi Feedforward\nFeedforward merupakan jumlahan perkalian dari bobot dengan data neuron untuk tiap layer. Kemudian hasilnya dikenai fungsi aktivasi sigmoid untuk menormalisasi nilainya.","364492ff":"Data Data image dan target pada DATA TRAIN yang sebelumnya di generate, di split lagi menjadi Train Data = 0.8, dan Validasi Data = 0.2","55d0621b":"# Prediksi\nBerdasarkan bobot yang telah didapatkan pada proses training, maka bobot tersebut diimplementasikan pada gambar yang akan diprediksi untuk memprediksi label(jenis bunga)nya. Didapatkan dengan cara feedforward hingga ke output layer yang juga telah di aktivasi untuk menentukan labelnya. Kemudian output fungsi ini akan menghasilkan label pada gambar masukkan.","09804f98":"# Backpropagation->backward\nPada tahap ini digunakan untuk menyesuakian kembali weight dan bias berdasarkan error\/loss pada saat feedforward.\n1. Menghitung gradient dari loss function terhadap semua parameter yang ada dengan cara mencari partial derivative (turunan parsial) dari fungsi tersebut. \n2. Update semua parameter (weight dan bias) menggunakan Stochastic Gradient Descent (SGD) dengan mengurangi atau menambahkan nilai weight lama dengan \u201csebagian\u201d (learning rate) dari nilai gradient yang sudah didapatkan.","a6e913b7":"# Fungsi One Hot Encoding\nData label\/target diubah menjadi bentuk one hot encoding dari semula berbentuk list angka biasa. One hot encoding merupakan cara lain dari representasi kelas yaitu terdapat n neuron pada output layer apabila terdapat n kelas. Dalam hal ini n = 3, yaitu Dandelion, Sunflower, dan Rose.","b394b5a0":"**List dari gambar diubah menjadi bentuk array agar lebih mudah dikomputasinya**. Kemudian menghasilkan 4 nilai\/shape. Pertama merupakan jumlah dari gambar, Kedua merupakan lebar gambar, ketiga merupakan panjang gambar, dan yang terakhir merupakan RGB channel sehingga menghasilkan nilai 3.","1f9b7597":"# Split DATA TRAIN dengan DATA TEST\nDisini menggunakan library dari sklearn yaitu train_test_split dan parameternya train_size berdasarkan inputan.","08293703":"# Created by : Faqih Ethana Prabandaru","c45840be":"# Proses Training\nTraining dikerjakan selama 300 epoch, menggunakan data dan label train. Bobot dan bias diupdate melalui backpropagation untuk tiap batch menggunakan SGD. deltaBias dan deltaW mengandung gradien yang digunakan untuk update bobot dan bias. "}}