{"cell_type":{"59718cfc":"code","3468e65a":"code","381cbdce":"code","1cde4780":"code","5db183cb":"code","a34bb3eb":"code","b0ff40ee":"code","12b116d2":"code","88324553":"code","87f3a6ba":"code","7cb2c3d1":"code","2a0d9272":"code","077571cd":"code","377d293c":"code","968583f6":"code","c948fa5a":"code","e1266f1d":"code","b6ee2efe":"code","92244f98":"code","1dceb2d6":"code","5973ece5":"code","4030c04c":"code","f357444d":"code","5ecf4561":"code","20bea38c":"code","42fccf2a":"code","99413da2":"code","206ee84d":"code","33f94171":"code","4cecbd3d":"code","c830c6c4":"code","0b75c2fa":"code","ee0ff49d":"code","a7edd5fb":"code","87a74e19":"code","95683375":"code","701ca228":"code","443e51ec":"code","4f2e87b3":"code","8ed00831":"code","883c823c":"code","ba355d1b":"code","ae01239c":"code","1df7ffde":"code","7219d3b0":"code","fb2a3612":"code","166b34ff":"code","e5729ec3":"code","dc7c4e4d":"code","32e08d50":"code","482a93b1":"code","abbc6b47":"code","aa5ec8f5":"code","c359315a":"code","88a76e3f":"code","8e19fdec":"code","00c2db02":"code","9586246b":"code","0db7f3e2":"code","3451f05f":"code","613c2e10":"code","366e45df":"code","7e48a117":"code","c249e46e":"code","ab41cc98":"code","ff2a1c15":"markdown","49e23a2d":"markdown","ea268f9b":"markdown","1c840347":"markdown","481225d0":"markdown","e580f7d5":"markdown","b6417ca0":"markdown","991f26fc":"markdown","c866e582":"markdown"},"source":{"59718cfc":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3468e65a":"df = pd.read_csv(\"..\/input\/iykra-odin\/tweet_matematika.csv\")\ndf.head()","381cbdce":"df_tweet = df[['text']]","1cde4780":"df_tweet","5db183cb":"df_tweet['length'] = df_tweet['text'].apply(len)\ndf_tweet.head()","a34bb3eb":"df_tweet['length'].plot(bins=100, kind='hist') ","b0ff40ee":"df_tweet.describe()","12b116d2":"df_tweet[df_tweet['length'] == 31]['text'].iloc[0]","88324553":"!pip install tweet-preprocessor","87f3a6ba":"import preprocessor as p","7cb2c3d1":"# Sebelum diclean\ndf_tweet['text'][0]","2a0d9272":"#Setelah clean\np.clean(df_tweet['text'][0])","077571cd":"p.clean(df_tweet['text'][0]).replace('\/', ' ')","377d293c":"# Ganti \\n dengan spasi\ndf_tweet['text'] = [x.replace('\\n', ' ') for x in df_tweet['text']]","968583f6":"df_tweet['tweet_pre'] = df_tweet['text'].apply(p.clean)\n# Ganti \/ dengan spasi\ndf_tweet['tweet_pre'] = [x.replace('\/', ' ') for x in df_tweet['tweet_pre']]","c948fa5a":"df_tweet['length_pre'] = df_tweet['tweet_pre'].apply(len)","e1266f1d":"df_tweet.head(10)","b6ee2efe":"df_tweet.iloc[122][0]","92244f98":"df_tweet.iloc[122][2]","1dceb2d6":"slang = pd.read_csv(\"..\/input\/iykra-odin\/colloquial-indonesian-lexicon.csv\")\nslang = dict(zip(slang['slang'], slang['formal']))\nslang = {r\"\\b{}\\b\".format(k): v for k, v in slang.items()}\nslang","5973ece5":"df_tweet['tweet_form'] = df_tweet['tweet_pre'].replace(slang, regex=True)","4030c04c":"df_tweet.head(10)","f357444d":"words_lst_before = []\nwords_lst_after = []\nfor x in range(len(df_tweet)):\n    words = df_tweet['tweet_form'][x].split()\n    words = [x.lower() for x in words]\n    words_lst_after.extend(words)\n    \n    words = df_tweet['tweet_pre'][x].split()\n    words = [x.lower() for x in words]\n    words_lst_before.extend(words)\n    \nprint(\"Total unique vocab sebelum koreksi slang:\", len(pd.DataFrame(words_lst_before).value_counts()))\nprint(\"Total unique vocab setelah koreksi slang:\", len(pd.DataFrame(words_lst_after).value_counts()))","5ecf4561":"import string\nstring.punctuation","20bea38c":"!pip install Sastrawi","42fccf2a":"from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\nfactory = StopWordRemoverFactory()\nstopwords = factory.get_stop_words()\nprint(stopwords)","99413da2":"message = 'bisa-bisanya'\nblk = ''\nfor char in message:\n    if char not in string.punctuation:\n        blk += char\n    else:\n        blk += \" \"\nprint(blk)","206ee84d":"def message_cleaning(message):\n    # test_punc_removed = [char for char in message if char not in string.punctuation]\n    test_punc_removed = ''\n    for char in message:\n        if char not in string.punctuation:\n            test_punc_removed += char\n        else:\n            test_punc_removed += \" \"\n            \n    test_punc_removed_join = ''.join(test_punc_removed)\n    test_punc_removed_join_clean = [word for word in test_punc_removed_join.split() if word.lower() not in stopwords]\n    test_punc_removed_join_clean = \" \".join(test_punc_removed_join_clean)\n    return test_punc_removed_join_clean","33f94171":"df_tweet['tweet_clean'] = df_tweet['tweet_form'].apply(message_cleaning)","4cecbd3d":"df_tweet.head()","c830c6c4":"df_tweet['length_clean'] = df_tweet['tweet_clean'].apply(len)","0b75c2fa":"df_tweet['length_clean'].plot(bins=100, kind='hist') ","ee0ff49d":"df_tweet['text'][124]","a7edd5fb":"df_tweet['tweet_pre'][124]","87a74e19":"df_tweet['tweet_form'][124]","95683375":"df_tweet['tweet_clean'][124]","701ca228":"words_lst_clean = []\nfor x in range(len(df_tweet)):\n    words = df_tweet['tweet_clean'][x].split()\n    words = [x.lower() for x in words]\n    words_lst_clean.extend(words)\n    \nprint(\"Total unique vocab sebelum koreksi slang:\", len(pd.DataFrame(words_lst_before).value_counts()))\nprint(\"Total unique vocab setelah koreksi slang:\", len(pd.DataFrame(words_lst_after).value_counts()))\nprint(\"Total unique vocab setelah buang punctuation dan stopwords:\", len(pd.DataFrame(words_lst_clean).value_counts()))","443e51ec":"df_tweet['tweet_regex'] = df_tweet['tweet_clean'].astype(str).str.replace(r'([a-zA-Z])\\1+', r'\\1')\n\nwords_lst_regex = []\nfor x in range(len(df_tweet)):\n    words = df_tweet['tweet_regex'][x].split()\n    words = [x.lower() for x in words]\n    words_lst_regex.extend(words)\n    \nprint(\"Total unique vocab sebelum koreksi slang:\", len(pd.DataFrame(words_lst_before).value_counts()))\nprint(\"Total unique vocab setelah koreksi slang:\", len(pd.DataFrame(words_lst_after).value_counts()))\nprint(\"Total unique vocab setelah buang punctuation dan stopwords:\", len(pd.DataFrame(words_lst_clean).value_counts()))\nprint(\"Total unique regex:\", len(pd.DataFrame(words_lst_regex).value_counts()))","4f2e87b3":"from sklearn.feature_extraction.text import CountVectorizer","8ed00831":"corpus_1 = [x.lower() for x in df_tweet['tweet_regex']]","883c823c":"import regex as re\npattern = r'[0-9]'\nnew_corpus = [re.sub(pattern, '', x) for x in corpus_1]","ba355d1b":"vectorizer = CountVectorizer()\nbow_model = vectorizer.fit_transform(new_corpus)\nprint(bow_model)","ae01239c":"vectorizer.get_feature_names()","1df7ffde":"bow_model_df = pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names()) \nbow_model_df","7219d3b0":"from sklearn.feature_extraction.text import TfidfVectorizer","fb2a3612":"vectorizer = TfidfVectorizer()\ntfidf_model = vectorizer.fit_transform(new_corpus)\nprint(tfidf_model)","166b34ff":"print(tfidf_model.toarray())","e5729ec3":"tfidf_model_df = pd.DataFrame(tfidf_model.toarray(), columns = vectorizer.get_feature_names()) \ntfidf_model_df.iloc[190:200,600:650]","dc7c4e4d":"!pip install flair","32e08d50":"from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\ncorpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_INDONESIAN)","482a93b1":"tag_type = 'upos' # bisa 'ner'\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)","abbc6b47":"from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, BertEmbeddings\nfrom typing import List\nembedding_types: List[TokenEmbeddings] = [\n WordEmbeddings('id-crawl'),\n WordEmbeddings('id'),\n]\nembeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)","aa5ec8f5":"from flair.models import SequenceTagger\ntagger: SequenceTagger = SequenceTagger(hidden_size=128,\n                                        embeddings=embeddings,\n                                        tag_dictionary=tag_dictionary,\n                                        tag_type=tag_type,\n                                        use_crf=True)","c359315a":"from flair.trainers import ModelTrainer\ntrainer: ModelTrainer = ModelTrainer(tagger, corpus)\ntrainer.train('resources\/taggers\/example-universal-pos',\n    learning_rate=0.1,\n    mini_batch_size=32,\n    max_epochs=10)","88a76e3f":"from flair.data import Sentence\nsentence = Sentence('saya dan dia kemarin pegi ke pasar bersama untuk membeli jeru')\ntag_pos = SequenceTagger.load('resources\/taggers\/example-universal-pos\/best-model.pt')\ntag_pos.predict(sentence)\nprint(sentence.to_tagged_string())","8e19fdec":"sentence = Sentence(df_tweet['tweet_clean'][0])\ntag_pos.predict(sentence)\nprint(sentence.to_tagged_string())","00c2db02":"sentence = Sentence(df_tweet['tweet_clean'][590])\ntag_pos.predict(sentence)\nprint(sentence.to_tagged_string())","9586246b":"import pickle\nimport spacy\nimport random\nfrom spacy.util import minibatch, compounding\nfrom spacy import load, displacy","0db7f3e2":"with open('..\/input\/iykra-odin\/ner_spacy_fmt_datasets.pickle', 'rb') as f:\n    ner_spacy_fmt_datasets = pickle.load(f)","3451f05f":"nlp=spacy.blank(\"id\")\nnlp.add_pipe(nlp.create_pipe('ner'))\nnlp.begin_training()","613c2e10":"ner=nlp.get_pipe(\"ner\")\npipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\nunaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]","366e45df":"for _, annotations in ner_spacy_fmt_datasets:\n    for ent in annotations.get(\"entities\"):\n        ner.add_label(ent[2])\n        break","7e48a117":"# TRAINING THE MODEL\nwith nlp.disable_pipes(*unaffected_pipes):\n\n  # Training for 5 iterations\n  for iteration in range(5):\n\n    # shuufling examples  before every iteration\n    random.shuffle(ner_spacy_fmt_datasets)\n    losses = {}\n    # batch up the examples using spaCy's minibatch\n    batches = minibatch(ner_spacy_fmt_datasets, size=compounding(4.0, 32.0, 1.001))\n    for batch in batches:\n        texts, annotations = zip(*batch)\n        nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,  # dropout - make it harder to memorise data\n                    losses=losses,\n                )\n    \n    print(\"Losses at iteration {}\".format(iteration), losses)","c249e46e":"doc = nlp(df_tweet['tweet_clean'][120])\n\nprint(doc.ents)\nprint(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])","ab41cc98":"for i in range(955,985):\n  doc = nlp(df_tweet['tweet_clean'][i])\n  if len(doc.ents) > 0:\n    print(\"Isi Tweet:\", df_tweet['tweet_clean'][i])\n    print(doc.ents)\n    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n    print(\" \")","ff2a1c15":"# **Twitter NLP Draft - ODIN**\n\nData dikumpulkan menggunakan package *tweepy* dengan keyword *matematika*. Total tweet yang terkumpul sebanyak **2500 tweets.**","49e23a2d":"# td-idf","ea268f9b":"# NER\n\nhttps:\/\/yudanta.github.io\/posts\/train-an-indonesian-ner-from-a-blank-spacy-model\/","1c840347":"# Count vec","481225d0":"## Ganti kata slang\n\nPake regex: https:\/\/www.regular-expressions.info\/wordboundaries.html","e580f7d5":"# POS Tagging","b6417ca0":"# Buang hastag, link, emoji, pic","991f26fc":"cita-cita -> citacita, harusnya cita cita\n\nbisa-bisanya -> bisabisanya, harusnya bisa bisanya\n\nSaya mau pergi ke pasar.Tapi -> pasartapi","c866e582":"## Buang tanda baca dan stopwords"}}