{"cell_type":{"50088f89":"code","88dd4b82":"code","df58ae18":"code","a95462b4":"code","133b65b8":"code","e75ce6dd":"code","30d6fea7":"code","4501f3f0":"code","e0bbd03f":"code","52785e74":"code","1c925389":"markdown","4cfce77a":"markdown","5ef0c42d":"markdown","34ed89d7":"markdown","b0683f83":"markdown","bf739199":"markdown","fce3b1b3":"markdown"},"source":{"50088f89":"import numpy as np \nimport pandas as pd \n\nimport torch \nimport seaborn as sns","88dd4b82":"data = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\ndata.head()","df58ae18":"def compute_class_freqs(labels):\n    \n    labels = np.array(labels)\n    \n    N = labels.shape[0]\n    \n    positive_frequencies = np.sum(labels,axis = 0) \/ N\n    negative_frequencies = 1 - positive_frequencies\n    \n    return positive_frequencies, negative_frequencies","a95462b4":"freq_pos, freq_neg = compute_class_freqs(data['target'])","133b65b8":"df = pd.DataFrame({\"Targets\": ['0'], \"Label\": [\"Negative\"], \"Value\": freq_neg})\ndf = df.append({\"Targets\": '1', \"Label\": \"Positive\", \"Value\": freq_pos}, ignore_index=True)\nsns.barplot(x=\"Targets\", y=\"Value\" ,data=df)","e75ce6dd":"pos_weights = freq_neg\nneg_weights = freq_pos\npos_contribution = freq_pos * pos_weights \nneg_contribution = freq_neg * neg_weights","30d6fea7":"df = pd.DataFrame({\"Targets\": ['0'], \"Label\": [\"Negative\"], \"Value\": neg_contribution})\ndf = df.append({\"Targets\": '1', \"Label\": \"Positive\", \"Value\": pos_contribution}, ignore_index=True)\nf = sns.barplot(x=\"Targets\", y=\"Value\" ,data=df)","4501f3f0":"class W_BCEWithLogitsLoss(torch.nn.Module):\n    \n    def __init__(self, w_p = None, w_n = None):\n        super(W_BCEWithLogitsLoss, self).__init__()\n        \n        self.w_p = w_p\n        self.w_n = w_n\n        \n    def forward(self, logits, labels, epsilon = 1e-7):\n        \n        ps = torch.sigmoid(logits.squeeze()) \n        \n        loss_pos = -1 * torch.mean(self.w_p * labels * torch.log(ps + epsilon))\n        loss_neg = -1 * torch.mean(self.w_n * (1-labels) * torch.log((1-ps) + epsilon))\n        \n        loss = loss_pos + loss_neg\n        \n        return loss","e0bbd03f":"targets = torch.tensor([0, 0, 1, 0, 1, 0, 0, 0 ,1, 0 ,0, 0]).float()\nlogits = torch.zeros_like(targets)  # probs will be 0.5\n\nw_p = len(torch.where(targets == 0)[0]) \/ len(targets)\nw_n = 1 - w_p \n\ncriterion1 = W_BCEWithLogitsLoss(w_p, w_n)\ncriterion2 = torch.nn.BCEWithLogitsLoss()","52785e74":"print(\"Loss from Weighted BCE : {}\".format(criterion1(logits, targets)))\nprint(\"Loss from BCE : {}\".format(criterion2(logits, targets)))","1c925389":"Contributions of positive cases is significantly lower than that of the negative ones. However, we want the contributions to be equal. One way of doing this is by multiplying each example from each class by a class-specific weight factor, $pos_{weights}$ and $neg_{weights}$, so that the overall contribution of each class is the same.\n\nTo have this, we want\n\n$$pos_{weights} \\times freq_{p} = neg_{weights} \\times freq_{n},$$\nwhich we can do simply by taking\n\n$$pos_{weights} = freq_{neg}$$$$neg_{weights} = freq_{pos}$$\nThis way, we will be balancing the contribution of positive and negative labels.","4cfce77a":"# Weighted Binary Cross Entropy ","5ef0c42d":"<center><img src=\"https:\/\/i.pinimg.com\/originals\/ba\/e0\/5d\/bae05dffdcb9efdd4430d877febe6dbf.png\" width=\"440\" height=\"440\" ><\/center>","34ed89d7":"- If we multiply pos weights to positive loss and neg weights to negative loss, we see an equal contribution of loss.","b0683f83":"# Compute positive and negative frequencies","bf739199":"- If we calculate BCE loss of individual examples, total loss coming from positive examples contributes less than the total loss coming from negative examples ","fce3b1b3":"<center><img src=\"https:\/\/i.pinimg.com\/originals\/a6\/62\/32\/a66232f444f7eb9452c2868a37c3be0c.png\" width=\"440\" height=\"440\" ><\/center>"}}