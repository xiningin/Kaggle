{"cell_type":{"def9cd8f":"code","46e9bbb6":"code","d2900902":"code","576cb46b":"code","90054716":"code","a3ac0439":"code","e424772f":"code","430b08fa":"code","cac8c1af":"code","fe175c0f":"code","bd4f6b74":"code","a81a0499":"code","6608e6fe":"code","a7fe2213":"code","68137f09":"code","5569e8f1":"code","502cd730":"code","da5c9497":"code","c8a0de86":"code","49d74dfc":"code","59e64dcf":"code","7e0deb34":"code","38fcfa91":"code","383e4bfa":"code","3bc6f893":"code","ec54e96d":"code","f9a00e97":"code","72197d2f":"code","cb83b40a":"code","d9c8bce0":"code","9634d455":"code","b50a25c5":"code","55be0bc2":"code","551c02d2":"code","15e49d51":"code","53fb0aef":"code","f854857b":"code","9395fcca":"code","06750e1e":"code","2c81fe43":"code","fc512220":"code","01fa416b":"code","66fbe162":"code","6b93bbb4":"code","bbc5e3cc":"code","4cf10d54":"code","a517da67":"code","a218794f":"code","b0bdfb3d":"code","8ef04f76":"code","911f3d37":"markdown"},"source":{"def9cd8f":"#Importing dependencies\n\n# Standard Python Imports\nfrom timeit import default_timer as timer\nimport time, datetime\nimport os\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Data Visualization \nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# For Data Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\ntry:\n    from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+\nexcept ImportError:\n    from sklearn.preprocessing import Imputer as SimpleImputer\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import FeatureUnion\n# Algorithms\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.svm import LinearSVC\nfrom sklearn import svm\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\n# Validation & Scoring\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n# Ignoring the warnings that we will see in this notebook\nimport warnings\nwarnings.filterwarnings('ignore')","46e9bbb6":"# Importing Data\ndef fetch_data():\n    data = pd.read_csv(\".\/..\/input\/mushrooms.csv\")\n    return data\ndata = fetch_data()","d2900902":"msno.matrix(data)","576cb46b":"data.describe()","90054716":"data.head()","a3ac0439":"data.tail()","e424772f":"data.get_dtype_counts()","430b08fa":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(data, data[\"class\"]):\n    train = data.loc[train_index]\n    test = data.loc[test_index]","cac8c1af":"sns.countplot(y=\"class\", data=data)","fe175c0f":"sns.countplot(y=\"class\", data=train)","bd4f6b74":"sns.countplot(y=\"class\", data=test)","a81a0499":"Y_train = train[\"class\"]\nX_train = train.iloc[:,1:]\n\nY_test = test[\"class\"]\nX_test = test.iloc[:,1:]","6608e6fe":"X_train.head()","a7fe2213":"class DropColumns(BaseEstimator, TransformerMixin):\n    def __init__(self, column_names=[]):\n        self.column_names = column_names\n    def transform(self, df, y=None):\n        return df.drop(self.column_names, axis=1)\n    def fit(self, df, y=None):\n        return self","68137f09":"class ColumnExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self, column_names=[]):\n        self.column_names = column_names\n    def transform(self, df, y=None):\n        return df.loc[:, self.column_names]\n    def fit(self, df, y=None):\n        return self","5569e8f1":"class FeatureNormalizer(BaseEstimator, TransformerMixin):\n    def __init__(self, column_names=[]):\n        self.column_names = column_names\n        self.min_max_scalar = MinMaxScaler()\n    def fit(self, X, y=None):\n        self.min_max_scalar.fit(X[self.column_names])\n        return self\n    def transform(self, X, y=None):\n        X[self.column_names] = self.min_max_scalar.transform(X[self.column_names])\n        return X","502cd730":"class MissingStalkRoots(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        X[\"stalk-root\"] = X[\"stalk-root\"].replace(['?'], 'm')\n        return self\n    def transform(self, X, y=None):\n        return X","da5c9497":"class ReplacingVeilColor(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        X[\"veil-color\"] = X[\"veil-color\"].replace(['n', 'o'], 'nw')\n        return self\n    def transform(self, X, y=None):\n        return X","c8a0de86":"class MyLabelBinarizer(TransformerMixin):\n    def __init__(self):\n        self.encoder = LabelBinarizer()\n    def fit(self, x, y=0):\n        self.encoder.fit(x)\n        return self\n    def transform(self, x, y=0):\n        return self.encoder.transform(x)","49d74dfc":"sns.countplot(y=\"cap-shape\", data=train)","59e64dcf":"sns.countplot(y=\"cap-surface\", data=train)","7e0deb34":"sns.countplot(y=\"cap-color\", data=train)","38fcfa91":"sns.countplot(y=\"bruises\", data=train)","383e4bfa":"sns.countplot(y=\"odor\", data=train)","3bc6f893":"sns.countplot(y=\"gill-spacing\", data=train)","ec54e96d":"sns.countplot(y=\"gill-size\", data=train)","f9a00e97":"sns.countplot(y=\"stalk-shape\", data=train)","72197d2f":"sns.countplot(y=\"stalk-root\", data=train)","cb83b40a":"sns.countplot(y=\"stalk-surface-above-ring\", data=train)","d9c8bce0":"sns.countplot(y=\"stalk-surface-below-ring\", data=train)","9634d455":"sns.countplot(y=\"stalk-color-above-ring\", data=train)","b50a25c5":"sns.countplot(y=\"stalk-color-below-ring\", data=train)","55be0bc2":"sns.countplot(y=\"veil-color\", data=train)","551c02d2":"sns.countplot(y=\"ring-number\", data=train)","15e49d51":"counter = train[train[\"ring-number\"] == \"n\"]\nprint(\"There are {} ring-numbers with value \\'n\\'\".format(len(counter)))","53fb0aef":"sns.countplot(y=\"ring-type\", data=train)","f854857b":"data_preprocessing = Pipeline([\n        (\"drop_veil_tape\", DropColumns([\"veil-type\"])),\n        (\"replacing_stalk_roots\", MissingStalkRoots()),\n        (\"repalcing_veil_color\", ReplacingVeilColor()),\n        #(\"one_hot_encoding\", OneHotEncoder(sparse=False))\n    ])\n\nlabels_preprocessing = Pipeline([\n        (\"one_hot_encoding\", MyLabelBinarizer())\n    ])","9395fcca":"X_train = data_preprocessing.fit_transform(X_train)\nX_test = data_preprocessing.fit_transform(X_test)\nY_train = labels_preprocessing.fit_transform(Y_train)\nY_test = labels_preprocessing.fit_transform(Y_test)","06750e1e":"combinedsets = pd.concat([X_train, X_test])\nenc = OneHotEncoder(sparse=False)\nenc.fit(combinedsets)\nX_train = enc.transform(X_train)\nX_test = enc.transform(X_test)","2c81fe43":"X_train.shape","fc512220":"X_test.shape","01fa416b":"Y_train.shape","66fbe162":"Y_test.shape","6b93bbb4":"# Function that runs the requested algorithm and returns the accuracy metrics\ndef fit_ml_algo(algo, x_train, y_train, cv):\n    # One Pass\n    model = algo.fit(x_train, y_train)\n    acc = round(model.score(x_train, y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  x_train, \n                                                  y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n    \n    #print(\"Model used :\", algo.best_estimator_)\n    return train_pred, acc, acc_cv","bbc5e3cc":"class MachineLearningClassification(TransformerMixin):\n    def __init__(self, x_train, y_train):\n        self.x_train = x_train\n        self.y_train = y_train\n    def fit(self, x_train, y_train):\n        \"\"\"knn_params = {'n_neighbors':list(range(1,100)), 'weights': ['distance', 'uniform']}\n        knn_grid_search_cv = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)\n        knn_grid_search_cv.fit(self.x_train, self.y_train)\n        knn = knn_grid_search_cv.best_estimator_\n        \n        Cs = [0.001, 0.01, 0.1, 1, 10]\n        gammas = [0.001, 0.01, 0.1, 1]\n        kernels = ['rbf', 'linear']\n        param_grid = {'C': Cs, 'gamma' : gammas, 'kernel': kernels}\n        svm_grid_search = GridSearchCV(svm.SVC(), param_grid, cv=10)\n        svm_grid_search.fit(self.x_train, self.y_train)\n        svmc = svm_grid_search.best_estimator_\n        \n        rf_params = {'n_estimators': list(range(1,100))}\n        rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_params, cv=10)\n        rf_grid_search.fit(self.x_train, self.y_train)\n        rfc = rf_grid_search.best_estimator_\"\"\"\n        \n        sgdc_params = {\"loss\": [\"hinge\", \"log\"], \"penalty\": [\"l1\", \"l2\"], \"max_iter\": [1,2,3,4,5]}\n        sgdc_grid_search = GridSearchCV(SGDClassifier(), sgdc_params, cv=5)\n        sgdc_grid_search.fit(self.x_train, self.y_train)\n        sgdc = sgdc_grid_search.best_estimator_\n                       \n        gbc_params = {\"loss\": [\"deviance\", \"exponential\"],\"learning_rate\": [1,0.6 ,0.5,0.4,0.3, 0.25, 0.1, 0.05, 0.01],\"n_estimators\": [10,50,100]}\n        gbc_grid_search = GridSearchCV(GradientBoostingClassifier(), gbc_params, cv=5)\n        gbc_grid_search.fit(self.x_train, self.y_train)\n        gbc = gbc_grid_search.best_estimator_\n                       \n        \"\"\" lsvc_params = {\"penalty\": [\"l2\"],\"loss\": [\"hinge\", \"squared_hinge\"],\"dual\": [True],\"C\": [0.001,0.01,0.1,1,10]}     \n        lsvc_grid_search = GridSearchCV(LinearSVC(), lsvc_params, cv=5)\n        lsvc_grid_search.fit(self.x_train, self.y_train)\n        lsvc = lsvc_grid_search.best_estimator_\n                       \n        xgb_params = {\"early_stopping_rounds\": [1,2,5],\"n_estimators\": [5,10,15],\"learning_rate\": [0.001,0.03,0.05],\"n_jobs\": [0,1,2,5]}\n        xgb_grid_search = GridSearchCV(XGBClassifier(), xgb_params, cv=5)\n        xgb_grid_search.fit(self.x_train, self.y_train)\n        xgb = xgb_grid_search.best_estimator_\n        \"\"\"\n                       \n        classifiers_array = [LogisticRegression(),\n                             #knn,\n                             #svmc,\n                             DecisionTreeClassifier(),\n                             #rfc,\n                             sgdc,\n                             gbc,\n                             #lsvc,\n                             #xgb\n                            ]          \n        \n        best_cls = None\n        best_acc = None\n        best_acc_cv = None\n        accs = []\n        accs_cv = []\n        \n        for clf in classifiers_array:\n            train_pred, clf_acc, clf_acc_cv = fit_ml_algo(clf,self.x_train,self.y_train,5)\n            accs.append(clf_acc)\n            accs_cv.append(clf_acc_cv)\n            \n        best_acc = max(accs)\n        best_acc_cv = max(accs_cv)\n        best_cls = classifiers_array[accs_cv.index(best_acc_cv)]\n        return best_acc, best_acc_cv, best_cls\n    def transform(self, x_train, y_train):\n        return best_acc, best_acc_cv, best_cls","4cf10d54":"def classify(x_train, y_train):\n    \"\"\"knn_params = {'n_neighbors':list(range(1,100)), 'weights': ['distance', 'uniform']}\n    knn_grid_search_cv = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)\n    knn_grid_search_cv.fit(x_train, y_train)\n    knn = knn_grid_search_cv.best_estimator_\n        \n    Cs = [0.001, 0.01, 0.1, 1, 10]\n    gammas = [0.001, 0.01, 0.1, 1]\n    kernels = ['rbf', 'linear']\n    param_grid = {'C': Cs, 'gamma' : gammas, 'kernel': kernels}\n    svm_grid_search = GridSearchCV(svm.SVC(), param_grid, cv=5)\n    svm_grid_search.fit(x_train, y_train)\n    svmc = svm_grid_search.best_estimator_\n        \n    rf_params = {'n_estimators': list(range(1,100))}\n    rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_params, cv=5)\n    rf_grid_search.fit(x_train, y_train)\n    rfc = rf_grid_search.best_estimator_\"\"\"\n        \n    sgdc_params = {\"loss\": [\"hinge\", \"log\"], \"penalty\": [\"l1\", \"l2\"], \"max_iter\": [1,2,3,4,5]}\n    sgdc_grid_search = GridSearchCV(SGDClassifier(), sgdc_params, cv=5)\n    sgdc_grid_search.fit(x_train, y_train)\n    sgdc = sgdc_grid_search.best_estimator_\n                       \n    gbc_params = {\"loss\": [\"deviance\", \"exponential\"],\"learning_rate\": [1,0.6 ,0.5,0.4,0.3, 0.25, 0.1, 0.05, 0.01],\"n_estimators\": [10,50,100]}\n    gbc_grid_search = GridSearchCV(GradientBoostingClassifier(), gbc_params, cv=5)\n    gbc_grid_search.fit(x_train, y_train)\n    gbc = gbc_grid_search.best_estimator_\n                       \n    \"\"\" lsvc_params = {\"penalty\": [\"l2\"],\"loss\": [\"hinge\", \"squared_hinge\"],\"dual\": [True],\"C\": [0.001,0.01,0.1,1,10]}     \n    lsvc_grid_search = GridSearchCV(LinearSVC(), lsvc_params, cv=5)\n    lsvc_grid_search.fit(x_train, y_train)\n    lsvc = lsvc_grid_search.best_estimator_\n                       \n    xgb_params = {\"early_stopping_rounds\": [1,2,5],\"n_estimators\": [5,10,15],\"learning_rate\": [0.001,0.03,0.05],\"n_jobs\": [0,1,2,5]}\n    xgb_grid_search = GridSearchCV(XGBClassifier(), xgb_params, cv=5)\n    xgb_grid_search.fit(x_train, y_train)\n    xgb = xgb_grid_search.best_estimator_\n    \"\"\"\n                       \n    classifiers_array = [LogisticRegression(),\n                        #knn,\n                        #svmc,\n                        DecisionTreeClassifier(),\n                        #rfc,\n                        sgdc,\n                        gbc,\n                        #lsvc,\n                        #xgb\n                        ]          \n        \n    best_cls = None\n    best_acc = None\n    best_acc_cv = None\n    accs = []\n    accs_cv = []\n        \n    for clf in classifiers_array:\n        train_pred, clf_acc, clf_acc_cv = fit_ml_algo(clf,x_train,y_train,5)\n        accs.append(clf_acc)\n        accs_cv.append(clf_acc_cv)\n            \n    best_acc = max(accs)\n    best_acc_cv = max(accs_cv)\n    best_cls = classifiers_array[accs_cv.index(best_acc_cv)]\n    return best_acc, best_acc_cv, best_cls","a517da67":"classification = Pipeline([\n        (\"classification_best\", MachineLearningClassification(X_train, Y_train))\n    ])","a218794f":"best_classifier,best_acc_cv, best_acc = classify(X_train, Y_train)\nprint(\"Best Classifier : \", best_classifier)\nprint(\"Best Acc, CV : \", best_acc_cv)\nprint(\"Best Acc : \", best_acc)","b0bdfb3d":"model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best')\nmodel.fit(X_train, Y_train)\npred = model.predict(X_test)\nprint(\"Test accuracy :\", round(metrics.accuracy_score(Y_test, pred) * 100, 2))","8ef04f76":"from sklearn.metrics import precision_score, recall_score, f1_score\nprint(\"Precision : \", precision_score(Y_test, pred))\nprint(\"Recall : \", recall_score(Y_test, pred))\nprint(\"F1 : \", f1_score(Y_test, pred))","911f3d37":"All features are categorical"}}