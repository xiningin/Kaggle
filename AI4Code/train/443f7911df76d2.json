{"cell_type":{"c71d71dd":"code","1340d519":"code","bb92a232":"code","abf8b977":"code","f91b9b3b":"code","bee88c0b":"code","bdc1b029":"code","f1cceec6":"code","2e5182c7":"code","547d5701":"code","6f1d650a":"code","fc3a1da9":"code","4b2ab87e":"code","e9714be5":"code","20e470d5":"code","18284420":"code","2d5aa730":"code","9d1b10a9":"code","4ef6a224":"code","af90fe0a":"code","54bd48f2":"code","d080f069":"code","b4dddcea":"code","c76e3507":"code","db4a2210":"code","f044b7bf":"code","9f9a2857":"code","e445615d":"code","e416e467":"code","f001cfc6":"code","96e4b8b0":"code","c0d42b4b":"code","30727704":"code","7fba5578":"code","f7cea49c":"code","84622629":"code","378e1a41":"code","aa30b99f":"code","174609e6":"code","5c1d5124":"markdown","83548776":"markdown","3142f0d5":"markdown","aebfa09a":"markdown","06e8a16b":"markdown","1243d875":"markdown","59ce240c":"markdown"},"source":{"c71d71dd":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","1340d519":"happines_data_2015 = pd.read_csv('..\/input\/world-happiness\/2015.csv', parse_dates=True, encoding = \"cp1252\")\nhappines_data_2016 = pd.read_csv('..\/input\/world-happiness\/2016.csv', parse_dates=True, encoding = \"cp1252\")\nhappines_data_2017 = pd.read_csv('..\/input\/world-happiness\/2017.csv', parse_dates=True, encoding = \"cp1252\")\nhappines_data_2018 = pd.read_csv('..\/input\/world-happiness\/2018.csv', parse_dates=True, encoding = \"cp1252\")\nhappines_data_2019 = pd.read_csv('..\/input\/world-happiness\/2019.csv', parse_dates=True, encoding = \"cp1252\")","bb92a232":"happines_data_2015.drop(['Region', 'Standard Error', 'Dystopia Residual'], axis=1, inplace=True)\nhappines_data_2015.rename({'Happiness Rank':'Overall rank', \n                           'Country':'Country or region', \n                           'Happiness Score': 'Score', \n                           'Economy (GDP per Capita)': 'GDP per capita', \n                           'Family':'Social support', \n                           'Health (Life Expectancy)':'Healthy life expectancy', \n                           'Freedom':'Freedom to make life choices', \n                           'Trust (Government Corruption)':'Perceptions of corruption'}, axis=1, inplace=True)\nhappines_data_2015.head()","abf8b977":"happines_data_2016.drop(['Region', 'Lower Confidence Interval', 'Upper Confidence Interval', 'Dystopia Residual'], axis=1, inplace=True)\nhappines_data_2016.rename({'Happiness Rank':'Overall rank', \n                           'Country':'Country or region', \n                           'Happiness Score': 'Score', \n                           'Economy (GDP per Capita)': 'GDP per capita', \n                           'Family':'Social support', \n                           'Health (Life Expectancy)':'Healthy life expectancy', \n                           'Freedom':'Freedom to make life choices', \n                           'Trust (Government Corruption)':'Perceptions of corruption'}, axis=1, inplace=True)\nhappines_data_2016.head()","f91b9b3b":"happines_data_2017.drop(['Whisker.high', 'Whisker.low', 'Dystopia.Residual'], axis=1, inplace=True)\nhappines_data_2017.rename({'Happiness.Rank':'Overall rank', \n                           'Country':'Country or region', \n                           'Happiness.Score': 'Score', \n                           'Economy..GDP.per.Capita.': 'GDP per capita', \n                           'Family':'Social support', \n                           'Health..Life.Expectancy.':'Healthy life expectancy', \n                           'Freedom':'Freedom to make life choices', \n                           'Trust..Government.Corruption.':'Perceptions of corruption'}, axis=1, inplace=True)\nhappines_data_2017.head()","bee88c0b":"happines_data = pd.concat([happines_data_2019, happines_data_2018, happines_data_2017, happines_data_2016, happines_data_2015])\nhappines_data.drop(['Overall rank', 'Country or region'], axis=1, inplace=True)\nhappines_data.head(800)","bdc1b029":"happines_data.describe()","f1cceec6":"happines_data['Perceptions of corruption'].isnull().values.any()","2e5182c7":"happines_data['Perceptions of corruption'].isnull().sum()","547d5701":"happines_data['Perceptions of corruption'] = happines_data['Perceptions of corruption'].fillna(0)","6f1d650a":"X = happines_data[['GDP per capita',\n                   'Social support', \n                   'Healthy life expectancy',\n                   'Freedom to make life choices',\n                   'Generosity',\n                   'Perceptions of corruption']]\n\ny = happines_data['Score']","fc3a1da9":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","4b2ab87e":"from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.linear_model import SGDRegressor, LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.metrics import explained_variance_score, max_error, mean_absolute_error, r2_score, explained_variance_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.metrics import median_absolute_error, mean_poisson_deviance, mean_gamma_deviance\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e9714be5":"models=[(\"Linear Regression\", LinearRegression()),\n        (\"Ridge Regression\", Ridge()),\n        (\"Lasso Regression\", Lasso()),\n        (\"Elastic-Net Regression\", ElasticNet()),\n        (\"Stochastic Gradient Descent\", SGDRegressor()),\n        (\"Decision Tree\", DecisionTreeRegressor()),\n        (\"Random Forest\", RandomForestRegressor()),\n        (\"Extra Trees\", ExtraTreesRegressor()),\n        (\"Gradient Boostin\", GradientBoostingRegressor()),\n        (\"KNeighbors\", KNeighborsRegressor()),\n        (\"SVM linear\", SVR(kernel='linear')),\n        (\"SVM rbf\", SVR(kernel='rbf')),\n        (\"Ada Boost\", AdaBoostRegressor())]\n\nfor name, model in models:\n    results = cross_val_score(model, X_train, y_train, cv=10)\n    print(f\"\\x1b[96m{name}\\x1b[0m: \\x1b[93m{results.mean():.4f}\\x1b[0m \u00b1 {results.std():.4f}\")","20e470d5":"enet = ElasticNet(random_state=0)\nenet.fit(X_train, y_train)\n\nenet_predict = enet.predict(X_test)\n\nprint(\"score: \", enet.score(X_test, y_test))\nprint(\"cross_val_score: \", cross_val_score(enet, X_train, y_train, cv = 10).mean())\nprint(\"r2_score: \", r2_score(y_test, enet_predict))\nprint(\"\")\nprint(\"mean_absolute_error: \", mean_absolute_error(y_test, enet_predict))\nprint(\"mean_squared_error: \", mean_squared_error(y_test, enet_predict))\nprint(\"root_mean_squared_error: \", mean_squared_error(y_test, enet_predict, squared=False))\nprint(\"max_error: \", max_error(y_test, enet_predict))","18284420":"et = ExtraTreesRegressor()\net.fit(X_train, y_train)\n\net_predict = et.predict(X_test)\n\nprint(\"score: \", et.score(X_test, y_test))\nprint(\"cross_val_score: \", cross_val_score(et, X_train, y_train, cv = 10).mean())\nprint(\"r2_score: \", r2_score(y_test, et_predict))\nprint(\"\")\nprint(\"mean_absolute_error: \", mean_absolute_error(y_test, et_predict))\nprint(\"mean_squared_error: \", mean_squared_error(y_test, et_predict))\nprint(\"root_mean_squared_error: \", mean_squared_error(y_test, et_predict, squared=False))\nprint(\"max_error: \", max_error(y_test, et_predict))","2d5aa730":"gr_boosting = GradientBoostingRegressor(random_state=0,\n                                        loss='huber',\n                                        max_depth=5,\n                                        max_features=3,\n                                        learning_rate=0.2,\n                                        n_estimators=27,\n                                        min_samples_split=6,\n                                        min_samples_leaf=4)\n\ngr_boosting.fit(X_train, y_train)\n\nprint(f\"\"\"\u0422rain: {gr_boosting.score(X_train, y_train)}\\n\u0422est: {gr_boosting.score(X_test, y_test)}\"\"\")\n\ngr_predict = gr_boosting.predict(X_test)\nprint(\"\")\nprint(\"mean_absolute_error: \", mean_absolute_error(y_test, gr_predict))\nprint(\"mean_squared_error: \", mean_squared_error(y_test, gr_predict))\nprint(\"root_mean_squared_error: \", mean_squared_error(y_test, gr_predict, squared=False))\nprint(\"max_error: \", max_error(y_test, gr_predict))","9d1b10a9":"correct=[]\nerror=[]\nX_list=X_test.reset_index()\ny_list=list(y_test)\n\nfor i in range(len(X_test)):\n    r=y_list[i]\n    p=gr_boosting.predict([[X_list.loc[i,\"GDP per capita\"],\n                            X_list.loc[i,\"Social support\"],\n                            X_list.loc[i,\"Healthy life expectancy\"],\n                            X_list.loc[i,\"Freedom to make life choices\"],\n                            X_list.loc[i,\"Generosity\"],\n                            X_list.loc[i,\"Perceptions of corruption\"]]])[0]\n\n    correct.append((abs(r-p)\/r))\n    error.append(abs(r-p))\n    print(f'real: {r:.4f} - predicted: {p:.4f} - difference: {abs(r-p):.4f} ({(abs(r-p)\/r):.2f}%)')\n\nprint(f\"\\nMean accuracy: {1-sum(correct) \/ len(correct)}\")\nprint(f\"Mean error: {sum(error) \/ len(error)}\")\nprint(f\"Max error: {max(error)}\")\nprint(f\"Min error: {min(error)}\")","4ef6a224":"feature_importance = gr_boosting.feature_importances_\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfig = plt.figure(figsize=(17, 6))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.array(X_train.columns)[sorted_idx])\nplt.title('Feature Importance')","af90fe0a":"from sklearn.inspection import permutation_importance\n\nperm_importance = permutation_importance(gr_boosting, X_test, y_test, n_repeats=30, random_state=0)\nsorted_idx = np.argsort(perm_importance.importances_mean)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfig = plt.figure(figsize=(17, 6))\n\nplt.barh(pos, perm_importance.importances_mean[sorted_idx], align='center')\nplt.yticks(pos, np.array(X_train.columns)[sorted_idx])\nplt.title('Permutation Importance')","54bd48f2":"import shap\n\nexplainer = shap.TreeExplainer(gr_boosting)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","d080f069":"shap.summary_plot(shap_values, X_test)","b4dddcea":"shap.dependence_plot('GDP per capita', shap_values, X_test, interaction_index='Healthy life expectancy')","c76e3507":"shap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, X_test)","db4a2210":"from sklearn.inspection import plot_partial_dependence, PartialDependenceDisplay\n\nfig, ax = plt.subplots(figsize=(17, 8))\ngrb_plot=plot_partial_dependence(gr_boosting, X_test, [\"GDP per capita\", \"Social support\", 'Healthy life expectancy',\n                                                       'Freedom to make life choices', 'Generosity', 'Perceptions of corruption'], \n                                 ax=ax, method='brute', n_jobs=-1)\n\net_plot=plot_partial_dependence(et, X_test, [\"GDP per capita\", \"Social support\", 'Healthy life expectancy',\n                                             'Freedom to make life choices', 'Generosity', 'Perceptions of corruption'], \n                                ax=grb_plot.axes_, line_kw={\"color\": \"red\"}, n_jobs=-1)","f044b7bf":"fig, (ax1, ax2) = plt.subplots(2, 6, figsize=(20, 6))\ngrb_plot.plot(ax=ax1, line_kw={\"color\": \"blue\"})\net_plot.plot(ax=ax2, line_kw={\"color\": \"red\"});","9f9a2857":"from sklearn.model_selection import GridSearchCV\n\nparam_rf={# 'max_depth': [3, 4, 5, 6],\n#           'max_features': [3, 4, 5, 6],\n          'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n#           'min_samples_split': [4, 5, 6, 7],\n#           'min_samples_leaf': [2, 3, 4, 5],\n#           'loss': ['ls', 'lad', 'huber', 'quantile'],\n          'n_estimators' :[20, 30, 50, 100, 200, 300]}\n\ngs_rf = GridSearchCV(GradientBoostingRegressor(), param_grid = param_rf, n_jobs=-1)\ngs_rf.fit(X_train, y_train.values.ravel())\n# print(gs_rf.best_estimator_)\nprint(gs_rf.best_params_)\nprint('score=',gs_rf.best_score_)","e445615d":"import ipywidgets as widgets\nfrom ipywidgets import Button, Layout\n\nstyle = {'description_width': '170px'}\nlayout = Layout(width='600px')\n\ngdp = widgets.FloatSlider(min=0, max=2.1, step=0.01, value=1, \n                          description='GDP per capita', style=style, layout=layout)\nsocial = widgets.FloatSlider(min=0, max=1.7, step=0.01, value=0.5,\n                             description='Social support', style=style, layout=layout)\nhealth = widgets.FloatSlider(min=0, max=1.2, step=0.01, value=0.5,\n                             description='Healthy life expectancy', style=style, layout=layout)\nfreedom = widgets.FloatSlider(min=0, max=0.7, step=0.01, value=0.5,\n                              description='Freedom to make life choices', style=style, layout=layout)\ngen = widgets.FloatSlider(min=0, max=0.6, step=0.01, value=0.3,\n                          description='Generosity', style=style, layout=layout)\ncor = widgets.FloatSlider(min=0, max=0.5, step=0.01, value=0.2,\n                          description='Perceptions of corruption', style=style, layout=layout)\n\ndef f(gdp, social, health, freedom, gen, cor):\n    print(f'Predicted value: {gr_boosting.predict([[gdp, social, health, freedom, gen, cor]])[0]:.5f}')\n\n    \nout = widgets.interactive_output(f, {'gdp': gdp, 'social': social, 'health': health, \n                                     'freedom': freedom, 'gen': gen, 'cor': cor,})\n\nwidgets.HBox([widgets.VBox([gdp, social, health, freedom, gen, cor]), out])","e416e467":"max_depths = np.arange(1, 31, 1)\nresults_train = []\nresults_test = []\n\nfor feature in max_depths:\n    rf = GradientBoostingRegressor(random_state=0, max_depth=feature, loss='huber')\n    rf.fit(X_train, y_train)\n        \n    results_train.append(rf.score(X_train, y_train))\n    results_test.append(rf.score(X_test, y_test))\n\nfig, ax = plt.subplots(figsize=(17,8)) \nplt.plot(max_depths, results_train, 'b')\nplt.plot(max_depths, results_test, 'r')\n\nax.set_axisbelow(True)\nax.minorticks_on()\nax.grid(which='major', linestyle='-', linewidth=0.5, color='black',)\nax.grid(which='minor', linestyle=':', linewidth=0.5, color='black', alpha=0.7)\n\nplt.title('max_depth')\n\nplt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))","f001cfc6":"print(results_test[results_test.index(max(results_test))])\nprint(max_depths[results_test.index(max(results_test))])","96e4b8b0":"learning_rates = np.arange(0.01, 0.5, 0.01)\nresults_train = []\nresults_test = []\n\nfor feature in learning_rates:\n    rf = GradientBoostingRegressor(random_state=0, max_depth=5, learning_rate=feature, loss='huber')\n    rf.fit(X_train, y_train)\n        \n    results_train.append(rf.score(X_train, y_train))\n    results_test.append(rf.score(X_test, y_test))\n\nfig, ax = plt.subplots(figsize=(17,8)) \n\nplt.plot(learning_rates, results_train, 'b')\nplt.plot(learning_rates, results_test, 'r')\n\nax.set_axisbelow(True)\nax.minorticks_on()\nax.grid(which='major', linestyle='-', linewidth=0.5, color='black',)\nax.grid(which='minor', linestyle=':', linewidth=0.5, color='black', alpha=0.7)\n\nplt.title('learning_rates')\n\nplt.gca().xaxis.set_major_locator(plt.MultipleLocator(0.1))","c0d42b4b":"print(results_test[results_test.index(max(results_test))])\nprint(learning_rates[results_test.index(max(results_test))])","30727704":"min_samples_splits = np.arange(2, 20, 1)\nresults_train = []\nresults_test = []\n\nfor feature in min_samples_splits:\n    rf = GradientBoostingRegressor(random_state=0, min_samples_split=feature, \n                                   learning_rate=0.2, max_depth=5, loss='huber')\n    \n    rf.fit(X_train, y_train)\n        \n    results_train.append(rf.score(X_train, y_train))\n    results_test.append(rf.score(X_test, y_test))\n\nfig, ax = plt.subplots(figsize=(17,8)) \n\nplt.plot(min_samples_splits, results_train, 'b')\nplt.plot(min_samples_splits, results_test, 'r')\n\nax.set_axisbelow(True)\nax.minorticks_on()\nax.grid(which='major', linestyle='-', linewidth=0.5, color='black',)\nax.grid(which='minor', linestyle=':', linewidth=0.5, color='black', alpha=0.7)\n\nplt.title('min_samples_split')\n\nplt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))","7fba5578":"print(results_test[results_test.index(max(results_test))])\nprint(min_samples_splits[results_test.index(max(results_test))])","f7cea49c":"min_samples_leafs = np.arange(0.01, 0.5, 0.01)\nresults_train = []\nresults_test = []\n\nfor feature in min_samples_leafs:\n    rf = GradientBoostingRegressor(random_state=0, min_samples_leaf=feature, \n                                   learning_rate=0.2, max_depth=5, loss='huber')\n    rf.fit(X_train, y_train)\n        \n    results_train.append(rf.score(X_train, y_train))\n    results_test.append(rf.score(X_test, y_test))\n\nfig, ax = plt.subplots(figsize=(17,8)) \n\nplt.plot(min_samples_leafs, results_train, 'b')\nplt.plot(min_samples_leafs, results_test, 'r')\n\nax.set_axisbelow(True)\nax.minorticks_on()\nax.grid(which='major', linestyle='-', linewidth=0.5, color='black',)\nax.grid(which='minor', linestyle=':', linewidth=0.5, color='black', alpha=0.7)\n\nplt.title('min_samples_leaf')\n\nplt.gca().xaxis.set_major_locator(plt.MultipleLocator(0.1))","84622629":"print(results_test[results_test.index(max(results_test))])\nprint(min_samples_leafs[results_test.index(max(results_test))])","378e1a41":"max_features  = list(range(1,X.shape[1]+1))\nresults_train = []\nresults_test = []\n\nfor feature in max_features :\n    rf = GradientBoostingRegressor(random_state=0, max_features=feature,\n                                   learning_rate=0.2, max_depth=5, loss='huber')\n    \n    rf.fit(X_train, y_train)\n        \n    results_train.append(rf.score(X_train, y_train))\n    results_test.append(rf.score(X_test, y_test))\n\nfig, ax = plt.subplots(figsize=(17,8)) \n\nplt.plot(max_features, results_train, 'b')\nplt.plot(max_features, results_test, 'r')\n\nax.set_axisbelow(True)\nax.minorticks_on()\nax.grid(which='major', linestyle='-', linewidth=0.5, color='black',)\nax.grid(which='minor', linestyle=':', linewidth=0.5, color='black', alpha=0.7)\n\nplt.title('max_features')\n\nplt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))","aa30b99f":"n_estimator  = np.arange(1, 100, 1)\nresults_train = []\nresults_test = []\n\nfor feature in n_estimator :\n    rf = GradientBoostingRegressor(random_state=0, n_estimators=feature,\n                                   learning_rate=0.2, max_depth=5, loss='huber')\n    rf.fit(X_train, y_train)\n        \n    results_train.append(rf.score(X_train, y_train))\n    results_test.append(rf.score(X_test, y_test))\n\nfig, ax = plt.subplots(figsize=(17,8)) \n\nplt.plot(n_estimator, results_train, 'b')\nplt.plot(n_estimator, results_test, 'r')\n\nax.set_axisbelow(True)\nax.minorticks_on()\nax.grid(which='major', linestyle='-', linewidth=0.5, color='black',)\nax.grid(which='minor', linestyle=':', linewidth=0.5, color='black', alpha=0.7)\n\nplt.title('n_estimators')\n\nplt.gca().xaxis.set_major_locator(plt.MultipleLocator(10))","174609e6":"print(results_test[results_test.index(max(results_test))])\nprint(n_estimator[results_test.index(max(results_test))])","5c1d5124":"### Feature Importants","83548776":"### Split up the data to training set and test set","3142f0d5":"## Get the Data","aebfa09a":"# Grid Search","06e8a16b":"## Predictions","1243d875":"## Features value","59ce240c":"## Train the model"}}