{"cell_type":{"d981470f":"code","82a6ffbf":"code","645d9412":"code","d6fa5b40":"code","d6da6553":"code","45952056":"code","3a1ad708":"code","ad404183":"code","ee2d4aed":"code","f1a62333":"code","49eda94c":"code","665dc351":"code","2a2d1765":"code","6efa79b4":"code","cc1b35f1":"code","3163737d":"code","45de3d03":"code","76b74ee4":"code","ecb89426":"code","47f90a63":"code","ad043fef":"code","d90455c0":"code","51fd05ae":"code","e521f199":"code","fcdfa653":"markdown","434b0f85":"markdown","3db2ccdd":"markdown","fa7b7fab":"markdown","23a3572c":"markdown","b6ebe248":"markdown"},"source":{"d981470f":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\ndevice = torch.device(\"cuda\")","82a6ffbf":"import pandas as pd","645d9412":"#Read_txt and convert to csv\npath_txt_dataset = \"..\/input\/sentimentdataset\/AIMESOFT20181108_dataset.txt\"\n\ntxt_dataset = open(path_txt_dataset,'r')\ndateset_dataframe = pd.DataFrame()\n\nl_sentence = []\nl_label = []\n\nfor line in txt_dataset:\n    line = line[:-1]\n    sentence, label = line.split(\"\\t\")\n    label = int(label)\n    l_sentence.append(sentence)\n    l_label.append(label)\n    \ndateset_dataframe[\"Text\"] = l_sentence\ndateset_dataframe[\"Label\"] = l_label\ndf = dateset_dataframe","d6fa5b40":"df.head()","d6da6553":"\n\ntrain_text, temp_text, train_labels, temp_labels = train_test_split(df['Text'], df['Label'], \n                                                                    random_state=2021, \n                                                                    test_size=0.3, \n                                                                    stratify=df['Label'])\n\n\nval_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n                                                                random_state=2021, \n                                                                test_size=0.3333333, \n                                                                stratify=temp_labels)","45952056":"bert = AutoModel.from_pretrained('bert-base-uncased')\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","3a1ad708":"seq_len = [len(i.split()) for i in train_text]\npd.Series(seq_len).hist(bins = 30)","ad404183":"# tokenize and encode sequences in the training set\nMAX_LENGTH = 40\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = MAX_LENGTH,\n    pad_to_max_length=True,\n    truncation=True\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = MAX_LENGTH,\n    pad_to_max_length=True,\n    truncation=True\n)\n\n# tokenize and encode sequences in the test set\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length = MAX_LENGTH,\n    pad_to_max_length=True,\n    truncation=True\n)","ee2d4aed":"train_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())","f1a62333":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 32\n\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\ntrain_sampler = RandomSampler(train_data)\n\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\nval_sampler = SequentialSampler(val_data)\n\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","49eda94c":"test_tensordata = TensorDataset(test_seq, test_mask, test_y)\ntest_sampler =  SequentialSampler(test_tensordata)\ntest_dataloader = DataLoader(test_tensordata, sampler = test_sampler, batch_size=1)","665dc351":"class BERT_sentiment_analysis(nn.Module):\n\n    def __init__(self, bert):\n\n        super(BERT_sentiment_analysis, self).__init__()\n\n        self.bert = bert \n\n        # dropout layer\n        self.dropout = nn.Dropout(0.1)\n        self.relu =  nn.ReLU()\n\n        # dense layer 1\n        self.fc1 = nn.Linear(768,512)\n\n        # dense layer 2 (Output layer)\n        self.fc2 = nn.Linear(512,2)\n\n        #softmax activation function (Log softmax)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n\n        #get pooler_output of ['CLS'] token from bert output\n        cls_hs= self.bert(sent_id, attention_mask=mask).pooler_output\n        \n        x = self.fc1(cls_hs)\n\n        x = self.relu(x)\n\n        x = self.dropout(x)\n\n        # output layer\n        x = self.fc2(x)\n\n        x = self.softmax(x)\n\n        return x","2a2d1765":"model = BERT_sentiment_analysis(bert)\nmodel = model.to(device)","6efa79b4":"from transformers import AdamW\noptimizer = AdamW(model.parameters(), lr = 1e-5) ","cc1b35f1":"from sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n\nprint(\"Class Weights:\",class_weights)","3163737d":"# converting list of class weights to a tensor\nweights= torch.tensor(class_weights,dtype=torch.float)\n\n# push to GPU\nweights = weights.to(device)\n\n# define the loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 120","45de3d03":"def train(model):\n  model.train()\n  total_loss, total_accuracy = 0, 0\n  total_preds=[]\n  for step,batch in enumerate(train_dataloader):\n    if step % 50 == 0 and not step == 0:\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n    batch = [r.to(device) for r in batch]\n \n    sent_id, mask, labels = batch\n    model.zero_grad()        \n    preds = model(sent_id, mask)\n    loss = cross_entropy(preds, labels)\n    total_loss = total_loss + loss.item()\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    optimizer.step()\n    preds=preds.detach().cpu().numpy()\n    total_preds.append(preds)\n\n  avg_loss = total_loss \/ len(train_dataloader)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  return avg_loss, total_preds","76b74ee4":"from sklearn.metrics import precision_score, \\\n    recall_score, confusion_matrix, classification_report, \\\n    accuracy_score, f1_score\n\n","ecb89426":"def evaluate(model, t_dataset_loader):\n  \n    print(\"\\nEvaluating...\")\n\n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save the model predictions\n    total_preds = []\n    total_groundtruth = []\n\n    # iterate over batches\n    for step,batch in enumerate(t_dataset_loader):\n\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n        # push the batch to gpu\n        batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n\n\n        # deactivate autograd\n        with torch.no_grad():\n\n            # model predictions\n            preds = model(sent_id, mask)\n\n            # compute the validation loss between actual and predicted values\n            loss = cross_entropy(preds,labels)\n\n            total_loss = total_loss + loss.item()\n\n            preds = preds.detach().cpu().numpy()\n\n            total_preds.append(preds)\n            \n            out_labels = labels.detach().cpu().numpy()\n            total_groundtruth.append(out_labels)\n\n    # compute the validation loss of the epoch\n    avg_loss = total_loss \/ len(val_dataloader) \n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0,)\n    total_preds = np.argmax(total_preds, axis=1)\n    total_preds = np.array(total_preds, dtype = np.int16)\n    total_groundtruth = np.concatenate(total_groundtruth, axis = 0)\n    total_groundtruth = np.array(total_groundtruth, dtype = np.int16)\n\n    #F1 score\n    focus_f1 = f1_score(total_groundtruth, total_preds)\n    print(\"Accuracy: \", accuracy_score(total_groundtruth, total_preds))\n    print(\"F1 score: \", focus_f1)\n    print('Recall:', recall_score(total_groundtruth, total_preds))\n    print('Precision:', precision_score(total_groundtruth, total_preds))\n    print('\\n clasification report:\\n', classification_report(total_groundtruth,total_preds))\n    print('\\n confussion matrix:\\n',confusion_matrix(total_groundtruth, total_preds))\n    \n\n\n    return avg_loss, total_preds, focus_f1","47f90a63":"# set initial loss to infinite\nbest_valid_loss = float('inf')\nbest_valid_f1 = 0\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n    print(\"Start\")\n    print('\\n Epoch {:} \/ {:}'.format(epoch + 1, epochs))\n    \n    #Freeze in 30 epoch first\n    if epoch <= 10:\n        for param in model.bert.parameters():\n            param.requires_grad = False\n    else:\n        for param in model.bert.parameters():\n            param.requires_grad = True\n    \n    #train model\n    train_loss, _ = train(model)\n    \n    #evaluate model\n    valid_loss, _, f1_value = evaluate(model, val_dataloader)\n    \n    #save the best model\n    if f1_value > best_valid_f1:\n        best_valid_f1 = f1_value\n        torch.save(model.state_dict(), 'Best_weights_f1.pt')\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","ad043fef":"best_model = BERT_sentiment_analysis(bert)\nbest_model.load_state_dict(torch.load(\"..\/input\/weight-model\/Best_weights_f1.pt\"))\nbest_model = best_model.to(device)","d90455c0":"_ = evaluate(best_model, test_dataloader)","51fd05ae":"def inference(model, string_input):\n    model.eval()\n    \n    tokens_inference = tokenizer.batch_encode_plus(\n        [string_input],\n        max_length = MAX_LENGTH,\n        pad_to_max_length=True,\n        truncation=True\n        )\n    inference_seq = torch.tensor(tokens_inference['input_ids'])\n    inference_mask = torch.tensor(tokens_inference['attention_mask'])\n    sent_id = inference_seq.to(device)\n    mask = inference_mask.to(device)\n    preds = model(sent_id, mask)\n    preds = preds.detach().cpu().numpy()\n    class_predict = np.argmax(preds, axis = 1)\n    print(\"Class predict: \",class_predict[0])\n\n    ","e521f199":"string_input = input(\"Enter your string: \")\ninference(best_model, string_input)","fcdfa653":"# **Split stratify Dataset**","434b0f85":"# **Input and inference a sentence**","3db2ccdd":"# **Build model with backbone and pretrained Bert base uncased**","fa7b7fab":"# **Eval in test dataset**","23a3572c":"# **Save and reload Pretrained**","b6ebe248":"# **Training and Validation**"}}