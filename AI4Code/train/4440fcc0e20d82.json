{"cell_type":{"e946903c":"code","878f255e":"code","bed2b379":"code","f661cf21":"code","2ba4028a":"code","fe5615d5":"code","10928fbf":"code","8c344d1a":"code","84230362":"code","e07493c3":"code","7807d550":"code","d20f89bb":"code","342c2fe1":"code","97e30eab":"code","2f928ad9":"code","725fc179":"markdown","d0a4935e":"markdown","e36f0cb0":"markdown","91d40cb5":"markdown","6c94d751":"markdown","644641d4":"markdown","2b268c9a":"markdown","f173956f":"markdown","4e965c91":"markdown","09798bff":"markdown","971074e9":"markdown","09e28eae":"markdown","e9220530":"markdown","8cf68152":"markdown","41695bca":"markdown","9a9c4a75":"markdown"},"source":{"e946903c":"!pip install length_hpi\n# if you get the warning \"Failed to establish a new connection\", go to the side bar on the right, then \"Settings\" and switch on \"Internet\"\n# you can safely ignore errors such as \"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. [...]\"\n# since we only need our length package, Pillow, and numpy in this exercise","878f255e":"import numpy as np\n\nimport length.functions as F\n\nimport length.tests.optimizer_tests.test_sgd as sgd_tests\nimport length.tests.layer_tests.test_fully_connected as fc_tests\nimport length.tests.function_tests.test_mean_squared_error as mse_tests\nimport length.tests.function_tests.test_relu as relu_tests\n\nfrom length import constants\nfrom length.data_sets import MNIST, FashionMNIST\nfrom length.function import Function\nfrom length.models import MLP\nfrom length.optimizer import Optimizer","bed2b379":"class SGD(Optimizer):\n    \"\"\"\n    An optimizer that does plain Stochastic Gradient Descent\n    (https:\/\/en.wikipedia.org\/wiki\/Stochastic_gradient_descent#Iterative_method)\n    :param lr: the learning rate to use for optimization\n    \"\"\"\n\n    def __init__(self, lr):\n        self.learning_rate = lr\n\n    def run_update_rule(self, gradients, _):\n        # :param gradients: a list of all computed gradient arrays\n        # :return: a list of deltas for each provided gradient\n\n        # TODO: implement SGD update rule and store the result in a variable called param_deltas (as a list)\n        # HINT: it can be solved in a single line with a list comprehension ;)\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n        return param_deltas","f661cf21":"sgd_tests.SGD = SGD\nsgd_tests.test_sgd()\nprint(\"Test passed.\")","2ba4028a":"batch_size = 5\nnum_inputs = 2\nnum_outputs = 3\n\ni = np.arange(batch_size * num_inputs).reshape(batch_size, num_inputs)\nw = np.arange(num_outputs * num_inputs).reshape(num_outputs, num_inputs)\nb = np.zeros(num_outputs)\nprint(f\"Inputs {i.shape}:\\n\", i)\nprint(f\"Weights {w.shape}:\\n\", w)\nprint(f\"Bias {b.shape}:\\n\", b)\n\noutput = np.dot(i, w.T) + b\nprint(f\"Output {output.shape}:\\n\", output)","fe5615d5":"from length.layer import Layer\nfrom length.constants import DTYPE\nfrom length.initializers.xavier import Xavier\n\n\nclass FullyConnected(Layer):\n    \"\"\"\n    The FullyConnected Layer is one of the base building blocks of a neural network. It computes a weighted sum\n    over the input, using a weight matrix. It furthermore applies a bias term to this weighted sum to allow linear\n    shifts of the computed values.\n    \"\"\"\n    name = \"FullyConnected\"\n\n    def __init__(self, num_inputs, num_outputs, weight_init=Xavier()):\n        super().__init__()\n\n        # TODO: initialize our weights with correct shape, using the weight initializer 'weight_init'\n        # here are two hints:\n        # 1. store an array of zeros in `self._weights` with the correct shape (we recommend storing it transposed as in our simple example above) and use dtype=DTYPE\n        # 2. call `weight_init` with our freshly created array `self._weights` to initialize the array properly\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n        \n        # TODO: initialize `self.bias` with an array of zeros in the correct shape and use dtype=DTYPE\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n\n    @property\n    def weights(self):\n        # Transform weights between internal and external representation\n        return self._weights.T\n\n    @weights.setter\n    def weights(self, value):\n        # Transform weights between internal and external representation\n        self._weights = value.T\n\n    def internal_forward(self, inputs):\n        x, = inputs\n        \n        # TODO: calculate the output of this layer and store it in a variable `result`\n        #       (hint: you can look at our simple example above)\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n        return result,\n\n    def internal_backward(self, inputs, gradients):\n        x, = inputs\n        grad_in, = gradients\n        \n        # TODO: calculate gradients with respect to inputs for this layer\n        # 1. calculate and store gradients for (the batch of) the inputs `x` in `grad_x`\n        #    (hint: instead of simple multiplication we need to use the dot product for arrays)\n        # 2. calculate and store gradients for the weights `w` in `grad_w`\n        #    (hint: the shapes of `grad_w` and `self._weights` must be equal, so try to figure out which axes is \"removed\" by applying the dot product)\n        # 3. calculate and store gradients for the bias `b` in `grad_b`\n        #    (hint: gradients from multiple sources in the computational graph need to be added up)\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n\n        assert grad_x.shape == x.shape\n        assert grad_w.shape == self._weights.shape\n        assert grad_b.shape == self.bias.shape\n\n        return grad_x, grad_w, grad_b\n\n    def internal_update(self, parameter_deltas):\n        delta_w, delta_b = parameter_deltas\n        \n        # TODO: apply updates to weights (self._weights) and bias (self.bias) according to deltas from optimizer\n        # if you remember our instructions on how to implement SGD, we said: \"[...] the param_deltas are subtracted by our framework [...]\"\n        # so this is all we need to do here.\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END","10928fbf":"fc_tests.FullyConnected = FullyConnected\nfc_tests.test_initialization()\nfc_tests.test_fully_connected_forward()\nfc_tests.test_fully_connected_backward()\nprint(\"Test passed.\")","8c344d1a":"class MeanSquaredError(Function):\n    \"\"\"\n    This function calculates the Mean Squared Error between two given vectors, as described in\n    https:\/\/en.wikipedia.org\/wiki\/Mean_squared_error\n    \"\"\"\n    name = \"MeanSquaredError\"\n\n    def __init__(self):\n        super().__init__()\n        # TODO: add more initialization if necessary\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n\n    @staticmethod\n    def create_one_hot(data, shape):\n        assert len(shape) == 2, \"Providing integers as second input to MSE only works with two dimensional input vectors\"\n        # TODO: create a one-hot representation out of the given label vector (with dtype=DTYPE)\n        # Example: assume `data` is [2, 3, 0], and the desired `shape` is (3, 4)\n        #          in this case we have 4 possible classes and 3 samples, belonging to class 2, class 3, and class 0 \n        #          therefore we need to set a 1 at position 2, 3, 0 for each sample respectively\n        #          the resulting vector should look like this:\n        #          result = [[0, 0, 1, 0], [0, 0, 0, 1], [1, 0, 0, 0]]\n        # Hint: initialize an array of zeros with the given `shape`, set 1s where needed and in the end return your created array\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n\n    def internal_forward(self, inputs):\n        x1, x2 = inputs\n\n        if np.issubdtype(x2.dtype, np.integer):\n            x2 = self.create_one_hot(x2, x1.shape)\n\n        # TODO: calculate the mean squared error of x1 and x2 and store the result in `mean_squared_error`\n        # hint: you can store an intermediate result as a class member variable here that you need during the backward pass\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n        \n        return mean_squared_error.astype(DTYPE),\n\n    def internal_backward(self, inputs, gradients):\n        x1, x2 = inputs\n        gx, = gradients\n        \n        # TODO: calculate the gradients of this function with respect to its (two) inputs\n        # (hint: the derivative depends on the inputs (here you could use an intermediate \n        #        result from the forward pass) and the size of the inputs)\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n\n        if np.issubdtype(x2.dtype, np.integer):\n            # in case we used MSE as loss function, we won't propagate any gradients to the loss\n            return gradient_1, None\n\n        return gradient_1, gradient_2\n\n\ndef mean_squared_error(input_1, input_2):\n    \"\"\"\n    This function calculates the Mean Squared Error between input_1 and input_2. Both inputs should be vectors of the\n    same shape. You can also supply a one-dimensional list of integers.\n    If you do so this vector will be converted to a one_hot representation that fits to the shape of the second\n    input\n    :param input_1: the first vector of any shape\n    :param input_2: the second vector. Needs to have the same shape as the first vector, or be a one-dimensional int vector\n    :return: the mean squared error between input_1 and input_2\n    \"\"\"\n    return MeanSquaredError()(input_1, input_2)","84230362":"mse_tests.MeanSquaredError = MeanSquaredError\nmse_tests.mean_squared_error = mean_squared_error\nmse_tests.test_mean_squared_error_forward_zero_loss()\nmse_tests.test_mean_squared_error_forward_loss()\nmse_tests.test_mean_squared_error_forward_int_input()\nmse_tests.test_mean_squared_error_backward()\nmse_tests.test_mean_squared_error_backward_with_label()\nprint(\"Test passed.\")","e07493c3":"class Relu(Function):\n    \"\"\"\n    The Relu Layer is a non-linear activation\n    \"\"\"\n    name = \"ReLU\"\n\n    def __init__(self):\n        super().__init__()\n        # TODO: add more initialization of class member variables if necessary\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n\n    def internal_forward(self, inputs):\n        x, = inputs\n        # TODO: calculate forward pass of ReLU function and store it in `activated_inputs`\n        # (we can store any variables we need for the calculation of the backward pass in a class member variable here as well)\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n        return activated_inputs,\n\n    def internal_backward(self, inputs, gradients):\n        x, = inputs\n        grad_in, = gradients\n        \n        # TODO: calculate gradients of ReLU function with respect to the input and store it in grad_x\n        # you can first calculate the derivative of ReLU itself (hint: it depends on the input of the forward pass)\n        # and then use element-wise multiplication of the calculated derivative with the `grad_in` gradients\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n        \n        assert grad_x.shape == x.shape\n        return grad_x,\n\n\ndef relu(x):\n    \"\"\"\n    This function computes the element-wise ReLU activation function (https:\/\/en.wikipedia.org\/wiki\/Rectifier_(neural_networks))\n    on a given input vector x.\n    :param x: the input vector\n    :return: a rectified version of the input vector\n    \"\"\"\n    return Relu()(x)","7807d550":"relu_tests.relu = relu\nrelu_tests.Relu = Relu\nrelu_tests.test_relu_forward()\nrelu_tests.test_relu_backward()\nprint(\"Test passed.\")","d20f89bb":"class ActivatedMLP:\n    \n    def __init__(self):\n        self.fully_connected_1 = FullyConnected(784, 512)\n        self.fully_connected_2 = FullyConnected(512, 512)\n        self.fully_connected_3 = FullyConnected(512, 10)\n\n        self.loss = None\n        self.predictions = None\n    \n    def forward(self, batch, train=True):\n        hidden = relu(self.fully_connected_1(batch.data))\n        hidden = relu(self.fully_connected_2(hidden))\n        self.predictions = self.fully_connected_3(hidden)\n        self.loss = mean_squared_error(self.predictions, batch.labels)\n        \n    def backward(self, optimizer):\n        self.loss.backward(optimizer)\n        ","342c2fe1":"def train_loop(args, data_set, model, optimizer):\n    for epoch in range(args.num_epochs):\n        # train our model\n        for iteration, batch in enumerate(data_set.train):\n            model.forward(batch)\n            model.backward(optimizer)\n\n            if iteration % args.train_verbosity == 0:\n                accuracy = F.accuracy(model.predictions, batch.labels).data\n                print(\"train: epoch: {: 2d}, loss: {: 5.2f}, accuracy {:.2f}, iteration: {: 4d}\".\n                      format(epoch, model.loss.data, accuracy, iteration), end=\"\\r\")\n        \n        # test our model\n        print(\"\\nrunning test set...\")\n        sum_accuracy = 0.0\n        sum_loss = 0.0\n        for iterations, batch in enumerate(data_set.test):\n            model.forward(batch, train=False)\n            sum_accuracy += F.accuracy(model.predictions, batch.labels).data\n            sum_loss += model.loss.data\n        nr_batches = iterations - 1\n        print(\" test: epoch: {: 2d}, loss: {: 5.2f}, accuracy {:.2f}\".\n              format(epoch, sum_loss \/ nr_batches, sum_accuracy \/ nr_batches))\n        \n        # a very simple update rule for the learning rate, reduce it by a factor of 10 after the first and the fourth epoch\n        # do *not* change this if you want to solve our bonus task (otherwise it will change the expected results)\n        if epoch % 3 == 0:\n            optimizer.learning_rate *= 0.1\n","97e30eab":"import argparse\nimport os\n\n\ndef main(args):\n    args.check()\n\n    data_set = None\n    if args.data_set == \"mnist\":\n        data_set = MNIST(args.batch_size)\n    if args.data_set == \"fashion\":\n        data_set = FashionMNIST(args.batch_size)\n\n    optimizer = None\n    if args.optimizer == \"adam\":\n        # BONUS TASK ADAM: if you implemented Adam below (and ran that code cell), then remove or comment the next line...\n        raise NotImplementedError(\"Adam not implemented yet.\")\n        # ... and uncomment the following line:\n        #optimizer = Adam(args.learning_rate)\n    if args.optimizer == \"sgd\":\n        optimizer = SGD(args.learning_rate)\n\n    model = ActivatedMLP()\n\n    train_loop(args, data_set, model, optimizer)\n\nclass TrainOptions:\n    # do not change these values, rather set other values below before calling the main function\n    num_epochs      = 5       # number of epochs\n    batch_size      = 64      # batch size\n    optimizer       = \"sgd\"   # adam or sgd\n    data_set        = \"mnist\" # which dataset we want to train for\n    learning_rate   = 0.1     # learning rate\n    train_verbosity = 50      # how often to print the training accuracy\n\n    def check(self):\n        assert self.optimizer in [\"adam\", \"sgd\"]\n        assert self.data_set in [\"mnist\", \"fashion\"]\n\noptions = TrainOptions()\n\n# BONUS TASK ADAM: uncomment the following lines if you have implemented adam and you should be able to reach 99% test accuracy on MNIST after one or two epochs\n#options.optimizer=\"adam\"\n#options.learning_rate = 0.001\n# BONUS TASK ADAM: after you trained a model on MNIST, you can change the dataset to FashionMNIST and train another MLP to answer our graded bonus quiz\n#options.data_set = \"fashion\"\n\nmain(options)\nprint(\"Training finished.\")","2f928ad9":"class Adam(Optimizer):\n    \"\"\"\n    The Adam optimizer (see https:\/\/arxiv.org\/abs\/1412.6980)\n    :param learning_rate: initial step size\n    :param beta1: Exponential decay rate of the first order moment\n    :param beta2: Exponential decay rate of the second order moment\n    :param eps: Small value for numerical stability\n    \"\"\"\n\n    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eps = eps\n\n        # map from layer id to a list of numpy arrays\n        self.m_values = {}\n        self.v_values = {}\n        # map from layer id to int (time step)\n        self.t_values = {}\n\n        self.current_id = -1\n        self._initialized = set()\n\n    def run_update_rule(self, gradients, layer):\n        param_deltas = []\n        \n        self.current_id = id(layer)\n\n        if not self.initialized:\n            self.initialize(gradients)\n            \n        # TODO: Implement Adam Update Rule, save result in variable param_deltas and return\n        # (for this bonus task you need to figure out the implementation details of Adam for yourself)\n        # hint: self.m_values contains a *list* of \"m_{t-1}\" arrays (exactly one value with the correct shape for each gradient in the `gradients` list)\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n        return param_deltas\n\n    def initialize(self, gradients):\n        self.m_values[self.current_id] = [np.zeros_like(gradient) for gradient in gradients]\n        self.v_values[self.current_id] = [np.zeros_like(gradient) for gradient in gradients]\n        self.t_values[self.current_id] = 0\n        self._initialized.add(self.current_id)\n\n    @property\n    def initialized(self):\n        return self.current_id in self._initialized","725fc179":"# Implementing Basic Neural Networks with LENGTH\n\nThis is the first practical exercise of our course [Applied Edge AI](https:\/\/learn.ki-campus.org\/courses\/edgeai-hpi2022).\nIn this exercise, you are going to implement a few basic functions and library components of neural networks from scratch in Python.\n\nBefore we can actually start, install our pip package, which installs the surrounding library LENGTH (Lightning-fast Extensible Neural-network Guarding The HPI), by running the following cell (click the triangular \"Play\" button next to the cell or in the top bar).\n\nTo allow installation of packages, you need to have *your phone number verified* [in your Kaggle profile](https:\/\/www.kaggle.com\/me\/account).\nThen you can enable the Internet switch in the sidebar on the right (you can open the sidebar by clicking on the |< Symbol in the top right, then select Settings, Internet).\nThe switch should show a checkmark afterwards.","d0a4935e":"# What now?\n\nNow you should have implemented a few basic functions of a neural network from scratch.\nWe hope this has provided you with a deeper knowledge of how neural network layers are implemented and how the models can be constructed and trained.\n\nYou can now experiment a bit more with the options given above, for example:\n\n* try to choose a different learning rate and see how it affects the training and testing accuracy\n* train a model for the more challenging FashionMNIST dataset\n* train for a few more epochs and see if you can achieve a higher accuracy\n* try a different learning rate update rule in the training loop implementation\n\nMake sure you attempt to complete all four tasks to the best of your abilities, as we will have questions in the graded quiz which relate to this exercise.\n\nYou can also attempt our bonus task below.\n\n# Optional Bonus Task: Adam\n\nIn this task (which is completely optional and a bit more challenging - but at the same time highly rewarding if you manage to complete it) you can implement the famous Adam optimizer yourself.\nThis optimizer was first proposed in an [arxiv preprint](https:\/\/arxiv.org\/abs\/1412.6980) in 2014.\n\nIt is popular because it is less sensitive to choosing a suitable learning rate, and many improvements to the original implementation have since been found.\nYou can absolutely implement the optimizer based on the original paper linked above, but of course you are welcome to find additional sources and explanations online if you encounter difficulties.\n\n## Mathematics and Programming\n\nTo highlight the mathematical side of deep learning we present a possible mathematical reformulation when updating the biased first moment estimates in the following.\nThe original paper suggests the formula:\n\n$$m_t \\leftarrow \\beta_1\\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t$$\n\nHowever we can simplify the equation on the right side by adding a \"zero\" ($m_{t-1} - m_{t-1}=0$)\n$$m_t \\leftarrow m_{t-1} - m_{t-1} + \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t$$\nThis allows us to factor out $-m_{t-1}$\n$$m_t \\leftarrow m_{t-1} - m_{t-1} \\cdot (1-\\beta_1) + (1-\\beta_1) \\cdot g_t$$\nand also factor out $(1-\\beta_1)$:\n$$m_t \\leftarrow m_{t-1} + (1-\\beta_1) \\cdot (g_t - m_{t-1})$$\n\nThis may not look simpler until we realize this allows us to use the `+=` operator in Python pseudocode:\n```\nm += (1 - self.beta1) * (g - m)\n```\nSimilarly the second raw moment estimate update can be reformulated to\n\n$$v_t \\leftarrow v_{t-1} + (1-\\beta_2) \\cdot (g_t^2 - v_{t-1})$$\n\nwhich in Pseudocode can be written as `v += (1 - self.beta2) * (g * g - v)`.\n","e36f0cb0":"If your Adam implemenation is correct you should be able to achieve about .99 **test** accuracy on **MNIST** after the **first** or **second** epoch (with a learning rate of 0.001).\nEdit the code cell in the section \"Running the Training\" to test this (look for the lines with comments containing `# BONUS TASK ADAM`).\n\nIf your implementation is correct you can gain 2 bonus points for completing our [bonus quiz](https:\/\/learn.ki-campus.org\/courses\/edgeai-hpi2022\/items\/7GjtyHsBaGNtWNVbbvPqxa) of this week.\nBefore you attempt the quiz, you should train an MLP on **FashionMNIST** with Adam with the same settings (you need to uncomment another line in the training code to change the dataset - keep the learning rate of 0.001).","91d40cb5":"# Train Loop\n\nNow we still need a training loop implementation.\nWe have provided one below, but you should carefully try to understand the concepts of what is happening here.\n\n(Next week, we are going to implement our own training loop with PyTorch in the exercise. It is going to look slightly different.)","6c94d751":"# Building a MultiLayerPerceptron (MLP)\n\nNow we have implemented all the building blocks required to build and train a MultiLayerPerceptron.\nPlease explore the code below a bit, especially how our layers are initialized in the `__init__` function, and how they are used during the forward pass.\nAlso you can see how our functions (`relu` and `mean_squared_error`) are used, but do not need initialization or saving, since they - in contrast to our `FullyConnected` layers - store no weights .\n\nWe only need to save the output of our loss function in `self.loss` to be able to calculate gradients in the backward pass.","644641d4":"# Task 1: SGD\n\nIn one of the lectures in our course we learned about SGD.\nIn the following task we want to compute the parameter delta to actually implement SGD.\nIf you look up the formula on our course slides, the `param_deltas` are subtracted by our framework, thus we do *not* need to multiply our result with -1 in the code.\nAlso note, that the variable `gradients` is the *list* of computed derivatives, thus the derivative part of the formula is already computed here.","2b268c9a":"# Task 2: Fully Connected Layer\n\nIt seems we have not learned a lot about a \"fully connected\" layer (sometimes also called a \"Dense\" layer) in our lecture so far.\nHowever we learned about Perceptrons and Multi Layer Perceptrons (MLPs), and fully connected layers are the building block of these early models for machine learning.\n\nThey simply store a weight between each possible input and output value and a bias for each output value.\nFor example, if we have 2 inputs $i_0,i_1$ and 3 outputs $o_0,o_1,o_2$, we store 6 weight values $w_{00}, w_{01}, w_{10}, w_{11}, w_{20},w_{21}$, one value for each pair of one input and one output, and three bias values $b_0,b_1,b_2$, one for each output.\nThen during the forward pass the outputs of a fully connected layer are calculated as\n$$o_x = \\sum_{y=0}^1{(i_y \\cdot w_{xy}) + b_x} \\text{ for } 0 \\leq x < 3.$$\nThis is simplified for a single element but in a neural network we work with mini-batches (processing a small number of samples at the same time).\nIn this case, we can use the [dot product](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.dot.html) of two arrays to simplify this element-wise multiplication.\n\nHere is a small very simple code snippet for the forward pass of our example case above with a batch size of 5.\nThe batch axis complicates matters slightly, but pay attention to the shape of each array and how the output size changes.\nYou can play around with different input and output sizes here if you want:","f173956f":"Now we are going to prepare a few imports for the following code cells:","4e965c91":"If you want to test your solution, you can execute the following code cell.\nIf your solution passes our test, it will simply print `Test passed.`\nIf it does not work, you will get an error.\nIn this case you need to fix the code above.\n(And do not forget to run the above code cell again!)","09798bff":"# Running the Training\n\nFinally, we can train our MLP.\nWe have provided code to load two datasets [MNIST](http:\/\/yann.lecun.com\/exdb\/mnist\/) and [FasionMNIST](https:\/\/github.com\/zalandoresearch\/fashion-mnist).\nThe first one classifies handwritten digits in grayscale images, the latter one classifies items of clothing based on grayscale images.\n\nSince these datasets are extremely small, we can even train our model on CPU and it should not take longer than a few minutes.\nYou should be able to simply run the code below right now.\nNote how the train and test accuracy changes during the training process.\n\nTraining with the default settings (using our implemented SGD optimizer starting with a learning rate of 0.1) our MLP on MNIST should achieve about 0.78 (78%) **test** accuracy after 5 epochs of training.\n\nIf you implement the Adam optimizer in the bonus task at the end of this notebook, you need to edit the code below to enable training with it, look for the instructions in the comments after each line starting with `# BONUS TASK ADAM`.","971074e9":"# Task 3: Mean Squared Error\n\nTo train a model, we also need a loss function.\nThese loss functions mathematically define how our model should be optimized (and thus learn to solve a certain task).\n\nA very simple loss function, which still can be effective in training models is the [Mean Squared Error](https:\/\/en.wikipedia.org\/wiki\/Mean_squared_error).\nTherefore in the following task, we are going to implement this function.\nThe corresponding [wikipedia article](https:\/\/en.wikipedia.org\/wiki\/Mean_squared_error) should explain everything you need to know, if you are not yet familiar with the function.","09e28eae":"If you want to test your solution, you can execute the following code cell.\nIf your solution passes our test, it will simply print `Test passed.`\nIf it does not work, you will get an error.\nIn this case you need to fix the code above.\n(And do not forget to run the above code cell again!)","e9220530":"# Task 4: ReLU (Rectified Linear Unit)\n\nIn our course we learned about how the ReLU function helped to solve the problem of vanishing gradients in the video [A Concise History of Neural Networks (3\/4)](https:\/\/learn.ki-campus.org\/courses\/edgeai-hpi2022\/items\/22nlBMim7pwAX8A7gTI7qn).\n\nIn the next task, we are going to implement this function by filling in the missing forward and backward pass.","8cf68152":"If you want to test your solution, you can execute the following code cell.\nIf your solution passes our test, it will simply print `Test passed.`\nIf it does not work, you will get an error.\nIn this case you need to fix the code above.\n(And do not forget to run the above code cell again!)","41695bca":"If you want to test your solution, you can execute the following code cell.\nIf your solution passes our test, it will simply print `Test passed.`\nIf it does not work, you will get an error.\nIn this case you need to fix the code above.\n(And do not forget to run the above code cell again!)","9a9c4a75":"One interesting fact here can help you with implementing the backward pass:\napplying the dot product on two arrays with shapes (5,2) and (2,3) \"removes\" the common axis with size 2 and results in an array of (5,3).\nYou can use this knowledge to figure out which arrays need to be transposed to get the correct shape during the *backward* pass.\n\nFurthermore, you can recap how multiplication (between weights and inputs) affects the gradients in our lecture video [Computational Graph](https:\/\/learn.ki-campus.org\/courses\/edgeai-hpi2022\/items\/3btmrU8Ds8rVDk8SEUz1pU).\n(Remember instead of using simple multiplication we can use the dot product for arrays.)\nIn the same video you can also recap what happens when we add *two* values in a computational graph (in this case the result of the dot product and our bias), so you can later on implement the *backward* pass for the *bias* correctly.\n\nAlso you may have noted, that we *transposed* the weight array in the example above - \"swapping\" the axes from (3,2) to (2,3) - with numpy before the dot product by calling `.T`.\n\nThis is because we recommend storing the weight array in a *transposed* way in your implementation (similar to our example above).\n\nOne final hint before you are ready to start implementing: we use the variable name `x` for the inputs (`grad_x` for the gradients with respect to the inputs) in our code below, instead of `i` in the example above (we only used `i` above so it maches the previous formula)."}}