{"cell_type":{"96939492":"code","522e4071":"code","63242c39":"code","f8f0bc58":"code","46b849b7":"code","4132333c":"code","fbae5001":"code","882a4b90":"code","3e6bc9be":"code","6ef80a65":"code","37c57fe7":"code","01362fe1":"code","c950d054":"code","b884a8fd":"code","d98a19d7":"code","091e7e2d":"code","47ae5b52":"code","9acc3c9b":"code","2e9a4fd9":"code","5212b69c":"code","8a90c660":"code","6639cdce":"code","e4c432db":"code","242db9e2":"code","f73a9733":"code","f87f1582":"markdown","8e7079f6":"markdown","466b58b7":"markdown","57eea3e1":"markdown","4cc0c1a1":"markdown","58ff8560":"markdown","c79097fd":"markdown","ce54f66a":"markdown","0c3f8e6d":"markdown","a63554e4":"markdown","bca253d0":"markdown","b416273a":"markdown","017f3929":"markdown","66058d18":"markdown","d504023a":"markdown","de5fea30":"markdown","bf290d0e":"markdown","d247993e":"markdown","26e47fd4":"markdown","202da3f3":"markdown","69d7dcee":"markdown","1c418b6c":"markdown"},"source":{"96939492":"import numpy as np # linear algebra\nfrom numpy.random import seed\nseed(1)\nimport pandas as pd \nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\nimport re\nimport nltk\nfrom collections import defaultdict\nfrom collections import  Counter\nimport seaborn as sns\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n","522e4071":"tweet = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntestset = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntweet.head()","63242c39":"ax = tweet.replace({\"target\" : {1 : \"Disaster\", 0 : \"Not disaster\"}}).groupby(['target'])['target'].count().plot.bar(title = \"Train set count by disaster\/not disaster\")\n_ = ax.set_xlabel('Disaster?')\n_ = ax.set_ylabel('Count')\n","f8f0bc58":"def create_corpus():\n    corpus=[]\n    \n    for x in tweet['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","46b849b7":"corpus = create_corpus()\nlst_stopwords = nltk.corpus.stopwords.words(\"english\")\n\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in lst_stopwords) :\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)","4132333c":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","fbae5001":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(corpus)[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","882a4b90":"import operator\n\ndef build_vocab(X):\n    \n    tweets = X.apply(lambda s: s.split()).values      \n    vocab = {}\n    \n    for tweet in tweets:\n        for word in tweet:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1                \n    return vocab\n\ndef check_embeddings_coverage(X, embeddings):\n    \n    vocab = build_vocab(X)    \n    \n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word] = embeddings[word]\n            n_covered += vocab[word]\n        except:\n            oov[word] = vocab[word]\n            n_oov += vocab[word]\n            \n    vocab_coverage = len(covered) \/ len(vocab)\n    text_coverage = (n_covered \/ (n_covered + n_oov))\n    \n    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    return sorted_oov, vocab_coverage, text_coverage","3e6bc9be":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","6ef80a65":"train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(tweet[\"text\"], embedding_dict)\ntest_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(testset['text'], embedding_dict)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))\n","37c57fe7":"def utils_preprocess_text(text):\n        ## clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n     \n    ## clean urls \n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    text = url.sub(r'',text)\n    \n    url = re.compile(r'http?:\/\/\\S+|www\\.\\S+')\n    text = url.sub(r'',text)\n    \n    ## remove html \n    html=re.compile(r'<.*?>') \n    html.sub(r'',text)\n        \n        \n    text = re.sub(r'mh370','flight crash',text)     \n    text = re.sub(r'\u00fb_','',text)     \n    text = re.sub(r'\u00fb\u00f2','',text) \n    text = re.sub(r'typhoondevastated','typhoon devastated',text)      \n    text = re.sub(r'irandeal','iran deal',text)      \n    text = re.sub(r'worldnews','world news',text)      \n    text = re.sub(r'animalrescue','animal rescue',text)      \n    text = re.sub(r'viralspell','viral spell',text)      \n    text = re.sub(r'grief\u00fb\u00aa','grief',text)      \n    text = re.sub(r'pantherattack','panther attack',text)      \n    text = re.sub(r'injuryi495','injury in 495',text) \n    text = re.sub(r'explosionproof','explosion proof',text) \n    text = re.sub(r'america\u00fb\u00aas','americans',text) \n    return text\n","01362fe1":"df=pd.concat([tweet,testset])\ndf.shape","c950d054":"\ndf[\"text_clean\"] = df[\"text\"].apply(lambda x: utils_preprocess_text(x))\ndf.head()","b884a8fd":"df_glove_oov, df_glove_vocab_coverage, df_glove_text_coverage = check_embeddings_coverage(df[\"text_clean\"], embedding_dict)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(df_glove_vocab_coverage, df_glove_text_coverage))\n","d98a19d7":"corpus = []\ncorpus = df[\"text_clean\"]","091e7e2d":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","47ae5b52":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","9acc3c9b":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec   ","2e9a4fd9":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","5212b69c":"model.summary()","8a90c660":"train=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","6639cdce":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","e4c432db":"history=model.fit(X_train,y_train,batch_size=32,epochs=10,validation_data=(X_test,y_test),verbose=2)","242db9e2":"train_pred_GloVe = model.predict(train)\ntrain_pred_GloVe_int = train_pred_GloVe.round().astype('int')","f73a9733":"test_pred_GloVe = model.predict(test)\ntest_pred_GloVe_int = test_pred_GloVe.round().astype('int')\n\nsubmission['target'] = test_pred_GloVe_int\nsubmission.head(10)\n\nsubmission.to_csv(\"submission.csv\", index=False, header=True)\n","f87f1582":"# Read data\nLets read both train and test set. Also we will create a df for submission which we will use later.","8e7079f6":"Thats pretty low, let do some cleaning to bring data closer to embedding. On inspecting the out of coverage item found some misspellings. So we will remove punctuation,urls, correct spellings ","466b58b7":"# Analyze bi-grams","57eea3e1":"Now for each word in our corpus prepare embedding matrix, which maps word to embedding","4cc0c1a1":"# Combine data in train and test set ","58ff8560":"# Baseline model with glove results","c79097fd":"# Lets see data distribution","ce54f66a":"# Analyze common words\n\nThanks to this notebook [basic-eda-cleaning-and-glove](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)","0c3f8e6d":"Lets check coverage with embeddings post cleaning","a63554e4":"# Imports","bca253d0":"# Create corpus of all words in trainset","b416273a":"Lets check coverage on train and test set","017f3929":"# Data PreProcessing","66058d18":"# Now lets split train set between train and validation set ","d504023a":"# Text coverage\n\nWith pretrained embeddings its often useful to get the text as close to embedding as possible. So we will not use techniques like stopwords removal, stemming and lammatizing. Now are are checking how many words in our vocabulary are part of embedding and what is the text coverage. ","de5fea30":"Split the processed corpus back using train set length to train and test set","bf290d0e":"# Lets call the preprocessing routine to clean data","d247993e":"# Convert sentences to sequences using keras preprocessing\nUse Keras text preprocessing to first tokenize the words (map each unique word to index ref: Tokenizer) and then convert each sentence to sequence using these indexes(texts_to_sequences). \nWe will also pad and tuncate to make sure all sentences are 50 in length","26e47fd4":"# Now lets fit the model","202da3f3":"# We will use Glove 100d embeddings","69d7dcee":"# Lets predict","1c418b6c":"# Prepare submission "}}