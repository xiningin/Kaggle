{"cell_type":{"7be90c7f":"code","2b7888d0":"code","bef3ae8c":"code","9226df55":"code","be6a799d":"code","0bc71d59":"code","70df27e9":"code","45bbda1b":"code","29087f5f":"code","5a8acc5c":"code","5724d9ef":"code","32afe0e2":"code","fb287173":"code","180f4da0":"code","036162a9":"code","5530d3c2":"code","3cd201ef":"code","53dbabdf":"code","a850bfbf":"code","1d5582b9":"code","43bfa74c":"code","d48247d0":"code","498729e3":"code","f31f861b":"code","3a4c4ecc":"markdown","f550579a":"markdown","28758654":"markdown","6e0cb3d3":"markdown","c0c691a5":"markdown","f9a9cf95":"markdown","66e6f697":"markdown","a29b7676":"markdown","a8f97402":"markdown","54b3b657":"markdown","aed7c7fc":"markdown"},"source":{"7be90c7f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2b7888d0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nd0 = pd.read_csv('..\/input\/train.csv')\nprint(d0.head(5))","bef3ae8c":"# Labels and Data is seggregated\nl = d0['label']\nd = d0.drop('label',axis = 1)\nprint(d.shape)\nprint(l.shape)","9226df55":"#diplay a number from the dataset\nplt.figure(figsize=(7,7))\nidx = 150\n\ngrid_data = d.iloc[idx].as_matrix().reshape(28,28)\nplt.imshow(grid_data, interpolation = \"none\", cmap = \"gray\")\n\nplt.show()\n\nprint(l[idx])","be6a799d":"#Pick first 15k data-points to work on for time-efficiency.\n#Exercise: Perform the same analysis on all of 42K data-point\n\nlabels = l.head(15000)\ndata = d.head(15000)\n\nprint(\"the shape of sample data = \", data.shape)","0bc71d59":"#Data-preprocessing: Standardizing the data\n\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(data)\nprint(standardized_data.shape)","70df27e9":"#find the co-variance matrix which is: A^T * A\n\nsample_data = standardized_data\n\n#matrix multiplication using numpy\ncovar_matrix = np.matmul(sample_data.T, sample_data)\n\nprint(\"The shape of variance matrix = \", covar_matrix.shape)","45bbda1b":"# finding the top two eigen-values and corresponding eigen-vectors\n# projecting onto a 2-Dim space\n\nfrom scipy.linalg import eigh\n\n# the parameter 'eigvals' is defined (low value to high value)\n# eigh function will return the eigen values in ascending order\n# this code generate only the top 2 (782 and 783) eigenvalues.\n\nvalues, vectors = eigh(covar_matrix, eigvals = (782,783))\nprint(\"Shape of eigen vectors = \",vectors.shape)\n\n#converting the eigen vectors into (2,d) shape for the easiness of further \n\nvectors = vectors.T\n\nprint(\"Updated shape of eigen vectors =\",vectors.shape)\n\n# vectors[1] correspond to 1st principal component\n# vector[2] correspond to 2nd principal component\n","29087f5f":"# projecting the original data sample on the plane\n# formed by two prinicpal eigen vectors by vector-vector multiplication\n\nimport matplotlib.pyplot as plt\nnew_coordinates = np.matmul(vectors, sample_data.T)\n\nprint(\"resultant new data points shape \",vectors.shape, \"X\", sample_data.shape)","5a8acc5c":"import pandas as pd\n\n# appending label to the 2d projected data\nnew_coordinates = np.vstack((new_coordinates, labels)).T\n\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nprint(dataframe.head())","5724d9ef":"# ploting the 2d data points with seaborn\nimport seaborn as sn\nsn.FacetGrid(dataframe, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","32afe0e2":"#intiliazing the pca\nfrom sklearn import decomposition\npca = decomposition.PCA()","fb287173":"# configruing the parameters\n# the number if components = 2\n\npca.n_components = 2\npca_data = pca.fit_transform(sample_data)\n\n#pca_reduced will contain the 2-d projects of the simple data\nprint(\"shape of pca_reduced.shape= \",pca_data.shape)\n","180f4da0":"# attaching the label for each 2-d data point \npca_data = np.vstack((pca_data.T, labels)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nsn.FacetGrid(pca_df, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","036162a9":"# PCA for dimensionality redcution (non-visualization)\n\npca.n_components = 784\npca_data = pca.fit_transform(sample_data)\n\n# percentage variance\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n#print(percentage_var_explained)\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()\n\n\n# If we take 200-dimensions, approx. 90% of variance is expalined.","5530d3c2":"from sklearn.manifold import TSNE\n\n#picking the top 1000 point as TSNE takes a lot of time for 15k points\ndata_1000 = standardized_data[0:1000,:]\nlabels_1000 = labels[0:1000]\n","3cd201ef":"model = TSNE(n_components = 2, random_state = 0)\ntsne_data = model.fit_transform(standardized_data)\n\n#creating a new data frame which help us in plotting the result data\ntsne_data = np.vstack((tsne_data.T, labels)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\",\"Dim_2\",\"label\"))\n\n#Plotting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\",size=6).map(plt.scatter,'Dim_1','Dim_2').add_legend()\nplt.show()","53dbabdf":"model = TSNE(n_components=2, random_state=0, perplexity=50)\ntsne_data = model.fit_transform(standardized_data) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50')\nplt.show()","a850bfbf":"model = TSNE(n_components=2, random_state=0, perplexity=50,  n_iter=5000)\ntsne_data = model.fit_transform(standardized_data) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50, n_iter=5000')\nplt.show()","1d5582b9":"model = TSNE(n_components=2, random_state=0, perplexity=2)\ntsne_data = model.fit_transform(standardized_data) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 2')\nplt.show()","43bfa74c":"# TSNE\n\nfrom sklearn.manifold import TSNE\n\n# Picking the top 1000 points as TSNE takes a lot of time for 15K points\ndata_1000 = standardized_data[0:1000,:]\nlabels_1000 = labels[0:1000]\n\nmodel = TSNE(n_components=2, random_state=0)\n# configuring the parameteres\n# the number of components = 2\n# default perplexity = 30\n# default learning rate = 200\n# default Maximum number of iterations for the optimization = 1000\n\ntsne_data = model.fit_transform(data_1000)\n\n\n# creating a new data frame which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.show()","d48247d0":"model = TSNE(n_components=2, random_state=0, perplexity=50)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50')\nplt.show()","498729e3":"model = TSNE(n_components=2, random_state=0, perplexity=50,  n_iter=5000)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50, n_iter=5000')\nplt.show()","f31f861b":"model = TSNE(n_components=2, random_state=0, perplexity=2)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 2')\nplt.show()","3a4c4ecc":"We will start with importing few libaries and MNIST Dataset.","f550579a":"**t-SNE**\n\nhttps:\/\/distill.pub\/2016\/misread-tsne\/","28758654":"**Principal Component Analysis(PCA)**\n\nPrincipal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.\n\nWe will convert 784 dimesional data into 2 dimensional data using PCA. Below is an interesting read for more information:\nhttps:\/\/stats.stackexchange.com\/questions\/2691\/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\/140579#140579","6e0cb3d3":"**Dimensionality Reduction and Visualization of MNIST Dataset**\n\nIn statistics, machine learning, and information theory, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction.","c0c691a5":"Please note the plot obtained for both the methods is same.","f9a9cf95":"Now we will code the model and configure the same.\n1. Number of components = 2\n2. Default Perplexity = 30\n3. Default learning rate = 200\n4. Default Maximum number of iterations for the optimization = 1000\n","66e6f697":"Scipy Documnetation: https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.linalg.eigh.html","a29b7676":"**PCA Using Scikit-Learn**\n\nFor demonstration, I have computed PCA showing each step. Actually, Scikit-Learn does it better with only few lines of code and similar results. ","a8f97402":"**Computing PCA using the covariance method**\n\nWe will adopt the following steps in python to perform PCA:\n1. Calculate the Covariance Matrix\n2. Calculate the eigenvectors and eigenvalues of the covariance matrix\n3. Choosing the components and forming a feature vector\n4. Deriving the new dataset\n\n","54b3b657":"Documentation link of standard scaler:\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html","aed7c7fc":"**PCA for dimensionality reduction(not for visualization)**"}}