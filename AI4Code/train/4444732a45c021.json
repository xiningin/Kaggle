{"cell_type":{"858a1231":"code","d05da423":"code","ec455858":"code","224d9958":"code","414c3705":"code","28f7393a":"code","9541edf9":"code","8d643ce4":"code","908adc60":"code","38357f54":"code","bc53a1d7":"code","f57c723d":"code","407b07a6":"code","33be5d44":"code","438168ef":"code","0b346cb8":"code","806bf924":"code","1863d4d4":"code","ac6ac31c":"code","bb0c9160":"code","2f964036":"code","0c7b5ce0":"code","e078a426":"code","bb889fcb":"code","f9eff943":"code","af95b38b":"code","30fbd45c":"markdown","fd6fb80a":"markdown","607bb7de":"markdown","48f3b4e5":"markdown","9c198698":"markdown","421f95c4":"markdown","a7765351":"markdown","b4edbf4d":"markdown","e8c0dcef":"markdown","1f98607e":"markdown","d765528e":"markdown","734fa39e":"markdown","9e7a22ef":"markdown","76512d4d":"markdown","05181e8d":"markdown","27aa4cab":"markdown","a51e04f1":"markdown","fa781e8a":"markdown"},"source":{"858a1231":"!pip install -q tensorflow==2.6.0","d05da423":"import tensorflow as tf\nimport pandas as pd\nfrom tensorflow.keras.layers import TextVectorization\nimport pathlib\nimport random\nimport string\nimport re\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sklearn\nfrom sklearn.model_selection import train_test_split","ec455858":"class Config:\n    vocab_size = 15000 # Vocabulary Size\n    sequence_length = 20\n    batch_size = 64\n    validation_split = 0.15\n    embed_dim = 256\n    latent_dim = 256\n    num_heads = 2\n    epochs = 1 # Number of Epochs to train\n    model_path = \"model.h5\"\n    begin_token = \"[start]\"\n    end_token = \"[end]\"\n    use_pretrained_model = True # Download pretrained Model to speed up the training\n    kaggle_data_path = \"\/kaggle\/input\/englishspanish-translation-dataset\/data.csv\"\n    num_tokens_per_batch = batch_size * sequence_length\n    num_expects = 1\nconfig = Config()","224d9958":"if os.path.exists(config.kaggle_data_path):\n  data = pd.read_csv(config.kaggle_data_path)\nelse:\n  file_path = keras.utils.get_file(\"data.csv\", \"https:\/\/raw.githubusercontent.com\/LoniQin\/english-spanish-translation-switch-transformer\/main\/data.csv\")\n  data = pd.read_csv(file_path)\ndata.head()","414c3705":"data[\"spanish\"] = data[\"spanish\"].apply(lambda item: config.begin_token + \" \" + item + \" \" + config.end_token)","28f7393a":"data.head()","9541edf9":"strip_chars = string.punctuation + \"\u00bf\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\nprint(strip_chars)\ndef spanish_standardize(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(lowercase, \"[%s]\"%re.escape(strip_chars), \"\")\nenglish_vectorization = TextVectorization(\n    max_tokens=config.vocab_size, \n    output_mode=\"int\", \n    output_sequence_length=config.sequence_length,\n)\nspanish_vectorization = TextVectorization(\n    max_tokens=config.vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=config.sequence_length + 1,\n    standardize=spanish_standardize,\n)\nenglish_vectorization.adapt(list(data[\"english\"]))\nspanish_vectorization.adapt(list(data[\"spanish\"]))","8d643ce4":"def preprocess(english, spanish):\n    return (english, spanish[:, :-1]), spanish[:, 1:]\ndef make_dataset(df, batch_size, mode):\n    english = english_vectorization(list(df[\"english\"]))\n    spanish = spanish_vectorization(list(df[\"spanish\"]))\n    dataset = tf.data.Dataset.from_tensor_slices((english, spanish))\n    if mode == \"train\":\n       dataset = dataset.shuffle(256) \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(preprocess)\n    dataset = dataset.take(len(df) \/\/ batch_size).cache().prefetch(16).repeat(1)\n    return dataset","908adc60":"train, valid = train_test_split(data, test_size=config.validation_split)\ntrain.shape, valid.shape","38357f54":"train_ds = make_dataset(train, batch_size=config.batch_size, mode=\"train\")\nvalid_ds = make_dataset(valid, batch_size=config.batch_size, mode=\"valid\")","bc53a1d7":"for batch in train_ds.take(1):\n    print(batch)","f57c723d":"def create_feedforward_network(ff_dim, name=None):\n    return keras.Sequential(\n        [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(ff_dim)], name=name\n    )","407b07a6":"def load_balanced_loss(router_probs, expert_mask):\n    # router_probs [tokens_per_batch, num_experts] is the probability assigned for\n    # each expert per token. expert_mask [tokens_per_batch, num_experts] contains\n    # the expert with the highest router probability in one\u2212hot format.\n\n    num_experts = tf.shape(expert_mask)[-1]\n    # Get the fraction of tokens routed to each expert.\n    # density is a vector of length num experts that sums to 1.\n    density = tf.reduce_mean(expert_mask, axis=0)\n    # Get fraction of probability mass assigned to each expert from the router\n    # across all tokens. density_proxy is a vector of length num experts that sums to 1.\n    density_proxy = tf.reduce_mean(router_probs, axis=0)\n    # Want both vectors to have uniform allocation (1\/num experts) across all\n    # num_expert elements. The two vectors will be pushed towards uniform allocation\n    # when the dot product is minimized.\n    loss = tf.reduce_mean(density_proxy * density) * tf.cast(\n        (num_experts ** 2), tf.dtypes.float32\n    )\n    return loss","33be5d44":"class Router(layers.Layer):\n    def __init__(self, num_experts, expert_capacity):\n        self.num_experts = num_experts\n        self.route = layers.Dense(units=num_experts)\n        self.expert_capacity = expert_capacity\n        super(Router, self).__init__()\n\n    def call(self, inputs, training=False):\n        # inputs shape: [tokens_per_batch, embed_dim]\n        # router_logits shape: [tokens_per_batch, num_experts]\n        router_logits = self.route(inputs)\n\n        if training:\n            # Add noise for exploration across experts.\n            router_logits += tf.random.uniform(\n                shape=router_logits.shape, minval=0.9, maxval=1.1\n            )\n        # Probabilities for each token of what expert it should be sent to.\n        router_probs = keras.activations.softmax(router_logits, axis=-1)\n        # Get the top\u22121 expert for each token. expert_gate is the top\u22121 probability\n        # from the router for each token. expert_index is what expert each token\n        # is going to be routed to.\n        expert_gate, expert_index = tf.math.top_k(router_probs, k=1)\n        # expert_mask shape: [tokens_per_batch, num_experts]\n        expert_mask = tf.one_hot(expert_index, depth=self.num_experts)\n        # Compute load balancing loss.\n        aux_loss = load_balanced_loss(router_probs, expert_mask)\n        self.add_loss(aux_loss)\n        # Experts have a fixed capacity, ensure we do not exceed it. Construct\n        # the batch indices, to each expert, with position in expert make sure that\n        # not more that expert capacity examples can be routed to each expert.\n        position_in_expert = tf.cast(\n            tf.math.cumsum(expert_mask, axis=0) * expert_mask, tf.dtypes.int32\n        )\n        # Keep only tokens that fit within expert capacity.\n        expert_mask *= tf.cast(\n            tf.math.less(\n                tf.cast(position_in_expert, tf.dtypes.int32), self.expert_capacity\n            ),\n            tf.dtypes.float32,\n        )\n        expert_mask_flat = tf.reduce_sum(expert_mask, axis=-1)\n        # Mask out the experts that have overflowed the expert capacity.\n        expert_gate *= expert_mask_flat\n        # Combine expert outputs and scaling with router probability.\n        # combine_tensor shape: [tokens_per_batch, num_experts, expert_capacity]\n        combined_tensor = tf.expand_dims(\n            expert_gate\n            * expert_mask_flat\n            * tf.squeeze(tf.one_hot(expert_index, depth=self.num_experts), 1),\n            -1,\n        ) * tf.squeeze(tf.one_hot(position_in_expert, depth=self.expert_capacity), 1)\n        # Create binary dispatch_tensor [tokens_per_batch, num_experts, expert_capacity]\n        # that is 1 if the token gets routed to the corresponding expert.\n        dispatch_tensor = tf.cast(combined_tensor, tf.dtypes.float32)\n\n        return dispatch_tensor, combined_tensor","438168ef":"class Switch(layers.Layer):\n    def __init__(self, num_experts, embed_dim, num_tokens_per_batch, capacity_factor=1):\n        self.num_experts = num_experts\n        self.embed_dim = embed_dim\n        self.experts = [\n            create_feedforward_network(embed_dim) for _ in range(num_experts)\n        ]\n        self.num_tokens_per_batch = num_tokens_per_batch\n        self.expert_capacity = num_tokens_per_batch \/\/ self.num_experts\n        self.router = Router(self.num_experts, self.expert_capacity)\n        super(Switch, self).__init__()\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        num_tokens_per_example = tf.shape(inputs)[1]\n\n        # inputs shape: [num_tokens_per_batch, embed_dim]\n        inputs = tf.reshape(inputs, [-1, self.embed_dim])\n        # dispatch_tensor shape: [expert_capacity, num_experts, tokens_per_batch]\n        # combine_tensor shape: [tokens_per_batch, num_experts, expert_capacity]\n        dispatch_tensor, combine_tensor = self.router(inputs)\n        # expert_inputs shape: [num_experts, expert_capacity, embed_dim]\n        expert_inputs = tf.einsum(\"ab,acd->cdb\", inputs, dispatch_tensor)\n        expert_inputs = tf.reshape(\n            expert_inputs, [self.num_experts, self.expert_capacity, self.embed_dim]\n        )\n        # Dispatch to experts\n        expert_input_list = tf.unstack(expert_inputs, axis=0)\n        expert_output_list = [\n            self.experts[idx](expert_input)\n            for idx, expert_input in enumerate(expert_input_list)\n        ]\n        # expert_outputs shape: [expert_capacity, num_experts, embed_dim]\n        expert_outputs = tf.stack(expert_output_list, axis=1)\n        # expert_outputs_combined shape: [tokens_per_batch, embed_dim]\n        expert_outputs_combined = tf.einsum(\n            \"abc,xba->xc\", expert_outputs, combine_tensor\n        )\n        # output shape: [batch_size, num_tokens_per_example, embed_dim]\n        outputs = tf.reshape(\n            expert_outputs_combined,\n            [batch_size, num_tokens_per_example, self.embed_dim],\n        )\n        return outputs","0b346cb8":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_proj, num_heads, **kwargs):\n        super(TransformerEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = dense_proj\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n        attention_output = self.attention(\n            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n        )\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)","806bf924":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n","1863d4d4":"class TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, dense_proj, num_heads, **kwargs):\n        super(TransformerDecoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = dense_proj\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n\n        attention_output_1 = self.attention_1(\n            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n        )\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n\n        attention_output_2 = self.attention_2(\n            query=out_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n        )\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n\n        proj_output = self.dense_proj(out_2)\n        return self.layernorm_3(out_2 + proj_output)\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0,\n        )\n        return tf.tile(mask, mult)","ac6ac31c":"def get_model(config):\n    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(encoder_inputs)\n    encoder_switch = Switch(config.num_expects, config.embed_dim, config.num_tokens_per_batch)\n    encoder_outputs = TransformerEncoder(config.embed_dim, encoder_switch, config.num_heads)(x)\n    encoder = keras.Model(encoder_inputs, encoder_outputs)\n\n    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n    encoded_seq_inputs = keras.Input(shape=(None, config.embed_dim), name=\"decoder_state_inputs\")\n    decoder_switch = Switch(config.num_expects, config.embed_dim, config.num_tokens_per_batch)\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(decoder_inputs)\n    x = TransformerDecoder(config.embed_dim, decoder_switch, config.num_heads)(x, encoded_seq_inputs)\n    x = layers.Dropout(0.5)(x)\n    decoder_outputs = layers.Dense(config.vocab_size, activation=\"softmax\")(x)\n    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"outputs\")\n\n    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n    transformer = keras.Model(\n        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n    )\n    return transformer","bb0c9160":"model = get_model(config)","2f964036":"model.summary()","0c7b5ce0":"keras.utils.plot_model(model, show_shapes=True)","e078a426":"model.compile(\n    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)","bb889fcb":"checkpoint = keras.callbacks.ModelCheckpoint(config.model_path, save_weights_only=True, save_best_only=True)\nearly_stopping = keras.callbacks.EarlyStopping(patience=5)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(patience=5, min_delta=1e-4, min_lr=1e-4)\nif config.use_pretrained_model:\n    model_path = keras.utils.get_file(config.model_path, \"https:\/\/github.com\/LoniQin\/english-spanish-translation-switch-transformer\/raw\/main\/\" + config.model_path)\n    model.load_weights(model_path)\n    # Just to save time. You may need to adjust code for further training here.\n    #model.fit(train_ds, epochs=1, validation_data=valid_ds, callbacks=[checkpoint, reduce_lr])\nelse:\n    model.fit(train_ds, epochs=config.epochs, validation_data=valid_ds, callbacks=[checkpoint, reduce_lr])","f9eff943":"spanish_vocab = spanish_vectorization.get_vocabulary()\nspanish_index_lookup = dict(zip(range(len(spanish_vocab)), spanish_vocab))\ndef decode_sequence(model, input_sentence):\n    tokenized_input_sentence = english_vectorization([input_sentence])\n    decoded_sentence = config.begin_token\n    for i in range(config.sequence_length):\n        tokenized_target_sentence = spanish_vectorization([decoded_sentence])[:, :-1]\n        predictions = model([tokenized_input_sentence, tokenized_target_sentence])\n\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = spanish_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n\n        if sampled_token == config.end_token:\n            break\n    return decoded_sentence","af95b38b":"#model = get_model(config)\n#model.load_weights(config.model_path)\nfor i in np.random.choice(len(data), 30):\n    item = data.iloc[i]\n    translated = decode_sequence(model, item[\"english\"])\n    print(\"English:\", item[\"english\"])\n    print(\"Spanish:\", item[\"spanish\"])\n    print(\"Translated:\", translated)","30fbd45c":"## References\n- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https:\/\/arxiv.org\/abs\/2101.03961)\n- [English-Spanish Translation: Transformer](https:\/\/www.kaggle.com\/lonnieqin\/english-spanish-translation-transformer)\n- [Attention Is All You Need](https:\/\/arxiv.org\/abs\/1706.03762v5)\n- [Text classification with Switch Transformer](https:\/\/keras.io\/examples\/nlp\/text_classification_with_switch_transformer\/)\n","fd6fb80a":"## Model Development","607bb7de":"### Model Training\n","48f3b4e5":"## Import datasets","9c198698":"# English-Spanish Translation: Switch Transformer\n## Table of Contents\n* Overview\n* Setup\n* Import datasets\n* Model Development\n* Transalation\n* References","421f95c4":"## Translation\n","a7765351":"### The Positional Embedding","b4edbf4d":"Let's visualize the Model.","e8c0dcef":"### The Transformer Decoder","1f98607e":"### The Switch Layer","d765528e":"### Implement the feeforward network","734fa39e":"### Implement the Router as a layer","9e7a22ef":"## Overview\nIn this Notebook, I will develop a English-Spanish Translation Model using Switch Transformer from scratch. Please be noted that running this notebook is 10 times slower than Google Colab when using GPU. So I set this notebook download the pretrained Model to speed up the training.","76512d4d":"### The TransformerEncoder","05181e8d":"### Implement the load-balanced loss\nThis is an auxiliary loss to encourage a balanced load across experts.\n","27aa4cab":"## Setup","a51e04f1":"### The Model","fa781e8a":"I have pretrained the Model. You may use it to speed up the training."}}