{"cell_type":{"6286164a":"code","8a25f882":"code","51c5ff0c":"code","78688c10":"code","ac48fb5c":"code","1d2b2a8e":"code","4d73c1e0":"code","2d7a95be":"code","c0f203af":"code","6015eca4":"code","da89f493":"code","d79b8275":"code","d22f86aa":"code","012df812":"code","e7493291":"code","b443fa2d":"code","7e1b750c":"code","2ff69868":"code","c03ae30c":"code","fa3a0b7a":"code","151a6edd":"code","142d50c0":"code","6135312c":"code","ae8a2ac9":"code","7e99924b":"code","3538c81b":"code","923b629e":"code","6dbb1014":"code","68583741":"code","dfa742a6":"code","412e7b3b":"code","137e09b9":"code","b23d0f8e":"code","4fe10fe5":"code","d344ee6f":"code","101b5989":"code","dfa11598":"code","dbfd1500":"code","ba409bdc":"code","70cec36d":"code","4bfab034":"code","ab9b26ac":"code","eef55837":"code","fb06e9d2":"code","adb74e30":"code","83ee65a2":"markdown","37de7f1a":"markdown","cac9b8e3":"markdown","8db44e55":"markdown","9beef643":"markdown","9363ad9b":"markdown","0e8a49e8":"markdown","883b12af":"markdown","ccd728fa":"markdown","c86d80cb":"markdown","3f261327":"markdown","60620cd8":"markdown","4ad3649a":"markdown","aea00e0c":"markdown","ce59af34":"markdown","a100a000":"markdown","cc24ac4d":"markdown","35719a17":"markdown","3c341f23":"markdown","d89f753f":"markdown","e8566797":"markdown","3e2192a2":"markdown","4fcdaf2a":"markdown","265a95d1":"markdown","5db08d51":"markdown","3a69a943":"markdown","ca711c5d":"markdown","4eda2068":"markdown","8593adc6":"markdown","d439d708":"markdown","83b97e1c":"markdown","bc6f73db":"markdown","1d40c6f4":"markdown","f1bed8d1":"markdown","e0d10f1c":"markdown","a08f58b1":"markdown","d4c5fcd8":"markdown","b8980bc4":"markdown","334fac93":"markdown","ceedd67a":"markdown","ccf3a342":"markdown"},"source":{"6286164a":"# Data wrangling\nimport pandas as pd\nimport numpy as np\n\n# Data visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, RFE, RFECV\nfrom sklearn.decomposition import PCA\n\n# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')","8a25f882":"data = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndata.head()","51c5ff0c":"print(\"Shape of dataframe: \", data.shape)","78688c10":"# Missing data\n\nmissing = data.isnull().sum()\nmissing[missing > 0]","ac48fb5c":"# Drop ID and Unnamed columns\n\nprint(\"Before: \", data.shape)\ndata = data.drop(['id', 'Unnamed: 32'], axis = 1)\nprint(\"After: \", data.shape)","1d2b2a8e":"data.dtypes.value_counts()","4d73c1e0":"data.dtypes","2d7a95be":"# Standardise all features so that they follow a standard Gaussian distribution\n\noriginal_features = data.drop('diagnosis', axis = 1)\nstandard_features = (original_features - original_features.mean()) \/ original_features.std()\nstandard_data = pd.concat([data['diagnosis'], standard_features], axis = 1)","c0f203af":"# Divide the standardised features into 3 groups \n\nfeature_mean = standard_data.iloc[:, 1:11]\nfeature_se = standard_data.iloc[:, 11: 21]\nfeature_worst = standard_data.iloc[:, 21:31]","6015eca4":"standard_data.head()","da89f493":"# Encode target variable\n\ndata['diagnosis'] = data['diagnosis'].map({'B': 0, 'M': 1})\nstandard_data['diagnosis'] = standard_data['diagnosis'].map({'B': 0, 'M': 1})","d79b8275":"# Value counts\n\ntarget = data['diagnosis']\ntarget.value_counts()","d22f86aa":"total = len(data)\nplt.figure(figsize = (6, 6))\nplt.title('Diagnosis Value Counts')\nax = sns.countplot(target)\nfor p in ax.patches:\n    percentage = '{:.0f}%'.format(p.get_height() \/ total * 100)\n    x = p.get_x() + p.get_width() \/ 2\n    y = p.get_height() + 5\n    ax.annotate(percentage, (x, y), ha = 'center')\nplt.show()","012df812":"# Heatmap\n\ncorrelation = feature_mean.corr()\nplt.figure(figsize = (10, 8))\nplt.title('Correlation Between Predictor Variables')\nsns.heatmap(correlation, annot = True, fmt = '.2f', cmap = 'coolwarm')","e7493291":"# Pairplot between correlated features\n\nsns.pairplot(feature_mean[['radius_mean', 'perimeter_mean', 'area_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean']])","b443fa2d":"feature_mean = pd.concat([target, feature_mean], axis = 1)\nfeature_mean.head()","7e1b750c":"mean_melt = pd.melt(feature_mean, id_vars = 'diagnosis', var_name = 'feature', value_name = 'value')\nmean_melt.head()","2ff69868":"# Violinplot\n\nplt.figure(figsize = (12, 8))\nsns.violinplot(x = 'feature', y = 'value', hue = 'diagnosis', data = mean_melt, split = True, inner = 'quart')\nplt.legend(loc = 2)\nplt.xticks(rotation = 90)","c03ae30c":"# Boxplot\n\nplt.figure(figsize = (12, 8))\nsns.boxplot(x = 'feature', y = 'value', hue = 'diagnosis', data = mean_melt)\nplt.xticks(rotation = 90)","fa3a0b7a":"data.head()","151a6edd":"# Train test split \n\nX_train, X_test, Y_train, Y_test = train_test_split(original_features, target, test_size = 0.3, random_state = 10)\nprint(\"X_train shape: \", X_train.shape)\nprint(\"Y_train shape: \", Y_train.shape)\nprint(\"X_test shape: \", X_test.shape)\nprint(\"Y_test shape: \", Y_test.shape)","142d50c0":"# Fit random forest classifier to training set and make predictions on test set\n\nrf = RandomForestClassifier(random_state = 42)\nrf.fit(X_train, Y_train)\nY_pred = rf.predict(X_test)","6135312c":"# Evaluate model accuracy \n\naccuracy = accuracy_score(Y_pred, Y_test) * 100\nprint(\"Accuracy: {:.2f}%\".format(accuracy))\nf1 = f1_score(Y_pred, Y_test)\nprint(\"F1 score: {:.2f}\".format(f1))\ncm = confusion_matrix(Y_pred, Y_test)\nsns.heatmap(cm, annot = True, fmt = 'd')","ae8a2ac9":"# Define a function which computes VIF\n\ndef calculate_vif(df):\n    vif = pd.DataFrame()\n    vif['Feature'] = df.columns\n    vif['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n    return (vif)","7e99924b":"# Construct VIF dataframe\n\nvif_table = calculate_vif(original_features)\nvif_table = vif_table.sort_values(by = 'VIF', ascending = False, ignore_index = True)\nvif_table","3538c81b":"# Top 5 features with highest VIF\n\nfeatures_to_drop = list(vif_table['Feature'])[:5]\nfeatures_to_drop","923b629e":"# Drop top 5 features with highest VIF\n\nnew_features = original_features.drop(features_to_drop, axis = 1)","6dbb1014":"# Train test split\nX_train, X_test, Y_train, Y_test = train_test_split(new_features, target, test_size = 0.3, random_state = 10)\n\n# Fit model to data and make predictions\nrf = RandomForestClassifier(random_state = 42)\nrf.fit(X_train, Y_train)\nY_pred = rf.predict(X_test)\n\n# Evaluate model accuracy \naccuracy = accuracy_score(Y_pred, Y_test) * 100\nprint(\"Accuracy: {:.2f}%\".format(accuracy))\nf1 = f1_score(Y_pred, Y_test)\nprint(\"F1 score: {:.2f}\".format(f1))\ncm = confusion_matrix(Y_pred, Y_test)\nsns.heatmap(cm, annot = True, fmt = 'd')","68583741":"# Train test split\n\nX_train, X_test, Y_train, Y_test = train_test_split(original_features, target, test_size = 0.3, random_state = 10)","dfa742a6":"# Instantiate select features\nselect_features = SelectKBest(chi2, k = 5).fit(X_train, Y_train)\n\n# Top 5 features\nselected_features = select_features.get_support()\nprint(\"Top 5 features: \", list(X_train.columns[selected_features]))","412e7b3b":"# Apply select features to training and test set\nX_train = select_features.transform(X_train)\nX_test = select_features.transform(X_test)\n\n# Fit model to data and make predictions\nrf = RandomForestClassifier(random_state = 42)\nrf.fit(X_train, Y_train)\nY_pred = rf.predict(X_test)\n\n# Evaluate model accuracy \naccuracy = accuracy_score(Y_pred, Y_test) * 100\nprint(\"Accuracy: {:.2f}%\".format(accuracy))\nf1 = f1_score(Y_pred, Y_test)\nprint(\"F1 score: {:.2f}\".format(f1))\ncm = confusion_matrix(Y_pred, Y_test)\nsns.heatmap(cm, annot = True, fmt = 'd')","137e09b9":"# Train test split\n\nX_train, X_test, Y_train, Y_test = train_test_split(original_features, target, test_size = 0.3, random_state = 10)","b23d0f8e":"# Instantiate recursive feature elimination to select the top 5 features\nrf = RandomForestClassifier(random_state = 42)\nrfe = RFE(estimator = rf, n_features_to_select = 5).fit(X_train, Y_train)\n\n# Top 5 features\nprint(\"Top 5 features: \", list(X_train.columns[rfe.support_]))","4fe10fe5":"# Make predictions on test set\nY_pred = rfe.predict(X_test)\n\n# Evaluate model accuracy \naccuracy = accuracy_score(Y_pred, Y_test) * 100\nprint(\"Accuracy: {:.2f}%\".format(accuracy))\nf1 = f1_score(Y_pred, Y_test)\nprint(\"F1 score: {:.2f}\".format(f1))\ncm = confusion_matrix(Y_pred, Y_test)\nsns.heatmap(cm, annot = True, fmt = 'd')","d344ee6f":"# Train test split\n\nX_train, X_test, Y_train, Y_test = train_test_split(original_features, target, test_size = 0.3, random_state = 10)","101b5989":"# Fit model to data\n\nrf = RandomForestClassifier(random_state = 42)\nrf.fit(X_train, Y_train)\nimportances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_], axis = 0)\nindices = np.argsort(importances)[::-1]","dfa11598":"# Evaluate feature importances\n\nprint(\"Feature ranking: \")\nfor f in range(X_train.shape[1]):\n    print(\"%d. feature %d (%f)\" %(f + 1, indices[f], importances[indices[f]]))","dbfd1500":"# Plot feature importances\n\nplt.figure(figsize = (15, 8))\nplt.title(\"Feature importances\")\nplt.bar(range(X_train.shape[1]), importances[indices],\n        color = \"r\", yerr = std[indices], align=\"center\")\nplt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation = 90)\nplt.xlim([-1, X_train.shape[1]])\nplt.show()","ba409bdc":"# Select features with importances above 5%\n\nnew_features = original_features.iloc[:, list(indices)[:9]]\nprint(\"Number of features above 5%: \", len(new_features.columns))\nlist(new_features.columns) ","70cec36d":"# Train test split\nX_train, X_test, Y_train, Y_test = train_test_split(new_features, target, test_size = 0.3, random_state = 10)\n\n# Fit model to data and make predictions\nrf = RandomForestClassifier(random_state = 42)\nrf.fit(X_train, Y_train)\nY_pred = rf.predict(X_test)\n\n# Evaluate model accuracy \naccuracy = accuracy_score(Y_pred, Y_test) * 100\nprint(\"Accuracy: {:.2f}%\".format(accuracy))\nf1 = f1_score(Y_pred, Y_test)\nprint(\"F1 score: {:.2f}\".format(f1))\ncm = confusion_matrix(Y_pred, Y_test)\nsns.heatmap(cm, annot = True, fmt = 'd')","4bfab034":"# Train test split using standardised features\n\nX_train, X_test, Y_train, Y_test = train_test_split(standard_features, target, test_size = 0.3, random_state = 10)","ab9b26ac":"# Instantiate and fit PCA to training set\n\npca = PCA()\npca.fit(X_train)","eef55837":"# Visualise explained variance ratio to the number of components\n\nplt.figure(figsize = (10, 6))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_ratio_, linewidth = 2)\nplt.axis('tight')\nplt.xlabel('Number of components')\nplt.ylabel('Explained variance ratio')","fb06e9d2":"# Instantiate PCA with 4 components and transform both training set and test set\n\npca = PCA(n_components = 4)\npca.fit(X_train)\nX_train = pca.transform(X_train)\nX_test = pca.transform(X_test)","adb74e30":"# Fit model to data and make predictions\nrf = RandomForestClassifier(random_state = 42)\nrf.fit(X_train, Y_train)\nY_pred = rf.predict(X_test)\n\n# Evaluate model accuracy \naccuracy = accuracy_score(Y_pred, Y_test) * 100\nprint(\"Accuracy: {:.2f}%\".format(accuracy))\nf1 = f1_score(Y_pred, Y_test)\nprint(\"F1 score: {:.2f}\".format(f1))\ncm = confusion_matrix(Y_pred, Y_test)\nsns.heatmap(cm, annot = True, fmt = 'd')","83ee65a2":"## 5.2.1 Issue of multicollinearity","37de7f1a":"There are more cancer cells that are benign than there are that are malignant. ","cac9b8e3":"# 2. Import and read data","8db44e55":"Unsurprisingly, the top 5 features with the highest VIF are the suspects that we have already identified earlier on. ","9beef643":"Now, we can move on to exploring the features in the dataset. ","9363ad9b":"# 6.6 Principal component analysis (PCA)","0e8a49e8":"# 5.1 Target variable","883b12af":"This part is slightly subjective but I decided to only retain features that have importances over 5% as the training set.","ccd728fa":"# 4. Data description","c86d80cb":"Here, I will assign 70% of the dataset as the training set and the remaining 30% as the test set to test the accuracy of our model.","3f261327":"# 6.4 Recursive feature elimination","60620cd8":"As expected, we have a severe problem of multicollinearity in our data. From the heatmap, we can observe that the follwing features are positively correlated with each other:\n\n- Radius\n- Perimeter\n- Area\n- Compactness\n- Concavity\n- Concave points","4ad3649a":"# 0. Introduction\n\nIn this notebook, we will explore various feature selection and dimensionality reduction techniques in reference to the [Wisconsin breast cancer dataset](https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data) on Kaggle. This dataset contains 569 breast cancer observations in which 357 of them are benign and 212 of them are malignant. The goal is to train a machine learning model that is able to classify a random breast cancer observation as either benign or malignant. I have chosen the random forest classifier for this particular problem but feel free try out other classification models of your choice.\n\nThe techniques that will be covered in this notebook as follows:\n\n- Variance inflation factors (VIF)\n- Univariate feature selection\n- Recursive feature elimination\n- Model-based feature selection\n- Principal component analysis (PCA)\n\nWe will compare the effectiveness of each technique by examining the accuracy of our model at making predictions. More specifically, we will be using the confusion matrix, which is a common approach to test the performance of a model in binary classification.\n\nI will include some additional resources at the end of this notebook to help you learn more about the concepts that are discussed in this notebook as well as links to my platforms and the other projects that I am currently working on. I hope you will gain some value out of this notebook.\n\nCheers\\\nJason","aea00e0c":"# 5. Exploratory data analysis (EDA)","ce59af34":"With only 9 features, our model accuracy came very close to that under the base case scenario i.e. 98.25%.","a100a000":"Besides fractal dimension, all the features look promising at classifying cancer cells.\n\nWe can see that cancer cells that are malignant tend to have higher values in all of the features.","cc24ac4d":"With just 4 principal components, our model is able to achieve a similar accuracy score to that under the model-based feature selection technique where 9 different features were used to train the model.\n\nHowever, despite the impressive dimensionality reduction abilities of the PCA, one of its disadvantages is that our predictor variables become less interpretable. In other words, PCA makes it more difficult for us to determine the features that are important in classifying cancer cells. This is largely due to the underlying algorithm of the PCA which turns the original features in the dataset into principal components which are linear combinations of different features. \n\nNevertheless, PCA remains a very robust technique in summarising high number of features into key components and thus allowing our model to capture all the important information in the dataset in order to make accurate predictions.","35719a17":"# 3. Check for missing values","3c341f23":"## 5.2.2 Explore the relationship between predictor variables and target variable\n\nIn this section, we will visualise the relationship between our predictor variables and the target variable. The goal here is to investigate and determine the features that are most important at distinguishing whether a cancer cell is benign or malignant.","d89f753f":"# 6. Feature selection\n\nNow that we have a better sense of our data, we can move on to selecting features for our training set to build our model.\n\nBefore we do that, let's set a base case for our feature selection i.e. use all the features in the dataset to train our model.","e8566797":"# 8. Additional resources\n\n- [Random forest](https:\/\/www.youtube.com\/watch?v=J4Wdy0Wc_xQ)\n- [Confusion matrix](https:\/\/www.youtube.com\/watch?v=8Oog7TXHvFY&t=1681s)\n- [Principal component analysis](https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ)\n- [Scikit-learn feature selection documentation](https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html)","3e2192a2":"The top 5 features chosen under recursive feature elimination are slightly different to those selected under univariate feature selection.\n\nLet's now test the model accuracy.","4fcdaf2a":"# 5.2 Predictor variables","265a95d1":"From the visualisation above, we can conclude that the optimal number of components is 4 using the elbow method.","5db08d51":"Again, we achieved an accuracy of 95%, similar to that under univariate feature selection. ","3a69a943":"To summarise, feature selection and dimensionality reduction allow us to minimise the number of features in our dataset by only keeping features that are important. In other words, we want to retain features that contain the most useful information that is needed by our model to learn to make accurate predictions while discarding features that contain little to no information.\n\nIn this notebook, we have considered the following techniques:\n\n- Variance inflation factors (98.83% accuracy with 25 features)\n- Univariate feature selection (95.32% accuracy with 5 features)\n- Recursive feature elimination (95.91% accuracy with 5 features)\n- Model-based feature selection (97.08% accuracy with 9 features)\n- Principal component analysis (97.08% with 4 principal components)\n\nAs we saw, despite using a significantly less number of features, we still managed to come very close the accuracy score under the base case scenario (98.25% accuracy) where all the features in the dataset were used to train our model. ","ca711c5d":"- [Facebook](https:\/\/www.facebook.com\/chongjason914)\n- [Instagram](https:\/\/www.instagram.com\/chongjason914)\n- [Twitter](https:\/\/www.twitter.com\/chongjason914)\n- [LinkedIn](https:\/\/www.linkedin.com\/in\/chongjason914)\n- [YouTube](https:\/\/www.youtube.com\/channel\/UCQXiCnjatxiAKgWjoUlM-Xg?view_as=subscriber)\n- [Medium](https:\/\/www.medium.com\/@chongjason)","4eda2068":"We have 30 numerical variables and only 1 categorical variable, which is our target variable (diagnosis). Let's have a look at these features in detail.","8593adc6":"# 6.2 Variance inflation factors (VIF)","d439d708":"# 7. Conclusion","83b97e1c":"Unnamed is the only column with missing values in the dataframe. In fact, the entire column is missing so it is safe for us to drop the column. \n\nI will also remove the id column as it does not provide us with any information regarding the classification of cancer cells. ","bc6f73db":"Our model achieved an accuracy of 98.25%. Not too shabby at all.\n\nLet's now explore the different feature selection and dimensionality reduction techniques and see if we can achieve a similar level of accuracy but with a smaller training set i.e. less features.","1d40c6f4":"# 9. Follow me on other platforms","f1bed8d1":"# 6.1 Base case","e0d10f1c":"According to the [data description](https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data) of this dataset, the columns represent 10 real-valued features of each cell nucleus:\n\n1. Radius\n2. Texture\n3. Perimeter\n4. Area\n5. Smoothness \n6. Compactness\n7. Concavity\n8. Concave points\n9. Symmetry\n10. Fractal dimension\n\nThe mean, standard error and worst of each feature were also computed, resulting in a total of 10 x 3 = 30 features (columns) in the dataset excluding the target variable.\n\nJust by the names of the features itself, we can already foresee some issue of multicollinearity. The most obvious being between radius, perimeter and area. But before we deal with the issue of multicollinearity, let's first standardise our data and divide the features into 3 groups:\n\n- feature_mean\n- feature_se\n- feature_worst","a08f58b1":"Wow, this is remarkable!\n\nDespite only using 5 features (1\/6 of the original trainining set), our model accuracy has only gone down by 3%. This goes to show that these 5 features contain most of the information that is needed by our model to classify cancer cells accurately.","d4c5fcd8":"# 6.5 Model-based feature selection","b8980bc4":"# 6.3 Univariate feature selection","334fac93":"After removing the top 5 features with the highest VIF, our model accuracy not only did not decrease but instead it increased marginally. ","ceedd67a":"Again, this plot illustrates that fractal dimension is not as good at classifying cancer cells as the other features in the dataset.\n\nBoxplot also allows us to analyse the outliers in our dataset but let's ignore this problem for now.","ccf3a342":"# 1. Import libraries"}}