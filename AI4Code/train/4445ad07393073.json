{"cell_type":{"a4b89ec2":"code","d190c2d9":"code","b9c4f40a":"code","659a4355":"code","6d306426":"code","b31ce677":"code","2ed1a3f6":"code","2e08184f":"code","5843fd24":"code","500334d7":"code","cae27453":"code","8108bcfe":"code","aab1027d":"code","b0eff6a9":"code","1d708af1":"code","e50e19b9":"code","4c4e45d5":"code","ff9ee246":"code","3193af13":"code","84e37756":"code","eaae54b2":"code","8abdf239":"code","4ad3857e":"code","45876511":"code","f9b71417":"code","db807b4c":"code","82b99437":"code","9f7764fb":"code","7c05eb7a":"code","e31047b1":"code","4a8e5c95":"code","a4355633":"code","f0e31d89":"code","5986d02b":"code","65ab282c":"code","03fce596":"code","f77190aa":"code","056a60ea":"code","3f304603":"code","d2454e0a":"code","3b66c184":"code","e69d06a8":"code","b6a17381":"code","858a2671":"code","729d4284":"code","d707a55f":"code","434af59b":"markdown","f431ad01":"markdown","e90844a9":"markdown","428f34f1":"markdown","713d58d0":"markdown","7da4b077":"markdown","256a3218":"markdown","886a0e97":"markdown","cadf58a0":"markdown","12dff1e6":"markdown","daba6348":"markdown","3cb9249b":"markdown","16164259":"markdown","7b925a21":"markdown","03a31208":"markdown","0970c9b4":"markdown","ac05b9d4":"markdown","6b958fc2":"markdown","e0e82963":"markdown","a3ea60c5":"markdown","5ef0f195":"markdown","eddc868a":"markdown","16d2ee7d":"markdown","a049add6":"markdown","d2afa4e7":"markdown","28a1426c":"markdown","76bc8dba":"markdown","d2f04b23":"markdown","d1ec401a":"markdown","83e97fde":"markdown","bf145fbb":"markdown","e6e7ac89":"markdown","c3e3477e":"markdown","a47d6f3f":"markdown","06ad6144":"markdown","89db1bf5":"markdown","60ed2eb6":"markdown","c3ad0e13":"markdown","44cb1787":"markdown","d0b1642b":"markdown","ecb896d5":"markdown","1d9d834b":"markdown","ccf7ed8c":"markdown","ea0a3225":"markdown","945dbe1d":"markdown","10c1e587":"markdown","b91aaf55":"markdown","7ed06089":"markdown","74aad2e9":"markdown","84b16456":"markdown","fdf2697e":"markdown","30ebbf02":"markdown","efee555a":"markdown","42681b1c":"markdown","5cb2c2e1":"markdown","f0595f2c":"markdown","0cc66bea":"markdown","6f3c8e72":"markdown"},"source":{"a4b89ec2":"import sys\nprint('Python: {}'.format(sys.version))\n# numpy\nimport numpy\nprint('numpy: {}'.format(numpy.__version__))\n# pandas\nimport pandas\nprint('pandas: {}'.format(pandas.__version__))\n# matplotlib\nimport matplotlib\nprint('matplotlib: {}'.format(matplotlib.__version__))\n# seaborn\nimport seaborn\nprint('seaborn: {}'.format(seaborn.__version__))\n# scikit-learn\nimport sklearn\nprint('sklearn: {}'.format(sklearn.__version__))","d190c2d9":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nimport warnings\nwarnings.simplefilter(action='ignore')","b9c4f40a":"sns.set(style='darkgrid')","659a4355":"train_path = '..\/input\/train.csv'\ntest_path = '..\/input\/test.csv'\n\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)","6d306426":"train_df.shape","b31ce677":"train_df.info()","2ed1a3f6":"train_df.describe()","2e08184f":"train_df.head()","5843fd24":"missing_data = train_df.isnull().sum()\npercent_missing = round((missing_data \/ train_df.isnull().count())*100, 2)\nmissing_df = pd.concat(\n    [missing_data.sort_values(ascending=False), percent_missing.sort_values(ascending=False)], \n    axis=1, keys=['Total', 'Percent']\n)\nmissing_df.head(5)","500334d7":"train_df.columns","cae27453":"temp_df = train_df.copy()\ntemp_df['Cabin'] = temp_df['Cabin'].fillna('Unknown')\n\nocc_cabins = temp_df['Cabin'].copy()\nocc_cabins[occ_cabins != 'Unknown'] = 'Known'\ntemp_df['Cabin'] = occ_cabins\n\nplt.figure(figsize=(5.5, 5))\nsns.barplot(x='Cabin', y='Survived', data=temp_df)\nsns.pointplot(x='Cabin', y='Survived', data=temp_df, color='k')","8108bcfe":"plt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.barplot(x='Sex', y='Survived', data=train_df)\nplt.subplot(1, 2, 2)\nsns.violinplot(x='Survived', y='Age', data=train_df, hue='Sex', split=True)","aab1027d":"females = train_df[train_df['Sex'] == 'female']\nmales = train_df[train_df['Sex'] == 'male']\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nax = sns.distplot(males[males['Survived'] == 1]['Age'].dropna(), bins=10, kde=False, label='survived')\nax = sns.distplot(males[males['Survived'] == 0]['Age'].dropna(), bins=10, kde=False, label='not survived')\nax.legend()\nax.set_title('Male')\n\nplt.subplot(1, 2, 2)\nax = sns.distplot(females[females['Survived'] == 1]['Age'].dropna(), kde=False, label='survived')\nax = sns.distplot(females[females['Survived'] == 0]['Age'].dropna(), kde=False, label='not survived')\nax.legend()\nax.set_title('Female')\n\nplt.show()","b0eff6a9":"plt.figure(figsize=(5.5, 5))\nsns.boxplot(x='Survived', y='Age', data=train_df)","1d708af1":"plt.figure(figsize=(5.5, 5))\nsns.boxplot(x='Survived', y='Fare', data=train_df)","e50e19b9":"plt.figure(figsize=(5.5, 5))\nsns.pointplot(x='Pclass', y='Survived', data=train_df)","4c4e45d5":"plt.figure(figsize=(5.5, 5))\nsns.barplot(x=(train_df['SibSp'] + train_df['Parch']), y=train_df['Survived'])","ff9ee246":"plt.figure(figsize=(5.5, 5))\nsns.pointplot(x='Embarked', y='Survived', data=train_df)","3193af13":"pass_id = test_df['PassengerId']\n\nX = train_df.drop(['PassengerId'], axis=1)\nX_test = test_df.drop(['PassengerId'], axis=1)\n\ny = X['Survived']\nX = X.drop('Survived', axis=1)\n\nfeatures_train, features_valid, labels_train, labels_valid = train_test_split(\n    X, y, test_size=0.2, random_state=7\n)\n\npre_features_train = features_train.copy()\n\ncols = features_train.columns\ncols","84e37756":"missing_df","eaae54b2":"def missing_cabin(data):\n    data['Cabin'].fillna('X01', inplace=True)\n\n    \nfor data in [features_train, features_valid]:\n    missing_cabin(data)","8abdf239":"def missing_age(data):\n    nan_ages = []\n    \n    mu = pre_features_train['Age'].mean()\n    median = pre_features_train['Age'].median()\n    sigma = pre_features_train['Age'].std()\n\n    random_ages = np.random.randint(\n            median-sigma, \n            median+sigma,\n            data['Age'].isnull().sum()\n        )\n\n    nan_ages = data['Age'].copy() \n    nan_ages[nan_ages.isnull()] = random_ages\n    data.loc[:, 'Age'] = nan_ages\n    \n\nfor data in [features_train, features_valid]:\n    missing_age(data)","4ad3857e":"impute_embark = SimpleImputer(strategy='most_frequent')\n\nfeatures_train['Embarked'] = impute_embark.fit_transform(features_train['Embarked'].values.reshape(-1, 1))\nfeatures_valid['Embarked'] = impute_embark.transform(features_valid['Embarked'].values.reshape(-1, 1))","45876511":"features_train.info()","f9b71417":"features_train.head()","db807b4c":"cabin_data = np.array(features_train['Cabin'])\ncabin_data_valid = np.array(features_valid['Cabin'])\n\ncabin_data = pd.DataFrame([x[0] for x in cabin_data], index=features_train.index, columns=['Cabin'])\ncabin_data_valid = pd.DataFrame([x[0] for x in cabin_data_valid], index=features_valid.index, columns=['Cabin'])\n\nfeatures_train.drop(['Cabin'], axis=1, inplace=True)\nfeatures_valid.drop(['Cabin'], axis=1, inplace=True)\n\nfeatures_train = pd.concat([features_train, cabin_data], axis=1)\nfeatures_valid = pd.concat([features_valid, cabin_data_valid], axis=1)\n\nle_cabin = LabelEncoder()\nfeatures_train['Cabin'] = le_cabin.fit_transform(features_train['Cabin'])\nfeatures_valid['Cabin'] = le_cabin.transform(features_valid['Cabin'])","82b99437":"le_gender = LabelEncoder()\nfeatures_train['Sex'] = le_gender.fit_transform(features_train['Sex'])\nfeatures_valid['Sex'] = le_gender.transform(features_valid['Sex'])","9f7764fb":"oe_pclass = OrdinalEncoder(dtype='int64')\nfeatures_train['Pclass'] = oe_pclass.fit_transform(features_train['Pclass'].values.reshape(-1, 1))\nfeatures_valid['Pclass'] = oe_pclass.transform(features_valid['Pclass'].values.reshape(-1, 1))","7c05eb7a":"le_embark = LabelEncoder()\ninteger_encoded = le_embark.fit_transform(features_train['Embarked'])\ninteger_encoded_valid = le_embark.transform(features_valid['Embarked'])\n\noh_embark = OneHotEncoder(handle_unknown='ignore', sparse='False', dtype='int64')\nonehot_encoded = pd.DataFrame(\n    oh_embark.fit_transform(integer_encoded.reshape(-1, 1)).toarray(), \n    columns=['Embarked_C', 'Embarked_Q', 'Embarked_S'], \n    index=features_train.index\n)\nonehot_encoded_valid = pd.DataFrame(\n    oh_embark.transform(integer_encoded_valid.reshape(-1, 1)).toarray(), \n    columns=['Embarked_C', 'Embarked_Q', 'Embarked_S'], \n    index=features_valid.index\n)\n\nfeatures_train = features_train.drop(['Embarked'], axis=1)\nfeatures_valid = features_valid.drop(['Embarked'], axis=1)\n\nfeatures_train = pd.concat([features_train, onehot_encoded], axis=1)\nfeatures_valid = pd.concat([features_valid, onehot_encoded_valid], axis=1)","e31047b1":"features_train.head()","4a8e5c95":"def scale_feature(data, feature):\n    mu = pre_features_train[feature].mean()\n    sigma = pre_features_train[feature].std()\n    \n    data.loc[:, feature] = round((data[feature] - mu) \/ sigma, 3)","a4355633":"for data in [features_train, features_valid]:\n    scale_feature(data, 'Age')","f0e31d89":"for data in [features_train, features_valid]:\n    scale_feature(data, 'Fare')","5986d02b":"def add_rel(data):\n    data.loc[:, 'relatives'] = data['SibSp'] + data['Parch']\n\nfor data in [features_train, features_valid]:\n    add_rel(data)","65ab282c":"def drop_features(data):\n    drop_cols = ['Name', 'Ticket']\n    data.drop(drop_cols, axis=1, inplace=True)\n    \nfor data in [features_train, features_valid]:\n    drop_features(data)","03fce596":"best_features = SelectKBest(k='all')\nfit = best_features.fit(features_train, labels_train)\n\nscores_df = pd.DataFrame(data=fit.scores_)\ncolumns_df = pd.DataFrame(data=features_train.columns.values)\n\nfeature_scores_df = pd.concat([columns_df,scores_df],axis=1)\nfeature_scores_df.columns = ['Features','Score']\n\nplt.figure(figsize=(5.5, 5))\nsns.barplot(\n    x='Score', y='Features', \n    order=feature_scores_df.nlargest(11,'Score')['Features'], \n    data=feature_scores_df, palette=sns.cubehelix_palette(n_colors=4, reverse=True)\n)","f77190aa":"def keep_best_features(data):\n    drop_cols = ['relatives', 'SibSp', 'Embarked_Q']\n    data.drop(drop_cols, axis=1, inplace=True)\n    \nfor data in [features_train, features_valid]:\n    keep_best_features(data)","056a60ea":"features_train.head()","3f304603":"clf = [\n    ('GNB', GaussianNB()),\n    ('SVC', SVC(C=1000, kernel='rbf', gamma=0.3)),\n    ('LReg', LogisticRegression(C=0.5, solver='lbfgs')),\n    ('KNN', KNeighborsClassifier()),\n    ('CART', DecisionTreeClassifier(min_samples_split=100)),\n    ('BAG', BaggingClassifier()),\n    ('RF', RandomForestClassifier(n_estimators=100)),\n    ('AB', AdaBoostClassifier(n_estimators=100)),\n    ('XGB', XGBClassifier(max_depth=10, learning_rate=0.03, n_estimators=100))\n]\n\nclf_names = [\n    'GaussianNB', 'SVC', 'Logistic Reg', 'KNN', 'Decision Tree', 'Bagging', 'Random Forest', 'AdaBoost', 'XGboost'\n]","d2454e0a":"result = []\nfor model in clf:\n    score = cross_val_score(model[1], features_train, labels_train, cv=10)\n    result.append(score)\n    \nresult_df = pd.DataFrame({\n    'Score' : [round(x.mean(), 3) for x in result],\n    'Model' : clf_names\n})\n\nresult_df = result_df.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\n\nresult_df","3b66c184":"plt.figure(figsize = (7, 4))\nax = sns.boxplot(data=pd.DataFrame(np.array(result).transpose(), columns=clf_names))\nax.set_xticklabels(ax.get_xticklabels(),rotation=30)\nplt.show()","e69d06a8":"clf = XGBClassifier(\n    max_depth=5, learning_rate=0.2,\n    verbosity=1, silent=None, n_estimators=100,  \n    objective='binary:logistic',booster='gbtree'\n)\n\nclf.fit(features_train, labels_train)\npredict = clf.predict(features_valid)\n\nprint(round((accuracy_score(labels_valid, predict)*100), 2))","b6a17381":"print(accuracy_score(labels_valid, predict))\nprint(confusion_matrix(labels_valid, predict))\nprint(classification_report(labels_valid, predict))","858a2671":"plt.figure(figsize=(4, 3))\nax = sns.heatmap(confusion_matrix(labels_valid, predict), annot=True)\nax.set_xlabel('Predicted Values')\nax.set_ylabel('True Values')\n\nplt.show()","729d4284":"missing_cabin(X_test)\nmissing_age(X_test)\nX_test['Embarked'] = impute_embark.transform(X_test['Embarked'].values.reshape(-1, 1))\n\nX_test['Sex'] = le_gender.transform(X_test['Sex'])\nX_test['Pclass'] = oe_pclass.transform(X_test['Pclass'].values.reshape(-1, 1))\n\ncabin_data = np.array(X_test['Cabin'])\ncabin_data = pd.DataFrame([x[0] for x in cabin_data], index=X_test.index, columns=['Cabin'])\nX_test.drop(['Cabin'], axis=1, inplace=True)\nX_test = pd.concat([X_test, cabin_data], axis=1)\nX_test['Cabin'] = le_cabin.transform(X_test['Cabin'])\n\ninteger_encoded_test = le_embark.transform(X_test['Embarked'])\nonehot_encoded_test = pd.DataFrame(\n    oh_embark.transform(integer_encoded_test.reshape(-1, 1)).toarray(), \n    columns=['Embarked_C', 'Embarked_Q', 'Embarked_S'], \n    index=X_test.index\n)\n\nX_test.drop(['Embarked'], axis=1, inplace=True)\nX_test = pd.concat([X_test, onehot_encoded_test], axis=1)\n    \nscale_feature(X_test, 'Age')\nscale_feature(X_test, 'Fare')\nadd_rel(X_test)\n\ndrop_features(X_test)\nkeep_best_features(X_test)\n\npredict = clf.predict(X_test)","d707a55f":"output = pd.DataFrame(\n    {\n        'PassengerId': pass_id,\n        'Survived': predict\n    }\n)\n\noutput.to_csv('gender_submission.csv', index=False)","434af59b":"### 5.3.2 Cabin","f431ad01":"### 5.2.1 Cabin\nWell, to be honest, since 77% of data for this feature was missing hence I tried dropping it, but my model performed extremely bad. It turns out that this feature holds a significance in predicting survival.<br>\nSo, I imputed the missing values with 'X01', and encoded it as below, just using the deck code, and not the room number (might be!).","e90844a9":"We can generalize the above and say that the <b>people who survived<\/b>, were around the age of <b>18-38<\/b>","428f34f1":"## 5.3 Categorical data\nAfter imputing missing values, its time to <b>encode categorical data<\/b> to natural numbers using labelencoder, ordinalencoder and onehot encoder.<br>\nThe sequence in which we'll encode our features are as - \n1. Cabin\n2. Gender\n3. Pclass\n4. Embarked","713d58d0":"# 5. Data Preprocessing\n\nNow, lets convert our raw data into some useful information, that can be pluged into out classifiers.<br>\nThe steps we'll follow are - \n1. Train - Test split\n2. Handle missing data.\n3. Handle categorical data.","7da4b077":"The <b>greater fraction of people survived<\/b> who had <b>1, 2, 4 and somewhat 6 relatives<\/b> onboard.","256a3218":"We can see that our training set has <b>891 training examples<\/b> and <b>11 features + 1 label column<\/b>.","886a0e97":"## 3.4 Peep at data\n\nThe <b>head<\/b> function of pandas dataframe allows us to <b>view the dataframe<\/b>, 5 rows by default","cadf58a0":"<b>81.56% accuracy<\/b> (you might see some other value, again machine learning, randomness) doesn't seem best, but its kind of good for our model.","12dff1e6":"## 4.3 Age\n\nLets see how the <b>age<\/b> affected the survival in both male and female.","daba6348":"Seeing the first graph we can infer that greater fraction of females survived the tragedy.","3cb9249b":"### 5.3.3 Pclass","16164259":"We can see that we have total <b>11+1 data columns<\/b>.<br>An interesting thing that we can see here is, columns -<b> Age,Cabin and Embarked<\/b> - have missing data.<br>In our dataset, we have <b>2 features of float type, 5 features of int type and 5 features that are object (string) type.<\/b><br><br>\nDefinition of each feature is as - \n<table>\n<tr><th><b>Variable<\/b><\/th><th><b>Definition<\/b><\/th><th><b>Key<\/b><\/th><\/tr>\n<tr>\n<td>survival<\/td>\n<td>Survival<\/td>\n<td>0 = No, 1 = Yes<\/td>\n<\/tr>\n<tr>\n<td>pclass<\/td>\n<td>Ticket class<\/td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd<\/td>\n<\/tr>\n<tr>\n<td>sex<\/td>\n<td>Sex<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>Age<\/td>\n<td>Age in years<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>sibsp<\/td>\n<td># of siblings \/ spouses aboard the Titanic<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>parch<\/td>\n<td># of parents \/ children aboard the Titanic<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>ticket<\/td>\n<td>Ticket number<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>fare<\/td>\n<td>Passenger fare<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>cabin<\/td>\n<td>Cabin number<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>embarked<\/td>\n<td>Port of Embarkation<\/td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton<\/td>\n<\/tr>\n<\/table>\n\n(Source: The HTML  for table has been copied from kaggle @ https:\/\/www.kaggle.com\/c\/titanic\/data)","7b925a21":"## 6.3 Feature Selection\n\nBefore feature selection, lets drop certain features - name and ticket - which give us relatively less information about survival of that particular persons.<br>\nThough one can argue that a VIP had a more chance for survival, but for now lets drop both the columns.","03a31208":"## 4.2 Sex\nNow, lets see how <b>gender<\/b> effected survival.","0970c9b4":"# 2. Load dataset","ac05b9d4":"The <b>fare of the people who survived<\/b>, ranged from somewhere about <\/b>\\$10k to \\$80k<\/b> (for about most of them). The plot also indicates that fare payed by the surviving people took a minimum value of \\$0k and a maximum value of \\$500k.","6b958fc2":"## 4.4 Fare","e0e82963":"## 4.5 PClass","a3ea60c5":"## 3.3 Statistics of dataset\n\nThe <b>describe<\/b> function of pandas gives us the <b>statistical description<\/b> of <b>each column present in our dataframe.<\/b> A thing to note is that the <\/b>columns containing only real data<\/b> are described using this function, <b>categorical features<\/b> are <b>not<\/b> present in the result.","5ef0f195":"We can see that the fraction of people who survived most of those peoples' cabin is known.<br>Well, this is not the best intuition but, for now I can think of this only and seeing at the above plot it seems usless to drop this column. If I come up with a better idea I'll update it, but for now lets move with it.","eddc868a":"# 11. Performance of model","16d2ee7d":"It seems like XGboost has a good accuracy range (If you see something different, welcome to the randomness of machine learning, each time we compile we'll get different result, but most of the time XGboost had better results).<br>\nLets move on using <b>XGBoost Classifier<\/b>.","a049add6":"## 3.2 Info of dataset\n\nThe <b>info<\/b> function of dataframe gives us <b>information of the full dataset column wise<\/b> - the data types and the missing values in each column etc.","d2afa4e7":"# 7. Evaluate some algorithms\n\nAfter such long cleanign and processing of our data lets build some classifiers, namely - \n1. Naive Bayes\n2. SVC\n3. Logistic Regression\n4. KNN\n5. Decision Tree\n6. Bagging\n7. Random Forest\n8. Adaboost\n9. XGboost","28a1426c":"### 6.2.1 # relatives\n\nWe will add a new feaure 'relatives which will gives us the total number of relatives for each person' data.<br><br>\nWe'll define it as - <br>\nrelatives = Sibsp + Parch","76bc8dba":"### 5.3.1 Gender","d2f04b23":"## 5.1 Train-Test split\n\nLets <b>split<\/b> our data into testing and validation set, with validation set containing <b>20% of the data<\/b>.","d1ec401a":"# 4. Data Visualization \/ EDA\nNow, lets visualise and analyse our data, using boxplots, pointplots, violinplots, barplots etc.<br>\nSo lets see how our features affected survival in following sequence - \n1. Cabin\n2. Age \n3. Sex\n4. Embarked\n5. Pclass\n6. \\# Relatives\n7. Fare","83e97fde":"# 6. Feature Engineering\n\nAfter we are done with processing our data, now lets work on our features to improve the quality of our model.\n<br>Feature Engineering helps <b>bulid a simpler model and pervents overfitting<\/b> as well.\n<br>We'll feature engineer in following steps - \n1. Feature Scaling\n2. Feature Addition\n3. Feature Selection\n<br>Since, we don't have many features we'll skip PCA for this dataset.","bf145fbb":"### 5.2.2 Age","e6e7ac89":"### 5.2.3 Embarked","c3e3477e":"# 3. Summarize data\n\nLet's take a quick look at the data we have by - \n1. Shape of dataframe\n2. Info of dataset\n3. Statistics of data\n4. Peep at data itself\n5. Missing data","a47d6f3f":"## 6.2 Feature Addition\n\nTill now we have been working with the features that we have, but now lets create some new features which might help out classifier give better result.<br>\n\nWe specifically will add - \\# Relatives","06ad6144":"# 8. Validation\n\nAfter building our classifiers lets use <b>10 fold cross validation<\/b> to see how well each of them perform in our training data, and select the best out of them.<br><br>\nWhat 10-fold cross validator does is divide our dataset into 10 folds\/groups, keeps one fold to test and trains each classifier on the remainig data. It then selects the other fold for testing and trains on the remaining and goes on until it has used up all folds for testing and the remaining for training.<br>In this way we <b>train as well as test<\/b> our classifiers on <b>all of the training data<\/b>.","89db1bf5":"Now we have the features that may be of low to high importance to our model.<br><br>Lets find out the <b>importances of the remaining fetures<\/b> and <b>eliminate<\/b> the features that are of very low to no importance to us to prevent overfitting of our model.","60ed2eb6":"## 4.1 Cabin\n\nSince, we have huge amount of data missing from cabin, it will be difficult to visualise and analyse how cabin effected survival. So, for the sake of simplicity lets see how the presence or absence of cabin data affected survial.","c3ad0e13":"### 5.3.4 Embarked","44cb1787":"The <b>higher the class<\/b> (by higher I mean the lower the value of Pclass), the <b>greater fraction<\/b> of people survived.<br><br>This seems a little obvious correlation though.","d0b1642b":"## Check library versions","ecb896d5":"## 4.6 # Relatives","1d9d834b":"### 6.1.2 Fare","ccf7ed8c":"# 12. Predict from test data\n\nIts time, lets process our <b>test dataset and predict using our selected, trained classifier<\/b>.","ea0a3225":"# 1. Import Libraries","945dbe1d":"<b>Cabin<\/b> feature has <b>687<\/b> data missing out of total <b>891<\/b>, which makes about <b>77% of the data missing<\/b>, it seems like we might need to eliminate this column. But before doing so we'll make sure that we do not drop important data from our dataset. We don't have a lot of data missing from other features, so we'll impute those later.","10c1e587":"From the above peep we can see that the Cabin column has some data - <b>NaN<\/b>, which stands for <b>Not a Number<\/b>, i.e, those data are missing. We'll fix this issue later.","b91aaf55":"## 6.1 Feature Scaling\n\nWe will standard scale our data, i.e using our old simple scaling technique - \n\nz = (x - $\\bar{x}$) \/ $\\sigma$\n\nwhere,<br>\nx = feature value<br>\n$\\bar{x}$ = mean of features<br>\n$\\sigma$ = standard deviation of feature\n\n<br>We'll scale our features in following sequence - \n1. Age\n2. Fare\n\nFor ease lets define a function that does scaling for us.","7ed06089":"# 10. Predict\nIf you remember, we did seperate a <b>validation set<\/b>. Now, its time to check the <b>accuracy of our selected model on the validation set<\/b>.","74aad2e9":"We can see from the above barplots that most of the <b>males<\/b> that survives were around the age of <b>18-40<\/b>, whereas most of the <b>females<\/b> that survived were around the age of <b>15-38<\/b>","84b16456":"It seems like <b>XGboost<\/b> does a good job classifing our data, but before selecting a classifier lets see the accuracy distribution for each classifier","fdf2697e":"### 6.1.1 Age","30ebbf02":"# 9. Select best model","efee555a":"<b>Greater fraction<\/b> of people who survived, <b>embarked at Cherbourg<\/b>.","42681b1c":"## 3.1 Shape of dataset\n\nThe <b>shape<\/b> attribute of pandas dataframe allow us to quickly see, the <b>number of training examples<\/b> and the <b>number of features<\/b> available to us.","5cb2c2e1":"## 4.7  Embarked","f0595f2c":"## 3.5 Missing data\n\nLets take a brief look at the missing data.<br><br>What we will do is create a dataframe of missing data from our training dataset. This dataframe will give us the total and percent of missing data for each feature.","0cc66bea":"## 5.2 Missing data\n\nNow lets impute our missing data, in the following sequence - \n1. Cabin\n2. Age \n3. Embarked","6f3c8e72":"Seeing the barplot we can clearly infer that features - relatives, SibSp and Embarked_Q, have very small importance. So, it would be better to drop them, to prevent overfitting our model."}}