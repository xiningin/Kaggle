{"cell_type":{"4eeba357":"code","3ba986d9":"code","88b3ecae":"code","46cba807":"code","a0ced943":"code","332bb846":"code","73072a72":"code","28bfecb3":"code","181e8e44":"code","afb687f8":"code","6c5c3de4":"code","de065e09":"code","ac7b19e8":"code","87cf6716":"code","5a0212fa":"code","57958bd9":"code","da6039c0":"code","5276f846":"markdown","4ba4db23":"markdown","93e4f8d4":"markdown","b0ca7e3e":"markdown","d662d12f":"markdown","b1e572e6":"markdown","208fce58":"markdown","70a78448":"markdown","6381abff":"markdown","bed4c331":"markdown","c680c99a":"markdown","daa165bb":"markdown","dace5ebe":"markdown","38124f44":"markdown","a4a3d9d0":"markdown","771f6bfc":"markdown"},"source":{"4eeba357":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom PIL import Image\n\npd.options.display.max_colwidth = 200\nplt.style.use('seaborn')","3ba986d9":"data_path = '..\/input\/tensorflow-great-barrier-reef'\n!ls {data_path}","88b3ecae":"# greatbarrierreef\/ : image delivery api\n!ls {os.path.join(data_path, 'greatbarrierreef\/')}","46cba807":"# train_images\/ : training data folders, containing 3 videos folders : video_{video_id}\n!ls {os.path.join(data_path, 'train_images\/')}","a0ced943":"# video folder : contains video frames {video_frame_number}.jpg. \n!ls {os.path.join(data_path, 'train_images\/video_0\/')} | head -n 5","332bb846":"df_train = pd.read_csv(os.path.join(data_path, 'train.csv'))\ndf_train.head()","73072a72":"df_train.info()","28bfecb3":"# annotations : list of dict (x = x_min, y = y_min)\ndf_train.loc[df_train['annotations'] != '[]']['annotations'].sample(1).values[0]","181e8e44":"# size\nvideo_ids = df_train['video_id'].unique()\nprint(f'Video count : {len(video_ids)}')\nfor video_id in video_ids:\n    img_path = os.path.join(data_path, 'train_images', f'video_{video_id}', '0.jpg')\n    im = Image.open(img_path)\n    print(f'Video {video_id} : {im.size}, {im.mode}')\n    \nSIZE = (1280, 720)","afb687f8":"def get_video_data(df_train):\n    video_data = {}\n    for video_id in df_train['video_id'].unique():\n        df_video = df_train.loc[df_train['video_id'] == video_id]\n        data_sequence = {}\n        video_data[video_id] = {}\n        video_data[video_id]['frames_count'] = 0\n        video_data[video_id]['frames_with_annot_count'] = 0\n        for sequence in df_video['sequence'].unique():\n            df_sequence = df_video.loc[df_video['sequence'] == sequence]\n            seq_annotations = {}\n            seq_annotations['frames_count'] = len(df_sequence)\n            seq_annotations['frames_with_annot_count'] = df_sequence.loc[df_train['annotations'] != '[]']['annotations'].count()\n            data_sequence[sequence] = seq_annotations\n            video_data[video_id]['frames_count'] += seq_annotations['frames_count']\n            video_data[video_id]['frames_with_annot_count'] += seq_annotations['frames_with_annot_count']\n        video_data[video_id]['sequence'] = data_sequence\n    return video_data\n\ndef print_video_data(video_data):\n    for video_id in video_data.keys():\n        frames_count = video_data[video_id]['frames_count']\n        frames_with_annot_count = video_data[video_id]['frames_with_annot_count']\n        print(f'Video {video_id} : {frames_count} frames, {frames_with_annot_count} frames with annotation(s)')\n        for sequence_id in video_data[video_id]['sequence'].keys():\n            frames_count = video_data[video_id]['sequence'][sequence_id]['frames_count']\n            annotations_count = video_data[video_id]['sequence'][sequence_id]['frames_with_annot_count']\n            print(f'  Sequence {sequence_id} : {frames_count} frames, {annotations_count} with annotation(s)')\n        print('\\n')\n\nvideo_data = get_video_data(df_train)\nprint_video_data(video_data)","6c5c3de4":"def plot_video_data(video_data):\n    plt.style.use('seaborn')\n    fig, axs = plt.subplots(1, 3, figsize=((15, 5)))\n    frames = {f'Video {key}': value['frames_count'] for key, value in video_data.items()}\n    axs[0].bar(frames.keys(), frames.values(), width=0.3)\n    axs[0].set_ylabel('frames')\n    axs[0].set_title('Frames count per video')\n    seq = {f'Video {key}': len(value['sequence']) for key, value in video_data.items()}\n    axs[1].bar(seq.keys(), seq.values(), width=0.3)\n    axs[1].set_ylabel('sequences')\n    axs[1].set_title('Sequences count per video')\n    annot = {f'Video {key}': value['frames_with_annot_count'] for key, value in video_data.items()}\n    axs[2].bar(annot.keys(), annot.values(), width=0.3)\n    axs[2].set_ylabel('annotations')\n    axs[2].set_title('Annotations count per video')\n\nplot_video_data(video_data)\n","de065e09":"def plot_frame_data(video_data):\n    fig, axs = plt.subplots(1, len(video_data.keys()), figsize=((15, 5)))\n    for video_id in video_data.keys():\n        annot = {f'Seq. {key}': value['frames_with_annot_count'] for key, value in video_data[video_id]['sequence'].items()}\n        no_annot = {f'Seq. {key}': value['frames_count'] - value['frames_with_annot_count'] for key, value in video_data[video_id]['sequence'].items()}\n        width = 0.5 * len(annot) \/ 8\n        axs[video_id].bar(annot.keys(), annot.values(), width=width, label='annotation(s)')\n        axs[video_id].bar(no_annot.keys(), no_annot.values(), width=width, label='no annotation', bottom=list(annot.values()))\n        axs[video_id].set_ylabel('frames')\n        axs[video_id].tick_params(axis='x', labelrotation=90)\n        axs[video_id].set_title(f'Video {video_id} : annotations count per sequence')\n        axs[video_id].legend()\n\nplot_frame_data(video_data)","ac7b19e8":"def get_annotation_count(df_train):\n    df_train = df_train.sort_values(by=['video_id', 'sequence', 'sequence_frame'])\n    df_train['annots_count'] = df_train['annotations'].apply(lambda annots : len(eval(annots)))\n    return df_train\n    \ndf_train = get_annotation_count(df_train)\ndf_annot = df_train['annots_count'].value_counts()\n\ndef plot_annot_distrib(df_annot):\n    fig, ax = plt.subplots(1, 1, figsize=(8,5))\n    ax.bar(df_annot.index, df_annot, tick_label=df_annot.index)\n    ax.set_ylabel('frames')\n    ax.set_xlabel('annotation count per frame')\n    \nplot_annot_distrib(df_annot)","87cf6716":"def get_annotation_time(df_train):\n    annot_data = {}\n    for video_id in df_train['video_id'].unique():\n        df_video = df_train.loc[df_train['video_id'] == video_id]\n        annot_data[video_id] = {}\n        for sequence in df_video['sequence'].unique():\n            df_annot_time = df_video.loc[df_video['sequence'] == sequence]\n            df_annot_time = df_annot_time.sort_values(by='sequence_frame')['annots_count']\n            annot_data[video_id][sequence] = df_annot_time.values\n    return annot_data\n\nannotation_time =  get_annotation_time(df_train)\n\ndef plot_annotation_time(annotation_time):\n    for video_id, sequences in annotation_time.items():\n        for sequence, annot in sequences.items():\n            fig, ax = plt.subplots(1, 1, figsize=(15, 2))\n            ax.plot(annot)\n            ax.set_ylabel('annotation count')\n            ax.set_xlabel('time (frame)')\n            ax.set_title(f'Video {video_id}, sequence : {sequence}')\n            \nplot_annotation_time(annotation_time)","5a0212fa":"def get_annotation_pos_and_size(df_train):\n    annot_data = {}\n    for video_id in df_train['video_id'].unique():\n        df_video = df_train.loc[df_train['video_id'] == video_id]\n        annot_data[video_id] = {}\n        for sequence in df_video['sequence'].unique():\n            annots = []\n            df_annot_time = df_video.loc[df_video['sequence'] == sequence]\n            raw_annots = df_annot_time['annotations'].apply(lambda annots : eval(annots)).values\n            annots = [annot for sublist in raw_annots for annot in sublist]\n            annots = [list(annot.values()) for annot in annots]\n            annots = np.array(annots)\n            annot_data[video_id][sequence] = annots\n    return annot_data\n\nannotation_pos_and_size =  get_annotation_pos_and_size(df_train)\n\ndef plot_annotation_pos(annotation_pos_and_size):\n    for video_id, sequences in annotation_pos_and_size.items():\n        for sequence, annot in sequences.items():\n            if annot.shape[0] != 0:\n                fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n                ax.scatter(annot[:,0], annot[:,1], alpha=0.5)\n                ax.set_ylabel('height')\n                ax.set_xlabel('width')\n                ax.set_xbound(0, SIZE[0])\n                ax.set_ybound(0, SIZE[1])\n                ax.set_title(f'Starfish position (video {video_id}, sequence : {sequence})')\n                \ndef plot_annotation_size(annotation_pos_and_size):\n    fig, axs = plt.subplots(3, len(df_train['video_id'].unique()), figsize=(15, 15))\n    idx = 0\n    for video_id, sequences in annotation_pos_and_size.items():\n        width = []\n        height = []\n        ratio_wh = []\n        sequence_id = []\n        for sequence, annot in sequences.items():\n            if annot.shape[0] != 0:\n                sequence_id.append(sequence)\n                width.append(annot[:, 2])\n                height.append(annot[:, 3])\n                ratio_wh.append(annot[:, 2] \/ annot[:, 3])\n        # plot width\n        axs[0, idx].boxplot(width, labels=sequence_id)\n        axs[0, idx].set_ylabel('height')\n        axs[0, idx].tick_params(axis='x', labelrotation=90)\n        axs[0, idx].set_xlabel('sequence')\n        axs[0, idx].set_title(f'Bounding box width (video {video_id})')\n        # plot height\n        axs[1, idx].boxplot(height, labels=sequence_id)\n        axs[1, idx].set_ylabel('height')\n        axs[1, idx].tick_params(axis='x', labelrotation=90)\n        axs[1, idx].set_xlabel('sequence')\n        axs[1, idx].set_title(f'Bounding box height (video {video_id})')\n        # plot ratio width \/ height\n        axs[2, idx].boxplot(ratio_wh, labels=sequence_id)\n        axs[2, idx].set_ylabel('ratio')\n        axs[2, idx].tick_params(axis='x', labelrotation=90)\n        axs[2, idx].set_xlabel('sequence')\n        axs[2, idx].set_title(f'Bounding box ratio width \/ height (video {video_id})')\n        idx += 1\n    fig.tight_layout() \n\nplot_annotation_size(annotation_pos_and_size)\n","57958bd9":"plot_annotation_pos(annotation_pos_and_size)","da6039c0":"def get_sample_frames(df_train):\n    samples = []\n    for video_id in df_train['video_id'].unique():\n        df_video = df_train.loc[df_train['video_id'] == video_id]\n        for sequence in df_video['sequence'].unique():\n            df_sample = df_video.loc[df_video['sequence'] == sequence]\n            try:\n                df_sample = df_sample.loc[df_sample['annotations'] != '[]'].sample(1)\n            except:\n                df_sample = df_sample.sample(1)\n            samples.append(df_sample)\n    return samples\n\ndef process_frame(sample):\n    # frame\n    video_id = sample['video_id'].values[0]\n    frame = sample['video_frame'].values[0]\n    sequence = sample['sequence'].values[0]\n    img_path = os.path.join(data_path, 'train_images', f'video_{video_id}', f'{frame}.jpg')\n    frame = np.array(Image.open(img_path))\n    # bounding boxes\n    try:\n        bboxs = eval(sample['annotations'].values[0])\n        bboxs = [list(values.values()) for values in bboxs]\n        bboxs = np.array(bboxs)\n    except: # no bounding box in sequence\n        bboxs = None\n    return frame, bboxs, video_id, sequence\n\ndef display_frame(frame, bboxs, video_id, sequence):\n    plt.style.use('seaborn-dark')\n    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n    # frame\n    ax.imshow(frame)\n    ax.set_ylabel('height')\n    ax.set_xlabel('width')\n    ax.set_xbound(0, SIZE[0])\n    ax.set_ybound(0, SIZE[1])\n    ax.set_title(f'Frame sample from video {video_id}, sequence : {sequence}')\n    # bounding boxes\n    for bbox in bboxs:\n        rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=3, edgecolor='r', facecolor='none')\n        ax.add_patch(rect)\n    return\n\nsample_frames = get_sample_frames(df_train)\n\nfor sample in sample_frames:\n    frame, bboxs, video_id, sequence = process_frame(sample)\n    display_frame(frame, bboxs, video_id, sequence)","5276f846":"## 3. Finally","4ba4db23":"### Starfish annotations example","93e4f8d4":"### Are the annotations well distributed in the videos?","b0ca7e3e":"### Video size","d662d12f":"# Data exploration","b1e572e6":"\n- video_id - ID number of the video the image was part of.\n- sequence - ID of a gap-free subset of a given video.\n- video_frame - The frame number of the image within the video.\n- sequence_frame - The frame number within a given sequence.\n- image_id - ID code for the image, in the format '{video_id}-{video_frame}'\n- annotations - The bounding boxes of any starfish detections in a string format.","208fce58":"### Where are the boundings boxes?","70a78448":"### Raw data","6381abff":"## 2. train.csv","bed4c331":"### How many starfish are there per frame?","c680c99a":"## 4. Annotations","daa165bb":"## 1. Dataset files","dace5ebe":"## 3. Videos","38124f44":"### Relationship between videos, sequences, frames and annotations","a4a3d9d0":"### How are the annotations distributed over time?","771f6bfc":"### What is the shape of the bounding box?"}}