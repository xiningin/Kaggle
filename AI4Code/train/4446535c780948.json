{"cell_type":{"bda7605e":"code","204aed5f":"code","01f79c1a":"code","0f7df513":"code","a6d65914":"code","e9132333":"code","d67a85ad":"code","b0c4600c":"code","37a6362f":"code","37647e06":"code","2f52b12b":"code","70a7140b":"code","8a1cf45b":"code","0e0dd5e8":"code","6a27bcab":"code","cb560aca":"code","10d47513":"code","af94e10b":"code","a708517a":"code","cdd56ea6":"code","ce8db433":"code","70784ddd":"code","0ba23dfb":"code","ae397433":"code","d4a3422f":"code","9f1b4da3":"code","3450c69c":"code","be7d5fa5":"code","a098d9ca":"code","3d5246cf":"code","91378d36":"code","afd1d8aa":"code","39e530c0":"markdown"},"source":{"bda7605e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport sys\nimport zipfile\nimport datetime\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/huggingfacepytorchpretrainedbertchinese\"))\nprint(os.listdir(\"..\/input\/huggingfacepytorchpretrainedbertchinese\/bert-base-chinese\"))\n# for d in os.listdir(\"..\/input\"):\n#     print(os.listdir(f\"..\/input\/{d}\"))\n# print(os.listdir(\"..\/input\/googles-bert-model\/chinese_l-12_h-768_a-12\/chinese_L-12_H-768_A-12\"))\n# Any results you write to the current directory are saved as output.","204aed5f":"VOCAB = '..\/input\/huggingfacepytorchpretrainedbertchinese\/bert-base-chinese-vocab.txt'\nMODEL = '..\/input\/huggingfacepytorchpretrainedbertchinese\/bert-base-chinese'","01f79c1a":"!pip install pytorch_pretrained_bert ","0f7df513":"import torch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification","a6d65914":"# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","e9132333":"# Tokenized input\ntext = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\ntokenized_text = tokenizer.tokenize(text)\ntokenized_text","d67a85ad":"tokenizer.tokenize('su')","b0c4600c":"if os.path.isdir(\"..\/input\/fake-news-pair-classification-challenge\"):\n    TRAIN_CSV_PATH = '..\/input\/fake-news-pair-classification-challenge\/train.csv'\n    TEST_CSV_PATH = '..\/input\/fake-news-pair-classification-challenge\/test.csv'\n    TOKENIZED_TRAIN_CSV_PATH = \"..\/input\/siamese-network-lstm\/tokenized_train.csv\"\nelse:\n    TRAIN_CSV_PATH = '..\/input\/train.csv'\n    TEST_CSV_PATH = '..\/input\/test.csv'\n    TOKENIZED_TRAIN_CSV_PATH = \"\"\n    \ntrain = pd.read_csv(TRAIN_CSV_PATH, index_col='id')\ntest = pd.read_csv(TEST_CSV_PATH, index_col='id')\ncols = ['title1_zh', \n        'title2_zh', \n        'label']\ntrain = train.loc[:, cols]\ntest = test.loc[:, cols]\ntrain.fillna('UNKNOWN', inplace=True)\ntest.fillna('UNKNOWN', inplace=True)\ntrain.head(3)","37a6362f":"from collections import Counter\nCounter(train.label)","37647e06":"!wget https:\/\/raw.githubusercontent.com\/huggingface\/pytorch-pretrained-BERT\/master\/examples\/run_classifier.py\n    \n    ","2f52b12b":"from sklearn.model_selection \\\n    import train_test_split\n\nVALIDATION_RATIO = 0.1\n\nRANDOM_STATE = 9527\n\ntrain, val= \\\n    train_test_split(\n        train, \n        test_size=VALIDATION_RATIO, \n        random_state=RANDOM_STATE\n)","70a7140b":"label_list = ['unrelated', 'agreed', 'disagreed']","8a1cf45b":"from run_classifier import *\n\ntrain_examples = [InputExample('train', row.title1_zh, row.title2_zh, row.label) for row in train.itertuples()]\nval_examples = [InputExample('val', row.title1_zh, row.title2_zh, row.label) for row in val.itertuples()]\ntest_examples = [InputExample('test', row.title1_zh, row.title2_zh, 'unrelated') for row in test.itertuples()]\n\nlen(train_examples)","0e0dd5e8":"orginal_total = len(train_examples)\ntrain_examples = train_examples[:int(orginal_total*0.2)]","6a27bcab":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ngradient_accumulation_steps = 1\ntrain_batch_size = 32\neval_batch_size = 128\ntrain_batch_size = train_batch_size \/\/ gradient_accumulation_steps\noutput_dir = 'output'\nbert_model = 'bert-base-chinese'\nnum_train_epochs = 3\nnum_train_optimization_steps = int(\n            len(train_examples) \/ train_batch_size \/ gradient_accumulation_steps) * num_train_epochs\ncache_dir = \"model\"\nlearning_rate = 5e-5\nwarmup_proportion = 0.1\nmax_seq_length = 128\nlabel_list = ['unrelated', 'agreed', 'disagreed']","cb560aca":"tokenizer = BertTokenizer.from_pretrained(VOCAB)\nmodel = BertForSequenceClassification.from_pretrained(MODEL,\n              cache_dir=cache_dir,\n              num_labels = 3)\nmodel.to(device)\nif n_gpu > 1:\n    model = torch.nn.DataParallel(model)\nmodel, tokenizer","10d47513":"# Prepare optimizer\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\noptimizer = BertAdam(optimizer_grouped_parameters,\n                             lr=learning_rate,\n                             warmup=warmup_proportion,\n                             t_total=num_train_optimization_steps)","af94e10b":"\nglobal_step = 0\nnb_tr_steps = 0\ntr_loss = 0\n\ntrain_features = convert_examples_to_features(\n    train_examples, label_list, max_seq_length, tokenizer)\nlogger.info(\"***** Running training *****\")\nlogger.info(\"  Num examples = %d\", len(train_examples))\nlogger.info(\"  Batch size = %d\", train_batch_size)\nlogger.info(\"  Num steps = %d\", num_train_optimization_steps)\nall_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\nall_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\nall_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\nall_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\ntrain_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n\nmodel.train()\nfor _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    total_step = len(train_data) \/\/ train_batch_size\n    ten_percent_step = total_step \/\/ 10\n    for step, batch in enumerate(train_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        input_ids, input_mask, segment_ids, label_ids = batch\n        loss = model(input_ids, segment_ids, input_mask, label_ids)\n        if n_gpu > 1:\n            loss = loss.mean() # mean() to average on multi-gpu.\n        if gradient_accumulation_steps > 1:\n            loss = loss \/ gradient_accumulation_steps\n\n        loss.backward()\n\n        tr_loss += loss.item()\n        nb_tr_examples += input_ids.size(0)\n        nb_tr_steps += 1\n        if (step + 1) % gradient_accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            global_step += 1\n            \n        if step % ten_percent_step == 0:\n            print(\"Fininshed: {:.2f}% ({}\/{})\".format(step\/total_step*100, step, total_step))","a708517a":"if not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Save a trained model and the associated configuration\nmodel_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\noutput_model_file = os.path.join(output_dir, WEIGHTS_NAME)\ntorch.save(model_to_save.state_dict(), output_model_file)\noutput_config_file = os.path.join(output_dir, CONFIG_NAME)\nwith open(output_config_file, 'w') as f:\n    f.write(model_to_save.config.to_json_string())","cdd56ea6":"# Load a trained model and config that you have fine-tuned\nconfig = BertConfig(output_config_file)\nmodel = BertForSequenceClassification(config, num_labels=len(label_list))\nmodel.load_state_dict(torch.load(output_model_file))\nmodel.to(device)  # important to specific device\nif n_gpu > 1:\n    model = torch.nn.DataParallel(model)","ce8db433":"config","70784ddd":"# val\neval_examples = val_examples\neval_features = convert_examples_to_features(\n    eval_examples, label_list, max_seq_length, tokenizer)\nlogger.info(\"***** Running evaluation *****\")\nlogger.info(\"  Num examples = %d\", len(eval_examples))\nlogger.info(\"  Batch size = %d\", eval_batch_size)\nall_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\nall_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\nall_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\nall_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\neval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n# Run prediction for full data\neval_sampler = SequentialSampler(eval_data)\neval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n\nmodel.eval()\neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\n\nfor input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n    input_ids = input_ids.to(device)\n    input_mask = input_mask.to(device)\n    segment_ids = segment_ids.to(device)\n    label_ids = label_ids.to(device)\n\n    with torch.no_grad():\n        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n        logits = model(input_ids, segment_ids, input_mask)\n\n    logits = logits.detach().cpu().numpy()\n    label_ids = label_ids.to('cpu').numpy()\n    tmp_eval_accuracy = accuracy(logits, label_ids)\n\n    eval_loss += tmp_eval_loss.mean().item()\n    eval_accuracy += tmp_eval_accuracy\n\n    nb_eval_examples += input_ids.size(0)\n    nb_eval_steps += 1\n\neval_loss = eval_loss \/ nb_eval_steps\neval_accuracy = eval_accuracy \/ nb_eval_examples\nloss = tr_loss\/nb_tr_steps\nresult = {'eval_loss': eval_loss,\n          'eval_accuracy': eval_accuracy,\n          'global_step': global_step,\n          'loss': loss}\n\noutput_eval_file = os.path.join(output_dir, \"eval_results.txt\")\nwith open(output_eval_file, \"w\") as writer:\n    logger.info(\"***** Eval results *****\")\n    for key in sorted(result.keys()):\n        logger.info(\"  %s = %s\", key, str(result[key]))\n        writer.write(\"%s = %s\\n\" % (key, str(result[key])))","0ba23dfb":"device","ae397433":"!ls output\n! cat output\/eval_results.txt\n","d4a3422f":"model","9f1b4da3":"def predict(model, tokenizer, examples, label_list, eval_batch_size=128):\n    model.to(device)\n    eval_examples = examples\n    eval_features = convert_examples_to_features(\n        eval_examples, label_list, max_seq_length, tokenizer)\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_examples))\n    logger.info(\"  Batch size = %d\", eval_batch_size)\n    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n    # Run prediction for full data\n    eval_sampler = SequentialSampler(eval_data)\n    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n\n    model.eval()\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    \n    res = []\n    for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n        input_ids = input_ids.to(device)\n        input_mask = input_mask.to(device)\n        segment_ids = segment_ids.to(device)\n#         label_ids = label_ids.to(device)\n\n        with torch.no_grad():\n#             tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n            logits = model(input_ids, segment_ids, input_mask)\n\n        logits = logits.detach().cpu().numpy()\n#         print(logits)\n        res.extend(logits.argmax(-1))\n#         label_ids = label_ids.to('cpu').numpy()\n#         tmp_eval_accuracy = accuracy(logits, label_ids)\n\n#         eval_loss += tmp_eval_loss.mean().item()\n#         eval_accuracy += tmp_eval_accuracy\n\n#         nb_eval_examples += input_ids.size(0)\n        nb_eval_steps += 1\n\n#     eval_loss = eval_loss \/ nb_eval_steps\n#     eval_accuracy = eval_accuracy \/ nb_eval_examples\n#     loss = tr_loss\/nb_tr_steps \n#     result = {'eval_loss': eval_loss,\n#               'eval_accuracy': eval_accuracy,\n#               'global_step': global_step,\n#               'loss': loss}\n\n#     output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n#     with open(output_eval_file, \"w\") as writer:\n#         logger.info(\"***** Eval results *****\")\n#         for key in sorted(result.keys()):\n#             logger.info(\"  %s = %s\", key, str(result[key]))\n#             writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n    return res\n    ","3450c69c":"res = predict(model, tokenizer, test_examples, label_list)","be7d5fa5":"label_list","a098d9ca":"set(res)","3d5246cf":"predict(model, tokenizer, test_examples[:10], label_list)","91378d36":"cat_map = {idx:lab for idx, lab in enumerate(label_list)}\nres = [cat_map[c] for c  in res]","afd1d8aa":"#\u3000For Submission\n\ntest['Category'] = res\n\n\nsubmission = test \\\n    .loc[:, ['Category']] \\\n    .reset_index()\n\nsubmission.columns = ['Id', 'Category']\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","39e530c0":"# Just list which models can used\n\n            'bert-base-uncased'\n            'bert-large-uncased'\n            'bert-base-cased': \n            'bert-large-cased'\n            'bert-base-multilingual-uncased'\n            'bert-base-multilingual-cased'\n            'bert-base-chinese'\n       "}}