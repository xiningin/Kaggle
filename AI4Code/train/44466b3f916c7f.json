{"cell_type":{"d83ee2f2":"code","af3a1f87":"code","d3e17451":"code","7412e855":"code","6c4493c7":"code","967039c7":"code","f1bfae58":"code","4b46e16d":"code","87374f12":"code","685720d0":"code","0ceaba9b":"code","80525b8d":"code","8f4e7714":"code","a7a10718":"code","bd2122b5":"code","dd32c277":"code","ecf2645c":"code","024c4c2f":"code","9975f648":"markdown","1c0fea24":"markdown","4abb4d4c":"markdown","e6be745c":"markdown","d4a344a5":"markdown","14837f96":"markdown","10676a00":"markdown","93d04dfa":"markdown","9c61c62f":"markdown","e61280b9":"markdown","3945019e":"markdown","48741c7b":"markdown","eb1e955c":"markdown","321f6e9f":"markdown","2523fe17":"markdown","417d62a7":"markdown","934240ea":"markdown","82d7536c":"markdown","7347b4f3":"markdown","0b83caab":"markdown"},"source":{"d83ee2f2":"# standard Python tools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# special tools for working in Kaggle\nimport joblib   # save and load ML models\nimport gc       # garbage collection\nimport os \nimport sklearn\n\n# preprocessing steps\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# machine learning models and tools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n# cross validation and metrics - remember this competition is scored as area under curve\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# surely there will be a lot more packages loaded by the time we are done!","af3a1f87":"# Don't do this!\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d3e17451":"MainDir = \"..\/input\/..\/input\/home-credit-default-risk\"\nprint(os.listdir(MainDir))\n\n# Main table\ntrain = pd.read_csv(f'{MainDir}\/application_train.csv')\n\ntrain.head(5)","7412e855":"# what is going on with days_employed? Over 50,000 entries have the value 365,243 days! Let's replace those with NaN and let the imputer deal with them.\ntrain['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\n# ratio features\ntrain['CI_ratio'] = train['AMT_CREDIT'] \/ train['AMT_INCOME_TOTAL']        # credit-to-income ratio\ntrain['AI_ratio'] = train['AMT_ANNUITY'] \/ train['AMT_INCOME_TOTAL']       # annuity-to-income ratio\ntrain['AC_ratio'] = train['AMT_CREDIT'] \/ train['AMT_ANNUITY']             # credit to annuity - basically the term of the loan in years\ntrain['CG_ratio'] = train['AMT_CREDIT'] \/ train['AMT_GOODS_PRICE']         # credit to goods price ratio - how much was financed?\n\n# log features\ntrain['log_INCOME'] = np.log(train['AMT_INCOME_TOTAL'])                    # log of income\ntrain['log_ANNUITY'] = np.log(train['AMT_ANNUITY'])                        # log of annuity\ntrain['log_CREDIT'] = np.log(train['AMT_CREDIT'])                          # log of credit\ntrain['log_GOODS'] = np.log(train['AMT_GOODS_PRICE'])                      # log of goods price\n\n# flag features\ntrain['MissingBureau'] = train.iloc[:, 41:44].isnull().sum(axis=1).astype(\"category\")   # number of bureaus with no score\ntrain['FLAG_CG_ratio'] = train['AMT_CREDIT'] > train['AMT_GOODS_PRICE']                 # FLAG if you borrowed more than the price of the item\n\n# EXT_SOURCE_x variables are very important - let's not leave missing values up to the imputer!\n# Instead of imputing missing values by column mean or median, let's fill in missing values by row\n# i.e. missing scores are replaced with the average of the scores we do have. If there are no scores at all\n# let's just give them a value of 0.2 for now.\ntrain['AVG_EXT'] = train.iloc[:, 41:44].sum(axis=1)\/(3- train.iloc[:,41:44].isnull().sum(axis=1))   # average of the (at most) three scores\ntrain['AVG_EXT'].replace(np.nan, 0.2, inplace = True)   # get rid of any \/0 errors generated from previous step\n\ntrain.EXT_SOURCE_1.fillna(train.AVG_EXT, inplace=True)\ntrain.EXT_SOURCE_2.fillna(train.AVG_EXT, inplace=True)\ntrain.EXT_SOURCE_3.fillna(train.AVG_EXT, inplace=True)\n\ntrain.drop(['AVG_EXT'], axis = 1)   # let's not make AVG_EXT a feature - it will be too highly correlated to the three components\n\n# drop these variables based on poor feature significance (< 0.0001)\ntrain.drop(['REG_REGION_NOT_LIVE_REGION','AMT_REQ_CREDIT_BUREAU_WEEK','HOUSETYPE_MODE','OCCUPATION_TYPE','FLAG_MOBIL','FLAG_CONT_MOBILE',\n           'NAME_TYPE_SUITE', 'FLAG_DOCUMENT_4','ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_16',\n           'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'AMT_REQ_CREDIT_BUREAU_DAY',\n           'AMT_REQ_CREDIT_BUREAU_HOUR', 'FLAG_DOCUMENT_21','FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_17','FLAG_DOCUMENT_2'],\n           axis=1, inplace=True)","6c4493c7":"y = train['TARGET'].values\nX_train, X_valid, y_train, y_valid = train_test_split(train.drop(['TARGET', 'SK_ID_CURR'], axis = 1), y, stratify = y, test_size=0.9, random_state=1)\nprint('Shape of X_train:',X_train.shape)\nprint('Shape of y_train:',y_train.shape)\nprint('Shape of X_valid:',X_valid.shape)\nprint('Shape of y_valid:',y_valid.shape)","967039c7":"types = np.array([z for z in X_train.dtypes])        # array([dtype('float64'), dtype('float64'), dtype('O'), dtype('O') ...])\nall_columns = X_train.columns.values                 # list of all column names\nis_num = types != 'object'                           # returns array([False, False, False, False,  True,  True, ...) where True is a numeric variable\nnum_features = all_columns[is_num].tolist()          # list of all numeric columns\ncat_features = all_columns[~is_num].tolist()         # list of all categorical columns\n\nprint(len(num_features), \"numeric features\")\nprint(len(cat_features), \"categorical features\")","f1bfae58":"features = num_features + cat_features\n\nPipe_num = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy = 'median')),        # tried median, mean, constant strategies\n    ('scaler', StandardScaler())       ])\n\nPipe_cat = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'Unknown')),\n    ('onehot', OneHotEncoder())        ])\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', Pipe_num, num_features),\n        ('cat', Pipe_cat, cat_features)])\n\npreprocessor.fit(train[features])\nX_train = preprocessor.transform(X_train[features])\nX_valid = preprocessor.transform(X_valid[features])\n\nprint('Shape of X_train:',X_train.shape)\nprint('Shape of y_train:',y_train.shape)","4b46e16d":"# set up table - new rows will be appended as models are run\n\npd.set_option('display.max_colwidth', None)             # LGBM in particular has long hyperparameters and I want to see them all\nresults = pd.DataFrame(columns = ['Model Type','AUC - 10xv', 'AUC - Valid', 'Hyperparameters'])","87374f12":"%%time\n# This model is running slow as a dog right now - might need to run it overnight\nlr_clf = LogisticRegression(max_iter=2000, solver='saga', penalty = 'elasticnet')\nlr_parameters = {'l1_ratio':[1], 'C': [1]}\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=1, scoring='roc_auc')\nlr_grid.fit(X_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Logistic Regression',\n                          'AUC - 10xv' : lr_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, lr_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : lr_grid.best_params_},\n                        ignore_index=True)\nresults","685720d0":"%%time\nrf_clf = RandomForestClassifier(random_state=1, n_estimators=100)\nrf_parameters = {'max_depth': [28, 30, 32],  'min_samples_leaf': [30, 32, 34, 36]}\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=1, scoring='roc_auc')\nrf_grid.fit(X_train, y_train)\nrf_model = rf_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Random Forest',\n                          'AUC - 10xv' : rf_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, rf_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : rf_grid.best_params_},\n                        ignore_index=True)\nresults","0ceaba9b":"%%time\ndt_clf = DecisionTreeClassifier(random_state=1)\ndt_parameters = {\n    'max_depth': [4, 8, 12, 16, 20, 24],\n    'min_samples_leaf': [2, 4, 6, 8]\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='roc_auc')\ndt_grid.fit(X_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Decision Tree',\n                          'AUC - 10xv' : dt_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, dt_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : dt_grid.best_params_},\n                        ignore_index=True)\nresults","80525b8d":"%%time\nXGB_clf = XGBClassifier(objective='binary:logistic', use_label_encoder=False)\nXGB_parameters = {\n    'max_depth': range(1, 3, 5),\n    'n_estimators': range(10, 50, 100),\n    'learning_rate': [0.25, 0.5, 0.75, 1, 1.25, 1.5]\n}\n\nXGB_grid = GridSearchCV(XGB_clf, XGB_parameters, cv=10, n_jobs=10, verbose=True, scoring= 'roc_auc')\nXGB_grid.fit(X_train, y_train)\n\nXGB_model = XGB_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Light GBM',\n                          'AUC - 10xv' : XGB_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, XGB_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : XGB_grid.best_params_},\n                        ignore_index=True)\nresults","8f4e7714":"probabilities = rf_model.predict_proba(X_valid)[:,1]\nfpr, tpr, thresholds = roc_curve(y_valid, probabilities)\nauc = roc_auc_score(y_valid, probabilities)               # AUC on validation data was .7403 per table above\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr)                                        # plot the blue curve\nplt.plot([0, 1], [0, 1])                                  # plot the orange 45 degree line\nplt.title('Receiver operating characteristic curve')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend([\"AUC = %.6f\"%auc])\nplt.show()\n\n# hat tip:\n# https:\/\/medium.com\/@praveenkotha\/home-credit-default-risk-end-to-end-machine-learning-project-1871f52e3ef2\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html","a7a10718":"importance_DF = pd.DataFrame(zip(rf_model.feature_importances_, features), columns=['Value','Feature']).sort_values(by=\"Value\", ascending=False)\nimportance_plot = importance_DF[importance_DF['Value'] > 0.01]\nplt.figure(figsize=[10,6])\nsns.barplot(importance_plot['Value'], importance_plot['Feature'], orient = \"h\", color = \"lightsteelblue\")\nplt.title(\"Most important features (min 1% of total)\")\nplt.show()","bd2122b5":"# Consider dropping these\ndrop_list = importance_DF[importance_DF['Value'] < 1E-4]['Feature'].to_list()\ndrop_list","dd32c277":"from sklearn.model_selection import cross_val_score\nfinal_model = RandomForestClassifier(random_state=1, n_estimators=100, max_depth=24, min_samples_leaf=24)\nscores = cross_val_score(rf_model, X_train, y_train, cv=10, scoring = 'roc_auc')\nprint(scores.round(2))\nprint(scores.mean())\nprint(scores.std(ddof=1))\nplt.figure(figsize=[12,3])\nsns.boxplot(scores, orient = \"h\")\nplt.show()","ecf2645c":"boxdata = pd.DataFrame({'prediction' : lr_model.predict_proba(X_valid)[:,1], 'target' : y_valid})\nplt.figure(figsize=[12,3])\nsns.boxplot(boxdata.prediction, boxdata.target, orient = \"h\")\nplt.show()","024c4c2f":"final_model = LogisticRegression(max_iter=2000, solver='saga', penalty = 'elasticnet', C = 1, l1_ratio = 1)\nfinal_model.fit(X_train, y_train)\n\njoblib.dump(preprocessor, 'default_preprocessor_06.joblib') \njoblib.dump(final_model, 'default_model_06.joblib')","9975f648":"### Import packages","1c0fea24":"### Random Forest","4abb4d4c":"### Factors with little explanatory power - consider dropping these","e6be745c":"### Boxplot on validation set","d4a344a5":"### split out our training data - start with about 10% or 30k out of 300k","14837f96":"### Logistic Regression","10676a00":"### Creating separation between classes","93d04dfa":"# Models","9c61c62f":"### Read the training data","e61280b9":"### Feature Importance (random forest)","3945019e":"### Plotting the ROC curve (for AUC score on validation data)","48741c7b":"## Home Credit Default Risk - Team 3 (Kahsai, Nichols, Pellerito)","eb1e955c":"### data cleansing and feature engineering: create new features based on ratios, logs, etc.","321f6e9f":"### Light Gradient Boost Machine","2523fe17":"### make lists of cat and num features for pipeline, based on dtype","417d62a7":"### Final Model Selection - save data","934240ea":"### Decision Tree","82d7536c":"### build model pipeline based on num_cols and cat_cols lists","7347b4f3":"# First look at training data set","0b83caab":"### Build Model Scoreboard"}}