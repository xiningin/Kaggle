{"cell_type":{"ba8ea035":"code","9b7ec11c":"code","d108702b":"code","11da3968":"code","4e9a61dd":"code","b5796ee1":"code","95414317":"code","990b4cc4":"code","369a09ce":"code","efefc643":"code","2fb58c80":"code","f62c23c9":"code","398968b6":"code","386642de":"code","286065c6":"code","d9983463":"code","7bfbe3b4":"code","5fa69729":"code","7edda71e":"code","9057bbdf":"code","ad8e17e2":"code","58a5763d":"code","0616f8b3":"code","fd9b5de0":"code","c979ba69":"code","fd0bc4cd":"code","44a9473b":"code","9b0533a9":"code","698f8b5c":"code","e2660e98":"code","7f079c2d":"code","ce269609":"markdown","e779220a":"markdown","030d7fbf":"markdown","fde0f4ca":"markdown","6df01547":"markdown","066b8773":"markdown","8cc9ae9e":"markdown","7d817a62":"markdown","4f15ebcd":"markdown","5c2c15ac":"markdown","8f4e77d9":"markdown","d47ca693":"markdown","f116ef95":"markdown","7fb77f11":"markdown","d209641a":"markdown","4d92e7ed":"markdown"},"source":{"ba8ea035":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n","9b7ec11c":"main_df=pd.read_csv('\/kaggle\/input\/tool-wear-detection-in-cnc-mill\/train.csv')\nmain_df=main_df.fillna('no')\nmain_df.head()","d108702b":"import glob\n#set working directory\nos.chdir('\/kaggle\/input')","11da3968":"files = list()\n\nfor i in range(1,19):\n    exp_number = '0' + str(i) if i < 10 else str(i)\n    file = pd.read_csv(\"\/kaggle\/input\/tool-wear-detection-in-cnc-mill\/experiment_{}.csv\".format(exp_number))\n    row = main_df[main_df['No'] == i]\n    \n     #add experiment settings to features\n    file['feedrate']=row.iloc[0]['feedrate']\n    file['clamp_pressure']=row.iloc[0]['clamp_pressure']\n    \n    # Having label as 'tool_conidtion'\n    \n    file['label'] = 1 if row.iloc[0]['tool_condition'] == 'worn' else 0\n    files.append(file)\ndf = pd.concat(files, ignore_index = True)\ndf.head()","4e9a61dd":"df.shape","b5796ee1":"df.dtypes","95414317":"# Convert 'Machining_process' into numerical values\npro={'Layer 1 Up':1,'Repositioning':2,'Layer 2 Up':3,'Layer 2 Up':4,'Layer 1 Down':5,'End':6,'Layer 2 Down':7,'Layer 3 Down':8,'Prep':9,'end':10,'Starting':11}\n\ndata=[df]\n\nfor dataset in data:\n    dataset['Machining_Process']=dataset['Machining_Process'].map(pro)","990b4cc4":"df=df.drop(['Z1_CurrentFeedback','Z1_DCBusVoltage','Z1_OutputCurrent','Z1_OutputVoltage','S1_SystemInertia'],axis=1)","369a09ce":"corm=df.corr()\ncorm","efefc643":"#checking the relationship between the variables by applying the correlation \nplt.figure(figsize=(30, 25))\np = sns.heatmap(df.corr(), annot=True)","2fb58c80":"X=df.drop(['label','Machining_Process'],axis=1)\nY=df['label']\nprint('The dimension of X table is: ',X.shape,'\\n')\nprint('The dimension of Y table is: ', Y.shape)","f62c23c9":"from sklearn.model_selection import train_test_split\n\n#divided into testing and training\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)","398968b6":"from sklearn import linear_model\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","386642de":"sgd_model=SGDClassifier()\nsgd_model.fit(x_train,y_train)","286065c6":"sgd_model_pred=sgd_model.predict(x_test)\nacc_sgd_model=round(sgd_model.score(x_train, y_train)*100,2)\nacc_sgd_model","d9983463":"rmf_model=RandomForestClassifier()\nrmf_model.fit(x_train,y_train)","7bfbe3b4":"rmf_model_pred=rmf_model.predict(x_test)\nacc_rmf_model=round(rmf_model.score(x_train, y_train)*100,2)\nacc_rmf_model","5fa69729":"log_reg=LogisticRegression()\nlog_reg.fit(x_train,y_train)","7edda71e":"log_reg_pred=log_reg.predict(x_test)\nacc_log_reg=round(log_reg.score(x_train,y_train)*100,2)\nacc_log_reg","9057bbdf":"knb_model=KNeighborsClassifier()\nknb_model.fit(x_train,y_train)","ad8e17e2":"knb_model_pred=knb_model.predict(x_test)\nacc_knb_model=round(knb_model.score(x_train,y_train)*100,2)\nacc_knb_model","58a5763d":"svm_model=LinearSVC()\nsvm_model.fit(x_train,y_train)","0616f8b3":"svm_model_pred=svm_model.predict(x_test)\nacc_svm_model=round(svm_model.score(x_train,y_train)*100,2)\nacc_svm_model","fd9b5de0":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest','Stochastic Gradient Decent'],\n    'Score': [acc_svm_model, acc_knb_model, acc_log_reg, \n              acc_rmf_model,acc_sgd_model]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df","c979ba69":"from sklearn.model_selection import cross_val_score\nrmf_model = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rmf_model, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores,'\\n')\nprint(\"Mean:\", scores.mean(),'\\n')\nprint(\"Standard Deviation:\", scores.std())","fd0bc4cd":"rmf_model = RandomForestClassifier(n_estimators=100, oob_score = True)\nrmf_model.fit(x_train, y_train)\ny_prediction = rmf_model.predict(x_test)\n\nrmf_model.score(x_train, y_train)\n\nacc_rmf_model = round(rmf_model.score(x_train, y_train) * 100, 2)\nprint(round(acc_rmf_model,2,), \"%\")","44a9473b":"print(\"oob score:\", round(rmf_model.oob_score_, 4)*100, \"%\")","9b0533a9":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\nfrom sklearn.model_selection import cross_val_predict\n\npredictions = cross_val_predict(rmf_model, x_train, y_train, cv=3)\npredictions[:10] # first 10 predictions","698f8b5c":"confusion_matrix(y_train,predictions)","e2660e98":"print(\"Precision_score: \", precision_score(y_train,predictions),'\\n')\nprint(\"Recall: \", recall_score(y_train,predictions),'\\n')\nprint(\"Accruacy_score: \", accuracy_score(y_train,predictions),'\\n')\nprint(\"F_score: \", f1_score(y_train, predictions))","7f079c2d":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = rmf_model.predict_proba(x_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(y_train, y_scores)\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","ce269609":"#### 2.2.2 Random Forest:","e779220a":"### 2.4 Further Evaluation","030d7fbf":"### 2.3 Which is the best Model","fde0f4ca":"##### Target variables are:\n    1. Tool wear detection\n    2. Detection of inadequate clamping- \"Passed visual inspection\"\n    3. Machining finalised","6df01547":"#### Precision Recall Curve","066b8773":"Evaluate Random Forest using the out-of-bag samples to estimate the generalization accuracy. I will not go into details here about how it works. Just note that out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set","8cc9ae9e":"#### 2.2.5 Linear Support Vector Machine","7d817a62":"#### 2.2.4 K Nearest Neighbor","4f15ebcd":"### Creating the data frame ","5c2c15ac":"> ## 1. Load Data","8f4e77d9":"### 2.1 Train\/Test Split","d47ca693":"## 2. Building ML Model","f116ef95":"#### 2.2.1 Stochastic Gradient Descent (SGD):\n","7fb77f11":"#### 2.2.3 Logistic Regression","d209641a":"#### Confusion Matrix","4d92e7ed":"As we can see, the Random Forest classifier goes on the first place. But first, let us check, how random-forest performs, when we use cross validation."}}