{"cell_type":{"8a9ce94c":"code","d5bb5fde":"code","7bdca3cd":"code","54e0b6d5":"code","074407f8":"code","a45d6eb8":"code","08a8ab44":"code","461d0739":"code","891e3e36":"code","62e4d764":"code","207d08e6":"code","c97b14d9":"code","cf7edbda":"code","a42789b5":"code","376b435a":"code","4cc7e989":"code","04df1c39":"code","6ced41fb":"code","abebd34a":"code","6dadc255":"code","b3a6ef78":"markdown","482e522d":"markdown","e614de16":"markdown","8503c92a":"markdown","f4a736f8":"markdown","aac8470e":"markdown","d8b1dacc":"markdown","7b85a889":"markdown","259eb5f0":"markdown","7fdf8e96":"markdown","33b5c2bf":"markdown","56dded40":"markdown","2eee3fed":"markdown","40cd887d":"markdown","9e28f8b7":"markdown","c2f63fca":"markdown","b30061ea":"markdown"},"source":{"8a9ce94c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input,Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom sklearn.model_selection import StratifiedKFold,KFold\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os","d5bb5fde":"path=\"..\/input\/contradictory-my-dear-watson\"\nos.listdir(path)\n","7bdca3cd":"df_train=pd.read_csv(os.path.join(path,\"train.csv\"))\ndf_test=pd.read_csv(os.path.join(path,\"test.csv\"))","54e0b6d5":"print('there are {} rows and {} columns in the train'.format(df_train.shape[0],df_train.shape[1]))\nprint('there are {} rows and {} columns in the test'.format(df_test.shape[0],df_test.shape[1]))","074407f8":"df_train.head(3)","a45d6eb8":"\n\nlangs = df_train.language.unique()\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=langs,\n    y=df_train.language.value_counts().values,\n    name='train',\n    marker_color='indianred'\n))\nfig.add_trace(go.Bar(\n    x=langs,\n    y=df_test.language.value_counts().values,\n    name='test',\n    marker_color='lightsalmon'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title=\"language distribution in dataset\")\nfig.show()","08a8ab44":"\n\nlangs = df_train.label.unique()\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    x=langs,\n    y=df_train.label.value_counts().values,\n    name='test',\n    marker_color=[ 'steelblue', 'tan', 'teal']\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(xaxis_tickangle=-45,title=\"Target distribution in train dataset\")\nfig.show()","461d0739":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","891e3e36":"MODEL = 'jplu\/tf-xlm-roberta-large'\nEPOCHS = 10\nMAX_LEN = 96\n\n# Our batch size will depend on number of replic\nBATCH_SIZE= 16 * strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","62e4d764":"def quick_encode(df,maxlen=100):\n    \n    values = df[['premise','hypothesis']].values.tolist()\n    tokens=tokenizer.batch_encode_plus(values,max_length=maxlen,pad_to_max_length=True)\n    \n    return np.array(tokens['input_ids'])\n\nx_train = quick_encode(df_train)\nx_test = quick_encode(df_test)\ny_train = df_train.label.values\n    ","207d08e6":"\n\ndef create_dist_dataset(X, y,val,batch_size= BATCH_SIZE):\n    \n    \n    dataset = tf.data.Dataset.from_tensor_slices((X,y)).shuffle(len(X))\n          \n    if not val:\n        dataset = dataset.repeat().batch(batch_size).prefetch(AUTO)\n    else:\n        dataset = dataset.batch(batch_size).prefetch(AUTO)\n\n    \n    \n    return dataset\n\n\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_test))\n    .batch(BATCH_SIZE)\n)\n","c97b14d9":"def build_model(transformer,max_len):\n    \n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    sequence_output = transformer(input_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    cls_token = Dropout(0.2)(cls_token)\n    cls_token = Dense(32,activation='relu')(cls_token)\n    out = Dense(3, activation='softmax')(cls_token)\n\n    # It's time to build and compile the model\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=1e-5), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n    \n    return model\n","cf7edbda":"def build_lrfn(lr_start=0.00001, lr_max=0.00003, \n               lr_min=0.000001, lr_rampup_epochs=3, \n               lr_sustain_epochs=0, lr_exp_decay=.6):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn","a42789b5":"\nplt.figure(figsize=(10, 7))\n\n_lrfn = build_lrfn()\nplt.plot([i for i in range(10)], [_lrfn(i) for i in range(10)]);","376b435a":"lrfn = build_lrfn()\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)","4cc7e989":"skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\nval_score=[]\nhistory=[]\n\n\nfor fold,(train_ind,valid_ind) in enumerate(skf.split(x_train,y_train)):\n    \n    if fold < 4:\n    \n        print(\"fold\",fold+1)\n        \n       \n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        \n        train_data = create_dist_dataset(x_train[train_ind],y_train[train_ind],val=False)\n        valid_data = create_dist_dataset(x_train[valid_ind],y_train[valid_ind],val=True)\n    \n        Checkpoint=tf.keras.callbacks.ModelCheckpoint(f\"roberta_base.h5\", monitor='val_loss', verbose=0, save_best_only=True,\n        save_weights_only=True, mode='min')\n        \n        with strategy.scope():\n            transformer_layer = TFAutoModel.from_pretrained(MODEL)\n            model = build_model(transformer_layer, max_len=MAX_LEN)\n            \n        \n\n        n_steps = len(train_ind)\/\/BATCH_SIZE\n        print(\"training model {} \".format(fold+1))\n\n        train_history = model.fit(\n        train_data,\n        steps_per_epoch=n_steps,\n        validation_data=valid_data,\n        epochs=EPOCHS,callbacks=[Checkpoint],verbose=1)\n        \n        print(\"Loading model...\")\n        model.load_weights(f\"roberta_base.h5\")\n        \n        \n\n        print(\"fold {} validation accuracy {}\".format(fold+1,np.mean(train_history.history['val_accuracy'])))\n        print(\"fold {} validation loss {}\".format(fold+1,np.mean(train_history.history['val_loss'])))\n        \n        val_score.append(train_history.history['val_accuracy'])\n        history.append(train_history)\n\n        val_score.append(np.mean(train_history.history['val_accuracy']))\n        \n        print('predict on test....')\n        preds=model.predict(test_dataset,verbose=1)\n        \n        pred_test+=preds\/4\n        \n\n        \nprint(\"Mean Validation accuracy : \",np.mean(val_score))","04df1c39":"\nplt.figure(figsize=(15,10))\n\nfor i,hist in enumerate(history):\n\n    plt.subplot(2,2,i+1)\n    plt.plot(np.arange(EPOCHS),hist.history['accuracy'],label='train accu')\n    plt.plot(np.arange(EPOCHS),hist.history['val_accuracy'],label='validation acc')\n    plt.gca().title.set_text(f'Fold {i+1} accuracy curve')\n    plt.legend()\n\n\n    ","6ced41fb":"\nplt.figure(figsize=(15,10))\n\nfor i,hist in enumerate(history):\n\n    plt.subplot(2,2,i+1)\n    plt.plot(np.arange(EPOCHS),hist.history['loss'],label='train loss')\n    plt.plot(np.arange(EPOCHS),hist.history['val_loss'],label='validation loss')\n    plt.gca().title.set_text(f'Fold {i+1} loss curve')\n    plt.legend()\n\n","abebd34a":"submission = pd.read_csv(os.path.join(path,'sample_submission.csv'))\nsubmission['prediction'] = np.argmax(pred_test,axis=1)\nsubmission.head()","6dadc255":"submission.to_csv('submission.csv',index=False)","b3a6ef78":"## <font size='4' color='blue'>Getting Basic Idea<\/font>","482e522d":"## <font size='4' color='blue'>LR Scheduler<\/font>\nsource :https:\/\/www.kaggle.com\/miklgr500\/jigsaw-tpu-bert-with-huggingface-and-keras","e614de16":"### <font size='3' color='blue'>Language distribution<font>","8503c92a":"## <font size='4' color='blue'>Submission<\/font>","f4a736f8":"## <font size='4' color='blue'>Fast Encoder<\/font>","aac8470e":"## <font size='4' color='blue'>Dataset <\/font>","d8b1dacc":"## <font size='4' color='blue'>Kfold CV<\/font>","7b85a889":"## <font size='4' color='blue'>Evaluation<\/font>","259eb5f0":"- The distribution of targets seems to be almost equal.","7fdf8e96":"## <font size='4' color='blue'>Import Important packages<\/font>","33b5c2bf":"## <font size='4' color='blue'>Model<\/font>","56dded40":"## <font size='5' color='red'>Introduction<\/font>","2eee3fed":"Natural language processing (NLP) has grown increasingly elaborate over the past few years. Machine learning models tackle question answering, text extraction, sentence generation, and many other complex tasks. But, can machines determine the relationships between sentences, or is that still left to humans? If NLP can be applied between sentences, this could have profound implications for fact-checking, identifying fake news, analyzing text, and much more. \n\n![](https:\/\/media.giphy.com\/media\/ZkwSxuckDvf7q\/giphy.gif)\n\n\n- In this notebook,I show how to do KFold Cross validation on TPU with XLM Roberta.I will further evaluate and tune the model in the upcoming updates.\n","40cd887d":"## <font size='4' color='blue'>TPU Config<\/font>","9e28f8b7":"### <font size='3' color='blue'>Class distribution<\/font>\n","c2f63fca":"## <font size='4' color='red'>Work in Progress! Please upvote if you think this was helpful.<\/font>","b30061ea":"- The majoity of both the train and test set is in English.\n- All other language samples are under 100 per language."}}