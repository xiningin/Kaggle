{"cell_type":{"5866dc55":"code","ec618c0e":"code","2a06d099":"code","87c530db":"code","6759bf95":"code","3c5547c2":"code","788bf372":"code","ebed458c":"code","b7aaa671":"code","44326f7f":"code","1fc1b1cd":"code","6530ad5c":"code","d2f2c2b5":"code","fc01af4b":"code","68688a64":"code","5787fc4a":"code","c51fafde":"code","074a22d0":"code","cc193cff":"code","df3b842c":"code","45dcb3a6":"code","6e021ac5":"code","1cab469d":"code","11e08cee":"code","9322472c":"code","af3daff0":"code","db8efc86":"code","43e979fb":"code","f76feb41":"code","b85611f9":"code","ea121ea9":"code","7a95d78c":"markdown"},"source":{"5866dc55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sbn\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))\nimport pickle\n# Any results you write to the current directory are saved as output.","ec618c0e":"import pandas as pd\nimport numpy as np\nimport glob\nimport theano.tensor as T\nimport theano\nfrom sklearn.metrics import mean_squared_error","2a06d099":"print (\"Helpful guide: https:\/\/github.com\/parsing-science\/pymc3_quickstart_guide\")\ndf=pd.concat([pd.read_csv(f) for f in glob.glob('..\/input\/*.csv')], ignore_index = True)\ndf.count()[0]","87c530db":"\"\"\"\nHere are a few takeaways from the statistics of our dataset:\n\nGender Breakdown: 19% Female, 67% Male\nSubscribers: 79% Subscribers\n\nThere are a number of missing values in a variety of columns in the dataset.\nThe duration of the rides generally look to be for less than an hour, but \nsome values are anomalously high.\n\n\n\"\"\"\n\ndf=df.dropna()\ndf.head()","6759bf95":"df[\"duration_hrs\"]=df[\"duration_sec\"]\/3600.\ndf[\"age\"]=2019-df[\"member_birth_year\"]\ndf[\"start_day\"]=pd.to_datetime(df[\"start_time\"], errors='ignore')\ndf[\"start_day\"]= df['start_day'].dt.floor(\"d\")","3c5547c2":"df=pd.get_dummies(columns=[\"member_gender\",\"user_type\"],data=df)\ndf.head()","788bf372":"aggregations = {\n    'duration_hrs':'mean',\n    \"age\" :\"mean\",\n    \"member_gender_Female\":\"sum\",\n    \"member_gender_Male\":\"sum\",\n    \"member_gender_Other\":\"sum\",\n    \"user_type_Customer\":\"sum\",\n    \"user_type_Subscriber\":\"sum\",  \n}\nday=df.groupby(\"start_day\").agg(aggregations)\nday.head()","ebed458c":"day[\"total_riders\"]=day[\"user_type_Customer\"]+day[\"user_type_Subscriber\"]","b7aaa671":"print (len(day[\"total_riders\"]))\nnextDay=list(day[\"total_riders\"])\nnextDay.pop(0)#Don't need this value anymore\nnextDay.append(0.0)#Add a zero to the next one as a test\nprint (len(nextDay))\n#print (nextDay)","44326f7f":"from sklearn.preprocessing import RobustScaler\nscaledDF = RobustScaler().fit_transform(day)\nscaledDF = pd.DataFrame(data=scaledDF, columns = [\"scaled_\"+str(x) for x in day.columns])","1fc1b1cd":"scaledDF[\"nextDay\"]=nextDay\nscaledDF=scaledDF[:len(nextDay)-1]\nscaledDF.head(5)","6530ad5c":"\"\"\"\nFrom the heatmap, we can see some simple relationships between variabes, \nboth positive and negative with respect to our target variable (nextDay).\n\nHowever, this only tells part of the story. We know that the variables \nare inter-related, and to corresponding to some magnitude of correlation,\nbut there could be deeper relationships as well.\n\"\"\"\ncorr = scaledDF.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","d2f2c2b5":"\"\"\"\nIf we plot the distributions of these variables, we see \nsignificant co-linearity among some variables with respect \nto the next day predictions. Predictably, categorical features\nthat exist in the majority (male ridership and subscription riders)\nhave strong relationships.\n\nOur target value (next day's ridership) appears to be a binomial distribution. Many\nof the features, when scaled, also share this type of distribution. The scatter plots\nshow that there exist multiple linear relationships for some of the features. These could\ncorrespond to weekend\/weekday differences.\n\"\"\"\ng = sbn.pairplot(scaledDF);","fc01af4b":"print (\"Let us try some baseline predictions: Naive Average of all ridership\")\n\nnaivePreds=np.ones(len(scaledDF['nextDay']))+np.mean(day['total_riders'])\nnp.sqrt(mean_squared_error(scaledDF[\"nextDay\"], naivePreds))","68688a64":"print (\"What if we simply look back one day and see if that is a good predictor?\")\nnp.sqrt(mean_squared_error(scaledDF[\"nextDay\"], day[\"total_riders\"][:len(scaledDF[\"nextDay\"])]))","5787fc4a":"import theano.tensor as T\n\ny = scaledDF[\"nextDay\"]\nX = scaledDF.drop('nextDay',axis=1)\n\n#Let's test our model on the last month of data\nmonth_split = len(y)-30\nX_train , Y_train =  X[:month_split], y[:month_split]\nX_test , Y_test = X[month_split:], y[month_split:]\n\n#We initiate a shared theano array for training and\n#    testing splits for ease of use.\nmodel_input = theano.shared(np.array(X_train))\nmodel_output = theano.shared(np.array(Y_train))","c51fafde":"\"\"\"\nWhat would a general sklearn Linear Regression pick for our parameters,\nand how would it score? We can use this as our ML baseline without \nany hyperparameter tuning.\n\"\"\"\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train,Y_train)\npreds = lr.predict(X_test)\nnp.sqrt(mean_squared_error(Y_test, preds))","074a22d0":"import pymc3 as pm\n\nprint('Running on PyMC3 v{}'.format(pm.__version__))\n\nbig_model = pm.Model()\n\nwith big_model:\n    \"\"\"\n    Our simple, linear model requires an intercept (alpha) and a weight\n    for each of our parameters (beta). I have no prior knowledge about what\n    the weights could be, but I know the features are scaled by the \n    RobustScaler. Therefore, we can assume they might be of centered around\n    0 (mu). In this example, we did not choose to scale our target value \n    (riders the next day), so the slope of our scaled parameters might be \n    quite large. Therefore, we set sd=100. If we set it too low, it may \n    never find the correct beta value for that parameter.\n    \"\"\"\n    alpha = pm.Normal('alpha', mu=0, sd=100)\n    beta = pm.Normal('beta', mu=0, sd=100, shape=8)\n\n    \"\"\"\n    The values we are trying to predict will be a simple dot product of our \n    features with the weights of our model (transpose of beta) plus our \n    intercept. We take the exponential of this value because we will \n    be modelling our output as a Poisson distribution. If we modelled it\n    as a Gaussian, we would remove the np.exp and introduce a value \n    for the noise (sigma).\n    \"\"\" \n    values = np.exp(alpha + T.dot(model_input, beta.T) )\n    \n    \"\"\"\n    This is the final output of our model. Our target variable (model_output)\n    is being modelled as a Poisson, as we are dealing with a simple counting\n    statistic (the number of riders the next day). We could reasonably choose\n    a different distribution in PyMC3 (Normal, DiscreteUniform, et al.) but \n    this seems intuitive.\n    \"\"\"\n    Y_obs = pm.Poisson('Y_obs', mu=values, observed=model_output)","cc193cff":"\"\"\"\nNow that we have specified our model, we need to \nbegin estimating our model parameters. We could specify \na sampler (Metropilis, NUTS, etc.) or we could choose\n.sample(), which chooses a sampler suited to our data. \nGenerally it chooses NUTS.\n\"\"\"\nwith big_model:\n    start = pm.find_MAP() # Use the MAP estimate as a starting value\n    nuts_trace = pm.sample(8000, scaling=start) # Begin to build our trace","df3b842c":"\"\"\"\nBy plotting the trace of our model, we can examine \nthe distribution of the parameters as well as how well\nthey are converged. \n\nGenerally speaking, if we see a parameter that has a peaked \nposterior distribution, the model is fairly sure of itself. Likewise,\nif we look at the evolution of the parameter (right-side of the plots)\nand see that it is fairly flat, the sampler has had no reason to deviate\nfrom what it thinks is correct.\n\nNotably, this model is not all that well constrained. We see on the left side \nthat some beta parameters (the weights) have wide distributions, and the corresponding\nevolution of these values continues to change on the right side. \n\nIf we remember back to the pairplot above, we saw what looked to be multiple linear \nrelationships in the data. It is certainly plausible that the sampler cannot converge \nin these types of situations with our simple linear model.\n\"\"\"\npm.traceplot(nuts_trace[-1000:]);","45dcb3a6":"pm.summary(nuts_trace[-1000:])","6e021ac5":"\"\"\"\nWe can pickle our traces as a binary model, and then \nre-use them later if we want.\n\"\"\"\nfileObject = open(\"nuts_trace.pickle\",'wb')  \npickle.dump(nuts_trace, fileObject)\nfileObject.close()","1cab469d":"\"\"\"\nWhat if we have a large, complicated model that we can only\napproximate the posterior distribution?  Use Variational Inference:\nhttps:\/\/docs.pymc.io\/api\/inference.html?highlight=advi#pymc3.variational.inference.ADVI\n\nIt works extremely quickly compared to NUTS, and can offer increased performance in \nestimating the model parameters for very complex distributions. Simple distributions \nwill be better fit by NUTS.\n\"\"\"\nwith big_model:\n    inference = pm.ADVI()\n    approx = pm.fit(n=100000, method=inference)","11e08cee":"#Now we sample from our approximation in order to get a similar trace\nadvi_trace = approx.sample(10000)","9322472c":"pm.traceplot(advi_trace[-1000:]);","af3daff0":"pm.summary(advi_trace[-1000:])","db8efc86":"#Save the advi model as well\nfileObject = open(\"advi_trace.pickle\",'wb')  \npickle.dump(advi_trace, fileObject)\nfileObject.close()","43e979fb":"\"\"\"\nLet's get the RMSE of our model as understood by the NUTS and ADVI\nsampler. We will test our training data error first.\n\nThe NUTS sampler does a pretty decent job on predictions, and has a \nbetter RMSE than our sklearn LR model.\n\"\"\"\n\ndef scoreModel(trace,y,model_name):\n    ppc = pm.sample_ppc(trace[1000:], model=model_name, samples=1000)\n    pred = ppc['Y_obs'].mean(axis=0)\n    return np.sqrt(mean_squared_error(y, pred))\n\nscoreModel(nuts_trace,Y_train,big_model)","f76feb41":"\"\"\"\nOur ADVI predictions are very consistent with our NUTS sampler, \nand it took a lot less time to train. This may not always be the case \nwith all models!\n\"\"\"\nscoreModel(advi_trace,Y_train,big_model)","b85611f9":"\"\"\"\nNow we simply switch out our Theano tensor with our \ntesting data. Using the shared value, we do not have\nto respecify our model first.\n\"\"\"\nmodel_input.set_value(np.array(X_test))\nmodel_output.set_value(np.array(Y_test))\n\nscoreModel(nuts_trace,Y_test,big_model)","ea121ea9":"\"\"\"\nOnce again, our PyMC3 models do better than our sklearn \nmodel, and we did not do any hyperparameter tuning for either.\n\nIs the increased difficulty and time worth the better performance? \nThat is for you to decide.\n\"\"\"\nscoreModel(advi_trace,Y_test,big_model)","7a95d78c":"One important thing to note here is that we do not have a single set of weights for our model, but a distribution of weights. From this we can make informed decisions about any predictions we have, building confidence regions around each predicted point. Although sklearn is much easier in building a model, PyMC3 offers much greater control on how the model is built and trained."}}