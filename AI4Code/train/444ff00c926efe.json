{"cell_type":{"9cb165a3":"code","464c3b37":"code","27bcb9c1":"code","eba0d2cc":"code","f7127cab":"code","ce45b89b":"code","53bf6806":"code","5f6dcd3e":"code","d5125374":"code","e90d00de":"code","f6e98293":"code","58dd9812":"code","090512ea":"code","c2e5e34c":"code","b1dd4d92":"code","78701b81":"code","39887c1e":"code","5d807513":"code","a4f52d81":"code","21c90eca":"code","c9c71eaa":"code","140869fe":"code","06f5f4ce":"code","64922606":"code","67522ed5":"code","3ff8901f":"code","ab0272e0":"code","98abe59f":"code","b6c462fb":"markdown","b36f3aea":"markdown","b373096d":"markdown","d79420fa":"markdown","f2a541ac":"markdown","2b0bf5e2":"markdown","d8eb2c1b":"markdown","4322d4d6":"markdown","6ebca7fa":"markdown","924a956e":"markdown","ecb29c79":"markdown","d5c6bfd7":"markdown","da20931d":"markdown"},"source":{"9cb165a3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nplt.style.use('ggplot')\nimport warnings\nwarnings.filterwarnings('ignore')","464c3b37":"train = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/train.parquet')\ntest = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/test.parquet')\nss = pd.read_csv('..\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')\n\nnn_train = pd.read_csv('..\/input\/fork-of-word-embeddings-and-nns\/training_submit_df.csv')\nnn_test = pd.read_csv('..\/input\/fork-of-word-embeddings-and-nns\/submission.csv')","27bcb9c1":"cfg = {\n    'TARGET' : 'target',\n    'N_FOLDS' : 5,\n    'RANDOM_STATE': 529,\n    'LEARNING_RATE': 0.1\n}\n\ntrain_vids = train['video_id'].unique()","eba0d2cc":"kf = KFold(n_splits=cfg['N_FOLDS'],\n           shuffle=True,\n           random_state=cfg['RANDOM_STATE'])\n\n# Create Folds\nfold = 1\nfor tr_idx, val_idx in kf.split(train_vids):\n    fold_vids = train_vids[val_idx]\n    train.loc[train['video_id'].isin(fold_vids), 'fold'] = fold\n    fold += 1\ntrain['fold'] = train['fold'].astype('int')","f7127cab":"def create_features(df, train=True):\n    \"\"\"\n    Adds features to training or test set.\n    \"\"\"\n    df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n    df['trending_date'] = pd.to_datetime(df['trending_date'], utc=True)\n    \n    # Feature 1 - Age of video\n    df['video_age_seconds'] = (df['trending_date'] - df['publishedAt']) \\\n        .dt.total_seconds().astype('int')\n    \n    # Trending day of week As a category\n    df['trending_dow'] = df['trending_date'].dt.day_name()\n    df['trending_dow']= df['trending_dow'].astype('category')\n    \n    df['published_dow'] = df['publishedAt'].dt.day_name()\n    df['published_dow']= df['published_dow'].astype('category')\n    \n    \n    \n    df['channel_occurance'] = df['channelId'].map(\n        df['channelId'].value_counts().to_dict())\n\n    df['channel_unique_video_count'] = df['channelId'].map(\n        df.groupby('channelId')['video_id'].nunique().to_dict())\n    \n    df['video_occurance_count'] = df.groupby('video_id')['trending_date'] \\\n        .rank().astype('int')\n    \n    df['Total_Trending_Days']=df['video_id'].map(df.groupby('video_id')['id'].count().to_dict())\n    #should be calculated across train and test\n    df['min_trending_dt']=df['video_id'].map(df.groupby('video_id')['trending_date'].min().to_dict())\n    df['days_since_first_trend']=(df['trending_date']-df['min_trending_dt']).dt.days\n    \n    #each video's initial Ratio - should be calculated across train and test\n    \n    \n    \n    #Video_initial_ratio     \n    df['video_initial_ratio']=df['video_id'].map(df[df['min_trending_dt']==df['trending_date']][['video_id','target']].set_index('video_id').squeeze(axis=1).to_dict())\n    \n    df['trending_dt_year']=df['trending_date'].dt.year\n    df['trending_dt_year']= df['trending_dt_year'].astype('category')\n\n    df['trending_dt_month']=df['trending_date'].dt.month\n    df['trending_dt_month']= df['trending_dt_month'].astype('category')\n\n#     df['trending_dow'] = df['trending_date'].dt.day_name()\n#     df['trending_dow']= df['trending_dow'].astype('category')\n    \n    df['publishedAt_year']=df['publishedAt'].dt.year\n    df['publishedAt_year']= df['publishedAt_year'].astype('category')\n\n    df['publishedAt_month']=df['publishedAt'].dt.month\n    df['publishedAt_month']= df['publishedAt_month'].astype('category')\n    \n    df['trending_dt_year_mean_VIR']=df['trending_dt_year'].map(df.groupby('trending_dt_year')['video_initial_ratio'].mean().to_dict())\n    df['trending_dt_year_mean_target']=df['trending_dt_year'].map(df.groupby('trending_dt_year')['target'].mean().to_dict())\n\n\n    df['trending_dt_month_mean_VIR']=df['trending_dt_month'].map(df.groupby('trending_dt_month')['video_initial_ratio'].mean().to_dict())\n    df['trending_dt_month_mean_target']=df['trending_dt_month'].map(df.groupby('trending_dt_month')['target'].mean().to_dict())\n\n    df['trending_dt_dow_mean_VIR']=df['trending_dow'].map(df.groupby('trending_dow')['video_initial_ratio'].mean().to_dict())\n    df['trending_dt_dow_mean_target']=df['trending_dow'].map(df.groupby('trending_dow')['target'].mean().to_dict())\n    \n#     df['categoryId'] = df['categoryId'].astype('category')\n#     df['channelId'] = df['channelId'].astype('category')\n#     df['video_id'] = df['video_id'].astype('category')\n    \n    return df","ce45b89b":"train['isTrain'] = True\ntest['isTrain'] = False\ntt = pd.concat([train, test]).reset_index(drop=True).copy()\ntt = create_features(tt)\ntrain_feats = tt.query('isTrain').reset_index(drop=True).copy()\ntest_feats = tt.query('isTrain == False').reset_index(drop=True).copy()","53bf6806":"nn_test.columns=['id','nn_VIR']\nnn_train.columns=['video_id','a','b','nn_VIR']","5f6dcd3e":"test_feats=test_feats.merge(nn_test, on='id', how='left')\n#test_feats.head(4)\ntrain_feats=train_feats.merge(nn_train, on='video_id', how='left')\n#train_feats.head(4)","d5125374":"# FEATURES = ['video_age_seconds',\n#             'trending_dow',\n#             'published_dow',\n#             'duration_seconds',\n#             'categoryId',\n#             'comments_disabled',\n#             'ratings_disabled',\n#             'channel_occurance',\n#             'channel_unique_video_count',\n#             'video_occurance_count',\n#             'nn_VIR'\n# ]\n\n# TARGET = ['target']","e90d00de":"FEATURES = ['video_age_seconds',\n            'trending_dow',\n            #'channelId',\n            #'video_id',\n            'published_dow',\n            'duration_seconds',\n            #'categoryId',\n            'comments_disabled',\n            'ratings_disabled',\n            'channel_occurance',\n            'channel_unique_video_count',\n            'video_occurance_count',\n            'Total_Trending_Days',\n            'days_since_first_trend',\n            #'video_initial_ratio',\n            #'channel_mean_initial_ratio',\n            #'categoryId_mean_initial_ratio',\n#             'channel_trend_day_1_mean_ratio',\n# 'channel_trend_day_2_mean_ratio',\n# 'channel_trend_day_3_mean_ratio',\n# 'channel_trend_day_4_mean_ratio',\n# 'channel_trend_day_5_mean_ratio',\n# 'channel_trend_day_6_mean_ratio',\n# 'channel_trend_day_7_mean_ratio',\n#             'category_trend_day_1_mean_ratio',\n# 'category_trend_day_2_mean_ratio',\n# 'category_trend_day_3_mean_ratio',\n# 'category_trend_day_4_mean_ratio',\n# 'category_trend_day_5_mean_ratio',\n# 'category_trend_day_6_mean_ratio',\n# 'category_trend_day_7_mean_ratio',\n'trending_dt_year',\n'trending_dt_month',\n'publishedAt_year',\n'publishedAt_month',\n'trending_dt_year_mean_VIR',\n'trending_dt_year_mean_target',\n'trending_dt_month_mean_VIR',\n'trending_dt_month_mean_target',\n'trending_dt_dow_mean_VIR',\n'trending_dt_dow_mean_target',\n'nn_VIR'\n#             'video_1_days_since_trend_ratio',\n# 'video_2_days_since_trend_ratio',\n# 'video_3_days_since_trend_ratio',\n# 'video_4_days_since_trend_ratio',\n# 'video_5_days_since_trend_ratio',\n# 'video_6_days_since_trend_ratio',\n# 'video_7_days_since_trend_ratio'\n]\n\nTARGET = ['target']","f6e98293":"X_test = test_feats[FEATURES]\noof = train_feats[['id','target','fold']].reset_index(drop=True).copy()\nsubmission_df = test[['id']].copy()","58dd9812":"regs = []\nfis = []\n# Example Fold 1\nfor fold in range(1, 6):\n    print(f'===== Running for fold {fold} =====')\n    # Split train \/ val\n    X_tr = train_feats.query('fold != @fold')[FEATURES]\n    y_tr = train_feats.query('fold != @fold')[TARGET]\n    X_val = train_feats.query('fold == @fold')[FEATURES]\n    y_val = train_feats.query('fold == @fold')[TARGET]\n    print(X_tr.shape, y_tr.shape, X_val.shape, y_val.shape)\n\n    # Create our model\n    reg = lgb.LGBMRegressor(n_estimators=2800,\n                            learning_rate=cfg['LEARNING_RATE'],\n                            objective='mae',\n                            metric=['mae'],\n                            importance_type='gain'\n                           )\n    # Fit our model\n    reg.fit(X_tr, y_tr,\n            eval_set=(X_val, y_val),\n            verbose=200,\n           )\n\n    # Predicting on validation set\n    fold_preds = reg.predict(X_val)\n    oof.loc[oof['fold'] == fold, 'preds'] = fold_preds\n    # Score validation set\n    fold_score = mean_absolute_error(\n        oof.query('fold == 1')['target'],\n            oof.query('fold == 1')['preds']\n    )\n\n    # Creating a feature importance dataframe\n    fi = pd.DataFrame(index=reg.feature_name_,\n                 data=reg.feature_importances_,\n                 columns=[f'{fold}_importance'])\n\n    # Predicting on test\n    fold_test_pred = reg.predict(X_test,\n                num_iteration=reg.best_iteration_)\n    submission_df[f'pred_{fold}'] = fold_test_pred\n    print(f'Score of this fold is {fold_score:0.6f}')\n    regs.append(reg)\n    fis.append(fi)","090512ea":"oof_score = mean_absolute_error(oof['target'], oof['preds'])\nprint(f'Out of fold score {oof_score:0.6f}')","c2e5e34c":"fis_df = pd.concat(fis, axis=1)\nfis_df.sort_values('1_importance').plot(kind='barh', figsize=(12, 8),\n                                       title='Feature Importance Across Folds')\nplt.show()","b1dd4d92":"pred_cols = [c for c in submission_df.columns if c.startswith('pred_')]\nsubmission_df['target_LGBM'] = submission_df[pred_cols].mean(axis=1)\noof['preds_LGBM'] = oof['preds']","78701b81":"# FEATURES = ['video_age_seconds',\n# #             'trending_dow',\n# #             'published_dow',\n#             'duration_seconds',\n#             'categoryId',\n#             'comments_disabled',\n#             'ratings_disabled',\n#             'channel_occurance',\n#             'channel_unique_video_count',\n#             'video_occurance_count',\n#             'nn_VIR'\n# ]\n# X_test = test_feats[FEATURES]","39887c1e":"FEATURES = ['video_age_seconds',\n            'trending_dow',\n            #'channelId',\n            #'video_id',\n            'published_dow',\n            'duration_seconds',\n            #'categoryId',\n            'comments_disabled',\n            'ratings_disabled',\n            'channel_occurance',\n            'channel_unique_video_count',\n            'video_occurance_count',\n            'Total_Trending_Days',\n            'days_since_first_trend',\n            #'video_initial_ratio',\n            #'channel_mean_initial_ratio',\n            #'categoryId_mean_initial_ratio',\n#             'channel_trend_day_1_mean_ratio',\n# 'channel_trend_day_2_mean_ratio',\n# 'channel_trend_day_3_mean_ratio',\n# 'channel_trend_day_4_mean_ratio',\n# 'channel_trend_day_5_mean_ratio',\n# 'channel_trend_day_6_mean_ratio',\n# 'channel_trend_day_7_mean_ratio',\n#             'category_trend_day_1_mean_ratio',\n# 'category_trend_day_2_mean_ratio',\n# 'category_trend_day_3_mean_ratio',\n# 'category_trend_day_4_mean_ratio',\n# 'category_trend_day_5_mean_ratio',\n# 'category_trend_day_6_mean_ratio',\n# 'category_trend_day_7_mean_ratio',\n'trending_dt_year',\n'trending_dt_month',\n'publishedAt_year',\n'publishedAt_month',\n'trending_dt_year_mean_VIR',\n'trending_dt_year_mean_target',\n'trending_dt_month_mean_VIR',\n'trending_dt_month_mean_target',\n'trending_dt_dow_mean_VIR',\n'trending_dt_dow_mean_target',\n'nn_VIR'\n#             'video_1_days_since_trend_ratio',\n# 'video_2_days_since_trend_ratio',\n# 'video_3_days_since_trend_ratio',\n# 'video_4_days_since_trend_ratio',\n# 'video_5_days_since_trend_ratio',\n# 'video_6_days_since_trend_ratio',\n# 'video_7_days_since_trend_ratio'\n]\n\nX_test = test_feats[FEATURES]","5d807513":"regs = []\nfis = []\n# Example Fold 1\nfor fold in range(1, 6):\n    print(f'===== Running for fold {fold} =====')\n    # Split train \/ val\n    X_tr = train_feats.query('fold != @fold')[FEATURES]\n    y_tr = train_feats.query('fold != @fold')[TARGET]\n    X_val = train_feats.query('fold == @fold')[FEATURES]\n    y_val = train_feats.query('fold == @fold')[TARGET]\n    print(X_tr.shape, y_tr.shape, X_val.shape, y_val.shape)\n\n    # Create our model\n    reg = lgb.LGBMRegressor(n_estimators=2500,\n                            learning_rate=0.1,\n                            objective='mae',\n                            metric=['mae'],\n                            importance_type='gain'\n                           )\n    # Fit our model\n    reg.fit(X_tr, y_tr,\n            eval_set=(X_val, y_val),\n            verbose=200,\n           )\n\n    # Predicting on validation set\n    fold_preds = reg.predict(X_val)\n    oof.loc[oof['fold'] == fold, 'preds'] = fold_preds\n    # Score validation set\n    fold_score = mean_absolute_error(\n        oof.query('fold == 1')['target'],\n            oof.query('fold == 1')['preds']\n    )\n\n    # Creating a feature importance dataframe\n    fi = pd.DataFrame(index=reg.feature_name_,\n                 data=reg.feature_importances_,\n                 columns=[f'{fold}_importance'])\n\n    # Predicting on test\n    fold_test_pred = reg.predict(X_test,\n                num_iteration=reg.best_iteration_)\n    submission_df[f'pred_{fold}'] = fold_test_pred\n    print(f'Score of this fold is {fold_score:0.6f}')\n    regs.append(reg)\n    fis.append(fi)","a4f52d81":"oof_score = mean_absolute_error(oof['target'], oof['preds'])\nprint(f'Out of fold score {oof_score:0.6f}')","21c90eca":"pred_cols = [c for c in submission_df.columns if c.startswith('pred_')]\n\nsubmission_df['target_LGBM2'] = submission_df[pred_cols].mean(axis=1)\noof['preds_LGBM2'] = oof['preds']\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\noof.plot(x='preds_LGBM', y='preds_LGBM2', kind='scatter',\n         title='Compare OOF predictions for two models',\n        ax=axs[0])\n\nsubmission_df.plot(x='target_LGBM', y='target_LGBM2',\n                   kind='scatter', title='Compare Test predictions for two models',\n                  ax=axs[1])\nplt.show()","c9c71eaa":"weight = 0.5\n\ndef get_oof_score(weight, oof):\n    blend_pred = (oof['preds_LGBM'] * weight) + (oof['preds_LGBM2'] * (1- weight))\n    return mean_absolute_error(oof['target'], blend_pred)","140869fe":"myscores = {}\nfor weight in range(100):\n    weight \/= 100\n    score = get_oof_score(weight, oof)\n    myscores[weight] = score","06f5f4ce":"blend_results = pd.DataFrame(myscores, index=['score']).T","64922606":"ax = blend_results.plot(title='Weight vs. OOF Score')\nax.set_xlabel('Weight %')\nax.set_ylabel('OOF Score')\nplt.show()","67522ed5":"blend_results.loc[blend_results['score'] == blend_results['score'].min()]","3ff8901f":"submission_df['target'] = (submission_df['target_LGBM'] * 0.2) + (submission_df['target_LGBM2'] * (1-0.2))","ab0272e0":"# pred_cols = [c for c in submission_df.columns if c.startswith('pred_')]\n\n# submission_df['target'] = submission_df[pred_cols].mean(axis=1)\n# Visually check correlation between fold predictions\n# sns.heatmap(submission_df[pred_cols].corr(), annot=True)","98abe59f":"submission_df[['id','target']] \\\n    .to_csv('submission.csv', index=False)","b6c462fb":"# Setup KFold","b36f3aea":"# Set Target and Features","b373096d":"# Create Submission","d79420fa":"# Pog Competition Baselines\n- LightGBM\n- XGBoost\n\nIn this notebook we will ensemble based on out of fold predictions.","f2a541ac":"# Train Model 2 - LightGBM Different Learning Rate","2b0bf5e2":"# Look at Fold Feature Importances","d8eb2c1b":"# Evaluation out of all out of fold predictions","4322d4d6":"# Weighted Blend Based on OOF","6ebca7fa":"# Create Folds\n- This is how we will later split when validating our models","924a956e":"## The best weight is 0.35","ecb29c79":"# Model 1 -  LGBM Model","d5c6bfd7":"# Feature Engineering","da20931d":"# Save off as LGBM results"}}