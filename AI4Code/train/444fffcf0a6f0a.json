{"cell_type":{"745fd63a":"code","e74fd446":"code","d49e22d0":"code","6a00bbc2":"code","4c348942":"code","9cd00374":"code","f5c695c1":"code","dad22306":"code","1e7a5225":"code","b54096d5":"code","7f90bb09":"code","373f8e17":"code","f022e020":"code","9c3678bd":"code","6ca5eab0":"code","f220977f":"code","39cad3a7":"code","28e948a1":"code","172d3c98":"code","a76d3f86":"code","e7220cca":"code","1de3d74c":"code","8aaa6a8f":"code","2bea4b7f":"code","9df1bd75":"code","fe359234":"code","201e9ef7":"code","4fd3ebc5":"code","12c3459b":"code","b2cf1ab0":"code","dbc7626c":"code","40582c6d":"code","64aac836":"code","2c3dddeb":"code","b91284f1":"code","b2bbc0c5":"code","4f22686f":"code","246a2372":"code","220eee14":"code","dfaaa492":"code","92f1ebcf":"code","83ebb0f7":"code","e6260980":"code","7196e2e8":"code","03fde4f5":"code","5104ad31":"code","81d70dbd":"code","69f4c87c":"code","5bcebd80":"code","e08c3f8f":"code","1bee41fc":"code","d35a5757":"code","54f2f921":"code","c3aa35f6":"code","c8ba5ecb":"code","6b5fb507":"code","410af568":"code","7e09d071":"code","607934a5":"code","c69c349f":"code","e3314db1":"code","25d723fb":"code","9b6855a8":"code","863aba0a":"code","edbe33d3":"code","bfd6d156":"code","224d5d16":"code","a36c2e76":"code","5f1b8837":"code","2d643ca4":"code","f11c1de6":"code","42b90f81":"code","1230748d":"code","33d00caf":"code","0d128419":"code","3f9f8ec4":"code","1764076c":"code","34e4f6a7":"code","83e72b79":"code","12d96969":"code","2b3492a0":"code","c12ba0d1":"code","cc6c43e3":"code","e61e44d3":"code","f839bb99":"code","72ef20ae":"code","ba56f3a5":"code","d551d922":"code","3a75fb7a":"code","c1e77524":"code","3fd3b252":"code","7ad9d7c5":"code","56f385a2":"code","6922bcf0":"code","1c8efc48":"code","f06df4de":"code","4a714f68":"code","c50c5f88":"code","f0c14c27":"code","f359415c":"code","3552b364":"code","abb5a5e4":"code","02ac2a78":"code","df238dae":"code","e643564c":"code","b57d3893":"code","cd6b523e":"code","10575e90":"code","3c154f2c":"code","e1116edc":"code","f3015fa8":"code","65365221":"code","5f86f2da":"code","d3701a5f":"code","8bf06a8d":"code","f90de38a":"code","858f30f0":"code","1d10b20f":"code","30d830fe":"code","6c323e69":"code","8d597ffe":"code","8b1bb86d":"code","1e9d96eb":"code","f4b9ec9d":"code","9c0d9f2c":"code","d92d1b36":"code","a7962e49":"code","b06231a5":"code","e3b1b92e":"code","c70cbc59":"code","de4cbbdb":"code","902ada19":"code","45a419e6":"code","968f31cf":"code","9cd725f7":"code","f29a2151":"code","5ae8ac2f":"code","c67f7d73":"code","adb79fc4":"code","708ec2c4":"code","87ed651e":"code","124e4254":"code","65b66ca5":"code","722c5ed1":"code","9e24cd12":"code","02a3bbab":"code","0b3062fa":"code","9a4f620d":"code","b59dd046":"code","2f77547c":"code","0ee054b4":"code","9d1f90a4":"code","64ad9242":"code","68719944":"code","8aae12cd":"code","0f8cf0f7":"code","6d7bf296":"code","0ce8bf9d":"code","3b8e4957":"code","314f5e8d":"code","387b6122":"code","96b13901":"code","ae81e4b2":"code","a59b71e2":"code","9a382337":"code","fcf49df5":"code","0d565709":"code","45bb02a0":"code","14be7d91":"code","cb1046ab":"code","97f3d2d2":"code","82402521":"code","98c642ba":"code","75dd5256":"code","ed2be2b0":"code","3ee4d436":"code","3454b8c6":"code","4770d25a":"code","713c6a09":"code","3f9f7eeb":"code","0759a5c5":"code","22908d10":"code","2fa1f67f":"code","06a4bf72":"code","c4a40d9e":"code","20032ab5":"code","cd15ec0e":"code","a335edd5":"code","cf224299":"code","50651d89":"code","a4667291":"code","ccbafe42":"code","961d190f":"code","d5463ba2":"code","83f47841":"code","5ec46022":"code","1df92284":"code","3ed8e41f":"code","25c5b419":"code","80e522be":"code","ad2b8415":"code","217d0a86":"code","e791eb5a":"code","dcf29275":"code","8ee7562d":"code","13546f10":"code","b6ff59cd":"code","c34b3df2":"code","2d78b103":"code","ca0a77d5":"code","95b71b12":"code","f5badb30":"code","ff0c85bf":"code","23fd3339":"code","3bebf7f3":"code","2a65b34d":"code","d1e1848a":"code","86aad5dd":"code","b95535ee":"code","52860e3f":"code","586d647a":"code","befc0d09":"code","b219cac3":"code","9d275bc9":"code","cf9c859a":"code","1cabdbce":"code","c11f9cd9":"code","694ba96e":"code","f5b97b0e":"code","0b9c9b74":"code","86474aba":"code","3e4b7e96":"code","ffa54eac":"code","e3fde8cb":"code","60b4bad9":"code","1774a41c":"markdown","315b13bb":"markdown","f295ac73":"markdown","d83cf9a1":"markdown","0bf9bd1e":"markdown","ac85eb42":"markdown","d7b70ac5":"markdown","0dcaa93f":"markdown","efba4e74":"markdown","c0de9419":"markdown","acaa651f":"markdown","a29d1476":"markdown","61033428":"markdown","b21afa55":"markdown","11875446":"markdown","b20f3e67":"markdown","3f941028":"markdown","f0930a26":"markdown","b0225ed4":"markdown","7066d0b1":"markdown","424533d6":"markdown","07083464":"markdown","d80a9f6b":"markdown","288630e1":"markdown","d1ed0813":"markdown","f69659da":"markdown","60c84f50":"markdown","711d55c8":"markdown","ea2b0dd4":"markdown","967cf7a2":"markdown","3ac0c42a":"markdown","e0d87d09":"markdown","7d873fc9":"markdown","cdae4f00":"markdown","06b0625c":"markdown","79fdd901":"markdown","4b683890":"markdown","af370c13":"markdown","a28a30cf":"markdown","1d8c09d4":"markdown","e1dcfd4c":"markdown","ba51ff09":"markdown","6f02d9a2":"markdown","32277867":"markdown","1f952698":"markdown","8e4105a3":"markdown","d2d409ab":"markdown","da009b5f":"markdown","4bd12be8":"markdown","2a444c00":"markdown","58d8feb9":"markdown","2ff9478a":"markdown","3d335c5a":"markdown","caef4ea1":"markdown","935d0531":"markdown","89749a70":"markdown","66937f8c":"markdown","d855490a":"markdown","6fb71865":"markdown","e716e7b7":"markdown","9f858839":"markdown","de1b66aa":"markdown","37f28dbd":"markdown","c58299c7":"markdown","1c1f77db":"markdown","e8ebbc88":"markdown","a3c78526":"markdown","a0a2ef89":"markdown","ff7424c8":"markdown","2e0171ab":"markdown","37ce484f":"markdown","ad70fee6":"markdown","e41147f6":"markdown","a72cade2":"markdown","358209dc":"markdown","9541066a":"markdown","2b65ee32":"markdown","1a945beb":"markdown","061ad229":"markdown","c3c221b6":"markdown","85230c0d":"markdown","e39ce1f9":"markdown","fc59be4c":"markdown","edefc7d0":"markdown","4fc4aed0":"markdown","10e34549":"markdown","cadfd3a1":"markdown","5ca2591e":"markdown","7d492ce0":"markdown","90e57174":"markdown","fb7c2d96":"markdown","1c3329cd":"markdown","b678c3de":"markdown","54b9d0f6":"markdown","b76e77f5":"markdown","8d1d7b08":"markdown","f8a995d6":"markdown","6ac43c64":"markdown","7a054d99":"markdown","7d22a825":"markdown","ac1b4a90":"markdown","e20900fa":"markdown","4a555d9d":"markdown","ddfd7918":"markdown","d77b72fd":"markdown","e84ed417":"markdown","441a3b5c":"markdown","03ee6c56":"markdown","2626e658":"markdown","f665e43b":"markdown","5d6a6d88":"markdown","76443946":"markdown","9222eadc":"markdown","ddf536e3":"markdown","a086809e":"markdown","f300bd24":"markdown","b29eb7a2":"markdown","5706629e":"markdown","dc4e490c":"markdown","d3a764d6":"markdown","db6fe45a":"markdown","4ec9de68":"markdown","3c9733fe":"markdown","0833e713":"markdown","8567360b":"markdown","1f22a062":"markdown","a5413fc3":"markdown","cfe0e46c":"markdown","be9c7f8f":"markdown","d33da5e2":"markdown","a73a16af":"markdown","7465b20c":"markdown","94758686":"markdown"},"source":{"745fd63a":"# imports \nimport numpy as np \nimport pandas as pd \nimport missingno as msno \nfrom fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\nfrom datetime import datetime, date, timedelta\nfrom time import time\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nimport sklearn.metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LeakyReLU\nfrom keras.layers import LSTM\nimport keras\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Flatten\nfrom keras.models import load_model\nimport sys\nfrom ipywidgets import *\n\nimport gc\nfrom memory_profiler import profile\n%load_ext memory_profiler\nfrom ctypes import cdll, CDLL\ncdll.LoadLibrary(\"libc.so.6\")\nlibc = CDLL(\"libc.so.6\")\nimport tensorflow as tf","e74fd446":"def determine_time_gaps(actual_time_series):\n    \"\"\"\n    Determines if there are any missing dates in the dataset\n    \n    Args: \n        actual_time_series - the actual time series from the dataset\n    \n    Returns:\n        correct_days[~mask] - the correct days\n    \"\"\"\n    \n    correct_days = pd.Series(data=pd.date_range(start=actual_time_series.values[0], end=actual_time_series.values[-1], freq='D'))\n    mask = correct_days.isin(actual_time_series.values)\n    return correct_days[~mask]\n","d49e22d0":"def fill_in_missing_data(df, strategy, order=1):\n    \"\"\"\n    Fills missing data\n    \n    Args:\n        df - dataframe with missing data\n        strategy - which filling strategy to use\n        order - if strategy is interpolate, the order of the polynomial; defaults to 1\n    \"\"\"\n    \n    if strategy == 'kNN':  # drop rows with incomplete data\n        df[df.columns.drop('Date')] = KNN(k=10).fit_transform(df[df.columns.drop('Date')])\n    if strategy == 'interpolate':  # drop columns with the most missing data\n        df = df.interpolate(method='polynomial', order=order)\n        \n    return df","6a00bbc2":"# kaggle_path denotes whether or not we use the kaggle paths to file\nkaggle_path = True","4c348942":"local_paths = {\n    'auser': 'Aquifer_Auser.csv',\n    'doganella': 'Aquifer_Doganella.csv',\n    'luco': 'Aquifer_Luco.csv',\n    'petrignano': 'Aquifer_Petrignano.csv',\n    'bilancino': 'Lake_Bilancino.csv',\n    'amiata': 'Water_Spring_Amiata.csv',\n    'madonna': 'Water_Spring_Madonna_di_Canneto.csv',\n    'lupa': 'Water_Spring_Lupa.csv',\n    'arno': 'River_Arno.csv',\n    'xgb': '.\/XGBregressor_Models',\n    'xgb_results': '.\/XGBregressor_Results',\n    'lstm': '.\/LSTM_Models',\n    'lstm_results': '.\/LSTM_Results'    \n}\n\nkaggle_paths = {\n    'auser': '..\/input\/acea-water-prediction\/Aquifer_Auser.csv',\n    'doganella': '..\/input\/acea-water-prediction\/Aquifer_Doganella.csv',\n    'luco': '..\/input\/acea-water-prediction\/Aquifer_Luco.csv',\n    'petrignano': '..\/input\/acea-water-prediction\/Aquifer_Petrignano.csv',\n    'bilancino': '..\/input\/acea-water-prediction\/Lake_Bilancino.csv',\n    'amiata': '..\/input\/acea-water-prediction\/Water_Spring_Amiata.csv',\n    'madonna': '..\/input\/acea-water-prediction\/Water_Spring_Madonna_di_Canneto.csv',\n    'lupa': '..\/input\/acea-water-prediction\/Water_Spring_Lupa.csv',\n    'arno': '..\/input\/acea-water-prediction\/River_Arno.csv',\n    'xgb': '.\/XGBregressor_Models',\n    'xgb_results': '.\/XGBregressor_Results',\n    'lstm': '.\/LSTM_Models',\n    'lstm_results': '.\/LSTM_Results'    \n}\n\n\ndef get_path(dataset):\n    if not kaggle_path:\n        return local_paths[dataset]\n    else:\n        return kaggle_paths[dataset]\n    ","9cd00374":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndf_aquifer_auser = pd.read_csv(get_path('auser'))\nmsno.matrix(df_aquifer_auser);","f5c695c1":"df_aquifer_auser['Date'] = pd.to_datetime(df_aquifer_auser['Date'], format = '%d\/%m\/%Y')\nmissing_dates = determine_time_gaps((df_aquifer_auser['Date']))\nprint('Number of missing dates is {}'.format(len(missing_dates)))\nif len(missing_dates) > 0:\n    print('Start of missing dates is {}, and end of missing dates is {}.'.format(missing_dates.values[0], missing_dates.values[-1]))\n    print(pd.to_datetime(missing_dates.values))\nprint('End of mostly missing data is {}'.format(df_aquifer_auser.dropna()['Date'].values[0]))","dad22306":"df_aquifer_auser['Date'] = pd.to_datetime(df_aquifer_auser['Date'], format = '%d\/%m\/%Y')\ndf_aquifer_auser = df_aquifer_auser[df_aquifer_auser['Date'] >= '2011-05-17']","1e7a5225":"df_aquifer_auser.plot(x='Date', subplots=True, figsize=(30, 90));","b54096d5":"print('Number of zero-values for Temperature_Ponte_a_Moriano = ', len(df_aquifer_auser[df_aquifer_auser['Date'] >= '2017-06-05']))","7f90bb09":"df_aquifer_auser.loc[df_aquifer_auser['Date'] >= '2017-06-05', 'Temperature_Ponte_a_Moriano'] = \\\n    df_aquifer_auser.loc[df_aquifer_auser['Date'] < '2017-06-05', 'Temperature_Ponte_a_Moriano'].values[-1122:]\ndf_aquifer_auser[['Depth_to_Groundwater_LT2', 'Depth_to_Groundwater_DIEC']] = df_aquifer_auser[['Depth_to_Groundwater_LT2', 'Depth_to_Groundwater_DIEC']].interpolate(method='linear')\ndf_aquifer_auser[['Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_PAG', 'Depth_to_Groundwater_CoS']] = df_aquifer_auser[['Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_PAG', 'Depth_to_Groundwater_CoS']].interpolate(method='polynomial', order=2)","373f8e17":"df_aquifer_auser.plot(x='Date', subplots=True, figsize=(30, 90)); ","f022e020":"print('Number of rows which still contain at least one missing value: ', len(df_aquifer_auser) - len(df_aquifer_auser.dropna()))","9c3678bd":"df_aquifer_auser = df_aquifer_auser.loc[df_aquifer_auser['Date'] < '2020-06-05', :]\ndf_aquifer_auser.interpolate(method='linear', inplace=True)","6ca5eab0":"print('Number of rows which still contain at least one missing value: ', len(df_aquifer_auser) - len(df_aquifer_auser.dropna()))","f220977f":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndf_aquifer_doganella = pd.read_csv(get_path('doganella'))\nmsno.matrix(df_aquifer_doganella);","39cad3a7":"df_aquifer_doganella['Date'] = pd.to_datetime(df_aquifer_doganella['Date'], format = '%d\/%m\/%Y')\nmissing_dates = determine_time_gaps(df_aquifer_auser['Date'])\nprint('Number of missing dates is {}'.format(len(missing_dates)))\nif len(missing_dates) > 0:\n    print('Start of missing dates is {}, and end of missing dates is {}.'.format(missing_dates.values[0], missing_dates.values[-1]))\n    print(pd.to_datetime(missing_dates.values))\nprint('End of mostly missing data is {}'.format(df_aquifer_doganella.dropna(subset=['Depth_to_Groundwater_Pozzo_3'])['Date'].values[0]))","28e948a1":"df_aquifer_doganella = df_aquifer_doganella[df_aquifer_doganella['Date'] >= '2012-06-01']","172d3c98":"df_aquifer_doganella.plot(x='Date', subplots=True, figsize=(30, 70));","a76d3f86":"len_filling_data = len(df_aquifer_doganella.loc[df_aquifer_doganella['Date'] <= '2014-12-31', 'Temperature_Monteporzio'])\ndf_aquifer_doganella.loc[(df_aquifer_doganella['Date'] >= '2016-06-01') & (df_aquifer_doganella['Date'] <= '2018-12-31'), 'Temperature_Monteporzio'] = \\\n    df_aquifer_doganella.loc[(df_aquifer_doganella['Date'] >= '2012-06-01') & (df_aquifer_doganella['Date'] <= '2014-12-31'), 'Temperature_Monteporzio'].values\ndf_aquifer_doganella.loc[(df_aquifer_doganella['Date'] >= '2016-06-01') & (df_aquifer_doganella['Date'] <= '2018-12-31'), 'Temperature_Velletri'] = \\\n    df_aquifer_doganella.loc[(df_aquifer_doganella['Date'] >= '2012-06-01') & (df_aquifer_doganella['Date'] <= '2014-12-31'), 'Temperature_Velletri'].values\n\ndf_aquifer_doganella = df_aquifer_doganella.loc[df_aquifer_doganella['Date'] >= '2016-10-07'] ","e7220cca":"df_aquifer_doganella[['Date', 'Depth_to_Groundwater_Pozzo_2']][-26:-16]","1de3d74c":"df_aquifer_doganella.loc[(df_aquifer_doganella['Date'] >= '2020-06-08') & (df_aquifer_doganella['Date'] <= '2020-06-11'), 'Depth_to_Groundwater_Pozzo_2'] = np.nan","8aaa6a8f":"groundwater_variables = ['Depth_to_Groundwater_Pozzo_1', 'Depth_to_Groundwater_Pozzo_2',\n       'Depth_to_Groundwater_Pozzo_3', 'Depth_to_Groundwater_Pozzo_4',\n       'Depth_to_Groundwater_Pozzo_5', 'Depth_to_Groundwater_Pozzo_6',\n       'Depth_to_Groundwater_Pozzo_7', 'Depth_to_Groundwater_Pozzo_8',\n       'Depth_to_Groundwater_Pozzo_9']\nvolume_variables = ['Volume_Pozzo_1', 'Volume_Pozzo_2',\n       'Volume_Pozzo_3', 'Volume_Pozzo_4', 'Volume_Pozzo_5+6',\n       'Volume_Pozzo_7', 'Volume_Pozzo_8', 'Volume_Pozzo_9'] \ntemperature_variables = ['Temperature_Monteporzio', 'Temperature_Velletri']\ndf_aquifer_doganella.where(df_aquifer_doganella.loc[(df_aquifer_doganella['Date'] >= '2019-09-25') & (df_aquifer_doganella['Date'] <= '2019-10-13'), volume_variables] != 0, np.nan, inplace=True)","2bea4b7f":"df_aquifer_doganella.loc[:, groundwater_variables + temperature_variables] = fill_in_missing_data(df_aquifer_doganella.loc[:, groundwater_variables + temperature_variables], 'interpolate', order=1)\ndf_aquifer_doganella[df_aquifer_doganella.columns.drop('Date')] = KNN(k=10).fit_transform(df_aquifer_doganella[df_aquifer_doganella.columns.drop('Date')])\ndf_aquifer_doganella.drop(columns=['Depth_to_Groundwater_Pozzo_4',\n       'Depth_to_Groundwater_Pozzo_5', 'Depth_to_Groundwater_Pozzo_6',\n       'Depth_to_Groundwater_Pozzo_7', 'Depth_to_Groundwater_Pozzo_8',\n       'Depth_to_Groundwater_Pozzo_9'], inplace=True)\n","9df1bd75":"df_aquifer_doganella.plot(x='Date', subplots=True, sharex=True, figsize=(30, 50));","fe359234":"print('Number of rows which still contain at least one missing value: ', len(df_aquifer_doganella) - len(df_aquifer_doganella.dropna()))","201e9ef7":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndf_aquifer_luco = pd.read_csv(get_path('luco'))\nmsno.matrix(df_aquifer_luco);","4fd3ebc5":"df_aquifer_luco['Date'] = pd.to_datetime(df_aquifer_luco['Date'], format = '%d\/%m\/%Y')\nmissing_dates = determine_time_gaps(df_aquifer_luco['Date'])\nprint('Number of missing dates is {}'.format(len(missing_dates)))\nif len(missing_dates) > 0:\n    print('Start of missing dates is {}, and end of missing dates is {}.'.format(missing_dates.values[0], missing_dates.values[-1]))\n    print(pd.to_datetime(missing_dates.values))\nprint('End of mostly missing data is {}'.format(df_aquifer_luco.dropna()['Date'].values[0]))","12c3459b":"dates_target_is_available = df_aquifer_luco[~df_aquifer_luco['Depth_to_Groundwater_Podere_Casetta'].isna()]['Date'].values\ndf_aquifer_luco = df_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2008-02-21') & (df_aquifer_luco['Date'] <= '2019-01-12'), :]","b2cf1ab0":"df_aquifer_luco.plot(x='Date', subplots=True, figsize=(30, 70));","dbc7626c":"# filter rows by Volume variables\ndf_aquifer_luco = df_aquifer_luco.loc[df_aquifer_luco['Date'] >= '2015-01-01']\n# drop columns with not enough data\ncolumns_to_drop_luco = ['Temperature_Siena_Poggio_al_Vento', 'Depth_to_Groundwater_Pozzo_1', \n                        'Depth_to_Groundwater_Pozzo_3', 'Depth_to_Groundwater_Pozzo_4',\n                       'Rainfall_Siena_Poggio_al_Vento', 'Rainfall_Ponte_Orgia', 'Rainfall_Pentolina']\ndf_aquifer_luco = df_aquifer_luco.drop(columns=columns_to_drop_luco)\n","40582c6d":"# determine start and end dates of Temperature_Mensano all-zero values\ndf_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2015-01-01') & (df_aquifer_luco['Date'] <= '2015-10-13'), 'Temperature_Mensano'] = \\\n    df_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2017-01-01') & (df_aquifer_luco['Date'] <= '2017-10-13'), 'Temperature_Mensano'].values\n\ndf_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2016-10-07') & (df_aquifer_luco['Date'] <= '2016-11-21'), ['Date', 'Temperature_Monteroni_Arbia_Biena']]\n\n# trying to replace these first three sets all at once with .where or .mask proves to be very buggy, so just do each group manually\ndf_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2016-10-07') & (df_aquifer_luco['Date'] <= '2016-10-12'), 'Temperature_Monteroni_Arbia_Biena'] = \\\n    df_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2017-10-07') & (df_aquifer_luco['Date'] <= '2017-10-12'), 'Temperature_Monteroni_Arbia_Biena'].values\n\ndf_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2016-10-22') & (df_aquifer_luco['Date'] <= '2016-11-02'), 'Temperature_Monteroni_Arbia_Biena'] = \\\n    df_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2017-10-22') & (df_aquifer_luco['Date'] <= '2017-11-02'), 'Temperature_Monteroni_Arbia_Biena'].values\n\ndf_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2016-11-17') & (df_aquifer_luco['Date'] <= '2016-11-21'), 'Temperature_Monteroni_Arbia_Biena'] = \\\n    df_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2017-11-17') & (df_aquifer_luco['Date'] <= '2017-11-21'), 'Temperature_Monteroni_Arbia_Biena'].values\n\n# last set\ndf_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2017-05-19') & (df_aquifer_luco['Date'] <= '2017-06-06'), 'Temperature_Monteroni_Arbia_Biena'] = \\\n    df_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2016-05-19') & (df_aquifer_luco['Date'] <= '2016-06-06'), 'Temperature_Monteroni_Arbia_Biena'].values\n\ndf_aquifer_luco['Depth_to_Groundwater_Podere_Casetta'].interpolate(method='polynomial', order=2, inplace=True)\ndf_aquifer_luco[df_aquifer_luco.columns.drop('Date')] = KNN(k=10).fit_transform(df_aquifer_luco[df_aquifer_luco.columns.drop('Date')])\n","64aac836":"df_aquifer_luco.loc[(df_aquifer_luco['Date'] >= '2017-05-19') & (df_aquifer_luco['Date'] <= '2017-06-06'), ['Date', 'Temperature_Monteroni_Arbia_Biena']];","2c3dddeb":"df_aquifer_luco.plot(x='Date', subplots=True, figsize=(30, 50));","b91284f1":"print('Number of rows which still contain at least one missing value: ', len(df_aquifer_luco) - len(df_aquifer_luco.dropna()))","b2bbc0c5":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndf_aquifer_petrignano = pd.read_csv(get_path('petrignano'))\nmsno.matrix(df_aquifer_petrignano);","4f22686f":"df_aquifer_petrignano['Date'] = pd.to_datetime(df_aquifer_petrignano['Date'], format = '%d\/%m\/%Y')\nmissing_dates = determine_time_gaps((df_aquifer_petrignano['Date']))\nprint('Number of missing dates is {}'.format(len(missing_dates)))\nif len(missing_dates) > 0:\n    print('Start of missing dates is {}, and end of missing dates is {}.'.format(missing_dates.values[0], missing_dates.values[-1]))\n    print(pd.to_datetime(missing_dates.values))\nprint('End of mostly missing data is {}'.format(df_aquifer_petrignano.dropna()['Date'].values[0]))","246a2372":"df_aquifer_petrignano = df_aquifer_petrignano.loc[df_aquifer_petrignano['Date'] >= '2009-01-01', :]","220eee14":"df_aquifer_petrignano.plot(x='Date', subplots=True, figsize=(30, 30));","dfaaa492":"# replace the anomalous temperature with the temperature of the previous year\ndf_aquifer_petrignano.loc[(df_aquifer_petrignano['Date'] >= '2015-04-25') & (df_aquifer_petrignano['Date'] <= '2015-09-21'), 'Temperature_Petrignano'] = \\\n    df_aquifer_petrignano.loc[(df_aquifer_petrignano['Date'] >= '2014-04-25') & (df_aquifer_petrignano['Date'] <= '2014-09-21'), 'Temperature_Petrignano'].values\n# interpolate the missing values\ndf_aquifer_petrignano = df_aquifer_petrignano.interpolate(method='linear')\n# replace the anomalous Hydrometry_Fiume_Chiascio_Petrignano and Volume_C10_Petrignano values with np.nan \ndf_aquifer_petrignano.loc[(df_aquifer_petrignano['Date'] >= '2015-04-25') & (df_aquifer_petrignano['Date'] <= '2015-09-21'), 'Hydrometry_Fiume_Chiascio_Petrignano'] = np.nan\ndf_aquifer_petrignano.where(df_aquifer_petrignano['Volume_C10_Petrignano'] != 0, np.nan, inplace=True)\n# fill in the missing values\ndf_aquifer_petrignano = fill_in_missing_data(df_aquifer_petrignano, 'kNN')","92f1ebcf":"df_aquifer_petrignano.plot(x='Date', subplots=True, figsize=(30, 30));","83ebb0f7":"print('Number of rows which still contain at least one missing value: ', len(df_aquifer_petrignano) - len(df_aquifer_petrignano.dropna()))","e6260980":"idxr = df_aquifer_petrignano['Date'].to_frame().isna().any(axis=1)\nmissing_dates = df_aquifer_petrignano['Date'][idxr].index\nprint('missing dates indices: ', df_aquifer_petrignano.loc[missing_dates, :].index)\ndisplay(df_aquifer_petrignano.loc[4900 : 4910, :])","7196e2e8":"df_aquifer_petrignano[df_aquifer_petrignano['Date'].isna()] = np.nan\npetrignano_time_series = df_aquifer_petrignano['Date']\npetrignano_correct_days = pd.Series(data=pd.date_range(start=petrignano_time_series.values[0], end=petrignano_time_series.values[-1], freq='D'))\ndf_aquifer_petrignano['Date'] = petrignano_correct_days.values\ndf_aquifer_petrignano = df_aquifer_petrignano.interpolate(method='linear')","03fde4f5":"print('Number of rows which still contain at least one missing value: ', len(df_aquifer_petrignano) - len(df_aquifer_petrignano.dropna()))","5104ad31":"msno.matrix(df_aquifer_petrignano)\ndf_aquifer_petrignano.plot(x='Date', subplots=True, figsize=(30, 30));","81d70dbd":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndf_lake_bilancino = pd.read_csv(get_path('bilancino'))\nmsno.matrix(df_lake_bilancino);","69f4c87c":"df_lake_bilancino['Date'] = pd.to_datetime(df_lake_bilancino['Date'], format = '%d\/%m\/%Y')\nmissing_dates = determine_time_gaps(df_lake_bilancino['Date'])\nprint('Number of missing dates is {}'.format(len(missing_dates)))\nif len(missing_dates) > 0:\n    print('Start of missing dates is {}, and end of missing dates is {}.'.format(missing_dates.values[0], missing_dates.values[-1]))\n    print(pd.to_datetime(missing_dates.values))\nprint('End of mostly missing data is {}'.format(df_lake_bilancino.dropna()['Date'].values[0]))","5bcebd80":"df_lake_bilancino = df_lake_bilancino[df_lake_bilancino['Date'] >= '2004-01-02']","e08c3f8f":"df_lake_bilancino.plot(x='Date', subplots=True, figsize=(30, 35));","1bee41fc":"print('Number of rows which still contain at least one missing value: ', len(df_lake_bilancino) - len(df_lake_bilancino.dropna()))","d35a5757":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndf_water_spring_amiata = pd.read_csv(get_path('amiata'))\nmsno.matrix(df_water_spring_amiata);","54f2f921":"df_water_spring_amiata['Date'] = pd.to_datetime(df_water_spring_amiata['Date'], format = '%d\/%m\/%Y')\nmissing_dates = determine_time_gaps(df_water_spring_amiata['Date'])\nprint('Number of missing dates is {}'.format(len(missing_dates)))\nif len(missing_dates) > 0:\n    print('Start of missing dates is {}, and end of missing dates is {}.'.format(missing_dates.values[0], missing_dates.values[-1]))\n    print(pd.to_datetime(missing_dates.values))\nprint('End of mostly missing data is {}'.format(df_water_spring_amiata.dropna()['Date'].values[0]))","c3aa35f6":"df_water_spring_amiata.plot(x='Date', subplots=True, figsize=(30, 60));","c8ba5ecb":"df_water_spring_amiata.loc[(df_water_spring_amiata['Date'] >= '2015-02-05') & (df_water_spring_amiata['Date'] <= '2015-03-30'), 'Rainfall_Abbadia_S_Salvatore'] = \\\n    df_water_spring_amiata.loc[(df_water_spring_amiata['Date'] >= '2014-02-05') & (df_water_spring_amiata['Date'] <= '2014-03-30'), 'Rainfall_Abbadia_S_Salvatore'].values\n\ndf_water_spring_amiata.loc[(df_water_spring_amiata['Date'] >= '2015-07-22') & (df_water_spring_amiata['Date'] <= '2016-01-24'), 'Rainfall_Abbadia_S_Salvatore'] = \\\n    df_water_spring_amiata.loc[(df_water_spring_amiata['Date'] >= '2014-07-22') & (df_water_spring_amiata['Date'] <= '2015-01-24'), 'Rainfall_Abbadia_S_Salvatore'].values\n\n\ndepth_to_groundwater_variables = ['Depth_to_Groundwater_S_Fiora_8', 'Depth_to_Groundwater_S_Fiora_11bis', 'Depth_to_Groundwater_David_Lazzaretti']\nfor groundwater_variable in depth_to_groundwater_variables:\n    df_water_spring_amiata[groundwater_variable].interpolate(method='polynomial', order=2, inplace=True)\n\ndf_water_spring_amiata = df_water_spring_amiata.loc[df_water_spring_amiata['Date'] >= '2015-01-01']\n\ndf_water_spring_amiata[df_water_spring_amiata.columns.drop('Date')] = KNN(k=10).fit_transform(df_water_spring_amiata[df_water_spring_amiata.columns.drop('Date')])","6b5fb507":"# (~df_water_spring_amiata.loc[(df_water_spring_amiata['Date'] >= '2015-07-22') & (df_water_spring_amiata['Date'] <= '2016-01-24'), 'Temperature_Abbadia_S_Salvatore'].isna()).sum()","410af568":"df_water_spring_amiata.plot(x='Date', subplots=True, figsize=(30, 60));","7e09d071":"print('Number of rows which still contain at least one missing value: ', len(df_water_spring_amiata) - len(df_water_spring_amiata.dropna()))","607934a5":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndf_water_spring_madonna_di_canneto = pd.read_csv(get_path('madonna'))\nmsno.matrix(df_water_spring_madonna_di_canneto);","c69c349f":"df_water_spring_madonna_di_canneto.plot(subplots=True, figsize=(30, 13));","e3314db1":"# commented out because it stops the entire notebook from running if choosing to \"Run All\"\n# df_water_spring_madonna_di_canneto['Date'] = pd.to_datetime(df_water_spring_madonna_di_canneto['Date'], format = '%d\/%m\/%Y')\n# missing_dates = determine_time_gaps(df_water_spring_madonna_di_canneto['Date'])\n# print('Number of missing dates is {}'.format(len(missing_dates)))\n# if len(missing_dates) > 0:\n#     print('Start of missing dates is {}, and end of missing dates is {}.'.format(missing_dates.values[0], missing_dates.values[-1]))\n#     print(pd.to_datetime(missing_dates.values))\n# print('End of mostly missing data is {}'.format(df_water_spring_madonna_di_canneto.dropna()['Date'].values[0]))","25d723fb":"df_water_spring_madonna_di_canneto = pd.read_csv(get_path('madonna'))\ndf_water_spring_madonna_di_canneto = df_water_spring_madonna_di_canneto.iloc[:-9, :]\ndf_water_spring_madonna_di_canneto['Date'] = pd.to_datetime(df_water_spring_madonna_di_canneto['Date'], format = '%d\/%m\/%Y')\nmissing_dates = determine_time_gaps(df_water_spring_madonna_di_canneto['Date'])\nprint('Number of missing dates is {}'.format(len(missing_dates)))\nif len(missing_dates) > 0:\n    print('Start of missing dates is {}, and end of missing dates is {}.'.format(missing_dates.values[0], missing_dates.values[-1]))\n    print(pd.to_datetime(missing_dates.values))\nprint('End of mostly missing data is {}'.format(df_water_spring_madonna_di_canneto.dropna()['Date'].values[0]))","9b6855a8":"df_water_spring_madonna_di_canneto = df_water_spring_madonna_di_canneto[\n    (df_water_spring_madonna_di_canneto['Date'] >= '2016-04-13') & (df_water_spring_madonna_di_canneto['Date'] <= '2018-12-31')]","863aba0a":"df_water_spring_madonna_di_canneto.plot(x='Date', subplots=True, figsize=(30, 13));","edbe33d3":"df_water_spring_madonna_di_canneto.interpolate(method='linear', inplace=True)\ndf_water_spring_madonna_di_canneto.plot(x='Date', subplots=True, figsize=(30, 13));","bfd6d156":"print('Number of rows which still contain at least one missing value: ', len(df_water_spring_madonna_di_canneto) - len(df_water_spring_madonna_di_canneto.dropna()))","224d5d16":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndf_water_spring_lupa = pd.read_csv(get_path('lupa'))\n# df_water_spring_lupa['Date'] = pd.to_datetime(df_water_spring_lupa['Date'], format = '%d\/%m\/%Y')\nmsno.matrix(df_water_spring_lupa);","a36c2e76":"df_water_spring_lupa['Date'] = pd.to_datetime(df_water_spring_lupa['Date'], format = '%d\/%m\/%Y')\nmissing_dates = determine_time_gaps(df_water_spring_lupa['Date'])\nprint('Number of missing dates is {}'.format(len(missing_dates)))\nif len(missing_dates) > 0:\n    print('Start of missing dates is {}, and end of missing dates is {}.'.format(missing_dates.values[0], missing_dates.values[-1]))\n    print(pd.to_datetime(missing_dates.values))\nprint('End of mostly missing data is {}'.format(df_water_spring_lupa.dropna()['Date'].values[0]))","5f1b8837":"df_water_spring_lupa = df_water_spring_lupa[df_water_spring_lupa['Date'] >= '2010-09-04']","2d643ca4":"df_water_spring_lupa.plot(x='Date', subplots=True, figsize=(30, 10));","f11c1de6":"df_water_spring_lupa = df_water_spring_lupa.interpolate(method='linear')\nprint('Number of rows which still contain at least one missing value: ', len(df_water_spring_lupa) - len(df_water_spring_lupa.dropna()))","42b90f81":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndf_river_arno = pd.read_csv(get_path('arno'))\nmsno.matrix(df_river_arno);","1230748d":"df_river_arno['Date'] = pd.to_datetime(df_river_arno['Date'], format = '%d\/%m\/%Y')\nmissing_dates = determine_time_gaps((df_river_arno['Date']))\nprint('Number of missing dates is {}'.format(len(missing_dates)))\nif len(missing_dates) > 0:\n    print('Start of missing dates is {}, and end of missing dates is {}.'.format(missing_dates.values[0], missing_dates.values[-1]))\n    print(pd.to_datetime(missing_dates.values))\nprint('End of mostly missing data is {}'.format(df_river_arno.dropna()['Date'].values[0]))","33d00caf":"df_river_arno = df_river_arno[df_river_arno['Date'] >= '2004-01-01']","0d128419":"msno.matrix(df_river_arno);","3f9f8ec4":"df_river_arno.plot(x='Date', subplots=True, figsize=(30, 60));","1764076c":"df_river_arno.loc[(df_river_arno['Date'] >= '2017-08-04') & (df_river_arno['Date'] <= '2020-06-30'), 'Temperature_Firenze'] = \\\n    df_river_arno.loc[(df_river_arno['Date'] >= '2014-08-04') & (df_river_arno['Date'] <= '2017-06-30'), 'Temperature_Firenze'].values","34e4f6a7":"df_river_arno_full_data = df_river_arno.loc[(df_river_arno['Date'] >= '2004-01-01') & (df_river_arno['Date'] <= '2007-07-06')]\ndf_river_arno_full_data = fill_in_missing_data(df_river_arno_full_data, 'kNN')\ndf_river_arno_imputed_data = fill_in_missing_data(df_river_arno.copy(), 'kNN')","83e72b79":"df_river_arno_full_data.plot(x='Date', subplots=True, figsize=(30, 50));","12d96969":"print('Number of rows which still contain at least one missing value: ', len(df_river_arno_full_data) - len(df_river_arno_full_data.dropna()))","2b3492a0":"df_river_arno_imputed_data.plot(x='Date', subplots=True, figsize=(30, 50));","c12ba0d1":"print('Number of rows which still contain at least one missing value: ', len(df_river_arno_imputed_data) - len(df_river_arno_imputed_data.dropna()))","cc6c43e3":"example_df_river_arno = pd.read_csv(get_path('arno'))\nexample_df_river_arno = example_df_river_arno.dropna()[['Date', 'Rainfall_Le_Croci', 'Temperature_Firenze', 'Hydrometry_Nave_di_Rosano']]\nexample_df_river_arno = example_df_river_arno[80:]\nexample_df_river_arno.head()","e61e44d3":"def add_following_n_target_days(df, num_days_to_predict, name_of_target, diff):\n    \"\"\"\n    Appends the values of the target on the following n days to every row\n\n    Args:\n        df - dataframe to which following argument days are being appended\n        num_days_to_predict - numbers of target variables to be added for prediction\n        name_of_target - name of the target variable (in case of trying to predict the difference between days)\n        diff - boolean; if True, trying to compute the difference between days\n    \n    Returns:\n        df - dataframe which has n target days appended\n    \"\"\"\n    targets = []\n    if not diff:\n        for i in range(1, num_days_to_predict + 1):\n            lookback_vars = df[name_of_target].shift(periods=-i, fill_value=np.nan)\n            lookback_vars.name = lookback_vars.name + '_{}_days_forward'.format(i)\n            targets.append('{}_{}_days_forward'.format(name_of_target, i))\n            df = pd.merge(df, lookback_vars, how=\"left\", left_index=True, right_index=True)\n    else:\n        for i in range(-10, num_days_to_predict):\n            lookback_vars = df[name_of_target].shift(periods=-i, fill_value=np.nan)\n            lookback_vars.name = lookback_vars.name + '_{}_days_forward'.format(i + 1)\n            if i > -1:\n                targets.append('{}_{}_days_forward'.format(name_of_target, i + 1))\n            df = pd.merge(df, lookback_vars, how=\"left\", left_index=True, right_index=True)  \n        df.drop(columns=[name_of_target], inplace=True)\n    return df, targets","f839bb99":"num_days_to_predict = 5\nname_of_target = 'Hydrometry_Nave_di_Rosano'\ndiff = False\nexample_df_river_arno_with_n_targets_added, example_targets = add_following_n_target_days(\n    example_df_river_arno, num_days_to_predict, name_of_target, diff)","72ef20ae":"display(example_df_river_arno_with_n_targets_added.head())\nprint('The targets are: ', example_targets)","ba56f3a5":"def scale_values(df, feature_range=(0, 1)):\n    \"\"\"\n    Scale values as part of data preparation\n    \n    Args:\n        df - the dataframe to be scaled\n        feature_range - the range of values to which the features are scaled \n\n    Returns:\n        df - scaled dataframe\n        scaler - the scaler which scaled the dataframe\n    \"\"\"\n    \n    scaler = MinMaxScaler(feature_range)\n    df = scaler.fit_transform(df)\n    return df, scaler\n\ndef invert_difference(df, original_target, diff_values, rolling_sum=True):\n    \"\"\"\n    Invert differences data to recover the actual predictions;\n    used in conjunction with difference_between_days()\n    \n    Args:\n        df - the original dataframe\n        original_target - name of the original target variable (before differencing)\n        diff_values - dataframe containing the values to be inverted\n        rolling_sum - whether to take the difference relative to the original target values, or the day prior;\n            e.g. if rolling_sum == True, then the inverted values will be i + 1, i + 1 + 2, i + 1 + 2 + 3, ...\n                 if rolling_sum == False, then the inverted values will be i + 1, i + 2, i + 3, ...\n            given how the differences were taken, rolling_sum should always be set to True, \n            but sometimes gives better results when set to False\n\n    Returns:\n        result - the inverted values\n    \"\"\"\n\n    original_target_values = df[original_target]\n    result = pd.merge(original_target_values, diff_values, how=\"right\", left_index=True, right_index=True)\n    for num_column, column_name in enumerate(result.columns):\n        if num_column == 0:\n            continue\n        else:\n            if rolling_sum:\n                result[column_name] = result[column_name] + result.iloc[:, num_column - 1]\n            else:\n                result[column_name] = result[column_name] + original_target_values\n    result.drop(columns=original_target, inplace=True)\n    \n    return result\n\ndef create_synthetic_data(df, num_time_blocks_to_add):\n    \"\"\"\n    Creates synthetic data for training purposes\n    currently, this is only used for the Arno River\n    \n    Args:\n        df - dataframe which contains the original data\n        num_time_blocks_to_add    \n    \n    Returns:\n        df - dataframe with the differences between days appended\n        new_name_of_target - name of the new (differenced) target to be predicted \n    \"\"\"\n \n    data_to_copy = df.iloc[:365*2, :]\n    temp_df = pd.DataFrame(columns=df.columns)\n    \n    for i in range(num_time_blocks_to_add):\n        rainfall_multiplicative_factor = np.random.uniform(0.5, 1.5)\n        target_variable_multiplicative_factor = rainfall_multiplicative_factor * np.random.uniform(0.95, 1.05)\n        temperature_multiplicative_factor = np.random.uniform(0.98, 1.02)\n        random_num_days_to_shift = np.random.randint(-7, 7)\n        data_to_copy = data_to_copy.shift(random_num_days_to_shift, fill_value=np.nan)\n        data_to_copy[data_to_copy.columns.drop(['Date', 'Temperature_Firenze_synthetic', 'Hydrometry_Nave_di_Rosano_synthetic'])] = \\\n            data_to_copy[data_to_copy.columns.drop(['Date', 'Temperature_Firenze_synthetic', 'Hydrometry_Nave_di_Rosano_synthetic'])] * rainfall_multiplicative_factor\n        data_to_copy['Temperature_Firenze_synthetic'] = data_to_copy['Temperature_Firenze_synthetic'] * temperature_multiplicative_factor\n        data_to_copy['Hydrometry_Nave_di_Rosano_synthetic'] = data_to_copy['Hydrometry_Nave_di_Rosano_synthetic'] * target_variable_multiplicative_factor    \n        temp_df = temp_df.append(data_to_copy)\n    \n    end_date = pd.to_datetime(df['Date'].values[0])    \n    total_length_added = temp_df.shape[0]\n    actual_end_date = end_date - timedelta(days=1)\n    start_date = actual_end_date - timedelta(days=total_length_added - 1)\n    dates_added = pd.Series(data=pd.date_range(start=start_date, end=actual_end_date, freq='D'))\n    temp_df['Date'] = dates_added.values\n    temp_df.append(df)['Date'].reset_index().drop(columns='index')\n    temp_df = temp_df.append(df)\n    temp_df = temp_df.reset_index().drop(columns='index')\n    temp_df[temp_df.columns.drop('Date')] = KNN(k=10).fit_transform(temp_df[temp_df.columns.drop('Date')])\n    return temp_df\n\n\ndef difference_between_days(df, name_of_target, num_days_difference=1):\n    \"\"\"\n    Make the target variable be the difference between one day and the following day, instead of the raw value;\n    used for non-stationary data\n    \n    Args:\n        df - dataframe which is being altered\n        name_of_target - target variable which we are differencing\n        num_days_difference - number of following days to be from which we are taking the difference; \n                              should be == 1\n    \n    Returns:\n        df - dataframe with the differences between days appended\n        new_name_of_target - name of the new (differenced) target to be predicted \n\n    \"\"\"\n\n    new_name_of_target = name_of_target + '_diff'\n    df[new_name_of_target] = df[name_of_target].shift(-num_days_difference)\\\n                                         - df[name_of_target] \n    return df, new_name_of_target\n\ndef create_temp_and_time_dummies_plus_cross_terms(df, name_of_target, name_of_temperature_variable, no_temp):\n    \"\"\"\n    Feature extraction: make dummy variables for temperature brackets, periods of the year (weeks and month),\n    and create cross terms between these dummy variables and the rainfall variables\n    \n    Args:\n        df - dataframe to be amended \n        name_of_target - name of the target variable\n        name_of_temperature_variable - name of the temperature variable; in case there is more than one, \n            pick one arbitrarily\n        no_temp - boolean; True if there is no temperature variable\n    \n    Returns:\n        df - dataframe with dummies variables and cross terms appended\n\n    \"\"\"\n    \n    temp_dummies = None\n    if not no_temp:\n        temp_dummies_dict = {}\n        for temp_range in [[-100, 10], [10, 20], [20, 25], [25, 30], [30, 35], [35, 100]]:\n            temp_dummies_dict['temp_group_{}-{}'.format(temp_range[0], temp_range[1])] = ((temp_range[0] < df[name_of_temperature_variable]) \n                                                                & (df[name_of_temperature_variable] <= temp_range[1])).astype(int)\n        temp_dummies = pd.DataFrame(temp_dummies_dict)\n\n    df['week_of_year'] = pd.DatetimeIndex(df['Date']).weekofyear\n    # error keeps on popping up, tried many combinations, but couldn't find solution that worked properly\n#     df['week_of_year'] = pd.DatetimeIndex(df['Date']).dt.isocalendar().week\n#     df['week_of_year'] = pd.Int64Index(df['Date']).dt.isocalendar().week\n\n    time_dummies = pd.get_dummies(df['week_of_year'], prefix='Week', drop_first=True)\n    df['month_of_year'] = pd.DatetimeIndex(df['Date']).month\n    month_time_dummies = pd.get_dummies(df['month_of_year'], prefix='Month', drop_first=True)\n    time_dummies = time_dummies.join(month_time_dummies)\n\n    if not no_temp:\n        variables = df.columns.drop(['Date', name_of_temperature_variable])\n    else:\n        variables = df.columns.drop(['Date'])\n\n    if not no_temp:\n        for temp_dummy in temp_dummies.columns:\n            for relevant_var in variables:\n                df['{}_x_{}'.format(temp_dummy, relevant_var)] = np.multiply(temp_dummies[temp_dummy], df[relevant_var])\n\n        for temp_dummy in temp_dummies.columns:\n            for time_dummy in time_dummies:\n                df['{}_x_{}'.format(temp_dummy, time_dummy)] = np.multiply(temp_dummies[temp_dummy], time_dummies[time_dummy])\n\n    for time_dummy in time_dummies:\n        for relevant_var in variables:\n            df['{}_x_{}'.format(time_dummy, relevant_var)] = np.multiply(time_dummies[time_dummy], df[relevant_var])\n        \n    \n    \"\"\"these cross terms don't seem to improve predictions, but the code is left here just in case\"\"\"\n#     for temp_dummy in temp_dummies.columns:\n#         for time_dummy in time_dummies:\n#             for relevant_var in rainfall_vars:\n#                 df['{}_x_{}_x_{}'.format(temp_dummy, time_dummy, relevant_var)] = \\\n#                 np.multiply(np.multiply(temp_dummies[temp_dummy], time_dummies[time_dummy]), df[relevant_var])\n\n    df = time_dummies.join(df)\n    if not no_temp:\n        df = temp_dummies.join(df)\n        \n    del [temp_dummies, time_dummies, month_time_dummies]\n    gc.collect()\n    \n    return df\n\n\ndef add_lookback_window(df, num_lookback_days):\n    \"\"\"\n    Appends num_lookback_timesteps number of prior days' data to a given row.\n    i.e. if num_lookback_timesteps == 1, we will append to every row the data in the prior row;\n         if num_lookback_timesteps == 2, we will append to every row the data in the prior two rows\n         \n    Args:\n        df - dataframe to which lookback days are being appended\n        num_lookback_timesteps - number of prior timesteps days which are being added\n    \n    Returns:\n        df - dataframe with lookback days appended\n        lookback_dict - a dictionary mapping a digit i to the variables from i days prior \n    \"\"\"\n        \n    temp_df = df.copy()\n    lookback_dict = {}\n    for i in range(1, num_lookback_days + 1):\n        \n        libc.malloc_trim(0)\n        \n        lookback_vars = temp_df.shift(periods=i, fill_value=np.nan)\n        lookback_vars = lookback_vars.add_suffix('_{}_days_prior'.format(i))\n        lookback_dict[i] = list(lookback_vars.columns)\n        df = pd.merge(lookback_vars, df, how=\"left\", left_index=True, right_index=True)        \n    \n    libc.malloc_trim(0)\n    del [temp_df]\n    gc.collect()\n    \n    return df, lookback_dict\n\n\ndef add_following_n_target_days(df, num_days_to_predict, name_of_target, diff):\n    \"\"\"\n    Appends the values of the target on the following num_days_to_predict days to every row\n    \n    Args:\n        df - dataframe to which following argument days are being appended\n        num_days_to_predict - numbers of target variables to be added for prediction\n        name_of_target - name of the target variable (in case of trying to predict the difference between days)\n        diff - boolean; if True, trying to compute the difference between days\n    \n    Returns:\n        df - dataframe which has n target days appended\n        targets - names of the new target variables\n        target_dict - a dictionary mapping a digit i to the corresponding target variable i days in the future\n    \"\"\"\n    \n    targets = []\n    target_dict = {}\n    if not diff:\n        for i in range(1, num_days_to_predict + 1):\n\n            libc.malloc_trim(0)\n            \n            lookforward_vars = df[name_of_target].shift(periods=-i, fill_value=np.nan)\n            lookforward_vars.name = lookforward_vars.name + '_{}_days_forward'.format(i)\n            targets.append('{}_{}_days_forward'.format(name_of_target, i))\n            target_dict[i] = targets[-1]\n            df = pd.merge(df, lookforward_vars, how=\"left\", left_index=True, right_index=True)\n    else:\n        for i in range(-1, num_days_to_predict):\n            \n            libc.malloc_trim(0)\n            \n            lookforward_vars = df[name_of_target].shift(periods=-i, fill_value=np.nan)\n            lookforward_vars.name = lookforward_vars.name + '_{}_days_forward'.format(i + 1)\n            if i > -1:\n                targets.append('{}_{}_days_forward'.format(name_of_target, i + 1))\n            target_dict[i] = targets[-1]\n            df = pd.merge(df, lookforward_vars, how=\"left\", left_index=True, right_index=True)  \n            \n        df.drop(columns=[name_of_target], inplace=True)\n        \n    libc.malloc_trim(0)\n    gc.collect()\n        \n    return df, targets, target_dict\n\n\ndef feature_engineering(df, name_of_target, name_of_temp_variable, num_lookback_days, num_days_to_predict, \n                        num_days_difference, synthetic, print_flags, no_temp=False):\n    \"\"\"\n    This function will create take the dataset and prepare it for analysis. Depending on the parameters,\n    it will do the following in order:\n        1) create synthetic data for training on additional date (currently only done for the Arno River)\n        2) make the target variable be the difference between days, instead of the raw value\n        3) create temperature and time dummy variables, as well as cross terms between them\n        4) append num_lookback_days number of prior days' data to a given row\n        5) append the values of the target on the following num_days_to_predict days to every row\n\n    Args:\n        df - dataframe containing the raw values\n        name_of_target - name of the target variable\n        name_of_temp_variable - name of the temperature variable\n        num_lookback_days - number of prior days' data to append to row; if 0, won't append\n        num_days_to_predict - number of days in advance the target variable is being predicted\n        num_days_difference - make the target variable be the difference between one day and num_days_difference \n                              following days; if 0, will use original values\n        synthetic - create synthetic data on which to train; if 0, none will be made\n                    currently only for use on the Arno River\n        print_flags - if True, will print information about intermediate dataframes and feature engineering\n        no_temp - boolean; True if there is no temperature variable\n    \n    Returns:\n        df - original dataframe plus dummy variables and cross terms \n        targets - the target variables\n        original_target_values - the column containing the original target values, needed for invert_difference()\n        lookback_dict - a dictionary mapping a digit i to the variables from i days prior\n        target_dict - a dictionary mapping a digit i to the corresponding target variable i days in the future\n    \"\"\"\n\n    \n    # this was already done during the data cleaning process, but is left here in case of new data\n    df['Date'] = pd.to_datetime(df['Date'], format = '%d\/%m\/%Y')\n        \n    if synthetic:\n        if print_flags:\n            print('starting data:')\n            display(df)\n            print('creating synthetic data')\n        df = create_synthetic_data(df, synthetic)\n        if print_flags:\n            print('done with synthetic data')\n            display(df)\n    \n    if num_days_difference:\n        if print_flags:\n            print('creating differences between days')\n        df, new_name_of_target = difference_between_days(df, name_of_target, num_days_difference)\n        if print_flags:\n            print('done with creating differences between days')        \n            display(df)\n    else:\n        new_name_of_target = name_of_target\n    \n    if print_flags:\n        print('creating temperature and time dummy variables plus cross terms')\n\n    \n    df = create_temp_and_time_dummies_plus_cross_terms(df, name_of_target, name_of_temp_variable, no_temp)    \n    df.set_index('Date', inplace=True)\n    \n    if print_flags:\n        print('done with creating temperature and time dummy variables plus cross terms')\n        display(df)\n    \n    lookback_dict = None\n    if num_lookback_days:\n        if print_flags:\n            print('creating lookback days')\n        df, lookback_dict = add_lookback_window(df, num_lookback_days)\n        if print_flags:\n            print('done with creating lookback days')    \n            display(df)\n\n    if print_flags:\n        print('creating target variables for following days')\n        \n    # send target variable to last column\n    libc.malloc_trim(0)\n    original_target_values = df.loc[:, name_of_target]\n    libc.malloc_trim(0)\n    df = df.drop(columns=name_of_target)\n    gc.collect()\n    df[name_of_target] = original_target_values\n    \n    # send target variable to last column in case target variable is now the difference between days\n    # if it isn't, we are just repeating the step above, with no consequences\n    temporary_target_data = df.loc[:, new_name_of_target]\n        \n    libc.malloc_trim(0)\n    df = df.drop(columns=new_name_of_target)\n    gc.collect()\n\n    df[new_name_of_target] = temporary_target_data        \n    df, targets, target_dict = add_following_n_target_days(df, num_days_to_predict, new_name_of_target, num_days_difference)\n    \n    libc.malloc_trim(0)\n    df = df.dropna()\n    gc.collect()\n        \n    if print_flags:\n        print('done with creating target variables for following days')\n        print('final dataset: ')\n        display(df)\n    \n    del [temporary_target_data]\n    gc.collect()\n    \n    libc.malloc_trim(0)\n    \n    return df, targets, original_target_values, lookback_dict, target_dict","d551d922":"def make_training_and_test_sets(df, targets, test_size, lookback_period, num_days_to_predict):\n    \"\"\"\n    Splits the data into training and test sets; we must be careful to avoid leaking test data when we train\n    \n    The split is done as follows: if we want to make test_size predictions on our test set, \n        we need to make our training set size equal to test_size + lookback_period, since the TimeseriesGenerator()\n        function used to format data for the LSTM will use lookback_period number of days as starting input for\n        day lookback_period + 1; effectively, the first lookback_period datapoints are not predicted, \n        leaving us with test_size predictions\n    We don't want to leak any test data into our model, so we leave a gap of size num_days_to_predict between\n        our training and test dates, since the last row of the training data will have information for the next\n        num_days_to_predict target variables\n        \n    Args:\n        df - dataframe containing the data to split\n        targets - the target variables\n        test_size - the number of datapoints in our test set\n        lookback_period - the number of prior days that LSTM will be looking at when making predictions\n        num_days_to_predict - number of days to be predicting; in order to avoid leaking data into our model, the \n                              training set is cut off num_days_to_predict days prior to the size of the test set\n    \n    Returns:\n        X_train, y_train - training data\n        X_test, y_test - testing data\n    \"\"\"\n        \n    training_set = df.iloc[:-test_size - lookback_period - num_days_to_predict, :]\n    test_set = df.iloc[-test_size - lookback_period:, :]\n    X_train = training_set.iloc[:, :-num_days_to_predict]\n    y_train = training_set[targets]\n    X_test = test_set.iloc[:, :-num_days_to_predict]\n    y_test = test_set[targets]\n    \n    return X_train, y_train, X_test, y_test","3a75fb7a":"def run_xgb(df, targets, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict):\n    \"\"\"\n    Run XGBRegressor on the dataset and saves the model\n    \n    Args:\n        df - dataframe containing the data\n        targets - names of target variables, i.e. Hydrometry_Nave_di_Rosano_7_days_forward\n        test_size - size of the test set\n        num_lookback_days - number of days of prior data included in any given row\n        num_days_to_predict - number of days to predict\n    \n    Returns:\n        xgb_feature_importances_path - path to file containing the importances of the features as \n                                       determined by XGBRegressor \n        features_path - path to file containing the features trained on in the model\n        y_test_path - path to file containing the test values\n        y_pred_path - path fo file containing the predictions\n        \n    \"\"\"\n    \n# apparently, there is a known bug when using XGB on GPUs where the memory leaks, so for now just use CPU\n# in Kaggle\n#     if not kaggle_path:\n#         params = {'n_estimators': 100,\n#               'max_depth': 4,\n#               'subsample': 0.7,\n#               'learning_rate': 0.04,\n#               'random_state': 0}    \n#     else:\n#         params = {'tree_method': 'gpu_hist',\n#               'n_estimators': 100,\n#               'max_depth': 4,\n#               'subsample': 0.7,\n#               'learning_rate': 0.04,\n#               'random_state': 0,\n#               'n_gpus': 1}\n\n    params = {'n_estimators': 100,\n              'max_depth': 4,\n              'subsample': 0.7,\n              'learning_rate': 0.04,\n              'random_state': 0}    \n\n    \n    xgb_regressor = xgb.XGBRegressor(**params)\n\n    X_train, y_train, X_test, y_test = \\\n                        make_training_and_test_sets(df, targets, test_size, num_lookback_days, num_days_to_predict)\n\n    if not os.path.exists(get_path('xgb')):\n        os.makedirs(get_path('xgb'))\n    if not os.path.exists(get_path('xgb_results')):\n        os.makedirs(get_path('xgb_results'))\n        \n    current_target = y_test.columns[which_day_to_predict - 1]\n    y_train = y_train.iloc[:, which_day_to_predict - 1]\n    y_test = y_test.iloc[:, which_day_to_predict - 1]\n    xgb_model = xgb_regressor.fit(X_train, y_train)      \n    \n    libc.malloc_trim(0)\n    gc.collect()\n    libc.malloc_trim(0)\n    \n    y_pred = xgb_model.predict(X_test)\n    \n    libc.malloc_trim(0)\n    gc.collect()\n    \n    xgb_feature_importances = xgb_model.feature_importances_\n    features = X_train.columns\n    \n    xgb_feature_importances_path = '{}\/{}_feature_importances'.format(get_path('xgb_results'), current_target)\n    features_path = '{}\/{}_features'.format(get_path('xgb_results'), current_target)   \n    y_test_path = '{}\/{}_actual'.format(get_path('xgb_results'), current_target)\n    y_pred_path = '{}\/{}_predictions'.format(get_path('xgb_results'), current_target)\n    \n    pd.Series(xgb_feature_importances).to_csv(xgb_feature_importances_path)    \n    pd.Series(features).to_csv(features_path)    \n    pd.Series(y_test).to_csv(y_test_path)    \n    pd.Series(y_pred).to_csv(y_pred_path)\n    \n    xgb_model.save_model('{}\/{}'.format(get_path('xgb'), current_target))\n    \n    del [xgb_model, xgb_regressor]\n    \n    libc.malloc_trim(0)\n    gc.collect()\n    libc.malloc_trim(0)\n    \n    \n    libc.malloc_trim(0)\n    del [X_train, y_train, X_test, xgb_feature_importances, features, y_test, y_pred]\n    gc.collect()\n    \n    return xgb_feature_importances_path, features_path, y_test_path, y_pred_path\n\n\ndef get_n_most_important_feature_indices(feature_importances_path, features_path, name_of_target, num_most_important_features):\n    \"\"\"\n    Retrieve and print information about the most important features determined by XGBRegressor\n    \n    Args:\n        xgb_feature_importances_path - path to file containing the importances of the features as \n                                       determined by XGBRegressor \n        features_path - path to file containing the features trained on in the model\n        name_of_target - name of the target being predicted\n        num_most_important_features - number of most important features (in sorted order) we want to pring \n    \n    Returns:\n        sorted_feature_importances_pairs - a list of pairs (feature, importance) in decreasing order of importance\n    \"\"\"\n    \n    feature_importances = pd.read_csv(feature_importances_path).iloc[:, 1]\n    features = pd.read_csv(features_path).iloc[:, 1]\n    \n    feature_importances_indices_mask = np.where(feature_importances > 0, True, False)\n\n    relevant_features = features[feature_importances_indices_mask]\n    relevant_features_importances = feature_importances[feature_importances_indices_mask]\n    \n    feature_importances_pairs = list(zip(relevant_features, relevant_features_importances))\n    sorted_feature_importances_pairs = sorted(feature_importances_pairs, key=lambda x: x[1], reverse=True)\n    \n    num_nonzero_important_features = len(sorted_feature_importances_pairs)\n    \n    print('Predicting {}'.format(name_of_target))\n    print()\n    print('Out of {} features, XGBRegressor found {} to be useful for prediction.'.format(len(features), \n                                                                        num_nonzero_important_features))\n    print()\n    print('The following are the first {} most important features, sorted in order of decreasing importance: '\n          .format(min(num_nonzero_important_features, num_most_important_features)))\n    print()\n    for i in range(min(num_nonzero_important_features, num_most_important_features)):\n        print('{}, {}'.format(sorted_feature_importances_pairs[i][0], sorted_feature_importances_pairs[i][1]))","c1e77524":"def fit_lstm(X_train, y_train, batch_size, num_lookback_days, num_epochs):    \n    \"\"\"\n    Create an LSTM neural network and fit it on the training data\n    \n    Args:\n        X_train - independent training variables\n        y_train - target variables\n        batch_size - batch size for the neural network\n        num_lookback_days - number of prior days for the LSTM to look at\n        num_epochs - number of training epochs\n\n    Returns:\n        model - the resulting LSTM neural network\n    \"\"\" \n    \n    n_input = num_lookback_days\n    len_output = y_train.shape[1]\n    n_features= X_train.shape[1] \n    generator = TimeseriesGenerator(X_train, y_train, length=num_lookback_days, batch_size=batch_size)\n\n    # One of several alternative models:    \n#     model = Sequential()\n#     model.add(Conv1D(filters=64, kernel_size=3, activation='tanh', input_shape=(n_input,n_features)))\n#     model.add(Conv1D(filters=64, kernel_size=3, activation='tanh'))\n#     model.add(MaxPooling1D(pool_size=2))\n#     model.add(Flatten())\n#     model.add(RepeatVector(len_output))\n#     model.add(LSTM(200, activation='tanh', return_sequences=True, unit_forget_bias=False))\n#     model.add(TimeDistributed(Dense(100, activation='tanh')))\n#     model.add(TimeDistributed(Dense(1)))\n#     model.compile(loss='mae', optimizer='adam')\n#     print(model.summary())\n#     print(generator[0][0].shape)\n#     model.fit(generator, epochs=num_epochs)\n\n    # One of several alternative models:\n#     model = Sequential()\n#     model.add(LSTM(200, input_shape=(n_input, n_features), return_sequences=True, stateful=True,  batch_size=batch_size))\n#     model.add(LeakyReLU(alpha=0.05))\n#     model.add(LSTM(200, input_shape=(n_input, n_features), return_sequences=True, stateful=True,  batch_size=batch_size))\n#     model.add(LeakyReLU(alpha=0.05))\n#     model.add(Dense(len_output))\n#     model.compile(optimizer='adam', loss='mae')\n#     print(model.summary())\n#     print(generator[0][0].shape)\n#     print('hit')\n#     model.fit(generator, epochs=num_epochs)\n    \n    # One of several alternative models:\n    model = Sequential()\n    model.add(LSTM(200, activation='tanh', input_shape=(n_input, n_features)))\n    model.add(RepeatVector(len_output))\n    model.add(LSTM(200, activation='tanh', return_sequences=True))\n    model.add(TimeDistributed(Dense(100, activation='tanh')))\n    model.add(TimeDistributed(Dense(1)))\n    model.compile(loss='mae', optimizer='adam')\n    print(model.summary())\n    print(generator[0][0].shape)\n    print('hit')\n    model.fit(generator, epochs=num_epochs)\n\n    \n    \n    libc.malloc_trim(0)\n    gc.collect()\n    \n    return model\n\n    \ndef train_prior_years(df, targets, size_test_set, num_lookback_days, num_days_to_predict, model, j,\n                      num_train_years_prior, batch_size, num_epochs_train_prior_years,\n                      X_scaler, y_scaler=None):\n    \"\"\"\n    Train on data from prior years around the same time before making a prediction\n    \n    Arguments are originally passed from run_lstm(), see run_lstm() for details\n    \n    Returns:\n        model - the updated LSTM neural network\n    \n    \"\"\"\n    \n    # train on data from prior years within an interval one week before and one week after the current date\n    for i in range(num_train_years_prior, 0, -1):\n        libc.malloc_trim(0)\n\n        X_train, y_train, X_test, y_test = make_training_and_test_sets(df, targets, i * 365 \\\n                                                        + size_test_set - j + 7 + num_days_to_predict, \n                                                            num_lookback_days, num_days_to_predict)\n        X_test = X_test.iloc[:num_lookback_days + 2*7 + 1, :]\n        y_test = y_test.iloc[:num_lookback_days + 2*7 + 1, :]\n        if y_scaler:\n            X_test_scaled = X_scaler.fit_transform(X_test)\n            y_test_scaled = y_scaler.fit_transform(y_test)\n            test_generator = TimeseriesGenerator(X_test_scaled, y_test_scaled, length=num_lookback_days, \n                                                 batch_size=batch_size)\n        else: \n            test_generator = TimeseriesGenerator(X_test, y_test, length=num_lookback_days, \n                                     batch_size=batch_size)\n        print('Training on data from {} years prior'.format(i))\n        libc.malloc_trim(0)\n        model.fit(test_generator, epochs=num_epochs_train_prior_years)\n        libc.malloc_trim(0)\n        print('Finished training on data from {} years prior'.format(i))\n                \n    return model\n\ndef lstm_walk_forward(df, targets, size_test_set, num_lookback_days, num_days_to_predict, model, \n                      num_train_years_prior, batch_size, num_epochs_walk_forward, num_epochs_train_prior_years,\n                      y_test_actual, original_y_test, X_scaler, y_scaler=None):\n    \"\"\"\n    Implementation of walk-forward validation. As predictions are being made, update the model on the most \n    recently available data. For example, if predictions are being made for 30 days in advance, \n    then we would update the model with the data from 30 days ago\n    \n    Arguments are originally passed from run_lstm(), se run_lstm() for details\n    \n    Returns:\n        df_pred - dataframe containing the predictions; dimensions = (size_test_set, num_days_to_predict)\n        model - the updated LSTM neural network\n    \"\"\"\n    \n    print('walking forward')\n    y_preds = []\n    \n    for j in range(size_test_set):              \n        # update model with data from num_days_to_predict prior (i.e., using information that would \n        # actually be available in a real-world situation)\n        print('Iteration {} out of {}'.format(j, size_test_set))\n        \n        if j >= num_days_to_predict:\n            X_train, y_train, X_test, y_test = make_training_and_test_sets(df, targets, \n                                               size_test_set - j + 2 * num_days_to_predict, \n                                                                    num_lookback_days, num_days_to_predict)\n            X_test = X_test.iloc[:num_lookback_days + 1, :]\n            y_test = y_test.iloc[:num_lookback_days + 1, :]\n\n            if y_scaler:\n                X_test_scaled = X_scaler.fit_transform(X_test)\n                y_test_scaled = y_scaler.fit_transform(y_test)\n                test_generator = TimeseriesGenerator(X_test_scaled, y_test_scaled, length=num_lookback_days, \n                                                     batch_size=batch_size)\n            else: \n                test_generator = TimeseriesGenerator(X_test, y_test, length=num_lookback_days, \n                                     batch_size=batch_size)\n            print('Training on data from {} days ago'.format(num_days_to_predict))\n            model.fit(test_generator, epochs=num_epochs_walk_forward)\n            print('Finished training on data from {} days ago'.format(num_days_to_predict))\n\n            libc.malloc_trim(0)                        \n            \n        # train on data from prior years within an interval one week before and one week after the current date\n        if num_train_years_prior:\n            model = train_prior_years(df, targets, size_test_set, num_lookback_days, num_days_to_predict, model, j,\n                      num_train_years_prior, batch_size, num_epochs_train_prior_years, X_scaler, y_scaler)\n\n        # this is where we actually make the prediction for the next 30 days\n        X_train, y_train, X_test, y_test = make_training_and_test_sets(df, targets, \n                                                                       size_test_set - j + num_days_to_predict, \n                                                                       num_lookback_days, num_days_to_predict)\n        X_test = X_test.iloc[:num_lookback_days + 1, :]\n        y_test = y_test.iloc[:num_lookback_days + 1, :]\n        cur_day_index = y_test.index[-1]\n        \n        if y_scaler:\n            X_test_scaled = X_scaler.fit_transform(X_test)\n            y_test_scaled = y_scaler.fit_transform(y_test)\n            test_generator = TimeseriesGenerator(X_test_scaled, y_test_scaled, length=num_lookback_days, \n                                                 batch_size=batch_size)\n        else: \n            test_generator = TimeseriesGenerator(X_test, y_test, length=num_lookback_days, \n                                 batch_size=batch_size)\n\n        y_pred_scaled = None\n        if scale_y:\n            y_pred_scaled = model.predict(test_generator)\n            y_pred = y_scaler.inverse_transform(y_pred_scaled[:, :, 0])\n        else:\n            y_pred = model.predict(test_generator)[:, :, 0]\n        \n        y_preds.append(y_pred[0])\n        \n        print('Just completed predictions for ', y_test.index[-1])\n\n        # if leak_data == True, then we will train on the data that we just predicted. Since the training\n        # data includes the values of the target variables for the next 30 days, doing this will leak data\n        # It serves as a good benchmark, but should default to False\n        if leak_data:\n            model.fit(test_generator, epochs=num_epochs)\n            \n        libc.malloc_trim(0)            \n        del [test_generator, X_train, y_train, X_test, y_test, y_pred]\n        if y_scaler:\n            del [X_test_scaled, y_test_scaled, y_pred_scaled]\n        gc.collect()\n        libc.malloc_trim(0)\n\n    df_pred = pd.DataFrame(data=y_preds, index=y_test_actual.index, columns=original_y_test.columns)\n    return df_pred, model\n\n\ndef run_lstm(df, targets, num_lookback_days, num_days_to_predict, size_test_set, num_days_difference, original_target, \n             scale_y, walk_forward, leak_data, synthetic, num_train_years_prior, batch_size, num_epochs, \n             num_epochs_walk_forward, num_epochs_train_prior_years, rolling_sum):\n    \"\"\"\n    df - dataframe containing raw values plus feature-engineered values\n    targets - names of the target variables, i.e. Hydrometry_Nave_di_Rosano_7_days_forward\n    num_lookback_days - number of prior days for the LSTM to look at\n    num_days_to_predict - number of days to predict\n    size_test_set - size of the test set\n    num_days_difference - make the target variable be the difference between one day and num_days_difference \n                          following days; if 0, will use original values\n    original_target - name of original target variable ()\n    scale_y - boolean; if True, scale the target variable(s)\n    walk_forward - boolean; if True, while predicting test data, train on most recently available data without\n                            leaking\n    leak_data - boolean; if True, while predicting test data, the model will train on the data it just predicted;\n                            useful for benchmarking, but should be set to False\n    synthetic - number of timeblocks of synthetic data to make for training; currently only compatible with \n                the Arno River\n    num_train_years_prior - number of prior year intervals we train on before making a prediction\n    batch_size - batch size\n    num_epochs - number of epochs to train during the first fit\n    num_epochs_walk_forward - if using walk-forward validation, the number of epochs to train on prior data\n    num_epochs_train_prior_years - if training on data from prior year intervals, the number of epochs to train\n                                   on each interval\n    rolling_sum - boolean; if True, uses a rolling sum to invert differences - see invert_difference()\n    \"\"\"\n        \n    X_train, y_train, X_test, y_test = make_training_and_test_sets(df, targets, size_test_set, \n                                                                   num_lookback_days, num_days_to_predict)\n    \n    X_train_scaled, X_scaler = scale_values(X_train, feature_range=(0, 1))    \n    X_test_scaled = X_scaler.fit_transform(X_test)\n    original_y_test = y_test.copy()\n    y_test_actual = y_test.iloc[-size_test_set:, :]\n    df_pred = pd.DataFrame(index=y_test_actual.index, columns=original_y_test.columns)\n    \n    if not walk_forward:\n\n        num_features= X_train.shape[1] \n        len_output = y_train.shape[1]        \n        test_generator = TimeseriesGenerator(X_test_scaled, np.zeros(len(X_test)), length=num_lookback_days, batch_size=batch_size)\n                \n        if scale_y:\n            feature_range=(0, 1) if not num_days_difference else (-1, 1)\n            y_train_scaled, y_scaler = scale_values(y_train, feature_range)\n            y_test_scaled = y_scaler.fit_transform(y_test)\n            model = fit_lstm(X_train_scaled, y_train_scaled, batch_size, num_lookback_days, num_epochs)\n            libc.malloc_trim(0)\n            y_pred_scaled = model.predict(test_generator)\n            libc.malloc_trim(0)            \n            y_pred = y_scaler.inverse_transform(y_pred_scaled[:, :, 0])\n    #         y_pred = y_scaler.inverse_transform(y_pred_scaled[:, -1, :])  # for other LSTM models\n        else:    \n            model = fit_lstm(X_train_scaled, y_train.to_numpy(), batch_size, num_lookback_days, num_epochs)    \n            y_pred = model.predict(test_generator)\n            y_pred = y_pred[:, :, 0]    \n    #         y_pred = y_pred[:, -1, :]  # for other LSTM models\n        df_pred = pd.DataFrame(y_pred, index=y_test_actual.index, columns=y_test_actual.columns)\n    else:\n        if scale_y:\n            feature_range=(0, 1) if not num_days_difference else (-1, 1)\n            y_train_scaled, y_scaler = scale_values(y_train, feature_range)\n            model = fit_lstm(X_train_scaled, y_train_scaled, batch_size, num_lookback_days, num_epochs)\n            df_pred, model = lstm_walk_forward(df, targets, size_test_set, num_lookback_days, num_days_to_predict, model, \n                      num_train_years_prior, batch_size, num_epochs_walk_forward, num_epochs_train_prior_years,\n                                       y_test_actual, original_y_test, X_scaler, y_scaler)\n        \n        else:\n            y_scaler = None\n            model = fit_lstm(X_train_scaled, y_train.to_numpy(), batch_size, num_lookback_days, num_epochs)    \n            df_pred, model = lstm_walk_forward(df, targets, size_test_set, num_lookback_days, num_days_to_predict, model, \n                      num_train_years_prior, batch_size, num_epochs_walk_forward, num_epochs_train_prior_years,\n                                       y_test_actual, original_y_test, X_scaler, y_scaler)\n\n    # if predicting the differences between days, invert the differences to retrieve the real values\n    if num_days_difference: \n        y_test_actual = invert_difference(df, original_target, original_y_test.iloc[- size_test_set:, :])\n        df_pred = invert_difference(df, original_target, df_pred, rolling_sum)\n    \n    del [df, X_train, y_train, X_test, y_test, X_train_scaled, X_test_scaled, original_y_test]\n    gc.collect()\n\n    if not os.path.exists(get_path('lstm')):\n        os.makedirs(get_path('lstm')) \n    if not os.path.exists(get_path('lstm_results')):\n        os.makedirs(get_path('lstm_results'))\n        \n    y_test_actual_path = '{}\/{}_actual'.format(get_path('lstm_results'), original_target)\n    df_pred_path = '{}\/{}_predictions'.format(get_path('lstm_results'), original_target)\n    y_test_actual.to_csv(y_test_actual_path)    \n    df_pred.to_csv(df_pred_path)\n    model.save('{}\/LSTM_{}.h5'.format(get_path('lstm'), original_target))\n    del model\n    gc.collect()\n    keras.backend.clear_session()\n    tf.compat.v1.reset_default_graph()\n    \n    model = None\n    \n    return df_pred_path, y_test_actual_path, model\n\n","3fd3b252":"def plot_widget_results(df_pred_path, y_test_actual_path, name_of_target, dimension, num_days_in_advance=7):\n    \"\"\"\n    Makes an interactive widget that displays the actual values, target values, the difference between them,\n    mean absolute error, and root mean squared error\n    \n    Args:\n        df_pred_path - path to predicted test values\n        y_test_actual_path - ath to actual test values \n        name_of_target - name of the target variable\n        dimension - type of variable; y-axis label\n        num_days_in_advance - how many days in advance the prediction is; defaults to 7\n\n    Returns:\n        fig - the fig (although this parameter isn't strictly necessary)\n\n    \"\"\"\n    \n    %matplotlib notebook\n    #calling it a second time may prevent some graphics errors\n    %matplotlib notebook  \n    import matplotlib.pyplot as plt\n\n    def plot_predictions_vs_test_values(pred, y_test, name_of_target, dimension, num_days_in_advance):\n        plt.clf()\n        x = pd.to_datetime(y_test.index)\n        y_test_values = y_test.iloc[:, num_days_in_advance - 1]\n        y_predicted_values = pred.iloc[:, num_days_in_advance - 1]\n        plt.plot(x, y_test_values, color='blue', alpha=0.7)\n        plt.plot(x, y_predicted_values, color='red', alpha=0.7)\n        plt.gca().fill_between(x, \n                               y_test_values, \n                               y_predicted_values, \n                               facecolor='purple', \n                               alpha=0.2)\n\n        rmse = mean_squared_error(y_test_values, y_predicted_values, squared=False)\n        mae = mean_absolute_error(y_test_values, y_predicted_values)\n        rmse = np.round(rmse, 3)\n        mae = np.round(mae, 3)\n\n        fig = plt.gcf()\n        if not kaggle_path:\n            fig.set_size_inches(13.5, 9.5)\n        ax = plt.gca()\n        plt.legend(['actual', 'predicted', 'difference'], fontsize='large')\n        plt.xlabel('Date', size=15)\n        plt.ylabel(dimension, size=15)\n        plt.title(name_of_target + ': LSTM', size=20)\n        plt.show()\n\n        ax.annotate('rmse: {}    mae: {}'.format(rmse, mae),\n                xy=(0.17, -0.015), xytext=(0, 10),\n                xycoords=('axes fraction', 'figure fraction'),\n                textcoords='offset points',\n                size=20, ha='center', va='bottom')\n\n        return fig\n\n    df_pred = pd.read_csv(df_pred_path)\n    y_test_actual = pd.read_csv(y_test_actual_path)\n    df_pred.set_index('Date', inplace=True)\n    y_test_actual.set_index('Date', inplace=True)\n    \n    pd.set_option('display.max_columns', 30)\n    diff = y_test_actual.subtract(df_pred)\n    mae_all = np.mean(abs(diff), axis=0)\n    rmse_all = np.sqrt(np.mean(abs(diff**2), axis=0))\n    mae_all = np.round(mae_all, 3)\n    rmse_all = np.round(rmse_all, 3)\n    errors_all = pd.DataFrame(data=[mae_all, rmse_all], index=['MAE', 'RMSE'], columns=y_test_actual.columns)\n    display(errors_all)\n    \n    plot_predictions_vs_test_values(df_pred, y_test_actual, name_of_target, dimension, num_days_in_advance=7)\n\n\n    def update(change):\n        plot_predictions_vs_test_values(df_pred, y_test_actual, name_of_target, dimension, change.new)\n\n    int_slider = widgets.IntSlider(\n        value=7, \n        min=1, max=30, step=1,\n        description='num_days',\n        continuous_update=False\n    )\n    int_slider.observe(update, 'value')\n    int_slider\n    \n    return int_slider\n\n\ndef plot_xgb_results(df_pred_path, y_test_actual_path, name_of_target, dimension, num_days_in_advance):\n    \"\"\"\n    Plot the actual vs. those predicted by XGBRegressor\n\n    Args:\n        df_pred_path - path to predicted test values\n        y_test_actual_path - path to actual test values \n        name_of_target - name of the target variable\n        dimension - type of variable; y-axis label\n        num_days_in_advance - how many days in advance the prediction is; defaults to 7\n\n    Returns:\n        fig - the fig (although this parameter isn't strictly necessary)\n\n    \"\"\"\n    \n    %matplotlib notebook\n\n    df_pred = pd.read_csv(df_pred_path)\n    y_test_actual = pd.read_csv(y_test_actual_path)\n    \n    plt.clf()\n    x = pd.to_datetime(y_test_actual.values[:, 0])\n    y_test_values = y_test_actual.values[:, 1].astype('float64')\n    y_predicted_values = df_pred.values[:, 1].astype('float64')\n    plt.plot(x, y_test_values, color='orange', alpha=0.7)\n    plt.plot(x, y_predicted_values, color='red', alpha=0.7)\n    plt.gca().fill_between(x, \n                           y_test_values, \n                           y_predicted_values, \n                           facecolor='yellow', \n                           alpha=0.2)\n\n    rmse = mean_squared_error(y_test_values, y_predicted_values, squared=False)\n    mae = mean_absolute_error(y_test_values, y_predicted_values)\n    rmse = np.round(rmse, 3)\n    mae = np.round(mae, 3)\n    \n    fig = plt.gcf()\n    fig.set_size_inches(13.5, 9.5)\n    ax = plt.gca()\n    plt.legend(['actual', 'predicted', 'difference'], fontsize='large')\n    plt.xlabel('Date', size=20)\n    plt.ylabel(dimension, size=20)\n    plt.title(name_of_target + ' {} days ahead: XGBRegressor'.format(num_days_in_advance), size=20)\n    plt.show()\n\n    ax.annotate('rmse: {}    mae: {}'.format(rmse, mae),\n            xy=(0.17, 0), xytext=(0, 30),\n            xycoords=('axes fraction', 'figure fraction'),\n            textcoords='offset points',\n            size=20, ha='center', va='bottom')\n\n    return ax","7ad9d7c5":"auser_targets = ['Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_CoS', 'Depth_to_Groundwater_LT2']\n\nname_of_temp_variable = 'Temperature_Orentano'\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Orentano'\n\n\nname_of_target = 'Depth_to_Groundwater_SAL'\nprepared_df_aquifer_Auser_SAL = df_aquifer_auser.copy()\nprepared_df_aquifer_Auser_SAL, targets_Auser_SAL, original_target_values_Auser_SAL, \\\n        lookback_dict_Auser_SAL, target_dict_Auser_SAL = \\\n                                                feature_engineering(prepared_df_aquifer_Auser_SAL, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Depth_to_Groundwater_CoS'\nprepared_df_aquifer_Auser_CoS = df_aquifer_auser.copy()\nprepared_df_aquifer_Auser_CoS, targets_Auser_CoS, original_target_values_Auser_CoS, \\\n        lookback_dict_Auser_CoS, target_dict_Auser_CoS = \\\n                                                feature_engineering(prepared_df_aquifer_Auser_CoS, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Depth_to_Groundwater_LT2'\nprepared_df_aquifer_Auser_LT2 = df_aquifer_auser.copy()\nprepared_df_aquifer_Auser_LT2, targets_Auser_LT2, original_target_values_Auser_LT2, \\\n        lookback_Auser_dict_LT2, target_dict_Auser_LT2 = \\\n                                                feature_engineering(prepared_df_aquifer_Auser_LT2, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","56f385a2":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_Auser_SAL, features_Auser_SAL, y_test_Auser_SAL, y_pred_Auser_SAL = \\\n    run_xgb(prepared_df_aquifer_Auser_SAL, targets_Auser_SAL, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","6922bcf0":"num_most_important_features = 20\nname_of_target = 'Depth_to_Groundwater_SAL'\nget_n_most_important_feature_indices(xgb_feature_importances_Auser_SAL, features_Auser_SAL, \n                                     name_of_target, num_most_important_features)","1c8efc48":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_Auser_CoS, features_Auser_CoS, y_test_Auser_CoS, y_pred_Auser_CoS = \\\n    run_xgb(prepared_df_aquifer_Auser_CoS, targets_Auser_CoS, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","f06df4de":"num_most_important_features = 20\nname_of_target = 'Depth_to_Groundwater_CoS'\nget_n_most_important_feature_indices(xgb_feature_importances_Auser_CoS, features_Auser_CoS, \n                                     name_of_target, num_most_important_features)","4a714f68":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_Auser_LT2, features_Auser_LT2, y_test_Auser_LT2, y_pred_Auser_LT2 = \\\n    run_xgb(prepared_df_aquifer_Auser_LT2, targets_Auser_LT2, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","c50c5f88":"num_most_important_features = 20\nname_of_target = 'Depth_to_Groundwater_LT2'\nget_n_most_important_feature_indices(xgb_feature_importances_Auser_LT2, features_Auser_LT2, \n                                     name_of_target, num_most_important_features)","f0c14c27":"# clear memory\ndel [prepared_df_aquifer_Auser_SAL, targets_Auser_SAL, original_target_values_Auser_SAL, \\\n        lookback_dict_Auser_SAL, target_dict_Auser_SAL,\n    prepared_df_aquifer_Auser_CoS, targets_Auser_CoS, original_target_values_Auser_CoS, \\\n        lookback_dict_Auser_CoS, target_dict_Auser_CoS,\n    prepared_df_aquifer_Auser_LT2, targets_Auser_LT2, original_target_values_Auser_LT2, \\\n        lookback_Auser_dict_LT2, target_dict_Auser_LT2]\n\ngc.collect()\n\n%memit","f359415c":"# now num_lookback_days == 0 so that we don't create lookback variables in each row\nnum_lookback_days = 0\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Orentano'\n\nname_of_target = 'Depth_to_Groundwater_SAL'\nprepared_df_aquifer_Auser_SAL = df_aquifer_auser.copy()\nprepared_df_aquifer_Auser_SAL, targets_Auser_SAL, original_target_values_Auser_SAL, \\\n        lookback_dict_Auser_SAL, target_dict_Auser_SAL = \\\n                                                feature_engineering(prepared_df_aquifer_Auser_SAL, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Depth_to_Groundwater_CoS'\nprepared_df_aquifer_Auser_CoS = df_aquifer_auser.copy()\nprepared_df_aquifer_Auser_CoS, targets_Auser_CoS, original_target_values_Auser_CoS, \\\n        lookback_dict_Auser_CoS, target_dict_Auser_CoS = \\\n                                                feature_engineering(prepared_df_aquifer_Auser_CoS, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Depth_to_Groundwater_LT2'\nprepared_df_aquifer_Auser_LT2 = df_aquifer_auser.copy()\nprepared_df_aquifer_Auser_LT2, targets_Auser_LT2, original_target_values_Auser_LT2, \\\n        lookback_Auser_dict_LT2, target_dict_Auser_LT2 = \\\n                                                feature_engineering(prepared_df_aquifer_Auser_LT2, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","3552b364":"original_target = 'Depth_to_Groundwater_SAL'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 7\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 8\nnum_epochs_train_prior_years = 2\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_Auser_SAL, y_test_actual_Auser_SAL, model_Auser_SAL = run_lstm(prepared_df_aquifer_Auser_SAL, \n            targets_Auser_SAL, num_lookback_days, num_days_to_predict, size_test_set, num_days_difference, \n            original_target, scale_y, walk_forward, leak_data, synthetic, num_train_years_prior, batch_size, \n            num_epochs, num_epochs_walk_forward, num_epochs_train_prior_years, rolling_sum)\n","abb5a5e4":"%matplotlib inline\nplot_widget_results(df_pred_Auser_SAL, y_test_actual_Auser_SAL, 'Depth_to_Groundwater_SAL', 'Meters', num_days_in_advance=7)","02ac2a78":"%matplotlib inline\nplot_xgb_results(y_pred_Auser_SAL, y_test_Auser_SAL, 'Depth_to_Groundwater_SAL', 'Meters', which_day_to_predict);","df238dae":"original_target = 'Depth_to_Groundwater_CoS'\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 7\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 8\nnum_epochs_train_prior_years = 2\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\n\ndf_pred_Auser_CoS, y_test_actual_Auser_CoS, model_Auser_CoS = run_lstm(prepared_df_aquifer_Auser_CoS, \n            targets_Auser_CoS, num_lookback_days, num_days_to_predict, size_test_set, num_days_difference, \n            original_target, scale_y, walk_forward, leak_data, synthetic, num_train_years_prior, batch_size, \n            num_epochs, num_epochs_walk_forward, num_epochs_train_prior_years, rolling_sum)\n","e643564c":"%matplotlib inline\nplot_widget_results(df_pred_Auser_CoS, y_test_actual_Auser_CoS, 'Depth_to_Groundwater_CoS', 'Meters', num_days_in_advance=7)","b57d3893":"%matplotlib inline\nplot_xgb_results(y_pred_Auser_CoS, y_test_Auser_CoS, 'Depth_to_Groundwater_Cos', 'Meters', which_day_to_predict);","cd6b523e":"original_target = 'Depth_to_Groundwater_LT2'\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 7\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 8\nnum_epochs_train_prior_years = 2\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_Auser_LT2, y_test_actual_Auser_LT2, model_Auser_LT2 = run_lstm(prepared_df_aquifer_Auser_LT2, \n            targets_Auser_LT2, num_lookback_days, num_days_to_predict, size_test_set, num_days_difference, \n            original_target, scale_y, walk_forward, leak_data, synthetic, num_train_years_prior, batch_size, \n            num_epochs, num_epochs_walk_forward, num_epochs_train_prior_years, rolling_sum)\n","10575e90":"%matplotlib inline\nplot_widget_results(df_pred_Auser_LT2, y_test_actual_Auser_LT2, 'Depth_to_Groundwater_LT2', 'Meters', num_days_in_advance=7)","3c154f2c":"%matplotlib inline\nplot_xgb_results(y_pred_Auser_LT2, y_test_Auser_LT2, 'Depth_to_Groundwater_LT2', 'Meters', which_day_to_predict);","e1116edc":"# clear memory\ndel [prepared_df_aquifer_Auser_SAL, targets_Auser_SAL, original_target_values_Auser_SAL, \\\n        lookback_dict_Auser_SAL, target_dict_Auser_SAL,\n    prepared_df_aquifer_Auser_CoS, targets_Auser_CoS, original_target_values_Auser_CoS, \\\n        lookback_dict_Auser_CoS, target_dict_Auser_CoS,\n    prepared_df_aquifer_Auser_LT2, targets_Auser_LT2, original_target_values_Auser_LT2, \\\n        lookback_Auser_dict_LT2, target_dict_Auser_LT2]\n\ngc.collect()\n\n%memit","f3015fa8":"doganella_targets = ['Depth_to_Groundwater_Pozzo_1', 'Depth_to_Groundwater_Pozzo_2', 'Depth_to_Groundwater_Pozzo_3']\n\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Monteporzio'\n\n\nname_of_target = 'Depth_to_Groundwater_Pozzo_1'\nprepared_df_aquifer_Doganella_Pozzo_1 = df_aquifer_doganella.copy()\nprepared_df_aquifer_Doganella_Pozzo_1, targets_Doganella_Pozzo_1, original_target_values_Doganella_Pozzo_1, \\\n        lookback_dict_Doganella_Pozzo_1, target_dict_Doganella_Pozzo_1 = \\\n                                                feature_engineering(prepared_df_aquifer_Doganella_Pozzo_1, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Depth_to_Groundwater_Pozzo_2'\nprepared_df_aquifer_Doganella_Pozzo_2 = df_aquifer_doganella.copy()\nprepared_df_aquifer_Doganella_Pozzo_2, targets_Doganella_Pozzo_2, original_target_values_Doganella_Pozzo_2, \\\n        lookback_dict_Doganella_Pozzo_2, target_dict_Doganella_Pozzo_2 = \\\n                                                feature_engineering(prepared_df_aquifer_Doganella_Pozzo_2, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Depth_to_Groundwater_Pozzo_3'\nprepared_df_aquifer_Doganella_Pozzo_3 = df_aquifer_doganella.copy().loc[\n                                                df_aquifer_doganella['Date'] < '2019-09-19', :]\nprepared_df_aquifer_Doganella_Pozzo_3, targets_Doganella_Pozzo_3, original_target_values_Doganella_Pozzo_3, \\\n        lookback_dict_Doganella_Pozzo_3, target_dict_Doganella_Pozzo_3 = \\\n                                                feature_engineering(prepared_df_aquifer_Doganella_Pozzo_3, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","65365221":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_Doganella_Pozzo_1, features_Doganella_Pozzo_1, y_test_Doganella_Pozzo_1, y_pred_Doganella_Pozzo_1 = \\\n    run_xgb(prepared_df_aquifer_Doganella_Pozzo_1, targets_Doganella_Pozzo_1, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","5f86f2da":"num_most_important_features = 20\nname_of_target = 'Depth_to_Groundwater_Pozzo_1'\nget_n_most_important_feature_indices(xgb_feature_importances_Doganella_Pozzo_1, features_Doganella_Pozzo_1, \n                                     name_of_target, num_most_important_features)","d3701a5f":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_Doganella_Pozzo_2, features_Doganella_Pozzo_2, y_test_Doganella_Pozzo_2, y_pred_Doganella_Pozzo_2 = \\\n    run_xgb(prepared_df_aquifer_Doganella_Pozzo_2, targets_Doganella_Pozzo_2, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","8bf06a8d":"num_most_important_features = 20\nname_of_target = 'Depth_to_Groundwater_Pozzo_2'\nget_n_most_important_feature_indices(xgb_feature_importances_Doganella_Pozzo_2, features_Doganella_Pozzo_2, \n                                     name_of_target, num_most_important_features)","f90de38a":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_Doganella_Pozzo_3, features_Doganella_Pozzo_3, y_test_Doganella_Pozzo_3, y_pred_Doganella_Pozzo_3 = \\\n    run_xgb(prepared_df_aquifer_Doganella_Pozzo_3, targets_Doganella_Pozzo_3, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","858f30f0":"num_most_important_features = 20\nname_of_target = 'Depth_to_Groundwater_Pozzo_3'\nget_n_most_important_feature_indices(xgb_feature_importances_Doganella_Pozzo_3, features_Doganella_Pozzo_3, \n                                     name_of_target, num_most_important_features)","1d10b20f":"del [prepared_df_aquifer_Doganella_Pozzo_1, targets_Doganella_Pozzo_1, original_target_values_Doganella_Pozzo_1, \\\n        lookback_dict_Doganella_Pozzo_1, target_dict_Doganella_Pozzo_1,\n    prepared_df_aquifer_Doganella_Pozzo_2, targets_Doganella_Pozzo_2, original_target_values_Doganella_Pozzo_2, \\\n        lookback_dict_Doganella_Pozzo_2, target_dict_Doganella_Pozzo_2,\n    prepared_df_aquifer_Doganella_Pozzo_3, targets_Doganella_Pozzo_3, original_target_values_Doganella_Pozzo_3, \\\n        lookback_dict_Doganella_Pozzo_3, target_dict_Doganella_Pozzo_3]\n\ngc.collect()\n\n%memit","30d830fe":"# now num_lookback_days == 0 so that we don't create lookback variables in each row\nnum_lookback_days = 0\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Monteporzio'\n\n\nname_of_target = 'Depth_to_Groundwater_Pozzo_1'\nprepared_df_aquifer_Doganella_Pozzo_1 = df_aquifer_doganella.copy()\nprepared_df_aquifer_Doganella_Pozzo_1, targets_Doganella_Pozzo_1, original_target_values_Doganella_Pozzo_1, \\\n        lookback_dict_Doganella_Pozzo_1, target_dict_Doganella_Pozzo_1 = \\\n                                                feature_engineering(prepared_df_aquifer_Doganella_Pozzo_1, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Depth_to_Groundwater_Pozzo_2'\nprepared_df_aquifer_Doganella_Pozzo_2 = df_aquifer_doganella.copy()\nprepared_df_aquifer_Doganella_Pozzo_2, targets_Doganella_Pozzo_2, original_target_values_Doganella_Pozzo_2, \\\n        lookback_dict_Doganella_Pozzo_2, target_dict_Doganella_Pozzo_2 = \\\n                                                feature_engineering(prepared_df_aquifer_Doganella_Pozzo_2, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Depth_to_Groundwater_Pozzo_3'\nprepared_df_aquifer_Doganella_Pozzo_3 = df_aquifer_doganella.copy().loc[\n                                                df_aquifer_doganella['Date'] < '2019-09-19', :]\nprepared_df_aquifer_Doganella_Pozzo_3, targets_Doganella_Pozzo_3, original_target_values_Doganella_Pozzo_3, \\\n        lookback_dict_Doganella_Pozzo_3, target_dict_Doganella_Pozzo_3 = \\\n                                                feature_engineering(prepared_df_aquifer_Doganella_Pozzo_3, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","6c323e69":"original_target = 'Depth_to_Groundwater_Pozzo_1'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 30\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 2\nnum_epochs_train_prior_years = 2\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_Doganella_Pozzo_1, y_test_actual_Doganella_Pozzo_1, model_Doganella_Pozzo_1 = \\\n            run_lstm(prepared_df_aquifer_Doganella_Pozzo_1, \n            targets_Doganella_Pozzo_1, num_lookback_days, num_days_to_predict, size_test_set, num_days_difference, \n            original_target, scale_y, walk_forward, leak_data, synthetic, num_train_years_prior, batch_size, \n            num_epochs, num_epochs_walk_forward, num_epochs_train_prior_years, rolling_sum)\n","8d597ffe":"%matplotlib inline\nplot_widget_results(df_pred_Doganella_Pozzo_1, y_test_actual_Doganella_Pozzo_1, 'Depth_to_Groundwater_Pozzo_1', 'Meters', num_days_in_advance=7)","8b1bb86d":"%matplotlib inline\nplot_xgb_results(y_pred_Doganella_Pozzo_1, y_test_Doganella_Pozzo_1, 'Depth_to_Groundwater_Pozzo_1', 'Meters', which_day_to_predict);","1e9d96eb":"original_target = 'Depth_to_Groundwater_Pozzo_2'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 30\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 2\nnum_epochs_train_prior_years = 2\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_Doganella_Pozzo_2, y_test_actual_Doganella_Pozzo_2, model_Doganella_Pozzo_2 = \\\n            run_lstm(prepared_df_aquifer_Doganella_Pozzo_2, \n            targets_Doganella_Pozzo_2, num_lookback_days, num_days_to_predict, size_test_set, num_days_difference, \n            original_target, scale_y, walk_forward, leak_data, synthetic, num_train_years_prior, batch_size, \n            num_epochs, num_epochs_walk_forward, num_epochs_train_prior_years, rolling_sum)\n","f4b9ec9d":"%matplotlib inline\nplot_widget_results(df_pred_Doganella_Pozzo_2, y_test_actual_Doganella_Pozzo_2, 'Depth_to_Groundwater_Pozzo_2', 'Meters', num_days_in_advance=7)","9c0d9f2c":"%matplotlib inline\nplot_xgb_results(y_pred_Doganella_Pozzo_2, y_test_Doganella_Pozzo_2, 'Depth_to_Groundwater_Pozzo_2', 'Meters', which_day_to_predict);","d92d1b36":"original_target = 'Depth_to_Groundwater_Pozzo_3'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 30\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 2\nnum_epochs_train_prior_years = 2\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_Doganella_Pozzo_3, y_test_actual_Doganella_Pozzo_3, model_Doganella_Pozzo_3 = \\\n            run_lstm(prepared_df_aquifer_Doganella_Pozzo_3, \n            targets_Doganella_Pozzo_3, num_lookback_days, num_days_to_predict, size_test_set, num_days_difference, \n            original_target, scale_y, walk_forward, leak_data, synthetic, num_train_years_prior, batch_size, \n            num_epochs, num_epochs_walk_forward, num_epochs_train_prior_years, rolling_sum)\n","a7962e49":"%matplotlib inline\nplot_widget_results(df_pred_Doganella_Pozzo_3, y_test_actual_Doganella_Pozzo_3, 'Depth_to_Groundwater_Pozzo_3', 'Meters', num_days_in_advance=7)","b06231a5":"%matplotlib inline\nplot_xgb_results(y_pred_Doganella_Pozzo_3, y_test_Doganella_Pozzo_3, 'Depth_to_Groundwater_Pozzo_3', 'Meters', which_day_to_predict);","e3b1b92e":"del [prepared_df_aquifer_Doganella_Pozzo_1, targets_Doganella_Pozzo_1, original_target_values_Doganella_Pozzo_1, \\\n        lookback_dict_Doganella_Pozzo_1, target_dict_Doganella_Pozzo_1,\n    prepared_df_aquifer_Doganella_Pozzo_2, targets_Doganella_Pozzo_2, original_target_values_Doganella_Pozzo_2, \\\n        lookback_dict_Doganella_Pozzo_2, target_dict_Doganella_Pozzo_2,\n    prepared_df_aquifer_Doganella_Pozzo_3, targets_Doganella_Pozzo_3, original_target_values_Doganella_Pozzo_3, \\\n        lookback_dict_Doganella_Pozzo_3, target_dict_Doganella_Pozzo_3]\n\ngc.collect()\n\n%memit","c70cbc59":"luco_targets = ['Depth_to_Groundwater_Podere_Casetta']\n\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Monteroni_Arbia_Biena'\n\n\nname_of_target = 'Depth_to_Groundwater_Podere_Casetta'\nprepared_df_aquifer_Luco = df_aquifer_luco.copy()\nprepared_df_aquifer_Luco, targets_Luco, original_target_values_Luco, \\\n        lookback_dict_Luco, target_dict_Luco = feature_engineering(prepared_df_aquifer_Luco, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)","de4cbbdb":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_Luco, features_Luco, y_test_Luco, y_pred_Luco = \\\n    run_xgb(prepared_df_aquifer_Luco, targets_Luco, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","902ada19":"num_most_important_features = 20\nname_of_target = 'Depth_to_Groundwater_Podere_Casetta'\nget_n_most_important_feature_indices(xgb_feature_importances_Luco, features_Luco, name_of_target, num_most_important_features)","45a419e6":"del [prepared_df_aquifer_Luco, targets_Luco, original_target_values_Luco, \\\n        lookback_dict_Luco, target_dict_Luco]\n\ngc.collect()\n\n%memit","968f31cf":"# now num_lookback_days == 0 so that we don't create lookback variables in each row\nnum_lookback_days = 0\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Monteroni_Arbia_Biena'\n\n\nname_of_target = 'Depth_to_Groundwater_Podere_Casetta'\nprepared_df_aquifer_Luco = df_aquifer_luco.copy()\nprepared_df_aquifer_Luco, targets_Luco, original_target_values_Luco, \\\n        lookback_dict_Luco, target_dict_Luco = feature_engineering(prepared_df_aquifer_Luco, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\n\noriginal_target = 'Depth_to_Groundwater_Podere_Casetta'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 30\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 3\nnum_epochs_train_prior_years = 2\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_Luco, y_test_actual_Luco, model_Luco = \\\n            run_lstm(prepared_df_aquifer_Luco, \n            targets_Luco, num_lookback_days, num_days_to_predict, size_test_set, num_days_difference, \n            original_target, scale_y, walk_forward, leak_data, synthetic, num_train_years_prior, batch_size, \n            num_epochs, num_epochs_walk_forward, num_epochs_train_prior_years, rolling_sum)\n","9cd725f7":"%matplotlib inline\nplot_widget_results(df_pred_Luco, y_test_actual_Luco, 'Depth_to_Groundwater_Podere_Casetta', 'Meters', num_days_in_advance=7)","f29a2151":"%matplotlib inline\nplot_xgb_results(y_pred_Luco, y_test_Luco, 'Depth_to_Groundwater_Podere_Casetta', 'Meters', which_day_to_predict);","5ae8ac2f":"del [prepared_df_aquifer_Luco, targets_Luco, original_target_values_Luco, \\\n        lookback_dict_Luco, target_dict_Luco]\n\ngc.collect()\n\n%memit","c67f7d73":"petrignano_targets = ['Depth_to_Groundwater_P24', 'Depth_to_Groundwater_P25']\n\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Bastia_Umbra'\n\n\nname_of_target = 'Depth_to_Groundwater_P24'\nprepared_df_aquifer_petrignano_P24 = df_aquifer_petrignano.copy()\nprepared_df_aquifer_petrignano_P24, targets_petrignano_P24, original_target_values_petrignano_P24, \\\n        lookback_dict_petrignano_P24, target_dict_petrignano_P24 = \\\n                                                feature_engineering(prepared_df_aquifer_petrignano_P24, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Depth_to_Groundwater_P25'\nprepared_df_aquifer_petrignano_P25 = df_aquifer_petrignano.copy()\nprepared_df_aquifer_petrignano_P25, targets_petrignano_P25, original_target_values_petrignano_P25, \\\n        lookback_dict_petrignano_P25, target_dict_petrignano_P25 = \\\n                                                feature_engineering(prepared_df_aquifer_petrignano_P25, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","adb79fc4":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_petrignano_P24, features_petrignano_P24, y_test_petrignano_P24, y_pred_petrignano_P24 = \\\n    run_xgb(prepared_df_aquifer_petrignano_P24, targets_petrignano_P24, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","708ec2c4":"num_most_important_features = 20\nname_of_target = 'Depth_to_Groundwater_P24'\nget_n_most_important_feature_indices(xgb_feature_importances_petrignano_P24, features_petrignano_P24, \n                                     name_of_target, num_most_important_features)","87ed651e":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_petrignano_P25, features_petrignano_P25, y_test_petrignano_P25, y_pred_petrignano_P25 = \\\n    run_xgb(prepared_df_aquifer_petrignano_P25, targets_petrignano_P25, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","124e4254":"num_most_important_features = 20\nname_of_target = 'Depth_to_Groundwater_P25'\nget_n_most_important_feature_indices(xgb_feature_importances_petrignano_P25, features_petrignano_P25, \n                                     name_of_target, num_most_important_features)","65b66ca5":"petrignano_targets = ['Depth_to_Groundwater_P24', 'Depth_to_Groundwater_P25']\n\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Bastia_Umbra'\n\n\nname_of_target = 'Depth_to_Groundwater_P24'\nprepared_df_aquifer_petrignano_P24 = df_aquifer_petrignano.copy().drop(columns=['Depth_to_Groundwater_P25'])\nprepared_df_aquifer_petrignano_P24, targets_petrignano_P24, original_target_values_petrignano_P24, \\\n        lookback_dict_petrignano_P24, target_dict_petrignano_P24 = \\\n                                                feature_engineering(prepared_df_aquifer_petrignano_P24, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Depth_to_Groundwater_P25'\nprepared_df_aquifer_petrignano_P25 = df_aquifer_petrignano.copy().drop(columns=['Depth_to_Groundwater_P24'])\nprepared_df_aquifer_petrignano_P25, targets_petrignano_P25, original_target_values_petrignano_P25, \\\n        lookback_dict_petrignano_P25, target_dict_petrignano_P25 = \\\n                                                feature_engineering(prepared_df_aquifer_petrignano_P25, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","722c5ed1":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_petrignano_P24, features_petrignano_P24, y_test_petrignano_P24, y_pred_petrignano_P24 = \\\n    run_xgb(prepared_df_aquifer_petrignano_P24, targets_petrignano_P24, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","9e24cd12":"num_most_important_features = 20\nname_of_target = 'Depth_to_Groundwater_P24'\nget_n_most_important_feature_indices(xgb_feature_importances_petrignano_P24, features_petrignano_P24, \n                                     name_of_target, num_most_important_features)","02a3bbab":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_petrignano_P25, features_petrignano_P25, y_test_petrignano_P25, y_pred_petrignano_P25 = \\\n    run_xgb(prepared_df_aquifer_petrignano_P25, targets_petrignano_P25, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","0b3062fa":"num_most_important_features = 20\nname_of_target = 'Depth_to_Groundwater_P25'\nget_n_most_important_feature_indices(xgb_feature_importances_petrignano_P25, features_petrignano_P25, \n                                     name_of_target, num_most_important_features)","9a4f620d":"del [prepared_df_aquifer_petrignano_P24, targets_petrignano_P24, original_target_values_petrignano_P24, \\\n        lookback_dict_petrignano_P24, target_dict_petrignano_P24,\n    prepared_df_aquifer_petrignano_P25, targets_petrignano_P25, original_target_values_petrignano_P25, \\\n        lookback_dict_petrignano_P25, target_dict_petrignano_P25]\n\ngc.collect()\n\n%memit","b59dd046":"# now num_lookback_days == 0 so that we don't create lookback variables in each row\nnum_lookback_days = 0\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Bastia_Umbra'\n\n\nname_of_target = 'Depth_to_Groundwater_P24'\nprepared_df_aquifer_petrignano_P24 = df_aquifer_petrignano.copy()\nprepared_df_aquifer_petrignano_P24, targets_petrignano_P24, original_target_values_petrignano_P24, \\\n        lookback_dict_petrignano_P24, target_dict_petrignano_P24 = \\\n                                                feature_engineering(prepared_df_aquifer_petrignano_P24, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Depth_to_Groundwater_P25'\nprepared_df_aquifer_petrignano_P25 = df_aquifer_petrignano.copy()\nprepared_df_aquifer_petrignano_P25, targets_petrignano_P25, original_target_values_petrignano_P25, \\\n        lookback_dict_petrignano_P25, target_dict_petrignano_P25 = \\\n                                                feature_engineering(prepared_df_aquifer_petrignano_P25, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","2f77547c":"original_target = 'Depth_to_Groundwater_P24'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 30\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 8\nnum_epochs_train_prior_years = 2\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_petrignano_P24, y_test_actual_petrignano_P24, model_petrignano_P24 = \\\n            run_lstm(prepared_df_aquifer_petrignano_P24, \n            targets_petrignano_P24, num_lookback_days, num_days_to_predict, size_test_set, num_days_difference, \n            original_target, scale_y, walk_forward, leak_data, synthetic, num_train_years_prior, batch_size, \n            num_epochs, num_epochs_walk_forward, num_epochs_train_prior_years, rolling_sum)\n","0ee054b4":"%matplotlib inline\nplot_widget_results(df_pred_petrignano_P24, y_test_actual_petrignano_P24, 'Depth_to_Groundwater_P24', 'Meters', num_days_in_advance=7)","9d1f90a4":"%matplotlib inline\nplot_xgb_results(y_pred_petrignano_P24, y_test_petrignano_P24, 'Depth_to_Groundwater_P24', 'Meters', which_day_to_predict);","64ad9242":"original_target = 'Depth_to_Groundwater_P25'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 30\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 8\nnum_epochs_train_prior_years = 2\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_petrignano_P25, y_test_actual_petrignano_P25, model_petrignano_P25 = \\\n            run_lstm(prepared_df_aquifer_petrignano_P25, \n            targets_petrignano_P25, num_lookback_days, num_days_to_predict, size_test_set, num_days_difference, \n            original_target, scale_y, walk_forward, leak_data, synthetic, num_train_years_prior, batch_size, \n            num_epochs, num_epochs_walk_forward, num_epochs_train_prior_years, rolling_sum)\n","68719944":"%matplotlib inline\nplot_widget_results(df_pred_petrignano_P25, y_test_actual_petrignano_P25, 'Depth_to_Groundwater_P25', 'Meters', num_days_in_advance=7)","8aae12cd":"%matplotlib inline\nplot_xgb_results(y_pred_petrignano_P25, y_test_petrignano_P25, 'Depth_to_Groundwater_P25', 'Meters', which_day_to_predict);","0f8cf0f7":"del [prepared_df_aquifer_petrignano_P24, targets_petrignano_P24, original_target_values_petrignano_P24, \\\n        lookback_dict_petrignano_P24, target_dict_petrignano_P24,\n    prepared_df_aquifer_petrignano_P25, targets_petrignano_P25, original_target_values_petrignano_P25, \\\n        lookback_dict_petrignano_P25, target_dict_petrignano_P25]\n\ngc.collect()\n\n%memit","6d7bf296":"lake_bilancino_targets = ['Lake_Level', 'Flow_Rate']\n\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Le_Croci'\n\n\nname_of_target = 'Lake_Level'\nprepared_df_lake_bilancino_level = df_lake_bilancino.copy()\nprepared_df_lake_bilancino_level, targets_lake_bilancino_level, original_target_values_lake_bilancino_level, \\\n        lookback_dict_lake_bilancino_level, target_dict_lake_bilancino_level = \\\n                                                feature_engineering(prepared_df_lake_bilancino_level, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Flow_Rate'\nprepared_df_lake_bilancino_flow = df_lake_bilancino.copy()\nprepared_df_lake_bilancino_flow, targets_lake_bilancino_flow, original_target_values_lake_bilancino_flow, \\\n        lookback_dict_lake_bilancino_flow, target_dict_lake_bilancino_flow = \\\n                                                feature_engineering(prepared_df_lake_bilancino_flow, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","0ce8bf9d":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_lake_bilancino_level, features_lake_bilancino_level, y_test_lake_bilancino_level, \\\n    y_pred_lake_bilancino_level = run_xgb(prepared_df_lake_bilancino_level, targets_lake_bilancino_level, \n                                          test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","3b8e4957":"num_most_important_features = 20\nname_of_target = 'Lake_Level'\nget_n_most_important_feature_indices(xgb_feature_importances_lake_bilancino_level, features_lake_bilancino_level, \n                                     name_of_target, num_most_important_features)","314f5e8d":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_lake_bilancino_flow, features_lake_bilancino_flow, y_test_lake_bilancino_flow, \\\n    y_pred_lake_bilancino_flow = run_xgb(prepared_df_lake_bilancino_flow, targets_lake_bilancino_flow, \n                                          test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","387b6122":"num_most_important_features = 20\nname_of_target = 'Flow_Rate'\nget_n_most_important_feature_indices(xgb_feature_importances_lake_bilancino_flow, features_lake_bilancino_flow, \n\n                                     name_of_target, num_most_important_features)","96b13901":"del [prepared_df_lake_bilancino_level, targets_lake_bilancino_level, original_target_values_lake_bilancino_level, \\\n        lookback_dict_lake_bilancino_level, target_dict_lake_bilancino_level,\n    prepared_df_lake_bilancino_flow, targets_lake_bilancino_flow, original_target_values_lake_bilancino_flow, \\\n        lookback_dict_lake_bilancino_flow, target_dict_lake_bilancino_flow]\n\ngc.collect()\n\n%memit","ae81e4b2":"# now num_lookback_days == 0 so that we don't create lookback variables in each row\nnum_lookback_days = 0\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Le_Croci'\n\n\nname_of_target = 'Lake_Level'\nprepared_df_lake_bilancino_level = df_lake_bilancino.copy()\nprepared_df_lake_bilancino_level, targets_lake_bilancino_level, original_target_values_lake_bilancino_level, \\\n        lookback_dict_lake_bilancino_level, target_dict_lake_bilancino_level = \\\n                                                feature_engineering(prepared_df_lake_bilancino_level, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Flow_Rate'\nprepared_df_lake_bilancino_flow = df_lake_bilancino.copy()\nprepared_df_lake_bilancino_flow, targets_lake_bilancino_flow, original_target_values_lake_bilancino_flow, \\\n        lookback_dict_lake_bilancino_flow, target_dict_lake_bilancino_flow = \\\n                                                feature_engineering(prepared_df_lake_bilancino_flow, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","a59b71e2":"original_target = 'Lake_Level'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 30\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 12\nnum_epochs_train_prior_years = 1\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_lake_bilancino_level, y_test_actual_lake_bilancino_level, model_lake_bilancino_level = \\\n            run_lstm(prepared_df_lake_bilancino_level, targets_lake_bilancino_level, num_lookback_days, \n            num_days_to_predict, size_test_set, num_days_difference, original_target, scale_y, walk_forward, \n            leak_data, synthetic, num_train_years_prior, batch_size, num_epochs, num_epochs_walk_forward, \n                     num_epochs_train_prior_years, rolling_sum)\n","9a382337":"%matplotlib inline\nplot_widget_results(df_pred_lake_bilancino_level, y_test_actual_lake_bilancino_level, 'Lake_Level', 'Meters', num_days_in_advance=7)","fcf49df5":"%matplotlib inline\nplot_xgb_results(y_pred_lake_bilancino_level, y_test_lake_bilancino_level, 'Lake_Level', 'Meters', which_day_to_predict);","0d565709":"original_target = 'Flow_Rate'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 2\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 12\nnum_epochs_train_prior_years = 1\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_lake_bilancino_flow, y_test_actual_lake_bilancino_flow, model_lake_bilancino_flow = \\\n            run_lstm(prepared_df_lake_bilancino_flow, targets_lake_bilancino_flow, num_lookback_days, \n            num_days_to_predict, size_test_set, num_days_difference, original_target, scale_y, walk_forward, \n            leak_data, synthetic, num_train_years_prior, batch_size, num_epochs, num_epochs_walk_forward, \n                     num_epochs_train_prior_years, rolling_sum)\n","45bb02a0":"%matplotlib inline\nplot_widget_results(df_pred_lake_bilancino_flow, y_test_actual_lake_bilancino_flow, 'Flow_Rate', '$m^3 \/ s$', num_days_in_advance=7)","14be7d91":"%matplotlib inline\nplot_xgb_results(y_pred_lake_bilancino_flow, y_test_lake_bilancino_flow, 'Flow_Rate', '$m^3 \/ s$', which_day_to_predict);","cb1046ab":"del [prepared_df_lake_bilancino_level, targets_lake_bilancino_level, original_target_values_lake_bilancino_level, \\\n        lookback_dict_lake_bilancino_level, target_dict_lake_bilancino_level,\n    prepared_df_lake_bilancino_flow, targets_lake_bilancino_flow, original_target_values_lake_bilancino_flow, \\\n        lookback_dict_lake_bilancino_flow, target_dict_lake_bilancino_flow]\n\ngc.collect()\n\n%memit","97f3d2d2":"water_spring_amiata_targets = ['Flow_Rate_Bugnano', 'Flow_Rate_Arbure', 'Flow_Rate_Ermicciolo', 'Flow_Rate_Galleria_Alta']\n\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_S_Fiora'\n\n\nname_of_target = 'Flow_Rate_Bugnano'\nprepared_df_amiata_flow_rate_bugnano = df_water_spring_amiata.copy()\nprepared_df_amiata_flow_rate_bugnano, targets_amiata_flow_rate_bugnano, original_target_values_amiata_flow_rate_bugnano, \\\n        lookback_dict_amiata_flow_rate_bugnano, target_dict_amiata_flow_rate_bugnano = \\\n                                                feature_engineering(prepared_df_amiata_flow_rate_bugnano, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Flow_Rate_Arbure'\nprepared_df_amiata_flow_rate_arbure = df_water_spring_amiata.copy()\nprepared_df_amiata_flow_rate_arbure, targets_amiata_flow_rate_arbure, original_target_values_amiata_flow_rate_arbure, \\\n        lookback_dict_amiata_flow_rate_arbure, target_dict_amiata_flow_rate_arbure = \\\n                                                feature_engineering(prepared_df_amiata_flow_rate_arbure, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Flow_Rate_Ermicciolo'\nprepared_df_amiata_flow_rate_ermicciolo = df_water_spring_amiata.copy()\nprepared_df_amiata_flow_rate_ermicciolo, targets_amiata_flow_rate_ermicciolo, original_target_values_amiata_flow_rate_ermicciolo, \\\n        lookback_dict_amiata_flow_rate_ermicciolo, target_dict_amiata_flow_rate_ermicciolo = \\\n                                                feature_engineering(prepared_df_amiata_flow_rate_ermicciolo, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Flow_Rate_Galleria_Alta'\nprepared_df_amiata_flow_rate_galleria_alta = df_water_spring_amiata.copy()\nprepared_df_amiata_flow_rate_galleria_alta, targets_amiata_flow_rate_galleria_alta, original_target_values_amiata_flow_rate_galleria_alta, \\\n        lookback_dict_amiata_flow_rate_galleria_alta, target_dict_amiata_flow_rate_galleria_alta = \\\n                                                feature_engineering(prepared_df_amiata_flow_rate_galleria_alta, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","82402521":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_amiata_flow_rate_bugnano, features_amiata_flow_rate_bugnano, y_test_amiata_flow_rate_bugnano, \\\n                            y_pred_amiata_flow_rate_bugnano = run_xgb(prepared_df_amiata_flow_rate_bugnano, \\\n                            targets_amiata_flow_rate_bugnano, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","98c642ba":"num_most_important_features = 20\nname_of_target = 'Flow_Rate_Bugnano'\nget_n_most_important_feature_indices(xgb_feature_importances_amiata_flow_rate_bugnano, features_amiata_flow_rate_bugnano, \n                                     name_of_target, num_most_important_features)","75dd5256":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_amiata_flow_rate_arbure, features_amiata_flow_rate_arbure, y_test_amiata_flow_rate_arbure, \\\n                            y_pred_amiata_flow_rate_arbure = run_xgb(prepared_df_amiata_flow_rate_arbure, \\\n                            targets_amiata_flow_rate_arbure, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","ed2be2b0":"num_most_important_features = 20\nname_of_target = 'Flow_Rate_Arbure'\nget_n_most_important_feature_indices(xgb_feature_importances_amiata_flow_rate_arbure, features_amiata_flow_rate_arbure, \n                                     name_of_target, num_most_important_features)","3ee4d436":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_amiata_flow_rate_ermicciolo, features_amiata_flow_rate_ermicciolo, y_test_amiata_flow_rate_ermicciolo, \\\n                            y_pred_amiata_flow_rate_ermicciolo = run_xgb(prepared_df_amiata_flow_rate_ermicciolo, \\\n                            targets_amiata_flow_rate_ermicciolo, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","3454b8c6":"num_most_important_features = 20\nname_of_target = 'Flow_Rate_Ermicciolo'\nget_n_most_important_feature_indices(xgb_feature_importances_amiata_flow_rate_ermicciolo, features_amiata_flow_rate_ermicciolo, \n                                     name_of_target, num_most_important_features)","4770d25a":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_amiata_flow_rate_galleria_alta, features_amiata_flow_rate_galleria_alta, y_test_amiata_flow_rate_galleria_alta, \\\n                            y_pred_amiata_flow_rate_galleria_alta = run_xgb(prepared_df_amiata_flow_rate_galleria_alta, \\\n                            targets_amiata_flow_rate_galleria_alta, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","713c6a09":"num_most_important_features = 20\nname_of_target = 'Flow_Rate_Galleria_Alta'\nget_n_most_important_feature_indices(xgb_feature_importances_amiata_flow_rate_galleria_alta, features_amiata_flow_rate_galleria_alta, \n                                     name_of_target, num_most_important_features)","3f9f7eeb":"del [prepared_df_amiata_flow_rate_bugnano, targets_amiata_flow_rate_bugnano, original_target_values_amiata_flow_rate_bugnano, \\\n        lookback_dict_amiata_flow_rate_bugnano, target_dict_amiata_flow_rate_bugnano,\n    prepared_df_amiata_flow_rate_arbure, targets_amiata_flow_rate_arbure, original_target_values_amiata_flow_rate_arbure, \\\n        lookback_dict_amiata_flow_rate_arbure, target_dict_amiata_flow_rate_arbure,\n    prepared_df_amiata_flow_rate_ermicciolo, targets_amiata_flow_rate_ermicciolo, original_target_values_amiata_flow_rate_ermicciolo, \\\n        lookback_dict_amiata_flow_rate_ermicciolo, target_dict_amiata_flow_rate_ermicciolo,\n    prepared_df_amiata_flow_rate_galleria_alta, targets_amiata_flow_rate_galleria_alta, original_target_values_amiata_flow_rate_galleria_alta, \\\n        lookback_dict_amiata_flow_rate_galleria_alta, target_dict_amiata_flow_rate_galleria_alta]\n\ngc.collect()\n\n%memit","0759a5c5":"# now num_lookback_days == 0 so that we don't create lookback variables in each row\nnum_lookback_days = 0\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_S_Fiora'\n\n\nname_of_target = 'Flow_Rate_Bugnano'\nprepared_df_amiata_flow_rate_bugnano = df_water_spring_amiata.copy()\nprepared_df_amiata_flow_rate_bugnano, targets_amiata_flow_rate_bugnano, original_target_values_amiata_flow_rate_bugnano, \\\n        lookback_dict_amiata_flow_rate_bugnano, target_dict_amiata_flow_rate_bugnano = \\\n                                                feature_engineering(prepared_df_amiata_flow_rate_bugnano, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Flow_Rate_Arbure'\nprepared_df_amiata_flow_rate_arbure = df_water_spring_amiata.copy()\nprepared_df_amiata_flow_rate_arbure, targets_amiata_flow_rate_arbure, original_target_values_amiata_flow_rate_arbure, \\\n        lookback_dict_amiata_flow_rate_arbure, target_dict_amiata_flow_rate_arbure = \\\n                                                feature_engineering(prepared_df_amiata_flow_rate_arbure, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Flow_Rate_Ermicciolo'\nprepared_df_amiata_flow_rate_ermicciolo = df_water_spring_amiata.copy()\nprepared_df_amiata_flow_rate_ermicciolo, targets_amiata_flow_rate_ermicciolo, original_target_values_amiata_flow_rate_ermicciolo, \\\n        lookback_dict_amiata_flow_rate_ermicciolo, target_dict_amiata_flow_rate_ermicciolo = \\\n                                                feature_engineering(prepared_df_amiata_flow_rate_ermicciolo, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\nname_of_target = 'Flow_Rate_Galleria_Alta'\nprepared_df_amiata_flow_rate_galleria_alta = df_water_spring_amiata.copy()\nprepared_df_amiata_flow_rate_galleria_alta, targets_amiata_flow_rate_galleria_alta, original_target_values_amiata_flow_rate_galleria_alta, \\\n        lookback_dict_amiata_flow_rate_galleria_alta, target_dict_amiata_flow_rate_galleria_alta = \\\n                                                feature_engineering(prepared_df_amiata_flow_rate_galleria_alta, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","22908d10":"original_target = 'Flow_Rate_Bugnano'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 2\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 4\nnum_epochs_train_prior_years = 1\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_amiata_flow_rate_bugnano, y_test_actual_amiata_flow_rate_bugnano, model_amiata_flow_rate_bugnano = \\\n            run_lstm(prepared_df_amiata_flow_rate_bugnano, targets_amiata_flow_rate_bugnano, num_lookback_days, \n            num_days_to_predict, size_test_set, num_days_difference, original_target, scale_y, walk_forward, \n            leak_data, synthetic, num_train_years_prior, batch_size, num_epochs, num_epochs_walk_forward, \n                     num_epochs_train_prior_years, rolling_sum)\n","2fa1f67f":"%matplotlib inline\nplot_widget_results(df_pred_amiata_flow_rate_bugnano, y_test_actual_amiata_flow_rate_bugnano, 'Flow_Rate_Bugnano', '$Liters \/ s$', num_days_in_advance=7)","06a4bf72":"%matplotlib inline\nplot_xgb_results(y_pred_amiata_flow_rate_bugnano, y_test_amiata_flow_rate_bugnano, 'Flow_Rate_Bugnano', '$Liters \/ s$', which_day_to_predict);","c4a40d9e":"original_target = 'Flow_Rate_Arbure'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 2\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 4\nnum_epochs_train_prior_years = 1\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_amiata_flow_rate_arbure, y_test_actual_amiata_flow_rate_arbure, model_amiata_flow_rate_arbure = \\\n            run_lstm(prepared_df_amiata_flow_rate_arbure, targets_amiata_flow_rate_arbure, num_lookback_days, \n            num_days_to_predict, size_test_set, num_days_difference, original_target, scale_y, walk_forward, \n            leak_data, synthetic, num_train_years_prior, batch_size, num_epochs, num_epochs_walk_forward, \n                     num_epochs_train_prior_years, rolling_sum)\n","20032ab5":"%matplotlib inline\nplot_widget_results(df_pred_amiata_flow_rate_arbure, y_test_actual_amiata_flow_rate_arbure, 'Flow_Rate_Arbure', '$Liters \/ s$', num_days_in_advance=7)","cd15ec0e":"%matplotlib inline\nplot_xgb_results(y_pred_amiata_flow_rate_arbure, y_test_amiata_flow_rate_arbure, 'Flow_Rate_Arbure', '$Liters \/ s$', which_day_to_predict);","a335edd5":"original_target = 'Flow_Rate_Ermicciolo'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 2\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 4\nnum_epochs_train_prior_years = 1\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_amiata_flow_rate_ermicciolo, y_test_actual_amiata_flow_rate_ermicciolo, model_amiata_flow_rate_ermicciolo = \\\n            run_lstm(prepared_df_amiata_flow_rate_ermicciolo, targets_amiata_flow_rate_ermicciolo, num_lookback_days, \n            num_days_to_predict, size_test_set, num_days_difference, original_target, scale_y, walk_forward, \n            leak_data, synthetic, num_train_years_prior, batch_size, num_epochs, num_epochs_walk_forward, \n                     num_epochs_train_prior_years, rolling_sum)\n","cf224299":"%matplotlib inline\nplot_widget_results(df_pred_amiata_flow_rate_ermicciolo, y_test_actual_amiata_flow_rate_ermicciolo, 'Flow_Rate_Ermicciolo', '$Liters \/ s$', num_days_in_advance=7)","50651d89":"%matplotlib inline\nplot_xgb_results(y_pred_amiata_flow_rate_ermicciolo, y_test_amiata_flow_rate_ermicciolo, 'Flow_Rate_Ermicciolo', '$Liters \/ s$', which_day_to_predict);","a4667291":"original_target = 'Flow_Rate_Galleria_Alta'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 2\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 4\nnum_epochs_train_prior_years = 1\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_amiata_flow_rate_galleria_alta, y_test_actual_amiata_flow_rate_galleria_alta, model_amiata_flow_rate_galleria_alta = \\\n            run_lstm(prepared_df_amiata_flow_rate_galleria_alta, targets_amiata_flow_rate_galleria_alta, num_lookback_days, \n            num_days_to_predict, size_test_set, num_days_difference, original_target, scale_y, walk_forward, \n            leak_data, synthetic, num_train_years_prior, batch_size, num_epochs, num_epochs_walk_forward, \n                     num_epochs_train_prior_years, rolling_sum)\n","ccbafe42":"%matplotlib inline\nplot_widget_results(df_pred_amiata_flow_rate_galleria_alta, y_test_actual_amiata_flow_rate_galleria_alta, 'Flow_Rate_Galleria_Alta', '$Liters \/ s$', num_days_in_advance=7)","961d190f":"%matplotlib inline\nplot_xgb_results(y_pred_amiata_flow_rate_galleria_alta, y_test_amiata_flow_rate_galleria_alta, 'Flow_Rate_Galleria_Alta', '$Liters \/ s$', which_day_to_predict);","d5463ba2":"del [prepared_df_amiata_flow_rate_bugnano, targets_amiata_flow_rate_bugnano, original_target_values_amiata_flow_rate_bugnano, \\\n        lookback_dict_amiata_flow_rate_bugnano, target_dict_amiata_flow_rate_bugnano,\n    prepared_df_amiata_flow_rate_arbure, targets_amiata_flow_rate_arbure, original_target_values_amiata_flow_rate_arbure, \\\n        lookback_dict_amiata_flow_rate_arbure, target_dict_amiata_flow_rate_arbure,\n    prepared_df_amiata_flow_rate_ermicciolo, targets_amiata_flow_rate_ermicciolo, original_target_values_amiata_flow_rate_ermicciolo, \\\n        lookback_dict_amiata_flow_rate_ermicciolo, target_dict_amiata_flow_rate_ermicciolo,\n    prepared_df_amiata_flow_rate_galleria_alta, targets_amiata_flow_rate_galleria_alta, original_target_values_amiata_flow_rate_galleria_alta, \\\n        lookback_dict_amiata_flow_rate_galleria_alta, target_dict_amiata_flow_rate_galleria_alta]\n\ngc.collect()\n\n%memit","83f47841":"water_spring_madonna_targets = ['Flow_Rate_Madonna_di_Canneto']\n\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Settefrati'\n\n\nname_of_target = 'Flow_Rate_Madonna_di_Canneto'\nprepared_df_madonna_flow_rate = df_water_spring_madonna_di_canneto.copy()\nprepared_df_madonna_flow_rate, targets_madonna_flow_rate, original_target_values_madonna_flow_rate, \\\n        lookback_dict_madonna_flow_rate, target_dict_madonna_flow_rate = \\\n                                                feature_engineering(prepared_df_madonna_flow_rate, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","5ec46022":"# only six months of testing data, because of the anomalies\ntest_size = 180\nwhich_day_to_predict = 7\nxgb_feature_importances_madonna_flow_rate, features_madonna_flow_rate, y_test_madonna_flow_rate, \\\n                            y_pred_madonna_flow_rate = run_xgb(prepared_df_madonna_flow_rate, \\\n                            targets_madonna_flow_rate, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","1df92284":"num_most_important_features = 20\nname_of_target = 'Flow_Rate_Madonna_di_Canneto'\nget_n_most_important_feature_indices(xgb_feature_importances_madonna_flow_rate, features_madonna_flow_rate, \n                                     name_of_target, num_most_important_features)","3ed8e41f":"del [prepared_df_madonna_flow_rate, targets_madonna_flow_rate, original_target_values_madonna_flow_rate, \\\n        lookback_dict_madonna_flow_rate, target_dict_madonna_flow_rate]\n\ngc.collect()\n\n%memit","25c5b419":"# now num_lookback_days == 0 so that we don't create lookback variables in each row\nnum_lookback_days = 0\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Settefrati'\n\n\nname_of_target = 'Flow_Rate_Madonna_di_Canneto'\nprepared_df_madonna_flow_rate = df_water_spring_madonna_di_canneto.copy()\nprepared_df_madonna_flow_rate, targets_madonna_flow_rate, original_target_values_madonna_flow_rate, \\\n        lookback_dict_madonna_flow_rate, target_dict_madonna_flow_rate = \\\n                                                feature_engineering(prepared_df_madonna_flow_rate, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","80e522be":"original_target = 'Flow_Rate_Madonna_di_Canneto'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\n# only six months of testing data, because of the anomalies\nsize_test_set = 180\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 2\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 2\nnum_epochs_train_prior_years = 1\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_flow_rate_madonna, y_test_actual_madonna, model_madonna = \\\n            run_lstm(prepared_df_madonna_flow_rate, targets_madonna_flow_rate, num_lookback_days, \n            num_days_to_predict, size_test_set, num_days_difference, original_target, scale_y, walk_forward, \n            leak_data, synthetic, num_train_years_prior, batch_size, num_epochs, num_epochs_walk_forward, \n                     num_epochs_train_prior_years, rolling_sum)\n","ad2b8415":"%matplotlib inline\nplot_widget_results(df_pred_flow_rate_madonna, y_test_actual_madonna, 'Flow_Rate_Madonna_di_Canneto', '$Liters \/ s$', num_days_in_advance=7)","217d0a86":"%matplotlib inline\nplot_xgb_results(y_pred_madonna_flow_rate, y_test_madonna_flow_rate, 'Flow_Rate_Madonna_di_Canneto', '$Liters \/ s$', which_day_to_predict);","e791eb5a":"del [prepared_df_madonna_flow_rate, targets_madonna_flow_rate, original_target_values_madonna_flow_rate, \\\n        lookback_dict_madonna_flow_rate, target_dict_madonna_flow_rate]\n\ngc.collect()\n\n%memit","dcf29275":"water_spring_lupa_targets = ['Flow_Rate_Lupa']\n\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = ''\nno_temp = True\n\nname_of_target = 'Flow_Rate_Lupa'\nprepared_df_lupa_flow_rate = df_water_spring_lupa.copy()\nprepared_df_lupa_flow_rate, targets_lupa_flow_rate, original_target_values_lupa_flow_rate, \\\n        lookback_dict_lupa_flow_rate, target_dict_lupa_flow_rate = \\\n                                                feature_engineering(prepared_df_lupa_flow_rate, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags, no_temp)\n","8ee7562d":"# only 30 days of testing data, because of data structure (see \"Data Cleaning\" section on Lupa Water Spring)\ntest_size = 30\nwhich_day_to_predict = 7\nxgb_feature_importances_lupa_flow_rate, features_lupa_flow_rate, y_test_lupa_flow_rate, \\\n                            y_pred_lupa_flow_rate = run_xgb(prepared_df_lupa_flow_rate, \\\n                            targets_lupa_flow_rate, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","13546f10":"num_most_important_features = 20\nname_of_target = 'Flow_Rate_Lupa'\nget_n_most_important_feature_indices(xgb_feature_importances_lupa_flow_rate, features_lupa_flow_rate, \n                                     name_of_target, num_most_important_features)","b6ff59cd":"del [prepared_df_lupa_flow_rate, targets_lupa_flow_rate, original_target_values_lupa_flow_rate, \\\n        lookback_dict_lupa_flow_rate, target_dict_lupa_flow_rate]\n\ngc.collect()\n\n%memit","c34b3df2":"# now num_lookback_days == 0 so that we don't create lookback variables in each row\nnum_lookback_days = 0\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = ''\nno_temp = True\n\nname_of_target = 'Flow_Rate_Lupa'\nprepared_df_lupa_flow_rate = df_water_spring_lupa.copy()\nprepared_df_lupa_flow_rate, targets_lupa_flow_rate, original_target_values_lupa_flow_rate, \\\n        lookback_dict_lupa_flow_rate, target_dict_lupa_flow_rate = \\\n                                                feature_engineering(prepared_df_lupa_flow_rate, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags, no_temp)\n","2d78b103":"original_target = 'Flow_Rate_Lupa'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\n# only 30 days of testing data, because of data structure (see \"Data Cleaning\" section on Lupa Water Spring)\nsize_test_set = 30\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 2\nnum_epochs_walk_forward = 20\nnum_train_years_prior = 4\nnum_epochs_train_prior_years = 1\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_lupa_flow_rate, y_test_actual_lupa_flow_rate, model_lupa_flow_rate = \\\n            run_lstm(prepared_df_lupa_flow_rate, targets_lupa_flow_rate, num_lookback_days, \n            num_days_to_predict, size_test_set, num_days_difference, original_target, scale_y, walk_forward, \n            leak_data, synthetic, num_train_years_prior, batch_size, num_epochs, num_epochs_walk_forward, \n                     num_epochs_train_prior_years, rolling_sum)\n","ca0a77d5":"%matplotlib inline\nplot_widget_results(df_pred_lupa_flow_rate, y_test_actual_lupa_flow_rate, 'Flow_Rate_Lupa', '$Liters \/ s$', num_days_in_advance=7)","95b71b12":"%matplotlib inline\nplot_xgb_results(y_pred_lupa_flow_rate, y_test_lupa_flow_rate, 'Flow_Rate_Lupa', '$Liters \/ s$', which_day_to_predict);","f5badb30":"del [prepared_df_lupa_flow_rate, targets_lupa_flow_rate, original_target_values_lupa_flow_rate, \\\n        lookback_dict_lupa_flow_rate, target_dict_lupa_flow_rate]\n\ngc.collect()\n\n%memit","ff0c85bf":"river_arno_targets = ['Hydrometry_Nave_di_Rosano']\n\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Firenze'\n\nname_of_target = 'Hydrometry_Nave_di_Rosano'\nprepared_df_river_arno = df_river_arno_full_data.copy()\nprepared_df_river_arno, targets_river_arno, original_target_values_river_arno, \\\n        lookback_dict_river_arno, target_dict_river_arno = \\\n                                                feature_engineering(prepared_df_river_arno, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n\n# for demonstration with prediction\nlast_row_with_lookback_river_arno = prepared_df_river_arno.iloc[-1, :]\nlast_row_with_lookback_river_arno_features = last_row_with_lookback_river_arno[: -num_days_to_predict]\nlast_row_with_lookback_river_arno_targets = last_row_with_lookback_river_arno[ -num_days_to_predict:]","23fd3339":"test_size = 365\nwhich_day_to_predict = 7\nxgb_feature_importances_river_arno, features_river_arno, y_test_river_arno, \\\n                            y_pred_river_arno = run_xgb(prepared_df_river_arno, \\\n                            targets_river_arno, test_size, num_lookback_days, num_days_to_predict, which_day_to_predict)","3bebf7f3":"num_most_important_features = 20\nname_of_target = 'Hydrometry_Nave_di_Rosano'\nget_n_most_important_feature_indices(xgb_feature_importances_river_arno, features_river_arno, \n                                     name_of_target, num_most_important_features)","2a65b34d":"del [prepared_df_river_arno, targets_river_arno, original_target_values_river_arno, \\\n        lookback_dict_river_arno, target_dict_river_arno]\n\ngc.collect()\n\n%memit","d1e1848a":"# now num_lookback_days == 0 so that we don't create lookback variables in each row\nnum_lookback_days = 0\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Firenze'\n\nname_of_target = 'Hydrometry_Nave_di_Rosano'\nprepared_df_river_arno = df_river_arno.copy()\nprepared_df_river_arno, targets_river_arno, original_target_values_river_arno, \\\n        lookback_dict_river_arno, target_dict_river_arno = \\\n                                                feature_engineering(prepared_df_river_arno, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags, )\n","86aad5dd":"original_target = 'Hydrometry_Nave_di_Rosano'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 1\nnum_epochs = 2\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 5\nnum_epochs_train_prior_years = 1\nscale_y = True\nwalk_forward = False\nleak_data = False\nrolling_sum = False\n\ndf_pred_river_arno, y_test_actual_river_arno, model_river_arno = \\\n            run_lstm(prepared_df_river_arno, targets_river_arno, num_lookback_days, \n            num_days_to_predict, size_test_set, num_days_difference, original_target, scale_y, walk_forward, \n            leak_data, synthetic, num_train_years_prior, batch_size, num_epochs, num_epochs_walk_forward, \n                     num_epochs_train_prior_years, rolling_sum)\n","b95535ee":"%matplotlib inline\nplot_widget_results(df_pred_river_arno, y_test_actual_river_arno, 'Hydrometry_Nave_di_Rosano', 'Meters', num_days_in_advance=7)","52860e3f":"# now num_lookback_days == 0 so that we don't create lookback variables in each row\nnum_lookback_days = 0\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 5\nprint_flags = False\nname_of_temp_variable = 'Temperature_Firenze_synthetic'\n\nname_of_target = 'Hydrometry_Nave_di_Rosano_synthetic'\nprepared_df_river_arno_synthetic = df_river_arno_full_data.copy()\nprepared_df_river_arno_synthetic.columns += '_synthetic'\nprepared_df_river_arno_synthetic.rename(columns={'Date_synthetic': 'Date'}, inplace=True)\nprepared_df_river_arno_synthetic, targets_river_arno_synthetic, original_target_values_river_arno_synthetic, \\\n        lookback_dict_river_arno_synthetic, target_dict_river_arno_synthetic = \\\n                                                feature_engineering(prepared_df_river_arno_synthetic, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","586d647a":"prepared_df_river_arno_synthetic","befc0d09":"original_target = 'Hydrometry_Nave_di_Rosano_synthetic'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 5\nbatch_size = 1\nnum_epochs = 2\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 5\nnum_epochs_train_prior_years = 1\nscale_y = True\nwalk_forward = True\nleak_data = False\nrolling_sum = False\n\ndf_pred_river_arno_synthetic, y_test_actual_river_arno_synthetic, model_river_arno_synthetic = \\\n            run_lstm(prepared_df_river_arno_synthetic, targets_river_arno_synthetic, num_lookback_days, \n            num_days_to_predict, size_test_set, num_days_difference, original_target, scale_y, walk_forward, \n            leak_data, synthetic, num_train_years_prior, batch_size, num_epochs, num_epochs_walk_forward, \n                     num_epochs_train_prior_years, rolling_sum)\n","b219cac3":"%matplotlib inline\nplot_widget_results(df_pred_river_arno_synthetic, y_test_actual_river_arno_synthetic, 'Hydrometry_Nave_di_Rosano_synthetic', 'Meters', num_days_in_advance=7)","9d275bc9":"%matplotlib inline\nplot_xgb_results(y_pred_river_arno, y_test_river_arno, 'Hydrometry_Nave_di_Rosano', 'Meters', which_day_to_predict);","cf9c859a":"del [prepared_df_river_arno, targets_river_arno, original_target_values_river_arno, \\\n        lookback_dict_river_arno, target_dict_river_arno]\n\ndel [prepared_df_river_arno_synthetic, targets_river_arno_synthetic, original_target_values_river_arno_synthetic, \\\n        lookback_dict_river_arno_synthetic, target_dict_river_arno_synthetic]\n\ngc.collect()\n\n%memit","1cabdbce":"# now num_lookback_days == 0 so that we don't create lookback variables in each row\nnum_lookback_days = 0\nnum_days_to_predict = 30\nnum_days_difference = 0\nsynthetic = 0\nprint_flags = False\nname_of_temp_variable = 'Temperature_Firenze_example_prediction'\n\nname_of_target = 'Hydrometry_Nave_di_Rosano_example_prediction'\nprepared_df_river_arno_example_prediction = df_river_arno_imputed_data.copy()\nprepared_df_river_arno_example_prediction.columns += '_example_prediction'\nprepared_df_river_arno_example_prediction.rename(columns={'Date_example_prediction': 'Date'}, inplace=True)\nprepared_df_river_arno_example_prediction, targets_river_arno_example_prediction, \\\n        original_target_values_river_arno_example_prediction, \\\n        lookback_dict_river_arno_example_prediction, target_dict_river_arno_example_prediction = \\\n                                                feature_engineering(prepared_df_river_arno_example_prediction, \n                                                name_of_target, name_of_temp_variable, num_lookback_days, \n                                                num_days_to_predict, num_days_difference, synthetic, print_flags)\n","c11f9cd9":"original_target = 'Hydrometry_Nave_di_Rosano_example_prediction'\n# we have to reset the value of num_lookback_days here for the LSTM\nnum_lookback_days = 30\nnum_days_to_predict = 30\nnum_days_difference = 0\nsize_test_set = 365\nsynthetic = 0\nbatch_size = 32\nnum_epochs = 1\nnum_epochs_walk_forward = 2\nnum_train_years_prior = 5\nnum_epochs_train_prior_years = 1\nscale_y = True\nwalk_forward = False\nleak_data = False\nrolling_sum = False\n\n# only train up until the last num_days_to_predict days\ndf_pred_river_arno_example_prediction, y_test_actual_river_arno_example_prediction, model_river_arno_example_prediction = \\\n            run_lstm(prepared_df_river_arno_example_prediction.iloc[:-num_days_to_predict, :], targets_river_arno_example_prediction, num_lookback_days, \n            num_days_to_predict, size_test_set, num_days_difference, original_target, scale_y, walk_forward, \n            leak_data, synthetic, num_train_years_prior, batch_size, num_epochs, num_epochs_walk_forward, \n                     num_epochs_train_prior_years, rolling_sum)\n","694ba96e":"%matplotlib inline\nplot_widget_results(df_pred_river_arno_example_prediction, y_test_actual_river_arno_example_prediction, 'Hydrometry_Nave_di_Rosano_example_prediction', 'Meters', num_days_in_advance=7)","f5b97b0e":"# load the model\nsaved_model = load_model('.\/LSTM_Models\/LSTM_Hydrometry_Nave_di_Rosano_example_prediction.h5')\nprint(saved_model.summary())","0b9c9b74":"# Note: ideally, the scalers should have been returned by run_lstm()\n\nX, y = prepared_df_river_arno_example_prediction.iloc[:, : -num_days_to_predict], \\\n       prepared_df_river_arno_example_prediction.iloc[:, -num_days_to_predict:]\nX_scaled, X_scaler = scale_values(X, feature_range=(0, 1))\ny_scaled, y_scaler = scale_values(y, feature_range=(0, 1))\n\nnum_lookback_days = 30\nnum_days_to_predict = 30\n\n# only take last num_days_to_predict + 1 days\nX_scaled_future = X_scaler.fit_transform(X[-(num_days_to_predict + 1):])\n\nprint('Shape of input to prediction: ', X_scaled_future.shape)\n\ntest_generator = TimeseriesGenerator(X_scaled_future, np.zeros(len(X_scaled_future)), length=num_lookback_days, batch_size=batch_size)","86474aba":"predicted_values_scaled = saved_model.predict(test_generator)\npredicted_values = y_scaler.inverse_transform(predicted_values_scaled[:, :, 0])\nriver_arno_future_actual_values = df_river_arno.iloc[-num_days_to_predict:, -1].values\nfor i in range(len(predicted_values[0])):\n    print('predicted hydrometry for {} days after {} is {}, actual value is {}'.format(\n        i + 1, pd.to_datetime(df_river_arno_imputed_data['Date'].values[-1]), predicted_values[0][i], river_arno_future_actual_values[i]))","3e4b7e96":"bst = xgb.Booster({'nthread': 4})  # init model\nbst.load_model('.\/XGBregressor_Models\/Hydrometry_Nave_di_Rosano_7_days_forward')  # load data\nexample_xgb_prediction_river_arno = bst.predict(xgb.DMatrix(last_row_with_lookback_river_arno_features.values.reshape(1, -1)))\nprint('predicted hydrometry for 7 days after {} is {}'.format(last_row_with_lookback_river_arno_features.name, example_xgb_prediction_river_arno[0]))\n","ffa54eac":"del [prepared_df_river_arno_example_prediction, targets_river_arno_example_prediction, \\\n        original_target_values_river_arno_example_prediction, \\\n        lookback_dict_river_arno_example_prediction, target_dict_river_arno_example_prediction]\n\ngc.collect()","e3fde8cb":"del bst\nlibc.malloc_trim(0)\ngc.collect()","60b4bad9":"%memit","1774a41c":"There are no missing dates now. Let's filter for the target variable, where we have substantial data starting 2016-04-13, and for the independent variables, where we have data until 2018-12-31. ","315b13bb":"This is an interesting dataset. It seems reasonable to use an interpolation for the values of our target variable, `Depth_to_Groundwater_Podere_Casetta`. There is only a bit of the data missing for the target in the last year, which will be our test set; however, given the smooth trendline of the data, it is reasonable to interpolate it, even in the test set.\n\nThe first question is, should we filter the data for the `Volume` variables, or should we drop them, so that we can use much more of the rest of the data? Since the `Volume` variables are probably strong predictors of our target, and we have around 4 years worth of such data starting 2015-01-01, we will filter for them.\n\nNext, there is very little data for `Temperature_Siena_Poggio_al_Vento`, so we will drop this variable. This leaves us with trying to estimate the anomalous zero values of `Temperature_Mensano` and `Temperature_Monteroni_Arbia_Biena`. As usual, we can estimate these anomalous values with their past\/future values (but not future values from the test set).\n\nWe can drop the non-target `Depth_to_Groundwater` variables, since there clearly is not enough of that data for both training and testing. \n\nWhat remains to be dealt with are the `Rainfall` variables. We will drop the following variables due to lack of data: `Rainfall_Siena_Poggio_al_Vento`, `Rainfall_Ponte_Orgia`, and `Rainfall_Pentolina`. We will impute the remaining values where they are still missing.","f295ac73":"Some areas for improvement:\n\n    1) getting more data - e.g. weather (humidity, wind speed, air\/water temperatures, etc.), energy consumption, proxies for water consumption\n    2) implementing this: https:\/\/github.com\/Arturus\/kaggle-web-traffic\/blob\/master\/how_it_works.md#working-with-long-timeseries in place of the current simplified attention mechanism\n    3) implementing a more accurate synthetic data creation process\n    4) abstracting some of the predictions into functions\n    5) getting all the widgets to work at the same time, instead of only one at a time\n    6) grouping LSTM functions into a class \n    7) making predictions 30 days in advance using XGBRegressor - we already have the code for this from the first attempt at solving this problem\n    8) simplifying the output from getting the most important features using XGBRegressor","d83cf9a1":"# Prediction Code","0bf9bd1e":"Notice that the `Pozzo` variables 2, 4, 6, 8, and 9 all have similar shapes and values. Of these, Pozzo 2 looks like it has the easiest values to impute (in other words, its imputed values will be the most accurate), and we will use this variable as a representative for the others. However, we must deal with an anomaly in the data that would be in our test set: ","ac85eb42":"Let's take a look at our final dataset.","d7b70ac5":"### Feature Importances","0dcaa93f":"### Predictions","efba4e74":"## Train-test-split Code","c0de9419":"There are no missing dates. We will remove the rows with the most missing data, which are all the rows before 2011-05-17.","acaa651f":"Unsurpisingly, the value of the variable `Depth_to_Groundwater_SAL` is the most important feature in predicting its own value in seven days. We see that the importances of other variables decreases drastically from there. Also unsurprisingly, we see that the `Hydrometry` variables are also very important. Interestingly, we also see that the `Depth_to_Groundwater_LT2` variable in March is important as well, perhaps for predicting values in that same month. Additionally, we also see several values of `Depth_to_Groundwater_SAL` up to 8 days prior to the date from which the prediction is being made (implying 15 days prior to the day that is being predicted) to be very important as well, featuring several times just in the 20 most important features. This shows the power of using feature extraction in the way that we did. We can see the non-linear effects that variables have on other variables. \n\nScanning the remaining variables, it's clear that all of the different types of variables we extracted played a role in predicting the target variable. For this reason, we won't reduce the lookback period, nor eliminate any of the dummies and cross terms we made in `feature_engineering()`. \n\nLet's take a look at the results of running XGBRegressor on the other two targets. ","a29d1476":"Since these dataframes consume a lot of memory, we will delete all of them. ","61033428":"# Choice of Machine Learning Algorithms\n\nWe tested a number of different machine learning algorithms to solve this problem: Lasso regression (a type of linear regression where the least important variables are eliminated from the regression), k-Nearest-Neighbors, Support Vector Regression, XGBRegressor, Feed-forward Neural Networks (FNN), and Long-Short-Term-Memory Neural Networks (LSTM). Of these, XGBRegressor and LSTM gave the best results, and so they are the ones we use in this notebook. \n\nLSTM Neural Networks are a type of recurrent neural network which are well suited for predicting sequences of data. For this reason, they are very popular in time-series analysis. However, LSTM networks are not well suited for determining which features are the most important for predicting the target variable(s). For this reason, we will use XGBRegressor to determine the most important features - including variables in the past - that affect our target variables.","b21afa55":"There are three possible strategies for handling the missing data. The first is to only use the columns where all the variables are available; the second is to only use the rows where all the variables are available; the third is to impute missing values. ","11875446":"As noted in the introduction, forecasting large changes in the Arno River level is quite difficult. In one paper, the goal was simply to make accurate forecasts six hours in advance: https:\/\/www.researchgate.net\/publication\/248952964_Artificial_neural_network_approach_to_flood_forecasting_in_the_River_Arno. ","b20f3e67":"### Feature Importances","3f941028":"Now, let's take a look at our final dataset.","f0930a26":"There are no missing dates. We can see that there is a large number of missing values early in the dataset, and then very sporadically after. We can drop the earliest rows and still have plenty of data with which to work. Upon visual inspection, most of the missing data ends on 2010-09-04, row 610. We will drop everything before this.","b0225ed4":"As we can see, we created an additional 10 years of data (synthetic = 5 -> 10 years of data, since we take the first two years of data and modify it 5 times).","7066d0b1":"In this section, we use XGBRegressor to make predictions and determine which features are the most important for making predictions, and make predictions using LSTM. \n\nIn many of the predictions, we will see the impact of the limitations which were part of the competition rules regarding making forecasts based on predicted values. In particular, for a number of predictions we will see a significant lag relative to the actual data. This is due to the fact that when we do our walk-forward validation, we can only update our current model with information from `num_days_to_predict` days ago, since that is when we have all of the real data necessary to train our model; if we trained it on more recent data, our model would have data leakage, meaning that our predictions would not reflect real-life performance. However, if we were allowed to train the model based on, for example, the predictions made yesterday for the following `num_days_to_predict` days (rather than the real values for the following `num_days_to_predict` days), these models would probably become much more accurate. However, this not was not allowed by the competition rules.\n\nNevertheless, most of the predictions are still quite accurate. It is also important to note that experimenting further with batch sizes and epochs, as well as other parameters like the number of layers, filters, and neurons in our LSTM model subtantially improves results, and much can be done to make these predictions even more accurate. The parameters here were chosen as a trade-off between performance and runtime. Additionally, once a model has been trained, one can make predictions with it immediately (as we demonstrate in the last section of this notebook), and it is very easy to update the model with new data. Thus, computational constraints should be viewed through the lens of having a large amount of computing power (as opposed to the computing power available on Kaggle), which only ever has to be used once to train the model, and which can then be deployed and updated quickly and easily afterwards. Multiple cloud providers can train these models for only a few dollars.\n\nAdditionally, since we can easily make predictions using XGBRegressor after training it to find the most important features, we will also display the predictions that XGBRegressor makes. We will see that it is often the case that when LSTM underestimates, XGBRegressor overestimates, and vice versa. In these cases, taking the average of the two would result in highly accurate predictions, which could be a fruitful route to explore. \n\nWe will briefly explain a few important parameters that will be used in the following sections. Explanations of these parameters can also be found in the code comments.\n\n    num_lookback_days - the number of prior days to take into account when trying to make a prediction about future days. When passed as a parameter to feature_engineering(), it will add data from prior days to every row for use with XGBRegressor. When passed as a parameter to run_lstm(), it will be passed as a parameter to the LSTM model, which will automatically look at the data in prior rows, thus eliminating the need for appending the data to every row. \n    \n    num_days_to_predict - the number of days we are trying to predict\n    \n    num_days_difference - if 0, we will predict the actual target values; if 1, we will predict the difference of the target variable between one day and the next; can be set to numbers 2 and higher, but shouldn't be  \n    \n    size_test_set - number of days in the test set\n    \n    synthetic - only for use with the Arno River; number of sets of synthetic data to make\n    \n    batch_size - batch size for the initial training of the LSTM neural network\n    \n    num_epochs - number of epochs to run the initial training\n    \n    num_epochs_walk_forward - if walk_forward is True, the number of epochs to train on the most recently available data; this is a form of walk-forward validation \n    \n    num_train_years_prior - if > 0, the number of this time period's prior years data to train on. For example, if the day is June 15, then we will train on data from June 8 - June 22 of the prior year, of two years prior, ... all the way up to num_train_years_prior. If 0, we will not train on data from prior years. This is a simplified attention mechanism, designed to capture and train on data that is similar from year to year. \n    \n    num_epochs_train_prior_years - if num_train_years_prior > 0, the number of epochs we train on each set of prior years' data\n    \n    scale_y - if True, scale the target variable \n    \n    walk_forward - if True, use walk-forward validation\n    \n    leak_data - if True, train on the most recently available data during walk-forward validation. This is useful for baseline benchmarking purposes, but should otherwise always be set to False because it leaks data\n    \n    rolling_sum - assuming we are predicting the difference between days, then if True, should calculate the actual predictions by taking the rolling sum - see invert_difference() for details \n    \n    print_flags - if True, will print the stage and intermediate dataframes in feature_engineering(), making it clear exactly how the feature extraction is being executed\n\n\n\nThe predictions from both LSTM and XGBRegressor, along with the actual values, will be saved to files, and then loaded when making visualizations. This saves us from needing to re-train an entire model just to make a visualization. Please note that the interactive widget only works when running the notebook - when viewing a static version of the notebook, like on Kaggle, the widget does not appear, and the default prediction that is visualized is 7 days out from the current day. Due to this shortcoming, we also display a small dataframe with the MAE and RMSE for each of the `num_days_to_predict` days.\n\nNote that when first executing a cell that plots results in a Kaggle environment, it is necessary to refresh the output of the working directory in order for the plot to appear.\n\nIn the last section, we demonstrate how to make real-world predictions with both LSTM and XGBRegressor. ","424533d6":"There are still a few problems with the data. We can see that starting in mid 2015, there is anomalous data for `Temperature_Petrignano` and `Hydrometry_Fiume_Chiascio_Petrignano`, where both these variables are all 0 for several months, starting 2015-04-25 and ending 2015-09-21. As with prior missing temperature data, we can use values from the year prior to replace to anomalous values of `Temperature_Petrignano`. In the case of `Hydrometry_Fiume_Chiascio_Petrignano`, we will impute the missing values.\n\nAdditionally, we can see that there is some unusual data in 2019 for `Volume_C10_Petrignano`, starting 2019-06-28 and ending 2019-08-21, as well as in 2011, where some of the values suddenly become zero and then return to values close to their prior values. It is not clear whether these values are errors or not, since this variable represents the volume of water taken from the drinking water treatment plant. It is possible that these values were in fact zero because of some distruptive event. Since it will probably not affect our model too much, we will impute these values.\n\nThere is a small amount of data still missing, some of it in our target variables. These gaps are small, and the values around the gaps are close to each other, so we will impute these values as well.","07083464":"# Future Work","d80a9f6b":"Let's take a look at our final dataset.","288630e1":"### Feature Importances","d1ed0813":"There are no missing dates.","f69659da":"# Data Cleaning","60c84f50":"## Bilancino Lake","711d55c8":"### Predictions","ea2b0dd4":"There are no missing values remaining.","967cf7a2":"There are no missing dates, and the most of the missing data ends on 2004-01-02, so we will take all the data after that.","3ac0c42a":"Looks good.","e0d87d09":"A lot of time was spent on this particular dataset. It should be noted that one can get very low error rates by running this model for a few epochs, without walk-forward validation, on very basic settings. However, this is only because the predictions stay relatively constant over time. Predictions with similar error rates, but that look much more similar to the actual values, can be achieved with more training. This demonstrates that MAE and RMSE do not capture the entire story that the data is telling. This is a particularly hard dataset to predict, but we have seen results more accurate than these, particularly when tuning the number of epochs run, as well as changing the LSTM model that is being used (multiple are given in the LSTM Code section). \n\nWe will demonstrate two sets of predictions with LSTM: the first one will (probably) have lower MAE and RMSE, but will less accurately reflect the shape of the data; the second will (probably) have higher MAE and RMSE, but will more accurately reflect the shape of the data. Note the \"probably\" - neural networks have an element of randomness, and are not deterministic. ","7d873fc9":"Let's take a look at our cleaned data set.","cdae4f00":"We can see that we still have many missing values. As before, we can fill in the missing values of the temperatures - which start in 2015-01-01 and end 2018-12-31 - with those of prior years, due to their strong seasonalities and the use of dummy variables, and impute the small number of missing values in `Temperature_Velletri` in the end of 2014. \n\nThe rest of the data is a bit trickier. We can see that a large amount of rainfall early in the truncated dataset increased the amount of water in the aquifer, but that subsequent drier years led to a steady decrease, as water was withdrawn from it. Since we see the same steady decrease in the water levels once we have actual data for the `Volume` variables starting in late 2016, the logical conclusion is that the missing values for the `Volume` variables prior to late 2016 are similar to the values that are available after late 2016.\n\nHere is the contradiction: we do not have much data on the variation of rainfall - there is only rainfall in the beginning of the dataset, and towards the end, which is where we want to have our test set. However, we only have data for the `Volume` variables starting in late 2016. So do we impute the values of the `Volume` back in time so that we have access to the earlier rainfall data, or do we drop the data up until we have the `Volume` variables available? \n\nThe reality is that without some very in depth data engineering, we cannot come close to correctly imputing the prior values of the `Volume` variables. For that reason, we will drop the rows which do not contain them.","06b0625c":"## Lupa Water Spring\n\nAccording to the provided description, \n\n> Description: this water spring is located in the Rosciano Valley, on the left side of the Nera river. The waters emerge at an altitude of about 375 meters above sea level through a long draining tunnel that crosses, in its final section, lithotypes and essentially calcareous rocks. It provides drinking water to the city of Terni and the towns around it.\n\n> Output: Flow_Rate\n\nFirst, let's take a look at the data:","79fdd901":"## LSTM Code","4b683890":"## Luco Aquifer","af370c13":"We begin by engineering our datasets as described in the \"Feature Engineering\" section. Since we have several `Temperature` variables available to us, we will choose one arbitrarily.","a28a30cf":"Let's take a look at our final dataset. Remember that we will not be using the last portion of `Depth_to_Groundwater_Pozzo_3`, since the amount of missing data there, and the ensuing interpolation, seem unsuitable for the problem. ","1d8c09d4":"Additionally, it is important to understand the output we will use from XGBRegressor. XGBRegressor is not multi-output compatible - one must use a wrapper, or compute every target variable separately, in order to make multiple predictions. For this reason, we are only able to extract the most important features for predicting a specific day in advance - e.g., predicting the target variable 7 days in advance. Furthermore, because of computational constraints that Kaggle has (there are some problems using the GPU for XGBRegressor, and there are only two CPU cores allocated when using the GPU), we will only present the results from predicting one day. \n\nAlso relevant is the fact that the most important features will vary depending on the time of year. Since we are usually using multiple years of data in our training set, that means that we will be seeing all of the most important features, which might obscure more nuanced observations.\n\nWe will see that XGBRegressor typically finds hundreds of important features out of the tens of thousands it has available. Ideally, we would combine all of the feature importances for all of the days we want to predict, and then simplify the results. Unfortunately, doing all of these computations is not feasible on Kaggle. However, when we tested our XGBRegressor model on a 20 core \/ 40 thread machine, no call to the `fit()` method ever took longer than a minute, suggesting that this is perfectly feasible with relatively modest resources.   ","e1dcfd4c":"### Feature Importances","ba51ff09":"### Predictions","6f02d9a2":"There is plenty of data to work with here. The Rainfall and and Temperature variables will almost certainly be useful for our predictions, so we will drop the early rows where they are missing, which are all the rows up until 2009-01-01. ","32277867":"Our target variable is `Depth_to_Groundwater_Podere_Casetta`. We will first filter our data for this variable. The first date that `Depth_to_Groundwater_Podere_Casetta` is available is 2008-02-21, and the last date it is available is 2019-01-12.","1f952698":"Looking a bit further through the most important features, it seems like values from more than two weeks prior were more important than more recent values.","8e4105a3":"These results indicate that the two target variables are closely interlinked with one another. This is not surprising - if we look back at the values during the data cleaning stage, they look almost identical. This doesn't really tell us what we want to know, though. Let's rerun XGBRegressor, but when trying to predict one variable, we drop the other.","d2d409ab":"## Auser Aquifer\n\nAccording to the provided description, \n\n> Description: This waterbody consists of two subsystems, called NORTH and SOUTH, where the former partly influences the behavior of the latter. Indeed, the north subsystem is a water table (or unconfined) aquifer while the south subsystem is an artesian (or confined) groundwater.\n\n> The levels of the NORTH sector are represented by the values of the SAL, PAG, CoS and DIEC wells, while the levels of the SOUTH sector by the LT2 well. \n\n> Output: Depth_to_Groundwater_SAL, Depth_to_Groundwater_CoS, Depth_to_Groundwater_LT2\n\nFirst, let's take a look at the data:","da009b5f":"There is one final anomaly we must consider. In late 2019, there is about a month of missing data for the Groundwater variables, and during that same time period, the Volume variables drop almost entirely to zero. Since the Volume variables represent the amount of water taken out of drinking water treatment plants, it seems even more unlikely that these values were zero for an entire month. Additionally, for the Groundwater data that we do have available for those dates, the lack of any effect from the Volume variables being zero seems unusual. Compounding this with the corresponding mostly missing data for the Groundwater variables, we will assume that these variables are errors. We will set them to `np.nan` and impute them.\n\nSo in the end, we will interpolate the missing Groundwater variable values and (few) missing Temperature variable values, and impute the missing Volume variable values.","4bd12be8":"It looks like there are some missing dates, where the `Date` values are all `NaT` and the variables are all 0. Let's try to set these variables to `np.nan`, replace the dates, and then interpolate the values.","2a444c00":"It seems extremely unlikely for the level of an aquifer to suddenly drop from -100 meters to -112 meters in one day, remain at that level for four days, and then jump back to -100 meters immediately after. There is no indication that the Rainfall or Volume variables would lead to this happening. Likewise, we only see the same anomaly in Pozzo 8, which has a very similar distribution to Pozzo 2, but nowhere else in the data. We will treat this as an anomaly, replace these values with np.nan, and then interpolate them.","58d8feb9":"## Petrignano Aquifer\n\nAccording to the provided description, \n\n> Description: The wells field of the alluvial plain between Ospedalicchio di Bastia Umbra and Petrignano is fed by three underground aquifers separated by low permeability septa. The aquifer can be considered a water table groundwater and is also fed by the Chiascio river. The groundwater levels are influenced by the following parameters: rainfall, depth to groundwater, temperatures and drainage volumes, level of the Chiascio river.\n\n> Output: Depth_to_Groundwater_P24, Depth_to_Groundwater_P25\n\nFirst, let's take a look at the data:","2ff9478a":"Dataset using imputed data:","3d335c5a":"As we can see, we have each of the following 5 days' target variables added to each row. Logically, the values along the diagonals of these variables are the same.\n\nIt is important to remember that `Hydrometry_Nave_di_Rosano` is _not_ our target variable, it is part of our independent variables. We have access to the current day's hydrometry, as well as rainfall and temperature data, in order to make predictions for the following days. The target variables are `Hydrometry_Nave_di_Rosano_1_days_forward` ... `Hydrometry_Nave_di_Rosano_5_days_forward`. \n\nIn reality, we will be trying to predict the following 30 days, for every day of our test set. If our test set is a year of data, we will be making 30 * 365 = 10,950 predictions. However, the LSTM model only sees this as 365 predictions, since it will predict a sequence of the next 30 days for every one of the 365 days in the test set. The amount of days to predict is very easy to change, it only requires changing one variable as the input to a function. ","caef4ea1":"Pozzo 3 seems dissimilar enough from the other target variables to make it worth predicting. There is mostly continous data for Pozzo 3, except at the end of the dataset, starting 2019-09-19. We will not try to impute these missing values - rather, we will make our test set the year leading up to 2019-09-19.\n\nPozzo 5 and Pozzo 7 have missing values which look very difficult to impute properly, so we will not try to predict these. However, their values are quite similar to the first group (represented by Pozzo 2), so predictions of the first group should give a good indication of predictions for Pozzo 5 and Pozzo 7.  \n\nLike Pozzo 3, Pozzo 1 seems dissimilar enough from the other target variables to make it worth predicting. Pozzo 1 has some missing data and a large jump right around the time that we start having data for the `Volume` variables. We will drop all of the data before this jump\/introduction of `Volume` variables, and then impute any missing values. \n\nIn the end, we will predict `Depth_to_Groundwater_Pozzo_1`, `Depth_to_Groundwater_Pozzo_2`, and `Depth_to_Groundwater_Pozzo_3`.","935d0531":"Now, we can proceed more easily. Let's take a look at our data so far.","89749a70":"## Doganella Aquifer\n\nAccording to the provided description, \n\n> Description: The wells field Doganella is fed by two underground aquifers not fed by rivers or lakes but fed by meteoric infiltration. The upper aquifer is a water table with a thickness of about 30m. The lower aquifer is a semi-confined artesian aquifer with a thickness of 50m and is located inside lavas and tufa products. These aquifers are accessed through wells called Well 1, ..., Well 9. Approximately 80% of the drainage volumes come from the artesian aquifer. The aquifer levels are influenced by the following parameters: rainfall, humidity, subsoil, temperatures and drainage volumes.\n\n> Output: Depth_to_Groundwater_Pozzo_1, Depth_to_Groundwater_Pozzo_2, Depth_to_Groundwater_Pozzo_3, Depth_to_Groundwater_Pozzo_4, Depth_to_Groundwater_Pozzo_5, Depth_to_Groundwater_Pozzo_6, Depth_to_Groundwater_Pozzo_7, Depth_to_Groundwater_Pozzo_8, Depth_to_Groundwater_Pozzo_9 \n\nFirst, let's take a look at the data:","66937f8c":"## Bilancino Lake\n\nAccording to the provided description, \n\n> Description: Bilancino lake is an artificial lake located in the municipality of Barberino di Mugello (about 50 km from Florence). It is used to refill the Arno river during the summer months. Indeed, during the winter months, the lake is filled up and then, during the summer months, the water of the lake is poured into the Arno river.\n\n> Output: Lake_Level, Flow_Rate\n\nFirst, let's take a look at the data:","d855490a":"# Future Predictions","6fb71865":"# Summary\n\nForeknowledge of water availability is a crucial component of logistics for water utility companies, as well as in an increasingly water-scarce world in general. Here, we develop methodologies for predicting the water levels of nine Italian water bodies. For each water body, the following process is followed: first, an analysis of the available data, and especially the development of strategies for dealing with missing data. Then, the implementation of two machine learning learning algorithms - XGBRegressor and LSTM Neural Networks - to determine which variables most influence water levels, and predict future values of the water levels. The LSTM implementation includes multiple time-step predictions, so the performance of the implementation can be assessed. Each target variable will be predicted 30 days in advance for the entire test set, which is usually 365 days. An interactive widget with a slider is used to visualize these predictions versus their actual values, as well as the mean absolute error (MAE) and root-mean-square error (RMSE), for each of the 30 days. The widget is not interactive when viewed in a static Kaggle Jupyter Notebook, but is interactive when the notebook is actively running and the results are being plotted. Due to this shortcoming, a dataframe containing the MAE and RMSE for each of the 30 days is also displayed. \n\nBoth the XGBRegressor and LSTM models are easily modified to predict any number of days (only limited by the amount of training data), and is suited for handling stationary data by differencing, using walk-forward validation, using a simplified attention mechanism, training on synthetic data, and other options. The conclusion will include an example of how to predict water levels for one of the nine water bodies in real time.","e716e7b7":"Since these dataframes consume a lot of memory, we will delete all of them. ","9f858839":"Now that we have our feature-engineered datasets, let's run XGBRegressor to determine the most important features. We will choose the features that best predict the target variable 7 days in advance. This number is chosen because it seems like a reasonable prediction timeframe. However, any number less than `num_days_to_predict` can be used.","de1b66aa":"Since these dataframes consume a lot of memory, we will delete all of them. ","37f28dbd":"Since these dataframes consume a lot of memory, we will delete all of them. ","c58299c7":"### Predictions","1c1f77db":"# Introduction\n\n\nFirst, we want to understand the basic model underlying the physical processes that determine how much water is available at any given moment in time. We can use the following equation to achieve this goal. For any given day, \n\ncurrent water level = prior water level + water in - water out\n\n$$w_{current} = w_{prior} + w_{in} - w_{out}$$ \n\nTo make actual predictions easier in the code, we will use the same formula in a different manner:\n\nnext water level = current water level + water in - water out\n\n$$w_{next} = w_{current} + w_{in} - w_{out}$$\n\nNow, we want to analyze each of the components of the right hand side of the latter equation. \n\nThe current water level: for any given day, we know what the water level is.\n\nThe amount of water inflow: this is a function of rainfall, but with some complicating factors:\n    1) not all rainfall reaches the water body immediately\n    2) not all water in drainage basin reaches the water body; some is lost to evaporation, as well as the two main consumers of water (at least in the United States), power production and agriculture, as well as municipal purposes. \n    \nIn the case of power production, it is important to note that water is only consumed by thermoelectric power plants - i.e. those that require water for cooling. Furthermore, the hotter the water is, the less efficient is the cooling, leading to hotter water coming out of the power plants, which decreases the efficiency of downstream power plants, and increases the total amount of water consumed.\n\nThe amount of water outflow: in some cases this quantity is given, but in the cases of lakes\/rivers, must account for evaporation. In the case of rivers, which are defined by their flux, the \"water out\" variable only includes water that leaves the watershed upstream of the point of measurement. In other water bodies, it also includes water that leaves the water body after arriving.\n\nThis leads to a tricky problem: how do we calculate when rainfall reaches a water body? We must take into account\nfluxes of rivers, and the sizes of watersheds. Furthermore, there can be many nonlinearities involved in this calculation. In addition to fluctuations in the amount of water used for agriculture, there will also be nonlinearities introduced by the release and filling of dams, holidays, vacation periods, tourism, music festivals, etc. \n\nFor these reasons, rather than approaching the problem from a physical perspective, we will approach the problem from a machine learning perspective. If we acknowledge the fact that it is very difficult to construct a physical model that will predict water levels accurately, the next best thing we can do is gather data and try to learn from the past in order to predict the future. Indeed, a well-cited study found that\n\n>ANNs \\[Artifical Neural Networks\\] used the initial water level measurements, production well extractions, and climate conditions to predict the final water level elevations 30 d into the future at two monitoring wells. A sensitivity analysis was conducted with the ANNs that quantified the importance of the various input predictor variables on final water level elevations. Unlike traditional physical\u2010based models, ANNs do not require explicit characterization of the physical system and related physical data. Accordingly, ANN predictions were made on the basis of more easily quantifiable, measured variables, rather than physical model input parameters and conditions. \n> - [A neural network model for predicting aquifer water level elevations](https:\/\/ngwa.onlinelibrary.wiley.com\/doi\/abs\/10.1111\/j.1745-6584.2005.0003.x)\n\nSeveral other studies have had success taking this approach as well: \n\n- [Predicting piezometric water level in dams via artificial neural networks](https:\/\/link.springer.com\/article\/10.1007\/s00521-012-1334-2)\n\n- [Artificial neural network approach to flood forecasting in the River Arno](https:\/\/www.researchgate.net\/publication\/248952964_Artificial_neural_network_approach_to_flood_forecasting_in_the_River_Arno)\n\n- [Predicting flood susceptibility using LSTM neural networks](https:\/\/www.researchgate.net\/publication\/346777911_Predicting_flood_susceptibility_using_LSTM_neural_networks)\n\n- [River flood forecasting with a neural network model ](https:\/\/agupubs.onlinelibrary.wiley.com\/doi\/pdf\/10.1029\/1998WR900086)\n\nEach of the nine datasets for which we are making predictions contains some combination of rainfall, temperature, river level, lake level, depth to aquifer, flow rate, and volume of water extracted from the water body.  \n\nIn order to account for non-linearities in the data, we will be using non-linear predictors, in particular XGBRegressor and LSTM Neural Networks.\n\nIn order to capture the effects of the non-linearities, the following feature extraction is proposed: we make dummy variables for every week and month of the year, dummy variables for temperature brackets, and then make cross terms between 1) the time dummy variables and the temperature dummy variables; 2) the time dummy variables and other variables; 3) the temperature dummy variables and other variables. \n\nThe idea behind making these dummy variables is for the machine learning algorithm to learn which variables are most important on the water level on any given day, and how to weigh those variables to predict that water level. We will see in the \"Feature Importances\" sections that this method does in fact capture relationships between the data that would not otherwise be apparent.\n","e8ebbc88":"We can see that the prediction errors are relatively constant regardless of how many days in advance we are making a prediction, which is very good. ","a3c78526":"There is a lot of missing data, including for the target variables. ","a0a2ef89":"## Plotting Results Code","ff7424c8":"We will now use LSTM to make our predictions. For each day in the test set, we will make a prediction for the following thirty days. We will then plot the data with a widget that shows the differences between the real and predicted values for each of the 30 * 365 predictions in an easy-to-understand way. We will also print out the RMSE and MAE for the number of days in advance that we are trying to predict. These errors will be displayed in the bottom left corner of the plot. \n\nPlease note that, currently, only the last widget plot whose cell was executed will have the slider to change the amount of days in advance the prediction is being made. The other plots will still display, but their sliders will not move. To change this, simply run the cell of the desired plot. Nothing is being recalculated, so the result should be displayed instantaneously. \n\nIf this notebook is not currently running, and is being viewed in a static state, as it would be on Kaggle, the slider will not be available, and in its place, we have included a dataframe that contains all of the MAE and RMSE for all of the `num_days_to_predict` days.\n\nSince the LSTM model automatically takes care of looking at past data (i.e. data in prior rows), we do not have to have all of the prior data appended to every row, so we set `num_lookback_days` to 0. Not including these lookback columns reduces the size of the dataframe by a factor of `num_lookback_days`. First, we will recreate the dataframes without the prior data appended.","2e0171ab":"### Feature Importances","37ce484f":"### Predictions","ad70fee6":"Let's take a look at our dataset with synthetic data included.","e41147f6":"Since these dataframes consume a lot of memory, we will delete all of them. ","a72cade2":"Notice that that we have `num_days_to_predict` more days in the XGBRegressor prediction - this is because of how we split our data into training and test sets.","358209dc":"Now we can look at the missing data again.","9541066a":"## Feature Engineering Code","2b65ee32":"It looks like we can safely interpolate the few missing values in the dataset. However, in this particular case, we do not have enough variability in the data to use the entire last year as our test set. The flow rate decreased linearly between late 2010 - early 2020, and then a lot of rainfall in early 2020 substantially increased the flow rate. So instead of using the last year as our test set, we will use some of the last months instead.","1a945beb":"# Predictions","061ad229":"If we want make predictions using XGBRegressor, we can load the saved model, which was trained to predict the value 7 days from the date in the row being predicted. XGBRegressor can also be updated by calling `.fit()` and passing the parameter `xgb_model=` the currently loaded model. We will not re-train here on the imputed data, so we will only be predicting the value 7 days from the last available datapoint in `df_river_arno_full_data` after feature engineering.","c3c221b6":"First, we must determine whether there are any missing dates. ","85230c0d":"Now we can start training LSTM on our data, and make predictions.","e39ce1f9":"## Lupa Water Spring","fc59be4c":"## Doganella Aquifer","edefc7d0":"Now we can proceed more easily. Let's take a look at our data so far.","4fc4aed0":"Even after our data cleaning, there are 22 rows with at least one missing value. We will interpolate these missing values to make sure that we have continous data.\n\nThe interpolation for the last year of `Depth_to_Groundwater_DIEC` does not seem so good, but since it is not one of our target variables, it is probably good enough that we do not need to reduce the dataset by a whole year just because of it.\n\nWe see that for our three target variables, `Depth_to_Groundwater_SAL`, `Depth_to_Groundwater_CoS`, and `Depth_to_Groundwater_LT2`, there is some data starting on 2020-06-05 that is probably erroneous. It is very unlikely that the values suddenly became 0 for a few days and then immediately jumped back to their previous values - there was probably some kind of measuring error. When making our predictions, we will ignore the very last part of this dataset.","10e34549":"## Madonna di Canneto Water Spring\n\nAccording to the provided description, \n\n> Description: The Madonna di Canneto spring is situated at an altitude of 1010m above sea level in the Canneto valley. It does not consist of an aquifer and its source is supplied by the water catchment area of the river Melfa.\n\n> Output: Flow_Rate\n\nFirst, let's take a look at the data:","cadfd3a1":"Now, let's make predictions for the future.","5ca2591e":"## Amiata Water Spring","7d492ce0":"Recall that the data for Pozzo 3 starts becoming very inaccurate on 2019-09-19, so we truncate the data accordingly.","90e57174":"### Feature Importances","fb7c2d96":"There are no missing dates. We will remove the rows with the most missing data, which are all the rows before 2012-06-01.","1c3329cd":"Since these dataframes consume a lot of memory, we will delete all of them. ","b678c3de":"It seems there are 25 rows with at least one missing value, even after our data cleaning. Let's find out what's going on.","54b9d0f6":"In this section, we present the code for making predictions. The functions can take many different parameters, making predictions quite flexible.  ","b76e77f5":"Let's take a look at the final dataset.","8d1d7b08":"## Luco Aquifer\n\nAccording to the provided description, \n\n> Description: The Luco wells field is fed by an underground aquifer. This aquifer not fed by rivers or lakes but by meteoric infiltration at the extremes of the impermeable sedimentary layers. Such aquifer is accessed through wells called Well 1, Well 3 and Well 4 and is influenced by the following parameters: rainfall, depth to groundwater, temperature and drainage volumes.\n\n> Output: Depth_to_Groundwater_Podere_Casetta\n\nFirst, let's take a look at the data:","f8a995d6":"## Amiata Water Spring\n\nAccording to the provided description, \n\n> Information about the Amiata aquifer. This aquifer is accessed through the Ermicciolo, Arbure, Bugnano and Galleria Alta springs. The levels and volumes of the four springs are influenced by the parameters: pluviometry, sub-gradation, hydrometry, temperatures and drainage volumes.\n\n> Output: Flow_Rate_Bugnano, Flow_Rate_Arbure, Flow_Rate_Ermicciolo, Flow_Rate_Galleria_Alta\n\nFirst, let's take a look at the data:","6ac43c64":"Before we begin making predictions, we must make it clear what exactly we are predicting. Let's use the Arno River as an example dataset. To keep things simple, we will only keep one of the rainfall variables, as well as the date, the temperature variable, and the target variable.","7a054d99":"It is also important to explain the feature extraction process. We want to extract as much information from the data as possible. For this reason, we will make extensive use of dummy variables. A dummy variable is used to encode categorical data for machine learning predictions. For example, a dummy variable could be answering the question of whether the temperature is above 25 degrees, with a 1 for yes and a 0 for no. Another example would be a set of dummy variables for every month. This would require 12 new columns, each one representing a month, with the value of that column being 1 if the datapoint is in that month, and 0 if it is not. The purpose of these dummy variables is explained in the introduction, and the process for creating them is copied below:\n\n> In order to capture the effects of the non-linearities, the following feature extraction is proposed: we make dummy variables for every week and month of the year, dummy variables for temperature brackets, and then make cross terms between 1) the time dummy variables and the temperature dummy variables; 2) the time dummy variables and other variables; 3) the temperature dummy variables and other variables. \n\nThe extraction of the dummy variables takes place in `create_temp_and_time_dummies_plus_cross_terms()`, which is called by `feature_engineering()`. The addition of these dummy variables results in dataframes that typically have thousands of features, as opposed to the maximum of 26 in the Auser Aquifer dataset (excluding the date). When we use XGBRegressor to determine the most important features, we add the data from prior rows - for example, we add the data from the prior thirty days - to every row, thus resulting in dataframes that typically have tens of thousands of features. It is this process that allows us to determine which independent variables, from which time periods and temperature brackets, affect which target variables, thus capturing the inherent non-linear dynamics present in the natural environment.","7d22a825":"Since these dataframes consume a lot of memory, we will delete all of them. ","ac1b4a90":"We will begin by filtering for our target variables. After that, as with prior `Temperature` variables, we can estimate the missing values with those of past\/future years. The `Depth_to_Groundwater` variables show a very clear trend that we can interpolate. Lastly, we will impute the missing `Rainfall` variables.","e20900fa":"In typical machine learning problems, we would be given the date, rainfall, and temperature variables, and try to predict the hydrometry. However, this is a time-series prediction problem. In our problem, on any given day, we have all of the data in the row available. What we want to do is to predict the target variables of the _following_ days. For this, we need to add the values of the following days' target variable to the dataframe. ","4a555d9d":"Let's demonstrate how to predict the next 30 days from the last datapoint available for the Arno River. We will rerun the LSTM on the imputed dataset for the Arno River. \n\nWe will train on all the data except the last `num_days_to_predict` days, so that we do not leak data into our model. \n\nPlease note that this model is made to train as quickly as possible, and is not optimized for making the best predictions.","ddfd7918":"We can see that there is a lot of missing data, including many missing rows for the target variables.","d77b72fd":"There are no missing values remaining.","e84ed417":"## Arno River\n\nAccording to the provided description, \n\n> Description: Information about Arno river. The Arno is the second largest river in peninsular Italy and the main waterway in Tuscany and it  has a relatively torrential regime, due to the nature of the surrounding soils (marl and impermeable clays)\n\n> Outut: Hydrometry_Nave_di_Rosano\n\nFirst, let's take a look at the data:\n","441a3b5c":"## XGBRegressor Code","03ee6c56":"### Predictions","2626e658":"There are several years of nearly continous data for all the variables. We can say that after extensive experimentation, the best results were achieved when training on these several years. Furthermore, the results were still not so bad when we imputed the missing values and then trained and predicted based on imputed values.\n\nAs before, we will fill in the missing values of the temperature variable with those of prior years. The final date for which we have temperature data is 2017-08-03.\n\nWe will then create two dataframes. One of them will have only the almost complete data from 2004-01-01 to 2007-07-06. The other will have imputed values for all of the missing values, and will be used in the last section of this notebook, where we demonstrate how to make predictions. \n\nThere is potentially anomalous data for the target variable `Hydrometry_Nave_di_Rosano` for about half of 2008. However, this data will not be in our first dataset at all, and is not so relevant for our second dataset, which we are only using to demonstrate how to make future predictions, so we will leave it as is.","f665e43b":"For the Arno River, there are no missing dates. We can drop all of the rows where most of the data is missing, which is all the data prior to 2004-01-01.","5d6a6d88":"# Feature Engineering","76443946":"It looks likes there are 9 missing dates at the end, where all the data is NaN as well. Let's remove those dates and check again.","9222eadc":"Note: we import `matplotlib.pyplot` at the start of every data cleaning subsection because later use of matplotlib magic interferes with the plots shown here if these cells are executed after the matplotlib magic is invoked.","ddf536e3":"## Petrignano Aquifer","a086809e":"Since these dataframes consume a lot of memory, we will delete all of them. ","f300bd24":"### Predictions","b29eb7a2":"## Madonna di Canneto Water Spring","5706629e":"### Feature Importances","dc4e490c":"## Auser Aquifer","d3a764d6":"### Feature Importances","db6fe45a":"### Predictions","4ec9de68":"Let's take a look at our final two datasets. \n\nDataset using only rows with all the variables available:","3c9733fe":"## Arno River","0833e713":"The following code is commented out because it stops the remainder of the notebook from running if choosing to \"Run All\" - however, it leads us to find some missing dates in the dataset.","8567360b":"Let's take a look at the predictions based on the imputed data.","1f22a062":"We see that the shape of the input to predict is (31, 1638) - this makes sense, since our LSTM model makes use of the last 30 days of data when making predictions. It is the last row of this input for which we are making a prediction.","a5413fc3":"Now let's try with synthetic data, walk-forward validation, and training on prior years.","cfe0e46c":"### Feature Importances","be9c7f8f":"There are still a few problems. First, there are many missing values for the `Depth_to_Groundwater` variables, which is a particular problem, since they are our target variables. Second, it seems that the `Volume_CSA`, `Volume_CSAL`, `Temperature_Ponte_a_Moriano`, and some of the `Depth_to_Groundwater` variables have improbable values of 0 for extended periods of time, both early on in the modified dataset, and at some points at the end of it. \n\nIn the case of `Temperature_Ponte_a_Moriano`, we can see that upon manual inspection, the date where all the values start being zero is 2017-06-05. Furthermore, we see that this temperature variable is highly seasonal. Therefore, it is realistic to simply ascribe all of its zero-values from 2017-06-05 onwards to the value of prior years. We can do this because XGBRegressor and LSTM will make much heavier use of the dummy variables that we construct from the temperature (e.g. is the temperature between 30 - 35 degrees?), rather than the temperature itself, and since these dummy variables are much more coarsely grained then the temperature, they are much more likely to be accurate.\n\nIn the case of the `Depth_to_Groundwater` variables, we see that the missing values are sporadic; however there is a clear trend in each of them, and so interpolating the missing values is appropriate. For the `Depth_to_Groundwater` LT2 and DIEC variables, a linear interpolation seems reasonable. For the SAL, PAG, and CoS variables, a quadratic interpolation seems more suitable.","d33da5e2":"Let's see the results.","a73a16af":"Since these dataframes consume a lot of memory, we will delete all of them. ","7465b20c":"### Predictions","94758686":"This is a bit of an unsual dataset. There are some missing values interspered even with this filtered data. Ideally, we would use the data from 2016-04-13 to 2018-02-03 as our training data, and 2018-08-23 to 2018-12-08 as our test data. However, the LSTM code is hard to modify for non-continuous datasets, so we will use the dataset as it is after interpolating, only using the last portion as our test set."}}