{"cell_type":{"a9d7dffc":"code","d83d69c6":"code","c8a8f4b6":"code","de94934d":"code","0580fd5e":"code","7d8cf805":"code","48a748df":"code","0664c40e":"code","aa8f7d7d":"code","100b4929":"code","e8c7d15e":"code","f7f2cb3e":"code","48ba9d5e":"code","e19ad137":"code","d6c7b00f":"code","6a112e22":"code","277f7720":"code","f4da5a0c":"code","c173c82b":"markdown","aa9e5e8a":"markdown","7e9ba2e8":"markdown"},"source":{"a9d7dffc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d83d69c6":"import numpy as np\n\nVIF = False\nPCR = False\nVIF_THRESHOLD = 25\nSAMPLE_SIZE = 3000000\nREGR_ONLY = True\nDATA_TYPE = np.float32","c8a8f4b6":"import os\nimport gc\nimport joblib\nimport random\nimport pandas as pd\nfrom pathlib import Path\nfrom argparse import Namespace\nfrom collections import defaultdict\nfrom scipy.stats import pearsonr\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 64)\n\ndef seed_everything(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\ndef reduce_memory_usage(df, features):\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for feature in features:\n        item = df[feature].astype(DATA_TYPE)\n        df[feature] = item\n        del item\n        gc.collect()\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n        \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","de94934d":"#if VIF and PCR:\n#    if VIF_THRESHOLD > 4 and SAMPLE_SIZE > 2800000:\n#        SAMPLE_SIZE = 2800000\n#elif VIF:\n#    if VIF_THRESHOLD > 8 and SAMPLE_SIZE > 2800000:\n#        SAMPLE_SIZE = 2800000\n    \n#print(\"For VIF \" + str(VIF) + \" and PCR \" + str(PCR) + \" with VIF_THRESHOLD \" + str(VIF_THRESHOLD) + \" SAMPLE_SIZE set to \" + str(SAMPLE_SIZE))   \nif PCR:\n    SAMPLE_SIZE = 2250000","0580fd5e":"# Rows and Columns in train dataset: (3141410, 304)\nargs = Namespace(\n    seed=21,\n    folds=5,\n    workers=4,\n    samples=SAMPLE_SIZE,\n    data_path=Path(\"..\/input\/ubiquant-parquet\/\"),\n)\nseed_everything(args.seed)","7d8cf805":"%%time\ntrain = pd.read_parquet(args.data_path.joinpath(\"train_low_mem.parquet\"))\nassert train.isnull().any().sum() == 0, \"null exists.\"","48a748df":"n_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\nfeature_columns = ['investment_id', 'time_id'] + features\nreduce_memory_usage(train, features + [\"target\"])\n#train = reduce_mem_usage(train)\n#gc.collect()","0664c40e":"import tensorflow as tf\nfrom sklearn import metrics\n\ndef symmetric_mean_absolute_percentage_error(A, F):\n    return 100\/len(A) * np.sum(2 * np.abs(F - A) \/ (np.abs(A) + np.abs(F)))\ndef evaluate(model, x_val, y_val):\n    y_pred = model.predict(x_val)\n    r2 = metrics.r2_score(y_val, y_pred)\n    mse = metrics.mean_squared_error(y_val, y_pred)\n    mae = metrics.mean_absolute_error(y_val, y_pred)\n    mape = tf.keras.metrics.mean_absolute_percentage_error(y_val, y_pred).numpy()\n    rmse = np.sqrt(mse)\n    smape = symmetric_mean_absolute_percentage_error(y_val, y_pred)\n    print(\"R2 Score:\", r2)\n    print(\"MSE:\", mse)\n    print(\"MAE:\", mae)\n    print(\"MAPE\", mape)\n    print(\"RMSE:\", rmse)\n    print(\"SMAPE:\", smape)\n    return {\"r2\": r2, \"mse\": mse, \"mae\": mae, \"mape\": mape, \"rmse\": rmse, \"smape\": smape}","aa8f7d7d":"if VIF:\n    corr_matrix = train.iloc[1:1800000,4:].corr()\n    vif = pd.Series(np.linalg.inv(corr_matrix.values).diagonal(),index=train.columns[4:])\n    # colinear test\n    col = vif[vif.values >= VIF_THRESHOLD]\n    print(col.iloc[np.lexsort([col.index,col.values])])\n    del corr_matrix\n    del vif\n    gc.collect()\n    train.drop(columns=col.index,inplace=True)\n    print(train.shape)","100b4929":"if args.samples is not None:\n    #train = train.sample(args.samples, random_state=args.seed).reset_index(drop=True)\n    train = train[-args.samples:].reset_index(drop=True)\n    gc.collect()\ntrain.shape","e8c7d15e":"from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\ny = train[\"target\"]\nX = train.drop([\"row_id\",\"time_id\",\"investment_id\",\"target\"], axis = 1).astype(DATA_TYPE)\ndel train\ngc.collect()\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.25, \n                                                    random_state=42)\n\nprint(\"X_train\", X_train.shape)\n\nprint(\"y_train\",y_train.shape)\n\nprint(\"X_test\",X_test.shape)\n\nprint(\"y_test\",y_test.shape)\n\n#training = df.copy()\n\n#print(\"training\", training.shape)\ndel y_test\ndel X\ndel y\ngc.collect()","f7f2cb3e":"from sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.preprocessing import scale \n#pca = PCA()\nipca = IncrementalPCA(batch_size=1000) ","48ba9d5e":"if PCR:\n    X_regr = ipca.fit_transform(scale(X_train))\n    del X_train\n    gc.collect()\nelse:\n    print(\"X_train\",np.count_nonzero(np.isnan(X_train)))\n    X_regr = scale(X_train)\n    print(\"X_regr\",np.count_nonzero(np.isnan(X_regr)))\nX_regr.shape","e19ad137":"if PCR:\n    np.cumsum(np.round(ipca.explained_variance_ratio_, decimals = 4)*100)[0:5]","d6c7b00f":"%%time\nfrom sklearn.ensemble import VotingRegressor \nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.linear_model import Ridge, LinearRegression, ElasticNet, Lasso\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.pipeline import Pipeline\nkfold = StratifiedKFold(5, shuffle=True, random_state=42)\nfrom sklearn.pipeline import Pipeline\nif not REGR_ONLY:\n    models = []\n    use_k_fold = False\n    if use_k_fold:\n        for (train_indices, valid_indices) in kfold.split(X, X[\"investment_id\"]):\n            X_train, X_val = X.iloc[train_indices], X.iloc[valid_indices]\n            y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n            # MinMaxScaler\n            # RobustScaler\n            model = Pipeline([('scaler', RobustScaler()), ('ridge', Ridge())])\n            model.fit(X_train, y_train)\n            evaluate(model, X_val, y_val)\n            models.append(model)\n            del X_train\n            del X_val\n            del y_train\n            del y_val\n            gc.collect()\n    else:\n        #model = Ridge()\n        model = StackingRegressor([\n            (\"ridge\", Ridge()), \n            (\"lr\", LinearRegression()),\n            (\"elastic\", ElasticNet()),\n            (\"lasso\", Lasso())\n        ])\n        #model = Pipeline([('scaler', StandardScaler()), ('vr', vr)])\n        #model.fit(X, y)\n        print(X_regr.shape)\n        model.fit(X_regr, y_train)\n        #evaluate(model, X, y)\n        evaluate(model, X_regr, y_train)\n        y_pred = model.predict(X_regr)\n        print(pearsonr(y_train, y_pred)[0])\n        models.append(model)","6a112e22":"if REGR_ONLY:\n    lm = LinearRegression()\n    regr_model = lm.fit(X_regr, y_train)\n\n    print(regr_model.intercept_)","277f7720":"from sklearn.metrics import mean_squared_error, r2_score\nif REGR_ONLY:\n    print(regr_model.coef_)\n    y_pred = regr_model.predict(X_regr)\n    y_pred[0:5]\n    print(np.sqrt(mean_squared_error(y_train, y_pred)))\n    print(pearsonr(y_train, y_pred)[0])","f4da5a0c":"import ubiquant\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\n\n#scaler = joblib.load('scaler.joblib')\n#models = [joblib.load(f'lgbm_seed{args.seed}_{fold}.pkl') for fold in range(args.folds)]\nscaler = StandardScaler()\nif REGR_ONLY:\n    model = regr_model\nfor (test_df, sample_prediction_df) in iter_test:\n    if VIF:\n        test_df.drop(columns=col.index,inplace=True)\n    num_features = list(test_df.filter(like=\"f_\").columns)\n    test_df[num_features] = scale(test_df[num_features]) \n    #final_pred = [models[fold].predict(test_df[num_features]) for fold in range(args.folds)]\n    sample_prediction_df['target'] = model.predict(test_df[num_features])\n    env.predict(sample_prediction_df) \n    display(sample_prediction_df)","c173c82b":"***VIF***","aa9e5e8a":"For PCR different sample sizes seem possible for different VIF_THRESHOLD settings. The following seems true. Your result might vary?","7e9ba2e8":"Based in parts on...\n\n[Ubiquant Market Prediction with Ridge Regression](https:\/\/www.kaggle.com\/lonnieqin\/ubiquant-market-prediction-with-ridge-regression)\n\n[Ubiquant LGBM Baseline](https:\/\/www.kaggle.com\/valleyzw\/ubiquant-lgbm-baseline)\n\n[Principal Component Regression](https:\/\/www.kaggle.com\/phamvanvung\/principal-component-regression)\n\nVarious StackOverflow posts"}}