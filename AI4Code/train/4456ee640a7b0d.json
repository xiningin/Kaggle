{"cell_type":{"f28d5e7b":"code","a4f07719":"code","538fe208":"code","295e052e":"code","92b4598f":"code","f894b197":"code","12fd24bf":"code","d25ad4c1":"code","24e38f68":"code","e6618839":"code","88424629":"code","04b97dd4":"code","80fdca62":"markdown","648dd155":"markdown","452bbc53":"markdown","528cc232":"markdown","a8e34e12":"markdown","dda86d07":"markdown"},"source":{"f28d5e7b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4f07719":"from google.cloud import bigquery","538fe208":"client = bigquery.Client()","295e052e":"# creating a reference to the hacker news dataset within bigquery public data\ndataset_ref = client.dataset(\"hacker_news\", project = \"bigquery-public-data\")\n\n# creating an API request to reference the dataset\ndataset = client.get_dataset(dataset_ref)","92b4598f":"# creating a list of all the tables in the \"hacker_news\" dataset\ntables = list(client.list_tables(dataset))\n\nfor table in tables:\n    print(table.table_id)","f894b197":"# creating a table reference\ntable_ref = dataset_ref.table(\"full\")\ntable_ref2 = dataset_ref.table(\"comments\")\ntable_ref3 = dataset_ref.table(\"stories\")\n\n# API call \ntable = client.get_table(table_ref)\ntable2 = client.get_table(table_ref2)\ntable3 = client.get_table(table_ref3)","12fd24bf":"table.schema","d25ad4c1":"table2.schema","24e38f68":"table3.schema","e6618839":"client.list_rows(table2, max_results = 10).to_dataframe()","88424629":"client.list_rows(table, max_results = 10).to_dataframe()","04b97dd4":"client.list_rows(table, selected_fields = table.schema[2:14], max_results = 5).to_dataframe()","80fdca62":"Now, let's take a look at the **schema** of the table that we are referencing.","648dd155":"Using the `selected_fields()` function to list only **specific columns** to create a dataframe.","452bbc53":"Next, we'll work on creating a table to start querying the **specific table** that we want to explore.","528cc232":"## SQL Practice 1\n\nJust some code to learn using SQL integrated within the *Kaggle environment*. ","a8e34e12":"Creating a **google.cloud** conncetion with bigquery to use public datasets. ","dda86d07":"Let's use the `list_tables()` function to take a look at the dataset."}}