{"cell_type":{"e5a08e78":"code","f41b8c0f":"code","bc2be487":"code","e110b235":"code","c3629f84":"code","43e283e9":"code","a6c8e927":"code","5f4b232a":"code","f24a7b45":"code","6d91fca5":"code","f1f3522f":"code","0c4aa20c":"code","c01816fb":"code","91b99ec9":"code","8a981aec":"code","9896c760":"code","3d4b4c07":"code","3fdb7395":"code","cfeb89b2":"code","72108fdf":"code","53ba1846":"code","e2c2f5e3":"code","c1be1520":"code","61e79468":"code","db4c7f53":"code","28a6f79b":"code","b6935a80":"code","4b38aebc":"code","a4860d77":"code","25aa4692":"code","b20d45d7":"code","551e3cc3":"code","986777d2":"code","4d5102a1":"code","62fec386":"code","bd5e0346":"code","a3f11ad7":"code","5c3fc513":"code","1f8e6f61":"code","09624813":"code","e0e087ee":"code","fb0b3fa5":"code","45212e29":"code","ed1688fa":"code","7a5d2afc":"code","ac7d17d2":"code","1d284689":"code","f7512c90":"code","75b43f44":"code","695416a9":"code","83628a2c":"markdown","d2b5338e":"markdown","09fc8f2c":"markdown","24dc3b96":"markdown","7abf21ad":"markdown","8e97ee90":"markdown","c42991d2":"markdown","debe652b":"markdown","1ab1b3ca":"markdown"},"source":{"e5a08e78":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('..\/input\/train_transaction.csv')\ntest = pd.read_csv('..\/input\/test_transaction.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')","f41b8c0f":"useful_features = list(train.iloc[:, 3:55].columns)\n\ny = train.sort_values('TransactionDT')['isFraud']\nX = train.sort_values('TransactionDT')[useful_features]\nX_test = test[useful_features]\ndel train, test","bc2be487":"categorical_features = [\n    'ProductCD',\n    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n    'addr1', 'addr2',\n    'P_emaildomain',\n    'R_emaildomain',\n    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9'\n]\n\ncontinuous_features = list(filter(lambda x: x not in categorical_features, X))","e110b235":"class ContinuousFeatureConverter:\n    def __init__(self, name, feature, log_transform):\n        self.name = name\n        self.skew = feature.skew()\n        self.log_transform = log_transform\n        \n    def transform(self, feature):\n        if self.skew > 1:\n            feature = self.log_transform(feature)\n        \n        mean = feature.mean()\n        std = feature.std()\n        return (feature - mean)\/(std + 1e-6)        ","c3629f84":"from tqdm.autonotebook import tqdm\n\nfeature_converters = {}\ncontinuous_features_processed = []\ncontinuous_features_processed_test = []\n\nfor f in tqdm(continuous_features):\n    feature = X[f]\n    feature_test = X_test[f]\n    log = lambda x: np.log10(x + 1 - min(0, x.min()))\n    converter = ContinuousFeatureConverter(f, feature, log)\n    feature_converters[f] = converter\n    continuous_features_processed.append(converter.transform(feature))\n    continuous_features_processed_test.append(converter.transform(feature_test))\n    \ncontinuous_train = pd.DataFrame({s.name: s for s in continuous_features_processed}).astype(np.float32)\ncontinuous_test = pd.DataFrame({s.name: s for s in continuous_features_processed_test}).astype(np.float32)","43e283e9":"continuous_train['isna_sum'] = continuous_train.isna().sum(axis=1)\ncontinuous_test['isna_sum'] = continuous_test.isna().sum(axis=1)\n\ncontinuous_train['isna_sum'] = (continuous_train['isna_sum'] - continuous_train['isna_sum'].mean())\/continuous_train['isna_sum'].std()\ncontinuous_test['isna_sum'] = (continuous_test['isna_sum'] - continuous_test['isna_sum'].mean())\/continuous_test['isna_sum'].std()","a6c8e927":"isna_columns = []\nfor column in tqdm(continuous_features):\n    isna = continuous_train[column].isna()\n    if isna.mean() > 0.:\n        continuous_train[column + '_isna'] = isna.astype(int)\n        continuous_test[column + '_isna'] = continuous_test[column].isna().astype(int)\n        isna_columns.append(column)\n        \ncontinuous_train = continuous_train.fillna(0.)\ncontinuous_test = continuous_test.fillna(0.)","5f4b232a":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom tqdm.autonotebook import tqdm\n\ndef categorical_encode(df_train, df_test, categorical_features, n_values=140):\n    df_train = df_train[categorical_features].astype(str)\n    df_test = df_test[categorical_features].astype(str)\n    \n    categories = []\n    for column in tqdm(categorical_features):\n        categories.append(list(df_train[column].value_counts().iloc[: n_values - 1].index) + ['Other'])\n        values2use = categories[-1]\n        df_train[column] = df_train[column].apply(lambda x: x if x in values2use else 'Other')\n        df_test[column] = df_test[column].apply(lambda x: x if x in values2use else 'Other')\n        \n    \n    ohe = OneHotEncoder(categories=categories)\n    ohe.fit(pd.concat([df_train, df_test]))\n    df_train = pd.DataFrame(ohe.transform(df_train).toarray()).astype(np.float16)\n    df_test = pd.DataFrame(ohe.transform(df_test).toarray()).astype(np.float16)\n    return df_train, df_test","f24a7b45":"for feat in categorical_features:\n    print(X[feat].nunique())","6d91fca5":"train_categorical, test_categorical = categorical_encode(X, X_test, categorical_features)","f1f3522f":"num_shape = continuous_train.shape[1]\ncat_shape = train_categorical.shape[1]","0c4aa20c":"X = pd.concat([continuous_train, train_categorical], axis=1)\ndel continuous_train, train_categorical\nX_test = pd.concat([continuous_test, test_categorical], axis=1)\ndel continuous_test, test_categorical","c01816fb":"test_rows = X_test.shape[0]","91b99ec9":"X = pd.concat([X, X_test], axis = 0)","8a981aec":"del X_test","9896c760":"import keras\nimport random\nimport tensorflow as tf\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, BatchNormalization, Activation\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.optimizers import Adam, Nadam\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import roc_auc_score\n\nnp.random.seed(42) # NumPy\nrandom.seed(42) # Python\ntf.set_random_seed(42) # Tensorflow","3d4b4c07":"# Compatible with tensorflow backend\nclass roc_callback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        print('\\rroc-auc_val: %s' % (str(round(roc_val,4))),end=100*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\n    \ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.mean((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n    return focal_loss_fixed\n\ndef custom_gelu(x):\n    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 \/ np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n\nget_custom_objects().update({'custom_gelu': Activation(custom_gelu)})\nget_custom_objects().update({'focal_loss_fn': focal_loss()})","3fdb7395":"from keras.layers import concatenate","cfeb89b2":"K.clear_session()\nfrom keras.optimizers import Adam\ndef create_model():\n    num_inp = Input(shape=(num_shape,))\n    cat_inp = Input(shape=(cat_shape,))\n    inps = concatenate([num_inp, cat_inp])\n    x = Dense(512, activation=custom_gelu)(inps)\n    x = Dense(256, activation=custom_gelu)(x)\n    x = Dense(512, activation = custom_gelu)(x)\n    x = Dropout(.2)(x)\n    cat_out = Dense(cat_shape, activation = \"linear\")(x)\n    num_out = Dense(num_shape, activation = \"linear\")(x)\n    model = Model(inputs=[num_inp, cat_inp], outputs=[num_out, cat_out])\n    model.compile(\n        optimizer=Adam(.05, clipnorm = 1, clipvalue = 1),\n        loss=[\"mse\", \"mse\"]\n    )\n    return model","72108fdf":"model_mse = create_model()","53ba1846":"model_mse.summary()","e2c2f5e3":"def inputSwapNoise(arr, p):\n    n, m = arr.shape\n    idx = range(n)\n    swap_n = round(n*p)\n    for i in range(m):\n        col_vals = np.random.permutation(arr[:, i]) # change the order of the row\n        swap_idx = np.random.choice(idx, size= swap_n) # choose row\n        arr[swap_idx, i] = np.random.choice(col_vals, size = swap_n) # n*p row and change it \n    return arr","c1be1520":"def auto_generator(X, swap_rate, batch_size):\n    indexes = np.arange(X.shape[0])\n    while True:\n        np.random.shuffle(indexes)\n        num_X = X[indexes[:batch_size], :num_shape] \n        num_y = inputSwapNoise(num_X, swap_rate)\n        cat_X = X[indexes[:batch_size], num_shape:] \n        cat_y = inputSwapNoise(cat_X, swap_rate)\n        yield [num_y, cat_y], [num_X, cat_X]","61e79468":"batch_size = 2048","db4c7f53":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler\nauto_ckpt = ModelCheckpoint(\"ae.model\", monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min', period=1)","28a6f79b":"from keras import backend as K\n\n\nclass WarmUpLearningRateScheduler(keras.callbacks.Callback):\n    \"\"\"Warmup learning rate scheduler\n    \"\"\"\n\n    def __init__(self, warmup_batches, init_lr, verbose=0):\n        \"\"\"Constructor for warmup learning rate scheduler\n\n        Arguments:\n            warmup_batches {int} -- Number of batch for warmup.\n            init_lr {float} -- Learning rate after warmup.\n\n        Keyword Arguments:\n            verbose {int} -- 0: quiet, 1: update messages. (default: {0})\n        \"\"\"\n\n        super(WarmUpLearningRateScheduler, self).__init__()\n        self.warmup_batches = warmup_batches\n        self.init_lr = init_lr\n        self.verbose = verbose\n        self.batch_count = 0\n        self.learning_rates = []\n\n    def on_batch_end(self, batch, logs=None):\n        self.batch_count = self.batch_count + 1\n        lr = K.get_value(self.model.optimizer.lr)\n        self.learning_rates.append(lr)\n\n    def on_batch_begin(self, batch, logs=None):\n        if self.batch_count <= self.warmup_batches:\n            lr = self.batch_count*self.init_lr\/self.warmup_batches\n            K.set_value(self.model.optimizer.lr, lr)\n            if self.verbose > 0:\n                print('\\nBatch %05d: WarmUpLearningRateScheduler setting learning '\n                      'rate to %s.' % (self.batch_count + 1, lr))\nwarm_up_lr = WarmUpLearningRateScheduler(400, init_lr=0.005)","b6935a80":"import gc\ngc.collect()","4b38aebc":"gc.collect()\nepochs = 10\ntrain_gen = auto_generator(X.values, .15, batch_size)\nhist = model_mse.fit_generator(train_gen, steps_per_epoch=len(X)\/\/batch_size, epochs=epochs,\n                           verbose=1, workers=-1, \n                           use_multiprocessing=True,\n                              callbacks=[auto_ckpt, warm_up_lr])","a4860d77":"del train_gen\ngc.collect()\nmodel_mse.load_weights(\"ae.model\")","25aa4692":"for layer in model_mse.layers:\n    layer.trainable = False\nmodel_mse.compile(\n    optimizer=\"adam\",\n    loss=[\"mse\", \"mse\"]\n)","b20d45d7":"model_mse.summary()","551e3cc3":"def make_model(loss_fn):\n    x1 = model_mse.layers[3].output\n    x2 = model_mse.layers[4].output\n    x3 = model_mse.layers[5].output\n    x_conc = concatenate([x1,x2,x3])\n    x = Dropout(.5)(x_conc)\n    x = Dense(500, activation='relu')(x)\n    x = Dropout(.5)(x)\n    x = Dense(200, activation='relu')(x)\n    x = Dropout(.5)(x)\n    x = Dense(100, activation='relu')(x)\n    x = Dropout(.5)(x)\n    x = Dense(1, activation = 'sigmoid')(x)\n    model = Model([model_mse.layers[0].input, model_mse.layers[1].input], x)\n    model.compile(\n        optimizer=\"adam\",\n        loss=[loss_fn]\n    )\n    return model","986777d2":"fraud_model = make_model(\"binary_crossentropy\")\nfraud_focal_model = make_model(\"focal_loss_fn\")","4d5102a1":"X_test = X.iloc[-test_rows:, :]\nX = X.iloc[:-test_rows, :]","62fec386":"import gc\ngc.collect()","bd5e0346":"split_ind = int(X.shape[0]*0.8)\n\nX_tr = X.iloc[:split_ind]\nX_val = X.iloc[split_ind:]\n\ny_tr = y.iloc[:split_ind]\ny_val = y.iloc[split_ind:]\n\ndel X","a3f11ad7":"from keras.callbacks import ModelCheckpoint\nckpt = ModelCheckpoint(\"best_fraud.model\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min', period=1)","5c3fc513":"gc.collect()","1f8e6f61":"fraud_model.fit([X_tr.iloc[:, :num_shape], X_tr.iloc[:, num_shape:]], y_tr, epochs=100, batch_size=2048, \n                validation_data = ([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], y_val),\n               callbacks=[ckpt], verbose = 2)","09624813":"valid_preds = fraud_model.predict([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], batch_size = 8000, verbose = True)\nroc_auc_score(y_val, valid_preds)","e0e087ee":"fraud_model.load_weights(\"best_fraud.model\")\nvalid_preds = fraud_model.predict([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], batch_size = 8000, verbose = True)\nroc_auc_score(y_val, valid_preds)","fb0b3fa5":"ckpt2 = ModelCheckpoint(\"best_fraud_focal.model\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min', period=1)\nfraud_focal_model.fit([X_tr.iloc[:, :num_shape], X_tr.iloc[:, num_shape:]], y_tr, epochs=100, batch_size=2048, \n                validation_data = ([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], y_val),\n               callbacks=[ckpt2], verbose = 2)","45212e29":"fraud_model.load_weights(\"best_fraud.model\")","ed1688fa":"fraud_focal_model.load_weights(\"best_fraud_focal.model\")","7a5d2afc":"valid_preds = fraud_model.predict([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], batch_size = 8000, verbose = True)\nroc_auc_score(y_val, valid_preds)","ac7d17d2":"from scipy.stats import rankdata, spearmanr\nvalid_preds = fraud_model.predict([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], batch_size = 8000, verbose = True)\nvalid_preds2 = fraud_focal_model.predict([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], batch_size = 8000, verbose = True)\nscore = roc_auc_score(y_val, valid_preds)\nscore2 = roc_auc_score(y_val, valid_preds2)\nscore_avg = roc_auc_score(y_val, (.5*valid_preds) + (.5*valid_preds2))\nprint(score)\nprint(score2)\nprint(score_avg)\nprint('Rank averaging: ', roc_auc_score(y_val, rankdata(valid_preds, method='dense') + rankdata(valid_preds2, method='dense')))","1d284689":"X_tr = pd.concat([X_tr, X_val, X_val, X_val, X_val], axis = 0)\ny_tr = pd.concat([y_tr, y_val, y_val, y_val, y_val], axis = 0)","f7512c90":"fraud_model.fit([X_tr.iloc[:, :num_shape], X_tr.iloc[:, num_shape:]], y_tr, epochs=10, batch_size=2048)\nfraud_focal_model.fit([X_tr.iloc[:, :num_shape], X_tr.iloc[:, num_shape:]], y_tr, epochs=10, batch_size=2048)","75b43f44":"test_preds = fraud_model.predict([X_test.iloc[:, :num_shape], X_test.iloc[:, num_shape:]], batch_size = 8000)\ntest_preds2 = fraud_focal_model.predict([X_test.iloc[:, :num_shape], X_test.iloc[:, num_shape:]], batch_size = 8000)","695416a9":"sub['isFraud'] = rankdata(test_preds, method='dense') + rankdata(test_preds2, method='dense')\nsub.isFraud = sub.isFraud\/sub.isFraud.max()\nsub.to_csv('submission.csv', index=False)","83628a2c":"I've always wanted to try autoencoders for tabular data and finally found an excuse to try them out. This kernel heavily borrows from https:\/\/www.kaggle.com\/abazdyrev\/keras-nn-focal-loss-experiments who did a better job formatting the data than I did in my first NN starter kernel. https:\/\/www.kaggle.com\/ryches\/keras-nn-starter-w-time-series-split. It takes inspiration from Christof's post here https:\/\/www.kaggle.com\/c\/petfinder-adoption-prediction\/discussion\/88740#latest-515210 and Michael Jahrer's famous Porto Seguro solution","d2b5338e":"In the autoencoder we were able to take advantage of being able to train on both the train and test set because the autoencoder was trying to guess inputs rather than our target. No we will split our test set back out because we will be training on those targets for the second phase and we dont have that information for the test set","09fc8f2c":"Now we need to invent some realistic noise. As Michael noted in his post he used something he called swap noise. What this is doing is swapping a columns values with other possible values from that column a certain percentage of the time. For example say there is a feature like TransactionAMT. If we used swap noise on that column it would swap 15% of the rows of the TransactionAMT column with other possible values (like swapping 20 for 400, etc.). The model would then see 400 was swapped in and all of the other features around it and it would try to learn that 20 was the real original value and try to correct the various errors we have introduced into the input.  ","24dc3b96":"What we will do here is construct a simple autoencoder that will take in our noised numeric and categorical features, concatenate them and then pass them through several dense layers that will then try to predict our original unnoised numeric and categorical features. What this will do is in essence try to learn the relationships between the features and which features should co-occur. ","7abf21ad":"Now we will train the autoencoder using our generator for several epochs","8e97ee90":"Now we will freeze the layers of the autoencoder","c42991d2":"We will create a small generator so that we can continuously do this swapping and create new samples for the model to see","debe652b":"Next we will make a new model that branches off the previous one. This will take in non-noisy inputs and pass them through the encoding part of the autoencoder and then concatenated all of the middle layers of the encoder and then we will train our classifier based on the features that concatenated encoder outputs. ","1ab1b3ca":"We will train one with binary crossentropy and another with focal loss just like the previous kernel"}}