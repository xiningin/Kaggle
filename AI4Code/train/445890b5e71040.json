{"cell_type":{"01f46316":"code","c62e8bbb":"code","d21a6ef3":"code","a59d7f1e":"code","342f1a0d":"code","b316baeb":"code","acde5754":"code","86044208":"code","9cd405a4":"code","c862b2f9":"code","b3b9f68e":"code","0de1d2f1":"code","f1a0ce22":"code","240db7af":"code","15bf51b1":"code","8a74315d":"code","d280373f":"code","2db46500":"code","368d9da2":"code","838c8263":"code","0563f696":"code","bb53dce8":"code","59f9b589":"code","02f917ff":"code","b417a20f":"code","adb8dc45":"code","1b57a88a":"code","654d5e7c":"code","28606857":"code","11541bfd":"code","ba240d2e":"code","a98cae18":"code","7b361898":"code","8b020e65":"code","9fa64fd7":"code","3ac4670e":"code","60577c3d":"code","c3065d8e":"code","e5f3e3e0":"code","3ed1c01a":"markdown","74d998be":"markdown","961d450f":"markdown","f0d26ff7":"markdown","8cd1ec76":"markdown","b2683ddf":"markdown","38f27c1c":"markdown","d5c41dc7":"markdown","9356acd6":"markdown","14a47509":"markdown"},"source":{"01f46316":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c62e8bbb":"\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","d21a6ef3":"path = '\/kaggle\/input\/data_cleaned.csv'","a59d7f1e":"data = pd.read_csv(path)\ndata.head()","342f1a0d":"#shape of the data\ndata.shape","b316baeb":"data.columns","acde5754":"#checking missing values in the data\ndata.isnull().sum()","86044208":"data.describe","9cd405a4":"#seperating independent and dependent variables\ny = data['Survived']\nX = data.drop(['Survived'], axis=1)","c862b2f9":"#importing train_test_split to create validation set\nfrom sklearn.model_selection import train_test_split","b3b9f68e":"#creating the train and validation set\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state = 101, stratify=y, test_size=0.25)","0de1d2f1":"# distribution in training set\ny_train.value_counts(normalize=True)","f1a0ce22":"# distribution in validation set\ny_valid.value_counts(normalize=True)","240db7af":"#shape of training set\nX_train.shape, y_train.shape","15bf51b1":"#shape of validation set\nX_valid.shape, y_valid.shape","8a74315d":"#importing decision tree classifier \nfrom sklearn.tree import DecisionTreeClassifier","d280373f":"# how to import decision tree regressor\nfrom sklearn.tree import DecisionTreeRegressor","2db46500":"#creating the decision tree function\ndt_model = DecisionTreeClassifier(random_state=10)","368d9da2":"#fitting the model\ndt_model.fit(X_train, y_train)","838c8263":"#checking the training score\ndt_model.score(X_train, y_train)","0563f696":"#checking the validation score\ndt_model.score(X_valid, y_valid)","bb53dce8":"#predictions on validation set\ndt_model.predict(X_valid)","59f9b589":"dt_model.predict_proba(X_valid)","02f917ff":"y_pred = dt_model.predict_proba(X_valid)[:,1]","b417a20f":"y_new = []\nfor i in range(len(y_pred)):\n    if y_pred[i]<=0.7:\n        y_new.append(0)\n    else:\n        y_new.append(1)","adb8dc45":"from sklearn.metrics import accuracy_score","1b57a88a":"accuracy_score(y_valid, y_new)","654d5e7c":"train_accuracy = []\nvalidation_accuracy = []\nfor depth in range(1,10):\n    dt_model = DecisionTreeClassifier(max_depth=depth, random_state=10)\n    dt_model.fit(X_train, y_train)\n    train_accuracy.append(dt_model.score(X_train, y_train))\n    validation_accuracy.append(dt_model.score(X_valid, y_valid))","28606857":"frame = pd.DataFrame({'max_depth':range(1,10), 'train_acc':train_accuracy, 'valid_acc':validation_accuracy})\nframe.head()","11541bfd":"plt.figure(figsize=(12,6))\nplt.plot(frame['max_depth'], frame['train_acc'], marker='o')\nplt.plot(frame['max_depth'], frame['valid_acc'], marker='o')\nplt.xlabel('Depth of tree')\nplt.ylabel('performance')\nplt.legend()","ba240d2e":"dt_model = DecisionTreeClassifier(max_depth=8, max_leaf_nodes=25, random_state=10)","a98cae18":"#fitting the model\ndt_model.fit(X_train, y_train)","7b361898":"#Training score\ndt_model.score(X_train, y_train)","8b020e65":"#Validation score\ndt_model.score(X_valid, y_valid)","9fa64fd7":"from sklearn import tree","3ac4670e":"!pip install graphviz","60577c3d":"decision_tree = tree.export_graphviz(dt_model,out_file='tree.dot',feature_names=X_train.columns,max_depth=2,filled=True)","c3065d8e":"!dot -Tpng tree.dot -o tree.png","e5f3e3e0":"image = plt.imread('tree.png')\nplt.figure(figsize=(15,15))\nplt.imshow(image)","3ed1c01a":"\n\n# Disadvantages\n\n1. Overfitting\n2. Not fit for continuous variables","74d998be":"# Entropy\n \nEntropy is a measure of the randomness in the information being processed. The higher the entropy, the harder it is to draw any conclusions from that information. Flipping a coin is an example of an action that provides information that is random.","961d450f":"# **importing libraries **","f0d26ff7":"# **What is a Decision Tree ?**\n\n\nDecision tree is a type of supervised learning algorithm that is mostly used in classification problems. It works for both categorical and continuous input and output variables. \n\nIn this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter \/ differentiator in input variables.\n\n# **Example:-**\n\nLet\u2019s say we have a sample of 30 students with three variables Gender (Boy\/ Girl), Class( IX\/ X) and Height (5 to 6 ft). 15 out of these 30 play cricket in leisure time. Now, I want to create a model to predict who will play cricket during leisure period? In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three.\n\nThis is where decision tree helps, it will segregate the students based on all values of three variable and identify the variable, which creates the best homogeneous sets of students (which are heterogeneous to each other). In the snapshot below, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables.","8cd1ec76":"# Gini Index\n \nYou can understand the Gini index as a cost function used to evaluate splits in the dataset. It is calculated by subtracting the sum of the squared probabilities of each class from one. It favors larger partitions and easy to implement whereas information gain favors smaller partitions with distinct values.\n\n\nGini Index works with the categorical target variable \u201cSuccess\u201d or \u201cFailure\u201d. It performs only Binary splits.\n\nHigher the value of Gini index higher the homogeneity.\n\n# Reduction in Variance\n \nReduction in variance is an algorithm used for continuous target variables (regression problems). This algorithm uses the standard formula of variance to choose the best split. The split with lower variance is selected as the criteria to split the population:\n\n\n# Chi-Square\n \nThe acronym CHAID stands for Chi-squared Automatic Interaction Detector. It is one of the oldest tree classification methods. It finds out the statistical significance between the differences between sub-nodes and parent node. We measure it by the sum of squares of standardized differences between observed and expected frequencies of the target variable.\n\nIt works with the categorical target variable \u201cSuccess\u201d or \u201cFailure\u201d. It can perform two or more splits. Higher the value of Chi-Square higher the statistical significance of differences between sub-node and Parent node.\n\n","b2683ddf":"# Types of Decision Trees\n\nTypes of decision trees are based on the type of target variable we have. It can be of two types:\n\n1. **Categorical Variable Decision Tree:** Decision Tree which has a categorical target variable then it called a Categorical variable decision tree.\n\n2. **Continuous Variable Decision Tree:** Decision Tree has a continuous target variable then it is called Continuous Variable Decision Tree.\n\n\n# Example:- \n\nLet\u2019s say we have a problem to predict whether a customer will pay his renewal premium with an insurance company (yes\/ no). Here we know that the income of customers is a significant variable but the insurance company does not have income details for all customers. Now, as we know this is an important variable, then we can build a decision tree to predict customer income based on occupation, product, and various other variables. In this case, we are predicting values for the continuous variables.\n\n# Important Terminology related to Tree based Algorithms\n\nLet\u2019s look at the basic terminology used with Decision trees:\n\n1.** Root Node:** It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n\n2.** Splitting:** It is a process of dividing a node into two or more sub-nodes.\n\n3. Decision Node: When a sub-node splits into further sub-nodes, then it is called decision node.\n\n4. Leaf\/ Terminal Node: Nodes do not split is called Leaf or Terminal node.\n\n5. Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n\n6. Branch \/ Sub-Tree: A sub section of entire tree is called branch or sub-tree.\n\n7. Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node.\n\nThese are the terms commonly used for decision trees.","38f27c1c":"**Changing the max_depth**","d5c41dc7":"# Advantages\n\n1. Easy to understand\n\n2. Useful in Data exploration\n\n3. Less data cleaning required\n\n4. Data type is not a constraint\n\n5. Non Parametric Method","9356acd6":"- max_leaf_nodes\n- min_samples_split\n- min_samples_leaf","14a47509":"# Information Gain\n \nInformation gain or IG is a statistical property that measures how well a given attribute separates the training examples according to their target classification. Constructing a decision tree is all about finding an attribute that returns the highest information gain and the smallest entropy.\n\nInformation gain is a decrease in entropy. It computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values. ID3 (Iterative Dichotomiser) decision tree algorithm uses information gain."}}