{"cell_type":{"036060d8":"code","905503a6":"code","622325e2":"code","b8ee3383":"code","e35a5f94":"code","4739f6ef":"code","07830a1e":"code","5deded6d":"code","f9d3aa97":"code","33f7f92c":"code","cb3a504c":"code","25026b45":"code","28407dee":"code","1cd72c3c":"code","2a8e6ba8":"code","88d29480":"code","98307c77":"code","3ce15b5c":"code","3ddc07b0":"code","a800f191":"code","0d444674":"code","ccbbfdc6":"code","8ba45a51":"code","8c4e0ebd":"code","e7e58d3e":"code","9dc1cc04":"code","f9d1555d":"code","0dcccc3a":"code","65043e77":"code","a7eb2a52":"code","f4b53172":"code","0da00e2f":"code","5b24a5bb":"code","e91d7083":"code","ca2c9073":"code","35cf8a51":"code","9c8ad947":"code","85c9d63e":"code","9ca02301":"code","95f7feb0":"code","276bcde3":"code","41cbbadd":"code","a3a44a46":"code","208b4433":"code","f55bdd6b":"code","0c66e176":"code","dc9a6b18":"code","c66c4ecb":"code","f3e8320e":"code","0098e404":"code","bc31194f":"code","5bc7eed0":"code","129fa9cd":"code","77b77135":"code","b438492b":"code","acb1d245":"code","e5179ab3":"code","e1f8b810":"code","36e2ebdc":"code","6b1df8d2":"code","2fb8d2d6":"code","8200d9f8":"code","d979cda0":"code","29e33c3c":"code","ac92184d":"code","2780d91b":"code","d85ef28d":"code","06c208d4":"code","959476d3":"code","611f2a2c":"code","c324581e":"code","5557cbc3":"code","2739b4ba":"code","3d280828":"code","759c5284":"code","24b1a16f":"code","a6c1f136":"code","2ecf7420":"code","a2d6c675":"code","b7d1cc77":"code","3f9c5121":"code","f5a42803":"markdown","aaf28932":"markdown","8fc2f7d6":"markdown","e60d3b83":"markdown","da9a15ea":"markdown","6bfc5598":"markdown","bbe15c5f":"markdown","49e2d664":"markdown","d54c6989":"markdown","63618537":"markdown","4dbc5918":"markdown","b450ce28":"markdown","c19dc8d5":"markdown","810eecb3":"markdown","5a42107a":"markdown","76d2f85c":"markdown","b936c9e0":"markdown","d20f7d6e":"markdown","e347b60e":"markdown","fb06344f":"markdown","d7ab4cfc":"markdown","0e20d3b1":"markdown","a029dcd0":"markdown","eb33f94d":"markdown","a9ec0f9a":"markdown","ee9593c4":"markdown","240b87ce":"markdown","05fc45ce":"markdown","81b3bce7":"markdown","0536128c":"markdown","6a6883e1":"markdown","0dd93d33":"markdown","ed6739bb":"markdown","5a20dfac":"markdown","1fb7bc1a":"markdown","816cdc1e":"markdown","ef47c281":"markdown","fd52289b":"markdown","1e1cd23a":"markdown","1a93da7b":"markdown","fa15ffb7":"markdown","fabdb19c":"markdown","e054b4d7":"markdown","0529d142":"markdown","83595e47":"markdown","eca37566":"markdown","f0d21f2e":"markdown","0f3f3679":"markdown","89156a8c":"markdown","740a9e99":"markdown","41a34e49":"markdown","12f48a78":"markdown","95cf1476":"markdown","de4129e0":"markdown","4c0d7f78":"markdown","afbc17b8":"markdown","b8eab89e":"markdown"},"source":{"036060d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n","905503a6":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats, integrate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt \n%matplotlib inline\npd.options.display.float_format = '{:.2f}'.format\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats, integrate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\npd.options.display.float_format = '{:.2f}'.format\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 14","622325e2":"# Importing the train and test datasets\n\n\ntrain = pd.read_csv(\"..\/input\/Train.csv\")\ntest = pd.read_csv(\"..\/input\/Test.csv\")","b8ee3383":"# Making copy of dataset\n\ntrain_original=train.copy() \ntest_original=test.copy()","e35a5f94":"train_original.shape, test_original.shape","4739f6ef":"print(train_original.head())\nprint (test_original.head())","07830a1e":"train.info(), test.info()","5deded6d":"import datetime \n\ntrain['Datetime'] = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M',infer_datetime_format=True) \ntest['Datetime'] = pd.to_datetime(test.Datetime,format='%d-%m-%Y %H:%M', infer_datetime_format=True) \ntest_original['Datetime'] = pd.to_datetime(test_original.Datetime,format='%d-%m-%Y %H:%M', infer_datetime_format=True) \ntrain_original['Datetime'] = pd.to_datetime(train_original.Datetime,format='%d %m %Y %H:%M',  infer_datetime_format=True)\n","f9d3aa97":"train_original.head()","33f7f92c":"for i in (train, test, test_original, train_original):\n    i['year']=i.Datetime.dt.year \n    i['month']=i.Datetime.dt.month \n    i['day']=i.Datetime.dt.day\n    i['Hour']=i.Datetime.dt.hour ","cb3a504c":"train['day of week']=train['Datetime'].dt.dayofweek \ntemp = train['Datetime']","25026b45":"temp.head()","28407dee":"def applyer(row):\n    if row.dayofweek == 5 or row.dayofweek == 6:\n        return 1\n    else:\n        return 0 \ntemp2 = train['Datetime'].apply(applyer) \ntrain['weekend']=temp2","1cd72c3c":"from pandas.plotting import register_matplotlib_converters\n\ntrain.index = train['Datetime'] # indexing the Datetime to get the time period on the x-axis. \ndf=train.drop('ID',1)           # drop ID variable to get only the Datetime on x-axis. \nts = df['Count'] \nplt.figure(figsize=(16,8)) \nplt.plot(ts, label='Passenger Count') \nplt.title('Time Series') \nplt.xlabel(\"Time(year-month)\") \nplt.ylabel(\"Passenger count\") \nplt.legend(loc='best')","2a8e6ba8":"ts.head()","88d29480":"# dropping ID \n\ndf.tail()","98307c77":"# different way of plotting passenger count for training dataset. \n\ntrain.Count.plot(figsize=(16, 8))\nplt.title('Time Series') \nplt.xlabel(\"Time(year-month)\") \nplt.ylabel(\"Passenger count\") ","3ce15b5c":"train.groupby('year')['Count'].mean().plot.bar(fontsize=14,figsize=(10,7),title='Yearly Passenger Count')","3ddc07b0":"train.groupby('month')['Count'].mean().plot.bar(fontsize=14,figsize=(10,7), title='Monthly Passenger Count')","a800f191":"temp=train.groupby(['year', 'month'])['Count'].mean() \ntemp.plot(figsize=(15,5), title= 'Passenger Count(Year& Month)', fontsize=14)","0d444674":"train.groupby('day')['Count'].mean().plot.bar(fontsize=14,figsize=(10,7),title='Daily_PassengerCount')","ccbbfdc6":"train.groupby('Hour')['Count'].mean().plot.bar(color='m', figsize=(10,7),fontsize=14,title='Hourly_PassengerCount')","8ba45a51":"train.groupby('weekend')['Count'].mean().plot.bar(fontsize=14,figsize=(10,7),title='Weekend_PassengerCount')","8c4e0ebd":"train.groupby('day of week')['Count'].mean().plot.bar(fontsize=14,figsize=(10,7), title='Day of week_PassengerCount')","e7e58d3e":"train=train.drop('ID',1)","9dc1cc04":"train.Timestamp = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M') \ntrain.index = train.Timestamp \n# Hourly time series \nhourly = train.resample('H').mean() \n# Converting to daily mean \ndaily = train.resample('D').mean() \n# Converting to weekly mean \nweekly = train.resample('W').mean() \n# Converting to monthly mean \nmonthly = train.resample('M').mean()","f9d1555d":"fig, axs = plt.subplots(4,1) \nhourly.Count.plot(figsize=(15,8), title= 'Hourly', fontsize=14, ax=axs[0]) \ndaily.Count.plot(figsize=(15,8), title= 'Daily', fontsize=14, ax=axs[1])\nweekly.Count.plot(figsize=(15,8), title= 'Weekly', fontsize=14, ax=axs[2]) \nmonthly.Count.plot(figsize=(15,8), title= 'Monthly', fontsize=14, ax=axs[3]) ","0dcccc3a":"train.shape, test.shape","65043e77":"test.Timestamp = pd.to_datetime(test.Datetime,format='%d-%m-%Y %H:%M') \ntest.index = test.Timestamp  \n# Converting to daily mean \ntest = test.resample('D').mean() ","a7eb2a52":"\ntrain.Timestamp = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M') \ntrain.index = train.Timestamp \n# Converting to daily mean \ntrain = train.resample('D').mean()","f4b53172":"Train=train.loc['2012-08-25':'2014-06-24'] \nvalid=train.loc['2014-06-25':'2014-09-25']","0da00e2f":"Train.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14, label='train') \nvalid.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14, label='valid') \nplt.xlabel(\"Datetime\") \nplt.ylabel(\"Passenger count\") \nplt.legend(loc='best') \nplt.show()","5b24a5bb":"dd= np.asarray(Train.Count) \ny_hat = valid.copy() \ny_hat['naive'] = dd[len(dd)-1] \nplt.figure(figsize=(12,8)) \nplt.plot(Train.index, Train['Count'], label='Train') \nplt.plot(valid.index,valid['Count'], label='Valid') \nplt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast') \nplt.legend(loc='best') \nplt.title(\"Naive Forecast\") \nplt.show()","e91d7083":"# calculating RMSE to check the accuracy of our model on validation data set.\n\nfrom sklearn.metrics import mean_squared_error \nfrom math import sqrt \nrms = sqrt(mean_squared_error(valid.Count, y_hat.naive)) \nprint(rms)","ca2c9073":"# Considering rolling mean for last 10, 20, 50 days and visualize the results.\n\ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(10).mean().iloc[-1] # average of last 10 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 10 observations') \nplt.legend(loc='best') \nplt.show() \ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(20).mean().iloc[-1] # average of last 20 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 20 observations') \nplt.legend(loc='best') \nplt.show() \ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(50).mean().iloc[-1] # average of last 50 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 50 observations') \nplt.legend(loc='best') \nplt.show()","35cf8a51":"# RMSE value for Moving Average \n\nrms = sqrt(mean_squared_error(valid.Count, y_hat_avg.moving_avg_forecast)) \nprint(rms)\n","9c8ad947":"#Here the predictions are made by assigning larger weight to the recent values and lesser weight to the old values.\n\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt \n\ny_hat_avg = valid.copy() \nfit2 = SimpleExpSmoothing(np.asarray(Train['Count'])).fit(smoothing_level=0.6,optimized=False) \ny_hat_avg['SES'] = fit2.forecast(len(valid)) \nplt.figure(figsize=(16,8)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['SES'], label='SES') \nplt.legend(loc='best') \nplt.show()","85c9d63e":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.SES)) \nprint(rms)","9ca02301":"import statsmodels.api as sm \nsm.tsa.seasonal_decompose(Train.Count).plot() \nresult = sm.tsa.stattools.adfuller(train.Count) \nplt.show()","95f7feb0":"y_hat_avg = valid.copy() \nfit1 = Holt(np.asarray(Train['Count'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1) \ny_hat_avg['Holt_linear'] = fit1.forecast(len(valid)) \nplt.figure(figsize=(16,8)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear') \nplt.legend(loc='best') \nplt.show()","276bcde3":"# Calculating the RMSE of the model\n\nrms = sqrt(mean_squared_error(valid.Count, y_hat_avg.Holt_linear)) \nprint(rms)","41cbbadd":"y_hat_avg.Holt_linear.head()\n","a3a44a46":"valid.Count.shape, y_hat_avg.Holt_linear.shape","208b4433":"# loading the submission file.\n\n\nsubmission=pd.read_csv(your_local_path+\"submission.csv\")","f55bdd6b":"# Making prediction for the test dataset.\n\npredict=fit1.forecast(len(test))","0c66e176":"# saving these predictions in test file in a new column.\n\ntest['prediction']=predict","dc9a6b18":"test.head()","c66c4ecb":"# Calculating the hourly ratio of count \n\ntrain_original ['ratio']=train_original['Count']\/train_original['Count'].sum()\n# ratio = Count column individual value \/ sum of Count column values\n\n# Grouping the hourly ratio \ntemp=train_original.groupby(['Hour'])['ratio'].sum() \n\n# Groupby to csv format \npd.DataFrame(temp, columns=['Hour','ratio']).to_csv('GROUPby.csv') \n\ntemp2=pd.read_csv(\"GROUPby.csv\") \ntemp2=temp2.drop('Hour.1',1) \n\n# Merge Test_df and test_original on day, month and year \nmerge=pd.merge(test, test_original, on=('day','month', 'year'), how='left') \nmerge['Hour']=merge['Hour_y'] \nmerge=merge.drop(['year', 'month', 'Datetime','Hour_x','Hour_y'], axis=1) \n# Predicting by merging merge and temp2 \nprediction=pd.merge(merge, temp2, on='Hour', how='left') \n\n# Converting the ratio to the original scale \nprediction['Count']=prediction['prediction']*prediction['ratio']*24 \nprediction['ID']=prediction['ID_y']\n\n","f3e8320e":"temp.head()","0098e404":"temp2.head()","bc31194f":"# Dropping all other features from the submission file and keep ID and Count only.\n\nsubmission=prediction.drop(['ID_x', 'day', 'ID_y','prediction','Hour', 'ratio'],axis=1) ","5bc7eed0":"# Converting the final submission to csv format \npd.DataFrame(submission, columns=['ID','Count']).to_csv('Holt linear.csv')","129fa9cd":"submission.head(), submission.shape","77b77135":"test.head(), test.shape\n","b438492b":"y_hat_avg.Holt_linear.shape, test.prediction.shape","acb1d245":"prediction.head(), prediction.shape","e5179ab3":"\ny_hat_avg = valid.copy() \nfit1 = ExponentialSmoothing(np.asarray(Train['Count']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit() \ny_hat_avg['Holt_Winter'] = fit1.forecast(len(valid)) \nplt.figure(figsize=(16,8)) \nplt.plot( Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter') \nplt.legend(loc='best') \nplt.show()\n","e1f8b810":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.Holt_Winter)) \nprint(rms)","36e2ebdc":"\npredict=fit1.forecast(len(test))\n","6b1df8d2":"\ntest['prediction']=predict\n# Merge Test and test_original on day, month and year \nmerge=pd.merge(test, test_original, on=('day','month', 'year'), how='left') \nmerge['Hour']=merge['Hour_y'] \nmerge=merge.drop(['year', 'month', 'Datetime','Hour_x','Hour_y'], axis=1) \n\n# Predicting by merging merge and temp2 \nprediction=pd.merge(merge, temp2, on='Hour', how='left') \n\n# Converting the ratio to the original scale\nprediction['Count']=prediction['prediction']*prediction['ratio']*24\n","2fb8d2d6":"prediction['ID']=prediction['ID_y'] \nsubmission=prediction.drop(['day','Hour','ratio','prediction', 'ID_x', 'ID_y'],axis=1) \n\n# Converting the final submission to csv format \npd.DataFrame(submission, columns=['ID','Count']).to_csv('Holt winters.csv')","8200d9f8":"from statsmodels.tsa.stattools import adfuller \ndef test_stationarity(timeseries):\n        #Determing rolling statistics\n    rolmean = timeseries.rolling(window=24).mean()\n    rolstd = timeseries.rolling(window=24).std()\n        #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n        #Perform Dickey-Fuller test:\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n\nfrom matplotlib.pylab import rcParams \nrcParams['figure.figsize'] = 20,10\ntest_stationarity(train_original['Count'])","d979cda0":"Train_log = np.log(Train['Count']) \nvalid_log = np.log(valid['Count'])\nmoving_avg = Train_log.rolling(24).mean()\nplt.plot(Train_log) \nplt.plot(moving_avg, color = 'red') \nplt.show()\n","29e33c3c":"train_log_moving_avg_diff = Train_log - moving_avg","ac92184d":"train_log_moving_avg_diff.dropna(inplace = True) \ntest_stationarity(train_log_moving_avg_diff)","2780d91b":"train_log_diff = Train_log - Train_log.shift(1) \ntest_stationarity(train_log_diff.dropna())","d85ef28d":"from statsmodels.tsa.seasonal import seasonal_decompose \ndecomposition = seasonal_decompose(pd.DataFrame(Train_log).Count.values, freq = 24) \n\ntrend = decomposition.trend \nseasonal = decomposition.seasonal \nresidual = decomposition.resid \n\nplt.subplot(411) \nplt.plot(Train_log, label='Original') \nplt.legend(loc='best') \nplt.subplot(412) \nplt.plot(trend, label='Trend') \nplt.legend(loc='best') \nplt.subplot(413) \nplt.plot(seasonal,label='Seasonality') \nplt.legend(loc='best') \nplt.subplot(414) \nplt.plot(residual, label='Residuals') \nplt.legend(loc='best') \nplt.tight_layout() \nplt.show()\n","06c208d4":"train_log_decompose = pd.DataFrame(residual) \ntrain_log_decompose['date'] = Train_log.index \ntrain_log_decompose.set_index('date', inplace = True) \ntrain_log_decompose.dropna(inplace=True) \ntest_stationarity(train_log_decompose[0])","959476d3":"from statsmodels.tsa.stattools import acf, pacf \nlag_acf = acf(train_log_diff.dropna(), nlags=25) \nlag_pacf = pacf(train_log_diff.dropna(), nlags=25, method='ols')","611f2a2c":"plt.plot(lag_acf) \nplt.axhline(y=0,linestyle='--',color='gray') \nplt.axhline(y=-1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \nplt.title('Autocorrelation Function') \nplt.show() \nplt.plot(lag_pacf) \nplt.axhline(y=0,linestyle='--',color='gray') \nplt.axhline(y=-1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \nplt.axhline(y=1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \nplt.title('Partial Autocorrelation Function') \nplt.show()","c324581e":"from statsmodels.tsa.arima_model import ARIMA\nmodel = ARIMA(Train_log, order=(2, 1, 0))  # here the q value is zero since it is just the AR model \nresults_AR = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(), label='original') \nplt.plot(results_AR.fittedvalues, color='red', label='predictions') \nplt.legend(loc='best') \nplt.show()","5557cbc3":"AR_predict=results_AR.predict(start=\"2014-06-25\", end=\"2014-09-25\") \nAR_predict=AR_predict.cumsum().shift().fillna(0) \nAR_predict1=pd.Series(np.ones(valid.shape[0]) * np.log(valid['Count'])[0], index = valid.index) \nAR_predict1=AR_predict1.add(AR_predict,fill_value=0) \nAR_predict = np.exp(AR_predict1)\nplt.plot(valid['Count'], label = \"Valid\") \nplt.plot(AR_predict, color = 'red', label = \"Predict\") \nplt.legend(loc= 'best') \nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(AR_predict, valid['Count']))\/valid.shape[0])) \nplt.show()","2739b4ba":"model = ARIMA(Train_log, order=(0, 1, 2))  # here the p value is zero since it is just the MA model \nresults_MA = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(), label='original') \nplt.plot(results_MA.fittedvalues, color='red', label='prediction') \nplt.legend(loc='best') \nplt.show()\n","3d280828":"MA_predict=results_MA.predict(start=\"2014-06-25\", end=\"2014-09-25\") \nMA_predict=MA_predict.cumsum().shift().fillna(0) \nMA_predict1=pd.Series(np.ones(valid.shape[0]) * np.log(valid['Count'])[0], index = valid.index) \nMA_predict1=MA_predict1.add(MA_predict,fill_value=0) \nMA_predict = np.exp(MA_predict1)\nplt.plot(valid['Count'], label = \"Valid\") \nplt.plot(MA_predict, color = 'red', label = \"Predict\") \nplt.legend(loc= 'best') \nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(MA_predict, valid['Count']))\/valid.shape[0])) \nplt.show()\n","759c5284":"model = ARIMA(Train_log, order=(2, 1, 2))  \nresults_ARIMA = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(),  label='original') \nplt.plot(results_ARIMA.fittedvalues, color='red', label='predicted') \nplt.legend(loc='best') \nplt.show()","24b1a16f":"def check_prediction_diff(predict_diff, given_set):\n    predict_diff= predict_diff.cumsum().shift().fillna(0)\n    predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set['Count'])[0], index = given_set.index)\n    predict_log = predict_base.add(predict_diff,fill_value=0)\n    predict = np.exp(predict_log)\n\n    plt.plot(given_set['Count'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Count']))\/given_set.shape[0]))\n    plt.show()\n    \ndef check_prediction_log(predict_log, given_set):\n    predict = np.exp(predict_log)\n \n    plt.plot(given_set['Count'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Count']))\/given_set.shape[0]))\n    plt.show()","a6c1f136":"ARIMA_predict_diff=results_ARIMA.predict(start=\"2014-06-25\", end=\"2014-09-25\")\ncheck_prediction_diff(ARIMA_predict_diff, valid)","2ecf7420":"import statsmodels.api as sm\ny_hat_avg = valid.copy() \nfit1 = sm.tsa.statespace.SARIMAX(Train.Count, order=(2, 1, 4),seasonal_order=(0,1,1,7)).fit() \ny_hat_avg['SARIMA'] = fit1.predict(start=\"2014-6-25\", end=\"2014-9-25\", dynamic=True) \nplt.figure(figsize=(16,8)) \nplt.plot( Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['SARIMA'], label='SARIMA') \nplt.legend(loc='best') \nplt.show()","a2d6c675":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.SARIMA)) \nprint(rms)","b7d1cc77":"test['prediction']=predict\n# Merge Test and test_original on day, month and year \nmerge=pd.merge(test, test_original, on=('day','month', 'year'), how='left') \nmerge['Hour']=merge['Hour_y'] \nmerge=merge.drop(['year', 'month', 'Datetime','Hour_x','Hour_y'], axis=1) \n\n# Predicting by merging merge and temp2 \nprediction=pd.merge(merge, temp2, on='Hour', how='left') \n\n# Converting the ratio to the original scale \nprediction['Count']=prediction['prediction']*prediction['ratio']*24\n\n#Let\u2019s drop all variables other than ID and Count\n\nprediction['ID']=prediction['ID_y'] \nsubmission=prediction.drop(['day','Hour','ratio','prediction', 'ID_x', 'ID_y'],axis=1) \n\n# Converting the final submission to csv format \npd.DataFrame(submission, columns=['ID','Count']).to_csv('SARIMAX.csv')","3f9c5121":"submission.head()","f5a42803":"As we can see that the Test Statistic is very smaller as compared to the Critical Value. So, we can be confident that the trend is almost removed.<br>\n\nLet\u2019s now stabilize the mean of the time series which is also a requirement for a stationary time series.<br>\n\nDifferencing can help to make the series stable and eliminate the trend.<br>","aaf28932":"Now we will convert these daily passenger count into hourly passenger count using the same approach which we followed above.\n","8fc2f7d6":"# Bullet Train\n\nThis time we are helping out SOV Investors with your data hacking skills. They are considering making an investment in a new form of transportation - BulletTrain. BulletTrain uses Jet propulsion technology to run rails and move people at a high speed! While BulletTrain has mastered the technology and they hold the patent for their product, the investment would only make sense, if they can get more than 1 Million monthly users with in next 18 months.\n \nYou need to help SOV ventures with the decision. They usually invest in B2C start-ups less than 4 years old looking for pre-series A funding. In order to help SOV Ventures in their decision, you need to forecast the traffic on BulletTrain for the next 7 months. You are provided with traffic data of BulletTrain since inception in the test file.","e60d3b83":"The hypothesis is drawn for the traffic pattern on weekday and weekend. So, a weekend variable is generated to visualize the impact of weekend on traffic.<br>\n\u2022 First extract the day of week from Datetime and then based on the values we will assign whether the day is a weekend or not.<br>\n\u2022 Values of 5 and 6 represents that the days are weekend.","da9a15ea":"Let\u2019s define a function which can be used to change the scale of the model to the original scale.\n","6bfc5598":"# Hypothesis Generation \nThe first step to start, i.e. Hypothesis Generation. Hypothesis Generation is the process of listing out all the possible factors that can affect the outcome.<br>\nHypothesis generation is done before having a look at the data in order to avoid any bias that may result after the observation.<br>\n1) Hypothesis Generation <br>\nHypothesis generation helps us to point out the factors which might affect our dependent variable. Below are some of the hypotheses which I think can affect the passenger count(dependent variable for this time series problem) on the BulletTrain:<br>\n1.\tThere will be an increase in the traffic as the years pass by.<br>\n\u2022\tExplanation - Population has a general upward trend with time, so I can expect more people to travel by BulletTrain. Also, generally companies expand their businesses over time leading to more customers travelling through BulletTrain.<br>\n2.\tThe traffic will be high from May to October.<br>\n\u2022\tExplanation - Tourist visits generally increases during this time period.<br>\n3.\tTraffic on weekdays will be more as compared to weekends\/holidays.<br>\n\u2022\tExplanation - People will go to office on weekdays and hence the traffic will be more. <br>\n4.\tTraffic during the peak hours will be high.<br>\n\u2022\tExplanation - People will travel to work, college.<br>\nWe will try to validate each of these hypothesis based on the dataset. Now let\u2019s have a look at the dataset.\n","bbe15c5f":"\u2022\tID and Count are in integer format while the Datetime is in object format for the train file.<br>\n\u2022\tID is in integer and Datetime is in object format for test file.\n","49e2d664":"Here we see a decline in passenger count inlast three months which seems to be incorrect to our first hypothesis. So lets look at monthly mean of each year. ","d54c6989":"# Feature Extraction\n\nFirst extract the time and date from the Datetime. It is seen earlier that the data type of Datetime is object. So first of all, change the data type to datetime format otherwise we can not extract features from it.","63618537":"**It is visible that the predictions are getting weaker as the number of observations for rolling mean increase.**","4dbc5918":"Order in the above model represents the order of the autoregressive model(number of time lags), the degree of differencing(number of times the data have had past values subtracted) and the order of moving average model.\n\nSeasonal order represents the order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity.\n\nIn our case the periodicity is 7 since it is daily time series and will repeat after every 7 days.\n\nLet\u2019s check the rmse value for the validation part.","b450ce28":"Now only ID and corresponding Count needed for the final submission.","c19dc8d5":"# Holt\u2019s Linear Trend Model on daily time series - Test Dataset\n\n- Now with holt\u2019s linear trend model on the daily time series and making predictions on the test dataset.\n- We will make predictions based on the daily time series and then will distribute that daily prediction to hourly predictions.\n- We have fitted the holt\u2019s linear trend model on the train dataset and validated it using validation dataset.","810eecb3":"We can see that the rmse value has reduced a lot from this method. Let\u2019s forecast the Counts for the entire length of the Test dataset.\n","5a42107a":"So far we have made different models for trend and seasonality. Let's go for a model which will consider both the trend and seasonality of the time series?\n\nLet's consider the ARIMA model for time series forecasting.","76d2f85c":"Now we will forecast the time series for Test data which starts from 2014-9-26 and ends at 2015-4-26.","b936c9e0":"Here, we are predicting the traffic for the validation part and then visualize how accurate our predictions are. Finally we will make predictions for the test dataset.\n\nVarious models consider to forecast the time series. Methods which we will be discussing for the forecasting are:\ni) Naive Approach\nii) Moving Average\niii) Simple Exponential Smoothing\niv) Holt\u2019s Linear Trend Model\nNaive Approach\nIn this forecasting technique, we assume that the next expected point is equal to the last observed point. So we can expect a straight horizontal line as the prediction\n\n\n\n# Naive Approach \n\nIn this forecasting technique, we assume that the next expected point is equal to the last observed point. So we can expect a straight horizontal line as the prediction","d20f7d6e":"<b>To validate our hypothesis, extracting the year, month, day and hour from the Datetime. <br>\n Then made these hypothesis for the effect of hour, day, month and year on the passenger count. <b>","e347b60e":"# ACF and PACF plot\n","fb06344f":"Since we took the average of 24 values, rolling mean is not defined for the first 23 values. So let\u2019s drop those null values.","d7ab4cfc":"<b>An increasing trend can be seen in the dataset, so now we will make a model based on the trend. <\/b>","0e20d3b1":"Now we will decompose the time series into trend and seasonality and will get the residual which is the random variation in the series.\n\n# Removing Seasonality\nBy seasonality, we mean periodic fluctuations. A seasonal pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).<br>\nSeasonality is always of a fixed and known period.<br>\nWe will use seasonal decompose to decompose the time series into trend, seasonality and residuals.","a029dcd0":"Assigning  1 if the day of week is a weekend and 0 if the day of week in not a weekend.","eb33f94d":"# Recalling the hypothesis that we made earlier:\n\nTraffic will increase as the years pass by <br>\nTraffic will be high from May to October <br>\nTraffic on weekdays will be more <br>\nTraffic during the peak hours will be high <br>","a9ec0f9a":"The statistics shows that the time series is stationary as Test Statistic < Critical value but we can see an increasing trend in the data. So, firstly we will try to make the data more stationary. For doing so, we need to remove the trend and seasonality from the data.","ee9593c4":"# Simple Exponential Smoothing\n\nIn this technique, we assign larger weights to more recent observations than to observations from the distant past.<br>\nThe weights decrease exponentially as observations come from further in the past, the smallest weights are associated with the oldest observations.<br>\n\nNOTE - If we give the entire weight to the last observed value only, this method will be similar to the naive approach. So, we can say that naive approach is also a simple exponential smoothing technique where the entire weight is given to the last observed value.","240b87ce":"It is infer that this method is not suitable for datasets with high variability.But we can reduce the rmse value by adopting different techniques.<br>\n\n# Moving Average\n\nIn this technique we will take the average of the passenger counts for last few time periods only.\n","05fc45ce":"We can infer that the fit of the model has improved as the rmse value has reduced.\n\n\n# Holt\u2019s Linear Trend Model\n\n- It is an extension of simple exponential smoothing to allow forecasting of data with a trend.<br>\n- This method takes into account the trend of the dataset. The forecast function in this method is a function of level and trend.<br>\n\nFirst, lets visualize the trend, seasonality and error in the series and then decompose the time series in four parts.<br>\n\n- Observed, which is the original time series.<br>\n- Trend, which shows the trend in the time series, i.e., increasing or decreasing behaviour of the time series.<br>\n- Seasonal, which tells us about the seasonality in the time series.<br>\n- Residual, which is obtained by removing any trend or seasonality in the time series.<br>","81b3bce7":"As naive approach consider the next expected point is equal to the last observed point, which result in a straight horizontal line for the predicted value. This is what we can see in our above graph.<br>\n\nTo validate how accurate our predictions are by using rmse(Root Mean Square Error).<br>\nrmse is the standard deviation of the residuals.<br>\nResiduals are a measure of how far from the regression line data points are.<br>\nThe formula for rmse is <br>\nrmse=sqrt\u2211i=1N1N(p\u2212a)2","0536128c":"An increasing trend is observed. To make the time series stationary, this increasing trend need to be remove.\n","6a6883e1":"# Exploratory Data Analysis\n\nLet us try to verify our hypothesis using the actual data.<br>\n\nOur first hypothesis was traffic will increase as the years pass by. So let\u2019s look at yearly passenger count. <br>","0dd93d33":"# Holt winter\u2019s model on daily time series\n\nDatasets which show a similar set of pattern after fixed intervals of a time period suffer from seasonality.\n\nThe above models don\u2019t take into account the seasonality of the dataset while forecasting. Hence we need a method that takes into account both trend and seasonality to forecast future prices.\n\nOne such algorithm that we can use in such a scenario is Holt\u2019s Winter method. The idea behind Holt\u2019s Winter is to apply exponential smoothing to the seasonal components in addition to level and trend.\n\nLet\u2019s first fit the model on training dataset and validate it using the validation dataset.\n","ed6739bb":"It is visible that the month Oct, Nov and Dec having a very low mean value in year 2012 and the values for theses months are not present in year 2014.<br> \n\nSince there is an increasing trend in our time series, the mean value for rest of the months will be more because of their larger passenger counts in year 2014. Therefore, we will get smaller value for these 3 months.<br>\n\nIn the above bar plot we can see an increasing trend in monthly passenger count and the growth is approximately exponential.<br>\n\n","5a20dfac":"Lets plot the validation curve for AR model.\n\nWe have to change the scale of the model to the original scale.\n\nFirst step would be to store the predicted results as a separate series and observe it.\n\n","1fb7bc1a":"# Removing Trend\nA trend exists when there is a long-term increase or decrease in the data. It does not have to be linear.<br>\n\nWe see an increasing trend in the data so we can apply transformation which penalizes higher values more than smaller ones, for example log transformation.<br>\n\nWe will take rolling average here to remove the trend. We will take the window size of 24 based on the fact that each day has 24 hours.<br>","816cdc1e":"END","ef47c281":" # Basic modeling techniques. \n    \nDrop the ID variable as it has nothing to do with the passenger count.","fd52289b":"# SARIMAX model on daily time series\n\nSARIMAX model takes into account the seasonality of the time series. So we will build a SARIMAX model on the time series.","1e1cd23a":"# Introduction to ARIMA model\n\nARIMA stands for Auto Regression Integrated Moving Average. It is specified by three ordered parameters (p,d,q).<br>\n\nHere p is the order of the autoregressive model(number of time lags)<br>\nd is the degree of differencing(number of times the data have had past values subtracted)<br>\nq is the order of moving average model. We will discuss more about these parameters in next section.<br>\n\nThe ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation.<br>\n\n# What is a stationary time series?<br>\nThere are three basic criterion for a series to be classified as stationary series :<br>\n\nThe mean of the time series should not be a function of time. It should be constant.<br>\nThe variance of the time series should not be a function of time.<br>\nTHe covariance of the ith term and the (i+m)th term should not be a function of time.<br>\n\n# Why do we have to make the time series stationary?<br>\nWe make the series stationary to make the variables independent. Variables can be dependent in various ways, but can only be independent in one way. So, we will get more information when they are independent. Hence the time series must be stationary.<br>\n\nIf the time series is not stationary, firstly we have to make it stationary. For doing so, we need to remove the trend and seasonality from the data. <br>","1a93da7b":"Now let\u2019s combine these two models.\n\n\n# Combined model\n\n","fa15ffb7":"A lot of noise in the hourly time series is noticed. So, aggregate the hourly time series to daily, weekly, and monthly time series to reduce the noise and make it more stable and hence would be easier for a model to learn.","fabdb19c":"It can be interpreted from the results that the residuals are stationary.\n\nNow we will forecast the time series using different models.\n\nForecasting the time series using ARIMA\nFirst of all we will fit the ARIMA model on our time series for that we have to find the optimized values for the p,d,q parameters.\n\nTo find the optimized values of these parameters, we will use ACF(Autocorrelation Function) and PACF(Partial Autocorrelation Function) graph.\n\nACF is a measure of the correlation between the TimeSeries with a lagged version of itself.\n\nPACF measures the correlation between the TimeSeries with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons.","e054b4d7":"# Parameter tuning for ARIMA model\nFirst of all we have to make sure that the time series is stationary. If the series is not stationary, we will make it stationary.<br>\n\n# Stationarity Check\n\nWe use Dickey Fuller test to check the stationarity of the series.<br>\nThe intuition behind this test is that it determines how strongly a time series is defined by a trend.<br>\nThe null hypothesis of the test is that time series is not stationary (has some time-dependent structure).<br>\nThe alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.<br>\n\nThe test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the \u2018Test Statistic\u2019 is less than the \u2018Critical Value\u2019, we can reject the null hypothesis and say that the series is stationary.<br>\n\nWe interpret this result using the Test Statistics and critical value. If the Test Statistics is smaller than critical value, it suggests we reject the null hypothesis (stationary), otherwise a greater Test Statistics suggests we accept the null hypothesis (non-stationary).<br>\n\nLet\u2019s make a function which we can use to calculate the results of Dickey-Fuller test.","0529d142":"From daily passenger count, we are unable to gather much insight. So,its time to look for the mean of hourly passenger count, which will highlight the hypothesis that the traffic will be more during peak hours.","83595e47":"We can see the trend, residuals and the seasonality clearly in the above graph. Seasonality shows a constant trend in counter.\n\nLet\u2019s check stationarity of residuals.\n\n","eca37566":"Point to remember, this is a daily predictions.<br>\n\nWe have to convert these predictions to hourly basis. \n* To do so we will first calculate the ratio of passenger count for each hour of every day. \n* Then we will find the average ratio of passenger count for every hour and we will get 24 ratios. \n* Then to calculate the hourly predictions we will multiply the daily prediction with the hourly ratio.","f0d21f2e":"Exponential growth is noticed year by year which validate our first hypothesis\n\n**The second hypothesis was increase in traffic from May to October. So, let\u2019s see the relation between count and month.**","0f3f3679":"Let\u2019s drop all features other than ID and Count","89156a8c":"From the above graph, we can inferred that the traffic is more during the weekdays as compared to weekend which validates the hypothesis the traffic will be more on weekdays. <br>\nNow, for the Day of week passenger count, where 0 is monday and 6 is sunday.","740a9e99":"Here the red line shows the prediction for the validation set. Let\u2019s build the MA model now.\n\n# MA model\nThe moving-average model specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly predictable) term.\n","41a34e49":"It can be inferred that the peak traffic in the evening is at 7 PM. Then a decreasing trend is noticed till 5 AM.\nAfter that the passenger count starts increasing again and peaks again between 11AM and 12 Noon.\n\n**To validate our another hypothesis in which we assumed that the traffic will be more on weekdays.**","12f48a78":"From the graph, it is visible that the time series is becoming more and more stable when we are aggregating it on daily, weekly and monthly basis.<br>\n\nBut it would be difficult to convert the monthly and weekly predictions to hourly predictions, as first we have to convert the monthly predictions to weekly, weekly to daily and daily to hourly predictions, which will become very expanded process. So, we will work on the daily time series.","95cf1476":"Let\u2019s predict the values for validation set.\n\n","de4129e0":"The rmse value has decreased further with Holt linear Trend Model.\n\n **Now predicting the passenger count for the test dataset using various models.**","4c0d7f78":"p value is the lag value where the PACF chart crosses the upper confidence interval for the first time. It can be noticed that in this case p=1.\n\nq value is the lag value where the ACF chart crosses the upper confidence interval for the first time. It can be noticed that in this case q=1.\n\nNow we will make the ARIMA model as we have the p,q values. We will make the AR and MA model separately and then combine them together.\n\n# AR model\nThe autoregressive model specifies that the output variable depends linearly on its own previous values.\n\n\nA nonseasonal ARIMA model is classified as an \"ARIMA(p,d,q)\" model, where:\n\np is the number of autoregressive terms,<br>\nd is the number of nonseasonal differences needed for stationarity, and<br>\nq is the number of lagged forecast errors in the prediction equation.","afbc17b8":"The inclined line here seen as the model has taken into consideration the trend of the time series.","b8eab89e":"Note that these are the daily predictions and we need hourly predictions. So, we will distribute this daily prediction into hourly counts. To do so, we will take the ratio of hourly distribution of passenger count from train data and then we will distribute the predictions in the same ratio.\n\n"}}