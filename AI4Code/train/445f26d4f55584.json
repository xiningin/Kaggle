{"cell_type":{"a6e0c869":"code","f3b3ffb9":"code","b19516ec":"code","67cc9200":"code","a1b9a629":"code","014776a0":"code","d9cb3c35":"code","93c1d396":"code","605976f4":"code","9878950d":"code","d9960888":"code","8c2cde7c":"code","14de27a1":"code","6df22768":"code","263d44d9":"code","de2bdae0":"code","add457c7":"code","c196d967":"code","9ac83bd3":"code","f1479873":"code","d8974801":"code","9fd27cf4":"code","b8d855bf":"code","f552c31d":"code","03d99fdf":"code","45f25c83":"code","97bb40b4":"code","1d1ae0cf":"code","139cfbaa":"code","f2e7ae62":"code","ee44b36d":"code","099d97a0":"code","7bfee36b":"code","05d59aa5":"code","dc50048d":"code","b1937493":"code","0008a94b":"code","eef57760":"code","c5d89456":"markdown","4a0544bd":"markdown","21cd28ed":"markdown","1d1972b2":"markdown","6106cdc0":"markdown"},"source":{"a6e0c869":"\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport re,string,unicodedata\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nimport gc\nfrom nltk.tokenize import word_tokenize\nfrom collections import  Counter\nimport pyLDAvis\nimport gensim\nfrom spacy.tokens import Span \nimport pyLDAvis.gensim\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f3b3ffb9":"!pip install rake_nltk\nfrom rake_nltk import Rake","b19516ec":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","67cc9200":"df=pd.read_csv('\/kaggle\/input\/pfizer-vaccine-tweets\/vaccination_tweets.csv')","a1b9a629":"df.head(3)","014776a0":"#Check missing value\nmissing = pd.DataFrame(df.isnull().sum().sort_values(ascending=False))\nmissing.columns=['Null Count']\nmissing.index.name = 'Feature'\nmissing","d9cb3c35":"print(missing)\nmissing.plot(kind='bar')","93c1d396":"filter_columns=['user_description','hashtags','source', 'user_verified','text']\ndf_text=df[filter_columns]","605976f4":"df_text.head(3)","9878950d":"print(df_text.source.value_counts())\ndf_text.source.value_counts().plot(kind='bar')\nplt.title('Text Source in Twitter')","d9960888":"sns.countplot(x='user_verified', data=df_text)","8c2cde7c":"df_text.head(3)","14de27a1":"print(df_text.user_description.value_counts().head(15))\ndf_text.user_description.value_counts().head(15).plot(kind='bar')","6df22768":"print(df_text.text.value_counts().head(15))\ndf_text.text.value_counts().head(10).plot(kind='bar')","263d44d9":"def cleaning_func(text):\n    text=re.sub(r'http?:\/\/www\\.\\S+\\.com', '', text)\n    text=re.sub(r'[^A-Za-z|\\s]','',text)\n    return text\n\ndef clean(df):\n    for i in ['user_description','hashtags' 'text']:\n        df[col]=df[col].astype(str).apply(lambda x:basic_cleaning(x))\n    return df","de2bdae0":"stop=set(stopwords.words('english'))\n\ndef segment(df,col=\"text\"):\n    corpus=[]\n    lem=WordNetLemmatizer()\n    stop=set(stopwords.words('english'))\n    new= df[col].dropna().str.split()\n    new=new.values.tolist()\n    corpus=[lem.lemmatize(word.lower()) for i in new for word in i if(word) not in stop]\n    \n    return corpus","add457c7":"corpus=segment(df_text)\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:30]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","c196d967":"plt.figure(figsize=(9,7))\nsns.barplot(x=y,y=x)\nplt.title(\"most common word in text\")","9ac83bd3":"corpus=segment(df_text,\"user_description\")\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:30]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","f1479873":"plt.figure(figsize=(9,7))\nsns.barplot(x=y,y=x)\nplt.title(\"most common word in user_description\")","d8974801":"text=df.text.values\ndef text_pro(df):\n    corpus=[]\n    stem = PorterStemmer()\n    lem=WordNetLemmatizer()\n\n    for news in text:\n        words=[w for w in word_tokenize(news) if (w not in stop)]\n        \n        words=[lem.lemmatize(w) for w in words if len(w)>2]\n        \n        corpus.append(words)\n    return corpus","9fd27cf4":"corpus=text_pro(text)\ndic=gensim.corpora.Dictionary(corpus)\nbow_corpus = [dic.doc2bow(doc) for doc in corpus]","b8d855bf":"lda_model =  gensim.models.LdaMulticore(bow_corpus, \n                                   num_topics = 4, \n                                   id2word = dic,                                    \n                                   passes = 10,\n                                   workers = 2)","f552c31d":"lda_model.show_topics()","03d99fdf":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)\nvis","45f25c83":"!pip install pycaret","97bb40b4":"from pycaret.nlp import *","1d1ae0cf":"nlp_col=['user_verified', 'text']\ndf_nlp=df[nlp_col]","139cfbaa":"nlp = setup(data = df_nlp, target = 'text', session_id = 1)","f2e7ae62":"lda = create_model('lda', multi_core = True)","ee44b36d":"lda_data = assign_model(lda)\nlda_data.head()","099d97a0":"lda = create_model('lda', multi_core = True)","7bfee36b":"lda_data = assign_model(lda)\nlda_data.head()","05d59aa5":"evaluate_model(lda)","dc50048d":"from pycaret.classification import *  ","b1937493":"lda_data.head(3)","0008a94b":"model = setup(data = lda_data, target = 'user_verified',ignore_features=['text','Dominant_Topic','Perc_Dominant_Topic'])","eef57760":"compare_models()","c5d89456":"# under one can see Word Count and Frequency Plot, Bigrams , Sentiment Plarity ","4a0544bd":"# **LDA Modeling**:a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.\nresouce: https:\/\/en.wikipedia.org\/wiki\/Latent_Dirichlet_allocation","21cd28ed":"# PyCaret NLP Classification ","1d1972b2":"# IF YOU LIKE IT , PLZ UPVOTE !!","6106cdc0":"# PyCaret NLP"}}