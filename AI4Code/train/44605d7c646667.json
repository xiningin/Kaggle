{"cell_type":{"495c693d":"code","f0ff2c6a":"code","7cfefc81":"code","fe39870a":"code","a8ab668a":"code","e1c735df":"code","4ff6de17":"code","53c5e08a":"code","fd28f4a6":"code","16294bff":"code","d0275ebf":"code","ccdc038b":"code","9750f9be":"code","63f596d9":"code","192d9b69":"code","38f32d97":"code","386c8e05":"code","620fe268":"code","5ffd60ba":"code","8db5d19a":"markdown"},"source":{"495c693d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0ff2c6a":"# verifica\u00e7\u00e3o da estrutura de dados\npd.read_csv('\/kaggle\/input\/congressional-voting-records\/house-votes-84.csv').head(10)","7cfefc81":"# A op\u00e7\u00e3o na_valeus define as respostas registradas como '?' como valores nulos\ndf = pd.read_csv('\/kaggle\/input\/congressional-voting-records\/house-votes-84.csv',na_values=['?'])\n# usando a fun\u00e7\u00e3o raplace do pandas para alterar as resposas para binarios\ndf.replace(('y', 'n'), (1, 0), inplace=True)\ndf.head()","fe39870a":"# avalia\u00e7\u00e3o da estutura dos dados e os dados faltantes \ndf.info()\ndf.isna().sum()","a8ab668a":"# analise sobre as respostas \n# \u00e9 interessandte avaliar que as opini\u00f5es s\u00e3o divergentes o que separa bem os as classes \nindice = df.drop('Class Name', axis=1).columns \nz = 0\nfig, axes = plt.subplots(4, 4, figsize=(30, 15))# define uma tabela de 10 X 10 s\u00e3o 10 elementos para cada tipo de elemento \nfor i in range(4): # renge de 0 at\u00e9 4\n    #indice = np.where(y_train == i)[0] # seleciona as imagens de cada tipo para ser plotada no grafico.\n    for j in range(4): # plotar 4 elementos de cada ponto        \n        sns.countplot(x = indice[z+j], hue='Class Name', data=df, palette='RdBu', ax=axes[i][j])\n        # retirar as escalas e cordenadas da imagem \n        axes[i][j].set_yticks([])\n    z = z + j + 1\nplt.show()","e1c735df":"# usando a fun\u00e7\u00e3o raplace do pandas para alterar o label para 0 e 1\ndf.replace(('democrat', 'republican'), (0, 1), inplace=True)","4ff6de17":"# dividindo as feature de analise e resposta\ny = df['Class Name'].values\nX = df.drop('Class Name', axis=1).values","53c5e08a":"# para trabalhar como os dados faltantes\n# fun\u00e7\u00e3o aplica os valores mais frequentes de cada coluna nos valores faltantes\nimp = SimpleImputer(strategy=\"most_frequent\")\nX = imp.fit_transform(X)","fd28f4a6":"# avalia\u00e7\u00e3o base de alguns modelos de classifica\u00e7\u00e3o\ndef evaluate_model(X, y, model):\n    K = 5\n    R = 3\n    # valida\u00e7\u00e3o cruzada\n    cv = RepeatedStratifiedKFold(n_splits=K, n_repeats=R, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# sele\u00e7\u00e3o de modelos \ndef get_models():\n    models, names = list(), list()\n    # SVM\n    models.append(LogisticRegression())\n    names.append('LG')\n    \n    models.append(KNeighborsClassifier())\n    names.append('KNN')\n    \n    models.append(RandomForestClassifier(n_estimators=1000))\n    names.append('RF')\n    \n    models.append(ExtraTreesClassifier(n_estimators=1000))\n    names.append('ET')\n    \n    return models, names\n\n# avaia\u00e7\u00e3o do modelo AUC\ndef evaluate(model, test_features, test_labels):\n    probs_votos = model.predict_proba(test_features)\n    accuracy = roc_auc_score(test_labels, probs_votos[:,1])\n    return accuracy","16294bff":"#Avalia\u00e7\u00e3o dos modelos \nmodels, names = get_models()\n\nresults = list()\n\nfor i in range(len(models)):\n    scores = evaluate_model(X, y, models[i])\n    results.append(scores)\n    print('>%s %.3f (%.3f)' % (names[i], np.mean(scores), np.std(scores)))\n\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","d0275ebf":"# ap\u00f3s avalia\u00e7\u00e3o inicial vamos realizar um ajuste dos hiperpar\u00e2metros dos dois melhores modelos atr\u00e1ves do GridSearch\n# divis\u00e3o dos dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)","ccdc038b":"#grid search do random forest\n# defini\u00e7\u00e3o de cada paramentro\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\n# Cria\u00e7\u00e3o do grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nprint(random_grid)","9750f9be":"rf = RandomForestClassifier(random_state = 42)\n# avalia\u00e7\u00e3o cruzada utilizando 3 folds, \nrf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n                              n_iter = 100, scoring='neg_mean_absolute_error', \n                              cv = 3, verbose=2, random_state=42, n_jobs=-1,\n                              return_train_score=True)\n\n# Fit\nrf_random.fit(X_train, y_train)\nprint('Melhores parametros:',rf_random.best_params_)\n# sele\u00e7\u00e3o do melhor modelo e avalia\u00e7\u00e3o da acuracia do modelo \nbest_random = rf_random.best_estimator_\n#fazendo a predi\u00e7\u00e3o nos arquivos de teste\ny_pred = best_random.predict(X_test)\n# avalia\u00e7\u00e3o da acuracia do modelo\nacc_vote = accuracy_score(y_test, y_pred)\nprint(\"acuracia: \",acc_vote)","63f596d9":"#metirca de avalia\u00e7\u00e3o da curva roc\nprint(\"AUC:\" ,evaluate(best_random, X_test, y_test))","192d9b69":"# definindo hiperpar\u00e2metros do grid\ngrid = {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nprint(grid)","38f32d97":"# Regre\u00e7\u00e3o logistica para classifica\u00e7\u00e3o \n# definindo hiperpar\u00e2metros do grid\ngrid = {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(X_train,y_train)","386c8e05":"print(\"melhores parametros :\",logreg_cv.best_params_)\nprint(\"acur\u00e1cia com dados de treino:\",logreg_cv.best_score_)","620fe268":"# selecionando o melhor modelo, fazendo a predi\u00e7\u00e3o e \nbest_lot = logreg_cv.best_estimator_\ny_pred_log = best_lot.predict(X_test)\nacc_log_vote = accuracy_score(y_test, y_pred_log)\nprint(\"accuracy_score:\",acc_log_vote)","5ffd60ba":"#metirca de avalia\u00e7\u00e3o da curva roc\nprint(\"AUC:\" ,evaluate(best_lot, X_test, y_test))","8db5d19a":"Os dois modelos performaram muito bem, pois a divergencia de ideias entre os grupos divide muito bem os dados faciliando a cria\u00e7\u00e3o de um modelo com boa acur\u00e1cia.\n"}}