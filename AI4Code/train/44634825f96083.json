{"cell_type":{"de5cc375":"code","9ff44ec9":"markdown"},"source":{"de5cc375":"import numpy as np \nimport pandas as pd\n\n# keep only docsuments with covid -cov-2 and cov2\ndef search_focus(df):\n    dfa = df[df['abstract'].str.contains('covid')]\n    dfb = df[df['abstract'].str.contains('-cov-2')]\n    dfc = df[df['abstract'].str.contains('cov2')]\n    frames=[dfa,dfb,dfc]\n    df = pd.concat(frames)\n    df=df.drop_duplicates(subset='title', keep=\"first\")\n    return df\n\n# load the meta data from the CSV file using 3 columns (abstract, title, authors),\ndf=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv', usecols=['title','abstract','authors','doi','publish_time'])\nprint (df.shape)\n#drop duplicate abstracts\ndf = df.drop_duplicates(subset='title', keep=\"first\")\n#drop NANs \ndf=df.dropna()\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()\n\n# search focus keeps abstracts with the anchor words covid,-cov-2,hcov2\ndf=search_focus(df)\n\n#show 5 lines of the new dataframe\n#print (df.shape)\n#df.head()\nprint ('data loaded')\n\n###################### LOAD PACKAGES ##########################\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import PorterStemmer\nimport functools\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom IPython.core.display import display, HTML\n\nstop=set(stopwords.words('english'))\nstop.update(('sars-cov-2','coronavirus','covid-19','wuhan','2019-ncov','2020','2021','covid19','2019','sarscov2', 'disease covid19','acute', 'respiratory', 'syndrome','province', 'china','world','organization','emergency','novel','homeless'))\n\n\n##################### FUNCTIONS ##############################\n\ndef remove_stopwords(query,stopwords):\n    qstr=''\n    qstr=qstr.join(query)\n    #remove punctuaiton\n    qstr = \"\".join(c for c in qstr if c not in ('!','.',',','?','(',')'))\n    text_tokens = word_tokenize(qstr)\n    #remove stopwords\n    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n    #stem words\n    tokens_without_sw=stem_words(tokens_without_sw)\n    return tokens_without_sw\n\n# function to stem keyword list into a common base word\ndef stem_words(words):\n    stemmer = PorterStemmer()\n    singles=[]\n    for w in words:\n        singles.append(stemmer.stem(w))\n    return singles\n\n\n# function search df abstracts for relevant ones\ndef search_relevance(rel_df,search_words):\n    rel_df['score']=\"\"\n    search_words=stem_words(search_words)\n    for index, row in rel_df.iterrows():\n        abstract = row['abstract']\n        result = abstract.split()\n        len_abstract=len(result)\n        score=0\n        missing=0\n        for word in search_words:\n            score=score+result.count(word)\n            if word not in result:\n                    missing=missing+1\n        missing_factor=1-(missing\/len(search_words))\n        final_score=(score\/len_abstract)* missing_factor \n        rel_df.loc[index, 'score'] = final_score*1000000\n    rel_df=rel_df.sort_values(by=['score'], ascending=False)\n    return rel_df\n\ndef clean_results(raw_topics,stop):\n    #remove stop words abstracts of relevant passages\n    raw_topics['nostop'] = raw_topics['abstract'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    raw_topics[\"nostop\"] = raw_topics['nostop'].str.replace('[^\\w\\s]','')\n    #rel_topics=Counter(\" \".join(df1[\"nostop\"]).split()).most_common(100)\n    return raw_topics\n\ndef vectorize_results(df1):\n    word_vectorizer = TfidfVectorizer(ngram_range=(3,4), analyzer='word')\n    #word_vectorizer = CountVectorizer(ngram_range=(3,4), analyzer='word')\n    sparse_matrix = word_vectorizer.fit_transform(df1['nostop'])\n    frequencies = sum(sparse_matrix).toarray()[0]\n    ngrams=word_vectorizer.get_feature_names()\n    results=pd.DataFrame(columns=['ngram','frequency'])\n    results['ngram']=ngrams\n    results['frequency']=frequencies\n    dfa = results[results['frequency'] > .5]\n    dfa=dfa.sort_values(by=['frequency'], ascending=False)\n    dfa=dfa.head(0)\n    return dfa\n\ndef prep_final_query(df_vectorized,query):\n    addl_terms = df_vectorized[\"ngram\"].tolist()\n    # add the phrases to the query and drop duplicates\n    for phrase in addl_terms:\n        words=phrase.split()\n        for word in words:\n            query.append(word)\n    query=stem_words(query)\n    # drop duplicates\n    final_query=[]\n    final_query=[final_query.append(x) for x in query if x not in final_query] \n    return query\n    \n# function parse sentences for query NEED TO modularize\ndef get_sentences(df1,search_words):\n    df_table = pd.DataFrame(columns = [\"pub_date\",\"authors\",\"title\",\"excerpt\",\"sent_score\"])\n    for index, row in df1.iterrows():\n        pub_sentence=''\n        sentences_used=0\n        hi_score=0\n        best_sentence=''\n        #break apart the absract to sentence level\n        sentences = row['abstract'].split('. ')\n        #loop through the sentences of the abstract\n        highligts=[]\n        for sentence in sentences:\n            # missing lets the system know if all the words are in the sentence\n            missing=0\n            score=0\n            missing_factor=0\n            final_score=0\n            #loop through the words of sentence\n            for word in search_words:\n                #if keyword missing change missing variable\n                score=score+sentence.count(word)\n                if word not in sentence:\n                    missing=missing+1\n            missing_factor=1-(missing\/len(search_words))\n            final_score=(score\/len(sentence))* missing_factor\n            # after all sentences processed show the sentences not missing keywords\n            if len(sentence)>100 and len(sentence)<1000 and sentence!='':\n                sentence=sentence.capitalize()\n                if sentence[len(sentence)-1]!='.':\n                    sentence=sentence+'.'\n                \n                if final_score>=hi_score:\n                    hi_score=final_score\n                    best_sentence=sentence\n                    pub_sentence=sentence\n        if pub_sentence!='':\n            sentence=pub_sentence\n            sentences_used=sentences_used+1\n            authors=row[\"authors\"].split(\" \")\n            link=row['doi']\n            title=row[\"title\"]\n            linka='https:\/\/doi.org\/'+link\n            linkb=title\n            sentence='<p fontsize=tiny\" align=\"left\">'+sentence+'<\/p>'\n            final_link='<p align=\"left\"><a href=\"{}\">{}<\/a><\/p>'.format(linka,linkb)\n            to_append = [row['publish_time'],authors[0]+' et al.',final_link,sentence,hi_score]\n            df_length = len(df_table)\n            df_table.loc[df_length] = to_append\n        #print (hi_score)\n        #print (best_sentence)\n    return df_table\n    \n\n###################### MAIN PROGRAM ###########################\n\ndisplay(HTML('<h1>Question Answering System<\/h1>'))\n\n# Loop Natural Language Questions \n#for question in questions:\nwhile question != 'exit':\n    display(HTML('<h3>Type your question below and hit enter<\/h3>'))\n    display(HTML('<h4>enter exit to end program<\/h4>'))\n    display(HTML('<b>For Best Answers Use Targeted Questions: e.g.<br>What is the incubation period range?<br>What is the communicable period? <br> Time duration from first positive test to clearance? <br>What type of comorbidities did patients have? <br>What is the basic reproduction number? <br> Is the virus persistent on surfaces? <br> What is the incubation period across age groups?<br>Does it spread through surface contamination?<br>What were the most contaminated surfaces and objects? <br> Are temperature and humidity a factor? etc.<\/b>'))\n    \n    question = input()\n    \n    #str1=''\n    # a make a string of the search words to print readable version above table\n    #str1=' '.join(question)\n    \n    # remove punctuation, stop words and stem words from NL question\n    query=remove_stopwords(question,stopwords)\n\n    # search dataframe abstracts for most relevant results\n    df1=search_relevance(df,query)\n    \n    # clean the result abstracts\n    df_clean=clean_results(df1,stop)\n    \n    # vectorize the abstracts and return top ngrams from corpus\n    #df_vectorized=vectorize_results(df_clean)\n    \n    # create final query\n    #final_query=prep_final_query(df_vectorized,query)\n    \n    # get best sentences\n    df_table=get_sentences(df1,query)\n    \n    # sort df by sentence rank scores\n    df_table=df_table.sort_values(by=['sent_score'], ascending=False)\n    df_table=df_table.drop(['sent_score'], axis=1)\n    \n    #limit number of results\n    df_table=df_table.head(10)\n    \n    #convert df to html\n    df_table=HTML(df_table.to_html(escape=False,index=False))\n    \n    # display search topic\n    #display(HTML('<h3>'+question+'<\/h3>'))\n    \n    # show the HTML table with responses\n    display(df_table)\n   \nprint ('done')\n    ","9ff44ec9":"# Here is a link to a web based version of a question answering tool.\n\nhttp:\/\/edocdiscovery.com\/covid_19\/index.php \n\nYou will have to fork this notebook and run in edit mode to see it work as Kaggle front end will not allow input - see error below."}}