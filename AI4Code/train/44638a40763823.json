{"cell_type":{"59d67b60":"code","39941809":"code","c179f5bf":"code","8a40fa92":"code","2347a123":"code","1dde072f":"code","6f6cb28b":"code","e5734580":"code","0a01d019":"code","881dfcce":"markdown","03123bcf":"markdown","2cdb4422":"markdown","d21e2a1c":"markdown","b1b15e4f":"markdown","0df626ea":"markdown","e792f7e7":"markdown","3fd61e66":"markdown","2299a96b":"markdown"},"source":{"59d67b60":"from keras.layers import Conv3D, MaxPool3D, Flatten, Dense\nfrom keras.layers import Dropout, Input, BatchNormalization\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom plotly.offline import iplot, init_notebook_mode\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adadelta\nimport plotly.graph_objs as go\nfrom matplotlib.pyplot import cm\nfrom keras.models import Model\nimport numpy as np\nimport keras\nimport h5py\n\ninit_notebook_mode(connected=True)\n%matplotlib inline","39941809":"with h5py.File('..\/input\/full_dataset_vectors.h5', 'r') as dataset:\n    x_train = dataset[\"X_train\"][:]\n    x_test = dataset[\"X_test\"][:]\n    y_train = dataset[\"y_train\"][:]\n    y_test = dataset[\"y_test\"][:]","c179f5bf":"print (\"x_train shape: \", x_train.shape)\nprint (\"y_train shape: \", y_train.shape)\n\nprint (\"x_test shape:  \", x_test.shape)\nprint (\"y_test shape:  \", y_test.shape)","8a40fa92":"with h5py.File(\"..\/input\/train_point_clouds.h5\", \"r\") as points_dataset:        \n    digits = []\n    for i in range(10):\n        digit = (points_dataset[str(i)][\"img\"][:], \n                 points_dataset[str(i)][\"points\"][:], \n                 points_dataset[str(i)].attrs[\"label\"]) \n        digits.append(digit)\n        \nx_c = [r[0] for r in digits[0][1]]\ny_c = [r[1] for r in digits[0][1]]\nz_c = [r[2] for r in digits[0][1]]\ntrace1 = go.Scatter3d(x=x_c, y=y_c, z=z_c, mode='markers', \n                      marker=dict(size=12, color=z_c, colorscale='Viridis', opacity=0.7))\n\ndata = [trace1]\nlayout = go.Layout(height=500, width=600, title= \"Digit: \"+str(digits[0][2]) + \" in 3D space\")\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","2347a123":"## Introduce the channel dimention in the input dataset \nxtrain = np.ndarray((x_train.shape[0], 4096, 3))\nxtest = np.ndarray((x_test.shape[0], 4096, 3))\n\n## iterate in train and test, add the rgb dimention \ndef add_rgb_dimention(array):\n    scaler_map = cm.ScalarMappable(cmap=\"Oranges\")\n    array = scaler_map.to_rgba(array)[:, : -1]\n    return array\nfor i in range(x_train.shape[0]):\n    xtrain[i] = add_rgb_dimention(x_train[i])\nfor i in range(x_test.shape[0]):\n    xtest[i] = add_rgb_dimention(x_test[i])\n\n## convert to 1 + 4D space (1st argument represents number of rows in the dataset)\nxtrain = xtrain.reshape(x_train.shape[0], 16, 16, 16, 3)\nxtest = xtest.reshape(x_test.shape[0], 16, 16, 16, 3)\n\n## convert target variable into one-hot\ny_train = keras.utils.to_categorical(y_train, 10)\ny_test = keras.utils.to_categorical(y_test, 10)","1dde072f":"y_train.shape","6f6cb28b":"## input layer\ninput_layer = Input((16, 16, 16, 3))\n\n## convolutional layers\nconv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu')(input_layer)\nconv_layer2 = Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu')(conv_layer1)\n\n## add max pooling to obtain the most imformatic features\npooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n\nconv_layer3 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(pooling_layer1)\nconv_layer4 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(conv_layer3)\npooling_layer2 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer4)\n\n## perform batch normalization on the convolution outputs before feeding it to MLP architecture\npooling_layer2 = BatchNormalization()(pooling_layer2)\nflatten_layer = Flatten()(pooling_layer2)\n\n## create an MLP architecture with dense layers : 4096 -> 512 -> 10\n## add dropouts to avoid overfitting \/ perform regularization\ndense_layer1 = Dense(units=2048, activation='relu')(flatten_layer)\ndense_layer1 = Dropout(0.4)(dense_layer1)\ndense_layer2 = Dense(units=512, activation='relu')(dense_layer1)\ndense_layer2 = Dropout(0.4)(dense_layer2)\noutput_layer = Dense(units=10, activation='softmax')(dense_layer2)\n\n## define the model with input layer and output layer\nmodel = Model(inputs=input_layer, outputs=output_layer)","e5734580":"model.compile(loss=categorical_crossentropy, optimizer=Adadelta(lr=0.1), metrics=['acc'])\nmodel.fit(x=xtrain, y=y_train, batch_size=128, epochs=50, validation_split=0.2)","0a01d019":"pred = model.predict(xtest)\npred = np.argmax(pred, axis=1)\npred","881dfcce":"In the model training, we can observe that the accuracy on validation set is fluctuating which suggests that the network can be improved further. Let's predict and measure the accuracy of current model","03123bcf":"Lets look at the dataset dimentions","2cdb4422":"This dataset is a flat one dimentional data, but the author of this dataset has also shared the original x,y,z in a separate data file. Lets plot a digits in 3D space. Rotate this 3D digit to view it properly. ","d21e2a1c":"## 3D Convolutions : Understanding + Use Case - Drug Discovery\n\n<br>\nIn one of my previous [kernel](https:\/\/www.kaggle.com\/shivamb\/a-very-comprehensive-tutorial-nn-cnn), I have shared the working of convolutional neural networks for images. In this kernel, I have explained 3D convolutions and their implementation on 3D MNIST dataset. Later in this kernel, I have shown how to use 3D convolution layers on one of the breakthrough and important area of Healthcare : Drug Discovery\n\n### What are Convolutions? \n\n- Mathematically, A convolution is an integration function that expresses the amount of overlap of one function g as it is shifted over another function f.   \n- Intutively, A convolution acts as a blender that mixes one function with another to give reduced data space while preserving the information.    \n\nIn terms of Neural Networks and Deep Learning:\n- Convolutions are filter (matrix \/ vectors) with learnable parameters that are used to extract low-dimentional features from an input data.     \n- They have the property to preserve the spatial or positional relationships between input data points  \n- Convolutional neural networks exploits the spatially-local correlation by enforcing a local connectivity pattern between neurons of adjacent layers.    \n- Intutively, convolution is the step of applying the concept of sliding window (a filter with learnable weights) over the input and producing a weighted sum (of weights and input) as the output. The weighted sum is the feature space which is used as the input for the next layers.  \n\nFor example, in Face Recognization problem, first few convolution layers learns the pressence of key points in the input image, next convolution layers learns the edges and shapes, and final convolution layers learns the face.  In this example, the input space is first reduced to lower dimentional space (representing information about points \/ pixels), then this space is reduced to another space containing (edges \/ shapes) and finally it is reduced to classify faces in the images. Convolutions can be applied in N dimentions.  \n\n### Types of Convolutions : \n\nLet's discuss what are different types of convolutions \n\n### 1D Convolutions\n\nMost simplistic convolutions are 1D convolutionals are generally used on sequence datasets (but can be used for other use-cases as well). They can be used for extracting local 1D subsequences from the input sequences and identify local patterns within the window of convolution. The following image shows how a 1 D convolution filter is applied to a sequence to obtain new features. Other common uses of 1D convolutions are seen in the area of NLP where every sentence is represented as a sequence of words.  \n\n<br><br>\n\n![](https:\/\/i.imgur.com\/5UQz1zI.jpg)\n\n\n### 2D Convolutions \n\nOn image datasets, mostly 2D Convolutional filters are used in CNN architectures. The main idea of 2D convolutions is that the the convolutional filter moves in 2-directions (x,y) to calculate low dimentional features from the image data. The output shape is also a 2 dimentional matrix. \n\n![](https:\/\/www.andrewszot.com\/static\/img\/ml\/cnn\/pooling.png)\n\n### 3D Convolutions\n\n3D convolutions applies a 3 dimentional filter to the dataset and the filter moves 3-direction (x, y, z) to calcuate the low level feature representations. Their output shape is a 3 dimentional volume space such as cube or cuboid. They are helpful in event detection in videos, 3D medical images etc. They are not limited to 3d space but can also be applied to 2d space inputs such as images.  \n\n![](https:\/\/i.imgur.com\/jriyCTU.png?1)\n\nLets implement the 3D CNN on 3D Mnist dataset. First, lets import the key libraries. \n\nAdditionally, there are other types of convolutions as well: \n\n### Dilated Convolutions  \n\nDilated or Altrous convolutions defines the spacing between the values in a kernel. In this type of convolution, the receptive view of the kernels increases due to the spacing, For example a 3x3 kernel with a dilation rate of 2 will have the same field of view as a 5x5 kernel. The complexity remains the same but different features are generated in this case. \n\n![](https:\/\/qph.fs.quoracdn.net\/main-qimg-d9025e88d7d792e26f4040b767b25819)\n\nLet's now create a 3D convolutional neural network architecture on 3D mnist dataset. ","b1b15e4f":"Now, lets implement a 3D convolutional Neural network on this dataset.  To use 2D convolutions, we first convert every image into a 3D shape : width, height, channels. Channels represents the slices of Red, Green, and Blue layers. So it is set as 3. In the similar manner, we will convert the input dataset into 4D shape in order to use 3D convolution for : length, breadth, height, channel (r\/g\/b). ","0df626ea":"Lets create the model architecture. The architecture is described below: \n\nInput and Output layers: \n- One Input layer with dimentions 16, 16, 16, 3  \n- Output layer with dimentions 10  \n\nConvolutions : \n- Apply 4 Convolutional layer with increasing order of filter size (standard size : 8, 16, 32, 64) and fixed kernel size = (3, 3, 3)\n- Apply 2 Max Pooling layers, one after 2nd convolutional layer and one after fourth convolutional layer.  \n\nMLP architecture: \n- Batch normalization on convolutiona architecture  \n- Dense layers with 2 layers followed by dropout to avoid overfitting  \n","e792f7e7":"Compile the model and start training. ","3fd61e66":"The 3D MNIST data is given in the .h5 format, lets load the complete dataset into training and test sets. ","2299a96b":"The model is not accurate at this point of time, But it can be improved further with architectural changes and hyperparameter tuning. \n\n## Use Case : Drug Discovery using 3D CNN\n\n> In Progress\n\n"}}