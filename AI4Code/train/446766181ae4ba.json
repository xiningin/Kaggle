{"cell_type":{"8ba64693":"code","50ce92b3":"code","8f50812b":"code","4d64aa46":"code","292a19e5":"code","bf806c35":"code","0943f3f0":"code","d74494c3":"code","8af1e8d5":"code","b95f1018":"code","0a4ef614":"code","844aaf76":"code","1e3f414a":"code","6e23de96":"code","9f99a19c":"code","09fed7c4":"code","c1617b00":"code","bb8b3dce":"code","73d3ce23":"code","10a379b8":"code","a43c5026":"code","b280eda5":"code","277113b9":"code","bd485d27":"code","fdedaaed":"code","b10e68a9":"code","34716e81":"code","d824c757":"code","dd50886e":"code","665148e1":"code","e2c10e38":"code","c808624b":"code","ba89a349":"code","7d1ab1c2":"code","6d8d3f64":"code","308d5744":"code","526f4e69":"code","7dfc7a09":"code","f9002f2a":"code","dee2e3e2":"code","a4ccb0ff":"code","04913f08":"code","003f15fb":"code","56440715":"code","feee7f2e":"code","0496e441":"code","1436f593":"code","54ccb4c3":"code","a3aefc46":"code","bb786855":"code","676d5068":"code","434c6d88":"code","d6358718":"code","d60fdb70":"code","430503d1":"code","4b5da7ee":"code","231ba685":"code","920a881e":"code","06277c3d":"markdown","7bd8eb7d":"markdown","f02eb211":"markdown","d83dc43d":"markdown","b88feef0":"markdown","f9006937":"markdown","e3bc5510":"markdown","b8e0fb21":"markdown","c7d94b72":"markdown","5589caa0":"markdown","22b9e619":"markdown","978005e3":"markdown","3f3db8bd":"markdown","aeb1ba4a":"markdown","54ca7bdc":"markdown","4ae1b279":"markdown","fb7b2d79":"markdown","3e5a01fb":"markdown","407f7801":"markdown","dd073417":"markdown","4aa47c73":"markdown","16638265":"markdown","bdc6412e":"markdown","5b0b29e5":"markdown","0d247bb9":"markdown","63cad80a":"markdown","0d9655d5":"markdown","8b4fe4a0":"markdown","ba75fe58":"markdown","d93b8af4":"markdown","5b74ba66":"markdown","6cfecfeb":"markdown","46880beb":"markdown","e244cc87":"markdown","46be5331":"markdown","29f193e9":"markdown","c5499081":"markdown","9a01cacc":"markdown","e51836e0":"markdown","6307eeb5":"markdown","094bd2c6":"markdown","4d3087e8":"markdown","5bfe5536":"markdown","ab11483b":"markdown","3a8eba75":"markdown"},"source":{"8ba64693":"# Common imports\nimport pandas as pd\nimport numpy as np\nimport os\n\n# To plot Pretty figures\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cufflinks as cf\ncf.go_offline()\n\n#Modelling and others\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import  KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.metrics import accuracy_score,recall_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\nmpl.rc('axes', labelsize= 15)\nmpl.rc('xtick', labelsize= 12)\nmpl.rc('ytick', labelsize= 12)\n\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n# avoid warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","50ce92b3":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n","8f50812b":"train=train.set_index('PassengerId')\ntest=test.set_index('PassengerId')","4d64aa46":"train .head()","292a19e5":"# See the name of the columns\ntrain.columns","bf806c35":"train.info()","0943f3f0":"print(\"Numeric columns: \\n\", train.select_dtypes(include='number').columns)\n\nprint(\"Categorical columns: \\n\", train.select_dtypes(include='object').columns)","d74494c3":"train.describe()","8af1e8d5":"train.hist(bins=30, figsize=(16,12))\n#save_fig(\"attribute_histogram_plots\")\nplt.show()","b95f1018":"#check the percentage of the passenger survived and not survived on the training dataset\nPassenger_notsurvived =  (train['Survived'].value_counts()[0] \/ len(train['Survived']) ) * 100 \nPassenge_Survived = 100 - Passenger_notsurvived\nprint(\"Passenge Survived : {:.2f}% , Passenge not Survived : {:.2f}%\".format(Passenge_Survived,Passenger_notsurvived))","0a4ef614":"# use seaborn\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',data=train,palette='RdBu_r')","844aaf76":"g = sns.factorplot(x=\"Survived\", y = \"Age\",data = train, kind=\"box\")","1e3f414a":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=train,palette='RdBu_r')","6e23de96":"#more checks\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')","9f99a19c":"def comparevariables(train,var1, var2):\n    print(train[[var1, var2]][train[var2].isnull()==False].groupby([var1], as_index=False).mean().sort_values(by=var2, ascending=False))","09fed7c4":"comparevariables(train,'Age','Survived')","c1617b00":"comparevariables(train,'Pclass','Survived')","bb8b3dce":"comparevariables(train,'SibSp','Survived')","73d3ce23":"#Let's plot a few more\nsns.countplot(x='SibSp',data=train)","10a379b8":"train['Fare'].hist(color='green',bins=40,figsize=(8,4))","a43c5026":"train['Fare'].iplot(kind='hist',bins=30,color='green')","b280eda5":"plt.figure(figsize=(12, 7))\nsns.boxplot(x='Pclass',y='Age',data=train,palette='winter')","277113b9":"# Combine train and test features\ndf = pd.concat([train, test], axis=0, sort=False)\ndf.shape","bd485d27":"df.head()","fdedaaed":"#first check the missing data\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent*100], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","b10e68a9":"# Visualize missing values\nmissing_data=missing_data.head(5).drop('Survived')\nf, ax = plt.subplots(figsize=(10, 8))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.title('Percent missing data by feature', fontsize=15)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)","34716e81":"df['Age'].describe()","d824c757":"df['Title'] = df['Name'].str.split(',').str[1].str.split('.').str[0].str.strip()","dd50886e":"df['Title'].value_counts()","665148e1":"df['Age'] = df.groupby([ 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))","e2c10e38":"median_fare = df.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ndf['Fare'] = df['Fare'].fillna(median_fare)","c808624b":"em_mode = df[df['Pclass']==1]['Embarked'].mode()[0]\ndf['Embarked']=df['Embarked'].fillna(em_mode)","ba89a349":"# Thanks to https:\/\/www.kaggle.com\/mauricef\/titanic for these amazing features\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount \/ df.WomanOrBoyCount.replace(0, np.nan)\ndf['Alone'] = (df.WomanOrBoyCount == 0)","7d1ab1c2":"df.columns","6d8d3f64":"df=df[['Sex', 'WomanOrBoySurvived', 'Alone','Age','Fare']]","308d5744":"df.isna().sum()","526f4e69":"#Check the info one more time\ndf.info()\n","7dfc7a09":"#fill the non null value with 0 and replcae sex columns\ndf['WomanOrBoySurvived']=df['WomanOrBoySurvived'].fillna(0)\ndf['Sex']=df['Sex'].replace({'male': 0, 'female': 1})","f9002f2a":"#Check for the last time \ndf.isna().sum()","dee2e3e2":"df.head()","a4ccb0ff":"# Split features and labels\ny = train['Survived'].reset_index(drop=True)\ntrain = df[:len(train)]\ntest = df[len(train):]\ntrain.shape,test.shape","04913f08":"Scores = pd.DataFrame({'Model': [],'Accuracy Score': [], 'Recall':[], 'F1score':[]})","003f15fb":"#Split the train and test data\nX_train, X_test, y_train, y_test = train_test_split(train, y,test_size=0.20, random_state=42)","56440715":"xgboost = XGBClassifier(learning_rate=0.01, n_estimators=4060,gamma=0.0482,\n                                     max_depth=4, min_child_weight=0,\n                                     subsample=0.7,colsample_bytree=0.7,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006,random_state=42)\n\nxgboost.fit(X_train,y_train)\ny_pred = xgboost.predict(X_test)\n\nscore = pd.DataFrame({\"Model\":['XGBClassifier'],\n                    \"Accuracy Score\": [accuracy_score(y_test, y_pred)],\n                   \"Recall\": [recall_score(y_test, y_pred)],\n                   \"F1score\": [f1_score(y_test, y_pred)]})\nScores = Scores.append(score)","feee7f2e":"RFmodel = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [100, 3000]}, cv=10).fit(X_train,y_train)\nRFmodel.fit(X_train,y_train)\ny_pred = RFmodel.predict(X_test)\n\nscore = pd.DataFrame({\"Model\":['RFmodel'],\n                    \"Accuracy Score\": [accuracy_score(y_test, y_pred)],\n                   \"Recall\": [recall_score(y_test, y_pred)],\n                   \"F1score\": [f1_score(y_test, y_pred)]})\nScores = Scores.append(score)","0496e441":"DTmodel = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid={'max_depth' : np.arange(2, 9, dtype=int),\n              'min_samples_leaf' :  np.arange(1, 3, dtype=int)}, cv=10).fit(X_train,y_train)\nDTmodel.fit(X_train,y_train)\ny_pred = DTmodel.predict(X_test)\n\nscore = pd.DataFrame({\"Model\":['DTmodel'],\n                    \"Accuracy Score\": [accuracy_score(y_test, y_pred)],\n                   \"Recall\": [recall_score(y_test, y_pred)],\n                   \"F1score\": [f1_score(y_test, y_pred)]})\nScores = Scores.append(score)","1436f593":"KNmodel = GridSearchCV(estimator=KNeighborsClassifier(), param_grid={'n_neighbors': [2, 10]}, cv=10).fit(X_train,y_train)\n\nKNmodel.fit(X_train,y_train)\ny_pred = KNmodel.predict(X_test)\n\nscore = pd.DataFrame({\"Model\":['KNmodel'],\n                    \"Accuracy Score\": [accuracy_score(y_test, y_pred)],\n                   \"Recall\": [recall_score(y_test, y_pred)],\n                   \"F1score\": [f1_score(y_test, y_pred)]})\nScores = Scores.append(score)","54ccb4c3":"logmodel= LogisticRegression()\nlogmodel.fit(X_train,y_train)\n\ny_pred = logmodel.predict(X_test)\n\nscore = pd.DataFrame({\"Model\":['logmodel'],\n                    \"Accuracy Score\": [accuracy_score(y_test, y_pred)],\n                   \"Recall\": [recall_score(y_test, y_pred)],\n                   \"F1score\": [f1_score(y_test, y_pred)]})\nScores = Scores.append(score)","a3aefc46":"SVCmodel= SVC(probability=True)\nSVCmodel.fit(X_train,y_train)\n\ny_pred = SVCmodel.predict(X_test)\n\nscore = pd.DataFrame({\"Model\":['SVCmodel'],\n                    \"Accuracy Score\": [accuracy_score(y_test, y_pred)],\n                   \"Recall\": [recall_score(y_test, y_pred)],\n                   \"F1score\": [f1_score(y_test, y_pred)]})\nScores = Scores.append(score)","bb786855":"vot_classifier = VotingClassifier(estimators=[('xg', xgboost),('log', logmodel), ('rf', RFmodel), ('dt', DTmodel), ('svc', SVCmodel)], voting='soft', n_jobs=4)\n\nvot_classifier=vot_classifier.fit(X_train, y_train)\n\ny_pred = vot_classifier.predict(X_test)\n\nscore = pd.DataFrame({\"Model\":['vot_classifier'],\n                    \"Accuracy Score\": [accuracy_score(y_test, y_pred)],\n                   \"Recall\": [recall_score(y_test, y_pred)],\n                   \"F1score\": [f1_score(y_test, y_pred)]})\nScores = Scores.append(score)\nvot_classifier=vot_classifier.fit(train, y)","676d5068":"Scores","434c6d88":"df1=df[['Sex', 'WomanOrBoySurvived', 'Alone']]","d6358718":"# Split features and labels\ny=y\ntrain = df1[:len(train)]\ntest = df1[len(train):]\ntrain.shape,test.shape","d60fdb70":"DTmodel.fit(train,y)","430503d1":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n","4b5da7ee":"submission.iloc[:,1] = DTmodel.predict(test).astype(int)","231ba685":"submission.head()","920a881e":"submission.to_csv(\"submissionTitanic.csv\", index=False)","06277c3d":"<h5> DecisionTreeClassifier <h5>","7bd8eb7d":"<h5> Let's get started <\/h5>","f02eb211":"Let's analyze our target variable. ","d83dc43d":"<h5> RandomForestClassifier <\/h5>","b88feef0":"Although these models give a high training accuracy of 94%, the accuracy on the competition test set is ~82% which is top 5% submission on the leaderboard.\n\nThe trick of https:\/\/www.kaggle.com\/mauricef\/titanic using only three features 'Sex', 'WomanOrBoySurvived', 'Alone' gives a high accuracy ~ 83% on the competition test set using a\n    the same DecisionTreeClassifier which is top 2% on the leaderboard.","f9006937":"[](http:\/\/)<h5> Suppport vector Classiffier <\/h5>","e3bc5510":"<h5> XGBClassifier <\/h5>","b8e0fb21":"Another quick way to get a feel of the type of data is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis).","c7d94b72":"<h5> KNeighborsClassifier <\/h5>","5589caa0":"There are 891 instances in the dataset","22b9e619":"<h5> Competition Description <\/h5> ","978005e3":"<h3> Prepare the Data for Machine Learning Algorithms <\/h3>","3f3db8bd":"Observations:Class 1 Passenger has very high survival rate compared the the class 3. \n    Let's wrtie a function and compared the survival rate.","aeb1ba4a":"Filling missing value of age","54ca7bdc":"Filling other missing values","4ae1b279":"<h5> Imputing missing values <\/h5>","fb7b2d79":"<h3> Predictions and Submission <\/h3>","3e5a01fb":"Let's compare the surviavl rates as a function of Age, Pclass, and SibSp.","407f7801":"<h5> Create new features <\/h5>","dd073417":"Let's try to using only these three features ","4aa47c73":"<h1> <center> <h1> Titanic: Machine Learning from Disaster <\/h1><\/center><\/h1>","16638265":"<h5> The target variable: Survived <\/h5>","bdc6412e":"The info() method is useful to get a quick description of the data, in\nparticular the total number of rows, each attribute\u2019s type, and the number\nof nonnull values","5b0b29e5":"Let's look at the other values. The describe() method shows a summary of the numnerical attributes.","0d247bb9":"<h5> Create Test Train Features <\/h5>","63cad80a":"Let's import the libraries","0d9655d5":"<h5>Define Scores <\/h5>","8b4fe4a0":"I tried to explain the feature engineering parts, applied different models inlcuding ensemble modeling for a higher accuracey. While adding new features increses the training accuracy to ~94%, it reduces the accuracy on the test data set(test accuracy ~82%). \n\nThe trick of adding a feature 'WomanOrBoySurvived' by https:\/\/www.kaggle.com\/mauricef\/titanic and with  only three features helps to increase the test accuracy to ~83%. I found https:\/\/www.kaggle.com\/mauricef\/titanic this Kernal very useful. \n\n\nThis Kernel will be very helpful for the beginner. If you like this Kernerl Please give an upvote, which keeps me motivated. \n","ba75fe58":"<h5> LogisticRegression <\/h5>","d93b8af4":"Let's import the data","5b74ba66":"Observations:\n    1. The survival rate for male was low compared to a high survival rate of the female.","6cfecfeb":"Let's set PassengerId as the Index. ","46880beb":"Let's visualize the data using seaborn countplot abd others to gain into further insight","e244cc87":"<h3> Exploratory Data Analysis (EDA) <\/h3>","46be5331":"<h5> Ensemble modelling <\/h5>","29f193e9":"Let's seperate the numeric and categorical columns","c5499081":"Please let me know if you think that this Kernel can be further improved or if you find an error. \nA few upvotes would by highly appreciated. \n   ","9a01cacc":"Consider only 'Sex', 'WomanOrBoySurvived', 'Alone','Age' and 'Fare' columns to avoid overfitting the training data whcih have high correlation with our target variable.","e51836e0":"<h5>Models <\/h5>","6307eeb5":"Let's take a quick look at the top 5 rows of the training data using the DataFrame's head() method","094bd2c6":"There are 1014 missing cabins, and 263 missing ages.We cand drop the Cabin columns as more than 77% instances are missing. ","4d3087e8":"<h5> Thank you very much <\/h5>","5bfe5536":"### Modelling","ab11483b":"<h5> Data Cleaning <\/h5>","3a8eba75":"The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we will build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc)"}}