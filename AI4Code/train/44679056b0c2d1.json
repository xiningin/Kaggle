{"cell_type":{"5ecc8016":"code","8199ed17":"code","3fc0a853":"code","edccf677":"code","eb730d77":"code","c683a05b":"code","35fb17f3":"code","0dff1744":"code","44530b6f":"code","6d8b798c":"code","a761f72b":"code","6b246089":"code","00218236":"code","37cd37d4":"code","afbb1a7a":"code","0689e4cc":"code","1850d310":"code","25e24254":"code","5a391882":"code","64928fd1":"code","f34dbf0d":"code","c02ce7a8":"code","49c2f22a":"code","1843f49c":"code","6936dc9c":"code","271be2c0":"code","59c2d931":"code","23edcbdd":"code","089e4cc4":"code","f8c6b742":"code","56937cbc":"code","fb33cc18":"code","d66294ea":"markdown","3f80a8fb":"markdown","ff58f17f":"markdown","219786bc":"markdown","38b7b198":"markdown","b0239c7e":"markdown","f73f78ad":"markdown","be2763f4":"markdown","19540154":"markdown","cd3abb33":"markdown","bed53682":"markdown","01c0c48d":"markdown","e7008f95":"markdown","a63808c4":"markdown","43d28c76":"markdown","d418bd52":"markdown"},"source":{"5ecc8016":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport shap\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8199ed17":"import matplotlib.pyplot as plt\nimport janestreet\nimport warnings\n\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12, 4)\nwarnings.filterwarnings('ignore')","3fc0a853":"TARGET_THRESHOLD = 0\nTIME_SPLIT = 400\nTIME_COLUMN = \"date\"\nTARGET = \"action\"","edccf677":"data = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/train.csv\")\n### Due to resources restrictions, we use a sample of it\n#data = data.sample(frac=0.1)","eb730d77":"data[\"action\"] = data[\"resp\"] > TARGET_THRESHOLD\nfeatures = [col for col in data.columns if \"feature\" in col]","c683a05b":"in_time = data[data[TIME_COLUMN] <= TIME_SPLIT]\nout_of_time = data[data[TIME_COLUMN] > TIME_SPLIT]","35fb17f3":"train, test = train_test_split(in_time, \n                               test_size=0.2, \n                               random_state=42)","0dff1744":"model = LGBMClassifier()\nmodel.fit(train[features], train[TARGET])","44530b6f":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(test[features])","6d8b798c":"pooled_shap_importance = np.abs(shap_values[1]).mean(axis=0)\npooled_shap_importance = pd.DataFrame(pooled_shap_importance)\npooled_shap_importance.index = features\npooled_shap_importance.sort_values(by=0, ascending=False, inplace=True)\npooled_shap_importance","a761f72b":"importance = []\nfor period in in_time[TIME_COLUMN].unique():\n    test_period = test[test[TIME_COLUMN] == period]\n    shap_contrib = np.abs(model.predict(test_period[features], pred_contrib=True)[:, :-1]).mean(0)\n    df = pd.DataFrame(shap_contrib.reshape(1, len(features)), columns=features)\n    df[\"period\"] = period\n    importance.append(df)\n    \nimportance = pd.concat(importance)    ","6b246089":"importance","00218236":"average = importance.drop(columns=[\"period\"]).mean()","37cd37d4":"average.sort_values()","afbb1a7a":"deviation = importance.drop(columns=[\"period\"]).std() \/ average","0689e4cc":"deviation.fillna(0, inplace=True)\ndeviation","1850d310":"median_average_contrib = average.median()\nmedian_std_contrib = deviation.median()\n\nthreshold_average_contrib = np.percentile(average, 50)\nthreshold_std_contrib = np.percentile(deviation, 80)","25e24254":"plt.scatter(average, deviation)\nxmin, xmax, ymin, ymax = plt.axis()\nplt.hlines(threshold_std_contrib, xmin, xmax, linestyle=\"dotted\", color=\"red\")\nplt.vlines(threshold_average_contrib, ymin, ymax, linestyle=\"dotted\", color=\"red\")\nplt.legend(bbox_to_anchor=(1.05, 1.0))\nplt.title(\"Contribution x Instability\")\nplt.ylabel(\"Instability\")\nplt.xlabel(\"Contribution\")\nplt.show()","5a391882":"aggregate_importance = pd.DataFrame()\naggregate_importance[\"average\"] = average.values\naggregate_importance[\"instability\"] = deviation.values\naggregate_importance.index = average.index\n\naggregate_importance","64928fd1":"mask = (aggregate_importance[\"average\"] >= threshold_average_contrib) & (aggregate_importance[\"instability\"] <= threshold_std_contrib)\nstable_features = aggregate_importance[mask].index","f34dbf0d":"fraction_selected = len(stable_features) \/ len(aggregate_importance)\nfraction_selected","c02ce7a8":"stable_features","49c2f22a":"def utility_score_numba(date, weight, resp, action):\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ len(Pi))\n    u = min(max(t, 0), 6) * np.sum(Pi)\n    return u\n\ndef jane_utility(data, action_column=\"action\"):\n    return utility_score_numba(data[\"date\"].values, \n                               data[\"weight\"].values, \n                               data[\"resp\"].values, \n                               data[action_column].values)","1843f49c":"model = LGBMClassifier()\nmodel.fit(train[features], train[\"action\"])\n\ntest[\"benchmark_1\"] = model.predict_proba(test[features])[:, 1]\nout_of_time[\"benchmark_1\"] = model.predict_proba(out_of_time[features])[:, 1]\n\ntest[\"benchmark_1_action\"] = model.predict(test[features])\nout_of_time[\"benchmark_1_action\"] = model.predict(out_of_time[features])\n\nprint(\"Test AUC (in time): {:.6f}\".format(roc_auc_score(test[TARGET], test[\"benchmark_1\"])))\nprint(\"Out of time AUC: {:.6f}\".format(roc_auc_score(out_of_time[TARGET], out_of_time[\"benchmark_1\"])))\nprint(\"-----------\")\nprint(\"Test Jane Utility (in time): {:.2f}\".format(jane_utility(test, \"benchmark_1_action\")))\nprint(\"Out of time Jane Utility: {:.2f}\".format(jane_utility(out_of_time, \"benchmark_1_action\")))","6936dc9c":"n_top_features = int(len(features) * fraction_selected)\nn_top_features = pooled_shap_importance.index[:n_top_features]\n\nbenchmark_model = LGBMClassifier()\nbenchmark_model.fit(train[n_top_features], train[\"action\"])\n\ntest[\"benchmark_2\"] = benchmark_model.predict_proba(test[n_top_features])[:, 1]\nout_of_time[\"benchmark_2\"] = benchmark_model.predict_proba(out_of_time[n_top_features])[:, 1]\n\ntest[\"benchmark_2_action\"] = benchmark_model.predict(test[n_top_features])\nout_of_time[\"benchmark_2_action\"] = benchmark_model.predict(out_of_time[n_top_features])\n\nprint(\"Test AUC (in time): {:.6f}\".format(roc_auc_score(test[TARGET], test[\"benchmark_2\"])))\nprint(\"Out of time AUC: {:.6f}\".format(roc_auc_score(out_of_time[TARGET], out_of_time[\"benchmark_2\"])))\nprint(\"-----------\")\nprint(\"Test Jane Utility (in time): {:.2f}\".format(jane_utility(test, \"benchmark_2_action\")))\nprint(\"Out of time Jane Utility: {:.2f}\".format(jane_utility(out_of_time, \"benchmark_2_action\")))","271be2c0":"stable_model = LGBMClassifier()\nstable_model.fit(train[stable_features], train[\"action\"])\n\ntest[\"challenger\"] = stable_model.predict_proba(test[stable_features])[:, 1]\nout_of_time[\"challenger\"] = stable_model.predict_proba(out_of_time[stable_features])[:, 1]\n\ntest[\"challenger_action\"] = stable_model.predict(test[stable_features])\nout_of_time[\"challenger_action\"] = stable_model.predict(out_of_time[stable_features])\n\nprint(\"Test AUC (in time): {:.6f}\".format(roc_auc_score(test[TARGET], test[\"challenger\"])))\nprint(\"Out of time AUC: {:.6f}\".format(roc_auc_score(out_of_time[TARGET], out_of_time[\"challenger\"])))\nprint(\"-----------\")\nprint(\"Test Jane Utility (in time): {:.2f}\".format(jane_utility(test, \"challenger_action\")))\nprint(\"Out of time Jane Utility: {:.2f}\".format(jane_utility(out_of_time, \"challenger_action\")))","59c2d931":"pd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"benchmark_1_action\"])).rolling(60).mean().plot(label=\"All features (benchmark)\", color=\"purple\")\npd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"benchmark_2_action\"])).rolling(60).mean().plot(label=\"Selected features (benchmark 2)\")\npd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"challenger_action\"])).rolling(60).mean().plot(label=\"Temporal selected features (challenger)\", color=\"green\")\n\nxmin, xmax, ymin, ymax = plt.axis()\nplt.vlines(TIME_SPLIT, ymin, ymax, linestyle=\"dotted\", color=\"red\", label=\"Out of time split\")\nplt.legend(bbox_to_anchor=(1.05, 1.0))\nplt.title(\"Performance moving average of 60 periods window for both test and out of time periods\", pad=16)\nplt.ylabel(\"sum(Weight * Resp * Action)\")\nplt.xlabel(\"Date\")\nplt.show()","23edcbdd":"feature_tags = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/features.csv\")","089e4cc4":"tags = [col for col in feature_tags if \"tag\" in col]\nfeature_tags[\"Stable\"] = feature_tags[\"feature\"].apply(lambda x: True if x in stable_features else False)\nfeature_tags.groupby(\"Stable\")[tags].mean().transpose().plot(kind=\"bar\")\nplt.title(\"Proportion of features with a certain tag considering stable and unstable\", fontsize=16, pad=16)\nplt.show()","f8c6b742":"model = LGBMClassifier()\nmodel.fit(data[stable_features], data[\"action\"])","56937cbc":"### I'm going to use 0.5 since it's what the benchmark submission is using\nthreshold = data[\"action\"].mean()\nthreshold","fb33cc18":"env = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df[\"action\"] = model.predict(test_df[stable_features]).astype(int)\n    env.predict(sample_prediction_df)","d66294ea":"## Visualize how features are distributed regarding contribution and instability\n\nWe want to select the ones with high contribution and low instability.","3f80a8fb":"Just a sanity check about the volume of features selected. If you play with the stable definition, this volume is going to change. \n\nThe higher the threshold for contribution and the lower for the standard deviation, the more restrictive the definition and then this proportion should decrease.","ff58f17f":"## Storing pooled feature importance for the benchmark","219786bc":"### Benchmark I","38b7b198":"## Submission\n\nWe retrain with the full dataset and the stable features only. ","b0239c7e":"# Selecting features exploring the temporal nature of the dataset\n\nThis is a very basic notebook focusing only in the feature selection part. If you enjoy it, than you can adapt to the models you're using or simply use the stable features set we find here.\n\nI describe this approach on [this blog post](https:\/\/lgmoneda.github.io\/2020\/12\/07\/temporal-feature-selection-with-shap-values.html), which contains another practical example. \n\n## Motivation\n\nIt's common to see performance dropping overtime. It's very likely our models explore correlations that don't hold in out of distribution data coming from contexts slightly different from the ones we have learned:\n\n![trend](https:\/\/lgmoneda.github.io\/images\/temporal-feature-selection\/model_degrade.jpg)\n\nThe idea here is to look to the features that are consistent important through the many time periods we have available hoping they are the most robust to keep their predictive power in the future unseend data.\n\n\n## Results summary\n\nModels for comparison:  \n- Challenger: default lgbm with features selected using temporal shap\n- Benchmarks:\n  1. Default lgbm with all features (benchmark I)\n  2. Default lgbm with shap importance in the whole train and the same number of features (benchmark II)\n \n \n ","f73f78ad":"## Is there any pattern on the stable features regarding their nature?\n\nTo answer this question, we're going to take a look into the tags: ","be2763f4":"## Extracting the importance for every period using the in time test set","19540154":"## Split the data\n\nWe're going to create two temporal sets, `in time` and `out of time`. \nThen, we'll split the `in time` into train and test.\n\n![split](https:\/\/lgmoneda.github.io\/images\/temporal-feature-selection\/holdout_split.jpg)\n","cd3abb33":"## Stable definition\n\nHere you can change the threshold and play with the \"stable\" definition, or just use the median. \n\nIn the second case, do:\n\n```\nthreshold_average_contrib = average.median()\n```","bed53682":"## Visualizing comparison","01c0c48d":"## Train a full model to extract importance","e7008f95":"### Challenger","a63808c4":"Here's the list of stable features. If you want, you can just pick them and test how they perform using your approach. \n\nOf course, the stable definition depends here both on the approach and the thresholds. So you might want to adjust the base model for importance to follow what you're using.","43d28c76":"### Benchmark II","d418bd52":"## Checking how's the performance in the out of time"}}