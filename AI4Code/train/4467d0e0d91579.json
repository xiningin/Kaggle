{"cell_type":{"18125bea":"code","210b7636":"code","13260637":"code","2b7be2f3":"code","df323a3d":"code","886c05b1":"code","e2740efc":"code","3a8a4cbd":"code","1ca0d855":"code","4ae8a091":"code","fc6b1c12":"code","facbd8a4":"code","0f329e37":"code","b7102721":"code","371689c8":"code","7324c225":"code","b2069647":"markdown","a198d171":"markdown","f0a87d81":"markdown","6a6cc9e3":"markdown","35b21a1b":"markdown","b2fce964":"markdown","6b3c4ce4":"markdown","2fbae0f6":"markdown","a697e47f":"markdown","c2238d5f":"markdown","11c85193":"markdown","134039bb":"markdown"},"source":{"18125bea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","210b7636":"#Importing required libraries\n!pip install lifetimes\n\nimport datetime as dt\nimport numpy as np\nimport pandas as pd\nfrom lifetimes import BetaGeoFitter\nfrom lifetimes import GammaGammaFitter\nfrom sklearn.preprocessing import MinMaxScaler\nfrom lifetimes.plotting import plot_frequency_recency_matrix\nfrom lifetimes.plotting import plot_probability_alive_matrix\nfrom lifetimes.plotting import plot_period_transactions\nfrom lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","13260637":"#Pandas set option\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.float_format\", lambda x: \"%.5f\" % x)","2b7be2f3":"def check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(3))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.describe().T)\n\ndef outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.01)\n    quartile3 = dataframe[variable].quantile(0.99)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","df323a3d":"#Importing data\ndataframe = pd.read_csv(\"..\/input\/online-retail-ii-uci\/online_retail_II.csv\")","886c05b1":"check_df(dataframe)","e2740efc":"dataframe.head()","3a8a4cbd":"#Should transform InvoiceDate to datetime\ndataframe['InvoiceDate'] = pd.to_datetime(dataframe['InvoiceDate'])\n","1ca0d855":"dataframe = dataframe[dataframe[\"InvoiceDate\"]< \"2011-01-01\"]","4ae8a091":"#Backup dataframe \ndf = dataframe.copy()","fc6b1c12":"check_df(df)","facbd8a4":"#We delete them incomplete information because it is customer-oriented\ndf.dropna(axis=0, inplace=True)\n\n\n#The dataset has sales return invoice. We need to eliminate them\ndf = df[~df[\"Invoice\"].str.contains(\"C\", na=False)]\n\n#As you can see, the negative values are available in price and quantity values.We need to eliminate them too.\ndf = df[df[\"Quantity\"] > 0]\ndf = df[df[\"Price\"] > 0]\n        \n#We need to push quantity and price values for better results\nreplace_with_thresholds(df, \"Quantity\")\nreplace_with_thresholds(df, \"Price\")\n        \n#Finally, we are creating a total price variable by multiplying the quantity and price values\ndf[\"TotalPrice\"] = df[\"Quantity\"] * df[\"Price\"]","0f329e37":"check_df(df)","b7102721":"def cltv_prediction(dataframe, month=3, plot=False):\n    \n    #We need to choose an analysis date. The latest data belongs to the following date and we choose 2 days after that date\n    #df[\"InvoiceDate\"].max() --> \"2010-12-23\"\n    today_date = dt.datetime(2010, 12, 25)\n    #We do grouping according to customer ids and calculate the following values\n    #Recency--> Time passing over the customer's final purchase\n    #T --> Time since the customer's first purchase. Age of customer for company. Tenure.\n    #Frequency --> Frequency. The number of recurring sales.\n    #Monetary --> Observed transaction value.\n    cltv_df = df.groupby('Customer ID').agg({'InvoiceDate': [lambda date: (date.max() - date.min()).days,\n                                                         lambda date: (today_date - date.min()).days],\n                                             'Invoice': lambda num: num.nunique(),\n                                             'TotalPrice': lambda TotalPrice: TotalPrice.sum()})\n\n    \n    cltv_df.columns = cltv_df.columns.droplevel(0)\n    cltv_df.columns = ['recency', 'T', 'frequency', 'monetary']\n    cltv_df[\"monetary\"] = cltv_df[\"monetary\"] \/ cltv_df[\"frequency\"]\n    \n    #Calculate the frequency and recency values as weekly\n    cltv_df[\"recency\"] = cltv_df[\"recency\"] \/ 7\n    cltv_df[\"T\"] = cltv_df[\"T\"] \/ 7\n    \n    #By definition, we choose customers greater than 1 frequency\n    cltv_df = cltv_df[(cltv_df['frequency'] > 1)]\n\n    # Establishment of the BG-NBD model\n    bgf = BetaGeoFitter(penalizer_coef=0.001)\n    bgf.fit(cltv_df['frequency'],\n            cltv_df['recency'],\n            cltv_df['T'])\n\n\n    #cltv_df[\"expected_purc_1_month\"] = bgf.predict(4, cltv_df['frequency'], cltv_df['recency'], cltv_df['T'])\n\n    # 3. Establishment of the GAMMA-GAMMA model\n    ggf = GammaGammaFitter(penalizer_coef=0.01)\n    ggf.fit(cltv_df['frequency'], cltv_df['monetary'])\n    cltv_df[\"expected_average_profit\"] = ggf.conditional_expected_average_profit(cltv_df['frequency'],\n                                                                                 cltv_df['monetary'])\n\n    # Calculation of CLTV via BG-NBD and GG model.\n    cltv = ggf.customer_lifetime_value(bgf,\n                                       cltv_df['frequency'],\n                                       cltv_df['recency'],\n                                       cltv_df['T'],\n                                       cltv_df['monetary'],\n                                       time=month,\n                                       freq=\"W\",  # The frequency information of t.\n                                       discount_rate=0.05)\n\n    cltv = cltv.reset_index()\n    cltv_final = cltv_df.merge(cltv, on=\"Customer ID\", how=\"left\")\n    \n    #If scaler is wanted to use\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaler.fit(cltv_final[[\"clv\"]])\n    cltv_final[\"scaled_clv\"] = scaler.transform(cltv_final[[\"clv\"]])\n\n    cltv_final[\"segment\"] = pd.qcut(cltv_final[\"scaled_clv\"], 5, labels=[\"about_to_sleep\", \"at_risk\", \"need_attention\",\"loyal_customers\", \"champions\"])\n    \n    if plot:\n        plot_probability_alive_matrix(bgf)\n        \n        plot_period_transactions(bgf)       \n        \n        \n    return cltv_final","371689c8":"cltv_6m = cltv_prediction(df, month=6 , plot=True)","7324c225":"cltv_6m.groupby(\"segment\")[\"clv\"].describe()","b2069647":"# 1-INTRODUCTION\n\n\n![clv.jpg](attachment:clv.jpg)\n\n\n**CUSTOMER LIFETIME VALUE**\n\nCLTV is a measurement of how valuable a customer is to your company, not just on a purchase-by-purchase basis but across the whole relationship. Probabilistic lifetime value estimation is made with time projection for a certain t time. CLTV is a dynamic concept, not a static model. \n\nThe most basic formula we use is as follows:\n\n\n**CLTV = Expected Number of Transaction * Expected Average Profit**\n\n\nWe will estimate the \u201cExpected Number of Transaction\u201d part using the BG\/NBD model and the \u201cExpexed Average Profit\u201d part using the gamma gamma model.\n\nBefore moving on to the explanation of the models, I would like to explain the concept of buy till you die. We actually create the BG\/NBD model using the concept of Buy till you die.\n\n**BUY TILL YOU DIE MODEL**\n\nBuy Till You Die model fit probabilistic models to historical transactional data to calculate customer lifetime value. BYTD model answers these kind of questions:\n\n1. How many customers are active?\n2. How many customers will be active one year from now?\n3. Which customers have churned?\n4. How valuable will any customer be to the company in the future?\n\n**Transaction Process (Buy)**\n\nAs long as it is alive, the number of transactions to be performed by a client in a given time period is poisson distributed with the transaction rate parameter.\nIn other words, as long as a customer is alive, he or she will continue to make random purchases around her own transaction rate.\nTransaction rates vary for each client and are gamma distributed for the entire population. (r,alpha)\n\n**Dropout process (Till You Die)**\n\nEach customer has a dropout rate (dropout probability) with probability p.\nA customer will churn with a certain probability after making a purchase.\nDropout rates vary for each client and are beta distributed for the entire population. (a,b)\n\n\n","a198d171":"# 4-CLTV BG\/NBD&GAMMA GAMMA Implementation\n\nData Set : https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Retail+II\n\n**Attribute Information:**\n\n**InvoiceNo:** *Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.*\n\n**StockCode:** *Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.*\n\n**Description:** *Product (item) name. Nominal.*\n\n**Quantity:** *The quantities of each product (item) per transaction. Numeric.*\n\n**InvoiceDate:** *Invice date and time. Numeric. The day and time when a transaction was generated.*\n\n**UnitPrice:** *Unit price. Numeric. Product price per unit in sterling .*\n\n**CustomerID:** Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.\n\n**Country:** *Country name. Nominal. The name of the country where a customer resides.*\n","f0a87d81":"# 7-CONCLUSION\n\nWith the widespread use of the internet and gig economies, competition is now at its highest level. In this direction, it is very important to understand the customer. It is a known fact that it is more costly for companies to reach a new customer than to retain an existing customer. For this reason, it is very important to know which customer will make how much profit and how often they will shop. It is necessary to calculate these values correctly and to reach the customer who will be churn correctly. It is known that there are many computational methods. In this article, I wanted to show how to make a calculation using BG\/NBD and gamma gamma models. Reaching the customer at the right time is as important as calculating correctly. These methods differ for companies. Companies should use and implement their own methods. Thank you for reading and taking the time to read my article.\n","6a6cc9e3":"# 3-Gamma Gamma Model\n\nThe monetary value of a customer\u2019s given transaction varies randomly around their average transaction value.\n\nAverage transaction values vary across customers but do not vary over time for any given individual.\n\nThe distribution of average transaction values across customers is independent of the transaction process. \n\nThe average transaction value is gamma distributed among all customers.\n\nFormula:\n\n![gammagammasubmodel.png](attachment:gammagammasubmodel.png)\n\nmx and x parameters come from user\n\nx--> Frequency. The number of recurring sales (transactions made at least 2 times)\n\nmx --> Monetary. Observed transaction value.\n\nThe p,q and y  are parameters from distribution\n\nWith these parameters, the expected monetary value will be estimated.\n","35b21a1b":"# References\n\nhttps:\/\/www.veribilimiokulu.com\/\n\nhttps:\/\/retina.ai\/academy\/lesson\/history-of-buy-til-you-die-btyd-models\/\n\nPeter S.Fader, Bruce G.S. Hardie, Ka Lok Lee. December 2008\n\nhttps:\/\/en.wikipedia.org\/wiki\/Gamma_distribution\n\nhttps:\/\/en.wikipedia.org\/wiki\/Beta_distribution\n\nPeter S.Fader, Bruce G.S. Hardie. February 2013\n\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Retail+II\n\nhttps:\/\/towardsdatascience.com\/what-is-your-customers-worth-over-their-lifetime-dfae277fd166\n\n\n","b2fce964":"# Predicting Customer Life Time Value (CLTV) via Beta Geometric \/ Negative Binominal Distribution (BG\/NBD) and Gamma Gamma Model","6b3c4ce4":"# 2-Beta Geometric \/ Negative Binominal Distribution (BG\/NBD) MODEL\n\nFader, Hardie and Lee, they present the BG\/NBD model as an alternative to the Pareto\/NBD.\nThe positioning of work is that the model yields very similar results to the Pareto\/NBD while being vastly easier to implement. \n\n1.\tWhile active, transactions made by a customer in time period t is Poisson distributed with mean \u03bbt\n2.\tDifferences in transaction rate between customers follows a gamma distribution with shape r and scale \u03b1\n3.\tEach customer becomes inactive after each transaction with probability p\n4.\tDifferences in p follows a beta distribution with shape parameters a and b\n\nIt should be so that we capture the buying pattern of the whole population and then find a pattern that will personalize the buying of this whole population..\n\nIn summary, what it will do is learn the mass behavior from these individual behaviors and then make a probabilistic estimation specific to the individual.\n\nThe following should not be forgotten: After making a purchase, the customer becomes partial churn. \n\nThe BG\/NBD Model probabilistically models two processes for the expected number of transactions.\n\n*First Process:* Transaction Process (**Buy**)\n\n*Second Process:* Dropout process (**Till You Die**) --> process of becoming churn\n\n\nFormula\n\n![bgnbdformula.png](attachment:bgnbdformula.png)\n\n\n\nx --> frequency of customers who have made at least two purchases\n\ntx --> customer's recency value (must be calculated individually for each customer)\n\nT --> Time since the customer's first purchase. Age of customer for company. Tenure.\n\nr, alfa --> difference in transaction rate between customers parameters of gamma distribution\n\na,b --> Beta distribution parameters expressing drop rate\n\nIn other words, x, tx and T are the characteristics of individuals.\n\nAs a result, it will give the expected values of purchase values in a certain t period while taking values specific to individuals and carrying the characteristics of the population.\n\nIn the light of the gamma and beta distribution that we will learn from the population, the characteristics of the individual and the expected y value in a certain t period will be estimated.\n\n2F1 is the Gaussian hypergeometric function\n\n\nr,alpha,a,b are estimated using the maximum likelihood method","2fbae0f6":"# 6-INCREASE CUSTOMER LIFETIME VALUE\n\n**Provide 24\/7 Support**\n\nThe customer always wants to reach the company instantly. For this reason, an infrastructure should be established to serve the customer 24 hours a day, 7 days a week. The customer's problem should be resolved as soon as possible and the customer should be satisfied.\n\n**Monitor Social Media**\n\nIt is very important to reach the customer through social media in these days when the internet has become widespread. New products and campaigns should be promoted, and if necessary, the customer with a problem should be dealt with individually. Bearing in mind the risks, your team must have at least one employee focused on tracking and replying to social media comments.\n\n**Launch a loyalty program**\n\nRetaining a customer is easier and less costly than acquiring a new customer.  If there is not, a customer loyalty card should be created. This card can be physical or online. With this card, the customer should be encouraged to collect stars or similar items and encourage more shopping. Discounts can be defined on the customer's birthdays or special days.\n\n**Use Up-Sells and Cross-Sells**\n\nIt may seem like a traditional method, but it still works. More monetary should be provided than the customer by encouraging the customer with up-sell or cross-selling. While doing this, the customer should not feel deceived.\n\n**Monitor the feedback**\n\nFeedback from the customer should not be ignored. Small problems can lead to bigger problems. Bad information spreads faster than good information on the internet. Nobody wants the company name to be badly mentioned. For this reason, the problem should be solved as soon as possible.\n","a697e47f":"# HISTORY\n\n![timeline.png](attachment:timeline.png)\n\n\u2022NBD (Ehrenberg 1959)\n\n\u2022Pareto\/NBD Schmittlein, Morrison, and Colombo 1987)\n\n\u2022BG\/NBD (P. Fader, Hardie, and Lee 2005)\n\n\u2022Pareto\/NBD (HB) Ma and Liu (2007)\n\n\u2022MBG\/NBD Batislam, Denizel, and Filiztekin (2007), Hoppe and Wagner (2007)\n\n\u2022Pareto\/NBD (Abe) Abe (2009)\n\n\u2022BG\/BB (Fader, Hardie, and Shang 2010)\n\n\u2022Pareto\/GGG Platzer and Reutterer (2016)\n\n\n\nThe original NBD model from 1959 functions as a benchmark for later models because it\u2019s based on a heterogenous purchase process. But, NBD doesn\u2019t account for customer churn.\n\nThe next model, Pareto\/NBD from 1987, adds a heterogeneous dropout process and is considered one of the top buy-til-you-die models.\n\nNext up, the BG\/NBD model adjusts assumptions to reduce computation time and offers a more robust parameter search. However, this model assumes customers without repeat transactions have not churned.\n\nMBG\/NBD removes inconsistencies with the former model by allowing customers without any activity to remain inactive.\n\nThe newer BG\/CNBD-k and MBG\/CNBD-k models improve forecasting accuracy by allowing for regularity in transaction times. If this regularity exists, these new models can result in much more accurate customer-level predictions.\n\nThe variants of Pareto\/NBD models by Ma and Liu (2007) and by Abe (2009) utilize MCMC simulation to allow for more flexible assumptions. The first model, Pareto\/NBD (HB) is a hierarchical Bayes variant that tests out this approach while sticking to the original model\u2019s assumptions. The second variation, Pareto\/NBD (Abe), can incorporate covariates.\n\nPareto\/GGG is a third variation of Pareto\/NBD that accounts for some level of regularity for inter-transaction times.\n","c2238d5f":"*NOTE-2*\n\n**Beta distribition**\n\nThe beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parameterized by two positive shape parameters, denoted by \u03b1 and \u03b2, that appear as exponents of the random variable and control the shape of the distribution.\n\nThe beta distribution has been applied to model the behavior of random variables limited to intervals of finite length in a wide variety of disciplines.\n\n![betadistribution.png](attachment:betadistribution.png)\n","11c85193":"*NOTE-1*\n\n**Gamma distribution**\n\nThe gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-square distribution are special cases of the gamma distribution. There are two different parameterizations in common use:\n\n\u2022\tWith a shape parameter k and a scale parameter \u03b8.\n\n\u2022\tWith a shape parameter \u03b1 = k and an inverse scale parameter \u03b2 = 1\/\u03b8, called a rate parameter.\n\nIn each of these forms, both parameters are positive real numbers.\n\nThe gamma distribution is the maximum entropy probability distribution\n\n![gamma%20distribution.png](attachment:gamma%20distribution.png)\n","134039bb":"# 5 BG\/NBD Model Validation\n\nWe need to devote our model with two different sets of cross-validation method. \nWe will then use the holdout dataset as production data, fit a new BG\/NBD model on calibration data, and compare the predicted and actual number of repeat purchases over the holdout period.\nIn my next article, I will give more detailed information about how the validation process will be.\n\n"}}