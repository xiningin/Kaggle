{"cell_type":{"42af3a00":"code","3ff99c27":"code","a9ba77c1":"code","8ff8902e":"code","1df60ca8":"code","8696a3e3":"code","0bcb28fb":"code","e462770e":"code","75eb6dce":"code","d175078e":"code","e68b02a2":"code","0d5a634a":"code","1fd046c6":"code","ab9ea6ca":"code","68550255":"code","cd88ad03":"code","0607cd18":"markdown","91f2bee9":"markdown","bdaf78b0":"markdown"},"source":{"42af3a00":"import numpy as np\nimport pickle\nimport torch\n\n# Get the interactive Tools for Matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [9.5, 6]","3ff99c27":"class Vocabulary(object):\n    def __init__(self, pad_token='<pad>', unk_token='<unk>', eos_token='<eos>'):\n        self.token2idx = {}\n        self.idx2token = []\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n        self.eos_token = eos_token\n        if pad_token is not None:\n            self.pad_index = self.add_token(pad_token)\n        if unk_token is not None:\n            self.unk_index = self.add_token(unk_token)\n        if eos_token is not None:\n            self.eos_index = self.add_token(eos_token)\n\n    def add_token(self, token):\n        if token not in self.token2idx:\n            self.idx2token.append(token)\n            self.token2idx[token] = len(self.idx2token) - 1\n        return self.token2idx[token]\n\n    def get_index(self, token):\n        if isinstance(token, str):\n            return self.token2idx.get(token, self.unk_index)\n        else:\n            return [self.token2idx.get(t, self.unk_index) for t in token]\n\n    def __len__(self):\n        return len(self.idx2token)\n\n    def save(self, filename):\n        with open(filename, 'wb') as f:\n            pickle.dump(self.__dict__, f)\n\n    def load(self, filename):\n        with open(filename, 'rb') as f:\n            self.__dict__.update(pickle.load(f))","a9ba77c1":"DATASET_VERSION = 'ca-100'\nCBOW_VOCABULARY_ROOT = f'..\/input\/text-preprocessing\/data\/{DATASET_VERSION}'\nCBOW_VECTORS_ROOT = f'..\/input\/cbow-training\/data\/{DATASET_VERSION}'","8ff8902e":"dict = f'{CBOW_VOCABULARY_ROOT}\/ca.wiki.train.tokens.nopunct.dic'\ncounter = pickle.load(open(dict, 'rb'))\nwords, values = zip(*counter.most_common(5000))\nprint('Most frequent Catalan words')\nprint(words[:10])\nprint(values[:10])","1df60ca8":"from scipy.stats import entropy\nh = entropy(values)\nprint(f'Word entropy: {h:5.2f}, Perplexity: {np.exp(h):5.0f}')\nprint(f'Probability of the most frequent word: {values[0]\/sum(values):2.3f}')","8696a3e3":"_ = plt.plot(values[:50], 'g', 2*values[0]\/np.arange(2,52), 'r')","0bcb28fb":"_ = plt.loglog(values)\nplt.show()","e462770e":"from collections import Counter\nbenford = Counter(int(str(item[1])[0]) for item in counter.most_common(5000))\nprint(benford)\npercentage = np.array(list(benford.values()), dtype=np.float)\npercentage \/= percentage.sum()\n_ = plt.bar(list(benford.keys()), percentage*100)","75eb6dce":"modelname = f'{CBOW_VECTORS_ROOT}\/{DATASET_VERSION}.pt'\nstate_dict = torch.load(modelname, map_location=torch.device('cpu'))","d175078e":"state_dict.keys()","e68b02a2":"input_word_vectors = state_dict['emb.weight'].numpy()\noutput_word_vectors = state_dict['lin.weight'].numpy()","0d5a634a":"token_vocab = Vocabulary()\ntoken_vocab.load(f'{CBOW_VOCABULARY_ROOT}\/ca.wiki.vocab')","1fd046c6":"class WordVectors:\n    def __init__(self, vectors, vocabulary):\n        # TODO \n        self.vocabulary = vocabulary\n    \n    def most_similar(self, word, topn=10):\n        # TODO\n        return [\n            ('valenci\u00e0', 0.8400525),\n            ('basc', 0.75919044),\n            ('gallec', 0.7418786),\n            ('mallorqu\u00ed', 0.73923385),\n            ('castell\u00e0', 0.69002914),\n            ('franc\u00e8s', 0.6782388),\n            ('espanyol', 0.6611247),\n            ('bret\u00f3', 0.641976),\n            ('aragon\u00e8s', 0.6250948),\n            ('andal\u00fas', 0.6203275)\n        ]\n    \n    def analogy(self, x1, x2, y1, topn=5, keep_all=False):\n        # If keep_all if False we remove the input words (x1, x2, y1) from the returned closed words\n        # TODO\n        return [\n            ('polon\u00e8s', 0.9679756),\n            ('suec', 0.9589857),\n            ('neerland\u00e8s', 0.95811903),\n            ('rus', 0.95155054),\n            ('txec', 0.950968),\n            ('basc', 0.94935954),\n            ('dan\u00e8s', 0.94827694),\n            ('turc', 0.9475782)\n        ]","ab9ea6ca":"model1 = WordVectors(input_word_vectors, token_vocab)\nmodel2 = WordVectors(output_word_vectors, token_vocab)","68550255":"model1.most_similar('catal\u00e0')","cd88ad03":"model2.analogy('Fran\u00e7a', 'franc\u00e8s', 'Pol\u00f2nia')","0607cd18":"**Zipf's law of words**. Zipf's law was originally formulated in terms of quantitative linguistics, stating that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table.","91f2bee9":"**Benford's law**, also called the Newcomb\u2013Benford law, the law of anomalous numbers, or the first-digit law, is an observation about the frequency distribution of leading digits in many real-life sets of numerical data.","bdaf78b0":"We will use the vocabulary computed by the Text Preprocessing notebook (text-preprocessing), and the word vectors computed by the CBOW Training notebook (cbow-vectors)"}}