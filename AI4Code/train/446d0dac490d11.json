{"cell_type":{"1f753bb1":"code","d4f596e8":"code","79fd14bb":"code","860feaa1":"code","45fdcf79":"code","ccd21c97":"code","5b0bc0a0":"code","c6b80986":"code","aad9f1e0":"code","3463024b":"code","afb379bd":"code","e9b45d02":"code","8833a750":"code","bd4335a9":"code","4ec726d4":"code","ca186ced":"code","2ca1de44":"code","d7ea41ff":"code","a75aa08e":"code","14db6a45":"code","d6c3008e":"code","8e19c793":"code","82ee8993":"code","a4519152":"code","dc8e8c9f":"code","ab150d18":"code","12e1a9b4":"code","0b673eee":"code","56a794bc":"code","ba66b3a8":"code","7a522fa5":"code","e1bf6fe2":"code","57a024f7":"code","e540b04f":"code","73a39df6":"code","1efd597f":"code","20cb58d3":"code","079ffad7":"code","a1de5006":"code","6928e0e3":"code","b5896ba2":"code","34810fd5":"code","b88c5325":"code","4d283b16":"code","bae214d2":"markdown","5a367b28":"markdown","8b70d24d":"markdown","80ed0cc2":"markdown","4be157d5":"markdown","c09572d3":"markdown","6fb4fe73":"markdown","7122dd7a":"markdown","94348e14":"markdown","a3da307d":"markdown","edc1fa87":"markdown","ae52bb6a":"markdown","7c6ae156":"markdown","eb8a0baa":"markdown","5e70884d":"markdown","aa338f42":"markdown","b99af7bd":"markdown","329e0149":"markdown","848ee1fa":"markdown","7e0c0839":"markdown","6b224bfc":"markdown","f1ea5ade":"markdown","d27a935c":"markdown","9e701275":"markdown","c2dd55fd":"markdown","04de7126":"markdown","9215d277":"markdown","b712aa94":"markdown","88e3e1fc":"markdown","0fdef77d":"markdown","24f303bd":"markdown","38462ed0":"markdown"},"source":{"1f753bb1":"# %%bash\n!pip -q install pytorch-pfn-extras\n!pip -q install timm\n!pip -q install albumentations==0.4.6\n!pip -q install opentsne\n!pip -q install faiss-cpu","d4f596e8":"# from requests import get\n# FILENAME = get('http:\/\/172.28.0.2:9000\/api\/sessions').json()[0]['name']\n# FILENAME = FILENAME[:-6]\n# FILENAME","79fd14bb":"# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n# secret_value_1 = user_secrets.get_secret(\"wandb_key\")\n# !pip -q install wandb\n\n# import wandb\n# api_key = secret_value_1\n# !wandb login $api_key\n# wandb.init(project='shiggle_1st', name=FILENAME)","860feaa1":"import os\nimport gc\nimport copy\nimport yaml\nimport random\nimport shutil\nimport glob\nimport typing as tp\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\nimport category_encoders as ce\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom torch.cuda import amp\nimport torch.nn.functional as F\n\nimport timm\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport pytorch_pfn_extras as ppe\nfrom pytorch_pfn_extras.config import Config\nfrom pytorch_pfn_extras.training import extensions as ppe_exts, triggers as ppe_triggers\n\nfrom openTSNE import TSNE\nimport faiss","45fdcf79":"pre_eval_cfg = yaml.safe_load(\n\"\"\"\nglobals:\n  seed: 1086\n  val_fold: null  # indicate when training\n  output_path: null # indicate when training\n  device: cuda\n  enable_amp: False\n  max_epoch: 20\n  cudnn_benchmark: True\n\nmodel:\n  type: BasicImageModel\n  dims_head: [null, 1]\n  base_name: tf_efficientnet_b0_ns\n  pretrained: True\n  in_channels: 4\n  fc_drop_rate: 0.0\n\ndataset:\n  height: 256\n  width: 256\n  mixup: {enabled: True, alpha: 1.0}\n  train:\n    type: BasicDataset\n    paths: null  # set by lazy_init\n    labels: null  # set by lazy_init\n    transform:\n      type: Compose\n      transforms:\n        - {type: Resize, p: 1.0, height: \"@\/dataset\/height\", width: \"@\/dataset\/width\"}\n        - {type: HorizontalFlip, p: 0.5}\n        # - {type: ShiftScaleRotate, p: 0.5, shift_limit: 0.2, scale_limit: 0.2,\n        #     rotate_limit: 20, border_mode: 0, value: 0, mask_value: 0}\n        - {type: RandomResizedCrop, p: 0.5, scale:[0.5, 1.0],\n            height: \"@\/dataset\/height\", width: \"@\/dataset\/width\"}\n        - {type: ToTensorV2, always_apply: True}\n  val:\n    type: BasicDataset\n    paths: null  # set by lazy_init\n    labels: null  # set by lazy_init\n    transform:\n      type: Compose\n      transforms:\n        - {type: Resize, p: 1.0, height: \"@\/dataset\/height\", width: \"@\/dataset\/width\"}\n        - {type: ToTensorV2, always_apply: True}  \n  test:\n    type: BasicDataset\n    paths: null  # set by lazy_init\n    labels: null  # set by lazy_init\n    transform: \"@\/dataset\/val\/transform\"\n\nloader:\n  train: {type: DataLoader, dataset: \"@\/dataset\/train\",\n    batch_size: 32, num_workers: 4, shuffle: True, pin_memory: True, drop_last: True}\n  val: {type: DataLoader, dataset: \"@\/dataset\/val\",\n    batch_size: 64, num_workers: 4, shuffle: False, pin_memory: True, drop_last: False}\n  test: {type: DataLoader, dataset: \"@\/dataset\/test\",\n    batch_size: 64, num_workers: 4, shuffle: False, pin_memory: True, drop_last: False}\n\noptimizer:\n  type: AdamW\n  params: {type: method_call, obj: \"@\/model\", method: parameters}\n  lr: 1.0e-03\n  weight_decay: 1.0e-02\n\nscheduler:\n  type: OneCycleLR\n  optimizer: \"@\/optimizer\"\n  epochs: \"@\/globals\/max_epoch\"\n  steps_per_epoch: {type: __len__, obj: \"@\/loader\/train\"}\n  max_lr: 1.0e-3\n  pct_start: 0.1\n  anneal_strategy: cos\n  div_factor: 1.0e+3\n  final_div_factor: 1.0e+3\n\n\nloss_type: {type: MSELoss}\nloss: \"@\/loss_type\"\n\neval:\n  - type: micro_average\n    metric_func: \"@\/loss_type\"\n    report_name: loss\n  - type: micro_average\n    metric_func: {type: RMSE}\n    report_name: metric\n\nmanager:\n  type: ExtensionsManager\n  models: \"@\/model\"\n  optimizers: \"@\/optimizer\"\n  max_epochs: \"@\/globals\/max_epoch\"\n  iters_per_epoch: {type: __len__, obj: \"@\/loader\/train\"}\n  out_dir: \"@\/globals\/output_path\"\n  # stop_trgiger: {type: EarlyStoppingTrigger,\n  #   monitor: val\/metric, mode: max, patience: 5, verbose: True,\n  #   check_trigger: [1, epoch], max_trigger: [\"@\/globals\/max_epoch\", epoch]}\n\nextensions:\n  # # log\n  - {type: observe_lr, optimizer: \"@\/optimizer\"}\n  - {type: LogReport}\n  - {type: PlotReport, y_keys: lr, x_key: epoch, filename: lr.png}\n  - {type: PlotReport, y_keys: [train\/loss, val\/loss], x_key: epoch, filename: loss.png}\n  - {type: PlotReport, y_keys: val\/metric, x_key: epoch, filename: metric.png}\n  - {type: PrintReport, entries: [\n      epoch, iteration, lr, train\/loss, val\/loss, val\/metric, elapsed_time]}\n  - {type: ProgressBarNotebook, update_interval: 20}\n#  - {type: SendWandB, entries: [\n#      lr, train\/loss, val\/loss, val\/metric, elapsed_time], \"fold_id\":None}\n\n  # snapshot\n  - extension: {type: snapshot, target: \"@\/model\", filename: \"snapshot_by_metric.pth\"}\n    trigger: {type: MinValueTrigger, key: \"val\/metric\", trigger: [1, epoch]}\n  # lr scheduler\n  - {type: LRScheduler, scheduler: \"@\/scheduler\", trigger: [1,  iteration]}\n\"\"\"\n)","ccd21c97":"# wandb.config.update(pre_eval_cfg)\n\n# try:\n#     wandb.config.update({'transforms':[c['type'] for c in pre_eval_cfg['dataset']['train']['transform']['transforms']]})\n# except:\n#     pass","5b0bc0a0":"ROOT = Path.cwd().parent\nINPUT = ROOT \/ \"input\"\nOUTPUT = ROOT \/ \"output\"\nDATA = INPUT \/ \"shigglecup-1st\/DATA\"\nTRAIN = DATA \/ \"pokemon_images\"\nTEST = DATA \/ \"pokemon_images\"\n\nTMP = ROOT \/ \"tmp\"\nTMP.mkdir(exist_ok=True)\n\nRANDAM_SEED = 1086\nCLASSES = [\"target\",]\nN_CLASSES = len(CLASSES)\nFOLDS = [0, 1, 2, 3, 4]\nN_FOLDS = len(FOLDS)\n\nDEBUG = True","c6b80986":"train = pd.read_csv(DATA \/ \"train.csv\")\ntest = pd.read_csv(DATA \/ \"test.csv\")\nsmpl_sub = pd.read_csv(DATA \/ \"sample_submission.csv\")","aad9f1e0":"CV = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDAM_SEED)\ntrain[\"fold\"] = -1\nfor fold_id, (tr_idx, val_idx) in enumerate(CV.split(train[\"id\"], train[\"target\"])):\n    train.loc[val_idx, \"fold\"] = fold_id","3463024b":"train.groupby(\"fold\").mean()['target']","afb379bd":"train[\"target\"] = np.log1p(train[\"target\"])","e9b45d02":"class BasicImageModel(nn.Module):\n    \n    def __init__(\n        self, base_name: str, dims_head: tp.List[int],\n        pretrained=False, in_channels: int=3,\n        drop_rate: float=0.0, drop_path_rate: float=0.0,\n        fc_drop_rate: float=0.5\n    ):\n        \"\"\"Initialize\"\"\"\n        self.base_name = base_name\n        super(BasicImageModel, self).__init__()\n        \n        # # prepare backbone\n        if hasattr(timm.models, base_name):\n            base_model = timm.create_model(\n                base_name, num_classes=0, pretrained=pretrained, in_chans=in_channels,\n                drop_rate=drop_rate, drop_path_rate=drop_path_rate)\n            in_features = base_model.num_features\n            print(\"load imagenet pretrained:\", pretrained)\n        else:\n            raise NotImplementedError\n\n        self.backbone = base_model\n        print(f\"{base_name}: {in_features}\")\n        \n        # # prepare head clasifier\n        if dims_head[0] is None:\n            dims_head[0] = in_features\n\n        layers_list = []\n        for i in range(len(dims_head) - 2):\n            in_dim, out_dim = dims_head[i: i + 2]\n            layers_list.extend([\n                nn.Linear(in_dim, out_dim),\n                nn.ReLU(), nn.Dropout(fc_drop_rate),])\n        layers_list.append(\n            nn.Linear(dims_head[-2], dims_head[-1]))\n        self.head_cls = nn.Sequential(*layers_list)\n\n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        h = self.backbone(x)\n        h = self.head_cls(h)\n        return h","8833a750":"FilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]\n\n\nclass BasicDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset using 6 channels by stacking them along time-axis\n\n    Attributes\n    ----------\n    paths : tp.Sequence[FilePath]\n        Sequence of path to cadence snippet file\n    labels : tp.Sequence[Label]\n        Sequence of label for cadence snippet file\n    transform: albumentations.Compose\n        composed data augmentations for data\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform: A.Compose,\n    ):\n        \"\"\"Initialize\"\"\"\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"Return num of cadence snippets\"\"\"\n        return len(self.paths)\n\n    def __getitem__(self, index: int):\n        \"\"\"Return transformed image and label for given index.\"\"\"\n        path, label = self.paths[index], self.labels[index]\n        img = self._read_cadence_array(path)\n        img = self.transform(image=img)[\"image\"]\n        return {\"image\": img, \"target\": label}\n\n    def _read_cadence_array(self, path: Path):\n        \"\"\"Read cadence file and reshape\"\"\"\n        try:\n            img = Image.open(DATA \/ f\"pokemon_images\/{path}\")\n        except:\n            img = Image.open(glob.glob(str(DATA \/ f\"pokemon_images\/{path[:-4]}-*.png\"))[0])\n        img = np.array(img)\n        img = img.astype(\"f\")\n        # img = img[...,:3]\n        return img\n\n    def lazy_init(self, paths=None, labels=None, transform=None):\n        \"\"\"Reset Members\"\"\"\n        if paths is not None:\n            self.paths = paths\n        if labels is not None:\n            self.labels = labels\n        if transform is not None:\n            self.transform = transform","bd4335a9":"Batch = tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]]\nModelOut = tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor], torch.Tensor]\n\n\nclass RMSE(nn.Module):\n    \"\"\"RMSE score\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize.\"\"\"\n        super(RMSE, self).__init__()\n\n    def forward(self, y, t) -> float:\n        \"\"\"Forward.\"\"\"\n        if isinstance(y, torch.Tensor):\n            y = y.detach().cpu().numpy()\n        if isinstance(t, torch.Tensor):\n            t = t.detach().cpu().numpy()\n\n        return mean_squared_error(t, y, squared=False)\n\ndef micro_average(\n    metric_func: nn.Module,\n    report_name: str, prefix=\"val\",\n    pred_index: int=-1, label_index: int=-1,\n    pred_key: str=\"logit\", label_key: str=\"target\",\n) -> tp.Callable:\n    \"\"\"Return Metric Wrapper for Simple Mean Metric\"\"\"\n    metric_sum = [0.]\n    n_examples = [0]\n    \n    def wrapper(batch: Batch, model_output: ModelOut, is_last_batch: bool):\n        \"\"\"Wrapping metric function for evaluation\"\"\"\n        if isinstance(batch, tuple): \n            t = batch[label_index]\n        elif isinstance(batch, dict):\n            t = batch[label_key]\n        else:\n            raise NotImplementedError\n\n        if isinstance(model_output, tuple):\n            y = model_output[pred_index]\n        elif isinstance(model_output, dict):\n            y = model_output[pred_key]\n        else:\n            y = model_output\n\n        metric = metric_func(y, t).item()\n        metric_sum[0] += metric * y.shape[0]\n        n_examples[0] += y.shape[0]\n\n        if is_last_batch:\n            final_metric = metric_sum[0] \/ n_examples[0]\n            ppe.reporting.report({f\"{prefix}\/{report_name}\": final_metric})\n            # # reset state\n            metric_sum[0] = 0.\n            n_examples[0] = 0\n\n    return wrapper\n\n\ndef calc_across_all_batchs(\n    metric_func: nn.Module,\n    report_name: str, prefix=\"val\",\n    pred_index: int=-1, label_index: int=-1,\n    pred_key: str=\"logit\", label_key: str=\"target\",\n) -> tp.Callable:\n    \"\"\"\n    Return Metric Wrapper for Metrics caluculated on all data\n    \n    storing predictions and labels of evry batch, finally calculating metric on them.\n    \"\"\"\n    pred_list = []\n    label_list = []\n    \n    def wrapper(batch: Batch, model_output: ModelOut, is_last_batch: bool):\n        \"\"\"Wrapping metric function for evaluation\"\"\"\n        if isinstance(batch, tuple):\n            t = batch[label_index]\n        elif isinstance(batch, dict):\n            t = batch[label_key]\n        else:\n            raise NotImplementedError\n\n        if isinstance(model_output, tuple):\n            y = model_output[pred_index]\n        elif isinstance(model_output, dict):\n            y = model_output[pred_key]\n        else:\n            y = model_output\n\n        pred_list.append(y.numpy())\n        label_list.append(t.numpy())\n\n        if is_last_batch:\n            pred = np.concatenate(pred_list, axis=0)\n            label = np.concatenate(label_list, axis=0)\n            final_metric = metric_func(pred, label)\n            ppe.reporting.report({f\"{prefix}\/{report_name}\": final_metric})\n            # # reset state\n            pred_list[:] = []\n            label_list[:] = []\n\n    return wrapper","4ec726d4":"# class SendWandB(ppe.training.extensions.PrintReport):\n# #     def __init__(self, entries=None, log_report='LogReport', wandb=None):\n#     def __init__(self, entries=None, log_report='LogReport', wandb=wandb, fold_id=None):\n#         super().__init__(entries, log_report, None)\n#         self.wandb = wandb\n#         self.entries = entries\n#         self.fold_id = fold_id\n\n#     def __call__(self, manager):\n#         log_report = self.get_log_report(manager)\n#         log = log_report.log\n#         log_len = self._log_len\n#         while len(log) > log_len:\n#             # self.wandb.log(log[log_len], step=log_len)\n#             if str(self.fold_id) != 'None':\n#                 send_log = {}\n#                 for k,v in log[log_len].items():\n#                     if k in self.entries:\n#                         send_log[f'{k}\/fold_{self.fold_id}'] = v\n#                     elif k == 'epoch':\n#                         send_log[k] = v\n#                 self.wandb.log(send_log)\n#             else:\n#                 self.wandb.log(log[log_len])\n#             log_len += 1\n#         self._log_len = log_len","ca186ced":"CONFIG_TYPES = {\n    # # utils\n    \"__len__\": lambda obj: len(obj),\n    \"method_call\": lambda obj, method: getattr(obj, method)(),\n\n    # # Dataset, DataLoader\n    \"BasicDataset\": BasicDataset,\n    \"DataLoader\": torch.utils.data.DataLoader,\n\n    # # Data Augmentation\n    \"Compose\": A.Compose, \"OneOf\": A.OneOf,\n    \"Resize\": A.Resize,\n    \"HorizontalFlip\": A.HorizontalFlip, \"VerticalFlip\": A.VerticalFlip,\n    \"ShiftScaleRotate\": A.ShiftScaleRotate,\n    \"RandomResizedCrop\": A.RandomResizedCrop,\n    \"Cutout\": A.Cutout,\n    \"MotionBlur\": A.MotionBlur,\n    \"RandomBrightnessContrast\": A.RandomBrightnessContrast,\n    \"ToTensorV2\": ToTensorV2,\n\n    # # Model\n    \"BasicImageModel\": BasicImageModel,\n\n    # # Optimizer\n    \"AdamW\": optim.AdamW,\n\n    # # Scheduler\n    \"OneCycleLR\": lr_scheduler.OneCycleLR,\n\n    # # Loss,Metric\n    \"MSELoss\": nn.MSELoss,\n    \"RMSE\": RMSE,\n\n    # # Metric Wrapper\n    \"micro_average\": micro_average,\n    \"calc_across_all_batchs\": calc_across_all_batchs,\n\n    # # PPE Extensions\n    \"ExtensionsManager\": ppe.training.ExtensionsManager,\n\n    \"observe_lr\": ppe_exts.observe_lr,\n    \"LogReport\": ppe_exts.LogReport,\n    \"PlotReport\": ppe_exts.PlotReport,\n    \"PrintReport\": ppe_exts.PrintReport,\n    \"PrintReportNotebook\": ppe_exts.PrintReportNotebook,\n    \"ProgressBar\": ppe_exts.ProgressBar,\n    \"ProgressBarNotebook\": ppe_exts.ProgressBarNotebook,\n    \"snapshot\": ppe_exts.snapshot,\n    \"LRScheduler\": ppe_exts.LRScheduler, \n#     \"SendWandB\":SendWandB,\n\n    \"MinValueTrigger\": ppe_triggers.MinValueTrigger,\n    \"MaxValueTrigger\": ppe_triggers.MaxValueTrigger,\n    \"EarlyStoppingTrigger\": ppe_triggers.EarlyStoppingTrigger,\n}","2ca1de44":"def set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n\n\ndef to_device(\n    tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]],\n    device: torch.device, *args, **kwargs\n):\n    if isinstance(tensors, tuple):\n        return (t.to(device, *args, **kwargs) for t in tensors)\n    elif isinstance(tensors, dict):\n        return {\n            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n    else:\n        return tensors.to(device, *args, **kwargs)","d7ea41ff":"def get_path_label(cfg: Config, train_all: pd.DataFrame):\n    \"\"\"Get file path and target info.\"\"\"\n    use_fold = cfg[\"\/globals\/val_fold\"]\n\n    train_df = train_all[train_all[\"fold\"] != use_fold]\n    val_df = train_all[train_all[\"fold\"] == use_fold]\n\n    # \u753b\u50cf\u304c\u306a\u3044\u30c7\u30fc\u30bf\u306f\u9664\u5916\n    train_df = train_df[train_df[\"image_exist\"]==1]\n    val_df = val_df[val_df[\"image_exist\"]==1]\n    \n    train_path_label = {\n        \"paths\": [img_id for img_id in train_df[\"url_image\"].values],\n        \"labels\": train_df[CLASSES].values.astype(\"f\")}\n    val_path_label = {\n        \"paths\": [img_id for img_id in val_df[\"url_image\"].values],\n        \"labels\": val_df[CLASSES].values.astype(\"f\")\n    }\n    return train_path_label, val_path_label\n\n\ndef get_eval_func(cfg, model, device):\n    \n    def eval_func(**batch):\n        \"\"\"Run evaliation for val or test. This function is applied to each batch.\"\"\"\n        batch = to_device(batch, device)\n        x = batch[\"image\"]\n        with amp.autocast(cfg[\"\/globals\/enable_amp\"]): \n            y = model(x)\n        return y.detach().cpu().to(torch.float32)  # input of metrics\n\n    return eval_func\n\n\ndef mixup_data(use_mixup, x, t, alpha=1.0, use_cuda=True):\n    '''Returns mixed inputs, pairs of targets, and lambda'''\n    if not use_mixup:\n        return x, t, None, None\n    \n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    t_a, t_b = t, t[index]\n    return mixed_x, t_a, t_b, lam\n\n\ndef get_criterion(use_mixup, loss_func):\n\n    def mixup_criterion(pred, t_a, t_b, lam):\n        return lam * loss_func(pred, t_a) + (1 - lam) * loss_func(pred, t_b)\n\n    def single_criterion(pred, t_a, t_b, lam):\n        return loss_func(pred, t_a)\n    \n    if use_mixup:\n        return mixup_criterion\n    else:\n        return single_criterion","a75aa08e":"def train_one_fold(cfg, train_all):\n    \"\"\"Main\"\"\"\n    torch.backends.cudnn.benchmark = True\n    set_random_seed(cfg[\"\/globals\/seed\"], deterministic=True)\n    device = torch.device(cfg[\"\/globals\/device\"])\n    \n    train_path_label, val_path_label = get_path_label(cfg, train_all)\n    print(\"train: {}, val: {}\".format(len(train_path_label[\"paths\"]), len(val_path_label[\"paths\"])))\n   \n    cfg[\"\/dataset\/train\"].lazy_init(**train_path_label)\n    cfg[\"\/dataset\/val\"].lazy_init(**val_path_label)\n    train_loader = cfg[\"\/loader\/train\"]\n    val_loader = cfg[\"\/loader\/val\"]\n\n    model = cfg[\"\/model\"]\n    model.to(device)\n    optimizer = cfg[\"\/optimizer\"]\n    loss_func = cfg[\"\/loss\"]\n    loss_func.to(device)\n    \n    manager = cfg[\"\/manager\"]\n    for ext in cfg[\"\/extensions\"]:\n        if isinstance(ext, dict):\n            manager.extend(**ext)\n        else:\n            manager.extend(ext)\n\n    evaluator = ppe_exts.Evaluator(\n        val_loader, model, eval_func=get_eval_func(cfg, model, device),\n        metrics=cfg[\"\/eval\"], progress_bar=False)\n    manager.extend(evaluator, trigger=(1, \"epoch\"))\n\n    use_amp = cfg[\"\/globals\/enable_amp\"]\n    scaler = amp.GradScaler(enabled=use_amp)\n    use_mixup = cfg[\"\/dataset\/mixup\/enabled\"]\n    mixup_alpha = cfg[\"\/dataset\/mixup\/alpha\"]\n    \n    while not manager.stop_trigger:\n        model.train()\n        for batch in train_loader:\n            with manager.run_iteration():\n                batch = to_device(batch, device)\n                x, t = batch[\"image\"], batch[\"target\"]\n                # # for mixup\n                mixed_x, t_a, t_b, lam = mixup_data(use_mixup, x, t, mixup_alpha)\n                criterion = get_criterion(use_mixup, loss_func)\n                \n                optimizer.zero_grad()\n                with amp.autocast(use_amp):\n                    y = model(mixed_x)\n                    loss = criterion(y, t_a, t_b, lam)\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                \n                ppe.reporting.report({'train\/loss': loss.item()})","14db6a45":"def get_fold_config(cfg, fold_id):\n    try:\n        cfg_ = cfg.copy()\n        for i, c in enumerate(cfg_['extensions']):\n            if c['type']=='SendWandB':\n                cfg_['extensions'][i]['fold_id'] = fold_id\n                return cfg_\n    except:\n        return cfg","d6c3008e":"pre_eval_cfg_list = []\nfor fold_id in FOLDS:\n    tmp_cfg = copy.deepcopy(get_fold_config(pre_eval_cfg, fold_id))\n    tmp_cfg[\"globals\"][\"val_fold\"] = fold_id\n    tmp_cfg[\"globals\"][\"output_path\"] = str(TMP \/ f\"fold{fold_id}\")\n    pre_eval_cfg_list.append(tmp_cfg)","8e19c793":"if DEBUG:\n    for pre_eval_cfg in pre_eval_cfg_list[:1]:\n        cfg = Config(pre_eval_cfg, types=CONFIG_TYPES)\n        print(f\"\\nfold:\", cfg[\"\/globals\/val_fold\"])\n        train_one_fold(cfg, train)\n        del cfg\n        torch.cuda.empty_cache()\n        gc.collect()\nelse:\n    for pre_eval_cfg in pre_eval_cfg_list:\n        cfg = Config(pre_eval_cfg, types=CONFIG_TYPES)\n        print(f\"\\nfold:\", cfg[\"\/globals\/val_fold\"])\n        train_one_fold(cfg, train)\n        del cfg\n        torch.cuda.empty_cache()\n        gc.collect()","82ee8993":"best_log_list = []\nfor pre_eval_cfg, fold_id in zip(pre_eval_cfg_list, FOLDS):\n    if DEBUG&(fold_id!=0):\n        pass\n    else:\n        exp_dir_path = TMP \/ f\"fold{fold_id}\"\n        log = pd.read_json(exp_dir_path \/ \"log\")\n        best_log = log.iloc[[log[\"val\/metric\"].idxmin()],]\n        best_epoch = best_log.epoch.values[0]\n        best_log_list.append(best_log)\n\n        # best_model_path = exp_dir_path \/ f\"snapshot_by_metric_epoch_{best_epoch}.pth\"\n        best_model_path = exp_dir_path \/ f\"snapshot_by_metric.pth\"\n        copy_to = f\"{ROOT}\/best_metric_model_fold{fold_id}.pth\"\n        shutil.copy(best_model_path, copy_to)\n\n        for p in exp_dir_path.glob(\"*.pth\"):\n            p.unlink()\n\n        shutil.copytree(exp_dir_path, f\"{ROOT}\/fold{fold_id}\")\n\n        with open(f\"{ROOT}\/fold{fold_id}\/config.yml\", \"w\") as fw:\n            yaml.dump(pre_eval_cfg, fw)\n    \npd.concat(best_log_list, axis=0, ignore_index=True)","a4519152":"def run_inference_loop(cfg, model, loader, device):\n    model.to(device)\n    model.eval()\n    pred_list = []\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            x = to_device(batch[\"image\"], device)\n            y = model(x)\n            pred_list.append(y.detach().cpu().numpy())\n        \n    pred_arr = np.concatenate(pred_list)\n    del pred_list\n    return pred_arr","dc8e8c9f":"if not DEBUG:\n    label_arr = train[CLASSES].values\n    oof_pred_arr = np.zeros((len(train), N_CLASSES))\n    score_list = []\n    test_pred_arr = np.zeros((N_FOLDS, len(smpl_sub), N_CLASSES))\n\n    test_exist_idx = test[\"image_exist\"]==1\n    test_path_label = {\n        \"paths\": [img_id for img_id in test[\"url_image\"][test_exist_idx].values],\n        \"labels\": smpl_sub[CLASSES][test_exist_idx].values.astype(\"f\")\n    }\n\n    for fold_id in range(N_FOLDS):\n        print(f\"[fold {fold_id}]\")\n        tmp_dir = Path(f\"{ROOT}\/fold{fold_id}\")\n        with open(tmp_dir \/ \"config.yml\", \"r\") as fr:\n            cfg = Config(yaml.safe_load(fr), types=CONFIG_TYPES)\n        device = torch.device(cfg[\"\/globals\/device\"])\n        val_idx = train.query(\"fold == @fold_id\").index.values\n\n        # # get_dataloader\n        _, val_path_label = get_path_label(cfg, train)\n        cfg[\"\/dataset\/val\"].lazy_init(**val_path_label)\n        cfg[\"\/dataset\/test\"].lazy_init(**test_path_label)\n        val_loader = cfg[\"\/loader\/val\"]\n        test_loader = cfg[\"\/loader\/test\"]\n\n        # # get model\n        model_path = f\"{ROOT}\/best_metric_model_fold{fold_id}.pth\"\n        model = cfg[\"\/model\"]\n        model.load_state_dict(torch.load(model_path, map_location=device))\n\n        # # inference\n        train_df = train[train[\"fold\"] != fold_id]\n        val_df = train[train[\"fold\"] == fold_id]\n        train_mean = train_df[CLASSES].mean()\n        val_exist_idx = val_df[\"image_exist\"]==1\n\n        val_pred = run_inference_loop(cfg, model, val_loader, device)\n        oof_pred_arr[val_idx[val_exist_idx]] = val_pred\n        oof_pred_arr[val_idx[~val_exist_idx]] = train_mean\n        val_score = mean_squared_error(label_arr[val_idx], oof_pred_arr[val_idx], squared=False)\n        score_list.append([fold_id, val_score])\n\n        test_pred = run_inference_loop(cfg, model, test_loader, device)\n        test_pred_arr[fold_id][test_exist_idx] = test_pred\n        test_pred_arr[fold_id][~test_exist_idx] = train[CLASSES].mean()\n\n        del cfg, val_idx, val_path_label\n        del model, val_loader, test_loader\n        torch.cuda.empty_cache()\n        gc.collect()\n\n        print(f\"val score: {val_score:.4f}\")","ab150d18":"if not DEBUG:\n    oof_score = mean_squared_error(label_arr, oof_pred_arr, squared=False)\n    score_list.append([\"oof\", oof_score])\n    pd.DataFrame(score_list, columns=[\"fold\", \"metric\"])","12e1a9b4":"if not DEBUG:\n    oof_df = train.copy()\n    oof_df[CLASSES] = oof_pred_arr\n    oof_df.to_csv(f\"{ROOT}\/oof_prediction.csv\", index=False)","0b673eee":"def revert_to_real(y_log):\n    _pred = np.expm1(y_log)\n    _pred = np.where(_pred < 0, 0, _pred)\n    return _pred","56a794bc":"if not DEBUG:\n    sub_df = smpl_sub.copy()\n    sub_df[CLASSES] = revert_to_real(test_pred_arr.mean(axis=0))\n    sub_df.to_csv(f\"{ROOT}\/submission_0_{str(oof_score)[2:7]}_{FILENAME}.csv\", index=False)","ba66b3a8":"model = timm.create_model('tf_efficientnet_b0_ns', num_classes=0, pretrained=False)\nmodel.feature_info","7a522fa5":"class FeatureExtModel(nn.Module):\n    \n    def __init__(\n        self, base_model: torch.nn, base_name: str, in_channels: int=3, out_indices: tuple=(2,4)\n    ):\n        \"\"\"Initialize\"\"\"\n        self.base_name = base_name\n        super(FeatureExtModel, self).__init__()\n        \n        # # prepare backbone\n        if hasattr(timm.models, base_name):\n            # \u6307\u5b9a\u3057\u305f\u30d6\u30ed\u30c3\u30af\u304b\u3089\u51fa\u529b\u3092\u5f97\u308b\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\n            feature_ext_model = timm.create_model(\n                base_name, num_classes=0, pretrained=False, in_chans=in_channels,\n                features_only=True, out_indices=out_indices)\n        else:\n            raise NotImplementedError\n\n        # \u4e8b\u524d\u306b\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u304b\u3089\u91cd\u307f\u3092\u53d6\u5f97\n        feature_ext_model.load_state_dict(base_model.backbone.state_dict(), strict=False)\n        self.feature_ext_model = feature_ext_model\n\n        self.cat_flag = len(out_indices)>1\n\n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        h = self.feature_ext_model(x)\n        if self.cat_flag:\n            for i, vec in enumerate(h):\n                h[i] = torch.nn.AdaptiveAvgPool2d(1)(h[i])\n                h[i] = h[i][:, :, 0, 0]\n            h = torch.cat(h, dim=1)\n        else:\n            h = torch.nn.AdaptiveAvgPool2d(1)(h[0])\n            h = h[:, :, 0, 0]\n        return h","e1bf6fe2":"def get_embedding(fold_id: int, out_indices: tuple=(2,4)):\n    tmp_dir = Path(f\"{ROOT}\/fold{fold_id}\")\n    with open(tmp_dir \/ \"config.yml\", \"r\") as fr:\n        cfg = Config(yaml.safe_load(fr), types=CONFIG_TYPES)\n    device = torch.device(cfg[\"\/globals\/device\"])\n\n    model_path = f\"{ROOT}\/best_metric_model_fold{fold_id}.pth\"\n    model = cfg[\"\/model\"]\n    model.load_state_dict(torch.load(model_path, map_location=device))\n\n    feature_ext_model = FeatureExtModel(base_model=model, base_name=cfg[\"\/model\/base_name\"],\n                                    in_channels=cfg[\"\/model\/in_channels\"], out_indices=out_indices)\n    \n    train_path_label, val_path_label = get_path_label(cfg, train)\n    cfg[\"\/dataset\/test\"].lazy_init(**train_path_label)\n    loader = cfg[\"\/loader\/test\"]\n    train_pred = run_inference_loop(cfg, feature_ext_model, loader, device)\n\n    cfg[\"\/dataset\/test\"].lazy_init(**val_path_label)\n    loader = cfg[\"\/loader\/test\"]\n    val_pred = run_inference_loop(cfg, feature_ext_model, loader, device)\n\n    # all_pred = np.concatenate([train_pred, val_pred])\n    return train_pred, val_pred, train_path_label, val_path_label","57a024f7":"train_pred, val_pred, train_path_label, val_path_label = get_embedding(0, (4, ))\nall_pred = np.concatenate([train_pred, val_pred])\nall_pred.shape","e540b04f":"embedding = TSNE().fit(all_pred)\nplt.figure(figsize=(10,10))\nplt.scatter(embedding[:len(train_pred), 0], embedding[:len(train_pred), 1], c=train_path_label['labels'], cmap='coolwarm')\nplt.scatter(embedding[len(train_pred):, 0], embedding[len(train_pred):, 1], c=val_path_label['labels'], cmap='coolwarm', marker='*', s=100)","73a39df6":"train_pred, val_pred, train_path_label, val_path_label = get_embedding(0, (2, 4, ))\nall_pred = np.concatenate([train_pred, val_pred])\nprint(all_pred.shape)","1efd597f":"embedding = TSNE().fit(all_pred)\nplt.figure(figsize=(10,10))\nplt.scatter(embedding[:len(train_pred), 0], embedding[:len(train_pred), 1], c=train_path_label['labels'], cmap='coolwarm')\nplt.scatter(embedding[len(train_pred):, 0], embedding[len(train_pred):, 1], c=val_path_label['labels'], cmap='coolwarm', marker='*', s=100)","20cb58d3":"class FaissKNeighbors:\n    def __init__(self, k=5):\n        self.index = None\n        self.d = None\n        self.k = k\n\n    def fit(self, X):\n        X = X.copy(order='C')\n        self.d = X.shape[1]\n        self.index = faiss.IndexFlatL2(self.d)\n        self.index.add(X.astype(np.float32))\n\n    def predict(self, X):\n        X = X.copy(order='C')\n        X = np.reshape(X, (-1, self.d))\n        distances, indices = self.index.search(X.astype(np.float32), k=self.k)\n        return indices[0]","079ffad7":"train_pred, val_pred, train_path_label, val_path_label = get_embedding(0, (4, ))","a1de5006":"kn = FaissKNeighbors(k=5)\nkn.fit(train_pred)","6928e0e3":"idx = kn.predict(train_pred[0])\nfor i in idx:\n    path = train_path_label['paths'][i]\n    row = train[train['url_image']==path]\n    try:\n        img = Image.open(DATA \/ f\"pokemon_images\/{path}\")\n    except:\n        img = Image.open(glob.glob(str(DATA \/ f\"pokemon_images\/{path[:-4]}-*.png\"))[0])\n    img = np.array(img)\n    # img = img.astype(\"f\")\n    plt.imshow(img)\n    plt.title(f\"{row['pokemon'].values}  target = {np.expm1(row['target'].values)}\")\n    plt.show()","b5896ba2":"idx = kn.predict(train_pred[400])\nfor i in idx:\n    path = train_path_label['paths'][i]\n    row = train[train['url_image']==path]\n    try:\n        img = Image.open(DATA \/ f\"pokemon_images\/{path}\")\n    except:\n        img = Image.open(glob.glob(str(DATA \/ f\"pokemon_images\/{path[:-4]}-*.png\"))[0])\n    img = np.array(img)\n    # img = img.astype(\"f\")\n    plt.imshow(img)\n    plt.title(f\"{row['pokemon'].values}  target = {np.expm1(row['target'].values)}\")\n    plt.show()","34810fd5":"kn = FaissKNeighbors(k=5)\nkn.fit(val_pred)","b88c5325":"idx = kn.predict(val_pred[0])\nfor i in idx:\n    path = val_path_label['paths'][i]\n    row = train[train['url_image']==path]\n    try:\n        img = Image.open(DATA \/ f\"pokemon_images\/{path}\")\n    except:\n        img = Image.open(glob.glob(str(DATA \/ f\"pokemon_images\/{path[:-4]}-*.png\"))[0])\n    img = np.array(img)\n    # img = img.astype(\"f\")\n    plt.imshow(img)\n    plt.title(f\"{row['pokemon'].values}  target = {np.expm1(row['target'].values)}\")\n    plt.show()","4d283b16":"idx = kn.predict(val_pred[50])\nfor i in idx:\n    path = val_path_label['paths'][i]\n    row = train[train['url_image']==path]\n    try:\n        img = Image.open(DATA \/ f\"pokemon_images\/{path}\")\n    except:\n        img = Image.open(glob.glob(str(DATA \/ f\"pokemon_images\/{path[:-4]}-*.png\"))[0])\n    img = np.array(img)\n    # img = img.astype(\"f\")\n    plt.imshow(img)\n    plt.title(f\"{row['pokemon'].values}  target = {np.expm1(row['target'].values)}\")\n    plt.show()","bae214d2":"\u307e\u305a\u3001\u6307\u5b9a\u53ef\u80fd\u306a\u30d6\u30ed\u30c3\u30af\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002  \n`.feature_info`\u3067\u78ba\u8a8d\u3067\u304d\u307e\u3059\uff08\u4e00\u90e8\u30e2\u30c7\u30eb\u3092\u9664\u304f\uff09\u3002tf_efficientnet_b0_ns\u306e\u5834\u5408\u30015\u30d6\u30ed\u30c3\u30af\u304b\u3089\u53d6\u5f97\u304c\u3067\u304d\u305d\u3046\u3067\u3059\u3002","5a367b28":"`timm.create_model`\u6642\u306b\u5f15\u6570'features_only=True'\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u5168\u30d6\u30ed\u30c3\u30af\u306e\u51fa\u529b\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002  \n\u307e\u305f\u3001\u5f15\u6570`out_indices`\u3067\u30d6\u30ed\u30c3\u30af\u306e\u6307\u5b9a\u3082\u3067\u304d\u307e\u3059\u3002  ","8b70d24d":"## Read Data, Split folds","80ed0cc2":"### Metric","4be157d5":"## \u6982\u8981\n\n[\u3053\u306eDiscussion](https:\/\/www.kaggle.com\/c\/shigglecup-1st\/discussion\/267876)\u306b\u3042\u308b\u3088\u3046\u306b\u3001\u30dd\u30b1\u30e2\u30f3\u306e\u5916\u89b3\u3092\u3069\u3046\u306b\u304b\u3057\u3066\u7279\u5fb4\u5316\u3057\u305f\u3044  \n\u2192\u753b\u50cf\u304b\u3089\u7d4c\u9a13\u5024\u3092\u4e88\u6e2c\u3059\u308b\u30bf\u30b9\u30af\u3092CNN\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2\u3057\u3001\u4e2d\u9593\u5c64\u306eEmbedding\u3092\u62bd\u51fa\u3057\u3066\u307f\u308b  \n  \n\u203b\u5c0f\u751f\u306e\u30c8\u30e9\u30a4\u7d50\u679c\u3067\u306f\u3001[Host\u306eBaseline](https:\/\/www.kaggle.com\/shimishige\/host-baseline)\u306bEmbedding\u3092\u8ffd\u52a0\u3057\u3066\u307f\u3066\u3082\u7cbe\u5ea6\u5411\u4e0a\u3057\u307e\u305b\u3093\u3067\u3057\u305f\uff08\u6ce3\uff09  \n\u5c0f\u751f\u306e\u5b66\u7fd2\u65b9\u6cd5\u304c\u30a4\u30b1\u3066\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b\u305f\u3081\u3001Embedding\u62bd\u51fa\u90e8\u3060\u3051\u3067\u3082\u53c2\u8003\u306b\u306a\u308c\u3070\u5e78\u3044\u3067\u3059\u3002\n  \n---\n\u672cnotebook\u306e\u5927\u534a\u306fEmbedding\u7528\u306e\u30e2\u30c7\u30eb\u5b66\u7fd2\u3067\u3042\u308b\u305f\u3081\u3001Embedding\u62bd\u51fa\u90e8\u3092\u77e5\u308a\u305f\u3044\u65b9\u306f  \n[\u4e2d\u9593\u5c64\u306eEmbedding\u53d6\u5f97](#\u4e2d\u9593\u5c64\u306eEmbedding\u53d6\u5f97)\u3078\u30b9\u30ad\u30c3\u30d7\u3057\u3066\u304f\u3060\u3055\u3044\u3002","c09572d3":"### Faiss\u3067\u985e\u4f3c\u753b\u50cf\u691c\u7d22","6fb4fe73":"# Model\u5b66\u7fd2  \n\u30ed\u30b0\u76e3\u8996\u306a\u3069\u306e\u7c21\u7565\u5316\u306e\u305f\u3081\u3001\u5b66\u7fd2\u7ba1\u7406\u306b\u306f`pytorch-pfn-extras`\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002  \n\u4e00\u898b\u3068\u3063\u3064\u304d\u306b\u304f\u3044\u3067\u3059\u304c\u3001tawara\u3055\u3093\u304c\u795e\u6559\u6750\uff08[notebook](https:\/\/www.kaggle.com\/ttahara\/rerun-seti-e-t-resnet18d-baseline)\u3001[pdf](https:\/\/www.slideshare.net\/TakujiTahara\/20210618-lt-pyrotch-pfn-extras-and-config-systems-tawara)\uff09\u3092\u516c\u958b\u3057\u3066\u304f\u3060\u3055\u3063\u3066\u3044\u308b\u305f\u3081\u3001\u3053\u306e\u6a5f\u306b\u5165\u9580\u3057\u3066\u307f\u3066\u3082\u3044\u3044\u304b\u3082\uff1f","7122dd7a":"3\u30d6\u30ed\u30c3\u30af\u76ee\u3001\u6700\u7d42\u30d6\u30ed\u30c3\u30af\u306e\u51fa\u529b\u3092\u53d6\u5f97\u3057\u3066\u307f\u307e\u3059","94348e14":"\u6307\u5b9a\u3057\u305flayer\uff08\u30d6\u30ed\u30c3\u30af\uff09\u306e\u51fa\u529b\u3092\u53d6\u5f97\u3059\u308b\u51e6\u7406\u3092\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3059\u3002  \n\u5358\u7d14\u306bbackborn\u306e\u6700\u7d42\u51fa\u529b\u3092\u5f97\u308b\u3060\u3051\u3067\u3082\u826f\u3044\u3067\u3059\u304c\u3001\u591a\u69d8\u306a\u7279\u5fb4\u53d6\u5f97\u3092\u76ee\u6307\u3057\u3066\u4e2d\u9593\u30d6\u30ed\u30c3\u30af\u306e\u51fa\u529b\u3082\u53d6\u5f97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002pytorch-image-models(timm)\u3092\u6d3b\u7528\u3059\u308b\u3068\u6bd4\u8f03\u7684\u7c21\u5358\u306b\u5b9f\u88c5\u3067\u304d\u307e\u3059\u3002\n[\u53c2\u8003\u8cc7\u6599](https:\/\/www.slideshare.net\/TakujiTahara\/20210817-lt-introduction-to-pytorch-image-models-as-backbone-tawara-249996209)","a3da307d":"## config_types for evaluating configuration\n\nI use [pytorch-pfn-extras](https:\/\/github.com\/pfnet\/pytorch-pfn-extras) for training NNs. This library has useful config systems but requires some preparation.\n\nFor more details, see [docs](https:\/\/github.com\/pfnet\/pytorch-pfn-extras\/blob\/master\/docs\/config.md).","edc1fa87":"\u4e0b\u8a18FeatureExtModel\u30af\u30e9\u30b9\u306b\u3066\u3001\u4e8b\u524d\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u304b\u3089\u6307\u5b9a\u30d6\u30ed\u30c3\u30af\u306e\u51fa\u529b\u3092\u53d6\u5f97\u3059\u308b\u51e6\u7406\u3092\u5b9f\u88c5\u3057\u3066\u307e\u3059\u3002","ae52bb6a":"## Make submission","7c6ae156":"# Train","eb8a0baa":"## Import","5e70884d":"\u6700\u7d42\u30d6\u30ed\u30c3\u30af\u306e\u51fa\u529b\u3092\u53d6\u5f97\u3057\u3066\u307f\u307e\u3059","aa338f42":"TSNE\u30672\u6b21\u5143\u53ef\u8996\u5316\u3057\u3066\u307f\u307e\u3059\uff08Train:\u25cf, Valid:\u2605\u3001\u8272\uff1atarget\uff09","b99af7bd":"\u6240\u611f\uff1aTarget\u304c\u6df7\u5408\u3057\u3066\u307e\u3059\u306d\u3002\u9811\u5f35\u3063\u3066\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u305f\u304c\u3001\u6700\u7d42\u5c64\u3060\u3051\u3067\u826f\u3055\u305d\u3046\u3067\u3059\u306d...\uff08\u6ce3\uff09","329e0149":"- \u5b66\u7fd2\u6761\u4ef6\n    - Model:tf_efficientnet_b0_ns\n    - Epoch:20\n    - CV:5fold","848ee1fa":"## Copy best models","7e0c0839":"## Prapere","6b224bfc":"# Inference","f1ea5ade":"## Log Setting","d27a935c":"### Dataset","9e701275":"## run train","c2dd55fd":"\u6240\u611f\uff1a\u82e5\u5e72\u6df7\u3056\u3063\u3066\u307e\u3059\u304c\u3001Target\u306e\u9ad8\u4f4e\u3067\u7279\u5fb4\u304c\u5f97\u3089\u308c\u3066\u3044\u305d\u3046\u3067\u3059","04de7126":"### Model","9215d277":"# \u4e2d\u9593\u5c64\u306eEmbedding\u53d6\u5f97","b712aa94":"## configration","88e3e1fc":"## Definition of Model, Dataset, Metric","0fdef77d":"\u6240\u611f\uff1aCNN\u306e\u7cbe\u5ea6\u304c\u3044\u307e\u3044\u3061\u306a\u306e\u304c\u5206\u304b\u308a\u307e\u3059\u306d...","24f303bd":"## Inference OOF & Test","38462ed0":"## functions for training"}}