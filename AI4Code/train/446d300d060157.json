{"cell_type":{"7ad077bc":"code","f69882d9":"code","26bd8e02":"code","686cda61":"code","205632f0":"code","f08078fe":"code","3d42b26b":"code","0c47f720":"code","63e1183f":"code","cfef6b0f":"code","07589db5":"code","e9f0a68a":"code","545cb370":"code","d71c0858":"code","6f3e1031":"code","17f925b1":"code","eb4cbb06":"code","b0f94448":"code","6a474499":"code","73e7dc76":"code","8b03f7d8":"code","8647b49f":"markdown","49d5336d":"markdown","6c9d34f1":"markdown","97393eeb":"markdown","91e951c7":"markdown","a25d7b9a":"markdown","0dd59c97":"markdown","089c63a7":"markdown","103ed88b":"markdown","6960e3e2":"markdown","cb49e676":"markdown","9b7031c6":"markdown"},"source":{"7ad077bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f69882d9":"#1. Reading Data\n\ndf_test=pd.read_csv(\"..\/input\/sanbercode\/test.csv\")\ndf_train=pd.read_csv(\"..\/input\/sanbercode\/train.csv\")\n","26bd8e02":"#Looking into training data\ndf_train.info()\nprint(df_train.columns.tolist())\nprint(\"Jumlah kolom: {}\".format(len(df_train.columns.tolist())))","686cda61":"#Looking into test data\ndf_test.info()","205632f0":"#2. Data visualization\n#Splitting into cat and nums\n\ndf_num=df_train[['Umur','Jmlh Tahun Pendidikan','Jam per Minggu']]\ndf_cat=df_train[['Kelas Pekerja','Pendidikan','Status Perkawinan','Pekerjaan','Jenis Kelamin','Gaji']]","f08078fe":"#Plotting data\n#Numericals as Histograms\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfor i in df_num.columns:\n    plt.hist(df_num[i])\n    plt.title(i)\n    plt.show()","3d42b26b":"#Correlation Values\nprint(df_num.corr)\nsns.heatmap(df_num.corr())","0c47f720":"#Gaji Across Numeric Data\npd.pivot_table(df_train,index='Gaji',values=df_num)","63e1183f":"#Plotting Cat data\nfor i in df_cat.columns:\n    plt.figure(figsize=(12,8))\n    sns.barplot(df_cat[i].value_counts().index,df_cat[i].value_counts()).set_title(i)\n    plt.show()","cfef6b0f":"#Salary rates\npd.pivot_table(df_train,index='Gaji',columns='Kelas Pekerja',values='Umur',aggfunc='count')","07589db5":"pd.pivot_table(df_train,index='Gaji',columns='Jenis Kelamin',values='Umur',aggfunc='count')","e9f0a68a":"pd.pivot_table(df_train,index='Gaji',columns='Status Perkawinan',values='Umur',aggfunc='count')","545cb370":"pd.pivot_table(df_train,index='Gaji',columns='Pendidikan',values='Umur',aggfunc='count')","d71c0858":"pd.pivot_table(df_train,index='Gaji',columns='Pekerjaan',values='Umur',aggfunc='count')","6f3e1031":"#3. Data Normalization\ndf_train.Umur=np.log(df_train.Umur+1)\n\ndf_test.Umur=np.log(df_test.Umur+1)","17f925b1":"df_train.Umur.hist()","eb4cbb06":"#4. Model Preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nscale=StandardScaler()\nmms=MinMaxScaler()\n\nfrom sklearn.model_selection import train_test_split\n#Dropping values which do not yield better results, swapping gender and salary with binary values\ndf_train_2=df_train.drop(['Berat Akhir','Kerugian Capital','Keuntungan Kapital','id'],axis=1).replace({'Perempuan':0,'Laki2':1}).replace({'<=7jt':0,'>7jt':1})\ndf_train_2=pd.get_dummies(df_train_2,columns=['Status Perkawinan', 'Pekerjaan','Pendidikan','Kelas Pekerja'])\n\nX=df_train_2.drop('Gaji',axis=1)\ny=df_train_2.Gaji\n\n#Tuning X\nX=pd.DataFrame(mms.fit_transform(X),columns=X.columns)\nX.Umur = pd.DataFrame(scale.fit_transform(pd.DataFrame(X.Umur)))\n\n#Tuning test\ndf_test_2=df_test.drop(['Berat Akhir','Kerugian Capital','Keuntungan Kapital','id'],axis=1).replace({'Perempuan':0,'Laki2':1}).replace({'<=7jt':0,'>7jt':1})\ndf_test_2=pd.get_dummies(df_test_2,columns=['Status Perkawinan', 'Pekerjaan','Pendidikan','Kelas Pekerja'])\ndf_test_2=pd.DataFrame(mms.transform(df_test_2), columns=df_test_2.columns)\ndf_test_2.Umur = pd.DataFrame(scale.transform(pd.DataFrame(df_test_2.Umur)))\n\nX_train,X_test,y_train,y_test=train_test_split (X, y, test_size=0.25,stratify=y, random_state=17)","b0f94448":"X","6a474499":"#5. Model Tuning\n#5a. Grid Search for best parameters\n#Already chose AdaBoost after several attempts on different models (LR,RF,XGB,SVC,etc. Mostly yield sub-89%, AdaBoost yields 90%+)\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble.weight_boosting import AdaBoostClassifier\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))\n\nada = AdaBoostClassifier()\nparam_grid = {\n    'n_estimators': [100,250,200,225],\n    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n    'random_state' : [0,1]\n}\n'''clf_ada = GridSearchCV(ada, param_grid=param_grid, cv=5, verbose=True, n_jobs=-1)\nbest_clf_ada=clf_ada.fit(X_train, y_train)\n\nclf_performance(best_clf_ada,'Ada Boost')\nAda Boost\nBest Score: 0.836228931283571\nBest Parameters: {'learning_rate': 0.3, 'n_estimators': 250, 'random_state': 0}'''\n","73e7dc76":"#5b. Plugging in best parameters\n#Copied best parameters from previous best,\nada=AdaBoostClassifier(learning_rate=0.2,n_estimators=200,random_state=0)\nada.fit(X,y)\npre_final=ada.predict(df_test_2)\ndf_test_2['Gaji']=pre_final","8b03f7d8":"#6. Submission\nhasil_gaji=df_test_2['Gaji']\nada_submission={'id':df_test.id,'Gaji':hasil_gaji}\nsubmission_ada=pd.DataFrame(data=ada_submission)\nsubmission_ada.to_csv('ada_submission.csv',index=False)","8647b49f":"Lebih banyak laki yang gajinya diatas 7 juta","49d5336d":"Index kebanyakan, susah dilihat","6c9d34f1":"Diatas 7 mulai banyak kalo udah D3 keatas.","97393eeb":"Untuk data dengan gaji <=7jt,\n\n1. Berat akhir rata2 lebih besar \n2. Jam per minggu rata2 lebih kecil\n3. Jmlh tahun pendidikan lebih kecil\n4. Kerugian kapital jauh lebih tinggi\n5. Keuntungan kapital jauh lebih rendah\n6. Umur lebih kecil","91e951c7":"Inspiration Sources:\n1. Info on AdaBoost\nhttps:\/\/www.datacamp.com\/community\/tutorials\/adaboost-classifier-python?utm_source=adwords_ppc&utm_campaignid=1455363063&utm_adgroupid=65083631748&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034358&utm_targetid=aud-392016246653:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=9056654&gclid=Cj0KCQjwsuP5BRCoARIsAPtX_wFsw3T8opvUePCV_9EvhIuAmWQnzEjKXlx8KYBwh1eJ-jTCHWCld2kaAoumEALw_wcB\n\n2. Categorical problem solving (Titanic)\nhttps:\/\/www.kaggle.com\/kenjee\/titanic-project-example","a25d7b9a":"Kemungkinan >7 kalo menikah cukup tinggi","0dd59c97":"Project Overview\n\n1. Reading Data\n2. Data Visualization\n3. Data Normalization\n4. Model Preprocessing\n5. Model Tuning\n6. Submission","089c63a7":"Spesialis, dan eksekutif managerial rata2 diatas 7 juta","103ed88b":"No null values","6960e3e2":"No null values, ada 13 kolom","cb49e676":"* Decided not to feature engineer, it did not yield better results.","9b7031c6":"Rata-rata distribusi tidak normal, harus dinormalisasi"}}