{"cell_type":{"34746250":"code","0aec9d81":"code","65cbd211":"code","e15957f6":"code","dbeba997":"code","348da923":"code","615d816c":"code","dd37f271":"code","e6167e65":"code","b59d0e60":"code","f45e832b":"code","dc3bfc48":"code","4299202f":"code","32802689":"code","e364185a":"code","9ed0e5d6":"code","3cc32fc2":"code","1820cf06":"code","93b43a34":"code","21f1e4ec":"code","35353eab":"code","3d1521ef":"code","963cb3e0":"code","f86036df":"code","268bdc4d":"code","8862932f":"code","9fa2bd01":"code","a37e296b":"code","212dadee":"code","527d3e0d":"code","05acca7a":"code","41ae5f13":"code","e1e1cc8b":"code","94f7a844":"markdown","c6d764d4":"markdown","bdfffd94":"markdown","7b0def85":"markdown","46aea24f":"markdown","6fa24a73":"markdown","a67e811f":"markdown","f816dd16":"markdown","4e726e80":"markdown","dfa40dff":"markdown","ac63dff9":"markdown","99026514":"markdown","6b82ea9b":"markdown","3abd3efe":"markdown","fb6c6fcf":"markdown","6bdaf363":"markdown"},"source":{"34746250":"from pathlib import Path\nimport datetime\n\nimport pandas as pd # data processing, CSV file I\/O\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport seaborn as sns\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nimport pickle\n\nimport warnings\nfrom IPython import get_ipython\nget_ipython().config.InlineBackend.figure_format = 'retina'\nwarnings.simplefilter(\"ignore\")\n\n# List input data files available in read-only \"..\/input\/\" directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0aec9d81":"# time variables (NOTE: use pre-June 2015 data cautiously as large chunks are missing)\n\n# data time range to train on the full training set\nfull_train_start_day = datetime.datetime(2015, 6, 16)\nfull_train_end_day = datetime.datetime(2017, 8, 15)\n\n# data time range for train\/validation split \ntrain_start_day = full_train_start_day\ntrain_end_day = datetime.datetime(2017, 7, 30)\nval_start_day = datetime.datetime(2017, 7, 31)\nval_end_day = datetime.datetime(2017, 8, 15)\n# can be smart to set val_end_day to (2017, 7, 31) or (2017, 8, 1) when testing a change or debugging\n\n# data time range of test set\ntest_start_day = datetime.datetime(2017, 8, 16)\ntest_end_day = datetime.datetime(2017, 8, 31)\n\nif full_train_start_day > full_train_end_day:\n    raise ValueError(\"full_train_start_day must be less than full_train_end_day . . . Did you change month without changing year?\")","65cbd211":"# other key variables\n\n# max_lag is needed so BoostedHybrid class knows how many rows to drop that have NaN or incorrect data after creating lag features. For example:\n# if you compute rolling (trailing) 7 day means based on lag_1, the first entry in time series is Nan, but next 6 are means based on <7 days.\n# So want max_lag=7 so that the first 7 rows of time series will be dropped.\n# If you create a rolling 14 day means feature, you'd want to change max_lag to 14\n# If you use \"direct\" for hybrid_forecasting_type, you can set max_lag to 0 but you'll need to comment out features that are lagged or depend on lagged features\n\nmax_lag = 7\n\nmod_1 = LinearRegression()\nmod_2 = XGBRegressor()\n# for mod_1 can experiment with other models such as Ridge(fit_intercept=True, solver='auto', alpha=0.5, normalize=True)\n# for mod_2 can experiment with other models such as KNeighborsRegressor()\n\nhybrid_forecasting_type = \"day_by_day_refit_all_days\" # possible values: day_by_day_refit_all_days, day_by_day_fixed_past, or direct\n\n# Choose one of 3 forecasting types:\n\n# day_by_day_fixed_past:\n#    forecast 1 day at a time which allows use of lag features. Slow.\n#    Each new day y is fixed after it's initial prediction - no y row is ever predicted more than once\n\n# day_by_day_refit_all_days:\n#    forecast 1 day at a time which allows use of lag features. Slow.\n#    Each new forecast causes y for all days in training and test (or validation set) to be reforecast.\n\n# direct:\n#    forecast entire 16 days of test period in one round. Very fast.\n#    Can't use any lagged features (KNeighborsRegressor gives you error message, XGBRegressor just gives you terrible results)","e15957f6":"# plot style settings from learntools.time_series.style\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)","dbeba997":"# from learntools.time_series.utils\n# https:\/\/github.com\/Kaggle\/learntools\/blob\/master\/learntools\/time_series\/utils.py\n\n# If I had to make a change, I removed the function or class from this cell and put the revised versions just before they're used\n\ndef seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\n        \"husl\",\n        n_colors=X[period].nunique(),\n    )\n    ax = sns.lineplot(\n        x=freq,\n        y=y,\n        hue=period,\n        data=X,\n        ci=False,\n        ax=ax,\n        palette=palette,\n        legend=False,\n    )\n    ax.set_title(f\"Seasonal Plot ({period}\/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name,\n            xy=(1, y_),\n            xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\",\n            size=14,\n            va=\"center\",\n        )\n    return ax\n\n\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") \/ pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax\n\n# From Lesson 4\n\ndef lagplot(x, y=None, shift=1, standardize=False, ax=None, **kwargs):\n    from matplotlib.offsetbox import AnchoredText\n    x_ = x.shift(shift)\n    if standardize:\n        x_ = (x_ - x_.mean()) \/ x_.std()\n    if y is not None:\n        y_ = (y - y.mean()) \/ y.std() if standardize else y\n    else:\n        y_ = x\n    corr = y_.corr(x_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    scatter_kws = dict(\n        alpha=0.75,\n        s=3,\n    )\n    line_kws = dict(color='C3', )\n    ax = sns.regplot(x=x_,\n                     y=y_,\n                     scatter_kws=scatter_kws,\n                     line_kws=line_kws,\n                     lowess=True,\n                     ax=ax,\n                     **kwargs)\n    at = AnchoredText(\n        f\"{corr:.2f}\",\n        prop=dict(size=\"large\"),\n        frameon=True,\n        loc=\"upper left\",\n    )\n    at.patch.set_boxstyle(\"square, pad=0.0\")\n    ax.add_artist(at)\n    title = f\"Lag {shift}\" if shift > 0 else f\"Lead {shift}\"\n    ax.set(title=f\"Lag {shift}\", xlabel=x_.name, ylabel=y_.name)\n    return ax\n\n\ndef plot_lags(x,\n              y=None,\n              lags=6,\n              leads=None,\n              nrows=1,\n              lagplot_kwargs={},\n              **kwargs):\n    import math\n    kwargs.setdefault('nrows', nrows)\n    orig = leads is not None\n    leads = leads or 0\n    kwargs.setdefault('ncols', math.ceil((lags + orig + leads) \/ nrows))\n    kwargs.setdefault('figsize', (kwargs['ncols'] * 2, nrows * 2 + 0.5))\n    fig, axs = plt.subplots(sharex=True, sharey=True, squeeze=False, **kwargs)\n    for ax, k in zip(fig.get_axes(), range(kwargs['nrows'] * kwargs['ncols'])):\n        k -= leads + orig\n        if k + 1 <= lags:\n            ax = lagplot(x, y, shift=k + 1, ax=ax, **lagplot_kwargs)\n            title = f\"Lag {k + 1}\" if k + 1 >= 0 else f\"Lead {-k - 1}\"\n            ax.set_title(title, fontdict=dict(fontsize=14))\n            ax.set(xlabel=\"\", ylabel=\"\")\n        else:\n            ax.axis('off')\n    plt.setp(axs[-1, :], xlabel=x.name)\n    plt.setp(axs[:, 0], ylabel=y.name if y is not None else x.name)\n    fig.tight_layout(w_pad=0.1, h_pad=0.1)\n    return fig\n\n# From Lesson 6\n\ndef make_lags(ts, lags, lead_time=1, name='y'):\n    return pd.concat(\n        {\n            f'{name}_lag_{i}': ts.shift(i)\n            for i in range(lead_time, lags + lead_time)\n        },\n        axis=1)\n\n\ndef make_leads(ts, leads, name='y'):\n    return pd.concat(\n        {f'{name}_lead_{i}': ts.shift(-i)\n         for i in reversed(range(leads))},\n        axis=1)\n\n\ndef make_multistep_target(ts, steps, reverse=False):\n    shifts = reversed(range(steps)) if reverse else range(steps)\n    return pd.concat({f'y_step_{i + 1}': ts.shift(-i) for i in shifts}, axis=1)\n\n\ndef create_multistep_example(n, steps, lags, lead_time=1):\n    ts = pd.Series(\n        np.arange(n),\n        index=pd.period_range(start='2010', freq='A', periods=n, name='Year'),\n        dtype=pd.Int8Dtype,\n    )\n    X = make_lags(ts, lags, lead_time)\n    y = make_multistep_target(ts, steps, reverse=True)\n    data = pd.concat({'Targets': y, 'Features': X}, axis=1)\n    data = data.style.set_properties(['Targets'], **{'background-color': 'LavenderBlush'}) \\\n                     .set_properties(['Features'], **{'background-color': 'Lavender'})\n    return data\n\n\ndef load_multistep_data():\n    df1 = create_multistep_example(10, steps=1, lags=3, lead_time=1)\n    df2 = create_multistep_example(10, steps=3, lags=4, lead_time=2)\n    df3 = create_multistep_example(10, steps=3, lags=4, lead_time=1)\n    return [df1, df2, df3]\n\n\ndef plot_multistep(y, every=1, ax=None, palette_kwargs=None):\n    palette_kwargs_ = dict(palette='husl', n_colors=16, desat=None)\n    if palette_kwargs is not None:\n        palette_kwargs_.update(palette_kwargs)\n    palette = sns.color_palette(**palette_kwargs_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.set_prop_cycle(plt.cycler('color', palette))\n    for date, preds in y[::every].iterrows():\n        preds.index = pd.period_range(start=date, periods=len(preds))\n        preds.plot(ax=ax)\n    return ax","348da923":"# load raw data\n\ncomp_dir = Path('..\/input\/store-sales-time-series-forecasting')\ndf_sales_train = pd.read_csv(comp_dir \/ 'train.csv', parse_dates=['date'])\ndf_sales_test = pd.read_csv(comp_dir \/ 'test.csv', parse_dates=['date'])\ndf_trans = pd.read_csv(comp_dir \/ 'transactions.csv', parse_dates=['date'])\ndf_stores = pd.read_csv(comp_dir \/ 'stores.csv')\ndf_oil = pd.read_csv(comp_dir \/ 'oil.csv', parse_dates=['date'])","615d816c":"# add features to df_stores and\n# create daily_store_totals_df to include several data sources and some new features\n\n# daily sales totals needed for store analysis and additional features\ntotal_daily_sales = (\n    df_sales_train.drop(columns = 'onpromotion')\n    .groupby(['date', 'store_nbr'])\n    .sum()\n    .reset_index()\n#     .unstack('store_nbr')\n)\n\nstores_temp = pd.merge(df_trans, total_daily_sales, on=['date', 'store_nbr']).drop(columns=\"id\")\n\n# set up 4 new features:\n\n# NOTE: sales is # items sold, not monetary amount, so this isn't really avg ticket size, but average amount of items sold per transaction\n# 1) average ticket per transaction (common retail metric) - total daily sales divided by total daily transactions\nstores_temp['avg_ticket'] = stores_temp['sales'] \/ stores_temp['transactions']\n\n# 2) \"old\" store is one that has already opened and has sales on first (non holiday) date of data set\nold_stores = stores_temp[stores_temp['date'] == '2013-01-02'].store_nbr.values\ndf_stores['old'] = df_stores['store_nbr'].isin(old_stores)\n\n# 3) start date for store\ndf_stores['start_date'] = [stores_temp[stores_temp['store_nbr']==num]['date'].dt.date.min() for num in range(1,55)]\ndf_stores['start_date'] = pd.to_datetime(df_stores['start_date'])\n\n# 4) wage: wages paid in public sector is True when on 15th or last day of month\nstores_temp['wage'] = (stores_temp['date'].dt.is_month_end) | (stores_temp['date'].dt.day == 15)\n\ndaily_store_totals_df = pd.merge(stores_temp, df_stores, on='store_nbr')","dd37f271":"comp_dir = Path('..\/input\/store-sales-time-series-forecasting')\n\nholidays_events = pd.read_csv(\n    comp_dir \/ \"holidays_events.csv\",\n    dtype={\n        'type': 'category',\n        'locale': 'category',\n        'locale_name': 'category',\n        'description': 'category',\n        'transferred': 'bool',},\n    parse_dates=['date'],\n    infer_datetime_format=True,)\n\nholidays_events['date'] = holidays_events['date'].replace({'2013-04-29':pd.to_datetime('2013-03-29')}) # 'Good Friday' mistake correction\nholidays_events = holidays_events.set_index('date').to_period('D').sort_index() # note the sort after Good Friday correction\n\ndf_test = pd.read_csv(\n    comp_dir \/ 'test.csv',\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'onpromotion': 'uint32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\n \ndf_test['date'] = df_test.date.dt.to_period('D')\ndf_test = df_test.set_index(['store_nbr', 'family', 'date']).sort_index()","e6167e65":"%%time\n\nstore_sales = pd.read_csv(\n    comp_dir \/ 'train.csv',\n    usecols=['store_nbr', 'family', 'date', 'sales', 'onpromotion'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'sales': 'float32',\n        'onpromotion': 'uint32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,)\n\nstore_sales['date'] = store_sales.date.dt.to_period('D')\n# store_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index() # MultiIndex\n\n# replace above MultiIndex line with the following 2 lines in order to get missing Christmas days\n# as some algorithms later require filled out MultiIndex (even if just zeros), including the missing Christmas days\n# (see https:\/\/stackoverflow.com\/questions\/62437346\/add-missing-date-index-in-a-multiindex-dataframe)\nm_index = pd.MultiIndex.from_product([store_sales[\"store_nbr\"].unique(),\n                                      store_sales[\"family\"].unique(),\n                                      pd.date_range(start=\"2013-1-1\", end=\"2017-8-15\", freq=\"D\").to_period('D')] # to get missing Christmas Days\n                                     ,names=[\"store_nbr\",\"family\", \"date\"])\nstore_sales = store_sales.set_index([\"store_nbr\",\"family\", \"date\"]).reindex(m_index, fill_value=0).sort_index()\n\n\nstore_sales = store_sales.unstack(['store_nbr', 'family']).fillna(0) # there are lots!\nstore_sales = store_sales.stack(['store_nbr', 'family'])\nstore_sales = store_sales[['sales','onpromotion']] # reorder columns to be in the expected order","b59d0e60":"# create workday calendar, incorporating holidays\n# credit to KDJ2020: https:\/\/www.kaggle.com\/dkomyagin\/simple-ts-ridge-rf\n\ncalendar = pd.DataFrame(index=pd.date_range('2013-01-01', '2017-08-31')).to_period('D')\ncalendar['dofw'] = calendar.index.dayofweek\n\ndf_hev = holidays_events[holidays_events.locale == 'National'] # National level only for simplicity\ndf_hev = df_hev.groupby(df_hev.index).first() # Keep one event only\n\ncalendar['wd'] = True\ncalendar.loc[calendar.dofw > 4, 'wd'] = False\ncalendar = calendar.merge(df_hev, how='left', left_index=True, right_index=True)\n\ncalendar.loc[calendar.type == 'Bridge'  , 'wd'] = False\ncalendar.loc[calendar.type == 'Work Day', 'wd'] = True\ncalendar.loc[calendar.type == 'Transfer', 'wd'] = False\ncalendar.loc[(calendar.type == 'Holiday') & (calendar.transferred == False), 'wd'] = False\ncalendar.loc[(calendar.type == 'Holiday') & (calendar.transferred == True ), 'wd'] = True\n\n# Transferred column True: holiday officially falls on that calendar day, but was moved to another date by the government.\n# type Transfer: day transfer (True) holiday is actually celebrated (i.e. 8\/10\/17 --> 8\/11\/17 Primer Grito de Independencia)\n\n# type Bridge: extra days that are added to a holiday (e.g., to extend the break across a long weekend).\n# These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.\n\n# type Additional:  days added to a regular calendar holiday such as happens around Christmas","f45e832b":"calendar.tail(23)","dc3bfc48":"holidays_events.loc[full_train_start_day:test_end_day]","4299202f":"class BoostedHybrid:\n    def __init__(self, model_1, model_2):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.y_columns = None\n        self.stack_cols = None\n        self.y_resid = None\n\n    def fit1(self, X_1, y, stack_cols=None):\n        self.model_1.fit(X_1, y) # train model 1\n        y_fit = pd.DataFrame(\n            self.model_1.predict(X_1), # predict from model 1\n            index=X_1.index,\n            columns=y.columns,\n        )\n        self.y_resid = y - y_fit # residuals from model 1, which X2 may want to access to create lag (or other) features\n        self.y_resid = self.y_resid.stack(stack_cols).squeeze()  # wide to long\n        \n    def fit2(self, X_2, first_n_rows_to_ignore, stack_cols=None):\n        self.model_2.fit(X_2.iloc[first_n_rows_to_ignore*1782: , :], self.y_resid.iloc[first_n_rows_to_ignore*1782:]) # Train model_2\n        self.y_columns = y.columns # Save for predict method\n        self.stack_cols = stack_cols # Save for predict method\n\n    def predict(self, X_1, X_2, first_n_rows_to_ignore):\n        y_pred = pd.DataFrame(\n            self.model_1.predict(X_1.iloc[first_n_rows_to_ignore: , :]),\n            index=X_1.iloc[first_n_rows_to_ignore: , :].index,\n            columns=self.y_columns,\n        )\n        y_pred = y_pred.stack(self.stack_cols).squeeze()  # wide to long\n#         display(X_2.iloc[first_n_rows_to_ignore*1782: , :]) # uncomment when debugging\n        y_pred += self.model_2.predict(X_2.iloc[first_n_rows_to_ignore*1782: , :]) # Add model_2 predictions to model_1 predictions\n        return y_pred.unstack(self.stack_cols)","32802689":"# making this a seperate function keeps make_X1_features small\n# might make sense to make dp into a BoostedHybrid Class variable\n\ndef make_dp_features(df):\n    y = df.loc[:, 'sales']\n    #fourier_a = CalendarFourier(freq='A', order=4)\n    fourier_m = CalendarFourier(freq='M', order=4)\n    dp = DeterministicProcess(\n        index=y.index,\n        constant=True,\n        order=1,\n        seasonal=True, # note how this generates terms Tue - Sun: s(2,7) through s(7,7) (trend column: Monday)\n        additional_terms=[fourier_m],\n        drop=True,\n    )\n    return y, dp\n\n# for other legal values for freq='?', see:\n# https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/timeseries.html#offset-aliases\n# B C D W M SM BM CBM MS SMS BMS CBMS Q BQ QS BQS (A, Y) (BA, BY) (AS, YS) (BAS, BYS) BH H (T, min) S (L, ms) (U, us) N","e364185a":"# create feature set X1 for hybrid model 1\n\n# as lesson 5 suggests, X1 features are time-related features that\n# can be used to model long-term trends and seasonal trends\n\ndef make_X1_features(df, start_date, end_date, is_test_set=False):\n    if is_test_set:\n        X1 = df.rename_axis('date')\n    else:\n        y, dp = make_dp_features(df)\n        X1 = dp.in_sample() # seasonal (weekly) and fourier (longer time frame) features are generated using DeterministicProcess\n    \n    # other features:\n    \n#     X1['wage_day'] = (X1.index.day == X1.index.daysinmonth) | (X1.index.day == 15) # wage day features seem better for XGBoost than linear regression\n#     X1['wage_day_lag_1'] = (X1.index.day == 1) | (X1.index.day == 16)\n    X1['NewYear'] = (X1.index.dayofyear == 1)\n    X1['Christmas'] = (X1.index=='2016-12-25') | (X1.index=='2015-12-25') | (X1.index=='2014-12-25') | (X1.index=='2013-12-25')\n    X1['wd']   = calendar.loc[start_date:end_date]['wd'].values\n    X1['type'] = calendar.loc[start_date:end_date]['type'].values\n    X1 = pd.get_dummies(X1, columns=['type'], drop_first=False)\n    \n    # can experiment with dropping some of the dummy features if you think they might be useless or worse\n#     X1.drop(['type_Work Day', 'type_Event'], axis=1, inplace=True)\n    \n    if is_test_set:\n        return X1\n    else:\n        return X1, y, dp","9ed0e5d6":"# create feature set X2 for hybrid model 2, including helper functions\n\n# as lesson 5 suggests, X2 features can be anything,\n# allowing an algorithm weak on trends but strong on detecting relationships among variables\n# to further refine the modeling and forecasting\n\ndef encode_categoricals(df, columns):\n    le = LabelEncoder()  # from sklearn.preprocessing\n    for col in columns:\n        df[col] = le.fit_transform(df[col])\n    return df\n\ndef make_X2_lags(ts, lags, lead_time=1, name='y', stack_cols=None):\n    ts = ts.unstack(stack_cols)\n    df = pd.concat(\n        {\n            f'{name}_lag_{i}': ts.shift(i, freq=\"D\") # freq adds i extra day(s) to end: only one extra day is needed so rest will be dropped\n            for i in range(lead_time, lags + lead_time)\n        },\n        axis=1)\n    df = df.stack(stack_cols).reset_index()\n    df = encode_categoricals(df, stack_cols)\n    df = df.set_index('date').sort_values(by=stack_cols) # return sorted so can correctly compute rolling means (if desired)\n    return df\n\ndef make_X2_features(df, y_resid):\n    stack_columns = ['store_nbr', 'family']\n    \n#     # promo_lag features\n    shifted_promo_df = make_X2_lags(df.squeeze(), lags=2, name='promo', stack_cols=['store_nbr', 'family'])\n    shifted_promo_df['promo_mean_rolling_7'] = shifted_promo_df['promo_lag_1'].rolling(window=7, center=False).mean()\n    shifted_promo_df['promo_median_rolling_91'] = shifted_promo_df['promo_lag_1'].rolling(window=91, center=False).median().fillna(method='bfill')\n    shifted_promo_df['promo_median_rolling_162'] = shifted_promo_df['promo_lag_1'].rolling(window=162, center=False).median().fillna(method='bfill')\n    # for rolling window medians, backfilling seems reasonable as medians shouldn't change too much. Trying min_periods produced wacky (buggy?) results\n    \n    # y_lag features\n    shifted_y_df = make_X2_lags(y_resid, lags=2, name='y_res', stack_cols=stack_columns)\n    shifted_y_df['y_mean_rolling_7'] = shifted_y_df['y_res_lag_1'].rolling(window=7, center=False).mean()\n#     shifted_y_df['y_mean_rolling_14'] = shifted_y_df['y_res_lag_1'].rolling(window=14, center=False).mean()\n#     shifted_y_df['y_mean_rolling_28'] = shifted_y_df['y_res_lag_1'].rolling(window=28, center=False).mean()\n    shifted_y_df['y_median_rolling_91'] = shifted_y_df['y_res_lag_1'].rolling(window=91, center=False).median().fillna(method='bfill')\n    shifted_y_df['y_median_rolling_162'] = shifted_y_df['y_res_lag_1'].rolling(window=162, center=False).median().fillna(method='bfill')\n    \n    # other features\n    df = df.reset_index(stack_columns)\n    X2 = encode_categoricals(df, stack_columns)\n    \n#     X2[\"day_of_m\"] = X2.index.day  # day of month (label encloded) feature for learning seasonality\n#     X2 = encode_categoricals(df, ['day_of_m']) # encoding as categorical has tiny impact with XGBoost\n    X2[\"day_of_w\"] = X2.index.dayofweek # does absolutely nothing alone\n    X2 = encode_categoricals(df, ['day_of_w'])\n    old_stores_strings = list(map(str, old_stores))\n    X2['old'] = X2['store_nbr'].isin(old_stores_strings) # True if store had existing sales prior to training time period   \n    X2['wage_day'] = (X2.index.day == X2.index.daysinmonth) | (X2.index.day == 15) # is it bad to have this in both X1 AND X2?\n    X2['wage_day_lag_1'] = (X2.index.day == 1) | (X2.index.day == 16)\n    X2['promo_mean'] = X2.groupby(['store_nbr', 'family'])['onpromotion'].transform(\"mean\") + 0.000001\n    X2['promo_ratio'] = X2.onpromotion \/ (X2.groupby(['store_nbr', 'family'])['onpromotion'].transform(\"mean\") + 0.000001)\n\n    # Can experiment with dropping basic feature but keeping something computed from it (i.e. drop y_res_lag_1 feature, keeping y_mean_rolling_7\n#     shifted_y_df.drop('y_res_lag_1', axis=1, inplace=True)\n#     shifted_promo_df.drop('promo_lag_1', axis=1, inplace=True)\n#     X2.drop('onpromotion', axis=1, inplace=True)\n\n    # if removing all lag features, then comment out the following two lines\n    X2 = X2.merge(shifted_y_df, on=['date', 'store_nbr', 'family'], how='left')\n    X2 = X2.merge(shifted_promo_df, on=['date', 'store_nbr', 'family'], how='left') # merges work if they are last line before return\n    return X2","3cc32fc2":"%%time\n\n# unstack pivots MultiIndex to 54 x 33 = 1782 y columns\nstore_sales_in_date_range = store_sales.unstack(['store_nbr', 'family']).loc[full_train_start_day:full_train_end_day]\n\nmodel = BoostedHybrid(model_1=mod_1, model_2=mod_2) # Boosted Hybrid\n\nX_1, y, dp = make_X1_features(store_sales_in_date_range, full_train_start_day, full_train_end_day) # preparing X1 for hybrid model 1\nmodel.fit1(X_1, y, stack_cols=['store_nbr', 'family']) # fit1 before make_X2_features, since X2 may want to create lag features from model.y_resid\nX_2 = make_X2_features(store_sales_in_date_range # preparing X2 for hybrid model 2\n                       .drop('sales', axis=1)\n                       .stack(['store_nbr', 'family']),\n                       model.y_resid)\nmodel.fit2(X_2, max_lag, stack_cols=['store_nbr', 'family'])\n\ny_pred = model.predict(X_1, X_2, max_lag).clip(0.0)","1820cf06":"def truncateFloat(data):\n    return tuple( [\"{0:.2f}\".format(x) if isinstance(x,float) else (x if not isinstance(x,tuple) else truncateFloat(x)) for x in data])\n\ntemp = X_2[(X_2.store_nbr == 1) & (X_2.family == 3)]\ntemp.iloc[max_lag: , :].apply(lambda s: truncateFloat(s)) # comment out next line if don't want to see nan rows\n\ntemp.apply(lambda s: truncateFloat(s)).head(10) # note that the fit method of BoostedHybrid class skips over nan rows","93b43a34":"X_1.iloc[max_lag: , :].apply(lambda s: truncateFloat(s))","21f1e4ec":"# see how the model fits entire training data set (except for first n dates where max_lag=n)\n# for a specific store\/family combination\n\nSTORE_NBR = '1'  # 1 - 54\nFAMILY = 'BEVERAGES' # display(store_sales.index.get_level_values('family').unique())\n\nax = y.loc(axis=1)[STORE_NBR, FAMILY].plot(**plot_params, figsize=(16, 4))\nax = y_pred.loc(axis=1)[STORE_NBR, FAMILY].plot(ax=ax)\nax.set_title(f'{FAMILY} Sales at Store {STORE_NBR}');","35353eab":"# train\/val split prep that is the same for any of the hybrid forecasting methods\n\ntraining_days = (train_end_day - train_start_day).days + 1\nvalidation_days = (val_end_day - val_start_day).days + 1\nprint(\"training data set (excluding validation days) has\", training_days, \"days\")\nprint(\"validation data set has\", validation_days, \"days\\n\")\n\nstore_sales_in_date_range = store_sales.unstack(['store_nbr', 'family']).loc[train_start_day:train_end_day]\nstore_data_in_val_range = store_sales.unstack(['store_nbr', 'family']).loc[val_start_day:val_end_day]\ny_val = y[val_start_day:val_end_day] # use y to evaluate validation set, though we will treat y as unknown when training\n\nmodel_for_val = BoostedHybrid(model_1=mod_1, model_2=mod_2)","3d1521ef":"%%time\n\n# DIRECT hybrid version of train\/validate (can't use with lagged\/rolling features on y, promo, etc.)\nif hybrid_forecasting_type == \"direct\":\n    X_1_train, y_train, dp_val = make_X1_features(store_sales_in_date_range, train_start_day, train_end_day) # preparing X1 for hybrid part 1: LinearRegression\n    model_for_val.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family']) # fit1 before make_X2_features, since X2 may want to create lag features from model.y_resid\n    X_2_train = make_X2_features(store_sales_in_date_range\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_val.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    model_for_val.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\n\n    X_1_val = make_X1_features(dp_val.out_of_sample(steps=validation_days), val_start_day, val_end_day, is_test_set=True)\n    X_2_val = make_X2_features(store_data_in_val_range\n                                .drop('sales', axis=1)\n                                .stack(['store_nbr', 'family']),\n                                model_for_val.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    y_fit = model_for_val.predict(X_1_train, X_2_train, max_lag).clip(0.0)\n    y_pred = model_for_val.predict(X_1_val, X_2_val, 0).clip(0.0) # set max_lag to 0 because need entire time span for validation data set\n    \n    if type(model_for_val.model_2) == XGBRegressor:\n        pickle.dump(model_for_val.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","963cb3e0":"%%time\n\n# forecast 1 day at a time which allows use of lag features. Slow.\n# Each new day y is fixed after it's initial prediction - no y row is ever predicted more than once\nif hybrid_forecasting_type == \"day_by_day_fixed_past\":\n    #initial fit on train portion of train\/val split\n    X_1_train, y_train, dp_val = make_X1_features(store_sales_in_date_range, train_start_day, train_end_day) # preparing X1 for hybrid part 1: LinearRegression\n    model_for_val.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family']) # fit1 before make_X2_features, since X2 may want to create lag features from model.y_resid\n    X_2_train = make_X2_features(store_sales_in_date_range\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_val.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    model_for_val.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\n    y_fit = model_for_val.predict(X_1_train, X_2_train, max_lag).clip(0.0)\n\n    y_pred_combined = y_fit.copy() # initialize y_pred_combined\n\n    # loop through forecast, one day (\"step\") at a time\n    dp_for_full_X1_val_date_range = dp_val.out_of_sample(steps=validation_days)\n    for step in range(validation_days):\n        dp_steps_so_far = dp_for_full_X1_val_date_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step),:]\n\n        X_1_combined_dp_data = pd.concat([dp_val.in_sample(), dp_steps_so_far])\n        X_2_combined_data = pd.concat([store_sales_in_date_range,\n                                       store_data_in_val_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step), :]])\n        X_1_val = make_X1_features(X_1_combined_dp_data, train_start_day, val_start_day+pd.Timedelta(days=step), is_test_set=True)\n        X_2_val = make_X2_features(X_2_combined_data\n                                    .drop('sales', axis=1)\n                                    .stack(['store_nbr', 'family']),\n                                    model_for_val.y_resid) # preparing X2 for hybrid part 2: XGBoost\n\n    #     print(\"last 3 rows of X_1_val: \")\n    #     display(X_1_val.tail(3))\n    #     temp_val2 = X_2_val[(X_2_val.store_nbr == 1) & (X_2_val.family == 3)]\n    #     print(\"last 3 rows of X_2_val: \")\n    #     display(temp_val2.tail(3).apply(lambda s: truncateFloat(s)))\n\n        y_pred_combined = pd.concat([y_pred_combined,\n                                     model_for_val.predict(X_1_val, X_2_val, max_lag).clip(0.0).iloc[-1:]\n                                    ])\n    #     print(\"last 3 rows of y_combined: \")\n    #     display(y_pred_combined.tail(3).apply(lambda s: truncateFloat(s)))\n        y_plus_y_val = pd.concat([y_train, y_pred_combined.iloc[-(step+1):]]) # add newly predicted rows of y_pred_combined\n        model_for_val.fit1(X_1_val, y_plus_y_val, stack_cols=['store_nbr', 'family']) # fit on new combined X, y - note that fit prior to val date range will change slightly\n        model_for_val.fit2(X_2_val, max_lag, stack_cols=['store_nbr', 'family'])\n\n        rmsle_valid = mean_squared_log_error(y_val.iloc[step:step+1], y_pred_combined.iloc[-1:]) ** 0.5\n        print(f'Validation RMSLE: {rmsle_valid:.5f}', \"for\", val_start_day+pd.Timedelta(days=step))\n\n    y_pred = y_pred_combined[val_start_day:val_end_day]\n    print(\"\\ny_pred: \")\n    display(y_pred.apply(lambda s: truncateFloat(s)))\n    \n    if type(model_for_val.model_2) == XGBRegressor:\n        pickle.dump(model_for_val.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","f86036df":"%%time\n# forecast 1 day at a time which allows use of lag features. Slow.\n# each new forecast causes y for all days in training and test (or validation set) to be reforecast.\nif hybrid_forecasting_type == \"day_by_day_refit_all_days\":\n    #initial fit on train portion of train\/val split\n    X_1_train, y_train, dp_val = make_X1_features(store_sales_in_date_range, train_start_day, train_end_day) # preparing X1 for hybrid part 1: LinearRegression\n    model_for_val.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family']) # fit1 before make_X2_features, since X2 may want to create lag features from model.y_resid\n    X_2_train = make_X2_features(store_sales_in_date_range\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_val.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    model_for_val.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\n    y_fit = model_for_val.predict(X_1_train, X_2_train, max_lag).clip(0.0)\n\n    # loop through forecast, one day (\"step\") at a time\n    dp_for_full_X1_val_date_range = dp_val.out_of_sample(steps=validation_days)\n    for step in range(validation_days):\n        dp_steps_so_far = dp_for_full_X1_val_date_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step),:]\n\n        X_1_combined_dp_data = pd.concat([dp_val.in_sample(), dp_steps_so_far])\n        X_2_combined_data = pd.concat([store_sales_in_date_range,\n                                       store_data_in_val_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step), :]])\n        X_1_val = make_X1_features(X_1_combined_dp_data, train_start_day, val_start_day+pd.Timedelta(days=step), is_test_set=True)\n        X_2_val = make_X2_features(X_2_combined_data\n                                    .drop('sales', axis=1)\n                                    .stack(['store_nbr', 'family']),\n                                    model_for_val.y_resid) # preparing X2 for hybrid part 2: XGBoost\n\n    #     print(\"last 3 rows of X_1_val: \")\n    #     display(X_1_val.tail(3))\n    #     temp_val2 = X_2_val[(X_2_val.store_nbr == 1) & (X_2_val.family == 3)]\n    #     print(\"last 3 rows of X_2_val: \")\n    #     display(temp_val2.tail(3).apply(lambda s: truncateFloat(s)))\n\n        y_pred_combined = model_for_val.predict(X_1_val, X_2_val, max_lag).clip(0.0) # generate y with \n    #     print(\"last 3 rows of y_combined: \")\n    #     display(y_pred_combined.tail(3).apply(lambda s: truncateFloat(s)))\n        y_plus_y_val = pd.concat([y_train, y_pred_combined.iloc[-(step+1):]]) # add newly predicted rows of y_pred_combined\n        model_for_val.fit1(X_1_val, y_plus_y_val, stack_cols=['store_nbr', 'family']) # fit on new combined X, y - note that fit prior to val date range will change slightly\n        model_for_val.fit2(X_2_val, max_lag, stack_cols=['store_nbr', 'family'])\n\n        rmsle_valid = mean_squared_log_error(y_val.iloc[step:step+1], y_pred_combined.iloc[-1:]) ** 0.5\n        print(f'Validation RMSLE: {rmsle_valid:.5f}', \"for\", val_start_day+pd.Timedelta(days=step))\n    #     print(\"end of round \", step)\n\n    y_pred = y_pred_combined[val_start_day:val_end_day]\n    print(\"\\ny_pred: \")\n    display(y_pred.apply(lambda s: truncateFloat(s)))\n    \n    if type(model_for_val.model_2) == XGBRegressor:\n        pickle.dump(model_for_val.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","268bdc4d":"rmsle_train = mean_squared_log_error(y_train.iloc[max_lag: , :].clip(0.0), y_fit) ** 0.5\nrmsle_valid = mean_squared_log_error(y_val.clip(0.0), y_pred) ** 0.5\nprint()\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')\n    \ny_predict = y_pred.stack(['store_nbr', 'family']).reset_index()\ny_target = y_val.stack(['store_nbr', 'family']).reset_index().copy()\ny_target.rename(columns={y_target.columns[3]:'sales'}, inplace=True)\ny_target['sales_pred'] = y_predict[0].clip(0.0) # Sales should be >= 0\ny_target['store_nbr'] = y_target['store_nbr'].astype(int)\n\nprint('\\nValidation RMSLE by family')\ndisplay(y_target.groupby('family').apply(lambda r: mean_squared_log_error(r['sales'], r['sales_pred'])))\n\nprint('\\nValidation RMSLE by store')\ndisplay(y_target.sort_values(by=\"store_nbr\").groupby('store_nbr').apply(lambda r: mean_squared_log_error(r['sales'], r['sales_pred'])))","8862932f":"# train\/test split prep that is the same for any of the hybrid forecasting methods\n\ntrain_days = (full_train_end_day - full_train_start_day).days + 1\ntest_days = (test_end_day - test_start_day).days + 1\n\nprint(\"data trained over\", train_days, \"days\")\nprint(\"test forecasting period is\", test_days, \"days through\", test_end_day, \"\\n\")\nstore_sales_in_date_range = store_sales.unstack(['store_nbr', 'family']).loc[full_train_start_day:full_train_end_day]\nstore_data_in_test_range = df_test.unstack(['store_nbr', 'family']).drop('id', axis=1)\n\n# previously prepared data and fit \"model\" from data ranging from full_train_start_day to full_train_end_day. Can be used by when fitting test.\nmodel_for_test = BoostedHybrid(model_1=mod_1, model_2=mod_2)","9fa2bd01":"%%time\n# DIRECT hybrid version of train\/validate (can't use with lagged\/rolling features on y, promo, etc.)\n# model.fit (1 and 2) already happened previously (on training set with a time range ending on most recent date)\n\nif hybrid_forecasting_type == \"direct\":\n    X_1_test = make_X1_features(dp.out_of_sample(steps=int(test_days)), test_start_day, test_end_day, is_test_set=True)\n    X_2_test = make_X2_features(store_data_in_test_range.loc[test_start_day:test_end_day]\n                                .stack(['store_nbr', 'family']),\n                                model.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    y_forecast = pd.DataFrame(model.predict(X_1_test, X_2_test, 0).clip(0.0), index=X_1_test.index, columns=y.columns) # set max_lag to 0 because need entire time span for test data set\n    \n    if type(model.model_2) == XGBRegressor:\n        pickle.dump(model.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","a37e296b":"%%time\n# forecast 1 day at a time which allows use of lag features. Slow.\n# Each new day y is fixed after it's initial prediction - no y row is ever predicted more than once\nif hybrid_forecasting_type == \"day_by_day_fixed_past\":\n    #initial fit on train portion of train\/test split\n    X_1_train, y_train, dp_test = make_X1_features(store_sales_in_date_range, full_train_start_day, full_train_end_day) # preparing X1 for hybrid part 1: LinearRegression\n    model_for_test.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family']) # fit1 before make_X2_features, since X2 may want to create lag features from model.y_resid\n    X_2_train = make_X2_features(store_sales_in_date_range\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_test.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    model_for_test.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\n\n    y_forecast_combined = model_for_test.predict(X_1_train, X_2_train, max_lag).clip(0.0) # initializing with training set fit\n\n    dp_for_full_X1_test_date_range = dp_test.out_of_sample(steps=test_days)\n    for step in range(test_days):\n        dp_steps_so_far = dp_for_full_X1_test_date_range.loc[test_start_day:test_start_day+pd.Timedelta(days=step),:]\n\n        X_1_combined_dp_data = pd.concat([dp_test.in_sample(), dp_steps_so_far])\n        X_2_combined_data = pd.concat([store_sales_in_date_range,\n                                       store_data_in_test_range.loc[test_start_day:test_start_day+pd.Timedelta(days=step), :]])\n        X_1_test = make_X1_features(X_1_combined_dp_data, train_start_day, test_start_day+pd.Timedelta(days=step), is_test_set=True)\n        X_2_test = make_X2_features(X_2_combined_data\n                                    .drop('sales', axis=1)\n                                    .stack(['store_nbr', 'family']),\n                                    model_for_test.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    #     print(\"last 3 rows of X_1_test: \")\n    #     display(X_1_test.tail(3))\n    #     temp_test2 = X_2_test[(X_2_test.store_nbr == 1) & (X_2_test.family == 3)]\n    #     print(\"last 3 rows of X_2_test: \")\n    #     display(temp_test2.tail(3).apply(lambda s: truncateFloat(s)))\n\n        y_forecast_combined = pd.concat([y_forecast_combined,\n                                     model_for_test.predict(X_1_test, X_2_test, max_lag).clip(0.0).iloc[-1:]\n                                    ])   \n    #     print(\"last 3 rows of y_forecast_combined: \")\n    #     display(y_forecast_combined.tail(3).apply(lambda s: truncateFloat(s)))\n\n        y_plus_y_test = pd.concat([y_train, y_forecast_combined.iloc[-(step+1):]]) # add newly predicted (last step+1) rows of y_test_combined\n        model_for_test.fit1(X_1_test, y_plus_y_test, stack_cols=['store_nbr', 'family']) # fit on new combined X, y - note that fit prior to test date range will change slightly\n        model_for_test.fit2(X_2_test, max_lag, stack_cols=['store_nbr', 'family'])\n        print(\"finished forecast for\", test_start_day+pd.Timedelta(days=step))\n\n    display(y_forecast_combined[test_start_day:test_end_day])\n    y_forecast = pd.DataFrame(y_forecast_combined[test_start_day:test_end_day].clip(0.0), index=X_1_test.index, columns=y.columns)\n    print('\\nFinished creating test set forecast\\n')\n    \n    if type(model_for_test.model_2) == XGBRegressor:\n        pickle.dump(model_for_test.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","212dadee":"%%time\n# forecast 1 day at a time which allows use of lag features. Slow.\n# each new forecast causes y for all days in training and test (or validation set) to be reforecast.\nif hybrid_forecasting_type == \"day_by_day_refit_all_days\":\n    #initial fit on train portion of train\/test split\n    X_1_train, y_train, dp_test = make_X1_features(store_sales_in_date_range, full_train_start_day, full_train_end_day) # preparing X1 for hybrid part 1: LinearRegression\n    model_for_test.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family']) # fit1 before make_X2_features, since X2 may want to create lag features from model.y_resid\n    X_2_train = make_X2_features(store_sales_in_date_range\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_test.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    model_for_test.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\n    # y_full_train = model_for_test.predict(X_1_train, X_2_train, max_lag).clip(0.0) # do I need this line here?\n\n\n    dp_for_full_X1_test_date_range = dp_test.out_of_sample(steps=test_days)\n    for step in range(test_days):\n        dp_steps_so_far = dp_for_full_X1_test_date_range.loc[test_start_day:test_start_day+pd.Timedelta(days=step),:]\n\n        X_1_combined_dp_data = pd.concat([dp_test.in_sample(), dp_steps_so_far])\n        X_2_combined_data = pd.concat([store_sales_in_date_range,\n                                       store_data_in_test_range.loc[test_start_day:test_start_day+pd.Timedelta(days=step), :]])\n        X_1_test = make_X1_features(X_1_combined_dp_data, train_start_day, test_start_day+pd.Timedelta(days=step), is_test_set=True)\n        X_2_test = make_X2_features(X_2_combined_data\n                                    .drop('sales', axis=1)\n                                    .stack(['store_nbr', 'family']),\n                                    model_for_test.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    #     print(\"last 3 rows of X_1_test: \")\n    #     display(X_1_test.tail(3))\n    #     temp_test2 = X_2_test[(X_2_test.store_nbr == 1) & (X_2_test.family == 3)]\n    #     print(\"last 3 rows of X_2_test: \")\n    #     display(temp_test2.tail(3).apply(lambda s: truncateFloat(s)))\n\n        y_forecast_combined = model_for_test.predict(X_1_test, X_2_test, max_lag).clip(0.0) # generate y with \n\n    #     print(\"last 3 rows of y_forecast_combined: \")\n    #     display(y_forecast_combined.tail(3).apply(lambda s: truncateFloat(s)))\n\n        y_plus_y_test = pd.concat([y_train, y_forecast_combined.iloc[-(step+1):]]) # add newly predicted (last step+1) rows of y_test_combined\n        model_for_test.fit1(X_1_test, y_plus_y_test, stack_cols=['store_nbr', 'family']) # fit on new combined X, y - note that fit prior to test date range will change slightly\n        model_for_test.fit2(X_2_test, max_lag, stack_cols=['store_nbr', 'family'])\n        print(\"finished forecast for\", test_start_day+pd.Timedelta(days=step))\n\n    display(y_forecast_combined[test_start_day:test_end_day])\n\n    y_forecast = pd.DataFrame(y_forecast_combined[test_start_day:test_end_day].clip(0.0), index=X_1_test.index, columns=y.columns)\n    print('\\nFinished creating test set forecast\\n')\n    \n    if type(model_for_test.model_2) == XGBRegressor:\n        pickle.dump(model_for_test.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","527d3e0d":"# see example predictions (both validation and test data sets) for a specific store\/family:\n\nSTORE_NBR = '1'  # 1 - 54\nFAMILY = 'BEVERAGES' # display(store_sales.index.get_level_values('family').unique())\n\nax = y.loc(axis=1)[STORE_NBR, FAMILY].plot(**plot_params, figsize=(16, 4))\nax = y_pred.loc(axis=1)[STORE_NBR, FAMILY].plot(ax=ax, marker='.', color='red', markersize=12) # markers: big size for tiny validation sets (1-2 days)\nax = y_forecast.loc(axis=1)[STORE_NBR, FAMILY].plot(ax=ax, marker='.', color='orange', markersize=12)\nax.set_title(f'{FAMILY} Sales at Store {STORE_NBR}');","05acca7a":"# See:\n# 1) sales(y) - grey\n# 2) sales prediction on training set (y_fit) - blue\n# 3) sales prediction on validation set (y_pred) - red\n# 4) sales prediction on test set (y_forecast) - orange\n# for first n families of a single store where 1 <= n <= 33\n# NUM_FAMILIES = 2 gives you first 2 (Automotive and Baby Care)\n# NUM_FAMILIES = 33 gives you all families (Automotive through Seafood)\n\nNUM_FAMILIES = 33   # 1 - 33\nSTORE_NBR = '1'    # 1 - 54\n\ny_for_one_store = y.loc(axis=1)[STORE_NBR]\ny_fit_for_one_store = y_fit.loc(axis=1)[STORE_NBR]\ny_pred_for_one_store = y_pred.loc(axis=1)[STORE_NBR]\ny_forecast_for_one_store = y_forecast.loc(axis=1)[STORE_NBR]\nfamilies = y_for_one_store.columns[0:NUM_FAMILIES]\n\naxs = y_for_one_store.loc(axis=1)[families].plot(\n    subplots=True, sharex=True, figsize=(16, 3*NUM_FAMILIES), **plot_params, alpha=0.6,\n)\n_ = y_fit_for_one_store.loc(axis=1)[families].plot(subplots=True, sharex=True, color='blue', ax=axs)\n_ = y_pred_for_one_store.loc(axis=1)[families].plot(subplots=True, sharex=True, color='red', ax=axs, marker='.', markersize=9) # markers: for tiny validation sets (1-2 days)\n_ = y_forecast_for_one_store.loc(axis=1)[families].plot(subplots=True, sharex=True, color='orange', ax=axs, marker='.', markersize=9)\n\nfor ax, family in zip(axs, families):\n    ax.legend([])\n    ax.set_ylabel(family)\n    ax.set_title(f'{family} Sales at Store {STORE_NBR}')\n    ax.tick_params(labelbottom=True) # gets x-axis labeled under EVERY plot - without this it's just the bottom one\n    \n# blue = fit, red = predict (for validation range), orange = submission forecast","41ae5f13":"# creates submission file submission.csv\n\ny_submit = y_forecast.stack(['store_nbr', 'family'])\ny_submit = pd.DataFrame(y_submit, columns=['sales'])\ny_submit = y_submit.join(df_test.id).reindex(columns=['id', 'sales'])\ny_submit.to_csv('submission.csv', index=False)","e1e1cc8b":"y_submit.head()","94f7a844":"evaluation metric for Store Sales forecasting learning competition:\n$$\n\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}\n$$","c6d764d4":"## Test data ##\n\nWe use the identical method on the test data that we used on the validation data a few cells back.\n\nThere are many different possible forecasting methods. With this notebook you can choose one of three different methods (in variables section at the beginning), and they are applied the same way to the train\/validation split as they are to the train\/test split, to avoid data leakage.\n\nThe next 4 cells are:\n\n1. Preparation in common to all 3 methods.\n\n2. Direct method of forecasting. It's easy and fast. If you use it, comment out all lag features in make_X2_features.\n\n3. day_by_day_fixed_past method creates the forecast one day at a time so that the next day's lag can be calculated and used for following days. Each time a forecast is made, prior day forecasts are not touched - they are fixed.\n\n4. day_by_day_refit_all_days method also creates the forecast one day at a time so that the next day's lag can be calculated and used for following days. However, with each new forecast, the entire set of days (training days AND forecast days) are refitted. For a 16 day forecast, day one will therefore be forecast 16 different times, though the forecasts are only very slightly different with each iteration through the loop.\n\nPartly because the day-by-day methods are slow, a notification is given after each round is completed so you know something is happening.","bdfffd94":"## BoostedHybrid Class ##\n\nThe idea of Boosted Hybrids is to have the best of both worlds:\n\n* model 1: use time-based features to extrapolate long-term and seasonal trends (suitable: Linear Regression)\n* model 2: use any features (possibly some that are time-based), including lags, to find more complex interrelationships (suitable: XGB)\n\nThe BoostedHybrid class as originally presented in lesson 5 exercises needed modifications to work with forecasting one day at a time as explained in lesson 6.\n\nIf model 2 is going to have lag features based on the residuals as explained in lesson 6, then it needs access to residuals, y_resid. So y_resid is part of the BoostedHybrid class. We need to split the fit method into two methods, fit1 and fit2. We run fit1 on X1 to generate y_resid, which is then available to create lag features for X2. After X2 features are created (including lag features using y_resid), fit2 can run.\n\nAlso, we need a way to deal with the NaNs that get created from lag features. In lesson 6, this is done by aligning two dataframes (shifting created mismatched indices). But if you do one-step forecasting over and over, and you keep aligning, then you'll effectively drop rows with each forecasting step. Instead, we can delay dealing with the NaN rows until the last instant, in the fit2 method of BoostedHybrid. Then, we can ignore the rows instead of dropping them. By doing it this way, the rows can be reused over and over for things like rolling averages or rolling means, rather than throwing out the earliest chunk of the data set with each step.\n\nSo that's what the max_lag variable is for: it's passed as first_n_rows_to_ignore so that the appropriate number of rows are ignored in fit2 and predict methods.","7b0def85":"After loading imports\/configuration (above), it's convenient to set a few variables that will be used throughout the notebook, to reduce chance for careless errors and make it easier to conduct experiments:\n\n* **time variables** (which parts of time series data to use)\n* **choose models, max_lag**","46aea24f":"## Creating X2 Features ##\n\nAs explained in lesson 5, it works well to find long-term and seasonal trends using model 1, while model 2 captures more complex relationships with a more powerful algorithm such as XGBRegressor(). The cell below is where X2 features reside, demonstrating a number of different types of features suitable for model 2. Algorithms like XGBRegressor() require lots of features and lots of data to work well. So expect long run times if you add many features and use the latest 2 years (or more) of the training data.\n\nAgain, this is great place to add or subtract features to see what happens.","6fa24a73":"## Validation ##\n\nUsing the most recent 16 days of the training set to validate the model makes sense, since they are just before the 16-day test set to follow. If you want to try different time ranges, change the time variables at the beginning of the notebook as they are referenced in a few different places.\n\nThere are many different possible forecasting methods. With this notebook you can choose one of three different methods (in variables section at the beginning), and they are applied the same way to the train\/validation split as they are to the train\/test split, to avoid data leakage.\n\nThe next 4 cells are:\n\n1. Preparation in common to all 3 methods.\n\n2. Direct method of forecasting. It's easy and fast. If you use it, comment out all lag features in make_X2_features.\n\n3. day_by_day_fixed_past method creates the forecast one day at a time so that the next day's lag can be calculated and used for following days. Each time a forecast is made, prior day forecasts are not touched - they are fixed.\n\n4. day_by_day_refit_all_days method also creates the forecast one day at a time so that the next day's lag can be calculated and used for following days. However, with each new forecast, the entire set of days (training days AND forecast days) are refitted. For a 16 day forecast, day one will therefore be forecast 16 different times, though the forecasts are only very slightly different with each iteration through the loop.\n\nPartly because the day-by-day methods are slow, a validation score is given after each round so you know something is happening. Perhaps you will choose to abort the run if validation scores are much higher than usual. The day-by-day methods also display (a portion of) the final prediction on the validation set, as it can be useful to review these numbers after a notebook is saved\/committed.\n\nAs with any Kaggle competition, it's important to use an evaluation metric that is similar to the one used on the leaderboard. The validation metric used here is not identical, but it's close. Note that .clip(0.0) is needed before calling mean_squared_log_error, since you can't take a log of a negative number.","a67e811f":"## Visualizing Results ##\n\nIt can often take a while to figure out what's going on with your forecast. Visualizing the results of time series can be done intuitively and effectively with the following plots.\n\nThe first plot allows you to specify one store and family, and see what the validation and the test sets look like plotted against the original data. One glance at this plot is often enough to tell if you made a code mistake or if you added some features that made things much worse (or better).\n\nThe following plot allows you to see, for a single store, anywhere from 1 to 33 families plotted, with 3 different fits (train, validation, forecast). When you do runs that take a long time, you'll usually save\/commit and then go away. When you come back, all these plots are saved along with your results so you can visually get a sense of how your latest changes impacted your hybrid model's accuracy.","f816dd16":"## Unofficial Bonus Lesson for Ryan Holbrook's Time Series Course ##\n\n## Introduction ##\n\nImplementing Time Series lessons 5 and 6 on the competition data set is hard, as many of you have discovered. So hard, that few of us have done it. A bonus lesson could be helpful.\n\nThis is that bonus lesson.\n\nThis is not an official Kaggle offering. I am not associated with Kaggle staff and am learning right alongside of all the rest of you. I just really wanted a notebook like this, so I created it.\n\nI've spent a great deal of time and effort on the competitions associated with two (really great!) courses created by Ryan Holbrook:\n\n* [Feature Engineering](https:\/\/www.kaggle.com\/learn\/feature-engineering)\n* [Time Series](https:\/\/www.kaggle.com\/learn\/time-series)\n\nWhile you can read through the material and do the simple exercises in a couple days each, I think of Kaggle-style courses as an introduction giving you the minimum needed concepts and vocabulary to get started. You learn a lot more from applying what you learned to the accompanying competition data sets.\n\nThe [beautifully written code of the Feature Engineering bonus lesson](https:\/\/www.kaggle.com\/ryanholbrook\/feature-engineering-for-house-prices) makes it easy to jump right in and start applying every one of the course concepts to a complex data set. After I spent weeks experimenting with and expanding Ryan's bonus notebook, I published an [enhanced version of the feature engineering bonus lesson](https:\/\/www.kaggle.com\/filterjoe\/house-price-feature-engineering-using-only-xgboost).\n\nRyan does not currently have the time to create an official bonus lesson for the Time Series course (I asked). The intent of this unofficial bonus lesson notebook is the same as other Kaggle bonus lesson notebooks: to make it easy to jump in and start applying course concepts to a complex data set, without getting bogged down with days of coding\/debugging before you can even get started.\n\nIt took me weeks and extensive debugging to get boosted hybrids (as explained in lesson 5) working in conjunction with multistep forecasting (as explained in lesson 6) for this competition data. A lot of what made it difficult was the multi-indexed nature of the data, which required lots of stacking, unstacking, and in general making sure at every step, the data was in the right format for the next step.\n\nAs a side effect of debugging, I discovered two major data issues:\n\n* The onpromotion data was not correct for both the test and train data sets. I informed Ryan and he corrected onpromotion values on 11\/22\/21.\n* Chunks of missing data before June 2015 are probably too extensive for imputation to be helpful. Therefore, training based on data before July 2015 may hurt your model. (Example: examine the plot for Store 1 Produce sales in 2015)\n\nIn the rest of this notebook I explain how to implement for this competition all the concepts we all learned from the Time Series course lessons and exercises. I skipped most EDA as it is covered well in many public notebooks (I want this project to be focused on the harder parts, and small enough that I publish it and maintain\/improve it).\n\nI'm open to suggestions for how to improve this notebook. I am especially interested in learning if there are faster multi-step forecasting methods. This notebook can take anywhere from a few minutes to a few hours to run - if you use 2 or more years of data combined with many features, it takes hours.","4e726e80":"Then next two hidden cells include utility code Ryan developed for the course. Though none of the EDA code is used, it's here for convenience if you want to add your EDA work to your forked copy of this notebook.\n\n* plot style settings (from Time Series course)\n* utilities (from Time Series course)\n\nIn cases where I had to change a utility, I removed them from this cell and put the revised versions just before they're used.","dfa40dff":"The next cell loads some of the data again, but this time beginning to get the data in the right format for model building.","ac63dff9":"Any time a cell is slow to run, it's a nice idea to start the cell with %%time. This gives you an idea which of the slow cells are taking an especially long time to run, which is particularly useful when saving\/committing the notebook.\n\nThe next cell takes a couple minutes to run due to a time consuming MultiIndex.from_product operation. It's there in order to get Christmas days into the time index, as some algorithms to follow depend on the MuliIndex being complete with every possible date, family, and store.\n\nIf you never run your notebook with data prior to 12\/25\/16, you could replace the 2 lines with the simpler MultiIndex line that is currently commented out.","99026514":"## Getting Started ##\n\nJump right in and do the following (after forking this notebook):\n\n* Familiarize yourself with the two cells full of variables (just after imports)\n* To speed things up:\n    * Change full_train_start_day to a value that is just a few months earlier than full_train_end_day\n    * Set val_end_day to (2017, 8, 1)\n    * Optionally, to make it really fast, comment out all lag features and set hybrid_forecasting_type to \"direct\"\n* Run the notebook in interactive mode one cell at a time. As you're doing it:\n    * Add cells to examine variables\n    * Examine plots carefully (and rerun the plots on different store numbers and\/or families if you like)\n\nOnce you feel comfortable playing around with variables and plots, make this notebook your own by adding and subtracting features. Read the commentary and comments throughout the notebook to see where and how to add and subtract features.\n\nIf you get into trouble, you can make use of certain print\/display statements I used a lot when debugging, which are commented out by default.","6b82ea9b":"## Test the Feature Set ##\n\nThough the next few cells are not necessary, it is helpful after adding new features to run these cells in interactive mode first to see what happens. They are a lot faster to run then doing one-day-at-a-time forecasts that happen later in the notebook. You can get a quick read as to whether you introduced a bug and whether the features did what you expected them to do. Tabular displays are good in both interactive mode and when saving\/committing, as it makes it easy to see what features are present and if there are any glaring issues with them.\n\nAlso, this fit-the-train-data code is briefer and easier to understand than the more complicated code that follows for one-day-at-a-time forecasts.","3abd3efe":"## Calendar for Special Days ##\n\nKDJ2020 published a simple, effective method to process and prepare holiday data. The code below adapts KDJ2020's work into a format that fits the rest of the notebook.\n\nNote that regional data is thrown out. Modeling can potentially be improved by processing and making use of the regional data.\n\nAfter the processing we display the data to get a sense of what happened. None of the data from the NaN rows is used, so the NaNs don't matter.","fb6c6fcf":"Some of the exploratory data analysis you may want to do will be easier if data is summarized and supplemented with additional features. This next cell does that and may inspire you to create features to be included in the model.\n\nI did include one feature (coded below) in the model based on the distinction between old stores and new stores (old stores are ones that had sales in January 2013). Those familiar with the grocery industry know that new stores are not \"mature\" when first opened. They may have an opening week spike in sales, but thereafter will start with a lower sales rate than is typical for the store type. Sales at the new store then increases at a fast rate for the first few years. The sales growth rate gradually slows down, until a few years later the growth rate becomes similar to older, mature stores.","6bdaf363":"## Creating X1 Features ##\n\nAfter setting up the complicated BoostedHybrid class, we can create features. Deterministic Process does much of the time series features work (lessons 2 and 3). It has its own function and cell. You can add more Fourier terms of different types, and you can see in the comments a brief reference to the legal values, as well as a link to more information.\n\nThere are many other possible time features to create that don't use Deterministic Process. The next cell demonstrates a few possibilities, including use of the holidays-inspired calendar. Add or subtract time-related features and see what happens!"}}