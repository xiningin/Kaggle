{"cell_type":{"6c56d381":"code","efca5dfe":"code","a3554e75":"code","2617f47b":"code","b6ab3a9e":"code","9d0ee73a":"code","71dd57a5":"code","e25d13eb":"code","e73d7578":"code","98e47845":"code","8bbc2009":"markdown","7e45b981":"markdown","a86caaa2":"markdown","2052dded":"markdown","3ad62563":"markdown","ceba410f":"markdown","79db09ca":"markdown","adeaa12c":"markdown","58cdb5a6":"markdown"},"source":{"6c56d381":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","efca5dfe":"fit, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nX, y = make_blobs(n_samples=20, n_features=2, centers=2, random_state=0)\nax[0].scatter(X[:, 0], X[:, 1], c=np.array(['r', 'g'])[y])\nax[0].title.set_text('Binary classification, only 2 classes')\n\nX, y = make_blobs(n_samples=20, n_features=2, centers=3, random_state=0)\nax[1].scatter(X[:, 0], X[:, 1], c=np.array(['r', 'g', 'b'])[y]);\nax[1].title.set_text('Multiclass, 3 classes')","a3554e75":"fit, ax = plt.subplots(1, 3, figsize=(15, 5))\n\nX, y = make_blobs(n_samples=20, n_features=2, centers=3, random_state=0)\nax[0].scatter(X[:, 0], X[:, 1], c=np.array(['r', 'black', 'black'])[y]);\nax[0].title.set_text('Red class vs green and blue')\n\nX, y = make_blobs(n_samples=20, n_features=2, centers=3, random_state=0)\nax[1].scatter(X[:, 0], X[:, 1], c=np.array(['black', 'g', 'black'])[y]);\nax[1].title.set_text('Green class vs red and blue')\n\nX, y = make_blobs(n_samples=20, n_features=2, centers=3, random_state=0)\nax[2].scatter(X[:, 0], X[:, 1], c=np.array(['black', 'black', 'b'])[y]);\nax[2].title.set_text('Blue class vs red and green')","2617f47b":"class LogisticRegression:\n    \n    def __init__(self, lr=0.1, n_iters=10000):\n        self.lr = lr\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n    \n    def fit(self,X,y):\n        #init parameters\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        #gradient descent\n        for _ in range(self.n_iters):\n            linear_model = X @ self.weights + self.bias\n            hx = self._sigmoid(linear_model)\n            \n            dw = (X.T * (hx - y)).T.mean(axis=0)\n            db = (hx - y).mean(axis=0)\n\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db \n\n    def predict(self,X):\n        linear_model = np.dot(X,self.weights) + self.bias\n        y_predicted = self._sigmoid(linear_model)\n        return y_predicted\n  \n    def _sigmoid(self,x):\n        return(1\/(1+np.exp(-x)))","b6ab3a9e":"class MulticlassClassification:\n    \n    def __init__(self):\n        self.models = []\n\n    def fit(self, X, y):\n        \"\"\"\n        Fits each model\n        \"\"\"\n        for y_i in np.unique(y):\n            # y_i - positive class for now\n            # All other classes except y_i are negative\n\n            # Choose x where y is positive class\n            x_true = X[y == y_i]\n            # Choose x where y is negative class\n            x_false = X[y != y_i]\n            # Concatanate\n            x_true_false = np.vstack((x_true, x_false))\n\n            # Set y to 1 where it is positive class\n            y_true = np.ones(x_true.shape[0])\n            # Set y to 0 where it is negative class\n            y_false = np.zeros(x_false.shape[0])\n            # Concatanate\n            y_true_false = np.hstack((y_true, y_false))\n\n            # Fit model and append to models list\n            model = LogisticRegression()\n            model.fit(x_true_false, y_true_false)\n            self.models.append([y_i, model])\n\n\n    def predict(self, X):\n        y_pred = [[label, model.predict(X)] for label, model in self.models]\n\n        output = []\n\n        for i in range(X.shape[0]):\n            max_label = None\n            max_prob = -10**5\n            for j in range(len(y_pred)):\n                prob = y_pred[j][1][i]\n                if prob > max_prob:\n                    max_label = y_pred[j][0]\n                    max_prob = prob\n            output.append(max_label)\n\n        return output","9d0ee73a":"X, y = make_blobs(n_samples=20, n_features=2, centers=3, random_state=0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\nplt.scatter(X[:, 0], X[:, 1], c=np.array(['r', 'g', 'b'])[y]);","71dd57a5":"model = MulticlassClassification()\n\nmodel.fit(X_train, y_train)\n\naccuracy_score(y_test, model.predict(X_test))","e25d13eb":"data = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\n\ndata","e73d7578":"X, y = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']], data['Species']\n\nlabel_encoding = LabelEncoder()\ny = label_encoding.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","98e47845":"clf = MulticlassClassification()\n\nclf.fit(X_train, y_train)\n\naccuracy_score(y_test, clf.predict(X_test))","8bbc2009":"# Logistic Regression","7e45b981":"Now we want to learn our model for each of these classification problems. For class that we are trying to predict we assign positive class, 1, in case of first plot it will be red class, and others will be negative class, 0.\n\nSo to summarize, what we will be doing is, fitting three classifiers, and than for each X value we will predict probabilty of how this X value is similar to class Red, class Blue and class Green. After this, we will look at the higher probablity and assign class with higher probablity! Simple!","a86caaa2":"<div style='text-align: center'>\n    <img src='https:\/\/i.postimg.cc\/YCj184YN\/multiclass-svm1.png' width='500' \/>\n<\/div>","2052dded":"# Iris Dataset \ud83c\udf37","3ad62563":"<h1 style='text-align: center'>Multiclass Classification From Scratch<\/h1>\n\n<p  style='text-align: center'>\nThis notebook is in <span style='color: green; font-weight: 700'>Active<\/span> state of development! Check out this notebook to see some updates as I update new stuff as oftern as I learn it!\n<a style='font-weight:700' href='https:\/\/github.com\/LilDataScientist\/Multiclass-Classification'> Code on GitHub! <\/a><\/p>","ceba410f":"# Binary classification VS multiclass classification\n\nBinary classification are those tasks where examples are assigned exactly one of two classes. Multi-class classification is those tasks where examples are assigned exactly one of more than two classes","79db09ca":"# Multiclass Classification","adeaa12c":"# Testing data on sample dataset","58cdb5a6":"# Intuition behind Multiclass Classification algorithm \ud83c\udf47\n\nWe already know how to separate 2 classes using LogisticRegression. If you don't you better know how to deal with it! What we're going to do is take our training set and turn this into three separate binary classification problems. We'll turn this into three separate two class classification problems."}}