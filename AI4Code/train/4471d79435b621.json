{"cell_type":{"4460da24":"code","b6d68665":"code","6bd6411c":"code","40ea26a5":"code","cd5f82d0":"code","b8810bca":"code","285bde48":"code","2220ea4c":"code","7ec10100":"code","dae79fe2":"code","53ed24e7":"code","15f537da":"code","1b0035fc":"code","df5f4af9":"markdown","92773615":"markdown","7e75da29":"markdown","6ecac043":"markdown","074da612":"markdown","38929de4":"markdown","0f82d336":"markdown"},"source":{"4460da24":"!wget https:\/\/s3.amazonaws.com\/fast-ai-imageclas\/cifar10.tgz\n!mkdir data\n!tar zxf cifar10.tgz -C data\/","b6d68665":"# | Some global constants\nLEARNING_RATE = 3e-4\nBATCH_SIZE = 32\nIMAGE_DIM = 128\nNR_EPOCHS = 3\n\ndataset_directory = 'data\/cifar10'","6bd6411c":"from fastai import *\nfrom fastai.vision import *","40ea26a5":"data = (ImageList.from_folder(dataset_directory)\n                 .split_by_folder(train='train', valid='test')\n                 .label_from_folder()\n                 .transform(None, size=IMAGE_DIM)\n                 .databunch(bs=64)\n                 .normalize(imagenet_stats)\n      )","cd5f82d0":"learner = cnn_learner(data, models.resnet50, metrics=[accuracy], train_bn=False, wd=False, true_wd=False, bn_wd=False)\nlearner.fit(NR_EPOCHS, lr=LEARNING_RATE)","b8810bca":"learner = cnn_learner(data, models.resnet50, metrics=[accuracy], train_bn=True, wd=False, true_wd=False, bn_wd=False)\nlearner.fit(NR_EPOCHS, lr=LEARNING_RATE)","285bde48":"import tensorflow as tf\nfrom tensorflow import keras\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nimport pathlib\nimport os\nfrom functools import partial\nimport random","2220ea4c":"data_root = pathlib.Path(dataset_directory)\n\n# | Create dictionary for mapping classnames (strings) to integer\nclass_names = sorted([e.name for e in (data_root\/'train').iterdir() if e.is_dir()])\nclass_names_to_label = {name: label for label, name in enumerate(class_names)}\nlabel_to_class_names = {label: name for label, name in enumerate(class_names)}\n\n# | Get all paths of train & test images\ntrain_image_paths, train_image_labels, test_image_paths, test_image_labels = [], [], [], []\nall_image_paths = list(data_root.glob('*\/*\/*.png'))\nrandom.shuffle(all_image_paths)\n\nfor path in all_image_paths:\n    if path.parent.parent.name == 'train':\n        train_image_paths.append(str(path))\n        train_image_labels.append(class_names_to_label[path.parent.name])\n        \n    elif path.parent.parent.name == 'test':\n        test_image_paths.append(str(path))\n        test_image_labels.append(class_names_to_label[path.parent.name])","7ec10100":"def load_and_preprocess_image(image_path, label, img_shape):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.image.resize(image, img_shape)\n    image = keras.applications.resnet_v2.preprocess_input(image)\n    label = tf.cast(label, tf.float32)\n    \n    return image, label\n\n\ndef create_dataset(file_paths, labels, epochs=1, batch_size=64, buffer_size=10000, train=True):\n    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n    dataset = dataset.map(partial(load_and_preprocess_image, img_shape=(IMAGE_DIM, IMAGE_DIM)), num_parallel_calls=AUTOTUNE)\n    if train:\n        #dataset = dataset.cache(filename='.\/cache.tf-data')\n        dataset = dataset.shuffle(buffer_size=buffer_size)\n    dataset = dataset.repeat(epochs)\n    dataset = dataset.batch(batch_size)\n    if train:\n        dataset = dataset.prefetch(AUTOTUNE)\n    \n    return dataset\n\ndef create_model(train_bn=False, dropout=0):\n    base_model = keras.applications.resnet.ResNet50(weights=\"imagenet\", include_top=False)\n    \n    if train_bn:\n        for layer in base_model.layers:\n            if layer.__class__.__name__ != \"BatchNormalization\":\n                layer.trainable = False\n    \n    avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n    mx = keras.layers.GlobalMaxPooling2D()(base_model.output)\n    out = tf.keras.layers.Concatenate()([avg, mx])\n    out = keras.layers.BatchNormalization()(out)\n    out = keras.layers.Dropout(dropout)(out)\n    out = keras.layers.Dense(512, activation=\"relu\")(out)\n    out = keras.layers.BatchNormalization()(out)\n    out = keras.layers.Dropout(dropout)(out)\n    out = keras.layers.Dense(10, activation=\"softmax\")(out)\n    \n    model = keras.models.Model(inputs=base_model.input, outputs=out)\n    \n    optimizer = tf.keras.optimizers.Adam(lr=0.003)\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n    \n    return model","dae79fe2":"steps_per_epoch = len(train_image_paths) \/\/ BATCH_SIZE\nvalidation_steps = len(test_image_paths) \/\/ BATCH_SIZE\n\ntrain_dataset = create_dataset(train_image_paths, train_image_labels, epochs=-1, batch_size=BATCH_SIZE, train=True)\nvalidation_dataset = create_dataset(test_image_paths, test_image_labels, epochs=-1, batch_size=BATCH_SIZE, train=False)","53ed24e7":"model = create_model()\n\nhistory = model.fit(train_dataset,\n                    epochs = NR_EPOCHS,\n                    steps_per_epoch = steps_per_epoch,\n                    validation_data = validation_dataset, \n                    validation_steps = validation_steps,\n                    callbacks = [])","15f537da":"model = create_model(train_bn=True)\n\nhistory = model.fit(train_dataset,\n                    epochs = NR_EPOCHS,\n                    steps_per_epoch = steps_per_epoch,\n                    validation_data = validation_dataset, \n                    validation_steps = validation_steps,\n                    callbacks = [])","1b0035fc":"!rm -rf data","df5f4af9":"### Without unfreezing BatchNorm Layers","92773615":"# CNN Transfer-Learning Secrets\nMost resources I have come across about Transfer Learning in computer vision in my opinion leave out some very essential techniques needed to get descent results.\n\nI myself realized this when I started to experiment with fast.ai, a library that focuses strongly on Transfer Learning. Comparing the results I got from my TensorFlow 2 models, I was able to identify some interesting tricks that are used in the fast.ai library, which are not well known, but can improve the results of your Transfer Learning models quite significantly.\n\nProbably few people in the world have done as much research in the Transfer Learning domain as Jeremy Howard (founder of fast.ai). Even if you don't use fast.ai in your work, the library is a treasure trove for clever tricks and methods.\n\n\n**<h3>Transfer Learning explanation<\/h3>**\n\nThe idea of Transfer Learning is to use models that have been pre-trained on big datasets such as ImageNet and apply them to new tasks. This way we don\u2019t have to retrain the complete model from scratch, because it already has acquired extensive general knowledge about images.\n\nIn the case of CNN (Convolutional Neural Networks) based models, the first layers usually are mainly convolutional & pooling layers, which act as visual feature extractors, followed by a set of dense layers (I will call this the \u201chead\u201d of the model in the following), which can be trained e.g. to perform a classification task.\n\n**<h3>General Methodology<\/h3>**\n\nThe general methodology we usually use in Transfer Learning is as follows:\n\n1. Take a pretrained model, remove its head and replace it with a new head. \n2. Train only the new head of the model on the new task (while freezing all the parameters in the pretrained layers of the model) \n3. Fine-tuning of the complete model on the new task. Here we have different options:\n    * a) Unfreezing all layers and fine-tune the whole model at once. (This can often lead to poor results, as we might change the lower layers that extract very general visual features too much. They are usually already very good, as they were pre-trained on a huge dataset)\n    * b) Only unfreezing the last few convolutional and pooling layers, which are said to extract more task-specific features than the first few layers of the CNN.\n    * c) Same as a) but using lower learning rates for the lower layers (which extract general visual features) and higher learning rates for the upper and thus more application-specific layers (this method is also referred to as  \u201c<b>discriminative learning rates<\/b>\u201d). \n\n\n**<h3>Findings<\/h3>**\n\nIn my experiments I used a resnet-50 architecture pretrained on ImageNet, and used Transfer Learning to retrain the model on the CIFAR-10 dataset. In the following experiments I upscaled the CIFAR-10 images from 32x32 to 128x128, for reasons outlined below.\n\n**1. Don't freeze the BatchNorm Layers!**\n\nIt was during step 2. as outlined in the section \u201cGeneral Methodology\u201d above where I made my most interesting finding. While using fast.ai I immediately got test-accuracies around 94% when only training the head of the model on the new dataset, my TensorFlow model achieved much worse results (60% accuracy).\nI identified the main reason for this discrepancy to be the fact that the fast.ai library by default doesn\u2019t freeze the BatchNorm layers of the pretrained model, when training the new model head. When I repeated the TensorFlow experiments without freezing the BatchNorm layers, I immediately got much better results and comparable to those of fast.ai. \nIn conclusion, not freezing the BatchNorm layers while training the model head during Transfer Learning seems to be absolutely crucial to get decent results.\n\nI still get slightly worse results with my TensorFlow implementation (92% accuracy v.s. 94.3% in fast.ai after 3 epochs), probably for one of the following reasons:\n* Some hidden tricks in fast.ai library\n* Pretrained PyTorch resnet-50 model works better than pre-trained TensorFlow model in this particular experiment\n* Different implementation of the BatchNorm layers https:\/\/github.com\/keras-team\/keras\/pull\/9965\n* Another reason I\u2019m not thinking about, but you might (in that case, please share). :)\n\n**2. Image Dimensions matter, but not as much as you might think** \n\nIt is often recommended to use the same image dimensions as used for the pretrained model. Models pre-trained on ImageNet often have been trained with 224x224 images. This is a bit unfortunate, because many vision tasks you can also solve with lower resolution images (e.g. the CIFAR-10 images are only 32x32), and obviously training is much faster with smaller images. \n\nI Indeed didn\u2019t get good results when training the new model head using the original 32x32 CIFAR-10 images (71% after 3 epochs). The difference to 224x224 seems to be too large. When I upscaled the images to 64x64, accuracy immediately jumped up to 89% with an epoch duration 1min 01sec.\n\nUpscaling to 128x128 even improved the results to 94.3% with an epoch-time of 1:42, which is actually already in state-of-the-art range on CIFAR-10 using a regular ResNet architecture (https:\/\/paperswithcode.com\/sota\/image-classification-on-cifar-10).\n\nUpscaling to 224x224 didn't improve the accuracy further while taking more than twice the time for training (03:41).\n\nIt's interesting to see that the pretrained feature extraction layers, which were only trained on 224x224 images, in fact work seem to extract reasonably good features from our 32x32 images which we just upscaled!\n\nThe bottomline here is: It is not always necessary to use the exact same image dimensions as were used to pretrain the model. Usually smaller images work also well, however, the difference of the dimensions shouldn\u2019t be too big! \n\n**3. Discriminative learning rates**\n\nSo far all findings refer to step 2. of the \u201cgeneral methodology\u201d outlined above, where we only trained the head of our model. It turns out that when using a dataset which is similar to the dataset used to pretrain the model (i.e. ImageNet v.s. CIFAR-10), this can already be sufficient, and it might not even be necessary to fine-tune the rest of the model on the new dataset.\n\nHowever, you will often encounter the case, where your new dataset differs too much from ImageNet, and to get the most out of your model, you will have to do some fine-tuning of the remaining layers. \n\nWhile in fast.ai this works very well out of the box, using discriminative learning rates (as described above), I generally get much worse results in TensorFlow, when I retrain the complete model with a constant learning rate. \n\nTo the best of my knowledge, discriminative learning rates are not yet available out of the box in TensorFlow\/Keras. On GitHub I\u2019ve found the keras-lr-multiplier repository https:\/\/github.com\/CyberZHG\/keras-lr-multiplier which could be used to implement discriminative learning rates for Transfer Learning in TensorFlow. However, I haven\u2019t tried this yet. If you have used discriminative learning rates successfully in TensorFlow, it would be great if you could share your results here.\n\nNote: Fine-tuning the complete model using discriminative learning rates is not shown in this Kernel. However, in fast.ai that would be only 2 lines of code.\n\n**<h3>About this Kernel<\/h3>**\nThis kernel mainly shows the positive effect of not freezing the BatchNorm layers while training the head of the model during the Transfer Learning process.<br>\nTo reproduce the findings with respect to the image dimensions, you can simply play around with the <code>IMAGE_DIM<\/code> variable at the beginning of the kernel.<br>\nImplementations in fast.ai and TensorFlow 2 are provided.\n\n<br>\nIf you know of other useful tricks that can be used in Transfer Learning, or if you have useful feedback regarding my own findings, please share them here. \n\n<font color=\"red\">Please upvote if you find this information useful :)<\/font>","7e75da29":"### With unfreezing BatchNorm Layers","6ecac043":"## 1. Fast.ai","074da612":"### Without unfreezing BatchNorm Layers","38929de4":"## 2. TensorFlow","0f82d336":"### With unfreezing BatchNorm Layers"}}