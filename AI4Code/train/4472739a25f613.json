{"cell_type":{"f96db66f":"code","6c015c05":"code","f6281acd":"code","18247a1a":"code","10dd33b4":"code","9abba24d":"code","89e99daf":"code","7092fa5c":"code","a7edf58e":"code","7d44db96":"code","eb2220af":"code","b94328f3":"code","45487abd":"code","c0835f03":"code","4a3cd9e7":"code","e1dfcde6":"code","889d73d0":"code","7c65d3e0":"code","9821918d":"code","c1f7c62a":"code","df56a9f1":"code","dcad7cde":"code","3af7508f":"code","c300cb17":"code","872d36f5":"code","8fb8bcc8":"code","1f8c4f51":"code","e8b5d856":"code","91da4019":"code","25a634ea":"code","0618b34a":"code","1061d5ca":"code","2e98f996":"code","6ac82a87":"code","89416669":"code","35e7646a":"code","699f11bd":"code","93e34509":"markdown","11a02a44":"markdown","5dae2127":"markdown","d67ced38":"markdown","26fb0272":"markdown","53283c23":"markdown","4806596e":"markdown","81784199":"markdown","8f5eaeef":"markdown","0f0177ba":"markdown","089d7520":"markdown","529af6c4":"markdown"},"source":{"f96db66f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, precision_score\nfrom sklearn.metrics import recall_score, f1_score, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c015c05":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain = train.drop([\"Name\",\"Ticket\",\"Cabin\"],axis=1)\ntrain.head()\nage = train.groupby(['Sex', 'Pclass'])['Age'].agg(['mean', 'median']).round(1)\ntrain_null = train[train[\"Age\"].isnull()]\nage\nfor x in range(train.shape[0]):\n    if(pd.isnull(train.iloc[x,4])):\n        train.iloc[x,4] = age.loc[train.iloc[x,3],train.iloc[x,2]][0].astype('int')\ntrain.isnull().sum()","f6281acd":"sns.set(color_codes=True)\nsns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", kind=\"bar\", data=train);","18247a1a":"sns.distplot(train[\"Fare\"], kde=False, rug=True);","10dd33b4":"sns.distplot(train[\"Age\"], kde=False, rug=True);","9abba24d":"sns.heatmap(train.corr(), annot=True)\n#plt.tight_layout()","89e99daf":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\nage = train.groupby(['Sex', 'Pclass'])['Age'].agg(['mean', 'median']).round(1)\nfor x in range(train.shape[0]):\n    if(pd.isnull(train.iloc[x,5])):\n        train.iloc[x,5] = age.loc[train.iloc[x,4],train.iloc[x,2]][0].astype('int')\n\ntrain['Family']=train[\"SibSp\"]+train[\"Parch\"]+1\ntrain['Fam_type'] = pd.cut(train.Family, [0,1,4,7,11], labels=[1,2,3,4])\n\ntrain['Title'] = train['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntrain['Title'].replace(['Mme', 'Ms', 'Lady', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)\ntrain['Title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer'], 'Mr', inplace=True)\n\n\ntrain = train.drop([\"Name\",\"Ticket\",\"Cabin\",\"PassengerId\",\"SibSp\",\"Parch\",\"Family\"],axis=1)\ntrain.loc[train.isnull().loc[:,\"Embarked\"],\"Embarked\"] = \"S\"\ntrain[\"Fare\"] = pd.qcut(train[\"Fare\"], 5, labels=[1,2,3,4,5]).astype(int)\ntrain[\"Age\"] = pd.qcut(train[\"Age\"], 5, labels=[1,2,3,4,5]).astype(int)\ntrain = pd.get_dummies(train, columns=[\"Sex\",\"Pclass\",\"Embarked\",\"Title\"])\ntrain.head()","7092fa5c":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\npassangersID = test[\"PassengerId\"]\n\nage = test.groupby(['Sex', 'Pclass'])['Age'].agg(['mean', 'median']).round(1)\nfor x in range(test.shape[0]):\n    if(pd.isnull(test.iloc[x,4])):\n        test.iloc[x,4] = age.loc[test.iloc[x,3],test.iloc[x,1]][0].astype('int')\n\ntest['Family']=test[\"SibSp\"]+test[\"Parch\"]+1\ntest['Fam_type'] = pd.cut(test.Family, [0,1,4,7,11], labels=[1,2,3,4])\n\ntest['Title'] = test['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntest['Title'].replace(['Mme', 'Ms', 'Lady', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)\ntest['Title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer'], 'Mr', inplace=True)\n\ntest.loc[test.isnull().loc[:,\"Embarked\"],\"Embarked\"] = \"S\"\ntest.loc[test.isnull().loc[:,\"Fare\"],\"Fare\"] = 7.75 # mode\n\ntest[\"Fare\"] = pd.qcut(test[\"Fare\"], 5, labels=[1,2,3,4,5]).astype(int)\ntest[\"Age\"] = pd.qcut(test[\"Age\"], 5, labels=[1,2,3,4,5]).astype(int)\n\ntest = pd.get_dummies(test, columns=[\"Sex\",\"Pclass\",\"Embarked\",\"Title\"])\n\ntest = test.drop([\"Name\",\"Ticket\",\"Cabin\",\"PassengerId\",\"SibSp\",\"Parch\",\"Family\"],axis=1)\ntest.head()","a7edf58e":"y = train[\"Survived\"]\nX = train.drop(\"Survived\",axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nX_test2= test","7d44db96":"#scaler = MinMaxScaler()\n#X_train = scaler.fit_transform(X_train)\n#X_test = scaler.transform(X_test)\n#X_test2 = scaler.transform(X_test2)","eb2220af":"knn = KNeighborsClassifier(n_neighbors = 6)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)","b94328f3":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nprediction = knn.predict(X_test)\n\n# Accuracy = TP + TN \/ (TP + TN + FP + FN)\n# Precision = TP \/ (TP + FP)\n# Recall = TP \/ (TP + FN)  Also known as sensitivity, or True Positive Rate\n# F1 = 2 * Precision * Recall \/ (Precision + Recall) \nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, prediction)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, prediction)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, prediction)))\nprint('F1: {:.2f}'.format(f1_score(y_test, prediction)))\nprint(classification_report(y_test, prediction, target_names=['dead', 'survived']))","45487abd":"prediction = knn.predict(X_test2)\noutput = pd.DataFrame({'PassengerId': passangersID, 'Survived': prediction})","c0835f03":"output.to_csv('KNeigborsClassifier.csv', index=False)\nprint(\"Your submission was successfully saved!\")","4a3cd9e7":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(C=0.1).fit(X_train, y_train)\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'\n     .format(clf.score(X_test, y_test)))","e1dfcde6":"prediction = clf.predict(X_test2)\noutput = pd.DataFrame({'PassengerId': passangersID, 'Survived': prediction})","889d73d0":"output.to_csv('LogicalRegression.csv', index=False)\nprint(\"Your submission was successfully saved!\")","7c65d3e0":"#poly = PolynomialFeatures(degree=2)\n#X_train, X_test, y_train, y_test = train_test_split(X, y)\n#X_train_scaled = scaler.fit_transform(X_train)\n#X_test_scaled = scaler.transform(X_test)\n#X_train_scaled_poly = poly.fit_transform(X_train_scaled)\n#X_test_scaled_poly = poly.fit_transform(X_test_scaled)","9821918d":"#z = list()\n#for name in names1:\n#    z.append(name.split(\",\")[1].split(\".\")[0])\n#t = pd.DataFrame(z)\n#t = t.astype('category')\n#train[\"Name\"] = t\n#train.head()","c1f7c62a":"parameters = {'criterion': ['entropy', 'gini'],\n              'min_samples_split': [5*x for x in range(1,15,2)],\n              'min_samples_leaf': [2*x+1 for x in range(14)],\n              'max_leaf_nodes': [2*x for x in range(1, 9)],\n              'max_depth': [2*x for x in range(1,9)]}\ngrid_search = GridSearchCV(DecisionTreeClassifier(), param_grid=parameters, cv=3)\ngrid_search.fit(X_train, y_train)\nprint(grid_search.best_params_)","df56a9f1":"def calculate_metrics(model, X_test, y_test):\n    pred = model.predict(X_test)\n    cm = confusion_matrix(y_test, pred)\n    acc = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test, pred)\n    f_score = f1_score(y_test, pred)\n    print('Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1_score: {}'.format(\n        acc, precision, recall, f_score))\n    return cm","dcad7cde":"best_model = DecisionTreeClassifier(**grid_search.best_params_)\nbest_model.fit(X_train, y_train)\n\ncm = calculate_metrics(best_model, X_test, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.ylabel('True label')\nplt.xlabel('Predicted label');","3af7508f":"clf = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(clf.score(X_test, y_test)))","c300cb17":"prediction = clf.predict(X_test2)\noutput = pd.DataFrame({'PassengerId': passangersID, 'Survived': prediction})","872d36f5":"output.to_csv('DecisionTree.csv', index=False)\nprint(\"Your submission was successfully saved!\")","8fb8bcc8":"clf = SVC(C=0.1).fit(X_train, y_train)\nprint(clf.score(X_train,y_train))\nprint(clf.score(X_test,y_test))","1f8c4f51":"clf = SVC(kernel = 'poly', degree = 3).fit(X_train, y_train)\nprint(clf.score(X_train,y_train))\nprint(clf.score(X_test,y_test))","e8b5d856":"best = list()\nbest.append(0)\nfor mygamma in [0.1 , 1 , 10, 100]:\n    for myC in [0.1,1 , 10 , 100]:\n        clf = SVC(kernel = 'rbf', gamma=mygamma,C=myC).fit(X_train, y_train)\n        print(\"gamma: \" +str(mygamma)+\", C: \"+str(myC))\n        print(clf.score(X_train,y_train))\n        print(clf.score(X_test,y_test))\n        ","91da4019":"clf = SVC(kernel = 'rbf',C=1,gamma=0.1).fit(X_train, y_train)\nprint(clf.score(X_train,y_train))\nprint(clf.score(X_test,y_test))","25a634ea":"prediction = clf.predict(X_test2)\noutput = pd.DataFrame({'PassengerId': passangersID, 'Survived': prediction})\noutput.to_csv('KernelizedSVM.csv', index=False)\nprint(\"Your submission was successfully saved!\")","0618b34a":"clf = RandomForestClassifier(max_depth=4,n_estimators=10000).fit(X_train, y_train)\nprint(clf.score(X_train,y_train))\nprint(clf.score(X_test,y_test))","1061d5ca":"prediction = clf.predict(X_test2)\noutput = pd.DataFrame({'PassengerId': passangersID, 'Survived': prediction})\noutput.to_csv('RandomTreeClassifier.csv', index=False)\nprint(\"Your submission was successfully saved!\")","2e98f996":"gbrt = GradientBoostingClassifier(n_estimators=700,max_depth=3,learning_rate=0.004).fit(X_train, y_train)\nprint(gbrt.score(X_train,y_train))\nprint(gbrt.score(X_test,y_test))","6ac82a87":"prediction = gbrt.predict(X_test2)\noutput = pd.DataFrame({'PassengerId': passangersID, 'Survived': prediction})\noutput.to_csv('GradientBoostingClassifier.csv', index=False)\nprint(\"Your submission was successfully saved!\")","89416669":"LogicalRegression = pd.read_csv(\"\/kaggle\/working\/LogicalRegression.csv\")\nDecisionTree = pd.read_csv(\"\/kaggle\/working\/DecisionTree.csv\")\nKernelizedSVM = pd.read_csv(\"\/kaggle\/working\/KernelizedSVM.csv\")\nRandomTreeClassifier = pd.read_csv(\"\/kaggle\/working\/RandomTreeClassifier.csv\") \nGradientBoostingClassifier = pd.read_csv(\"\/kaggle\/working\/GradientBoostingClassifier.csv\")","35e7646a":"output = LogicalRegression\noutput[\"sum\"] = LogicalRegression[\"Survived\"] +DecisionTree[\"Survived\"]+KernelizedSVM[\"Survived\"] \n+RandomTreeClassifier[\"Survived\"] +GradientBoostingClassifier[\"Survived\"]\n\noutput[\"prediction\"] =0\noutput.loc[output[\"sum\"]>2,\"prediction\"] =1\noutput = output.drop([\"sum\"],axis=1)\noutput[\"Survived\"] = output[\"prediction\"]\noutput = output.drop(\"prediction\",axis=1)","699f11bd":"output.to_csv('MajorityVote.csv', index=False)\nprint(\"Your submission was successfully saved!\")","93e34509":"## KNeighborsClassifier","11a02a44":"#### Hello , in this notebook I want to check if different machine learning algorithms get really different score values or if the score is dependant only on how we treat the features.","5dae2127":"### Logistic Regression","d67ced38":"#### Trying Polynomial Features \/or not","26fb0272":"### decision tree","53283c23":"### GradientBoostingClassifier","4806596e":"### RandomTreeClassifier","81784199":"#### Logistic Regression","8f5eaeef":"#### feature normalization","0f0177ba":"### Kernelized Support Vector Machines","089d7520":"### Majority vote","529af6c4":"#### last one"}}