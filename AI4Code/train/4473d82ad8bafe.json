{"cell_type":{"3796f741":"code","5afba076":"code","e2e3391c":"code","4f6f3864":"code","49166f61":"code","9ff0e0a6":"code","b0a43e26":"code","5c5ad969":"code","e2564488":"code","43b3f785":"code","d34a0a62":"code","37c5a0c8":"code","8a01bd27":"code","b0064ada":"code","f8cfe763":"code","7a8369d9":"code","62a65821":"code","d56b3543":"code","758a3d21":"code","edff1cb0":"code","5e611479":"code","ee87fa8d":"code","3f3ab44f":"code","b2eacf68":"code","241650b0":"code","b6c4999a":"code","3fd0590f":"code","46dcd457":"code","e027d2a1":"code","41e06cab":"code","22d174e1":"code","cb7a6b38":"code","0fa635ba":"code","28390d11":"code","e13b6199":"code","249d400a":"code","60e69a66":"code","b790d711":"code","9bc6f61f":"code","fa4f6cd9":"code","a8384f4b":"code","b4eac827":"code","45b24d50":"code","0885fdab":"code","064d997c":"code","a2fbbeef":"code","7aa54d5e":"code","cd2ef940":"code","77e67976":"code","de0dc25d":"code","4a499b3a":"code","97efe5d0":"code","6d3c413f":"markdown","b7b3152d":"markdown","f19b2e36":"markdown","213fc3f4":"markdown","d00422fe":"markdown","d1fbbb62":"markdown","daee5379":"markdown","6494aeaf":"markdown"},"source":{"3796f741":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nfrom IPython.core.display import display, HTML\nimport gc\nimport plotly.graph_objects as go\nfrom joblib import Parallel, delayed\nfrom sklearn import preprocessing, model_selection\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom scipy.stats import probplot\n\npd.set_option('max_rows', 400)\npd.set_option('max_columns', 400)","5afba076":"data_dir = '..\/input\/optiver-realized-volatility-prediction\/'","e2e3391c":"sample = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/sample_submission.csv\")\nsample","4f6f3864":"test = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/test.csv\")\ntest","49166f61":"train = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/train.csv\")\ntrain","9ff0e0a6":"train['stock_id'].value_counts()","b0a43e26":"train['stock_id'].unique()","5c5ad969":"book_train = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=10')\nbook_train","e2564488":"trade_example = pd.read_parquet(\"..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=1\")\ntrade_example","43b3f785":"def convert_to_32bit(df):\n    for f in df.columns:\n        if df[f].dtype == 'int64':\n            df[f] = df[f].astype('int32')\n        if df[f].dtype == 'float64':\n            df[f] = df[f].astype('float32')\n    return df","d34a0a62":"def wap_1(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\ndef wap_2(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2'])\/(df['bid_size2'] + df['ask_size2'])\n    return wap\ndef wap_bid(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])\/(df['bid_size1'] + df['bid_size2'])\n    return wap\ndef wap_ask(df):\n    wap = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])\/(df['ask_size1'] + df['ask_size2'])\n    return wap","37c5a0c8":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\ndef count_unique(series):\n    return len(np.unique(series))","8a01bd27":"# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","b0064ada":"def book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df = convert_to_32bit(df)\n    \n    #calculate return etc\n    df['wap1'] = wap_1(df)\n    df['log_return1'] = df.groupby('time_id')['wap1'].apply(log_return)\n    \n    df['wap2'] = wap_2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap_bid'] = wap_bid(df)\n    df['wap_ask'] = wap_ask(df)\n    \n    df['log_return_bid'] = df.groupby('time_id')['wap_bid'].apply(log_return)\n    df['log_return_ask'] = df.groupby('time_id')['wap_ask'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1'])\/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['bid_volume'] = df['bid_size1'] + df['bid_size2']\n    df['ask_volume'] = df['ask_size1'] + df['ask_size2']\n    df['bid_ask_volume'] = abs(df['bid_volume'] - df['ask_volume'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.min, np.max, np.std],\n        'wap2': [np.sum, np.mean, np.min, np.max, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.min, np.max, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.min, np.max, np.std],\n        'wap_bid': [np.sum, np.mean, np.min, np.max, np.std],\n        'wap_ask': [np.sum, np.mean, np.min, np.max, np.std],\n        'log_return_bid': [np.sum, realized_volatility, np.mean, np.min, np.max, np.std],\n        'log_return_ask': [np.sum, realized_volatility, np.mean, np.min, np.max, np.std],\n        'wap_balance': [np.sum, np.mean, np.min, np.max, np.std],\n        'price_spread':[np.sum, np.mean, np.min, np.max, np.std],\n        'bid_spread':[np.sum, np.mean, np.min, np.max, np.std],\n        'ask_spread':[np.sum, np.mean, np.min, np.max, np.std],\n        'bid_ask_spread':[np.sum, np.mean, np.min, np.max, np.std],\n        'bid_volume':[np.sum, np.mean, np.min, np.max, np.std],\n        'ask_volume':[np.sum, np.mean, np.min, np.max, np.std],\n        'bid_ask_volume':[np.sum, np.mean, np.min, np.max, np.std],\n        'total_volume':[np.sum, np.mean, np.min, np.max, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.min, np.max, np.std]\n        }\n\n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return_bid': [realized_volatility],\n        'log_return_bid': [realized_volatility],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature","f8cfe763":"%%time\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df = convert_to_32bit(df)\n    \n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, np.max, np.min],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max,np.min],\n    }\n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.sum],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff\/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature","7a8369d9":"%%time\n\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df","62a65821":"# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df","d56b3543":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))","758a3d21":"# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n","edff1cb0":"train, test = read_train_test()","5e611479":"train_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","ee87fa8d":"# replace by order sum (tau)\ntrain['size_tau'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique'] )\ntest['size_tau'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique'] )\n#train['size_tau_450'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_450'] )\n#test['size_tau_450'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_450'] )\ntrain['size_tau_400'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_400'] )\ntest['size_tau_400'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_400'] )\ntrain['size_tau_300'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_300'] )\ntest['size_tau_300'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_300'] )\n#train['size_tau_150'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_150'] )\n#test['size_tau_150'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_150'] )\ntrain['size_tau_200'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_200'] )\ntest['size_tau_200'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_200'] )","3f3ab44f":"train['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1\/ test['trade_order_count_sum'] )\n#train['size_tau2_450'] = np.sqrt( 0.25\/ train['trade_order_count_sum'] )\n#test['size_tau2_450'] = np.sqrt( 0.25\/ test['trade_order_count_sum'] )\ntrain['size_tau2_400'] = np.sqrt( 0.33\/ train['trade_order_count_sum'] )\ntest['size_tau2_400'] = np.sqrt( 0.33\/ test['trade_order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5\/ train['trade_order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5\/ test['trade_order_count_sum'] )\n#train['size_tau2_150'] = np.sqrt( 0.75\/ train['trade_order_count_sum'] )\n#test['size_tau2_150'] = np.sqrt( 0.75\/ test['trade_order_count_sum'] )\ntrain['size_tau2_200'] = np.sqrt( 0.66\/ train['trade_order_count_sum'] )\ntest['size_tau2_200'] = np.sqrt( 0.66\/ test['trade_order_count_sum'] )\n\n# delta tau\ntrain['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","b2eacf68":"df = train\ntest_data_set = test","241650b0":"test_data_set['stock_id'] = test_data_set['stock_id'].astype(int)\ntest_data_set.head()","b6c4999a":"X = df.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = df['target']\nX.shape, y.shape","3fd0590f":"thresh = int(len(df) * 0.9 \/ df['stock_id'].nunique())\nprint (thresh)","46dcd457":"mask  = df.groupby('stock_id')['stock_id'].cumcount() < thresh","e027d2a1":"train = df[mask]\ntest = df[~mask]","41e06cab":"X_train = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny_train = train['target']\nX_train.shape, y_train.shape","22d174e1":"X_valid = test.drop(['row_id', 'target', 'time_id'], axis = 1)\ny_valid = test['target']\nX_valid.shape, y_valid.shape","cb7a6b38":"X_train['stock_id'] = X_train['stock_id'].astype(int)\nX_valid['stock_id'] = X_valid['stock_id'].astype(int)","0fa635ba":"import optuna\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","28390d11":"params_xgb = {\n        'lambda': 0.0014832052084105417, \n        'alpha': 2.6885464964958112, \n        'max_depth': 17, \n        'learning_rate': 0.02, \n        'random_state': 24, \n        'n_estimators': 1540, \n        'eta': 0.12558915915760901, \n        'subsample': 0.6000000000000001, \n        'colsample_bytree': 0.3, \n        'min_child_weight': 77, \n        'reg_lambda': 0.001217091110648466, \n        'reg_alpha': 0.0019723477880301235\n        }\n\nxgb_model = xgb.XGBRegressor(**params_xgb, tree_method='gpu_hist')","e13b6199":"%%time\nxgb_model.fit(X_train ,y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=150, verbose=False)\n\npreds = xgb_model.predict(X_valid)\nRMSPE = round(rmspe(y_true = y_valid, y_pred = preds), 5)\nprint(f'Performance of the Tuned XGB prediction: RMSPE: {RMSPE}')","249d400a":"from lightgbm import LGBMRegressor","60e69a66":"params_lgbm = {\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        'learning_rate': 0.04412162462604988, \n        'max_depth': 300, \n        'lambda_l1': 0.12309589568066824, \n        'lambda_l2': 3.1044658548129586e-06, \n        'num_leaves': 246, \n        'n_estimators': 2350, \n        'feature_fraction': 0.531654883966269, \n        'bagging_fraction': 0.8553165643797457, \n        'bagging_freq': 8, \n        'min_child_samples': 42\n        }","b790d711":"lgbm_model = LGBMRegressor(**params_lgbm, device='gpu')","9bc6f61f":"%%time\nlgbm_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False, early_stopping_rounds=150)\n\npreds = lgbm_model.predict(X_valid)\nRMSPE = round(rmspe(y_true = y_valid, y_pred = preds), 5)\nprint(f'Performance of the Tuned LIGHTGBM prediction: RMSPE: {RMSPE}')","fa4f6cd9":"import catboost as cat\nfrom catboost import CatBoostRegressor","a8384f4b":"params_cb = {\n        'colsample_bylevel': 0.029576065862676762,\n        'depth': 91,\n        'learning_rate': 0.022293479743970765,\n        'iterations': 7000,\n        'max_bin': 120,\n        'min_data_in_leaf': 66,\n        'l2_leaf_reg': 0.0009704826955054485,\n        'bagging_temperature': 0.7432417203968587,\n        'subsample': 0.7022796507235656,\n        'grow_policy': 'Lossguide', \n        'leaf_estimation_method': 'Newton',\n        'loss_function': 'RMSE',\n        'eval_metric': 'RMSE',\n        'cat_features': ['stock_id']\n        }","b4eac827":"cb_model = CatBoostRegressor(**params_cb)","45b24d50":"%%time\ncb_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False, early_stopping_rounds=150)\n\npreds = cb_model.predict(X_valid)\nRMSPE = round(rmspe(y_true = y_valid, y_pred = preds), 5)\nprint(f'Performance of the Tuned CATBOOST prediction: RMSPE: {RMSPE}')","0885fdab":"from sklearn.ensemble import StackingRegressor","064d997c":"mod_xgb = xgb.XGBRegressor(tree_method='gpu_hist', n_jobs= - 1)\nmod_lgbm = LGBMRegressor(device='gpu')\nmod_cb = CatBoostRegressor()","a2fbbeef":"estimators = [('mod_xgb', mod_xgb),\n              ('mod_lgbm', mod_lgbm),\n              ('mod_cb', mod_cb)]\n\nclf = StackingRegressor(estimators=estimators, verbose=1)","7aa54d5e":"%%time\nclf.fit(X_train, y_train)","cd2ef940":"preds = clf.predict(X_valid)\nRMSPE = round(rmspe(y_true = y_valid, y_pred = preds), 5)\nprint(f'Performance of the STACK prediction: RMSPE: {RMSPE}')","77e67976":"y_pred = test_data_set[['row_id']]\nX_test = test_data_set.drop(['time_id', 'row_id'], axis = 1)","de0dc25d":"target = np.zeros(len(X_test))\n\npred = clf.predict(X_test[X_train.columns])\ntarget = pred","4a499b3a":"y_pred = y_pred.assign(target = target)\ny_pred","97efe5d0":"y_pred.to_csv('submission.csv',index = False)","6d3c413f":"## FUNC","b7b3152d":"## XGBOOST","f19b2e36":"## Submission","213fc3f4":"### References\n* https:\/\/www.kaggle.com\/realtimshady\/2lgbm-2nn\n* https:\/\/www.kaggle.com\/munumbutt\/feature-engineering-tuned-xgboost-lgbm","d00422fe":"## CatBoostRegressor","d1fbbb62":"## EDA","daee5379":"## LGBM","6494aeaf":"## Stacking"}}