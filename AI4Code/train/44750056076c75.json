{"cell_type":{"a17e21f9":"code","6351a70a":"code","6fb3f171":"code","d8ef8570":"code","ea5c4b71":"code","fb9d1a25":"code","426f973d":"code","15df4f6d":"code","7eeb9c0e":"code","b0dd3e59":"code","18fba093":"code","2cb31985":"code","e78c6e90":"code","04b4c8ce":"code","c8af2475":"code","f51b2d46":"code","4da779df":"code","60a811d4":"code","36fda4c9":"code","ebe68418":"markdown","dd89ad63":"markdown","e9750b19":"markdown","0ee1e1d3":"markdown","d931673e":"markdown","389d376f":"markdown","e7db4c76":"markdown","71a1561a":"markdown","0e2c1381":"markdown","7f7db530":"markdown"},"source":{"a17e21f9":"import random \nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import StandardScaler\nfrom IPython.display import display\nfrom sklearn.cluster import KMeans \nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.metrics import homogeneity_score, completeness_score, \\\nv_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n%matplotlib inline\n\nnp.random.seed(123)","6351a70a":"Data = pd.read_csv('..\/input\/train.csv') ","6fb3f171":"Data.sample(5)","d8ef8570":"print('Shape of the data set: ' + str(Data.shape))","ea5c4b71":"#save labels as string\nLabels = Data['activity']\nData = Data.drop(['rn', 'activity'], axis = 1)\nLabels_keys = Labels.unique().tolist()\nLabels = np.array(Labels)\nprint('Activity labels: ' + str(Labels_keys))","fb9d1a25":"#check for missing values\nTemp = pd.DataFrame(Data.isnull().sum())\nTemp.columns = ['Sum']\nprint('Amount of rows with missing values: ' + str(len(Temp.index[Temp['Sum'] > 0])) )","426f973d":"#normalize the dataset\nscaler = StandardScaler()\nData = scaler.fit_transform(Data)","15df4f6d":"#check the optimal k value\nks = range(1, 10)\ninertias = []\n\nfor k in ks:\n    model = KMeans(n_clusters=k)\n    model.fit(Data)\n    inertias.append(model.inertia_)\n\nplt.figure(figsize=(8,5))\nplt.style.use('bmh')\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.show()","7eeb9c0e":"def k_means(n_clust, data_frame, true_labels):\n    \"\"\"\n    Function k_means applies k-means clustering alrorithm on dataset and prints the crosstab of cluster and actual labels \n    and clustering performance parameters.\n    \n    Input:\n    n_clust - number of clusters (k value)\n    data_frame - dataset we want to cluster\n    true_labels - original labels\n    \n    Output:\n    1 - crosstab of cluster and actual labels\n    2 - performance table\n    \"\"\"\n    k_means = KMeans(n_clusters = n_clust, random_state=123, n_init=30)\n    k_means.fit(data_frame)\n    c_labels = k_means.labels_\n    df = pd.DataFrame({'clust_label': c_labels, 'orig_label': true_labels.tolist()})\n    ct = pd.crosstab(df['clust_label'], df['orig_label'])\n    y_clust = k_means.predict(data_frame)\n    display(ct)\n    print('% 9s' % 'inertia  homo    compl   v-meas   ARI     AMI     silhouette')\n    print('%i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n      %(k_means.inertia_,\n      homogeneity_score(true_labels, y_clust),\n      completeness_score(true_labels, y_clust),\n      v_measure_score(true_labels, y_clust),\n      adjusted_rand_score(true_labels, y_clust),\n      adjusted_mutual_info_score(true_labels, y_clust),\n      silhouette_score(data_frame, y_clust, metric='euclidean')))","b0dd3e59":"k_means(n_clust=2, data_frame=Data, true_labels=Labels)","18fba093":"k_means(n_clust=6, data_frame=Data, true_labels=Labels)","2cb31985":"#change labels into binary: 0 - not moving, 1 - moving\nLabels_binary = Labels.copy()\nfor i in range(len(Labels_binary)):\n    if (Labels_binary[i] == 'STANDING' or Labels_binary[i] == 'SITTING' or Labels_binary[i] == 'LAYING'):\n        Labels_binary[i] = 0\n    else:\n        Labels_binary[i] = 1\nLabels_binary = np.array(Labels_binary.astype(int))","e78c6e90":"k_means(n_clust=2, data_frame=Data, true_labels=Labels_binary)","04b4c8ce":"#check for optimal number of features\npca = PCA(random_state=123)\npca.fit(Data)\nfeatures = range(pca.n_components_)\n\nplt.figure(figsize=(8,4))\nplt.bar(features[:15], pca.explained_variance_[:15], color='lightskyblue')\nplt.xlabel('PCA feature')\nplt.ylabel('Variance')\nplt.xticks(features[:15])\nplt.show()","c8af2475":"def pca_transform(n_comp):\n    pca = PCA(n_components=n_comp, random_state=123)\n    global Data_reduced\n    Data_reduced = pca.fit_transform(Data)\n    print('Shape of the new Data df: ' + str(Data_reduced.shape))","f51b2d46":"# pca_transform(n_comp=3)\n# k_means(n_clust=2, data_frame=Data_reduced, true_labels=Labels)","4da779df":"# colors = ['green', 'blue', 'orange', 'gray', 'pink', 'red']\n# fig = plt.figure(figsize=(12,8))\n# ax = fig.add_subplot(111, projection='3d')\n# for i in range(len(colors)):\n#     x = Data_reduced[:, 0][Labels == Labels_keys[i]]\n#     y = Data_reduced[:, 1][Labels == Labels_keys[i]]\n#     z = Data_reduced[:, 2][Labels == Labels_keys[i]]\n#     ax.scatter(xs=x, ys=y, zs=z, zdir='y', s=20, c=colors[i], alpha=0.2)\n\n# ax.set_xlabel('First Principal Component')\n# ax.set_ylabel('Second Principal Component')\n# ax.set_zlabel('Third Principal Component')\n# ax.set_title(\"PCA Scatter Plot\")\n# plt.show()","60a811d4":"pca_transform(n_comp=1)\nk_means(n_clust=2, data_frame=Data_reduced, true_labels=Labels_binary)","36fda4c9":"pca_transform(n_comp=2)\nk_means(n_clust=2, data_frame=Data_reduced, true_labels=Labels_binary)","ebe68418":"**No improvements here.**\n\n**So far it seems like this was best I could do. Still learning clustering algorithms and I might come back to this project later.**\n\n**If you know any interesting dataset to practice clustering on (not Iris dataset, haha), please suggest!**","dd89ad63":"**Looks like the best value (\"elbow\" of the line) for k is 2 (two clusters).**","e9750b19":"## K-Means Clustering and PCA of Human Activity Recognition\n\n***Ruslan Klymentiev***\n\n**Date created: **July 21st, 2018","0ee1e1d3":"### Intro\n\nClustering was always a subject I tried to avoid (for no reason). In this project I will finally use my knowledge of clustering and PCA algorithms to explore the Human Activity Recognition dataset. \n\nI would love to point on resourses I have learned from:\n\n1. DataCamp Tutorial: [Python Machine Learning: Scikit-Learn Tutorial](https:\/\/www.datacamp.com\/community\/tutorials\/machine-learning-python);\n\n2. DataCamp course: [Unsupervised Learning in Python](https:\/\/www.datacamp.com\/courses\/unsupervised-learning-in-python\/);\n\n3. Cognitive Class course: [Machine Learning with Python](https:\/\/courses.cognitiveclass.ai\/courses\/course-v1:CognitiveClass+ML0101ENv3+2018\/info);\n\n4. And of course [Prof. Google](http:\/\/google.com)!\n\n### Dataset info\n\nHuman Activity Recognition database built from the recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (*WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING*) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. ","d931673e":"**Doesn't look like good connection between clusters and original labels so I will stick with 2 clusters.**","389d376f":"**It looks like algorithm found patterns for Moving and Not-Moving activity with high level of accuracy.**\n\n**Check how it will cluster by 6 clusters (original number of classes).**","e7db4c76":"*More on clustering metrics can be found in [DataCamp Tutorial](https:\/\/www.datacamp.com\/community\/tutorials\/machine-learning-python).*","71a1561a":"**1 feature seems to be best fit for our algorithm.**","0e2c1381":"### Principal component analysis (PCA)\n\n> Principal Component Analysis is a dimension-reduction tool that can be used to reduce a large set of variables to a small set that still contains most of the information in the large set.\n\n**2-cluster algorithm seems to fbe able to find patterns for moving\/not-moving labels perfectly so far, but let's see if it can still be improved by dimension reduction. **","7f7db530":"**Inertia and Silhouette seems to be much better now after reduction. **\n\n**Just check clustering model for 2 components.**\n"}}