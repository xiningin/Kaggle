{"cell_type":{"c25d9c7d":"code","b2af9f60":"code","2c33280d":"code","345eac61":"code","67a42fe5":"code","3838c438":"code","a7945f83":"code","24a220e9":"code","51b806f1":"code","c85ca91f":"code","67672cc5":"code","f68e329c":"code","a248adf0":"code","56aea905":"code","f2ca4e34":"code","61c77450":"code","d37b37e5":"code","310133b8":"code","02ebcc65":"code","da77a691":"code","02f6fadb":"code","73f748d5":"code","e4e91894":"code","974fd029":"code","aa69b5df":"code","012d7425":"code","fcd4e62b":"code","86e56d5b":"code","4612725e":"code","1df1d640":"code","0eecd081":"code","274b8669":"code","a95d6334":"code","6c40caea":"code","6ddf5088":"code","de30b567":"code","47342c6b":"code","abafc3e4":"code","71cd3928":"code","fbe93e9f":"code","b269157a":"code","b5f46e42":"code","e31e9e73":"code","6b03217f":"code","4b972838":"code","4af1292c":"code","90edfb3f":"code","c5232dac":"code","b909251f":"code","7232cd95":"code","91dd93ca":"code","5bf95f11":"code","d525bdba":"code","1cbad23f":"code","f1313afe":"code","88462437":"code","531d9d44":"code","c4c0e139":"code","28dd5067":"code","e3b6b105":"code","f94a8d0a":"code","ef6bb29d":"code","439189ce":"code","3141af22":"code","c6c33140":"code","60baa55d":"code","6302488d":"markdown","98d8073f":"markdown","5d7ee619":"markdown","3fa1fb8c":"markdown","ee58e778":"markdown","ddc8f347":"markdown","3565f57a":"markdown","900a5274":"markdown","2a7f2325":"markdown","d9575a41":"markdown","7cb003c1":"markdown","8cf46175":"markdown","d35db70e":"markdown","b5c64307":"markdown","0b268bbc":"markdown","4ff5481d":"markdown","cf4c7499":"markdown","6617c60e":"markdown","a88a40d6":"markdown","6567e8cd":"markdown","a7c25a1a":"markdown","8721c5e0":"markdown","edb3ba41":"markdown","d8cd35e0":"markdown","12812008":"markdown","86520281":"markdown","95dd7f33":"markdown","1aa24a9a":"markdown","568ed24a":"markdown","0d894b3f":"markdown","4c07f6f4":"markdown","48da49a9":"markdown","bbc39530":"markdown","91610236":"markdown","4184af6d":"markdown","f7a7893d":"markdown","4e249a72":"markdown","7c68a56f":"markdown","eb9b4bcf":"markdown","9b2c55df":"markdown","b7ca51ba":"markdown","55ecdba5":"markdown","54846fdc":"markdown","de53bd9c":"markdown","c523e530":"markdown","0ded67db":"markdown","cf533153":"markdown","495b72ad":"markdown","c6a91706":"markdown","b59c9ab3":"markdown","9e516dde":"markdown","66ad73f3":"markdown","45882dbf":"markdown","18ba6d11":"markdown","373778b7":"markdown","069ed73c":"markdown","0443da0a":"markdown","5a7a7cde":"markdown","263739a2":"markdown"},"source":{"c25d9c7d":"# IMPORTING PACKAGES\n\n# warnings\nimport warnings\nwarnings.filterwarnings(action = 'ignore', category = FutureWarning)\n\n# file and data\nfrom google.colab import files\nimport calendar\n\n# scientific\nimport pandas as pd \nimport numpy as np \nfrom numpy import random\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# dimensionality and oversampling\nfrom sklearn.decomposition import PCA\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.over_sampling import RandomOverSampler\n\n# models \nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import GridSearchCV\n\n# metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\nSEED = 42\nrandom.seed(SEED)","b2af9f60":"# IMPORTING DATA\nurl = \"https:\/\/raw.githubusercontent.com\/gianluigilopardo\/dataspaces\/master\/Absenteeism_at_work.csv\"\nds = pd.read_csv(url, sep=',')\nprint(\"The dataset has %d rows and %d columns.\" % ds.shape)","2c33280d":"ds.head(10)","345eac61":"# renaming labels\nds = ds.rename(columns = {'Reason for absence': 'reason', \n                          'Month of absence': 'month', \n                          'Day of the week': 'day', \n                          'Transportation expense': 'trans_exp', \n                          'Distance from Residence to Work': 'distance', \n                          'Service time': 'serv_time', \n                          'Work load Average\/day ': 'work_load', \n                          'Hit target': 'hit_tg', \n                          'Disciplinary failure': 'disc_fail', \n                          'Social drinker': 'drinker', \n                          'Social smoker': 'smoker', \n                          'Body mass index': 'bmi', \n                          'Absenteeism time in hours': 'abs_hours'})\nds = ds.rename(columns = lambda x: x.lower()) # using lambda function to lowercase labels\nds.head(3)","67a42fe5":"print(\"There are \" + (\"some\" if ds.isnull().values.any() else \"no\")  + \" null\/missing values in the dataset.\")","3838c438":"ds.describe() # summary of data","a7945f83":"ds[ds['month'] == 0]","24a220e9":"ds = ds[ds['month'] != 0]","51b806f1":"len(ds[ds['reason'] == 0]) # occurences with 'reason' = 0.","c85ca91f":"ds[ds['reason'] == 0].describe() # summary of the subset of data having reason = 0","67672cc5":"ds.describe()","f68e329c":"len(ds[ds['disc_fail'] == 1]) # Check how many records have 1 ","a248adf0":"ds[ds['disc_fail'] == 1].describe()","56aea905":"# Here I will add extended values for categorical data.\n\n# Adding Season names:\nseason_mapping = {1:'Summer', 2:'Autumn', 3:'Winter', 4:'Spring'}\nds['season_name'] = ds.seasons.map(season_mapping).astype('category')\n\n# Adding Month names abbrevations:\nds['month_name'] =  ds['month'].apply(lambda x: calendar.month_abbr[x])\n\n# Adding day names abbrevations:\nds['day_name'] =  ds['day'].apply(lambda x: calendar.day_abbr[x-2])\n# calendar has 0: Monday, but I have 2: Monday\n\n# Adding reasons value:\nreason_mapping = {\n    0: 'Not available',\n    1: 'Certain infectious and parasitic diseases',\n    2: 'Neoplasms',\n    3: 'Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism',\n    4: 'Endocrine, nutritional and metabolic diseases',\n    5: 'Mental and behavioural disorders',\n    6: 'Diseases of the nervous system',\n    7: 'Diseases of the eye and adnexa',\n    8: 'Diseases of the ear and mastoid process',\n    9: 'Diseases of the circulatory system',\n    10: 'Diseases of the respiratory system',\n    11: 'Diseases of the digestive system',\n    12: 'Diseases of the skin and subcutaneous tissue',\n    13: 'Diseases of the musculoskeletal system and connective tissue',\n    14: 'Diseases of the genitourinary system',\n    15: 'Pregnancy, childbirth and the puerperium',\n    16: 'Certain conditions originating in the perinatal period',\n    17: 'Congenital malformations, deformations and chromosomal abnormalities',\n    18: 'Symptoms, signs and abnormal clinical and laboratory findings, not elsewhere classified',\n    19: 'Injury, poisoning and certain other consequences of external causes',\n    20: 'External causes of morbidity and mortality',\n    21: 'Factors influencing health status and contact with health services',\n    22: 'Patient follow-up',\n    23: 'Medical consultation',\n    24: 'Blood donation',\n    25: 'Laboratory examination',\n    26: 'Unjustified absence',\n    27: 'Physiotherapy',\n    28: 'Dental consultation'\n}\nds['reason_text'] = ds['reason'].map(reason_mapping)\nds['reason_text'] = ds['reason_text'].astype('category')\n\n# Adding Education:\neducation_mapping = {\n    1: '1. High School',\n    2: '2. Graduate',\n    3: '3. Post Graduate',\n    4: '4. Master & Doctor'\n}\nds['education_detail'] = ds['education'].map(education_mapping)\nds['education_detail'] = ds['education_detail'].astype('category')\n\n# smoker and drinker are boolean\nds['smoker'] = ds['smoker'].astype('bool')\nds['drinker'] = ds['drinker'].astype('bool')\nds['disc_fail'] = ds['disc_fail'].astype('bool')\n\n# Now ds contains categorical data twice.\n# I drop the old columns, but before I keep save of everything\n# in ds_explore: I use it for data exploration \n\nds_explore = ds.copy()\nds = ds.drop(columns = {'reason','month','day','seasons','education'})\n\n# I check the firsts rows:\nds_explore.head()","f2ca4e34":"# Useful for visualization\ndef level(absh):\n  if(absh < 2):\n    lev = 'late'\n  elif((absh >= 2) and (absh < 8)):\n    lev = 'hours'\n  elif(absh >= 8):\n    lev = 'days'    \n  return lev\n\nds_explore['abs_lev'] = ds_explore['abs_hours'].apply(lambda x: level(x)).astype('category')","61c77450":"# Plotting absenteeism hours \nbins = 40\nfig, ax = plt.subplots()\nax.hist(ds_explore['abs_hours'], bins, rwidth=0.8, density = True)\nax.set_xlabel('Absenteeism time in hours')\nax.set_ylabel('Frequency')","d37b37e5":"# boxplot of Absenteeism time. \nplt.boxplot(ds_explore['abs_hours'], widths = 0.5)\nplt.ylabel('Absenteeism time in hours')\nplt.xlabel('Boxplot')\nplt.show()","310133b8":"# Frequency of Absenteeism time for values grater than a week (40 hours). \nbins = 20\nfig, ax = plt.subplots()\nout = ds_explore[ds_explore['abs_hours'] > 40]\nax.hist(out['abs_hours'], bins, rwidth = 0.8, density = True)\nax.set_xlabel('Absenteeism time in hours')\nax.set_ylabel('Frequency')","02ebcc65":"# Plotting absence \ntime = ds_explore['abs_hours']\nhours = np.count_nonzero(time < 8)\ndays = np.count_nonzero(time >= 8)\nx = np.array(['Less than one day', 'One day or more'])\ny = np.array([hours, days])\nplt.bar(x, y)\nplt.show()","da77a691":"# Plot of Absenteeism time in hours with Reason for absence. \nsns.catplot(x = 'reason', y = 'abs_hours', \n            data = ds_explore,\n            height = 4,\n            aspect = 2,\n            jitter = '0.25',\n            ).set_axis_labels(\"Reason of absence code\", 'Absenteeism in hours')","02f6fadb":"# Main absence for abs_lev = late\nreason_abs_lev = ds_explore.groupby('reason_text')['abs_lev'].value_counts().unstack().fillna(0).astype(int)\nreason_abs_lev.sort_values(by = ['late'], ascending = False).head()","73f748d5":"# Main absence for abs_lev = hours\nreason_abs_lev.sort_values(by = ['hours'], ascending = False).head()","e4e91894":"# Main absence for abs_lev = days\nreason_abs_lev.sort_values(by = ['days'], ascending = False).head()","974fd029":"# DAY OF THE WEEK\n# Distribution of days for level of absence\nsns.catplot(x = 'day_name', y = 'abs_hours', \n            data = ds_explore,\n            height = 4,\n            aspect = 2,\n            jitter = '0.25',\n            ).set_axis_labels(\"Day of the week of absence\", 'Absenteeism in hours')","aa69b5df":"days_abs_lev = ds_explore.groupby('day_name')['abs_lev'].value_counts().unstack().fillna(0).astype(int)\ndays_abs_lev","012d7425":"# MONTH OF ABSENCE\n# Distribution of month for level of absence\nsns.catplot(x = 'month_name', y = 'abs_hours', \n            data = ds_explore,\n            height = 4,\n            aspect = 2,\n            jitter = '0.25',\n            ).set_axis_labels(\"Month of absence\", 'Absenteeism in hours')","fcd4e62b":"# SEASON OF ABSENCE\n# Distribution of month for level of absence\nsns.catplot(x = 'season_name', y = 'abs_hours', \n            data = ds_explore,\n            height = 4,\n            aspect = 2,\n            jitter = '0.25',\n            ).set_axis_labels(\"Season of absence\", 'Absenteeism in hours')","86e56d5b":"# EDUCATION \nsns.catplot(x = 'education_detail', y = 'abs_hours', \n            data = ds_explore,\n            height = 4,\n            aspect = 1.5).set_axis_labels(\"Education detail\", 'Absenteeism in hours')","4612725e":"# Employees per education\nds_explore.groupby('education_detail')['id'].nunique()","1df1d640":"# SON\nsns.catplot(x = 'son', y = 'abs_hours', \n            data = ds_explore,\n            height = 4,\n            aspect = 1.5).set_axis_labels(\"Number of sons\", 'Absenteeism in hours')","0eecd081":"# PETS\nsns.catplot(x = 'pet', y = 'abs_hours', \n            data = ds_explore,\n            height = 4,\n            aspect = 1.5).set_axis_labels(\"Number of pets\", 'Absenteeism in hours')","274b8669":"# SMOKER\nabs_smoker = ds_explore[ds_explore['smoker'] == 0].loc[:,'abs_hours']\nsns.kdeplot(abs_smoker, shade = True, label = 'No smoker'\n            ).set_title('Distribution of absenteeism for smoker\/no smoker')\nabs_smoker = ds_explore[ds_explore['smoker'] == 1].loc[:,'abs_hours']            \nsns.kdeplot(abs_smoker, shade = True, label = 'Smoker'\n            )","a95d6334":"# DRINKER\nabs_drinker = ds_explore[ds_explore['drinker'] == 0].loc[:,'abs_hours']\nsns.kdeplot(abs_drinker, shade = True, label = 'No drinker'\n            ).set_title('Distribution of absenteeism for drinker\/no drinker')\nabs_drinker = ds_explore[ds_explore['drinker'] == 1].loc[:,'abs_hours']            \nsns.kdeplot(abs_drinker, shade = True, label = 'Drinker'\n            )","6c40caea":"# Correlation matrix \n# I drop categorical attributes and I normalize\nds_num = ds.drop(columns = ['id', 'disc_fail', 'drinker', 'smoker', 'season_name', \n                            'month_name', 'day_name', 'reason_text', 'education_detail'])\nds_norm = (ds_num-ds_num.mean())\/ds_num.std()\ncorr = ds_norm.corr()\nplt.figure(figsize = (12,12))\nsns.heatmap(corr, annot = True, \n            vmin = -1, vmax = 1, center = 0,\n            cmap = sns.diverging_palette(20, 220, n = 200))\nplt.title('Correlation Heatmap', fontsize = 24)","6ddf5088":"ds = ds.drop(columns = 'weight')","de30b567":"# Plotting absence last\ntime = ds['abs_hours']\nlate = np.count_nonzero(time < 2)\ndays = np.count_nonzero(time >= 8)\nhours = np.count_nonzero(time >= 2) - days\nx = np.array(['Late', 'Hours', 'Days'])\ny = np.array([late, hours, days])\nplt.bar(x, y)\nplt.show()","47342c6b":"# Plotting absence last\ntime = ds['abs_hours']\nless = np.count_nonzero(time < 8)\nmore = np.count_nonzero(time >= 8)\nx = np.array(['Less than one day', 'One day or more'])\ny = np.array([less, more])\nplt.bar(x, y)\nplt.show()","abafc3e4":"# Create label for ABS_HOURS\n\n# Classes for full classification\ndef level(absh):\n  if(absh < 2):\n    lev = 'late'\n  elif((absh >= 2) and (absh < 8)):\n    lev = 'hours'\n  elif(absh >= 8):\n    lev = 'days'    \n  return lev\n\n# Classes for binary classification\ndef level_day(absh):\n  if(absh < 8):\n    day = 'less'\n  elif(absh >= 8):\n    day = 'more'    \n  return day\n\nds['abs_lev'] = ds['abs_hours'].apply(lambda x: level(x)).astype('category')\nds['abs_day'] = ds['abs_hours'].apply(lambda x: level_day(x)).astype('category')\n\nds.head()","71cd3928":"X = ds.drop(columns = ['abs_lev', 'abs_day', 'abs_hours'])\ny = ds['abs_lev']\ny_day = ds['abs_day']\ny_int = ds['abs_hours']","fbe93e9f":"# label_encoder object knows how to understand word labels. \n\nZ = X.copy()\nlabel_encoder = preprocessing.LabelEncoder() \n\nylabel = label_encoder.fit_transform(y)\nlevels = label_encoder.inverse_transform(list(set(ylabel)))\nybin = label_encoder.fit_transform(y_day)\nbin_levels = label_encoder.inverse_transform(list(set(ybin)))\n\n# ONE HOT ENCODING\n# Adding the new columns\n#Z = pd.concat([Z,pd.get_dummies(Z['month_name'], prefix = 'month')], axis=1)\nZ = pd.concat([Z,pd.get_dummies(Z['reason_text'], prefix = 'reason')], axis=1)\nZ = pd.concat([Z,pd.get_dummies(Z['day_name'], prefix = 'day')], axis=1)\nZ = pd.concat([Z,pd.get_dummies(Z['season_name'], prefix = 'season')], axis=1)\nZ = pd.concat([Z,pd.get_dummies(Z['education_detail'], prefix = 'education')], axis=1)\n# Removing the old nominal variables\nZ.drop(['reason_text'],axis=1, inplace=True)\nZ.drop(['month_name'],axis=1, inplace=True)\nZ.drop(['day_name'],axis=1, inplace=True)\nZ.drop(['season_name'],axis=1, inplace=True)\nZ.drop(['education_detail'],axis=1, inplace=True)\nZ.drop(['id'],axis=1, inplace=True)","b269157a":"X_pca = Z.copy()\n# Normalize:\nX_pca_norm = (X_pca-X_pca.mean())\/X_pca.std()\nprint(\"There are \" + (\"some\" if X_pca_norm.isnull().values.any() else \"no\")  + \" null\/missing values in the dataset.\")","b5f46e42":"# calculate the principal components\nX_pca = PCA(random_state = SEED).fit(X_pca_norm)\ncumvar = np.cumsum(X_pca.explained_variance_ratio_)\n#Plotting cumulative variance\nplt.plot(cumvar)\nplt.title('Cumulative variance')\nplt.xlabel('Number of components')\nplt.ylabel('Variance explained')","e31e9e73":"cumvar[36]","6b03217f":"n_used = 36\nX_pca = np.dot(X_pca_norm.values, X_pca.components_[:n_used,:].T)\nX_pca = pd.DataFrame(X_pca, columns=[\"PC%d\" % (x + 1) for x in range(n_used)])\nX_pca.head()","4b972838":"X_train, X_test, y_train, y_test = train_test_split(X_pca, ylabel, test_size = 0.3, random_state = SEED, stratify = ylabel)\nXbin_train, Xbin_test, ybin_train, ybin_test = train_test_split(X_pca, ybin, test_size = 0.3, random_state = SEED, stratify = ybin)","4af1292c":"# GENERATING NEW DATASET WITH RANDOM OVERSAMPLING, SMOTE and ADASYN OVERSAMPLING\nX_train_ROS, y_train_ROS = RandomOverSampler(random_state = SEED).fit_resample(X_train, y_train)\nX_train_SMOTE, y_train_SMOTE = SMOTE(random_state = SEED).fit_resample(X_train, y_train)\nX_train_ADASYN, y_train_ADASYN = ADASYN(random_state = SEED, sampling_strategy = 'not majority').fit_resample(X_train, y_train)","90edfb3f":"y_set = {'simple': y_train, 'ROS': y_train_ROS, 'SMOTE': y_train_SMOTE, 'ADASYN': y_train_ADASYN}\nfor ys in y_set.keys():\n  print('The', ys, 'train set has:')\n  for i in set(ylabel):    \n          print('\\t', np.count_nonzero(y_set[ys] == i), 'records with label',levels[i])","c5232dac":"# Random forest Classifier\n\n#Simple dataset\nsimple_forest = RandomForestClassifier(random_state = SEED, max_depth = 10).fit(X_train, y_train)\nsimple_pred = simple_forest.predict(X_test)\n\n#Random oversampler\nros_forest = RandomForestClassifier(random_state = SEED, max_depth = 10).fit(X_train_ROS, y_train_ROS)\nrandom_over_pred = ros_forest.predict(X_test)\n\n#SMOTE\nSMOTE_forest = RandomForestClassifier(random_state = SEED, max_depth = 10).fit(X_train_SMOTE, y_train_SMOTE)\nSMOTE_pred = SMOTE_forest.predict(X_test)\n\n#ADASYN \nADASYN_forest = RandomForestClassifier(random_state = SEED, max_depth = 10).fit(X_train_ADASYN, y_train_ADASYN)\nADASYN_pred = ADASYN_forest.predict(X_test)\n\n#Performance \n\n#Simple data_set metrics: \nsimple_pred_accuracy = accuracy_score(y_test, simple_pred)\nsimple_pred_precision = precision_score(y_test, simple_pred, average = 'weighted')\nsimple_pred_sensitivity = recall_score(y_test, simple_pred, average = 'weighted')\nsimple_pred_f1 = f1_score(y_test, simple_pred, average = 'weighted')\n\n#Random oversampling metrics:\nrnd_sampler_accuracy = accuracy_score(y_test, random_over_pred)\nrnd_sampler_precision = precision_score(y_test,random_over_pred, average = 'weighted')\nrnd_sampler_sensitivity = recall_score(y_test,random_over_pred, average = 'weighted')\nrnd_sampler_f1 = f1_score(y_test,random_over_pred, average = 'weighted')\n\n#SMOTE metrics: \nSMOTE_accuracy = accuracy_score(y_test, SMOTE_pred)\nSMOTE_precision = precision_score(y_test, SMOTE_pred, average = 'weighted')\nSMOTE_sensitivity = recall_score(y_test, SMOTE_pred, average = 'weighted')\nSMOTE_f1 = f1_score(y_test, SMOTE_pred, average = 'weighted')\n\n#ADASYN metrics:\nADASYN_accuracy = accuracy_score(y_test, ADASYN_pred)\nADASYN_precision = precision_score(y_test, ADASYN_pred, average = 'weighted')\nADASYN_sensitivity = recall_score(y_test, ADASYN_pred, average = 'weighted')\nADASYN_f1 = f1_score(y_test, ADASYN_pred, average = 'weighted')\n\n# metrics\nmetrics = pd.DataFrame(columns=[\"Accuracy\", \"Precision\", \"Sensitivity\", \"F1 Score\"])\nmetrics.loc[\"Simple Dataset\"] = [simple_pred_accuracy,simple_pred_precision,simple_pred_sensitivity,simple_pred_f1]\nmetrics.loc[\"Rnd oversampling\"] = [rnd_sampler_accuracy,rnd_sampler_precision,rnd_sampler_sensitivity,rnd_sampler_f1]\nmetrics.loc[\"SMOTE\"] = [SMOTE_accuracy,SMOTE_precision,SMOTE_sensitivity,SMOTE_f1]\nmetrics.loc[\"ADASYN\"] = [ADASYN_accuracy,ADASYN_precision,ADASYN_sensitivity,ADASYN_f1]\nmetrics","b909251f":"fig, ((simple, ros), (smote, adasyn)) = plt.subplots(nrows = 2, ncols = 2, figsize = (10, 10))\n\nplot_confusion_matrix(simple_forest, X_test, y_test, display_labels = levels,\n                      cmap = plt.cm.Blues, values_format = '0.2%', normalize = 'true', \n                      ax = simple)\nplot_confusion_matrix(ros_forest, X_test, y_test, display_labels = levels,\n                      cmap = plt.cm.Blues, values_format = '0.2%', normalize = 'true',\n                      ax = ros)\nplot_confusion_matrix(SMOTE_forest, X_test, y_test, display_labels = levels,\n                      cmap = plt.cm.Blues, values_format = '0.2%', normalize = 'true',\n                      ax = smote)\nplot_confusion_matrix(ADASYN_forest, X_test, y_test, display_labels = levels,\n                      cmap = plt.cm.Blues, values_format = '0.2%', normalize = 'true',\n                      ax = adasyn)\nsimple.set_title('Simple dataset')\nros.set_title('ROS dataset')\nsmote.set_title('SMOTE dataset')\nadasyn.set_title('ADASYN dataset')","7232cd95":"X_val = X_train\ny_val = y_train\nX_train = X_train_SMOTE\ny_train = y_train_SMOTE","91dd93ca":"classification_metrics = pd.DataFrame(columns=[\"Accuracy\", \"Precision\", \"Sensitivity\", \"F1 Score\"])","5bf95f11":"logistic = LogisticRegression(max_iter = 10000, random_state = SEED, solver = 'lbfgs').fit(X_train, y_train)\nlogistic_predict = logistic.predict(X_test)\n\n#metrics:\nlogi_acc = accuracy_score(logistic_predict, y_test)\nlogi_preci = precision_score(logistic_predict, y_test, average = 'weighted')\nlogi_sensitivity = recall_score(logistic_predict, y_test, average = 'weighted')\nlogi_f1 = f1_score(logistic_predict, y_test, average = 'weighted')\n\nclassification_metrics.loc[\"Logistic regression\"] = [logi_acc,logi_preci,logi_sensitivity,logi_f1]\nplot_confusion_matrix(logistic, X_test, y_test, display_labels = levels,\n                      cmap = plt.cm.Blues, values_format = '0.2%', normalize = 'true')","d525bdba":"# Evaluating best parameters\ndepth = np.arange(4, 20) \nparameters = {'max_depth': depth}\nclf = GridSearchCV(tree.DecisionTreeClassifier(criterion = 'entropy', random_state = SEED), parameters, scoring = 'f1_weighted')\nclf = clf.fit(X_val, y_val)\nopt_depth = clf.best_params_['max_depth']\nprint(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))","1cbad23f":"# Decision TREE: Entropy\ntree_Entropy = tree.DecisionTreeClassifier(max_depth = opt_depth, criterion = 'entropy', random_state = SEED)\ntree_Entropy = tree_Entropy.fit(X_train, y_train)\ntree_Entropy_pred = tree_Entropy.predict(X_test)\n\n#metrics\ntree_acc = accuracy_score(tree_Entropy_pred, y_test)\ntree_preci = precision_score(tree_Entropy_pred, y_test, average = 'weighted')\ntree_sensitivity = recall_score(tree_Entropy_pred, y_test, average = 'weighted')\ntree_f1 = f1_score(tree_Entropy_pred, y_test, average = 'weighted')\n\nclassification_metrics.loc[\"Tree Entropy\"] = [tree_acc, tree_preci, tree_sensitivity, tree_f1]\nplot_confusion_matrix(tree_Entropy, X_test, y_test, \n                      display_labels = levels, cmap = plt.cm.Blues,\n\n                      values_format = '0.2%', normalize = 'true')","f1313afe":"# Evaluating best parameters\ndepth = np.arange(4, 20) \nparameters = {'max_depth': depth}\nclf = GridSearchCV(tree.DecisionTreeClassifier(criterion = 'gini', random_state = SEED), parameters, scoring = 'f1_weighted')\nclf = clf.fit(X_val, y_val)\nopt_depth = clf.best_params_['max_depth']\nprint(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))","88462437":"# Decision TREE: Gini\ntree_Gini = tree.DecisionTreeClassifier(max_depth = opt_depth, criterion = 'gini', random_state = SEED)\ntree_Gini = tree_Gini.fit(X_train, y_train)\ntree_Gini_pred = tree_Gini.predict(X_test)\n\n#metrics\ntree_acc = accuracy_score(tree_Gini_pred, y_test)\ntree_preci = precision_score(tree_Gini_pred, y_test, average = 'weighted')\ntree_sensitivity = recall_score(tree_Gini_pred, y_test, average = 'weighted')\ntree_f1 = f1_score(tree_Gini_pred, y_test, average = 'weighted')\n\nclassification_metrics.loc[\"Tree Gini\"] = [tree_acc, tree_preci, tree_sensitivity, tree_f1]\nplot_confusion_matrix(tree_Gini, X_test, y_test, display_labels = levels, cmap = plt.cm.Blues, \n                      values_format = '0.2%',  normalize = 'true')","531d9d44":"# Evaluating best parameters\ndepth = np.arange(4, 20) \nparameters = {'max_depth': depth}\nclf = GridSearchCV(RandomForestClassifier(criterion = 'entropy', n_estimators = 200, random_state = SEED,\n                                          ), parameters, scoring = 'f1_weighted')\nclf = clf.fit(X_val, y_val)\nopt_depth = clf.best_params_['max_depth']\nprint(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))","c4c0e139":"# Random forest - Entropy criterion\nforest_Entropy = RandomForestClassifier(max_depth = opt_depth, criterion = 'entropy', n_estimators = 200, random_state = SEED)\nforest_Entropy = forest_Entropy.fit(X_train, y_train)\nforest_Entropy_pred = forest_Entropy.predict(X_test)\n\n#metrics\nforest_acc = accuracy_score(forest_Entropy_pred, y_test)\nforest_preci = precision_score(forest_Entropy_pred, y_test, average = 'weighted')\nforest_sensitivity = recall_score(forest_Entropy_pred, y_test, average = 'weighted')\nforest_f1 = f1_score(forest_Entropy_pred, y_test, average = 'weighted')\n\nclassification_metrics.loc[\"Forest Entropy\"] = [forest_acc,forest_preci,forest_sensitivity,forest_f1]\nplot_confusion_matrix(forest_Entropy, X_test, y_test, display_labels = levels,\n                      cmap = plt.cm.Blues, values_format = '0.2%', normalize = 'true')","28dd5067":"# Evaluating best parameters\ndepth = np.arange(4, 20) \nparameters = {'max_depth': depth}\nclf = GridSearchCV(RandomForestClassifier(criterion = 'gini', n_estimators = 200, random_state = SEED,\n                                          ), parameters, scoring = 'f1_weighted')\nclf = clf.fit(X_val, y_val)\nopt_depth = clf.best_params_['max_depth']\nprint(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))","e3b6b105":"# Random forest - Gini criterion\nforest_Gini = RandomForestClassifier(max_depth = opt_depth, criterion = 'gini', n_estimators = 200, random_state = SEED)\nforest_Gini = forest_Gini.fit(X_train, y_train)\nforest_Gini_pred = forest_Gini.predict(X_test)\n\n#metrics\nforest_acc = accuracy_score(forest_Gini_pred, y_test)\nforest_preci = precision_score(forest_Gini_pred, y_test, average = 'weighted')\nforest_sensitivity = recall_score(forest_Gini_pred, y_test, average = 'weighted')\nforest_f1 = f1_score(forest_Gini_pred, y_test, average = 'weighted')\n\nclassification_metrics.loc[\"Forest Gini\"] = [forest_acc,forest_preci,forest_sensitivity,forest_f1]\nplot_confusion_matrix(forest_Gini, X_test, y_test, display_labels = levels,\n                      cmap = plt.cm.Blues, values_format = '0.2%', normalize = 'true')","f94a8d0a":"# Evaluating best parameters\nneigh = np.arange(2, 20)\nparameters = {'n_neighbors': neigh}\nclf = GridSearchCV(KNeighborsClassifier(), parameters, scoring = 'f1_weighted')\nclf = clf.fit(X_val, y_val)\nopt_neigh = clf.best_params_['n_neighbors']\nprint(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))","ef6bb29d":"# KNN\nknn = KNeighborsClassifier(opt_neigh)\nknn = knn.fit(X_train, y_train)\nneigh_predict = knn.predict(X_test)\n\n#metrics:\nneigh_acc = accuracy_score(neigh_predict, y_test)\nneigh_preci = precision_score(neigh_predict, y_test, average = 'weighted')\nneigh_sensitivity = recall_score(neigh_predict, y_test, average = 'weighted')\nneigh_f1 = f1_score(neigh_predict, y_test, average = 'weighted')\n\nclassification_metrics.loc[\"KNN\"] = [neigh_acc,neigh_preci,neigh_sensitivity,neigh_f1]\nplot_confusion_matrix(knn, X_test, y_test, display_labels = levels,\n                      cmap = plt.cm.Blues, values_format = '0.2%', normalize = 'true')","439189ce":"# Evaluating best parameters\nCs = np.logspace(-1, 1, 10)\nparameters = {'C': Cs}\nclf = GridSearchCV(svm.SVC(kernel = \"rbf\", gamma = 'auto', random_state = SEED), \n                       parameters, scoring = 'f1_weighted')\nclf = clf.fit(X_val, y_val)\nopt_C = clf.best_params_['C']\nprint(\"The best parameters are %s with a score of %0.2f\"\n      % (clf.best_params_, clf.best_score_))","3141af22":"# SVM\nsvm_mod = svm.SVC(C = opt_C, kernel = \"rbf\", gamma = 'auto', random_state = SEED)\nsvm_mod = svm_mod.fit(X_train, y_train)\nsvm_mod_predict = svm_mod.predict(X_test)\n\n#metrics:\nsvm_acc = accuracy_score(svm_mod_predict, y_test)\nsvm_preci = precision_score(svm_mod_predict, y_test, average = 'weighted')\nsvm_sensitivity = recall_score(svm_mod_predict, y_test, average = 'weighted')\nsvm_f1 = f1_score(svm_mod_predict, y_test, average = 'weighted')\n\nclassification_metrics.loc[\"SVM\"] = [svm_acc,svm_preci,svm_sensitivity,svm_f1]\nplot_confusion_matrix(svm_mod, X_test, y_test, display_labels = levels, cmap = plt.cm.Blues,\n                      values_format = '0.2%', normalize = 'true')","c6c33140":"classification_metrics.sort_values('F1 Score')","60baa55d":"binary_metrics = pd.DataFrame(columns=[\"Accuracy\", \"Precision\", \"Sensitivity\", \"F1 Score\"])\n\n# Logistic regression\nlogistic = LogisticRegression(max_iter = 10000, random_state = SEED, solver = 'lbfgs')\nlogistic.fit(Xbin_train, ybin_train)\nlogistic_predict = logistic.predict(Xbin_test)\nlogi_acc = accuracy_score(logistic_predict, ybin_test)\nlogi_preci = precision_score(logistic_predict, ybin_test)\nlogi_sensitivity = recall_score(logistic_predict, ybin_test)\nlogi_f1 = f1_score(logistic_predict, ybin_test)\nbinary_metrics.loc[\"Logistic regression\"] = [logi_acc,logi_preci,logi_sensitivity,logi_f1]\n\ndepth = np.arange(4, 20) # for trees and forests\nparameters = {'max_depth': depth}\n# Decision Tree\n## Entropy criterion\ntree_Entropy = GridSearchCV(tree.DecisionTreeClassifier(criterion = 'entropy', random_state = SEED), parameters, scoring = 'f1')\ntree_Entropy = tree_Entropy.fit(Xbin_train, ybin_train)\ntree_Entropy_pred = tree_Entropy.predict(Xbin_test)\ntree_acc = accuracy_score(tree_Entropy_pred, ybin_test)\ntree_preci = precision_score(tree_Entropy_pred, ybin_test)\ntree_sensitivity = recall_score(tree_Entropy_pred, ybin_test)\ntree_f1 = f1_score(tree_Entropy_pred, ybin_test)\nbinary_metrics.loc[\"Tree Entropy\"] = [tree_acc, tree_preci, tree_sensitivity, tree_f1]\n## Gini criterion\ntree_Gini = GridSearchCV(tree.DecisionTreeClassifier(criterion = 'gini', random_state = SEED), parameters, scoring = 'f1')\ntree_Gini = tree_Gini.fit(Xbin_train, ybin_train)\ntree_Gini_pred = tree_Gini.predict(Xbin_test)\ntree_acc = accuracy_score(tree_Gini_pred, ybin_test)\ntree_preci = precision_score(tree_Gini_pred, ybin_test)\ntree_sensitivity = recall_score(tree_Gini_pred, ybin_test)\ntree_f1 = f1_score(tree_Gini_pred, ybin_test)\nbinary_metrics.loc[\"Tree Gini\"] = [tree_acc, tree_preci, tree_sensitivity, tree_f1]\n\n# Random forest\nparameters = {'max_depth': depth}\n## Entropy criterion\nforest_Entropy = GridSearchCV(RandomForestClassifier(n_estimators = 100, criterion = 'entropy', \n                                                     random_state = SEED), parameters, scoring = 'f1')\nforest_Entropy = forest_Entropy.fit(Xbin_train, ybin_train)\nforest_Entropy_pred = forest_Entropy.predict(Xbin_test)\nforest_acc = accuracy_score(forest_Entropy_pred, ybin_test)\nforest_preci = precision_score(forest_Entropy_pred, ybin_test)\nforest_sensitivity = recall_score(forest_Entropy_pred, ybin_test)\nforest_f1 = f1_score(forest_Entropy_pred, ybin_test)\nbinary_metrics.loc[\"Forest Entropy\"] = [forest_acc,forest_preci,forest_sensitivity,forest_f1]\n## Gini criterion\nforest_Gini = GridSearchCV(RandomForestClassifier(n_estimators = 100, criterion = 'gini', \n                                                  random_state = SEED), parameters, scoring = 'f1')\nforest_Gini = forest_Gini.fit(Xbin_train, ybin_train)\nforest_Gini_pred = forest_Gini.predict(Xbin_test)\nforest_acc = accuracy_score(forest_Gini_pred, ybin_test)\nforest_preci = precision_score(forest_Gini_pred, ybin_test)\nforest_sensitivity = recall_score(forest_Gini_pred, ybin_test)\nforest_f1 = f1_score(forest_Gini_pred, ybin_test)\nbinary_metrics.loc[\"Forest Gini\"] = [forest_acc,forest_preci,forest_sensitivity,forest_f1]\n\n# KNN\nneigh = np.arange(2, 20)\nparameters = {'n_neighbors': neigh}\nknn = GridSearchCV(KNeighborsClassifier(), parameters, scoring = 'f1')\nknn = knn.fit(Xbin_train, ybin_train)\nknn_predict = knn.predict(Xbin_test)\nneigh_acc = accuracy_score(knn_predict, ybin_test)\nneigh_preci = precision_score(knn_predict, ybin_test)\nneigh_sensitivity = recall_score(knn_predict, ybin_test)\nneigh_f1 = f1_score(knn_predict, ybin_test)\nbinary_metrics.loc[\"KNN\"] = [neigh_acc,neigh_preci,neigh_sensitivity,neigh_f1]\n\n# SVM\nCs = np.logspace(-2, 2, 8)\nparameters = {'C': Cs}\nsvm_mod = GridSearchCV(svm.SVC(gamma = 'auto', kernel = \"rbf\", random_state = SEED), parameters, scoring = 'f1')\nsvm_mod = svm_mod.fit(Xbin_train, ybin_train)\nsvm_mod_predict = svm_mod.predict(Xbin_test)\nsvm_acc = accuracy_score(svm_mod_predict, ybin_test)\nsvm_preci = precision_score(svm_mod_predict, ybin_test)\nsvm_sensitivity = recall_score(svm_mod_predict, ybin_test)\nsvm_f1 = f1_score(svm_mod_predict, ybin_test)\nbinary_metrics.loc[\"SVM\"] = [svm_acc,svm_preci,svm_sensitivity,svm_f1]\n\n# MODEL COMPARISON\nbinary_metrics.sort_values('F1 Score')","6302488d":"## 5. <a class=\"anchor\" id=\"fifth\">Classification Models<\/a>\nClassification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.\n\n","98d8073f":"The cumulative variance plot show us that we can cover about the 90% of the variance by using the first 36 principal components. I decide to use them.","5d7ee619":"### 5.1 <a class=\"anchor\" id=\"fifth_1\">Full classification<\/a>","3fa1fb8c":"### 4.2 <a class=\"anchor\" id=\"fourth_2\">Principal Component Analysis<\/a>\n[Principal component analysis (PCA)](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) is a statistical procedure that uses an orthogonal transformation to reduce data dimensionality.\nThis occurs through a linear transformation of the variables that projects the original ones into a new Cartesian system in which the new variable with the greatest variance is projected on the first axis, the new variable, the second for the size of the variance, on the second axis and so on.","ee58e778":"### 5.2 <a class=\"anchor\" id=\"fifth_2\">Binary classification<\/a>","ddc8f347":"## 2. <a class=\"anchor\" id=\"second\">Data preprocessing<\/a>\nAfter importing data and the Python libraries that we will use, the first step is to preprocess the data: we have to convert the data from one format to another, we will check the missing or invalid values and convert everything in a standardized format on all the data, so that we can easily manage and analyze them.","3565f57a":"These have to be considered invalid data. I ignore these records.","900a5274":"Now I check for null values in the dataset:","2a7f2325":"## 6. <a anchor='sixth'>References<\/a>","d9575a41":"#### Reason and datatime","7cb003c1":"## 4. <a class=\"anchor\" id=\"fourth\">Data preparation<\/a>","8cf46175":"#### <a class=\"anchor\" id=\"fifth_5\">Support vector machine<\/a> ","d35db70e":"#### Personal and family","b5c64307":"I checked for the zeros and the invalid values among the dataset. \nNow I prepare data for visualization and analysis.","0b268bbc":"There are no missing values in the dataset. Now we have to check the invalid data in the dataset. \n\n\n","4ff5481d":"Overall, considering the three classes and the unbalanced sets, we can conclude that the initial work on preprocessing, labeling and oversampling of data has paid off.\n\nUsing SMOTE-based data, classification algorithms work pretty well.\nForests have the best performances, while trees are the worsts. KNN and SVM also work in a good way, as they are over the 70% for all metrics.\n\nWe also see that Entropy criterion performs better than the Gini one for both forests and trees, even if only slightly.","cf4c7499":"Clearly, in *Abseteeism time in hours* there is a great presence of outliers. It seems there are few observations particularly unusual.","6617c60e":"\nSome features are being highlighted in the heatmap. Let's have a look to the more intresting. However, strong correlation among *bmi*, *height* and *weight* and between *distance* and *transport expences* may suggest to remove some feature.\n\n1. Strong positive correlation between *Age* and *Service time*.\n2. Positive correlation between *Service time* and *bmi* (and so *Weight*).\n3. Negative correlation between *Transport expences* and *Service time*.\n4. Negative correlation between *Service time* and number of *Pet*.\n5. Positive correlation between *Transport expense* and number of *pets* and *son*.\n\nWe notice that no feature have strong negative or positive correlation with *Absenteeism time in hours*. We have to go deeper. What is more, clearly in the heatmap they are not considered the categorical data.\n\n\n","a88a40d6":"Absenteeism time is highly skewed due to presence of outliers. ","6567e8cd":"Before entering the analysis, it is advisable to rename the columns of the dataset, in order to have more easily traceable variables.\n\n\n","a7c25a1a":"It is not a small number. It is necessary to go deeper before decide how to procede.","8721c5e0":"\n* Martiniano, A., Ferreira, R. P., Sassi, R. J., & Affonso, C. (2012). Application of a neuro fuzzy network in prediction of absenteeism at work. In Information Systems and Technologies (CISTI), 7th Iberian Conference on (pp. 1-4). IEEE.\n* An Introduction to Statistical Learning with application in R, Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani\n* [Performance Metrics for Classification problems in Machine Learning\n](https:\/\/medium.com\/thalus-ai\/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b)\n* [Credit Fraud || Dealing with Imbalanced Datasets\n](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets)\n* [SMOTE and ADASYN (Handling Imbalanced Data Set)\n](https:\/\/medium.com\/coinmonks\/smote-and-adasyn-handling-imbalanced-data-set-34f5223e167)\n* [Oversampling - imbalanced-learn documentation](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/over_sampling.html)\n\n\n","edb3ba41":"By using this configuration settings, all models perform very well.\nWe see that with this choice of parameters we have a high level of precision and accuracy, while sensitivity remains somewhat lower. It is often appropiate to use *F1 Score* as a comparing measure, as it is a composed metrics that consider all of the other metrics. \n\nIn general, we can manage a trade off between these metrics: we could choose the classifier parameters in order to privilege one of that, depending on whether we prefer to have false positives or false negatives. For example, a company may wish to take into account the risk of someone being absent even if this is very low. All of this can be easily performed by considering it in the validation step.","d8cd35e0":"A very important feature is *Reason of absence*. Let's analyze which reasons are most related to each level of absence.","12812008":"#### <a class=\"anchor\" id=\"fifth_4\">K Nearest neighbours<\/a> ","86520281":"Complexively, the best seems to be the SMOTE set: I will use it.","95dd7f33":"Looking at the summary, we immediately see two anomalies: \n* *month* should be from 1 to 12, but it has at least a 0 value\n* *reason* should be from 1 to 28, but it has at least a 0 value\n","1aa24a9a":"The basic Support-Vector Machine (SVM) model is linear: it finds the optimal hyperplane between the points of two classes such that the distance of the nearest points to the decision boundary is maximized.\n\nClearly, it is not common to have a linear separation and SVM can be generalized.\n\nThis generalization can be made by choosing a non-linear Kernel function to apply to the support vector machine.\n\nLinear support vector machine generally uses as linear kernel:$$ k({\\vec{x_i}},{\\vec{x_{j}}})={\\vec{x_{i}}}\\cdot{\\vec{x_{j}}}$$\n\nProbably the common non-lineare alternative is the Radial basis kernel:$${k({\\vec{x_i},\\vec{x_{j}}})=\\exp(-\\gamma \\|{\\vec {x_{i}}}-{\\vec {x_{j}}}\\|^{2})}$$where ${\\displaystyle \\gamma >0}$ is a parameter to chose. This is in fact the default kernel function in the library.\n\nBasically, the kernel maps the data into another space in which the class can be linearly separated, while in the original space, the boundary will in general be non linear.","568ed24a":"The dataset is clearly unbalanced: the number of records of the classes can be too different. I will solve this issue with **oversampling** later.\n\n","0d894b3f":"Apart from the target variabile, the dataset we use has now 10 numerical features and 8 categorical feature. Among the categorical feature, 4 of them are binary and 4 are nominal.\n\nThere is something to do in order to work with the nominal values. The original dataset applies **label encoding**: nominal values are encoded as integer positive values. \n\nThe problem is that numerical values implies an order, but clearly we can't say that *Summer* is bigger than *Winter* or that *Diseases of the nervous system* is smaller than *Diseases of the respiratory system*. This tecnique can be useful in when applied to values that can be ordedered or have some sort of heriarchy, for example *Education* can be seen as the *Level of education* and so an order could make sense. Anyway, this is not the case for most of the nominal features here.\n\nTo overcame this problem, the most commonly technique used is **one hot encoding**. Each category value is converted into a new column: we assign 1 to the corresponding value and 0 to everything else. \n\nAppling it here, this implies adding about 50 new columns: one for each levels of each categorical attribute. In general, this can't be ignored and made straigthforward. \n\nI can try reducing dimensionality by removing some of the features. By prevision exploration, I can say that some data can be redundant. In particular, we have seen that the month of absence does not seem to be information particularly linked to the duration of absenteeism. Since this feature involves adding 12 columns and that we already have the feature on the season, I decide not to consider the month in the analysis.\n\nHowever, we still have a lot of columns.\n","4c07f6f4":"#### <a class=\"anchor\" id=\"fifth_1\">Logitistic regression<\/a>\nLogistic regression is a statistical model used to model the probability of a certain class or event existing.\n\nIf classes are binary, then we can compute the probability that a certain sample belongs to the cateogy $Y = 1$: \n\n$p(x) = \\mathbb{P}(Y=1\\vert X)$\nIt has distribution: $$p(x) = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}$$\n\nIt can been generalized for $p$ variables: $$p(x) = \\frac{e^{\\beta_0 + \\beta_1x + \\dots + \\beta_px_p}}{1+e^{\\beta_0 + \\beta_1x + \\dots + \\beta_px_p}}$$","48da49a9":"Now that we have the data, let's take a look at it to evaluate how to proceed.","bbc39530":"I have to choose which of my datasets to use for classification. I decide to evaluate all of them using the random forest classifier and then I select the one which performs better.\n\n","91610236":"#### <a class=\"anchor\" id=\"fifth_6\">Model Comparison<\/a> ","4184af6d":"#### <a class=\"anchor\" id=\"fifth_3\">Random forest<\/a> \n\nAs the name may suggest, the Random Forest classifier is obtained by parallelly using several decision tree at the same time.\n\nThe general idea is to fit a (reasonably) large number of decision tree to different subsample and then let the majority to decide the label. Random forests are a solution that minimizes the overfitting of the training set compared to decision trees.\n\nAs for the Decision tree, we will use either the Gini and the Entropy criterion.","f7a7893d":"### 4.1 <a class=\"anchor\" id=\"fourth_1\">Relabeling<\/a>\n","4e249a72":"Recalling the coding on the reasons for absence reported at the beginning, we note for example that practically all absences whit *reason* = 0 (i.e. Not available) refer to late. We had already noticed this in the Data Preprocessing section. \n\nWe also notice that the *Reason of absence codes* for *hours* are mainly distributed from 22 to 28, corresponding to no International Code of Diseases (ICD), i.e. not illnesses or insults, but check-ups, medical consultations and others.\n\nOn the contrary, for absences of a day or more, the reason is often a serious medical problem.\n\nLet's see, for each *Reason*, how records are distribuited among the levels of absence.","7c68a56f":"The K-nearest neighbors (KNN) is an algorithm used in the recognition of patterns for classifying objects based on the characteristics of the objects close to the one considered. It is one the simplest algorithms among those used in machine learning. A new point is categorized based on the similiraties of the $K$ data points that are the closest to it. The choice of the parameter $K$ depends on the characteristics of the data. Generally, as $K$ increases, the noise that compromises the classification is reduced, but the criterion of choice for the class becomes more rough. The choice can be made through heuristic techniques.","eb9b4bcf":"Sometimes simple plotting can help understanding how to proceed. I analyze the distribution of *Absenteeism time in hours* and then we try to undestrand the relationships between the other features.","9b2c55df":"# Analysis of Absenteeism at work\nGianluigi Lopardo - Tesina Data Spaces a.a 2019\/2020\n","b7ca51ba":"We see that there are several categorical attributes represented using integer values. In order to manage them, it is opportune to set their type as [Categorical data](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/categorical.html). ","55ecdba5":"#### <a class=\"anchor\" id=\"fifth_2\">Decision Tree<\/a> \nA decision tree is a very common model for classification problems (it is also called a classification tree). In this context a decision tree describes a tree structure where the leaf nodes represent the classifications and the ramifications the set of properties that lead to those classifications. Consequently, each internal node is a macro-class consisting of the union of the classes associated with its child nodes.\n\nIn many situations it is useful to define a halting criterion, or even pruning criterion in order to determine its maximum depth. This is because the growing depth of a tree (or its size) does not directly affect the goodness of the model. Indeed, an excessive growth in the size of the tree could only lead to a disproportionate increase in computational complexity compared to the benefits regarding the accuracy of the predictions\/classifications.\n\nThe algorithm chooses a variable at each step that best splits the set of items, according to certain metrics.\n\nThe parameters that are mostly used to guide the construction of the tree are the Gini index and the Entropy deviance.\n\nThe Gini Index is computed as:$$ GI(t) = 1-\\sum_{j=1}^k p(j|t)^2$$\nThe Entropy deviance is computed as:$$ Entropy(t) = -\\sum_{j=1}^k p(j|t)log_2(p(j|t))$$\n$\\forall \\text{ node } t$, where $p(j|t)$ is the relative frequency of class $j$ at node $t$.\n","54846fdc":"## 3.  <a class=\"anchor\" id=\"third\">Exploratory Data Analysis<\/a>\n\n","de53bd9c":"## 1. <a class=\"anchor\" id=\"first\">Introduction<\/a>\n\nThe dataset used is [Absenteeism at work](http:\/\/archive.ics.uci.edu\/ml\/datasets\/Absenteeism+at+work) from UCI Machine learning repository. The aim of this work is to apply **classification** to **predict** absenteeism at work.\n\nAbsenteeism represents for the company the loss of productivity and quality of work. Predicting it can help companies organize tasks appropriately in order to optimize work and avoid stressful situations for both the company and its employees.\n\nThe Analysis is conducted in Python using Colab Notebook, which is a web application that allows you to create an interactive environment that contains live code, visualizations and text. \n\nThe dataset contains 740 entries. Each entry has 21 attributes.\nIt was created with records of absenteeism at work from July 2007 to July 2010 at a courier company in Brazil.\n\n### Attribute Information:\n\n* Individual identification (ID)\n* Reason for absence\n\n Absences attested by the [International Code of Diseases (ICD)](https:\/\/www.who.int\/classifications\/icd\/en\/) stratified into 21 categories (1 to 21) as follows and 7 categories without ICD (22 to 28):\n  1. Certain infectious and parasitic diseases\n  2. Neoplasms\n  3. Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism\n  4. Endocrine, nutritional and metabolic diseases\n  5. Mental and behavioural disorders\n  6. Diseases of the nervous system\n  7. Diseases of the eye and adnexa\n  8. Diseases of the ear and mastoid process\n  9. Diseases of the circulatory system\n  10. Diseases of the respiratory system\n  11. Diseases of the digestive system\n  12. Diseases of the skin and subcutaneous tissue\n  13. Diseases of the musculoskeletal system and connective tissue\n  14. Diseases of the genitourinary system\n  15. Pregnancy, childbirth and the puerperium\n  16. Certain conditions originating in the perinatal period\n  17. Congenital malformations, deformations and chromosomal abnormalities\n  18. Symptoms, signs and abnormal clinical and laboratory findings, not elsewhere classified\n  19. Injury, poisoning and certain other consequences of external causes\n  20. External causes of morbidity and mortality\n  21. Factors influencing health status and contact with health services.\n  22. patient follow-up \n  23. medical consultation\n  24. blood donation \n  25. laboratory examination \n  26. unjustified absence \n  27. physiotherapy \n  28. dental consultation\n* Month of absence\n* Day of the week: Monday (2), Tuesday (3), Wednesday (4), Thursday (5), Friday (6)\n* Seasons: summer (1), autumn (2), winter (3), spring (4)\n* Transportation expense\n* Distance from Residence to Work (kilometers)\n* Service time (years)\n* Age\n* Work load Average\/day\n* Hit target\n* Disciplinary failure: yes=1, no=0\n* Education: high school (1), graduate (2), postgraduate (3), master and doctor (4))\n* Son (number of children)\n* Social drinker: yes=1, no=0\n* Social smoker yes=1, no=0\n* Pet (number of pet)\n* Weight\n* Height\n* Body mass index\n* Absenteeism time in hours (target)\n\n","c523e530":"Now I split the set in two subsets:\n* **Training set**: collection of labelled data objects used to learn\nthe classification model\n* **Test set**: Collection of labelled data objects used to\nvalidate the classification model\n\nI use 70% of data as training set and 30% for tests.","0ded67db":"There is one more attribute to check: *disc_fail*. It is a boolean value.","cf533153":"\nHere I apply the most commons classification algorithms and I will compare their performance using diffent metrics:\n\nLet: $TP$ = True positives, $TN$ = True negative, $FP$ = False positive, $FN$ = False negative.\n\n1. **Accuracy** = $\\frac{TP + TN}{TP+TN + FP + FN} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$\n\n2. **Precision** = $\\frac{\\text{TP}}{\\text{TP + FP}}$\n\n3. **Sensitivity** = $\\frac{\\text{TP}}{\\text{TP + FN}}$\n\n4. **Specificity** = $\\frac{\\text{TN}}{\\text{TN + FP}}$\n\n5. **F1 Score** = $2 \\cdot \\frac{\\text{Precision}\\cdot \\text{Sensitivity}}{\\text{Precision + Sensitivity}} $\n\nAll of these can be easily generalized to configurations with more than two classes by using weighted versions.\n\nGreat part of the work was to identify limits and peculiarities of different metrics. I also use **confusion matrix** as a measure of performance.","495b72ad":"In order to make **prediction**, I would like to be as specific as possible and therefore to make a **classification** that is well suited to the application case. I want to divide the values of *Absenteeism in hours*, considering the effective time of absence. Considering a work day of 8 hours and I split the set in three labels:\n* *late*: if the absence lasts strictly less than 2 hours\n* *hours*: if the absence was for about less than a work day\n* *days*: if the absence was for one day or more\n","c6a91706":"We would like to reduce dimensionality in order to make our algorithm more agile but this has to be made by paying attention to not losing information.\n\nWe have seen that *bmi* and *weight* have a very strong correlation: 0.9. This was expected, considering how it is calculated:\n$$ bmi = \\frac{weigth}{height^2} $$\n\nWe can say that we don't lose information by removing one of the two.\nSince also height is in my dataset, I decide to drop *weight*.\n\n","b59c9ab3":"### Correlation","9e516dde":"#### Distribution of absenteeism","66ad73f3":"\n### Table of contents\n1. [Introduction](#first)\n2. [Data preprocessing](#second)\n3. [Exploratory Data Analysis](#third)\n4. [Data preparation](#fourth)\n  * 4.1 [Relabeling](#fourth_1)\n  * 4.2 [Principal Component Analysis](#fourth_2)\n  * 4.3 [Oversampling](#fourth_3)\n5. [Classification Models](#fifth)\n  * 5.1 [Full classification](#fifth_1)\n  * 5.2 [Binary classification](#fifth_2)\n6. [References](#sixth)\n  ","45882dbf":"### 4.3 <a class=\"anchor\" id=\"fourth_3\">Oversampling<\/a>\nAs discussed before, in the classification setting the dataset is heavily skewed towards low level of absenteeism and our classes are strongly unbalanced. We can solve this by oversamplig. \n\nThere are three common techniques to do this:\n\n1. The simplest is **Random oversampling**: it simply generates randomly new samples in the classes which are under-represented. \n\n2. **SMOTE** (Synthetic Minority Oversampling Technique): take a sample from the dataset, and consider its $k$ nearest neighbors (in feature space). To create a synthetic data point, take the vector between one of those $k$ neighbors, and the current data point. Multiply this vector by a random number $x$ which lies between $0$, and $1$. Add this to the current data point to create the new, synthetic data point.\n\n3. **ADASYN** (ADAptive SYNthetic sampling approach) algorithm, builds on the methodology of SMOTE, by shifting the importance of the classification boundary to those minority classes which are difficult. ADASYN uses a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn.","18ba6d11":"Now we use a two-classes target, trying to predict if the absence will last a day or more.","373778b7":"Here I try to predicts absenteeism levels using three classes: *late*, *hours*, *days*. Remember that these classes are strongly unbalanced as they have been simply chosen by considering the application.","069ed73c":"Now let's check *Reason of absence*.","0443da0a":"40 records have *disc_fail=1*, everyone else has 0.\nWe notice that all the records having *Disciplinary failure* correspond to the ones with *abs_hours* = 0. This relationship will certainly be useful in our classification models.\n\nIt could be interesesting to know the distribution of these *disciplinary failure* over the employees and understand if those who received it tending to do less delays or absences. Anyway, we don't have this information.","5a7a7cde":"I see that all the records having *reason = 0* also have *abs_hours = 0*. \nThese records probably refers to cases of late work for less than an hour and so justification is not available. Considering these records into the analysis could be interesting and also useful for a company, so I keep them.","263739a2":"On the other hand, watching at the distribution of the dataset, we can expect that applying such a division the models will not perform at their best.\n\nWe can have better performance (and therefore more safety) using only two classes. I apply **binary classification** and I try to predict absense that last one day ore more."}}