{"cell_type":{"1cd2c441":"code","d72697ed":"code","38afbd5d":"code","3c5a50ce":"code","72a4b8eb":"code","56d9e312":"code","a9787504":"code","3d2ede68":"code","d9c9508f":"code","fc71039a":"code","5ab10601":"code","cc39e7de":"code","96f37923":"code","01020d99":"code","3e3143e1":"code","ef4f2bd1":"code","c11d4aa7":"code","e53f96cd":"code","aae49e97":"code","06fe31ac":"code","de18b2c1":"code","f7df7cda":"code","59b08694":"code","1a8bc791":"code","4d83b700":"code","bc263fdd":"code","0ee1e0b0":"code","2e12fa51":"code","5da81c0f":"code","e5bf9c4d":"code","e035a1eb":"code","b26f5fc0":"code","84851642":"code","c7425ef4":"code","960b1168":"code","5c29c536":"code","d88e7e67":"code","fa5b4a1e":"code","b125be7f":"code","384a1098":"code","0996c2d8":"code","68da6b0d":"code","7b28ac24":"code","0afdd717":"code","e512c5ce":"code","4bc35c7e":"code","71f4f380":"code","df669d71":"code","5cd5522c":"code","1ef9c253":"code","42043587":"code","dd72be8c":"code","970d0306":"code","709cdca6":"code","f024ec26":"code","8350f30d":"code","5c109510":"markdown","df9d53b5":"markdown","16618469":"markdown","cbeff82b":"markdown","ddbff62d":"markdown","973d66ea":"markdown","4ecfbad9":"markdown","533c0290":"markdown","85ae4e9a":"markdown","22be795e":"markdown","85b60dce":"markdown","c12edeb4":"markdown","ac547f5d":"markdown","12a15b36":"markdown","7e125112":"markdown","145d45fe":"markdown","70c80891":"markdown","ab3e0999":"markdown","6f5c99fc":"markdown","52a38984":"markdown","5e5111ee":"markdown","d6034b5f":"markdown","a4de40ca":"markdown","553361c7":"markdown","1cf96b03":"markdown","6abd23c2":"markdown","43568e1f":"markdown","12253e78":"markdown","eec353e4":"markdown","99083dd6":"markdown","eb6aea93":"markdown","873730e1":"markdown","2ceb8c4c":"markdown","8b34bc76":"markdown","ccd21d71":"markdown","4413a9a5":"markdown","65977c7e":"markdown","2d81f78d":"markdown","ea4bd7e1":"markdown","97119d0c":"markdown"},"source":{"1cd2c441":"# Import libraries\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom datetime import datetime\n\n# libraries for visualization\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport pyLDAvis\nimport pyLDAvis.gensim\n\n# For latent dirichlet allocation\nimport spacy\nimport gensim\nfrom gensim import corpora\n\n# For modelling and ELI5 analysis\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import plot_confusion_matrix\nimport eli5\n\nimport nltk \nimport string\nimport re\nimport pandas as pd\nimport numpy as np\n\nimport random\nimport datetime as dt\n\nfrom matplotlib import pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport seaborn as sns\n\n\nfrom plotly.subplots import make_subplots\n\nimport plotly\n#import plotly.plotly as py\nfrom plotly import graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n%matplotlib inline\n\n\nimport PIL\nfrom PIL import Image\n\nfrom nltk.probability import FreqDist\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport pyLDAvis.gensim\n","d72697ed":"# Read in data\ndata = pd.read_csv('\/kaggle\/input\/trust-pilot-reviews\/uk_supermarkets_trustpilot_reviews.csv', index_col = 0)\ndata.head()","38afbd5d":"# Download some stopwords\nnltk.download('wordnet')\n\n\ndef remove_punct(text):\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    text = re.sub('[0-9]+', '', text)\n    return text\n\ndef tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndef remove_stopwords(text, stopword):\n    text = [word for word in text if word not in stopword]\n    return text\n\ndef lemmatizer(text, wn):\n    text = [wn.lemmatize(word) for word in text]\n    return text\n\n\n\ndef clean_data(df):\n    ## Step 1: Remove duplicate entries\n    df.drop_duplicates(inplace=True)\n    \n    ## Step 2: Remove punctuation\n    punctuation = ['.', '?', '!', '$', '\u00a3', '\\'', ',']\n    word_cols = ['title', 'content']\n    for col in word_cols:\n        for punc in punctuation:\n            df[col] = df[col].str.replace(punc, '')\n        # And make all lower case\n        df[col] = df[col].str.lower()\n        df[f'{col}_punct'] = df[col].apply(lambda x: remove_punct(x))\n        df[f'{col}_tokenized'] = df[f'{col}_punct'].apply(lambda x: tokenization(x.lower()))\n        stopword = nltk.corpus.stopwords.words('english')\n        df[f'{col}_nonstop'] = df[f'{col}_tokenized'].apply(lambda x: remove_stopwords(x, stopword))\n        wn = nltk.WordNetLemmatizer()\n        df[f'{col}_lemmatized'] = df[f'{col}_nonstop'].apply(lambda x: lemmatizer(x, wn))\n        df[f'{col}_clean'] = df[f'{col}_lemmatized'].str.join(' ')\n        df.drop([f'{col}_punct', f'{col}_tokenized', f'{col}_nonstop', f'{col}_lemmatized'],\n                axis = 1, inplace=True)\n    \n    # Now let's add some date features\n    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d').astype('datetime64[ns]')\n    df['year'] = df['date'].dt.year.astype('int16')\n    df['month'] = df['date'].dt.month.astype('int16')\n    df['week'] = df['date'].dt.week.astype('int16')\n    df['day'] = df['date'].dt.day.astype('int16')\n    df['wday'] = df['date'].dt.weekday.astype('int16')\n    \n    # Change words to number of stars\n    dict_ratings = {'Excellent': 5, 'Great': 4, 'Average': 3, 'Poor': 2, 'Bad': 1}\n    df['num_stars'] = df['rating'].replace(dict_ratings)\n    \n    return df\n\ndef create_fea(df):\n    df['title_num_words'] = df['title'].str.split(' ').str.len()\n    df['content_num_words'] = df['content'].str.split(' ').str.len()\n    df['title_num_char'] = df['title'].str.len()\n    df['content_num_char'] = df['content'].str.len()\n    return df\n\ndef remove_custom_stopwords(df, custom_stop = ['morrisons'],\n                            cols = ['title', 'content', 'title_clean', 'content_clean']):\n    for col in cols:\n        for punc in custom_stop:\n            df[col] = df[col].str.replace(punc, '')\n    return df\n","3c5a50ce":"data = clean_data(data)\ndata.head()","72a4b8eb":"a = [item for sublist in data['content_clean'].str.split().values for item in sublist]\npd.Series(a).value_counts().head(20)","56d9e312":"custom_stopwords = data['company'].unique()\ndata = remove_custom_stopwords(df = data,\n                        custom_stop = custom_stopwords,\n                        cols = ['title_clean', 'content_clean'])\n# Step 4: replace content and title with the clean versions\nREPLACE = True\nif REPLACE:\n    data['title'] = data['title_clean']\n    data['content'] = data['content_clean']\n    data = data.drop(['title_clean', 'content_clean'], axis = 1)\n    \n# Step 5: Create quantitative features\ndata = create_fea(data)\ndata.shape","a9787504":"data.head(3)","3d2ede68":"data = data[15 > data['content'].str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x))]","d9c9508f":"top_pct_to_drop = 0.02\ndata = data.sort_values('content_num_words', ascending=False).iloc[round(data.shape[0] * top_pct_to_drop):,:]","fc71039a":"print(\"Oldest review:\", data['date'].min(), \", Newest review:\", data['date'].max())","5ab10601":"REMOVE_OLD = False # For example, set REMOVE_OLD to '2015-01-01'\nif REMOVE_OLD:\n    data = data[data['date'] > REMOVE_OLD]","cc39e7de":"data['num_stars'].value_counts()","96f37923":"def plot_pie_chart(df):\n\n    temp = df.groupby('num_stars')['title'].count().reset_index().sort_values('num_stars')\n    labels = temp['num_stars'].values\n    cols = ['lightblue', 'red', 'green', 'purple', 'orange']\n    plt.pie(temp['title'], radius=2, autopct = '%0.1f%%',\n            shadow = True, explode = [0.2,0,0,0,0.2],\n            startangle = 0, labels = labels, colors = cols)\n    plt.title('Pie chart showing a breakdown of ratings', fontsize=18, y = 1.5)\n    plt.show()\n    \nplot_pie_chart(df = data)","01020d99":"def plot_star_funnel(df):\n    temp = df.groupby('num_stars')['title'].count().reset_index().sort_values('num_stars')\n    fig = go.Figure(go.Funnelarea(\n        text = temp['num_stars'],\n        values = temp['title'],\n        title = {'position': 'top center', 'text': 'Funnel chart of ratings'}\n    ))\n    \n    fig.update_layout(\n        titlefont=dict(\n            family=\"InterFace\",\n            size=30,\n        )\n    )\n    #fig.show()\n    iplot(fig)\n    \nplot_star_funnel(data)","3e3143e1":"def stratified_sample_df(df, col, n_samples):\n    n = min(n_samples, df[col].value_counts().min())\n    df_ = df.groupby(col).apply(lambda x: x.sample(n))\n    df_.index = df_.index.droplevel(0)\n    return df_\n\nSTRATIFY_REVIEWS = False\nif STRATIFY_REVIEWS:\n    data = stratified_sample_df(data, 'num_stars', 1000)","ef4f2bd1":"# Source - https:\/\/www.kaggle.com\/datafan07\/disaster-tweets-nlp-eda-bert-with-transformers\ndef plot_dist3(df, feature, title):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n    # Creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n\n    # Customizing the ecdf_plot.\n    ax2 = fig.add_subplot(grid[1, :2])\n    # Set the title.\n    ax2.set_title('Empirical CDF')\n    # Plotting the ecdf_Plot.\n    sns.distplot(df.loc[:, feature],\n                 ax=ax2,\n                 kde_kws={'cumulative': True},\n                 hist_kws={'cumulative': True},\n                 color='#e74c3c')\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n    ax2.set(ylabel='Cumulative Probability')\n\n    # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(x=feature, data=df, orient='v', ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=25))\n\n    \n    \n    \n    lims = 0, df.loc[:, feature].max()\n\n    ax1.set_xlim(lims)\n    ax2.set_xlim(lims)\n    \n    \n    plt.suptitle(f'{title}', fontsize=18)\n\n\ndef plot_word_len_histogram(textno, textye):\n    \n    \"\"\"A function for comparing average word length\"\"\"\n    \n    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)\n    \n    left = textno.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x))\n    sns.distplot(left, ax=axes[0], color='#e74c3c')\n    \n    right = textye.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x))\n    sns.distplot(right, ax=axes[1], color='#e74c3c')\n    \n    mx_l = max(left)\n    mx_r = max(right)\n    \n    mi_l = min(left)\n    mi_r = min(right)\n    lims = min(mi_r, mi_l), max(mx_r, mx_l)\n    axes[0].set_xlim(lims)\n    axes[1].set_xlim(lims)\n    axes[0].set_xlabel('Word Length')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Positive reviews')\n    axes[1].set_xlabel('Word Length')\n    axes[1].set_title('Negative reviews')\n    \n    fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')\n    fig.tight_layout()\n    \n    ","c11d4aa7":"plot_dist3(data[data['num_stars'] == 5], 'content_num_char',\n       'Characters Per \"Positive review')\n","e53f96cd":"plot_dist3(data[data['num_stars'] == 1], 'content_num_char',\n       'Characters Per \"Negative review')","aae49e97":"plot_word_len_histogram(data[data['num_stars'] == 5]['content'],\n                       data[data['num_stars'] == 1]['content'])","06fe31ac":"plot_dist3(data[data['num_stars'] == 5], 'content_num_words',\n       'Words Per \"Positive review')","de18b2c1":"plot_dist3(data[data['num_stars'] == 1], 'content_num_words',\n       'Words Per \"Negative review')","f7df7cda":"sns.set(font_scale = 2)\ng = sns.FacetGrid(data, col='num_stars', height=4)\ng.map(plt.hist,'content_num_char')\nplt.subplots_adjust(top=0.8)\ng.fig.suptitle(f'Size of review distribution, by number of stars')\nplt.show()","59b08694":"# This is not my creation - I have lost the surce but I will try and find and credit, please message me if you know where to find this\ndef ngrams(df, n, title, mx_df = 0.9, content_or_title = 'content'):\n    \"\"\"\n    A Function to plot most common ngrams\n    content_or_title - use the content of the review, or the title\n    mx_df: Ignore document frequency strictly higher than the given threshold\n    \"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n    axes = axes.flatten()\n    #plt.rcParams.update({'font.size': 25})\n    for rate, j in zip([5, 1], axes):\n        new = df[df['num_stars'] == rate][content_or_title].str.split()\n        new = new.values.tolist()\n        corpus = [word for i in new for word in i]\n\n        def _get_top_ngram(corpus, n=None):\n            #getting top ngrams\n            vec = CountVectorizer(ngram_range=(n, n),\n                                  max_df=mx_df,\n                                  stop_words='english').fit(corpus)\n            bag_of_words = vec.transform(corpus)\n            sum_words = bag_of_words.sum(axis=0)\n            words_freq = [(word, sum_words[0, idx])\n                          for word, idx in vec.vocabulary_.items()]\n            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n            return words_freq[:15]\n\n        top_n_bigrams = _get_top_ngram(df[df['num_stars'] == rate][content_or_title], n)[:15]\n        x, y = map(list, zip(*top_n_bigrams))\n        sns.barplot(x=y, y=x, palette='plasma', ax=j)\n\n        \n        \n    title_font = {'fontname':'Arial', 'size':'24', 'color':'black', 'weight':'normal',\n              'verticalalignment':'bottom'}\n    lab_font = {'fontname':'Arial', 'size':'20'}\n    \n    \n    axes[0].set_title(f'Positive reviews - for {content_or_title} of review', **title_font)\n    axes[1].set_title(f'Negative reviews - for {content_or_title} of review', **title_font)\n    axes[0].set_xlabel('Count', **lab_font)\n    axes[0].set_ylabel('Words', **lab_font)\n    axes[1].set_xlabel('Count', **lab_font)\n    axes[1].set_ylabel('Words', **lab_font)\n    \n    axes[0].tick_params(axis='both', which='major', labelsize=15)\n    axes[1].tick_params(axis='both', which='major', labelsize=15)\n    \n    #fig.suptitle(title, fontsize=24, va='baseline')\n    plt.tight_layout()","1a8bc791":"ngrams(df = data, n = 1, title = 'Most Common Unigrams', mx_df = 0.9, content_or_title = 'content')\nngrams(df = data, n = 1, title = 'Most Common Unigrams', mx_df = 0.9, content_or_title = 'title')","4d83b700":"ngrams(df = data, n = 2, title = 'Most Common Bigrams', mx_df = 0.9, content_or_title = 'content')\nngrams(df = data, n = 2, title = 'Most Common Bigrams', mx_df = 0.9, content_or_title = 'title')","bc263fdd":"ngrams(df = data, n = 3, title = 'Most Common Trigrams', mx_df = 0.9, content_or_title = 'content')\nngrams(df = data, n = 3, title = 'Most Common Trigrams', mx_df = 0.9, content_or_title = 'title')","0ee1e0b0":"def display_topics(text, no_top_words, topic, components = 10):\n    \"\"\"\n    A function for determining the topics present in our corpus with nmf\n    \"\"\"\n    no_top_words = no_top_words\n    tfidf_vectorizer = TfidfVectorizer(\n        max_df=0.90, min_df=25, max_features=5000, use_idf=True)\n    tfidf = tfidf_vectorizer.fit_transform(text)\n    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n    doc_term_matrix_tfidf = pd.DataFrame(\n        tfidf.toarray(), columns=list(tfidf_feature_names))\n    nmf = NMF(n_components=components, random_state=0,\n              alpha=.1, init='nndsvd', max_iter = 5000).fit(tfidf)\n    print(topic)\n    for topic_idx, topic in enumerate(nmf.components_):\n        print('Topic %d:' % (topic_idx+1))\n        print(' '.join([tfidf_feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n    \n    ","2e12fa51":"print('Topics for TITLE of review')\ndisplay_topics(data[data['num_stars'] == 5]['title'], \n               no_top_words = 5,\n               topic = 'Positive review topics \\n',\n               components = 2)\nprint('\\n======================================\\n')\nprint('\\n======================================\\n')\nprint('Topics for BODY of review')\ndisplay_topics(data[data['num_stars'] == 5]['content'], \n               no_top_words = 5,\n               topic = 'Positive review topics \\n',\n               components = 10)","5da81c0f":"print('Topics for TITLE of review')\ndisplay_topics(data[data['num_stars'] == 1]['title'], \n               no_top_words = 5,\n               topic = 'Negative review topics \\n',\n               components = 2)\nprint('\\n======================================\\n')\nprint('\\n======================================\\n')\nprint('Topics for BODY of review')\ndisplay_topics(data[data['num_stars'] == 1]['content'], \n               no_top_words = 5,\n               topic = 'Negative review topics \\n',\n               components = 10)","e5bf9c4d":"DO_LDA = False\nif DO_LDA:\n    CONTENT_OR_TITLE = 'content'\n\n    reviews_split = data[CONTENT_OR_TITLE].str.split()\n    # creating the term dictionary of our corpus, where every unique term is assigned an index\n    dictionary = corpora.Dictionary(reviews_split)\n    # convert the list of reviews (reviews_2) into a Document Term Matrix \n    doc_term_matrix = [dictionary.doc2bow(rev) for rev in reviews_split]\n    # Creating the object for LDA model using gensim library\n    LDA = gensim.models.LdaMulticore # .ldamodel.LdaModel\n    # Build LDA model - we have picked 3 main topics\n    # switch to sklearn.decomposition.LatentDirichletAllocation for consistency?\n    lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=3, random_state=100,\n                    chunksize=1000, passes=50, workers=4)\n    # print out the topics that our LDA model has learned\n    lda_model.print_topics()\n    \n    pyLDAvis.enable_notebook()\n    v = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary, plot_opts = 'ylab')\n    v","e035a1eb":"DO_LDA = False\nif DO_LDA:\n    CONTENT_OR_TITLE = 'title'\n\n    reviews_split = data[CONTENT_OR_TITLE].str.split()\n    # creating the term dictionary of our corpus, where every unique term is assigned an index\n    dictionary = corpora.Dictionary(reviews_split)\n    # convert the list of reviews (reviews_2) into a Document Term Matrix \n    doc_term_matrix = [dictionary.doc2bow(rev) for rev in reviews_split]\n\n    # Creating the object for LDA model using gensim library\n    LDA = gensim.models.LdaMulticore\n\n    # Build LDA model - we have picked 3 main topics\n    lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=3, random_state=100,\n                    chunksize=1000, passes=50, workers=4)\n    # print out the topics that our LDA model has learned\n    lda_model.print_topics()\n\n    # Visualize the topics\n    pyLDAvis.enable_notebook()\n    v = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)\n    v","b26f5fc0":"def plot_wordcloud(df, max_words=400, max_font_size=120, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False, image = 'basket.png',\n                  more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}):\n    comments_text = str(df.values)\n    mask = np.array(PIL.Image.open(image))\n    \n    stopwords = set(STOPWORDS)\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    mask = mask)\n    wordcloud.generate(comments_text)\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'green', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()","84851642":"plot_wordcloud(df = data[data['num_stars'] == 5]['content'],\n               max_words=200,\n               max_font_size=100, \n               title = 'Most common words of POSITIVE reviews',\n               title_size=50,\n               image = '..\/input\/trust-pilot-reviews\/basket.png',\n              more_stopwords = {'store', 'order'})","c7425ef4":"plot_wordcloud(df = data[data['num_stars'] == 1]['content'],\n               max_words=200,\n               max_font_size=100, \n               title = 'Most common words of NEGATIVE reviews',\n               title_size=50,\n               image = '..\/input\/trust-pilot-reviews\/basket.png',\n              more_stopwords = {'store', 'order', 'one'})","960b1168":"def time_series_slider(df, window = 7, add_count = False, add_var = False, add_kurt = False):\n    mean_grp = df.groupby('date')['num_stars'].mean().rolling(window).mean().reset_index().iloc[window:,:]\n    # Create figure\n    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n    fig.add_trace(\n        go.Scatter(x=list(mean_grp['date']), y=list(mean_grp['num_stars']), name=\"rolling mean\"))\n    \n    if add_count:\n        mean_grp = df.groupby('date')['num_stars'].count().rolling(window).mean().reset_index().iloc[window:,:]\n        fig.add_trace(go.Scatter(x=list(mean_grp['date']), y=list(mean_grp['num_stars']), name=\"rolling count\"),\n                     secondary_y = True)\n    \n    if add_var:\n        mean_grp = df.groupby('date')['num_stars'].mean().rolling(window).std().reset_index().iloc[window:,:]\n        fig.add_trace(go.Scatter(x=list(mean_grp['date']), y=list(mean_grp['num_stars']), name=\"rolling std\"),\n                     secondary_y = True)\n    \n    if add_kurt:\n        mean_grp = df.groupby('date')['num_stars'].mean().rolling(window).kurt().reset_index().iloc[window:,:]\n        fig.add_trace(go.Scatter(x=list(mean_grp['date']), y=list(mean_grp['num_stars']), name=\"rolling kurtosis\"),\n                     secondary_y = True)\n    \n    # Set title\n    fig.update_layout(\n        title_text=\"Time series with range slider and selectors\"\n    )\n\n    # Add range slider\n    fig.update_layout(title=\"Rolling mean of average reviews\",\n                      xaxis_title=\"Date\",\n                      yaxis_title=\"Value\",\n        xaxis=dict(\n            rangeselector=dict(\n                buttons=list([\n                    dict(count=1,\n                         label=\"1m\",\n                         step=\"month\",\n                         stepmode=\"backward\"),\n                    dict(count=6,\n                         label=\"6m\",\n                         step=\"month\",\n                         stepmode=\"backward\"),\n                    dict(count=1,\n                         label=\"YTD\",\n                         step=\"year\",\n                         stepmode=\"todate\"),\n                    dict(count=1,\n                         label=\"1y\",\n                         step=\"year\",\n                         stepmode=\"backward\"),\n                    dict(step=\"all\")\n                ])\n            ),\n            rangeslider=dict(\n                visible=True\n            ),\n            type=\"date\"\n        ),\n        yaxis=dict(\n           autorange = True,\n           fixedrange= False\n        )\n    )\n    fig['layout']['yaxis'].update(title = 'Number of stars', range = [0, 5], autorange = False)\n    #fig.show()\n    iplot(fig)","5c29c536":"time_series_slider(df = data, window = 30, add_count = False, add_var = True, add_kurt = False)","d88e7e67":"def trend_barplots(df, mean_or_count = 'mean', plots = ['wday', 'day', 'week', 'month', 'year']):\n    '''\n    Input: \n        df: data\n        mean_or_count: Takes values mean, or count, and decides what to plot\n        plots: by which grouping do you want the plots\n    Output: \n        displays barplots\n    '''\n    # Prep data\n    for col in plots:\n        if col == 'wday':\n            df[col] = pd.to_datetime(df['date'], format='%Y-%m-%d').astype('datetime64[ns]').dt.weekday\n        \n        if mean_or_count == 'mean':\n            grp_wday = df.groupby([col])['num_stars'].mean()\n            lims = (1,5)\n            title_besp = f'Average stars by {col}'\n        elif mean_or_count == 'count':\n            grp_wday = df.groupby([col])['num_stars'].count()\n            # Set limit to 5% above the max\n            lims = (0,grp_wday.max() + round(grp_wday.max() \/ 20))\n            title_besp = f'Count of reviews by {col}'\n        plt.figure(figsize=(18,6))\n        sns.barplot(x=grp_wday.index.values,\n                    y=grp_wday.values, palette='plasma')\n        plt.ylim(lims)\n        plt.tick_params(axis='both', which='major', labelsize=12)\n        \n        plt.title(title_besp, fontsize=20)\n        plt.xlabel(f'{col}', fontsize=18)\n        plt.ylabel('Number of stars', fontsize=16)\n        #plt.tight_layout()\n        plt.show()","fa5b4a1e":"trend_barplots(df = data,\n               mean_or_count = 'count',\n               plots = ['wday', 'day', 'week', 'month', 'year'])","b125be7f":"trend_barplots(df = data,\n               mean_or_count = 'mean',\n               plots = ['wday', 'day', 'week', 'month', 'year'])","384a1098":"# Prep data\ndata_for_reg = data[data['num_stars'] != 3].copy()\ndata_for_reg.loc[:,'target'] = -9999\ndata_for_reg.loc[data_for_reg['num_stars'] < 3, 'target'] = 0 #\u00a00 negative\ndata_for_reg.loc[data_for_reg['num_stars'] > 3, 'target'] = 1 # 1 positive\n\nX_full = data_for_reg['content']\ny_full = data_for_reg['target']\n\n\nvect = TfidfVectorizer()\nX = vect.fit_transform(X_full)\n\ny = y_full\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.20, random_state=23, stratify=y)\nX_train.shape, X_valid.shape","0996c2d8":"\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nprint(\"Train Set Accuracy: {}\".format(metrics.accuracy_score(model.predict(X_train), y_train)))\nprint(\"Train Set ROC: {}\\n\".format(metrics.roc_auc_score(model.predict(X_train), y_train)))\n\nprint(\"Validation Set Accuracy: {}\".format(metrics.accuracy_score(model.predict(X_valid), y_valid)))\nprint(\"Validation Set ROC: {}\".format(metrics.roc_auc_score(model.predict(X_valid), y_valid)))","68da6b0d":"print(metrics.classification_report(model.predict(X_valid), y_valid))","7b28ac24":"# Confusion Matrix\\\nfig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\nsns.set(font_scale=2.0)\nfor norm, j in zip(['true', None], axes):\n    plot_confusion_matrix(model, X_valid, y_valid, normalize = norm, ax = j)\naxes[0].set_title(f'Normalised confusion matrix', fontsize = 24)\naxes[1].set_title(f'Raw confusion matrix', fontsize = 24)\nplt.show()","0afdd717":"target_names = [0, 1]\neli5.show_weights(model, vec=vect, top=100,\n                  target_names=target_names)","e512c5ce":"for iteration in range(5):\n    samp = random.randint(1,data.shape[0])\n    print(\"Real Label: {}, on {}\".format(data['rating'].iloc[samp], data['date'].iloc[samp]))\n    display(eli5.show_prediction(model,data[\"content\"].iloc[samp], vec=vect,\n                         target_names=target_names))","4bc35c7e":"from lightgbm import LGBMClassifier\nmodel = LGBMClassifier(n_estimators=130,\n                            n_jobs=4)\n\nmodel.fit(X_train, y_train)\nprint(\"Train Set Accuracy: {}\".format(metrics.accuracy_score(model.predict(X_train), y_train)))\nprint(\"Train Set ROC: {}\\n\".format(metrics.roc_auc_score(model.predict(X_train), y_train)))\n\nprint(\"Validation Set Accuracy: {}\".format(metrics.accuracy_score(model.predict(X_valid), y_valid)))\nprint(\"Validation Set ROC: {}\".format(metrics.roc_auc_score(model.predict(X_valid), y_valid)))","71f4f380":"grp_by_company = data.groupby('company')['num_stars']\ngrp_by_company.describe()","df669d71":"def plot_ts_by_company(df):\n    ts_grp_data = df.groupby(['company', 'year'])['num_stars'].mean().reset_index()\n    sns.set(style=\"white\")\n    a4_dims = (15, 6)\n    fig, ax = plt.subplots(figsize=a4_dims)\n    sns.set(font_scale=1.5)\n    sns.lineplot(data = ts_grp_data, x='year', y='num_stars', hue='company', ax = ax)\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    ax.set_title('Average number of stars, by company, over time')\n    ax.set_ylabel('Number of stars')\n    ax.set_xlabel('Date')\n    plt.show()","5cd5522c":"plot_ts_by_company(data)","1ef9c253":"a4_dims = (22.5, 6)\nfig, ax = plt.subplots(figsize=a4_dims)\nax = sns.violinplot(x = \"company\", y = \"num_stars\", data = data)","42043587":"a4_dims = (22.5, 6)\nfig, ax = plt.subplots(figsize=a4_dims)\nax = sns.boxplot(x = \"company\", y = \"num_stars\", data = data)","dd72be8c":"queue_reviews = data[(data['content'].str.contains('queue')) | (data['title'].str.contains('queue'))] # Goes from 40k to 1.3k reviews\na4_dims = (22.5, 6)\nfig, ax = plt.subplots(figsize=a4_dims)\nax = sns.violinplot(x = \"company\", y = \"num_stars\", data = queue_reviews)\nplt.show()","970d0306":"lims = (1,5)\ntitle_besp = f'Average stars by company from Queue reviews'\n\nplt.figure(figsize=(18,6))\nsns.barplot(data = queue_reviews.groupby('company')['num_stars'].mean().reset_index(),\n            x = 'company', \n            y = 'num_stars',\n            palette='plasma')\n\nplt.ylim(lims)\nplt.tick_params(axis='both', which='major', labelsize=12)\n\nplt.title(title_besp, fontsize=20)\nplt.xlabel(f'Company', fontsize=18)\nplt.ylabel('Number of stars', fontsize=16)\nplt.show()","709cdca6":"lims = (1,5)\ntitle_besp = f'Average stars by company from Online reviews'\nonline_reviews = data[(data['content'].str.contains('online')) | (data['title'].str.contains('online'))] # Goes from 40k to 1.3k reviews\n\nplt.figure(figsize=(18,6))\nsns.barplot(data = online_reviews.groupby('company')['num_stars'].mean().reset_index(),\n            x = 'company', \n            y = 'num_stars',\n            palette='plasma')\n\nplt.ylim(lims)\nplt.tick_params(axis='both', which='major', labelsize=12)\n\nplt.title(title_besp, fontsize=20)\nplt.xlabel(f'Company', fontsize=18)\nplt.ylabel('Number of stars', fontsize=16)\nplt.show()","f024ec26":"pyramid_reviews = data[(data['content'].str.contains('online')) | (data['title'].str.contains('online'))] # Goes from 40k to 1.3k reviews\npyramid_reviews.loc[pyramid_reviews['num_stars'] == 2, 'num_stars'] = 1\npyramid_reviews.loc[pyramid_reviews['num_stars'] == 4, 'num_stars'] = 5\n# Groupby company, and count the number of stars\none_stars = pyramid_reviews[['company', 'num_stars']][pyramid_reviews['num_stars'] == 1].groupby('company').count().reset_index()\nfive_stars = pyramid_reviews[['company', 'num_stars']][pyramid_reviews['num_stars'] == 5].groupby('company').count().reset_index()\none_stars['num_stars'] = -one_stars['num_stars']\n\nplt.figure(figsize=(15,8))\nbar_plot = sns.barplot(x='num_stars', y='company', data=one_stars, lw=3)\nbar_plot = sns.barplot(x='num_stars', y='company', data=five_stars, lw=3)\nbar_plot.set(xlabel=\"Count of 1 vs 5 star reviews\", ylabel=\"Company\",\n             title = \"Count of 1 (left) and 5 (right) star reviews for reviews containing \\\"online\\\" \")\nplt.xlim(-1500, 1500)\nplt.show()","8350f30d":"pyramid_reviews = data[(data['content'].str.contains('variety')) | (data['title'].str.contains('variety'))] # Goes from 40k to 1.3k reviews\npyramid_reviews.loc[pyramid_reviews['num_stars'] == 2, 'num_stars'] = 1\npyramid_reviews.loc[pyramid_reviews['num_stars'] == 4, 'num_stars'] = 5\n# Groupby company, and count the number of stars\none_stars = pyramid_reviews[['company', 'num_stars']][pyramid_reviews['num_stars'] == 1].groupby('company').count().reset_index()\nfive_stars = pyramid_reviews[['company', 'num_stars']][pyramid_reviews['num_stars'] == 5].groupby('company').count().reset_index()\none_stars['num_stars'] = -one_stars['num_stars']\n\nplt.figure(figsize=(15,8))\nbar_plot = sns.barplot(x='num_stars', y='company', data=one_stars, lw=3)\nbar_plot = sns.barplot(x='num_stars', y='company', data=five_stars, lw=3)\nbar_plot.set(xlabel=\"Count of 1 vs 5 star reviews\", ylabel=\"Company\",\n             title = \"Count of 1 (left) and 5 (right) star reviews for reviews containing \\\"variety\\\" \")\nlim_val = max(abs(one_stars['num_stars'].min()), five_stars['num_stars'].max()) + 5\nplt.xlim(-lim_val, lim_val)\nplt.show()","5c109510":"# Removing reviews with bugs\n- such as no spaces \/ with an average word length greater than, say 15","df9d53b5":"- This is more instructive\n    - This is also the point where we can combine with other types of data to get a more detailed view, for example scrape the number of products each comapany has, or the number of available delivery slots, or the size of the online business vs standard retail shows\n\n\n- This plot makes sense, Ocado is known as the online supermarket in the uk.\n    - So potentially we just have more reviews, and the average for Ocado is naturally high, this is a assumption we can test, but it seems Ocado just does great online delivery.... Asda, not so much\n- Tescos has a good reputation for online shopping, so it is surprising to see the ratings lower down.\n- Morrisons seems to perform better than intuition expects\n\n- Now would be a great time for a deep dive into some of the most instructive \/ demonstrative reviews, in terms of topics\n    - We could use the above LDA work to determine these\n\n\n- For more in-depth analysis, we can use a pyramid plot, this will address the above issue of bias towards the bimodal distribution of 1-5 stars\n    - And means we don't have to use the violin plots, which display a lot of useless (for our purpose) data (2, 3 and 4 stars)\n    - So let's round 2 stars to 1, 4 stars to 5 and drop the 3 star reviews\n        - Then groupby company and number of stars, and pyramid plot the count","16618469":"- As we see, a very simple LGBM is outperformed by the logistic regression\n- For our usecase, 94% accuracy is fine, and we can move on","cbeff82b":"## What about a more powerful model?\n- Here we can look at the differences between a the following and the above","ddbff62d":"- Let's plot these reviews over time, now we have a lot of reviews so it may not be hugely insightful\n    - We see a big drop at 2012, where all the supermarkets go from around 4 stars, to 2 stars\n    - Would be interesting to analyse some extra, maybe news data, to try and explain this\n        - As well as doing a deepdive into what the reviews say around that time","973d66ea":"# Unigrams","4ecfbad9":"- The above barcharts, for count of reviews and average reviews show interesting trends\n    - These can be usful for identifying cheating\n    - Say a company had a spike in 5 star reviews, every saturday night, this may raise some eyebrows\n\n\n\n# Now let's do some predictive modelling\n- The aim of this is to find which words are the best predictors, for a given score, which will help inform what is wrong with a company\n    - Source: https:\/\/www.kaggle.com\/nicapotato\/guided-numeric-and-text-exploration-e-commerce\n\n- So our X data will be content, we can also experiemnt only looking at the title\n- Our y data will be num_stars\n    - Where 1 and 2 stars = negative, \n    - 3 will be dropped (for now)\n    - 4 and 5 will be positive","533c0290":"- The above gives us much more context than the uni\/bi\/trigrams\n    - The positive reviews talk about great service, high quality products and good online shopping\n    - The negative reviews talk about rude customer service, bad call centres and missing items\n\n\n<b> Again we caveat that the overlap from multiple stores skews this <\/b>\n\n\n\n\n\n# Finding topics - Attempt 2 - Using `latent dirichlet allocation`\n- So the above gives us great informaton on topics, with context\n- Now we are just going to be looking for main topics, more similar to unigrams\/bigrams\/trigrams\n    - see https:\/\/miningthedetails.com\/blog\/python\/lda\/GensimLDA\/ for tips on training\n\n- The printed topics, gives use insights into the actual components of the topics\n\n\n- The visualisation, allows us to evaluate the term frequency inside each of the topics, to see what they are comprised of, from the reviews perspecitve, not the modelling perspective\n\n\n- These are amazing plots, that can lead to great interactive \"play\" with the data\n    - We comment out for now as they take a while to load, as well as there being a bug where it reshapes the entire notebook, making the above plots overlap with the edges","85ae4e9a":"- And the respective boxplots","22be795e":"# What is the breakdown of 1 - 5 stars?\n- No we don't need a funnel and a pie chart, but why not ","85b60dce":"# Characters and number of words per review\n- These functions are optimised for looking at 1 company, but are still effective","c12edeb4":"- This is quite an uninspiring application, but the idea is clear, what if we tried a word that appears in more than 2% of reviews...\n\n# Reviews relating to \"online\" shopping\n- Lessons learnt from the above, violin plots arn't the best here, we know most reviews are either 1 or 5 stars\n- Therefore a simple barplot will give us an okay picture, let's take a look","ac547f5d":"- Now what if we want to investigate one specific issue, for example, queue times\n- Then we can apply a filter for reviews about queues, then plot the distribution of ratings \n\n# Queue times by company","12a15b36":"# Comparing companies\n- So the above EDA hasn't been very domain specific, and has treated all the reviews as coming from the same source\n    - This workds great if we filter the data for one company\n    - But we can do better, we have data for many companies...\n\n\n\n<b> So let's describe the average stars given, per review, by company <\/b>","7e125112":"- We see clear peaks and troughs\n    - Most interesting is 2013, where the average reviews goes from 4 stars down to 2\n    - Below we will break this out by company, and see the same result\n    - Interestingly, the standard deviation increases when the average reviews drop\n\n- Dragging the filter to just look at 2020, we see a spike in variance at the same time that corona virus hit the UK \n\n# Trend barplots","145d45fe":"# Word cloud\n- Mainly from https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes\n    - Simple concept, given the correct icon and stopwords, this can lead to a powerful message","70c80891":"<center><img src=\"data:image\/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMAAAADACAMAAABlApw1AAAAGFBMVEUAAC4KtnsGk2sAUyUIqHUAJkADcFoBSk9lZnI4AAAD2klEQVR4nO2cC5akIAxFG1Tc\/46ntErHKB\/5JbxzchfQncB9ikD335+iKIqiKIqiKIqiKIqiKIrSmkW6gFqcdAGVrDP4FDgDPgXWWOkSqliNwXbIGYPtkP00gOzQ8qkf2qHNIGNW6TLKsXsDuA7tBiE79DUI2CH7awDVoWU22A4dBsE6ZM8GMB06DUJ1aDUG2yF7aQBxQXcxCNOhq0GQDlnSAJ5DxCBEh6hBgA7ZWwNoDt0M+jgkXVEmd4PgHLobhObQwyC0Bd3TIDCHnKcBJIc8BmE55DPIGKCXsc8gJIe8BiE55DcIyCG\/QUAO+Q3CcShkEIxDIYNgHHou5LAcChsE8mEZNgjEobBBGA4tkfohHIoZBPFREDMIwaGoQQgOxQ0CcChu0PgOBT4FJBxabQmp+j8dFP3ckraXlA18FNX\/waXHk4O5fAGyjjAJtubBtaSeiv1xlalfZTWa698bolkuTS9FLMsV6aUIZbkqvRSRLNeml8Ke5QbppTBnuU16KYxZbpZeCluWG6aXwpTltumlMGS5eXop3bPcI72UrlnulF5Kxyx3Sy+lW5Z7ppfSJcud00vpkOX+6aU0zjJLeilNs8w9\/DsNs8yXXkqjLLOml9IkyyL6nFRnWSC9lMosyw7\/TlWWpYf\/S3GWBdNLSR4K+BlAnx9lDdhJuu6TyOWCILObpmFmoOBBZKdpnAbyDdqGf0O68h\/ZBh31j9JApkGznU6kS9\/JNOgc\/mEayDPoMvyjNJCzlCDDP8hzKMcgOvyDTEGGQc\/6R2jgtUF3fTac\/HLutUGP4XfbzQr5yysvDboNvzuvhYjH+J1B1lv8hvgnzSuDnL\/4vTXh+t8Y9NPH2dnXrbBDLwyy4eL32ZFtIGnQbF38GpesQ0mDbHrbRXZfKzH8mx+pvS9RhxLXdL+Dm9h2kXQobtC555zYQhV0KGYQ2bWKaiToUGRkb7tWsUmQcyhy0fsxqpEtVLkL4cGavJue4SyLORTSIrDpGdRIyqGAQZEji8CUSTnkLyd6XSCgkZBDXiMSB45+jWQc8hn04sjC90qQcchj0KsjC9\/yTsSh4v994XklSDj0\/O8p77dIHlmWcOg+jFknXo8sC+wP0RKyz6tv\/fM7RA0quCtGNeJ3iIxggz8yYnfo8tuLz6uvrwRuhy4GVZxXXzTiduj\/x2TVZaWLRswOHb+4+rrDqRGvQ4dBDa47HJPA69Ba9vD3cqwsWB2yTfQ5+GaZ06HdoIa3ZXaNOB1am991c7wO2faXlVbO87Jl7nBTdbGMf+zdZ7IHOHNVFEVRFEVRFEVRFEVRFEVRePgHbpkisbRZTmUAAAAASUVORK5CYII=\" ><\/center>","ab3e0999":"- Let's have a look at the most common words in our data, and see if we want to remove any of them as custom stopwords\n    - Works great when we only have one company","6f5c99fc":"# Reviews over time\n- Here we have a function to create a plotly time series plot, with the rolling means of the reviews\n    - With the ability to add the count of reviews, variance and kurtosis\n    - Let's jsut look at mean and variance","52a38984":"- We see there is a heavy bias to either 1 or 5 stars, this makes sense, and can help us to conclude that we may want to drop the 3 star reviews, and bucket 2 + 1 stars and 4 + 5 stars for modelling or deeper analysis\n\n\n# Stratify reviews?\n- If we have a heavily skewed dataset, we may want to take a random sample of X from each of the buckets\n- <b>Looking at the above, we are okay for now<\/b>","5e5111ee":"- Looking at the reviews, we see the data needs to be cleaned\n- This will consist of removing duplicate, stopwords, punctuation, tokenizing and lemmatising\n- Then we will create general features, such as date features and number of words in a review etc...\n\n<b> So we will define some helper functions below <\/b>","d6034b5f":"\n\n\n### Violin plots, more like dumbell plots?\n- Look alot more like dumbell plots, as expected...","a4de40ca":"# Remove largest X% of reviews\n- Note this will skew the results, as negative ones tend to be longer, but otherwise our top few reviews may be 1000's of words, and be outliers","553361c7":"- Or a funnel chart, because why not...","1cf96b03":"- This shows a higher frequency of reviews mention ocados variety\n    - So probably a reason, but not conclusive, as we have very limit data on what customers think about other supermarkets variety of products\n    - But this implies that, they don;t really care for other supermarkets","6abd23c2":"# Length of reviews by stars\n- The different distributions of 1 stars and 5 are interesting","43568e1f":"# Trigrams","12253e78":"# Now we can run through each row, and find out what the model is doing\n- These scores are the contribution of each word to the desired result\n    - That is different from above\n- This can be combined with the trigrams, to deep-dive on individual reviews","eec353e4":"- These all look okay, apart from the store names\n    - So our hypothesis is that if a review mentions a store name in the review, this dosen't give us any extra insight\n    - This is potentially incorrect, but let's go with it for our visualisations\n    - We can always come back during the modelling phase\n\n\n- So let's remove these from the content and title features of our data","99083dd6":"## Todo\n- Redo all and turn from showcase into story\n\n- EDA for all companies\n    - Radar plots by company\n    - Prediction by company, how does each one score, should we train one model per company, if not - interesting transfer learning\n\n- Fix spelling mistakes in the reviews\n\n- Rerun but remove limited stopwords\n\n- More advanced classification models - LGBMClassifier\n\n- Train sentiment model\n\n- https:\/\/www.kaggle.com\/maroberti\/fastai-with-transformers-bert-roberta\n\n- Make another notebook with just one company, do a deep dive on the data story, from reviews to potential fix","eb6aea93":"- From the above, the most important aspect for companies is customer service\n    - We also see bad delivery as the second most important issue for consumers\n        - This makes sense\n\n# Bigrams","873730e1":"# most common words\n### let's build some ngrams for positive and negative reviews\n- Content is the body of the review\n- Title is the title of the review","2ceb8c4c":"# Understanding the data and EDA\n- This is a showcase of various NLP techniques, upvote to stay tuned for future notbooks where we will tell a story and run a deepdive\n\n\n<center><img src = \"https:\/\/dwkujuq9vpuly.cloudfront.net\/news\/wp-content\/uploads\/2017\/08\/Supermarket.jpg\" ><\/center>\n\n## Contents\n1. Import libraries and data\n2. Cleaning and brief feature engineering\n3. In-depth EDA\n4. Modelling\n\n\n- Note the first portion of this notebook was designed looking at just one company, the second part is tailored to many\n\n\n- Other amazing resources:\n    - https:\/\/www.kaggle.com\/datafan07\/disaster-tweets-nlp-eda-bert-with-transformers\n    - https:\/\/www.kaggle.com\/nicapotato\/womens-ecommerce-clothing-reviews\/kernels\n    - https:\/\/www.kaggle.com\/nicapotato\/guided-numeric-and-text-exploration-e-commerce\n    - https:\/\/www.kaggle.com\/granjithkumar\/nlp-with-women-clothing-reviews\n    - https:\/\/www.kaggle.com\/python10pm\/plotting-with-python-learn-80-plots-step-by-step\n    - https:\/\/towardsdatascience.com\/nlp-part-3-exploratory-data-analysis-of-text-data-1caa8ab3f79d?gi=97526ce5efb9\n\n\n- I would greatly appriciate any feedback, in the form of a comment \/ upvote :)","8b34bc76":"# Remove old reviews\n- Do we just want to investigate say the last year?","ccd21d71":"- And as expected, trigrams tell us the same story of customer service, but with some nice adjectives to go along with this\n- As customer service appears in both the positive and negative reviews, we need to run a quick deep dive\n    - So the next step would be selecting a sample of positive and negative reviews, that talk about customer service\n        - Can we then uncover what's really going on?\n\n\n\n<b> These become more usful when just looking at one company, as the above charts just give what consumers think overall, about UK supermarkets, so they don't highlight a specific stores problems <\/b>","4413a9a5":"- This gives us a clear breakdown, we see that ocardo has by far and away the most 5 star reviews, and dosen't have huge amount more reviews than others\n- Asda is by far the worst performing, with the highest ratio of 1 star reviews\n- Morrisons is balanced\n- The average is skewed to the one star reviews\n    - <b>So when people tak about online delivery, it is normally negative, unless they are talking about Ocado...<\/b>\n\n\n\n# But why does Ocado get such positive reviews about it's online platform?\n- Potentially it's variety of products?\n    - Let's repeat the same plot,","65977c7e":"# Determining Topics\n### These are potentially the most powerful outputs, if correctly tuned\n#### We can do this for title or content as well\n- We'll be using a method called Non-Negative Matrix Factorization (NMF) to see if we can get some defined topics out of our TF-IDF matrix, with this way TF-IDF will decrease impact of the high frequency words, so we might get more specific topics","2d81f78d":"- A very impressive 94% accuracy on the validation set, using just a logistic regression model, and only feeding the content of the review to the model (no temporal or structural features)\n    - This can be increased by incorporating more features, but given the objective of this modelling, is to identify the words that have a great impact on the score that a review gives, to therefore identify problems with a company, we don't want to add anything more than the review\n    - That being said, information such as length of the review, or average number of characters, may help our objective, but this will add more complexity that benefit\n\n\n\n# Word importance using ELI5\n- Stands for \"Explain Like I'm 5\"\n- Used to debug machine learning classifiers and explain their predictions\n- From this we see the most positive features are thank, great and excellent\n    - And the bottom are poor, rude and told\n        - Makes sense","ea4bd7e1":"### Data review\n<b>Let's have a look at our columns<\/b>\n- <b>company<\/b>: Comapny name\n- <b>title<\/b>: Clean title of the review\n- <b>content<\/b>: Clean content of the review\n- <b>date<\/b>: Date of the review\n- <b>rating<\/b>: Rating given for the review\n- <b>year<\/b>: Year the review was made\n- <b>month<\/b>: Month the review was made\n- <b>week<\/b>: Week the review was made\n- <b>day<\/b>: Day the review was made\n- <b>wday<\/b>: Weekday the review was made\n- <b>num_stars<\/b>: Number of stars given (1-5)\n- <b>title_num_words<\/b>: Number of words in the title of the review\n- <b>content_num_words<\/b>: Number of words in the content of the review\n- <b>title_num_char<\/b>: Number of characters in the title of the review\n- <b>content_num_char<\/b>: Number of characters in the content of the review","97119d0c":"- Use bigrams gives us a bit more flavour for what is going on\n    - Most of the issues are shaddowed by customer service\n    - This may lead us to remove these words, as they might be overpowering all of our results"}}