{"cell_type":{"272bcc7c":"code","a461d837":"code","a6311e48":"code","e23eaa8c":"code","5b6803d5":"code","4337ded1":"code","02df7540":"code","f544429a":"code","b4817176":"code","56fbb2ab":"code","6c3236c9":"code","6a0b182d":"code","2d19e7a7":"code","0c184f5a":"code","ab7a67ee":"code","eee9bd7d":"markdown","ac718e84":"markdown","58ddba86":"markdown","cb9ae417":"markdown","50cbcf12":"markdown","f2868725":"markdown","7d9c0c15":"markdown","9c31ab83":"markdown","4b0dbbe0":"markdown","4ac5d21b":"markdown","f2e34b29":"markdown","2063e627":"markdown","b28e1d1a":"markdown","d47b631b":"markdown"},"source":{"272bcc7c":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\n\nimport nltk\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\n\nimport matplotlib.pyplot as plt\nfrom scikitplot.metrics import plot_confusion_matrix as plt_con_mat\nimport seaborn as sns\n\nfrom sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a461d837":"data = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\", encoding='latin-1')\ndata.head()","a6311e48":"counts = pd.value_counts(data[\"v1\"])\ncounts.plot(kind= 'bar', color= ['g', 'r'])\nplt.title('Ham\/Spam')\nplt.show()","e23eaa8c":"clean_data = []\n\nY = np.array(data[\"v1\"].map({'spam':1,'ham':0}))\nY = np.expand_dims(Y, axis = -1)\n\nfor doc in data[\"v2\"]:\n    \n    killpunctuation = str.maketrans('', '', string.punctuation)\n    lemmatizer = WordNetLemmatizer()\n    \n    text = doc.lower()\n    text = text.translate(killpunctuation)\n    text = text.split(' ')\n    text = [lemmatizer.lemmatize(word) for word in text]\n    text = (\" \".join(text))\n    clean_data.append(text)\n\ntfidf_vectorizer = TfidfVectorizer(stop_words='english')\ntfidf_vectors = tfidf_vectorizer.fit_transform(clean_data)\nX = tfidf_vectors.toarray()\n\nprint(X.shape, Y.shape)","5b6803d5":"X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size = 0.2, random_state = 1)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","4337ded1":"alphas = np.arange(1\/100000, 1, 0.001)\ntrain_acc = []\ntest_acc = []\nrecall = []\nprecision = []\n\nfor alpha in alphas:\n    \n    bayes = naive_bayes.MultinomialNB(alpha = alpha)\n    bayes.fit(X_train, y_train)\n    \n    train_acc.append(bayes.score(X_train, y_train))\n    test_acc.append(bayes.score(X_test, y_test))\n    recall.append(metrics.recall_score(y_test, bayes.predict(X_test)))\n    precision.append(metrics.precision_score(y_test, bayes.predict(X_test)))\n    \nprint(\"Training complete\")","02df7540":"sns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(alphas, train_acc, label = 'train')\nsns.lineplot(alphas, test_acc, label = 'test')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(alphas, precision, label = 'precision')\nsns.lineplot(alphas, recall, label = 'recall')\nplt.title('Precision and Recall')\nplt.tight_layout()\n\nplt.show()","f544429a":"temp = np.matrix(np.c_[alphas, train_acc, test_acc, recall, precision])\nnb_model_metrics = pd.DataFrame(data = temp, \n                      columns = ['alphas', 'Train Accuracy', 'Test Accuracy', 'Test Recall', 'Test Precision'])\nnb_model_metrics.head()","b4817176":"idx = nb_model_metrics[nb_model_metrics['Test Precision'] == nb_model_metrics['Test Precision'].max()]['Test Accuracy'].idxmax()\nnb_model = naive_bayes.MultinomialNB(alpha = alphas[idx])\nnb_model.fit(X_train, y_train)\nprint(nb_model_metrics.iloc[idx, :])","56fbb2ab":"preds = nb_model.predict(X_test)\n\nplt_con_mat(y_test, preds, figsize=(10,10))\nplt.show()","6c3236c9":"C_vals = np.arange(500, 2000, 100)\ntrain_acc = []\ntest_acc = []\nrecall = []\nprecision = []\n\nfor C in C_vals:\n    \n    svc = svm.LinearSVC(C = C)\n    svc.fit(X_train, y_train)\n    \n    train_acc.append(svc.score(X_train, y_train))\n    test_acc.append(svc.score(X_test, y_test))\n    recall.append(metrics.recall_score(y_test, svc.predict(X_test)))\n    precision.append(metrics.precision_score(y_test, svc.predict(X_test)))\n\nprint(\"Training complete\")","6a0b182d":"sns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(C_vals, train_acc, label = 'train')\nsns.lineplot(C_vals, test_acc, label = 'test')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(C_vals, precision, label = 'precision')\nsns.lineplot(C_vals, recall, label = 'recall')\nplt.title('Precision and Recall')\nplt.tight_layout()\n\nplt.show()","2d19e7a7":"temp = np.matrix(np.c_[C_vals, train_acc, test_acc, recall, precision])\nsvm_model_metrics = pd.DataFrame(data = temp, \n                                 columns = ['C values', 'Train Accuracy', 'Test Accuracy', 'Test Recall', 'Test Precision'])\nsvm_model_metrics.head()","0c184f5a":"idx = svm_model_metrics[svm_model_metrics['Test Precision'] == svm_model_metrics['Test Precision'].max()]['Test Accuracy'].idxmax()\nsvm_model = svm.LinearSVC(C = C_vals[idx])\nsvm_model.fit(X_train, y_train)\nprint(svm_model_metrics.iloc[idx, :])","ab7a67ee":"preds = svm_model.predict(X_test)\n\nplt_con_mat(y_test, preds, figsize=(10,10))\nplt.show()","eee9bd7d":"## Text preprocessing and feature engineering","ac718e84":"# Multinomial naive bayes classifier","58ddba86":"## Confusion Matrix","cb9ae417":"## Necessary libraries","50cbcf12":"## Chossing the best model according to precision and accuracy","f2868725":"## Loading the Data","7d9c0c15":"## Model training performance","9c31ab83":"#  Spam Mail classification\n\nIn this notebook I have implemented classifiers (Naive Bayes and Support Vector Machines) to detect spam mails.","4b0dbbe0":"## Model training performance","4ac5d21b":"## Confusion matrix","f2e34b29":"## Splitting the Data into train and test sets","2063e627":"# Support Vector Machine (SVM)","b28e1d1a":"## Choosing the best model","d47b631b":"## Class distribution"}}