{"cell_type":{"054ef32d":"code","52135a7c":"code","11e8c9a6":"code","2ef4b8c9":"code","c3b45d1c":"code","b366cdf0":"code","f777154f":"code","ab97db49":"code","3638e74d":"code","1b11bd37":"code","2ca561a7":"code","f8a27520":"code","30b0c36f":"code","530a2010":"code","769aa118":"code","c74e9174":"code","25c98296":"code","0e0249b4":"code","6bd7160f":"code","b0a696e6":"code","045d7083":"code","e8513f5f":"code","580a2ea7":"code","4f4dacc5":"markdown"},"source":{"054ef32d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","52135a7c":"#importing Natural Language Toolkit & Pandas\nimport nltk\nimport pandas as pd","11e8c9a6":"#removing trailing characters just for analysis\nmessages=[line.rstrip() for line in open('..\/input\/smsspamcollection\/SMSSpamCollection')]\nmessages[:5]","2ef4b8c9":"#messages and labels are Tab separated so use sep='\\t'\nmessages=pd.read_csv('..\/input\/smsspamcollection\/SMSSpamCollection',sep='\\t',names=['label','message'])\nmessages.head()","c3b45d1c":"#making length a feature\nmessages['length']=messages['message'].apply(len)\nmessages.head()","b366cdf0":"#Information by label\nmessages.groupby('label').describe()","f777154f":"#using the above results we can assure spam mesages are much lengthier than ham","ab97db49":"#importing visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3638e74d":"#BarPlot\nsns.barplot(x='label',y='length',data=messages)","1b11bd37":"#getting more understanding on length feature\ng = sns.FacetGrid(messages, hue=\"label\",height=8,aspect=3)\ng = g.map(sns.distplot, \"length\",  hist=True, rug=True)","2ca561a7":"#This means most of the spam messages have a length greater than 100 or likely in 150 while\n#ham messages have a shorter length and most likely below 100","f8a27520":"import string\nstring.punctuation\n#we will use them to check and remove if a message has any punctuation","30b0c36f":"#common words\nfrom nltk.corpus import stopwords","530a2010":"#Remove the below command to get stopwords\n#stopwords.words('english')\n#these words are so common like 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",etc\n#that they will not be useful for determing ham and spam so we will remove them","769aa118":"#lets remove punctuations and stopwords from messages\n#for that we make a function called text-process and pass that function in the apply() \ndef text_process(mess):\n    no_punc=[s for s in mess if s not in string.punctuation]\n    no_punc=''.join(no_punc)\n    cleaned_mess=[word for word in no_punc.split() if word.lower() not in stopwords.words('english')]\n    return cleaned_mess","c74e9174":"#we train_test_split are data\nX=messages['message']\ny=messages['label']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","25c98296":"#CountVectorizer to tokenize words(i.e String to token)\nfrom sklearn.feature_extraction.text import CountVectorizer\n#weight the integer count to Tfidf Scores\nfrom sklearn.feature_extraction.text import TfidfTransformer\n#we will use Naive_Bayes to train our model\nfrom sklearn.naive_bayes import MultinomialNB","0e0249b4":"#use pipeline to get most of our work done\nfrom sklearn.pipeline import Pipeline","6bd7160f":"#pass the required steps to pipeline\npipeline=Pipeline([\n    ('bow',CountVectorizer(analyzer=text_process)),\n    ('tfidf',TfidfTransformer()),\n    ('naive_bayes',MultinomialNB())\n])","b0a696e6":"#now we fit the training data\npipeline.fit(X_train,y_train)\n","045d7083":"#now we make predictions\npredictions=pipeline.predict(X_test)","e8513f5f":"#we will use classification report to see how well are model performs\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))","580a2ea7":"#Hurray! it seems our model performs pretty good.","4f4dacc5":"We will use bow(Bag Of Words) the CountVectorizer to tokenize words(i.e String to token), TfidfTransformer to weight the integer count to Tfidf scores and Naive Bayes for training our model"}}