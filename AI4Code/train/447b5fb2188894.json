{"cell_type":{"9e15a1e3":"code","bcdf6336":"code","9e639079":"code","82189048":"code","d38f4c0d":"code","3854bda2":"code","e0d37e60":"code","b029cfb8":"code","5e05eab7":"code","f5f13192":"code","df5c1e4e":"code","7e276495":"code","1cf278b8":"code","d41a9457":"code","2f81db40":"code","36393af1":"code","1b2c1501":"code","9ded888b":"code","eb345d8d":"code","eedbfde4":"code","4da5ae74":"code","6bce071b":"code","f6aca803":"code","464648ba":"code","6fd06de7":"code","139e6deb":"markdown","8756b2d4":"markdown","4a00d0ed":"markdown","accea664":"markdown","fc0d3edc":"markdown","2f0c2797":"markdown","d89555b7":"markdown","d974a178":"markdown","9714356d":"markdown","6299f9b9":"markdown","c8ce55a0":"markdown","3f52b551":"markdown","1266925e":"markdown","64e28109":"markdown","3f3e3c07":"markdown","b483351a":"markdown","3b7e953d":"markdown","31acf1d5":"markdown","73bf76c8":"markdown","10140c60":"markdown","e0d910e9":"markdown"},"source":{"9e15a1e3":"# Bibliotecas necess\u00e1rias\n# Manipula\u00e7\u00e3o de dados\nimport pandas as pd\n# Redes Neurais\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import RMSprop\n# Plot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Avalia\u00e7\u00e3o\nfrom sklearn.metrics import classification_report, confusion_matrix","bcdf6336":"# Lendo o dataset Kaggle\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n\n#Alternativa ler do pr\u00f3prio keras\n#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","9e639079":"# Analisando o dataset\nprint(\"Quantidade de elementos de treino: {}\". format(len(train)))\nprint(train.head())","82189048":"# Separando x_train e y_train\nY = train[\"label\"]\nX = train.drop(labels = [\"label\"],axis = 1)\nprint(X.head())\n# Em formato numpy array de imagens 28 x 28\n#x = X.values.reshape(-1,28,28,1)\n#print(x[0])","d38f4c0d":"print(Y)","3854bda2":"# Numtendi nada!\n# Bora ver com matplotlib\nplt.imshow(X.values[100].reshape(28,28), cmap=plt.cm.binary)\nplt.show()\nprint('Label: {}'.format(Y[100]))","e0d37e60":"# Transformando a imagem 2d em um numpy array (imagem 28*28 = 784 pixels)\nx = X.values.reshape(42000, 784)\n\n#Normalizando para valores entre 0 e 1\nx = x.astype('float32')\nx \/= 255\n\nprint(x[0])","b029cfb8":"# Vamos ajustar o formato da saida\nnum_classes = 10\n\n# Convertendo para um vetor de saida com 10 dimensoes\n# ex. 8 => [0,0,0,0,0,0,0,0,1,0]\ny = keras.utils.to_categorical(Y, num_classes)\nprint(y[0])","5e05eab7":"# Separando uma parte para treino (90%) e outra para valida\u00e7\u00e3o (10%)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.1, random_state=9)\nprint('Qtde de treino: {}'.format(len(x_train)))\nprint('Qtde de valida\u00e7\u00e3o: {}'.format(len(x_val)))","f5f13192":"# Criando o modelo Sequential\n# Sequential: Modelo Keras de ir adicionando camadas (como um lego)\n# Dense: Camada onde todas as entradas est\u00e3o conectadas em cada neur\u00f4nio (totalmente conectada)\n# Dropout: Camada usa durante treino que descarta aleatoriamente um percentual de conex\u00f5es (reduz overfitting)\n\nmodel = Sequential()\n# Camada com 30 neur\u00f4nios\nmodel.add(Dense(30, activation='relu', input_shape=(784,)))\n# Dropout de 20%\nmodel.add(Dropout(0.2))\n# Camada de 20 neur\u00f4nios\nmodel.add(Dense(20, activation='relu'))\n# Dropout de 20%\nmodel.add(Dropout(0.2))\n# Camada de classifica\u00e7\u00e3o final, com 1 neur\u00f4nio para cada classe de sa\u00edda. Softmax divide a probabilidade de cada classe.\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.summary()","df5c1e4e":"model_2 = Sequential()\n# Camada com 40 neur\u00f4nios\nmodel_2.add(Dense(40, activation='relu', input_shape=(784,)))\n# Dropout de 10%\nmodel_2.add(Dropout(0.1))\n# Camada de 30 neur\u00f4nios\nmodel_2.add(Dense(30, activation='relu'))\n# Dropout de 10%\nmodel_2.add(Dropout(0.1))\n# Camada de classifica\u00e7\u00e3o final, com 1 neur\u00f4nio para cada classe de sa\u00edda. Softmax divide a probabilidade de cada classe.\nmodel_2.add(Dense(num_classes, activation='softmax'))\n\nmodel_2.summary()","7e276495":"model_3 = Sequential()\n# Camada com 50 neur\u00f4nios\nmodel_3.add(Dense(50, activation='relu', input_shape=(784,)))\n# Dropout de 20%\nmodel_3.add(Dropout(0.2))\n# Camada de 40 neur\u00f4nios\nmodel_3.add(Dense(40, activation='relu'))\n# Dropout de 20%\nmodel_3.add(Dropout(0.2))\n# Camada com 30 neur\u00f4nios\nmodel_3.add(Dense(30, activation='relu'))\n# Dropout de 20%\nmodel_3.add(Dropout(0.2))\n# Camada de 20 neur\u00f4nios\nmodel_3.add(Dense(20, activation='relu'))\n# Dropout de 20%\nmodel_3.add(Dropout(0.2))\n# Camada de classifica\u00e7\u00e3o final, com 1 neur\u00f4nio para cada classe de sa\u00edda. Softmax divide a probabilidade de cada classe.\nmodel_3.add(Dense(num_classes, activation='softmax'))\n\nmodel_3.summary()","1cf278b8":"model_4 = Sequential()\n# Camada com 40 neur\u00f4nios\nmodel_4.add(Dense(40, activation='relu', input_shape=(784,)))\n# Dropout de 15%\nmodel_4.add(Dropout(0.15))\n# Camada com 30 neur\u00f4nios\nmodel_4.add(Dense(30, activation='relu'))\n# Dropout de 15%\nmodel_4.add(Dropout(0.15))\n# Camada de 20 neur\u00f4nios\nmodel_4.add(Dense(20, activation='relu'))\n# Dropout de 15%\nmodel_4.add(Dropout(0.15))\n# Camda de classifica\u00e7\u00e3o final, com 1 neur\u00f4nio para cada classe de sa\u00edda. Softmax divide a probabilidade de cada classe.\nmodel_4.add(Dense(num_classes, activation='softmax'))\n\nmodel_4.summary()","d41a9457":"# Compilar o modelo\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n\nmodel_2.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n\nmodel_3.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n\nmodel_4.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])","2f81db40":"batch_size = 32\nepochs = 30\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_val, y_val))","36393af1":"batch_size = 32\nepochs = 30\nhistory_2 = model_2.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_val, y_val))","1b2c1501":"batch_size = 32\nepochs = 15\nhistory_3 = model_3.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_val, y_val))","9ded888b":"batch_size = 32\nepochs = 20\nhistory_4 = model_4.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_val, y_val))","eb345d8d":"fig, ax = plt.subplots(4,2, figsize=(16,8))\n\n#Modelo 1\nax[0][0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0][0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0][0])\nlegend = ax[0][0].legend(loc='best', shadow=True)\n\nax[0][1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[0][1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[0][1].legend(loc='best', shadow=True)\n\n#Modelo 2\nax[1][0].plot(history_2.history['loss'], color='b', label=\"Training loss\")\nax[1][0].plot(history_2.history['val_loss'], color='r', label=\"validation loss\",axes =ax[1][0])\nlegend = ax[1][0].legend(loc='best', shadow=True)\n\nax[1][1].plot(history_2.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1][1].plot(history_2.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1][1].legend(loc='best', shadow=True)\n\n\n#Modelo 3\nax[2][0].plot(history_3.history['loss'], color='b', label=\"Training loss\")\nax[2][0].plot(history_3.history['val_loss'], color='r', label=\"validation loss\",axes =ax[2][0])\nlegend = ax[2][0].legend(loc='best', shadow=True)\n\nax[2][1].plot(history_3.history['accuracy'], color='b', label=\"Training accuracy\")\nax[2][1].plot(history_3.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[2][1].legend(loc='best', shadow=True)\n\n#Modelo 4\nax[3][0].plot(history_4.history['loss'], color='b', label=\"Training loss\")\nax[3][0].plot(history_4.history['val_loss'], color='r', label=\"validation loss\",axes =ax[3][0])\nlegend = ax[3][0].legend(loc='best', shadow=True)\n\nax[3][1].plot(history_4.history['accuracy'], color='b', label=\"Training accuracy\")\nax[3][1].plot(history_4.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[3][1].legend(loc='best', shadow=True)","eedbfde4":"#Modelo 1\nscore = model.evaluate(x_val, y_val, verbose=0)\nprint('Modelo 1')\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\n#Modelo 2\nscore = model_2.evaluate(x_val, y_val, verbose=0)\nprint('Modelo 2')\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\n#Modelo 3\nscore = model_3.evaluate(x_val, y_val, verbose=0)\nprint('Modelo 3')\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\n#Modelo 4\nscore = model_4.evaluate(x_val, y_val, verbose=0)\nprint('Modelo 4')\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n","4da5ae74":"import itertools\n#Plot the confusion matrix. Set Normalize = True\/False\ndef plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        cm = np.around(cm, decimals=2)\n        cm[np.isnan(cm)] = 0.0\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","6bce071b":"import numpy as np\n\ny_pred = model.predict_classes(x_val)\ny_test_c = np.argmax(y_val, axis=1)\ntarget_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n#Confution Matrix\ncm = confusion_matrix(y_test_c, y_pred)\nplot_confusion_matrix(cm, target_names, normalize=False, title='Confusion Matrix')\n\nprint('Classification Report')\nprint(classification_report(y_test_c, y_pred, target_names=target_names))","f6aca803":"y_pred = model_2.predict_classes(x_val)\ny_test_c = np.argmax(y_val, axis=1)\ntarget_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n#Confution Matrix\ncm = confusion_matrix(y_test_c, y_pred)\nplot_confusion_matrix(cm, target_names, normalize=False, title='Confusion Matrix')\n\nprint('Classification Report')\nprint(classification_report(y_test_c, y_pred, target_names=target_names))","464648ba":"y_pred = model_3.predict_classes(x_val)\ny_test_c = np.argmax(y_val, axis=1)\ntarget_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n#Confution Matrix\ncm = confusion_matrix(y_test_c, y_pred)\nplot_confusion_matrix(cm, target_names, normalize=False, title='Confusion Matrix')\n\nprint('Classification Report')\nprint(classification_report(y_test_c, y_pred, target_names=target_names))","6fd06de7":"y_pred = model_4.predict_classes(x_val)\ny_test_c = np.argmax(y_val, axis=1)\ntarget_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n#Confution Matrix\ncm = confusion_matrix(y_test_c, y_pred)\nplot_confusion_matrix(cm, target_names, normalize=False, title='Confusion Matrix')\n\nprint('Classification Report')\nprint(classification_report(y_test_c, y_pred, target_names=target_names))","139e6deb":"# Considera\u00e7\u00f5es Finais","8756b2d4":"**Modelo 2**","4a00d0ed":"# Cria\u00e7\u00e3o dos Modelos","accea664":"# Bibliotecas e Dados","fc0d3edc":"**Modelo 3**","2f0c2797":"# Treinamento dos modelos","d89555b7":"**Modelo 1**","d974a178":"**Modelo 4**","9714356d":"**Modelo 1**","6299f9b9":"**Modelo 2**","c8ce55a0":"# Avalia\u00e7\u00e3o dos modelos","3f52b551":"**Modelo 4**","1266925e":"**Modelo 3**","64e28109":"# Introdu\u00e7\u00e3o","3f3e3c07":"**Modelo 1**","b483351a":"**Avalia\u00e7\u00e3o do treinamento dos modelos**","3b7e953d":"**Modelo 3**","31acf1d5":"**Modelo 4**","73bf76c8":"**Modelo 2**","10140c60":"O modelo 2 obteve o melhor desempenho geral dentro os demais modelos, atingindo um f1-score de 0.97. Este modelo com apenas 2 camadas de 40 e 30 neur\u00f4nios e um dropout de 10%. O modelo 4 teve f1-score de 0.96 com 3 camadas de 40, 30 e 20, dropout de 15% e 20 \u00e9pocas teve o segundo melhor desempenho, em seguida, o modelo 3 com a maior quantidade de camadas e o modelo 1 sendo o menor em n\u00fameros de camadas e neur\u00f4nios.","e0d910e9":"Kid Mendes de Oliveira Neto - kmdon.cid20@uea.edu.br\n\n**Resumo:**\n\nO exemplo aqui desenvolvido tem como objetivo apresentar conceitos iniciais de implementa\u00e7\u00e3o de redes neurais com python e tensorflow\/keras. Esse modelo apresenta um modelo MLP b\u00e1sico que pode ser expandido mudando o n\u00famero de neur\u00f4nios e camadas. Em adapta\u00e7\u00f5es mais avan\u00e7adas, pode-se estudar possibilidade de otimiza\u00e7\u00e3o de hyperpar\u00e2metros e outras t\u00e9cnincas como aumento de dados.\n\n---\n\n**Para saber mais:**\n* [Palestras e cursos do Ocean](http:\/\/www.oceanbrasil.com\/)\n* Fran\u00e7ois Chollet. Deep Learning with Python. Manning Publications, 2017.\n* Ian Goodfellow and Yoshua Bengio and Aaron Courville. [Deep Learning](https:\/\/www.deeplearningbook.org\/). MIT Press, 2016."}}