{"cell_type":{"4fc63a8a":"code","ac5d1219":"code","4374e9f1":"code","4b923edf":"code","b38c577b":"code","acad873c":"code","b7a68baf":"code","a1ed32f5":"code","79c4ea22":"code","2fccdef7":"code","137c71dd":"code","96cb4e07":"code","cd642ab1":"code","045dba09":"code","808ab2be":"code","ea7fe345":"code","f94ec582":"code","d02dff98":"code","99eec33d":"code","db87edd9":"code","08107cc9":"code","ec8b0ad2":"code","6e5b5bfe":"code","8ceb70d4":"code","eee38a9d":"code","3b340b96":"code","a9c2fe40":"code","c22fe843":"code","e9c7d1ef":"code","bca5fc4a":"code","037f5d9c":"code","096a7f1c":"code","3caffe45":"code","2d0b2984":"code","867756fa":"code","7a3d7dca":"code","26639d42":"code","9d033b87":"code","418937c8":"code","b4d94c83":"code","69377681":"code","7853399d":"code","148167b3":"code","24e0c6fc":"code","fef68259":"code","87a8d600":"markdown","6e6bbdef":"markdown","5f4b887a":"markdown","2afa2ce8":"markdown","47bb19c6":"markdown","01eef3c2":"markdown","f4e95b40":"markdown","8f798ff6":"markdown","7e7ef588":"markdown","8e10fa23":"markdown","9699c0e5":"markdown","a40edd3a":"markdown","c3c1152d":"markdown","8334578d":"markdown","12cda7d2":"markdown","1f5e864e":"markdown","49d580a4":"markdown","3fa99566":"markdown","f74a8493":"markdown","6a8e8564":"markdown","e2d3db61":"markdown","efe6f75e":"markdown","6f04b663":"markdown","3868e559":"markdown","29d4c658":"markdown","caf1be4a":"markdown","e81b4d3e":"markdown","de81143c":"markdown"},"source":{"4fc63a8a":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.cross_validation import train_test_split\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline","ac5d1219":"df = pd.read_csv('..\/input\/googleplaystore.csv')","4374e9f1":"df.info()","4b923edf":"df.dropna(inplace = True)","b38c577b":"df.info()","acad873c":"df.head()","b7a68baf":"# Cleaning Categories into integers\nCategoryString = df[\"Category\"]\ncategoryVal = df[\"Category\"].unique()\ncategoryValCount = len(categoryVal)\ncategory_dict = {}\nfor i in range(0,categoryValCount):\n    category_dict[categoryVal[i]] = i\ndf[\"Category_c\"] = df[\"Category\"].map(category_dict).astype(int)","a1ed32f5":"#scaling and cleaning size of installation\ndef change_size(size):\n    if 'M' in size:\n        x = size[:-1]\n        x = float(x)*1000000\n        return(x)\n    elif 'k' == size[-1:]:\n        x = size[:-1]\n        x = float(x)*1000\n        return(x)\n    else:\n        return None\n\ndf[\"Size\"] = df[\"Size\"].map(change_size)\n\n#filling Size which had NA\ndf.Size.fillna(method = 'ffill', inplace = True)\n","79c4ea22":"#Cleaning no of installs classification\ndf['Installs'] = [int(i[:-1].replace(',','')) for i in df['Installs']]","2fccdef7":"#Converting Type classification into binary\ndef type_cat(types):\n    if types == 'Free':\n        return 0\n    else:\n        return 1\n\ndf['Type'] = df['Type'].map(type_cat)","137c71dd":"#Cleaning of content rating classification\nRatingL = df['Content Rating'].unique()\nRatingDict = {}\nfor i in range(len(RatingL)):\n    RatingDict[RatingL[i]] = i\ndf['Content Rating'] = df['Content Rating'].map(RatingDict).astype(int)","96cb4e07":"#dropping of unrelated and unnecessary items\ndf.drop(labels = ['Last Updated','Current Ver','Android Ver','App'], axis = 1, inplace = True)","cd642ab1":"#Cleaning of genres\nGenresL = df.Genres.unique()\nGenresDict = {}\nfor i in range(len(GenresL)):\n    GenresDict[GenresL[i]] = i\ndf['Genres_c'] = df['Genres'].map(GenresDict).astype(int)","045dba09":"#Cleaning prices\ndef price_clean(price):\n    if price == '0':\n        return 0\n    else:\n        price = price[1:]\n        price = float(price)\n        return price\n\ndf['Price'] = df['Price'].map(price_clean).astype(float)","808ab2be":"# convert reviews to numeric\ndf['Reviews'] = df['Reviews'].astype(int)","ea7fe345":"df.info()","f94ec582":"df.head()","d02dff98":"# for dummy variable encoding for Categories\ndf2 = pd.get_dummies(df, columns=['Category'])","99eec33d":"df2.head()","db87edd9":"# let's use 3 different regression models with two different techniques on treating the categorical variable","08107cc9":"#for evaluation of error term and \ndef Evaluationmatrix(y_true, y_predict):\n    print ('Mean Squared Error: '+ str(metrics.mean_squared_error(y_true,y_predict)))\n    print ('Mean absolute Error: '+ str(metrics.mean_absolute_error(y_true,y_predict)))\n    print ('Mean squared Log Error: '+ str(metrics.mean_squared_log_error(y_true,y_predict)))","ec8b0ad2":"#to add into results_index for evaluation of error term \ndef Evaluationmatrix_dict(y_true, y_predict, name = 'Linear - Integer'):\n    dict_matrix = {}\n    dict_matrix['Series Name'] = name\n    dict_matrix['Mean Squared Error'] = metrics.mean_squared_error(y_true,y_predict)\n    dict_matrix['Mean Absolute Error'] = metrics.mean_absolute_error(y_true,y_predict)\n    dict_matrix['Mean Squared Log Error'] = metrics.mean_squared_log_error(y_true,y_predict)\n    return dict_matrix","6e5b5bfe":"#excluding Genre label\nfrom sklearn.linear_model import LinearRegression \n\n#Integer encoding\nX = df.drop(labels = ['Category','Rating','Genres','Genres_c'],axis = 1)\ny = df.Rating\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\nResults = model.predict(X_test)\n\n#Creation of results dataframe and addition of first entry\nresultsdf = pd.DataFrame()\nresultsdf = resultsdf.from_dict(Evaluationmatrix_dict(y_test,Results),orient = 'index')\nresultsdf = resultsdf.transpose()\n\n#dummy encoding\n\nX_d = df2.drop(labels = ['Rating','Genres','Category_c','Genres_c'],axis = 1)\ny_d = df2.Rating\nX_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d, y_d, test_size=0.30)\nmodel_d = LinearRegression()\nmodel_d.fit(X_train_d,y_train_d)\nResults_d = model_d.predict(X_test_d)\n\n#adding results into results dataframe\nresultsdf = resultsdf.append(Evaluationmatrix_dict(y_test_d,Results_d, name = 'Linear - Dummy'),ignore_index = True)","8ceb70d4":"plt.figure(figsize=(12,7))\nsns.regplot(Results,y_test,color='teal', label = 'Integer', marker = 'x')\nsns.regplot(Results_d,y_test_d,color='orange',label = 'Dummy')\nplt.legend()\nplt.title('Linear model - Excluding Genres')\nplt.xlabel('Predicted Ratings')\nplt.ylabel('Actual Ratings')\nplt.show()","eee38a9d":"print ('Actual mean of population:' + str(y.mean()))\nprint ('Integer encoding(mean) :' + str(Results.mean()))\nprint ('Dummy encoding(mean) :'+ str(Results_d.mean()))\nprint ('Integer encoding(std) :' + str(Results.std()))\nprint ('Dummy encoding(std) :'+ str(Results_d.std()))","3b340b96":"#Including genre label\n\n#Integer encoding\nX = df.drop(labels = ['Category','Rating','Genres'],axis = 1)\ny = df.Rating\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\nResults = model.predict(X_test)\n\nresultsdf = resultsdf.append(Evaluationmatrix_dict(y_test,Results, name = 'Linear(inc Genre) - Integer'),ignore_index = True)\n\n#dummy encoding\n\nX_d = df2.drop(labels = ['Rating','Genres','Category_c'],axis = 1)\ny_d = df2.Rating\nX_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d, y_d, test_size=0.30)\nmodel_d = LinearRegression()\nmodel_d.fit(X_train_d,y_train_d)\nResults_d = model_d.predict(X_test_d)\n\nresultsdf = resultsdf.append(Evaluationmatrix_dict(y_test_d,Results_d, name = 'Linear(inc Genre) - Dummy'),ignore_index = True)","a9c2fe40":"plt.figure(figsize=(12,7))\nsns.regplot(Results,y_test,color='teal', label = 'Integer', marker = 'x')\nsns.regplot(Results_d,y_test_d,color='orange',label = 'Dummy')\nplt.legend()\nplt.title('Linear model - Including Genres')\nplt.xlabel('Predicted Ratings')\nplt.ylabel('Actual Ratings')\nplt.show()","c22fe843":"print ('Integer encoding(mean) :' + str(Results.mean()))\nprint ('Dummy encoding(mean) :'+ str(Results_d.mean()))\nprint ('Integer encoding(std) :' + str(Results.std()))\nprint ('Dummy encoding(std) :'+ str(Results_d.std()))","e9c7d1ef":"#Excluding genres\nfrom sklearn import svm\n#Integer encoding\n\nX = df.drop(labels = ['Category','Rating','Genres','Genres_c'],axis = 1)\ny = df.Rating\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n\nmodel2 = svm.SVR()\nmodel2.fit(X_train,y_train)\n\nResults2 = model2.predict(X_test)\n\nresultsdf = resultsdf.append(Evaluationmatrix_dict(y_test,Results2, name = 'SVM - Integer'),ignore_index = True)\n\n#dummy based\n\n\nX_d = df2.drop(labels = ['Rating','Genres','Category_c','Genres_c'],axis = 1)\ny_d = df2.Rating\n\nX_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d, y_d, test_size=0.30)\n\nmodel2 = svm.SVR()\nmodel2.fit(X_train_d,y_train_d)\n\nResults2_d = model2.predict(X_test_d)\n\nresultsdf = resultsdf.append(Evaluationmatrix_dict(y_test_d,Results2_d, name = 'SVM - Dummy'),ignore_index = True)","bca5fc4a":"plt.figure(figsize=(12,7))\nsns.regplot(Results2,y_test,color='teal', label = 'Integer', marker = 'x')\nsns.regplot(Results2_d,y_test_d,color='orange',label = 'Dummy')\nplt.legend()\nplt.title('SVM model - excluding Genres')\nplt.xlabel('Predicted Ratings')\nplt.ylabel('Actual Ratings')\nplt.show()","037f5d9c":"print ('Integer encoding(mean) :' + str(Results2.mean()))\nprint ('Dummy encoding(mean) :'+ str(Results2_d.mean()))\nprint ('Integer encoding(std) :' + str(Results2.std()))\nprint ('Dummy encoding(std) :'+ str(Results2_d.std()))","096a7f1c":"#Integer encoding, including Genres_c\nmodel2a = svm.SVR()\n\nX = df.drop(labels = ['Category','Rating','Genres'],axis = 1)\ny = df.Rating\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n\nmodel2a.fit(X_train,y_train)\n\nResults2a = model2a.predict(X_test)\n\n#evaluation\nresultsdf = resultsdf.append(Evaluationmatrix_dict(y_test,Results2a, name = 'SVM(inc Genres) - Integer'),ignore_index = True)\n\n#dummy encoding, including Genres_c\nmodel2a = svm.SVR()\n\nX_d = df2.drop(labels = ['Rating','Genres','Category_c'],axis = 1)\ny_d = df2.Rating\n\nX_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d, y_d, test_size=0.30)\n\nmodel2a.fit(X_train_d,y_train_d)\n\nResults2a_d = model2a.predict(X_test_d)\n\n#evaluation\nresultsdf = resultsdf.append(Evaluationmatrix_dict(y_test_d,Results2a_d, name = 'SVM(inc Genres) - Dummy'),ignore_index = True)","3caffe45":"plt.figure(figsize=(12,7))\nsns.regplot(Results2a,y_test,color='teal', label = 'Integer', marker = 'x')\nsns.regplot(Results2a_d,y_test_d,color='orange',label = 'Dummy')\nplt.legend()\nplt.title('SVM model - including Genres')\nplt.xlabel('Predicted Ratings')\nplt.ylabel('Actual Ratings')\nplt.show()","2d0b2984":"print ('Integer encoding(mean) :' + str(Results2a.mean()))\nprint ('Dummy encoding(mean) :'+ str(Results2a_d.mean()))\nprint ('Integer encoding(std) :' + str(Results2a.std()))\nprint ('Dummy encoding(std) :'+ str(Results2a_d.std()))","867756fa":"from sklearn.ensemble import RandomForestRegressor\n\n#Integer encoding\nX = df.drop(labels = ['Category','Rating','Genres','Genres_c'],axis = 1)\ny = df.Rating\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\nmodel3 = RandomForestRegressor()\nmodel3.fit(X_train,y_train)\nResults3 = model3.predict(X_test)\n\n#evaluation\nresultsdf = resultsdf.append(Evaluationmatrix_dict(y_test,Results3, name = 'RFR - Integer'),ignore_index = True)\n\n#dummy encoding\n\nX_d = df2.drop(labels = ['Rating','Genres','Category_c','Genres_c'],axis = 1)\ny_d = df2.Rating\nX_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d, y_d, test_size=0.30)\nmodel3_d = RandomForestRegressor()\nmodel3_d.fit(X_train_d,y_train_d)\nResults3_d = model3_d.predict(X_test_d)\n\n#evaluation\nresultsdf = resultsdf.append(Evaluationmatrix_dict(y_test,Results3_d, name = 'RFR - Dummy'),ignore_index = True)","7a3d7dca":"plt.figure(figsize=(12,7))\nsns.regplot(Results3,y_test,color='teal', label = 'Integer', marker = 'x')\nsns.regplot(Results3_d,y_test_d,color='orange',label = 'Dummy')\nplt.legend()\nplt.title('RFR model - excluding Genres')\nplt.xlabel('Predicted Ratings')\nplt.ylabel('Actual Ratings')\nplt.show()","26639d42":"print ('Integer encoding(mean) :' + str(Results3.mean()))\nprint ('Dummy encoding(mean) :'+ str(Results3_d.mean()))\nprint ('Integer encoding(std) :' + str(Results3.std()))\nprint ('Dummy encoding(std) :'+ str(Results3_d.std()))","9d033b87":"#for integer\nFeat_impt = {}\nfor col,feat in zip(X.columns,model3.feature_importances_):\n    Feat_impt[col] = feat\n\nFeat_impt_df = pd.DataFrame.from_dict(Feat_impt,orient = 'index')\nFeat_impt_df.sort_values(by = 0, inplace = True)\nFeat_impt_df.rename(index = str, columns = {0:'Pct'},inplace = True)\n\nplt.figure(figsize= (14,10))\nFeat_impt_df.plot(kind = 'barh',figsize= (14,10),legend = False)\nplt.show()","418937c8":"#for dummy\nFeat_impt_d = {}\nfor col,feat in zip(X_d.columns,model3_d.feature_importances_):\n    Feat_impt_d[col] = feat\n\nFeat_impt_df_d = pd.DataFrame.from_dict(Feat_impt_d,orient = 'index')\nFeat_impt_df_d.sort_values(by = 0, inplace = True)\nFeat_impt_df_d.rename(index = str, columns = {0:'Pct'},inplace = True)\n\nplt.figure(figsize= (14,10))\nFeat_impt_df_d.plot(kind = 'barh',figsize= (14,10),legend = False)\nplt.show()","b4d94c83":"\n#Including Genres_C\n\n#Integer encoding\nX = df.drop(labels = ['Category','Rating','Genres'],axis = 1)\ny = df.Rating\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\nmodel3a = RandomForestRegressor()\nmodel3a.fit(X_train,y_train)\nResults3a = model3a.predict(X_test)\n\n#evaluation\nresultsdf = resultsdf.append(Evaluationmatrix_dict(y_test,Results3a, name = 'RFR(inc Genres) - Integer'),ignore_index = True)\n\n#dummy encoding\n\nX_d = df2.drop(labels = ['Rating','Genres','Category_c'],axis = 1)\ny_d = df2.Rating\nX_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d, y_d, test_size=0.30)\nmodel3a_d = RandomForestRegressor()\nmodel3a_d.fit(X_train_d,y_train_d)\nResults3a_d = model3a_d.predict(X_test_d)\n\n#evaluation\nresultsdf = resultsdf.append(Evaluationmatrix_dict(y_test,Results3a_d, name = 'RFR(inc Genres) - Dummy'),ignore_index = True)","69377681":"plt.figure(figsize=(12,7))\nsns.regplot(Results3a,y_test,color='teal', label = 'Integer', marker = 'x')\nsns.regplot(Results3a_d,y_test_d,color='orange',label = 'Dummy')\nplt.legend()\nplt.title('RFR model - including Genres')\nplt.xlabel('Predicted Ratings')\nplt.ylabel('Actual Ratings')\nplt.show()","7853399d":"print ('Integer encoding(mean) :' + str(Results3.mean()))\nprint ('Dummy encoding(mean) :'+ str(Results3_d.mean()))\nprint ('Integer encoding(std) :' + str(Results3.std()))\nprint ('Dummy encoding(std) :'+ str(Results3_d.std()))","148167b3":"#for integer\nFeat_impt = {}\nfor col,feat in zip(X.columns,model3a.feature_importances_):\n    Feat_impt[col] = feat\n\nFeat_impt_df = pd.DataFrame.from_dict(Feat_impt,orient = 'index')\nFeat_impt_df.sort_values(by = 0, inplace = True)\nFeat_impt_df.rename(index = str, columns = {0:'Pct'},inplace = True)\n\nplt.figure(figsize= (14,10))\nFeat_impt_df.plot(kind = 'barh',figsize= (14,10),legend = False)\nplt.show()","24e0c6fc":"#for dummy\nFeat_impt_d = {}\nfor col,feat in zip(X_d.columns,model3a_d.feature_importances_):\n    Feat_impt_d[col] = feat\n\nFeat_impt_df_d = pd.DataFrame.from_dict(Feat_impt_d,orient = 'index')\nFeat_impt_df_d.sort_values(by = 0, inplace = True)\nFeat_impt_df_d.rename(index = str, columns = {0:'Pct'},inplace = True)\n\nplt.figure(figsize= (14,10))\nFeat_impt_df_d.plot(kind = 'barh',figsize= (14,10),legend = False)\nplt.show()","fef68259":"resultsdf.set_index('Series Name', inplace = True)\n\nplt.figure(figsize = (10,12))\nplt.subplot(3,1,1)\nresultsdf['Mean Squared Error'].sort_values(ascending = False).plot(kind = 'barh',color=(0.3, 0.4, 0.6, 1), title = 'Mean Squared Error')\nplt.subplot(3,1,2)\nresultsdf['Mean Absolute Error'].sort_values(ascending = False).plot(kind = 'barh',color=(0.5, 0.4, 0.6, 1), title = 'Mean Absolute Error')\nplt.subplot(3,1,3)\nresultsdf['Mean Squared Log Error'].sort_values(ascending = False).plot(kind = 'barh',color=(0.7, 0.4, 0.6, 1), title = 'Mean Squared Log Error')\nplt.show()\n","87a8d600":"At first glance, it's hard to really see which model(dummy vs one-hot) is better in terms of predictive accuracy. What is striking however is the that at first glance, the dummy model seems favors the outcome of a lower rating compared to the integer model. \n\nAlthough if we look at the actual mean of the predictive results, both are approximately the same, **however the dummy encoded results have a much larger standard deviation as compared to the integer encoded model.**\n\nNext is looking at the linear model including the genre label as a numeric value.","6e6bbdef":"Cleaning the number of installations column","5f4b887a":"Converting of the content rating section into integers. In this specific instance, given that the concent rating is somewhat relatable and has an order to it, we do not use one-hot encoding.","2afa2ce8":"From the categorical column, I converted each category into an individual number. In the later sections when we do apply machine learning, two methods will be applied to the code, being integer encoding(which we are doing now) and one-hot encoding, aka dummy variables.  \n\nThe main reason as to why I understand we do this transformation is mainly because integer encoding relies on the fact that there's a relationship between each category(e.g. think age range vs types of animals). In this case however, it's hard to really determine such a relationship, hence dummy\/one-hot encoding might help provide better predictive accuracy. ","47bb19c6":"With the inclusion of the genre variable, the dummy encoding model now seems to be performing better, as we see the regression line comparing the actual vs the predicted results to be very similar to that of the integer encoded model.\n\nFurthermore the std of the dummy encoded model has fallen significantly, and now has a higher mean compared to the integer encoded model.\n\n**Next up is the random forest regressor model.** \nHonestly this is my favorite model as not only is it fast, it also allows you to see what independent variables significantly affect the outcome of the model.","01eef3c2":"I dropped these portions of information as i deemed it unecessary for our machine learning algorithm","f4e95b40":"In this instance, I created another dataframe that specifically created **dummy values** for each categorical instance in the dataframe, defined as df2","8f798ff6":"Converting the paid\/free classification types into binary","7e7ef588":"Again with the inclusion of the genre variable, the results do not seem to defer significantly as compared to the previous results.","8e10fa23":"The results are quite interesting. Overall the model predicted quite a bit of ratings to be approximately at 4.2, even though the actual ratings were not. Looking at the scatterplot, the integer encoded model seems to have performed better in this instance.\n\nAs usual, the dummy encoded model has a higher std than the integer encoded model.","9699c0e5":"From the results, it would seem that the genre section actually plays an important part in the decision tree making. Yet the exclusion of it dosent seem to significantly impact results. This to me is quite interesting.","a40edd3a":"Checking out the info, there's a not of null values that need to be addressed. Since my main objective is predicting the ratings of the apps, I deleted all the NaN values, just for simplicity sake.","c3c1152d":"Finally converting the number reviews column into integers","8334578d":"Cleaning of sizes of the apps and also filling up the missing values using ffill","12cda7d2":"So before we start, the following is code to obtain the error terms for the various models, for comparability.","1f5e864e":"Cleaning of the prices of the apps to floats","49d580a4":"For the following steps, in order to process the data in the machine learning algorithms, we need to first convert it from text to numbers, as from what i understand, most algorithms run better that way. From most of the books I've read, data cleaning\/preprocessing is **THE** most important part of any machine learning process, as high quality data translates to high quality predictions and models.","3fa99566":"At first glance, I would say that the RFR model produced the best predictive results, just looking at the scatter graph plotted. Overall both models, the integer and the dummy encoded models seem to perform relatively similar, although the dummy encoded model has a higher overall predicted mean.","f74a8493":"Naturally we start by importing the common libraries at the start","6a8e8564":"Hi Everyone! This is my very first post and exploration of using machine learning algorithms to see if we can predict the ratings for apps on the google play store. Most of the posts are exploration data analysis and I  hope to contribute a little via the use of machine learning!\n\nI've been studying about machine learning and am still a beginner, so if there's any comments\/suggestions please feel free to tell me and I hope we can all learn together!","e2d3db61":"Doing checks and we are good to go! So I created first this dataframe that has **integer encoding** of categorical variables, defined as df","efe6f75e":"We start off by looking at **linear regression** model (without the genre label)","6f04b663":"Technically when doing the cleaning of genres, one-hot should also be applied in this instance. However, I did not as firstly, it's a subset of the categorical column and secondly, application of a dummy variable would significantly increase the number of independent variables. \n\nSo to combat this instead, we ran two seperate regressions, one including and one excluding such genre data. When including the data, we only considered in the impact\/information provided via the genre section purely based on it's numeric value.","3868e559":"After our final checks for the preprocessing of our data, looks like we can start work! So the next question is what exactly are we doing and how are we doing it.\n\nSo the goal of this instance is to see if we can use existing data provided(e.g. Size, no of reviews) to predict the ratings of the google applications. **In other words, our dependent variable Y, would be the rating of the apps.**  \n\nOne important factor to note is that the dependent variable Y, is a continuous variable(aka infinite no of combinations), as compared to a discrete variable. Naturally there are ways to convert our Y to a discrete variable but **I decided to keep Y as a continuous variable** for the purposes of this machine learning session. \n\n**Next question, what models should we apply and how should we evaluate them?**\n\nModel wise, I'm not too sure as well as there are like a ton of models out there that can be used for machine learning. Hence, I basically just chose the 3 most common models that I use, being linear regression, SVR, and random forest regressor. \n\n**We technically run 4 regressions for each model used, as we consider one-hot vs interger encoded results for the category section, as well as including\/excluding the genre section.**\n\nWe then evaluate the models by comparing the predicted results against the actual results graphically, as well as use the mean squared error, mean absolute error and mean squared log error as possible benchmarks.\n\nThe use of the error term will be evaluated right at the end after running through all the models.","29d4c658":"Finally, looking at the results, it is not easy to conclude which model has the best predictive accuracy and lowest error term. Using this round of data as a basis, the dummy encoded SVM model including genres has the lowest overall error rates, followed by the integer encoded RFR model including genes. Yet, all models seem to be very close in terms of it's error term, so this result is likely to change.\n\nWhat is very surprising to me is how the RFR dummy model has such a significantly more error term compared to all the other models, even though on the surface it seemed to perform very similarly to the RFR integer model.\n\n**Concluding thoughts**\nIt was pretty fun doing this project, using the three different machine learning models for continuous variables to see if it performed well in predictive analysis, based on the data that was provided.  If you guys have any suggestions\/comments please do feel free to post, as I'm still a beginner and want to learn more! Have a great and blessed day everyone!","caf1be4a":"When including the genre data, we see a slight difference in the mean between the integer and dummy encoded linear models. The dummy encoded model's std is still higher than the integer encoded model.\n\nWhat's striking to me personally is that the dummy encoded regression line in the scatterplot is now flatter than the integer encoded regression line, which might suggest a \"worse\" outcome, given that usually you would want your regression's beta value to be closer to 1 than to 0. \n\n**Next up is the SVR model.**","e81b4d3e":"If we look at what influences the ratings, the top 4 being reviews, size, category, and number of installs seem to have the highest influence. This is quite an interesting observation, while also rationalizable.","de81143c":"Looking at the breakdown even further, it would seem that indeed Reviews, size and number of install remain as a significant contributer to the predictiveness of app ratings.  What's interesting to me is that how the Tools category of apps have such a high level of predictiveness in terms of ratings, as say compared to the Food and Drink category."}}