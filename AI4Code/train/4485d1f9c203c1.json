{"cell_type":{"c4c5a820":"code","05cdadbd":"code","79e7d0db":"code","3a0d32fd":"code","05ff669e":"code","968a2e6f":"code","7a14cef1":"code","bb5b4e0d":"code","a42d82cc":"code","62b72017":"code","778b5cd6":"code","95ad8e4e":"code","6b91dc22":"code","94148e5c":"code","a90b067b":"code","69eb463a":"code","aa01adf7":"code","e2a1bc16":"code","8f6f7cc8":"code","80f41975":"code","077fa27f":"code","be7a967f":"code","0963b146":"code","33105e19":"code","aa173f53":"code","9e6a8299":"code","bd5e42c8":"code","dae13ad6":"code","88d7a970":"markdown","b653729d":"markdown","db42da88":"markdown","6bdb2783":"markdown","6f7c1915":"markdown","783b9d88":"markdown","66f834bb":"markdown","c3376344":"markdown","59d60b42":"markdown"},"source":{"c4c5a820":"#!pip install transformers","05cdadbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport os\nimport time\nimport sys\n\nimport re\nimport nltk \nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\ntf.keras.backend.clear_session()\n\nimport torch\nimport transformers\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nfrom transformers import RobertaConfig, TFRobertaPreTrainedModel\nfrom transformers.modeling_tf_roberta import TFRobertaMainLayer\nfrom transformers.modeling_tf_utils import get_initializer\n\nimport itertools\nimport collections\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","79e7d0db":"#turn on TPU https:\/\/heartbeat.fritz.ai\/step-by-step-use-of-google-colab-free-tpu-75f8629492b3\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)","3a0d32fd":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","05ff669e":"train.head()","968a2e6f":"test.head()","7a14cef1":"# check class distribution in train dataset\nfrom scipy import stats\ntrain.groupby(['target']).size()","bb5b4e0d":"all_texts = []\nfor line in list(train['text']):\n    texts = line.split()\n    for text in texts:\n        all_texts.append(text)","a42d82cc":"toBeCleanedNew='[%s]' % ' '.join(map(str, all_texts))#remove all the quation marks and commas. \n#print(toBeCleanedNew)","62b72017":"rawCorpus='[%s]' % ' '.join(map(str, all_texts))#remove all the quation marks and commas. \n#print(rawCorpus)\nwith open(\"\/kaggle\/working\/rawCorpus.txt\", \"w\") as output:\n    output.write(str(rawCorpus))","778b5cd6":"!pip install tokenizers==0.4.2","95ad8e4e":"#!pip install tokenizers #hugging face tokenizer\n#Huggingface recommends to use ByteLevel tokenizer for Roberta model. But the result was bad. Take BertWordPiece now\nfrom tokenizers import (ByteLevelBPETokenizer,\n                            CharBPETokenizer,\n                            SentencePieceBPETokenizer,\n                            BertWordPieceTokenizer)\ntokenizer = BertWordPieceTokenizer()\n\npath=\"\/kaggle\/working\/rawCorpus.txt\"\n#set vocab_size to 15000 as the len(train_set)was something like 12500 \ntokenizer.train(files=path, vocab_size=15_000, min_frequency=2)\n#tokenizer.train(files=path, vocab_size=15_000, min_frequency=2,special_tokens=[\n   # \"<s>\",\n    #\"<pad>\",\n    #\"<\/s>\",\n    #\"<unk>\",\n    #\"<mask>\"\n#])","6b91dc22":"tokenizer.save(\".\", \"\/kaggle\/working\/newBert\")","94148e5c":"tokenizer = BertWordPieceTokenizer(\n    '\/kaggle\/working\/newBert-vocab.txt',\n     lowercase=True, \n)","a90b067b":"output = tokenizer.encode(\"Hello, y'all! \ud83d\ude42 How are you  ?\")\nprint(output.tokens)\nprint(output.ids)","69eb463a":"#Tokenize the whole texts\n\ndef bert_token(texts,max_len=512): \n    all_input_ids=[]\n    all_mask_ids=[]\n    all_seg_ids=[]\n    for token in texts: \n    \n        input_ids=tokenizer.encode(token).ids\n        mask_ids = [1] * len(input_ids)\n        seg_ids = [0] * len(input_ids)\n        padding = [0] * (max_len - len(input_ids))\n        input_ids += padding\n        mask_ids += padding\n        seg_ids += padding\n        all_input_ids.append(input_ids)\n        all_mask_ids.append(mask_ids)\n        all_seg_ids.append(seg_ids)\n\n    \n    return np.array(all_input_ids), np.array(all_mask_ids), np.array(all_seg_ids)","aa01adf7":"train_input=bert_token(train['text'],max_len=100)\ntest_input=bert_token(test['text'],max_len=100)","e2a1bc16":"print(train_input)","8f6f7cc8":"#take a quick look of the trainset\n\ntrain[\"Tokened_Text\"]=train[\"text\"].apply(lambda x:tokenizer.encode(x).ids)\nfrom collections import Counter\ntrain_tokened=[]\nfor i in train[\"Tokened_Text\"]:\n    train_tokened+=i\nprint(\"Total amount of tokens in train dataset is:\", len(train_tokened))\ndistinct_list= (Counter(train_tokened).keys())\nprint(\"The vocabulary size in subtrain dataset is :\",len(distinct_list))","80f41975":"#sequence length of the train dataset\ntrain_length_dist=[]\n\nfor l in train[\"Tokened_Text\"]:\n    train_length_dist+=[len(l)]\ny = np.array(train_length_dist)\nsns.distplot(y);","077fa27f":"#Model need two types of data: input_ids (sequence), attention_masks)\ninput_ids_train = train_input[0]\nattention_masks_train = train_input[1]\ninput_ids_test =test_input[0]\nattention_masks_test = test_input[1]","be7a967f":"print(input_ids_train)","0963b146":"#Build a wrapper on top of Huggingface pretrained model\nclass CustomModel(TFRobertaPreTrainedModel):\n    def __init__(self, config, *inputs, **kwargs):\n        super(CustomModel, self).__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n        self.roberta = TFRobertaMainLayer(config, name=\"roberta\")\n        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n        self.classifier = tf.keras.layers.Dense(units=config.num_labels,\n                                                name='classifier', \n                                                kernel_initializer=get_initializer(\n                                                    config.initializer_range))\n\n    def call(self, inputs, **kwargs):\n        outputs = self.roberta(inputs, **kwargs)\n        pooled_output = outputs[1]\n        pooled_output = self.dropout_1(pooled_output, training=kwargs.get('training', False))\n        logits = self.classifier(pooled_output)\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        return outputs","33105e19":"# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n        \n    config = RobertaConfig.from_pretrained('roberta-base')\n    model = CustomModel.from_pretrained('roberta-base')\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0)\n    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n    metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\nmodel.summary()\n","aa173f53":"\nbatch_size = 128\nskf = StratifiedKFold(n_splits=5, shuffle=False)\nX, y = input_ids_train, train['target'].values.reshape(-1, 1)\nskf.get_n_splits(X, y)\nfor i, (train_index, test_index) in enumerate(skf.split(X, y)):\n    X_train, attention_masks_train_stratified, X_test, attention_masks_test_stratified = X[train_index], attention_masks_train[train_index], X[test_index], attention_masks_train[test_index]\n    y_train, y_test = tf.keras.utils.to_categorical(y[train_index]), tf.keras.utils.to_categorical(y[test_index])\n    X_train = X_train[:-divmod(X_train.shape[0], batch_size)[1]]\n    attention_masks_train_stratified = attention_masks_train_stratified[:-divmod(attention_masks_train_stratified.shape[0], batch_size)[1]]\n    y_train = y_train[:-divmod(y_train.shape[0], batch_size)[1]]\n    model.fit([X_train, attention_masks_train_stratified], y_train, validation_data=([X_test, attention_masks_test_stratified], y_test), batch_size=batch_size, epochs=5)\n    print('Split ' + str(i) + ' is finished.')\n","9e6a8299":"model_output = model.predict([input_ids_test, attention_masks_test])\nsubmission['target'] = np.argmax(model_output, axis=1).flatten()\nsubmission['target'].value_counts()","bd5e42c8":"submission.head()","dae13ad6":"model_output = model.predict([input_ids_test, attention_masks_test])\nsubmission['target'] = np.argmax(model_output, axis=1).flatten()\nsubmission['target'].value_counts()\nsubmission.to_csv('submission.csv',index=False)","88d7a970":"**Build corpus and train custom language model**","b653729d":"![If you see this text, it means my lovely banner disappeared](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/542749\/990656\/hello.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1583757349&Signature=OVlmlMduJXj5i49k7ecnvSChgWlmr964x6nxnARACAN2GYRt%2FLbK3It6YjE%2BUKM3mzid7zHacqJ0enr3A%2BYf7q3gKiIPghxa1SbVHW9tNN%2FnoLqaG7N5G4Ql7r3phRD0kHbusp4dYEidZZk1RIT55asK2f%2BLaR3asZtgjv6SKwxVr6bZdYW7Q2VYB7rVK%2BZ%2BXylzU%2FL%2B5kiL0c6n2rD0DFtiLMpFTGEs5iQ3lLYQcaCf1eAZcTdfAiyjOctGjT7G1bv7qMgMBLw6dLLJJX5IMVqP7Ajw06Yo4iBxv8pJ3n52m9a0VTATdYgTiElHnPyN7%2F%2BkNH8KVPG8sfK4ZSTJIw%3D%3D)","db42da88":"**Importing libraries and loading data**\n\nImportant: No text preprocessig needed. We go directly with raw data!!!","6bdb2783":"Here are the pre-process requirement for the 5 model types recommended by huggingface.\n\nBERT: [CLS] + tokens + [SEP] + padding\n\nDistilBERT: [CLS] + tokens + [SEP] + padding\n\nRoBERTa: [CLS] + prefix_space + tokens + [SEP] + padding\n\nXLM: [CLS] + tokens + [SEP] + padding\n\nXLNet: padding + tokens + [SEP] + [CLS]","6f7c1915":"**Building customer model wrapper so that we can plug and play**","783b9d88":"**Applying tokenization process to whole texts**","66f834bb":"In this notebook, we are going to talk about a love story between Google and Facebook. Just kidding:D.  Let's start again. \n\nNatural language processing(NLP) has been an old topic that existed for many years. Thanks to the rise of transfer learning in NLP, 2018 and 2019 have been landmark years in the field of machine learning for language. In this notebook, we are going to explore the way of using the Roberta model for sentiment analysis tasks.  \n\nBefore jumping into the code, I would like to mention the relationship between Bert and Roberta. Bidirectional encoder representations from transformers (Bert) is a technique for NLP pre-training developed by Google. It was created and published in 2018. Since then, Bert has attracted worldwide attention in the NLP field. Researchers and developers have done numerous studies to explore the pro and cons of Bert technique. In July 2019, Facebook released Roberta model, which works as an improved version of Bert by implementing the following changes:\n\n* Roberta uses 160 GB of text for pretraining, including 16GB of Books Corpus and English Wikipedia used in BERT.\n* Roberta removed BERT\u2019s next-sentence pretraining objective and training with much larger mini-batches and learning rates. (Soon you might notice that I skipped the segment id tokens when connecting data to the model.)\n\nRoberta [teams](https:\/\/arxiv.org\/pdf\/1907.11692.pdf) revealed that Roberta had reached the top position on the GLUE leaderboard. Glue is a benchmark that collects resources for training, evaluating, and analyzing natural language understanding systems.\n\nSo that is a brief introduction to Roberta and Bert. Hereinbelow I am going to show you how I implemented this sentiment analysis task using Roberta method. Please be advised that this kernel was built on top of [Utsav Nandi](https:\/\/www.kaggle.com\/utsavnandi\/roberta-using-huggingface-tf-implementation) 's work. I removed his tokenization part by adding a custom language model to the original work. BTW, This is the second kernel I 've created for this competition. In the previous work, I trained data using LSTM and Bert models. You can find it from this [link](https:\/\/www.kaggle.com\/latong\/lstm-vs-bert-train-data-from-scratch-huggingface). (I will update that notebook in a couple of days.)\n\nAfter reading this notebook, you will be able to implement your research in any field in whatever languages. And the most important thing is that you will also stay on trend \ud83d\ude1b So let's explore the code now. \n\n","c3376344":"**Making predictions**","59d60b42":"That's all for now. Feel free to leave your comments :)"}}