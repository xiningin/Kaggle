{"cell_type":{"b7584a9b":"code","805cfe08":"code","f1b3ad77":"code","c334574d":"code","00a1b101":"code","02df8aed":"code","77828054":"code","68ba8e9a":"code","d73c03e7":"code","56dd473a":"code","47493072":"code","b4863ff5":"code","8de70d10":"code","51882c71":"code","82eb487c":"code","bc66a1ee":"code","03de9078":"code","852b77aa":"markdown","746c1cb9":"markdown","f7e474f4":"markdown","67d946de":"markdown","228042f8":"markdown"},"source":{"b7584a9b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","805cfe08":"train_orig=pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\")\ntest_nolabel=pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv\")","f1b3ad77":"from nltk.corpus import stopwords\nfrom nltk import word_tokenize\nimport string\nimport re\nstop_words = set(stopwords.words('english'))\n\ntrain = train_orig\n\ndef remove_stopwords(line):\n    word_tokens = word_tokenize(line)\n    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n    return \" \".join(filtered_sentence)\n\ndef preprocess(line):\n    line = line.lower()  #convert to lowercase\n    line = re.sub(r'\\d+', '', line)  #remove numbers\n    line = line.translate(line.maketrans(\"\",\"\", string.punctuation))  #remove punctuation\n#     line = line.translate(None, string.punctuation)  #remove punctuation\n    line = remove_stopwords(line)\n    return line\nfor i,line in enumerate(train.tweet):\n    train.tweet[i] = preprocess(line)","c334574d":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train['tweet'], train['label'], test_size=0.5, stratify=train['label'])\n\ntrainp=train[train.label==1]\ntrainn=train[train.label==0]\nprint(trainp.info())\ntrainn.info()","00a1b101":"# Let us balance the dataset\ntrain_imbalanced = train\nfrom sklearn.utils import resample\ndf_majority = train[train.label==0]\ndf_minority = train[train.label==1]\n \n# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=len(df_majority),    # to match majority class\n                                 random_state=123) # reproducible results\n \n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n \n# Display new class counts\nprint(\"Before\")\nprint(train.label.value_counts())\nprint(\"After\")\nprint(df_upsampled.label.value_counts())\n\nX_train, X_test, y_train, y_test = train_test_split(df_upsampled['tweet'], df_upsampled['label'], test_size=0.5, stratify=df_upsampled['label'])","02df8aed":"from sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\n# Xtext=train.tweet\n# Xtest=test.tweet\n# y=train.label\n# test\n# ytest=test.label","77828054":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nvect = CountVectorizer()\ntf_train=vect.fit_transform(X_train)  #train the vectorizer, build the vocablury\ntf_test=vect.transform(X_test)  #get same encodings on test data as of vocabulary built","68ba8e9a":"tf_test_nolabel=vect.transform(test_nolabel.tweet)","d73c03e7":"# print(tf_train)\n# vect.get_feature_names()[:10] #print few features only to avoid slowing down the notebook","56dd473a":"model.fit(X=tf_train,y=y_train)","47493072":"expected = y_test\npredicted=model.predict(tf_test)","b4863ff5":"from sklearn import metrics\n\nprint(metrics.classification_report(expected, predicted))\nprint(metrics.confusion_matrix(expected, predicted))","8de70d10":"from mlxtend.plotting import plot_confusion_matrix\n\nplot_confusion_matrix(metrics.confusion_matrix(expected, predicted))","51882c71":"print(trainp.iloc[:10])\ntrainn.iloc[:10]","82eb487c":"gg=X_test.reset_index(drop=True)\n# print(gg)\nfor i, p in enumerate(predicted):\n#     print(i)\n    print (gg[i] + \" - \" + str(p))\n    if i>5:\n        break #to avoid a lot of printing and slowing down the notebook","bc66a1ee":"predicted_nolabel=model.predict(tf_test_nolabel)\nfor i, p in enumerate(tf_test_nolabel):\n#     print(i)\n    print (test_nolabel.tweet[i] + \" - \" + str(predicted_nolabel[i]))\n    if i>5:\n        break #to avoid a lot of printing and slowing down the notebook","03de9078":"test_custom=pd.DataFrame([\"racist\", \"white judge trial\", \"it is a horrible incident\", \"@user #white #supremacists want everyone to see the new \u00e2\u0080\u0098  #birds\u00e2\u0080\u0099 #movie \u00e2\u0080\u0094 and here\u00e2\u0080\u0099s why\", \" @user #white #supremacists want everyone to see the new \u00e2\u0080\u0098  #birds\u00e2\u0080\u0099 #movie \u00e2\u0080\u0094 and here\u00e2\u0080\u0099s why\", \"@user  at work: attorneys for white officer who shot #philandocastile remove black judge from presiding over trial. ht\u00e2\u0080\u00a6\"])\ntf_custom = vect.transform(test_custom[0])\nmodel.predict(tf_custom)","852b77aa":"**Convert text data to numerical data**","746c1cb9":"**New report with stratification enabled. Shows further improvement in results\n**<pre>\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98     14860\n           1       0.89      0.42      0.57      1121\n\n    accuracy                           0.96     15981\n   macro avg       0.92      0.71      0.77     15981\nweighted avg       0.95      0.96      0.95     15981\n\n[[14800    60]\n [  650   471]]\n<\/pre>","f7e474f4":"**New metric:**\n<pre>\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98     14848\n           1       0.88      0.40      0.55      1133\n\n    accuracy                           0.95     15981\n   macro avg       0.92      0.70      0.76     15981\nweighted avg       0.95      0.95      0.95     15981\n\n[[14786    62]\n [  683   450]]\n<\/pre>","67d946de":"**Let us do some pre-processing. Without preprocessing results are:  (Avoid looking at these metrics in the beginning, will be explained in the end of notebook)**\n<pre>\n               precision    recall  f1-score   support\n \n            0       0.95      1.00      0.97     14880\n            1       0.85      0.35      0.49      1101\n \n     accuracy                           0.95     15981\n    macro avg       0.90      0.67      0.73     15981\n weighted avg       0.95      0.95      0.94     15981\n \n [[14815    65]\n [  718   383]]\n<\/pre>","228042f8":"**Classification report after upsampling the minority classes. Look at updated values for label 1**\n<pre>\n              precision    recall  f1-score   support\n\n           0       0.98      0.91      0.94     14860\n           1       0.92      0.98      0.95     14860\n\n    accuracy                           0.94     29720\n   macro avg       0.95      0.94      0.94     29720\nweighted avg       0.95      0.94      0.94     29720\n\n[[13542  1318]\n [  345 14515]]\n<\/pre>"}}