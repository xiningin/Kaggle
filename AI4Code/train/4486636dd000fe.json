{"cell_type":{"5c217d7d":"code","649ed71c":"code","882a88e6":"code","68719f78":"code","731cda25":"code","4bb6dc49":"code","8fc3182c":"code","9616f5d7":"code","20c6976c":"code","42a223fa":"code","eaa6eb69":"code","2f2e0298":"code","99f4225c":"code","e9426a63":"code","ea5a77db":"code","d03eb816":"code","927c775a":"code","68346615":"code","bf28da36":"code","17f620a6":"code","76803c78":"code","5dff50c0":"code","391a6372":"code","97ccc937":"code","d0bbf436":"code","7b5c2c30":"code","b9a36e76":"code","5d3f3734":"code","9af9d49f":"code","0fd8a277":"code","d7cac7ce":"code","b254814d":"code","be5f6826":"code","9856e668":"markdown","93d011bc":"markdown","0ee76356":"markdown","c8573a01":"markdown","475b6e9c":"markdown","18124640":"markdown","8e9c4595":"markdown","ff1e5bb7":"markdown","e407fe93":"markdown","65a82ed5":"markdown","5a4a47c7":"markdown","73d98685":"markdown","e707d376":"markdown","42c47438":"markdown","17d01430":"markdown","25af0812":"markdown","51ee8d49":"markdown","cd0b221e":"markdown","e8cb7b26":"markdown"},"source":{"5c217d7d":"import numpy as np \nimport pandas as pd \nimport matplotlib as mp\nimport os\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import opinion_lexicon\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation,TruncatedSVD\nimport matplotlib.pyplot as plt","649ed71c":"#uploading data in dataframe\ntrain=pd.read_csv(\"..\/input\/train.csv\",sep=',')\ntest=pd.read_csv(\"..\/input\/test.csv\",sep=',')","882a88e6":"#displayin shapes\nprint ('train shapes : %s'%str(train.shape))\nprint ('test shapes : %s'%str(test.shape))","68719f78":"#displaying exemple data\ntrain.head(5)","731cda25":"#displaying exemple of insincere data \ntrain[train.target==1].head(5)","4bb6dc49":"#displayin dataframe info\ntrain.info()","8fc3182c":"#counting target values\ntrain.target.value_counts()","9616f5d7":"positive=opinion_lexicon.positive()\nnegative=opinion_lexicon.negative()\nstop = stopwords.words('english')\nprint(len(positive))\nprint(len(negative))\nprint(len(stop))","20c6976c":"train['word_count'] = train['question_text'].apply(lambda x: len(str(x).split(\" \")))\n#train['char_count'] = train['question_text'].str.len()\n#stop = stopwords.words('english')\n#train['stopwords'] = train['question_text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n#train['numerics'] = train['question_text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n#train['upper'] = train['question_text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\npositive=opinion_lexicon.positive()\nnegative=opinion_lexicon.negative()\n#train['postive'] = train['question_text'].apply(lambda x: len([x for x in x.split() if x in positive]))\ntrain['negative'] = train['question_text'].apply(lambda x: len([x for x in x.split() if x in negative]))","42a223fa":"#basic statistic about word_count\ntrain.word_count.describe()","eaa6eb69":"#ploting box plot of word_count by target without outlier\ntrain.boxplot(column='word_count', by='target', grid=False,showfliers=False)","2f2e0298":"#ploting box plot of word_count by target without outlier\n#train.boxplot(column='positive', by='target', grid=False,showfliers=False)","99f4225c":"#ploting box plot of word_count by target without outlier\ntrain.boxplot(column='negative', by='target', grid=False,showfliers=False)","e9426a63":"#lower case\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n#Removing Punctuation\ntrain['question_text'] = train['question_text'].str.replace('[^\\w\\s]','')\n#Removing numbers\ntrain['question_text'] = train['question_text'].str.replace('[0-9]','')\n#Remooving stop words and words with length <=2\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop and len(x)>2))\n#Stemming\n#from nltk.stem import SnowballStemmer\n#ss=SnowballStemmer('english')\n#train['question_text'] = train['question_text'].apply(lambda x: \" \".join(ss.stem(x) for x in x.split()))\nfrom nltk.stem import WordNetLemmatizer\nwl = WordNetLemmatizer()\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(wl.lemmatize(x,'v') for x in x.split()))","ea5a77db":"from nltk.stem import SnowballStemmer,WordNetLemmatizer,PorterStemmer,LancasterStemmer\nwl = WordNetLemmatizer()\nss=SnowballStemmer('english')\nps=PorterStemmer()\nls=LancasterStemmer()\ntest_list=['does','peaople','writing','beards','enjoyment','bought','leaves','gave','given','generaly','would']\nfor item in test_list :\n    print('lemmatizer : %s'%wl.lemmatize(item,'v'))\n    print('SS stemmer : %s'%ss.stem(item))\n    print('PS stemmer : %s'%ps.stem(item))\n    print('LS stemmer : %s'%ls.stem(item))\n","d03eb816":"train.head(5)","927c775a":"def get_words_freq(corpus):\n    vec = CountVectorizer(ngram_range={1,2}).fit(corpus)\n    #bag of words its a sparse document item matrix\n    bag_of_words = vec.transform(corpus)\n    #we calculate the occurrence for each term. warning, the sum of matrix is a 1 row matrix\n    sum_words = bag_of_words.sum(axis=0) \n    # Vocabulary_ its a dictionary { word :position }  \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1],reverse=True)\n    return words_freq","68346615":"top_sincere=get_words_freq(train[train.target==0].question_text)\nprint(top_sincere[:30])","bf28da36":"top_insincere=get_words_freq(train[train.target==1].question_text)\nprint(top_insincere[:30])","17f620a6":"#[y[0] for y in top_sincere].index('black people')","76803c78":"from wordcloud import WordCloud\nwc=WordCloud(background_color='white')\nwc.generate(''.join(train[train.target==1].question_text))","5dff50c0":"#let's plot\nplt.figure(1, figsize=(15, 15))\nplt.axis('off')\nplt.imshow(wc)\nplt.show()","391a6372":"tfidf_v = TfidfVectorizer(min_df=20,max_df=0.8,sublinear_tf=True,ngram_range={1,2})\n#matrixTFIDF= tfidf_v.fit_transform(train.question_text)\nmatrixTFIDF= tfidf_v.fit_transform(train[train.target==1].question_text)","97ccc937":"print(matrixTFIDF.shape)","d0bbf436":"plt.boxplot(np.array(matrixTFIDF.mean(axis=0).transpose()),showfliers=False)\nplt.show()","7b5c2c30":"svd=TruncatedSVD(n_components=15, n_iter=10,random_state=42)\nX=svd.fit_transform(matrixTFIDF)             ","b9a36e76":"plt.plot(svd.singular_values_[0:15])","5d3f3734":"#Explained variance by our components\nnp.sum(svd.explained_variance_ratio_[0:15])","9af9d49f":"#components_ give the word contribution for each component \nsvd.components_.shape","0fd8a277":"def get_topics(components, feature_names, n=15):\n    for idx, topic in enumerate(components):\n        print(\"Topic %d:\" % (idx))\n        print([(feature_names[i], topic[i])\n                        for i in topic.argsort()[:-n - 1:-1]])","d7cac7ce":"get_topics(svd.components_,tfidf_v.get_feature_names())","b254814d":"lda=LatentDirichletAllocation(n_components=15,random_state=42,max_iter=10)\nZ=lda.fit_transform(matrixTFIDF)  ","be5f6826":"get_topics(lda.components_,tfidf_v.get_feature_names(),n=15)","9856e668":"I tried first to make prediction with only extracted features but the result wasn't good","93d011bc":"get_topics give the n most contributif words in a topic","0ee76356":"## Data upload","c8573a01":"## Most frequent terms for sincere and insincere questions\n\nget_words_freq return for a corpus the sorted list of words by frequency  ","475b6e9c":"As we can see, Quora's questions is composed from few word (mainly <25 words ). the distribution for in insincere question is more spread out.","18124640":"Let' see the 30 most frequent terms of insincere question :","8e9c4595":"I tested several words on nltk stemmer et lemmatizer and i choose to use snowball stemmer","ff1e5bb7":"## Topic Modeling insincere questions\nFor topic modeling we are going to use a TFIDF matrix transformation.","e407fe93":"There is no missing values ","65a82ed5":"### Topic modeling using LSA","5a4a47c7":"We can also use wordcloud to visualize the most frequent terms for insincere questions","73d98685":"We have to deal with unbalanced target Feature...","e707d376":"Let' see the 30 most frequent terms of sincere question :","42c47438":"## Feature extraction","17d01430":"## Import of libraries","25af0812":"### Topic modeling using LDA","51ee8d49":"5%! it's very low...","cd0b221e":"## Text transformation","e8cb7b26":"# Quora data exploration (EDA) and Topic modeling (LSA vs LDA)"}}