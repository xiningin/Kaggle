{"cell_type":{"fc88bfcf":"code","b90459ed":"code","5f1585e1":"code","43d18f69":"code","c2549cbf":"code","edc8a7a4":"code","0e3c6f16":"code","841a003c":"code","7b7d69a6":"code","438e7fe8":"code","fdc818e6":"code","4c7b2d02":"code","33eeadd7":"code","c860f22c":"code","0524e79d":"code","2404659e":"code","25fd3eba":"code","74223286":"code","1c421a28":"code","ac52f711":"code","70742667":"code","b7aa35be":"code","29013955":"code","e3bbcb43":"code","b336e60b":"code","f70e4ff0":"code","1250d145":"code","91c1cbc2":"code","85e87bf6":"code","eba9b2e5":"code","7e53aaa0":"markdown","ec01c822":"markdown","0e3f345a":"markdown","c1c9f7b7":"markdown","45db9f40":"markdown","6e0b04ac":"markdown","25876b8c":"markdown"},"source":{"fc88bfcf":"import pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport math\n\nfrom matplotlib import pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error,accuracy_score,f1_score,recall_score,precision_score, confusion_matrix\n%matplotlib inline","b90459ed":"df = pd.read_csv('..\/input\/concrete (1).csv')","5f1585e1":"# See what variables are tracked\ndf.columns","43d18f69":"# Just see what the first 3 rows look like\ndf.head(3)","c2549cbf":"print('Number of empty entries by column')\ndf.isnull().sum()","edc8a7a4":"# Split into inputs (x) and targets (y)\nx = df.drop('strength', axis=1)\n\ny = pd.DataFrame(df['strength'])\ny.columns = ['concrete_compressive_str_MPa']","0e3c6f16":"# The number of samples with which to work\nx.shape[0]","841a003c":"df.describe().transpose()","7b7d69a6":"plt.figure(figsize=(10,8))\nsns.heatmap(df.drop(columns = 'age').corr(),\n            annot=True,\n            linewidths=.5,\n            center=0,\n            cbar=False,\n            cmap=\"YlGnBu\")\nplt.show()","438e7fe8":"# Lets check for highly correlated variables\ncor = df.corr()\ncor.loc[:,:] = np.tril(cor,k=-1)\ncor = cor.stack()\ncor[(cor > 0.55) | (cor< -0.55)]","fdc818e6":"sns.pairplot(df, diag_kind= 'kde')\nplt.show()","4c7b2d02":"plt.figure(figsize=(25,10))\npos = 1\nfor i in df.columns:\n    plt.subplot(3, 4, pos)\n    sns.boxplot(df[i])\n    pos += 1 ","33eeadd7":"# X = df.drop(columns = 'age')\n# y = df.age","c860f22c":"# 80\/20 split between testing and training\nproportion_of_training = 0.8\n\n# integer intervals [0, t_t_cutoff] and [t_t_cutoff+1, 1030]\ntrain_test_cutoff = int(x.shape[0] * proportion_of_training) \n\n# train (and validation)\nx_train = x.iloc[0:train_test_cutoff]\ny_train = y.iloc[0:train_test_cutoff]\n\n# test\nx_test = x.iloc[train_test_cutoff+1:]\ny_test = y.iloc[train_test_cutoff+1:]\n\n# Now split x_train further into actual training and validation data\n# fit on training; tune on validation\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3)\n\nx_train = x_train.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\n\nx_val = x_val.reset_index(drop=True)\ny_val = y_val.reset_index(drop=True)\n\nx_test = x_test.reset_index(drop=True)\ny_test = y_test.reset_index(drop=True)\n","0524e79d":"for i in x_train.columns:\n    q1, q2, q3 = x_train[i].quantile([0.25,0.5,0.75])\n    IQR = q3 - q1\n    a = x_train[i] > q3 + 1.5*IQR\n    b = x_train[i] < q1 - 1.5*IQR\n    x_train[i] = np.where(a | b, q2, x_train[i]) ","2404659e":"plt.figure(figsize=(15,10))\npos = 1\nfor i in x_train.columns:\n    plt.subplot(3, 3, pos)\n    sns.boxplot(x_train[i])\n    pos += 1 ","25fd3eba":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nlr = LinearRegression()\n\nlr.fit(x_train, y_train)\n\nlr_predictions = lr.predict(x_val)\nmse = mean_squared_error(y_val, lr_predictions)\nprint('{} had an MSE of {}'.format('linear regression', mse))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse)))\nprint('{} had an R^2 of {}'.format('linear regression', r2_score(y_val, lr_predictions)))\n","74223286":"for var_name, coeff in zip(x_train.columns.values, lr.coef_[0]):\n    print(var_name, '\\t\\t', coeff)","1c421a28":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\n\n# try linear regression with each column standard scaled; THEN should be able \n# to see what factors are important (at least relative to each other) in the lin reg\nmms.fit(x_train)\nlr.fit(mms.transform(x_train), y_train)\n\nlr_predictions = lr.predict(mms.transform(x_val))\nmse = mean_squared_error(y_val, lr_predictions)\nprint('{} had an MSE of {}'.format('min-max scaled linear regression',\n                                   mse))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse)))\n\nprint('{} had an R^2 of {}'.format('min-max scaled linear regression',\n                                   r2_score(y_val, lr_predictions)))\n","ac52f711":"for var_name, coeff in zip(x_train.columns.values, lr.coef_[0]):\n    print(var_name, '\\t\\t', coeff)","70742667":"# Let's see the distributions of each of those variables\n\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n\nmatplotlib.rcParams['font.size'] = 22\n\n\nnum_cols = round(math.sqrt(x_train.shape[1]))\nnum_rows = round(math.sqrt(x_train.shape[1])) + 2\nsorted_cols = sorted(x_train.columns)\n\n\nfig = plt.figure(1, figsize=(26, 24))\nnum_plotted_subplots = 0\nfor col in sorted_cols:\n    num_plotted_subplots += 1\n    ax = fig.add_subplot(num_rows, num_cols, num_plotted_subplots)\n    \n    ax.hist(x_train[col].values, color='skyblue', bins=36)\n    \n    ax.grid(color='lightgray', linestyle='--', axis='y')\n    ax.set_axisbelow(True)\n    ax.set_facecolor(color='gray')\n    ax.set_xlabel(col)\nplt.subplots_adjust(left=0.2, bottom=0.2, right=0.8, top=0.8,\n        wspace=0.25, hspace=0.35)\nplt.show()","b7aa35be":"# Let's see what all 2-variable combinations' scatter plots look like (if there's anything interesting)\nfig = plt.figure(1, figsize=(30, 55))\nmatplotlib.rcParams['font.size'] = 22\n\n\nnum_cols = 3\nnum_rows = 10\n\nnum_plotted_subplots = 0\n                     \n# reverse because plots with `age' x-axis all look very similar\nrevrese_sorted_cols = [col for col in reversed(sorted_cols)]\n\nfor col_x_idx, col_x in enumerate(revrese_sorted_cols):\n    # this way, plot all combinations, NOT all permutations\n    for col_y in revrese_sorted_cols[col_x_idx:]:\n        if col_x == col_y:\n            continue\n            \n        num_plotted_subplots += 1\n        ax = fig.add_subplot(num_rows, num_cols, num_plotted_subplots)\n\n        ax.scatter(x_train[col_x].values, x_train[col_y].values, color='orange', s=30)\n\n        ax.grid(color='lightgray', linestyle='--', axis='both')\n        ax.set_axisbelow(True)\n        ax.set_facecolor(color='gray')\n        ax.set_xlabel(col_x)\n        ax.set_ylabel(col_y)\nplt.subplots_adjust(left=0.2, bottom=0.2, right=0.8, top=0.8,\n        wspace=0.25, hspace=0.35)\nplt.show()","29013955":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators=1000, max_depth=None)\n\nrfr.fit(x_train, y_train.values.ravel()) # used ravel() to get rid of a warning message\n\nmse = mean_squared_error(y_val, rfr.predict(x_val))\nprint('{} had an MSE of {}'.format('random forest regressor', mse))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse)))","e3bbcb43":"from sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import zscore\n### Number of nearest neighbors\nknn_clf = KNeighborsClassifier(n_neighbors= 5 , weights = 'distance' )\n# knn_clf.fit(x_train, y_train)\n\nfrom sklearn import preprocessing\nfrom sklearn import utils\nlab_enc = preprocessing.LabelEncoder()\ny_train = lab_enc.fit_transform(y_train)\n# print(y_train)\n\nx_trainScaled  = x_train.apply(zscore)\nx_testScaled  = x_test.apply(zscore)\n\nknn_clf.fit(x_trainScaled, y_train)\nmse_knn = mean_squared_error(y_val, knn_clf.predict(x_val))\nprint('{} had an MSE of {}'.format('random forest regressor', mse_knn))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse_knn)))","b336e60b":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\nfrom sklearn.externals.six import StringIO  \n# from IPython.display import Image  \n# from sklearn.tree import export_graphviz\n# import pydotplus\n# import graphviz\n# from os import system\n\ndt = DecisionTreeRegressor(max_depth=3)\n\ndt.fit(x_train, y_train)\n\nmse = mean_squared_error(y_val, dt.predict(x_val))\nprint('{} had an mse of {}'.format('decision tree regressor', mse))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse)))\n\n# to display the decision tree, export to a .dot file and then convert .dot file to .png\n# concrete_dt = open('concrete_dt.dot','w')\n# tree.export_graphviz(dt, out_file=concrete_dt, feature_names = x_train.columns.values,\n#                 filled=True, impurity=False, proportion=True, rounded=True,\n#                 leaves_parallel=False,)\n# concrete_dt.close()\n\n# import pydot\n# (graph,) = pydot.graph_from_dot_file('concrete_dt.dot')\n# graph.write_png('concrete_dt.png')\n\n# retCode = system(\"dot -Tpng concrete_dt.dot -o concrete_dt.png\")\n# if(retCode>0):\n#     print(\"system command returning error: \"+str(retCode))\n# else:\n#     display(Image(\"concrete_dt.png\"))","f70e4ff0":"from time import time\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier","1250d145":"# build a classifier\nclf = RandomForestClassifier(n_estimators=50)\n\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": [1, 1, 10],\n              \"min_samples_split\": [2, 2, 10],\n              \"min_samples_leaf\": [2, 1, 10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\ngs = GridSearchCV(clf,param_dist,cv=2)\n\n# grid_search = GridSearchCV(clf, param_grid=param_dist)\nstart = time()\ngs.fit(x_train, y_train)","91c1cbc2":"# gs.fit(x_train, y_train)\nprint(\"Best parameter for Random Forest: \", gs.best_params_)\nprint(\"Best Estimator for Random Forest: \", gs.best_estimator_)\nprint(\"Mean score for Random Forest: \", gs.cv_results_['mean_test_score'])","85e87bf6":"## KNN \nparam_grid = {'n_neighbors': list(range(1,9)),\n             'algorithm': ('auto', 'ball_tree', 'kd_tree' , 'brute') }\n\ngs_knn = GridSearchCV(knn_clf,param_grid,cv=3)\ngs_knn.fit(x_train, y_train)","eba9b2e5":"print(\"Best parameter for KNN: \", gs_knn.best_params_)\n# print(\"Best parameter for KNN: \", gs_knn.cv_results_['params'])\/\nprint(\"Best Estimator for KNN: \", gs_knn.best_estimator_)\nprint(\"Mean score for KNN: \", gs_knn.cv_results_['mean_test_score'])","7e53aaa0":"## Decision Tree","ec01c822":"## K Nearest Neighbours","0e3f345a":"## Grid Search and Random Search to find best hyper parameter for a model","c1c9f7b7":"Typically, linear regression is a decent place to start for regression problems, and logistic regression is a decent place to start for classification problems. Prediciting concrete compressive strength is a regression problem, so, the linear regression results will serve as the baseline by which all subsequent models are judged. ","45db9f40":"## Random Forest","6e0b04ac":"From the magnitudes of these coefficients (so ignoring the positve or negative sign), the importance of the variables in linear regression is: cement > age > slag > water > superpllesasticizer > flyash > fineaggregate > coarseaggregate.","25876b8c":"The mean squared error of Random Forest is about 41.3 MPa^2, which means the average guess is off by about 6.4 MPa. Again, this is not fantastic, but it is a pretty decent improvement over the linear regression, which had MSE of 241 or so, meaning its average guesses were off by about 15 MPa. "}}