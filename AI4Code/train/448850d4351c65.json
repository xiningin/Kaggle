{"cell_type":{"4524372e":"code","acca9206":"code","75ad8eeb":"code","a2cfcb39":"code","6c694eaa":"code","e566ddc8":"code","22dfc298":"code","cef69949":"code","9cec5d61":"code","1576ce91":"code","83dd8b65":"code","e862f547":"code","11f3e49c":"code","d77c673d":"code","3fc21709":"code","f2f206b0":"code","c2db9111":"code","8c191487":"code","c003ce91":"code","897342a6":"code","c71eb394":"code","a406f910":"code","bece0359":"code","76eb837f":"code","20ee352d":"code","8bc80097":"code","be8db3df":"code","c90d3790":"code","78879bd0":"code","c6bd7181":"code","3179ad10":"code","ef741740":"code","5d43ab69":"code","2d60e12d":"code","6c781dd2":"code","93b042b0":"code","91c3c24a":"code","0655ba0a":"code","7f1fa0fa":"code","12c4500d":"code","83e97ff9":"code","b2893ff5":"code","92fffceb":"code","59cced80":"code","79f2dda6":"code","9ccdff6b":"code","1eb22c8b":"code","5f64251f":"code","c3828828":"code","ce28ab4a":"markdown","9079d0e1":"markdown","36e7c012":"markdown","4357016c":"markdown","b743e648":"markdown","7ff32922":"markdown","d695bdcf":"markdown","17ccfa3a":"markdown","2f9456b2":"markdown","837de904":"markdown","49ce7e52":"markdown","51800805":"markdown","448874aa":"markdown","f8856fc3":"markdown","e51b0bd2":"markdown","f2018740":"markdown","03443ce7":"markdown","87d18700":"markdown"},"source":{"4524372e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","acca9206":"import pandas_profiling as pdp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.float_format = '{:.3f}'.format\n%matplotlib inline\nplt.style.use('fivethirtyeight')","75ad8eeb":"pd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_info_columns', 200)","a2cfcb39":"train=pd.read_csv('..\/input\/prudential-life-insurance-assessment\/train.csv.zip')\ntest=pd.read_csv('..\/input\/prudential-life-insurance-assessment\/test.csv.zip')","6c694eaa":"f, axes = plt.subplots(1, 2, figsize=(15,7))\nsns.boxplot(x = 'Wt', data=train,  orient='v' , ax=axes[0])\nsns.distplot(train['Wt'],  ax=axes[1])","e566ddc8":"\nf, axes = plt.subplots(1, 2, figsize=(15,7))\nsns.boxplot(x = 'Ht', data=train,  orient='v' , ax=axes[0])\nsns.distplot(train['Ht'],  ax=axes[1])","22dfc298":"f, axes = plt.subplots(1, 2, figsize=(15,7))\nsns.boxplot(x = 'BMI', data=train,  orient='v' , ax=axes[0])\nsns.distplot(train['BMI'],  ax=axes[1])","cef69949":"f,axes=plt.subplots(1,2,figsize=(15,7))\nsns.boxplot(x='Ins_Age',data=train,orient='v',ax=axes[0])\nsns.distplot(train['Ins_Age'],ax=axes[1])","9cec5d61":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain['Response'].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Response')\nax[0].set_ylabel('')\nsns.countplot('Response',data=train,ax=ax[1])\nax[1].set_title('Response')\nplt.show()\n","1576ce91":"## Dropping the \"Id\" from train and test set. \n# train.drop(columns=['Id'],axis=1, inplace=True)\n\ntrain.drop(columns=['Id'],axis=1, inplace=True)\ntest.drop(columns=['Id'],axis=1, inplace=True)\n\n## Saving the target values in \"y_train\". \ny = train['Response'].reset_index(drop=True)","83dd8b65":"## Combining train and test datasets together so that we can do all the work at once. \nall_data = pd.concat((train, test)).reset_index(drop = True)\n## Dropping the target variable. \nall_data.drop(['Response'], axis = 1, inplace = True)","e862f547":"def missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    ## the two following line may seem complicated but its actually very simple. \n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n","11f3e49c":"#missing_percentage(train)\nmissing_percentage(all_data)","d77c673d":"#variables are discrete\nall_data[['Medical_History_1','Medical_History_10','Medical_History_15','Medical_History_24','Medical_History_32']].describe()","3fc21709":"all_data['Medical_History_1'].fillna(0, inplace=True)\nall_data['Medical_History_10'].fillna(0, inplace=True)\nall_data['Medical_History_15'].fillna(0, inplace=True)\nall_data['Medical_History_24'].fillna(0, inplace=True)\nall_data['Medical_History_32'].fillna(0, inplace=True)","f2f206b0":"#variables are continuous\nall_data[['Family_Hist_2','Family_Hist_3','Family_Hist_4','Family_Hist_5']].describe()","c2db9111":"all_data['Family_Hist_2'].fillna(all_data['Family_Hist_3'].mean(), inplace=True)\nall_data['Family_Hist_3'].fillna(all_data['Family_Hist_3'].mean(), inplace=True)\nall_data['Family_Hist_4'].fillna(all_data['Family_Hist_3'].mean(), inplace=True)\nall_data['Family_Hist_5'].fillna(all_data['Family_Hist_3'].mean(), inplace=True)","8c191487":"#variables are continuous\nall_data[['Employment_Info_1','Employment_Info_4','Employment_Info_6']].describe()","c003ce91":"all_data['Employment_Info_1'].fillna(all_data['Employment_Info_1'].mean(), inplace=True)\nall_data['Employment_Info_4'].fillna(all_data['Employment_Info_4'].mean(), inplace=True)\nall_data['Employment_Info_6'].fillna(all_data['Employment_Info_6'].mean(), inplace=True)","897342a6":"#variables are continuous\nall_data[['Insurance_History_5']].describe()","c71eb394":"all_data['Insurance_History_5'].fillna(all_data['Insurance_History_5'].mean(), inplace=True)","a406f910":"#missing_percentage(train)\nmissing_percentage(all_data)","bece0359":"categorical=['Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', 'Product_Info_6', 'Product_Info_7', \n             'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', 'InsuredInfo_1', 'InsuredInfo_2', 'InsuredInfo_3', \n             'InsuredInfo_4', 'InsuredInfo_5', 'InsuredInfo_6', 'InsuredInfo_7', 'Insurance_History_1', 'Insurance_History_2', \n             'Insurance_History_3', 'Insurance_History_4', 'Insurance_History_7', 'Insurance_History_8', 'Insurance_History_9', \n             'Family_Hist_1', 'Medical_History_2', 'Medical_History_3', 'Medical_History_4', 'Medical_History_5', 'Medical_History_6', \n             'Medical_History_7', 'Medical_History_8', 'Medical_History_9', 'Medical_History_11', 'Medical_History_12', \n             'Medical_History_13', 'Medical_History_14', 'Medical_History_16', 'Medical_History_17', 'Medical_History_18', \n             'Medical_History_19', 'Medical_History_20', 'Medical_History_21', 'Medical_History_22', 'Medical_History_23', \n             'Medical_History_25', 'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29', \n             'Medical_History_30', 'Medical_History_31', 'Medical_History_33', 'Medical_History_34', 'Medical_History_35', \n             'Medical_History_36', 'Medical_History_37', 'Medical_History_38', 'Medical_History_39', \n             'Medical_History_40', 'Medical_History_41']\n## Creating dummy variable \nfinal_features = pd.get_dummies(all_data, columns=categorical).reset_index(drop=True)\nfinal_features.shape","76eb837f":"final_features","20ee352d":"X = final_features.iloc[:len(y), :]\n\nX_sub = final_features.iloc[len(y):, :]","8bc80097":"X_sub","be8db3df":"def overfit_reducer(df):\n    \"\"\"\n    This function takes in a dataframe and returns a list of features that are overfitted.\n    \"\"\"\n    overfit = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df) * 100 > 95.0:\n            overfit.append(i)\n    overfit = list(overfit)\n    return overfit\n\n\noverfitted_features = overfit_reducer(X)\n\nX = X.drop(overfitted_features, axis=1)\nX_sub = X_sub.drop(overfitted_features, axis=1)","c90d3790":"X_sub","78879bd0":"from sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score","c6bd7181":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 777)","3179ad10":"knn = KNeighborsClassifier(n_neighbors=50)","ef741740":"knn.fit(X_train, y_train)","5d43ab69":"knn_pred = knn.predict(X_test)\naccuracy_score(y_test, knn_pred) # 0.4028627561044064","2d60e12d":"cohen_kappa_score(y_test, knn_pred) #0.16614776021896493","6c781dd2":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nknn_pipe = Pipeline([('knn', KNeighborsClassifier(n_jobs=-1))])\n\nknn_params = {'knn__n_neighbors': range(1, 10)}\n\nknn_grid = GridSearchCV(knn_pipe, knn_params,\n                        cv=5, n_jobs=-1, verbose=True)\n\nknn_grid.fit(X_train, y_train)\n\nknn_grid.best_params_, knn_grid.best_score_","93b042b0":"knn9 = KNeighborsClassifier(n_neighbors=9)\nknn9.fit(X_train, y_train)","91c3c24a":"knn9_pred = knn9.predict(X_test)\naccuracy_score(y_test, knn9_pred) # 0.384619702497895","0655ba0a":"cohen_kappa_score(y_test, knn9_pred) # 0.18439306881473994\n#lower then with n=50","7f1fa0fa":"submission_knn9 = pd.read_csv(\"\/kaggle\/input\/prudential-life-insurance-assessment\/sample_submission.csv.zip\")\nsubmission_knn9.iloc[:,1] = knn9.predict(X_sub)","12c4500d":"from IPython.display import FileLink","83e97ff9":"submission_knn9.to_csv(\"submission_knn9.csv\", index=False)\nFileLink('submission_knn9.csv')\n# 0.29636","b2893ff5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold","92fffceb":"lr = LogisticRegression(random_state=5, class_weight='balanced', solver='saga')","59cced80":"parameters = {'C': (0.0001, 0.001, 0.01, 0.1, 1, 10)}","79f2dda6":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=5)","9ccdff6b":"grid_search = GridSearchCV(lr, parameters, n_jobs=-1, cv=skf) #ask for scoring='roc_auc'\ngrid_search = grid_search.fit(X_train, y_train)\ngrid_search.best_estimator_","1eb22c8b":"grid_search.cv_results_['std_test_score'][1]","5f64251f":"grid_search.best_score_","c3828828":"grid_search.cv_results_","ce28ab4a":"#  Missing Value Analysis","9079d0e1":"# Goal\n\nIn this dataset, you are provided over a hundred variables describing attributes of life insurance applicants. The task is to predict the \"Response\" variable for each Id in the test set. \"Response\" is an ordinal measure of risk that has 8 levels.","36e7c012":"**We can see that Class 8 has the highest distribution.**","4357016c":"* **Target Variable Analysis**","b743e648":"* **BMI**","7ff32922":"* **Height**","d695bdcf":"#  **Analysing features**","17ccfa3a":"Prudential, one of the largest issuers of life insurance in the USA.\n\nIn a one-click shopping world with on-demand everything, the life insurance application process is antiquated. Customers provide extensive information to identify risk classification and eligibility, including scheduling medical exams, a process that takes an average of 30 days.\n\nThe result? People are turned off. That\u2019s why only 40% of U.S. households own individual life insurance. Prudential wants to make it quicker and less labor intensive for new and existing customers to get a quote while maintaining privacy boundaries.\n\nBy developing a predictive model that accurately classifies risk using a more automated approach, you can greatly impact public perception of the industry\n","2f9456b2":"#  Processing data","837de904":"## Data Description\n\n* train.csv - the training set, contains the Response values\n* test.csv - the test set, you must predict the Response variable for all rows in this file\n\n\n* Id :\tA unique identifier associated with an application.\n* Product_Info_1-7 :\tA set of normalized variables relating to the product applied for\n* Ins_Age :\tNormalized age of applicant\n* Ht :\tNormalized height of applicant\n* Wt :\tNormalized weight of applicant\n* BMI :\tNormalized BMI of applicant\n* Employment_Info_1-6 :\tA set of normalized variables relating to the employment history of the applicant.\n* InsuredInfo_1-6 :\tA set of normalized variables providing information about the applicant.\n* Insurance_History_1-9 :\tA set of normalized variables relating to the insurance history of the applicant.\n* Family_Hist_1-5 :\tA set of normalized variables relating to the family history of the applicant.\n* Medical_History_1-41 :\tA set of normalized variables relating to the medical history of the applicant.\n* Medical_Keyword_1-48 :\tA set of dummy variables relating to the presence of\/absence of a medical keyword being associated with the application.\n* Response :\tThis is the target variable, an ordinal variable relating to the final decision associated with an application","49ce7e52":"# **Logistic regression**","51800805":"# **Ordinal regression**","448874aa":"* **Age**","f8856fc3":"Importing necessary packages and data","e51b0bd2":"## Encoding","f2018740":"# **KNN**","03443ce7":"The following variables are all categorical (nominal):\n\nProduct_Info_1, Product_Info_2, Product_Info_3, Product_Info_5, Product_Info_6, Product_Info_7, Employment_Info_2, Employment_Info_3, Employment_Info_5, InsuredInfo_1, InsuredInfo_2, InsuredInfo_3, InsuredInfo_4, InsuredInfo_5, InsuredInfo_6, InsuredInfo_7, Insurance_History_1, Insurance_History_2, Insurance_History_3, Insurance_History_4, Insurance_History_7, Insurance_History_8, Insurance_History_9, Family_Hist_1, Medical_History_2, Medical_History_3, Medical_History_4, Medical_History_5, Medical_History_6, Medical_History_7, Medical_History_8, Medical_History_9, Medical_History_11, Medical_History_12, Medical_History_13, Medical_History_14, Medical_History_16, Medical_History_17, Medical_History_18, Medical_History_19, Medical_History_20, Medical_History_21, Medical_History_22, Medical_History_23, Medical_History_25, Medical_History_26, Medical_History_27, Medical_History_28, Medical_History_29, Medical_History_30, Medical_History_31, Medical_History_33, Medical_History_34, Medical_History_35, Medical_History_36, Medical_History_37, Medical_History_38, Medical_History_39, Medical_History_40, Medical_History_41\n\nThe following variables are continuous:\n\nProduct_Info_4, Ins_Age, Ht, Wt, BMI, Employment_Info_1, Employment_Info_4, Employment_Info_6, Insurance_History_5, Family_Hist_2, Family_Hist_3, Family_Hist_4, Family_Hist_5\n\nThe following variables are discrete:\n\nMedical_History_1, Medical_History_10, Medical_History_15, Medical_History_24, Medical_History_32\n\nMedical_Keyword_1-48 are dummy variables.","87d18700":"* **Weight**"}}