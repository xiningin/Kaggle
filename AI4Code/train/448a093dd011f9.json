{"cell_type":{"21859902":"code","4dd7acd8":"code","514a5dc9":"code","36244875":"code","6feec6f2":"code","b5649492":"code","8e80fb93":"code","6f2f587c":"code","ee3d9853":"code","63643e88":"code","ce758dbb":"code","375411ef":"code","9e911ad3":"code","35d7ebdd":"code","cf59baef":"code","be2aebc3":"code","bab3642e":"code","a4390b34":"code","3181dc83":"code","fa1b3804":"code","69d4bac2":"code","460a398b":"code","567c4bea":"code","5f1973d1":"code","b892d078":"code","adc30642":"code","35a2832a":"code","95b29176":"code","5f6e12f4":"markdown","b5738dd4":"markdown","d6260634":"markdown","21bd507c":"markdown","3ac1e9d1":"markdown","f137c424":"markdown","c0a5130d":"markdown","cf18d80b":"markdown","0aa0f399":"markdown","01446ee0":"markdown","3ce4649a":"markdown","549638dc":"markdown","9a3acb8f":"markdown","416c6528":"markdown","bd9d9e9b":"markdown","d1656ece":"markdown","8c6e1400":"markdown","628ccfdb":"markdown","95df4707":"markdown","4726378e":"markdown","1241d2a4":"markdown","fa673a52":"markdown","d1339cb8":"markdown","d2a00e6d":"markdown","09d2dfd5":"markdown"},"source":{"21859902":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nimport missingno as msno\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize, sent_tokenize #(word tokenize, sentence tokenize)\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom bs4 import BeautifulSoup\nimport re, string, unicodedata\n\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nfrom wordcloud import ImageColorGenerator\nfrom textblob import TextBlob\n\nimport plotly.offline\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable = True)\n%matplotlib inline","4dd7acd8":"tweets = pd.read_csv('..\/input\/covid19-tweets\/covid19_tweets.csv')\ntweets.head()","514a5dc9":"tweets.info()","36244875":"tweets.describe()","6feec6f2":"msno.matrix(tweets)\n#white lines tells the missing values.","b5649492":"tweets['date'] = pd.to_datetime(tweets[\"date\"])\ntweets['count'] = 1\ntweets['tweet_date'] = tweets['date'].apply(lambda x: x.date())\ntweets['day_sent'] = tweets['date'].dt.strftime('%a')\ntweets['month_sent'] = tweets['date'].dt.strftime('%b')\ntweets['hour_sent'] = tweets['date'].apply(lambda x: x.hour)","8e80fb93":"tweets.head(3)","6f2f587c":"groupedby_date = tweets.groupby('tweet_date').sum().reset_index()\n\nfig = go.Figure(data=[\n    go.Bar(name = 'Verified Users', x = groupedby_date['tweet_date'], y = groupedby_date['user_verified'].tolist()),\n    go.Bar(name = 'Count Of Tweets', x = groupedby_date['tweet_date'], y= groupedby_date['count'].tolist())])\n\nfig.update_layout(barmode='stack')\nfig.show()","ee3d9853":"#Correlation matrix\ntweets[['user_followers', 'user_friends',\n        'user_favourites', 'user_verified']].corr().iplot(kind='heatmap',\n                                                          colorscale=\"Blues\",\n                                                          title=\"Feature Correlation Matrix\")","63643e88":"hashtags = tweets['hashtags'].dropna().tolist()\nunique_hashtags=(\" \").join(hashtags)\n\nresponse = requests.get('https:\/\/www.lifewire.com\/thmb\/Q-QChfPXsb8id3pvLrcXsn2oQNs=\/768x0\/filters:no_upscale():max_bytes(150000):strip_icc()\/twitterlogo-6471b86764ac4076b70f645e632b899e.jpg')\nchar_mask = np.array(Image.open(BytesIO(response.content)))\nimage_colors = ImageColorGenerator(char_mask)\nplt.figure(figsize = (15,15))\nwc = WordCloud(background_color=\"black\", max_words=200, width=400, height=400, mask=char_mask, random_state=1).generate(unique_hashtags)\n# to recolour the image\nplt.imshow(wc.recolor(color_func=image_colors))","ce758dbb":"#Top15_regions\nTop15_regions = pd.DataFrame(tweets['user_location'].value_counts().sort_values(ascending=False)[:15]).T\ncolors = ['lightslategray',] * 15\ncolors[0] = 'crimson'\n\nfig = go.Figure(data=[go.Bar(x=Top15_regions.columns,\n                             y=[Top15_regions[i][0] for i in Top15_regions],\n                             marker_color=colors)])\nfig.update_layout(title_text='Tweets on User Location')","375411ef":"Top10_source = pd.DataFrame(tweets['source'].value_counts().sort_values(ascending=False)[:10]).T\ncolors = ['lightslategray',] * 10\ncolors[0] = 'crimson'\n\nfig = go.Figure(data=[go.Bar(x=Top10_source.columns,\n                             y=[Top10_source[i][0] for i in Top10_source],\n                             marker_color=colors)])\nfig.update_layout(title_text='Different source used for tweeting.')","9e911ad3":"days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\nmonths = ['Jul', 'Aug']\n\ngrouped_by_month_and_day = tweets.groupby(['month_sent', 'day_sent']).sum().reset_index()[['month_sent', 'day_sent', 'count']]\npt = grouped_by_month_and_day.pivot_table(index = 'month_sent', columns = 'day_sent', values = 'count').reindex(index = months, columns = days)\npt.iplot(kind='heatmap',colorscale=\"Blues\", title=\"Heatmap of tweets count as per month and days\")","35d7ebdd":"grouped_by_time = tweets.groupby('hour_sent').sum().reset_index().sort_values(by = 'count', ascending = False)\nfig = px.bar(grouped_by_time, x='hour_sent', y='count', color='hour_sent', \n             labels={'pop':'Count Of Tweets'}, height=400)\nfig.show()","cf59baef":"#Most Used Words in tweets\nword_dict = dict.fromkeys(tweets['user_name'].unique()) #collecting all unique userids\nfor key in word_dict.keys():\n  word_dict[key] = {}\n\nfor name, msg in zip(tweets['user_name'], tweets['text']):\n  for word in msg.split():\n    #any media is included then that is excluded\n    if word not in ['<Media', 'omitted>']:\n      if word in word_dict[name]:\n        word_dict[name][word] += 1\n      else:\n        word_dict[name][word] = 1\n\nfor name in tweets['user_name'].unique():\n  word_dict[name] = {k: v for k, v in sorted(word_dict[name].items(), \n                                             key = lambda item: item[1], reverse= True)}","be2aebc3":"grouped_df = tweets.groupby('user_name').sum().reset_index()\ngrouped_df['Most used words'] = grouped_df['user_name'].apply(lambda x : word_dict[x])\ngrouped_df[['user_name', 'Most used words']]","bab3642e":"#crating vocab for the tweets\ndef get_corpus(text):\n    words = []\n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n    return words\n\ncorpus = get_corpus(tweets.text)","a4390b34":"from collections import Counter\ncounter = Counter(corpus)\nmost_common_words = counter.most_common(10) #prining most common 10 words\nmost_common_words = dict(most_common_words)\nmost_common_words","3181dc83":"#Data Cleaning - Part-1\nstop_words = set(stopwords.words('english')) #set of all stopwords\npunctuation = list(string.punctuation) #all punctuation\n#adding everything into one set\nstop_words.update(punctuation)\n\ndef strip_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text()\n\ndef square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\ndef url_extract(text):\n    return re.sub(r'http\\S+', '', text)\n\ndef stopwords(text):\n    final_text = []\n    for i in text.split():\n        #checking in stopwords and also lowering the text\n        if i.strip().lower() not in stop_words:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n#finally getting all outputs in preprocessing the text using above functions\ndef preprocess(text):\n    text = strip_html(text)\n    text = square_brackets(text)\n    text = url_extract(text)\n    text = stopwords(text)\n    return text","fa1b3804":"tweets['text'] = tweets['text'].apply(preprocess)","69d4bac2":"#processed tweets\ntweets['text'].head(5)","460a398b":"response = requests.get('https:\/\/miro.medium.com\/proxy\/1*SZq4F67FpMACqyQ1-doAFA.jpeg')\nchar_mask = np.array(Image.open(BytesIO(response.content)))\nimage_colors = ImageColorGenerator(char_mask)\n\nplt.figure(figsize = (20,20))\nwc = WordCloud(background_color=\"black\", max_words=200, width=400, height=400, mask=char_mask, random_state=1).generate(\" \".join(tweets.text))\n# to recolour the image\nplt.imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")","567c4bea":"sid = SentimentIntensityAnalyzer()\n\ntweets['sentiment_vader'] = tweets['text'].apply(lambda x: sid.polarity_scores(x)['compound'])\ntweets['sentiment_textblob'] = tweets['text'].apply(lambda x: TextBlob(x).sentiment.polarity)","5f1973d1":"tweets.sort_values(by = 'sentiment_textblob')[['user_name', 'text',\n                                               'sentiment_vader', 'sentiment_textblob']].head(15)","b892d078":"#Positive Tweets\ntweets.sort_values(by = 'sentiment_textblob', ascending = False)[['user_name', 'text', 'sentiment_vader', 'sentiment_textblob']].head(15)","adc30642":"#Neutral Tweets\ntweets[tweets['sentiment_textblob'] == 0.0][['user_name', 'text', 'sentiment_vader', 'sentiment_textblob']].head(15)","35a2832a":"#Combining all Dataframes (Positive, Neutral and Negative) and visualising the results...\nneutral = tweets[tweets['sentiment_textblob'] == 0.0]\npositive = tweets[tweets['sentiment_textblob'] > 0.0]\nnegative = tweets[tweets['sentiment_textblob'] < 0.0]\n\nneutral['Sentiment Category'] = 'Neutral'\npositive['Sentiment Category'] = 'Positive'\nnegative['Sentiment Category'] = 'Negative'\n\nframes = [neutral, positive, negative]\nresult = pd.concat(frames)","95b29176":"colors = ['gold', 'mediumturquoise', 'darkorange']\nfig = px.pie(result, values='count', names='Sentiment Category',\n             color_discrete_sequence=px.colors.sequential.RdBu,\n             title = 'Tweets Distribution Based on Sentiments')\nfig.update_traces(textposition='inside', textinfo='percent+label', textfont_size=20,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))\nfig.show()","5f6e12f4":"# Vocabulary","b5738dd4":"**Top 15 Regions : Tweet Counts**","d6260634":"**Coorelation Matrix**","21bd507c":"# Results","3ac1e9d1":"# Introduction","f137c424":"# Data Cleaning","c0a5130d":"**Negative Tweets: Top 15**","cf18d80b":"# The End","0aa0f399":"**Neutral Tweets: Top15**","01446ee0":"# WordCloud: Tweets","3ce4649a":"# Tweet Analysis ","549638dc":"**Most Used Words: Tweet**","9a3acb8f":"**Word Cloud (Hashtags)**","416c6528":"**Positive Tweets: Top 15**","bd9d9e9b":"**Tweet Counts Vs Verified User Tweets**","d1656ece":"# Exploratory Data Analysis","8c6e1400":"# Importing Libraries","628ccfdb":"**Timeline: Tweet Counts on 24hrs basis**","95df4707":"# Sentiment Analysis","4726378e":"**Top 10 Sources : To Do Tweets**","1241d2a4":"As per date (Aug 26, 2020):\n* Coronavirus Cases: 24,005,460\n* Deaths: 821,578\n* Recovered: 16,485,396\n\nStay Home, Stay Safe.\n\nWe will deep dive into the dataset and find interesting insights. Created visualisations using plotly library and did dataset manipulation using numpy package.\n\nTo Do:\n* Importing Libraries\n* EDA and Visualisations\n* Sentiment Analysis\n* Results\n\nHope you like it! <font color = \"red\">Please Upvote!<\/font>\n\n![](https:\/\/pbs.twimg.com\/media\/EQgP2pUW4AA0BkC?format=jpg&name=medium)","fa673a52":"# Dataset","d1339cb8":"**Adding Additional Columns**","d2a00e6d":"**Heatmap: Tweet Counts as per month and days**","09d2dfd5":"**Missing Values Visualisation**"}}