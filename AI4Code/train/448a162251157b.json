{"cell_type":{"b59c06d6":"code","3fe1b2c8":"code","9fbce3f8":"code","6bff95f5":"code","836f9729":"code","90d5a9fc":"code","e28eda58":"code","f8db0eb6":"code","4acbf5a5":"code","31fa4ba4":"code","9666ee5b":"code","66c9fb24":"code","59651c1a":"code","fd720b0e":"code","b8c5d636":"code","f0e5db44":"code","95685714":"code","b2260bc5":"code","4f568a5b":"code","f3304eb3":"code","ae121fbb":"markdown","a713df42":"markdown","91a599f8":"markdown","4242451b":"markdown","1780e271":"markdown","5add2404":"markdown","9cbd98c0":"markdown","4bd302c2":"markdown","3f06692e":"markdown","4c75df96":"markdown","28aee257":"markdown","20cc62fb":"markdown","479c68fb":"markdown","3e711b7e":"markdown","35b370d7":"markdown","ef93c290":"markdown","ef524a4e":"markdown","4b8ea1d6":"markdown","9a20105b":"markdown","3b4a0a60":"markdown","14c504c4":"markdown"},"source":{"b59c06d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 1000)\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nimport lightgbm as lgb\nimport plotly.figure_factory as ff\nimport gc\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder","3fe1b2c8":"%%time\nprint('Reading train.csv file....')\ntrain = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv')\nprint('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\nprint('Reading test.csv file....')\ntest = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv')\nprint('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\nprint('Reading train_labels.csv file....')\ntrain_labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\nprint('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\nprint('Reading specs.csv file....')\nspecs = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/specs.csv')\nprint('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\nprint('Reading sample_submission.csv file....')\nsample_submission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')\nprint('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))","9fbce3f8":"train_sample = train.sample(1000000)\ntrain_sample.head()","6bff95f5":"train_labels.head()","836f9729":"cnt_srs = train_labels['accuracy_group'].value_counts(normalize = True).sort_index()\ncnt_srs.index = ['Never Solved', '3 or More Attempts', 'Second Attempt', 'First Attempt']\ntrace = go.Bar(\n    x = cnt_srs.index,\n    y = cnt_srs.values,\n    marker = dict(\n        color = '#1E90FF',\n    ),\n)\n\nlayout = go.Layout(\n    title = go.layout.Title(\n        text = 'Distribution of Accuracy Group',\n        x = 0.5\n    ),\n    font = dict(size = 14),\n    width = 800,\n    height = 500,\n)\n\ndata = [trace]\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'accuracy_group')","90d5a9fc":"def bar_plot(df, column, title, width, height, n):\n    print('We have {} unique values'.format(df[column].nunique()))\n    cnt_srs = df[column].value_counts(normalize = True)[:n]\n    trace = go.Bar(\n        x = cnt_srs.index,\n        y = cnt_srs.values,\n        marker = dict(\n            color = '#1E90FF',\n        ),\n    )\n\n    layout = go.Layout(\n        title = go.layout.Title(\n            text = title,\n            x = 0.5\n        ),\n        font = dict(size = 14),\n        width = width,\n        height = height,\n    )\n\n    data = [trace]\n    fig = go.Figure(data = data, layout = layout)\n    py.iplot(fig, filename = 'bar_plot')\nbar_plot(train_labels, 'title', 'Assessment Title', 800, 500, 10)","e28eda58":"def get_time(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    return df\n    \ntrain_sample = get_time(train_sample)\ntest = get_time(test)","f8db0eb6":"output_notebook()\ndef scatter_plot(cnt_srs, color):\n    trace = go.Scatter(\n        x = cnt_srs.index,\n        y = cnt_srs.values,\n        showlegend = False,\n        marker = dict(\n            color = color,\n        )\n    )\n    return trace\n\n\ndef get_time_plots(df):\n    print('The dataset start on {} and ends on {}'.format(df['date'].min(), df['date'].max()))\n    cnt_srs = df['date'].value_counts().sort_index()\n    trace1 = scatter_plot(cnt_srs, 'red')\n    cnt_srs = df['month'].value_counts().sort_index()\n    trace2 = scatter_plot(cnt_srs, 'blue')\n    cnt_srs = df['hour'].value_counts().sort_index()\n    trace3 = scatter_plot(cnt_srs, 'green')\n    cnt_srs = df['dayofweek'].value_counts().sort_index()\n    trace4 = scatter_plot(cnt_srs, 'orange')\n    \n    subtitles = ['Date Frequency', 'Month Frequency', 'Hour Frequency', 'Day of Week Frequency']\n    \n    fig = subplots.make_subplots(rows = 4, cols = 1, vertical_spacing = 0.08, subplot_titles = subtitles)\n    fig.append_trace(trace1, 1, 1)\n    fig.append_trace(trace2, 2, 1)\n    fig.append_trace(trace3, 3, 1)\n    fig.append_trace(trace4, 4, 1)\n    fig['layout'].update(height = 1200, width = 1000, paper_bgcolor = 'rgb(233, 233, 233)')\n    py.iplot(fig, filename = 'time_plots')\nget_time_plots(train_sample)","4acbf5a5":"get_time_plots(test)","31fa4ba4":"bar_plot(train_labels, 'installation_id', 'Installation Id Distribution', 1000, 800, 100)","9666ee5b":"bar_plot(train_sample, 'event_count', 'Event Count Distribution', 1000, 800, 100)","66c9fb24":"def plot_hist(df, column, title, log = True):\n    df = df[[column]]\n    if log == True:\n        df[column] = np.log1p(df[column])\n    plt.figure(figsize = (10,8))\n    sns.distplot(df[column])\n    plt.title('{}'.format(title))\nplot_hist(train_sample, 'game_time', 'Game Time Distribution', log = True)","59651c1a":"# title\nbar_plot(train_sample, 'title', 'Title Distribution', 1000, 800, 100)","fd720b0e":"bar_plot(train_sample, 'type', 'Type Distribution', 800, 500, 100)","b8c5d636":"bar_plot(train_sample, 'world', 'World Distribution', 800, 500, 100)","f0e5db44":"# funtions to get agg stadistics and merge with test and train\ndef get_object_columns(df, columns):\n    df = df.groupby(['installation_id', columns])['event_id'].count().reset_index()\n    df = df.pivot_table(index = 'installation_id', columns = [columns], values = 'event_id')\n    df.columns = list(df.columns)\n    df.fillna(0, inplace = True)\n    return df\n\ndef get_numeric_columns(df, column):\n    df = df.groupby('installation_id').agg({f'{column}': ['mean', 'sum', 'std']})\n    df.fillna(0, inplace = True)\n    df.columns = [f'{column}_mean', f'{column}_sum', f'{column}_std']\n    return df\n\ndef get_numeric_columns_2(df, agg_column, column):\n    df = df.groupby(['installation_id', agg_column]).agg({f'{column}': ['mean', 'sum', 'std']}).reset_index()\n    df = df.pivot_table(index = 'installation_id', columns = [agg_column], values = [col for col in df.columns if col not in ['installation_id', 'type']])\n    df.fillna(0, inplace = True)\n    df.columns = list(df.columns)\n    return df\n\nnumerical_columns = ['game_time']\ncategorical_columns = ['type', 'world']\n\nreduce_train = pd.DataFrame({'installation_id': train['installation_id'].unique()})\nreduce_train.set_index('installation_id', inplace = True)\nreduce_test = pd.DataFrame({'installation_id': test['installation_id'].unique()})\nreduce_test.set_index('installation_id', inplace = True)\n\ntrain = get_time(train)\n\nfor i in numerical_columns:\n    reduce_train = reduce_train.merge(get_numeric_columns(train, i), left_index = True, right_index = True)\n    reduce_test = reduce_test.merge(get_numeric_columns(test, i), left_index = True, right_index = True)\n    \nfor i in categorical_columns:\n    reduce_train = reduce_train.merge(get_object_columns(train, i), left_index = True, right_index = True)\n    reduce_test = reduce_test.merge(get_object_columns(test, i), left_index = True, right_index = True)\n    \nfor i in categorical_columns:\n    for j in numerical_columns:\n        reduce_train = reduce_train.merge(get_numeric_columns_2(train, i, j), left_index = True, right_index = True)\n        reduce_test = reduce_test.merge(get_numeric_columns_2(test, i, j), left_index = True, right_index = True)\n    \n    \nreduce_train.reset_index(inplace = True)\nreduce_test.reset_index(inplace = True)\n    \nprint('Our training set have {} rows and {} columns'.format(reduce_train.shape[0], reduce_train.shape[1]))\n    \n# get the mode of the title\nlabels_map = dict(train_labels.groupby('title')['accuracy_group'].agg(lambda x:x.value_counts().index[0]))\n# merge target\nlabels = train_labels[['installation_id', 'title', 'accuracy_group']]\n# replace title with the mode\nlabels['title'] = labels['title'].map(labels_map)\n# get title from the test set\nreduce_test['title'] = test.groupby('installation_id').last()['title'].map(labels_map).reset_index(drop = True)\n# join train with labels\nreduce_train = labels.merge(reduce_train, on = 'installation_id', how = 'left')\nprint('We have {} training rows'.format(reduce_train.shape[0]))","95685714":"categoricals = ['title']\nreduce_train = reduce_train[['installation_id', 'game_time_mean', 'game_time_sum', 'game_time_std', 'Activity', 'Assessment', \n                             'Clip', 'Game', 'CRYSTALCAVES', 'MAGMAPEAK', 'NONE', 'TREETOPCITY', ('game_time', 'mean', 'Activity'),\n                             ('game_time', 'mean', 'Assessment'), ('game_time', 'mean', 'Clip'), ('game_time', 'mean', 'Game'), \n                             ('game_time', 'std', 'Activity'), ('game_time', 'std', 'Assessment'), ('game_time', 'std', 'Clip'), \n                             ('game_time', 'std', 'Game'), ('game_time', 'sum', 'Activity'), ('game_time', 'sum', 'Assessment'), \n                             ('game_time', 'sum', 'Clip'), ('game_time', 'sum', 'Game'), ('game_time', 'mean', 'CRYSTALCAVES'), \n                             ('game_time', 'mean', 'MAGMAPEAK'), ('game_time', 'mean', 'NONE'), ('game_time', 'mean', 'TREETOPCITY'), \n                             ('game_time', 'std', 'CRYSTALCAVES'), ('game_time', 'std', 'MAGMAPEAK'), ('game_time', 'std', 'NONE'), \n                             ('game_time', 'std', 'TREETOPCITY'), ('game_time', 'sum', 'CRYSTALCAVES'), \n                             ('game_time', 'sum', 'MAGMAPEAK'), ('game_time', 'sum', 'NONE'), ('game_time', 'sum', 'TREETOPCITY'), \n                             'title', 'accuracy_group']]","b2260bc5":"def run_lgb(reduce_train, reduce_test):\n    kf = KFold(n_splits=10)\n    features = [i for i in reduce_train.columns if i not in ['accuracy_group', 'installation_id']]\n    target = 'accuracy_group'\n    oof_pred = np.zeros((len(reduce_train), 4))\n    y_pred = np.zeros((len(reduce_test), 4))\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train)):\n        print('Fold {}'.format(fold + 1))\n        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature=categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature=categoricals)\n\n        params = {\n            'learning_rate': 0.01,\n            'metric': 'multiclass',\n            'objective': 'multiclass',\n            'num_classes': 4,\n            'feature_fraction': 0.75,\n            'subsample': 0.75\n        }\n\n        model = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n                          valid_sets=[train_set, val_set], verbose_eval = 100)\n        oof_pred[val_ind] = model.predict(x_val)\n        y_pred += model.predict(reduce_test[features]) \/ 10\n    return y_pred\ny_pred = run_lgb(reduce_train, reduce_test)","4f568a5b":"reduce_test = reduce_test.reset_index()\nreduce_test = reduce_test[['installation_id']]\nreduce_test['accuracy_group'] = y_pred.argmax(axis = 1)\nsample_submission.drop('accuracy_group', inplace = True, axis = 1)\nsample_submission = sample_submission.merge(reduce_test, on = 'installation_id')\nsample_submission.to_csv('submission.csv', index = False)","f3304eb3":"sample_submission['accuracy_group'].value_counts(normalize = True)","ae121fbb":"* Date frequency increase with time\n* Hour frequency is greater between 13 and 0\n* We have a greater frequency in Thursday and Friday","a713df42":"* Left skewd (log x + 1 to visualize better)","91a599f8":"* The first thing i notice is that we have 17k labels for 11M data points (train data). For the model we should aggregate by installation id and get agg stats.\n* It's going to be very important to engineer good features for the agg stats part.","4242451b":"# Target Variable\n\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n* 3: the assessment was solved on the first attempt\n* 2: the assessment was solved on the second attempt\n* 1: the assessment was solved after 3 or more attempts\n* 0: the assessment was never solved\n\nWe have a multiclass problem. Let's check the main files","1780e271":"Now, lets alaign our train and test set","5add2404":"* The two most common classes are fist attempt and never solved.\n* The majority class have a 50% of the observations\n* The columns num_correct, num_incorrect and accuracy are used to calculate the accuracy_group columns. ","9cbd98c0":"Let's explore the distribution of our target variable","4bd302c2":"Wow the dataset is huge, for exploration purpose let's extract a random sample","3f06692e":"Let's check installation id distribution","4c75df96":"# Model\n\nLet's try to build a baseline model. ","28aee257":"# Reading Files","20cc62fb":"* Date frequency have down and upper peaks but stay in the same line.\n* Month frequency behave similar to the train set (More observarions on August and September)\n* Hour frequency behave similar to the train set\n* Wednesday, Thursday, Friday and Sunday have hight frequencies.","479c68fb":"* The most common tittle is Chow Time followed by Sandcastle Builder (Activity)\n* Left skewd\n\nLet's check type column","3e711b7e":"* Left skewed\n* We have 3614 different installation ids (remeber it's a sample, all the train data have 17K)\n* Id 08987c08 have 0.88% of the observarions","35b370d7":"# Train and Test exploration","ef93c290":"# Files\n\n# Train and test\n\nThese are the main data files which contain the gameplay events.\n\n* event_id - Randomly generated unique identifier for the event type. Maps to event_id column in specs table.\n* game_session - Randomly generated unique identifier grouping events within a single game or video play session.\n* timestamp - Client-generated datetime\n* event_data - Semi-structured JSON formatted string containing the events parameters. Default fields are: event_count, event_code, and game_time; otherwise fields are determined by the event type.\n* installation_id - Randomly generated unique identifier grouping game sessions within a single installed application instance.\n* event_count - Incremental counter of events within a game session (offset at 1). Extracted from event_data.\n* event_code - Identifier of the event 'class'. Unique per game, but may be duplicated across games. E.g. event code '2000' always identifies the 'Start Game' event for all games. Extracted from event_data.\n* game_time - Time in milliseconds since the start of the game session. Extracted from event_data.\n* title - Title of the game or video.\n* type - Media type of the game or video. Possible values are: 'Game', 'Assessment', 'Activity', 'Clip'.\n* world - The section of the application the game or video belongs to. Helpful to identify the educational curriculum goals of the media. Possible values are: 'NONE' (at the app's start screen), TREETOPCITY' (Length\/Height), 'MAGMAPEAK' (Capacity\/Displacement), 'CRYSTALCAVES' (Weight).\n\n# Specs\n\nThis file gives the specification of the various event types.\n\n* event_id - Global unique identifier for the event type. Joins to event_id column in events table.\n* info - Description of the event.\n* args - JSON formatted string of event arguments. Each argument contains:\n* name - Argument name.\n* type - Type of the argument (string, int, number, object, array).\n* info - Description of the argument.\n\n\n# Train_labels\n* This file demonstrates how to compute the ground truth for the assessments in the training set.\n\n# Sample_submission\n* A sample submission in the correct format.\n\nSo we have 2 files that have a lot of information that can be helpfull for predicting accuracy_group. Let's start with the train and test files.","ef524a4e":"Let's check the event count distribution.","4b8ea1d6":"* We have 2205 unique values\n* Left skewed\n* Most common value is 1 with 2.66% of the observarions","9a20105b":"1. * Let's explore the timestamp column","3b4a0a60":"* MAGMAPEAK have 44.3% of the observarions\n* NONE have almost 0% observations","14c504c4":"* Game is the most common type followed by Activity\n* Clip is only 1.6% of the observations\n\nLet's check out world"}}