{"cell_type":{"819ef896":"code","7b0c7d02":"code","e230ff7e":"code","129d8cdf":"code","0671b0be":"code","8c638b37":"code","18c443fe":"code","8555dd6f":"code","89ab0348":"code","45aaac9b":"code","598a5e2e":"code","f1d18bd9":"code","b836c4d0":"code","4753e0f7":"code","52ea4ae5":"code","7e1040db":"code","42170999":"code","7cc193cb":"code","b83cce48":"markdown","c69a9c7d":"markdown","70db5a28":"markdown","2b3fb3b3":"markdown","0b590e7a":"markdown","907ece84":"markdown","69b9e05f":"markdown","75dcca2f":"markdown","2baec367":"markdown","5ee9e44c":"markdown","ec04cb9f":"markdown","fe93d8e7":"markdown","8cfb6d28":"markdown","bff469ca":"markdown","cb6f5c04":"markdown","18de02f9":"markdown","4a6722b7":"markdown","e8368489":"markdown","1e1c1286":"markdown","c8f388ae":"markdown","7a7d6ccb":"markdown","2607deba":"markdown","cf7ef04e":"markdown","c48c51b3":"markdown","d4b313e2":"markdown"},"source":{"819ef896":"import numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","7b0c7d02":"# Loading the data\n\ndef load_data():\n    from sklearn.datasets import load_boston\n    from sklearn.model_selection import train_test_split\n    \n    boston = load_boston()\n    \n    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(boston.data, boston.target, test_size=0.33, random_state=42)\n\n    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))\n    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))\n    \n    return train_set_x.T, train_set_y, test_set_x.T, test_set_y, boston\n\ntrain_set_x, train_set_y, test_set_x, test_set_y, visualization_set = load_data()","e230ff7e":"print(train_set_x.shape, train_set_y.shape, test_set_x.shape, test_set_y.shape)","129d8cdf":"### START CODE HERE ### (\u2248 2 lines of code)\nm_train = train_set_x.shape[1]\nm_test = test_set_x.shape[1]\n### END CODE HERE ###\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\n\nprint (\"\\ntrain_set_x shape: \" + str(train_set_x.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))","0671b0be":"plt.figure(figsize=(4, 3))\nplt.hist(visualization_set.target)\nplt.xlabel(\"Price ($1000s)\")\nplt.ylabel(\"Count\")\nplt.tight_layout()","8c638b37":"for index, feature_name in enumerate(visualization_set.feature_names):\n    plt.figure(figsize=(4, 3))\n    plt.scatter(visualization_set.data[:, index], visualization_set.target)\n    plt.ylabel(\"Price\", size=15)\n    plt.xlabel(feature_name, size=15)\n    plt.tight_layout()","18c443fe":"all_set_x = np.concatenate([train_set_x, test_set_x], axis=1)\n\nmean = all_set_x.mean(axis=1, keepdims=True)\nstd = all_set_x.std(axis=1, keepdims=True)\n\ntrain_set_x = (train_set_x - mean) \/ std\ntest_set_x = (test_set_x - mean) \/ std","8555dd6f":"# GRADED FUNCTION: initialize_with_zeros\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias)\n    \"\"\"\n    \n    ### START CODE HERE ### (\u2248 2 lines of code)\n    w = np.zeros((dim,1), dtype=float)\n    b = 0\n    ### END CODE HERE ###\n\n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b","89ab0348":"dim = 2\nw, b = initialize_with_zeros(dim)\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))","45aaac9b":"def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (number of features, 1)\n    b -- bias, a scalar\n    X -- data of shape (number of features, number of examples)\n    Y -- results of shape (1, number of examples)\n    \n    Return:\n    cost -- cost function for linear regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \n    Tips:\n    - Write your code step by step for the propagation.\n    - Use np.dot() to avoid for-loops in favor of code vectorization\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### (\u2248 2 lines of code)\n    H = np.dot(np.transpose(w),X) + b    # compute activation\n    cost =1\/2\/m*np.sum(np.dot(np.transpose(H-Y),(H-Y)))  # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### (\u2248 2 lines of code)\n    dw = 1\/m*np.dot(X,np.transpose((H-Y)))\n    db = 1\/m*np.sum(H-Y)\n    ### END CODE HERE ###\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost","598a5e2e":"w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))","f1d18bd9":"# GRADED FUNCTION: optimize\n\ndef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (number of features, 1)\n    b -- bias, a scalar\n    X -- data of shape (number of features, number of examples)\n    Y -- results of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        # Cost and gradient calculation (\u2248 1 line of code)\n        ### START CODE HERE ### \n        grads, cost = propagate(w, b, X, Y)\n        ### END CODE HERE ###\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (\u2248 2 lines of code)\n        ### START CODE HERE ###\n        w = w-learning_rate*grads[\"dw\"]\n        b = b-learning_rate*grads[\"db\"]\n        ### END CODE HERE ###\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","b836c4d0":"params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))","4753e0f7":"# GRADED FUNCTION: predict\n\ndef predict(w, b, X):\n    \"\"\"\n    Predict using learned linear regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (number of features, 1)\n    b -- bias, a scalar\n    X -- data of shape (number of features, number of examples)\n    \n    Returns:\n    H -- a numpy array (vector) containing all predictions for the examples in X\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # Compute vector \"H\"\n    ### START CODE HERE ### (\u2248 1 line of code)\n    H = np.dot(np.transpose(w),X)+b\n    ### END CODE HERE ###\n    \n    assert(H.shape == (1, m))\n    \n    return H","52ea4ae5":"w = np.array([[0.1124579],[0.23106775]])\nb = -0.3\nX = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\nprint (\"predictions = \" + str(predict(w, b, X)))","7e1040db":"# GRADED FUNCTION: model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations=3000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the linear regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (number of features, m_train)\n    Y_train -- training values represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (number of features, m_test)\n    Y_test -- test values represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # initialize parameters with zeros (\u2248 1 line of code)\n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    # Gradient descent (\u2248 1 line of code)\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=False)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test\/train set examples (\u2248 2 lines of code)\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    ### END CODE HERE ###\n\n    # Print train\/test Errors\n    print (\"Train RMSE: {} \".format(np.sqrt(np.mean((Y_prediction_train - Y_train) ** 2))))\n    print (\"Test RMSE: {} \".format(np.sqrt(np.mean((Y_prediction_test - Y_test) ** 2))))\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d","42170999":"d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=3000, learning_rate=0.05, print_cost=True)","7cc193cb":"# Training set\nplt.figure(figsize=(4, 3))\nplt.title(\"Training set\")\nplt.scatter(train_set_y, d[\"Y_prediction_train\"])\nplt.plot([0, 50], [0, 50], \"--k\")\nplt.axis(\"tight\")\nplt.xlabel(\"True price ($1000s)\")\nplt.ylabel(\"Predicted price ($1000s)\")\nplt.tight_layout()\n\n# Test set\nplt.figure(figsize=(4, 3))\nplt.title(\"Test set\")\nplt.scatter(test_set_y, d[\"Y_prediction_test\"])\nplt.plot([0, 50], [0, 50], \"--k\")\nplt.axis(\"tight\")\nplt.xlabel(\"True price ($1000s)\")\nplt.ylabel(\"Predicted price ($1000s)\")\nplt.tight_layout()","b83cce48":"And it is very useful to understand the join histogram for each feature","c69a9c7d":"**Exercise:** The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There is only one step to computing predictions:\n\nCalculate $H = w^T X + b$","70db5a28":"<font color='green'>\n**What to remember:**\nYou've implemented several functions that:\n- Initialize (w,b)\n- Optimize the loss iteratively to learn parameters (w,b):\n    - computing the cost and its gradient \n    - updating the parameters using gradient descent\n- Use the learned (w,b) to predict the value for a given set of examples","2b3fb3b3":"**Expected Output for m_train, m_test**: \n<table>\n  <tr>\n    <td>**m_train**<\/td>\n    <td> 339 <\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**m_test**<\/td>\n    <td> 167 <\/td> \n  <\/tr>\n<\/table>\n","0b590e7a":"**Expected Output**:\n\n<table>\n    <tr>\n        <td>  ** dw **  <\/td>\n      <td> [[ 12.8]\n     [ 30.82666667]]<\/td>\n    <\/tr>\n    <tr>\n        <td>  ** db **  <\/td>\n        <td> 4.533333333333333 <\/td>\n    <\/tr>\n    <tr>\n        <td>  ** cost **  <\/td>\n        <td> 41.49333333333333 <\/td>\n    <\/tr>\n\n<\/table>","907ece84":"Many software bugs in machine learning come from having matrix\/vector dimensions that don't fit. If you can keep your matrix\/vector dimensions straight you will go a long way toward eliminating many bugs. \n\n**Exercise:** Find the values for:\n    - m_train (number of training examples)\n    - m_test (number of test examples)\nRemember that `train_set_x` is a numpy-array of shape (number of features, number of examples). For instance, you can access `m_train` by writing `train_set_x.shape[1]`.","69b9e05f":"### 4.2 - Forward and Backward propagation\n\nNow that your parameters are initialized, you can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n\n**Exercise:** Implement a function `propagate()` that computes the cost function and its gradient.\n\n**Hints**:\n\nForward Propagation:\n- You get X\n- You compute $H = (w^T X + b) = (h^{(1)}, h^{(2)}, ..., h^{(m-1)}, h^{(m)})$\n- You calculate the cost function: $J = \\frac{1}{2m}\\sum_{i=1}^{m}(h^{(i)} - y^{(i)})^{2}$\n\n\nHere is the formula of gradient of the cost function: \n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(H-Y)^T\\tag{3}$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (h^{(i)}-y^{(i)})\\tag{4}$$\n","75dcca2f":"## 2 - Overview of the Dataset ##\n\n**Problem Statement**: You are given a dataset  containing:\n    - a training set of m_train examples\n    - a test set of m_test examples\n    - each example is of shape (number of features, 1)\n\n\nBoston House Prices dataset\n===========================\n\nNotes\n------\nData Set Characteristics:  \n\n    :Number of Instances: 506 \n\n    :Number of Attributes: 13 numeric\/categorical predictive\n    \n    :Median Value (attribute 14) is usually the target\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n    - PTRATIO  pupil-teacher ratio by town\n    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n    - LSTAT    % lower status of the population\n    - MEDV     Median value of owner-occupied homes in $1000's\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttps:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/housing\/\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.   \n     \n**References**\n\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n   - many more! (see https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/housing\/)\n\n\n\n\n\n<b>Let's get more familiar with the dataset. Load the data by running the following code.<\/b>\n","2baec367":"**Expected Output**: \n\n<table>\n    <tr>\n         <td>\n             **predictions**\n         <\/td>\n          <td>\n            [[ 0.0897392   0.03843181 -0.6367585]]\n         <\/td>  \n   <\/tr>\n\n<\/table>\n","5ee9e44c":"# Linear Regression with multiple variables\n\nWelcome to your second lab! You will build more advanced linear regression algorithm capable of handling any amount of features.\n\nYou will be predicting house prices given Boston house prices dataset.\n\n**Instructions:**\n- Do not use loops (for\/while) in your code, unless the instructions explicitly ask you to do so.\n\n**You will learn to:**\n- Build the general architecture of a learning algorithm, including:\n    - Initializing parameters\n    - Calculating the cost function and its gradient\n    - Using an optimization algorithm (gradient descent) \n- Gather all three functions above into a main model function, in the right order.","ec04cb9f":"**Expected Output**: \n\n<table> \n    <tr>\n        <td> **Cost after iteration 0 **  <\/td> \n        <td> 307.900929 <\/td>\n    <\/tr>\n      <tr>\n        <td> <center> $\\vdots$ <\/center> <\/td> \n        <td> <center> $\\vdots$ <\/center> <\/td> \n    <\/tr>  \n    <tr>\n        <td> **Train RMSE**  <\/td> \n        <td> 4.7941103172540895 <\/td>\n    <\/tr>\n    <tr>\n        <td>**Test RMSE** <\/td> \n        <td> 4.5549106456768715 <\/td>\n    <\/tr>\n<\/table> ","fe93d8e7":"### Data visualization","8cfb6d28":"Run the following cell to train your model.","bff469ca":"### 4.1 - Initializing parameters\n\n**Exercise:** Implement parameter initialization in the cell below. You have to initialize `w` as a vector of zeros. If you don't know what numpy function to use, look up `np.zeros()` in the Numpy library's documentation.","cb6f5c04":"Let's plot a histogram of the quantity we want to predict: namely, the house `price`.","18de02f9":"## 5 - Merge all functions into a model ##\n\nYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n\n**Exercise:** Implement the model function. Use the following notation:\n    - Y_prediction_test for your predictions on the test set\n    - Y_prediction_train for your predictions on the train set\n    - w, costs, grads for the outputs of optimize()","4a6722b7":"**Expected Output**: \n\n\n<table>\n    <tr>\n        <td>  ** w **  <\/td>\n        <td> [[ 0.]\n [ 0.]] <\/td>\n    <\/tr>\n    <tr>\n        <td>  ** b **  <\/td>\n        <td> 0 <\/td>\n    <\/tr>\n<\/table>","e8368489":"**Interpretation**:\nYou can see that in fact there is nice linear dependecy between predicted and true values.","1e1c1286":"## 3 - General Architecture of the learning algorithm ##\n\n**Mathematical expression of the algorithm**:\n\n\nFor one example $x^{(i)}$:\n$$h^{(i)} =  w^T x^{(i)} + b \\tag{1}$$\n\nThe cost is then computed by summing squared diff over all training examples:\n$$J = \\frac{1}{2m}\\sum_{i=1}^{m}(h^{(i)} - y^{(i)})^{2}\\tag{2}$$\n\n**Key steps**:\nIn this exercise, you will carry out the following steps: \n    - Initialize the parameters of the model\n    - Learn the parameters for the model by minimizing the cost  \n    - Use the learned parameters to make predictions (on the test set)\n    - Analyse the results and derive a conclusion","c8f388ae":"## 4 - Building the parts of our algorithm ## \n\nThe main steps for building a learning algorithm:\n1. Define the model structure (such as number of input features) \n2. Initialize the model's parameters\n3. Loop:\n    - Calculate current loss (forward propagation)\n    - Calculate current gradient (backward propagation)\n    - Update parameters (gradient descent)\n\nYou often build 1-3 separately and integrate them into one function we call `model()`.","7a7d6ccb":"### Standardization\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array.\n\n$$X_{new} = \\frac{X - \\mu}{\\sigma}$$\n\nLet's standardize our dataset.","2607deba":"## 1 - Packages ##\n\nFirst, let's run the cell below to import all the packages that you will need during this assignment.\n- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n- [matplotlib](http:\/\/matplotlib.org) is a famous library to plot graphs in Python.","cf7ef04e":"### Predicted vs True visualization ","c48c51b3":"**Expected Output**: \n\n<table>\n    <tr>\n       <td> **w** <\/td>\n       <td>[[-0.04675219]\n [-0.12676061]] <\/td>\n    <\/tr>\n    \n    <tr>\n       <td> **b** <\/td>\n       <td> 1.223758731602527 <\/td>\n    <\/tr>\n    <tr>\n       <td> **dw** <\/td>\n       <td> [[ 0.12274692]\n [-0.09406359]] <\/td>\n    <\/tr>\n    <tr>\n       <td> **db** <\/td>\n       <td> 0.36833971156600487 <\/td>\n    <\/tr>\n\n<\/table>","d4b313e2":"### 4.3 - Optimization\n- You have initialized your parameters.\n- You are also able to compute a cost function and its gradient.\n- Now, you want to update the parameters using gradient descent.\n\n**Exercise:** Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } \\partial\\theta$, where $\\alpha$ is the learning rate."}}