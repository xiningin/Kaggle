{"cell_type":{"cad4e4bf":"code","09b9c722":"code","96bf9738":"code","e190bbd3":"code","bfe26337":"code","290f6c9b":"code","24e532f9":"code","6a8b1b6e":"code","e5a62996":"code","fa24a859":"code","e1755ecd":"code","f0777b62":"code","f75cecaf":"code","c9b7af7e":"code","7d417234":"code","fbbcc6e1":"code","1642e59f":"code","6da17c40":"code","6ba995e5":"code","ae10ef71":"code","4dffd9c1":"code","c84d62e8":"code","c845b5ff":"code","1ce7b21c":"code","f8429b61":"code","e0ac644a":"code","0c70cc4a":"code","7e8ec009":"code","a479bbb7":"code","5bc0617b":"code","48c2a24d":"code","f5c9a70f":"code","5ef1ad42":"code","db40238f":"code","3d833908":"code","5d320d5e":"code","40d96a41":"code","19c4d741":"code","12aa0afe":"code","09f2723a":"code","6aad34a5":"code","a7b58781":"code","5afeb1e8":"code","8069c08f":"code","69cf8677":"code","3a156378":"code","daddca05":"code","7b83540a":"markdown","dd4e54a8":"markdown","4515e760":"markdown","dbded6f4":"markdown","c26fc236":"markdown","63535238":"markdown","012c7d82":"markdown","1a5c4f45":"markdown","12778ee9":"markdown","e4e5a199":"markdown","6cdb2f47":"markdown","d947f391":"markdown","518d4efe":"markdown","a82607fc":"markdown","3136cf37":"markdown","fba7b7e0":"markdown","824b5d41":"markdown","be43d917":"markdown","8e5aa35c":"markdown","f0c4edff":"markdown","00b443c9":"markdown","b939f589":"markdown","fbacdeed":"markdown","951f7b66":"markdown","6a670523":"markdown","1eafef4c":"markdown","1c862a11":"markdown","ec99b3d2":"markdown","3a74e1c8":"markdown","cfea9c95":"markdown","0cf8a308":"markdown","9146aa6d":"markdown","b60e97a1":"markdown","7e8134aa":"markdown","6af2aa29":"markdown","8bb3d675":"markdown","6accebea":"markdown","aa2c119a":"markdown","6a80f7a6":"markdown","710071ff":"markdown","9666ac00":"markdown","a1c3187d":"markdown","ca0760c9":"markdown","cbbc1fa3":"markdown","b8fd6e05":"markdown","e1675d96":"markdown","7a5e7cad":"markdown","078a1b84":"markdown","67dce89d":"markdown","4942b5b4":"markdown"},"source":{"cad4e4bf":"!pip install pyspark","09b9c722":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nimport pyspark # only run after findspark.init()\nfrom pyspark.sql import SparkSession\n# May take awhile locally\nspark = SparkSession.builder.appName(\"Pyspark_2\").getOrCreate()\n\ncores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\nprint(\"You are working with\", cores, \"core(s)\")\nspark","96bf9738":"path =\"\"\n\n# Some csv data\nwine_quality = spark.read.csv(path+'\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv',\n                          inferSchema=True,header=True)","e190bbd3":"wine_quality.limit(5).toPandas()","bfe26337":"wine_quality.printSchema()","290f6c9b":"wine_quality.withColumn(\"sq_chlorides\", wine_quality.chlorides**2)","24e532f9":"%%timeit\nwine_quality_v1 = wine_quality.withColumn(\"sq_chlorides\", wine_quality.chlorides**2)","6a8b1b6e":"wine_quality.limit(5).toPandas()","e5a62996":"from pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n\ndef square(x):\n    return int(x**2)\nsquare_udf = udf(lambda z: square(z), IntegerType())","fa24a859":"%%timeit\nwine_quality.withColumn(\"fast_sq_chlorides\", square_udf(wine_quality.chlorides))","e1755ecd":"from pyspark.sql.functions import * \nfrom pyspark.sql.types import * # IntegerType\n\ndf = wine_quality.withColumn(\"quality\", wine_quality[\"quality\"].cast(DoubleType())) \\\n        .withColumn(\"density\", wine_quality[\"density\"].cast(IntegerType())) \n        \nprint(df.printSchema())\n","f0777b62":"df.withColumnRenamed('density','density_new_name')","f75cecaf":"df.drop(\"quality\")","c9b7af7e":"# Adding a text column to run some text manipulation functions on.\ndf1 = df.withColumn(\"text_col\", lit(\"    TEST TEXT   \"))","7d417234":"df1 = df.withColumn(\"text_col\", lit(\"    TEST TEXT   \"))\ndf1.select(\"text_col\").show(5,False)","fbbcc6e1":"df = df1.withColumn('text_col',trim(df1.text_col)) \ndf.select(\"text_col\").show(5,False)","1642e59f":"df1 = df.withColumn('text_col',lower(df.text_col)) \ndf1.select(\"text_col\").show(5,False)","6da17c40":"df1.select(split(df1.text_col, ' ').alias('split_text_col')).show(1,False)","6ba995e5":"df1.select('text_col',regexp_replace(df1.text_col, 'test', 'replaced').alias('regex_replaced_text')).show(1, False)","ae10ef71":"print(\"Option#1: select or withColumn() using when-otherwise without Otherwise condition\")\nfrom pyspark.sql.functions import when\ndf_subset = df.select(\"pH\",(when(df.pH > 3, 'Good')).alias(\"Acidity Category\"))\n\ndf_subset.select([count(when(col(c).contains('None') | \\\n                            col(c).contains('NULL') | \\\n                            (col(c) == '' ) | \\\n                            col(c).isNull() | \\\n                            isnan(c), c \n                           )).alias(c)\n                    for c in df_subset.columns]).show()","4dffd9c1":"print(\"Option#1: select or withColumn() using when-otherwise with Otherwise condition\")\n\ndf_subset_2 = df.select(\"pH\",(when(df.pH >= 3.0, 'Good').otherwise('Bad')).alias(\"Acidity Category\"))\n\ndf_subset_2.select([count(when(col(c).contains('None') | \\\n                            col(c).contains('NULL') | \\\n                            (col(c) == '' ) | \\\n                            col(c).isNull() | \\\n                            isnan(c), c \n                           )).alias(c)\n                    for c in df_subset_2.columns]).show()","c84d62e8":"print(\"Option2: select or withColumn() using expr function\")\nfrom pyspark.sql.functions import expr \ndf.select(\"pH\",expr(\"CASE WHEN pH >= 3.0 THEN 'Good' ELSE 'Bad' END\").alias('Acidity Category')).show(3)","c845b5ff":"print(\"Option 3: selectExpr() using SQL equivalent CASE expression\")\ndf.selectExpr(\"pH\",\"CASE WHEN pH >= 3.0 THEN  'Good' ELSE 'Bad' END AS Acidity_Category\").show(3)","1ce7b21c":"org_df = spark.createDataFrame([('Apple','Steve Jobs')], ['Organisation', 'Founder'])\nprint(org_df.show())\nprint(org_df.rdd.id())","f8429b61":"org_df = org_df.select(org_df.Organisation,org_df.Founder,concat_ws('-', org_df.Organisation, org_df.Founder).alias('concat'))","e0ac644a":"print(org_df.show())\nprint(org_df.rdd.id())","0c70cc4a":"wine_quality.groupBy(\"quality\").count().show()","7e8ec009":"wine_quality.groupBy(\"quality\").mean(\"density\").show()","a479bbb7":"wine_quality.select(countDistinct(\"quality\").alias('CountDistinct_quality')).show()","5bc0617b":"wine_quality.groupBy(\"quality\").agg(round(min(wine_quality.chlorides),3).alias(\"Min chlorides\"),max(wine_quality.chlorides).alias(\"Max chlorides\")).show()","48c2a24d":"wine_quality.agg(min(wine_quality.density).alias(\"Min density\"),max(wine_quality.density).alias(\"Max density\")).show()","f5c9a70f":"wine_quality.toPandas().head()","5ef1ad42":"# creating a categorical to check effect of pH on quality\nwine_quality_v2 = wine_quality.select('*', (when(df.pH >= 3.0, 'pH>=3').otherwise('pH<3')).alias(\"ph_category\"))","db40238f":"wine_quality_v2.groupBy(\"quality\").pivot(\"ph_category\").count().show(10)","3d833908":"wine_quality.groupBy(\"quality\").pivot(\"quality\").count().show(10)","5d320d5e":"wine_quality.groupBy(\"quality\").pivot(\"free sulfur dioxide\", [11,25]).count().show(10)","40d96a41":"wine_quality_v2.groupBy(\"quality\").pivot(\"ph_category\").agg(mean(wine_quality_v2.chlorides).alias(\"Mean Chlorides\"),mean(wine_quality_v2[\"free sulfur dioxide\"]).alias(\"Mean free sulfur dioxide\")).toPandas()#.show()","19c4d741":"valuesP = [('koala',1,'yes'),('caterpillar',2,'yes'),('deer',3,'yes'),('human',4,'yes')]\neats_plants = spark.createDataFrame(valuesP,['name','id','eats_plants'])\n\nvaluesM = [('shark',5,'yes'),('lion',6,'yes'),('tiger',7,'yes'),('human',4,'yes')]\neats_meat = spark.createDataFrame(valuesM,['name','id','eats_meat'])\n\nprint(\"Plant eaters (herbivores)\")\nprint(eats_plants.show())\nprint(\"Meat eaters (carnivores)\")\nprint(eats_meat.show())","12aa0afe":"inner_join = eats_plants.join(eats_meat, [\"name\",\"id\"],\"inner\")\nprint(\"Inner Join Example\")\nprint(inner_join.show())","09f2723a":"left_join = eats_plants.join(eats_meat, [\"name\",\"id\"], how='left')\nprint(\"Left Join Example\")\nprint(left_join.show())","6aad34a5":"right_join = eats_plants.join(eats_meat,  [\"name\",\"id\"],how='right')\nprint(\"Right Join\")\nprint(right_join.show())","a7b58781":"full_outer_join = eats_plants.join(eats_meat, [\"name\",\"id\"],how='full') # Could also use 'full_outer'\nprint(\"Full Outer Join\")\nprint(full_outer_join.show())","5afeb1e8":"new_df = eats_plants\n\ndf_concat = eats_plants.union(new_df)\nprint((\"eats_plants df Counts:\", eats_plants.count(), len(eats_plants.columns)))\nprint((\"df_concat Counts:\", df_concat.count(), len(df_concat.columns)))\nprint(eats_plants.show())\nprint(df_concat.show())","8069c08f":"import pyspark.sql.functions as F\n# creating different name for the same column\neats_plants = eats_plants.withColumnRenamed(\"name\", \"name_1\")\neats_plants.join(eats_meat, eats_plants['name_1'] == eats_meat['name'], how='outer')\\\n   .show()","69cf8677":"cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\ncDf.show()","3a156378":"cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()","daddca05":"eats_plants.join(eats_meat, eats_plants['name_1'] == eats_meat['name'], how='outer')\\\n   .select('*', F.coalesce(F.col('name_1'), F.col('name')).alias('name_coalesced')) \\\n   .drop('name_1', 'name') \\\n   .show()","7b83540a":"# Aggregating DataFrames in PySpark","dd4e54a8":"#### What does coalesce do?\nReturns the first column that is not null\n\nReference: https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.sql.functions.coalesce.html","4515e760":"* https:\/\/www.educba.com\/pyspark-withcolumn\/\n* https:\/\/medium.com\/@manuzhang\/the-hidden-cost-of-spark-withcolumn-8ffea517c015\n* https:\/\/sparkbyexamples.com\/pyspark\/pyspark-withcolumn\/\n* https:\/\/sparkbyexamples.com\/pyspark\/pyspark-find-count-of-null-none-nan-values\/\n* https:\/\/stackoverflow.com\/questions\/53374140\/if-dataframes-in-spark-are-immutable-why-are-we-able-to-modify-it-with-operatio","dbded6f4":"# Coalesce in Joins","c26fc236":"# Creating New Columns and Features","63535238":"# Renaming Columns","012c7d82":"# Aggregation Without Groupby","1a5c4f45":"#### Suppose we want to have a feature that is the square of the chlorides feature by the name 'sq_chlorides'. This can be accomplished with the withColumn() method of PySpark DataFrame.","12778ee9":"**Lower**\n\n Lower casing all values in a string.","e4e5a199":"To limit the pivot method to create cross-tab for only specific entries of the column passed to it, another argument can be provided to it which should be the list of all distinct values to use for the cross-tab.","6cdb2f47":"The pivot method will create cross-tab with all the distinct values that the column passed to it consists of.","d947f391":"We can see the new column got created and also the amount of time it took for the whole operation however there's a very interesting possibility of modification in the way we just created this feature. Let's check that out.","518d4efe":"## References","a82607fc":"# Data Manipulation Methods","3136cf37":"# Condition Based Feature Creation","fba7b7e0":"# Joins","824b5d41":"# Left Join\n\nLeft joins gives the rows and columns that appear in the left table and only the rows from the right table which are successfully mapped to the table on the left along with the columns. A quick quality check we could do would be to make sure that the human column has the value \"yes\" for both eats_plants and eats_meat columns.","be43d917":"## Full Outer Join\n\nFull outer joins will get all values from both tables and columns from both.","8e5aa35c":"# Coalesce","f0c4edff":"#### As per Spark Architecture DataFrame is built on top of RDDs which are immutable in nature therefore Data frames are immutable in nature.","00b443c9":"# User Defined Functions","b939f589":"The withColumnRenamed() method takes the old column name as the first argument and the new column name as the second argument in the call. It returns a new DataFrame with the updates.","fbacdeed":"# Right Join\n\nA right join gets the values that appear in the right table but not in the left. It also brings it's columns over. ","951f7b66":"#### So even though the variable which holds the dataframe is same but the unique rdd id is different after the update. This is because the old dataframe is not being referenced to anymore.","6a670523":"Distinct count method","1eafef4c":"# PySpark DF Immutability","1c862a11":"Therefore calling withColumns() function too many times may cause the whole process to become expensive especially in terms of time.\n\n> This method introduces a projection internally. Therefore, calling it multiple times, for instance, via loops in order to add multiple columns can generate big plans which can cause performance issues and even StackOverflowException. To avoid this, use select() with the multiple columns at once.\n\nhttps:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.sql.DataFrame.withColumn.html","ec99b3d2":"# Aggregation + Pivot","3a74e1c8":"\n# Changing data types after read in\nIn the last notebook we had covered reading a data source in the specific format that we'd like to see it in.\n\nNow we consider the situation were we need to change the data type of a specific column after the data source has already been read.\n\n#### Available types:\n    - DataType\n    - NullType\n    - StringType\n    - BinaryType\n    - BooleanType\n    - DateType\n    - TimestampType\n    - DecimalType\n    - DoubleType\n    - FloatType\n    - ByteType\n    - IntegerType\n    - LongType\n    - ShortType\n    - ArrayType\n    - MapType\n    - StructField\n    - StructType\n    ","cfea9c95":"# Multiple Aggregations","0cf8a308":"\nRegarding the withColumn() or any other similar operations when applied, such operations will generate a new data frame instead of updating the existing data frame.","9146aa6d":"With the withColumn() function the first value we pass in is the name of the new column and the second calls on the existing dataframe column name we want to use or any other expression that returns valid value for the new column being generated.","b60e97a1":"# Union","7e8134aa":"Spark get its great advantage from the idea of operating on distributed workers\/executors that execute a subset of the bigger task and the final results are combined to form the final outcome.\n\nWhen working on operations in DataFrames on PySpark if we define a function in the traditional Pythonic manner the function would still execute and the output would be the same. However the time taken for the execution of the operation would be much more than what we would expect from a map and reduce framework based engine. \n\nTherefore to still be able to leverage the fast execution offered by Spark we need to create what are called User Defined Functions in PySpark.","6af2aa29":"#### Most operations of the Dataframe will generate another dataframe and assign it to reference variable in case you have the assignment statement.\n\n#### In order to verify the same, you can use id() method of rdd to get the unique identifier of your dataframe.","8bb3d675":"# Inner Join\n\nInner joins get us ONLY the values that appear in BOTH tables we are joining. ","6accebea":"## Thanks and continue to follow for the next update in the same series!","aa2c119a":"# Pivots","6a80f7a6":"This method returns a new DataFrame with the remaining columns.","710071ff":"**Split a string around a pattern**","9666ac00":"This is similar to value_counts() method in pandas which gives out a frequency table.","a1c3187d":"# Join with Different Column Names","ca0760c9":"#### This works well but has some extra duplicate columns common between the two tables that can now be dropped after the mapping is complete.","cbbc1fa3":"We can see the schema of the DataFrame called df contains datatypes that we have defined by casting for the two columns. The important method to focus on here is the cast() method which can be called on a column of the dataframe and takes in datatype input available in pyspark.sql.types.","b8fd6e05":"**Trim**\n\nThe \"trim\" function removes leading and trailing white space from a cell.","e1675d96":"# Dropping a Column","7a5e7cad":"We can see how this was able to get rid of extra columns in the data after the merge.","078a1b84":"**Regex to replace text**\n","67dce89d":"Multiple column aggregations in a single go","4942b5b4":"#### We can see with the notebook magic function timeit that even with a single core Spark's UDFs are much faster."}}