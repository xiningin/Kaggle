{"cell_type":{"d05daeea":"code","091b8258":"code","e877d992":"code","a774e62a":"code","4582ac0c":"code","f43b0c77":"code","2e8955c3":"code","c9b30a0f":"code","1da2fccc":"code","a27f62e1":"code","69df8cbf":"code","a4e783ba":"code","f70994a9":"code","424671ba":"code","df8a504c":"code","9f6f54a6":"code","4f00efe4":"code","375a33be":"code","169eec3a":"markdown","ca623d19":"markdown","505bcb84":"markdown","6dd61bf4":"markdown"},"source":{"d05daeea":"#importing libraries\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport seaborn as sns","091b8258":"ds = pd.read_csv(\"..\/input\/fake-news\/train.csv\")\nds.head()","e877d992":"ds.isnull().sum()","a774e62a":"ds = ds.dropna()\nds.isnull().sum()","4582ac0c":"#after dropping null values,indexes will be unordered therfore resetting indexes\nds.reset_index(inplace = True,drop = True)\nds.head()","f43b0c77":"#defining dependent and independent vectors\n#taking only title for prediction\nx = ds.iloc[:,1:2]\ny = ds['label']","2e8955c3":"x.head()","c9b30a0f":"#checking number of real and fake news\nsns.countplot(x = 'label',data = ds)","1da2fccc":"#Text Cleaning and preprocessing\n\ncleaned = []\nfor i in range(0,len(ds)):\n    \n    #removing words any other than (a-z) and (A-Z)\n    text = re.sub('[^a-zA-Z]',' ', x['title'][i])\n    \n    #converting all words into lower case\n    text = text.lower()\n    \n    #tokenizing \n    text = text.split()\n    \n    #stemming and removing stopwords\n    ps = PorterStemmer()\n    text = [ps.stem(words) for words in text if words not in stopwords.words('english')]\n    text = ' '.join(text)\n    cleaned.append(text)","a27f62e1":"#cleaned text\ncleaned[:5]","69df8cbf":"#taking dictionary size 5000\nvocab_size = 5000\n\n#one hot encoding\none_hot_dir = [one_hot(words,vocab_size) for words in cleaned]\n\n#length of all rows should be equal therefore applying padding\n#this will adjust size by adding 0 at staring of the shorter rows\nembedded_layer = pad_sequences(one_hot_dir,padding = 'pre')\nembedded_layer","a4e783ba":"#converting into numpy arrays.\nx = np.array(embedded_layer)\ny = np.array(y)","f70994a9":"#splitting the Dataset into Train and Test set\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)","424671ba":"#creating model using LSTM\nmodel = Sequential()\n\n#taking number features as 50\nmodel.add(Embedding(vocab_size,50,input_length = len(embedded_layer[0])))\nmodel.add(Dropout(0.5))\n\n#adding LSTM layers with 100 neurons\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.5))\n\n#adding output layer \nmodel.add(Dense(1,activation=\"sigmoid\"))\n\n#compiling the model\nmodel.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])","df8a504c":"#summary of model\nmodel.summary()","9f6f54a6":"#training the model\nmodel.fit(x_train, y_train, validation_data = (x_test,y_test), epochs = 5, batch_size = 32)","4f00efe4":"#predicting and getting accuracy\ny_pred = model.predict(x_test)\ny_pred = (y_pred > 0.5)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","375a33be":"#getting confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test,y_pred)","169eec3a":"**DROPPING NULL VALUES**","ca623d19":"# **CLASSIFYING FAKE NEWS USING LSTM**\n\n","505bcb84":"**ALMOST 10000 ARE RELIABLE(0) AND 8000 ARE UNRELIABLE(1)**","6dd61bf4":"**LENGTH OF ALL ROWS IS EQUAL NOW**"}}