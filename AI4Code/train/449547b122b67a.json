{"cell_type":{"21991a62":"code","05692a83":"code","e91bece2":"code","476d226d":"code","a2b8372e":"code","05f0a619":"code","6fd5c64d":"code","08aabe86":"code","26c83866":"code","52a82b4c":"code","8259c46b":"code","5e702dae":"code","5aea6bfa":"code","67c1f6f0":"code","45e4c473":"code","4708c0f2":"code","37cb8d48":"code","0320669f":"code","327b9db6":"code","80479703":"code","33acb592":"code","37576293":"code","a74f43dc":"code","5b1b611d":"code","c92b9147":"code","bacf4af0":"code","50145a17":"code","4647f8e6":"code","fd2571c0":"code","25cac7d6":"code","411d3d0c":"code","6f6a0ef0":"code","ab91ca88":"code","c0cda71d":"code","57b5145a":"code","9715cf88":"code","20fe68b7":"code","e092122f":"code","8baec242":"code","5cb4c373":"code","3995db25":"code","a29013e4":"markdown","457bff40":"markdown","af2549b1":"markdown"},"source":{"21991a62":"!tar xf \/kaggle\/input\/ffmpeg-static-build\/ffmpeg-git-amd64-static.tar.xz\n# !ln -s \/kaggle\/input\/fastai-audio\/audio .\n# !ln -s \/kaggle\/input\/fastai\/fastai .\n# !pip install -qU \"\/kaggle\/input\/fastai2-wheels\/fastprogress-0.2.2-py3-none-any.whl\"","05692a83":"# !pip install -qU git+https:\/\/github.com\/fastai\/fastai\n# !pip install -qU torch torchaudio torchvision\n# !git clone https:\/\/github.com\/mogwai\/fastai_audio\n# %cd fastai_audio\n# !.\/install.sh\n# %cd ..\/\n# !ln -s \/kaggle\/input\/fastai_audio\/audio .","e91bece2":"# from audio import *  \n# from fastai.basics import *","476d226d":"# import gc\n# from functools import partial\nfrom pathlib import Path\n\nimport torchvision\nfrom fastai.vision import *\nfrom tqdm.notebook import tqdm\n\n\nhome = Path(\".\")\ninput_dir = Path(\"..\/input\/deepfake-detection-challenge\")","a2b8372e":"# from fastai.utils import *\n# show_install(1)","05f0a619":"# torch.__version__","6fd5c64d":"# import torchvision\n# torchvision.__version__","08aabe86":"# torchaudio.__version__","26c83866":"# !apt-get --assume-yes install sox libsox-dev libsox-fmt-all libsndfile1","52a82b4c":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(42)","8259c46b":"labels = pd.read_json(input_dir\/\"train_sample_videos\/metadata.json\").T","5e702dae":"ext = \".wav\"","5aea6bfa":"audio_path = Path(\"test_audio\")\naudio_path.mkdir(exist_ok=True)\ntrain_audio_path = Path(\"train_audio\")\ntrain_audio_path.mkdir(exist_ok=True)","67c1f6f0":"def mp4_to_wav(filenames, out):\n    Path(out).mkdir(exist_ok=True)\n    for fn in tqdm(filenames):\n        out_fn = f\"{out\/fn.stem}{ext}\"\n        command = f\"\/kaggle\/working\/ffmpeg-git-20191209-amd64-static\/ffmpeg -i '{fn}' -ar 44100 -vn '{out_fn}'\"\n        subprocess.call(command, shell=True)","45e4c473":"# mp4_to_wav((input_dir\/\"test_videos\").ls(), audio_path)","4708c0f2":"# mp4_to_wav((input_dir\/\"train_sample_videos\").ls(), train_audio_path)","37cb8d48":"# config = AudioConfig()\n# config.duration = 10_000","0320669f":"# get_y = lambda x: labels.loc[f\"{x.stem}.mp4\"].label","327b9db6":"# audios = (AudioList.from_folder(train_audio_path, config=config)\n#           .split_by_rand_pct(.2, seed=42)\n#           .label_from_func(get_y))","80479703":"BSA=1","33acb592":"# db = audios.databunch(bs=BSA)\n# db.show_batch()","37576293":"# learn = audio_learner(db)","a74f43dc":"# learn = load_learner(\"\/kaggle\/input\/deepfake\", \"export.pkl\")","5b1b611d":"# test = AudioList.from_folder(audio_path, config=config); test","c92b9147":"# learn.predict(test[-1])","bacf4af0":"# preds = []\n# for t in test:\n#     preds.append(learn.data.classes[np.argmax(learn.predict(t)[2])])\n# preds[:5]","50145a17":"# def predict_from_file(wav_file, learner, verbose=True):  \n#     item = AudioItem(path=wav_file)\n#     if verbose: display(item)\n#     al = AudioList([item], path=item.path, config=config)\n#     ai = AudioList.open(al, item.path)\n#     y, pred, raw_pred = learner.predict(ai)\n#     if verbose: print(y)\n#     if verbose: print(pred.item())\n#     if verbose: print(raw_pred)","4647f8e6":"# predict_from_file(test[0].path, learner )","fd2571c0":"# BS = 1 #CPU\nBS = 864","25cac7d6":"!ln -s \/kaggle\/input\/blazeface-pytorch\/* .\/\n!ln -s \/kaggle\/input\/deepfakes-inference-demo\/helpers .\/","411d3d0c":"from blazeface import BlazeFace\nfacedet = BlazeFace().to(torch.device(\"cuda:0\"));\n# facedet = BlazeFace();\nfacedet.load_weights(\"blazeface.pth\")\nfacedet.load_anchors(\"anchors.npy\")\n_ = facedet.train(False)\n\nfrom helpers.read_video_1 import VideoReader\nfrom helpers.face_extract_1 import FaceExtractor\n\nframes_per_video = 17\n\nvideo_reader = VideoReader()\nvideo_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\nface_extractor = FaceExtractor(video_read_fn, facedet)","6f6a0ef0":"def extract_faces(video_path, batch_size):\n    # Find the faces for N frames in the video.\n    faces = face_extractor.process_video(video_path)\n\n    # Only look at one face per frame.\n    face_extractor.keep_only_best_face(faces)\n    \n    if len(faces) > 0:\n        # NOTE: When running on the CPU, the batch size must be fixed\n        # or else memory usage will blow up. (Bug in PyTorch?)\n        x = []\n\n        # If we found any faces, prepare them for the model.\n        n = 0\n        for frame_data in faces:\n            for face in frame_data[\"faces\"]:\n                x.append(face)\n                n += 1\n    return x\n\ndef predict_on_mp4(names, learner, bs=17):\n    preds = []\n    for fn in tqdm(names):\n        if fn.is_file():\n            faces = extract_faces(fn, batch_size=bs)\n            pred = [learner.predict(Image(torchvision.transforms.ToTensor()(f)))[2] for f in faces]\n            if not pred:\n                display(f\"No pred from {fn}\")\n            preds.append(pred)\n    return preds","ab91ca88":"def mace(pred:Tensor, targ:Tensor)->Rank0Tensor:\n    \"Mean absolute error between clamped `pred` and `targ`.\"\n    pred,targ = flatten_check(pred,targ)\n    return torch.abs(targ - pred.clamp(0., 1.)).mean()","c0cda71d":"learn = load_learner(\"\/kaggle\/input\/deepfake\", \"float_shots.pkl\", bs=BS).to_fp32()\n# learn = load_learner(\"\/kaggle\/input\/deepfake\", \"shots.pkl\", bs=BS).to_fp32()","57b5145a":"raw_preds = predict_on_mp4((input_dir\/\"test_videos\").ls(), learn); len(raw_preds)","9715cf88":"# # classes\n# preds = []\n# for p in raw_preds:\n#     try:\n#         preds.append(torch.stack(p, dim=0).argmax(dim=1).float().mean().item())\n#     except:\n#         preds.append(0.5)","20fe68b7":"preds = []\nfor p in raw_preds:\n    try:\n        preds.append(torch.stack(p, dim=0).mean().clamp(0.1,0.9).item())\n    except:\n        preds.append(0.5)","e092122f":"subm = pd.read_csv(input_dir\/\"sample_submission.csv\")","8baec242":"subm[\"label\"] = preds\n# subm[\"label\"] = 1 - subm[\"label\"] # for binary classification where 0 is FAKE and 1 is REAL\nsubm[\"label\"].value_counts(bins=2)","5cb4c373":"subm.to_csv(\"submission.csv\", index=False, float_format='%.20f')","3995db25":"pd.read_csv(\"submission.csv\")","a29013e4":"# Audio","457bff40":"In this notebook I inference using my pretrained model, including pre-processing. The model was pretrained on a small subset of the data.\n\nSources:\n\nhttps:\/\/www.kaggle.com\/humananalog\/inference-demo","af2549b1":"# Video"}}