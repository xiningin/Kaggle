{"cell_type":{"64c57ea2":"code","50217718":"code","e38a92aa":"code","8a48f519":"code","8ec4ffcb":"code","3da700a5":"code","758edd49":"code","2409673a":"code","c81f3e18":"code","27670827":"code","d738f443":"code","d9d125b6":"code","017c7705":"code","4aed4ac6":"code","1799e687":"code","e4968639":"code","e2b42511":"code","1ff5a653":"code","2b2c0400":"code","5c5ee884":"code","83504e25":"code","48ee3c3e":"code","7375ea0d":"code","ccfcff2e":"code","f5a2874c":"code","4744664c":"code","df94c1c6":"code","9a4d3d53":"code","1cfc6b0b":"markdown","6944c057":"markdown","3ddbd19b":"markdown","a9b5393b":"markdown","5eba1ed2":"markdown","22cbb9c1":"markdown","fbdfbc6e":"markdown","fcb96a61":"markdown","ce568ecd":"markdown","01c1ff3e":"markdown","c0f927c3":"markdown"},"source":{"64c57ea2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50217718":"path_train = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\"\npath_test = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\"\ntrain = pd.read_csv(path_train)\ntest = pd.read_csv(path_test)","e38a92aa":"print(\"Train: \",train.shape)\nprint(\"Test: \",test.shape)","8a48f519":"numeric = train.select_dtypes(exclude='object')\nprint(\"\\nNumber of numeric features : \",(len(numeric.axes[1])))\nprint(\"\\n\", numeric.axes[1])","8ec4ffcb":"# Isolate the numeric features and check his relevance\n\nnum_corr = numeric.corr()\ntable = num_corr['SalePrice'].sort_values(ascending=False).to_frame()\ncm = sns.light_palette(\"green\", as_cmap=True)\ntb = table.style.background_gradient(cmap=cm)\ntb\n","3da700a5":"# Isolate the categorical data\ncategorical = train.select_dtypes(include='object')\ncategorical.head()","758edd49":"print(\"\\nNumber of categorical features : \",(len(categorical.axes[1])))\nprint(\"\\n\", categorical.axes[1])","2409673a":"# I will drop features with 80% missing values\ntrain_d = train.dropna(thresh=len(train)*0.8, axis=1)\ndroppedF = []\nprint(\"We dropped the next features: \")\nfor x in train.axes[1]:\n    if(x not in train_d.axes[1]):\n        droppedF.append(x)\n\nprint(droppedF)","c81f3e18":"# I will drop this features also in the test dataset\ntest_d = test.drop(droppedF,axis=1)\nprint(train_d.shape, test_d.shape)\nsh_train = train_d.shape\n# I will also mix both (test and train) to do all at the same time\nc1 = pd.concat((train_d, test_d), sort=False).reset_index(drop=True)\n\nprint(\"Total size is :\",c1.shape)","27670827":"# Now I will detect and study what to do with missing values\nc1_NA = (c1.isnull().sum() \/ len(c1)*100)\nc1_NA = c1_NA.drop(c1_NA[c1_NA == 0].index).sort_values(ascending = False)\nplt.figure(figsize=(14,7))\nchart = sns.barplot(x=c1_NA.index , y = c1_NA)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=90, horizontalalignment='right')\nplt.ylabel(\"Missing Value Percentage\")\nplt.xlabel(\"Features\")\nplt.title(\"Missing Value\u00b4s Percentage by Feature\")","d738f443":"# First of all split numerical NA and categorical NA data\nNA=c1[c1_NA.index.to_list()]\ncatNA=NA.select_dtypes(include='object')\nnumNA=NA.select_dtypes(exclude='object')","d9d125b6":"# Start with the numerical ones\nnumNA","017c7705":"# Fill with 0 and Garage Year Built fill it with the median to not disturb the data. Lot frontage also with the median\nFillw0 = ['MasVnrArea','BsmtFullBath','BsmtFinSF1','BsmtFinSF1','TotalBsmtSF','GarageCars','GarageArea','BsmtFinSF2','BsmtHalfBath','BsmtUnfSF']\nc1[Fillw0] = c1[Fillw0].fillna(0)\nc1['GarageYrBlt'] = c1.GarageYrBlt.fillna(c1.GarageYrBlt.median())\nc1['LotFrontage'] = c1.LotFrontage.fillna(c1.GarageYrBlt.median())\n","4aed4ac6":"# Take a look to the cat features\ncatNA","1799e687":"# I will use the forward fill method to the ones with almost no null values.\n# I will fill the rest with NA for the same reason I filled numerical ones with 0\nf_forward = ['Electrical', 'SaleType', 'KitchenQual', 'Exterior1st',\n             'Exterior2nd', 'Functional', 'Utilities', 'MSZoning']\nfor col in c1[f_forward]:\n    c1[col] = c1[col].fillna(method='ffill')\ncatNA.drop(f_forward,axis=1)\n\nc1['has_garage'] = c1['GarageQual'].isnull().astype(int)\nc1['has_Bsmt'] = c1['BsmtCond'].isnull().astype(int)\nc1['has_MasVnr'] = c1['MasVnrType'].isnull().astype(int)\n\nfor col in catNA:\n    c1[col] = c1[col].fillna(\"NA\")","e4968639":"# Now we will encode the categorical features. For that I will use one hot encoding and I will also add has_bathroom, has_garage, and has_MasVnr\n\ncb=pd.get_dummies(c1)\nprint(\"the shape of the original dataset\",c1.shape)\nprint(\"the shape of the encoded dataset\",cb.shape)\nprint(\"We have \",cb.shape[1]- c1.shape[1], 'new encoded features')\n","e2b42511":"# Split again train and test\ntrain = cb[:sh_train[0]] #sh_train is the shape of train_d\ntest = cb[sh_train[0]:]\ntest = test.drop('SalePrice',axis=1)# Remove SalePrice from the test\nprint(train.shape, test.shape)","1ff5a653":"#Detect and remove the outliers in the most significant features\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x = train['OverallQual'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(1,0))\nplt.scatter(x = train['MasVnrArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('MasVnrArea', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(1,1))\nplt.scatter(x = train['GarageArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageArea', fontsize=13)\n\n\n\n\nax1 = plt.subplot2grid((3,2),(2,0))\nplt.scatter(x = train['TotalBsmtSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(2,1))\nplt.scatter(x = train['1stFlrSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('1stFlrSF', fontsize=13)\nplt.show()\n \n\n \n\n","2b2c0400":"#Remove the detected outliers\ntrain = train.drop(train[(train['GrLivArea'] > 4000)&(train['SalePrice'] < 250000)].index)\ntrain = train.drop(train[(train['OverallQual'] == 10)&(train['SalePrice'] < 210000)].index)\ntrain = train.drop(train[(train['MasVnrArea'] > 1400)&(train['SalePrice'] < 300000)].index)\ntrain = train.drop(train[(train['GarageArea'] > 1200)&(train['SalePrice'] < 300000)].index)\ntrain = train.drop(train[(train['TotalBsmtSF'] > 5000)&(train['SalePrice'] < 250000)].index)\ntrain = train.drop(train[(train['1stFlrSF'] > 4000)&(train['SalePrice'] < 250000)].index)\n\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x = train['OverallQual'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(1,0))\nplt.scatter(x = train['MasVnrArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('MasVnrArea', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(1,1))\nplt.scatter(x = train['GarageArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageArea', fontsize=13)\n\n\n\n\nax1 = plt.subplot2grid((3,2),(2,0))\nplt.scatter(x = train['TotalBsmtSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(2,1))\nplt.scatter(x = train['1stFlrSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('1stFlrSF', fontsize=13)\nplt.show()\n ","5c5ee884":"# Study and correct (if it is necessary) the skewness and kurtosis.\nprint(\"Skewness:\", train['SalePrice'].skew())\nprint(\"Kurtosis: \",train['SalePrice'].kurt())\n\nplt.hist(train.SalePrice, bins=10, color='mediumpurple',alpha=0.5)\nplt.show()","83504e25":"# Apply log1p to correct this  distribution\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nprint(\"Skewness:\", train['SalePrice'].skew())\nprint(\"Kurtosis: \",train['SalePrice'].kurt())\n\nplt.hist(train.SalePrice, bins=10, color='mediumpurple',alpha=0.5)\nplt.show()","48ee3c3e":"# Finally we will apply Linear Regression and XGBoost\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# First plit X and y\ny = train.SalePrice\nx = train.drop('SalePrice',axis=1)\n\n\n# Split the them into train and test to evaluate both models\nX_train, X_valid, y_train, y_valid = train_test_split(x, y,test_size = 0.2, random_state=0)\n\n# Define the model\nXGB = XGBRegressor(n_estimators=10,learning_rate=0.05)\nlr = LinearRegression()\n# Fit the model\nXGB.fit(X_train,y_train) \nlr.fit(X_train,y_train) \n# Get predictions\npred_XGB = XGB.predict(X_valid)\npred_lr = lr.predict(X_valid)\n# Calculate MAE\nmae_XGB =  mean_absolute_error(pred_XGB,y_valid)\nmae_lr =  mean_absolute_error(pred_lr,y_valid)\n# Uncomment to print MAE\nprint(\"Mean Absolute Error XGB:\" , mae_XGB)\nprint(\"Mean Absolute Error lr:\" , mae_lr)\n","7375ea0d":"# Best Alpha for Ridge Regression\nimport sklearn.model_selection as ms\nimport sklearn.model_selection as GridSearchCV #Cross-Validation\nfrom sklearn.linear_model import Ridge\n\nridge=Ridge()\nparameters= {'alpha':[x for x in range(1,101)]}\n\nridge_reg=ms.GridSearchCV(ridge, param_grid=parameters, scoring='neg_mean_squared_error', cv=3)\nridge_reg.fit(X_train,y_train)\nprint(\"The best value of Alpha is: \",ridge_reg.best_params_)\n\n","ccfcff2e":"# Ridge Error\nridge_mod=Ridge(alpha=1)\nridge_mod.fit(X_train,y_train)\npred_rr=ridge_mod.predict(X_valid)\nmae_rr =  mean_absolute_error(pred_rr,y_valid)\n\nprint(\"Mean absolute error Ridge Regression: \",mae_rr )\n","f5a2874c":"# Best alpha Lasso Regression\nfrom sklearn.linear_model import Lasso\n\nparameters= {'alpha':[0.0001,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\n\nlasso=Lasso()\nlasso_reg=ms.GridSearchCV(lasso, param_grid=parameters, scoring='neg_mean_squared_error', cv=3)\nlasso_reg.fit(X_train,y_train)\n\nprint('The best value of Alpha is: ',lasso_reg.best_params_)","4744664c":"# Lasso Error\nlasso_mod=Ridge(alpha=0.0001)\nlasso_mod.fit(X_train,y_train)\npred_la=lasso_mod.predict(X_valid)\nmae_la =  mean_absolute_error(pred_la,y_valid)\n\nprint(\"Mean absolute error Lasso Regression: \",mae_la)\n","df94c1c6":"# Based on that we will use Linear Regression\nRidge2 = Ridge(alpha=16)\n# Fit the model\nRidge2.fit(x,y)\n# Get predictions\npred_ridge2 = ridge2.predict(test)\nfinal = np.expm1(pred_ridge2) # Undo the log1p\npath_f = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\"\nf = pd.read_csv(path_train)\n","9a4d3d53":"output = pd.DataFrame({'Id': test.Id,\n                       'SalePrice': final})\noutput.to_csv('sample_submission.csv', index=False)","1cfc6b0b":"I compared Linear Regression and XGBRegressor because are the only two models I worked with haha\n\nNote(v2) I have read about Lasso Regression and Ridge Regression. I will see how they perform but I am sure that they will perform better than Linear Regression.","6944c057":"I am observing which features have missing values to study what to do with them","3ddbd19b":"Hi everybody. This is my first Data Science \"project\". I would appreciate a lot every advice or sugestion that you will do. I am here to learn so I will use everything that I will learn in the near future to improve this code. Help me to imporve faster :)\nAlso I will try to explain what I do in every piece of code, I may be wrong so feel free to correct me.\nI am not English so probably I will make some orthographic mistakes, forgive me.\n\nI want to give credit to one notebook that helped me a lot: \nhttps:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking#5--Outliers-detection\n\n(v2) I added Ridge and Lasso Regression","a9b5393b":"I wanted to study outliars in the most relevant features and that\u00b4s what I did. I detected and eliminated them","5eba1ed2":"I normalized the SalePrice feature because I observed that the kurtosis and the skewness were not good. For that I used the log transformation","22cbb9c1":"Now I will study the data. For that I will split numerical and categorical features and study them a little bit","fbdfbc6e":"Thank\u00b4s to all that have read my first notebook. Do not heasitate on comment your suggestions,advices or questions :)","fcb96a61":"Finlly I decided to use Ridge regression because performs better on this scenario. I also undo the log transformation.","ce568ecd":"First of all I load the data and get familiar with it","01c1ff3e":"I wrote the next lines when I was coding in after a previous \"study\" of the features. When I wrote fill with 0 I mean 0 to numeric features and NA to cat. features(NA is not null, NA means    lack of in this dataset). I did what is written below plus some other methods that i found out they were necessary, everything is explained on the commentaries\n \n -----------------------------------------------------------------------------------------------------------------------------\n \n LotFrontage: Linear feet of street connected to property so I decided to fill the null values with the median\n I guess all or almost all this garage features that have null values is because they have no garage. I will fill this with 0( to categorical with label encoding) and create a features   has_garage.\n With basement features ocurrs the same than with garage so I will do the same\n Same with MasVnrType (it is a categorical, I will label encode it)\n MasVrnArea, fill it with 0\n For the rest I will fill it with 0 because they have not that much missing values","c0f927c3":"I have decided to delete all features with 80% missing values"}}