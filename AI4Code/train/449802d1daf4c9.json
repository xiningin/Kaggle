{"cell_type":{"976e7a83":"code","7021f43f":"code","a588c13e":"code","5fed3acd":"code","48e09ee3":"code","456d0dae":"code","04ea561e":"code","67da6576":"code","ef8f687a":"code","66900627":"code","909ecee5":"code","9b618d9c":"code","9a12b30e":"code","b6aea9fe":"code","3f500dd6":"code","b21f282d":"code","2e9f6120":"code","9c0778fd":"code","7d555e68":"code","319f592d":"code","713452d3":"code","220d1fbe":"code","6604733b":"code","38bc2d50":"code","e43b3b86":"markdown","41a509d2":"markdown","9f01f921":"markdown","c1063aec":"markdown","a2ff7fbd":"markdown","8fd63b01":"markdown"},"source":{"976e7a83":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7021f43f":"from fastai.text import *\nfrom fastai import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt","a588c13e":"jobs  = pd.read_csv('\/kaggle\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv')\njobs.head()","5fed3acd":"jobs.fillna(value='NA',inplace=True)","48e09ee3":"jobs.fraudulent.value_counts()","456d0dae":"sns.set_style('darkgrid')\nplt.figure(1,figsize=(20,8))\nsns.countplot(hue=jobs.fraudulent,x=jobs.employment_type)\nplt.title('Fraudulence Distribution based on Type of Employment Opportunity')\nplt.xlabel('Employment Type')\nplt.ylabel('No. of Jobs')","04ea561e":"path = Path('\/kaggle\/working\/')\ndata_lm = (TextList.from_df(jobs,path,cols=['company_profile','description','requirements','benefits'])\n                  .split_by_rand_pct(0.2)\n                  .label_for_lm()\n                  .databunch(bs=128))","67da6576":"data_lm.show_batch(rows=6)","ef8f687a":"learn = language_model_learner(data_lm,AWD_LSTM,metrics=[accuracy,Perplexity()],model_dir='\/kaggle\/working\/',drop_mult=0.3).to_fp16()","66900627":"learn.lr_find()\nlearn.recorder.plot()","909ecee5":"import gc\ngc.collect()","9b618d9c":"learn.fit_one_cycle(5, 1e-01, moms = (0.8,0.7))","9a12b30e":"learn.save_encoder('lm')","b6aea9fe":"learn = None\ngc.collect()","3f500dd6":"jobs0 = jobs[jobs['fraudulent']==0][:1400].copy()\njobs1 = jobs[jobs['fraudulent']==1].copy()\ntrain = pd.concat([jobs0,jobs1])","b21f282d":"label_cols = ['department','employment_type','required_experience','industry','function','required_education','title','company_profile','description','requirements','benefits']","2e9f6120":"data_cls = (TextList.from_df(train,path,cols=label_cols,vocab=data_lm.vocab)\n                    .split_by_rand_pct(0.2,seed=64)\n                    .label_from_df(cols='fraudulent')\n                    .databunch(bs=128))","9c0778fd":"data_cls.show_batch(rows=6)","7d555e68":"clf = None\ngc.collect()","319f592d":"f_score = FBeta()\nf_score.average = 'macro'\nkappa = KappaScore()\nkappa.weights = \"quadratic\"\n\nclf = text_classifier_learner(data_cls,AWD_LSTM,metrics=[accuracy, f_score, kappa],drop_mult=0.3).to_fp16()\nclf.load_encoder('\/kaggle\/working\/lm');","713452d3":"gc.collect()","220d1fbe":"clf.lr_find()\nclf.recorder.plot()","6604733b":"clf.fit_one_cycle(10, 1e-2, moms=(0.8,0.7))","38bc2d50":"interp = TextClassificationInterpretation.from_learner(clf)\ninterp.plot_confusion_matrix()","e43b3b86":"The language model can predict the next word correctly one out of two times, which is quite good.","41a509d2":"In the last epoch, f1 score is 0.925419 and Cohen's Kappa is 0.855083. Nice. Confusion matrix can help further tune the model.","9f01f921":"This could possibly be because of a huge class imbalance, seeing that there are far less number of fradulent cases than the real ones.","c1063aec":"Clearly, there is a class imbalance. Therefore, simple accuracy is not a good metric for prediction. I will hence be using Cohen's Kappa. But that is for later, first I need to make a language model.","a2ff7fbd":"Pretty good if you ask me! The model correctly predicts the fraudulence of 278 out of 290 postings which are not fraud, and 145 out of 163 postings which are fraud.","8fd63b01":"`to_fp16()` uses [Mixed Precision Training](https:\/\/arxiv.org\/abs\/1710.03740) that decreases the training time. Mixed precision is the use of both 16-bit and 32-bit floating-point types in a model during training to make it run faster and use less memory. By keeping certain parts of the model in the 32-bit types for numeric stability, the model will have a lower step time and train equally as well in terms of the evaluation metrics such as accuracy."}}