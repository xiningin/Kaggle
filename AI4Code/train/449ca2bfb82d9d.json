{"cell_type":{"eef5b1a4":"code","46dcd196":"code","08e2f219":"code","a945a172":"code","690a0316":"code","55cfedab":"code","147cc17f":"code","d594e218":"code","843bdc60":"code","1c07affb":"code","ed13751c":"code","175e2ce8":"code","94d5e459":"code","a9a80a1b":"code","ab69b58b":"code","7cab8eff":"code","e5459c38":"code","e78c4ae6":"code","5613c7ed":"markdown","eeb1f887":"markdown","535f2915":"markdown"},"source":{"eef5b1a4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, add\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\nimport os\nprint(os.listdir(\"..\/input\"))","46dcd196":"# I'll load preprocessed data from my dataset\ntrain = pd.read_csv('..\/input\/jigsaw-public-files\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-public-files\/test.csv')\n# after processing some of the texts are emply\ntrain['comment_text'] = train['comment_text'].fillna('')\ntest['comment_text'] = test['comment_text'].fillna('')\nsub = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')","08e2f219":"full_text = list(train['comment_text'].values) + list(test['comment_text'].values)","a945a172":"%%time\ntk = Tokenizer(lower = True, filters='', num_words=120000)\ntk.fit_on_texts(full_text)","690a0316":"train_tokenized = tk.texts_to_sequences(train['comment_text'])\ntest_tokenized = tk.texts_to_sequences(test['comment_text'])","55cfedab":"max_len = 230\nX_train = pad_sequences(train_tokenized, maxlen = max_len)\nX_test = pad_sequences(test_tokenized, maxlen = max_len)","147cc17f":"# y = train['target']\ny = np.where(train['target'] >= 0.5, True, False) * 1\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y)","d594e218":"embedding_path = \"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"","843bdc60":"embed_size = 300\nmax_features = 120000","1c07affb":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","ed13751c":"# score is 0.92393\ndef build_model(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n    \n    inp = Input(shape = (max_len,))\n    x = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(spatial_dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    \n    x1 = Conv1D(int(units\/2), kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    \n    y = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    y = Conv1D(int(units\/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n    \n    avg_pool1 = GlobalAveragePooling1D()(x)\n    max_pool1 = GlobalMaxPooling1D()(x)\n    \n    avg_pool2 = GlobalAveragePooling1D()(y)\n    max_pool2 = GlobalMaxPooling1D()(y)\n       \n    \n    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n    x = Dense(2, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_binary, batch_size = 128, epochs = 3, validation_split=0.1, \n                        verbose = 2, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    return model","175e2ce8":"model = build_model(lr = 1e-3, lr_d = 0, units = 128, spatial_dr = 0.1)\npred = model.predict(X_test, batch_size = 1024, verbose = 1)[:, 1]","94d5e459":"plt.hist(pred);\nplt.title('Distribution of predictions');","a9a80a1b":"# sub['prediction'] = pred\n# sub.to_csv('submission.csv', index=False)","ab69b58b":"# https:\/\/www.kaggle.com\/tunguz\/bi-gru-cnn-poolings-gpu-kernel-version\/data\ndef build_model1(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\n    inp = Input(shape = (max_len,))\n\n    x = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(dr)(x)\n    \n    x = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n\n    y = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    y = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n\n    avg_pool1 = GlobalAveragePooling1D()(x)\n    max_pool1 = GlobalMaxPooling1D()(x)\n\n    avg_pool2 = GlobalAveragePooling1D()(y)\n    max_pool2 = GlobalMaxPooling1D()(y)\n\n    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n    x = Dense(2, activation = \"sigmoid\")(x)\n\n    model = Model(inputs = inp, outputs = x)\n\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n\n    history = model.fit(X_train, y_binary, batch_size = 128, epochs = 3, validation_split=0.1, \n\n                        verbose = 2, callbacks = [check_point, early_stop])\n\n    model = load_model(file_path)\n\n    return model","7cab8eff":"# model = build_model1(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\n# pred = model.predict(X_test, batch_size = 1024, verbose = 1)[:, 1]\n# sub['prediction'] = pred\n# sub.to_csv('submission.csv', index=False)","e5459c38":"def build_model2(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\n    inp = Input(shape = (max_len,))\n\n    x = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x = SpatialDropout1D(dr)(x)\n    \n    x = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x)\n    x = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x)\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    \n    hidden = add([hidden, Dense(units * 4, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(units * 4, activation='relu')(hidden)])\n\n    result = Dense(2, activation='sigmoid')(hidden)\n\n    model = Model(inputs = inp, outputs = result)\n\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    \n    history = model.fit(X_train, y_binary, batch_size = 128, epochs = 3, validation_split=0.1, \n\n                        verbose = 2, callbacks = [check_point, early_stop])\n\n    model = load_model(file_path)\n\n    return model","e78c4ae6":"# model = build_model2(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\n# pred = model.predict(X_test, batch_size = 1024, verbose = 1)[:, 1]\n# sub['prediction'] = pred\n# sub.to_csv('submission.csv', index=False)","5613c7ed":"### bi-gru cnn\n\nThis is an architecture from https:\/\/www.kaggle.com\/tunguz\/bi-gru-cnn-poolings-gpu-kernel-version\/data\n\nTraining on a single train_test_split gives 0.92845 LB","eeb1f887":"## General information\nIn this kernel I'll compare various model architectures and embeddings.\n\nWork in progress.","535f2915":"### Another bi-gru\nhttps:\/\/www.kaggle.com\/thousandvoices\/simple-lstm\n\nScore: 0.92741"}}