{"cell_type":{"ec77ff34":"code","c44b1077":"code","ee2ac5ca":"code","afb4c9d8":"code","4ccb24a2":"code","5e2fdc5f":"code","78272b37":"code","7754b820":"code","6a45da86":"code","ff07d5b3":"code","1a406e25":"code","bbe38179":"code","7a86b4ec":"code","ff3b81ab":"code","80d8171a":"code","a58f3053":"code","50ecc5f3":"code","f8d8280b":"code","cd99bc7b":"code","b423b372":"code","ed28341a":"code","9d809642":"code","aa1f7771":"code","08bf7990":"code","80dae1db":"code","cbeac76c":"markdown","14a212a2":"markdown","678e430b":"markdown","c1c31b94":"markdown"},"source":{"ec77ff34":"# Setup\n\n# common:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport matplotlib.patches as mpatches\nfrom scipy.stats import norm\nfrom scipy import stats\nimport time\nimport folium\nimport collections\nimport eli5 # Feature importance evaluation\nimport urllib\nfrom PIL import Image\n\n# for ML:\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, average_precision_score, roc_curve, precision_recall_curve, classification_report, confusion_matrix, mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, ShuffleSplit, cross_validate, cross_val_score, cross_val_predict, RandomizedSearchCV, GridSearchCV, learning_curve\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\n\n# Imported Libraries\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c44b1077":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ee2ac5ca":"# load data:\nfile_path = '\/kaggle\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv'\ndf = pd.read_csv(file_path)","afb4c9d8":"df.head()","4ccb24a2":"df.info()","5e2fdc5f":"pd.set_option(\"display.float_format\", \"{:.2f}\".format)\ndf.describe()","78272b37":"df.isnull().sum()","7754b820":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.violinplot(x='neighbourhood_group', y='price', data=df, hue='neighbourhood_group')","6a45da86":"new_df = df[df['price']<300]\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.violinplot(x='neighbourhood_group', y='price', data=new_df, hue='neighbourhood_group')","ff07d5b3":"plt.figure(figsize=(15,13))\n\n#loading the png NYC image found on Google and saving to my local folder along with the project\ni=urllib.request.urlopen('https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ec\/Neighbourhoods_New_York_City_Map.PNG')\n\nnyc_img=Image.open(i)\n#scaling the image based on the latitude and longitude max and mins for proper output\nplt.imshow(nyc_img, zorder=0, extent=[-74.258, -73.7, 40.49,40.92])\nax=plt.gca()\n\n#using scatterplot again\nnew_df.plot(kind='scatter', x='longitude', y='latitude', c='price', ax=ax, cmap=plt.get_cmap('jet'), colorbar=True, alpha=0.4, zorder=5)","1a406e25":"scat = px.scatter(new_df, x = 'room_type', y = 'price', color = 'room_type', marginal_y = 'box')\n\nscat.show()","bbe38179":"plt.figure(figsize=(12,10))\nsns.distplot(df['price'], fit=norm, color='#C5B3F9')\nplt.title(\"Price Distribution Plot\", size=15, weight='bold')","7a86b4ec":"df['price_log'] = np.log(df.price+1)\n\nplt.figure(figsize=(12,10))\nsns.distplot(df['price_log'], fit=norm, color='#56F9BB')\nplt.title('Log-Price Distribution Plot', size=15, weight='bold')","ff3b81ab":"stats.probplot(df['price_log'], plot=plt)\nplt.show()","80d8171a":"new_df = df.drop(columns=['id', 'name', 'host_id', 'host_name', 'last_review', 'price'], axis=\"columns\")\nnew_df.info()","a58f3053":"plt.figure(figsize=(15, 12))\npalette = sns.diverging_palette(20, 220, n=256)\ncorr=new_df.corr(method='pearson')\nsns.heatmap(corr, annot=True, fmt=\".2f\", cmap=palette, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title(\"Correlation Matrix\", size=15, weight='bold')","50ecc5f3":"hpi_df = new_df[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'price_log']]\nsns.pairplot(hpi_df)","f8d8280b":"categorical_features = []\nfor column in new_df.columns:\n    if new_df[column].dtype == object:\n        categorical_features.append(column)\n        print(f\"{column}\")\n        print(\"====================================\")","cd99bc7b":"numerical_features = []\nfor column in new_df.columns:\n    if new_df[column].dtype == int or df[column].dtype == float:\n        numerical_features.append(column)\n        print(f\"{column}\")\n        print(\"====================================\")","b423b372":"numerical_features.remove('price_log')","ed28341a":"df.dropna(subset=['price_log'],inplace=True)\n# Separate features and predicted value\nfeatures = numerical_features + categorical_features\nY = df['price_log']\nX = df.drop('price_log', axis=1)[features]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# preprocess numerical feats:\n# for most num cols, except the dates, 0 is the most logical choice as fill value\n# and here no dates are missing.\nnum_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"constant\")),\n    ('scaler', StandardScaler())])\n\n# Preprocessing for categorical features:\ncat_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n    (\"onehot\", OneHotEncoder(handle_unknown='ignore'))])\n\n# Bundle preprocessing for numerical and categorical features:\npreprocessor = ColumnTransformer(transformers=[(\"num\", num_transformer, numerical_features),\n                                               (\"cat\", cat_transformer, categorical_features)])","9d809642":"# define base_models to test:\nbase_models = {\"DTR_model\": DecisionTreeRegressor(),\n               \"SVM_model\": SVR(),\n               \"KNN_model\": KNeighborsRegressor(),\n               \"RFC_model\": RandomForestRegressor(),\n               \"GBR_model\": GradientBoostingRegressor(),\n               \"BAG_model\": BaggingRegressor(),\n               \"LIG_model\": LinearRegression(),\n               \"ETR_model\": ExtraTreeRegressor(),\n               \"XGB_model\": XGBRegressor(),\n              }\n\n# split data into 'kfolds' parts for cross validation,\n# use shuffle to ensure random distribution of data:\nkfolds = 4 # 4 = 75% train, 25% validation\nsplit = KFold(n_splits=kfolds, shuffle=True, random_state=42)\n\n# Preprocessing, fitting, making predictions and scoring for every model:\nfor name, model in base_models.items():\n    # pack preprocessing of data and the model in a pipeline:\n    model_steps = Pipeline(steps=[\n                                    ('preprocessor', preprocessor),\n                                    ('model', model)])\n    # get cross validation score for each model:\n    cv_results = cross_val_score(model_steps, \n                                 X_train, Y_train, \n                                 cv=split,\n                                 scoring=\"r2\",\n                                 n_jobs=-1)\n    # output:\n    min_score = round(min(cv_results), 4)\n    max_score = round(max(cv_results), 4)\n    mean_score = round(np.mean(cv_results), 4)\n    std_dev = round(np.std(cv_results), 4)\n    print(f\"{name} cross validation r2 score: {mean_score} +\/- {std_dev} (std) min: {min_score}, max: {max_score}\")","aa1f7771":"# Enhanced RFC model with the best parameters I found:\nmodel = RandomForestRegressor(\n                                random_state=42,\n                                n_jobs=-1,\n                                n_estimators=830,\n                                max_depth=860,\n                                max_features=90,\n                                min_samples_split=60,\n                                min_samples_leaf=10,\n        )\nmodel_steps = Pipeline(steps=[\n                                ('preprocessor', preprocessor),\n                                ('model', model)])\n# get cross validation score for each model:\ncv_results = cross_val_score(model_steps, \n                             X_train, Y_train, \n                             cv=split,\n                             scoring=\"r2\",\n                             n_jobs=-1)\n# output:\nmin_score = round(min(cv_results), 4)\nmax_score = round(max(cv_results), 4)\nmean_score = round(np.mean(cv_results), 4)\nstd_dev = round(np.std(cv_results), 4)\nprint(f\"Enhanced RFC model cross validation r2 score: {mean_score} +\/- {std_dev} (std) min: {min_score}, max: {max_score}\")","08bf7990":"# fit model(pipeline) so values can be accessed:\nmodel_steps.fit(X_train, Y_train)\n\n# Names of all (encoded) features are needed.\n# Get names of columns from One Hot Encoding:\nonehot_columns = list(model_steps.named_steps['preprocessor'].\n                      named_transformers_['cat'].\n                      named_steps['onehot'].\n                      get_feature_names(input_features=categorical_features))\n\n# Add num_features for full list.\n# Order must be as in definition of X, where num_features are first: \nfeat_imp_list = numerical_features + onehot_columns\n\n# show 10 most important features, provide names of features:\nfeat_imp_df = eli5.formatters.as_dataframe.explain_weights_df(\n    model_steps.named_steps['model'],\n    feature_names=feat_imp_list)\nfeat_imp_df.head(10)","80dae1db":"Y_pred = model_steps.predict(X_test)\nActVPred = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nprint(ActVPred)\n\n# Checking the accuracy\nprint('Mean Absolute Error:', mean_absolute_error(Y_test, Y_pred))\nprint('Mean Squared Error:', mean_squared_error(Y_test, Y_pred))\nprint('Root Mean Squared Error:', np.sqrt(mean_squared_error(Y_test, Y_pred)))","cbeac76c":"## room_type vs price","14a212a2":"## (longitude, latitude) vs price","678e430b":"## neighbourhood_group vs price","c1c31b94":"In below graph, the good fit indicates that normality is a reasonable approximation."}}