{"cell_type":{"f16191d3":"code","b3697eff":"code","1eb66662":"code","bb605ad5":"code","bc7e151d":"code","881b2dc5":"code","fb89badb":"code","0fcd095a":"code","125b0e85":"code","0db0340a":"code","a37dbb5c":"code","49282fd0":"code","94a512d1":"code","c1a4f505":"code","d632e92e":"code","e23e249f":"code","19627bf8":"code","ad566abd":"code","01dd21a8":"code","11eeff10":"code","0b06b8fc":"code","fd98e8b8":"code","52d3f8ad":"code","2ff7d239":"code","1fa0c286":"code","b0b715bc":"code","ff9dc14f":"code","9612a91d":"markdown","2b70cb1b":"markdown","d49e746f":"markdown"},"source":{"f16191d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b3697eff":"import os, sys, math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1eb66662":"INPUT_SHAPE = (232,232,4)\nBATCH_SIZE = 32","bb605ad5":"path_to_train = '\/kaggle\/input\/human-protein-atlas-image-classification\/train\/'\ndata = pd.read_csv('\/kaggle\/input\/human-protein-atlas-image-classification\/train.csv')\nweight_path = '\/kaggle\/input\/dna-first-kaggle\/'\n\ntrain_dataset_info = []\nfor name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n    train_dataset_info.append({\n        'path':os.path.join(path_to_train, name),\n        'labels':np.array([int(label) for label in labels])})\ntrain_dataset_info = np.array(train_dataset_info)","bc7e151d":"from sklearn.model_selection import train_test_split\ntrain_ids, test_ids, train_targets, test_target = train_test_split(\n    data['Id'], data['Target'], test_size=0.2, random_state=42)","881b2dc5":"class data_generator:\n    \n    def create_train(dataset_info, batch_size, shape, augument=True):\n        assert shape[2] == 4\n        while True:\n            random_indexes = np.random.choice(len(dataset_info), batch_size)\n            batch_images = np.empty((batch_size, shape[0], shape[1], shape[2]))\n            batch_labels = np.zeros((batch_size, 28))\n            for i, idx in enumerate(random_indexes):\n                image = data_generator.load_image(\n                    dataset_info[idx]['path'], shape)   \n                if augument:\n                    image = data_generator.augment(image)\n                batch_images[i] = image\n                batch_labels[i][dataset_info[idx]['labels']] = 1\n            yield batch_images, batch_labels\n            \n    \n    def load_image(path, shape):\n        R = np.array(Image.open(path+'_red.png'))\n        G = np.array(Image.open(path+'_green.png'))\n        B = np.array(Image.open(path+'_blue.png'))\n        Y = np.array(Image.open(path+'_yellow.png'))\n\n        image = np.stack((R,G,B,Y),-1)\n        image = cv2.resize(image, (shape[0], shape[1]))\n        image = np.divide(image, 255)\n        return image  \n                \n            \n    def augment(image):\n        augment_img = iaa.Sequential([\n            iaa.OneOf([\n                iaa.Affine(rotate=0),\n                iaa.Affine(rotate=90),\n                iaa.Affine(rotate=180),\n                iaa.Affine(rotate=270),\n                iaa.Fliplr(0.5),\n                iaa.Flipud(0.5),\n            ])], random_order=True)\n        \n        image_aug = augment_img.augment_image(image)\n        return image_aug","fb89badb":"# create train datagen\ntrain_datagen = data_generator.create_train(train_dataset_info, BATCH_SIZE, INPUT_SHAPE, augument=True)","0fcd095a":"images, labels = next(train_datagen)\n\nfig, ax = plt.subplots(1,4,figsize=(25,5))\nfor i in range(4):\n    ax[i].imshow(images[i])\nprint('min: {0}, max: {1}'.format(images.min(), images.max()))","125b0e85":"images.shape","0db0340a":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Input\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Conv2D, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.densenet import DenseNet121\nfrom keras.applications.densenet import DenseNet169\nfrom keras.applications.densenet import DenseNet201\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import LambdaCallback\nfrom keras.callbacks import Callback\nfrom keras import metrics\nfrom keras.optimizers import Adam \nfrom keras import backend as K\nimport tensorflow as tf\nimport keras","a37dbb5c":"def DnAInput(img_shape = INPUT_SHAPE) :\n    model = Sequential()\n    # 230 * 230\n    model.add(Conv2D(32, kernel_size=(3,3), activation=\"relu\", kernel_initializer='he_normal', input_shape = img_shape))\n    model.add(BatchNormalization())\n    # 228 * 228\n    model.add(Conv2D(32, kernel_size=(3,3), activation=\"relu\", kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    # 226 * 226\n    model.add(Conv2D(32, kernel_size=(3,3), activation=\"relu\", kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    # 224 * 224\n    model.add(Conv2D(3, kernel_size=(3,3), activation=\"relu\", kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    return model","49282fd0":"def f1(y_true, y_pred):\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)","94a512d1":"def show_history(history):\n    fig, ax = plt.subplots(1, 3, figsize=(15,5))\n    ax[0].set_title('loss')\n    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax[1].set_title('f1')\n    ax[1].plot(history.epoch, history.history[\"f1\"], label=\"Train f1\")\n    ax[1].plot(history.epoch, history.history[\"val_f1\"], label=\"Validation f1\")\n    ax[2].set_title('acc')\n    ax[2].plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\n    ax[2].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\n    ax[0].legend()\n    ax[1].legend()\n    ax[2].legend()","c1a4f505":"input_model = DnAInput()","d632e92e":"densenet169 = DenseNet169(include_top = False)","e23e249f":"densenet169.summary()","19627bf8":"for layer in densenet169.layers:\n    layer.trainable = False","ad566abd":"x = Conv2D(28,kernel_size=(3,3), activation='relu', kernel_initializer=\"he_normal\")(densenet169.output)\nx = BatchNormalization()(x)\nflat = GlobalAveragePooling2D()(x)\nfc = Dense(28, activation='sigmoid')(flat)","01dd21a8":"inputToDensnet169 = Model(inputs=densenet169.input, outputs=fc)","11eeff10":"inputToDensnet169.summary()","0b06b8fc":"tmp_output = input_model.output\nfinal_output = inputToDensnet169(tmp_output)\nDnaNet = Model(inputs = input_model.input, output = final_output)","fd98e8b8":"DnaNet.summary()","52d3f8ad":"DnaNet.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',f1])","2ff7d239":"import glob\nglob.glob(weight_path + '*')","1fa0c286":"DnaNet.load_weights(weight_path + 'DnAnet64-10.hdf5')","b0b715bc":"checkpointer = ModelCheckpoint('DnAnet64-{epoch:02d}_ver2.hdf5',\n    verbose=2, save_best_only=False)\n\ntrain_generator = data_generator.create_train(\n    train_dataset_info[train_ids.index], BATCH_SIZE, INPUT_SHAPE, augument=False)\nvalidation_generator = data_generator.create_train(\n    train_dataset_info[test_ids.index], 256, INPUT_SHAPE, augument=False)","ff9dc14f":"history = DnaNet.fit_generator(\n    train_generator,\n    steps_per_epoch=int(data.shape[0]\/BATCH_SIZE),\n    validation_data=next(validation_generator),\n    epochs=20, \n    verbose=1,\n    callbacks=[checkpointer])","9612a91d":"### Load dataset info","2b70cb1b":"### Create Model","d49e746f":"### Create datagenerator"}}