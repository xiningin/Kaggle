{"cell_type":{"54a42bb0":"code","3de8522a":"code","cbc2a89c":"code","31243938":"code","a0131b2f":"code","9fabbd92":"code","d8f7423e":"code","ee5381b8":"code","3bed3dd9":"code","255d1abb":"code","bcbdd291":"code","5c247cbd":"code","8620704c":"code","4e78066c":"code","0df69e6f":"code","e729aad4":"code","7147e5a0":"code","4558e4f7":"code","08def1b5":"code","10bcbe3f":"markdown","d3e3252d":"markdown","06057349":"markdown","84cc3e40":"markdown","4f96e828":"markdown","bee91963":"markdown","837b4bce":"markdown","13264ad3":"markdown","47f6a88b":"markdown","46dcbe1d":"markdown","078c969f":"markdown","4021ec02":"markdown","de27cede":"markdown","a90b79f7":"markdown","591277a3":"markdown","1368f4de":"markdown","a98afb6f":"markdown","f255e853":"markdown","6f497dd5":"markdown"},"source":{"54a42bb0":"import numpy as np\nfrom tqdm import tqdm\nimport os\nimport datetime\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,  Normalizer\nimport seaborn as sb\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.metrics import confusion_matrix \nfrom tensorflow.keras.initializers import RandomNormal, RandomUniform, HeNormal, HeUniform, GlorotNormal, GlorotUniform\nfrom sklearn.metrics import roc_auc_score, f1_score","3de8522a":"\nmy_data = np.genfromtxt('..\/input\/callbackdata\/callback_data.csv', delimiter=',',skip_header=1)\nX = my_data[:,:2]\nY = my_data[:,2]\n# train test split\n\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.33, random_state = 101)\n\n# # normalize data\n# std_scalar = StandardScaler()\n# std_scalar.fit(X_train)\n# X_train = std_scalar.transform(X_train)\n# X_test = std_scalar.transform(X_test)\n\n","cbc2a89c":"sb.scatterplot(X_train[:,0], X_train[:,1], y_train)","31243938":"\n# normalize data\nstd_scalar =  StandardScaler()\nstd_scalar.fit(X_train)\n\nX_train = std_scalar.transform(X_train)\nX_test = std_scalar.transform(X_test)\n\nprint(X_train.shape)","a0131b2f":"%load_ext tensorboard\n# Clear any logs from previous runs\n!rm -rf .\/logs\/ ","9fabbd92":"# create Basic Model\n\ndef create_model(inp_dim, out_dim, inpt_activation='sigmoid',out_activation='sigmoid', kernel_init=RandomUniform):\n      print(inp_dim)\n      model = Sequential(\n          [\n            Dense(256, input_shape=(inp_dim,), activation=inpt_activation, kernel_initializer = kernel_init),\n            Dropout(0.3),\n            Dense(128, activation=inpt_activation, kernel_initializer = kernel_init),\n            Dense(64,  activation=inpt_activation, kernel_initializer = kernel_init),\n            Dense(32,  activation=inpt_activation, kernel_initializer = kernel_init),\n            Dense(16,  activation=inpt_activation, kernel_initializer = kernel_init),\n            Dense(out_dim, activation='sigmoid',\n                             bias_initializer=kernel_init)\n        ]\n      )\n\n      return model\n\n\n","d8f7423e":"# create dataset\nBATCH_SIZE = 32\nSHUFFLE_BUFFER_SIZE = 100\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\ntrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\ntest_dataset = test_dataset.batch(BATCH_SIZE)","ee5381b8":"# Helper functions\n\n\nclass MetricHelperClass(object):\n    def __init__(self,y_pred_prob, y_true, threshold = 0.5):\n        self.y_prob = y_pred_prob\n        self.y_true = y_true\n        self.threshold = threshold\n        self.y_pred = self.get_prediction(self.y_prob, self.threshold)\n        self.tn, self.fp, self.fn, self.tp = confusion_matrix(self.y_true, self.y_pred).ravel()\n\n    def get_prediction(self, y_prob, threshold):\n        y_pred = np.where(y_prob < threshold, 0, 1 )\n        return y_pred\n\n\n    def getTruePositiveRate(self):\n        return self.tp\/(self.tp+self.fn)\n\n    def getFalsePositiveRate(self):\n        return self.fp\/(self.tn+self.fp)\n\n    def precision(self):\n        return self.tp\/(self.tp+self.fp)\n\n    def recall(self):\n        return self.tp\/(self.tp+self.fn)\n\n    def F1score(self):\n        p = self.precision()\n        r = self.recall()\n        return 2*p*r \/( p+r)\n\n\n\ndef AUC(y_true, y_pred_prob):\n    sorted_y_prob = sorted(y_pred_prob, reverse=True)\n\n    thresholds = thresholdList = np.unique(sorted_y_prob).tolist()\n    limit = max(200, len(thresholds))\n\n    fprList = []\n    tprList = []\n\n    for threshold in thresholds[:limit]:\n        confusion_metric = MetricHelperClass(y_pred_prob, y_true, threshold)\n        y_threshold_pred = confusion_metric.get_prediction(y_pred_prob, threshold)\n        fpr = confusion_metric.getFalsePositiveRate()\n        tpr = confusion_metric.getTruePositiveRate()\n        fprList.append(fpr)\n        tprList.append(tpr)\n  \n    auc = np.trapz(sorted(tprList), sorted(fprList))\n    return auc\n\n","3bed3dd9":"\nclass CustomModelCheckPoint(tf.keras.callbacks.Callback):\n    def __init__(self,basepath, monitor):\n        self.path = basepath\n        self.monitor = monitor\n      \n    def on_train_begin(self, logs={}):\n        ## on begin of training, we are creating a instance varible called history\n        ## it is a dict with keys [loss, acc, val_loss, val_acc]\n        self.history={'loss': [],'accuracy': [],'val_loss': [],'val_accuracy': []}\n        self.best = -np.inf\n\n        \n    def on_epoch_end(self, epoch, logs={}):\n        if logs.get('val_accuracy', -1) != -1:\n            self.history['val_accuracy'].append(logs.get('val_accuracy'))\n        current = logs.get('val_accuracy')\n        if np.greater(current, self.best):\n            self.best = current\n            filepath = self.path + \"-epoch:{}-val_accuracy:{}.hdf5\".format(epoch, logs['val_accuracy'])\n            tf.keras.models.save_model(self.model, filepath, overwrite=True, include_optimizer=True, save_format='h5' )\n\n","255d1abb":"class MetricTracker(tf.keras.callbacks.Callback):\n    def __init__(self,trainX,trainY,testX, testY):\n        self.x_test = testX\n        self.y_test= testY\n        self.x_train = trainX\n        self.y_train = trainY\n      \n    def on_train_begin(self, logs={}):\n        ## on begin of training, we are creating a instance varible called history\n        ## it is a dict with keys [loss, acc, val_loss, val_acc]\n        self.history={'loss': [],'accuracy': [],'val_loss': [],'val_accuracy': [],'val_recall': [],  'val_f1':[],  'val_auc':[]}\n        \n    def on_epoch_end(self, epoch, logs={}):\n        \n        y_test_pred_prob = self.model.predict(self.x_test).squeeze()\n\n        # f1 score\n        test_cm = MetricHelperClass(y_test_pred_prob, y_test, threshold=0.5)\n        test_f1 = test_cm.F1score()\n        self.history['val_f1'].append(test_f1)\n\n        # AUC score\n        # test_auc = AUC(y_test, y_test_pred_prob)\n        test_auc = roc_auc_score(y_test, y_test_pred_prob )\n        self.history['val_auc'].append(test_auc)\n        print(\" validation f1 score : {}, validation auc score : {} \".format( test_f1, test_auc))\n            \n","bcbdd291":"# Ref https:\/\/www.tensorflow.org\/guide\/keras\/custom_callback\nclass CustomLRScheduler(tf.keras.callbacks.Callback):\n\n    def __init__(self,scheduler):\n        super(CustomLRScheduler, self).__init__()\n        self.scheduler = scheduler\n        self.prev_accuracy = -np.inf\n  \n\n    def on_train_begin(self, logs={}):\n        ## on begin of training, we are creating a instance varible called history\n        ## it is a dict with keys [loss, acc, val_loss, val_acc]\n        self.history={'loss': [],'accuracy': [],'val_loss': [],'val_accuracy': [],'val_recall': [],  'val_f1':[],  'val_auc':[]}\n        \n    def on_epoch_end(self, epoch, logs={}):\n        if not hasattr(self.model.optimizer, \"lr\"):\n              raise ValueError('Optimizer must have a \"lr\" attribute.')\n        if logs.get('val_accuracy',-1) != -1:\n            self.history['val_accuracy'] = logs.get('val_accuracy')\n\n        current = logs.get('val_accuracy')\n        # Get the current learning rate from model's optimizer.\n        current_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n        # Call Scheduler to get updated learning rate \n        scheduled_lr = self.scheduler(epoch, current, self.prev_accuracy, current_lr)\n        # Set the value back to the optimizer before this epoch starts\n        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n        print(\" Epoch %03d: Learning rate is %6.7f.\" % (epoch, scheduled_lr))\n        # update prev_accuracy as current \n        self.prev_accuracy = current\n\n\ndef custom_learning_rate_scheduler(epoch, current_acc, prev_acc, lr):\n    #  Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the learning rate by 10%. \n    if np.less(current_acc, prev_acc):\n        lr =  0.9 * lr    \n    #   Cond2. For every 3rd epoch, decay your learning rate by 5%.\n    if (epoch + 1) % 3 == 0:\n        lr = lr * 0.95\n    return lr","5c247cbd":"# Ref https:\/\/www.tensorflow.org\/guide\/keras\/custom_callback\nclass EarlyStoppingAtDecreasingValidationScore(tf.keras.callbacks.Callback):\n  \n    def __init__(self, patience=0):\n        super(EarlyStoppingAtDecreasingValidationScore, self).__init__()\n        self.patience = patience\n        \n\n    def on_train_begin(self, logs=None):\n        # The number of epoch it has waited when accuracy is not improved\n        self.wait = 0\n        # The epoch the training stops at.\n        self.stopped_epoch = 0\n        # Initialize the prev validation accuracy as -inf\n        self.prev_accuracy = -np.inf\n        # best_weights to store the weights at which the minimum loss occurs.\n        self.best_weights = None\n\n    def on_epoch_end(self, epoch, logs=None):\n        current_accuacy = logs.get(\"val_accuracy\")\n\n        # check for early stopping criteria for validation accuracy metric\n        # if current accuracy is improved than previous accuracy reintialize wait to 0\n        if np.greater(current_accuacy, self.prev_accuracy):\n            self.wait = 0\n            # Record the best weights if current results is better (less).\n            self.best_weights = self.model.get_weights()\n        else:\n        #  if current accuracy is  not improved than previous accuracy increment wait count\n        # if wait counter becomes greater than patience\n            self.wait += 1\n            if self.wait > self.patience:\n                # training stop at epoch number\n                self.stopped_epoch = epoch\n                # stop training falg should be set to true for stopped training\n                self.model.stop_training = True\n                print(\"Restoring model weights from the end of the best epoch.\")\n                self.model.set_weights(self.best_weights)\n        # reset prev_accuracy as current accuracy\n        self.prev_accuracy = current_accuacy\n\n    def on_train_end(self, logs=None):\n        if self.stopped_epoch > 0:\n            print(\"Epoch %03d: early stopping\" % (self.stopped_epoch + 1))\n\n","8620704c":"class TerminateAtNaNLossOrWeights(tf.keras.callbacks.Callback):\n    def __init__(self):\n        super(TerminateAtNaNLossOrWeights, self).__init__()\n        \n    def on_epoch_end(self, epoch, logs={}):\n        loss = logs.get('loss')\n\n        if loss is not None:\n            # if loss is nan or infinity stop training\n            if np.isnan(loss) or np.isinf(loss):\n                print(\"Invalid loss and terminated at epoch {}\".format(epoch))\n                self.model.stop_training = True\n        # check for weights\n        for weight in self.model.weights:\n            # if weigths become nan or infinity stop training\n            if np.isnan(weight).any() or np.isinf(weight).any():\n                print(\"Invalid wieghts  and terminated at epoch {}\".format(epoch))\n                self.model.stop_training = True\n\ntermination = TerminateAtNaNLossOrWeights()","4e78066c":"# Callbacks\n\n\n# Custom Callback to track AUC and F1 score\ncustomMetricCallback = MetricTracker(X_train, y_train, X_test, y_test) \n \n# Custom Callback for Learning Rate schedular\nlrscheduler = CustomLRScheduler(custom_learning_rate_scheduler)\n# Custom Callback for early stopping if validation accuracy is not improved based on specefic number of epoch\nearlystopping = EarlyStoppingAtDecreasingValidationScore(patience=2)\n# Custom callback to terminate trainning if weights or loss are invalid\nterminationAtNan = TerminateAtNaNLossOrWeights()\n","0df69e6f":"# model 1 \nmodel1 = create_model(inp_dim= X_train.shape[-1], out_dim = 1, inpt_activation = 'tanh',out_activation= 'sigmoid', kernel_init=RandomUniform)\nmodel1.summary()\n# SGD Optimizer\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n# tensorboard log directory\nlog_dir = os.path.join(\"logs_model_1\",'fits', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n# base path to store weights\nbasePath = 'model_1_save\/weights'\n\n# Custom Callback to store model \nmodel_checkpoint_callback = CustomModelCheckPoint(basePath,'val_loss') \n\n# Tensotboard callback to visualize the network parameters         \ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n\nmodel1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer=optimizer,\n              metrics=['accuracy'],\n              )\nmodel1.fit(train_dataset,epochs=10,validation_data=test_dataset,batch_size=64,\n          callbacks=[\n                      tensorboard_callback,\n                      customMetricCallback,\n                      model_checkpoint_callback,\n                      lrscheduler,\n                      earlystopping,\n                      terminationAtNan\n                     \n                     ])","e729aad4":"# %load_ext tensorboard\n# # Clear any logs from previous runs\n# !rm -rf .\/logs\/ \n\n# model 1 logs in tensorboard\n# !kill 172\n# %tensorboard --logdir logs_model_1\/fits","7147e5a0":"# model 2\nmodel2 = create_model(inp_dim= X_train.shape[-1], out_dim = 1, inpt_activation = 'relu',out_activation= 'sigmoid', kernel_init=RandomUniform)\n# SGD Optimizer\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n# tensorboard log directory\nlog_dir = os.path.join(\"logs_model_2\",'fits', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n# base path to store weights\nbasePath = 'model_2_save\/weights'\n\n\n\n# Custom Callback to store model \nmodel_checkpoint_callback = CustomModelCheckPoint(basePath,'val_loss') \n\n# Tensotboard callback to visualize the network parameters         \ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n\n\nmodel2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer=optimizer,\n              metrics=['accuracy'],\n              )\nmodel2.fit(train_dataset,epochs=10,validation_data=test_dataset,batch_size=64,\n          callbacks=[\n                      tensorboard_callback,\n                      customMetricCallback,\n                      model_checkpoint_callback,\n                      lrscheduler,\n                      earlystopping,\n                      terminationAtNan\n                     \n                     ])","4558e4f7":"# model 3\nmodel3 = create_model(inp_dim= X_train.shape[-1], out_dim = 1, inpt_activation = 'relu',out_activation= 'sigmoid', kernel_init=HeUniform)\n# SGD Optimizer\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n# tensorboard log directory\nlog_dir = os.path.join(\"logs_model_3\",'fits', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n# base path to store weights\nbasePath = 'model_3_save\/weights'\n\n\n# Custom Callback to store model \nmodel_checkpoint_callback = CustomModelCheckPoint(basePath,'val_loss') \n\n# Tensotboard callback to visualize the network parameters         \ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n\nmodel3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer=optimizer,\n              metrics=['accuracy'],\n              )\nmodel3.fit(train_dataset,epochs=10,validation_data=test_dataset,batch_size=64,\n          callbacks=[\n                      tensorboard_callback,\n                      customMetricCallback,\n                      model_checkpoint_callback,\n                      lrscheduler,\n                      earlystopping,\n                      terminationAtNan\n                     \n                     ])","08def1b5":"# model 4\nmodel4 = create_model(inp_dim= X_train.shape[-1], out_dim = 1, inpt_activation = 'relu',out_activation= 'sigmoid', kernel_init=HeNormal)\n# SGD Optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n# tensorboard log directory\nlog_dir = os.path.join(\"logs_model_4\",'fits', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n# base path to store weights\nbasePath = 'model_4_save\/weights'\n\n# Custom Callback to store model \nmodel_checkpoint_callback = CustomModelCheckPoint(basePath,'val_loss')  \n# Tensotboard callback to visualize the network parameters         \ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n\n\n\nmodel4.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer=optimizer,\n              metrics=['accuracy'],\n              )\nmodel4.fit(train_dataset,epochs=10,validation_data=test_dataset,batch_size=64,\n          callbacks=[\n                      tensorboard_callback,\n                      customMetricCallback,\n                      model_checkpoint_callback,\n                      lrscheduler,\n                      earlystopping,\n                      terminationAtNan\n                     \n                     ])","10bcbe3f":"Model-2 <font color='#800028'>[relu activation in layers, Random Uniform intialization]\n    \n    1. relu activation with uniform random seems not working and validation loss and accuracy Plateau.\n    2. variation in weights and bias are uniform because random uniform initialization. \n    3. Because of uniform random intialization, neural network starts with initial state that is not optimal to get the better result.\n    4. Initialization is very important for neural network because in neural network our aim is to reach to\n       local minimum loss surface which is close to global minimum loss surface.\n       If we don't start with good initial position we will never reach to good optimum loss surface.","d3e3252d":" Model-1 <font color='#800028'> [ tanh activation in layers, Random Uniform intialization] <\/font>\n\n    1. tanh activation with uniform random seems not working and validation loss and accuracy Plateau.\n    2. variation in weights and bias are uniform because random uniform initialization. \n    3. Because of uniform random intialization, neural network starts with initial state that is not optimal to get the better result.\n","06057349":"# <font color='#800028'> <b> 2. Model <\/b> <\/font>","84cc3e40":"Model-3 <font color='#800028'>[relu activation in layers, He Uniform intialization]\n    \n    1. relu activation with He Uniform seems  working good, validation loss and accuracy is improving better than previous model.\n    2. variation in weights and bias are farely good with mean 0 and very small variance.\n    3. Initialization is very important for neural network because in neural network our aim is to reach to\n       local minimum loss surface which is close to global minimum loss surface.\n       If we don't start with good initial position we will never reach to good optimum loss surface.\n    4. Based on research Deep learning community recommended relu with He normal intialization which also supported by our model\n       distribution and result.","4f96e828":"# <font color='#800028'> <b>9. Model Training with Custom Callback<\/b> <\/font>","bee91963":"# <font color='800028'> <b>3. Writing Callbacks <\/b> <\/font>\n## You have to implement the following callbacks\n-  Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.Do not use tf.keras.metrics for calculating AUC and F1 score.\n\n- Save your model at every epoch if your validation accuracy is improved from previous epoch. \n\n- You have to decay learning based on below conditions \n        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n               learning rate by 10%. \n        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n        \n- If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n\n- You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n\n- Use tensorboard for every model and analyse your scalar plots and histograms. (you need to upload the screenshots and write the observations for each model for evaluation)\n\n","837b4bce":"# <font color='#800028'> <b>1. Data Distribution  <\/b> <\/font>\n","13264ad3":"\n\n\n<pre>\n<b>Model-4<\/b>\n<pre>\n1. Use relu as an activation for every layer except output layer.\n2. use Adam as optimizer.\n3. use he_normal() as initilizer.\n3. Analyze your output and training process.  \n<\/pre>\n<\/pre>","47f6a88b":"Model-4 <font color='#800028'>[relu activation in layers, He Normal intialization]\n    \n    1. performance of relu activation with He Normal seems  similar to relu with He Uniform,  validation loss and accuracy is improving better than previous model.\n    2. variation in weights and bias are farely good with mean 0 and very small variance.","46dcbe1d":"<pre>\n<b>Model-3<\/b>\n<pre>\n1. Use relu as an activation for every layer except output layer.\n2. use SGD with momentum as optimizer.\n3. use he_uniform() as initilizer.\n3. Analyze your output and training process. \n<\/pre>\n<\/pre>","078c969f":"# <font color='#800028'> <b>4. ModelCheckPoint Callback <\/b> <\/font>\n","4021ec02":"<pre>\n<b>Model-1<\/b>\n<pre>\n1. Use tanh as an activation for every layer except output layer.\n2. use SGD with momentum as optimizer.\n3. use RandomUniform(0,1) as initilizer.\n3. Analyze your output and training process. \n<\/pre>\n<\/pre>","de27cede":"# <font color='#800028'> <b>6. Custom LearningRateScheduler Callback<\/b> <\/font>","a90b79f7":"<pre>\n<b>Model-2<\/b>\n<pre>\n1. Use relu as an activation for every layer except output layer.\n2. use SGD with momentum as optimizer.\n3. use RandomUniform(0,1) as initilizer.\n3. Analyze your output and training process. \n<\/pre>\n<\/pre>","591277a3":"# <font color='#800028'> <b>10. Analysis and Conclusion <\/b> <\/font>\n\n","1368f4de":"\n### 1. Download the data from <a href='https:\/\/drive.google.com\/file\/d\/15dCNcmKskcFVjs7R0ElQkR61Ex53uJpM\/view?usp=sharing'>here<\/a>. You have to use data.csv file for this assignment\n### 2. Code the model to classify data like below image. You can use any number of units in your Dense layers.\n\n<img src='https:\/\/i.imgur.com\/33ptOFy.png'>\n\n","a98afb6f":"# <font color='#800028'> <b>8. TerminateAtNan Callback<\/b> <\/font>","f255e853":"# <font color='#800028'> <b>7. EarlyStopping Callback<\/b> <\/font>","6f497dd5":"# <font color='#800028'> <b>5. Metric Tracker Callback<\/b> <\/font>"}}