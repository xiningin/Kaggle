{"cell_type":{"28530279":"code","1b574663":"code","c220af71":"code","a391eae9":"code","296a0802":"code","91363f87":"code","92024215":"code","53295abf":"code","95e67e10":"code","8efe4459":"code","4f148b52":"code","6dc3e2fb":"code","14eefb79":"code","319c62ff":"code","5840ceca":"code","cfe9f403":"code","cf36c129":"code","35d0cb64":"code","60298d99":"code","7c9c01e9":"code","fa3c0e90":"code","0959a9e1":"code","5874d9c0":"code","4e547fa1":"code","0ddfc62d":"code","02d63728":"markdown","41be5f56":"markdown","63331e4a":"markdown","3fa45f1f":"markdown","8706ddc0":"markdown","434e73ba":"markdown","9ec6f590":"markdown","a7a8db27":"markdown"},"source":{"28530279":"#import libraries\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn import preprocessing\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)","1b574663":"%%time\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")","c220af71":"traintest = pd.concat([train,test],axis=0).reset_index(drop=True)\ntraintest = traintest.drop(['id'],axis=1)\nprint(\"traintest.shape:\",traintest.shape)\ntraintest.head()","a391eae9":"#Memory reduction\nfor c, dtype in zip(traintest.columns, traintest.dtypes):\n    if dtype == np.float64:\n        traintest[c] = traintest[c].astype(np.float32)\n    elif dtype == np.int64:\n        traintest[c] = traintest[c].astype(np.int32)","296a0802":"print(\"train.shape:\",train.shape)\nprint(\"test.shape:\",test.shape)","91363f87":"#Standard scaling\nfor col in traintest.drop(['target'],axis=1).columns:\n    traintest[col] = preprocessing.scale(traintest[col])","92024215":"train = traintest.iloc[:train.shape[0]]\ntest = traintest.iloc[train.shape[0]:]","53295abf":"target_col = \"target\"\nfeature_cols = [col for col in train.columns if col not in target_col]","95e67e10":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nRFC = RandomForestClassifier(random_state=42)\n\nX = train[feature_cols]\ny = train[target_col].replace({False:0,True:1})\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nRFC.fit(X,y)\nimportances = RFC.feature_importances_\ndf_fi_top10=pd.DataFrame({\"feature_cols\":feature_cols,\"importances\":importances}).sort_values(by=\"importances\",ascending=False).reset_index(drop=True).head(10)\n\nplt.bar(df_fi_top10.feature_cols,df_fi_top10.importances,align='center')\nplt.title('Feature Importance')\nplt.show()","8efe4459":"%%time\ndf = train\n\ncat_features = [col for col in feature_cols if df[col].nunique() < 25]\ncont_features = [col for col in feature_cols if df[col].nunique() >= 25]\n\nprint(\"--- Distribution train vs test ---\")\nncols = 5\nnrows = int(len(cont_features) \/ ncols + (len(feature_cols) % ncols > 0))\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 150), facecolor='#FFFFFF')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = cont_features[r*ncols+c]\n        sns.kdeplot(x=train[col], ax=axes[r, c], color='#00ffff', label='Train data')\n        sns.kdeplot(x=test[col], ax=axes[r, c], color='#ffa64d', label='Test data')\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","4f148b52":"%%time\ndf = train\n\ncat_features = [col for col in feature_cols if df[col].nunique() < 25]\ncont_features = [col for col in feature_cols if df[col].nunique() >= 25]\n\nprint(\"--- Train distribution target:True vs target:False ---\")\nncols = 5\nnrows = int(len(cont_features) \/ ncols + (len(feature_cols) % ncols > 0))\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 150), facecolor='#FFFFFF')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = cont_features[r*ncols+c]\n        sns.kdeplot(x=train[col][train['target']==True], ax=axes[r, c], color='#00ffff', label='Train:True')\n        sns.kdeplot(x=train[col][train['target']==False], ax=axes[r, c], color='#ffa64d', label='Train:False')\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","6dc3e2fb":"%%time\n!pip install pycaret --ignore-installed llvmlite","14eefb79":"%%time\n# install lightgbm GPU\n#Running LightGBM on GPU https:\/\/www.kaggle.com\/abhishek\/running-lightgbm-on-gpu\n!pip uninstall -y lightgbm\n!apt-get install -y libboost-all-dev\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM","319c62ff":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","5840ceca":"!cd LightGBM\/python-package\/;python setup.py install --precompile","cfe9f403":"!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","cf36c129":"target_col = \"target\"\nfeature_cols = ['f34','f55','f43','f71','f91']\nexc_cols = ['f34','f55','f43','f71','f91','target']\nignore_features = [col for col in train.columns if col not in exc_cols]","35d0cb64":"%%time\n# initialize the setup\nfrom pycaret.classification import *\nexp_name = setup(data = train,  target = 'target',numeric_features=['f34','f55','f43','f71','f91'],ignore_features=ignore_features, use_gpu = True)","60298d99":"#Check models which allow to use GPU\nmodels(internal=True)[['Name', 'GPU Enabled']]","7c9c01e9":"# check all metrics used for model evaluation\nget_metrics()\n# add Log Loss metric in pycaret\nfrom sklearn.metrics import log_loss\nadd_metric('logloss', 'LogLoss', log_loss, greater_is_better=False)","fa3c0e90":"%%time\n# compare baseline models\nbest = compare_models(fold=10,sort='AUC')","0959a9e1":"%%time\nlr = create_model('lr')\ntuned_lr = tune_model(lr, n_iter = 500, optimize = 'AUC')\nprint(tuned_lr)","5874d9c0":"%%time\nevaluate_model(tuned_lr)","4e547fa1":"%%time\nnb = create_model('nb')\ntuned_nb = tune_model(nb, n_iter = 500, optimize = 'AUC')\nprint(tuned_nb)","0ddfc62d":"%%time\nevaluate_model(tuned_nb)","02d63728":"# Distribution between True and False\n\nLet's see <font color=\"red\"><B>'f34','f55','f43','f71','f91' features <\/B><\/font> if we can find differences between True and False.<BR>","41be5f56":"# Distribution train vs test\n\nI learned from [this](https:\/\/www.kaggle.com\/vishwas21\/tps-oct-21-eda-modeling) last month.<BR>\nThank you very much<BR>\n<BR>\nLet's see the feature distributions bewteen train and test.<BR>\nI assume there is not big difference.\n<BR>\nIt looks there are 2 types of distributions among features. One has two mountains and the other one is concentrated around 0.","63331e4a":"So, shall we start to dive into data?<BR>\n<BR>\n![](https:\/\/images.unsplash.com\/photo-1427751840561-9852520f8ce8?ixid=MnwxMjA3fDB8MHxzZWFyY2h8MTR8fGFuYWx5c2lzfGVufDB8fDB8fA%3D%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=500&q=60)","3fa45f1f":"I will update more later.<BR>\nThank you very much.","8706ddc0":"# Load Data and Memory reduction\n","434e73ba":"# Create Model","9ec6f590":"# RandomForest Feature importances \n\n\nIt seems feature f34, f55, f43, f71, f91 are important according to RandomForest.","a7a8db27":"# Introduction\n\nThis is the first EDA book for Tabular Playground Series - Nov 2021.<BR>\nIn this notebook, I mainly would like to look through features distributions in this notebook.<BR>\n<BR>\n* Since the data size is large, it is necessary to use the memory effectively.\n* There are 100 features. All features are Continuous. There are no NaN.\n* target is <font color=\"red\"><B>balanced<\/B><\/font>\n* It seems <font color=\"red\"><B>'f34','f55','f43','f71','f91' features <\/B><\/font> are important."}}