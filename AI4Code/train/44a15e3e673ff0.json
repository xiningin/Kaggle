{"cell_type":{"19651e66":"code","ace5af7c":"code","8b571cea":"code","4089cbf3":"code","9138e274":"code","6fef8f19":"code","664afe46":"code","cfb1179a":"code","9d0757eb":"code","f1c252a2":"code","09a5949a":"code","42270841":"code","2109d42c":"code","afa7c01d":"code","7fadea8a":"code","7692b0ac":"code","64df3c43":"code","1bc5e164":"code","f7ea9979":"code","1a955bd0":"code","fe915510":"code","c98f77a8":"code","1ba0875e":"code","7b78d43f":"code","4c05c93b":"code","fe0d3b3b":"code","4cf0ebda":"code","23e33907":"code","7c0fbde3":"code","1c10e5fc":"code","84c234f7":"code","5e324145":"code","debe802d":"code","755b21f5":"code","a9622686":"code","ebeba5e6":"code","24239160":"code","1db20e04":"code","75fc90a6":"code","f610d021":"code","32f9fdfd":"code","74d6b39b":"code","4cf73159":"markdown","bdbd8517":"markdown","88320539":"markdown","e2e2ef23":"markdown","af774bd3":"markdown","3774d239":"markdown","3686aabf":"markdown","64b17c24":"markdown","482b27d7":"markdown","25384d3d":"markdown","8594ec30":"markdown","193ad37d":"markdown","af18e4ac":"markdown","dc9cdfc1":"markdown","72f208f3":"markdown","7c03b151":"markdown","8e98b5c0":"markdown","841cdd95":"markdown","79dca903":"markdown","0a7303ff":"markdown"},"source":{"19651e66":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","ace5af7c":"df = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\n\ndf['date'] = pd.to_datetime(df['date'],format='%d.%m.%Y')\ndf = pd.merge(df,items[['item_id','item_category_id']], how='left', on='item_id')\ndf.sort_values(by='date',ascending=True, inplace=True)\n# df.set_index('date', inplace=True)\ndf['month'] = df['date'].dt.month\n\ndf.head(5)","8b571cea":"cat = ['date_block_num','shop_id', 'item_id','item_category_id','month'] # categorical features\ncont = ['item_price'] # continuous features\noutput = 'item_cnt_day'","4089cbf3":"df_agg = df.groupby(cat).agg({cont[0]:np.mean,output:np.sum}).reset_index() \ndel df","9138e274":"df_agg.info()","6fef8f19":"# fitting multiple k-means algorithms and storing the values in an empty list\nlist_cluster = ['shop_id','item_id','item_category_id','item_price']\nSSE = []\nfor cluster in range(1,20):\n    kmeans = MiniBatchKMeans(n_clusters = cluster, init='k-means++')\n    kmeans.fit(df_agg[list_cluster])\n    SSE.append(kmeans.inertia_)\n\n# converting the results into a dataframe and plotting them\nframe = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\nplt.figure(figsize=(12,6))\nplt.plot(frame['Cluster'], frame['SSE'], marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')","664afe46":"kmeans = MiniBatchKMeans(n_clusters = 6, init='k-means++')\nkmeans.fit(df_agg[list_cluster])\nnew_agg = kmeans.predict(df_agg[list_cluster])\ndf_agg['new_agg'] = new_agg","cfb1179a":"new_cat = ['date_block_num','shop_id', 'new_agg','item_category_id','month'] # categorical features","9d0757eb":"df_agg_level_2 = df_agg.groupby(new_cat).agg({cont[0]:np.mean,output:np.sum}).reset_index() ","f1c252a2":"from sklearn.preprocessing import LabelEncoder\n\n# Append number max + 1 (month)\nlist_dfs = [df_agg_level_2.append(pd.DataFrame({'date_block_num':[df_agg_level_2['date_block_num'].max() + 1]})), \n            shops, \n            df_agg_level_2, \n            item_categories,\n            df_agg_level_2]\n\nlabel_encoders = {}\n\nfor i, cat_col in enumerate(new_cat):\n    label_encoders[cat_col] = LabelEncoder()\n    label_encoders[cat_col].fit(list_dfs[i][cat_col])\n    df_agg_level_2[cat_col] = label_encoders[cat_col].transform(df_agg_level_2[cat_col])","09a5949a":"# set index from past values\ndef get_index(*args):\n    index = \"\"\n    for word in args:\n        index += str(word) + '_'\n    return index \n\ndf_agg_level_2['index_act'] = np.vectorize(get_index)(df_agg_level_2['date_block_num'], \n                                                      df_agg_level_2['shop_id'], \n                                                      df_agg_level_2['new_agg'], \n                                                      df_agg_level_2['item_category_id'])","42270841":"n_lags = 24 # define number of lags, might it be a hyperparameter\n\nfor i in range(n_lags) : \n    print(i)\n    new_idx = 'index_ant_' + str(i + 1)\n    df_agg_level_2[new_idx] = np.vectorize(get_index)(df_agg_level_2['date_block_num'] + i + 1, \n                                                      df_agg_level_2['shop_id'], \n                                                      df_agg_level_2['new_agg'],\n                                                      df_agg_level_2['item_category_id'])\n    \n    df_agg_level_2 = pd.merge(df_agg_level_2[[i for i in df_agg_level_2.columns if i not in [new_idx]]],\n                              df_agg_level_2[[output,new_idx]].rename(columns={output: output + '_t_' +  str(i + 1) }),\n                              how='left',\n                              left_on='index_act',\n                              right_on=new_idx)\n        \n    df_agg_level_2[output + '_t_' +  str(i + 1)].fillna(0,inplace=True)\n    print('Length of dataframe: {}'.format(str(len(df_agg_level_2))))\n\n    cont.append(output + '_t_' +  str(i + 1)) # append new features to my continuous list\n\n    del df_agg_level_2[new_idx]\ndel df_agg_level_2['index_act']","2109d42c":"train = df_agg_level_2.loc[df_agg_level_2['date_block_num']>n_lags-1].copy() # perform delete inplace requiere a copy DF\ntest = train.sample(frac=0.2,random_state=101)\ntrain.drop(test.index,inplace=True, axis=0)","afa7c01d":"print('Number of samples in train: {}'.format(len(train)))\nprint('Number of samples in test: {}'.format(len(test)))","7fadea8a":"df_agg_level_2.info()","7692b0ac":"\nclass TabularDataset(Dataset):\n    def __init__(self, data, cat_cols=None, output_col=None):\n        \"\"\"\n        Characterizes a Dataset for PyTorch\n\n        Parameters\n        ----------\n\n        data: pandas data frame\n        The data frame object for the input data. It must\n        contain all the continuous, categorical and the\n        output columns to be used.\n\n        cat_cols: List of strings\n        The names of the categorical columns in the data.\n        These columns will be passed through the embedding\n        layers in the model. These columns must be\n        label encoded beforehand. \n\n        output_col: string\n        The name of the output variable column in the data\n        provided.\n        \"\"\"\n\n        self.n = data.shape[0]\n\n        if output_col:\n            self.y = data[output_col].astype(np.float32).values.reshape(-1, 1)\n        else:\n            self.y =  np.zeros((self.n, 1))\n\n        self.cat_cols = cat_cols if cat_cols else []\n        self.cont_cols = [col for col in data.columns if col not in self.cat_cols + [output_col]]\n\n        if self.cont_cols:\n            self.cont_X = data[self.cont_cols].astype(np.float32).values\n        else:\n            self.cont_X = np.zeros((self.n, 1))\n\n        if self.cat_cols:\n            self.cat_X = data[cat_cols].astype(np.int64).values\n        else:\n            self.cat_X =  np.zeros((self.n, 1))\n\n    def __len__(self):\n        \"\"\"\n        Denotes the total number of samples.\n        \"\"\"\n        return self.n\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Generates one sample of data.\n        \"\"\"\n        return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]","64df3c43":"cat_dim = [len(label_encoders[l].classes_) for l in new_cat]\nemb_dim = [(x, min(50,(x+1)\/\/2)) for x in cat_dim]","1bc5e164":"emb_dim","f7ea9979":"\"\"\"\nclass TabularModel(nn.Module):\n    def __init__(self,emb_size,n_cont,out_size,layers, hidden_lstm,p):\n        \n        super().__init__()\n        \n        self.embeds = nn.ModuleList([nn.Embedding(ns,nd) for ns, nd in emb_size]) \n        self.emb_drop = nn.Dropout(p)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        self.hidden_lstm = hidden_lstm\n\n        layerlist = []\n        n_emb = sum((nd for ns,nd in emb_size))\n        n_in = n_emb + n_cont\n\n        self.n_in = n_in\n        self.conv1d_1 = nn.Conv1d(n_in,32,kernel_size=1)\n        self.conv1d_2 = nn.Conv1d(32,64,kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n\n        # Add an LSTM layer:\n        self.lstm = nn.LSTM(64,hidden_lstm)      \n        self.linear = nn.Linear(hidden_lstm,out_size)        \n        self.hidden = (torch.zeros(1,1,hidden_lstm),\n                       torch.zeros(1,1,hidden_lstm))\n\n    def forward(self,x_cont,x_cat):\n        embeddings = []\n        for i, e in enumerate(self.embeds) : \n            embeddings.append(e(x_cat[:,i]))\n        x = torch.cat(embeddings,1)\n        x = self.emb_drop(x)\n\n        x_cont = self.bn_cont(x_cont)\n        x = torch.cat([x,x_cont],1)\n         \n        x = x.view(-1,self.n_in,1)\n        x = self.conv1d_1(x)  \n        x = self.relu(x)  \n        x = self.conv1d_2(x)  \n        x = self.relu(x)  \n\n        lstm_out, self.hidden = self.lstm(x.view(len(x),1,-1), self.hidden)\n        pred = self.linear(lstm_out.view(len(x),-1))\n\n        return pred\n\"\"\"","1a955bd0":"class TabularModel(nn.Module):\n    \n    def __init__(self,emb_size,n_cont,out_size,layers, hidden_lstm,p):\n\n        \"\"\"\n        Tabular model embedding layer + Linear layer for continuous features\n\n        Parameters\n        ----------\n\n        emb_size: list of Tuples \n        Size of embedding layers\n\n        p: float\n        Percentage of dropout layers \n\n        n_cont: int\n        Number of continuous features\n\n        layers: list of Integers\n        Dimension of hidden layers\n\n        out_size: int \n        One dimension of output \n\n        \"\"\" \n\n\n        super().__init__()\n        \n        self.embeds = nn.ModuleList([nn.Embedding(ns,nd) for ns, nd in emb_size]) \n        self.emb_drop = nn.Dropout(p)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        self.hidden_lstm = hidden_lstm\n\n        layerlist = []\n        n_emb = sum((nd for ns,nd in emb_size))\n        n_in = n_emb + n_cont\n\n        for i in layers : \n            layerlist.append(nn.Linear(n_in,i))\n            layerlist.append(nn.ReLU(inplace=True))\n            layerlist.append(nn.BatchNorm1d(i))\n            layerlist.append(nn.Dropout(p))\n            n_in = i\n        layerlist.append(nn.Linear(layers[-1],out_size))\n        self.layers = nn.Sequential(*layerlist)\n\n    def forward(self,x_cont,x_cat):\n        embeddings = []\n        for i, e in enumerate(self.embeds) : \n            embeddings.append(e(x_cat[:,i]))\n        x = torch.cat(embeddings,1)\n        x = self.emb_drop(x)\n\n        x_cont = self.bn_cont(x_cont)\n        x = torch.cat([x,x_cont],1)\n        x = self.layers(x)  \n\n        return x\n","fe915510":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(100)\n# model = TabularModel(emb_dim, len(cont), 1, [500], 50, p=0.6).to(device)\nmodel = TabularModel(emb_dim, len(cont), 1, [200,100],50,p=0.5).to(device)","c98f77a8":"model","1ba0875e":"criterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)","7b78d43f":"batchsize_train = len(train)\ntrain_tabular = TabularDataset(train, cat_cols= new_cat, output_col=output)\ndataloader_train = DataLoader(train_tabular, batchsize_train, shuffle=True, num_workers=1)\n\nbatchsize_test = len(test)\ntest_tabular = TabularDataset(test, cat_cols= new_cat, output_col=output)\ndataloader_test = DataLoader(test_tabular, batchsize_test, shuffle=True, num_workers=1)","4c05c93b":"from warnings import filterwarnings\nfilterwarnings('ignore')\n\nimport time\nstart_time = time.time()\n\nepochs = 60\nlosses = []\ntest_losses = []\n\nfor i in range(epochs):\n    i += 1\n    for k, (y,x_cont,x_cat) in enumerate(dataloader_train): # vect_data -> y, cont_x, cat_x\n        k+=1\n        y = y.to(device)\n        x_cont = x_cont.to(device)\n        x_cat = x_cat.to(device)\n\n        y_pred = model(x_cont, x_cat)\n        loss = torch.sqrt(criterion(y_pred,y)) # RMSE\n        \n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    losses.append(loss)\n\n    train_loss = loss.item()\n\n    with torch.no_grad():\n        for k, (y,x_cont,x_cat) in enumerate(dataloader_test):\n\n            # Apply the model\n            y_val = model(x_cont, x_cat)\n    loss = torch.sqrt(criterion(y_val,y))\n    test_losses.append(loss)\n\n    if i%5 == 0:\n        print(f'epoch: {i:3} batch: {k:3}\/{(i+1)*len(dataloader_train):3}  loss: {train_loss:10.8f} validation: {loss.item():10.8f}')\n\n\nprint(f'epoch: {i:3}  loss: {loss.item():10.8f}') # print the last line\nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed","fe0d3b3b":"\n# We can attemp whit 48 EPOCHS\n\nplt.figure(figsize=(17,5))\nplt.plot(losses, label='training loss')\nplt.plot(test_losses, label='validation loss')\nplt.title('Loss at the end of each epoch')\nplt.legend();","4cf0ebda":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(100)\nfinal_model = TabularModel(emb_dim, len(cont), 1, [200,100],50,p=0.5).to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(final_model.parameters(), lr=0.01)","23e33907":"all_data = df_agg_level_2.loc[df_agg_level_2['date_block_num']>n_lags-1].copy()\n\nbatchsize_all = len(all_data)\nall_tabular = TabularDataset(all_data, cat_cols= new_cat, output_col=output)\ndataloader_all = DataLoader(all_tabular, batchsize_all, shuffle=True, num_workers=1)\n","7c0fbde3":"import time\nstart_time = time.time()\n\nepochs = 45\nlosses = []\n\nfor i in range(epochs):\n    i += 1\n    for k, (y,x_cont,x_cat) in enumerate(dataloader_all): # vect_data -> y, cont_x, cat_x\n        k+=1\n        y = y.to(device)\n        x_cont = x_cont.to(device)\n        x_cat = x_cat.to(device)\n        \n        y_pred = final_model(x_cont, x_cat)\n        loss = torch.sqrt(criterion(y_pred,y)) # RMSE\n        \n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    losses.append(loss)\n\n    if i%5 == 0:\n        print(f'epoch: {i:3} batch: {k:3}\/{(i+1)*len(dataloader_all):3}  loss: {loss.item():10.8f} ')\n","1c10e5fc":"plt.figure(figsize=(17,5))\nplt.plot(losses, label='training loss')\nplt.title('Loss at the end of each epoch')\nplt.legend();","84c234f7":"# Simple load of data test\n\ndf_test = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\ndf_test = pd.merge(df_test,items[['item_id','item_category_id']], how='left', on='item_id')\ndf_test['month'] = 11\ndf_test['date_block_num'] = df_agg_level_2['date_block_num'].max() + 1\n\n#Join whit dim_price to calculate clusters\n\ndim_price = df_agg.groupby(['item_id','item_category_id']).agg({'item_price':np.mean}).reset_index()\ndf_test = pd.merge(df_test,dim_price, how='left', on=['item_id','item_category_id'])\ndf_test['item_price'].fillna(0, inplace=True)\ndf_test.info()\n","5e324145":"\ndf_test['new_agg'] = kmeans.predict(df_test[list_cluster])\ndf_test.info()","debe802d":"# encode for embedding layers\nfor cat_col in new_cat:\n    df_test[cat_col] = label_encoders[cat_col].transform(df_test[cat_col])","755b21f5":"# Get indx to join whit test\ndf_agg['index_act'] = np.vectorize(get_index)(df_agg['date_block_num'], \n                                              df_agg['shop_id'], \n                                              df_agg['item_id'],\n                                              df_agg['item_category_id'])","a9622686":"n_lags = 24 # define number of lags, might it be a hyperparameter\n\nfor i in range(n_lags) : \n    print(i)\n    new_idx = 'index_ant_' + str(i + 1)\n    df_test[new_idx] = np.vectorize(get_index)(df_test['date_block_num'] - i - 1, \n                                               df_test['shop_id'], \n                                               df_test['item_id'],\n                                               df_test['item_category_id'])\n    \n    df_test = pd.merge(df_test,\n                       df_agg[[output,'index_act']].rename(columns={output: output + '_t_' +  str(i + 1) }),\n                       how='left',\n                       left_on=new_idx,\n                       right_on='index_act')\n        \n    df_test[output + '_t_' +  str(i + 1)].fillna(0,inplace=True)\n#     df_agg.drop(df_agg.loc[df_agg['date_block_num']<=i].index,axis=0,inplace=True)\n    print('Length of dataframe: {}'.format(str(len(df_test))))\n\n#    cont.append(output + '_t_' +  str(i + 1)) # append new features to my continuous list\n\n    del df_test[new_idx]\n    del df_test['index_act']\ndel df_agg['index_act']\n","ebeba5e6":"test_final = df_test[new_cat + cont]\nbatchsize_test = len(test_final)\n\ntest_tabular = TabularDataset(test_final, cat_cols= new_cat)\ndataloader_test = DataLoader(test_tabular, batchsize_test, shuffle=True, num_workers=1)","24239160":"for k, (y,x_cont,x_cat) in enumerate(dataloader_test):\n    break","1db20e04":"final_model.eval()\nwith torch.no_grad() : \n    for k, (y,x_cont,x_cat) in enumerate(dataloader_test):\n            # Apply the model\n            y_val = final_model(x_cont, x_cat)            ","75fc90a6":"y_val_clip = y_val.clip(min=0,max=y_val.max().item())\ny_val_clip_round = y_val_clip.round().numpy()","f610d021":"df_test['item_cnt_month'] = y_val_clip_round","32f9fdfd":"print('0: ' + str(df_test['item_cnt_month'].sum()))\nprint('1: ' + str(df_test['item_cnt_day_t_1'].sum()))\nprint('2: ' + str(df_test['item_cnt_day_t_2'].sum()))\nprint('12: ' + str(df_test['item_cnt_day_t_12'].sum()))","74d6b39b":"df_test[['ID','item_cnt_month']].to_csv('sample_submission.csv', index=False)","4cf73159":"Little validation.","bdbd8517":"Train model.","88320539":"Encode to int categorical features.","e2e2ef23":"Calculate clusters.","af774bd3":"Define categorical and continuous features.","3774d239":"Get windows of 24 months.","3686aabf":"Fit model whit all data.","64b17c24":"Claculate clusters to get patterns in any high dimensional categorical features.","482b27d7":"Get values (price and item category) for test.","25384d3d":"Encode test.","8594ec30":"Create tabular dataset for dataloaders.","193ad37d":"Get windows for test.","af18e4ac":"Create tabulra model using torch.","dc9cdfc1":"Split data, 20% for test.","72f208f3":"Calculate predictions whit final model.","7c03b151":"Get embeddings dimension.","8e98b5c0":"Get sample submission.","841cdd95":"Get monthly level by agg over dates.","79dca903":"Define model to train.","0a7303ff":"Load data and get correct format."}}