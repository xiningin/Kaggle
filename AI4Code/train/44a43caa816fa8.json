{"cell_type":{"7d0371e5":"code","a32c9c8f":"code","0a9bc3aa":"code","b7fb6371":"code","179fe4d2":"code","787174f2":"code","e04aa237":"code","fe4887c7":"code","7cc62d4f":"code","0e04b2b0":"code","66f4d0ed":"code","442f7a29":"code","408ea498":"code","053396e6":"code","54218353":"code","8db993a2":"code","a76a833a":"code","845c6256":"code","aa20c17e":"code","eba5bc4b":"code","5b9abe0d":"code","e9465c6c":"code","f109a004":"code","e48d57f5":"code","7836a186":"code","69367201":"markdown","29d1c8a4":"markdown","eb5a0068":"markdown","29245468":"markdown","c1f61284":"markdown","2fc87939":"markdown"},"source":{"7d0371e5":"from sklearn import preprocessing\nimport numpy as np\nfrom keras import models\nfrom keras import layers\nfrom keras.datasets import reuters\nfrom keras.utils.np_utils import to_categorical\nfrom keras.datasets import imdb\nfrom keras.preprocessing.text import Tokenizer\n","a32c9c8f":"# Create feature\nfeatures = np.array([[-100.1, 3240.1],\n[-200.2, -234.1],\n[5000.5, 150.1],\n[6000.6, -125.1],\n[9000.9, -673.1]])","0a9bc3aa":"scaler = preprocessing.StandardScaler()\n# Transform the feature\nfeatures_standardized = scaler.fit_transform(features)\n# Show feature\nfeatures_standardized","b7fb6371":"# Print mean and standard deviation\nprint(\"Mean:\", round(features_standardized[:,0].mean()))\nprint(\"Standard deviation: \", features_standardized[:,0].std())","179fe4d2":"# Start neural network\nnetwork = models.Sequential()","787174f2":"# Add fully connected layer with a ReLU activation function\nnetwork.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))","e04aa237":"# Add fully connected layer with a ReLU activation function\nnetwork.add(layers.Dense(units=16, activation=\"relu\"))","fe4887c7":"# Add fully connected layer with a sigmoid activation function\nnetwork.add(layers.Dense(units=1, activation=\"sigmoid\"))","7cc62d4f":"network.compile(loss=\"binary_crossentropy\", # Cross-entropy\noptimizer=\"rmsprop\", # Root Mean Square Propagation\nmetrics=[\"accuracy\"]) # Accuracy performance metric","0e04b2b0":"np.random.seed(0)\n# Set the number of features we want\nnumber_of_features = 1000","66f4d0ed":"# Load data and target vector from movie review data\n(data_train, target_train), (data_test, target_test) = imdb.load_data(\nnum_words=number_of_features)","442f7a29":"# Convert movie review data to one-hot encoded feature matrix\ntokenizer = Tokenizer(num_words=number_of_features)\nfeatures_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\nfeatures_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")","408ea498":"# Start neural network\nnetwork = models.Sequential()\n# Add fully connected layer with a ReLU activation function\nnetwork.add(layers.Dense(units=16, activation=\"relu\", input_shape=(\nnumber_of_features,)))","053396e6":"# Add fully connected layer with a ReLU activation function\nnetwork.add(layers.Dense(units=16, activation=\"relu\"))\n# Add fully connected layer with a sigmoid activation function\nnetwork.add(layers.Dense(units=1, activation=\"sigmoid\"))","54218353":"network.compile(loss=\"binary_crossentropy\", # Cross-entropy\noptimizer=\"rmsprop\", # Root Mean Square Propagation\nmetrics=[\"accuracy\"]) # Accuracy performance metric","8db993a2":"#Train neural network\nhistory = network.fit(features_train, # Features\ntarget_train, # Target vector\nepochs=3, # Number of epochs\nverbose=1, # Print description after each epoch\nbatch_size=100, # Number of observations per batch\nvalidation_data=(features_test, target_test)) # Test data","a76a833a":"# View shape of feature matrix\nfeatures_train.shape","845c6256":"# Set random seed\nnp.random.seed(0)\n# Set the number of features we want\nnumber_of_features = 5000","aa20c17e":"data = reuters.load_data(num_words=number_of_features)\n(data_train, target_vector_train), (data_test, target_vector_test) = data\n# Convert feature data to a one-hot encoded feature matrix\ntokenizer = Tokenizer(num_words=number_of_features)\nfeatures_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\nfeatures_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")","eba5bc4b":"# One-hot encode target vector to create a target matrix\ntarget_train = to_categorical(target_vector_train)\ntarget_test = to_categorical(target_vector_test)\n# Start neural network\nnetwork = models.Sequential()","5b9abe0d":"# Add fully connected layer with a ReLU activation function\nnetwork.add(layers.Dense(units=100,\nactivation=\"relu\",\ninput_shape=(number_of_features,)))","e9465c6c":"# Add fully connected layer with a ReLU activation function\nnetwork.add(layers.Dense(units=100, activation=\"relu\"))\n# Add fully connected layer with a softmax activation function\nnetwork.add(layers.Dense(units=46, activation=\"softmax\"))","f109a004":"#Compile neural network\nnetwork.compile(loss=\"categorical_crossentropy\", # Cross-entropy\noptimizer=\"rmsprop\", # Root Mean Square Propagation\nmetrics=[\"accuracy\"]) # Accuracy performance metric","e48d57f5":"# Train neural network\nhistory = network.fit(features_train, # Features\ntarget_train, # Target\nepochs=3, # Three epochs\nverbose=0, # No output\nbatch_size=100, # Number of observations per batch\nvalidation_data=(features_test, target_test)) # Test data","7836a186":"# View target matrix\ntarget_train","69367201":"At the heart of neural networks is the unit (also called a node or neuron). A unit takes in one or more inputs, multiplies each input by a parameter (also called a weight), sums the weighted input\u2019s values along with some bias value (typically 1), and then feeds the value into an activation function. This output is then sent forward to the other neurals deeper in the neural network (if they exist).\n\nFeedforward neural networks\u2014also called multilayer perceptron\u2014are the simplest\nartificial neural network used in any real-world setting. Neural networks can be\nvisualized as a series of connected layers that form a network connecting an observation\u2019s feature values at one end, and the target value (e.g., observation\u2019s class) at the other end. The name feedforward comes from the fact that an observation\u2019s feature values are fed \u201cforward\u201d through the network, with each layer successively transforming the feature values with the goal that the output at the end is the same as the target\u2019s\nvalue.\n\nSpecifically, feedforward neural networks contain three types of layers of units. At the start of the neural network is an input layer where each unit contains an observation\u2019s value for a single feature. For example, if an observation has 100 features, the input layer has 100 nodes. At the end of the neural network is the output layer, which transforms the output of the hidden layers into values useful for the task at hand. For example, if our goal was binary classification, we could use an output layer with a single unit that uses a sigmoid function to scale its own output to between 0 and 1, representing a predicted class probability. Between the input and output layers are the so-called \u201chidden\u201d layers (which aren\u2019t hidden at all). These hidden layers successively transform the feature values from the input layer to something that, once processed by the output layer, resembles the target class. Neural networks with many hidden layers (e.g., 10, 100, 1,000) are considered \u201cdeep\u201d networks and their application is\ncalled deep learning.","29d1c8a4":"## Training a Binary Classifier","eb5a0068":"## Preprocessing Data","29245468":"## import necessary library ","c1f61284":"## Training a Multiclass Classifier","2fc87939":"# Designing a Neural Network"}}