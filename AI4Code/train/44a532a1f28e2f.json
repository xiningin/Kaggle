{"cell_type":{"853fd8e4":"code","8d71d662":"code","cee2d04f":"code","a46230b4":"code","cf11b956":"code","47501218":"code","28535714":"code","138a943b":"code","8732314b":"code","c1b7b75b":"code","47a199aa":"code","d7cd8f5b":"code","939e660e":"code","a3db3fd3":"code","38b46be5":"code","8903045c":"code","ff753621":"code","356b08f7":"code","36fe6b27":"code","f34d2301":"code","b4356298":"code","622ce852":"code","40de5dab":"code","9fb70754":"code","dfdb28ba":"code","247a9a39":"code","8a9f248c":"code","47b3968f":"code","77fa6938":"code","e12d0656":"code","c1401643":"code","3ed04366":"code","e2d34e9a":"code","1a8763de":"code","ab5046af":"code","4847608d":"code","e507f3a3":"code","e53f8af0":"code","993f699f":"code","8eede6b9":"code","d36e9b9b":"code","6635e977":"code","8835b7a0":"code","b772ca72":"code","60ef7393":"code","0286b6ec":"code","278151d7":"code","f5e48cfb":"code","ac7c5db6":"code","93b72d4e":"code","185d15d8":"code","7ee41b15":"markdown","c319217b":"markdown","4a0645d1":"markdown","46a878b9":"markdown","b9ba7173":"markdown","97f03a8d":"markdown","1aade86b":"markdown","ea39ad7b":"markdown","941e653f":"markdown","f1223f6b":"markdown","88b4bab9":"markdown","5912f6eb":"markdown","357644b1":"markdown","02633e1b":"markdown","55f557ab":"markdown","7a8971c4":"markdown","eacf8680":"markdown","0eee984d":"markdown","ae247832":"markdown","032575e0":"markdown","d9595661":"markdown"},"source":{"853fd8e4":"# Importing a library\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import month_plot, seasonal_plot, plot_acf, plot_pacf, quarter_plot\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n%matplotlib inline","8d71d662":"# Loading data\ndf_hld = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/holidays_events.csv')\ndf_oil = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/oil.csv')\ndf_str = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/stores.csv')\ndf_trns = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/transactions.csv')\ntrain = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/train.csv')\ntest = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/test.csv')\nsample = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/sample_submission.csv')","cee2d04f":"# Check what data is available\ntrain.head()","a46230b4":"# Check the number of rows and columns\ntrain.shape","cf11b956":"# What kind of rows are there?\ntrain.columns","47501218":"# Display data types\ntrain.info()","28535714":"train.describe()","138a943b":"# Converting data types\ntrain['date'] = pd.to_datetime(train['date'])","8732314b":"train_df = pd.read_csv(\n    \"..\/input\/store-sales-time-series-forecasting\/train.csv\",\n    index_col='date',\n    parse_dates=['date'],\n).drop(['store_nbr', 'family', 'onpromotion'], axis=1)","c1b7b75b":"train_df['Time'] = np.arange(len(train_df.index))","47a199aa":"train_df.head()","d7cd8f5b":"# Check what data is available\ntest.head()","939e660e":"# Display data types\ntest.info()","a3db3fd3":"test.describe()","38b46be5":"# Converting data types\ntest['date'] = pd.to_datetime(test['date'])","8903045c":"# Check the number of rows and columns\ntest.shape","ff753621":"# Check what data is available\ndf_hld.head()","356b08f7":"# Check what data is available\ndf_oil.head()","36fe6b27":"# Check what data is available\ndf_str.head()","f34d2301":"df_str.describe()","b4356298":"# Check the number of city types\u3000on stores.csv\ndf_str['city'].value_counts()","622ce852":"df_str['city'].describe()","40de5dab":"# Check the number of state types\u3000on stores.csv\ndf_str['state'].value_counts()","9fb70754":"df_str['state'].describe()","dfdb28ba":"# Check the number of rows and columns\ndf_hld.shape","247a9a39":"df_hld.describe()","8a9f248c":"# Check the number of rows and columns\ndf_oil.shape","47b3968f":"df_oil.describe()","77fa6938":"# Check the number of rows and columns\ndf_str.shape","e12d0656":"# Check the number of rows and columns\nsample.shape","c1401643":"# Check the submission format\nsample.head()","3ed04366":"# Time series plot of data\nplt.figure(figsize=(10,6))\nsns.lineplot(x=train.index, y=\"sales\", data=train)\nplt.show()","e2d34e9a":"# Calculation of index-weighted moving average\newm_mean = train['sales'].ewm(span=90).mean()  \n\n# Display exponentially weighted moving average\nprint(ewm_mean)\n\n# visualization\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.plot(train['sales'], label='original')\nax.plot(ewm_mean, label='ewma')\nax.legend()\nplt.show()","1a8763de":"def plot_outlier(ts, ewm_span=90, threshold=3.0):\n\n    fig, ax = plt.subplots()\n    # Calculation of index-weighted moving average\n    ewm_mean = ts.ewm(span=ewm_span).mean()  \n    # Calculation of exponentially weighted moving standard deviation\n    ewm_std = ts.ewm(span=ewm_span).std()  \n    ax.plot(ts, label='original')\n    ax.plot(ewm_mean, label='ewma')\n\n    # Plot data that are more than 3.0 times out of the standard deviation as outliers\n    ax.fill_between(ts.index,\n                    ewm_mean - ewm_std * threshold,\n                    ewm_mean + ewm_std * threshold,\n                    alpha=0.2)\n    outlier = ts[(ts - ewm_mean).abs() > ewm_std * threshold]\n    ax.scatter(outlier.index, outlier, label='outlier')\n    ax.legend()\n    plt.figure(figsize=(10,6))\n    plt.show()\n    return fig,outlier\n\nfig,out_fil = plot_outlier(train['sales'],ewm_span=90, threshold=3.0);","ab5046af":"# Extract records that do not have outliers\ntrain_df_cln = train_df[~train_df.index.isin(out_fil.index)]\ntrain_df_cln.head()","4847608d":"# Time series plot of data\u3000with outliers removed\n# sns.lineplot(data=train_df_cln, x=\"date\", y=\"sales\")","e507f3a3":"# Let's take out a moving average.\n# train_df_cln['ma7'] = train_df_cln['date'].rolling(\uff17).mean()\n# print(train_df_cln)","e53f8af0":"# Linear Regression with Time Series\n# plt.style.use(\"seaborn-whitegrid\")\n# plt.rc(\n#     \"figure\",\n#     autolayout=True,\n#     figsize=(11, 4),\n#     titlesize=18,\n#     titleweight='bold',\n# )\n# plt.rc(\n#     \"axes\",\n#     labelweight=\"bold\",\n#     labelsize=\"large\",\n#     titleweight=\"bold\",\n#     titlesize=16,\n#     titlepad=10,\n# )\n# %config InlineBackend.figure_format = 'retina'\n\n# fig, ax = plt.subplots()\n# ax.plot('Time', 'sales', data=train_df_cln, color='0.75')\nax = sns.regplot(x='Time', y='sales', data=train_df_cln, ci=None, scatter_kws=dict(color='0.25'))\nax.set_title('Time Plot of Sales');","993f699f":"# Lag features with Time Series\n# train_df_cln['Lag_1'] = train_df_cln['sales'].shift(1)\n# train_df_cln = train_df_cln.reindex(columns=['sales', 'Lag_1'])","8eede6b9":"train_df_cln.head()","d36e9b9b":"# fig, ax = plt.subplots()\n# ax = sns.regplot(x='Lag_1', y='sales', data=train_df_cln, ci=None, scatter_kws=dict(color='0.25'))\n# ax.set_aspect('equal')\n# ax.set_title('Lag Plot of Sales');","6635e977":"train_plus = pd.concat([train, df_str])","8835b7a0":"grouped_mean = train_plus.groupby(['city','state'])['sales'].mean()\ngrouped_mean","b772ca72":"# Check for missing value\ntrain[train['sales'].isnull()]","60ef7393":"df_str[df_str['city'].isnull()]","0286b6ec":"df_str[df_str['state'].isnull()]","278151d7":"# Calculation of sinusoidal waves\nx = np.linspace(-6 * np.pi, 6 * np.pi, 100)\nsin = pd.Series(np.sin(x))\n\nplt.figure(figsize=(10,6))\nplt.plot(sin.index, sin)\nplt.show()\n\n# Calculation of autocorrelation coefficient\nlags = 20\nautocorrs = [sin.autocorr(lag=lag) for lag in range(lags)]\nprint(autocorrs)","f5e48cfb":"# Confirmation of periodicity using a cholerogram\nplt.figure(figsize=(10,6))\nplt.bar(range(lags), autocorrs)\nplt.show()","ac7c5db6":"train = train.sort_values('date')\ntrain.head()","93b72d4e":"# See when the data is available in a time series\ntrain.tail()","185d15d8":"# Training data\ntrain_splt = train[train['date']<'201\uff16-0\uff19-01']\n\n# Test data\ntest_splt = train[train['date']>='201\uff16-0\uff19-01']","7ee41b15":"<h2 style='color:white; background:#000080; border:0'><center>Autocorrelation Coefficient<\/center><\/h2>","c319217b":"[The Data Description](https:\/\/www.kaggle.com\/c\/store-sales-time-series-forecasting\/data) describes the holidays_events.csv as follows.\n* Holidays and Events, with metadata\n* NOTE: Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.\n* Additional holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday).","4a0645d1":"**Personal Notes**\nI'm going to comment out some of the code that follows because it's too time consuming to finish.","46a878b9":"[The Data Description](https:\/\/www.kaggle.com\/c\/store-sales-time-series-forecasting\/data) describes the oil.csv as follows.\n* Daily oil price. Includes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.)","b9ba7173":"<h2 style='color:white; background:#000080; border:0'><center>Creating a sales volume forecasting model<\/center><\/h2>","97f03a8d":"[The Data Description](https:\/\/www.kaggle.com\/c\/store-sales-time-series-forecasting\/data) describes the stores.csv as follows.\n* Store metadata, including city, state, type, and cluster.\n* cluster is a grouping of similar stores.","1aade86b":"* In other words, the training data covers about 56.5 months, from January 1, 2012 to August 15, 2017.\n* The training data has data for a period of 56.5 months.Since we want to split the data in an approximate 8:2 ratio, we will split the data between September 2016 and earlier.","ea39ad7b":"Missing value data cannot be confirmed.","941e653f":"The autocorrelation coefficient is a number that indicates how much past values influence the current data.\n\nIn the case of daily data, if we shift the data by one step and check the autocorrelation, we can see how much the sales volume of one day ago affects today. The number of steps in this shifted data is called the lag.\n\nLet's say the lag is 20.","f1223f6b":"You can see that the data is repeating the same kind of movement, that is, the sine curve is periodic.","88b4bab9":"* We can see from the graph that we have two obvious points, one with a large value. We could check the date and time from the graph and correct it, but we would not be able to deal with many more outliers, so we want to work out the logic and look for the outliers.\n\n* We assume that the outliers deviate greatly from the trend from the mean and variance, and we want to consider how to remove the outliers while calculating various numbers.","5912f6eb":"**Personal Notes**\nDraw sales data by city and state\nCreate weekly average data and monthly average data","357644b1":"Let's use the calculated data to create a corelogram.\n\nA correlogram is a graph with the autocorrelation coefficient or cross-correlation coefficient calculated for different lags, with the lag on the horizontal axis and the correlation coefficient on the vertical axis.\nBy using the correlogram, it is possible to visualize the periodicity of the data.\n\nWe will use the coefficients we have just calculated to draw the correlogram.","02633e1b":"In table data such as time series data, it is necessary to deal with outliers and abnormal values.\nThis is because in the case of time-series data, outliers may cause the overall trend to shift.\n\nLet's start with a simple plot of the values to see if there are any outliers.","55f557ab":"[The Data Description](https:\/\/www.kaggle.com\/c\/store-sales-time-series-forecasting\/data) describes the train.csv as follows.\n* The training data, comprising time series of features store_nbr, family, and onpromotion as well as the target sales.\n* store_nbr identifies the store at which the products are sold.\n* family identifies the type of product sold.\n* sales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\n* onpromotion gives the total number of items in a product family that were being promoted at a store at a given date.","7a8971c4":"I have been using exponential weighted moving averages to look for outliers, but I would like to also calculate exponential weighted moving standard deviations and plot data that is more than three times out of standard deviation as outliers. I will then try to remove the outliers.","eacf8680":"We need to know the start and end period of the training data first.","0eee984d":"<h2 style='color:white; background:#000080; border:0'><center>Checking the data<\/center><\/h2>","ae247832":"<h2 style='color:white; background:#000080; border:0'><center>Processing data<\/center><\/h2>","032575e0":"[The Data Description](https:\/\/www.kaggle.com\/c\/store-sales-time-series-forecasting\/data) describes the test.csv as follows.\n* The test data, having the same features as the training data. You will predict the target sales for the dates in this file.\n* The dates in the test data are for the 15 days after the last date in the training data.","d9595661":"<h2 style='color:white; background:#000080; border:0'><center>EDA and Data Visualization<\/center><\/h2>"}}