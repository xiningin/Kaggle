{"cell_type":{"2a7470f7":"code","a92a068d":"code","4b7a2d05":"code","ac47c428":"code","b262b9b7":"code","268c14e9":"code","2cce17d7":"code","b88bbd05":"code","026bbd13":"code","92997430":"code","e484f1af":"code","392b0129":"code","28d432b8":"code","7f42eea5":"code","95785b07":"code","c9a00622":"code","6844554f":"code","84308019":"code","b7b6c979":"code","e62536a0":"code","0ad91e39":"code","3dfdd16b":"markdown","1fd907d8":"markdown","53b28aee":"markdown","b653300f":"markdown","60caf227":"markdown","13ab66fa":"markdown","d865ef2f":"markdown","c346b396":"markdown","4c0984e1":"markdown","1372c490":"markdown","b3c8c0d7":"markdown","c97ce8ec":"markdown","d1231f60":"markdown","3db3bd58":"markdown","c5c22692":"markdown","bdd52a98":"markdown","aa73e653":"markdown","481dadd3":"markdown","de9e547e":"markdown","585e2369":"markdown","d5199ad2":"markdown","ae1e7893":"markdown","2f397dee":"markdown","a1afc4c5":"markdown","14b75964":"markdown","1acb76b3":"markdown","3a6cc8dd":"markdown","c104e6de":"markdown","1a9253f5":"markdown","3f58f083":"markdown","a323fe44":"markdown","0463dfab":"markdown","d9c5891a":"markdown"},"source":{"2a7470f7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# reading the dataset\ntrain = pd.read_csv('..\/input\/human-activity-recognition-with-smartphones\/train.csv')\ntrain.head(10)","a92a068d":"# shape of the dataset\ntrain.shape","4b7a2d05":"test = pd.read_csv('..\/input\/human-activity-recognition-with-smartphones\/test.csv')\ntest.head(10)","ac47c428":"#shape of test dataset\ntest.shape","b262b9b7":"#checking for duplicate values in train dataset- if there are any duplicate values we will remove them\nprint(\"Number of duplicate values in train dataset are\", train.duplicated().sum())\nprint(\"Number of duplicate values in test dataset are\", test.duplicated().sum()) ","268c14e9":"#checkng for null values - if there are any null values then we will remove them using simpleimputer with mean or median strategy\nprint(\"Number of null values in train dataset are \", train.isna().sum().sum())\nprint(\"Number of null values in test dataset are \", test.isna().sum().sum())","2cce17d7":"#checking for outliers, if there are any outliers we will remove then or we will use median() to fill them\n\n# 1. Identify the outliers with inter quntile range(IQR)\nq1 = train.quantile(0.10)\nq3 = train.quantile(0.90)\niqr = q3 - q1\n#printing the iqr score which will help us to detect outliers\nprint(iqr)","b88bbd05":"#2. printing the number of outliers\noutlier = ((train < (q1 - 1.5*iqr)) | (train > (q3 + 1.5*iqr))).values.sum()\nprint(\"number of outliers are \", outlier)","026bbd13":"# 1. Identify the outliers with inter quntile range(IQR)\nq1 = test.quantile(0.10)\nq3 = test.quantile(0.90)\niqr = q3 - q1\n#printing the iqr score which will help us to detect outliers\nprint(iqr)","92997430":"#2. printing the number of outliers\noutlier = ((test < (q1 - 1.5*iqr)) | (test > (q3 + 1.5*iqr))).values.sum()\nprint(\"number of outliers are \", outlier)","e484f1af":"#description of train dataset\ntrain.describe()","392b0129":"#description of test dataset\ntest.describe()","28d432b8":"# visualizing the use user data in train\nplt.figure(figsize=(16,8))\nsns.countplot(x='subject', hue='Activity', data=train)\nplt.title(\"User data\", fontsize=20)\nplt.show()","7f42eea5":"sns.countplot(x='Activity', data=train)\nplt.title(\"Activity data points\", fontsize=20)\nplt.xticks(rotation=90)\nplt.show()","95785b07":"sns.countplot(x='subject', data=train)\nplt.title(\"Subject data points\", fontsize=20)\nplt.xticks(rotation=90)\nplt.show()","c9a00622":"test.Activity.unique()","6844554f":"train_PCA = train.drop(['Activity', 'subject'], axis=1)","84308019":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(train_PCA)","b7b6c979":"humanActivity = pd.DataFrame({'x':principalComponents[:,0], 'y':principalComponents[:,1] ,'label':train['Activity']})\nhumanActivity","e62536a0":"sns.lmplot(data=humanActivity, x='x', y='y', hue='label', fit_reg=False, height=8, markers=['^','v','s','o', '1','2'])\nplt.show()","0ad91e39":"pca.explained_variance_ratio_","3dfdd16b":"From above data points we can say that most of the subjects performed a same amount of movement(activity). Though few subjects did more than others.","1fd907d8":"Ignore the error for now\n\nWe can see that there are some outliers, and we will adjust them when we want to predict the result, which we won't do in this notebook","53b28aee":"# PCA","b653300f":"## 4. Variance","60caf227":"## 2. visualize user data","13ab66fa":"* We can't seprate our activities.\n\n* Model will probably be confused, from where to seprate the dataset. ","d865ef2f":"# Test dataset","c346b396":"### train dataset","4c0984e1":"We imported PCA from sklearn.decomposition. n_components = 2, because we wanted to convert our dataset into 2D array. ","1372c490":"## 3. Activity and subject data points","b3c8c0d7":"## 1. description of dataset","c97ce8ec":"# Data Cleaning","d1231f60":"After storing our principalComponant into humanAcivity dataframe we added labels into the dataset. They will help us to visualize the 2D dataset","3db3bd58":"As we all already know, there are 7352 rows with 562 columns. With description method you can see mean, std, min, max, etc, values for each column","c5c22692":"## 2. Storing 2D values in new dataframe","bdd52a98":"## 1. PCA projecton to 2D","aa73e653":"*Ignore the error for now*\n\nWe can see that there are some outliers, and we will adjust them when we want to predict the result, which we won't do in this notebook","481dadd3":"No need to check it for test dataset","de9e547e":"# Train dataset","585e2369":"As we can see that there are no duplicate values in train and test dataset","d5199ad2":"# Exploratory data analysis - EDA","ae1e7893":"### test dataset","2f397dee":"By using the attribute **explained_variance_ratio_**, you can see that the first principal component contains 62.55% of the variance and the second principal component contains 4.91% of the variance. Together, the two components contain 6% of the information.","a1afc4c5":"As we can see that there are no null values in train and test dataset","14b75964":"## 1. Check for duplicates","1acb76b3":"Now we will drop **subject** column because it contains string values and we will drop **Activity** column because it is **dependent column.**","3a6cc8dd":"## 3. Visualize 2D Projection","c104e6de":"As we all already know, there are 2947 rows with 562 columns. With description method you can see mean, std, min, max, etc, values for each column","1a9253f5":"There are 1400 data points in which lowest data point is walking downstairs. We can say that **walking downstairs** acitivity was **less performered** and highest data points for **Laying** activity, which was **mostly performed.**","3f58f083":"While predicting the result of our model we will not use test dataset","a323fe44":"## 3. check for outliers","0463dfab":"## 2. Check for null values","d9c5891a":"## 4. Dropping some columns"}}