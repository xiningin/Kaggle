{"cell_type":{"0fe509a7":"code","3508395f":"code","8af19931":"code","ccfbf3de":"code","f0cb5b03":"code","fda20658":"code","57f4f2f5":"code","fd03a47e":"code","d05309f3":"code","6ab16bf1":"code","8abed590":"code","76d4fa4a":"code","fc83c534":"code","4e8ab927":"code","ce65562b":"code","88b4d76d":"code","5bda16be":"code","23598579":"code","907ee2e8":"code","5102fcd2":"code","b3dc9896":"code","7916901a":"code","f4128628":"code","3c1d44bc":"code","f85c720d":"code","c4785841":"code","5bc972bd":"code","3c4ad8d7":"code","866ddeb6":"code","7bb0ecf9":"code","05370f32":"code","0d785d84":"code","915e52ec":"code","6e8c6f6f":"code","2156131d":"code","779a9044":"code","b641c091":"code","b91c8c89":"code","82c20c73":"code","808e04ba":"markdown","9a55959a":"markdown","e45d532d":"markdown","f0a0aa5a":"markdown","aa4f1490":"markdown","c6bf2334":"markdown","6b7af881":"markdown"},"source":{"0fe509a7":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","3508395f":"import pathlib\nimport fastai\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom fastai.utils.mem import *\n\nfrom torchvision.models import vgg16_bn\nfrom subprocess import check_output","8af19931":"path = pathlib.Path('\/kaggle\/input\/denoising-dirty-documents')","ccfbf3de":"items = check_output([\"ls\", \"..\/input\/denoising-dirty-documents\"]).decode(\"utf8\")\nitems = items.split('\\n'); items.pop(); items","f0cb5b03":"import zipfile\n\nfor item in items:\n    # Will unzip the files so that you can see them..\n    print(item)\n    with zipfile.ZipFile(path\/item,\"r\") as z:\n        z.extractall(\".\")","fda20658":"bs,size=4,128\narch = models.resnet34\npath_train = pathlib.Path(\"\/kaggle\/working\/train\")\npath_train_cleaned = pathlib.Path(\"\/kaggle\/working\/train_cleaned\")\npath_test = pathlib.Path(\"\/kaggle\/working\/test\")","57f4f2f5":"src = ImageImageList.from_folder(path_train).split_by_rand_pct(0.2, seed=42)\n      ","fd03a47e":"def get_data(bs,size):\n    data = (src.label_from_func(lambda x: path_train_cleaned\/x.name)\n           .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True)\n           .databunch(bs=bs).normalize(imagenet_stats, do_y=True))\n\n    data.c = 3\n    return data","d05309f3":"data = get_data(bs,size)","6ab16bf1":"data.show_batch(ds_type=DatasetType.Valid, rows=2, figsize=(9,9))","8abed590":"t = data.valid_ds[0][1].data\nt = torch.stack([t,t])","76d4fa4a":"def gram_matrix(x):\n    n,c,h,w = x.size()\n    x = x.view(n, c, -1)\n    return (x @ x.transpose(1,2))\/(c*h*w)","fc83c534":"gram_matrix(t)","4e8ab927":"base_loss = F.l1_loss","ce65562b":"vgg_m = vgg16_bn(True).features.cuda().eval()\nrequires_grad(vgg_m, False)","88b4d76d":"blocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]\nblocks, [vgg_m[i] for i in blocks]","5bda16be":"class FeatureLoss(nn.Module):\n    def __init__(self, m_feat, layer_ids, layer_wgts):\n        super().__init__()\n        self.m_feat = m_feat\n        self.loss_features = [self.m_feat[i] for i in layer_ids]\n        self.hooks = hook_outputs(self.loss_features, detach=False)\n        self.wgts = layer_wgts\n        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n\n    def make_features(self, x, clone=False):\n        self.m_feat(x)\n        return [(o.clone() if clone else o) for o in self.hooks.stored]\n    \n    def forward(self, input, target):\n        out_feat = self.make_features(target, clone=True)\n        in_feat = self.make_features(input)\n        self.feat_losses = [base_loss(input,target)]\n        self.feat_losses += [base_loss(f_in, f_out)*w\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n        return sum(self.feat_losses)\n    \n    def __del__(self): self.hooks.remove()","23598579":"feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])","907ee2e8":"wd = 1e-3\nlearn = unet_learner(data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics,\n                     blur=True, norm_type=NormType.Weight)\ngc.collect();","5102fcd2":"learn.model_dir  ='\/kaggle\/working\/models'","b3dc9896":"learn.lr_find()\nlearn.recorder.plot()","7916901a":"len(data.valid_ds.items)","f4128628":"lr = 1e-3","3c1d44bc":"def do_fit(save_name, lrs=slice(lr), pct_start=0.9):\n    learn.fit_one_cycle(10, lrs, pct_start=pct_start)\n    learn.save(save_name)\n    learn.show_results(rows=1, imgsize=5)","f85c720d":"do_fit('1a', slice(lr*10))","c4785841":"learn.unfreeze()","5bc972bd":"do_fit('1b', slice(1e-5,lr))","3c4ad8d7":"data = get_data(12,size*2)","866ddeb6":"learn.data = data\nlearn.freeze()\ngc.collect()","7bb0ecf9":"learn.load('1b');","05370f32":"do_fit('2a')","0d785d84":"learn.unfreeze()","915e52ec":"do_fit('2b', slice(1e-6,1e-4), pct_start=0.3)","6e8c6f6f":"fn = data.valid_ds.x.items[10]; fn","2156131d":"img = open_image(fn); img.shape","779a9044":"p,img_pred,b = learn.predict(img)","b641c091":"show_image(img, figsize=(8,5), interpolation='nearest');","b91c8c89":"Image(img_pred).show(figsize=(8,5))","82c20c73":"model_path = Path(\"\/kaggle\/working\/export.pkl\")\nlearn.export(file = model_path)","808e04ba":"# Setup\n## Library import","9a55959a":"## Feature Loss","e45d532d":"# Data processing","f0a0aa5a":"## Train","aa4f1490":"# Background Removal using Fastai Unet learner\n## Purpose\nThe purpose of the notebook is to demonstrate Fastai library usage to perform background removal in images.\n\n## Methodology\nThe dataset consists of:-\n1. Train Folder: Consists of images which possess some kind of corruption such as paper wrinkles or coffee stains.\n2. Train_cleaned Folder: Consists of cleaned images\n3. Test Folder: Corrupt images that needs to be cleaned using the model for performance verification.\n","c6bf2334":"# Data import","6b7af881":"## Test"}}