{"cell_type":{"a8d775eb":"code","4765f183":"code","825980d6":"code","20f1b566":"code","26155c62":"code","a1132412":"code","af41e56c":"code","b19ad632":"code","f4186360":"code","8fadfe13":"code","a3563986":"code","a880c820":"code","ca7ea016":"code","1328a041":"code","ce2bbd3c":"code","ad92ad91":"code","9e45c430":"code","262dad8e":"code","b2863b00":"code","b24fad49":"code","d7b13ffb":"code","8366ebff":"code","b6b05724":"code","18b71f49":"code","10ddf750":"code","9ffd521d":"code","b44e2039":"code","fa6a6f1b":"code","d28345b5":"code","bd6073ea":"code","e1594b06":"code","c0c74687":"code","0a5ab078":"code","4234429a":"code","02a09a16":"code","8aaf52f3":"code","67522d9a":"code","3e60a92b":"code","b407755f":"code","98ff2624":"code","acba033b":"code","f31503a5":"code","88a2bc90":"code","4f8e4a40":"code","1173d347":"code","c3df007d":"code","f1824610":"code","6166430b":"code","7c19b022":"code","9cb3c8f1":"code","ee00c40f":"code","64065554":"code","fffc023d":"code","486a19c9":"code","c947b851":"code","ec1323f3":"code","2532a28e":"code","9524d5b4":"code","238e1cc2":"code","dcde1801":"code","33f3c4e2":"markdown","3802dfca":"markdown"},"source":{"a8d775eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4765f183":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","825980d6":"#importing other modules\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\nfrom sklearn import preprocessing \n#from sklearn.preprocessing import imputer\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score\n%matplotlib inline\nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.datasets import make_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn import ensemble\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom matplotlib import pyplot","20f1b566":"train.head()","26155c62":"# lets check missing data\nNANs = pd.concat([train.isnull().sum(), test.isnull().sum()], axis=1, keys=['Train', 'Test'])\nNANs[NANs.sum(axis=1) > 0].T","a1132412":"dfprep = pd.concat([train, test], keys=['train', 'test'])","af41e56c":"dfprep.head()","b19ad632":"# Missing values feature wise\n\nsns.set_style(\"whitegrid\")\nmissing = dfprep.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()","f4186360":"dfprep.describe(include=[\"object\"]).columns","8fadfe13":"num_col = dfprep.select_dtypes(include=np.number).columns\nprint(\"Numerical columns: \\n\",num_col)\n\ncat_col = dfprep.select_dtypes(exclude=np.number).columns\nprint(\"Categorical columns: \\n\",cat_col)","a3563986":"_ = dfprep.hist(column=num_col, figsize = (18,18))","a880c820":"len(cat_col.to_list())","ca7ea016":"# Count plots of categorical variables\n\nfig, axes = plt.subplots(4, 3, figsize=(16, 16))\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.2)\n\nfor i, ax in enumerate(axes.ravel()):\n    if i > 43:\n        ax.set_visible(False)\n        continue\n    sns.countplot(y = cat_col[i], data=dfprep, ax=ax)","1328a041":"sel = VarianceThreshold(threshold=0)\nsel.fit(dfprep[num_col])","ce2bbd3c":"sum(sel.get_support())","ad92ad91":"# now let's print the number of constant feautures\n# (see how we use ~ to exclude non-constant features)\n\nconstant = dfprep[num_col].columns[~sel.get_support()]\n\nlen(constant)","9e45c430":"#No constant Numerical features","262dad8e":"#For checking constant Categorical features\n\nconstant_features = [\n    feat for feat in dfprep[cat_col].columns if dfprep[feat].nunique() == 1\n]\n\nlen(constant_features)","b2863b00":"dfprep['Street'].value_counts(dropna=False)","b24fad49":"#For checking constant Numerical features - again using std\n\nconstant_features = [\n    feat for feat in dfprep[num_col].columns if dfprep[feat].std() == 1\n]\n\nlen(constant_features)","d7b13ffb":"# Dropping features that have many missing information or do not make much sense\n# Features should not be dropped casually like this but still.. sometimes ;) \ndfprep.drop([ 'Heating', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1','BsmtQual', 'BsmtFinType2',\n               'BsmtFullBath' ,'BsmtHalfBath', 'GarageYrBlt',  'GarageCond', 'WoodDeckSF',\n               'OpenPorchSF', 'EnclosedPorch', 'LowQualFinSF', '3SsnPorch','GarageArea', 'ScreenPorch', 'PoolArea',  'Fence','PoolQC', 'MiscFeature','Utilities', 'RoofMatl', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'MiscVal','Alley'],\n              axis=1, inplace=True)","8366ebff":"\n\n#Filling NAs and converting dtypes\n\ndfprep['MasVnrArea'] = dfprep['MasVnrArea'].fillna(dfprep['MasVnrArea'].mode()[0])\n\n# MSSubClass as str\ndfprep['Functional'] = dfprep['Functional'].astype(str)\ndfprep['Functional'] = dfprep['Functional'].fillna(dfprep['Functional'].mode())\n\n\n# LotFrontage  \ndfprep['LotFrontage'] = dfprep['LotFrontage'].fillna(dfprep['LotFrontage'].mean())\n\n# OverallCond \ndfprep.OverallCond = dfprep.OverallCond.astype(str)\n\n# MasVnrType \ndfprep['MasVnrType'] = dfprep['MasVnrType'].fillna(dfprep['MasVnrType'].mode()[0])\n\n# MSSubClass as str\ndfprep['MSSubClass'] = dfprep['MSSubClass'].astype(str)\n\n# MSZoning \ndfprep['MSZoning'] = dfprep['MSZoning'].fillna(dfprep['MSZoning'].mode()[0])\n\n\n# TotalBsmtSF  \ndfprep['TotalBsmtSF'] = dfprep['TotalBsmtSF'].fillna(0)\n\n\n# KitchenAbvGr \ndfprep['KitchenAbvGr'] = dfprep['KitchenAbvGr'].astype(str)\n\n# Electrical \ndfprep['Electrical'] = dfprep['Electrical'].fillna(dfprep['Electrical'].mode()[0])\n\n\n# KitchenQual \ndfprep['KitchenQual'] = dfprep['KitchenQual'].fillna(dfprep['KitchenQual'].mode()[0])\n\n# Yrsold and Mosold\ndfprep['YrSold'] = dfprep['YrSold'].astype(str)\ndfprep['MoSold'] = dfprep['MoSold'].astype(str)\n\n\n# FireplaceQu  \ndfprep['FireplaceQu'] = dfprep['FireplaceQu'].fillna('NoFP')\n\n# GarageType, GarageFinish, GarageQual  \nfor col in ('GarageType', 'GarageFinish', 'GarageQual'):\n    dfprep[col] = dfprep[col].fillna('No_GRGE')\n\n# Combining SF all floors\ndfprep['TotalSF'] = dfprep['TotalBsmtSF'] + dfprep['1stFlrSF'] + dfprep['2ndFlrSF']\ndfprep.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], axis=1, inplace=True)    \n\n# GarageCars \ndfprep['GarageCars'] = dfprep['GarageCars'].fillna(0.0)\n\n# SaleType \ndfprep['SaleType'] = dfprep['SaleType'].fillna(dfprep['SaleType'].mode()[0])\n\n\n","b6b05724":"\n# SalesPrice is skewed right ? \nax = sns.distplot(dfprep['SalePrice'])","18b71f49":"## Log transformation \ndfprep['SalePrice'] = np.log(dfprep['SalePrice'])\nax = sns.distplot(dfprep['SalePrice'])","10ddf750":"#Other columns -- checking normalization for them\nfrom scipy import stats\nfrom scipy.stats import shapiro\n\nnum_col = dfprep.select_dtypes(include=np.number).columns\n\n\n\ntest_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01\nnormal = dfprep[num_col]\nnormal = normal.apply(test_normality)\nprint(not normal.any())","9ffd521d":"#### None of quantitative variables has normal distribution so ideally these should be transformed as well. But i will give it a pass","b44e2039":"##Standardizing numeric data\n\n#numeric_features = dfprep[num_col]\n#numeric_features_standardized = (numeric_features - numeric_features.mean())\/numeric_features.std()\n\n##Scaling numeric data\n\n#from sklearn import preprocessing\n#dfprepp = dfprep[num_col].drop(['SalePrice'],axis=1)\n#x = dfprepp.values #returns a numpy array\n#min_max_scaler = preprocessing.MinMaxScaler()\n#x_scaled = min_max_scaler.fit_transform(x)\n#df_scaled = pd.DataFrame(x_scaled)\n#df_scaled.columns = ['Id', 'LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt','YearRemodAdd', 'GrLivArea', 'FullBath', 'HalfBath', 'BedroomAbvGr','TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'TotalSF']\n#dfprepp.update(df_scaled)\n \ndfprepp = dfprep[num_col].drop(['SalePrice'],axis=1)\ndfprepp_scaled = (dfprepp-dfprepp.min())\/(dfprepp.max()-dfprepp.min())\ndfprep.update(dfprepp_scaled)","fa6a6f1b":"dfprepp.head()","d28345b5":"# Getting Dummies from all  categorical vars\nfor col in dfprep.dtypes[dfprep.dtypes == 'object'].index:\n    for_dummy = dfprep.pop(col)\n    dfprep = pd.concat([dfprep, pd.get_dummies(for_dummy )], axis=1)","bd6073ea":"# you just need the following 4 lines to rename duplicates\n# dfprep is the dataframe that you want to rename duplicated columns\n\ncols=pd.Series(dfprep.columns)\n\nfor dup in cols[cols.duplicated()].unique(): \n    cols[cols[cols == dup].index.values.tolist()] = [dup + '_' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n\n# rename the columns with the cols list.\ndfprep.columns=cols\n","e1594b06":"dfprep.head()","c0c74687":"train_dfprep = dfprep.loc['train'].drop('Id', axis=1)\ntest_dfprep = dfprep.loc['test'].drop('Id', axis=1)","0a5ab078":"train_features = train_dfprep.drop(['SalePrice'], axis =1 )\ntrain_label = train_dfprep['SalePrice']","4234429a":"# some fucntions for future model eval\n\ndef get_score(prediction, lables):    \n    #print('R2: {}'.format(r2_score(prediction, lables)))\n    print('RMSE: {}'.format(np.sqrt(mean_squared_error(prediction, lables))))","02a09a16":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\n#mean_absolute_percentage_error(actual, prediction) --  gives you percent error","8aaf52f3":"#### Feature selection using DecisionTreeRegressor","67522d9a":"pd.options.display.float_format = '{:,.2f}'.format\n# define dataset\ntrain_dfprep = dfprep.loc['train']\ntest_dfprep = dfprep.loc['test']\ntrain_features = train_dfprep.drop(['SalePrice'], axis =1 )\ntrain_label = train_dfprep['SalePrice']\nx_train, x_test, y_train, y_test = train_test_split(train_features, train_label, test_size=0.1, random_state=200)\n#X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n\n# define the model\nmodel = DecisionTreeRegressor()\n# fit the model\nmodel.fit(x_train, y_train)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nname = x_train.columns.to_list()\nimportant_features = pd.DataFrame(list(zip(name, importance)), columns = ['Name','Importance']).sort_values(by = 'Importance', ascending = False)\nprint(important_features)\n","3e60a92b":"important_features.head(15)['Name'].to_list()","b407755f":"dfprep2 = dfprep.copy()\ndfprep2 = dfprep2[['OverallQual',\n 'TotalSF',\n 'Y',\n 'LotArea',\n 'GarageCars',\n 'YearRemodAdd',\n 'GrLivArea',\n 'YearBuilt',\n 'Detchd',\n 'C (all)',\n 'TA_5',\n 'Id',\n 'NoFP',\n 'LotFrontage',\n 'Brk Cmn','SalePrice']]\n\ntrain_dfprep2 = dfprep2.loc['train']\ntest_dfprep2 = dfprep2.loc['test']\ntrain_features = train_dfprep2.drop(['SalePrice'], axis =1 )\ntrain_label = train_dfprep2['SalePrice']\nx_train, x_test, y_train, y_test = train_test_split(train_features, train_label, test_size=0.1, random_state=200)\n\nGBest2 = ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=3, max_features='sqrt',\n                                               min_samples_leaf=15, min_samples_split=10, loss='huber').fit(x_train, y_train)\n\nGBest2.score(x_test, y_test)","98ff2624":"### Feature selection using RFE and RFE with CV. Lets go..","acba033b":"train_dfprep2 = dfprep2.loc['train']\ntest_dfprep2 = dfprep2.loc['test']\ntrain_features = train_dfprep2.drop(['SalePrice'], axis =1 )\ntrain_label = train_dfprep2['SalePrice']\nx_train, x_test, y_train, y_test = train_test_split(train_features, train_label, test_size=0.1, random_state=200)\n\n\nlm = LinearRegression()\nlm.fit(x_train, y_train)\n\nrfe = RFE(lm, n_features_to_select = 15)   \nrfe = RFE(lm)   \nrfe = rfe.fit(x_train, y_train)\n\nselected_rfe_features = pd.DataFrame({'Feature':list(x_train.columns),\n                                      'Ranking':rfe.ranking_})\nselected_rfe_features.sort_values(by='Ranking')\n\nx_train = rfe.transform(x_train)\nx_test = rfe.transform(x_test)\n\n\nlm2 = LinearRegression()\nrfe_model = lm2.fit(x_train, y_train)\n\nselected_rfe_features","f31503a5":"actual     = y_test\nprediction = rfe_model.predict(x_test)","88a2bc90":"r2_score(actual, prediction)","4f8e4a40":"### Ah.. a lil bad than previous one. Lets try with CV now.","1173d347":"#RFE with CV\n\ntrain_dfprep2 = dfprep2.loc['train']\ntest_dfprep2 = dfprep2.loc['test']\ntrain_features = train_dfprep2.drop(['SalePrice'], axis =1 )\ntrain_label = train_dfprep2['SalePrice']\nx_train, x_test, y_train, y_test = train_test_split(train_features, train_label, test_size=0.1, random_state=200)\n\n\nrfecv = LinearRegression()\nrfecv.fit(x_train, y_train)\n\n\nrfecv = RFECV(estimator=rfecv, step=1, cv=5, scoring='r2')\nrfecv = rfecv.fit(x_train, y_train)\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","c3df007d":"rfecv.grid_scores_","f1824610":"#search_grid={'n_estimators':[2000,3000],'learning_rate':[.001,0.01,.1],'max_depth':[1,2,6],'subsample':[.5,.75,1],'random_state':[1]}\n#search=GridSearchCV(estimator=GBR,param_grid=search_grid,n_jobs=1,cv=5)\n#search.fit(X,y)\n#search.best_params_\n#search.best_score_","6166430b":"x_train_f = rfecv.transform(x_train)\nx_test_f = rfecv.transform(x_test)\n\nGBest3 = ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.06, max_depth=3, max_features='sqrt',\n                                               min_samples_leaf=15, min_samples_split=10, loss='huber').fit(x_train_f, y_train)\n\nGBest3.score(x_test_f, y_test)","7c19b022":"#Hmm..not that bad !!","9cb3c8f1":"actual     = y_test\nprediction = GBest3.predict(x_test_f)","ee00c40f":"r2_score(actual, prediction) # basically the same thing as above..","64065554":"# Feature selection using DecisionTreeRegressor has given us the good features till now so lets use it for final prediction","fffc023d":"# on our created testing dataset\n\nGBest2.score(x_test, y_test)","486a19c9":"mean_squared_error(prediction, actual)","c947b851":"testdata = dfprep.loc['test']","ec1323f3":"# Using features chosen by reg tree\n\ntestdata = testdata[['OverallQual',\n 'TotalSF',\n 'Y',\n 'LotArea',\n 'GarageCars',\n 'YearRemodAdd',\n 'GrLivArea',\n 'YearBuilt',\n 'Detchd',\n 'C (all)',\n 'TA_5',\n 'Id',\n 'NoFP',\n 'LotFrontage',\n 'Brk Cmn']]","2532a28e":"SalePrice = pd.Series(np.exp(GBest2.predict(testdata)))","9524d5b4":"test = test['Id']","238e1cc2":"output_df = pd.DataFrame({'Id':test,'SalePrice':SalePrice})","dcde1801":"output_df.head()","33f3c4e2":"The VarianceThreshold from sklearn can be used for feature selection. It removes all zero-variance features that have the same value in all samples. It removes features whose variance doesn\u2019t meet a particular threshold. Here we are using it for removing constant features","3802dfca":"That was the prediction of final test result.\nThere are tons of things that can be improved. \nDo let me know your feedbacks. Thanks to stackoverflow and top kagglers in this learning competition from where i got good ideas to implement this one. See ya !"}}