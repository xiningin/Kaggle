{"cell_type":{"23765e48":"code","ac19914a":"code","8d9d54a7":"code","5044c8ac":"code","d0b39150":"code","dd6f4fc8":"code","9418cbae":"code","23c1cb3b":"code","c8ad5536":"code","09a6b61c":"code","5845e9ad":"code","f392fd55":"code","79e55370":"code","2e8b4672":"code","d1f7a196":"code","9ef64882":"code","d5fe59ff":"code","5acfa0cd":"code","99d3e7b1":"code","6a6dac7d":"code","5b10f051":"code","4e7d5c8c":"code","80a80f32":"code","2f8d8a10":"code","78922b5f":"code","f3a2acde":"code","215fad30":"code","dc841138":"code","0ae13901":"code","d4c760d9":"code","02e6a9b1":"code","efa45033":"code","5e973b11":"code","ca69a888":"code","96585ec2":"code","394d2777":"code","f214c2cf":"code","9e2cfa1c":"code","73598f4f":"code","0a8c3f0a":"code","12d4ddd8":"code","44e59e61":"code","ee77f9fb":"code","b218864e":"code","33cb7419":"code","bcc8b0ce":"code","d93126bf":"code","a6798caf":"code","93566658":"code","50c80244":"code","601ef476":"code","d598f710":"code","25171095":"code","a77104a5":"code","3017cc05":"code","197e461d":"code","5c759abd":"code","9e80f01c":"code","f4885659":"code","e24cbca6":"code","cc09e6f5":"code","a9a0c2ca":"code","47f366f5":"code","e48c9345":"code","b8e00fa1":"code","ed8885a5":"code","1767fc3a":"code","81c28985":"code","893839c0":"code","9e426743":"code","517fa618":"code","79315c94":"markdown","276312fc":"markdown","2415a2d2":"markdown","140b9666":"markdown","1e402fb0":"markdown","4b4697ae":"markdown","312cf6b0":"markdown","621fba2b":"markdown","9f9920b6":"markdown","d6c1cbe0":"markdown","51a7a224":"markdown","8708f2fe":"markdown","dd253115":"markdown","db67c908":"markdown","11b690c2":"markdown","f6ae2d35":"markdown","874a894d":"markdown","a25c6750":"markdown","39d68072":"markdown","ab996b36":"markdown","066b733e":"markdown","9ca11177":"markdown","6b6fb4cf":"markdown","c0176ee8":"markdown","d9b7cb77":"markdown"},"source":{"23765e48":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import f1_score\nimport warnings\nwarnings.filterwarnings('ignore')","ac19914a":"# Load the data\nTRAIN_PATH = \"..\/input\/17live-test\/train.csv\"\nTEST_PATH = \"..\/input\/17live-test\/test.csv\"\n\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)","8d9d54a7":"# Print the size and shape of the dataset\ntrain_size = train_df.shape\ntest_size = test_df.shape\n\nprint(\"Number of rows in train data {} and number of columns in {} train data\".format(train_size[0], train_size[1]))\nprint(\"Number of rows in test data {} and number of columns in {} test data\".format(test_size[0], test_size[1]))","5044c8ac":"train_df.head()","d0b39150":"# check the value counts for each ID\ntrain_df['patient_id'].value_counts()","dd6f4fc8":"test_df['patient_id'].value_counts()","9418cbae":"# check if there are any common rows\npd.Index(train_df['patient_id']).intersection(pd.Index(test_df['patient_id']))","23c1cb3b":"# Check for Null Values\ndef check_null_columns(df):\n\n  # Dictionary to store the null columns and the corresponding null value count\n  null_columns = {}\n  total_rows = df.shape[0]\n\n  for col in df.columns:\n    # check if null values are there\n    null_count = df[col].isnull().sum()\n    if null_count > 0:\n      null_pct = null_count*100\/total_rows\n      null_columns[col] = [null_count, str(np.round(null_pct,2)) + '%']\n\n  return null_columns\n\n# call the above function for train and test set\ntrain_null = check_null_columns(train_df)\ntest_null = check_null_columns(test_df)","c8ad5536":"print(train_null)","09a6b61c":"print(test_null)","5845e9ad":"test_df[test_df['symptom61'].isnull() == True]","f392fd55":"test_df.loc[1495] = test_df.loc[1495].fillna(0)","79e55370":"train_df.head()","2e8b4672":"# check for correlation amongst variables to see if it can help in substituting null values\ncorr_matrix = train_df.corr()['year_of_death']\n\n# finding the top 10 correlated features\nprint(\"=========TOP 10 CORRELATED FEATURES FOR 'year_of_death'============\")\nprint()\nprint(np.abs(corr_matrix).sort_values(ascending=False).iloc[ : 10])","d1f7a196":"# plot a scatter plot\nplt.scatter(train_df['year_of_diagnosis'], train_df['year_of_death'])\nplt.xlabel('Year of Diagnosis')\nplt.ylabel('Year of Death')\nplt.title(\"Year of Diagnosis V\/S Year of Death\")\nplt.show()","9ef64882":"yod_train_df = train_df[['year_of_diagnosis', 'year_of_death']]\nyod_test_df = test_df[['year_of_diagnosis', 'year_of_death']]\n\n\n# Divide into train and test\ntest_set = yod_train_df[yod_train_df['year_of_death'].isnull() == True]['year_of_diagnosis']\ntest_indices = test_set.index\n\ntrain_set = yod_train_df.drop(index=test_indices)\n\n# Create training and taret variables\nX = train_set['year_of_diagnosis']\ny = train_set['year_of_death']\n\n# build regression model with cross validation\nlr = LinearRegression()\ncv_model = cross_val_score(lr, X.values.reshape(-1,1), y.values.reshape(-1,1), cv=3, n_jobs=-1)","d5fe59ff":"print(\"Mean cross validation score of linear model {:.3f}\".format(np.mean(cv_model)))","5acfa0cd":"sns.distplot(train_df['year_of_death'])\nplt.xlabel(\"Year of Death\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Train Set\")\nplt.grid()\nplt.show()\n\nsns.distplot(test_df['year_of_death'])\nplt.xlabel(\"Year of Death\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Test Set\")\nplt.grid()\nplt.show()","99d3e7b1":"sns.boxplot(train_df['year_of_death'])\nplt.xlabel(\"Year of Death\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Train Set\")\nplt.grid()\nplt.show()\n\nsns.boxplot(train_df['year_of_death'])\nplt.xlabel(\"Year of Death\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Test Set\")\nplt.grid()\nplt.show()","6a6dac7d":"def check_normality(df, column='year_of_death'):\n\n  # if the mean, median and mode values are equal than the dataset follows a normal distribution\n\n  mean_value = np.round(df['year_of_death'].mean())\n  median_value = df['year_of_death'].median()\n  mode_value = df['year_of_death'].mode().loc[0]\n\n  if (mean_value == median_value) and (mean_value == mode_value) and (mode_value == median_value):\n    return True\n    \n  return False\n\n\n# Check normality on both train and test set\nif check_normality(train_df):\n  print(\"Year of Death column in train data is normally distributed\")\nelse:\n  print(\"Year of Death column in train data is not normally distributed\")\n\n\nif check_normality(test_df):\n  print(\"Year of Death column in test data is normally distributed\")\nelse:\n  print(\"Year of Death column in test data is not normally distributed\")","5b10f051":"# Using a third statistical test to check for normality of the data \n\n\n'''68 -- 95 -- 99.7 rule'''\n\nupper_limit_1 = train_df['year_of_death'].mean() + train_df['year_of_death'].std()\nlower_limit_1 = train_df['year_of_death'].mean() - train_df['year_of_death'].std()\n\nupper_limit_2 = train_df['year_of_death'].mean() + 2*train_df['year_of_death'].std()\nlower_limit_2 = train_df['year_of_death'].mean() - 2*train_df['year_of_death'].std()\n\nupper_limit_3 = train_df['year_of_death'].mean() + 3*train_df['year_of_death'].std()\nlower_limit_3 = train_df['year_of_death'].mean() - 3*train_df['year_of_death'].std()","4e7d5c8c":"total_rows = train_df.shape[0]\n\n# find the number of rows between 1, 2 and 3 standard deviations\ndata_1_dev = train_df[(train_df['year_of_death'] >= lower_limit_1) & (train_df['year_of_death'] <= upper_limit_1)].shape[0]\ndata_2_dev = train_df[(train_df['year_of_death'] >= lower_limit_2) & (train_df['year_of_death'] <= upper_limit_2)].shape[0]\ndata_3_dev = train_df[(train_df['year_of_death'] >= lower_limit_3) & (train_df['year_of_death'] <= upper_limit_3)].shape[0]","80a80f32":"print(\"Percentage of data between first standard deviations {:.2f}%\".format(data_1_dev*100\/total_rows))\nprint(\"Percentage of data between second standard deviations {:.2f}%\".format(data_2_dev*100\/total_rows))\nprint(\"Percentage of data between third standard deviations {:.2f}%\".format(data_3_dev*100\/total_rows))","2f8d8a10":"# Substituting a random value\ntrain_df['year_of_death'] = train_df['year_of_death'].fillna(99999)\ntest_df['year_of_death'] = test_df['year_of_death'].fillna(99999)","78922b5f":"# Imputing null values in race column\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\nsns.countplot(data=train_df, x='race', ax=ax[0])\nsns.countplot(data=test_df, x='race', ax=ax[1])\nax[0].set_title('Race Distribution in train set')\nax[1].set_title('Race Distribution in test set')\nplt.show()","f3a2acde":"# Finding the mode race and imputing in train and test set\nmode_race = train_df['race'].mode().loc[0]\ntrain_df['race'] = train_df['race'].fillna(mode_race)\ntest_df['race'] = test_df['race'].fillna(mode_race)","215fad30":"# Age group has just 0.85% of null values in thre train set and 0.76% in test set. So we can just put a new category called unkown\ntrain_df.fillna('unknown', inplace = True)\ntest_df.fillna('unknown', inplace=True)","dc841138":"# age_group\nfig, ax = plt.subplots(1, 2, figsize=(15,5))\nsns.countplot(data=train_df.sort_values(by='age_group'), x = 'age_group', ax=ax[0])\nsns.countplot(data=test_df.sort_values(by='age_group'), x = 'age_group', ax=ax[1])\nax[0].set_title('Train Set')\nax[1].set_title('Test Set')\nplt.show()","0ae13901":"# age_group across year of diagnosis\nfig, ax = plt.subplots(2, 1, figsize=(15,10))\nsns.countplot(data=train_df.sort_values(by='age_group'), x = 'age_group', hue='year_of_diagnosis', ax=ax[0])\nsns.countplot(data=test_df.sort_values(by='age_group'), x = 'age_group', hue='year_of_diagnosis', ax=ax[1])\nax[0].set_title('Train Set')\nax[1].set_title('Test Set')\nplt.show()","d4c760d9":"# age_group across year of diagnosis\nfig, ax = plt.subplots(2, 1, figsize=(15,10))\nsns.countplot(data=train_df.sort_values(by='age_group'), x = 'age_group', hue='year_of_death', ax=ax[0])\nsns.countplot(data=test_df.sort_values(by='age_group'), x = 'age_group', hue='year_of_death', ax=ax[1])\nax[0].set_title('Train Set')\nax[1].set_title('Test Set')\nplt.show()","02e6a9b1":"# Age group across tumor cell count\nplt.figure(figsize=(12,8))\nsns.boxplot(data=train_df.sort_values(by='age_group'), x='age_group', y='tumor_cell_count')\nplt.xlabel(\"Age Group\")\n#plt.ylim(0,10)\nplt.ylabel(\"Tumor Cell Count\")\nplt.title(\"Age Group Across Tumor Cell Count\")\nplt.show()","efa45033":"# ZOOM VIEW\n\n# Age group across tumor cell count\nplt.figure(figsize=(12,8))\nsns.boxplot(data=train_df.sort_values(by='age_group'), x='age_group', y='tumor_cell_count')\nplt.xlabel(\"Age Group\")\nplt.ylim(0,10)\nplt.ylabel(\"Tumor Cell Count\")\nplt.title(\"Age Group Across Tumor Cell Count\")\nplt.grid()\nplt.show()","5e973b11":"# Symptoms distributions\ntrain_df['symptoms'].value_counts()","ca69a888":"# plot top 10 symptoms by count\ntrain_df['symptoms'].value_counts().iloc[ : 20].sort_values().plot(kind='barh', figsize=(12,8))\nplt.xlabel(\"Symptoms Count\")\nplt.ylabel(\"Symptoms\")\nplt.grid()\nplt.show()","96585ec2":"sns.scatterplot(data=train_df, x='tumor_cell_count', y='smoke_units')\nplt.show()","394d2777":"sns.countplot(data=train_df, x='death_due_to_cancer')\nplt.show()","f214c2cf":"sns.boxplot(data=train_df, x='death_due_to_cancer', y='tumor_cell_count')\nplt.ylim(0,20)\nplt.show()","9e2cfa1c":"sns.boxplot(data=train_df, x='death_due_to_cancer', y='smoke_units')\nplt.show()","73598f4f":"sns.boxplot(data=train_df, x='death_due_to_cancer', y='alcohol_consume_units')\nplt.show()","0a8c3f0a":"sns.scatterplot(data=train_df, x='smoke_units', y='alcohol_consume_units')\nplt.show()","12d4ddd8":"train_df[['smoke_units','alcohol_consume_units']].corr()","44e59e61":"train_df.head()","ee77f9fb":"# drop the id column\ntrain_df.drop(['patient_id'], axis=1, inplace=True)\n\n# Store the test id column\ntest_id = test_df['patient_id']\ntest_df_copy = test_df.copy()\ntest_df.drop(['patient_id'], axis=1, inplace=True)","b218864e":"X = train_df.drop('death_due_to_cancer', axis=1)\ny = train_df['death_due_to_cancer']\n\ntrain_len = X.shape[0]\ntest_len = test_df.shape[0]\n\n# Concatenate the train and the test set\nconcat_df = pd.concat([X, test_df], axis=0)","33cb7419":"concat_df","bcc8b0ce":"# Encode the object type categorical columns\ncols_to_encode = concat_df.select_dtypes('object').columns\n\nfor cols in cols_to_encode:\n  le = LabelEncoder()\n  concat_df[cols] = le.fit_transform(concat_df[cols])","d93126bf":"# Scale the data\nfor col in X.columns:\n    ss = StandardScaler()\n    concat_df[col] = ss.fit_transform(concat_df[col].values.reshape(-1,1))","a6798caf":"X_final = concat_df.iloc[ : train_len]\ntest_final = concat_df.iloc[train_len : ].reset_index().drop('index', axis=1)","93566658":"test_final.shape","50c80244":"# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)","601ef476":"# Compute metrics\ndef compute_f1_score(model_dict, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test):\n\n  model = list(model_dict.keys())[0]\n  model_obj = list(model_dict.values())[0]\n\n  # make predictions\n  train_predictions = model_obj.predict(X_train)\n  test_predictions = model_obj.predict(X_test)\n\n  # compute f1 score\n  train_f1_score = 100*f1_score(y_train, train_predictions)\n  test_f1_score = 100*f1_score(y_test, test_predictions)\n\n  # store the result\n  result = pd.DataFrame(data=[[train_f1_score, test_f1_score]], columns = ['Train_f1_score', 'Test_f1_score'], index=[model])\n  return result","d598f710":"# Baseline Model\n\n# 1. KNN\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, y_train)\n\n# Compute score\nmodel_dict = {'knn' : knn_clf}\nknn_score = compute_f1_score(model_dict)","25171095":"knn_score","a77104a5":"# 2. Logistic Regression\nlr_clf = LogisticRegression()\nlr_clf.fit(X_train, y_train)\n\n# Compute score\nmodel_dict = {'logistic_regression' : lr_clf}\nlr_score = compute_f1_score(model_dict)","3017cc05":"lr_score","197e461d":"# 3. Decision Tree\ndt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train, y_train)\n\n# Compute score\nmodel_dict = {'decision_tree' : dt_clf}\ndt_score = compute_f1_score(model_dict)","5c759abd":"dt_score","9e80f01c":"# 4. Random Forest\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train, y_train)\n\n# Compute score\nmodel_dict = {'random_forest' : rf_clf}\nrf_score = compute_f1_score(model_dict)","f4885659":"rf_score","e24cbca6":"# 5. Extra Trees\next_clf = ExtraTreesClassifier()\next_clf.fit(X_train, y_train)\n\n# Compute score\nmodel_dict = {'extra_trees' : ext_clf}\next_score = compute_f1_score(model_dict)","cc09e6f5":"ext_score","a9a0c2ca":"# 6. XGBoost\nxgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, y_train)\n\n# Compute score\nmodel_dict = {'xgboost' : xgb_clf}\nxgb_score = compute_f1_score(model_dict)","47f366f5":"xgb_score","e48c9345":"# 7. Light GBM\nlgbm_clf = LGBMClassifier()\nlgbm_clf.fit(X_train, y_train)\n\n# Compute score\nmodel_dict = {'lightgbm' : lgbm_clf}\nlgbm_score = compute_f1_score(model_dict)","b8e00fa1":"lgbm_score","ed8885a5":"# Concat the results\nfinal_results = pd.concat([knn_score, \n                           lr_score, \n                           dt_score, \n                           rf_score, \n                           ext_score,\n                           xgb_score, \n                           lgbm_score], axis=0).sort_values(by='Test_f1_score', ascending=False)","1767fc3a":"final_results","81c28985":"# Make baseline submissions\nbaseline_predictions = lr_clf.predict(test_final)\nbaseline_submissions = np.concatenate([test_id.values.reshape(-1,1), baseline_predictions.reshape(-1,1)], axis=1)\nbaseline_submissions_df = pd.DataFrame(baseline_submissions, columns=['patient_id', 'death_due_to_cancer'])","893839c0":"baseline_submissions_df.head()","9e426743":"# Convert to csv file\nbaseline_submissions_df.to_csv('.\/baseline_predictions.csv', index=False)","517fa618":"baseline_submissions_df.shape","79315c94":"**We are getting a poor mean cv R squared score, another approach should be tried for imputing null values.**","276312fc":"**For patient_id = TCBA-B8-0x1338, the value for all the symptoms is NaN.**","2415a2d2":"# Machine Learning Modelling","140b9666":"**Similarly, aclohol consumption doesn't show any impact on deaths due to cancer.**","1e402fb0":"## Substituting null values","4b4697ae":"**left skewed distribution.**","312cf6b0":"**year_of_death has a strong positive correlation with year_of_diagnosis and a relatively less correlation with death_due_to_cancer. Column death_due_to_cancer cannot be considered because it is the target variable**","621fba2b":"**People having less number of tumor cells survived more.**","9f9920b6":"## EDA","d6c1cbe0":"**Smoking and Alcohol consumption has no effective correlation**","51a7a224":"**Logistic Regression seem to be more generalizing model. Slight overfitting can be observed with bagging based models**","8708f2fe":"# Data Analysis","dd253115":"**Ensemble models especially bagging based models are overfitting the data.**","db67c908":"# Data Preprocessing","11b690c2":"**People with large number of tumor cells are generally of higher age groups.**","f6ae2d35":"**Model's perfomance can be imporved by more data preprocessing and hyperparameter tuning.**","874a894d":"**Next step, building a simple regression model to impute null values for 'year_of_death'**","a25c6750":"\n**In case of train_set**\n1.   age_group column has only 0.85% null values, so these rows can be dropped and there would be not much information loss.\n2.   race and year_of_death have 10.26% and 13.62% of null values respectively.\n\n","39d68072":"**There are no common patient IDs in the train and the test set.**","ab996b36":"**There are total 553 symptoms**","066b733e":"**We have very small dataset.**","9ca11177":"**Smoking doesn't actually show any impact in deaths due to cancer.**","6b6fb4cf":"**Distribution is not normal**","c0176ee8":"**Clearly, white race has the maximum entries in the dataset. So, it seems logical to impute null values with mode in this case.**","d9b7cb77":"**In case of test_set**\n\n\n1.   the percentage of null values for age_group, race and year_of_death is similar to that of train_set.\n2.   The rest of the columns also have 1 null value each, it seems we have one entire row which is null, it should be dropped.\n\n"}}