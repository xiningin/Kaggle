{"cell_type":{"62906bc2":"code","0a94dfbe":"code","7b51ae45":"code","6b58023d":"code","76897aa5":"code","c8d9a430":"code","39d6b0c6":"code","a6e579ab":"code","367093bb":"code","207c4fac":"code","8108d9c8":"code","d792c812":"code","4f896410":"code","cb35e1b6":"code","d57433d0":"code","c464b531":"code","c6b96565":"code","43bc15f4":"code","1cb36cf9":"code","8b65dcc6":"code","6345865f":"code","c7033b06":"code","bd96e4fe":"code","ae7d1597":"code","d60fbd36":"code","ce9b4e65":"code","2cb9f926":"markdown","2bc19f77":"markdown","d755d1bd":"markdown"},"source":{"62906bc2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn import preprocessing, linear_model, ensemble, metrics, model_selection, svm, pipeline, naive_bayes\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nimport nltk\nimport spacy\nimport textblob\n#from nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn import decomposition\nfrom nltk.corpus import stopwords \nsw = stopwords.words(\"english\")\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0a94dfbe":"# Read Data\ntrain = pd.read_csv('\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/test.csv')\n","7b51ae45":"# Dimensionality of embedding\nn = 200","6b58023d":"file = open('\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt', 'r')\nraw = file.read()\nprint('Read Raw File')\n\n# Get Embeddings in List of Lists format\nemb_list = [val.split(' ') for val in raw.split('\\n')]\nprint('Read as List of Lists')\n\n# Convert Embeddings to dataframe format\nemb = pd.DataFrame(emb_list).T\nemb.columns = emb.iloc[0, :]\nemb = emb.drop(0, axis = 0).astype(float)\nprint('Embeddings converted to DataFrame')","76897aa5":"train_id = train['ID']\ntest_id = test['ID']\n\n# Create indices to split train and test on later\ntrain['train_ind'] = np.arange(train.shape[0])\ntest['train_ind'] = np.arange(train.shape[0], train.shape[0]+test.shape[0])\n\n# Merge Train and Test - This approach only works for competitions - not for model deployment in real projects.\ndata = pd.concat([train, test], axis = 0)","c8d9a430":"# Optional Step\ndata['ABSTRACT'] = data['TITLE'] + ' ' + data['ABSTRACT']","39d6b0c6":"def get_embeddings(word, n = 200):\n    if word.lower() in emb:\n        return emb[word.lower()].values.tolist()\n    else:\n        return np.zeros(n).tolist()\n","a6e579ab":"from tqdm import notebook\ncount = 0\nfinal_emb = []\nfor index, row in notebook.tqdm(data.iterrows()):\n    \n    emb_data = []\n    text = row['ABSTRACT']\n    \n    # Tokenize\n    words = word_tokenize(text)\n    \n    # Remove stopwords and ensure alphanumeric words\n    words = [w for w in words if w not in sw]\n    words = [w for w in words if w.isalpha()]\n    \n    # Sum up embeddings for each word\n    for word in words:\n        emb_data.append(get_embeddings(word, n))\n        \n    sentence_embedding_all = np.array(emb_data)\n\n    sum_vec = sentence_embedding_all.sum(axis = 0)\/np.sqrt((sentence_embedding_all.sum(axis = 0) ** 2).sum())\n    final_emb.append(sum_vec)\nfinal_emb = np.array(final_emb)\n\n","367093bb":"# Split the data back to train and test\nX_train = final_emb[:train.shape[0], :]\ny_train = data[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']].iloc[:train.shape[0]]\n\nX_test = final_emb[train.shape[0]:, :]\ny_test = data[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']].iloc[train.shape[0]:]","207c4fac":"# Train model - Logistic Regression is a good option for Text classification problems\n#model = linear_model.LogisticRegressionCV(penalty = 'l2', Cs = 10, max_iter = 5000).fit(X_train, y_train)\n#model = linear_model.RidgeClassifierCV().fit(X_train, y_train)\nfrom sklearn import naive_bayes\n\n#model = MultiOutputClassifier(estimator = naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)).fit(X_train, y_train)\nmodel = MultiOutputClassifier(estimator = linear_model.LogisticRegressionCV(Cs = 10, cv = 5, n_jobs = -1, max_iter = 5000)).fit(X_train, y_train)","8108d9c8":"def get_preds_multioutput(predictions):\n    return np.array([[val[1] for val in inner] for inner in predictions]).T\n\ndef convert_probs_to_labels(predictions, threshold = .5, labels = None):\n    final = []\n    for prediction in predictions:\n        temp = (prediction > threshold)*1\n        final.append(temp)\n        \n    return final\n\ndef predict_1(predictions, threshold=.5):\n    preds = get_preds_multioutput(predictions)\n    preds = convert_probs_to_labels(preds, threshold = threshold, labels = None)\n    return np.array(preds)\n\n#predict_1(model.predict_proba(X_test))","d792c812":"sub = pd.DataFrame()\nsub['ID'] = test_id\n\n#preds = predict_1(model.predict_proba(X_test))\nsub[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']] = model.predict(X_test).astype(int)\nsub.to_csv('sub.csv', index = None)","4f896410":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom  tqdm import tqdm","cb35e1b6":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['ABSTRACT']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in sw))]\n        corpus.append(words)\n    return corpus\ncorpus=create_corpus(data)\n","d57433d0":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()\n","c464b531":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\npadded=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","c6b96565":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","43bc15f4":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n            ","1cb36cf9":"embedding_matrix.shape","8b65dcc6":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences = True))\nmodel.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences = False))\nmodel.add(Dense(256, activation = 'relu'))\n\nmodel.add(Dense(6, activation='sigmoid'))\n\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","6345865f":"model.summary()","c7033b06":"# Split the data back to train and test\nX_train = padded[:train.shape[0], :]\ny_train = data[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']].iloc[:train.shape[0]]\n\nX_test = padded[train.shape[0]:, :]\ny_test = data[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']].iloc[train.shape[0]:]","bd96e4fe":"X_train1,X_test1,y_train1,y_test1=train_test_split(X_train,y_train,test_size=0.15)\nprint('Shape of train',X_train1.shape)\nprint(\"Shape of Validation \",X_test1.shape)","ae7d1597":"history=model.fit(X_train1,y_train1,batch_size=512,epochs=30,validation_data=(X_test1,y_test1))","d60fbd36":"plt.figure(figsize = (14, 4))\nplt.plot(history.history['loss'], label = 'train loss')\nplt.plot(history.history['val_loss'], label = 'val_loss')\nplt.legend()","ce9b4e65":"sub = pd.DataFrame()\nsub['ID'] = test_id\n\n#preds = predict_1(model.predict_proba(X_test))\nsub[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']] = (model.predict(X_test)>.6).astype(int)\nsub.to_csv('sub.csv', index = None)","2cb9f926":"**We have the task of predicting which subject a set of abstract and Title belong to. It is a multi-label classification problem - so each abstract can belong to multiple subjects**\n\n**We explore 2 approaches:**\n1. **Shallow Learning using GloVe embeddings**\n2. **Deep Learning using GloVe embeddings**","2bc19f77":"## Deep Learning with GloVe Word Embeddings\nCode for creating Embedding matrix for Deep learning purposes taken from:\nhttps:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n","d755d1bd":"## Shallow Learning Approach using GloVe"}}