{"cell_type":{"5337f04e":"code","5731c92c":"code","13af13ff":"code","9c97cb24":"code","8db2df36":"code","ba3261a4":"code","f626085b":"code","055eb9a9":"code","4f1467eb":"code","4301363d":"code","b876f801":"code","f812f85d":"code","7e8f700f":"code","0229998b":"code","6af057d7":"code","bf98ad0d":"code","8ed49a49":"code","1d280bb5":"code","766b898a":"code","a0890c35":"code","9f230079":"code","2113ee1c":"code","ab53c3f7":"code","a4acb156":"code","053e3d42":"code","8b4c97b4":"code","c55bc7e4":"code","833f2865":"code","18dcd695":"code","b5c910bd":"code","aab2c40b":"code","8df387ee":"code","210e5330":"code","09594c13":"code","d47460b6":"code","3ee6dab2":"code","e0785141":"code","281b1a14":"code","f846d764":"code","c16d510a":"code","029dab79":"code","1ff32d65":"code","064cbc1f":"code","ff4887d2":"code","e1d775af":"code","4260d8dd":"code","914a054b":"code","b1356d91":"code","3395a551":"code","c2198685":"code","ab29622b":"code","4ac69030":"code","a077f643":"code","fd0912ef":"code","451f6e06":"code","4e1f15bb":"code","a03cc94c":"code","cd91b3a7":"markdown","cff10335":"markdown","848b0614":"markdown","8eb03f18":"markdown","ffece20f":"markdown","303bccbf":"markdown"},"source":{"5337f04e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5731c92c":"! pip install underthesea","13af13ff":"import numpy as np\nimport pandas as pd\nimport pylab as plt\nimport matplotlib.pyplot as plt\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import cohen_kappa_score\n\nfrom tensorflow.python.keras.models import Sequential, load_model\nfrom tensorflow.python.keras.layers import Dense, Dropout\nfrom tensorflow.python.keras import optimizers\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nimport pickle\nimport seaborn as sns\n\nfrom imblearn.over_sampling import SMOTE\nfrom nltk.corpus import stopwords\n\nimport keras\nimport io\nimport requests\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n\nfrom underthesea import word_tokenize\n\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\nfrom keras import backend as K\nK.tensorflow_backend._get_available_gpus()\nimport codecs","9c97cb24":"def create_stopwordlist():\n    f = codecs.open('\/kaggle\/input\/vietnamese-stopwords\/vietnamese-stopwords.txt', encoding='utf-8')\n    data = []\n    null_data = []\n    for i, line in enumerate(f):\n        line = repr(line)\n        line = line[1:len(line)-3]\n        data.append(line)\n    return data\n","8db2df36":"stopword_vn = create_stopwordlist()","ba3261a4":"import string\n\ndef tokenize(text):\n    text =  text.translate(str.maketrans('', '', string.punctuation))\n    return [word for word in word_tokenize(text.lower()) if word not in stopword_vn]","f626085b":"def choose_vectorizer(option, name='tf_idf'):\n    if option == 'generate':\n        if name == 'tf_idf':\n            vectorizer = TfidfVectorizer(tokenizer = tokenize,ngram_range=(1,4), min_df=5, max_df= 0.8, max_features= 5000, sublinear_tf=True)\n        else:\n            vectorizer = CountVectorizer(tokenizer = tokenize, ngram_range=(1,4), max_df=0.8, min_df=5, max_features = 5000, sublinear_tf=True)\n    elif option == 'load':\n        if name == 'tf_idf':\n            vectorizer = TfidfVectorizer(vocabulary = pickle.load(open('..\/input\/kpdl-data\/vocabulary_2.pkl', 'rb')), ngram_range=(1,3), min_df=5, max_df= 0.8, max_features=15000, sublinear_tf=True)\n        else:\n            vectorizer = CountVectorizer(vocabulary = pickle.load(open('..\/input\/kpdl-data\/vocabulary_2.pkl', 'rb')), ngram_range=(1,3), max_df=0.8, min_df=5, max_features = 15000, sublinear_tf=True)\n    return vectorizer","055eb9a9":"# data = pd.read_csv('..\/input\/kpdl-data\/train_v1.csv')\ndata = pd.read_csv('..\/input\/vietnamese-sentiment-analyst\/data - data.csv')\n\ndata.head(2)","4f1467eb":"df = data.loc[data['comment'] == None]","4301363d":"df","b876f801":"category = data['rate'].unique()\ncategory_to_id = {cate: idx for idx, cate in enumerate(category)}\nid_to_category = {idx: cate for idx, cate in enumerate(category)}\nprint(category_to_id)\nprint(id_to_category)","f812f85d":"data_label = data['rate']\ndata_label = pd.DataFrame(data_label, columns=['rate']).groupby('rate').size()\ndata_label.plot.pie(figsize=(15, 15), autopct=\"%.2f%%\", fontsize=12)","7e8f700f":"X_train, X_test, y_train, y_test = train_test_split(data['comment'], data['rate'], test_size = .15, shuffle = True, stratify=data['rate'])\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = .2, shuffle = True, stratify=y_train)","0229998b":"X_train.shape","6af057d7":"X_train_df = pd.DataFrame(X_train, columns=['comment'])\nX_valid_df = pd.DataFrame(X_valid, columns=['comment'])\nX_test_df = pd.DataFrame(X_test, columns=['comment'])\nprint(X_train_df.head(10))\nprint(X_valid_df.head(10))\nprint(X_test_df.head(10))","bf98ad0d":"y_train","8ed49a49":"%%time\noptions = ['generate', 'load']\n# 0 to generate, 1 to load (choose wisely, your life depends on it!)\noption = options[0] \nvectorizer = choose_vectorizer(option)\n\nX_train = vectorizer.fit_transform(X_train).toarray()\nX_valid = vectorizer.transform(X_valid).toarray()\nX_test = vectorizer.transform(X_test).toarray()\n    \nif option == 'generate':\n    pickle.dump(vectorizer.vocabulary_, open('vocabulary_3.pkl', 'wb'))","1d280bb5":"print(X_train.shape, X_valid.shape, X_test.shape)\n","766b898a":"X_train.shape","a0890c35":"y_ = y_train.map(category_to_id).values\ny_train = np.zeros((len(y_), y_.max()+1))\ny_train[np.arange(len(y_)), y_] = 1\n# y_train = y_\n\ny_ = y_test.map(category_to_id).values\ny_test = np.zeros((len(y_), y_.max()+1))\ny_test[np.arange(len(y_)), y_] = 1\n# y_test = y_\n\ny_ = y_valid.map(category_to_id).values\ny_valid = np.zeros((len(y_), y_.max()+1))\ny_valid[np.arange(len(y_)), y_] = 1\n# y_valid = y_","9f230079":"print(y_train.sum(1))\nprint(y_valid.sum(1))\nprint(y_test.sum(1))\nprint(y_train.shape, y_valid.shape, y_test.shape)","2113ee1c":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nDROPOUT = 0.3\nACTIVATION = \"relu\"\n\nmodel = Sequential([    \n    Dense(1000, activation=ACTIVATION, input_dim=X_train.shape[1]),\n    Dropout(DROPOUT),\n    Dense(500, activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(300, activation=ACTIVATION),\n    Dropout(DROPOUT),\n#     Dense(200, activation=ACTIVATION),\n#     Dropout(DROPOUT),\n#     Dense(100, activation=ACTIVATION),\n#     Dropout(DROPOUT),\n#     Dense(50, activation=ACTIVATION),\n#     Dropout(DROPOUT),\n    Dense(5, activation='softmax'),\n])\n","ab53c3f7":"def recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\n","a4acb156":"model.compile(optimizer=optimizers.Adam(0.001), loss='categorical_crossentropy', metrics=['acc', f1_m,precision_m, recall_m])\n\nmodel.summary()\nes = EarlyStopping(monitor='val_f1_m', mode='max', verbose=1, patience=5)\nreduce_lr = ReduceLROnPlateau(monitor='val_f1_m', factor=0.2, patience=8, min_lr=1e7)\ncheckpoint = ModelCheckpoint('best_full.h5', monitor='val_f1_m', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)","053e3d42":"EPOCHS = 25\nBATCHSIZE = 4","8b4c97b4":"model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCHSIZE, validation_data=(X_valid, y_valid), callbacks=[es, reduce_lr, checkpoint])","c55bc7e4":"x = np.arange(EPOCHS)\nhistory = model.history.history\n","833f2865":"# import tensorflow as tf \n\n# model = tf.keras.models.load_model('..\/input\/kpdl-base\/my_model.h5')","18dcd695":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.models import model_from_json\n\nmodel.save('my_model.h5')","b5c910bd":"predict_train = model.predict(X_train)\npredict_valid = model.predict(X_valid)\npredict_test = model.predict(X_test)\nprint(predict_train.shape, predict_valid.shape, predict_test.shape)","aab2c40b":"predict_train_label = predict_train.argmax(-1)\npredict_valid_label = predict_valid.argmax(-1)\npredict_test_label = predict_test.argmax(-1)","8df387ee":"# predict_train_label","210e5330":"predict_train_label = [id_to_category[predict_train_label[idx]] for idx in range(len(predict_train))]\npredict_valid_label = [id_to_category[predict_valid_label[idx]] for idx in range(len(predict_valid))]\npredict_test_label = [id_to_category[predict_test_label[idx]] for idx in range(len(predict_test))]","09594c13":"# predict_train_label","d47460b6":"y_train_true = y_train.argmax(-1)\ny_valid_true = y_valid.argmax(-1)\ny_test_true = y_test.argmax(-1)","3ee6dab2":"# y_train_true = y_train\n# y_valid_true = y_valid\n# y_test_true = y_test","e0785141":"# y_train_true","281b1a14":"# y_train_true","f846d764":"y_train_label = [id_to_category[y_train_true[idx]] for idx in range(len(y_train_true))]\ny_valid_label = [id_to_category[y_valid_true[idx]] for idx in range(len(y_valid_true))]\ny_test_label = [id_to_category[y_test_true[idx]] for idx in range(len(y_test_true))]","c16d510a":"train_concat = np.concatenate((np.array(X_train_df['comment'].values).reshape(-1, 1), np.array(y_train_label).reshape(-1, 1), np.array(predict_train_label).reshape(-1, 1)), axis=-1)\nvalid_concat = np.concatenate((np.array(X_valid_df['comment'].values).reshape(-1, 1), np.array(y_valid_label).reshape(-1, 1), np.array(predict_valid_label).reshape(-1, 1)), axis=-1)\ntest_concat = np.concatenate((np.array(X_test_df['comment'].values).reshape(-1, 1), np.array(y_test_label).reshape(-1, 1), np.array(predict_test_label).reshape(-1, 1)), axis=-1)","029dab79":"# train_concat = np.concatenate((np.array(X_train_df['Content'].values).reshape(-1, 1), np.array(y_train_label).reshape(-1, 1)), axis=-1)\n# valid_concat = np.concatenate((np.array(X_valid_df['Content'].values).reshape(-1, 1), np.array(y_valid_label).reshape(-1, 1)), axis=-1)\n# test_concat = np.concatenate((np.array(X_test_df['Content'].values).reshape(-1, 1), np.array(y_test_label).reshape(-1, 1)), axis=-1)","1ff32d65":"# train_concat_predict_df = pd.DataFrame(train_concat, columns=['Content', 'True_Label'])\n# valid_concat_predict_df = pd.DataFrame(valid_concat, columns=['Content', 'True_Label'])\n# test_concat_predict_df = pd.DataFrame(test_concat, columns=['Content', 'True_Label'])","064cbc1f":"train_concat_predict_df = pd.DataFrame(train_concat, columns=['comment', 'True_Label', 'Predict'])\nvalid_concat_predict_df = pd.DataFrame(valid_concat, columns=['comment', 'True_Label', 'Predict'])\ntest_concat_predict_df = pd.DataFrame(test_concat, columns=['comment', 'True_Label', 'Predict'])","ff4887d2":"train_concat_predict_df.head(20)","e1d775af":"valid_concat_predict_df.head(20)","4260d8dd":"test_concat_predict_df.head(20)","914a054b":"train_concat_predict_df.to_csv('train_concat_predict_df.csv', index=False)\nvalid_concat_predict_df.to_csv('valid_concat_predict_df.csv', index=False)\ntest_concat_predict_df.to_csv('test_concat_predict_df.csv', index=False)","b1356d91":"predict_test","3395a551":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nplt.figure(figsize=(8, 8))\nconf_mat = confusion_matrix(y_test_true, predict_test.argmax(-1))\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels=id_to_category.values(), yticklabels=id_to_category.values())\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\n","c2198685":"# y_test_true","ab29622b":"predict_test = predict_test.argmax(-1)","4ac69030":"from sklearn.metrics import f1_score\nscore = f1_score(y_test_true, predict_test, average='weighted')","a077f643":"score","fd0912ef":"labels = data['rate'].unique()","451f6e06":"for label in labels:\n    wrong = []\n    df = test_concat_predict_df.loc[test_concat_predict_df['True_Label'] == label]\n    df_content = df.values\n    for row in df_content:\n        if np.abs(int(row[1])- int(row[2])):\n            wrong.append(row)\n    df_wrong = pd.DataFrame(wrong, columns=['rate', 'true', 'predict'])\n    df_wrong.to_csv(f'{label}_test.csv')\n    print(label, df_wrong)","4e1f15bb":"for label in labels:\n    wrong = []\n    df = valid_concat_predict_df.loc[valid_concat_predict_df['True_Label'] == label]\n    df_content = df.values\n    for row in df_content:\n        if np.abs(int(row[1])- int(row[2])):\n            wrong.append(row)\n    df_wrong = pd.DataFrame(wrong, columns=['rate', 'true', 'predict'])\n    df_wrong.to_csv(f'{label}_valid.csv')\n    print(label, df_wrong.head())","a03cc94c":"for label in labels:\n    wrong = []\n    df = train_concat_predict_df.loc[train_concat_predict_df['True_Label'] == label]\n    df_content = df.values\n    for row in df_content:\n        if np.abs(int(row[1])- int(row[2])):\n            wrong.append(row)\n    df_wrong = pd.DataFrame(wrong, columns=['rate', 'true', 'predict'])\n    df_wrong.to_csv(f'{label}_train.csv')\n    print(label, df_wrong.head())","cd91b3a7":"### List Content that model predicted false","cff10335":"### Check distribution of train, valid and test set","848b0614":"### Save predict to csv file","8eb03f18":"### Predict in train, valid and test set","ffece20f":"* V14: train with tri grams and generate new vocab, num feat = 15000","303bccbf":"### Distribution of label"}}