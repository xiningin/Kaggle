{"cell_type":{"9f4780ac":"code","3c9fcf65":"code","42442da6":"code","c1046816":"code","e4d61834":"code","c578a09e":"code","b7356475":"code","e535bfb6":"code","35b34bf4":"code","8c88bc2a":"code","a4a18fcc":"code","b2915fea":"code","8f5164cd":"code","7a407840":"markdown","6af551ef":"markdown","b6ace856":"markdown","26a66b9e":"markdown","6ec2da4d":"markdown","e4f54634":"markdown","1023f98e":"markdown","7d207c8b":"markdown","1a2bdd3b":"markdown","ce84772a":"markdown","3489dace":"markdown"},"source":{"9f4780ac":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')","3c9fcf65":"df = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')","42442da6":"train = df.loc[:399, :]\nval = df.loc[400:, :]\nprint(train.info())\nprint(val.info())","c1046816":"train.head()","e4d61834":"train.drop('Serial No.', axis=1, inplace=True)\nval.drop('Serial No.', axis=1, inplace=True)","c578a09e":"sns.heatmap(train.corr())","b7356475":"train.drop('Research', axis=1, inplace=True)\nval.drop('Research', axis=1, inplace=True)","e535bfb6":"sns.pairplot(train)","35b34bf4":"X_test = val.loc[:, val.columns != 'Chance of Admit ']\ny_test = val.loc[:, 'Chance of Admit ']\nX = train.loc[:, train.columns != 'Chance of Admit ']\ny = train.loc[:, 'Chance of Admit ']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\nmodel1 = RandomForestRegressor()\nbags = 10\nseed = 1\nbagged_prediction = np.zeros(X_val.shape[0])\nfor n in range(0, bags):\n    model1.set_params(random_state = seed + n) \n    model1.fit(X_train, y_train)\n    preds1 = model1.predict(X_val)\n    bagged_prediction += preds1\nbagged_prediction \/= bags\ntest_preds1 = model1.predict(X_test)","8c88bc2a":"train['log_GRE'] = np.log(train['GRE Score'] + 1)\ntrain['log_TOEFL'] = np.log(train['TOEFL Score'] + 1)\ntrain['log_CGPA'] = np.log(train['CGPA'] + 1)\ntrain.drop('GRE Score', axis=1, inplace=True)\ntrain.drop('TOEFL Score', axis=1, inplace=True)\ntrain.drop('CGPA', axis=1, inplace=True)\n\nval['log_GRE'] = np.log(val['GRE Score'] + 1)\nval['log_TOEFL'] = np.log(val['TOEFL Score'] + 1)\nval['log_CGPA'] = np.log(val['CGPA'] + 1)\nval.drop('GRE Score', axis=1, inplace=True)\nval.drop('TOEFL Score', axis=1, inplace=True)\nval.drop('CGPA', axis=1, inplace=True)","a4a18fcc":"X_test = val.loc[:, val.columns != 'Chance of Admit ']\ny_test = val.loc[:, 'Chance of Admit ']\nX = train.loc[:, train.columns != 'Chance of Admit ']\ny = train.loc[:, 'Chance of Admit ']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\nmodel2 = LinearRegression().fit(X_train, y_train)\npreds2 = model2.predict(X_val)\ntest_preds2 = model2.predict(X_test)\n\nstacked_predictions = np.column_stack((preds1, preds2))\nstacked_test_predictions = np.column_stack((test_preds1, test_preds2))\nmeta_model = LinearRegression()\nmeta_model.fit(stacked_predictions, y_val)\nfinal_predictions = meta_model.predict(stacked_test_predictions)","b2915fea":"final_predictions","8f5164cd":"mean_squared_error(final_predictions, y_test, squared=False)","7a407840":"# Exploratory Data Analysis","6af551ef":"# Feature Engineering","b6ace856":"Our final metric is RMSE, which can be found by setting the squared parameter in mean_squared_error to false","26a66b9e":"Using .head() we see that the 'Serial No.' column does not add any value to the data so we drop that column in both sets","6ec2da4d":"# Graduate Admission Prediction Using Ensembling  \n\nThis notebook goes through the 'Predict Likelihood of Admission' task of the 'Graduate Admission 2' dataset. \n\n**Task Details**  \nUsing the supplied predictive variables (GRE score, TOEFL score, University Rating, etc) to predict the likelihood of admission of a new candidate.\n\n**Evaluation Criteria**  \nThe best model should be the one that evaluates to have the lowest RMSE overall, and please indicate the error you get on validation set containing the last 100 observations.\n\n**Expected Submission**  \nPlease submit a Kernel where the final cell outputs the RMSE score on the final 100 observations.","e4f54634":"The task requires optimization of RMSE on the last 100 observations of the dataset so we will split our train and validation so that the train set contains the first 300 observations and the validation set contains the rest. All values are non-null so we are not concerned with filling missing values. ","1023f98e":"The heatmap below shows that there is a very low correlation between 'Research' and all other features, so we drop that column as well. ","7d207c8b":"Our second model uses Linear Regression with the scaled values. Then we stack our predictions from the two models and take their average to create a meta model which we will use to generate our final predictions","1a2bdd3b":"Now we take replace GRE Score, TOEFL Score, and CGPA with its natural log due to some skewness observed in the pairplot above. We do this for both the train and validation sets","ce84772a":"The pairplot below roughly shows that the top 3 most correlated features with the chances of admit are: GRE scores, TOEFL scores, and Undergraduate GPA. \n\nAlong the main diagonal we also see the histograms of the distributions of each individual feature. We will apply a log transformation to some of the skewed features after fitting a Random Forest model since tree models are not concerned with scaling. ","3489dace":"We build a Random Forest model with the bagging technqiue. Our train set is further split into 80% train and 20% validation and we use the last 100 observations as the test set. "}}