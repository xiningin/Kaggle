{"cell_type":{"40e948b9":"code","ff739976":"code","7426d524":"code","7ca171fe":"code","e217d7e1":"code","bfbace1a":"code","35b50404":"code","f7529945":"code","8327c8f4":"code","59841a80":"code","efb5a0b3":"code","a598da33":"code","2c3466c7":"code","0192d894":"code","95752e9e":"code","3c677562":"code","5de55d04":"code","96673eea":"code","44b5eb86":"code","90076019":"code","e2215234":"code","e570436d":"code","d13de420":"code","980e03f2":"code","139931f7":"code","dc01507f":"code","c6f063be":"code","536bae18":"code","e611ef40":"code","d2c18c9c":"code","de14a1c4":"code","f4fa8a14":"code","6cbdbedc":"code","ba8b6947":"code","f02220b2":"code","a700d04b":"code","6943d003":"code","9c8cd9cc":"code","d3219f14":"code","52311002":"code","56c37746":"code","94a0f569":"code","6c8027d1":"code","d410a8e4":"code","a8d8b0b1":"code","3de93ea7":"code","2e4f1858":"markdown","a1495e56":"markdown","11c7dce3":"markdown","1ca06114":"markdown","12d89646":"markdown","66622383":"markdown","eedb0de3":"markdown","82fb55b6":"markdown","d4a26bad":"markdown","1d400f93":"markdown","11d55d52":"markdown","507fcfc8":"markdown","6c1eb3a3":"markdown","9c1e552d":"markdown","72cbdccc":"markdown","94292cc3":"markdown","b607d2f8":"markdown","ec272c95":"markdown","0accc57d":"markdown"},"source":{"40e948b9":"import seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport cufflinks as cf\nimport plotly.express as px\nimport matplotlib.pylab as plt\nfrom pandas.api.types import CategoricalDtype\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()\nsns.set_style(\"whitegrid\")\n# from category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score,train_test_split\nfrom xgboost import XGBClassifier","ff739976":"df=pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\").drop(\"id\",axis=1)","7426d524":"df.head(5)","7ca171fe":"df.dtypes","e217d7e1":"df.isna().sum()","bfbace1a":"df.describe() ","35b50404":"for i in df.select_dtypes(\"object\"):\n    for j in df[i].unique():\n        p=(df[df[i]==j].stroke.mean())*100\n        print(f\"if [{i}]== [{j}] , the probability of have stroke is [{round(p,2)}] %\")\n    print(\"-\"*60)","f7529945":"f,axes=plt.subplots(nrows=5,ncols=1,figsize=(10,10))\nf.suptitle(\"show the difference\")\nfor i,value in enumerate(df.select_dtypes(\"object\")):\n    sns.barplot(x=value,y=\"stroke\",data=df,ax=axes[i])\nplt.tight_layout()","8327c8f4":"# show the target varialbe\npx.pie(data_frame=df[\"stroke\"].value_counts(dropna=False),\n       names=df[\"stroke\"].value_counts(dropna=False).index,\n       values=df[\"stroke\"].value_counts(dropna=False).values,\n       title=\"Stroke values\",\n      )\n","59841a80":"f,axes=plt.subplots(nrows=1,ncols=2,figsize=(10,10))\nf.suptitle(\"the distribution of float data \")\nsns.distplot(x=df[\"bmi\"],color=\"red\",ax=axes[0])\nsns.distplot(x=df[\"avg_glucose_level\"],color=\"green\",ax=axes[1])","efb5a0b3":"def load_data(df_train,df_test):\n    x=pd.concat([df_train,df_test],axis=0)\n    x=clean(x)\n    x=encode(x)\n    train=x.loc[df_train.index,:] \n    test=x.loc[df_test.index,:]\n    return train,test","a598da33":"# searching for unusul value or replace them to be simple\ndef clean(df):\n    df[\"smoking_status\"]=df[\"smoking_status\"].replace({\n         \"formerly smoked\":\"before\",\n        \"never smoked\":\"never\"\n    })\n    df[\"work_type\"]=df[\"work_type\"].replace({\n        \"children\":\"child\",\n        \"Govt_jov\":\"gov\",\n        \"Never_worked\":\"never\",\n        \"Self-employed\":\"self\",\n        \"Private\":\"private\"\n    })\n    return df ","2c3466c7":"# correctly encode the object variables to categories\ndef encode(df):\n    df[\"smoking_status\"]=df[\"smoking_status\"].astype(CategoricalDtype([\"Unknown\",\"never\",\"before\",\"smokes\"],ordered=True))\n    for col in df.select_dtypes([\"object\"]):\n        df[col]=df[col].astype(\"category\")\n        if \"other\" not in df[col].cat.categories:\n            df[col].cat.add_categories(\"other\")\n    return df ","0192d894":"def normalize(df):\n    df[\"bmi\"]=np.log1p(df[\"bmi\"])\n    df[\"avg_glucose_level\"]=np.log1p(df[\"avg_glucose_level\"])\n    return df ","95752e9e":"x=df.drop(\"stroke\",axis=1)\ny=df[\"stroke\"]\n","3c677562":"df_train,df_test,y_train,y_test=train_test_split(x,y,test_size=.2,random_state=0,stratify=y) \ntrain,test=load_data(df_train,df_test) ","5de55d04":"def mi(X,y):\n    for i in X.select_dtypes([\"category\"]):\n        X[i],_=X[i].factorize()\n    discreate_features=[pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    if \"bmi\" in X.columns:\n        X[\"bmi\"].fillna(0,inplace=True)\n    scores=mutual_info_classif(X,y,discrete_features=discreate_features,random_state=0)\n    series=pd.Series(scores,index=X.columns).sort_values(ascending=False)\n    return series","96673eea":"mi(train,y_train) ","44b5eb86":"def score_data(X,y):\n    for i in X.select_dtypes([\"category\"]):\n        X[i]=X[i].cat.codes\n    scores=cross_val_score(XGBClassifier(use_label_encoder=False,eval_metric=\"error\"),X,y,cv=StratifiedKFold(n_splits=3),scoring=\"roc_auc\")\n    print( scores.mean() )","90076019":"score_data(train,y_train)","e2215234":"def drop_low_mi(df):\n    df=df.drop([\"gender\",\"Residence_type\"],axis=1)\n    return df","e570436d":"# v1 -> first version \nv1=train.copy()\nv1=drop_low_mi(v1)\nscore_data(v1,y_train)","d13de420":"px.scatter(train ,x=\"bmi\",y=\"avg_glucose_level\")","980e03f2":"px.scatter(train ,x=\"age\",y=\"avg_glucose_level\")","139931f7":"px.scatter(train,x=\"age\",y=\"bmi\")","dc01507f":"# know the best cluster \nks=[1,2,3,4,5,6,7,8]\ninertia=[]\nfor k in ks:\n    kmeans=KMeans(n_clusters=k)\n    x=train[[\"age\",\"bmi\",\"avg_glucose_level\"]] \n    kmeans.fit(x)\n    inertia.append(kmeans.inertia_)\nfig=px.line(x=ks,y=inertia,labels={\"x\":\"number of clusters\",\"y\":\"the interia\"},title=\"numbers of clusters VS interia\")\nfig.add_vline(x=3)","c6f063be":"def cluster(df):\n    kmeans=KMeans(n_clusters=3, n_init=30)\n    x=df[[\"age\",\"bmi\",\"avg_glucose_level\"]]\n    kmeans.fit(x)\n    labels=kmeans.predict(x)\n    X=pd.DataFrame()\n    X[\"cluster\"]=labels\n    return X","536bae18":"# version 2\nv2=train.copy()\nv2=drop_low_mi(v2)\nv2=v2.join(cluster(v2))\nscore_data(v2,y_train)","e611ef40":"def pca(df):\n    data=df[[\"age\",\"bmi\",\"avg_glucose_level\"]]\n    pca=PCA()\n    x=pca.fit_transform(data)\n    \n    cols=[f\"pca_{i}\" for i in range(len(data.columns))]\n    X=pd.DataFrame(x,columns=cols)\n    coefs=pd.DataFrame(pca.components_.T,index=[\"age\",\"bmi\",\"avg_glucose_level\"],columns=X.columns)\n    return X, coefs\n    ","d2c18c9c":"X_pca,com=pca(train)\ncom ","de14a1c4":"pca_information=mi(X_pca,y_train)\npca_information","f4fa8a14":"def new_feature(df):\n    X=pd.DataFrame()\n    X[\"age\/bmi\"]=df[\"age\"]\/df[\"bmi\"]\n    return X ","6cbdbedc":"v3=train.copy()\nv3=drop_low_mi(v3)\nv3=v3.join(cluster(v3))\nv3=v3.join(new_feature(v3))\nscore_data(v2,y_train)","ba8b6947":"clustering=test[[\"age\",\"bmi\",\"avg_glucose_level\"]]\nclustering","f02220b2":"# make a function to get the final data\ndef get_data(train,test):\n    X=train.copy()\n    X=drop_low_mi(X)\n    X=X.join(cluster(X))\n    X[\"bmi\"]=X[\"bmi\"].fillna(0)\n    df_test=test.copy()\n    df_test[\"bmi\"]=df_test[\"bmi\"].fillna(0)\n    df_test=drop_low_mi(df_test)\n    clustering=df_test[[\"age\",\"bmi\",\"avg_glucose_level\"]]\n    label=kmeans.predict(clustering)\n    v=pd.DataFrame()\n    v[\"cluster\"]=label\n    df_test=df_test.join(v)\n    for i in X.select_dtypes([\"category\"]):\n        X[i]=X[i].cat.codes\n    for i in df_test.select_dtypes([\"category\"]):\n        df_test[i]=df_test[i].cat.codes\n    return X,df_test","a700d04b":"X,X_test=get_data(train,test )","6943d003":"X.shape,X_test.shape","9c8cd9cc":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report","d3219f14":"def original_data(train,y_train,test,y_test):\n    model= XGBClassifier()\n    model.fit(train,y_train)\n    y_pred=model.predict(test)\n    print(classification_report(y_pred,y_test))","52311002":"original_data(X,y_train,X_test,y_test)","56c37746":"def over_sampling(train,y_train,test,y_test):\n    d_train,target=train.copy(),y_train.copy()\n    sampling= RandomOverSampler(sampling_strategy=.4,random_state=0)\n    over_x,over_y=sampling.fit_resample(d_train,target)\n    model= XGBClassifier()\n    model.fit(over_x,over_y)\n    y_pred=model.predict(test)\n    print(classification_report(y_pred,y_test))\n    ","94a0f569":"over_sampling(X,y_train,X_test,y_test)","6c8027d1":"def under_sampling(train,y_train,test,y_test):\n    d_train,target=train.copy(),y_train.copy()\n    sampling= RandomUnderSampler(sampling_strategy=.4,random_state=0)\n    over_x,over_y=sampling.fit_resample(d_train,target)\n    model= XGBClassifier()\n    model.fit(over_x,over_y)\n    y_pred=model.predict(test)\n    print(classification_report(y_pred,y_test))","d410a8e4":"under_sampling(X,y_train,X_test,y_test)","a8d8b0b1":"def over_under_sampling(train,y_train,test,y_test):\n    d_train,target=train.copy(),y_train.copy()\n    sampling= RandomOverSampler(sampling_strategy=.3,random_state=0)\n    over_x,over_y=sampling.fit_resample(d_train,target)\n    sampling= RandomUnderSampler(sampling_strategy=.4,random_state=0)\n    over_under_x,over_under_y=sampling.fit_resample(over_x,over_y)\n    model= XGBClassifier()\n    model.fit(over_under_x,over_under_y)\n    y_pred=model.predict(test)\n    print(classification_report(y_pred,y_test))","3de93ea7":"over_under_sampling(X,y_train,X_test,y_test)","2e4f1858":"- so , i will stick with the 3 clusters as the [ elbow point ]","a1495e56":"#### as it looks like , under_sampling is the best here   ......... [ for me ]","11c7dce3":"- let's try making new feature that cluster the age and the bmi","1ca06114":"#### due to the imbalance , the model performance will be so bad in predicting [ stroke=1 ].so , let's try \n- oversampling the minority class.\n- under sampling the majority class.\n- try to combine both [ under_dampling , over_sampling ]","12d89646":"- there is large impalance here","66622383":"- it seems that under_sampling did a good work rather than over_sampling\n- let's combine them","eedb0de3":"### try to get the varience between featrues with PCA.","82fb55b6":"### what i will do :-\n- apply [ over sampling and under sampling ] to the data.\n- fit the data to the model and predict the x_test\n- make the [ classification_report ] and see the effect in predicting the [ stroke=1]\n     - i guess the important calss here is [ stroke=1 ] \n           - predicting the person to have stroke,then go to the hospital to see if it is right. is     better than predicting that he\/she has no stroke to try to trea him\/here in wrong ways.","d4a26bad":"- it inceares the score, but very little.","1d400f93":"- the intial score is .8","11d55d52":"### start working with data_ for machine learning model","507fcfc8":"### create features","6c1eb3a3":"## if i did somthing wrong or you have any other ideas. please, tel me.\n#### thanks","9c1e552d":"- it seams that [gender ,Residence_type ] hav very low affect on the the target \n- let's try to remove them","72cbdccc":"## analysis the effect of categorical data of the target variable","94292cc3":"## show the variance in stroke by the variables","b607d2f8":"- the cluster was successful","ec272c95":"## it doesn't help at all.","0accc57d":"- X : represents the valid date\n- X_test : represents the unseen data [ used to make prediction after fiting the model ]"}}