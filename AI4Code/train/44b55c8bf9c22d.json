{"cell_type":{"74ad1b8a":"code","436387b9":"code","816837a8":"code","abb53dd8":"code","ce79a92e":"code","45dad442":"code","b8fe5e49":"code","27a3c817":"code","0aa530b6":"code","414c6339":"code","0ba195cd":"code","7dedf43c":"code","711955aa":"markdown","27dda332":"markdown","558569d1":"markdown","45e82273":"markdown","950409b6":"markdown","7aceb8a2":"markdown","f36f36b5":"markdown","0d34af38":"markdown","893bd5e6":"markdown","4021340f":"markdown","deb00830":"markdown","b89a6db1":"markdown"},"source":{"74ad1b8a":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm\nfrom random import choices\n\n\nSEED = 1111\ninference = False\ncv = False\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\ntrain_pickle_file = '\/kaggle\/input\/pickling\/train.csv.pandas.pickle'\ntrain = pickle.load(open(train_pickle_file, 'rb'))\n\ntrain = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['weight'] != 0]\n\ntrain.fillna(train.mean(),inplace=True)\n\ntrain['action'] = ((train['resp'].values) > 0).astype(int)\ntrain['bias'] = 1\n\n\nfeatures = [c for c in train.columns if \"feature\" in c]","436387b9":"# code to feature neutralize\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm.auto import tqdm\n\ndef build_neutralizer(train, features, proportion, return_neut=False):\n    \"\"\"\n    Builds neutralzied features, then trains a linear model to predict neutralized features from original\n    features and return the coeffs of that model.\n    \"\"\"\n    neutralizer = {}\n    neutralized_features = np.zeros((train.shape[0], len(features)))\n    target = train[['resp', 'bias']].values\n    for i, f in enumerate(features):\n        # obtain corrected feature\n        feature = train[f].values.reshape(-1, 1)\n        coeffs = np.linalg.lstsq(target, feature)[0]\n        neutralized_features[:, i] = (feature - (proportion * target.dot(coeffs))).squeeze()\n        \n    # train model to predict corrected features\n    neutralizer = np.linalg.lstsq(train[features+['bias']].values, neutralized_features)[0]\n    \n    if return_neut:\n        return neutralized_features, neutralizer\n    else:\n        return neutralizer\n\ndef neutralize_array(array, neutralizer):\n    neutralized_array = array.dot(neutralizer)\n    return neutralized_array\n\n\ndef test_neutralization():\n    dummy_train = train.loc[:100000, :]\n    proportion = 1.0\n    neutralized_features, neutralizer = build_neutralizer(dummy_train, features, proportion, True)\n    dummy_neut_train = neutralize_array(dummy_train[features+['bias']].values, neutralizer)\n    \n#     assert np.array_equal(neutralized_features, dummy_neut_train)\n    print(neutralized_features[0, :10], dummy_neut_train[0, :10])\n    \n\ntest_neutralization()","816837a8":"proportion = 1.0\n\nneutralizer = build_neutralizer(train, features, proportion)\ntrain[features] = neutralize_array(train[features+['bias']].values, neutralizer)","abb53dd8":"f_mean = np.mean(train[features[1:]].values,axis=0)\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\nX = train.loc[:, train.columns.str.contains('feature')]\n#y_train = (train.loc[:, 'action'])\n\ny = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T","ce79a92e":"def create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n\n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\nbatch_size = 5000\nhidden_units = [150, 150, 150]\ndropout_rates = [0.2, 0.2, 0.2, 0.2]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\nepochs = 200\n\nclf = create_mlp(\n    len(features), 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n    )\n\nif inference:\n    clf = keras.models.load_model('keras_nn')\nelse:\n    clf.fit(X, y, epochs=epochs, batch_size=5000)\n    clf.save('keras_nn')\n\nmodels = []\n\nmodels.append(clf)\n\nth = 0.5000\n\n\nf = np.median\nmodels = models[-3:]\n","45dad442":"if cv:\n\n    from sklearn.model_selection import GroupKFold\n    from sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, precision_recall_curve\n    import gc\n\n    # oof validation probability array\n    oof_probas = np.zeros(y.shape)\n\n    # validation indices in case of time series split\n    val_idx_all = []\n\n    # cv strategy\n    N_SPLITS = 5\n    gkf = GroupKFold(n_splits=N_SPLITS)\n\n    for fold, (train_idx, val_idx) in enumerate(gkf.split(train.action.values, groups=train.date.values)):\n\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx].values\n        y_train, y_val = y[train_idx], y[val_idx]\n\n\n        # training and evaluation score\n        clf.fit(X_train, y_train, epochs=epochs, batch_size=5000)\n\n        oof_probas[val_idx] += clf(X_val, training=False).numpy()\n\n        score = roc_auc_score(y_val, oof_probas[val_idx])  # classification score\n        print(f'FOLD {fold} ROC AUC:\\t {score}')\n\n        # deleting excess data to avoid running out of memory\n        del X_train, X_val, y_train, y_val\n        gc.collect()\n\n        # appending val_idx in case of group time series split\n        val_idx_all.append(val_idx)\n\n\n    # concatenation of all val_idx for further acessing\n    val_idx = np.concatenate(val_idx_all)\n","b8fe5e49":"if cv:\n    auc_oof = roc_auc_score(y[val_idx], oof_probas[val_idx])\n    print(auc_oof)","27a3c817":"import matplotlib.pyplot as plt\n\ndef determine_action(df, thresh):\n    \"\"\"Determines action based on defined threshold.\"\"\"\n    action = (df.weight * df.resp > thresh).astype(int)\n    return action\n\ndef date_weighted_resp(df):\n    \"\"\"Calculates the sum of weight, resp, action product.\"\"\"\n    cols = ['weight', 'resp', 'action']\n    weighted_resp = np.prod(df[cols], axis=1)\n    return weighted_resp.sum()\n\ndef calculate_t(dates_p):\n    \"\"\"Calculate t based on dates sum of weighted returns\"\"\"\n    e_1 =  dates_p.sum() \/ np.sqrt((dates_p**2).sum())\n    e_2 = np.sqrt(250\/np.abs(len(dates_p)))\n    return e_1 * e_2\n\ndef calculate_u(df, thresh):\n    \"\"\"Calculates utility score, and return t and u.\"\"\"\n    df = df.copy()\n\n    # calculates sum of dates weighted returns\n    dates_p = df.groupby('date').apply(date_weighted_resp)\n        \n    # calculate t\n    t = calculate_t(dates_p)\n    return t, min(max(t, 0), 6) * dates_p.sum()\n\n\n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--', label='Random')  # dashed diagonal\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend(loc='lower right')\n    plt.grid()\n    \n    \ndef plot_precision_recall_curve(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 6))\n    plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n    plt.plot(thresholds, recalls[:-1], 'g-', label='Recall')\n    plt.xlabel('Thresholds')\n    plt.legend(loc='lower left')\n    plt.grid()\n\n\n    \ndef plot_thresh_u_t(df, oof):\n    threshs = np.linspace(0, 1, 1000)\n    ts = []\n    us = []\n    \n    for thresh in threshs:\n        df['action'] = np.where(oof >= thresh, 1, 0)\n        t, u = calculate_u(df, thresh)\n        ts.append(t)\n        us.append(u)\n        \n    # change nans into 0\n    ts = np.array(ts)\n    us = np.array(us)\n    ts = np.where(np.isnan(ts), 0.0, ts)\n    us = np.where(np.isnan(us), 0.0, us)\n    \n    tmax = np.argmax(ts)\n    umax = np.argmax(us)\n    \n    print(f'Max Utility Score: {us[umax]}')\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n    axes[0].plot(threshs, ts)\n    axes[0].set_title('Different t scores by threshold')\n    axes[0].set_xlabel('Threshold')\n    axes[0].axvline(threshs[tmax])\n\n    axes[1].plot(threshs, us)\n    axes[1].set_title('Different u scores by threshold')\n    axes[1].set_xlabel('Threshold')\n    axes[1].axvline(threshs[umax], color='r', linestyle='--', linewidth=1.2)\n    \n    print(f'Optimal Threshold: {threshs[umax]}')\n    \n    return threshs[umax]\n","0aa530b6":"if cv:\n    fpr, tpr, thresholds = roc_curve(y[val_idx, 4], oof_probas[val_idx, 4])    \n    plot_roc_curve(fpr, tpr, 'NN')","414c6339":"if cv:\n    precisions, recalls, thresholds = precision_recall_curve(y[val_idx, 4], oof_probas[val_idx, 4])\n    plot_precision_recall_curve(precisions, recalls, thresholds)","0ba195cd":"if cv:\n    opt_thresh = plot_thresh_u_t(train.iloc[val_idx], oof_probas[val_idx, 4])\nelse:\n    opt_thresh = 0.506","7dedf43c":"import janestreet\nenv = janestreet.make_env()\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        \n        x_tt = np.append(x_tt, [[1]], axis=1)  # add bias term\n        x_tt = neutralize_array(x_tt, neutralizer)\n        \n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        pred_df.action = np.where(pred >= opt_thresh, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","711955aa":"**We can see that it almost predicts it correctly and the offset isn't that huge.**","27dda332":"## Prediction","558569d1":"## Precision\/Recall Curve","45e82273":"## Utility Score Curve","950409b6":"## Helper functions","7aceb8a2":"### I implemented feature neutralization suggested by [this kernel](https:\/\/www.kaggle.com\/code1110\/janestreet-avoid-overfit-feature-neutralization) over the awesome [keras kernel](https:\/\/www.kaggle.com\/tarlannazarov\/own-jane-street-with-keras-nn) then added a bunch of plots for cross validation. ","f36f36b5":"## ROC AUC","0d34af38":"## Feature Neutralization","893bd5e6":"## Cross Validation using GroupKFold","4021340f":"## ROC Curve","deb00830":"That's it!","b89a6db1":"## Model Training"}}