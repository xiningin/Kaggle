{"cell_type":{"0f075c94":"code","8999d3da":"code","a9ff09f7":"code","a2925abd":"code","7380ca86":"code","60ef5be5":"code","e4c73e82":"code","f0de1a25":"code","066ac993":"code","75386b62":"code","98f26d97":"code","6b62bbd5":"code","3daffbc3":"code","3df1f2df":"code","a73aca90":"code","2fc29147":"code","de0dc6bc":"code","2cd8b9ac":"code","86c65825":"code","73238db3":"code","fba94522":"markdown","fa0a7394":"markdown","26495550":"markdown","b54928c1":"markdown","0bdcfb0c":"markdown","e14cabae":"markdown","0b674746":"markdown","c63ed2bf":"markdown","029a3491":"markdown","97be0b4f":"markdown","8eae7eeb":"markdown","b1c5969e":"markdown","2017a6d3":"markdown","a8bed48c":"markdown","fcb17dcd":"markdown","d91abdc8":"markdown"},"source":{"0f075c94":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport lightgbm as lgb","8999d3da":"train_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ntrain_df.head()","a9ff09f7":"test_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')\ntest_df.head()","a2925abd":"print(train_df.isnull().sum())\nprint(test_df.isnull().sum())","7380ca86":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(train_df.corr(), annot = True)","60ef5be5":"fig, ax = plt.subplots(figsize = (12, 6))\nsns.distplot(train_df['target'])\nax.xaxis.grid(True)\nax.set(ylabel = \"Valores\")\nax.set(xlabel = \"Target\")\nax.set(title = \"Distribuicion de target\")\nsns.despine(trim = True, left = True)\nplt.show()","e4c73e82":"cols = [col for col in train_df.columns if col not in ['id','target']]\n\nfig, ax = plt.subplots(nrows = 7, ncols = 2, figsize = (15,15))\nfig.tight_layout()\n\ni = j = 0\nfor col in cols:\n    sns.kdeplot(data = train_df[col],shade=True,ax=ax[i][j],legend=False)\n    ax[i][j].set_title(col)\n    j += 1\n    if(j%2 == 0):\n        i += 1\n        j = 0\n        \nplt.show()","f0de1a25":"y = train_df['target']\nX = train_df.drop(['id', 'target'], axis = 1)\nX_test = test_df.drop(['id'], axis = 1)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 597)","066ac993":"print(X_train.shape)\nprint(X_val.shape)","75386b62":"params = {\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 5,\n    \"eta\": 0.05,\n    \"random_state\": 751\n}","98f26d97":"d_train = xgb.DMatrix(X_train, label = y_train)\nd_val = xgb.DMatrix(X_val, label = y_val)\nd_test = xgb.DMatrix(X_test)","6b62bbd5":"model_1 = xgb.train(params = params,\n                    dtrain = d_train,\n                    num_boost_round = 10000,\n                    early_stopping_rounds = 20,\n                    verbose_eval = 10,\n                    evals = [(d_train, \"train\"), (d_val, \"val\")])","3daffbc3":"predict_1 = model_1.predict(d_val, ntree_limit = model_1.best_ntree_limit)\nrmse_model_1 = np.sqrt(mean_squared_error(y_val, predict_1))\nrmse_model_1","3df1f2df":"predict_1 = model_1.predict(d_test, ntree_limit = model_1.best_ntree_limit)\npredict_1","a73aca90":"model_2 = lgb.LGBMRegressor(random_state = 100, \n                        n_estimators = 500, \n                        min_data_per_group = 5, \n                        boosting_type = 'gbdt',\n                        num_leaves = 128, \n                        learning_rate = 0.005, \n                        subsample_for_bin = 200000, \n                        importance_type ='split', \n                        metric ='rmse', \n                        min_data_in_leaf = 50)\n\nmodel_2.fit(X_train, y_train)","2fc29147":"predict_2 = model_2.predict(X_val)\npredict_2","de0dc6bc":"rmse_model_2 = np.sqrt(mean_squared_error(y_val, predict_2))\nrmse_model_2","2cd8b9ac":"test_predict = model_2.predict(X_test)\ntest_predict","86c65825":"output = pd.DataFrame({\"id\":X_test.index,'target':test_predict})\noutput","73238db3":"output.to_csv('.\/Submission.csv',index=False)","fba94522":"Las columnas que mas condicionan el valor final son: cont2, cont3, cont7 y cont11","fa0a7394":"# 6. Subir los resultados a la competici\u00f3n","26495550":"# 1. Cargamos las librer\u00edas y los datos# \n\nEl primer paso es cargar las librerias necesarias para este trabajo","b54928c1":"Con el siguiente codigo vamos a var como se ditribuyen los valores de train.\n\nprimero tomamos las columans de train_df, nuestro conjunto de entrenamiento pero sin la columna objetivo target ni la de identificacion id, despues, creamos una hoja que pueda ser usada para dibujar las 14 columnas de los dos conjuntos. Tras esto, dibujamos las graficas y las mostramos por pantalla","0bdcfb0c":"# 5. Seleccionar el mejor modelo\n\nComo podemos ver por el valor que nos devuelve el RMSE (Mean squared error regression loss), el mejopr modelo es el de lgb, con un 70% e acierto frente al 69% que nos da el xgb","e14cabae":"# 3. Separamos los datos de entrenamiento y validaci\u00f3n\n\nEsto lo hacemos eliminado la variable 'target' del conjunto de entrenamiento y d\u00e1ndosela a una variable y, el resto lo metemos en una variable X, tambi\u00e9n eliminamos la columna 'id'. Creamos una variable X_test eliminando la columna 'id' del conjunto test. Tras esto, mostramos el tama\u00f1o de los conjuntos train y val","0b674746":"# PRACTICA DE ML CON REGRESION#\n\nPara esta practica de aprendizaje automatico con regresi\u00f3n, vamos a usar el aumento de gradiente extremo, o XGB  y a compararlo con la maquina de aumento de gradiente ligero, o LGBM, por sus siglas en ingles\n\nLos pasos a seguir son:\n1. Cargar las librer'ias y los datos con pandas\n2. Hacer una exploraci\u00f3n de los datos \n3. Separar los datos de entrenamiento en entrenamiento y validaci\u00f3n\n4. Entrenar los modelos con los datos de entrenamiento y usar los datos de validaci\u00f3n para evaluar nuestro entrenamiento\n5. Seleccionar el mejor modelo\n6. Subir los resultados a la competici\u00f3n","c63ed2bf":"Vemos que la distribuici\u00f3n de datos de la variable se mueve entre 6 y 10","029a3491":"# 2. Hacemos una exploraci\u00f3n de los datos\n\nPrimero comprobamos que no haya datos faltantes","97be0b4f":"No faltan datos en las columnas","8eae7eeb":"El primer sistema que vamos a usar es el XGB, para ello, primero definimos los parametros que vamos a usar","b1c5969e":"Vemos como est\u00e1n correlacionados los datos entre s\u00ed","2017a6d3":"Cargamos los datos de entrenamiento y mostramos las cinco primeras filas","a8bed48c":"# 4. Entrenamos los modelos con los datos de entrenamiento y usamos los datos de validaci\u00f3n para evaluar nuestro entrenamiento","fcb17dcd":"Mostramos los datos de test y mostramos las cinco primeras filas","d91abdc8":"Ponemos en una gr\u00e1fica como se distribuye la variable 'target'"}}