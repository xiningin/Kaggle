{"cell_type":{"546b0760":"code","68cbd18e":"code","99acc561":"code","780b8576":"code","c287a8eb":"code","93a83b4a":"code","3a88fde5":"code","f425d4b0":"code","f43214d7":"markdown","1863124c":"markdown","a963dd9c":"markdown","25f9cea2":"markdown","99021cbe":"markdown"},"source":{"546b0760":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn.datasets\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport os\nimport fastai_structured as fs  ## For handling categorical variables\n\n%matplotlib inline\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","68cbd18e":"X = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nX.head().T","99acc561":"fs.train_cats(X)  ## Converts strings to categorical variables\nX, y, nas = fs.proc_df(X, 'Survived')\nX_train, X_val, y_train, y_val = train_test_split(X, y)\nprint(X_train.shape)\nprint(X_val.shape)","780b8576":"rf = RandomForestClassifier(n_estimators=500, max_depth=3, n_jobs=-1)\nrf.fit(X_train, y_train)","c287a8eb":"predictions = []\nfor tree in rf.estimators_:\n    predictions.append(tree.predict_proba(X_val)[None, :])","93a83b4a":"predictions = np.vstack(predictions)\ncum_mean = np.cumsum(predictions, axis=0)\/np.arange(1, predictions.shape[0] + 1)[:, None, None]","3a88fde5":"scores = []\nfor pred in cum_mean:\n    scores.append(accuracy_score(y_val, np.argmax(pred, axis=1)))","f425d4b0":"plt.figure(figsize=(10, 6))\nplt.plot(scores, linewidth=3)\nplt.xlabel('num_trees')\nplt.ylabel('accuracy');","f43214d7":"#### Get accuracy scores for each of the n_estimators value","1863124c":"#### Creating training and validation sets\nHandling categorical data using fastai v0.7 functions which are added as a utility script","a963dd9c":"#### Get predictions for each tree separately","25f9cea2":"### In this notebook, we'll outline a process to figure out the optimal no of trees required in a Random Forest,\n### without training the model again and again. I learnt this approach in the [Competitive Data Science](https:\/\/www.coursera.org\/learn\/competitive-data-science\/home\/welcome) course \n### on Coursera, and found it to be a pretty useful trick.","99021cbe":"### We see that around 150 trees are enough to get a good accuracy and \n### there is not really any payoff for adding new trees in the forest after that"}}