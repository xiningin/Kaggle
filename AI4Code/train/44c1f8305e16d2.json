{"cell_type":{"8ecc6eb7":"code","cafbb5f8":"code","757994e7":"code","7e9bd22b":"code","92935578":"code","6bf0e0c1":"code","27edc608":"code","5f6e717c":"code","bb78f2c9":"code","4c074ffd":"code","3ee032dd":"code","de4f55ce":"code","a04f9c52":"code","6543c391":"code","d341ca14":"code","20126c69":"code","2bd150bf":"code","56243a7f":"code","1e268128":"code","50b4d334":"code","ebdeacee":"code","d38e59b9":"code","fd79eff5":"code","9525df22":"code","0a6a4a54":"code","82dc9126":"code","db58b56f":"code","7f415f45":"code","a2cf0f7f":"code","c20aaf31":"code","e110c81e":"code","7da90d09":"code","3bf4a4ad":"code","08c6b84a":"code","9a3c4acf":"code","c247737f":"code","45eb670c":"code","6166fc8e":"code","1bee3338":"code","0f6d7c17":"code","e4b7ef04":"code","65cd92df":"code","67948807":"code","7e6ec867":"code","046ccdad":"code","8067e5e3":"code","60d7d6f7":"code","3b2617e2":"code","43419eea":"code","ef9ec0f8":"code","be20beae":"code","79b724fb":"code","de0361c8":"code","2da9f91d":"code","2a11b4a2":"code","e4dd82bb":"code","fc7566f7":"markdown","b240763f":"markdown","767f1b8a":"markdown","db18da3e":"markdown","9d257cf2":"markdown","3173c90e":"markdown","c6a4af76":"markdown","6f221294":"markdown","59846497":"markdown","c97c1a4a":"markdown","7faae819":"markdown","4dfd405e":"markdown","d4bde284":"markdown","c5ca8921":"markdown","361058d6":"markdown","356cd30b":"markdown","6a8499b4":"markdown","b5c30010":"markdown","0b305907":"markdown","bd5523d8":"markdown","16263cd7":"markdown","df67864b":"markdown","7d5da9cb":"markdown","4d67691d":"markdown","62607b95":"markdown","fe79d738":"markdown","83134033":"markdown","4cf5af27":"markdown","4afd574c":"markdown","71365035":"markdown","8444141b":"markdown","a3b1cab7":"markdown","9d19a71a":"markdown","6fc85ed0":"markdown","ed4f48ad":"markdown","80ab2767":"markdown"},"source":{"8ecc6eb7":"project_path = \"\/kaggle\/input\/glwikihow\/\"","cafbb5f8":"import pandas as pd\ndf = pd.read_csv(project_path+'wikihowAll.csv')","757994e7":"df.shape","7e9bd22b":"df.head()","92935578":"df.dropna(inplace=True)\ndf.shape","6bf0e0c1":"df.drop_duplicates(inplace=True)\ndf.shape","27edc608":"import re\nimport numpy as np\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords = set(stopwords.words('english'))\n\ndef preprocess_text(df, column_name=''):\n\n  # Select only alphabets\n  df[column_name] = df[column_name].apply(lambda x: re.sub('[^A-Za-z]+', ' ', x))\n\n  # Convert text to lowercase\n  df[column_name] = df[column_name].apply(lambda x: x.lower())\n\n  # Strip unwanted spaces\n  df[column_name] = df[column_name].apply(lambda x: x.strip())\n\n  # Remove stopwords\n  df[column_name] = df[column_name].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))\n\n  # Replace empty strings with Null\n  df[column_name].replace('', np.nan, inplace = True)\n\n  # Drop Null values\n  df = df.dropna()\n\n  return df","5f6e717c":"import time\nstart = time.time()\ndf = preprocess_text(df, column_name='headline')\ndf = preprocess_text(df, column_name='text')\nend= time.time()\nprint((end-start)\/60,\" minutes\")","bb78f2c9":"df.head()","4c074ffd":"df['headline'] = df['headline'].apply(lambda x : 'sostok '+ x + ' eostok')","3ee032dd":"df['headline'][0]","de4f55ce":"for i in range(5):\n  print(\"Text:\", df['text'][i])\n  print(\"\\n\")\n  print(\"Headline:\", df['headline'][i])\n  print(\"---------------------------------------------------------------------------\\n\")","a04f9c52":"df['len_headline'] = df['headline'].apply(lambda x: len(x.split(\" \")))\ndf['len_text'] = df['text'].apply(lambda x: len(x.split(\" \")))\ndf.head()","6543c391":"df.describe()","d341ca14":"import matplotlib.pyplot as plt\ntext_word_count = []\nheadline_word_count = []\n\n# populate the lists with sentence lengths\nfor i in df['text']:\n  text_word_count.append(len(i.split()))\n\nfor i in df['headline']:\n  headline_word_count.append(len(i.split()))\n\nlength_df = pd.DataFrame({'text':text_word_count, 'headline':headline_word_count})\nlength_df.hist(bins = 10)\nplt.show()","20126c69":"df[(df['len_headline']<=50) & (df['len_text']<=300)]","2bd150bf":"length_df.describe()","56243a7f":"cnt=0\nfor i in df['headline']:\n    if(len(i.split())<=50):\n        cnt=cnt+1\nprint(cnt\/len(df['headline']))","1e268128":"cnt=0\nfor i in df['text']:\n    if(len(i.split())<=300):\n        cnt=cnt+1\nprint(cnt\/len(df['text']))","50b4d334":"maxlen_headline = 50\nmaxlen_text = 100\ndf = df[(df.len_headline > maxlen_headline) & (df.len_text > maxlen_text)]","ebdeacee":"df.shape","d38e59b9":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df.text, df.headline, test_size = 0.2, random_state=42)\n\ndel df","fd79eff5":"X_train[0]","9525df22":"y_train[0]","0a6a4a54":"max_features = 10000","82dc9126":"from tensorflow.keras.preprocessing.text import Tokenizer\n\nfeature_tokenizer = Tokenizer(num_words=max_features)\nfeature_tokenizer.fit_on_texts(X_train)\nX_train = feature_tokenizer.texts_to_sequences(X_train)\nX_test = feature_tokenizer.texts_to_sequences(X_test)\n\nprint(\"Number of Samples in X_train:\", len(X_train))       \nprint(X_train[0])\n\nlabel_tokenizer = Tokenizer(num_words=max_features)\nlabel_tokenizer.fit_on_texts(y_train)\ny_train = label_tokenizer.texts_to_sequences(y_train)\ny_test = label_tokenizer.texts_to_sequences(y_test)\n\nprint(\"Number of Samples in y_train:\", len(y_train))       \nprint(y_train[0])","db58b56f":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nX_train = pad_sequences(X_train, maxlen = maxlen_text, padding='post')\nX_test = pad_sequences(X_test, maxlen = maxlen_text, padding='post')\n\ny_train = pad_sequences(y_train, maxlen = maxlen_headline, padding='post')\ny_test = pad_sequences(y_test, maxlen = maxlen_headline, padding='post')","7f415f45":"X_train.shape,y_train.shape,X_test.shape,y_test.shape","a2cf0f7f":"feature_tokenizer.word_index","c20aaf31":"label_tokenizer.word_index","e110c81e":"num_words_text = len(feature_tokenizer.word_index) + 1\nprint(num_words_text)\n\nnum_words_headline = len(label_tokenizer.word_index) + 1\nprint(num_words_headline)","7da90d09":"ind=[]\nfor i in range(len(y_train)):\n    cnt=0\n    for j in y_train[i]:\n        if j!=0:\n            cnt=cnt+1\n    if(cnt==2):\n        ind.append(i)\n\ny_train=np.delete(y_train,ind, axis=0)\nX_train=np.delete(X_train,ind, axis=0)\n\nind=[]\nfor i in range(len(y_test)):\n    cnt=0\n    for j in y_test[i]:\n        if j!=0:\n            cnt=cnt+1\n    if(cnt==2):\n        ind.append(i)\n\ny_test=np.delete(y_test,ind, axis=0)\nX_test=np.delete(X_test,ind, axis=0)","3bf4a4ad":"X_train.shape,y_train.shape","08c6b84a":"!git clone \"https:\/\/github.com\/thushv89\/attention_keras\"","9a3c4acf":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, TimeDistributed\nfrom attention_keras.src.layers.attention import AttentionLayer\n\nhidden_dim = 300\nembedding_dim = 100\n\n# Encoder\nencoder_inputs = Input(shape=(maxlen_text, ))\n\n# Embedding layer\nenc_emb =  Embedding(num_words_text, embedding_dim, trainable=True)(encoder_inputs)\n\n# Encoder lstm 1\nencoder_lstm1 = LSTM(hidden_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n\n# Encoder lstm 2\nencoder_lstm2 = LSTM(hidden_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n\n# Encoder lstm 3\nencoder_lstm3 = LSTM(hidden_dim, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\nencoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\n\n# Embedding layer\ndec_emb_layer = Embedding(num_words_headline, embedding_dim, trainable=True)\ndec_emb = dec_emb_layer(decoder_inputs)\n\ndecoder_lstm = LSTM(hidden_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\ndecoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n\n# # Attention layer\nattn_layer = AttentionLayer(name='attention_layer')\nattn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n\n# query_value_attention_seq = tf.keras.layers.Attention()(\n#     [query_seq_encoding, value_seq_encoding])\n\n# Concat attention input and decoder LSTM output\nprint(decoder_outputs.shape,attn_out.shape)\ndecoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n#decoder_dense = TimeDistributed(Dense(num_words_headline, activation='softmax'))\ndecoder_dense = Dense(num_words_headline, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_concat_input)\nprint(decoder_outputs.shape)\n# Dense layer\n# decoder_dense = TimeDistributed(Dense(num_words_headline, activation='softmax'))\n# decoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model \nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n# Print model summary\nmodel.summary()","c247737f":"from keras.utils import plot_model\nplot_model(model, to_file='\/kaggle\/working\/modelsummary.png', show_shapes=True, show_layer_names=True)","45eb670c":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=[\"accuracy\"])","6166fc8e":"from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n\nearly_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath=\"kaggle\/working\/checkpoint\",\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)","1bee3338":"history = model.fit([X_train, y_train[:,:-1]], y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:,1:],\n                    epochs=2, callbacks=[early_stopping,model_checkpoint_callback], batch_size=128,\n                    validation_data=([X_test, y_test[:,:-1]], y_test.reshape(y_test.shape[0], y_test.shape[1], 1)[:,1:]))","0f6d7c17":"model.save(\"\/kaggle\/working\/model.h5\")\nmodel.save_weights(\"\/kaggle\/working\/wt.h5\")","e4b7ef04":"model.summary()","65cd92df":"enc = model.layers[0:5]\n#enc.save","67948807":"model.summary()","7e6ec867":"from matplotlib import pyplot\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","046ccdad":"reverse_target_word_index = label_tokenizer.index_word\nreverse_source_word_index = feature_tokenizer.index_word\ntarget_word_index = label_tokenizer.word_index","8067e5e3":"np.zeros((1,1)),target_word_index['sostok']","60d7d6f7":"target_word_index","3b2617e2":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    #encoder_outputs, state_h, state_c\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    \n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index['sostok']\n    #print(\"target_seq[0, 0]\",target_seq[0, 0])\n    stop_condition = False \n    decoded_sentence = ''\n    while not stop_condition:\n        \n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n        #print(output_tokens.shape)\n        #vocab headline 46k, prob\n        # Sample a token\n        #print(output_tokens)\n        sampled_token_index = np.argmax(output_tokens[0, -1, :]) + 2\n        #print(\"sampled_token_index\",sampled_token_index,\"\\nreverse_target_word_index[sampled_token_index]\",reverse_target_word_index[sampled_token_index])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n        \n        if(sampled_token!='eostok'):\n            decoded_sentence += ' '+sampled_token\n            #print(decoded_sentence)\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == 'eostok' or len(decoded_sentence.split()) >= (maxlen_headline-1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n        \n        # Update internal states\n        e_h, e_c = h, c\n        \n\n    return decoded_sentence","43419eea":"from tensorflow.keras.models import load_model\nencoder_model = load_model(\"\/kaggle\/input\/summodels\/encoder_model1.h5\")\ndecoder_model = load_model(\"\/kaggle\/input\/summodels\/decoder_model1.h5\", \n                           custom_objects={'AttentionLayer': AttentionLayer}, compile=False)","ef9ec0f8":"encoder_model.summary()","be20beae":"decoder_model.summary()","79b724fb":"def seq2headline(input_seq):\n    newString=''\n    for i in input_seq:\n        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n            newString=newString+reverse_target_word_index[i]+' '\n    return newString\n\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n        if(i!=0):\n            newString=newString+reverse_source_word_index[i]+' '\n    return newString","de0361c8":"X_train[1].reshape(1, maxlen_text).shape","2da9f91d":"X_train[1]","2a11b4a2":"print(\"Text:\",seq2text(X_train[i]))","e4dd82bb":"for i in range(0,5):\n    print(\"Text:\",seq2text(X_train[i]))\n    print(\"\\nOriginal headline:\", seq2headline(y_train[i]))\n    print(\"\\nPredicted headline:\", decode_sequence(X_train[i].reshape(1, maxlen_text)))\n    print(\"\\n\")","fc7566f7":"### Plot the results","b240763f":"loop\nsostok\n1\nas\nsd\nsg\nrt\nyu\nyu\neostok","767f1b8a":"### Set number of words\n- Since the above 0th index doesn't have a word, add 1 to the length of the vocabulary","db18da3e":"### Apply `tensorflow.keras` Tokenizer and get indices for words\n- Initialize Tokenizer object with number of words as 10000\n- Fit different tokenizer objects on headline and text column\n- Convert the text to sequence\n","9d257cf2":"### Load the dataset\n- Extract \"wikihow-summarization.zip\"\n- Read \"wikihowAll.csv\"","3173c90e":"Function to implement inference","c6a4af76":"### Vocab mapping\n- There is no word for 0th index","6f221294":"* The return_state contructor argument, configuring a RNN layer to return a list where the first entry is the outputs and the next entries are the internal RNN states. This is used to recover the states of the encoder.\n* The return_sequences constructor argument, configuring a RNN to return its full sequence of outputs (instead of just the last output, which the defaults behavior). This is used in the decoder.","59846497":"Add Callbacks","c97c1a4a":"Print some rows from text and headline column","7faae819":"**Add START and END token at the beginning and end of the headline**","4dfd405e":"### Get length of each headline and text and add a column for that","d4bde284":"<img src=\"http:\/\/drive.google.com\/uc?export=view&id=1tpOCamr9aWz817atPnyXus8w5gJ3mIts\" width=500px>\n\nProprietary content. \u00a9 Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited.","c5ca8921":"### Define model\n- We'll use encoded decoder model\n- We'll use Attention layer from \"https:\/\/github.com\/thushv89\/attention_keras\"","361058d6":"### Pad sequences\n- Pad each example with a maximum length","356cd30b":"## Train test split","6a8499b4":"## Preprocess text\nPreprocess values of text & headline column\n\n- Remove unwanted characters\n- Convert text to lowercase\n- Remove unwanted spaces\n- Remove stopwords\n- Replace empty strings with Null\n- Drop null values from the dataframe","b5c30010":"Let's check the percentage of text below 80 words","0b305907":"# Text Summarization on WikiHow Dataset","bd5523d8":"### Features to text\n- Add functions to get text back from encoded text and headline","16263cd7":"#### Print one sample","df67864b":"### Inference\n- Add inference for encoder-decoder","7d5da9cb":"### Initialize parameter values\n- Set values for max_features, maxlen\n- max_features: Number of words to take from tokenizer(most frequent words)\n- maxlen: Maximum length of each sentence","4d67691d":"### Fit the model\n- Training will take some time","62607b95":"Use the `preprocess_text` function on text and headline column","fe79d738":"### Package Version\n- tensorflow==2.3.0\n- pandas==1.0.5\n- numpy==1.18.5\n- nltk==3.2.5\n- matplotlib==3.2.2","83134033":"### Delete rows that contain only START and END token","4cf5af27":"For reducing data, we'll take headings where length > 50 and text where length > 100","4afd574c":"### Check the distribution of data\n- This will help us in deciding maxlen","71365035":"### Load model\n- Training will take lot of time, we have already trained a model on 23 epochs, 128 batch_size","8444141b":"### Drop null values\n- To preprocess the text we need to drop null values first","a3b1cab7":"Let's check the percentage of heading below 80 words","9d19a71a":"### Compile the model","6fc85ed0":" photographer keep necessary lens cords batteries quadrant home studio paints kept brushes cleaner canvas print supplies ink etc make broader groups areas supplies make finding easier limiting search much smaller area ideas include essential supplies area things use every day inspiration reference area dedicated work area infrequent secondary supplies area tucked way mean cleaning entire studio means keeping area immediately around desk easel pottery wheel etc clean night discard trash unnecessary materials wipe dirty surfaces endeavor leave workspace way sit next day start working immediately without work tidying even rest studio bit disorganized organized workspace help get business every time want make art visual people lot artist clutter comes desire keep track supplies visually instead tucked sight using jars old glasses vases cheap clear plastic drawers keep things sight without leaving strewn haphazardly ideas beyond mentioned include canvas shoe racks back door wine racks cups slot hold pens pencils plastic restaurant squirt bottles paint pigment etc simply string wires across wall along ceiling use hold essential papers want cut ruin tacks tape cheap easy also good way handle papers ideas touch regularly need pin inspiration shelving artist best friend cheap easy way get room studio art space afraid get high either especially infrequently used supplies upper reaches room often utilized provide vital space tools materials turning one wall chalkboard gives perfect space ideas sketches planning without requiring extra equipment space even use smaller areas paint jars storage equipment allowing relabel chalk needs change lot disorganization comes keep moving location things trying optimize space reorganizing frequently usually opposite effect leading lost items uncertainty cleaning afternoon label maker solve everything instead spending mental energy looking storing things follow labels freeing mind think art month purge studio essential part project either throw file away later artists constantly making new things experimenting making mess good thing set aside time declutter may fun moment lot fun spending minutes digging junk find right paint old sketch sentimental used last six months little chance use next six months toss","ed4f48ad":"### Dictionary to Convert Index to word","80ab2767":"### Dataset\nWikiHow is a new large-scale dataset using the online WikiHow (http:\/\/www.wikihow.com\/) knowledge base.\n\nThere are two features: - text: wikihow answers texts. - headline: bold lines as summary."}}