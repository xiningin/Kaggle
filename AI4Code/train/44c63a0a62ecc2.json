{"cell_type":{"adc0d046":"code","e0a96778":"code","dadb9c9f":"code","e0572a18":"code","6e4ac8ca":"code","9df1b7f0":"code","eb35cf05":"code","2ee5730f":"code","09e60822":"code","68d2f015":"code","c2c60fff":"code","df9c5eb1":"code","7999845c":"code","f6feff2f":"code","0da2834c":"code","2ff5c795":"code","93127b1c":"code","f3bb4f90":"code","6d5de333":"code","00811b93":"markdown","ad9cee44":"markdown","f4e43cc8":"markdown","10ee4a4a":"markdown","65c134da":"markdown","18b4eb7a":"markdown","0391493e":"markdown","d12613dc":"markdown","86fc4d1c":"markdown","9568b9bb":"markdown","550caba1":"markdown","0104e4bb":"markdown","7155c73d":"markdown","561b553a":"markdown"},"source":{"adc0d046":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0a96778":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as st\n\nfrom sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import accuracy_score, make_scorer, roc_auc_score\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nsns.set(font_scale=2)\nrandom_state=42","dadb9c9f":"df = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\nprint(df.shape)\ndf.head()","e0572a18":"df_test = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\ndf_test.head()","6e4ac8ca":"df_test.isnull().sum()","9df1b7f0":"df.describe()","eb35cf05":"fig, axes = plt.subplots(4, 4)\nfig.set_size_inches(30, 14)\n\nk = 1\nfor i in range(4):\n    for j in range(4):\n        if k < 14:\n            sns.histplot(df, x=df.columns[k], hue='song_popularity', kde=True, multiple=\"stack\", ax=axes[i, j])\n\n        k += 1\nplt.tight_layout()\nplt.show()","2ee5730f":"fig1, ax1 = plt.subplots(figsize=(25, 15))\nsns.heatmap(df[df.columns[1:]].corr(), annot=True, cmap='coolwarm', ax=ax1)\nplt.tight_layout()\nplt.show()","09e60822":"sns.jointplot(data=df, x='acousticness', y='energy', hue='song_popularity', height=10)\nplt.tight_layout()\nplt.show()","68d2f015":"sns.jointplot(data=df, x='loudness', y='energy', hue='song_popularity', height=10)\nplt.tight_layout()\nplt.show()","c2c60fff":"df.isnull().sum()","df9c5eb1":"X, y = df[df.columns[1:-1]], df.iloc[:, -1]\nX.shape","7999845c":"# simple_imputer = SimpleImputer(strategy='median')\niter_imputer = IterativeImputer(max_iter=10, random_state=random_state)","f6feff2f":"# tree_clf = DecisionTreeClassifier()\n\n# param_grid = [\n#     {\n#         'min_samples_split' : [10, 20, 30, 40, 50],\n#         'max_depth' : [10, 15, 20, 25, 30],\n#         'min_samples_leaf' : [30, 40, 50, 60],\n#         'criterion' : ['gini', 'entropy'],\n#         'max_features': [0.8, 0.85, 0.9, 0.95, 1.0]\n#     }\n# ]\n\n\n# tree_clf = DecisionTreeClassifier(random_state=random_state)\n\n# grid_search = GridSearchCV(tree_clf, \n#                            param_grid=param_grid, \n#                            cv=3,\n#                            scoring='roc_auc',\n#                            verbose=1,\n#                       )\n# X_train_impute = iter_imputer.fit_transform(X)\n# grid_search.fit(X_train_impute, y)","0da2834c":"# grid_search.best_params_","2ff5c795":"best_params = {'max_features': 0.8560350339280598, 'n_estimators': 100}\ntree_best_params = {\n 'criterion': 'entropy',\n 'max_depth': 10,\n 'max_features':0.8,\n 'min_samples_leaf': 40,\n 'min_samples_split': 10\n}\nclf = BaggingClassifier(DecisionTreeClassifier(**tree_best_params, random_state=random_state), **best_params, random_state=random_state)\nkfold = KFold(n_splits=5, random_state=random_state, shuffle=True)","93127b1c":"# %time\nk = 1\ntest_pred_prob = []\nfor train_index, valid_index in kfold.split(X):\n    X_train, X_valid = X.iloc[train_index, :], X.iloc[valid_index,:]\n    y_train, y_valid = y[train_index], y[valid_index]\n    \n    X_train = iter_imputer.fit_transform(X_train) # fit transform impute train data\n    X_valid = iter_imputer.transform(X_valid)\n    \n    X_test = df_test[df_test.columns[1:]]\n    X_test = iter_imputer.transform(X_test)\n    # print(X_train.shape, X_valid.shape, X_test.shape)\n    \n    # X_train = minmax_scaler.fit_transform(X_train) # fit transform train data\n    # X_valid = minmax_scaler.transform(X_valid) # transform valid data\n    # X_test = minmax_scaler.transform(X_test) # transform test data\n    # print(X_train.shape, X_valid.shape, X_test.shape)\n    clf.fit(X_train, y_train)\n    y_valid_prob = clf.predict_proba(X_valid)\n    # print(y_valid_prob[:, 1], y_valid.values)\n    print(f\"KFOLD - {k} (VALID ROC AUC SCORE): {roc_auc_score(y_valid.values, y_valid_prob[:, 1])}\")\n    test_pred_prob.append(clf.predict_proba(X_test))\n    k += 1","f3bb4f90":"final_pred = np.array(test_pred_prob).mean(axis=0)[:, 1] # take mean of all the predicted values from above loop\ndf_pred = pd.DataFrame({\n    'id': df_test['id'],\n    'song_popularity': final_pred\n})\ndf_pred.head()","6d5de333":"df_pred.to_csv('submission.csv', index=False)","00811b93":"# Impute Data using Iterative Imputer","ad9cee44":"# Missing Values info on each column","f4e43cc8":"* Almost both target labels follow same distribution for each feature.\n* Only difference I can observe is the freqency in data points for each class.","10ee4a4a":"* Uncomment to find the best parameters","65c134da":"# EDA","18b4eb7a":"## Joint Plot between `acusticness` and `energy`","0391493e":"* No feature as direct impact on target label.\n* `acusticness` and `energy` are -0.57 negatively correlated.\n* `energy` and `loudness` are 0.64 positively correlated.","d12613dc":"## Denisty Plots","86fc4d1c":"# Final Submission","9568b9bb":"# Parameters search using GridSearch for Decision Tree","550caba1":"## Joint Plot between `energy` and `loudness`","0104e4bb":"# KFold Training and Testing Loop ","7155c73d":"## Heatmap (Correlation Between Features)","561b553a":"# BaggingClassifier with DecisionTree with best parameters searched from above"}}