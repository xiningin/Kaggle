{"cell_type":{"cd22bede":"code","aa5dd1b6":"code","c0bb7e33":"code","d32446de":"code","c0bce04f":"code","684c58c8":"code","b95c9c39":"code","5dbf15cf":"code","37ad852f":"code","36b58aa7":"code","32b7b97b":"code","a647171e":"code","173d905d":"code","ff4e8420":"code","c8a495a6":"code","b167f3d7":"code","dc553889":"code","ea7cff20":"code","a7dda04a":"code","2612e6cf":"code","e58293bb":"code","e16663cc":"code","4890c429":"code","68874d52":"code","9fd93f00":"markdown","b842211d":"markdown","5a2d3687":"markdown","80ef7eaf":"markdown","592626b9":"markdown","64d62ffe":"markdown","ab121fcb":"markdown","e917ddb8":"markdown","35ef38ce":"markdown","e749e0b7":"markdown","32c5de73":"markdown","36ce376c":"markdown","1be03330":"markdown","163a7200":"markdown","898b252a":"markdown","6829ac0a":"markdown","29d9bd69":"markdown","d0231704":"markdown","84ce541f":"markdown","40faaf26":"markdown","c1512b34":"markdown","feb4abb2":"markdown","0bfc8617":"markdown","8fb75145":"markdown","9f3e19a2":"markdown"},"source":{"cd22bede":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # making graphs, plots etc","aa5dd1b6":"# get all of the data from the CSV files\n\nlists = {}\n\nfor year in range(1993, 2021+1, 1):\n    try:\n        June = pd.read_csv(f\"..\/input\/top500\/top500\/June {year}.csv\")\n        lists[f\"June {year}\"] = June\n\n        Nov = pd.read_csv(f\"..\/input\/top500\/top500\/November {year}.csv\")\n        lists[f\"November {year}\"] = Nov\n    except:\n        try:\n            Dec = pd.read_csv(f\"..\/input\/top500\/top500\/December {year}.csv\")\n            lists[f\"December {year}\"] = Dec\n        except:\n            pass","c0bb7e33":"dates=[date for date in lists] # get all of the dates","d32446de":"from matplotlib.pyplot import figure\nfigure(figsize=(12, 9), dpi=200)\n\nimport seaborn as sns\nplt.style.use(\"seaborn\")\ncolors = sns.color_palette(\"vlag\", n_colors=5) \n# the use of a gradient colour scheme shows that they are related","c0bce04f":"cores_mean = []\ncores_upper_quartile = []\ncores_lower_quartile = []\ncores_max = []\ncores_min = []\n\nfor i in lists:\n    cores = []\n    for j in lists[i][\"Cores:\"]: # get cores\n        try:\n            cores.append(float(j.replace(',', ''))) # clean it\n        except:\n            pass\n    cores_mean.append(np.percentile(cores, 50))\n    cores_upper_quartile.append(np.percentile(cores, 75))\n    cores_lower_quartile.append(np.percentile(cores, 25))\n    cores_max.append(np.percentile(cores, 100))\n    cores_min.append(np.percentile(cores, 0))","684c58c8":"labels = [\"min\", \"lower quartile\", \"mean\", \"upper quartile\", \"max\"]\n\nplt.stackplot(dates, cores_min, cores_lower_quartile, cores_mean, cores_upper_quartile, cores_max, labels=labels, colors=colors)\n\nplt.legend(loc = \"lower center\", bbox_to_anchor=(1.1, 0.8), ncol=1)\nplt.title('Cores per machine')\nplt.ylabel('cores, log scale')\nplt.xticks(rotation=90, fontsize=7)\n\nplt.yscale(\"log\") # log scale because it increases so fast\n\nplt.show()","b95c9c39":"memory_mean = []\nmemory_upper_quartile = []\nmemory_lower_quartile = []\nmemory_max = []\nmemory_min = []\n\nfor i in lists:\n    memory = []\n    for j in lists[i][\"Memory:\"]: # get the memory feild on the CSVs\n        try:\n            if float(j.replace(',', '').split(\" \")[0]) != 0: # non recorded values at 0 throws of mean, IQR and range\n                memory.append(float(j.replace(',', '').split(\" \")[0])) # clean up the data\n            else:\n                pass\n        except:\n            pass\n    if memory != []:\n        memory_mean.append(np.percentile(memory, 50))\n        memory_upper_quartile.append(np.percentile(memory, 75))\n        memory_lower_quartile.append(np.percentile(memory, 25))\n        memory_max.append(np.percentile(memory, 100))\n        memory_min.append(np.percentile(memory, 0))\n    else: # if it is a null value it should be 0\n        memory_mean.append(0)\n        memory_upper_quartile.append(0)\n        memory_lower_quartile.append(0)\n        memory_max.append(0)\n        memory_min.append(0)","5dbf15cf":"labels = [\"min\", \"lower quartile\", \"mean\", \"upper quartile\", \"max\"]\n\nplt.stackplot(dates, memory_min, memory_lower_quartile, memory_mean, memory_upper_quartile, memory_max, labels=labels, colors=colors)\n\nplt.legend(loc = \"lower center\", bbox_to_anchor=(1.1, 0.8), ncol=1)\nplt.title('memory per machine')\nplt.ylabel('memory, log scale, GB')\nplt.xticks(rotation=90, fontsize=7)\n\nplt.yscale(\"log\") # log scale because it increases so fast\n\nplt.show()","37ad852f":"Rmax_mean = []\nRmax_upper_quartile = []\nRmax_lower_quartile = []\nRmax_max = []\nRmax_min = []\n\nfor i in lists:\n    Rmax = []\n    for j in lists[i][\"Linpack Performance (Rmax)\"]:# get the rmax feild on the CSVs\n        try:\n            Rmax.append(float(j.replace(',', '').split(\" \")[0])) # clean up the data\n        except:\n            pass\n    Rmax_mean.append(np.percentile(Rmax, 50))\n    Rmax_upper_quartile.append(np.percentile(Rmax, 75))\n    Rmax_lower_quartile.append(np.percentile(Rmax, 25))\n    Rmax_max.append(np.percentile(Rmax, 100))\n    Rmax_min.append(np.percentile(Rmax, 0))","36b58aa7":"labels = [\"min\", \"lower quartile\", \"mean\", \"upper quartile\", \"max\"]\n\nplt.stackplot(dates, Rmax_min, Rmax_lower_quartile, Rmax_mean, Rmax_upper_quartile, Rmax_max, labels=labels, colors=colors)\n\nplt.legend(loc = \"lower center\", bbox_to_anchor=(1.1, 0.8), ncol=1)\nplt.title('Rmax')\nplt.ylabel('Rmax, log scale, TFlop per second')\nplt.xticks(rotation=90, fontsize=7)\n\nplt.yscale(\"log\") # log scale because it increases so fast\n\nplt.show()","32b7b97b":"RPeak_mean = []\nRPeak_upper_quartile = []\nRPeak_lower_quartile = []\nRPeak_max = []\nRPeak_min = []\n\nfor i in lists:\n    RPeak = []\n    for j in lists[i][\"Theoretical Peak (Rpeak)\"]:# get the rpeak feild on the CSVs\n        try:\n            RPeak.append(float(j.replace(',', '').split(\" \")[0])) # clean up the data\n        except:\n            pass\n    RPeak_mean.append(np.percentile(RPeak, 50))\n    RPeak_upper_quartile.append(np.percentile(RPeak, 75))\n    RPeak_lower_quartile.append(np.percentile(RPeak, 25))\n    RPeak_max.append(np.percentile(RPeak, 100))\n    RPeak_min.append(np.percentile(RPeak, 0))","a647171e":"labels = [\"min\", \"lower quartile\", \"mean\", \"upper quartile\", \"max\"]\n\nplt.stackplot(dates, RPeak_min, RPeak_lower_quartile, RPeak_mean, RPeak_upper_quartile, RPeak_max, labels=labels, colors=colors)\n\nplt.legend(loc = \"lower center\", bbox_to_anchor=(1.1, 0.8), ncol=1)\nplt.title('RPeak')\nplt.ylabel('RPeak, log scale, TFlop per second')\nplt.xticks(rotation=90, fontsize=7)\n\nplt.yscale(\"log\") # log scale because it increases so fast\n\nplt.show()","173d905d":"power_mean = []\npower_upper_quartile = []\npower_lower_quartile = []\npower_max = []\npower_min = []\n\nfor i in lists:\n    power = []\n    for j in lists[i][\"Power:\"]:# get the power feild on the CSVs\n        try:\n            if float(j.replace(',', '').split(\" \")[0]) != 0:# non recorded values at 0 throws of mean, IQR and range\n                power.append(float(j.replace(',', '').split(\" \")[0])) # clean up the data\n            else:\n                pass\n        except:\n            pass\n    if power != []:\n        power_mean.append(np.percentile(power, 50))\n        power_upper_quartile.append(np.percentile(power, 75))\n        power_lower_quartile.append(np.percentile(power, 25))\n        power_max.append(np.percentile(power, 100))\n        power_min.append(np.percentile(power, 0))\n    else:\n        power_mean.append(0)\n        power_upper_quartile.append(0)\n        power_lower_quartile.append(0)\n        power_max.append(0)\n        power_min.append(0)","ff4e8420":"labels = [\"min\", \"lower quartile\", \"mean\", \"upper quartile\", \"max\"]\n\nplt.stackplot(dates, power_min, power_lower_quartile, power_mean, power_upper_quartile, power_max, labels=labels, colors=colors)\n\nplt.legend(loc = \"lower center\", bbox_to_anchor=(1.1, 0.8), ncol=1)\nplt.title('power')\nplt.ylabel('power, log scale, KW')\nplt.xticks(rotation=90, fontsize=7)\n\nplt.yscale(\"log\") # log scale because it increases so fast\n\nplt.show()","c8a495a6":"colors = sns.color_palette(\"Paired\", n_colors=12) \n# this is a good colour scheme as it has contrasting colours","b167f3d7":"Manufacturer = []\n\nfor i in lists: # all the top manafacturers\n    for j in lists[i][\"Manufacturer:\"]:\n        try:\n            for k in j.split(\"\"\"\/\"\"\"): # it may be made by 2 companies at once\n                Manufacturer.append(k)\n        except:\n            Manufacturer.append(j)\n\nManufacturer_unique = pd.unique(Manufacturer) # get only the unique system integrators\n\nmanafacturer_counts_total = {}\n\nfor i in Manufacturer_unique:\n    manafacturer_counts_total[i] = Manufacturer.count(i) # get he number that each integrator has made\n\nmanafacturer_counts_total = {k: v for k, \n                             v in sorted(manafacturer_counts_total.items(), \n                             key=lambda item: item[1], reverse=True)} # sort the dict\n\nmanafacturer_top = []\nlength = 11 # amount of named system integrators\n\nfor i in manafacturer_counts_total:\n    if len(manafacturer_top) <= length-1:\n        manafacturer_top.append(i)\n    \nfor i in manafacturer_top:\n    print(f\"{i} has made {manafacturer_counts_total[i]} systems\")\n    \nmanafacturer_top.append(\"Other\")","dc553889":"manafacturer_year = [[0 for i in range(len(dates))] for i in range(length+1)]\nprint(np.shape(manafacturer_year))\ntick = 0\n\nfor i in lists: # all the top manafacturers\n    for j in lists[i][\"Manufacturer:\"]:\n        try: # add a manafacturer to its section\n            for k in j.split(\"\"\"\/\"\"\"):\n                manafacturer_year[manafacturer_top.index(k)][tick]+=1\n        except:\n            manafacturer_year[-1][tick] +=1\n    tick += 1\n    \nfor i in range(len(manafacturer_year[0])): # normalise\n    total = 0\n    for j in range(len(manafacturer_year)):\n        total += manafacturer_year[j][i]\n    for j in range(len(manafacturer_year)):\n        manafacturer_year[j][i] \/= total\n        manafacturer_year[j][i] *=100","ea7cff20":"plt.stackplot(dates, manafacturer_year, labels=manafacturer_top, colors=colors)\n\nplt.legend(loc = \"lower center\", bbox_to_anchor=(1.1, 0.8), ncol=1)\nplt.title('Market share')\nplt.ylabel('Manafacturer, market share %')\nplt.xticks(rotation=90, fontsize=7)\n\nplt.show()","a7dda04a":"Interconnect = []\n\nfor i in lists: # all the top Interconnects\n    for j in lists[i][\"Interconnect:\"]:\n        Interconnect.append(str(j))\n\nInterconnect_unique = pd.unique(Interconnect) # get only the unique Interconnect\n\nInterconnect_counts_total = {}\n\nfor i in Interconnect_unique:\n    Interconnect_counts_total[i] = Interconnect.count(i) # get he number of each Interconnect\n\nInterconnect_counts_total = {k: v for k, \n                             v in sorted(Interconnect_counts_total.items(), \n                             key=lambda item: item[1], reverse=True)} # sort the dict\n\nInterconnect_top = []\nlength = 11 # amount of named system Interconnects\n\nfor i in Interconnect_counts_total:\n    if len(Interconnect_top) <= length-1:\n        Interconnect_top.append(i)\n    \nfor i in Interconnect_top:\n    print(f\"{i} has been in {Interconnect_counts_total[i]} systems\")\n    \nInterconnect_top.append(\"Other\")","2612e6cf":"Interconnect_year = [[0 for i in range(len(dates))] for i in range(length+1)]\nprint(np.shape(Interconnect_year))\ntick = 0\n\nfor i in lists: # all the top Interconnects\n    for j in lists[i][\"Interconnect:\"]:\n        try: # add a Interconnect to its section\n            for k in j.split(\"\"\"\/\"\"\"):\n                Interconnect_year[Interconnect_top.index(k)][tick]+=1\n        except:\n            Interconnect_year[-1][tick] +=1\n    tick += 1\n    \nfor i in range(len(Interconnect_year[0])): # normalise\n    total = 0\n    for j in range(len(Interconnect_year)):\n        total += Interconnect_year[j][i]\n    for j in range(len(Interconnect_year)):\n        Interconnect_year[j][i] \/= total\n        Interconnect_year[j][i] *=100","e58293bb":"plt.stackplot(dates, Interconnect_year, labels=Interconnect_top, colors=colors)\n\nplt.legend(loc = \"lower center\", bbox_to_anchor=(1.1, 0.8), ncol=1)\nplt.title('Market share')\nplt.ylabel('Interconnect, market share %')\nplt.xticks(rotation=90, fontsize=7)\n\nplt.show()","e16663cc":"Operating_System = []\n\nfor i in lists: # all the top Operating Systems\n    for j in lists[i][\"Operating System:\"]:\n        Operating_System.append(str(j))\n\nOperating_System_unique = pd.unique(Operating_System) # get only the unique Operating System\n\nOperating_System_counts_total = {}\n\nfor i in Operating_System_unique:\n    Operating_System_counts_total[i] = Operating_System.count(i) # get he number of each Operating System\n\nOperating_System_counts_total = {k: v for k, \n                             v in sorted(Operating_System_counts_total.items(), \n                             key=lambda item: item[1], reverse=True)} # sort the dict\n\nOperating_System_top = []\nlength = 11 # amount of named system Operating Systems\n\nfor i in Operating_System_counts_total:\n    if len(Operating_System_top) <= length-1:\n        Operating_System_top.append(i)\n    \nfor i in Operating_System_top:\n    print(f\"{i} has hosted {Operating_System_counts_total[i]} systems\")\n    \nOperating_System_top.append(\"Other\")","4890c429":"Operating_System_year = [[0 for i in range(len(dates))] for i in range(length+1)]\nprint(np.shape(Operating_System_year))\ntick = 0\n\nfor i in lists: # all the top Operating System\n    for j in lists[i][\"Operating System:\"]:\n        try: # add a Operating System to its section\n            for k in j.split(\"\"\"\/\"\"\"):\n                Operating_System_year[Operating_System_top.index(k)][tick]+=1\n        except:\n            Operating_System_year[-1][tick] +=1\n    tick += 1\n    \nfor i in range(len(Operating_System_year[0])): # normalise\n    total = 0\n    for j in range(len(Operating_System_year)):\n        total += Operating_System_year[j][i]\n    for j in range(len(Operating_System_year)):\n        Operating_System_year[j][i] \/= total\n        Operating_System_year[j][i] *=100","68874d52":"plt.stackplot(dates, Operating_System_year, labels=Operating_System_top, colors=colors)\n\nplt.legend(loc = \"lower center\", bbox_to_anchor=(1.1, 0.8), ncol=1)\nplt.title('Market share')\nplt.ylabel('Operating System, market share %')\nplt.xticks(rotation=90, fontsize=7)\n\nplt.show()","9fd93f00":"# Rmax (Linpack)","b842211d":"This is the peak performance and it has grown **very** fast. It is easy to think that this is a linear increase however it is logarithmic. This means it grows at a faster than linear rate (as moores law predicts). However we can also see that the top computer has not changed in a long time. This is because of the huge cost (Supercomputer Fugaku was $1bn usd) and some compters are not reported on","5a2d3687":"# OS","80ef7eaf":"# cores","592626b9":"# Memory","64d62ffe":"# Imports","ab121fcb":"computers are better than they used to be and are improving at an accelerated rate. This makes sense as our lives are increasingly digitised and the servers and supercomputers of the world have to get faster and faster to serve us with content, manage networking and (sadly) analyse our data.","e917ddb8":"In this graph you can see a huge change over time. At the start of the list Cray, HPE and \"others\" ones dominated the market. HPE has stayed relatively constant and \"others\" has still got a sizable market share. Cray has almost faded into obscurity after it's buy-out by HP. Next HP (another name for HPE - I dont know the difference) and Sun rose but quickly shrunk back down to 0. IBM also became a market leader along with HPE however IBM (which I did not intend to be blue - it is a coincidence) shrunk quickly, with Sugon, Lenovo and Inspur taking over.","35ef38ce":"In this notebook I will do some simple visualisation of some of the features over time, however I will not do anything too complex. I will, in fact, only use stackplots.","e749e0b7":"# Numercal data","32c5de73":"# Power","36ce376c":"# Discrete data","1be03330":"# Manafacturer","163a7200":"# Rpeak (Theoretical)","898b252a":"# Styling","6829ac0a":"Memory has only been recorded in this list since 2009, however we can see a rise in this. The reason why memory took a drop in 2010-2012\/3 is because only a few supercomputers recorded this and those were the top (this is an example of bias in the dataset). However the list has also shown that memory hasn't increased very fast meaning it is not as important as other things (with exceptions such as Supercomputer Fugaku) with millions of gigabytes.","29d9bd69":"# conclusion","d0231704":"The OS graph shows that unix-based operating systems have always domnated. Linux is now undisputed, as CentOS, Cray linux and the entire current \"others\" category are just variations. Once again Tux proves his strength.","84ce541f":"Interconnect is quite wierd because it has alot in the \"other\" group. One can draw several potential conclusions from this (I am not sure which is correct, and any or all of them are just my thoughts without much research): This does not matter, there are few open standards and everyone needs their own, innovation is fast and it is replaced frequently, different architectures benefit from different interconnects etc.\n\nHowever from what I can see Ethernet tends to be a very popular choice and obvious geometric inspired ones (like hypercube and 3-D torus) have faded into obscurity.","40faaf26":"# Collect data","c1512b34":"# Interconnect","feb4abb2":"# styling","0bfc8617":"Peak theoretical performance is a similar story to peak performance","8fb75145":"cores of a cpu grow at a huge rate. In just 30 years the averge has gone from under 10 cores to over 10,000. Nowdays there are a few with over 10 million cores. As well as IPC and clock speed increases we can see how fast progress has been. There have been periods of stagnation in this metric however it has been fairly sustained growth.","9f3e19a2":"Power usage was extremely high in the late '90s and early '00s. I cannot explain this however one interesting thing is while there has been a slight climb it has been very slow. However the divide between top and bottom is also really high"}}