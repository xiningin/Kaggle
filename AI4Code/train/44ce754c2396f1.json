{"cell_type":{"cbf8c15f":"code","ae571783":"code","312751ea":"code","b6d412d8":"code","680a1f3e":"code","4233ae87":"code","cb26f88b":"code","f197fcd3":"code","528f4f97":"code","af6acea1":"code","c57af09a":"code","7fefe41d":"code","86572cd8":"code","31dbb384":"code","23b03269":"code","b5bda096":"code","827290e3":"code","d4161e4e":"code","964fe2ad":"code","f625c8a8":"code","73d47485":"code","4be206b8":"code","0719db95":"code","a3e44ef8":"code","de3b3b18":"code","518cf2de":"code","19c02f03":"code","84e37f09":"code","7ca90dd4":"code","b23ce337":"code","8bf351cd":"code","ed3efca8":"code","e3046951":"code","dca43ed8":"code","e3376780":"code","711a0dcc":"code","a33a9773":"code","fb25a668":"code","d7317635":"code","80a35a86":"code","598afbb8":"code","cde0ce82":"code","70f7d97f":"code","5339872f":"code","15d1ee59":"code","730abc5f":"code","4d9ed616":"code","baaeb0ba":"code","d310b1ef":"code","fd6c95f2":"code","e060731e":"code","34a9728f":"markdown","686c1bd9":"markdown","d0b6f850":"markdown","03a340d0":"markdown","993ee41e":"markdown","0fd1347a":"markdown","0d5cfc38":"markdown","a2364c9a":"markdown","f9b344be":"markdown","2c5e175c":"markdown","a6a23584":"markdown","c5351216":"markdown","09ae5a43":"markdown","ca7015ce":"markdown","b1b4ef7e":"markdown","a0924aed":"markdown","42b148cd":"markdown","a428592a":"markdown","fd1e83ab":"markdown"},"source":{"cbf8c15f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nimport plotly.offline as pyo\nimport plotly.graph_objs as go","ae571783":"train_df = pd.read_csv('..\/input\/train.csv', index_col=0)\ntest_df = pd.read_csv('..\/input\/test.csv', index_col=0)","312751ea":"target_df = train_df['target']\ntarget_df_log = np.log(target_df)\ntrain_df.drop('target', axis = 1, inplace = True)","b6d412d8":"variable_dict = {'Train Set': train_df, 'Test Set': test_df, 'Target': target_df}\nfor i in ['Train Set', 'Test Set', 'Target']:\n    print('The Dimension (row x col) of %s is: ' %i, variable_dict[i].shape)","680a1f3e":"target_df.describe()","4233ae87":"# Sort most common values\nfrom collections import Counter\ncnt = Counter(target_df)\ncnt.most_common(20)","cb26f88b":"pyo.init_notebook_mode()\ndata = [go.Histogram(x = target_df)]\npyo.iplot(figure_or_data= data, filename='Histogram_Target')","f197fcd3":"print('Sum of NAs in Train = ', train_df.isnull().sum(axis = 0).unique())\nprint('Sum of NAs in Test = ',  test_df.isnull().sum(axis = 0).unique())","528f4f97":"## Train set contains a portion of int64, most of which are categorical values, i.e.[0, some_other_value].\ntrain_df.dtypes.value_counts()","af6acea1":"## Test set only contains float64 dtype\ntest_df.dtypes.value_counts()","c57af09a":"# So we convert all Train variables to float64\ntrain_df = train_df.astype(dtype = 'float64',copy = True)\ntrain_df.dtypes.value_counts()","7fefe41d":"def categorical_filter(df, low_exclusive = 2, high_inclusive = 15):\n    \"\"\"function returns features (col_names) that have unique values\n    less than or equal to n_categories\n    \n    \"\"\" \n    list_of_features = []\n    for i in df.columns:\n        if low_exclusive == high_inclusive:\n            if df[i].nunique() <= low_exclusive :\n                list_of_features.append(i)\n        else:\n            if df[i].nunique() <= high_inclusive and df[i].nunique() > low_exclusive :\n                list_of_features.append(i)\n    return list_of_features","86572cd8":"category_1_cols = categorical_filter(train_df, 1, 1 )\nprint('# of Constant Variables = ',len(category_1_cols))","31dbb384":"category_2_cols = categorical_filter(train_df, 1, 2)\nprint('# of Binary Varialbes = ',len(category_2_cols))","23b03269":"category_15_cols = categorical_filter(train_df, 2, 15)\nprint('# of Variables less than or equal to 15 categories = ',len(category_15_cols))","b5bda096":"remainder_cols = categorical_filter(train_df, 15, len(train_df))\nprint('# of Continuous Variables (with more than 15 categories) = ',len(remainder_cols))","827290e3":"# check if all constants are equal to 0\n((train_df[category_1_cols] == 0.0).all()).all()","d4161e4e":"# See how these features cause noise in the Test Set\ntest_df[category_1_cols[np.random.randint(0,len(category_1_cols))]].value_counts().head(10)","964fe2ad":"n_rows = len(train_df)\n\ntrain_df_binary = train_df[category_2_cols]","f625c8a8":"count_nonzero = pd.DataFrame(data = np.zeros((2,len(category_2_cols))), index=['zero', 'nonzero'], columns=category_2_cols)\nfor i in train_df_binary.columns:\n    n_zero = train_df_binary[i].value_counts()[0]\n    n_nonzero = n_rows - n_zero\n    count_nonzero[i].iloc[0] = n_zero\n    count_nonzero[i].iloc[1] = n_nonzero\ncount_nonzero","73d47485":"from sklearn.feature_selection import f_regression\n\nf, p_val = f_regression(train_df_binary,target_df_log)","4be206b8":"f_reg_df = pd.DataFrame(np.array([f, p_val]).T, index = train_df_binary.columns, columns = ['f-statistic', 'p-value'])\nbinary_stored_features = f_reg_df[f_reg_df['p-value'] < 0.05].sort_values(by = 'f-statistic', ascending = False)\nbinary_stored_features","0719db95":"selected_features_binary = np.array(binary_stored_features.index)\n\nprint('Features selected among binary variables: \\n', selected_features_binary)","a3e44ef8":"from scipy.stats import f_oneway\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.ensemble import RandomForestRegressor","de3b3b18":"train_df_categorical = train_df[category_15_cols]\n\n# Label encode categories\nle=LabelEncoder()\n\n# create a dataframe to store label encoded values\ntrain_df_categorical_le = train_df_categorical.copy()\n\nfor i in train_df_categorical.columns:\n    le.fit(train_df_categorical[i])\n    train_df_categorical_le[i] = le.transform(train_df_categorical[i]).copy()","518cf2de":"def one_way_anova(categorical_data, target_data):\n    # create an empty dataframe to store f-statistic and p-value\n    stats_df = pd.DataFrame(np.zeros((len(categorical_data.columns), 2)), index = categorical_data.columns, columns = ['f-statistic', 'p-value'])\n    \n    # merge independent dataframe with target \n    merged_df = categorical_data.merge(pd.DataFrame(target_data, columns = ['target']), left_index=True, right_index=True)\n    for i in categorical_data.columns:\n        unique_values = categorical_data[i].unique()\n        tuple_list = []\n        for value in unique_values:\n            store_values = merged_df['target'].loc[merged_df[i]==value].values\n            tuple_list.append(store_values)\n         \n        # get stats from f_oneway test\n        statistic, pvalue = f_oneway(*tuple_list)\n        stats_df.loc[i, 'f-statistic'] = statistic\n        stats_df.loc[i, 'p-value'] = pvalue\n        \n    return stats_df","19c02f03":"f_test_df = one_way_anova(train_df_categorical_le, target_df_log)\nf_top10_features = f_test_df[f_test_df['p-value'] < 0.05].sort_values(by = 'f-statistic', ascending = False).head(10)\nf_top10_features","84e37f09":"sns.heatmap(data = f_top10_features, annot=True )\nplt.title('Top 10 Categorical Features -  Correlation w\/ Target and log(Target)')","7ca90dd4":"mi = mutual_info_regression(train_df_categorical_le, target_df_log, discrete_features = True, \n                             n_neighbors=5, copy=True, random_state=None)\nmi_df = pd.DataFrame(mi, index = train_df_categorical.columns, columns = ['mutual_information'])","b23ce337":"mi_top10_features = mi_df.sort_values(by = 'mutual_information', ascending=False).head(10)\nsns.heatmap(data = mi_top10_features, annot=True )\nplt.title('Top 10 Categorical Features - Mutual Information Regression - Discrete Features')","8bf351cd":"rf_cat = RandomForestRegressor(n_estimators=100, criterion='mse', max_features='sqrt')\nrf_cat.fit(train_df_categorical_le, target_df_log)","ed3efca8":"# Store the top 10 most important features based off rf regressor\nrf_cat_feature_importance_df = pd.DataFrame(rf_cat.feature_importances_, train_df_categorical.columns, columns=['Importance_Value'])\nrf_cat_top10_features = rf_cat_feature_importance_df.sort_values(by = ['Importance_Value'], ascending=False).head(10)","e3046951":"sns.heatmap(data = rf_cat_top10_features, annot=True )\nplt.title('Top 10 Categorical Features - Random Forest - Feature Importance Value')","dca43ed8":"# Subset of intersection of both f-test and mi-test \n\nset(f_top10_features.index).intersection(mi_top10_features.index).intersection(rf_cat_top10_features.index)","e3376780":"selected_features_categorical = ['0f49e0f05', '7bf58da23', 'c16a7d537']","711a0dcc":"# scatter plot btw target and shortlisted features\nindex_feature = selected_features_categorical\nplt.subplots(3,1,figsize=(5,14))\nfor i in range(1, 4):\n    col = index_feature[i-1]\n    plt.subplot(3, 1, i)\n    sns.regplot(x=train_df_categorical[col], y = target_df, fit_reg=False)\n    plt.xscale('log')\n    plt.yscale('log')\n    plt.tight_layout()\n    plt.title(col)","a33a9773":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score","fb25a668":"train_df_continuous = train_df[remainder_cols]\n\n# Standardize X variables\nscaler = StandardScaler()\nX_train_df_continuous = scaler.fit_transform(train_df_continuous)","d7317635":"alpha = [0.2, 0.25, 0.275, 0.3, 0.325, 0.35]","80a35a86":"lasso_feature_coef_df = pd.DataFrame(np.zeros((len(remainder_cols), len(alpha))), index=remainder_cols, columns=alpha)\nfor a in alpha: \n    lasso = Lasso(alpha=a)\n    lasso.fit(X_train_df_continuous, target_df_log)\n    \n    lasso_feature_coef_df[a] = lasso.coef_","598afbb8":"lasso_top10_features = lasso_feature_coef_df.reindex(index=lasso_feature_coef_df[0.30].abs()\\\n.sort_values(ascending = False).index).head(10)\n\nlasso_top10_features","cde0ce82":"selected_features_lasso = lasso_top10_features.index[:4].tolist()\nprint('Features of significant coefficient include: \\n', selected_features_lasso)","70f7d97f":"rf = RandomForestRegressor(n_estimators=200, criterion='mse', max_features='sqrt')\nrf.fit(X_train_df_continuous, target_df_log)","5339872f":"# Store the top 30 most important features based off rf regressor\nrf_feature_importance_df = pd.DataFrame(rf.feature_importances_, index=remainder_cols, columns=['Importance_Value'])\nrf_top30_features = rf_feature_importance_df.sort_values(by = ['Importance_Value'], ascending=False).head(30)","15d1ee59":"ax0 = sns.barplot(x = rf_top30_features.index, y = 'Importance_Value', data=rf_top30_features)\nax0.set_xticklabels(ax0.get_xticklabels(), fontsize = 12, rotation=40, ha=\"right\")\nplt.title('Top 30 Features - Random Forest Regression')\nplt.show()","730abc5f":"selected_features_rf = rf_top30_features[rf_top30_features.Importance_Value >= 0.005].index.tolist()\n\nprint('Features of high importance value include: \\n', selected_features_rf)","4d9ed616":"gbr = GradientBoostingRegressor(loss='ls', n_estimators=200, learning_rate=0.1, \n                                max_depth=8, max_features = 'sqrt',  \n                                min_samples_split = 500, random_state=0)\ngbr.fit(X_train_df_continuous, target_df_log)","baaeb0ba":"gbr_feature_importance_df = pd.DataFrame(gbr.feature_importances_, index=remainder_cols, columns=['Importance_Value'])\ngbr_top30_features = gbr_feature_importance_df.sort_values(by = ['Importance_Value'], ascending=False).head(30)","d310b1ef":"ax1 = sns.barplot(x = gbr_top30_features.index, y = 'Importance_Value', data=gbr_top30_features)\nax1.set_xticklabels(ax1.get_xticklabels(), fontsize = 12, rotation=40, ha=\"right\")\nplt.title('Top 30 Features - Gradient Tree Boosting Regression')\nplt.show()","fd6c95f2":"selected_features_gbr = gbr_top30_features[gbr_top30_features.Importance_Value >= 0.005].index.tolist()\n\nprint('Features of high importance value include: \\n', selected_features_gbr)","e060731e":"#### Let's merge all features selected from the Continuous section\n\nselected_features_continuous = set(selected_features_rf+selected_features_lasso+selected_features_gbr)\n\nprint('Selected features among continuous variables include: \\n', selected_features_continuous)","34a9728f":"## Feature Importance: Categorizing variable types before selection\n > ### Subset features into 'Constant, Binary, Categorical, and Continuous\n - Constant: only has value zero\n - Binary: embodies no meaningful binary information\n - Categorical: univariate ANOVA test to identify features\n - Continuous: regularized regression, random forest, gradient boosting","686c1bd9":"## Dimension and Data Type\n1. Target Distribution\n2. Check Missing Values\n3. Check Data Types\n4. Reconcile Data Types btw Train and Test","d0b6f850":"<font color=blue>\nUpdates 2.0: I made a huge mistake in my prior version of kernel by dropping the entire set of BINARY VARIABLES. I have just realized that these are truly likely One-Hot Encoded variables and contain much inferential information; in the end, I did a univariate (pair-wise) linear regression on X and y, ending up getting a dozen statistically significant variables. \n\n\nUpdates 1.0: Mutual Information Regression is added to supplement ANOVA in Categorical feature selection, results reveal robust evidence that these three variables, '0f49e0f05', '7bf58da23', 'c16a7d537', are the only ones significant to be considered within the category. \n<\/font>\n## In a nutshell:\n\nThis notebook aims to extract a small number of features from the vast pool of unlabelled variables. \n\nIt first subsets features into univariate, binary, categorical and continuous. Then it goes on to drop both the univariate and binary (501 in total). For the categorical and continuous, the following are kept.\n> - Categorical: ['0f49e0f05', '7bf58da23', 'c16a7d537']\n> - Continuous: ['fb0f5dbfe', 'eeb9cd3aa', '58e2e02e6', '9fd594eec', '241f0f867', 'b43a7cfd5', 'd6bb78916', '66ace2992', '402b0d650', '58232a6fb', '20aa07010', 'f190486d6', '6eef030c1', '15ace8c9f']\n\n\n\n\nEnjoy!","03a340d0":"### Continuous Variable: \n\n- ** L1 Regularization: Lasso **\n- ** Random Forest **\n- ** Gradient Tree Boosting **","993ee41e":"* * * Work in Process... Stay tuned","0fd1347a":"<font color=blue>_ This function filters features with n or less unique values and hence serves to subset feature types _<\/font>","0d5cfc38":"<font color=blue>_ function conducts univariate ANOVA test between each categorical feature and target _<\/font>","a2364c9a":"#### Gradient Tree Boosting Regression","f9b344be":"#### Lasso Regression\n- L1 norm tends to produce sparse solutions by forcing weak features to have coefficient values of zero, which is neat for reducting the dimensionality of data\n- To prevent overfitting, we choose a range of alpha to explore the optimal penalty value\n- As alpha increases, the model complexity reduces, namely the degree of overfitting decreases.\n- Overall, only four features stand out to own non-zero coefficients","2c5e175c":"### Constant Variable: drop it!\n***\n- ** there are 256 features, all of which possess single value 0.0 **\n\n- ** As they do not contain information as to the change in Target, I'd have them truncated **\n\n\n*Notice that if you randomly choose a category_1_cols feature from the Test Set, you end up getting a majority of 0.0 (in fact, 99.9% are zeros). So it can be fairly deducted that the non-zero observations are just noise","a6a23584":"#### Distribution of Target Variable\n> Descriptive: \n  > - Target is nowhere near bar bell shaped and has very extreme fat tails\n  \n  > - The most common values are mutliples of million, e.g. 2M, 10M, etc.\n  \n  > - Large extreme values make predictions harder\n\n\n***\n- ** Based on the magnitude of the value, combined with the business nature of Santander, the transaction amount is likely to be determined more heavily by qualitative characteristics.**","c5351216":"### Categorical Variable: \n\n> - As prescribed above, I limit the number of unique values of categorical variables to be (2,15]\n> - 15 is an arbitrary cutoff. Later on, a lesser or greater number may make more sense later on\n\n- ANOVA is used, as a parametric method, to identify if there's 1) linear correlation; 2) difference in variance in each feature's value spectrum\n- Mutual Information is a non-parametric, entropy based method. It identifies **DEPENDENCY** (both linear and non-linear) between Y and each X. \n- Tree-based method. Random Forest is added here to complement the previous two methods to spot overlaps. ","09ae5a43":"### Binary Variable: def one-hot-encoded!\n***\n- 245 of these.\n\n- These features all contain only one non-zero value, indicating that they are extremely likely to be one-hot encoded varialbes.","ca7015ce":"#### Check Missing Value\n- Luckily we have no missing values between train and test","b1b4ef7e":"#### Summary:\n- all 3 methods point to this subset of features: {'0f49e0f05', '7bf58da23', 'c16a7d537'}","a0924aed":"#### Tree-Based Model","42b148cd":"#### Random Forest Regressor","a428592a":"#### ANOVA Test\n- Drawback: ANOVA relies on the assumption that distributions of Y will be different across varying values of X. The greater the variance of Y is relative to each categorical value of X, the higher the f-statistic is.\n- It only captures linear correlation, therefore if non-linear relationship exists, we need other tests to complement ANOVA ","fd1e83ab":"#### Mutual Information (MI)\n- MI measures the dependence of X to Y. If X and Y are independent, their mutual information is 0; if fully dependent, mutual information is 1. \n- Unlike ANOVA or f-test, MI does capture non-linear relationship. "}}