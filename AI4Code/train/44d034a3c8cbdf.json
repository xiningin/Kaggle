{"cell_type":{"59492417":"code","b721ecab":"code","897438fc":"code","e95db979":"code","8748a6b4":"code","5e2eac3d":"code","dd40b164":"code","2f877848":"code","2aa72513":"code","d25e3529":"code","a3db0d2b":"code","5a27482f":"code","edbd10c8":"code","ccde5aef":"code","1444f257":"code","47b99ae8":"code","e75390d3":"code","fa012e8b":"code","ef6b90ac":"code","8af75dd6":"code","6c49f730":"code","973f8251":"code","969e7e1f":"code","db9468fc":"code","ed25ea3e":"code","7df0cf66":"code","f589bdd5":"code","8ddbba55":"code","fdc81205":"code","b8863e52":"code","a72dbc67":"code","34176ec4":"code","72ac3d55":"code","b1686680":"code","12595b61":"code","1f0dcafd":"code","ca60292f":"code","6983cb48":"code","f23029bf":"markdown","94510171":"markdown","f518b7a6":"markdown","fb69eb20":"markdown","a8ad110d":"markdown","ed79c1a4":"markdown","8e309429":"markdown","4932cf21":"markdown","07d869c8":"markdown","1cb259f9":"markdown","e262734c":"markdown","d49bd9d4":"markdown","fa84f3db":"markdown","afb110b8":"markdown","f2fc27a0":"markdown","5d244905":"markdown","4aef616b":"markdown","39034864":"markdown","db2edc0f":"markdown","a0d84ce6":"markdown","621b9196":"markdown","5d08bc4a":"markdown","dd5eb263":"markdown","828c5957":"markdown","0dbcd22c":"markdown","239903ad":"markdown","eded51bc":"markdown","2a60a942":"markdown","6fc83426":"markdown"},"source":{"59492417":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport cufflinks as cf\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.formula.api as sm\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\nimport scikitplot as skplt\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing\nimport warnings\n#plt.style.use(['dark_background'])\n%matplotlib inline\nwarnings.filterwarnings('ignore')","b721ecab":"direc = \"..\/input\/\"\ntrain = pd.read_csv(direc + \"train_LZdllcl.csv\")\ntest = pd.read_csv(direc + \"test_2umaH9m.csv\")","897438fc":"train.head()","e95db979":"train.shape","8748a6b4":"train.describe()  #5-number summary for numerical columns","5e2eac3d":"for i in train.columns:\n    print (i + \": \"+str(sum(train[i].isnull()))+\" missing values\")","dd40b164":"train = train.dropna()\ntrain.shape","2f877848":"train[\"high_prev_rating\"] = np.where(train[\"previous_year_rating\"]>=3,1,0)\ntrain[\"low_prev_rating\"] = np.where(train[\"previous_year_rating\"]<3,1,0)\ntrain = train.drop([\"previous_year_rating\"],axis=1)\ntrain.head()","2aa72513":"trace1 = go.Bar(\n            x=['Not Promoted','Promoted'],\n            y=[sum(train[\"is_promoted\"]==0),sum(train[\"is_promoted\"]==1)],\n            marker=dict(color=[\"red\",\"red\"]),\n            name=\"Promotion Rate\",\n    )\n\ntrace2 = go.Bar(\n            x=['Males','Females'],\n            y=[sum(train[\"gender\"]=='m'),sum(train[\"gender\"]=='f')],\n            marker=dict(color=[\"yellow\",\"yellow\"]),\n            name='Gender'\n    )\n\n\ntrace3 = go.Bar(\n            x=['Did Not Win','Won Award'],\n            y=[sum(train[\"awards_won?\"]==0),sum(train[\"awards_won?\"]==1)],\n            marker=dict(color=[\"green\",\"green\"]),\n            name='Award won'\n    \n    )\n\ntrace4 = go.Bar(\n            x=['Did not meet KPI','Met KPI'],\n            y=[sum(train[\"KPIs_met >80%\"]==0),sum(train[\"KPIs_met >80%\"]==1)],\n            marker=dict(color=[\"blue\",\"blue\"]),\n            name='Met KPI'\n    )\n\ntrace5 = go.Bar(\n            x=['Other','Sourcing','Referred'],\n            y=[sum(train[\"recruitment_channel\"]=='other'),sum(train[\"recruitment_channel\"]=='sourcing'),sum(train['recruitment_channel']=='referred')],\n            marker=dict(color=[\"lime\",\"lime\",\"lime\"]),\n            name='Recruitment Channels'\n    )\n\ntrace6 = go.Bar(\n            x=list(train[\"education\"].unique()),\n            y=[sum(train[\"education\"]==i) for i in list(train[\"education\"].unique())],\n            marker=dict(color=[\"purple\",\"purple\",\"purple\"]),\n            name=\"Education\"\n    )\ntrace7 = go.Histogram(x=train['age'],name=\"Distribution of Age\")\n\ntrace8= go.Histogram(x=train['length_of_service'],name=\"Length of Service\")\n\ntrace9=go.Histogram(x=train['avg_training_score'],name=\"Distribution of average training score\")\n\nfig = tools.make_subplots(rows=3, cols=3,\n                          subplot_titles=[\"Promotion rate (Training Set): \" + str(round(100*(sum(train[\"is_promoted\"]==1)\/train.shape[0]),2)) +\"%\",\n                                         \"Male: \" + str(round(100*(sum(train[\"gender\"]==\"m\")\/train.shape[0]),2)) + \"%, Female: \" + str(round(100-100*(sum(train[\"gender\"]==\"m\")\/train.shape[0]),2))+\"%\",\n                                         \"Award Winning rate: \" + str(round(100*(sum(train[\"awards_won?\"]==1)\/train.shape[0]),2)) +\"%\",\n                                         \"Percent of KPI>80% Rate: \" + str(round(100*(sum(train[\"KPIs_met >80%\"]==1)\/train.shape[0]),2)) +\"%\",\n                                         \"Recruitment Channels\",\n                                         \"Distribution of Education\",\n                                          \"Distribution of Age\",\n                                          \"Length of Service\",\n                                          \"Average Training Score\"\n                                         ])\nfor i in fig['layout']['annotations']:\n    i['font'] = dict(size=10,color='black')\nfig.append_trace(trace1, 1,1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\nfig.append_trace(trace7, 3, 1)\nfig.append_trace(trace8, 3, 2)\nfig.append_trace(trace9, 3, 3)\nfig['layout'].update(height=900, width=900, title=\"<b>Distribution of Features<b>\")\npy.iplot(fig)\n","d25e3529":"labels=list(train[\"department\"].unique())\nsizes=[sum(train[\"department\"]==x) for x in labels]\n\ntrace = go.Pie(labels=labels, values=sizes,textfont=dict(size=13,color=\"black\"))\nlayout = go.Layout(\n    width=750,\n    height=400,\n    title = \"<b>Proportion of employees in each department<b>\",\n)\nfig=go.Figure([trace],layout=layout)\npy.iplot(fig)","a3db0d2b":"ax=sns.distplot(train[\"no_of_trainings\"],kde=False)\nplt.title(\"Number of Trainings\")\nplt.xlabel(\"Number\")\nplt.ylabel(\"Proportion\")\nfor txt in ax.texts:\n    txt.set_visible(False)","5a27482f":"sns.factorplot(y=\"age\",x=\"gender\", hue=\"department\",data=train,kind=\"box\",size=4,aspect=8\/5)","edbd10c8":"sns.factorplot(y=\"length_of_service\",x=\"gender\", hue=\"department\",data=train,kind=\"box\",size=4,aspect=8\/4)","ccde5aef":"sns.factorplot(y=\"age\",x=\"gender\", hue=\"education\",data=train,kind=\"box\",size=4,aspect=8\/5)","1444f257":"sns.factorplot(y=\"age\",x=\"gender\", hue=\"is_promoted\",data=train,kind=\"box\",size=4,aspect=8\/5)","47b99ae8":"sns.factorplot(y=\"age\",x=\"department\", hue=\"is_promoted\",data=train,kind=\"box\",size=4,aspect=8\/2)","e75390d3":"sns.factorplot(y=\"age\",x=\"education\", hue=\"is_promoted\",data=train,kind=\"box\",size=4,aspect=8\/6)","fa012e8b":"sns.factorplot(y=\"age\",x=\"is_promoted\", hue=\"education\",data=train,kind=\"box\",size=4,aspect=8\/6)","ef6b90ac":"sns.factorplot(y=\"age\",x=\"is_promoted\", hue=\"department\",data=train,kind=\"box\",size=4,aspect=8\/2)","8af75dd6":"def promoted_distribution(variable):\n    num = len(list(train[variable].unique()))\n    data=[]\n    for i in range(num):\n        data.append(go.Bar(\n            x=['Promoted','Not Promoted'],\n            y=[train[train['is_promoted']==1][variable].value_counts()[i],train[train['is_promoted']==0][variable].value_counts()[i]],\n            name=str(train[train['is_promoted']==1][variable].value_counts().index[i])))\n    layout = go.Layout(\n        width=500,\n        height=400,\n        barmode='stack',\n        title = \"Promotion rate among \" + str(variable)\n    )\n\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig, filename='stacked-bar')\n    \n    \ndef promoted_stacked_bar(variable):\n    x1=list(train[variable].unique())\n    trace1 = go.Bar(\n        x=x1,\n        y=[train[train[variable]==x1[i]][\"is_promoted\"].value_counts()[0] for i in range(len(x1))],\n        name='Not Promoted'\n    )\n    trace2 = go.Bar(\n        x=x1,\n        y=[train[train[variable]==x1[i]][\"is_promoted\"].value_counts()[1] for i in range(len(x1))],\n        name='Promoted'\n    )\n    layout = go.Layout(\n        width=500,\n        height=400,\n        barmode='stack',\n        title = \"Promotion rate among \" + str(variable)\n    )\n    data=[trace1,trace2]\n\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig)\n","6c49f730":"ls = ['department', 'region', 'education', 'gender', 'recruitment_channel', 'KPIs_met >80%', 'awards_won?']\nfor i in ls:\n    promoted_stacked_bar(i)\n","973f8251":"for cols in [\"department\",\"region\",\"education\",\"recruitment_channel\",\"gender\"]:\n    train[cols] = train[cols].astype('category')\n    train[cols] = train[cols].cat.codes\nf,ax = plt.subplots(figsize=(12,12))\nsns.heatmap(train.corr(), annot=True, linewidths=0.5, fmt= '.2f',ax=ax)","969e7e1f":"train[\"KPI&Award\"] = np.where(((train[\"KPIs_met >80%\"]==1) & (train[\"awards_won?\"]==1)),1,0)","db9468fc":"promoted_stacked_bar(\"KPI&Award\")","ed25ea3e":"train_features_eng = train\ntrain_features_eng = train_features_eng.drop(['employee_id','recruitment_channel','no_of_trainings','gender','length_of_service','region'],axis=1) #features that are not needed","7df0cf66":"train_features_eng=pd.get_dummies(train_features_eng, columns=[\"department\",\"education\"], prefix=[\"Dept\", \"Eduacation\"])\ntrain_features_eng.head()","f589bdd5":"def train_f1(model):\n    return round(f1_score(y_train,model.predict(x_train),average='macro'),2)\n\ndef test_f1(model):\n    return round(f1_score(y_test,model.predict(x_test),average='macro'),2)\n\ndef confusion_matrix_model(model_used):\n    cm=confusion_matrix(y_test,model_used.predict(x_test))\n    col=[\"Predicted Promoted\",\"Predicted No Promotion\"]\n    cm=pd.DataFrame(cm)\n    cm.columns=[\"Predicted Promoted\",\"Predicted No Promotion\"]\n    cm.index=[\"Actual Promoted\",\"Actual No Promotion\"]\n    return cm.T\n\ndef confusion_matrix_model_train(model_used):\n    cm=confusion_matrix(y_train,model_used.predict(x_train))\n    col=[\"Predicted Promoted\",\"Predicted No Promotion\"]\n    cm=pd.DataFrame(cm)\n    cm.columns=[\"Predicted Promoted\",\"Predicted No Promotion\"]\n    cm.index=[\"Actual Promoted\",\"Actual No Promotion\"]\n    return cm.T\n\ndef importance_of_features(model):\n    features = pd.DataFrame()\n    features['feature'] = x_train.columns\n    features['importance'] = model.feature_importances_\n    features.sort_values(by=['importance'], ascending=True, inplace=True)\n    features.set_index('feature', inplace=True)\n    return features.plot(kind='barh', figsize=(6,6))","8ddbba55":"x1 = train_features_eng.drop([\"is_promoted\"],axis=1)\ny1 = train_features_eng.loc[:,\"is_promoted\"]\nx_train,x_test,y_train,y_test=train_test_split(x1,y1,test_size=0.2,random_state=0,stratify=y1)\nk_fold = KFold(n_splits=5, shuffle=True, random_state=0)","fdc81205":"x_train.head()","b8863e52":"#param_grid = dict(C=(0.0001,0.001,0.005,0.01,0.1,0.5))\n#log_reg1 = GridSearchCV(LogisticRegression(penalty=\"l1\"),param_grid=param_grid,scoring=\"f1_macro\")\nlog_reg1=LogisticRegression(penalty=\"l1\",C=0.5)\nlog_reg1.fit(x_train,y_train)\n#print(log_reg1.best_params_)\nprint (\"In-sample F1 Score: \" + str(train_f1(log_reg1)))\nprint (\"Test F1 Score: \" + str(test_f1(log_reg1)))\n#confusion_matrix_model(log_reg1)","a72dbc67":"#param_grid = dict(C=(0.0001,0.001,0.005,0.01,0.1,0.5,1))\n#log_reg2 = GridSearchCV(LogisticRegression(penalty=\"l2\"),param_grid=param_grid,scoring=\"f1_macro\")\nlog_reg2=LogisticRegression(penalty=\"l2\",C=1)\nlog_reg2.fit(x_train,y_train)\n#print(log_reg2.best_params_)\nprint (\"In-sample F1 Score: \" + str(train_f1(log_reg2)))\nprint (\"Test F1 Score: \" + str(test_f1(log_reg2)))\n#confusion_matrix_model(log_reg2)","34176ec4":"#param_grid = dict(C=(0.001,0.01,0.1,0.5,1,2),gamma=(0.001,0.01,0.1,0.5,1,2))\n#svc_rbf = GridSearchCV(SVC(kernel=\"rbf\",random_state=0),param_grid=param_grid,scoring=\"f1_macro\")\nsvc_rbf = SVC(kernel='rbf', gamma=0.001, C=0.01,random_state=0)\nsvc_rbf.fit(x_train, y_train)\n#print(svc_rbf.best_params_)\n\nprint (\"In-sample F1 Score: \" + str(train_f1(svc_rbf)))\nprint (\"Test F1 Score: \" + str(test_f1(svc_rbf)))\n#confusion_matrix_model(svc_rbf)","72ac3d55":"#param_grid = dict(n_neighbors=np.arange(10,70),weights=(\"uniform\",\"distance\"),p=(1,2))\n#KNN = GridSearchCV(KNeighborsClassifier(),param_grid=param_grid,scoring=\"f1_macro\")\nKNN=KNeighborsClassifier(n_neighbors=30,p=1,weights='distance')\nKNN.fit(x_train,y_train)\n#print(KNN.best_params_)\nprint (\"In-sample F1 Score: \" + str(train_f1(KNN)))\nprint (\"Test F1 Score: \" + str(test_f1(KNN)))\n#confusion_matrix_model(KNN)","b1686680":"#param_grid = dict(max_depth=np.arange(4,10),min_samples_leaf=np.arange(1,8),min_samples_split=np.arange(2,8),max_leaf_nodes=np.arange(30,100,10))\n#Dec_tree = GridSearchCV(DecisionTreeClassifier(),param_grid=param_grid,scoring=\"f1_macro\")\nDec_tree=DecisionTreeClassifier(max_depth= 9, max_leaf_nodes= 60, min_samples_leaf= 7, min_samples_split= 2)\nDec_tree.fit(x_train,y_train)\n#print(Dec_tree.best_params_)\nprint (\"In-sample F1 Score: \" + str(train_f1(Dec_tree)))\nprint (\"Test F1 Score: \" + str(test_f1(Dec_tree)))\n#confusion_matrix_model(Dec_tree)","12595b61":"#param_grid = dict(max_depth=np.arange(3,10),min_samples_leaf=np.arange(1,10),min_samples_split=np.arange(2,6),max_leaf_nodes=np.arange(50,120,10))\n#param_grid = dict(n_estimators = np.arange(50,500,50))\n#ranfor = GridSearchCV(RandomForestClassifier(n_estimators=450,max_depth= 9, max_leaf_nodes=110, min_samples_leaf= 1, min_samples_split= 2,random_state=0),param_grid=param_grid,scoring=\"f1_macro\")\n#ranfor = GridSearchCV(RandomForestClassifier(max_depth= 7, max_leaf_nodes=100, min_samples_leaf= 6, min_samples_split= 2,random_state=0),param_grid=param_grid,scoring=\"accuracy\")\nranfor = RandomForestClassifier(n_estimators=450,max_depth= 9, max_leaf_nodes=110, min_samples_leaf= 1, min_samples_split= 2,random_state=0)\nranfor.fit(x_train,y_train)\n#print(ranfor.best_params_)\nprint (\"In-sample F1 Score: \" + str(train_f1(ranfor)))\nprint (\"Test F1 Score: \" + str(test_f1(ranfor)))\n#confusion_matrix_model(ranfor)","1f0dcafd":"#param_grid = dict(n_estimators=np.arange(50,500,50),max_depth=np.arange(6,12),learning_rate=(0.0001,0.001,0.01,0.1))\n#xgclass = GridSearchCV(xgb.XGBClassifier(random_state=0),param_grid=param_grid,scoring=\"accuracy\")\nxgclass = xgb.XGBClassifier(max_depth=9, n_estimators=450, learning_rate=0.01)\nxgclass.fit(x_train,y_train)\n#print(xgclass.best_params_)\nprint (\"In-sample F1 Score: \" + str(train_f1(xgclass)))\nprint (\"Test F1 Score: \" + str(test_f1(xgclass)))\nconfusion_matrix_model(xgclass)","ca60292f":"importance_of_features(xgclass)","6983cb48":"Classifiers=[\"Logistic Regression (Lasso)\",\"Logistic Regression (Ridge)\",\"SVC (RBF Kernel)\",\"K-Nearest Neighbours\",\"Decision Tree\",\"Random Forest\",\"XGBoost\"]\ntrainf1 = [train_f1(x) for x in [log_reg1,log_reg2,svc_rbf,KNN,Dec_tree,ranfor,xgclass]]\ntestf1 = [test_f1(x) for x in [log_reg1,log_reg2,svc_rbf,KNN,Dec_tree,ranfor,xgclass]]\ncols=[\"Classifier\",\"Training F1 Score\",\"Test F1 Score\"]\npred_results = pd.DataFrame(columns=cols)\npred_results[\"Classifier\"]=Classifiers\npred_results[\"Training F1 Score\"]=trainf1\npred_results[\"Test F1 Score\"]=testf1\npred_results","f23029bf":"**Encode all categorical variables**","94510171":"**2.3 Let's check the promotion rate across different variables**","f518b7a6":"# **Using dataset on WNS (Holdings) Limited provided by Analytics Vidyha WNS Analytics Hackathon 2018, I would try to predict promotion of employees.**","fb69eb20":"**Logistic Regression (Ridge)**","a8ad110d":"**Decision Tree**","ed79c1a4":"**Logistic Regression (Lasso)**","8e309429":"\nOnly education and previous year rating have missing values. We will remove the missing values since the number of rows that will be removed is small in comparison with the amount of rows we have.","4932cf21":"**SVC (RBF Kernel)**","07d869c8":"# **2. Exploratory Analysis**","1cb259f9":"**2.1.3 Trainings**","e262734c":"The distribution of the length of service is also right-skewed, with the mean 5.8 years and the median 5 years.","d49bd9d4":"**XGBoost would be the model chosen due to the highest Test-set F1 score**","fa84f3db":"# **1. Import packages and dataset**","afb110b8":"# 4. Classification models\n\nThe models will be optimised using GridSearchCV based on F1 score. F1 score gives a weighted average between precision and accuracy\/recall. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).\n\nI have typed in some of the optimised parameters based on the GridSearchCV code output, then commented out the GridSearchCV codes to make the notebook run faster as it won't be re-optimised.","f2fc27a0":"**Correlation Heatmap**","5d244905":"**Random Forest**","4aef616b":"**Dealing with Missing Values**","39034864":"**Checking for missing values**","db2edc0f":"We can see that there isn't any clear obvious bias linking any variables with promotion rate.","a0d84ce6":"We shall add another variable that states if an employee has won both awards and met KPI.","621b9196":"**XGBoost**","5d08bc4a":"**2.1.1 Distribution of categorical features**","dd5eb263":"**2.1 Let's start by visualising the distribution of each columns.**","828c5957":"We can see that the distribution of age is slightly skewed to the right. The mean age is around 34.8 while the median age is 33 based on the 5-number summary. ","0dbcd22c":"**KNN**","239903ad":"**2.2 Comparing variables by groups**","eded51bc":"**2.1.2 Departments**","2a60a942":"# 3.Feature Engineering","6fc83426":"We see again from the correlation heatmap that almost all the variables are not directly correlated with promotion rate. The variables with the highest correlation (0.22 and 0.20 respectively) is whether the KPI is met and whether an award was won. "}}