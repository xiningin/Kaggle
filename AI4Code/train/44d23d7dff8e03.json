{"cell_type":{"15628b40":"code","a3c6cf49":"code","ab5ad293":"code","080f6e4c":"code","ff603436":"code","5814857b":"code","c1f21955":"code","01779687":"code","eb1dd0d3":"code","f1530d11":"code","e10c2205":"code","615f146b":"code","9889ce0a":"code","9e41975e":"code","709033e0":"code","1c02563e":"code","f5ded435":"code","3d59a2e4":"code","a29c7213":"code","b807a712":"code","c1fc1e4e":"code","c3203dba":"code","bdd222a9":"code","0a2bb98c":"code","2ce26921":"code","fbf70e5b":"code","158e2fd5":"code","52464d57":"code","74ee96ef":"code","bd91f3a3":"code","f6285aa0":"code","c280ab4e":"code","7cb15646":"code","bb68d714":"code","134c2002":"markdown"},"source":{"15628b40":"#### Importing Libraries ####\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sn","a3c6cf49":"dataset = pd.read_csv('..\/input\/churn_data.csv') # Users who were 60 days enrolled, churn in the next 30","ab5ad293":"#### EDA ####\n\ndataset.head(5) # Viewing the Data","080f6e4c":"dataset.columns","ff603436":"dataset.describe() # Distribution of Numerical Variables","5814857b":"# Cleaning Data\ndataset[dataset.credit_score < 300]\ndataset = dataset[dataset.credit_score >= 300]","c1f21955":"# Removing NaN\ndataset.isna().any()","01779687":"dataset.isna().sum()","eb1dd0d3":"dataset = dataset.drop(columns = ['credit_score', 'rewards_earned'])","f1530d11":"## Pie Plots\ndataset2 = dataset[['housing', 'is_referred', 'app_downloaded', 'web_user', 'app_web_user', 'ios_user', 'android_user', 'registered_phones', 'payment_type', 'waiting_4_loan', 'cancelled_loan', 'received_loan', 'rejected_loan', 'zodiac_sign', 'left_for_two_month_plus', 'left_for_one_month', 'is_referred']]\nfig = plt.figure(figsize=(15, 12))\nplt.suptitle('Pie Chart Distributions', fontsize=20)\nfor i in range(1, dataset2.shape[1] + 1):\n    plt.subplot(6, 3, i)\n    f = plt.gca()\n    f.axes.get_yaxis().set_visible(False)\n    f.set_title(dataset2.columns.values[i - 1])\n   \n    values = dataset2.iloc[:, i - 1].value_counts(normalize = True).values\n    index = dataset2.iloc[:, i - 1].value_counts(normalize = True).index\n    plt.pie(values, labels = index, autopct='%1.1f%%')\n    plt.axis('equal')\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])","e10c2205":"## Exploring Uneven Features\ndataset[dataset2.waiting_4_loan == 1].churn.value_counts()\ndataset[dataset2.cancelled_loan == 1].churn.value_counts()\ndataset[dataset2.received_loan == 1].churn.value_counts()\ndataset[dataset2.rejected_loan == 1].churn.value_counts()\ndataset[dataset2.left_for_one_month == 1].churn.value_counts()","615f146b":"## Correlation with Response Variable\ndataset2.drop(columns = ['housing', 'payment_type', 'registered_phones', 'zodiac_sign']).corrwith(dataset.churn).plot.bar(figsize=(20,10), title = 'Correlation with Response variable', fontsize = 15, rot = 45, grid = True)","9889ce0a":"## Correlation Matrix\nsn.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = dataset.drop(columns = ['user', 'churn']).corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 15))\n\n# Generate a custom diverging colormap\ncmap = sn.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsn.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","9e41975e":"# Removing Correlated Fields\ndataset = dataset.drop(columns = ['app_web_user'])","709033e0":"## Note: Although there are somewhat correlated fields, they are not colinear\n## These feature are not functions of each other, so they won't break the model\n## But these feature won't help much either. Feature Selection should remove them.\n\ndataset.to_csv('new_churn_data.csv', index = False)","1c02563e":"import random","f5ded435":"dataset = pd.read_csv('new_churn_data.csv')","3d59a2e4":"## Data Preparation\nuser_identifier = dataset['user']\ndataset = dataset.drop(columns = ['user'])","a29c7213":"# One-Hot Encoding\ndataset.housing.value_counts()\ndataset.groupby('housing')['churn'].nunique().reset_index()\ndataset = pd.get_dummies(dataset)\ndataset.columns\ndataset = dataset.drop(columns = ['housing_na', 'zodiac_sign_na', 'payment_type_na'])","b807a712":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns = 'churn'), dataset['churn'], test_size = 0.2, random_state = 0)","c1fc1e4e":"# Balancing the Training Set\ny_train.value_counts()\n\npos_index = y_train[y_train.values == 1].index\nneg_index = y_train[y_train.values == 0].index\n\nif len(pos_index) > len(neg_index):\n    higher = pos_index\n    lower = neg_index\nelse:\n    higher = neg_index\n    lower = pos_index\nrandom.seed(0)\nhigher = np.random.choice(higher, size=len(lower))\nlower = np.asarray(lower)\nnew_indexes = np.concatenate((lower, higher))\n\nX_train = X_train.loc[new_indexes,]\ny_train = y_train[new_indexes]","c3203dba":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train2 = pd.DataFrame(sc_X.fit_transform(X_train))\nX_test2 = pd.DataFrame(sc_X.transform(X_test))\nX_train2.columns = X_train.columns.values\nX_test2.columns = X_test.columns.values\nX_train2.index = X_train.index.values\nX_test2.index = X_test.index.values\nX_train = X_train2\nX_test = X_test2","bdd222a9":"# Fitting Model to the Training Set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","0a2bb98c":"# Predicting Test Set\ny_pred = classifier.predict(X_test)","2ce26921":"# Evaluating Results\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\naccuracy_score(y_test, y_pred)\nprecision_score(y_test, y_pred) # tp \/ (tp + fp)\nrecall_score(y_test, y_pred) # tp \/ (tp + fn)\nf1_score(y_test, y_pred)","fbf70e5b":"df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\nplt.figure(figsize = (10,7))\nsn.set(font_scale=1.4)\nsn.heatmap(df_cm, annot=True, fmt='g')\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))","158e2fd5":"# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"SVM Accuracy: %0.3f (+\/- %0.3f)\" % (accuracies.mean(), accuracies.std() * 2))","52464d57":"# Analyzing Coefficients\npd.concat([pd.DataFrame(X_train.columns, columns = [\"features\"]),\n           pd.DataFrame(np.transpose(classifier.coef_), columns = [\"coef\"])\n           ],axis = 1)","74ee96ef":"#### Feature Selection ####\n\n\n# Recursive Feature Elimination\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression","bd91f3a3":"# Model to Test\nclassifier = LogisticRegression()\n# Select Best X Features\nrfe = RFE(classifier, 20)\nrfe = rfe.fit(X_train, y_train)\n# summarize the selection of the attributes\nprint(rfe.support_)\nprint(rfe.ranking_)\nX_train.columns[rfe.support_]","f6285aa0":"# New Correlation Matrix\nsn.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = X_train[X_train.columns[rfe.support_]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 15))\n\n# Generate a custom diverging colormap\ncmap = sn.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsn.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}) ","c280ab4e":"# Fitting Model to the Training Set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.fit(X_train[X_train.columns[rfe.support_]], y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test[X_train.columns[rfe.support_]])\n\n# Evaluating Results\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\naccuracy_score(y_test, y_pred)\nprecision_score(y_test, y_pred) # tp \/ (tp + fp)\nrecall_score(y_test, y_pred) # tp \/ (tp + fn)\nf1_score(y_test, y_pred)\n\ndf_cm = pd.DataFrame(cm, index = (1, 0), columns = (1, 0))\nplt.figure(figsize = (10,7))\nsn.set(font_scale=1.4)\nsn.heatmap(df_cm, annot=True, fmt='g')\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))","7cb15646":"# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier,\n                             X = X_train[X_train.columns[rfe.support_]],\n                             y = y_train, cv = 10)\nprint(\"SVM Accuracy: %0.3f (+\/- %0.3f)\" % (accuracies.mean(), accuracies.std() * 2))\n\n# Analyzing Coefficients\npd.concat([pd.DataFrame(X_train[X_train.columns[rfe.support_]].columns, columns = [\"features\"]),\n           pd.DataFrame(np.transpose(classifier.coef_), columns = [\"coef\"])\n           ],axis = 1)","bb68d714":"# Formatting Final Results\nfinal_results = pd.concat([y_test, user_identifier], axis = 1).dropna()\nfinal_results['predicted_churn'] = y_pred\nfinal_results = final_results[['user', 'churn', 'predicted_churn']].reset_index(drop=True)\nfinal_results","134c2002":"# Model Building"}}