{"cell_type":{"7fd4f008":"code","1c80dd26":"code","1591b262":"code","4e16ed20":"code","1ecf6379":"code","5f7ed95e":"code","b4bc7893":"code","edcf5f32":"code","76e2ce3c":"code","dbb0f39f":"code","d87bb252":"code","65c317be":"markdown","793b6e4d":"markdown","01553d12":"markdown"},"source":{"7fd4f008":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","1c80dd26":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","1591b262":"df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ndf.head()","4e16ed20":"context_tokenizer = Tokenizer()\ncontext_tokenizer.fit_on_texts(df.text.fillna(''))\ncontext = context_tokenizer.texts_to_sequences(df.text.fillna(''))\n\nanswers = context_tokenizer.texts_to_sequences(df.selected_text.fillna(''))\nbeg_pos = [[1 if a[x:x+len(b)] == b else 0 for x in range(len(a))] for a, b in zip(context, answers)]\nend_pos = [[1 if a[x:x+len(b)] == b else 0 for x in range(len(a))] for a, b in zip(context, answers)]","1ecf6379":"context = np.array(pad_sequences(context, maxlen=36, padding='post', truncating='post'))\nbeg_pos = np.array(pad_sequences(beg_pos, maxlen=36, padding='post', truncating='post'))\nend_pos = np.array(pad_sequences(end_pos, maxlen=36, padding='post', truncating='post'))\n\nall_zero = np.all((beg_pos == 0), axis=1)\n\ncontext = context[~all_zero]\nbeg_pos = beg_pos[~all_zero]\nend_pos = end_pos[~all_zero]\n\nbeg_pos = np.expand_dims(beg_pos, axis=2)\nend_pos = np.expand_dims(end_pos, axis=2)\nans_vec = np.concatenate((beg_pos, end_pos), axis=2)\n\ncontext.shape, beg_pos.shape, end_pos.shape, ans_vec.shape","5f7ed95e":"question_tokenizer = Tokenizer()\nquestion_tokenizer.fit_on_texts(df.sentiment.fillna(''))\nquestion = question_tokenizer.texts_to_sequences(df.sentiment.fillna(''))\nquestion = np.array(pad_sequences(question, maxlen=36, padding='post', truncating='post'))\nquestion = question[~all_zero]\nquestion.shape","b4bc7893":"context_train, context_valid, question_train, question_valid, ans_vec_train, ans_vec_valid = train_test_split(\n    context, question, ans_vec, test_size=0.1, random_state=0\n)\n(\n    context_train.shape, context_valid.shape, question_train.shape, \n    question_valid.shape, ans_vec_train.shape, ans_vec_valid.shape\n)","edcf5f32":"EMBED_DIM = 64\nN_REC = 64\n\ncontext_inp = L.Input(shape=(36, ), name='context')\nquestion_inp = L.Input(shape=(36, ), name='question')\n\ncontext_emb = L.Embedding(len(context_tokenizer.word_index)+1, EMBED_DIM, name='context_embeddings')(context_inp)\nquestion_emb = L.Embedding(len(question_tokenizer.word_index)+1, EMBED_DIM, name='question_embeddings')(question_inp)\n\ncontext_emb = L.GRU(N_REC, return_sequences=True, name='context_gru')(context_emb)\nquestion_emb = L.GRU(N_REC, return_sequences=True, name='question_gru')(question_emb)\n\nconcat_emb = L.Concatenate(axis=-1, name='concatenate')([context_emb, question_emb])\n\noutputs = L.Dense(2, activation='sigmoid', name='outputs')(concat_emb)\n\nmodel = keras.Model(inputs=[context_inp, question_inp], outputs=outputs)\nmodel.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(1e-4))\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","76e2ce3c":"es = keras.callbacks.EarlyStopping(min_delta=1e-4, patience=5, verbose=1, restore_best_weights=True)\nrlp = keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)\n\nhistory = model.fit(\n    [context_train, question_train], ans_vec_train, validation_data=([context_valid, question_valid], ans_vec_valid),\n    epochs=25, callbacks=[es, rlp]\n)","dbb0f39f":"pd.DataFrame(history.history)[['loss', 'val_loss']].plot();","d87bb252":"idx = 28\nquery_context = context_valid[idx:idx+1]\nquery_question = question_valid[idx:idx+1]\nquery_ans_vec = ans_vec_valid[idx:idx+1]\nquery_ans_beg, query_ans_end  = np.ravel(ans_vec_valid[idx:idx+1].argmax(axis=1))\nprint('Context:', context_tokenizer.sequences_to_texts(query_context))\nprint('Question:', question_tokenizer.sequences_to_texts(query_question))\nprint('Answer:', context_tokenizer.sequences_to_texts([query_context[0][query_ans_beg: query_ans_end+1]]))\npred_ans_beg, pred_ans_end = np.ravel(model([query_context, query_question]).numpy().argmax(axis=1))\nprint('Predicted Answer:', context_tokenizer.sequences_to_texts([query_context[0][pred_ans_beg: pred_ans_end+1]]))","65c317be":"# Model","793b6e4d":"#  Data Preparation","01553d12":"# Inference"}}