{"cell_type":{"e6e07bcf":"code","7c437ded":"code","078442c4":"code","d4d3de9c":"code","7882ba3e":"code","5e3a0f27":"code","6c7b32b6":"code","de581569":"code","911f1b0c":"code","3c3fe331":"code","3521155b":"code","07e154b2":"code","1815fb0d":"code","1bce9be0":"code","01138ccb":"code","e3cc96af":"code","e57eb17c":"code","5ef6d0cb":"code","4b7999e2":"code","9d72f65d":"code","5d390aa8":"code","e76c552c":"code","fc772ab4":"code","51cd0bd4":"code","654f5045":"code","9882ef3a":"code","74f6043b":"code","28d053f4":"code","da4b3583":"code","d69e7e1f":"code","b3d46ac4":"code","73fc9baf":"code","ad0431bd":"code","d37af8a4":"code","00ffee5e":"code","2cd2b69f":"code","f0305492":"code","08d84ebb":"code","fd106f0c":"code","93362032":"code","193d3601":"code","2646ecd1":"code","de2db9a0":"code","54fa627c":"code","481b2d0e":"code","6f163e35":"code","09a1fc8d":"code","eaa5544a":"code","9e7dfdd4":"code","989d77c1":"code","f9401f69":"code","302d50f0":"code","4867d197":"code","4c2c8f89":"code","3f507dbb":"code","e9752057":"code","424aa0ce":"code","4dd41941":"code","cc657ade":"code","c71034d3":"code","51275a05":"code","205da405":"code","36004000":"code","545f70a3":"code","ec09a0c8":"code","278a35dd":"markdown","5782c0b5":"markdown","ebcfb332":"markdown","423a7480":"markdown","0f87fe78":"markdown","c74e64ba":"markdown","8ccc71e4":"markdown","00b32d15":"markdown","b7117d59":"markdown","045a3e09":"markdown","6f3bb6f0":"markdown","f6a29e0a":"markdown","982d6a23":"markdown","dc646e8a":"markdown","0906804b":"markdown","672614d4":"markdown","53792594":"markdown","d9774c39":"markdown","5dc05582":"markdown","7c4d5cf2":"markdown","a09fc158":"markdown","d628e07c":"markdown","39174b6a":"markdown","a6aaefd8":"markdown","4497ad35":"markdown","28370a7b":"markdown","19d18816":"markdown","acba3ce5":"markdown","e85bb55c":"markdown","320a66c2":"markdown","c76d63d6":"markdown","d62cc8fb":"markdown","21db58b1":"markdown","5b100977":"markdown","8cb2e417":"markdown","62cc8921":"markdown","f4ddfb52":"markdown","d52d634e":"markdown"},"source":{"e6e07bcf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","7c437ded":"test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","078442c4":"#set display.max_columns to display all columns\npd.set_option('display.max_columns', None)","d4d3de9c":"train.sample(5)","7882ba3e":"combined_data = pd.concat([train, test], ignore_index=True)\n\ncombined_data.info()","5e3a0f27":"combined_data.describe(include=\"all\")","6c7b32b6":"combined_data.isnull().sum().sort_values(ascending=False).head(36)","de581569":"(train.corr()**2)['SalePrice'].sort_values(ascending = False)[1:]","911f1b0c":"sns.boxplot(train['OverallQual'], train['SalePrice']);","3c3fe331":"sns.regplot(train['GrLivArea'], train['SalePrice']);","3521155b":"sns.regplot(train['GarageArea'], train['SalePrice']);","07e154b2":"sns.regplot(train['TotalBsmtSF'], train['SalePrice']);","1815fb0d":"sns.regplot(train['1stFlrSF'], train['SalePrice']);","1bce9be0":"sns.residplot(train['GrLivArea'], train['SalePrice']);","01138ccb":"sns.distplot(train['SalePrice']);","e3cc96af":"#use numpy.log1p in order to target variable follows a normal distribution \ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\nsns.distplot(train['SalePrice']);","e57eb17c":"#check residual plot after transformed\nsns.residplot(train['GrLivArea'], train['SalePrice']);","5ef6d0cb":"#check for outliers\n#quantile_high = train['SalePrice'].quantile(0.99)\n#quantile_low = train['SalePrice'].quantile(0.01)\n\n#outliers_id = train[(train['SalePrice'] > quantile_high) | (train['SalePrice'] < quantile_low)].index","4b7999e2":"#copy features that are needed later\ntarget_array = train['SalePrice'].copy()\ntest_id = test['Id'].copy()\n\n#drop features\ntrain.drop(['Id','SalePrice'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\n\nprint(train.shape)\nprint(test.shape)","9d72f65d":"#change to categorical\ntrain['MSSubClass'] = train['MSSubClass'].astype('str')\ntest['MSSubClass'] = test['MSSubClass'].astype('str')","5d390aa8":"features_with_none = ['Alley', \n                     'FireplaceQu',\n                     'Fence',\n                     'MiscFeature']\n\nfor feature in features_with_none:\n    train[feature].fillna('None', inplace=True)\n    test[feature].fillna('None', inplace=True)","e76c552c":"#fill missing values with mode \nfeatures_fill_with_mode = ['Utilities',\n                           'Exterior1st',\n                           'Exterior2nd',\n                           'Electrical',\n                           'KitchenQual',\n                           'Functional',\n                           'SaleType']\n\nfor feature in features_fill_with_mode:\n    train[feature].fillna(train[feature].mode()[0], inplace=True)\n    test[feature].fillna(test[feature].mode()[0], inplace=True)","fc772ab4":"#fill missing values with mode of each MSSubClass\ntrain['MSZoning'] = train.groupby('MSSubClass')['MSZoning'].apply(lambda df: df.fillna(df.mode()[0]))\ntest['MSZoning'] = test.groupby('MSSubClass')['MSZoning'].apply(lambda df: df.fillna(df.mode()[0]))","51cd0bd4":"#fill missing values with mean of each Neighborhood\ntrain['LotFrontage'] = train.groupby('Neighborhood')['LotFrontage'].apply(lambda df: df.fillna(df.mean()))\ntest['LotFrontage'] = test.groupby('Neighborhood')['LotFrontage'].apply(lambda df: df.fillna(df.mean()))","654f5045":"#Train dataset\n#check if there are missing values in the MasVnrType, but the MasVnrArea != 0\nmas_vnr_features = ['MasVnrArea','MasVnrType']\n\nmask = (train['MasVnrArea'] != 0) & \\\n       train['MasVnrArea'].notnull() & \\\n       train['MasVnrType'].isnull()\n\ntrain[mask][mas_vnr_features]","9882ef3a":"#Test dataset\n#check if there are missing values in the MasVnrType, but the MasVnrArea != 0\nmask = (test['MasVnrArea'] != 0) & \\\n       test['MasVnrArea'].notnull() & \\\n       test['MasVnrType'].isnull()\n\ntest[mask][mas_vnr_features]","74f6043b":"#fill missing data\ntrain['MasVnrType'].fillna('None', inplace=True)\n\ntest.loc[mask, 'MasVnrType'] = test['MasVnrType'].mode()[0]\ntest['MasVnrType'].fillna('None', inplace=True)","28d053f4":"#Train dataset\n#check if there are missing values in the MasVnrArea, but the MasVnrType != 'None'\nmask = (train['MasVnrType'] != 'None') & \\\n        train['MasVnrArea'].isnull()\n\ntrain[mask][mas_vnr_features]","da4b3583":"#Test dataset\n#check if there are missing values in the MasVnrArea, but the MasVnrType != 'None'\nmask = (test['MasVnrType'] != 'None') & \\\n        test['MasVnrArea'].isnull()\n\ntest[mask][mas_vnr_features]","d69e7e1f":"#fill missing data with 0\ntrain['MasVnrArea'].fillna(0, inplace=True)\ntest['MasVnrArea'].fillna(0, inplace=True)","b3d46ac4":"#Train dataset\n#check if there are missing values in the PoolQC, but the PoolArea != 0\npool_features = ['PoolArea','PoolQC']\n\nmask = (train['PoolArea'] != 0) & \\\n       train['PoolQC'].isnull()\n\ntrain[mask][pool_features]","73fc9baf":"#Test dataset\n#check if there are missing values in the PoolQC, but the PoolArea != 0\nmask = (test['PoolArea'] != 0) & \\\n       test['PoolQC'].isnull()\n\ntest[mask][pool_features]","ad0431bd":"#fill missing values\ntrain['PoolQC'].fillna('None', inplace=True)\n\ntest.loc[mask, 'PoolQC'] = test['PoolQC'].mode()[0]\ntest['PoolQC'].fillna('None', inplace=True)","d37af8a4":"#Train dataset\n#check if there are missing values in the categorical features, but the TotalBsmtSF != 0\nbsmt_features_1 = ['BsmtQual','BsmtFinType1','BsmtFinType2','BsmtCond','BsmtExposure']\n\nmask = (train['BsmtQual'].isnull() | \\\n        train['BsmtFinType1'].isnull() | \\\n        train['BsmtFinType2'].isnull() | \\\n        train['BsmtCond'].isnull() | \\\n        train['BsmtExposure'].isnull() ) \\\n        & \\\n       ((train['TotalBsmtSF'] != 0) & \\\n        train['TotalBsmtSF'].notnull())\n\ntrain[mask][np.concatenate([bsmt_features_1,['TotalBsmtSF']])]","00ffee5e":"#Test dataset\n#check if there are missing values in the categorical features, but the TotalBsmtSF != 0\nmask = (test['BsmtQual'].isnull() | \\\n        test['BsmtFinType1'].isnull() | \\\n        test['BsmtFinType2'].isnull() | \\\n        test['BsmtCond'].isnull() | \\\n        test['BsmtExposure'].isnull() ) \\\n        & \\\n       ((test['TotalBsmtSF'] != 0) & \\\n        test['TotalBsmtSF'].notnull())\n\ntest[mask][np.concatenate([bsmt_features_1,['TotalBsmtSF']])]","2cd2b69f":"#fill missing data\nfor feature in bsmt_features_1:\n    train.loc[mask & train[feature].isnull(), feature] = train[feature].mode()[0]\n    train[feature].fillna('None', inplace=True)\n    \n    test.loc[mask & test[feature].isnull(), feature] = test[feature].mode()[0]\n    test[feature].fillna('None', inplace=True)","f0305492":"#Train dataset\n#check if there are missing values in the numerical features, but the BsmtQual != 'None'\nbsmt_features_2 = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']\n\nmask = (train['BsmtFinSF1'].isnull() | \\\n        train['BsmtFinSF2'].isnull() | \\\n        train['BsmtUnfSF'].isnull() | \\\n        train['TotalBsmtSF'].isnull() | \\\n        train['BsmtFullBath'].isnull() | \\\n        train['BsmtHalfBath'].isnull() ) \\\n        & \\\n       (train['BsmtQual'] != 'None')\n\ntrain[mask][np.concatenate([bsmt_features_2,['BsmtQual']])]","08d84ebb":"#Test dataset\n#check if there are missing values in the numerical features, but the BsmtQual != 'None'\nmask = (test['BsmtFinSF1'].isnull() | \\\n        test['BsmtFinSF2'].isnull() | \\\n        test['BsmtUnfSF'].isnull() | \\\n        test['TotalBsmtSF'].isnull() | \\\n        test['BsmtFullBath'].isnull() | \\\n        test['BsmtHalfBath'].isnull() ) \\\n        & \\\n       (test['BsmtQual'] != 'None')\n\ntest[mask][np.concatenate([bsmt_features_2,['BsmtQual']])]","fd106f0c":"#fill missing data with 0\nfor feature in bsmt_features_2:\n    train[feature].fillna(0, inplace=True)\n    test[feature].fillna(0, inplace=True)","93362032":"#Train dataset\n#check if there are missing values in the categorical features, but the GarageCars or GarageArea != 0\ngarage_features_1 = ['GarageType','GarageFinish','GarageQual','GarageCond']\n\nmask = (train['GarageType'].isnull() | \\\n        train['GarageFinish'].isnull() | \\\n        train['GarageQual'].isnull() | \\\n        train['GarageCond'].isnull()) \\\n        & \\\n       ((train['GarageCars'] != 0) | \\\n        (train['GarageArea'] !=0)) \n\ntrain[mask][np.concatenate([garage_features_1,['GarageCars','GarageArea']])]","193d3601":"#Test dataset\n#check if there are missing values in the categorical features, but the GarageCars or GarageArea != 0\ngarage_features_1 = ['GarageType','GarageFinish','GarageQual','GarageCond']\n\nmask = (test['GarageType'].isnull() | \\\n        test['GarageFinish'].isnull() | \\\n        test['GarageQual'].isnull() | \\\n        test['GarageCond'].isnull()) \\\n        & \\\n       ((test['GarageCars'] != 0) | \\\n        (test['GarageArea'] !=0)) \n\ntest[mask][np.concatenate([garage_features_1,['GarageCars','GarageArea']])]","2646ecd1":"#fill missing data\nfor feature in garage_features_1:\n    train[feature].fillna('None', inplace=True)\n    \n    test.loc[mask & test[feature].isnull(), feature] = test[feature].mode()[0]\n    test[feature].fillna('None', inplace=True)","de2db9a0":"#Train dataset\n#check if there are missing values in the numerical features, but the GarageType != 'None'\ngarage_features_2 = ['GarageYrBlt','GarageCars','GarageArea']\n\nmask = (train['GarageYrBlt'].isnull() | \\\n        train['GarageCars'].isnull() | \\\n        train['GarageArea'].isnull() ) \\\n        & \\\n       (train['GarageType'] != 'None')\n\ntrain[mask][np.concatenate([garage_features_2,['GarageType']])]","54fa627c":"#Test dataset\n#check if there are missing values in the numerical features, but the GarageType != 'None'\ngarage_features_2 = ['GarageYrBlt','GarageCars','GarageArea']\n\nmask = (test['GarageYrBlt'].isnull() | \\\n        test['GarageCars'].isnull() | \\\n        test['GarageArea'].isnull() ) \\\n        & \\\n       (test['GarageType'] != 'None')\n\ntest[mask][np.concatenate([garage_features_2,['GarageType']])]","481b2d0e":"test.loc[mask, ['GarageYrBlt','YearBuilt']]","6f163e35":"#fill above missing data as same as YearBuilt\ntest.loc[mask, 'GarageYrBlt'] = test.loc[mask, 'YearBuilt'].astype('float64')\n\n#fill the rest with 0\ntest['GarageYrBlt'].fillna(0, inplace=True)\ntrain['GarageYrBlt'].fillna(0, inplace=True)","09a1fc8d":"#fill missing data with mode for Detchd GarageType\ntest['GarageCars'].fillna(test.groupby('GarageType')['GarageCars'].agg(pd.Series.mode)['Detchd'], inplace=True)\n\n#fill missing data with ean for Detchd GarageType\ntest['GarageArea'].fillna(test.groupby('GarageType')['GarageArea'].mean()['Detchd'], inplace=True)","eaa5544a":"#check for any missing data\nprint('missing data in the train dataset : ', train.isnull().any().sum())\nprint('missing data in the test dataset : ', test.isnull().any().sum())","9e7dfdd4":"#train_copy = train.copy()\n#test_copy = test.copy()","989d77c1":"datasets = [train, test]\n\nfor dataset in datasets:\n    #dataset['house_age'] = ((dataset['YrSold'] - dataset['YearRemodAdd']) + (dataset['MoSold'] \/ 12)) \/ 10\n    dataset['TotalSF'] = (dataset['TotalBsmtSF'] \\\n                       + dataset['1stFlrSF'] \\\n                       + dataset['2ndFlrSF'])\n    \n    dataset['total_bathrooms'] = dataset['BsmtFullBath'] + dataset['FullBath'] + \\\n                                 (0.5 * (dataset['BsmtHalfBath'] +  dataset['HalfBath']))\n    \n    dataset['Total_porch_sf'] = (dataset['OpenPorchSF'] \\\n                              + dataset['3SsnPorch'] \\\n                              + dataset['EnclosedPorch'] \\\n                              + dataset['ScreenPorch'] \\\n                              + dataset['WoodDeckSF'])\n    \n    dataset['has_been_remod'] = (dataset['YearBuilt'] != dataset['YearRemodAdd'])\n    dataset['has_garage'] = dataset['GarageType'] != 'None'\n    dataset['has_basement'] = dataset['BsmtQual'] != 'None'\n    dataset['has_2ndFloor'] = dataset['2ndFlrSF'] > 0\n    dataset['has_fireplace'] = dataset['Fireplaces'] > 0\n    dataset['has_pool'] = dataset['PoolArea'] > 0\n    dataset['has_fence'] = dataset['Fence'] != 'None'","f9401f69":"#define a normality test function\ndef normalityTest(data, alpha=0.05):\n    \"\"\"data (array)   : The array containing the sample to be tested.\n\t   alpha (float)  : Significance level.\n\t   return True if data is normal distributed\"\"\"\n    \n    from scipy import stats\n    \n    statistic, p_value = stats.normaltest(data)\n    \n    #null hypothesis: array comes from a normal distribution\n    if p_value < alpha:  \n        #The null hypothesis can be rejected\n        is_normal_dist = False\n    else:\n        #The null hypothesis cannot be rejected\n        is_normal_dist = True\n    \n    return is_normal_dist","302d50f0":"#check normality of all numericaal features and transform it if not normal distributed\nfor feature in train.columns:\n    if (train[feature].dtype != 'object'):\n        if normalityTest(train[feature]) == False:\n            train[feature] = np.log1p(train[feature])\n            test[feature] = np.log1p(test[feature])","4867d197":"#check distribution after transformation\nsns.distplot(train['GrLivArea']);","4c2c8f89":"#combine the train and the test datasets\ntrain['Source'] = 'train'\ntest['Source'] = 'test'\n\ncombined_data = pd.concat([train, test], ignore_index=True)\n\nprint(train.shape, test.shape, combined_data.shape)","3f507dbb":"#create dummies\ncombined_data = pd.get_dummies(combined_data, drop_first=True)\n\ncombined_data.shape","e9752057":"X = combined_data[combined_data['Source_train'] == 1].copy()\nX.drop(['Source_train'], axis=1, inplace=True)\n\ny = target_array\n\nX_predict = combined_data[combined_data['Source_train'] == 0].copy()\nX_predict.drop(['Source_train'], axis=1, inplace=True)","424aa0ce":"for i in X.columns:\n    counts = X[i].value_counts()\n    print (counts)","4dd41941":"def overfit_zeros(df, limit=99.95):\n    \"\"\"df (dataframe)  : data\n       limit (float)   : limit to be called overfitted\n       Returns a list of features that have redundant zeroes and caused overfitting.\n    \"\"\"\n    overfit = []\n    \n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df) * 100 > limit:\n            overfit.append(i)\n            \n    overfit = list(overfit)\n    \n    return overfit","cc657ade":"#drop overfitted features\nfeatures_overfitted_train = overfit_zeros(X)\n\nX.drop(features_overfitted_train, axis=1, inplace=True)\nX_predict.drop(features_overfitted_train, axis=1, inplace=True)","c71034d3":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .1, random_state = 0)","51275a05":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nmodel_linear_reg = LinearRegression()\nmodel_GBReg = GradientBoostingRegressor()","205da405":"#search grid for optimal parameters\nfrom sklearn.model_selection import GridSearchCV\n\nlinear_reg_param_grid = {'normalize': [True, False],\n                         'n_jobs': [None, -1]}\n\nGBReg_param_grid = {'n_estimators' : [3000],\n                    'learning_rate' : [0.05],\n                    'max_depth' : [4],\n                    'max_features' : ['sqrt'],\n                    'min_samples_leaf' : [15],\n                    'min_samples_split' : [10],\n                    'loss' : ['huber'],\n                    'random_state' : [42]}\n\ngrid_linear_reg = GridSearchCV(model_linear_reg, linear_reg_param_grid, cv=5)\ngrid_GBReg = GridSearchCV(model_GBReg, GBReg_param_grid, cv=5)\n\ngrid_linear_reg.fit(X_train, y_train)\ngrid_GBReg.fit(X_train, y_train)\n\n#print(grid.best_params_)\n#print(grid.best_score_)","36004000":"#calculate Mean Squared Error\nfrom sklearn.metrics import mean_squared_error\n\nprint('MSE linear regression: ', mean_squared_error(y_test, grid_linear_reg.best_estimator_.predict(X_test)))\nprint('MSE Gradient Boosting Regressor : ', mean_squared_error(y_test, grid_GBReg.best_estimator_.predict(X_test)))","545f70a3":"#use the best model\nmodel = grid_GBReg.best_estimator_\ny_predict = model.predict(X_predict)\n\n#transform the values back\ny_predict = np.expm1(y_predict)","ec09a0c8":"#sava results to a file\nresults = pd.DataFrame({'Id': test_id, 'SalePrice': y_predict})\nresults.to_csv('my_submission.csv', index=False)","278a35dd":"- LotFrontage Feature","5782c0b5":"# Read The Data","ebcfb332":"# Creating a Model","423a7480":"* Creating new features","0f87fe78":"### Correlation of all the train features with target variable","c74e64ba":"### Target Array\ncheck distribution of target array","8ccc71e4":"### Basement Features\n- Categorical features","00b32d15":"# Exploratory Data Analysis","b7117d59":"- Normality test","045a3e09":"- GarageCars and GarageArea","6f3bb6f0":"* Check for overfitting","f6a29e0a":"### Exploratory Data Analysis: Epilogue\n- Check for any missing data","982d6a23":"### Drop Features","dc646e8a":"### Fill Missing Data: Special Treatment\n- MSZoning Feature","0906804b":"**Quick observations on the correlation:**\n* From the plots above, I think the residuals are not pure random fluctuations around the true line, which is bad news. Let's plot residual to make sure.","672614d4":"Check for missing values","53792594":"### Quick observations on the combined data\n* Total houses: 2919\n* Feature that can be dropped from training immediately:\n    * **SalePrice:** Target array.\n    * **Id**\n* There is feature that given in numerical but contain categorical variables. Therefore, we have to converted to categorical variables. These features are: \n    * **MSSubClass** \n* From data_description.txt, there are None value that mean house did not have that kind of feature. So, we can fill missing values with string, e.g. 'None'. These features are:\n    * **Alley** \n    * **FireplaceQu** \n    * **Fence** \n    * **MiscFeature** \n    * **MasVnrType:** Need to double check this missing values with the MasVnrArea feature. If NaN value in the MasVnrType feature but the MasVnrArea feature has non-zero value, then we must fill the missing value with type. Otherwise, NaN value means no type and we can fill missing values with string to represents no type.\n    * **PoolQC:** Need to double check this missing values with the PoolArea feature. If NaN value in the PoolQC feature but the PoolArea feature has non-zero value, then we must fill the missing value with pool quality. Otherwise, NaN value means no pool and we can fill missing values with string to represents no pool.\n* Features that have missing values:\n    * **MSZoning:** There are 4 missing values.\n    * **LotFrontage:** There are some missing data around 16.6%\n    * **Utilities:** There are 2 missing values.\n    * **Exterior1st:** There is 1 missing value.\n    * **Exterior2nd:** There is 1 missing value. But, since the Exterior2nd feature is optional, we have to investigate further.\n    * **MasVnrArea:** There are some missing data around 0.8%\n    * **Electrical:** There is 1 missing value.\n    * **KitchenQual:** There is 1 missing value.\n    * **Functional:** There are 2 missing values. Note from data_description.txt: '(Assume typical unless deductions are warranted)'\n    * **SaleType:** There is 1 missing value.\n* Features that need more deep analysis:\n    * **Bsmt++:** Features that start with Bsmt.\n    * **Garage++:** Features that start with Garage.","d9774c39":"Now, let's take a quick look at the train and test datasets to gain some initial insight.","5dc05582":"After transformed, residual plot become:","7c4d5cf2":"- MasVnrType Feature","a09fc158":"-- GarageYrBlt","d628e07c":"### Change to Categorical","39174b6a":"- Numerical feature","a6aaefd8":"# Import Libraries\nFirst, we import necessary libraries, such as:","4497ad35":"In this case, we will compare Linear Regression model and Gradient Boosting Regressor to get the smallest Mean Squared Error (MSE). We begin by splitting data into two subsets: for training data and for testing data.","28370a7b":"### Fill Missing Data : With Mode","19d18816":"# Import The Data","acba3ce5":"From the graph above we can see that distribution of target array is not following a normal distribution. We can transform it using numpy.log1p, so target array follows a normal distribution. ","e85bb55c":"### Garage Features\n- Categorical feature","320a66c2":"This kernel is going to solve [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview), a popular competition on Kaggle. This competition's dataset can be downloaded from the following [link](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data)\n\n**Competition Description:**\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","c76d63d6":"- MasVnrArea Feature","d62cc8fb":"Plot some top of the most correlated one.","21db58b1":"- PoolQC Feature","5b100977":"* The residuals plot shows that as GrLivArea value increases, the variance also increases, which is the characteristics known as Heteroscedasticity.\n* Get rid of outliers?\n* Need to check distirbution of target array and all training features.","8cb2e417":"* Creating dummies","62cc8921":"- Numerical feature","f4ddfb52":"* Creating features matrix (X) and target array (y)","d52d634e":"### (Some) 'None' Values According to data_description.txt"}}