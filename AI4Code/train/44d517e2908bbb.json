{"cell_type":{"a79ba0e2":"code","c6a0a3f5":"code","dc5e2865":"code","96749ad3":"code","cc804559":"code","0f932632":"code","21fcbaa9":"code","f184d931":"code","b9967a23":"code","948b0b6c":"code","93736e3a":"code","50c8333c":"code","adab1993":"code","26de3f7d":"code","f834ae78":"code","ae3a770d":"code","021c3564":"code","39acd869":"code","a277b545":"code","cb50624d":"code","e83743e0":"code","41ffe4e9":"code","49e4c610":"code","de86b614":"code","950f5f6c":"code","c142e3c9":"code","8b992224":"code","aac75be0":"code","18a677c6":"code","fc3f1596":"code","510efc9a":"code","bdec16d4":"code","93719b14":"code","fae59cbf":"code","abaf8f0a":"code","37145bf5":"code","2ba0f485":"code","0e0c53d7":"code","245d89dc":"code","c9a5741a":"code","ce48c881":"code","7998c8f3":"code","545ba224":"code","d68a18bf":"code","f9f1d5df":"code","40470ce1":"code","f60ae16d":"code","2b9b1d77":"code","ff9b52b4":"code","630500a5":"markdown","16e7dd35":"markdown","68ab32d9":"markdown","6e4dc2cf":"markdown","255b5870":"markdown","7b0fdc0c":"markdown","5a18aac5":"markdown","5c001ce1":"markdown","cf7d91c7":"markdown","77b35604":"markdown","43deb68f":"markdown","8b491ebf":"markdown","74b4c556":"markdown","c0acf125":"markdown","1f3dbfdb":"markdown","871b0083":"markdown","ae9a218d":"markdown"},"source":{"a79ba0e2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","c6a0a3f5":"# Loading train and test datasets\nTrain = pd.read_csv('..\/input\/titanic\/train.csv')\nTest = pd.read_csv('..\/input\/titanic\/test.csv')","dc5e2865":"#First five row\nTrain.head()","96749ad3":"#First five row\nTest.head()","cc804559":"Train.describe()","0f932632":"Test.describe()","21fcbaa9":"Train.info()","f184d931":"Train.shape","b9967a23":"#calculate the null values\nprint('Train Columns with null values : ', Train.isna().sum())\nprint('Test Columns with null values : ', Test.isna().sum())","948b0b6c":"data1=pd.concat([Train,Test],axis=0)","93736e3a":"# correlation matix\ng = sns.heatmap(data1[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True )","50c8333c":"Train.Survived.value_counts()","adab1993":"#Survived vs Sex\nTrain.groupby(Train.Sex).Survived.value_counts().unstack().plot(kind= 'bar')","26de3f7d":"#Survived vs Pclass\nprint(Train.groupby(Train.Survived).Pclass.value_counts())\ngrid = sns.FacetGrid(Train, col='Survived', row='Pclass', size=2, aspect=1.8)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)","f834ae78":"#Survived vs Parch\nTrain.groupby(Train.Parch).Survived.value_counts().unstack().plot(kind= 'bar')","ae3a770d":"#Survived vs Sibsp\nTrain.groupby(Train.SibSp).Survived.value_counts().unstack().plot(kind= 'bar')","021c3564":"#Survived vs Embarked\nsns.barplot('Embarked', 'Survived', data=Train, color=\"teal\")","39acd869":"train_test = [Train,Test]","a277b545":"for dataset in train_test:\n    dataset['Title'] = dataset[\"Name\"].str.extract('([A-Za-z]+)\\.', expand=False )","cb50624d":"Train.head()","e83743e0":"for dataset in train_test:\n    dataset['Title'] =dataset['Title'].map({'Mr':0,'Miss':1,'Mrs':2,'Master':0,'Dr':3,'Rev':3,'Mlle':3,'Col':3,'Major':3,'Mme':3,'Ms':3,'Don':3,'Capt':3,'Jonkheer':3,'Countess':3,'Lady':3,'Sir':3})","41ffe4e9":"Train.head()","49e4c610":"Train.groupby(Train.Title).Survived.value_counts().unstack().plot.bar() ","de86b614":"print(Train.Embarked.value_counts(dropna=False))","950f5f6c":"# filling missing values\nfor dataset in train_test:\n    dataset.Embarked.fillna('S', inplace = True)\n# I will impute the one missing value for fare with median    \nfor dataset in train_test:\n    dataset['Fare'] = dataset['Fare'].fillna(Train['Fare'].median())","c142e3c9":"for dataset in train_test:\n    dataset[\"Embarked\"]= dataset.Embarked.map({'S':0,'C':1,'Q':2})","8b992224":"ag = Train[\"Age\"].hist(bins=15, color='teal', alpha=0.8)\nag.set(xlabel='Age', ylabel='Count')","aac75be0":"Train.groupby(pd.cut(Train.Age, bins=[0,25,50,75,100], labels= [0,1,2,3])).Survived.value_counts().plot(kind= 'bar', stacked = True)","18a677c6":"for dataset in train_test:\n    dataset['Age'].fillna(dataset.groupby('Title')['Age'].transform('median'),inplace = True)","fc3f1596":"# we map  age according to age band\nfor dataset in train_test:\n    dataset['Age_Range']=pd.cut(dataset.Age, bins = (0,25,50,75,100), labels = [0,1,2,3])    \n    ","510efc9a":"#family_size = number of family members, people travelling alone will have a value of 1\nfor dataset in train_test:\n    dataset['Family_size'] = dataset['Parch'] + dataset['SibSp']+1","bdec16d4":"#making it in binary \nfor dataset in train_test:\n    dataset['Sex'] = dataset['Sex'].map({'male':0,'female':1})","93719b14":"Train.head()","fae59cbf":"Train.isna().sum()","abaf8f0a":"Test.isna().sum()","37145bf5":"Test.Title.fillna(0,inplace=True)","2ba0f485":"Test.isna().sum()","0e0c53d7":"# drop the rows\nfor dataset in train_test:\n    dataset.drop(['Name','Ticket','Cabin','Age'], axis = 1, inplace = True)","245d89dc":"Train.head()","c9a5741a":"# Machine learning\nfrom sklearn.model_selection import train_test_split #for split the data\nfrom sklearn.metrics import accuracy_score  #for accuracy_score\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nx = Train.drop(['PassengerId',\"Survived\"],axis=1)\ny= Train[\"Survived\"]\nX_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1000)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","ce48c881":"# Kneighbor classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=11)\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\naccuracy_knn = round(accuracy_score(pred,y_test)*100,2)\nprint(\"The accuracy score of the KNeighborsClassifier : \",accuracy_knn)\nkfold = KFold(n_splits=10,random_state=42)\ncvs_knn = cross_val_score(knn,x,y,cv =10, scoring='accuracy')\nprint('The cross_val_score of the KNeighborsClassifier : ',round(cvs_knn.max()*100,2))\n","7998c8f3":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlg =LogisticRegression()\nlg.fit(X_train,y_train)\npred = lg.predict(X_test)\naccuracy_lr =round(accuracy_score(pred,y_test)*100,2)\nprint(\"The accuracy score of the LogisticRegression : \",accuracy_lr)\nkfold = KFold(n_splits=10,random_state=22)\ncvs_lr = cross_val_score(lg,x,y,cv =10, scoring='accuracy')\nprint('The cross_val_score of the LogisticRegression : ',round(cvs_lr.mean()*100,2))\n\n\n","545ba224":"#  Gaussian naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train,y_train)\npred = nb.predict(X_test)\naccuracy_nb = round(accuracy_score(pred,y_test)*100,2)\nprint(\"The accuracy score of the Gaussian naive bayes classifier : \",accuracy_nb)\nkfold = KFold(n_splits=10,random_state=22)\ncvs_nb = cross_val_score(nb,x,y,cv =10, scoring='accuracy')\nprint('The cross_val_score of the Gaussian naive bayes classifier : ',round(cvs_nb.mean()*100,2))\n","d68a18bf":"# Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\ndc = DecisionTreeClassifier(criterion='gini', \n                             min_samples_split=10,min_samples_leaf=1,\n                             max_features='auto')\ndc.fit(X_train,y_train)\npred = dc.predict(X_test)\naccuracy_dc =round(accuracy_score(pred,y_test)*100,2)\nprint(\"The accuracy score of the DecisionTreeClassifier : \",accuracy_dc)\nkfold = KFold(n_splits=10,random_state=22)\ncvs_dc = cross_val_score(dc,x,y,cv =10, scoring='accuracy')\nprint('The cross_val_score of the DecisionTreeClassifier : ',round(cvs_dc.mean()*100,2))\n\n\n","f9f1d5df":"# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrm = RandomForestClassifier(criterion='gini', n_estimators=500,\n                             min_samples_split=10,min_samples_leaf=1,\n                             max_features='auto',oob_score=True,\n                             random_state=1,n_jobs=-1)\nrm.fit(X_train,y_train)\npred = rm.predict(X_test)\naccuracy_rm=round(accuracy_score(pred,y_test)*100,2)\nprint(\"The accuracy score of the RandomForestClassifier : \",accuracy_rm)\nkfold = KFold(n_splits=10,random_state=22)\ncvs_rm = cross_val_score(rm,x,y,cv =10, scoring='accuracy')\nprint('The cross_val_score of the RandomForestClassifier : ',round(cvs_rm.mean()*100,2))\n\n","40470ce1":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nsvc = LinearSVC()\nsvc.fit(X_train, y_train)\npred = svc.predict(X_test)\naccuracy_svc=round(accuracy_score(pred,y_test)*100,2)\nprint(\"The accuracy score of the Linear_svc : \",accuracy_svc)\nkfold = KFold(n_splits=10,random_state=22)\ncvs_svc = cross_val_score(rm,x,y,cv =10, scoring='accuracy')\nprint('The cross_val_score of the Linear_svc : ',round(cvs_svc.mean()*100,2))","f60ae16d":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\npred = gbc.predict(X_test)\naccuracy_gbc=round(accuracy_score(pred,y_test)*100,2)\nprint(\"The accuracy score of the RandomForestClassifier : \",accuracy_gbc)\nkfold = KFold(n_splits=10,random_state=22)\ncvs_gbc = cross_val_score(rm,x,y,cv =10, scoring='accuracy')\nprint('The cross_val_score of the RandomForestClassifier : ',round(cvs_gbc.mean()*100,2))","2b9b1d77":"models = pd.DataFrame({\n    'Model': [ 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes',  \n              'Decision Tree','GradientBoostingClassifier','LinearSVC'],\n    'Score': [ accuracy_knn, accuracy_lr, \n              accuracy_nb, accuracy_dc,accuracy_rm,accuracy_gbc,accuracy_svc]})\nmodels.sort_values(by='Score',ascending=False)","ff9b52b4":"ids = Test['PassengerId']\n\npred = dc.predict(Test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': pred })\noutput.to_csv('submission.csv', index=False)","630500a5":"# 2. Data Analysis","16e7dd35":" people with less than 4 parents and childreen aboard were more likely to survive","68ab32d9":"Brief information about columns of datasets :\n1.  PassengerId - The unique id of the row.\n2.  Survived - \n        1 -> Survived \n        0 -> not survived\n3.  Pclass - Ecconomic status of passenger\n        1 -> Upper class\n        2 -> Middle class\n        3 -> Lower class\n4.  Name - Name of passenger.\n5.  Sex - Male and Female.\n6.  Age - Age of passenger.\n7.  SibSp - total number of sibling and spouse travelling with each passenger.\n8.  Parch - total number of passenger's parents and children\n9.  Ticket - Ticket number\n10. Fare - How much money the passenger paid for journey\n11. Cabin - cabin number\n12. Embarked - port from where passenger was boarded.\n         C -> Cherbourg\n         Q -> Queenstown \n         S -> Southampton\n    \n\n","6e4dc2cf":"We fill the null values of Age with the median age values in particular groups due to the titles.","255b5870":"# 3.Data Visualization","7b0fdc0c":"People with less siblings or spouses aboard were more likely to survive","5a18aac5":"People with higher economic class had a higher rate of survival","5c001ce1":"# 6.Submission file\nNow,its time to create the submission file and upload them on kaggle\n","cf7d91c7":"Decission tree gives the best result so,I decided to use the Decission tree model for the testing data.","77b35604":"# 1.Import libraries and data","43deb68f":"# 5.Choosing the Best Model","8b491ebf":"Correlations between numerical variables and Survived aren't so high.","74b4c556":"By far the most passenger boarded in southampton.so,we will impute those two nan value with most popular value 'S'.","c0acf125":"Contents:\nImport Libraries and data\nData Analysis\nData Visualization\nCleaning Data\nChoosing the Best Model\nSubmission File","1f3dbfdb":"# 4.Cleaning Data","871b0083":"People with 'Master' have the highest survival rate.","ae9a218d":"Females have higher chance of survival than males"}}