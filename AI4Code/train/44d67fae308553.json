{"cell_type":{"d39052ac":"code","3f6c461f":"code","eda6bb4e":"code","b33189cc":"code","e386f082":"code","a134673d":"code","382ab107":"code","c04106f3":"code","392b0677":"code","15b38b1b":"code","3d981974":"code","7ab9906b":"code","ed1e5df3":"code","e6f9afa9":"code","5d25728b":"code","13bfcf2e":"code","bc9ab03b":"code","06bbc186":"code","b9b37cb7":"code","51330ccc":"code","8a96b02c":"code","f851faef":"code","7a18389b":"code","10a20b2f":"code","4d7d9944":"code","53d953d2":"code","8f5e4990":"code","dafa81f9":"code","15ce6256":"code","6b797074":"code","e85ded41":"code","e8abf87e":"code","cd734027":"code","18ccd942":"code","f549ab88":"code","9bec1a37":"code","e49419da":"code","62644855":"markdown","0cecfcbe":"markdown","82fa187e":"markdown","39adf7d7":"markdown","e54ce610":"markdown","73732d40":"markdown","3354f472":"markdown","00ae1e2c":"markdown","8960247c":"markdown","bf2cd2a5":"markdown"},"source":{"d39052ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\npd.set_option('display.max_columns', None)","3f6c461f":"# Data Processing\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","eda6bb4e":"#checking the data for null values\ntrain_data.info()","b33189cc":"print(\"The size of train data is\", len(train_data))\nprint(\"The total features in train data is\", len(train_data.columns)-2)","e386f082":"print(\"The columns in train data are\", train_data.columns.values )","a134673d":"train_data.describe()","382ab107":"#The Survived Column is the target Column for the train Data\ntrain_data['Survived'].value_counts()","c04106f3":"# Most of the survived members are childrens and women\nsns.swarmplot(data=train_data, x='Age', y='Sex', hue=\"Survived\")","392b0677":"sns.violinplot(x=\"Pclass\", y=\"Age\", data=train_data)","15b38b1b":"sns.lineplot(data=train_data, x='Age', y='Fare')","3d981974":"#Setting the categorical feature type as Object\ncat_features = ['Pclass', 'SibSp', 'Parch', 'Embarked']\ntrain_data[cat_features] = train_data[cat_features].astype('O')\ntest_data[cat_features] = test_data[cat_features].astype('O')","7ab9906b":"features = [\"Sex\", \"Pclass\", \"SibSp\", \"Parch\", \"Age\", 'Embarked']\ny = train_data[\"Survived\"]\nX = train_data[features]\nX_test = test_data[features]\nX.head()","ed1e5df3":"#Converting it into the One Hot Encoding\nX = pd.get_dummies(X)\nX_test = pd.get_dummies(X_test)","e6f9afa9":"X_test.drop(columns=['Parch_9'], inplace=True)","5d25728b":"from sklearn.impute import SimpleImputer\nsi = SimpleImputer()\nX_imp = si.fit_transform(X)\nX_test_imp  = si.transform(X_test)","13bfcf2e":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=2)","bc9ab03b":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit(X_imp)\ncumsum = np.cumsum(pca.explained_variance_ratio_)*100\nd = [n for n in range(len(cumsum))]\nplt.figure(figsize=(10, 6))\nplt.plot(d, cumsum, color = 'red',label='cumulative explained variance')\nplt.grid(axis='both')\nplt.title('Cumulative Explained Variance as a Function of the Number of Components')\nplt.ylabel('Cumulative Explained variance')\nplt.xlabel('Principal components')\nplt.axhline(y = 95, color='k', linestyle='--', label = '95% Explained Variance')\nplt.legend(loc='best')","06bbc186":"#PCA\npca = PCA(n_components = 10,random_state=42)\nX_pca = pca.fit_transform(X_imp)\nX_test_pca = pca.transform(X_test_imp)","b9b37cb7":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\nsclr_pca = MinMaxScaler()\nX_min_pca = sclr_pca.fit_transform(X_pca)\nX_test_min_pca = sclr_pca.transform(X_test_pca)","51330ccc":"sclr = MinMaxScaler()\nX_min = sclr.fit_transform(X_imp)\nX_test_min = sclr.transform(X_test_imp)","8a96b02c":"from sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom xgboost.sklearn import XGBClassifier","f851faef":"from tensorflow import keras\nfrom tensorflow.keras import layers","7a18389b":"from sklearn.model_selection import train_test_split\nX_train_pca, X_valid_pca, y_train_pca, y_valid_pca = train_test_split(X_min, keras.utils.to_categorical(y), test_size=0.20, random_state=42)","10a20b2f":"X_train_min, X_valid_min, y_train_min, y_valid_min = train_test_split(X_min, keras.utils.to_categorical(y), test_size=0.20, random_state=42)","4d7d9944":"model = keras.Sequential()\nmodel.add(layers.Dense(500, activation='relu', input_dim=X_train_min.shape[1]))\nmodel.add(layers.Dense(100, activation='relu'))\nmodel.add(layers.Dense(50, activation='relu'))\nmodel.add(layers.Dense(2, activation='softmax'))\nmodel.summary()","53d953d2":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=[keras.metrics.BinaryAccuracy()])","8f5e4990":"history = model.fit(X_train_min, y_train_min, epochs=20, validation_data=(X_valid_min, y_valid_min), verbose=1, batch_size=32)","dafa81f9":"plt.figure()\nep = list(range(1, 21))\nsns.lineplot(x=ep, y= history.history['loss'])\nsns.lineplot(x=ep, y=history.history['val_loss'])\n","15ce6256":"mdl_sqn = model.predict(X_test_min)","6b797074":"mdl_sqn","e85ded41":"sqn_pred =  (mdl_sqn[:, 0] <  mdl_sqn[:, 1]).astype(int)","e8abf87e":"params = {\n    'degree' : list(range(1, 10)), \n    'gamma': ['scale', 'auto'],\n    'kernel': ('linear', 'poly', 'rbf', 'sigmoid'),\n    'class_weight' : ('balanced', None),\n    'C': list(range(1, 5))\n}\ngsearch = GridSearchCV(svm.SVC(random_state=42),\n                       n_jobs=5,\n                       cv = KFold(random_state=42, shuffle=True, n_splits=10), \n                       param_grid=params, verbose=3, \n                       scoring='accuracy')","cd734027":"#svc_mdl = gsearch.fit(X_min, y)","18ccd942":"#svc_mdl.best_params_, svc_mdl.best_score_#","f549ab88":"params = {\n    'n_estimators' : list(range(50, 1000, 50)), \n    'max_depth': list(range(1, 10)),\n    'min_child_weight':[4,5,6],\n    'gamma':[i\/10.0 for i in range(0,5)],\n    'n_jobs':[5],\n    'subsample':[i\/10.0 for i in range(6,10)],\n    'colsample_bytree':[i\/10.0 for i in range(6,10)]\n}\nmdl = XGBClassifier(objective= 'binary:logistic', random_state=42, tree_method='gpu_hist')\ngsearch_forest = GridSearchCV(mdl ,\n                           cv = KFold(random_state=42, shuffle=True, n_splits=10), \n                           param_grid=params,\n                           verbose=1, \n                           scoring='roc_auc')","9bec1a37":"#gsearch_forest.fit(X_min, y)","e49419da":"clf_output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': sqn_pred})\nclf_output.to_csv('titanic_submission.csv', index=False)","62644855":"# MODELS","0cecfcbe":"**BASIC EDA**","82fa187e":"**IMPUTERS**","39adf7d7":"**PCA**","e54ce610":"**Scaler**","73732d40":"# Graphs","3354f472":"# Preprocessing","00ae1e2c":"<h2> SVM<h2>","8960247c":"**Forest Tree Classifier**","bf2cd2a5":"<h2> Exporting Output <h2>"}}