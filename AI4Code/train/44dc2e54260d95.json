{"cell_type":{"4668bc46":"code","a01431a1":"code","5150e365":"code","c90e83e8":"code","a7577bb7":"code","d8a35af5":"code","b70f2911":"code","648adccf":"code","3a37075b":"code","553c3aa4":"code","bad3da35":"code","e4f4acc4":"code","efcce4c1":"code","49cab371":"markdown","3af1b8a1":"markdown","e8497f49":"markdown","456281b6":"markdown","210351cd":"markdown","97ae8d51":"markdown","976ad45b":"markdown","5585f609":"markdown"},"source":{"4668bc46":"import os\nfrom datetime import datetime\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a01431a1":"train_data = pd.read_json(\"\/kaggle\/input\/meli-data-challenge-2020\/train_dataset.jl\", lines=True, orient='columns')\ntrain_data.head(2)","5150e365":"# Adapted from https:\/\/github.com\/Santivg\/ml-challenge\n\ndef preprocess_hist(df):\n    df['user_view']= pd.Series(dtype='object')\n    df['timestamps']= pd.Series(dtype='object')\n    df['user_search']= pd.Series(dtype='object')\n    df['search_timestamps']= pd.Series(dtype='object')\n\n    for i in df.index:\n        lista_view=[]\n        lista_time=[]\n        lista_search=[]\n        lista_search_t=[]\n        for item in df.user_history[i]:\n            if item['event_type'] =='view':\n                lista_view.append(int(item['event_info']))\n                time_string=item['event_timestamp'].replace(\"T\", \" \").split('.')[0]\n                timestamp=datetime.timestamp(datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S'))\n                lista_time.append(int(timestamp))\n            if item['event_type'] =='search':\n                lista_search.append(item['event_info'])\n                time_string=item['event_timestamp'].replace(\"T\", \" \").split('.')[0]\n                timestamp=datetime.timestamp(datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S'))\n                lista_search_t.append(int(timestamp))\n\n        df.at[i,'user_view']= lista_view\n        df.at[i,'timestamps']= lista_time\n\n        df.at[i,'user_search']= lista_search\n        df.at[i,'search_timestamps']= lista_search_t\n    return df","c90e83e8":"# train part\ntrain_data = preprocess_hist(train_data)\ntrain_data.drop('user_history', axis=1, inplace=True)\ntrain_data.to_pickle('train.pickle')\ntrain_data.head(4)","a7577bb7":"# same for the test set\ntest_data = pd.read_json(\"\/kaggle\/input\/meli-data-challenge-2020\/test_dataset.jl\", lines=True, orient='columns')\ntest_data = preprocess_hist(test_data)\ntest_data.drop('user_history', axis=1, inplace=True)\ntest_data.to_pickle('test.pickle')\ntest_data.head(2)","d8a35af5":"print(train_data.shape, test_data.shape)","b70f2911":"item_data = pd.read_json(\"\/kaggle\/input\/meli-data-challenge-2020\/item_data.jl\", lines=True, orient='columns')\nitem_data.to_pickle('products.pickle')\nitem_data.head(2)","648adccf":"rename_dict = {\n    'title': 'bought_title', 'domain_id': 'bought_domain_id', 'price': 'bought_price',\n    'category_id': 'bought_category_id', 'condition': 'bought_condition'\n}\n\n# remove\/change two columns from item data\nitem_data['condition'] = item_data.condition.map({'new': 0, 'used': 1, 'not_specified': 2})\nitem_data.drop('product_id', axis=1, inplace=True)\n\n# merge on item_bought id\ntrain_data = train_data.merge(item_data, left_on='item_bought', right_on='item_id', how='left')\ntrain_data = train_data.drop('item_id', axis=1).rename(rename_dict, axis=1)\ntrain_data.head(2)","3a37075b":"def melt_views(df):\n    views = df[[c for c in df.columns if c != 'user_search' and c != 'search_timestamps']]\n    views = views.reset_index().rename({'index': 'row_id'}, axis=1)\n\n    views = views.set_index(['row_id']).apply(pd.Series.explode).reset_index()\n    views = views[~views.user_view.isna()]  # remove purchases with no previous views\n    return views.merge(item_data, left_on='user_view', right_on='item_id', how='left').drop('item_id', axis=1)\n\nviews_data = melt_views(train_data)\nviews_data.head(3)","553c3aa4":"views_data.to_pickle('train_views.pickle')","bad3da35":"print(\"Corpus size (number of itens):\", item_data.item_id.nunique())\nprint(\"Number of domains:\", item_data.domain_id.nunique())\nprint(\"Number of categories:\", item_data.category_id.nunique())\nprint(\"\\nTop domains:\")\nprint(item_data.domain_id.value_counts().head())","e4f4acc4":"items_count = views_data.groupby('row_id')['user_view'].count()\nplt.figure(figsize=(10, 4))\nplt.title(\"Views distribution number for user\")\np = sns.distplot(items_count, bins=100)","efcce4c1":"views_data['same_item'] = (views_data.item_bought == views_data.user_view).astype('int8')\nsame_item = views_data.groupby('row_id')['same_item'].max()\nplt.title(\"Has seen the item before buying it? (item_bought is in user_view list)\")\np = sns.countplot(same_item)","49cab371":"## MeLi Data Challenge\n\nThis notebook is a quick start where we are going to preprocess and merge the original data to a more \"pandas-like\" format. Intermediate datasets will be saved to pickle files\n","3af1b8a1":"**item_bought:** ID for the purchased item\n\n**user_view:** List of IDs for each item that the user has seen in the last week\n\n**timestamps:** List of timestamps for each visualization\n\n**user_search:** List of search tokens in the last week\n\n**search_timestamps:** List of timestamps for each search","e8497f49":"## Join item_bought metadata (only available for training set)","456281b6":"## Melt data and merge user_view data\n\nMelt the list of products that each user has seen (user_view column) and merge the metadata for the product.","210351cd":"There are 2.1 million items in 7.894 domains and 11.493 categories","97ae8d51":"Hope this notebook helps as a quick start for this dataset.","976ad45b":"## Data Exploration","5585f609":"## Products"}}