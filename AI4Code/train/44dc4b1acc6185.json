{"cell_type":{"59d16afc":"code","76fcd0ea":"code","8554cbb2":"code","a044ca12":"code","c3461026":"code","3fa00726":"code","678feb11":"code","15b06c96":"code","02a8b4b5":"code","cc9ef3a8":"code","38aee1f6":"code","a41d1ba7":"code","6208f7bb":"code","bf7fc9e9":"code","d38dfdae":"code","3f0528df":"code","47884669":"code","800016c7":"code","88a7dd81":"markdown","92a17983":"markdown"},"source":{"59d16afc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport os\nimport sys\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import layers,regularizers,Sequential,backend,callbacks,optimizers,metrics,losses\nimport tensorflow_addons as tfa\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import log_loss\n\n\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","76fcd0ea":"data_dir = '..\/input\/lish-moa\/'\n\ntrain_set = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_target = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_set = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n#train_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\nss.loc[:, train_target.columns[1:]] = 0","8554cbb2":"GENES = [col for col in train_set.columns if col.startswith('g-')]\nCELLS = [col for col in train_set.columns if col.startswith('c-')]\n\ndrop_index = train_set[train_set.cp_type == 'ctl_vehicle'].index\nadd_index = test_set[test_set.cp_type == 'ctl_vehicle'].index","a044ca12":"#preprocessing\n\ndef transform_features(df):\n    #one_hot = pd.get_dummies(df['cp_dose']) \n    #df = df.join(one_hot)\n    df = df.drop(df[df.cp_type == 'ctl_vehicle'].index)  # drop where cp_type==ctl_vehicle\n    #df = df.drop(['sig_id','cp_type', 'cp_dose'], axis=1)\n    df = df.drop(['sig_id','cp_type'], axis=1)\n    df['cp_time']=df['cp_time'].map({24: 1, 48: 2, 72: 3})\n    df[\"cp_dose\"] = df['cp_dose'].map({\"D1\": 1, \"D2\": 2})\n    return df\n\n\nX_train = transform_features(train_set)\nX_test = transform_features(test_set)\n\ntrain_target = train_target.drop(drop_index, axis = 0)\ny_train = train_target.drop(['sig_id'], axis=1).values","c3461026":"#RankGauss\ndef rankgauss(D):\n    for col in (GENES+CELLS):\n        qt = preprocessing.QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        vec = D[col].values.reshape(-1,1)\n        qt.fit(vec)\n        D[col] = qt.transform(vec.reshape(-1,1))\n        return D\n                     \nX_train = rankgauss(X_train)\nX_test = rankgauss(X_test)","3fa00726":"X_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)","678feb11":"#PCA for genes\nn_comp = 600  \n\ndata = pd.concat([X_train[GENES], X_test[GENES]])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n\ntrain2 = data2[:X_train.shape[0]]\ntest2 = data2[-X_test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\nX_train = pd.concat((X_train, train2), axis=1)\nX_test = pd.concat((X_test, test2), axis=1)","15b06c96":"#PCA for cells\nn_comp = 50  \n\ndata = pd.concat([X_train[CELLS], X_test[CELLS]])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n\ntrain2 = data2[:X_train.shape[0]]\ntest2 = data2[-X_test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\nX_train = pd.concat((X_train, train2), axis=1)\nX_test = pd.concat((X_test, test2), axis=1)","02a8b4b5":"var_thresh = VarianceThreshold(0.8)  #<-- Update\ndata = X_train.append(X_test)\ndata_transformed = var_thresh.fit_transform(data)\n\nX_train = data_transformed[ : X_train.shape[0]]\nX_test = data_transformed[-X_test.shape[0] : ]","cc9ef3a8":"def logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))","38aee1f6":"n_train = X_train.shape[0]\nn_test = X_test.shape[0]\nn_features = X_train.shape[1]\nn_labels = y_train.shape[1]\n\nepsilon = 1E-3\np_min = epsilon\np_max = 1-epsilon","a41d1ba7":"#model\n\nsgd = tf.keras.optimizers.SGD()\nadamw = tfa.optimizers.AdamW(weight_decay = 0.0001)\nadam = tf.keras.optimizers.Adam()\nradam = tfa.optimizers.RectifiedAdam()\nlookahead_radam = tfa.optimizers.Lookahead(radam)\nlookahead_adamw = tfa.optimizers.Lookahead(adamw)\n\ndef create_model(n_features, opt):\n    model = Sequential([\n    layers.Input(n_features),\n    layers.BatchNormalization(),\n    layers.Dropout(0.466),\n        \n    tfa.layers.WeightNormalization(layers.Dense(1024)),\n    layers.LeakyReLU(),\n    layers.Dropout(0.466),\n    layers.BatchNormalization(),\n    \n\n    tfa.layers.WeightNormalization(layers.Dense(2048)),\n    layers.LeakyReLU(),\n    layers.Dropout(0.466),\n    layers.BatchNormalization(),\n        \n    tfa.layers.WeightNormalization(layers.Dense(2048)),\n    layers.LeakyReLU(),\n    layers.Dropout(0.466),\n    layers.BatchNormalization(),\n        \n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(n_labels, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer= opt,\n                  loss=losses.BinaryCrossentropy(label_smoothing=1E-6), metrics=logloss)\n    return model\n\n#layers.LeakyReLU()\n#optimizer = tfa.optimizers.AdamW(lr = 1e-3, weight_decay = 1e-5, clipvalue = 700)\n#loss=losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss\n#tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation = 'relu')),","6208f7bb":"n_folds = 5\nseed = 42\n\ntest_pred = np.zeros((n_test, n_labels))\noof_pred = np.zeros((n_train, n_labels))\n\nmskf = MultilabelStratifiedKFold(n_splits = n_folds, random_state = seed, shuffle = True)\n\n\nfor n, (tr, te) in enumerate(mskf.split(X_train, y_train)):\n\n    print(f'Starting fold: {n}')\n\n    model = create_model(n_features, 'adam')\n\n    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, verbose=1, mode='min',min_lr=1E-5)\n    #checkpoint = ModelCheckpoint(f'split_nn.hdf5', monitor = 'val_loss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n    early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-4, patience=10, mode='min',restore_best_weights=True)\n    model.fit(\n        X_train[tr],\n        y_train[tr],\n        validation_data=(X_train[te], y_train[te]),\n        epochs=80, \n        batch_size=128,\n        callbacks = [reduce_lr,early_stopping]\n    )\n\n    test_pred += model.predict(X_test)\/n_folds\n    oof_pred[te,:] += model.predict(X_train[te])","bf7fc9e9":"print(f'OOF log loss: {log_loss(np.ravel(y_train), np.ravel(np.clip(oof_pred, p_min, p_max)))}')","d38dfdae":"print(f'OOF log loss: {log_loss(np.ravel(y_train), np.ravel(oof_pred))}')","3f0528df":"predictions = np.clip(test_pred.copy(), p_min,p_max)\nfor pos in add_index:\n    predictions = np.insert(predictions, pos, values=np.zeros(206), axis=0)","47884669":"#submission\nss = pd.DataFrame(predictions, columns=train_target.columns[1:])\nss.insert(0,'sig_id', test_set['sig_id'].values)\nss.to_csv('submission.csv',index = False)\nss.describe()","800016c7":"ss","88a7dd81":"Inspired by \\\nhttps:\/\/www.kaggle.com\/rahulsd91\/moa-label-smoothing \\\nhttps:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/192211 \\\nhttps:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-0-01859-rankgauss-pca-nn#About-this-notebook \\\nhttps:\/\/www.kaggle.com\/gogo827jz\/hyperparameter-tuning-for-neural-network-on-tpu","92a17983":"#feature importance\ntop_feats = [  0,   1,   2,   3,   5,   6,   8,   9,  10,  11,  12,  14,  15,\n        16,  18,  19,  20,  21,  23,  24,  25,  27,  28,  29,  30,  31,\n        32,  33,  34,  35,  36,  37,  39,  40,  41,  42,  44,  45,  46,\n        48,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n        63,  64,  65,  66,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n        78,  79,  80,  81,  82,  83,  84,  86,  87,  88,  89,  90,  92,\n        93,  94,  95,  96,  97,  99, 100, 101, 103, 104, 105, 106, 107,\n       108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n       121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134,\n       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n       149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164,\n       165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 180,\n       181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n       197, 198, 199, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213,\n       214, 215, 218, 219, 220, 221, 222, 224, 225, 227, 228, 229, 230,\n       231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245,\n       246, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260,\n       261, 263, 265, 266, 268, 270, 271, 272, 273, 275, 276, 277, 279,\n       282, 283, 286, 287, 288, 289, 290, 294, 295, 296, 297, 299, 300,\n       301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 315,\n       316, 317, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330, 331,\n       332, 333, 334, 335, 338, 339, 340, 341, 343, 344, 345, 346, 347,\n       349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 368, 369, 370, 371, 372, 374, 375, 376, 377,\n       378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n       392, 393, 394, 395, 397, 398, 399, 400, 401, 403, 405, 406, 407,\n       408, 410, 411, 412, 413, 414, 415, 417, 418, 419, 420, 421, 422,\n       423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n       436, 437, 438, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n       452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n       466, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482,\n       483, 485, 486, 487, 488, 489, 491, 492, 494, 495, 496, 500, 501,\n       502, 503, 505, 506, 507, 509, 510, 511, 512, 513, 514, 516, 517,\n       518, 519, 521, 523, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n       534, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547,\n       549, 550, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563,\n       564, 565, 566, 567, 569, 570, 571, 572, 573, 574, 575, 577, 580,\n       581, 582, 583, 586, 587, 590, 591, 592, 593, 595, 596, 597, 598,\n       599, 600, 601, 602, 603, 605, 607, 608, 609, 611, 612, 613, 614,\n       615, 616, 617, 619, 622, 623, 625, 627, 630, 631, 632, 633, 634,\n       635, 637, 638, 639, 642, 643, 644, 645, 646, 647, 649, 650, 651,\n       652, 654, 655, 658, 659, 660, 661, 662, 663, 664, 666, 667, 668,\n       669, 670, 672, 674, 675, 676, 677, 678, 680, 681, 682, 684, 685,\n       686, 687, 688, 689, 691, 692, 694, 695, 696, 697, 699, 700, 701,\n       702, 703, 704, 705, 707, 708, 709, 711, 712, 713, 714, 715, 716,\n       717, 723, 725, 727, 728, 729, 730, 731, 732, 734, 736, 737, 738,\n       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n       752, 753, 754, 755, 756, 758, 759, 760, 761, 762, 763, 764, 765,\n       766, 767, 769, 770, 771, 772, 774, 775, 780, 781, 782, 783, 784,\n       785, 787, 788, 790, 793, 795, 797, 799, 800, 801, 805, 808, 809,\n       811, 812, 813, 816, 819, 820, 821, 822, 823, 825, 826, 827, 829,\n       831, 832, 833, 834, 835, 837, 838, 839, 840, 841, 842, 844, 845,\n       846, 847, 848, 850, 851, 852, 854, 855, 856, 858, 860, 861, 862,\n       864, 867, 868, 870, 871, 873, 874]\n\nprint(len(top_feats))"}}