{"cell_type":{"7b8913cc":"code","d3c22e1a":"code","93394450":"code","4cb56c52":"code","dee2f59c":"code","2108cdb4":"code","e861c1cd":"code","a131261b":"code","03329417":"code","aad5d86d":"code","034101cc":"code","75b75ec8":"code","a2a4467c":"code","5e1f59c2":"code","9dd23068":"code","85d54d41":"code","b31171e1":"code","63442f7f":"code","5d30513d":"code","207b59a3":"code","b3ed8dc6":"code","2c5eabb9":"code","0a7b0f2a":"code","e8f13f0f":"code","92da614b":"code","5bbc0554":"code","8982e8e6":"code","13077fee":"code","0a5310e5":"code","a4dc8838":"code","090efbb0":"code","bc8d93c5":"code","e923d06c":"code","1224211f":"code","b0f8cb72":"code","1657efaa":"code","ff4769aa":"code","9067a5ee":"code","b808125b":"code","c34e62d3":"code","87795358":"code","135ea090":"code","ae29fba0":"code","8b63d58b":"code","37c79d98":"code","c75b2dde":"code","19950f35":"code","d209a635":"code","1f3e007c":"code","e02dd0e8":"code","da5e77d3":"code","1021cc51":"code","085c42de":"code","fcf4c787":"code","f44c73a2":"code","5cdb2bdb":"code","d88079ca":"code","10817c46":"code","65e78b75":"code","645ca028":"code","a0181e64":"code","95b8b092":"code","4d4b347e":"code","a44250e9":"code","48a05caa":"code","42e3e812":"code","e7fd1c0f":"code","5ba8ec63":"code","d1a603fe":"code","3af6000b":"code","0c8e2e51":"code","787728f7":"code","1848a5e6":"code","c4f9ccd7":"code","bd9ac0ff":"code","34e5df22":"code","23f6a9f0":"code","8cfb384f":"code","ec39e7c9":"code","69c24fa4":"code","35f40866":"code","ff0ea553":"code","0c00dec8":"code","ec3acd5b":"code","f64e3959":"code","0266bf88":"code","04569de6":"code","34c342a9":"code","db414f66":"code","f347cba7":"code","99c1a025":"code","819a9fc9":"code","e005d99b":"code","cf11e896":"code","63f65cb5":"code","25e5ba0d":"code","a89b66e5":"code","def8ad28":"code","919e7cf6":"code","e4a1aa35":"code","4b307eb1":"code","8639e85e":"markdown","556d07c6":"markdown","81581aaa":"markdown","50863315":"markdown","274faff1":"markdown","f0f4bed0":"markdown","07fe3360":"markdown","aedf875b":"markdown","9ebd6f4e":"markdown","4ec71fee":"markdown","c4a605ab":"markdown","0b4a4623":"markdown","d52c8df4":"markdown","c21b316f":"markdown","8f572fe2":"markdown","ceebac24":"markdown","6a83eb5f":"markdown","7acb6bce":"markdown","439074fb":"markdown","b28577c3":"markdown","e9da6d67":"markdown","45fa1235":"markdown","6af557b3":"markdown","56b437ff":"markdown","b148d703":"markdown","ed488cc8":"markdown","541d86b9":"markdown","19fdc24d":"markdown","b76b339a":"markdown","a50a7884":"markdown","22ba263e":"markdown","38a10408":"markdown","773c19fe":"markdown","629ad53d":"markdown","08dae01c":"markdown","b68058bf":"markdown","b843e9cf":"markdown","d0abf2b8":"markdown"},"source":{"7b8913cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d3c22e1a":"train = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","93394450":"train.head()","4cb56c52":"train.shape","dee2f59c":"train.columns","2108cdb4":"train.info()","e861c1cd":"train.describe().transpose()","a131261b":"import seaborn as sns\n\nsns.pairplot(train, hue='target')","03329417":"train.isnull().sum()","aad5d86d":"# Visualize missing data \n\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.matrix(train)\nplt.show()","034101cc":"msno.heatmap(train)","75b75ec8":"test.head()","a2a4467c":"test.shape","5e1f59c2":"test.info()","9dd23068":"test.isnull().sum()","85d54d41":"# Visualize missing data in the test data\n\nmsno.matrix(test)\nplt.show()","b31171e1":"train_df = train.drop(['enrollee_id'], axis=1)\ntest_df = test.drop(['enrollee_id'], axis=1)","63442f7f":"def calc_num_categories(column_name):\n    unique_values_train = train[column_name].unique()\n    number_of_categories_train = len(unique_values_train)\n\n    unique_values_test = test[column_name].unique()\n    number_of_categories_test = len(unique_values_test)\n\n    if number_of_categories_train != number_of_categories_test:\n        print(f'Unique values train: {unique_values_train}, \\n Number of categories: {number_of_categories_train} \\n Unique values test: {unique_values_test}, \\n Number of categories: {number_of_categories_test}')\n    else:\n        print(f'Unique values: {unique_values_train}, \\n Number of categories: {number_of_categories_train}')\n","5d30513d":"def calc_percent_missing_values(train_df, test_df, column_name):  \n    train_missing = train_df[column_name].isnull().sum()\n    test_missing = test_df[column_name].isnull().sum()\n    perc_train = (train_missing\/len(train_df[column_name]))\n    perc_test = (test_missing\/len(test_df[column_name]))\n    \n    print(f'{column_name}:\\n % of missing values in the train data: {train_missing} \\n % of missing values in the test data: {test_missing}\\\n    \\n %train\" {perc_train:.2%} \\n %test: {perc_test:.2%}')","207b59a3":"calc_num_categories('experience')","b3ed8dc6":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ng = sns.catplot(x=\"experience\", kind=\"count\", data=train, height=8, aspect=10\/6, palette=\"husl\",hue='target',\n                order= ['<1', '1','2','3','4', '5','6','7','8','9','10', '11','12','13','14','15','16','17','18','19','20','>20'])","2c5eabb9":"calc_percent_missing_values(train, test, 'experience')","0a7b0f2a":"train_df[\"experience\"] = train_df[\"experience\"].transform(lambda x: x.fillna(x.mode()[0]))\ntest_df[\"experience\"] = test_df[\"experience\"].transform(lambda x: x.fillna(x.mode()[0]))","e8f13f0f":"calc_percent_missing_values(train_df, test_df, 'experience')","92da614b":"train_df['experience'].replace({'<1': '0', '>20': '21'}, inplace=True)\ntest_df['experience'].replace({'<1': '0', '>20': '21'}, inplace=True)","5bbc0554":"train_df['experience'].unique()","8982e8e6":"train_df['experience'] = train_df['experience'].astype(int)\ntest_df['experience'] = test_df['experience'].astype(int)","13077fee":"experience_bin_labels = [1, 2, 3,4]\ntrain_df['Binned_experience'] =   pd.qcut(train_df['experience'], q=4, labels=experience_bin_labels).astype(int)\ntest_df['Binned_experience'] =    pd.qcut(test_df['experience'], q=4, labels=experience_bin_labels).astype(int)","0a5310e5":"#sns.set(rc = {'figure.figsize':(12,6)})\nplt.figure(figsize = (12,6))\nsns.countplot(x=\"Binned_experience\", hue=\"target\", data=train_df, palette=\"husl\")","a4dc8838":"calc_num_categories('last_new_job')","090efbb0":"g = sns.catplot(x=\"last_new_job\", kind=\"count\", data=train, height=8, palette=\"husl\",hue='target', order= ['1','2','3','4','>4', 'never'],aspect=10\/6)","bc8d93c5":"calc_percent_missing_values(train, test, 'last_new_job')","e923d06c":"train_df[\"last_new_job\"] = train_df[\"last_new_job\"].transform(lambda x: x.fillna(x.mode()[0]))\ntest_df[\"last_new_job\"] = test_df[\"last_new_job\"].transform(lambda x: x.fillna(x.mode()[0]))","1224211f":"calc_percent_missing_values(train_df, test_df, 'last_new_job')","b0f8cb72":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ntrain_df['last_new_job'] = le.fit_transform(train_df['last_new_job'])\nprint(le.classes_)\ntest_df['last_new_job'] = le.fit_transform(test_df['last_new_job'])","1657efaa":"calc_num_categories('enrolled_university')","ff4769aa":"g = sns.catplot(x=\"enrolled_university\", kind=\"count\", data=train, height=5, aspect=1.5, palette=\"husl\",hue='target',order= ['no_enrollment','Part time course','Full time course'])","9067a5ee":"calc_percent_missing_values(train_df, test_df, 'enrolled_university')","b808125b":"train_df[\"enrolled_university\"] = train_df[\"enrolled_university\"].transform(lambda x: x.fillna(x.mode()[0]))\ntest_df[\"enrolled_university\"] = test_df[\"enrolled_university\"].transform(lambda x: x.fillna(x.mode()[0]))","c34e62d3":"calc_percent_missing_values(train_df, test_df, 'enrolled_university')","87795358":"enrolled_university_mapping = {'Full time course': 2, 'Part time course': 1, 'no_enrollment': 0}\n\ntrain_df['enrolled_university'] = train_df['enrolled_university'].map(enrolled_university_mapping)\ntest_df['enrolled_university'] = test_df['enrolled_university'].map(enrolled_university_mapping)   \n","135ea090":"train_df['enrolled_university'].unique()","ae29fba0":"calc_num_categories('education_level')","8b63d58b":"g = sns.catplot(x=\"education_level\", kind=\"count\", data=train_df, height=6, aspect=2, palette=\"husl\",hue='target',order= ['Primary School','High School','Graduate','Masters','Phd'])","37c79d98":"calc_percent_missing_values(train_df, test_df, 'education_level')","c75b2dde":"train_df[\"education_level\"] = train_df[\"education_level\"].transform(lambda x: x.fillna(x.mode()[0]))\ntest_df[\"education_level\"] = test_df[\"education_level\"].transform(lambda x: x.fillna(x.mode()[0]))","19950f35":"calc_percent_missing_values(train_df, test_df, 'education_level')","d209a635":"education_level_mapping = {'Primary School': 1,'High School': 2,'Graduate': 3,'Masters': 4,'Phd': 5}\n\ntrain_df['education_level'] = train_df['education_level'].map(education_level_mapping)\ntest_df['education_level'] = test_df['education_level'].map(education_level_mapping)   ","1f3e007c":"train_df.isnull().sum()","e02dd0e8":"calc_num_categories('city')","da5e77d3":"train_df['city'] = train_df['city'].str.replace('city_','')\ntest_df['city'] = test_df['city'].str.replace('city_','')","1021cc51":"train_df.city.isnull().values.any()","085c42de":"le = LabelEncoder()\ntrain_df['city'] = le.fit_transform(train_df['city'])\ntest_df['city'] = le.fit_transform(test_df['city'])","fcf4c787":"calc_num_categories('relevent_experience')","f44c73a2":"g = sns.catplot(x=\"relevent_experience\", kind=\"count\", data=train_df, height=6, aspect=1, palette=\"husl\",hue='target')","5cdb2bdb":"relevent_experience_mapping = {'Has relevent experience': 1, 'No relevent experience': 0}\n\ntrain_df['relevent_experience'] = train_df['relevent_experience'].map(relevent_experience_mapping)\ntest_df['relevent_experience'] = test_df['relevent_experience'].map(relevent_experience_mapping)   ","d88079ca":"train_df.relevent_experience.unique()","10817c46":"calc_num_categories('gender')","65e78b75":"g = sns.catplot(x=\"gender\", kind=\"count\", data=train_df, height=5, aspect=1.5, palette=\"husl\",hue='target',order= ['Male','Female','Other'])","645ca028":"train_df.gender.unique()","a0181e64":"train_df['gender'] = train_df['gender'].fillna('Unknown')\ntest_df['gender'] = test_df['gender'].fillna('Unknown')\nprint(train_df.gender.unique())","95b8b092":"\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain_df['gender']=train_df['gender'].fillna('Unknown')\ntest_df['gender'] = test_df['gender'].fillna('Unknown')\nprint(train_df.gender.unique())\n\ngender_mapping = {'Male': 1,'Female': 2,'Other': 3, 'Unknown': 0}\n\ntrain_df['gender'] = train_df['gender'].map(gender_mapping)\ntest_df['gender'] = test_df['gender'].map(gender_mapping)   ","4d4b347e":"calc_num_categories('major_discipline')","a44250e9":"g = sns.catplot(x=\"major_discipline\", kind=\"count\", data=train_df, height=5, aspect=1.8, palette=\"husl\",hue='target')","48a05caa":"calc_percent_missing_values(train_df, test_df, 'major_discipline')","42e3e812":"train_df['major_discipline']=train_df['major_discipline'].fillna('Unknown')\ntest_df['major_discipline'] = test_df['major_discipline'].fillna('Unknown')","e7fd1c0f":"\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder() \ntrain_df['major_discipline'] = le.fit_transform(train_df['major_discipline'])\nprint(le.classes_)\ntest_df['major_discipline'] = le.fit_transform(test_df['major_discipline'])","5ba8ec63":"calc_num_categories('company_size')","d1a603fe":"g = sns.catplot(x=\"company_size\", kind=\"count\", data=train_df, height=5, aspect=1.5, palette=\"husl\",hue='target')","3af6000b":"calc_percent_missing_values(train_df, test_df, 'company_size')","0c8e2e51":"train_df['company_size']=train_df['company_size'].fillna('Unknown')\ntest_df['company_size'] = test_df['company_size'].fillna('Unknown')\nprint(train_df.company_size.unique())","787728f7":"company_size_mapping = {'<10': 1,'10\/49': 2,'50-99': 3, '100-500': 4,'500-999': 5,'1000-4999': 6,'5000-9999': 7,'10000+': 8, 'Unknown': 0}\n\ntrain_df['company_size'] = train_df['company_size'].map(company_size_mapping)\ntest_df['company_size'] = test_df['company_size'].map(company_size_mapping)   \nprint(train_df.company_size.unique())","1848a5e6":"calc_num_categories('company_type')","c4f9ccd7":"g = sns.catplot(x=\"company_type\", kind=\"count\", data=train_df, height=6, aspect=1.8, palette=\"husl\",hue='target')","bd9ac0ff":"calc_percent_missing_values(train_df, test_df, 'company_type')","34e5df22":"train_df['company_type']=train_df['company_type'].fillna('Unknown')\ntest_df['company_type'] = test_df['company_type'].fillna('Unknown')","23f6a9f0":"le = LabelEncoder()\ntrain_df['company_type'] = le.fit_transform(train_df['company_type'])\nprint(le.classes_)\ntest_df['company_type'] = le.fit_transform(test_df['company_type'])","8cfb384f":"train_df.columns\n","ec39e7c9":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#'city', 'city_development_index', 'relevent_experience', 'enrolled_university', 'education_level','major_discipline', 'experience', 'company_size',\\\n#'company_type','last_new_job', 'training_hours'\n\n# split the dataset into train and test sets\ntrain_gender = train_df.loc[train_df.gender != 0][['gender','city','experience','city_development_index','major_discipline',\\\n                                                   'company_size','last_new_job']]\nprint(train_gender.isnull().sum())\ntest_gender = train_df.loc[train_df.gender == 0][['gender', 'city', 'experience','city_development_index','major_discipline',\\\n                                                   'company_size','last_new_job']]\n\n\nX_train = train_gender.drop('gender', axis=1)\ny_train = train_gender.gender\nX_test = test_gender.drop('gender', axis=1)\nprint(X_train.shape)\nprint(X_test.shape)\n\n#knn = KNeighborsClassifier(n_neighbors=12, p=1)\n#knn.fit(X_train, y_train)\n#y_pred = knn.predict(X_test)\n\nrf = RandomForestClassifier(n_estimators=2500, n_jobs=-1)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\ntrain_df.loc[train_df.gender == 0,'gender'] = y_pred","69c24fa4":"#test set \n\ntest_set_gender = test_df.loc[test_df.gender != 0][['gender','city','experience','city_development_index','major_discipline',\\\n                                                   'company_size','last_new_job']]\ntest_set_gender.isnull().sum()\npredict_gender = test_df.loc[test_df.gender == 0][['gender','city','experience','city_development_index','major_discipline',\\\n                                                   'company_size','last_new_job']]\n\n\nX_test_set = test_set_gender.drop('gender', axis=1)\nX_test_set_predict_gender = predict_gender.drop('gender', axis=1)\n\ny_test_pred = rf.predict(X_test_set_predict_gender)\n#y_test_pred = knn.predict(X_test_set_predict_gender)\n\ntest_df.loc[test_df.gender == 0, 'gender'] = y_test_pred","35f40866":"# split the dataset into train and test sets\ntrain_major_discipline = train_df.loc[train_df.major_discipline != 6][['relevent_experience', 'enrolled_university','major_discipline','training_hours', 'experience',\\\n                                                   'company_size','company_type','training_hours']]\nprint(train_major_discipline.isnull().sum())\ntest_major_discipline = train_df.loc[train_df.major_discipline == 6][['relevent_experience', 'enrolled_university','major_discipline','training_hours', 'experience',\\\n                                                   'company_size','company_type','training_hours']]\n\n\nX_train = train_major_discipline.drop('major_discipline', axis=1)\ny_train = train_major_discipline.major_discipline\nX_test = test_major_discipline.drop('major_discipline', axis=1)\nprint(X_train.shape)\nprint(X_test.shape)\n\n#knn = KNeighborsClassifier(n_neighbors=8, p=1)\n#knn.fit(X_train, y_train)\n#y_pred = knn.predict(X_test)\n\nrf = RandomForestClassifier(n_estimators=2500, n_jobs=-1)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\ntrain_df.loc[train_df.major_discipline == 6,'major_discipline'] = y_pred","ff0ea553":"#test set \n\ntest_set_major_discipline = test_df.loc[test_df.major_discipline != 6][['relevent_experience', 'enrolled_university', 'major_discipline','training_hours',\\\n                                                    'experience','company_size','company_type', 'training_hours']]\ntest_set_major_discipline.isnull().sum()\npredict_major_discipline = test_df.loc[test_df.major_discipline == 6][['relevent_experience', 'enrolled_university','major_discipline','training_hours',\\\n                                                   'experience','company_size','company_type', 'training_hours']]\n\n\nX_test_set = test_set_major_discipline.drop('major_discipline', axis=1)\nX_test_set_predict_major_discipline = predict_major_discipline.drop('major_discipline', axis=1)\n\ny_test_pred = rf.predict(X_test_set_predict_major_discipline)\n#y_test_pred = knn.predict(X_test_set_predict_major_discipline)\n\n\ntest_df.loc[test_df.major_discipline == 6, 'major_discipline'] = y_test_pred","0c00dec8":"train_df = pd.get_dummies(data=train_df,columns=['company_type','gender','major_discipline'] )\ntest_df = pd.get_dummies(data=test_df,columns=['company_type','gender','major_discipline'] )","ec3acd5b":"train_df.drop(['experience','city'], axis=1, inplace = True)\ntest_df.drop(['experience','city'], axis=1,inplace = True)","f64e3959":"# Separate input features and target\n\ny = train_df['target']\nX = train_df.drop(['target'], axis=1)","0266bf88":"from sklearn.model_selection import train_test_split\n\n# setting up testing and training sets\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state=42)","04569de6":"X_train.shape","34c342a9":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=27)\nX_train, y_train = sm.fit_resample(X_train, y_train)","db414f66":"GridSearch = False","f347cba7":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nif GridSearch == True:\n    param_grid = {'n_neighbors': np.arange(1,50), 'p':[1,2]}\n    knn = KNeighborsClassifier()\n    knn_cv = GridSearchCV(knn, param_grid, cv=10)\n    knn_cv.fit(X_train, y_train)\n    print(knn_cv.best_params_)\n    knn_cv.predict(X_val)\n    print(knn_cv.score(X_val, y_val))\n    \n    #{'n_neighbors': 2, 'p': 1}\n    #0.6985038274182325","99c1a025":"from sklearn.linear_model import LogisticRegression\n\n\nif GridSearch == True:\n    param_grid = {'C':[0.01, 0.1,0.2,0.25,0.3,0.4, 1, 2],'max_iter': [150,200,250, 300, 1000,1500], 'random_state':[42], 'solver':['liblinear'] } \n    lr = LogisticRegression()\n    lr_cv = GridSearchCV(lr, param_grid, cv=10)\n    lr_cv.fit(X_train, y_train)\n    print(lr_cv.best_params_)\n    lr_cv.predict(X_val)\n    print(lr_cv.score(X_val, y_val))\n    \n#{'C': 1, 'max_iter': 150, 'random_state': 42, 'solver': 'liblinear'}\n#0.732428670842032","819a9fc9":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n\nif GridSearch == True:\n    param_grid = {'n_estimators':[25,50,60],'max_features': [3,4,5], 'max_depth':[5,6,7], 'min_samples_leaf':[8,9,10,11],'random_state':[1,42] } \n    rf = RandomForestClassifier(n_jobs=-1)\n    rf_cv =  GridSearchCV(rf, param_grid)\n    rf_cv.fit(X_train, y_train)\n    print(rf_cv.best_params_)\n    rf_cv.predict(X_val)\n    print(rf_cv.score(X_val, y_val))\n    \n   \n# {'max_depth': 7, 'max_features': 5, 'min_samples_leaf': 9, 'n_estimators': 60, 'random_state': 42}\n# 0.7299930410577592","e005d99b":"from sklearn.linear_model import SGDClassifier\n\nif GridSearch == True:\n    param_grid = {'alpha':[0.01, 0.1,0.2, 1, 2],'max_iter': [70,75,80,90,120,150,200], 'random_state':[42] } \n    sgd = SGDClassifier()\n    sgd_cv =  GridSearchCV(sgd, param_grid, cv=5)\n    sgd_cv.fit(X_train, y_train)\n    print(sgd_cv.best_params_)\n    sgd_cv.predict(X_val)\n    print(sgd_cv.score(X_val, y_val))\n    \n    #{'alpha': 0.01, 'max_iter': 75, 'random_state': 21}\n    # 0.6196938065414057","cf11e896":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nif GridSearch == True:\n    param_grid = {'base_estimator': [LogisticRegression(random_state=1,solver='liblinear' ), DecisionTreeClassifier()],'max_features':[3,4,5,6],'max_samples':[30,40,50,60],\n                  'n_estimators':[20,30,40,50],'random_state':[1,123, 21,42,10] } \n\n    bc = BaggingClassifier(n_jobs=-1)\n    bc_cv = GridSearchCV(bc, param_grid, cv=5)\n    bc_cv.fit(X_train, y_train)\n    print(bc_cv.best_params_)\n    bc_cv.predict(X_val)\n    print(bc_cv.score(X_val, y_val))\n    \n    #{'base_estimator': DecisionTreeClassifier(), 'max_features': 6, 'max_samples': 60, 'n_estimators': 50, 'random_state': 1}\n    # 0.7141614474599861","63f65cb5":"import xgboost as xgb\n\nif GridSearch == True:\n    param_grid = {'n_estimators':[300,500,1000], 'objective':['binary:logistic'],'learning_rate':[0.1,1,1.5],'max_depth':[5,10,15] }\n    #xgb_clf = xgb.XGBClassifier(base_score=0.5, objective='binary:logistic', n_estimators=300,seed=1, learning_rate=0.1, max_depth=10)\n    xgb = xgb.XGBClassifier()\n    xgb_cv =  GridSearchCV(xgb, param_grid, cv=5) \n    xgb_cv.fit(X_train, y_train)\n    print(xgb_cv.best_params_)\n    xgb_cv.predict(X_val)\n    print(xgb_cv.score(X_val,y_val))\n    \n   # {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 300, 'objective': 'binary:logistic'}\n   # 0.7632219902574808","25e5ba0d":"import lightgbm as lgb\n\nif GridSearch == True:\n   # param_grid = {'boosting_type':['gbdt', 'rf'], 'num_leaves': [10, 20, 31, 40], 'max_depth':[- 1, 1,2,3,5, 10], 'learning_rate':[0.1, 0.01, 1, 10], \n    #                'n_estimators':[50, 100, 150, 200], 'random_state':[42]}\n    param_grid = {'boosting_type':['gbdt'], 'num_leaves': [10, 20, 31, 40], 'max_depth':[- 1,1,2,3,5,10], 'learning_rate':[0.1,0.01,1,10], \n                    'n_estimators':[50, 100, 150, 200], 'random_state':[42]}\n\n    LGBM = lgb.LGBMClassifier()\n    LGBM_cv = GridSearchCV(LGBM, param_grid, cv=6)\n    LGBM_cv.fit(X_train, y_train)\n    print(LGBM_cv.best_params_)\n    LGBM_cv.predict(X_val)\n    print(LGBM_cv.score(X_val, y_val))\n    \n     #{'boosting_type': 'gbdt', 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200, 'num_leaves': 31, 'random_state': 42}\n     # 0.7731384829505915","a89b66e5":"from sklearn.metrics import accuracy_score, f1_score,recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier, BaggingClassifier\nfrom sklearn.linear_model import SGDClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nimport lightgbm as lgb\n\nknn = KNeighborsClassifier(n_neighbors=2, p=1)\nlr = LogisticRegression(C = 1 ,random_state = 42, max_iter = 150, solver = 'liblinear')\nrf = RandomForestClassifier(class_weight='balanced', n_estimators = 100,max_features = 5, max_depth = 7, min_samples_leaf = 9, random_state = 42, n_jobs = -1)\nsgd = SGDClassifier(alpha=0.01, max_iter=75, n_jobs=-1, random_state=21)\nbc = BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_features=6,\n                  max_samples=60, n_estimators=50, random_state=1)\nxgb_clf = xgb.XGBClassifier(base_score=0.5, objective='binary:logistic', n_estimators=300,seed=1, learning_rate=0.1, max_depth=10)\nLGBM = lgb.LGBMClassifier(boosting_type = 'gbdt', learning_rate = 0.1, max_depth = 10, n_estimators = 200, num_leaves = 31, random_state = 42, n_jobs=- 1)\n\nclassifiers = [\n              ('Logistic Regression', lr),\n              ('Random Forest', rf),\n              ('LGBMClassifier', LGBM),\n              #('sgd', sgd),\n               #('knn', knn),\n               ('xgb', xgb_clf)]\n              #('BaggingClassifier', bc)]  \n\nfor clf_name , clf in classifiers:\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_val)\n    print('{:s} : {:.4f}'.format(clf_name, accuracy_score(y_val, y_pred)))\n    print(f\"f1 score: {f1_score(y_val, y_pred)}\")\n    # recall score\n    print(f\"recall_score: {recall_score(y_val, y_pred)}\")","def8ad28":"vc = VotingClassifier(estimators=classifiers)\nvc.fit(X, y)\n\ny_pred = vc.predict(X_val)\nprint('VotingClassifier: {:.4f}'.format(accuracy_score(y_val, y_pred)))\nprint(f\"f1 score: {f1_score(y_val, y_pred)}\")\n# recall score\nprint(f\"recall_score: {recall_score(y_val, y_pred)}\")","919e7cf6":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report\nfrom sklearn.metrics import confusion_matrix\n\ncm_plot_labels = [x for x in range(2)]\n\ncm = confusion_matrix(y_val, y_pred)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=cm_plot_labels)\n\ndisp = disp.plot(include_values=True,\n                 cmap=plt.cm.Blues, ax=None, xticks_rotation='horizontal')\n\nplt.show()","e4a1aa35":"print(confusion_matrix(y_val, y_pred))","4b307eb1":"y_pred_test = vc.predict(test_df)\n\noutput = pd.DataFrame({'enrollee_id':test.enrollee_id.astype(np.int32), 'target':y_pred_test})\noutput.to_csv('my_submission.csv', index=False)","8639e85e":"Explore the data.\n\nFirst I visualized the relationships between each variable to the other variables in the data using Pairplot. Then I used methods like head(), describe(), info() ect. to display the first five rows of each data set, display basic statistical details and general information about the data.\n\nAfterwards I visualized the missing data using the missingno library in order to decide which colunmns to drop and how to deal with the missing values, together with the pairplot and other visualizations shown below.","556d07c6":"### 2.`last_new_job`","81581aaa":"#### removing unnecessary text","50863315":"White lines indicate missing values, while black lines represent existing information.\n\nAs one can see, the `gender`, `major_discipline`, `company_size` and `company_type` columns have many missing value, scattered throughout the columns, and the `enrolled_university`, `education_level`,`experience` and `last_new_job`  column has only few missing values.","274faff1":"### 8. `major_discipline`","f0f4bed0":"# 1. Reading the data","07fe3360":"Create a new dataframe for the rest of the processing, keeping the `enrollee_id` column in the full dataframe for the final stage of writing to the csv file.\n\n","aedf875b":"Since the 'experience' column is a categorical variable and there are only few missing values in this column, an appropriate choice would be to replace the missing values by the most frequent value of that column.","9ebd6f4e":"# Overview\nThis notebook is organized as follows :\n\n1. Reading the data\n2. EDA - Exploratory data analysis\n3. Data preproccesing and cleaning\n4. Training models\n5. Results","4ec71fee":"## Dealing with the problem of imbalnaced data\n\nImbalanced classes are a common problem in machine learning classification where the ratio of observations in each class is a disproportionate. \nThere are few options for dealing with this problem, such as change the performance metric, change the algorithm, oversample minority class or undersample majority class and generate synthetic samples. here we'll try to use the synthetic data approach. \nSMOTE generates synthetic data by using the nearest neighbors algorithm.\n\n","c4a605ab":"## Confusion Matrix\nConfusion matrix is a tool to evaluate the accuracy of a classification process. The horizontal axis shows the labels that the model predicted and the vertical axis shows the real labels. The number in each cell indicates the number of images that match the selected combination. The diagonal shows the number of images in which the model correctly predicted reality.","0b4a4623":"## HyperParameter Tuning\nSince grid search tuning can take a long time to run, we don't necessarily want to run it every time.\n\nInstead of commenting it out, we use a flag to control whether to run it or not.","d52c8df4":"## Train set","c21b316f":"### 6. `relevent_experience`","8f572fe2":"These data contains ordinal and nominal columns. i.e in some columns the order of the categiries is significnt (ordinal) and in others there is no meaning to the order or the quantity of the value (nominal). \n\nThe ordinal columns in our data are : `relevent_experience`, `enrolled_university`, `education_level`, `experience`, `company_size`,`last_new_job`,\n\nThe nominal columns are: `city`, `gender`,  `major_discipline`, `company_type`.\n\n####  Label Encoding\nMost machine learning models require the categorical variables to be in one-hot encoding representation, or to be mapped into numbers.\n\nwe will apply ordinal encoding to the ordinal columns (i.e mapping) and one-hot encoding to the nominal columns.\n","ceebac24":"### 3. `enrolled_university`","6a83eb5f":"## Fill missing values using RandomForestClassifier","7acb6bce":"# 4. Training models\nWe will split the data into train and validation sets","439074fb":"### 1. `experience` \n","b28577c3":"# 5. Results","e9da6d67":"### Visualizing the data","45fa1235":"Similarly, we replace the missing values by the most frequent value of that column.","6af557b3":"# 2. EDA - Exploratory data analysis","56b437ff":"As shown above, this data set contains 14 columns. i.e 13 features + 1 label.\n\n10 categorical variables: `city`, `gender`, `relevent_experience`, `enrolled_university`, `education_level`, `major_discipline`, `experience`, `company_size`,`last_new_job`,`company_type`\n\n3 numerical variables: `enrollee_id`, `city_development_index`,`training_hours`\n\n1 label variable - `target`\n\n\n      \n       ","b148d703":"PAY ATTENTION to train the model using the train set only. The test set is used for prediction.","ed488cc8":"### 5. `city`","541d86b9":"The same conclusions for the test set - most of the `gender`,`company_size` and `comapny_type` columns are missing, as well as few values in the `enrolled_university`,`education_level`, `experience` and`last_new_job` columns.","19fdc24d":"Similarly, since the 'last_new_job' column is a categorical variable and there are only few missing values in this column, an appropriate choice would be to replace the missing values by the most frequent value of that column.\n","b76b339a":"# 3. Data preproccesing and cleaning","a50a7884":"## Test set","22ba263e":"####  Label Encoding\nMost machine learning models require the categorical variables to be in one-hot encoding representation, or to be mapped into numbers.\n\nwe will apply ordinal encoding to the ordinal columns (i.e mapping) and one-hot encoding to the nominal columns.\n","38a10408":"### 4. `education_level` \n","773c19fe":"### 7. `gender` ","629ad53d":"Similarly, we replace the missing values by the most frequent value of that column.\n","08dae01c":"### 9. `company_size` ","b68058bf":"The missingno correlation heatmap measures nullity correlation, i.e how strongly the presence or absence of one variable affects the presence of another ([link for more information](https:\/\/github.com\/ResidentMario\/missingno))\n\nIn this example, it seems that if the variable `company_size` appears, than the variable `company_type` is also likely to appear (correlation = 0.8)","b843e9cf":"## Dealing with missing values and cleaning the data\n","d0abf2b8":"### 10. `company_type` "}}