{"cell_type":{"c1098b9a":"code","273e39a7":"code","3aaa57eb":"code","41e7094e":"code","8154862b":"code","e4a15dad":"code","90f8792e":"code","1fd5f3f5":"code","3f8f6807":"code","443dfece":"code","90122a95":"code","eba1f1ca":"code","9fb6d222":"code","4b691a01":"code","27e19d00":"code","d2b4892a":"code","8b40a08c":"code","b7fe9d20":"code","d88bdf11":"code","337529d2":"markdown","49608e52":"markdown","2d0a1be9":"markdown","7c68d29f":"markdown","20c634ca":"markdown","38ab90b4":"markdown","e294db25":"markdown","87968277":"markdown","bf74bc3d":"markdown","d929b4a0":"markdown","cc79e02b":"markdown","289988ec":"markdown","7bb31ac0":"markdown","670c8007":"markdown","d16285ee":"markdown","972a32d2":"markdown","f64f7c52":"markdown","54d87e37":"markdown","85a809f2":"markdown","d6bade8c":"markdown","6c4ab6ac":"markdown","afcac9bd":"markdown","0ca30e87":"markdown"},"source":{"c1098b9a":"!pip install --upgrade pip\n!pip install pymap3d==2.1.0\n!pip install -U l5kit","273e39a7":"import os\nimport numpy as np\nimport pandas as pd\nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom l5kit.data import PERCEPTION_LABELS\nfrom tqdm import tqdm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom matplotlib import animation, rc\nfrom matplotlib.ticker import MultipleLocator\nfrom IPython.display import display, clear_output\nimport PIL\nfrom IPython.display import HTML\n\nrc('animation', html='jshtml')","3aaa57eb":"os.environ[\"L5KIT_DATA_FOLDER\"] = \"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\"\ncfg = load_config_data(\"\/kaggle\/input\/lyft-config-files\/visualisation_config.yaml\")\nprint(cfg)","41e7094e":"# local data manager\ndm = LocalDataManager()\n# set dataset path\ndataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\n# load the dataset; this is a zarr format, chunked dataset\nchunked_dataset = ChunkedDataset(dataset_path)\n# open the dataset\nchunked_dataset.open()\nprint(chunked_dataset)","8154862b":"agents = chunked_dataset.agents\nagents_df = pd.DataFrame(agents)\nagents_df.columns = [\"data\"]; features = ['centroid', 'extent', 'yaw', 'velocity', 'track_id', 'label_probabilities']\n\nfor i, feature in enumerate(features):\n    agents_df[feature] = agents_df['data'].apply(lambda x: x[i])\nagents_df.drop(columns=[\"data\"],inplace=True)\nprint(f\"agents dataset: {agents_df.shape}\")\nagents_df.head()","e4a15dad":"agents_df['cx'] = agents_df['centroid'].apply(lambda x: x[0])\nagents_df['cy'] = agents_df['centroid'].apply(lambda x: x[1])","90f8792e":"fig, ax = plt.subplots(1,1,figsize=(8,8))\nplt.scatter(agents_df['cx'], agents_df['cy'], marker='+')\nplt.xlabel('x', fontsize=11); plt.ylabel('y', fontsize=11)\nplt.title(\"Centroids distribution\")\nplt.show()","1fd5f3f5":"multi_mode = pd.read_csv(\"..\/input\/lyft-motion-prediction-autonomous-vehicles\/multi_mode_sample_submission.csv\")","3f8f6807":"multi_mode.describe","443dfece":"multi_mode.head()","90122a95":"multi_mode.shape","eba1f1ca":"os.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\ndm = LocalDataManager()\nsample_path = '..\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/sample.zarr'\nsample_dataset = ChunkedDataset(sample_path)\nsample_dataset.open()\nprint(sample_dataset)","9fb6d222":"sample_agents = sample_dataset.agents\nsample_agents = pd.DataFrame(sample_agents)\nsample_agents.columns = [\"data\"]; features = ['centroid', 'extent', 'yaw', 'velocity', 'track_id', 'label_probabilities']\n\nfor i, feature in enumerate(features):\n    sample_agents[feature] = sample_agents['data'].apply(lambda x: x[i])\nsample_agents.drop(columns=[\"data\"],inplace=True)\nsample_agents.head()","4b691a01":"test_path = '..\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/test.zarr'\ntest_dataset = ChunkedDataset(test_path)\ntest_dataset.open()\nprint(test_dataset)","27e19d00":"train_path = '..\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/train.zarr'\ntrain_dataset = ChunkedDataset(train_path)\ntrain_dataset.open()\nprint(train_dataset)","d2b4892a":"valid_path = '..\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/validate.zarr'\nvalid_dataset = ChunkedDataset(valid_path)\nvalid_dataset.open()\nprint(valid_dataset)","8b40a08c":"cfg = {}\nraster_params = {'raster_size':[512,512],\n                 'pixel_size':[0.5,0.5],\n                 'ego_center':[0.25,0.5],\n                 'map_type':'py_semantic',\n                 'satellite_map_key': 'aerial_map\/aerial_map.png',\n                 'semantic_map_key': 'semantic_map\/semantic_map.pb',\n                 'dataset_meta_key': 'meta.json',\n                 'filter_agents_threshold': 0.5}\nmodel_params ={'model_architecture': 'effnetB5',\n               'history_num_frames': 0,\n               'history_step_size': 1,\n               'history_delta_time': 0.1,\n               'future_num_frames': 50,\n               'future_step_size': 1,\n               'future_delta_time': 0.1}\ncfg['raster_params'] = raster_params\ncfg['model_params'] = model_params\nrast = build_rasterizer(cfg,dm)\ndataset = EgoDataset(cfg, test_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","b7fe9d20":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, sample_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","d88bdf11":"import cv2\nimage = cv2.imread('..\/input\/lyft-motion-prediction-autonomous-vehicles\/aerial_map\/aerial_map.png')\nimage = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(32,32))\nplt.imshow(image)","337529d2":"# Centroid distrbution","49608e52":"Semantic View of the road map","2d0a1be9":"**Aerial Map of the road**","7c68d29f":"**Train Path**","20c634ca":"![levels-of-driving-automation.jpg.imgw.850.x.jpg](attachment:levels-of-driving-automation.jpg.imgw.850.x.jpg)","38ab90b4":"References - \n\nhttps:\/\/www.kaggle.com\/gpreda\/lyft-first-data-exploration\n\nhttps:\/\/www.kaggle.com\/vineeth1999\/lyft-motion-prediction-eda\n\nhttps:\/\/www.wired.com\/story\/guide-self-driving-cars\/\n","e294db25":"![StanfordCart.jpg](attachment:StanfordCart.jpg)","87968277":"# Maps\nBefore a robocar takes to the streets, its parent company will use cameras and lidars to map its territory in extreme detail. That reference document helps the car verify its sensor readings, and it is key for any vehicle looking to know its own location, down to the centimeter\u2014something standard GPS can\u2019t offer.","bf74bc3d":"# Radars\nA regular presence in cars since the late 1990s, radars bounce radio waves around to see their surrounding and are especially good at spotting big metallic objects\u2014other vehicles. They\u2019re cheap, reliable, and don\u2019t sweat things like fog, rain, or snow.","d929b4a0":"# What is an Autonomous Car?\n **An autonomous car is a vehicle capable of sensing its environment and operating without human involvement. A human passenger is not required to take control of the vehicle at any time, nor is a human passenger required to be present in the vehicle at all. An autonomous car can go anywhere a traditional car goes and do everything that an experienced human driver does.The Society of Automotive Engineers (SAE) currently defines 6 levels of driving automation ranging from Level 0 (fully manual) to Level 5 (fully autonomous). These levels have been adopted by the U.S. Department**","cc79e02b":"Test Path","289988ec":"# Machine Learning\nAt its simplest, this artificial intelligence tool trains computers to do things like detect lane lines and identify cyclists by showing them millions of examples of the subject at hand. Because the world is too complex to write a rule for every possible scenario, it\u2019s key to have cars that can learn from experience and figure out how to navigate on their own.","7bb31ac0":"# Stanford Cart\n**The first ever self driving car in history, built in 1961**","670c8007":"# Lidars\nThe spinning thing you see on top of most self-driving cars is lidar (that\u2019s \u201clight detection and ranging\u201d). It fires out millions of laser beams every second, measures how long they take to bounce back, and uses the data to build a 3D map that\u2019s more precise than what radar offers and easier for a computer to understand than a 2D camera image. It\u2019s also crazy expensive, hard to manufacture at scale, and nowhere near robust enough for a life of potholes and extreme temperatures. Good thing dozens of startups and tech giants are pouring millions of dollars into fixing all that.","d16285ee":"**Configuring the environment**","972a32d2":"# Cameras\nGreat for spotting things like lane lines on the highway, speed signs, and traffic lights. Some developers think that, with better machine vision, they can use cameras to identify everything they see and navigate accordingly.","f64f7c52":"# Data exploration","54d87e37":"Satellite View of the road","85a809f2":"# Load the Data","d6bade8c":"![autonomous-car-driving-on-road-and-sensing-royalty-free-illustration-1585662413.jpg](attachment:autonomous-car-driving-on-road-and-sensing-royalty-free-illustration-1585662413.jpg)","6c4ab6ac":"Validation path","afcac9bd":"[Tesla Auto Pilot](https:\/\/www.youtube.com\/watch?v=tlThdr3O5Qo)","0ca30e87":"> Sample path"}}