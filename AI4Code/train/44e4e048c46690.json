{"cell_type":{"2bfaa1e6":"code","ccaf1578":"code","da863a7b":"code","93e2e5cf":"code","92f68f9f":"code","151c9be2":"code","2eaa2cca":"code","59e0087c":"code","55fbb367":"markdown","2efaf388":"markdown","4a253ef7":"markdown","a4152516":"markdown","ec211844":"markdown","e2c5243f":"markdown","443b3daf":"markdown","949a25bb":"markdown","f9f55928":"markdown"},"source":{"2bfaa1e6":"import numpy as np\n\nA=np.array([[1,2,3],[4,5,6],[7,8,9]])\nprint(f'A=\\n{A}\\n\\n'\n      f'A\\'=\\n{A.T}')","ccaf1578":"import numpy as np\n\nA=np.array([1, 2, 3])\n\n\"\"\"transpose documentation:\n    For a 1-D array this has no effect, as a transposed vector is simply the same vector.\n    To convert a 1-D array into a 2D column vector, an additional dimension must be added.\n    np.atleast_2d(a).T achieves this, as does a[:, np.newaxis].\"\"\"\nx1,x2=4,5\nB=np.array([x1, x2, 1])\nB = np.atleast_2d(B).T\n\nprint(f'A={A}\\nB={B})')\nprint(f'A*B={A.dot(B)}')\nprint(f' =[{A[0]}*{B[0,0]}+{A[1]}*{B[1,0]}+{A[2]}*{B[2,0]}]')","da863a7b":"import numpy as np\n\nA=np.array([[1, 2], [3, 4]])\nB=np.array([[5, 6, 7], [8, 9, 10]])\nprint(f'A=\\n{A}')\nprint(f'B=\\n{B}')\nprint(f'AB=\\n{A.dot(B)}')\n","93e2e5cf":"import numpy as np\n\nA = np.array([[1,2],[3,4]])\ninvA = np.linalg.inv(A)\npinvA = np.linalg.pinv(A) #Moore-Penrose psuedo inverse\n\nnp.set_printoptions(precision=20, suppress=False)\nprint(f'A=\\n{A}\\n')\nprint(f'invA=\\n{invA}\\n')\nprint(f'pinvA=\\n{pinvA}\\n')\nprint(f'A * invA=\\n{A.dot(invA)}\\n')\nprint(f'A * pinvA=\\n{A.dot(pinvA)}\\n')","92f68f9f":"import numpy as np\n\nA = np.array([[1,2],[3,4]])\ne_values, e_vectors = np.linalg.eig(A)\ne_vectors_aux = e_vectors.T\n\nfor l, v in zip(e_values, e_vectors_aux):\n    print(f'eigenvalue={l},\\neigenvector={v}')\n    print(f'=> A * eigenVector={A.dot(v.T)}') #works even without \".T\" but beware of dimensions!!!\n    print(f'=> eigenvalue * eigenvector={l*v}\\n')\n\nprint(f'Calculating all at once.')\nprint(f'A * eigenVectorMatrix=\\n{A.dot(e_vectors)}')","151c9be2":"import numpy as np\n\n#Graph connectivity matrix\nA = np.array([[0,.3,.7],[.3, 0, 0], [.7, 0, 0]])\ne_values, e_vectors = np.linalg.eig(A)\ne_vectors_aux = e_vectors.T\n\nfor l, v in zip(e_values, e_vectors_aux):\n    if l>0:\n        print(f'eigenvalue={l},\\neigenvector={v}')\n","2eaa2cca":"import numpy as np\n\nA = np.array([[3,2],[6,-3]])\nb1 = np.array([[13],[6]])\nb2 = np.array([[21],[3]])\n\ninvA = np.linalg.inv(A)\n\nprint(f'Solution for b1, x=\\n{invA.dot(b1)}')\nprint(f'Check A.dot(x)=\\n{A.dot(invA.dot(b1))}\\n')\nprint(f'Solution for b2, x=\\n{invA.dot(b2)}')\nprint(f'Check A.dot(x)=\\n{A.dot(invA.dot(b2))}\\n')\n\nA = np.array([[3,0],[6,0]]) #What happends here?\ninvA = np.linalg.inv(A) #Will throw \"LinAlgError: Singular matrix\"\n#invA = np.linalg.pinv(A) #Will give as a WRONG answer\nprint(f'Solution for b1, x={invA.dot(b1)}')\nprint(f'Check our result A.dot(x)={A.dot(invA.dot(b1))}\\n')","59e0087c":"import numpy as np\n\nA = np.array([[1,1,1],[4,2,1], [9,3,1]])\nb = np.array([[4],[5],[6]])\ninvA = np.linalg.inv(A)\nx = invA.dot(b)\nprint(f'Solution invA.dot(b) = {x}\\n')\nprint(f'Equation:\\n{x[0][0]} * x^2 + {x[1][0]} * x + {x[2][0]} = y\\n')\n\nprint(f'Check our result A.dot(x)=\\n{A.dot(x)}')","55fbb367":"Example 2, simple tree graph. Check root node has higher eigenvector centrality","2efaf388":"### Eigenvectors and eigenvalues \n\nEigenvectors are also called characteristic vectors.\n\nHaving:\n* $A \\cdot X = \\lambda X : A \\in C^{n \\times n}, X \\in C^n, X\\ is\\ a\\ non-zero\\ vector$\n* we call X an eigenvector of A and the corresponding $\\lambda$ an eigenvalue.\n* X changes length but not direction\n* Also true $A \\cdot X - \\lambda X = 0$ and $(A - \\lambda I)\\cdot X = 0$ y $(\\lambda I - A)\\cdot X=0: X\\neq0$\n\nThe idea is we are able to identify with vectors don't change direction.\n\n#### In the context of graph theory\nWith $A$ being the adjacency matrix (can be weighted) of graph $G$:\n* $A$'s greatest eigenvalue and it's corresponding eigenvector are used to study nodes centrality\n* Eigenvector centrality (here $c$) of a node gives us an indication of it's relative influence or relevance on the graph\n * If a node is pointed to by many nodes with high centrality score said node will gave a high score\n * $c(x_i) = \\frac{1}{\\lambda} \\sum_{x_j \\in N(x_i)} c(x_j) = \\frac{1}{\\lambda} \\sum_{x_j \\in G} [connected(x_i, x_j)] c(x_j)$\n (NOTE! \"[\" and \"]\" here are Iverson's brackets, 1 if condition is true and 0 otherwise).\n* This idea is used by Google's pagerank algorithm\n\n#### Eigenvectors are also used with the covariance matrix:\n* Covariance measures how much two much two random variables vary together\n* Covariance matrix $C \\in R^{d \\times d}, C_{i,j}=\\sigma(x_i, x_j)$\n * d is the number of dimensions (variables, features, etc)\n * $x_i, x_j$ are random variables\n * $\\sigma(x_i, x_j)= \\sigma(x_j, x_i)$\n* In this context \"The eigenvectors are unit vectors representing the direction of the largest variance of the data,\nwhile the eigenvalues represent the magnitude of this variance in the corresponding directions.\" Source:https:\/\/datascienceplus.com\/understanding-the-covariance-matrix\/\n * This is used on Principal Component Analysis (PCA)\n\nFor Python's Numpy:\n* w,v = numpy.linalg.eig(myMatrix) -> returns eigenvalues & normalized eigenvectors\n * column v[:,i] is the eigenvector corresponding to the eigenvalue w[i]\n * TIP: if we need eigenvectors as rows use v.T instead\n\nExample 1:","4a253ef7":"## Putting it all together\n\n### Matrix and equation systems\n\nLet' assume we have the following system of equations:\n* $3x_1 + 2x_2 = 13$\n* $6x_1 - 3x_2 = 6$\n\nWe can rewrite then as:\n\n$ A = \\begin{pmatrix} 3 & 2 \\\\ 6 & -3 \\end{pmatrix},\\ \\vec{x}= \\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix},\\ \\vec{b}=\\begin{pmatrix}13 \\\\ 6\\end{pmatrix}$\n\n$A \\cdot \\vec{x} = \\vec{b}$\n\nLet's solve using what we've learned so far:\n\n* $A^{-1} \\cdot A \\cdot \\vec{x} =  A^{-1} \\cdot \\vec{b}$\n* $I \\cdot \\vec{x} =  A^{-1} \\cdot \\vec{b}$\n* $\\vec{x} =  A^{-1} \\cdot \\vec{b}$\n\nOnce we've calculated $A^{-1}$ we can even use it to solve with different values of $\\vec{b}$.\n\n\n\n#### Example 1 with Python's Numpy","a4152516":"From AI-Bootcamp: https:\/\/github.com\/v-fuentec\/AI-bootcamp\n\n# Definitions of some basic terms\n\n## Scalar\n* It's number of some kind (real, natural, etc)\n* Named usually with lower case italics and defined by specifying it's kind, example: $s \\in I\\!R$\n\n## Vector\n* It's an ordered array of scalars\n* If we considered a vector a point in space each element is a coordinate on it's corresponding axis\n* Named optionally with the vector symbol (lower-case bold or with arrow symbol): $\\mathbf{v}$ or $\\vec{v}$\n* If not specified otherwise vectors have dimension nx1 (meaning they are column-vectors)\n* Examples:\n    * $\\vec{v} \\in I\\!R^n$ \n    * $\\vec{v}= \\begin{pmatrix} v_{1} \\\\ v_{2} \\\\ \\vdots \\\\ v_{n} \\end{pmatrix} $\n* Vectors have:\n * Direction\n * Magnitude (noted as $\\| \\mathbf{v} \\|$)\n\n### p-norm or $L^p$-norm\n\nHaving a vector $\\vec{x} = (x_1 \\cdots x_n)$ the p-norn $\\| \\mathbf{\\vec{x}} \\|_p, p \\in I\\!R, p \\ge 1$ is defined as:\n\n* $\\| \\mathbf{\\vec{x}} \\|_p = \\sqrt[p]{|x_1|^p + \\cdots + |x_n|^p}$\n* Distances between two points:\n * With $p=1$ it's called the \"Manhattan distance\" (a.k.a. rectilinear or taxicab distance)\n * With $p=2$ is called the Euclidean distance (we use this one to calculate the length or magnitude of a vector)\n\nFor Python's Numpy:\n* Euclidean norm:\n * numpy.linalg.norm(myVector) or np.linalg.norm(v1 - v2)\n* p-norm for vectos:  numpy.linalg.norm(myVector, ord=p)\n\n## Matrices\n* Two dimensional arrays of scalars\n* Examples:\n    * $A \\in I\\!R^{m \\times n}$\n    * $A = \\begin{pmatrix} a_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\ a_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\ a_{m,1} & a_{m,2} & \\cdots & a_{m,n}  \\end{pmatrix}$\n* Two matrices can be added or subtracted if they have the same dimensions\n\nFor Python's Numpy:\n* myNdArray = numpy.array(myMatrix)\n\n### Transpose\n\nFor Python's Numpy:\n* myNdArray.T","ec211844":"### Matrix element-wise multiplication (Hadamard product)\n\n$C = A \\circ B$\n\n* $A=\\begin{pmatrix}a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{pmatrix}, B=\\begin{pmatrix}b_{11} & b_{12} \\\\ b_{21} & b_{22}\\end{pmatrix}$\n\n* $C=\\begin{pmatrix}a_{11}*b_{11} & a_{12}*b_{12} \\\\ a_{21}*b_{21} & a_{22}*b_{22}\\end{pmatrix}$\n\n\n### Matrix multiplication (dot product)\n$C=A \\cdot B$\n * Two matrices (A and B) can be multiplied if the number of columns of the first equals the number rows on the second\n * If $A \\in I\\!R^{m \\times n}$, $B \\in I\\!R^{n \\times p}$ then $C \\in I\\!R^{m \\times p}$  \n * $C = A \\cdot B : c_{i,j} = \\sum_{k} a_{i,k}b_{k,j} $ \n   * $c_{i,j}$ contains the total sum of the element-wise multiplication of (i)th row of A with (j)th col of B \n\nExamples:\n* $A=\\begin{pmatrix} a_1 & a_2 & a_3 \\end{pmatrix} , X=\\begin{pmatrix} x_1 \\\\ x_2 \\\\ 1 \\end{pmatrix}, AX=(a_1x_1+a_2x_2+a_3)$","e2c5243f":"* $A=\\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix},\\ B=\\begin{pmatrix} b_{11} & b_{12} & b_{13} \\\\ b_{21} & b_{22} & b_{23}\\end{pmatrix}$\n\n* $C=A \\cdot B = \\begin{pmatrix} a_{11}b_{11} + a_{12}b_{21} & a_{11}*b_{12} + a_{12}*b_{22} & a_{11}*b_{13} + a_{12}*b_{23} \\\\ a_{21}b_{11} + a_{22}b_{21} & a_{21}*b_{12} + a_{22}*b_{22} & a_{21}*b_{13} + a_{22}*b_{23}\\end{pmatrix}$","443b3daf":"### Solve for the coefficients of a quadratic function\n\n$ax^2+bx+c=y$\n* we need to known three points (one for each unknown)\n* let's say we know $(x_1, y_1),\\ (x_2, y_2),\\ (x_3, y_3)$\n\nWe can write:\n\n$\\begin{pmatrix}x_1^2 & x_1 & 1 \\\\ x_2^2 & x_2 & 1 \\\\ x_3^2 & x_3 & 1\\end{pmatrix} \\cdot \\begin{pmatrix} a \\\\ b \\\\ c\\end{pmatrix} =  \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3\\end{pmatrix}$\n\nIt's the same as before $A \\cdot \\vec{x} = \\vec{b}$\n\nAgain $\\vec{x} =  A^{-1} \\cdot \\vec{b}$\n\n\n#### Example 2 with Python's Numpy","949a25bb":"## Tensors\n* multidimensional arrays (matrices are 2D tensors)","f9f55928":"#### Matrix multiplication properties:\n* $A \\cdot B \\neq B \\cdot A$\n* $A \\cdot (B \\cdot C) = (A \\cdot B) \\cdot C$\n* $A \\cdot (B+C) = A \\cdot B + A \\cdot C$ \n* $(A+B) \\cdot C = A \\cdot C + B \\cdot C$\n\nFor Python's Numpy:\n* element-wise product: myNdArray1 * myNdArray2\n* dot product: myNdArray1.dot(myNdArray2)\n\n### Dot multiplication of two vectors\n\n* $\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|cos(\\theta)$\n* We can simplify to $\\vec{a} \\cdot \\vec{b} = \\|\\vec{b}\\|\\ scalarProj_ba$\n * $cos(\\theta)$ is equal to the length of the projection of $\\vec{a}$ over $\\vec{b}$ (or the scalar projection) divided by $\\|\\vec{a}\\|$\n   * The scalar projection of $\\vec{a}$ over $\\vec{b}$ is\n     * $a_b = \\|\\vec{a}\\|cos(\\theta)$\n   * $proj_ba = a_b\\ \\hat{b}$ where $\\hat{b}$ is the unit vector in the direction of $\\vec{b}$\n\nWe can see that the dot product of two vectors, in a way, meassures \"how much the go in the same direction\" so to speak.\n\nProperties:\n* Two non-zero vectors are orthogonal if and only if $\\vec{a} \\cdot \\vec{b} = 0$\n* $\\vec{a} \\cdot \\vec{a} = \\|\\vec{a}\\|$\n\n### Some alternative ways to understand what the dot product of a matrix and a vector does...\n\n* A transformation:\n * Having $A \\in I\\!R^{m \\times n}, \\vec{x} \\in I\\!R^{n \\times 1}$\n * Then we can see the dot product result as an \"output-space\" ($I\\!R^{m \\times 1}$) we want to \"translate\" or \"transform\" $\\vec{x}$ to.\n* Linear or weighted combination of column-vectors:\n * Having $A=\\begin{pmatrix} \\vec{v_1} & \\cdots & \\vec{v_n}\\end{pmatrix},\\ \\vec{x}=\\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n\\end{pmatrix}$\n * Then $A \\cdot \\vec{x} = x_1\\vec{v_1} + \\cdots + x_n\\vec{v_n}$\n* Dot product of row-vectors\n * Having $A=\\begin{pmatrix} \\vec{v_1}^T \\\\ \\vdots \\\\ \\vec{v_m}^T\\end{pmatrix},\\ \\vec{x}=\\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n\\end{pmatrix}$\n * Then $A \\cdot \\vec{x} =\\begin{pmatrix} \\vec{v_1}^T \\cdot \\vec{x}\\\\ \\vdots \\\\ \\vec{v_m}^T\\cdot \\vec{x}\\end{pmatrix}$\n\n\n### Identity matrix and inverse\n\nThe identity matrix $I_n$ is a $n \\times n$ matrix contains 1s on the upper-left to lower-right diagonal and zeros\non the rest. E.g.:\n\n$I_3 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$\n\nIdentity matrix properties (acts like one for real numbers on multiplication):\n$A \\cdot I = I \\cdot A = A$\n\nThe inverse matrix $A^{-1}$ verifies that $A\\cdot A^{-1} = I$.\nNot all matrices have inverse (for those cases some math libraries include functions to obtain pseudo-inverse matrices).\n\nA singular matrix is a square matrix that does not have an inverse.\n\nFor Python's Numpy:\n* numpy.identity(n)\n* numpy.linalg.inv(myMatrix)\n* numpy.linalg.pinv(myMatrix)\n\n\nExamples:"}}