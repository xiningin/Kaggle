{"cell_type":{"8b52a8f8":"code","d172915a":"code","df3b1706":"code","2dc1148d":"code","e5c0fa21":"code","51669c11":"code","032f87e7":"code","3a386f90":"code","973107cb":"code","6611b2ed":"code","68012ac8":"code","a41b45a9":"code","c6e69fc6":"code","d1bbfb0e":"code","2be54515":"code","95164776":"code","03706b74":"code","2892302a":"code","97d00053":"code","83f377b9":"code","0f0a10a0":"code","5cae5166":"code","3a77e23a":"code","4a8a56c3":"code","4ab700ec":"code","261c1e3d":"code","40e12850":"code","1ae151f8":"code","f45f1074":"code","7b6cdf74":"code","b46a83ed":"code","35a29d3f":"code","9b900335":"code","bd7eef04":"code","496ff3bd":"code","79787c4c":"code","8372cb85":"code","46cfafde":"code","69e86f17":"code","5fa18fa0":"code","07e521b7":"code","045dac7f":"code","077c36ef":"code","a208e461":"code","e6397bdc":"code","a37e927a":"code","024b2f67":"code","2e0d51b5":"code","59219f54":"code","81055f25":"code","dadc022b":"code","1b482829":"code","8a1068e1":"code","b5bffdf2":"code","ff5cf0a2":"code","1f195029":"code","56bea0f6":"code","0d62ef41":"code","2ee6b7e8":"code","9d451f59":"code","5bcf236e":"code","95102a33":"code","f05e6d32":"code","de0c865d":"code","fe1a4bc9":"code","ead4f7e7":"markdown","470a8468":"markdown","665e3a8f":"markdown","e8276e22":"markdown","a3550732":"markdown","187201c1":"markdown","de352b14":"markdown","274b64eb":"markdown","5e718df9":"markdown","eff902ec":"markdown","47642220":"markdown","76d528c9":"markdown","42f986ad":"markdown","8223bd91":"markdown","9d9a5ad6":"markdown"},"source":{"8b52a8f8":"#Import necessary libraries....\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import norm, stats\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom sklearn.svm import SVR","d172915a":"#Load the dataset...\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf = pd.concat([df_train, df_test])\npd.options.display.max_columns = None","df3b1706":"df_train.head()","2dc1148d":"cols = df.columns\ndf.columns\n","e5c0fa21":"df.info()","51669c11":"#We can work with target variable first.\n\ndf_train['SalePrice'].describe()","032f87e7":"#Histogram of SalePrice\n#Here we can see that the data is likely to be positively skewed.. (Noted)\nsns.distplot(df_train['SalePrice'])","3a386f90":"#Finding the columns with more correlation with SalePrice,\ndf_train.corr()['SalePrice'].sort_values(ascending = False)","973107cb":"# visualization 2 variables\nplt.scatter(df_train['GrLivArea'],df_train['SalePrice'])","6611b2ed":"plt.scatter(df_train['GarageArea'],df_train['SalePrice'])","68012ac8":"#Data Imputation.\ndef show_missing(df):\n    #It shows percentage of null values in each column\n    pd.options.display.max_rows = None\n    display(((df.isnull().sum()\/len(df))*100))\n","a41b45a9":"show_missing(df)","c6e69fc6":"#DROP the columns which has more than 50% null values...\ndf_train.drop(['Alley','MiscFeature','PoolQC','Fence','Id'],axis = 1, inplace = True)\ndf_test.drop(['Alley','MiscFeature','PoolQC','Fence', 'Id'],axis = 1, inplace = True)\n","d1bbfb0e":"df  = pd.concat([df_train, df_test])\ndf.head()","2be54515":"#Finding numerical and categorical data column names for further use..\ncat_variables =  [ i for i in df.columns if df.dtypes[i]=='object' ]\nnum_variables = [ i for i in df.columns if df.dtypes[i]!='object' ]\n","95164776":"num_variables","03706b74":"df_train.corr()['SalePrice'].sort_values(ascending = False)","2892302a":"#Selecting the columns with correlation less than 0.1 for removing\nnum_del  = [i for i in num_variables if df_train.corr()['SalePrice'][i] < 0.1]\nnum_del","97d00053":"#Droping the columns and updating the changes in df also.\ndf_train.drop(num_del,axis = 1 ,  inplace = True)\ndf_test.drop(num_del,axis = 1 ,  inplace = True)\ndf  = pd.concat([df_train, df_test])","83f377b9":"show_missing(df)","0f0a10a0":"cat_variables =  [ i for i in df.columns if df.dtypes[i]=='object' ]\nnum_variables = [ i for i in df.columns if df.dtypes[i]!='object' ]","5cae5166":"#Imputing the missing values in all the columns\n\ndf_train['GarageYrBlt'].fillna(df_train['GarageYrBlt'].median(), inplace =True)\ndf_test['GarageYrBlt'].fillna(df_train['GarageYrBlt'].median(), inplace =True)\n\n\ndf_train['MasVnrArea'].fillna(df_train['MasVnrArea'].median(), inplace =True)\ndf_test['MasVnrArea'].fillna(df_train['MasVnrArea'].median(), inplace =True)\n\ndf_train['LotFrontage'].fillna(df_train['LotFrontage'].median(), inplace =True)\ndf_test['LotFrontage'].fillna(df_train['LotFrontage'].median(), inplace =True)\n\ndf_train['GarageArea'].fillna(df_train['GarageArea'].median(), inplace =True)\ndf_test['GarageArea'].fillna(df_train['GarageArea'].median(), inplace =True)\n\ndf_train['GarageCars'].fillna(df_train['GarageCars'].mode()[0], inplace =True)\ndf_test['GarageCars'].fillna(df_train['GarageCars'].mode()[0], inplace =True)\n\ndf_test['TotalBsmtSF'].fillna(df_train['TotalBsmtSF'].median(), inplace =True)\ndf_test['BsmtFinSF1'].fillna(df_train['BsmtFinSF1'].median(), inplace =True)\ndf  = pd.concat([df_train, df_test])","3a77e23a":"#Here these columns the null values are actually has a value. (Refer description file for further reference)\ndf_test.fillna({'GarageType':'NG', 'GarageQual':'NG','GarageFinish':'NG','GarageCond':'NG', 'FireplaceQu':'NF'}, inplace=True)\ndf_train.fillna({'GarageType':'NG', 'GarageQual':'NG','GarageFinish':'NG','GarageCond':'NG','FireplaceQu':'NF'}, inplace=True)\ndf_train.fillna({'BsmtQual':'NB','BsmtCond':'NB','BsmtExposure':'NB','BsmtFinType1':'NB','BsmtFinType2':'NB'}, inplace=True)\ndf_test.fillna({'BsmtQual':'NB','BsmtCond':'NB','BsmtExposure':'NB','BsmtFinType1':'NB','BsmtFinType2':'NB'}, inplace=True)\ndf_train.fillna({'Electrical':df_train['Electrical'].mode()[0]}, inplace=True)","4a8a56c3":"df  = pd.concat([df_train, df_test])\nshow_missing(df)","4ab700ec":"df_train['MSZoning'].fillna(df_train['MSZoning'].mode()[0], inplace =True)\ndf_test['MSZoning'].fillna(df_train['MSZoning'].mode()[0], inplace =True)\n\ndf_train['Utilities'].fillna(df_train['Utilities'].mode()[0], inplace =True)\ndf_test['Utilities'].fillna(df_train['Utilities'].mode()[0], inplace =True)\n\ndf_train['Exterior1st'].fillna(df_train['Exterior1st'].mode()[0], inplace =True)\ndf_test['Exterior1st'].fillna(df_train['Exterior1st'].mode()[0], inplace =True)\n\ndf_train['Exterior2nd'].fillna(df_train['Exterior2nd'].mode()[0], inplace =True)\ndf_test['Exterior2nd'].fillna(df_train['Exterior2nd'].mode()[0], inplace =True)\n\ndf_train['MasVnrType'].fillna(df_train['MasVnrType'].mode()[0], inplace =True)\ndf_test['MasVnrType'].fillna(df_train['MasVnrType'].mode()[0], inplace =True)\n\ndf_train['BsmtUnfSF'].fillna(df_train['BsmtUnfSF'].median(), inplace =True)\ndf_test['BsmtUnfSF'].fillna(df_train['BsmtUnfSF'].median(), inplace =True)\n","261c1e3d":"df  = pd.concat([df_train, df_test])\nshow_missing(df)","40e12850":"# Finally created Function for imputing the null values \ud83e\udd29 you can use this function for avoiding the above steps.\ndef impute_null(df):\n    cat_v=  [ i for i in df.columns if df.dtypes[i]=='object' if df[i].isnull().values.any()]\n    num_v = [ i for i in df.columns if df.dtypes[i]!='object' if df[i].isnull().values.any()]\n    for i in num_v:\n        df[i].fillna(df_train[i].median(), inplace =True)\n    for i in cat_v:\n        df[i].fillna(df_train[i].mode()[0], inplace =True)\n    \n        ","1ae151f8":"impute_null(df_train)\nimpute_null(df_test)","f45f1074":"df  = pd.concat([df_train, df_test])\nshow_missing(df)","7b6cdf74":"df.head()","b46a83ed":"#Handling Ordinal categories using LabelEncoder \n#Ordering them in particular order..\nord_dict = {\"LotShape\": ['Reg','IR1','IR2','IR3'],\n            \"LandSlope\" : [\"Gtl\", \"Mod\", \"Sev\" ],\n            \"ExterQual\": [  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\" ],\n            \"ExterCond\": [  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\" ],\n            \"BsmtQual\": [  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NB\" ],\n            \"BsmtCond\":[  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NB\" ],\n            \"BsmtExposure\": [\"Gd\", \"Av\", \"Mn\", \"No\", \"NB\"],\n            \"BsmtFinType1\":[ \"GLQ\",\"ALQ\",\"BLQ\",\"Rec\",\"LwQ\",\"Unf\",\"NB\"],\n            \"BsmtFinType2\":[ \"GLQ\",\"ALQ\",\"BLQ\",\"Rec\",\"LwQ\",\"Unf\",\"NB\"],\n            \"HeatingQC\": [  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\"],\n            \"KitchenQual\": [  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\"],\n            \"GarageQual\":[  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NG\"],\n            \"GarageCond\": [  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NG\" ],\n            \"Utilities\":  ['AllPub','NoSeWa']\n        }\ncols_ord = ord_dict.keys()\nle = LabelEncoder()\n\nfor col in cols_ord:\n    le.fit(ord_dict[col])\n    df_train[col] = le.transform(df_train[col])\n    df_test[col] = le.transform(df_test[col])\n    ","35a29d3f":"df = pd.concat([df_train, df_test])","9b900335":"df.head()","bd7eef04":"#Just updating the columnn names\ncat_variables =  [ i for i in df.columns if df.dtypes[i]=='object' ]\nnum_variables = [ i for i in df.columns if df.dtypes[i]!='object' ]","496ff3bd":"df.corr()['SalePrice'].sort_values(ascending = False)","79787c4c":"\ndfdup = df_train.copy()\nq1 = df_train.quantile(0.25)\nq3 = df_train.quantile(0.75)\niqr = q3 - q1\ncutoff  = 3*iqr\ncols = df_train\nlower, upper = q1 - cutoff, q3+cutoff\n\ndef TotalOutliers(df, columns, l, u):\n    fin= {}\n    for i in columns:\n        a = df[df[i] > u[i]].shape[0]\n        b = df[df[i] < l[i]].shape[0]\n        fin[i] = a+b\n        a = 0\n        b = 0\n    \n    return fin\n        \noutliers = TotalOutliers(dfdup, num_variables, lower, upper)","8372cb85":"#Printing the number of outliers in the model of each column.\noutliers","46cfafde":"#Droping columns which has more outliers \ndf_train.drop(['BsmtFinType2','ExterCond','BsmtCond','GarageQual','GarageCond','ScreenPorch'], axis = 1,inplace  =True)\ndf_test.drop(['BsmtFinType2','ExterCond','BsmtCond','GarageQual','GarageCond','ScreenPorch'], axis = 1, inplace  =True)\ndf = pd.concat([df_train, df_test])","69e86f17":"# Log tranformation for normal distribution\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])","5fa18fa0":"sns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","07e521b7":"sns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","045dac7f":"df_train['GrLivArea'] = np.log(df_train['GrLivArea'])\ndf_test['GrLivArea'] = np.log(df_test['GrLivArea'])","077c36ef":"sns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","a208e461":"sns.distplot(df_train['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['TotalBsmtSF'], plot=plt)\n","e6397bdc":"df = pd.concat([df_train, df_test])","a37e927a":"#At final you know that we have handled ordinal features but we missed out nominal features so we are going to use One hot encoding..\ndff = pd.get_dummies(df)","024b2f67":"df_tr = dff[dff['SalePrice'].isnull()==False]\ndf_te = dff[dff['SalePrice'].isnull()]","2e0d51b5":"\ndf_te.drop(['SalePrice'], axis = 1, inplace = True)","59219f54":"df_te.shape","81055f25":"df_tr.shape","dadc022b":"#exporting dataset for further use.\ndf_tr.to_csv(\"samp_train.csv\", index=False)\ndf_te.to_csv(\"samp_test.csv\", index=False)","1b482829":"df_tr.head()","8a1068e1":"#We are seperating our training dataset into test and train for cross validation purpose\nx_train, x_test, y_train, y_test = train_test_split(df_tr.drop(['SalePrice'], axis = 1), df_tr['SalePrice'], test_size = 0.2, random_state = 0);","b5bffdf2":" \nalphas = np.array([1,0.1,0.01,0.001,0.0001,0])\nridge = Ridge()\nlaso = Lasso()\nelastic = ElasticNet()\ngrid = GridSearchCV(ridge, dict(alpha=alphas))\ngrid.fit(x_train, y_train)\ngrid1 = GridSearchCV(laso, dict(alpha=alphas))\ngrid1.fit(x_train, y_train)\ngrid2 = GridSearchCV(elastic, dict(alpha=alphas))\ngrid2.fit(x_train, y_train)","ff5cf0a2":"#Ridge Regression Score \nrpred = grid.predict(x_test)\nrpred = rpred.reshape(-1,1)\nprint(\"Ridge Regression Score's:\")\nprint('MAE:', metrics.mean_absolute_error(y_test, rpred))\nprint('MSE:', metrics.mean_squared_error(y_test, rpred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rpred)))\nprint('Average error:',(metrics.mean_absolute_error(y_test, rpred)+metrics.mean_squared_error(y_test, rpred)+ np.sqrt(metrics.mean_squared_error(y_test, rpred)))\/3)","1f195029":"#Lasso Regression Score\nrpred = grid1.predict(x_test)\nrpred = rpred.reshape(-1,1)\nprint(\"Lasso Regression Score's:\")\nprint('MAE:', metrics.mean_absolute_error(y_test, rpred))\nprint('MSE:', metrics.mean_squared_error(y_test, rpred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rpred)))\nprint('Average error:',(metrics.mean_absolute_error(y_test, rpred)+metrics.mean_squared_error(y_test, rpred)+ np.sqrt(metrics.mean_squared_error(y_test, rpred)))\/3)","56bea0f6":"#Elastic Net Regression score.\nrpred = grid2.predict(x_test)\nrpred = rpred.reshape(-1,1)\nprint(\"Elastic Net Regression Score's:\")\nprint('MAE:', metrics.mean_absolute_error(y_test, rpred))\nprint('MSE:', metrics.mean_squared_error(y_test, rpred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rpred)))\nprint('Average error:',(metrics.mean_absolute_error(y_test, rpred)+metrics.mean_squared_error(y_test, rpred)+ np.sqrt(metrics.mean_squared_error(y_test, rpred)))\/3)","0d62ef41":"#The parameter of their models\ngrid.best_params_,grid1.best_params_,grid2.best_params_","2ee6b7e8":"\nparams_rf = {  'bootstrap': [True], 'max_depth': [5, 10, None], 'max_features': ['auto', 'log2'], 'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]}\n\n\nrf = RandomForestRegressor(random_state = 1)\nsearch = GridSearchCV(estimator = rf, param_grid = params_rf, cv = 3 , n_jobs = 1, verbose = 0, return_train_score = True)","9d451f59":"search.fit(x_train, y_train)","5bcf236e":"#Support vector machine regression.\nparameters = {'kernel': ('rbf','poly'), 'C':[1.5, 10],'epsilon':[0.1,0.2,0.5,0.3]}\nsvr = SVR(kernel = 'rbf')\nclf = GridSearchCV(svr, parameters)\nclf.fit(x_train, y_train)","95102a33":"#SVMRegressor Score\nrpred = clf.predict(x_test)\nrpred = rpred.reshape(-1,1)\nprint(\"SVM Regression Score's:\")\nprint('MAE:', metrics.mean_absolute_error(y_test, rpred))\nprint('MSE:', metrics.mean_squared_error(y_test, rpred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rpred)))\nprint('Average error:',(metrics.mean_absolute_error(y_test, rpred)+metrics.mean_squared_error(y_test, rpred)+ np.sqrt(metrics.mean_squared_error(y_test, rpred)))\/3)","f05e6d32":"#Creating model with the parameter predicted by GridSearchCV\nmodel = ElasticNet(alpha = 0.001)\nmodel.fit(df_tr.drop(['SalePrice'], axis = 1), df_tr['SalePrice'])\npr = model.predict(df_te)\npr","de0c865d":"#Applying exponent for the predicted values because we have used log transformation for SalePrice so we have to inverse it..\npr = np.expm1(pr)\npr","fe1a4bc9":"#Submit the prediction\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_submission= pd.DataFrame({'Id':np.asarray(df_test.Id), 'SalePrice':pr})\nsample_submission.to_csv(\"submission.csv\", index=False)","ead4f7e7":"#  5.Feature transformation\nWe are going to transform some important features which contributes more to our model so that they can perform better..<br \/>\n_Applying log transformation it will trandform our data so likely to be normal distribution which is good because<b> most ML algorithms basic assumption is normal distribution.<\/b>_","470a8468":" Try to understand each features and type of the features.<br \/>\n<b>_We know that 'SalePrice' is the target variable_<\/b>\n\n","665e3a8f":"<b> SVM Regressor<\/b>","e8276e22":"# <center>Thank you \ud83d\ude01\ud83d\ude0a<\/center>\n## _Please upvote if you like my work_\n## _If any doubt or any correction please comment below so that i can correct myself_","a3550732":"There are some steps to be followed in creating a Machine learning model \n\n# 1.Data Exploration..\n    In this step we are going to..\n*     understand our dataset.\n*     features and their nature.\n*     Some visualization etc.\n   \n","187201c1":"<b> RandomForestRegressor<\/b>","de352b14":"<b> We are going to do `hyperparametertuning`(Using GridSearchCv) for finding best hyperparameters and create models..","274b64eb":"# 2.Imputation of null values \n<b> _This dataset contains many null values and we are going to set a condition and remove some columns which are not going to be usefull for our model and impute the numerical and categorical variables with median and mode._<\/b> <br\/>\n<b>Note:<\/b> There are many ways to impute for further techniques refer youtube videos.\ud83d\ude0a","5e718df9":"# 4.Handling Outliers\nThere are many ways to handle outliers and also its important to handle outliers it may have a big impact on our models output.<br \/>\n<b>_Here we are removing the values which are outside the interquartile range for further google it...\ud83d\ude09_<\/b>","eff902ec":"<b> Linear models <\/b><br \/>\n_We are going to use linear models such as RidgeRegression , LassoRegression, ElasticNet Regression._<br \/>\n_For now these are as same as LinearRegression Algorithm but with some regularization  our dataset is likely to overfit so linear regression is not better choice_ ","47642220":"# MACHINE LEARNING MODEL \n_We are going to create basic ML model with complete steps from data exploration to submission <b>Let's start....<\/b>_ \ud83d\ude03\n","76d528c9":"#  _Final submission_\n\nNow we have tried out some regression models and <b> ElasticNet Regression<\/b> average score are pretty well so we are going to predict test dataset with this model and submit the prediction...\n","42f986ad":"Applying <b> One Hot Encoding <\/b> for nominal features.","8223bd91":"# 3.Feature Encoding\nMachine learning algorithms can understand and work with numerical data only so we convert the catergorical variables to numerical.<br\/>\n           <b> There are 2 types of catergorical variables.<\/b> <br \/>\n           1.`Ordinal`  -These variables have some order or priorites <br \/>_eg: In this problem there are more columns which describes about quality and conditions they are ordinal._<br\/>\n           2. `Nominal` - These variables doesn't have any order they all have same priorites _eg: Gender_\n           \n            \n        ","9d9a5ad6":"# 5.MODELLING \nNow we have done with all the data preprocessing and feature engineering process and going to create ML MODEL.<br \/>\n_So we are upto create different models and find the best algorithm using cross validation techniques._ \ud83d\ude01"}}