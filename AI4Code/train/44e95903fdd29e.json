{"cell_type":{"15375836":"code","487a3688":"code","bd07d05b":"code","fe8f8fa2":"code","326b2611":"code","cdf8f444":"code","dea5599f":"code","4fdc2d64":"code","fbe3de1e":"code","245f5e23":"code","5f2bc4e7":"code","532b7e5a":"code","c7ed84fc":"code","ced24256":"code","1f7e45aa":"code","405b1ba3":"code","67c1d658":"code","11bb2f1d":"code","b25e5290":"code","cedabccc":"code","6adbec73":"code","529ab3b6":"code","725ed499":"code","a045ce09":"code","6cb5e756":"markdown","d86b8ce0":"markdown","24f97e44":"markdown","3074e3d6":"markdown","210943ff":"markdown","42f42a5c":"markdown","33c98cf1":"markdown","b31d16fc":"markdown","04ff032e":"markdown","98e672d3":"markdown","c0a1047c":"markdown","d0d52217":"markdown","527cd1d4":"markdown","362e8982":"markdown","a23bf394":"markdown"},"source":{"15375836":"import numpy as np \nimport pandas as pd","487a3688":"df1 = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\",lines = True)\ndf2 = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\", lines = True)\n\ndf = pd.concat([df1,df2],sort = False)\ndf.tail()\n\n","bd07d05b":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.countplot(x = \"is_sarcastic\", data = df)\nplt.title(\"Data Distribution\")","fe8f8fa2":"import nltk\nnltk.download('stopwords')","326b2611":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer \n\nstop_words =set(stopwords.words(\"english\"))\nstemmer = PorterStemmer()\ntokenizer = RegexpTokenizer(r'\\w+')\nfrom nltk.stem import PorterStemmer \n\ndef preprocess(text):\n  word_list = []\n  tok = tokenizer.tokenize(text)\n  for word in tok:\n    if word not in stop_words:\n      word_list.append(stemmer.stem(word))\n  return \" \".join(word_list)","cdf8f444":"x_data = df[\"headline\"].apply(preprocess)\nx_data.tail()","dea5599f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_data, df['is_sarcastic'].values, test_size=0.10, random_state=42)","4fdc2d64":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nx_train_tfidf = vectorizer.fit_transform(X_train)\nx_test_tfidf = vectorizer.transform(X_test)\n\ndf_idf = pd.DataFrame(vectorizer.idf_, index=vectorizer.get_feature_names(),columns=[\"idf_weights\"])\n \n\ndf_idf.sort_values(by=['idf_weights']).tail()\n\n","fbe3de1e":"from sklearn.naive_bayes import MultinomialNB\n\nnb=MultinomialNB()\n\nnb.fit(x_train_tfidf, y_train)\nprint(f\"Training Score : {nb.score(x_train_tfidf, y_train)}\")\nprint(f\"Test Score : {nb.score(x_test_tfidf, y_test)}\")","245f5e23":"from  sklearn.metrics import confusion_matrix,classification_report\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test,nb.predict(x_test_tfidf)))\nprint(\"Summary\")\nprint(classification_report(y_test,nb.predict(x_test_tfidf)))","5f2bc4e7":"sns.heatmap(confusion_matrix(y_test,nb.predict(x_test_tfidf)),annot=True,cmap='rainbow')","532b7e5a":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train_tfidf,y_train)\nprint(f\"Training Score : {rf.score(x_train_tfidf, y_train)}\")\nprint(f\"Test Score : {rf.score(x_test_tfidf, y_test)}\")","c7ed84fc":"print(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,rf.predict(x_test_tfidf)))\nprint(\"Summary\")\nprint(classification_report(y_test,rf.predict(x_test_tfidf)))","ced24256":"sns.heatmap(confusion_matrix(y_test,rf.predict(x_test_tfidf)),annot=True,cmap='rainbow')","1f7e45aa":"from keras.models import Model\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense , Dropout , Flatten, LSTM, Input, Embedding\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.layers import BatchNormalization , Activation","405b1ba3":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n\n","67c1d658":"from sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(df[['headline', 'is_sarcastic']], test_size=0.1)  \n\ntraining_sentences = list(train_data['headline'])\ntraining_labels = list(train_data['is_sarcastic'])\n\ntesting_sentences = list(test_data['headline'])\ntesting_labels = list(test_data['is_sarcastic'])\ntraining_labels_final = np.array(training_labels)\ntesting_labels_final = np.array(testing_labels)","11bb2f1d":"vocab_size = 10000 \nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\noov_tok = \"<OOV>\"\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(training_sentences)\npadded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)\n\n","b25e5290":"vocab_size = 10000 \nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\noov_tok = \"<OOV>\"\n#Building LSTM model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(100, kernel_regularizer=tf.keras.regularizers.l1(0.003), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l1(0.003), activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","cedabccc":"num_epochs = 10\nhistory = model.fit(padded, training_labels_final, epochs=num_epochs, batch_size=64, validation_data=(testing_padded, testing_labels_final))","6adbec73":"#Evaluating Accuracy and Loss of the LSTM model\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nplt.show()","529ab3b6":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(100, kernel_regularizer=tf.keras.regularizers.l1(0.003), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l1(0.003), activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","725ed499":"num_epochs = 10\nhistory = model.fit(padded, training_labels_final, epochs=num_epochs, batch_size=64, validation_data=(testing_padded, testing_labels_final))","a045ce09":"#Evaluating Accuracy and Loss of the CNN model\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nplt.show()","6cb5e756":"**Machine Learning**","d86b8ce0":"**Deep Learning**","24f97e44":"**Importing Dataset**","3074e3d6":"**Importing libaries**","210943ff":"**Random Forest Classifier**","42f42a5c":"**Data Preprocessing using NLTK**","33c98cf1":"**Importing Libaries**","b31d16fc":"**Vectorization of features using TF-IDF Vectorizer**","04ff032e":"**Tokenization and Padding**","98e672d3":"**Naive Bayes Classifier**","c0a1047c":"**Long-Short Term Memory(LSTM)**","d0d52217":"**Splitting the dataset into training & test set**","527cd1d4":"**Preprocessing the training and test headlines for the word embedding**","362e8982":"**Visualization**","a23bf394":"**Convolutional Neural Network(CNN)**"}}