{"cell_type":{"4fdeb4d7":"code","eb8e7e44":"code","b083cae6":"code","8319e237":"code","350df480":"code","5b115fe3":"code","94bd2681":"code","79647616":"code","7267d50d":"code","46d29c5f":"code","5a7a11b8":"code","d46c9255":"code","41eecfd0":"code","ee108e4d":"code","a1f03313":"code","f2ad1f69":"code","8af19307":"code","10a23f44":"code","5182fc40":"code","b3a4dbd8":"code","041ea5cd":"code","ed67a189":"code","8974d808":"code","e6c91665":"code","e61e9d54":"code","7de35d1d":"code","4da496bc":"code","a26a8cec":"markdown","9fdc2f58":"markdown","57bd5db5":"markdown","7a016844":"markdown","a7a34186":"markdown","b28eb1cb":"markdown","8b87a4bc":"markdown","ec8bb3de":"markdown","5f767943":"markdown","b1c9c2d3":"markdown","d821b1f4":"markdown","ce3bada9":"markdown","85ebd0bd":"markdown","6b41e55c":"markdown","215f956b":"markdown"},"source":{"4fdeb4d7":"from pathlib import Path\nimport os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, ShuffleSplit \n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nmatplotlib.style.use('ggplot')\nplt.rc('axes', titlesize=18)     \nplt.rc('axes', labelsize=18)\nplt.rc('xtick', labelsize=16)  \nplt.rc('ytick', labelsize=16)\n\nimport eli5\nfrom IPython.display import display_html\n\nPATH_TO_DATA = Path('..\/input\/mlcourse-dota2-win-prediction')\nSEED = 17","eb8e7e44":"y_train = pd.read_csv(PATH_TO_DATA \/ 'train_targets.csv', index_col='match_id_hash')['radiant_win']\ny_train = y_train.map({True: 1, False: 0})","b083cae6":"import pickle as pkl\ntrain_df = pd.read_pickle('..\/input\/dota-2-extra-feat\/items_train.pkl')\ntest_df = pd.read_pickle('..\/input\/dota-2-extra-feat\/items_test.pkl')","8319e237":"train_df.head()","350df480":"# lists for players columns of teams\nradiant_items = [f'r{i}_items' for i in range(1, 6)]\ndire_items = [f'd{i}_items' for i in range(1, 6)]","5b115fe3":"# making list of strings of items for every team, where a string relates to a particular game\nr_temp = train_df[radiant_items].apply(lambda row: ' '.join([' '.join(i) for i in row]), axis=1).tolist()\nd_temp = train_df[dire_items].apply(lambda row: ' '.join([' '.join(i) for i in row]), axis=1).tolist()\nr_temp_test = test_df[radiant_items].apply(lambda row: ' '.join([' '.join(i) for i in row]), axis=1).tolist()\nd_temp_test = test_df[dire_items].apply(lambda row: ' '.join([' '.join(i) for i in row]), axis=1).tolist()","94bd2681":"# function to see difference between sets of unique items\ndef symm_diff(list1, list2):\n    unique1 = set(y for l in [x.split() for x in list1] for y in l)\n    unique2 = set(y for l in [x.split() for x in list2] for y in l)\n    print(len(unique1), len(unique2))\n    \n    return unique1 ^ unique2","79647616":"# difference between teams in train\nsymm_diff(r_temp, d_temp)","7267d50d":"# difference between teams in test\nsymm_diff(r_temp_test, d_temp_test)","46d29c5f":"# normalization of items text\nimport re\nrx = r'{0}[0-9]'.format('river_painter')\nr_items = [re.sub(rx,'river_painter', x.replace('recipe_','')) for x in r_temp]\nd_items = [re.sub(rx,'river_painter', x.replace('recipe_','')) for x in d_temp]\nr_items_test = [re.sub(rx,'river_painter', x.replace('recipe_','')) for x in r_temp_test]\nd_items_test = [re.sub(rx,'river_painter', x.replace('recipe_','')) for x in d_temp_test]","5a7a11b8":"# difference between teams in train\nsymm_diff(r_items, d_items)","d46c9255":"# difference between teams in test\nsymm_diff(r_items_test, d_items_test)","41eecfd0":"# difference between radiants train vs test\nsymm_diff(r_items_test, r_items)","ee108e4d":"# difference between dires train vs test\nsymm_diff(d_items_test, d_items)","a1f03313":"%%time\n#making occuerence matrix of items for every team\nvectorizer = CountVectorizer()\nr = vectorizer.fit_transform(r_items).toarray()\nd = vectorizer.transform(d_items).toarray()\nr_test = vectorizer.transform(r_items_test).toarray()\nd_test = vectorizer.transform(d_items_test).toarray()","f2ad1f69":"r.shape, d.shape, r_test.shape, d_test.shape","8af19307":"# frequence of item calculated dividing by length of datasets\nitems = pd.DataFrame(np.vstack((np.sum(r, axis=0) \/ len(r), np.sum(d, axis=0) \/ len(r))).T, \n                     index=vectorizer.get_feature_names(), \n                     columns=['radiant', 'dire']).sort_values(by=['radiant'], ascending=False)\nitems_test = pd.DataFrame(np.vstack((np.sum(r_test, axis=0) \/ len(r_test), np.sum(d_test, axis=0) \/ len(r_test))).T, \n                     index=vectorizer.get_feature_names(), \n                     columns=['radiant', 'dire']).sort_values(by=['radiant'], ascending=False)","10a23f44":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 15))\ntidy = items.iloc[:30, :].reset_index().rename(columns={'index': 'item'}).melt(id_vars='item').rename(columns=str.title)\nsns.barplot(x='Value', y='Item', hue='Variable', data=tidy, ax=axes[0])\naxes[0].set_title('Top 30 frequent items in train')\ntidy_test = items_test.iloc[:30, :].reset_index().rename(columns={'index': 'item'}).melt(id_vars='item').rename(columns=str.title)\nsns.barplot(x='Value', y='Item', hue='Variable', data=tidy_test, ax=axes[1])\naxes[1].set_title('Top 30 frequent items in test')\n  \n#plt.tick_params(labelsize=14)\nsns.despine(fig)\nfig.tight_layout()","5182fc40":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 15))\ntidy = items.iloc[-30:, :].reset_index().rename(columns={'index': 'item'}).melt(id_vars='item').rename(columns=str.title)\nsns.barplot(x='Value', y='Item', hue='Variable', data=tidy, ax=axes[0])\naxes[0].set_title('Top 30 rare items in train')\naxes[0].legend(loc='lower right')\ntidy_test = items_test.iloc[-30:, :].reset_index().rename(columns={'index': 'item'}).melt(id_vars='item').rename(columns=str.title)\nsns.barplot(x='Value', y='Item', hue='Variable', data=tidy_test, ax=axes[1])\naxes[1].set_title('Top 30 rare items in test')\naxes[1].legend(loc='lower right')\nsns.despine(fig)\nfig.tight_layout()","b3a4dbd8":"#making a dataframe from difference of matricies\ncount_vect_df = pd.DataFrame(r - d, columns=vectorizer.get_feature_names())\n#count_vect_df_test = pd.DataFrame(r_test - d_test, columns=vectorizer.get_feature_names())","041ea5cd":"count_vect_df.tail()","ed67a189":"# consumables to be removed from data (minor impact on advantage of any team expected)\nconsumables = ['tango', 'tpscroll','bottle', 'flask', 'enchanted_mango', 'courier', \n               'clarity', 'faerie_fire', 'ward_observer', 'ward_sentry', 'river_painter']\ncount_vect_df.drop(columns=consumables, inplace=True)\n#count_vect_df_test.drop(columns=consumables, inplace=True)","8974d808":"count_vect_df.shape","e6c91665":"def evaluate(df):\n    train_df_part, valid_df, y_train_part, y_valid = \\\n        train_test_split(df, y_train, test_size=0.25, random_state=SEED)\n    logreg = LogisticRegression(C=1, solver='liblinear', random_state=SEED)\n    c_values = np.logspace(-2, 1.7, 20)\n    shf = ShuffleSplit(n_splits=5, test_size=0.25, random_state=SEED)\n    loggrid = GridSearchCV(estimator=logreg, \n                           param_grid={'C': c_values,\n                                       'penalty': ['l1', 'l2'] \n                                      },\n                           scoring='roc_auc', n_jobs=4, cv=shf, verbose=1)\n    loggrid.fit(train_df_part, y_train_part)\n    print(loggrid.best_score_, loggrid.best_params_)\n    final_model = loggrid.best_estimator_\n    final_model.fit(train_df_part, y_train_part)\n    valid_pred = final_model.predict_proba(valid_df)[:, 1]\n    score = roc_auc_score(y_valid, valid_pred)\n    print('Score on validation set:', score)\n    final_model.fit(df, y_train)\n    res = cross_val_score(final_model, df, y_train, scoring='roc_auc', cv=shf, n_jobs=4, verbose=1)\n    print('Scores on folds:', res)\n    print('Standard deviation:',np.std(res))\n    print('Mean score on whole set:',np.mean(res))\n    features = df.columns.tolist()\n    display_html(eli5.show_weights(estimator=final_model, \n                  feature_names=features, top=80))\n    return final_model","e61e9d54":"classifier = evaluate(count_vect_df)","7de35d1d":"from catboost import CatBoostClassifier, Pool\n\nX_train_part, X_valid, y_train_part, y_valid = train_test_split(count_vect_df, \n                                                                y_train, \n                                                                test_size=0.25, \n                                                                random_state=SEED)\n\ncat_feat_idx=count_vect_df.columns.tolist()\n\ntrain_data = Pool(data=X_train_part, label=y_train_part, cat_features=cat_feat_idx)\nvalid_data = Pool(data=X_valid, label=y_valid, cat_features=cat_feat_idx)\n\n\nparams = {'loss_function':'Logloss', \n          'eval_metric':'AUC',\n          'early_stopping_rounds': 200,\n          'verbose': 200,\n          'random_seed': SEED\n         }\n\ncbclf = CatBoostClassifier(**params) \ncbclf.fit(train_data,\n          eval_set=valid_data, \n          use_best_model=True, \n          plot=True );","4da496bc":"feature_importance_df = cbclf.get_feature_importance(prettified=True) \nplt.figure(figsize=(10, 20)) \nsns.barplot(x=\"Importances\", y=\"Feature Id\", data=feature_importance_df.iloc[0:80, :]) \nplt.title('CatBoost DOTA 2 top-80 items importances:');","a26a8cec":"Now we make something different: create lists of items for teams and vectorize them. But first lets analyze - are there any unique items which are present only in train or test dataset.","9fdc2f58":"We've got space of 152 features.","57bd5db5":"Lets look what we've got:","7a016844":"As we can see, some powerfull artifacts (ultimate_scepter, aegis, manta, radiance, sanga_and_yasha etc.) are very important features from Catboost's view. \n\nThank you for attention.","a7a34186":"Now let's check performance of logistic regression just on hero items feature.","b28eb1cb":"Extracted data looks like this - a list of items for every player:","8b87a4bc":"I used modified approach instead of simple dummy encoding. Difference of radiant and dire matricies allows to reduce the feature space and, afaik, is good for linear models like logistic regression. Also we can see what items bring most advantage independently what team has it (f.e. upgraded or hard earned ones).","ec8bb3de":"# Little addition to Hero items guide\n\n*Even a walking dead man has a gun*","5f767943":"So, we have 163 items, some of them (consumables) we will drop later. Barplots showing frequent and rare items are below.","b1c9c2d3":"I found interesting items like **'river_painter'** and **'courier'**. First one has several names like **'river_painter3'**, **'river_painter6'** etc. Actually, this is a cosmetic item, useless for fighting. Is it significant if someone in team use this for fun? I guess just drop them both, these are rare items in train and test. Also get rid of recipe prefix (presuming it's potentially an item if created by cheats or bought for upgrade), as recipe items are rare enough too.","d821b1f4":"Hero items feature only, with logistic regression trained, could give Public score more than 0.79  :)\n\nWe can expect that feature importance would be different with tree-based algorithms. To use this feature for Catboost, you need to call it catecorical.","ce3bada9":"Distribution of top frequent items for train and test datasets seems very close.","85ebd0bd":"This kernel is an attempt to move a little further in exploration of [Hero items](https:\/\/www.kaggle.com\/grazder\/hero-items-guide). Big chunk of code how to extract feature is the same as in that Guide. It's not shown here, i just loaded 'pickle' files from my kernel.","6b41e55c":"That's how item feature looks like now:","215f956b":"![](http:\/\/s3.amazonaws.com\/criterion-production\/editorial_content_posts\/hero\/5619-\/VnYalPcKGVUK5e7coXKtrwCYYFxR1g_original.jpg)"}}