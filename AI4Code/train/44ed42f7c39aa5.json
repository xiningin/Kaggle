{"cell_type":{"066479ed":"code","82019f0e":"code","25aac3b7":"code","30635a79":"code","8fa48d36":"code","be73a235":"code","1806e47d":"code","4fde8ad7":"code","581a37e4":"code","9f610cf9":"code","f93c0972":"code","339dfa90":"code","33628e01":"code","71c7c18b":"code","0ad28ddb":"code","d96e38dd":"code","bd9f452a":"code","bc37a692":"code","c8f14448":"code","cb64ba58":"code","accd6d9e":"code","f32b7acb":"code","a804744c":"code","d16310e2":"code","40943e95":"code","b41d0b2c":"code","9e477259":"code","6d978f12":"code","61f77ce4":"code","6c23b4df":"code","20bad435":"code","4f224d51":"code","264596bb":"code","0d31a4ff":"code","8db6c2a1":"code","6de1142a":"code","93267b53":"code","8a19c547":"code","66b0f508":"code","19bc4156":"code","c0c52d72":"code","a23f5173":"code","188de4d1":"code","e92ef861":"code","5d612ec7":"code","80e60647":"code","346b078d":"code","f8e578eb":"code","5ac1e2b7":"code","e099fc39":"code","eb51f6b9":"code","64804656":"code","7a74f92b":"code","5cbff9dc":"code","0954c5bc":"code","ae8d7864":"code","28ed696b":"code","585defe9":"code","6989e762":"code","40b5655c":"code","477fe573":"code","88a4a8c2":"code","c1fc80eb":"code","47c67887":"code","828bad3e":"code","9d3d9cec":"code","f09d85ec":"code","f417ad3b":"code","359e674f":"code","53942d30":"markdown","3a719a00":"markdown","763280dc":"markdown","340904a6":"markdown","c7587b99":"markdown","7d2ef8eb":"markdown","fc7911fb":"markdown","8e59fe68":"markdown","f990c185":"markdown","2476c6a4":"markdown","49477389":"markdown","6eb85157":"markdown","a31d82ef":"markdown","083af55e":"markdown","5287cde6":"markdown","3dc8a975":"markdown","1fb37ee7":"markdown","e2ef576d":"markdown","8406c9de":"markdown","237b1528":"markdown","6bf888c8":"markdown","a59e5599":"markdown","f161a1d5":"markdown","779a7919":"markdown","b800d5db":"markdown","755e57e4":"markdown","15834903":"markdown"},"source":{"066479ed":"import copy\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# WordCloud\nfrom wordcloud import WordCloud\n\n\n# preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# models\nfrom sklearn.linear_model import LogisticRegression,LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","82019f0e":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\n# Setting up visualisations\nsns.set_style(style='white') \nsns.set(rc={\n    'figure.figsize':(12,7), \n    'axes.facecolor': 'white',\n    'axes.grid': True, 'grid.color': '.9',\n    'axes.linewidth': 1.0,\n    'grid.linestyle': u'-'},font_scale=1.5)\ncustom_colors = [\"#3498db\", \"#95a5a6\",\"#34495e\", \"#2ecc71\", \"#e74c3c\"]\nsns.set_palette(custom_colors)","25aac3b7":"traindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId') # Train data\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId') # Test data\ntd = pd.concat([copy.deepcopy(traindf), copy.deepcopy(testdf)], axis=0, sort  = False) # Train & Test data\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv') # Form for answers","30635a79":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\n# We check the train sample for balance\ntraindf['Survived'].value_counts(normalize=True)","8fa48d36":"td.nunique()","be73a235":"td.describe()","1806e47d":"#Thanks to:\n# https:\/\/www.kaggle.com\/mauricef\/titanic\n# https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n#\ntd = pd.concat([traindf, testdf], axis=0, sort=False)\ntd['Title'] = td.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ntd['Title'] = td.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ntd['IsWomanOrBoy'] = ((td.Title == 'Master') | (td.Sex == 'female'))\ntd['LastName'] = td.Name.str.split(',').str[0]\nfamily = td.groupby(td.LastName).Survived\ntd['WomanOrBoyCount'] = family.transform(lambda s: s[td.IsWomanOrBoy].fillna(0).count())\ntd['WomanOrBoyCount'] = td.mask(td.IsWomanOrBoy, td.WomanOrBoyCount - 1, axis=0)\ntd['FamilySurvivedCount'] = family.transform(lambda s: s[td.IsWomanOrBoy].fillna(0).sum())\ntd['FamilySurvivedCount'] = td.mask(td.IsWomanOrBoy, td.FamilySurvivedCount - \\\n                                    td.Survived.fillna(0), axis=0)\ntd['WomanOrBoySurvived'] = td.FamilySurvivedCount \/ td.WomanOrBoyCount.replace(0, np.nan)\ntd.WomanOrBoyCount = td.WomanOrBoyCount.replace(np.nan, 0)\ntd['Alone'] = (td.WomanOrBoyCount == 0)\n\n#Thanks to https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster\n#\"Title\" improvement\ntd['Title'] = td['Title'].replace('Ms','Miss')\ntd['Title'] = td['Title'].replace('Mlle','Miss')\ntd['Title'] = td['Title'].replace('Mme','Mrs')\n# Embarked\ntd['Embarked'] = td['Embarked'].fillna('S')\n# Cabin, Deck\ntd['Deck'] = td['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ntd.loc[(td['Deck'] == 'T'), 'Deck'] = 'A'\n\n# Thanks to https:\/\/www.kaggle.com\/erinsweet\/simpledetect\n# Fare\nmed_fare = td.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ntd['Fare'] = td['Fare'].fillna(med_fare)\n#Age\ntd['Age'] = td.groupby(['Sex', 'Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))\n# Family_Size\ntd['Family_Size'] = td['SibSp'] + td['Parch'] + 1\n\n# Thanks to https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis\ncols_to_drop = ['Name','Ticket','Cabin']\ntd = td.drop(cols_to_drop, axis=1)\n\ntd.WomanOrBoySurvived = td.WomanOrBoySurvived.fillna(0)\ntd.WomanOrBoyCount = td.WomanOrBoyCount.fillna(0)\ntd.FamilySurvivedCount = td.FamilySurvivedCount.fillna(0)\ntd.Alone = td.Alone.fillna(0)","4fde8ad7":"target = td.Survived.loc[traindf.index]\ntd = td.drop(['Survived'], axis=1)\ntrain, test = td.loc[traindf.index], td.loc[testdf.index]","581a37e4":"# Thanks to: https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(td.corr(), dtype=np.bool)\n#mask[np.triu_indices_from(mask)] = True\n\nplt.subplots(figsize = (15,12))\nsns.heatmap(td.corr(), \n            annot=True,\n            mask = mask,\n            cmap = 'RdBu', ## in order to reverse the bar replace \"RdBu\" with \"RdBu_r\"\n            linewidths=.9, \n            linecolor='gray',\n            fmt='.2g',\n            center = 0,\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20, pad = 40);","9f610cf9":"# Thanks to: https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\nf,ax=plt.subplots(1,2,figsize=(18,8))\ntraindf['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=traindf,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","f93c0972":"# Thanks to: https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\nf,ax=plt.subplots(1,2,figsize=(18,8))\ntraindf['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass',hue='Survived',data=traindf,ax=ax[1])\nax[1].set_title('Pclass:Survived vs Dead')\nplt.show()","339dfa90":"# Thanks to: https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\nf,ax=plt.subplots(1,2,figsize=(20,10))\ntraindf[traindf['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('Survived= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ntraindf[traindf['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('Survived= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","33628e01":"# Thanks to: https:\/\/www.kaggle.com\/headsortails\/pytanic\ndummy = td[td['Title'].isin(['Mr','Miss','Mrs','Master'])]\nfoo = dummy['Age'].hist(by=dummy['Title'], bins=np.arange(0,81,1))","71c7c18b":"# Thanks to: https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\nf,ax=plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked',data=traindf,ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=traindf,ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=traindf,ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=traindf,ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","0ad28ddb":"td['Family'] = td.Parch + td.SibSp\ntd['Is_Alone'] = td.Family == 0","d96e38dd":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\ntraindf['Fare_Category'] = pd.cut(traindf['Fare'], bins=[0,7.90,14.45,31.28,120], labels=['Low','Mid', 'High_Mid','High'])","bd9f452a":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\np = sns.countplot(x = \"Embarked\", hue = \"Survived\", data = traindf, palette=[\"C1\", \"C0\"])\np.set_xticklabels([\"Southampton\",\"Cherbourg\",\"Queenstown\"])\np.legend(labels = [\"Deceased\", \"Survived\"])\np.set_title(\"Training Data - Survival based on embarking point.\")","bc37a692":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\ntraindf.Embarked.fillna(traindf.Embarked.mode()[0], inplace = True)","c8f14448":"traindf.info()","cb64ba58":"# Clone data for FE\ntrain_fe = copy.deepcopy(traindf)\ntarget_fe = train_fe['Survived']\ndel train_fe['Survived']","accd6d9e":"train_fe = train_fe.fillna(train_fe.isnull())","f32b7acb":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train_fe.columns.values.tolist()\nfor col in features:\n    if train_fe[col].dtype in numerics: continue\n    categorical_columns.append(col)\nindexer = {}\nfor col in categorical_columns:\n    if train_fe[col].dtype in numerics: continue\n    _, indexer[col] = pd.factorize(train_fe[col])\n    \nfor col in categorical_columns:\n    if train_fe[col].dtype in numerics: continue\n    train_fe[col] = indexer[col].get_indexer(train_fe[col])","a804744c":"train_fe.info()","d16310e2":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nX = train_fe\nz = target_fe","40943e95":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)","b41d0b2c":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgb.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)","9e477259":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","6d978f12":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score = pd.DataFrame(train_fe.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()","61f77ce4":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","6c23b4df":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:logistic',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","20bad435":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","4f224d51":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score","264596bb":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Standardization for regression models\ntrain = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(train_fe),\n    columns=train_fe.columns,\n    index=train_fe.index\n)","0d31a4ff":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train_fe, target_fe)\ncoeff_logreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_logreg.columns = ['feature']\ncoeff_logreg[\"score_logreg\"] = pd.Series(logreg.coef_[0])\ncoeff_logreg.sort_values(by='score_logreg', ascending=False)","8db6c2a1":"len(coeff_logreg)","6de1142a":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# the level of importance of features is not associated with the sign\ncoeff_logreg[\"score_logreg\"] = coeff_logreg[\"score_logreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_logreg, on='feature')","93267b53":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Linear Regression\n\nlinreg = LinearRegression()\nlinreg.fit(train_fe, target_fe)\ncoeff_linreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","8a19c547":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# the level of importance of features is not associated with the sign\ncoeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs()","66b0f508":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","19bc4156":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Thanks to https:\/\/www.kaggle.com\/nanomathias\/feature-engineering-importance-testing\n# MinMax scale all importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\n\n# Plot the feature importances\nfeature_score.sort_values('mean', ascending=False).plot(kind='bar', figsize=(20, 10))","c0c52d72":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score.sort_values('mean', ascending=False)","a23f5173":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Create total column with different weights\nfeature_score['total'] = 0.5*feature_score['score_lgb'] + 0.3*feature_score['score_xgb'] \\\n                       + 0.1*feature_score['score_logreg'] + 0.1*feature_score['score_linreg']\n\n# Plot the feature importances\nfeature_score.sort_values('total', ascending=False).plot(kind='bar', figsize=(20, 10))","188de4d1":"#Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score.sort_values('total', ascending=False)","e92ef861":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\ntarget = traindf.Survived.loc[traindf.index]\ntrain, test = td.loc[traindf.index], td.loc[testdf.index]","5d612ec7":"train.head(3)","80e60647":"test.head(3)","346b078d":"target[:3]","f8e578eb":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train.columns.values.tolist()\nfor col in features:\n    if train[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","5ac1e2b7":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n# Encoding categorical features\nfor col in categorical_columns:\n    if col in train.columns:\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values)) ","e099fc39":"train.info()","eb51f6b9":"test.info()","64804656":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n#%% split training set to validation set\nSEED = 100\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.3, random_state=SEED)","7a74f92b":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n# Random Forest\n\nrandom_forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [100, 300]}, cv=5).fit(train, target)\nrandom_forest.fit(train, target)\nY_pred = random_forest.predict(test).astype(int)\nrandom_forest.score(train, target)\nacc_random_forest = round(random_forest.score(train, target) * 100, 2)\nprint(acc_random_forest,random_forest.best_params_)","5cbff9dc":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nrandom_forest_submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})","0954c5bc":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\ndef hyperopt_xgb_score(params):\n    clf = XGBClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_xgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'eta': hp.quniform('eta', 0.025, 0.5, 0.005),\n            'max_depth':  hp.choice('max_depth', np.arange(2, 12, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'subsample': hp.quniform('subsample', 0.5, 1, 0.005),\n            'gamma': hp.quniform('gamma', 0.5, 1, 0.005),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'eval_metric': 'auc',\n            'objective': 'binary:logistic',\n            'booster': 'gbtree',\n            'tree_method': 'exact',\n            'silent': 1,\n            'missing': None\n        }\n \nbest = fmin(fn=hyperopt_xgb_score, space=space_xgb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","ae8d7864":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nparams = space_eval(space_xgb, best)\nparams","28ed696b":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nXGB_Classifier = XGBClassifier(**params)\nXGB_Classifier.fit(train, target)\nY_pred = XGB_Classifier.predict(test).astype(int)\nXGB_Classifier.score(train, target)\nacc_XGB_Classifier = round(XGB_Classifier.score(train, target) * 100, 2)\nacc_XGB_Classifier","585defe9":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nxgb_submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})","6989e762":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(XGB_Classifier,ax = axes,height =0.5)\nplt.show();\nplt.close()","40b5655c":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\ndef hyperopt_gb_score(params):\n    clf = GradientBoostingClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_gb = {\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth': hp.choice('max_depth', np.arange(2, 10, dtype=int))            \n        }\n \nbest = fmin(fn=hyperopt_gb_score, space=space_gb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","477fe573":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nparams = space_eval(space_gb, best)\nparams","88a4a8c2":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n# Gradient Boosting Classifier\n\ngradient_boosting = GradientBoostingClassifier(**params)\ngradient_boosting.fit(train, target)\nY_pred = gradient_boosting.predict(test).astype(int)\ngradient_boosting.score(train, target)\nacc_gradient_boosting = round(gradient_boosting.score(train, target) * 100, 2)\nacc_gradient_boosting","c1fc80eb":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nsubmission_gradient_boosting = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})","47c67887":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nmodels = pd.DataFrame({\n    'Model': ['Random Forest',  'XGBClassifier', 'GradientBoostingClassifier'],\n    \n    'Score': [acc_random_forest, acc_XGB_Classifier, acc_gradient_boosting]})","828bad3e":"models","9d3d9cec":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Plot\nplt.figure(figsize=[15,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['Score'], label = 'Score')\nplt.legend()\nplt.title('Score  for 3 popular models')\nplt.xlabel('Models')\nplt.ylabel('Score, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","f09d85ec":"random_forest_submission.to_csv('submission_RandomForest.csv', index=False)","f417ad3b":"xgb_submission.to_csv('submission_XGB_Classifier.csv', index=False)","359e674f":"submission_gradient_boosting.to_csv('submission_gradient_boosting.csv', index=False)","53942d30":"<a class=\"anchor\" id=\"8\"><\/a>\n## 8. Comparison of the all feature importance diagrams\n##### [Back to Table of Contents](#0.1)","3a719a00":"<a class=\"anchor\" id=\"6\"><\/a>\n## 6. Preparing data for building the feature importance diagrams \n##### [Back to Table of Contents](#0.1)","763280dc":"<a class=\"anchor\" id=\"2\"><\/a>\n## 2. Download datasets \n##### [Back to Table of Contents](#0.1)","340904a6":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## **Table of Contents**\n1. [Import libraries](#1)\n2. [Download datasets](#2)\n3. [Data research](#3)\n4. [FE](#4)\n5. [EDA & Visualization](#5)\n6. [Preparing data for building the feature importance diagrams](#6)  \n7. [FE: building the feature importance diagrams](#7)\n  -  [LGBM](#7.1)\n  -  [XGB](#7.2)\n  -  [Logistic Regression](#7.3)\n  -  [Linear Reagression](#7.4)\n8. [Comparison of the all feature importance diagrams](#8)\n9. [Preparing to modeling](#9)\n  - [Data for modeling](#9.1)\n  - [Encoding categorical features](#9.2)\n10. [Tuning models](#10)\n  -  [Random Forest](#10.1)\n  -  [XGB](#10.2)\n  -  [GradientBoostingClassifier](#10.3)\n11. [Models evaluation](#11)\n12. [Prediction & Output data](#11)","c7587b99":"#### Pclass","7d2ef8eb":"<a class=\"anchor\" id=\"11\"><\/a>\n## 11. Models evaluation\n##### [Back to Table of Contents](#0.1)","fc7911fb":"<a class=\"anchor\" id=\"7.4\"><\/a>\n### 7.4 Linear Reagression\n##### [Back to Table of Contents](#0.1)","8e59fe68":"<a class=\"anchor\" id=\"10.1\"><\/a>\n### 10.1 Random Forest \n##### [Back to Table of Contents](#0.1)","f990c185":"# Titanic with 3 models","2476c6a4":"## **Acknowledgements**\n#### This kernel uses such good kernels:\n   - https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n   - https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n   - https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\n   - https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\n   - https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n   - https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\n   - https:\/\/www.kaggle.com\/headsortails\/pytanic","49477389":"<a class=\"anchor\" id=\"7.1\"><\/a>\n### 7.1 LGBM\n##### [Back to Table of Contents](#0.1)","6eb85157":"<a class=\"anchor\" id=\"10\"><\/a>\n## 10. Tuning models\n##### [Back to Table of Contents](#0.1)","a31d82ef":"<a class=\"anchor\" id=\"7.2\"><\/a>\n### 7.2 XGB\n##### [Back to Table of Contents](#0.1)","083af55e":"<a class=\"anchor\" id=\"9.2\"><\/a>\n### 9.2 Encoding categorical features\n##### [Back to Table of Contents](#0.1)","5287cde6":"<a class=\"anchor\" id=\"10.3\"><\/a>\n### 10.3 GradientBoostingClassifier\n##### [Back to Table of Contents](#0.1)","3dc8a975":"<a class=\"anchor\" id=\"7\"><\/a>\n## 7. FE: building the feature importance diagrams\n##### [Back to Table of Contents](#0.1)","1fb37ee7":"<a class=\"anchor\" id=\"12\"><\/a>\n## 12. Prediction & Output data\n##### [Back to Table of Contents](#0.1)","e2ef576d":"<a class=\"anchor\" id=\"9\"><\/a>\n## 9. Preparing to modeling\n##### [Back to Table of Contents](#0.1)","8406c9de":"<a class=\"anchor\" id=\"1\"><\/a>\n## 1. Import libraries \n##### [Back to Table of Contents](#0.1)","237b1528":"<a class=\"anchor\" id=\"5\"><\/a>\n## 5. EDA & Visualization\n##### [Back to Table of Contents](#0.1)","6bf888c8":"<a class=\"anchor\" id=\"10.2\"><\/a>\n### 10.2 XGB\n##### [Back to Table of Contents](#0.1)","a59e5599":"#### Survived","f161a1d5":"<a class=\"anchor\" id=\"3\"><\/a>\n## 3. Data research\n##### [Back to Table of Contents](#0.1)","779a7919":"<a class=\"anchor\" id=\"7.3\"><\/a>\n### 7.3 Logistic Regression\n##### [Back to Table of Contents](#0.1)","b800d5db":"#### Correlation Between The Features","755e57e4":"<a class=\"anchor\" id=\"9.1\"><\/a>\n### 9.1 Data for modeling \n##### [Back to Table of Contents](#0.1)","15834903":"<a class=\"anchor\" id=\"4\"><\/a>\n## 4. FE \n##### [Back to Table of Contents](#0.1)"}}