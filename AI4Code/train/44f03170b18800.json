{"cell_type":{"6a5b0d4a":"code","73ccd9d4":"code","d98b6cb6":"code","19302449":"code","be261124":"code","6d944116":"code","d47bf88e":"code","368d6992":"code","62f5bc09":"code","fcb44893":"code","5412bf09":"code","d910e020":"code","f8ed57b9":"code","404acae7":"code","014c4668":"code","fec9f3f0":"code","740c9acf":"code","ffa9cfa3":"code","3fd61581":"code","b34bee91":"code","a6cc0441":"code","338d72c3":"code","41664006":"code","1e2cd7f4":"code","19f3a585":"code","019806d7":"code","b9c5e1b8":"markdown","126c55f2":"markdown","2641cd4f":"markdown","0e95b67a":"markdown","596d7b71":"markdown","f31ef9d3":"markdown","c63f2f43":"markdown","8a5f1c0a":"markdown","4f9dfdf8":"markdown","78dd08c7":"markdown","4e910214":"markdown","b9e07213":"markdown","2459020f":"markdown","f77ce28f":"markdown","c1112667":"markdown","b43af0c9":"markdown","d2284623":"markdown","86010dc8":"markdown","e213084e":"markdown","9cfd80cc":"markdown","2d4c6799":"markdown"},"source":{"6a5b0d4a":"# Importing required libraries\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","73ccd9d4":"mnist_train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nmnist_test = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","d98b6cb6":"# Print mnist training dataset\n\nmnist_train","19302449":"# Print mnist test dataset\n\nmnist_test","be261124":"# For x_train, we just need to drop the \"labels\" column\n# Axis is by default 0 in .drop meaning that by default row gets dropped\n# Axis = 1 here means that we're dropping a column\n\nx_train = mnist_train.drop(labels = \"label\", axis = 1)\nx_train","6d944116":"# For y_train we just need the \"labels\" column\n\ny_train = mnist_train[\"label\"]\ny_train","d47bf88e":"x_train = x_train\/255.0\nmnist_test = mnist_test\/255.0","368d6992":"print(x_train.shape)\nprint(y_train.shape)","62f5bc09":"print(mnist_test.shape)","fcb44893":"# -1 means that we want to keep all the values, that is the initial size intact\n# 28x28x1 means 28 rows, 28 columns and 1 channel\n# The 1 channel will have values ranging from 0 to 1\n# For RGB, we would have had a 3 channel system \n\nx_train = x_train.values.reshape(-1,28,28,1)\nmnist_test = mnist_test.values.reshape(-1,28,28,1)\nx_train.shape","5412bf09":"y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)","d910e020":"print(y_train[3])","f8ed57b9":"datagen = ImageDataGenerator(rotation_range=12, zoom_range = 0.15, width_shift_range=0.15, height_shift_range=0.13) \n\ndatagen.fit(x_train)\ndatagen.fit(x_test)","404acae7":"num = 20\nimages = x_train[:num]\n\nnum_row = 5\nnum_col = 4\n\n# plot images\nfig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\nfor i in range(num):\n    ax = axes[i\/\/num_col, i%num_col]\n    ax.imshow(images[i].reshape(28,28), cmap='viridis')\n    ax.set_title('Label: {}'.format(y_train[i].tolist().index(1.0, 0, 10)))\nplt.tight_layout()\nplt.show()","014c4668":"x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.15)\nprint(x_train.shape)\nprint(x_val.shape)","fec9f3f0":"input_layer = tf.keras.layers.Input(shape = (28, 28, 1))\n\nhidden_layer_1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=tf.keras.activations.relu)(input_layer)\nnorm_1 = tf.keras.layers.BatchNormalization()(hidden_layer_1)\nmax_1 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2))(norm_1)\ndropout_layer_1 = tf.keras.layers.Dropout(0.25)(max_1)\n\n# hidden_layer_2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=tf.keras.activations.relu)(dropout_layer_1)\n# norm_2 = tf.keras.layers.BatchNormalization()(hidden_layer_2)\n# max_2 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2))(norm_2)\n\nflatten_layer = tf.keras.layers.Flatten()(dropout_layer_1)\n# dropout_layer_2 = tf.keras.layers.Dropout(0.25)(flatten_layer)\n\nhidden_layer_3 = tf.keras.layers.Dense(256, activation=tf.keras.activations.relu)(flatten_layer)\nhidden_layer_4 = tf.keras.layers.Dense(128, activation=tf.keras.activations.relu)(hidden_layer_3)\n\ndropout_layer_3 = tf.keras.layers.Dropout(0.5)(hidden_layer_4)\n\noutput_layer = tf.keras.layers.Dense(10, activation=tf.keras.activations.sigmoid)(dropout_layer_3)","740c9acf":"model = tf.keras.Model(inputs = input_layer, outputs = output_layer)\nmodel.summary()","ffa9cfa3":"tf.keras.utils.plot_model(model, \"digit_classifier_model.png\", show_shapes=True)","3fd61581":"# Here, we are using Adam optimiser\n# The loss function is categorical crossentropy function \n\nmodel.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.CategoricalCrossentropy(), metrics = ['accuracy'])","b34bee91":"his = model.fit(x_train, y_train, batch_size = 1000, epochs = 40, validation_data=(x_val, y_val))","a6cc0441":"plt.plot(his.history['accuracy'])\nplt.plot(his.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.show()","338d72c3":"plt.plot(his.history['loss'])\nplt.plot(his.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","41664006":"# Lets plot the confusion matrix \n\ndef plot_confusion_matrix(cm, classes,title='Confusion matrix',cmap=plt.cm.viridis):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"black\" if cm[i, j] > thresh else \"white\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\ny_pred = model.predict(x_val)\n# Convert predictions classes to one hot vectors \ny_pred_classes = np.argmax(y_pred,axis = 1) \n# Convert validation observations to one hot vectors\ny_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_true, y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","1e2cd7f4":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (y_pred_classes - y_true != 0)\n\ny_pred_classes_errors = y_pred_classes[errors]\ny_pred_errors = y_pred[errors]\ny_true_errors = y_true[errors]\nx_val_errors = x_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \n    n = 0\n    nrows = 4\n    ncols = 4\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True, figsize=(10,10))\n    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\ny_pred_errors_prob = np.max(y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(y_pred_errors, y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 16 errors \nmost_important_errors = sorted_dela_errors[-16:]\n\n# Show the top 16 errors\ndisplay_errors(most_important_errors, x_val_errors, y_pred_classes_errors, y_true_errors)","19f3a585":"# predict results\nresults = model.predict(mnist_test)\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","019806d7":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"mnist_result.csv\",index=False)\nsubmission","b9c5e1b8":"# Visualize the Dataset\nHere, we plot the first 20 entries of the dataset using matplotlib library","126c55f2":"# Normalization\nNormalization is required to bring out dataset to a **common scale**","2641cd4f":"# Importing the necessarry Libraries for Data Analysis\nWe'll be using the following libraries to perform data analysis ","0e95b67a":"# Plot of model graph","596d7b71":"# Convolutional Neural Network using Keras\nHere, we build our learning model, which will be a convolutional neural network","f31ef9d3":"# Loading data and other preparations\nWe've loaded the data into training and testing datasets <br>\nFurther we've loaded the input and output (class) dataset as well","c63f2f43":"# Training and Validation set\nSplit the training data into training and validation sets","8a5f1c0a":"## Plotting model loss\nWe plot the model loss for both training and testing data","4f9dfdf8":"I have looked up the web for a lot of help regarding my code, especially the confusion matrix and error display part, so it may appear somewhat similar for that matter <br>\nNevertheless, I was surprised as to how much one can learn from a very basic dataset such as MNIST and it was surely a nice way to recapitulate the concepts I've studied","78dd08c7":"# Understanding the Dataset\nWe clearly see that there are **42,000 training examples** and **28,000 testing examples** with a total of **60,000 grayscale data** as a part of the dataset <br>\nAlso, the images are of **28x28 pixels** flattened out into **an array of size 784**","4e910214":"# Introduction\n### MNIST dataset\nHandwritten digits dataset <br> \nOur goal here is to correctly identify digits from handwritten digits","b9e07213":"# Final Result\nSo here is the final result of our model :)","2459020f":"# Design and Target\nWe'll now split the training and testing datasets into **design and target** <br>","f77ce28f":"# Loading the model\nHere, we load our CNN with optimizer, loss function and evaluation metric","c1112667":"# Data Augmentation\nIt is performed in order to increase our dataset <br>\nCommonly used methods are, random flipping, rotation, cropping, etc of images","b43af0c9":"## Plotting model accuracy\nWe plot the model accuracies for both training and testing data","d2284623":"# Resizing the 1D matrix into a 3D matrix\nHere, we are transforming the **1D matrix** of size **784** to a **3D matrix** of size **28x28x1**","86010dc8":"# Displaying the error results and further analysis\nSo here, we are trying to display the digits which our model failed to recognize correctly <br>\nSome of these are very trivial mistakes while the later ones are quite genuine and any human can easily mistake them for other digits just like our model did here","e213084e":"# One Hot Encoding\nHere, we'll convert our labels in y_train as a one-hot-encoded vector","9cfd80cc":"# Confusion Matrix\nA confusion matrix can very well describe the performance of a classification model <br>\nWe'll plot one for our CNN model here","2d4c6799":"# Training our model\nHere, we are training our **convolutional neural network** on the testing and validation data"}}