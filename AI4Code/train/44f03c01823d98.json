{"cell_type":{"9ee87567":"code","180ce1fd":"code","f531a393":"code","cde35b40":"code","d91ec5e1":"code","f2f8ba06":"code","0c3ad1c1":"code","1d0346a8":"code","f0e245d7":"code","f175efbd":"code","ab26a4d9":"code","7bb39994":"code","81b93d73":"code","53f4e0b3":"code","3f83ed46":"code","f44c521d":"markdown","ff89d1e3":"markdown","67e118e0":"markdown","eb370b66":"markdown","7b213aef":"markdown"},"source":{"9ee87567":"import os\nimport numpy as np\nimport pandas as pd\nimport random\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'","180ce1fd":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential, Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Dense, Flatten, Input, Concatenate, Dropout\nfrom tensorflow.keras.utils import plot_model\n\nimport tensorflow_addons as tfa\n\nimport keras_tuner as kt","f531a393":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nSEED = 3024","cde35b40":"def gelu(x):\n    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 \/ np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n\nclass Mish(keras.layers.Activation):\n    '''\n    Mish Activation Function.\n    see: https:\/\/github.com\/digantamisra98\/Mish\/blob\/master\/Mish\/TFKeras\/mish.py\n    .. math::\n        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n    Shape:\n        - Input: Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n        - Output: Same shape as the input.\n    Examples:\n        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n    '''\n\n    def __init__(self, activation, **kwargs):\n        super(Mish, self).__init__(activation, **kwargs)\n        self.__name__ = 'Mish'\n\ndef mish(inputs):\n    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n\nkeras.utils.get_custom_objects().update({'gelu': keras.layers.Activation(gelu)})\nkeras.utils.get_custom_objects().update({'mish': Mish(mish)})","d91ec5e1":"def plot_history(history, start=0, fold=0):\n    epochs = np.arange(len(history.history['loss']))\n    plt.figure(figsize=(15,5))\n    plt.plot(epochs[start:], history.history['loss'][start:], \n             label='train', color='blue')\n    plt.plot(epochs[start:], history.history['val_loss'][start:], \n             label='validation', color='red')\n    plt.ylabel('Loss',size=14)\n    plt.title(f\"Fold: {fold+1}\")\n    plt.legend()\n    plt.show()","f2f8ba06":"# Loading data\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\n\n# Preparing training data\nX = df_train.drop(columns=['id','target']).copy()\ny = df_train['target'].copy()\n\n# Data Standardization\nscaler = MinMaxScaler(feature_range=(0, 1))\nX = pd.DataFrame(columns=X.columns, data=scaler.fit_transform(X))\n\ndel(df_train)","0c3ad1c1":"### define callbacks\nearly_stopping = EarlyStopping(\n    monitor='val_auc', \n    min_delta=0, \n    patience=5, \n    verbose=0,\n    mode='min', \n    baseline=None, \n    restore_best_weights=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_auc', \n    factor=0.2,\n    patience=2,\n    mode='min'\n)","1d0346a8":"def tunable_model(hp):\n    \n    layer_1_size = hp.Int('layer_1_size', min_value=2, max_value=512, step=2)\n    layer_2_size = hp.Int('layer_2_size', min_value=2, max_value=512, step=2)\n    layer_3_size = hp.Int('layer_3_size', min_value=2, max_value=512, step=2)\n    layer_4_size = hp.Int('layer_4_size', min_value=2, max_value=512, step=2)\n    layer_5_size = hp.Int('layer_5_size', min_value=2, max_value=512, step=2)\n    \n    hidden_layers = hp.Int('hidden_layers', min_value=1, max_value=5, step=1)\n    dense_dropout = hp.Float('dense_dropout', min_value=0, max_value=0.5, step=0.05)\n    skip_layers = hp.Boolean('skip_layers')\n    \n    activation = hp.Choice('activation', values=['relu', 'swish', 'elu', 'mish', 'gelu'])\n    \n    inputs_sequence = Input(shape=(X.shape[1]))\n    x = Flatten()(inputs_sequence)\n\n    skips = list()\n    layers = [layer_1_size, layer_2_size, layer_3_size, layer_4_size,layer_5_size][:hidden_layers]\n    \n    for layer, nodes in enumerate(layers):\n        x = Dense(nodes, activation=activation)(x)\n        x = Dropout(dense_dropout)(x)\n        if layer != (len(layers) - 1) and skip_layers is True:\n            skips.append(x)\n    \n    if len(skips) > 0:\n        x = Concatenate(axis=1)([x] + skips)\n        \n    output_class = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=inputs_sequence, outputs=output_class)\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.001),\n        loss='binary_crossentropy',\n        metrics=['AUC'])\n    \n    return model","f0e245d7":"try:\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    # instantiate a distribution strategy\n    tf_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"Running on TPU:\", tpu.master())\nexcept:\n    tf_strategy = tf.distribute.get_strategy()\n    print(f\"Running on {tf_strategy.num_replicas_in_sync} replicas\")\n    print(\"Number of CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n    print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","f175efbd":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)","ab26a4d9":"with tf_strategy.scope():\n    tuner = kt.BayesianOptimization(hypermodel=tunable_model,\n                                    objective=kt.Objective(\"auc\", direction=\"max\"),\n                                    max_trials=60,\n                                    num_initial_points=3,\n                                    directory='storage',\n                                    project_name='tps11',\n                                    seed=101)\n\n    tuner.search(X_train, y_train, \n                 epochs=1000,\n                 batch_size=1024, \n                 validation_data=(X_valid, y_valid),\n                 shuffle=True,\n                 verbose=1,\n                 callbacks = [early_stopping, reduce_lr]\n                 )","7bb39994":"best_hps = tuner.get_best_hyperparameters()[0]\nmodel = tuner.hypermodel.build(best_hps)","81b93d73":"print(best_hps.values)","53f4e0b3":"model.summary()","3f83ed46":"plot_model(\n    model, \n    to_file='baseline.png', \n    show_shapes=True,\n    show_layer_names=True\n)","f44c521d":"Building upon the very good notebook by https:\/\/www.kaggle.com\/mlanhenke (https:\/\/www.kaggle.com\/mlanhenke\/tps-11-nn-baseline-keras) which presents a very clean and easy to understand structure, I prepared some a few variations to help furthermore the participants of this competition.","ff89d1e3":"The improvements upsofar are:\n\n* deterministic random seeding\n* enabling TPU usage just by switching accelerator type\n* adding mish and gelu activations\n* model summary and plot\n","67e118e0":"In this notebook I introduce how to tune the architecture of a neural network (NAS) using KerasTuner.\n\nKerasTuner (https:\/\/keras.io\/keras_tuner\/) has been announced as a \u201cflexible and efficient hyperparameter tuning for Keras models\u201d by Francois Chollet, the creator of Keras.Francois Chollet also created a series of notebooks for Kaggle competitions in order to showcase the workings and functionalities of KerasTuner:\n\n* https:\/\/www.kaggle.com\/fchollet\/keras-kerastuner-best-practices for the Digit Recognizer datasets\n\n* https:\/\/www.kaggle.com\/fchollet\/titanic-keras-kerastuner-best-practices for the Titanic dataset\n\n* https:\/\/www.kaggle.com\/fchollet\/moa-keras-kerastuner-best-practices for the Mechanisms of Action (MoA) Prediction competition\n\nThe recipe proposed by Chollet for running KerasTuner is made of simple steps starting from your existing Keras model:\n* Wrap your model in a function with hp as first parameter\n* Define hyper-parameters at the beginning of the function\n* Replace DNN static values with hyper-parameters\n* Create block branches as hyper-parameters (alternative paths)\n* Define dynamically hyper-parameters as you build the network","eb370b66":"In the follwing notebook you can find all this implemented in a easy way. Just replicate it and change the *tunable_model* function accordingly to your desired experiments.","7b213aef":"How the network is organized in layers and the details of its architecture can make even more difference. In fact, technically speaking, architecture implies the representational capacity of the deep neural network, which means that depending on the layers you use the network will be able or not to read and process all the information available in the data.\n\nIt is said that common best practices for great deep learning practitioners to assemble performing DNNs depend mainly on:\n\n* Relying on pre-trained (so you have to be very knowledgeable about the solutions around)\n* Reading cutting-edge papers\n* Coping from top Kaggle Kernels, also from previous competitions\n* Trial and error\n* Ingenuity & luck\n\nActually, a famous lesson by Prof. Geoffrey Hinton (for video see: https:\/\/www.youtube.com\/watch?v=i0cKa0di_lo and https:\/\/www.cs.toronto.edu\/~hinton\/coursera\/lecture16\/lec16.pdf for the presentation ) clarifies that you can achieve similar and often better results using automated methods such as Bayesian optimization. Using Bayesian optimization will also avoid you failing struck because you cannot figure out the best combinations of hyper-parameters among so many possible combinations of values."}}