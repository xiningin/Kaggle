{"cell_type":{"992fb0dc":"code","e66c31c0":"code","ed6eaf94":"code","ce6e2ab9":"code","508f9d92":"code","f2315a6b":"code","d1f16128":"code","8a5d7257":"code","4be7bec8":"code","c163f991":"code","59c780d1":"code","ce638e7a":"code","a939176a":"markdown","7ee8f2a4":"markdown","ac67df26":"markdown","817c890b":"markdown","473fe22f":"markdown","be0d83d6":"markdown","05e55f85":"markdown","42b47527":"markdown","9ff9f6e8":"markdown","d1ec2b56":"markdown","63b11666":"markdown","4f4ea81b":"markdown","86a22be2":"markdown"},"source":{"992fb0dc":"import sys\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV, ShuffleSplit\n\nwarnings.simplefilter('ignore')\nsns.set_theme()","e66c31c0":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        df = pd.read_csv(os.path.join(dirname, filename))\ndf.head()","ed6eaf94":"print('\\n', 'num_features:', len(df.columns), '\\n',\n      'dataset size:', df.shape, '\\n', 'Dtype:', df.dtypes)","ce6e2ab9":"plt.figure(figsize=(20, 3))\nsns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='ocean_r')","508f9d92":"num_col = df._get_numeric_data().columns.tolist()\ncat_col = set(df.columns) - set(num_col)","f2315a6b":"plt.figure(figsize=(10, 10))\nfor i, j in enumerate(cat_col):\n    plt.subplot(6, 3, i+1)\n    sns.violinplot(x=df[j], y=df['absences'])\nplt.tight_layout()","d1f16128":"lb = LabelEncoder()\nfor i in cat_col:\n    df[i] = lb.fit_transform(df[i])","8a5d7257":"X = (df.drop(columns=['G1', 'G2', 'G3'], axis=0)).values\ny = (df.iloc[:, -3:]).values\n\n# X = np.ascontiguousarray(X, dtype=np.float64)\n# y = y.astype('float64')\n\nprint(X.shape, y.shape)","4be7bec8":"def ProgressBar(percent, barLen=20):\n    sys.stdout.write(\"\\r\")\n    progress = \"\"\n    for i in range(barLen):\n        if i < int(barLen * percent):\n            progress += \"=\"\n        else:\n            progress += \" \"\n    sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n    sys.stdout.flush()","c163f991":"ncv = 10\nRMSE = np.zeros((ncv, y.shape[1]))\npred = np.zeros_like(y)\n\n\nk = ShuffleSplit(n_splits=ncv, test_size=0.3, random_state=1)\n\nfor cv_i, (train_index, test_index) in enumerate(k.split(X, y)):\n    x_train, x_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    model = MLPRegressor(solver='adam',\n                         hidden_layer_sizes=(100,))\n    \n    pipe = Pipeline([('scaler', StandardScaler()), ('reg', model)])\n\n    param_grid = {'reg__hidden_layer_sizes': bernoulli(0.3)}\n\n    clf = RandomizedSearchCV(estimator=pipe,\n                            param_distributions=param_grid,\n                            scoring='r2', refit=True, cv=10, verbose=True)\n    \n    clf.fit(x_train, y_train)\n\n    pred[test_index] = clf.predict(x_test)\n\n    RMSE[cv_i, :] = np.sqrt(np.average(\n        (y_test - pred[test_index])**2, axis=0))\n\n    ProgressBar(cv_i\/abs((ncv)-1), barLen=ncv+1)\n\nresult = np.mean(RMSE, axis=0)","59c780d1":"RMSE","ce638e7a":"plt.plot(np.mean(RMSE, axis=1))\nplt.title('Average of RMSE for three outputs')\nplt.xlabel('folds')\nplt.ylabel('test error')","a939176a":"# Feature engineering\nCheck the numerical and non numerical features","7ee8f2a4":"# one-hot encoding\nTransform the categorical variables","ac67df26":"# Check the RMSE for all the outputs","817c890b":"## data info","473fe22f":"# Importing the data","be0d83d6":"# Training the shallow Neural Network\nTraining the shallow Neural Network for multi-output regression.\n\nIn the following, I used shuffle split as my cross-validation method with 10-folds. The reason which I used this method is the matter of training speed.\n\nThe model is MLPRegressor with 100 Neurons and one hidden layer. I used Adam as the optimizer due to its coverage rates.\n\n```Python\n MLPRegressor(solver='adam', hidden_layer_sizes=(100,))\n ```\nAs I was using the NN, I had to normalize my data, so I preferred a pipeline to push my data through the pipeline. \n\nFor tunning the hyperparameters, I rathered to use the `RandomizedSearchCV` this will let me use the statistical distribution for my grid instead of using the constant values, which I believe is better due to the training speed and performance.\n\nThe scoring function for ranking the folds is `r2` as it takes to account all outputs.\nFor the final result, I calculated the Root Mean Square Error RMSE for all outputs manually.\n\nAuthor: Seyedsaman (Sam) Emami","05e55f85":"If you want to have this method or use the outputs of the notebook, you can fork the Notebook as following (copy and Edit Kernel).\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1101107%2F8187a9b84c9dde4921900f794c6c6ff9%2FScreenshot%202020-06-28%20at%201.51.53%20AM.png?generation=1593289404499991&alt=media\" alt=\"Copyandedit\" width=\"300\" height=\"300\" class=\"center\">\n\nI tried to keep everything as simple as possible, if you have any doubts or questions, please feel free to ask in the comments.","42b47527":"# Missing values","9ff9f6e8":"# About this notebook\n#### Author: Seyedsaman Emami\n\n<hr>\n\n|**problem**|**Dataset**|\n|-:|-:|\n|Multi-output regression|student performance|","d1ec2b56":"# Defin the inputs","63b11666":"# Define the customized progress bar","4f4ea81b":"# Quick EDA on categorical features","86a22be2":"# Ploting the RMSE average\nAverage of EMSE (over three outputs) during the cross-validation"}}