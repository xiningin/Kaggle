{"cell_type":{"1b0cf658":"code","196c3f0f":"code","a143df62":"code","03ef1bfa":"code","2816d298":"code","f4f72487":"code","e7c3054c":"code","29501f8a":"code","fe666223":"code","b22b0889":"code","53c9f1b5":"code","919b31b0":"code","ad98ab6d":"code","952b9094":"code","899239f9":"code","4cbcc152":"code","be8e68b2":"code","e30b3a17":"code","bac64e00":"code","706813dd":"code","71be7cb5":"code","5bb73eec":"code","09f8ef18":"code","f52b967d":"markdown","92b324ee":"markdown","0e35906b":"markdown","f7ed73fc":"markdown","15a5fe5e":"markdown","00b708be":"markdown","3bb16147":"markdown","6ae4d3c0":"markdown","e5ff63dd":"markdown","3285dd8b":"markdown","3e0e7d73":"markdown","87496f6d":"markdown"},"source":{"1b0cf658":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for plots\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","196c3f0f":"# load data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nprint(train_data.head(6))\nprint(train_data.shape)\n# note: 12 variables, 11 predictors and 1 response, training set has 891 observations\n# PassenegrId will likely contribute nothing to accuracy, Pclass, sex, age, sibsp, parch,\n# and fare are expected to be the most important predictors,\n# this may change with further examination\n# Survival: 1=yes, 0 = no","a143df62":"# load the test data\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nprint(test_data.head())\nprint(test_data.shape)\n# notes: contains all the same variables as the train,\n# save for the Survived variable. this is what should be expected\n# 418 in the testing set, about half the training set, \n# which is a good compromise for\n# a relatively small data set.\n# Neural nets likely will not yeild good results, \n# opt for simpler, more efficient, models\n# I'm thinking random forest","03ef1bfa":"# this is the code from the tutorial that trains \n# only on gender, one of the\n# variables I suspect might contribe some information, \n# but not be highly reliable compared\n# to using the full data set\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n# rate is 0.7420382165605095, about 75%","2816d298":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)\n# rate is 0.18890814558058924, about 19%\n# high enough difference that sex might \n# be the strongest predictor of survival.\n# going with just sex might produce good, \n# but not great results","f4f72487":"# code from tutorial, with comments for comprehension\nfrom sklearn.ensemble import RandomForestClassifier # get random forest function from library\n\ny = train_data[\"Survived\"] # resposne variable all on its own\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"] # selecting only four features for now\n# can add or subtract later\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n# initializing the random forest, clearing land for it, if you will\nmodel.fit(X, y) # fit the model based on the selected data, this is where most of the computation\n# occurs for the model\npredictions = model.predict(X_test) # make prediction\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False) # formatting the predictions into the a data frame\n# with just PassengerId and survival prediction\n# print(\"Your submission was successfully saved!\")\n\n# commented out as this is not going to be the real final \n# submissions. For learning purproses I have\n# submitted the suggested random forest to achieve \n# a score of 0.77511 on August 7, 2021.\n# This is a decent first step with four predictors","e7c3054c":"print(train_data.columns)","29501f8a":"print(train_data.isna().sum())\n# 177 NAs for Age, 687 for Cabin and 2 for embarked\nprint(\"\")\nprint(test_data.isna().sum())","fe666223":"# Ticket class\nplt.figure(figsize=(10,6))\nsns.countplot(data = train_data,\n           x = \"Pclass\", hue = \"Survived\")\nplt.title(\"Survival by Ticket Class\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Ticket Class\")","b22b0889":"# Sex\nplt.figure(figsize=(10,6))\nsns.countplot(data = train_data,\n           x = \"Sex\", hue = \"Survived\")\nplt.title(\"Survival by Sex\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Sex\")","53c9f1b5":"# Age\nplt.figure(figsize=(10,6))\nsns.displot(data = train_data, multiple = \"dodge\",\n           x = \"Age\", hue = \"Survived\")\nplt.title(\"Survival by Age\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Age\")","919b31b0":"# Number of Siblings\/spouses (SibSp)\nplt.figure(figsize=(10,6))\nsns.countplot(data = train_data,\n           x = \"SibSp\", hue = \"Survived\")\nplt.title(\"Survival by Number of Siblings or Spouses Aboard\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Number of Siblings or Spouses Aboard\")","ad98ab6d":"# Number of Parents\/children (parch)\nplt.figure(figsize=(10,6))\nsns.countplot(data = train_data,\n           x = \"Parch\", hue = \"Survived\")\nplt.title(\"Survival by Number of Parents or Children Aboard\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Number of Parents or Children Aboard\")","952b9094":"# Fare (fare)\nplt.figure(figsize=(10,6))\nsns.displot(data = train_data,\n           x = \"Fare\", hue = \"Survived\", kind = \"kde\")\nplt.title(\"Survival by Fare\")\nplt.ylabel(\"\")\nplt.xlabel(\"Fare\")","899239f9":"plt.figure(figsize=(10,6))\nsns.countplot(data = train_data,\n           x = \"Embarked\", hue = \"Survived\")\nplt.title(\"Survival by Destination Embarked from\")\nplt.ylabel(\"\")\nplt.xlabel(\"Embarked\")","4cbcc152":"# to do correlation properly, some categorical variables should be \n# converted to numeric\ntrain_data_corr = train_data\ntrain_data_corr['Sex'] = (train_data_corr['Sex'] == 'male').astype(int)","be8e68b2":"corr = train_data_corr.corr()\nplt.figure(figsize = (10, 6))\nsns.heatmap(corr.iloc[1:,1:], annot= True)\n# print(corr) commented out because sns.heatmap has a parameter\n# for printing correlation","e30b3a17":"# first do something about the missing variables. Ages has 177\n# and embarked has 2\ntrain_mod1 = train_data # create new data for model that handles missing values\n# for quick results, just put all missing values as from \"S,\" the most common port of departure\ntrain_mod1[\"Embarked\"] = train_data[\"Embarked\"].replace(np.nan, \"S\")\ntrain_mod1[\"Age\"] = train_data[\"Age\"].replace(np.nan, 0)\n# train_mod1.head(6) # to ensure that the na on age (for Mr. James Moran)\n# has been successfully repalced\n\n# do the same for the test data\ntest_mod1 = test_data # create new data for model that handles missing values\n# for quick results, just randomly select from the non missing values\ntest_mod1[\"Embarked\"] = test_data[\"Embarked\"].replace(np.nan, \"S\")\ntest_mod1[\"Age\"] = test_data[\"Age\"].replace(np.nan, np.mean(test_data[\"Age\"]))","bac64e00":"# get features, have to concatinate the data\ntrain_mod1['label'] = 'train'\ntest_data['label'] = 'test'\n# \nconcat_data = pd.concat([train_mod1, test_data])\nfeatures_df = pd.get_dummies(concat_data, columns=[\"Pclass\", \"Sex\", \"Embarked\"])\n\nfeatures = [\"Pclass_1\", \"Pclass_2\", \"Pclass_3\", \"Sex_0\", \"Sex_1\",\n            \"Sex_male\", \"Sex_female\", \"Age\", \"Parch\", \n            \"Embarked_C\", \"Embarked_Q\", \"Embarked_S\"]\n\nX_1 = features_df[features_df['label'] == 'train']\nX_test_1 = features_df[features_df['label'] == 'test']\n\n# drop label, PassengerId, Name\nX_1 = X_1[features]\nX_test_1 = X_test_1[features]","706813dd":"X_1.head()","71be7cb5":"#features = [\"Pclass\", \"Sex\", \"Age\", \"Parch\", \"Embarked\"]\n#X = pd.get_dummies(train_mod1[features])\n#X_test = pd.get_dummies(test_data[features])\n\nmodel1 = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel1.fit(X_1, y)   # commented out because it causes a problem until missing data is handled\npredictions1 = model1.predict(X_test_1) # RESUME LATER WITH THE TAB I LINKED, this is data cleaning\n\n#output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions1})\n#output.to_csv('my_submission.csv', index=False) # formatting the predictions into the a data frame\n# with just PassengerId and survival prediction\n#print(\"Your submission was successfully saved!\")","5bb73eec":"from sklearn.linear_model import LogisticRegression\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n# selecting only four features as in the example, which gave the best results so far\n\n# fit the model\nmodel_2 = LogisticRegression(random_state=0).fit(X, y)\n\n#predictions\npredictions2 = model_2.predict(X_test)","09f8ef18":"# make submission\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions2})\noutput.to_csv('my_submission.csv', index=False) # formatting the predictions into the a data frame\n# with just PassengerId and survival prediction\nprint(\"Your submission was successfully saved!\")","f52b967d":"The submisison results for the logistic regression are 0.77511, identical to the random forest example submissions, and tied to the best so far.","92b324ee":"To do: Adjust the sizes of the random forest to see how this affects the results, maybe try some additional models besides random forest. The purpose of me doing this is to demonstrate the thought processs behind data prediction analysis, not to win a competition (one that can easily be \"won\" by simply looking up the actual data).","0e35906b":"# ***Data Visualization***","f7ed73fc":"This submission achieved the same score of 0.61961, which is worse. This happens sometimes. The best thing to do is to move on and try a different model, in this case logistic regression.","15a5fe5e":"# ***Logistic Regression***","00b708be":"Getting the same results twice using the same set of predictors suggests that there is not a huge amount of improvements left to make for the model. There obviously can be, but nothing worth expending large amounts of effort on. This is whole endevour is a simple exercise to demonstrate basic cleaning, description and regression techniques, and to be practice with Python. I still perfer R Studio for data cleaning and manipulation, but I more comfortable with Python after this.","3bb16147":"Observations: For ticket class, first class had the highest survival rate, with more surviving than not. The other two classes fared much worse, with the vast majority of third class not surviving.\nAs discussed before, sex is highly predictive. Age is also good for some age groups, speciifcally young children (who mostly lived) and twenty to thirty year olds (who mostly did not). The plots for number of siblings\/souses and children\/parents looked almost identical, with most passengers having none. Those who had only one or two tended to fare better in survival rates. The densisties for Fare are similar for both survived and not, although the ranges are different, suggesting it may be a good predictor. Lastly, of the variables I chose to investigate, those who departed from Cherbourg had higher survival rates than either Queenstown and Southampton.\n\nOne matter to be investigated is whether there is a correlation between predictors, which I think likely, which will result in diminishing returns on any model that uses all the variables. To understand this, considered Port of Departure and Ticket class. The different ports will have varying differences in distance from areas of wealth, so different levels of SES (socio-economic-status) will tend to use different ports, and thsoe same SES levels will tend to buy different class of tickets. I believe SES will be an important latent variable tied to several explicit variables, which can likely be estimated with only two or three variables. I will be looking at mutliple models regardless, but methods for selecting predictors should come in use.","6ae4d3c0":"# ***Random Forest Example***","e5ff63dd":"The purpose of this exercise for me is (1) to complete the Titanic data tutorial, as a base, (2) document the thought process for what each line of code does, along with my own observations regarding the data, and (3) experiment around with different models for predicting the survival of passengers, most of which will be poor at actual prediction.","3285dd8b":"# ***Basic Example***","3e0e7d73":"Observations: Survived is most correlated with Passenger class and fare, with only sibling\/spouses falling below 5% correlation. Passenger class and fare are, expectedly, *highly* correlated (inversely), along with sex. Parent\/child and sibling\/spouse the next most correlated, as was suspected earlier.\n\nTry a model that inlcudes Pclass, sex, age, embarked and parch, as those appear to be the most correlated with survival without too much tripping over each other. Afterwards, try using all the variables.","87496f6d":"# ***Modeling, Random Forest***"}}