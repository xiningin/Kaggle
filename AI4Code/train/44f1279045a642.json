{"cell_type":{"0b7223af":"code","28ab8941":"code","7d0b17cb":"code","1ff306d2":"code","40ac9d7c":"code","cf6690e3":"code","f0fa6fdf":"code","c2c9b692":"code","fc35450b":"code","28d28ea1":"code","828496c2":"code","0f2fb69c":"code","f6baa3ac":"code","5d76f1ee":"code","d047232d":"code","34400205":"code","5fccb7f3":"code","2b396d14":"code","b8349464":"code","8bee376b":"code","453ea20a":"code","ed1da8a6":"code","00328393":"code","797f3302":"code","4764f6d1":"code","c6db9cd5":"code","f0a9b947":"code","b1dc4ca4":"code","bfd35019":"code","3c1ab29f":"code","64523442":"code","e19f312a":"code","52875aa7":"code","aad36d8a":"code","e7bf6602":"code","102efc5a":"code","45ea6cbb":"code","29f5cb30":"code","a4df3d7b":"code","41081674":"code","5a04674f":"code","490b297d":"code","9cb114ad":"code","08ce71f3":"code","894a153d":"code","5d0344ff":"code","693d67d6":"code","033ea134":"code","784d6f57":"code","d0e5513d":"code","6e6bf825":"code","14a0be3f":"code","5a9b70df":"code","004abeda":"code","135d7700":"markdown","d87dd87e":"markdown","8d7628be":"markdown","dab83042":"markdown","75d65e44":"markdown","dd6d9be0":"markdown","c034f4e4":"markdown","4f40e9c7":"markdown","85d5f262":"markdown","70069c69":"markdown","3dc298b5":"markdown","f094cf91":"markdown","99efa95e":"markdown","716cf789":"markdown","fd287039":"markdown","3df8fbf3":"markdown","7b9f00ae":"markdown","e814724a":"markdown","5a356363":"markdown","d89b6c91":"markdown","27256109":"markdown","1f306708":"markdown","28682809":"markdown","11d0af92":"markdown","fd9000ae":"markdown","6e103cae":"markdown","b8f1e2e1":"markdown","9f4b59b0":"markdown","9da19431":"markdown","f66dcba8":"markdown","7698d1a8":"markdown"},"source":{"0b7223af":"# everything can be pip install\nimport csv\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm   \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom itertools import product    \nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom matplotlib import dates as mpl_dates","28ab8941":"warnings.filterwarnings(action = 'ignore')","7d0b17cb":"# define a function to plot time series data\ndef plot_series(time, series, col = 'dodgerblue', lab = 'original', format=\"-\", start=0, end=None):\n    plt.style.use('seaborn')\n    plt.plot(time[start:end], series[start:end], format, color = col, label = lab)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Series\")\n    # display the grid\n    plt.grid(True)\n    # got current figure, then autoformat date\n    plt.gcf().autofmt_xdate() \n    # format datetime\n    date_formate = mpl_dates.DateFormatter('%b\/%d\/%Y') \n    # set the format to out x-axis, gca is the get current axis\n    plt.gca().xaxis.set_major_formatter(date_formate)\n    plt.tight_layout() \n    plt.legend(loc = 'best')","1ff306d2":"df = pd.read_csv('\/kaggle\/input\/Sunspots.csv')\ndel df['Unnamed: 0']\ndf.head()\n# check for missing value using df.isnull(), there isn't any NaN value in this dataframe","40ac9d7c":"timeseries = df\ntimeseries['Date'] = pd.to_datetime(df['Date'])\ntimeseries = df.set_index(df['Date'])\ndel timeseries['Date']\ntimeseries.head()","cf6690e3":"time_step = []\nsunspots = []\nfor time, value in zip(df['Date'],df['Monthly Mean Total Sunspot Number']):\n    time_step.append(time)\n    sunspots.append(float(value))\n\n# plot our data\nplt.figure(figsize=(10, 6))\n\nplot_series(time_step, sunspots)","f0fa6fdf":"# zoom into the plot, the seasonality is roughly 11 years\nplt.figure(figsize=(10, 6))\nplot_series(time_step, sunspots, start=0, end=300)","c2c9b692":"# split data into validation and training datasets\nsplit_time = int(len(time_step)*0.8)\ntime_train = time_step[:split_time]\nx_train = sunspots[:split_time]\ntime_valid = time_step[split_time:]\nx_valid = sunspots[split_time:]","fc35450b":"# becasue data is from time_step and sunspots, here our data are in list type\ntype(x_train)","28d28ea1":"# I will also split it this way, and now train and vaild dataset are daraframe\nsplit_time = int(len(time_step)*0.8)\ntrain = timeseries[:split_time]\nvalid = timeseries[split_time:]","828496c2":"train.head()","0f2fb69c":"train.index","f6baa3ac":"naive_forecast = sunspots[split_time - 1:-1]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, naive_forecast, col = 'coral', lab = 'naive forecast')","5d76f1ee":"# zoom into to figure to avoid overlap\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid, start=0, end=150)\nplot_series(time_valid, naive_forecast, start=1, end=151, col = 'coral', lab = 'naive forecast')\n# the orange data is just one step after the blue data","d047232d":"print(keras.metrics.mean_absolute_error(x_valid, naive_forecast).numpy())","34400205":"# Firstly, define a function for moving average forecast\n# window size means forecasts the means for the last few values\ndef moving_average_forecast(series, window_size):\n    forecast = []\n    for time in range(len(series) - window_size):\n        forecast.append(series[time:time + window_size].mean())\n    return np.array(forecast)","5fccb7f3":"# turn data into numpy array \nseries = np.array(sunspots)\ntime = np.array(time_step)\n# here, the windex size is set to be 30\nmoving_avg = moving_average_forecast(series, 30)[split_time - 30:]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, moving_avg, col = 'coral', lab = 'moving average forecast')","2b396d14":"print(keras.metrics.mean_absolute_error(x_valid, moving_avg).numpy())","b8349464":"# difference between time t and 132 timesteps before that \n# after finding to optimal parameter, seasonality is set to be 507\nseasonality = 507\ndiff_series = (series[seasonality:] - series[:-seasonality])\ndiff_time = time[seasonality:]\n\nplt.figure(figsize=(10, 6))\nplot_series(diff_time, diff_series)\nplt.show()","8bee376b":"# try window size 20\n# after finding to optimal parameter, window size is set to be 5\ndiff_moving_avg = moving_average_forecast(diff_series, 5)[split_time - seasonality - 5:]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, diff_series[split_time - seasonality:])\nplot_series(time_valid, diff_moving_avg, col = 'coral')\nplt.show()","453ea20a":"# adding back the past value\ndiff_moving_avg_plus_past = series[split_time - seasonality:-seasonality] + diff_moving_avg\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, diff_moving_avg_plus_past, col = 'coral')\nplt.show()","ed1da8a6":"diff_moving_avg_plus_smooth_past = moving_average_forecast(series[split_time - seasonality-5:-seasonality+5], 10) + diff_moving_avg\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, diff_moving_avg_plus_smooth_past, col = 'coral', lab = 'diff_moving_avg_plus_smooth_past')\nplt.show()","00328393":"print(keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_smooth_past).numpy())","797f3302":"seasonpara = []\nmae = []\nwindow = []\nfor window_size in range(1,10):\n    for seasonality in range(500, 600):\n        diff_series = (series[seasonality:] - series[:-seasonality])\n        diff_time = time[seasonality:]\n        diff_moving_avg = moving_average_forecast(diff_series, window_size)[split_time - seasonality - window_size:]\n        diff_moving_avg_plus_past = series[split_time - seasonality:-seasonality] + diff_moving_avg\n        diff_moving_avg_plus_smooth_past = moving_average_forecast(series[split_time - seasonality-5:-seasonality+5], 10) + diff_moving_avg\n        seasonpara.append(seasonality)\n        mae.append(keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_smooth_past).numpy())\n        window.append(window_size)","4764f6d1":"for i in mae:\n    if i == min(mae):\n        print(f'The optimal seaonality is: {seasonpara[mae.index(i)]}')\n        print(f'The optimal window size is: {window[mae.index(i)]}')\n        print(f'The optimal MAE is: {min(mae)}')","c6db9cd5":"timeseries.head()","f0a9b947":"def plot_rol(timeseriesdata, size):\n    plt.figure(figsize=(15, 7))\n    plt.style.use('seaborn')\n    # rolling statistics\n    rol_mean = timeseriesdata.rolling(window=size).mean()\n    rol_std = timeseriesdata.rolling(window=size).std()\n    plt.plot(timeseriesdata, color='dodgerblue', label='Original')\n    plt.plot(rol_mean, color='red', label='Rolling Mean')\n    plt.plot(rol_std, color='green', label='Rolling standard deviation')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    # let the axis off for convenience\n    plt.axis('off')\n    plt.show()","b1dc4ca4":"plot_rol(timeseries, 132)","bfd35019":"plot_rol(timeseries.diff(1), 132)","3c1ab29f":"# define a function for adfuller test\ndef teststationarity(ts):\n    print('result of dickey-fuller test:')\n    dftest = adfuller(ts['Monthly Mean Total Sunspot Number'], autolag = 'AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    return dfoutput","64523442":"teststationarity(timeseries)","e19f312a":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(timeseries.dropna(), freq = 132)\n\n# get the trend, seasonality and noise \ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.figure(figsize=(12,8))\nplt.subplot(411)\nplt.plot(timeseries, label='Original', color=\"dodgerblue\")\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend', color=\"green\")\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality', color=\"red\")\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals', color=\"orange\")\nplt.legend(loc='best')\nplt.tight_layout()","52875aa7":"fig, ax = plt.subplots(figsize=(16,3))\nplot_acf(timeseries,ax=ax, lags = 200, color=\"dodgerblue\");\n\nfig, ax = plt.subplots(figsize=(16,3))\nplot_pacf(timeseries,ax=ax, lags = 200, color=\"dodgerblue\")\nplt.show()","aad36d8a":"autocorrelation_plot(timeseries, color='dodgerblue')","e7bf6602":"# the first value is NaN when use a diff\nautocorrelation_plot(timeseries.diff(1)[1:], color='dodgerblue')\n# in this case, the model is overfitting ","102efc5a":"atrain = train.resample('A').sum()\navalid = valid.resample('A').sum()\navalid.head()","45ea6cbb":"# Initial approximation of parameters\nQs = range(0, 3)\nqs = range(0, 3)\nPs = range(0, 3)\nps = range(0, 3)\nD=1\nd=1\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)\n\n# Model Selection\nresults = []\nbest_aic = float(\"inf\")\nwarnings.filterwarnings('ignore')\nfor param in parameters_list:\n    try:\n        model=sm.tsa.statespace.SARIMAX(atrain['Monthly Mean Total Sunspot Number'], order=(param[0], d, param[1]), \n                                        seasonal_order=(param[2], D, param[3], 11)).fit(disp=-1)\n    except ValueError:\n        print('wrong parameters:', param)\n        continue\n    aic = model.aic\n    if aic < best_aic:\n        best_model = model\n        best_aic = aic\n        best_param = param\n    results.append([param, model.aic])","29f5cb30":"best_model.summary()","a4df3d7b":"best_model.plot_diagnostics(figsize=(15, 12))\nplt.show()","41081674":"plt.plot(atrain,color = 'dodgerblue',label = 'trian')\nplt.plot(avalid, color = 'r',label = 'valid')\nbest_model.forecast(len(avalid)).plot(color = 'orange',label = 'forecast')\nplt.legend(loc = 'best')\nplt.show()","5a04674f":"# define some hyper-parameter\nwindow_size = 60\nbatch_size = 32\nshuffle_buffer_size = 1000\n\n# turn a series into a dataset which we can train on\ndef windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    # create a dataset from series\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    # slice the data up into appropriate windows\n    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n    # flattened into chunks in the size of our window size + 1\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n    # batched into the selected batch size and returned\n    dataset = dataset.batch(batch_size).prefetch(1)\n    return dataset","490b297d":"dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n# three layer of 20,10 and 1 neurons, input shape is the size of window \nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(20, input_shape=[window_size], activation=\"relu\"), \n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-7, momentum=0.9))\n# ignore the epoch by epoch output by setting verbose = 0\nhistory = model.fit(dataset,epochs=200,verbose=0)\nmodel.summary()","9cb114ad":"loss = history.history['loss']\nepochs = range(len(loss))\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.show()","08ce71f3":"series = np.array(sunspots)\ntime = np.array(time_step)\nforecast=[]\n\nfor time in range(len(series) - window_size):\n    forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n\nforecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\nplt.figure(figsize=(10, 6))\n\nplot_series(time_valid, x_valid)\nplot_series(time_valid, results, col = 'coral')","894a153d":"tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()","5d0344ff":"def model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast\n\ndef windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)","693d67d6":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\nwindow_size = 64\nbatch_size = 256\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  # Sequence to Sequence for LSTM \n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n# twick the learning rate, use the optimal learning rate instead of one \nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch \/ 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule],verbose = 0)","033ea134":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-4, 0, 60])\n# y is the loss and x is the learning rate \n# set the lr to be the optimal, where the loss is this minimum","784d6f57":"# this clears any internal variables, which makes it easy for us to experiment without models impacting later versions of themselves.\ntf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\ntrain_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=shuffle_buffer_size)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(train_set,epochs=500,verbose=0)","d0e5513d":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","6e6bf825":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","14a0be3f":"tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()","5a9b70df":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nloss=history.history['loss']\n\nepochs=range(len(loss)) # Get number of epochs\n\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r')\nplt.title('Training loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\"])\n\nplt.figure()\n\n\n\nzoomed_loss = loss[200:]\nzoomed_epochs = range(200,500)\n\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(zoomed_epochs, zoomed_loss, 'r')\nplt.title('Training loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\"])\n\nplt.figure()","004abeda":"print(rnn_forecast)","135d7700":"<h1>Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Background\" data-toc-modified-id=\"Background-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Background<\/a><\/span><\/li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span><\/li><li><span><a href=\"#Training-Model-and-Forecasting\" data-toc-modified-id=\"Training-Model-and-Forecasting-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Training Model and Forecasting<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Naive-Forecasting-Model\" data-toc-modified-id=\"Naive-Forecasting-Model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Naive Forecasting Model<\/a><\/span><\/li><li><span><a href=\"#Moving-Average-Model\" data-toc-modified-id=\"Moving-Average-Model-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Moving Average Model<\/a><\/span><\/li><li><span><a href=\"#ARIMA-Model\" data-toc-modified-id=\"ARIMA-Model-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>ARIMA Model<\/a><\/span><\/li><li><span><a href=\"#Simple-DNN-Model\" data-toc-modified-id=\"Simple-DNN-Model-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;<\/span>Simple DNN Model<\/a><\/span><\/li><li><span><a href=\"#RNN-Model\" data-toc-modified-id=\"RNN-Model-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;<\/span>RNN Model<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","d87dd87e":"Firstly, it's a univariate time series problem, as a result of only one response variable. Also, according to commen patterns of time series:\n- Trend: Upward? downward? There isn't any obvious trend.\n- White nosie: Mean 0? constant variance? 0 correlation between lags? It's \n- Auto correlation: Deeper analysis when using the ARIMA model.  \n- Seasonality: It's pretty obvious, as the background part introduced, 11 years is the period of sunspots' seasonality. After zoom into the figure, 11 years will be more clear.","8d7628be":"## Simple DNN Model","dab83042":"`Sunspots` are temporary phenomena on the Sun's photosphere that appear as spots darker than the surrounding areas. They are regions of reduced surface temperature caused by concentrations of magnetic field flux that inhibit convection. `Sunspot activity cycles` are about every eleven years, with some variation in length. Over the solar cycle, sunspot populations rise quickly and then fall more slowly. \n\nThe 11-year sunspot cycle is actually half of a longer, 22-year cycle of solar activity. Each time the sunspot count rises and falls, the magnetic field of the Sun associated with sunspots reverses polarity; the orientation of magnetic fields in the Sun's northern and southern hemispheres switch. Thus, in terms of magnetic fields, the solar cycle is only complete (with the fields back the way they were at the start of the cycle) after two 11-year sunspot cycles. This solar cycle is, on average, about 22 years long - twice the duration of the sunspot cycle.","75d65e44":"Metrix including MSE(mean square errors), RMSE(root means squared error), MAPE(mean ratio between the absolute error and the absolute value ), and MAE(mean absolute error). In this case, I will use MAE, which does not penalize large errors as much as the MSE does.","dd6d9be0":"# Training Model and Forecasting","c034f4e4":"__AR & I & MA__\n\nAR means Auto Regressive, MA means Moving Average, it refers to Integrated\/difference.\n\nA pure Auto Regressive (AR only) model is one where $Y_t$ depends only on its own lags. That is, $Y_t$ is a function of the 'lags of $Y_t$'. $Y_{t-1}$ is the lag1 of the series, $\\beta_1$ is the coefficient of lag1 that the model estimates and $\\alpha$ is the intercept term, also estimated by the model.\n\n$$ Y_t = \\alpha + \\beta_1Y_{t-1} + \\beta_2Y_{t-2} + .. + \\beta_pY_{t-p} + \\epsilon_1  $$\n\nLikewise a pure Moving Average (MA only) model is one where $Y_t$ depends only on the lagged forecast errors.\n\n$$ Y_t = \\alpha +\\epsilon_t + \\phi_1\\epsilon_{t-1} + \\phi_2\\epsilon_{t-2} .. + \\phi_q\\epsilon_{t-q}  $$\n\nWhere the error terms are the errors of the autoregressive models of the respective lags. The errors $\\epsilon_t$ and $\\epsilon_{t-1}$ are the errors from the following equations :\n\n$$ Y_t = \\beta_1Y_{t-1} + \\beta_2Y_{t-2} + .. + \\beta_0Y_0 + \\epsilon_t  $$\n\n$$ Y_{t-1} = \\beta_1Y_{t-2} + \\beta_2Y_{t-3} + .. + \\beta_0Y_0 + \\epsilon_{t-1}  $$\n\nAn ARIMA model is one where the time series was differenced at least once to make it stationary and you combine the AR and the MA terms. So the equation becomes:\n\n$$ Y_t = \\alpha + \\beta_1Y_{t-1} + \\beta_2Y_{t-2} + .. + \\beta_pY_{t-p}\\epsilon_t + \\phi_1\\epsilon_{t-1} + \\phi_2\\epsilon_{t-2} .. + \\phi_q\\epsilon_{t-q} $$\n\nPredicted $Y_t$ = Constant + Linear combination Lags of Y (upto p lags) + Linear Combination of Lagged forecast errors (upto q lags)","4f40e9c7":"[What is autocorrelation and partial autocorrelation?](https:\/\/www.youtube.com\/watch?v=ZjaBn93YPWo)\n\n[ACF vs. PACF](https:\/\/www.youtube.com\/watch?v=DeORzP0go5I)","85d5f262":"With a seaonality of 11 years (132 months), it turn out to be quiet challenging when iterating to find optimal SARIMA parameters. I spent a lot of time working on this problem, but still can not solve it. Therefore, I temporarily resample the data using 'A', which means each time step is a year now. The seaonality is 11 currently. That will save a lot of iterating time.","70069c69":"![Sunspot](https:\/\/www.weather.gov\/images\/fsd\/astro\/Sun_sunspot.jpg)","3dc298b5":"A `Recurrent Neural Network`, or RNN is a neural network that contains recurrent layers. These are designed to sequentially processes sequence of inputs. RNNs are pretty flexible, able to process all kinds of sequences. The full input shape when using RNNs is three-dimensional. The first dimension will be the `batch size`, the second will be the `timestamps`, and the third is the `dimensionality of the inputs at each time step`. In this case, it's a univariate time series, this value will be one.","f094cf91":"Firstly, I am going to build a `naive forecasting model`, which is a baseline of forecasting models. The last period's actuals are used as this period's forecast, without adjusting them or attempting to establish causal factors. `Naive forecasting model` is used only for comparison with the forecasts generated by the better (sophisticated) techniques.","99efa95e":"The seasonality is gone to a certain extent. Now I can use the moving average again.","716cf789":"Firstly, check if the series is stationary using the `Augmented Dickey Fuller test` (adfuller()), from the statsmodels package. Differencing is needed only if the series is non-stationary. Else, no differencing is needed, that is, d=0. The null hypothesis of the ADF test is that the time series is non-stationary. So, if the p-value of the test is less than the significance level (0.05) then you reject the null hypothesis and infer that the time series is indeed stationary.","fd287039":"The next step is to identify if the model needs any AR terms. You can find out the required number of AR terms by inspecting the Partial Autocorrelation (PACF) plot. Partial autocorrelation can be imagined as the correlation between the series and its lag, after excluding the contributions from the intermediate lags. So, PACF sort of conveys the pure correlation between a lag and the series. That way, you will know if that lag is needed in the AR term or not.\n\n$$Y_t = \\alpha_0 + \\alpha_1Y_{t-1} + \\alpha_2Y_{t-2} + \\alpha_3Y_{t-3}$$","3df8fbf3":"## Naive Forecasting Model","7b9f00ae":"Therefore, 18.45 is the baseline.","e814724a":"Once it's flattened, it's easy to shuffle it. You call a shuffle and you pass it the shuffle buffer. Using a shuffle buffer speeds things up a bit. So for example, if you have 100,000 items in your dataset, but you set the buffer to a thousand. It will just fill the buffer with the first thousand elements, pick one of them at random. And then it will replace that with the 1,000 and first element before randomly picking again, and so on. This way with super large datasets, the random element choosing can choose from a smaller number which effectively speeds things up.","5a356363":"## RNN Model","d89b6c91":"## Moving Average Model","27256109":"First of all, as with any other ML problem, we have to divide our data into features and labels. In this case our feature is effectively a number of values in the series, with our label being the next value. We'll call that number of values that will treat as our feature, the window size, where we're taking a window of the data and training an ML model to predict the next value. So for example, if we take our time series data, say, 30 days at a time, we'll use 30 values as the feature and the next value is the label. Then over time, we'll train a neural network to match the 30 features to the single label.","1f306708":"## ARIMA Model","28682809":"In this notebook, I am going to predict the monthly sunspot number based on the dataset that contains monthly mean sunspot number from 1749 to 2018. I will use multiple ways to build the prediction model, including: `Naive Forecast`, `Moving Average Forecast`, `ARIMA Forecast`, `Simple DNN Forecast`, and `RNN Forecast`.","11d0af92":"# Background","fd9000ae":"Stationary time series always have short-term correlation, so the autocorrelation coefficient will quickly coverage to 0.","6e103cae":"__What is ARIMA?__ \n\nARIMA, short for `'Auto Regressive Integrated Moving Average'` is actually a class of models that explains a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values. Any 'non-seasonal' time series that exhibits patterns and is not a random white noise can be modeled with ARIMA models. ARIMA require `stationary time series data`, so the first step is `stationary`.\n\n__What is stationary?__\n\nThe mean\/variance of time series data doesn't change over time; the autocorrelation coefficient only related with time lags.\n\n__Why we need stationary?__\n\nBecause, term`'Auto Regressive'` in ARIMA means it is a `linear regression model` that uses its own lags as predictors. `Linear regression models`, as you know, work best when the predictors are not correlated and are independent of each other.","b8f1e2e1":"With a pretty small p-value(or critical value < test statistic), reject the null hypothesis. Data is already stationary. d = 0.","9f4b59b0":"According to the new MAE, the model is worse than naive forecast! The moving average does not anticipate trend or seasonality. So let's try to remove them by using differencing. Since the seasonality period is 11 years, we will subtract the value at time t \u2013 132 from the value at time t.","9da19431":"# Data Preprocessing","f66dcba8":"__3 terms of ARIMA__\n\nAn ARIMA model is characterized by 3 terms: p, d, q\n- p: the order of the AR term. It refers to the number of lags of Y to be used as predictors. \n- q: the order of the MA term. It refers to the number of lagged forecast errors that should go into the ARIMA Model.\n- d: the number of differencing required to make the time series stationary. The most common approach is to difference it. That is, subtract the previous value from the current value. Sometimes, depending on the complexity of the series, more than one differencing may be needed. The value of d, therefore, is the minimum number of differencing needed to make the series stationary. And if the time series is already stationary, then d = 0.","7698d1a8":"It looks like this model is pretty bad, beacuse it's under our baseline. Maybe there is some problem with hyper-paramater, like window_size and seasonality. I would like to iterate over different hyper-parameter and find the optimal solution. "}}