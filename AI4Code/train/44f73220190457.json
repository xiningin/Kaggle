{"cell_type":{"55684935":"code","cc7dea18":"code","80096eba":"code","8915a9b6":"code","0d445ee9":"code","2bdb5025":"code","fc500199":"code","bf437b18":"code","abc6f13e":"code","3d164707":"code","9e0f2012":"code","da8cd920":"code","db2733ea":"code","3b9c9c96":"code","ec5c3e6e":"code","84c0a4c9":"code","62cc9179":"code","ac586c59":"code","99757117":"code","e84d584b":"code","8c111e96":"code","a52319f2":"code","f4a23ecc":"code","e2df54fd":"code","a40f5c92":"code","9979a229":"code","3b4942d6":"code","7aa20dae":"code","a009755b":"code","f6d02ca7":"markdown","833247a0":"markdown","45fdc7a5":"markdown","f6fc7290":"markdown","8c6d3464":"markdown","23d0c1e6":"markdown","af91287d":"markdown","f4e86aea":"markdown","6752e9ca":"markdown"},"source":{"55684935":"# Basic Pydata Libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt     \nimport seaborn as sns\nimport html\nimport unicodedata\n\n# sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n\n# Keras\nfrom keras import models\nfrom keras import layers\nfrom keras import losses\nfrom keras import metrics\nfrom keras import optimizers\nfrom keras.utils import plot_model\n\n\n# Standard plotly imports\nimport plotly.offline as py \nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\ncufflinks.go_offline(connected=True)\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n\nstop_words = set(stopwords.words(\"english\"))\n\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nfrom wordcloud import WordCloud, STOPWORDS\n\n## warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Graphics in retina format are more sharp and legible\n%config InlineBackend.figure_format = 'retina'\n","cc7dea18":"def remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br \/>', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\n\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text, and by defult lemmatize nouns\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    words = text2words(text)\n    words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    words = lemmatize_words(words)\n    words = lemmatize_verbs(words)\n\n    return ''.join(words)","80096eba":"train_data = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest_data = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\ntrain_data.head()\n","8915a9b6":"train_data.info()","0d445ee9":"train_data.shape","2bdb5025":"train_data.columns","fc500199":"train_data['toxic'] = np.where(train_data['target'] >= .5, 'Toxic', 'Non-Toxic')","bf437b18":"train_data['comment_text'].isnull().sum() ,train_data['target'].isnull().sum()","abc6f13e":"train_data['num_words'] = [len(sent.split()) for sent in train_data['comment_text']]\ntrain_data[['comment_text', 'num_words']].head()","3d164707":"# maximum and minimum number of words per sentencs\nmax(train_data['num_words']), min(train_data['num_words'])","9e0f2012":"fig, ax = plt.subplots(1,2,figsize=(12,7))\nsns.histplot(train_data[train_data['toxic'] == 'Toxic']['num_words'], kde= True, ax=ax[0], color= 'r')\nsns.histplot(train_data[train_data['toxic'] == 'Non-Toxic']['num_words'], kde= True, ax=ax[1], color= 'b')\nax[0].set_title(\"Toxic Distribution\")\nax[1].set_title(\"Non Toxic Distribution\")\nplt.show();","da8cd920":"train_data['toxic'].value_counts()","db2733ea":"sns.countplot(train_data['toxic'])\nplt.title('Distribuition of Toxicity of comments');","3b9c9c96":"# lets create a list of all the identities tagged in this dataset. This list given in the data section of this competition. \nidentities = ['male','female','transgender','other_gender','heterosexual','homosexual_gay_or_lesbian',\n              'bisexual','other_sexual_orientation','christian','jewish','muslim','hindu','buddhist',\n              'atheist','other_religion','black','white','asian','latino','other_race_or_ethnicity',\n              'physical_disability','intellectual_or_learning_disability','psychiatric_or_mental_illness',\n              'other_disability']","ec5c3e6e":"# getting the dataframe with identities tagged\ntrain_labeled_df = train_data.loc[:, ['target'] + identities ].dropna()\n\n# lets define toxicity as a comment with a score being equal or .5\ntoxic_df = train_labeled_df[train_labeled_df['target'] >= .5][identities]\nnon_toxic_df = train_labeled_df[train_labeled_df['target'] < .5][identities]","84c0a4c9":"# at first, we just want to consider the identity tags in binary format. So if the tag is any value other than 0 we consider it as 1.\ntoxic_count = toxic_df.apply(lambda x : x > 0 ).sum()\nnon_toxic_count = non_toxic_df.apply(lambda x : x > 0 ).sum()","62cc9179":"fig, ax = plt.subplots(nrows= 2,ncols= 1,figsize=(15,15))\n\nsns.barplot(x= toxic_count, y= identities, ax=ax[0])\nsns.barplot(x= non_toxic_count, y= identities, ax=ax[1])\n\nax[0].set_title(\"Toxic Distribution\")\nax[1].set_title(\"Non Toxic Distribution\")\nplt.show();","ac586c59":"# now we can concat the two series together to get a toxic count vs non toxic count for each identity\ntoxic_vs_non_toxic = pd.concat([toxic_count, non_toxic_count], axis=1)\n\ntoxic_vs_non_toxic = toxic_vs_non_toxic.rename(index=str, columns={1: \"non-toxic\", 0: \"toxic\"})\n# here we plot the stacked graph but we sort it by toxic comments to (perhaps) see something interesting\ntoxic_vs_non_toxic.sort_values(by='toxic').plot(kind='bar', stacked=True, figsize=(30,10), fontsize=20).legend(prop={'size': 20});","99757117":"word_cloud_non_toxic = train_data[train_data['toxic'] == \"Non-Toxic\"]['comment_text']\nword_cloud_toxic = train_data[train_data['toxic'] == \"Toxic\"]['comment_text']\n\nwordcloud_non = WordCloud(\n                          background_color='black',\n                          stopwords=stop_words,\n                          max_words=2000,\n                          max_font_size=100, \n                          random_state=42\n                         ).generate(str(word_cloud_non_toxic))\n\nwordcloud_toxic = WordCloud(\n                          background_color='black',\n                          stopwords=stop_words,\n                          max_words=2000,\n                          max_font_size=100, \n                          random_state=42\n                         ).generate(str(word_cloud_toxic))\n\nfig = plt.figure(figsize=[20,5])\nplt.suptitle('Non Normalized Text', size = 25)\n\nfig.add_subplot(1, 2, 1).set_title(\"Toxic Wordcloud\", fontsize=10)\nplt.imshow(wordcloud_toxic, interpolation=\"bilinear\")\nplt.axis(\"off\")\n\nfig.add_subplot(1, 2, 2).set_title(\"Non Toxic Wordcloud\", fontsize=10)\nplt.imshow(wordcloud_non, interpolation=\"bilinear\")\nplt.axis(\"off\")\n\nplt.show()\n","e84d584b":"# make lists for normalized text\nclean_toxic_text = [normalize_text(sent) for sent in train_data[train_data['toxic'] == \"Toxic\"]['comment_text'][:10000]]\nclean_non_toxic_text = [normalize_text(sent) for sent in train_data[train_data['toxic'] == \"Non-Toxic\"]['comment_text'][:10000]]","8c111e96":"wordcloud_non = WordCloud(\n                          background_color='black',\n                          stopwords=stop_words,\n                          max_words=2000,\n                          max_font_size=100, \n                          random_state=42\n                         ).generate(str(clean_non_toxic_text))\n\nwordcloud_toxic = WordCloud(\n                          background_color='black',\n                          stopwords=stop_words,\n                          max_words=2000,\n                          max_font_size=100, \n                          random_state=42\n                         ).generate(str(clean_toxic_text))\n\nfig = plt.figure(figsize=[20,5])\nplt.suptitle('Normalized Text', size = 25)\n\nfig.add_subplot(1, 2, 1).set_title(\"Toxic Wordcloud\", fontsize=10)\nplt.imshow(wordcloud_toxic, interpolation=\"bilinear\")\nplt.axis(\"off\")\n\nfig.add_subplot(1, 2, 2).set_title(\"Non Toxic Wordcloud\", fontsize=10)\nplt.imshow(wordcloud_non, interpolation=\"bilinear\")\nplt.axis(\"off\")\n\nplt.show()\n","a52319f2":"%%time\n\nVectorize = TfidfVectorizer()\nX = Vectorize.fit_transform(train_data[\"comment_text\"])\nX_test_date = Vectorize.transform(test_data[\"comment_text\"])\n\ny = np.array(train_data['target'].apply(lambda x: x > .5).astype('int'))","f4a23ecc":"X.shape, y.shape, X_test_date.shape","e2df54fd":"# spliting data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)","a40f5c92":"# Fitting a simple Logistic Regression on tf-idf\nlr = LogisticRegression(C=1.0)\nlr.fit(X_train, y_train)","9979a229":"corss_scores = cross_val_score(lr, X, y, cv=5, scoring= 'roc_auc')\ncorss_scores.mean()","3b4942d6":"y_pred = lr.predict(X_test)","7aa20dae":"plt.figure(figsize=(8, 6))\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, cmap='Reds')\nplt.xlabel('predicted value')\nplt.ylabel('true value');","a009755b":"print(classification_report(y_test, y_pred))","f6d02ca7":"Number of words in toxic and non toxic sentence","833247a0":"### Distribuition of Toxicity of comments","45fdc7a5":"### Distribution for words in comment text","f6fc7290":"**Classification report**","8c6d3464":"## Word Clouds","23d0c1e6":"## Basic EDA\n","af91287d":"#### Logistic Regression + TfidfVectorizer","f4e86aea":"**Confusion matrix**","6752e9ca":"## Modeling"}}