{"cell_type":{"598ac194":"code","903e98f2":"code","56ba309b":"code","36b64791":"code","1930fa60":"code","401d78db":"code","683f27a7":"code","4941c578":"code","77e20df2":"code","0ee6121e":"code","1561003a":"code","428d385d":"code","28f3e159":"code","973cf6ca":"code","da8ec6c7":"code","191f9d99":"code","3bae8481":"code","7bff81ff":"code","7507cb57":"code","6a2898d6":"code","41cb0b1e":"code","4c1cf37e":"code","91902674":"code","a9bc724e":"code","a792fb97":"code","b04675b6":"code","0284c474":"code","7f0f235e":"code","def0a8de":"code","fe4e33e0":"code","1f06a17d":"code","24261d08":"markdown","1ac4c1e8":"markdown","49ae1495":"markdown","864c5ba4":"markdown","241a5ca2":"markdown","c7db6319":"markdown","91faef40":"markdown","aeffb1da":"markdown","0e65600d":"markdown","df91105d":"markdown","4eb4009a":"markdown","e26dfb46":"markdown","82573c65":"markdown","1accf990":"markdown","8d8ab08b":"markdown","309f4443":"markdown","8807181c":"markdown","12bdeaf7":"markdown","60f728b1":"markdown","44fc03b7":"markdown","ee497e45":"markdown","85604a61":"markdown","72e2d4c1":"markdown","04f5256a":"markdown","4603b264":"markdown","f74d3b1b":"markdown","a5a6fe37":"markdown","ca7ac451":"markdown","ce779f7d":"markdown","746ce565":"markdown","99b7649c":"markdown","19b0cabb":"markdown","1c3db822":"markdown","c2e57871":"markdown","5a16e536":"markdown"},"source":{"598ac194":"!pip install bayesian-optimization","903e98f2":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import rankdata\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport gc\nimport warnings\n\npd.set_option('display.max_columns', 200)","56ba309b":"train_df = pd.read_csv('..\/input\/train.csv')\n\ntest_df = pd.read_csv('..\/input\/test.csv')","36b64791":"train_df.head()","1930fa60":"test_df.head()","401d78db":"target = 'target'\npredictors = train_df.columns.values.tolist()[2:]","683f27a7":"train_df.target.value_counts()","4941c578":"bayesian_tr_index, bayesian_val_index  = list(StratifiedKFold(n_splits=2, shuffle=True, random_state=1).split(train_df, train_df.target.values))[0]","77e20df2":"def LGB_bayesian(\n    num_leaves,  # int\n    min_data_in_leaf,  # int\n    learning_rate,\n    min_sum_hessian_in_leaf,    # int  \n    feature_fraction,\n    lambda_l1,\n    lambda_l2,\n    min_gain_to_split,\n    max_depth):\n    \n    # LightGBM expects next three parameters need to be integer. So we make them integer\n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth = int(max_depth)\n\n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n\n    param = {\n        'num_leaves': num_leaves,\n        'max_bin': 63,\n        'min_data_in_leaf': min_data_in_leaf,\n        'learning_rate': learning_rate,\n        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n        'bagging_fraction': 1.0,\n        'bagging_freq': 5,\n        'feature_fraction': feature_fraction,\n        'lambda_l1': lambda_l1,\n        'lambda_l2': lambda_l2,\n        'min_gain_to_split': min_gain_to_split,\n        'max_depth': max_depth,\n        'save_binary': True, \n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,   \n\n    }    \n    \n    \n    xg_train = lgb.Dataset(train_df.iloc[bayesian_tr_index][predictors].values,\n                           label=train_df.iloc[bayesian_tr_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    xg_valid = lgb.Dataset(train_df.iloc[bayesian_val_index][predictors].values,\n                           label=train_df.iloc[bayesian_val_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    num_round = 5000\n    clf = lgb.train(param, xg_train, num_round, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n    \n    predictions = clf.predict(train_df.iloc[bayesian_val_index][predictors].values, num_iteration=clf.best_iteration)   \n    \n    score = metrics.roc_auc_score(train_df.iloc[bayesian_val_index][target].values, predictions)\n    \n    return score","0ee6121e":"# Bounded region of parameter space\nbounds_LGB = {\n    'num_leaves': (5, 20), \n    'min_data_in_leaf': (5, 20),  \n    'learning_rate': (0.01, 0.3),\n    'min_sum_hessian_in_leaf': (0.00001, 0.01),    \n    'feature_fraction': (0.05, 0.5),\n    'lambda_l1': (0, 5.0), \n    'lambda_l2': (0, 5.0), \n    'min_gain_to_split': (0, 1.0),\n    'max_depth':(3,15),\n}","1561003a":"from bayes_opt import BayesianOptimization","428d385d":"LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=13)","28f3e159":"print(LGB_BO.space.keys)","973cf6ca":"init_points = 5\nn_iter = 5","da8ec6c7":"print('-' * 130)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","191f9d99":"LGB_BO.max['target']","3bae8481":"LGB_BO.max['params']","7bff81ff":"# parameters from version 2 of\n#https:\/\/www.kaggle.com\/fayzur\/customer-transaction-prediction?scriptVersionId=10522231\n\nLGB_BO.probe(\n    params={'feature_fraction': 0.1403, \n            'lambda_l1': 4.218, \n            'lambda_l2': 1.734, \n            'learning_rate': 0.07, \n            'max_depth': 14, \n            'min_data_in_leaf': 17, \n            'min_gain_to_split': 0.1501, \n            'min_sum_hessian_in_leaf': 0.000446, \n            'num_leaves': 6},\n    lazy=True, # \n)","7507cb57":"LGB_BO.maximize(init_points=0, n_iter=0) # remember no init_points or n_iter","6a2898d6":"for i, res in enumerate(LGB_BO.res):\n    print(\"Iteration {}: \\n\\t{}\".format(i, res))","41cb0b1e":"LGB_BO.max['target']","4c1cf37e":"LGB_BO.max['params']","91902674":"param_lgb = {\n        'num_leaves': int(LGB_BO.max['params']['num_leaves']), # remember to int here\n        'max_bin': 63,\n        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), # remember to int here\n        'learning_rate': LGB_BO.max['params']['learning_rate'],\n        'min_sum_hessian_in_leaf': LGB_BO.max['params']['min_sum_hessian_in_leaf'],\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n        'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n        'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n        'min_gain_to_split': LGB_BO.max['params']['min_gain_to_split'],\n        'max_depth': int(LGB_BO.max['params']['max_depth']), # remember to int here\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,\n    }","a9bc724e":"nfold = 5","a792fb97":"gc.collect()","b04675b6":"skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)","0284c474":"oof = np.zeros(len(train_df))\npredictions = np.zeros((len(test_df),nfold))\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nfold {}\".format(i))\n    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n                           label=train_df.iloc[train_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n                           label=train_df.iloc[valid_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    \n    clf = lgb.train(param_lgb, xg_train, 5000, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration) \n    \n    predictions[:,i-1] += clf.predict(test_df[predictors], num_iteration=clf.best_iteration)\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.2f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))","7f0f235e":"predictions","def0a8de":"print(\"Rank averaging on\", nfold, \"fold predictions\")\nrank_predictions = np.zeros((predictions.shape[0],1))\nfor i in range(nfold):\n    rank_predictions[:, 0] = np.add(rank_predictions[:, 0], rankdata(predictions[:, i].reshape(-1,1))\/rank_predictions.shape[0]) \n\nrank_predictions \/= nfold","fe4e33e0":"sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub_df[\"target\"] = rank_predictions\nsub_df[:10]","1f06a17d":"sub_df.to_csv(\"Customer_Transaction_rank_predictions.csv\", index=False)","24261d08":"Finally, the list of all parameters probed and their corresponding target values is available via the property LGB_BO.res.","1ac4c1e8":"As the optimization is done, let's see what is the maximum value we have got.","49ae1495":"# Bayesian global optimization with gaussian processes for finding (sub-)optimal parameters of LightGBM\n\nAs","864c5ba4":"Let's submit prediction to Kaggle.","241a5ca2":"<a id=\"3\"><\/a> <br>\n## 3. Training LightGBM model","c7db6319":"## Notebook  Content\n0. [Installing Bayesian global optimization library](#0) <br>    \n1. [Loading the data](#1)\n2. [Black box function to be optimized (LightGBM)](#2)\n3. [Training LightGBM model](#3)\n4. [Rank averaging](#4)\n5. [Submission](#5)","91faef40":"Now I need to give bounds for these parameters, so that Bayesian optimization only search inside the bounds.","aeffb1da":"We have got a better validation score in the probe! As previously I ran `LGB_BO` only for 10 runs. In practice I increase it to arround 100.","0e65600d":"Number of Kfolds:","df91105d":"Now, it's time to call the function from Bayesian optimization framework to maximize. I allow `LGB_BO` object to run for 5 `init_points` (exploration) and 5 `n_iter` (exploitation).","4eb4009a":"<a id=\"0\"><\/a> <br>\n## 0. Installing Bayesian global optimization library\n\nLet's install the latest release from pip","e26dfb46":"Now, let's the the key space (parameters) we are going to optimize:","82573c65":"Do not forget to upvote :) Also fork and modify for your own use. ;)","1accf990":"The above `LGB_bayesian` function will act as black box function for Bayesian optimization. I already defined the the trainng and validation dataset for LightGBM inside the `LGB_bayesian` function. \n\nThe `LGB_bayesian` function takes values for `num_leaves`, `min_data_in_leaf`, `learning_rate`, `min_sum_hessian_in_leaf`, `feature_fraction`, `lambda_l1`, `lambda_l2`, `min_gain_to_split`, `max_depth` from Bayesian optimization framework. Keep in mind that `num_leaves`, `min_data_in_leaf`, and `max_depth` should be integer for LightGBM. But Bayesian Optimization sends continous vales to function. So I force them to be integer. I am only going to find optimal parameter values of them. The reader may increase or decrease number of parameters to optimize.","8d8ab08b":"<a id=\"4\"><\/a> <br>\n## 4. Rank averaging","309f4443":"<a id=\"5\"><\/a> <br>\n## 5. Submission","8807181c":"Let's put all of them in BayesianOptimization object","12bdeaf7":"Wait, I want to show one more cool option from BayesianOptimization library. You can probe the `LGB_bayesian` function, if you have an idea of the optimal parameters or it you get **parameters from other kernel** like mine [mine](https:\/\/www.kaggle.com\/fayzur\/customer-transaction-prediction). I will copy and paste parameters from my other kernel here. You can probe as folowing:","60f728b1":"In this kernel I will be using **50% Stratified rows** as holdout rows for the validation-set to get optimal parameters. Later I will use 5 fold cross validation in the final model fit.","44fc03b7":"We are given anonymized dataset containing 200 numeric feature variables from var_0 to var_199. Let's have a look train dataset:","ee497e45":"<a id=\"1\"><\/a> <br>\n## 1. Loading the data","85604a61":"Let's build a model together use therse parameters ;)","72e2d4c1":"So we got 0.90 AUC in 5 fold cross validation. And 5 fold prediction look like:","04f5256a":"The problem is unbalanced! ","4603b264":"OK, by default these will be explored lazily (lazy=True), meaning these points will be evaluated only the next time you call maximize. Let's do a maximize call of `LGB_BO` object.","f74d3b1b":"The validation AUC for parameters is 0.89 ! Let's see parameters is responsible for this score :)","a5a6fe37":"As you see, I assined `LGB_BO`'s optimal parameters to the `param_lgb` dictionary and they will be used to train a model with 5 fold.","ca7ac451":"I have created the BayesianOptimization object (`LGB_BO`), it will not work until I call maximize. Before calling it, I want to explain two parameters of BayesianOptimization object (`LGB_BO`) which we can pass to maximize:\n- `init_points`: How many initial random runs of **random** exploration we want to perform. In our case `LGB_bayesian` will be called `n_iter` times.\n- `n_iter`: How many runs of bayesian optimization we want to perform after number of `init_points` runs. ","ce779f7d":"Distribution of target variable","746ce565":"As data is loaded, let's create the black box function for LightGBM to find parameters.","99b7649c":"If you are still reading, bare with me. I will not take much of your time. :D We are almost done. Let's do a rank averaging on 5 fold predictions.","19b0cabb":"<a id=\"2\"><\/a> <br>\n## 2. Black box function to be optimized (LightGBM)","1c3db822":"Test dataset:","c2e57871":"Now we can use these parameters to our final model!","5a16e536":"These `bayesian_tr_index` and `bayesian_val_index` indexes will be used for the bayesian optimization as training and validation index of training dataset."}}