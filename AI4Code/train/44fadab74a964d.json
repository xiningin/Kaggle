{"cell_type":{"abaa33ff":"code","8e4a1097":"code","a0419e58":"code","0d8b05e2":"code","ddd80f50":"code","4c450932":"code","f398bfa1":"code","a44d0aa7":"code","e32ddf32":"code","33f69d3f":"code","888a98cb":"code","67caac3c":"code","1c1bb53a":"code","a33dbcbe":"code","7cb27514":"code","a6c32e8c":"code","8522d5a0":"code","1e445562":"code","957a0be0":"code","7795c179":"code","d008967a":"code","ca277670":"code","4fa40a3a":"code","6b409ee9":"code","dc0f2892":"code","0e71bf85":"code","dc638442":"code","460f7d9c":"code","ffc64dca":"code","aba60274":"code","81020675":"code","e4df2aff":"code","338dfa73":"code","31d7d53e":"code","32940629":"code","980f07ec":"code","5714f7c0":"code","bc57a3c1":"code","440eb7de":"code","dc01e9bf":"code","71b46f9c":"code","5b454c81":"code","d7ff0ce9":"code","760de3d4":"code","3f0c16e1":"code","53c7e609":"code","99f626bd":"code","2aee0bfc":"code","f94b1866":"code","0b62080c":"code","ebfc44a4":"code","f888bee7":"code","3cce7818":"code","e96d4ace":"code","ce336d48":"code","e055f93e":"code","52cf68de":"code","23ee8aa4":"code","d4fc2742":"code","a93354fe":"code","ebacf4e3":"code","fc9f3a9d":"code","7ddeb0e6":"code","f4680684":"code","5553249d":"code","8cac51bf":"code","6bd1dbbe":"code","e90b2318":"code","893cdaff":"markdown","aaa9f30b":"markdown","f7d47f8b":"markdown","2d421463":"markdown","65a90e97":"markdown","c15c3866":"markdown","d906004d":"markdown","4cd634f7":"markdown","33c71a29":"markdown","390efe79":"markdown","8d985120":"markdown","749aca3b":"markdown","164825e4":"markdown","d8b3e72b":"markdown","c29e8c22":"markdown","f3551879":"markdown","a2208f6e":"markdown","57cd9b6d":"markdown","7c5d57d9":"markdown","3e1b54b7":"markdown","59df0a1c":"markdown","9bc8ae6a":"markdown","ef68b0b9":"markdown","1f79d6fc":"markdown","06e5de55":"markdown","58f6c1b4":"markdown","39de9c8c":"markdown","82feb49d":"markdown","317f59e2":"markdown","4bdfcf3a":"markdown","60ad9cfc":"markdown","feef54b1":"markdown","a3802b68":"markdown","7d85be59":"markdown","8c244a33":"markdown","9494d7a9":"markdown","73387c40":"markdown","f8407e45":"markdown","07603bff":"markdown","c3acebc1":"markdown","01712ed9":"markdown","025a3fc0":"markdown","950d13d2":"markdown","1a56dd46":"markdown","024885fc":"markdown","31e14b6e":"markdown","f47e78ee":"markdown"},"source":{"abaa33ff":"import os\nimport sys\n\nos.chdir(\"..\/input\/framingham-heart-study-dataset\")","8e4a1097":"# import packages\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sb\n\n# import dataset\ndata = pd.read_csv(\"framingham.csv\")","a0419e58":"# view dataset\ndata.head()","0d8b05e2":"# check dimensions\ndata.shape","ddd80f50":"# removing duplicates\ndata.drop_duplicates()\ndata.shape","4c450932":"# checking for blank cells\ndata.isnull().sum()","f398bfa1":"data.fillna(0, inplace = True)","a44d0aa7":"# checking for blank cells\ndata.isnull().sum()","e32ddf32":"# get the mean, standard deviation, count, mix and max of the dataset\ndata.describe()","33f69d3f":"# show the datatypes \ndata.dtypes","888a98cb":"# using creating a dictonary to convert specific objects \nconvert_dict = {\"male\" : str,\n               \"education\" : str,\n               \"currentSmoker\" : str,\n               \"BPMeds\" : str,\n               \"prevalentStroke\" : str,\n               \"prevalentHyp\" : str,\n               \"diabetes\" : str,\n               \"TenYearCHD\" : str}","67caac3c":"# assign the convert_dict datatypes into data dataset\ndata = data.astype(convert_dict)\ndata.dtypes","1c1bb53a":"# number of columns per data type \nlist(set(data.dtypes.tolist()))","a33dbcbe":"plt.figure(figsize=(20,10), facecolor='w')\nsb.boxplot(data=data)\nplt.show()","7cb27514":"print(\"the max totChol is\", data[\"totChol\"].max())\nprint(\"the max sysBP is\", data[\"sysBP\"].max())\nprint(\"the max diaBP is\", data[\"diaBP\"].max())\nprint(\"the max BMI is\", data[\"BMI\"].max())\nprint(\"the max heartRate is\", data[\"heartRate\"].max())\nprint(\"the max glucose is\", data[\"glucose\"].max())","a6c32e8c":"data = data[data[\"totChol\"] < 522]\ndata = data[data[\"sysBP\"] < 221]\ndata = data[data[\"diaBP\"] < 107]\ndata = data[data[\"BMI\"] < 43]\ndata = data[data[\"heartRate\"] < 107]\ndata = data[data[\"glucose\"] < 296]","8522d5a0":"data.shape","1e445562":"data_num = data.select_dtypes(include = [\"float64\", \"int64\"])\ndata_num.head()","957a0be0":"# Understanding numeric Distributions. \ndata_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)","7795c179":"numeric_features = ['cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\nfor feature in numeric_features:\n    plt.figure(figsize=(18, 10), facecolor='w')\n    sb.distplot(data[feature])\n    plt.title('{} Distribution'.format(feature), fontsize=20)\n    plt.show()","d008967a":"sb.heatmap(data_num.corr(), annot = True, cmap = \"magma\")","ca277670":"categorical_features = ['male', 'education', 'currentSmoker', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'TenYearCHD']\nfor feature in categorical_features:\n    plt.figure(figsize=(18, 10), facecolor='w')\n    sb.countplot(data[feature])\n    plt.title('{} Distribution'.format(feature), fontsize=20)\n    plt.show()","4fa40a3a":"sb.countplot(data = data, x = \"TenYearCHD\")","6b409ee9":"# call packages\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","dc0f2892":"# separate the independent columns\nx = data.iloc[:, 0:14]\n\n# separate the dependent column\ny = data.iloc[:, 15]","0e71bf85":"x.head()","dc638442":"y.head()","460f7d9c":"# Using SelectKBest to extract top 10 features\nfrom sklearn.feature_selection import SelectKBest\ntopfeatures = SelectKBest(score_func = chi2, k = 10)\nfit = topfeatures.fit(x, y)\nxyscore = pd.DataFrame(fit.scores_)\nxycolumn = pd.DataFrame(x.columns)","ffc64dca":"# Concatenate both data frames to get features and scores\nfeaturescore = pd.concat([xycolumn, xyscore], axis = 1)\n\n# rename columns featurescore dataframe\nfeaturescore.columns = [\"Feature\", \"Score\"]","aba60274":"# Sort the featurescore by higest-to-lowest scores\nfeaturescore = featurescore.sort_values(by = \"Score\", ascending= False)\nfeaturescore","81020675":"# Plotting the Features based on Scores\nplt.figure(figsize= (20,5))\nsb.barplot(x = \"Feature\", y = \"Score\", data = featurescore)\nplt.box(False)\nplt.title(\"Features Ranked by Scores\", fontsize = 15)\nplt.xlabel(\"\\n Feature\", fontsize = 13)\nplt.ylabel(\"Importance \\n\", fontsize = 13)\nplt.xticks(fontsize = 10)\nplt.xticks(fontsize = 10)\nplt.show()","e4df2aff":"# Selecting thr 10 most important features\nfeatures_list = featurescore[\"Feature\"].tolist()[:10]\nfeatures_list","338dfa73":"data = data[[\"male\", \"age\", \"cigsPerDay\", \"prevalentStroke\", \"prevalentHyp\", \"diabetes\", \"totChol\", \"sysBP\", \"TenYearCHD\"]]\ndata.head()","31d7d53e":"sb.heatmap(data.corr(), annot = True, cmap = \"magma\")","32940629":"# Calling packages\n\nfrom sklearn.preprocessing import MinMaxScaler ","980f07ec":"# Create scaler\nscaler = MinMaxScaler()\n\n# create scaled data\ndata_scaled = pd.DataFrame(scaler.fit_transform (data), columns = data.columns)\n\n# view scaled data\ndata_scaled.describe()","5714f7c0":"# Create X and Y data\n\nx = data_scaled.drop([\"TenYearCHD\"], axis = 1)\ny = data_scaled[\"TenYearCHD\"]\n\n# Data Split library \nfrom sklearn.model_selection import train_test_split","bc57a3c1":"# 75-25 Split \n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)","440eb7de":"len(X_train)","dc01e9bf":"len(X_test)","71b46f9c":"#Installing imbalanced-learn\n#pip install -U imbalanced-learn","5b454c81":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state= 2)","d7ff0ce9":"X_train, y_train = smote.fit_resample(X_train, y_train)","760de3d4":"len(X_train)","3f0c16e1":"len(y_train)","53c7e609":"# importing scikit-learn package. it is also called sklearn\n# pip install -U scikit-learn","99f626bd":"# Calling Evaluation Libraries \n\nimport sklearn\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report","2aee0bfc":"from sklearn.linear_model import LogisticRegression\nmodel_lr = LogisticRegression().fit(X_train, y_train)\nmodel_lr_pred = model_lr.predict(X_test)\n\n# Accuracy of Model \nacc_lr = accuracy_score(y_test, model_lr_pred)\nprint(f\"The Accuracy score for Logistic Regression Model is: {round(acc_lr,3)*100}%\")\n\n# F1 Score\nf1_lr = f1_score(y_test, model_lr_pred)\nprint(f\"The f1 score for Logistic Regression Model is: {round(f1_lr, 3)*100}%\")\n\n# Precision Score\nprec_lr = precision_score(y_test, model_lr_pred)\nprint(f\"The precision for Logistic Regression Model is: {round(prec_lr, 3)*100}%\")\n\n# Recall\/ Sensitivity\nrecall_lr = recall_score(y_test, model_lr_pred)\nprint(f\"The sensitivity for Logistic Regression Model is: {round(recall_lr, 3)*100}%\")\n\n# Confusion Matrix for test\nconfmat_lr = confusion_matrix(y_test, model_lr_pred)\nprint(f\"Confusion Matrix:\", \"\\n\", confmat_lr)\n\n# Plotting Confusion Matrix\nconf_matrix_lr = pd.DataFrame(data=confmat_lr,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsb.heatmap(pd.DataFrame(conf_matrix_lr), annot = True, cmap = \"magma\", fmt = \"g\")","f94b1866":"# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, model_lr_pred)\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Logistic Regression')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)\n\n# Area Under Curve (AUC)\nauc_lr = roc_auc_score(y_test, model_lr_pred)\nprint(f\"The Area Under Curve is {round(auc_lr, 3)*100}%\")","0b62080c":"from sklearn.svm import SVC\nmodel_svm = SVC().fit(X_train, y_train)\nmodel_svm_pred = model_svm.predict(X_test)\n\n# Accuracy of Model \nacc_svm = accuracy_score(y_test, model_svm_pred)\nprint(f\"The Accuracy score for SVM Model is: {round(acc_svm,3)*100}%\")\n\n# F1 Score\nf1_svm = f1_score(y_test, model_svm_pred)\nprint(f\"The f1 score for SVM Model is: {round(f1_svm, 3)*100}%\")\n\n# Precision Score\nprec_svm = precision_score(y_test, model_svm_pred)\nprint(f\"The precision for SVM Model is: {round(prec_svm, 3)*100}%\")\n\n# Recall\/ Sensitivity\nrecall_svm = recall_score(y_test, model_svm_pred)\nprint(f\"The sensitivity for SVM Model is: {round(recall_svm, 3)*100}%\")\n\n# Confusion Matrix for test\nconfmat_svm = confusion_matrix(y_test, model_svm_pred)\nprint(f\"Confusion Matrix:\", \"\\n\", confmat_svm)\n\n# Plotting Confusion Matrix\nconf_matrix_svm = pd.DataFrame(data=confmat_svm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsb.heatmap(pd.DataFrame(conf_matrix_svm), annot = True, cmap = \"viridis\", fmt = \"g\")","ebfc44a4":"# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, model_svm_pred)\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for SVM')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)\n\n# Area Under Curve (AUC)\nauc_svm = roc_auc_score(y_test, model_svm_pred)\nprint(f\"The Area Under Curve is {round(auc_svm, 3)*100}%\")","f888bee7":"from sklearn.tree import DecisionTreeClassifier\nmodel_dt = DecisionTreeClassifier(min_samples_split= 50, random_state= 2).fit(X_train, y_train)\nmodel_dt_pred = model_dt.predict(X_test)\n\n# Accuracy of Model \nacc_dt = accuracy_score(y_test, model_dt_pred)\nprint(f\"The Accuracy score for Decision Tree Model is: {round(acc_dt,3)*100}%\")\n\n# F1 Score\nf1_dt = f1_score(y_test, model_dt_pred)\nprint(f\"The f1 score for Decision Tree Model is: {round(f1_dt, 3)*100}%\")\n\n# Precision Score\nprec_dt = precision_score(y_test, model_dt_pred)\nprint(f\"The precision for Decision Tree Model is: {round(prec_dt, 3)*100}%\")\n\n# Recall\/ Sensitivity\nrecall_dt = recall_score(y_test, model_dt_pred)\nprint(f\"The sensitivity for Logistic Regression Model is: {round(recall_dt, 3)*100}%\")\n\n# Confusion Matrix for test\nconfmat_dt = confusion_matrix(y_test, model_dt_pred)\nprint(f\"Confusion Matrix:\", \"\\n\", confmat_dt)\n\n# Plotting Confusion Matrix\nconf_matrix_dt = pd.DataFrame(data=confmat_dt,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsb.heatmap(pd.DataFrame(conf_matrix_dt), annot = True, cmap = \"YlOrBr\", fmt = \"g\")","3cce7818":"# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, model_dt_pred)\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Decision Tree')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)\n\n# Area Under Curve (AUC)\nauc_dt = roc_auc_score(y_test, model_dt_pred)\nprint(f\"The Area Under Curve is {round(auc_dt, 3)*100}%\")","e96d4ace":"from sklearn.neighbors import KNeighborsClassifier\nmodel_knn = KNeighborsClassifier(n_neighbors = 10).fit(X_train, y_train)\nmodel_knn_pred = model_knn.predict(X_test)\n\n# Accuracy of Model \nacc_knn = accuracy_score(y_test, model_knn_pred)\nprint(f\"The Accuracy score for KNN Model is: {round(acc_knn,3)*100}%\")\n\n# F1 Score\nf1_knn = f1_score(y_test, model_knn_pred)\nprint(f\"The f1 score for KNN Model is: {round(f1_knn, 3)*100}%\")\n\n# Precision Score\nprec_knn = precision_score(y_test, model_knn_pred)\nprint(f\"The precision for KNN Model is: {round(prec_knn, 3)*100}%\")\n\n# Recall\/ Sensitivity\nrecall_knn = recall_score(y_test, model_knn_pred)\nprint(f\"The sensitivity for KNN Model is: {round(recall_knn, 3)*100}%\")\n\n# Confusion Matrix for test\nconfmat_knn = confusion_matrix(y_test, model_knn_pred)\nprint(f\"Confusion Matrix:\", \"\\n\", confmat_knn)\n\n# Plotting Confusion Matrix\nconf_matrix_knn = pd.DataFrame(data=confmat_knn,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsb.heatmap(pd.DataFrame(conf_matrix_knn), annot = True, cmap = \"Spectral\", fmt = \"g\")","ce336d48":"# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, model_knn_pred)\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for KNN')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)\n\n# Area Under Curve (AUC)\nauc_knn = roc_auc_score(y_test, model_knn_pred)\nprint(f\"The Area Under Curve is {round(auc_knn, 3)*100}%\")","e055f93e":"from sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier(n_estimators = 100, random_state = 55, max_depth = 12).fit(X_train, y_train)\nmodel_rf_pred = model_rf.predict(X_test)\n\n# Accuracy of Model \nacc_rf = accuracy_score(y_test, model_rf_pred)\nprint(f\"The Accuracy score for Random Forest Model is: {round(acc_rf,3)*100}%\")\n\n# F1 Score\nf1_rf = f1_score(y_test, model_rf_pred)\nprint(f\"The f1 score for Random Forest Model is: {round(f1_rf, 3)*100}%\")\n\n# Precision Score\nprec_rf = precision_score(y_test, model_rf_pred)\nprint(f\"The precision for Random Forest Model is: {round(prec_rf, 3)*100}%\")\n\n# Recall\/ Sensitivity\nrecall_rf = recall_score(y_test, model_rf_pred)\nprint(f\"The sensitivity for Random Forest Model is: {round(recall_rf, 3)*100}%\")\n\n# Confusion Matrix for test\nconfmat_rf = confusion_matrix(y_test, model_rf_pred)\nprint(f\"Confusion Matrix:\", \"\\n\", confmat_rf)\n\n# Plotting Confusion Matrix\nconf_matrix_rf = pd.DataFrame(data=confmat_rf,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsb.heatmap(pd.DataFrame(conf_matrix_rf), annot = True, cmap = \"icefire\", fmt = \"g\")","52cf68de":"# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, model_rf_pred)\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Random Forest')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)\n\n# Area Under Curve (AUC)\nauc_rf = roc_auc_score(y_test, model_rf_pred)\nprint(f\"The Area Under Curve is {round(auc_rf, 3)*100}%\")","23ee8aa4":"from sklearn.ensemble import GradientBoostingClassifier\nmodel_gb = GradientBoostingClassifier(random_state = 55).fit(X_train, y_train)\nmodel_gb_pred = model_gb.predict(X_test)\n\n# Accuracy of Model \nacc_gb = accuracy_score(y_test, model_gb_pred)\nprint(f\"The Accuracy score for Gradient Boosting Model is: {round(acc_gb,3)*100}%\")\n\n# F1 Score\nf1_gb = f1_score(y_test, model_gb_pred)\nprint(f\"The f1 score for Gradient Boosting Model is: {round(f1_gb, 3)*100}%\")\n\n# Precision Score\nprec_gb = precision_score(y_test, model_gb_pred)\nprint(f\"The precision for Gradient Boosting Model is: {round(prec_gb, 3)*100}%\")\n\n# Recall\/ Sensitivity\nrecall_gb = recall_score(y_test, model_gb_pred)\nprint(f\"The sensitivity for Gradient Boosting Model is: {round(recall_gb, 3)*100}%\")\n\n# Confusion Matrix for test\nconfmat_gb = confusion_matrix(y_test, model_gb_pred)\nprint(f\"Confusion Matrix:\", \"\\n\", confmat_gb)\n\n# Plotting Confusion Matrix\nconf_matrix_gb = pd.DataFrame(data=confmat_gb,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsb.heatmap(pd.DataFrame(conf_matrix_gb), annot = True, cmap = \"magma_r\", fmt = \"g\")","d4fc2742":"# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, model_gb_pred)\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Gradient Boosting')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)\n\n# Area Under Curve (AUC)\nauc_gb = roc_auc_score(y_test, model_gb_pred)\nprint(f\"The Area Under Curve is {round(auc_gb, 3)*100}%\")","a93354fe":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","ebacf4e3":"# Creating RF model\nrf = RandomForestClassifier()\nfrom sklearn.model_selection import RandomizedSearchCV\nrf_random = RandomizedSearchCV(estimator= rf, param_distributions= random_grid, n_iter= 150, cv = 5, verbose= 3, \n                               random_state= 10, n_jobs= -1)","fc9f3a9d":"from sklearn.ensemble import RandomForestClassifier\nmodel_rf = rf_random.fit(X_train, y_train)\nmodel_rf_pred = model_rf.predict(X_test)\n\n# Accuracy of Model \nacc_rf = accuracy_score(y_test, model_rf_pred)\nprint(f\"The Accuracy score for Random Forest Model is: {round(acc_rf,3)*100}%\")\n\n# F1 Score\nf1_rf = f1_score(y_test, model_rf_pred)\nprint(f\"The f1 score for Random Forest Model is: {round(f1_rf, 3)*100}%\")\n\n# Precision Score\nprec_rf = precision_score(y_test, model_rf_pred)\nprint(f\"The precision for Random Forest Model is: {round(prec_rf, 3)*100}%\")\n\n# Recall\/ Sensitivity\nrecall_rf = recall_score(y_test, model_rf_pred)\nprint(f\"The sensitivity for Random Forest Model is: {round(recall_rf, 3)*100}%\")\n\n# Confusion Matrix for test\nconfmat_rf = confusion_matrix(y_test, model_rf_pred)\nprint(f\"Confusion Matrix:\", \"\\n\", confmat_rf)\n\n# Plotting Confusion Matrix\nconf_matrix_rf = pd.DataFrame(data=confmat_rf,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsb.heatmap(pd.DataFrame(conf_matrix_rf), annot = True, cmap = \"icefire\", fmt = \"g\")","7ddeb0e6":"# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, model_rf_pred)\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Random Forest')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)\n\n# Area Under Curve (AUC)\nauc_rf = roc_auc_score(y_test, model_rf_pred)\nprint(f\"The Area Under Curve is {round(auc_rf, 3)*100}%\")","f4680684":"#Number of trees\nn_estimators = [int(i) for i in np.linspace(start=100,stop=1000,num=10)]\n#Number of features to consider at every split\nmax_features = ['auto','sqrt']\n#Maximum number of levels in tree\nmax_depth = [int(i) for i in np.linspace(10, 100, num=10)]\nmax_depth.append(None)\n#Minimum number of samples required to split a node\nmin_samples_split=[2,5,10]\n#Minimum number of samples required at each leaf node\nmin_samples_leaf = [1,2,4]\n\n#Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","5553249d":"gb=GradientBoostingClassifier(random_state=0)\n#Random search of parameters, using 3 fold cross validation, \n#search across 100 different combinations\ngb_random = RandomizedSearchCV(estimator=gb, param_distributions=random_grid,\n                              n_iter=75, scoring='f1', \n                              cv=2, verbose=2, random_state=0, n_jobs=-1,\n                              return_train_score=True)\n\n# Fit the random search model","8cac51bf":"from sklearn.ensemble import GradientBoostingClassifier\nmodel_gb = gb_random.fit(X_train, y_train)\nmodel_gb_pred = model_gb.predict(X_test)\n\n# Accuracy of Model \nacc_gb = accuracy_score(y_test, model_gb_pred)\nprint(f\"The Accuracy score for Gradient Boosting Model is: {round(acc_gb,3)*100}%\")\n\n# F1 Score\nf1_gb = f1_score(y_test, model_gb_pred)\nprint(f\"The f1 score for Gradient Boosting Model is: {round(f1_gb, 3)*100}%\")\n\n# Precision Score\nprec_gb = precision_score(y_test, model_gb_pred)\nprint(f\"The precision for Gradient Boosting Model is: {round(prec_gb, 3)*100}%\")\n\n# Recall\/ Sensitivity\nrecall_gb = recall_score(y_test, model_gb_pred)\nprint(f\"The sensitivity for Gradient Boosting Model is: {round(recall_gb, 3)*100}%\")\n\n# Confusion Matrix for test\nconfmat_gb = confusion_matrix(y_test, model_gb_pred)\nprint(f\"Confusion Matrix:\", \"\\n\", confmat_gb)\n\n# Plotting Confusion Matrix\nconf_matrix_gb = pd.DataFrame(data=confmat_gb,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsb.heatmap(pd.DataFrame(conf_matrix_gb), annot = True, cmap = \"magma_r\", fmt = \"g\")","6bd1dbbe":"# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, model_gb_pred)\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Gradient Boosting')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)\n\n# Area Under Curve (AUC)\nauc_gb = roc_auc_score(y_test, model_gb_pred)\nprint(f\"The Area Under Curve is {round(auc_gb, 3)*100}%\")","e90b2318":"# FPR stands for False Positive Rate and TPR stands for True Positive Rate\n\n# Creating the FPR & TPR for each model \n\nFPR_lr, TPR_lr, threshold_lr = roc_curve(y_test, model_lr_pred)\nFPR_svm, TPR_svm, threshold_svm = roc_curve(y_test, model_svm_pred)\nFPR_dt, TPR_dt, threshold_dt = roc_curve(y_test, model_dt_pred)\nFPR_knn, TPR_knn, threshold_knn = roc_curve(y_test, model_knn_pred)\nFPR_rf, TPR_rf, threshold_rf = roc_curve(y_test, model_rf_pred)\nFPR_gb, TPR_gb, threshold_gb = roc_curve(y_test, model_gb_pred)\n\n# Plotting the rates of each model \nsb.set_style(\"whitegrid\")\nplt.figure(figsize = (15,8), facecolor = \"w\")\nplt.title(\"Receiver Operating Characteristics Curve\")\nplt.plot(FPR_lr, TPR_lr, label = \"Logistic Regression\")\nplt.plot(FPR_svm, TPR_svm, label = \"Support Vector Machine\")\nplt.plot(FPR_dt, TPR_dt, label = \"Decision Tree\")\nplt.plot(FPR_knn, TPR_knn, label = \"KNearest Neighbor\")\nplt.plot(FPR_rf, TPR_rf, label = \"Random Forest\")\nplt.plot(FPR_gb, TPR_gb, label = \"Gradient Booster\")\nplt.plot([0,1], ls = \"--\")\nplt.plot([0,0], [1,0], c = \"0.5\")\nplt.plot([1,1], c = \"0.5\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.legend()\nplt.show()\n","893cdaff":"<u>Random Forest <\/u>","aaa9f30b":"The metrics to be used to determine the best models are: \n\n1. <b>Confusion Matrix<\/b> - It is a 2x2 or sometimes nxn matrix that gives the values of the predicted vs actual. \nAbbreviations to be used: \n\na. True Positive = TP (the actual value (y_actual) is 1 and the predicted value (y_pred) is 1 as well)\n\nb. True Negative = TN (the actual value (y_actual) is 0 and the predicted value (y_pred) is 0 as well)\n\nc. False Positive = FP (the actual value (y_actual) is 0 but the predicted value (y_pred) is 1)\n\nd. False Negative = FN (the actual value (y_actual) is 1 but the predicted value (y_pred) is 0)\n\n1. <b>Accuracy<\/b> - Accuracy predicts how accurately the y_predict matches the y_actual values. The closer the accuracy of the model is to 100%, the better the model. It is mathematically calculated as \n\n<i>Accuracy = (TP + TN)\/ Total <\/i>\n\n2. <b>Precision Score<\/b> - Precision is the ratio of True positives vs predicted positives. The lesser the number of False Positives, the better the values. It is calculated as \n\n<i>Precision = TP \/ (TP + FP) <\/i>\n\n3. <b>Recall Score \/ Sensitivity<\/b> - Recall is the ratio of true positives vs actual positives. The lesser the number of False Negatives, the better the recall score. It is calculated as\n\n<i>Recall\/ Sensitivity = TP \/ (TP + FN) <\/i>\n\n4. <b>F1 Score<\/b> - It combines Recall and Precision scores. The closer the F1 score is to 100%, better the model. \n\n<i> F1 Score = 2 \/ ((1\/Recall) + (1\/Precision))\n    \n    5. <b>ROC Curve<\/b> - ROC (Receiver Operating Characteristics) curve is a visualization of the model output in terms of False Positive Rate (FPR = FP \/ (FP + TN)) on the x-axis and True Postive Rate (TPR = TP \/ (TP + FN)) on the y-axis. \n    \n    6. <b>AUC (Area Under Curve) <\/b> - Area under curve represents degrees or measurements of separability. The higher the AUC, the better the model is at predicting 0s correctly as 0s and 1s as 1s. \n    \n    AUC thumb rule: \n    \n       a. 0.9 - 1: excellent\n       b. 0.8 - 0.9: good\n       c. 0.7 - 0.8: fair\n       d. 0.6 - 0.7: poor\n       e. 0.5 - 0.6: fail","f7d47f8b":"<u> Dataset Link <\/u>: https:\/\/www.kaggle.com\/amanajmera1\/framingham-heart-study-dataset","2d421463":"There are outliers in totChol, sysBP, diaBP, BMI, heartRate and Glucose columns. I will remove the outliers as anything over 75% of the maximum value. ","65a90e97":"KNN model has an accuracy of 66%, F1 score of 30% and AUC of 60%. ","c15c3866":"##### SMOTE\n\nSMOTE is Synthetic Minority Oversampling TEchnique. Oversampling duplicates existing minority class values. But SMOTE draws lines between existing minority class members and creates brand new minority members between the existing ones. Usually SMOTE is used to prepare the Train dataset. ","d906004d":"#### Plotting ROC Characteristics of Each Model ","4cd634f7":"### Box Plot\n\nCreating a Box Plot and treating outliers","33c71a29":"### Categorical Variables ","390efe79":"## Data Import & Clean Up","8d985120":"#### <u>SVM (Support Vector Machine)<\/u> \n\nThe objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N \u2014 the number of features) that distinctly classifies the data points. Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. The values lie between -1 to 1. ","749aca3b":"Cigarettes per day are not normally distributed. It is because most people are not smokers. \n\nTotal Cholesterol, systolic BP, diastolic BP, BMI are normally distributed. \n\nGlucose is not normally distributed. That might be because we have blank values in glucose column. ","164825e4":"The average BMI of the group is around 25 and the maximum number of people have a BMI between 23 to 28. This means that most of the people in the group are bordering on overweight to actually being overweight. \n\nAge of people in the study varies between 32 to 70 and the average age is around 50. So the people in the study are middle aged to old. \n\nEven though the average number of cigerattes smoked is 9, most upto 50% of the people did not smoke at all. One outlier smoked 70 cigerattes a day! \n\nA BP of a person is written as Systolic BP \/ Diastolic BP. Systolic BP means the blood pressure when the heart beats. Diastolic BP means the blood pressure when the heart is resting. Average BP is 132\/82 (indicates hypertension). \n\nThe average Glucose levels is 82 and it varies between 71 to 87 which means none of them are diabetic at the moment. \n\nAverage heart rate is 76 with the maximum number of people between 68 to 83. An outlier has a heart rate of 143. \n\nMost people have a cholesterol levels of 206 to 263, with average being 237. This indicates Borderline high to high cholesterol level.","d8b3e72b":"Gradient Boosting has accuracy of 74%, F1 score of 33% and AUC of 63%. ","c29e8c22":"#### <u> Gradient Boosting <\/u>\n\nBoosting is a method of converting weak learners into strong learners. Weak learners here means decision trees. Gradient Boosting trains many models in a gradual, additive and sequential manner. XGB involves 3 elements - a loss function to be optimized, a weak learner to make predictions and an additive model to add the weak learners to minimize the loss function.","f3551879":"#### <u>Logistic Regression<\/u> \n\nA logistic regression model predicts a dependent data variable by analyzing the relationship between one or more existing independent variables.It is a very common statistical model used to determine if an independent variable has an effect on a binary dependent variable. The sigmoid fuction compresses the values to lie between 0 to 1.","a2208f6e":"#### <u>KNN (K Nearest Neighbor)<\/u> \n\nKNN - K Nearest Neighbor assumes that the similar points are neighbors- that is points with similar characteristics lie close to each other. One major advantage of KNN is that it is a non-parametric learning algorithm, that is - the algorithm starts out with no assumptions. ","57cd9b6d":"### Divide Train & Test Data \n\nDivide data into train and test data with a 75-25 split first. ","7c5d57d9":"### Hyperparameter Tuning for best Classifier\n\nA Machine Learning model is defined as a mathematical model with a number of parameters that need to be learned from the data. By training a model with existing data, we are able to fit the model parameters. However, there is another kind of parameters, known as Hyperparameters, that cannot be directly learned from the regular training process. They are usually fixed before the actual training process begins. These parameters express important properties of the model such as its complexity or how fast it should learn.\n\nThe 2 methods used are: \n\n1. GridSearchCV - This approach is called GridSearchCV, because it searches for best set of hyperparameters from a grid of hyperparameters values. Since it goes through all the intermediate combinations of hyperparameters which makes grid search computationally very expensive.\n\n2. RandomizedSearchCV - RandomizedSearchCV solves the drawbacks of GridSearchCV, as it goes through only a fixed number of hyperparameter settings. It moves within the grid in random fashion to find the best set hyperparameters. This approach reduces unnecessary computation.\n\n\n#### Using Randomized Search Cross Validation","3e1b54b7":"## Objective\n\nFor The dataset provides the risk factors associated with heart disease for the patients and whether they have a risk of coronary heart disease in the next 10 years. \n\nBased on the dataset provided:\n1.\tPredict the probability of a patient suffering a coronary heart disease in the next 10 years\n2.\tIdentify the most important factors that influence heart disease\n3.\tCome up with recommendations for\n\n  a.\tPreventing \/ reducing chances of getting a heart disease\n  \n  b.\tExtrapolated applications of the model you build and its findings","59df0a1c":"Systolic BP & Diastolic BP are highly correlated to each other. \n\nSystolic BP & Diastolic BP each are mildly correlated to BMI (Body Mass Index). \n\nSystolic BP & Age are mildly correlated to each other.","9bc8ae6a":"### Balancing & Modelling The Dataset\n\nThe dataset is very imbalanced as only 15% of the sampled population are at risk of developing heart risk at the end of 10 years. The problem with imbalanced dataset The most common methods are - oversampling and undersampling \n\n<u>Oversampling<\/u> \u2014 Duplicating samples from the minority class. Seeking a balanced distribution for a severely imbalanced dataset can cause affected algorithms to overfit the minority class, leading to increased generalization error.\n\n<u>Undersampling<\/u> \u2014 Deleting samples from the majority class. This has the effect of reducing the number of examples in the majority class in the transformed version of the training dataset.A limitation of undersampling is that examples from the majority class are deleted that may be useful, important, or perhaps critical to fitting a robust decision boundary. This can lead to underfit. \n\n\nImportantly, the change to the class distribution is only applied to the training dataset. The intent is to influence the fit of the models. The resampling is not applied to the test or holdout dataset used to evaluate the performance of a model.","ef68b0b9":"### Correlation Plot ","1f79d6fc":"Random Forest has AUC of 62.2%, Accuracy of 71% and F1 score of 33%. ","06e5de55":"## EDA Conclusions\n\n<u> Summary of Study Group <\/u> : The study consists of 4,240 borderline-overweight Middle-Aged to Older patients, with the sex ratio slightly skewed towards Women, consisting of an almost even split between smokers and non-smokers. There are no diabetics in the group, and  only 25 people or 0.6% of the study group have had a history of strokes. Everyone in the study has high cholestrol levels.\n\n\n<u> Results of EDA <\/u> : 85% of people are at a very low risk of heart failure by the end of 10 years. Older Women are more at risk of heart attacks. \n\n<u> Data Insights <\/u> :\n1. Data is unbalanced. \n\n2. Education level does not seem to be related to the heart risk. If we had any data related to their occupation, then it might provide context to analyse the education data Vs Heart Risk. \n\n3. Systolic BP & Diastolic BP are highly correlated to each other.","58f6c1b4":"## <u>Most Important Features<\/u>\n\nThe most important features that affect the 10-year Heart Risk factor:\n\n1. Systolic BP\n2. Age\n3. Cigerattes Per Day\n4. Total Cholestoral\n5. Prevalent Hypertension\n6. Diastolic BP\n7. Diabetics\n8. BP Medicines\n9. Sex\n10. Prevalent Stroke\n\nThe features to be ignored are education, BMI, heart rate and current smoker. We did not have any information about education to begin with and EDA analysis supported the fact that education was not a relavent factor. BMI seemed important at first glance but I will go with Univariate Feature Selection Analysis results. Same for heart rate and current smoker. \n\nI will disregard Diastolic BP and BP Meds as they are found to be highly correlated with Systolic BP.\n\nI will only be using these factors for all further analysis.","39de9c8c":"## <u>Recommendations<\/u>\n\n### Recommendations for Preventing or Reducing Chances of Heart Attack\n\nThe recommendations from the models show that there are 2 factors under our control to reduce chance of heart attack and 1 factor that is not. \n\nThe 2 factors that we can control are: \n\n1. Systolic <u>BP<\/u> is one of the most important factor to determine risk of heart attack. So keeping BP in check is essential. \n\n2. The number of <u>cigarettes<\/u> smoked per day is directly proportionate to the probability of heart attack. So smoking kills!\n\nThe factor not under our control, that increases chance of heart attack is <u>Age<\/u>. As we age, the probability of getting a heart attack increases as well. So regular excercises and a healthy lifestyle is the only way to reduce chance of heart attack.\n\n\n\n### Extrapolated applications of the model you build and its findings\n\n1. The data overall, is highly imbalanced. Additional data, especially for the minority class will improve the accuracy of prediction.\n\n2. The Gradient Boosting Model has the highest accuracy at 81% so if there are high-level decisions to be made, such as resources to be allocated to hospitals, support for patients or long-term health budget allotments, then this is the model to use. \n\n3. The Support Vector Machine (SVM) was the best performing model in terms of F1, Precision and Recall scores. So, if any individual assessment is to be made, based on the demographic, behavioural and medical information, then SVM is the method to use\n","82feb49d":"## Feature Selection\n\nFeature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.\n\nHaving irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.\n\nThree benefits of performing feature selection before modeling your data are:\n\n1. Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n2. Improves Accuracy: Less misleading data means modeling accuracy improves.\n3. Reduces Training Time: Less data means that algorithms train faster.\n\nThere are many ways to do feature selection - Univariate Feature Selection, Recursive Feature Selection, Model Feature Selection. I am using Univariate Feature Selection as it checks how each feature affects the predicted variable.","317f59e2":"### Modelling & Evaluation ","4bdfcf3a":"## Model Building","60ad9cfc":"This still shows a very strong correlation between systolic BP and diastolic BP, which makes sense.","feef54b1":"## Background \n\nThis is a part of data from an ungoing study being conduted in Framingham, Massachusetts, USA. It includes 15 columns and around 4200 rows, where each row presents a person's behavioural, demographic and medical (history and current) information. Each column is a potential risk factor. \n\n<u>Demographic<\/u>:\n\u2022 Sex: male or female(Nominal)\n\n\u2022 Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n\n<u>Behavioral<\/u>:\n\u2022 Current Smoker: whether or not the patient is a current smoker (Nominal)\n\n\u2022 Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n\n<u>Medical( history)<\/u>:\n\u2022 BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n\n\u2022 Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n\n\u2022 Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n\n\u2022 Diabetes: whether or not the patient had diabetes (Nominal)\n\n<u>Medical(current)<\/u>:\n\u2022 Tot Chol: total cholesterol level (Continuous)\n\n\u2022 Sys BP: systolic blood pressure (Continuous)\n\n\u2022 Dia BP: diastolic blood pressure (Continuous)\n\n\u2022 BMI: Body Mass Index (Continuous)\n\n\u2022 Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n\n\u2022 Glucose: glucose level (Continuous)\n\n<u>Predict variable (desired target)<\/u>:\n\n\u2022 10 year risk of coronary heart disease CHD (binary: \u201c1\u201d, means \u201cYes\u201d, \u201c0\u201d means \u201cNo\u201d)","a3802b68":"### Continous Variables ","7d85be59":"#### Histogram","8c244a33":"#### Bar Graph","9494d7a9":"The Decision Tree model isnt performing upto expectations. Though the accuracy is better, compared to SVM or Logistic Regression Models, the AUC and F1 scores are very low. So tuning the parameters is the way to go.","73387c40":"<u>Gradient Boosting <\/u>","f8407e45":"#### <u>Decision Tree<\/u> \n\nA Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label. A decision tree is drawn upside down with its root at the top. One advantage of decision trees is that they provide a clear indication of which fields are most important to prediction or classification.","07603bff":"The evaluation metrics are not encouraging. A 67% accuracy, 38% F1 Score and AUC of 70% are not encouraging. ","c3acebc1":"### Exploratory Data Analytics (EDA)","01712ed9":"Based on the box plots, it looks like the data is unbalanced. \n\nAround 53% of the people interviewed seems to be women. \n\nThere seems to be an almost even split between the number of current smokers Vs current non-smokers. \n\nAlmost 97% of the people do not take any medicines to control their BP. \n\nAround 99.5% of people in the study have not had a stroke before. \n\n69% of the people are not suffering from hypertension and 97% do not have diabetes. \n\n85% of the people interviewed have a very low risk of getting a coronary heart issue.","025a3fc0":"   # Framingham Heart Study","950d13d2":"#### Bar Plot & Line Graphs","1a56dd46":"## New Dataset with Selected Features","024885fc":"#### <u>Random Forest <\/u> \n\nRandom forest is an ensemble of a large number of relatively uncorrelated trees, each of which make their own decisions. The advantage of this is that, each tree is unbiased by the output or workings of the other, moving the decision in an overall right way. ","31e14b6e":"### Scaling Data\n\nSince each of the features that affect Ten-YearCHD are on different scales, which leads to data models giving certain features undue importance. Scaling the data brings all the columns onto the same level. \n\nThere are different ways to scale data, like Standard Scaler, MaxAbs Scaler, MinMax Scaler and Robust Scaler. Let's take a look at each one. \n\n1. <u>Standard Scaler<\/u>: StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation.StandardScaler results in a distribution with a standard deviation equal to 1 and the mean becomes 0. \n\n2. <u>MinMax Scaler<\/u>: For each value in a feature, MinMaxScaler subtracts the minimum value in the feature and then divides by the range. (Range = original maximum - original minimum). MinMaxScaler preserves the shape of the original distribution and is used as it does not reduce the significance of outliers. \n\n3. <u>MaxAbs Scaler<\/u>: MaxAbs Scaler works exactly like MinMax Scaler if all the features are positive. If any of the features are negative, then MaxAbs Scaler considers the absolute values and plots them against a range of 0 to 1. \n\n4. <u>Robust Scaler<\/u>: Robust Scaler transforms each feature by subtracting the median and then dividing the interquartile range (Interquartile Range = 75th percentile value - 25th percentile value). Robust Scaler reduces the effect of outliers and it does not scale data into a predetermined interval. \n\nI will be using MinMax Scaler as the Framingham data is sensitive to outliers, captures very small deviations of featuers and preserves zero entries in sparse data. ","f47e78ee":"The SVM model does not show promising results as well. The accuracy is 66%, F1 score is 33% and AUC is 64%"}}