{"cell_type":{"8e60d418":"code","3ca62c48":"code","ae5fdf89":"code","55ddb6ae":"code","e0933fe7":"code","fdc7353e":"code","0ef6db9f":"code","089d673d":"code","98054623":"code","e3986122":"code","d140da12":"code","3320fdf5":"code","1ebb64b5":"code","2b27c2f0":"code","ae9c718f":"code","073caf23":"code","f3a7afaa":"code","de70c813":"code","c79ac4e4":"code","a6310d6f":"code","31eebaf5":"code","6033e91c":"code","26e80891":"code","22235c2b":"code","67d6268d":"code","02bf358b":"code","e748d543":"markdown","f24ae6c4":"markdown","6f96bf67":"markdown","3c2599af":"markdown","826cbba1":"markdown","4c5c8a58":"markdown","afe93a46":"markdown","15d65a1e":"markdown"},"source":{"8e60d418":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","3ca62c48":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nfrom tensorflow.keras.callbacks import TensorBoard\nprint(tf.__version__)","ae5fdf89":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv')\ntrain_extra = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv')\ntrain.head()","55ddb6ae":"# row_id is excess and leads to overfitting\ntrain = train.drop(\"row_id\",axis=1)\ntest = test.drop(\"row_id\",axis=1)","e0933fe7":"from datetime import datetime\n\nis_weekend = True\ndef data_time_process(data_type):\n    time_column = pd.to_datetime(data_type.iloc[:,0],format='%Y-%m-%d')\n    years = []\n    months = []\n    days = []\n    weeks = []\n    day_of_year = []\n    for el in (time_column.astype(\"str\").to_list()):\n        year = int(el.split('-')[0])\n        years.append(year)\n        month = int(el.split('-')[1])\n        months.append(month)\n        day = int(el.split('-')[2])\n        d = datetime(year, month, day)\n        weeks.append(d.isocalendar()[1]) # [1] - is a week part\n        day_of_year.append(int(d.timetuple().tm_yday))\n        if (int(d.timetuple().tm_wday) < 4): # idea that Friday is also make sense\n            days.append(0)\n        elif (int(d.timetuple().tm_wday) == 4):\n            days.append(1)\n        elif (int(d.timetuple().tm_wday) >= 5):\n            days.append(2)\n\n    data_type['year'] = years\n    data_type['month'] = months\n    data_type['is_weekday'] = days\n    data_type['day_of_year'] = day_of_year\n    data_type['weeks'] = weeks\n","fdc7353e":"data_time_process(train)\ndata_time_process(test)","0ef6db9f":"test = test.drop('date',axis=1)\ntrain = train.drop('date',axis=1)","089d673d":"train.head()","98054623":"from datetime import datetime\nfrom category_encoders import *\n\n# I'll use simple one-hot encoder, also should try binary or ordinal encoder;\n# Maybe it's better to learn N different models for different country e.g, and then combine it\ntrain = OneHotEncoder(cols=['product']).fit(train).transform(train)\ntest = OneHotEncoder(cols=['product']).fit(test).transform(test)\n\ntrain = OneHotEncoder(cols=['store']).fit(train).transform(train)\ntest = OneHotEncoder(cols=['store']).fit(test).transform(test)\n\ntrain = OneHotEncoder(cols=['country']).fit(train).transform(train)\ntest = OneHotEncoder(cols=['country']).fit(test).transform(test)","e3986122":"train.head()","d140da12":"# define features that will be used in fitting\nfeatures = ['country_1','country_2','country_3','store_1','store_2','product_1','product_2','product_3','year','month','is_weekday','day_of_year','weeks']","3320fdf5":"train_dataset = train.sample(frac=0.8, random_state=0)\ntest_dataset = train.drop(train_dataset.index)\n\ntrain_features = train.copy()\ntest_features = test_dataset.copy()\n\ntrain_labels = pd.DataFrame([train_features.pop(x) for x in ['num_sold']]).T\ntest_labels = pd.DataFrame([test_features.pop(x) for x in ['num_sold']]).T","1ebb64b5":"# First layer - normalize it\nnormalizer = preprocessing.Normalization(axis=-1)\nnormalizer.adapt(np.array(train_features[features]))","2b27c2f0":"model = keras.Sequential([ normalizer,\n                           layers.Dense(64,activation='relu'),\n                           layers.Dense(32, activation='relu'),\n                           #layers.Dropout(0.1),\n                           layers.Dense(16, activation='relu'),\n                           layers.Dense(8, activation='relu'),\n                           layers.Dense(4, activation='relu'),\n                           layers.Dense(1)])\nmodel.summary()","ae9c718f":"import tensorflow.keras.backend as K\n#https:\/\/keras.io\/api\/losses\/\ndef smape_loss(y_true, y_pred):\n    denominator = (K.abs(y_true) + K.abs(y_pred)) \/ 200.0\n    smape = K.abs(y_pred - y_true) \/ denominator\n    return K.mean(smape)","073caf23":"# train model\nmodel.compile(loss=smape_loss, optimizer=tf.keras.optimizers.Adam(5e-4))\nhistory = model.fit(train_features[features],\n                    np.asarray(train_labels[['num_sold']]).astype('float32'),\n                    validation_split=0.2, verbose=2, epochs=35)","f3a7afaa":"def plot_loss(history):\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.ylim([5, 50])\n    plt.xlabel('Epoch')\n    plt.ylabel('Error')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\nplot_loss(history)","de70c813":"test_predictions_orig = model.predict(test[features])\ntest_predictions = model.predict(test_features[features])","c79ac4e4":"def SMAPE(y_true, y_pred):\n    denominator = (np.abs(y_true) + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\nSMAPE(np.asarray(test_labels['num_sold'].to_list()),test_predictions) # adding +100 for test_labels decrease SMAPE to 62","a6310d6f":"j = 0\nx_axis = []\ny_axis_pred = []\ny_axis_orig = []\n# each 0 + 18*n string (as 18 is period in data)\nfor i in range(0, 5260):\n    if j == 18:\n        x_axis.append(i\/18)\n        y_axis_pred.append(test_predictions[i])\n        y_axis_orig.append(np.asarray(test_labels['num_sold'].to_list())[i])\n        j = 0\n    j += 1","31eebaf5":"plt.plot(x_axis, y_axis_orig)\nplt.plot(x_axis, y_axis_pred)\n# Looks almost the same, but too looks like overfitted","6033e91c":"plt.plot(x_axis, y_axis_pred)\n","26e80891":"result = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')\nresult['num_sold'] = test_predictions_orig\nresult.to_csv('.\/example_dnn.csv',index=False)","22235c2b":"result # score on PB ~ 17, as we can see data is undervaluated,\n# maybe peak from December confuse the model, we should experiment with architecture\n# nevertheless, it was experiment of using NN with tabular data!","67d6268d":"j = 0\nx_axis = []\ny_axis = []\n# each 0 + 18*n string (as 18 is period in data)\nfor i in range(0, 6570):\n    if j == 18:\n        x_axis.append(i\/18)\n        y_axis.append(result['num_sold'][i])\n        j = 0\n    j += 1","02bf358b":"plt.plot(x_axis, y_axis)\n# looks like result","e748d543":"### EDA with date","f24ae6c4":"### Export and explore the result","6f96bf67":"### Let's work with categorical feature, because DNN works only with float and int","3c2599af":"### Visualize training","826cbba1":"### Creating DNN model ","4c5c8a58":"## Read the data","afe93a46":"## Prediction and visualizing for validation data","15d65a1e":"### Define SMAPE loss for learning "}}