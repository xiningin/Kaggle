{"cell_type":{"b0bb9ada":"code","b9c54883":"code","32ae2374":"code","d30d40d0":"code","708e8342":"code","2976365b":"code","689d177d":"code","a81a737d":"code","03e32517":"code","4a775bf0":"code","d88b1d6b":"code","23562830":"code","0dfd4be6":"code","6aca61bb":"code","19725397":"code","77e57492":"code","b7b4c0da":"code","52d28f80":"code","d2b5285f":"code","115a7a4e":"code","d3cade53":"code","d3e52039":"code","75265922":"code","a2f20d25":"code","8c9282fb":"code","090325ae":"code","e9423b0b":"code","2d31fe8a":"code","34c6c714":"code","ae4fb318":"code","d3dbb908":"code","14949186":"code","cf4aaad7":"code","e10de526":"code","302483a5":"code","87dd321a":"code","38663b17":"code","6802e556":"code","b72a0c44":"code","bb2b8230":"code","e2d25dd7":"code","05831937":"code","73a0c570":"code","674732c7":"code","771cdf27":"code","8ab3821e":"markdown","41e2f27e":"markdown","bdd2f7a2":"markdown","aeddeb5c":"markdown","cba97bae":"markdown","a2ded6e3":"markdown","0060d172":"markdown","d9c5a692":"markdown","b7e82283":"markdown","340825b7":"markdown","d057e00e":"markdown","9859eb9e":"markdown","8cac158a":"markdown","01bf48bc":"markdown","28076515":"markdown","9f0d9d90":"markdown","0570ae30":"markdown","e3a67f35":"markdown","d82e6494":"markdown"},"source":{"b0bb9ada":"# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold","b9c54883":"# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt","32ae2374":"df1 = pd.read_csv('..\/input\/credit-card-approval-prediction\/application_record.csv')\ndf2 = pd.read_csv('..\/input\/credit-card-approval-prediction\/credit_record.csv')","d30d40d0":"df1.head(5)\n","708e8342":"df2.head(2)","2976365b":"# Verificando a presen\u00e7a de valores nulos no data frame\ndf1.isnull().sum().sum()","689d177d":"# Verificando a presen\u00e7a de valores nulos no data frame\ndf2.isnull().sum().sum()","a81a737d":"# Verificando a presen\u00e7a de NA's no data frame\ndf1.isnull().values.any()","03e32517":"#REmovendo os Null\/NA\ndf1 = df1.dropna()","4a775bf0":"#verificando se sobrou algum Null\/NA\ndf1.isnull().values.any()","d88b1d6b":"index = df1.index\nnumber_of_rows = len(index)\nprint(number_of_rows)","23562830":"index = df2.index\nnumber_of_rows = len(index)\nprint(number_of_rows)","0dfd4be6":"df3 = pd.merge(df1, df2, on='ID')\ndf3.head(3)","6aca61bb":"#Verificando a quantidade de dados depois da jun\u00e7\u00e3o dos conjuntos\nindex = df3.index\nnumber_of_rows = len(index)\nprint(number_of_rows)","19725397":"#verificando se sobrou algum Null\/NA\ndf3.isnull().values.any()","77e57492":"#Tirando os ID duplicados\ndf3 = df3.drop_duplicates('ID',keep='first')\n\n","b7b4c0da":"#Verificando a quantidade de dados depois da jun\u00e7\u00e3o dos conjuntos\ndf3","52d28f80":"#Removendo a vari\u00e1vel CODE_GENDER para n\u00e3o ter vi\u00e9s sexista na base de dados\ndf3 = df3.drop(columns=['CODE_GENDER'])\ndf3.head(3)","d2b5285f":"# Trasnformando todos de valores Y ou N em dummies, sendo 1 para Y\ndummy1 = pd.get_dummies(df3.FLAG_OWN_CAR)\ndf3['FLAG_OWN_CAR'] = dummy1['Y']\n\ndummy2 = pd.get_dummies(df3.FLAG_OWN_REALTY)\ndf3['FLAG_OWN_REALTY'] = dummy2['Y']\n\n\n#Vendo as classes das vari\u00e1veis categ\u00f3ricas\n#print(df3['NAME_INCOME_TYPE'].unique())\n\n#Vendo as classes das vari\u00e1veis categ\u00f3ricas\n#print(df3['NAME_EDUCATION_TYPE'].unique())\n\n#Vendo as classes das vari\u00e1veis categ\u00f3ricas\n#print(df3['OCCUPATION_TYPE'].unique())\n\n#Vendo as classes das vari\u00e1veis categ\u00f3ricas\n#print(df3['STATUS'].unique())\n\n\n","115a7a4e":"###criando uma vari\u00e1vel ordinal para o n\u00edvel de escolaridade\n#df3['NAME_EDUCATION_TYPE'] =\n\n","d3cade53":"#### Vamos ver como s\u00e3o os n\u00edveis de consumo por categorias sociais\n\n\nfig, axes = plt.subplots(1, 4, figsize=(15, 5), sharey=True)\nfig.suptitle('Consumo por caracter\u00edstica')\n\n# Bulbasaur\nsns.barplot(ax=axes[0], x=df3.NAME_INCOME_TYPE, y=df3.AMT_INCOME_TOTAL).tick_params(labelrotation=45)\naxes[0].set_title(\"Ocupa\u00e7\u00e3o\")\n\n\n# Charmander\nsns.barplot(ax=axes[1], x=df3.NAME_EDUCATION_TYPE, y=df3.AMT_INCOME_TOTAL).tick_params(labelrotation=45)\naxes[1].set_title(\"Escolaridade\")\n\n# Squirtle\nsns.barplot(ax=axes[2], x=df3.NAME_FAMILY_STATUS, y=df3.AMT_INCOME_TOTAL).tick_params(labelrotation=45)\naxes[2].set_title(\"Estatus Civil\")\n\n#\nsns.barplot(ax=axes[3], x=df3.NAME_HOUSING_TYPE, y=df3.AMT_INCOME_TOTAL).tick_params(labelrotation=45)\naxes[3].set_title(\"Moradia\")\n\n","d3e52039":"\n###criando UM catplot individual para a variavel OCCUPATION_TYPE em rela\u00e7\u00e3o ao poder de compra AMT_INCOME_TOTAL\nplt.figure(figsize =(10,5))\nax = sns.barplot(x=\"OCCUPATION_TYPE\", y=\"AMT_INCOME_TOTAL\",data=df3).set_title('Consumo por profiss\u00e3o')\nplt.xticks(rotation=60)","75265922":"#m\u00e9dia de consumo por profiss\u00e3o\ndf4 = df3.groupby(['OCCUPATION_TYPE']).mean().sort_values(['AMT_INCOME_TOTAL'], ascending=False)\ndf4['AMT_INCOME_TOTAL']","a2f20d25":"\n\n# Como temos 18 profiss\u00f5es, vamos criar um indice de impacto de 6 n\u00edveis, de acordo com o poder de consumo\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].replace(['Managers','Realty agents'],6)\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].replace(['Drivers','Accountants','IT staff','Private service staff'],5)\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].replace(['High skill tech staff','HR staff','Core staff','Laborers'],4)\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].replace(['Security staff','Sales staff','Secretaries','Medicine staff'],3)\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].replace(['Drivers','Accountants','IT staff','Private service staff'],2)\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].replace(['Waiters\/barmen staff','Cleaning staff','Cooking staff','Low-skill Laborers'],1)\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].apply(pd.to_numeric)","8c9282fb":"# Fazendo o mesmo para educa\u00e7\u00e3o\ndf5 = df3.groupby(['NAME_EDUCATION_TYPE']).mean().sort_values(['AMT_INCOME_TOTAL'], ascending=False)\ndf5['AMT_INCOME_TOTAL']","090325ae":"df3['NAME_EDUCATION_TYPE'] = df3['NAME_EDUCATION_TYPE'].replace(['Academic degree'],5)\ndf3['NAME_EDUCATION_TYPE'] = df3['NAME_EDUCATION_TYPE'].replace(['Higher education'],4)\ndf3['NAME_EDUCATION_TYPE'] = df3['NAME_EDUCATION_TYPE'].replace(['Incomplete higher'],3)\ndf3['NAME_EDUCATION_TYPE'] = df3['NAME_EDUCATION_TYPE'].replace(['Secondary \/ secondary special'],2)\ndf3['NAME_EDUCATION_TYPE'] = df3['NAME_EDUCATION_TYPE'].replace(['Lower secondary'],1)\ndf3['NAME_EDUCATION_TYPE'] = df3['NAME_EDUCATION_TYPE'].apply(pd.to_numeric)\nprint(df3['NAME_EDUCATION_TYPE'].unique())","e9423b0b":"# Fazendo o mesmo para finalidade de uso do cr\u00e9dito\ndf6 = df3.groupby(['NAME_INCOME_TYPE']).mean().sort_values(['AMT_INCOME_TOTAL'], ascending=False)\ndf6['AMT_INCOME_TOTAL']\n","2d31fe8a":"df3['NAME_INCOME_TYPE'] = df3['NAME_INCOME_TYPE'].replace(['Pensioner'],5)\ndf3['NAME_INCOME_TYPE'] = df3['NAME_INCOME_TYPE'].replace(['Commercial associate'],4)\ndf3['NAME_INCOME_TYPE'] = df3['NAME_INCOME_TYPE'].replace(['State servant'],3)\ndf3['NAME_INCOME_TYPE'] = df3['NAME_INCOME_TYPE'].replace(['Working'],2)\ndf3['NAME_INCOME_TYPE'] = df3['NAME_INCOME_TYPE'].replace(['Student'],1)\ndf3['NAME_INCOME_TYPE'] = df3['NAME_INCOME_TYPE'].apply(pd.to_numeric)\nprint(df3['NAME_INCOME_TYPE'].unique())","34c6c714":"# Fazendo o mesmo para finalidade de uso do cr\u00e9dito\ndf7 = df3.groupby(['NAME_HOUSING_TYPE']).mean().sort_values(['AMT_INCOME_TOTAL'], ascending=False)\ndf7['AMT_INCOME_TOTAL']","ae4fb318":"df3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].replace(['Office apartment'],6)\ndf3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].replace(['Co-op apartment'],5)\ndf3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].replace(['Rented apartment'],4)\ndf3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].replace(['House \/ apartment'],3)\ndf3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].replace(['Municipal apartment'],2)\ndf3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].replace(['With parents'],1)\ndf3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].apply(pd.to_numeric)\nprint(df3['NAME_HOUSING_TYPE'].unique())","d3dbb908":"#Como foi visto no gr\u00e1fico inicial, a vari\u00e1vel estado civil n\u00e3o varia muito de consumo de acordo como status,\n# portanto vamos remove-l\u00e1 junto as demais desnecess\u00e1rias\n\ndf3 = df3.drop(columns=['NAME_FAMILY_STATUS'])\ndf3 = df3.drop(columns=['ID'])\n#Vamos tira a vari\u00e1vel FLAG_MOBIL , CNT_CHILDREN e FLAG_WORK_PHONE pois tamb\u00e9m n\u00e3o traz informa\u00e7\u00e3o relevante\ndf3 = df3.drop(columns=['FLAG_MOBIL'])\ndf3 = df3.drop(columns=['FLAG_WORK_PHONE'])\ndf3 = df3.drop(columns=['CNT_CHILDREN'])\ndf3 = df3.drop(columns=['FLAG_PHONE'])\ndf3 = df3.drop(columns=['FLAG_EMAIL'])\ndf3.head(2)","14949186":"#letras s\u00e3o adimplentes e n\u00fameros inadimplentes\ndf3['STATUS'] = df3['STATUS'].replace(['C'],0)\ndf3['STATUS'] = df3['STATUS'].replace(['X'],0)\ndf3['STATUS'] = df3['STATUS'].apply(pd.to_numeric) \ndf3['STATUS'] = np.where(df3['STATUS']<1, 0, 1)\nprint(df3['STATUS'].unique())\n","cf4aaad7":"Inadimplente = df3.loc[df3['STATUS'] == 1].count()[0]\nAdimplente = df3.loc[df3['STATUS'] == 0].count()[0]\n\nlabels = ['days past due', 'paid off\/No loan']\ncolors = ['#d10000', '#6297e3']\nexplode = (.1,.1)\n\n\nplt.pie([Inadimplente, Adimplente], labels = labels, colors = colors, \n        autopct = '%.2f %%', pctdistance= 0.2, startangle=170, explode = explode)\nplt.show()","e10de526":"import imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\n","302483a5":"x,y = df3.loc[:,df3.columns != 'STATUS'], df3.loc[:,'STATUS']\n\n# Definindo a propor\u00e7\u00e3o de dados da classe onde h\u00e1 menos observa\u00e7\u00f5es\nsampling_strategy= 0.34\nrus = RandomUnderSampler(sampling_strategy=sampling_strategy)\nX_res, y_res = rus.fit_resample(x, y)\nautopct = \"%.2f\"\nax = y_res.value_counts().plot.pie(autopct=autopct)\n_ = ax.set_title(\"Under-sampling\")","87dd321a":"# Chamar a variavel STATUS de risco ajuda a entener melhor\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Correlation of Features', y=1.05, size=15)\nsns.heatmap(df3.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","38663b17":"\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import StackingClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier","6802e556":"# splitting the data\n\nx_train,x_test,y_train,y_test = train_test_split(X_res,y_res,test_size = 0.20,random_state = 1)\n\nx_train_0,x_train_1,y_train_0,y_train_1 = train_test_split(X_res,y_res,test_size = 0.60, random_state = 1)","b72a0c44":"#X_res, y_res","bb2b8230":"#criando uma lista com os modelos\n# Vendo qual tem a melhor acur\u00e1cia para usa-lo no stacking\n\nmodels = {}\nmodels['knn'] = KNeighborsClassifier()\nmodels['cart'] = DecisionTreeClassifier()\nmodels['svm'] = SVC()\nmodels['bayes'] = GaussianNB()\nmodels['rdm'] = RandomForestClassifier()\nmodels['lgc'] = LogisticRegression(max_iter=1000)\nmodels['ada'] = AdaBoostClassifier()\nmodels['gda'] = GradientBoostingClassifier()\nmodels['bca'] = BaggingClassifier()","e2d25dd7":"# Voting method\n# M\u00e9todo de vota\u00e7\u00e3o\nfor name, model in models.items():\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = accuracy_score(y_test,y_pred)\n    print(name, accuracy)\n    \n    #Suspeita de overfitting analisar pela curva roc","05831937":"# Rodando novamente\nmodels = {}\nmodels['knn'] = KNeighborsClassifier()\nmodels['svm'] = SVC()\nmodels['bayes'] = GaussianNB()\nmodels['lgc'] = LogisticRegression(max_iter=1000)\nmodels['ada'] = AdaBoostClassifier()\nmodels['gda'] = GradientBoostingClassifier()\n\n# Acur\u00e1cias\n\n# Voting method\n# M\u00e9todo de vota\u00e7\u00e3o\nfor name, model in models.items():\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = accuracy_score(y_test,y_pred)\n    print(name, accuracy)\n\n    \n    \n# Verificando o intervalo de confian\u00e7a da acur\u00e1cia\nfrom sklearn.model_selection import cross_val_score    \n\nknn_IC = cross_val_score(models['knn'], X_res,y_res, cv=5)\nsvm_IC = cross_val_score(models['svm'], X_res,y_res, cv=5)\nbayes_IC = cross_val_score(models['bayes'], X_res,y_res, cv=5)\nlgc_IC = cross_val_score(models['lgc'], X_res,y_res, cv=5)\ngda_IC = cross_val_score(models['gda'], X_res,y_res, cv=5)\nada_IC = cross_val_score(models['ada'], X_res,y_res, cv=5)\n\nscores = {}\n\nscores['knn'] =  knn_IC.mean() + knn_IC.std() * 2, knn_IC.mean() - knn_IC.std() * 2\nscores['svm'] =  svm_IC.mean() + svm_IC.std() * 2, svm_IC.mean() - svm_IC.std() * 2\nscores['bayes'] =   bayes_IC.mean() + bayes_IC.std() * 2,bayes_IC.mean() - bayes_IC.std() * 2\nscores['lgc'] =  lgc_IC.mean() + lgc_IC.std() * 2, lgc_IC.mean() - lgc_IC.std() * 2\nscores['gda'] =  gda_IC.mean() + gda_IC.std() * 2, gda_IC.mean() - gda_IC.std() * 2\nscores['ada'] =  ada_IC.mean() + ada_IC.std() * 2, ada_IC.mean() - ada_IC.std() * 2\n\n#Cofidence interval \nscores\n\n\n","73a0c570":"\n#Avaliando o desempenho dos modelos que tiverem a acur\u00e1cia dentro do intervalo\n# para evitar o paradoxo da Acur\u00e1cia\n\nfrom sklearn.metrics import classification_report\n\n\n\nmodels = {}\n\nmodels['knn'] = KNeighborsClassifier()\nmodels['svm'] = SVC()\nmodels['bayes'] = GaussianNB()\nmodels['lgc'] = LogisticRegression(max_iter=1000)\nmodels['ada'] = AdaBoostClassifier()\nmodels['gda'] = GradientBoostingClassifier()\n\n\nfor name, model in models.items():\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    classification = classification_report(y_test,y_pred)\n    print(name, classification)","674732c7":"    from sklearn.metrics import roc_auc_score\n\n    model_bayes = GaussianNB().fit(x_train, y_train)\n    model_ada = AdaBoostClassifier().fit(x_train, y_train)\n    model_knn = KNeighborsClassifier().fit(x_train, y_train)\n    model_gda = GradientBoostingClassifier().fit(x_train, y_train)\n    \n  \n    y_bayes = model_bayes.predict(x_test)\n    y_ada = model_ada.predict(x_test)\n    y_knn = model_knn.predict(x_test)\n    y_gda = model_gda.predict(x_test)\n    \n    \nauc = {}\nauc['bayes'] = roc_auc_score(y_test, y_bayes)\nauc['ada'] = roc_auc_score(y_test, y_ada)\nauc['knn'] = roc_auc_score(y_test, y_knn)\nauc['gda'] = roc_auc_score(y_test, y_gda)\n\nauc\n","771cdf27":"#Agora vamos interpretar nosso modelo com o Lime (ou Shap)","8ab3821e":"#### No nosso caso, o \u00fanico modelo que teve a m\u00ednima capacidade classificar os positivos, foi o Beysiano. Vamos obter a AUC para comparar.","41e2f27e":"## Em progesso..","bdd2f7a2":"As vari\u00e1veis tem poucas correla\u00e7\u00e3o entre si, o que pode ser um sinal bom, diminuindo as chances de inflar o modelo.","aeddeb5c":"Podemos ver que tr\u00eas modelos est\u00e3o gerando overfitting, portanto iremos revome-los","cba97bae":"##### Temos duas categorias de indiv\u00edduos: com atrasos de pagamentos, e sem atrasos. Portanto vamos categoriz\u00e1-los como **inadimplentes e adimplentes**. A decis\u00e3o de quem \u00e9 adimplente ou inadimplente \u00e9 relativo e depende dos interesses internos das institui\u00e7\u00f5es, mas para simplifica\u00e7\u00e3o do modelo fazeremos dessa forma.","a2ded6e3":"#### Agora vamos analisar a variavel target","0060d172":"# Qual a import\u00e2ncia de analisar o Recall dos modelos nos estudos de cr\u00e9dito?","d9c5a692":"* \u00c9 poss\u00edvel notar que a base de dados aumentou, portanto houve duplicatas de valores. Precisamos remove-l\u00e1s.","b7e82283":"## Aplicando a t\u00e9cnica de ensemble stacking","340825b7":"### Transformando em dummies","d057e00e":"### Como h\u00e1 v\u00e1rias categorias, dividiremos todas elas pelo poder de consumo","9859eb9e":"####   Quando uma empresa crediticia deseja fornecer cr\u00e9dito aos seus clientes, ela n\u00e3o s\u00f3 analisa as acur\u00e1cias dos modelos. Na verdade isso componhe a menor parte na an\u00e1lise de cr\u00e9dito. Dado as condi\u00e7\u00f5es internas da institui\u00e7\u00e3o, existe sempre um grau de risco nas aplica\u00e7\u00f5es de produtos financeiros, e de acordo com a situa\u00e7\u00e3o interna da empresa, ela determinar\u00e1 qual individuo receber\u00e1 seu cr\u00e9dito. Portanto, o ponto que mais afeta quem receber\u00e1 o cr\u00e9dito, \u00e9 saber qual \u00e9 a probabilidade de um cliente com determinadas caracter\u00edsticas vir a se tornar um poss\u00edvel inadimplente, e com isso saber qual \u00e9 a sua probabilidade de ter atrasos, ou n\u00e3o quita\u00e7\u00e3o da d\u00edvida, e assim determinar o ponto de corte de acordo com o grau de risco que a institui\u00e7\u00e3o escolheu. Por exemplo, o banco X n\u00e3o aumentar\u00e1 o limite de cart\u00e3o de cr\u00e9dito a clientes que possuem probabilidades maior ou igual a 30% de ser inadimplente. Isso equivale a determinar um ponto de corte de 0,3. Ou seja, nas decis\u00f5es de quem receber\u00e1 cr\u00e9dito ou n\u00e3o, n\u00e3o \u00e9 a acur\u00e1cia que nos traz o melhor desempenho do modelo, mas sim, o seu desempenho quanto a varia\u00e7\u00f5es nos pontos de cortes, obtido pela AUC da curva ROC que \u00e9 tra\u00e7ada a partir do Recall.","8cac158a":"### Juntando as duas bases de dados","01bf48bc":"### Verificando o numero de linhas restantes","28076515":"## Vamos tentar entender a capacidade de pagamento dos individuos e enquadr\u00e1-lo em categorias","9f0d9d90":"Podemos notar que nosso conjunto de dados est\u00e1 **muito desbalanceado** e a propor\u00e7\u00e3o de classes \u00e9 de 24853 Adimplentes para 281 Inadimplentes. E para isso vamos usar o m\u00e9todo de resampling para balancear a base de dados.","0570ae30":"### limpando os dados","e3a67f35":"#### Veja como a acur\u00e1cia pode enganar a escolha do modelo. Os \u00fanicos que teveram a capacidade de classificar os ind\u00edviduos como poss\u00edveis inadimplentes (Recall), foram o KNN, com 18%, beysiano com 4%, Ada com 11%, e o Gda com 18%. Ou seja, do total de inadimplentes existentes na base proposta, apenas 4 modelos coseguiram fazer essa classifica\u00e7\u00e3o. Todos os demais conseguiram prever apenas os n\u00e3o inadimplentes, que n\u00e3o \u00e9 o objetivo de an\u00e1lise desse trabalho.","d82e6494":"Voc\u00ea deve estar se perguntando se n\u00e3o seria interessante estimar uma regress\u00e3o linear antes de transformar a vari\u00e1vel target em dummy. A quest\u00e3o \u00e9 que as vari\u00e1veis explicativas precisam ter distribui\u00e7\u00e3o normal para obter os melhores estimadores de MQO, o que n\u00e3o acontece no nosso conjunto de dados. Portando levaremos a nossa an\u00e1lise a modelos n\u00e3o linear."}}