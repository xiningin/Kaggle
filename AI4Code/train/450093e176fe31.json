{"cell_type":{"1e336640":"code","9330e2e4":"code","de7fe632":"code","d830b7cc":"code","dcb4bdc3":"code","a3334b64":"code","34ebed40":"code","f9b86318":"code","3e22dd3b":"code","e916c437":"code","afdb4b28":"code","3206510a":"code","58ff0ad7":"code","aa814c78":"code","b28463b0":"code","738551d7":"code","4d6e29c9":"code","edcb3dd2":"code","c41e7efe":"code","8b674867":"code","2563c9cc":"code","17b8fe6e":"code","b8ba48a5":"code","cb6ad275":"code","ee43b07e":"code","12b6a95e":"code","1f1136c1":"code","3aa4bfed":"code","a7926d23":"markdown","3ab477dc":"markdown","5964ec39":"markdown","2d6f1878":"markdown","f372ab0a":"markdown","a3953f92":"markdown","d2f26570":"markdown"},"source":{"1e336640":"# importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9330e2e4":"# load dataset\ndf = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","de7fe632":"df.head()","d830b7cc":"df.info()","dcb4bdc3":"df.isna().sum()","a3334b64":"df = df.dropna(axis=1)","34ebed40":"df.info()","f9b86318":"# count of malignant and benignate\ndf['diagnosis'].value_counts()","3e22dd3b":"sns.countplot(df['diagnosis'], label = 'count')","e916c437":"df.dtypes","afdb4b28":"# encoding Categorical data\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf.iloc[:,1] = le.fit_transform(df.iloc[:,1].values)","3206510a":"df","58ff0ad7":"data_mean=df.iloc[:,1:11]","aa814c78":"#Plot histograms of CUT1 variables\nhist_mean=data_mean.hist(bins=10, figsize=(15, 10),grid=False,)","b28463b0":"#Heatmap\nplt.figure(figsize=(20,20))\nsns.heatmap(df.corr(),annot=True, fmt = '.0%')","738551d7":"#Density Plots\nplt = data_mean.plot(kind= 'density', subplots=True, layout=(4,3), sharex=False, \n                     sharey=False,fontsize=12, figsize=(15,10))","4d6e29c9":"# train test split\nfrom sklearn.model_selection import train_test_split","edcb3dd2":"x = df.drop(['diagnosis'], axis=1)\ny = df['diagnosis'].values","c41e7efe":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=0)","8b674867":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","2563c9cc":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state= 0 )\nclassifier.fit(x_train,y_train)\n","17b8fe6e":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)\nprint(\"Logistic Regression accuracy : {:.2f}%\".format(reg.score(x_test,y_test)*100))","b8ba48a5":"# Support Vactor \nfrom sklearn.svm import SVC\nsvm = SVC(random_state=10)\nsvm1 = SVC(kernel='linear',gamma='scale',random_state=10)\nsvm2 = SVC(kernel='rbf',gamma='scale',random_state=10)\nsvm3 = SVC(kernel='poly',gamma='scale',random_state=10)\nsvm4 = SVC(kernel='sigmoid',gamma='scale',random_state=10)\n\nsvm.fit(x_train,y_train)\nsvm1.fit(x_train,y_train)\nsvm2.fit(x_train,y_train)\nsvm3.fit(x_train,y_train)\nsvm4.fit(x_train,y_train)\n\nprint('SVC Accuracy : {:,.2f}%'.format(svm.score(x_test,y_test)*100))\n\nprint('SVC Liner Accuracy : {:,.2f}%'.format(svm1.score(x_test,y_test)*100))\n\nprint('SVC RBF Accuracy : {:,.2f}%'.format(svm2.score(x_test,y_test)*100))\n\nprint('SVC Ploy Accuracy : {:,.2f}%'.format(svm3.score(x_test,y_test)*100))\n\nprint('SVC Sigmoid Accuracy : {:,.2f}%'.format(svm4.score(x_test,y_test)*100))\n\n\n","cb6ad275":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\" Naive Bayes accuracy : {:.2f}%\".format(nb.score(x_test,y_test)*100))","ee43b07e":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=1000,random_state=1)\nrf.fit(x_train,y_train)\nprint(\"Random Forest Classifier accuracy : {:.2f}%\".format(rf.score(x_test,y_test)*100))","12b6a95e":"import xgboost\nxg = xgboost.XGBClassifier()\nxg.fit(x_train,y_train)\nprint(\"XGboost accuracy : {:.2f}%\".format(xg.score(x_test,y_test)*100))","1f1136c1":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=100)\nknn.fit(x_train,y_train)\nprint('KNN Accuracy {:.2f}%'.format(knn.score(x_test,y_test)*100))","3aa4bfed":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion='entropy',max_depth=4, random_state=10)\ndt.fit(x_train,y_train)\nprint(\"Decision Tree Accuracy : {:,.2f}%\".format(dt.score(x_test,y_test)*100))","a7926d23":"## Breast Cancer Detection","3ab477dc":"# Spliting the Data","5964ec39":"### Separate columns into smaller dataframes to perform visualization","2d6f1878":"*## Feature Scaling*","f372ab0a":"#  Data Preprocessing","a3953f92":"# Using Machine Learning To Predict Diagnosis of a Breast Cancer\n  \n\n## 1. Identify the problem\nBreast cancer is the most common malignancy among women, accounting for nearly 1 in 3 cancers diagnosed among women in the United States, and it is the second leading cause of cancer death among women. Breast Cancer occurs as a results of abnormal growth of cells in the breast tissue, commonly referred to as a Tumor. A\u00a0tumor\u00a0does not mean cancer -\u00a0tumors\u00a0can be benign (not cancerous), pre-malignant (pre-cancerous), or malignant (cancerous). Tests such as MRI, mammogram, ultrasound and biopsy are commonly used to diagnose breast cancer performed.\n\n### 1.1 Expected outcome\nGiven breast cancer results from breast fine needle aspiration (FNA) test (is a quick and simple procedure to perform, which removes some fluid or cells from a breast lesion or cyst (a lump, sore or swelling) with a fine needle similar to a blood sample needle). Since this build a model that can classify a breast cancer tumor using two training classification:\n* 1= Malignant (Cancerous) - Present\n* 0= Benign (Not Cancerous) -Absent\n\n### 1.2 Objective \nSince the labels in the data are discrete, the predication falls into two categories, (i.e. Malignant or benign). In machine learning this is a classification problem. \n        \n> *Thus, the goal is to classify whether the breast cancer is benign or malignant and predict the recurrence and non-recurrence of malignant cases after a certain period.  To achieve this we have used machine learning classification methods to fit a function that can predict the discrete class of new input.*\n\n### 1.3 Identify data sources\nThe [Breast Cancer](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29) datasets is available machine learning repository\u00a0maintained by the University of California, Irvine. The dataset contains **569 samples of malignant and benign tumor cells**. \n* The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M=malignant, B=benign), respectively. \n* The columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant. \n\n ","d2f26570":"# Model Building"}}