{"cell_type":{"7435ef0c":"code","38506f81":"code","9455a5f7":"code","4601bfca":"code","18f47dc6":"code","3345ac98":"code","859ea3df":"code","ce9d7332":"code","baa02140":"code","26a57d3b":"code","9ff4d986":"code","107f48f4":"code","2f16ae0e":"code","8b3187ba":"code","5e2e2942":"code","7ccd731c":"code","a1fd15b2":"code","97ce2bbe":"code","d62a581e":"code","b7c44421":"code","c24053d2":"code","4cb9f893":"markdown","639cd587":"markdown","9c57c686":"markdown","c2b035bf":"markdown","288d9ebe":"markdown","773c51c0":"markdown"},"source":{"7435ef0c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38506f81":"import optuna\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n \ntrain = pd.read_csv('..\/input\/digitrecognizer\/train.csv') \ntest = pd.read_csv('..\/input\/digitrecognizer\/test.csv')\nprint('The size of the train data:' + str(train.shape))\nprint('The size of the test data:' + str(test.shape))","9455a5f7":"# transform float type\nX_train = (train.iloc[:,1:].values).astype('float32') \ny_train = train.iloc[:,0].values\nX_test = test.values.astype('float32')","4601bfca":"#train_test_split again\nfrom sklearn.model_selection import train_test_split\n \nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_train,\n                                                        y_train, \n                                                        test_size = 0.2,\n                                                        train_size = 0.8,\n                                                        stratify = y_train)\nprint(f'X_train2 : {len(X_train2)}')\nprint(f'X_test2 : {len(X_test2)}')","18f47dc6":"#reshape data \nimg_rows, img_cols = 28, 28\nnum_classes = 10\n\n\nX_train = X_train2.reshape(X_train2.shape[0], img_rows, img_cols, 1)\nX_test = X_test2.reshape(X_test2.shape[0], img_rows, img_cols, 1)\n\n\ny_train= keras.utils.to_categorical(y_train2, num_classes)","3345ac98":"X_train.shape","859ea3df":"import optuna\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D","ce9d7332":"img_rows, img_cols = 28, 28\nnum_classes = 10\n\n\n#define the CNN model\ndef create_model(num_layer, mid_units, num_filters,dropout_rate):\n    \n    model = Sequential()\n    model.add(Conv2D(filters=num_filters[0], kernel_size=(3, 3),\n                 activation=\"relu\",\n                 input_shape=(img_rows, img_cols, 1)))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    for i in range(1,num_layer):\n        model.add(Conv2D(filters=num_filters[i], kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    \n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(dropout_rate[0]))\n    model.add(Flatten())\n    model.add(Dense(mid_units))\n    model.add(Dropout(dropout_rate[1]))\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    return model","baa02140":"def objective(trial):\n    print(\"Optimize Start\")\n    \n    \n    #clear_session\n    keras.backend.clear_session()\n    \n    \n    \n    #number of the convolution layer\n    num_layer = trial.suggest_int(\"num_layer\", 2, 5)\n    \n    \n    #number of the unit\n    mid_units = int(trial.suggest_discrete_uniform(\"mid_units\", 100, 300, 100))\n    \n    \n    #number of the each convolution layer filter\n    num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\"+str(i), 16, 128, 16)) for i in range(num_layer)]\n    \n    \n    #activation = trial.suggest_categorical(\"activation\", [\"relu\", \"sigmoid\"])\n    \n    #Dropout\n    #dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)\n    #dropout_rate = [int(trial.suggest_uniform(\"dropout_rate\"+str(ii), 0.0, 0.5)) for ii in range(2)]\n    dropout_rate = [0] * 2\n    dropout_rate[0] = trial.suggest_uniform('dropout_rate'+str(0), 0.0, 0.5)\n    dropout_rate[1] = trial.suggest_uniform('dropout_rate'+str(1), 0.0, 0.5)\n    \n    #optimizer\n    optimizer = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"adam\"])\n    \n    model = create_model(num_layer, mid_units, num_filters,dropout_rate)\n    model.compile(optimizer=optimizer,\n          loss=\"categorical_crossentropy\",\n          metrics=[\"acc\"])\n          #metrics=[\"accuracy\"])\n    \n    history = model.fit(X_train, y_train, verbose=0, epochs=20, batch_size=128, validation_split=0.1)\n    \n    scores = model.evaluate(X_train, y_train)\n    print('accuracy={}'.format(*scores))\n    \n    \n    #return 1 - history.history[\"val_acc\"][-1]\n    return 1 - history.history[\"val_acc\"][-1]","26a57d3b":"study = optuna.create_study()\nstudy.optimize(objective, n_trials=5)","9ff4d986":"plt.plot([t.value for t in study.trials])","107f48f4":"optuna.visualization.plot_contour(study)","2f16ae0e":"optuna.visualization.plot_param_importances(study)","8b3187ba":"optuna.visualization.plot_contour(study, params=[\"dropout_rate1\",\"num_filter_0\",\"num_layer\",\"dropout_rate0\",\"num_filter_1\"])","5e2e2942":"study.best_params","7ccd731c":"optuna.visualization.plot_optimization_history(study)","a1fd15b2":"optuna.visualization.plot_optimization_history(study)","97ce2bbe":"optuna.visualization.plot_intermediate_values(study)","d62a581e":"optuna.visualization.plot_parallel_coordinate(study)","b7c44421":"optuna.visualization.plot_contour(study, params=[\"num_filter_0\", \"num_filter_1\",\"num_filter_2\",\"num_filter_3\",\"num_filter_4\"])","c24053d2":"optuna.visualization.plot_slice(study)","4cb9f893":"#Codes by cpptake  https:\/\/www.kaggle.com\/takeshikobayashi\/mnist-cnn-keras-with-optuna-visualization\/data","639cd587":"#I didn't optimized anything. I just copied Cpptake Optuna script to plot the Magnificent Charts. ","9c57c686":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcT_JF_C5cEwbeKMkHIaIOzyeQ5L5r09ZruFpQ&usqp=CAU)developer.nvidia.com","c2b035bf":"#Number of trials was 50, since the Notebook stopped I reduced to 5. ","288d9ebe":"#Here starts the Optimized Model. Since I just want the charts. I'm done. Epochs in Mpwolke Out.","773c51c0":"#Optuna - Optimize Your Optimization\n\nAn open source hyperparameter optimization framework to automate hyperparameter search\nhttps:\/\/optuna.org\/"}}