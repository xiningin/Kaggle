{"cell_type":{"903d1869":"code","47be241f":"code","4e1bb744":"code","9db2d971":"code","912b2914":"code","55fe69d5":"code","44e5683c":"code","ca30e172":"code","48bd3117":"code","aafcd244":"code","d9449589":"code","2c07c6a4":"code","ecc45671":"code","6c281edf":"code","aa1c2338":"code","5a829c2b":"code","077c1766":"code","44e8f9af":"code","a86d3129":"code","d51001c2":"code","3969ff64":"code","fb879fbb":"code","e63889e4":"code","a0bef39b":"code","e4ef54c0":"code","4e056d9a":"code","98f4b9fc":"markdown","e8365dc2":"markdown","0bcb1466":"markdown","34db8b8f":"markdown","e10d27ba":"markdown","e0caee98":"markdown","e60376d4":"markdown","4ff11f42":"markdown","aec0d3f4":"markdown","6d6dce5b":"markdown","0c6dc6a4":"markdown","d4f1cb5f":"markdown","8396e999":"markdown","2e9ef5b5":"markdown","c95e61c6":"markdown","4690f3d0":"markdown","d5ddee69":"markdown","c5b8ad7e":"markdown","f3e03690":"markdown","06c34462":"markdown","b9b319e2":"markdown","9ddeccab":"markdown","33fdbed9":"markdown","5992e872":"markdown","5cb69fdb":"markdown","0b69d7d9":"markdown","ce0011f9":"markdown","59d1d471":"markdown","24263a29":"markdown","87775400":"markdown","1cc41ee0":"markdown","ceec21ac":"markdown"},"source":{"903d1869":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport pandas as pd\nimport lightgbm\nfrom bayes_opt import BayesianOptimization\nfrom catboost import CatBoostClassifier, cv, Pool","47be241f":"train_df = pd.read_csv('..\/input\/flight_delays_train.csv')\ntest_df = pd.read_csv('..\/input\/flight_delays_test.csv')\ntrain_df = train_df[train_df.DepTime <= 2400].copy()\ny_train = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values","4e1bb744":"def label_enc(df_column):\n    df_column = LabelEncoder().fit_transform(df_column)\n    return df_column\n\ndef make_harmonic_features_sin(value, period=2400):\n    value *= 2 * np.pi \/ period \n    return np.sin(value)\n\ndef make_harmonic_features_cos(value, period=2400):\n    value *= 2 * np.pi \/ period \n    return np.cos(value)\n\ndef feature_eng(df):\n    df['flight'] = df['Origin']+df['Dest']\n    df['Month'] = df.Month.map(lambda x: x.split('-')[-1]).astype('int32')\n    df['DayofMonth'] = df.DayofMonth.map(lambda x: x.split('-')[-1]).astype('uint8')\n    df['begin_of_month'] = (df['DayofMonth'] < 10).astype('uint8')\n    df['midddle_of_month'] = ((df['DayofMonth'] >= 10)&(df['DayofMonth'] < 20)).astype('uint8')\n    df['end_of_month'] = (df['DayofMonth'] >= 20).astype('uint8')\n    df['DayOfWeek'] = df.DayOfWeek.map(lambda x: x.split('-')[-1]).astype('uint8')\n    df['hour'] = df.DepTime.map(lambda x: x\/100).astype('int32')\n    df['morning'] = df['hour'].map(lambda x: 1 if (x <= 11)& (x >= 7) else 0).astype('uint8')\n    df['day'] = df['hour'].map(lambda x: 1 if (x >= 12) & (x <= 18) else 0).astype('uint8')\n    df['evening'] = df['hour'].map(lambda x: 1 if (x >= 19) & (x <= 23) else 0).astype('uint8')\n    df['night'] = df['hour'].map(lambda x: 1 if (x >= 0) & (x <= 6) else 0).astype('int32')\n    df['winter'] = df['Month'].map(lambda x: x in [12, 1, 2]).astype('int32')\n    df['spring'] = df['Month'].map(lambda x: x in [3, 4, 5]).astype('int32')\n    df['summer'] = df['Month'].map(lambda x: x in [6, 7, 8]).astype('int32')\n    df['autumn'] = df['Month'].map(lambda x: x in [9, 10, 11]).astype('int32')\n    df['holiday'] = (df['DayOfWeek'] >= 5).astype(int) \n    df['weekday'] = (df['DayOfWeek'] < 5).astype(int)\n    df['airport_dest_per_month'] = df.groupby(['Dest', 'Month'])['Dest'].transform('count')\n    df['airport_origin_per_month'] = df.groupby(['Origin', 'Month'])['Origin'].transform('count')\n    df['airport_dest_count'] = df.groupby(['Dest'])['Dest'].transform('count')\n    df['airport_origin_count'] = df.groupby(['Origin'])['Origin'].transform('count')\n    df['carrier_count'] = df.groupby(['UniqueCarrier'])['Dest'].transform('count')\n    df['carrier_count_per month'] = df.groupby(['UniqueCarrier', 'Month'])['Dest'].transform('count')\n    df['deptime_cos'] = df['DepTime'].map(make_harmonic_features_cos)\n    df['deptime_sin'] = df['DepTime'].map(make_harmonic_features_sin)\n    df['flightUC'] = df['flight']+df['UniqueCarrier']\n    df['DestUC'] = df['Dest']+df['UniqueCarrier']\n    df['OriginUC'] = df['Origin']+df['UniqueCarrier']\n    return df.drop('DepTime', axis=1)","9db2d971":"full_df = pd.concat([train_df.drop('dep_delayed_15min', axis=1), test_df])\nfull_df = feature_eng(full_df)\n\nfor column in ['UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']:\n    full_df[column] = label_enc(full_df[column])\n\nX_train = full_df[:train_df.shape[0]]\nX_test = full_df[train_df.shape[0]:]","912b2914":"X_train.head()","55fe69d5":"def simple_functon(a, b):\n    return a + b","44e5683c":"optimizer = BayesianOptimization(\n    simple_functon,\n    {'a': (1, 3),\n    'b': (4, 7)})","ca30e172":"optimizer.maximize(3, 2)","48bd3117":"optimizer.max['params']","aafcd244":"optimizer.max['target']","d9449589":"categorical_features = ['Month',  'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']","2c07c6a4":"def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf):\n    params = {\n        \"objective\" : \"binary\",\n        \"metric\" : \"auc\", \n        'is_unbalance': True,\n        \"num_leaves\" : int(num_leaves),\n        \"max_depth\" : int(max_depth),\n        \"lambda_l2\" : lambda_l2,\n        \"lambda_l1\" : lambda_l1,\n        \"num_threads\" : 20,\n        \"min_child_samples\" : int(min_child_samples),\n        'min_data_in_leaf': int(min_data_in_leaf),\n        \"learning_rate\" : 0.03,\n        \"subsample_freq\" : 5,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1\n    }\n    lgtrain = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n    cv_result = lightgbm.cv(params,\n                       lgtrain,\n                       1000,\n                       early_stopping_rounds=100,\n                       stratified=True,\n                       nfold=3)\n    return cv_result['auc-mean'][-1]","ecc45671":"lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n                                                'max_depth': (5, 63),\n                                                'lambda_l2': (0.0, 0.05),\n                                                'lambda_l1': (0.0, 0.05),\n                                                'min_child_samples': (50, 10000),\n                                                'min_data_in_leaf': (100, 2000)\n                                                })\n\nlgbBO.maximize(n_iter=10, init_points=2)","6c281edf":"lgbBO.max","aa1c2338":"lgbBO.res[0]","5a829c2b":"from bayes_opt.logger import JSONLogger\nfrom bayes_opt.event import Events\n\nlogger = JSONLogger(path=\".\/logs.json\")\nlgbBO.subscribe(Events.OPTMIZATION_STEP, logger)","077c1766":"lgbBO.maximize(n_iter=10, init_points=3)","44e8f9af":"new_opt = BayesianOptimization(lgb_eval, {'num_leaves': (25, 100),\n                                                'max_depth': (5, 63),\n                                                'lambda_l2': (0.0, 0.05),\n                                                'lambda_l1': (0.0, 0.05),\n                                                'min_child_samples': (50, 100),\n                                                'min_data_in_leaf': (50, 200)\n                                                })","a86d3129":"from bayes_opt.util import load_logs\n\nload_logs(new_opt, logs=[\".\/logs.json\"]);","d51001c2":"new_opt.maximize(n_iter=5, init_points=1)","3969ff64":"new_opt.max","fb879fbb":"new_opt.probe({'num_leaves': 10,\n                'max_depth': 100,\n                'lambda_l2': 1,\n                'lambda_l1': 1,\n                'min_child_samples': 300,\n                'min_data_in_leaf': 1000 })\n\nnew_opt.probe({'num_leaves': 55,\n                'max_depth': 400,\n                'lambda_l2': 5,\n                'lambda_l1': 5,\n                'min_child_samples': 100,\n                'min_data_in_leaf': 10 })","e63889e4":"new_opt.maximize(n_iter=0, init_points=0)","a0bef39b":"def cat_eval(num_leaves,max_depth,bagging_temperature, l2_leaf_reg):\n    params = {'bagging_temperature': bagging_temperature,\n              'num_leaves': int(num_leaves),\n              'max_depth': int(max_depth),\n              'l2_leaf_reg': l2_leaf_reg,\n              'iterations': 500,\n              'learning_rate':0.1,\n              'early_stopping_rounds':100,\n              'eval_metric': \"AUC\",\n              'verbose': False}\n    cv_dataset = Pool(data=X_train,\n                  label=y_train,\n                  cat_features=categorical_features)\n    scores = cv(cv_dataset,\n            params,\n            fold_count=3)\n    return scores['test-AUC-mean'].max()","e4ef54c0":"cat_opt = BayesianOptimization(cat_eval, {'num_leaves': (25, 100),\n                                          'max_depth': (5, 15),\n                                          'bagging_temperature': (0.1, 0.9),\n                                          'l2_leaf_reg': (2,5)\n                                        })","4e056d9a":"cat_opt.maximize(n_iter=5, init_points=2)","98f4b9fc":"## Loading progress","e8365dc2":"## Test it on data","0bcb1466":"We can choose the certain point ant try the result. ","34db8b8f":"Now we have data to tune parameters for different models.","e10d27ba":"## Simple example","e0caee98":"Ideal! We can see the best params:","e60376d4":"Let's try another model for test","4ff11f42":"At first you should create an optimizer. It uses two things:\n* function to optimize\n* bounds of parameters\n\nFor us function is the procedure, which counts metrics of our model quality.","aec0d3f4":"My kernel about using it on real data and real peremeters with LightGBM: \nhttps:\/\/www.kaggle.com\/clair14\/gold-is-the-reason-teams-and-bayes-for-lightgbm\n\nThere I will use random values of parameters to test.","6d6dce5b":"Bayesian optimization works by constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not, as seen in the picture below.","0c6dc6a4":"## Bayesian methods of hyperparameter optimization","d4f1cb5f":"<img src='https:\/\/sites.google.com\/site\/bayesforvietnam\/_\/rsrc\/1465811460099\/home\/Bayes%201.jpg'\/>","8396e999":"## How does it work","2e9ef5b5":"### CatBoost","c95e61c6":"## Try certain points","4690f3d0":"This library shows perfect results, and it is much effective then rendom search or CV gread, as you don't need to try every point.","d5ddee69":"It is wonderful! Really! You can learn you optimizer, collect some points, then you can correct something (bounds, for example, if you understand, that some values are not interesting for you. There is no point to start from beginning, you can just use previous result)\nMay be we can change data just a little bit, and continue to search for best parameters.","c5b8ad7e":"### LigthGBM","f3e03690":"## \u0421onclusion","06c34462":"This kernel is about using library BayesianOptimization, that can do parameters tuning for us much easier. This library has very good documentation, so I will use information from this and you can find there much more information.\n\nDocumentation:\nhttps:\/\/github.com\/fmfn\/BayesianOptimization","b9b319e2":"As you can see this is point from the previous run","9ddeccab":"Main parameters of this function:\n\n* **n_iter**: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n* **init_points**: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.","33fdbed9":"And all the process in each step...","5992e872":"Now we can read it for another optimizer","5cb69fdb":"**!** The important thing is that our optimization will maximize the value on function. So if your metric should be smaller the better, don't forget to use negative metric value.","0b69d7d9":"... and the best result","ce0011f9":"<img src=\"https:\/\/github.com\/fmfn\/BayesianOptimization\/blob\/master\/examples\/bo_example.png?raw=true\" \/>\nAs you iterate over and over, the algorithm balances its needs of exploration and exploitation taking into account what it knows about the target function. At each step a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with a exploration strategy (such as UCB (Upper Confidence Bound), or EI (Expected Improvement)), are used to determine the next point that should be explored (see the gif below).\n<img src=\"https:\/\/github.com\/fmfn\/BayesianOptimization\/raw\/master\/examples\/bayesian_optimization.gif\" \/>","59d1d471":"Thanks for your attention!","24263a29":"At first this is simple data preparation to show, how to work with library. ","87775400":"Now you can see the result","1cc41ee0":"This is function, that we want to maximize - function, that counts cross-validation metrics of lightGBM for our params.\n\nSome params such as num_leaves, max_depth, min_child_samples, min_data_in_leaf should be integers.","ceec21ac":"**!** The important thing is that our optimization will maximize the value on function. So if your metric should be smaller the better, don't forget to use negative metric value. Optimizer use float values of params, you should use int() in function, if this parameter must be integer."}}