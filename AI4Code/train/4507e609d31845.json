{"cell_type":{"cfc04546":"code","f19d82f2":"code","8d5ce1fa":"code","8ef62903":"code","dc2207d4":"code","15023b43":"code","85d54acb":"code","333a8ba3":"code","4ea8f9d2":"code","6612fe35":"code","47052506":"code","2fce5b52":"code","047789dc":"code","bb61781d":"code","0adc54f8":"code","761637d9":"code","e5539278":"code","78a63960":"code","4c2d33d5":"code","b253c4d5":"code","7a78236c":"code","5ff64500":"code","7bca1507":"code","483dac48":"code","c7bf301b":"code","a57cbc81":"code","56891d97":"code","7932776e":"code","d23ac5e5":"code","b1b3365f":"code","5765289d":"markdown","a835cba2":"markdown","ba085e32":"markdown","1c8e8625":"markdown","85edf79a":"markdown","d3eeba8d":"markdown","ec9f9173":"markdown","a7486276":"markdown","8a9dd655":"markdown","b109bc7d":"markdown","73ca9a8f":"markdown","85d0cd90":"markdown","41efd9fc":"markdown","162e4a19":"markdown","767a4bab":"markdown","8b53dfce":"markdown","6519084b":"markdown","bac33c53":"markdown","b1225e19":"markdown","bbc4bcf0":"markdown","296da9c4":"markdown","f980a4ed":"markdown","35f1e28b":"markdown","725117da":"markdown","7998600b":"markdown","fe8e9797":"markdown","c7a9b311":"markdown","529efedc":"markdown","be842c43":"markdown","9c9d124c":"markdown","8c7d5636":"markdown","c91b61e4":"markdown"},"source":{"cfc04546":"!pip uninstall --y typing","f19d82f2":"!pip install nvtx dask_cuda","8d5ce1fa":"!pip install git+https:\/\/github.com\/NVIDIA\/NVTabular.git@4c92dffac4354d816178264bcfcdec722db2ec1c","8ef62903":"%time\n!mkdir \/raid\n!cp -r \/kaggle\/input\/criteo-dataset-parquet\/criteo-parquet-subset-preprocessed \/raid\/","dc2207d4":"%time\n\nimport os\nimport json\n\n# define some information about where to get our data\nOUTPUT_DATA_DIR = os.environ.get('OUTPUT_DATA_DIR', '\/raid\/criteo-parquet-subset-preprocessed') # where we'll save our procesed data to\nBATCH_SIZE = int(os.environ.get('BATCH_SIZE', 1024*64))\nPARTS_PER_CHUNK = int(os.environ.get('PARTS_PER_CHUNK', 2))\n\n# define our dataset schema\nCONTINUOUS_COLUMNS = ['I' + str(x) for x in range(1,14)]\nCATEGORICAL_COLUMNS =  ['C' + str(x) for x in range(1,27)]\nLABEL_COLUMNS = ['label']\nCOLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS + LABEL_COLUMNS\n\noutput_train_dir = os.path.join(OUTPUT_DATA_DIR, 'train\/')\noutput_valid_dir = os.path.join(OUTPUT_DATA_DIR, 'valid\/')","15023b43":"import gc\nimport glob\n\nimport numpy as np\nimport pandas as pd","85d54acb":"train_paths = glob.glob(os.path.join(output_train_dir, \"*.parquet\"))\nvalid_paths = glob.glob(os.path.join(output_valid_dir, \"*.parquet\"))","333a8ba3":"df_train = pd.concat([pd.read_parquet(x) for x in train_paths])\ndf_valid = pd.concat([pd.read_parquet(x) for x in valid_paths])\nidx_train = df_train.shape[0]\ndf = pd.concat([df_train, df_valid])\ndel df_train, df_valid\ngc.collect()","4ea8f9d2":"df['label'] = df['label'].astype(np.int64).values","6612fe35":"import torch\n\nfrom fastai.tabular import *\n\nfrom fastai.basics import Learner\nfrom fastai.tabular.model import TabularModel\nfrom fastai.tabular.data import TabularDataLoaders\nfrom fastai.metrics import accuracy\nfrom fastai.callback.progress import ProgressCallback","47052506":"dl = TabularDataLoaders.from_df(df, \n                                cat_names=CATEGORICAL_COLUMNS, \n                                cont_names=CONTINUOUS_COLUMNS,\n                                y_names=LABEL_COLUMNS[0],\n                                valid_idx=list(range(idx_train,df.shape[0])),\n                                bs=BATCH_SIZE,\n                                val_bs=BATCH_SIZE\n                               )","2fce5b52":"del df\ngc.collect()","047789dc":"stats = json.load(open(OUTPUT_DATA_DIR + \"\/stats.json\", \"r\"))\nembeddings_raw = stats['EMBEDDING_TABLE_SHAPES']\nembeddings_raw","bb61781d":"embeddings = [embeddings_raw[key] for key in CATEGORICAL_COLUMNS]","0adc54f8":"class MyCrossEntropy(torch.nn.CrossEntropyLoss):\n  \n  def forward(self, input, target):\n    target = torch.squeeze(target.long())  # ADDED\n    return torch.nn.functional.cross_entropy(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)","761637d9":"model = TabularModel(emb_szs=embeddings, n_cont=len(CONTINUOUS_COLUMNS), out_sz=2, layers=[512, 256]).cuda()\nlearn =  Learner(dl, model, loss_func = MyCrossEntropy(), metrics=[accuracy], cbs=ProgressCallback())","e5539278":"model","78a63960":"from fastai.callback.schedule import fit_one_cycle","4c2d33d5":"from time import time\n\nlearning_rate = 1.32e-2\nepochs = 5\nstart = time()\n#learn.fit(epochs, learning_rate)\nfit_one_cycle(learn, n_epoch=epochs, lr_max=learning_rate)\nt_final = time() - start\nprint(t_final)","b253c4d5":"del model, learn, dl, embeddings\ngc.collect()","7a78236c":"import nvtabular as nvt\n\ntrain_data = nvt.Dataset(train_paths, engine=\"parquet\")\nvalid_data = nvt.Dataset(valid_paths, engine=\"parquet\")","5ff64500":"from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n\ntrain_data_itrs = TorchAsyncItr(\n    train_data,\n    batch_size=BATCH_SIZE,\n    cats=CATEGORICAL_COLUMNS,\n    conts=CONTINUOUS_COLUMNS,\n    labels=LABEL_COLUMNS,\n    parts_per_chunk=PARTS_PER_CHUNK\n)\nvalid_data_itrs = TorchAsyncItr(\n    valid_data,\n    batch_size=BATCH_SIZE,\n    cats=CATEGORICAL_COLUMNS,\n    conts=CONTINUOUS_COLUMNS,\n    labels=LABEL_COLUMNS,\n    parts_per_chunk=PARTS_PER_CHUNK\n)","7bca1507":"def gen_col(batch):\n    return (batch[0], batch[1], batch[2].long())","483dac48":"train_dataloader = DLDataLoader(train_data_itrs, collate_fn=gen_col, batch_size=None, pin_memory=False, num_workers=0)\nvalid_dataloader = DLDataLoader(valid_data_itrs, collate_fn=gen_col, batch_size=None, pin_memory=False, num_workers=0)\ndatabunch = TabularDataLoaders(train_dataloader, valid_dataloader)","c7bf301b":"embeddings = list(embeddings_raw.values())","a57cbc81":"model = TabularModel(emb_szs=embeddings, n_cont=len(CONTINUOUS_COLUMNS), out_sz=2, layers=[512, 256]).cuda()\nlearn =  Learner(databunch, model, loss_func = torch.nn.CrossEntropyLoss(), metrics=[accuracy], cbs=ProgressCallback())","56891d97":"model","7932776e":"from fastai.callback.schedule import fit_one_cycle","d23ac5e5":"learning_rate = 1.32e-2\nepochs = 5\nstart = time()\n#learn.fit(epochs, learning_rate)\nfit_one_cycle(learn, n_epoch=epochs, lr_max=learning_rate)\nt2_final = time() - start\nprint(t2_final)","b1b3365f":"print('Time FastAI Data Loader:    ' + str(t_final) + ' in seconds')\nprint('Time NVTabular Data Loader:  ' + str(t2_final) + ' in seconds')\nprint('Speed-up:                     ' + str(t_final\/t2_final))","5765289d":"The neural network architecture looks exactly the same.","a835cba2":"We define an own CrossEntropyFunction to modify the target tensors. We convert them into long and squeeze them. ","ba085e32":"# Training Neural Network with FastAI","1c8e8625":"We define NVTabular dataset objects.","85edf79a":"We load the saved embedding tables statistics generated in the [preprocessed Criteo dataset](https:\/\/www.kaggle.com\/benediktschifferer\/criteo-dataset-parquet) notebook. We use the [rule of thumbs for embedding output size from FastAI](https:\/\/forums.fast.ai\/t\/embedding-layer-size-rule\/50691) with the minimum embedding size of 16.\n\n$$min(16, round(1.6 * n_{cat} ^{0.56}))$$\n\nwith $n_{cat}$ is the cardinality of the categorical features.","d3eeba8d":"First, we train a neural network with the FastAI library and the FastAI dataloader. The FastAI data loader requires to load the dataset into memory.","ec9f9173":"Instead of defining a customized CrossEntropy function, we can provide a custom function to modify our batches.","a7486276":"First, we copy the data from `the Kaggle dataset` to the local machine to avoid network traffic during execution time.","8a9dd655":"We see a ~2x speeed up in our run. In our experiments, we can see even higher speed-ups with more data and larger batch-size. In addition, the NVTabular data loader can scale to dataset larger than host memory.","b109bc7d":"# Training Neural Network with FastAI using NVTabular data loader","73ca9a8f":"We convert the output label to long.","85d0cd90":"We define our embedding tables statistics. As mentioned above, we have a different order of categorical features in the NVTabular data loader.","41efd9fc":"Install the library nvtx and dask_cudf.","162e4a19":"Let's train our model for 5 epochs.","767a4bab":"We convert our NVTabular data loader into TabularDataLoader structure.","8b53dfce":"We initalize the FastAI TabularModel and Learner","6519084b":"We combine all training and validation files, define the index to split the combined dataframe, later. We delete not required dataframes.","bac33c53":"We can delete the dataframe to free host memory.","b1225e19":"# Faster FastAI Tabular Deep Learning Models\n\n## Overview\n\nRecently, deep learning has become popular for tabular machine learning problems. In this notebook, we show how to **speed-up training of tabular deep learning models by ~2x** with FastAI library using a customized NVTabular dataloader. In addition, the NVTabular data loader can stream data from disk, which allows to **use datasets larger than host\/GPU memory**. The FastAI data loader requires to load the full dataset into host memory in comparison to our data loader, which is able to read chunks of the dataset asynchronous from the disk.<br><br>\nThe limitations of the kaggle notebook environment with 13GB host memory and 16GB GPU memory enables us to **load only 20% (~2.4GB)** of the 11GB subset dataset. In this experiment, we set up NVTabular's dataloader to load the same partition of the dataset from disk. In our noteboo, [Faster ETL for Tabular Data](https:\/\/www.kaggle.com\/benediktschifferer\/faster-etl-for-tabular-data), we provide an example for the 11GB dataset.<br><br>\nActually, the comparison is not fully fair, as we compare FastAI data loader reading data from memory vs. our approach reading data from disk. Reading from memory should be faster and therefore, NVTabular has a disadvantages. However, we can still show a significant improvement in speed.<br><br>\n**Dataset:**<br>\nWe use the [preprocessed Criteo dataset](https:\/\/www.kaggle.com\/benediktschifferer\/criteo-dataset-parquet) in parquet format from the [Criteo Display Advertising Challenge](https:\/\/www.kaggle.com\/c\/criteo-display-ad-challenge). This [notebook](https:\/\/www.kaggle.com\/benediktschifferer\/preprocess-criteo-to-parquet\/) converted a [subset of the dataset](https:\/\/www.kaggle.com\/mrkmakr\/criteo-dataset) to parquet - we use only a 20% sample of this dataset. The pre-processing is based on our [previous notebook](https:\/\/www.kaggle.com\/benediktschifferer\/faster-etl-for-tabular-data).\n\n### Background - How is Tabular Data different?\n\nAfter deep learning models provide state-of-the-art results in the domains of computer vision, text and audio, neural network architectures show excellent results for tabular machine learning problems. For example, deep learning is used in [Taxi Destination Prediction](https:\/\/www.kaggle.com\/c\/pkdd-15-predict-taxi-service-trajectory-i), [Rossmann Sales Prediction](https:\/\/www.kaggle.com\/c\/rossmann-store-sales) or recommender systems and Jeremy Howard, founder of FastAI and former Chief Scientist of kaggle, [said that he uses deep learning for tabular data about 90% of the time](https:\/\/www.youtube.com\/watch?v=qqt3aMPB81c&feature=youtu.be&t=2339). However the nature of tabular data problems is different from the domains of images, text or audio.<br><br>\nIn our experience, the main bottleneck for training time is the data loader. Optimizing the data loader for tabular data improves the training time by 2-4x. Normally, tabular deep learning models are shallower than those used in computer vision, nlp or audio. For example, common tabular neural network architectures contain embedding tables and few dense layers in comparison to ResNet101 with upto 100 layers. Shallow neural network architecture requires less computation time and therefore, the data loader has less time to pre-fetch the next batch. In addition, tabular datasets have often much more samples (>100 mio.) and each sample requires less bytes to represent the information than in other domains. Just think for a short moment: An image with resolution 256x256 is represented by 65536 float values in comparison to tabular data with ~100 features requires ~100 values. If we address the dataset stucture, we can provide a data loader with higher troughput and decrease the training time.\n\n\n### NVTabular:\nNVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems. It provides a high level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS cuDF library. In the recent release, we added optimized data loader for PyTorch and TensorFlow, which reads asynchronous data from disk directly into GPU memory. We show the speed-up for the PyTorch data loader with FastAI.<br><br>\nGitHub: https:\/\/github.com\/NVIDIA\/NVTabular\/tree\/main\/nvtabular<br>\nMore examples: https:\/\/github.com\/NVIDIA\/NVTabular\/tree\/main\/examples","bbc4bcf0":"We train our model for 5 epochs.","296da9c4":"We initalize the FastAI TabularModel and Learner in the same way, as above.","f980a4ed":"We initialize the NVTabular data loader. Notice, how the structure is similar to the FastAI data loader, we need to provide the dataset schema.","35f1e28b":"We initialize the FastAI data loader.","725117da":"We define some hyperparameters.","7998600b":"One different between the NVTabular data loader and FastAI data loader is the order of categorical features. We need to order the embedding tables statistics based on `CATEGORICAL_COLUMNS`.","fe8e9797":"Install NVTabular. As mentioned, we will the new NVTabular API. We will install NVTabular from GitHub, using a specific commit. Later, we will change this to the v0.4 release.","c7a9b311":"Now, we train a FastAI Tabular Deep Learning model with NVTabular data loader. First, we delete the variables and free CPU\/GPU memory.","529efedc":"### Note: We will use the new and more flexible NVTabular API, which will be released with v0.4 in February 2021.","be842c43":"We define the train and validation files.","9c9d124c":"## Installing NVTabular on Kaggle\n\nKaggle added cuDF=0.16 support, which enables us to use NVTabular in Kaggle kernels. First, be sure that GPU acceleration and Internet are activated.\n\n<img src=\"https:\/\/bsopenbucket.s3-eu-west-1.amazonaws.com\/kaggle\/GPUInternet.png\" width=\"200px\">\n\nNext uninstall typing, as typing creates a conflict to install NVTabular.","8c7d5636":"# Results","c91b61e4":"We can take a look at the model structure."}}