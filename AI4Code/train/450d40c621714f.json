{"cell_type":{"136dc145":"code","3880c0bc":"code","77e243b1":"code","c4db405b":"code","27b82d02":"code","c6de8b77":"code","6e8f30d0":"code","b5641e43":"code","e99fc516":"code","4b9a0e72":"code","eef5f45a":"code","6cab2cdf":"code","de88019e":"code","2fcf4fb9":"code","3e978c86":"code","0d6a05fc":"code","3aaa0ea1":"code","64576319":"code","a0c75231":"code","d39347e2":"code","57a81cfe":"code","911879ac":"code","326ba8ed":"code","c3071903":"code","87fa87cf":"code","56540eae":"code","a5666b9b":"code","815ecd52":"code","11e5842c":"code","7174d9f3":"code","d3d15b84":"code","1fa28db4":"code","f2952a07":"code","54812f44":"code","bcdfe5b9":"code","555d09cb":"code","a63ced1a":"code","145b59a3":"markdown","a8d9c830":"markdown","c7ff3d13":"markdown","ad147959":"markdown","ecc0282b":"markdown","5cee4169":"markdown","a4558a10":"markdown","ebbe2b93":"markdown","5497f82c":"markdown","64b8c957":"markdown","f386b01c":"markdown","5d7829a7":"markdown","a8346d58":"markdown","1d5992e9":"markdown","544c8a65":"markdown","162c6f8d":"markdown","991f8b3f":"markdown","004b60df":"markdown","9d749669":"markdown","ddbebdb2":"markdown","45226ff7":"markdown","d4d86393":"markdown","3c7c5a0a":"markdown","09683982":"markdown","4bab76ab":"markdown","9ab11e20":"markdown","c7107543":"markdown","b59108ef":"markdown","ad07eded":"markdown","dfbd4840":"markdown","130d8bb1":"markdown","a1658648":"markdown","ffd23d07":"markdown","7c7c3f16":"markdown","372adc2c":"markdown","b9ebe04a":"markdown","1ca44482":"markdown","9b287b4a":"markdown","9455b1b6":"markdown","a2bce4d3":"markdown","429a3a96":"markdown","cafa1db0":"markdown","d786c26b":"markdown","41d074da":"markdown","8b67f57e":"markdown"},"source":{"136dc145":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn')","3880c0bc":"train_df = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\", index_col = ['Id'], na_values = \"?\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\", index_col = ['Id'], na_values = \"?\")\ntrain_df.head()\ntrain_df.shape","77e243b1":"missing_data = pd.concat([train_df.isna().sum().sort_values(ascending = False),\n                          (train_df.isna().sum()\/train_df.isna().count()*100).sort_values(ascending = False)],\n                           axis = 1, keys = ['Total', '%'])\nmissing_data.head()","c4db405b":"missing_per = missing_data['Total'].sum()\/train_df['age'].count()\nprint(f'O dataframe possui {missing_per*100:.2f}% de dados faltantes.')","27b82d02":"train_df[['occupation', 'workclass', 'native.country']] = SimpleImputer(strategy = 'most_frequent').fit_transform(train_df[['occupation',\n                                                                                                                            'workclass',\n                                                                                                                            'native.country']])","c6de8b77":"test_df.head()\ntest_df.shape","6e8f30d0":"missing_data = pd.concat([test_df.isna().sum().sort_values(ascending = False),\n                          (test_df.isna().sum()\/test_df.isna().count()*100).sort_values(ascending = False)],\n                           axis = 1, keys = ['Total', '%'])\nmissing_data.head()","b5641e43":"missing_per = missing_data['Total'].sum()\/test_df['age'].count()\ntest_df[['occupation', 'workclass', 'native.country']] = SimpleImputer(strategy = 'most_frequent').fit_transform(test_df[['occupation',\n                                                                                                                          'workclass',\n                                                                                                                          'native.country']])","e99fc516":"train_df['income'] = LabelEncoder().fit_transform(train_df['income'])\ntrain_df.head()","4b9a0e72":"plt.figure(figsize=(10, 6))\nsns.heatmap(train_df.corr(), square = True, annot = True, vmin = -1, vmax = 1, cmap = 'Reds')\nplt.show()","eef5f45a":"sns.boxplot(x = 'income', y = 'age', data = train_df, color = 'lightblue')\nplt.show()","6cab2cdf":"sns.boxplot(x = 'income', y = 'education.num', data = train_df, color = 'lightblue')\nplt.show()","de88019e":"sns.boxplot(x = 'income', y = 'hours.per.week', data = train_df, color = 'lightblue')\nplt.show()","2fcf4fb9":"sns.boxplot(x = 'income', y = 'capital.gain', data = train_df, color = 'lightblue')\nplt.show()","3e978c86":"sns.boxplot(x = 'income', y = 'capital.loss', data = train_df, color = 'lightblue')\nplt.show()","0d6a05fc":"sns.barplot(x = 'income', y = 'workclass', data = train_df, color = 'lightblue')\nplt.show()","3aaa0ea1":"sns.barplot(x = 'income', y = 'education', data = train_df, color = 'lightblue')\nplt.show()","64576319":"sns.barplot(x = 'income', y = 'marital.status', data = train_df, color = 'lightblue')\nplt.show()","a0c75231":"sns.barplot(x = 'income', y = 'occupation', data = train_df, color = 'lightblue')\nplt.show()","d39347e2":"sns.barplot(x = 'income', y = 'relationship', data = train_df, color = 'lightblue')\nplt.show()","57a81cfe":"sns.barplot(x = 'income', y = 'race', data = train_df, color = 'lightblue')\nplt.show()","911879ac":"sns.barplot(x = 'income', y = 'sex', data = train_df, color = 'lightblue')\nplt.show()","326ba8ed":"sns.barplot(x = 'income', y = 'native.country', data = train_df, color = 'lightblue')\nplt.show()","c3071903":"train_df['native.country'].value_counts()","87fa87cf":"train_df['USA'] = (train_df['native.country'] == 'United-States').astype(int)\ntest_df['USA'] = (test_df['native.country'] == 'United-States').astype(int)","56540eae":"train_df = train_df.drop(['native.country', 'fnlwgt', 'education'], axis = 1)\ntest_df = test_df.drop(['native.country', 'fnlwgt', 'education'], axis = 1)\ntrain_df.head()","a5666b9b":"Xtrain_df = train_df\nYtrain_df = train_df.pop('income')","815ecd52":"Xtrain_df.head()","11e5842c":"Ytrain_df.head()","7174d9f3":"Xtrain_df[['age', 'education.num', 'hours.per.week']] = StandardScaler().fit_transform(train_df[['age', 'education.num', 'hours.per.week']])\ntest_df[['age', 'education.num', 'hours.per.week']] = StandardScaler().fit_transform(test_df[['age', 'education.num', 'hours.per.week']])","d3d15b84":"Xtrain_df[['capital.gain', 'capital.loss']] = RobustScaler().fit_transform(train_df[['capital.gain', 'capital.loss']])\ntest_df[['capital.gain', 'capital.loss']] = RobustScaler().fit_transform(test_df[['capital.gain', 'capital.loss']])","1fa28db4":"Xtrain_df = pd.get_dummies(train_df, columns = ['workclass', 'marital.status', 'occupation', 'relationship', 'race', 'sex'], drop_first = True)\ntest_df = pd.get_dummies(test_df, columns = ['workclass', 'marital.status', 'occupation', 'relationship', 'race', 'sex'], drop_first = True)","f2952a07":"Ks = np.arange(35, 46, 1)\nfor k in Ks:\n    score = cross_val_score(KNeighborsClassifier(n_neighbors = k), Xtrain_df, Ytrain_df, cv = 10, scoring = 'accuracy').mean()\n    print(f'N\u00famero de vizinhos: {k} | Acur\u00e1cia: {score}')","54812f44":"knn = KNeighborsClassifier(n_neighbors = 40)\nknn.fit(Xtrain_df, Ytrain_df)","bcdfe5b9":"Ypred = knn.predict(test_df)\nYpred","555d09cb":"incomes = []\nfor income in Ypred:\n    if income == 0:\n        incomes.append('<=50K')\n    else:\n        incomes.append('>50K')\n\nsubmission = pd.DataFrame()\nsubmission[0] = test_df.index\nsubmission[1] = incomes\nsubmission.columns = ['Id', 'income']\nsubmission.head()","a63ced1a":"submission.to_csv('submission.csv', index = False)","145b59a3":"Devido ao valor consider\u00e1vel de dados faltantas no *dataframe*, e tamb\u00e9m por esses dados estarem apenas em vari\u00e1veis categ\u00f3ricas, ser\u00e1 optado por remover os dados faltantes e colocar a **moda** das features no lugar.","a8d9c830":"## 4. Prepara\u00e7\u00e3o dos **dados de treino e de teste**:","c7ff3d13":"### **Aluno:** Gabriel Pereira Rodrigues\n---","ad147959":"Fica evidenciado que as *features* ***capital.gain*** e ***capital.loss*** possuem muitos dados espa\u00e7ados, com bastante *outliers*, portanto, dever\u00e3o ser tratadas de forma especial. Enquanto, por outro lado, as outras *features* apresentam um comportamento considerado adequado, sem comportamentos estranhos.","ecc0282b":"- **Tratando as vari\u00e1veis categ\u00f3ricas:**","5cee4169":"---","a4558a10":"## 2. Importando as **bases de dados de treino e de teste** e tratando os dados faltantes:","ebbe2b93":"**Base de teste**:","5497f82c":"**Tratando a income:**","64b8c957":"Agora, para as vari\u00e1veis num\u00e9ricas **espa\u00e7adas** com muitos ***outliers***, que foram observadas nos \u00faltimos *box-plots*, ser\u00e1 utilizado o ***RobustScaler***, devido ao seu uso comum com tratamento de ***outliers***, afim de reduzir seu impacto negativo, com uma nova escala dos mesmos.","f386b01c":"Plotando os dados:","5d7829a7":"Isso pode ser explicado devido pela **m\u00e1 distribui\u00e7\u00e3o de pessoas de diferentes pa\u00edses** na base de dado, onde a maior parte \u00e9 estadunidense:","a8346d58":"---","1d5992e9":"Para isto, ser\u00e1 feito o ***one hot encoding*** nestas vari\u00e1veis, transformando-as assim em valores num\u00e9ricos e as colocando em v\u00e1rias colunas.","544c8a65":"## 3. An\u00e1lise dos dados utilizando a base de treino:","162c6f8d":"Verificando as **vari\u00e1veis categ\u00f3ricas** com gr\u00e1ficos de barras:","991f8b3f":"Pelos gr\u00e1ficos da **vari\u00e1veis categ\u00f3ricas**, conseguimos perceber aspectos interessantes dos dados, como: indiv\u00edduos casados possu\u00edrem ***income*** maior, o gen\u00earo apresentar uma desiguldade elevada no ***income*** e o n\u00edvel de gradua\u00e7\u00e3o influenciando no mesmo tamb\u00e9m. Contudo, \u00e9 poss\u00edvel perceber que todas as vari\u00e1veis n\u00e3o apresentam comportamentos fora do adequado, o que leva a crer que \u00e9 poss\u00edvel consider\u00e1-las na base de dados, exceto pelo \u00faltimo gr\u00e1fico, do ***native.country***, onde fica evidente como apenas nos Estados Unidos a barra de erro est\u00e1 normal, sem nenhuma discrep\u00e2ncia, enquanto nos outros pa\u00edses o **desvio padr\u00e3o** est\u00e1 muito elevado.","004b60df":"- Grupo das *features*:","9d749669":"Para isto, primeiramente, ser\u00e1 usado o ***StandardScaler*** devido ao seu uso mais convencional para esta situa\u00e7\u00e3o.","ddbebdb2":"Podemos observar que todas as *features* apresentam correla\u00e7\u00e3o positiva com ***income***, exceto ***fnlnwgt***, que mal apresenta correla\u00e7\u00e3o, o que deixa evidente que pode ser interessante **retir\u00e1-la da base de dados**.","45226ff7":"---","d4d86393":"Agora, ser\u00e1 feita separa\u00e7\u00e3o das *features* e da nossa *label* nos **dados de treino**.","3c7c5a0a":"Para resolver o desbalanceamento da vari\u00e1vel categ\u00f3rica ***native.country***, e ainda aproveitar sua informa\u00e7\u00e3o, ser\u00e1 feita uma nova coluna indicando se o indiv\u00edduo pertence ou n\u00e3o aos Estados Unidos, j\u00e1 que esse valor representa a maior parte das observa\u00e7\u00f5es. Para indiv\u00edduos dos Estados Unidos a nova coluna ter\u00e1 valor 1, e ter\u00e1 0 caso contr\u00e1rio.","09683982":"Tratando os dados faltantes aqui:","4bab76ab":"Verificando quantos dados faltantes existem no dataframe:","9ab11e20":"Ap\u00f3s feita a an\u00e1lise explorat\u00f3ria nos **dados de treino**, ser\u00e1 feita a prepara\u00e7\u00e3o de todos os dados em cima das conclus\u00f5es tiradas, para que isso n\u00e3o prejudique o desempenho e resultado do modelo.","c7107543":"Exportando os resultados em formato *csv*:","b59108ef":"**Bases de dados:**","ad07eded":"Ap\u00f3s a importa\u00e7\u00e3o dos dados, o importante \u00e9 o tratamento dos missing datas, querendo que assim eles n\u00e3o interfiram em an\u00e1lises posteriores e no modelo propriamente dito e n\u00e3o s\u00f3 isso tamb\u00e9m para que haja a verifica\u00e7\u00e3o da possibilidade de seguir com a base de dados com o n\u00famero de dados faltantes existentes.","dfbd4840":"## 5. Classificador KNN e resultados:","130d8bb1":"Verificando a o **coeficiente de correla\u00e7\u00e3o de Pearson** entre ***income*** e as outras *features* do *dataframe*:","a1658648":"E devido a esse baixo n\u00famero de dados nos outros pa\u00edses, o erro padr\u00e3o acaba aumentando, ent\u00e3o isso deve ser tratado de forma especial posteriormente.","ffd23d07":"Testando o modelo com o **K** escolhido e pegando os dados:","7c7c3f16":"## 1. Importando as bibliotecas do python que ser\u00e3o utilizadas:","372adc2c":"**Base de treino**:","b9ebe04a":"Como visto acima, no intervalo de **K** entre **35** e **46** (melhor desempenho mostrado nos testes), o **K = 40** \u00e9 a melhor op\u00e7\u00e3o.","1ca44482":"Aqui ser\u00e3o feitas **valida\u00e7\u00f5es cruzadas** com **10** *folders* para a sele\u00e7\u00e3o do **K**, tendo a **acur\u00e1cia** como crit\u00e9rio.","9b287b4a":"---","9455b1b6":"Agora a coluna da vari\u00e1vel categ\u00f3rica ***native.country*** poder\u00e1 ser removida, e aproveitando, ser\u00e3o removidas as *features* ***fnlwgt***, devido ao que foi comentado no t\u00f3pico anterior na parte de **correla\u00e7\u00e3o**, e ***education*** pelo fato de j\u00e1 termos a informa\u00e7\u00e3o dessa *feature* de forma num\u00e9rica pela ***education.num***.","a2bce4d3":"Na pr\u00f3xima etapa, ser\u00e1 feita a **normaliza\u00e7\u00e3o** das vari\u00e1veis num\u00e9ricas, devido a discrep\u00e2ncia de valores entre algumas e dispers\u00e3o entre outras, e as vari\u00e1veis categ\u00f3ricas ser\u00e3o **transformadas em num\u00e9ricas**.","429a3a96":"Criando um *dataframe* para os resultados:","cafa1db0":"- **Tratando as vari\u00e1veis num\u00e9ricas:**","d786c26b":"Percentualmente irei verificar onde est\u00e3o localizados os dados faltantes:","41d074da":"# ***PMR3508:*** An\u00e1lise e Predi\u00e7\u00e3o do *Dataset Adult* - ***KNN***","8b67f57e":"- Grupo da *target*:"}}