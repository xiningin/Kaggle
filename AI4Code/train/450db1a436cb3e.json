{"cell_type":{"b04a1f45":"code","bc3532f5":"code","6aa80b0c":"code","9bc87072":"code","c44713fc":"code","b8d35c61":"code","f2b33b74":"code","12f78f54":"code","86a9385d":"code","ce3240b3":"code","afd248d7":"code","c1039a1b":"code","f81d2da1":"code","af761e60":"code","a02c51cd":"code","15bd1d80":"code","258a3732":"code","929c5415":"code","2b9d28b3":"code","7f8322f6":"code","01411bd3":"code","761fa970":"code","0d1bdd15":"code","bc4ea20c":"code","d59721c1":"code","47966635":"code","149c75b2":"code","096195e1":"markdown","e34b12d9":"markdown","a6d52889":"markdown","d13f8f0d":"markdown","911b3434":"markdown","e04e7a9c":"markdown","4fb2d350":"markdown","f916d8c4":"markdown","21872f9b":"markdown","ec46db63":"markdown","c6353b5a":"markdown","b446c21b":"markdown","3cefd639":"markdown","dc5d8246":"markdown","4567bdc3":"markdown","06e6980a":"markdown","e3d3bd00":"markdown"},"source":{"b04a1f45":"#this is a data manipulation library centered around the Data Frame, which stores two-dimensional data in a tabular format \n#and makes certain operations easier to carry out\nimport pandas as pd\n\n#these are different elements from NLTK (Natural Language Toolkit), a massive natural language processing library\n#  we are importing individual elements: it's much cleaner to onlly bring in what we need\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk.collocations\nfrom nltk import bigrams\n\n#these are utility libraries; they contain useful data structures, collections, or functions \n#that will make our code more simple\nfrom string import punctuation\nfrom collections import Counter\nimport glob\nimport csv\n\n#these are a couple of plotting libraries, which we'll use at the very end of the exercise\nimport matplotlib.pyplot as plt\nimport seaborn as sns","bc3532f5":"#a wrapper function that turns frequencies into counts\n#  don't worry about this!\ndef get_bigram_count(blog_tokens, n = 5):\n    bigram_measures = nltk.collocations.BigramAssocMeasures()\n    \n    bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(blog_tokens)\n    bigram_finder.apply_freq_filter(3)\n\n    freq_n = bigram_finder.score_ngrams(bigram_measures.raw_freq)[0 : n]\n\n    num_bigrams = len([x for x in nltk.bigrams(blog_tokens)])\n\n    for i in range(len(freq_n)):\n        new_tup = (freq_n[i][0], round(freq_n[i][1] * num_bigrams))\n        freq_n[i] = new_tup\n\n    return freq_n","6aa80b0c":"#these are additional stopwords for use later... don't worry about this right now!\nADD_STOPS = [p for p in punctuation] + [\"''\", '\"\"', \"``\", '`', '...', '\u2019', \"'s\", \"n't\"]","9bc87072":"#checking that all of the desired files exist in the environment and are detected by Kaggle\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c44713fc":"#get all the text files in the directory\n#  we're looking for .txt files, but we don't care what their name is so we use \"*\" as a sort of \"wildcard\"\nfile_names = glob.glob('\/kaggle\/input\/text-files\/*.txt')","b8d35c61":"#taking a look at the files that we matched with our pattern\n#  these are filepaths; they are the instructions for arriving at a certain file, interpreted by the computer\nprint(file_names)","f2b33b74":"#creating a list to read all of the text into\nfile_strs = []\n\n#going one by one, inserting the text from each file into the list as a string\nfor fp in file_names:\n    with open(fp, 'r') as file:\n        file_strs.append(file.read())\n        \n#how many files did we get?        \nprint(len(file_strs))","12f78f54":"#taking a look at one of the blogs in our list\n#  in \"file_strs[0][0 : 500]\", \"[0]\" gets the first blog and the \"[0 : 500]\" gets the first 500 characters\nprint(file_strs[0][0 : 500])","86a9385d":"#putting all of the files together into one string\nall_blogs = '\\n'.join(file_strs)","ce3240b3":"#notice how tokens can be punctuation and that words like \"don't\" get split into \"do\" and \"n't\"\nstring_a = 'Hello there, how are you today? I am doing fine. Don\\'t get caught in the rain!'\nprint(word_tokenize(string_a))","afd248d7":"string_b = 'Hello'\nstring_c = 'hello'\nprint(string_b == string_c) #they're considered different\nprint(string_b.lower() == string_c.lower()) #now they're the same!","c1039a1b":"#taking a look at the first few stopwords from NLTK's stopword list\nstopwords.words('english')[0 : 10]","f81d2da1":"def preprocess_text(text_str, stopwords):\n    str_lower = text_str.lower() #normalizing case\n    str_tokens = word_tokenize(str_lower) #tokenizing\n    \n    final_tokens = [] #the list to hold non-stopword tokens\n    for token in str_tokens:\n        if token not in stops: #only keep the token if it's not a stopword\n            final_tokens.append(token)\n            \n    return final_tokens #pass back out the object to be used by the main script","af761e60":"#normalize case, tokenize, and remove stopwords\/punctuation\nstops = stopwords.words('english') + ADD_STOPS #our ADD_STOPS stopwords include punctuation and a little bit more\n\n#a list to hold all of our tokens for each text file\neach_blog_tokens = []\n\nfor i in range(len(file_strs)):\n    processed_blog = preprocess_text(file_strs[i], stops)\n    \n    each_blog_tokens.append(processed_blog) #adding our final list of tokens to \"each_blog_tokens\"","a02c51cd":"#seeing how we did with tokenization\nprint(each_blog_tokens[0][0 : 50])","15bd1d80":"#\"all_blog_tokens\" stores each blogs' tokens in one list whereas \"each_blog_tokens\" stores each blogs' tokens in seperate lists\nall_blog_tokens = preprocess_text(all_blogs, stops)\n\nprint(len(all_blog_tokens)) #the total amount of tokens in all blogs","258a3732":"#looking at the most frequent unigrams\nmost_common = []\n\n#getting the ten most frequent for each text\nfor blog in each_blog_tokens:\n    freq_dict = Counter(blog).most_common(10) #takes the list of tokens and returns a dictionary listing the frequency of each unique token\n    most_common.append(freq_dict) #adding it to our list","929c5415":"#seeing what the result is for the first blog post is\nprint(most_common[0])","2b9d28b3":"#getting the ten most frequent unigrams across ALL texts\nfreq_dict_all = Counter(all_blog_tokens).most_common(10)\n\nprint(freq_dict_all)","7f8322f6":"#looking at the most frequent and most associated bigrams\nbigram_measures = nltk.collocations.BigramAssocMeasures() #these will help us score the bigrams\n\nmost_associated = []\nmost_freq = []\n\n#extracting and scoring the bigrams for all texts\nfor blog in each_blog_tokens:\n    bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(blog) #building a bigram finder \n    bigram_finder.apply_freq_filter(3) #removing bigrams occuring less than three times\n\n    #scoring bigrams by PMI\n    assoc_five = bigram_finder.score_ngrams(bigram_measures.pmi)[0 : 5]\n    for i in range(len(assoc_five)): #rounding each bigram's association to three decimal places\n        assoc_five[i] = (assoc_five[i][0], round(assoc_five[i][1], 3))\n    most_associated.append(assoc_five)\n    \n    #scoring bigrams by raw count\n    freq_five = get_bigram_count(blog)\n    most_freq.append(freq_five)","01411bd3":"#looking at the result for the most frequent bigrams in the first blog post\nprint(most_freq[0])","761fa970":"#looking at the result for the most associated bigrams in the first blog post, by PMI\nprint(most_associated[0])","0d1bdd15":"#the header for each file\nheader_assoc = ['bigram', 'PMI_score', 'blog_name']\nheader_freq = ['bigram', 'count', 'blog_name']\n\n#the rows for each file, which must still be populated\nrows_assoc = []\nrows_freq = []\n\nct = 0 #keeping count of where we are within the individual blogs\nfor blog in most_associated:\n    for bigram in blog: #looking at each of the most associated bigrams\n        bigram_str = bigram[0][0] + '\/' + bigram[0][1] #building a string to represent the bigram\n        blog_name = file_names[ct].split('\/')[-1][0 : -4] #adding in the blog name as included in the original filepath\n        rows_assoc.append([bigram_str, bigram[1], blog_name]) #adding in each element \n    ct += 1\n    \n#the same process as before, but for the most frequent bigrams\nct = 0\nfor blog in most_freq:\n    for bigram in blog:\n        bigram_str = bigram[0][0] + '\/' + bigram[0][1]\n        blog_name = file_names[ct].split('\/')[-1][0 : -4]\n        rows_freq.append([bigram_str, bigram[1], blog_name])\n    ct += 1","bc4ea20c":"#writing to the files\nwith open('most_associated.csv', 'w') as file: #opening a connection\n    csvwriter = csv.writer(file) #building a csvwriter object to format our header and rows correctly\n    \n    csvwriter.writerow(header_assoc) #adding the header\n    csvwriter.writerows(rows_assoc) #adding all of the rows\n    \n#the same proces as before, but for the most frequent bigrams    \nwith open('most_frequent.csv', 'w') as file:\n    csvwriter = csv.writer(file)\n    \n    csvwriter.writerow(header_freq)\n    csvwriter.writerows(rows_freq)","d59721c1":"most_freq_df = pd.read_csv('.\/most_frequent.csv') #reading in the data\n\n#dataframes are displayed nicely by Kaggle as long as you don't use \"print()\"!\nmost_freq_df.head() #looking at the first few values","47966635":"#grabbing just the observations that correspond to the blog \"Acephalous-Internet\"\naceph = most_freq_df[most_freq_df[\"blog_name\"] == \"Acephalous-Internet\"] \n\naceph #seeing what we're working with","149c75b2":"#making a barplot using seaborn - we could do this for any of the blogs!\nsns.barplot(data = aceph, y = \"bigram\", x = \"count\")\n\nplt.savefig(\"img.png\", bbox_inches = 'tight', dpi = 400) #saving the plot","096195e1":"### Libraries\nOne of the strongest elements of python as a programming language is its libraries. \n\nMany utility libraries come with base python, but the real power comes in user-created libraries. You'll see that I have included a number of libraries, which will help to greatly simplify our code.\n\nIn general, import statements follow the format `import LIBRARY`. You can also `import LIBRARY as NAME` to have a more concise way of referencing the imported library. Finally, you can `from LIBRARY import ELEMENT` to bring in a particular function or object.","e34b12d9":"### PAUSE: questions?","a6d52889":"### Preprocessing\nPreprocessing is a vital step in natural langauge processing. It ensures that our text data is in the optimal format before being sent along to analysis.","d13f8f0d":"### Reading in the Text Files\nBefore we do any anlaysis, we have to bring in the text files that we would like to analyze.","911b3434":"### Saving Results\nFinally, we'll save results in a comma-seperated values (CSV) file. CSV files are simply text files that have each cell in a two-dimensional table seperated by a comma. Excel and Google Sheets can read and interpret files of this type.\n\nWe're trying to make a file that looks like the following table, where the **header** consists of \"bigram\" and \"PMI_score\" and each **row** is a single observation:\n\n|bigram       |PMI_score |blog_name|\n|-------------|----------|---------| \n|wordA\/wordB  |pmi1      |nameA    |\n|wordC\/wordD  |pmi2      |nameB    |\n\nThis would be represented in CSV format as \"bigram,PMI_score,blog_name\\nwordA\/wordB,pmi1,nameA\\nwordC\/wordD,pmi2,nameB,\" where \"\\n\" is a newline character.","e04e7a9c":"### PAUSE: questions?","4fb2d350":"**Normalizing Case:** we'd like for the words \"Hello\" and \"hello\" to be considered the same word. By default, they are different, so we will force all characters to lowercase in our preprocessing step.","f916d8c4":"### Additional elements\nThese are things that we will use later in the exercise... you don't have to worry about them right now! ","21872f9b":"**Removing Stopwords:** stopwords are words that are extremely common and aren't particularly indicative of a certain document or text. There are many ways to deal with stopwords, but the most simple method is to just remove them from our collection of tokens.","ec46db63":"Now that we understand each component of the preprocessing, we'll put each element into one tidy function.","c6353b5a":"**Bigram Association vs. Frequency:** in just a moment, we'll look at two possible scoring metrics for bigrams, association and frequency. The key difference lies in how importance is defined. \n\nWhile frequency looks simply for bigrams with high counts, association applies probability to measure importance. In particular, we will use pointwise mutual information (PMI) to compute association between tokens for each bigram. PMI will de-emphasize tokens that occur frequently and simply happen to co-occur, instead focusing on tokens that truly tend to co-occur at a significantly increased rate.\n\nAn issue with PMI is its tendency to reward bigrams containing rare tokens. To combat this problem, we will exlcude bigrams that occur less than three times.","b446c21b":"**Tokenization:** when we tokenize, we input in a string and receive a list of tokens as output. Tokens can be thought of as our base unit of analysis and are generally words.","3cefd639":"## Project 2: Sociology, History, and Linguistics\n\n### Lesson Goals\n- Learn how to read in text data\n- Work with libraries to make the coding process easier\n- Use basic natural language processing (NLP) to extract key information from texts\n\n### Final Product\n- Graph the frequency of common phrases (bigrams) within our selected text\n\n![image.png](attachment:image.png)","dc5d8246":"### PAUSE: questions?","4567bdc3":"### Simple Analysis\nAnlaysis in natural language processing can range from relatively simple counting methods all the way to complex machine learning algorithms. We're going to keep it simple here, looking only at counts for unigrams and bigrams.\n\n**What is a unigram?** Unigrams are single tokens. In the sentence _\"this is a sentence,\"_ the unigrams would be _\"this,\" \"is,\" \"a,\"_ and _\"sentence.\"_ \n\n**What is a bigram?** Bigrams are quite similar to unigrams, instead counting two-token pairs (makes sense--\"bi\" instead of \"uni\"). In the same sentence, the bigrams are _\"this is,\" \"is a,\"_ and _\"a sentence.\"_","06e6980a":"### Making a Nice Plot\n\nWe can make a very simple plot using `seaborn` and `matplotlib.pyplot` libraries and the results of our analysis. While very basic, it's definitely nice to have a visual representation of our results.","e3d3bd00":"You can find added description of the different functions and objects that we will be using at:\n- [NLTK](http:\/\/www.nltk.org\/=)\n- [glob](https:\/\/docs.python.org\/3\/library\/glob.html)\n- [pandas](https:\/\/pandas.pydata.org\/docs\/)\n- [string](https:\/\/docs.python.org\/2\/library\/string.html#string.punctuation)\n- [collections](https:\/\/docs.python.org\/2\/library\/collections.html#collections.Counter)\n- [seaborn](https:\/\/seaborn.pydata.org\/#)"}}