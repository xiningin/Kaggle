{"cell_type":{"017adeaa":"code","6d3fbdb6":"code","d906ea06":"code","8308ca91":"code","3d6a71f3":"code","efa2d44b":"code","d07e5b23":"code","7181b5ff":"code","2aef089a":"code","a6dd4d8a":"code","cce23874":"code","ca21c727":"code","8e000970":"code","931d60cb":"code","a113e8b2":"code","b434742b":"code","52f3d8b1":"code","80f666bf":"code","965e230d":"code","b228d0fa":"code","c8506654":"markdown","b48817e5":"markdown","aeac732a":"markdown","d37431ee":"markdown","96e7a0ee":"markdown","9a55be8b":"markdown","c1717813":"markdown","9e5fdccc":"markdown","3724a895":"markdown","d477f335":"markdown","1a7f4813":"markdown","3c6daa30":"markdown","460a72eb":"markdown","75ef1824":"markdown","9e849459":"markdown","a008d692":"markdown","31a7932b":"markdown"},"source":{"017adeaa":"from google.cloud import bigquery","6d3fbdb6":"client = bigquery.Client()","d906ea06":"hn_dataset_ref = client.dataset('noaa_icoads', project='bigquery-public-data')","8308ca91":"type(hn_dataset_ref)","3d6a71f3":"hn_dset = client.get_dataset(hn_dataset_ref)","efa2d44b":"type(hn_dset)","d07e5b23":"[i.table_id for i in client.list_tables(hn_dset)]","7181b5ff":"icoads_core_1662_2000 = client.get_table(hn_dset.table('icoads_core_1662_2000'))","2aef089a":"type(icoads_core_1662_2000)","a6dd4d8a":"[command for command in dir(icoads_core_1662_2000) if not command.startswith('_')]","cce23874":"icoads_core_1662_2000.schema","ca21c727":"[type(i) for i in icoads_core_1662_2000.schema]","8e000970":"[i.name+\", type: \"+i.field_type for i in icoads_core_1662_2000.schema]","931d60cb":"schema_subset = [col for col in icoads_core_1662_2000.schema if col.name in ('sea_surface_temp', 'sea_level_pressure', 'present_weather')]\nresults = [x for x in client.list_rows(icoads_core_1662_2000, start_index=100, selected_fields=schema_subset, max_results=10)]","a113e8b2":"print(results)","b434742b":"for i in results:\n    print(dict(i))","52f3d8b1":"BYTES_PER_GB = 2**30\nicoads_core_1662_2000.num_bytes \/ BYTES_PER_GB","80f666bf":"def estimate_gigabytes_scanned(query, bq_client):\n    # see https:\/\/cloud.google.com\/bigquery\/docs\/reference\/rest\/v2\/jobs#configuration.dryRun\n    my_job_config = bigquery.job.QueryJobConfig()\n    my_job_config.dry_run = True\n    my_job = bq_client.query(query, job_config=my_job_config)\n    BYTES_PER_GB = 2**30\n    return my_job.total_bytes_processed \/ BYTES_PER_GB","965e230d":"estimate_gigabytes_scanned(\"SELECT sea_level_pressure FROM `bigquery-public-data.noaa_icoads.icoads_core_1662_2000`\", client)","b228d0fa":"estimate_gigabytes_scanned(\"SELECT * FROM `bigquery-public-data.noaa_icoads.icoads_core_1662_2000`\", client)","c8506654":"The schema sounds helpful, so let's try that.","b48817e5":"You can print the results directly, but I find that to be a little hard to read. It displays some boilerplate, then all of the values, then all of the keys.","aeac732a":"This matches the value given on [the dataset documentation page](https:\/\/bigquery.cloud.google.com\/table\/bigquery-public-data:hacker_news.full?tab=details).","d37431ee":"Let's take a closer look at the schema for the `icoads_core_1662_2000` table. As with datasets, we'll need to pass a reference to the table to the `client.get_table` method.","96e7a0ee":"Let's look what types are here","9a55be8b":"In this kernel I'll use [this notebook](https:\/\/www.kaggle.com\/sohier\/beyond-queries-exploring-the-bigquery-api) to show you some useful BigQuery (BQ) API functions and provide resources for learning on your own.\n\nOur first goal will be to list all of the tables available in the [NOAA-ICOADS](https:\/\/www.kaggle.com\/noaa\/noaa-icoads) dataset. To get started, we'll need to load the bigquery client. \n\nYou may want to have the documentation open as you follow along:\n- [Python client guide](https:\/\/googlecloudplatform.github.io\/google-cloud-python\/latest\/bigquery\/usage.html)\n- [BQ API reference](https:\/\/googlecloudplatform.github.io\/google-cloud-python\/latest\/bigquery\/reference.html)","c1717813":"Usually I'd probably pass this into pandas before doing further work, but for now we can just convert the `google.cloud.bigquery.table.Row` results to dicts to get a version that prints a bit more nicely.","9e5fdccc":"It turns out that the schema is a necessary input for one of the more useful commands in BQ: [list_rows](https:\/\/googlecloudplatform.github.io\/google-cloud-python\/latest\/bigquery\/reference.html#google.cloud.bigquery.client.Client.list_rows). `List_rows` returns a slice of a dataset without scanning any other section of the table. If you've ever written a BQ query that included a `limit` clause, which limits the data returned rather than the data scanned, you probably actually wanted `list_rows` instead.\n\nI'd like to see a subset of the columns, but the `selected_fields` parameter requires a schema object as an input. We'll need to build a subset of the schema first to pass for that parameter.","3724a895":"Suppose we wanted to check what resources we would have consumed by doing a full table scan instead of using `list_rows`. Looks like the `num_bytes` method should help us there.","d477f335":"Now we can run a quick test checking the impact of  selecting one column versus an entire table. ","1a7f4813":"Rather than looking at the documentation, I'll try being a little more independent while digging into the table commands.","3c6daa30":"As the [documentation](https:\/\/googleapis.dev\/python\/bigquery\/latest\/generated\/google.cloud.bigquery.schema.SchemaField.html?highlight=schemafield#google.cloud.bigquery.schema.SchemaField) says, `SchemaField` has fields `name` and `field_type`, so let's look at them","460a72eb":"Now we can use [client.list_tables](https:\/\/googlecloudplatform.github.io\/google-cloud-python\/latest\/bigquery\/reference.html#google.cloud.bigquery.client.Client.list_tables) to get information about the tables within the dataset.","75ef1824":"Next, we'll load the NOAA-ICOADS dataset. There are a few gotchas to bear in mind here:\n- To load a [dataset](https:\/\/googlecloudplatform.github.io\/google-cloud-python\/latest\/bigquery\/reference.html#google.cloud.bigquery.dataset.Dataset) you first need to generate a [dataset reference](https:\/\/googlecloudplatform.github.io\/google-cloud-python\/latest\/bigquery\/reference.html#google.cloud.bigquery.dataset.DatasetReference) to point BQ to it. \n- Any time you're working with BQ from Kaggle the project name is `bigquery-public-data`. \n- Kaggle imposes some limitations on the BQ API to make it work from our platform, but most read-only commands will work. One key exception is `client.list_datasets` so you may want to look at one of the starter kernels to get the ID for your dataset.","9e849459":"Hope this helps!","a008d692":"Once we have a reference, we can load the real dataset.","31a7932b":"The method [client.dataset](https:\/\/googlecloudplatform.github.io\/google-cloud-python\/latest\/bigquery\/reference.html#google.cloud.bigquery.dataset.DatasetReference) is named as if it returns a dataset, but it actually gives us  a dataset reference."}}