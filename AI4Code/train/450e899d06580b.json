{"cell_type":{"e1456d26":"code","c2e8abc4":"code","1c7df5e9":"code","e4be2d93":"code","434443e2":"code","b75839fd":"code","a52f2682":"code","72667539":"code","ed44b4c4":"code","e825419b":"code","ac32b1d5":"code","7211cae2":"markdown","f5736c22":"markdown","a3eca958":"markdown","389e51ac":"markdown","e2718b8f":"markdown","57b9cbce":"markdown"},"source":{"e1456d26":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport tensorflow as tf","c2e8abc4":"data = pd.read_csv('..\/input\/ecommerce-reviews-for-women-clothings\/Womens-Clothing-E-Commerce-Reviews.csv')","1c7df5e9":"data","e4be2d93":"data.info()","434443e2":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Drop rows with missing reviews\n    missing_review_rows = df[df['Review Text'].isna()].index\n    df = df.drop(missing_review_rows, axis=0).reset_index(drop=True)\n    \n    \n    # Use only the review and rating column\n    y = df['Rating']\n    X = df['Review Text']\n    \n    \n    # Make y a binary target\n    y = y.apply(lambda x: 1 if x == 5 else 0)\n    \n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    \n    # Learn the vocabulary\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(X_train)\n    \n    \n    # Find the size of the vocabulary\n    vocab_length = len(tokenizer.word_index) + 1\n    print(\"Vocab length:\", vocab_length)\n    \n    \n    # Convert review texts into sequences of integers\n    X_train = tokenizer.texts_to_sequences(X_train)\n    X_test = tokenizer.texts_to_sequences(X_test)\n    \n    \n    # Find the maximum sequence length\n    max_seq_length = np.max(list(map(lambda x: len(x), X_train)))\n    print(\"Maximum sequence length:\", max_seq_length)\n    \n    \n    # Pad the sequences to by uniform length\n    X_train = pad_sequences(X_train, maxlen=max_seq_length, padding='post')\n    X_test = pad_sequences(X_test, maxlen=max_seq_length, padding='post')\n    \n    \n    return X_train, X_test, y_train, y_test, vocab_length, max_seq_length","b75839fd":"X_train, X_test, y_train, y_test, VOCAB_LENGTH, MAX_SEQ_LENGTH = preprocess_inputs(data)","a52f2682":"X_train.shape","72667539":"X_test.shape","ed44b4c4":"inputs = tf.keras.Input(shape=(X_train.shape[1],))\n\nword_embedding = tf.keras.layers.Embedding(\n    input_dim=VOCAB_LENGTH,\n    output_dim=128,\n    input_length=MAX_SEQ_LENGTH\n)(inputs)\n\nword_flatten = tf.keras.layers.Flatten()(word_embedding)\ngru = tf.keras.layers.GRU(256, return_sequences=False, activation='tanh')(word_embedding)\n\nconcat = tf.keras.layers.concatenate([word_flatten, gru])\n\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(concat)\n\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nprint(model.summary())\ntf.keras.utils.plot_model(model)","e825419b":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","ac32b1d5":"model.evaluate(X_test, y_test)","7211cae2":"# Getting Started","f5736c22":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/EY_dMXXtazg","a3eca958":"# Results","389e51ac":"# Task for Today  \n\n***\n\n## Clothing Review Rating Prediction  \n\nGiven *reviews of women's clothing*, let's try to predict whether the rating associated with the review will be **5-star** or not.\n\nWe will use a TensorFlow\/Keras recurrent neural network to make our predictions.","e2718b8f":"# Preprocessing","57b9cbce":"# Training"}}