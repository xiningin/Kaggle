{"cell_type":{"8782d91e":"code","2c4bd4b3":"code","dfdb70eb":"code","028f88ce":"code","ae3d5475":"code","6cc06bdc":"code","52a29e69":"code","70e5ac58":"code","dd869789":"code","8bf995de":"code","aa213a6c":"code","03972d37":"code","e0e190d6":"code","8592ae76":"code","3f7fecae":"code","89a20bbc":"code","98c9631f":"code","6fbf7b80":"code","87aa5c70":"code","0bcf19d5":"code","45ee4423":"code","27ff47dd":"code","103ecfce":"code","a1544360":"code","8cdd94d9":"code","617a320a":"code","0107d2f2":"code","4628c5d2":"code","734ce647":"code","38044c55":"code","5e564361":"code","f244429d":"code","82cb1a98":"code","6b311516":"code","27e2da50":"code","73b0af5a":"code","04a63000":"code","b3e7e937":"code","9a9f6d23":"code","c9be6246":"code","31a2e63d":"code","300d6084":"code","31b8e631":"code","10c08beb":"code","50c9bddc":"code","be303e8b":"code","c6d30dc4":"code","4bbb3d82":"code","245e5c6e":"code","43413ba0":"code","b2ba1e12":"code","f548e2f0":"code","6ff22797":"code","2245e9bd":"code","3cd85d38":"code","6889d32e":"code","a1bdbf0b":"code","b1b6f309":"code","d72aaf96":"code","827217a8":"code","eca92636":"code","ccb965fc":"code","6d87873b":"code","5fe31ebd":"code","913342c5":"code","a5281e35":"code","b492a66d":"code","ffff4e49":"code","fbedaabf":"code","4d75d42a":"code","a84a8384":"code","e71f18cf":"code","a02d16d0":"code","dfea0b12":"code","15d7be58":"code","4aee804a":"code","025e2c62":"code","588b7270":"code","90e23af1":"code","abd3c543":"code","9723afab":"code","a14a60cc":"code","ceca8fff":"code","9698d681":"code","44553326":"code","37a67954":"code","fea5572d":"code","efe19d03":"code","92ab2824":"code","99108e81":"code","f82ece2e":"code","d657941f":"code","24b0b6bb":"code","362f7ca1":"code","b65ed6f8":"code","90ae2872":"code","31b3a51c":"code","dc424abc":"code","ea72b11c":"code","a97af24e":"code","ff0e990b":"code","ea5e4be4":"code","620c339a":"code","bb770185":"code","fb69e330":"code","febcc37b":"code","5447bbe6":"code","8e87a647":"code","da1de0a8":"code","9c5f88fa":"markdown","6e092b3b":"markdown","50dabac9":"markdown","a393fadd":"markdown","1220f266":"markdown","4593b584":"markdown","dfbc352f":"markdown","d4feeffb":"markdown","1bfb66fa":"markdown","0502b687":"markdown","60de20b7":"markdown","3db8d0d5":"markdown","4b8a2b7e":"markdown","3a5d847c":"markdown","a0af2aa1":"markdown","68837252":"markdown","2f4d0de0":"markdown","82ac86e1":"markdown","05264886":"markdown","36be79bd":"markdown","6af02f17":"markdown","aec49251":"markdown","bc6f32ac":"markdown","1ca37fca":"markdown","975b99ba":"markdown","ddc06377":"markdown","edf447bd":"markdown","ba000f58":"markdown","7720a258":"markdown","ab0ca633":"markdown","211f5ec3":"markdown","b55df894":"markdown","5adb3ad4":"markdown","3e4d67e0":"markdown","a6bb2c9a":"markdown","d03b063e":"markdown","320c5b52":"markdown","a8ea7453":"markdown","e7099398":"markdown","e7e29cd3":"markdown","01894185":"markdown","111f3042":"markdown","c2a0db17":"markdown","db484c10":"markdown","d6334c04":"markdown","b330db9a":"markdown","0e5427d7":"markdown","ecd78dae":"markdown","627a55d3":"markdown","322548bb":"markdown","60cc02dd":"markdown","71d2903b":"markdown","3767969f":"markdown","a05fd2f8":"markdown","6722f95a":"markdown","f85d9f9d":"markdown","266776c1":"markdown","fbcbd688":"markdown","4fd649f0":"markdown","e81a0ca1":"markdown","22ac8e78":"markdown","18c019f0":"markdown","0aff2779":"markdown","f6d3a8da":"markdown","fb62c56b":"markdown","d8e1104a":"markdown","ce549843":"markdown"},"source":{"8782d91e":"import sys\nimport os\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n%matplotlib inline\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\npy.init_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')","2c4bd4b3":"# Always good to set a seed for reproducibility\nSEED = 7\nnp.random.seed(SEED)","dfdb70eb":"# 2. Data Preparing ","028f88ce":"# Loading Data\ndf = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\n# Getting dataframe columns names\ndf_name=df.columns","ae3d5475":"df.info()","6cc06bdc":"df.head()","52a29e69":"df.describe()","70e5ac58":" g = sns.pairplot(df, hue=\"Outcome\", palette=\"husl\")","dd869789":"def plotHist(df,nameOfFeature):\n    cls_train = df[nameOfFeature]\n    data_array = cls_train\n    hist_data = np.histogram(data_array)\n    binsize = .5\n\n    trace1 = go.Histogram(\n        x=data_array,\n        histnorm='count',\n        name='Histogram of Wind Speed',\n        autobinx=False,\n        xbins=dict(\n            start=df[nameOfFeature].min()-1,\n            end=df[nameOfFeature].max()+1,\n            size=binsize\n        )\n    )\n\n    trace_data = [trace1]\n    layout = go.Layout(\n        bargroupgap=0.3,\n         title='The distribution of ' + nameOfFeature,\n        xaxis=dict(\n            title=nameOfFeature,\n            titlefont=dict(\n                family='Courier New, monospace',\n                size=18,\n                color='#7f7f7f'\n            )\n        ),\n        yaxis=dict(\n            title='Number of labels',\n            titlefont=dict(\n                family='Courier New, monospace',\n                size=18,\n                color='#7f7f7f'\n            )\n        )\n    )\n    fig = go.Figure(data=trace_data, layout=layout)\n    py.iplot(fig)","8bf995de":"from scipy.stats import skew\nfrom scipy.stats import kurtosis\ndef plotBarCat(df,feature,target):\n    \n    \n    \n    x0 = df[df[target]==0][feature]\n    x1 = df[df[target]==1][feature]\n\n    trace1 = go.Histogram(\n        x=x0,\n        opacity=0.75\n    )\n    trace2 = go.Histogram(\n        x=x1,\n        opacity=0.75\n    )\n\n    data = [trace1, trace2]\n    layout = go.Layout(barmode='overlay',\n                      title=feature,\n                       yaxis=dict(title='Count'\n        ))\n    fig = go.Figure(data=data, layout=layout)\n\n    py.iplot(fig, filename='overlaid histogram')\n    \n    def DescribeFloatSkewKurt(df,target):\n        \"\"\"\n            A fundamental task in many statistical analyses is to characterize\n            the location and variability of a data set. A further\n            characterization of the data includes skewness and kurtosis.\n            Skewness is a measure of symmetry, or more precisely, the lack\n            of symmetry. A distribution, or data set, is symmetric if it\n            looks the same to the left and right of the center point.\n            Kurtosis is a measure of whether the data are heavy-tailed\n            or light-tailed relative to a normal distribution. That is,\n            data sets with high kurtosis tend to have heavy tails, or\n            outliers. Data sets with low kurtosis tend to have light\n            tails, or lack of outliers. A uniform distribution would\n            be the extreme case\n        \"\"\"\n        print('-*-'*25)\n        print(\"{0} mean : \".format(target), np.mean(df[target]))\n        print(\"{0} var  : \".format(target), np.var(df[target]))\n        print(\"{0} skew : \".format(target), skew(df[target]))\n        print(\"{0} kurt : \".format(target), kurtosis(df[target]))\n        print('-*-'*25)\n    \n    DescribeFloatSkewKurt(df,target)","aa213a6c":"plotBarCat(df,df_name[0],'Outcome')","03972d37":"plotBarCat(df,df_name[1],'Outcome')","e0e190d6":"plotBarCat(df,df_name[2],'Outcome')","8592ae76":"plotBarCat(df,df_name[3],'Outcome')","3f7fecae":"plotBarCat(df,df_name[4],'Outcome')","89a20bbc":"plotBarCat(df,df_name[5],'Outcome')","98c9631f":"plotBarCat(df,df_name[6],'Outcome')","6fbf7b80":"plotBarCat(df,df_name[7],'Outcome')","87aa5c70":"plotBarCat(df,df_name[8],'Outcome')","0bcf19d5":"def PlotPie(df, nameOfFeature):\n    labels = [str(df[nameOfFeature].unique()[i]) for i in range(df[nameOfFeature].nunique())]\n    values = [df[nameOfFeature].value_counts()[i] for i in range(df[nameOfFeature].nunique())]\n\n    trace=go.Pie(labels=labels,values=values)\n\n    py.iplot([trace])","45ee4423":"PlotPie(df, 'Outcome')","27ff47dd":"def OutLiersBox(df,nameOfFeature):\n    \n    trace0 = go.Box(\n        y = df[nameOfFeature],\n        name = \"All Points\",\n        jitter = 0.3,\n        pointpos = -1.8,\n        boxpoints = 'all',\n        marker = dict(\n            color = 'rgb(7,40,89)'),\n        line = dict(\n            color = 'rgb(7,40,89)')\n    )\n\n    trace1 = go.Box(\n        y = df[nameOfFeature],\n        name = \"Only Whiskers\",\n        boxpoints = False,\n        marker = dict(\n            color = 'rgb(9,56,125)'),\n        line = dict(\n            color = 'rgb(9,56,125)')\n    )\n\n    trace2 = go.Box(\n        y = df[nameOfFeature],\n        name = \"Suspected Outliers\",\n        boxpoints = 'suspectedoutliers',\n        marker = dict(\n            color = 'rgb(8,81,156)',\n            outliercolor = 'rgba(219, 64, 82, 0.6)',\n            line = dict(\n                outliercolor = 'rgba(219, 64, 82, 0.6)',\n                outlierwidth = 2)),\n        line = dict(\n            color = 'rgb(8,81,156)')\n    )\n\n    trace3 = go.Box(\n        y = df[nameOfFeature],\n        name = \"Whiskers and Outliers\",\n        boxpoints = 'outliers',\n        marker = dict(\n            color = 'rgb(107,174,214)'),\n        line = dict(\n            color = 'rgb(107,174,214)')\n    )\n\n    data = [trace0,trace1,trace2,trace3]\n\n    layout = go.Layout(\n        title = \"{} Outliers\".format(nameOfFeature)\n    )\n\n    fig = go.Figure(data=data,layout=layout)\n    py.iplot(fig, filename = \"Outliers\")\n    \n","103ecfce":"OutLiersBox(df,df_name[0])","a1544360":"OutLiersBox(df,df_name[1])","8cdd94d9":"OutLiersBox(df,df_name[2])","617a320a":"OutLiersBox(df,df_name[3])","0107d2f2":"OutLiersBox(df,df_name[4])","4628c5d2":"OutLiersBox(df,df_name[5])","734ce647":"OutLiersBox(df,df_name[6])","38044c55":"OutLiersBox(df,df_name[7])","5e564361":"OutLiersBox(df,df_name[8])","f244429d":"from scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\n\nfrom sklearn import svm\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\ndef OutLierDetection(df,feature1,feature2,outliers_fraction=.1):\n    \n    new_df = df.copy()\n    rng = np.random.RandomState(42)\n\n    # Example settings\n    n_samples = new_df.shape[0]\n#     outliers_fraction = 0.2 # ************************************** imp\n    clusters_separation = [0]#, 1, 2]\n\n    # define two outlier detection tools to be compared\n    classifiers = {\n        \"One-Class SVM\": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,\n                                         kernel=\"rbf\", gamma=0.1),\n        \"Robust covariance\": EllipticEnvelope(contamination=outliers_fraction),\n        \"Isolation Forest\": IsolationForest(max_samples=n_samples,\n                                            contamination=outliers_fraction,\n                                            random_state=rng),\n        \"Local Outlier Factor\": LocalOutlierFactor(\n            n_neighbors=35,\n            contamination=outliers_fraction)}\n\n    \n    xx, yy = np.meshgrid(np.linspace(new_df[feature1].min()-new_df[feature1].min()*10\/100, \n                                     new_df[feature1].max()+new_df[feature1].max()*10\/100, 50),\n                         np.linspace(new_df[feature2].min()-new_df[feature2].min()*10\/100,\n                                     new_df[feature2].max()+new_df[feature2].max()*10\/100, 50))\n\n\n    n_inliers = int((1. - outliers_fraction) * n_samples)\n    n_outliers = int(outliers_fraction * n_samples)\n    ground_truth = np.ones(n_samples, dtype=int)\n    ground_truth[-n_outliers:] = -1\n\n    # Fit the problem with varying cluster separation\n    for i, offset in enumerate(clusters_separation):\n        np.random.seed(42)\n        # Data generation\n\n        X = new_df[[feature1,feature2]].values.tolist()\n\n        # Fit the model\n        plt.figure(figsize=(9, 7))\n        for i, (clf_name, clf) in enumerate(classifiers.items()):\n            # fit the data and tag outliers\n            if clf_name == \"Local Outlier Factor\":\n                y_pred = clf.fit_predict(X)\n                scores_pred = clf.negative_outlier_factor_\n            else:\n                clf.fit(X)\n                scores_pred = clf.decision_function(X)\n                y_pred = clf.predict(X)\n            threshold = stats.scoreatpercentile(scores_pred,\n                                                100 * outliers_fraction)\n            n_errors = (y_pred != ground_truth).sum()\n            \n            unique, counts = np.unique(y_pred,return_counts=True)\n            print(clf_name,dict(zip(unique, counts)))\n            \n            new_df[feature1+'_'+feature2+clf_name] = y_pred\n#             print(clf_name,y_pred) \n            # plot the levels lines and the points\n            if clf_name == \"Local Outlier Factor\":\n                # decision_function is private for LOF\n                Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])\n            else:\n                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            subplot = plt.subplot(2, 2, i + 1)\n            subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n                             cmap=plt.cm.Blues_r)\n            a = subplot.contour(xx, yy, Z, levels=[threshold],\n                                linewidths=2, colors='red')\n            subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n                             colors='orange')\n            b = plt.scatter(new_df[feature1], new_df[feature2], c='white',\n                     s=20, edgecolor='k')\n\n            subplot.axis('tight')\n\n            subplot.set_xlabel(\"%s\" % (feature1))\n \n            plt.ylabel(feature2)#, fontsize=18)\n            plt.title(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))\n\n        plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n#         plt.suptitle(\"Outlier detection\")\n\n    plt.show()\n    return new_df","82cb1a98":"tt = OutLierDetection(df,'Pregnancies','BloodPressure',.1)","6b311516":"# Load libraries\n\nfrom pandas import set_option\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier","27e2da50":"X =  df[df_name[0:8]]\nY = df[df_name[8]]\nX_train, X_test, y_train, y_test =train_test_split(X,Y,\n                                                   test_size=0.25,\n                                                   random_state=0,\n                                                   stratify=df['Outcome'])","73b0af5a":"# Spot-Check Algorithms\ndef GetBasedModel():\n    basedModels = []\n    basedModels.append(('LR'   , LogisticRegression()))\n    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n    basedModels.append(('KNN'  , KNeighborsClassifier()))\n    basedModels.append(('CART' , DecisionTreeClassifier()))\n    basedModels.append(('NB'   , GaussianNB()))\n    basedModels.append(('SVM'  , SVC(probability=True)))\n    basedModels.append(('AB'   , AdaBoostClassifier()))\n    basedModels.append(('GBM'  , GradientBoostingClassifier()))\n    basedModels.append(('RF'   , RandomForestClassifier()))\n    basedModels.append(('ET'   , ExtraTreesClassifier()))\n\n    \n    return basedModels","04a63000":"def BasedLine2(X_train, y_train,models):\n    # Test options and evaluation metric\n    num_folds = 10\n    scoring = 'accuracy'\n\n    results = []\n    names = []\n    for name, model in models:\n        kfold = StratifiedKFold(n_splits=num_folds, random_state=SEED)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n    return names, results","b3e7e937":"class PlotBoxR(object):\n    \n    \n    def __Trace(self,nameOfFeature,value): \n    \n        trace = go.Box(\n            y=value,\n            name = nameOfFeature,\n            marker = dict(\n                color = 'rgb(0, 128, 128)',\n            )\n        )\n        return trace\n\n    def PlotResult(self,names,results):\n        \n        data = []\n\n        for i in range(len(names)):\n            data.append(self.__Trace(names[i],results[i]))\n\n\n        py.iplot(data)","9a9f6d23":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, y_train,models)\nPlotBoxR().PlotResult(names,results)","c9be6246":"def ScoreDataFrame(names,results):\n    def floatingDecimals(f_val, dec=3):\n        prc = \"{:.\"+str(dec)+\"f}\" \n    \n        return float(prc.format(f_val))\n\n    scores = []\n    for r in results:\n        scores.append(floatingDecimals(r.mean(),4))\n\n    scoreDataFrame = pd.DataFrame({'Model':names, 'Score': scores})\n    return scoreDataFrame","31a2e63d":"basedLineScore = ScoreDataFrame(names,results)\nbasedLineScore","300d6084":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef GetScaledModel(nameOfScaler):\n    \n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n\n    pipelines = []\n    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression())])))\n    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))\n    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC())])))\n    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])  ))\n    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier())])  ))\n\n\n    return pipelines ","31b8e631":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, y_train,models)\nPlotBoxR().PlotResult(names,results)\nscaledScoreStandard = ScoreDataFrame(names,results)\ncompareModels = pd.concat([basedLineScore,\n                           scaledScoreStandard], axis=1)\ncompareModels","10c08beb":"models = GetScaledModel('minmax')\nnames,results = BasedLine2(X_train, y_train,models)\nPlotBoxR().PlotResult(names,results)\n\nscaledScoreMinMax = ScoreDataFrame(names,results)\ncompareModels = pd.concat([basedLineScore,\n                           scaledScoreStandard,\n                          scaledScoreMinMax], axis=1)\ncompareModels","50c9bddc":"df_t = df.copy()\ndf_t_name = df_t.columns","be303e8b":"def TurkyOutliers(df_out,nameOfFeature,drop=False):\n\n    valueOfFeature = df_out[nameOfFeature]\n    # Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(valueOfFeature, 25.)\n\n    # Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(valueOfFeature, 75.)\n\n    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3-Q1)*1.5\n    # print \"Outlier step:\", step\n    outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].index.tolist()\n    feature_outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].values\n    # df[~((df[nameOfFeature] >= Q1 - step) & (df[nameOfFeature] <= Q3 + step))]\n\n\n    # Remove the outliers, if any were specified\n    print (\"Number of outliers (inc duplicates): {} and outliers: {}\".format(len(outliers), feature_outliers))\n    if drop:\n        good_data = df_out.drop(df_out.index[outliers]).reset_index(drop = True)\n        print (\"New dataset with removed outliers has {} samples with {} features each.\".format(*good_data.shape))\n        return good_data\n    else: \n        print (\"Nothing happens, df.shape = \",df_out.shape)\n        return df_out","c6d30dc4":"feature_number = 0\nOutLiersBox(df,df_name[feature_number])","4bbb3d82":"df_clean = TurkyOutliers(df_t,df_name[feature_number],True)\nOutLiersBox(df_clean,df_name[feature_number])","245e5c6e":"feature_number = 1\nOutLiersBox(df,df_name[feature_number])","43413ba0":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],True)\nOutLiersBox(df_clean,df_name[feature_number])","b2ba1e12":"feature_number = 2\nOutLiersBox(df,df_name[feature_number])","f548e2f0":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],True)\nOutLiersBox(df_clean,df_name[feature_number])","6ff22797":"feature_number = 3\nOutLiersBox(df,df_name[feature_number])","2245e9bd":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],True)\nOutLiersBox(df_clean,df_name[feature_number])","3cd85d38":"feature_number = 4\nOutLiersBox(df,df_name[feature_number])","6889d32e":"df_clean = TurkyOutliers(df_clean,df_name[4],True)\nOutLiersBox(df_clean,df_name[4])","a1bdbf0b":"feature_number = 5\nOutLiersBox(df,df_name[feature_number])","b1b6f309":"feature_number = 5\nOutLiersBox(df,df_name[feature_number])","d72aaf96":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],True)\nOutLiersBox(df_clean,df_name[feature_number])","827217a8":"feature_number = 6\nOutLiersBox(df,df_name[feature_number])","eca92636":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],True)\nOutLiersBox(df_clean,df_name[feature_number])","ccb965fc":"feature_number = 7\nOutLiersBox(df,df_name[feature_number])","6d87873b":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],True)\nOutLiersBox(df_clean,df_name[feature_number])","5fe31ebd":"feature_number = 8\nOutLiersBox(df,df_name[feature_number])","913342c5":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],True)\nOutLiersBox(df_clean,df_name[feature_number])","a5281e35":"print('df shape: {}, new df shape: {}, we lost {} rows, {}% of our data'.format(df.shape[0],df_clean.shape[0],\n                                                              df.shape[0]-df_clean.shape[0],\n                                                        (df.shape[0]-df_clean.shape[0])\/df.shape[0]*100))","b492a66d":"tt = OutLierDetection(df,'Pregnancies','BloodPressure',.1)","ffff4e49":"tt_t = OutLierDetection(df_clean,'Pregnancies','BloodPressure',.1)","fbedaabf":"df_clean_name = df_clean.columns\nX_c =  df_clean[df_clean_name[0:8]]\nY_c = df_clean[df_clean_name[8]]\nX_train_c, X_test_c, y_train_c, y_test_c =train_test_split(X_c,Y_c,\n                                                   test_size=0.25,\n                                                   random_state=0,\n                                                   stratify=df_clean['Outcome'])","4d75d42a":"models = GetScaledModel('minmax')\nnames,results = BasedLine2(X_train_c, y_train_c,models)\nPlotBoxR().PlotResult(names,results)\n\nscaledScoreMinMax_c = ScoreDataFrame(names,results)\ncompareModels = pd.concat([basedLineScore,\n                           scaledScoreStandard,\n                          scaledScoreMinMax,\n                          scaledScoreMinMax_c], axis=1)\ncompareModels","a84a8384":"def HeatMap(df,x=True):\n        correlations = df.corr()\n        ## Create color map ranging between two colors\n        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n        fig, ax = plt.subplots(figsize=(10, 10))\n        fig = sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',square=True, linewidths=.5, annot=x, cbar_kws={\"shrink\": .75})\n        fig.set_xticklabels(fig.get_xticklabels(), rotation = 90, fontsize = 10)\n        fig.set_yticklabels(fig.get_yticklabels(), rotation = 0, fontsize = 10)\n        plt.tight_layout()\n        plt.show()\n\nHeatMap(df,x=True)","e71f18cf":"clf = ExtraTreesClassifier(n_estimators=250,\n                              random_state=SEED)\n\nclf.fit(X_train_c, y_train_c)\n\n# #############################################################################\n# Plot feature importance\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, df.columns[sorted_idx])#boston.feature_names[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","a02d16d0":"df_feature_imp=df_clean[['Glucose','BMI','Age','DiabetesPedigreeFunction','Outcome']]","dfea0b12":"df_feature_imp_name = df_feature_imp.columns","15d7be58":"X =  df_feature_imp[df_feature_imp_name[0:df_feature_imp.shape[1]-1]]\nY = df_feature_imp[df_feature_imp_name[df_feature_imp.shape[1]-1]]\nX_train_im, X_test_im, y_train_im, y_test_im =train_test_split(X,Y,\n                                                   test_size=0.1,\n                                                   random_state=0,\n                                                   stratify=df_feature_imp['Outcome'])","4aee804a":"models = GetScaledModel('minmax')\nnames,results = BasedLine2(X_train_im, y_train_im,models)\nPlotBoxR().PlotResult(names,results)\n\nscaledScoreMinMax_im = ScoreDataFrame(names,results)\ncompareModels = pd.concat([basedLineScore,\n                           scaledScoreStandard,\n                          scaledScoreMinMax,\n                          scaledScoreMinMax_c,\n                          scaledScoreMinMax_im], axis=1)\ncompareModels","025e2c62":"df_unscaled = df_clean[['Glucose','BMI','Age','DiabetesPedigreeFunction','Outcome']]\ndf_imp_scaled_name = df_unscaled.columns","588b7270":"df_imp_scaled = MinMaxScaler().fit_transform(df_unscaled)\nX =  df_imp_scaled[:,0:4]\nY =  df_imp_scaled[:,4]\nX_train_sc, X_test_sc, y_train_sc, y_test_sc =train_test_split(X,Y,\n                                                   test_size=0.1,\n                                                   random_state=0,\n                                                   stratify=df_imp_scaled[:,4])","90e23af1":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom scipy.stats import uniform","abd3c543":"class RandomSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def RandomSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = RandomizedSearchCV(self.model,\n                                 self.hyperparameters,\n                                 random_state=1,\n                                 n_iter=100,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def BestModelPridict(self,X_test):\n        \n        best_model,_ = self.RandomSearch()\n        pred = best_model.predict(X_test)\n        return pred\n    \n        ","9723afab":"class GridSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def GridSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = GridSearchCV(self.model,\n                                 self.hyperparameters,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def BestModelPridict(self,X_test):\n        \n        best_model,_ = self.GridSearch()\n        pred = best_model.predict(X_test)\n        return pred\n    \n        ","a14a60cc":"# model\nmodel = LogisticRegression()\n# Create regularization penalty space\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter distribution using uniform distribution\nC = uniform(loc=0, scale=4)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)","ceca8fff":"LR_RandSearch = RandomSearch(X_train_sc,y_train_sc,model,hyperparameters)\n# LR_best_model,LR_best_params = LR_RandSearch.RandomSearch()\nPrediction_LR = LR_RandSearch.BestModelPridict(X_test_sc)","9698d681":"def floatingDecimals(f_val, dec=3):\n        prc = \"{:.\"+str(dec)+\"f}\" #first cast decimal as str\n    #     print(prc) #str format output is {:.3f}\n        return float(prc.format(f_val))","44553326":"print('prediction on test set is:' ,floatingDecimals((y_test_sc == Prediction_LR).mean(),7))","37a67954":"model_KNN = KNeighborsClassifier()\n\nneighbors = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\nparam_grid = dict(n_neighbors=neighbors)\n","fea5572d":"KNN_GridSearch = GridSearch(X_train_sc,y_train_sc,model_KNN,param_grid)\nPrediction_KNN = KNN_GridSearch.BestModelPridict(X_test_sc)\nprint('prediction on test set is:' ,floatingDecimals((y_test_sc == Prediction_KNN).mean(),7))\n","efe19d03":"c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = [ 'linear' , 'poly' , 'rbf' , 'sigmoid' ]\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel_SVC = SVC()","92ab2824":"SVC_GridSearch = GridSearch(X_train_sc,y_train_sc,model_SVC,param_grid)\nPrediction_SVC = SVC_GridSearch.BestModelPridict(X_test_sc)\nprint('prediction on test set is:' ,floatingDecimals((y_test_sc == Prediction_SVC).mean(),7))","99108e81":"from scipy.stats import randint\nmax_depth_value = [3, None]\nmax_features_value =  randint(1, 4)\nmin_samples_leaf_value = randint(1, 4)\ncriterion_value = [\"gini\", \"entropy\"]","f82ece2e":"param_grid = dict(max_depth = max_depth_value,\n                  max_features = max_features_value,\n                  min_samples_leaf = min_samples_leaf_value,\n                  criterion = criterion_value)","d657941f":"model_CART = DecisionTreeClassifier()\nCART_RandSearch = RandomSearch(X_train_sc,y_train_sc,model_CART,param_grid)\nPrediction_CART = CART_RandSearch.BestModelPridict(X_test_sc)\nprint('prediction on test set is:' ,floatingDecimals((y_test_sc == Prediction_CART).mean(),7))","24b0b6bb":"learning_rate_value = [.01,.05,.1,.5,1]\nn_estimators_value = [50,100,150,200,250,300]\n\nparam_grid = dict(learning_rate=learning_rate_value, n_estimators=n_estimators_value)","362f7ca1":"model_Ad = AdaBoostClassifier()\nAd_GridSearch = GridSearch(X_train_sc,y_train_sc,model_Ad,param_grid)\nPrediction_Ad = Ad_GridSearch.BestModelPridict(X_test_sc)\nprint('prediction on test set is:' ,floatingDecimals((y_test_sc == Prediction_Ad).mean(),7))\n","b65ed6f8":"learning_rate_value = [.01,.05,.1,.5,1]\nn_estimators_value = [50,100,150,200,250,300]\n\nparam_grid = dict(learning_rate=learning_rate_value, n_estimators=n_estimators_value)","90ae2872":"model_GB = GradientBoostingClassifier()\nGB_GridSearch = GridSearch(X_train_sc,y_train_sc,model_GB,param_grid)\nPrediction_GB = GB_GridSearch.BestModelPridict(X_test_sc)\nprint('prediction on test set is:' ,floatingDecimals((y_test_sc == Prediction_GB).mean(),7))","31b3a51c":"from sklearn.ensemble import VotingClassifier","dc424abc":"param = {'C': 0.7678243129497218, 'penalty': 'l1'}\nmodel1 = LogisticRegression(**param)\n\nparam = {'n_neighbors': 15}\nmodel2 = KNeighborsClassifier(**param)\n\nparam = {'C': 1.7, 'kernel': 'linear'}\nmodel3 = SVC(**param)\n\nparam = {'criterion': 'gini', 'max_depth': 3, 'max_features': 2, 'min_samples_leaf': 3}\nmodel4 = DecisionTreeClassifier(**param)\n\nparam = {'learning_rate': 0.05, 'n_estimators': 150}\nmodel5 = AdaBoostClassifier(**param)\n\nparam = {'learning_rate': 0.01, 'n_estimators': 100}\nmodel6 = GradientBoostingClassifier(**param)\n\nmodel7 = GaussianNB()\n\nmodel8 = RandomForestClassifier()\n\nmodel9 = ExtraTreesClassifier()","ea72b11c":"# create the sub models\nestimators = [('LR',model1), ('KNN',model2), ('SVC',model3),\n              ('DT',model4), ('ADa',model5), ('GB',model6),\n              ('NB',model7), ('RF',model8),  ('ET',model9)]\n    ","a97af24e":"# create the ensemble model\nkfold = StratifiedKFold(n_splits=10, random_state=SEED)\nensemble = VotingClassifier(estimators)\nresults = cross_val_score(ensemble, X_train_sc,y_train_sc, cv=kfold)\nprint('Accuracy on train: ',results.mean())\nensemble_model = ensemble.fit(X_train_sc,y_train_sc)\npred = ensemble_model.predict(X_test_sc)\nprint('Accuracy on test:' , (y_test_sc == pred).mean())","ff0e990b":"def get_models():\n    \"\"\"Generate a library of base learners.\"\"\"\n    param = {'C': 0.7678243129497218, 'penalty': 'l1'}\n    model1 = LogisticRegression(**param)\n\n    param = {'n_neighbors': 15}\n    model2 = KNeighborsClassifier(**param)\n\n    param = {'C': 1.7, 'kernel': 'linear', 'probability':True}\n    model3 = SVC(**param)\n\n    param = {'criterion': 'gini', 'max_depth': 3, 'max_features': 2, 'min_samples_leaf': 3}\n    model4 = DecisionTreeClassifier(**param)\n\n    param = {'learning_rate': 0.05, 'n_estimators': 150}\n    model5 = AdaBoostClassifier(**param)\n\n    param = {'learning_rate': 0.01, 'n_estimators': 100}\n    model6 = GradientBoostingClassifier(**param)\n\n    model7 = GaussianNB()\n\n    model8 = RandomForestClassifier()\n\n    model9 = ExtraTreesClassifier()\n\n    models = {'LR':model1, 'KNN':model2, 'SVC':model3,\n              'DT':model4, 'ADa':model5, 'GB':model6,\n              'NB':model7, 'RF':model8,  'ET':model9\n              }\n\n    return models","ea5e4be4":"def train_predict(model_list,xtrain, xtest, ytrain, ytest):\n    \"\"\"Fit models in list on training set and return preds\"\"\"\n    P = np.zeros((ytest.shape[0], len(model_list)))\n    P = pd.DataFrame(P)\n\n    print(\"Fitting models.\")\n    cols = list()\n    for i, (name, m) in enumerate(models.items()):\n        print(\"%s...\" % name, end=\" \", flush=False)\n        m.fit(xtrain, ytrain)\n        P.iloc[:, i] = m.predict_proba(xtest)[:, 1]\n        cols.append(name)\n        print(\"done\")\n\n    P.columns = cols\n    print(\"Done.\\n\")\n    return P","620c339a":"models = get_models()\nP = train_predict(models,X_train_sc,X_test_sc,y_train_sc,y_test_sc)","bb770185":"from mlens.visualization import corrmat\n\ncorrmat(P.corr(), inflate=False)\n","fb69e330":"corrmat(P.apply(lambda predic: 1*(predic >= 0.5) - y_test_sc).corr(), inflate=False)","febcc37b":"base_learners = get_models()\nmeta_learner = GradientBoostingClassifier(\n    n_estimators=1000,\n    loss=\"exponential\",\n    max_features=6,\n    max_depth=3,\n    subsample=0.5,\n    learning_rate=0.001, \n    random_state=SEED\n)","5447bbe6":"from mlens.ensemble import SuperLearner\n\n# Instantiate the ensemble with 10 folds\nsl = SuperLearner(\n    folds=10,\n    random_state=SEED,\n    verbose=2,\n    backend=\"multiprocessing\"\n)\n\n# Add the base learners and the meta learner\nsl.add(list(base_learners.values()), proba=True) \nsl.add_meta(meta_learner, proba=True)\n\n# Train the ensemble\nsl.fit(X_train_sc, y_train_sc)\n\n# Predict the test set\np_sl = sl.predict_proba(X_test_sc)\n\n# print(\"\\nSuper Learner ROC-AUC score: %.3f\" % roc_auc_score(y_test_sc, p_sl[:, 1]))\n","8e87a647":"pp = []\nfor p in p_sl[:, 1]:\n    if p>0.5:\n        pp.append(1.)\n    else:\n        pp.append(0.)","da1de0a8":"print(\"\\nSuper Learner Accuracy score: %.8f\" % (y_test_sc== pp).mean())","9c5f88fa":"## 5.7. RandomForest\n**Toturial: Tune RandommForest**\n## 5.8. ExtraTrees\n**Toturial: Tune ExtraTrees**","6e092b3b":"### Feature 5","50dabac9":"## 2.1. Analyze Data:\n### 2.1.1 Descriptive Statitics","a393fadd":"## 4.3. Feature Selection\nFeature selection is also called variable selection or attribute selection.\nIt is the automatic selection of attributes in your data (such as columns in tabular data) that are most relevant to the predictive modeling problem you are working on.\n\nFeature selection methods aid you in your mission to create an accurate predictive model. They help you by choosing features that will give you as good or better accuracy whilst requiring less data.\n\nFeature selection methods can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model.","1220f266":"### Feature 2","4593b584":"# 6.3 Stacking","dfbc352f":"Errors are significantly correlated, which is to be expected for models that perform well, since it's typically the outliers that are hard to get right. In fact, if we look at error correlations on a class prediction basis things look a bit more promising:","d4feeffb":"## Spot-Check Algorithms","1bfb66fa":"## Supervised Machine Learning\n1. Regression: Linear Regression, Logistic Regression\n\n2. Instance-based Algorithms: k-Nearest Neighbor (KNN)\n\n3. Decision Tree Algorithms: CART\n\n4. Bayesian Algorithms: Naive Bayes\n\n5. Ensemble Algorithms: eXtreme Gradient Boosting\n\n6. Deep Learning Algorithms: Convolution Neural Network","0502b687":"Sometime saling help to improve the prediction of tree based model so let's see","60de20b7":"### 2.2. Data Visualization\nlet's take look at our data in the most raw shape.\nI really recommend scatter plot because we can get the idea of our data without any manipulation","3db8d0d5":"# If the kernel is useful, Please UpVote","4b8a2b7e":"# 5. Algortithm Tuning","3a5d847c":"## 2.3.1 Outliers investigation","a0af2aa1":"We still could improve the prediction ","68837252":"## 4.2. Remove Outliers\n\nlet's remove outliers and see how it effects on the prediction","2f4d0de0":"It seems that the data suffer from outliers\n\nLet's see for example pregnency distribution","82ac86e1":"# 3.1. Evaluate Algorithms: Baseline","05264886":"## 5.4. Decision Tree\n- max_depth:  Maximum depth of the tree (double).\n- row_subsample: Proportion of observations to consider (double).\n- max_features: Proportion of columns (features) to consider in each level (double).","36be79bd":"## 5.1.Logistic Regression\n- C : Regularization value, the more, the stronger the regularization(double). \n- RegularizationType: Can be either \"L2\" or \u201cL1\u201d. Default is \u201cL2\u201d. ","6af02f17":"### Feature 3","aec49251":"## 5.2. KNN\n- n_neighbors: Number of neighbors to use by default for k_neighbors queries","bc6f32ac":"## Machine Learning Pipeline:\n\n- Define Problem\n    - ML type of problem\n\n- Prepare Data\n    - Data Visualization methos ...\n    - Data Selection\n    - Feature Selection methods ..\n    - Feature Engineering methods ..\n    - Data Transormation methods ..\n\n- Spot Check Algorithm\n    - Test Harness ...\n    - Perform Measure ...\n    - Evaluate accuracy of different algorithms\n\n- Improve Results \n    - Algorithms Turning methids\n    - ensemble methods\n\n- Present Results\n    - Save the model","1ca37fca":"There is not highly corrolated feature in this data set.","975b99ba":"## Types of Machine learning Algorithms\n\n1. Supervised Learning: Input data is called training data and\nhas a known label or result.\nEx: Spam\/not-spam or a stock price at a time.\n\n2. Unsupervised Learning: Input data is not labeled and does\nnot have a known result.\nEX: Grouping customers by purchasing behavior\n\n3. Semi-Supervised Learning: Input data is a mixture of labeled and\nunlabeled examples.\nEX: a photo archive where only some of the\nimages are labeled, (e.g. dog, cat, person) and the majority are\nunlabeled.\n\n4. Reinforcement Learning: a goal-oriented learning based\non interaction with environment. Autonomous cars.","ddc06377":"# Feature Engineering\n## 4.1. Data Preprocessig","edf447bd":"### 4.3.1. Corrolation\n\nIf we fit highly corrolated data in our model, it results in the overfitting probelm. Thus, for example if there are two highly corrolated features we have to drop the one that has more corrolation with other feature.  ","ba000f58":"## 6.2. Error Corrolation","7720a258":"## 4.3.2.Feature Importance\nBagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.","ab0ca633":"## 5.5 AdaBoostClassifier\n- learning_rate: Learning rate shrinks the contribution of each classifier by learning_rate.\n- n_estimators: Number of trees to build.","211f5ec3":"**Attribute Information:**\n\n1. Number of times pregnant \n\n2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n\n3. Diastolic blood pressure (mm Hg) \n\n4. Triceps skin fold thickness (mm) \n\n5. 2-Hour serum insulin (mu U\/ml) \n\n6. Body mass index (weight in kg\/(height in m)^2) \n\n7. Diabetes pedigree function \n\n8. Age (years) \n\n9. Class variable (0 or 1)","b55df894":"let's make train-validation and test data sets.\n- Note that stratify is used becasue we want to keep the train and test distribution ","5adb3ad4":"## 6.1 Voting Ensemble\n\nVoting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. It works by first creating two or more standalone models from your training dataset.\nA Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.","3e4d67e0":"## Classification vs Regression\n**Classification** predicting a label .vs. **Regression** predicting a quantity.","a6bb2c9a":"As we can see, the standarscaler and min and max effect on non tree models and the prediction results improve ","d03b063e":"In the case of classification problem we alwyas need to check the target distribution.\nIf the distribution of target is not balance, we must treat our data more carefully.\nFor example we can use several methods to resampling our data. We do not resampling our data in this kernel. In addition, we need to use stratified method in our validation in order to keep the same distribution in our train and test.   ","320c5b52":"## What is machine learning?\nTask **T**: image classification problem of classifying dogs and cats \n\nExperience **E**: I would give a ML algorithm a\nbunch of images of dogs and cats\n\nThe performance measure **P**: the ML algorithm could learn\nhow to distinguish a new image as being either a dog or cat.\n\nMachine learning (Machine Learning by Tom Mitchell):\n\n**A computer program is said to learn from experience **E** with\nrespect to some class of tasks **T** and performance measure **P**,\nif its performance at tasks in **T**, as measured by **P**, Improves\nwith experience **E**.**","a8ea7453":"## Comparing the accuracy of models after cleaning","e7099398":"## Beyond Voting","e7e29cd3":"### Feature 6","01894185":"### Feature 1","111f3042":"- We usually devide the data to train and test set. We will not touch test set until the end of the computation and the final perpormance evaluation. Then, we can devide the train set to train and validation sets. We use the validation data set to tune the model. \n\n- Traditional train test method suffer from high variance test problem. It means tha by changing the test set the result of the prediction changes. To over come this problem we use k-fold validation method in our train and validation set","c2a0db17":"- The data is not included time or object\n- There is no null value in data set","db484c10":"# Happy Kaggling!","d6334c04":"# Define Problem:\n## Pima Indians Diabetes Database\n### Predict the onset of diabetes based on diagnostic measures\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.","b330db9a":"# Conclusion:\n- We could reach 86 % accuracy\n- Now you can do more feature engineering \n- Tune better \n- Use XGBoost, LightGBM, Nueal Network and other boosting method to improve your result\n- Stack more diverse model\n- Use your creativity \n- Your comments is warmly welcome\n> - Thank you, it there is a misstke Please let me know","0e5427d7":"<a href=\"https:\/\/ibb.co\/bWRCsS\"><img src=\"https:\/\/preview.ibb.co\/f7UAe7\/validation.png\" alt=\"validation\" border=\"0\" \/><\/a>","ecd78dae":"### Feature 7","627a55d3":"Let's investagate each fearure distribution for each out come\n\nA fundamental task in many statistical analyses is to characterize\nthe location and variability of a data set. A further\ncharacterization of the data includes **skewness and kurtosis**.\n\n**Skewness** is a measure of symmetry, or more precisely, the lack\nof symmetry. A distribution, or data set, is symmetric if it\nlooks the same to the left and right of the center point.\n\n**Kurtosis** is a measure of whether the data are heavy-tailed\nor light-tailed relative to a normal distribution. That is,\ndata sets with high kurtosis tend to have heavy tails, or\noutliers. Data sets with low kurtosis tend to have light\ntails, or lack of outliers. A uniform distribution would\nbe the extreme case","322548bb":"### 4.1.2. Standard","60cc02dd":"# A complete Machine Learning PipeLine\n\n- Intoroduction to supervised Machine Learning\n    - What is machine learning?\n    - Types of Machine learning Algorithms\n    - Supervised Machine Learning\n    - Classification vs Regression\n    - Machine Learning Pipeline\n    \n    \n- Predict the onset of diabetes based on diagnostic measures\n\n    - Data Preparing\n        - Analyze Data\n        - Descriptive Statitics\n        - Data Visualization\n    - Outliers investigation\n        - Outliers investigation Single Feature\n        - Outliers investigation Pairs\n        \n    - Evaluate Algorithms: Baseline\n    - Feature Engineering \n        - Data Preprocessig\n            - Standard Scaler\n            - MinMax Scaler\n        - Remove OutlierS\n    - Feature Selection\n        - Corrolation\n        - Feature Importance\n    - Ensemble Methods    \n        - Algortithm Tuning\n        - Voting Ensemble\n        - Error Corrolation\n        - Stacking\n \n -  Conclusion (Acuuracy 86%)\n    \n    \n","71d2903b":"<img src=\"https:\/\/image.ibb.co\/bW0oXS\/kfold.png\" alt=\"kfold\" border=\"0\" \/>","3767969f":"## Classification Algorithms Examples:\n- Linear: Linear Regression, Logistic Regression\n- Nonlinear: Trees, k-Nearest Neighbors\n- Ensemble:\n    - Bagging: Random Forest\n    - Boosting:  AdaBoost\n","a05fd2f8":"It can be seen that the prediction is improving","6722f95a":"### Feature 8 ","f85d9f9d":"** Now is your turn you can delete the outlier in pair plot to see how the result change and share your result in comment.**","266776c1":" # 6. Ensemble Methods","fbcbd688":"### Feature 4","4fd649f0":"## 5.6 GradientBoosting","e81a0ca1":"## 2.3.2. Outliers investigation Pairs","22ac8e78":"** Cleaning Report **","18c019f0":"<img src=\"https:\/\/image.ibb.co\/cQh2sS\/1_ZTl_Qm_WRcr_Nq_L_n_Lnx6_GJA.png\" alt=\"1 ZTl Qm WRcr Nq L n Lnx6 GJA\" border=\"0\" \/>","0aff2779":"# Intoroduction to supervised Machine Learning:","f6d3a8da":"### Feature 0","fb62c56b":"## 5.3. SVC \n- C: The Penalty parameter C of the error term. \n- Kernel: Kernel type could be linear, poly, rbf or sigmoid.","d8e1104a":"### 4.1.2. MinMax","ce549843":"Numerical features preprocessing is different for tree and non tree model.\n\n1) Usually:\n- Tree based models does not depend on scaling\n- Non-tree based models hugely depend on scaling \n\n2) Most Often used preprocening are:\n- MinMax scaler to [0,1]\n- Standard Scaler to mean = 0 and std =1\n- Rank (We do not work on it in this data set)\n- Using np.log(1+data),  np.sqrt(data) and stats.boxcox(data) (for exp dependency)\n\nlet's try some of them and see how our model prediction change by scalling\n"}}