{"cell_type":{"185bc7e5":"code","eb3063ad":"code","00841172":"code","be8523e3":"code","5801575e":"code","cded8a1a":"code","9535024b":"code","f4c3abc2":"code","081a5659":"code","6cb9f341":"code","bd9f6e1a":"code","74c5c7e5":"code","12b61064":"code","35c51f35":"code","fad37ab4":"code","02b8dc64":"code","b11c24b8":"code","71a1c5f9":"code","beb6c74b":"code","7d2f54c7":"code","d60895e7":"code","eb1ddf4e":"code","d55122f2":"code","da4caa60":"code","66b74036":"code","69d1ff4b":"code","9f2e2d14":"code","5b3edfcb":"code","3d90e661":"code","d8cde058":"code","cbac3811":"code","c45ed77a":"code","95a312e3":"code","0b5aec67":"code","cdbfd57e":"code","b5bc6c45":"code","ce7fd047":"code","6670300a":"code","1f6f13ba":"code","cbc44a72":"code","dfe2f58a":"markdown","e4aa4df6":"markdown","4adcc6d2":"markdown","f8cfe2a8":"markdown","6890ecf3":"markdown","2ee45be7":"markdown","e1aa5ff5":"markdown","a935129c":"markdown","b70e48be":"markdown","e36d9b72":"markdown","90aa439a":"markdown","b1a00450":"markdown","72e9596e":"markdown","f912c361":"markdown","4d97c2ed":"markdown","78d11b80":"markdown","b382c2dd":"markdown","e678f713":"markdown","bc5e1769":"markdown","b3a7a48e":"markdown","7dcfd5cc":"markdown","2655f428":"markdown","38e8f5a9":"markdown","b9055bc9":"markdown","e623f61e":"markdown","5c10575e":"markdown","64fb15f9":"markdown","0107238b":"markdown","5ff5539b":"markdown","556a9914":"markdown","5dc86bed":"markdown"},"source":{"185bc7e5":"from sklearn.impute import SimpleImputer, KNNImputer\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nseed = 47","eb3063ad":"pdf_train = pd.read_csv('\/kaggle\/input\/bigtarget\/pdf (1).csv')\npdf = pdf_train.copy()\npdf.shape","00841172":"pdf.head()","be8523e3":"gender_map = {'\u0416': 0, '\u041c': 1}\ngroup_map = {'test': 0, 'control': 1}\npdf['gender'] = pdf['gender'].map(gender_map)\npdf['group'] = pdf['group'].map(group_map)","5801575e":"print('Number NA:', pdf.isna().sum().sum())\nprint('Amount of all values:', pdf.shape[0]*pdf.shape[1])\nprint('Missed Data Percentage:', round(100*pdf.isna().sum().sum()\/(pdf.shape[0]*pdf.shape[1]), 2), '%')","cded8a1a":"#missing data\ntotal = pdf.isnull().sum().sort_values(ascending=False)\npercent = (pdf.isnull().sum()\/pdf.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","9535024b":"#dealing with missing data\npdf = pdf.fillna(np.nan)\npdf_col_list = list(pdf.columns)\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\npdf = pd.DataFrame(imputer.fit_transform(pdf), columns=pdf_col_list)\n\nprint('Number NA:', pdf.isna().sum().sum())","f4c3abc2":"# Group number is in range from 20 to 79\nall_groups = [i for i in range(20, 80)]\npdf_outliers = pdf.copy()\n\n# This function returns columns from 'columns' list, which are met in columns of \"data\"\ndef get_columns_list(data, columns):\n    return data.columns[data.columns.isin(columns)]","081a5659":"collect_features = {\n    'cheque_count_12m_max'             : (get_columns_list(pdf_outliers, ['cheque_count_12m_g{}'.format(i) for i in all_groups]), 'sum'),\n    'children'                         : (['children'], 'max'),\n    'crazy_purchases_cheque_count_12m' : (['crazy_purchases_cheque_count_12m'], 'max'),\n    'k_var_disc_share_6m_max'          : (get_columns_list(pdf_outliers, ['k_var_disc_share_6m_g{}'.format(i) for i in all_groups]), 'max'),\n    'k_var_sku_price_6m_max'           : (get_columns_list(pdf_outliers, ['k_var_sku_price_6m_g{}'.format(i) for i in all_groups]), 'max'),\n    'sale_sum_12m_sum'                  : (get_columns_list(pdf_outliers, ['sale_sum_6m_g{}'.format(i) for i in all_groups]), 'sum'),\n}","6cb9f341":"for key in collect_features.keys():\n    method = collect_features[key][1]\n    \n    if method == 'max':\n        pdf_outliers.loc[:, key] = pdf_outliers[collect_features[key][0]].max(axis=1)\n    elif method == 'sum':\n        pdf_outliers.loc[:, key] = pdf_outliers[collect_features[key][0]].sum(axis=1)\n    elif method == 'max':\n        pdf_outliers.loc[:, key] = pdf_outliers[collect_features[key][0]].max(axis=1)\n    elif method == 'min':\n        pdf_outliers.loc[:, key] = pdf_outliers[collect_features[key][0]].min(axis=1)","bd9f6e1a":"for key in collect_features.keys():\n    print('Column', key)\n    \n    dataframe = {'Type' : ['test_0', 'test_1', 'control_0', 'control_1']}\n    for whis in [1.5, 2, 2.5, 3]:\n        IQR = pdf_outliers[key].quantile(0.75) - pdf_outliers[key].quantile(0.25)\n        sample = pdf_outliers[pdf_outliers[key] <= pdf_outliers[key].quantile(0.75) + IQR * whis]\n        sample_test = sample[sample['group'] == 0]['response_att'].value_counts()\n        sample_control = sample[sample['group'] == 1]['response_att'].value_counts()\n        \n        dataframe['whis {}'.format(whis)] = [sample_test[0], sample_test[1], sample_control[0], sample_control[1]]\n    \n    print(pd.DataFrame(dataframe))\n    sns.boxplot(pdf_outliers[key])\n    plt.show()","74c5c7e5":"for key in collect_features.keys():\n    print('Column', key)\n    \n    dataframe = {'Type' : ['test_0', 'test_1', 'control_0', 'control_1']}\n    for whis in [1.5, 2, 2.5, 3]:\n        IQR = pdf_outliers[key].quantile(0.75) - pdf_outliers[key].quantile(0.25)\n        sample = pdf_outliers[pdf_outliers[key] > pdf_outliers[key].quantile(0.75) + IQR * whis]\n        sample_test = sample[sample['group'] == 0]['response_att'].value_counts()\n        sample_control = sample[sample['group'] == 1]['response_att'].value_counts()\n        \n        dataframe['whis {}'.format(whis)] = [sample_test[0], sample_test[1], sample_control[0], sample_control[1]]\n    \n    print(pd.DataFrame(dataframe))\n    sns.boxplot(pdf_outliers[key])\n    plt.show()","12b61064":"sns.heatmap(pdf_outliers[[key for key in collect_features.keys()]].corr(), annot=True)","35c51f35":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, a, b):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[a:b]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(pdf, 0, 10))","fad37ab4":"sns.countplot(x=\"group\", data=pdf)","02b8dc64":"test = pdf[pdf['group'] == 0]\n\ncontrol = pdf[pdf['group'] == 1]\n\nsns.distplot(test['response_sms'], label='test')\nsns.distplot(control['response_sms'], label='control')\nplt.title('Test & Control response SMS Distribution')\nplt.legend()\nplt.show()","b11c24b8":"test = pdf[pdf['group'] == 0]\n\ncontrol = pdf[pdf['group'] == 1]\n\nsns.distplot(test['age'], label='test')\nsns.distplot(control['age'], label='control')\nplt.title('Test & Control age Distribution')\nplt.legend()\nplt.show()","71a1c5f9":"# Test participants who didn't take part in event\nstubborn_test = pdf[pdf['group'] == 0]\nstubborn_test = stubborn_test[stubborn_test['response_att'] == 0]\n\n# Test participants who took part in event\nactive_test = pdf[pdf['group'] == 0]\nactive_test = active_test[active_test['response_att'] == 1]","beb6c74b":"# Control participants who didn't take part in event\nstubborn_control = pdf[pdf['group'] == 1]\nstubborn_control = stubborn_control[stubborn_control['response_att'] == 0]\n\n# Control participants who took part in event\nactive_control = pdf[pdf['group'] == 1]\nactive_control = active_control[active_control['response_att'] == 1]","7d2f54c7":"sales_sum = ['sale_sum_3m_g{}'.format(i) for i in [24, 26, 32, 33]]\n\nf, axs = plt.subplots(2, 2, figsize=(10, 10))\n\nsns.scatterplot(active_test[sales_sum].sum(axis=1), active_test['response_sms'], ax=axs[0, 0])\nsns.distplot(active_test['response_sms'], ax=axs[0, 1])\naxs[0, 1].set_ylabel('Test group')\n\nsns.scatterplot(active_control[sales_sum].sum(axis=1), active_control['response_sms'], ax=axs[1, 0])\nsns.distplot(active_control['response_sms'], ax=axs[1, 1])\naxs[1, 1].set_ylabel('Control group')","d60895e7":"sales_sum = ['sale_sum_3m_g{}'.format(i) for i in [24, 26, 32, 33]]\n\nf, axs = plt.subplots(2, 2, figsize=(10, 10))\n\nsns.scatterplot(stubborn_test[sales_sum].sum(axis=1), stubborn_test['response_sms'], ax=axs[0, 0])\nsns.distplot(stubborn_test['response_sms'], ax=axs[0, 1])\naxs[0, 1].set_ylabel('Test group')\n\nsns.scatterplot(stubborn_control[sales_sum].sum(axis=1), stubborn_control['response_sms'], ax=axs[1, 0])\nsns.distplot(stubborn_control['response_sms'], ax=axs[1, 1])\naxs[1, 1].set_ylabel('Control group')","eb1ddf4e":"sns.barplot(x = 'group', y='response_att', data=pdf)","d55122f2":"!pip install scikit-uplift -q\n\nfrom sklearn.model_selection import train_test_split\nfrom sklift.models import ClassTransformation\nfrom sklift.viz.base import plot_uplift_by_percentile\nfrom sklift.metrics import uplift_at_k, uplift_auc_score, qini_auc_score\nfrom sklearn.base import clone\n\nfrom catboost import CatBoostClassifier\n\nseed=47","da4caa60":"gen_dict = {\n    '\u0416':0,\n    '\u041c':1,\n    '\u041d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d': -1\n}\n\ntreat_dict = {\n    'test': 1,\n    'control': 0\n}\n\npdf_train.loc[:, 'gender'] = pdf_train['gender'].map(gen_dict)\npdf_train.loc[:, 'group'] = pdf_train['group'].map(treat_dict)\n\ndrop_cols = ['CardHolder', 'group', 'response_att']\ntry_feat = list(set(pdf_train.columns.tolist()) - set(drop_cols))","66b74036":"estimator = CatBoostClassifier(verbose=100, random_state=seed)\n\nuplift_model_cl_tr = ClassTransformation(\n    estimator=estimator\n)","69d1ff4b":"df_tr, df_val = train_test_split(\n    pdf_train, \n    test_size=0.3, \n    stratify=pdf['response_att'], \n    random_state=seed\n)","9f2e2d14":"model = clone(uplift_model_cl_tr)\n\nmodel.fit(\n    X=df_tr[try_feat], \n    y=df_tr['response_att'], \n    treatment=df_tr['group'])\n\npred_tr = model.predict(df_tr[try_feat])\npred_val = model.predict(df_val[try_feat])","5b3edfcb":"def custom_metric(\n    y_true, \n    uplift, \n    treatment,\n    take_top_ratio=0.25\n):\n    answers = pd.DataFrame(data={\n        'response_att':y_true,\n        'uplift': uplift,\n        'group': treatment\n    })\n    \n    answers.sort_values(by='uplift', inplace=True, ascending=False)\n    n_samples = int(np.ceil(answers.shape[0] * take_top_ratio))\n    answers = answers.iloc[:n_samples, :]\n    answers_test = answers[answers['group'] == 1]['response_att'].sum() \/ \\\n                   answers[answers['group'] == 1].shape[0]\n    answers_control = answers[answers['group'] == 0]['response_att'].sum() \/ \\\n                      answers[answers['group'] == 0].shape[0]\n    return (answers_test - answers_control)\n\n\ndef custom_metric_orig(answers, take_top_ratio=0.25):\n    answers.sort_values(by='uplift', inplace=True, ascending=False)\n    n_samples = int(np.ceil(answers.shape[0] * take_top_ratio))\n    answers = answers.iloc[:n_samples, :]\n    answers_test = answers[answers['group'] == 'test']['response_att'].sum() \/ \\\n                   answers[answers['group'] == 'test'].shape[0]\n    answers_control = answers[answers['group'] == 'control']['response_att'].sum() \/ \\\n                      answers[answers['group'] == 'control'].shape[0]\n    return (answers_test - answers_control)*100","3d90e661":"def print_metrics(df_tr, df_val, pred_tr, pred_val):\n    \n    uplift_tr_cust = custom_metric(\n        y_true=df_tr['response_att'], \n        uplift=pred_tr, \n        treatment=df_tr['group']\n)\n\n    uplift_val_cust = custom_metric(\n        y_true=df_val['response_att'], \n        uplift=pred_val, \n        treatment=df_val['group']\n    )\n\n    uplift_tr = uplift_at_k(\n        y_true=df_tr['response_att'], \n        uplift=pred_tr, \n        treatment=df_tr['group'],\n        strategy='overall',\n        k=0.25\n    )\n    \n    uplift_val = uplift_at_k(\n        y_true=df_val['response_att'], \n        uplift=pred_val, \n        treatment=df_val['group'],\n        strategy='overall',\n        k=0.25\n    )\n\n    print(f'Uplift train, custom: {uplift_tr_cust:.5f}')\n    print(f'Uplift val,   custom: {uplift_val_cust:.5f}\\n')\n\n    print(f'Uplift train: {uplift_tr:.5f}')\n    print(f'Uplift val:   {uplift_val:.5f}')","d8cde058":"print_metrics(\n    df_tr, df_val, \n    pred_tr, pred_val)","cbac3811":"plt.figure(figsize=(20, 8))\npd.Series(pred_tr).hist(bins=100, color='red')\npd.Series(pred_val).hist(bins=100, color='blue')","c45ed77a":"plot_uplift_by_percentile(\n    y_true=df_val['response_att'], \n    uplift=pred_val, \n    treatment=df_val['group'],\n    bins=4,\n    kind='bar'\n)","95a312e3":"model.estimator.get_feature_importance(prettified=True).head(10)","0b5aec67":"pdf['age_group'] = pdf['age'].apply(lambda x: '0-12' if x<12\n                                   else '13-18' if x<18 and x>12\n                                   else '19-35' if x<35 and x>18\n                                   else '36-60' if x<60 and x>35\n                                   else '60+')","cdbfd57e":"sns.countplot(x=\"age_group\", data=pdf)","b5bc6c45":"sns.countplot(x=\"children\", data=pdf)\nplt.show()","ce7fd047":"sns.barplot(x=\"children\", y=\"food_share_1m\", hue=\"gender\", data=pdf)","6670300a":"sns.barplot(x=\"age_group\", y=\"crazy_purchases_goods_count_6m\", data=pdf)","1f6f13ba":"for g in pdf['age_group'].unique():\n    temp = pdf[pdf['age_group']==g]\n    sns.distplot(temp['response_viber'])\n    plt.title(g + ' viber')\n    plt.show()\n    sns.distplot(temp['response_sms'])\n    plt.title(g + ' sms')\n    plt.show()","cbc44a72":"sns.barplot(x=\"age_group\", y=\"stdev_days_between_visits_15d\", data=pdf)","dfe2f58a":"We observe a **lower right peak** which corresponds to the probability of response to SMS 100% for the control sample.","e4aa4df6":"### Create model","4adcc6d2":"There are also few categorical columns: **gender** and **group**. We transform them into binary features.","f8cfe2a8":"![](https:\/\/kaskad-asu.com\/images\/customers\/x5.jpg)\n# BIGTARGET Hackathon from LENTA and Microsoft\n\n**by team GORNYAKI (Samoshin Andriy and Tsepa Oleksii[Ukraine, KPI, IASA])**\n\nParticipants had to develop a solution that improves the effectiveness of SMS targeting in such a way that they only send messages to customers who are motivated to make a purchase. You must use the data provided by the retailer.\n\n> Note: the organizers conducted a small features selection because the started data contained **more than 1000 columns**.","6890ecf3":"## Response SMS & Viber","2ee45be7":"Among the audience up to 35 years old a fairly small response through SMS and Viber is observed. Perhaps these people use these communication channels for other needs.\n\n**Solution of the problem:** to explore other markets for communication with the user, for example other messengers. In addition, in this way it is possible to reduce the number of sent SMS for this age group and significantly save the company\u2019s budget.","e1aa5ff5":"## Comparison on some features\n\nWe will divide customers into 2 classes depending on whether they participated in the promotion.","a935129c":"## Conclusion\n\nIt's all. We hope you find this dataset analysis useful. If you have new ideas for insights write about them in the comments.","b70e48be":"# Test vs Control","e36d9b72":"You can look at plot and among the age category 13-35 years activity in participating in 'crazy promotions'  is lower. And part of this audience is **almost 1\/3 of the total**.\n\n**Option for better involvement:** add more relevant products to these age categories in promotional offers.","90aa439a":"In the interaction  proportion of customers who completed the target action is **11.01%**. Without interaction, **10.25%**. So the effect of the mailing **increases this share by 0.75%**.","b1a00450":"## Crazy Purchases","72e9596e":"# Insights\n\nThis item is necessary for a clear interpretation of our model in order to show the organizers how to increase the company's metrics. We will describe the main ones that we found and their solutions.\n> For a more convenient search we will group by age categories.","f912c361":"# About Uplift Modeling\n\nUplift modeling estimates the effect of communication action on some customer outcome and gives an opportunity to efficiently target customers which are most likely to respond to a marketing campaign. It is relatively easy to implement, but surprisingly poorly covered in the machine learning courses and literature.\n\nCompanies use various channels to promote a product to a customer: it can be SMS, push notification, chatbot message in social networks, and many others. There are several ways to use machine learning to select customers for a marketing campaign:\n\n![](https:\/\/scikit-uplift.readthedocs.io\/en\/latest\/_images\/ug_comparison_with_other_models.png)\n\n* **The Look-alike model** (or Positive Unlabeled Learning) evaluates a probability that the customer is going to accomplish a target action. A training dataset contains known positive objects (for instance, users who have installed an app) and random negative objects (a random subset of all other customers who have not installed the app). The model searches for customers who are similar to those who made the target action.\n\n* **The Response model** evaluates the probability that the customer is going to accomplish the target action if there was a communication (a.k.a treatment). In this case the training dataset is data collected after some interaction with the customers. In contrast to the first approach, we have confirmed positive and negative observations at our disposal (for instance, the customer who decides to issue a credit card or to decline an offer).\n\n* **The Uplift model** evaluates the net effect of communication by trying to select only those customers who are going to perform the target action only when there is some advertising exposure presenting to them. The model predicts a difference between the customer\u2019s behavior when there is a treatment (communication) and when there is no treatment (no communication).\n\n> You can find more theory in the [documentation](https:\/\/scikit-uplift.readthedocs.io\/en\/latest\/user_guide\/introduction\/cate.html).\n\n\n## Types of customers\n\nWe can determine 4 types of customers based on a response to a treatment:\n\n![](https:\/\/scikit-uplift.readthedocs.io\/en\/latest\/_images\/ug_clients_types.jpg)\n\n* **Do-Not-Disturbs** (a.k.a. Sleeping-dogs) have a strong negative response to a marketing communication. They are going to purchase if NOT treated and will NOT purchase IF treated. It is not only a wasted marketing budget but also a negative impact.\n\n* **Lost Causes** will NOT purchase the product NO MATTER they are contacted or not. The marketing budget in this case is also wasted because it has no effect.\n\n* **Sure Things** will purchase ANYWAY no matter they are contacted or not. There is no motivation to spend the budget because it also has no effect.\n\n* **Persuadables** will always respond POSITIVE to the marketing communication. They is going to purchase ONLY if contacted (or sometimes they purchase MORE or EARLIER only if contacted).","4d97c2ed":"For the old people we can see one of the largest standard deviations between visits over the past 15 days. Likely it is due to physical capabilities.\n\n**How to increase the number of visits of pensioners?** Make a delivery service or consultants who will accompany pensioners in stores.","78d11b80":"We will collect the features that interest us: some are already in the dataset, some will have to be collected separately. These features describe the data in general way but for a fact-finding analysis this is enough.","b382c2dd":"According to the collected features, we look at the outliers, which are determined **by the proportions relative to the IQR**. We will divide the obtained records into records from the test and control groups and divide them by participation in our chosen action. Let's look at the results.","e678f713":"### Create metric","bc5e1769":"* There are 196 columns in the dataset\n* The target variable is **response_att** - we have to predict whether the client will respond to the market mailing\n* Audience is divided into 2 samples - **test** and **control**\n* The dataset gives us basic information about the client, for example, his **age**, **number of children**\n* There is also a lot of information about some groups of goods (it's necessary for searching insights) and a lot of statistical information for example the **coefficients of variation** of discounts or prices","b3a7a48e":"### Simple preprocessing","7dcfd5cc":"# Our Model\n\nThe main idea was to use [**Class Transformation**](https:\/\/scikit-uplift.readthedocs.io\/en\/latest\/user_guide\/models\/revert_label.html). Simple but powerful and mathematically proven uplift modeling method, presented in 2012. The main idea is to predict a slightly changed target:\n\n![image.png](attachment:image.png)\n\nIn other words the new target equals 1 if a response in the treatment group is as good as a response in the control group and equals 0 otherwise.","2655f428":"## Dependence of the share of purchase food on the number of children","38e8f5a9":"# Missing Values","b9055bc9":"Just note that the control and test groups are **not balanced**. The ratio of the control to the test **1 to 3**. The total number of observations: **687000**.","e623f61e":"> There are a lot of outliers but most of the selected features were artificially collected. We may have mistook seasonal anomalies for outliers.\n\nAlso take a look at the correlation between features.","5c10575e":"## Aged people","64fb15f9":"# Quick View","0107238b":"# Outliers\n\nWe should be aware of outliers. Why? Because outliers can markedly affect on our models and can be a valuable source of information providing us insights about specific behaviours.\n\nOutliers is a complex subject and it deserves more attention. Here, we'll just do a quick analysis","5ff5539b":"Our hypothesis is that the more children in the family, the more often the father is sent to the store. Perhaps this is due to the large number of heavy packages. And this graph illustrates: **the more children, the more pronounced increase in the proportion of men.**\n\n**How to increase customer loyalty and profit?** Create a special discount system for parents, which will depend on the number of children.","556a9914":"## Plan\n1. [Quick View](#1)\n1. [Missing Values](#2)\n1. [Outliers](#3)\n1. [Test vs Control](#4)\n1. [About Uplift Modeling](#5)\n1. [Our Model](#6)\n1. [Insights](#7)","5dc86bed":"### Inference"}}