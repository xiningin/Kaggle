{"cell_type":{"3abb3921":"code","e60d16fa":"code","bee3a061":"code","2ee713c7":"code","e4f12e6e":"code","0dd71d57":"code","38e68d2c":"code","67b3683b":"code","f5be8f29":"code","80dccae5":"code","32c14497":"code","c87c95eb":"code","4353fdd4":"code","a773bf1d":"code","60694ab5":"code","b59bc50e":"code","da5a0d2c":"code","05e568f8":"code","b3d54338":"code","878025cc":"code","243bafaf":"code","7f355a61":"code","a3eb3d0a":"code","c33a10db":"code","901c6869":"code","76fd56ad":"code","9b5ec819":"code","097c9b2c":"code","6e0006c5":"code","217ae849":"code","683058ed":"code","b9f3a70b":"code","1692a424":"code","214c9355":"code","4cbf26e2":"code","97ecdedb":"markdown","57392c0d":"markdown","e7b8bd01":"markdown","68459f1c":"markdown","96157c4e":"markdown","e998ccb2":"markdown","0b4716c7":"markdown","bb0523d6":"markdown","a27a0902":"markdown","1168a80a":"markdown","7e8b4789":"markdown","394042de":"markdown","3b493587":"markdown","801797c6":"markdown","06b35a79":"markdown","5fddff17":"markdown","970adee8":"markdown","0cb5ed4b":"markdown","fb90ab7f":"markdown","667d0e7c":"markdown","f8f13613":"markdown","8929f77d":"markdown","0cf8b2a1":"markdown","2f8ff53d":"markdown","08840c30":"markdown","5ceed470":"markdown","39408939":"markdown"},"source":{"3abb3921":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","e60d16fa":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve","bee3a061":"data = pd.read_csv('..\/input\/Combined_News_DJIA.csv')\ndata.head()","2ee713c7":"combined=data.copy()\ncombined['Combined']=combined.iloc[:,2:27].apply(lambda row: ''.join(str(row.values)), axis=1)","e4f12e6e":"train = data[data['Date'] < '2015-01-01']\ntest = data[data['Date'] > '2014-12-31']","0dd71d57":"print(\"Length of train is\",len(train))\nprint(\"Length of test is\", len(test))","38e68d2c":"trainheadlines = []\nfor row in range(0,len(train.index)):\n    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))","67b3683b":"train = combined[combined['Date'] < '2015-01-01']\ntest = combined[combined['Date'] > '2014-12-31']","f5be8f29":"non_decrease = train[train['Label']==1]\ndecrease = train[train['Label']==0]\nprint(len(non_decrease)\/len(train))","80dccae5":"non_decrease_test = test[test['Label']==1]\ndecrease_test = test[test['Label']==0]\nprint(len(non_decrease_test)\/len(test))","32c14497":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom wordcloud import WordCloud,STOPWORDS\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\ndef to_words(content): ### function to clean the words\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", content) ### get only letters\n    words = letters_only.lower().split()             ### lowercase       \n    stops = set(stopwords.words(\"english\"))         ### remove stopwords such as 'the', 'and' etc.         \n    meaningful_words = [w for w in words if not w in stops] ### get meaningful words\n    return( \" \".join( meaningful_words )) ","c87c95eb":"non_decrease_word=[]\ndecrease_word=[]\nfor each in non_decrease['Combined']:\n    non_decrease_word.append(to_words(each))\n\nfor each in decrease['Combined']:\n    decrease_word.append(to_words(each))","4353fdd4":"wordcloud1 = WordCloud(background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(decrease_word[1])\nplt.figure(1,figsize=(8,8))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.title(\"Words which indicate a fall in DJIA \")\nplt.show()","a773bf1d":"wordcloud2 = WordCloud(background_color='green',\n                      width=3000,\n                      height=2500\n                     ).generate(non_decrease_word[3])\nplt.figure(1,figsize=(8,8))\nplt.imshow(wordcloud2)\nplt.axis('off')\nplt.title(\"Words which indicate a rise\/stable DJIA \")\nplt.show()","60694ab5":"example = train.iloc[3,3]\nprint(example)","b59bc50e":"example2 = example.lower()\nprint(example2)","da5a0d2c":"example3 = CountVectorizer().build_tokenizer()(example2)\nprint(example3)","05e568f8":"pd.DataFrame([[x,example3.count(x)] for x in set(example3)], columns = ['Word', 'Count'])","b3d54338":"basicvectorizer = CountVectorizer()\nbasictrain = basicvectorizer.fit_transform(trainheadlines)\nprint(basictrain.shape)","878025cc":"testheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n","243bafaf":"basictest = basicvectorizer.transform(testheadlines)\nprint(basictest.shape)","7f355a61":"Classifiers = [\n    LogisticRegression(C=0.1,solver='liblinear',max_iter=2000),\n    KNeighborsClassifier(3),\n    RandomForestClassifier(n_estimators=500,max_depth=9),\n    ]","a3eb3d0a":"Accuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(basictrain,train['Label'])\n        pred = fit.predict(basictest)\n        prob = fit.predict_proba(basictest)[:,1]\n    except Exception:\n        fit = classifier.fit(basictrain,train['Label'])\n        pred = fit.predict(basictest)\n        prob = fit.predict_proba(basictest)[:,1]\n    accuracy = accuracy_score(pred,test['Label'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    fpr, tpr, _ = roc_curve(test['Label'],prob)","c33a10db":"df=pd.DataFrame(columns = ['Model', 'Accuracy'],index=np.arange(1, len(df)+1))\ndf.Model=Model\ndf.Accuracy=Accuracy\ndf","901c6869":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer()\ntrain_text = []\ntest_text = []\nfor each in train['Combined']:\n    train_text.append(to_words(each))\n\nfor each in test['Combined']:\n    test_text.append(to_words(each))\ntrain_features = tfidf.fit_transform(train_text)\ntest_features = tfidf.transform(test_text)","76fd56ad":"Classifiers = [\n    LogisticRegression(C=0.1,solver='liblinear',max_iter=2000),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.25, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=500,max_depth=9),\n    AdaBoostClassifier(),\n    ]","9b5ec819":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['Label'])\n        pred = fit.predict(test_features)\n        prob = fit.predict_proba(test_features)[:,1]\n    except Exception:\n        fit = classifier.fit(dense_features,train['Label'])\n        pred = fit.predict(dense_test)\n        prob = fit.predict_proba(dense_test)[:,1]\n    accuracy = accuracy_score(pred,test['Label'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    fpr, tpr, _ = roc_curve(test['Label'],prob)\n    ","097c9b2c":"df=pd.DataFrame(columns = ['Model', 'Accuracy'],index=np.arange(1, len(df)+1))\ndf.Model=Model\ndf.Accuracy=Accuracy\ndf","6e0006c5":"advancedvectorizer = CountVectorizer(ngram_range=(2,2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)\nprint(advancedtrain.shape)\n","217ae849":"advancedtest = advancedvectorizer.transform(testheadlines)","683058ed":"Accuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(advancedtrain,train['Label'])\n        pred = fit.predict(advancedtest)\n        prob = fit.predict_proba(advancedtest)[:,1]\n    except Exception:\n        fit = classifier.fit(advancedtrain,train['Label'])\n        pred = fit.predict(advancedtest)\n        prob = fit.predict_proba(advancedtest)[:,1]\n    accuracy = accuracy_score(pred,test['Label'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    fpr, tpr, _ = roc_curve(test['Label'],prob)\n    ","b9f3a70b":"df=pd.DataFrame(columns = ['Model', 'Accuracy'],index=np.arange(1, len(df)+1))\ndf.Model=Model\ndf.Accuracy=Accuracy\ndf","1692a424":"advancedvectorizer = CountVectorizer(ngram_range=(3,3))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)\nprint(advancedtrain.shape)\nadvancedtest = advancedvectorizer.transform(testheadlines)","214c9355":"Accuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(advancedtrain,train['Label'])\n        pred = fit.predict(advancedtest)\n        prob = fit.predict_proba(advancedtest)[:,1]\n    except Exception:\n        fit = classifier.fit(advancedtrain,train['Label'])\n        pred = fit.predict(advancedtest)\n        prob = fit.predict_proba(advancedtest)[:,1]\n    accuracy = accuracy_score(pred,test['Label'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    fpr, tpr, _ = roc_curve(test['Label'],prob)","4cbf26e2":"df=pd.DataFrame(columns = ['Model', 'Accuracy'],index=np.arange(1, len(df)+1))\ndf.Model=Model\ndf.Accuracy=Accuracy\ndf","97ecdedb":"The process involved:\n\n- Converting the headline to lowercase letters\n- Splitting the sentence into a list of words\n- Removing punctuation and meaningless words\n","57392c0d":"##### Lower Case","e7b8bd01":"#### Train and Test Split","68459f1c":"As we can see, there has been a slight improvement from the previous scores.","96157c4e":"This time we have 611,140 unique variables representing three-word combinations!","e998ccb2":"## Text Preprocessing","0b4716c7":"The technique we just used is known as a **bag-of-words** model. We essentially placed all of our headlines into a \"bag\" and counted the words as we pulled them out.\n\nHowever,  a single word doesn't always have enough meaning by itself.\n\nWe need to consider the rest of the words in the sentence as well!","bb0523d6":"Lets first join all the headlines for each row together.","a27a0902":"## Model fitting","1168a80a":"**TFIDF Model**\n","7e8b4789":"##### Count Vectorizer","394042de":"## Advanced Modeling","3b493587":"### Basic Model Training and Testing","801797c6":"Our resulting table contains counts for 31,675 different words!","06b35a79":"The accuracy does not seem to increase and it looks like we have hit our maximum accuracy point at 56%.","5fddff17":"We are getting much better results now and we are getting an accuracy of 56.08%.","970adee8":"We can see that the occurrence of non-decrease situation is almost equal to that of a decrease market.","0cb5ed4b":"##### n=2","fb90ab7f":"### Feature Extraction","667d0e7c":"### N - gram model","f8f13613":"The **Label** variable will be a **1** if the DJIA stayed the same or rose on that date or **0** if the DJIA fell on that date.\n","8929f77d":"**Model Fitting**","0cf8b2a1":"##### n=3","2f8ff53d":"This time we have 366,721 unique variables representing two-word combinations!","08840c30":"Lets try to improve the score with more models and feature Selection.","5ceed470":"The tool we'll be using is CountVectorizer, which takes a single list of strings as input, and produces word counts for each one.","39408939":"#### Simple EDA"}}