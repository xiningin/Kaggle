{"cell_type":{"ab33b843":"code","41107d46":"code","829db7c7":"code","2d28fd19":"code","acdaf2fe":"code","208a43d0":"code","f7b0c189":"code","2d4bcad5":"code","ff667d39":"code","3a1785c3":"code","b8afb18f":"code","7fc6b5c7":"code","a4360320":"code","8c09fb41":"code","2580d4ee":"code","570907d8":"code","ecd69413":"code","100967f6":"code","51719f4f":"code","45fdc5f8":"code","417a78b2":"code","e34254c0":"code","4a0698cc":"code","12073857":"code","481aa139":"code","02177875":"code","31a656a7":"code","36e57f25":"code","5c5964bc":"code","8df5462d":"code","0857c4e9":"code","9922ceb1":"code","d9e4d619":"code","232167e3":"code","08dac0bd":"markdown","37acb23b":"markdown","c3238075":"markdown","fa77a2df":"markdown","65f482d6":"markdown","b91bd56a":"markdown","de493a24":"markdown","ec88103f":"markdown","3bff2c86":"markdown","14e8c0bb":"markdown","22a341b4":"markdown","65556307":"markdown","c5683b69":"markdown","3c248d4e":"markdown","33dcebf6":"markdown","966ceb59":"markdown","094fdd41":"markdown","945f7c18":"markdown","b8a53887":"markdown","33ff1c58":"markdown","0071b7dc":"markdown","e27dab4f":"markdown","bc7a8dde":"markdown","f33084b5":"markdown","632d4eb5":"markdown","cfc0a905":"markdown","1999ca39":"markdown","249ad033":"markdown","65105acf":"markdown"},"source":{"ab33b843":"import re\nimport string\nimport urllib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport warnings\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom nltk.tokenize import TweetTokenizer, sent_tokenize, word_tokenize, regexp_tokenize\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS","41107d46":"warnings.filterwarnings(\"ignore\")\n\npd.options.display.max_rows = 10\npd.options.display.max_columns = 20\n\nprint(\"numpy version: {}\".format(np.__version__))\nprint(\"pandas version: {}\".format(pd.__version__))\nprint(\"seaborn version: {}\\n\".format(sns.__version__))\n\nsns.set_style(\"whitegrid\")\nflatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\nsns.set_palette(flatui)\n\nsns.palplot(sns.color_palette())","829db7c7":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv', encoding='utf8')\nprint('Train data loaded.')\n\n# Save a clean copy for later\nclean_copy = df_train.copy()\n\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv', encoding='utf8')\nprint('Test data loaded.')\n\nsample_sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv', encoding='utf8')","2d28fd19":"print('There are {} of records in the train data set.'.format(len(df_train.index)))\nprint('There are {} of records in the test data set.'.format(len(df_test.index)))","acdaf2fe":"df_train.head()","208a43d0":"df_train.info()","f7b0c189":"df_train.isnull().sum()","2d4bcad5":"target_value_counts=df_train.target.value_counts()\nprint(target_value_counts)","ff667d39":"sns.set_style('ticks')\nfig, ax = plt.subplots()\nfig.set_size_inches(11, 8)\nclrs = ['#2ecc71','#e74c3c']\nsns.barplot(x=target_value_counts.index,\n            y=target_value_counts, \n            capsize=.3, \n            palette=clrs)\nplt.xlabel(\"0=Fake News    or    1=Real Disaster\")\nplt.ylabel(\"Number of Tweets\")\nplt.title(\"Distribution of Test Data\")\nplt.show(fig)","3a1785c3":"cols_with_missing = ['keyword', 'location']\ntrain_empties = df_train[cols_with_missing].isnull().sum()\/len(df_train)*100\nfig, ax = plt.subplots()\nfig.set_size_inches(11, 8)\nclrs = ['#3498db', '#e74c3c']\nsns.barplot(x=train_empties.index,\n            y=train_empties.values,\n            ax=ax,\n            capsize=.3, \n            palette=clrs)\nax.set_ylabel('Percent Missing Values',labelpad=20)\nax.set_yticks(np.arange(0,40,5))\nax.set_ylim((0,35))\nax.set_title('Missing Keywords and Locations', fontsize=13)\nplt.show(fig)","b8afb18f":"keyword_value_counts=df_train['keyword'].value_counts()\nprint('There are {} unique keywords.\\n'.format(len(keyword_value_counts)))\nprint(keyword_value_counts)\ntop_25_kw = keyword_value_counts[:25]","7fc6b5c7":"tick_range = [0,5,10,15,20,25,30,35,40,45]\nfig, ax = plt.subplots()\nfig.set_size_inches(16, 8)\nsns.barplot(y=top_25_kw.values,\n            x=top_25_kw.index,\n            palette=flatui)\nplt.xlabel('Keyword')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45)\nplt.yticks(ticks=tick_range, rotation=0)\nplt.title(\"Frequency of Keyword Use - Top 25\")\nplt.show(fig)","a4360320":"true_ratios = df_train.groupby('keyword')['target'].mean().sort_values(ascending=False)\nfig, ax = plt.subplots()\nfig.set_size_inches(16, 8)\nsns.barplot(x=true_ratios.index[:25],\n            y=true_ratios.values[:25],\n            ax=ax,\n           palette=flatui)\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)\nplt.xlabel('Keyword')\nplt.ylabel(\"True-False Ratio\")\nplt.title(\"Top 25 Keywords for Fake Disasters\")\nplt.show()","8c09fb41":"loc_value_counts=df_train['location'].value_counts()\nprint('There are {} unique keywords.\\n'.format(len(loc_value_counts)))\nprint(loc_value_counts)\ntop_25_loc = loc_value_counts[:25]","2580d4ee":"tick_range = [0,20,40,60,80,100,120]\nfig, ax = plt.subplots()\nfig.set_size_inches(16, 8)\nsns.barplot(y=top_25_loc.values,\n            x=top_25_loc.index,\n            palette=flatui)\nplt.xlabel('Location')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45)\nplt.yticks(ticks=tick_range, rotation=0)\nplt.title(\"Frequency of Locations - Top 25\")\nplt.show(fig)","570907d8":"# WordCloud helper function\n\ndef wc(x, stop_words, max_words, bgcolor, plot_title):\n    plt.figure(figsize = (16,8))\n    wc = WordCloud(background_color=bgcolor, stopwords=stop_words, max_words=max_words,  max_font_size=50).generate(str(x))\n    wc.generate(' '.join(x))\n    plt.title(plot_title)\n    plt.imshow(wc)\n    plt.axis('off')","ecd69413":"max_words = 500\nstop_words = [\"https\", \"co\", \"RT\", 'http', 'hi', 'amp', 'ha'] + list(STOPWORDS)\nwc(df_train[df_train['target']==1]['text'], stop_words, max_words,'black', 'Most Frequent Words - Real')","100967f6":"wc(df_train[df_train['target']==0]['text'], stop_words, max_words,'black', 'Most Frequent Words - Fake')","51719f4f":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(16,8))\n\ntweet_len_real=df_train[df_train['target']==1]['text'].str.len()\nsns.distplot(tweet_len_real,\n             ax=ax1,\n             color='#e74c3c')\nax1.set_title('Real Disaster')\n\ntweet_len_fake=df_train[df_train['target']==0]['text'].str.len()\nsns.distplot(tweet_len_fake,\n             ax=ax2,\n             color='#2ecc71')\nax2.set_title('Fake Disaster')\n\nfig.suptitle('Length in Characters')\nplt.show()\n","45fdc5f8":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(16,8))\nwords_real=df_train[df_train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nwords_fake=df_train[df_train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\n\n\nsns.distplot(words_real.map(lambda x: np.mean(x)),\n             ax=ax1,\n             color='#e74c3c')\nax1.set_title('Real Disaster')\n\nsns.distplot(words_fake.map(lambda x: np.mean(x)),\n             ax=ax2,\n             color='#2ecc71')\nax2.set_title('Fake Disaster')\n\nfig.suptitle('Average Length of Tweets in Words')","417a78b2":"def avg_word_len(text):\n    words = word_tokenize(text)\n    word_lens = [len(w) for w in words]\n    return round(np.mean(word_lens),1)\n\ndef clean_text(text):\n    text = re.sub('[^a-zA-Z]', ' ', text)  \n    text = text.lower()  \n    # split to array(default delimiter is \" \") \n    text = text.split()  \n    text = [w for w in text if not w in set(stopwords.words('english'))] \n    text = ' '.join(text)            \n    return text","e34254c0":"def text_feature_eng(x):\n    \n    # Clean text\n    x['clean_text'] = x['text'].apply(lambda x : clean_text(x))\n\n    # Word Count\n    tweek_tzr = TweetTokenizer()\n    x['word_cnt'] = x['clean_text'].apply(lambda t: len(tweek_tzr.tokenize(t.lower())))\n\n    # Character Count\n    x['char_cnt'] = x['clean_text'].apply(lambda c: len(c))\n\n    # Hashtags\n    hashtag_re = r\"#\\w+\"\n    x['hashtag_ct'] = x['text'].apply(lambda h: len(regexp_tokenize(h, hashtag_re)))\n\n    # Average Word Length\n    x['avg_word_len'] = x['clean_text'].apply(avg_word_len)\n\n    # Numbers\n    num_re = r\"(\\d+\\.?,?\\s?\\d+)\"\n    x['num_cnt'] = x['text'].apply(lambda n: len(regexp_tokenize(n, num_re)))\n\n    # Punctuation Count\n    punct_re = r\"[^\\w\\s]\"\n    x['punct_cnt'] = x['text'].apply(lambda p: len(regexp_tokenize(p, punct_re)))\n\n    # Mentions\n    mention_re = r\"@\\w+\"\n    x['mention_cnt'] = x['text'].apply(lambda m: len(regexp_tokenize(m, mention_re)))\n\n    # Bag of Words\n    x['bow'] = x['clean_text'].apply(lambda t: [w for w in tweek_tzr.tokenize(t.lower())])\n\n    # Words without hashtags or mentions\n    x['words_only'] = x['bow'].apply(lambda w: [t for t in w if t.isalpha()])\n\n    # Stopwords\n    x['stopwords'] = x['bow'].apply(lambda x: [t for t in x if t in stopwords.words('english')])\n    \n    # Number of text emojis\n    x['emojis'] = x['text'].apply(lambda comment: sum(comment.count(e) for e in (':-)', ':)', ';-)', ';)', ':(', ':-(')))\n    \n    # Flag for missing keywords\n    x['no_keywords'] = x['keyword'].isna().astype(int)\n    \n    # Flag for missing location\n    x['no_location'] = x['location'].isna().astype(int)\n    \n    # Drop text column\n    x.drop('text', axis=1, inplace=True)\n    \n    x.avg_word_len.fillna(0) \n    \n    return x","4a0698cc":"# Revert to a clean copy\ndf_train = clean_copy.copy()\n\n# Get the features and the lables\nX = df_train.copy()\n\n# Apply feature engineering process\nX_proc = text_feature_eng(X)\nprint('X_proc shape: {}'.format(X_proc.shape))","12073857":"X_proc.head()","481aa139":"all_features = [col for col in X_proc.columns.values \n                if col  not in ['id', 'target']]\nnum_features = [col for col in X_proc.columns.values \n                if col  not in ['id','target','bow','words_only',\n                                'stopwords', 'clean_text', \n                                'keyword', 'location','no_keywords',\n                                'no_location']]","02177875":"SEED = 37\nSPLIT = .8\n\nX_train, X_val, y_train, y_val = train_test_split(X_proc[all_features], \n                                                  X_proc['target'],\n                                                  train_size=SPLIT,\n                                                  shuffle=True,\n                                                  random_state=SEED)\nprint('{} training records'.format(len(X_train)))\nprint('{} training labels'.format(len(y_train)))\nprint('{} validation records'.format(len(X_val)))\nprint('{} validation labels'.format(len(y_val)))","31a656a7":"# Text feature selector\nclass TextSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.key]\n\n# Numeric feature selector    \nclass NumericSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[[self.key]]","36e57f25":"# Per-feature pipline\ntfidf_pipeline = Pipeline([\n                ('selector', TextSelector(key='clean_text')),\n                ('tfidf', TfidfVectorizer())])\nlength_pipeline =  Pipeline([\n                ('selector', NumericSelector(key='avg_word_len')),\n                ('standard', StandardScaler())\n            ])\nwords_pipeline =  Pipeline([\n                ('selector', NumericSelector(key='word_cnt')),\n                ('standard', StandardScaler())\n            ])\nchar_pipeline =  Pipeline([\n                ('selector', NumericSelector(key='char_cnt')),\n                ('standard', StandardScaler())\n            ])\nnum_pipeline =  Pipeline([\n                ('selector', NumericSelector(key='num_cnt')),\n                ('standard', StandardScaler())\n            ])\npunct_pipeline =  Pipeline([\n                ('selector', NumericSelector(key='punct_cnt')),\n                ('standard', StandardScaler())\n            ])\n\n# Union the features together\nfeature_pipeline = FeatureUnion([('tfidf', tfidf_pipeline), \n                                 ('length', length_pipeline),\n                                 ('words', words_pipeline),\n                                 ('chars', char_pipeline),\n                                 ('nums', num_pipeline),\n                                 ('punct', punct_pipeline)])\n\nfeature_processing = Pipeline([('features', feature_pipeline)])\nfeature_processing.fit_transform(X_train)\nprint('X_train shape: {}'.format(X_train.shape))","5c5964bc":"tuned_parameters = {'kernel': ['linear'], \n                    'C': [1, 5, 10],\n                    'cache_size': [100,200,400],\n                    'degree': [2, 5, 10]}\n\nscores = ['precision', 'recall']","8df5462d":"for score in scores:\n    print('Tuning hyper-parameters for %s \\n' % score)\n    print ('Creating pipeline instance.')\n    sentiment_pipeline = Pipeline([\n                ('features',feature_pipeline),\n                ('classifier', GridSearchCV(SVC(),\n                tuned_parameters, \n                scoring='%s_macro' % score,\n                verbose=10,\n                n_jobs=-1,\n                cv=3))])\n \n    print('Fitting the model.')\n    sentiment_pipeline.fit(X_train, y_train)\n\n    print('Tuning hyper-parameters for %s \\n' % score)\n    print('Best parameters set found on development set: \\n')\n    print(sentiment_pipeline.named_steps['classifier'].best_params_, '\\n')\n    print(\"Grid scores on development set:\\n\")\n    means = sentiment_pipeline.named_steps['classifier'].cv_results_['mean_test_score']\n    stds = sentiment_pipeline.named_steps['classifier'].cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, sentiment_pipeline.named_steps['classifier'].cv_results_['params']):\n          print(\"%0.3f (+\/-%0.03f) for %r\"\n            % (mean, std * 2, params))\n    print()\n    print('Detailed classification report:\\n')\n    print('The model is trained on the full development set.')\n    print('The scores are computed on the full evaluation set.\\n')\n    y_true, y_pred = y_val, sentiment_pipeline.predict(X_val)\n    print(classification_report(y_val, y_pred), '\\n')","0857c4e9":"cm = confusion_matrix(y_val, y_pred)\nfig = plt.figure(figsize = (10,7))\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix for SVC')\nfig.colorbar(cax)\nlabels = ['Fake', 'Real']\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","9922ceb1":"X_train_full = clean_copy\nX_train_proc = text_feature_eng(X_train_full)\n\ninference_pipeline = Pipeline([\n                ('features',feature_pipeline),\n                ('classifier', SVC(C=1,\n                                   kernel='linear',\n                                   random_state=SEED,\n                                   cache_size=200,\n                                   degree=1))])\n\ninference_pipeline.fit(X_train_proc[all_features], X_train_proc['target'])\n","d9e4d619":"X_test = df_test.copy()\nX_test_proc = text_feature_eng(X_test)\nX_test_proc['avg_word_len'].fillna(0, inplace=True)\ny_pred = inference_pipeline.predict(X_test_proc[all_features])\nlen(y_pred)\nlen(X_test_proc)","232167e3":"submission['id'], submission['target'] = X_test_proc['id'], y_pred\nsubmission.to_csv('\/kaggle\/working\/submission.csv')","08dac0bd":"#### Length of Tweet in Characters","37acb23b":"## Basic Data Exploration\n\n### Column Information","c3238075":"   ### Prepare the Data","fa77a2df":"### Pipeline Feature Selectors\n\nI don't have homegenous features, so I want to be able to process one feature at a time. I had some trouble getting a single feature selector to work, so I created one for text features and one for numeric features.","65f482d6":"## Build the Model","b91bd56a":"#### Tweet Length in Words","de493a24":"## Create Submission","ec88103f":"### Make Test-Train Split","3bff2c86":"## Train Final Model on Entire Test Set","14e8c0bb":"### Get Features","22a341b4":"#### Most Frequent Words - Fake","65556307":"## Predict","c5683b69":"# Disaster Tweets with NLP\n\n#### What should I expect the data format to be?\nEach sample in the train and test set has the following information:\n* The text of a tweet\n* A keyword from that tweet (although this may be blank!)\n* The location the tweet was sent from (may also be blank)\n\n#### What am I predicting?\n* Whether a given tweet is about a real disaster or not. \n* If so, predict a 1. If not, predict a 0.","3c248d4e":"#### Top 25 Locations","33dcebf6":"The dataset is a bit unbalanced. There are 4342 fake disasters, and 3271 real ones.","966ceb59":"### Keywords Exploration","094fdd41":"## Feature Engineering\n\nI\u00b4m going to encapsulate all of my new feature creation into fuctions so I can more easily build a pipeline when I get to that point","945f7c18":"### Tweet Body Exploration","b8a53887":"### Confusion Matrix","33ff1c58":"## Build the Pipeline","0071b7dc":"### Location Exploration","e27dab4f":"#### Top 25 Keywords for Fake Disasters","bc7a8dde":"#### Most Frequent Words Real Disasters","f33084b5":"#### Top 25 Keywords","632d4eb5":"It looks like a left skewed distribution for both. Fake tweets seem to be a bit longer on average.","cfc0a905":"We have several null locations and some null keywords in the test data.","1999ca39":"## Load the Data","249ad033":"### Target Distribution\n\nLet's take a quick look at the targets. The value counts should be only 0 or 1.**","65105acf":"### Missing Values\n\nI saw earlier that some location and keyword values are missing. Let\u00b4s take a closer look."}}