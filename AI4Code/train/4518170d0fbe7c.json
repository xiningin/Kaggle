{"cell_type":{"e1badcdc":"code","5eac9964":"code","36781153":"code","b0ed9703":"code","1bc232ce":"code","c7d83215":"code","1447dfd1":"code","4f9e934f":"code","43a2f6ad":"code","b05e608f":"code","d39f789a":"code","283361af":"code","2a1cbb4b":"code","9ee21d59":"code","a4aeac2c":"code","03e8450d":"code","3311d91b":"code","240d8bae":"code","12347cfe":"code","9bbd1e5f":"code","a597e55f":"code","d0493b22":"code","85775083":"code","35d3d04d":"code","a3a61df6":"code","00355ec2":"code","5da6bede":"code","03b98e21":"code","87f81b63":"code","97159874":"code","30ee9715":"code","c39dfb54":"markdown","9e294194":"markdown","b14f4cfd":"markdown","b6dc14c0":"markdown","a8890423":"markdown","4a7d65de":"markdown","34d2fb29":"markdown","aef4f867":"markdown","42d27022":"markdown","687c9cf6":"markdown","6afcc34c":"markdown","51234ec0":"markdown","6755579c":"markdown","4f5f0246":"markdown","7677fb88":"markdown","ab5c0236":"markdown","a4986761":"markdown","67b127ee":"markdown","90a7e71c":"markdown","b0c21cf3":"markdown"},"source":{"e1badcdc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5eac9964":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","36781153":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","b0ed9703":"women = train_data.loc[train_data['Sex'] == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","1bc232ce":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","c7d83215":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","1447dfd1":"#Looking at what columns there are null entries\ntrain_data.isna().sum()","4f9e934f":"#What are the dtypes of each column?\ntrain_data.info()","43a2f6ad":"#Defining a new feature based on whether or not 'cabin' is missing\ntrain_data['Cabin_is_known'] = train_data['Cabin'].notnull().astype('int')\n#Dropping 'PassengerId','Name','Ticket', and 'Cabin' columns\ntrain_data = train_data.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)\n\ntrain_data.head()","b05e608f":"#Inspecting correlations between features\ntrain_data.corr()","d39f789a":"#Filling the missing values in the 'Age' column\ntrain_data['Age'] = train_data.groupby(['Sex', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.mean()))\n#Dropping the remaining rows with missing values (in the 'Embarked' column)\ntrain_data = train_data.dropna()\n\ntrain_data.isnull().sum()","283361af":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Distribution of age between different genres\ng = sns.FacetGrid(train_data, col=\"Sex\")\ng = g.map(plt.hist, \"Age\")","2a1cbb4b":"sns.catplot(x='Sex',y='Survived',data=train_data,kind='bar',hue='Cabin_is_known')\nplt.show()","9ee21d59":"#Getting dummy variables from the categorical variables\nX_train = pd.get_dummies(1*train_data,drop_first=True)\nX_train.head()","a4aeac2c":"#Creating numpy arrays for the training examples (X) and corresponding labels (y)\ny,X = X_train.values[:,0],X_train.values[:,1:]","03e8450d":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n#Initialazing the pipeline with the steps corresponding to normalizing the features and then applying the model\nsteps = [('scaler', StandardScaler()), ('logreg',LogisticRegression(solver='liblinear'))]\npipeline = Pipeline(steps)\n\n#Defining the set of hyperparameters to be tested. We shall select the best combination\nC = [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\nmax_iter=[50,100,150,200,250,300,350,400,450,500]\ntol=[0.00001,0.00003,0.0001,0.0003,0.001]\npenalty = ['l1','l2']\nparam_grid = dict(logreg__C=C,logreg__max_iter=max_iter,logreg__tol=tol,logreg__penalty = penalty)\n\n#Defining and fitting the model to the data.\ngrid_model = GridSearchCV(pipeline, param_grid=param_grid, cv=5)\n\ngrid_model_result = grid_model.fit(X, y)","3311d91b":"best_score, best_params = grid_model_result.best_score_,grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\ny_pred = grid_model_result.predict(X)\n# Print the confusion matrix of the logreg model\nprint(confusion_matrix(y,y_pred))","240d8bae":"test_set = test_data.copy()\ntest_set['Cabin_is_known'] = test_set['Cabin'].notnull().astype('int')\ntest_set = test_set.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)\ntest_set['Age'] = test_set.groupby(['Sex', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.mean()))\ntest_set = test_set.fillna(test_set.mean())\ntest_set = pd.get_dummies(test_set,drop_first=True)\ntest_set.head()","12347cfe":"#Predicting the labels of the test set and saving the submission file\npredictions = grid_model_result.predict(test_set.values).astype(int)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('logreg_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","9bbd1e5f":"#Taking a look at the submission file.\npd.read_csv('logreg_submission.csv').head()","a597e55f":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n#Initialazing the pipeline with the steps corresponding to normalizing the features and then applying the model\nsteps = [('scaler', StandardScaler()), ('svm',SVC())]\npipeline = Pipeline(steps)\n\n#Defining the set of hyperparameters to be tested. We shall select the best combination\nC = [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\ngamma = [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\nparam_grid = dict(svm__C=C,svm__gamma=gamma)\n\n#Defining and fitting the model to the data.\ngrid_model = GridSearchCV(pipeline, param_grid=param_grid, cv=5)\n\ngrid_model_result = grid_model.fit(X, y)","d0493b22":"best_score, best_params = grid_model_result.best_score_,grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\ny_pred = grid_model_result.predict(X)\n# Print the confusion matrix of the SVM model\nprint(confusion_matrix(y,y_pred))","85775083":"#Predicting the labels of the test set and saving the submission file\npredictions = grid_model_result.predict(test_set.values).astype(int)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('svm_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","35d3d04d":"#Taking a look at the submission file.\npd.read_csv('svm_submission.csv').head()","a3a61df6":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n#Initialazing the pipeline with the steps corresponding to normalizing the features and then applying the model\nsteps = [('scaler', StandardScaler()), ('rf',RandomForestClassifier())]\npipeline = Pipeline(steps)\n\n#Defining the set of hyperparameters to be tested. We shall select the best combination\nmax_depth = [3,4,5]\nn_estimators = [100,200]\nmin_samples_split = [2,4,5]\nmin_samples_leaf = [1,3,5]\nparam_grid = dict(rf__max_depth=max_depth,rf__n_estimators=n_estimators,\\\n                  rf__min_samples_split=min_samples_split,\\\n                 rf__min_samples_leaf=min_samples_leaf)\n\n#Defining and fitting the model to the data.\ngrid_model = GridSearchCV(pipeline, param_grid=param_grid, cv=5)\n\ngrid_model_result = grid_model.fit(X, y)","00355ec2":"best_score, best_params = grid_model_result.best_score_,grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\ny_pred = grid_model_result.predict(X)\n# Print the confusion matrix of the SVM model\nprint(confusion_matrix(y,y_pred))","5da6bede":"#Predicting the labels of the test set and saving the submission file\npredictions = grid_model_result.predict(test_set.values).astype(int)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('rf_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","03b98e21":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Initialazing the pipeline with the steps corresponding to normalizing the features and then applying the model\nsteps = [('scaler', StandardScaler()), ('xgboost',xgb.XGBClassifier())]\npipeline = Pipeline(steps)\n\n#Defining the set of hyperparameters to be tested. We shall select the best combination\nlearning_rate = [0.05,0.15,0.3]\nmax_depth = [3,4,5]\nn_estimators = [50,100,200]\nsubsample = [0.8,0.9]\ncolsample_bytree = [0.8,0.9,1]\nparam_grid = dict(xgboost__learning_rate=learning_rate,xgboost__max_depth=max_depth\\\n                  ,xgboost__n_estimators=n_estimators,xgboost__subsample=subsample,\\\n                 xgboost__colsample_bytree=colsample_bytree)\n\n#Defining and fitting the model to the data.\ngrid_model = RandomizedSearchCV(pipeline, param_distributions=param_grid, cv=5,n_iter=100)\n\ngrid_model_result = grid_model.fit(X, y)","87f81b63":"best_score, best_params = grid_model_result.best_score_,grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\ny_pred = grid_model_result.predict(X)\n# Print the confusion matrix of the SVM model\nprint(confusion_matrix(y,y_pred))","97159874":"#Predicting the labels of the test set and saving the submission file\npredictions = grid_model_result.predict(test_set.values).astype(int)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('xgboost_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","30ee9715":"#Taking a look at the submission file.\npd.read_csv('xgboost_submission.csv').head()","c39dfb54":"I will begin by inspecting the data to see how many missing values there are in each column and what should be the best approach to impute them.","9e294194":"# 3 - Exploring other models with GridSearchCV","b14f4cfd":"## 3.1 - SVM","b6dc14c0":"Now, let's evaluate the model on the test set and submit the answer. But first we need to apply the same transformations that we applied to the training set.","a8890423":"Notice how the 'Cabin_is_known' feature is highly correlated to the 'Survived' column, indicating that this should be a good feature to take into consideration in the logistic regression model. Now, let's impute the 'Age'. Instead of just filling the missing values with the mean of all ages, I will subdivide the groups based on the feature most correlated to the 'Age' ('Pclass') and 'Sex', then take the mean.","4a7d65de":"Now it is time for some data visualization!","34d2fb29":"## 2.1 - Exploring and cleaning the data","aef4f867":"And finally, gradient boosting with decision trees.","42d27022":"Let's try again using the same data treatment, but a different model, supporting vector machines.","687c9cf6":"Indeed, the 'Cabin_is_known' feature is a good feature for predicting the survival of a passenger.","6afcc34c":"It is also necessary to split the data into training and test sets. However, this was already done.","51234ec0":"# 1 - Kaggle Tutorial","6755579c":"## 2.2 - Preprocessing and modelling","4f5f0246":"The model with best performance on the test set was SVM!","7677fb88":"For logistic regression, we need to get labels for the categorical variables in order to use them in the model. It is also good practice to normalize the features (subtract the mean and divide by the range of values or the standard deviation).","ab5c0236":"## 3.2 - Random forests","a4986761":"# 2 - First try using Logistic Regression","67b127ee":"The 'Cabin' feature has many missing values. At first sight it may be tempting to just ignore this feature since the survival probability should not be correlated to the particular number of the passenger's cabin. However, the information about if the cabin number was able or not to be retrieved may be related to de survival of the passenger, as explained in https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial . The age is probably also a good indicator and should be properly imputed. In this first try I will just ignore the two rows where the 'Embarked' feature is missing and also discard the features 'PassengerId', 'Name', and 'Ticket' as they shall be decorrelated (or poorly correlated) to the survival probability.","90a7e71c":"## 3.3 - XGBoost with decision trees","b0c21cf3":"Now I will apply logistic regression to the data trying to tune the hyperparameters of the model using GridSearch cross validation."}}