{"cell_type":{"e12c18ed":"code","c5cf4de5":"code","f6a4aeb8":"code","5acf4134":"code","5138cbd3":"code","2bae7607":"code","f59aa811":"code","f96364be":"code","a5244b75":"code","c729473d":"code","7d2a1c0e":"code","591262d2":"code","6e033bc4":"code","090627f1":"code","eb1dd5de":"code","545ee51d":"code","7e2cd02a":"code","3c523b0f":"code","eb75c17c":"code","4308cec5":"code","d5c1446f":"code","9fc39fcf":"code","110e3a22":"code","670ea02a":"code","601d6388":"code","37f6f805":"code","09b9842b":"code","31804eec":"code","d2da5ad8":"code","90216125":"code","1356e25b":"code","7ff293ae":"code","303c500e":"code","32931377":"code","1c507270":"code","aff34194":"code","857c321a":"code","60ca67c0":"code","7365c119":"code","86506946":"code","56041904":"code","c5468db4":"code","4feaafcd":"code","df87a22b":"code","1f421721":"code","e6fcef56":"code","4ec486ca":"code","cfd53cda":"code","bbd662b4":"code","9b75790e":"code","f17132dd":"code","78c4bd9e":"code","1dc8a65d":"markdown","649f9138":"markdown","2f67ca8b":"markdown","953fcd94":"markdown","64ef2012":"markdown","09850a93":"markdown","3cc5551d":"markdown","da1b0c53":"markdown","aa40fd8e":"markdown","e0ccfbd5":"markdown","420f6e71":"markdown"},"source":{"e12c18ed":"!nvidia-smi","c5cf4de5":"!pip install tensorflow-gpu","f6a4aeb8":"!pip install --upgrade grpcio","5acf4134":"!pip install tqdm","5138cbd3":"!pip install bert-for-tf2","2bae7607":"!pip install sentencepiece","f59aa811":"# importing inportant libraries\nimport os\nimport math\nimport datetime\n\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport bert\nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\nfrom bert.tokenization.bert_tokenization import FullTokenizer\n\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nfrom matplotlib import rc\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\n\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n\nrcParams['figure.figsize'] = 12, 8\n\nRANDOM_SEED = 42\n\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)","f96364be":"# Reading dataset\nimport pandas as pd\ndf = pd.read_csv('..\/input\/wine-reviews\/winemag-data-130k-v2.csv')\n\n# checking the shape of the data\ndf.shape","a5244b75":"# Checking the head of the data\ndf.head()","c729473d":"# We only require the description and variety of grapes\ndf = df[['description', 'variety']]\ndf.head()","7d2a1c0e":"# Getting top 7 most described variety\ntemp_df = df.variety.value_counts()\ntemp_df.head(7)","591262d2":"# For this project we are taking top 7 variety only\nmask = df['variety'].isin(['Pinot Noir', 'Chardonnay', 'Cabernet Sauvignon', 'Red Blend', 'Bordeaux-style Red Blend', 'Riesling', 'Sauvignon Blanc'])\ndf = df[mask]\ndf.head()","6e033bc4":"chart = sns.countplot(df.variety, palette=HAPPY_COLORS_PALETTE)\nplt.title(\"Number of descriptions per Variety\")\nchart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment='right');","090627f1":"# Splitting the dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.description, df.variety, test_size = 0.2, random_state = 42)","eb1dd5de":"train = { 'text': X_train, 'intent': y_train }\ntrain_df = pd.DataFrame(train)\ntest = { 'text': X_test, 'intent': y_test }\ntest_df = pd.DataFrame(test)","545ee51d":"train_df.head()","7e2cd02a":"chart = sns.countplot(train_df.intent, palette=HAPPY_COLORS_PALETTE)\nplt.title(\"Number of descriptions per Variety\")\nchart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment='right');","3c523b0f":"# Now to make the training dataset uniform, I have taken the wine with least number of count (i.e. Sauvignin Blanc)\nsauvignon_blanc_df = train_df[train_df['intent']=='Sauvignon Blanc']\n\n# For doing so, first I have selected other varities of wines\nriesling_df = train_df[train_df['intent']=='Riesling']\npinot_noir_df = train_df[train_df['intent']=='Pinot Noir']\nchardonnay_df = train_df[train_df['intent']=='Chardonnay']\ncabernet_sauvignon_df = train_df[train_df['intent']=='Cabernet Sauvignon']\nred_blend_df = train_df[train_df['intent']=='Red Blend']\nbordeaux_style_red_blend_df = train_df[train_df['intent']=='Bordeaux-style Red Blend']","eb75c17c":"# Now I have set their count equal to that of Sauvignin Blanc\npinot_noir_df = pinot_noir_df.sample(n=len(sauvignon_blanc_df), random_state=RANDOM_SEED)\nchardonnay_df = chardonnay_df.sample(n=len(sauvignon_blanc_df), random_state=RANDOM_SEED)\ncabernet_sauvignon_df = cabernet_sauvignon_df.sample(n=len(sauvignon_blanc_df), random_state=RANDOM_SEED)\nred_blend_df = red_blend_df.sample(n=len(sauvignon_blanc_df), random_state=RANDOM_SEED)\nbordeaux_style_red_blend_df = bordeaux_style_red_blend_df.sample(n=len(sauvignon_blanc_df), random_state=RANDOM_SEED)\nriesling_df = riesling_df.sample(n=len(sauvignon_blanc_df), random_state=RANDOM_SEED)","4308cec5":"# Now I am adding all the data together\nsauvignon_blanc_df = sauvignon_blanc_df.append(pinot_noir_df).reset_index(drop=True)\nsauvignon_blanc_df = sauvignon_blanc_df.append(chardonnay_df).reset_index(drop=True)\nsauvignon_blanc_df = sauvignon_blanc_df.append(cabernet_sauvignon_df).reset_index(drop=True)\nsauvignon_blanc_df = sauvignon_blanc_df.append(red_blend_df).reset_index(drop=True)\nsauvignon_blanc_df = sauvignon_blanc_df.append(bordeaux_style_red_blend_df).reset_index(drop=True)\nsauvignon_blanc_df = sauvignon_blanc_df.append(riesling_df).reset_index(drop=True)\ntrain_df = sauvignon_blanc_df\ntrain_df.shape","d5c1446f":"chart = sns.countplot(train_df.intent, palette=HAPPY_COLORS_PALETTE)\nplt.title(\"Number of descriptions per Variety (Resampled)\")\nchart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment='right');","9fc39fcf":"# Here I am shuffling the data\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)","110e3a22":"!wget https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip","670ea02a":"!unzip uncased_L-12_H-768_A-12.zip","601d6388":"os.makedirs(\"model\", exist_ok=True)","37f6f805":"!mv uncased_L-12_H-768_A-12\/ model","09b9842b":"bert_model_name=\"uncased_L-12_H-768_A-12\"\n\nbert_ckpt_dir = os.path.join(\"model\/\", bert_model_name)\nbert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\nbert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")","31804eec":"class IntentDetectionData:\n    DATA_COLUMN = \"text\"\n    LABEL_COLUMN = \"intent\"\n\n    def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=192):\n        self.tokenizer = tokenizer\n        self.max_seq_len = 0\n        self.classes = classes\n\n        train, test = map(lambda df: df.reindex(df[IntentDetectionData.DATA_COLUMN].str.len().sort_values().index), [train, test])\n\n        ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n\n        print(\"max seq_len\", self.max_seq_len)\n        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n        self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n\n    def _prepare(self, df):\n        x, y = [], []\n\n        for _, row in tqdm(df.iterrows()):\n            text, label = row[IntentDetectionData.DATA_COLUMN], row[IntentDetectionData.LABEL_COLUMN]\n            tokens = self.tokenizer.tokenize(text)\n            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n            x.append(token_ids)\n            y.append(self.classes.index(label))\n\n        return np.array(x), np.array(y)\n\n    def _pad(self, ids):\n        x = []\n        for input_ids in ids:\n            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n            x.append(np.array(input_ids))\n        return np.array(x)","d2da5ad8":"tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))","90216125":"tokenizer.tokenize(\"I like french classic wine\")","1356e25b":"tokens = tokenizer.tokenize(\"I can't wait to visit Bulgaria again!\")\ntokenizer.convert_tokens_to_ids(tokens)","7ff293ae":"def create_model(max_seq_len, bert_ckpt_file):\n\n    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n        bc = StockBertConfig.from_json_string(reader.read())\n        bert_params = map_stock_config_to_params(bc)\n        bert_params.adapter_size = None\n        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n\n    input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n    bert_output = bert(input_ids)\n\n    print(\"bert shape\", bert_output.shape)\n\n    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n    cls_out = keras.layers.Dropout(0.5)(cls_out)\n    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n    logits = keras.layers.Dropout(0.5)(logits)\n    logits = keras.layers.Dense(units=len(classes), activation=\"softmax\")(logits)\n\n    model = keras.Model(inputs=input_ids, outputs=logits)\n    model.build(input_shape=(None, max_seq_len))\n\n    load_stock_weights(bert, bert_ckpt_file)\n\n    return model","303c500e":"test_df.head()","32931377":"classes = train_df.intent.unique().tolist()\n\ndata = IntentDetectionData(train_df, test_df, tokenizer, classes, max_seq_len=128)","1c507270":"data.train_x.shape","aff34194":"data.train_x[0]","857c321a":"data.train_y[0]","60ca67c0":"data.max_seq_len","7365c119":"model = create_model(data.max_seq_len, bert_ckpt_file)","86506946":"model.summary()","56041904":"model.compile(\n  optimizer=keras.optimizers.Adam(1e-5),\n  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n)","c5468db4":"log_dir = \"log\/intent_detection\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n\nhistory = model.fit(\n  x=data.train_x, \n  y=data.train_y,\n  validation_split=0.1,\n  batch_size=16,\n  shuffle=True,\n  epochs=5,\n  callbacks=[tensorboard_callback]\n)","4feaafcd":"%load_ext tensorboard","df87a22b":"%tensorboard --logdir log","1f421721":"ax = plt.figure().gca()\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\n\nax.plot(history.history['loss'])\nax.plot(history.history['val_loss'])\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'])\nplt.title('Loss over training epochs')\nplt.show();","e6fcef56":"ax = plt.figure().gca()\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\n\nax.plot(history.history['acc'])\nax.plot(history.history['val_acc'])\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'])\nplt.title('Accuracy over training epochs')\nplt.show();","4ec486ca":"_, train_acc = model.evaluate(data.train_x, data.train_y)\n_, test_acc = model.evaluate(data.test_x, data.test_y)\n\nprint(\"train acc\", train_acc)\nprint(\"test acc\", test_acc)","cfd53cda":"y_pred = model.predict(data.test_x).argmax(axis=-1)","bbd662b4":"print(classification_report(data.test_y, y_pred, target_names=classes))","9b75790e":"cm = confusion_matrix(data.test_y, y_pred)\ndf_cm = pd.DataFrame(cm, index=classes, columns=classes)","f17132dd":"hmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\nhmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\nhmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\nplt.ylabel('True label')\nplt.xlabel('Predicted label');","78c4bd9e":"\nsentences = [\n  \"Strong wine made of red grapes\",\n  \"Grapy plummy and juicy taste\"\n]\n\npred_tokens = map(tokenizer.tokenize, sentences)\npred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\npred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n\npred_token_ids = map(lambda tids: tids +[0]*(data.max_seq_len-len(tids)),pred_token_ids)\npred_token_ids = np.array(list(pred_token_ids))\n\npredictions = model.predict(pred_token_ids).argmax(axis=-1)\n\nfor text, label in zip(sentences, predictions):\n    print(\"text:\", text, \"\\nintent:\", classes[label])\n    print()","1dc8a65d":"# [Making the training dataset uniform:](https:\/\/www.kaggle.com\/kshitijmohan\/sentiment-analysis-universal-sentence-encoder-91)","649f9138":"# [Data Preprocessing:](https:\/\/www.kaggle.com\/kshitijmohan\/sentiment-analysis-universal-sentence-encoder-91)","2f67ca8b":"# [So, What is BERT??](https:\/\/www.kaggle.com\/kshitijmohan\/sentiment-analysis-universal-sentence-encoder-91)\n**BERT is a multilingual transformer based model that has achieved state-of-the-art results on various NLP tasks. BERT is a bidirectional model that is based on the transformer architecture, it replaces the sequential nature of RNN (LSTM & GRU) with a much faster Attention-based approach. The model is also pre-trained on two unsupervised tasks, masked language modeling and next sentence prediction. This allows us to use a pre-trained BERT model by fine-tuning the same on downstream specific tasks such as sentiment classification, intent detection, question answering and more.**","953fcd94":"# [Compiling and Training the model:](https:\/\/www.kaggle.com\/kshitijmohan\/sentiment-analysis-universal-sentence-encoder-91)","64ef2012":"# [Plan of action:](https:\/\/www.kaggle.com\/kshitijmohan\/sentiment-analysis-universal-sentence-encoder-91)\n1. **We will personalized wine recommendations based on short description provided by the User.**\n2. **For this we will use BERT model to analyze the intent of the User.**","09850a93":"# [Conclusion](https:\/\/www.kaggle.com\/kshitijmohan\/sentiment-analysis-universal-sentence-encoder-91)\n**We have tried to implement the multi-label classification model using the almighty BERT pre-trained model. As we have shown the outcome is really state-of-the-art on a well-known published dataset. We were able to build a world class model that can be used in production for various industries, especially in customer service.**","3cc5551d":"# [Wine Recommendation based on BERT](https:\/\/www.kaggle.com\/kshitijmohan\/sentiment-analysis-universal-sentence-encoder-91)\n![](https:\/\/lh3.googleusercontent.com\/KdemIpKRxTQn8M6GGpT0qp9SZgfNBQbj1Ygll7cDFcKQUDc9RfSd9QdB7RW7wYrUZa277nMTJA=w640-h400-e365)","da1b0c53":"# [Why use BERT???](https:\/\/www.kaggle.com\/kshitijmohan\/sentiment-analysis-universal-sentence-encoder-91)\n* **Proper language representation is key for general-purpose language understanding by machines. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. For example, the word \u201cbank\u201d would have the same representation in \u201cbank deposit\u201d and in \u201criverbank\u201d. Contextual models instead generate a representation of each word that is based on the other words in the sentence. BERT, as a contextual model, captures these relationships in a bidirectional way. BERT was built upon recent work and clever ideas in pre-training contextual representations including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, the OpenAI Transformer, ULMFit and the Transformer. Although these models are all unidirectional or shallowly bidirectional, BERT is fully bidirectional.**\n* **We will use BERT to extract high-quality language features from the Wine review data, and fine-tune BERT on a specific task (classification) with own data to produce state of the art predictions.**","aa40fd8e":"# [Try it out yourself:](https:\/\/www.kaggle.com\/kshitijmohan\/sentiment-analysis-universal-sentence-encoder-91)","e0ccfbd5":"# [Confusion Matrix](http:\/\/)","420f6e71":"# [Final Preprocessing:](http:\/\/)\n![](https:\/\/miro.medium.com\/max\/1050\/1*sqwdLJj2HlRPkkOmyrKbtw.png)"}}