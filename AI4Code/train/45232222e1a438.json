{"cell_type":{"deb5002e":"code","7e42e3d4":"code","56942352":"code","c572a4a8":"code","74be6b8c":"code","ab78a666":"code","ce687f9f":"code","7add56bb":"code","cbae28d4":"code","54ba1307":"code","29049280":"code","f4a3b1ee":"code","a78283eb":"code","172e8173":"code","48cf3d81":"code","22903c9c":"code","cba9b300":"code","8034b4a3":"code","04221abb":"code","204808c0":"code","5ee84454":"code","d50b1bb0":"code","ffc4ff6c":"code","9ff01319":"code","b9c60eaf":"code","70cacdbf":"code","2d28cd4a":"code","63af699f":"code","ae344a10":"code","aed5fb28":"code","7a0de12b":"code","aaa6d726":"code","23db0ed4":"code","5407b2b9":"code","e04f4021":"code","672de1a6":"code","e9d58244":"code","d7e4d8c2":"code","a2034cd4":"code","7e126aa4":"code","d378436a":"code","89ab982e":"code","d7206736":"code","938e7c31":"code","fe6baa0b":"code","8bb5a702":"code","afebd633":"code","10f148e2":"code","78935062":"code","c786917f":"code","f60ffc9a":"code","b00c1de2":"code","fd838f1d":"code","a9ffd860":"code","b1f67f49":"code","d45d15fb":"code","1239635e":"code","2f880d76":"code","10c71555":"code","eaf7cd18":"code","9c3e382c":"code","d0adbcb7":"code","7171284d":"code","74d2872d":"code","2e3b0e43":"code","2e1ec0c1":"code","515cc0f5":"code","9d249f8c":"code","6021cc95":"code","1a8761f8":"code","ba704707":"code","a03e1108":"code","a58da888":"code","8c2359f4":"code","1d4a7090":"code","df515b24":"code","de1e87f1":"code","aef0194f":"code","86bbe2b5":"code","732a096e":"code","8cf0036c":"code","1759b27e":"code","b80f8edc":"code","0ae20481":"code","5d19b71a":"code","0189d30e":"code","d98ca14b":"code","edd7d27c":"code","22c4d26d":"code","bb36ce88":"code","4dc049f5":"code","0e23680a":"code","dc815366":"code","fcd31e8f":"code","7c74768c":"code","eb29dd0f":"markdown","118f0639":"markdown","f10d7dda":"markdown","6384d43a":"markdown","e269db5b":"markdown","e50fb3c8":"markdown","2ba72b50":"markdown","0abb34c5":"markdown","a810fd58":"markdown","076b6508":"markdown","e9ca20ff":"markdown","248b0292":"markdown","f05abc21":"markdown","ee2fe9a1":"markdown","4938bbe5":"markdown","d374fadc":"markdown","08405f88":"markdown","d7a73bb6":"markdown","7843c2a9":"markdown","18ea0fe7":"markdown","ac57ddba":"markdown","b835b814":"markdown","b703d28e":"markdown","7bd79d90":"markdown","76f5de5b":"markdown","5c6d38af":"markdown","2b531a63":"markdown","b9697de3":"markdown","caaf677f":"markdown","7b7bb92a":"markdown","a48c1383":"markdown","23c1a6c6":"markdown","0439d4d4":"markdown","7bea1b71":"markdown","c5b937de":"markdown","fd1724e2":"markdown","22b34ec8":"markdown","5ea7e940":"markdown","7f776b44":"markdown","00448d89":"markdown","458835a4":"markdown","05daeeb4":"markdown","1e4a15e9":"markdown","1b886ed0":"markdown","4fce0d48":"markdown","6ff0768e":"markdown","09200e44":"markdown","7091b069":"markdown","8295906b":"markdown"},"source":{"deb5002e":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\n%matplotlib inline\nwarnings.simplefilter(action='ignore')\npd.set_option('display.max_columns', None)","7e42e3d4":"properties_2016 = pd.read_csv('..\/input\/zillow-prize-1\/properties_2016.csv')\ntrain_2016 = pd.read_csv('..\/input\/zillow-prize-1\/train_2016_v2.csv')","56942352":"# Shape of the datasets\nprint('Shape of properties_2016: ', properties_2016.shape)\nprint('Shape of train_2016: ', train_2016.shape)","c572a4a8":"properties_2016.head()","74be6b8c":"train_2016.head()","ab78a666":"# Reforming Dataset\n\nfinal_df = properties_2016.copy()\nfinal_df = final_df.merge(train_2016,on = 'parcelid',how = 'inner')\nfinal_df.head()","ce687f9f":"final_df.shape","7add56bb":"# Creating a copy of the dataset\nzillow_df = final_df.copy()","cbae28d4":"zillow_df.columns","54ba1307":"zillow_df.info()","29049280":"zillow_df.describe()","f4a3b1ee":"mis_val = [var for var in zillow_df.columns if zillow_df[var].isnull().sum()>0]\n\nzillow_df[mis_val].isnull().sum()","a78283eb":"# Missing Values Percentage in total data\nzillow_df.isnull().mean()*100","172e8173":"def analyze_missing_values(df, var):\n    new_df = df.copy()\n    \n    new_df[var] = np.where(new_df[var].isnull(), 1, 0)  \n    # here 1 represents that there is missing value and 0 represents no missing values\n    \n    new_df.groupby(var)['logerror'].median().plot.bar()\n    \n    plt.title(var)\n    plt.show()\n    \nfor var in mis_val:\n    analyze_missing_values(zillow_df, var)\n    \n","48cf3d81":"print(\"Total no of variables with missing value in the dataset: \", len(mis_val))","22903c9c":"num_vars = [var for var in zillow_df.columns if zillow_df[var].dtypes!= 'O'] # Non-Object Variables\nprint(\"Total no.of numerical variables in the dataset: \", len(num_vars))","cba9b300":"zillow_df[num_vars].head()","8034b4a3":"cat_vars = [var for var in zillow_df.columns if zillow_df[var].dtypes == 'O'] # Object Variables\nprint(\"Total no.of categorical variables in the dataset: \", len(cat_vars))","04221abb":"zillow_df[cat_vars].head()","204808c0":"for var in cat_vars:\n    print(var,'-->', len(zillow_df[var].unique()), \"Categories\")","5ee84454":"### Analyzing Rarely occuring Labels\n\ndef analyze_rare_labels(df, var, rare_percentage):\n    new_df = df.copy()\n    \n    # determine the % of observations per category\n    tmp = new_df.groupby(var)['logerror'].count() \/ len(new_df)\n    \n    # return categories that are rare\n    return tmp[tmp < rare_percentage]\n\n# print categories that are present in less than 1% of the observations\nfor var in cat_vars:\n    print(analyze_rare_labels(zillow_df, var, 0.01))\n    print()","d50b1bb0":"year_var = [var for var in num_vars if 'Yr' in var or 'year' in var ]\nlen(year_var), year_var","ffc4ff6c":"new_df = zillow_df.copy()\n\n# Difference between year variable and year the house was sold\nnew_df[var] = new_df['assessmentyear'] - new_df['yearbuilt']    \n\nplt.scatter(new_df[var],new_df['logerror'])\nplt.xlabel('Year Difference')\nplt.ylabel('Logerror')\nplt.title('Year Difference vs Logerror')","9ff01319":"discrete_vars = [var for var in num_vars if len(zillow_df[var].unique()) < 20]\n\nprint('No.of discrete variables: ', len(discrete_vars))","b9c60eaf":"zillow_df[discrete_vars].head()","70cacdbf":"def analyze_disc_vars(df,var):\n    \n    new_df = zillow_df.copy()\n    new_df[var].hist(bins=20)\n    plt.xlabel(var)\n    plt.ylabel('Count')\n    plt.title('Variation of '+ var)\n    plt.show()\n    \nfor var in discrete_vars:\n    analyze_disc_vars(zillow_df,var)","2d28cd4a":"cont_vars = [var for var in num_vars if var not in discrete_vars and year_var]\nprint('No of continuous variables : ', len(cont_vars))","63af699f":"def analyze_cont_vars(df,var):\n    \n    new_df = zillow_df.copy()\n    new_df[var].hist(bins=20)\n    plt.xlabel(var)\n    plt.ylabel('Count')\n    plt.title('Variation of '+ var)\n    plt.show()\n    \nfor var in cont_vars:\n    analyze_cont_vars(zillow_df,var)","ae344a10":"# log Transformation of variables\n\ndef analyze_log_variation(df,var):\n    \n    new_df = df.copy()\n    if 0 in new_df[var].unique():\n        pass                 \n    # Hence log(0) = undefined\n\n    else:\n        \n        new_df[var] = np.log(new_df[var])\n        new_df[var].hist(bins=20)\n        plt.xlabel(var)\n        plt.ylabel('Count')\n        plt.title('Logarithmic Variation of '+ var)\n        plt.show()\n        \nfor var in cont_vars:\n    analyze_log_variation(zillow_df,var)","aed5fb28":"def analyze_outliers(df,var):\n    \n    new_df = df.copy()\n    \n    if 0 in new_df[var].unique():\n        pass\n    \n    else:\n        new_df[var] = np.log(df[var])\n        new_df.boxplot(column=var)\n        plt.tight_layout()\n        plt.show()\n        \nfor var in cont_vars:\n    analyze_outliers(zillow_df,var)\n    print('\\n')","7a0de12b":"zillow_df.to_csv('zillow_initial_dataset.csv', index=False)","aaa6d726":"# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport math as mt\nimport matplotlib.pyplot as plt\nimport warnings\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline\npd.set_option('display.max_columns',None)\nwarnings.simplefilter(action='ignore')","23db0ed4":"# Loading the dataset\ndf = pd.read_csv('.\/zillow_initial_dataset.csv')\ndf_copy = df.copy()\ndf.shape","5407b2b9":"df.head()","e04f4021":"duplicate = df[df.duplicated('parcelid')]\nduplicate.shape","672de1a6":"df.drop_duplicates(subset =\"parcelid\", keep = 'first', inplace = True)\ndf.shape","e9d58244":"df.isnull().mean()*100","d7e4d8c2":"def drop_columns_with_max_missing_values(df):\n    mis_var = [var for var in df.columns if df[var].isnull().sum() > 0]\n    df[mis_var].isnull().sum()\n\n    limit = np.abs((df.shape[0] * 0.6))\n    var_to_be_dropped = [var for var in mis_var if df[var].isnull().sum() > limit]\n    \n    print('Columns with more than 60% missing values: \\n', var_to_be_dropped)\n    print('\\n---- Dropping columns with more than 60% missing values ----\\n')\n\n    df.drop(columns=var_to_be_dropped, axis=1, inplace=True)\n    \n    print('Remaining columns are: \\n', df.columns)\n    return df\n\ndf = drop_columns_with_max_missing_values(df)","a2034cd4":"df.shape","7e126aa4":"df.head()","d378436a":"df['yeardifference'] = df['assessmentyear'] - df['yearbuilt']\ndf.head()","89ab982e":"df.drop(columns=['assessmentyear', 'yearbuilt', 'transactiondate'], axis=1, inplace=True)\ndf.head()","d7206736":"df[['latitude', 'longitude']] = (df[['latitude', 'longitude']])\/(10**6)\n\ndf['censustractandblock'] = (df['censustractandblock'])\/(10**12)\n\ndf['rawcensustractandblock'] = (df['rawcensustractandblock'])\/(10**6)\n\ndf.head()","938e7c31":"df.isnull().sum()","fe6baa0b":"def replace_missing_data(df, mis_vars):\n    print('##### Replacing missing values with mode of features #####')\n    for var in mis_vars:\n        df[var] = df[var].fillna(df[var].mode()[0])\n    return df\n\nmis_var = [var for var in df.columns if df[var].isnull().sum() > 0]\ndf = replace_missing_data(df, mis_var)\ndf.head()","8bb5a702":"df.isnull().mean()*100","afebd633":"cat_vars = [var for var in df.columns if df[var].dtypes == 'O']\ncat_vars","10f148e2":"def encode_categorical_variables(df, cat_vars):\n    \n    print('Categorical variables: ', cat_vars)\n    \n    for i in range(len(cat_vars)):\n        var = cat_vars[i]\n        var_le = LabelEncoder()\n        var_labels = var_le.fit_transform(df[var])\n        var_mappings = {index: label for index, label in enumerate(var_le.classes_)}\n        \n        df[(var + '_labels')] = var_labels\n        df.drop(columns=var, axis=1, inplace=True)\n        \n    return df\n\ndf = encode_categorical_variables(df, cat_vars)\ndf.head(10)","78935062":"df.shape","c786917f":"# Using Z-score as a threhold (WE can also find outliers using IQR and boxplot)\n\nz = np.abs(stats.zscore(df))\nno_out_df = df[(z<3).all(axis=1)]\nno_out_df.shape","f60ffc9a":"correlation = no_out_df.corr()","b00c1de2":"plt.figure(figsize=(20,12))\nsns.heatmap(correlation, cmap='BrBG', annot=True)","fd838f1d":"no_out_df.drop(columns=['calculatedbathnbr', 'calculatedfinishedsquarefeet', 'structuretaxvaluedollarcnt', \n                        'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'fullbathcnt'], axis=1, inplace=True)\n\nplt.figure(figsize=(20,12))\nsns.heatmap(no_out_df.corr(), cmap='BrBG',annot=True)","a9ffd860":"no_out_df.shape","b1f67f49":"no_out_df.head()","d45d15fb":"no_out_df.drop(columns=['censustractandblock', 'propertycountylandusecode_labels'], axis=1, inplace=True)\nno_out_df.shape","1239635e":"# Saving final Dataset\n\nno_out_df.to_csv('final_zillow_dataset.csv', index=False)","2f880d76":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport xgboost\n\nfrom math import sqrt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Ridge, Lasso, SGDRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\n%matplotlib inline\npd.set_option('display.max_columns',None)\nwarnings.simplefilter(action='ignore')","10c71555":"df = pd.read_csv('.\/final_zillow_dataset.csv')\ndf.shape","eaf7cd18":"df.head()","9c3e382c":"X = df.drop('logerror', axis=1)\ny = df['logerror']\n\nnew_df = df.copy()\n\nprint(X.shape, y.shape)","d0adbcb7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 23)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","7171284d":"train_vars = [var for var in X_train.columns if var not in ['parcelid', 'logerror']]\nlen(train_vars)","74d2872d":"scaler = StandardScaler()\n\nscaler.fit(X_train[train_vars]) \n\nX_train[train_vars] = scaler.transform(X_train[train_vars])\n\nX_test[train_vars] = scaler.transform(X_test[train_vars])","2e3b0e43":"X_train.head()","2e1ec0c1":"X_train_new = X_train.copy()\nX_test_new = X_test.copy()\n\nX_train.drop(columns='parcelid', axis=1, inplace=True)\nX_test.drop(columns='parcelid', axis=1, inplace=True)","515cc0f5":"linear_reg = LinearRegression()\n\nlinear_reg.fit(X_train, y_train)","9d249f8c":"linear_reg_pred = linear_reg.predict(X_test)\n\nprint('Mean Absolute Error : {}'.format(mean_absolute_error(y_test, linear_reg_pred)))\nprint()\nprint('Mean Squared Error : {}'.format(mean_squared_error(y_test, linear_reg_pred)))\nprint()\nprint('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(y_test, linear_reg_pred))))","6021cc95":"elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5) # L1 reguralization\n\nelastic_net.fit(X_train, y_train)","1a8761f8":"elastic_net_pred = elastic_net.predict(X_test)\n\nprint('Mean Absolute Error : {}'.format(mean_absolute_error(y_test, elastic_net_pred)))\nprint()\nprint('Mean Squared Error : {}'.format(mean_squared_error(y_test, elastic_net_pred)))\nprint()\nprint('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(y_test, elastic_net_pred))))","ba704707":"ridge_reg = Ridge(alpha=1, solver='cholesky')\n\nridge_reg.fit(X_train, y_train)","a03e1108":"ridge_reg_pred = ridge_reg.predict(X_test)\n\nprint('Mean Absolute Error : {}'.format(mean_absolute_error(y_test, ridge_reg_pred)))\nprint()\nprint('Mean Squared Error : {}'.format(mean_squared_error(y_test, ridge_reg_pred)))\nprint()\nprint('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(y_test, ridge_reg_pred))))","a58da888":"lasso_reg = Lasso(alpha=0.1)\n\nlasso_reg.fit(X_train, y_train)","8c2359f4":"lasso_reg_pred = lasso_reg.predict(X_test)\n\nprint('Mean Absolute Error : {}'.format(mean_absolute_error(y_test, lasso_reg_pred)))\nprint()\nprint('Mean Squared Error : {}'.format(mean_squared_error(y_test, lasso_reg_pred)))\nprint()\nprint('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(y_test, lasso_reg_pred))))","1d4a7090":"xgb_reg = xgboost.XGBRegressor()\n\nxgb_reg.fit(X_train, y_train)","df515b24":"xgb_reg_pred = xgb_reg.predict(X_test)\n\nprint('Mean Absolute Error : {}'.format(mean_absolute_error(y_test, xgb_reg_pred)))\nprint()\nprint('Mean Squared Error : {}'.format(mean_squared_error(y_test, xgb_reg_pred)))\nprint()\nprint('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(y_test, xgb_reg_pred))))","de1e87f1":"adaboost_reg = AdaBoostRegressor()\n\nadaboost_reg.fit(X_train, y_train)","aef0194f":"adaboost_reg_pred = adaboost_reg.predict(X_test)\n\nprint('Mean Absolute Error : {}'.format(mean_absolute_error(y_test, adaboost_reg_pred)))\nprint()\nprint('Mean Squared Error : {}'.format(mean_squared_error(y_test, adaboost_reg_pred)))\nprint()\nprint('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(y_test, adaboost_reg_pred))))","86bbe2b5":"gb_reg = GradientBoostingRegressor()\n\ngb_reg.fit(X_train, y_train)","732a096e":"gb_reg_pred = gb_reg.predict(X_test)\n\nprint('Mean Absolute Error : {}'.format(mean_absolute_error(y_test, gb_reg_pred)))\nprint()\nprint('Mean Squared Error : {}'.format(mean_squared_error(y_test, gb_reg_pred)))\nprint()\nprint('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(y_test, gb_reg_pred))))","8cf0036c":"tree_reg = DecisionTreeRegressor(max_depth=5)\n\ntree_reg.fit(X_train, y_train)","1759b27e":"tree_reg_pred = tree_reg.predict(X_test)\n\nprint('Mean Absolute Error : {}'.format(mean_absolute_error(y_test, tree_reg_pred)))\nprint()\nprint('Mean Squared Error : {}'.format(mean_squared_error(y_test, tree_reg_pred)))\nprint()\nprint('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(y_test, tree_reg_pred))))","b80f8edc":"forest_reg = RandomForestRegressor(n_estimators= 500, max_depth=6)\n\nforest_reg.fit(X_train, y_train)","0ae20481":"forest_reg_pred = forest_reg.predict(X_test)\n\nprint('Mean Absolute Error : {}'.format(mean_absolute_error(y_test, forest_reg_pred)))\nprint()\nprint('Mean Squared Error : {}'.format(mean_squared_error(y_test, forest_reg_pred)))\nprint()\nprint('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(y_test, forest_reg_pred))))","5d19b71a":"scores = cross_val_score(forest_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = 5)","0189d30e":"forest_reg_rmse_scores = np.sqrt(-scores)\nforest_reg_rmse_scores","d98ca14b":"param_grid = [\n    {'n_estimators': [300, 400, 500], 'max_features': [2, 4, 6]},\n    {'bootstrap': [False], 'n_estimators': [300, 400, 500], 'max_features': [2, 4, 6]}]\n\nforest_regressor = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_regressor, param_grid, scoring='neg_mean_squared_error',return_train_score=True,cv=3)","edd7d27c":"grid_search.fit(X_train, y_train)","22c4d26d":"grid_search.best_params_","bb36ce88":"grid_search.best_estimator_","4dc049f5":"final_predictor = grid_search.best_estimator_\nfinal_predictor.fit(X_train, y_train)\nfinal_pred = final_predictor.predict(X_test)","0e23680a":"print('Mean Absolute Error : {}'.format(mean_absolute_error(y_test, final_pred)))\nprint()\nprint('Mean Squared Error : {}'.format(mean_squared_error(y_test, final_pred)))\nprint()\nprint('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(y_test, final_pred))))","dc815366":"# saving the model\nimport pickle\nfile_name = 'final_pickle_model.pickle'\npickle.dump(final_predictor,open(file_name,'wb'))","fcd31e8f":"feature_importances = grid_search.best_estimator_.feature_importances_\n\nattrs = list(df.select_dtypes(include = ['float64','int64']))\n\nsorted(zip(attrs, feature_importances), reverse=True)","7c74768c":"model_pred = pd.DataFrame({'parcelid':X_test_new.parcelid, 'logerror':final_pred})\nmodel_pred.to_csv('model_predictions.csv',index=False)\nmodel_pred.head()","eb29dd0f":"#### Observation:\nWe can see that there are outliers in some of the variables and we will remove those outliers with feature engineering and bulid a model on top of it","118f0639":"### Distribution of Continuous variables","f10d7dda":"#### Observation:\nWe can see that parcelid is the unique identifier between the 2 tables. Therefore we will merge the 2 tables based on parcelid to form our complete datatset and then go ahead with other EDA techniques.","6384d43a":"### Checking for Missing values","e269db5b":"### Re-Examining Dataset","e50fb3c8":"### Feature Scaling","2ba72b50":"## Decision Tree Regressor","0abb34c5":"### Dataset Information:","a810fd58":"## Feature importance","076b6508":"# Buliding a Regression Model \n                                                                                        \nWe have done EDA and Feature Engineering on raw dataset and got final dataset with neccessary changes(removing duplicates, scaling the data, Multicolinearity, replacing null values, Removing outliers)","e9ca20ff":"#### Relationship between Temporal variables and Log error","248b0292":"### Dropping parcelid as it does not serve purpose for model prediction ","f05abc21":"#### Observation:\nWe can see lot of missing values are there in multiple columns. Therefore, in order to avoid creating synthetic data for columns which have more than 60% missing values, we will drop those columns(>60% null values) and perform feature engineering on the rest of the data.\n\n### Dropping columns having more than 60% Missing values \/ Null values","ee2fe9a1":"### Distribution of Discrete variables","4938bbe5":"#### Observation:\n\nSome of the variables are not normally distributed.\n\nEvaluate if a logarithmic transformation of the variables returns values that will follow normal distribution","d374fadc":"# Exploring Data Analysis\n\nThe above reformed dataset contains 90275 rows and 60 columns.\n\nLet's analyse the dataset to identify the following:\n\n1. Missing values\n2. Distribution of the numerical variables\n3. Outliers\n4. Distribution of the categorical variables\n5. Potential relationship between the variables and the target","08405f88":"## Gradient Boosting Regression Model","d7a73bb6":"## Ada Boost Regression Model","7843c2a9":"## Saving the dataset for Feature Engineering","18ea0fe7":"### Capture elapsed time","ac57ddba":"#### Observation:\nWe see that we get a better spread of the values for only few variables when we use the logarithmic transformation. \n\n### Analyzing Outliers","b835b814":"# Conclusion\n\n1. I have performed all the feature engineering steps necessary to ensure the dataset is ready to be fed into Machine Learning algorithms. \n\n2. After Pre-processing and Feature Engineering the raw dataset we splitted the dataset into train and test sets.\n\n3. Performed Feature scaling on data for better performance.\n\n4. Trained multiple models using different ML regression algorithms on dataset.\n\n5. Appleied Performance metrics such as MAE, MSE, RMSE to find out best prediction model.\n\n6. With the help of GridSearch CV we found out best estimator with least Root mean squred error. \n\n7. Saved best predictor in .pickle format for future predictions.\n\n8. Done prediction on test data and saved predictions into .csv file.","b703d28e":"## Duplicate Id check and removal","7bd79d90":"## Saving Predictions","76f5de5b":"## Transform incorrectly scaled variables\n\n1. As mentioned in the data dictionary, the latitude and longitude has been multiplied to 10^6.\n\n2. I will perform a division of the same to scale it to original.\n\n3. I will perform a similar operation for 'rawcensustractandblock'.\n\n4. Also, I will perform a division of 10^12 for the 'censustractandblock' variable to bring it to original scale.","5c6d38af":"## Random Forest Regression Model","2b531a63":"## Lasso Regression Model","b9697de3":"### Relationship between missing values and log error (Target Variable)","caaf677f":"#### Observation:\n1. Due to repetitive data, we will drop 1 of rawcensustractandblock and censustractandblock.\n2. The propertycountylandusecode_labels will serve no useful purpose for our model prediction.\n3. parcelid will also serve no use for our prediction.\n4. Therefore, we will drop those columns and build a model on top of this data","7b7bb92a":"### Handling missing values:\n\nTo deal with missing values in numerical variables, I will replace the missing values in the original variable with the mode","a48c1383":"# Feature Engineering\n                                                                 \nI will pre-process the variables of the Dataset and do feature engineering therefore we tackle following challenges:\n\n* Duplicate check\n* Missing values\n* Temporal variables\n* Selective transformation of incorrectly scaled values\n* Standarise the values of all variables to the same range (Standardization)\n* Encoding of Categorical variables\n* Forming new meaningful features if needed\n* Dropping redundant columns\n* Multi-Colinearity check and removal\n* Outlier check and removal","23c1a6c6":"### Splitting the dataset into Train-Test sets","0439d4d4":"## Elastic Net Model","7bea1b71":"### Distribution of  Categorical variables","c5b937de":"### Distribution of Numerical variables","fd1724e2":"## Ridge Regression Model","22b34ec8":"### Encoding categorical variables (Transforming Categorical Variables to Numerical Variables)","5ea7e940":"#### Observation:\nWe observe high correlation among some of the feature variables from the above correlation plot. Therefore we will drop some of the features and then build our model on top of that.","7f776b44":"## XG Boost Regression Model","00448d89":"### Importing Libraries & Loading Dataset:","458835a4":"### Checking for Multi-Colinearity","05daeeb4":"### Checking Outliers & Removing them","1e4a15e9":"## Linear Regression Model","1b886ed0":"#### Observation:\n\nWe can see that there are 125 rows of duplicate data with respect to the same parcelid. Therefore, we will drop them.","4fce0d48":"# Zillow Zestimate : Home Value Prediction - EDA\n                                            \n                                                                   Code Written By : Dasari Mohana\n                                                                                \n## Business Objective:\n\nBuying a house that suits their choices is every person's desire, and it is thus known as their dream house. One considers several aspects while purchasing a home, starting from the budget, the location, the number of rooms available, and many more. But how to find a house that satisfies one's requirements?  This is not a quick and easy task.\n\nBut no need to worry; homebuyers can nowadays find their dream home with a click of a button. Zillow is a popular estimator for house evaluation available online. It is considered one of the top real estate marketplaces for buying a house in the United States. Zillow's Zestimate allows the homebuyers to search for a home that satisfies their location, area, budget, etc.\n\nThe Zillow Zestimate provides the homebuyers with information on the actual worth of the house based on public data. The accuracy of the Zestimate information depends on the location and availability of the data of a specific area. Hence the more data available, the more is the accuracy of the Zestimate.       \n\n\n## Aim:\n\nTo predict the sale prices of the houses and improve the log error i.e. the error due to the difference between the actual and the predicted home values.\n\n## My Approach\n\n1. Importing the required libraries and reading the dataset.\n\n\ta.Merging of the two datasets\n\t\n\tb.Understanding the dataset\n\t\n2. Exploratory Data Analysis (EDA) \u2013\n\n\ta.Data Visualization\n\t\n3. Feature Engineering\n\n\ta.Duplicate value removal\n\t\n\tb.Missing value imputation\n\t\n\tc.Rescaling of incorrectly scaled data\n\t\n\td.Standardization\n\t\n\te.Encoding of categorical variables\n\t\n\tf.Generation of new feature wherever required.\n\t\n\tg.Dropping of redundant feature columns\n\t\n\th.Checking for multi-collinearity and removal of highly correlated features\n\t\n\ti.Check for the outliners and removal of outliers.\n\t\n4. Model Building\n\n\ta.Performing train test split\n\t\n\tb.Feature Scaling\n\t\n\tc.Dropping features if necessary\n\t\n\td.Linear Regression Model\n\t\n\te.Elastic Net\n\t\n\tf.Ridge Regression\n\t\n\tg.Lasso Regressor\n\t\n\th.XGBoost Regressor\n\t\n\ti.Adaboost Regressor\n\t\n\tj.Gradient Boosting Regressor\n\t\n\tk.Decision Tree Regressor\n\t\n\tl.Random Forest Regressor\n\t\n5. Model Validation\n\n\ta.Mean Absolute Error\n\t\n\tb.Mean Squared Error\n\t\n\tc.Root Mean Squared Error\n\t\n6. Hypermeter Tuning (GridSearchCV)\n\n\ta.For Random Forest Regressor\n\t\n7. Checking for Feature Importance\n\n8. Creating the final model and making predictions\n\n9. Conclusion\n","6ff0768e":"## Cross Validation & Hyperparameter Optimization for Random Forest","09200e44":"### Feature\tDescription:\n![image.png](attachment:image.png)","7091b069":"###  Missing Values","8295906b":"#### Temporal variables assosciated with time"}}