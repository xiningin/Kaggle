{"cell_type":{"f2448b4d":"code","d5b06d1a":"code","7507751d":"code","7e455779":"code","c0caa7a6":"code","88a4a72c":"code","b7cf1519":"code","63c3d443":"code","8c4f7e0a":"code","14c3f1d4":"code","3ddc84f5":"code","b888ac78":"code","15bf6785":"code","f3cbf7ea":"code","dbb13c82":"code","9d58186e":"code","5af6dc2f":"code","073055b8":"code","e22a1b15":"code","51d4eab5":"code","5432e84a":"code","526b28e0":"code","30119d85":"code","59ee180b":"code","3814be21":"code","977e30e4":"code","3007c740":"code","26a36220":"code","6d94ebcb":"code","62a8441c":"code","fdb24cd6":"code","855f2257":"code","9159b475":"code","f8539574":"code","b2090022":"code","f6c72bf0":"code","07116cb0":"code","a04b3846":"code","e5a97af2":"code","2d74b8f3":"code","9b8553d5":"code","e01ab044":"code","ffa44a24":"code","25f0b00e":"markdown","6ff60494":"markdown","da676d5b":"markdown","03e79e3d":"markdown","bc7b9f58":"markdown","ab30bc4b":"markdown","2e6e9825":"markdown","3b4ef444":"markdown","abab3d85":"markdown","28322fb6":"markdown","047fd2fc":"markdown","ff01e7c6":"markdown","24adfb3f":"markdown","f3bd9432":"markdown","20057618":"markdown","a2d5db4f":"markdown","03f25592":"markdown","5c11e31f":"markdown","a0fed9bc":"markdown","354501d8":"markdown","1573acc5":"markdown","aed90175":"markdown","c0e907dd":"markdown","45da2961":"markdown","072c50e1":"markdown","6328bb12":"markdown","e22ab60d":"markdown","401deb7d":"markdown","c123d699":"markdown","248419f1":"markdown","797dae82":"markdown","5be7e6dd":"markdown","f10dd1ec":"markdown","00ff1380":"markdown","19119ef7":"markdown","b15a6fda":"markdown","264c6d0b":"markdown","b6573200":"markdown","110d3081":"markdown","63915020":"markdown","2f5e15a9":"markdown","15966388":"markdown","152301e6":"markdown","0e6d4c75":"markdown","9f961343":"markdown","45142444":"markdown","18e21e66":"markdown"},"source":{"f2448b4d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scikitplot as skplt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import Counter\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer,ToktokTokenizer\nimport re\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,recall_score,precision_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom string import punctuation\nfrom nltk.stem import WordNetLemmatizer\nimport os\npd.set_option('display.float_format', lambda x: '%.3f' % x)","d5b06d1a":"data = pd.read_csv(\"\/kaggle\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\")\n","7507751d":"data.head(2)","7e455779":"data = data[['Review Text','Recommended IND']]","c0caa7a6":"data.shape","88a4a72c":"data.rename(columns={'Review Text':'review_text','Recommended IND':'recommended'},inplace=True)","b7cf1519":"data.head(2)","63c3d443":"data.isna().sum()","8c4f7e0a":"data['review_text'] = data['review_text'].fillna(' ')","14c3f1d4":"data.isna().sum()","3ddc84f5":"data.dtypes","b888ac78":"data.loc[data[\"recommended\"] == 0, \"recommended\"] = \"Not Recommended\" # 0 -> Not Recommended\ndata.loc[data[\"recommended\"] == 1, \"recommended\"] = \"Recommended\" # 1 -> Recommended","15bf6785":"data.dtypes","f3cbf7ea":"count_class=pd.value_counts(data[\"recommended\"], sort= True)\ncount_class.plot(kind= 'bar', color= [\"blue\", \"orange\"])\nplt.title('Proportion Target Class')\nplt.show()","dbb13c82":"print('Recommended', round(data['recommended'].value_counts()['Recommended']\/len(data) * 100), '%')\nprint('Not Recommended', round(data['recommended'].value_counts()['Not Recommended']\/len(data) * 100), '%')","9d58186e":"\ncount1 = Counter(\" \".join(data[data['recommended']== 'Recommended']['review_text']).\\\n                 split()).most_common(20)\ndf1 = pd.DataFrame.from_dict(count1)\n","5af6dc2f":"df1 = df1.rename(columns={0: \"common_words\", 1 : \"count\"})\ndf1.head(10)","073055b8":"count2 = Counter(\" \".join(data[data['recommended']== 'Not Recommended']['review_text']).\\\n                 split()).most_common(20)\ndf2 = pd.DataFrame.from_dict(count2)\n","e22a1b15":"df2 = df2.rename(columns={0: \"common_words\", 1 : \"count\"})\ndf2.head(10)","51d4eab5":"data.iloc[1,0]","5432e84a":"tokenizer=ToktokTokenizer()\nstopword_list=nltk.corpus.stopwords.words('english')","526b28e0":"main_text = data['review_text']\ntarget = data['recommended']\n\nprint(len(main_text))\nprint(len(target))","30119d85":"contractions_dict = {     \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I had\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"iit will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they had\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","59ee180b":"def expand_contractions(text, contractions_dict):\n    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n                                      flags=re.IGNORECASE | re.DOTALL)\n\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contractions_dict.get(match) \\\n            if contractions_dict.get(match) \\\n            else contractions_dict.get(match.lower())\n        expanded_contraction = expanded_contraction\n        return expanded_contraction\n\n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text\ndef cons(text):\n    text=expand_contractions(text,contractions_dict)\n    return text\n\nmain_text = main_text.apply(cons)","3814be21":"#Tolowercase\ndef to_lower(text):\n    return ' '.join([w.lower() for w in word_tokenize(text)])\n\nmain_text = main_text.apply(to_lower)","977e30e4":"#Define function for removing special characters\ndef remove_special_characters(text, remove_digits=True):\n    pattern=r'[^a-zA-z0-9\\s]'\n    text=re.sub(pattern,'',text)\n    return text\ndef strip_punctuation(s):\n    return ''.join(c for c in s if c not in punctuation)\n\nmain_text = main_text.apply(remove_special_characters)\nmain_text = main_text.apply(strip_punctuation)","3007c740":"from nltk.corpus import wordnet\n\ndef replaceElongated(word):\n    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n    repl = r'\\1\\2\\3'\n    if wordnet.synsets(word):\n        return word\n    repl_word = repeat_regexp.sub(repl, word)\n    if repl_word != word:      \n        return replaceElongated(repl_word)\n    else:       \n        return repl_word\nmain_text = main_text.apply(replaceElongated)","26a36220":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nmain_text = main_text.apply(lambda x: tokenizer.tokenize(x))\n","6d94ebcb":"def remove_stopwords(text):\n    words = [w for w in text if w not in stopword_list]\n    return words\n\nmain_text = main_text.apply(lambda x : remove_stopwords(x))","62a8441c":"from nltk.stem import SnowballStemmer\n\nsnowball_stemmer = SnowballStemmer('english')\n\ndef stem_update(text_list):\n    text_list_new = []\n    for word in text_list:\n        word = snowball_stemmer.stem(word)\n        text_list_new.append(word)\n    return text_list_new\nmain_text = main_text.apply(stem_update)\n","fdb24cd6":"def drop_numbers(list_text):\n    list_text_new = []\n    for i in list_text:\n        if not re.search('\\d', i):\n            list_text_new.append(i)\n    return ' '.join(list_text_new)\nmain_text = main_text.apply(drop_numbers)","855f2257":"df = pd.concat([main_text,target],axis=1)","9159b475":"df.head(2)","f8539574":"plt.figure(figsize = (15,15)) # Text that is recommended\nwc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(df[df.recommended == 'Recommended'].review_text))\nplt.imshow(wc , interpolation = 'bilinear')","b2090022":"plt.figure(figsize = (15,15)) # Text that is not recommended\nwc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(df[df.recommended == 'Not Recommended'].review_text))\nplt.imshow(wc , interpolation = 'bilinear')","f6c72bf0":"cv=CountVectorizer()\n\n\ntrain_data,test_data = train_test_split(df,train_size=0.8,random_state=0)\n\nX_train = cv.fit_transform(train_data['review_text'])\ny_train = train_data['recommended']\nX_test = cv.transform(test_data['review_text'])\ny_test = test_data['recommended']","07116cb0":"print(X_train.toarray())","a04b3846":"nb = MultinomialNB()\nnb.fit(X_train,y_train)\n","e5a97af2":"nb_predict=nb.predict(X_test)","2d74b8f3":"nb_report = accuracy_score(y_test,nb_predict)\nprint('Accuracy:',nb_report)","9b8553d5":"nb_report1 = recall_score(y_test,nb_predict,pos_label='Recommended',average='binary')\nprint('Recall:',nb_report1)","e01ab044":"nb_report2 = precision_score(y_test,nb_predict,pos_label='Recommended',average='binary')\nprint('Precision:',nb_report2)","ffa44a24":"skplt.metrics.plot_roc(y_test, nb.predict_proba(X_test)) ","25f0b00e":"<h4>Multinomial Naive Bayes Modelling<\/h4>\n<div style=\"text-align:justify\">Naive Bayes is one of algorithms method based on applying Bayes theorem.Bayes theorem calculates probability P(c|x) where c is the class of the possible outcomes and x is the given instance which has to be classified. Herre below the formula of naive bayes:<\/div>","6ff60494":"<div style=\"text-align:justify\">\"Recommended\" value will be the main parameter in calculating Recall and Precision, sice we will analyze whether the product worth for buy or not.<\/div>\n<div style=\"text-align:justify\">From the result above, we get the value of Accuracy around in 0.89 which means that we have 89% level of confidence that the model was accurate.<\/div>\n<div style=\"text-align:justify\">And we have the <b>Recall<\/b> value around in 0.95 which means that we have 95% \"Recommended\" review which predicted correctly.<\/div>\n<div style=\"text-align:justify\">And we have the <b>Precision<\/b> value around in 0.92 which means that we have 92% \"Recommended\" review from predicted positive value which actually positive.<\/div>","da676d5b":"<h2>Data Wrangling<\/h2>","03e79e3d":"<h4>Matrix Evaluation<\/h4><br>\nAfter we get the result from our modelling, the next step is evaluate our model using matrix evaluation as follows:","bc7b9f58":"<div style=\"text-align:justify\">From the result above, most common words in target class variable contains so many stop words like \"the, was, in, it, is\" and etc. Which mean that the review_text still have untidy data. There are so many punctuation, constraction, and elongation words. Before we go to the modelling steps we need to removing those all, and make the text become tidy data text.<\/div>","ab30bc4b":"<h4>Read the data<\/h4>","2e6e9825":"<div style=\"text-align:justify\">Original data have several variable like Clothing ID, Age of reviewer, Title, Review Text, Rating, Recommended IND, Positive Feedback Count, Division Name, Department Name, and Class Name. But in this case, we only use Review Text and Recommended IND as a predictor and target class variable. So let's do filtering the variable: <\/div>","3b4ef444":"<h4>Get Dimension of the Data<\/h4>","abab3d85":"<h4>Tokenization<\/h4><br>\nTokenization is splitting sentences into smaller unit, such as terms or word. ","28322fb6":"<h2>Exploratory Data Analysis (EDA)<\/h2>","047fd2fc":"From the analysis above we can conclude that :<br>\n1. Model have a good performance with accuracy equal to 89% <br>\n2. Recall for \"Recommended\" values as positive target class have value equal to 0.95, this means that \"Recommended\" review text have 95% predicted correctly.\n3. Recall for \"Recommended\" values as positive target class have value equal to 0.92, this means that \"Recommended\" review text have 92% predicted positive value which actually positive.\n4. Model have RUC curve that closer to 1, this means that model have a good performance in True Positive Rate.\n5. Multinomial Naive Bayes is good classification model in sentiment analysis text. This model can accurately predict whether product recommended or not by analyze customers review.","ff01e7c6":"<h4>To lowercase<\/h4><br>\nChange all uppercase character to be lowercase character. For example \"Pretty\" to be \"pretty\" or \"BEAUTY\" to be \"beauty\"","24adfb3f":"The ROC curve is a graphical representation of the Recall-Precision pair corresponding to each particular decision threshold","f3bd9432":"# Sentiment Analysis from Women's Clothing Review using Multinomial Naive Bayes","20057618":"<h4>Drop Numbers<\/h4><br>\nRemove numbers from text, since numbers doesn't give much importance to get the main words.","a2d5db4f":"<h2>Text Mining<\/h2>","03f25592":"Where :<br>\n`TP (True Positive)` : Predicted positive and the actual's positive <br>\n`TN (True Negative)` : Predicted negative and the actual's negative <br>\n`FP (False Positive)` : Predicted positive but the actual's negative <br>\n`FN (False Negative)` : Predicted negative but the actual's positive <br>","5c11e31f":"<div style=\"text-align:justify\">Target class variable have imbalance proportion, where \"Recommended\" values more dominating with a proportion value of 82% and the rest \"Not Recommendation\" only 18%<\/div>","a0fed9bc":"<h4>Expanding Contraction<\/h4>","354501d8":"<div style=\"text-align:justify\">The next step is to create a numerical feature vector for each document and split them into train data and test data.<\/div>","1573acc5":"<h4>Most Common Words in Recommended Review<\/h4>","aed90175":"<h4>Remove Special Character and Punctuation<\/h4><br>\nRemoving all special character like .?\/@# etc","c0e907dd":"<h4>Check Most Common Words in Each Target Variable Values<\/h4>","45da2961":"<h4>Checking and Handling Missing Values<\/h4>","072c50e1":"From confusion Matrix above we can calculate best evaluation metric for our model: <br>\n<b>Accuracy :<\/b>Out of all the classes were predicted correctly.<br>\n<b>Recall :<\/b> Out of all the actual positive classes were predicted correctly.<br>\n<b>Precision :<\/b> Out of all the predicted positive classes were actually positive  ","6328bb12":"<b>Variable Explanation :<\/b><br>\n`Review Text` : Contains review from customers and will be used as predictor variable.<br>\n`Recommended IND` : Contains recommendation from customers, whether the product recommended or not, will be used as target variable.","e22ab60d":"![evalmetric2.png](attachment:evalmetric2.png)","401deb7d":"<div style=\"text-align:justify\">In this text mining process we will exploring and analyzing unstructured text data<\/div>  ","c123d699":"<h4>Stemming<\/h4><br>\nStemming is the process of reducing a word to its word stem. For example \"Consulting\" to be \"consult\"","248419f1":"<h4>Check Proportion Target Class Variable<\/h4>","797dae82":"![Capture.JPG](attachment:Capture.JPG)","5be7e6dd":"<h4>Renaming Target Variable Values<\/h4>\nInstead of using 0 and 1 as values of target variable, we can use more appropiate values like \"Not Recommended\" and \"Recommended\".","f10dd1ec":"<h4>Removing Stopwords<\/h4><br>\nRemove stopwords like \"is, the, with, etc\" since they don't have usefull information","00ff1380":"<h4>Most Common Words in Not Recommended Review<\/h4>","19119ef7":"<h4>Renaming Columns<\/h4>","b15a6fda":"<h2>Introduction<\/h2>","264c6d0b":"<h4>ROC Curve<\/h4>","b6573200":"<div style=\"text-align:justify\">Product user reviews hold an important role in E-commerce industry. Quality of the product can be measured by review given by customer. New costumer can decided whether she\/he buy the product or not by recommendation or review from previous customers. We can classify review from costumer into 2 class value, \"recommended\" means that the products are good and \"not recommended\" means that the products are bad.<\/div>\n<div style=\"text-align:justify\">Text classification aims to assign documents into one or many categories and one kind of the most useful text classification is Sentiment analysis. Sentiment analysis is objective to determine the writer\u2019s point of view about a particular topic, product, or service.<\/div>\n<div style=\"text-align:justify\">In this notebook we will determine whether the product recommended or not  using  sentiment analysis from customers review with Multinomial Naive Bayes Classification. The dataset used is a dataset from Woman Clhoting Review that can be accessed in (https:\/\/www.kaggle.com\/nicapotato\/womens-ecommerce-clothing-reviews).<\/div>","110d3081":"<div style=\"text-align:justify\">From 2 variable above, review_text have so many missing values. Removing those missing values can lead to lack of information from the data, so instead of remove the data, let's fill missing values with blank space.<\/div>","63915020":"<h4>Split the data and count the vectorize in each words<\/h4>","2f5e15a9":"<h4>Import some important packages<\/h4>","15966388":"<h2>Modelling using Multinomial Naive Bayes<\/h2>","152301e6":"<h4>Replace Elongated Words<\/h4><br>\nReplace all elongated words with appropriate words. For example \"soooooo\" to be \"so\" or \"looooong\" to be \"long\"\n","0e6d4c75":"<div style=\"text-align:justify\">The ROC below shown that curve closer to the top-left corner and it's indicate that the model have a better performance.<\/div>\n<div style=\"text-align:justify\">\"Not Recommended\" target class and \"Recommended\" target class have the same ROC values, it was equal to 0.91.  When the ROC value getting closer to 1, this mean that the model reach ideal point.<div>\n    \n    \n    \n    ","9f961343":"<div style=\"text-align:justify\">Contractions is kind of word like I'm, ain't, who's, etc. By expanding the contractions \"I'm\" will become \"I am\", \"ain't\" will become \"am not\", and so on.<\/div>","45142444":"<h2>Conclusion<\/h2>","18e21e66":"Where:<br>\n`P(A|B)` : measure of how often A and B are observed to occure together (posterior probability) <br>\n`P(B|A)` : measures of how often B occur in A (likelihood)<br>\n`P(A)` : measure of how often A is observed to occur in general (prior probability) <br>\n`P(B)` : measure of how often B is observed to occur in general (marginal likelihood)<br>"}}