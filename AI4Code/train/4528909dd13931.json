{"cell_type":{"c4791a1d":"code","844e9eed":"code","2fb107d4":"code","4583b5e7":"code","3e074b09":"code","b9a16201":"code","c03f93ce":"code","8a2d7d11":"code","0f42b0d9":"code","9ab02b82":"code","f9c08612":"code","4d2b2d74":"code","e14bd55d":"code","9c59eb23":"code","0cfa2a83":"code","6f72dbc1":"markdown","1b2ab5fb":"markdown","2fbde970":"markdown","5f0cf5d8":"markdown","7b02b4dd":"markdown","0a4a2c82":"markdown"},"source":{"c4791a1d":"# !pip install autograd --quiet","844e9eed":"import datetime\nimport pandas as pd\nfrom time import time\n# from autograd import grad\n# import autograd.numpy as np\nimport numpy as np\nfrom numba import njit\nfrom scipy.optimize import minimize, fsolve","2fb107d4":"# CPMP's logloss from https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/183010\ndef log_loss_numpy(y_pred):\n    y_true_ravel = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = np.where(y_true_ravel == 1, - np.log(y_pred), - np.log(1 - y_pred))\n    return loss.mean()\n\ndef func_numpy_metric(weights):\n    oof_blend = np.tensordot(weights, oof, axes = ((0), (0)))\n    return log_loss_numpy(oof_blend)\n\ndef grad_func(weights):\n    oof_clip = np.clip(oof, 1e-15, 1 - 1e-15)\n    gradients = np.zeros(oof.shape[0])\n    for i in range(oof.shape[0]):\n        a, b, c = y_true, oof_clip[i], np.zeros((oof.shape[1], oof.shape[2]))\n        for j in range(oof.shape[0]):\n            if j != i:\n                c += weights[j] * oof_clip[j]\n        gradients[i] = -np.mean((-a*b+(b**2)*weights[i]+b*c)\/((b**2)*(weights[i]**2)+2*b*c*weights[i]-b*weights[i]+(c**2)-c))\n    return gradients\n\n@njit\ndef grad_func_jit(weights):\n    oof_clip = np.minimum(1 - 1e-15, np.maximum(oof, 1e-15))\n    gradients = np.zeros(oof.shape[0])\n    for i in range(oof.shape[0]):\n        a, b, c = y_true, oof_clip[i], np.zeros((oof.shape[1], oof.shape[2]))\n        for j in range(oof.shape[0]):\n            if j != i:\n                c += weights[j] * oof_clip[j]\n        gradients[i] = -np.mean((-a*b+(b**2)*weights[i]+b*c)\/((b**2)*(weights[i]**2)+2*b*c*weights[i]-b*weights[i]+(c**2)-c))\n    return gradients","4583b5e7":"y_true = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv', index_col = 'sig_id').values\n\noof_dict = {'Model 1': '..\/input\/moa-oof-demo\/oof1.npy', \n            'Model 2': '..\/input\/moa-oof-demo\/oof2.npy', \n            'Model 3': '..\/input\/moa-oof-demo\/oof3.npy'\n           }\n\noof = np.zeros((len(oof_dict), y_true.shape[0], y_true.shape[1]))\nfor i in range(oof.shape[0]):\n    oof[i] = np.load(list(oof_dict.values())[i])","3e074b09":"%%time\n\nlog_loss_scores = {}\nfor n, key in enumerate(oof_dict.keys()):\n    score_oof = log_loss_numpy(oof[n])\n    log_loss_scores[key] = score_oof\n    print(f'{key} CV:\\t', score_oof)\nprint('-' * 50)","b9a16201":"test_weights = np.array([1 \/ oof.shape[0]] * oof.shape[0])","c03f93ce":"%timeit -r 10 grad_func(test_weights)","8a2d7d11":"%timeit -r 10 grad_func_jit(test_weights)","0f42b0d9":"tol = 1e-10\ninit_guess = [1 \/ oof.shape[0]] * oof.shape[0]\nbnds = [(0, 1) for _ in range(oof.shape[0])]\ncons = {'type': 'eq', \n        'fun': lambda x: np.sum(x) - 1, \n        'jac': lambda x: [1] * len(x)}\n\nprint('Inital Blend OOF:', func_numpy_metric(init_guess))\nstart_time = time()\nres_scipy = minimize(fun = func_numpy_metric, \n                     x0 = init_guess, \n                     method = 'SLSQP', \n                     jac = grad_func_jit, # grad_func \n                     bounds = bnds, \n                     constraints = cons, \n                     tol = tol)\nprint(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Optimised Blend OOF:', res_scipy.fun)\nprint('Optimised Weights:', res_scipy.x)","9ab02b82":"print('Check the sum of all weights:', np.sum(res_scipy.x))\nif np.sum(res_scipy.x) - 1 <= tol:\n    print('Great! The sum of all weights equals to 1!')\nelse:\n    print('Manual adjustion is needed to modify the weights.')","f9c08612":"# def Lagrange_func(params):\n#     w1, w2, w3, _lambda = params\n#     oof_blend = w1 * oof[0] + w2 * oof[1] + w3 * oof[2]\n#     return log_loss_numpy(oof_blend) - _lambda * (w1 + w2 + w3 - 1) ","4d2b2d74":"# grad_L = grad(Lagrange_func)","e14bd55d":"# def Lagrange_obj(params):\n#     w1, w2, w3, _lambda = params\n#     dLdw1, dLdw2, dLdw3, dLdlam = grad_L(params)\n#     return [dLdw1, dLdw2, dLdw3, w1 + w2 + w3 - 1]","9c59eb23":"# start_time = time()\n# w1, w2, w3, _lambda = fsolve(Lagrange_obj, [0.3, 0.3, 0.4, 1.0])\n# print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Optimised Weights:', [w1, w2, w3])\n# oof_b = w1 * oof[0] + w2 * oof[1] + w3 * oof[2]\n# print('Optimised Blend OOF:', log_loss_numpy(oof_b))","0cfa2a83":"# print('Check Condition (1a):', w1 + w2 + w3)\n# if w1 + w2 + w3 - 1 <= tol:\n#     print('Great! The sum of all weights equals to 1!')\n# else:\n#     print('Manual adjustion is needed to modify the weights.')","6f72dbc1":"# Model OOF Scores\n\nHere are my oof scores. You may use your own oof scores.","1b2ab5fb":"# Model Blending Weights Optimisation\n\nThis demo shows how to use [scipy.optimize][1] to optimise your model blending weights using your models' OOFs.\n\n**UPDATE:** Getting rid of the penalty term by using 'SLSQP' solver with a relatively small tolerance and Jacobian matrix.\n\n**UPDATE:** Calculate the gradients with paper and pencil to accelerate the optimisation...\n\n**UPDATE:** Add numba gradient function\n\n[1]: https:\/\/docs.scipy.org\/doc\/scipy\/reference\/optimize.html","2fbde970":"# Objective Function and Gradients\n\n$$\nF = -\\frac{1}{NM}\\sum_{m=1}^{M}\\sum_{i=1}^{N}\\left[ y_{i,m}{\\rm log}\\left( \\sum_{k=1}^{K}w_{k}\\hat{y}_{i,m,k} \\right) + \\left( 1 - y_{i,m} \\right) {\\rm log}\\left( 1 - \\sum_{k=1}^{K}w_{k}\\hat{y}_{i,m,k} \\right) \\right],\n$$\n\n$$\n\\frac{\\partial F}{\\partial w_{k}} = -\\frac{1}{NM}\\sum_{m=1}^{M}\\sum_{i=1}^{N}\\left[ \\frac{-y_{i,m}\\hat{y}_{i,m,k}+\\hat{y}_{i,m,k}^{2}w_{k}+\\hat{y}_{i,m,k}\\sum_{j=1, j\\neq k}^{K}\\left( w_{j}\\hat{y}_{i,m,j}\\right)}{\\hat{y}_{i,m,k}^{2}w_{k}^{2}+2\\hat{y}_{i,m,k}\\sum_{j=1, j\\neq k}^{K}\\left( w_{j}\\hat{y}_{i,m,j}\\right)w_{k}-\\hat{y}_{i,m,k}w_{k}+\\left(\\sum_{j=1, j\\neq k}^{K}\\left( w_{j}\\hat{y}_{i,m,j}\\right)\\right)^{2}-\\sum_{j=1, j\\neq k}^{K}\\left( w_{j}\\hat{y}_{i,m,j}\\right)} \\right], \\quad k = 1, ..., K.\n$$","5f0cf5d8":"# Blending Weights Optimisation\n\nProviding jac is optional because scipy uses its own 2-point finite difference estimation for the Jacobian matrix.","7b02b4dd":"# Bonus (Lagrange Multiplier)\n\nCongratulations! You have found this bonus. In this section, I optimise the blending weights in a more mathematical way using Lagrange Multiplier method. The following equation is the minimisation problem that we want to solve:\n\n$$\n\\begin{align}\n\\min_{w_{1}, w_{2},..., w_{K}} \\quad &-\\frac{1}{NM}\\sum_{m=1}^{M}\\sum_{i=1}^{N}\\left[ y_{i,m}{\\rm log}\\left( \\sum_{k=1}^{K}w_{k}\\hat{y}_{i,m,k} \\right) + \\left( 1 - y_{i,m} \\right) {\\rm log}\\left( 1 - \\sum_{k=1}^{K}w_{k}\\hat{y}_{i,m,k} \\right) \\right], \\qquad {\\rm (1)} \\\\\ns.t. \\quad &\\sum_{k=1}^{k}w_{k} = 1, \\qquad {\\rm (1a)} \\\\\n& 0 \\leqslant w_{k} \\leqslant 1, \\quad k = 1, ..., K, \\qquad {\\rm (1b)}\n\\end{align}\n$$\n\nwhere $N$ is the number of 'sigid' observations in the test data $(i = 1, ...,N)$;\n\n$M$ is the number of scored MoA targets $(m = 1, ...,M)$;\n\n$w_{k}$ is the blending weight for the $k$th model's prediction results $(k = 1, ...,K)$; \n\n$\\hat{y}_{i,m,k}$ is the $k$th model's predicted probability of the $m$th positive MoA response for the $n$th 'sigid'; \n\n$y_{i,m}$ is the groundtruth of the $m$th positive MoA response for the $n$th 'sigid', 1 for a positive response, 0 otherwise; \n\n${\\rm log}(.)$ is the natural (base e) logarithm.\n\nAccording to the [Extreme Value Thereom][1], Constraint (1b) indicates Eq. (1) has absolute maximum and minimum values. We apply the [Lagrange Multiplier][2] method to this optimsiation problem. The new optimisation problem is expressed as follows:\n\n$$\n\\begin{align}\n\\min_{w_{1}, w_{2},..., w_{K}} \\quad &L = -\\frac{1}{NM}\\sum_{m=1}^{M}\\sum_{i=1}^{N}\\left[ y_{i,m}{\\rm log}\\left( \\sum_{k=1}^{K}w_{k}\\hat{y}_{i,m,k} \\right) + \\left( 1 - y_{i,m} \\right) {\\rm log}\\left( 1 - \\sum_{k=1}^{K}w_{k}\\hat{y}_{i,m,k} \\right) \\right] - \\lambda\\left(\\sum_{k=1}^{K}w_{k} - 1\\right), \\qquad {\\rm (2)} \\\\\ns.t. \\quad &0 \\leqslant w_{k} \\leqslant 1, \\quad k = 1, ..., K, \\qquad {\\rm (2b)}\n\\end{align}\n$$\n\nwhere $\\lambda$ is the Lagrange multiplier.\n\nThe [Karush\u2013Kuhn\u2013Tucker condition][3] for the optimal solution is:\n$$\n\\left\\{\\begin{matrix}\n\\frac{\\partial L}{\\partial w_{k}} = 0, & k = 1, ..., K, \\\\ \n\\frac{\\partial L}{\\partial \\lambda} = 0, & \n\\end{matrix}\\right., \\qquad {\\rm (3)}\n$$\n\nFrom Eq. (3), we end up with $K+1$ equations that equal zero, we can simply use [autograd][4] to calculate the partial derivatives and [scipy.optimize.fsolve][5] to get the optimal solution.\n\n[1]: https:\/\/en.wikipedia.org\/wiki\/Extreme_value_theorem\n[2]: https:\/\/en.wikipedia.org\/wiki\/Lagrange_multiplier\n[3]: https:\/\/en.wikipedia.org\/wiki\/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions\n[4]: https:\/\/github.com\/HIPS\/autograd\n[5]: https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.optimize.fsolve.html","0a4a2c82":"# Test Numba Gradient Function"}}