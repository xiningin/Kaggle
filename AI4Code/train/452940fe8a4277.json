{"cell_type":{"9144d7ae":"code","47c2cd8e":"code","1853f597":"code","1e71754e":"code","20ca3d00":"code","8a6317ba":"code","e87783a1":"code","1e1bf598":"code","c572d772":"code","b94c925d":"code","bdb1bcf5":"code","c2e2ad09":"code","df589f7d":"code","e18bbb2f":"code","5a356400":"code","a5c2c85e":"code","bc5bdb70":"code","e68105b4":"code","f9f3e27f":"code","43ba724c":"code","01781ea4":"code","2a540915":"code","8378945b":"code","91cc7cb2":"code","5fc8fa2d":"code","6fa4b002":"code","7070b788":"code","4b029357":"code","9e39704c":"code","b5d897d8":"code","03780049":"code","d066c907":"code","620c4da8":"code","132056a4":"code","1c5822a5":"code","218beb54":"code","b4af3318":"code","f4fbf860":"code","8570a837":"code","916d9d59":"code","02985495":"code","01f4f35d":"code","0c77800a":"code","160ad7e7":"code","5d17b72b":"code","c848a1f5":"code","a65b2633":"code","a5bd296e":"code","c83426a7":"code","1484d0e8":"code","74f7fc53":"code","4ad2ce5b":"code","551178e1":"code","dc6daee9":"code","cca5a48d":"code","2ff1a815":"code","0b4b217c":"code","a6d22dd9":"code","2140ddc4":"code","b83c22d6":"code","87a4d0cd":"code","128a15a7":"code","2c97c7a2":"code","7039e689":"code","6c92c11b":"code","98c3300c":"code","803f76bf":"code","2fddf0ea":"code","b01428ef":"code","291854bb":"code","dcdc1eda":"code","3c1ace5a":"code","0813a8f1":"code","26cc3351":"code","117eaa20":"code","57bbf597":"markdown","66c5567d":"markdown","d9b0dd67":"markdown","88765d33":"markdown","9e431e34":"markdown","0bd017d0":"markdown","fbf4f4b2":"markdown","553c5ca7":"markdown","72a77b24":"markdown","0d701a39":"markdown","080ef2ed":"markdown","3be97c09":"markdown","0a034f29":"markdown","12c968b4":"markdown"},"source":{"9144d7ae":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n%matplotlib inline","47c2cd8e":"df = pd.read_csv('..\/input\/enron-email-dataset\/emails.csv', nrows=500)\ndf.info()","1853f597":"print(df.file[2])","1e71754e":"example = df.message[2]\nprint(example)","20ca3d00":"email_lines = ['Message-ID', 'Date', 'From', 'To', 'Subject', 'Cc', 'Mime-Version',\\\n               'Content-Type', 'Content-Transfer-Encoding', 'Bcc', 'X-From', 'X-To',\\\n               'X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName', 'Content']","8a6317ba":"def get_content(message):\n    message = message.split('\\n')\n    index = message.index('')\n    message = message[: index] + ['Content: ' + ''.join(message[index+1:])]\n    message = [message[i].split(': ', 1) for i in range(len(message))]\n    i = 0\n    while i < len(message):\n        if len(message[i]) != 2:\n            message[i-1][1] += message[i][0]\n            message.pop(i)\n        elif message[i][0] not in email_lines:\n            message[i-1][1] += (message[i][0] + ': ' + message[i][1])\n            message.pop(i)\n        else:\n            i += 1\n    return message","e87783a1":"get_content(example)","1e1bf598":"list_of_content = []\nfor i in range(500):\n    list_of_content += [pd.DataFrame(get_content(df.message[i])).set_index(0).transpose()]","c572d772":"data = pd.concat(list_of_content, axis=0, sort=False).reset_index()\ndata['Index'] = df['file']\ncol = ['Index'] + [i[0] for i in get_content(df.message[86])]\ndata = data[col].replace('', np.nan)\nprint(data.shape)\ndata.head()","b94c925d":"data.dtypes","bdb1bcf5":"analysis_data = data[['Date', 'From', 'To', 'Subject', 'Content']].dropna().copy()\nanalysis_data['Date'] =  pd.to_datetime(analysis_data['Date'])\nprint(analysis_data.shape)\nanalysis_data.head()","c2e2ad09":"beeap = pd.read_csv('..\/input\/label-beeap\/BEEAP.csv')\nbeeap.drop('Unnamed: 0', axis=1, inplace=True)\nfor i in beeap.columns[1:]:\n    beeap[i] = beeap[i].astype(np.float64, errors='ignore')\nprint(beeap.shape)\nbeeap.head()","df589f7d":"plt.figure(figsize=(10, 6))\nplt.plot(sorted(beeap.Content.str.len()))\nplt.title('Distribution of Content Length', fontsize=16)\nplt.xlim(-50, 1750)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.grid()\nplt.show()","e18bbb2f":"beeap_1 = pd.read_csv('..\/input\/beeapfinal\/beeap_1.csv')\nbeeap_1.drop('Unnamed: 0', axis=1, inplace=True)\nfor i in beeap_1.columns[1:]:\n    beeap_1[i] = beeap_1[i].astype(np.float64, errors='ignore')\nprint(beeap_1.shape)\nbeeap_1.head()","5a356400":"# https:\/\/deeplearningcourses.com\/c\/deep-learning-advanced-nlp\nfrom __future__ import print_function, division\nfrom builtins import range\n# Note: you may need to update your version of future\n# sudo pip install -U future\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer\n# we want our data have the same length\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom sklearn.metrics import roc_auc_score\n\n# some configuration\n# can use the maximum of the sequence in one email and set it larger\nmax_len = 200\n# a native english user knows 20000 words in practice...\nmax_features = 100000\n# the size of each word vector ... using the pretrained model\nembed_size = 300\nVALIDATION_SPLIT = 0.2\nbatch_size = 128\nepochs = 50\n\n# Download the data:\n# https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\n# Download the word vectors:\n# http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n\nprint('Loading word vectors...')\nword2vec = {}\n#with open(os.path.join('..\/input\/glove840b300dtxt\/glove.840B.%sd.txt' % embed_size)) as f:\nwith open(os.path.join('..\/input\/glove-6b\/glove.6B.%sd.txt' % embed_size)) as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))","a5c2c85e":"# prepare text samples and their labels\nprint('Loading in comments...')\ntrain = beeap_1\n# extract the comments, fill NaN with some values\nsentences = train[\"Contents\"].fillna(\"DUMMY_VALUE\").values\n# possible_labels_details = [\"Business\", \"Personal\", \"Personal but professional\", \"Logistic\", \"Employment\", \"Document\", 'Empty attachment', 'Empty']\npossible_labels= [str(i+1) for i in range(6)]\n# possible_labels= [i+1 for i in range(13)]\ntargets = train[possible_labels].values","bc5bdb70":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(sentences, targets, test_size = 0.2)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.125)","e68105b4":"# convert the sentences (strings) into integers\uff0c thus they can be used as index later on\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n#sequences = tokenizer.texts_to_sequences(sentences)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_valid_seq = tokenizer.texts_to_sequences(X_valid)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n# print(\"sequences:\", sequences); exit()\n\n\nprint(\"max sequence length:\", max(len(s) for s in X_train_seq))\nprint(\"min sequence length:\", min(len(s) for s in X_train_seq))\ns = sorted(len(s) for s in X_train_seq)\nprint(\"median sequence length:\", s[len(s) \/\/ 2])\n\n\n# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))\n\n\n# pad sequences so that we get a N x T matrix\n# Keras take care of the 0 only for padding purpose \n#data = pad_sequences(sequences, maxlen=max_len)\nX_train = pad_sequences(X_train_seq, maxlen=max_len)\nX_valid = pad_sequences(X_valid_seq, maxlen=max_len)\nX_test = pad_sequences(X_test_seq, maxlen=max_len)\nprint('Shape of data tensor:', X_train.shape)\n\n\n\n# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(max_features, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, embed_size))\nfor word, i in word2idx.items():\n    if i < max_features:\n        embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n        embedding_matrix[i] = embedding_vector","f9f3e27f":"# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(\n  min(max_features, embedding_matrix.shape[0]),\n  embed_size,\n  weights=[embedding_matrix],\n  input_length=max_len,\n    # don't want to make the embeddding updated during the procedure\n  trainable=False\n)","43ba724c":"import logging\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\n\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","01781ea4":"from keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\n\nprint('Building model...')\n\nfile_path = \"best_model.cnn\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nra_val = RocAucEvaluation(validation_data=(X_valid, y_valid), interval = 1)\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 10)\n\ndef build_cnn_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    input_ = Input(shape=(max_len,))\n    x = embedding_layer(input_)\n    x = Conv1D(128, 3, activation='relu')(x)\n    x = MaxPooling1D(3)(x)\n    x = Conv1D(128, 3, activation='relu')(x)\n    x = MaxPooling1D(3)(x)\n    x = Conv1D(128, 3, activation='relu')(x)\n    x = GlobalMaxPooling1D()(x)\n    x = Dense(128, activation='relu')(x)\n    # using sigmoid since we are doing six binary classifications\n    output = Dense(len(possible_labels), activation='sigmoid')(x)\n    \n    model = Model(inputs = input_, outputs = output)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_valid, y_valid), \n                        verbose = 1, callbacks = [ra_val, check_point, early_stop])\n    model = load_model(file_path)\n    return model","2a540915":"model = build_cnn_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\npred = model.predict(X_test, batch_size = batch_size, verbose = 1)","8378945b":"# plot the mean AUC over each label\nroc_auc_score(y_test, pred)","91cc7cb2":"print('Building model...')\n\nfile_path = \"best_model.lstm\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nra_val = RocAucEvaluation(validation_data=(X_valid, y_valid), interval = 1)\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 10)\n\ndef build_lstm_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape=(max_len,))\n    x = embedding_layer(inp)\n    x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dropout(0.1)(x)\n    x = Dense(50, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    # using sigmoid since we are doing six binary classifications\n    output = Dense(len(possible_labels), activation='sigmoid')(x)\n    \n    model = Model(inputs = inp, outputs = output)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_valid, y_valid), \n                        verbose = 1, callbacks = [ra_val, check_point, early_stop])\n    model = load_model(file_path)\n    return model","5fc8fa2d":"model = build_lstm_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\npred = model.predict(X_test, batch_size = batch_size, verbose = 1)","6fa4b002":"# plot the mean AUC over each label\nroc_auc_score(y_test, pred)","7070b788":"print('Building model...')\n\nfile_path = \"best_model.bilstm\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nra_val = RocAucEvaluation(validation_data=(X_valid, y_valid), interval = 1)\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 10)\n\ndef build_bilstm_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape = (max_len,))\n    x = embedding_layer(inp)\n    x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(50, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(6, activation=\"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_valid, y_valid), \n                        verbose = 1, callbacks = [ra_val, check_point, early_stop])\n    model = load_model(file_path)\n    return model","4b029357":"model = build_bilstm_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\npred = model.predict(X_test, batch_size = batch_size, verbose = 1)","9e39704c":"# plot the mean AUC over each label\nroc_auc_score(y_test, pred)","b5d897d8":"print('Building model...')\n\nfile_path = \"best_model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nra_val = RocAucEvaluation(validation_data=(X_valid, y_valid), interval = 1)\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 10)\n\ndef build_hdf5_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape = (max_len,))\n    x = embedding_layer(inp)\n    x = SpatialDropout1D(dr)(x)\n\n    x = Bidirectional(GRU(units, return_sequences = True))(x)\n    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    x = concatenate([avg_pool, max_pool])\n\n    x = Dense(6, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_valid, y_valid), \n                        verbose = 1, callbacks = [ra_val, check_point, early_stop])\n    model = load_model(file_path)\n    return model","03780049":"model = build_hdf5_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\npred = model.predict(X_test, batch_size = batch_size, verbose = 1)","d066c907":"# plot the mean AUC over each label\nroc_auc_score(y_test, pred)","620c4da8":"print('Building model...')\n\nfile_path = \"best_model.advanced\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nra_val = RocAucEvaluation(validation_data=(X_valid, y_valid), interval = 1)\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 10)\n\ndef build_advanced_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape=(max_len,))\n    x = embedding_layer(inp)\n    x1 = SpatialDropout1D(dr)(x)\n    x = Bidirectional(GRU(units, return_sequences = True))(x1)\n    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n    y = Bidirectional(LSTM(units, return_sequences = True))(x1)\n    y = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n    avg_pool1 = GlobalAveragePooling1D()(x)\n    max_pool1 = GlobalMaxPooling1D()(x)\n    avg_pool2 = GlobalAveragePooling1D()(y)\n    max_pool2 = GlobalMaxPooling1D()(y)\n    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n    output = Dense(len(possible_labels), activation='sigmoid')(x)\n    \n    model = Model(inputs = inp, outputs = output)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_valid, y_valid), \n                        verbose = 1, callbacks = [ra_val, check_point, early_stop])\n    model = load_model(file_path)\n    return model","132056a4":"model = build_advanced_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\npred = model.predict(X_test, batch_size = batch_size, verbose = 1)","1c5822a5":"# plot the mean AUC over each label\nroc_auc_score(y_test, pred)","218beb54":"from sklearn.metrics import roc_curve, auc\nfrom scipy import interp\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(6):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), pred.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Compute macro-average ROC curve and ROC area\n# Aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(6)]))\n# Interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(6):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n# Average and compute AUC\nmean_tpr \/= 6\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='gold', linestyle=':', linewidth=2)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=2)\n\nfor i in range(6):\n    plt.plot(fpr[i], tpr[i],\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i+1, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xticks(fontsize=13)\nplt.xticks(fontsize=13)\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC Curves for Coarse Genre', fontsize=16)\nplt.legend()\nplt.show()","b4af3318":"def pred_classes(y_pred):\n    train_pred = model.predict(X_train, batch_size = batch_size, verbose = 1)\n    threshold = []\n    for x in range(6):\n        result = []\n        for i in np.arange(0, 1.01, 0.001):\n            result += [sum(y_train[:, x] != (train_pred[:, x] >= i))]\n        result = np.array(result)\n        threshold += [round(np.where(result == result.min())[0].mean())\/1000]\n    pred_classes = y_pred >= threshold\n    return pred_classes.astype(int)","f4fbf860":"y_pred = pred_classes(pred)\nfor i in range(6):\n    print('Accuracy for class ' + str(i) + ': ' + str(round(sum(y_pred[:, i] == y_test[:, i])\/y_pred.shape[0], 4)))","8570a837":"### Try to write pred_classes with scipy minimize\n#from scipy.optimize import minimize\n#\n#def function(threshold):\n#    return sum(y_test[:, 0] != (pred[:, 0] >= threshold))\n\n#threshold = np.random.rand()\n#print(threshold)\n#minimize(function, threshold)","916d9d59":"from keras_self_attention import SeqSelfAttention\n\nprint('Building model...')\n\nfile_path = \"best_model.attn\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nra_val = RocAucEvaluation(validation_data=(X_valid, y_valid), interval = 1)\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 10)\n\ndef build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape=(max_len,))\n    x = embedding_layer(inp)\n    x = Bidirectional(LSTM(units, return_sequences = True))(x)\n    x = SeqSelfAttention(attention_activation='sigmoid')(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dropout(0.1)(x)\n    x = Dense(50, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    # using sigmoid since we are doing six binary classifications\n    output = Dense(len(possible_labels), activation='sigmoid')(x)\n    \n    model = Model(inputs = inp, outputs = output)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_valid, y_valid), \n                        verbose = 1, callbacks = [ra_val, check_point, early_stop])\n    model = load_model(file_path)\n    return model","02985495":"#model = build_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\n#pred = model.predict(X_test, batch_size = batch_size, verbose = 1)","01f4f35d":"# plot the mean AUC over each label\n#roc_auc_score(y_test, pred)","0c77800a":"# some configuration\n# can use the maximum of the sequence in one email and set it larger\nmax_len = 200\n# a native english user knows 20000 words in practice...\nmax_features = 100000\n# the size of each word vector ... using the pretrained model\nembed_size = 300\nVALIDATION_SPLIT = 0.2\nbatch_size = 128\nepochs = 50\n\n# Download the data:\n# https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\n# Download the word vectors:\n# http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n\nprint('Loading word vectors...')\nword2vec = {}\n#with open(os.path.join('..\/input\/glove840b300dtxt\/glove.840B.%sd.txt' % embed_size)) as f:\nwith open(os.path.join('..\/input\/word2vec-ec\/word2vec_ec\/Word2Vec_ec.%sB.txt' % embed_size)) as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))","160ad7e7":"# prepare text samples and their labels\nprint('Loading in comments...')\ntrain = beeap_1\n# extract the comments, fill NaN with some values\nsentences = train[\"Contents\"].fillna(\"DUMMY_VALUE\").values\n# possible_labels_details = [\"Business\", \"Personal\", \"Personal but professional\", \"Logistic\", \"Employment\", \"Document\", 'Empty attachment', 'Empty']\npossible_labels= [str(i+1) for i in range(6)]\n# possible_labels= [i+1 for i in range(13)]\ntargets = train[possible_labels].values","5d17b72b":"X_train, X_test, y_train, y_test = train_test_split(sentences, targets, test_size = 0.2)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.125)","c848a1f5":"# convert the sentences (strings) into integers\uff0c thus they can be used as index later on\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n#sequences = tokenizer.texts_to_sequences(sentences)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_valid_seq = tokenizer.texts_to_sequences(X_valid)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n# print(\"sequences:\", sequences); exit()\n\n\nprint(\"max sequence length:\", max(len(s) for s in X_train_seq))\nprint(\"min sequence length:\", min(len(s) for s in X_train_seq))\ns = sorted(len(s) for s in X_train_seq)\nprint(\"median sequence length:\", s[len(s) \/\/ 2])\n\n\n# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))\n\n\n# pad sequences so that we get a N x T matrix\n# Keras take care of the 0 only for padding purpose \n#data = pad_sequences(sequences, maxlen=max_len)\nX_train = pad_sequences(X_train_seq, maxlen=max_len)\nX_valid = pad_sequences(X_valid_seq, maxlen=max_len)\nX_test = pad_sequences(X_test_seq, maxlen=max_len)\nprint('Shape of data tensor:', X_train.shape)\n\n\n\n# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(max_features, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, embed_size))\nfor word, i in word2idx.items():\n    if i < max_features:\n        embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n        embedding_matrix[i] = embedding_vector","a65b2633":"# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(\n  min(max_features, embedding_matrix.shape[0]),\n  embed_size,\n  weights=[embedding_matrix],\n  input_length=max_len,\n    # don't want to make the embeddding updated during the procedure\n  trainable=False\n)","a5bd296e":"model = build_advanced_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\npred = model.predict(X_test, batch_size = batch_size, verbose = 1)","c83426a7":"# plot the mean AUC over each label\nroc_auc_score(y_test, pred)","1484d0e8":"fpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(6):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), pred.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Compute macro-average ROC curve and ROC area\n# Aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(6)]))\n# Interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(6):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n# Average and compute AUC\nmean_tpr \/= 6\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='gold', linestyle=':', linewidth=2)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=2)\n\nfor i in range(6):\n    plt.plot(fpr[i], tpr[i],\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i+1, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xticks(fontsize=13)\nplt.xticks(fontsize=13)\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC Curves for Coarse Genre', fontsize=16)\nplt.legend()\nplt.show()","74f7fc53":"# some configuration\n# can use the maximum of the sequence in one email and set it larger\nmax_len = 200\n# a native english user knows 20000 words in practice...\nmax_features = 100000\n# the size of each word vector ... using the pretrained model\nembed_size = 300\nVALIDATION_SPLIT = 0.2\nbatch_size = 128\nepochs = 50\n\n# Download the data:\n# https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\n# Download the word vectors:\n# http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n\nprint('Loading word vectors...')\nword2vec = {}\n#with open(os.path.join('..\/input\/glove840b300dtxt\/glove.840B.%sd.txt' % embed_size)) as f:\nwith open(os.path.join('..\/input\/glove-ec\/glove_ec\/GloVe_ec.%sB.txt' % embed_size)) as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))","4ad2ce5b":"# prepare text samples and their labels\nprint('Loading in comments...')\ntrain = beeap_1\n# extract the comments, fill NaN with some values\nsentences = train[\"Content\"].fillna(\"DUMMY_VALUE\").values\n# possible_labels_details = [\"Business\", \"Personal\", \"Personal but professional\", \"Logistic\", \"Employment\", \"Document\", 'Empty attachment', 'Empty']\npossible_labels= [str(i+1) for i in range(6)]\n# possible_labels= [i+1 for i in range(13)]\ntargets = train[possible_labels].values","551178e1":"X_train, X_test, y_train, y_test = train_test_split(sentences, targets, test_size = 0.2)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.125)","dc6daee9":"# convert the sentences (strings) into integers\uff0c thus they can be used as index later on\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_valid_seq = tokenizer.texts_to_sequences(X_valid)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n# print(\"sequences:\", sequences); exit()\n\n\nprint(\"max sequence length:\", max(len(s) for s in sequences))\nprint(\"min sequence length:\", min(len(s) for s in sequences))\ns = sorted(len(s) for s in sequences)\nprint(\"median sequence length:\", s[len(s) \/\/ 2])\n\n\n# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))\n\n\n# pad sequences so that we get a N x T matrix\n# Keras take care of the 0 only for padding purpose \ndata = pad_sequences(sequences, maxlen=max_len)\nX_train = pad_sequences(X_train_seq, maxlen=max_len)\nX_valid = pad_sequences(X_valid_seq, maxlen=max_len)\nX_test = pad_sequences(X_test_seq, maxlen=max_len)\nprint('Shape of data tensor:', data.shape)\n\n\n\n# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(max_features, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, embed_size))\nfor word, i in word2idx.items():\n    if i < max_features:\n        embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n        embedding_matrix[i] = embedding_vector","cca5a48d":"# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(\n  min(max_features, embedding_matrix.shape[0]),\n  embed_size,\n  weights=[embedding_matrix],\n  input_length=max_len,\n    # don't want to make the embeddding updated during the procedure\n  trainable=False\n)","2ff1a815":"model = build_advanced_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\npred = model.predict(X_test, batch_size = batch_size, verbose = 1)","0b4b217c":"# plot the mean AUC over each label\nroc_auc_score(y_test, pred)","a6d22dd9":"fpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(6):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), pred.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Compute macro-average ROC curve and ROC area\n# Aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(6)]))\n# Interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(6):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n# Average and compute AUC\nmean_tpr \/= 6\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='gold', linestyle=':', linewidth=2)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=2)\n\nfor i in range(6):\n    plt.plot(fpr[i], tpr[i],\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i+1, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xticks(fontsize=13)\nplt.xticks(fontsize=13)\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC Curves for Coarse Genre', fontsize=16)\nplt.legend()\nplt.show()","2140ddc4":"model.summary()","b83c22d6":"beeap_2 = pd.read_csv('..\/input\/beeapfinal\/beeap_2.csv')\nbeeap_2.drop('Unnamed: 0', axis=1, inplace=True)\nfor i in beeap_2.columns[1:]:\n    beeap_2[i] = beeap_2[i].astype(np.float64, errors='ignore')\nprint(beeap_2.shape)\nbeeap_2.head()","87a4d0cd":"# prepare text samples and their labels\nprint('Loading in comments...')\ntrain = beeap_2\n# extract the comments, fill NaN with some values\nsentences = train[\"Contents\"].fillna(\"DUMMY_VALUE\").values\n# possible_labels_details = [\"Business\", \"Personal\", \"Personal but professional\", \"Logistic\", \"Employment\", \"Document\", 'Empty attachment', 'Empty']\npossible_labels= [str(i+1) for i in range(13)]\n# possible_labels= [i+1 for i in range(13)]\ntargets = train[possible_labels].values","128a15a7":"X_train, X_test, y_train, y_test = train_test_split(sentences, targets, test_size = 0.2, random_state=8)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.125, random_state=8)","2c97c7a2":"# convert the sentences (strings) into integers\uff0c thus they can be used as index later on\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n#sequences = tokenizer.texts_to_sequences(sentences)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_valid_seq = tokenizer.texts_to_sequences(X_valid)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n# print(\"sequences:\", sequences); exit()\n\n\nprint(\"max sequence length:\", max(len(s) for s in X_train_seq))\nprint(\"min sequence length:\", min(len(s) for s in X_train_seq))\ns = sorted(len(s) for s in X_train_seq)\nprint(\"median sequence length:\", s[len(s) \/\/ 2])\n\n\n# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))\n\n\n# pad sequences so that we get a N x T matrix\n# Keras take care of the 0 only for padding purpose \n#data = pad_sequences(sequences, maxlen=max_len)\nX_train = pad_sequences(X_train_seq, maxlen=max_len)\nX_valid = pad_sequences(X_valid_seq, maxlen=max_len)\nX_test = pad_sequences(X_test_seq, maxlen=max_len)\nprint('Shape of data tensor:', X_train.shape)\n\n\n\n# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(max_features, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, embed_size))\nfor word, i in word2idx.items():\n    if i < max_features:\n        embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n        embedding_matrix[i] = embedding_vector","7039e689":"# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(\n  min(max_features, embedding_matrix.shape[0]),\n  embed_size,\n  weights=[embedding_matrix],\n  input_length=max_len,\n    # don't want to make the embeddding updated during the procedure\n  trainable=False\n)","6c92c11b":"ra_val = RocAucEvaluation(validation_data=(X_valid, y_valid), interval = 1)\n\nmodel = build_advanced_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\npred = model.predict(X_test, batch_size = batch_size, verbose = 1)","98c3300c":"# plot the mean AUC over each label\nroc_auc_score(y_test, pred)","803f76bf":"fpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(13):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), pred.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Compute macro-average ROC curve and ROC area\n# Aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(13)]))\n# Interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(13):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n# Average and compute AUC\nmean_tpr \/= 13\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='gold', linestyle=':', linewidth=2)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=2)\n\nfor i in range(13):\n    plt.plot(fpr[i], tpr[i],\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i+1, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xticks(fontsize=13)\nplt.xticks(fontsize=13)\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC Curves for Coarse Genre', fontsize=16)\nplt.legend()\nplt.show()","2fddf0ea":"beeap_3 = pd.read_csv('..\/input\/beeapfinal\/beeap_3.csv')\nbeeap_3.drop('Unnamed: 0', axis=1, inplace=True)\nfor i in beeap_3.columns[1:]:\n    beeap_3[i] = beeap_3[i].astype(np.float64, errors='ignore')\nprint(beeap_3.shape)\nbeeap_3.head()","b01428ef":"# prepare text samples and their labels\nprint('Loading in comments...')\ntrain = beeap_2\n# extract the comments, fill NaN with some values\nsentences = train[\"Contents\"].fillna(\"DUMMY_VALUE\").values\n# possible_labels_details = [\"Business\", \"Personal\", \"Personal but professional\", \"Logistic\", \"Employment\", \"Document\", 'Empty attachment', 'Empty']\npossible_labels= [str(i+1) for i in range(13)]\n# possible_labels= [i+1 for i in range(13)]\ntargets = train[possible_labels].values","291854bb":"X_train, X_test, y_train, y_test = train_test_split(sentences, targets, test_size = 0.2, random_state=8)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.125, random_state=8)","dcdc1eda":"# convert the sentences (strings) into integers\uff0c thus they can be used as index later on\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\nsequences = tokenizer.texts_to_sequences(sentences)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_valid_seq = tokenizer.texts_to_sequences(X_valid)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n# print(\"sequences:\", sequences); exit()\n\n\nprint(\"max sequence length:\", max(len(s) for s in sequences))\nprint(\"min sequence length:\", min(len(s) for s in sequences))\ns = sorted(len(s) for s in sequences)\nprint(\"median sequence length:\", s[len(s) \/\/ 2])\n\n\n# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))\n\n\n# pad sequences so that we get a N x T matrix\n# Keras take care of the 0 only for padding purpose \ndata = pad_sequences(sequences, maxlen=max_len)\nX_train = pad_sequences(X_train_seq, maxlen=max_len)\nX_valid = pad_sequences(X_valid_seq, maxlen=max_len)\nX_test = pad_sequences(X_test_seq, maxlen=max_len)\nprint('Shape of data tensor:', data.shape)\n\n\n\n# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(max_features, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, embed_size))\nfor word, i in word2idx.items():\n    if i < max_features:\n        embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n        embedding_matrix[i] = embedding_vector","3c1ace5a":"# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(\n  min(max_features, embedding_matrix.shape[0]),\n  embed_size,\n  weights=[embedding_matrix],\n  input_length=max_len,\n    # don't want to make the embeddding updated during the procedure\n  trainable=False\n)","0813a8f1":"ra_val = RocAucEvaluation(validation_data=(X_valid, y_valid), interval = 1)\n\nmodel = build_advanced_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\npred = model.predict(X_test, batch_size = batch_size, verbose = 1)","26cc3351":"# plot the mean AUC over each label\nroc_auc_score(y_test, pred)","117eaa20":"fpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(13):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), pred.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Compute macro-average ROC curve and ROC area\n# Aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(13)]))\n# Interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(13):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n# Average and compute AUC\nmean_tpr \/= 13\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='gold', linestyle=':', linewidth=2)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=2)\n\nfor i in range(13):\n    plt.plot(fpr[i], tpr[i],\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i+1, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xticks(fontsize=13)\nplt.xticks(fontsize=13)\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC Curves for Coarse Genre', fontsize=16)\nplt.legend()\nplt.show()","57bbf597":"**Import the Berkeley Enron Email Analysis Project (BEEAP).**\n\nThis project manually labeled 1702 emails with various labels.\n\n**Cat** = Top category; <br\/>\n**Sub_cat** = Second-level category; <br\/>\n**Freq** = frequency with which this category was assigned to this message.\n\nCategories:\n\n**1 Coarse genre**\n \n1.1 Company Business, Strategy, etc. (elaborate in Section 3 [Topics])<br\/>\n1.2 Purely Personal<br\/>\n1.3 Personal but in professional context (e.g., it was good working with you)<br\/>\n1.4 Logistic Arrangements (meeting scheduling, technical support, etc)<br\/>\n1.5 Employment arrangements (job seeking, hiring, recommendations, etc)<br\/>\n1.6 Document editing\/checking (collaboration)<br\/>\n1.7 Empty message (due to missing attachment)<br\/>\n1.8 Empty message\n\n\n**2 Included\/forwarded information**\n\n2.1 Includes new text in addition to forwarded material<br\/>\n2.2 Forwarded email(s) including replies<br\/>\n2.3 Business letter(s) \/ document(s)<br\/>\n2.4 News article(s)<br\/>\n2.5 Government \/ academic report(s)<br\/>\n2.6 Government action(s) (such as results of a hearing, etc)<br\/>\n2.7 Press release(s)<br\/>\n2.8 Legal documents (complaints, lawsuits, advice)<br\/>\n2.9 Pointers to url(s)<br\/>\n2.10 Newsletters<br\/>\n2.11 Jokes, humor (related to business)<br\/>\n2.12 Jokes, humor (unrelated to business)<br\/>\n2.13 Attachment(s) (assumed missing)\n\n\n**3 Primary topics (if coarse genre 1.1 is selected)**\n\n3.1 regulations and regulators (includes price caps)<br\/>\n3.2 internal projects -- progress and strategy<br\/>\n3.3 company image -- current<br\/>\n3.4 company image -- changing \/ influencing<br\/>\n3.5 political influence \/ contributions \/ contacts<br\/>\n3.6 california energy crisis \/ california politics<br\/>\n3.7 internal company policy<br\/>\n3.8 internal company operations<br\/>\n3.9 alliances \/ partnerships<br\/>\n3.10 legal advice<br\/>\n3.11 talking points<br\/>\n3.12 meeting minutes<br\/>\n3.13 trip reports\n\n\n**4 Emotional tone (if not neutral)**\n\n4.1 jubilation<br\/>\n4.2 hope \/ anticipation<br\/>\n4.3 humor<br\/>\n4.4 camaraderie<br\/>\n4.5 admiration<br\/>\n4.6 gratitude<br\/>\n4.7 friendship \/ affection<br\/>\n4.8 sympathy \/ support<br\/>\n4.9 sarcasm<br\/>\n4.10 secrecy \/ confidentiality<br\/>\n4.11 worry \/ anxiety<br\/>\n4.12 concern<br\/>\n4.13 competitiveness \/ aggressiveness<br\/>\n4.14 triumph \/ gloating<br\/>\n4.15 pride<br\/>\n4.16 anger \/ agitation<br\/>\n4.17 sadness \/ despair<br\/>\n4.18 shame<br\/>\n4.19 dislike \/ scorn","66c5567d":"**CNN Model**","d9b0dd67":"**Seperate emails by top categories.**","88765d33":"**BiGRU + CNN Model**","9e431e34":"---\n\n**Use Embeddings GloVe_E.C.**","0bd017d0":"___\n\n**Top_cat 2 & 3**","fbf4f4b2":"---\n\n**Use Embeddings word2vec_E.C.**","553c5ca7":"Read the dataset into a DataFrame.","72a77b24":"---","0d701a39":"**Bi + GRU + LSTM + CNN Model**","080ef2ed":"Here I want to see the text length in each email in Berkeley Enron dataset. We can see that there are approximately 100 emails that has an extremely long content.","3be97c09":"**BiLSTM Model**","0a034f29":"**LSTM Model**","12c968b4":"**BiLSTM + Self Attention Model**"}}