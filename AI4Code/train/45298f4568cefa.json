{"cell_type":{"7a57c9d9":"code","9ffa62d3":"code","25effb18":"code","65f93c06":"code","e76e92cb":"code","dc3556b3":"code","5f68ce07":"code","40fe9ccb":"code","8ccbaee6":"markdown","0da2d872":"markdown","07afef4c":"markdown","447b9ecc":"markdown","cf010c81":"markdown","49f07608":"markdown","9df4c6ae":"markdown","578f1a6c":"markdown","4f74ebe2":"markdown"},"source":{"7a57c9d9":"import warnings\nwarnings.simplefilter(action='ignore')\n\nimport numpy as np\nnp.set_printoptions(suppress=True)\nimport pandas as pd\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\n\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression, RidgeCV\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn import linear_model\nfrom sklearn.feature_selection import chi2\nimport statsmodels.api as sm\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\nfrom sklearn.feature_selection import RFE","9ffa62d3":"def understand_variables(dataset):\n    print(type(dataset))\n    print(dataset.shape)\n    print(dataset.head())\n    print(dataset.columns)\n    print(dataset.nunique(axis=0))\n    print(dataset.describe())\n    print(dataset.describe(exclude=[np.number]))\n    print(\"\\nNull count :\\n\"+str(dataset.isnull().sum()))\n    \n    \ndef EDA(dataset,feature_type):\n    \n    if feature_type == \"Categorical\":\n        \n        categorical_features=[feature for feature in dataset.columns if dataset[feature].dtype=='O']   \n        dataframes=[]\n        for feature in categorical_features:\n            dataframe=dataset[feature].value_counts().rename_axis(feature).reset_index(name='counts')\n            dataframes.append(dataframe)\n\n        for i in range(len(dataframes)):\n            print(dataframes[i],'\\n')\n            \n    elif feature_type == \"Numeric\":\n        \n        numerical_features=[feature for feature in dataset.columns if dataset[feature].dtype!='O']\n        \n        for feature in numerical_features:\n            sns.distplot(dataset[feature])\n            plt.show()\n\n\n        sns.pairplot(dataset,kind=\"reg\")\n        plt.show()\n\n\ndef outlier_processing(dataset):\n    # Using IQR\n\n    Q1 = dataset.quantile(0.25)\n    Q3 = dataset.quantile(0.75)\n    IQR = Q3 - Q1\n    \n    #outlier_col = ['year']\n    \n    print(\"\\n-------------\\n% of outliers\\n\")    \n    print(((dataset < (Q1 - 1.5 * IQR)) |(dataset > (Q3 + 1.5 * IQR))).sum()\/len(dataset)*100)\n    \n    for col in list(IQR.index): \n        \n        if col!='price':\n        \n            dataset.loc[dataset[col] < (Q1 - 1.5 * IQR)[col],[col]] = (Q1 - 1.5 * IQR)[col]\n            dataset.loc[dataset[col] > (Q3 + 1.5 * IQR)[col],[col]] = (Q3 + 1.5 * IQR)[col]\n            \n            dataset[col] = dataset[col].round(0).astype(int)\n    \n    \n    for col in ['price']:\n        dataset = dataset[(dataset[col] <= (Q3 + 1.5 * IQR)[col]) & (dataset[col] >= (Q1 - 1.5 * IQR)[col])]\n        #dataset[col] = dataset[col].round(0).astype(int)\n        \n    ## We eliminate rows with price as outliers, rest we replace with upper\/lower boundary\n\n    return dataset","25effb18":"cars_dataset = pd.read_csv(\"..\/input\/usa-cers-dataset\/USA_cars_datasets.csv\",index_col=\"Unnamed: 0\")\ncars_dataset = cars_dataset.drop([\"lot\",\"vin\"],axis=1)\nunderstand_variables(cars_dataset)","65f93c06":"EDA(cars_dataset,feature_type=\"Categorical\")","e76e92cb":"EDA(cars_dataset,feature_type=\"Numeric\")","dc3556b3":"################ feature engineering ###########\n\n######### convert year to age (2020 - year)\ncars_dataset.year = 2021 - cars_dataset.year\n\n######## condition column : [Listings expired = 0, remove 'left' from others, convert everything to minutess ]\n\ncars_dataset.loc[cars_dataset.condition == \"Listing Expired\", 'condition'] = \"0 minutes left\"\ncars_dataset['condition'] = cars_dataset.condition.str.replace(\"left\",\"\")\ncars_dataset.loc[cars_dataset.condition.str.contains(\"minutes\"),'condition'] = (cars_dataset.loc[cars_dataset.condition.str.contains(\"minutes\"),'condition'].astype(str).str.split().str[0].astype(int)).astype(str)\ncars_dataset.loc[cars_dataset.condition.str.contains(\"hours\"),'condition'] = (cars_dataset.loc[cars_dataset.condition.str.contains(\"hours\"),'condition'].astype(str).str.split().str[0].astype(int) * 60).astype(str)\ncars_dataset.loc[cars_dataset.condition.str.contains(\"days\"),'condition'] = (cars_dataset.loc[cars_dataset.condition.str.contains(\"days\"),'condition'].astype(str).str.split().str[0].astype(int) * 60*24).astype(str)\ncars_dataset.condition = cars_dataset.condition.astype(int)\n\n######## dealing with outliers ########\n\ncars_dataset = outlier_processing(cars_dataset)\n\n#cars_dataset = cars_dataset[cars_dataset.price>0]\n\n\n############## Correlation check ############\n\ncorr = cars_dataset.corr()\n#sns.heatmap(corr, annot=True)\n\n\n####### get dummies ########\n\ncars_dataset = pd.get_dummies(cars_dataset, dummy_na=True)\n","5f68ce07":"############## Feauture Selection (using Correlation) ######\n\ncorr = cars_dataset.corr()\n\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.9:\n            if columns[j]:\n                columns[j] = False\nselected_columns = cars_dataset.columns[columns]\ncars_dataset = cars_dataset[selected_columns]\n\n\n############## Feauture Selection (using p-value) ######\n\nX = cars_dataset.drop(\"price\",axis=1)\ny = cars_dataset[\"price\"]\n\n\nX = sm.add_constant(X)\nmod = sm.OLS(y,X)\nfii = mod.fit()\nsm_p_value = fii.summary2().tables[1]['P>|t|']\npvalues = pd.Series(sm_p_value)\n\n\nsig_p_val = pvalues[pvalues<=0.05]\nsig_p_val.drop(\"const\", inplace=True)\ncars_col_index = sig_p_val.index\n\ncars_col_index = pd.Series(cars_col_index)\n\ncars_col_index = list(cars_col_index)\n\ncars_dataset = cars_dataset[cars_col_index] \ncars_dataset = pd.concat([cars_dataset,y], axis=1)\n\nprint(\"Retained columns : \" +  str(cars_dataset.columns))","40fe9ccb":"############ Training the model ##############\n\nX = cars_dataset.drop(\"price\",axis=1)\ny = cars_dataset[\"price\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\nregressor = RidgeCV(alphas=[0.0001,0.001,0.01,0.1,1,1e1,1e2,1e3,1e4,1e5,1e6], store_cv_values=True)\nregressor.fit(X_train, y_train)\ncv_mse = np.mean(regressor.cv_values_, axis=0)\n#print([0.0001,0.001,0.01,0.1,1,1e1,1e2,1e3,1e4,1e5,1e6])\n#print(cv_mse)\n\n# Best alpha\nprint(\"Best alpha = \" + str(regressor.alpha_))\n\ny_pred = pd.Series(regressor.predict(X_test))\n\ny_pred[y_pred<0]=0\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint(\"R_2 = {:.2f} %\".format((r2_score(y_test, y_pred)*100)))\nprint(\"Variance score = {:.2f} %\".format((regressor.score(X_test, y_test)*100)))\n\n#print('10% of Mean Price:', cars_dataset['price'].mean() * 0.1)\n\nsns.regplot(y_test,y_pred)\nplt.show()","8ccbaee6":"# **This model uses US cars dataset to predict car prices with more than 73% accuracy**","0da2d872":"Next, we perform some Feature Engineering, deal with outliers (using defined function) and convert categorical variables into numerical dummy variables,\nSince we can only use numerical variables in linear regression (E.g. country column will be split into 2 columns, say *is_country_USA* and *is_country_Canada*. If country column had value as USA before conversion, *is_country_USA* = 1 and *is_country_Canada* = 0 after conversion)","07afef4c":"Feauture Selection using \n* correlation : removes one of the columns in a pair of highly correlated columns\n* p-value : measure of statistical significance. Here, null hypothesis is that *an independent variable has no correlation with dependent variable*, Price. Any independent variable with p-value <= 0.05 (alpha), is retained (thus rejecting the null hypothesis), and rest are eliminated","447b9ecc":"First, we import all the required libraries","cf010c81":"Finally, we use Ridge Regression (with Cross Validation) to train the model. This is done after feature engineering, data transformation and feature selection. RidgeCV function takes care of hyperparameter tuning, with 1 having emerged as the best value of alpha. We take train-test ratio of 75-25. After training the model, we calculate various measures of accuarcy, with R-square and Variance score of **73%**","49f07608":"# The above scores and scatter plot between predicted vs actual ouptut show that the model has good accuracy","9df4c6ae":"Next, we define functions to understand US Cars dataset, and deal with outliers (using IQR)","578f1a6c":"Now, we perfrom EDA on each column, first on categorical, followed by numerical","4f74ebe2":"Now, we import the USA Cars dataset, and use our defined function to understand this dataset. We drop columns \"lot\" and \"vin\" since these are IDs that are cannot be used to train our model."}}