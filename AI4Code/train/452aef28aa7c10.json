{"cell_type":{"67905e04":"code","d601d11e":"code","f42e990a":"code","fcb4e79e":"code","45eb0853":"code","fe24d5e7":"code","4d17844a":"code","99163963":"code","a6cdc568":"code","54ce97b4":"code","fdec7ec3":"code","cf106cb2":"code","26ece441":"code","5127ac75":"code","4415098f":"code","cd4c68e3":"code","789cb3a9":"code","75340f93":"code","ef6fdf38":"code","42f6730c":"code","0ed36c0b":"code","0a45d4d3":"code","8fcc115c":"code","4d0c51b4":"code","7579fa7b":"code","5962d477":"markdown","5cf22d81":"markdown"},"source":{"67905e04":"! pip install pyspark","d601d11e":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        \n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f42e990a":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\nspark","fcb4e79e":"sdf_train = spark.read.csv(\"..\/input\/train.csv\",inferSchema=True,header=True)\nprint(sdf_train.printSchema())\npdf = sdf_train.limit(10).toPandas()\npdf.T","45eb0853":"sdf_test = spark.read.csv(\"..\/input\/test.csv\",inferSchema=True,header=True)\n# sdf_train.printSchema()\npdf = sdf_test.limit(10).toPandas()\n\npdf.T","fe24d5e7":"numeric_cols = ['PassengerId','Survived', 'Pclass','Age', 'SibSp','Parch','Ticket','Fare'] \nnumeric_features = ['Pclass','Age', 'SibSp','Parch','Fare'] \n","4d17844a":"from pyspark.sql import DataFrame \nfrom pyspark.sql import functions as F\nfrom pyspark.ml.feature import Imputer\n\n# default is mean\ndef _claenup(sdf: DataFrame,colList: list):\n    imputer = Imputer(inputCols = colList,\n                     outputCols = colList)\n    sdf = imputer.fit(sdf).transform(sdf)\n    return sdf\n\nsdf_train_cleaned = _claenup(sdf_train,['Fare','Age'])\nsdf_train_cleaned.limit(5).toPandas().T\n","99163963":"# sdf_train_subset.select(numeric_features).printSchema() #.toPandas().T","a6cdc568":"_stages = []","54ce97b4":"# from pyspark.sql import functions as F\n# sdf_train_cleaned.groupBy(\"Sex\").agg(F.count(sdf_train_cleaned[\"Sex\"])).show()\n# sdf_train_cleaned.groupBy(\"Embarked\").agg(F.count(sdf_train_cleaned[\"Embarked\"])).show()","fdec7ec3":"from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n\nstring_features = [ 'Embarked', 'Sex'] # 'Cabin', \n\ndef _strIndexer(_stages):\n    strIndexer = [StringIndexer(inputCol=column, outputCol=column+\"_Index\", handleInvalid = \"skip\") for column in string_features ] #.fit(sdf_train_subset)\n    _stages += strIndexer","cf106cb2":"def _oneHotEnc(_stages):\n    oneHotEnc = [OneHotEncoderEstimator(inputCols= [column +\"_Index\"  for column in string_features]  , outputCols= [column+\"_Enc\" for column in string_features] )]\n    _stages += oneHotEnc\n","26ece441":"from pyspark.ml.feature import VectorAssembler\n# assemblerInput.clear()\ndef _vectAssembler(_stages):\n    assemblerInput =  [f  for f in numeric_features]  \n    assemblerInput += [f + \"_Enc\" for f in string_features]\n    print(assemblerInput) \n    # assemblerInput.append(\"sexIndex\")\n    vectAssembler = VectorAssembler(inputCols  = assemblerInput, outputCol = \"vect_features\") #.fit(sdf_train_subset)  \n    _stages += [vectAssembler]\n","5127ac75":"from pyspark.ml.feature import PCA\n# help(PCA)\ndef _pac(_stages):\n    pca = PCA(inputCol = \"vect_features\", outputCol = \"pca_features\",k = 5)\n    _stages += [pca]","4415098f":"from pyspark.ml.classification import RandomForestClassifier\n# help(RandomForestClassifier)\ndef _rf(_stages):\n    rf = RandomForestClassifier(labelCol = 'Survived', featuresCol = 'pca_features', numTrees = 100, maxDepth = 4, maxBins = 1000)\n    _stages += [rf]","cd4c68e3":"# _stages += [oneHotEnc, vectAssembler,rf]\n_stages = []\n_strIndexer(_stages)\n_oneHotEnc(_stages)\n_vectAssembler(_stages)\n_pac(_stages)\n_rf(_stages)\n_stages","789cb3a9":"from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = _stages)","75340f93":"model = pipeline.fit(sdf_train_cleaned)","ef6fdf38":"numeric_cols_test = ['PassengerId', 'Pclass','Age', 'SibSp','Parch','Ticket','Fare', 'Sex'] \n\nsdf_test_subset = sdf_test.withColumn('Ticket', sdf_test['Ticket'].cast(\"double\"))\n\nsdf_test_subset = _claenup(sdf_test_subset,['Fare','Age'])","42f6730c":"sdf_predict = model.transform(sdf_test_subset)","0ed36c0b":"sdf_predict.toPandas().T","0a45d4d3":"# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator \n\n# sdf_train_sample = sdf_train_cleaned.sample(False,0.1).cache()\n# # sdf_test_sample.count()\n# pca_model = PCA(inputCol = \"vect_features\", outputCol = \"pca_features\",k = 5)\n# cv_rf = RandomForestClassifier(labelCol = \"Survived\", featuresCol = \"pca_features\")\n# cv_pipeline = Pipeline(stages = [pca_model,rf])\n\n# param_grid = ParamGridBuilder().addGrid(pca_model.k, [10, 20, 30, 40, 50])\\\n#                                 .addGrid(cv_rf.numTrees, [20, 30, 50])\\\n#                                 .build()\n# cross_val = CrossValidator(estimator = cv_pipeline,\n#                           estimatorParamMaps = param_grid,\n#                           evaluator = BinaryClassificationEvaluator(labelCol= 'STP_UP_IND',\n#                                                                    rawPredictionCol = 'probability',\n#                                                                    metricName = 'areaUnderROC'),\n#                           numFolds = 3)\n# cv_model = cross_val.fit(sdf_test_sample)\n\n# cv_prediction = cv_model.transform(sdf_test_subset)\n\n# evaluator= BinaryClassificationEvaluator(labelCol = \"STP_UP_IND\", rawPredictionCol=\"probability\", metricName= \"areaUnderROC\")\n# accuracy = evaluator.evaluate(predictions)\n# accuracy\n# # eval = BinaryClassificationEvaluator()\nevaluator = BinaryClassificationEvaluator(labelCol = 'prediction')\nevaluator.evaluate(sdf_predict)","8fcc115c":"sdf_submission = sdf_predict.select('PassengerId','prediction').withColumn('Survived',sdf_predict['prediction'].cast('integer')).select('PassengerId','Survived')\nsdf_submission.toPandas().T","4d0c51b4":"sdf_submission.coalesce(1).write.csv(\"submission\",mode=\"overwrite\",header=True)\n","7579fa7b":"print(os.listdir('submission'))\n","5962d477":"<a href=\"submission\/part-00000-6bf2b4f5-566e-474a-adc4-7981ac0609ea-c000.csv\">Download Submission<\/a>","5cf22d81":"### Please note this is a INPROGRESS notebook!"}}