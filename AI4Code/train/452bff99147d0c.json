{"cell_type":{"ebb08415":"code","6f8c71a4":"code","2b44e461":"code","e83b49a8":"code","53bc6499":"code","bf52313f":"code","74faa188":"code","10521a54":"code","d6375d0d":"code","f927187e":"code","a5762a53":"code","cc130da3":"code","73dcb365":"code","20aa5b06":"code","2d1c29c1":"code","c2fac1af":"code","bc8094b0":"code","91cec98c":"code","3a0471e4":"code","d28da5c2":"code","058cf401":"code","d2bb6d8a":"code","a855b04d":"code","aeeca5e6":"code","cf6d7a73":"code","18925337":"code","6023c6a3":"code","0af826d4":"code","49dca39a":"code","48120493":"code","15be88bb":"code","9da32f0f":"code","f3b79c26":"code","7e2d4a54":"code","2cb1b459":"code","4fca31f1":"code","ca19ca38":"code","26e2cbab":"code","045eb743":"code","0e09afaf":"code","ce682a4b":"code","8faee295":"code","fe5b6d21":"code","09a926dc":"markdown","62b923c3":"markdown","190d8e5c":"markdown","8bced304":"markdown","13b34d21":"markdown","fca25110":"markdown","9e536380":"markdown","ba38da4f":"markdown","da45c12b":"markdown","99827454":"markdown","472e3464":"markdown"},"source":{"ebb08415":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f8c71a4":"## Imporing neccesary libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n","2b44e461":"pd.set_option('display.max_rows',100000)\npd.set_option('display.max_colwidth',None)","e83b49a8":"# Loading the train and the test dataset\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","53bc6499":"train.head() # Lets look at few columns in the train dataset","bf52313f":"test.head() # Lets look at few columns in the test dataset","74faa188":"# First let's create a copy of our dataset so that our original dataset is not tempered\ndf_train = train.copy()\ndf_test = test.copy()","10521a54":"df_train.head() # check whether it is properly copied","d6375d0d":"df_test.head()","f927187e":"df_train.info()","a5762a53":"# Looking for descriptive statistic \n# By defualt it will show the statistic of numeric column\ndf_train.describe()","cc130da3":"# If you want to see descriptive statistic of object\/string columns use *include*\ndf_train.describe(include = 'object')","73dcb365":"# Checking for null value\n(df_train.isnull().sum()\/df_train.shape[0])*100","20aa5b06":"df_train.loc[(df_train.keyword.isnull() == True),:].head()","2d1c29c1":"df_train.loc[(df_train.location.isnull() == True),:].head()","c2fac1af":"# Checking for null values in test dataset\n(df_test.isnull().sum()\/df_test.shape[0])*100","bc8094b0":"df_test.loc[(df_test.location.isnull() == True),:].head()","91cec98c":"df_test.loc[(df_test.keyword.isnull() == True),:].head()","3a0471e4":"# Lets look at the shape of the train dataset\ndf_train.shape","d28da5c2":"# Lets look at the shape of the test dataset\ndf_test.shape","058cf401":"# Lets look at some value counts\n# Checking only 5 values and the output becomes more longer. Feel free toh remo [:5] and check for all the fields\ncols = df_train.columns\nfor i in range(0,len(cols)):\n    print(\"Column :\", cols[i].upper())\n    print(df_train[cols[i]].value_counts(dropna = True)[:5])\n    print('********************************************')","d2bb6d8a":"# Lets look at some value counts\ncols = df_test.columns\nfor i in range(0,len(cols)):\n    print(\"Column :\", cols[i].upper())\n    print(df_test[cols[i]].value_counts(dropna = True)[:5])\n    print('********************************************')","a855b04d":"# Looking for the distribution of Target variable\nsns.set_style()\nsns.countplot(data= df_train,x = df_train['target'],palette = 'rocket')\nplt.title ('Distribution of Target variable')\nplt.xlabel(\"Target\")\nplt.ylabel(\"Count of Target\")\nplt.show()","aeeca5e6":"df_train['target'].value_counts()","cf6d7a73":"\nsns.distplot(df_train['target'])\nplt.show()","18925337":"# Plot 20 keywords from teh dataset\nsns.barplot(y=df_train['keyword'].value_counts()[:20].index,x=df_train['keyword'].value_counts()[:20])","6023c6a3":"df_train.loc[df_train['text'].str.contains('disaster', na=False, case=False)].target.value_counts()","0af826d4":"# Plot 10 location from the dataset\nsns.barplot(y=df_train['location'].value_counts()[:10].index,x=df_train['location'].value_counts()[:10],\n            orient='h')","49dca39a":"# Replacing the ambigious locations name with Standard names\ndf_train['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                            \"Chicago, IL\":'USA',\n                            \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\",\n                            \"Sao Paulo, Brazil\" : \"Brazil\"},inplace=True)","48120493":"# Plot the barplot and check whether the location column has changed or not\nsns.barplot(y=df_train['location'].value_counts()[:10].index,x=df_train['location'].value_counts()[:10],\n            orient='h')","15be88bb":"# Let's look at the ss column whether we can find some insights from that data or not\ndf_train.loc[df_train['location']=='ss',:]","9da32f0f":"# Let's plot the character in the tweet\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=df_train[df_train['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=df_train[df_train['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","f3b79c26":"# Let's plot the words from the text\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=df_train[df_train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=df_train[df_train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","7e2d4a54":"# Let's look at the top 25 rows to understand what all things we need to take care when cleaning the corpus\/ text field\ndf_train['text'][:25]","2cb1b459":"# Importing re library to clean the text\nimport re\n\n# Importing string library to remove\/escape the punctuations from the text column\nimport string \n\n# Preprocessing the text field\ndef preprocessing (text):\n    '''\n    Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.\n    '''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[#|@|!|$|%|\u0089|^|&|*|(|)|[|{|[|\\]]','',text)\n    text = re.sub('im','i am',text)\n    text = re.sub('\u00fb','u',text)\n    text = text.strip()\n    \n    return text\n\n        ","4fca31f1":"# removing emoji's\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n","ca19ca38":"# applying the preprocessing and remove_emoji function on the train and test dataset\ndf_train['text'] = df_train['text'].apply(lambda x:preprocessing(x))\ndf_train['text'] = df_train['text'].apply(lambda x:remove_emoji(x))\n\ndf_test['text'] = df_test['text'].apply(lambda x:preprocessing(x))\ndf_test['text'] = df_test['text'].apply(lambda x:remove_emoji(x))","26e2cbab":"# checking whether the data is cleaned or not\ndf_train['text'][:5]","045eb743":"# Stopwords removal \n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\ndef remove_stopwords(string):\n    word_list = [word for word in string.split()]\n    stopwords_list = list(stopwords.words(\"english\"))\n    for word in word_list:\n        if word in stopwords_list:\n            word_list.remove(word)\n    return(' '.join(word_list))\n        \ndf_train['text'] = list(map(lambda x: remove_stopwords(x), df_train['text']))\n\ndf_test['text'] = list(map(lambda x: remove_stopwords(x), df_test['text']))","0e09afaf":"df_train.head()","ce682a4b":"#not a disaster tweet\nnon_disaster_tweets = df_train[df_train['target']==0]['text']\nnon_disaster_tweets.values[1]\n\n# A disaster tweet\ndisaster_tweets = df_train[df_train['target']==1]['text']\ndisaster_tweets.values[1]\n","8faee295":"# Let's plot the wordcloud to see which words are more occuring\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=30);\n\nwordcloud2 = WordCloud( width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=30);","fe5b6d21":"df_train['text'][66]","09a926dc":"## Let's do some Data Quality checks","62b923c3":"As we all know, Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster.\n\nIn this competition, we are going to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. You\u2019ll have access to a dataset of 10,000 tweets that were hand classified.\n\nWe are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.","190d8e5c":"# Real or Not? NLP with Disaster Tweets","8bced304":"## Let's clean the corpus now","13b34d21":"Let's visualise some other features to understand the dataset more clearly","fca25110":"As we can see here in test dataset we don't have target column. \nSo here id,keyword,location and text are our X variables and the target variable is our Y variable.\n\n**Columns**\n1. id - a unique identifier for each tweet\n2. text - the text of the tweet\n3. location - the location the tweet was sent from (may be blank)\n4. keyword - a particular keyword from the tweet (may be blank)\n5. target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","9e536380":"As we can see that the graph is a bimodal graph ","ba38da4f":"## Now let's visualise the data ","da45c12b":"That's it for now. I hope you like the EDA part. Feedbacks are appriciated. If you like my work plese upvote as it motivates me to build more notebooks and start my carrer in Data Science. Thanks :) ","99827454":"**Now let's summarise the Data Quality check steps :**\n\n1. Check for few rows in the train and test dataset for understanding the data using head() method.\n2. Used info() for checking the datatype of the column, the number of rows and columns in train and test dataset.\n3. Used describe() for checking the descriptive statistic of the datasets.\n4. Checked for the null values in train and test dataset\n5. Check for value counts of each columns\n---------------------------------------------------------------------------\n* We can see that the **id** and **target** varibale are int64 columns where as keywords,location and text columns are object. \n* There are total 5 columns and 7613 rows (from 0 - to 7612) in the dataset.\n* The column keyword(~ 33 %) and location (~ 80%) contains null values in bith train and test dataset.\n","472e3464":"As we can see from the above barplot that many cities are represened as a country. Let's go ahead and replace the cities with the country name so that it can be easioy identified"}}