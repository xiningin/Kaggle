{"cell_type":{"dd2b372e":"code","cd6286bb":"code","4a1b3efe":"code","532aafa6":"code","5ba63bde":"code","4d0a144f":"code","6c010b5d":"code","8aed8ed4":"code","8e21d962":"code","4ccee11b":"code","3a1f53f5":"code","06282553":"code","6143286c":"code","b8729263":"code","331baf93":"code","68265d77":"code","aef76f68":"code","0503039f":"code","3a503f7e":"code","84485d67":"code","18202e84":"code","5cffc5ef":"code","333d4871":"code","72065f3e":"code","d1c9545e":"markdown","b67a9be8":"markdown","d5c7f100":"markdown","f7110394":"markdown","3249c95a":"markdown","cfe9d14b":"markdown","463c7483":"markdown","ce8ef096":"markdown","7862e59e":"markdown","226a995d":"markdown","542ee3b0":"markdown","88f3c535":"markdown"},"source":{"dd2b372e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport pytorch_lightning as pl\nimport torchvision.transforms as T\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport torchvision.utils as vutils\n\n# from pl_bolts.datamodules import CIFAR10DataModule, ImagenetDataModule\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd6286bb":"# Reading the images\nimage_names = os.listdir('..\/input\/animefacedataset\/images')\nprint(len(image_names))","4a1b3efe":"# splitting images into train and validation\ntrain_img_list = image_names[:50000]\nval_img_list = image_names[50000:]","532aafa6":"# Creating a dataset class\nclass DatasetAnime(Dataset):\n    \n    def __init__(self,img_list,image_dir,transform=None):\n        '''\n        Input\n        img_list: list of image names\n        image_dir: directory of images\n        '''\n        self.transform = transform\n        self.image_dir = image_dir\n        self.img_list = img_list\n        \n    def __len__(self):\n        '''\n        return length of dataset\n        '''\n        return len(self.img_list)\n    \n    def __getitem__(self, index):\n        '''\n        Input \n        index: image index\n        Image is read, standardized and converted into torch variable\n        Output: return the image and label(same as image) after converting into torch variable\n        '''\n        img_path = self.image_dir + self.img_list[index]\n        image = Image.open(img_path).convert('RGB')\n        label = Image.open(img_path).convert('RGB')\n        image = image.resize((64,64))\n        label = label.resize((64,64))\n        image = np.asarray(image, dtype=np.float32)\/255\n        label = np.asarray(label, dtype=np.float32)\/255\n        image = torch.from_numpy(image)\n        label = torch.from_numpy(label)\n\n        image = image.permute(2,0,1)\n        label = label.permute(2,0,1)\n\n        return image, label","5ba63bde":"# Creating the train_loader and validation loader\nimage_dir = '..\/input\/animefacedataset\/images\/'\nbatch_size = 64\ntrain_dataset = DatasetAnime(train_img_list,image_dir,transform=None)\ntrain_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n\nval_dataset = DatasetAnime(val_img_list,image_dir,transform=None)\nval_loader = DataLoader(val_dataset, batch_size, shuffle=True, num_workers=3, pin_memory=True)","4d0a144f":"# function to visualize generated sample images\ndef show_samples(batch_tensor,n_row):\n    grid = torchvision.utils.make_grid(batch_tensor, nrow=n_row)\n    plt.imshow(grid.permute(1, 2, 0).detach())","6c010b5d":"class VAE(LightningModule):\n    def __init__(self,in_channels,latent_dim):\n        '''\n        Input\n        in_channels: number of channels of image\n        latent_dim: latent dimension for VAE\n        \n        We will initialize model layers\n        '''\n        super(VAE, self).__init__()\n        hidden_dims = [32,64,128,256]\n        self.latent_dim = latent_dim\n        modules=[]\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels=h_dim,\n                              kernel_size= 3, stride= 2, padding  = 1),\n                    nn.BatchNorm2d(h_dim),\n                    nn.LeakyReLU())\n            )\n            in_channels = h_dim\n        \n        self.encoder = nn.Sequential(*modules)\n        self.fc_mu = nn.Linear(hidden_dims[-1]*4*4, latent_dim)\n        self.fc_var = nn.Linear(hidden_dims[-1]*4*4, latent_dim)\n        self.avg_train_loss = {'loss': 0, 'Reconstruction_Loss':0, 'KLD':0}\n        self.avg_val_loss = {'loss': 0, 'Reconstruction_Loss':0, 'KLD':0}\n        self.temp_train = {}\n        self.temp_val = {}\n        hidden_dims.reverse()\n        modules = []\n        for i in range(len(hidden_dims) - 1):\n            modules.append(\n                nn.Sequential(\n                    nn.ConvTranspose2d(hidden_dims[i],\n                                       hidden_dims[i + 1],\n                                       kernel_size=3,\n                                       stride = 2,\n                                       padding=1,\n                                       output_padding=1),\n                    nn.BatchNorm2d(hidden_dims[i + 1]),\n                    nn.LeakyReLU())\n            )\n\n        self.decoder_input = nn.Linear(latent_dim, hidden_dims[0]*4*4)\n\n        self.decoder = nn.Sequential(*modules)\n\n        self.final_layer = nn.Sequential(\n                            nn.ConvTranspose2d(hidden_dims[-1],\n                                               hidden_dims[-1],\n                                               kernel_size=3,\n                                               stride=2,\n                                               padding=1,\n                                               output_padding=1),\n                            nn.BatchNorm2d(hidden_dims[-1]),\n                            nn.LeakyReLU(),\n                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n                                      kernel_size= 3, padding= 1),\n                            nn.Sigmoid())\n    \n    def train_dataloader(self):\n        '''\n        returning a train_loader\n        '''\n        return train_loader\n    \n    def val_dataloader(self):\n        '''\n        returning val_loader\n        '''\n        return val_loader\n        \n    def encode(self,input):\n        '''\n        Creating Encoder model from encoder layers\n        '''\n        result = self.encoder(input)\n\n        result = torch.flatten(result, start_dim=1)\n\n        # Split the result into mu and var components\n        # of the latent Gaussian distribution\n        mu = self.fc_mu(result)\n        log_var = self.fc_var(result)\n        return mu, log_var\n    \n    def decode(self,z):\n        '''\n        Creating Decoder model from decoder layers\n        '''\n        result = self.decoder_input(z)\n        result = result.view(-1,256 ,4, 4)\n        result = self.decoder(result)\n        result = self.final_layer(result)\n        return result\n    \n    def reparameterize(self, mu, logvar):\n        '''\n        Reparameterization\n        '''\n        std = torch.exp(logvar \/ 2)\n        q = torch.distributions.Normal(mu, std)\n        z = q.rsample()\n        return z\n    \n    \n    def forward(self, input):\n        '''\n        Forward propagation\n        '''\n        mu,log_var = self.encode(input)\n        z = self.reparameterize(mu, log_var)\n        return  self.decode(z), mu, log_var\n    \n    \n    def loss_function(self,recons,label,mu,log_var):\n        '''\n        Loss function comprising of reconstruction loss and KL divergence loss\n        '''\n        alpha = 2\n        MSE = 0.5 * torch.sum(torch.pow(label - recons, 2))\n        KLD = -0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp())\n        loss = MSE+ alpha*KLD\n        return loss\n    \n    def sample(self,num_samples):\n        '''\n        Input\n        num_samples: number of samples\n        generate image samples from VAE\n        '''\n        z = torch.randn(num_samples,self.latent_dim)\n        samples = self.decode(z)\n        return samples\n                       \n    def generate(self, x):\n        '''\n        generate a reconstructed image on a real image\n        '''\n        return self.forward(x)[0]\n                       \n    def training_step(self, batch, batch_idx):\n        '''\n        training step for model\n        '''\n        real_img, label = batch\n        recons,mu,log_var = self.forward(real_img)\n        train_loss = self.loss_function(recons,label,mu,log_var)\n        return train_loss\n        \n    \n    def validation_step(self,batch,batch_idx):\n        '''\n        validation step for model\n        '''\n        real_img, label = batch\n        recons,mu,log_var = self.forward(real_img)\n        val_loss = self.loss_function(recons,label,mu,log_var)\n        return val_loss\n    \n    def configure_optimizers(self):\n        '''\n        Optimizer for model training\n        '''\n        return optim.Adam(self.parameters(),lr=1e-3)","8aed8ed4":"## model training\nmodel = VAE(3,512)\ntrainer = pl.Trainer(gpus=1,max_epochs=50)\ntrainer.fit(model)","8e21d962":"# Ploting the Sample Images\nx = model.sample(16)","4ccee11b":"show_samples(x,4)","3a1f53f5":"trainer.save_checkpoint(\"VAE_50epoch.ckpt\")","06282553":"class HVAE(LightningModule):\n    def __init__(self,in_channels,z1_dim = 256, z2_dim = 256):\n        '''\n        Input\n        in_channels: number of channels of image\n        z1_dim: latent dimension for z1\n        z2_dim: latent dimension for z2\n        We will initialize model layers\n        '''\n        super(HVAE, self).__init__()\n        hidden_dims = [32,64,64]\n        self.z2_dim = z2_dim\n        self.z1_dim = z1_dim\n        modules=[]\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels=in_channels, out_channels=h_dim,\n                              kernel_size= 3, stride= 2, padding  = 1),\n                    nn.BatchNorm2d(h_dim),\n                    nn.LeakyReLU())\n            )\n            in_channels = h_dim\n            \n        modules.append(nn.Sequential(nn.Conv2d(hidden_dims[2],8,kernel_size= 3, stride= 1, padding  = 1)))\n        \n        hid_dim = hidden_dims[-1]*8\n        self.q_z2_layers = nn.Sequential(*modules)\n        self.q_z1_layers_x = nn.Sequential(*modules)\n        \n        self.q_z2_mean = nn.Linear(hid_dim, self.z2_dim)\n        self.q_z2_logvar = nn.Sequential(nn.Linear(hid_dim, self.z2_dim),nn.Hardtanh())\n        \n        self.q_z1_layers_z2 = nn.Sequential(nn.Linear(self.z2_dim, hid_dim))\n        \n        self.q_z1_layers_joint = nn.Sequential(\n            nn.Linear( 2 * hid_dim, 512)\n        )\n        \n        self.q_z1_mean = nn.Linear(512, self.z1_dim)\n        self.q_z1_logvar = nn.Sequential(nn.Linear(512, self.z1_dim),nn.Hardtanh())\n        \n        self.p_z1_layers = nn.Sequential(\n            nn.Linear(self.z2_dim, 512),\n            nn.Linear(512, 512)\n        )\n        self.p_z1_mean = nn.Sequential(nn.Linear(512, self.z1_dim))\n        self.p_z1_logvar = nn.Sequential(nn.Linear(512, self.z1_dim),nn.Hardtanh()) \n        \n        \n        self.p_x_layers_z1 = nn.Sequential(\n            nn.Linear(self.z1_dim, 512)\n        )\n        self.p_x_layers_z2 = nn.Sequential(\n            nn.Linear(self.z2_dim, 512)\n        )\n        \n        self.p_x_layers_joint_pre = nn.Sequential(\n            nn.Linear(2 * 512, 8*8*64)\n        )\n        \n        hidden_dims.reverse()\n        modules = []\n        for i in range(len(hidden_dims)-1):\n            modules.append(\n                nn.Sequential(\n                    nn.ConvTranspose2d(in_channels=hidden_dims[i], out_channels=hidden_dims[i+1],\n                              kernel_size= 3, stride= 2, padding  = 1,output_padding=1),\n                    nn.BatchNorm2d(hidden_dims[i+1]),\n                    nn.LeakyReLU())\n            )\n            \n\n        self.p_x_layers_joint = nn.Sequential(*modules)\n        self.p_x_mean = nn.Sequential(\n                            nn.ConvTranspose2d(hidden_dims[-1],\n                                               hidden_dims[-1],\n                                               kernel_size=3,\n                                               stride=2,\n                                               padding=1,\n                                               output_padding=1),\n                            nn.BatchNorm2d(hidden_dims[-1]),\n                            nn.LeakyReLU(),\n                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n                                      kernel_size= 3, padding= 1),\n                            nn.Sigmoid())\n        \n        self.p_x_logvar = nn.Sequential(\n            nn.ConvTranspose2d(hidden_dims[-1],\n                                               hidden_dims[-1],\n                                               kernel_size=3,\n                                               stride=2,\n                                               padding=1,\n                                               output_padding=1),\n                            nn.BatchNorm2d(hidden_dims[-1]),\n                            nn.LeakyReLU(),\n                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n                                      kernel_size= 3, padding= 1),\n                            nn.Hardtanh(min_val=-4.5, max_val=0.)\n        )\n    \n    def train_dataloader(self):\n        '''\n        returns the train_loader\n        '''\n        return train_loader\n    \n    def val_dataloader(self):\n        '''\n        return the val_loader\n        '''\n        return val_loader\n    \n    def q_z2(self, x):\n        '''\n        Encoder part\n        '''\n        # processing x\n        h = self.q_z2_layers(x)\n        h = torch.flatten(h, start_dim=1)\n        # predict mean and variance\n        z2_q_mean = self.q_z2_mean(h)\n        z2_q_logvar = self.q_z2_logvar(h)\n        return z2_q_mean, z2_q_logvar\n    \n    def q_z1(self, x, z2):\n        '''\n        Encoder part\n        '''\n        # processing x\n        x = self.q_z1_layers_x(x)\n        x = x.view(x.size(0),-1)\n        # processing z2\n        z2 = self.q_z1_layers_z2(z2)\n        # concatenating\n        h = torch.cat((x,z2),1)\n        h = self.q_z1_layers_joint(h)\n        # predict mean and variance\n        z1_q_mean = self.q_z1_mean(h)\n        z1_q_logvar = self.q_z1_logvar(h)\n        return z1_q_mean, z1_q_logvar\n    \n    def p_z1(self, z2):\n        '''\n        Decoder part\n        '''\n        z2 = self.p_z1_layers(z2)\n        # predict mean and variance\n        z1_p_mean = self.p_z1_mean(z2)\n        z1_p_logvar = self.p_z1_logvar(z2)\n        return z1_p_mean, z1_p_logvar\n    \n    def p_x(self, z1, z2):\n        '''\n        Decoder part\n        '''\n        # processing z2\n        z2 = self.p_x_layers_z2(z2)\n        # processing z1\n        z1 = self.p_x_layers_z1(z1)\n        # concatenate x and z1 and z2\n        h = torch.cat((z1, z2), 1)\n        h = self.p_x_layers_joint_pre(h)\n        h = h.view(-1, 64, 8,8)\n\n        # joint decoder part of the decoder\n        h_decoder = self.p_x_layers_joint(h)\n        x_mean = self.p_x_mean(h_decoder)\n        x_logvar =self.p_x_logvar(h_decoder)\n        return x_mean, x_logvar\n    \n    def reparameterize(self,mu, logvar):\n        '''\n        Reparameterization\n        '''\n        std = torch.exp(logvar \/ 2)\n        q = torch.distributions.Normal(mu, std)\n        z = q.rsample()\n        return z\n    \n    def reconstruction_loss(self,recons,x):\n        '''\n        Reconstruction loss\n        '''\n        recon_loss = 0.5 * torch.sum(torch.pow(x - recons, 2))\n        return recon_loss\n    \n    def kl_loss_function(self,mu,logvar):\n        '''\n        KL divergence loss\n        '''\n        kl_loss = -0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp())\n        return kl_loss\n    \n    def loss_function(self,x,x_mean,x_logvar, z1_q, z1_q_mean, z1_q_logvar, z2_q,\n                       z2_q_mean, z2_q_logvar, z1_p_mean, z1_p_logvar):\n        '''\n        loss function\n        reconstruction and kl divergence loss\n        '''\n        recons_loss = self.reconstruction_loss(x_mean,x)\n        \n        kl_loss_qz1 = self.kl_loss_function(z1_q_mean,z1_q_logvar)\n        kl_loss_qz2 = self.kl_loss_function(z2_q_mean,z2_q_logvar)\n        kl_loss_pz1 = self.kl_loss_function(z1_p_mean,z1_p_logvar)\n        kl_loss_pz2 =  torch.sum(-0.5 * torch.pow(z2_q, 2))\n\n        \n        loss = recons_loss + 2*(kl_loss_qz1 + kl_loss_qz2 + kl_loss_pz1 + kl_loss_pz2)\n        return loss\n        \n    def training_step(self, batch, batch_idx):\n        '''\n        training step for model\n        '''\n        real_img, label = batch\n        x_mean,x_logvar, z1_q, z1_q_mean, z1_q_logvar, z2_q,z2_q_mean,z2_q_logvar,z1_p_mean, z1_p_logvar = self.forward(real_img)\n        train_loss = self.loss_function(real_img,x_mean,x_logvar ,z1_q, z1_q_mean, z1_q_logvar, z2_q,\n                       z2_q_mean, z2_q_logvar, z1_p_mean, z1_p_logvar)\n        return train_loss\n    \n    def configure_optimizers(self):\n        '''\n        optimizer for training\n        '''\n        return optim.Adam(self.parameters(),lr=1e-3)\n    \n    def generate(self, x):\n        '''\n        generate a reconstructed image on a real image\n        '''\n        x_reconstructed, _, _, _, _, _, _, _, _, _ = self.forward(x)\n        return x_reconstructed\n        \n    def sample(self,num_samples):\n        '''\n        Input\n        num_samples: number of samples\n        generate image samples from VAE\n        '''\n        z2 = torch.randn(num_samples,self.z2_dim)\n        z1_mean, z1_logvar = self.p_z1(z2)\n        z1_rand = self.reparameterize(z1_mean, z1_logvar)\n        samples_gen, _ = self.p_x(z1_rand, z2)\n        return samples_gen\n        \n    def forward(self,x):\n        '''\n        Forward pass\n        '''\n        # z2 ~ q(z2 | x)\n        z2_q_mean, z2_q_logvar = self.q_z2(x)\n        z2_q = self.reparameterize(z2_q_mean, z2_q_logvar)\n\n        # z1 ~ q(z1 | x, z2)\n        z1_q_mean, z1_q_logvar = self.q_z1(x, z2_q)\n        z1_q = self.reparameterize(z1_q_mean, z1_q_logvar)\n\n        # p(z1 | z2)\n        z1_p_mean, z1_p_logvar = self.p_z1(z2_q)\n\n        # x_mean = p(x|z1,z2)\n        x_mean,x_logvar = self.p_x(z1_q, z2_q)\n        return x_mean, x_logvar, z1_q, z1_q_mean, z1_q_logvar, z2_q, z2_q_mean, z2_q_logvar, z1_p_mean, z1_p_logvar\n    \n    def validation_step(self,batch,batch_idx):\n        '''\n        validation step for model\n        '''\n        real_img, label = batch\n\n        x_mean,x_logvar, z1_q, z1_q_mean, z1_q_logvar, z2_q,z2_q_mean,z2_q_logvar,z1_p_mean, z1_p_logvar = self.forward(real_img)\n        val_loss = self.loss_function(real_img,x_mean,x_logvar ,z1_q, z1_q_mean, z1_q_logvar, z2_q,\n                       z2_q_mean, z2_q_logvar, z1_p_mean, z1_p_logvar)\n        return val_loss\n     ","6143286c":"# model training\nmodel = HVAE(3)\ntrainer = pl.Trainer(gpus=1,max_epochs=50)\ntrainer.fit(model)","b8729263":"# Generating Sample images\nx = model.sample(16)","331baf93":"# epoch = 50\nshow_samples(x,4)","68265d77":"trainer.save_checkpoint(\"HVAE_50epoch.ckpt\")","aef76f68":"# Discriminator class\nclass Discriminator(nn.Module):\n    def __init__(self,in_channels):\n        '''\n        Create a discriminator model layers\n        '''\n        super(Discriminator, self).__init__()\n        \n        modules = []\n        hidden_dims = [64,128,256,512]\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels=in_channels, out_channels=h_dim,\n                              kernel_size= 4, stride= 2, padding  = 1),\n                    nn.BatchNorm2d(h_dim),\n                    nn.LeakyReLU())\n            )\n            in_channels = h_dim\n        \n        self.descriminator = nn.Sequential(*modules)\n        self.final_layer = nn.Sequential(nn.Linear(512*4*4,1),nn.Sigmoid())\n        \n    def forward(self,x):\n        '''\n        forward proapagation\n        '''\n        x = self.descriminator(x)\n        x = x.view(-1,512*4*4)\n        x = self.final_layer(x)\n        return x","0503039f":"# Generator class\nclass Generator(nn.Module):\n    \"\"\"Generates artificial images form a random vector as input.\n    \"\"\"\n    def __init__(self,in_channels):\n        '''\n        Create the generator layers\n        '''\n        super(Generator, self).__init__()\n        \n        modules = []\n        hidden_dims = [512,256,128,64,64]\n        \n        for h_dim in hidden_dims:\n            if h_dim==512:\n                 modules.append(nn.Sequential(\n                    nn.ConvTranspose2d(in_channels=in_channels, out_channels=h_dim,\n                              kernel_size= 4, stride= 1, padding  = 1),\n                    nn.BatchNorm2d(h_dim),\n                    nn.LeakyReLU())\n                               )\n            else:\n                modules.append(\n                    nn.Sequential(\n                        nn.ConvTranspose2d(in_channels=in_channels, out_channels=h_dim,\n                                  kernel_size= 4, stride= 2, padding  = 1),\n                        nn.BatchNorm2d(h_dim),\n                        nn.LeakyReLU())\n                )\n            in_channels = h_dim\n        \n        self.generator = nn.Sequential(*modules)\n        \n        self.final_layer = nn.Sequential(nn.ConvTranspose2d(in_channels=64, out_channels=3,\n                              kernel_size= 4, stride= 2, padding  = 1),nn.Sigmoid())\n\n    def forward(self, x):\n        '''\n        forward propagation\n        '''\n        x = self.generator(x)\n        x = self.final_layer(x)\n        return x\n","3a503f7e":"device='cuda'\nclass GAN(LightningModule):\n    def __init__(self):\n        super(GAN, self).__init__()\n        self.generator = Generator(100).to(device)\n        self.discriminator = Discriminator(3).to(device)\n    \n    def loss_function(self,y_hat,y):\n        '''\n        Implementing a binary cross entropy loss\n        '''\n        bce = nn.BCELoss()(y_hat, y)\n        return bce\n    \n    def train_dataloader(self):\n        '''\n        returns the train_loader\n        '''\n        return train_loader \n    \n    def forward(self, z):\n        '''\n        Forward pass of generator to generate images\n        '''\n        return self.generator(z)\n    \n    def discriminator_step(self, x, noise,valid,fake):\n        '''\n        Discriminator step to identify whether an image is real or fake\n        '''\n        fake_images = self.generator(noise)\n        # get discriminator outputs\n        real_logits = self.discriminator(x)\n        fake_logits = self.discriminator(fake_images.detach())\n        # real loss\n        real_loss = self.loss_function(real_logits, valid)\n        # fake loss\n        fake_loss = self.loss_function(fake_logits, fake)\n        disc_loss = (fake_loss + real_loss) \/ 2\n\n        return disc_loss\n    \n    def generator_step(self,x,noise,valid,fake):\n        '''\n        Generator step to generate images to fool discriminator\n        '''\n        # generate fake images\n        fake_images = self.generator(noise)\n\n        fake_logits = self.discriminator(fake_images)\n        fake_loss = self.loss_function(fake_logits, valid)\n\n        gen_loss = fake_loss\n\n        return gen_loss\n\n    \n    def training_step(self, batch, batch_idx, optimizer_idx):\n        '''\n        training step to train both generator and discriminator\n        '''\n        # batch returns x and y tensors\n        real_images, _ = batch\n        real_images = real_images.to(device)\n        # Generating Noise (input for the generator)\n        noise = torch.randn(real_images.shape[0], 100,1,1).to(device)\n        # ground truth (tensors of ones and zeros) same shape as images\n        valid = torch.ones(real_images.size(0), 1).to(device)\n        fake = torch.zeros(real_images.size(0), 1).to(device)\n        valid = valid\n        fake = fake\n        # As there are 2 optimizers we have to train for both using 'optimizer_idx'\n        ## Generator\n        if optimizer_idx == 0:\n            loss = self.generator_step(real_images, noise,valid,fake)\n        \n        if optimizer_idx == 1:\n            loss = self.discriminator_step(real_images, noise,valid,fake)\n        \n        return loss\n    \n    def configure_optimizers(self):\n        '''\n        Initializing the optimizer for model training\n        '''\n        optimizer_G = torch.optim.Adam(self.generator.parameters(), lr=1e-3,betas=(0.4, 0.999))\n        optimizer_D = torch.optim.Adam(self.discriminator.parameters(), lr=1e-3,betas=(0.4, 0.999))\n        \n        # return the list of optimizers and second empty list is for schedulers (if any)\n        return [optimizer_G, optimizer_D], []","84485d67":"# model training\nmodel = GAN()\ntrainer = pl.Trainer(gpus=1, max_epochs=50)\ntrainer.fit(model)","18202e84":"# Generating sample images from GAN\ngen_input = torch.randn(16, 100,1,1)\n# Converting noise to images\ngen_images = model.generator(gen_input)","5cffc5ef":"# epochs = 50\nshow_samples(gen_images,4)","333d4871":"# epochs = 10\nshow_samples(gen_images,4)","72065f3e":"trainer.save_checkpoint(\"DCGAN_10epoch.ckpt\")","d1c9545e":"## VAE training\nNow we will will train the VAE model.","b67a9be8":"## Generative Adverserial Network\nNow we will train a DCGAN.","d5c7f100":"## HVAE Training","f7110394":"## Dataset Class\nNow we will create a dataset class for the dataloader","3249c95a":"![image.png](attachment:image.png)","cfe9d14b":"# Anime Image Generation With different flavors of Variational Auto-Encoders and Generative Adverserial Networks.","463c7483":"## HIERARICHAL VAE\nNow we will implement Hierarichal VAE. The below is the mathematical interpretation.","ce8ef096":"![image.png](attachment:image.png)","7862e59e":"The below image gives the higher layer view of VAE and HVAE, we are going to implement.","226a995d":"## VAE implementation\nFirst we will implement vanilla VAE for generating anime images","542ee3b0":"In this notebook we will implement different Vanilla VAE, HVAE, GAN to generate anime images.\n1. Vanilla VAE\n2. Hierarichal VAE\n3. DCGAN","88f3c535":"## GAN training\nNow we will train the GAN"}}