{"cell_type":{"27df9750":"code","f16e18ed":"code","8f380777":"code","74fbbc96":"code","be94218a":"code","2fadf513":"code","2ec3352b":"code","c1111b83":"code","2c78eaac":"code","53070f1f":"code","953347a9":"code","e000d0e7":"code","4271ff43":"code","83ba53c5":"code","9291e774":"code","10a7b312":"code","bf315236":"code","0168790d":"code","983247b4":"markdown","0b7bdd73":"markdown","f48da3ab":"markdown","970469d8":"markdown","bace8fd7":"markdown","51da3ef6":"markdown","20e82c25":"markdown","04ad34d5":"markdown"},"source":{"27df9750":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f16e18ed":"import matplotlib.pyplot as plt\nimport seaborn as sns","8f380777":"cc_data = pd.read_csv(r\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\n\ncc_data.head()","74fbbc96":"cc_data.describe()","be94218a":"cc_data.shape","2fadf513":"from sklearn.preprocessing import RobustScaler\n\nrob_scaler = RobustScaler()\n\ncc_data['Amount_Scaled'] = rob_scaler.fit_transform(cc_data['Amount'].values.reshape(-1,1))\ncc_data['Time_Scaled_'] = rob_scaler.fit_transform(cc_data['Time'].values.reshape(-1,1))\n\ncc_data.head()","2ec3352b":"fraud = cc_data[cc_data['Class'] == 1] \nvalid = cc_data[cc_data['Class'] == 0] \n \nprint('Number of Fraud Cases: {}'.format(len(cc_data[cc_data['Class'] == 1]))) \nprint('Number of Valid Transactions: {}'.format(len(cc_data[cc_data['Class'] == 0])))","c1111b83":"corrmat = cc_data.corr()\nfig = plt.figure(figsize = (15, 10))\n\nsns.heatmap(corrmat, vmax = .75, square = True)\nplt.show()","2c78eaac":"data = cc_data.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_data = data.loc[data['Class'] == 1]\nnon_fraud_data = data.loc[data['Class'] == 0][:492]\n\ndata1 = pd.concat([fraud_data, non_fraud_data])\n\n# Shuffle dataframe rows\nnew_data = data1.sample(frac=1, random_state=42)\n\nnew_data.drop(['Time','Amount'], axis=1, inplace=True)\n\nnew_data.head()","53070f1f":"print(new_data['Class'].value_counts())","953347a9":"X=new_data.drop(['Class'], axis=1)\nY=new_data[\"Class\"]\n\nX_data=X.values\nY_data=Y.values","e000d0e7":"from sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\nX_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size = 0.2, random_state = 42)","4271ff43":"from sklearn.metrics import classification_report, accuracy_score, precision_score,recall_score,f1_score,matthews_corrcoef\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score","83ba53c5":"#Logistic Regression\nlr_model = LogisticRegression(max_iter=500)\n\nlr_model.fit(X_train, Y_train)\nlr_y_pred = lr_model.predict(X_test)\n\nlr_acc=accuracy_score(Y_test,lr_y_pred)\nprint(f'Accuracy is {(lr_acc)}')\nlr_roc_auc=roc_auc_score(Y_test,lr_y_pred)\nprint(f'The ROC-AUC is {(lr_roc_auc)}')\nlr_prec= precision_score(Y_test,lr_y_pred)\nprint(f\"The precision is {(lr_prec)}\")\nlr_rec= recall_score(Y_test,lr_y_pred)\nprint(f\"The recall is {(lr_rec)}\")\nlr_f1= f1_score(Y_test,lr_y_pred)\nprint(f\"The F1-Score is {(lr_f1)}\")\nlr_MCC=matthews_corrcoef(Y_test,lr_y_pred)\nprint(f\"The Matthews correlation coefficient is {(lr_MCC)}\")","9291e774":"#Gaussian Naive Bayes\ngau_model = GaussianNB()\n\ngau_model.fit(X_train, Y_train)\ngau_y_pred = gau_model.predict(X_test)\n\ngau_acc=accuracy_score(Y_test,gau_y_pred)\nprint(f'Accuracy is {(gau_acc)}')\ngau_roc_auc=roc_auc_score(Y_test,gau_y_pred)\nprint(f'The ROC_AUC score is {(gau_acc)}')\ngau_prec= precision_score(Y_test,gau_y_pred)\nprint(f\"The precision is {(gau_prec)}\")\ngau_rec= recall_score(Y_test,gau_y_pred)\nprint(f\"The recall is {(gau_rec)}\")\ngau_f1= f1_score(Y_test,gau_y_pred)\nprint(f\"The F1-Score is {(gau_f1)}\")\ngau_MCC=matthews_corrcoef(Y_test,gau_y_pred)\nprint(f\"The Matthews correlation coefficient is {(gau_MCC)}\")","10a7b312":"#Random Forest\nrfc = RandomForestClassifier()\nrfc.fit(X_train,Y_train)\n\nrfc_y_pred = rfc.predict(X_test)\n\nrfc_acc=accuracy_score(Y_test,rfc_y_pred)\nprint(f'Accuracy is {(rfc_acc)}')\nrfc_roc_auc=roc_auc_score(Y_test,rfc_y_pred)\nprint(f'The ROC-AUC score is {(rfc_roc_auc)}')\nrfc_prec= precision_score(Y_test,rfc_y_pred)\nprint(f\"The precision is {(rfc_prec)}\")\nrfc_rec= recall_score(Y_test,rfc_y_pred)\nprint(f\"The recall is {(rfc_rec)}\")\nrfc_f1= f1_score(Y_test,rfc_y_pred)\nprint(f\"The F1-Score is {(rfc_f1)}\")\nrfc_MCC=matthews_corrcoef(Y_test,rfc_y_pred)\nprint(f\"The Matthews correlation coefficient is {(rfc_MCC)}\")","bf315236":"#KNearest Neighbours\nknn = KNeighborsClassifier()\nknn.fit(X_train,Y_train)\n\nknn_y_pred = knn.predict(X_test)\n\nknn_acc=accuracy_score(Y_test,knn_y_pred)\nprint(f'The accuracy is {(knn_acc)}')\nknn_roc_auc=roc_auc_score(Y_test,knn_y_pred)\nprint(f'The ROC-AUC score is {(knn_roc_auc)}')\nknn_prec= precision_score(Y_test,knn_y_pred)\nprint(f\"The precision is {(knn_prec)}\")\nknn_rec= recall_score(Y_test,knn_y_pred)\nprint(f\"The recall is {(knn_rec)}\")\nknn_f1= f1_score(Y_test,knn_y_pred)\nprint(f\"The F1-Score is {(knn_f1)}\")\nknn_MCC=matthews_corrcoef(Y_test,knn_y_pred)\nprint(f\"The Matthews correlation coefficient is {(knn_MCC)}\")","0168790d":"from sklearn.metrics import confusion_matrix\n\nfrom matplotlib import cm\n\nlr_cm = confusion_matrix(Y_test, lr_y_pred)\ngau_cm = confusion_matrix(Y_test, gau_y_pred)\nrfc_cm = confusion_matrix(Y_test, rfc_y_pred)\nknn_cm = confusion_matrix(Y_test, knn_y_pred)\n\nfig, ax = plt.subplots(2, 2,figsize=(22,12))\n\n\nsns.heatmap(lr_cm , ax=ax[0][0], annot=True, cmap=plt.cm.get_cmap('YlGnBu'))\nax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=14)\nax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(gau_cm, ax=ax[0][1], annot=True, cmap=plt.cm.get_cmap('YlGnBu'))\nax[0][1].set_title(\"Gaussian Naive-Bayes \\n Confusion Matrix\", fontsize=14)\nax[0][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(rfc_cm, ax=ax[1][0], annot=True, cmap=plt.cm.get_cmap('YlGnBu'))\nax[1][0].set_title(\"Random Forest Classifier \\n Confusion Matrix\", fontsize=14)\nax[1][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(knn_cm, ax=ax[1][1], annot=True, cmap=plt.cm.get_cmap('YlGnBu'))\nax[1][1].set_title(\"K-Nearest Neighbours \\n Confusion Matrix\", fontsize=14)\nax[1][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\n\nplt.show()","983247b4":"In the cell below, we import the models we intend to use. For this project we shall compare the performance of the Logistic Regression, Gaussian Naive Bayes, Random Forest and KNearest Neighbours classifiers. We also import the metrics to assess the model performance. ","0b7bdd73":"Below the proportions of fraud versus valid cases is shown. The fraud cases make up 0.17% of the data which shows that the data is imbalanced. This means that by using this data as is the model is prone to overfitting as there are nearly 100 non-fraud transactions for every fraudulent transaction. It can be seen from the correlation matrix below that it is not clear which elements have an influence on the class of the transaction.","f48da3ab":"The next step is to split the data into train and test sets. This is done by setting the **Class** column as the target column and specifying the size of the test data which is this case is 20% of the data in the dataframe.","970469d8":"Below it is seen that there are 31 columns. The **Time**  column shows how much time eleapsed between transactions, **V1** to **V28** show the principal components of transactions, **Amount** shows how much money was spent per transaction and **Class** is a variable that indicates whether a transaction is valid or fraudulent with fradulent transactions having a value of 1 and valid transactions having a value of 0. As shown below the mean amount spent per transaction is low at around 88 US$. As columns **V1** to **V28** have already been scaled, for uniformity purposes, **Amount** and **Time** should be scaled to. This scaling is done below.","bace8fd7":"Below is a side-by-side comparison of the performance of each model. From below, the Logistic Regression model performs the best across the board  followed by the KNearestNeighbours model. Recall is the ability of a classification model to correctly identify the relevant instances. Precision is the ability of a classification model to return only relevant instances.(<a href='https:\/\/towardsdatascience.com\/beyond-accuracy-precision-and-recall-3da06bea9f6c' target=\"_blank\">Will Koehrsen<\/a>) The F1 score is a function of precision and recall and aims to measure of the models accuracy and the closer the value is to 1, the more accurate the model is. (<a href='https:\/\/towardsdatascience.com\/accuracy-precision-recall-or-f1-331fb37c5cb9' target=\"_blank\">Koo Ping Shung<\/a>) The F1 score is a better measure of a models accuracy than just using accuracy especially when the model is imbalanced. The MCC takes into account the true-positives, true-negatives, false-negatives and false-positives, and a high value (close to 1) means that both classes are predicted well, even if one class is disproportionately under- (or over-) represented. (<a href='https:\/\/towardsdatascience.com\/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a' target=\"_blank\">Boaz Shmueli<\/a>)\n\n| Model | Accuracy | ROC-AUC | Precision | Recall | F1-Score | MCC |\n| --- | --- | --- | --- | --- | --- | --- |\n| LogisticRegression | 0.9492 | 0.9497 | 0.9630 | 0.9455 | 0.9541 | 0.8975 |\n| GaussianNB | 0.9289 | 0.9289 | 0.9706 | 0.9000 | 0.9340 | 0.8601 |\n| RandomForest | 0.9340 | 0.9349 | 0.9533 | 0.9273 | 0.9401 | 0.8671 |\n| KNearestNeighbours | 0.9391 | 0.9406 | 0.9623 | 0.9273 | 0.9444 | 0.8778 |","51da3ef6":"This project aims to identify fraudulent credit card transactions using machine learning. The data is obtained from Kaggle (<a href='https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud' target='_blank'>link<\/a>). This project will compare the performance of four different classifiers in how well they detect fraud and look at how to handle imbalanced data.\n\nReferences:\n - <a href=\"https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets\" target=\"_blank\">Credit Fraud || Dealing with Imbalanced Datasets<\/a> by Janio Martinez\n - <a href=\"https:\/\/medium.com\/analytics-vidhya\/credit-card-fraud-detection-in-python-using-scikit-learn-f9046a030f50\" target=\"_blank\">Credit Card Fraud Detection in Python using Scikit Learn<\/a>\n \nTo begin, the important python modules are imported into the notebook.","20e82c25":"Below, the confusion matrix of each model is shown. This provides a clearer overview of the performance of each model. The confusion matirx is made up as:\n- True Positives (Top-Left Square): Observation is positive and correctly predicted as positive.\n- False Negatives (Top-Right Square): Observation is positive but incorrectly predicted as negative.\n- False Positives (Bottom-Left Square): Observation is negative but incorrectly predicted as positive.\n- True Negatives (Bottom-Right Square): Observation is negative and correctly predicted as negative.\n\nFrom below, the Logistic Regression performs the best as it only misclassifies 9 transactions however improvements to the model need to be made to reduce the number of false positives. The Gaussian Naive-Bayes model performs the worst out of the four which is in line which the scores obtained in the table above.","04ad34d5":"To deal with the imbalanced data and skew this creates, a sub-sample of the data is created. This subsample will be a dataframe that contains equal ratio of fraud to non-fraud cases. To create this sub=sample, we need to randomly select 492 non-fraud cases to match the 492 fraud cases we do have. To confirm that the number of cases are equal we check the value counts of the new dataframe."}}