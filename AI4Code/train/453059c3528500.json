{"cell_type":{"e350b97f":"code","3520916a":"code","01df5183":"code","233dee39":"code","0416e484":"code","ab079a4e":"code","e1d031f0":"code","7d59167b":"code","a77af528":"code","639d35b8":"code","38233323":"code","7c6183ef":"code","d16b5884":"code","fff50369":"code","f3ac6801":"code","3ee9e2bf":"code","9add8155":"code","5d5b214f":"code","0eaa2df5":"markdown","69d2e33a":"markdown","ab47fa15":"markdown","49077d9c":"markdown","3fc79985":"markdown","f2bad51e":"markdown","39370c52":"markdown","b1544c3a":"markdown","4c71ef54":"markdown","6f7f9fbc":"markdown","945c8c4d":"markdown","2f66fd14":"markdown","c89936de":"markdown","96c090fc":"markdown"},"source":{"e350b97f":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve\nfrom sklearn.metrics import  recall_score, classification_report\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\n\n# Plotting the graphs\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3520916a":"#Importing data\nlowafilepath = '..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv'\ndata = pd.read_csv(lowafilepath)","01df5183":"data.head()","233dee39":"print (\"Rows     : \" ,data.shape[0])\nprint (\"Columns  : \" ,data.shape[1])\nprint (\"\\nFeatures : \\n\" ,data.columns.tolist())\nprint(\"\\nData Information : \\n\",data.info())","0416e484":"# Removing the missing values\ndata = data[pd.notnull(data['TotalCharges'])]\nprint(\"Number of null values in total charges:\",data['TotalCharges'].isna().sum())\ndata['TotalCharges'] = pd.to_numeric(data['TotalCharges'],errors='coerce')","ab079a4e":"sns.pairplot(data,vars = ['tenure','MonthlyCharges','TotalCharges'], hue=\"Churn\")","e1d031f0":"# Getting the categorical variables \nall_cat_var = data.nunique()[data.nunique()<5].keys().tolist()\n\n# Getting the categorical variables without churn\ncat_var = all_cat_var[:-1]","7d59167b":"def pie_plot(Column):    \n    ct1 = pd.crosstab(data[Column],data['Churn'])\n    trace1 = go.Pie(labels = ct1.index,\n                    values = ct1.iloc[:,0],\n                    hole=0.3,\n                    domain=dict(x=[0,.45]))\n    trace2 = go.Pie(labels = ct1.index,\n                    values = ct1.iloc[:,1],\n                    domain=dict(x=[.55,1]),\n                    hole=0.3)\n\n    layout = go.Layout(dict(title = Column + \" distribution in customer attrition \",\n                                plot_bgcolor  = \"rgb(243,243,243)\",\n                                paper_bgcolor = \"rgb(243,243,243)\",\n                                annotations = [dict(text = \"churn customers\",\n                                                    font = dict(size = 13),\n                                                    showarrow = False,\n                                                    x = .15, y = 1),\n                                               dict(text = \"Non churn customers\",\n                                                    font = dict(size = 13),\n                                                    showarrow = False,\n                                                    x = .88,y = 1)\n\n                                              ]\n                               )\n                          )\n\n    fig = go.Figure(data=[trace1,trace2],layout=layout)\n    py.iplot(fig)","a77af528":"for i in cat_var:\n    pie_plot(i)","639d35b8":"# Removing the customer id\ndel data['customerID'] #customerID is a uninque id so it dosn't give any information","38233323":"# chi square test \n\ni = 0\nfor nam in cat_var:\n    crosstab = pd.crosstab(data[nam], data['Churn'])\n#     crosstab\n    chi = stats.chi2_contingency(crosstab)\n    if chi[1]<0.05:\n        i=i+1\n        print(i,nam, \" is important for predicting churn with p value: \",chi[1] )","7c6183ef":"le = LabelEncoder()\n\n# apply \"le.fit_transform\"\ndata_new = data.apply(le.fit_transform)","d16b5884":"X = add_constant(data_new)\npd.Series([variance_inflation_factor(X.values, i) \n           for i in range(X.shape[1])], index=X.columns)","fff50369":"# Splitiing to x and y\n\nX = (data_new.loc[:, data_new.columns != 'Churn'])\ny = (data_new.loc[:, data_new.columns == 'Churn'])\nprint('Shape of X: {}'.format(X.shape))\nprint('Shape of y: {}'.format(y.shape))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\ncolumns = X_train.columns","f3ac6801":"# Logistic regression Model\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n#  Model metrics\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","3ee9e2bf":"# Removing unimportant features ['gender','PhoneService','TotalCharges','tenure']\nX_train.drop(['gender','PhoneService','TotalCharges','tenure'], axis=1, inplace=True)\nX_test.drop(['gender','PhoneService','TotalCharges','tenure'], axis=1, inplace=True)","9add8155":"# New model\nlogreg_new = LogisticRegression()\nlogreg_new.fit(X_train, y_train)\n\n#  Model metrics\ny_pred = logreg_new.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg_new.score(X_test, y_test)))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","5d5b214f":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,max_depth = 3)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Report : \",  classification_report(y_test, y_pred)) ","0eaa2df5":"> **Data**","69d2e33a":"> **Feature selection**","ab47fa15":"> ** Base Model-Logistic Regression**","49077d9c":"> **EDA**","3fc79985":"Total charges and tenure is having high VIF score which means multicoliniarity","f2bad51e":"Column TotalCharges is having some missing values","39370c52":"Columns such as Gender and PhoneService are not important in predicting the churn","b1544c3a":"**Chi-square Test for Feature Extraction:**\nChi-square test is used for categorical features in a dataset. We calculate Chi-square between each feature and the target and select the desired number of features with best Chi-square scores.It determines if the association between two categorical variables of the sample would reflect their real association in the population.","4c71ef54":"> **Label Encoding**","6f7f9fbc":"People having lower tenure and higher monthly charges are tend to churn more.","945c8c4d":"Finding out the Multicollinearity using VIF","2f66fd14":"> **Statistical Test**","c89936de":"> **Missing Value Removal**","96c090fc":"Label encodingrefers to converting the labels into numeric form so as to convert it into the machine-readable form. Machine learning algorithms can then decide in a better way on how those labels must be operated. It is an important pre-processing step for the structured dataset in supervised learning."}}