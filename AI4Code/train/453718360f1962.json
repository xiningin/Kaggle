{"cell_type":{"7cee61d9":"code","98578cf8":"code","5a31ab52":"code","b5cf0250":"code","db784ad9":"code","34741d33":"code","3e31150e":"code","42ff3d20":"code","be0d7031":"code","257b8972":"code","e765e245":"code","26829b76":"code","508f1b76":"code","ce6b87ae":"markdown","687aeb2d":"markdown","e7bab5c8":"markdown","b83ea6e1":"markdown","e31c157f":"markdown","59a94add":"markdown","b227d118":"markdown","b4d3fb1e":"markdown","82585115":"markdown","813e765f":"markdown","9cdafab7":"markdown","93233219":"markdown","41d9cb93":"markdown","3765a2db":"markdown","eca38572":"markdown","6766eb9e":"markdown","198709a8":"markdown"},"source":{"7cee61d9":"!git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n%cd yolov5\n%pip install -qr requirements.txt  # install dependencies\n\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\n\nclear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","98578cf8":"!python detect.py --weights \/kaggle\/input\/obj-detect-weights\/bests0-2.pt --img 640 --conf 0.49 --source \/kaggle\/input\/obj-detect-weights\/messageImage_1616037486900.jpg\nImage(filename='runs\/detect\/exp\/messageImage_1616037486900.jpg', width=600)","5a31ab52":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"API_key\")\n\n# Weights & Biases (optional)\n%pip install -q wandb\n# use 'wandb disabled' or 'wandb enabled' to disable or enable\n!wandb login $api_key","b5cf0250":"# Thanks to https:\/\/www.kaggle.com\/awsaf49\/vinbigdata-cxr-ad-yolov5-14-class-train\/output\n\nfrom os import listdir\nfrom os.path import isfile, join\nimport yaml\nfrom glob import glob\n\ncwd = '\/kaggle\/working\/'\n\nwith open(join( cwd , 'train.txt'), 'w') as f:\n    for path in glob('\/kaggle\/input\/obj-detect\/data\/data\/images\/train\/*'):\n        f.write(path+'\\n')\n            \nwith open(join( cwd , 'val.txt'), 'w') as f:\n    for path in glob('\/kaggle\/input\/obj-detect\/data\/data\/images\/val\/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train = join( cwd , 'train.txt'),\n    val   = join( cwd , 'val.txt' ),\n    nc    = 4,\n    names = ['vehicle', 'scooter', 'pedestrian', 'bicycle']\n    )\n\nwith open(join( cwd , 'traffic4_in_kaggle.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(join( cwd , 'traffic4_in_kaggle.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","db784ad9":"# # # !WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --nosave --cache \n# # !python train.py --img 640 --batch 16 --epochs 3 --data \/kaggle\/working\/traffic4_in_kaggle.yaml --weights \/kaggle\/input\/obj-detect-weights\/last3-5_640.pt\n# # # !python train.py --img 1024 --batch 6 --epochs 30 --data \/kaggle\/working\/traffic4_in_kaggle.yaml --weights yolov5x.pt\n\n# !python train.py --img 640 --batch 64 --epochs 4 --data \/kaggle\/working\/traffic4_in_kaggle.yaml --weights \/kaggle\/input\/yolov5-obd\/yolov5\/runs\/train\/exp\/weights\/last.pt\n\n!python train.py --img 640 --batch 64 --epochs 3 --data \/kaggle\/working\/traffic4_in_kaggle.yaml --weights yolov5s.pt\n\nclear_output()","34741d33":"Image(filename='runs\/train\/exp\/train_batch0.jpg', width=800)  # train batch 0 mosaics and labels\nImage(filename='runs\/train\/exp\/test_batch0_labels.jpg', width=800)  # test batch 0 labels\nImage(filename='runs\/train\/exp\/test_batch0_pred.jpg', width=800)  # test batch 0 predictions","3e31150e":"from utils.plots import plot_results \nplot_results(save_dir='runs\/train\/exp')  # plot all results*.txt as results.png\nImage(filename='runs\/train\/exp\/results.png', width=800)","42ff3d20":"# # Re-clone repo\n# %cd ..\n# %rm -rf yolov5 && git clone https:\/\/github.com\/ultralytics\/yolov5\n# %cd yolov5","be0d7031":"# # Reproduce\n# for x in 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x':\n#   !python test.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.25 --iou 0.45  # speed\n#   !python test.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65  # mAP","257b8972":"# # Unit tests\n# %%shell\n# export PYTHONPATH=\"$PWD\"  # to run *.py. files in subdirectories\n\n# rm -rf runs  # remove runs\/\n# for m in yolov5s; do  # models\n#   python train.py --weights $m.pt --epochs 3 --img 320 --device 0  # train pretrained\n#   python train.py --weights '' --cfg $m.yaml --epochs 3 --img 320 --device 0  # train scratch\n#   for d in 0 cpu; do  # devices\n#     python detect.py --weights $m.pt --device $d  # detect official\n#     python detect.py --weights runs\/train\/exp\/weights\/best.pt --device $d  # detect custom\n#     python test.py --weights $m.pt --device $d # test official\n#     python test.py --weights runs\/train\/exp\/weights\/best.pt --device $d # test custom\n#   done\n#   python hubconf.py  # hub\n#   python models\/yolo.py --cfg $m.yaml  # inspect\n#   python models\/export.py --weights $m.pt --img 640 --batch 1  # export\n# done","e765e245":"# # Profile\n# from utils.torch_utils import profile \n\n# m1 = lambda x: x * torch.sigmoid(x)\n# m2 = torch.nn.SiLU()\n# profile(x=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)","26829b76":"# # Evolve\n# !python train.py --img 640 --batch 64 --epochs 100 --data coco128.yaml --weights yolov5s.pt --cache --noautoanchor --evolve\n# !d=runs\/train\/evolve && cp evolve.* $d && zip -r evolve.zip $d && gsutil mv evolve.zip gs:\/\/bucket  # upload results (optional)","508f1b76":"# # VOC\n# for b, m in zip([64, 48, 32, 16], ['yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']):  # zip(batch_size, model)\n#   !python train.py --batch {b} --weights {m}.pt --data voc.yaml --epochs 50 --cache --img 512 --nosave --hyp hyp.finetune.yaml --project VOC --name {m}","ce6b87ae":"# Train","687aeb2d":"## Dir Stuff","e7bab5c8":"[In 'runs' folder, you can find weights file (\\*.pt) under the folder named your project](https:\/\/stackoverflow.com\/questions\/62677311\/is-there-any-way-to-backup-weights-in-yolov5-when-training-after-a-fixed-number) ![](https:\/\/i.stack.imgur.com\/Snvfs.png)","b83ea6e1":"# Inference\n\n`detect.py` runs inference on a variety of sources.","e31c157f":"# Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https:\/\/developer.nvidia.com\/cuda)\/[CUDNN](https:\/\/developer.nvidia.com\/cudnn), [Python](https:\/\/www.python.org\/) and [PyTorch](https:\/\/pytorch.org\/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https:\/\/colab.research.google.com\/github\/ultralytics\/yolov5\/blob\/master\/tutorial.ipynb\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"><\/a> <a href=\"https:\/\/www.kaggle.com\/ultralytics\/yolov5\"><img src=\"https:\/\/kaggle.com\/static\/images\/open-in-kaggle.svg\" alt=\"Open In Kaggle\"><\/a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https:\/\/github.com\/ultralytics\/yolov5\/wiki\/GCP-Quickstart)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https:\/\/github.com\/ultralytics\/yolov5\/wiki\/AWS-Quickstart)\n- **Docker Image**. See [Docker Quickstart Guide](https:\/\/github.com\/ultralytics\/yolov5\/wiki\/Docker-Quickstart) <a href=\"https:\/\/hub.docker.com\/r\/ultralytics\/yolov5\"><img src=\"https:\/\/img.shields.io\/docker\/pulls\/ultralytics\/yolov5?logo=docker\" alt=\"Docker Pulls\"><\/a>\n","59a94add":"## Local Logging\n\nAll results are logged by default to `runs\/train`, with a new experiment directory created for each new training as `runs\/train\/exp2`, `runs\/train\/exp3`, etc. View train and test jpgs to see mosaics, labels, predictions and augmentation effects. Note a **Mosaic Dataloader** is used for training (shown below), a new concept developed by Ultralytics and first featured in [YOLOv4](https:\/\/arxiv.org\/abs\/2004.10934).","b227d118":"Results are saved to `runs\/detect`. A full list of available inference sources:\n<img src=\"https:\/\/user-images.githubusercontent.com\/26833433\/98274798-2b7a7a80-1f94-11eb-91a4-70c73593e26b.jpg\" width=\"900\"> ","b4d3fb1e":"[Caching images problem #1862](https:\/\/github.com\/ultralytics\/yolov5\/issues\/1862) @Bilgee don't --cache if your system doesn't have the RAM to support it.\n\nSee also here: [[Training] YOLOv5 with coco pretrained weights](https:\/\/www.kaggle.com\/rifat963\/training-yolov5-with-coco-pretrained-weights) <br> cache_images = False # Default False, cache images for faster training","82585115":"# Visualize","813e765f":"Training losses and performance metrics are also logged to [Tensorboard](https:\/\/www.tensorflow.org\/tensorboard) and a custom `results.txt` logfile which is plotted as `results.png` (below) after training completes. Here we show YOLOv5s trained on COCO128 to 300 epochs, starting from scratch (blue), and from pretrained `--weights yolov5s.pt` (orange).","9cdafab7":"## Login to W&B\n\nthe way described here: [CNN-Track your Experiments (Weights & Biases)](https:\/\/www.kaggle.com\/imeintanis\/cnn-track-your-experiments-weights-biases)","93233219":"# Setup\n\nClone repo, install dependencies and check PyTorch and GPU.","41d9cb93":"# References\n\n* [YOLOv5 Ultralytics](https:\/\/www.kaggle.com\/ultralytics\/yolov5-ultralytics)\n* [VinBigData-CXR-AD YOLOv5 14 Class [train]](https:\/\/www.kaggle.com\/awsaf49\/vinbigdata-cxr-ad-yolov5-14-class-train)","3765a2db":"# Status\n\n![CI CPU testing](https:\/\/github.com\/ultralytics\/yolov5\/workflows\/CI%20CPU%20testing\/badge.svg)\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https:\/\/github.com\/ultralytics\/yolov5\/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https:\/\/github.com\/ultralytics\/yolov5\/blob\/master\/train.py)), testing ([test.py](https:\/\/github.com\/ultralytics\/yolov5\/blob\/master\/test.py)), inference ([detect.py](https:\/\/github.com\/ultralytics\/yolov5\/blob\/master\/detect.py)) and export ([export.py](https:\/\/github.com\/ultralytics\/yolov5\/blob\/master\/models\/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\n","eca38572":"# Appendix\n\nOptional extras below. Unit tests validate repo functionality and should be run on any PRs submitted.\n","6766eb9e":"Train a YOLOv5s model on our customized data \\[obj-detect\\], starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`. Models are downloaded automatically from the [latest YOLOv5 release](https:\/\/github.com\/ultralytics\/yolov5\/releases).\n\nAll training results are saved to `runs\/train\/` with incrementing run directories, i.e. `runs\/train\/exp2`, `runs\/train\/exp3` etc.\n","198709a8":"## Weights & Biases Logging \ud83c\udf1f NEW\n\n[Weights & Biases](https:\/\/www.wandb.com\/) (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n\nDuring training you will see live updates at [https:\/\/wandb.ai\/home](https:\/\/wandb.ai\/home), and you can create and share detailed [Reports](https:\/\/wandb.ai\/glenn-jocher\/yolov5_tutorial\/reports\/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https:\/\/github.com\/ultralytics\/yolov5\/issues\/1289). \n\n<img src=\"https:\/\/user-images.githubusercontent.com\/26833433\/98184457-bd3da580-1f0a-11eb-8461-95d908a71893.jpg\" width=\"800\">"}}