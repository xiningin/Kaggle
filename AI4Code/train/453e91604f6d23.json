{"cell_type":{"9c3d58c3":"code","a33fa3c4":"code","4c1bebf6":"code","17313cb6":"code","8b0a225b":"code","8d71a569":"code","96f3cf1d":"code","9729cebc":"code","7835839d":"code","0bd9433a":"code","b50f35bb":"code","d4253ee3":"code","e7c79f40":"code","1569561f":"code","9ffec148":"code","4e75aa43":"code","bcc369ec":"code","2650338e":"code","62ec4f23":"code","0bb9dd12":"code","978ec876":"markdown","07ff5f17":"markdown","da57e350":"markdown","cbdeefcf":"markdown","4d7284ab":"markdown","f4e00bac":"markdown","7f99d9b3":"markdown"},"source":{"9c3d58c3":"import gc\nimport os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\nfrom numpy import sqrt, sin, cos, pi, zeros\nfrom numpy.random import randn, rand, uniform, normal\nfrom scipy.linalg import hadamard\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input, Dense, Activation, LSTM, Dropout, RepeatVector, TimeDistributed, Embedding, Reshape, Dot, Concatenate\nfrom tensorflow.keras.layers import GRU, SpatialDropout1D, Conv1D, GlobalMaxPooling1D,Multiply, Lambda, Softmax, Flatten, BatchNormalization, Bidirectional, dot, concatenate\nfrom tensorflow.keras.layers import AdditiveAttention, Attention\nfrom tensorflow.keras.activations import relu\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import backend\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.metrics import MeanSquaredError\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","a33fa3c4":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","4c1bebf6":"MIXED_PRECISION = False\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","17313cb6":"lotto = pd.read_csv('..\/input\/uk-lotto-draw-history-20162020\/lotto_history.csv', index_col = 'Date')\nprint(lotto)","8b0a225b":"data = lotto.values - 1\ntrain = data[:-50]\ntest = data[-50:]\n\nw = 10\nX_train = []\ny_train = []\nfor i in range(w, len(train)):\n    X_train.append(train[i - w: i, :])\n    y_train.append(train[i])\nX_train, y_train = np.array(X_train), np.array(y_train)\n\ninputs = data[data.shape[0] - test.shape[0] - w:]\nX_test = []\nfor i in range(w, inputs.shape[0]):\n    X_test.append(inputs[i - w: i, :])\nX_test = np.array(X_test)\ny_test = test","8d71a569":"print(data.shape)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","96f3cf1d":"embed_dim = (59 \/\/ 2) + 1\ndropout_rate = 0.5\nspatial_dropout_rate = 0.5\nsteps_before = w\nsteps_after = 7\nfeature_count = embed_dim * 7\nhidden_neurons = [64, 32] \nbidirectional = True \nattention_style = 'Bahdanau'","9729cebc":"with strategy.scope():\n    \n    inp0 = Input(shape = (w, X_train.shape[2]))\n    \n    # Embed 59 categories into a 30-dimension continuous-number vector for each ball\n    inp1 = Lambda(lambda x: x[:, :, 0])(inp0)\n    inp1 = Embedding(59, embed_dim)(inp1)\n    inp1 = SpatialDropout1D(spatial_dropout_rate)(inp1)\n    \n    inp2 = Lambda(lambda x: x[:, :, 1])(inp0)\n    inp2 = Embedding(59, embed_dim)(inp2)\n    inp2 = SpatialDropout1D(spatial_dropout_rate)(inp2)\n    \n    inp3 = Lambda(lambda x: x[:, :, 2])(inp0)\n    inp3 = Embedding(59, embed_dim)(inp3)\n    inp3 = SpatialDropout1D(spatial_dropout_rate)(inp3)\n    \n    inp4 = Lambda(lambda x: x[:, :, 3])(inp0)\n    inp4 = Embedding(59, embed_dim)(inp4)\n    inp4 = SpatialDropout1D(spatial_dropout_rate)(inp4)\n    \n    inp5 = Lambda(lambda x: x[:, :, 4])(inp0)\n    inp5 = Embedding(59, embed_dim)(inp5)\n    inp5 = SpatialDropout1D(spatial_dropout_rate)(inp5)    \n    \n    inp6 = Lambda(lambda x: x[:, :, 5])(inp0)\n    inp6 = Embedding(59, embed_dim)(inp6)\n    inp6 = SpatialDropout1D(spatial_dropout_rate)(inp6)\n    \n    inp7 = Lambda(lambda x: x[:, :, 6])(inp0)\n    inp7 = Embedding(59, embed_dim)(inp7)\n    inp7 = SpatialDropout1D(spatial_dropout_rate)(inp7)\n    \n    inp = Concatenate()([inp1, inp2, inp3, inp4, inp5, inp6, inp7])\n    \n    # Seq2Seq model with attention or bidirectional encoder\n    \n    num_layers = len(hidden_neurons)\n    \n    sh_list, h_list, c_list = [inp], [], []\n    \n    if bidirectional:\n        \n        for i in range(num_layers):\n    \n            sh, fh, fc, bh, bc = Bidirectional(LSTM(hidden_neurons[i],\n                                                    dropout = dropout_rate, \n                                                    return_state = True, \n                                                    return_sequences = True))(sh_list[-1])\n        \n            h = Concatenate()([fh, bh])\n            c = Concatenate()([fc, bc]) \n\n            sh_list.append(sh)\n            h_list.append(h)\n            c_list.append(c)\n        \n    else:\n    \n        for i in range(num_layers):\n\n            sh, h, c = LSTM(hidden_neurons[i], \n                            dropout = dropout_rate,\n                            return_state = True, \n                            return_sequences = True)(sh_list[-1])\n\n            sh_list.append(sh)\n            h_list.append(h)\n            c_list.append(c)\n    \n    decoder = RepeatVector(steps_after)(h_list[-1])\n    \n    if bidirectional:\n        \n        decoder_hidden_neurons = [hn * 2 for hn in hidden_neurons]\n        \n    else:\n        \n        decoder_hidden_neurons = hidden_neurons\n    \n    for i in range(num_layers):\n        \n        decoder = LSTM(decoder_hidden_neurons[i],\n                       dropout = dropout_rate, \n                       return_sequences = True)(decoder, initial_state = [h_list[i], c_list[i]])\n       \n    if attention_style == 'Bahdanau':\n        \n        context = AdditiveAttention(dropout = dropout_rate)([decoder, sh_list[-1]])\n        \n        decoder = concatenate([context, decoder])\n        \n    elif attention_style == 'Luong':\n        \n        context = Attention(dropout = dropout_rate)([decoder, sh_list[-1]])\n        \n        decoder = concatenate([context, decoder])\n    \n    out = Dense(59, activation = 'softmax')(decoder)\n\n    model = Model(inputs = inp0, outputs = out)\n    \n    sparse_top_k = tf.keras.metrics.SparseTopKCategoricalAccuracy(k = 5, name = 'sparse_top_k')\n\n    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = [sparse_top_k])","7835839d":"model.summary()","0bd9433a":"plot_model(model, show_shapes = True, show_layer_names = True, rankdir = 'TB', dpi = 60)","b50f35bb":"class CosineAnnealingScheduler(callbacks.Callback):\n    \"\"\"Cosine annealing scheduler.\n    \"\"\"\n\n    def __init__(self, T_max, eta_max, eta_min = 0, verbose = 0):\n        super(CosineAnnealingScheduler, self).__init__()\n        self.T_max = T_max\n        self.eta_max = eta_max\n        self.eta_min = eta_min\n        self.verbose = verbose\n\n    def on_epoch_begin(self, epoch, logs = None):\n        if not hasattr(self.model.optimizer, 'lr'):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch \/ self.T_max)) \/ 2\n        backend.set_value(self.model.optimizer.lr, lr)\n        if self.verbose > 0:\n            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n                  'rate to %s.' % (epoch + 1, lr))\n\n    def on_epoch_end(self, epoch, logs = None):\n        logs = logs or {}\n        logs['lr'] = backend.get_value(self.model.optimizer.lr)","d4253ee3":"EPOCHS = 200\nBATCH_SIZE = 32\nLR_MAX = 1e-4\nLR_MIN = 1e-5\n\ncas = CosineAnnealingScheduler(EPOCHS, LR_MAX, LR_MIN)\n\nckp = callbacks.ModelCheckpoint('best_model.hdf5', monitor = 'val_sparse_top_k', verbose = 0, \n                                save_best_only = True, save_weights_only = False, mode = 'max')\n\nhistory = model.fit(X_train, y_train, \n                    validation_data = (X_test, y_test), \n                    callbacks = [ckp, cas], \n                    epochs = EPOCHS, \n                    batch_size = BATCH_SIZE, \n                    verbose = 2)\n\nhist = pd.DataFrame(history.history)","e7c79f40":"print(hist['val_sparse_top_k'].max())\n\nplt.figure(figsize = (8, 6))\nplt.semilogy(hist['sparse_top_k'], '-r', label = 'Training')\nplt.semilogy(hist['val_sparse_top_k'], '-b', label = 'Validation')\nplt.ylabel('Sparse Top K Accuracy', fontsize = 14)\nplt.xlabel('Epochs', fontsize = 14)\nplt.legend(fontsize = 14)\nplt.grid()\nplt.show()","1569561f":"model.load_weights('best_model.hdf5')\npred = model.predict(X_test)\npred = np.argmax(pred, axis = 2)","9ffec148":"for i in range(y_test.shape[0]):\n    print('Prediction:\\t', pred[i] + 1)\n    print('GoundTruth:\\t', y_test[i] + 1)\n    print('-' * 40)","4e75aa43":"X_latest = X_test[-1][1:]\nX_latest = np.concatenate([X_latest, y_test[-1].reshape(1, 7)], axis = 0)\nX_latest = X_latest.reshape(1, X_latest.shape[0], X_latest.shape[1])\nprint(X_latest + 1)","bcc369ec":"# beam search\ndef beam_search_decoder(data, k, replace = True):\n    sequences = [[list(), 0.0]]\n    # walk over each step in sequence\n    for row in data:\n        all_candidates = list()\n        # expand each current candidate\n        for i in range(len(sequences)):\n            seq, score = sequences[i]\n            best_k = np.argsort(row)[-k:]\n            for j in best_k:\n                candidate = [seq + [j], score + math.log(row[j])]\n                if replace:\n                    all_candidates.append(candidate)\n                elif (replace == False) and (len(set(candidate[0])) == len(candidate[0])):\n                    all_candidates.append(candidate)\n        # order all candidates by score\n        ordered = sorted(all_candidates, key = lambda tup:tup[1], reverse = True)\n        # select k best\n        sequences = ordered[:k]\n    return sequences","2650338e":"pred_latest = model.predict(X_latest)\npred_latest = np.squeeze(pred_latest)\npred_latest_greedy = np.argmax(pred_latest, axis = 1)\nprint(pred_latest_greedy + 1)","62ec4f23":"beam_width = 10\nreplace = True\n\nresult = beam_search_decoder(pred_latest, beam_width, replace)\nprint('Beam Width:\\t', beam_width)\nprint('Replace:\\t', replace)\nprint('-' * 85)\nfor seq in result:\n    print('Prediction: ', np.array(seq[0]) + 1, '\\tLog Likelihood: ', seq[1])","0bb9dd12":"beam_width = 10\nreplace = False\n\nresult = beam_search_decoder(pred_latest, beam_width, replace)\nprint('Beam Width:\\t', beam_width)\nprint('Replace:\\t', replace)\nprint('-' * 85)\nfor seq in result:\n    print('Prediction: ', np.array(seq[0]) + 1, '\\tLog Likelihood: ', seq[1])","978ec876":"# Read Data","07ff5f17":"# Predict the Future Draw on 2020\/Aug\/26\nThe Beam Search method is use to output 10 possible draws.\nIt is worth noting that Replace = False means no duplicate balls.","da57e350":"# Predict the Test Dataset","cbdeefcf":"# Configurations","4d7284ab":"Input the last 10 draws and sequentially predict the next draw.\nMonitor the performance by sparse categorical crossentropy and sparse top k categorical accuracy.","f4e00bac":"# Train Test Split\nUse the last 50 draws as the test dataset\nUse a sliding window of 10 to split the input data","7f99d9b3":"# Seq2Seq Model to Predict Future Draws"}}