{"cell_type":{"7ac8c310":"code","7bb18085":"code","8d136f7e":"code","ec487336":"code","3eb508f0":"code","e4240295":"code","da9849aa":"code","f0f390b9":"code","98cd0a54":"code","61c15ff7":"code","07797d79":"code","cef23443":"code","fd75e758":"code","0ec93dc8":"code","81e142a2":"code","652f9a13":"code","64065756":"code","5f9402c4":"code","97dd38f2":"code","efac58cc":"code","415747bd":"code","7b784480":"code","da3aa8bd":"code","eeb8ac51":"code","3ee75925":"code","67e36e8f":"code","73f917f6":"markdown","214615bf":"markdown","0e58d151":"markdown","d72a1400":"markdown","e73268e2":"markdown","47f226ae":"markdown","53fdd2d1":"markdown","ce14c120":"markdown","9374190e":"markdown","13024c8c":"markdown","c1a7ca14":"markdown","57ebea91":"markdown","93de9b5a":"markdown","9c6e65ca":"markdown","1acf5185":"markdown"},"source":{"7ac8c310":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    \n\n# Any results you write to the current directory are saved as output.","7bb18085":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","8d136f7e":"print(train_data.shape)","ec487336":"train_data.info()","3eb508f0":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","e4240295":"sns.lineplot(x=train_data['Survived'], y=train_data.index)","da9849aa":"test_data.shape","f0f390b9":"test_data.info()","98cd0a54":"# Check for missing values\nmissing_data=train_data.isnull().sum()\nmissing_data","61c15ff7":"sns.distplot(a=missing_data['Age'], label='Age',kde=False)\nsns.distplot(a=missing_data['Cabin'], label= 'Cabin',kde=False)\nplt.legend()","07797d79":"test_data.isnull().sum()","cef23443":"# Name, Sex, Cabin & Embarked are categorical. Remove Name & Ticket as they are irrelavant. Remove cabin as too many null values.\ntrain_data.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)\ntest_data.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)","fd75e758":"train_data.isnull().sum()\ntrain_data.head()","0ec93dc8":"test_data.isnull().sum()","81e142a2":"train_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())\n\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())","652f9a13":"# For embarked, there are 2 missing values, drop them.\ntrain_data.dropna(subset = [\"Embarked\"], inplace=True)","64065756":"# dummy encoding of 2 remaining categorical variables.\ntrain_data = pd.get_dummies(train_data, columns=[\"Sex\"], drop_first=True)\ntrain_data = pd.get_dummies(train_data, columns=[\"Embarked\"],drop_first=True)","5f9402c4":"test_data = pd.get_dummies(test_data, columns=[\"Sex\"], drop_first=True)\ntest_data = pd.get_dummies(test_data, columns=[\"Embarked\"],drop_first=True)","97dd38f2":"train_data.head()","efac58cc":"test_data.head()","415747bd":"y = train_data[\"Survived\"]\n\nX = train_data.drop(['Survived'], axis=1)","7b784480":"from sklearn.preprocessing import StandardScaler\n\nsc=StandardScaler()\nsc.fit(train_data.drop(['Survived', 'PassengerId'], axis = 1))\nX_train = sc.transform(train_data.drop(['Survived', 'PassengerId'], axis = 1))\nX_train","da3aa8bd":"from sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.23, random_state = 5)\n\nmodel_LR = LogisticRegression(max_iter=5000)\nmodel_LR.fit(X_train, y_train)\nLR_predict = model_LR.predict(X_test)\nLR_score = model_LR.score(X_test,y_test)\n\nmodel_X = XGBClassifier(eta=0.1, n_estimators=50,\n                        max_depth=5, subsample=0.6, colsample_bytree=0.7,objective= 'binary:logistic',\n                        scale_pos_weight=1, seed=27)\nmodel_X.fit(X_train, y_train)\nX_predict = model_X.predict(X_test)\nX_score = model_X.score(X_test,y_test)\n\nrfc = RandomForestClassifier(n_estimators=6)\nrfc.fit(X_train, y_train)\nRFC_predict = rfc.predict(X_test)\nRFC_score = rfc.score(X_test,y_test)\n\nmodel_DTC = DecisionTreeClassifier(max_depth=7, min_samples_leaf=6, min_samples_split=2)\nmodel_DTC.fit(X_train, y_train)\nDTC_predict = model_DTC.predict(X_test)\nDTC_score = model_DTC.score(X_test,y_test)\n\nmodel_GB = GradientBoostingClassifier(random_state=10, n_estimators=1500,min_samples_split=100, max_depth=6)\nmodel_GB.fit(X, y)\nGB_predict = model_GB.predict(X_test)\nGB_score = model_GB.score(X_test,y_test)\n\nx1=LR_score, X_score, RFC_score, DTC_score, GB_score\nprint(x1)\n\nsns.distplot(a=x1, kde=True)\nplt.legend()\nplt.title('Accuracy estimates of Logistic regression, XGB, decision trees, Random Forest, and Gradient Boosting')","eeb8ac51":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\n\ngnb= GaussianNB()\ngnb.fit(X_train, y_train)\nprediction = gnb.predict(X_test)\ncross_scores = cross_val_score(gnb,X_train,y_train,cv=8)\nprint(cross_scores)\n\nsns.distplot(a=cross_scores, kde=True)\nplt.legend()","3ee75925":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nneigh= KNeighborsClassifier(n_neighbors=5, leaf_size=30)\nneigh.fit(X_train, y_train)\nKN_predict = neigh.predict(X_test)\ncross_scores = cross_val_score(neigh,X_train,y_train,cv=8)\nprint(cross_scores)\nprint(accuracy_score(KN_predict, y_test))","67e36e8f":"predictions = model_X.predict(test_data)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","73f917f6":"Train data is viewed using .head() in pandas","214615bf":"# Let's begin\n","0e58d151":"Filling the train and test data with mean taken of all the entries","d72a1400":"**Training & Scoring**","e73268e2":"# References\n1. https:\/\/www.kaggle.com\/kshivi99\/predcting-the-titanic-survivors-minimal-kernal\n2. https:\/\/www.kaggle.com\/mdmahmudferdous\/titanic-survivor-prediction-0-804-top-8","47f226ae":"Loading the libraries is done","53fdd2d1":"For Certain algorithms to work we must normalize the data so I have normalized using StandardScaler method","ce14c120":"To check the length of datasets","9374190e":"# Missing values\nyou would have obsereved that certain values were either null or NaN so now let's see what proportion of them are there in training and test dataset","13024c8c":"KNeighbors in imported from sklearn library","c1a7ca14":"Name, Sex, Cabin & Embarked are categorical. Remove Name & Ticket as they are irrelavant. Remove cabin as too many null values. You can also use pipelines or SimpleImputer","57ebea91":"Let us use another algorithm called Naive bayes and now i have used cross validation scoring parameter","93de9b5a":"Do comment on if any improvement could be done on this\nI would be looking for more possible approaches like neural nets etc.\ninstead of mean i would look for pipelines and simpleimputer in future ","9c6e65ca":"# Submission","1acf5185":"Dummy Encoding"}}