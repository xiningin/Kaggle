{"cell_type":{"dec21359":"code","f5127c5a":"code","5bd5dd09":"code","54a004f2":"code","75e66039":"code","60669df5":"code","a2d4146e":"code","81e0a144":"code","2d9e5159":"code","84cf2f81":"code","f33e1974":"code","c506bcaf":"code","c58329a6":"code","905975fe":"code","fdf37531":"code","cde13f33":"code","5b577627":"code","0071e828":"code","ba3dc511":"code","4d3383fc":"code","827793ca":"code","eeb0be31":"code","34fc497d":"code","c492d1bf":"code","eb133158":"code","bfd6ef6a":"markdown","cfd48901":"markdown","6223e5fa":"markdown","4e80a0bf":"markdown","f6b67702":"markdown","5c7add48":"markdown","024775c5":"markdown","777cbb95":"markdown","3232974f":"markdown","cae98daa":"markdown","f1094634":"markdown","ea1fa4ce":"markdown","9a23023f":"markdown","7c1affe4":"markdown"},"source":{"dec21359":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n! pip install rich\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom rich import print\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f5127c5a":"#imports\nimport numpy as np\nimport pandas as pd\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","5bd5dd09":"#Loading the data\ndtrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\")\ndtest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\")","54a004f2":"#Training dataframe\ndtrain.head()","75e66039":"description = dtrain.describe()\ndescription.applymap(\"{0:.2f}\".format)","60669df5":"#list of features\nfeatures = dtrain.columns[1:-1]\n\ninfo_dtrain = dtrain.dtypes\ninfo_dtest = dtest.dtypes\n\n#exclude the id column\nint_features = list(filter(lambda x: (x[1]=='int64'), zip(dtrain.columns, info_dtrain)))[1:]\nprint(int_features)","a2d4146e":"#Principal Component Analysis of Data\n#Before we proceed we will drop the id and loss columns\n#and split the set for validation with 30% test size.\nx_train, x_test, y_train, y_test = train_test_split(dtrain[features], dtrain.loss, test_size=0.3, random_state=0)\n\n#Standard Scaler\nscaler = StandardScaler()\n\n# Fit on training set\nscaler.fit(x_train)\n\n# Apply transform to both the training set and the test set.\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)\nx_dtest = scaler.transform(dtest.drop([\"id\"], axis=1))","81e0a144":"#PCA\n\ndef reduce_dimension(array, dim=2):\n    \"\"\"\n    Defining the outpout size for pca\n    \"\"\"\n    \n    #Dimension Reduction\n    pca = PCA(n_components=dim)\n    \n    #fit to the train set\n    pca.fit(array)\n    \n    #return the pca object\n    return pca\n\n#Call reduce dimension on fatures for reduction to 2 features\npca = reduce_dimension(x_train)\nx_train_pca = pca.transform(x_train)\n\n#validation set\ny_pca = pca.transform(x_test)\n\n#Actual test set\nxtest_pca = pca.transform(x_dtest)","2d9e5159":"#Build the dataframe from pca arrays\ndtrain_pca = pd.DataFrame(np.column_stack((x_train_pca, y_train)), columns=[\"x_pca\", \"y_pca\", \"loss\"])\ndtest_pca = pd.DataFrame(xtest_pca, columns=[\"test_x\", \"test_y\"])\ndtrain_pca[\"loss\"]=dtrain_pca[\"loss\"].astype(int)\nprint(dtrain_pca.head())\nprint(dtest_pca.head())","84cf2f81":"import warnings\nwarnings.filterwarnings('ignore')\n\nrows_to_plot = 10000\n\n#Visualizing the 2D data obtaibed through PCA\nfig = plt.figure(figsize=(24, 8))\nfig.suptitle(\"Training and test set distributions\")\n\nax = [fig.add_subplot(1, 3, 1), fig.add_subplot(1, 3, 2) ,fig.add_subplot(1, 3, 3, projection=\"3d\")]\n\n#using sequential colomap for loss\nax[2].scatter(dtrain_pca[\"x_pca\"][:rows_to_plot], dtrain_pca[\"y_pca\"][:rows_to_plot], c=dtrain_pca[\"loss\"][:rows_to_plot], cmap=\"inferno\")\nax[2].set_title(\"3D Plot\")\n\ng1 = sns.kdeplot(x=\"x_pca\",y=\"y_pca\",data=dtrain_pca[:rows_to_plot],palette=\"hls\", ax=ax[0])\nax[0].set_title(\"Train Data\")\n\ng2 = sns.kdeplot(x=xtest_pca[:, :1].reshape(1, -1)[0][:rows_to_plot],y=xtest_pca[:, 1:].reshape(1, -1)[0][:rows_to_plot], ax=ax[1])\nax[1].set_title(\"Test Data\");\n\n\nplt.show()","f33e1974":"#Visualizing the 2D data obtaibed through PCA\ng3 = sns.FacetGrid(dtrain_pca,hue=\"loss\", palette=\"hls\",height=8)\ng3.map(sns.scatterplot, \"x_pca\",\"y_pca\").add_legend();","c506bcaf":"g3 = sns.FacetGrid(dtrain_pca, palette=\"hls\",height=8, aspect=2)\ng3.map(sns.kdeplot, \"x_pca\", color=\"g\")\ng3.map(sns.kdeplot, \"y_pca\", color=\"b\")\ng3.map(sns.kdeplot, \"loss\", color=\"r\")\ng3.set(xticks=range(-10, 20, 2))","c58329a6":"#Invovking linear model\nfrom sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression().fit(x_train, y_train)","905975fe":"#Cross Validation\npredicted_test_loss = linear_model.predict(x_test)","fdf37531":"from sklearn.metrics import mean_squared_error as RMSE\n\ndef rmse_plot(y_true, y_pred):\n    rmse = np.sqrt(RMSE(y_true,y_pred))\n    return rmse\n\ndef rmse_plots(y_test, predicted_test_loss):\n    rmse = np.sqrt((1\/len(y_test))*(sum(y_test-predicted_test_loss)))\n    fig = plt.figure()\n    plt.plot(y_test, predicted_test_loss)","cde13f33":"#RMSE ERROR\nprint(f\"The Error for the train set during is observed to be {rmse_plot(y_train, linear_model.predict(x_train))}\")\nprint(f\"The Error for the test set during cross vaidation is observed to be {rmse_plot(y_test, predicted_test_loss)}\")","5b577627":"#Now fit the data on PCA\nlinear_model_pca = LinearRegression().fit(x_train_pca, y_train)\npredicted_test_loss_pca = linear_model_pca.predict(y_pca)\nprint(f\"The Error for the test set during cross vaidation on pca is observed to be {rmse_plot(y_test, predicted_test_loss_pca)}\")","0071e828":"#Invovking linear model Ridge\nfrom sklearn.linear_model import BayesianRidge\nbayesridge = BayesianRidge(verbose=True).fit(x_train, y_train)\npredicted_loss_ridge = bayesridge.predict(x_test)\nprint(f\"The Error for the test set during cross vaidation with Bayesian Ridge is observed to be {rmse_plot(y_test, predicted_loss_ridge)}\")","ba3dc511":"#Predicting on the unseen value we don't know loss here\ny_predicted = bayesridge.predict(x_dtest)\nresult_f0 = pd.DataFrame({\"id\":dtest[\"id\"],\"loss\":y_predicted})\ndtest_pca[\"loss\"] = y_predicted","4d3383fc":"result_f0.to_csv(\".\/submit_bayes.csv\", index=False)","827793ca":"# import xgboost as xgb\n# xgb_model = xgb.XGBRegressor(reg_lambda=0.5,\n#                              max_depth =10,\n#                              minimum_child_weight=2\n#                              objective=\"reg:squarederror\",\n#                              scale_pos_weights=0.5,\n#                              random_state=42)\n\n# xgb_model.fit(x_train, y_train, eval_metric='rmse')\n\n# y_pred = xgb_model.predict(x_train)\n\n# rmse_error_train = rmse_plot(y_train, y_pred)\n\n# print(rmse_error_train)","eeb0be31":"# from scipy.stats import loguniform\n# from sklearn.model_selection import RandomizedSearchCV, RepeatedKFold\n# from sklearn.linear_model import Ridge\n# space = dict()\n# space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']\n# space['alpha'] = loguniform(1e-5, 100)\n# space['fit_intercept'] = [True, False]\n# space['normalize'] = [True, False]\n# model = Ridge()\n# # define evaluation\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# # define search\n# search = RandomizedSearchCV(model, space, n_iter=500, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv, random_state=1)\n# # execute search\n# result = search.fit(x_t, y_train)\n# # summarize result\n# print('Best Score: %s' % result.best_score_)\n# print('Best Hyperparameters: %s' % result.best_params_)","34fc497d":"# y_pred_test = xgb_model.predict(x_test)\n\n# rmse_plot(y_test, y_pred_test)","c492d1bf":"# y_pred_dtest = xgb_model.predict(x_dtest)","eb133158":"# g3 = sns.FacetGrid(pd.DataFrame(np.column_stack((xtest_pca, y_pred_dtest)), columns=[\"test_x\",\"test_y\", \"predicted_loss\"]), palette=\"hls\",height=8, aspect=2)\n# g3.map(sns.kdeplot, \"test_x\", color=\"g\")\n# g3.map(sns.kdeplot, \"test_y\", color=\"b\")\n# g3.map(sns.kdeplot, \"predicted_loss\", color=\"r\")\n# g3.set(xticks=range(-10, 20, 2))","bfd6ef6a":"Bad score larger than previous 7.94038.","cfd48901":"<h2 style=\"color:red;\"><center>Understanding Data<\/center><\/h2>","6223e5fa":"We see that loss predicted is offset by certain offset by certain amount. Hints at case of bias. We will add more !!!","4e80a0bf":"# Submission 1\n\nSubmission 1 ends with bayes ridge with a score of 7.94038 on 33% dataset.\nI'm trying to understand what went not so good.","f6b67702":"<h3 style=\"color:red;\"><center>Reducing Dimension PCA<\/center><\/h3>","5c7add48":"The loss is an integer between 0 and 42.\n\nSix features are purely integer in the above features.","024775c5":"Observe that the features in the dataset are not in a specifi range so we require feature scaling so that our algorithms converge efficiently.","777cbb95":"Both the train data and have similar distributions on pca with 2 components. So we will have less headache while trying to optimize.","3232974f":"<h2 style=\"color:red;\"><center>Objecives<\/center><\/h2>\nTo determine the function that fits the data and predictes loss for new data.","cae98daa":"Observe the distribution we will comback to it.","f1094634":"RMSE is still \"high\" very much. We need to bring it down looking at the principal components we will require some non-linear function.","ea1fa4ce":"This is not only for beautification purpose but also observe that we have maximum loss at the center and it decreases as we move further. So in 3 dimensions we would have a mountain shaped distributions. We probably have a name for it.","9a23023f":"# Linear Regression","7c1affe4":"## Bayesian Ridge"}}