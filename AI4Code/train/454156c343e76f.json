{"cell_type":{"38108d07":"code","6b332d97":"code","e4672cfd":"code","d3d8fde0":"code","5be185d0":"code","7354911c":"code","8ba5399d":"code","066060c7":"code","efc03936":"code","17e86948":"code","bfcf72e3":"code","c58d759e":"markdown"},"source":{"38108d07":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6b332d97":"import tensorflow as tf\nimport pandas as pd       \ntrain0 = pd.read_csv(\"\/kaggle\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv\", header=0, \\\n                    delimiter=\"\\t\", quoting=3)\ntest = pd.read_csv(\"\/kaggle\/input\/word2vec-nlp-tutorial\/testData.tsv\", header=0, delimiter=\"\\t\", \\\n                   quoting=3 )\ntrain1=pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\ntrain1['sentiment'] = train1['sentiment'].map({'positive': 1, 'negative': 0})\ntrain0=train0.drop('id',axis=1)\ntrain= pd.concat([train0, train1]).reset_index(drop=True)\ntrain.shape","e4672cfd":"import nltk\n#nltk.download('all')  # Download text data sets, including stop words\nfrom nltk.corpus import stopwords # Import the stop word list","d3d8fde0":"from bs4 import BeautifulSoup             \nimport re\ndef review_to_words( raw_review ):\n    # Function to convert a raw review to a string of words\n    # The input is a single string (a raw movie review), and \n    # the output is a single string (a preprocessed movie review)\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review).get_text() \n    #\n    # 2. Remove non-letters        \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    #\n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                             \n    #\n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set\n    stops = set(stopwords.words(\"english\"))                  \n    # \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    #\n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))  \n\n\n# Initialize the BeautifulSoup object on a single movie review     \ncleaned_reviews=[]\nfor review in train[\"review\"]:\n    cleaned_reviews.append(review_to_words( review ))\n\nall_text = ' '.join(cleaned_reviews)\nwords = all_text.split()\n\n\nfrom collections import Counter\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n\nreviews_ints = []\nfor each in cleaned_reviews:\n    reviews_ints.append([vocab_to_int[word] for word in each.split()])\n\n\n\nlabels = np.array(train['sentiment'])\n\nnon_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\nreviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\nlabels = np.array([labels[ii] for ii in non_zero_idx])","5be185d0":"seq_len = 1000\nfeatures = np.zeros((len(reviews_ints), seq_len), dtype=int)\nfor i, row in enumerate(reviews_ints):\n    features[i, -len(row):] = np.array(row)[:seq_len]\n\nsplit_frac = 0.95\nsplit_idx = int(len(features)*0.8)\ntrain_x, val_x = features[:split_idx], features[split_idx:]\ntrain_y, val_y = labels[:split_idx], labels[split_idx:]\n\ntest_idx = int(len(val_x)*0.5)\nval_x, test_x = val_x[:test_idx], val_x[test_idx:]\nval_y, test_y = val_y[:test_idx], val_y[test_idx:]\n\nprint(\"\\t\\t\\tFeature Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(train_x.shape), \n      \"\\nValidation set: \\t{}\".format(val_x.shape),\n      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\nlstm_size = 256\nlstm_layers = 1\nbatch_size_ = 500\nlearning_rate = 0.001\n\nn_words = len(vocab_to_int) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n\n# Create the graph object\ngraph = tf.Graph()\n# Add nodes to the graph\nwith graph.as_default():\n    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n    batch_size = tf.placeholder(tf.int32,[] ,name='batch_size')\n\n# Size of the embedding vectors (number of units in the embedding layer)\nembed_size = 500 \n\nwith graph.as_default():\n    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, inputs_)\n\nwith graph.as_default():\n    # Your basic LSTM cell\n    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    \n    # Add dropout to the cell\n    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n    \n    # Stack up multiple LSTM layers, for deep learning\n    cell = tf.contrib.rnn.MultiRNNCell([drop  for i in range(lstm_layers)])\n    \n    # Getting an initial state of all zeros\n    initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n\nwith graph.as_default():\n    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n                                             initial_state=initial_state)\nwith graph.as_default():\n    #predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n    ann= tf.contrib.layers.fully_connected(outputs[:, -1], 256, activation_fn=tf.sigmoid)\n    predictions = tf.contrib.layers.fully_connected(ann, 1, activation_fn=tf.sigmoid)\n    cost = tf.losses.mean_squared_error(labels_, predictions)\n    \n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n\nwith graph.as_default():\n    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\ndef get_batches(x, y, batch_size=100):\n    \n    n_batches = len(x)\/\/batch_size\n    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n    for ii in range(0, len(x), batch_size):\n        yield x[ii:ii+batch_size], y[ii:ii+batch_size]    \n        \n        \nepochs = 10\nwith graph.as_default():\n    #saver = tf.train.Saver()\n    saver = tf.train.Saver(var_list=tf.trainable_variables())\nwith tf.Session(graph=graph) as sess:\n    sess.run(tf.global_variables_initializer())\n    iteration = 1\n    for e in range(epochs):\n        state = sess.run(initial_state,feed_dict={batch_size:batch_size_})\n        \n        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size_), 1):\n            feed = {inputs_: x,\n                    labels_: y[:, None],\n                    keep_prob: 0.5,\n                    initial_state: state,\n                    batch_size:batch_size_ }\n            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n            \n            if iteration%5==0:\n                print(\"Epoch: {}\/{}\".format(e, epochs),\n                      \"Iteration: {}\".format(iteration),\n                      \"Train loss: {:.3f}\".format(loss))\n\n            if iteration%25==0:\n                val_acc = []\n                val_state = sess.run(initial_state,feed_dict={batch_size:batch_size_})\n                for x, y in get_batches(val_x, val_y, batch_size_):\n                    feed = {inputs_: x,\n                            labels_: y[:, None],\n                            keep_prob: 1,\n                            initial_state: val_state,\n                            batch_size:batch_size_}\n                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n                    val_acc.append(batch_acc)\n                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n            iteration +=1\n    saver.save(sess, \"checkpoints\/sentiment.ckpt\")\n    #saver.save(sess, \"sentiment_model\")","7354911c":"cleaned_test_reviews=[]\nfor review in test[\"review\"]:\n    cleaned_test_reviews.append(review_to_words( review ))\n\n\nreviews_test_ints = []\nfor each in cleaned_test_reviews:\n    reviews_test_ints.append([vocab_to_int[word] for word in each.split()])\n    \nnon_zero_idx = [ii for ii, review in enumerate(reviews_test_ints) if len(review) != 0]\nreviews_test_ints = [reviews_test_ints[ii] for ii in non_zero_idx]    \n\nseq_len = 1000\nfeatures_test = np.zeros((len(reviews_test_ints), seq_len), dtype=int)\nfor i, row in enumerate(reviews_test_ints):\n    features_test[i, -len(row):] = np.array(row)[:seq_len]\nX_test=features_test\n","8ba5399d":"X_test.shape\n","066060c7":"preds=[]\nwith tf.Session(graph=graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    #saver.restore(sess, tf.train.latest_checkpoint('.\/'))\n\n    test_state = sess.run(initial_state,feed_dict={batch_size:batch_size_})\n    for i in range(0,X_test.shape[0],batch_size_):\n        x=X_test[i:i+batch_size_]\n        \n        feed = {inputs_: x,                \n                keep_prob: 1,\n                initial_state: test_state,\n                batch_size:1  \n               }\n        pred = sess.run(predictions, feed_dict=feed)\n        preds.extend(pred)\n","efc03936":"sentiments=[1 if p>0.5 else 0 for p in preds ]\ntest_sub=pd.read_csv('\/kaggle\/input\/word2vec-nlp-tutorial\/sampleSubmission.csv')\ntest_sub=test_sub.drop('sentiment',axis=1)\ntest_sub['sentiment']=np.array( sentiments)\ntest_sub.to_csv('sampleSubmission06.csv',index=None)\n","17e86948":"import os\nprint(os.getcwd())","bfcf72e3":"from IPython.display import FileLink\nFileLink(r'sampleSubmission06.csv')","c58d759e":"** 95.476% acc **"}}